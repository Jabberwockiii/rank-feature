[
  {
    "title": "SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering",
    "link": "https://arxiv.org/pdf/2311.12775.pdf",
    "upvote": "28",
    "text": "SuGaR: Surface-Aligned Gaussian Splatting for\nEfficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\nAntoine Gu\u00b4edon\nVincent Lepetit\nLIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, France\nhttps://anttwo.github.io/sugar/\nFigure 1. We introduce a method that extracts accurate and editable meshes from 3D Gaussian Splatting representations within minutes\non a single GPU. The meshes can be edited, animated, composited, etc. with very realistic Gaussian Splatting rendering, offering new\npossibilities for Computer Graphics. Note for example that we changed the posture of the robot between the captured scene on the bottom\nleft and the composited scene on the right. The supplementary material provides more examples, including a video illustrating our results.\nAbstract\nWe propose a method to allow precise and extremely fast\nmesh extraction from 3D Gaussian Splatting [15]. Gaus-\nsian Splatting has recently become very popular as it yields\nrealistic rendering while being significantly faster to train\nthan NeRFs. It is however challenging to extract a mesh\nfrom the millions of tiny 3D Gaussians as these Gaussians\ntend to be unorganized after optimization and no method\nhas been proposed so far. Our first key contribution is a\nregularization term that encourages the Gaussians to align\nwell with the surface of the scene. We then introduce a\nmethod that exploits this alignment to extract a mesh from\nthe Gaussians using Poisson reconstruction, which is fast,\nscalable, and preserves details, in contrast to the March-\ning Cubes algorithm usually applied to extract meshes from\nNeural SDFs. Finally, we introduce an optional refinement\nstrategy that binds Gaussians to the surface of the mesh,\nand jointly optimizes these Gaussians and the mesh through\nGaussian splatting rendering. This enables easy editing,\nsculpting, animating, and relighting of the Gaussians by\nmanipulating the mesh instead of the Gaussians themselves.\nRetrieving such an editable mesh for realistic rendering is\ndone within minutes with our method, compared to hours\nwith the state-of-the-art method on SDFs, while providing a\nbetter rendering quality.\nErratum\nWe identified a minor typographical error in Subsection 4.1\nin the earlier version of the paper.\nIn the computation of our regularization term R in Equa-\ntion 8, we use p \u2192 \u00b1sg\u2217\np\n\u22122 log (d(p)) instead of p \u2192\n\u00b1sg\u2217\nq\n\u22122 log\n\u0000 \u00afd(p)\n\u0001\nas an \u2019ideal\u2019 distance function asso-\nciated with the density d (Equation 7). As detailed in the\npaper, this distance function aligns with the true surface of\nthe scene in an ideal scenario where d = \u00afd. We have up-\ndated Equation 7 to clarify this matter.\n1. Introduction\nAfter NeRFs [22], 3D Gaussian Splatting [15] has recently\nbecome very popular for capturing a 3D scene and render-\ning it from novel points of view. 3D Gaussian Splatting\noptimizes the positions, orientations, appearances (repre-\nsented as spherical harmonics), and alpha blending of many\ntiny 3D Gaussians on the basis of a set of training images\nof the scene to capture the scene geometry and appearance.\nBecause rendering the Gaussians is much faster than ren-\ndering a neural field, 3D Gaussian Splatting is much faster\nthan NeRFs and can capture a scene in a few minutes.\nWhile the Gaussians allow very realistic renderings of\nthe scene, it is still however challenging to extract the sur-\n1\narXiv:2311.12775v3  [cs.GR]  2 Dec 2023\nFigure 2. Our algorithm can extract a highly detailed mesh from any 3D Gaussian Splatting scene [15] within minutes on a single GPU (top:\nRenderings of our meshes without texture, bottom: Renderings of the meshes with bound Gaussians).\nface of the scene from them: As shown in Figure 3, after\noptimization by 3D Gaussian Splatting, the Gaussians do\nnot take an ordered structure in general and do not corre-\nspond well to the actual surface of the scene. In addition to\nthe surface itself, it is also often desirable to represent the\nscene as a mesh, which remains the representation of choice\nin many pipelines: A mesh-based representation allows for\npowerful tools for editing, sculpting, animating, and relight-\ning the scene. Because the Gaussians after Gaussian Splat-\nting are unstructured, it is very challenging to extract a mesh\nfrom them. Note that this is also challenging with NeRFs\nalbeit for different reasons.\nIn this paper, we first propose a regularization term that\nencourages the Gaussians to be well distributed over the\nscene surface so that the Gaussians capture much better the\nscene geometry, as shown in Figure 3. Our approach is to\nderive a volume density from the Gaussians under the as-\nsumption that the Gaussians are flat and well distributed\nover the scene surface. By minimizing the difference be-\ntween this density and the actual one computed from the\nGaussians during optimization, we encourage the 3D Gaus-\nsians to represent well the surface geometry.\nThanks to this regularization term, it becomes easier to\nextract a mesh from the Gaussians. In fact, since we intro-\nduce a density function to evaluate our regularization term,\na natural approach would be to extract level sets of this den-\nsity function. However, Gaussian Splatting performs den-\nsification in order to capture details of the scene with high\nfidelity, which results in a drastic increase in the number of\nGaussians. Real scenes typically end up with one or several\nmillions of 3D Gaussians with different scales and rotations,\nthe majority of them being extremely small in order to re-\nproduce texture and details in the scene. This results in a\ndensity function that is close to zero almost everywhere, and\nthe Marching Cubes algorithm [21] fails to extract proper\nlevel sets of such a sparse density function even with a fine\nvoxel grid, as also shown in Figure 3.\nInstead, we introduce a method that very efficiently sam-\nples points on the visible part of a level set of the den-\nsity function, allowing us to run the Poisson reconstruction\nalgorithm [14] on these points to obtain a triangle mesh.\nThis approach is scalable, by contrast with the Marching\nCubes algorithm for example, and reconstructs a surface\nmesh within minutes on a single GPU, compared to other\nstate of the art methods relying on Neural SDFs for ex-\ntracting meshes from radiance fields, that require at least\n24 hours on one GPU [20, 36, 38, 39] and rely on multiple\nGPUs to speed up the process [26].\nAs illustrated in Figures 2 and 4, our method produces\nhigh quality meshes. The challenge is in efficiently iden-\ntifying points lying on the level set. To do this, we rely\non the Gaussians depth maps seen from the training view-\npoints. These depth maps can be obtained by extending\nthe Gaussian Splatting rasterizer, and we show how to ac-\n2\nwithout our regularization term\nwith our regularization term\nzoom on Gaussians\nmesh with\nmesh with our\non a planar surface\nMarching Cubes\nextraction method\nFigure 3. Extracting a mesh from Gaussians. Without regular-\nization, the Gaussians have no special arrangement after optimiza-\ntion, which makes extracting a mesh very difficult. Without our\nregularization term, Marching Cubes fail to extract an acceptable\nmesh. With our regularization term, Marching Cubes recover an\nextremely noisy mesh even with a very fine 3D grid. Our scalable\nextraction method obtains a mesh even without our regularization\nterm. Still, the mesh is noisy. By contrast, our full method suc-\nceeds in reconstructing an accurate mesh very efficiently.\ncurately sample points on the level set starting from these\ndepth maps.\nFinally, after extracting this mesh, we propose an op-\ntional refinement strategy that jointly optimizes the mesh\nand a set of 3D Gaussians through Gaussian splatting ren-\ndering only. This optimization enables high-quality render-\ning of the mesh using Gaussian splatting rendering rather\nthan traditional textured mesh rendering. This results in\nhigher performance in terms of rendering quality than other\nradiance field models relying on an underlying mesh at in-\nference [6, 26, 39]. As shown in Figure 1, this makes pos-\nsible the use of traditional mesh-editing tools for editing a\nGaussian Splatting representation of a scene, offering end-\nless possibilities for Computer Graphics.\nTo summarize, our contributions are:\n\u2022 a regularization term that makes the Gaussians capture\naccurately the geometry of the scene;\n\u2022 an efficient algorithm that extracts an accurate mesh from\nthe Gaussians within minutes;\n\u2022 a method to bind the Gaussians to the mesh, resulting in\na more accurate mesh, higher rendering quality than state\nof the art methods using a mesh for Novel View Synthe-\nsis [6, 26, 39], and allowing editing the scene in many\ndifferent ways.\nWe call our approach SuGaR. In the remainder of the pa-\nper, we discuss related work, give a brief overview of vanilla\n3D Gaussian Splatting, describe SuGaR, and compare it to\nthe state of the art.\n2. Related Work\nImage-based rendering (IBR) methods rely on a set of two-\ndimensional images of a scene to generate a representation\nof the scene and render novel views. The very first novel-\nview synthesis approaches were based on light fields [19],\nand developed the concept of volume rendering for novel\nviews. Their work emphasized the importance of efficiently\ntraversing volumetric data to produce realistic images.\nVarious scene representations have been proposed since,\nsuch as triangle meshes, point clouds, voxel grids, multi-\nplane images, or neural implicit functions.\nTraditional mesh-based IBR methods.\nStructure-from-\nmotion (SfM) [32] and subsequent multi-view stereo\n(MVS) [10] allow for 3D reconstruction of surfaces, lead-\ning to the development of several view synthesis algorithms\nrelying on triangle meshes as the primary 3D representa-\ntion of scenes. Such algorithms consider textured triangles\nor warp and blend captured images on the mesh surface to\ngenerate novel views [4, 12, 37]. [29, 30] consider deep\nlearning-based mesh representations for better view synthe-\nsis, bridging the gap between traditional graphics and mod-\nern machine learning techniques. While these mesh-based\nmethods take advantage of existing graphics hardware and\nsoftware for efficient rendering, they struggle with the cap-\nture of accurate geometry and appearance in complex re-\ngions.\nVolumetric IBR methods.\nVolumetric methods use voxel\ngrids, multiplane images, or neural networks to represent\nscenes as continuous volumetric functions of density and\ncolor. Recently, Neural Radiance Fields (NeRF) [22] intro-\nduced a novel scene representation based on a continuous\nvolumetric function parameterized by a multilayer percep-\ntron (MLP). NeRF produces photorealistic renderings with\nfine details and view-dependent effects, achieved through\nvolumetric ray tracing. However, the original NeRF is com-\nputationally expensive and memory intensive.\nTo address these challenges, several works have im-\nproved NeRF\u2019s performance and scalability. These meth-\nods leverage discretized or sparse volumetric representa-\ntions like voxel grids and hash tables as ways to store\nlearnable features acting as positional encodings for 3D\npoints [5, 13, 23, 34, 41], hierarchical sampling strate-\ngies [2, 11, 28, 40], or low-rank approximations [5]. How-\never, they still rely on volumetric ray marching, which\nis incompatible with standard graphics hardware and soft-\nware designed for rendering polygonal surfaces.\nRecent\nworks have proposed modifying the NeRF\u2019s representation\nof geometry and emitted radiance to allow for better recon-\nstruction of specular materials [35] or relighting the scene\nthrough an explicit decomposition into material and lighting\nproperties [3, 18, 33, 43].\n3\nHybrid IBR methods.\nSome methods build on differen-\ntiable rendering to combine the advantages of mesh-based\nand volumetric methods, and allow for surface reconstruc-\ntion as well as better editability. They use a hybrid volume-\nsurface representation, which enables high-quality meshes\nsuitable for downstream graphics applications while effi-\nciently modeling view-dependent appearance.\nIn partic-\nular, some works optimize neural signed distance func-\ntions (SDF) by training neural radiance fields in which the\ndensity is derived as a differentiable transformation of the\nSDF [7, 8, 20, 24, 36, 38]. A triangle mesh can finally\nbe reconstructed from the SDF by applying the Marching\nCubes algorithm [21]. However, most of these methods do\nnot target real-time rendering.\nAlternatively, other approaches \u201cbake\u201d the rendering ca-\npacity of an optimized NeRF or neural SDF into a much ef-\nficient structure relying on an underlying triangle mesh [6]\nthat could benefit from the traditional triangle rasteriza-\ntion pipeline. In particular, the recent BakedSDF [39] re-\nconstructs high quality meshes by optimizing a full neural\nSDF model, baking it into a high-resolution triangle mesh\nthat combines mesh rendering for interpolating features and\ndeep learning to translate these features into images, and\nfinally optimizes a view-dependent appearance model.\nHowever, even though it achieves real-time rendering\nand produces impressive meshes of the surface of the scene,\nthis model demands training a full neural SDF with an ar-\nchitecture identical to Mip-NeRF360 [1], which necessi-\ntates 48 hours of training.\nSimilarly, the recent method NeRFMeshing [26] pro-\nposes to also bake any NeRF model into a mesh structure,\nachieving real-time rendering. However, the meshing per-\nformed in this method lowers the quality of the rendering\nand results in a PSNR much lower than our method. Ad-\nditionally, this method still requires training a full NeRF\nmodel beforehand, and needs approximately an hour of\ntraining on 8 V100 NVIDIA GPUs to allow for mesh train-\ning and extraction.\nOur method is much faster at retrieveing a 3D mesh from\n3D Gaussian Splatting, which is itself much faster than\nNeRFs. As our experiments show, our rendering done by\nbounding Gaussians to the mesh results in higher quality\nthan previous solutions based on meshes.\nPoint-based IBR methods.\nAlternatively, point-based\nrepresentations for radiance field excel at modeling thin ge-\nometry and leverage fast point rasterization pipelines to ren-\nder images using \u03b1-blending rather than ray-marching [17,\n31]. In particular, the very recent 3D Gaussian Splatting\nmodel [15] allows for optimizing and rendering scenes with\nspeed and quality never seen before.\n3. 3D Gaussian Splatting\nFor the sake of completeness, we briefly describe the orig-\ninal 3D Gaussian Splatting method here. The scene is rep-\nresented as a (large) set of Gaussians, where each Gaussian\ng is represented by its mean \u00b5g and its covariance \u03a3g is pa-\nrameterized by a scaling vector sg \u2208 R3 and a quaternion\nqg \u2208 R4 encoding the rotation of the Gaussian. In addition,\neach Gaussian is associated with its opacity \u03b1g \u2208 [0, 1] and\na set of spherical harmonics coordinates describing the col-\nors emitted by the Gaussian for all directions.\nAn image of a set of Gaussians can be rendered from a\ngiven viewpoint thanks to a rasterizer. This rasterizer splats\nthe 3D Gaussians into 2D Gaussians parallel to the image\nplane for rendering, which results in an extremely fast ren-\ndering process. This is the key component that makes 3D\nGaussian Splatting much faster than NeRFs, as it is much\nfaster than the ray-marching compositing required in the op-\ntimization of NeRFs.\nGiven a set of images, the set of Gaussians is initialized\nfrom the point cloud produced by SfM [32]. The Gaus-\nsians\u2019 parameters (means, quaternions, scaling vectors, but\nalso opacities and spherical harmonics parameters) are opti-\nmized to make the renderings of the Gaussians match the in-\nput images. During optimization, more Gaussians are added\nto better fit the scene\u2019s geometry. As a consequence, Gaus-\nsian Splatting generally produces scenes with millions of\nGaussians that can be extremely small.\n4. Method\nWe present our SuGaR in this section:\n\u2022 First, we detail our loss term that enforces the alignment\nof the 3D Gaussians with the surface of the scene during\nthe optimization of Gaussian Splatting.\n\u2022 We then detail our method that exploits this alignment\nfor extracting a highly detailed mesh from the Gaussians\nwithin minutes on a single GPU.\n\u2022 Finally, we describe our optional refinement strategy that\njointly optimizes the mesh and 3D Gaussians located on\nthe surface of the mesh using Gaussian Splatting render-\ning. This strategy results in a new set of Gaussians bound\nto an editable mesh.\n4.1. Aligning the Gaussians with the Surface\nAs discussed in the introduction, to facilitate the creation\nof a mesh from the Gaussians, we introduce a regulariza-\ntion term into the Gaussian Splatting optimization that en-\ncourages the Gaussians to be aligned with the surface of the\nscene and well distributed over this surface. Our approach is\nto derive an SDF from the Gaussians under the assumption\nthat the Gaussians have the desired properties. By mini-\nmizing the difference between this SDF and the actual SDF\ncomputed for the Gaussians, we encourage the Gaussians to\nhave these properties.\n4\n(a) Mesh & Gaussians\n(b) Mesh (No Texture)\n(c) Mesh normals\n(a) Mesh & Gaussians\n(b) Mesh (No Texture)\n(c) Mesh normals\nFigure 4. Examples of (a) renderings and (b) reconstructed meshes with SuGaR. The (c) normal maps help visualize the geometry.\nFor a given Gaussian Splatting scene, we start by con-\nsidering the corresponding density function d : R3 \u2192 R+,\ncomputed as the sum of the Gaussian values weighted by\ntheir alpha-blending coefficients at any space location p:\nd(p) =\nX\ng\n\u03b1g exp\n\u0012\n\u22121\n2(p \u2212 \u00b5g)T \u03a3\u22121\ng (p \u2212 \u00b5g)\n\u0013\n,\n(1)\nwhere the \u00b5g, \u03a3g, and \u03b1g are the centers, covariances, and\nalpha-blending coefficients of the Gaussians, respectively.\nLet us consider what this density function becomes if the\nGaussians are well distributed and aligned with the surface.\nFirst, in such scenario, the Gaussians would have limited\noverlap with their neighbors. As illustrated in Figure 3 (top-\nleft), this is not the case in general. Then, for any point\np \u2208 R3 close to the surface of the scene, the Gaussian g\u2217\nclosest to the point p is likely to contribute much more than\nothers to the density value d(p). We could then approximate\nthe Gaussian density at p by:\n\u03b1g\u2217 exp\n\u0012\n\u22121\n2(p \u2212 \u00b5g\u2217)T \u03a3\u22121\ng\u2217 (p \u2212 \u00b5g\u2217)\n\u0013\n,\n(2)\nwhere the \u201cclosest Gaussian\u201d g\u2217 is taken as the Gaussian\nwith the largest contribution at point p:\ng\u2217 = arg min\ng\n\b\n(p \u2212 \u00b5g)T \u03a3\u22121\ng (p \u2212 \u00b5g)\n\t\n.\n(3)\nEq. (2) thus considers that the contribution of the closest\nGaussian g\u2217 to the density at p is much higher than the con-\ntribution of the other Gaussians. This will help us encourage\nthe Gaussians to be well spread.\nWe also would like the 3D Gaussians to be flat, as they\nwould then be aligned more closely with the surface of the\nmesh. Consequently, every Gaussian g would have one of\nits three scaling factors close to 0 and:\n(p \u2212 \u00b5g)T \u03a3\u22121\ng (p \u2212 \u00b5g) \u2248 1\ns2g\n\u27e8p \u2212 \u00b5g, ng\u27e92 ,\n(4)\nwhere sg the smallest scaling factor of the Gaussian and ng\nthe direction of the corresponding axis. Moreover, because\nwe want Gaussians to describe the true surface of the scene,\nwe need to avoid semi-transparent Gaussians. Therefore,\nwe want Gaussians to be either opaque or fully transpar-\nent, in which case we can drop them for rendering. Conse-\nquently, we want to have \u03b1g = 1 for any Gaussian g.\nIn such scenario, the density of the Gaussians could fi-\nnally be approximated by density \u00afd(p) with:\n\u00afd(p) = exp\n\u0012\n\u2212 1\n2s2\ng\u2217 \u27e8p \u2212 \u00b5g\u2217, ng\u2217\u27e92\n\u0013\n.\n(5)\nA first strategy to enforce our regularization is to add\nterm |d(p) \u2212 \u00afd(p)| to the optimization loss. While this ap-\nproach works well to align Gaussians with the surface, we\nnoticed that computing a slightly different loss relying on an\nSDF rather than on density further increases the alignment\nof Gaussians with the surface of the scene. For a given flat\nGaussian, i.e., sg = 0, considering level sets is meaning-\nless since all level sets would degenerate toward the plane\npassing through the center of the Gaussian \u00b5g with normal\nng. The distance between point p and the true surface of the\nscene would be approximately |\u27e8p \u2212 \u00b5g\u2032, ng\u2032\u27e9|, the distance\nfrom p to this plane. Consequently, the zero-crossings of\nthe Signed Distance Function\n\u00aff(p) = \u00b1sg\u2217\nq\n\u22122 log\n\u0000 \u00afd(p)\n\u0001\n(6)\ncorresponds to the surface of the scene. More generally, we\ndefine\nf(p) = \u00b1sg\u2217\np\n\u22122 log (d(p))\n(7)\nas the \u201cideal\u201d distance function associated with the density\n5\nFigure 5. Efficiently estimating \u02c6f(p) of the SDF of the sur-\nface generated from Gaussians. We render depth maps of the\nGaussians, sample points p in the viewpoint according to the dis-\ntribution of the Gaussians. Value \u02c6f(p) is taken as the 3D distance\nbetween p and the intersection between the line of sight for p and\nthe depth map.\nfunction d. This distance function corresponds to the true\nsurface of the scene in an ideal scenario where d = \u00afd. We\ntherefore take our regularization term R as\nR =\n1\n|P|\nX\np\u2208P\n| \u02c6f(p) \u2212 f(p)| ,\n(8)\nby sampling 3D points p and summing the differences at\nthese points between the ideal SDF f(p) and an estimate\n\u02c6f(p) of the SDF of the surface created by the current Gaus-\nsians. P refers to the set of sampled points.\nComputing efficiently \u02c6f(p) is a priori challenging. To\ndo so, we propose to use the depth maps of the Gaussians\nfrom the viewpoints used for training\u2014these depth maps\ncan be rendered efficiently by extending the splatting raster-\nizer. Then, as shown in Figure 5, for a point p visible from a\ntraining viewpoint, \u02c6f(p) is the difference between the depth\nof p and the depth in the corresponding depth map at the\nprojection of p. Moreover, we sample points p following\nthe distribution of the Gaussians:\np \u223c\nY\ng\nN(.; \u00b5g, \u03a3g) ,\n(9)\nwith N(.; \u00b5g, \u03a3g) the Gaussian distribution of mean \u00b5g and\ncovariance \u03a3g as these points are likely to correspond to a\nhigh gradient for R.\nWe also add a regularization term to encourage the nor-\nmals of SDF f and the normals of SDF \u00aff to also be similar:\nRNorm =\n1\n|P|\nX\np\u2208P\n\r\r\r\r\n\u2207f(p)\n\u2225\u2207f(p)\u22252\n\u2212 ng\u2217\n\r\r\r\r\n2\n2\n.\n(10)\n4.2. Efficient Mesh Extraction\nTo create a mesh from the Gaussians obtained after op-\ntimization using our regularization terms in Eq. (8) and\nFigure 6. Sampling points on a level set for Poisson reconstruc-\ntion. Left: We sample points on the depth maps of the Gaussians\nand refine the point locations to move the points on the level set.\nRight: Comparison between the extracted mesh without (left) and\nwith (right) our refinement step. Since splatted depth maps are not\nexact, using directly the depth points for reconstruction usually re-\nsults in a large amount of noise and missing details.\nEq. (10), we sample 3D points on a level set of the den-\nsity computed from the Gaussians. The level set depends\non a level parameter \u03bb. Then, we obtain a mesh by simply\nrunning a Poisson reconstruction [14] on these points. Note\nthat we can also easily assign the points with the normals of\nthe SDF, which improves the mesh quality.\nThe challenge is in efficiently identifying points lying on\nthe level set. For this, as shown in Figure 6, we again rely\non the depth maps of the Gaussians as seen from the train-\ning viewpoints. We first randomly sample pixels from each\ndepth map. For each pixel m, we sample its line of sight\nto find a 3D point on the level set. Formally, we sample n\npoints p + tiv, where p is the 3D point in the depth map\nthat reprojects on pixel m, v is the direction of the line of\nsight, and ti \u2208 [\u22123\u03c3g(v), 3\u03c3g(v)] where \u03c3g(v) is the stan-\ndard deviation of the 3D Gaussian g in the direction of the\ncamera. The interval [\u22123\u03c3g(v), 3\u03c3g(v)] is the confidence\ninterval for the 99.7 confidence level of the 1D Gaussian\nfunction of t along the ray.\nThen, we compute the density values di = d(p + tiv)\nfrom Eq. (1) of these sampled points. If there exist i, j such\nthat di < \u03bb < dj, then there is a level set point located in\nthis range. If so, we use linear interpolation to compute the\ncoefficient t\u2217 such that p + t\u2217v is the level set point closest\nto the camera, verifying d(p + t\u2217v) = \u03bb. We also compute\nthe normals of the surface at points \u02c6p, which we naturally\ndefine as the normalized analytical gradient of the density\n\u2207d(\u02c6p)\n\u2225\u2207d(\u02c6p)\u22252 .\nFinally, we apply Poisson reconstruction to reconstruct a\nsurface mesh from the level set points and their normals.\n4.3. Binding New 3D Gaussians to the Mesh\nOnce we have extracted a first mesh, we can refine this mesh\nby binding new Gaussians to the mesh triangles and opti-\nmize the Gaussians and the mesh jointly using the Gaussian\nSplatting rasterizer. This enables the edition of the Gaus-\n6\nFigure 7. Joint refinement of mesh and Gaussians.\nLeft: We\nbind Gaussians to the triangles of the mesh. Depending on the\nnumber of triangles in the scene, we bind a different number of\nGaussians per triangle, with predefined barycentric coordinates.\nRight: Mesh before and after joint refinement.\nsian splatting scene with popular mesh editing tools while\nkeeping high-quality rendering thanks to the Gaussians.\nGiven the initial mesh, we instantiate new 3D Gaussians\non the mesh. More exactly, we associate a set of n thin\n3D Gaussians to each triangle of the mesh, sampled on the\nsurface of the triangle, as illustrated in Figure 7. To do so,\nwe slightly modify the structure of the original 3D Gaussian\nSplatting model.\nWe explicitly compute the means of the Gaussians from\nthe mesh vertices using predefined barycentric coordinates\nin the corresponding triangles during optimization. Also,\nthe Gaussians have only 2 learnable scaling factors instead\nof 3 and only 1 learnable 2D rotation encoded with a com-\nplex number rather than a quaternion, to keep the Gaussians\nflat and aligned with the mesh triangles. More details about\nthis parameterisation are given in the supplementary mate-\nrial. Like the original model, we also optimize an opacity\nvalue and a set of spherical harmonics for every Gaussian\nto encode the color emitted in all directions.\nFigure 7 shows an example of a mesh before and after\nrefinement. Figure 1 and the supplementary material give\nexamples of what can be done by editing the mesh.\n5. Experiments\n5.1. Implementation details\nAll our models are optimized on a single GPU Nvidia Tesla\nV100 SXM2 32 Go.\nRegularization.\nFor all scenes, we start by optimizing a\nGaussian Splatting with no regularization for 7,000 itera-\ntions in order to let the 3D Gaussians position themselves\nwithout any additional constraint. Then, we perform 2,000\niterations with an additional entropy loss on the opacities\n\u03b1g of the Gaussians, as a way to enforce them to become\nbinary.\nFinally, we remove Gaussians with opacity values un-\nder 0.5 and perform 6,000 iterations with the regularization\nterm introduced in Subsection 4.1, which makes a total of\n15,000 iterations. To compute the density values of points\nfrom a Gaussian g, we sum only the Gaussian functions\nfrom the 16 nearest Gaussians of g and update the list of\nnearest neighbors every 500 iterations. Optimization typ-\nically takes between 15 and 45 minutes depending on the\nscene.\nMesh extraction.\nFor all experiments except the ablation\npresented in Table 2, we extract the \u03bb-level set of the density\nfunction for \u03bb = 0.3. We perform Poisson reconstruction\nwith depth 10 and apply mesh simplification using quadric\nerror metrics [9] to decrease the resolution of the meshes.\nMesh extraction generally takes between 5 and 10 minutes\ndepending on the scene.\nJoint refinement.\nWe jointly refine the mesh and the\nbound 3D Gaussians for either 2,000, 7,000 or 15,000 iter-\nations. Depending on the number of iterations, the duration\nof refinement goes from a few minutes to an hour.\n5.2. Real-Time Rendering of Real Scenes\nFor evaluating our model, we follow the approach from\nthe original 3D Gaussian Splatting paper [15] and com-\npare the performance of several variations of our method\nSuGaR after refinement on real 3D scenes from 3 differ-\nent datasets: Mip-NeRF360 [1], DeepBlending [12] and\nTanks&Temples [16].\nWe call R-SuGaR-NK a refined\nSuGaR model optimized for N iterations during refinement.\nFollowing [15], we select the same sets of 2 scenes from\nTanks&Temples (Truck and Train) and 2 scenes from Deep-\nBlending (Playroom and Dr. Johnson). However, due to li-\ncensing issues and the unavailability of the scenes Flowers\nand Treehill, we perform the evaluation of all methods only\non 7 scenes from Mip-NeRF360 instead of the full set of 9\nscenes.\nWe compute the standard metrics PSNR, SSIM and\nLPIPS [44] to evaluate the quality of SuGaR\u2019s rendering us-\ning our extracted meshes and their bound surface Gaussians.\nNote that [6, 26, 39] also do not use plain textured mesh\nrendering. We compare to several baselines, some of them\nfocusing only on Novel View Synthesis [2, 15, 23, 41] and\nothers relying on a reconstructed mesh [6, 26, 39], just like\nour method SuGaR. Results on the Mip-NeRF360 dataset\nare given in Table 1. Results on Tanks&Temple and Deep-\nBlending are similar and can be found in the supplementary\nmaterial.\nEven though SuGaR focuses on aligning 3D Gaussians\nfor reconstructing a high quality mesh during the first stage\nof its optimization, it significantly outperforms the state of\nthe art methods for Novel View Synthesis using a mesh\nand reaches better performance than several famous models\n7\nIndoor scenes\nOutdoor scenes\nAverage on all scenes\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nNo mesh (except SuGaR)\nPlenoxels [42]\n24.83\n0.766\n0.426\n22.02\n0.542\n0.465\n23.62\n0.670\n0.443\nINGP-Base [23]\n28.65\n0.840\n0.281\n23.47\n0.571\n0.416\n26.43\n0.725\n0.339\nINGP-Big [23]\n29.14\n0.863\n0.242\n23.57\n0.602\n0.375\n26.75\n0.751\n0.299\nMip-NeRF360 [2]\n31.58\n0.914\n0.182\n25.79\n0.746\n0.247\n29.09\n0.842\n0.210\n3DGS [15]\n30.41\n0.920\n0.189\n26.40\n0.805\n0.173\n28.69\n0.870\n0.182\nR-SuGaR-15K (Ours)\n29.43\n0.910\n0.216\n24.40\n0.699\n0.301\n27.27\n0.820\n0.253\nWith mesh\nMobile-NeRF [6]\n\u2013\n\u2013\n\u2013\n21.95\n0.470\n0.470\n\u2013\n\u2013\n\u2013\nNeRFMeshing [26]\n23.83\n\u2013\n\u2013\n22.23\n\u2013\n\u2013\n23.15\n\u2013\n\u2013\nBakedSDF [39]\n27.06\n0.836\n0.258\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nR-SuGaR-2K (Ours)\n26.29\n0.872\n0.262\n22.97\n0.648\n0.360\n24.87\n0.776\n0.304\nR-SuGaR-7K (Ours)\n28.73\n0.904\n0.226\n24.16\n0.691\n0.313\n26.77\n0.813\n0.263\nR-SuGaR-15K (Ours)\n29.43\n0.910\n0.216\n24.40\n0.699\n0.301\n27.27\n0.820\n0.253\nTable 1. Quantitative evaluation of rendering quality on the Mip-NeRF360 dataset [2]. SuGaR is best among the methods that recover\na mesh, and still performs well compared to NeRF methods and vanilla 3D Gaussian Splatting.\nExtraction method\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMarching Cubes [21]\n23.91\n0.703\n0.392\nPoisson (centers) [14]\n23.76\n0.756\n0.340\nOurs (Surface level 0.1)\n24.62\n0.765\n0.313\nOurs (Surface level 0.3)\n24.87\n0.776\n0.304\nOurs (Surface level 0.5)\n24.91\n0.777\n0.304\nTable 2. Ablation for different mesh extraction methods on\nthe Mip-NeRF360 dataset [2] after applying our regulariza-\ntion term. For \u2019Poisson (centers)\u2019, we apply Poisson reconstruc-\ntion [14] using as surface points the centers of the 3D Gaussians.\nFor fair comparison, we calibrate the methods to enforce all ex-\ntracted meshes to have approximately 1,000,000 vertices.\nthat focus only on rendering, such as Instant-NGP [23] and\nPlenoxels [41]. This performance is remarkable as SuGaR\nis able to extract a mesh significantly faster than other meth-\nods.\nMoreover, SuGaR even reaches performance similar to\nstate-of-the-art models for rendering quality [2, 15] on some\nof the scenes used for evaluation. Two main reasons explain\nthis performance. First, the mesh extracted after the first\nstage of optimization serves as an excellent initialization for\npositioning Gaussians when starting the refinement phase.\nThen, the Gaussians constrained to remain on the surface\nduring refinement greatly increase the rendering quality as\nthey play the role of an efficient texturing tool and help re-\nconstructing very fine details missing in the extracted mesh.\nAdditional qualitative results are available in Figure 4.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n1M vertices (3DGS)\n24.51\n0.768\n0.295\n1M vertices (UV)\n21.24\n0.609\n0.478\n200K vertices (3DGS)\n24.24\n0.757\n0.300\n200K vertices (UV)\n21.44\n0.656\n0.419\nTable 3. Comparison between surface-aligned 3D Gaussians\nand an optimized traditional UV texture on the Mip-NeRF360\ndataset [2]. For fair comparison, we only use the diffuse spherical\nharmonics component when rendering images with SuGaR. Using\n3D Gaussians bound to the mesh greatly improves rendering qual-\nity, even though it contains less parameters than the UV texture.\n5.3. Mesh Extraction\nTo demonstrate the ability of our mesh extraction method\nfor reconstructing high-quality meshes that are well-suited\nfor view synthesis, we compare different mesh extraction\nalgorithms. In particular, we optimize several variations of\nSuGaR by following the exact same pipeline as our stan-\ndard model, except for the mesh extraction process: We\neither extract the mesh using a very fine marching cubes\nalgorithm [21], by applying Poisson reconstruction [14] us-\ning the centers of the 3D Gaussians as the surface point\ncloud, or by applying our mesh extraction method on differ-\nent level sets. Quantitative results are available in Table 2\nand show the clear superiority of our approach for meshing\n3D Gaussians. Figure 3 also illustrates how the marching\ncubes algorithm fails in this context.\n8\n5.4. Mesh Rendering Ablation\nTable 3 provides additional results to quantify how various\nparameters impact rendering performance. In particular, we\nevaluate how the resolution of the mesh extraction, i.e., the\nnumber of triangles, modifies the rendering quality. For\nfair comparison, we increase the number of surface-aligned\nGaussians per triangle when we decrease the number of tri-\nangles. Results show that increasing the number of vertices\nincreases the quality of rendering with surface Gaussians,\nbut meshes with less triangles are already able to reach state\nof the art results.\nThen, we illustrate the benefits of using Gaussians\naligned on the surface as a texturing tool for rendering\nmeshes. To this end, we also optimize traditional UV tex-\ntures on our meshes using differentiable mesh rendering\nwith traditional triangle rasterization. Even though render-\ning with surface-aligned Gaussians provides better perfor-\nmance, rendering our meshes with traditional UV textures\nstill produces satisfying results, which further illustrates the\nquality of our extracted meshes. Qualitative comparisons\nare provided in the supplementary material.\n6. Conclusion\nWe proposed a very fast algorithm to obtain an accurate 3D\ntriangle mesh for a scene via Gaussian Splatting. Moreover,\nby combining meshing and Gaussian Splatting, we make\npossible intuitive manipulation of the captured scenes and\nrealistic rendering, offering new possibilities for creators.\nAcknowledgements.\nThis work was granted access to the HPC resources\nof IDRIS under the allocation 2023-AD011013387R1\nmade by GENCI. We thank George Drettakis and Elliot\nVincent for inspiring discussions and valuable feedback.\nReferences\n[1] Jonathan T. Barron. Mip-NeRF: A Multiscale Representa-\ntion for Anti-Aliasing Neural Radiance Fields. In Interna-\ntional Conference on Computer Vision, 2021. 4, 7\n[2] Jonathan T. Barron.\nMip-NeRF 360: Unbounded Anti-\nAliased Neural Radiance Fields. In Conference on Computer\nVision and Pattern Recognition, 2022. 3, 7, 8, 2, 4\n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\nron, Ce Liu, and Hendrik P. A. Lensch. NeRD: Neural Re-\nflectance Decomposition from Image Collections. In Inter-\nnational Conference on Computer Vision, 2021. 3\n[4] Chris Buehler, Michael Bosse, Leonard Mcmillan, Steven\nGortler, and Michael Cohen. Unstructured Lumigraph Ren-\ndering. In ACM SIGGRAPH, 2001. 3\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. TensoRF: Tensorial Radiance Fields. In European\nConference on Computer Vision, 2022. 3\n[6] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi.\nMobileNeRF: Exploiting the Polygon\nRasterization Pipeline for Efficient Neural Field Rendering\non Mobile Architectures. In Conference on Computer Vision\nand Pattern Recognition, 2023. 3, 4, 7, 8\n[7] Chong Bao and Bangbang Yang, Zeng Junyi, Bao Hu-\njun, Zhang Yinda, Cui Zhaopeng, and Zhang Guofeng.\nNeuMesh: Learning Disentangled Neural Mesh-Based Im-\nplicit Field for Geometry and Texture Editing. In European\nConference on Computer Vision, 2022. 4\n[8] Franc\u00b8ois Darmon, B\u00b4en\u00b4edicte Bascle, Jean-Cl\u00b4ement Devaux,\nPascal Monasse, and Mathieu Aubry. Improving Neural Im-\nplicit Surfaces Geometry with Patch Warping. In Conference\non Computer Vision and Pattern Recognition, 2022. 4\n[9] Michael Garland and Paul S. Heckbert. Surface Simplifi-\ncation Using Quadric Error Metrics. In ACM SIGGRAPH,\n1997. 7\n[10] Michael Goesele, Noah Snavely, Brian Curless, Hugues\nHoppe, and Steven Seitz. Multi-View Stereo for Community\nPhoto Collections. In International Conference on Computer\nVision, 2007. 3\n[11] Peter Hedman and Pratul P. Srinivasan. Baking Neural Radi-\nance Fields for Real-Time View Synthesis. In International\nConference on Computer Vision, 2021. 3\n[12] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow.\nDeep Blending\nfor Free-Viewpoint Image-Based Rendering. In ACM SIG-\nGRAPH, 2018. 3, 7, 2, 4\n[13] Animesh Karnewar, Tobias Ritschel, Oliver Wang, and Niloy\nMitra. ReLU Fields: The Little Non-Linearity That Could.\nIn ACM SIGGRAPH, 2022. 3\n[14] Michael M. Kazhdan, Matthew Bolitho, and Hugues Hoppe.\nPoisson Surface Reconstruction. In Eurographics, 2006. 2,\n6, 8\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3D Gaussian Splatting for Real-Time\nRadiance Field Rendering. In ACM SIGGRAPH, 2023. 1, 2,\n4, 7, 8\n[16] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun.\nTanks and Temples: Benchmarking Large-Scale\nScene Reconstruction. In ACM SIGGRAPH, 2017. 7, 2, 4\n[17] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. Point-Based Neural Rendering with Per-\nView Optimization. In Computer Graphics Forum, 2021. 4\n[18] Zhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng\nHuang, Panos Achlioptas, and Sergey Tulyakov. NeROIC:\nNeural Rendering of Objects from Online Image Collections.\nIn ACM SIGGRAPH, 2022. 3\n[19] Marc Levoy and Pat Hanrahan. Light Field Rendering. In\nACM SIGGRAPH, 1996. 3\n[20] Zhaoshuo Li, Thomas M\u00a8uller, Alex Evans, Russell H. Tay-\nlor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin.\nNeuralangelo: High-Fidelity Neural Surface Reconstruction.\nIn Conference on Computer Vision and Pattern Recognition,\n2023. 2, 4\n[21] William E. Lorensen and Harvey E. Cline. Marching Cubes:\nA High Resolution 3D Surface Construction Algorithm. In\nACM SIGGRAPH, 1987. 2, 4, 8\n9\n[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View\nSynthesis.\nIn European Conference on Computer Vision,\n2020. 1, 3\n[23] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant Neural Graphics Primitives with a Mul-\ntiresolution Hash Encoding. In ACM SIGGRAPH, 2022. 3,\n7, 8, 2\n[24] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUNISURF: Unifying Neural Implicit Surfaces and Radiance\nFields for Multi-View Reconstruction. In International Con-\nference on Computer Vision, 2021. 4\n[25] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary Devito, Martin Rai-\nson, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An\nImperative Style, High-Performance Deep Learning Library.\nIn Advances in Neural Information Processing Systems. Cur-\nran Associates Inc., 2019. 1\n[26] Marie-Julie Rakotosaona, Fabian Manhardt, Diego Martin\nArroyo, Michael Niemeyer, Abhijit Kundu, and Federico\nTombari. NeRFMeshing: Distilling Neural Radiance Fields\ninto Geometrically-Accurate 3D Meshes. In DV, 2023. 2, 3,\n4, 7, 8\n[27] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor\nGordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari.\nAccelerating 3D Deep Learning with PyTorch3D. In arXiv\nPreprint, 2020. 1\n[28] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger.\nKiloNeRF: Speeding Up Neural Radiance Fields\nwith Thousands of Tiny MLPs. In International Conference\non Computer Vision, 2021. 3\n[29] Gernot Riegler and Vladlen Koltun. Free View Synthesis. In\nEuropean Conference on Computer Vision, 2020. 3\n[30] Gernot Riegler and Vladlen Koltun. Stable View Synthesis.\nIn Conference on Computer Vision and Pattern Recognition,\n2021. 3\n[31] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger.\nADOP: Approximate Differentiable One-Pixel Point Ren-\ndering. In ACM SIGGRAPH, 2022. 4\n[32] Noah Snavely, Steven M. Seitz, and Richard Szeliski. Photo\nTourism: Exploring Photo Collections in 3D. In ACM SIG-\nGRAPH, 2006. 3, 4\n[33] Pratul P. Srinivasan,\nBoyang Deng,\nXiuming Zhang,\nMatthew Tancik, Ben Mildenhall, and Jonathan T. Barron.\nNeRV: Neural Reflectance and Visibility Fields for Relight-\ning and View Synthesis. In Conference on Computer Vision\nand Pattern Recognition, 2021. 3\n[34] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct Voxel\nGrid Optimization: Super-Fast Convergence for Radiance\nFields Reconstruction. In Conference on Computer Vision\nand Pattern Recognition, 2022. 3\n[35] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan.\nRef-NeRF:\nStructured View-Dependent Appearance for Neural Radi-\nance Fields. In Conference on Computer Vision and Pattern\nRecognition, 2022. 3\n[36] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. NeuS: Learning Neural Im-\nplicit Surfaces by Volume Rendering for Multi-View Recon-\nstruction.\nIn Advances in Neural Information Processing\nSystems, 2021. 2, 4\n[37] Daniel N. Wood, Daniel I. Azuma, Ken Aldinger, Brian Cur-\nless, Tom Duchamp, David H. Salesin, and Werner Stuet-\nzle. Surface Light Fields for 3D Photography. In ACM SIG-\nGRAPH, 2000. 3\n[38] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume Rendering of Neural Implicit Surfaces. In Advances in\nNeural Information Processing Systems, 2021. 2, 4\n[39] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P. Srinivasan, Richard Szeliski, and Jonathan T. Bar-\nron. BakedSDF: Meshing Neural SDFs for Real-Time View\nSynthesis. In ACM SIGGRAPH, 2023. 2, 3, 4, 7, 8\n[40] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees For Real-Time Rendering\nof Neural Radiance Fields. In International Conference on\nComputer Vision, 2021. 3\n[41] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance Fields Without Neural Networks. In Conference\non Computer Vision and Pattern Recognition, 2022. 3, 7, 8\n[42] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance Fields Without Neural Networks. In Conference\non Computer Vision and Pattern Recognition, 2022. 8, 2\n[43] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and\nNoah Snavely.\nPhySG: Inverse Rendering with Spherical\nGaussians for Physics-Based Material Editing and Relight-\ning. In Conference on Computer Vision and Pattern Recog-\nnition, 2021. 3\n[44] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The Unreasonable Effectiveness of\nDeep Features as a Perceptual Metric.\nIn Conference on\nComputer Vision and Pattern Recognition, 2018. 7, 2, 4\n10\nSuGaR: Surface-Aligned Gaussian Splatting for\nEfficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\nSupplementary Material\nIn this supplementary material, we provide the following\nelements:\n\u2022 Details about the parameterisation of the bound gaussians\noptimized during our joint refinement strategy.\n\u2022 Additional implementation details.\n\u2022 Detailed quantitative results for real-time rendering of\nreal scenes, and mesh rendering ablation.\nWe also provide a video that offers an overview of the ap-\nproach and showcases additional qualitative results. Specif-\nically, the video demonstrates how SuGaR meshes can be\nused to animate Gaussian Splatting representations.\n7. Parameterisation of Gaussians bound to the\nsurface\nAs we explained in Section 4, once we have extracted the\nmesh from the Gaussian Splatting representation, we refine\nthis mesh by binding new Gaussians to the mesh triangles\nand optimize the Gaussians and the mesh jointly using the\nGaussian Splatting rasterizer. To keep the Gaussians flat\nand aligned with the mesh triangles, we explicitly compute\nthe means of the Gaussians from the mesh vertices using\npredefined barycentric coordinates in the corresponding tri-\nangles during optimization. Also, the Gaussians have only\n2 learnable scaling factors instead of 3 and only 1 learnable\n2D rotation. Indeed, we do not optimize a full quaternion\nthat would encode a 3D rotation, as performed in [15]; In-\nstead, we optimize a 2D rotation in the plane of the triangle.\nTherefore, the Gaussians stay aligned with the mesh trian-\ngles, but are allowed to rotate on the local surface. Like\nthe original model, we also optimize an opacity value and a\nset of spherical harmonics for every Gaussian to encode the\ncolor emitted in all directions.\nIn practice, for each Gaussian, we optimize a learnable\ncomplex number x + iy rather than a quaternion, encod-\ning the 2D rotation inside the triangle\u2019s plane. During op-\ntimization, we still need to compute an explicit 3D quater-\nnion encoding the 3D rotation of the Gaussians in the world\nspace to apply the rasterizer. To recover the full 3D quater-\nnion, we proceed as follows: For any 3D Gaussian g, we\nfirst compute the matrix R = [R(0), R(1), R(2)] \u2208 R3\u00d73\nencoding the rotation of its corresponding triangle: We se-\nlect as the first column R(0) of the matrix the normal of\nthe triangle, and as the second column R(1) a fixed edge\nof the triangle. We compute the third column R(2) with\na cross-product. Then, we compute the matrix Rg encod-\ning the full 3D rotation of the Gaussian by applying the\nlearned 2D complex number to the rotation of the trian-\ngle, as follows: R(0)\ng\n= R(0), R(1)\ng\n= x\u2032R(1) + y\u2032R(2)\nand R(2)\ng\n= \u2212y\u2032R(1) + x\u2032R(2), where x\u2032 =\nx\n|x2+y2| and\ny\u2032 =\ny\n|x2+y2|.\nAdjusting parameters for edition.\nBecause our learned\ncomplex numbers represent rotations in the space of the cor-\nresponding triangles, our representation is robust to mesh\nedition or animation: When editing the underlying mesh\nat inference, there is no need to update the learned 2D ro-\ntations as they remain the same when rotating or moving\ntriangles.\nConversely, when scaling or deforming a mesh, the tri-\nangle sizes might change, necessitating adjustments to the\nlearned scaling factors of the bound surface Gaussians. For\nexample, if the mesh size doubles, all Gaussian scaling fac-\ntors should similarly be multiplied by 2. In our implemen-\ntation, when editing the mesh, we modify in real-time the\nlearned scaling factors of a bound surface Gaussian by mul-\ntiplying them by the ratio between (a) the average length\nof the triangle\u2019s sides after modification and (b) the average\nlength of the original triangle\u2019s sides.\n8. Additional implementation details\nImplementation\nWe implemented our model with Py-\nTorch [25] and use 3D data processing tools from Py-\nTorch3D [27].\nWe also use the differentiable Gaussian\nSplatting rasterizer from the original 3D Gaussian Splatting\npaper [15]. We thank the authors for providing this amazing\ntool.\nMesh extraction.\nIn practice, we apply two Poisson re-\nconstructions for mesh extraction:\none for foreground\npoints, and one for background points.\nWe define fore-\nground points as points located inside the bounding box of\nall training camera poses, and background points as points\nlocated outside. We chose this simple distinction between\nforeground and background in order to design an approach\nas general as possible. However, depending on the content\nof the scene and the main objects to reconstruct, defining a\ncustom bounding box for foreground points could improve\nthe quality and precision of the extracted mesh.\nJoint refinement.\nDuring joint refinement, we also com-\npute a normal consistency term on the mesh\u2019s faces to fur-\nther regularize the surface. This term doesn\u2019t affect perfor-\nmance in terms of PSNR, SSIM, or LPIPS. However, it does\n1\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPlenoxels [42]\n21.07\n0.719\n0.379\nINGP-Base [23]\n21.72\n0.723\n0.330\nINGP-Big [23]\n21.92\n0.744\n0.304\nMip-NeRF360 [2]\n22.22\n0.758\n0.257\n3DGS [15]\n23.14\n0.841\n0.183\nR-SuGaR-2K (Ours)\n19.70\n0.743\n0.284\nR-SuGaR-7K (Ours)\n21.09\n0.786\n0.233\nR-SuGaR-15K (Ours)\n21.58\n0.795\n0.219\nTable 4.\nQuantitative evaluation on Tanks&Temples [16].\nSuGaR is not as good as as vanilla 3D Gaussian Splatting in terms\nof rendering quality as it relies on a mesh but higher than the other\nmethods that do not recover a mesh.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPlenoxels [42]\n23.06\n0.794\n0.510\nINGP-Base [23]\n23.62\n0.796\n0.423\nINGP-Big [23]\n24.96\n0.817\n0.390\nMip-NeRF360 [2]\n29.40\n0.901\n0.244\n3DGS [15]\n29.41\n0.903\n0.242\nR-SuGaR-2K (Ours)\n27.31\n0.873\n0.303\nR-SuGaR-7K (Ours)\n29.30\n0.893\n0.273\nR-SuGaR-15K (Ours)\n29.41\n0.893\n0.267\nTable 5. Quantitative evaluation on DeepBlending [12]. SuGaR\nis not as good as as vanilla 3D Gaussian Splatting in terms of ren-\ndering quality as it relies on a mesh but higher than the other meth-\nods that do not recover a mesh.\nmarginally enhance visual quality by promoting smoother\nsurfaces.\n9. Additional Results for Real-Time Rendering\nof Real Scenes\nWe compute the standard metrics PSNR, SSIM and\nLPIPS [44] to evaluate the quality of SuGaR\u2019s rendering\nusing our extracted meshes and their bound surface Gaus-\nsians. Results on the Mip-NeRF360 dataset are given in\nTable 1 in the main paper. Results on Tanks&Temple and\nDeepBlending are given in Tables 4 and 5. Tables 6, 7 and\n8 provide the detailed results for all scenes in the datasets.\n10. Additional Results for Mesh Renderig Ab-\nlation\nWe provide additional qualitative results to illustrate how\nvarious parameters impact rendering performance.\nFirst, we provide in Figure 8 a simple example show-\ning how the Gaussians constrained to remain on the surface\nduring refinement greatly increase the rendering quality as\nthey play the role of an efficient texturing tool and help re-\n(a) 2,000 iterations\n(b) 7,000 iterations\n(c) 15,000 iterations\nFigure 8. Refined SuGaR renderings with different numbers\nof refinement iterations. 2,000 iterations are usually enough to\nobtain high quality rendering (a), since the extracted mesh \u201ctex-\ntured\u201d with surface Gaussians is already an excellent initialization\nfor optimizing the model. However, further refinement helps the\nGaussians to capture texturing details and reconstruct extremely\nthin geometry that is finer that the resolution of the mesh, such as\nthe spokes of the bicycle, as seen in (b), (c).\n(a) SuGaR render\n(b) Mesh (No texture)\n(c) Mesh normals\nFigure 9. SuGaR renderings with (top:) 200,000 and (bottom:)\n1,000,000 vertices. Even with low-poly meshes, the 3D Gaussians\nbound to the mesh produce high quality renderings. Moreover,\nlow-poly meshes help to better regularize the surface.\nFigure 10. Qualitative comparison between (top:) a traditional\nUV texture optimized from training images, and (bottom:) the\nbound surface Gaussians. Even though high resolution UV tex-\ntures have good quality and can be rendered with our meshes using\nany traditional software, using 3D Gaussians bound to the surface\nof the mesh greatly improves the rendering quality. Meshes in\nthese images have 200,000 vertices only.\nconstructing very fine details missing in the extracted mesh.\nThen, in Figure 9 we illustrate how the resolution of\nthe mesh extraction, i.e., the number of triangles, modifies\nthe rendering quality. For fair comparison, we increase the\nnumber of surface-aligned Gaussians per triangle when we\ndecrease the number of triangles. Results show that increas-\ning the number of vertices increases the quality of rendering\nwith surface Gaussians, but meshes with lower triangles are\nalready able to reach state of the art results.\n2\nFinally, Figure 10 illustrates the benefits of using Gaus-\nsians aligned on the surface as a texturing tool for render-\ning meshes. To this end, we also optimize traditional UV\ntextures on our meshes using differentiable mesh rendering\nwith traditional triangle rasterization. Even though render-\ning with surface-aligned Gaussians provides better perfor-\nmance, rendering our meshes with traditional UV textures\nstill produces satisfying results, which further illustrates the\nquality of our extracted meshes.\n3\nMip-NeRF360 [2]\nDeepBlending [12]\nTanks&Temples [16]\nGarden\nKitchen\nRoom\nBicycle\nCounter\nBonsai\nStump\nPlayroom\nDr. Johnson\nTrain\nTruck\n200K vertices\nR-SuGaR-2K\n23.30\n25.74\n27.58\n21.53\n24.41\n26.50\n23.45\n27.83\n26.51\n18.15\n21.03\nR-SuGaR-7K\n24.99\n28.78\n29.47\n22.69\n26.86\n29.33\n24.45\n30.02\n28.41\n19.82\n22.31\nR-SuGaR-15K\n25.29\n29.38\n29.95\n22.91\n27.47\n30.42\n24.55\n30.08\n28.59\n20.40\n22.65\n1M vertices\nR-SuGaR-2K\n23.56\n26.15\n27.68\n21.80\n24.62\n26.70\n23.56\n27.93\n26.70\n18.32\n21.09\nR-SuGaR-7K\n25.06\n28.96\n29.57\n22.86\n26.92\n29.47\n24.55\n30.13\n28.47\n19.85\n22.34\nR-SuGaR-15K\n25.36\n29.56\n30.03\n23.14\n27.62\n30.51\n24.70\n30.12\n28.71\n20.50\n22.67\nTable 6. Quantitative evaluation of rendering quality in terms of PSNR on all scenes. A higher PSNR indicates better rendering\nquality. We adjust the number of bound surface-aligned Gaussians per triangle when we reduce the number of vertices, aiming for a similar\ncount across all models. Results show that increasing the number of vertices (i.e. increasing the resolution of the geometry) increases the\nquality of rendering with surface Gaussians, but meshes with less triangles are already able to reach state of the art results.\nMip-NeRF360 [2]\nDeepBlending [12]\nTanks&Temples [16]\nGarden\nKitchen\nRoom\nBicycle\nCounter\nBonsai\nStump\nPlayroom\nDr. Johnson\nTrain\nTruck\n200K vertices\nR-SuGaR-2K\n0.713\n0.859\n0.881\n0.572\n0.844\n0.895\n0.641\n0.883\n0.864\n0.694\n0.787\nR-SuGaR-7K\n0.762\n0.901\n0.904\n0.621\n0.883\n0.926\n0.679\n0.898\n0.888\n0.749\n0.822\nR-SuGaR-15K\n0.771\n0.907\n0.909\n0.631\n0.890\n0.933\n0.681\n0.897\n0.888\n0.763\n0.827\n1M vertices\nR-SuGaR-2K\n0.719\n0.866\n0.882\n0.583\n0.846\n0.894\n0.642\n0.883\n0.863\n0.698\n0.788\nR-SuGaR-7K\n0.764\n0.903\n0.905\n0.628\n0.884\n0.925\n0.680\n0.899\n0.887\n0.750\n0.821\nR-SuGaR-15K\n0.775\n0.908\n0.909\n0.640\n0.891\n0.932\n0.683\n0.898\n0.889\n0.764\n0.827\nTable 7. Quantitative evaluation of rendering quality in terms of SSIM on all scenes. A higher SSIM indicates better rendering quality.\nWe adjust the number of bound surface-aligned Gaussians per triangle when we reduce the number of vertices, aiming for a similar count\nacross all models. Results show that increasing the number of vertices (i.e. increasing the resolution of the geometry) increases the quality\nof rendering with surface Gaussians, but meshes with less triangles are already able to reach state of the art results.\nMip-NeRF360 [2]\nDeepBlending [12]\nTanks&Temples [16]\nGarden\nKitchen\nRoom\nBicycle\nCounter\nBonsai\nStump\nPlayroom\nDr. Johnson\nTrain\nTruck\n200K vertices\nR-SuGaR-2K\n0.280\n0.221\n0.280\n0.413\n0.288\n0.259\n0.390\n0.284\n0.314\n0.335\n0.235\nR-SuGaR-7K\n0.232\n0.175\n0.252\n0.363\n0.245\n0.228\n0.345\n0.260\n0.277\n0.274\n0.187\nR-SuGaR-15K\n0.218\n0.166\n0.243\n0.349\n0.234\n0.219\n0.336\n0.257\n0.268\n0.258\n0.174\n1M vertices\nR-SuGaR-2K\n0.281\n0.215\n0.282\n0.408\n0.287\n0.262\n0.391\n0.286\n0.319\n0.333\n0.236\nR-SuGaR-7K\n0.233\n0.173\n0.253\n0.360\n0.245\n0.231\n0.347\n0.265\n0.282\n0.275\n0.190\nR-SuGaR-15K\n0.220\n0.165\n0.246\n0.345\n0.234\n0.221\n0.338\n0.261\n0.273\n0.260\n0.178\nTable 8. Quantitative evaluation of rendering quality in terms of LPIPS [44] on all scenes. A lower LPIPS indicates better rendering\nquality. We adjust the number of bound surface-aligned Gaussians per triangle when we reduce the number of vertices, aiming for a similar\ncount across all models. The results indicate that the stronger regularity due to a smaller number of vertices leads to smoother surfaces and\nhigher LPIPS metrics when using the bound Gaussians.\n4\n"
  },
  {
    "title": "MagicDance: Realistic Human Dance Video Generation with Motions & Facial Expressions Transfer",
    "link": "https://arxiv.org/pdf/2311.12052.pdf",
    "upvote": "28",
    "text": "MagicPose: Realistic Human Poses and Facial Expressions Retargeting with\nIdentity-aware Diffusion\nDi Chang 1 2 Yichun Shi 2 Quankai Gao 1 Jessica Fu 1 Hongyi Xu 2 Guoxian Song 2 Qing Yan 2 Yizhe Zhu 2\nXiao Yang 2 Mohammad Soleymani 1\n1University of Southern California\n2ByteDance Inc.\ndichang@usc.edu\nAbstract\nIn this work, we propose MagicPose, a diffusion-\nbased model for 2D human pose and facial expres-\nsion retargeting. Specifically, given a reference\nimage, we aim to generate a person\u2019s new images\nby controlling the poses and facial expressions\nwhile keeping the identity unchanged. To this end,\nwe propose a two-stage training strategy to disen-\ntangle human motions and appearance (e.g., facial\nexpressions, skin tone and dressing), consisting\nof (1) the pre-training of an appearance-control\nblock and (2) learning appearance-disentangled\npose control. Our novel design enables robust\nappearance control over generated human images,\nincluding body, facial attributes, and even back-\nground. By leveraging the prior knowledge of im-\nage diffusion models, MagicPose generalizes well\nto unseen human identities and complex poses\nwithout the need for additional fine-tuning. More-\nover, the proposed model is easy to use and can\nbe considered as a plug-in module/extension to\nStable Diffusion.\n1. Introduction\nHuman motion transfer is a challenging task in computer\nvision. This problem involves retargeting body and facial\nmotions, from one source image to a target image. Such\nmethods can be used for image stylization, editing, digital\nhuman synthesis, and possibly data generation for training\nperception models.\nTraditionally, human motion transfer is achieved by training\na task-specific generative model, such as generative adver-\nsarial networks (GANs) on specific datasets, e.g., (Siarohin\net al., 2018; 2019b; Liu et al., 2019; Wei et al., 2020; Sun\net al., 2022) for body pose and (Wu et al., 2020; Qiao et al.,\n2018; Hong et al., 2022) for facial expressions. Such meth-\nods commonly suffer from two issues: (1) they are typically\ndependent on an image warping module (Siarohin et al.,\nReference\nPose 1\nPose 2\nPose 3\nFigure 1. MagicPose can provide zero-shot and realistic human\nposes and facial expressions retargeting for human images of dif-\nferent styles and poses. A shared model is used here for in-the-wild\ngeneralization without any fine-tuning on target domains. Our pro-\nposed modules can be treated as an extension/plug-in to the original\ntext-to-image model without modifying its pre-trained weight.\n2018; 2019b) and hence struggle to interpolate the body\nparts that are invisible in the reference image due to per-\nspective change or self-occlusion, and (2) they can hardly\ngeneralize to images that are different from the training data,\ngreatly limiting their application scope.\nRecently, diffusion models (Ho et al., 2020; Song et al.,\n2020; Rombach et al., 2021; Zhang et al., 2023) have exhib-\nited impressive ability on image generation (Bertalmio et al.,\n2000; Yeh et al., 2017; Lugmayr et al., 2022). By learn-\ning from web-scale image datasets, these models present\npowerful visual priors for different downstream tasks, such\nas image inpainting (Lugmayr et al., 2022; Saharia et al.,\n2022a; Jam et al., 2021), video generation (Ho et al., 2022;\nWu et al., 2023; Singer et al., 2022), 3D generation (Poole\n1\narXiv:2311.12052v2  [cs.CV]  2 Feb 2024\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\net al., 2022; Raj et al., 2023; Shi et al., 2023) and even image\nsegmentations (Amit et al., 2021; Baranchuk et al., 2021;\nWolleb et al., 2022). Thus, such diffusion priors are great\ncandidates for human motion transfer. Two recent studies,\nDreamPose (Karras et al., 2023) and DisCo (Wang et al.,\n2023), have attempted to adapt diffusion models for human\nbody re-posing. However, we found that they are still lim-\nited in either generation quality, identity preservation (as\ndiscussed in Section. 5.3), or temporal consistency due to the\nlimits in model design and training strategy. Moreover, there\nis no clear advantage of these methods over GAN-based\nmethods in generalizability. For example, Disco (Wang\net al., 2023) still needs to be fine-tuned to adapt to images\nof out-of-domain styles.\nIn this work, we propose MagicPose to fully exploit the po-\ntential of image diffusion priors for human pose retargeting,\ndemonstrating superior visual quality, identity preservation\nability, and domain generalizability, as illustrated in Fig-\nure. 1. Our key idea is to decompose the problem into two\ntasks: (1) identity/appearance control and (2) pose/motion\ncontrol, which we consider useful capabilities required by\nimage diffusion priors to achieve accurate motion transfer.\nCorrespondingly, as shown in Figure. 2, MagicPose has two\nsub-modules besides the Stable Diffusion (SD) (Rombach\net al., 2021): 1) Appearance Control Model that provides\nappearance guidance from a reference image to the SD via\nMulti-Source Attention Module, and 2) Pose ControlNet,\nwhich provides pose/expression guidance from a condition\nimage. A multi-stage training strategy is also proposed to ef-\nfectively learn these sub-modules to disentangle the appear-\nance and pose control. Extensive experiments demonstrate\nthe effectiveness of MagicPose which can retain well the\nkey features of the reference identities, including skin tone\nand clothing, while following the pose skeleton and facial\nlandmark inputs. Moreover, MagicPose can generalize well\nto unseen identities and motions without any fine-tuning.\nThe main contributions of this work are as follows:\n\u2022 An effective method (MagicPose) for human pose and\nexpression retargeting as a plug-in for Stable Diffusion.\n\u2022 Multi-Source Attention Module that offers detailed ap-\npearance guidance.\n\u2022 A two stage training strategy that enables appearance-pose-\ndisentangled generation.\n\u2022 Experiment on out-of-domain data demonstrating strong\ngeneralizability of our model to diverse image styles and\nhuman poses.\n\u2022 Comprehensive experiments conducted on TikTok dataset\nshowing model\u2019s superior performance in pose retargeting.\n2. Related Work\n2.1. Human Motion/Expression Transfer\nEarly work in human motion transfer primarily involved\nmanipulation of given image sequence segments to create\na desired action (Bregler et al., 1997; Efros et al., 2003;\nBeier & Neely, 1992). Subsequent solutions shifted their\nfocus towards generating three-dimensional (3D) represen-\ntations of human subjects and performing motion transfer\nwithin 3D environments (Cheung et al., 2004; Xu et al.,\n2011). However, these approaches were characterized by\nsignificant time and labor requirements. In contrast, recent\nadvancements leverage deep learning to learn detailed rep-\nresentations of the input (Tulyakov et al., 2018; Kim et al.,\n2018; Chan et al., 2019a). This shift has facilitated motion\ntransfer with heightened realism and increased automation.\nGenerative Adversarial Networks (GANs) have been a clear\ndeep learning approach to motion transfer tasks (AlBahar\net al., 2021; Bregler et al., 1997; Efros et al., 2003), pro-\nviding realistic image generation and Conditional GANs\nadding further conditioning (Mirza & Osindero, 2014). Kim\net al. (Kim et al., 2018) took synthetic renderings, interior\nface model, and gaze map to transfer head position and facial\nexpression from one human subject to another, presenting\nthe results as detailed portrait videos. MoCoGAN (Tulyakov\net al., 2018) also implements unsupervised adversarial train-\ning to perform motion and facial expression transfer onto\nnovel subjects. Chan et al. (Chan et al., 2019a) further ad-\nvanced this approach to full-body human motion synthesis\nby utilizing a video-to-video approach, taking in 2D video\nsubjects and 2D pose stick figures to produce transferred\ndance sequences on new human subjects. In the sub-domain\nof fashion video synthesis, DreamPose (Karras et al., 2023)\nused SD with human image input and pose sequence in-\nput to generate videos featuring human subjects executing\npose sequences with intricate fabric motion. DisCo (Wang\net al., 2023), another SD-based model, contributed to the\nuse-case of human dance generation, enabling controllable\nhuman reference, background reference, and pose maps to\nproduce arbitrary compositions that maintain faithfulness\nand generalizability to unseen subjects.\n2.2. Image/Video Diffusion Models\nPrevious research has demonstrated the effectiveness of\ndiffusion probabilistic models (Song et al., 2021a;b) for\nimage generation\n(Ramesh et al., 2022; Saharia et al.,\n2022b; Nichol et al., 2021). Latent diffusion models (Ho\net al., 2020) have further advanced this domain by reduc-\ning computational costs by executing the diffusion step in a\nlower-dimensional latent space rather than pixel space. With\ncustomization and specification being important aspects of\ncontent generation, the text-to-image approach has gained\npopularity as a means of achieving controllable image gen-\n2\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nNoise\nAppearance Control Model\nPose ControlNet \nTarget Condition Map(s)\nStable Diffusion UNet\nReference\nAppearance Control\nPose Control\n:\n:\nTransformerBlock\nMotion Module (optional)\n:\n:\n:\nResNetBlock\nSelf-attention\nQ1\nK1\nK2\nV1\nV2\nK2\nV2\nQ2\n\u2026\n\u2026\nMulti-Source Self-Attention Module\nSelf-attention\n\u2026\n\u2026\na) Appearance Control Pretraining\nb) Appearance-Disentangled Pose Control\n: Zero Convolution\nFigure 2. Overview of the proposed MagicPose pipeline for controllable human poses and facial expressions retargeting with motions\n& facial expressions transfer. The Appearance Control Model is a copy of the entire Stable-Diffusion UNet, initialized with the same\nweight. The Stable-Diffusion UNet is frozen throughout the training. During a) Appearance Control Pretraining, we train the appearance\ncontrol model and its Multi-Source Self-Attention Module. During b) Appearance-disentangled Pose Control, we jointly fine-tune the\nAppearance Control Model, initialized with weights from a), and the Pose ControlNet. After these steps, an optional motion module can\nbe integrated into the pipeline and fine-tuned for better sequential output generation quality.\neration, with notable examples such as Imagen (Saharia\net al., 2022b) and SD (Rombach et al., 2021). The intro-\nduction of ControlNet (Zhang et al., 2023) extended the\napproach to controllable generation by introducing addi-\ntional conditioning to SD models, enabling input sources\nsuch as segmentation maps, pose key points, and more. Ad-\nditional condition inputs has enabled a higher degree of\ncustomization and task-specificity in the generated outputs,\nproviding a contextual foundation for conditional image\ngeneration. With the advancement of conditional image\ngeneration, there is a natural extension towards the synthe-\nsis of dynamic visual content. Blattmann et al. (Blattmann\net al., 2023) showed the use-case of latent diffusion models\nfor video generation by integrating a temporal dimension\nto the latent diffusion model and further fine-tuning the\nmodel on encoded image sequences. Similar to image gen-\neration, video generation has seen both text-based as well\nas condition-based approaches to control the synthesized\noutput.\n3. Preliminary\nLatent Diffusion Models (Rombach et al., 2021)\n(LDM) (Rombach et al., 2021), represent those diffusion\nmodels uniquely designed to operate within the latent\nspace facilitated by an autoencoder, specifically D(E(\u00b7)).\nA notable instance of such models is the Stable Diffu-\nsion (SD) (Rombach et al., 2022), which integrates a Vec-\ntor Quantized-Variational AutoEncoder (VQ-VAE) (Van\nDen Oord et al., 2017) and a U-Net structure (Ronneberger\net al., 2015). SD employs a CLIP-based transformer archi-\ntecture as a text encoder (Radford et al., 2021) to convert\ntext inputs into embeddings, denoted by ctext. The training\nregime of SD entails presenting the model with an image I\nand a text condition ctext. This process involves encoding the\nimage to a latent representation z0 = E(I) and subjecting\nit to a predefined sequence of T diffusion steps governed\nby a Gaussian process. This sequence yields a noisy latent\nrepresentation zT , which approximates a standard normal\ndistribution N(0, 1). SD\u2019s learning objective is iteratively\ndenoising zT back into the latent representation z0, formu-\nlated as follows:\nL = EE(I),ctext,\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, ctext)\u22252\n2\n\u0003\n(1)\nwhere \u03f5\u03b8 is the UNet with learnable parameters \u03b8 and\nt = 1, ..., T denotes the time-step embedding in denois-\ning. These modules employ convolutional layers, specifi-\ncally Residual Blocks (ResNetBlock), and incorporate both\nself- and cross-attention mechanisms through Transformer\nBlocks (TransformerBlock).\nControlNet is an extension of SD that is able to control\nthe generated image layout of SD without modifying the\noriginal SD\u2019s parameters. It achieves this by replicating\nthe encoder of SD to learn feature residuals for the latent\nfeature maps in SD. It has been successfully applied to\ndifferent controlled image generation tasks including pose-\nconditioned human image generation (Zhang et al., 2023).\n4. MagicPose\nGiven a image IR with a person in it, the objective of Mag-\nicPose to re-pose the person in the given image to the target\n3\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\npose {P, F}, where P is the human pose skeleton and F\nis the facial landmarks. Such a pipeline can be decom-\nposed into two sub-tasks: (1) keeping and transferring the\nappearance of the human individual and background from\nreference image and (2) controlling generated images with\nthe pose and expression defined by {P, F}. To ensure the\ngenerazability of the model, MagicPose is designed to in-\nherit the structures and parameters as much as possible from\npre-trained stable diffusion models. To this end, we propose\nan attention-based appearance controller by replicating the\nstructures of the original UNet. An additional ControlNet\nis then trained jointly to control the pose and expression of\nthe person. We train MagicPose on human video datasets\nwhere image pairs of the same person but different poses\nare available. Then during testing, the reference IR and\nposes {P, F} could come from different sources for pose\ntransfer. The overview of the proposed method (MagicPose)\nis illustrated in Figure. 2. We first presents our prelimi-\nnary experiments in terms of appearance control in Sec. 4.1,\nwhich motivates us to propose the Appearance Control Mod-\nule as elaborated in Sec. 4.2. Then, Sec. 4.3 presents the\nfine-tuning of the Appearance-disentangled Pose Control.\n4.1. Exploration of Appearance Control Mechanism\nWe first evaluated vanilla ControlNet for appearance control.\nAs shown in Figure 3, we found that ControlNet is not able\nto maintain the appearance when generating human images\nof different poses, making it unsuitable for the re-targeting\ntask. On the other side, recent studies (Cao et al., 2023; Lin\net al., 2023b; Zhang) have found that self-attention layers\nin the diffusion models is highly relevant to the appearance\nof the generated images. Inspired by them, we conduct\nan experiment on self-attention for zero-shot appearance\ncontrol, where the reference image and the noisy image\nare both forwarded through the diffusion UNet with their\nself-attention layers connected. A critical observation is that\nis such an architecture can naturally lead to an appearance\nresemblance between the two images, even without any\nfine-tuning (Figure 3 connected attention). One plausible\nexplanation is that self-attention layers in the UNet plays\nan important role to transmit the appearance information\nspatially and hence it could serve as a deformation module to\ngenerate similar images with different geometric structures.\nFrom another perspective, such an forward process mimics\nthe generation of two image as a single one, and thus, their\nappearance tend to be similar. However, the problem with\nsuch a zero-shot approach is that the generation results are\nnot stable.\n4.2. Appearance Control Pretraining\nGiven the above observations, we introduce our Appearance\nControl Model, which inherits the structure and capability\nof the zero-shot attention-based control but further extends\nConnected Attention\nReference Image \nControlNet\nOurs\nFigure 3. Identity and appearance control ability comparison be-\ntween different architectural designs.\nits stability by introducing task-specific parameters. In par-\nticular, it is designed as an auxiliary UNet branch to provide\nlayer-by-layer attention guidance. As shown in Figure. 2,\nour Appearance Control Model consists of another train-\nable copy of the original SD-UNet, which connects to the\nAppearance Control Model by sharing the key and value\nthrough the Multi-Source Self Attention Module.\nFormally,\nthe calculation of self-attention in Trans-\nformerBlocks of SD-UNet can be written as:\nSelf Attn = softmax( Q\u00b7KT\n\u221a\nd ) \u00b7 V\n(2)\nwhere Q, K, V are query, key, and value. d denotes the\ndimension of the key and query. In our Multi-Source Self\nAttention Module, we concatenate the key-value pairs from\nthe Appearance Control Model with SD-UNet together as\nnew key-value pairs and calculate the attention similar to\nEq. 2:\nOur Attn = softmax( Q1\u00b7(K1\u2295K2)T\n\u221a\nd\n) \u00b7 (V1 \u2295 V2)\n(3)\nwhere Q1, K1, V1 are query, key, and value from self-\nattention layers in the TransformerBlocks of SD-UNet and\nK2, V2 are from the Appearance Control Model. \u2295 refers to\nvector concatenation. In essence, the only modification for\nthe SD-UNet is to change the calculation of self-attention\nfrom Eq. 2 to Eq. 3.\nIn order to maintain the generalizability of the SD, in the\nfirst training stage (Appearance Control Pre-training), we\nfix the original UNet and only train the Appearance Control\nmodule. The pose ControlNet is not included in this stage.\nThe objective of Appearance Control Pretraining is:\nL = EE(I),A\u03b8(IR),\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, A\u03b8(IR))\u22252\n2\n\u0003 (4)\nwhere A\u03b8 is the Appearance Control Model taking reference\nimage IR as input. \u03f5\u03b8 is the SD-UNet, which takes the noisy\nlatent zt, denoising step t and Our Attn as inputs.\nComparison with ControlNet The proposed Appearance\nControl Model is novel and different in many ways from\nControlNets. In term of control objective, ControlNet was\nintroduced to control the geometrical shape and structural\ninformation in the text-to-image model, while our appear-\nance Control Model aims to provide identity and appearance\ninformation for the generated subject regardless of the given\ntext. In term of structure design, ControlNet copies the en-\ncoder and middle blocks of SD-UNet, whose output feature\n4\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nmaps are added to the decoder of SD-UNet to realize pose\ncontrol. On the other side, the proposed Appearance Con-\ntrol Model replicates a whole UNet model to controls the\ngeneration process of pre-trained diffusion model via atten-\ntion layers, enabling more flexible information interchange\namong distant pixels. And therefore it is more suited for the\ntask of pose retargeting.\n4.3. Appearance-disentangled Pose Control\nTo control the pose in the generated images, a naive so-\nlution directly integrates the pre-trained OpenPose Con-\ntrolNet model (Zhang et al., 2023) with our pre-trained\nAppearance Control Model without fine-tuning. However,\nour experiments indicate that such a combination struggles\nwith appearance-independent pose control, leading to se-\nvere errors between the generated poses and the input poses.\nTo address the issue, we reuse our pre-trained Appearance\nControl module to disentangle the pose ControlNet from\nappearance information. In particular, assuming the Appear-\nance Controller already provides a complete guidance for\nthe generated image\u2019s appearance, we fine-tune the Pose\nControlNet jointly with our Appearance Control Model. As\nsuch, Pose ControlNet exclusively modulates the pose at-\ntributes of the human, while the Appearance Control Model\nfocuses on appearance control. Specifically, we fine-tune\nMagicPose with an objective similar to latent diffusion train-\ning (Rombach et al., 2022):\nL = EE(I),A\u03b8(IR),P\u03b8(IC),\u03f5\u223cN (0,1),t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, A\u03b8(IR), P\u03b8(IC))\u22252\n2\n\u0003 (5)\nwhere P\u03b8 is the Pose ControlNet taking poses IC as inputs.\n5. Experiments\n5.1. Datasets\nTikTok (Jafarian & Park, 2021) dataset consists of 350\nsingle-person dance videos (with video length of 10-15\nseconds). Most of these videos contain the face and upper-\nbody of a human. For each video, we extract frames at 30fps\nand run OpenPose (Cao et al., 2019; Simon et al., 2017;\nCao et al., 2017; Wei et al., 2016) on each frame to infer the\nhuman pose skeleton, facial landmarks, and hand poses. 335\nvideos are sampled as the training split. We follow (Wang\net al., 2023) and use their 10 TikTok-style videos depicting\ndifferent people from the web as the testing split.\nEverybody Dance Now (Chan et al., 2019b) consists of full-\nbody videos of five subjects. Experiments on this dataset\naim to test our method\u2019s generalization ability to in-the-wild,\nfull-body motions.\nSelf-collected Out-of-Domain Images come from online\nresources. We use them to test our method\u2019s generalization\nability to in-the-wild appearance.\n5.2. Implementation Details\nWe first pre-train the appearance control model on 8\nNVIDIA A100 GPUs with batch size 64 for 10k steps with\nimage size 512 \u00d7 512 and learning rate 0.0001. We then\njointly fine-tune the appearance and pose control model on\n8 NVIDIA A100 GPUs with batch size 16 for 20K steps.\nThe Stable-Diffusion UNet weight is frozen during all ex-\nperiments. During training, we randomly sampled the two\nframes of the video as the reference and target. Both refer-\nence and target images are randomly cropped at the same\nposition along the height dimension with the aspect ratio of\n1 before resizing to 512 \u00d7 512. For evaluation, we apply\ncenter cropping instead of random cropping. We initialize\nthe U-Net model with the pre-trained weights of Stable-\nDiffusion Image Variations (Justin & Lambda, 2022). The\nAppearance Control Model branch is initialized with the\nsame weight as the U-Net model. After Appearance Con-\ntrol pre-training, we initialize the U-Net and Appearance\nControl Model branch with the previous pre-trained weights\nand initialize the Pose ControlNet branch with the weight\nfrom (Zhang et al., 2023), for joint fine-tuning. After these\nsteps, an optional motion module can be further fine-tuned.\n5.3. Qualitative and Quantitative Comparison\nWe conduct a comprehensive evaluation of TikTok (Jafarian\n& Park, 2021) in comparison to established motion transfer\nmethodologies, including FOMM (Siarohin et al., 2019a),\nMRAA (Siarohin et al., 2021), and TPS (Zhao & Zhang,\n2022), as well as recent advancements in the field such as\nDisco (Wang et al., 2023). Disco (Wang et al., 2023) lever-\nages a CLIP encoder to integrate appearance information\nfrom the reference image into the Transformer Blocks of\nthe Stable-Diffusion UNet and Pose ControlNet while re-\ntaining OpenPose (Cao et al., 2019; Simon et al., 2017; Cao\net al., 2017; Wei et al., 2016) as the pose condition. Though\nOpenPose has the limitation of incomplete detection of the\nhuman skeleton (More details in supplementary), we follow\nprevious work and adopt OpenPose as the pose detector.\nFor image quality evaluation, we adhere to the methodology\noutlined in Disco (Wang et al., 2023) and report metrics\nsuch as frame-wise FID (Heusel et al., 2017), SSIM (Wang\net al., 2004), LPIPS (Zhang et al., 2018), PSNR (Hore &\nZiou, 2010), and L1. In addition to these established metrics,\nwe introduce a novel image-wise metric called Face-Cos,\nwhich stands for Face Cosine Similarity. This metric is\ndesigned to gauge the model\u2019s capability to preserve the\nidentity information of the reference image input. To com-\npute this metric, we first align and crop the facial region\nin both the generated image and the ground truth. Sub-\nsequently, we calculate the cosine similarity between the\nextracted feature by AdaFace (Kim et al., 2022), frame by\nframe of the same subject in the test set, and report the\naverage value as the Face-Cos score. The experimental\n5\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nMagicPose\nDisco\nTPS\nMRAA\nMagicPose\nDisco\nTPS\nMRAA\nPose 1\nPose 2\nPose 3\nReference\nFigure 4. Qualitative comparison of human poses and facial expressions retargeting between TPS (Zhao & Zhang, 2022), MRAA (Siarohin\net al., 2019a), Disco (Wang et al., 2023) and MagicPose. Previous methods suffer from inconsistent facial expressions and human pose\nidentity.\nTable 1. Quantitative comparisons of MagicPose with the recent SOTA methods DreamPose (Karras et al., 2023) and Disco (Wang\net al., 2023). \u2193 indicates that the lower the better, and vice versa. Methods with \u2217 directly use the target image as the input, including\nmore information compared to the OpenPose (Cao et al., 2019; Simon et al., 2017; Cao et al., 2017; Wei et al., 2016). \u2020 represents that\nDisco (Wang et al., 2023) is pre-trained on other datasets (Fu et al., 2022; Ge et al., 2019; Schuhmann et al., 2021; Lin et al., 2014)\nmore than our proposed MagicPose, which uses only 335 video sequences in the TikTok (Jafarian & Park, 2021) dataset for pretraning\nand fine-tuning. Face-Cos represents the cosine similarity of the extracted feature by AdaFace (Kim et al., 2022) of face area between\ngeneration and ground truth image.\nMethod\nImage\nVideo\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nL1 \u2193\nFace-Cos \u2191\nFID-VID \u2193\nFOMM\u2217 (Siarohin et al., 2019a)\n85.03\n0.648\n29.01\n0.335\n3.61E-04\n0.190\n90.09\nMRAA\u2217 (Siarohin et al., 2021)\n54.47\n0.672\n29.39\n0.296\n3.21E-04\n0.337\n66.36\nTPS\u2217 (Zhao & Zhang, 2022)\n53.78\n0.673\n29.18\n0.299\n3.23E-04\n0.280\n72.55\nDisCo (Wang et al., 2023)\n50.68\n0.648\n28.81\n0.309\n4.27E-04\n-\n69.68\nDisCo\u2020 (Wang et al., 2023)\n30.75\n0.668\n29.03\n0.292\n3.78E-04\n0.166\n59.90\nMagicPose\n25.50\n0.752\n29.53\n0.292\n0.81E-04\n0.426\n46.30\nresults, in Table 1, unequivocally establish MagicPose as a\nfront-runner, showcasing substantial improvements across\nall metrics compared to alternative approaches. Notably,\nMagicPose attains a Face-Cos score of \u223c 0.426, represent-\ning a substantial +0.260 enhancement over Disco. This\nperformance shows MagicPose\u2019s robust capacity to preserve\nidentity information. The substantial performance improve-\nment over the state-of-the-art methods demonstrates the\nsuperiority of the MagicPose framework. We qualitatively\ncompare MagicPose to previous methods (Zhao & Zhang,\n2022; Siarohin et al., 2019a; Wang et al., 2023) in Figure 4.\nTPS (Zhao & Zhang, 2022), MRAA (Siarohin et al., 2019a),\nand Disco (Wang et al., 2023) suffer from inconsistent facial\nexpressions and human appearances. Please check the sup-\nplementary materials to see more examples of real-human\nposes and facial expressions re-targeting.\nUser Study We provide a user study for comparison be-\nTable 2. User study of MagicPose. We collect the number of votes\nfor eight subjects in the test set. The participants found that Mag-\nicPose preserves the best identity and appearance information in\npose and facial expression retargeting.\nMethod\nAverage\nMRAA (Siarohin et al., 2019a)\n3%\nFOMO (Siarohin et al., 2021)\n4%\nTPS (Zhao & Zhang, 2022)\n3%\nDisco (Wang et al., 2023)\n19%\nMagicPose\n71%\ntween MagicPose and previous works (Siarohin et al.,\n2019a; 2021; Zhao & Zhang, 2022; Wang et al., 2023).\nWe collect reference images, openpose conditions, and pose\nretargeting results from prior works and MagicPose of 8\nsubjects in the test set. For each subject, we visualize differ-\nent human poses and facial expressions and ask 40 users to\nchoose only one method, which preserves the best identity\n6\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference \nPose \nMagicPose \nwo/ Image-CFG \nwo/ Disentangle\nwo/ App-Pretraing\nFigure 5. Ablation Analysis of MagicPose. The proposed Appearance Control Pretraining and Appearance-disentangled Pose Control\nprovide better identity control and generation quality effectively.\nTable 3. Ablation Analysis of MagicPose with different training and inference settings. App-Pretrain stands for Appearance Control\nPretraining through Multi-Source Attention Module and Disentangle denotes Appearance-disentangled Pose Control. Image-CFG\ndenotes classifier free guidance. Data Aug indicates the model is trained with data augmentation of random masking of facial landmarks\nand hand poses.\nApp-Pretrain\nDisentangle\nImage CFG.\nData Aug.\nImage\nVideo\nFID \u2193\nSSIM \u2191\nPSNR \u2191\nLPIPS \u2193\nL1 \u2193\nFace-Cos \u2191\nFID-VID \u2193\n\u2717\n\u2717\n\u2713\n\u2713\n288.64\n0.291\n27.85\n0.695\n2.69E-04\n0.038\n382.10\n\u2713\n\u2717\n\u2713\n\u2713\n35.76\n0.727\n28.79\n0.321\n0.97E-04\n0.397\n65.72\n\u2713\n\u2713\n\u2717\n\u2713\n61.33\n0.659\n28.31\n0.357\n1.29E-04\n0.272\n98.96\n\u2713\n\u2713\n\u2713\n\u2717\n28.71\n0.751\n29.14\n0.296\n0.86E-04\n0.417\n47.26\n\u2713\n\u2713\n\u2713\n\u2713\n25.50\n0.752\n29.53\n0.292\n0.81E-04\n0.426\n46.30\nand appearance information for each subject. We present\nthe averaged vote result in Table. 2. Visualization examples\nand detailed user studies can be found in supplementary\nmaterial.\n5.4. Ablation Analysis\nIn this section, a comprehensive ablation analysis of Mag-\nicPose on the TikTok (Jafarian & Park, 2021) dataset is\npresented. The impact of various training and inference\nconfigurations within MagicPose is systematically analyzed\nin Table 3. We examine the proposed Appearance Con-\ntrol Model and its Multi-Source Self-Attention Module,\nspecifically assessing their contributions when omitted. The\nabsence of Appearance Control Pretraining and Appearance-\ndisentangled Pose Control reveals the significance of these\ncomponents, which can be observed in Figure. 5 as well.\nNotably, the introduction of Appearance Control Pretraining\nmarkedly enhances generation quality, evidenced by a sub-\nstantial increase of +944.73% in Face-Cos and +149.82%\nin SSIM. Additionally, the implementation of Appearance-\ndisentangled Pose Control demonstrates its efficacy, yield-\ning improvements of +7.30% in Face-Cos and +3.43% in\nSSIM. Furthermore, we highlight the necessity of incorpo-\nrating the data augmentation technique of randomly mask-\ning facial landmarks and hand poses during training. This\nis particularly crucial due to the occasional limitations of\nOpenPose (Cao et al., 2019; Simon et al., 2017; Cao et al.,\n2017; Wei et al., 2016) in providing complete and accu-\nrate detection of hand pose skeletons and facial landmarks,\nwhich can result in artifacts in generated images. Therefore,\nto enhance the robustness of MagicPose against incomplete\nhuman pose estimations by OpenPose (Cao et al., 2019;\nSimon et al., 2017; Cao et al., 2017; Wei et al., 2016), this\ndata augmentation strategy is proposed and leads to incre-\nmental improvements in Face-Cos and SSIM by +2.20%\nand +0.13%, respectively. Moreover, the application of\nclassifier-free guidance (Image-CFG) in the training process,\nas discussed in prior work (Wang et al., 2023; Ho, 2022;\nLin et al., 2023a; Balaji et al., 2022; Dao et al., 2022) on\ndiffusion models, further augments the quality of generation.\nThe implementation of Image-CFG enhances Face-Cos by\n+56.62% and SSIM by +14.11%, underscoring its value in\nthe image generation context.\n5.5. Generalization Ability\nIt is also worth highlighting that MagicPose can gener-\nalize to out-of-domain reference images of unseen styles\nand poses with surprisingly good appearance controllabil-\n7\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nTable 4. Quantitative evaluation of generalization ability of MagicPose. MagicPose\u2020 denotes the pipeline is directly evaluated on test set\nof Everybody Dance Now (Chan et al., 2019b) after being trained on TikTok (Jafarian & Park, 2021), and MagicPose\u2021 represents the\npipeline is further fine-tuned on Everybody Dance Now (Chan et al., 2019b) train set and evaluated on test set.\nMethod\nSubject1\nSubject2\nSubject3\nSubject4\nSubject5\nAverage\nFID \u2193\nPSNR \u2191\nFID \u2193\nPSNR \u2191\nFID \u2193\nPSNR \u2191\nFID \u2193\nPSNR \u2191\nFID \u2193\nPSNR \u2191\nFID \u2193\nPSNR \u2191\nMagicPose\u2020\n22.59\n30.67\n22.21\n30.13\n35.43\n29.35\n31.72\n29.53\n31.24\n28.48\n28.64\n29.63\nMagicPose\u2021\n22.50\n30.67\n22.61\n28.40\n27.38\n29.10\n36.73\n33.95\n21.99\n30.94\n26.24\n30.61\nMagicPose\nDisco\nTPS\nMRAA\nPose 1\nPose 2\nPose 3\nReference\nFigure 6. Comparison of zero-shot pose and facial expression\nretargeting on out-of-domain image.\nity, even without any further fine-tuning on the target do-\nmain. Figure. 6 compares the zero-shot results of applying\nTPS (Zhao & Zhang, 2022), MRAA (Siarohin et al., 2019a),\nDisco (Wang et al., 2023) and MagicPose to out-of-domain\nimages, whose visual style is distinct from corresponding\ntraining data of the real-human upper-body images. For real-\nhuman reference images, we observe that most of the human\nsubjects from TikTok (Jafarian & Park, 2021) dataset and\nthe self-collected test set of Disco (Wang et al., 2023) are\nyoung women. So we test our method on more in-the-wild\nreal-human examples, e.g. elder people, in Figure 7. We\nalso evaluate the in-the-wild motions generalization abil-\nity of MagicPose on Everybody Dance Now (Chan et al.,\n2019b), which is a full-body dataset, in contrast to the upper-\nbody images used in the TikTok dataset. We directly apply\nMagicPose to such full-body reference images and visualize\nthe qualitative results in Figure. 8 and provide a quantita-\ntive evaluation in Table. 4. Experiments show that Magic-\nPose generalizes surprisingly well to full body images even\nthough it has never been trained on such data. Furthermore,\nbetter quality of generation can be achieved after fine-tuning\non specific datasets as well. More visualizations of zero-\nshot Animation and results on Everybody Dance Now (Chan\net al., 2019b) can be found in the supplementary materials.\nReference\nPose 1\nPose 2\nPose 3\nFigure 7. Visualization of zero-shot pose and facial expression\nretargeting on in-the-wild real-human with different ethnicity and\nage from training data (Tiktok).\nReference\nMagicPose\nZero Shot\nGT\nFigure 8. Visualization of zero-shot Human Motion and Facial\nExpression Transfer on EverybodyDancNow Dataset (Chan et al.,\n2019b). MagicPose can generalize perfectly to in-the-wild human\nmotions.\n6. Conclusion\nIn this work, we propose MagicPose, a novel approach in\nthe realm of realistic human poses and facial expressions\nretargeting. By seamlessly incorporating motion and facial\nexpression transfer and enabling the generation of consistent\nin-the-wild animations without any further fine-tuning, Mag-\nicPose shows a significant advancement over prior methods.\nNotably, our approach demonstrates a superior capacity to\ngeneralize over diverse human identities and complex mo-\n8\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\ntion sequences. Moreover, MagicPose boasts a practical\nimplementation as a plug-in module or extension compati-\nble with existing models such as Stable Diffusion (Rombach\net al., 2022). This combination of innovation, efficiency,\nand adaptability establishes MagicPose as a promising tool\nin the field of poses and facial expressions retargeting.\nReferences\nAlBahar, B., Lu, J., Yang, J., Shu, Z., Shechtman, E., and Huang,\nJ.-B. Pose with Style: Detail-preserving pose-guided image\nsynthesis with conditional stylegan.\nACM Transactions on\nGraphics, 2021.\nAmit, T., Shaharbany, T., Nachmani, E., and Wolf, L. Segdiff:\nImage segmentation with diffusion probabilistic models. arXiv\npreprint arXiv:2112.00390, 2021.\nBalaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K.,\nAittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and\nLiu, M.-Y. ediff-i: Text-to-image diffusion models with an\nensemble of expert denoisers. ArXiv, abs/2211.01324, 2022.\nBaranchuk, D., Rubachev, I., Voynov, A., Khrulkov, V., and\nBabenko, A. Label-efficient semantic segmentation with diffu-\nsion models. arXiv preprint arXiv:2112.03126, 2021.\nBeier, T. and Neely, S. Feature-based image metamorphosis. In\nProceedings of the 19th Annual Conference on Computer Graph-\nics and Interactive Techniques, SIGGRAPH \u201992, pp. 35\u201342,\nNew York, NY, USA, 1992. Association for Computing Ma-\nchinery. ISBN 0897914791. doi: 10.1145/133994.134003.\nURL https://doi.org/10.1145/133994.134003.\nBertalmio, M., Sapiro, G., Caselles, V., and Ballester, C. Image\ninpainting. In Proceedings of the 27th annual conference on\nComputer graphics and interactive techniques, pp. 417\u2013424,\n2000.\nBlattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S. W.,\nFidler, S., and Kreis, K. Align your latents: High-resolution\nvideo synthesis with latent diffusion models. In IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR),\n2023.\nBregler, C., Covell, M., and Slaney, M. Video rewrite: Driv-\ning visual speech with audio.\nIn Proceedings of the 24th\nAnnual Conference on Computer Graphics and Interactive\nTechniques, SIGGRAPH \u201997, pp. 353\u2013360, USA, 1997. ACM\nPress/Addison-Wesley Publishing Co.\nISBN 0897918967.\ndoi: 10.1145/258734.258880. URL https://doi.org/\n10.1145/258734.258880.\nCao, M., Wang, X., Qi, Z., Shan, Y., Qie, X., and Zheng, Y. Mas-\nactrl: Tuning-free mutual self-attention control for consistent\nimage synthesis and editing. arXiv preprint arXiv:2304.08465,\n2023.\nCao, Z., Simon, T., Wei, S.-E., and Sheikh, Y. Realtime multi-\nperson 2d pose estimation using part affinity fields. In CVPR,\n2017.\nCao, Z., Hidalgo Martinez, G., Simon, T., Wei, S., and Sheikh,\nY. A. Openpose: Realtime multi-person 2d pose estimation\nusing part affinity fields. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2019.\nChan, C., Ginosar, S., Zhou, T., and Efros, A. A. Everybody dance\nnow. In IEEE International Conference on Computer Vision\n(ICCV), 2019a.\nChan, C., Ginosar, S., Zhou, T., and Efros, A. A. Everybody dance\nnow. In Proceedings of the IEEE/CVF international conference\non computer vision, pp. 5933\u20135942, 2019b.\nCheung, G., Baker, S., Hodgins, J., and Kanade, T.\nMarker-\nless human motion transfer.\nIn Proceedings. 2nd Interna-\ntional Symposium on 3D Data Processing, Visualization and\nTransmission, 2004. 3DPVT 2004., pp. 373\u2013378, 2004. doi:\n10.1109/TDPVT.2004.1335262.\nDao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00b4e, C. FlashAttention:\nFast and memory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Systems, 2022.\nEfros, Berg, Mori, and Malik. Recognizing action at a distance. In\nProceedings Ninth IEEE International Conference on Computer\nVision, pp. 726\u2013733 vol.2, 2003. doi: 10.1109/ICCV.2003.\n1238420.\nFu, J., Li, S., Jiang, Y., Lin, K.-Y., Qian, C., Loy, C. C., Wu, W.,\nand Liu, Z. Stylegan-human: A data-centric odyssey of human\ngeneration. In ECCV, 2022.\nGe, Y., Zhang, R., Wang, X., Tang, X., and Luo, P. Deepfash-\nion2: A versatile benchmark for detection, pose estimation,\nsegmentation and re-identification of clothing images. In CVPR,\n2019.\nGuo, Y., Yang, C., Rao, A., Wang, Y., Qiao, Y., Lin, D., and\nDai, B.\nAnimatediff: Animate your personalized text-to-\nimage diffusion models without specific tuning. arXiv preprint\narXiv:2307.04725, 2023.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochre-\niter, S. Gans trained by a two time-scale update rule converge\nto a local nash equilibrium. NeurIPS, 2017.\nHo, J. Classifier-free diffusion guidance. ArXiv, abs/2207.12598,\n2022.\nHo, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic\nmodels. arXiv preprint arxiv:2006.11239, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A.,\nKingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen\nvideo: High definition video generation with diffusion models.\narXiv preprint arXiv:2210.02303, 2022.\nHong, F.-T., Zhang, L., Shen, L., and Xu, D. Depth-aware genera-\ntive adversarial network for talking head video generation. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 3397\u20133406, 2022.\nHore, A. and Ziou, D. Image quality metrics: Psnr vs. ssim. In\nICPR, 2010.\nJafarian, Y. and Park, H. S. Learning high fidelity depths of dressed\nhumans by watching social media dance videos. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 12753\u201312762, June 2021.\nJam, J., Kendrick, C., Walker, K., Drouard, V., Hsu, J. G.-S., and\nYap, M. H. A comprehensive review of past and present image\ninpainting methods. Computer vision and image understanding,\n203:103147, 2021.\n9\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nJustin,\nP. and Lambda.\nStable Diffusion Image Vari-\nations.\nhttps://huggingface.co/lambdalabs/\nsd-image-variations-diffusers, 2022.\nKarras, J., Holynski, A., Wang, T.-C., and Kemelmacher-\nShlizerman, I. Dreampose: Fashion image-to-video synthesis\nvia stable diffusion. arXiv preprint arXiv:2304.06025, 2023.\nKim, H., Garrido, P., Tewari, A., Xu, W., Thies, J., Nie\u00dfner, M.,\nP\u00b4erez, P., Richardt, C., Zoll\u00a8ofer, M., and Theobalt, C. Deep\nvideo portraits. ACM Transactions on Graphics (TOG), 37(4):\n163, 2018.\nKim, M., Jain, A. K., and Liu, X. Adaface: Quality adaptive\nmargin for face recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022.\nLin, S., Liu, B., Li, J., and Yang, X. Common diffusion noise\nschedules and sample steps are flawed. 2023a.\nLin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan,\nD., Doll\u00b4ar, P., and Zitnick, C. L. Microsoft coco: Common\nobjects in context. In ECCV, 2014.\nLin, Y., Han, H., Gong, C., Xu, Z., Zhang, Y., and Li, X. Con-\nsistent123: One image to highly consistent 3d asset using\ncase-aware diffusion priors. arXiv preprint arXiv:2309.17261,\n2023b.\nLiu, W., Piao, Z., Min, J., Luo, W., Ma, L., and Gao, S. Liquid\nwarping gan: A unified framework for human motion imitation,\nappearance transfer and novel view synthesis. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision,\npp. 5904\u20135913, 2019.\nLugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R.,\nand Van Gool, L. Repaint: Inpainting using denoising diffu-\nsion probabilistic models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp.\n11461\u201311471, 2022.\nMirza, M. and Osindero, S. Conditional generative adversarial\nnets, 2014.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P.,\nMcGrew, B., Sutskever, I., and Chen, M. GLIDE: towards\nphotorealistic image generation and editing with text-guided\ndiffusion models. CoRR, abs/2112.10741, 2021. URL https:\n//arxiv.org/abs/2112.10741.\nPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion:\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988,\n2022.\nQiao, F., Yao, N., Jiao, Z., Li, Z., Chen, H., and Wang, H.\nGeometry-contrastive gan for facial expression transfer. arXiv\npreprint arXiv:1802.01822, 2018.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agar-\nwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.\nLearning transferable visual models from natural language su-\npervision. In ICML, 2021.\nRaj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall,\nB., Zada, S., Aberman, K., Rubinstein, M., Barron, J., et al.\nDreambooth3d: Subject-driven text-to-3d generation. arXiv\npreprint arXiv:2303.13508, 2023.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.\nHierarchical text-conditional image generation with clip latents,\n2022.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.\nHigh-resolution image synthesis with latent diffusion models,\n2021.\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.\nHigh-resolution image synthesis with latent diffusion models.\nIn CVPR, 2022.\nRonneberger, O., Fischer, P., and Brox, T. U-net: Convolutional\nnetworks for biomedical image segmentation. In MICCAI, 2015.\nSaharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T.,\nFleet, D., and Norouzi, M. Palette: Image-to-image diffusion\nmodels. In ACM SIGGRAPH 2022 Conference Proceedings, pp.\n1\u201310, 2022a.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.,\nGhasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes,\nR. G., Salimans, T., Ho, J., Fleet, D. J., and Norouzi, M. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding, 2022b.\nSchuhmann, C., Vencu, R., Beaumont, R., Kaczmarczyk, R.,\nMullis, C., Katta, A., Coombes, T., Jitsev, J., and Komatsuzaki,\nA. Laion-400m: Open dataset of clip-filtered 400 million image-\ntext pairs. arXiv preprint arXiv:2111.02114, 2021.\nShi, Y., Wang, P., Ye, J., Long, M., Li, K., and Yang, X. Mv-\ndream: Multi-view diffusion for 3d generation. arXiv preprint\narXiv:2308.16512, 2023.\nSiarohin, A., Sangineto, E., Lathuili`ere, S., and Sebe, N. De-\nformable gans for pose-based human image generation. In The\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2018.\nSiarohin, A., Lathuili`ere, S., Tulyakov, S., Ricci, E., and Sebe, N.\nFirst order motion model for image animation. NeurIPS, 2019a.\nSiarohin, A., Lathuili`ere, S., Sangineto, E., and Sebe, N. Ap-\npearance and pose-conditioned human image generation using\ndeformable gans. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2019b.\nSiarohin, A., Woodford, O. J., Ren, J., Chai, M., and Tulyakov,\nS. Motion representations for articulated animation. In CVPR,\n2021.\nSimon, T., Joo, H., Matthews, I., and Sheikh, Y. Hand keypoint\ndetection in single images using multiview bootstrapping. In\nCVPR, 2017.\nSinger, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu,\nQ., Yang, H., Ashual, O., Gafni, O., et al. Make-a-video: Text-\nto-video generation without text-video data. arXiv preprint\narXiv:2209.14792, 2022.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion implicit\nmodels. In International Conference on Learning Representa-\ntions, 2021a. URL https://openreview.net/forum?\nid=St1giarCHLP.\n10\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S.,\nand Poole, B. Score-based generative modeling through stochas-\ntic differential equations. arXiv preprint arXiv:2011.13456,\n2020.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Er-\nmon, S., and Poole, B.\nScore-based generative modeling\nthrough stochastic differential equations. In International Con-\nference on Learning Representations, 2021b. URL https:\n//openreview.net/forum?id=PxTIG12RRHS.\nSun, Y.-T., Fu, Q.-C., Jiang, Y.-R., Liu, Z., Lai, Y.-K., Fu, H.,\nand Gao, L. Human motion transfer with 3d constraints and\ndetail enhancement. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 45(4):4682\u20134693, 2022.\nTulyakov, S., Liu, M.-Y., Yang, X., and Kautz, J. MoCoGAN:\nDecomposing motion and content for video generation. In\nIEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 1526\u20131535, 2018.\nVan Den Oord, A., Vinyals, O., et al. Neural discrete representation\nlearning. NeurIPS, 2017.\nWang, T., Li, L., Lin, K., Lin, C.-C., Yang, Z., Zhang, H.,\nLiu, Z., and Wang, L. Disco: Disentangled control for re-\nferring human dance generation in real world. arXiv preprint\narXiv:2307.00040, 2023.\nWang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image\nquality assessment: from error visibility to structural similarity.\nIEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\nWei, D., Xu, X., Shen, H., and Huang, K. Gac-gan: A general\nmethod for appearance-controllable human video motion trans-\nfer. IEEE Transactions on Multimedia, 23:2457\u20132470, 2020.\nWei, S.-E., Ramakrishna, V., Kanade, T., and Sheikh, Y. Convolu-\ntional pose machines. In CVPR, 2016.\nWolleb, J., Sandk\u00a8uhler, R., Bieder, F., Valmaggia, P., and Cattin,\nP. C. Diffusion models for implicit image segmentation ensem-\nbles. In International Conference on Medical Imaging with\nDeep Learning, pp. 1336\u20131348. PMLR, 2022.\nWu, J. Z., Ge, Y., Wang, X., Lei, S. W., Gu, Y., Shi, Y., Hsu, W.,\nShan, Y., Qie, X., and Shou, M. Z. Tune-a-video: One-shot\ntuning of image diffusion models for text-to-video generation.\nIn Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 7623\u20137633, 2023.\nWu, R., Zhang, G., Lu, S., and Chen, T. Cascade ef-gan: Progres-\nsive facial expression editing with local focuses. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), June 2020.\nXu, F., Liu, Y., Stoll, C., Tompkin, J., Bharaj, G., Dai, Q., Sei-\ndel, H.-P., Kautz, J., and Theobalt, C. Video-based charac-\nters: Creating new human performances from a multi-view\nvideo database. ACM Trans. Graph., 30(4), jul 2011. ISSN\n0730-0301. doi: 10.1145/2010324.1964927. URL https:\n//doi.org/10.1145/2010324.1964927.\nYeh, R. A., Chen, C., Yian Lim, T., Schwing, A. G., Hasegawa-\nJohnson, M., and Do, M. N. Semantic image inpainting with\ndeep generative models. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 5485\u20135493,\n2017.\nZhang, L. [major update] reference-only control \u00b7 mikubill/sd-\nwebui-controlnet \u00b7 discussion #1236.\nURL https://\ngithub.com/Mikubill/sd-webui-controlnet/\ndiscussions/1236.\nZhang, L., Rao, A., and Agrawala, M. Adding conditional control\nto text-to-image diffusion models, 2023.\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang, O.\nThe unreasonable effectiveness of deep features as a perceptual\nmetric. In CVPR, 2018.\nZhao, J. and Zhang, H. Thin-plate spline motion model for image\nanimation. In CVPR, 2022.\n11\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nA. Detailed User Study\nIn this section, we provide a comprehensive user study for\nqualitative comparison between MagicPose and previous\nworks (Siarohin et al., 2019a; 2021; Zhao & Zhang, 2022;\nWang et al., 2023). As we mentioned in the experiment,\nwe collect reference images, openpose conditions, and pose\nretargeting results from prior works and MagicPose of 8\nsubjects in the test set. For each subject, we visualize dif-\nferent human poses and facial expressions. Some examples\nare shown in Figure. 10 and Figure. 11. The methods are\nanonymized as A, B, C, D, E, and the order of the generated\nimage from the corresponding method is randomized in each\nsubject comparison. We ask 40 users to choose only one\nmethod which preserves the best identity and appearance\ninformation for each subject. We present the full result in\nTable. 5.\nB. Additional Visulizations\nB.1. TikTok\nWe provide more visualizations on the test set of the ex-\nperiments on TikTok (Jafarian & Park, 2021) in Figure. 12,\nFigure. 13, and Figure. 14.\nReference \nPose 1\nPose 2\nPose 3\nFigure 9. Visualization of generalization to unseen image styles\nthat are different from our training set (Tiktok).\nB.2. EverybodyDanceNow\nWe provide more visualizations of zero-shot generation on\nEverybody Dance Now dataset (Chan et al., 2019b) in Fig-\nure. 15 and Figure. 16.\nB.3. Zero-Shot Animation\nB.3.1. OUT-OF-DOMAIN IMAGES\nWe provide more visualizations of zero-shot generation of\nout-of-domain images in Figure. 9, Figure. 17, Figure. 18,\nand Figure. 19.\nB.3.2. COMBINE WITH T2I MODEL\nA potential application of our proposed model is that it\ncan be combined with the existing Text to Image (T2I)\ngeneration model (Zhang et al., 2023; Rombach et al., 2022)\nand used to edit the generation result. We visualized some\nsamples in Figure. 20.\nC. Sequence Generation with Motion Module\nAs mentioned in our main paper, the Appearance Control\nModel and Apperance-disentangled Pose ControlNet to-\ngether already achieve accurate image-to-image motion\ntransfer, but we can further integrate an optional motion\nmodule into the primary SD-UNet architecture to improve\nthe temporal consistency. We initially employed the widely-\nused AnimateDiff (Guo et al., 2023), which provides an\nassortment of motion modules tailored to the stable dif-\nfusion model v1.5., but we found that AnimateDiff faces\nlimitations in achieving seamless transition across frames,\nparticularly with more complex movement patterns present\nin human dance, as opposed to more subdued video content.\nTo solve this issue, we fine-tuned the AnimateDiff motion\nmodules until satisfactory temporal coherence was observed\nduring the evaluation. We freeze the weights of all parts in\nour Appearance Control Model and Apperance-disentangled\nPose ControlNet, and fine-tune the motion module with pre-\ntrained weights from AnimateDiff (Guo et al., 2023) for\n30k steps with a batch size of 8. Each batch contains 16\nframes of a video sequence as the target output. For more\nsmooth and consistent video generation quality, we also\npropose a special sampling strategy for DDIM (Song et al.,\n2021a) during inference. Figure. 12, Figure. 17, Figure. 18,\nand Figure. 19 are examples of sequential output from our\nmodel.\nD. Limitations\nIn MagicPose, We follow previous work (Wang et al., 2023)\nand adopt OpenPose (Cao et al., 2019; Simon et al., 2017;\nCao et al., 2017; Wei et al., 2016) as the human pose detector,\nwhich is crucial for pose control, significantly affecting\nthe generated images\u2019 quality and temporal consistency.\nHowever, challenges arise in accurately detecting complete\npose skeletons and facial landmarks, especially under rapid\nmovement, occlusions, or partial visibility of subjects. As\nillustrated in the second row of Figure 4, we can observe\n1\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nCondition\nTPS\nFOMM\nMRAA\nReference\nDisco\nMagicPose\nGT\nFigure 10. Visualization of human pose and facial expression retargeting (Subjects 1-4 in the user study): MagicPose preserves identity\nand appearance information from the reference image the best.\nTable 5. User study of MagicPose. We collect the number of votes for eight subjects in the test set and report the percentage. The\nparticipants found that MagicPose preserves the best identity and appearance information in pose and facial expression retargeting.\nMethod\nSubject1\nSubject2\nSubject3\nSubject4\nSubject5\nSubject6\nSubject7\nSubject8\nAverage\nMRAA (Siarohin et al., 2019a)\n10%\n10%\n0%\n3%\n0%\n0%\n3%\n3%\n3%\nFOMO (Siarohin et al., 2021)\n5%\n0%\n8%\n0%\n3%\n0%\n5%\n10%\n4%\nTPS (Zhao & Zhang, 2022)\n3%\n18%\n0%\n3%\n0%\n3%\n3%\n0%\n3%\nDisco (Wang et al., 2023)\n13%\n3%\n10%\n18%\n10%\n28%\n43%\n28%\n19%\nMagicPose\n70%\n70%\n83%\n78%\n88%\n70%\n48%\n60%\n71%\nthat the skeleton and hand pose are partially missing in the\ndetection result, especially in the right half of the row. In\nfuture works, a more advanced pose detector can be adopted\nfor better image editing quality.\nE. Discussion on motivation and future works\nIn addition to the suggestion of replacing openpose with\na more advanced pose detector, we also would like to dis-\ncuss future works from our motivation. Our understanding\nof image generation is that it can be decomposed into two\naspects: (1) identity control (appearance of human) and\n(2) shape/geometry control (pose and motion of human).\nMagicPose was introduced to maintain the appearance and\nidentity information in generation from reference image in-\nput strictly while editing the geometry shape and structural\ninformation under the guidance of human pose skeleton. In\nthis paper, we demonstrate the identity-preserving abil-\nity of the Appearance Control Model and its Multi-Source\nAttention Module by human pose and facial expression re-\ntargeting task. The design of this Multi-Source Attention\nModule can be further extended to other tasks as well, e.g.\nnovel view synthesis of general objects under the shape\ncondition of the camera, shape manipulation of the natural\nscenes under the geometry condition of depth/segmentation\nmap, and motion transfer of animals under the animal pose\ncondition of skeletons, etc.\n2\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nCondition\nTPS\nFOMM\nMRAA\nReference\nDisco\nMagicPose\nGT\nFigure 11. Visualization of human pose and facial expression retargeting (Subjects 5-8 in the user study): MagicPose preserves identity\nand appearance information from the reference image the best.\n3\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference \nFigure 12. Visualization of Human Motion and Facial Expression Transfer on TikTok (Jafarian & Park, 2021).\n4\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nFigure 13. Visualization of Human Motion and Facial Expression Transfer on TikTok (Jafarian & Park, 2021). MagicPose is able to\ngenerate vivid and realistic motion and expressions under the condition of diverse pose skeleton and face landmark input, while accurately\nmaintaining identity information from the reference image input.\n5\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nFigure 14. Visualization of Human Motion and Facial Expression Transfer on TikTok (Jafarian & Park, 2021).\n6\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference\nCondition\nGT\nMagicPose\nZero Shot\nReference\nCondition\nGT\nMagicPose\nZero Shot\nReference\nCondition\nMagicPose\nZero Shot\nFigure 15. Visualization of Zero-Shot Human Motion and Facial Expression Transfer on Everybody Dance Now Dataset (Chan et al.,\n2019b).\n7\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nGT\nMagicPose\nZero Shot\nReference\nCondition\nGT\nMagicPose\nZero Shot\nReference\nCondition\nGT\nMagicPose\nZero Shot\nFigure 16. Visualization of Zero-Shot Human Motion and Facial Expression Transfer on Everybody Dance Now Dataset (Chan et al.,\n2019b).\n8\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference \nFigure 17. Visualization of Zero-Shot Animation. MagicPose can provide a precise generation with identity information from out-of-\ndomain images even without any further fine-tuning after being trained on real-human dance videos.\n9\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference \nFigure 18. Visualization of Zero-Shot 2D Cartoon Generation.\n10\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nReference \nFigure 19. Visualization of Zero-Shot 2D Cartoon Generation.\n11\nMagicPose: Realistic Human Pose and Facial Expression Retargeting with Identity-aware Diffusion\nT2I\nModel\nT2I\nModel\nT2I\nModel\n\u201cA blonde woman \nin the gym.\u201d\n\u201cAnalog film photo, \nbeautiful woman,\ndark,black hair\u201d\n\u201cA red-haired woman\n in the woods on a snowy \nday\u201d\n\u201cA Blonde woman \nstanding in cave\n with mountains \nin background\u201d\nT2I\nModel\nFigure 20. Usage of combining with T2I model. MagicPose can provide a precise generation with identity information from T2I-generated\nimages even without further fine-tuning after training on real-human dance videos.\n12\n"
  },
  {
    "title": "HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis",
    "link": "https://arxiv.org/pdf/2311.12454.pdf",
    "upvote": "26",
    "text": "JOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n1\nHierSpeech++: Bridging the Gap between Semantic\nand Acoustic Representation of Speech by Hierarchical\nVariational Inference for Zero-shot Speech Synthesis\nSang-Hoon Lee\n, Ha-Yeong Choi\n, Seung-Bin Kim\n, and Seong-Whan Lee\n, Fellow, IEEE\nAbstract\u2014Large language models (LLM)-based speech synthesis has been widely adopted in zero-shot speech synthesis. However, they\nrequire a large-scale data and possess the same limitations as previous autoregressive speech models, including slow inference speed\nand lack of robustness. This paper proposes HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech (TTS) and\nvoice conversion (VC). We verified that hierarchical speech synthesis frameworks could significantly improve the robustness and\nexpressiveness of the synthetic speech. Furthermore, we significantly improve the naturalness and speaker similarity of synthetic speech\neven in zero-shot speech synthesis scenarios. For text-to-speech, we adopt the text-to-vec framework, which generates a self-supervised\nspeech representation and an F0 representation based on text representations and prosody prompts. Then, HierSpeech++ generates\nspeech from the generated vector, F0, and voice prompt. We further introduce a high-efficient speech super-resolution framework from 16\nkHz to 48 kHz. The experimental results demonstrated that the hierarchical variational autoencoder could be a strong zero-shot speech\nsynthesizer given that it outperforms LLM-based and diffusion-based models. Moreover, we achieved the first human-level quality\nzero-shot speech synthesis. Audio samples and source code are available at https://github.com/sh-lee-prml/HierSpeechpp.\nIndex Terms\u2014Text-to-Speech, Voice Conversion, Speech Super-resolution, Zero-shot Speech Synthesis, Zero-shot Voice Cloning\n\u2726\n1\nINTRODUCTION\nThe advent of large language models (LLM) have facil-\nitated the widespread adoption of LLM-based models in\nspeech synthesis and audio generation tasks. Conventional\nspeech synthesis frameworks have advanced significantly\ndriven by the integration of new features such as neural\naudio codecs for discrete speech or audio units. Although\nthere is still a room for improvement in LLM-based speech\nmodels, these models possess four major limitations: 1) the\nfirst drawback is their auto-regressive generative manner,\nwhich has a slow inference speed and lack of robustness,\nresulting in repeating, skipping, and mispronunciation; 2)\nthey are highly dependent on the pre-trained neural audio\ncodec or discrete speech unit; 3) the audio quality of these\nmodels puts the clock back before the advent of the strong\nend-to-end speech synthesis framework proposed in [35]; 4)\nthey require a large-scale dataset to train the model.\nVITS [35] successfully introduced an end-to-end (E2E)\nspeech synthesis framework by adopting a variational au-\ntoencoder (VAE) augmented with normalizing flow and\nadversarial training. Driven by the ability to generate high-\nquality waveform audio within a fully end-to-end train-\ning pipeline, the perceptual quality of synthetic speech is\n\u2022\nS.-H. Lee, H.-Y. Choi, S.-B. Kim, and S.-W. Lee are with the Depart-\nment of Artificial Intelligence, Korea University, Seoul 02841, South\nKorea (e-mail: sh_lee@korea.ac.kr; hayeong@korea.ac.kr; sb-kim@korea.ac.kr;\nsw.lee@korea.ac.kr)\nCorresponding author: Seong-Whan Lee\n\u2022\nThis work was supported by Institute of Information & communications\nTechnology Planning & Evaluation (IITP) grant funded by the Korea\ngovernment (MSIT) (No. 2019-0-00079, Artificial Intelligence Graduate\nSchool Program (Korea University) and No. 2021-0-02068, Artificial\nIntelligence Innovation Hub).\nManuscript received November xx, 20xx; revised November xx, 20xx.\nsignificantly better than that of two-stage speech synthesis\nmodels such as conventional text-to-speech (TTS) models\nand recent codec-based speech synthesis models. HierSpeech\n[48] further improved the reconstruction quality by adopting\na hierarchical conditional VAE using self-supervised speech\nrepresentation. They proposed a novel TTS frameworks,\nwhich can train the model without any text transcripts\nby leveraging self-supervised speech representation and\nhierarchical VAE. However, the E2E models have limitations\nin terms of zero-shot voice cloning. Although E2E models can\nsynthesize speech with high-quality audio, their synthetic\nspeech still has a little speaker similarity in zero-shot voice\ncloning scenarios, and their training processes require high\ncomputational complexity.\nMeanwhile, diffusion-based speech synthesis models\nhave also shown their strengths in terms of speaker adapta-\ntion. Diff-VC [64] introduced a conditional diffusion proba-\nbilistic model with a data-dependent prior for zero-shot voice\nconversion. Additionally, the effectiveness of diffusion mod-\nels for speaker adaptation, including DDDM-VC [10], Diff-\nhierVC [11], and UnitSpeech [32], has been proven. Although\ndiffusion models exhibit a good adaptation performance,\nthey have several limitations: 1) they have a slow inference\nspeed with their iterative generation processes, and 2) they\nare vulnerable to noisy data for speaker adaptation. With\nnoisy speech prompts, they may generate much more noisy\nspeech mainly due to the powerful adaptation performance,\nand this may further results in the degradation of the\nperceptual audio quality. 3) Although diffusion models have\nshown strong generative performance, they still possess\nlower audio quality owing the train-inference mismatch\nof the two-stage generation between the ground-truth and\ngenerated Mel-spectrogram.\narXiv:2311.12454v2  [cs.SD]  27 Nov 2023\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n2\nIn this paper, we present HierSpeech++, a fast and strong\nzero-shot speech synthesis model that uses a hierarchical\nspeech synthesis framework. By adopting the E2E speech\nsynthesis frameworks to take the advantage of high-quality\nwaveform generation, we solved the limitation of style adap-\ntation by adopting a self-supervised speech representation\nas a semantic speech representation and bridging the gap\nbetween semantic and acoustic representation hierarchically.\nWe propose a novel speech synthesis framework consisting\nof a hierarchical speech synthesizer, text-to-vec (TTV), and\nspeech super-resolution (SpeechSR).\nBased on HierVST [45], we introduce an improved hier-\narchical speech synthesizer using a hierarchical conditional\nVAE. To improve audio quality beyond perceptual quality,\nwe adopt a dual-audio acoustic encoder in order to enhance\nthe acoustic posterior and utilize a BigVGAN-based hierar-\nchical adaptive generator with conditional and unconditional\ngeneration for better out-of-distribution generalization (zero-\nshot voice cloning). In addition, we adopt a source-filter\ntheory-based multi-path semantic encoder to disentangle\nspeech components and enhance the semantic prior for\nspeaker-agnostic and speaker-related semantic information.\nBy using a hierarchical VAE, we can connect and learn\nthese representations hierarchically and infer the waveform\naudio by progressively adapting to the target voice style. For\nbetter adaptation and train-inference mismatch reduction,\nwe introduce bidirectional normalizing flow Transformer\nnetworks using AdaLN-Zero. Without a text-speech paired\ndataset, we can simply scale-up the dataset to train a\nhierarchical speech synthesizer for zero-shot voice cloning.\nFor text-to-speech, we introduce a TTV that can generate\na semantic representation and an F0 from text sequences.\nOwing to the semantic information that is extracted from self-\nsupervised learning, we can transfer prosody information\nthat is irrelevant to voice style. By connecting the TTV\nand a hierarchical speech synthesizer, we can synthesize\nhigh-quality speech from text by hierarchically adapting the\nprosody and voice style even in zero-shot scenarios. We\nalso propose a simple speech super-resolution framework to\nupsample high-resolution waveform audio from 16 kHz to\n48 kHz. This can facilitate data accessibility for scaling up\ndatasets in that we can utilize low-resolution speech data\nsuch as the automatic speech recognition (ASR) dataset to\ntrain the speech synthesizer and TTV models.\nThe main contributions of this study are as follows:\n\u2022\nFor fast and strong zero-shot speech synthesis, we\npresented HierSpeech++, a novel fully-parallel hierar-\nchical speech synthesis framework.\n\u2022\nProsody and voice style can be transferred and\ncontrolled using a hierarchical speech synthesis frame-\nwork.\n\u2022\nWe also present SpeechSR which can upsample wave-\nform audio from 16 kHz to 48 kHz for high-resolution\nspeech synthesis and data scalability.\n\u2022\nHierSpeech++ achieved the first human-level quality\nfor zero-shot text-to-speech and voice conversion\ntasks.\n\u2022\nAudio samples and source code are available at https:\n//sh-lee-prml.github.io/HierSpeechpp-demo/\n2\nRELATED WORK\n2.1\nNeural Codec Language Models\nConventional sequence-to-sequence auto-regressive TTS\nmodels, such as Tacotron [79], have successfully paved the\nway for speech synthesis technologies. TransformerTTS [53]\nfirst adopted a Transformer network for TTS, and VTN [28]\nalso utilizes a Transformer network for VC. However, these\nauto-regressive models suffer from a slow inference speed,\nin addition to a lack of robustness owing to challenges in\naligning text and acoustic representations and the difficulty\nin predicting a continuous acoustic representation. Recently,\nneural audio codec model [16], [89] have replaced conven-\ntional acoustic representations with a high-compressed audio\ncodec, which can reproduce the original waveform audio.\nVall-E [78] was the first neural codec language model for\nspeech synthesis utilizing a discrete audio unit and language\nmodels. By scaling up the dataset to 60,000 h, Vall-E could\nperform in-context learning using a neural audio codec.\nHowever, it possessed the same limitations as auto-regressive\nTTS models, such as a slow inference speed and a lack\nof robustness. Furthermore they have a high-dependency\nof their pre-trained neural audio codec, resulting in low-\nquality audio. To overcome this limitation, high-quality\nneural audio codec models, such as HiFi-Codec [85] and\nDAC [41], have been investigated. Furthermore, SPEAR-\nTTS [31] and Make-A-Voice [23] introduced a hierarchical\nspeech synthesis framework from semantic to acoustic token\nto reduce the gap between text and speech. Moreover, to\nreduce inference speed and improve the robustness of auto-\nregressive methods, SoundStorm [6] proposed parallel audio\ngeneration methods that generate the token of a neural audio\ncodec. UniAudio [86] presented a multi-scale Transformer\narchitecture to reduce the computational complexity of long\naudio sequences.\n2.2\nNon-autoregressive Models\nFor fast and robust speech synthesis, FastSpeech [68] intro-\nduced a duration predictor to synthesize speech in parallel,\nand they significantly improved the robustness of speech\nsynthesis by addressing the limitations of auto-regressive\nmodels such as repeating and skipping. To reduce the one-\nto-many mapping problem in non-autoregressive speech\nsynthesis, FastSpeech 2 [67] adopted a variance adaptor that\ncan reflect pitch and energy information. However, these\nmodels require an external duration extractor to align the\ntext and speech. Glow-TTS [34] introduces a monotonic\nalignment search and normalizing flow to learn text-speech\nalignment and train the TTS model simultaneously. They\nadd a blank token interspersed between phoneme tokens to\nincrease robustness. VITS [35] combined the TTS model and\na neural vocoder using VAE for end-to-end TTS frameworks\nwith the aim of improving the quality of synthetic speech.\nNaturalSpeech [75] achieved human-level quality in a single\nspeaker TTS by introducing a bidirectional normalizing\nflow and adopting a differentiable duration modeling and\nphoneme pre-training. Moreover, HierSpeech [48] leveraged\na self-supervised speech representation in end-to-end speech\nsynthesis, which significantly reduced the information gap\nbetween text and speech, thus addressing speech mispro-\nnunciations. In addition, HierVST [45] utilized a hierarchical\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n3\nVAE for zero-shot voice style transfer, and which significantly\nimproved the voice style transfer performance in end-to-end\nspeech synthesis models without any labels. ZS-TTS [37],\nWavthruVec [71] and VQTTS [17] utilized a self-supervised\nspeech representation as an intermediate acoustic representa-\ntion for robust speech synthesis. NANSY++ [9] introduced\na unified speech synthesizer for various voice applications\nsuch as TTS, VC, singing voice synthesis, and voice control.\nSome studies [26] have combined a parallel TTS with LLM-\nbased prosody modeling for expressive speech synthesis.\n2.3\nDiffusion Models\nDiffusion models have also demonstrated their powerful\ngenerative performances in speech synthesis. Grad-TTS [63]\nfirst introduced a score-based decoder to generate a Mel-\nspectrogram, and Diff-VC demonstrated the high-adaptation\nperformance of diffusion models in zero-shot voice con-\nversion scenarios. DiffSinger achieved a state-of-the-art\nperformance in SVS task by generating a high-quality singing\nvoice with a powerful adaptation performance. DDDM-VC\n[10] significantly improved speech representation disentangle\n[3] and voice conversion performance by a disentnalged\ndenoising diffusion model and prior mixup. Diff-HierVC [11]\nintroduced a hierarchical voice style transfer frameworks that\ngenerates pitch contour and voice hierarchically based on dif-\nfusion models. Guided-TTS [33] and Guided-TTS 2 [38] have\nalso shown good speaker adaptation performance for TTS.\nUnitSpeech [32] introduced a unit-based speech synthesis\nwith diffusion models. Furthermore, recent studies utilized a\ndiffusion model in latent representation. Naturalspeech 2 [70]\nand HiddenSinger [24] utilized the acoustic representation\nof an audio autoencoder as a latent representation, and\ndeveloped a conditional latent diffusion model for speech\nor singing voice synthesis. StyleTTS 2 [54] proposed a style\nlatent diffusion for style adaptation. Although all the above\nmodels have shown powerful adaptation performance, they\nhave a slow inference speed for their iterative generation\nmanner. To reduce the inference speed, CoMoSpeech [87]\nand Multi-GradSpeech [83] adopted a consistency model for\na diffusion-based TTS model. Recently, VoiceBox [43] and\nP-Flow [39] utilized flow matching with optimal transport for\nfast sampling. However, these models still have a training-\ninference mismatch problem that arises from two-stage\nspeech synthesis frameworks and they are vulnerable to\nnoisy target voice prompt.\n2.4\nZero-shot Voice Cloning\nZero-shot learning [81] for voice cloning is a task to synthe-\nsize speech with a novel speaker, which has not been previ-\nously observed during training. A majority of the studies on\nvoice cloning [25], [74] focused on cloning the voice styles,\nsuch as timbre and environment, and speaking styles, such\nas prosody and pronunciation. [72] presented a reference\nencoder for prosody modeling, and GST [80] utilized a\nlearnable token for style modeling from the reference speech\nor manually control. [51] proposed a fine-grained prosody\ncontrol for expressive speech synthesis from reference speech.\nMulti-SpectroGAN [50] utilized adversarial feedback and\na mixup strategy for an expressive and diverse zero-shot\nTTS. Meta-StyleSpeech [59] introduced meta-learning for\nstyle modeling, and GenerSpeech [22] utilized a mix-style\nlayer normalization for better generalization on the out-of-\ndomain style transfer. PVAE-TTS [44] utilized a progressive\nstyle adaptation for high-quality zero-shot TTS. AdaSpeech\n[8] introduced adaptive layer normalization for adaptive\nspeech synthesis. YourTTS [7] trained the VITS [35] with a\nspeaker encoder and Grad-StyleSpeech [29] utilized a style-\nconditioned prior on a score-based Mel-spectrogram decoder\nfor better adaptive TTS. Built upon VQTTS [17], TN-VQTTS\n[18] introduce a timbre-normalized vector-quantized acoustic\nfeatures for speaking style and timbre transfer. Meanwhile,\nthere are text prompt-based style generation models which\ncan describe a speaking or voice style from text descriptions\n[19], [52], [84].\n3\nHIERSPEECH++\nIn this study, we propose HierSpeech++, a human-level zero-\nshot speech synthesis model in terms of naturalness and\nvoice similarity. We present a novel and efficient hierarchical\nspeech synthesis framework that consists of a hierarchical\nspeech synthesizer, text-to-vec (TTV), and speech super-\nresolution (SpeechSR) as illustrated in Fig 1. This framework\nfacilitates the training of each model with scale-up in that\nwe can simply utilize large-scale low-resolution speech data\nfor voice cloning. The details are described in the following\nsubsections.\n3.1\nSpeech Representations\nWe utilize downsampled audio at 16 kHz for speech synthe-\nsizer because the human voice frequency band ranges under\n4 kHz, and it is necessary to use at least twice the highest\ncomponent of the voice frequency for the reconstruction of\nthe voice signal. Furthermore, a low-resolution ASR dataset\ncan be used to train the speech synthesizer. For better\nperceptual quality, we upsample the audio from 16 kHz\nto 48 kHz for a post-processing using SpeechSR. For acoustic\nand semantic representations, we utilize a low-resolution\nrepresentation of 50 Hz for efficient training.\n3.1.1\nAcoustic Representation\nFor acoustic representation, conventional TTS systems utilize\nthe Mel-spectrogram as an intermediate acoustic feature,\nwhich is transformed from the waveform using a short-time\nFourier transform (STFT). Recently, a neural audio codec was\ninvestigated in the TTS model, wherein the Mel-spectrogram\nwas replaced with a trained audio codec that can be decoded\nto waveform signals. However, acoustic features comprise\nvarious attributes, including semantic information, such as\npronunciation and context, as well as voice information,\nsuch as timbre, intonation, and recording environment.\nIn this regard, it is difficult to directly infer these rich\nrepresentations from text by exacerbating the one-to-many\nmapping problem, and this may result in mispronunciation,\nover-smoothed speech, and a lack of similarity. To reduce the\naforementioned problems, HierSpeech [48] adopted a self-\nsupervised speech representation as an additional semantic\nrepresentation to bridge the gap between text and acoustic\nfeatures as described in the following subsection.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n4\n1\nText sequence\n\u201cHello, World!\u201d\n(a) Semantic Modeling \n(without any transcript)\n(b) Hierarchical Speech Synthesis  \n(without any transcript)\nHierarchical\nSpeech \nSynthesizer\nSpeech-to-Semantic\nModeling\nSpeech\nSuper-\nResolution\n(c) Speech Super-resolution\n(without any transcript)\n(d) Text-to-Vec\nVoice\nModeling\nSpeech\n16 kHz\nWaveform\n48 kHz\nWaveform\nVoice\nPrompt\nText-to-Vec\nProsody\nModeling\nFig. 1: Hierarchical speech synthesis pipeline of HierSpeech++. (a) For speech-to-semantic modeling, we use MMS which is a\npre-trained Wav2Vec 2.0 with 1,406 language speech data. (b) Hierarchical speech synthesizer generates a 16 kHz waveform\naudio from semantic representation and voice prompt. (c) SpeechSR upsamples a 16 kHz waveform audio to 48 kHz. (d) For\nTTS, text-to-vec system generates a semantic representation from text sequence and prosody prompt.\n3.1.2\nSemantic Representation\nWe utilize Wav2Vec 2.0 [2] to extract a continuous semantic\nrepresentation from a waveform without any label. In recent\nyears, many researches have adopted a self-supervised\nspeech representation for semantic representation given that\nthe representation from the middle layer of these models\ncontains linguistic information learned in a self-supervised\nmanner. Meanwhile, phonetic posteriorgrams (PPG) that are\nextracted from ASR models [56] or phoneme information\n[49] could be a good alternative for a rich resourced mono-\nlingual model. However, this decreases the expressiveness\nand robustness of the zero-shot voice cloning and multi-\nlingual speech synthesis scenarios. Unlike HierSpeech [48]\nwhich used XLS-R [1], we utilize a massively multilingual\nspeech (MMS) [65] which is a pre-trained Wav2Vec 2.0 with\na massive scale. MMS was trained with 1,406 language\nspeech data and it was observed that MMS performed\nbetter than XLS-R in many down-stream tasks. For zero-\nshot cross-lingual speech synthesis, we extract the semantic\nrepresentation from the middle layer of the MMS.\n3.1.3\nStyle Representation\nWe use two global style representations for the prosody and\nvoice style. For prosody style representation, we extract a\nglobal prosody representation from the Mel-spectrogram of\nthe reference prosody prompt, which is conditioned to the\nTTV model. For voice style representation, we also extract a\nglobal prosody representation from the Mel-spectrogram\nof the reference voice prompt, and this is utilized for\nconditioning the hierarchical speech synthesizer. As we\ndisentangle the semantic and acoustic modeling, we can\ntransfer the prosody and voice styles in TTV and the\nhierarchical speech synthesizer separately. Although we\nextract both representations from the same Mel-spectrogram,\neach representation is trained to reflect each characteristic\nbecause each target representation contains semantic and\nacoustic information respectively.\n3.2\nHierarchical Speech Synthesizer\nWe propse a hierarchical speech synthesizer as the backbone\nspeech synthesizer for HierSpeech++. We can train this\nmodule using only speech data without any labels such\nas speaker id and text transcripts. Building on HierVST, we\nsignificantly improved the model for human-level audio\nquality and zero-shot style transfer, as illustrated in Fig 2.\n3.2.1\nDual-audio Acoustic Encoder\nAlthough previous models successfully increased the acous-\ntic capacity by replacing the Mel-spectrogram with a linear\nspectrogram, the autoencoder that is trained by minimizing a\nKL divergence has shown low reconstruction quality in terms\nof PESQ, Mel-spectrogram distance, pitch periodity, and\nvoice/unvoice F1 score. This may reduce a perceptual quality\nin zero-shot voice cloning. To address the limitations of using\nlinear spectrogram and improve the perceptual quality, we\nintroduce a dual-audio acoustic encoder designed to capture\na more comprehensive and richer acoustic representation.\nWe add a waveform encoder for distilling information from\nthe raw waveform audio and concatenate a representation\nfrom waveform audio with a representation from a linear\nspectrogram. Finally, the acoustic representation is projected\nfrom a concatenated representation.\n3.2.2\nSource-filter Multi-path Semantic Encoder\nFollowing HierVST, we also utilize a multi-path self-\nsupervised speech representation for speaker-related and\nspeaker-agnostic semantic representations. Each representa-\ntion is used as a prior for hierarchical style adaptation. We\nextract semantic representations from the middle layer of\nMMS to obtain linguistic information1. We also utilize a fun-\ndamental frequency (F0) to improve speech disentanglement\nfor enhanced prior, which enables a manual control of the\npitch contour. For speaker-agnostic semantic representation,\nwe utilize a speech perturbation to remove speaker-related\ninformation from the self-supervised speech representation.\nFor speaker-related semantic representation, we do not use\nspeech perturbation to reduce the gap between semantic and\nacoustic representation.\n3.2.3\nHierarchical Variational Autoencoder\nWe adopt the structure of HierSpeech [48] wherein we\nreplaced the text encoder with a linguistic restorer. We\nutilize the perturbed linguistic representation xw2v,pert as\n1. Specifically, we utilize the representation from the seventh layer of\nMMS.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n5\n\ud835\udc65\ud835\udc642\ud835\udc63\nF0\n\ud835\udc65\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50\nMel\nSpeaker-agnostic\nSource-Filter\nEncoder\n\ud835\udc53\ud835\udc4e(\ud835\udc67\ud835\udc4e)\nSpeaker-related\nSource-Filter\nEncoder\nF0\n\ud835\udc65\ud835\udc642\ud835\udc63, \ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc61\ns\ns\nWav\ns\nBiT-Flow\n\ud835\udc53\ud835\udc60\ud835\udc53(\ud835\udc67\ud835\udc60\ud835\udc5f)\n\ud835\udc67\ud835\udc60\ud835\udc4e\nKL\nFlow \ud835\udc53\ud835\udc60\ud835\udc53\nKL\n\ud835\udf07\ud835\udc4e,\ud835\udf0e\ud835\udc4e\nFlow \ud835\udc53\ud835\udc4e\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udf07\ud835\udc60\ud835\udc4e,\ud835\udf0e\ud835\udc60\ud835\udc4e\n\ud835\udf07\ud835\udc60\ud835\udc5f,\ud835\udf0e\ud835\udc60\ud835\udc5f\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udc53\ud835\udc60\ud835\udc53\n\u22121(\ud835\udc67\ud835\udc60\ud835\udc4e)\n\ud835\udc67\ud835\udc4e\nKL\n\ud835\udc53\ud835\udc4e\u22121(\ud835\udc67\ud835\udc60\ud835\udc5f)\nKL\nBiT-Flow\nSlice\nDual-audio\nAcoustic\nEncoder\nHierarchical\nAdaptive\nGenerator\ns\nProsody\nEncoder\nMel\n(20 bin)\ns or \u2205\nStyle\nEncoder\nMel-\nSpectrogram\ns\nYAPPT\nF0\nWaveform\nMMS\n\ud835\udc65\ud835\udc642\ud835\udc63\nWaveform\nFlow \ud835\udc53\ud835\udc60\ud835\udc53\n\u22121\nFlow \ud835\udc53\ud835\udc4e\u22121\nSpeaker-agnostic\nSource-Filter\nEncoder\nF0\nHierarchical\nAdaptive\nGenerator\nStyle\nEncoder\nMel-\nSpectrogram\ns\ns\ns\nMMS\n\ud835\udc65\ud835\udc642\ud835\udc63\nYAPPT\nSTFT\nWav\nReal / Fake\nReal / Fake\ns\nMPD\nMS-\nSTFTD\n(a) Training\n\ud835\udc65\ud835\udc642\ud835\udc63\nF0\n\ud835\udc65\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50\nMel\nMPD\nMS-\nSTFTD\nSpeaker-agnostic\nSource-Filter\nEncoder\n\ud835\udc53\ud835\udc4e(\ud835\udc67\ud835\udc4e)\nSpeaker-related\nSource-Filter\nEncoder\nF0\n\ud835\udc65\ud835\udc642\ud835\udc63, \ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc61\ns\ns\nWav\ns\nBiT-Flow\n\ud835\udc53\ud835\udc60\ud835\udc53(\ud835\udc67\ud835\udc60\ud835\udc5f)\n\ud835\udc67\ud835\udc60\ud835\udc4e\nKL\nFlow \ud835\udc53\ud835\udc60\ud835\udc53\nKL\n\ud835\udf07\ud835\udc4e,\ud835\udf0e\ud835\udc4e\nFlow \ud835\udc53\ud835\udc4e\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udf07\ud835\udc60\ud835\udc4e,\ud835\udf0e\ud835\udc60\ud835\udc4e\n\ud835\udf07\ud835\udc60\ud835\udc5f,\ud835\udf0e\ud835\udc60\ud835\udc5f\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udc53\ud835\udc60\ud835\udc53\n\u22121(\ud835\udc67\ud835\udc60\ud835\udc4e)\n\ud835\udc67\ud835\udc4e\nKL\n\ud835\udc53\ud835\udc4e\u22121(\ud835\udc67\ud835\udc60\ud835\udc5f)\nKL\nBiT-Flow\nSlice\nDual-audio\nAcoustic\nEncoder\nHierarchical\nAdaptive\nGenerator\ns\nProsody\nEncoder\nMel\n(20 bin)\ns or \u2205\nStyle\nEncoder\nMel-\nSpectrogram\ns\nYAPPT\nF0\nWaveform\nMMS\n\ud835\udc65\ud835\udc642\ud835\udc63\nWaveform\nFlow \ud835\udc53\ud835\udc60\ud835\udc53\n\u22121\nFlow \ud835\udc53\ud835\udc4e\u22121\nSpeaker-agnostic\nSource-Filter\nEncoder\nF0\nHierarchical\nAdaptive\nGenerator\nStyle\nEncoder\nMel-\nSpectrogram\ns\ns\ns\nMMS\n\ud835\udc65\ud835\udc642\ud835\udc63\nYAPPT\nSTFT\nWav\nReal / Fake\nReal / Fake\ns\n(b) Inference\nFig. 2: Hierarchical speech synthesizer of HierSpeech++.\nconditional information c to hierarchically generate wave-\nform audio. In addition we use an enhanced linguistic\nrepresentation from the self-supervised representation of\nthe original waveform, which is not perturbed. Moreover,\nthe raw waveform audio is reconstructed from the acoustic\nrepresentation which is extracted using a linear spectrogram\nand waveform audio during training. To connect the acoustic\nand multi-path linguistic representations, we utilize hierar-\nchical variation inference. The optimization objective of a\nhierarchical speech synthesizer can be defined as follows:\nlog p\u03b8(x|c) \u2265 Eq\u03d5(z|x)\nh\nlog p\u03b8d(x|za)\n\u2212 log q\u03d5a(za|x)\np\u03b8a(za|zsr) \u2212 log q\u03d5sr(zsr|xw2v, F0)\np\u03b8sr(zsr|c)\ni (1)\nwhere q\u03d5a(za|x) and q\u03d5sr(zsr|xw2v, F0) are the approximate\nposteriors for the acoustic and linguistic representations\nrespectively. Here, p\u03b8sr(zsr|c) represents a prior distribution\nof linguistic latent variables zsr given condition c, p\u03b8a(za|zsr)\ndenotes a prior distribution on acoustic latent variables za,\nand p\u03b8d(x|za) is the likelihood function represented by a\nHAG that produces data x given latent variables za. Con-\ndition c consists of a perturbed representation of xw2v,pert\nand a log-scale fundamental frequency F0. In addition, we\nuse a normalizing flow to improve the expressiveness of\neach linguistic representation. For reconstruction loss, we\nuse multiple reconstruction terms of a HAG with adversarial\ntraining as described in the following subsection.\n3.2.4\nHierarchical Adaptive Generator\nFor semantic-to-waveform generation, we introduce the\nHAG G which comprises of source generator Gs and\nwaveform generator Gw as illustrated in Fig 3. The generated\nrepresentations including acoustic representation za and style\nrepresentation s are fed to Gs, and Gs generates the refined\npitch representation ph and auxiliary F0 predictor is used to\nenforce the F0 information on ph as follows:\nLpitch = \u2225px \u2212 Gs(za, s)\u22251,\n(2)\nwhere px is the ground-truth F0. Subsequently, Gw synthe-\nsizes the waveform audio from za, ph, s hierarchically, and\nwe use the reconstruction loss between the GT and generated\nMel-spectrogram transformed from waveform audio using\nSTFT with Mel-filter \u03c8 as follows:\nLST F T = \u2225\u03c8(x) \u2212 \u03c8(Gw(za, ph, s))\u22251.\n(3)\nFor better perceptual audio quality, we utilize adversarial\ntraining [12], [50]. We adopt the multi-period discriminator\n(MPD) [40] and the multi-scale STFT discriminator (MS-\nSTFTD) [15] which can reflect the characteristics of the real\nand imaginary components from a complex-valued STFT as:\nLadv(D) = E(x,za)\nh\n(D(x) \u2212 1)2 + D(G(za, s))2i\n,\n(4)\nLadv(\u03d5a, \u03b8d) = E(za)\nh\n(D(G(za, s)) \u2212 1)2i\n(5)\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n6\nPerturbed\nwav\nContent\nFilter\nEncoder\nSource\nEncoder\nAdaptive\nEncoder\nMel\nYAPPT\nF0\nWav\nStyle\nEncoder\n\ud835\udf07, \ud835\udf0e\nWav\nEncoder\nProjection\nSpec\nEncoder\n\ud835\udc65\ud835\udc60\ud835\udc5d\ud835\udc52\ud835\udc50\nWav\n\ud835\udf07\ud835\udc4e,\ud835\udf0e\ud835\udc4e\n(a) Source-Filter Encoder\nMMS\ns\n(b) Dual-audio Acoustic Encoder\nTransposed\nConv1d\n\u00d7 4\nPreConv\nTransposed\nConv1d\nF0\nProjection\ns\n\ud835\udc5d\u210e\nAlias-Free \nActivation1d\n\u00d7 2\nAMP\n\u00d7 3\nConv1d\n\ud835\udc5d\ud835\udc67\n\ud835\udc5d\ud835\udc51\n\ud835\udc67\ud835\udc4e\nSlice\n(d) Hierarchical Adaptive Generator\nPreConv\ns\nConv1d\n\ud835\udc67\ud835\udc4e\nSlice\nAlias-Free \nActivation1d\nProjection\nAMP\n\u00d7 3\nTransposed\nConv1d\nAMP\n(c) Bidirectional Transformer Flow\nResidual coupling layer\n\ud835\udc65\ud835\udc4e, \ud835\udc56\u22121\n\ud835\udc65\ud835\udc4f, \ud835\udc56\u22121\n\ud835\udc65\ud835\udc4e, \ud835\udc56\nMLP\nPreConv\n\ud835\udc65\nPostConv\nSplit\ns\n\ud835\udf07\n\ud835\udc65\ud835\udc4f,\ud835\udc56\u22121 \u2299 exp \ud835\udf0e + \ud835\udf07\nTransformer \nblock\n\u00d7 3\n\u00d7 4\nLayer Norm\nScale, Shift\nMulti-Head\nSelf-Attention\nLayer Norm\nScale, Shift\nFeed-Forward\nNetwork\nScale\nScale\n\ud835\udefe1, \ud835\udefd1\n\ud835\udefc1\n\ud835\udefc2\n\ud835\udefe2, \ud835\udefd2\ns\nFig. 3: Architecture details for hierarchical speech synthesizer. For inference, we can generate a waveform from semantic\nrepresentation, F0, and voice prompt. The semantic representation and F0 can be extracted from source speech for voice\nconversion, and can generated by TTV for Text-to-speech.\n3.2.5\nBidirectional Transformer Flow\nPreviously, VITS utilized a normalizing flow to improve the\nexpressiveness of the prior distribution, which possessed\nthe ability to bridge the posterior and prior distributions\nwell, such that the audio quality was significantly improved.\nHowever, there is a train-inference mismatch problem\nand limitations on speaker adaptation in this framework.\nTherefore, we propose BiT-Flow, which is a bidirectional\nTransformer-based normalizing flow. First, we adopt the\nbidirectional normalizing flow proposed by NaturalSpeech\n[70]. We also replace WaveNet-based adaptive networks with\nconvolutional feedforward-based Transformer networks [20]\nto capture a large context in the latent representation. Unlike\nVITS2, we utilize AdaLN-Zero in Transformer block [62] and\ntrain the blocks bidirectionally with dropout. The details are\nillustrated in Fig 3 For efficient training, we sliced audio\nsequence during training the model so we do not utilize a\npositional embedding in Transformer networks.\n3.2.6\nProsody Distillation\nWe utilize prosody distillation to enhance the linguistic\nrepresentation zsr from the speaker-related source-filter\nencoder. zsr is fed to the prosody decoder to reconstruct the\nfirst 20 bins of the Mel-spectrogram containing the prosody\ninformation. By conditioning the voice style representation,\nwe enforce zsr to learn speaker-related prosody to enhance\nlinguistic information. We train the model with prosody loss\nLprosody which minimizes the l1 distance between the 20\nbins of the GT and the reconstructed Mel-spectrogram.\n3.2.7\nUnconditional Generation\nFollowing [45], we utilize unconditional generation in a\nhierarchical adaptive generator for progressive speaker\nadaptation. The use of unconditional generation makes the\nacoustic representations adopt speaker characteristics, thus\nimproving the speaker adaptation performance in the entire\nmodel. We simply replace the voice style representation s\nwith the null style embedding \u2205 with a 10% chance during\ntraining. In the inference stage, we utilize only the target\nvoice style representation for conditional generation.\n3.3\nText-to-Vec\nFor TTS, we introduce a text-to-vec (TTV) model that gener-\nates a semantic representation and F0 from a text sequence.\nFollowing VITS [35], we utilize a variational autoencoder and\na monotonic alignment search (MAS) to align the text and\nspeech internally, as shown in Fig 4. We replace the linear\nspectrogram with a self-supervised speech representation for\nthe input of posterior encoder, and we reconstruct the same\nself-supervised speech representation for the output of TTV.\nFurthermore, we predict a F0 with four\u00d7 larger resolutions\nthan the self-supervised speech representation. We use a text\nsequence and prosody prompt as conditional information\nto generate a self-supervised speech representation of the\ndata. We utilize a prosody conditional text representation\nas the prior information. A prosody style representation\nis extracted from the full-length input speech as a global\nstyle embedding. Owing to the semantic information of self-\nsupervised speech representation, we can transfer a prosody\nstyle in the TTV framework which is almost irrelevant to the\nvoice style. To increase the linguistic capacity of a semantic\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n7\n(a) Training of Text-to-Vec\n(b) Inference of T\nText Encoder\nText (Phoneme)\nProjection\nFlow \nContent\nDecoder\nContent\n\ud835\udc65\ud835\udc642\ud835\udc63\nF0\nSpeaker-agnostic\nSource-Filter\nEncoder\nSpeaker-related\nSource-Filter\nEncoder\nF0\n\ud835\udc65\ud835\udc642\ud835\udc63, \ud835\udc5d\ud835\udc52\ud835\udc5f\ud835\udc61\ns\ns\nBiT-Flow\n\ud835\udc53\ud835\udc60\ud835\udc53(\ud835\udc67\ud835\udc60\ud835\udc5f)\n\ud835\udc67\ud835\udc60\ud835\udc4e\nKL\nFlow \ud835\udc53\ud835\udc60\ud835\udc53\nKL\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udf07\ud835\udc60\ud835\udc4e,\ud835\udf0e\ud835\udc60\ud835\udc4e\n\ud835\udf07\ud835\udc60\ud835\udc5f,\ud835\udf0e\ud835\udc60\ud835\udc5f\n\ud835\udc67\ud835\udc60\ud835\udc5f\n\ud835\udc53\ud835\udc60\ud835\udc53(\ud835\udc67\ud835\udc60\ud835\udc4e)\ns\nProsody\nEncoder\nMel\n(20 bin)\nStyle\nEncoder\nMel-\nSpectrogram\ns\nMMS\n\ud835\udc65\ud835\udc642\ud835\udc63\nYAPPT\nF0\nWaveform\nWaveform\nText \nEncoder\nPhoneme\n(w. blank)\nT-Flow \ud835\udc53\n\ud835\udc53(\ud835\udc67)\nMAS\nProjection\nWaveform\nMMS\nContent \nEncoder\nPitch\nPredictor\nDuration\nPredictor\nNoise\nContent \nDecoder\nStop\ngradient\n\ud835\udc67\nPhoneme\nPredictor\nPhoneme\n(w.o. blank)\nx\ud835\udc642\ud835\udc63\n\u0ddc\ud835\udc65\ud835\udc642\ud835\udc63\nFig. 4: Text-to-Vec\nrepresentation, a latent representation is fed to the phoneme\nencoder, and the connectionist temporal classification (CTC)\nloss is minimized. We found that this could improve text-\nspeech alignment by significantly decreasing the CER and\nWER of synthetic speech. Furthermore, we use a Transformer-\nbased normalizing flow with AdaLN-Zero for better prosody\nadaptation.\n3.4\nSpeech Super-resolution\nWe use a low-resolution speech dataset to train the model\nin terms of data availability and efficiency. In this stage, we\nsimply upsample a low-resolution speech waveform to a\nhigh-resolution speech waveform from 16 kHz to 48 kHz\nas illustrated in Fig 5. We use only one anti-aliased multi-\nperiodicity composition (AMP) block of BigVGAN, which\nconsists of a low-pass filter and periodic activation Snake\nfunction for inductive bias and anti-aliasing. We further\nreplace a transposed convolution with nearest neighbor (NN)\nupsampler. Previously, an NN upsampler was shown to\nalleviate tonal artifacts caused by transposed convolutions.\nWe found that the NN upsampler also reduce the error in\nthe high spectrum compared to the transposed convolution.\nBased on our hierarchical speech synthesizer, we use MPD\nand MS-STFTD for high-quality audio synthesis. Addi-\ntionally, we propose a DWT-based sub-band discriminator\n(DWTD) to decompose the audio component and reflect\nthe features of each sub-band, respectively as shown in Fig\n5. Previously, Fre-GAN [36] and Fre-GAN 2 [47] already\nutilize a DWT-based discriminator to replace average polling\nwith a DWT for lossless downsampling. However, in this\nwork, we also decompose a discriminator into sub-band\ndiscriminators for each sub-audio ([0 kHz, 12 kHz], [12 kHz,\n24 kHz], [24 kHz, 36 kHz], [36 kHz, 48 kHz]), which improves\nthe reconstruction quality for high-frequency bands.\n3.4.1\nHierarhical Speech Synthesizer\nDual-audio acoustic encoder consists of waveform audio\nencoder (wav encoder) and linear spectrogram encoder (spec\nencoder). The wav encoder consists of AMP blocks of the\nBigVGAN and downsampling blocks to map the temporal\nsequence between spectrogram and wav2vec representation.\nWe use downsampling rates of [8,5,4,2] with kernel sizes of\n[17,10,8,4] and hidden sizes of [16,32,64,128,192]. For the spec\nencoder, we utilize 16 layers of non-causal WaveNet with\nhidden size of 192. The HAG consists of source generator and\nwaveform generator. We replace multi-receptive field fusion\n(MRF) blocks with AMP blocks that has a low-pass filter and\nReal / Fake\nReal / Fake\nReal / Fake\nMPD\nMS-\nSTFTD\nMel\n16 kHz\n48 kHz\nSpeechSR\nDWTD\n0 ~ 12 kHz\n12 ~ 24 kHz\n24 ~ 36 kHz\n36 ~ 48 kHz\nDWT\nDWT-based Sub-band Discriminator\nDWTD\nDWTD\nDWTD\nDWTD\nFig. 5: Speech Super-resolution\nperiodic activation Snake function for inductive bias and anti-\naliasing. For the source generator, we utilize an upsampling\nrate of [2,2]] with an initial channel of 256. For the waveform\ngenerator, we utilize upsampling rates of [4,5,4,2,2] with\ninitial channel of 512. For a discriminator, we utilize a multi-\nperiod discriminator (MPD) with the period of [2,3,5,7,11]\nand a multi-scale STFT-based discriminator (MS-STFTD)\nwith five different sizes of window ([2048,1024,512,256,128]).\nFor the Source-filter encoder, source, filter, and adaptive\nencoders consist of eight layers of non-causal WaveNet\nwith hidden size of 192. BiT-Flow consists of four residual\ncoupling layers which comprises a previous convolutional\nnetworks (preConv), three Transformer blocks, and post\nconvolutional networks (postConv). We adopt convolutional\nneural networks with a kernel size of five in Transformer\nblocks for encoding adjacent information and AdaLN-Zero\nfor better voice style adaptation. We utilize a hidden size\nof 192, a filter size of 768, and two attention heads for the\nTransformer blocks. We utilize a dropout rate of 0.1 for BiT-\nFlow. The style encoder consists of two spectral encoders\nwith linear projection and two temporal encoder with 1D\nconvolutional networks, and multi-head self-attention.\n3.5\nModel Architecture\n3.5.1\nText-to-Vec\nThe content encoder of the TTV consists of 16 layers of non-\ncausal WaveNet with a hidden size of 256 and a kernel size\nof five. Content decoder consists of eight layers of non-causal\nWaveNet with hidden size of 512 and kernel size of five.\nThe text encoder is composed of three unconditional Trans-\nformer networks and three prosody-conditional Transformer\nnetworks with a kernel size of nine, a hidden size of 256\nand a filter size of 1024. We utilize a dropout rate of 0.2 for\ntext encoder. T-Flow consists of four residual coupling layers\nwhich are composed of a preConv, three Transformer blocks,\nand postConv. We adopt convolutional neural networks\nwith a kernel size of 5 in Transformer blocks for encoding\nadjacent information and AdaLN-Zero for better prosody\nstyle adaptation. We utilize a hidden size of 256, a filter size\nof 1024, and four attention heads for T-Flow. We utilize a\ndropout rate of 0.1 for T-Flow. For the pitch predictor, we\nutilize the source generator with the same structure as that\nof HAG.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n8\n1\nSemantic Modeling \nHigh-quality Speech Synthesis\nTTV\n(a)  Voice Conversion\nHierarchical \nSpeech Synthesizer\nSpeechSR\nContent\nPitch\nContent\nPitch\nMMS\nYAPPT\nHierarchical \nSpeech Synthesizer\nSpeechSR\n(b)  Text-To-Speech\n\u201cHello, World!\u201d\nConverted Speech\nSynthesized speech\nVoice Style \nVoice Modeling\nProsody Modeling\nVoice Style \nVoice Modeling\nFig. 6: Inference scenarios for voice conversion and text-to-speech.\n3.5.2\nSpeechSR\nThe SpeechSR consists of a single AMP block with an initial\nchannel of 32 without an upsampling layer. We utilize an\nNN upsampler for upsampling the hidden representations.\nFor the discriminator, we utilize the MPD with the period\nof [2,3,5,7,11] and MS-STFTD with six different sizes of win-\ndow ([4096,2048,1024,512,256,128]). Additionally, we utilize\nDWTD which has four sub-band discriminators.\n4\nSPEECH SYNTHESIS TASKS\n4.1\nVoice Conversion\nFig 6 illustrates the entire inference pipeline. For voice\nconversion, we first extract the semantic representation by\nMMS from the audio at 16 kHz, and F0 using the YAPPT\nalgorithm. Before feeding F0 to Hierarchical Synthesizer, we\nnormalize F0 using the mean and standard deviation of the\nsource speech. Then, we denormalize a normalized F0 by\nthe mean and standard deviation of the target speech. The\nspeech synthesizer synthesizes 16 kHz speech with a target\nvoice style from the target voice prompt. The SpeechSR can\nupsample the synthesized speech to a high-resolution speech\nof 48 kHz. For a fair comparison, we do not utilize SpeechSR\nto evaluate the VC performance.\n4.2\nText-to-Speech\nFor text-to-speech, we extract semantic representations from\ntext instead of speech. The TTV can generate a semantic\nrepresentation with the target prosody from the prosody\nprompt. The hierarchical speech synthesizer generates speech\nfrom semantic representations, and the SpeechSR can upsam-\nple it to a high-resolution from 16 kHz to 48 kHz. For a\nfair comparison, SpeechSR was not used during the TTS\nevaluation. The prosody and voice styles can be transferred\nfrom different target prompts, respectively.\n4.3\nStyle Prompt Replication\nWe found a simple trick to transfer the style even with\na one second speech prompt by introducing style prompt\nreplication (SPR). Similar to the DNA replication, we copy the\nProsody\nModeling\nVoice\nModeling\nShort\nPrompt\nReplicated\nPrompt (2x)\nReplicated\nPrompt (4x)\nProsody\nStyle\nVoice\nStyle\nFig. 7: Style Prompt Replication\nsame sequence of prompt as shown in Fig 7. The replicated\nprompt by n times is fed to the style encoder to extract the\nstyle representation. Specifically, because the prompt style\nencoder usually encounters a long sequence of prompts over\n3s, synthetic speech from short prompts may be generated\nincorrectly. However, SPR can deceive the style encoder as it\nseems like long prompts, thus we can synthesize the speech\neven with 1s speech prompt.\n5\nEXPERIMENT AND RESULT\nTABLE 1: Training Dataset. We utilize public-available speech\ndataset to train the model. For TTV, we utilize only LibriTTS\ndataset.\nDataset (Subset)\nLanguage\nSR (Hz)\nSpeaker\nHour\nLibriTTS (train-clean-460)\nEnglish\n24,000\n1,151\n245\nLibriTTS (train-other-500)\nEnglish\n24,000\n1,160\n310\nLibri-light* (Medium)\nEnglish\n16,000\n2,075\n1,007\nEXPRESSO\nEnglish\n48,000\n4\n47\nNIKL\nKorean\n16,000\n118\n98\nMSSS\nKorean\n48,000\n2,791\n1,101\nTotal\n-\n-\n7,299\n2,796\n5.1\nDataset\nWe utilized LibriTTS dataset [90] to train the hierarchical\nspeech synthesizer. First, we trained the model with train-\nclean subsets of LibriTTS (train-clean-100 and train-clean-360)\nfor a fair comparison. Additionally, we utilized the train-\nother-500 subsets of LibriTTS for better voice style transfer.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n9\nFurthermore, we scaled-up the dataset to 1 kh to improve the\nrobustness and diversity, as indicated in TABLE 12. For the\nLibri-light [27] and Multi-Speaker Speech Synthesis (MSSS)\ndataset of AIHub 3, we sampled a small portion of speech\nfrom each speaker. We used a EXPRESSO [61] and NIKL4.\nWe downsampled the audio at 16 kHz, and normalized it\nusing a scale of [-0.95, 0.95]. For text-to-vec, we utilized all\nthe train subsets of LibriTTS. For speechSR, we used a VCTK\ndataset [76] which has a sampling rate of 48 kHz to compare\nthe models. However, we also trained the model with a large-\nscale dataset for better speech super-resolution performance\nby including MSSS dataset, VCTK, and EXPRESSO.\n5.2\nPreprocessing\nWe utilized a MMS (0.3B) [65] which is a pre-trained Wav2Vec\n2.0 model with a massively larget-scale cross-lingual speech\ndataset containing speech dataset of 1000 languages. To map\nthe semantic and acoustic representations, we used a hop\nsize of 320 to extract a linear spectrogram. For the style\nencoder, we utilized a Mel-spectrogram with 80 bins. We\nextracted F0 using YAPPT [30] algorithm with a hop of 80.\nFor phoneme transformation, we utilized an International\nPhonetic Alphabet (IPA) sequence with an open-source\nPhonemizer [4]. Following [48], we did not utilize blank\ntokens for the target phoneme sequences of the CTC decoder.\nHowever, we used blank tokens for the input phoneme\nsequences.\n5.3\nTraining\nFor reproducibility, we will release the source code and the\ndetails of all the hyperparameters will be included at the\nhttps://github.com/sh-lee-prml/HierSpeechpp. We trained\nHierSpeech++ using the AdamW optimizer [57] with \u03b21 =\n0.8, \u03b22 = 0.99, and weight decay \u03bb = 0.01, and we apply\nthe learning rate schedule by the decay of 0.9991/8 at an\ninitial learning rate of 1 \u00d7 10\u22124 for the HierSpeech++ with\nLibriTTS dataset and a batch size of 80 for 1,200k steps on\nfour NVIDIA A6000 GPUs. The final model which is trained\nwith the entire dataset continued to train from HierSpeech++\nthat was trained with LibriTTS, and the decay decreased by\n0.999. We trained HierSpeech++ with a batch size of 160 for\n1,000k steps on eight NVIDIA A6000 GPUs. For the ablation\nstudy, the models were trained with a batch size of 80 for 300k\nsteps. We sliced the audio using 61,440 frames for efficient\ntraining, and we used windowed generator training for the\ngenerator using 9,600 frames. HierSpeech++ consists of 63M\nparameters for inference and additional 34M parameters\nonly for training. For TTV, we trained the model using the\nAdamW optimizer [57] with \u03b21 = 0.8, \u03b22 = 0.99, and weight\ndecay \u03bb = 0.01, and apply the learning rate schedule by the\ndecay of 0.999 at an initial learning rate of 2 \u00d7 10\u22124 with a\nbatch size of 128 for 950k steps on four NVIDIA A100 GPUs.\nTTV consists of 107M For SpeechSR, we utilize the same\nconfiguration as BigVGAN, and trained the model with a\nbatch size of 128 for 100k steps on four NVIDIA A6000 GPUs.\n2. Although we hope to increase the data scale to over 10k Hours, this\nis the maximum limit in our academic resources.\n3. https://aihub.or.kr\n4. https://www.nia.or.kr/\n5.4\nEvaluation Metrics\nFor the reconstruction and resynthesis tasks, we conducted\nseven objective metrics: a log-scale Mel error distance (Mel),\nperceptual evaluation of speech quality (PESQ)5, Pitch, peri-\nodicity (Period.), voice/unvoice (V/UV) F1 score, and log-\nscale F0 consistency F0c. We used the official implementation\nof CARGAN [60] for pitch, periodicity, and U/UV F16. For\nF0c, we calculated the L1 distance between the log-scale\nground-truth and the predicted F0 in the HAG.\nFor VC, we used two subjective metrics: naturalness mean\nopinion score (nMOS) and voice similarity MOS (sMOS)\nwith a CI of 95%; and three objective metrics for naturalness:\nUTMOS [69], character error rate (CER) and word error\nrate (WER); two objective metrics for similarity: automatic\nspeaker verification equal error rate (EER), and speaker\nencoder cosine similarity (SECS). We utilized the open-source\nUTMOS7 which is an MOS prediction model for a naturalness\nmetric. Although this can not be considered an absolute\nevaluation metric, we believe that it is a simple way to\nestimate the audio quality of synthetic speech. Additionally,\nthis method does not require ground-truth audio or labels\nto estimate the score. Therefore, we highly recommend\nusing this simple metric during validation by adding a\nsingle line. For CER and WER, we utilized the Whisper\u2019s\nofficial implementation. We used a large model with 1,550\nM parameters and calculated the CER and WER after text\nnormalization, as presented in the official implementation.\nWe utilized a pre-trained automatic speaker verification\nmodels [42]8 which was trained with a large-scale speech\ndataset, VoxCeleb2 [14]. In [13], the effectiveness of metric\nlearning in automatic speaker verification was demonstrated.\nFurthermore, [42] introduced online data augmentation,\nwhich decreased the EER from 2.17% to 1.17%. In addition,\nwe utilized the pre-trained speaker encoder, Resemblyzer 9\nto extract a speaker representation, and we calculated the\ncosine similarity between the speaker representation of the\ntarget speech and synthetic speech.\nFor TTS, we additionally utilize a prosody MOS (pMOS).\nSixty samples were randomly selected for each model. The\nnMOS was rated by 10 listeners on a scale of 1-5, and the\nsMOS and pMOS were rated by 10 listeners on a scale of 1-4.\nA confidence interval of 95% was reported for MOS.\n5.5\nAblation Study\nBefore we compare the model with other baselines in the TTS\nand VC tasks, we conducted ablation studies by comparing\nReconstruction10, Resynthesis 11, and VC performance to\nverify the effectiveness of each component in HierSpeech++.\nFirst, although previous E2E models have shown high-\nquality waveform audio generation, the zero-shot speech\nsynthesis performance was considerably low, and some stud-\nies must fine-tune or use speaker id for speaker adaptation.\nRecently, HierVST has significantly improved a voice style\ntransfer performance of the E2E model; therefore so we\n5. https://github.com/ludlows/PESQ\n6. https://github.com/descriptinc/cargan\n7. https://github.com/tarepan/SpeechMOS\n8. https://github.com/clovaai/voxceleb_trainer\n9. https://github.com/resemble-ai/Resemblyzer\n10. Reconstruction: Posterior Encoder \u2192 Generator \u2192 Audio\n11. Resynthesis: Prior Encoder \u2192 Generator \u2192 Audio\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n10\nTABLE 2: Reconstruction results for ablation studies. LT-460 denotes LibriTTS train-clean-100 and 360 subsets, and LT-960\nadditionally utilize a train-other-500 subset with LT-460. ALL denotes that the model is trained with all dataset in Section 5.1.\nMethod\nDataset\nAMP\nSFE\nDAE\nT-Flow\nBi-Flow\n\u03bbbi\nMel\nPESQ-wb\nPESQ-nb\nPitch\nPeriod.\nV/UV F1\nF0c\nALL\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.50\n2.59\n3.20\n19.49\n0.087\n0.955\n0.103\nHierSpeech++\nLT-960\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.53\n2.47\n3.09\n20.58\n0.089\n0.952\n0.102\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.58\n2.29\n2.91\n24.17\n0.099\n0.946\n0.110\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.1\n0.55\n2.50\n3.14\n20.21\n0.090\n0.950\n0.107\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n-\n0.54\n2.55\n3.17\n21.23\n0.086\n0.953\n0.108\nAblation Study\nLT-460\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n-\n0.56\n2.35\n3.01\n23.71\n0.098\n0.949\n0.110\nLT-460\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n-\n0.63\n2.29\n2.85\n28.68\n0.102\n0.947\n0.133\nLT-460\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n-\n0.63\n2.23\n2.82\n23.85\n0.106\n0.941\n0.146\nHierVST\nLT-460\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n-\n0.66\n2.18\n2.77\n24.53\n0.104\n0.944\n0.125\nTABLE 3: Resynthesis results for ablation studies\nMethod\nDataset\nAMP\nSFE\nDAE\nT-Flow\nBi-Flow\n\u03bbbi\nMel\nPESQ-wb\nPESQ-nb\nPitch\nPeriod.\nV/UV F1\nF0c\nALL\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.58\n2.32\n2.95\n20.21\n0.093\n0.953\n0.073\nHierSpeech++\nLT-960\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.61\n2.23\n2.84\n21.29\n0.090\n0.951\n0.073\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n0.67\n2.04\n2.63\n24.26\n0.107\n0.943\n0.077\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.1\n0.67\n2.10\n2.72\n24.35\n0.104\n0.944\n0.080\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n-\n0.68\n2.07\n2.71\n23.56\n0.099\n0.947\n0.080\nAblation Study\nLT-460\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n-\n0.68\n2.05\n2.68\n24.85\n0.106\n0.943\n0.080\nLT-460\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n-\n0.71\n2.03\n2.63\n26.47\n0.109\n0.944\n0.111\nLT-460\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n-\n0.72\n1.93\n2.50\n34.31\n0.117\n0.935\n0.180\nHierVST\nLT-460\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n-\n0.74\n1.93\n2.47\n35.59\n0.117\n0.938\n0.157\nTABLE 4: Voice Conversion results for ablation studies. Params. denotes the number of model parameters.\nMethod\nDataset\nAMP\nSFE\nDAE\nT-Flow\nBi-Flow\n\u03bbbi\nUTMOS\nCER\nWER\nEER\nSECS\nParams.\nHierSpeech++\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.5\n4.16\n1.09\n3.17\n4.23\n0.856\n63M(+34M)\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n0.1\n4.15\n1.15\n3.49\n4.82\n0.849\n63M(+34M)\nLT-460\n\u2713\n\u2713\n\u2713\n\u2713\n\u2717\n-\n4.12\n0.83\n2.82\n4.86\n0.847\n63M(+34M)\nAblation Study\nLT-460\n\u2713\n\u2713\n\u2713\n\u2717\n\u2717\n-\n4.13\n0.95\n2.98\n7.50\n0.845\n49M(+34M)\nLT-460\n\u2713\n\u2713\n\u2717\n\u2717\n\u2717\n-\n4.15\n1.58\n4.02\n7.01\n0.846\n49M(+26M)\nLT-460\n\u2713\n\u2717\n\u2717\n\u2717\n\u2717\n-\n4.14\n2.01\n4.75\n8.00\n0.837\n45M(+21M)\nHierVST\nLT-460\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n-\n4.08\n2.18\n5.01\n9.09\n0.830\n45M(+21M)\nconduct ablation studies by building up on HierVST.\nAMP Block We first replaced the MRF block of HiFi-GAN\nwith the AMP of BigVGAN for OOD generation. The AMP\nimproved the performance of all tasks in terms of all metrics\nwithout F0 consistency. The results show that the BigVGAN-\nbased HAG performs better but the loss balance may lean\ntoward optimizing the waveform reconstruction rather than\nF0 prediction; however, the naturalness and similarity of\nthe converted speech improved in terms of all metrics.\nSpecifically, objective naturalness exhibited better UTMOS.\nSF Encoder\nTo address F0 consistency, we utilize an SF\nencoder (SFE) for a dual-path semantic encoder, which\nenhances the semantic prior in terms of all metrics. This\nsignificantly improved the F0 consistency of inference sce-\nnario. It is worth noting that F0 can be manually controlled.\nDual-audio Encoder We also utilized a dual-audio posterior\nencoder (DAE) to increase the acoustic capacity of the\nacoustic representation, which significantly increases the\nreconstruction performance. Although the linear spectrogram\ncontains useful information for reconstructing a waveform\naudio, this representation still lacks the ability to reproduce\nall information; therefore, additional information from wave-\nform audio could complement a wave-level acoustic repre-\nsentation. It is worth noting that the DAE was only utilized\nduring training but significantly improved reconstruction\nand pronunciation. However, we found that the enhanced\nacoustic posterior contains a large information resulting in\nreducing a VC performance.\nT-Flow To bridge the gap between each representation, we\nreplace a wavenet-based normalizing flow with Transformer-\nbased normalizing flow (T-Flow) using AdaLN-Zero for style\nadaptation. This also improved the entire performance of all\nmetrics. Moreover, speaker similarity significantly improved.\nBi-Flow\nMoreover, we adopt a bidirectional normaliz-\ning flow (Bi-Flow) to reduce the train-inference mismatch\nproblem. The results show that Bi-Flow slightly decreases\nthe reconstruction quality. However, this could regularize\na posterior by conditioning the information used in the\ninference scenario, thereby improving the VC performance.\nWe also found that high weight of Bi-Flow significantly\ndecreased a reconstruction performance, and thus, we use \u03bb\nof 0.5 for weak regularization.\nLarge-scale Data In addition, we demonstrated that our\nmodel is robust to data scale-up. We did not use any labels\nto train the model, and only used a low-resolution speech\ndataset of 16 kHz, which we could obtain simply due to\nSpeechSR. For scaling-up, we did not conduct any pre-\nprocessing to train the model without down-sampling (any\nto 16 kHz), so there are noisy samples in our dataset however\nwe did not experience any problems.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n11\nTABLE 5: Zero-shot VC results on unseen speakers from VCTK dataset. DiffVC\u2660 denotes pre-trained model from the official\nimplementation trained with LibriTTS. YourTTS\u2663 denotes a pre-trained model from the official implementation trained with\nLibriTTS, VCTK, and others so the result of YourTTS is not zero-shot VC result.\nMethod\nDataset\nnMOS (\u2191)\nsMOS (\u2191)\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nEER (\u2193)\nSECS (\u2191)\nGT\n-\n4.41\u00b10.06\n3.89\u00b10.02\n4.04\n0.21\n2.17\n-\n-\nAutoVC [66]\nLT-460\n3.15\u00b10.09\n2.41\u00b10.08\n3.04\n5.14\n10.55\n37.32\n0.715\nVoiceMixer [46]\nLT-460\n3.71\u00b10.07\n3.13\u00b10.06\n3.19\n1.08\n3.31\n20.75\n0.797\nDiffVC\u2660 [64]\nLT-460\n4.00\u00b10.07\n3.08\u00b10.06\n3.49\n6.86\n13.77\n9.25\n0.826\nDiff-HierVC [11]\nLT-460\n4.14\u00b10.06\n3.50\u00b10.05\n3.34\n0.83\n3.11\n3.29\n0.861\nDDDM-VC [10]\nLT-460\n4.13\u00b10.06\n3.51\u00b10.05\n3.40\n1.77\n4.35\n6.49\n0.858\nYourTTS\u2663 [7]\nLT-460, VCTK, Other\n3.38\u00b10.08\n3.13\u00b10.06\n3.09\n2.42\n6.08\n4.02\n0.848\nHierVST [45]\nLT-460\n4.59\u00b10.04\n3.35\u00b10.06\n4.19\n1.14\n3.46\n5.06\n0.850\nHierSpeech++ (Ours)\nLT-460\n4.54\u00b10.05\n3.63\u00b10.04\n4.19\n0.90\n2.99\n2.50\n0.862\nHierSpeech++ (Ours)\nLT-960\n4.48\u00b10.05\n3.62\u00b10.04\n4.13\n0.79\n3.01\n1.27\n0.875\nHierSpeech++ (Ours)\nLT-960, LL, Expresso, MSSS, NIKL\n4.44\u00b10.05\n3.63\u00b10.04\n4.13\n0.74\n2.87\n1.31\n0.883\n5.6\nZero-shot Voice Conversion\nWe compared the voice style transfer performance of Hier-\nSpeech++ with other basemodels: 1) AutoVC [66], which is\nan autoencoder-based non-autoregressive VC model using\nan information bottleneck to disentangle the content and\nstyle, 2) VoiceMixer [46], which is a GAN-based parallel\nVC model using similarity-based information bottleneck, 3-\n5) Diffusion-based models (DiffVC [64], Diff-HierVC [11],\nand DDDM-VC [10]), 6) YourTTS [7], VITS-based end-to-end\nVC models utilizing phoneme sequences to extract content\ninformation, 7) HierVST [45], hierspeech-based end-to-end\nVC model using hierarchical style adaptation. For a fair\ncomparison, we trained all model with the same dataset (LT-\n460, train-clean-460 subsets of LibriTTS) without YourTTS.\nWe utilized the official implementation of YourTTS which\nwas trained with an additional dataset. We also trained the\nmodel with a large-scale dataset such as LT-960, all training\nsubsets of LibriTTS) and additional datasets to verify the\neffectiveness of scaling-up the dataset.\nFor the subjective objective, TABLE 5 demonstrates\nthat our model significantly improves the naturalness and\nsimilarity of the converted speech in terms of nMOS and\nsMOS. We found that our model with a large-scale dataset\nshowed a better naturalness than ground-truth speech.\nHowever, the results also showed that increasing the\ndataset without filtering noisy data slightly decreased an\naudio quality in terms of nMOS and UTMOS but the\nsimilarity increased consistently according to the data scale.\nIn addition, WER also showed better performance than\nthe other models. Furthermore, the results of the similarity\nmeasurement show that our models perform better in terms\nof the EER and SECS. Moreover, the results verified that\nHierSpeech++ which was trained with a large-scale dataset\nis a much stronger zero-shot speech synthesizer.\nWe will include zero-shot cross-lingual voice style transfer\nresults and additional zero-shot voice conversion results\nwith noisy speech prompts on our demo page. We highly\nrecommend listening to the demo samples and will release\nthe source code of the hierarchical speech synthesizer for a\nstrong zero-shot speech synthesizer. Furthermore, we can\nalso upsample the audio using SpeechSR from 16 kHz to 48\nkHz, which can simply improve the perceptual audio quality\nas described in Section 5.10.\nTABLE 6: Temperature parameter search. For TTS, we have\ntwo controllable temperature, Tttv of TTV and Th of hierar-\nchical speech synthesizer. We utilize a HierSpeech++ trained\nwith LT-960 for this experiment and fix the random seed.\nWe found that low temperatures improve the robustness of\nsynthetic speech in terms of CER and WER. However, if you\nhope to generate diverse and expressive speech, you could\nchoose high temperatures. The CER and WER of ground-\ntruth is 2.31 and 4.13, respectively. UTMOS is presented with\nstandard deviation.\nTttv\nTh\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (\u2191)\n0.333\n0.333\n4.36\u00b10.25\n2.39\n4.20\n0.907\n0.333\n0.500\n4.35\u00b10.25\n2.35\n4.21\n0.908\n0.333\n0.667\n4.34\u00b10.26\n2.40\n4.31\n0.909\n0.333\n1.000\n4.28\u00b10.29\n2.51\n4.51\n0.911\n0.500\n0.333\n4.36\u00b10.24\n2.17\n4.03\n0.907\n0.500\n0.500\n4.35\u00b10.25\n2.42\n4.35\n0.908\n0.500\n0.667\n4.33\u00b10.26\n2.54\n4.52\n0.909\n0.500\n1.000\n4.27\u00b10.29\n2.64\n4.84\n0.911\n0.667\n0.333\n4.35\u00b10.25\n2.53\n4.45\n0.907\n0.667\n0.500\n4.34\u00b10.25\n2.53\n4.43\n0.908\n0.667\n0.667\n4.32\u00b10.26\n2.56\n4.54\n0.909\n0.667\n1.000\n4.25\u00b10.30\n3.06\n5.33\n0.911\n1.000\n0.333\n4.30\u00b10.27\n4.70\n6.73\n0.905\n1.000\n0.500\n4.29\u00b10.27\n3.65\n6.51\n0.906\n1.000\n0.667\n4.27\u00b10.29\n4.19\n6.71\n0.907\n1.000\n1.000\n4.19\u00b10.32\n5.23\n7.91\n0.909\n5.7\nHigh-diversity but High-fidelity Speech Synthesis\nFollowing Glow-TTS [34], speech with different styles can\nbe synthesized by controlling the temperature parameters\nin the TTV and hierarchical speech synthesizer. TABLE 6\nshows that lower temperatures ensure the robustness of\nthe synthetic speech in terms of pronunciation. However,\nthe diversity and speaker similarity can be increased by\ncontrolling the temperature. Specifically, we found that\nincreasing Tttv improved the similarity of prosody, such\nas intonation and pronunciation to target prosody prompts\nand increasing Th improved the similarity of voice style in\nterms of SECS. However, when the value of Tttv is close to\n1, the CER and WER are decreased; therefore, we utilized a\nvalue under 1 for robust speech synthesis. In addition, we\ncan synthesize speech differently with different Gaussian\nnoises, and control the duration by multiplying the duration\nby a specific value.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n12\nTABLE 7: Zero-shot TTS results with noisy prompt on unseen speakers from the test-clean subset of LibriTTS. We synthesize\nall sentences of subset (4,837 samples). For HierSpeech++, we only utilize the text sequences from LibriTTS train-960.\nMethod\nDataset\nnMOS (\u2191)\npMOS (\u2191)\nsMOS (\u2191)\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (\u2191)\nGT\n-\n4.32\u00b10.06\n3.94\u00b10.01\n3.88\u00b10.02\n4.04\u00b10.37\n2.31\n4.13\n-\nYourTTS [7]\nLT-460, VCTK, Other\n3.38\u00b10.07\n2.69\u00b10.06\n3.15\u00b10.06\n3.60\u00b10.42\n6.16\n10.53\n0.831\nHierSpeech [48]\nLT-460, VCTK\n4.02\u00b10.07\n2.71\u00b10.07\n3.35\u00b10.06\n4.05\u00b10.43\n5.21\n8.17\n0.820\nVALL-E-X [78]\nLT-960, Others (>1,739 hours)\n3.50\u00b10.08\n2.75\u00b10.07\n3.27\u00b10.06\n3.45\u00b10.62\n21.52\n29.33\n0.865\nXTTS\nUnknown (>49,000 hours)\n3.32\u00b10.08\n3.57\u00b10.05\n3.50\u00b10.05\n3.38\u00b10.44\n15.93\n18.97\n0.788\nHierSpeech++ (Ours)\nLT-460\n4.56\u00b10.05\n3.35\u00b10.06\n3.70\u00b10.04\n4.35\u00b10.25\n2.71\n4.59\n0.899\nHierSpeech++ (Ours)\nLT-960\n4.55\u00b10.04\n3.31\u00b10.06\n3.74\u00b10.04\n4.36\u00b10.25\n2.39\n4.20\n0.907\nHierSpeech++ (Ours)\nLT-960, LL, Expresso, MSSS, NIKL\n4.50\u00b10.05\n3.31\u00b10.06\n3.72\u00b10.04\n4.37\u00b10.23\n2.19\n3.87\n0.911\nTABLE 8: Zero-shot TTS results with very noisy prompt on unseen speakers from the test-other subset of LibriTTS. We\nsynthesize all sentences of test-other subset (5,120 samples).\nMethod\nDataset\nnMOS (\u2191)\npMOS (\u2191)\nsMOS (\u2191)\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (\u2191)\nGT\n-\n4.15\u00b10.06\n3.89\u00b10.02\n3.89\u00b10.02\n3.46\u00b10.58\n3.37\n7.06\n-\nYourTTS [7]\nLT-460, VCTK, Other\n3.64\u00b10.08\n2.73\u00b10.06\n2.96\u00b10.06\n3.48\u00b10.47\n7.62\n13.40\n0.771\nHierSpeech [48]\nLT-460, VCTK\n4.08\u00b10.07\n2.79\u00b10.06\n3.26\u00b10.06\n3.88\u00b10.45\n6.94\n7.58\n0.783\nVALL-E-X [78]\nLT-960, Others (>1,739 hours)\n3.53\u00b10.08\n2.74\u00b10.07\n3.14\u00b10.06\n3.23\u00b10.58\n24.69\n36.42\n0.839\nXTTS\nUnknown (>49,000 hours)\n3.02\u00b10.09\n3.45\u00b10.06\n3.48\u00b10.05\n2.94\u00b10.56\n35.19\n38.80\n0.748\nHierSpeech++ (Ours)\nLT-460\n4.53\u00b10.05\n3.36\u00b10.05\n3.65\u00b10.04\n4.17\u00b10.34\n3.23\n5.80\n0.871\nHierSpeech++ (Ours)\nLT-960\n4.53\u00b10.05\n3.38\u00b10.05\n3.66\u00b10.04\n4.13\u00b10.37\n2.36\n5.12\n0.885\nHierSpeech++ (Ours)\nLT-960, LL, Expresso, MSSS, NIKL\n4.50\u00b10.05\n3.35\u00b10.06\n3.68\u00b10.04\n4.12\u00b10.38\n2.32\n5.15\n0.887\n5.8\nZero-shot Text-to-Speech\nWe compared the zero-shot TTS performance of Hier-\nSpeech++ with other baselines: 1) YourTTS, VITS-based\nend-to-end TTS model, 2) HierSpeech, an end-to-end TTS\nmodel using hierarchical VAE, 3) VALL-E-X, a neural codec\nlanguage models-based multi-lingual zero-shot TTS model\nand we utilize an unofficial implementation which has an\nimproved audio quality with Vocos decoder, 4) XTTS12,\na TTS product XTTS v1 from Coqui Corp., and XTTS is\nbuilt on a open-source TTS model, TorToise [5] which was\ntrained with unprecedented large-scale speech dataset for\nthe first time. For zero-shot TTS, we utilized a noisy speech\nprompt from the test-clean and test-other subsets of LibriTTS.\nHierSpeech++ synthesizes the speech with Tttv of 0.333 and\nTh of 0.333 in TABLE 7 and 8.\nThe results demonstrate that our model is a strong zero-\nshot TTS model in terms of all subjective and objective\nmetrics. We conducted three MOS experiments for natu-\nralness, prosody, and similarity. Our model beats all models\nsignificantly, and our model has even surpassed the ground-\ntruth in terms of naturalness. However, XTTS has a better\nperformance in pMOS, and this means learning prosody\nrequires more datasets to improve expressiveness. Although\nother models show limitations in synthesizing speech with\nnoisy prompts, our model synthesizes a speech robustly.\nFurthermore, our model has a better CER and WER than\nground-truth, and this also demonstrates the robustness\nof our model. In summary, all results demonstrate the\nsuperiority of our model in naturalness, expressiveness, and\nrobustness for zero-shot TTS.\nIn addition, we could further improve the zero-shot TTS\nperformance by introducing a style prompt replication (SPR)\nin the following subsection. Note that we do not apply the\nSPR in TABLE 2-8. The audio could be upsampled to 48 kHz.\nLastly, we could also synthesize noise-free speech even with\nnoisy speech. The details will be described in Section 6.\n12. https://github.com/coqui-ai/TTS\nTABLE 9: Results on the different length of speech prompt.\nWe utilize all sentence over 10s from the test-clean subset\nof LibriTTS (1,002 samples). SPR denotes style prompt\nreplication and we replicate a short prompt five times for\nrobust style transfer. Because we randomly slice a speech\nwithout considering voice/unvoice part, the results of 1s\nprompts are lower than others. UTMOS is presented with\nstandard deviation.\nModel\nLength\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (\u2191)\nGT\n-\n4.16\u00b10.25\n1.16\n2.32\n-\n1\n4.07\u00b10.42\n6.23\n10.52\n0.842\nHierSpeech++\n3\n4.38\u00b10.13\n1.76\n3.27\n0.914\n(with SPR)\n5\n4.41\u00b10.10\n2.00\n3.60\n0.930\n10\n4.44\u00b10.08\n1.34\n2.72\n0.942\n1\n2.67\u00b11.05\n63.62\n86.66\n0.757\nHierSpeech++\n3\n4.30\u00b10.22\n3.87\n6.87\n0.909\n(without SPR)\n5\n4.40\u00b10.10\n2.59\n4.74\n0.929\n10\n4.44\u00b10.07\n1.78\n3.38\n0.942\n5.9\nZero-shot Text-to-Speech with 1s Prompt\nWe compare the performance of zero-shot TTS according to\ndifferent prompt lengths of 1s, 3s 5s, and 10s. For evaluation,\nwe use all samples over 10s from the test-clean subset of\nLibriTTS (1,002 samples), and we randomly slice a speech\nfor each prompt length. TABLE 9 shows that our model has\na robust style transfer performance using 3s, 5s, and 10s\nprompts. However, using 1s prompt could not synthesize\na speech well. We can discuss two problems: 1) we do\nnot consider an unvoice part during slicing the speech so\nsome prompts contain only a small portion of speech in\ntheir prompt, and we also found that there is no voice part\nin prompts. 2) we utilize a full-length of prompt during\ntraining so synthesizing long sentences may require a long\nspeech prompt for robust speech synthesis, specifically in\nthe prosody encoder. To reduce this problem, we propose\na style prompt replication as in section 4.3, and this style\nprompt replication significantly improves the robustness of\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n13\n(a) GT (24 kHz)\n(b) GT (16 kHz)\n(c) AudioSR\n(d) SpeechSR\n(e) HierSpeech++ (16 kHz)\n(f) HierSpeech++ (+AudioSR)\n(g) HierSpeech++ (+ SpeechSR)\nFig. 8: Spectrograms of GT and speech super-resolution results with AudioSR and SpeechSR (Ours).\nTABLE 10: Results of Speech super-resolution on the VCTK\ndataset.\nModel\nLSD (\u2193)\nLSD-HF (\u2193)\nLSD-LF (\u2193)\nViSQOL (\u2191)\nPESQ (wb/nb) (\u2191)\nResampling\n3.41\n4.17\n0.20\n1.88\n4.64/4.55\nNu-Wave 2 [21]\n1.01\n1.18\n0.48\n2.62\n4.27/4.44\nUDM+ [88]\n1.13\n1.38\n0.17\n2.73\n4.58/4.55\nAudioSR [55]\n1.36\n1.61\n0.56\n2.87\n4.39/4.55\nSpeechSR (Ours)\n0.82\n0.98\n0.30\n3.34\n4.63/4.55\nw.o DWTD\n0.82\n0.98\n0.28\n3.33\n4.62/4.55\nw.o NNU\n0.83\n0.99\n0.26\n3.28\n4.63/4.55\nGT (16 kHz)\nNo Difference\nGT (+ SpeechSR) (48 kHz)\nAudioSR (48 kHz)\nNo Difference\nSpeechSR (48 kHz)\n11.6%\n11.7%\n76.7%\n38.3%\n18.3%\n43.4%\nHierSpeech++ (16 kHz)\nNo Difference\nHierSpeech++ (+SpeechSR) (48 kHz)\n8.3%\n8.3%\n83.4%\nFig. 9: ABX preference test for SpeechSR.\nTTS. By replicating the prompt like DNA replication, we\nsimply extend a style prompt by n\u00d7 and the replicated\nprompt is fed to the style encoder. This simple trick for style\ntransfer significantly improves the robustness and similarity.\nWith HierSpeech++ using SPR, we could synthesize a speech\nwith only 1s speech prompt even in a zero-shot TTS scenario.\n5.10\nSpeech Super-resolution\nWe introduced SpeechSR for a simple and efficient speech\nsuper-resolution for real-world practical application [73].\nBecause we train a target-specific SpeechSR which can\nupsample 16 kHz to 48 kHz, our model shows the best\nperformance even with a simple architecture indicated in\nTABLE 10. For a fair comparison, we trained the model\nwith the VCTK dataset and compare some publicly-available\nsuper-resolution models. Although other models perform\nmulti-task super-resolution, we simply focus on 16-48 kHz\nTABLE 11: Comparison with VALL-E, NaturalSpeech2, and\nStyleTTS 2. We only compared four samples in the demo\npage of NaturalSpeech 2 and StyleTTS 2 so this experiment\nis just for reference.\nModel\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (w. Prompt/GT) (\u2191)\nGT\n4.13\u00b10.20\n1.58\n3.85\n0.851/-\nVall-E\n3.37\u00b10.16\n2.58\n5.10\n0.839/0.849\nNaturalSpeech 2\n3.79\u00b10.10\n0.91\n1.92\n0.822/0.837\nStyleTTS 2\n4.11\u00b10.09\n2.02\n3.46\n0.788/0.776\nHierSpeech++\n4.20\u00b10.07\n0.0\n0.0\n0.867/0.810\nHierSpeech++ w. SPR\n4.26\u00b10.11\n0.0\n0.0\n0.870/0.822\nupsampling for the speech synthesis model. In addition,\nDTW-based discriminators also improve the super-resolution\nperformance. Furthermore, we scale up the dataset for more\nrobust speech super-resolution and we will release the source\ncode and checkpoint which is trained with the large-scale\nhigh-resolution open-source dataset. For the preference test,\nFig 9 shows that the upsampled speech also shows a better\nperformance than the original speech. Furthermore, it is\nworth noting that SpeechSR has a 742\u00d7 faster inference\nspeed than AudioSR (Speech version) and has a 1,986\u00d7\nsmaller parameter size (SpeechSR has only 0.13M parameters\nbut AudioSR has 258.20M parameters). However, we ac-\nknowledge that other models also have a good performance\nand could upsample any input audio with a sampling rate\nof 2 kHz to a high-resolution audio with 48 kHz.\n5.11\nAdditional Experiments with Other Baselines\nWe compared the zero-shot TTS performance of our model\nwith Vall-E, NaturalSpeech 2, and StyleTTS 2. Because\nthere are no official implementations of them, we utilize\nthe demo samples in NaturalSpeech 2 and StyleTTS 2. We\nonly compared four samples for this experiment. We also\nadded the audio samples to the demo pages. For naturalness,\nwe utilized UTMOS and our model shows a significantly\nhigher score than others. We also compared the similarity\nwith prompt and GT. TABLE 11 shows that our model has\nmuch higher similarity with prompts than others. However,\nour model has a lower similarity with GT than others. We\nfound that SECS between prompt and GT also shows a low\nsimilarity in these samples and this means the prompt and\nGT have a slightly different style even from the same speaker.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n14\nTABLE 12: Results on Speech Prompts with Noise Suppres-\nsion. HierSpeech++\u2660 denotes the cascaded denoising results\nof Hierspeech++ after speech synthesis. We only utilize the\ndenoised audio as speech prompt for style encoder to extract\nthe denoised style representation.\nModel\nratiod\nUTMOS (\u2191)\nCER (\u2193)\nWER (\u2193)\nSECS (\u2191)\nGT\n-\n3.46\u00b10.58\n3.37\n7.06\n-\nDenoised GT\n-\n3.40\u00b10.61\n6.91\n10.63\n0.915\nHierSpeech++\n-\n4.12\u00b10.38\n2.32\n5.15\n0.887\nHierSpeech++\u2660\n-\n4.09\u00b10.39\n3.22\n6.28\n0.841\nHierSpeech++\n0.1\n4.15\u00b10.36\n2.73\n5.59\n0.888\n(+Denoised Style)\n0.2\n4.18\u00b10.35\n2.63\n5.72\n0.885\n0.3\n4.20\u00b10.33\n2.42\n5.18\n0.881\n0.4\n4.22\u00b10.32\n2.56\n5.27\n0.874\n0.5\n4.23\u00b10.32\n2.45\n5.10\n0.867\n0.6\n4.24\u00b10.31\n2.56\n5.28\n0.859\n0.7\n4.25\u00b10.31\n2.39\n5.12\n0.852\n0.8\n4.25\u00b10.31\n2.37\n5.16\n0.846\n0.9\n4.25\u00b10.31\n3.66\n6.39\n0.840\n1.0\n4.25\u00b10.32\n3.99\n6.88\n0.834\nIn addition, we only utilize four samples. Meanwhile, we\nthought that the similarity between prompt and generated\nspeech is more important for zero-shot speech synthesis.\n6\nLIMITATION AND QUICK FIX\nAlthough our model improve the zero-shot speech synthesis\nperformance significantly, our model also synthesizes the\nnoisy environmental information from noisy prompt. In this\nwork, we do not disentangle the voice and noise in voice\nmodeling so the model generates a repeated background\nnoise from the global voice style representation. To address\nthis issue, we utilize a denoiser [58] to remove a noisy\nrepresentation in voice style representation. Before fed to\nstyle encoder, the audio is fed to denoiser, and the denoised\naudio is transformed by STFT. Then, the denoised Mel-\nspectrogram is fed to style encoder. TABLE 12 shows that\nthe using denoiser for style encoder simply improve the\naudio quality in terms of UTMOS. However, the denoised\nstyle also decreases the reconstruction quality in terms of\nCER and WER. As the results of denoised GT degraded all\nmetrics, we found that the denoiser we used also removed\nthe speech part. In this case, the pronunciation of synthetic\nspeech also decreases. To reduce this issue, we interpolate the\nstyle representations from the original speech and denoised\nspeech by denoising ratio of ratiod. This simple interpolation\nsignificantly improves the audio quality by removing the\nnoisy environmental information without the decrease of\nCER and WER. For SECS, the denoised speech also shows\nlow SECS and this means that Resemblyzer is also affected\nby environmental information such as reverberation. It is\nworth noting that we only utilize a denoiser for style encoder\nduring inference. Meanwhile, using denoiser on the synthetic\nspeech also degrades performance in terms of all metrics as\nthe results of HierSpeech++\u2660.\n7\nCONCLUSION\nIn this work, we propose HierSpeech++, which achieves\na human-level high-quality zero-shot speech synthesis per-\nformance. We introduce an efficient and powerful speech\nsynthesis framework by disentangling semantic modeling,\nspeech synthesizer, and speech super-resolution. We thor-\noughly analyze the components of our model to demonstrate\nhow to achieve a human-level speech synthesis performance\neven in zero-shot scenarios. Moreover, we simply achieve\nthis performance with a small-scale open-source dataset,\nLibriTTS. In addition, our model has a significantly faster\ninference speed than recently proposed zero-shot speech\nsynthesis models. Furthermore, we introduce a style prompt\nreplication for 1s voice cloning, and noise-free speech syn-\nthesis by adopting a denoised style prompt. Furthermore,\nSpeechSR simply upsamples the audio to 48 kHz for high-\nresolution audio generation. We will release the source code\nand checkpoint of all components including TTV, hierarchical\nspeech synthesizer, and SpeechSR. For future works, we will\nextend the model to cross-lingual and emotion-controllable\nspeech synthesis models by utilizing the pre-trained models\nsuch as [77]. Furthermore, we see that our hierarchical\nspeech synthesis framework could be adopted to a speech-to-\nspeech translation system by introducing non-autoregressive\ngeneration [82].\nACKNOWLEDGMENTS\nWe\u2019d like to thank Hongsun Yang for helpful discussions\nand contributions to our work. This study used an open-\nsource korean speech datasets, NIKL dataset from the NIA\nand multi-speaker speech synthesis (MSSS) dataset from the\nAIHub.\nREFERENCES\n[1]\nA. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,\nK. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau,\nand M. Auli. XLS-R: Self-supervised cross-lingual speech repre-\nsentation learning at scale. In Proc. Interspeech, pages 2278\u20132282,\n2022.\n[2]\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli. wav2vec 2.0: A\nframework for self-supervised learning of speech representations.\nProc. Adv. Neural Inf. Process. Syst., 33:12449\u201312460, 2020.\n[3]\nY. Bengio, A. Courville, and P. Vincent. Representation learning:\nA review and new perspectives. IEEE Trans. Pattern Anal. Mach.\nIntell., 35(8):1798\u20131828, 2013.\n[4]\nM. Bernard and H. Titeux. Phonemizer: Text to phones transcription\nfor multiple languages in python. Journal of Open Source Software,\n6(68):3958, 2021.\n[5]\nJ. Betker. Better speech synthesis through scaling. arXiv preprint\narXiv:2305.07243, 2023.\n[6]\nZ. Borsos, M. Sharifi, D. Vincent, E. Kharitonov, N. Zeghidour, and\nM. Tagliasacchi. Soundstorm: Efficient parallel audio generation.\narXiv preprint arXiv:2305.09636, 2023.\n[7]\nE. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and\nM. A. Ponti. YourTTS: Towards Zero-Shot Multi-Speaker TTS and\nZero-Shot Voice Conversion for everyone. In Proc. Int. Conf. on\nMach. Learn., pages 2709\u20132720. PMLR, 2022.\n[8]\nM. Chen, X. Tan, B. Li, Y. Liu, T. Qin, S. Zhao, and T.-Y. Liu.\nAdaspeech: Adaptive text to speech for custom voice. arXiv preprint\narXiv:2103.00993, 2021.\n[9]\nH.-S. Choi, J. Yang, J. Lee, and H. Kim.\nNansy++: Unified\nvoice synthesis with neural analysis and synthesis. arXiv preprint\narXiv:2211.09407, 2022.\n[10] H.-Y. Choi, S.-H. Lee, and S.-W. Lee.\nDddm-vc: Decoupled\ndenoising diffusion models with disentangled representation and\nprior mixup for verified robust voice conversion. arXiv preprint\narXiv:2305.15816, 2023.\n[11] H.-Y. Choi, S.-H. Lee, and S.-W. Lee.\nDiff-HierVC: Diffusion-\nbased Hierarchical Voice Conversion with Robust Pitch Generation\nand Masked Prior for Zero-shot Speaker Adaptation.\nIn Proc.\nINTERSPEECH, pages 2283\u20132287, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n15\n[12] H. Chung, S.-H. Lee, and S.-W. Lee. Reinforce-Aligner: Reinforce-\nment Alignment Search for Robust End-to-End Text-to-Speech. In\nProc. Interspeech, pages 3635\u20133639, 2021.\n[13] J. S. Chung, J. Huh, S. Mun, M. Lee, H.-S. Heo, S. Choe, C. Ham,\nS. Jung, B.-J. Lee, and I. Han. In Defence of Metric Learning for\nSpeaker Recognition. In Proc. Interspeech 2020, pages 2977\u20132981,\n2020.\n[14] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker\nrecognition. In Proc. Interspeech, pages 1086\u20131090, 2018.\n[15] A. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural\naudio compression. arXiv preprint arXiv:2210.13438, 2022.\n[16] A. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi. High fidelity neural\naudio compression. Trans. Mach. Learn. Research, 2023.\n[17] C. Du, Y. Guo, X. Chen, and K. Yu. Vqtts: High-fidelity text-to-\nspeech synthesis with self-supervised vq acoustic feature. arXiv\npreprint arXiv:2204.00768, 2022.\n[18] C. Du, Y. Guo, X. Chen, and K. Yu. Speaker adaptive text-to-speech\nwith timbre-normalized vector-quantized feature. IEEE/ACM Trans.\nAudio, Speech, Lang. Process., pages 1\u201312, 2023.\n[19] Z. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan. Prompttts: Controllable\ntext-to-speech with text descriptions. In IEEE Int. Conf. Acoust.,\nSpeech, Signal Process., pages 1\u20135. IEEE, 2023.\n[20] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu, Z. Yang, Y. Zhang, and D. Tao. A survey on vision\ntransformer. IEEE Trans. Pattern Anal. Mach. Intell., 45(1):87\u2013110,\n2023.\n[21] S. Han and J. Lee. NU-Wave 2: A General Neural Audio Upsam-\npling Model for Various Sampling Rates. In Proc. Interspeech, pages\n4401\u20134405, 2022.\n[22] R. Huang, Y. Ren, J. Liu, C. Cui, and Z. Zhao. Generspeech: Towards\nstyle transfer for generalizable out-of-domain text-to-speech. In\nProc. Adv. Neural Inf. Process. Syst., 2022.\n[23] R. Huang, C. Zhang, Y. Wang, D. Yang, L. Liu, Z. Ye, Z. Jiang,\nC. Weng, Z. Zhao, and D. Yu. Make-a-voice: Unified voice synthesis\nwith discrete representation. arXiv preprint arXiv:2305.19269, 2023.\n[24] J.-S. Hwang, S.-H. Lee, and S.-W. Lee. Hiddensinger: High-quality\nsinging voice synthesis via neural audio codec and latent diffusion\nmodels. arXiv preprint arXiv:2306.06814, 2023.\n[25] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen,\nR. Pang, I. Lopez Moreno, Y. Wu, et al. Transfer learning from\nspeaker verification to multispeaker text-to-speech synthesis. Proc.\nAdv. Neural Inf. Process. Syst., 31, 2018.\n[26] Z. Jiang, J. Liu, Y. Ren, J. He, C. Zhang, Z. Ye, P. Wei, C. Wang,\nX. Yin, Z. Ma, et al. Mega-tts 2: Zero-shot text-to-speech with\narbitrary length speech prompts. arXiv preprint arXiv:2307.07218,\n2023.\n[27] J. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar\u00e9,\nJ. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhoma-\nnenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.\nLibri-light: A benchmark for asr with limited or no supervision. In\nICASSP 2020 - 2020 IEEE Int. Conf. on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 7669\u20137673, 2020.\n[28] H. Kameoka, W.-C. Huang, K. Tanaka, T. Kaneko, N. Hojo, and\nT. Toda. Many-to-many voice transformer network. IEEE/ACM\nTrans. Audio, Speech, Lang. Process., 29:656\u2013670, 2021.\n[29] M. Kang, D. Min, and S. J. Hwang. Grad-stylespeech: Any-speaker\nadaptive text-to-speech synthesis with diffusion models. In IEEE\nInt. Conf. Acoust., Speech, Signal Process., pages 1\u20135. IEEE, 2023.\n[30] K. Kasi and S. A. Zahorian. Yet another algorithm for pitch tracking.\nIn IEEE Int. Conf. Acoust., Speech, Signal Process., volume 1, pages\nI\u2013361, 2002.\n[31] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin,\nO. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour. Speak,\nread and prompt: High-fidelity text-to-speech with minimal super-\nvision. arXiv preprint arXiv:2302.03540, 2023.\n[32] H. Kim, S. Kim, J. Yeom, and S. Yoon.\nUnitspeech: Speaker-\nadaptive speech synthesis with untranscribed data. arXiv preprint\narXiv:2306.16083, 2023.\n[33] H. Kim, S. Kim, and S. Yoon. Guided-TTS: A diffusion model for\ntext-to-speech via classifier guidance. In Proc. Int. Conf. on Mach.\nLearn., pages 11119\u201311133, 2022.\n[34] J. Kim, S. Kim, J. Kong, and S. Yoon. Glow-TTS: A generative\nflow for text-to-speech via monotonic alignment search. Proc. Adv.\nNeural Inf. Process. Syst., 33:8067\u20138077, 2020.\n[35] J. Kim, J. Kong, and J. Son. Conditional variational autoencoder\nwith adversarial learning for end-to-end text-to-speech. In Proc. Int.\nConf. on Mach. Learn., pages 5530\u20135540. PMLR, 2021.\n[36] J.-H. Kim, S.-H. Lee, J.-H. Lee, and S.-W. Lee. Fre-GAN: Adversarial\nFrequency-Consistent Audio Synthesis. In Proc. Interspeech, pages\n2197\u20132201, 2021.\n[37] M. Kim, M. Jeong, B. J. Choi, S. Ahn, J. Y. Lee, and N. S. Kim.\nTransfer Learning Framework for Low-Resource Text-to-Speech\nusing a Large-Scale Unlabeled Speech Corpus. In Proc. Interspeech,\npages 788\u2013792, 2022.\n[38] S. Kim, H. Kim, and S. Yoon. Guided-TTS 2: A diffusion model\nfor high-quality adaptive text-to-speech with untranscribed data.\narXiv preprint arXiv:2205.15370, 2022.\n[39] S. Kim, K. J. Shih, R. Badlani, J. F. Santos, E. Bakhturina, M. T. Desta,\nR. Valle, S. Yoon, and B. Catanzaro. P-flow: A fast and data-efficient\nzero-shot TTS through speech prompting. In Proc. Adv. Neural Inf.\nProcess. Syst., 2023.\n[40] J. Kong, J. Kim, and J. Bae. HiFi-GAN: Generative adversarial\nnetworks for efficient and high fidelity speech synthesis. Proc. Adv.\nNeural Inf. Process. Syst., 33:17022\u201317033, 2020.\n[41] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar.\nHigh-fidelity audio compression with improved RVQGAN. In Proc.\nAdv. Neural Inf. Process. Syst., 2023.\n[42] Y. Kwon, H. S. Heo, B.-J. Lee, and J. S. Chung. The ins and outs of\nspeaker recognition: lessons from VoxSRC 2020. In IEEE Int. Conf.\nAcoust., Speech, Signal Process., 2021.\n[43] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson,\nV. Manohar, Y. Adi, J. Mahadeokar, and W.-N. Hsu. Voicebox:\nText-guided multilingual universal speech generation at scale. In\nProc. Adv. Neural Inf. Process. Syst., 2023.\n[44] J.-H. Lee, S.-H. Lee, J.-H. Kim, and S.-W. Lee. Pvae-tts: Adaptive\ntext-to-speech via progressive style adaptation. In IEEE Int. Conf.\nAcoust., Speech, Signal Process., pages 6312\u20136316. IEEE, 2022.\n[45] S.-H. Lee, H.-Y. Choi, H.-S. Oh, and S.-W. Lee. HierVST: Hierar-\nchical Adaptive Zero-shot Voice Style Transfer. In Proc. Interspeech,\npages 4439\u20134443, 2023.\n[46] S.-H. Lee, J.-H. Kim, H. Chung, and S.-W. Lee.\nVoiceMixer:\nAdversarial voice style mixup. Proc. Adv. Neural Inf. Process. Syst.,\n34:294\u2013308, 2021.\n[47] S.-H. Lee, J.-H. Kim, K.-E. Lee, and S.-W. Lee. Fre-gan 2: Fast and\nefficient frequency-consistent audio synthesis. In IEEE Int. Conf.\nAcoust., Speech, Signal Process., pages 6192\u20136196, 2022.\n[48] S.-H. Lee, S.-B. Kim, J.-H. Lee, E. Song, M.-J. Hwang, and S.-W. Lee.\nHierSpeech: Bridging the gap between text and speech by hierar-\nchical variational inference using self-supervised representations\nfor speech synthesis. In Proc. Adv. Neural Inf. Process. Syst., 2022.\n[49] S.-H. Lee, H.-R. Noh, W.-J. Nam, and S.-W. Lee. Duration control-\nlable voice conversion via phoneme-based information bottleneck.\nIEEE/ACM Trans. Audio, Speech, Lang. Process., 30:1173\u20131183, 2022.\n[50] S.-H. Lee, H.-W. Yoon, H.-R. Noh, J.-H. Kim, and S.-W. Lee.\nMulti-spectrogan: High-diversity and high-fidelity spectrogram\ngeneration with adversarial style combination for speech synthesis.\nIn Proc. AAAI Conf. Artif. Intell., volume 35, pages 13198\u201313206,\n2021.\n[51] Y. Lee and T. Kim. Robust and fine-grained prosody control of\nend-to-end speech synthesis. In IEEE Int. Conf. Acoust., Speech,\nSignal Process., pages 5911\u20135915. IEEE, 2019.\n[52] Y. Leng, Z. Guo, K. Shen, X. Tan, Z. Ju, Y. Liu, Y. Liu, D. Yang,\nL. Zhang, K. Song, et al. Prompttts 2: Describing and generating\nvoices with text prompt. arXiv preprint arXiv:2309.02285, 2023.\n[53] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu. Neural speech synthesis\nwith transformer network.\nIn Proc. AAAI Conf. Artif. Intell.,\nvolume 33, pages 6706\u20136713, 2019.\n[54] Y. A. Li, C. Han, V. S. Raghavan, G. Mischler, and N. Mesgarani.\nStyletts 2: Towards human-level text-to-speech through style\ndiffusion and adversarial training with large speech language\nmodels. arXiv preprint arXiv:2306.07691, 2023.\n[55] H. Liu, K. Chen, Q. Tian, W. Wang, and M. D. Plumbley. Au-\ndiosr: Versatile audio super-resolution at scale.\narXiv preprint\narXiv:2309.07314, 2023.\n[56] S. Liu, Y. Cao, D. Wang, X. Wu, X. Liu, and H. Meng. Any-to-\nmany voice conversion with location-relative sequence-to-sequence\nmodeling. IEEE/ACM Trans. Audio, Speech, Lang. Process., 29:1717\u2013\n1728, 2021.\n[57] I. Loshchilov and F. Hutter. Decoupled weight decay regularization.\nIn Proc. Int. Conf. Learn. Representations, 2019.\n[58] Y.-X. Lu, Y. Ai, and Z.-H. Ling. MP-SENet: A Speech Enhancement\nModel with Parallel Denoising of Magnitude and Phase Spectra. In\nProc. INTERSPEECH, pages 3834\u20133838, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. XX, NO. X, OCTOBER 2023\n16\n[59] D. Min, D. B. Lee, E. Yang, and S. J. Hwang. Meta-stylespeech:\nMulti-speaker adaptive text-to-speech generation. In Proc. Int. Conf.\non Mach. Learn., pages 7748\u20137759. PMLR, 2021.\n[60] M. Morrison, R. Kumar, K. Kumar, P. Seetharaman, A. Courville,\nand Y. Bengio.\nChunked autoregressive GAN for conditional\nwaveform synthesis. In Proc. Int. Conf. Learn. Representations, 2022.\n[61] T. A. Nguyen, W.-N. Hsu, A. D\u2019Avirro, B. Shi, I. Gat, M. Fazel-\nZarani, T. Remez, J. Copet, G. Synnaeve, M. Hassid, F. Kreuk,\nY. Adi, and E. Dupoux. Expresso: A Benchmark and Analysis of\nDiscrete Expressive Speech Resynthesis. In Proc. INTERSPEECH,\npages 4823\u20134827, 2023.\n[62] W. Peebles and S. Xie. Scalable diffusion models with transformers.\nIn Proc. of the IEEE/CVF Int. Conf. on Computer Vision, pages 4195\u2013\n4205, 2023.\n[63] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov.\nGrad-TTS: A diffusion probabilistic model for text-to-speech. In\nProc. Int. Conf. on Mach. Learn., pages 8599\u20138608, 2021.\n[64] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, M. S. Kudinov, and\nJ. Wei.\nDiffusion-based voice conversion with fast maximum\nlikelihood sampling scheme. In Proc. Int. Conf. Learn. Representations,\n2022.\n[65] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu,\nA. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi, et al. Scaling speech\ntechnology to 1,000+ languages. arXiv preprint arXiv:2305.13516,\n2023.\n[66] K. Qian, Y. Zhang, S. Chang, X. Yang, and M. Hasegawa-Johnson.\nAutoVC: Zero-shot voice style transfer with only autoencoder loss.\nIn Proc. Int. Conf. on Mach. Learn., pages 5210\u20135219, 2019.\n[67] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu.\nFastspeech 2: Fast and high-quality end-to-end text to speech.\narXiv preprint arXiv:2006.04558, 2020.\n[68] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu.\nFastspeech: Fast, robust and controllable text to speech. Proc. Adv.\nNeural Inf. Process. Syst., 32, 2019.\n[69] T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and\nH. Saruwatari. UTMOS: UTokyo-SaruLab System for VoiceMOS\nChallenge 2022. In Proc. Interspeech 2022, pages 4521\u20134525, 2022.\n[70] K. Shen, Z. Ju, X. Tan, Y. Liu, Y. Leng, L. He, T. Qin, S. Zhao,\nand J. Bian. Naturalspeech 2: Latent diffusion models are natural\nand zero-shot speech and singing synthesizers.\narXiv preprint\narXiv:2304.09116, 2023.\n[71] H. Siuzdak, P. Dura, P. van Rijn, and N. Jacoby. Wavthruvec: Latent\nspeech representation as intermediate features for neural speech\nsynthesis. arXiv preprint arXiv:2203.16930, 2022.\n[72] R. Skerry-Ryan, E. Battenberg, Y. Xiao, Y. Wang, D. Stanton, J. Shor,\nR. Weiss, R. Clark, and R. A. Saurous. Towards end-to-end prosody\ntransfer for expressive speech synthesis with tacotron. In Proc. Int.\nConf. on Mach. Learn., pages 4693\u20134702. PMLR, 2018.\n[73] S. Son, J. Kim, W.-S. Lai, M.-H. Yang, and K. M. Lee. Toward\nreal-world super-resolution via adaptive downsampling models.\nIEEE Trans. Pattern Anal. Mach. Intell., 44(11):8657\u20138670, 2022.\n[74] H. Sun, D. Wang, L. Li, C. Chen, and T. F. Zheng. Random cycle\nloss and its application to voice conversion. IEEE Trans. Pattern\nAnal. Mach. Intell., 45(8):10331\u201310345, 2023.\n[75] X. Tan, J. Chen, H. Liu, J. Cong, C. Zhang, Y. Liu, X. Wang, Y. Leng,\nY. Yi, L. He, et al.\nNaturalspeech: End-to-end text to speech\nsynthesis with human-level quality. arXiv preprint arXiv:2205.04421,\n2022.\n[76] C. Veaux, J. Yamagishi, K. MacDonald, et al. Superseded-CSTR\nVCTK corpus: English multi-speaker corpus for CSTR voice cloning\ntoolkit. 2017.\n[77] J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt,\nF. Burkhardt, F. Eyben, and B. W. Schuller. Dawn of the transformer\nera in speech emotion recognition: Closing the valence gap. IEEE\nTrans. Pattern Anal. Mach. Intell., 45(9):10745\u201310759, 2023.\n[78] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu,\nH. Wang, J. Li, et al. Neural codec language models are zero-shot\ntext to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023.\n[79] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly,\nZ. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis,\nR. Clark, and R. A. Saurous. Tacotron: Towards End-to-End Speech\nSynthesis. In Proc. Interspeech, pages 4006\u20134010, 2017.\n[80] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor,\nY. Xiao, Y. Jia, F. Ren, and R. A. Saurous. Style tokens: Unsupervised\nstyle modeling, control and transfer in end-to-end speech synthesis.\nIn Proc. Int. Conf. on Mach. Learn., pages 5180\u20135189. PMLR, 2018.\n[81] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata.\nZero-shot\nlearning\u2014a comprehensive evaluation of the good, the bad and the\nugly. IEEE Trans. Pattern Anal. Mach. Intell., 41(9):2251\u20132265, 2019.\n[82] Y. Xiao, L. Wu, J. Guo, J. Li, M. Zhang, T. Qin, and T.-Y. Liu.\nA survey on non-autoregressive generation for neural machine\ntranslation and beyond. IEEE Trans. Pattern Anal. Mach. Intell.,\n45(10):11407\u201311427, 2023.\n[83] H. Xue, S. Guo, P. Zhu, and M. Bi. Multi-gradspeech: Towards\ndiffusion-based multi-speaker text-to-speech using consistent diffu-\nsion models. arXiv preprint arXiv:2308.10428, 2023.\n[84] D. Yang, S. Liu, R. Huang, G. Lei, C. Weng, H. Meng, and D. Yu.\nInstructtts: Modelling expressive tts in discrete latent space with\nnatural language style prompt. arXiv preprint arXiv:2301.13662,\n2023.\n[85] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y. Zou. Hifi-codec:\nGroup-residual vector quantization for high fidelity audio codec.\narXiv preprint arXiv:2305.02765, 2023.\n[86] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao,\nJ. Bian, X. Wu, et al. Uniaudio: An audio foundation model toward\nuniversal audio generation. arXiv preprint arXiv:2310.00704, 2023.\n[87] Z. Ye, W. Xue, X. Tan, J. Chen, Q. Liu, and Y. Guo. Comospeech:\nOne-step speech and singing voice synthesis via consistency model.\narXiv preprint arXiv:2305.06908, 2023.\n[88] C.-Y. Yu, S.-L. Yeh, G. Fazekas, and H. Tang.\nConditioning\nand sampling in variational diffusion models for speech super-\nresolution. In IEEE Int. Conf. Acoust., Speech, Signal Process., pages\n1\u20135, 2023.\n[89] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasac-\nchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM\nTrans. Audio, Speech, Lang. Process., 30:495\u2013507, 2021.\n[90] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen,\nand Y. Wu. LibriTTS: A Corpus Derived from LibriSpeech for\nText-to-Speech. pages 1526\u20131530, 2019.\n"
  },
  {
    "title": "NeuroPrompts: An Adaptive Framework to Optimize Prompts for Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2311.12229.pdf",
    "upvote": "24",
    "text": "NeuroPrompts: An Adaptive Framework to Optimize Prompts for\nText-to-Image Generation\nShachar Rosenman\nVasudev Lal\nPhillip Howard\nIntel Labs\n{shachar.rosenman,vasudev.lal,phillip.r.howard}@intel.com\nAbstract\nDespite impressive recent advances in text-to-\nimage diffusion models, obtaining high-quality\nimages often requires prompt engineering by\nhumans who have developed expertise in using\nthem. In this work, we present NeuroPrompts,\nan adaptive framework that automatically en-\nhances a user\u2019s prompt to improve the quality\nof generations produced by text-to-image mod-\nels. Our framework utilizes constrained text de-\ncoding with a pre-trained language model that\nhas been adapted to generate prompts similar\nto those produced by human prompt engineers.\nThis approach enables higher-quality text-to-\nimage generations and provides user control\nover stylistic features via constraint set specifi-\ncation. We demonstrate the utility of our frame-\nwork by creating an interactive application for\nprompt enhancement and image generation us-\ning Stable Diffusion. Additionally, we conduct\nexperiments utilizing a large dataset of human-\nengineered prompts for text-to-image genera-\ntion and show that our approach automatically\nproduces enhanced prompts that result in su-\nperior image quality. We make our code1 and\na screencast video demo2 of NeuroPrompts\npublicly available.\n1\nIntroduction\nText-to-image generation has recently become in-\ncreasingly popular as advances in latent diffusion\nmodels have enabled widespread use. However,\nthese models are sensitive to perturbations of the\nprompt used to describe the desired image, mo-\ntivating the development of prompt engineering\nexpertise by users to increase the quality of the\nresulting images generated by the model.\nPrompt design is crucial in ensuring that the\nmodel accurately comprehends the user\u2019s intent.\nText-to-image models face a significant challenge\n1https://github.com/IntelLabs/multimodal_\ncognitive_ai/tree/main/Demos/NeuroPrompts\n2https://youtu.be/Cmca_RWYn2g\nin this aspect as their text encoders have limited\ncapacity, which can make it difficult to produce\naesthetically pleasing images. Additionally, as em-\npirical studies have shown, common user input\nmay not be enough to produce satisfactory results.\nTherefore, developing innovative techniques to op-\ntimize prompt design for these models is crucial to\nimproving their generation quality.\nTo address this challenge, we introduce Neuro-\nPrompts, a novel framework which automatically\noptimizes user-provided prompts for text-to-image\ngeneration models. A key advantage of our frame-\nwork is its ability to automatically adapt a user\u2019s\nnatural description of an image to the prompting\nstyle which optimizes the quality of generations\nproduced by diffusion models. We achieve this\nautomatic adaptation through the use of a language\nmodel trained with Proximal Policy Optimization\n(PPO) (Schulman et al., 2017) to generate text in\nthe style commonly used by human prompt engi-\nneers. This results in higher quality images which\nare more aesthetically pleasing, as the prompts are\nautomatically optimized for the diffusion model.\nFurthermore, our approach allows the user to main-\ntain creative control over the prompt enhancement\nprocess via constrained generation with Neurologic\nDecoding (Lu et al., 2021b), which enables more\npersonalized and diverse image generations.\nOur NeuroPrompts framework is integrated\nwith Stable Diffusion in an interactive applica-\ntion for text-to-image generation. Given a user-\nprovided prompt, our application automatically op-\ntimizes it similar to expert human prompt engi-\nneers, while also providing an interface to control\nattributes such as style, format, and artistic sim-\nilarity. The optimized prompt produced by our\nframework is then used to generate an image with\nStable Diffusion, which is presented to the user\nalong with the optimized prompt.\nWe validate the effectiveness of NeuroPrompts\nby using our framework to produce optimized\narXiv:2311.12229v1  [cs.AI]  20 Nov 2023\nprompts and images for over 100k baseline\nprompts. Through automated evaluation, we show\nthat our optimized prompts produce images with\nsignificantly higher aesthetics than un-optimized\nbaseline prompts.\nThe optimized prompts pro-\nduced by our approach even outperform those cre-\nated by human prompt engineers, demonstrating\nthe ability of our application to unlock the full po-\ntential of text-to-image generation models to users\nwithout any expertise in prompt engineering.\n2\nNeuroPrompts Framework\nGiven an un-optimized prompt provided by a user,\nwhich we denote as xu, our NeuroPrompts frame-\nwork generates an optimized prompt xo to increase\nthe likelihood that text-to-image diffusion mod-\nels produce an aesthetically-pleasing image when\nprompted with xo. We specifically consider the\ncase where xu is the prefix of xo and produce the\nenhanced prompt via a two-stage approach. First,\nwe adapt a language model (LM) to produce a text\nwhich is steered towards the style of prompts pro-\nduced by human prompt engineers. We then gener-\nate enhanced prompts via our steered LM using a\nconstrained text decoding algorithm (NeuroLogic),\nwhich enables user customizability and improves\nthe coverage of image enhancement keywords.\n2.1\nLM Adaptation for Prompt Enhancement\nTo adapt LMs for prompt engineering, we use a\ncombination of supervised fine-tuning followed by\nreinforcement learning via the PPO algorithm.\n2.1.1\nSupervised fine-tuning (SFT)\nFirst, we fine-tune a pre-trained LM to adapt the\nLM\u2019s generated text to the style of language com-\nmonly used by human prompt engineers. We use a\npre-trained GPT-2 LM throughout this work due to\nits demonstrated exceptional performance in natu-\nral language processing tasks. However, our frame-\nwork is broadly compatible with any autoregressive\nLM. To fine-tune the LM, we use a large corpus of\nhuman-created prompts for text-to-image models,\nwhich we describe subsequently in Section 3.1.\n2.1.2\nReinforcement Learning via PPO\nFollowing SFT, we further train our LM by for-\nmulating a reward model based on predicted hu-\nman preferences of images generated by enhanced\nprompts. We then use our reward model to further\ntrain the LM via the PPO algorithm.\nExtracting prefixes from human prompts\nIn\norder to emulate the type of prompts that a non-\nexpert user might enter into our application for\nenhancement, we created a dataset of un-optimized\nprompts which is derived from human-authored\nprompts. Human prompt engineers commonly opti-\nmize prompts by adding a comma-separated list of\nkeywords describing artists, styles, vibes, and other\nartistic attributes at the end of the prompt. Thus,\nwe truncate each of the human-authored prompts\nin our training dataset to contain only the substring\nprior to the first occurrence of a comma. We refer\nto the resulting prompts as prefixes.\nImage generation with Stable Diffusion\nLet xu\nhereafter denote a prompt prefix, which we utilize\nas a proxy for an un-optimized prompt provided\nby a user. For each xu derived from our train-\ning dataset, we create a corresponding optimized\nprompt xo using our SFT-trained LM. Given the\nprefix, the SFT model generates a continuation of\nit, leveraging the prompt distribution it has learned\nfrom the training dataset (e.g., incorporating modi-\nfiers). We employ beam search with a beam size of\n8 and a length penalty of 1.0 for this stage of SFT.\nWe then use Stable Diffusion to generate images\nyu and yo for prompts xu and xo, respectively.\nReward modeling (RM)\nWe evaluate the effec-\ntiveness of our SFT LM at optimizing prompts\nusing PickScore (Lu et al., 2021b), a text-image\nscoring function for predicting user preferences.\nPickScore was trained on the Pick-a-Pic dataset,\nwhich contains over 500k text-to-image prompts,\ngenerated images, and user-labeled preferences.\nPickScore utilizes the architecture of CLIP;\ngiven a prompt x and an image y, the scoring func-\ntion s computes a d-dimensional vector represen-\ntation of x and y using a text and image decoder\n(respectively), returning their inner product:\ngpick(x, y) = Etxt(x) \u00b7 Eimg(y)T\n(1)\nwhere gpick(x, y) denotes the score of the quality of\na generated image y given the prompt x. A higher\nPickScore indicates a greater likelihood that a user\nwill prefer image y for prompt x.\nReinforcement learning (RL)\nWe further train\nour LM using PPO (Schulman et al., 2017). Given\nthe images generated previously for the optimized\nprompt and prompt prefix, we use PPO to optimize\nthe reward determined by the PickScore:\nR (x, y) = E(x,yu,yo)\u223cD[gpick (x, yo) \u2212 gpick (x, yu)]\nwhere gpick(x, y) is the scalar output of the\nPickScore model for prompt x and image y, yu is\nthe image generated from the un-optimized prompt,\nyo is the image generated from the optimized\nprompt, and D is the dataset. This phase of train-\ning with PPO further adapts the LM by taking into\nconsideration the predicted human preferences for\nimages generated by the optimized prompts.\n2.2\nConstrained Decoding via NeuroLogic\nAfter training our LM via SFT and PPO, we gener-\nate enhanced prompts from it at inference time us-\ning NeuroLogic Decoding (Lu et al., 2021b). Neu-\nroLogic is a constrained text decoding algorithm\nthat enables control over the output of autoregres-\nsive LMs via lexical constraints. Specifically, Neu-\nroLogic generates text satisfying a set of clauses\n{Ci | i \u2208 1, \u00b7 \u00b7 \u00b7 m} consisting of one or more pred-\nicates specified in conjunctive normal form:\n(D1 \u2228 D2 \u00b7 \u00b7 \u00b7 \u2228 Di)\n|\n{z\n}\nC1\n\u2227 \u00b7 \u00b7 \u00b7\u2227(Dk \u2228 Dk+1 \u00b7 \u00b7 \u00b7 \u2228 Dn)\n|\n{z\n}\nCm\nwhere Di is a predicate representing a constraint\nD(ai, y) which evaluates as true if the subsequence\nai appears in the generated sequence y. Neuro-\nLogic also supports negation of predicates (i.e.,\n\u00acDi), specifying the minimum and/or maximum\nnumber of predicates within a clause which can\nbe used to satisfy it, and enforcement of clause\nsatisfaction order (Howard et al., 2023).\nWe use a curated set of prompt enhancement key-\nwords3 to formulate clauses which must be satisfied\nin the optimized prompt. Specifically, we create six\nclauses consisting of keywords for styles, artists,\nformats, perspectives, boosters, and vibes (see Ta-\nble 3 of Appendix A.2 for details). Each clause is\nsatisfied when the generated sequence contains one\nof the keywords from each category. By default, a\nclause contains five randomly sampled keywords\nfrom its corresponding category. However, our ap-\nplication allows users to manually specify which\nkeywords can satisfy each clause to provide more\nfine-grained control over the optimized prompt.\n3\nExperiments\n3.1\nDataset\nFor supervised fine-tuning and reinforcement learn-\ning, we utilize the DiffusionDB dataset (Wang et al.,\n2022), a large dataset of human-created prompts.\n3From prompt engineering templates\nModel\nAesthetics Score\nOriginal prefix\n5.64\nOriginal (human) prompt\n5.92\nSFT only\n6.02\nNeuroPrompts w/o PPO\n6.05\nNeuroPrompts w/o NeuroLogic\n6.22\nNeuroPrompts\n6.27\nTable 1: Aesthetics scores calculated for images gener-\nated by NeuroPrompts and baseline methods\nIn the reinforcement learning stage, we truncate the\nprompt to contain only the substring before the first\noccurrence of a comma, as previously described in\nSection 2.1.2. This allows for improved exploration\nof paraphrasing (see App. A.1 for details).\n3.2\nExperimental setting\nTo adapt GPT-2 to the style of prompts created by\nhuman prompt engineering, we train it on 600k\nprompts sampled from DiffusionDB. Specifically,\nwe fine-tune the model for 15,000 steps with a\nlearning rate of 5e-5 and batch size of 256. We\nthen further train our SFT LM with PPO for 10k\nepisodes using a batch size of 128, a minibatch size\nof one, four PPO epochs per batch, and a constant\nlearning rate of 5e-5. We used a value loss coef-\nficient of 0.1 and a KL reward coefficient of 0.2.\nThis stage of training was conducted using the PPO\nimplementation from (von Werra et al., 2020).\nWe use two metrics to evaluate the benefits of\nour prompt adaptation for text-to-image models:\naesthetics score and PickScore. Aesthetics score\nis a measure of the overall quality of the gener-\nated image and is computed by a model4 trained on\nLAION (Schuhmann et al., 2022) which predicts\nthe likelihood that a human would find the image\naesthetically pleasing. As detailed in Section 2.1.2,\nPickScore measures how likely a human would\nprefer the generated image using a fine-tuned clip\nmodel. We use a different set of 100k prompts\n(non-overlapping with our 600k training set) sam-\npled from DiffusionDB for this evaluation and com-\npare the performance of our prompt optimization\nmethod to three baselines: (1) the original human-\nauthored prompt from DiffusionDB; (2) the prefix\nextracted from human-authored prompts, which we\nconsider a proxy for user-provided prompts; and (3)\nprompts enhanced only using our LM trained with\nsupervised fine-tuning (i.e., without PPO training).\n4We use Improved Aesthetic Predictor\nFigure 1: The interface of NeuroPrompts in side-by-side comparison mode\n3.3\nResults\nOptimized prompts produce images with higher\naesthetics score\nTable 1 provides the mean aes-\nthetic scores of images produced by our optimized\nprompts as well as other baseline methods. Neuro-\nPrompts outperforms all other baselines, achieving\nan average aesthetics score of 6.27, which is an ab-\nsolute improvement of 0.63 over images produced\nby un-optimized prompt prefixes. NeuroPrompts\neven outperform human-authored prompts by a\nmargin of 0.35. These results demonstrate our\nframework\u2019s effectiveness at generating prompts\nthat produce aesthetically pleasing images.\nTo analyze the impact of different components\nof our framework, Table 1 provides results for vari-\nations without PPO training and constrained de-\ncoding. PPO training significantly outperforms ap-\nproaches that only utilize our SFT LM, improving\nthe aesthetics score by approximately 0.2 points.\nConstrained decoding with NeuroLogic further im-\nproves the aesthetics of our PPO-trained model by\n0.05, which could be attributed to greater cover-\nage of prompt enhancement keywords. Beyond\nimprovements in aesthetics score, NeuroLogic also\nenables user control over prompt enhancement.\nOptimized prompts achieve higher PickScores\nWe further investigated the effect of Neuro-\nPrompts on the predicted PickScore of generated\nimages. Specifically, for each prompt in our Diffu-\nsionDB evaluation set, we calculated the PickScore\nusing images generated for the prompt prefix and\nour optimized prompt. Our optimized prompts con-\nsistently achieve a higher PickScore than prompt\nprefixes, with NeuroPrompts having an average\nPickScore of 60%. This corresponds a 20% ab-\nsolute improvement in the predicted likelihood of\nhuman preference for our optimized images rela-\ntive to those produced by prompt prefixes.\nDiscussion\nOur experiments demonstrate that\nNeuroPrompts\nconsistently\nproduce\nhigher-\nquality images, indicating that our framework can\nbe used as a practical tool for artists, designers,\nand other creative professionals to generate high-\nquality and personalized images without requiring\nspecialized prompt engineering expertise.\n4\nNeuroPrompts\nThe user interface of NeuroPrompts is depicted in\nFigure 1. The application\u2019s inputs include the ini-\ntial prompt as well as selection fields for specifying\nthe clauses used to populate constraints for style,\nartist, format, booster, perspective, and vibe. Ad-\nditionally, a negative constraints input allows the\nuser to specify one or more phrases which should\nbe excluded from the optimized prompt. While the\ninitial prompt is required, all other fields are op-\ntional; if left unselected, clauses for each constraint\nset will be automatically populated as described\npreviously in Section 2.2. This functionality al-\nlows the user to take control of the constrained\ngeneration process if desired or simply rely on our\nframework to optimize the prompt automatically.\nAfter clicking the submit button, the optimized\nprompt is displayed at the top of the screen. If\nconstraints were selected by the user, the optimized\nprompt will appear with color-coded highlighting\nto show where each constraint has been satisfied in\nthe generated sequence. The image produced by\nStable Diffusion for the optimized prompt is dis-\nplayed directly below the optimized prompt in the\ncenter of the interface. If the user selects the side-\nby-side comparison tab, an image generated for the\noriginal prompt is also displayed to the right of the\noptimized image. Additionally, the application cal-\nculates PickScore and a normalized aesthetics score\nfor the two images, which is displayed in a table\nbelow the images. This side-by-side comparison\nfunctionality allows the user to directly assess the\nimpact of our prompt optimizations on the quality\nof images generated by Stable Diffusion.\nExamples of images generated from original and\noptimized prompts\nTo further illustrate the im-\npact of NeuroPrompts on image quality, Table 2\nprovides examples of images generated from origi-\nnal prompts and our optimized prompts. Each row\nof the table provides an original (un-optimized)\nprompt along with images generated by Stable Dif-\nfusion for the original prompt (center) and an opti-\nmized prompt produced by NeuroPrompts (right).\nThese examples illustrate how NeuroPrompts con-\nsistently produces a more aesthetically-pleasing\nimage than un-optimized prompts.\n5\nRelated Work\nPrompt engineering.\nPrevious studies have\ndemonstrated the superior performance of mod-\nels trained on manually designed prefix prompts\n(Brown et al., 2020). However, these models are\nheavily dependent on the prompt components (Liu\net al., 2021). Research on text-to-image models\nhas focused on proposing keywords (Oppenlaen-\nder, 2022) and design guidelines (Liu and Chilton,\n2022). Additionally, prior studies have explored the\nenhancement of LM prompts through differentiable\ntuning of soft prompts (Lester et al., 2021; Qin and\nEisner, 2021). Similar to our approach, Hao et al.\n(2022) proposed an automatic prompt engineering\nscheme via reinforcement learning. In contrast to\nthis prior work, NeuroPrompts preserves user in-\nterpretabilty and control over the prompt optimiza-\ntion process via the use of symbolic constraints.\nLearning from human preference.\nHuman feed-\nback has been used to improve various machine\nlearning systems, and several recent investigations\ninto reinforcement learning from human feedback\n(RLHF) have shown encouraging outcomes in ad-\ndressing machine learning challenges. These stud-\nies include applications to instruction following\n(Ouyang et al., 2022), summarization (Stiennon\net al., 2020) and text-to-image models (Lee et al.,\n2023). While Hao et al. (2022) also leverage RLHF\nfor the purpose of prompt engineering, our ap-\nproach uses a different reward function based on\nhuman preferences for images (PickScore) while\nproviding user control via constrained decoding.\nNeuroLogic Decoding\nNeuroLogic Decoding\n(Lu et al., 2021b) has been extended and applied to\nvarious use cases, including A* search (Lu et al.,\n2021a) counterfactual generation (Howard et al.,\n2022), inductive knowledge distillation (Bhagavat-\nula et al., 2022), and the acquisition of comparative\nknowledge (Howard et al., 2023). To the best of\nour knowledge, our work is the first to explore the\napplicability of constrained text generation with\nNeuroLogic to prompt optimization.\n6\nConclusion\nWe presented NeuroPrompts, an application\nwhich automatically optimizes user prompts for\ntext-to-image generation. NeuroPrompts unlocks\nthe full potential of text-to-image diffusion models\nto users without requiring any training in how to\nconstruct an optimal prompt for the model. There-\nfore, we expect it to increase the accessibility of\nsuch models while improving their ability to be\ndeployed in a more automated fashion. In future\nwork, we would like to extend NeuroPrompts to\nvideo generation models and other settings which\ncan benefit from automated prompt engineering.\nLimitations\nWhile NeuroPrompts is broadly compatible with\nany text-to-image generation model, we only eval-\nuated its use with Stable Diffusion in this work\ndue to limited computational resources. Images\nOriginal Image\nNeuroPrompts Optimized Image\nA boy on a horse\nA tropical beach with palm trees\nTwo women working in a kitchen\nTable 2: Examples of images generated from original prompts and our optimized prompts. The original (un-\noptimized) prompt is shown in rotated text to the left of each image pair\ngenerated from Stable Diffusion have been shown\nto exhibit societal biases (Luccioni et al., 2023);\ntherefore, it is expected that images generated us-\ning NeuroPrompts will also exhibit similar biases.\nThe automated nature of our prompt enhancement\nand image generation framework introduces the\npossibility of content being generated which may\nbe considered offensive or inappropriate to certain\nindividuals. Consequently, user discretion is ad-\nvised when interacting with NeuroPrompts.\nReferences\nChandra Bhagavatula, Jena D Hwang, Doug Downey,\nRonan Le Bras, Ximing Lu, Keisuke Sakaguchi,\nSwabha Swayamdipta, Peter West, and Yejin Choi.\n2022.\nI2d2:\nInductive knowledge distillation\nwith neurologic and self-imitation. arXiv preprint\narXiv:2212.09246.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022.\nOptimizing prompts for text-to-image generation.\narXiv preprint arXiv:2212.09611.\nPhillip Howard, Gadi Singer, Vasudev Lal, Yejin\nChoi, and Swabha Swayamdipta. 2022.\nNeuro-\ncounterfactuals: Beyond minimal-edit counterfac-\ntuals for richer data augmentation. arXiv preprint\narXiv:2210.12365.\nPhillip Howard, Junlin Wang, Vasudev Lal, Gadi Singer,\nYejin Choi, and Swabha Swayamdipta. 2023. Neuro-\ncomparatives: Neuro-symbolic distillation of compar-\native knowledge. arXiv preprint arXiv:2305.04978.\nKimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Moham-\nmad Ghavamzadeh, and Shixiang Shane Gu. 2023.\nAligning text-to-image models using human feed-\nback. arXiv preprint arXiv:2302.12192.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021.\nWhat\nmakes good in-context examples for gpt-3? arXiv\npreprint arXiv:2101.06804.\nVivian Liu and Lydia B Chilton. 2022. Design guide-\nlines for prompt engineering text-to-image generative\nmodels. In Proceedings of the 2022 CHI Conference\non Human Factors in Computing Systems, pages 1\u2013\n23.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang,\nJungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-\nhui Qin, Youngjae Yu, Rowan Zellers, et al. 2021a.\nNeurologic a* esque decoding: Constrained text gen-\neration with lookahead heuristics. arXiv preprint\narXiv:2112.08726.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras,\nChandra Bhagavatula, and Yejin Choi. 2021b. Neu-\nroLogic decoding: (un)supervised neural text genera-\ntion with predicate logic constraints. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4288\u20134299,\nOnline. Association for Computational Linguistics.\nAlexandra Sasha Luccioni, Christopher Akiki, Margaret\nMitchell, and Yacine Jernite. 2023. Stable bias: An-\nalyzing societal representations in diffusion models.\narXiv preprint arXiv:2303.11408.\nJonas Oppenlaender. 2022. A taxonomy of prompt mod-\nifiers for text-to-image generation. arXiv preprint\narXiv:2204.13988.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\narXiv preprint arXiv:2104.06599.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, Patrick Schramowski, Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt,\nRobert Kaczmarczyk, and Jenia Jitsev. 2022. Laion-\n5b: An open large-scale dataset for training next\ngeneration image-text models.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u2013\n3021.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, and Nathan Lam-\nbert. 2020. Trl: Transformer reinforcement learning.\nhttps://github.com/lvwerra/trl.\nZijie J Wang, Evan Montoya, David Munechika,\nHaoyang Yang, Benjamin Hoover, and Duen Horng\nChau. 2022.\nDiffusiondb: A large-scale prompt\ngallery dataset for text-to-image generative models.\narXiv preprint arXiv:2210.14896.\nA\nAppendix\nA.1\nDataset\nTo train and evaluate our adaptive framework for\nprompt enhancement in text-to-image generation,\nwe utilized the DiffusionDB dataset (Wang et al.,\n2022), a large dataset of human-created prompts.\nWe use a subset of 600k prompts from this dataset\nto conduct supervised fine-tuning of our LM. For\nthe reinforcement learning stage of training, we\nuse a different subset of 400k prompts from Diffu-\nsionDB. For each of the 400k prompts, we truncate\nthe prompt to contain only the substring before the\nfirst occurrence of a comma, assuming that modi-\nfiers generally appear after the first comma. This\napproach allows for improved exploration of para-\nphrasing by our policy. We filtered examples with\na significant overlap between the prefix and the\nentire prompt. To achieve this, we used a sentence\nsimilarity threshold of 0.6 overlap and excluded\ncases which exceeded this threshold.\nA.2\nPrompt enhancement keywords\nTable 3 provides the complete set of prompt en-\nhancement keywords utilized in our constraint sets.\nStyle\nArtist\nFormat\nBoosters\nVibes\nPerspective\nexpressionism\npablo picasso\nwatercolor painting\ntrending on artstation\ncontrol the soul\nlong shot\nsuminagashi\nedvard munch\ncrayon drawing\noctane render\nfuturistic\nplain background\nsurrealism\nhenri matisse\nUS patent\nultra high poly\nutopian\nisometric\nanime\nthomas cole\nkindergartener drawing\nextremely detailed\ndystopian\npanoramic\nart deco\nmark rothko\ncartoon\nvery beautiful\nblade runner\nwide angle\nphotorealism\nalphonse mucha\nin Mario Kart\nstudio lighting\ncinematic\nhard lighting\ncyberpunk\nleonardo da vinci\npixel art\nfantastic\nfantasy\nknolling\nsynthwave\nclaude monet\ndiagram\npostprocessing\nelegant\nshallow depth of field\nrealism\njames gurney\nalbum art cover\nwell preserved\nmagnificent\nextreme wide shot\npop art\ntoshi yoshida\nunder an electron microscope\n4k\nretrofuturistic\ndrone\npixar movies\nzdzislaw beksinski\nphotograph\narnold render\nawesome\nfrom behind\nabstract organic\ngustave dor\u00e9\npencil sketch\ndetailed\ntranshumanist\nlandscape\ndadaism\ngeorges braque\nstained glass window\nhyperrealistic\nbright\n1/1000 sec shutter\nneoclassicism\nbill watterson\nadvertising poster\nrendering\nwormhole\nfrom below\nancient art\nmichelangelo\nmugshot\nvfx\neclectic\nhead-and-shoulders shot\nbaroque\ngreg rutkowski\ncross-stitched sampler\nhigh detail\nepic\nfrom above\nart nouveau\nvincent van gogh\nillustration\nzbrush\ntasteful\noversaturated filter\nimpressionist\ncaravaggio\npencil and watercolor drawing\n70mm\ngorgeous\naerial view\nsymbolism\ndiego rivera\nin Fortnite\nhyper realistic\nopaque\ntelephoto\nhudson river school\ndean cornwell\nline art\n8k\nold\nmotion blur\nsuprematism\nralph mcquarrie\nproduct photography\nprofessional\nlsd trip\n85mm\nrococo\nren\u00e9 magritte\nin GTA San Andreas\nbeautiful\nlo-fi\nviewed from behind\npointillism\njohn constable\nnews crew reporting live\ntrending on artstation\nemo\nthrough a porthole\nvaporwave\ngustave dore\nline drawing\nstunning\nlucid\ndark background\nfuturism\njackson pollock\ncourtroom sketch\ncontest winner\nmoody\nfisheye lens\nskeumorphism\nhayao miyazaki\non Sesame Street\nwondrous\ncrystal\nthrough a periscope\nukiyo-e\nlucian freud\nwikiHow\nlook at that detail\nmelancholy\nwhite background\nmedieval art\njohannes vermeer\ndaguerreotype\nhighly detailed\ncosmos\non canvas\ncorporate memphis\nhieronymus bosch\n3d render\n4k resolution\nfaded\ntilted frame\nminimalism\nhatsune miku\nmodeling photoshoot\nrendered in unreal engine\nuplight\nframed\nfauvism\nutagawa kuniyoshi\none-line drawing\nphotorealistic\nconcept art\nlow angle\nrenaissance\nroy lichtenstein\ncharcoal drawing\nblender 3d\natmospheric\nlens flare\nconstructivism\nyoji shinkawa\ncaptured on CCTV\ndigital art\ndust\nclose face\ncubism\ncraig mullins\npainting\nvivid\nparticulate\nover-the-shoulder shot\nmemphis design\nclaude lorrain\nmacro 35mm photograph\nwow\ncute\nclose up\nromanticism\nfunko pop\non America\u2019s Got Talent\nhigh poly\nstormy\nextreme close-up shot\nhieroglyphics\nkatsushika hokusai\npastel drawing\nunreal engine\nmagical\nmidshot\nTable 3: Prompt enhancement keywords utilized in constraint sets\n"
  },
  {
    "title": "PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics",
    "link": "https://arxiv.org/pdf/2311.12198.pdf",
    "upvote": "19",
    "text": "PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics\nTianyi Xie1\u2217\nZeshun Zong1\u2217\nYuxing Qiu1\u2217\nXuan Li1\u2217\nYutao Feng2,3\nYin Yang3\nChenfanfu Jiang1\n1 UCLA, 2 Zhejiang University, 3 University of Utah\nt\nWhat You Simulate\nWhat You See\nFigure 1. PhysGaussian is a unified simulation-rendering pipeline where 3D Gaussians are seamlessly simulatable and renderable with\nnovel motion and views.\nAbstract\nWe introduce PhysGaussian, a new method that seamlessly\nintegrates physically grounded Newtonian dynamics within\n3D Gaussians to achieve high-quality novel motion synthe-\nsis. Employing a custom Material Point Method (MPM),\nour approach enriches 3D Gaussian kernels with physically\nmeaningful kinematic deformation and mechanical stress\nattributes, all evolved in line with continuum mechanics\nprinciples. A defining characteristic of our method is the\nseamless integration between physical simulation and vi-\nsual rendering: both components utilize the same 3D Gaus-\nsian kernels as their discrete representations. This negates\nthe necessity for triangle/tetrahedron meshing, marching\ncubes, \u201ccage meshes,\u201d or any other geometry embedding,\nhighlighting the principle of \u201cwhat you see is what you sim-\nulate (WS2).\u201d Our method demonstrates exceptional ver-\nsatility across a wide variety of materials\u2013including elas-\ntic entities, metals, non-Newtonian fluids, and granular\nmaterials\u2013showcasing its strong capabilities in creating di-\nverse visual content with novel viewpoints and movements.\nOur project page is at: https://xpandora.github.\nio/PhysGaussian/.\n* indicates equal contributions.\n1. Introduction\nRecent strides in Neural Radiance Fields (NeRFs) have\nshowcased significant advancements in 3D graphics and vi-\nsion [23]. Such gains have been further augmented by the\ncutting-edge 3D Gaussian Splatting (GS) framework [15].\nDespite many achievements, a noticeable gap remains in\nthe application towards generating novel dynamics. While\nthere exist endeavors that generate new poses for NeRFs\n[11, 26, 45, 48], they typically cater to quasi-static geometry\nshape editing tasks and often require meshing or embedding\nvisual geometry in coarse proxy meshes such as tetrahedra.\nMeanwhile, the traditional physics-based visual content\ngeneration pipeline has been a tedious multi-stage process:\nconstructing the geometry, making it simulation-ready (of-\nten through techniques like tetrahedralization), simulating\nit with physics, and finally rendering the scene. This se-\nquence, while effective, introduces intermediary stages that\ncan lead to discrepancies between simulation and final visu-\nalization. Even within the NeRF paradigm, a similar trend\nis observed, as the rendering geometry is embedded into a\nsimulation geometry. This division, in essence, contrasts\nwith the natural world, where the physical behavior and vi-\nsual appearance of materials are intrinsically intertwined.\nOur overarching philosophy seeks to align these two facets\nby advocating for a unified representation of a material sub-\nstance, employed for both simulation and rendering.\nIn\n1\narXiv:2311.12198v2  [cs.GR]  22 Nov 2023\nessence, our approach champions the principle of \u201cwhat\nyou see is what you simulate\u201d (WS2), aiming for a more\ngenuine and coherent integration of simulation, capturing,\nand rendering.\nBuilding towards this goal, we introduce PhysGaussian:\nphysics-integrated 3D Gaussians for generative dynamics.\nThis novel approach empowers 3D Gaussians to encapsu-\nlate physically sound Newtonian dynamics, including real-\nistic behaviors and inertia effects inherent in solid materi-\nals. More specifically, we impart physics to 3D Gaussian\nkernels, endowing them with kinematic attributes such as\nvelocity and strain, along with mechanical properties like\nelastic energy, stress, and plasticity. Notably, through con-\ntinuum mechanics principles and a custom Material Point\nMethod (MPM), PhysGaussian ensures that both physical\nsimulation and visual rendering are driven by 3D Gaus-\nsians.\nThis eradicates the necessity for any embedding\nmechanisms, thus eliminating any disparity or resolution\nmismatch between the simulated and the rendered.\nWe present PhysGaussian\u2019s versatile adeptness in syn-\nthesizing generative dynamics across various materials,\nsuch as elastic objects, metals, non-Newtonian viscoplastic\nsubstances (e.g. foam or gel), and granular mediums (e.g.\nsand or soil). To summarize, our contributions include\n\u2022 Continuum Mechanics for 3D Gaussian Kinematics:\nWe introduce a continuum mechanics-based strategy tai-\nlored for evolving 3D Gaussian kernels and their associ-\nated spherical harmonics in physical Partial Differential\nEquation (PDE)-driven displacement fields.\n\u2022 Unified Simulation-Rendering Pipeline: We present an\nefficient simulation and rendering pipeline with a unified\n3D Gaussian representation. Eliminating the extra effort\nfor explicit object meshing, the motion generation process\nis significantly simplified.\n\u2022 Versatile Benchmarking and Experiments: We con-\nduct a comprehensive suite of benchmarks and experi-\nments across various materials. Enhanced by real-time\nGS rendering and efficient MPM simulations, we achieve\nreal-time performance for scenes with simple dynamics.\n2. Related Work\nRadiance Fields Rendering for View Synthesis.\nRadi-\nance field methods have gained considerable interest in re-\ncent years due to their extraordinary ability to generate\nnovel-view scenes and their great potential in 3D recon-\nstruction.\nThe adoption of deep learning techniques has\nled to the prominence of neural rendering and point-based\nrendering methods, both of which have inspired a multi-\ntude of subsequent works.\nOn the one hand, the NeRF\nframework employs a fully-connected network to model\none scene [23].\nThe network takes spatial position and\nviewing direction as inputs and produces the volume den-\nsity and radiance color. These outputs are subsequently uti-\nlized in image generation through volume rendering tech-\nniques.\nBuilding upon the achievements of NeRF, fur-\nther studies have focused on enhancing rendering quality\nand improving training speeds [1, 7, 24, 38, 44]. On the\nother hand, researchers have also investigated differentiable\npoint-based methods for real-time rendering of unbounded\nscenes. Among the current investigations, the state-of-the-\nart results are achieved by the recently published 3D Gaus-\nsian Splatting framework [15]. Contrary to prior implicit\nneural representations, GS employs an explicit and unstruc-\ntured representation of one scene, offering the advantage\nof straightforward extension to post-manipulation. More-\nover, its fast visibility-aware rendering algorithm also en-\nables real-world dynamics generations.\nDynamic Neural Radiance Field.\nAn inherent evolution\nof the NeRF framework entails the integration of a tem-\nporal dimension to facilitate the representation of dynamic\nscenes. For example, both Pumarola et al. [28] and Park\net al. [25] decompose time-dependent neural fields into\nan inverse displacement field and canonical time-invariant\nneural fields. In this context, the trajectory of query rays\nis altered by the inverse displacement field and then po-\nsitioned within the canonical space.\nSubsequent stud-\nies have adhered to the aforementioned design when ex-\nploring applications related to NeRF deformations, such\nas static scene editing and dynamic scene reconstruction\n[4, 19, 26, 29, 30, 48]. Additionally, Liu et al. [19], Qiao\net al. [29], Yuan et al. [48] have contributed to the incorpo-\nration of physics-based deformations into the NeRF frame-\nwork. However, the effectiveness of these methodologies\nrelies on the usage of exported meshes derived from NeRFs.\nTo circumvent this restriction, explicit geometric represen-\ntations [15, 44] have been explored for forward displace-\nment modeling. In particular, Chen et al. [5], Luiten et al.\n[20], Wu et al. [43], Yang et al. [46, 47] directly manipulate\nNeRF fields. Li et al. [18] extends this approach by includ-\ning physical simulators to achieve more dynamic behaviors.\nIn this study, we leverage the explicit 3D Gaussian Splat-\nting ellipsoids as a unified representation for both physics\nand graphics. In contrast to previous dynamic GS frame-\nworks, which either maintain the shapes of Gaussian kernels\nor learn to modify them, our approach uniquely leverages\nthe first-order information from the displacement map (de-\nformation gradient) to assist dynamic simulations. In this\nway, we are able to deform the Gaussian kernels and seam-\nlessly integrate the simulation within the GS framework.\nMaterial Point Method.\nThe Material Point Method\n(MPM) is a widely used simulation framework for a broad\nrange of multi-physics phenomena [9]. The inherent capa-\nbility of the MPM system allows for topology changes and\nfrictional interactions, making it suitable for simulating var-\n2\nPhysics-grounded \nNovel Motion\nMultiple-Viewpoint Renderer\nPhysics Integration\nGaussian Ellipsoids \nas Continuum\nKinematics\nGaussian Evolution\nHarmonics Transform\nDynamics\nTime Integration\nContinuum Mechanics\nInput Images  \n+ Camera Info\n3D Gaussian \nSplatting\nAnisotropic \nLoss Term\n3D Gaussian \nKernel Filling\nOptional Step\nOptimization\nFigure 2. Method Overview. PhysGaussian is a unified simulation-rendering pipeline that incorporates 3D Gaussian splatting representa-\ntion and continuum mechanics to generate physics-based dynamics and photo-realistic renderings simultaneously and seamlessly.\nious materials, including but not limited to elastic objects,\nfluids, sand, and snow [12, 16, 37]. MPM can also be ex-\npanded to simulate objects that possess codimensional char-\nacteristics [14]. In addition, the efficacy of utilizing GPU(s)\nto accelerate MPM implementations has also been demon-\nstrated in [8, 10, 31, 42]. Owing to its well-documented ad-\nvantages, we employ the MPM to support the latent phys-\nical dynamics. This choice allows us to efficiently import\ndynamics into various scenarios with a shared particle rep-\nresentation alongside the Gaussian Splatting framework.\n3. Method Overview\nWe propose PhysGaussian (Fig. 2), a unified simulation-\nrendering framework for generative dynamics based on con-\ntinuum mechanics and 3D GS. Adopting Kerbl et al. [15],\nwe first reconstruct a GS representation of a static scene,\nwith an optional anisotropic loss term to regularize over-\nskinny kernels.\nThese Gaussians are viewed as the dis-\ncretization of the scene to be simulated. Under our novel\nkinematics, we directly splat the deformed Gaussians for\nphoto-realistic renderings. For better physics compliance,\nwe also optionally fill the internal regions of objects. We\ndetail these in this section.\n3.1. 3D Gaussian Splatting\n3D Gaussian Splatting method [15] reparameterizes NeRF\n[23] using a set of unstructured 3D Gaussian kernels\n{xp, \u03c3p, Ap, Cp}p\u2208P, where xp, \u03c3p, Ap, and Cp represent\nthe centers, opacities, covariance matrices, and spherical\nharmonic coefficients of the Gaussians, respectively.\nTo\nrender a view, GS projects these 3D Gaussians onto the im-\nage plane as 2D Gaussians, differing from traditional NeRF\ntechniques that emit rays from the camera. The final color\nof each pixel is computed as\nC =\nX\nk\u2208P\n\u03b1kSH(dk; Ck)\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j).\n(1)\nHere \u03b1k represents the z-depth ordered effective opacities,\ni.e., products of the 2D Gaussian weights and their over-\nall opacities \u03c3k; dk stands for the view direction from the\ncamera to xk. Per-view optimizations are performed us-\ning L1 loss and SSIM loss. This explicit representation of\nthe scene not only significantly accelerates training and ren-\ndering speeds, but also enables direct manipulation of the\nNeRF scene. The data-driven dynamics are supported by\nmaking xp, Ap time-dependent [43] and minimizing ren-\ndering losses over videos. In Sec. 3.4, we show that this\ntime-dependent evolution can be given by the continuum\ndeformation map.\n3.2. Continuum Mechanics\nContinuum mechanics describes motions by a time-\ndependent continuous deformation map x\n=\n\u03d5(X, t)\nbetween the undeformed material space \u21260 and the de-\nformed world space \u2126t at time t. The deformation gradient\nF(X, t) = \u2207X\u03d5(X, t) encodes local transformations in-\ncluding stretch, rotation, and shear [2]. The evolution of the\ndeformation \u03d5 is governed by the conservation of mass and\nmomentum. Conservation of mass ensures that the mass\nwithin any infinitesimal region B0\n\u03f5 \u2208 \u21260 remains constant\nover time:\nZ\nBt\u03f5\n\u03c1(x, t) \u2261\nZ\nB0\u03f5\n\u03c1(\u03d5\u22121(x, t), 0),\n(2)\nwhere Bt\n\u03f5 = \u03d5(B0\n\u03f5 , t) and \u03c1(x, t) is the density field char-\nacterizing material distribution. Denoting the velocity field\nwith v(x, t), the conservation of momentum [2, 13] is given\nby\n3\n\u03c1(x, t) \u02d9v(x, t) = \u2207 \u00b7 \u03c3(x, t) + f ext,\n(3)\nwhere \u03c3 =\n1\ndet(F )\n\u2202\u03a8\n\u2202F (F E)F ET is the Cauchy stress ten-\nsor associated with a hyperelastic energy density \u03a8(F ), and\nf ext is the external force per unit volume. Here the total de-\nformation gradient can be decomposed into an elastic part\nand a plastic part F = F EF P to support permanent rest\nshape changes caused by plasticity. The evolution of F E\nfollows some specific plastic flow [2] such that it is always\nconstrained within a predefined elastic region.\n3.3. Material Point Method\nMaterial Point Method (MPM) solves the above governing\nequations by combining the strengths of both Lagrangian\nparticles and Eulerian grids [13, 37].\nThe continuum is\ndiscretized by a collection of particles, each representing\na small material region. These particles track several time-\nvarying Lagrangian quantities such as position xp, veloc-\nity vp, and deformation gradient Fp. The mass conserva-\ntion in Lagrangian particles ensures the constancy of total\nmass during movement. Conversely, momentum conserva-\ntion is more natural in Eulerian representation, which avoids\nmesh construction. We follow Stomakhin et al. [37] to in-\ntegrate these representations using C1 continuous B-spline\nkernels for two-way transfer. From time step tn to tn+1, the\nmomentum conservation, discretized by the forward Euler\nscheme, is represented as\nmi\n\u2206t (vn+1\ni\n\u2212 vn\ni ) = \u2212 P\np V 0\np\n\u2202\u03a8\n\u2202F (F E,n\np\n)F E,n\np\nT \u2207wn\nip + f ext\ni\n.\n(4)\nHere i and p represent the fields on the Eulerian grid and\nthe Lagrangian particles respectively; wn\nip is the B-spline\nkernel defined on i-th grid evaluated at xn\np; V 0\np is the\ninitial representing volume, and \u2206t is the time step size.\nThe updated grid velocity field vn+1\ni\nis transferred back\nonto particle to vn+1\np\n, updating the particles\u2019 positions to\nxn+1\np\n= xn\np + \u2206tvn+1\np\n. We track F E rather than both F\nand F P [35], which is updated by\nF E,n+1\np\n=\n(I\n+ \u2206t\u2207vp)F E,n\np\n=\n(I\n+\n\u2206t P\ni vn+1\ni\n\u2207wn\nip\nT )F E,n\np\nand regularized by an addi-\ntional return mapping to support plasticity evolution:\nF E,n+1\np\n\u2190 Z(F E,n+1\np\n). Different plasticity models define\ndifferent return mappings.\nWe refer to the appendix for\ndetails of the simulation algorithm and different return\nmappings.\n3.4. Physics-Integrated 3D Gaussians\nWe treat Gaussian kernels as discrete particle clouds for\nspatially discretizing the simulated continuum. As the con-\ntinuum deforms, we let the Gaussian kernels deform as well.\nHowever, for a Gaussian kernel defined at Xp in the ma-\nterial space, Gp(X) = e\u2212 1\n2 (X\u2212Xp)T A\u22121\np\n(X\u2212Xp), the de-\nformed kernel under the deformation map \u03d5(X, t),\nGp(x, t) = e\u2212 1\n2 (\u03d5\u22121(x,t)\u2212Xp)T A\u22121\np\n(\u03d5\u22121(x,t)\u2212Xp)\n(5)\nis not necessarily Gaussian in the world space, which vio-\nlates the requirements of the splatting process. Fortunately,\nif we assume particles undergo local affine transformations\ncharacterized by the first-order approximation\n\u02dc\u03d5p(X, t) = xp + Fp(X \u2212 Xp),\n(6)\nthe deformed kernel becomes Gaussian as desired:\nGp(x, t) = e\u2212 1\n2 (x\u2212xp)T (FpApF T\np )\u22121(x\u2212xp).\n(7)\nThis transformation naturally\nprovides\na\ntime-dependent\nversion of xp and Ap for the\n3D GS framework:\nxp(t) = \u03d5(Xp, t),\nap(t) = Fp(t)ApFp(t)T .\n(8)\nIn\nsummary,\ngiven\nthe\n3D\nGS\nof\na\nstatic\nscene\n{Xp, Ap, \u03c3p, Cp}, we use simulation to dynamize the scene\nby evolving these Gaussians to produce dynamic Gaussians\n{xp(t), ap(t), \u03c3p, Cp}. Here we assume that the opacity and\nthe coefficients of spherical harmonics are invariant over\ntime, but the harmonics will be rotated as discussed in the\nnext section. We also initialize other physical quantities in\nEq. (4): the representing volume of each particle V 0\np is ini-\ntialized as background cell volume divided by the number\nof contained particles; the mass mp is then inferred from\nuser-specified density \u03c1p as mp = \u03c1pV 0\np . To render these\ndeformed Gaussian kernels, we use the splatting from the\noriginal GS framework [15]. It should be highlighted that\nthe integration of physics into 3D Gaussians is seamless:\non the one hand, the Gaussians themselves are viewed as\nthe discretization of the continuum, which can be simulated\ndirectly; on the other hand, the deformed Gaussians can be\ndirectly rendered by the splatting procedure, avoiding the\nneed for commercial rendering software in traditional ani-\nmation pipelines. Most importantly, we can directly simu-\nlate scenes reconstructed from real data, achieving WS2.\n3.5. Evolving Orientations of Spherical Harmonics\nSurface\nView\nRotated View\nRendering the world-space\n3D Gaussians can already\nobtain high-quality results.\nHowever,\nwhen the ob-\nject undergoes rotations,\nthe spherical harmonic bases are still represented in the ma-\nterial space, resulting in varying appearances even if the\nview direction is fixed relatively to the object. The solution\nis simple: when an ellipsoid is rotated over time, we rotate\n4\nthe orientations of its spherical harmonics as well. How-\never, the bases are hard-coded inside the GS framework. We\nequivalently achieve this evolution by applying inverse rota-\ntion on view directions. This effect is illustrated in the inset\nfigure. We remark that the rotation of view directions is not\nconsidered in Wu et al. [43]. Chen et al. [5] tackles this\nissue in the Point-NeRF framework, but requires tracking\nof surface orientation. In our framework, the local rotation\nis readily obtained in the deformation gradient Fp. Denote\nf 0(d) as a spherical harmonic basis in material space, with\nd being a point on the unit sphere (indicating view direc-\ntion). The polar decomposition, Fp = RpSp, leads us to\nthe rotated harmonic basis:\nf t(d) = f 0(RT d).\n(9)\n3.6. Incremental Evolution of Gaussians\nWe also propose an alternative way for Gaussian kinemat-\nics that better fits the updated Lagrangian framework, which\navoids the dependency on the total deformation gradient\nF . This approach also paves the way for physical mate-\nrial models that do not rely on employing F as the strain\nmeasure. Following conventions from computational fluid\ndynamics [3, 22], the update rule for the world-space co-\nvariance matrix a can also be derived by discretizing the\nrate form of kinematics \u02d9a = (\u2207v)a + a(\u2207v)T :\nan+1\np\n= an\ni + \u2206t(\u2207vpan\np + an\np\u2207vT\np ).\n(10)\nThis formulation facilitates the incremental update of the\nGaussian kernel shapes from time step tn to tn+1 with-\nout the need to obtain Fp. The rotation matrix Rp of each\nspherical harmonics basis can be incrementally updated in\na similar manner. Starting from R0\np = I, we extract the\nrotation matrix Rn+1\np\nfrom (I + \u2206tvp)Rn\np using the polar\ndecomposition.\n3.7. Internal Filling\nThe internal structure is occluded by the object\u2019s surface,\nas the reconstructed Gaussians tend to distribute near the\nsurface, resulting in inaccurate behaviors of volumetric ob-\njects. To fill particles into the void internal region, inspired\nby Tang et al. [40], we borrow the 3D opacity field from 3D\nGaussians\nd(x) = P\np \u03c3p exp\n\u0000\u2212 1\n2(x \u2212 xp)T A\u22121\np (x \u2212 xp)\n\u0001\n.\n(11)\nCondition 1\nCondition 2\nInternal Grid\nRay\nExternal Grid\nThis continuous field is\ndiscretized onto a 3D\ngrid.\nTo achieve ro-\nbust internal filling, we\nfirst define the concept\nof \u201cintersection\u201d within\nthe opacity field, guided\nby a user-defined threshold \u03c3th. Specifically, we consider it\nan intersection when a ray passes from a lower opacity grid\n(\u03c3i < \u03c3th) to a higher opacity one (\u03c3j > \u03c3th). Based on\nthis definition, we identify candidate grids by casting rays\nalong 6 axes and checking intersections (condition 1). Rays\noriginating from internal cells will always intersect with the\nsurface. To further refine our selection of candidate grids,\nwe employ an additional ray to assess the intersection num-\nber (condition 2), thus ensuring greater accuracy.\nVisualization of these internal particles is also crucial\nas they may get exposed due to large deformation. Those\nfilled particles inherit \u03c3p, Cp from their closet Gaussian ker-\nnels.\nEach particle\u2019s covariance matrix is initialized as\ndiag(r2\np, r2\np, r2\np), where r is the particle radius calculated\nfrom its volume: rp = (3V 0\np /4\u03c0)\n1\n3 .\n3.8. Anisotropy Regularizer\nThe anisotropy of Gaussian kernels increases the efficiency\nof 3D Gaussian representation. However, too-skinny ker-\nnels may point outward from the object surface under large\ndeformations, leading to unexpected plush artifacts.\nWe\npropose the following training loss during 3D Gaussian re-\nconstruction:\nLaniso =\n1\n|P|\nX\np\u2208P\nmax{max(Sp)/ min(Sp), r}\u2212r, (12)\nwhere Sp are the scalings of 3D Gaussians [15]. This loss\nessentially constrains that the ratio between the major axis\nlength and minor axis length does not exceed r. If desired,\nthis term can be added to the training loss.\n4. Experiments\nIn this section, we show the versatility of our approach\nacross a wide range of materials. we also evaluate the ef-\nfectiveness of our method across a comprehensive suite of\nbenchmarks.\n4.1. Evaluation of Generative Dynamics\nDatasets\nWe evaluate our proposed method for generat-\ning diverse dynamics using several sources of input.\nIn\naddition to the synthetic data (sofa suite) generated by\nBlenderNeRF [32], we utilize fox, plane, and ruins from\nthe datasets of Instant-NGP [24], Nerfstudio [39] and the\nDroneDeploy NeRF [27], respectively.\nFurthermore, we\ncollect two real-world datasets (referred to as toast and jam)\nwith an iPhone. Each scene contains 150 photos. The ini-\ntial point clouds and camera parameters are acquired by\nCOLMAP [33, 34].\nSimulation Setups\nWe build upon MPM from Zong et al.\n[50]. To generate novel physics-based dynamics of a 3D\nGaussian scene, we manually select a simulation region and\n5\nPaste\nElasticity\nSand\nMetal\nFracture\nCollision\nStatic\nPhysics-based Dynamics\nFigure 3. Material Versatility. We demonstrate exceptional versatility of our approach across a wide variety of examples: fox (elastic\nentity), plane (metal), toast (fracture), ruins (granular material), jam (viscoplastic material), and sofa suite (collision).\nnormalize it to a cube with edge length 2. The internal parti-\ncle filling can be performed before simulation. The cuboid\nsimulation domain is discretized by a 3D dense grid. We\nselectively modify the velocities of specific particles to in-\nduce controlled movement. The remaining particles follow\nnatural motion patterns governed by the established physi-\ncal laws. All our experiments are performed on a 24-core\n3.50GHz Intel i9-10920X machine with a Nvidia RTX 3090\nGPU.\nResults\nWe simulate a wide range of physics-based dy-\nnamics. For each type of dynamics, we visualize one ex-\nample with its initial scene and deformation sequence, as\nshown in Fig. 3. Additional experiments are included in the\nappendix. The dynamics include: Elasticity refers to the\nproperty where the rest shape of the object remains invariant\n6\nWolf (bend)\nGround Truth\nOurs\nDeforming-NeRF\nNeRF-Editing\nPAC-NeRF\nWolf (twist)\nStool (bend)\nStool (twist)\nPlant (bend)\nPlant (twist)\nFigure 4. Comparisons. For each benchmark case, we select one\ntest viewpoint and visualize all comparisons. We zoom in on some\nregions to highlight the ability of our method to maintain high-\nfidelity rendering quality after deformations. We use a black back-\nground to avoid the interference of the background.\nduring deformation, representing the simplest form of daily-\nlife dynamics. Metal can undergo permanent rest shape\nchanges, which follows von-Mises plasticity model. Frac-\nture is naturally supported by MPM simulation, where large\ndeformations can cause particles to separate into multiple\ngroups. Sand follows Druker-Prager plasticity model [16],\nwhich can capture granular-level frictional effects among\nparticles. Paste is modeled as viscoplastic non-Newtonian\nfluid, adhering to Herschel-Bulkley plasticity model [49].\nCollision is another key feature of MPM simulation, which\nis automatically handled by grid time integration. Explicit\nMPM can be highly optimized to run on GPUs. We high-\nlight that some of the cases can achieve real-time based on\nthe 1/24-s frame duration: plane (30 FPS), toast (25 FPS)\nand jam (36 FPS).\n4.2. Lattice Deformation Benchmarks\nDataset\nDue to the absence of ground truth for post-\ndeformation, we utilize BlenderNeRF [32] to synthesize\nseveral scenes, applying bending and twisting with the lat-\ntice deformation tool. For each scene, we create 100 multi-\nview renderings of the undeformed state for training, and\nGround Truth\nOurs\nFixed Cov.\nRigid Cov.\nFixed Harmonics\nFigure 5. Ablation Studies. Non-extensible Gaussians can lead to\nsevere visual artifacts during deformations. Although direct ren-\ndering the deformed Gaussian kernels can already obtain good re-\nsults, additional rotations on spherical harmonics can improve the\nrendering quality.\n100 multi-view renderings of each deformed state to serve\nas ground truth for the deformed NeRFs. The lattice defor-\nmations are set as input to all methods for fair comparisons.\nComparisons\nWe compare our method with several state-\nof-the-art NeRF frameworks that support manual deforma-\ntions: 1) NeRF-Editing [48] deforms NeRF using an ex-\ntracted surface mesh, 2) Deforming-NeRF [45] utilizes a\ncage mesh for deformation, and 3) PAC-NeRF [18] manip-\nulates individual initial particles.\nWe show qualitative results in Fig. 4 and quantitative re-\nsults in Tab. 1. NeRF-Editing uses NeuS [41] as the scene\nrepresentation, which is more suited for surface reconstruc-\ntions rather than high-fidelity renderings. Consequently, its\nrendering quality is inherently lower than that of Gaussian\nsplitting. Furthermore, the deformation highly depends on\nthe precision of the extracted surface mesh and the dilated\ncage mesh \u2013 an overly tight mesh might not encompass\nthe entire radiance field, while an excessively large one\ncould result in a void border, as observed in the twisting\nstool and plant examples. Deforming-NeRF, on the other\nhand, provides clear renderings, but its internal deforma-\ntion, interpolated from the bounding cage vertices, does\nnot perfectly match lattice deformations, limiting its ability\nto manipulate individual parts. PAC-NeRF is designed for\nsimpler objects and textures in system identification tasks.\nWhile offering flexibility through its particle representation,\nit does not achieve high rendering fidelity.\nOur method\nutilizes both zero-order information (the deformation map)\nand first-order information (the deformation gradient) from\neach lattice cell. It outperforms the other methods across all\ncases, as high rendering qualities are well preserved after\ndeformations. Although not primarily designed for editing\ntasks, this comparison showcases our method\u2019s significant\npotential for realistic editing of static NeRF scenes.\nAblation Studies\nWe further conduct several ablation\nstudies on these benchmark scenes to validate the neces-\nsity of the kinematics of Gaussian kernels and spherical har-\nmonics: 1) Fixed Covariance only translates the Gaussian\nkernels. 2) Rigid Covariance only applys rigid transforma-\n7\nTable 1. We synthesize a lattice deformation benchmark dataset\nto compare with baselines and conduct ablation studies to validate\nour design choices. PSNR scores are reported (higher is better).\nOur method outperforms the others across all cases.\nTest Case\nWolf\nStool\nPlant\nDeformation Operator\nBend\nTwist\nBend\nTwist\nBend\nTwist\nNeRF-Editing [48]\n26.74\n24.37\n25.00\n21.10\n19.85\n19.08\nDeforming-NeRF [45]\n21.65\n21.72\n22.32\n21.16\n17.90\n18.63\nPAC-NeRF [18]\n26.91\n25.27\n21.83\n21.26\n18.50\n17.78\nFixed Covariance\n26.77\n26.02\n29.94\n25.31\n23.95\n23.09\nRigid Covariance\n26.84\n26.16\n30.28\n25.70\n24.09\n23.53\nFixed Harmonics\n26.83\n26.02\n30.87\n25.75\n25.09\n23.69\nOurs\n26.96\n26.46\n31.15\n26.15\n25.81\n23.87\nw/o Internal Filling\nw/ Internal Filling\nE \u2191\nE \u2191\n\u03bd \u2191\nFigure 6. Internal filling enables more realistic simulation results.\nOur method also supports flexible control of dynamics via material\nparameters. A larger Young\u2019s modulus E indicates higher stiffness\nwhile a larger poission ratio \u03bd leads to better volume preservation.\ntions on the Gaussians, as assumed in Luiten et al. [20]. 3)\nFixed Harmonics does not rotate the orientations of spher-\nical harmonics, as assumed in Wu et al. [43].\nHere we visualize one example in Fig. 5. We can ob-\nserve that Gaussians will not properly cover the surface af-\nter deformation if they are non-extensible, leading to severe\nvisual artifacts. Enabling the rotation of spherical harmon-\nics can slightly improve the consistency with the ground\ntruth. We include quantitative results on all test cases in\nTab. 1, which shows that all these enhancements are needed\nto achieve the best performance of our method.\n4.3. Additional Qualitative Studies\nInternal Filling\nTypically, the 3D Gaussian splatting\nframework focuses on the surface appearance of objects and\noften fails to capture their internal structure. Consequently,\nthe interior of the modeled object remains hollow, resem-\nbling a mere shell. This is usually not true in the real world,\nleading to unrealistic simulation results. To address this\nchallenge, we introduce an internal filling method leverag-\ning a reconstructed density field, which is derived from the\nopacity of Gaussian kernels. Fig. 6 showcases our simula-\ntion results with varying physical parameters. Objects de-\nvoid of internal particles tend to collapse when subjected to\ngravity forces, irrespective of the material parameters used.\nIn contrast, our approach assisted by internal filling allows\nOurs\nStretch\nNeRF-Editing\nStretch\nFigure 7. Volume Conservation. Compared to the geometry-\nbased editing method [48], our physics-based method is able to\ncapture volumetric behaviors, leading to more realistic dynamics.\nw/ Regularizer\nw/o Regularizer\nFigure 8. Anisotropy Regularizer. We introduce an anisotropy\nconstraint for Gaussian kernels, effectively enhancing the fidelity\nof the Gaussian-based representation under dynamic conditions.\nfor nuanced control over object dynamics, effectively ad-\njusting to different material characteristics.\nVolume Conservation\nExisting approaches to NeRF ma-\nnipulation focus primarily on geometric adjustments with-\nout incorporating physical properties. A key attribute of\nreal-world objects is their inherent ability to conserve vol-\nume during deformation.\nIn Fig. 7, we conduct a com-\nparison study between our method and NeRF-Editing [48],\nwhich utilizes surface As-Rigid-As-Possible (ARAP) defor-\nmation [36]. Unlike NeRF-Editing, our approach accurately\ncaptures and maintains the volume of the deformed objects.\nAnisotropy Regularizer\n3D Gaussian models inherently\nrepresent anisotropic ellipsoids. However, excessively slen-\nder Gaussian kernels can lead to burr-like visual artifacts,\nespecially pronounced during large deformations To tackle\nthis issue, we introduce an additional regularization loss\nEq. (12) to constrain anisotropy. As demonstrated in Fig. 8,\nthis additional loss function effectively mitigates the arti-\nfacts induced by extreme anisotropy.\n5. Discussion\nConclusion.\nThis paper introduces PhysGaussian, a uni-\nfied simulation-rendering pipeline that generates physics-\nbased dynamics and photo-realistic renderings simultane-\nously and seamlessly.\nLimitation.\nEvolution of shadows are not considered in\nour framework. Additionally, we use one-point quadrature\n8\nfor integrating volume integrals, which may not adequately\nrepresent the size of individual Gaussian ellipsoids. MPM\nwith high-order quadratures [6] may be adopted. Further-\nmore, while PDE-based dynamics offer a useful approx-\nimation, incorporating neural networks learned from real\ndata [17, 21] or data-driven dynamics could provide more\nrealistic modeling. Future work will also explore handling\nmore versatile materials like liquids and integrating more\nintuitive user controls, possibly leveraging advancements in\nLarge Language Models (LLMs).\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5470\u20135479, 2022. 2\n[2] Javier Bonet and Richard D Wood.\nNonlinear continuum\nmechanics for finite element analysis. Cambridge university\npress, 1997. 3, 4\n[3] S Chandrasekhar. Ellipsoidal figures of equilibrium\u2014an his-\ntorical account. Communications on Pure and Applied Math-\nematics, 20(2):251\u2013265, 1967. 5\n[4] Hsiao-yu Chen, Edith Tretschk, Tuur Stuyck, Petr Kadlecek,\nLadislav Kavan, Etienne Vouga, and Christoph Lassner. Vir-\ntual elastic objects. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n15827\u201315837, 2022. 2\n[5] Jun-Kun Chen, Jipeng Lyu, and Yu-Xiong Wang. Neuraled-\nitor: Editing neural radiance fields via manipulating point\nclouds.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 12439\u2013\n12448, 2023. 2, 5\n[6] David G Dritschel, Jean N Reinaud, and WILLIAM J McK-\nIVER. The quasi-geostrophic ellipsoidal vortex model. Jour-\nnal of Fluid Mechanics, 505:201\u2013223, 2004. 9\n[7] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 2\n[8] Ming Gao, Xinlei Wang, Kui Wu, Andre Pradhana, Efty-\nchios Sifakis, Cem Yuksel, and Chenfanfu Jiang. Gpu opti-\nmization of material point methods. ACM Transactions on\nGraphics (TOG), 37(6):1\u201312, 2018. 3\n[9] Yuanming Hu, Yu Fang, Ziheng Ge, Ziyin Qu, Yixin Zhu,\nAndre Pradhana, and Chenfanfu Jiang.\nA moving least\nsquares material point method with displacement disconti-\nnuity and two-way rigid body coupling. ACM Transactions\non Graphics (TOG), 37(4):1\u201314, 2018. 2\n[10] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan\nRagan-Kelley, and Fr\u00b4edo Durand.\nTaichi:\na language\nfor high-performance computation on spatially sparse data\nstructures. ACM Transactions on Graphics (TOG), 38(6):\n1\u201316, 2019. 3\n[11] Cl\u00b4ement Jambon,\nBernhard Kerbl,\nGeorgios Kopanas,\nStavros Diolatzis, Thomas Leimk\u00a8uhler, and George\u201d Dret-\ntakis.\nNerfshop:\nInteractive editing of neural radiance\nfields\u201d. Proceedings of the ACM on Computer Graphics and\nInteractive Techniques, 6(1), 2023. 1\n[12] Chenfanfu Jiang, Craig Schroeder, Andrew Selle, Joseph\nTeran, and Alexey Stomakhin.\nThe affine particle-in-cell\nmethod. ACM Transactions on Graphics (TOG), 34(4):1\u201310,\n2015. 3, 11, 12\n[13] Chenfanfu Jiang, Craig Schroeder, Joseph Teran, Alexey\nStomakhin, and Andrew Selle. The material point method\nfor simulating continuum materials. In Acm siggraph 2016\ncourses, pages 1\u201352. 2016. 3, 4, 11\n[14] Chenfanfu Jiang,\nTheodore Gast,\nand Joseph Teran.\nAnisotropic elastoplasticity for cloth, knit and hair frictional\ncontact. ACM Transactions on Graphics (TOG), 36(4):1\u201314,\n2017. 3\n[15] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(ToG), 42(4):1\u201314, 2023. 1, 2, 3, 4, 5\n[16] Gergely Kl\u00b4ar, Theodore Gast, Andre Pradhana, Chuyuan\nFu, Craig Schroeder, Chenfanfu Jiang, and Joseph Teran.\nDrucker-prager elastoplasticity for sand animation.\nACM\nTransactions on Graphics (TOG), 35(4):1\u201312, 2016. 3, 7,\n11, 12\n[17] Xuan Li, Yadi Cao, Minchen Li, Yin Yang, Craig Schroeder,\nand Chenfanfu Jiang.\nPlasticitynet: Learning to simulate\nmetal, sand, and snow for optimization time integration.\nAdvances in Neural Information Processing Systems, 35:\n27783\u201327796, 2022. 9\n[18] Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy\nJatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang\nGan. PAC-neRF: Physics augmented continuum neural ra-\ndiance fields for geometry-agnostic system identification. In\nThe Eleventh International Conference on Learning Repre-\nsentations, 2023. 2, 7, 8\n[19] Ruiyang Liu, Jinxu Xiang, Bowen Zhao, Ran Zhang, Jingyi\nYu, and Changxi Zheng.\nNeural impostor: Editing neu-\nral radiance fields with explicit shape manipulation. arXiv\npreprint arXiv:2310.05391, 2023. 2\n[20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan.\nDynamic 3d gaussians:\nTracking\nby persistent dynamic view synthesis.\narXiv preprint\narXiv:2308.09713, 2023. 2, 8\n[21] Pingchuan Ma, Peter Yichen Chen, Bolei Deng, Joshua B\nTenenbaum, Tao Du, Chuang Gan, and Wojciech Ma-\ntusik.\nLearning neural constitutive laws from motion ob-\nservations for generalizable pde dynamics. arXiv preprint\narXiv:2304.14369, 2023. 9\n[22] WILLIAM J McKIVER and David G Dritschel. The mo-\ntion of a fluid ellipsoid in a general linear background flow.\nJournal of Fluid Mechanics, 474:147\u2013173, 2003. 5\n[23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021. 1,\n2, 3, 13\n9\n[24] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2, 5\n[25] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 2\n[26] Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng,\nShanyan Guan, Bowen Pan, Guangtao Zhai, and Xiaokang\nYang. Cagenerf: Cage-based neural radiance field for gen-\neralized 3d deformation and animation. Advances in Neural\nInformation Processing Systems, 35:31402\u201331415, 2022. 1,\n2\n[27] Nicholas Pilkington. Dronedeploy nerf dataset, 2022. 5\n[28] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n10318\u201310327, 2021. 2\n[29] Yi-Ling Qiao, Alexander Gao, and Ming Lin.\nNeu-\nphysics: Editable neural geometry and physics from monoc-\nular videos. Advances in Neural Information Processing Sys-\ntems, 35:12841\u201312854, 2022. 2\n[30] Yi-Ling Qiao, Alexander Gao, Yiran Xu, Yue Feng, Jia-Bin\nHuang, and Ming C Lin.\nDynamic mesh-aware radiance\nfields. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 385\u2013396, 2023. 2\n[31] Yuxing Qiu, Samuel Temple Reeve, Minchen Li, Yin Yang,\nStuart Ryan Slattery, and Chenfanfu Jiang.\nA sparse dis-\ntributed gigascale resolution material point method. ACM\nTransactions on Graphics, 42(2):1\u201321, 2023. 3\n[32] Maxime Raafat. BlenderNeRF, 2023. 5, 7, 13\n[33] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-motion revisited.\nIn Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 5\n[34] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys,\nand Jan-Michael Frahm. Pixelwise view selection for un-\nstructured multi-view stereo.\nIn European Conference on\nComputer Vision (ECCV), 2016. 5\n[35] Juan C Simo and Thomas JR Hughes. Computational inelas-\nticity. Springer Science & Business Media, 2006. 4\n[36] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface\nmodeling.\nIn Symposium on Geometry processing, pages\n109\u2013116. Citeseer, 2007. 8\n[37] Alexey Stomakhin, Craig Schroeder, Lawrence Chai, Joseph\nTeran, and Andrew Selle. A material point method for snow\nsimulation. ACM Transactions on Graphics (TOG), 32(4):\n1\u201310, 2013. 3, 4\n[38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 2\n[39] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstudio: A\nmodular framework for neural radiance field development.\nIn ACM SIGGRAPH 2023 Conference Proceedings, pages\n1\u201312, 2023. 5\n[40] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 5\n[41] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction.\nIn Advances in Neural Information Processing\nSystems, 2021. 7\n[42] Xinlei Wang, Yuxing Qiu, Stuart R Slattery, Yu Fang,\nMinchen Li, Song-Chun Zhu, Yixin Zhu, Min Tang, Dinesh\nManocha, and Chenfanfu Jiang. A massively parallel and\nscalable multi-gpu material point method.\nACM Transac-\ntions on Graphics (TOG), 39(4):30\u20131, 2020. 3\n[43] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023. 2, 3, 5, 8\n[44] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:\nPoint-based neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5438\u20135448, 2022. 2\n[45] Tianhan Xu and Tatsuya Harada. Deforming radiance fields\nwith cages. In European Conference on Computer Vision,\npages 159\u2013175. Springer, 2022. 1, 7, 8\n[46] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 2\n[47] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representa-\ntion and rendering with 4d gaussian splatting. arXiv preprint\narXiv:2310.10642, 2023. 2\n[48] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: geometry editing of\nneural radiance fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18353\u201318364, 2022. 1, 2, 7, 8\n[49] Yonghao Yue, Breannan Smith, Christopher Batty, Changxi\nZheng, and Eitan Grinspun. Continuum foam: A material\npoint method for shear-dependent flows. ACM Transactions\non Graphics (TOG), 34(5):1\u201320, 2015. 7, 13\n[50] Zeshun Zong, Xuan Li, Minchen Li, Maurizio M Chiara-\nmonte, Wojciech Matusik, Eitan Grinspun, Kevin Carlberg,\nChenfanfu Jiang, and Peter Yichen Chen.\nNeural stress\nfields for reduced-order elastoplasticity and fracture. arXiv\npreprint arXiv:2310.17790, 2023. 5, 11\n10\nAppendix\nA. MPM Algorithm\nIn MPM, a continuum body is discretized into a set of La-\ngrangian particles p, and time is discretized into a sequence\nof time steps t = 0, t1, t2, .... Here we take a fixed stepsize\n\u2206t, so tn = n\u2206t.\nAt each time step, masses and momentums on particles\nare first transferred to grid nodes. Grid velocities are then\nupdated using forward Euler\u2019s method and transferred back\nto particles for subsequent advection. Let mp, xn\np, vn\np , F n\np ,\n\u03c4 n\np , and Cn\np denote the mass, position, velocity, deforma-\ntion gradient, Kirchhoff stress, and affine momentum on\nparticle p at time tn. Let mi, xn\ni and vn\ni denote the mass,\nposition, and velocity on grid node i at time tn. Here, par-\nticle masses are invariant due to mass conservation law. Let\nmn\ni , xn\ni and vn\ni denote the mass, position, and velocity on\ngrid node i at time tn. We summarize the explicit MPM\nalgorithm as follows:\n1. Transfer Particles to Grid. Transfer mass and momen-\ntum from particles to grids as\nmn\ni =\nX\np\nwn\nipmp,\nmn\ni vn\ni =\nX\np\nwn\nipmp\n\u0000vn\np + Cn\np\n\u0000xi \u2212 xn\np\n\u0001\u0001\n.\n(13)\nWe adopt the APIC scheme [12] for momentum transfer.\n2. Grid Update. Update grid velocities based on forces at\nthe next timestep by\nvn+1\ni\n= vn\ni \u2212 \u2206t\nmi\nX\np\n\u03c4 n\np \u2207wn\nipV 0\np + \u2206tg.\n(14)\n3. Transfer Grid to Particles. Transfer velocities back to\nparticles and update particle states.\nvn+1\np\n=\nX\ni\nvn+1\ni\nwn\nip,\nxn+1\np\n= xn\np + \u2206tvn+1\np\n,\nCn+1\np\n=\n12\n\u2206x2(b + 1)\nX\ni\nwn\nipvn+1\ni\n\u0000xn\ni \u2212 xn\np\n\u0001T ,\n\u2207vn+1\np\n=\nX\ni\nvn+1\ni\n\u2207wn\nip\nT ,\nF E, tr\np\n= (I + \u2207vn+1\np\n)F E,n,\nF E,n+1\np\n= Z(F E, tr\np\n),\n\u03c4 n+1\np\n= \u03c4(F E,n+1\np\n).\n(15)\nHere b is the B-spline degree, and \u2206x is the Eulerian grid\nspacing. The computation of the return map Z and the\nKirchhoff stress \u03c4 is outlined in Appendix B. We refer\nthe readers to [13] for the detailed derivations from the\ncontinuous conservation law to its MPM discretization.\nTable 2. Model Settings.\nScene\nFigure\nConstitutive Model\nVasedeck\nFig. 1\nFixed corotated\nFicus\nFig. 2\nFixed corotated\nFox\nFig. 3\nFixed corotated\nPlane\nFig. 3\nvon Mises\nToast\nFig. 3\nFixed corotated\nRuins\nFig. 3\nDrucker-Prager\nJam\nFig. 3\nHerschel-Bulkley\nSofa Suite\nFig. 3\nFixed corotated\nMaterials\nFig. 6\nFixed corotated\nMicrophone\nFig. 7\nNeo-Hookean\nBread\nFig. 9\nFixed corotated\nCake\nFig. 9\nHerschel-Bulkley\nCan\nFig. 9\nvon Mises\nWolf\nFig. 9\nDrucker-Prager\nTable 3. Material Parameters.\nNotation\nMeaning\nRelation to E, \u03bd\nE\nYoung\u2019s modulus\n/\n\u03bd\nPoisson\u2019s ratio\n/\n\u00b5\nShear modulus\n\u00b5 =\nE\n2(1+\u03bd)\n\u03bb\nLam\u00b4e modulus\n\u03bb =\nE\u03bd\n(1+\u03bd)(1\u22122\u03bd)\n\u03ba\nBulk modulus\n\u03ba =\nE\n3(1\u22122\u03bd)\nB. Elasticity and Plasticity Models\nWe adopt the constitutive models used in [50]. We list the\nmodels used for each scene in Tab. 2 and summarize all the\nparameters needed in discussing the constitutive models in\nTab. 3.\nIn all plasticity models used in our work, the deformation\ngradient is multiplicatively decomposed into F = F EF P\nfollowing some yield stress condition. A hyperelastic con-\nstitutive model is applied to F E to compute the Kirch-\nhoff stress \u03c4. For a pure elastic continuum, we simply take\nF E = F .\nB.1. Fixed Corotated Elasticity\nThe Kirchhoff stress \u03c4 is defined as\n\u03c4 = 2\u00b5(F E \u2212 R)F ET + \u03bb(J \u2212 1)J,\n(16)\nwhere R = UV T and F E = U\u03a3V T is the singular value\ndecomposition of elastic deformation gradient. J is the de-\nterminant of F E [12].\nB.2. StVK Elasticity\nThe Kirchhoff stress \u03c4 is defined as\n\u03c4 = U (2\u00b5\u03f5 + \u03bb sum(\u03f5)1) V T ,\n(17)\nwhere \u03f5 = log(\u03a3) and F E = U\u03a3V T [16].\n11\nStatic\nPhysics-based Dynamics\nFigure 9. Additional Evaluation. Examples from top to bottom are: vasedeck (elastic entity), bread (fracture), cake (viscoplastic material),\ncan (metal) and wolf (granular material).\nB.3. Neo-Hookean Elasticity\nThe Kirchhoff stress \u03c4 is defined as\n\u03c4 = \u00b5(F EF ET \u2212 I) + log(J)I,\n(18)\nwhere J is the determinant of F E [12].\nB.4. Drucker-Prager Plasticity\nThe return mapping of Drucker-Prager plasticity for sand\n[16] is, given F = U\u03a3V T and \u03f5 = log(\u03a3),\nF E = UZ(\u03a3)V T ,\n(19)\nZ(\u03a3) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1,\nsum(\u03f5) > 0,\n\u03a3,\n\u03b4\u03b3 \u2264 0, and sum(\u03f5) \u2264 0,\nexp\n\u0010\n\u03f5 \u2212 \u03b4\u03b3\n\u02c6\u03f5\n\u2225\u02c6\u03f5\u2225\n\u0011\n,\notherwise.\n(20)\nHere \u03b4\u03b3 = \u2225\u02c6\u03f5\u2225 + \u03b1 (d\u03bb+2\u00b5) sum(\u03f5)\n2\u00b5\n, \u03b1 =\nq\n2\n3\n2 sin \u03d5f\n3\u2212sin \u03d5f , and\n\u03d5f is the friction angle. \u02c6\u03f5 = dev(\u03f5).\nB.5. von Mises Plasticity\nSimilar to Drucker-Prager plasticity, given F = U\u03a3V T\nand \u03f5 = log(\u03a3),\nF E = UZ(\u03a3)V T ,\n12\nwhere\nZ(\u03a3) =\n(\n\u03a3,\n\u03b4\u03b3 \u2264 0,\nexp\n\u0010\n\u03f5 \u2212 \u03b4\u03b3\n\u02c6\u03f5\n\u2225\u03f5\u2225\n\u0011\n,\notherwise,\n(21)\nand \u03b4\u03b3 = \u2225\u02c6\u03f5\u2225F \u2212 \u03c4Y\n2\u00b5 . Here \u03c4Y is the yield stress.\nB.6. Herschel-Bulkley Plasticity\nWe follow Yue et al. [49] and take the simple case where\nh = 1. Denote strial = dev(\u03c4 trial), and strial = ||strial||. The\nyield condition is \u03a6(s) = s \u2212\nq\n2\n3\u03c3Y \u2264 0. If it is violated,\nwe modify strial by\ns = strial \u2212\n \nstrial \u2212\nr\n2\n3\u03c3Y\n!\n/\n\u0012\n1 +\n\u03b7\n2\u00b5\u2206t\n\u0013\n.\ns can then be recovered as s = s \u00b7\nstrial\n||strial||. Define bE =\nF EF ET . The Kirchhoff stress \u03c4 is computed as\n\u03c4 = \u03ba\n2\n\u0000J2 \u2212 1\n\u0001\nI + \u00b5 dev\nh\ndet(bE)\u2212 1\n3 bEi\n.\nC. Additional Evaluations\nWe present additional evaluations of our method in Fig. 9.\nThe vasedeck data is from the Nerf dataset [23] and the oth-\ners are synthetic data, generated using BlenderNeRF [32].\n13\n"
  },
  {
    "title": "Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models",
    "link": "https://arxiv.org/pdf/2311.12092.pdf",
    "upvote": "19",
    "text": "Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models\nRohit Gandikota1\nJoanna Materzy\u00b4nska2\nTingrui Zhou3\nAntonio Torralba2\nDavid Bau1\n1Northeastern University\n2Massachusetts Institute of Technology\n3 Independent Researcher\nRaw             Slider from Text        Cooked \nSlider for Fixing Hands \nSlider Transferred from StyleGAN\n- \n+ \nSlider Created by Artist + \n- \n+ \n+ \nFigure 1. Given a small set of text prompts or paired image data, our method identifies low-rank directions in diffusion parameter space\nfor targeted concept control with minimal interference to other attributes. These directions can be derived from pairs of opposing textual\nconcepts or artist-created images, and they are composable for complex multi-attribute control. We demonstrate the effectivness of our\nmethod by fixing distorted hands in Stable Diffusion outputs and transferring disentangled StyleGAN latents into diffusion models.\nAbstract\nWe present a method to create interpretable concept slid-\ners that enable precise control over attributes in image gen-\nerations from diffusion models. Our approach identifies a\nlow-rank parameter direction corresponding to one concept\nwhile minimizing interference with other attributes. A slider\nis created using a small set of prompts or sample images;\nthus slider directions can be created for either textual or\nvisual concepts. Concept Sliders are plug-and-play: they\ncan be composed efficiently and continuously modulated, en-\nabling precise control over image generation. In quantitative\nexperiments comparing to previous editing techniques, our\nsliders exhibit stronger targeted edits with lower interference.\nWe showcase sliders for weather, age, styles, and expressions,\nas well as slider compositions. We show how sliders can\ntransfer latents from StyleGAN for intuitive editing of vi-\nsual concepts for which textual description is difficult. We\nalso find that our method can help address persistent quality\nissues in Stable Diffusion XL including repair of object de-\nformations and fixing distorted hands. Our code, data, and\ntrained sliders are available at sliders.baulab.info\n1. Introduction\nArtistic users of text-to-image diffusion models [4, 9, 19,\n36, 37] often need finer control over the visual attributes\nand concepts expressed in a generated image than currently\npossible. Using only text prompts, it can be challenging to\nprecisely modulate continuous attributes such as a person\u2019s\nage or the intensity of the weather, and this limitation hinders\ncreators\u2019 ability to adjust images to match their vision [43].\nIn this paper, we address these needs by introducing in-\nterpretable Concept Sliders that allow nuanced editing of\nconcepts within diffusion models. Our method empowers\ncreators with high-fidelity control over the generative pro-\ncess as well as image editing. Our code and trained sliders\nwill be open sourced.\nConcept Sliders solve several problems that are not well-\naddressed by previous methods. Direct prompt modifica-\ntion can control many image attributes, but changing the\nprompt often drastically alters overall image structure due\nto the sensitivity of outputs to the prompt-seed combina-\n1[gandikota.ro,davidbau]@northeastern.edu\n2[jomat, torralba]@mit.edu\n3shu_teiei@outlook.jp\n1\narXiv:2311.12092v2  [cs.CV]  27 Nov 2023\ntion [22, 38, 44].\nPost-hoc techniques such PromptTo-\nPrompt [13] and Pix2Video [3] enable editing visual con-\ncepts in an image by inverting the diffusion process and\nmodifying cross-attentions. However, those methods require\nseparate inference passes for each new concept and can sup-\nport only a limited set of simultaneous edits. They require\nengineering a prompt suitable for an individual image rather\nthan learning a simple generalizable control, and if not care-\nfully prompted, they can introduce entanglement between\nconcepts, such as altering race when modifying age (see\nAppendix). In contrast, Concept Sliders provide lightweight\nplug-and-play adaptors applied to pre-trained models that\nenable precise, continuous control over desired concepts in\na single inference pass, with efficient composition (Figure 6)\nand minimal entanglement (Figure 11).\nEach Concept Slider is a low-rank modification of the\ndiffusion model. We find that the low-rank constraint is a\nvital aspect of precision control over concepts: while finetun-\ning without low-rank regularization reduces precision and\ngenerative image quality, low-rank training identifies the\nminimal concept subspace and results in controlled, high-\nquality, disentangled editing (Figure 11). Post-hoc image\nediting methods that act on single images rather than model\nparameters cannot benefit from this low-rank framework.\nConcept Sliders also allow editing of visual concepts that\ncannot be captured by textual descriptions; this distinguishes\nit from prior concept editing methods that rely on text [7, 8].\nWhile image-based model customization methods [6, 25, 38]\ncan add new tokens for new image-based concepts, those are\ndifficult to use for image editing. In contrast, Concept Sliders\nallow an artist to provide a handful of paired images to\ndefine a desired concept, and then a Concept Slider will then\ngeneralize the visual concept and apply it to other images,\neven in cases where it would be infeasible to describe the\ntransformation in words.\nOther generative image models, such as GANs, have\npreviously exhibited latent spaces that provide highly disen-\ntangled control over generated outputs. In particular, it has\nbeen observed that StyleGAN [20] stylespace neurons offer\ndetailed control over many meaningful aspects of images\nthat would be difficult to describe in words [45]. To further\ndemonstrate the capabilities of our approach, we show that\nit is possible to create Concept Sliders that transfer latent di-\nrections from StyleGAN\u2019s style space trained on FFHQ face\nimages [20] into diffusion models. Notably, despite origi-\nnating from a face dataset, our method successfully adapts\nthese latents to enable nuanced style control over diverse\nimage generation. This showcases how diffusion models can\ncapture the complex visual concepts represented in GAN\nlatents, even those that may not correspond to any textual\ndescription.\nWe demonstrate that the expressiveness of Concept Slid-\ners is powerful enough to address two particularly practical\napplications\u2014enhancing realism and fixing hand distortions.\nWhile generative models have made significant progress in\nrealistic image synthesis, the latest generation of diffusion\nmodels such as Stable Diffusion XL [36] are still prone to\nsynthesizing distorted hands with anatomically implausible\nextra or missing fingers [31], as well as warped faces, float-\ning objects, and distorted perspectives. Through a perceptual\nuser study, we validate that a Concept Slider for \u201crealistic\nimage\u201d as well as another for \u201cfixed hands\u201d both create a\nstatistically significant improvement in perceived realism\nwithout altering image content.\nConcept Sliders are modular and composable. We find\nthat over 50 unique sliders can be composed without de-\ngrading output quality. This versatility gives artists a new\nuniverse of nuanced image control that allows them to blend\ncountless textual, visual, and GAN-defined Concept Sliders.\nBecause our method bypasses standard prompt token limits,\nit empowers more complex editing than achievable through\ntext alone.\n2. Related Works\nImage Editing\nRecent methods propose different ap-\nproaches for single image editing in text-to-image diffu-\nsion models. They mainly focus on manipulation of cross-\nattentions of a source image and a target prompt [13, 22, 35],\nor use a conditional input to guide the image structure [30].\nUnlike those methods that are applied to a single image, our\nmodel creates a semantic change defined by a small set of\ntext pairs or image pairs, applied to the entire model. Ana-\nlyzing diffusion models through Riemannian geometry, Park\net al. [33] discovered local latent bases that enable semantic\nediting by traversing the latent space. Their analysis also\nrevealed the evolving geometric structure over timesteps\nacross prompts, requiring per-image latent basis optimiza-\ntion. In contrast, we identify generalizable parameter direc-\ntions, without needing custom optimization for each image.\nInstruct-pix2pix [1] finetunes a diffusion model to condition\nimage generation on both an input image and text prompt.\nThis enables a wide range of text-guided editing, but lacks\nfine-grained control over edit strength or visual concepts not\neasily described textually.\nGuidance Based Methods\nHo et al. [14] introduce classi-\nfier free guidance that showed improvement in image quality\nand text-image alignment when the data distribution is driven\ntowards the prompt and away from unconditional output. Liu\net al. [28] present an inference-time guidance formulation\nto enhance concept composition and negation in diffusion\nmodels. By adding guidance terms during inference, their\nmethod improves on the limited inherent compositionality\nof diffusion models. SLD [40] proposes using guidance to\nmoderate unsafe concepts in diffusion models. They propose\na safe prompt which is used to guide the output away from\nunsafe content during inference.\n2\nModel Editing\nOur method can be seen as a model editing\napproach, where by applying a low-rank adaptor, we single\nout a semantic attribute and allow for continuous control\nwith respect to the attribute. To personalize the models for\nadding new concepts, customization methods based on fine-\ntuning exist [6, 25, 38]. Custom Diffusion [25] proposes a\nway to incorporate new visual concepts into pretrained diffu-\nsion models by finetuning only the cross-attention layers. On\nthe other hand, Textual Inversion [6] introduces new textual\nconcepts by optimizing an embedding vector to activate de-\nsired model capabilities. Previous works [7, 12, 23, 24, 46]\nproposed gradient based fine-tuning-based methods for the\npermanent erasure of a concept in a model. Ryu et al. [39]\nproposed adapting LoRA [16] for diffusion model customiza-\ntion. Recent works [47] developed low rank implementa-\ntions of erasing concepts [7] allowing the ability to adjust\nthe strength of erasure in an image. [17] implemented image\nbased control of concepts by merging two overfitted LoRAs\nto capture an edit direction. Similarly, [8, 32] proposed\nclosed-form formulation solutions for debiasing, redacting\nor moderating concepts within the model\u2019s cross-attention\nweights. Our method does not modify the underlying text-to-\nimage diffusion model and can be applied as a plug-and-play\nmodule easily stacked across different attributes.\nSemantic Direction in Generative models\nIn Generative\nAdversarial Networks (GANs), manipulation of semantic\nattributes has been widely studied. Latent space trajectories\nhave been found in a self-supervised manner [18]. PCA has\nbeen used to identify semantic directions in the latent or\nfeature spaces [11]. Latent subspaces corresponding to de-\ntailed face attributes have been analyzed [42]. For diffusion\nmodels, semantic latent spaces have been suggested to exist\nin the middle layers of the U-Net architecture [26, 34]. It\nhas been shown that principal directions in diffusion model\nlatent spaces (h-spaces) capture global semantics [10]. Our\nmethod directly trains low-rank subspaces corresponding\nto semantic attributes. By optimizing for specific global di-\nrections using text or image pairs as supervision, we obtain\nprecise and localized editing directions. Recent works have\n[49] introduced the low-rank representation adapter, which\nemploys a contrastive loss to fine-tune LoRA to achieve\nfine-grained control of concepts in language models.\n3. Background\n3.1. Diffusion Models\nDiffusion models are a subclass of generative models that\noperationalize the concept of reversing a diffusion process\nto synthesize data. Initially, the forward diffusion process\ngradually adds noise to the data, transitioning it from an\norganized state x0 to a complete Gaussian noise xT . At any\ntimestep t, the noised image is modelled as:\nxt \u2190\np\n1 \u2212 \u03b2tx0 +\np\n\u03b2t\u03f5\n(1)\nWhere \u03f5 is a randomly sampled gaussian noise with zero\nmean and unit variance. Diffusion models aim to reverse\nthis diffusion process by sampling a random Gaussian noise\nXT and gradually denoising the image to generate an image\nx0. In practice [15, 29], the objective of diffusion model is\nsimplified to predicting the true noise \u03f5 from Eq. 1 when xt\nis fed as input with additional inputs like the timestep t and\nconditioning c.\n\u2207\u03b8||\u03f5 \u2212 \u03f5\u03b8(xt, c, t)||2\n(2)\nWhere \u03f5\u03b8(xt, c, t) is the noise predicted by the diffusion\nmodel conditioned on c at timestep t. In this work, we work\nwith Stable Diffusion [37] and Stable Diffusion XL [36],\nwhich are latent diffusion models that improve efficiency\nby operating in a lower dimensional latent space z of a pre-\ntrained variational autoencoder. They convert the images\nto a latent space and run the diffusion training as discussed\nabove. Finally, they decode the latent z0 through the VAE\ndecoder to get the final image x0\n3.2. Low-Rank Adaptors\nThe Low-Rank Adaptation (LoRA) [16] method enables\nefficient adaptation of large pre-trained language models to\ndownstream tasks by decomposing the weight update \u2206W\nduring fine-tuning. Given a pre-trained model layer with\nweights W0 \u2208 Rd\u00d7k, where d is the input dimension and k\nthe output dimension, LoRA decomposes \u2206W as\n\u2206W = BA\n(3)\nwhere B \u2208 Rd\u00d7r and A \u2208 Rr\u00d7k with r \u226a min(d, k) being\na small rank that constrains the update to a low dimensional\nsubspace. By freezing W0 and only optimizing the smaller\nmatrices A and B, LoRA achieves massive reductions in\ntrainable parameters. During inference, \u2206W can be merged\ninto W0 with no overhead by a LoRA scaling factor \u03b1:\nW = W0 + \u03b1\u2206W\n(4)\n4. Method\nConcept Sliders are a method for fine-tuning LoRA adap-\ntors on a diffusion model to enable concept-targeted image\ncontrol as shown in Figure 2. Our method learns low-rank\nparameter directions that increase or decrease the expression\nof specific attributes when conditioned on a target concept.\nGiven a target concept ct and model \u03b8, our goal is to obtain\n\u03b8\u2217 that modifies the likelihood of attributes c+ and c\u2212 in\nimage X when conditioned on ct - increase likelihood of\nattribute c+ and decrease likelihood of attribute c\u2212.\nP\u03b8\u2217(X|ct) \u2190 P\u03b8(X|ct)\n\u0012P\u03b8(c+|X)\nP\u03b8(c\u2212|X)\n\u0013\u03b7\n(5)\n3\n(xt, ct/+/\u2212, t)\n\u03f5\u03b8*(xt, ct, t) + \u03b7[\u03f5\u03b8*(xt, c+, t) \u2212 \u03f5\u03b8*(xt, c\u2212t)]\nLoRA Slider \nParameters \u03b8\nL2 \nLoss\nFrozen \nOriginal SD\n\u03b8*\n\u03f5\u03b8(xt, ct, t)\nTarget concept\nct\nAttribute to \nenhance\nc+\nAttribute to \nsuppress\nc\u2212\nInference without slider\nInference with slider\n\u201cPerson\u201d\n\u201cOld\u201d\n \u201cYoung\u201d\nFigure 2. Concept Sliders are created by fine-tuning LoRA adaptors\nusing a guided score that enhances attribute c+ while suppressing\nattribute c\u2212 from the target concept ct. The slider model generates\nsamples xt by partially denoising Gaussian noise over time steps 1\nto t, conditioned on the target concept ct.\nWhere P\u03b8(X|ct) represents the distribution generated by\nthe original model when conditioned on ct. Expanding\nP(c+|X) = P (X|c+)P (c+)\nP (X)\n, the gradient of the log proba-\nbility \u2207 log P\u03b8\u2217(X|ct) would be proportional to:\n\u2207 log P\u03b8(X|ct) + \u03b7 (\u2207 log P\u03b8(X|c+) \u2212 \u2207 log P\u03b8(X|c\u2212))\n(6)\nBased on Tweedie\u2019s formula [5] and the reparametriza-\ntion trick of [15], we can introduce a time-varying noising\nprocess and express each score (gradient of log probability)\nas a denoising prediction \u03f5(X, ct, t). Thus Eq. 6 becomes:\n\u03f5\u03b8\u2217(X, ct, t) \u2190 \u03f5\u03b8(X, ct, t) +\n\u03b7 (\u03f5\u03b8(X, c+, t) \u2212 \u03f5\u03b8(X, c\u2212, t))\n(7)\nThe proposed score function in Eq. 7 shifts the distribu-\ntion of the target concept ct to exhibit more attributes of\nc+ and fewer attributes of c\u2212. In practice, we notice that a\nsingle prompt pair can sometimes identify a direction that is\nentangled with other undesired attributes. We therefore in-\ncorporate a set of preservation concepts p \u2208 P (for example,\nrace names while editing age) to constrain the optimization.\nInstead of simply increasing P\u03b8(c+|X), we aim to increase,\nfor every p, P\u03b8((c+, p)|X), and reduce P\u03b8((c\u2212, p)|X). This\nleads to the disentanglement objective:\n\u03f5\u03b8\u2217(X, ct, t) \u2190 \u03f5\u03b8(X, ct, t) +\n\u03b7\nX\np\u2208P\n(\u03f5\u03b8(X, (c+, p), t) \u2212 \u03f5\u03b8(X, (c\u2212, p), t))\n(8)\nThe disentanglement objective in Equation 8 finetunes the\nConcept Slider modules while keeping pre-trained weights\n\u201cMakeup\u201d\n+ \n- \n\u201cAge\u201d\n+ \n- \n\u201cSmiling\u201d\n+ \n- \nFigure 3. Our text-based sliders allow precise editing of desired\nattributes during image generation while maintaining the overall\nstructure. Traversing the sliders towards the negative direction\nproduces an opposing effect on the attributes.\nfixed. Crucially, the LoRA formulation in Equation 4 intro-\nduces a scaling factor \u03b1 that can be modified at inference\ntime. This scaling parameter \u03b1 allows adjusting the strength\nof the edit, as shown in Figure 1. Increasing \u03b1 makes the\nedit stronger without retraining the model. Previous model\nediting method [7], suggests a stronger edit by retraining\nwith increased guidance \u03b7 in Eq. 8. However, simply scaling\n\u03b1 at inference time produces the same effect of strengthening\nthe edit, without costly retraining.\n4.1. Learning Visual Concepts from Image Pairs\nWe propose sliders to control nuanced visual concepts that\nare harder to specify using text prompts. We leverage small\npaired before/after image datasets to train sliders for these\nconcepts. The sliders learn to capture the visual concept\nthrough the contrast between image pairs (xA, xB).\nOur training process optimizes the LORA applied in both\nthe negative and positive directions. We shall write \u03f5\u03b8+ for\nthe application of positive LoRA and \u03f5\u03b8\u2212 for the negative\ncase. Then we minimize the following loss:\n||\u03f5\u03b8\u2212(xA\nt , \u2018 \u2019, t) \u2212 \u03f5||2 + ||\u03f5\u03b8+(xB\nt , \u2018 \u2019, t) \u2212 \u03f5||2\n(9)\nThis has the effect of causing the LORA to align to a direc-\ntion that causes the visual effect of A in the negative direction\nand B in the positive direction. Defining directions visually\nin this way not only allows an artist to define a Concept\nSlider through custom artwork; it is also the same method\nwe use to transfer latents from other generative models such\nas StyleGAN.\n5. Experiments\nWe evaluate our approach primarily on Stable Diffusion XL\n[36], a high-resolution 1024-pixel model, and we conduct ad-\n4\nPrompt2Prompt\nOur Method\nComposition\n\u2206 CLIP\nLPIPS\n\u2206 CLIP\nLPIPS\n\u2206 CLIP\nLPIPS\nAge\n1.10\n0.15\n3.93\n0.06\n3.14\n0.13\nHair\n3.45\n0.15\n5.59\n0.10\n5.14\n0.15\nSky\n0.43\n0.15\n1.56\n0.13\n1.55\n0.14\nRusty\n7.67\n0.25\n7.60\n0.09\n6.67\n0.18\nTable 1. Compared to Prompt2Prompt [13], our method achieves\ncomparable efficacy in terms of \u2206 CLIP score while inducing\nfiner edits as measured by LPIPS distance to the original image.\nThe \u2206 CLIP metric measures the change in CLIP score between\nthe original and edited images when evaluated on the text prompt\ndescribing the desired edit. Results are shown for a single positive\nscale of the trained slider.\nditional experiments on SD v1.4 [37]. All models are trained\nfor 500 epochs. We demonstrate generalization by testing\nsliders on diverse prompts - for example, we evaluate our\n\"person\" slider on prompts like \"doctor\", \"man\", \"woman\",\nand \"barista\". For inference, we follow the SDEdit technique\nof Meng et al. [30]: to maintain structure and semantics, we\nuse the original pre-trained model for the first t steps, set-\nting the LoRA adaptor multipliers to 0 and retaining the\npre-trained model priors. We then turn on the LoRA adaptor\nfor the remaining steps.\n5.1. Textual Concept Sliders\nWe validate the efficacy of our slider method on a diverse\nset of 30 text-based concepts, with full examples in the Ap-\npendix. Table 1 compares our method against two baselines:\nan approach we propose inspired by SDEdit [30] and Liu et\nal.[28] that uses a pretrained model with the standard prompt\nfor t timesteps, then starts composing by adding prompts to\nsteer the image, and prompt2prompt[13], which leverages\ncross-attention for image editing after generating reference\nimages. While the former baseline is novel, all three en-\nable finer control but differ in how edits are applied. Our\nmethod directly generates 2500 edited images per concept,\nlike \"image of a person\", by setting the scale parameter at\ninference. In contrast, the baselines require additional infer-\nence passes for each new concept (e.g \"old person\"), adding\ncomputational overhead. Our method consistently achieves\nhigher CLIP scores and lower LPIPS versus the original,\nindicating greater coherence while enabling precise control.\nThe baselines are also more prone to entanglement between\nconcepts. We provide further analysis and details about the\nbaselines in the Appendix.\nFigure 3 shows typical qualitative examples, which main-\ntains good image structure while enabling fine grained edit-\ning of the specified concept.\n5.2. Visual Concept Sliders\nSome visual concepts like precise eyebrow shapes or eye\nsizes are challenging to control through text prompts alone.\nEyebrow Shape + \nEye Size\n+ \nFigure 4. Controlling fine-grained attributes like eyebrow shape\nand eye size using image pair-driven concept sliders with optional\ntext guidance. The eye size slider scales from small to large eyes\nusing the Ostris dataset [2].\nTraining\nCustom\nTextual\nOur\nData\nDiffusion\nInversion\nMethod\n\u2206eye\n1.84\n0.97\n0.81\n1.75\nLPIPS\n0.03\n0.23\n0.21\n0.06\nTable 2. Our results demonstrate the effectiveness of our sliders\nfor intuitive image editing based on visual concepts. The metric\n\u2206eye represents the ratio of change in eye size compared to the\noriginal image. Our method achieves targeted editing of eye size\nwhile maintaining similarity to the original image distribution, as\nmeasured by the LPIPS.\nTo enable sliders for these granular attributes, we leverage\npaired image datasets combined with optional text guidance.\nAs shown in Figure 4, we create sliders for \"eyebrow shape\"\nand \"eye size\" using image pairs capturing the desired trans-\nformations. We can further refine the eyebrow slider by\nproviding the text \"eyebrows\" so the direction focuses on\nthat facial region. Using image pairs with different scales,\nlike the eye sizes from Ostris [2], we can create sliders with\nstepwise control over the target attribute.\nWe quantitatively evaluate the eye size slider by detecting\nfaces using FaceNet [41], cropping the area, and employing\na face parser [48]to measure eye region across the slider\nrange. Traversing the slider smoothly increases the average\neye area 2.75x, enabling precise control as shown in Table\n2. Compared to customization techniques like textual inver-\nsion [6] that learns a new token and custom diffusion [25]\nthat fine-tunes cross attentions, our slider provides more\ntargeted editing without unwanted changes. When model\nediting methods [6, 25] are used to incorporate new visual\nconcepts, they memorize the training subjects rather than\ngeneralizing the contrast between pairs. We provide more\ndetails in the Appendix.\n5\n+ \n- \nSlider Transferred from StyleGAN-v3 \n Layer 4 - Neuron 77 + \n- \nLayer 4 - Neuron 847\nFigure 5. We demonstrate transferring StyleGAN style space latents\nto the diffusion latent space. We identify three neurons that edit\nfacial structure: neuron 77 controls cheekbone structure, neuron\n646 selectively adjusts the left side face width, and neuron 847\nedits inter-ocular distance. We transfer these StyleGAN latents to\nthe diffusion model to enable structured facial editing.\n5.3. Sliders transferred from StyleGAN\nFigure 5 demonstrates sliders transferred\nfrom the\nStyleGAN-v3 [21] style space that is trained on FFHQ [20]\ndataset. We use the method of [45] to explore the StyleGAN-\nv3 style space and identify neurons that control hard-to-\ndescribe facial features. By scaling these neurons, we collect\nimages to train image-based sliders. We find that Stable\nDiffusion\u2019s latent space can effectively learn these Style-\nGAN style neurons, enabling structured facial editing. This\nenables users to control nuanced concepts that are indescrib-\nable by words and styleGAN makes it easy to get generate\nthe paired dataset.\n5.4. Composing Sliders\nA key advantage of our low-rank slider directions is com-\nposability - users can combine multiple sliders for nuanced\ncontrol rather than being limited to one concept at a time.\nFor example, in Figure 6 we show blending \"cooked\" and\n\"fine dining\" food sliders to traverse this 2D concept space.\nSince our sliders are lightweight LoRA adaptors, they are\neasy to share and overlay on diffusion models. By download-\ning interesting slider sets, users can adjust multiple knobs\nsimultaneously to steer complex generations. In Figure 7 we\nqualitatively show the effects of composing multiple sliders\nprogressively up to 50 sliders at a time. We use far greater\nthan 77 tokens (the current context limit of SDXL [36]) to\ncreate these 50 sliders. This showcases the power of our\nmethod that allows control beyond what is possible through\nprompt-based methods alone. We further validate multi-\nslider composition in the appendix.\n6. Concept Sliders to Improve Image Quality\nOne of the most interesting aspects of a large-scale gener-\native model such as Stable Diffusion XL is that, although\nCooked\nFine Dining\n+\n-\n-\n+\nFigure 6. Composing two text-based sliders results in a complex\ncontrol over food images. We show the effect of applying both\nthe \"cooked\" slider and \"fine-dining\" slider to a generated image.\nThese sliders can be used in both positive and negative directions.\n50 sliders\n+ Small Eyes\n10 sliders\n+ Pixar\n+ Professional\n10 sliders\n50 sliders\nOriginal SD\nOriginal SD\n+ Clay\nFigure 7. We show composition capabilities of concept sliders.\nWe progressively compose multiple sliders in each row from left\nto right, enabling nuanced traversal of high-dimensional concept\nspaces.\nWe demonstrate composing sliders trained from text\nprompts, image datasets, and transferred from GANs.\ntheir image output can often suffer from distortions such\nas warped or blurry objects, the parameters of the model\ncontains a latent capability to generate higher-quality output\nwith fewer distortions than produced by default. Concept\nSliders can unlock these abilities by identifying low-rank\nparameter directions that repair common distortions.\nFixing Hands\nGenerating realistic-looking hands is a per-\nsistent challenge for diffusion models: for example, hands\nare typically generated with missing, extra, or misplaced\nfingers. Yet the tendency to distort hands can be directly\ncontrolled by a Concept Slider: Figure 9 shows the effect\n6\nRepair Slider\n+ \n(a)\n(b)\n(c)\n(d)\nFigure 8. The repair slider enables the model to generate images that are more realistic and undistorted. The parameters under the control of\nthis slider help the model correct some of the flaws in their generated outputs like distorted humans and pets in (a, b), unnatural objects in (b,\nc, d), and blurry natural images in (b,c)\nFixing Hands \n+ \nFigure 9. We demonstrate a slider for fixing hands in stable diffu-\nsion. We find a direction to steer hands to be more realistic and\naway from \"poorly drawn hands\".\nof a \"fix hands\" Concept Slider that lets users smoothly ad-\njust images to have more realistic, properly proportioned\nhands. This parameter direction is found using a complex\nprompt pair boosting \u201crealistic hands, five fingers, 8k hyper-\nrealistic hands\u201d and suppressing \u201cpoorly drawn hands, dis-\ntorted hands, misplaced fingers\u201d. This slider allows hand\nquality to be improved with a simple tweak rather manual\nprompt engineering.\nTo measure the \u201cfix hands\" slider, we conduct a user\nstudy on Amazon Mechanical Turk. We present 300 random\nimages with hands to raters\u2014half generated by Stable Diffu-\nsion XL and half by XL with our slider applied (same seeds\nand prompts). Raters are asked to assess if the hands appear\ndistorted or not. Across 150 SDXL images, raters find 62%\nhave distorted hands, confirming it as a prevalent problem.\nIn contrast, only 22% of the 150 slider images are rated as\nhaving distorted hands.\nRepair Slider\nIn addition to controlling specific concepts\nlike hands, we also demonstrate the use of Concept Sliders to\nguide generations towards overall greater realism. We iden-\ntify single low-rank parameter direction that shifts images\naway from common quality issues like distorted subjects, un-\nSDXL\nRepair Slider\nOriginal\nRepair\nFigure 10. We demonstrate the effect of our \u201crepair\u201d slider on fine\ndetails: it improves the rendering of densely arranged objects, it\nstraightens architectural lines, and it avoids blurring and distortions\nat the edges of complex shapes.\nnatural object placement, and inconsistent shapes. As shown\nin Figures 8 and 10, traversing this \u201crepair\" slider noticeably\nfixes many errors and imperfections.\nThrough a perceptual study, we evaluate the realism of\n250 pairs of slider-adjusted and original SD images. A ma-\njority of participants rate the slider images as more realistic\nin 80.39% of pairs, indicating our method enhances real-\nism. However, FID scores do not align with this human\n7\nassessment, echoing prior work on perceptual judgment gaps\n[27]. Instead, distorting images along the opposite slider\ndirection improves FID, though users still prefer the realism-\nenhancing direction. We provide more details about the user\nstudies in the appendix.\n7. Ablations\nWe analyze the two key components of our method to ver-\nify that they are both necessary: (1) the disentanglement\nformulation and (2) low-rank adaptation. Table 3 shows\nquantitative measures on 2500 images, and Figure 11 shows\nqualitative differences. In both quantitative and quantita-\ntive measures, we find that the disentanglement objective\nfrom Eq.8 success in isolating the edit from unwanted at-\ntributes (Fig.11.c); for example without this objective we\nsee undesired changes in gender when asking for age as\nseen in Table 3, Interference metric which measures the per-\ncentage of samples with changed race/gender when making\nthe edit. The low-rank constraint is also helpful: it has the\neffect of precisely capturing the edit direction with better\ngeneralization (Fig.11.d); for example, note how the back-\nground and the clothing are better preserved in Fig.11.b.\nSince LORA is parameter-efficient, it also has the advantage\nthat it enables lightweight modularity. We also note that\nthe SDEdit-inspired inference technique allows us to use a\nwider range of alpha values, increasing the editing capac-\nity, without losing image structure. We find that SDEdit\u2019s\ninference technique expands the usable range of alpha be-\nfore coherence declines relative to the original image. We\nprovide more details in the Appendix.\nw/o\nw/o\nOurs Disentanglement Low Rank\n\u2206CLIP\n3.93\n3.39\n3.18\nLPIPS\n0.06\n0.17\n0.23\nInterference\n0.10\n0.36\n0.19\nTable 3. The disentanglement formulation enables precise control\nover the age direction, as shown by the significant reduction in\nthe Interference metric which measures the percentage of samples\nwith gender/race change, compared to the original images. By\nusing LoRA adaptors, sliders achieve finer editing in terms of\nboth structure and edit direction, as evidenced by improvements\nin LPIPS and Interference. Concept strength is maintained, with\nsimilar \u2206CLIP scores across ablations.\n8. Limitations\nWhile the disentanglement formulation reduces unwanted\ninterference between edits, we still observe some residual\neffects as shown in Table 3 for our sliders. This highlights\nthe need for more careful selection of the latent directions\nto preserve, preferably an automated method, in order to\nOriginal SD\nw/o  \nDisentanglement\nOur Sliders\nw/o Low Rank\nAge\nProfessional\n(a)\n(b)\n(c)\n(d)\nFigure 11. The disentanglement objective (Eq. 8) helps avoid\nundesired attribute changes like change in race or gender when\nediting age. The low-rank constraint enables a precise edit.\nfurther reduce edit interference. Further study is required\nto determine the optimal set of directions that minimizes\ninterference while retaining edit fidelity. We also observe\nthat while the inference SDEdit technique helps preserve\nimage structure, it can reduce edit intensity compared to the\ninference-time method, as shown in Table 1. The SDEdit\napproach appears to trade off edit strength for improved\nstructural coherence. Further work is needed to determine\nif the edit strength can be improved while maintaining high\nfidelity to the original image.\n9. Conclusion\nConcept Sliders are a simple and scalable new paradigm\nfor interpretable control of diffusion models. By learning\nprecise semantic directions in latent space, sliders enable in-\ntuitive and generalized control over image concepts. The ap-\nproach provides a new level of flexiblilty beyond text-driven,\nimage-specific diffusion model editing methods, because\nConcept Sliders allow continuous, single-pass adjustments\nwithout extra inference. Their modular design further en-\nables overlaying many sliders simultaneously, unlocking\ncomplex multi-concept image manipulation.\nWe have demonstrated the versatility of Concept Sliders\nby measuring their performance on Stable Diffusion XL and\nStable Diffusion 1.4. We have found that sliders can be\ncreated from textual descriptions alone to control abstract\nconcepts with minimal interference with unrelated concepts,\noutperforming previous methods. We have demonstrated\nand measured the efficacy of sliders for nuanced visual con-\ncepts that are difficult to describe by text, derived from small\nartist-created image datasets. We have shown that Concept\nSliders can be used to transfer StyleGAN latents into diffu-\nsion models. Finally, we have conducted a human study that\nverifies the high quality of Concept Sliders that enhance and\ncorrect hand distortions. Our code and data will be made\npublicly available.\n8\nAcknowledgments\nWe thank Jaret Burkett (aka Ostris) for the continued discus-\nsion on the image slider method and for sharing their eye size\ndataset. RG and DB are supported by Open Philanthropy.\nCode\nOur methods are available as open-source code. Source\ncode, trained sliders, and data sets for reproducing\nour results can be found at sliders.baulab.info and at\nhttps://github.com/rohitgandikota/sliders.\nReferences\n[1] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022. 2\n[2] Jarret Burkett. Ostris/ai-toolkit: Various ai scripts. mostly\nstable diffusion stuff., 2023. 5\n[3] Duygu Ceylan, Chun-Hao Huang, and Niloy J. Mitra.\nPix2video: Video editing using image diffusion. In Inter-\nnational Conference on Computer Vision (ICCV), 2023. 2\n[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural information\nprocessing systems, 34:8780\u20138794, 2021. 1\n[5] Bradley Efron. Tweedie\u2019s formula and selection bias. Journal\nof the American Statistical Association, 106(496):1602\u20131614,\n2011. 4\n[6] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit H Bermano, Gal Chechik, and Daniel Cohen-Or. An\nimage is worth one word: Personalizing text-to-image genera-\ntion using textual inversion. arXiv preprint arXiv:2208.01618,\n2022. 2, 3, 5\n[7] Rohit Gandikota,\nJoanna Materzy\u00b4nska,\nJaden Fiotto-\nKaufman, and David Bau. Erasing concepts from diffusion\nmodels. In Proceedings of the 2023 IEEE International Con-\nference on Computer Vision, 2023. 2, 3, 4\n[8] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna\nMaterzy\u00b4nska, and David Bau. Unified concept editing in dif-\nfusion models. IEEE/CVF Winter Conference on Applications\nof Computer Vision, 2024. 2, 3\n[9] Google. Imagen, unprecedented photorealism x deep level of\nlanguage understanding, 2022. 1\n[10] Ren\u00e9 Haas, Inbar Huberman-Spiegelglas, Rotem Mulayoff,\nand Tomer Michaeli. Discovering interpretable directions in\nthe semantic latent space of diffusion models. arXiv preprint\narXiv:2303.11073, 2023. 3\n[11] Erik H\u00e4rk\u00f6nen, Aaron Hertzmann, Jaakko Lehtinen, and Syl-\nvain Paris. Ganspace: Discovering interpretable gan controls.\nAdvances in neural information processing systems, 33:9841\u2013\n9850, 2020. 3\n[12] Alvin Heng and Harold Soh. Selective amnesia: A continual\nlearning approach to forgetting in deep generative models.\narXiv preprint arXiv:2305.10120, 2023. 3\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2, 5\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 3, 4\n[16] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021. 3\n[17] Norm Inui. Sd/sdxl tricks beneath the papers and codes, 2023.\n3\n[18] Ali Jahanian, Lucy Chai, and Phillip Isola. On the\" steer-\nability\" of generative adversarial networks. arXiv preprint\narXiv:1907.07171, 2019. 3\n[19] et al James Betker. Improving image generation with better\ncaptions. OpenAI Reports, 2023. 1\n[20] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nProceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 4401\u20134410, 2019. 2, 6\n[21] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. Advances in Neural Infor-\nmation Processing Systems, 34:852\u2013863, 2021. 6\n[22] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. arXiv\npreprint arXiv:2210.09276, 2022. 2\n[23] Sanghyun Kim, Seohyeon Jung, Balhae Kim, Moonseok Choi,\nJinwoo Shin, and Juho Lee. Towards safe self-distillation of\ninternet-scale text-to-image diffusion models. arXiv preprint\narXiv:2307.05977, 2023. 3\n[24] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shecht-\nman, Richard Zhang, and Jun-Yan Zhu. Ablating concepts in\ntext-to-image diffusion models. In International Conference\non Computer Vision (ICCV), 2023. 3\n[25] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-\nman, and Jun-Yan Zhu. Multi-concept customization of text-\nto-image diffusion. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2023. 2, 3, 5\n[26] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space. arXiv preprint\narXiv:2210.10960, 2022. 3\n[27] Tuomas Kynk\u00e4\u00e4nniemi, Tero Karras, Miika Aittala, Timo\nAila,\nand Jaakko Lehtinen.\nThe role of imagenet\nclasses in fr\\\u2019echet inception distance.\narXiv preprint\narXiv:2203.06026, 2022. 8\n[28] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and\nJoshua B Tenenbaum.\nCompositional visual genera-\ntion with composable diffusion models.\narXiv preprint\narXiv:2206.01714, 2022. 2, 5\n[29] Calvin Luo.\nUnderstanding diffusion models: A unified\nperspective. arXiv preprint arXiv:2208.11970, 2022. 3\n9\n[30] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-\nYan Zhu, and Stefano Ermon. Sdedit: Image synthesis and\nediting with stochastic differential equations. arXiv preprint\narXiv:2108.01073, 2021. 2, 5\n[31] Mothrider. \u201ccan an ai draw hands?\u201d, 2022. 2\n[32] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. Editing\nimplicit assumptions in text-to-image diffusion models. In\nProceedings of the 2023 IEEE International Conference on\nComputer Vision, 2023. 3\n[33] Yong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo\nJo, and Youngjung Uh. Understanding the latent space of\ndiffusion models through the lens of riemannian geometry.\narXiv preprint arXiv:2307.12868, 2023. 2\n[34] Yong-Hyun Park, Mingi Kwon, Junghyo Jo, and Youngjung\nUh. Unsupervised discovery of semantic latent directions in\ndiffusion models. arXiv preprint arXiv:2302.12469, 2023. 3\n[35] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun\nLi, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-to-image\ntranslation. In ACM SIGGRAPH 2023 Conference Proceed-\nings, pages 1\u201311, 2023. 2\n[36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00fcller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 1,\n2, 3, 4, 6\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00c3\u00b6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2022. 1, 3, 5\n[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. arXiv preprint arXiv:2208.12242, 2022. 2, 3\n[39] Simo Ryu. Cloneofsimo/lora: Using low-rank adaptation to\nquickly fine-tune diffusion models.s, 2023. 3\n[40] Patrick Schramowski, Manuel Brack, Bj\u00f6rn Deiseroth, and\nKristian Kersting. Safe latent diffusion: Mitigating inap-\npropriate degeneration in diffusion models. arXiv preprint\narXiv:2211.05105, 2022. 2\n[41] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A unified embedding for face recognition and clus-\ntering. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 815\u2013823, 2015. 5\n[42] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. In-\nterpreting the latent space of gans for semantic face editing.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 3\n[43] Staffell. The sheer number of options and sliders using stable\ndiffusion is overwhelming., 2023. 1\n[44] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang.\nUncovering the disentanglement capability in text-to-image\ndiffusion models. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n1900\u20131910, 2023. 2\n[45] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace\nanalysis: Disentangled controls for stylegan image generation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12863\u201312872, 2021.\n2, 6\n[46] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and\nHumphrey Shi. Forget-me-not: Learning to forget in text-to-\nimage diffusion models. arXiv preprint arXiv:2303.17591,\n2023. 3\n[47] Tingrui Zhou. Github - p1atdev/leco: Low-rank adaptation\nfor erasing concepts from diffusion models., 2023. 3\n[48] Zllrunning. Using modified bisenet for face parsing in pytorch,\n2019. 5\n[49] Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip\nGuo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas\nMazeika, Ann-Kathrin Dombrowski, et al. Representation\nengineering: A top-down approach to ai transparency. arXiv\npreprint arXiv:2310.01405, 2023. 3\n10\nConcept Sliders: LoRA Adaptors for Precise Control in Diffusion Models\nSupplementary Material\nOld person\nYoung person\nOld asian person\nYoung asian person\nYoung black person\nOld black person\nEntangled Direction\nDisentangled Direction\nFigure 12. In this schematic we illustrate how multiple preservation\nconcepts are used to disentangle a direction. For the sake of clarity\nin figure, we show examples for just two races. In practice, we\npreserve a diversity of several protected attribute directions.\n10. Disentanglement Formulation\nWe visualize the rationale behind our disentangled formu-\nlation for sliders. When training sliders on single pair of\nprompts, sometimes the directions are entangled with unin-\ntended directions. For example, as we show show in Fig-\nure 11, controlling age can interfere with gender or race. We\ntherefore propose using multiple paired prompts for finding a\ndisentangled direction. As shown in Figure 12, we explicitly\ndefine the preservation directions (dotted blue lines) to find a\nnew edit direction (solid blue line) invariant to the preserve\nfeatures.\n11. SDEdit Analysis\nWe ablate SDEdit\u2019s contribution by fixing slider scale while\nvarying SDEdit timesteps over 2,500 images. Figure 13\nshows inverse trends between LPIPS and CLIP distances\nas SDEdit time increases. Using more SDEdit maintains\nstructure, evidenced by lower LPIPS score, while maintain-\ning lower CLIP score. This enables larger slider scales be-\nfore risking structural changes. We notice that on average,\ntimestep 750 - 850 has the best of both worlds with spatial\nstructure preservation and increased efficacy.\n12. Textual Concepts Sliders\nWe quantify slider efficacy and control via CLIP score\nchange and LPIPS distance over 15 sliders at 12 scales in\nFigure 14. CLIP score change validates concept modifi-\ncation strength. Tighter LPIPS distributions demonstrate\nprecise spatial manipulation without distortion across scales.\nWe show additional qualitative examples for textual concept\nsliders in Figures 27-32.\nTimesteps with Slider\nCLIP\nTimesteps with Slider\nLPIPS\nLittle slider effect\nLarge image \nstructure change \nFigure 13. The plot examines CLIP score change and LPIPS dis-\ntance when applying the same slider scale but with increasing\nSDEdit times. Higher timesteps enhance concept attributes con-\nsiderably per CLIP while increased LPIPS demonstrates change in\nspatial stability. On the x-axis, 0 corresponds to no slider applica-\ntion while 1000 represents switching from start.\n12.1. Baseline Details\nWe compare our method against Prompt-to-prompt and\na novel inference-time prompt composition method. For\nPrompt-to-prompt we use the official implementation code.\nWe use the Refinement strategy they propose, where new\ntoken is added to the existing prompt for image editing. For\nexample, for the images in Figure 15, we add the token\n\u201cold\u201d for the original prompt \u201cpicture of person\u201d to make\nit \u201cpicture of old person\u201d. For the compostion method, we\nuse the principles from Liu et al . Specifically, we compose\nthe score functions coming from both \u201cpicture of person\u201d\nand \u201cold person\u201d through additive guidance. We also utilize\nthe SDEdit technique for this method to allow finer image\nediting.\n12.2. Entanglement\nThe baselines are sometimes prone to interference with con-\ncepts when editing a particular concept. Table 4 shows quan-\ntitative analysis on interference while Figure 15 shows some\nqualititative examples. We find that Prompt-to-prompt and\ninference composition can sometimes change the race/gender\nwhen editing age. Our sliders with disentaglement object 8,\nshow minimal interference as seen by Interference metric,\nwhich shows the percentage samples with race or gender\nchanged out of 2500 images we tested. We also found\nthrough LPIPS metric that our method shows finer editing\nhttps://github.com/google/prompt-to-prompt/\nhttps://energy-based-model.github.io/Compositional-Visual-\nGeneration-with-Composable-Diffusion-Models/\n1\nLPIPS\nSlider Scale\nSlider Scale\nCLIP\nFigure 14. Analyzing attribute isolation efficacy vs stylistic variation for 15 slider types across 12 scales. We divide our figure into two\ncolumns. The left column contains concepts that have words for antonyms (e.g. expensive - cheap) showing symmetric CLIP score deltas\nup/down. The right column shows harder to negate sliders (e.g. no glasses) causing clipped negative range. We also note that certain sliders\nhave higher lpips, such as \u201ccluttered\u201d room slider, which intuitively makes sense.\ncapabilities. We find similar conclusions through quanlita-\ntive samples from Figure 15, that P2P and composition can\nalter gender, race or both when controlling age.\nP2P\nComposition\nOurs\n\u2206CLIP\n1.10\n3.14\n3.93\nLPIPS\n0.15\n0.13\n0.06\nInterference\n0.33\n0.38\n0.10\nTable 4. The disentanglement formulation enables precise control\nover the age direction, as shown by the significant reduction in\nthe Interference metric which measures the percentage of samples\nwith gender/race change, compared to the original images. By\nusing LoRA adaptors, sliders achieve finer editing in terms of\nboth structure and edit direction, as evidenced by improvements\nin LPIPS and Interference. Concept strength is maintained, with\nsimilar \u2206CLIP scores across ablations.\nOriginal SD\nP2P - Age\nComposition \nOur Age Slider\nFigure 15. Concept Sliders demonstrate minimal entanglement\nwhen controlling a concept. Prompt-to-prompt and inference-time\ntextual composition sometimes tend to alter race/gender when edit-\ning age.\n2\nOur \nSlider\nTextual \nInversion\nCustom \nDiffusion\nTraining \nData\nFigure 16. Concept Sliders demonstrate more diverse outputs while\nalso being effective at learning the new concepts. Customization\nmethods can sometimes tend to learn unintended concepts like hair\nand eye colors.\n13. Visual Concept\n13.1. Baseline Details\nWe compare our method to two image customization base-\nlines: custom diffusion and textual inversion . For fair\ncomparison, we use the official implementations of both,\nmodifying textual inversion to support SDXL. These base-\nlines learn concepts from concept-labeled image sets. How-\never, this approach risks entangling concepts with irrelevant\nattributes (e.g. hair, skin tone) that correlate spuriously in\nthe dataset, limiting diversity.\n13.2. Precise Concept Capturing\nFigure 16 shows non-cherry-picked customization samples\nfrom all methods trained on the large-eyes Ostris dataset .\nWhile exhibiting some diversity, samples frequently include\nirrelevant attributes correlated with large eyes in the dataset,\ne.g. blonde hair in custom diffusion, blue eyes in textual\ninversion. In contrast, our paired image training isolates\nconcepts by exposing only local attribute changes, avoiding\nspurious correlation learning.\n14. Composing Sliders\nWe show a 2 dimensional slider by composing \u201ccooked\u201d and\n\u201cfine dining\u201d food sliders in Figure 17. Next, we show pro-\ngessive composition of sliders one by one in Figures 18,19.\nFrom top left image (original SDXL), we progressively gen-\nerate images by composing a slider at each step. We show\nhow our sliders provide a semantic control over images.\nhttps://github.com/adobe-research/custom-diffusion\nhttps://github.com/rinongal/textual_inversion\nhttps://github.com/ostris/ai-toolkit\n\u201cFine Dining\u201d\n- \n+ \n- Cooked             Slider from Text        Raw \n+ \nFigure 17. Composing two text-based sliders results in a complex\ncontrol over thanksgiving food options. We show the effect of\napplying both the \"cooked\" slider and \"fine-dining\" slider to a\ngenerated image of thanksgiving dinner. These sliders can be used\nin both positive and negative directions.\nOriginal SDXL\n+Glasses\n+Smiling\n+Long Hair\n+Muscular\n+Beard\n+Surprised\n+Curly Hair\n+Chubby\n+Old\nFigure 18. Concept Sliders can be composed for a more nuanced\nand complex control over attributes in an image. From stable\ndiffusion XL image on the top left, we progressively compose a\nslider on top of the previously added stack of sliders. By the end,\nbottom right, we show the image by composing all 10 sliders.\n15. Editing Real Images\nConcept sliders can also be used to edit real images. Man-\nually engineering a prompt to generate an image similar to\nthe real image is very difficult. We use null inversion which\nfinetunes the unconditional text embedding in the classifier\nfree guidance during inference. This allows us to find the\nright setup to turn the real image as a diffusion model gener-\nated image. Figure 20 shows Concept Sliders used on real\nimages to precisely control attributes in them.\nhttps://null-text-inversion.github.io\n3\nOriginal SDXL\n+Glasses\n+Old\n+Smiling\n+Long Hair\n+Muscular\n+Beard\n+Surprised\n+Curly Hair\n+Chubby\nFigure 19. Concept Sliders can be composed for a more nuanced\nand complex control over attributes in an image. From stable\ndiffusion XL image on the top left, we progressively compose a\nslider on top of the previously added stack of sliders. By the end,\nbottom right, we show the image by composing all 10 sliders.\n+\nAge\nSmile\nLipstick\nChubby\nReal Image\nFigure 20. Concept Sliders can be used to edit real images. We\nuse null inversion method to convert real image as a diffusion\nmodel generated image. We then run our Concept Sliders on that\ngeneration to enable precise control of concepts.\n16. Sliders to Improve Image Quality\nWe provide more qualitative examples for \"fix hands\" slider\nin Figure 21. We also show additional examples for the\n\"repair\" slider in Figure 22-24\n16.1. Details about User Studies\nWe conduct two human evaluations analyzing our \u201crepair\u201d\nand \u201cfix hands\u201d sliders. For \u201cfix hands\u201d, we generate 150\nimages each from SDXL and our slider using matched seeds\nand prompts. We randomly show each image to an odd\nnumber users and have them select issues with the hands: 1)\nmisplaced/distorted fingers, 2) incorrect number of fingers,\n3) none. as shown in Figure 25 62% of the 150 SDXL\nimages have hand issues as rated by a majority of users. In\ncontrast, only 22% of our method\u2019s images have hand issues,\nvalidating effectiveness of our fine-grained control.\nSDXL\nFix Hands Slider\nFigure 21. Concept Sliders can be used to fix common distortions\nin diffusion model generated images. We demonstrate \"Fix Hands\"\nslider that can fix distorted hands.\nWe conduct an A/B test to evaluate the efficacy of our\nproposed \u201crepair\u201dit slider. The test set consists of 300 image\npairs (Fig. 26), where each pair contains an original image\nalongside the output of our method when applied to that\nimage with the same random seed. The left/right placement\nof these two images is randomized. Through an online user\nstudy, we task raters to select the image in each pair that\nexhibits fewer flaws or distortions, and to describe the rea-\nsoning behind their choice as a sanity check. For example,\none rater selected the original image in Fig. 22.a, comment-\ning that \u201cThe left side image is not realistic because the chair\nis distorted.\u201d . Similarly a user commented \u201cGiraffes heads\nare separate unlikely in other image\u201d for Fig. 23.c. Across\nall 300 pairs, our \u201crepair\u201d slider output is preferred as having\nfewer artifacts by 80.39% of raters. This demonstrates that\nthe slider effectively reduces defects relative to the original.\nWe manually filter out responses with generic comments\n(e.g., \u201cmore realistic\u201d), as the sanity check prompts raters\nfor specific reasons. After this filtering, 250 pairs remain for\nanalysis.\n4\nRepair Slider\n+ \n(a)\n(b)\n(c)\n(d)\nFigure 22. Concept Sliders can be used to fix common distortions in diffusion model generated images. The repair slider enables the model\nto generate images that are more realistic and undistorted.\nRepair Slider\n+ \n(a)\n(b)\n(c)\n(d)\nFigure 23. Concept Sliders can be used to fix common distortions in diffusion model generated images. The repair slider enables the model\nto generate images that are more realistic and undistorted.\n5\nRepair Slider\n+ \nFigure 24. Concept Sliders can be used to fix common distortions in diffusion model generated images. The repair slider enables the model\nto generate images that are more realistic and undistorted.\n6\nFigure 25. User study interface on Amazon Mechanical Turk. Users are shown images randomly sampled from either SDXL or our \u201cfix\nhands\u201d slider method, and asked to identify hand issues or mark the image as free of errors. Aggregate ratings validate localization capability\nof our finger control sliders. For the example shown above, users chose the option \u201cFingers in wrong place\u201d\n7\nFigure 26. Interface for our \"realistic\" slider user study. Users are shown an original SDXL image and the corresponding output from our\nslider, with left/right placement randomized. Users select the image they find more photorealistic and describe their rationale as a sanity\ncheck. For example, one user selected the slider image as more realistic in the shown example, commenting \u201cThe black-haired boy\u2019s face,\nright arm and left foot are distorted in right image.\u201d Another user also chose the slider output, noting \u201cThe right side image has a floating\nhead\u201d. Asking raters to give reasons aims to reduce random selections.\n8\nCurly Hair\nSurprised\nStraight Hair\nChubby\nOriginal SDXL\n+\n+\n+\n+\nFigure 27. We demonstrate the effects of modifying an image with different sliders like \u201ccurly hair\u201d, \u201csurprised\u201d, \u201cchubby\u201d. Our text-based\nsliders allow precise editing of desired attributes during image generation while maintaining the overall structure.\n9\nOriginal SDXL\nPixar\nClay\nSculpture\nHighly Detailed\nFigure 28. We demonstrate style sliders for \"pixar\", \"realistic details\", \"clay\", and \"sculpture\". Our text-based sliders allow precise editing of\ndesired attributes during image generation while maintaining the overall structure.\n10\nOriginal SDXL\nDelightful\nDark\nTropical\nWinter\nFigure 29. We demonstrate weather sliders for \"delightful\", \"dark\", \"tropical\", and \"winter\". For delightful, we notice that the model\nsometimes make the weather bright or adds festive decorations. For tropical, it adds tropical plants and trees. Finally, for winter, it adds\nsnow.\n11\nOriginal SDXL\nGlasses\nBeard\nLong Hair\nMuscular\nFigure 30. We demonstrate sliders to add attributes to people like \"glasses\", \"muscles\", \"beard\", and \"long hair\". Our text-based sliders allow\nprecise editing of desired attributes during image generation while maintaining the overall structure.\n12\nOriginal SD\nFuturistic\nDamaged\nRusty\nFigure 31. We demonstrate sliders to control attributes of vehicles like \u201crusty\u201d, \u201cfuturistic\u201d, \u201cdamaged\u201d. Our text-based sliders allow precise\nediting of desired attributes during image generation while maintaining the overall structure.\n13\nOriginal SDXL\nRoyal\nGood Interior\nModern\nFigure 32. Our sliders can also be used to control styles of furniture like \u201croyal\u201d, \u201cModern\u201d. Our text-based sliders allow precise editing of\ndesired attributes during image generation while maintaining the overall structure.\n14\n"
  },
  {
    "title": "PF-LRM: Pose-Free Large Reconstruction Model for Joint Pose and Shape Prediction",
    "link": "https://arxiv.org/pdf/2311.12024.pdf",
    "upvote": "15",
    "text": "PF-LRM:\nPOSE-FREE\nLARGE\nRECONSTRUCTION\nMODEL FOR JOINT POSE AND SHAPE PREDICTION\nPeng Wang\u2217\nAdobe Research & HKU\ntotoro97@outlook.com\nHao Tan\nAdobe Research\nhatan@adobe.com\nSai Bi\nAdobe Research\nsbi@adobe.com\nYinghao Xu\u2217\nAdobe Research & Stanford\nyhxu@stanford.edu\nFujun Luan\nAdobe Research\nfluan@adobe.com\nKalyan Sunkavalli\nAdobe Research\nsunkaval@adobe.com\nWenping Wang\nTexas A&M University\nwenping@tamu.edu\nZexiang Xu\nAdobe Research\nzexu@adobe.com\nKai Zhang\nAdobe Research\nkaiz@adobe.com\nABSTRACT\nWe propose a Pose-Free Large Reconstruction Model (PF-LRM) for reconstruct-\ning a 3D object from a few unposed images even with little visual overlap, while\nsimultaneously estimating the relative camera poses in \u223c1.3 seconds on a single\nA100 GPU. PF-LRM is a highly scalable method utilizing the self-attention blocks\nto exchange information between 3D object tokens and 2D image tokens; we pre-\ndict a coarse point cloud for each view, and then use a differentiable Perspective-\nn-Point (PnP) solver to obtain camera poses. When trained on a huge amount of\nmulti-view posed data of \u223c1M objects, PF-LRM shows strong cross-dataset gen-\neralization ability, and outperforms baseline methods by a large margin in terms\nof pose prediction accuracy and 3D reconstruction quality on various unseen eval-\nuation datasets. We also demonstrate our model\u2019s applicability in downstream\ntext/image-to-3D task with fast feed-forward inference. Our project website is at:\nhttps://totoro97.github.io/pf-lrm.\n1\nINTRODUCTION\n3D reconstruction is a classical computer vision problem, with applications spanning imaging,\nperception, and computer graphics. While both traditional photogrammetry (Barnes et al., 2009;\nSch\u00a8onberger et al., 2016; Furukawa & Ponce, 2009) and modern neural reconstruction meth-\nods (Mildenhall et al., 2020; Wang et al., 2021; Yariv et al., 2021) have made significant progress\nin high-fidelity geometry and appearance reconstruction, they rely on having images with calibrated\ncamera poses as input. These poses are typically computed using a Structure-from-Motion (SfM)\nsolver (Schonberger & Frahm, 2016; Snavely et al., 2006).\nSfM assumes dense viewpoints of the scene where input images have sufficient overlap and matching\nimage features. This is not applicable in many cases, e.g., e-commerce applications, consumer\ncapture scenarios, and dynamic scene reconstruction problems, where adding more views incurs a\nhigher cost and thus the captured views tend to be sparse and have a wide baseline (i.e., share little\noverlap). In such circumstances, SfM solvers become unreliable and tend to fail. As a result, (neural)\nreconstruction methods, including sparse methods (Niemeyer et al., 2022; Deng et al., 2022; Long\net al., 2022; Zhou & Tulsiani, 2023; Wang et al., 2023) that require accurate camera poses, cannot\nbe reliably used for such applications.\nIn this work, we present PF-LRM, a category-agnostic method for jointly predicting both camera\nposes and object shape and appearance (represented using a triplane NeRF (Chan et al., 2022; Peng\n\u2217This work is done while the author is an intern at Adobe Research.\n1\narXiv:2311.12024v2  [cs.CV]  23 Nov 2023\nGenerated/Synthetic\nReal\nFigure 1:\n(Top block) To demonstrate our model\u2019s generalizability to unseen in-the-wild images,\nwe take 2-4 unposed images from prior/concurrent 3D-aware generation work, and use our PF-\nLRM to jointly reconstruct the NeRF and estimate relative poses in a feed-forward manner. (Bottom\nblock) we also show our model\u2019s generalizability on real captures. Sources of generated/synthetic\nimages: Column 1 (top-to-bottom), Magic3D (Lin et al., 2023b), DreamFusion (Poole et al., 2022),\nWonder3D (Long et al., 2023); Column 2 (top-to-bottom), Zero-1-to-3 (Liu et al., 2023a), Sync-\nDreamer (Liu et al., 2023b), Consistent-1-to-3 (Ye et al., 2023); Column 3 (top-to-bottom), MV-\nDream (Shi et al., 2023b), NeRF (Mildenhall et al., 2020), Zero123++ (Shi et al., 2023a). Source of\nreal images: Row 1 column 1, HuMMan Dataset (Cai et al., 2022); Row 1 column 2, RelPose++ (Lin\net al., 2023a); Others, our phone captures.\n2\net al., 2020)). As shown in Fig. 1, our approach can robustly reconstruct accurate poses and real-\nistic 3D objects using as few as 2\u20134 sparse input images from diverse input sources. The core of\nour approach is a novel scalable single-stream transformer model (see Fig. 3) that computes self-\nattention over the union of the two token sets: the set of 2D multi-view image tokens and the set of\n3D triplane NeRF tokens, allowing for comprehensive information exchange across all 2D and 3D\ntokens. We use the final NeRF tokens, contextualized by 2D images, to represent a triplane NeRF\nand supervise it by a novel view rendering loss. On the other hand, we use the final image patch\ntokens contextualized by NeRF tokens for predicting the coarse point clouds used to solve per-view\ncamera poses.\nUnlike previous methods that regress pose parameters from images directly, we estimate the 3D\nobject points corresponding to 2D patch centers from their individual patch tokens (contextualized\nby NeRF tokens). These points are supervised by the NeRF geometry in an online manner dur-\ning training and enable accurate pose estimation using a differentiable Perspective-n-Point (PnP)\nsolver (Chen et al., 2022b). In essence, we transform the task from per-view pose prediction into\nper-patch 3D surface point prediction, which is more suitable for our single-stream transformer that\u2019s\ndesigned for token-wise operations, leading to more accurate results than direct pose regression.\nPF-LRM is a large transformer model with \u223c590 million parameters trained on large-scale\nmulti-view posed renderings from Objaverse (Deitke et al., 2023) and real-world captures from\nMVImgNet (Yu et al., 2023) that cover \u223c1 million objects in total, without direct 3D supervision.\nDespite being trained under the setting of 4 input views, it generalizes well to unseen datasets and\ncan handle a variable number of 2\u20134 unposed input images during test time (see Fig. 1), achieving\nstate-of-the-art results for both pose estimation and novel view synthesis in the case of very sparse\ninputs, outperforming baseline methods (Jiang et al., 2022; Lin et al., 2023a) by a large margin.\nWe also showcase some potential downstream applications of our model, e.g., text/image-to-3D, in\nFig. 1.\n2\nRELATED WORK\nNeRF from sparse posed images. The original NeRF technique (Mildenhall et al., 2020) required\nhundreds of posed images for accurate reconstruction. Recent research on sparse-view NeRF re-\nconstruction has proposed either regularization strategies (Wang et al., 2023; Niemeyer et al., 2022;\nYang et al., 2023; Kim et al., 2022) or learning priors from extensive datasets (Yu et al., 2021; Chen\net al., 2021; Long et al., 2022; Ren et al., 2023; Zhou & Tulsiani, 2023; Irshad et al., 2023; Li et al.,\n2023; Xu et al., 2023). These approaches still assume precise camera poses for every input image;\nhowever determining camera poses given such sparse-view images is non-trivial and off-the-shelf\ncamera estimation pipelines (Schonberger & Frahm, 2016; Snavely et al., 2006) tend to fail. In con-\ntrast, our method efficiently reconstructs a triplane NeRF (Chan et al., 2022; Chen et al., 2022a; Peng\net al., 2020) from sparse views without any camera pose inputs; moreover, our method is capable of\nrecovering the unknown relative camera poses during inference time.\nStructure from Motion. Structure-from-Motion (SfM) techniques (Schonberger & Frahm, 2016;\nSnavely et al., 2006; Mohr et al., 1995) find 2D feature matches across views, and then solve for\ncamera poses and sparse 3D scene structure from these 2D correspondences at the same time. These\nmethods work pretty well in the presence of sufficient visual overlap between nearby views and\nadequate discriminative features, leading to accurate camera estimation. However, when the input\nviews are extremely sparse, for instance, when there are only 4 images looking from the front-, left-,\nright-, back- side of an object, it becomes very challenging to match features across views due to\nthe lack of sufficient overlap, even with modern learning-based feature extractors (DeTone et al.,\n2018; Dusmanu et al., 2019; Revaud et al., 2019) and matchers (Sarlin et al., 2020; 2019; Liu et al.,\n2021). In contrast, our method relies on the powerful learnt shape prior from a large amount of data\nto successfully register the cameras in these challenging scenarios.\nNeural pose prediction from RGB images. A series of methods (Lin et al., 2023a; Rockwell\net al., 2022; Cai et al., 2021) have sought to address this issue by directly regressing camera poses\nthrough network predictions. Notably, these methods do not incorporate 3D shape information\nduring the camera pose prediction process. We demonstrate that jointly reasoning about camera\npose and 3D shape leads to significant improvement over these previous methods that only regress\nthe camera pose. SparsePose (Sinha et al., 2023), FORGE (Jiang et al., 2022) and FvOR (Yang\n3\n\u00a9 2023 Adobe. All Rights Reserved. Adobe Confidential.\nTriplane position emb.\nTriplane\nReference view enc.\nTransformer\nMLP\nSource view enc.\nTriplane tokens\nMLP\nMLP\nImage \ntokenizer \n(DINO)\nImage \ntokenizer \n(DINO)\nImage \ntokenizer \n(DINO)\nMLP\n+\n+\nSelf-\nattention\ncamera\nVolume \nrendering\nReshape \n& \nUpsample\nRendered image\nTarget image\nRendering \nloss\nDiff. PnP\nImage tokens\nPoint cloud\n+\n+\n+\nMLP\nIntrinsic \nparam.\nMLP\nMLP\nFigure 2:\nOverview of our pipeline. Given unposed sparse input images, we use a large trans-\nformer model to reconstruct a triplane NeRF while simultaneously estimating the relative camera\nposes of all source views with respect to the reference one. During training, the triplane tokens are\nsupervised with a rendering loss at novel viewpoints using ground-truth camera poses. For camera\nregistration, instead of directly regressing the camera poses, we map the image tokens to a coarse\n3D geometry in the form of a point cloud (top right), where we predict a 3D point from each patch\ntoken corresponding to the patch center. We then use a differentiable PnP solver to obtain the camera\nposes from these predicted 3D-2D correspondences (Sec. 3.3).\net al., 2022) implement a two-stage prediction pipeline, initially inferring coarse camera poses and\ncoarse shapes by neural networks and then refining these pose predictions (through further network\nevaluations (Sinha et al., 2023) or per-object optimizations (Jiang et al., 2022; Yang et al., 2022))\njointly with 3D structures. Our method employs a single-stage inference pipeline to recover both\ncamera poses and 3D NeRF reconstructions at the same time. To predict camera poses, we opt\nnot to regress them directly as in prior work. Instead, we predict a coarse point cloud in the scene\ncoordinate frame (Shotton et al., 2013) for each view from their image patch tokens; these points,\nalong with image patch centers, establish a set of 3D-2D correspondences, and allow us to solve\nfor poses using a differentiable PnP solver (Chen et al., 2022b; Brachmann et al., 2017). This is\nin contrast to solving poses from frame-to-frame scene flows (3D-3D correspondences) used by\nFlowCam (Smith et al., 2023), and better suits the sparse view inputs with little overlap. Moreover,\nour backbone model is a simple transformer-based model that is highly scalable; hence it can be\ntrained on massive multi-view posed data of diverse and general objects to gain superior robustness\nand generalization. This distinguishes us from the virtual correspondence work (Ma et al., 2022)\nthat\u2019s designed specifically for human images.\n3\nMETHOD\nGiven a set of N images {Ii|i = 1, .., N} with unknown camera poses capturing a 3D object, our\ngoal is to reconstruct the object\u2019s 3D model and estimate the pose of each image. In particular, we\ndesignate one input image (i.e., I1) as a reference view, and predict a 3D triplane NeRF and camera\nposes of other images relative to the reference view. This is expressed by\nT , y2, ..., yN = PF-LRM(I1, ..., IN),\n(1)\nwhere T is the triplane NeRF defined in the coordinate frame of the reference view 1 and y2,...,yN\nare the predicted camera poses of view 2, . . . , N relative to view 1.\nWe achieve this using a transformer model as illustrated in Fig. 3. Specifically, we tokenize both\ninput images and a triplane NeRF, and apply a single-stream multimodal transformer (Chen et al.,\n2020; Li et al., 2019) to process the concatenation of NeRF tokens and image patch tokens with self-\nattention layers (Sec. 3.1). The output NeRF tokens represent a triplane NeRF for neural rendering,\nmodeling object\u2019s geometry and appearance (Sec. 3.2), and the output image patch tokens are used to\nestimate per-view coarse point cloud for pose estimation with a differentiable PnP solver (Sec. 3.3).\n4\n3.1\nSINGLE-STREAM TRANSFORMER\nImage tokenization, view encoding, intrinsics conditioning. We use the pretrained DINO (Caron\net al., 2021) Vision Transformer (Dosovitskiy et al., 2020) to tokenize our input images. We specifi-\ncally take DINO-ViT-B/16 with a patch size of 16\u00d716 and a 12-layer transformer of width D = 768.\nEach input image of resolution H \u00d7 W is tokenized into M = H/16 \u00d7 W/16 tokens.\nTo distinguish reference image tokens from source image tokens, we use two additional learnable\n768-dimensional features, vr and vs, as view encoding vectors \u2013 one vr for the reference view\n(i = 1) and another vs for all other source views (i = 2, .., N). These view encoding vectors\nallow our model to perform shape reconstruction and pose estimation relative to the reference view.\nIn addition, to make our model aware of input cameras\u2019 intrinsics, we use a shared MLP to map\neach view\u2019s intrinsics [fx, fy, cx, cy] to a intrinsics conditioning vector i \u2208 R768; hence we have\nir, ii, i = 2, ..., N for reference and source views, respectively. We then pass the addition of each\nview\u2019s view encoding and intrinsics conditioning vectors to the newly-added adaptive layer norm\nblock inside each transformer block (self-attention + MLP), following prior work (Hong et al., 2023;\nPeebles & Xie, 2022; Huang & Belongie, 2017).\nTriplane tokenization and position embedding. We tokenize a triplane T of shape 3\u00d7HT \u00d7\nWT \u00d7DT into 3\u00d7HT \u00d7WT tokens, where HT , WT , DT denote triplane height, width and channel,\nrespectively. We additionally learn a triplane position embedding Tpos consisting of 3\u00d7HT \u00d7WT\nposition markers for triplane tokens; they are mapped to the target triplane tokens by a transformer\nmodel sourcing information from input image tokens.\nSingle-stream transformer. The full process of this single-stream transformer can be written as\nT , {ai,j|i = 1, .., N; j = 1, ..., M} = PF-LRM(Tpos, I1, ..., IN, vr, vs).\n(2)\nHere ai,j represents the token of the jth patch at view i, and PF-LRM is a sequence of transformer\nlayers. Each transformer layer is composed of a self-attention layer and a multi-layer perceptron\nlayer (MLP), where both use residual connections. We simply concatenate the image tokens and\nthe triplane tokens as Transformer\u2019s input as shown in Fig. 3. The output triplane tokens T and\nimage tokens ai,j are used for volumetric NeRF rendering and per-view pose prediction, which we\nwill discuss later. Our model design is inspired by LRM (Hong et al., 2023) and its follow-ups (Li\net al., 2023; Xu et al., 2023), but are different and has its own unique philosophy in that we adopt a\nsingle-stream architecture where information exchange is mutual between image tokens and NeRF\ntokens due to that we predict both a coherent NeRF and per-view coarse geometry used for camera\nestimation (detailed later in Sec. 3.3), while prior work adopts an encoder-decoder design where\nNeRF tokens source unidirectional information from image tokens using cross-attention layers.\n3.2\nNERF SUPERVISION VIA DIFFERENTIABLE VOLUME RENDERING\nTo supervise the learning of shape and appearance, we use neural differentiable volume rendering to\nrender images at novel viewpoints from the triplane NeRF, as done in (Mildenhall et al., 2020; Chan\net al., 2022). This process is expressed by\nC =\nK\nX\nk=1\n\u03c4k\u22121(1 \u2212 exp(\u2212\u03c3k\u03b4k))ck,\n\u03c4k = exp(\u2212\nk\nX\nk\u2032=1\n\u03c3k\u2032\u03b4k\u2032),\n\u03c3k, ck = MLPT (T (xk)). (3)\nHere, C is the rendered RGB pixel color, \u03c3k and ck are volume density and color decoded from the\ntriplane NeRF T at the 3D location xk on the marching ray through the pixel, and \u03c4k (\u03c40 is defined\nto be 1) and \u03b4k are the volume transmittance and step size; T (xk) represents the features that are\nbilinearly sampled and concatenated from the triplane at xk, and we apply an MLP network MLPT\nto decode the density and color used in volume rendering.\nWe supervise our NeRF reconstruction with L2 and VGG-based LPIPS (Zhang et al., 2018) render-\ning loss:\nLC = \u03b3\u2032\nC \u2225C \u2212 Cgt\u22252 + \u03b3\u2032\u2032\nC Llpips(C, Cgt),\n(4)\nwhere Cgt is the ground-truth pixel color, and \u03b3\u2032\nC, \u03b3\u2032\u2032\nC are loss weights. In practice, we render crops\nof size h \u00d7 w for each view to compute the rendering loss LC, and divide the L2 loss with h \u00d7 w.\n5\n3.3\nPOSE PREDICTION VIA DIFFERENTIABLE PNP SOLVER\nWe estimate relative camera poses from the per-view image patch tokens contextualized by the NeRF\ntokens. Note that a straightforward solution is to directly regress camera pose parameters from the\nimage tokens using an MLP decoder and supervise the poses with the ground truth; however, such a\nna\u00a8\u0131ve solution lacks 3D inductive biases and, in our experiments (See Tab. 10), often leads to limited\npose estimation accuracy. Therefore, we propose to predict per-view coarse geometry (in the form\nof a sparse point cloud, i.e., predicting one 3D point for each patch token) that is supervised to be\nconsistent with the NeRF geometry, allowing us to obtain the camera poses with a PnP solver given\nthe 3D-2D correspondences from the per-patch predicted points and patch centers.\nIn particular, from each image patch token output by the transformer ai,j, we use an MLP to predict\na 3D point and the prediction confidence:\npi,j, \u03b1i,j, wi,j = MLPa(ai,j),\n(5)\nwhere pi,j represents the 3D point location on the object seen through the central pixel of the image\npatch, \u03b1i,j is the pixel opacity that indicates if the pixel covers the foreground object, and wi,j is an\nadditional confidence weight used to determine the point\u2019s contribution to the PnP solver.\nNote that in training stage, where the ground-truth camera poses are known, the central pixel\u2019s point\nlocation and opacity can also be computed from a NeRF as done in previous work (Mildenhall et al.,\n2020). This allows us to enforce the consistency between the per-patch point estimates and the\ntriplane NeRF geometry with following losses:\nLp =\nX\ni,j \u2225pi,j \u2212 \u00afxi,j\u22252,\nL\u03b1 =\nX\ni,j(\u03b1i,j \u2212 (1 \u2212 \u00af\u03c4i,j))2,\n(6)\nwhere \u00afx and \u00af\u03c4 are computed along the pixel ray (marched from the ground-truth camera poses)\nusing the volume rendering weights in Eqn. 3 by\n\u00afx =\nK\nX\nk=1\n\u03c4k\u22121(1 \u2212 exp(\u2212\u03c3k\u03b4k))xk,\n\u00af\u03c4 = \u03c4K = exp(\u2212\nK\nX\nk\u2032=1\n\u03c3k\u2032\u03b4k\u2032).\n(7)\nHere \u00afx represents the expected 3D location and \u00af\u03c4 is the final volume transmittance \u03c4K. Essentially,\nwe distill the geometry of our learnt NeRF reconstruction to supervise our per-view coarse point\ncloud prediction in an online manner, as we only use multi-view posed images to train our model\nwithout accessing 3D ground-truth. This online distillation is critical to stabilize the differentiable\nPnP loss mentioned later in Eq. 11, without which we find the training tend to diverge in our exper-\niments.\nWhen pi,j and \u03b1i,j are estimated, we can already compute each pose yi with a standard weighted\nPnP solver that solves\narg min\nyi=[Ri,ti]\n1\n2\nM\nX\nj=1\n\u03be(yi, pi,j, \u03b2i,j),\n(8)\n\u03be(yi, pi,j, \u03b1i,j) = \u03b2i,j\u2225P(Ri \u00b7 pi,j + ti) \u2212 qi,j\u22252,\n(9)\n\u03b2i,j = \u03b1i,jwi,j,\n(10)\nwhere qi,j is the 2D central pixel location of the patch, [Ri, ti] are the rotation and translation compo-\nnents of the pose yi, P is the projection function with camera intrinstics involved, and \u03be(\u00b7) represents\nthe pixel re-projection error weighted by predicted opacity and PnP confidence. Here, the predicted\nopacity values are used to weigh the errors to prevent the non-informative white background points\nfrom affecting the pose prediction.\nHowever, computing the solution of PnP is a non-convex problem prone to local minimas. Therefore,\nwe further apply a robust differentiable PnP loss, proposed by EPro-PnP (Chen et al., 2022b) 1, to\nregularize our pose prediction, leading to much more accurate results (See Tab. 10). This loss is\nexpressed by\nLyi = 1\n2\nX\nj \u03be(ygt\ni , pi,j, \u03b2i,j) + log\nZ\nexp\n\u0012\n\u22121\n2\nX\nj \u03be(yi, pi,j, \u03b2i,j)\n\u0013\ndyi,\n(11)\n1We take the public implementation in https://github.com/tjiiv-cprg/EPro-PnP.\n6\nwhere the first term minimizes the reprojection errors of the predicted points with the ground-truth\nposes and the second term minimizes the reprojection errors with the predicted pose distribution\nusing Monte Carlo integral; we refer readers to the EPro-PnP paper (Chen et al., 2022b) for details\nabout computing the integral term. This differentiable PnP loss, combined with our point prediction\nlosses (in Eqn. 6), leads to plausible per-patch point location and confidence estimates, allowing for\naccurate final pose prediction.\n3.4\nLOSS FUNCTIONS AND IMPLEMENTATION DETAILS\nLoss. Combining all losses (Eqn. 4,6,11), our final training objective is\nL = LC + \u03b3pLp + \u03b3\u03b1L\u03b1 + \u03b3y\nXM\ni=2 Lyi,\n(12)\nwhere LC represents the rendering loss and \u03b3p, \u03b3\u03b1, \u03b3y are the weights for individual loss terms\nrelated to per-view coarse geometry prediction, opacity prediction and differentiable PnP loss.\nImplementation details. Our single-stream transformer model consists of 36 self-attention layers.\nWe predict triplane of shape HT = WT = 64, DT = 32. In order to decrease the tokens used in\ntransformer, the triplane tokens used in transformer is 3072 = 3\u00d732\u00d732 and will be upsampled\nto 64 with de-convolution, similar to LRM (Hong et al., 2023). We set the loss weights \u03b3\u2032\nC, \u03b3\u2032\u2032\nC\n(Eq. 4), \u03b3p, \u03b3\u03b1, \u03b3y (Eq. 12) to 1, 2, 1, 1, 1, respectively. We use AdamW (Loshchilov & Hutter,\n2017) (\u03b21 = 0.9, \u03b22 = 0.95) optimizer with weight decay 0.05 for model optimization. The initial\nlearning rate is zero, which is linearly warmed up to 4\u00d710\u22124 for the first 3k steps and then decay to\nzero by cosine scheduling. The batch size per GPU is 8. Training this model for 40 epochs takes 128\nNvidia A100 GPUs for about one week. We use the deferred back-propagation technique (Zhang\net al., 2022) to save GPU memory in NeRF rendering. For more implementation details, please refer\nto Sec. A.3 of the appendix.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETTINGS\nTraining datasets. Our model only requires multi-view posed images to train. To construct a\nlarge-scale multi-view posed dataset, we use a mixture of multi-view posed renderings from Obja-\nverse (Deitke et al., 2023) and posed real captures from MVImgNet (Yu et al., 2023). We render the\nObjavere dataset following the same protocol as LRM (Hong et al., 2023) and DMV3D (Xu et al.,\n2023): each object is normalized to [\u22121, 1]3 box and rendered at 32 random viewpoints. We also\npreprocess the MVImgNet captures to crop out objects, remove background 2, and normalizing ob-\nject sizes in the same way as LRM and DMV3D. In total, we have multi-view images of \u223c1 million\nobjects in our training set: \u223c730k from Objaverse, \u223c220k from MVImgNet.\nEvaluation datasets.\nTo evaluate our model\u2019s cross-dataset generalization capability, we uti-\nlize a couple of datasets, including OmniObject3D (Wu et al., 2023), Google Scanned Objects\n(GSO) (Downs et al., 2022), Amazon Berkeley Objects (ABO) (Collins et al., 2022), Common\nObjects 3D (CO3D) (Reizenstein et al., 2021), and DTU (Aan\u00e6s et al., 2016). For OmniObject3D,\nGSO, ABO datasets, we randomly choose 500 objects for assessing our model\u2019s performance given\nsparse images as inputs. We render out 5 images from randomly selected viewpoints for each ob-\nject; to ensure view sparsity, we make sure viewing angles between any two views are at least 45\ndegrees. We feed randomly-chosen 4 images to our model to predict a NeRF and poses, while using\nthe remaining 1 to measure our novel-view rendering quality. For CO3D dataset, we use the 400\nheld-out captures provided by RelPose++ (Lin et al., 2023a), which covers 10 object categories. To\nremove background, we use the masks included in the CO3D dataset. However, we note that these\nmasks can be very noisy sometimes, negatively affecting our model\u2019s performance and the baseline\nRelPose++ (mask variant). We randomly select 4 random input views for each capture. For DTU\ndataset, we take the 15 objects with manually annotated masks provided by IDR (Yariv et al., 2020);\nfor each object, we randomly select 8 different combinations of four input views, resulting in a total\nof 120 different testing inputs.\n2Mask removal tool: https://github.com/danielgatis/rembg\n7\nBaselines.\nAs our PF-LRM can do joint pose and shape estimation, we evaluate its perfor-\nmance against baselines on both tasks. For the pose estimation task, we compare PF-LRM with\nFORGE (Jiang et al., 2022), RelPose++ (Lin et al., 2023a), and the SfM-based method HLoc (Sarlin\net al., 2019; Schonberger & Frahm, 2016). We also compare with FORGE in terms of the reconstruc-\ntion quality. We did not compare with SparsePose (Sinha et al., 2023) as there is no public source\ncode available. SRT (Sajjadi et al., 2022) is geometry-free and does not directly predict shapes like\nus; hence we did not compare with it due to this clear distinction in problem scopes.\nMetrics. Since we only care about relative pose estimation in the pose estimation task, we use\npair-wise relative pose errors as our metric: for each image pair in the input image set, we measure\nthe rotation part of the relative pose by computing the error as the minimal rotation angle between\nthe prediction and ground-truth. We also report the percentage of image pairs with relative rotation\nerrors below thresholds 15\u25e6 and 30\u25e6. The translation part of the predicted relative pose is measured\nby its absolute difference from the ground-truth one. We evaluate the reconstruction quality by\ncomparing renderings of our reconstructed NeRF using both predicted input-view poses and ground-\ntruth novel-view poses against the ground-truth. We report the PSNR, SSIM and LPIPS (Zhang\net al., 2018) metrics for measuring the image quality. We use 4 images as inputs for each object\nwhen comparing the performance of different methods.\n4.2\nEXPERIMENT RESULTS\n4.2.1\nPOSE PREDICTION QUALITY\nAs shown in Tab. 1, our model achieves state-of-the-art results in pose estimation accuracy and\nrendering quality given highly sparse input images on unseen datasets including OmniObjects3D,\nABO, GSO, CO3D, and DTU, consistently outperforming baselines by a large margin across all\ndatasets and all metrics. This is an especially challenging evaluation setting, as we are assessing the\ncross-dataset generalization capability of different methods, which reflects their performance when\ndeployed in real-world applications. In this regard, we directly use the pretrained checkpoints from\nbaselines, including FORGE (Jiang et al., 2022), HLoc (Sarlin et al., 2019) and RelPose++ (Lin\net al., 2023a), for comparisons.\nOn OmniObject3D, GSO, ABO datasets where the input images are explicitly sparsified (see\nSec. 4.1), we achieve an average of 14.6x reduction in rotation error for the predicted poses com-\npared with FORGE, while the average rotation error reductions are 15.3x compared with HLoc and\n14.7x compared with RelPose++. As FORGE expects input images to have black background, we\nreplace the white background in our rendered images with a black one using the rendered alpha\nmask before feeding them into FORGE. RelPose++, however, has two variants: one trained on im-\nages with background (w/ bg) and the other trained on images without backgrund (w/o bg). We\nevaluate the w/o bg variant on these datasets featuring non-informative white background. In ad-\ndition, we observe that HLoc has a very high failure rate (more than 97%) on these very sparse\ninputs, due to that matching features is too hard in this case; this also highlights the difficulty of\npose prediction under extremely sparse views, and the contributions this work to the area.\nOn the held-out CO3D test set provided by RelPose++, our rotation error is 5x smaller than FORGE,\n3.6x smaller than HLoc, and 1.8x smaller than RelPose++ (w/ bg). Note that FORGE, HLoc and\nour method are all tested on input images with background removed. The inaccurate foreground\nmasks provided by CO3D can negatively influence these methods\u2019 performance; this can explain\nour performance degradation from datasets like OmniObject3D to datasets like CO3D. It will be\ninteresting to explore ways to extend our method to handle background directly in future work.\nWe also note that CO3D captures may not cover the objects in 360, hence we do not sparsify the\ninput poses but instead use randomly selected views; evaluation on this dataset may not reflect\ndifferent models\u2019 performance on highly sparse inputs. In addition, RelPose++ is trained on the\nCO3D training set while the other methods, including ours, are not. On DTU dataset where none\nof the methods are trained on, we achieve 4x rotation error reduction than RelPose++ (w/ bg), and\n7.6x reduction than FORGE, showing much better generalization capability than these methods.\nInterestingly, as we do not sparsify input views intentionally on this data for the same reason as\nCO3D, HLoc, which relies on traditional SfM (Schonberger & Frahm, 2016) to solve poses, can\nproduce reasonably accurate pose estimations if enough features are correctly matched (failure rate\n8\nTable 1: On pose prediction task, we compare cross-dataset generalization to OmniObject3D (Wu\net al., 2023), GSO (Downs et al., 2022), ABO (Collins et al., 2022), CO3D (Reizenstein et al.,\n2021), DTU (Aan\u00e6s et al., 2016) with baselines FORGE (Jiang et al., 2022), HLoc (Sarlin et al.,\n2019), RelPose++ (Lin et al., 2023a). Note that RelPose++ is trained on CO3D training set; hence\nits numbers on CO3D test set are not exactly cross-dataset performance. On OmniObject3D, GSO,\nABO where background is white in the rendered data, we evaluate the w/o bg variant of RelPose++,\nwhile on CO3D and DTU where real captures contain background, we evalute its w/ bg variant.\nOmniObject3D\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nFORGE\n71.06\n0.071\n0.232\n0.726\nHLoc (F. rate 99.6%)\n98.65\n0.083\n0.083\n1.343\nRelPose++ (w/o bg)\n69.22\n0.070\n0.273\n0.712\nOurs\n6.32\n0.962\n0.990\n0.067\nGSO\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nFORGE\n103.81\n0.012\n0.056\n1.100\nHLoc (F. rate 97.2%)\n97.12\n0.036\n0.131\n1.199\nRelPose++ (w/o bg)\n107.49\n0.037\n0.098\n1.143\nOurs\n3.99\n0.956\n0.976\n0.041\nABO\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nFORGE\n105.23\n0.014\n0.059\n1.107\nHLoc (F. rate 98.8%)\n94.84\n0.067\n0.178\n1.302\nRelPose++ (w/o bg)\n102.30\n0.060\n0.144\n1.103\nOurs\n16.27\n0.865\n0.885\n0.150\nCO3D\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nFORGE\n77.74\n0.139\n0.278\n1.181\nHLoc (F. rate 89.0%)\n55.87\n0.288\n0.447\n1.109\nRelPose++ (w/ bg)\n28.24\n0.748\n0.840\n0.448\nOurs\n15.53\n0.850\n0.899\n0.242\nDTU\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nFORGE\n78.88\n0.046\n0.188\n1.397\nHLoc (F. rate 47.5%)\n11.84\n0.725\n0.915\n0.520\nRelPose++ (w/ bg)\n41.84\n0.369\n0.657\n0.754\nOurs\n10.42\n0.900\n0.951\n0.187\nis 47.5%); however, it\u2019s performance is still far worse than ours, especially on the metric Acc.@15\u25e6\nthat measures the percentage of pair-wise rotation errors below the threshold of 15 degrees.\nWe attribute our model\u2019s success to the prediction of both camera poses and object shapes at the\nsame time, where the synergy of the two tasks are exploited by the self-attention mechanism. The\ngeneric shape prior learned on the large Objaverse and MVImgNet datasets differentiate our method\nfrom other methods, as we find it particularly helpful in estimating camera parameters given sparse\ninputs (See Sec. 4.4). Prior methods like RelPose++ failed to utilize this synergy, as they solve for\nposes directly from images without simultaneously reconstructing the 3D object. FORGE designed a\nlearning framework to introduce shape prior into the pose estimation process, but its training process\nis composed of six stages, which seems fragile and not easy to scale up, compared with our single-\nstream transformer design. Therefore it shows much weaker cross-dataset generalization capability\nthan our method. This said, we also acknowledge that if one can successfully scale up the training of\nthe baseline RelPose++ and FORGE on large-scale datasets, their performance can also be improved\ncompared with their pretrained model using limited data. We show one such experiment where we\nre-train RelPose++ on Objaverse data in appendix A.6; however, our model, trained on exactly the\nsame Objaverse data, still outperforms this re-trained baseline by a wide margin in terms of pose\nprediction accuracy on various evaluation datasets, demonstrating the superiority of our method. We\nleave the investigation of scaling up FORGE to future work due to its complex pipeline.\n9\nGSO\nABO\nOmniObject3D\nCO3D\nDTU\nInput images\nNovel G.T. images  \nOur geometry\nRender our pred. NeRF w/ \nour pred. poses \nRender our pred. NeRF w/ \nnovel poses\nFigure 3: Cross-dataset generalization to unseen OmniObject3D (Wu et al., 2023), GSO (Downs\net al., 2022) and ABO (Collins et al., 2022) datasets. Renderings of our predicted NeRF at predicted\nposes (second column) closely match the input unposed images (first column), demonstrating the\nexcellent accuracy of both predictions; we also show novel-view rendering of our reconstructed\nNeRF (fourth column) and the corresponding ground-truth (third column) to show our high-quality\nNeRF reconstruction, from which we can also easily extract meshes (last column) by fusing the\nmult-view RGBD images rendered from NeRF using RGBD fusion (Curless & Levoy, 2023). More\nvisual examples can be found in Fig. 7 in the appendix.\nAlthough the SfM method HLoc solves for poses and 3D shape (in the form of sparse point cloud)\nat the same time, it relies on feature matching across views which is extremely challenging in the\ncase of sparse-views; hence it performs poorly in our application scenario.\n4.2.2\nRECONSTRUCTION QUALITY\nWe use the surrogate view synthesis quality to compare the quality of our reconstructed NeRF to\nthat of FORGE (Jiang et al., 2022). To isolate the influence of inaccurate masks on measuring the\nview synthesis quality, we evaluate on unseen OmniObject3D, GSO, and ABO datasets and compare\n10\nwith the baselines FORGE. In this experiment, we use the same input settings as in the above pose\nprediction comparisons. We use PSNR, SSIM (Wang et al., 2004), and LPIPS (Zhang et al., 2018)\nas image metrics.\nAs shown in Tab. 2, our PF-LRM achieves an average PSNR of 24.8 on OmniObject3D, GSO, and\nABO datasets, while the baseline FORGE\u2019s average PSNR is only 13.4. This shows that our model\ngeneralizes very well and produce high-quality reconstructions on unseen datasets while FORGE\ndoes not. Note that we actually feed images with black background into FORGE, and evaluate PSNR\nusing images with black background; this is, in fact, an evaluation setup that bias towards FORGE,\nas images with black background tends to have higher PSNR than those with white background.\nOn the other hand, we think there\u2019s an important objective to fulfill in the task of joint pose and\nNeRF prediction; that is, the predicted NeRF, when rendered at predicted poses, should match well\nthe input unposed images. This is an objective complimentary to the novel view quality and requiring\naccurate predictions of both poses and NeRF. We show in Tab. 2 that FORGE does poorly on this\ngoal evidenced by the low PSNR scores, especially on the GSO and ABO datasets. In contrast, we\nperform much better.\nIn general, our model learns a generic shape prior effectively from massive multi-view datasets\nincluding Objaverse and MVImgNet, thanks to its scalable single-stream transformer design.\nFORGE\u2019s multi-stage training, though, is challenging to scale due to error accumulation across\nstages. Fig. 3 qualitatively shows the high-quality NeRF reconstruction and accurate pose prediction\nfrom our model. Renderings of the our predicted NeRF using our predicted poses closely match the\ninput images, and the novel view rendering resembles the ground-truth a lot. We also demonstrate\nhigh-quality extracted meshes from our reconstructed NeRF; the meshes are extracted by first ren-\ndering out 100 RGBD images uniformly distributed in a sphere and then fusing them using RGBD\nfusion (Curless & Levoy, 2023).\nTable 2: On 3D reconstruction task, we compare novel view synthesis quality with baseline\nFORGE (Jiang et al., 2022) on OmniObject3D (Wu et al., 2023), GSO\n(Downs et al., 2022),\nABO (Collins et al., 2022) datasets. Neither methods are trained on these evaluation datasets. Note\nthat both methods predict input cameras\u2019 poses and hence the predicted NeRF are aligned with their\nown predicted cameras; we align the predicted input cameras to the ground-truth one, and transform\nthe reconstructed NeRF accordingly before rendering them with novel-view cameras for computing\nthe image metrics. We show evaluate renderings of the predicted NeRF at the predicted camera\nposes against the inputs to show how consistent both predictions are in terms of matching inputs.\nOmniObject3D\nGoogle Scanned Objects\nAmazon Berkeley Objects\nMethod\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nEvaluate renderings of our predicted NeRF at novel-view poses\nFORGE\n17.95\n0.800\n0.215\n11.43\n0.754\n0.760\n10.92\n0.669\n0.325\nOurs\n23.02\n0.877\n0.083\n25.04\n0.879\n0.096\n26.23\n0.887\n0.097\nEvaluate renderings of our predicted NeRF at our predicted poses\nFORGE\n19.03\n0.829\n0.189\n11.90\n0.760\n0.202\n11.32\n11.32\n0.209\nOurs\n27.27\n0.916\n0.054\n27.01\n0.914\n0.0645\n27.19\n0.894\n0.083\n4.3\nROBUSTNESS TESTS\nVariable number of input views. Our model naturally supports variable number of input views as\na result of the transformer-based architecture. We test our model\u2019s performance on variable number\nof input images on the 500 selected GSO objects (see Sec. 4.1). As shown in Tab. 3, with decreased\nnumber of views, we observe a consistent drop in reconstruction quality and pose prediction quality,\nbut the performance degradation is acceptable. Note that for pose evaluation, we only evaluate\nthe relative pose errors of the first two views for fair comparison. PSNRinput reflects how well our\nmodel\u2019s predicted NeRF and poses can explain the input images, while the PSNRall is an aggregated\nmetrics including both input views and held-out novel views (we have 4 views in total for each\nobject).\nImperfect segmentation masks. In this experiment we add noises on the input segmentation masks\nby adding different levels of elastic transform (Simard et al., 2003). As shown in Tab. 4, we can\nsee that our model is robust to certain level of noise, but its performance drop significantly when the\n11\n\u00a9 2023 Adobe. All Rights Reserved. Ad\nInput images \nOurs (L)\nOurs (S)\nOurs (S) w/o pose prediction \nsupervision\nFigure 4: Ablation studies on GSO data (Downs et al., 2022). \u2018Ours (L)\u2019 results in highest recon-\nstruction quality with sharpest details, while reducing the model size (\u2018Ours (S)\u2019) causes the texture\nto become blur. Further removing pose prediction branch (\u2018Ours (S) w/o pose prediction\u2019) makes\nthe texture even worse. Note that for a fair comparison of different ablation variants, especially the\none without pose prediction, we render out our reconstructed NeRF using the same ground-truth\nposes corresponding to input images (as opposed to predicted ones).\nTable 3: Inference on variable number of input views on unseen GSO dataset using our PF-LRM\ntrained on 4 views (no re-training or fine-tuning is involved). For pose evaluation, we only evaluate\nthe relative pose erros of the first two views for fair comparison.\n#Views\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nPSNRinput\nPSNRall\n4\n4.19\n0.956\n0.974\n27.76\n27.76\n3\n5.83\n0.946\n0.962\n27.59\n26.76\n2\n10.38\n0.886\n0.924\n27.35\n24.87\n1\n-\n-\n-\n29.27\n21.56\nmasks are very noisy. This is also aligned with the observation that the inaccurate masks provided by\nCO3D (Reizenstein et al., 2021) can harm our model\u2019s performance on it, e.g., the Couch category in\nTab. 6 of the appendix. Note that PSNRg.t. reflects how well renderings of our predicted NeRF using\nground-truth input poses match the input images, while PSNRpred measures how well renderings of\nour predicted NeRF using our pose predictions match the inputs.\n12\nTable 4: Inference on images with varying level of segmentation mask errors on unseen GSO dataset\nusing our PF-LRM.\nNoise level\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nT. error\nPSNRg.t.\nPSNRpred.\n0\n2.46\n0.976\n0.985\n0.026\n29.42\n28.38\n1\n4.84\n0.951\n0.968\n0.050\n27.19\n26.84\n2\n7.15\n0.921\n0.946\n0.075\n26.25\n26.26\n3\n10.34\n0.881\n0.916\n0.106\n25.515\n25.567\n4\n14.13\n0.844\n0.894\n0.141\n24.934\n24.975\nTable 5: Ablation study of model size and training objectives on the GSO dataset.\nSetting\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nT. error\nPSNRg.t.\nPSNRpred.\nOurs (L)\n2.46\n0.976\n0.985\n0.026\n29.42\n28.38\nOurs (S)\n13.08\n0.848\n0.916\n0.135\n23.80\n22.82\n- NeRF Pred. (S)\n111.89\n0.000\n0.000\n1.630\n-\n-\n- pose Pred. (S)\n-\n-\n-\n-\n22.48\n-\n4.4\nABLATION STUDIES\nIn the ablation studies, we train our models with different settings on the synthetic Objaverse\ndataset (Deitke et al., 2023) and evaluate on GSO dataset (Downs et al., 2022) to isolate the in-\nfluence of noisy background removals. For better energy efficiency, we conduct ablations mostly on\na smaller version of our model, dubbed as Ours (S). It has 24 self-attention layers with 1024 token\ndimension, and is trained on 8 A100 GPUs for 20 epochs (\u223c100k iterations), which takes around 5\ndays. In addition, to show the scaling law with respect to model sizes, we train a large model (Ours\n(L)) on 128 GPUs for 100 epochs (\u223c70k iterations).\nUsing smaller model. \u2018Ours (L)\u2019 outperforms the smaller one \u2018Ours (S)\u2019 by a great margin in terms\nof pose prediction accuracy and NeRF reconstruction quality, as shown in Tab. 5 and Fig. 4. It aligns\nwith the recent findings that larger model can learn better from data (Hong et al., 2023).\nRemoving NeRF prediction. We evaluated two different settings without NeRF prediction: 1)\nusing differentiable PnP for pose prediction as described in Sec. 3.3; 2) using MLP to directly\npredict poses from the concatenated patch features. For 1), we notice that the training becomes\nvery unstable and tends to diverge in this case, as we find that our point loss (Eqn. 6; relying on\nNeRF prediction for supervision) helps stabilize the differentiable PnP loss (Eqn. 11). For 2), we\nfind that the predicted pose is almost random, as shown in Tab. 5; this indicates that the training\nand evaluation cases of highly sparse views (e.g., four images looking at the front, back, left- and\nright-side parts of an object) seem to pose a convergence challenge for a purely images-to-poses\nregressor when trained on the massive Objaverse dataset (Deitke et al., 2023).\nRemoving pose prediction. We find that jointly predicting pose helps the model learn better 3D\nreconstruction with sharper textures, as shown in Tab. 5 (comparing \u2018-pose Pred. (S)\u2019 and \u2018Ours (S)\u2019)\nand Fig. 4 (comparing \u2018Our (S) w/o pose prediction\u2019 and \u2018Ours (S)\u2019). This could be that by forcing\nthe model to figure out the correct spatial relationship of input views, we reduce the uncertainty and\ndifficulty of shape reconstruction.\n4.5\nAPPLICATION\nText/image-to-3D generation. Since our model can reconstruct NeRF from 2-4 unposed images,\nit can be readily used in downstream text-to-3D applications to build highly efficient two-stage 3D\ngeneration pipelines. In the first stage, one can use geometry-free multi-view image generators,\ne.g., MVDream (Shi et al., 2023b), Instant3D (Li et al., 2023), to generate a few images from a\nuser-provided text prompt. Then the unposed generated images can be instantly lifted into 3D by\nour PF-LRM with a single feed-forward inference (see Fig. 1). Or alternatively, one can generate a\nsingle image from text prompts using Stable Diffusion (Rombach et al., 2022), feed the single image\nto image-conditioned generators, e.g., Zero-1-to-3 (Liu et al., 2023a), Zero123++ (Shi et al., 2023a),\nto generate at least one additional view, then reconstruct a NeRF from the multiple unposed images\n13\nusing our PF-LRM. In the latter approach, we can have a feed-forward single-image-to-3D pipeline\nas well, if the text-to-image step is skipped, as shown in Fig. 1.\n5\nCONCLUSION\nIn this work, we propose a large reconstruction model based on the transformer architecture to jointly\nestimate camera parameters and reconstruct 3D shapes in the form of NeRF. Our model employs\nself-attention to allow triplane tokens and image patch tokens to communicate information with each\nother, leading to improved NeRF reconstruction quality and robust per-patch surface point prediction\nfor solving poses using a differentiable PnP solver. Trained on multi-view posed renderings of the\nlarge-scale Objaverse and real MVImgNet datasets, our model outperforms baseline methods by a\nlarge margin in terms of pose prediction accuracy and reconstruction quality. We also show that our\nmodel can be leveraged in downstream applications like text/image-to-3D generation.\nLimitations. Despite the impressive reconstruction and pose prediction performance of our model,\nthere are a few limitations to be addressed in future works: 1) First, we ignore the background in-\nformation that might contain rich cues about camera poses, e.g., vanishing points, casting shadows,\netc, while predicting camera poses. It will be interesting to extend our work to handle background\nwith spatial warpings as in (Zhang et al., 2020; Barron et al., 2022). 2) Second, we are also not\nable to model view-dependent effects due to our modelling choice of per-point colors, compared\nwith NeRF (Mildenhall et al., 2020; Verbin et al., 2022). Future work will include recovering view-\ndependent appearance from sparse views. 3) The resolution of our predicted triplane NeRF can also\nbe further increased by exploring techniques like coarse-to-fine modelling or other high-capacity\ncompact representations, e.g., multi-resolution hashgrid (M\u00a8uller et al., 2022), to enable more de-\ntailed geometry and texture reconstructions. 4) Our model currently assumes known intrinsics (see\nSec. 3.1) from the camera sensor metadata or a reasonable user guess; future work can explore tech-\nniques to predict camera intrinscis as well. 5) Although our model is pose-free during test time, it\nstill requires ground-truth pose supervision to train; an intriguing direction is to lift the camera pose\nrequirement during training in order to consume massive in-the-wild video training data.\nEthics Statement.\nThe model proposed in this paper is a reconstruction model that can convert\nmulti-view images to the 3D shapes. This techniques can be used to reconstruct images with human.\nHowever, the current shape resolution is still relatively low which would not get accurate recon-\nstruction of the face region/hand region. The model is trained to be a deterministic model thus it is\nhard to leak the data used in training. The users can use this model to reconstruct the shape of the\nimages where there might be a commercial copyright of the shape. This model also utilizes a train-\ning compute that is significantly larger than previous 3D reconstruction models. Thus the model can\npotentially lead to a trend of pursuing large reconstruction models in the 3D domain, which further\ncan introduce the environmental concerns like the current trend of large language model.\nReproducibility Statement.\nWe have elucidate our model design in the paper including the train-\ning architecture (transformer in Sec. 3.1, NeRF rendering in Sec. 3.2) the losses (pose loss in Sec. 3.3\nand final loss in Sec. 3.4). The training details are shown in Sec. 3.4 and further extended in Ap-\npendix. We also pointed to the exact implementation of the Diff. PnP method in Sec. 3.3 to resolve\nuncertainty over the detailed implementation. Lastly, we will involve in the discussion regarding\nimplementation details of our paper.\nAcknowledgement.\nWe want to thank Nathan Carr, Duygu Ceylan, Paul Guerrero, Chun-Hao\nHuang, and Niloy Mitra for discussions on this project. We thank Yuan Liu for helpful discussions\non pose estimation.\nREFERENCES\nHenrik Aan\u00e6s, Rasmus Ramsb\u00f8l Jensen, George Vogiatzis, Engin Tola, and Anders Bjorholm Dahl.\nLarge-scale data for multiple-view stereopsis. International Journal of Computer Vision, pp. 1\u2013\n16, 2016.\n14\nConnelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A ran-\ndomized correspondence algorithm for structural image editing. ACM Trans. Graph., 28(3):24,\n2009.\nJonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 5470\u20135479, 2022.\nEric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan\nGumhold, and Carsten Rother. Dsac-differentiable ransac for camera localization. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 6684\u20136692, 2017.\nRuojin Cai, Bharath Hariharan, Noah Snavely, and Hadar Averbuch-Elor. Extreme rotation estima-\ntion using dense correlation volumes. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 14566\u201314575, 2021.\nZhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao Yu, Wenjia Wang, Xiangyu Fan, Yang\nGao, Yifan Yu, Liang Pan, Fangzhou Hong, Mingyuan Zhang, Chen Change Loy, Lei Yang, and\nZiwei Liu. HuMMan: Multi-modal 4d human dataset for versatile sensing and modeling. In 17th\nEuropean Conference on Computer Vision, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart VII, pp. 557\u2013577. Springer, 2022.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 9650\u20139660, 2021.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware\n3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 16123\u201316133, 2022.\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su.\nMvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pp. 14124\u201314133, 2021.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields. In European Conference on Computer Vision (ECCV), 2022a.\nHansheng Chen, Pichao Wang, Fan Wang, Wei Tian, Lu Xiong, and Hao Li. Epro-pnp: Generalized\nend-to-end probabilistic perspective-n-points for monocular object pose estimation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2781\u20132790,\n2022b.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning. In European conference on\ncomputer vision, pp. 104\u2013120. Springer, 2020.\nJasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu,\nXi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and\nbenchmarks for real-world 3d object understanding. In CVPR, pp. 21126\u201321136, 2022.\nBrian Curless and Marc Levoy. A Volumetric Method for Building Complex Models from Range\nImages. Association for Computing Machinery, New York, NY, USA, 1 edition, 2023. ISBN\n9798400708978. URL https://doi.org/10.1145/3596711.3596726.\nTri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv\npreprint arXiv:2307.08691, 2023.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In CVPR, pp. 13142\u201313153, 2023.\n15\nKangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views\nand faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), June 2022.\nDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest\npoint detection and description. In Proceedings of the IEEE conference on computer vision and\npattern recognition workshops, pp. 224\u2013236, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An im-\nage is worth 16x16 words: Transformers for image recognition at scale. In International Confer-\nence on Learning Representations, 2020.\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,\nThomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset\nof 3d scanned household items. In 2022 International Conference on Robotics and Automation\n(ICRA), pp. 2553\u20132560. IEEE, 2022.\nMihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and\nTorsten Sattler. D2-net: A trainable cnn for joint description and detection of local features. In\nProceedings of the ieee/cvf conference on computer vision and pattern recognition, pp. 8092\u2013\n8101, 2019.\nYasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE trans-\nactions on pattern analysis and machine intelligence, 32(8):1362\u20131376, 2009.\nYicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli,\nTrung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. 2023.\nXun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normal-\nization. In Proceedings of the IEEE international conference on computer vision, pp. 1501\u20131510,\n2017.\nMuhammad Zubair Irshad, Sergey Zakharov, Katherine Liu, Vitor Guizilini, Thomas Kollar, Adrien\nGaidon, Zsolt Kira, and Rares Ambrus. Neo 360: Neural fields for sparse view synthesis of\noutdoor scenes. 2023. URL https://arxiv.org/abs/2308.12967.\nHanwen Jiang, Zhenyu Jiang, Kristen Grauman, and Yuke Zhu. Few-view object reconstruction\nwith unknown categories and camera poses. ArXiv, 2212.04492, 2022.\nMijeong Kim, Seonguk Seo, and Bohyung Han. Infonerf: Ray entropy minimization for few-shot\nneural volume rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 12912\u201312921, 2022.\nJiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan\nSunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view gen-\neration and large reconstruction model. 2023.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019.\nAmy Lin, Jason Y Zhang, Deva Ramanan, and Shubham Tulsiani. Relpose++: Recovering 6d poses\nfrom sparse-view observations. arXiv preprint arXiv:2305.04926, 2023a.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023b.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object, 2023a.\nYuan Liu, Lingjie Liu, Cheng Lin, Zhen Dong, and Wenping Wang. Learnable motion coherence\nfor correspondence pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 3237\u20133246, 2021.\n16\nYuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.\nSyncdreamer: Generating multiview-consistent images from a single-view image. arXiv preprint\narXiv:2309.03453, 2023b.\nXiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang. Sparseneus: Fast gen-\neralizable neural surface reconstruction from sparse views. In European Conference on Computer\nVision, pp. 210\u2013227. Springer, 2022.\nXiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma,\nSong-Hai Zhang, Marc Habermann, Christian Theobalt, and Wenping Wang. Wonder3d: Single\nimage to 3d using cross-domain diffusion, 2023.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nWei-Chiu Ma, Anqi Joyce Yang, Shenlong Wang, Raquel Urtasun, and Antonio Torralba. Virtual\ncorrespondence: Humans as a cue for extreme-view geometry. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 15924\u201315934, June 2022.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\nRoger Mohr, Long Quan, and Franc\u00b8oise Veillon. Relative 3d reconstruction using multiple uncali-\nbrated images. The International Journal of Robotics Research, 14(6):619\u2013632, 1995.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM Transactions on Graphics (ToG), 41(4):1\u201315,\n2022.\nMichael Niemeyer, Jonathan T Barron, Ben Mildenhall, Mehdi SM Sajjadi, Andreas Geiger, and\nNoha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from sparse inputs.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n5480\u20135490, 2022.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint\narXiv:2212.09748, 2022.\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Con-\nvolutional occupancy networks. In Computer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16, pp. 523\u2013540. Springer, 2020.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and\nDavid Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d cate-\ngory reconstruction. In International Conference on Computer Vision, 2021.\nYufan Ren, Tong Zhang, Marc Pollefeys, Sabine S\u00a8usstrunk, and Fangjinhua Wang.\nVolrecon:\nVolume rendering of signed ray distance functions for generalizable multi-view reconstruction.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n16685\u201316695, 2023.\nJerome Revaud, Philippe Weinzaepfel, C\u00b4esar De Souza, Noe Pion, Gabriela Csurka, Yohann Cabon,\nand Martin Humenberger. R2d2: repeatable and reliable detector and descriptor. arXiv preprint\narXiv:1906.06195, 2019.\nChris Rockwell, Justin Johnson, and David F Fouhey. The 8-point algorithm as an inductive bias for\nrelative pose prediction by vits. In 2022 International Conference on 3D Vision (3DV), pp. 1\u201311.\nIEEE, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\n17\nMehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs Bergmann, Klaus Greff, Noha Radwan,\nSuhani Vora, Mario Lucic, Daniel Duckworth, Alexey Dosovitskiy, Jakob Uszkoreit, Thomas\nFunkhouser, and Andrea Tagliasacchi. Scene Representation Transformer: Geometry-Free Novel\nView Synthesis Through Set-Latent Scene Representations.\nCVPR, 2022.\nURL https://\nsrt-paper.github.io/.\nPaul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine:\nRobust hierarchical localization at large scale. In CVPR, 2019.\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:\nLearning feature matching with graph neural networks. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 4938\u20134947, 2020.\nJohannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 4104\u20134113, 2016.\nJohannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise\nview selection for unstructured multi-view stereo. In European Conference on Computer Vision\n(ECCV), 2016.\nRuoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen,\nChong Zeng, and Hao Su. Zero123++: a single image to consistent multi-view diffusion base\nmodel, 2023a.\nYichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view\ndiffusion for 3d generation. arXiv:2308.16512, 2023b.\nJamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew\nFitzgibbon. Scene coordinate regression forests for camera relocalization in rgb-d images. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 2930\u20132937,\n2013.\nPatrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural\nnetworks applied to visual document analysis. In Icdar, volume 3. Edinburgh, 2003.\nSamarth Sinha, Jason Y Zhang, Andrea Tagliasacchi, Igor Gilitschenski, and David B Lin-\ndell. Sparsepose: Sparse-view camera pose regression and refinement. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21349\u201321359, 2023.\nCameron Omid Smith, Yilun Du, Ayush Tewari, and Vincent Sitzmann. Flowcam: Training gener-\nalizable 3d radiance fields without camera poses via pixel-aligned scene flow. In Thirty-seventh\nConference on Neural Information Processing Systems, 2023. URL https://openreview.\nnet/forum?id=apFDDJOYf5.\nNoah Snavely, Steven M Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in\n3d. In ACM siggraph 2006 papers, pp. 835\u2013846. 2006.\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srini-\nvasan.\nRef-nerf: Structured view-dependent appearance for neural radiance fields.\nIn 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5481\u20135490.\nIEEE, 2022.\nGuangcong Wang, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth\nranking for few-shot novel view synthesis. arXiv preprint arXiv:2303.16196, 2023.\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction. In Advances\nin Neural Information Processing Systems, 2021.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013\n612, 2004.\n18\nTong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren, Liang Pan, Wayne Wu, Lei Yang, Jiaqi\nWang, Chen Qian, et al. Omniobject3d: Large-vocabulary 3d object dataset for realistic percep-\ntion, reconstruction and generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 803\u2013814, 2023.\nYinghao Xu, Hao Tan, Fujun Luan, Sai Bi, Peng Wang, Jiahao Li, Zifan Shi, Kalyan Sunkavalli,\nGordon Wetzstein, Zexiang Xu, and Kai Zhang. Dmv3d: Denoising multi-view diffusion using\n3d large reconstruction model, 2023.\nJiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with\nfree frequency regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 8254\u20138263, 2023.\nZhenpei Yang, Zhile Ren, Miguel Angel Bautista, Zaiwei Zhang, Qi Shan, and Qixing Huang. Fvor:\nRobust joint shape and pose optimization for few-view object reconstruction. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2497\u20132507, 2022.\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lip-\nman. Multiview neural surface reconstruction by disentangling geometry and appearance. Ad-\nvances in Neural Information Processing Systems, 33:2492\u20132502, 2020.\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces.\nIn Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\nJianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent im-\nage to 3d view synthesis via geometry-aware diffusion models. arXiv preprint arXiv:2310.03020,\n2023.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from\none or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 4578\u20134587, 2021.\nXianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan,\nChenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of\nmulti-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 9150\u20139161, 2023.\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving\nneural radiance fields. arXiv preprint arXiv:2010.07492, 2020.\nKai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf:\nArtistic radiance fields, 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\nZhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d\nreconstruction. In CVPR, 2023.\n19\n\u00a9 2023 Adobe. All Rights Reserved. Adobe Confidential.\nCorrupted images\nRender our pred. NeRF w/\nour pred. poses\nCorrupt masks\nOurs error maps\nFigure 5: Our PF-LRM is robust to small mask segmentation errors.\nA\nAPPENDIX\nA.1\nVISUAL COMPARISONS OF PREDICTED CAMERA POSES\nIn Fig. 6, we present visual comparisons of the predicted camera poses with our method and base-\nline methods. We can see that it\u2019s common for baseline methods FORGE (Jiang et al., 2022) and\nRelPose++ (Lin et al., 2023a) to make predictions significantly deviating from the ground truth and\nin some situations, their predicted camera poses can be even on the opposite side. In contrast, our\npredicted poses closely align with the ground truth consistently.\nTable 6: Category-level comparison of pose prediction results with baseline RelPose++ (Lin et al.,\n2023a) on CO3D dataset (Reizenstein et al., 2021). We report the mean pose errors and (top two\nrows) and rotation accuracy@15\u25e6 (bottom two rows) on 10 different test categories.\nBall\nBook\nCouch\nFris.\nHot.\nKite\nRem.\nSand.\nSkate.\nSuit.\nRelPose++ (w/ bg)\n30.29\n31.34\n24.82\n34.01\n21.61\n50.18\n32.00\n30.84\n36.91\n14.13\nOurs\n17.17\n8.36\n29.04\n20.16\n27.88\n16.18\n6.05\n15.92\n25.39\n12.03\nRelPose++ (w/ bg)\n0.613\n0.782\n0.787\n0.742\n0.742\n0.570\n0.767\n0.697\n0.630\n0.893\nOurs\n0.787\n0.947\n0.688\n0.780\n0.697\n0.807\n0.944\n0.890\n0.778\n0.923\nA.2\nADDITIONAL EXPERIMENTS\nRobustness to novel environment lights. We evaluate our model\u2019s robustness to different en-\nvironment lights in Table 7.\nThe evaluations are conducted in 100 object samples from GSO\ndataset (Downs et al., 2022). Our model shows consistent results under different lighting condi-\ntions. We also qualitatively shows our model robustness to different illuminations in Fig. 8. Note\nthat PSNRg.t. reflects how well renderings of our predicted NeRF using ground-truth input poses\nmatch the input images, while PSNRpred measures how well renderings of our predicted NeRF\nusing our poses predictions match the inputs.\nTable 7: Evaluation results on GSO data with different novel environment lights. The evaluations\nare conducted in 100 objects samples. Note our synthesized multi-view training images are rendered\nusing uniform light. Our method can generalize well to novel environment lights.\nMethod\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nT. error\nPSNRg.t.\nPSNRpred.\nSunset\n2.40\n0.968\n0.983\n0.027\n27.56\n26.74\nSunrise\n2.22\n0.985\n0.993\n0.024\n27.17\n26.21\nStudio\n2.82\n0.983\n0.992\n0.029\n27.31\n26.69\nUniform\n3.94\n0.968\n0.972\n0.040\n27.50\n26.80\nAblations of pose prediction methods. We illustrate the effectiveness of our differentiable PnP\npose prediction method in Tab. 10 by replacing it with alternative solutions. The first line \u2018diff. PnP\u2019\nis our model with small config, i.e., \u2018Ours (S)\u2019. For other lines, we replace the \u2018diff. PnP\u2019 with other\nalternatives. \u2018MLP pose (CLS token)\u2019 takes the [CLS] token of each view in the last transformer\nlayer to a MLP to predict pose, and supervise pose with a quaternion loss and a translation loss.\n20\n\u00a9 2023 Adobe. A\nOurs\nFORGE\nRelPose++\nGT\nFigure 6: Predicted poses from our method align much more closely with the ground-truth than\nthose from baseline methods including FORGE (Jiang et al., 2022), RelPose++ (Lin et al., 2023a).\n21\nInput images\nGSO\nABO\nOmniObject3D\nNovel G.T. images  \nOur geometry\nCO3D\nDTU\nRender our pred. NeRF w/ \nour pred. poses \nRender our pred. NeRF w/ \nnovel poses\nFigure 7: Additional qualitative results of our model\u2019s cross-dataset generalization to unseen Om-\nniObject3D (Wu et al., 2023), GSO (Downs et al., 2022), ABO (Collins et al., 2022), CO3D (Reizen-\nstein et al., 2021), and DTU (Aan\u00e6s et al., 2016) datasets.\n22\n\u00a9 2023 Adobe. All Rights Reserved. Adobe Confidential.\nInput images (novel lighting)\nInput images (uniform lighting)\nRender our pred. NeRF w/\nour pred. poses\nRender our pred. NeRF w/\nour pred. poses\nFigure 8: Our PF-LRM is robust to illumination changes.\nAlthough this model can predict plausible reconstructions and poses, its performance is far worse\nthan our full model where we use a differentiable PnP solver to predict poses. We argue that this\nis because pose prediction has multiple local minimas, and the regression-based pose loss is more\nprone to such local minimas, compared with the EPro-PnP solver (Chen et al., 2022b) we use in this\nwork. \u2018MLP pose (Patch tokens)\u2019 take concatenated patch-wise features to a MLP for predicting\npose. It aims to leverage more dense patch token information in the pose prediction. The perfor-\nmance of this variant is roughly the same as \u2018MLP pose (CLS token)\u2019. \u2018non-diff. PnP\u2019 removes the\ndifferentiable PnP prediction and only use the losses Lp and L\u03b1. This way, we have a set of 3D-2D\ncorrespondences weighted by predicted opacity that are passed to a PnP solver for getting the poses.\nWe find that this variant leads to worse performance than its differentiable PnP counterpart, due to\nthe lack of learning proper confidence of 3D-2D correspondences. Note that PSNRg.t. reflects how\nwell renderings of our predicted NeRF using ground-truth input poses match the input images, while\nPSNRpred measures how well renderings of our predicted NeRF using our pose predictions match\nthe inputs. We observe that worse pose predictions tend to lead to worse reconstruction quality, as\nshown by the positive correlation between pose accuracy and PSNRg.t. scores in Tab. 10.\nA.3\nADDITIONAL IMPLEMENTATION DETAILS\nOur model uses a pre-trained DINO ViT as our image encoder. We bilinearly interpolate the original\npositional embedding to the desired image size. For each view, its view encoding vector and camera\nintrinsics are first mapped to a modulation feature, and then passed to the adaptive layer norm\nblock (Hong et al., 2023; Peebles & Xie, 2022; Huang & Belongie, 2017) to predict scale and bias\nfor modulating the intermediate feature activations inside each transformer block (self-attention +\nMLP) of the DINO ViT (Caron et al., 2021). Take the reference view as an example; its modulation\nfeature mr is defined as:\nmr = MLPintrin.([fx, fy, cx, cy]) + vr,\n(13)\nwhere fx, fy, cx, cy are camera intrinsics, and vr is the view encoding vector. We then use the\nmodulation feature mr in the same way as the camera feature in LRM (Li et al., 2023).\nWe then concatenate the image tokens with the learnable triplane position embedding to get a long\ntoken sequence, which is used as input to the single-stream transformer. We use the multi-head\nattention with head dimension 64. During rendering, the three planes are queried independently\nand the three features are concatenated as input of the NeRF MLP to get the RGB color and NeRF\ndensity. For per-view geometry prediction used for PnP solver, we use the image tokens output by\nthe transformer with MLP layers to get the point predictions, the confidence predictions, and also\nthe alpha predictions.\nIn our experiments we have models with two different sizes. In the ablation studies as described\nin Sec. 4.4, the \u2018Ours (S)\u2019 model has 24 self-attention layers, while the \u2018Ours (L)\u2018 model has 36\nself-attention leyers. More details of the two model configurations are presented in Tab. 8.\nWe use the following techniques to save the GPU memory for our model training: 1) Mixed preci-\nsion with BFloat16, 2) deferred back-propagation in NeRF rendering (Zhang et al., 2022), and 3)\nGradient checkpointing at every 4 self-attention layers. We also adopt the FlashAttention V2 (Dao,\n2023) to reduce the overall training time.\n23\nOurs (S)\nOurs (L)\nDINO Encoder\nImage resolution\n256\u00d7256\n512\u00d7512\nPatch size\n16\n16\nAtt. Layers\n12\n12\nAttention channels\n768\n768\nView encoding\n768\n768\nIntrinsics-cond. MLP layers\n5\n5\nIntrinsics-cond. MLP width\n768\n768\nIntrinsics-cond. MLP act.\nGeLU\nGeLU\nTransformer\nTriplane tokens\n32 \u00d7 32 \u00d7 3\n32 \u00d7 32 \u00d7 3\nAttention channels\n1024\n1024\nAttention heads\n16\n16\nAttention layers\n24\n36\nTriplane upsample\n1\n2\nTriplane shape\n32 \u00d7 32 \u00d7 3 \u00d7 32\n64 \u00d7 64 \u00d7 3 \u00d7 32\nRenderer\nRendering patch size\n64\n128\nRay-marching steps\n64\n128\nMLP layers\n5\n5\nMLP width\n64\n64\nActivation\nReLU\nReLU\nPoint MLP\nMLP layers\n4\n4\nMLP width\n512\n512\nActivation\nGeLU\nGeLU\nTraininig\nLearning rate\n4e-4\n4e-4\nOptimizer\nAdamW\nAdamW\nBetas\n(0.9, 0.95)\n(0.9, 0.95)\nWarm-up steps\n3000\n3000\nBatch size per GPU\n16\n8\n#GPUS\n8\n128\nTable 8: Configuration of our models.\nA.4\nADDITIONAL RESULTS\nCategory-level results on CO3D dataset. In Tab. 6 we report the per-category results and com-\nparisons to RelPose++ on held-out CO3D test set provided by RelPose++ Lin et al. (2023a). We\noutperform RelPose++ (w/ bg) on 8 out of 10 categories, despite that we are not trained on CO3D\ntraining set while RelPose++ is. In addition, our model is now limited to handle images without\nbackground; hence we use the masks included in the CO3D dataset to remove background before\ntesting our model. The masks, however, seem to be very noisy upon our manual inspection; this\nnegatively influenced our model\u2019s performance, but not RelPose++ (w/ bg). An interesting future\ndirection is to extend our model to support images with background to in order to lift the impacts of\n2D mask errors.\nTable 9: Evaluation results on GSO data (Downs et al., 2022) rendered by FORGE (Jiang et al.,\n2022). We note that these renderings are a bit darker than majority of our training images, but our\nmodel still generalizes well to this dataset. Our model produces sharper renderings than FORGE\n(indicated by the higher SSIM score), while producing more accurate camera estimates.\nMethod\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nT. error\nPSNRg.t.\nPSNRpred.\nSSIMg.t.\nSSIMpred.\nFORGE\n50.20\n0.253\n0.514\n0.573\n21.25\n22.90\n0.767\n0.793\nFORGE (refine)\n49.02\n0.307\n0.527\n0.548\n22.08\n25.89\n0.767\n0.838\nOurs\n8.37\n0.908\n0.954\n0.105\n23.05\n24.42\n0.860\n0.886\nA.5\nADDITIONAL CROSS-DATASET EVALUATIONS\nTo further demonstrate the generalization capability of our model, we evaluate our model (trained\non a mixture of Objaverse and MVImgNet) on another version of GSO dataset (Downs et al., 2022)\n24\nTable 10: Ablation study of different pose prediction methods on the GSO data (Downs et al.,\n2022). Ablations are conducted using methods are our small model, i.e., \u2018Ours (S)\u2019. Compared with\nour method of predicting per-view coarse geometry followed by differentiable PnP (Chen et al.,\n2022b), the MLP-based pose prediction method conditioning on either the per-view CLS token or\nthe concatenated patch tokens perform much worse due to the lack of explicit geometric inductive\nbias (either 3D-2D correspondences or 2D-2D correspondences) in pose registrations. Besides, we\nalso find that differentiable PnP learns to weigh the 3D-2D correspondences induced from the per-\nview predicted coarse geometry properly, resulting a boost in pose estimation accuracy.\nSetting\nR. error\nAcc.@15\u25e6\nAcc.@30\u25e6\nT. error\nPSNRg.t.\nPSNRpred.\ndiff. PnP (our default setting)\n13.08\n0.848\n0.916\n0.135\n23.80\n22.82\nMLP pose (CLS token)\n25.32\n0.655\n0.809\n0.264\n22.27\n19.80\nMLP pose (Patch tokens)\n21.60\n0.688\n0.836\n0.230\n22.02\n19.76\nnon-diff. PnP\n22.03\n0.570\n0.814\n0.236\n23.56\n18.65\n(which is rendered by the FORGE paper). Note that these renderings are a bit darker than majority\nof our training images, but as shown in Tab. 1, our model still generalizes well to this dataset.\nOur model produces sharper renderings than FORGE with and without its per-scene optimization-\nbased refinement (indicated by the higher SSIM score), while producing much more accurate camera\nestimates. Note that PSNRg.t., SSIMg.t. reflect how well renderings of our predicted NeRF using\nground-truth input poses match the input images, while PSNRpred, SSIMpred measures how well\nrenderings of our predicted NeRF using ground-truth input poses match the inputs.\nA.6\nSCALING UP TRAINING OF RELPOSE++\nTo further demonstrate our method\u2019s superiority over the baseline method RelPose++ (Lin et al.,\n2023a), we re-train RelPose++ on the Objaverse dataset until full convergence for a more fair com-\nparison. We then compare the re-trained model with our model (\u2018Ours (S)\u2019 and \u2018Ours (L)\u2019) trained\non exactly the same Objaverse renderings in Tab. 11. The re-trained RelPose++ using Objaverse\ndoes improve over the pretrained one using CO3D on the unseen test sets, OmniObject3D, GSO and\nABO. However, our models (both \u2018Ours (S)\u2019 and \u2018Ours (L)\u2019) consistently outperform the re-trained\nbaseline by a large margin in terms of rotation and translation prediction accuracy. We attribute this\nto our joint prediction of NeRF and poses that effectively exploit the synergy between these two\ntasks; in addition, unlike RelPose++ that regresses poses, we predict per-view coarse point cloud\n(supervised by distilling our predicted NeRF geometry in an online manner) and use a differentiable\nsolver to get poses. This make us less prone to getting stuck in pose prediction local minimas than\nregression-based predictors, as also pointed out by Chen et al. (2022b).\n25\nTable 11: Comparisons of cross-dataset generalization on GSO (Downs et al., 2022), ABO (Collins\net al., 2022), OmniObject3D (Wu et al., 2023) with RelPose++ (Lin et al., 2023a) using the author-\nprovided checkpoint (trained on CO3D (Reizenstein et al., 2021) and our re-trained checkpoint\n(trained on Objaverse (Deitke et al., 2023)). \u2018Ours (S)\u2019 and \u2018Ours (L)\u2019 are trained only on Objaverse\nas well for fair comparison. Though the re-trained RelPose++ improves over the pretrained version,\nwe (both \u2018Ours (S)\u2019 and \u2018Ours (L)\u2019) still achieve much better pose prediction accuracy than it.\nOmniObject3D\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nRelPose++ (w/o bg, pretrained)\n69.22\n0.070\n0.273\n0.712\nRelPose++ (w/o bg, Objaverse)\n58.67\n0.304\n0.482\n0.556\nOurs (S)\n15.06\n0.695\n0.910\n0.162\nOurs (L)\n7.25\n0.958\n0.976\n0.075\nGSO\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nRelPose++ (w/o bg, pretrained)\n107.49\n0.037\n0.098\n1.143\nRelPose++ (w/o bg, Objaverse)\n45.58\n0.600\n0.686\n0.407\nOurs (S)\n13.08\n0.848\n0.916\n0.135\nOurs (L)\n2.46\n0.976\n0.985\n0.026\nABO\nMethod\nR. error \u2193\nAcc.@15\u25e6 \u2191\nAcc.@30\u25e6 \u2191\nT. error \u2193\nRelPose++ (w/o bg, pretrained)\n102.30\n0.060\n0.144\n1.103\nRelPose++ (w/o bg, Objaverse)\n45.39\n0.693\n0.708\n0.395\nOurs (S)\n26.31\n0.785\n0.822\n0.249\nOurs (L)\n13.99\n0.883\n0.892\n0.131\n26\n"
  },
  {
    "title": "GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning",
    "link": "https://arxiv.org/pdf/2311.12631.pdf",
    "upvote": "12",
    "text": "GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via\nBlender-Oriented GPT Planning\nJiaxi Lv1,2\u2217\nYi Huang1,2\u2217\nMingfu Yan1,2\u2217\nJiancheng Huang1,2\nJianzhuang Liu1\nYifan Liu1\nYafei Wen3\nXiaoxin Chen3\nShifeng Chen1\u2020\n1Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences,\n2University of Chinese Academy of Sciences,3VIVO AI Lab\nhttps://GPT4Motion.github.io\nAbstract\nRecent advances in text-to-video generation\nhave harnessed the power of diffusion mod-\nels to create visually compelling content con-\nditioned on text prompts. However, they usu-\nally encounter high computational costs and\nstruggle to produce videos with coherent phys-\nical motions. To tackle these issues, we pro-\npose GPT4Motion, a training-free framework\nthat leverages the planning capability of large\nlanguage models like GPT, the physical sim-\nulation strength of Blender, and the image\ngeneration ability of text-to-image diffusion\nmodels to enhance video synthesis. Specifi-\ncally, GPT4Motion employs GPT-4 to gener-\nate a Blender script based on a user textual\nprompt, which commands Blender\u2019s built-in\nphysics engine to craft fundamental scene com-\nponents containing coherent physical motions\nacross frames. Then these components are in-\nputted into Stable Diffusion to generate a video\naligned with the textual prompt. Experimen-\ntal results on three basic physical motion sce-\nnarios demonstrate that GPT4Motion can gen-\nerate high-quality videos efficiently in main-\ntaining motion coherency and entity consis-\ntency. GPT4Motion offers new insights in text-\nto-video research, enhancing its quality and\nbroadening its horizon for further explorations.\n1\nIntroduction\nDiffusion models (Ho et al., 2020; Song et al.,\n2021a,b) have recently advanced high-quality\ntext-to-image (T2I) synthesis (Ramesh et al., 2022;\nRombach et al., 2022; Saharia et al., 2022), leading\nto the exploration of text-to-video (T2V) genera-\ntion. Earlier works train T2V diffusion models in\npixel (Ho et al., 2022b; Singer et al., 2023; Ho et al.,\n2022a) or latent spaces (Blattmann et al., 2023;\nZhou et al., 2022; Wang et al., 2023). Despite the\npromising results they yield, the heavy computa-\ntional costs are unbearable. To reduce training\n\u2217Equal contributions.\n\u2020Corresponding Author.\nPrompt\nGPT4Motion\nAnimateDiff\nModelScope\nText2Video-Zero\nDirecT2V\nFigure 1: Comparison of the video results generated by\ndifferent text-to-video models with the prompt \u201cA bas-\nketball free falls in the air\". Best viewed with Acrobat\nReader for animation.\nefforts, recent works have shifted towards training-\nfree approaches, such as Text2Video-Zero (Khacha-\ntryan et al., 2023), which use pretrained T2I mod-\nels to synthesize videos without additional train-\ning, aiming to lessen resource demands. However,\nthese methods struggle with motion coherence. To\novercome this, recent studies (Huang et al., 2023a;\nHong et al., 2023; Lian et al., 2023; Lin et al., 2023)\nhave harnessed the descriptive power of large lan-\n1\narXiv:2311.12631v2  [cs.CV]  18 Feb 2024\nguage models (LLMs) (Ouyang et al., 2022; Wei\net al., 2021), such as GPT-4 (OpenAI, 2023), to\ngenerate frame-by-frame descriptions and explicit\nspatiotemporal layouts, enhancing narrative con-\ntinuity and motion coherence in video sequences\ngenerated from a single user prompt. Despite the\nenhanced video quality they achieve, maintaining\nmotion coherence in the scenes of large motion\nshifts is still challenging.\nMotivated by them, we propose GPT4Motion, a\ntraining-free framework leveraging GPT-4\u2019s plan-\nning capability, the physical simulation strength\nof Blender1, and the image generation ability of\nStable Diffusion (Rombach et al., 2022) to enhance\nthe quality of video synthesis. Given a user textual\nprompt, GPT4Motion first employs GPT-4 to pro-\nduce Blender scripts that drive the creation of basic\nvideo scene elements, including edges and depth\nmaps. These elements then serve as conditions for\nStable Diffusion to generate the final video. This\nmethodology ensures that the resulting video not\nonly faithfully aligns with the textual prompt but\nalso ensures motion coherence across all frames, as\nshown in Figure 1. The contributions of our work\nare summarized in the following.\n\u2022 We demonstrate GPT-4\u2019s ability to guide\nBlender in simulating physical motion scenes,\nhighlighting LLMs\u2019 role in creating physics-\nbased videos.\n\u2022 We propose GPT4Motion, a training-free\nframework that employs scripts generated by\nGPT-4 for Blender simulations, allowing for\nthe generation of temporally coherent videos\nthrough Stable Diffusion.\n\u2022 Experiments on three basic physical motion\nscenarios prove GPT4Motion\u2019s capability to\ngenerate high-quality videos with both motion\ncoherency and entity consistency.\n2\nRelated Work\n2.1\nText-to-Video Generation\nText-to-video (T2V) generation, aiming to cre-\nate videos from textual descriptions, remains in\nits early stages despite significant advancements\nin text-to-image (T2I) synthesis (Rombach et al.,\n2022; Dhariwal and Nichol, 2021; James et al.,\n1Blender is a popular open-source 3D creation suite that of-\nfers a comprehensive set of tools for 3D modeling, animation,\nand rendering. See https://www.blender.org/ for details.\n2023; Saharia et al., 2022). The introduction of dif-\nfusion models (Ho et al., 2020; Song et al., 2021a)\nhas facilitated developments in T2V, yet challenges\nsuch as motion incoherence and entity inconsis-\ntency persist. Large language models (LLMs) like\nGPT-4 (OpenAI, 2023), PaLM (Anil et al., 2023),\nand BLOOM (Scao et al., 2022) have demonstrated\ntheir versatility across various multimodal tasks,\nsuggesting their potential utility in T2V. Incorpo-\nrating LLMs into T2V, innovations have emerged,\nsuch as narrative generation through Free-bloom\n(Huang et al., 2023a) and spatiotemporal layout cre-\nation in LVD (Lian et al., 2023), which guide the\nsynthesis process. This paper introduces an innova-\ntive method that leverages the combined strengths\nof GPT-4 and Blender, addressing key challenges\nsuch as motion incoherence and physical accuracy.\nThis represents a significant step forward, bridg-\ning the gap between textual descriptions and high-\nquality video generation.\n2.2\nBlender in Deep Learning\nBlender is an open-source 3D creation suite that\nprovides tools for modeling, animation, and render-\ning, facilitating the creation of detailed 3D scenes.\nIt is increasingly used in deep learning for syn-\nthetic data generation, as seen in the S2RDA bench-\nmark (Tang and Jia, 2023) for image classification\nand in projects like 3D-GPT (Sun et al., 2023),\nwhich leverages Blender for procedural 3D model-\ning. However, Blender\u2019s potential in text-to-video\n(T2V) synthesis remains unexplored, mainly due\nto the requirements of much professional technical\nknowledge and complex manual procedures such\nas texturing, rigging, animation, lighting and com-\npositing. Our approach, GPT4Motion, leverages\nGPT-4 to automate Blender scripting, offering a\nstreamlined, user-friendly method for producing\nhigh-quality videos that are textually aligned and\nphysically accurate in object motion, marking a\nsignificant advancement in T2V technology.\n3\nMethod\n3.1\nTask Formulation\nGiven a user prompt about some basic physical\nmotion scenario, we aim to generate a physically\naccurate video. Physical phenomena are often asso-\nciated with the material of the object. We focus on\nsimulating three common types of object materials\nencountered in daily life: 1) Rigid Objects, such as\nballs, which maintain their shapes when subjected\n2\nFigure 2: The architecture of our GPT4Motion. First, the user prompt is inserted into our designed prompt template.\nThen, the Python script generated by GPT-4 drives the Blender physics engine to simulate the corresponding motion,\nproducing sequences of edge maps and depth maps. Finally, two ControlNets are employed to constrain the physical\nmotion of video frames generated by Stable Diffusion, where a temporal consistency constraint is designed to\nenforce the coherence among frames.\nto forces; 2) Cloth, such as flags, characterized by\ntheir softness and propensity to flutter; 3) Liquid,\nsuch as water, which exhibits continuous and de-\nformable motions. Moreover, we give particular\nattention to several typical motion modes for these\nmaterials, including collisions (direct impacts be-\ntween objects), wind effects (motion induced by air\ncurrents), and flow (continuously and easily move\nin one direction). Simulating these physical scenar-\nios typically involves knowledge of Classical Me-\nchanics (Goldstein et al., 2002), Fluid Mechanics\n(Kundu et al., 2015) and other physical knowledge.\nCurrent text-to-video diffusion models struggle to\ncapture this complex physical knowledge through\ntraining, thereby failing to produce videos that ad-\nhere to physical principles.\nTo address these challenges, we propose a novel\ntraining-free text-to-video generation framework,\nnamed GPT4Motion, which is illustrated in Figure\n2. The advantage of our approach is that GPT-4\u2019s\nsemantic understanding and code generation capa-\nbilities are leveraged to translate the user prompt\ninto a Blender Python script. This script can drive\nBlender\u2019s built-in physics engine to simulate the\ncorresponding physical scene. We then introduce\nControlNet (Zhang and Agrawala, 2023), which\ntakes as input the dynamic results of the Blender\nsimulation and directs Stable Diffusion to generate\neach frame of the video. This framework ensures\nthat the generated video is not only consistent with\nthe user prompt, but also physically correct. In\nthe next sections, we describe the details of our\nframework.\n3.2\nBlender Simulations via GPT-4\nGPT-4 is a large language model pre-trained on\nhuge amounts of Internet data with great capability\nfor semantic understanding and code generation.\nWe have observed that while GPT-4 has a certain\nknowledge about the Blender Python API, it still\nstruggles with generating Blender Python scripts\nbased on user prompts. On the one hand, asking\nGPT-4 to create even a simple 3D model (like a\nbasketball) directly in Blender seems to be an over-\nwhelming task (Sun et al., 2023). On the other\nhand, because the Blender Python API has fewer\nresources and its API version is updated quickly,\nGPT-4 can easily misuse certain functions or make\nerrors due to version differences. To address these\nissues, we propose the following schemes:\nLeveraging External 3D Models.\nCreating 3D\nmodels typically requires professional artists to\nmanually craft them, spending substantial time\n3\nFigure 3: Our prompt template designed for GPT-4. It\ncontains information about functions, external assets,\nand instruction. The user prompt is inserted into the\nplaceholder \u201c{PROMPT}\".\nsculpting details, painting fine texture maps, and\noptimizing the model topology, which GPT-4 can-\nnot independently accomplish. Fortunately, there\nis a large amount of 3D models available on the\nInternet2. Hence, we have collected common 3D\nobjects from everyday life and can automatically\nload the 3D models via scripts corresponding to\ntextual prompts.\n2https://www.blenderkit.com/\nEncapsulating Blender Functions.\nAlthough\nGPT-4 possesses the necessary knowledge of the\nBlender Python API, writing a lengthy script to ren-\nder an entire scene remains challenging. We note\nthat for our target scenarios, Blender Python scripts\ntypically consist of several fixed steps, including\nscene initialization, rendering, object creation and\nimport, and physical effects. Thus, we guide GPT-\n4 to encapsulate these reusable functions (see the\nAppendix Section D). By doing so, we have greatly\nsimplified the entire process from user prompts to\nrendering corresponding physical scenarios. These\nencapsulated functions can be broadly categorized\ninto three types:\n\u2022 Scene initialization and rendering functions.\nThese functions are responsible for clearing\nthe default initial scene and performing the\nrendering. In Blender, one can set up the si-\nmultaneous image outputs of depth, normal,\nedge, and segmentation for a video. We find\nthat using edge and depth images yields good\nperformance in our framework, so we render\nthese edge and depth images for video genera-\ntion.\n\u2022 Object creation and import functions. These\nfunctions offer the capability to create ba-\nsic objects (such as viewpoints, floors, cubes,\nspheres, etc.) within a Blender scene. In ad-\ndition to creating simple objects, we also pro-\nvide import functions that allow users to bring\nexternal 3D models into Blender.\n\u2022 Physics effect functions. These functions en-\ncapsulate the basic physics and material effect\nsettings within Blender. For instance, they\ncan assign different physical types (such as\nrigid, cloth, or liquid) to objects, impart initial\nvelocities and rotations to objects, or set up\nwind force effects.\nTranslating User Prompts into Physics.\nFigure\n3 shows the general prompt template we design for\nGPT-4. It includes encapsulated Blender functions,\nexternal assets, and instruction. We define the di-\nmensions of the virtual world in the template and\nprovide information about the camera\u2019s position\nand viewpoint. Such information aids GPT-4 in\nbetter understanding the layout of the 3D space.\nUltimately, the user prompt becomes part of the\ninstruction, directly guiding GPT-4 to generate the\ncorresponding Blender Python script. Finally, with\n4\nthis script, Blender renders the edge and depth im-\nage sequences.\n3.3\nVideo Synthesis with Physical Conditions\nOur goal is to generate a consistent and realistic\nvideo based on the user prompt and corresponding\nphysical motion conditions provided by Blender.\nWe adopt Stable Diffusion XL (SDXL) (Podell\net al., 2023), an upgraded version of Stable Diffu-\nsion (Rombach et al., 2022). We made the follow-\ning modifications to SDXL.\nPhysics\nMotion\nConstraints.\nControlNet\n(Zhang and Agrawala, 2023) is a network ar-\nchitecture that can control the image generation\nof a pretrained text-to-image diffusion model\nwith additional conditions, such as edge or depth.\nHowever, a single ControlNet is limited to one\ntype of condition. The generation of some physical\nmotion videos requires the control of multiple\nconditions. For example, when generating a video\nof a basketball in free fall, its edges can accurately\nreflect its texture changes, but the edges cannot\nreflect 3D layout of the scene, resulting in the lack\nof realism in the video. On the other hand, the\ndepth map of the scene helps address this problem\nbut is unable to capture the texture changes of the\nbasketball. Therefore, we leverage a combination\nof Canny-edge-based ControlNet and depth-based\nControlNet to precisely control the generation of\nthe video. Specifically, we add the intermediate\nresults of the two ControlNets together to serve as\nthe final conditions for SDXL.\nTemporal Consistency Constraint.\nTo ensure\ntemporal consistency across different frames of a\nvideo, we modify the self attention (SA) in the\nU-Net of SDXL into cross-frame attention (CFA).\nSpecifically, the self attention in the U-Net uses\nlinear projections W Q, W K, and W V to project\nthe feature Fi of the i-th frame (for simplicity, we\nignore the time-step t) into Qi = W QFi, Ki =\nW KFi, and Vi = W V Fi, and perform the self\nattention calculation:\nSA(Qi, Ki, Vi) = Softmax\n\u0012QiKT\ni\n\u221a\nd\n\u0013\nVi,\n(1)\nwhere d is a scaling factor. To obtain the cross-\nframe attention, we concatenate the feature of the\nframe Fi, i \u0338= 1, with the first frame F1 for K and\nV , while keeping Q unchanged:\nKi,1 = W K[F1, \u03b1Fi], Vi,1 = W V [F1, Fi],\n(2)\nand the cross-frame attention operation is:\nCFA(Qi, Ki,1, Vi,1) = Softmax\n \nQiKT\ni,1\n\u221a\nd\n!\nVi,1,\n(3)\nwhere [\u00b7, \u00b7] denotes the concatenation, and \u03b1 \u2208\n[0, 1] is a hyperparameter. We find that increasing\n\u03b1 improves the fidelity of the moving object but\nat the same time brings more flickering; on the\ncontrary, decreasing \u03b1 reduces the flickering but\nalso decreases the fidelity of the moving object.\nThe cross-frame attention has the effect that the\ni-th frame pays attention to not only itself but also\nthe first frame. Surprisingly, by this cross-frame\nattention design, the generated video frames exhibit\nremarkable content consistency. Additionally, we\nemploy the same initial noise for SDXL to generate\nall the frames of the video, which further enhances\nthe temporal consistency.\n4\nExperiments\n4.1\nImplementation Details\nIn our experiments, we use the Stable Diffusion\nXL 1.0-base model3, along with Canny-edge-based\nControlNet4 and depth-based ControlNet5. The \u03b1\nin the rigid object, cloth, and liquid experiments\nare set to 0.9, 0.75, and 0.4, respectively.\nWe\nuse the DDIM sampler (Song et al., 2021a) with\nclassifier-free guidance (Ho and Salimans, 2022)\nand 50 sampling steps in our experiments on one\nNVIDIA A6000 GPU. The version of the Blender\nis 3.6. We generate 80-frame sequences of edge\nand depth maps at a resolution of 1920 \u00d7 1080\nfor each prompt. Theoretically, our method can\ngenerate motion video of any length and resolu-\ntion. For conciseness, in this paper, we show the\ncropped video with 1080\u00d71080 resolution. By the\nway, the videos in this experimental section may\nlook slow, which is because too many videos are\ndisplayed at the same time on the same page. To\nview the motion in these videos, please use Acrobat\nReader6. The original videos can be found in our\nsupplementary material.\n3https://huggingface.co/stabilityai/\nstable-diffusion-xl-base-1.0\n4https://huggingface.co/diffusers/\ncontrolnet-canny-sdxl-1.0\n5https://huggingface.co/diffusers/\ncontrolnet-depth-sdxl-1.0\n6https://www.adobe.com/acrobat/pdf-reader.\nhtml\n5\nFigure 4: GPT4Motion\u2019s results on basketball drop and\ncollision. Best viewed with Acrobat Reader for anima-\ntion.\nFigure 5: GPT4Motion\u2019s results on a fluttering flag.\n4.2\nControlling Physical Properties\nWe show the generative capabilities of our method\nin three physical scenarios.\nFurthermore, we\ndemonstrate how our approach allows for control\nover specific physical properties solely through\nuser prompts, thereby influencing the overall gen-\neration results.\nBasketball Drop and Collision.\nFigure 4 dis-\nplays basketball motion videos generated by our\nmethod with three prompts. In Figure 4 (left), the\nbasketball maintains a high degree of realism in its\ntexture while spinning, and accurately replicates\nthe bouncing behavior after collision with the floor.\nFigure 4 (middle) demonstrates that our method\ncan precisely control the number of basketballs and\nefficiently generate the collisions and bounces that\noccur when multiple basketballs land. Impressively,\nas shown in Figure 4 (right), when the user requests\nthat the basketball is thrown towards the camera,\nGPT-4 calculates the necessary initial velocity of\nthe basketball based on its fall time in the gener-\nated script, thereby achieving a visually convincing\neffect. This demonstrates that our approach can be\ncombined with the physical knowledge that GPT-4\nhas to control the content of the video generation\n(see the Appendix Section E).\nCloth Fluttering in Wind.\nFigures 5 and 6 val-\nidate our method\u2019s capability in generating the\nFigure 6: GPT4Motion\u2019s results on a fluttering T-shirt.\nmotion of cloth objects influenced by wind. Uti-\nlizing existing physics engines for simulation,\nGPT4Motion generates the fluctuations and waves\nof cloth under different wind strengths. In Figure 5,\nwe present the generated results of a flag fluttering.\nThe flag exhibits complex ripple and wave patterns\nunder different wind strengths. Figure 6 shows the\nmotion of an irregular cloth object, T-shirt, under\ndifferent wind strengths. Influenced by the physi-\ncal properties of the fabric, such as elasticity and\nweight, the T-shirt undergoes flapping and twisting,\nwith visible changes in creases and wrinkles.\nWater Pouring into a Mug.\nFigure 7 shows\nthree videos of water of different viscosities being\npoured into a mug. When the viscosity is low, the\nflowing water collides and merges with the water\nin the mug, creating complex turbulence on the sur-\nface. As the viscosity increases, the flow becomes\nslower and the water begins to stick together.\n4.3\nComparisons with Baselines\nWe compare our GPT4Motion against four base-\nlines: 1) AnimateDiff (Guo et al., 2023): combines\nStable Diffusion with a motion module, augmented\nby Realistic Vision DreamBooth7; 2) ModelScope\n(Wang et al., 2023), uses spatial-temporal convolu-\ntion and attention in Stable Diffusion for T2V tasks,\nutilizing LAION (Schuhmann et al., 2021) and We-\nbVid (Bain et al., 2021) datasets; 3) Text2Video-\nZero (Khachatryan et al., 2023), leverages image-\nto-image capabilities for generating videos through\ncross-attention and modified latent code sampling;\n4) DirecT2V (Hong et al., 2023), uses a LLM for\nframe-level descriptions from prompts, with rota-\ntional value mapping and dual-softmax for continu-\nity. To maintain the size of the paper, we only com-\npare GPT4Motion with these baselines on three\nexamples. More comparisons are given in the Ap-\npendix Section A.\n7https://civitai.com/models/4201?\nmodelVersionId=29460\n6\nFigure 7: GPT4Motion\u2019s results on the water pouring.\nBest viewed with Acrobat Reader for animation.\nAnimateDiff\nModelScope\nText2Video-Zero\nDirecT2V\nAnimateDiff\nModelScope\nText2Video-Zero\nDirecT2V\nFigure 8: Videos generated by four text-to-video base-\nlines with two user prompts.\nA Basketball Free Falls in the Air.\nThe visual\ncomparison of our method with other baselines is\npresented in Figure 1. Obviously, the baselines\u2019\nresults do not match the user prompt. DirecT2V\nand Text2Video-Zero face challenges in texture re-\nalism and motion consistency, whereas AnimateD-\niff and ModelScope improve video smoothness but\nstruggle with consistent textures and realistic move-\nments. In contrast to these methods, GPT4Motion\ncan generate smooth texture changes during the\nfalling of the basketball, and bouncing after colli-\nsion with the floor, which appear more realistic.\nA White Flag Flaps in the Wind.\nAs shown in\nFigure 8 (1st row), the videos generated by Ani-\nmateDiff and Text2Video-Zero exhibit artifacts/dis-\ntortions in the flags, whereas ModelScope and Di-\nrecT2V are unable to smoothly generate the gradual\ntransition of flag fluttering in the wind. However,\nas shown in the middle of Figure 5, the video gen-\nerated by GPT4Motion can show the continuous\nchange of wrinkles and ripples on the flag under\nthe effect of gravity and wind.\nWater Flows into a White Mug on a Table, Top-\nDown View.\nAs shown in Figure 8 (2nd row),\nMethod\nMotion\u2191\nCLIP\u2191\nFlickering\u2191\nGPT4Motion\n0.993 \u00b1 0.003\n0.260 \u00b1 0.022\n0.990 \u00b1 0.006\nAnimateDiff\n0.991 \u00b1 0.002\n0.257 \u00b1 0.020\n0.988 \u00b1 0.002\nModelScope\n0.937 \u00b1 0.051\n0.252 \u00b1 0.036\n0.924 \u00b1 0.059\nText2Video-Zero\n0.946 \u00b1 0.015\n0.252 \u00b1 0.024\n0.928 \u00b1 0.009\nDirecT2V\n0.879 \u00b1 0.067\n0.253 \u00b1 0.033\n0.870 \u00b1 0.071\nTable 1: Quantitative comparison across various meth-\nods. The best performances are denoted in bold.\nall the baselines\u2019 results fail to align with the user\nprompt. While the videos from AnimateDiff and\nModelScope reflect changes in the water flow, they\ncannot capture the physical effects of water pouring\ninto a mug. The videos generated by Text2Video-\nZero and DirecT2V, on the other hand, show a\nconstantly jittering mug. In comparison, as shown\nin Figure 7 (left), GPT4Motion generates the video\nthat accurately depicts the surge of water as it col-\nlides with the mug, offering a more realistic effect.\nQuantitative Evaluation and User Study.\nWe\nselect three metrics for quantitative comparisons:\nMotion Smoothness (Huang et al., 2023b), which\nrepresents the fluidity of video motion and reflects\nthe physical accuracy to some extent; CLIP scores\n(Liu et al., 2023), indicative of the alignment be-\ntween the prompt and the video; and Temporal\nFlickering (Huang et al., 2023b), which illustrates\nthe flickering level of the generated videos.Please\nrefer to the Appendix Section B for details on each\nmetric. The results, as shown in Table 1, demon-\nstrate that our GPT4Motion, leveraging GPT-4 for\nunderstanding and invoking Blender to simulate\nphysical scenes, outperforms the other four meth-\nods on all the metrics. While videos generated\nby GPT4Motion still exhibit some flickering, they\nshow a significant improvement in flickering level\ncompared to the other four models. However, these\nmetrics might not encompass the entire scope of\nvideo generation quality, leading us to undertake\na user study for a more comprehensive evaluation.\nWe also conduct a user study with 30 participants,\nwhere we show videos generated by different meth-\nods under the same prompt and ask the participants\nto vote for the best video based on three evaluation\ncriteria: physical accuracy, text-video alignment,\nand the least amount of video flickering. Remark-\nably, our GPT4Motion\u2019s results obtain 100% of the\nparticipants\u2019 votes.\n4.4\nAblation Study\nWe perform an ablation study to evaluate the impor-\ntance of control conditions, cross-frame attention,\nand \u03b1 values in Eq. 2, analyzing the effect of each\n7\nw/o edge\nw/o depth\nFFA\ni\ni + 1\ni + 2\ni + 3\nFigure 9: Ablation experiments on various control con-\nditions and cross-frame attention. Four consecutive\nframes are shown.\ndesign separately. Experiments are conducted with\nthe user prompt \u201cA white flag flaps in the wind\",\nand the video of the complete model is shown in\nFigure 5 (middle).\nControl Conditions.\nFigure 9 exhibits the results\nacross frames under different controlling condi-\ntions, which shows that the model without the edge\ncondition (w/o edge) fails to generate correct ob-\nject edges (see the first row). Additionally, the\nmodel without the depth condition (w/o depth) not\nonly adds extra cloth to the flag, but also mixes\nthe flag and the cloud due to the lack of depth-of-\nfield information. The result of Figure 5 (middle)\ndemonstrates that the joint use of both control con-\nditions preserves the integrity of the object edges\nand well handles the problem of mixing up the flag\nwith the sky.\nFirst-Frame Attention (FFA).\nIn this setting,\nKi,1 is replaced with K1 = W KF1, and Vi,1 is\nreplaced with V1 = W V F1 during the generation\nof the i-th frame in Eq. 3. This means that the\ni-th frame only attends to the first frame (without\npaying attention to itself). As shown in Figure 9\n(3rd row), the model FFA results in incomplete flag\ngeneration, where part of the flag merges with the\nsky and white clouds. Conversely, our cross-frame\nattention allows the i-th frame during its genera-\ntion to focus not only on the features of the first\nframe but also on its own characteristics, thereby\nmaintaining temporal consistency and ensuring the\ncompleteness of the generated object.\n\u03b1 = 0.1\n\u03b1 = 0.75\n\u03b1 = 1.0\ni\ni + 1\ni + 2\ni + 3\nFigure 10: Ablation experiments on different \u03b1 values.\nFour consecutive frames are shown.\nDifferent \u03b1 Values.\nTo explore the balance of the\nfirst frame and current frame in keeping temporal\nconsistency, we select three different \u03b1 values for\ncomparison. Figure 10 presents four generated\nconsecutive frames. It is clear that when the \u03b1\nvalue is too small, the generated results suffer from\ndistortion, while a large \u03b1 value causes flickering\n(inconsistent flag color intensity). By adjusting\nthe \u03b1 value to an appropriate level (i.e., 0.75), the\ngenerated results maintain the fidelity of the flag\nand reduce the flickering.\n5\nConclusions\nThis paper proposes GPT4Motion, a new training-\nfree framework that effectively combines the ad-\nvanced planning capability of Large Language\nModels (LLMs) with the robust simulation tool,\nBlender, for efficient text-to-video (T2V) synthe-\nsis. By generating Blender\u2019s scripts via GPT-4,\nGPT4Motion significantly simplifies the video gen-\neration process, making it more accessible and less\nreliant on extensive manual effort or a deep, spe-\ncialized technical knowledge in 3D modeling. Ex-\nperimental results on three basic physical motion\nscenarios, including rigid object drop and colli-\nsion, cloth draping and swinging, and liquid flow,\ndemonstrate GPT4Motion\u2019s impressive capability\nto efficiently generate high-quality videos with tem-\nporal coherence, surpassing previous T2V methods.\nGPT4Motion opens up new perspectives for T2V\ngeneration. Its integration of LLM-driven scripting\nand advanced Blender simulation paves a promis-\ning path for tackling more complex scenes in future\nresearch.\n8\n6\nLimitations\nAlthough GPT4Motion advances the field of T2V\nsynthesis, it has several limitations that set the di-\nrections for future research. While GPT4Motion\nsuccessfully handles basic physical motions related\nto specific object materials, we have not extended\nit to more complex motion scenarios. We hypoth-\nesize that complex motions could be decomposed\ninto a series of basic motions, requiring more re-\nfined instructions for LLMs. Another limitation is\nthat sometimes the generated videos still have flick-\nering in some frames. Despite these limitations,\nwe believe that GPT4Motion provides a promising\nway for T2V generation.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nMax Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zis-\nserman. 2021. Frozen in time: A joint video and\nimage encoder for end-to-end retrieval. In ICCV.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim\nDockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. 2023.\nAlign your latents: High-\nresolution video synthesis with latent diffusion mod-\nels. In CVPR.\nPrafulla Dhariwal and Alexander Nichol. 2021. Dif-\nfusion models beat gans on image synthesis.\nIn\nNeurIPS.\nHerbert Goldstein, Charles Poole, and John Safko. 2002.\nClassical mechanics.\nYuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang,\nYu Qiao, Dahua Lin, and Bo Dai. 2023. Animated-\niff: Animate your personalized text-to-image diffu-\nsion models without specific tuning. arXiv preprint\narXiv:2307.04725.\nJonathan Ho, William Chan, Chitwan Saharia, Jay\nWhang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J\nFleet, et al. 2022a.\nImagen video: High defini-\ntion video generation with diffusion models. arXiv\npreprint arXiv:2210.02303.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. In NeurIPS.\nJonathan Ho and Tim Salimans. 2022.\nClassifier-\nfree\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. 2022b.\nVideo diffusion models. In NeurIPS.\nSusung Hong,\nJunyoung Seo,\nSunghwan Hong,\nHeeseong Shin, and Seungryong Kim. 2023. Large\nlanguage models are frame-level directors for zero-\nshot text-to-video generation.\narXiv preprint\narXiv:2305.14330.\nHanzhuo Huang, Yufan Feng, Cheng Shi, Lan Xu,\nJingyi Yu, and Sibei Yang. 2023a. Free-bloom: Zero-\nshot text-to-video generator with llm director and\nldm animator. In NeurIPS.\nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang,\nChenyang Si, Yuming Jiang, Yuanhan Zhang, Tianx-\ning Wu, Qingyang Jin, Nattapol Chanpaisit, Yao-\nhui Wang, Xinyuan Chen, Limin Wang, Dahua Lin,\nYu Qiao, and Ziwei Liu. 2023b. VBench: Compre-\nhensive benchmark suite for video generative models.\narXiv preprint arXiv:2311.17982.\nBetker James, Goh Gabriel, Jing Li, Brooks Tim, Wang\nJianfeng, Li Linjie, Ouyang Long, and et.al. 2023.\nImproving image generation with better captions.\nLevon Khachatryan, Andranik Movsisyan, Vahram\nTadevosyan, Roberto Henschel, Zhangyang Wang,\nShant Navasardyan, and Humphrey Shi. 2023.\nText2video-zero: Text-to-image diffusion models are\nzero-shot video generators. In ICCV.\nPijush K Kundu, Ira M Cohen, and David R Dowling.\n2015. Fluid mechanics. Academic press.\nZhen Li, Zuo-Liang Zhu, Ling-Hao Han, Qibin Hou,\nChun-Le Guo, and Ming-Ming Cheng. 2023. Amt:\nAll-pairs multi-field transforms for efficient frame\ninterpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 9801\u20139810.\nLong Lian, Baifeng Shi, Adam Yala, Trevor Darrell,\nand Boyi Li. 2023. Llm-grounded video diffusion\nmodels. arXiv preprint arXiv:2309.17444.\nHan Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal.\n2023.\nVideodirectorgpt: Consistent multi-scene\nvideo generation via llm-guided planning.\narXiv\npreprint arXiv:2309.15091.\nYaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang,\nYong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng,\nRaymond Chan, and Ying Shan. 2023. Evalcrafter:\nBenchmarking and evaluating large video generation\nmodels.\nOpenAI. 2023.\nGpt-4 technical report.\narXiv\n2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\n9\nDustin Podell, Zion English, Kyle Lacey, Andreas\nBlattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. 2023. Sdxl: Improving latent\ndiffusion models for high-resolution image synthesis.\narXiv preprint arXiv:2307.01952.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International Con-\nference on Machine Learning, pages 8748\u20138763.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen. 2022.\nHierarchical text-\nconditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In CVPR.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al. 2022. Photorealistic\ntext-to-image diffusion models with deep language\nunderstanding. In NeurIPS.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin,\nJie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, et al. 2023. Make-a-video:\nText-to-video generation without text-video data. In\nICLR.\nJiaming Song, Chenlin Meng, and Stefano Ermon.\n2021a.\nDenoising diffusion implicit models.\nIn\nICLR.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma,\nAbhishek Kumar, Stefano Ermon, and Ben Poole.\n2021b. Score-based generative modeling through\nstochastic differential equations. ICLR.\nChunyi Sun, Junlin Han, Weijian Deng, Xinlong Wang,\nZishan Qin, and Stephen Gould. 2023. 3d-gpt: Proce-\ndural 3d modeling with large language models. arXiv\npreprint arXiv:2310.12945.\nHui Tang and Kui Jia. 2023. A new benchmark: On the\nutility of synthetic data with blender for bare super-\nvised learning and downstream domain adaptation.\nIn CVPR.\nJiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya\nZhang, Xiang Wang, and Shiwei Zhang. 2023. Mod-\nelscope text-to-video technical report. arXiv preprint\narXiv:2308.06571.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2021.\nFinetuned language\nmodels are zero-shot learners. In ICLR.\nLvmin Zhang and Maneesh Agrawala. 2023. Adding\nconditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543.\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. 2022. Magicvideo: Effi-\ncient video generation with latent diffusion models.\narXiv preprint arXiv:2211.11018.\n10\nGPT4Motion\nAnimateDiff\nModelScope\nText2Video-Zero\nDirecT2V\nFigure 11: Comparison of the video results generated by different text-to-video models under different physical\nconditions. Best viewed with Acrobat Reader for animation.\nA\nMore Comparison with Baselines\nIn the main paper, we have compared GPT4Motion\nwith four baselines (AnimateDiff (Guo et al., 2023),\nModelScope (Wang et al., 2023), Text2Video-Zero\n(Khachatryan et al., 2023), and DirecT2V (Hong\net al., 2023)) on three scenarios (rigid object drop\nand collision, cloth draping and swinging, and liq-\nuid flow). Here, we further conduct an experiment\non dynamic effects of a T-shirt being blown by the\nwind under three wind strengths. The results are\nshown in Figure 11, where the seed is randomly\nchosen and fixed in all the generations. We can\nsee that these baselines all fail to generate videos\nthat match the user prompts and are unable to con-\ntrol the intensity of physical phenomena solely\nbased on the linguistic descriptions. In contrast,\nour GPT4Motion not only precisely designs the pa-\nrameters of Blender encapsulated functions (such\nas wind strength) through GPT-4, but also leverages\nBlender\u2019s physics engine to simulate the complex\nflapping and twisting dynamics of the T-shirt in the\nwind.\nB\nQuantitative Evaluation Metrics\nHere, we introduce the metrics employed in the\nmain paper:\n1. Motion Smoothness (Huang et al., 2023b).\nThis metric evaluates the smoothness of mo-\ntion in generated videos, ensuring it conforms\nto the physical laws of the real world. The\nevaluation utilizes motion priors from the\nvideo frame interpolation model (Li et al.,\n2023) to assess the smoothness of generated\nmotions.\n2. Temporal Flickering (Huang et al., 2023b).\nThis metric identifies imperfections in tempo-\nral consistency within generated videos, es-\npecially in local and high-frequency details.\nThe method involves analyzing static frames\nand computing the mean absolute difference\nacross frames. These values are then sub-\n11\ntracted from 255 and normalized between 0\nand 1.\n3. CLIP-Score (Liu et al., 2023). This metric is\ndesigned to assess Text-Video Consistency. It\nleverages a pretrained ViT-B/32 CLIP model\n(Radford et al., 2021) as a feature extractor to\ncalculate the cosine similarity between each\nvideo frame and the associated text.\nC\nBlender Settings\nWe use Blender to generate two sequences of scene\ndepth maps and edge maps. The edge maps are\nobtained by Blender\u2019s built-in Freestyle8 feature,\nwhich is an advanced rendering technique for non-\nphotorealistic line drawings from 3D scenes. It not\nonly allows for various line styles, such as hand-\ndrawn or sketch, but also serves as an independent\nrendering pass without losing edge information of\nthe scene and additional post-processing. More-\nover, we normalize the scene\u2019s Z-channel and ren-\nder it as an independent depth map. Since the ren-\ndering of edge maps and depth maps is unrelated\nto ray tracing, we choose Blender\u2019s \u201cWorkbench\"\nrendering mode. This mode avoids the substantial\ncomputational resources required for ray tracing,\nsignificantly enhancing rendering efficiency. In our\nexperiments, completing the entire rendering pro-\ncess using Blender scripts takes only 2 to 3 minutes,\nwith most of the time spent on calculating physical\neffects.\nD\nGPT-4\u2019s Generation of Encapsulated\nPython Functions for Blender\nBlender incorporates an independent Python li-\nbrary, enabling the transformation of various man-\nual processes within the software to automate tasks\nthrough its Python API. Leveraging GPT-4\u2019s ca-\npabilities of grasping Blender\u2019s Python API, we\ncan rapidly generate encapsulated Python functions\nfor Blender. This involves inputting specific func-\ntional goals and the associated Blender operation\nsteps into GPT-4. Consequently, GPT-4 can au-\ntonomously create Python code that corresponds\nto these steps, effectively encapsulating them into\nPython functions. One example is given below.\nUser:\nYou\nare\nan\nexpert\nin\nBlender\n8https://docs.blender.org/manual/en/latest/\nrender/freestyle/introduction.html\nand\nPython.\nPlease\ngenerate\nencapsulated\nBlender\nPython\nfunctions\nby\nfollowing\nour\nstep-by-step\ninstructions,\nwriting\nthe\nBlender\nPython\nAPI corresponding to each step,\ndesigning each function\u2019s name,\nand explaining its functionality\nand the meaning of each parameter\nin the docstring.\nGPT-4:\nSure,\nI\u2019d\nbe\nhappy\nto\nhelp\nwith that.\nPlease go ahead and\nprovide me with the step-by-step\ninstructions\nfor\nthe\nBlender\nPython functions you need, and I\nwill write the corresponding code\nfor you.\nUser:\nPlease build a function to remove\nall objects, cameras and lights\nfrom the initial scene.\nGPT-4:\nCertainly!\nTo create a Blender\nPython function that removes all\nobjects, cameras, and lights from\nthe initial scene, you can define\na function like this:\n1\n# bpy is the Python module\nused for scripting within\nBlender\n2\nimport bpy\n3\n4\ndef clear_scene ():\n5\n\"\"\"\n6\nClears all objects from\nthe current Blender\nscene.\n7\n8\nThis function selects all\nobjects in the scene\nand deletes them.\n9\nIt is useful when\nstarting a new scene\nsetup or resetting\nthe scene to a blank\nstate.\n10\n11\nNo inputs or outputs.\n12\n13\nTypically called at the\nbeginning of a script\nwhen starting a new\nscene setup.\n14\n\"\"\"\n15\nbpy.ops.object.select_all\n(action='SELECT ')\n12\n16\nbpy.ops.object.delete ()\nYou\ncan\ncall\nthis\nfunction\nwhenever you need to clear the\nscene of all types of objects.\nJust\nmake\nsure\nto\nbe\nin\nthe\ncorrect context where the scene\nis accessible.\nUser:\nPlease\nnow\ncreate\ntwo\nphysics\nfunctions,\none\nthat\nadds\ncollision\nproperties\nto\nthe\ninputted\nobject,\nand\nanother\nthat adds rigid body properties\nto the object.\nThen, construct\na\nfunction\nto\ncreate\na\nfloor\nthat scales it up 50 times in\nall\ndirections,\nwith\nphysics\nset to collision and rigid body\n(passive, with elasticity set to\n1).\nGPT-4:\nTo accomplish this task,\nwe\u2019ll\ncreate\nthree\nfunctions\nin\nBlender\u2019s Python API:\n1. Add\nCollision\nProperties:\nThis\nfunction\nwill\nadd\ncollision\nproperties\nto\na\nspecified object.\n2. Add\nRigid\nBody\nProperties:\nThis\nfunction\nwill\nadd\nrigid body properties to a\nspecified object.\n3. Create a Floor with Physics:\nThis function will create a\nfloor,\nscale\nit,\nand\nset\nits\nphysics\nproperties\nto\ncollision and rigid body.\n1\nimport bpy\n2\n3\ndef add_collision(obj):\n4\n\"\"\"\n5\nAdds a collision modifier\nto a Blender object\nif it does not\nalready have one.\n6\n7\nParameters:\n8\n- obj (Blender Object):\nThe object to which\nthe collision\nmodifier will be\nadded.\n9\n10\nReturns:\n11\n- None , but the object\nwill have a collision\nmodifier added to it\nif it wasn't present\nbefore.\n12\n13\nThis function is used in\nphysics simulations\nwhere it's necessary\nfor objects to\ninteract with each\nother ,\n14\nsuch as rigid body or\nsoft body simulations\n. The collision\nmodifier makes the\nobject a collider in\nthe physics\n15\nsimulation , allowing\nother objects to\nbounce off or slide\nalong its surface.\n16\n\"\"\"\n17\nif \"Collision\" not in obj\n.modifiers:\n18\nbpy.ops.object.\nmodifier_add(type\n='COLLISION ')\n19\n20\ndef add_rigid_body(obj , mass\n=1, elasticity =0.5,\nrigid_body_type='ACTIVE ')\n:\n21\n\"\"\"\n22\nAdds a rigid body physics\ncharacteristic to a\nBlender object.\n23\n24\nArgs:\n25\n- obj (Blender Object):\nThe object to which\nthe rigid body\nphysics will be\napplied.\n26\n- mass (float): The mass\nof the object ,\ndefault is 1.\n27\n- elasticity (float): The\nrestitution (\nbounciness) of the\nobject , default is\n0.5.\n28\n- rigid_body_type (str):\nThe type of rigid\nbody , can be 'ACTIVE '\nor 'PASSIVE ',\ndefault is 'ACTIVE '.\n29\n30\nReturns:\n31\n- None , but the object is\nnow a rigid body\nwith the specified\nphysics properties.\n32\n33\nThis function is crucial\nfor physics\n13\nsimulations , defining\nhow the object\nbehaves under\nphysical forces.\n34\n\"\"\"\n35\nbpy.ops.object.select_all\n(action='DESELECT ')\n36\nobj.select_set(True)\n37\nbpy.context.view_layer.\nobjects.active = obj\n38\nbpy.ops.rigidbody.\nobject_add ()\n39\nobj.rigid_body.type =\nrigid_body_type\n40\nobj.rigid_body.mass =\nmass\n41\nobj.rigid_body.\nrestitution =\nelasticity\n42\nobj.rigid_body.\ncollision_shape = '\nMESH'\n43\n44\ndef create_floor(elasticity\n=1):\n45\n\"\"\"\n46\nCreates a floor plane in\nBlender , scales it,\nand sets it up with\ncollision and rigid\nbody physics.\n47\nThe created floor is\nscaled to be large\nenough to act as a\nground plane for most\nscenes.\n48\n49\nParameters:\n50\n- elasticity (float): The\nrestitution or '\nbounciness ' of the\nfloor. A value of 1\nmeans perfectly\nelastic , while 0\nmeans no elasticity.\nDefault is 1.\n51\n\"\"\"\n52\nbpy.ops.mesh.\nprimitive_plane_add(\nsize=1,\nenter_editmode=False ,\nalign='WORLD ',\nlocation =(0, 0, 0))\n53\nfloor = bpy.context.\nactive_object\n54\nfloor.scale = (50, 50,\n50)\n55\nfloor.name = 'GROUND '\n56\nadd_collision(floor)\n57\nadd_rigid_body(floor ,\nrigid_body_type='\nPASSIVE ', elasticity=\nelasticity)\nThese\nfunctions\ntogether\nallow\nyou to dynamically add physics\nproperties to objects and create\na large, physical floor in your\nBlender scene.\nRemember to run\nthese\nfunctions\nin\na\nBlender\nPython\nscripting\ncontext\nwhere\nthe bpy module is accessible.\nAfter generating the function, we enhance its\nusability for GPT-4 by incorporating instructions\n(such as the strength interval of the wind) in the\nfunction\u2019s docstring. This ensures that GPT-4\u2019s\nusage aligns more closely with our expectations.\nE\nCombination of Blender and GPT-4\u2019s\nPhysics Knowledge\nWe discover that GPT4Motion can not only use\nGPT-4\u2019s semantic understanding and coding ca-\npabilities to generate Blender Python scripts, but\nit can also leverage GPT-4\u2019s physical knowledge\nto control object motion.\nFor example, when\nprompted with \u201ca basketball thrown towards the\ncamera.\", we find that GPT-4 can calculate the time\nit takes for the basketball to fall, based on the height\ndifference between the initial position of the bas-\nketball and the camera. This calculation helps to\ndetermine the initial velocity required for the bas-\nketball to fly to the camera. The speed calculation\ncode generated by GPT-4 is as follows:\n1\n# Setup the basketball\n2\nbasketball = setup_object(\n3\nos.path.join(ASSETS_PATH , \"\nbasketball.obj\"),\n4\n\"Basketball\",\n5\nsize =0.24 ,\n# Standard basketball\ndiameter in meters\n6\nmass =0.625 ,\n# Standard\nbasketball mass in kilograms\n7\nposition =(0, 0, 4)\n# Initial\nposition for the basketball\n8\n)\n9\n10\n# Calculate the initial velocity for\nthe basketball to hit the camera\n11\ng = 9.81\n# Acceleration due to\ngravity in m/s^2\n12\ncamera_height = 1.8521\n13\nheight_difference = 4 - camera_height\n14\ntime_to_fall = math.sqrt(2 *\nheight_difference / g)\n15\ninitial_horizontal_velocity = 13.665\n/ time_to_fall\n# Distance to\ncamera along Y-axis\n16\n17\n# Set the initial velocity and\nrotation for the basketball\n18\nadd_initial_velocity_for_rigid_body(\n19\nbasketball ,\n20\ninitial_velocity =(0,\ninitial_horizontal_velocity ,\n0),\n21\ninitial_rotation =(0, 0, 0)\n# No\ninitial rotation\n22\n)\n14\nIn the above script, GPT-4 first calculates the\nheight difference between the initial position of\nthe basketball and the camera, which determines\nthe time required for the basketball to fall to the\nsame height as the camera. Subsequently, GPT-\n4 calculates the distance between the basketball\nand the camera along the Y-axis to determine the\nrequired initial velocity of the basketball. This\nprocess effectively integrates basic principles of\nphysics, such as the equations of motion, to solve a\npractical problem in a simulated environment like\nBlender.\n15\n"
  }
]