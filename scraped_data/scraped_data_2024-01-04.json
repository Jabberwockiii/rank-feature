[
  {
    "title": "aMUSEd: An Open MUSE Reproduction",
    "link": "https://arxiv.org/pdf/2401.01808.pdf",
    "upvote": "26",
    "text": "Technical Report\nAMUSED: AN OPEN MUSE REPRODUCTION\nSuraj Patil1, William Berman1, Robin Rombach2, Patrick von Platen1\n1Hugging Face, 2Stability AI\n{suraj, patrick}@huggingface.co\nWLBberman@gmail.com\nrobin@stability.ai\nABSTRACT\nWe present aMUSEd, an open-source, lightweight masked image model (MIM)\nfor text-to-image generation based on MUSE (Chang et al. (2023)). With 10%\nof MUSE\u2019s parameters, aMUSEd is focused on fast image generation. We be-\nlieve MIM is underexplored compared to latent diffusion (Rombach et al. (2022)),\nthe prevailing approach for text-to-image generation. Compared to latent diffu-\nsion, MIM requires fewer inference steps (Chang et al. (2023)) and is more inter-\npretable. Additionally, MIM can be fine-tuned to learn additional styles with only\na single image (Sohn et al. (2023)). We hope to encourage further exploration of\nMIM by demonstrating its effectiveness on large-scale text-to-image generation\nand releasing reproducible training code. We also release checkpoints for two\nmodels which directly produce images at 256x256 and 512x512 resolutions.\n1\nINTRODUCTION\nIn recent years, diffusion based text-to-image generative models have achieved unprecedented qual-\nity (Rombach et al. (2022); Podell et al. (2023); DeepFloyd (2023); Saharia et al. (2022); Betker\net al. (2023); Ramesh et al. (2022; 2021); Peebles & Xie (2023)). Improvements can be mainly\nattributed to large open-source pre-training datasets (Schuhmann et al. (2022a)), pre-trained text\nencoders (Radford et al. (2021); Raffel et al. (2023)), latent image encoding methods (Kingma &\nWelling (2019), Esser et al. (2021)) and improved sampling algorithms (Song et al. (2020); Zhang\n& Chen (2023); Lu et al. (2022a;b); Dockhorn et al. (2022); Song et al. (2021); Karras et al. (2022)).\nMIM has proven to be a promising alternative to diffusion models for image generation (Chang\net al. (2023; 2022)). MIM\u2019s repeated parallel prediction of all tokens is particularly efficient for\nhigh-resolution data like images. While diffusion models usually require 20 or more sampling steps\nduring inference, MIM allows for image generation in as few as 10 steps.\nMIM brings the modeling approach closer to the well-researched field of language modeling (LM).\nConsequently, MIM can directly benefit from findings of the LM research community, including\nquantization schemes (Dettmers et al. (2022; 2023)), token sampling methods (Fan et al. (2018),\nHoltzman et al. (2020)), and token-based uncertainty estimation Guo et al. (2017).\nAs MIM\u2019s default prediction objective mirrors in-painting, MIM demonstrates impressive zero-shot\nin-painting performance, whereas diffusion models generally require additional fine-tuning (Run-\nwayML (2022)). Moreover, recent style-transfer (Sohn et al. (2023)) research has shown effective\nsingle image style transfer for MIM, but diffusion models have not exhibited the same success.\nDespite MIM\u2019s numerous benefits over diffusion-based image generation methods, its adoption has\nbeen limited. Proposed architectures require significant computational resources, e.g. MUSE uses a\n4.6b parameter text-encoder, a 3b parameter base transformer, and a 1b parameter super-resolution\ntransformer. Additionally, previous models have not released training code and modeling weights.\nWe believe an open-source, lightweight model will support the community to further develop MIM.\nIn this work, we introduce aMUSEd, an efficient, open-source 800M million parameter model1\nbased on MUSE. aMUSEd utilizes a CLIP-L/14 text encoder (Radford et al. (2021)), SDXL-style\n1Including all parameters from the U-ViT, CLIP-L/14 text encoder, and VQ-GAN.\n1\narXiv:2401.01808v1  [cs.CV]  3 Jan 2024\nTechnical Report\nmicro-conditioning (Podell et al. (2023)), and a U-ViT backbone (Hoogeboom et al. (2023)). The\nU-ViT backbone eliminates the need for a super-resolution model, allowing us to successfully train\na single-stage 512x512 resolution model. The design is focused on reduced complexity and re-\nduced computational requirements to facilitate broader use and experimentation within the scientific\ncommunity.\nWe demonstrate many advantages such as 4bit and 8bit quantization, zero-shot in-painting, and\nsingle image style transfer with styledrop (Sohn et al. (2023)). We release all relevant model weights\nand source code.\n2\nRELATED WORK\n2.1\nTOKEN-BASED IMAGE GENERATION\nEsser et al. (2021) demonstrated the effectiveness of VQ-GAN generated image token embeddings\nfor auto-regressive transformer based image modeling. With large-scale text-to-image datasets,\nauto-regressive image generation can yield state-of-the-art results in image quality (Yu et al. (2022;\n2023)). Additionally, auto-regressive token prediction allows framing image and text-generation as\nthe same task, opening an exciting research direction for grounded multimodal generative models\n(Huang et al. (2023); Aghajanyan et al. (2022)). While effective, auto-regressive image generation\nis computationally expensive. Generating a single image can require hundreds to thousands of token\npredictions.\nAs images are not inherently sequential, Chang et al. (2022) proposed MIM. MIM predicts all\nmasked image tokens in parallel for a fixed number of inference steps. On each step, a predeter-\nmined percentage of the most confident predictions are fixed, and all other tokens are re-masked.\nMIM\u2019s training objective mirrors BERT\u2019s training objective (Devlin et al. (2018)). However, MIM\nuses a varied masking ratio to support iterative sampling starting from only masked tokens.\nConsequently, MUSE successfully applied MIM to large-scale text-to-image generation (Chang\net al. (2023)). MUSE uses a VQ-GAN (Esser et al. (2021)) with a fine-tuned decoder, a 3 billion pa-\nrameter transformer, and a 1 billion parameter super-resolution transformer. Additionally, MUSE is\nconditioned on text embeddings from the pre-trained T5-XXL text encoder (Raffel et al. (2023)). To\nimprove image quality when predicting 512x512 resolution images, MUSE uses a super-resolution\nmodel conditioned on predicted tokens from a 256x256 resolution model. As MIM\u2019s default pre-\ndiction objective mirrors in-painting, MUSE demonstrates impressive zero-shot in-painting results.\nIn contrast, diffusion models generally require additional fine-tuning for in-painting (RunwayML\n(2022)).\nMIM has not been adopted by the research community to the same degree as diffusion models.\nWe believe this is mainly due to a lack of lightweight, open-sourced models, e.g. MUSE is closed\nsource and has a 4.5 billion parameter text encoder, a 3 billion parameter base model, and a 1 billion\nparameter super-resolution model.\n2.2\nFEW-STEP DIFFUSION MODELS\nDiffusion models are currently the prevailing modeling approach for text-to-image generation. Dif-\nfusion models are trained to remove noise from a target image at incrementally decreasing levels\nof noise. Models are frequently trained on 1000 noise levels (Rombach et al. (2022); Podell et al.\n(2023); Saharia et al. (2022); Chen et al. (2023)), but noise levels can be skipped or approximated\nwithout suffering a significant loss in image quality (Song et al. (2021); Karras et al. (2022); Song\net al. (2020); Zhang & Chen (2023); Lu et al. (2022a;b); Dockhorn et al. (2022)). As of writing\nthis report, effective denoising strategies (Lu et al. (2022b); Zhao et al. (2023); Zheng et al. (2023))\nrequire as few as 20 steps to generate images with little to indistinguishable quality degradation\ncompared to denoising at each trained noise level.\n20 sampling steps is still prohibitively expensive for real-time image generation. Diffusion models\ncan be further distilled to sample in as few as 1 to 4 sampling steps. Salimans & Ho (2022) shows\nhow a pre-trained diffusion model can be distilled to sample in half the number of sampling steps.\nThis distillation can be repeated multiple times to produce a model that requires as few as 2 to\n2\nTechnical Report\n4 sampling steps. Additionally, framing the denoising process as a deterministic ODE integration,\nconsistency models can learn to directly predict the same fully denoised image from any intermediate\nnoisy image on the ODE trajectory (Song et al. (2021)). Luo et al. (2023a) and Luo et al. (2023b)\nwere the first to successfully apply consistency distillation to large-scale text-to-image datasets,\ngenerating high-quality images in as few as 4 inference steps. Sauer et al. (2023) demonstrated that\nan adversarial loss objective and a score distillation sampling (Poole et al. (2022)) objective can be\ncombined to distill few step sampling.\nDistilled diffusion models are faster than the current fastest MIM models. However, distilled diffu-\nsion models require a powerful teacher model. A teacher model requires additional training com-\nplexity, additional training memory, and limits the image quality of the distilled model. MIM\u2019s\ntraining objective does not require a teacher model or approximate inference algorithm and is fun-\ndamentally designed to require fewer sampling steps.\n2.3\nINTERPRETABILITY OF TEXT-TO-IMAGE MODELS\nAuto-regressive image modeling and MIM output explicit token probabilities, which naturally mea-\nsure prediction confidence (Guo et al. (2017)). Token probability-based language models have been\nused to research model interpretability (Jiang et al. (2021)). We do not extensively explore the in-\nterpretability of token prediction-based image models, but we believe this is an interesting future\nresearch direction.\n3\nMETHOD\nVQ-GAN\nWe trained a 146M parameter VQ-GAN (Esser et al. (2021)) with no self-attention\nlayers, a vocab size of 8192, and a latent dimension of 64. Our VQ-GAN downsamples resolutions\nby 16x, e.g. a 256x256 (512x512) resolution image is reduced to 16x16 (32x32) latent codes. We\ntrained our VQ-GAN for 2.5M steps.\nText Conditioning\nDue to our focus on inference speed, we decided to condition our model on\ntext embeddings from a smaller CLIP model (Radford et al. (2021)) instead of T5-XXL (Raffel\net al. (2023)). We experimented with both the original CLIP-l/14 (Radford et al. (2021)) and the\nequivalently sized CLIP model released with DataComp (Gadre et al. (2023)). Even with the re-\nported improvements in Gadre et al. (2023), we found that the original CLIP-l/14 resulted in qual-\nitatively better images. The penultimate text encoder hidden states are injected via the standard\ncross-attention mechanism. Additionally, the final pooled text encoder hidden states are injected via\nadaptive normalization layers (Perez et al. (2017)).\nU-ViT\nFor the base model, we used a variant of the U-ViT (Hoogeboom et al. (2023)), a trans-\nformer (Vaswani et al. (2023)) inspired scalable U-Net (Ronneberger et al. (2015)). Hoogeboom\net al. (2023) finds that U-Nets can be effectively scaled by increasing the number of low-resolution\nblocks as the increased parameters are more than compensated for by the small feature maps. Addi-\ntionally, Hoogeboom et al. (2023) turns the lowest resolution blocks into a transformer by replacing\nconvolution blocks with MLPs. For our 256x256 resolution model, we used no downsampling or\nupsampling in the convolutional residual blocks. For our 512x512 resolution model, we used a sin-\ngle 2x downsampling and corresponding 2x upsampling in the convolutional residual blocks. As a\nresult, the lower resolution U-ViT of the 256x256 and 512x512 models receive an input vector se-\nquence of 256 (16x16) with a feature dimension of 1024. The 256x256 resolution model has 603M\nparameters, and the 512x512 resolution model has 608M parameters. The 5M additional parameters\nin the 512x512 resolution model are due to the additional down and upsampling layers.\nMasking Schedule\nFollowing MUSE (Chang et al. (2023)) and MaskGIT (Chang et al. (2022)),\nwe use a cosine based masking schedule. After each step t, of predicted tokens, those with the\nmost confident predictions are permanently unmasked such that the proportion of tokens masked is\ncos( t\nT \u00b7 \u03c0\n2 ), with T being the total number of sampling steps. We use T = 12 sampling steps in all\nof our evaluation experiments. Through ablations, Chang et al. (2022) shows that concave masking\nschedules like cosine outperform convex masking schedules. Chang et al. (2022) hypothesizes that\n3\nTechnical Report\nFigure 1: The diagram shows the training and inference pipelines for aMUSEd. aMUSEd consists\nof three separately trained components: a pre-trained CLIP-L/14 text encoder, a VQ-GAN, and a\nU-ViT. During training, the VQ-GAN encoder maps images to a 16x smaller latent resolution. The\nproportion of masked latent tokens is sampled from a cosine masking schedule, e.g. cos(r \u00b7 \u03c0\n2 )\nwith r \u223c Uniform(0, 1). The model is trained via cross-entropy loss to predict the masked tokens.\nAfter the model is trained on 256x256 images, downsampling and upsampling layers are added, and\ntraining is continued on 512x512 images. During inference, the U-ViT is conditioned on the text\nencoder\u2019s hidden states and iteratively predicts values for all masked tokens. The cosine masking\nschedule determines a percentage of the most confident token predictions to be fixed after every\niteration. After 12 iterations, all tokens have been predicted and are decoded by the VQ-GAN into\nimage pixels.\nconcave masking schedules benefit from fewer fixed predictions earlier in the denoising process and\nmore fixed predictions later in the denoising process.\nMicro-conditioning\nJust as Podell et al. (2023), we micro-condition on the original image resolu-\ntion, crop coordinates, and LAION aesthetic score (Schuhmann (2022)). The micro-conditioning\nvalues are projected to sinusoidal embeddings and appended as additional channels to the final\npooled text encoder hidden states.\n4\nEXPERIMENTAL SETUP\n4.1\nPRE-TRAINING\nData Preparation\nWe pre-trained on deduplicated LAION-2B (Schuhmann et al. (2022a)) with\nimages above a 4.5 aesthetic score (Schuhmann (2022)). We filtered out images above a 50% water-\nmark probability or above a 45% NSFW probability. The deduplicated LAION dataset was provided\nby Laurenc\u00b8on et al. (2023) using the strategy presented in Webster et al. (2023).\nTraining Details\nFor pre-training, the VQ-GAN and text encoder weights were frozen, and only\nthe U-ViTs of the respective models were trained. The 256x256 resolution model2 was trained on 2\n2https://huggingface.co/amused/amused-256\n4\nTechnical Report\n8xA100 servers for 1,000,000 steps and used a per GPU batch size of 128 for a total batch size of\n2,048. The 512x512 resolution model3 was initialized from step 84,000 of the 256x256 resolution\nmodel and continued to train for 554,000 steps on 2 8xA100 servers. The 512x512 resolution model\nused a per GPU batch size 64 for a total batch size of 1024.\nMasking Rate Sampling\nFollowing Chang et al. (2022) and Chang et al. (2023), the percent-\nage of masked latent tokens was sampled from a cosine masking schedule, e.g. cos(r \u00b7 \u03c0\n2 ) with\nr \u223c Uniform(0, 1). Chang et al. (2022) ablates different choices of masking schedules, finding that\nconcave functions outperform convex functions. They hypothesize that this is due to more challeng-\ning masking ratios during training.\n4.2\nFINE-TUNING\nWe further fine-tuned the 256x256 resolution model for 80,000 steps on journeydb (Sun et al.\n(2023)). We also further fine-tuned the 512x512 model for 2,000 steps on journeydb, synthetic\nimages generated by SDXL (Podell et al. (2023)) from LAION-COCO captions (Schuhmann et al.\n(2022b)), unsplash lite, and LAION-2B above a 6 aesthetic score (Schuhmann et al. (2022a); Schuh-\nmann (2022)). We found that the synthetic image generated by SDXL (Podell et al. (2023)) from\nLAION-COCO captions (Schuhmann et al. (2022b)) qualitatively improved text-image alignment.\nThe 512x512 resolution model was fine-tuned for much fewer steps than the 256x256 model because\nit began to overfit on the fine-tuning data.\nTo improve the reconstruction of high-resolution images, we further fine-tuned the VQ-GAN de-\ncoder on a dataset of images greater than 1024x1024 resolution. The VQ-GAN decoder was fine-\ntuned on 2 8xA100 servers for 200,000 steps and used a per GPU batch size of 16 for a total batch\nsize of 256.\n5\nRESULTS\nFigure 2: A100 40GB end to end image generation time. Full A100 and 4090 benchmarks can be\nfound in appendix A.\n5.1\nINFERENCE SPEED\naMUSEd\u2019s inference speed is superior to non-distilled diffusion models and competitive with few-\nstep distilled diffusion models. Compared to many popular diffusion models, aMUSEd scales par-\n3https://huggingface.co/amused/amused-512\n5\nTechnical Report\nticularly well with batch size, making it a good choice for text-to-image applications that require\nhigh throughput4.\nFor batch size 1, single-step distilled diffusion models such as sd-turbo and sdxl-turbo (Sauer et al.\n(2023)) outperform both of our 256x256 and 512x512 resolution models. Notably, sd-turbo gener-\nates higher resolution images than our 256x256 resolution model while being 3.5x faster.\nCompared to batch size 1, the end to end generation time for batch size 8 of sd-turbo (sdxl-turbo)\nis reduced by 3.6x (3.38x). However, aMUSEd\u2019s 256x256 (512x12) resolution model\u2019s inference\ntime only decreases by 1.28x (1.8x). At batch size 8, sd-turbo is still the fastest image generation\nmodel, but it is only 1.3x faster than our 256x256 resolution model. At batch size 8, aMUSEd\u2019s\n256x256 (512x512) resolution model outperforms the 4-step latent consistency model by a factor of\n3x (1.8x).\nBoth aMUSEd models are significantly faster than non-distilled diffusion models. Compared to\nstable diffusion 1.55 (Rombach et al. (2022)), the 512x512 resolution aMUSEd model is 1.6x (3x)\nfaster at batch size 1 (batch size 8). At batch size 8, the state of the art SDXL (Podell et al. (2023))\nis orders of magnitude slower than both aMUSEd models.\n5.2\nMODEL QUALITY\nWe benchmarked both aMUSEd models on zero-shot FID (Heusel et al. (2017)), CLIP (Radford\net al. (2021)), and inception score (Salimans et al. (2016)) on the MSCOCO (Lin et al. (2015)) 2017\nvalidation set with 2 samples per caption for a total of 10k samples. Due to either a lack of re-\nported metrics or ambiguities in measurement methodologies, we manually ran quality benchmarks\nfor all models we compared against. Our 512x512 resolution model has competitive CLIP scores.\nHowever, both our 256x256 and 512x512 resolution models lag behind in FID and Inception scores.\nSubjectively, both models perform well at low detail images with few subjects, such as landscapes.\nBoth models may perform well for highly detailed images such as faces or those with many subjects\nbut require prompting and cherry-picking. See 3.\n(a) CLIP vs. FID tradeoff curve\n(b) CLIP score vs. classifier free guidance (cfg) scale\nFigure 4: See appendix B for additional FID, CLIP, and inception score measurements.\n4batch size \u00d7 latency\n5Stable diffusion 1.5 outputs images at the same 512x512 resolution as the 512x512 resolution aMUSEd\nmodel.\n6\nTechnical Report\nFigure 3: Cherry-picked images from 512x512 and 256x256 resolution models. Images are slightly\ndegraded for file size considerations\n5.3\nSTYLEDROP\nStyledrop (Sohn et al. (2023)) is an efficient fine-tuning method for learning a new style from a\nsmall number of images. It has an optional first stage to generate additional training samples, which\ncan be used to augment the training dataset. Styledrop demonstrates effective single example image\nstyle adoption on MUSE and aMUSEd. Sohn et al. (2023) shows that similar fine-tuning proce-\ndures such as LoRa Dreambooth (Ruiz et al. (2023)) on Stable Diffusion (Rombach et al. (2022))\nand Dreambooth on Imagen (Saharia et al. (2022)) do not show the same degree of style adherence.\nFigure 5 compares a LoRa Dreambooth Stable Diffusion training run6 with a styledrop training run\non aMUSEd. Using the same reference training image and example prompts, styledrop on aMUSEd\ndemonstrates much stronger style adherence. In our experiments with aMUSEd, we achieved good\nresults with fine-tuning on a single image and not generating any additional training samples. Style-\ndrop can cheaply fine-tune aMUSEd in as few as 1500-2000 training steps.\n6Using the same training parameters as Sohn et al. (2023) - 400 training steps, UNet LR 2e-4, CLIP LR\n5e-6\n7\nTechnical Report\nFigure 5\nModel\nLearning Rate\nBatch Size\nMemory Required\nSteps\nLoRa Alpha\nLoRa Rank\namused-256\n4e-4\n1\n6.5 GB\n1500-2000\n32\n16\namused-512\n1e-3\n1\n5.6 GB\n1500-2000\n1\n16\nTable 1: Styledrop configs. LoRa applied to all QKV projections.\n5.4\n8BIT QUANTIZATION\nToken based modeling allows for the use of techniques from the language modeling literature, such\nas 8-bit quantization for transformer feed-forward and attention projection layers (Dettmers et al.\n(2022)). Using 8-bit quantization, we can load the whole model with as little as 800 MB of VRAM,\nmaking mobile and CPU applications more feasible.\n(a) a horse in the wild\n(b) the mountains\n(c) a house on a hill\nFigure 6: aMUSEd 256x256 images with 8-bit quantization\n5.5\nTASK TRANSFER\nImage variation and in-painting\nSimilar to Chang et al. (2023), aMUSEd performs zero-shot\nimage editing tasks such as image variation and in-painting. For masked token based image model-\ning, both image variation and in-painting are close to the default training objective, so both tasks use\n8\nTechnical Report\nthe regular decoding procedure. For image variation, some number of latent tokens are masked with\nmore masked latent tokens corresponding to more variation from the original image. For in-painting,\nthe in-painting mask directly determines which tokens are initially masked.\n(a) Original image\n(b) apple watercolor\nFigure 7: aMUSEd 256x256 image variation\n(a) Original image\n(b) winter mountains\nFigure 8: aMUSEd 512x512 image variation\n(a) Original Image\n(b) Mask\n(c) fall mountains\nFigure 9: aMUSEd 512x512 in-painting\nVideo generation\nWe further extended aMUSEd to zero-shot video generation by modifying\ntext2video-zero (Khachatryan et al. (2023)). Text2video-zero operates on stable diffusion\u2019s (Rom-\nbach et al. (2022)) continuous latents. Noised latents are warped by varying amounts to produce\nlatents for successive frames. Additional noise is then added to the frame latents. During the stan-\ndard denoising process, self attention is replaced with cross-attention over the first frame to maintain\ntemporal consistency. Because aMUSEd operates on quantized latents, we must first de-quantize the\nlatents before they are warped. We can then re-quantize the warped latents. Because the aMUSEd\nlatent space is discrete, we completely re-mask the boundary of the image warp, which creates con-\nsistent image backgrounds from frame to frame. We found that the between-frame cross-attention\ndegraded quality for frames warped too far away from the initial frame, so we did not use the modi-\nfied self attention and instead performed the warp much later in the denoising process.\n9\nTechnical Report\nFigure 10: Video generation examples. Full videos\n.\n6\nETHICS AND SAFETY\nWe filtered out images in the training data above a 50% watermark probability or above a 45%\nNSFW probability. We manually checked that both models do not accurately follow NSFW prompts\nand therefore concluded that our NSFW filtering helps prevent possible harmful use cases.\n7\nCONCLUSION\nWe introduced aMUSEd, a lightweight and open-source reproduction of MUSE. Our primary goal\nwas to achieve fast sampling and provide an efficient alternative to diffusion models. In our re-\nproduction, aMUSEd demonstrated competitive zero-shot image variation and in-painting without\nrequiring task specific training. We made several modifications for efficiency, including the use\nof the smaller CLIP-l/14 text encoder (Radford et al. (2021)) and an efficient U-ViT (Hoogeboom\net al. (2023)) backbone. Our results show that aMUSEd\u2019s inference speed is competitive with dis-\ntilled diffusion-based text-to-image models, particularly when scaling with batch size. Additionally,\naMUSEd demonstrates efficient fine-tuning capabilities, providing flexibility for various applica-\ntions. We hope that by open-sourcing all model weights and code, future research into masked\nimage modeling for text-to-image generation is made more accessible.\n8\nCONTRIBUTION & ACKNOWLEDGEMENT\nSuraj led training. William led data and supported training. Patrick supported both training and\ndata and provided general guidance. Robin trained the VQ-GAN and provided general guidance.\n10\nTechnical Report\nAlso, immense thanks to community contributor Isamu Isozaki for helpful discussions and code\ncontributions.\n11\nTechnical Report\nREFERENCES\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal,\nDmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et al. Cm3: A causal masked multi-\nmodal model of the internet. arXiv preprint arXiv:2201.07520, 2022.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao,\nand Aditya Ramesh. Improving image generation with better captions. 2023.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative\nimage transformer, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T. Freeman, Michael Rubinstein, Yuanzhen Li, and Dilip Krishnan.\nMuse: Text-to-image generation via masked generative transformers, 2023.\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James\nKwok, Ping Luo, Huchuan Lu, et al. Pixart-\u03b1: Fast training of diffusion transformer for photore-\nalistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023.\nDeepFloyd.\nStability ai releases deepfloyd if, a powerful text-to-image model that can smartly\nintegrate text into images. https://stability.ai/news/deepfloyd-if-text-to-image-model, 2023.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm.int8(): 8-bit matrix multi-\nplication for transformers at scale, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nTim Dockhorn, Arash Vahdat, and Karsten Kreis. Genie: Higher-order denoising diffusion solvers,\n2022.\nPatrick Esser, Robin Rombach, and Bj\u00a8orn Ommer. Taming transformers for high-resolution image\nsynthesis, 2021.\nAngela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation, 2018.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim En-\ntezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen\nMussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexan-\nder Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh,\nAlex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In\nsearch of the next generation of multimodal datasets, 2023.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\nnetworks, 2017.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration, 2020.\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for\nhigh resolution images, 2023.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n12\nTechnical Report\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\nZhengbao Jiang, Jun Araki, Haibo Ding, and Graham Neubig. How can we know when language\nmodels know? on the calibration of language models for question answering. Transactions of the\nAssociation for Computational Linguistics, 9:962\u2013977, 2021. doi: 10.1162/tacl a 00407. URL\nhttps://aclanthology.org/2021.tacl-1.57.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. Advances in Neural Information Processing Systems, 35:26565\u201326577,\n2022.\nLevon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang\nWang, Shant Navasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models\nare zero-shot video generators, 2023.\nDiederik P. Kingma and Max Welling.\nAn introduction to variational autoencoders.\nCoRR,\nabs/1906.02691, 2019. URL http://arxiv.org/abs/1906.02691.\nHugo Laurenc\u00b8on, Lucile Saulnier, L\u00b4eo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov,\nThomas Wang, Siddharth Karamcheti, Alexander M. Rush, Douwe Kiela, Matthieu Cord, and\nVictor Sanh. Obelics: An open web-scale filtered dataset of interleaved image-text documents,\n2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro\nPerona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00b4ar. Microsoft coco: Common objects\nin context, 2015.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\nDpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast\nsolver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095,\n2022b.\nSimian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Syn-\nthesizing high-resolution images with few-step inference, 2023a.\nSimian Luo, Yiqin Tan, Suraj Patil, Daniel Gu, Patrick von Platen, Apolin\u00b4ario Passos, Longbo\nHuang, Jian Li, and Hang Zhao. Lcm-lora: A universal stable-diffusion acceleration module,\n2023b.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers, 2023.\nEthan Perez, Florian Strub, Harm de Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual\nreasoning with a general conditioning layer, 2017.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer, 2023.\n13\nTechnical Report\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation, 2015.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation, 2023.\nRunwayML.\nStable diffusion inpainting.\nhttps://huggingface.co/runwayml/stable-diffusion-\ninpainting, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image diffu-\nsion models with deep language understanding, 2022.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\npreprint arXiv:2202.00512, 2022.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans, 2016.\nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion dis-\ntillation, 2023.\nChristoph Schuhmann. Laion-aesthetics. https://laion.ai/blog/laion-aesthetics/, 2022.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b:\nAn open large-scale dataset for training next generation image-text models.\narXiv preprint\narXiv:2210.08402, 2022a.\nChristoph Schuhmann, Andreas K\u00a8opf, Richard Vencu, Theo Coombes, and Romain Beaumont.\nLaion coco: 600m synthetic captions from laion2b-en. https://laion.ai/blog/laion-coco/, 2022b.\nKihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred\nBarber, Lu Jiang, Glenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael Rubinstein, and Dilip\nKrishnan. Styledrop: Text-to-image generation in any style, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations, 2021.\nKeqiang Sun, Junting Pan, Yuying Ge, Hao Li, Haodong Duan, Xiaoshi Wu, Renrui Zhang, Aojun\nZhou, Zipeng Qin, Yi Wang, Jifeng Dai, Yu Qiao, Limin Wang, and Hongsheng Li. Journeydb:\nA benchmark for generative image understanding, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.\nPatrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca, Nathan Lambert, Kashif Rasul,\nMishig Davaadorj, and Thomas Wolf.\nDiffusers: State-of-the-art diffusion models.\nURL\nhttps://github.com/huggingface/diffusers.\n14\nTechnical Report\nRyan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b,\n2023.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, Jason Baldridge, and Yonghui Wu. Scaling autoregressive models for content-rich\ntext-to-image generation, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam Polyak, Russell Howes,\nVasu Sharma, Puxin Xu, Hovhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen Li, Susan\nZhang, Richard James, Gargi Ghosh, Yaniv Taigman, Maryam Fazel-Zarandi, Asli Celikyilmaz,\nLuke Zettlemoyer, and Armen Aghajanyan. Scaling autoregressive multi-modal models: Pretrain-\ning and instruction tuning, 2023.\nQinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator,\n2023.\nWenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: A unified predictor-\ncorrector framework for fast sampling of diffusion models. arXiv preprint arXiv:2302.04867,\n2023.\nKaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode\nsolver with empirical model statistics, 2023.\n15\nTechnical Report\nA\nINFERENCE SPEED\nAll measurements were taken in fp16 with the diffusers library (von Platen et al.) model implemen-\ntations. All measurements are of end to end image generation.\nModel\ninference\ntime\ntimesteps\nresolution\nsd-turbo\n0.13 s\n1\n512\nsdxl-turbo\n0.21 s\n1\n1024\nlatent consistency models\n0.32 s\n4\n512\namused-256\n0.47 s\n12\n256\namused-512\n0.54 s\n12\n512\nstable diffusion 1.5\n0.85 s\n20\n512\nSSD-1B\n1.75 s\n20\n1024\nw\u00a8urstchen\n1.96 s\n41\n1024\nsdxl\n2.7 s\n20\n1024\n(a) batch size 1\nModel\ninference\ntime\ntimesteps\nresolution\nsd-turbo\n0.47 s\n1\n512\namused-256\n0.6 s\n12\n256\nsdxl-turbo\n0.71 s\n1\n1024\namused-512\n1.0 s\n12\n512\nlatent consistency models\n1.77 s\n4\n512\nstable diffusion 1.5\n2.96 s\n20\n512\nw\u00a8urstchen\n10.9 s\n41\n1024\nSSD-1B\n12.62 s\n20\n1024\nsdxl\n18.47 s\n20\n1024\n(b) batch size 8\nFigure 11: A100 inference time measurements\nModel\ninference\ntime\ntimesteps\nresolution\nsd-turbo\n0.07 s\n1\n512\nsdxl-turbo\n0.11 s\n1\n1024\namused-256\n0.2 s\n12\n256\nlatent consistency models\n0.24 s\n4\n512\namused-512\n0.24 s\n12\n512\nstable diffusion 1.5\n0.55 s\n20\n512\nw\u00a8urstchen\n1.34 s\n41\n1024\nSSD-1B\n1.88 s\n20\n1024\nsdxl\n2.84 s\n20\n1024\n(a) batch size 1\nModel\ninference\ntime\ntimesteps\nresolution\nsd-turbo\n0.44 s\n1\n512\namused-256\n0.45 s\n12\n256\nsdxl-turbo\n0.75 s\n1\n1024\namused-512\n0.76 s\n12\n512\nlatent consistency models\n1.61 s\n4\n512\nstable diffusion 1.5\n3.16 s\n20\n512\nw\u00a8urstchen\n11.15 s\n41\n1024\nSSD-1B\n11.73 s\n20\n1024\nsdxl\n18.38 s\n20\n1024\n(b) batch size 8\nFigure 12: 4090 inference time measurements\n16\nTechnical Report\nB\nMODEL QUALITY\nFigure 13\nFigure 14\n17\nTechnical Report\nModel\nCLIP\nguidance scale\ntimesteps\nresolution\nssd-1b\n27.38\n10.0\n20\n1024\nsdxl\n27.03\n10.0\n20\n1024\nstable diffusion 1.5\n26.54\n10.0\n20\n512\namused-256\n25.97\n5.0\n12\n256\nlatent consistency models\n25.91\n7.0\n4\n512\nw\u00a8urstchen\n25.82\n4.0\n41\n1024x1536\namused-512\n24.78\n8.0\n12\n512\n(a) CLIP\nModel\nFID\nguidance scale\ntimesteps\nresolution\nstable diffusion 1.5\n15.97\n3.0\n20\n512\nsdxl\n20.21\n5.0\n20\n1024\nssd-1b\n23.43\n4.0\n20\n1024\nw\u00a8urstchen\n28.28\n4.0\n41\n1024x1536\nlatent consistency models\n29.91\n7.0\n4\n512\namused-512\n34.87\n7.0\n12\n512\namused-256\n38.7\n3.0\n12\n256\n(b) FID\nModel\nISC\nguidance scale\ntimesteps\nresolution\nsdxl\n38.88\n7.0\n20\n1024\nw\u00a8urstchen\n38.82\n4.0\n41\n1024x1536\nstable diffusion 1.5\n36.94\n6.0\n20\n512\nssd-1b\n36.79\n6.0\n20\n1024\nlatent consistency models\n34.93\n7.0\n4\n512\namused-512\n26.16\n6.0\n12\n512\namused-256\n23.82\n3.0\n12\n256\n(c) Inception Score\nFigure 15: Model Quality Tables\n18\nTechnical Report\nC\nFINETUNING\naMUSEd can be finetuned on simple datasets relatively cheaply and quickly. Using LoRa (Hu et al.\n(2021)), and gradient accumulation, aMUSEd can be finetuned with as little as 5.6 GB VRAM.\n(a) a pixel art character\n(b)\nsquare red glasses on a pixel\nart character with a baseball-shaped\nhead\n(c) a pixel art character with square\nblue glasses, a microwave-shaped\nhead and a purple-colored body on\na sunny background\nFigure 16: Example outputs of finetuning 256x256 model on dataset\n(a) minecraft pig\n(b) minecraft character\n(c) minecraft Avatar\nFigure 17: Example outputs of fine-tuning 512x512 model on dataset\n8bit Adam\nLoRa\nSingle Step Batch Size\nGrad. Accum. Steps\nLearning Rate\nMemory\nSteps\nNo\nNo\n8\n1\n1e-4\n19.7 GB\n750-1000\nNo\nNo\n4\n2\n1e-4\n18.3 GB\n750-1000\nNo\nNo\n1\n8\n1e-4\n17.9 GB\n750-1000\nYes\nNo\n16\n1\n2e-5\n20.1 GB\n\u223c 750\nYes\nNo\n8\n2\n2e-5\n15.6 GB\n\u223c 750\nYes\nNo\n1\n16\n2e-5\n10.7 GB\n\u223c 750\nNo\nYes\n16\n1\n8e-4\n14.1 GB\n1000-1250\nNo\nYes\n8\n2\n8e-4\n10.1 GB\n1000-1250\nNo\nYes\n1\n16\n8e-4\n6.5 GB\n1000-1250\nTable 2: amused-256 fine-tuning configs. All LoRa trainings used rank 16 and alpha 32. LoRa\napplied to all QKV projections. dataset\n19\nTechnical Report\n8bit Adam\nLoRa\nSingle Step Batch Size\nGrad. Accum. Steps\nLearning Rate\nMemory\nSteps\nNo\nNo\n8\n1\n8e-5\n24.2 GB\n500-1000\nNo\nNo\n4\n2\n8e-5\n19.7 GB\n500-1000\nNo\nNo\n1\n8\n8e-5\n16.99 GB\n500-1000\nYes\nNo\n8\n1\n5e-6\n21.2 GB\n500-1000\nYes\nNo\n4\n2\n5e-6\n13.3 GB\n500-1000\nYes\nNo\n1\n8\n5e-6\n9.9 GB\n500-1000\nNo\nYes\n8\n1\n1e-4\n12.7 GB\n500-1000\nNo\nYes\n4\n2\n1e-4\n9.0 GB\n500-1000\nNo\nYes\n1\n8\n1e-4\n5.6 GB\n500-1000\nTable 3: amused 5-12 fine-tuning configs. All LoRa trainings used rank 16 and alpha 32. LoRa\napplied to all QKV projections. dataset\n20\nTechnical Report\nD\nSTYLEDROP EXAMPLES\n(a)\nReference image: A\nmushroom in [V] style\n(b) A tabby cat walking in\nthe forest in [V] style\n(c) A church on the street\nin [V] style\n(d) A chihuahua walking\non the street in [V] style\nFigure 18: Styledrop amused-256\n(a)\nReference image: A\nmushroom in [V] style\n(b) A tabby cat walking in\nthe forest in [V] style\n(c) A church on the street\nin [V] style\n(d) A chihuahua walking\non the street in [V] style\nFigure 19: LoRa Dreambooth Stable Diffusion\n(a)\nReference image: A\nbear in [V] style.png\n(b) A tabby cat walking in\nthe forest in [V] style\n(c) A church on the street\nin [V] style\n(d) A chihuahua walking\non the street in [V] style\nFigure 20: Styledrop amused-512\nModel\nLearning Rate\nBatch Size\nMemory Required\nSteps\nLoRa Alpha\nLoRa Rank\namused-256\n4e-4\n1\n6.5 GB\n1500-2000\n32\n16\namused-512\n1e-3\n1\n5.6 GB\n1500-2000\n1\n16\nTable 4: Styledrop configs. LoRa applied to all QKV projections.\n21\n"
  },
  {
    "title": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations",
    "link": "https://arxiv.org/pdf/2401.01885.pdf",
    "upvote": "25",
    "text": "From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations\nEvonne Ng1, 2\nJavier Romero1\nTimur Bagautdinov1\nShaojie Bai1\nTrevor Darrell2\nAngjoo Kanazawa2\nAlexander Richard1\n1Codec Avatars Lab, Meta, Pittsburgh\n2University of California, Berkeley\nInput: conversational audio\nIntermediate Output: 3D face, hand, and body motion\nFinal output: Photoreal conversational avatar\nFigure 1. Synthesizing photoreal conversational avatars. Given the audio from a dyadic conversation, we generate realistic conversa-\ntional motion for the face, body, and hands. The motion can then be rendered as a photorealistic video. Please see results video.\nAbstract\nWe present a framework for generating full-bodied pho-\ntorealistic avatars that gesture according to the conversa-\ntional dynamics of a dyadic interaction. Given speech au-\ndio, we output multiple possibilities of gestural motion for\nan individual, including face, body, and hands. The key be-\nhind our method is in combining the benefits of sample di-\nversity from vector quantization with the high-frequency de-\ntails obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using\nhighly photorealistic avatars that can express crucial nu-\nances in gestures (e.g. sneers and smirks). To facilitate this\nline of research, we introduce a first-of-its-kind multi-view\nconversational dataset that allows for photorealistic recon-\nstruction. Experiments show our model generates appropri-\nate and diverse gestures, outperforming both diffusion- and\nVQ-only methods. Furthermore, our perceptual evaluation\nhighlights the importance of photorealism (vs. meshes) in\naccurately assessing subtle motion details in conversational\ngestures. Code and dataset available on project page.\n1. Introduction\nConsider talking to your friend in a telepresent world,\nwhere they appear as the generic golden mannequin shown\nin Figure 1 (middle). Despite the mannequin\u2019s ability to\nact out rhythmic strokes of arm motion that seemingly fol-\nlow your friend\u2019s voice, the interaction will inevitably feel\nrobotic and uncanny. This uncanniness stems from the limi-\ntations imposed by non-textured meshes which mask subtle\nnuances like eye gaze or smirking. Photorealistic details can\neffectively convey these nuances, allowing us to express di-\nverse moods during conversation. For example, a sentence\nspoken while avoiding eye contact differs significantly from\none expressed with sustained gaze. As humans, we are es-\npecially perceptive to these micro-expressions and move-\nments, which we use to formulate a higher-order under-\nstanding of our conversational partner\u2019s intentions, comfort,\nor understanding [10]. Developing conversational avatars\nwith the level of photorealism that can capture these sub-\ntleties is therefore essential for virtual agents to meaning-\nfully interact with humans.\nOur ability to perceive these fine-grain motion patterns\nbreaks down as we represent the motion in more abstracted\nforms. Chaminade et al. [8] demonstrates that humans have\na more difficult time distinguishing real vs. fake key-framed\nmotions (such as walking) in skeletons than in textured\nmeshes, and even more-so in point-based representations\nthan in skeletons. In faces, McDonnell et al. [26] shows\nthat large facial motion anomalies are considerably less dis-\ncernible on Toon (i.e. plain colored, comic-like) characters,\narXiv:2401.01885v1  [cs.CV]  3 Jan 2024\nthan on characters with human textures applied. Although\nabstract representations cannot precisely represent the level\nof detail needed for humans to interpret subtle conversa-\ntional cues, the majority of prior works in gesture gener-\nation [2, 22, 23, 40] still assess their methods using mesh-\nbased or skeletal representations. In this paper we advocate\nthe importance of developing photorealistic conversational\navatars which not only allow us to express subtle motion\npatterns, but also allow us to more accurately evaluate the\nrealism of the synthesized motion.\nTo this end, we present a method for generating pho-\ntorealistic avatars, conditioned on the speech audio of a\ndyadic conversation.\nOur approach synthesizes diverse\nhigh-frequency gestures (e.g. pointing and smirking) and\nexpressive facial movements that are well-synchronized\nwith speech. For the body and hands, we leverage advan-\ntages of both an autoregressive VQ-based method and a dif-\nfusion model.\nOur VQ transformer takes conversational\naudio as input and outputs a sequence of guide poses at\na reduced frame rate, allowing us to sample diverse poses\n(e.g. pointing) while avoiding drift. We then pass both the\naudio and guide poses into the diffusion model, which in-\nfills intricate motion details (e.g. finger wag) at a higher\nfps. For the face, we use an audio conditioned diffusion\nmodel. The predicted face, body, and hand motion are then\nrendered with a photorealistic avatar. We demonstrate the\nadded guide pose conditioning on the diffusion model al-\nlows us to generate more diverse and plausible conversa-\ntional gestures compared to prior works. In a perceptual\nstudy, we further illustrate that evaluators can better distin-\nguish differences between two approaches when motion is\nvisualized with photorealistic avatars than with meshes.\nTo support our approach in modeling the intricacies of\nhuman conversation, we introduce a rich dataset of dyadic\ninteractions captured in a multi-view system. This system\nallows for highly accurate body/face tracking and photore-\nalistic 3D reconstructions of both participants simultane-\nously. The non-scripted, long-form conversations cover a\nwide range of topics and emotions. In contrast to prior full-\nbody datasets that support skeletal [22,23] or Toon-like vi-\nsualizations [24], we reconstruct photorealistic renders of\neach individual in the dataset. Our data also captures the\ndynamics of inter-personal conversations rather than indi-\nvidual monologues [13,24,40]. We will release the dataset\nand renderer, and hope these will encourage the investiga-\ntion of gesture generation in a photorealistic manner.\nTo the best of our knowledge, we are the first to in-\nvestigate the generation of photorealistic face, body, and\nhand motion for interpersonal conversational gestures. Our\nVQ- and diffusion-based method synthesizes more realistic\nand diverse motion compared to prior works. Furthermore,\nwe pose an important question on the validity of evaluat-\ning conversational motion using non-textured meshes, as\nSHOW [40]\n(mesh)\nHumorous\nSerious\nOurs\n(photoreal)\nFigure 2. Importance of photorealism Top: Mesh annotations\nfrom prior work [40]. Bottom: Our photorealistic renderings. For\nthe mesh, differences in laughing (top left) vs. speaking (top right)\nare difficult to perceive. In contrast, photorealism allows us to\ncapture subtle details such as the smirk (bottom left) vs. grimace\n(bottom right), which completely changes the perception of her\ncurrent mood despite similar coarse body poses.\nhumans may overlook or be less critical of inaccuracies in\nthese representations. Finally, to support this investigation,\nwe introduce a novel dataset of long-form conversations that\nenable renderings of photorealistic conversational avatars.\nCode, dataset, and renderers will all be publicly available.\n2. Related Work\nInterpersonal conversational dynamics.\nTraditionally,\nanimating conversational avatars have involved construct-\ning rule-based guides from lab captured motion data [6, 7,\n14, 18]. These methods are often limited in variety of ges-\ntures and rely on simplifying assumptions that do not hold\non in-the-wild data. As a result, there has been greater focus\non using learning-based methods to predict coarse aspects\nof a conversation such as turn-taking [1, 22] or a single fa-\ncial expression to summarize a conversation [19,31]. While\nthese methods focus on higher-level dynamics, our method\nfocuses on the lower-level complexities of interactions by\nmodeling the full range of facial expressions and body-hand\nmotion. In contrast, Tanke et al. [33] predicts the full body\npose, but focuses on a different task of motion forecasting,\nwhere the goal is to generate plausible future body poses for\na triad given their past body motion.\nMore recently, there have been works on modeling cross-\nperson interaction dynamics by predicting the listener\u2019s\nfine-grain 2D [12] or 3D gestural motion from the speaker\u2019s\nmotion and audio [21, 28], text [29], or stylized emotion\n[45]. However, all these methods generate only the head\npose and facial expression of the listener alone. On the other\nextreme, Lee et al. [23] models only the finger motion of the\nspeaker in a dyadic conversation. In contrast, our method is\nthe first to consider the full range of 3D face, body, and hand\nmotion for interpersonal conversation while using a single\nmodel to handle both speaking and listening motion.\nExpression codes\n\ud835\udc53!, \u2026 \ud835\udc53\" \u2208 \u211d#!\nJoint angles\n\ud835\udc57!, \u2026 \ud835\udc57\" \u2208 \u211d$\u00d7#\"\nAvatar\nRenderer\nInput: audio\nOutput: photorealistic motion\nPerson 1 Audio\nPerson 2 Audio\nPose motion \nmodel\nFace motion \nmodel\nFigure 3. Method Overview Our method takes as input conver-\nsational audio and generates corresponding face codes and body-\nhand poses. The output motion is then fed into our trained avatar\nrenderer, which generates a photorealistic video. For details on the\nface/pose models, please see Figure 4.\nGestural motion generation.\nPrior works on diffusion\nhave explored audio to dance [36], text to motion [34], or\neven audio to gestures [2,3,41,44]. In [2,3], body motion\nof a speaker is synthesized using a diffusion model con-\nditioned on audio or text respectively. Meanwhile, Yu et\nal. [41] focuses only on the face by using a diffusion-based\nmethod with contrastive learning to produce lip sync that\nis both accurate and can be disentangled from lip-irrelevant\nfacial motion. While these methods model only the body\nor the face, our approach generates the full face, body, and\nhands of the conversational agent simultaneously.\nSHOW [40] addresses this issue by training separate\nVQ\u2019s to produce face, body, and hand motion given a\nspeaker\u2019s audio. While our approach similarly focuses on\ngenerating the full range of face, body, and hand motion\nfor a conversational agent, our approach significantly differs\nin that we visualize on photorealistic avatars as opposed to\nmesh-based renderings. As depicted in Figure 2, their mesh\ncan represent large arm movements that follow a rhythm,\nbut struggles to capture crucial distinctions between a laugh\nand opening one\u2019s mouth to speak (top). In contrast, we are\nthe first to employ photoreal avatars that can express subtle\nnuances such as a grimace vs. a smirk (bottom). We demon-\nstrate in our analysis (Sec. 5.2) that photorealism greatly\naffects the evaluation paradigm for conversational agents.\nWe further differentiate from these prior works [2,3,40,\n41] in that we model interpersonal communication dynam-\nics of a dyadic conversation as opposed to a single speaker\nin a monadic setting. As a result, our method must model\nboth listener and speaker motion, and generate motion that\nnot only looks realistic with respect to the audio, but also\nreacts realistically to the other individual in conversation.\nConversational datasets. There is a growing number of\nlarge scale datasets for conversational motion [23, 24, 27,\n40]. Pose parameters for the face, body and hands of a\nmonologue speaker are released at large scale in [24, 40].\nSimilarly [23, 28] provide only the body and hand recon-\nstructions. However, all these datasets release only enough\ninformation to reconstruct coarse human meshes or tex-\ntured avatars through blendshapes that lack photorealism\nand high-frequency details [24].\nGiven the popularity of the task of audio-driven lip sync-\ning, there are many datasets with open-sourced pipelines\nfor generating facial motion [9,20,32,35,39,41,43], though\nthese approaches are limited to either 2D video or 3D mesh-\nbased animation. Complementing such work with a focus\non the face, Ginosar et al. [13] provides a way to render out\nthe body and hands of a monologue speaker. To the best of\nour knowledge, we are the first to provide a dataset with full\nsimultaneous reconstructions of the face, body, and hands,\nand to consider this in a dyadic conversational setting.\n3. Photoreal full body motion synthesis\nGiven raw audio from a conversation between two peo-\nple, we introduce a model that generates corresponding\nphotorealistic face, body, and hand motion for one of the\nagents in the dyad.\nWe represent the face as latent ex-\npression codes from the recorded multi-view data follow-\ning [25], and the body pose as joint angles in a kinematic\nskeleton. As shown in Fig. 3, our system consists of two\ngenerative models that produce sequences of expression\ncodes and body poses given audio from the dyadic con-\nversation as input.\nExpression codes and body pose se-\nquences can then be rendered frame-by-frame using our\ntrained neural avatar renderer [5] which produces the full\ntextured avatar with the face, body, and hands from a given\ncamera view.1\nNote that the body and face follow highly different dy-\nnamics. First, the face is strongly correlated with the input\naudio, particularly in terms of lip motion, while the body\nhas a weaker correlation with speech. This leads to greater\ndiversity in plausible body gestures for a given speech in-\nput. Second, since we represent face and body in two differ-\nent spaces (learned expression codes vs. joint angles), each\nof them follow different temporal dynamics. We therefore\nmodel the face and body with two separate motion models.\nThis allows the face model to spend its capacity on generat-\ning speech-consistent facial details, and the body model to\nfocus on generating diverse yet plausible body motion.\nThe face motion model is a diffusion model conditioned\non input audio and lip vertices produced by a pre-trained\nlip regressor (Fig. 4a). For the body motion model, we\nfound that a purely diffusion-based model conditioned only\non audio produces less diverse motion that appears tempo-\nrally uncanny. However, the quality improves when we con-\ndition on diverse guide poses. We therefore split the body\nmotion model into two parts: First, an autoregressive audio-\nconditioned transformer predicts coarse guide poses at 1fps\n1Face expression codes, tracked joint angles, and the pre-trained full-\nbody renderer are released as part of the dataset.\nFiLM\nMLP\nFiLM\nCross Attn.\nFiLM\nSelf Attn.\n\u2026\n,\n\u2026\nLip Regressor\n\ud835\udc00\n\ud835\udc0b\nF\na)\nEncoder\nDecoder\n33\n40\n05\ns\n22\n33\n40\n05\nP\n\ud835\udc00\nb)\nc)\nFiLM\nMLP\nFiLM\nCross Attn.\nCross Attn.\nFiLM\nSelf Attn.\nConv.\nP\nJ\n\ud835\udc00\nNoise\nF\n\u03c4\n\u03c4\nJ\nP\nFigure 4. Motion generation (a) Given conversational audio A, we generate facial motion F using a diffusion network conditioned on\nboth audio and the output of a lip regression network L, which predicts synced lip geometry from speech audio. (b) For the body-hand\nposes, we first autoregressively generate guide poses P at a low fps using a VQ-Transformer. (c) The pose diffusion model then uses these\nguide poses and audio to produce a high-frequency motion sequence J.\n(Fig. 4b), which are then consumed by the diffusion model\nto in-fill fine grain and high-frequency motion (Fig. 4c). We\ndescribe the model components in detail below.\nNotation. We denote the audio of the agent as aself and\nthe audio of the conversation partner as aother. For both\naudio streams, we extract Wav2Vec [4] features such that\nthe audio input is A = (aself , aother) \u2208 R2\u00d7da\u00d7T , with da\ndenoting the feature dimension of Wav2Vec features.\nWe denote a sequence of T face expression codes as F =\n(f1, . . . , fT ), where each ft \u2208 R256 represents a face ex-\npression for frame t. A body motion sequence of T frames\nis represented by J = (j1, . . . , jT ), where jt \u2208 Rdj\u00d73 is\na vector containing three rotation angles for each of the dj\nbody joints that define a pose at frame t. We follow the for-\nward kinematic representation [5], where the body and hand\npose of a given person can be constructed from the relative\nrotations of each joint with respect to its parent joint.\n3.1. Face Motion Diffusion Model\nTo generate facial motion from audio input, we con-\nstruct an audio-conditioned diffusion model. We follow the\nDDPM [15] definition of diffusion. The forward noising\nprocess is defined as:\nq(F(\u03c4)|F(\u03c4\u22121)) \u223c N\n\u0000\u221a\u03b1\u03c4F(\u03c4\u22121), (1 \u2212 \u03b1\u03c4)I\n\u0001\n,\n(1)\nwhere F(0) approximates the clean (noise-free) sequence of\nface expression codes F, \u03c4 \u2208 [1, . . . , \u02d9T] denotes the for-\nward diffusion step, and \u03b1\u03c4 \u2208 (0, 1) follows a monotoni-\ncally decreasing noise schedule such that as \u03c4 approaches\n\u02d9T, we can sample F( \u02d9T ) \u223c N(0, I).\nTo reverse the noising process, we follow [15, 30] and\ndefine a model to denoise F(0) from the noisy F(\u03c4). The\nnext step F(\u03c4\u22121) of the reverse process can then be obtained\nby applying the forward process to the predicted F(0). We\npredict F(0) with a neural network F:\nF(0) \u2248 F(F(\u03c4); \u03c4, A, L),\n(2)\nwhere A are the input audio features and L = (l1, . . . , lT )\nis the output of a pre-trained audio-to-lip regressor fol-\nlowing [9], but limited to lip vertices instead of full face\nmeshes. We train the lip-regressor on 30h of in-house 3D\nmesh data. Each lt \u2208 Rdl\u00d73 is a predicted set of dl lip\nvertices at frame t given audio A. Tab. 2 shows, condition-\ning on both the lip regressor output and audio significantly\nimproves lip sync quality over conditioning on audio alone.\nThe diffusion model is trained with the simplified ELBO\nobjective [15],\nLsimple = E\u03c4,F\n\u0002\nF \u2212 F(F(\u03c4); \u03c4, A, L)\n\u0003\n.\n(3)\nWe train our model for classifier-free guidance [16] by\nrandomly replacing either conditioning with A = \u2205 and\nL = \u2205 during training with low probabilities. To incorpo-\nrate the audio and lip vertex information, we use a cross\nattention layer. Timestep information is incorporated with a\nfeature-wise linear modulation (FiLM) layer, see Fig. 4a.\n3.2. Body Motion Model\nTo generate body motion, we extend the conditional dif-\nfusion model by introducing guide poses sampled at 1fps as\nadditional conditioning. This allows us to model more ex-\npressive motion. Similar to the face model that did not gen-\nerate accurate lip motion when conditioned on audio alone,\nwe found that the body model generates less plausible mo-\ntion with limited diversity when conditioned on audio only.\nMore formally, to generate a full body motion sequence\nat 30fps, we train the body diffusion model with guide\nposes P = {jk\u00b730|1 \u2264 k \u2264 T/30} taken at 1fps. These\nguide poses are obtained by subsampling the original 30 fps\nInput: audio\nOutput: sample 1\nOutput: sample 2\nOutput: sample 3\nFigure 5. Diversity of guide pose rollouts Given the input audio for the conversation (predicted person\u2019s audio in gold), the transformer P\ngenerates diverse samples of guide pose sequences with variations in listening reactions (top), speech gestures (middle), and interjections\n(bottom). Sampling from a rich codebook of learned poses, P can produce \u201cextreme\u201d poses e.g. pointing, itching, clapping, etc. with high\ndiversity across different samples. These diverse poses are then used to condition the body diffusion model J .\nground truth body pose sequence J. The body motion dif-\nfusion model J is then the same network as the face mo-\ntion diffusion model F, but is conditioned on the subsam-\npled guide poses, i.e. J(0) \u2248 J (J(\u03c4); \u03c4, A, P). The guide\nposes are incorporated using an additional cross attention\nlayer (see Fig. 4c). At inference time, however, ground truth\nguide poses are not available and need to be generated.\nGuide pose generation. To generate guide-poses at infer-\nence time, we train an autoregressive transformer to output\ncoarse keyframes at 1fps that adhere to the conversational\ndynamics. As autoregressive transformers typically operate\non discrete tokens [11, 28, 40], we first quantize the 1 fps\nguide pose sequence using a residual VQ-VAE [38]. Resid-\nual VQ-VAEs are similar to vanilla VQ-VAEs [37], but they\nrecursively quantize the residuals of the previous quantiza-\ntion step instead of stopping after a single quantization step.\nThis leads to higher reconstruction quality [38,42].\nLet Z = (z1, . . . , zK) be the resulting quantized em-\nbedding of the K-length guide pose sequence P, where\nzk \u2208 {1, . . . , C}N, C is codebook size, and N is the\nnumber of residual quantization steps.\nWe flatten this\nK \u00d7 N-dimensional quantized embedding Z to obtain \u02c6Z =\n(\u02c6z1, . . . , \u02c6zK\u00b7N). We predict \u02c6Z with an audio-conditioned\ntransformer P, which outputs a categorical distribution over\nthe next token given prior predictions and audio,\np(\u02c6zk|\u02c6z1:k\u22121, A) = P(\u02c6z1:k\u22121; A).\n(4)\nWe train the transformer using a simple cross entropy loss\non the task of next-token prediction with teacher forcing:\nLP = \u2212\nX\nk\u2208{1,...,K\u00b7N}\nlog Pr [P(z1:k\u22121, A) = zk] .\n(5)\nAt test time, we use nucleus sampling [17] to predict the se-\nquence of motion tokens. We can easily control the level of\nvariability seen across samples by increasing or decreasing\nthe cumulative probability.\nThe guide-pose transformer is illustrated in Fig. 4b. For\nfurther architecture details on the residual VQ-VAE and the\ntransformer architecture refer to Appendix B.\n3.3. Photorealistic Avatar Rendering\nGiven both the generated facial expression sequence F\nand the generated body pose sequence J, the full photore-\nalistic avatar can be rendered as illustrated in Fig. 3. Fol-\nlowing [5], we use a learning-based method to build our\ndrivable avatars. The model takes as input one frame of fa-\ncial expression ft, one frame of body pose jt, and a view\ndirection. It then outputs a registered geometry and view-\ndependent texture, which is used to synthesize images via\nrasterization. The model is a conditional variational auto-\nencoder (cVAE) consisting of an encoder and decoder, both\nparameterized as convolutional neural networks. The cVAE\nis trained end-to-end in a supervised manner to reconstruct\nimages of a subject captured in a multi-view capture setup.\nWe train a personalized avatar renderer for each subject in\nour dataset. For details, please refer to [5].\n4. Photorealistic conversational dataset\nWhile there are a plethora of datasets on dyadic inter-\nactions [23, 28], all such datasets are only limited to upper\nbody or facial motion. Most related is Joo et al. [22], which\nintroduces a small-scale dataset of triadic interactions as a\nsubset of the Panoptic Studio dataset. The data includes 3D\nskeletal reconstructions of face, body and hands as well as\nSpeaking\nListening\nFigure 6. Results Our method produces gestural motion that is synchronous with the conversational audio. During periods where the\nperson is listening (top), our model correctly produces still motion, seemingly as if the avatar is paying attention. In contrast, during\nperiods of talking (bottom), the model produces diverse gestures that move synchronously with the audio.\naudio and multi-view raw video footage. The limited (\u2248 3\nhours) and specific data (only focused on haggling), makes\nit difficult to learn diverse motion distributions.\nInspired by this work, we introduce a medium-scale\ndataset capturing dyadic conversations between pairs of in-\ndividuals totaling to 8 hours of video data from 4 partic-\nipants, each engaging in 2 hours of paired conversational\ndata. To ensure diversity of expressions and gestures, we\nprompt the actors with a diversity of situations such as sell-\ning, interviews, difficult scenarios, and everyday discourse.\nMost notably, to the best of our knowledge, we are the\nfirst to provide a dataset accompanied with fully photore-\nalistic renderings of the conversational agents. Rather than\ngenerating and evaluating motion via 3D meshes, our multi-\nview dataset allows us to reconstruct the full face, body, and\nhands in a photorealistic manner. Visualizing via these ren-\nderings allow us to be more perceptive to fine-grain details\nin motion that are often missed when rendered via coarse\n3D meshes. Our evaluations confirm the importance of eval-\nuating gestural motion using photo-real avatars.\nTo create the photorealistic renderings, we captured both\nindividuals simultaneously in multi-view capture domes.\nOne person stood in a full-body dome while the other sat in\na head-only dome. During the conversations, both viewed\nscreens of the other person in real-time. We can then recon-\nstruct high fidelity renderings of the face only for one indi-\nvidual [25], and the face, body, and hands for the other [5].\nTo train our method, we use the ground truth from the full-\nbody capture to supervise our method. We will publicly\nrelease audio, video, precomputed joint angles, face expres-\nsion codes, and trained personalized avatar renderers.\n5. Experiments\nWe evaluate the ability of our model to effectively gener-\nate realistic conversational motion. We quantitatively mea-\nsure the realism and diversity of our results against tracked\nground truth data (F, J). We also perform a perceptual eval-\nuation to corroborate the quantitative results and to measure\nappropriateness of our generated gestures in the given con-\nversational setting. Our results demonstrate evaluators are\nmore perceptive to subtle gestures when rendered on photo-\nrealistic avatars than on 3D meshes.\n5.1. Experimental Setup\nEvaluation Metrics.\nFollowing a combination of prior\nworks [2, 3, 28], we use a composition of metrics to mea-\nsure the realism and diversity of generated motion.\n\u2022 FDg: \u201cgeometric\u201d realism measured by distribution dis-\ntance between generated and ground truth static poses.\nWe directly calculated the Frechet distance (FD) in the\nexpression Rdf and pose space Rdj\u00d73.\n\u2022 FDk: \u201ckinetic\u201d motion realism.\nSimilar to above but\ndistributions calculated on the velocities of motion se-\nquences \u03b4P. Computed in expression RT \u00d7df and pose\nspace RT \u00d7dj\u00d73.\n\u2022 Divg: \u201cgeometric\u201d pose diversity. We randomly sample\n30 expression, pose pairs within a motion sequence and\ncompute average L2 distances between pairs to measure\ndiversity of static expressions/poses in the set.\n\u2022 Divk: Temporal variance across a sequence of expres-\nsions/poses. Measures amount of motion in a sequence.\n\u2022 Divsample: Diversity across different samples. We group\nsamples generated by the same audio and calculate vari-\nance across the samples.\nTogether, these metrics measure both the realism and di-\nversity of the generated gestures in conversation.\nBaselines and ablations.\nWe compare to the following:\n\u2022 Random: Random motion sequences from the train set.\n\u2022 KNN: A segment-search method commonly used for syn-\nthesis. Given input audio, we find its nearest neighbor\nfrom the training set and use its corresponding motion\nsegment as the prediction. We use audio features from\nWav2Vec [4] to encode the audio.\nFDg \u2193\nFDk \u2193\nDivg \u2191\nDivk \u2191\nDivsample \u2191\nGT\n3.09\n2.50\nRandom\n9.371.4\n1.440.04\n3.100.09\n2.490.4\n3.970.8\nKNN\n8.441.6\n0.620.09\n2.130.05\n1.210.3\n1.960.3\nSHOW [40]\n4.970.7\n2.600.10\n2.100.09\n0.770.1\n2.820.2\nLDA [2]\n5.080.2\n1.040.07\n2.450.06\n1.880.3\n2.680.4\nOurs Uncond\n8.451.3\n1.530.08\n2.740.07\n2.060.4\n2.940.3\nOurs w/o P\n5.080.4\n1.130.09\n2.470.06\n1.670.3\n2.060.4\nOurs w/o A\n3.940.1\n0.980.10\n2.690.08\n2.160.4\n2.710.3\nOurs\n2.940.2\n0.960.07\n2.980.07\n2.360.4\n3.580.5\nTable 1. Baselines and ablations vs. ground truth poses (GT).\n\u2193 indicates lower is better. We average across all subjects in the\ndataset. We sample 5 sequences for Divsample and average across\nall samples for each metric. Standard deviation as subscript (\u00b5\u03c3).\n.\nHorizontal L2 Error \u2193\nVertical L2 Error\u2193\nMesh L2 \u2193\nSHOW [40]\n2.76\n2.15\n2.25\nOurs w/o L\n2.62\n2.43\n2.24\nOurs\n2.29\n1.89\n1.76\nTable 2. Lip reconstructions The vertical (horizontal) distance\nis the distance between top and bottom (left and right) keypoints\nalong the y (x) axis. The errors shown are L2 differences between\nground truth and generated distances. Mesh L2 is the error in gen-\nerated vs. GT mesh vertices on the lip region. Errors in mm2.\n\u2022 SHOW [40]: VQ-VAE based method that uses a trans-\nformer to autoregressively output motion conditioned on\nthe audio of a speaker. They have separate models for\nface, body, and hands. Given [40] is trained on mono-\nlogues, we retrain their model for our domain.\n\u2022 LDA [2]: Audio to motion diffusion model trained in a\nmonologue setting. We re-train to adapt to our domain.\n\u2022 Ours Uncond: (ablation) unconditional motion genera-\ntion without audio or guide pose conditioning.\n\u2022 Ours w/o P: (ablation) audio conditioned motion diffu-\nsion without guide pose conditioning. Similar to LDA [2].\n\u2022 Ours w/o A: (ablation) guide pose conditioned motion\ndiffusion model but without audio conditioning. Similar\nto diffusion infilling approaches.\n5.2. Results\nThrough quantitative evaluations, we show that our pro-\nposed method outputs realistic motion more diverse than\ncompeting baselines. In our Mechanical Turk A/B evalua-\ntions, we demonstrate our method generates compelling and\nplausible gestures, consistently outperforming our strongest\nbaseline. Additionally, the A/B tests highlight that photore-\nalism effectively captures subtle nuances in gestures that are\nchallenging to discern from 3D meshes. Yet these details\nsignificantly effect the evaluation of conversational motion.\n0\n50\n100\n150\n200\n250\n300\n350\n400\n1.2\n0.8\n0.4\n0.0\nTime\nL2 from neutral pose\nugh,          no! No,       Joe! I don\u2019t wanna drive around\nOurs\nLDA[2]\nGT\nFigure 7. Motion correlation with audio Given audio (top), we\nplot the L2 distance of each pose to the mean neutral pose across\n400 frames. Ours (rendered avatars, orange line) closely matches\nthe peaks corresponding to large motion also seen in ground truth\n(e.g. a flick of the hand preempting the \u201cugh\u201d). LDA [2] (pink)\nmisses these peaky motions. Also note how our method generates\nhighly expressive facial motion matching the speech.\nQuantitative Results.\nTable 1 shows that compared to\nprior works, our method achieves the lowest FD scores\nwhile generating motion with highest diversity. While Ran-\ndom has good diversity that matches that of GT, the ran-\ndom segments do not appropriately match the correspond-\ning conversational dynamics, resulting in higher FDg. A\nslight improvement to Random is KNN, conventionally\nused for motion synthesis.\nWhile KNN performs better\nin terms of realism, matching the \u201ckinetic\u201d distribution of\nthe ground truth sequences better than Ours, the diversity\nacross and within samples is significantly lower, also indi-\ncated by the higher \u201cgeometric\u201d FD. In Fig. 5, we demon-\nstrate the diversity of guide poses our method generates.\nSampling via the VQ-based transformer P allows us to pro-\nduce significantly different styles of poses conditioned on\nthe same audio input. The diffusion model then learns to\nproduce dynamic motion (Fig. 6), where the motion faith-\nfully follows the conversational audio.\nOur method outperforms both a VQ-only approach\nSHOW [40] and a diffusion-only approach LDA [2],\nachieving better realism and diversity across samples.\nWithin sequences, our method generates more motion, re-\nsulting in a higher Divk. Fig. 7 highlights this, demonstrat-\ning LDA [2] produces dampened motion with less variation.\nIn contrast, our method synthesizes variations in motion that\nclosely match ground-truth.\nOur ablations justify our design decisions. Applying our\nmethod without any conditioning (Ours Uncond) performs\nnotably worse, with a realism and variance similar to that\nof Random. This suggests that while the motion gener-\nated does not match the given conversation sequence, it is\nOurs vs. LDA\nOurs vs. GT\nMesh\nPhotoreal\nStrongly pref. Ours\nSlightly pref. Ours\nSlightly pref. Theirs\nStrongly pref. Theirs\n16.4\n43.6\n26.4\n13.6\n65.2\n13.9\n7.1\n13.7\n29.3\n41.4\n9.3\n20.0\n30.3\n18.0\n27.9\n23.7\nFigure 8.\nPerceptual evaluation on Ours vs. ground truth or\nOurs vs. our strongest baseline LDA [2]. We compare using mesh\nvs. photorealistic visualizations. Ours outperforms LDA [2] in\nboth mesh and photoreal settings (top). Further, we note people\nare able to distinguish GT more often in the photoreal setting than\nwith meshes (bottom). The results suggest that evaluating with\nphotorealistic avatars leads to more accurate evaluations.\nsimilar to real motion in the dataset. Audio only condi-\ntioning (Ours w/o P) improves over unconditional gener-\nation and performs similarly to LDA [2], an audio to mo-\ntion diffusion-based method. The lower diversity in both\nthe static poses and across a temporal sequence results in\nhigher FD scores. When adding only the guide pose con-\nditioning (Ours w/o A), both the diversities and FD scores\nimprove significantly. This suggests that the coarse-to-fine\nparadigm, introduced through the predicted guide poses,\nhelps to add diversity to the diffusion results. It also sug-\ngests that the coarse guide poses produced by the trans-\nformer P follow a trajectory that faithfully matches the dy-\nnamics of the conversational audio. The FD scores and di-\nversities further improve when adding both the audio and\nguide pose conditioning in the body motion model J .\nFurthermore, we analyze the accuracy of our method in\ngenerating lip motion. In Table 2, we calculate the ver-\ntical and horizontal distances between two pairs of key-\npoints representing the top/bottom and left/right corners of\nthe mouth, respectively. The vertical distance measures er-\nrors in mouth opening while the horizontal distance mea-\nsures mouth expressions, e.g. a smile shifts the positions\nof the left/right mouth corner and increases the horizon-\ntal distance. We compare these distances against ground\ntruth and compute the L2 error. Our approach (Ours in\nTable 2) substantially outperforms an ablation without the\npretrained lip regressor (Ours w/o L in Table 2) and the\nbaseline SHOW [40]. Qualitatively, the pretraining of the\nlip regressor not only improves lip syncing, but also pre-\nvents the mouth from randomly opening and closing while\nnot talking. This results in better overall lip reconstructions,\nwith lower errors on the face mesh vertices (Mesh L2).\nPerceptual Evaluation.\nGiven the challenge of quantify-\ning the coherence of gestures in conversation, we primarily\nevaluate this aspect through a perceptual evaluation. We\nconducted two variations of A/B tests on Amazon Mechan-\nical Turk. In the first, evaluators viewed motion rendered\non a generic non-textured mesh. In the second, they viewed\nvideos of the motion on photorealistic avatars.\nIn both cases, evaluators watched a series of video pairs.\nIn each pair, one video was from our model and the other\nwas from either our strongest baseline LDA [2] or ground\ntruth. Evaluators were then asked to identify the motion that\nlooked more plausible given the conversational audio. They\nwere also asked to indicate how confident they were in their\nanswer by selecting \u201cslightly prefer\u201d vs. \u201cstrongly prefer\u201d.\nAs shown in Fig. 8, ours significantly outperforms\nagainst our strongest baseline LDA [40], with about 70%\nof evaluators preferring our method in both the mesh and\nphotoreal settings.\nInterestingly, evaluators shifted from\nslightly to strongly preferring ours when visualized in a pho-\ntorealistic manner (top row). This trend continues when\nwe compare our method against ground truth (bottom row).\nWhile ours performs competitively against ground truth in a\nmesh-based rendering, it lags in the photoreal domain with\n43% of evaluators strongly preferring ground truth over\nours. Since meshes often obscure subtle motion details, it is\ndifficult to accurately evaluate the nuances in gestures lead-\ning to evaluators being more forgiving of \u201cincorrect\u201d mo-\ntions. Our results suggest that photorealism is essential to\naccurately evaluating conversational motion.\n6. Conclusion\nIn this work, we explored generating conversational ges-\ntures conditioned on audio for fully embodied photorealis-\ntic avatars. To this end, we combine the benefits of vector\nquantization with diffusion to generate more expressive and\ndiverse motion. We train on a novel multi-view, long-form\nconversational dataset that allows for photorealistic recon-\nstructions. Our method produces diverse face, body, and\nhand motion that accurately matches the conversational dy-\nnamics. The results also underscore the significance of pho-\ntorealism in evaluating fine-grain conversational motion.\nLimitations and ethical considerations. While our model\nproduces realistic motion, it operates on short-range audio.\nIt thus fails to generate gestures requiring long-range lan-\nguage understanding (e.g. counting), which we leave for fu-\nture work. Further, our work is limited to photorealistic gen-\neration of four subjects in our dataset. This limitation ad-\ndresses ethical concerns since only consenting participants\ncan be rendered, as opposed to arbitrary non-consenting hu-\nmans. In releasing a dataset with full participant consent,\nwe hope to provide researchers the opportunity to explore\nphotorealistic motion synthesis in an ethical setting.\nAcknowledgements. The work of Ng and Darrell is sup-\nported by DoD and/or BAIR Commons resources.\nReferences\n[1] Chaitanya Ahuja, Shugao Ma, Louis-Philippe Morency, and\nYaser Sheikh. To react or not to react: End-to-end visual\npose forecasting for personalized avatar during dyadic con-\nversations. In 2019 International Conference on Multimodal\nInteraction, pages 74\u201384, 2019. 2\n[2] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and\nGustav Eje Henter.\nListen, denoise, action! audio-driven\nmotion synthesis with diffusion models. ACM Transactions\non Graphics (TOG), 42(4):1\u201320, 2023. 2, 3, 6, 7, 8\n[3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:\nGesture diffusion model with clip latents.\narXiv preprint\narXiv:2303.14613, 2023. 3, 6\n[4] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and\nMichael Auli. wav2vec 2.0: A framework for self-supervised\nlearning of speech representations. Advances in neural infor-\nmation processing systems, 33:12449\u201312460, 2020. 4, 6\n[5] Timur Bagautdinov, Chenglei Wu, Tomas Simon, Fabi\u00b4an\nPrada, Takaaki Shiratori, Shih-En Wei, Weipeng Xu, Yaser\nSheikh, and Jason Saragih. Driving-signal aware full-body\navatars. ACM Trans. Graph., 40(4), 2021. 3, 4, 5, 6\n[6] Dan Bohus and Eric Horvitz. Facilitating multiparty dialog\nwith gaze, gesture, and speech. In International Conference\non Multimodal Interfaces and the Workshop on Machine\nLearning for Multimodal Interaction, ICMI-MLMI \u201910, New\nYork, NY, USA, 2010. Association for Computing Machin-\nery. 2\n[7] Justine Cassell, Catherine Pelachaud, Norman Badler, Mark\nSteedman, Brett Achorn, Tripp Becket, Brett Douville, Scott\nPrevost, and Matthew Stone. Animated conversation: Rule-\nbased generation of facial expression, gesture and spoken\nintonation for multiple conversational agents. In Proceed-\nings of the 21st Annual Conference on Computer Graphics\nand Interactive Techniques, SIGGRAPH \u201994, New York, NY,\nUSA, 1994. Association for Computing Machinery. 2\n[8] Thierry Chaminade, Jessica Hodgins, and Mitsuo Kawato.\nAnthropomorphism influences perception of computer-\nanimated characters\u2019 actions. Social cognitive and affective\nneuroscience, 2(3):206\u2013216, 2007. 1\n[9] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag\nRanjan, and Michael Black. Capture, learning, and synthesis\nof 3D speaking styles. In Proceedings IEEE Conf. on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10101\u2013\n10111, 2019. 3, 4\n[10] Paul Ekman and Wallace V Friesen. Nonverbal leakage and\nclues to deception. Psychiatry, 32(1):88\u2013106, 1969. 1\n[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021. 5\n[12] Will Feng, Anitha Kannan, Georgia Gkioxari, and Larry Zit-\nnick. Learn2smile: Learning non-verbal interaction through\nobservation. IROS, 2017. 2\n[13] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, An-\ndrew Owens, and Jitendra Malik. Learning individual styles\nof conversational gesture. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 3497\u20133506, 2019. 2, 3\n[14] Jonathan Gratch, Anna Okhmatovskaia, Francois Lamothe,\nStacy Marsella, Mathieu Morales, Rick J van der Werf,\nand Louis-Philippe Morency. Virtual rapport. In Interna-\ntional Workshop on Intelligent Virtual Agents, pages 14\u201327.\nSpringer, 2006. 2\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 4\n[16] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance.\nNeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 4\n[17] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin\nChoi. The curious case of neural text degeneration. arXiv\npreprint arXiv:1904.09751, 2019. 5\n[18] Lixing Huang,\nLouis-Philippe Morency,\nand Jonathan\nGratch. Virtual rapport 2.0. In International workshop on\nintelligent virtual agents, pages 68\u201379. Springer, 2011. 2\n[19] Yuchi Huang and Saad M Khan. Dyadgan: Generating fa-\ncial expressions in dyadic interactions. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion Workshops, pages 11\u201318, 2017. 2\n[20] Xinya Ji,\nHang Zhou,\nKaisiyuan Wang,\nWayne Wu,\nChen Change Loy, Xun Cao, and Feng Xu. Audio-driven\nemotional video portraits. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 14080\u201314089, 2021. 3\n[21] Patrik Jonell, Taras Kucherenko, Erik Ekstedt, and Jonas\nBeskow.\nLearning non-verbal behavior for a social robot\nfrom youtube videos. In ICDL-EpiRob Workshop on Natu-\nralistic Non-Verbal and Affective Human-Robot Interactions,\nOslo, Norway, August 19, 2019, 2019. 2\n[22] Hanbyul Joo, Tomas Simon, Mina Cikara, and Yaser Sheikh.\nTowards social artificial intelligence: Nonverbal social sig-\nnal prediction in a triadic interaction.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10873\u201310883, 2019. 2, 5\n[23] Gilwoo Lee, Zhiwei Deng, Shugao Ma, Takaaki Shiratori,\nSiddhartha S Srinivasa, and Yaser Sheikh.\nTalking with\nhands 16.2 m: A large-scale dataset of synchronized body-\nfinger motion and audio for conversational motion analy-\nsis and synthesis. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 763\u2013772,\n2019. 2, 3, 5\n[24] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng,\nZhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng.\nBeat: A large-scale semantic and emotional multi-modal\ndataset for conversational gestures synthesis. arXiv preprint\narXiv:2203.05297, 2022. 2, 3\n[25] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser\nSheikh. Deep appearance models for face rendering. ACM\nTrans. on Graphics, 37(4), 2018. 3, 6\n[26] Rachel McDonnell, Martin Breidt, and Heinrich H B\u00a8ulthoff.\nRender me real? investigating the effect of render style on\nthe perception of animated virtual humans. ACM Transac-\ntions on Graphics (TOG), 31(4):1\u201311, 2012. 1\n[27] Evonne Ng, Shiry Ginosar, Trevor Darrell, and Hanbyul Joo.\nBody2hands: Learning to infer 3d hands from conversational\ngesture body dynamics. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11865\u201311874, 2021. 3\n[28] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor Darrell,\nAngjoo Kanazawa, and Shiry Ginosar. Learning to listen:\nModeling non-deterministic dyadic facial motion. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 20395\u201320405, 2022. 2, 3, 5,\n6\n[29] Evonne Ng, Sanjay Subramanian, Dan Klein, Angjoo\nKanazawa, Trevor Darrell, and Shiry Ginosar. Can language\nmodels learn to listen? In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), 2023.\n2\n[30] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171, 2021.\n4\n[31] Behnaz Nojavanasghari, Yuchi Huang, and Saad Khan.\nInteractive generative adversarial networks for facial ex-\npression generation in dyadic interactions.\narXiv preprint\narXiv:1801.09092, 2018. 2\n[32] Alexander Richard, Michael Zollhoefer, Yandong Wen, Fer-\nnando de la Torre, and Yaser Sheikh. Meshtalk: 3d face an-\nimation from speech using cross-modality disentanglement.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV), 2021. 3\n[33] Julian Tanke, Linguang Zhang, Amy Zhao, Chengcheng\nTang, Yujun Cai, Lezi Wang, Po-Chen Wu, Juergen Gall,\nand Cem Keskin. Social diffusion: Long-term multiple hu-\nman motion anticipation. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 9601\u2013\n9611, 2023. 2\n[34] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model. arXiv preprint arXiv:2209.14916, 2022. 3\n[35] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian\nTheobalt, and Matthias Nie\u00dfner.\nNeural voice puppetry:\nAudio-driven facial reenactment. In Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August\n23\u201328, 2020, Proceedings, Part XVI 16, pages 716\u2013731.\nSpringer, 2020. 3\n[36] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 448\u2013458, 2023. 3\n[37] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 5\n[38] A Vasuki and PT Vanathi. A review of vector quantization\ntechniques. IEEE Potentials, 25(4):39\u201347, 2006. 5\n[39] Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic.\nRealistic speech-driven facial animation with gans. Interna-\ntional Journal of Computer Vision, 128:1398\u20131413, 2020. 3\n[40] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong\nWen, Timo Bolkart, Dacheng Tao, and Michael J Black.\nGenerating holistic 3d human motion from speech. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 469\u2013480, 2023. 2, 3, 5, 7, 8\n[41] Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn\nWong, and Baoyuan Wang. Talking head generation with\nprobabilistic audio-to-visual diffusion priors. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 7645\u20137655, 2023. 3\n[42] Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan\nSkoglund, and Marco Tagliasacchi. Soundstream: An end-\nto-end neural audio codec. IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, 30:495\u2013507, 2021.\n5\n[43] Jiangning Zhang, Liang Liu, Zhucun Xue, and Yong Liu.\nApb2face: Audio-guided face reenactment with auxiliary\npose and blink signals. In ICASSP 2020-2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 4402\u20134406. IEEE, 2020. 3\n[44] Yihao Zhi, Xiaodong Cun, Xuelin Chen, Xi Shen, Wen Guo,\nShaoli Huang, and Shenghua Gao. Livelyspeaker: Towards\nsemantic-aware co-speech gesture generation. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), pages 20807\u201320817, October 2023. 3\n[45] Mohan Zhou, Yalong Bai, Wei Zhang, Ting Yao, Tiejun\nZhao, and Tao Mei. Responsive listening head generation:\nA benchmark dataset and baseline. In ECCV, 2022. 2\nAppendix\nA. Results Video\nThe supplementary video shows sequences of various\nindividuals in different conversational settings from our\ndataset. Below, we denote the time stamp range associated\nwith the discussion - (@mm:ss-mm:ss).\nThe results show that our model successfully models\nplausible, motion that is synchronous with the ongoing\nconversational dynamics.\nFor instance, it correctly gen-\nerates facial expressions and body language of someone\nfeeling disgruntled e.g. dismissive hand wave and turning\naway (@02:58-03:11).\nThe generated gestures are well-\ntimed with the conversation e.g. raised finger with \u201cI think\u201d\n(@02:30-@02:50).\nAdditionally, our approach can pro-\nduce multiple plausible motion trajectories based on a sin-\ngle conversational audio input, each with distinct variations\n(@03:15-@03:55).\nCompared against baselines and prior works, our method\ngenerates more \u201cpeaky\u201d motion such as wrist flicks while\nlisting (@04:16), and finger pointing (@04:50), which are\nboth missed by a diffusion-based method LDA [Alexander-\nson et al. 2023].\nIn comparison to a VQ-based method\nSHOW [Yi et al. 2023], ours produces more dynamic mo-\ntion with increased arm movement (@04:52), and seamless\ntransitions between poses when switching from asking a\nquestion, to listening, to responding (@05:12-05:30). In\ncontrast, SHOW moves to the audio but hovers around the\nsame pose throughout. In comparison to both Random and\nKNN, gestures by our approach match the audio far better.\nNotably, without any retraining, our method generalizes\nto conversational audio not seen in the dataset, such as a ran-\ndom movie clip audio (@05:44-@06:03). This is possibly\ndue to the identity-agnostic training of Wav2Vec. We can\nalso extend our method to the application of video editing,\nwhere we can reanimate a target person with a different mo-\ntion trajectory by swapping guide poses (@06:10-06:27).\nB. Method\nB.1. Pose representation\nWhile we use a standard SO(3) representation for the\njoint angles, we note that not all joints are parameterized\nwith 3 degrees of freedom ( e.g. arm twist is only repre-\nsented with roll, head bend with yaw, etc. ). In total, we\nhave 104 rotation angles across all of the joints.\nB.2. Residual VQ-VAE\nThe residual VQ-VAE allows us to capture finer-grain\ndetails by employing a cascade of codebooks to capture pro-\ngressively finer approximations. We use residual length of\n4. In practice, this means we need a sequence of 4 VQ to-\nkens to represent a single pose. To generate poses during\ntest time for the diffusion model, we autoregressively out-\nput 4 \u00d7 K tokens one at a time, where K is the length of\nthe downsampled sequence. For the both the encoder and\ndecoder, we use a series of 1D convolutions of kernel size\n2. The total receptive field for both the encoder and decoder\nis 8. We use a codebook size of 1024, and embedding size\nof 64. We train for 300k steps.\nB.3. Guide pose Transformer\nWe adapt the diffusion model\u2019s architecture for the guide\npose network. The transformer architecture is composed of\nmasked self-attention layers that focuses only on previous\ntimesteps to enable autoregressive prediction. The audio\nis then incorporated using non-causal cross attention lay-\ners. This means the network doesn\u2019t see past motion, but\nsees the full context of audio.\nWe then remove the dif-\nfusion timestep \u03c4 conditioning, and instead feed in an au-\ndio embedding (averaged over the whole time series) to the\nFiLM layers. While not necessary, this slightly helps the\ntransformer to generate more plausible poses on the very\nfirst time-step. We use 2 masked self-attention layers and\n6 cross-attention layers, all with 8 heads.\nWe train for\n\u2248 100k iterations depending on the individual.\nB.4. Implementation details\nWe use a max sequence length of 600 frames at 30 fps\n(20 second videos). During training, we randomly sample\na sequence between 240 frames and 600 frames. We then\ntrain on padded sequences of random lengths for all of our\nnetworks. This allows us to generate sequences of arbitrary\nlength during test time. We train each network for each\nsubject in the data separately. All networks are trained on a\nsingle A100. Approximate train times: face diffusion model\n(8 hr), VQ + coarse pose predictor (5 hr), pose diffusion\nmodel (8 hr).\nC. Results\nC.1. Perceptual evaluation\nFor each Ours vs. GT (mesh), vs. GT (photoreal),\nvs. LDA (mesh), vs. LDA (photoreal), we generate 50 A-B\ntests. For each test, we ask 3 different evaluators, totalling\nto 600 evaluators. Each A-B test contained 14 questions.\nPrior to the actual test, we provide a headphone check to\nmake sure the evaluators are listening to audio. However,\nwe do not ask additional questions that check to see if they\nare actually listening to the speech. The landing page de-\nscribes the task and walks evaluators through 2 examples.\nTo ensure the evaluators are not randomly clicking, we in-\nclude 3 questions with an obvious mismatch (one speaker\nlaughing while the listener is neutral) twice. If the evaluator\nselects a different response for these duplicated questions,\nwe do not allow them to submit.\nC.2. Ablation with VQ-only method\nIn the main paper, the VQ-only baseline is represented\nwith prior work SHOW [Alexanderson et al. 2023], which\nis very similar to our guide pose network. For complete-\nness, we also train a VQ-only baseline using our network\narchitecture. We see very similar results to SHOW and sim-\nilar limitations. Quantitatively, FDg = 5.00, FDk = 2.80,\nDivg = 2.20, Divk = 1.89. Note the higher FD and lower\ndiversity compared to our complete method. We notice that\nafter many timesteps, drift often happens which causes the\nmethod to either get stuck in a local minima (no motion).\n"
  },
  {
    "title": "GPT-4V(ision) is a Generalist Web Agent, if Grounded",
    "link": "https://arxiv.org/pdf/2401.01614.pdf",
    "upvote": "19",
    "text": "GPT-4V(ision) is a Generalist Web Agent, if Grounded\nBoyuan Zheng 1 Boyu Gou 1 Jihyung Kil 1 Huan Sun 1 Yu Su 1\nhttps://osu-nlp-group.github.io/SeeAct\nAbstract\nThe recent development on large multimodal mod-\nels (LMMs), especially GPT-4V(ision) and Gem-\nini, has been quickly expanding the capability\nboundaries of multimodal models beyond tradi-\ntional tasks like image captioning and visual ques-\ntion answering. In this work, we explore the po-\ntential of LMMs like GPT-4V as a generalist web\nagent that can follow natural language instruc-\ntions to complete tasks on any given website. We\npropose SEEACT, a generalist web agent that har-\nnesses the power of LMMs for integrated visual\nunderstanding and acting on the web. We evaluate\non the recent MIND2WEB benchmark. In addi-\ntion to standard offline evaluation on cached web-\nsites, we enable a new online evaluation setting by\ndeveloping a tool that allows running web agents\non live websites. We show that GPT-4V presents a\ngreat potential for web agents\u2014it can successfully\ncomplete 51.1% of the tasks on live websites if we\nmanually ground its textual plans into actions on\nthe websites. This substantially outperforms text-\nonly LLMs like GPT-4 or smaller models (FLAN-\nT5 and BLIP-2) specifically fine-tuned for web\nagents. However, grounding still remains a major\nchallenge. Existing LMM grounding strategies\nlike set-of-mark prompting turns out to be not\neffective for web agents, and the best grounding\nstrategy we develop in this paper leverages both\nthe HTML structure and visuals. Yet, there is still\na substantial gap with oracle grounding, leaving\nample room for further improvement. All code,\ndata, and evaluation tools are available at https:\n//github.com/OSU-NLP-Group/SeeAct.\n1. Introduction\nLarge multimodal models (LMMs; Li et al. (2023); Alayrac\net al. (2022); Liu et al. (2023b)), especially recent ones\nsuch as GPT-4V(ision) (OpenAI, 2023) and Gemini (Anil\n1The Ohio State University.\nRent a truck near zip 08817 on December 10 at 11:30AM returned to the exact location and date.\nLMM\nMove the cursor over\nthe \"Find Your Truck\"\nbutton located in the\ncentral portion of the\nwebpage, just below the\ninput fields for rental\ndetails, and perform a\nclick action.\nGrounding\nElement: <input id=19 button find your truck />\nOperation: CLICK\nSeeAct\nBrowser Event\nFigure 1: SEEACT leverages an LMM like GPT-4V to visu-\nally perceive websites and generate plans in textual forms.\nThe textual plans are then grounded onto the HTML ele-\nments and operations to act on the website.\net al., 2023), have shown a remarkable capability on\nstandard vision-and-language understanding and reasoning\nbenchmarks (Kazemzadeh et al., 2014; Goyal et al., 2016;\nHendrycks et al., 2020; Saikh et al., 2022; Lu et al., 2022;\nZhong et al., 2023; Yue et al., 2023). While web content has\nbeen a primary source of training data, a largely overlooked\npart of the web is the websites themselves\u2014every website\nis designed to be rendered visually for easy consumption\nby human users. This poses a new challenge and a new\nopportunity for LMMs. On the one hand, screenshots of\nrendered websites, which could contain thousands of ele-\nments with rich relations, are more complex than the images\nin most existing benchmarks, which are usually object- or\nscene-centric. On the other hand, if LMMs can accurately\ncomprehend websites, it will open the door for numerous\napplications on the web.\nIn this work, we aim to investigate the potential of LMMs\nas generalist web agents (Deng et al., 2023). A generalist\nweb agent, as defined in MIND2WEB (Deng et al., 2023), is\nexpected to follow natural language instructions and com-\nplete tasks on any given real-world website (e.g., Figure 1).\n1\narXiv:2401.01614v2  [cs.IR]  12 Mar 2024\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nThe tasks can be fairly diverse and complex, with one task\npossibly taking 10+ actions across multiple dynamically ren-\ndered webpages. Existing work (Deng et al., 2023; Liu et al.,\n2023d) primarily uses large language models (LLMs) such\nas GPT-4 (OpenAI, 2023) on the raw HTML input. How-\never, HTML code is noisier than the rendered visuals and\nhas a lower information density. For example, the screen-\nshot in Figure 1 contains 423 HTML elements that would\nrequire 186,490 textual tokens with the GPT-2 Tokenizer,\nwhile requiring only 1,445 visual tokens using GPT-4V\u2019s\nvisual tokenizer. Furthermore, HTML alone provides in-\ncomplete information and misses critical semantics from,\ne.g., embedded images.\nTo this end, we propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual\nunderstanding and acting on the web. We will focus on\nGPT-4V, the most advanced LMM publicly available to\ndate, and compare it with smaller LMMs such as BLIP-\n2 (Li et al., 2023) and LLaVA-1.5 (Liu et al., 2023a;c). We\nfind that GPT-4V exhibits a strong capability in visually un-\nderstanding rendered webpages and generate the right plans\nin textual forms across a wide range of websites and tasks.\nHowever, grounding (Chandu et al., 2021; Gu et al., 2023),\ni.e., converting the textual plan into precise actions on the\nwebsite, remains a major challenge. It involves selecting\nthe right HTML element to interact with as well as the right\noperation (e.g., Click, Type, or Select). We propose\nmultiple grounding methods, including superpositioning\nbounding boxes and index labels onto the image, similar to\nset-of-mark prompting (Yang et al., 2023a) that has been\nshown effective on object- or scene-centric images. How-\never, we find that on complex images with rich semantic\nand spatial relationships like webpage screenshots, severe\nhallucination is observed from GPT-4V. The most effective\ngrounding strategy leverages the known correspondence be-\ntween HTML element and their visual rendering, a unique\nproperty for websites compared to natural images.\nWe evaluate SEEACT on the MIND2WEB dataset (Deng\net al., 2023) and compare it with text-only large language\nmodels (LLMs) like GPT-4 (OpenAI, 2023) as well as\nsmaller models (FLAN-T5 (Chung et al., 2022), BLIP-2 (Li\net al., 2023), LLaVA-1.5 (Liu et al., 2023a;c), and CogA-\ngent (Hong et al., 2023). In addition to the standard offline\nevaluation setting on cached websites, we further establish\na new online evaluation setting by developing a tool that\nallows for running web agents on live websites. The major\nfindings from our exploration are summarized below:\n\u2022 SEEACT with GPT-4V is a strong generalist web agent,\nif oracle grounding is provided. In online evaluation, it\ncan successfully complete 51.1% of tasks on different\nwebsites, substantially outperforming existing methods\nlike GPT-4 (13.3%) or FLAN-T5 (8.9%). This strongly\ndemonstrates the potential of LMMs like GPT-4V for\nweb agents.\n\u2022 However, grounding is still a major challenge. The best\ngrounding strategy still has a 20-30% gap with oracle\ngrounding. Among the various grounding strategies,\nthe best one organically leverages both HTML text and\nvisuals, substantially outperforming image annotation\nstrategies (Yang et al., 2023a) by up to 10%.\n\u2022 In-context learning with large models (both LMMs\nand LLMs) shows better generalization to unseen web-\nsites, while supervised fine-tuning still has an edge on\nwebsites seen during training.\n\u2022 There is a non-negligible discrepancy between online\nand offline evaluation because there can often be mul-\ntiple viable plans for completing the same task. Online\nevaluation is more indicative of a model\u2019s true perfor-\nmance.\n2. SeeAct\nIn this section, we first explain the problem formulation of\nweb agents and then introduce SEEACT, a generalist web\nagent based on LMMs. Specifically, given a web-based task\n(e.g., \u201cRent a truck with the lowest rate\u201d in the car rental\nwebsite), we examine two essential capabilities of LMMs as\na generalist web agent: (i) Action Generation to produce\nan action description at each step (e.g., \u201cMove the cursor\nover the \u2018Find Your Truck\u2019 button and perform a click\u201d)\ntowards completing the task, and (ii) Element Grounding\nto identify an HTML element (e.g., \u201c[button] Find Your\nTruck\u201d) at the current step on the webpage.\n2.1. Formulation\nGiven a website S (e.g., a car rental website) and a task\nT (e.g., \u201cRent a truck with the lowest rate\u201d), the web\nagent should generate a sequence of executable actions\nA = [a1, a2, ..., an] to complete the task. Specifically, at\ntime step t, the agent should generate an action at based on\nthe current environment observation st, the previous actions\n{a1, a2, ..., at\u22121}, and the task T:\nat = \u03c0(st, T, {a1, a2, ..., at\u22121})\nThe environment observation st comprises an HTML doc-\nument ht and a screenshot image it. LLMs can only be\ngrounded on the HTML document, while LMMs can be\ngrounded on both the HTML document and the screenshot\nimage. The website status is updated accordingly after each\naction:\nst+1 = S(at) = {ht+1, it+1}\nFor simplicity, in subsequent step-wise formulations, the\ntime step notation t is omitted.\nAn action a corresponds to a browser event provided by the\n2\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTEXT: Find Your Truck\nTYPE: BUTTON\nMove\nthe\ncursor\nover\nthe\n\"Find\nYour\nTruck\"\nbutton\nlocated in the central portion\nof the webpage, just below the\ninput fields for rental details,\nand perform a click action.\nAction Description\nElement Attributes\nImage Annotation\nCHOICE: G\nA\nB\nC\nD\nE\nF\nG\nTextual Choices\nCHOICE: G\nA: <a id=\"0\">Moving Trucks & Accessories</a>\nB: <input type=\"text\" id=\"1\">placeholder=\"US City,State\nor Zip Code\"</input>\n\u2026 \u2026 \u2026 \nF: <input type=\"radio\" id=\"5\">No name=\"one-way-\nradio\"</input>\nG: <input type=\"button\" id=\"6\">value=\"Find Your \nTruck\"</input>\nH: None \nFigure 2: An example of the element grounding process for a single action during completing the given task with three\ndifferent methods. In this action step, the model needs to click the \"Find Your Truck\" button to perform a search. For\ngrounding with textual choices, some element candidates represented with HTML text are given, the model is required to\ngenerate the choice index of the target element. For image annotation, bounding boxes and index labels are added to the\nimage. The model is required to generate the label on the bottom-left of the target element. For grounding with element\nattributes, the model needs to predict the text and type of the target element.\nwebsite environment. Therefore, we formulate an action\nas a triplet of three necessary variables for a browser event\n(e, o, v). e \u2208 E identifies the target webpage element to\noperate on, such as the \"Find Your Truck\" button in Fig-\nure 2. E represents the set of webpage elements within the\nenvironment S. The operation o \u2208 O is the action to be\nperformed on the target element, with O encompassing all\npossible operations in S (e.g., Click, Type). The variable\nv denotes the additional value needed for a certain operation\n(e.g., the date 12/10/2023 for a Type operation).\nHowever, agents based on LLMs or LMMs are typically\nunable to directly generate the three variables (e, o, v) re-\nquired for a browser event. Instead, they generate a textual\ndescription of the intended action \u02dca, containing information\nabout these variables as (\u02dce, \u02dco, \u02dcv). This process is referred to\nas Action Generation. To interact with the environment, a\nfurther step is required to convert \u02dca into a, which we refer\nto as Action Grounding.\n2.2. Action Generation\nWe explicitly instruct GPT-4V to imitate humans browsing\na webpage and analyze the task, webpage, and previous\nactions. It is asked to generate an action description \u02dca based\non its analysis and reasoning. We take the screenshot image\ni as the visual context without utilizing the HTML document\nh for action generation.\n2.3. Action Grounding\nDespite the capability of GPT-4V in identifying and de-\nscribing the next action to complete the given task in nat-\nural language, it is still challenging to convert the action\ndescription \u02dca into an executable action a within the envi-\nronment. Deriving operation type o and value v from the\naction description \u02dca can be solved through string parsing\nreasonably well. The key challenge is to identify the tar-\nget element e from the generated \u02dce, which we refer to as\nElement Grounding.\nTo address this challenge, we explore three approaches us-\ning different types of information: Grounding via Element\nAttributes, Grounding via Textual Choices, and Grounding\nvia Image Annotation, as depicted in Figure 2. The prompt-\ning details of action generation and grounding are included\nin Appendix D.\nGrounding via Element Attributes.\nThis approach in-\nvolves prompting the model to generate as detailed attributes\nof the target element as possible, thereby providing more in-\nformation to precisely match with the target HTML element.\nSpecifically, we prompt the model to not only describe the\nelement e, but also specify the target element\u2019s type and\nthe textual content in \u02dce. For example, as illustrated in Fig-\nure 2, the model would generate element text as \"Find Your\nTruck\" and identify its type as a \"BUTTON.\" Following this,\na heuristic search is performed across the DOM elements,\nusing the element text and type to locate matching elements.\nIn cases where a single match is found, it is automatically se-\nlected. Otherwise, when multiple matches arise, the model\nis further prompted to select the final selection.\n3\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nGrounding via Textual Choices.\nThe above approach\ndemands precise and sufficient attribute descriptions from\nGPT-4V and accurate matching by the heuristic search,\nwhich can be highly demanding. For instance, many ele-\nments may have no textual content or have textual informa-\ntion in a nearby element instead of itself.\nAlternatively, we provide the model with textual representa-\ntions of elements as choices to facilitate grounding, which\nhas already been proven effective in MindAct (Deng et al.,\n2023). Specifically, MindAct utilizes a ranking model to\nselect top-k candidate elements (e1, e2, ..., ek) with a pre-\ntrained cross-encoder. Each candidate element is repre-\nsented as a choice in a multi-choice question with its HTML\ntext, as illustrated in Figure 2. After generating the action\ndescription \u02dca, the model is further asked a multi-choice\nquestion to choose its intended element from the given mul-\ntiple choices (including a \u2018none\u2019 option).\nGrounding via Image Annotation.\nTextual representa-\ntions alone are sometimes insufficient to distinguish similar\nor identical elements, as illustrated in Appendix G. There-\nfore, in this approach, we propose to overlay a bounding box\nfor each candidate element e selected by the ranker as well\nas a label around the bounding box1 with a label assignment\nmethod to avoid overlapping between markups. The model\nis expected to generate the label corresponding to the target\nelement.\nOracle Action Grounding. Ideally, the action description\n\u02dca must encompass all necessary details to precisely identify\neach variable (e, o, v) of the action triplet. To assess the per-\nformance of action generation, an oracle grounding method,\nwhich ensures the variables be identified as long as they are\nmentioned in the action description, is desired. Here we\napproximate the oracle grounding method by asking human\nannotators to identify the model\u2019s intended actions.\n3. Experiments\n3.1. Dataset\nWe evaluate our methods on MIND2WEB (Deng et al.,\n2023), a comprehensive dataset encompassing over 2,000\ncomplex web tasks with annotated actions. This dataset\nspans 137 websites across 31 low-level domains, catego-\nrized into 12 high-level domains. It supports three primary\noperations: Click, Type, and Select, with Hover and\nPress Enter operations integrated into Click to avoid\nambiguity.\nThe dataset\u2019s test sets aim to measure the generalization of\nweb agents across different tasks, websites, and domains.\nSpecifically, the Cross-Task setting focuses on evaluating\n1We use the Supervision library for image annotation: https:\n//supervision.roboflow.com/\nagents on tasks that are new to the training data but within\nincluded domains and websites. The Cross-Website setting\nevaluates agents with tasks across 10 new websites for each\nof the top-level domains in the training data. The Cross-\nDomain setting assesses agent performance on tasks in two\ntop-level domains held out from the training data.\nWe align each HTML document in the dataset with its corre-\nsponding webpage screenshot image from the MIND2WEB\nraw dump, which undergoes human verification to confirm\nelement visibility and correct rendering for action prediction.\nThis cleaned version of the dataset is called Multimodal\nMind2Web2, with the statistics in Table 1.\n3.2. Methods\nSeeAct. In grounding via image annotation and textual\nchoices, we first employ the DeBERTa-base cross-encoder\nfrom MindAct (Deng et al., 2023) to rank the top 50 ele-\nments for better comparison with its text-only counterparts.\nThen, we cluster elements into groups of 17 options for\ninference. In grounding via element attributes, no candidate\nelement is provided. We experiment all three grounding\nmethods with GPT-4V API, and use the best-performing\ngrounding method for Gemini Pro Vision (Anil et al., 2023),\nand LLaVA-1.5 (Liu et al., 2023a;c).\nMindAct. To compare with SEEACT, we also implement\nmethods based on text-only LLMs and BLIP-2 (Li et al.,\n2023) following the two-stage strategy of MindAct (Deng\net al., 2023). Firstly, we employ the ranker above to pick\nthe top 50 elements. Subsequently, the action generation\nproblem is formulated as a multi-choice question answering\nproblem, with the candidate elements as options, including\na \"None\" option if the target element is absent. During\ninference, elements are clustered into groups of 5 elements,\nwith iterative refinement, until a single choice is made or all\noptions are discarded. We evaluate supervised fine-tuning\n(SFT) methods using FLAN-T5 (Chung et al., 2022) and\nBLIP-2-T5 and in-context learning (ICL) methods using\nGPT-3.5 and GPT-4.\nPixel-Level Grounding. LMMs can generate target element\ncoordinates in the image via training on datasets augmented\nwith object coordinates, especially for open-sourced mod-\nels (Hong et al., 2023; Cheng et al., 2024; You et al., 2023).\nWe choose CogAgent (Hong et al., 2023) as a representative\nmodel for this experiment with the ICL method. Details of\neach method can be found in Appendix A.\n3.3. Offline Evaluation\nWe adopt the evaluation metrics utilized in MIND2WEB.\nElement Accuracy (Ele. Acc) compares the predicted el-\n2The dataset is released at https://huggingface.co/\ndatasets/osunlp/Multimodal-Mind2Web.\n4\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable 1: Statistics of the cleaned Multimodal Mind2Web dataset. The average # of visual tokens is based on OpenAI visual\ntoken calculator.\nSplit\n# Tasks\n# Domains\n# Websites\nAvg #\nAvg #\nAvg # HTML\nActions\nVisual Tokens\nElements\nTokens\nTrain\n1,009\n17\n73\n7.7\n4,240\n602\n128,827\nCross-Domain\n694\n13\n53\n5.9\n4,314\n494\n91,163\nCross-Task\n177\n17\n64\n7.6\n4,172\n607\n123,274\nCross-Website\n142\n9\n10\n7.2\n4,653\n612\n114,358\nement with the ground-truth elements. Operation F1 (Op.\nF1) calculates the token-level F1 score for the predicted\noperation comprised of action and input value. Step Suc-\ncess Rate (Step SR) measures the success of each action\nstep. A step is successful only if the selected element and\nthe predicted operation are correct. We report macro av-\nerages across tasks for these step-wise metrics. Success\nRate (SR) measures the success of an entire task. A task is\nregarded successful only if all steps have succeeded. This\nmetric is stringent without allowing the model any space\nfor exploration and error correction. Therefore, for offline\nevaluation, we focus on the first three metrics. However, we\nalso conduct online evaluation on live websites for better\nevaluation on the whole task success rate, as detailed below.\n3.4. Online Evaluation\nWe develop a new online evaluation tool using Playwright3\nto evaluate web agents on live websites (instead of cached\nwebsites in offline evaluation). Our tool can efficiently\ntunnel multimodal inputs from the browser to the agent and\nconvert the predicted action (e, o, v) into a browser event for\nexecution. To adhere to ethical standards, our experiments\nare restricted to non-login tasks in compliance with user\nagreements, and we closely monitor agent activities during\nonline evaluation to prevent any actions that have potentially\nharmful impacts, like placing an order or modifying the user\nprofile.\n4. Results and Analysis\n4.1. Offline Evaluation Results\nGPT-4V can be a Generalist Web Agent with Oracle\nAction Grounding.\nGiven an effective action ground-\ning method, GPT-4V has the potential to serve as a gen-\neralist web agent.\nSpecifically, as described in subsec-\ntion 2.3, we provide GPT-4V with an oracle action ground-\ning method (SEEACTOracle) through human annotation, the\nmodel achieves a step success rate of 61.9%, 65.0%, and\n62.1% across three test splits, respectively. As shown in\nTable 2, this method substantially outperforms other mod-\nels under all metrics across three test splits. Specifically,\n3https://playwright.dev/\nit achieves a 8.4% step success rate improvement over the\nsecond-best method in the Cross-Task setting. The per-\nformance advantage is more pronounced under the Cross-\nWebsite and Cross-Domain settings, where it leads by 23.9%\nand 23.2% step success rates, demonstrating its generality\ncompared with supervised fine-tuning. This observation is\nfurther corroborated within the online evaluation (Table 4).\nElement Grounding Method Comparison.\nHowever,\nthere is a noticeable gap between oracle grounding and\nall three proposed grounding methods, as shown in Ta-\nble 3. This demonstrates that grounding, especially element\ngrounding, is a major bottleneck. Element grounding via\ntextual choice (SEEACTChoice) demonstrates the best perfor-\nmance under all metrics across all settings, comparable to\nsupervised fine-tuning and showing a substantial improve-\nment over text-only LLMs.\nGrounding via image annotation (SEEACTAnnotation) offers\nan intuitive approach and shows promising results in recent\nwork that focuses on object- or scene-centric images (Yang\net al., 2023a). However, we find that on complex images\nwith rich semantic and spatial relationships like webpage\nscreenshots, severe hallucination is observed from GPT-4V.\nSpecifically, it often fails to correctly map its generated ele-\nment description (which is often correct according to oracle\ngrounding) to the right bounding box and index label in the\nimage, leading to a low element accuracy. This limitation\nprimarily arises from GPT-4V\u2019s weakness in understanding\nimage details and relative spatial location, a topic that we\nwill further delve into in Appendix E. We leverage bottom-\nleft number labels around bounding boxes as it is identified\nas the optimal markups in the ablation study in Appendix B.\nGrounding via element attributes (SEEACTAttribute) also\ndemonstrates inferior performance. This method\u2019s effec-\ntiveness is primarily limited by its heuristic-based element\nlocalization strategy, which depends on textual and locality\ncharacteristics. This becomes problematic as not all web-\npage elements contain text, and sometimes the relevant text\nis associated with a nearby but distinct element.\nLMMs vs. LLMs. The SEEACTChoice with GPT-4V demon-\nstrates a substantial performance advantage over the text-\nonly GPT-4 under all three metrics across all three test splits.\nSpecifically, it outperforms GPT-4 in step success rate of\n5\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable 2: Performance of different models. All models under SEEACT utilize \u201cChoices\u201d for grounding. Methods with * mark\nare conducted on a subset with 30 tasks for each task split.\nModel\nCross-Task\nCross-Website\nCross-Domain\nEle. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR Ele. Acc Op. F1 Step SR\nSupervised Fine-Tuning\nFLAN-T5-XL\n57.1\n75.7\n53.5\n43.8\n67.7\n41.1\n41.4\n65.9\n38.9\nBLIP-2-T5-XL\n50.1\n77.0\n47.0\n39.4\n69.3\n37.0\n41.2\n69.3\n38.9\nIn-Context Learning\nGPT-3.5*\n19.4\n59.2\n16.8\n14.9\n56.5\n14.1\n25.2\n57.9\n24.1\nGPT-4*\n40.8\n63.1\n32.3\n30.2\n61.0\n27.0\n35.4\n61.9\n29.7\nCOGAGENT\n22.4\n53.0\n17.6\n18.4\n42.2\n13.4\n20.6\n42.0\n15.5\nSEEACT\n\u2013 LLAVA-1.5\n9.7\n65.6\n8.1\n9.1\n60.8\n7.5\n10.9\n63.9\n8.5\n\u2013 GEMINI PRO VISION\n21.5\n67.7\n19.6\n17.1\n61.3\n15.4\n20.7\n64.3\n18.0\n\u2013 GPT-4V\n46.4\n73.4\n40.2\n38.0\n67.8\n32.4\n42.4\n69.3\n36.8\n\u2013 GPT-4V-Oracle*\n66.4\n79.2\n61.9\n69.5\n78.9\n65.0\n72.8\n73.6\n62.1\nTable 3: Step success rate (%) of GPT-4V on a subset of 30\ntasks for each task split with different grounding methods.\n\u201cAttributes\u201d, \u201cChoices\u201d, \u201cAnnotation\u201d, and \u201cOracle\u201d refer to\nelement grounding via Element Attributes, Textual Choices,\nImage Annotation, and Human Annotation, respectively, as\ndescribed in subsection 2.3.\nGrounding\nCross-Task Cross-Website Cross-Domain\nSEEACTAttribute\n16.1\n12.1\n19.0\nSEEACTAnnotation\n20.3\n13.9\n23.7\nSEEACTChoice\n39.1\n32.7\n42.0\nSEEACTOracle\n61.9\n65.0\n62.1\n6.8%, 5.7%, and 12.3% on three settings, respectively. Inter-\nestingly, fine-tuned BLIP-2-T5 does not show a noticeable\ngain over FLAN-T5, despite having additional visual input.\nSeveral factors may contribute to this. First, the CLIP model\nused as the image encoder may not be sufficiently adept at\nimage details, as explored by Shen et al. (2021). This lim-\nitation is particularly relevant for our web navigation task,\nwhich demands a high level of image detail comprehension.\nSecond, BLIP-2-T5 utilizes an off-the-shelf CLIP model\nthat may not be optimal for webpage screenshots. Finally, al-\nthough the screenshots in the test splits are error-free, some\nof the examples in the training set might contain issues\nsuch as rendering failures or inaccuracies when annotators\ncapture the screenshots.\nSFT vs. ICL. We compare SFT and ICL methods to offer\ninsights for developing web agents in different scenarios.\nICL (with SEEACT) demonstrates consistent and robust per-\nformance across three test splits. ICL is particularly advan-\ntageous in scenarios lacking annotations or requiring strong\ngeneralization capabilities for new domains and websites.\nAs grounding methods improve towards oracle grounding,\nTable 4: Whole task success rate (%) under both offline\nand online evaluation. Offline0 and Offline1 refer to no\ntolerance for error at any step and allowing for error at one\nstep, respectively.\nOffline0\nOffline1\nOnline\nFLAN-T5-XL\n4.4\n24.4\n8.9\nGPT-4\n1.1\n12.2\n13.3\nSEEACTChoice\n3.3\n12.2\n37.8\nSEEACTOracle\n13.3\n27.8\n51.1\nICL is poised to show even stronger performance. On the\nother hand, SFT methods show better generalization across\ntasks on websites already seen during training. Consider-\ning the high cost of data annotation for web agents and\nthe billions of websites on the Internet, ICL offers a more\ncompelling solution for generalist web agents. However, if\none only needs to develop a strong web agent for a certain\nwebsite, SFT is still a competitive solution.\n4.2. Online Evaluation Results\nIn online evaluation, we pair a web agent with a human\nannotator, where the human was tasked to monitor agent\nactions that may change real-world states and determine\nwhether each task was successfully completed. For compar-\native analysis, we include success rates from offline evalua-\ntion, denoted as Offline0 (allowing zero wrong action) and\nOffline1 (allowing one wrong action). For a fair comparison\nbetween offline and online evaluations, we only re-write\ntime-sensitive tasks to ensure they are still valid when the\nevaluation is conducted. For instance, we update the dates\nfor flight-related tasks. Finally, we conduct the online eval-\nuation on the same subset of total 90 tasks from the three\ntest splits. For tasks invalidated due to website change, we\n6\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nEasy\nMedium\nHard\n0\n10\n20\n30\n40\n50\n60\n70\nSuccess Rate\nSeeAct-Oracle\nSeeAct-Choices\nGPT-4\nFLAN-T5-XL\nFigure 3: Whole task success rate across task difficulty\nlevels. We categorize tasks based on the number of actions\nto complete, i.e., Easy: 1-4, Medium: 5-9, and Hard: 10-18,\nwith 37, 35, and 18 tasks in each group, respectively.\nresample from the corresponding test split.\nTable 4 shows that the whole task success rate in online\nevaluation substantially exceeds that of offline evaluation\n(Offline0). This finding suggests that the whole task success\nrate is likely underestimated in the offline evaluation due to\nthe variability in actions and plans. In other words, there\nmay be multiple viable plans for a task, but the reference\nplan in offline evaluation only captures one of them.\nAcross all three settings, SEEACTChoice with GPT-4V outper-\nforms both GPT-4 and FLAN-T5-XL by a large margin of\nover 20% whole task success rate. Using oracle grounding\nfurther improves the performance substantially, reaching\na remarkable whole task success rate of 51.1%. Although\nGPT-4 shows much worse performance than FLAN-T5-XL\nin step success rate under offline evaluation (Table 2), it out-\nperforms FLAN-T5-XL by 4.4% whole task success rate in\nthe online evaluation. These results further confirm the po-\ntential of large models for generalist web agents compared\nwith fine-tuned small models.\n4.3. Analysis\nOnline Success Rate by Task Difficulty. We investigate\nthe performance of web agents on tasks across different\ndifficulty levels. We estimate the task difficulty based on the\nnumber of actions taken by annotators during action trace\nannotation. As shown in Figure 3, the whole task success\nrate is negatively correlated with the number of actions\u2014\nit decreases as the number of actions increases across all\nfour methods. SEEACTOracle consistently outperforms other\nmethods across all difficulty levels. Interestingly, the gap be-\ntween SEEACTOracle and SEEACTChoice enlarges on longer-\nhorizon tasks. This is understandable because grounding\nerrors cascade to later steps; nonetheless, it further shows\nthe challenge of grounding for GPT-4V and the need for\nbetter grounding methods.\nError Analysis in Grounding via Image Annotation.\nSet-of-mark prompting (Yang et al., 2023a) uses a simi-\nlar method as grounding via image annotation and has been\nshown effective on object- or scene-centric images (Lin\net al., 2014; Plummer et al., 2015; Zhou et al., 2017). How-\never, this grounding method is suboptimal on webpage\nscreenshot images that are complex and contain rich se-\nmantic and spatial relationships. To analyze the reasons\nbehind the failures, we randomly sample 100 action predic-\ntions with correct action generation but wrong grounding\nresults. We observes major types of errors as : (1) Making\nup bounding box & label; (2) Failure to link bounding boxes\nwith the correct labels. Illustrative examples are included in\nAppendix E.\nOur analysis reveals that 54% of the errors can be attributed\nto GPT-4V\u2019s tendency of visual illusion (Guan et al., 2023),\nwhere the model misinterprets and fabricates content over\nthe image. Specifically, the target element described in ac-\ntion generation does not have a bounding box or a label on\nthe bottom-left, where the model is supposed to generate\n\"NA\". However, the model falsely assumes the presence of\na bounding box and makes up a label as the answer. Another\n46% of errors are caused by GPT-4V\u2019s limitation in recog-\nnizing the relative position within an image. Specifically,\nthe model is capable of identifying the target element within\nthe bounding box. However, it struggles to correctly link\nthe bounding box with its corresponding label.\n4.4. Case Study\nGPT-4V exhibits promising capabilities, ranging from spec-\nulative planning, webpage content reasoning, and error cor-\nrection to surpassing the limitations of superficial textual\nsimilarity matching inherent in fine-tuned, text-only models.\nWorld Knowledge. GPT-4V demonstrates substantial ad-\nvantages in tasks requiring certain knowledge over fine-\ntuned models at a smaller scale. As shown in Appendix H,\nGPT-4V is able to identify the IATA code of the airport in\nLos Cabos as SJD. In contrast, smaller models are typically\nweaker at knowledge-intensive tasks and are also likely to\nlose knowledge during the fine-tuning process due to catas-\ntrophic forgetting.\nWorld Model (for Websites). GPT-4V exhibits the po-\ntential of a \"world model\" for websites. As shown in Ap-\npendix F, GPT-4V can predict the state transitions on a\nwebsite (e.g., what would happen if I clicked this button).\nBased on its awareness of website state transitions, GPT-4V\ncan conduct speculative planning involving a sequence of\nsubsequent actions in the future to complete the given task.\nError Correction Awareness. GPT-4V also exhibits the\n7\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nawareness of error correction in the previous actions. In the\nexample in Appendix I, it realizes that the mobile phone\nnumber is invalid due to the wrong format and generates the\ndescription of the action to correct this error. This highlights\nthe model\u2019s potential for adaptation in online settings, where\nactions may not always follow pre-defined, ideal paths as in\noffline evaluations. This capability paves the way for adding\nrobustness and reasonable dynamic planning.\n5. Related Work\nWeb Agent. Many works have focused on improving web\nagents relying on the HTML document (Deng et al., 2023;\nGur et al., 2023; 2022; 2023; Kim et al., 2023; Sridhar\net al., 2023).\nHowever, a raw HTML document is of-\nten massive making it infeasible or cost-prohibitively to\nfeed into LLMs directly.\nMindAct (Deng et al., 2023)\ninstead employs a small language model to rank each\nHTML element and selectively consider top elements as\nthe context.\nWebAgent (Gur et al., 2023) proposes an\nenhanced planning strategy by summarizing the HTML\ndocuments and decomposing the instruction into multiple\nsub-instructions. Another stream considers visual informa-\ntion for web agents (Shaw et al., 2023; Furuta et al., 2023;\nHong et al., 2023). Pix2Act (Shaw et al., 2023) leverages\nPix2Struct (Lee et al., 2022) to parse screenshot images into\nsimplified HTML to complete GUI-based tasks (Shaw et al.,\n2023; Liu et al., 2018; Shi et al., 2017; Mazumder & Riva,\n2020; Yao et al., 2022). WebGUM (Furuta et al., 2023) and\nCogAgent (Hong et al., 2023) pre-train an LMM with mas-\nsive screenshot-HTML data to enhance its decision-making\non real-world web navigation like Mind2Web. While all\nthese prior works show promise, generalizing to various\nweb environments remains a challenge for existing models.\nThus, SEEACT explores recently released, more powerful\nLMMs such as GPT-4V and Gemini, to demonstrate their\npotential as generalist web agents with comprehensive on-\nline and offline evaluation and analysis. In a concurrent\nwork (Yan et al., 2023), GPT-4V exhibits strong perfor-\nmance on mobile UI understanding, which is less complex\nthan the desktop websites we study.\nLarge Multimodal Models. GPT-4V (OpenAI, 2023) and\nGemini (Anil et al., 2023) represent significant progress\nin LMMs. Several studies (Akter et al., 2023; OpenAI,\n2023; Yang et al., 2023c; Zhang et al., 2023; Yang et al.,\n2023a; Yan et al., 2023) have highlighted their remarkable\nmultimodal capabilities, emphasizing the advanced and ver-\nsatile integration of visual and language reasoning abilities.\nTheir performance on a series of benchmarks (Kazemzadeh\net al., 2014; Goyal et al., 2016; Hendrycks et al., 2020;\nSaikh et al., 2022; Lu et al., 2022; Zhong et al., 2023;\nYue et al., 2023) also showcases remarkable capabilities\non vision-and-language understanding and reasoning. Al-\nthough open-sourced models still exhibit a performance gap\nwith GPT-4V, they have the advantages of controllability\nand ease of fine-tuning for various applications. For exam-\nple, in CogAgent (Hong et al., 2023), LMMs are fine-tuned\non HTML and screenshot image pairs to enhance webpage\nunderstanding ability and further enhanced with an image\nencoder for high-resolution image details. Ferret (You et al.,\n2023) is finetuned to allow visual referring and grounding.\nVisual Grounding. Despite LMMs having achieved remark-\nable vision-language understanding capabilities, they still\nface challenges in fine-grained visual grounding. Various vi-\nsual prompting (Shtedritski et al., 2023; Yang et al., 2023b;c;\nYan et al., 2023) methods have been proposed to augment\nGPT-4V\u2019s image detail grounding ability by overlaying\nvisual marks onto the image. SoM (Yang et al., 2023a) in-\nvolves segmenting the image into semantically meaningful\nregions and overlaying an array of visual marks like num-\nbers, alphabets, masks, or bounding boxes. Fine-tuning\nvision-language models with image-annotated data is effec-\ntive. Kosmos-2 (Peng et al., 2023) represents bounding box\nlocations through textual location tokens. BuboGPT (Zhao\net al., 2023) extract entities and find corresponding masks\nfor objects in the image. Shikra (Chen et al., 2023) handles\nimage detail referring and grounding by applying spatial\ncoordinates as text tokens in inputs and outputs, respec-\ntively. Ferret (You et al., 2023) represents regions with both\ndiscrete coordinates and continuous features along with a\nspatial-aware visual sampler to handle diverse spatial char-\nacteristics across various shapes.\n6. Conclusion\nIn this work, we developed SEEACT, a generalist web\nagent that harnesses the power of large multimodal models\n(LMMs) like GPT-4V to integrate visual understanding and\nacting on the web. We showed that LMMs present a great\npromise for generalist web agents, with a success rate of\n50% on live websites given an oracle grounding method.\nGPT-4V also exhibits impressive capabilities, such as er-\nror correction and speculative planning. However, fine-\ngrained visual grounding is still a major challenge. The\nmost effective grounding strategies we explored in this paper\nstill exhibit a 20-25% performance gap compared to oracle\ngrounding. Future work should better leverage the unique\nproperties of the Web, e.g., the known correspondence be-\ntween HTML and visual elements, for improving grounding\nand reducing hallucinations from LMMs. Furthermore, we\nshow a significant discrepancy between online and offline\nevaluations, emphasizing the importance of online evalu-\nation for an accurate assessment of a model\u2019s capabilities.\nThis discrepancy is largely due to the variability in potential\nplans for completing the same task, pointing to the dynamic\nnature of web interactions.\n8\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n7. Impact Statements\nGeneralist web agents hold the potential to automate rou-\ntine web tasks, enhance user experiences, and promote web\naccessibility, safety concerns related to their real-world de-\nployment are also critical. These concerns span privacy\nissues, such as access to users\u2019 personal profiles, and sensi-\ntive operations, such as financial transactions or application\nform submissions. During the online evaluation, we noticed\nthe possibility for these web agents to generate harmful\nactions on the web, and we manually validated the safety\nof all the actions before execution. It is critical for further\nresearch to thoroughly assess and mitigate the safety risks\nassociated with web agents, ensuring they are safeguarded\nagainst producing and executing harmful actions. The code\nwill also be released solely for research purposes, with the\ngoal of making the web more accessible via language tech-\nnologies under an OPEN-RAIL License. We are strongly\nagainst any potentially harmful use of the data or technology\nby any party.\nAcknowledgments\nThe authors would like to thank colleagues from the OSU\nNLP group for their thoughtful comments. This research\nwas supported in part by ARL W911NF2220144 and Cisco.\nReferences\nAkter, S. N., Yu, Z., Muhamed, A., Ou, T., Bauerle, A.,\nCabrera, \u00c1. A., Dholakia, K., Xiong, C., and Neubig,\nG.\nAn in-depth look at gemini\u2019s language abilities.\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:266359502.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr,\nI., Hasson, Y., Lenc, K., Mensch, A., Millican, K.,\nReynolds, M., Ring, R., Rutherford, E., Cabi, S., Han,\nT., Gong, Z., Samangooei, S., Monteiro, M., Menick, J.,\nBorgeaud, S., Brock, A., Nematzadeh, A., Sharifzadeh,\nS., Binkowski, M., Barreira, R., Vinyals, O., Zisserman,\nA., and Simonyan, K.\nFlamingo: a visual language\nmodel for few-shot learning. ArXiv, abs/2204.14198,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:248476411.\nAnil, G. T. G. R., Borgeaud, S., and et al., Y. W. Gem-\nini: A family of highly capable multimodal models.\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:266361876.\nChandu, K. R., Bisk, Y., and Black, A. W. Grounding\n\u2018grounding\u2019in nlp. In Findings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021, pp. 4283\u2013\n4305, 2021.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F.,\nand Zhao, R.\nShikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic.\nArXiv, abs/2306.15195,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259262082.\nCheng,\nK.,\nSun,\nQ.,\nChu,\nY.,\nXu,\nF.,\nLi,\nY.,\nZhang,\nJ.,\nand\nWu,\nZ.\nSeeclick:\nHarness-\ning gui grounding for advanced visual gui agents.\n2024.\nURL https://api.semanticscholar.\norg/CorpusID:267069082.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y.,\nFedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S.,\nWebson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,\nChowdhery, A., Valter, D., Narang, S., Mishra, G., Yu,\nA. W., Zhao, V., Huang, Y., Dai, A. M., Yu, H., Petrov,\nS., hsin Chi, E. H., Dean, J., Devlin, J., Roberts, A.,\nZhou, D., Le, Q. V., and Wei, J. Scaling instruction-\nfinetuned language models.\nArXiv, abs/2210.11416,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:253018554.\nDeng, X., Gu, Y., Zheng, B., Chen, S., Stevens, S., Wang,\nB., Sun, H., and Su, Y. Mind2web: Towards a generalist\nagent for the web.\narXiv preprint arXiv:2306.06070,\n2023.\nFuruta, H., Nachum, O., Lee, K.-H., Matsuo, Y., Gu, S. S.,\nand Gur, I. Multimodal web navigation with instruction-\nfinetuned foundation models. ArXiv, abs/2305.11854,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258823350.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answering.\nInternational Journal of Computer Vision, 127:398 \u2013 414,\n2016.\nURL https://api.semanticscholar.\norg/CorpusID:8081284.\nGu, Y., Deng, X., and Su, Y. Don\u2019t generate, discrimi-\nnate: A proposal for grounding language models to real-\nworld environments. In Rogers, A., Boyd-Graber, J., and\nOkazaki, N. (eds.), Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pp. 4928\u20134949, Toronto, Canada,\nJuly 2023. Association for Computational Linguistics.\ndoi: 10.18653/v1/2023.acl-long.270.\nURL https:\n//aclanthology.org/2023.acl-long.270.\nGuan, T., Liu, F., Wu, X., Xian, R., Li, Z., Liu,\nX., Wang, X., Chen, L., Huang, F., Yacoob, Y.,\nManocha, D., and Zhou, T. Hallusionbench: An ad-\nvanced diagnostic suite for entangled language halluci-\nnation&visual illusion in large vision-language models.\n9\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265499116.\nGur, I., Nachum, O., Miao, Y., Safdari, M., Huang,\nA., Chowdhery, A., Narang, S., Fiedel, N., and\nFaust, A.\nUnderstanding html with large language\nmodels.\nIn Conference on Empirical Methods in\nNatural Language Processing, 2022.\nURL https:\n//api.semanticscholar.org/CorpusID:\n252780086.\nGur, I., Furuta, H., Huang, A., Safdari, M., Mat-\nsuo, Y., Eck, D., and Faust, A.\nA real-world\nwebagent with planning, long context understand-\ning, and program synthesis.\nArXiv, abs/2307.12856,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:260126067.\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\nSong, D. X., and Steinhardt, J. Measuring massive multi-\ntask language understanding. ArXiv, abs/2009.03300,\n2020.\nURL https://api.semanticscholar.\norg/CorpusID:221516475.\nHong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J.,\nWang, Y., Wang, Z., Dong, Y., Ding, M., and Tang,\nJ. Cogagent: A visual language model for gui agents.\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:266210390.\nKazemzadeh, S., Ordonez, V., andre Matten, M., and\nBerg, T. L.\nReferitgame:\nReferring to objects in\nphotographs of natural scenes.\nIn Conference on\nEmpirical Methods in Natural Language Processing,\n2014.\nURL https://api.semanticscholar.\norg/CorpusID:6308361.\nKim, G., Baldi, P., and McAleer, S. M. Language mod-\nels can solve computer tasks. ArXiv, abs/2303.17491,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:257834038.\nKoh, J. Y., Lo, R., Jang, L., Duvvur, V., Lim, M. C.,\nHuang, P.-Y., Neubig, G., Zhou, S., Salakhutdi-\nnov, R., and Fried, D.\nVisualwebarena:\nEvaluat-\ning multimodal agents on realistic visual web tasks.\n2024.\nURL https://api.semanticscholar.\norg/CorpusID:267199749.\nLee, K., Joshi, M., Turc, I., Hu, H., Liu, F., Eisen-\nschlos, J. M., Khandelwal, U., Shaw, P., Chang,\nM.-W., and Toutanova, K.\nPix2struct:\nScreenshot\nparsing as pretraining for visual language understand-\ning.\nArXiv, abs/2210.03347, 2022.\nURL https:\n//api.semanticscholar.org/CorpusID:\n252762394.\nLi, J., Li, D., Savarese, S., and Hoi, S. C. H.\nBlip-\n2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language mod-\nels.\nArXiv, abs/2301.12597, 2023.\nURL https:\n//api.semanticscholar.org/CorpusID:\n256390509.\nLin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P.,\nRamanan, D., Doll\u00e1r, P., and Zitnick, C. L. Microsoft\ncoco: Common objects in context. In European Confer-\nence on Computer Vision, 2014. URL https://api.\nsemanticscholar.org/CorpusID:14113767.\nLiu, E. Z., Guu, K., Pasupat, P., Shi, T., and Liang, P. Rein-\nforcement learning on web interfaces using workflow-\nguided exploration.\nIn International Conference on\nLearning Representations (ICLR), 2018. URL https:\n//arxiv.org/abs/1802.08802.\nLiu, H., Li, C., Li, Y., and Lee, Y. J. Improved baselines\nwith visual instruction tuning, 2023a.\nLiu,\nH.,\nLi,\nC.,\nWu,\nQ.,\nand Lee,\nY. J.\nVi-\nsual instruction tuning.\nArXiv,\nabs/2304.08485,\n2023b.\nURL https://api.semanticscholar.\norg/CorpusID:258179774.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning, 2023c.\nLiu, X., Yu, H., Zhang, H., Xu, Y., Lei, X., Lai, H., Gu, Y.,\nGu, Y., Ding, H., Men, K., Yang, K., Zhang, S., Deng,\nX., Zeng, A., Du, Z., Zhang, C., Shen, S., Zhang, T., Su,\nY., Sun, H., Huang, M., Dong, Y., and Tang, J. Agent-\nbench: Evaluating llms as agents. ArXiv, abs/2308.03688,\n2023d.\nURL https://api.semanticscholar.\norg/CorpusID:260682249.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering.\nArXiv, abs/2209.09513,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:252383606.\nMazumder, S. and Riva, O. Flin: A flexible natural language\ninterface for web navigation. ArXiv, abs/2010.12844,\n2020.\nURL https://api.semanticscholar.\norg/CorpusID:225067907.\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:257532815.\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F. Kosmos-2: Grounding multimodal large\nlanguage models to the world. ArXiv, abs/2306.14824,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259262263.\n10\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nPlummer, B. A., Wang, L., Cervantes, C. M., Caicedo, J. C.,\nHockenmaier, J., and Lazebnik, S. Flickr30k entities:\nCollecting region-to-phrase correspondences for richer\nimage-to-sentence models. International Journal of Com-\nputer Vision, 123:74 \u2013 93, 2015. URL https://api.\nsemanticscholar.org/CorpusID:6941275.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nSaikh, T., Ghosal, T., Mittal, A., Ekbal, A., and\nBhattacharyya, P.\nScienceqa: a novel resource for\nquestion answering on scholarly articles.\nInterna-\ntional Journal on Digital Libraries, 23:289 \u2013 301,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:250729995.\nShaw, P., Joshi, M., Cohan, J., Berant, J., Pasupat, P.,\nHu, H., Khandelwal, U., Lee, K., and Toutanova, K.\nFrom pixels to ui actions: Learning to follow instructions\nvia graphical user interfaces. ArXiv, abs/2306.00245,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258999511.\nShen, S., Li, L. H., Tan, H., Bansal, M., Rohrbach, A.,\nChang, K.-W., Yao, Z., and Keutzer, K. How much can\nclip benefit vision-and-language tasks? arXiv preprint\narXiv:2107.06383, 2021.\nShi, T., Karpathy, A., Fan, L. J., Hern\u00e1ndez, J. Z., and\nLiang, P.\nWorld of bits: An open-domain platform\nfor web-based agents.\nIn International Conference\non Machine Learning, 2017.\nURL https://api.\nsemanticscholar.org/CorpusID:34953552.\nShtedritski, A., Rupprecht, C., and Vedaldi, A.\nWhat\ndoes clip know about a red circle?\nvisual prompt\nengineering\nfor\nvlms.\nArXiv,\nabs/2304.06712,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258108138.\nSridhar, A., Lo, R., Xu, F. F., Zhu, H., and Zhou,\nS.\nHierarchical prompting assists large language\nmodel on web navigation.\nArXiv, abs/2305.14257,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258841249.\nYan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J.,\nYang, J., Zhong, Y., McAuley, J., Gao, J., Liu, Z.,\nand Wang, L.\nGpt-4v in wonderland: Large multi-\nmodal models for zero-shot smartphone gui navigation.\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265149992.\nYang, J., Zhang, H., Li, F., Zou, X., yue Li, C., and\nGao, J.\nSet-of-mark prompting unleashes extraordi-\nnary visual grounding in gpt-4v. ArXiv, abs/2310.11441,\n2023a.\nURL https://api.semanticscholar.\norg/CorpusID:264172201.\nYang, L., Wang, Y., Li, X., Wang, X., and Yang, J.\nFine-grained visual prompting. ArXiv, abs/2306.04356,\n2023b.\nURL https://api.semanticscholar.\norg/CorpusID:259096008.\nYang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z.,\nand Wang, L.\nThe dawn of lmms: Preliminary ex-\nplorations with gpt-4v(ision). ArXiv, abs/2309.17421,\n2023c.\nURL https://api.semanticscholar.\norg/CorpusID:263310951.\nYao, S., Chen, H., Yang, J., and Narasimhan, K. Web-\nshop: Towards scalable real-world web interaction with\ngrounded language agents.\nArXiv, abs/2207.01206,\n2022.\nURL https://api.semanticscholar.\norg/CorpusID:250264533.\nYou,\nH.,\nZhang,\nH.,\nGan,\nZ.,\nDu,\nX.,\nZhang,\nB., Wang, Z., Cao, L., Chang, S.-F., and Yang,\nY.\nFerret:\nRefer\nand\nground\nanything\nany-\nwhere at any granularity.\nArXiv, abs/2310.07704,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:263834718.\nYue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G.,\nStevens, S., Jiang, D., Ren, W., Sun, Y., Wei, C., Yu, B.,\nYuan, R., Sun, R., Yin, M., Zheng, B., Yang, Z., Liu, Y.,\nHuang, W., Sun, H., Su, Y., and Chen, W. Mmmu: A mas-\nsive multi-discipline multimodal understanding and rea-\nsoning benchmark for expert agi. ArXiv, abs/2311.16502,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:265466525.\nZhang, X., Lu, Y., Wang, W., Yan, A., Yan, J., Qin,\nL., Wang, H., Yan, X., Wang, W. Y., and Pet-\nzold, L. R.\nGpt-4v(ision) as a generalist evaluator\nfor vision-language tasks.\nArXiv, abs/2311.01361,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:264935635.\nZhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J.,\nand Kang, B.\nBubogpt:\nEnabling visual ground-\ning in multi-modal llms.\nArXiv, abs/2307.08581,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:259937702.\nZhong, W., Cui, R., Guo, Y., Liang, Y., Lu, S.,\nWang, Y., Saied, A. S. S., Chen, W., and Duan,\nN.\nAgieval: A human-centric benchmark for eval-\nuating foundation models.\nArXiv, abs/2304.06364,\n2023.\nURL https://api.semanticscholar.\norg/CorpusID:258108259.\n11\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nZhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso,\nA., and Torralba, A.\nScene parsing through ade20k\ndataset.\n2017 IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 5122\u20135130,\n2017.\nURL https://api.semanticscholar.\norg/CorpusID:5636055.\n12\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable of Content:\n1. Appendix A: Offline Experiments Method Details\n2. Appendix B: Markup Type Ablation Study\n3. Appendix C: Online Experiment Details\n4. Appendix D:Offline Experiment Prompts\n5. Appendix E: Error Examples for Grounding via Image Annotation\n6. Appendix F: Strong Capability of Planning\n7. Appendix G: Challenges in Grounding via Textual Choices\n8. Appendix H: Knowledge and Reasoning Requirements\n9. Appendix I: Path Variation and Awareness of Error Correction\n13\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nA. Offline Experiments Method Details\nFLAN-T5. We fine-tune FLAN-T5 using a left-to-right language modeling objective with the target sequence of ground-truth\nactions in the Mind2Web training data. The fine-tuned FLAN-T5 then serves as the backbone for inference, enabling action\ngeneration in the target format for parsing.\nBLIP-2-T5. The BLIP-2 model combines a vision encoder and an LLM with a bridging component for modality connection.\nWe jointly fine-tune the LLM and the bridge module on MIND2WEB training data while keeping the vision encoder frozen.\nFor the vision encoder, we leverage the ViT-L/14 pre-trained from CLIP (Radford et al., 2021) with an image resolution of\n2,048. To ensure a fair comparison with the FLAN-T5-based text-only model, we choose FLAN-T5 as the language model\nand initialize it with the parameters fine-tuned on MIND2WEB.\nGPT-3.5 and GPT-4. We also conduct experiments with text-only LLMs, specifically GPT-3.5-turbo-0613 and GPT-4-\nturbo-1106-preview, using in-context learning in 3-shot settings. We use the same multiple-choice formulation and include\nthree demonstration examples for in-context learning as specified in MindAct.\nSeeAct We experiment with GPT-4-vision-preview, Gemini Pro Vision, LLaVA-1.5. Gemini Pro Vision supports only\nsingle-turn conversations; therefore, we merge the two turns used in other models for compatibility.\nCogAgent We utilize the cogagent-chat-hf checkpoint that hasn\u2019t been fine-tuned on Mind2Web for experiments.\nB. Markup Type Ablation Study\nThe markup types might influence model performance as shown in Yang et al. (2023a); Yan et al. (2023); Koh et al.\n(2024). We first tested grounding via image annotation through different types of text labels of numerical value, single-digit\ncharacters, and two-digit characters, at two different positions to choose a relatively better markup type. The results are\nshown in Table 5.\nTable 5: Grounding via Image Annotation with different markup types and locations. Method with * mark means the\nannotated image is used in action generation.\nLabel\nLocation\nEle. Acc Op. F1 Step SR\nNumber\nBottom-Left\n27.0\n73.7\n24.3\nBottom-Center\n23.0\n76.4\n21.8\nSingle Letter\nBottom-Left\n19.4\n81.0\n17.2\nBottom-Center\n19.7\n78.8\n19.7\nDouble Letter Bottom-Left\n19.8\n68.3\n18.3\nBottom-Center\n22.4\n74.6\n22.4\nNUMBER*\nBottom-Left*\n26.6\n73.9\n22.3\nC. Online Experiment Details\nWe develop an online evaluation tool using Playwright to load webpages, acquire textual representation of interactive\nelements, and perform operations generated by web agents. We manually monitor each step of the model and assess whether\nit finishes the tasks. Attempts to log in, make final submissions, or perform other potentially harmful actions are prohibited\nto avoid negative consequences.\nMindAct. We adhere strictly to the original settings in MindAct-FLAN-T5 and MindAct-GPT-4, as in offline experiment.\nFor consistency with the MindAct framework and models, we use the scripts from Mind2Web for processing webpages,\nelements, and generating options, employing the same action space: Click, Type, and Select.\nSEEACTOracle. In the oracle setting, we manually implement the model\u2019s intended actions. The action history is automatically\ngenerated by the model, with an added requirement to summarize actions in the \"Element\", \"Operation\", \"Value\" format in\nanother turn conversation. To avoid overly long screenshots, we use screenshots of current views, and hence allow intentions\nof scrolling.\nSEEACTChoice. For a relatively fair comparison, we still adopt the ranker and top-50 candidate setting, batching them into\nthree option groups as described in offline experiments. We enable PRESS ENTER and TERMINATE for the model to\n14\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nmake confirmation or stop the process.\nDuring our tests, pop-up ads on webpages were manually closed. The MindAct model, not trained on handling pop-up ads,\nlacks the feature to automatically manage them, potentially causing stalls. In contrast, SEEACT models can proactively\nsuggest closing ads through visual analysis and reasoning.\nD. Offline Experiment Prompts\nThe prompt for action generation is shown in Table 6. For grounding via textual choices, image annotation, and element\nattributes, the prompts are shown in Tables 7 to 9, along with specific tasks and examples in Figures 4 to 8.\nTable 6: Prompt for SEEACT Action Generation with LMMs.\nSystem Role\nImagine that you are imitating humans doing web navigation for a task step by\nstep. At each stage, you can see the webpage like humans by a screenshot and\nknow the previous actions before the current step decided by yourself through\nrecorded history. You need to decide on the first following action to take. You\ncan click an element with the mouse, select an option, or type text with the\nkeyboard. (For your understanding, they are like the click(), select_option() and\ntype() functions in playwright respectively) One next step means one operation\nwithin the three.\nAction Generation\nYou are asked to complete the following task: {TASK}\nPrevious Actions:\n{PREVIOUS ACTIONS}\nThe screenshot below shows the webpage you see. Follow the following guidance\nto think step by step before outlining the next action step at the current stage:\n(Current Webpage Identification)\nFirstly, think about what the current webpage is.\n(Previous Action Analysis)\nSecondly, combined with the screenshot, analyze each step of the previous action\nhistory and their intention one by one. Particularly, pay more attention to the last\nstep, which may be more related to what you should do now as the next step.\n(Screenshot Details Analysis)\nClosely examine the screenshot to check the status of every part of the webpage\nto understand what you can operate with and what has been set or completed.\nYou should closely examine the screenshot details to see what steps have been\ncompleted by previous actions even though you are given the textual previous\nactions. Because the textual history may not clearly and sufficiently record some\neffects of previous actions, you should closely evaluate the status of every part\nof the webpage to understand what you have done.\n(Next Action Based on Webpage and Analysis)\nThen, based on your analysis, in conjunction with human web browsing habits\nand the logic of web design, decide on the following action. And clearly outline\nwhich element in the webpage users will operate with as the first next target\nelement, its detailed location, and the corresponding operation.\nTo be successful, it is important to follow the following rules:\n1. You should only issue a valid action given the current observation.\n2. You should only issue one action at a time.\n15\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable 7: Prompt for SEEACT grounding via element attributes. We make a slight modification to enhance action generation\nand only show the modified part here to save space, as well as the prompts in Table 8 and Table 9.\nSystem Role\nSame as Table 6\nAction Generation\nSlightly modified from Table 6\n...\n(Next Action Based on Webpage and Analysis)\nThen, based on your analysis, in conjunction with human web browsing habits\nand the logic of web design, decide on the following action. And clearly outline\nwhich element in the webpage users will operate with as the first next target\nelement, its detailed location, and the corresponding operation. Please also\nclosely examine the screenshot to adequately describe its position relative to\nnearby elements and its textual or visual content (if it has). If you find multiple\nelements similar to your target element, use a more precise description to ensure\npeople can distinguish your target element from them through your answer.\n...\nFormat Answer\n(Final Answer) Finally, conclude your answer using the format below. Ensure\nyour answer is strictly adhering to the format provided below. Please do not\nleave any explanation in your answers of the final standardized format part, and\nthis final part should be clear and certain. The element, element type, element\ntext, action and value should be in five separate lines.\nFormat:\nELEMENT: Please describe which element you need to operate with. Describe\nit as detailed as possible, including what it is and where it is.\nELEMENT TYPE: Please specify its type from these options: BUTTON,\nTEXTBOX, SELECTBOX, or LINK.\nELEMENT TEXT: Please provide the exact text displayed on the element. Do\nnot invent or modify the text; reproduce it as-is from the screenshot.\nACTION: Choose an action from {CLICK, TYPE, SELECT}.\nVALUE: Provide additional input based on ACTION.\nThe VALUE means: If ACTION == TYPE, specify the text to be typed. If\nACTION == SELECT, specify the option to be chosen. If ACTION == CLICK,\nwrite \"None\".\n16\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable 8: Prompt for SEEACT grounding via textual choices.\nSystem Role\nSame as Table 6\nAction Generation\nSame as Table 6\nReferring Description\n(Reiteration)\nFirst, reiterate your next target element, its detailed location, and the correspond-\ning operation.\n(Multichoice Question)\nBelow is a multi-choice question, where the choices are elements in the webpage.\nFrom the screenshot, find out where and what each one is on the webpage. Then,\ndetermine whether one matches your target element. Please examine the choices\none by one. Choose the matching one. If multiple options match your answer,\nchoose the most likely one by re-examining the screenshot, the choices, and your\nfurther reasoning.\nIf none of these elements match your target element, please select [None of the\nother options match the correct element].\nA. [CHOICE A]\nB. [CHOICE B]\n...\nFormat Answer\n(Final Answer)\nFinally, conclude your answer using the format below. Ensure your answer\nis strictly adhering to the format provided below. Please do not leave any\nexplanation in your answers of the final standardized format part, and this final\npart should be clear and certain. The element choice, action, and value should be\nin three separate lines.\nFormat:\nELEMENT: The uppercase letter of your choice.\nACTION: Choose an action from {CLICK, TYPE, SELECT}.\nVALUE: Provide additional input based on ACTION.\nThe VALUE means: If ACTION == TYPE, specify the text to be typed. If\nACTION == SELECT, specify the option to be chosen. If ACTION == CLICK,\nwrite \"None\".\n17\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nTable 9: Prompt for SEEACT grounding via image annotation.\nSystem Role\nSame as Table 6\nAction Generation\nSame as Table 6\nReferring Description\n(Reiteration)\nFirst, reiterate your next target element, its detailed location, and the correspond-\ning operation.\n(Verification with the Screenshot)\nThen, please closely re-examine the screenshot to find whether your target\nelement is marked by a red bounding box and has a white number on a black\nbackground at the bottom left corner of the bounding box, which is positioned\nclosely next to the bounding box. If yes, use that number for your final answer.\nIf not, please do not make them up. If it is not marked, please output \"NA\" as\nyour target element in the following final answer part.\nFormat Answer\n(Final Answer)\nFinally, conclude your answer using the format below. Ensure your answer\nis strictly adhering to the format provided below. Please do not leave any\nexplanation in your answers of the final standardized format part, and this final\npart should be clear and certain. The element choice, action, and value should be\nin three separate lines.\nFormat:\nELEMENT: The number of your choice.\nACTION: Choose an action from {CLICK, TYPE, SELECT}.\nVALUE: Provide additional input based on ACTION.\nThe VALUE means: If ACTION == TYPE, specify the text to be typed. If\nACTION == SELECT, specify the option to be chosen. If ACTION == CLICK,\nwrite \"None\".\n18\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nE. Error Examples for Grounding via Image Annotation\nIn the method of grounding via image annotation, we observe significant hallucination errors that can be classified into the\nfollowing categories:\nMaking up bounding box & label. In our grounding method, if the correct element is absent from the set of candidate\nelements, the model is anticipated to generate \"NA\" as the answer. However, as depicted in Figure 10 and Figure 11, the\nmodel erroneously claims the element is included within a red bounding box and makes up a wrong index label as the\nanswer.\nFailure to link bounding boxes with the correct labels. Another challenge arises in accurately linking bounding boxes\nto their corresponding index labels. This challenge can be attributed to both LMMs\u2019 limitations in understanding relative\nspatial positions and the complex, dense layout of webpage elements. The model often mistakenly associates the labels of\nadjacent elements (as illustrated in Figure 12 and Figure 13), rather than accurately predicting the intended index label for\nthe targeted element.\nF. Strong Capability of Planning\nGPT-4V shows remarkable understanding and planning capabilities during our experiments. As depicted in Figure 14, the\nmodel is capable of understanding the website and generating a full plan for the given task involving multiple low-level\ntasks. Specifically, GPT-4V could understand reasonably well about the process and the remaining work of the task by its\ncareful examination of the webpage, as shown in Figure 15.\nG. Challenges in Grounding via Textual Choices\nAlthough textual choices achieved the best results among the three grounding approaches, it still suffers from challenges of\nsimilar or identical elements which are common in webpages. The model tends to choose the first text choice that seemingly\ncorresponds to its intention. Moreover, this is inevitable, as web pages indeed contain many elements that may even have\nexact identical HTML information, as the \"Schedule\" button shown in Figure 16.\nH. Knowledge and Reasoning Requirements\nSome tasks require a certain degree of reasoning and knowledge, which may be challenging for fine-tuned models like\nMindAct. For instance, the task in Figure 17 necessitates the model to know the specific district of Dublin in Virginia. In the\ntask of Figure 18, the model correctly provided the IATA airport code of airports in Indira Gandhi and Los Cabos.\nI. Path Variation and Awareness of Error Correction\nOn webpages, multiple paths often exist to accomplish a given task. For instance, varying the execution order of actions\nwithin an interchangeable sequence can result in diverse routes to task completion. Additionally, the agent can navigate to\ndifferent webpages but still accomplish the give tasks. Figure 19 presents a straightforward example where the model chose\na more direct route that differs from the ground truth annotated in the dataset.\nWhen running on live website, the agent\u2019s previous action histories is likely to be filled with redundant, unnecessary,\nerroneous, failed operations generated, or merely exploratory attempts by the model, resulting in a final path that deviates\nsignificantly from the ground truth. Despite these circumstances, the model can still accomplish the task amidst numerous\nincorrect explorations. The process of exploration and correction requires the model to possess a sense of self-correction.\nAs shown in Figure 20, GPT-4V demonstrates this awareness of correcting errors caused by previous steps.\n19\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Element Attributes \u2013 1st Turn\nInput \u2013 1st turn: Action Generation Prompts\nTask: View the profile of a Wedding Photographer near 10203 for a 4 hour wedding on april 13\nPrevious Actions:\n[textbox] Search on Thumbtack -> TYPE: wedding photographer\n[textbox] Zip code -> TYPE: 10203\nGPT-4V: (Current Webpage Identification)\nThe current webpage appears to be the homepage of Thumbtack, which is a local services marketplace where users can find\nprofessionals for various services, including wedding photography.\n(Previous Action Analysis)\nThe previous actions indicate that the user has typed \"wedding photographer\" into the search box and entered the zip code \"10203\" to\nfind a wedding photographer near that location. The intention behind these actions is to search for a wedding photographer in the\nspecified area.\n(Screenshot Details Analysis)\nThe screenshot shows the Thumbtack homepage with the search bar at the top, containing the previously entered search term \"wedding\nphotographer\" and the zip code \"10203.\" Below the search bar, there are suggestions for different services and popular articles, but these\nare not relevant to the task at hand. The search button next to the zip code field is visible, indicating that the search has not yet been\ninitiated.\n(Next Action Based on Webpage and Analysis)\nBased on the analysis, the next logical step is to initiate the search for wedding photographers in the specified zip code area. To do this,\nthe user should click on the \"Search\" button.\nThe \"Search\" button is located to the right of the zip code field at the top of the webpage. It is a rectangular button with rounded\ncorners, and the text \"Search\" is visible on it. This button is directly adjacent to the zip code field where \"10203\" has been entered.\nThe first next action to take is to click the \"Search\" button to proceed with finding a wedding photographer for the specified\nrequirements.\nFigure 4: An example of action generation in grounding via element attributes.\n20\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Element Attributes \u2013 2nd Turn\nInput \u2013 2nd turn: Referring Prompts\nGPT-4V: ELEMENT: The \"Search\" button located to the right of the zip code field at the top of the Thumbtack homepage.\nELEMENT TYPE: BUTTON\nELEMENT TEXT: Search\nACTION: CLICK\nVALUE: None\nFigure 5: An example of grounding via element attributes after action generation.\n21\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Textual Choices \u2013 1st Turn\nInput \u2013 1st turn: Action Generation Prompts\nTask: View the profile of a Wedding Photographer near 10203 for a 4 hour wedding on april 13\nPrevious Actions:\n[textbox] Search on Thumbtack -> TYPE: wedding photographer\n[textbox] Zip code -> TYPE: 10203\nGPT-4V: (Current Webpage Identification)\nThe current webpage appears to be the homepage of Thumbtack, a platform for hiring local professionals for various services.\n(Previous Action Analysis)\nThe previous actions indicate that the user is looking for a wedding photographer. The user has typed \"wedding photographer\" into a\nsearch box and entered the zip code \"10203\" into another box, presumably to localize the search to that area.\n(Screenshot Details Analysis)\nThe screenshot shows the Thumbtack homepage with the search fields already populated with \"wedding photographer\" and \"10203.\"\nThe user has not yet initiated the search, as the search results are not displayed on the screen. The \"Search\" button is visible next to the\nzip code field, indicating that the next logical step is to execute the search.\n(Next Action Based on Webpage and Analysis)\nBased on the analysis, the next action should be to initiate the search for wedding photographers in the specified zip code area. The first\nnext target element is the \"Search\" button located to the right of the zip code field.\nThe corresponding operation is to click the \"Search\" button to proceed with finding a list of wedding photographers near the zip code\n10203.\nFigure 6: An example of action generation in grounding via textual choices.\n22\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Textual Choices \u2013 2nd Turn\nInput \u2013 2nd turn: Referring Prompts\nChoices:\nA. <button id=0 button header dropdown button> <div> <span> JS </span>\nB. <a id=1> <span> Refer a friend, get $50 off </span>\nC. <a id=2> <div> Profile </div> </a>\nD. <button id=3 button> HIRE A PRO </button>\nE. <button id=4 button> FIND CUSTOMERS </button>\nF. <input id=5 search on thumbtack wedding photographer whats on your\nG. <input id=6 zip code zip_code 10203 />\nH. <button id=7 submit search> Search </button>\nI. <span id=8> , </span>\nJ. <span id=9> . </span>\nK. <button id=10 button navigation arrow down />\nL. <a id=11> <source image/webp /> <div furniture moving and heavy\nM. <input id=12 search on thumbtack whats on your to-do list?\nN. <input id=13 zip code zip_code 43235 />\nO. <button id=14 button> Cancel </button>\nP. <button id=15 submit> Search </button>\nQ. None of the other options match the correct element\nGPT-4V: (Reiteration)\nThe next target element is the \"Search\" button located to the right of the zip code field.\n(Multichoice Question)\nH. <button id=7 submit search> Search </button>\n(Final Answer)\nELEMENT: H\nACTION: CLICK\nVALUE: None\nFigure 7: An example of grounding via textual choices after action generation.\n23\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Image Annotation \u2013 1st Turn\nInput \u2013 1st turn: Action Generation Prompts\nTask: View the profile of a Wedding Photographer near 10203 for a 4 hour wedding on april 13\nPrevious Actions:\n[textbox] Search on Thumbtack -> TYPE: wedding photographer\n[textbox] Zip code -> TYPE: 10203\nGPT-4V: (Current Webpage Identification)\nThe current webpage appears to be the homepage of Thumbtack, a local services marketplace where users can find professionals for\nvarious services, including home improvement, events, and more.\n(Previous Action Analysis)\nThe first action was typing \"wedding photographer\" into a search box, indicating the user's intent to find a photographer for a wedding.\nThe second action involved typing \"10203\" into a zip code field, specifying the location where the service is needed.\n(Screenshot Details Analysis)\nThe screenshot shows the Thumbtack homepage with the search fields at the top. The \"wedding photographer\" text is visible in the\nsearch box, and \"10203\" is in the zip code field. There is a \"Search\" button next to these fields, which has not been clicked yet, as\nindicated by the presence of the search terms in the fields and the lack of search results on the page.\nThe first next target element is the \"Search\" button located to the right of the zip code field at the top of the webpage. The\ncorresponding operation is to click this button to execute the search for wedding photographers near the zip code 10203.\nFigure 8: An example of grounding via image annotation.\n24\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nExample of Grounding via Image Annotation \u2013 2nd Turn\nInput \u2013 2nd turn: Referring Prompts\nGPT-4V: ELEMENT: 4\nACTION: CLICK\nVALUE: None\nFigure 9: An example of grounding via image annotation.\n25\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nMakeup Image Annotation \u2013 Example 1\nTask: As a Verizon user, finance a new blue iPhone 13 with 256gb along with monthly apple care.\nPrevious Actions:\n\u2026\n[checkbox] AppleCare+ Theft and Loss for iPhone 13 - Monthly ... -> CLICK\n[button] Add to Cart -> CLICK\n[button] Let\u02bcs Get Started -> CLICK\n[label] Yes -> CLICK\nAction Generation: \u2026The first next target element is the \"Continue\" button located in the center of the page, just below the message\nabout adding a new line to the Verizon account. The corresponding operation is to click this button.\nAction Grounding: ELEMENT: 12\nACTION: CLICK\nVALUE: None\nFigure 10: In this example, the model describes a correct element in action generation. However, the identified element is\nabsent from the set of candidate elements. Despite this, the model erroneously assigns it the index number \"12\".\n26\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nMakeup Image Annotation - Example 2\nTask: Find me the cheapest Dodge Ram 1500 within 50 miles of 21122\nPrevious Actions:\n[combobox] Make -> SELECT: Dodge\n[combobox] Model -> SELECT: Ram 1500\nAction Generation: ... Next Action:\n- Target Element: 'Distance' dropdown menu\n- Location: Under the 'Model' dropdown, to the right of where '20 miles' is currently displayed\n- Operation: Click on the 'Distance' dropdown menu to expand it and then select '50 miles\u2019.\nGrounding: ELEMENT: 5\nACTION: CLICK\nVALUE: None\nFigure 11: In this example, the model predicts the appropriate element. Nevertheless, the identified element is not present in\nthe provided image options. Despite this, the model erroneously assigns it with the label \u20195\u2019 nearby.\n27\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nFailure of Linking - Example 1\nTask: Book a cheapest bundle and save option for 2 adults from Ahmedabad to Dubai on April 5 with free cancellation options, hotel\nshould be a 3 star near Burj Khalifa with guest rating above 4, one night\nPrevious Actions:\n[span] Bundle & Save -> CLICK\nAction Generation: The first next target element is the \"From\" field, located in the search form at the top of the page, which currently\nhas \"Pune\" as the input. The corresponding operation is to clear the current input and type \"Ahmedabad.\"\nNext action: Click on the \"From\" field and type \"Ahmedabad.\"\nAction Grounding:\nELEMENT: 10\nACTION: TYPE\nVALUE: Ahmedabad\nFigure 12: In this case, while the model predicts the appropriate element, it incorrectly associates the element with the\nnearby label \u201910\u2019 instead of the correct label \u201911\u2019.\n28\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nFailure of Linking - Example 2\nTask: Find a psychiatrist who offers virtual appointmentsthat has experience with treating neurodevelopment disorders and accepts\nnew patients.\nPrevious Actions:\n[searchbox] Search doctors, conditions, or procedures -> TYPE: psychiatrist\n[em] Psychiatrist -> CLICK\nAction Generation: ...Next action:\n- Click on the \"Telehealth\" filter located at the top of the page, in the row of filters, to apply this criterion to the search results.\nAction Grounding:\nELEMENT: 7\nACTION: CLICK\nVALUE: None\nFigure 13: In this case, while the model predicts the appropriate element, it incorrectly associates the element with the\nnearby label \u20197\u2019 instead of the correct label \u20196\u2019.\n29\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nStrong Capability of Planning\nTask: View the speakers that are bluetooth and wireless and filter the results to only show models that are on sale and cost less than $50.\nPrevious Actions:\nNone\nGPT-4V: \u2026 To proceed with the task, the user should: Locate the menu bar to find the category for audio equipment or speakers.\nThis is typically found in a dropdown under a section such as \u201cProducts\u201d, \u201cCategories\u201d, or directly as \u201cAudio\u201d. Once the correct\ncategory is found, the user should click on it to navigate to the section of the website that lists speakers. After reaching the speakers\nsection, the user will need to use the website\u2018s filtering options to select \u2019Bluetooth\u2018 and \u2019Wireless\u2018 features, and then set the price\nfilter to show only the models that are on sale and cost less than $50. \u2026\nFigure 14: In this example, the model generates a comprehensive plan for the task, including subsequent actions on following\npages that are not currently visible.\n30\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nStrong Capability of Understanding\nTask: Rent a truck with the lowest rent with two dozen furniture pads for 100 miles at zip 08817 on April 12 at 2:30 pm rented truck is to\nbe returned to the exact location and date, and the pickup and drop off will be at the nearest location.\nPrevious Actions:\n\u2026\n[textbox] US City,State or Zip Code -> TYPE: 08817\n[textbox] mm/dd/yyyy -> CLICK\n[link] 12 -> CLICK\n[select] Select Pick-up Time -> SELECT: 02:30 PM\n[input] -> CLICK\nGPT-4V: \u2026 The current webpage appears to be the truck rental reservation page of the Budget Truck Rental website. This page is\ndesigned for customers to input their rental details such as pickup location, dates, and times.\n(Screenshot Details and Previous Action Analysis)\nThe screenshot shows that the user has already entered the pickup location (zip code 08817), selected the pickup date (April 12, 2023),\nand chosen the pickup time (2:30 PM). The drop-off date is also set to the same day, which aligns with the task requirement of\nreturning the truck to the same location on the same date. The option \"Will you return the truck to a different location\" is set to \"No,\"\nwhich is correct for the task at hand. The last action taken was a click, but it's not clear on what element the click was performed.\nHowever, based on the task, it's likely that the user is ready to proceed to the next step of finding a truck.\n(Intended Action Based on Current Page and Previous Actions)\nThe next logical step for the user to take would be to click on the \"Find Your Truck\" button. This action will likely lead to a page\nwhere the user can select the type of truck they wish to rent and add any additional accessories, such as the two dozen furniture pads\nmentioned in the task. Since the task specifies renting the truck with the lowest rent, the user will need to compare the available options\nand select the most cost-effective one.\nThe \"Find Your Truck\" button is located in the central portion of the webpage, just below the input fields for the rental details. The\ncorresponding operation would be to move the cursor over the \"Find Your Truck\" button and perform a click action.\u2026\nFigure 15: In this case, two critical pieces of information are inadequately captured by the textual history. Firstly, the\nwebsite automatically set the drop-off date to the same day. Secondly, secondly, the \u2019No\u2019 button was selected (However it\nwas missed in previous actions due to the button\u2019s lack of text). Nevertheless, the model discerns these details through a\nmeticulous analysis of the screenshot, enabling it to make the correct decision for the next step.\n31\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nIdentical Elements\nTask: Schedule a repair service for my gaming console closest to the zip code 10001 on March 24, anytime after 6 pm.\nPrevious Actions:\n\u2026\n[[link] Schedule a Service -> CLICK\n[p] Video Games & Entertainment -> CLICK\n[heading] Schedule a repair or tune-up -> CLICK\n[button] Update Location -> CLICK\n[textbox] Enter Zip Code or City and State -> TYPE: 10001\n[button] Find Stores -> CLICK\n[button] Change Date -> CLICK\n[button] March 24, 2023, is available -> CLICK\n[combobox] Select a Time Slot -> SELECT: 6:20 p.m. \u2013 6:40 p.m. (EDT)\nGPT-4V: \u2026 The user is currently in the process of selecting a store location and time for the service.(Details and Previous Action\nAnalysis)\nThe user initiated this process by navigating to the support & services section of the Best Buy menu, then selecting the option to\nschedule a service. They narrowed down their service type by selecting the \"Video Games & Entertainment\" category and chose to\nschedule a repair or tune-up. The user has also updated their location using the zip code 10001 to find stores near that area. They then\nselected the date of March 24, 2023, and chose the available time slot of 6:20 p.m. \u2013 6:40 p.m. (EDT) for their service. This time slot fits the\nrequirement of scheduling the service anytime after 6 pm on March 24th.From the screenshot, three store options near the zip code\n10001 are listed. The closest one is the Midtown Manhattan location, which is 1.0 mile away from the provided zip code. This store\nalready has the selected date and time of Friday, March 24, from 6:20 p.m. to 6:40 p.m. The last action from the previous action history\nindicates that the user has selected the time slot of 6:20 p.m. \u2013 6:40 p.m. (EDT). This suggests that the user is now ready to finalize and\nconfirm this scheduled service.(Intended Action Based on Current Page and Previous Actions)\nConsidering the details and the objective to schedule a repair service, the next logical step for the user would be to confirm and schedule\nthe service at the selected store, date, and time. On the current page, the element that seems to be relevant to this action is the \"Schedule\"\nbutton associated with the selected store, date, and time. Therefore, the next action step is:\n[button] Schedule (associated with the Midtown Manhattan location for the selected date and time) -> CLICK.\nFigure 16: In this example, there are three identical \u2019Schedule\u2019 buttons, making it impossible for SEEACTChoice to distinguish\namong them. We empirically find that the model tends to choose the first one among the choices.\n32\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nKnowledge Requirement \u2013 Example 1\nGPT-4V: \u2026 The current page has a list of districts, which implies that the user may need to select a district closest to Dublin, Virginia,\nto continue their search. However, since there is no district explicitly named \"Dublin,\" the user must know which district Dublin\nfalls under or look for an option that allows for a more specific search.\n(Intended Action Based on Current Page and Previous Actions)\nThe first next target element that the user would likely need to operate with is the district name that includes or is closest to Dublin,\nVirginia. However, the screenshot does not provide enough detail to determine which district Dublin is in\u2026.\nTask: Find a driver training school in Dublin\nPrevious Actions:\n\u2026\n[link] All -> CLICK\n[link] Attending a Driver Training School -> CLICK\n[link] Driver Training Schools -> CLICK\n[link] Find a Driver Training School Near You -> CLICK\nFigure 17: In this example, the task necessitates knowledge about which district Dublin is located in.\n33\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nKnowledge Requirement \u2013 Example 2\nTask: Find flights going from Indira Gandhi to Los Cabos.\nPrevious Actions:\n[textbox] Origin -> TYPE: DEL\nGPT-4V: \u2026 Then, the user needs to identify the IATA code for Los Cabos International Airport, which is \"SJD,\" and enter it into the \ndestination field. Thus, the next target element is the destination field, which is located to the right of the origin field in the \"Search by \nRoute\" section of the search interface. \nHere is the action to be taken: \n[textbox] Destination -> TYPE: SJD (after ensuring the origin field correctly contains \"DEL\").\nFigure 18: In this example, the task requires knowledge of the IATA code for Los Cabos International Airport. GPT-4V\naccurately provides the correct code.\n34\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nPath Variation\nTask: Browse the natural products database.\nGround Truth:\n[link] More Resources -> CLICK\n[link] Natural products database -> CLICK\nVariation: [link]  Natural products information -> CLICK\nFigure 19: In this example, the ground truth in Mind2Web is to firstly click \"More\", then click \"Natural products database\"\non the second page. In contrast, the model identifies a more direct approach, achieving the target page through \u2019Natural\nproducts information\u2019 on the first page.\n35\nGPT-4V(ision) is a Generalist Web Agent, if Grounded\nAwareness of Error Correction\n<image 1>\nTask: Signup for virtual healthcare visit.\nPrevious Actions:\n\u2026\n[span] James Smith -> CLICK\n[button] Select patient and continue -> CLICK\n[textbox] Date of birth -> CLICK\n[textbox] Date of birth -> TYPE: 01/01/2000\n[radio] Male -> CLICK\n[radio] No -> CLICK\n[textbox] Mobile phone number -> TYPE: 888888888\n[textbox] Street address -> TYPE: 123 st\nGPT-4V: \u2026 Since the mobile phone number field is highlighted, it suggests there might be a problem with the entry\u2014such as an\nincorrect format or not enough digits. Therefore, the next action would be to correct the mobile phone number by clicking on the\nmobile phone number text box and entering a valid number. Once that's corrected, the user should continue filling out the rest of the\nform with the city, state, and ZIP code before proceeding to click the \"Continue\" button to move forward with the signup process\u2026\nFigure 20: In this example, the webpage displays an error message indicating an invalid phone number, a consequence of\nprior actions. The model identifies this error and prioritizes its immediate rectification, foregoing the subsequent planned\nsteps.\n36\n"
  },
  {
    "title": "Image Sculpting: Precise Object Editing with 3D Geometry Control",
    "link": "https://arxiv.org/pdf/2401.01702.pdf",
    "upvote": "18",
    "text": "Image Sculpting: Precise Object Editing with 3D Geometry Control\nJiraphon Yenphraphai1\nXichen Pan1\nSainan Liu2\nDaniele Panozzo1\nSaining Xie1\n1New York University\n2Intel Labs\n(b) Precise Rotation\n(d) Precise 3D Composition\n\ud83e\ude84\n\ud83e\ude84\nCompose\n\ud83e\ude84\n#4\n#3\n#1\n#2\n\ud83e\ude84\nCarving Out\n\ud83e\ude84\n\u0394\ud835\udc51 =4m\n\ud83e\ude84\n\u0394\ud835\udc65 = -43\u00b0\n\u0394y = -60\u00b0\n\u0394z = -20\u00b0\n\u0394\ud835\udc65 = 65\u00b0\n\u0394y = -60\u00b0\n\u0394z = -23\u00b0\n(a) Precise Pose Editing\n(e) Precise Carving\n(c) Precise Translation\n(f) Serial Addition\n\u0394\ud835\udc65 = 37\u00b0\nFigure 1. Achieving precise control in image editing tasks can be challenging with standard 2D generative pipelines. Our Image Sculpting\nframework offers the ability to interact with 3D geometry starting with a single image. This enables users to perform detailed, quantifiable,\nand physically-plausible edits, including precise pose editing, rotation, translation, 3D composition, carving, and serial addition.\nAbstract\nWe present Image Sculpting, a new framework for edit-\ning 2D images by incorporating tools from 3D geometry\nand graphics. This approach differs markedly from exist-\ning methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and lim-\nited control. Image Sculpting converts 2D objects into 3D,\nenabling direct interaction with their 3D geometry. Post-\nediting, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through\na coarse-to-fine enhancement process. The framework sup-\nports precise, quantifiable, and physically-plausible editing\noptions such as pose editing, rotation, translation, 3D com-\nposition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative\nmodels with the precision of graphics pipelines.\nCode and project page available here.\n1. Introduction\nRecent developments in the field of image generative mod-\neling [60, 63, 65, 86] have unlocked new potentials in cre-\native content creation, offering unprecedented opportuni-\nties for the generation of diverse visual content by mate-\nrializing ideas and concepts articulated through language\nprompts.\nHowever, the integration of these models into\nreal-world content creation workflows still poses significant\nchallenges. Among the most critical is the need for users to\nhave detailed control over various aspects of generated ob-\njects, including their pose, shape, location, layout, and spa-\ntial compositions. The precision extends to quantifiable ma-\nnipulations, such as rotating an object by a specific angle or\nmaking physically-realistic modifications, such as position-\ning a character in a way that conforms to basic anatomical\nand physical principles. Interestingly, such a quest for pre-\ncision and controllability aligns closely with the core prin-\nciples of computer graphics, which strive to generate pho-\n1\narXiv:2401.01702v1  [cs.GR]  2 Jan 2024\ntorealistic images with artistic control.\nIn virtual effects (VFX) and rendering pipelines, experts\nmeticulously craft and edit every detail within a fully con-\ntrollable 3D environment, striving for utmost realism. For\ndecades, methods for accurately manipulating and render-\ning objects have been explored, leading to the development\nof numerous advanced techniques in 3D model acquisition,\nrigging, posing, lighting, texturing, and scene rendering.\nThese methods form the bedrock of the modern computer\ngraphics pipeline. However, it often requires custom hard-\nware and software for (1) acquiring production-quality 3D\nmodels or designing them from scratch, (2) making these\nmodels possible to animate (rigging), (3) creating visually\nplausible animations (animation), (4) rendering back in the\n2D world after applying material and setting up the light-\ning, and (5) compositing the resulting image with a back-\nground or other objects. This process often employs teams\nof artists and engineers for each one of these steps, as it re-\nquires substantial manual input using specialized tools (e.g.\nAfter Effects [2], Substance [4], and 3ds Max [27]).\nIn contrast, AI-based image generation avoids all this\nmanual work, requiring only a text prompt. Leveraging the\npower of human language and large datasets of curated con-\ntent, transforming a text description into a visually striking\nimage is more accessible than ever. Yet, when it comes to\nprecise object manipulation, the current 2D-based genera-\ntive approach faces inherent limitations due to the lack of a\nthird dimension, leading to incomplete information, limited\nuser interaction on a flat plane, and possible ambiguities.\nThe gap in controllability with respect to image generation\nusing computer graphics techniques is striking, and closing\nit is a major goal of our work.\nMost interfaces for image editing frameworks rely on\ntext-based instructions.\nFor example, techniques such\nas Prompt-to-Prompt [24], Plug-and-Play [76], Instruct-\nPix2Pix [10], Imagic [34] and Object 3DIT [47] offer adapt-\nable language control. However, achieving precise manipu-\nlation through these models remains a challenge. Straight-\nforward manipulations such as \u201cchanging a style to mimic\nVan Gogh\u201d are manageable. However, more specific in-\nstructions such as \u201clift the object by 5 cm and rotate it by\n42 degrees.\u201d are less likely to be successful, as current gen-\nerative models cannot fulfill such detailed requests through\ntextual prompts alone. 2D-based interactive methods such\nas DragGAN [54], FreeDrag [39], and DragDiffusion [70]\ndemonstrate the ability to alter part of an object through\ntransitions in the latent space. Despite this, they have their\nlimitations: 1) they can accomplish basic deformations, but\nthe outcomes are not entirely predictable, often leading to\nresults that do not align with the user\u2019s intentions; 2) these\nlatent transformations operate within the 2D feature space,\nwhich inherently limits their ability to represent 3D trans-\nformations and handle occlusions accurately; 3) they lack\nphysics-awareness, which complicates incorporating exter-\nnal constraints, such as skeletal structures.\nOur work draws inspiration from the computer graph-\nics pipeline and ventures into a novel approach for 2D\nimage-based object manipulation tasks. Our proposed Im-\nage Sculpting framework, which metaphorically suggests\nthe flexible and precise sculpting of a 2D image in a 3D\nspace, integrates three key components: (1) single-view\n3D reconstruction, (2) manipulation of objects in 3D, and\n(3) a coarse-to-fine generative enhancement process. More\nspecifically, 2D objects are converted into 3D models,\ngranting users the ability to interact with and manipulate the\n3D geometry directly, which allows for precision in editing.\nThe manipulated objects are then seamlessly reincorporated\ninto their original 2D contexts, maintaining visual coher-\nence and fidelity. A critical hurdle in this process is the\nsingle-view 3D reconstruction method, a task that, despite\nrapid progress [25, 38, 40\u201343, 59, 68], often results in rela-\ntively low-fidelity, coarse geometric and texture representa-\ntions. Unlike manually crafted 3D assets used for graphics,\ntheir rendered version is far from photo-realistic. Nonethe-\nless, the extracted geometries are sufficient for interactive\nand precise control. To achieve high-quality final images, a\nseparate enhancement procedure is necessary. In summary,\nour Image Sculpting pipeline has three key phases:\nPhase 1. For the 3D reconstruction phase, we employ\na zero-shot single image reconstruction model (Zero-1-to-\n3 [41]), which has been trained on extensive datasets [15]\nof 3D objects.\nPhase 2. The deformation process utilizes established\ngeometric processing tools, such as As-Rigid-As-Possible\n(ARAP) [75] and linear-based skinning [45], enabling in-\nteractive and precise manipulation of the 3D models.\nPhase 3. For the generative enhancement process, we\ndevelop a coarse-to-fine enhancement approach, using an\nimproved feature injection technique [76].\nOur method\nstrikes a balance between maintaining the original texture of\nthe object and the modified geometry, utilizing a pre-trained\ntext-to-image diffusion model with additional controls.\nOur Image Sculpting framework showcases an array of\nprecise and quantifiable image editing capabilities. These\ninclude precise pose editing, rotation, translation, multi-\nobject 3D composition, carving, and serial addition. This\nsuite of functionalities demonstrates the versatility of our\napproach and its superiority in precision and control com-\npared to existing image editing methods. Our method also\noutperforms various baselines in image quality, as con-\nfirmed by both qualitative and quantitative evaluations on\nthe new SculptingBench benchmark. We believe that our\nmethod can foster new opportunities in merging the flexi-\nbility of generative models with the precise controllability\ninherent in traditional graphics pipelines.\n2\n(a) Space Deformations\n(b) Shape-Aware Deformation\n(c) Linear Blend Skinning\nFigure 2. Illustration of three mesh deformation methods applied to a 3D model. In cage-based space deformation (a), the model is placed\nin a cage and deformed when the user moves the cage vertices [31]. As-Rigid-As-Possible (ARAP) [75] deformation (b) deforms the model\nwhen user-selected blue handle points are moved towards designated red target points. Linear blend skinning (c) maps the deformation of\na skeleton to the model [30]. Following deformation, a diffusion rendering process can be added for controllable generation. Each mesh\ndeformation technique offers a different balance of control, speed, and precision. Our framework can use any of these techniques.\n2. Related Work\nGenerative Image Editing\nIn computer graphics, exten-\nsive research on interactive raster image editing exists, and\nwe defer its detailed review to the next section. In com-\nputer vision, the advent of image generative models such as\nGANs [20, 32, 33] has expanded the scope of image edit-\ning to include style transfer [19], image-to-image transla-\ntion [28, 88], latent manipulation [67, 81], and text-based\nmanipulation [1, 55, 82]. Recently, capabilities in image\nediting have advanced significantly with the rise of diffusion\nmodels [16, 56, 63]. The leading systems [48, 53, 60, 65]\nallow users to generate image variations or use inpaint-\ning masks [51] to generate specific parts of scenes based\non a text prompt.\nOther work explores enhancing pre-\ntrained diffusion models with text-guided editing capabil-\nities [10, 24, 49, 76]. Yet, text-based editing has limita-\ntions in precisely controlling object shapes and positions.\nControlNet [87] incorporates additional conditional inputs\nsuch as depth [61], poses [11], and edges [83] for con-\ntrollable generation. For more intuitive interactions, Drag-\nGAN [54] enables users to drag control points on objects\nwith GANs, and similar techniques have been adapted for\ndiffusion models [39, 70].\nHowever, these methods are\nmostly confined to 2D and face challenges in tasks requir-\ning more complex, out-of-plane transformations. 3D-aware\ngenerative models such as EG3D [12] and StyleNeRF [21]\nhave explored this direction. OBJect-3DIT [47], a base-\nline in our paper, studied 3D-aware editing using language\ninstructions. However, its effectiveness is somewhat con-\nstrained due to its training on a synthetic dataset.\nSingle-View Reconstruction Single-view 3D reconstruc-\ntion is a long-standing problem in computer vision [23].\nWhile algorithmic advancements are important, the signif-\nicance of training data has been increasingly recognized.\nEarlier efforts were geared towards training models [52, 73,\n80, 85] using smaller, simplistic 3D datasets [13, 62]. Re-\ncent approaches [58, 77] have started to utilize density dis-\ntillation from pre-trained 2D diffusion models trained on\nlarge-scale text-image datasets, lessening the reliance on 3D\ndata. Nonetheless, for improved view-consistency, the de-\nmand for high-quality 3D data is indispensable. The emer-\ngence of large-scale 3D datasets, such as Objaverse [14, 15],\nhas spurred methods such as Zero-1-to-3 [41] to combine\n2D score distillation with 3D data training. This has led to\na surge in new models in this domain, noticeably enhancing\nreconstruction quality [40, 59, 69, 79]. Current 3D recon-\nstruction models, while not perfect, have attained a level of\nmaturity that makes them suitable for shape editing.\n3. Overview of 3D Shape Deformation\nThe deformation of 3D shapes has been extensively stud-\nied in the last four decades, with both traditional and data-\ndriven methods being proposed and successfully used in\nrobotics, graphics, and engineering. We review the main\napproaches and their usability within our framework.\nSpace Deformations\nThe older and still widely used ap-\nproach is applying a volumetric warp function f : R3 \u2192\nR3 to all points of a 3D domain [66]. This approach can be\napplied to explicit (triangular or polygonal meshes) or im-\nplicit representations. The map can be parametrized using\nsplines on lattices [66], vertices on a cage [31], or neural\nfields [17]. A limitation of these approaches is that they are\nunaware of the object shape, making them more challenging\nto use on complex articulated objects [9].\nShape-Aware Deformation\nShape-aware deformations\nprovide a set of controls linked to the objects\u2019 surface.\nIn Computer-Aided-Design (CAD), a small set of control\npoints define a smooth surface using spline patches [18].\nDespite its flexibility and quality, extracting spline patches\nfrom 3D models or NeRFs is a challenging and open prob-\nlem [7]. Partial differential equation (PDE)-based methods\nsimulate the deformation of an object, representing it as a\nvolumetric deformable solid [72] or as a thin rubber shell\n[75]. The forces guiding the deformation are applied by\nmoving handles selected on a surface [8], making them in-\ntuitive to use and requiring minimal user interaction.\n3\nNeRF\nDiffuse Texture\nUV map\nSDS\nDDIM+\nInversion\nCoarse Geometry\nMarching\nCubes\n3D\nDeformation\nCoarse Rendering\nTexturing\nHigh-fidelity Edited Image\nInput Image\n\ud835\udc61 =T\n\ud835\udc61 = 0\nCoarse Image DDIM+ Denoising\nGeometry Injection End\n\ud835\udc61 =T\n\ud835\udc61 = 0\nEnhanced Image DDIM+ Denoising\nGeometry Injection End\n\ud835\udc65!\n\"\n\ud835\udc34#\n\"\n\ud835\udc53#\n\"\n\ud835\udc34#\n\"\n\ud835\udc53#\n\"\n\ud835\udc34#\n\"\n\ud835\udc53#\n\"\n\ud835\udc3c\"\nFigure 3. Overview of our Image Sculpting pipeline, DDIM+ represents DDIM with the DreamBooth fine-tuned and depth controlled\nmodel. The process begins by converting the input image into a textured 3D model through a de-rendering process. This model is then\nprepared for interactive deformation by creating a skeleton and calculating skinning weights. The user can modify the skeleton to deform\nthe model, resulting in an initial coarse image. To refine this edited image, we invert the coarse rendering Ic into the noise xc\nT . We then\ninject self-attention maps Ac\nt and feature maps f c\nt from the initial image\u2019s denoising process into the enhanced image denoising steps.\nThis technique helps in preserving the geometry of the modified object while restoring the visual quality of the edited image.\nLinear Blend Skinning\nThe most popular deformation\napproach is linear blend skinning [30], which defines a\nspace deformation function as a blended average of a set\nof affine transformations weighted by shape-aware scalar\nfunctions, often computed with methods based on solutions\nof PDEs on surfaces [29] or manually edited. This approach\noffers complete control and flexibility, as the affine trans-\nformation can be attached to points, vertexes of a cage, or\nsegments in a skeleton [6].\nOur approach We can use any of these algorithms to pre-\ncisely control the shape deformation and, thus, the rendered\nimage. We show an example of one representative method\nfor each class in Fig 2, and we leave as future work addi-\ntional automation of this step.\n4. Methods\nGiven a single 2D image, our objective is to enable pre-\ncise manipulation of the objects and their orientations in 3D\nspace, before converting this back into a high-quality edited\n2D image. To achieve this, we have developed a novel edit-\ning pipeline tailored for image sculpting (see Fig 3) com-\nposed of three steps: (1) We initially convert the input image\ninto a 3D model, (2) the 3D model is edited by deforming it\nin 3D space, and (3) we use a coarse-to-fine generative en-\nhancement pipeline to turn the coarse rendering of the 3D\nmodel into a high-fidelity image.\n4.1. De-Rendering and Deformation\nGiven an image of an object, our goal is to perform 3D re-\nconstruction to obtain its 3D model.\nImage to NeRF\nWith advancements in text-to-image\nfoundation models [63] and the viewpoint-conditioned im-\nage translation model [41], our initial step involves seg-\nmenting the selected object from the input image using\nSAM [35]. Building upon this, we then train a NeRF using\nScore Distillation Sampling (SDS) [58].\nNeRF to 3D Model We use the implementation in three-\nstudio [22] to convert a NeRF volume into a mesh. This\nalgorithm transforms the volume density into a signed dis-\ntance function, extracts an isosurface [44], and parameter-\nizes it [84] for texture mapping [71]. The texture is ex-\ntracted by differentiable rendering [36].\n3D Model Deformation After obtaining the 3D model, a\nuser can manually construct a skeleton and interactively ma-\nnipulate it by rotating the bones to achieve the target pose.\nThe mesh deformation affects the vertex positions of the ob-\nject but not the UV coordinates used for texture mapping;\nthis procedure thus deforms the texture mapped on the ob-\nject following its deformation.\nHowever, the resulting image quality depends on the 3D\nreconstruction\u2019s accuracy, which, in our case, is coarse and\ninsufficient for the intended visual outcome (Fig 3). There-\nfore, we rely on an image enhancement pipeline to convert\nthe coarse rendering into a high-quality output.\n4\nInput Image\nCoarse Rendering\nDreamBooth [64]\nSDEdit [46] (t0 = 0.4)\nSDEdit [46] (t0 = 0.6)\nOurs w/o Feature Injection\nOurs w/o Depth Control\nOurs\nFigure 4. Comparison of our final method with various baseline methods and ablations. Our approach effectively maintains the geometric\ninformation while ensuring the texture quality. In contrast, other methods typically preserve either the texture or the geometry, but not both.\nOptional\nInstruction\nLatent\nNoise\nImage\nCoarse\nImage\nControlNet\nFigure 5. Overview of the coarse-to-fine generative enhancement\nmodel architecture. The red module denotes the one-shot Dream-\nBooth [64], which requires tuning; the grey module is the SDXL\nRefiner [5], which is frozen in our experiments.\n4.2. Coarse-to-Fine Generative Enhancement\nThis section focuses on blending a coarsely rendered image\nback to its original background. The aim is to restore textu-\nral details while keeping the edited geometry intact. Image\nrestoration and enhancement are commonly approached as\nimage-to-image translation tasks [78], leveraging the strong\ncorrelation between the source and target images. Our chal-\nlenge, however, presents a unique scenario: despite overall\nsimilarities in appearance and texture between the input and\ndesired output, the input object\u2019s geometry changes, some-\ntimes significantly, after user editing.\nIn exploring possible solutions, one approach is to\nuse subject-driven personalization techniques like Dream-\nBooth [64]. They aim to preserve key details from the input,\nbut might compromise the edited geometry. Alternatively,\nimage-to-image translation methods like SDEdit [46] can\nbe used to preserve the edited geometry, but this might dis-\nturb the textural consistency with the original image. This\ndichotomy was clear in our preliminary study, as shown in\nFig 4. SDEdit can maintain the geometry, but it was un-\nable to accurately replicate the textures. On the other hand,\nDreamBooth produced high-fidelity outputs, but struggled\nto preserve both the texture and geometry effectively.\nTo address the balance between preserving texture and\ngeometry, our approach begins by \u201cpersonalizing\u201d a pre-\ntrained text-to-image diffusion model. To capture the ob-\nject\u2019s key features, we fine-tune the diffusion model with\nDreamBooth on one input reference image. To maintain the\ngeometry, we adapt a feature and attention injection tech-\nnique [76], originally designed for semantic layout control.\nFurthermore, we incorporate depth data from the 3D model\nthrough ControlNet [87]. We find this integration crucial in\nminimizing uncertainties during the enhancement process.\nOne-shot Dreambooth\nDreamBooth [64] fine-tunes a\npre-trained diffusion model with a few images for subject-\ndriven generation. The original DreamBooth paper [64] has\nshown its ability to leverage the semantic class priors to\ngenerate novel views of an object, given only a few frontal\nimages of the subject. This aspect is particularly useful in\nour setting, since the coarse rendering we work with lacks\nexplicit viewpoint information. In our application, we train\nDreamBooth using just a single example, which is the input\nimage. Notably, this one-shot approach with DreamBooth\nalso effectively captures the detailed texture, thereby filling\nin the textural gaps present in the coarse rendering.\nDepth Control We use depth ControlNet [87] to preserve\nthe geometric information of user editing. The depth map is\nrendered directly from the deformed 3D model, bypassing\nthe need for any monocular depth estimation. For the back-\nground region, we don\u2019t use the depth map. This depth map\nserves as a spatial control signal, guiding the geometry gen-\neration in the final edited images. However, relying solely\non depth control is not sufficient \u2013 although it can preserve\nthe geometry to some extent, it still struggles in local, more\nnuanced editing, such as capturing the specific shapes of a\npumpkin\u2019s eyes or the bent legs of a chair (Fig 4).\nFeature Injection To better preserve the geometry, we use\nfeature injection. As demonstrated in Fig 3, this step be-\ngins with DDIM inversion [74] (with the DreamBooth fine-\ntuned, depth controlled diffusion model) of the coarse ren-\ndering image to obtain the inverted latents. At each denois-\ning step, we denoise the inverted latent of the coarse ren-\n5\nTranslation\nComposition\nCarving\nSerial Addition\nReplace with\nCarving Out\nAddition\nMixture\nPose Editing\nRotate\nPose Editing & Rotation\nFigure 6. A compilation of qualitative results from six image editing tasks. Additionally, we include additional examples (termed as\n\u2018Mixture\u2019 in the final row) to illustrate the versatile combination of these capabilities.\n6\nBackground\nDenoising Loop\nMask-Filled Latent\nDenoised Latent\nFigure 7. Our blend-in process. At every denoising step, we mask\nthe background areas and blend them with the unmasked regions\nfrom the denoised latent. This process helps maintain visual co-\nherence and preserve the background.\nFigure 8. Our blend-in process yields visually harmonious results.\nTop: Results from direct copy-pasting. Bottom: Our results.\ndering along with the latent of the refined image, extracting\ntheir respective feature maps (from the residual blocks) and\nself-attention maps (from the transformer blocks). It has\nbeen shown in [76] that the feature maps carry semantic in-\nformation, while the self-attention maps contain the geom-\netry and layout of the generated images. By overriding the\nfeature and self-attention maps during the enhanced image\ndenoising steps with those from the coarser version, we en-\nsure the geometry of the enhanced image can reflect those\nof the coarse rendering. The pseudo code for our genera-\ntive enhancement is detailed in Appendix A. Note that our\nmethod differs from the original Plug-and-Play use cases:\nwe use feature injection to preserve the geometry during\nthe coarse-to-fine process rather than translating the image\naccording to a new text prompt. We present the injection\nlayer selection and the replacement schedule in Section 5.\nBackground Blend-In To maintain the consistency of the\nbackground between the input and edited images, we first\ninpaint the area initially occupied by the object in the input\nimage, thus obtaining an unobstructed background. How-\never, another challenge arises in merging the edited object\ninto this background smoothly. Merely copy-pasting it onto\nthe background leads to an unrealistic visual effect, such\nas the improper water reflections over the fish and the ab-\nsence of shadow casting from the truck (Fig 8). To over-\ncome this, as demonstrated in Fig 7, our approach involves\nmasking the background areas during the denoising steps\nMove the dog to\n(0.45, 0.2)\nRotate the cat by 72\u00b0\nInsert a hat at\n(0.62, 0.91)\nInsert a cherry at\n(0.5, 0.8)\nMove the truck to\n(0.6, 0.4)\nOurs\n3DIT [47]\nInput Image\nFigure 9. Comparisons with OBJect-3DIT [47] on object transla-\ntion, rotation, and composition tasks.\nDrag Control\nDragDiffusion [70]\nOpenpose Control\nControlNet [87]\nOurs\nFigure 10. Comparisons with DragDiffusion [70] and Control-\nNet [87] on pose editing. These techniques face difficulties in\nhandling complex pose modifications.\nto preserve their original background. This means we retain\nthe unedited background by blending the unmasked (edited)\nregions from the denoising step with the masked (original)\nbackground. We use SDXL [57] as our pre-trained text-\nto-image model, which includes a refiner module by de-\nfault. We keep this module in our pipeline, as empirically it\nslightly enhances the results by reducing artifacts.\n5. Experiments\nExperimental Setup\nwe follow [22] to obtain the initial\nNeRF representation and to extract the textured 3D model.\nWe use Instant-NGP [50] and a grid size of 256 for the 3D\nmodel extraction from NeRF. During the coarse-to-fine gen-\nerative enhancement process, for one-shot DreamBooth, we\nfine-tune the SDXL-1.0 [57] model using LoRA [26] for\n800 steps with a learning rate of 1e-5. For feature injection\nstage, we utilize all the self-attention layers of the SDXL\ndecoder and the first block of the SDXL\u2019s upsampling de-\ncoder. We set \u03c4A = 0.5 and \u03c4f = 0.2. The SDXL refiner is\napplied after t = 0.1T. For background inpainting, we use\nAdobe generative fill [3].\n7\nSource Image\nAdd one cherry\nAdd another cherry\nAdd the third cherry\nAdd the forth cherry\nOurs\nDALL\u00b7E 3 [53]\nInstructPix2Pix [10]\nFigure\n11.\nComparisons\nwith\nInstructPix2Pix\n[10]\nand\nDALL\u00b7E 3 [53] on serial addition. These text-based editing meth-\nods fail to follow precise and quantifiable instructions.\nMethods\nDINO\u2191\nCLIP-I\u2191\nD-RMSE\u2193\nOriginal Coarse Rendering\n0.758\n0.902\n0.00\nSDEdit [46] (t0 = 0.4)\n0.788\n0.919\n1.71\nSDEdit [46] (t0 = 0.6)\n0.800\n0.920\n2.12\nOurs w/o Feature Injection\n0.848\n0.925\n2.33\nOurs w/o Depth Control\n0.851\n0.921\n2.15\nOurs\n0.853\n0.921\n1.99\nTable 1. Ablation studies of the enhancement methods on Sculpt-\ningBench. DINO score and CLIP-I score measure the textural de-\ntails, and D-RMSE measures the geometric fidelity. We observe\nthat depth control and feature injection can significantly enhance\ntexture quality while maintaining geometric consistency.\nQualitative Results\nWe showcase qualitative results in\nFig 6, covering six precise image editing tasks. Detailed de-\nscriptions of these tasks are presented in Appendix B. Qual-\nitatively, our method combines the creative freedom of gen-\nerative models with the precision of graphics pipelines to\nachieve precise, quantifiable, and physically plausible out-\ncomes for object editing across a variety of scenarios.\nOur approach introduces new editing features through\nprecise 3D geometry control, a capability not present in ex-\nisting methods. We compare our method with the state-of-\nthe-art object editing techniques for a comprehensive anal-\nysis. In Fig 9, we show that 3DIT [47], designed for 3D-\naware editing via language instructions, faces limitations\nwhen applied to real, complex images, largely because its\ntraining is based on a synthetic dataset. In Fig 10, we com-\npare the pose editing ability with DragDiffusion [70] and\nControlNet [87]. This comparison reveals that these meth-\nods encounter difficulties with complex pose manipulations\nbecause they are constrained to the 2D domain. Further-\nmore, in Fig 11, we show how text-based editing methods\nlike InstructPix2Pix [10] and DALL\u00b7E 3 [53] struggle with\nprecise and quantifiable instructions.\nAblation Studies\nWe create a new dataset SculptingBench\nto evaluate our new image editing capabilities. This dataset\ncontains 28 images covering six categories: pose editing,\nrotation, translation, composition, carving, and serial addi-\ntion (see Appendix C). We perform quantitative studies us-\ning different coarse-to-fine enhancement methods. To mea-\nsure the visual similarity between the edited images and the\noriginal ones, particularly in terms of maintaining textural\ndetails through the editing process, we employ DINO and\nCLIP-I scores [64] as our metrics. To evaluate the geomet-\nric fidelity of user edits after enhancement, we introduce a\nnovel metric, D-RMSE. This metric is specifically created\nto evaluate how well geometric information is retained af-\nter the enhancement procedure. D-RMSE measures the dis-\ncrepancies between the depth maps of the coarse renderings\nand their enhanced counterparts:\nD-RMSE =\np\nE [(depthcoarse \u2212 depthenhanced)2]\nwhere depthcoarse, depthenhanced denote the depth maps Mi-\nDaS [61] estimates, for the coarse rendering and the en-\nhanced output image, respectively. In Table 1, we show\nthat without any enhancement, the textural quality metrics\n(DINO and CLIP-I scores) are quite low. SDEdit effec-\ntively preserves the edited geometry with a low D-RMSE,\nyet the visual quality significantly deteriorates compared to\nthe original image (see Fig. 4). Our method offers a more\nadvantageous balance, significantly enhancing texture qual-\nity as demonstrated by higher DINO and CLIP-I scores,\nwhile preserving geometric consistency, evidenced by a low\nD-RMSE score. We observe that both feature injection and\ndepth control contribute to enhanced geometric consistency\nand can lead to further improvement when used together.\nAdditionally, we conduct an empirical study to explore the\nideal number of self-attention layers for injection. Fig 12\nshows that more layers improve alignment with user edits.\nIn our work, we use all layers for injection.\n6. Limitation\nOur method is an initial step towards integrating traditional\ngeometric processing with advanced diffusion-based gen-\nerative models for precise object editing. Yet, it has lim-\nitations. A significant challenge is the dependency on the\nquality of single-view 3D reconstruction, which is antici-\npated to improve over time. Additionally, mesh deformation\noften requires some manual efforts for model rigging. Fu-\nture research might explore data-driven techniques [37] to\nautomate this process. The output resolution of our pipeline\nalso falls short of industrial rendering systems, and incor-\nporating super-resolution methods could be a solution for\nfuture improvements. Another issue is the lack of back-\nground lighting adjustment, which undermines the realism\nof the scene; future work could benefit from integrating dy-\nnamic (re-)lighting techniques. We present some instances\nof failure our our system in Appendix D.\n8\nInjecting more layers\nFigure 12. Ablation studies on feature injection layers. From left to right, progressively injecting more self-attention layers can result in\nincreasingly improved alignment with user edits.\nAcknowledgements.\nWe thank Ellis Brown, Fred Lu,\nSanghyun Woo, Adithya Iyer and Oscar Michel for help-\nful discussions. The research is partly supported by Intel,\nCirrascale and the Google TRC program.\nReferences\n[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and\nPeter Wonka. CLIP2StyleGAN: Unsupervised extraction of\nStyleGAN edit directions. In SIGGRAPH, 2022. 3\n[2] Adobe. Adobe After Effects. https://www.adobe.\ncom/products/aftereffects.html, 2023. 2\n[3] Adobe. Adobe Firefly. https://www.adobe.com/\nsensei/generative-ai/firefly.html, 2023. 7\n[4] Adobe. Adobe Substance 3D. https://www.adobe.\ncom/creativecloud/3d-ar.html, 2023. 2\n[5] Stability AI.\nStable Diffusion XL Refiner 1.0.\nhttps:\n/ / huggingface . co / stabilityai / stable -\ndiffusion-xl-refiner-1.0, 2023. 5\n[6] Ilya Baran and Jovan Popovi\u00b4c. Automatic rigging and ani-\nmation of 3D characters. TOG, 2007. 4, 12\n[7] D. Bommes, B. L\u00b4evy, N. Pietroni, E. Puppo, C. Silva, M.\nTarini, and D. Zorin. State of the art in quad meshing. In\nSTARs, 2012. 3\n[8] Mario Botsch and Olga Sorkine. On linear variational sur-\nface deformation methods. TVCG, 2008. 3\n[9] Mario Botsch, Leif Kobbelt, Mark Pauly, Pierre Alliez, and\nBruno L\u00b4evy. Polygon Mesh Processing. AK Peters, 2010. 3\n[10] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 2, 3, 8\n[11] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.\nSheikh. Openpose: Realtime multi-person 2d pose estima-\ntion using part affinity fields. 2019. 3\n[12] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In CVPR, 2022. 3\n[13] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\nlis Savva, Shuran Song, Hao Su, et al.\nShapeNet: An\ninformation-rich 3D model repository.\narXiv:1512.03012,\n2015. 3\n[14] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-XL: A\nuniverse of 10M+ 3D objects. In NeurIPS, 2023. 3\n[15] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3D objects. In CVPR, 2023. 2, 3\n[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. In NeurIPS, 2021. 3\n[17] Ana Dodik, Oded Stein, Vincent Sitzmann, and Justin\nSolomon. Variational barycentric coordinates. TOG, 2023.\n3\n[18] Gerald Farin. Curves and Surfaces for CAGD: A Practical\nGuide. Morgan Kaufmann Publishers Inc., 5th edition, 2001.\n3\n[19] Leon A Gatys, Alexander S Ecker, and Matthias Bethge.\nA neural algorithm of artistic style.\narXiv preprint\narXiv:1508.06576, 2015. 3\n[20] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial networks. In\nNeurIPS, 2014. 3\n[21] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStyleNeRF: A style-based 3D-aware generator for high-\nresolution image synthesis. In ICLR, 2022. 3\n[22] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-Xin\nZou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. three-\nstudio: A unified framework for 3D content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 4, 7\n[23] Richard Hartley and Andrew Zisserman. Multiple view ge-\nometry in computer vision.\nCambridge university press,\n2003. 3\n[24] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In ICLR, 2023. 2, 3\n[25] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\n9\nTan. LRM: Large reconstruction model for single image to\n3D. arXiv:2311.04400, 2023. 2\n[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In\nICLR, 2021. 7\n[27] Autodesk Inc. AutoDesk 3ds Max 2023. https://www.\nautodesk.com/products/3ds- max/overview,\n2023. 2\n[28] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, 2017. 3\n[29] Alec Jacobson, Ilya Baran, Jovan Popovi\u00b4c, and Olga Sorkine.\nBounded biharmonic weights for real-time deformation.\nTOG, 2011. 4\n[30] Alec Jacobson, Zhigang Deng, Ladislav Kavan, and JP\nLewis. Skinning: Real-time shape deformation. TOG, 2014.\n3, 4\n[31] Tao Ju, Scott Schaefer, and Joe Warren. Mean value coordi-\nnates for closed triangular meshes. TOG, 2005. 3\n[32] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 3\n[33] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of stylegan. In CVPR, 2020. 3\n[34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, 2023. 2\n[35] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv:2304.02643, 2023. 4\n[36] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. TOG, 2020. 4\n[37] Peizhuo Li, Kfir Aberman, Rana Hanocka, Libin Liu, Olga\nSorkine-Hornung, and Baoquan Chen. Learning skeletal ar-\nticulations with neural blend shapes. TOG, 2021. 8\n[38] Yukang Lin, Haonan Han, Chaoqun Gong, Zunnan Xu,\nYachao Zhang, and Xiu Li. Consistent123: One image to\nhighly consistent 3D asset using case-aware diffusion priors.\narXiv:2309.17261, 2023. 2\n[39] Pengyang Ling, Lin Chen, Pan Zhang, Huaian Chen, and Yi\nJin. FreeDrag: Point tracking is not you need for interactive\npoint-based image editing. arXiv:2307.04684, 2023. 2, 3\n[40] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen, Zexi-\nang Xu, Hao Su, et al.\nOne-2-3-45: Any single image\nto 3D mesh in 45 seconds without per-shape optimization.\narXiv:2306.16928, 2023. 2, 3\n[41] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3D object. In ICCV, 2023. 2, 3, 4\n[42] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. SyncDreamer: Gen-\nerating multiview-consistent images from a single-view im-\nage. arXiv:2309.03453, 2023.\n[43] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann,\nChristian Theobalt,\net al.\nWon-\nder3D: Single image to 3D using cross-domain diffusion.\narXiv:2310.15008, 2023. 2\n[44] William E Lorensen and Harvey E Cline. Marching cubes: A\nhigh resolution 3D surface construction algorithm. In Semi-\nnal graphics: pioneering efforts that shaped the field. ACM,\n1998. 4\n[45] Nadia. Magnenat-Thalmann, Richard Laperri`ere, and Daniel\nThalmann. Joint-dependent local deformations for hand ani-\nmation and object grasping. In GI. CIPS, 1989. 2\n[46] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In ICLR, 2022. 5, 8\n[47] Oscar Michel, Anand Bhattad, Eli VanderBilt, Ranjay Kr-\nishna, Aniruddha Kembhavi, and Tanmay Gupta.\nOB-\nJECT 3DIT: Language-guided 3D-aware image editing.\narXiv:2307.11073, 2023. 2, 3, 7, 8\n[48] MidJourney. MidJourney. www.midjourney.com. 3\n[49] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, 2023. 3\n[50] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. TOG, 2022. 7\n[51] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. 2022. 3\n[52] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3d representations without 3d supervision. In\nCVPR, 2020. 3\n[53] OpenAI. DALL\u00b7E 3 System Card. https://openai.\ncom/research/dall-e-3-system-card, 2023. 3,\n8\n[54] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag Your\nGAN: Interactive point-based manipulation on the generative\nimage manifold. In TOG, 2023. 2, 3\n[55] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. StyleCLIP: Text-Driven Manipulation\nof StyleGAN Imagery. In ICCV, 2021. 3\n[56] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In CVPR, 2023. 3\n[57] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. SDXL: Improving latent diffusion mod-\nels for high-resolution image synthesis. arXiv:2307.01952,\n2023. 7, 12\n[58] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D diffusion. ICLR,\n2023. 3, 4\n10\n[59] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3D object generation using both\n2D and 3D diffusion priors. arXiv:2306.17843, 2023. 2, 3\n[60] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv:2204.06125, 2022. 1, 3\n[61] Ren\u00b4e Ranftl,\nKatrin Lasinger,\nDavid Hafner,\nKonrad\nSchindler, and Vladlen Koltun. Towards robust monocular\ndepth estimation: Mixing datasets for zero-shot cross-dataset\ntransfer. TPAMI, 2020. 3, 8\n[62] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon objects in 3D: Large-scale learning and evaluation of\nreal-life 3D category reconstruction. In ICCV, 2021. 3\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 3, 4\n[64] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 5, 8, 12\n[65] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. In NeurIPS, 2022. 1, 3\n[66] Thomas W. Sederberg and Scott R. Parry. Free-form defor-\nmation of solid geometric models. In Computer Graphics.\nACM, 1986. 3\n[67] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter-\npreting the latent space of gans for semantic face editing. In\nCVPR, 2020. 3\n[68] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv:2310.15110, 2023. 2\n[69] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 3\n[70] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai.\nDragDiffusion: Harnessing\ndiffusion models for interactive point-based image editing.\narXiv:2306.14435, 2023. 2, 3, 7, 8\n[71] Peter Shirley and Steve Marschner. Fundamentals of Com-\nputer Graphics. AK Peters, 2009. 4\n[72] Eftychios Sifakis and Jernej Barbic.\nFem simulation of\n3d deformable solids: A practitioner\u2019s guide to theory, dis-\ncretization and model reduction. TOG, 2012. 3\n[73] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks: Continuous 3D-\nstructure-aware neural scene representations.\nIn NeurIPS,\n2019. 3\n[74] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2021. 5, 12\n[75] Olga Sorkine and Marc Alexa. As-rigid-as-possible surface\nmodeling. In SGP, 2007. 2, 3\n[76] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In CVPR, 2023. 2, 3, 5, 7, 12\n[77] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2D diffusion models for 3D generation. In CVPR,\n2023. 3\n[78] Zhihao Wang, Jian Chen, and Steven CH Hoi. Deep learning\nfor image super-resolution: A survey. TPAMI, 43(10):3365\u2013\n3387, 2020. 5\n[79] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. In NeurIPS, 2023. 3\n[80] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph\nFeichtenhofer, and Georgia Gkioxari. Multiview compres-\nsive coding for 3d reconstruction. In CVPR, 2023. 3\n[81] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace\nanalysis: Disentangled controls for stylegan image genera-\ntion. In CVPR, 2021. 3\n[82] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.\nTedigan: Text-guided diverse face image generation and ma-\nnipulation. In CVPR, 2021. 3\n[83] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In ICCV, 2015. 3\n[84] Jonathan Young. xatlas. https://github.com/jpcy/\nxatlas, 2023. 4\n[85] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021. 3\n[86] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation.\nTMLR,\n2023. 1\n[87] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 3, 5, 7, 8\n[88] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A\nEfros.\nUnpaired image-to-image translation using cycle-\nconsistent adversarial networks. In ICCV, 2017. 3\n11\nA. Generative Enhancement Details\nAlgorithm 1: Generative Enhancement\nDefine: Pre-trained 2D text-to-image diffusion model M,\ninput image I, coarse image Ic, enhanced image If,\ninversion prompt yinv, prompt y, depth map D\n\u02c6\nM \u2190FINE-TUNE DREAMBOOTH(I)\nxc\n0, . . . xc\nT \u2190 DDIM-INVERSION(Ic, yinv, D; \u02c6\nM);\nxf\nT \u2190 xc\nT\nfor t \u2190 T to 0 do\nf c\nt, Ac\nt \u2190 \u03f5\u03b8(xc\nt, yinv, D, t; \u02c6\nM)\nf f\nt , Af\nt \u2190 \u03f5\u03b8(xf\nt , y, D, t; \u02c6\nM)\nif t > \u03c4f then f f\nt \u2190 f c\nt\nif t > \u03c4A then Af\nt \u2190 Ac\nt\n\u03f5f\nt\u22121 \u2190 \u03f5\u03b8(xf\nt , y, D, t; f f\nt , Af\nt ; \u02c6\nM)\nxf\nt\u22121 \u2190 DDIM-DENOISING(xf\nt , \u03f5f\nt\u22121; \u02c6\nM)\nend\nResult: If \u2190 DECODER(xf\n0)\nThe generative enhancement pipeline starts with fine-\ntuning DreamBooth [64] with the input image.\nSubse-\nquently, we apply depth control DDIM inversion [74] to the\ncoarse rendering image. The prompt yinv, which describes\nthe coarse rendering, is used to obtain the inverted latent\nfor each time step. During each denoising step, we denoise\nthe inverted latent of the coarse rendering and the latent of\nthe refined image, extracting their respective feature maps,\nf c\nt and f f\nt , as well as their self-attention maps Ac\nt and Af\nt .\nThis step is formulated as:\n(f c\nt, Ac\nt) \u2190 \u03f5\u03b8(xc\nt, yinv, D, t)\n(f f\nt , Af\nt ) \u2190 \u03f5\u03b8(xf\nt , y, D, t)\nHere, \u03f5\u03b8(\u00b7) is the text-to-image diffusion model, specifically\nin our context, the Stable Diffusion XL [57] model. For the\ncoarse rendering, the latent is denoted by xc\nt, the inversion\nprompt by yinv, and the depth map by D. For the refined\nimage, the latent is represented by xf\nt , and the prompt by y.\nFollowing Plug-and-Play [76], we replace the feature and\nself-attention maps of the enhanced image with those from\nthe coarse input:\n\u03f5f\nt\u22121 = \u03f5\u03b8(xf\nt , y, D, t; f f\nt , Af\nt )\nHere \u03f5\u03b8(\u00b7; f f\nt , Af\nt ) represents the model with replaced fea-\nture and self-attention maps, and \u03f5f\nt\u22121 is the prediction for\nthe refined image. Replacement stops once the current time\nstep is below the thresholds \u03c4f and \u03c4A. The threshold is im-\nportant because the feature/self-attention maps may contain\nundesired artifacts from coarse 3D reconstruction and mesh\ndeformation.\nB. Implementation Details\nIn this section, we provide implementation details of all our\n6 tasks.\nPose Editing Pose editing is carried out by manually cre-\nating a skeleton for each 3D model and computing its skin-\nning weights [6]. The object\u2019s pose is edited by adjusting\nthe skeleton\u2019s bones. A text prompt is not required to de-\nscribe the pose.\nRotation\nRotation is achieved by spinning the 3D model\naround its centroid. This allows us to rotate the model at\nany angle and then render it back into a 2D image. How-\never, it becomes challenging to discern the viewpoint (e.g.,\nfront, back, or side), given only the coarse rendering image.\nOptional text prompts are helpful in guiding the denoising\nstep and preventing the Janus Problem. If the rotation an-\ngle ranges from [\u221245\u25e6, 45\u25e6], we add \u201cfront view\u201d to the text\nprompt. For angle between [135\u25e6, 225\u25e6], we append \u201cback\nview\u201d to the text prompt. For all other angles, we use \u201cside\nview\u201d.\nTranslation\nTranslation can be done by moving the 3D\nmodel within the 3D space in any direction and over any\nspecific distance. As the translated model is rendered, the\ncamera perspective adjusts accordingly. As illustrated in\nFig 6, moving the dog or the truck closer to the camera re-\nsults in an enlarged image of the object, consistent with the\ncamera\u2019s perspective.\nComposition Our method allows for the addition of artist-\ncreated 3D objects to the scene. In Fig 6, we insert various\nmodels into the scene. Despite the 3D models not being\nof high quality, our coarse-to-fine strategy significantly en-\nhances their detail, as evident in the tiger example where\nthe texture displays hair details and a realistic face in the\nfinal output, blending well with the environment. Note that\nthese models are not used for fine-tuning during Dream-\nBooth training. In certain cases, text prompts prove helpful\nin guiding the denoising step and supplementing our geo-\nmetric guidance.\nCarving\nBeyond mesh deformation, our method enables\ncutting and removing parts of the mesh through the use of\nmolds. In Fig 6, a moon-and-star-shaped mold is positioned\nagainst a pumpkin\u2019s surface. By calculating and excising\nthe overlapping areas, the resulting mesh resembles a finely\ncarved pumpkin in specific shapes.\nSerial Addition\nSimilar to composing elements, we can\ntake meshes reconstructed from images and integrate them\ninto the scene one by one. In Fig 6, we adjust each fish and\nduck\u2019s size, pose and their orientation before adding it to the\nscene. Our approach realistically merges the coarse 3D fish\nmodels into the scene, maintaining a realistic appearance\neven with reflections on the water\u2019s surface.\n12\nAdd cherries & Cut off a slice\n1. Add a Joker\n2. Add a plant\n3. Add a jocker and a plant\nReplace the dog with a tiger\n1. Add a banana\n2. Add a tiger\n1. Add a hat\n2. Add a parrot\nCut off a slice\nCarve into a jack o' lantern\nCarve into a jack o' lantern\nAdd cherries\nAdd ducks\nAdd fish\nMove the truck closer\nMove the dog closer\nMove the fish to the top\n1. Floating with outstretched \narms and slightly bent legs.\n2. Floating with arms extended \nand legs casually apart\n3. In a swimming pose\nRotation only\nBend the legs\nDancing with one arm raised \nand one leg bent\nDancing with one leg extended \nback, arms open\nIn Spider-Man\u2019s hand pose\nBend the worm\nStanding with arms \noutstretched to the sides and \nknees slightly bent.\nPose editing\nPose editing and rotation\nTranslation\nComposition\nCarving\nSerial Addition\nFigure 13. All 28 edits and 15 input images of our SculptingBench. We provide textual descriptions of the edits here. However, in practice\nwe aim to make precise, quantifiable edits directly to 3D models, without relying on text prompts.\n13\nC. SculptingBench\nOur SculptingBench dataset comprises 28 edits applied to\n15 images, encompassing each of the 6 editing tasks we\nhave developed. The full dataset is illustrated in Fig. 13.\nThese instances present significant challenges to current ob-\nject editing techniques, thereby serving as an ideal platform\nfor testing and developing precise object editing methods.\nD. Failure Cases\nOne key limitation lies in its dependence on the quality and\nreliability of single-view reconstruction techniques, partic-\nularly when dealing with unseen perspectives. Any errors\nin this process can result in editing failures.\nAs demonstrated in Fig 14, the reconstruction occasion-\nally fails to produce detailed textures, leading to a blurred\nface in the top row example. Challenges also arise in mesh\nreconstruction and extraction. The middle row displays ar-\ntifacts beneath the man\u2019s armpit, stemming from imprecise\nreconstruction in that region. In the bottom row example,\nwrong color reconstruction resulted in an less realistic final\ncolor in the output.\nTexture details\nMesh shape\nColor\nInput Image\nCoarse Rendering\nOur Result\nFigure 14. Failure cases due to inaccurate reconstruction of tex-\nture, geometry, and color.\n14\n"
  },
  {
    "title": "Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions",
    "link": "https://arxiv.org/pdf/2401.01827.pdf",
    "upvote": "13",
    "text": "MoonShot: Towards Controllable Video Generation and Editing with\nMultimodal Conditions\nDavid Junhao Zhang\u2666\u2662, Dongxu Li\u2666, Hung Le\u2666, Mike Zheng Shou\u2662*, Caiming Xiong\u2666, Doyen Sahoo\u2666\n\u2666Salesforce Research\n\u2662Show Lab, National University of Singapore\nhttps://showlab.github.io/Moonshot/\nZero-Shot Subject Customized Video Generation\nA robot is landing to fly\nA robot is walking\nImage Animation\nGiven Image\nGiven Image\nA can running\nwith heavy snow\nWearing a suit\nDirectly Use Image ControlNet\nVideo Editing\nFigure 1. Our foundational video diffusion model, underpinned by multimodal conditioning, effectively facilitates image animation, the\ncreation of customized videos, and precise control over geometric structures through the image ControlNet. Additionally, it enables video\nediting guided by text and image inputs, producing better results than recent state-of-the-art methods in these applications.\nAbstract\nMost existing video diffusion models (VDMs) are limited\nto mere text conditions. Thereby, they are usually lacking\nin control over visual appearance and geometry structure\nof the generated videos.\nThis work presents MoonShot,\na new video generation model that conditions simultane-\nously on multimodal inputs of image and text. The model\nbuilts upon a core module, called multimodal video block\n(MVB), which consists of conventional spatialtemporal lay-\ners for representing video features, and a decoupled cross-\nattention layer to address image and text inputs for ap-\npearance conditioning.\nIn addition, we carefully design\nthe model architecture such that it can optionally integrate\nwith pre-trained image ControlNet modules for geometry\nvisual conditions, without needing of extra training over-\nhead as opposed to prior methods. Experiments show that\nwith versatile multimodal conditioning mechanisms, Moon-\nShot demonstrates significant improvement on visual qual-\nity and temporal consistency compared to existing models.\nIn addition, the model can be easily repurposed for a vari-\nety of generative applications, such as personalized video\n\u2217Corresponding Author\ngeneration, image animation and video editing, unveiling\nits potential to serve as a fundamental architecture for con-\ntrollable video generation. Models will be made public on\nhttps://github.com/salesforce/LAVIS.\n1. Introduction\nRecently, text-to-video diffusion models (VDMs) [5, 12, 15,\n21, 24, 50, 60, 78, 82, 83] have developed significantly,\nallowing creation of high-quality visual appealing videos.\nHowever, most existing VDMs are limited to mere text con-\nditional control, which is not always sufficient to precisely\ndescribe visual content. Specifically, these methods are usu-\nally lacking in control over the visual appearance and ge-\nometry structure of the generated videos, rendering video\ngeneration largely reliant on chance or randomness.\nIt is well acknowledged that text prompts are not suffi-\ncient to describe precisely the appearance of generations,\nas illustrated in Fig. 2. To address this issue, in the con-\ntext of text-to-image generation, efforts are made to achieve\npersonalized generation [9, 31, 34, 46, 75] by fine-tuning\ndiffusion models on input images. Similarly for video gen-\neration, AnimateDiff relies on customized model weights\nto inject conditional visual content, either via LoRA [27] or\narXiv:2401.01827v1  [cs.CV]  3 Jan 2024\nWhat does the\ncar look like?\nText prompt\nA car running\nin the desert\n\u2026\nA car running\nin the desert\nImage prompt\nText prompt\n\u2026\n(a) Video with text only \n(b) Video with text and image\nFigure 2. A single text prompt (a) lacks the precision and detail for\naccurate subject description. However, a picture (b) conveys much\nmore, worth a thousand words. By combining a picture with text,\nwe can produce videos that more closely match user requirements.\nDreamBooth tuning [46]. Nonetheless, such an approach\nincurs repetitive and tedious fine-tuning for each individ-\nual visual conditional inputs, hindering it from efficiently\nscaling to wider applications. This inefficiency, as hinted\nby the prior work [34], stems from the fact that most pre-\ntrained text-to-video models are not able to condition both\non images and text inputs. To overcome this issue, we in-\ntroduce a decoupled multimodal cross-attention module to\nsimultaneously condition the generation on both image and\ntext inputs, thus facilitating to better control the visual ap-\npearance, while minimizing required fine-tuning efforts and\nunlocking zero-shot subject customized video generation.\nIn terms of geometric structure control, despite methods\nsuch as ControlNet [79] and T2I-Adapter [38] are devel-\noped to leverage depth, edge maps as visual conditions for\nimage generation, analogous strategies for video synthesis\nremain indeterminate. Among the few existing attempts,\nVideoComposer [63] adds video-specific ControlNet mod-\nules to video diffusion models (VDMs) and subsequently\nre-trains the added modules from scratch, incurring substan-\ntial extra training overhead. In contrast, alternative meth-\nods [10, 81] reuse pre-trained ControlNet modules for im-\nages. However, they require adapting text-to-image mod-\nels for video generation via frame propagation [16, 74] or\ncross-frame attention [29, 68], resulting in subpar temporal\nconsistency compared to those based on VDMs.\nBuilding upon the aforementioned observations, our ob-\njective is to explore a model architecture that combines the\nstrengths of both realms.\nNamely, we expect the model\nto be a high quality VDM that produces consistent video\nframes, while also able to directly leverage pre-trained im-\nage ControlNet to condition on geometry visual inputs. In\nthis regard, we observe that as shown in Fig. 3(b), prior\nwork [13, 60, 68, 83] typically insert temporal modules,\nsuch as temporal convolutions layers, in between of spa-\ntial modules, usually before self-attention and after spatial\nconvolution layers. This design modifies the spatial feature\nvisual quality\nimage ControlNet\nspatial layers\nresnet2d\nself-attn\ntext-attn\nimage-attn\ntemporal layers\ninsert\ntemp-conv\ntemp-attn\nfrozen/train\n(a) original\n(b) composed\n(c) decomposed\n(d) MVB\nFigure 3. Different designs of spatial-temporal modules include:\n(a) the original spatial module from U-Net, (b) a temporal module\nadded within the spatial module, which hinders the image control-\nnet, and (c) a temporal module appended after the spatial module,\nallowing for image control network functionality but failing to pro-\nduce high-quality videos with text-only conditioning. In contrast,\nour MVB block, conditioned on both image and text, enables the\nimage controlnet and can generate high-quality videos.\ndistribution, thereby making direct integration with image\nControlNet not feasible. In contrast, AnimateDiff [19] dis-\ncards temporal convolution layers and only inserts temporal\nattention layers after spatial layers, as shown in Fig. 3(c).\nSuch design preserves spatial feature distribution thus fa-\ncilitating immediate reuse of image ControlNet. However,\nit depends exclusively on text conditions, which do not of-\nfer sufficient visual cues. Consequently, temporal modules\nhave to learn extra spatial information for compensation,\nwhich diminishes the focus on temporal consistency, caus-\ning increased flickering and a decline in video quality.\nTo address these issues, we introduce MoonShot, a video\ngeneration model that consumes both image and text con-\nditional inputs.\nThe foundation of the model is a new\nbackbone module for video generation, called multimodal\nvideo block (MVB). Specifically, each MVB highlights\nthree main design considerations:\n\u2022 a conventional spatial-temporal module for video genera-\ntion, which in order consists of a spatial convolution layer,\na self-attention layer and a temporal attention layer that\naggregates spatial features. Such a design allows reuse of\npre-trained weights from text-to-image generation mod-\nels without altering its spatial feature distribution, thus\nsubsuming its generation quality.\n\u2022 a decoupled multimodal cross-attention layer that condi-\ntions the generation on both text and image inputs. These\ntwo conditions complement each other to guide the gen-\neration. In addition, image input offers reference visual\ncues, allowing temporal modules to focus on video con-\nsistency. This improves overall generation quality and\nframe coherence, as evidenced experimentally;\n\u2022 optionally, since spatial feature distribution is preserved,\npre-trained image ControlNet modules can be immedi-\nately integrated to control the geometric structure of the\ngeneration, without needing of extra training overhead.\nAs a result, our model generates highly consistent videos\nbased on multimodal inputs, and can further utilize geom-\netry inputs, such as depth and edge maps, to control the\ncompositional layout of the generation. Moreover, thanks\nto its generic architecture with versatile conditioning mech-\nanisms, we show that MoonShot can be easily repurposed\nfor a variety of generative applications, such as image an-\nimation and video editing. Qualitative and quantitative re-\nsults show that MoonShot obtains superior performance on\npersonalized video generation, image animation and video\nediting. When provided with a video frame as the image\ncondition, MoonShot demonstrates competitive or better re-\nsults with state-of-the-art foundation VDMs, validating the\neffectiveness of the model.\n2. Related Work\nText to Video Generation. Previous research in this field\nhas leveraged a diversity of generative models, such as\nGANs [47, 49, 54, 55, 59], Autoregressive models [14, 25,\n33, 53, 72], and implicit neural representations [51, 77].\nDriven by the diffusion model\u2019s significant achievements in\nimage synthesis, a number of recent studies have explored\nthe application of diffusion models in both conditional and\nunconditional video synthesis [1, 6, 20, 26, 28, 36, 40, 57,\n57, 62, 67, 68, 73, 83]. Most existing approaches focus\non conditioning a single modality. For instance, Imagen\nVideo [24] and Make-A-Video are conditioned exclusively\non text, while I2VGen-XL [80] relies solely on images.\nIn contrast, our method supports multimodal conditions\nwith both image and text, enabling more precise control.\nVideo Editing and ControlNet. Beginning with Tune-A-\nVideo [68], numerous works [7, 8, 16, 18, 29, 44, 61, 74]\nadapt the Text to Image (T2I) Model [45] for video edit-\ning. These methods introduce additional mechanisms like\ncross-frame attention, frame propagation, or token flow to\nmaintain temporal consistency. While Dreammix [37] uses\nthe VDM for video editing, it necessitates source video fine-\ntuning. In contrast, our method directly employs VDM for\nvideo editing, achieving temporally consistent videos with-\nout the need for fine-tuning or complex designs.\nControlNet [79] and T2I-Adapter [38] utilize structural\nsignals for image generation.\nWhile ControlVideo [81]\napplies this approach with a T2I model for video genera-\ntion, resulting in limited temporal consistency. Control-A-\nVideo [10] integrates a temporal layer into both the base\nVDM and ControlNet, training them jointly. Gen-1 [12]\nconcatenates structural signals with noise as the UNet input.\nHowever, without these signals, the base VDMs in Control-\nA-Video and Gen-1 will fail. Videocomposer [63] incurs\nhigh training costs by adding a video controlnet after VDM\ntraining. Our method stands out as our VDM independently\nproduces high-quality videos and directlu uses image Con-\ntrolNets without extra training.\nGeneration Model Customization.\nCustomizing large\npre-trained foundation models [11, 17, 32, 46, 52, 65]\nenhances user-specific preferences while retaining robust\ngenerative capabilities.\nDreambooth [46] leads the way\nin this field, though it requires extensive fine-tuning. IP\nadapter [76] and BLIP-Diffusion [34] achieve zero-shot\ncustomization using an additional image cross-attention\nlayers and text-aligned image embeddings.\nAnimated-\niff [19] pioneers subject customization in videos but needs\nadditional Dreambooth training for the new subject.\nOur method differs from IP-adapter in image domains by\nintroducing a decoupled image-text cross-attention layer in\nthe video block. We explore its effectiveness in facilitating\nhigh-quality and smooth video creation and further apply it\nto video tasks involving image animation and video editing.\nUnlike Animatediff, our approach enables zero-shot subject\ncustomization directly in videos.\nImage Animation. This task aims to generate subsequent\nframes given an initial image. Existing methods [2, 30, 30,\n35, 35, 39, 41, 70] have focused on specific domains like hu-\nman actions and nature scenes, while recent advancements\nlike I2VGen-XL [80], DynamicCrafter [69], and Video-\ncomposer [63] target open-domain image animation with\nvideo diffusion models. However, they often struggle to\nmaintain the appearance of the conditioning image.\nIn contrast, our approach, using masked condition and\ndecoupled image-text attention, effectively aligns the first\nframe of the animated video with the provided image, en-\nsuring a more accurate retention of its original identity. Our\nwork is concurrent with DynamicCrafter [69] (Oct.,2023)\nand we make comparisons in Fig. 7.\n3. Model Architecture and Adaptations\nText-to-video latent diffusion models generate videos by\ndenoising a sequence of Gaussian noises with the guidance\nof text prompts. The denoising network \u03b8 is usually a U-\nNet-like model, optimized by a noise prediction loss:\nL = Ez0,y,\u03f5\u223cN (0,I ),t\u223cU(0,T)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, y))\u22252\n2\n\u0003\n,\n(1)\nwhere z0 is the latent code of training videos from VAE en-\ncoder, y is the text prompt, \u03f5 is the Gaussian noise added\nto the latent code, t is the time step and \u03f5\u03b8 is the noise pre-\ndiction by the model. In the following, we first introduce\nthe MoonShot model architecture including its core com-\nponent, multimodal video block. Then, we showcase the\ncapabilities of the model by describing methods to repur-\npose the model for various video generation tasks, such as\nLatent\nFeature\nW!\n\"\n#\"\n#\n#$\n#\nW\"\nW$\nImage \nFeature\n$#\n%#\n%\n$\nText \nFeature\n\u2026\nA corgi running on the grass\nImage     \nEncoder\nText\nEncoder\n\u2130\n!%%\n!%&\nVideo\nLatents\n\u2026\n!%'\n!(%\n!(&\n\u2026\nNoise\nUNet\nDown Block\nUp Block\nImage Cross-Attn\nText  Cross-Attn\n!('\n\u2026\n\u2026\nPrediction\nFigure 4. The overall workflow and the structure of our decoupled multimodal cross-attention layers. In the training phase, we use the\ninitial frame of the video as the image condition. For inference, the model accepts any image along with accompanying text.\n0\nz !\"\n1\n0\n0\n0\nz\"\n\"\nImage animation\nnoise\ncondition\ncondition  \nmask\nz !\n#\nz\"\n\"\nFigure 5. Masked condition for image animation.\ngeometry controlled video generation, image animation and\nvideo editing.\n3.1. Multimodal Video Block\nOur model architecture builds upon multimodal video\nblocks (MVB). There are three primary objectives under-\nlying the design of this key module. First, we aim for the\nmodel to consistently produce video frames of high qual-\nity. Second, it is desired for immediate integration of pre-\ntrained image ControlNet. In this way, we can facilitate the\nuse of geometric images for controlling the compositional\nlayout without extra training. Third, the model is expected\nto accommodate multimodal text and image inputs for bet-\nter visual appearance conditioning. To this end, each MVB\nconsists of two groups of layers, spatialtemporal U-Net lay-\ners and decoupled multimodal cross-attention layers as de-\ntailed in order below.\nSpatialtemporal U-Net Layers.\nTypical U-Net in text-\nto-image models consists in order of a spatial convolu-\ntion layer (ResNet2D), a self-attention layer, and a cross-\nattention layer that conditions the generation on texts. Prior\nwork [5, 50, 60] adapts these models for video generation\nby introducing additional temporal convolution and atten-\ntion layers. As shown in Fig. 3(b), the temporal convolution\nlayer is usually inserted before each self-attention layer and\nafter the spatial convolution layer. While this architecture\nmay enhance temporal consistency, it alters the distribution\nof the spatial feature baked in the pre-trained text-to-image\ngeneration models. As a result, the model not only loses\nthe ability of text-to-image generation, but also becomes in-\ncompatible with established techniques developed for text-\nto-image models, such as ControlNet, making direct inte-\ngration of these techniques infeasible.\nDifferently, we observe that the addition of temporal at-\ntention layers after the cross-attention layer does not signif-\nicantly modify the spatial feature distribution, while con-\ntributing effectively for temporal feature aggregation. By\nfreezing the spatial layers during training, we can reuse\nControlNet to condition the generation on geometry visual\ninputs by broadcasting it along the temporal axis, as shown\nin Fig. 3(c). In particular, we use space-time attention sim-\nilar to [4], where each patch attends to those at the same\nspatial location and across frames.\nDecoupled Multimodal Cross-attention Layers. Most ex-\nisting video generation models use a cross-attention module\nto condition the generation on texts. Given fy the embed-\nding of text prompts, diffusion models condition on it to\nenhance the U-Net features fx via cross-attention layers,\nwhere the query Q is obtained from U-Net features fx, key\nK and value V are from the text embedding fy. The cross-\nattention operation is then defined as:\n(\nQ = WQ \u00b7 fx; K = WK \u00b7 fy; V = WV \u00b7 fy;\nCrossAttention(Q, K, V) = softmax( QKT\n\u221a\nd ) \u00b7 V,\n(2)\nwhere Q \u2208 RBN\u00d7H\u00d7W \u00d7C, K, V \u2208 RBN\u00d7L\u00d7C, with B\nthe batch size, N the number of frames, H the height, W\nthe width and C the number of channels, L the number of\ntext tokens, d the hidden size. Note that text embeddings\nare duplicated for video frames.\nThere are two issues with this design.\nFirst, relying\nsolely on text prompts usually proves inadequate in accu-\nrately describing highly customized visual concepts for the\ndesired generation. Second, specifically for video genera-\ntion, the absence of visual conditioning mechanism places\nexcessive burden on the temporal attention layers. They\nmust simultaneously ensure the consistency across frames\nand also preserve high-quality spatial features, often re-\nsulting in compromises on both fronts with flickered low-\nquality videos.\nTo address these issues, we introduce decoupled multi-\nmodal cross-attention, where one extra key and value trans-\nformation are optimized for image conditions, denoted as\nKI, V I \u2208 RBN\u00d7L\u00d7C. The attention is formulated as:\nCrossAttention(Q, K, V) + CrossAttention(Q, KI, VI).\n(3)\nThis approach enables the model to effectively manage both\nthe image and text conditions. Additionally, conditioning\non visual cues allows the subsequent temporal modules to\nfocus more on maintaining temporal consistency, resulting\nin smoother and higher-quality video outputs. With the ad-\nditional image condition, the training loss is thereby refor-\nmulated as:\nL = Ez0,y,y\u2032,\u03f5\u223cN (0,I ),t\u223cU(0,T)\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, y, y\u2032))\u22252\n2\n\u0003\n,\n(4)\nwhere y\u2032 is the image condition.\n3.2. Adapting for Video Generation Applications\nMasked Condition for Image Animation. With the addi-\ntional conditioning on image input, our model is a natural\nfit for the task of image animation, which aims to trans-\nform an input image into a short video clip with consistent\ncontent. To enhance the content consistency, we adapt the\nmask-conditioning mechanism introduced by [5] for image\nanimation. In particular, we use the first frame as an addi-\ntional input condition for the U-Net. Apart from the orig-\ninal four latent channels, as illustrated in Fig. 5, we add\nfive more channels to the U-Net\u2019s input. Among them, four\nchannels represent the replicated first frame latent z0\n0, and\none binary channel is used to denote the masked frames.\nThis approach encourages that the identity of the subject in\nthe animated video remains identical to that in the condi-\ntioning image. We observe that incorporating an extra im-\nage cross attention layer is essential for image animation. It\nhelps significantly to prevent sudden changes in appearance\nand reduce temporal flickering, which are seen commonly\nin models driven merely by text conditions.\nVideo Editing with Video Diffusion Models.\nDream-\nMix [37] shows that VDM can be repurposed for video\nediting, which however requires extensive fine-tuning. Our\nmodel is for general-purpose video generation, yet can\nbe used for video editing without needing of fine-tuning.\nSpecifically, for a selected encoded source video z0, we\nadd Gaussian noise using the DDPM [23] forward process.\nNext, we employ diffusion directly with VDM, conditioned\non both text and image. This process effectively replaces\nthe subject in the original video with that from the image\nWearing a glass\nImage\nControlNet\nText-Image to Video\nGiven Image\nA bear astronaut\nDepth Map\nWear a hat on the beach\nFigure 6. Subject customized video generation results with the\noption of utilizing an image controlnet network or not.\ncondition and incorporates visually appealing elements as\ndescribed in the text, resulting in a smooth edited video.\nGeometry Controlled Generation. Since our model pre-\nserves the spatial features of the pre-trained text-to-image\nmodels, we can directly integrate the image ControlNet for\ngeometry conditioning.\nTo achieve this, we attach pre-\ntrained ControlNet modules to the model. Then condition\nfeatures for each frame is added to the corresponding fea-\nture maps via residues. Due to the careful design of spatial-\ntemporal U-Net layers, we observe satisfactory geometry\ncontrol effect without needing of video-specific fine-tuning.\n4. Experiments\n4.1. Implementation Details\nOur spatial weights are initialized using SDXL [42], which\nare fixed throughout the training process. Initially, follow-\ning IP-Adpter [76], we train the image cross-attention layers\nusing the LAION [48] dataset at a resolution of 512 \u00d7 320.\nSubsequently, we keep the spatial weights unchanged and\nproceed to train only the temporal attention layers. This\ntraining step utilizes the WebVid10M [3] dataset, each clip\nsampled 16 frames at a 512\u00d7320 resolution, with condition-\ning on video captions as text conditions and the first frame\nas image conditions. Further refinement is carried out on\na set of 1000 videos from the InternVideo [64] collection,\naimed at removing watermarks. We use 16 A100 40G GPUs\nfor training. More details can be found in supplementary\nmaterial.\n4.2. Human Evaluations\nWe follow Make-a-Video [50] to perform human evalu-\nations on Amazon Mechanical Turk.\nIn the realm of\nvideo editing tasks (Tab. 3), in line with FateZero [44] and\nGiven Image\nFirst Frame\nOurs\nDynamiCrafter\n(VideoCrafter)\nVideocomposer Touch the helmet\nDynamiCrafter\n(VideoCrafter)\nOurs\nFirst Frame\nBig waves\nVideocomposer\nRobot is walking\nI2VGen-XL\nI2VGen-XL\nGiven Image\nFigure 7. Image animation. Comparing our method with I2VGEN-XL [80], DynamiCrafter [69], and VideoComposer [63], it stands out\nin its ability to control the animated video\u2019s first frame to match the provided image exactly, maintaining the identity of the given image\nmore precisely and animating the image according to text prompts. In contrast, I2VGen-XL and DynamicCrafter show relatively weaker\nidentity preservation in their animated videos, and their response to text prompts is less effective.\nModel\nDINO\nCLIP-I\nCLIP-T\nNon-Customized T2V\n0.283\n0.594\n0.296\nI2VGen-XL [80]\n0.542\n0.737\n0.218\nAnimateDiff [19]\n0.582\n0.784\n0.243\n300 finetune steps\nOurs (zero-shot)\n0.556\n0.763\n0.292\nOurs (80 finetune steps)\n0.624\n0.802\n0.292\nTable 1.\nSubject customized video generation performance on\nDreambooth dataset [46]. Higher metrics are better.\nRender-A-Video [74], we direct annotators to identify the\nmost superior outcomes from five methods, judging by three\nstandards: 1) the precision of prompt-to-edited-frame align-\nment, 2) temporal coherence of the video, and 3) the overall\nquality. In addition, for ControlNet evaluations (see Tab. 5),\nwe request annotators to judge whether the created video\nadheres to the control signal. Furthermore, regarding the\ntext-to-video ablation studies (Tab. 6), we invite annotators\nto evaluate the overall video quality, the accuracy of text-\nvideo alignment and motion fidelity.\n4.3. Subject Customized Generation\nQuantitative Results. To evaluate our method for subject-\ncustomized video generation, we perform experiments on\nthe DreamBooth [46] dataset, which includes 30 subjects,\neach with 4-7 text prompts. We utilize DINO and CLIP-I\nscores to assess subject alignment, and CLIP-T for video-\ntext alignment, calculating average scores for all frames. As\nshown in Tab. 1, our method achieves strong zero-shot cus-\ntomization, surpassing non-customized text-to-video (T2V)\nmodels by a large margin.\nDifferent from AnimateDiff,\nwhich requires repetitive re-training for new subjects, our\nmethod utilizes pre-trained decoupled multimodal attention\nlayers, achieving zero-shot customization with compared\nDINO (First)\nDINO (Avg)\nCLIP-T (Avg)\nGT\n0.781\n0.644\n\u2013\nI2VGen-XL [80]\n0.624\n0.573\n0.232\nVideoComposer [63]\n0.751\n0.285\n0.269\nOurs\n0.765\n0.614\n0.284\nTable 2. Image animation results. Our model shows better visual\nand textual alignment than competing methods.\nperformance. If fine-tuned with as few as 80 steps, our ap-\nproach further surpasses AnimateDiff by a significant mar-\ngin, demonstrating the effectiveness of our model.\nQualitative Results. As shown in Fig. 6, our model pro-\nduces customized videos that align with both the subject of\nimage condition and the text condition. Additionally, the\nimage ControlNet can be directly integrated to realize con-\ntrol over geometric structures.\n4.4. Image Animation\nQuantitative Results. To assess image animation capabil-\nities, we select 128 video-text pairs from the Webvid eval-\nuation set, covering diverse themes. We use DINO (First)\nto measure the similarity between the first frame of the an-\nimated video and the conditioning image, DINO (Average)\nfor the average similarity across all video frames compared\nto the conditioning image, and CLIP-T for the overall align-\nment between text prompts and the animated video across\nframes. As shown in Tab. 2, our method outperforms others\nin all metrics, demonstrating superior identity preservation,\ntemporal consistence and text alignment.\nQualitative Results. We compare our results qualitatively\nwith I2VGEN-XL [80], DynamiCrafter [69], and Video-\nComposer [63] as shown in Fig. 7, it\u2019s seen that the iden-\ntity or appearance in animated videos from I2VGEN-XL\nand DynamiCrafter is different from the original image.\nRender a Video\nFateZero + Tune a Video\nOurs\nA car running on the road, Van Gogh style.    +\nA  blue Lamborghini running on the road, Van Gogh style.\nA  blue Lamborghini running on the road, Van Gogh style.\nFigure 8. Visual comparisons with SOTA video editing methods.\nMetric\nFateZ [44]\nPix2V [7]\nT2V-Z [29]\nRender-A-V [74]\nOurs\nFram-Acc (\u2191)\n0.534\n0.978\n0.943\n0.959%\n0.976\nTem-Con (\u2191)\n0.953\n0.942\n0.963\n0.965\n0.986\nPixel-MSE (\u2193)\n0.092\n0.256\n0.091\n0.073\n0.064\nUser-Balance\n4.4%\n6.2%\n7.4%\n21.4%\n60.6%\nUser-Temporal\n3.6%\n2.0%\n3.8%\n18.2%\n72.4%\nUser-Overall\n3.1%\n3.1%\n7.0%\n24.6%\n62.2%\nTable 3. Quantitative comparisons and user preference rates for\nvideo editing task.\nWhile VideoComposer replicates the conditioning image as\nthe first frame, subsequent frames show abrupt changes in\nappearance, as indicated by its high DINO (First) and low\nDINO (Average) scores in Tab. 2. This issue may stem from\nits limited capacity to extract visual cues. Our method, in\ncontrast, utilizes multimodal cross-attention layers and con-\ndition masks, and excels by promoting the similarity be-\ntween the first frame of the animated video and the con-\nditioning image, maintaining more effectively appearance,\nand enabling animation in line with text prompts.\n4.5. Video Editing\nQuantitative Results. We compare with four video edit-\ning methods: FateZero [44], Pix2Video [7], Text2Video-\nZero [29], and Render-A-Video [74].\nNotably, Render-\nA-Video and Text2Video-Zero employ customized models\nthat incorporate ControlNet. In contrast, our approach uti-\nlizes the base VDM model without integrating ControlNet.\nFollowing the FateZero and Pix2Video, we utilize 72 videos\nfrom Davis [43] and various in-the-wild sources. We report\nthree metrics in Tab. 3: Fram-Acc, a CLIP-based measure\nof frame-wise editing accuracy; Tmp-Con, assessing the co-\nsine similarity between consecutive frames using CLIP; and\nModels\nFID-vid (\u2193)\nFVD (\u2193)\nCLIP-T (\u2191)\nN \u00a8UWA [67]\n47.68\n-\n0.2439\nCogVideo (Chinese) [25]\n24.78\n-\n0.2614\nCogVideo (English) [25]\n23.59\n1294\n0.2631\nMagicVideo [83]\n-\n1290\n-\nVideo LDM [5]\n-\n-\n0.2929\nMake-A-Video [50]\n13.17\n-\n0.3049\nModelScopeT2V [60]\n11.09\n550\n0.2930\nOurs\n10.98\n542\n0.3068\nTable 4. Quantitative comparisons on MSR-VTT [71].\nPixel-MSE, the averaged mean-squared pixel error between\naligned consecutive frames. Our method excels in tempo-\nral consistency and ranks second in frame editing accuracy.\nAnd our method achieves better human preference on all\nevaluation metrics (Sec. 4.2). In fact, prior methods typi-\ncally utilize image models with frame propagation or cross-\nframe attention mechanisms, which tend to yield worse tem-\nporal consistency compared to our approach. This demon-\nstrates the clear advantage of using foundation VDMs for\nvideo editing, compared to those relying on image models.\nQualitative Results.\nAs shown in Fig. 8, FateZero re-\nconstructs the input sports car frame well but the result is\nnot aligned with the prompt. Render-A-Video struggles to\nswap the jeep to a sports car, facing challenges with shape\nchanges. Conversely, our method adeptly replaces the jeep\nwith a sports car as specified in the conditioning image\nwhile also adhering to the text prompts.\n4.6. Text to Video Generation\nSince the spatial layers are frozen during training, we first\ngenerate an image according to the text prompt. The image\nis later combined with text for multimodal conditioned gen-\neration. We use MSR-VTT dataset [71] to evaluate qual-\nity of zero-shot generation, whose test set contains 2,990\nvideos of 320x240 resolution, accompanied by 59,794 cap-\ntions in total. We compare our model with state-of-the-art\nmethods as shown in Tab. 4. Our method achieves the best\nresults across FID-vid [22], FVD [56], and CLIP-T [66],\ndemonstrating better visual quality and text alignment.\n4.7. Ablation Studies\nSpatial temporal modules designs. We investigate the de-\nsign of temporal modules that allows direct integration of\nthe image ControlNet. As indicated in Tab 5, the design\nwhich inserts the temporal convolution within spatial mod-\nules (Fig.3b) alters the original spatial features, rendering\nthe image ControlNet ineffective, even with fixed spatial\n(fs) weights during video training. Conversely, placing the\ntemporal attention after all spatial modules in each block\nand fixing spatial weight during video training renders the\nimage ControlNet feasible.\nImpact of image condition on video consistency and\nquality. In Tab. 6, we explore the impact of mutlimodal\nonly text\ncross-attn\ndecoupled\nmultimodal\ncross-attn\nAn astronaut walking on the moon\nappearance flickering and suddenly change \nappearance and smooth motion\ntwo consecutive frames\ntwo consecutive frames\nFigure 9. Comparison between text-only and multimodal conditioned VDMs. We compare MoonShot with AnimateDiff-XL [19], which\nuses only text conditions. Despite trained on high-quality internal data, AnimateDiff-XL suffers appearance flickering and abrupt changes.\nIn contrast, our multimodal-conditioned model trained on a public dataset shows improved temporal consistency and visual quality.\nonly text\ncross-attn\nA rabbit is turning around\ndecoupled\nmultimodal\ncross-attn\nFigure 10. Impact of multimodal conditions for image animation.\ncondition on video generation. We freeze the spatial layers\nand only train the temporal ones. Animatediff-XL also em-\nploys fixed SDXL spatial weights conditioned only by text\nand is included in this ablation. However, since it\u2019s trained\non internal high-quality data, we\u2019ve also trained a model\nwithout image conditions on the same data used for our final\nmodel, to ensure a fair ablation. Text-Video (T-V) Align-\nment and Motion Fidelity are agreement scores from human\nevaluation (Sec 4.2). We find that using only text condition\nleads to weaker temporal consistency, motion fidelity, and\nvisual quality compared to multimodal conditions. Fig. 9\nalso shows that with only text condition, the resulting video\nexperiences significant temporal flickering and sudden ap-\npearance changes. These outcomes validate that when train-\ning temporal modules only, the additional image cross at-\ntention provides effective visual signals, thereby allowing\nthe temporal module to focus on video consistency, leading\nto reduced flickering and improved video quality.\nImpact of image condition and masked condition on im-\nage animation. Tab. 7 shows that masked condition helps\ncomposed\ncomposed-fs\ndecomposed-fs\nMVB\nrate\n0%\n13%\n97%\n97%\nTable 5. Image controlent successful rates with different spatial\ntemporal designs in VDMs.\nFVD(\u2193)\nT-V alignment\nMotion-Fidelty\nQuality\nText Only\nOurs w/o image condition\n602\n8%\n4%\n0%\nAnimatediff-XL\n589\n40%\n12%\n30%\nMVB\nOurs\n542\n52%\n84%\n70%\nTable 6. Impact of multi-modal condition for the VDM.\nDINO (First)\nDINO (Avg)\nCLIP-T (Avg)\ntext only\n0.264\n0.262\n0.285\n+ masked condition\n0.760\n0.296\n0.210\n+ image condtion\n0.638\n0.562\n0.282\n+ both\n0.765\n0.614\n0.284\nTable 7. Impact of conditions masks and decoupled cross attention\nfor image animation. Higher metrics are better.\nto produces the first animation frame that matches the con-\nditioning image, yet cannot guarantee the temporal consis-\ntency, as suggested by a high DINO (First) score and low\nDINO (Avg) score. Adding the image condition improves\ntemporal consistence and subject identify, as evidenced by\nthe high DINO (Avg) score and confirmed in Fig. 10. With\nboth masked condition and image condition, our model pro-\nduces the first frame that highly preserves the conditioning\nimage, as well as visually consistent animations.\n5. Conclusion\nWe present MoonShot, a new video generation model that\nconditions on both image and text inputs using the Mul-\ntimodal Video Block (MVB). Our model stands out in\nproducing high-quality videos with controllable visual ap-\npearance. In the meantime, our model is able to harvest\nthe pre-trained image ControlNet to control over geome-\ntry without extra training overhead.\nThe model offers a\ngeneric architecture and versatile conditioning mechanisms,\nwhich allows it to be easily adapted for various video gen-\neration tasks, such as image animation, video editing and\nsubject-customized video generation, with superior genera-\ntion quality than previous methods, revealing its great po-\ntential to serve as a foundation model for video generation\nresearch and application.\n6. Ethic\nOur model operates with both image and text inputs, and\nsimilarly to the Blip-Diffusion [34], we aim to ensure a\nstrong relevance of the generation samples to the supplied\nimage. This image can originate from two sources: either\ndirectly uploaded by human users or generated by a text-\nto-image model like DALL-E 3 or Stable Diffusion XL.\nWhen the image is produced by a text-to-image model, the\nbias, fairness, and toxicity levels of our model are inherent\nand comparable to those of the text-to-image model. Con-\nversely, when the image is provided by the user, these as-\npects are mostly governed by the user, not our model. To\nmitigate harmful content that may result from inappropri-\nate image/text inputs, we plan to implement an NSFW (Not\nSafe For Work) detector, which is open-sourced in diffusers\nlibrary [58] and can be directly integrated into our model.\nThis detector will strengthen the model governance layer to\ncontrol the input contents, either from human users or from\nthe text-to-image model, thereby reducing harmful compo-\nnents. In addition to the above measures, we are conducting\nred-teaming experiments to understand and minimize harm-\nful generation contents as a result of red-teaming attacks.\nNote that by default, our model is designed to generate non-\ntoxic elements as long as the provided inputs do not elicit\nharmful or biased content. Regardless, we are committed to\neliminating any toxicity or biases in our model and we will\nnot open source or make our model publicly available until\nthe safety measures are properly in place.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 3\n[2] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan,\nRoy H Campbell, and Sergey Levine. Stochastic variational\nvideo prediction. arXiv preprint arXiv:1710.11252, 2017. 3\n[3] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 5\n[4] Gedas Bertasius, Heng Wang, and Lorenzo Torresani.\nIs\nspace-time attention all you need for video understanding?\nIn Proceedings of the International Conference on Machine\nLearning (ICML), 2021. 4\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 1, 4, 5, 7\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 3\n[7] Duygu Ceylan,\nChun-Hao Huang,\nand Niloy J. Mi-\ntra.\nPix2video:\nVideo editing using image diffusion.\narXiv:2303.12688, 2023. 3, 7\n[8] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. arXiv preprint arXiv:2308.09592, 2023. 3\n[9] Li Chen,\nMengyi Zhao,\nYiheng Liu,\nMingxu Ding,\nYangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing\nLiu, Kang Du, et al.\nPhotoverse:\nTuning-free image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2309.05793, 2023. 1\n[10] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models,\n2023. 2, 3\n[11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n3\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 1, 3\n[13] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin,\nDevi Parikh, and Yaniv Taigman.\nMake-a-scene: Scene-\nbased text-to-image generation with human priors. In Com-\nputer Vision\u2013ECCV 2022: 17th European Conference, Tel\nAviv, Israel, October 23\u201327, 2022, Proceedings, Part XV,\npages 89\u2013106. Springer, 2022. 2\n[14] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer.\narXiv preprint arXiv:2204.03638,\n2022. 3\n[15] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An-\ndrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,\nMing-Yu Liu, and Yogesh Balaji. Preserve your own cor-\nrelation: A noise prior for video diffusion models. arXiv\npreprint arXiv:2305.10474, 2023. 1\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arxiv:2307.10373, 2023. 2, 3\n[17] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yun-\npeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning\nChang, Weijia Wu, et al. Mix-of-show: Decentralized low-\nrank adaptation for multi-concept customization of diffusion\nmodels. arXiv preprint arXiv:2305.18292, 2023. 3\n[18] Yuchao Gu, Yipin Zhou, Bichen Wu, Licheng Yu, Jia-Wei\nLiu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang,\nMike Zheng Shou, and Kevin Tang. Videoswap: Customized\nvideo subject swapping with interactive semantic point cor-\nrespondence. arXiv preprint arXiv:2312.02087, 2023. 3\n[19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6, 8\n[20] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion modeling\nof long videos. arXiv preprint arXiv:2205.11495, 2022. 3\n[21] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 1\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 7\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\n5\n[24] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[25] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 3, 7\n[26] Tobias H\u00a8oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling. arXiv preprint arXiv:2206.07696, 2022. 3\n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 1\n[28] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 3\n[29] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3, 7\n[30] Yunji Kim, Seonghyeon Nam, In Cho, and Seon Joo\nKim.\nUnsupervised keypoint learning for guiding class-\nconditional video prediction. Advances in neural informa-\ntion processing systems, 32, 2019. 3\n[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 1\n[32] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 3\n[33] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid.\nCcvs: Context-aware controllable video synthesis. NeurIPS,\n2021. 3\n[34] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 1, 2, 3, 9\n[35] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin\nLu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal\nvideo prediction from still images. In Proceedings of the Eu-\nropean Conference on Computer Vision (ECCV), pages 600\u2013\n615, 2018. 3\n[36] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan.\nVideofusion: Decomposed diffusion models for\nhigh-quality video generation. In CVPR, 2023. 3\n[37] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 3, 5\n[38] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n3\n[39] Haomiao Ni, Changhao Shi, Kai Li, Sharon X Huang, and\nMartin Renqiang Min. Conditional image-to-video gener-\nation with latent flow diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 18444\u201318455, 2023. 3\n[40] Yaniv Nikankin, Niv Haim, and Michal Irani.\nSinfusion:\nTraining diffusion models on a single image or video. arXiv\npreprint arXiv:2211.11743, 2022. 3\n[41] Junting Pan, Chengyu Wang, Xu Jia, Jing Shao, Lu Sheng,\nJunjie Yan, and Xiaogang Wang. Video generation from sin-\ngle semantic label map. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 3733\u20133742, 2019. 3\n[42] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 5\n[43] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Ar-\nbel\u00b4aez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017\ndavis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017. 7\n[44] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 3, 5, 7\n[45] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 3\n[46] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. 2022. 1, 2, 3, 6\n[47] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-\nral generative adversarial nets with singular value clipping.\nIn ICCV, 2017. 3\n[48] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nTheo Coombes, Cade Gordon, Aarush Katta, Robert Kacz-\nmarczyk, and Jenia Jitsev.\nLAION-5B: laion-5b: A new\nera of open large-scale multi-modal datasets.\nhttps:\n//laion.ai/laion-5b-a-new-era-of-open-\nlarge-scale-multi-modal-datasets/, 2022. 5\n[49] Xiaoqian Shen,\nXiang Li,\nand Mohamed Elhoseiny.\nMostgan-v: Video generation with temporal motion styles.\nIn CVPR, 2023. 3\n[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 4, 5, 7\n[51] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. arXiv preprint\narXiv:2112.14683, 2021. 3\n[52] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting\nHua, Zsolt Kira, Yilin Shen, and Hongxia Jin. Continual dif-\nfusion: Continual customization of text-to-image diffusion\nwith c-lora. arXiv preprint arXiv:2304.06027, 2023. 3\n[53] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms. In ICML, 2015. 3\n[54] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng,\nDimitris N. Metaxas, and Sergey Tulyakov. A good image\ngenerator is what you need for high-resolution video synthe-\nsis. In ICLR, 2021. 3\n[55] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocogan: Decomposing motion and content for\nvideo generation. In CVPR, 2018. 3\n[56] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 7\n[57] Vikram Voleti, Alexia Jolicoeur-Martineau, and Christopher\nPal. Masked conditional video diffusion for prediction, gen-\neration, and interpolation. arXiv preprint arXiv:2205.09853,\n2022. 3\n[58] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro\nCuenca, Nathan Lambert, Kashif Rasul, Mishig Davaadorj,\nand Thomas Wolf.\nDiffusers:\nState-of-the-art diffusion\nmodels.\nhttps://github.com/huggingface/\ndiffusers, 2022. 9\n[59] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. NIPS, 2016. 3\n[60] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n2, 4, 7\n[61] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 3\n[62] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap at-\ntention in spatiotemporal diffusions for text-to-video gener-\nation. arXiv preprint arXiv:2305.10874, 2023. 3\n[63] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 3, 6\n[64] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun\nHuang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun\nWang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali\nWang, Limin Wang, and Yu Qiao.\nInternvideo: General\nvideo foundation models via generative and discriminative\nlearning. arXiv preprint arXiv:2212.03191, 2022. 5\n[65] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 3\n[66] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 7\n[67] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In ECCV, pages\n720\u2013736. Springer, 2022. 3, 7\n[68] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei,\nYuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image\ndiffusion models for text-to-video generation. arXiv preprint\narXiv:2212.11565, 2022. 2, 3\n[69] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion priors.\n2023. 3, 6\n[70] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo.\nLearning to generate time-lapse videos using multi-stage dy-\nnamic generative adversarial networks. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 2364\u20132373, 2018. 3\n[71] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, 2016. 7\n[72] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind\nSrinivas. Videogpt: Video generation using vq-vae and trans-\nformers. arXiv preprint arXiv:2104.10157, 2021. 3\n[73] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation.\narXiv\npreprint arXiv:2203.09481, 2022. 3\n[74] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. In ACM SIGGRAPH Asia Conference Proceed-\nings, 2023. 2, 3, 6, 7\n[75] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 1\n[76] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 3, 5\n[77] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos\nwith dynamics-aware implicit generative adversarial net-\nworks. In ICLR, 2021. 3\n[78] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 1\n[79] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2, 3\n[80] Shiwei* Zhang, Jiayu* Wang, Yingya* Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and\nJingren Zhou. I2vgen-xl: High-quality image-to-video syn-\nthesis via cascaded diffusion models. 2023. 3, 6\n[81] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 2, 3\n[82] Rui Zhao, Yuchao Gu, Jay Zhangjie Wu, David Junhao\nZhang, Jiawei Liu, Weijia Wu, Jussi Keppo, and Mike Zheng\nShou.\nMotiondirector: Motion customization of text-to-\nvideo diffusion models. arXiv preprint arXiv:2310.08465,\n2023. 1\n[83] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 1, 2, 3, 7\n"
  },
  {
    "title": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2401.01647.pdf",
    "upvote": "12",
    "text": "SIGNeRF: Scene Integrated Generation for Neural Radiance Fields\nJan-Niklas Dihlmann\nAndreas Engelhardt\nHendrik Lensch\nUniversity of T\u00a8ubingen\n{jan-niklas.dihlmann, andreas.engelhardt, hendrik.lensch}@uni-tuebingen.de\nReference Sheet\nReference Sheet\n+\n+\n\u201cA grizzly bear\u201d\n\u201cA brown bunny\u201d\nShape Selection\nProxy Selection\nOriginal NeRF\nObject Editing\nObject Generation\nFigure 1. SIGNeRF \u2013 A method that enables NeRF scene editing by either shape selection for object modification or by proxy object\nplacement to insert a synthesized object. The transformed scene is represented by a NeRF trained on a set of edited input images, which are ef-\nficiently generated by a diffusion process using a multiview consistent reference sheet. Project page available at https://signerf.jdihlmann.com/\nAbstract\nAdvances in image diffusion models have recently led to\nnotable improvements in the generation of high-quality im-\nages. In combination with Neural Radiance Fields (NeRFs),\nthey enabled new opportunities in 3D generation. However,\nmost generative 3D approaches are object-centric and ap-\nplying them to editing existing photorealistic scenes is not\ntrivial. We propose SIGNeRF, a novel approach for fast and\ncontrollable NeRF scene editing and scene-integrated ob-\nject generation. A new generative update strategy ensures\n3D consistency across the edited images, without requiring\niterative optimization. We find that depth-conditioned dif-\nfusion models inherently possess the capability to generate\n3D consistent views by requesting a grid of images instead\nof single views. Based on these insights, we introduce a\nmulti-view reference sheet of modified images. Our method\nupdates an image collection consistently based on the ref-\nerence sheet and refines the original NeRF with the newly\ngenerated image set in one go. By exploiting the depth con-\nditioning mechanism of the image diffusion model, we gain\nfine control over the spatial location of the edit and enforce\nshape guidance by a selected region or an external mesh.\n1. Introduction\nIn this work, we focus on scene-integrated generation for\nediting existing 3D scenes using generative 2D diffusion\nmodels (Fig. 1). We propose an approach to create or modify\nobjects within existing NeRF [32] scenes while preserv-\ning the original scenes\u2019 structure and appearance. Previous\nmethods such as Instruct-NeRF2NeRF [13] and DreamEdi-\ntor [64] already demonstrated scene modification with text\ninstructions. However, their pipelines are complex, involve\nadditional training and lack fine-grained control.\nWe observe that ControlNet [61], an image diffusion\nmodel with additional depth guidance is inherently capa-\nble of generating coherent and consistent views of an object\nwhen applied to image tiles in a grid layout. Using this com-\nbined image generation, we construct a reference sheet of\nedits that can subsequently be applied to the original NeRF\nscene by updating the underlying image dataset. The gen-\neration process of the reference sheet is conditioned on the\nexisting NeRF scene and a text prompt that can be tuned\n1\narXiv:2401.01647v1  [cs.CV]  3 Jan 2024\nPlaced Reference Cameras\nSampled Cameras\nComposed Grids\nImage Grids\nReference Sheet\nControlnet\nOriginal NeRF\nEdited Dataset\nEdited NeRF\nCloseup\nEdited NeRF\nNeRF Dataset\n\u201cA brown bunny\u201d\n\u201cA brown bunny\u201d\n1\n5\n0\n2\n4\n5\n3\n6\n7\n1\nFigure 2. SIGNeRF pipeline for NeRF scene editing \u2013 Here, object generation. First, the original NeRF scene is trained (1), and a proxy\nobject is placed into the scene (2). After a precise selection, we place reference cameras (5 here) into the scene (3), render the corresponding\ncolor, depth, and mask images, and arrange them into image grids (4). These grids are used to generate the reference sheet with conditioned\nimage diffusion (5). To propagate the edits to the entire image set, for each camera, a color, depth, and mask image are rendered and\nplaced into the empty slot of the fixed reference sheet. We generate a new edited image consistent with the reference sheet by leveraging an\ninpainting mask. The step is repeated for all cameras (6). Finally, the NeRF is fine-tuned on the edited images (7)\neasily to one\u2019s liking. This editing capability is further re-\nfined by introducing two selection methods in scene space to\ncondition the edits of the diffusion model. A mesh proxy can\nbe placed and composed with the depth data of the existing\nNeRF to guide the insertion of new objects or a rough bound-\ning box can be used to select existing parts of the scene. This\naddition grants enhanced control over the position and visual\nrepresentation of generated entities and preserves the quality\nof the unedited parts. In summary, our method consists of\nthe following key elements:\n\u2022 Reference-sheet-based assembly as a simple and easy-to-\nimplement approach to improve consistency in generated\nmulti-view image data when using the ControlNet [61]\nimage diffusion model.\n\u2022 A modular pipeline to update a NeRF dataset based on a\nreference sheet that is generated and refined to the user\u2019s\nliking using text prompts. This update scheme can be\neasily repeated to further increase quality if needed.\n\u2022 Fine-grained control of the generation process by offering\nmultiple selection modes to constrain edits in scene space.\nCompared to existing methods like DreamEditor [64] and\nInstruct-NeRF2NeRF [13] we introduce a simplified, more\nmodular pipeline that achieves comparable or improved re-\nsults while adding control to the editing process.\n2. Related Work\nText-to-Image Generation\nRecent advances in diffusion\nprobabilistic models [16, 49] in combination with the avail-\nability of large datasets [46], enabled the generation of high-\nresolution and diverse images [35, 41, 43]. Fine-tuning these\nmodels can significantly enhance the capacity to guide and\npersonalize image generation [17, 42]. In addition, Control-\nNet [61] has demonstrated that image diffusion models can\nbe fine-tuned to condition them on various secondary inputs,\nsuch as depth, poses, or even sketches [53]. Furthermore,\nediting existing images in a generative manner with instruc-\ntions is possible as outlined by InstructPix2Pix [4].\nText-to-3D Generation\nThe field of text-to-3D generation\nhas recently experienced a significant breakthrough, with\nthe combination of image diffusion models and NeRFs [39].\nPreviously, the emphasis of text-driven 3D generation relied\non optimizing meshes [7, 22, 31], point clouds [44], vox-\nels [6], and NeRFs [19], guided by Contrastive Language-\nImage Pretraining (CLIP) [40] embeddings and supervision.\nWhile mesh generation guided by CLIP continues to be an\nactive area of research [11], recent advances in image dif-\nfusion models address a significant challenge in text-to-3D\ngeneration, namely the scarcity of 3D data. The pioneering\nwork by DreamFusion [39] introduced Score-Distillation-\nSampling (SDS), leveraging a pre-trained image diffusion\nmodel conditioned on a text input prompt to generate novel\n3D objects. Building upon this, Latent-NeRF [30] extended\nDreamFusion to the latent domain, accelerating the genera-\ntion process using Stable Diffusion [41].\nThe generation quality and object variation can be further\nimproved by modifying SDS [18, 26, 54]. Although SDS has\nproven itself as a powerful tool for 3D object generation, it\n2\nrequires a large amount of memory and time to train. In con-\ntrast, Instruct-NeRF2NeRF [13] proposed that conditioned\nimage diffusion models can be used for 3D object genera-\ntion without requiring a direct gradient update, making it\nvastly faster. By iteratively updating the NeRF image set\nwith conditioned generated images, the NeRF can be trained\nto generate the desired 3D object. The final appearance, how-\never, emerges slowly over time while our method gives an\nimmediate preview.\nWhile there are other promising paths to 3D generation,\nincluding the use of 3D diffusion models [24, 33, 36] and\nsynthesizing novel views from single or multiple images to\nlift an image representation into 3D space [9, 27, 28, 51,\n55, 56, 63], this work utilizes 2D image diffusion, given its\napplicability to NeRF representations.\nNeRF Editing\nAlthough NeRFs have proven themselves\nas a state-of-the-art tool for novel view synthesis and 3D\nscene reconstruction [3, 32, 34], editing NeRFs is still a chal-\nlenging task and is the subject of ongoing research. Efforts\nto manipulate NeRFs were initiated by NeRF-Editing [60],\nwhich proposed a method to deform NeRFs by modifying\nthe underlying implicit function. NeRFShop [25] extended\nthe work of cage-based deformation [57], introducing an\nintuitive selection of objects for both affine and non-affine\ntransformations and object duplication. However, these ap-\nproaches are limited to simple deformations and do not allow\nfor more complex edits or additions to the scene. In contrast,\nNeuMesh [58] learns a disentangled neural mesh-based im-\nplicit field to edit geometry and appearance, enabling geom-\netry deformation and texture swapping, filling, and painting.\nIt has also been demonstrated that NeRFs can be combined\nby inserting objects into pre-existing NeRF scenes [23]. De-\nspite the strides made in NeRF editing, the existing solutions\nprovide only a basic level of editing functionality compared\nto conventional 3D editing software. Generally, obtaining\nvisually pleasing results still requires artistic proficiency and\noften manual labor.\nGenerative NeRF Editing\nWith advances in text-to-3D\ngeneration, a new area in NeRF editing emerged, incorporat-\ning generative 3D models to modify existing NeRF scenes.\nSet-the-Scene [8] and Compositional 3D [37] present meth-\nods for controlled scene generation using proxy objects\nand bounding boxes. In addition, Composable 3D Diffu-\nsion enables moving generated objects within the composed\nscene, which consists of multiple NeRFs. On the other hand,\nSINE [2] allows direct NeRF editing by changing a reference\nimage in 2D space. It uses an editing field to adjust the tem-\nplate NeRF\u2019s geometry and appearance to match the image\nchanges. Similarly, VOX-E [47] combines an original NeRF\nwith an edited NeRF generated using text-based SDS. It\nmerges the field by using the attention mask from the image\ndiffusion model. Yet, these methods often result in overlay-\ning edits and increased memory and training time due to us-\ning multiple NeRFs. In contrast, Instruct-NeRF2NeRF [13]\nuses an Iterative Dataset Update (IDU) strategy to edit\nNeRF\u2019s image dataset with InstructPix2Pix [4], such that\nthe NeRF can be transformed based on editing instructions.\nInspired by the work of Instruct-NeRF2NeRF [13], we also\nupdate the NeRF image dataset with an image diffusion\nmodel. However, unlike Instruct-NeRF2NeRF, we do this\nin a preprocessing stage using the depth-conditioned Con-\ntrolNet [61], gaining more control over the generational\nprocess. DreamEditor [64] also allows for controlled text-\nbased scene editing by focusing the selection with finetuning\nan image diffusion model DreamBooth [42] before using\nSDS. Nevertheless, using SDS and DreamBooth is very time-\nconsuming compared to our approach. Furthermore, Instruct-\nNeRF2NeRF and DreamEditor can be challenging to control\nby only semantics and often fail to generate new objects\nwithin the scene at specific locations. Blended-NeRF [12]\nimproves positional control by limiting the generation within\nmanually controlled bounding boxes. However, Blended-\nNeRF results often do not fit the scene and get clipped by its\nbounding boxes, highlighting the need for further improve-\nments in controlled generative NeRF editing. We introduce\na new approach to generative NeRF editing, providing more\nprecise control over the generated edits with a manual se-\nlection or proxy object guidance to generate new and fitting\nobjects within an existing scene.\n3. Method\nSIGNeRF is a method for scene-integrated generation, in-\ncluding edits and object generation within an existing NeRF\nscene. At the core, we introduce the concept of a reference\nsheet image grid to maintain multi-view coherence and to\ngain control over the generation process (Figs. 2 and 4).\nGiven a NeRF model trained on the original image set, a\nset of views is selected to compose a grid of images. The\nscene edits are performed on this grid by ControlNet, a\nconditioned image diffusion model (Sec. 3.1) in one go to\nproduce the reference sheet. In the second step, the refer-\nence sheet constrains the generation of an updated version of\nthe full image set. We observe that this two-step procedure\nalready supports generating quite consistent edited views.\nThe edited 3D scene is obtained by finetuning the original\nNeRF scene with these newly generated views. Optionally,\nif the multi-view consistency needs to be improved further\na second iteration can be performed, where the reference\nsheet is updated based on the once-edited NeRF to generate\na second image set. An overview of the pipeline on consis-\ntent grid image generation and subsequent scene-integrated\ngeneration is presented in Fig. 2.\n3\nOriginal NeRF\nObject Generation\nProxy Object\nGenerated Object\nOriginal NeRF & Selection\nObject Editing\n\u201cA man with white sports clothes\u201d\n\u201cA pirate standing in a garage\u201d\n\u201cA batman standing in a garage\u201d\nFigure 3. Object insertion and object modification. \u2013 (top) The cow geometry is centrally placed on a meadow to obtain a photorealistic\nmodel. (middle) Note how occlusions are properly handled when generating the synthetic house based on the inserted proxy. (bottom)\nObjects can easily be transformed based on a prompt. Due to the more complex surface texture and geometric changes the pirate and the\nBatman costume required an additional iteration to obtain the same level of consistency compared to the simpler sports clothes.\n3.1. Background\nNeRF\nNeural Radiance Fields (NeRFs) [32] implicitly\nrepresent a scene by learning a continuous function of volu-\nmetric density and color. A 5D coordinate, composed of a\nspatial location (x, y, z) and a viewing direction (\u03b8, \u03c6), is\nmapped to a view-dependent emitted radiance (r, g, b) and\na volume density \u03c3. Volumetric rendering accumulates the\ndensities and colors at multiple sample locations along the\nview ray rj of a virtual camera C to obtain the final pixel\ncolor \u02c6C(rj) to either calculate a novel view or to update the\nneural representation based on the difference to some of the\ninput images I = {I1, ..., IN}.\nControlNet\nControlNet [61] is a specific image diffusion\nmodel that allows for constraining the image generation pro-\ncess with additional conditions, such as sketches, edge, or\ndepth maps. In the typical process of image-to-image diffu-\nsion, an image x is first encoded with an encoder E(x) = z\nto produce a latent image z. Then the latent image z is itera-\ntively updated by the diffusion model, which is guided by a\nnoise predictor UNet \u03f5\u03b8 conditioned on the encoded text in-\nput \u03c4\u03b8(y) and the current time step t. The final latent image\nz is then decoded by a decoder D to obtain the generated\nimage y. In the case of ControlNet, the image generation\nprocess is guided by an additional condition cf, that serves\nas an additional input to the noise predictor UNet \u03f5\u03b8, (Eq. 1).\n\u03f5 = \u03f5\u03b8 (zt, t, cf, \u03c4\u03b8(y))\n(1)\nThis functionality is used in our pipeline to guide the image-\nto-image generation process with depth maps.\n3.2. Controlled Consistent Generation\nThe key challenge of 3D generation techniques is to gen-\nerate consistent views with an image diffusion model. Our\napproach is based on reference sheet generation which is\nsimpler, faster and features direct control compared to the\nmethods introduced in Sec. 2 which rely on a repetitive cycle\nof intertwined diffusion and NeRF updates.\nReference Sheet Generation\nWe observe that the image\ndiffusion model ControlNet [61] can already generate multi-\n4\nIndividual Image Generation\nReference Sheet Generation\n4\n3\n1\n1\n2\nFigure 4. Reference Sheet Generation \u2013 Using ControlNet\n[61]inpainting to edit scene parts image-by-image results in dras-\ntically different looks per view (left) although all parameters and\nthe seed are the same. In contrast, we obtain a consistent reference\nsheet (right) by arranging the input images into a grid, letting Con-\ntrolNet process the entire sheet in a single generation step.\nview consistent images of a scene without the need for itera-\ntive refinement. While generating individual views sequen-\ntially introduces too much variation to integrate them into\na consistent 3D model, arranging them in a grid of images\nthat are processed by ControlNet in one pass significantly\nimproves the multi-view consistency as depicted in Fig. 4.\nBased on the depth maps rendered from the original NeRF\nscene we employ a depth-conditioned inpainting variant of\nControlNet to generate such a reference sheet of the edited\nscene. A mask specifies the scene region where the genera-\ntion should occur.\nThis step gives a lot of control to the user. Different ap-\npearances can be produced by generating reference sheets\nwith different seeds or prompts. The one sheet finally se-\nlected will directly determine the look of the final 3D scene.\nImage Set Update\nDespite the potential of grid genera-\ntion, we are limited in the number of images we can place in\none sheet due to the memory and attention limitations of the\nimage diffusion model. Given that a proper NeRF training\nmight require hundreds of images, an alternative method for\ngenerating consistent scene views is necessary. We solve\nthis challenge in a two-step process. First, we generate the\nreference sheet and subsequently use it to iteratively update\nthe images in the NeRF dataset image-by-image. To ensure\nthat all other views are also consistent with the reference\nsheet, we originally left one slot empty when producing the\nreference sheet, e.g. the bottom right corner, and condition\nthis slot with the appropriate original image, mask and depth\nof the to-be-generated view while keeping the rest of the ref-\nerence sheet fixed. This way, the appearance of the reference\nsheet is propagated to all input views.\n3.3. Scene Integrated Generation\nBased on these ideas we present the full scene-integrated\ngeneration pipeline as depicted in Fig. 2 for the specific case\nof object generation.\n1. Original NeRF Scene \u2013 A NeRF scene S is re-\nconstructed using set of N input images IS\n=\n{IS\n1 , IS\n2 , ..., IS\nN} alongside the corresponding cam-\neras CS = {CS\n1 , CS\n2 , ..., CS\nN}. This NeRF serves as the\nfoundation for the subsequent scene-integrated genera-\ntion.\n2. Object Selection \u2013 The next step is to mark the 3D region\nto be edited. For region-based edits, we use a bounding\nbox. To introduce a new object, we use a proxy mesh that\nis placed in the scene.\n3. Reference Camera Placement \u2013 Around the selected\nedit region, we place the M << N cameras CR =\n{CR\n1 , CR\n2 , ..., CR\nM} for the reference sheet generation.\nThey need to cover a sufficient range of views around\nthe object and have the object properly centered.\n4. Reference Input Image Rendering \u2013 For each reference\ncamera we use the original NeRF scene S to render the\nRGB images IR = {IR\n1 , IR\n2 , ..., IR\nM}, with correspond-\ning depths DR and inpainting masks M R.\n5. Reference Sheet Assembly and Generation \u2013 Each set\nof M reference input images is arranged in a 2D grid\nresulting in \u00afIR, \u00afDR and\n\u00af\nM R, respectively. One grid\ncell remains empty on purpose. The reference sheet R\nis generated by piping the created grids as inputs and\nconditions to ControlNet with the selected prompt y:\nR \u2190 ControlNet(\u00afIR, \u00afDR, \u00af\nM R, y)\n(2)\n6. Image Set Update \u2013 After producing the desired\nreference sheet, it is used to generate a new im-\nage dataset I \u02c6S\n= {I \u02c6S\n1 , I \u02c6S\n2 , ..., I \u02c6S\nN}. We first create\ndepth maps DS = {DS\n1 , DS\n2 , ..., DS\nN} and inpainting\nmasks M S = {M S\n1 , M S\n2 , ..., M S\nN} from the original\nNeRF scene S and then replace the empty grid cells with\nthe corresponding image or depth map. This results in the\nfollowing update rule for the image dataset I \u02c6S:\nI\n\u02c6S\ni \u2190ControlNet(Ri, \u00afD\nR\ni , M S\ni , y)\n\u2200i \u2208 {1, 2, ..., N}\nwith Ri \u2190 R \u2295 {IS\ni } and \u00afD\nR\ni \u2190 \u00afDR \u2295 {DS\ni }\n(3)\n7. Finetuning the NeRF Scene \u2013 The final step is to fine-\ntune the original NeRF scene S with the generated image\ndataset I \u02c6S to receive the edited scene \u02c6S.\n8. Multiple Iteration (optional) \u2013 While the generated ref-\nerence sheet always shows a consistent style in all tiles\nit might happen that the underlying 3D shape is not yet\nfully consistent. If this leads to visible artifacts Steps\n4 to 7 can be optionally repeated once more, this time\n5\nOriginal NeRF\nOriginal NeRF\nSIGNeRF\nSIGNeRF\nSIGNeRF\nSIGNeRF\nSIGNeRF\nSIGNeRF\nInstruct-NeRF2NeRF\nDreamEditor\nDreamEditor\nDreamEditor\nInstruct-NeRF2NeRF\nInstruct-NeRF2NeRF\nFigure 5. Qualitative Comparison \u2013 SIGNeRF results are compared to Instruct-NeRF2NeRF [13] (top) and DreamEditor [64] (bottom). For\nthe bear, the generated fur texture with SIGNeRF (left) shows a more distinguished structure and the snout regions is clearly more consistent.\nCompared to DreamEditor the images are different but the image quality comparible.\nstarting with the updated NeRF \u02c6S to render the reference\ninput images. The parameters of the ControlNet in Steps\n5 and 6 have to be tuned to stick closer to the input in this\nsecond iteration.\nA key feature of our pipeline is its modularity. Each step can\nbe developed and optimized independently. Compared to the\ntight iterative NeRF/image diffusion update loops of other\napproaches, in our pipeline individual steps can easily be\nexchanged, e.g. to enable different scene modifications, or\nto repeat some as indicated in the optional Step 8.\nSelection Modes\nFor precise control over the generation\nlocation, we introduce two basic selection modes: Shape\nselection and proxy selection (Fig. 1). With shape selection,\na region of the scene can be selected by an axis-aligned\nbounding box. We use this bounding box and generate a per-\ncamera mask by comparing its depth to the rendered NeRF\ndepth. Further, we a combined depth map by clamping the\nrendered NeRF depth within the point closest and furthest\naway to the rendering camera within the bounding box.\nWith the proxy selection mode, one can position an ar-\nbitrary mesh within the NeRF scene. Similar to the shape\nselection, we generate a depth map and a mask for each\ncamera view, but herefore combine the rendered depth map\nof the NeRF scene with the depth map rendered of the proxy\nobject relative to the camera. In difference, we use the visible\npart of the proxy object in the rendered view as the mask. As\npreviously described the generated depth map conditions the\nimage diffusion model, while the generated mask is used as\nan inpainting mask, to blend the generated image with the\noriginal NeRF image. The masks can further be dilated in\nimage space to allow control over the to-be-generated area.\nReference Sheet\nThe quality of the reference sheet directly\nimpacts the generation results. One important aspect is the\nnumber of reference cameras and their position within the\nscene. Optimally, we use the fewest cameras necessary to\ncapture the region of interest from all angles. This strategy\nminimizes generation time while maintaining consistency.\nAnother criterion is the proximity of reference cameras to\nthe original cameras. Generally, it is best to render reference\nviews from a position close to the original camera positions.\nIn cases where the edit region is too distant from any original\ncamera, the reference cameras need to move closer in order\nto increase the number of pixels in the masks for more details\nin the generated edits. A standout feature of SIGNeRF is its\nability to preview the reference sheet before generating the\ncomplete updated image set. Besides choosing the intended\nappearance we recommend iterative adjustments to the refer-\nence sheet by replacing undesired generated images of the\nimage grid until satisfactory results are obtained.\n4. Experiments\nThe proposed pipeline facilitates the generation of new ob-\njects within a scene and the editing of existing ones. We\nassess the quality of the 3D scenes generated by SIGNeRF\nand compare it to existing methods.\n4.1. Experimental Setup\nDatasets\nWe use various types of scenes for our ex-\nperiments, most of them in real-world settings. While\nsome are front-facing scenes, our primary focus is on 360\u00b0\nview scenes due to their inherent challenges and po-\ntential in 3D scene generation. Datasets from Instruct-\nNeRF2NeRF [13], DTU [20] and BlendedMVS [59] are\n6\nMethod\nImage\nNeRF\nBackground Preserv.\nCLIP T2I Dir. Sim. \u2191\nPSNR \u2191\nSSIM \u2191\nInstruct-N2N [13]\n0.1603\n0.1600\n30.09\n0.64\nDreamEditor [64]\n-\n0.1849\n-\n-\nOurs\n0.23\n0.2125\n32.22\n0.83\nTable 1. Quantitative Evaluation \u2013 Highlighting the CLIP text-\nto-image directional similarity comparing the diffusion Image edit\n(rendered NeRF view edited with the method-specific diffusion\nmode) and the trained NeRF render (rendered view from the edited\nNeRF) to the original image with corresponding prompts. The\nbackground preservation capability of our method is evaluated\nusing PSNR and SSIM by masking the edited object, comparing\nthe original and the edited NeRF result.\nutilized. Additionally, to address the scarcity of realistic\nNeRF datasets, we created custom scenes using smartphones\nwith PolyCam [14] or drones. Camera parameters were ei-\nther sourced directly from relevant apps or inferred using\nCOLMAP [45]. The scenes contain 30 to 300 images.\nImplementation Details\nSIGNeRF is built upon Nerfs-\ntudio using Nerfacto [50] as the underlying NeRF imple-\nmentation. For the image diffusion model, we modified an\ninpainting version of ControlNet [38, 41, 48, 61] with the\nSDXL [38]-diffusion backbone to allow for conditioning on\nmasked content. ControlNet scale is set between [0.4, 1.0],\nguidance values range from [6, 10] and denoising strength\nvaries between [0.5, 0.95]. However, these parameters can\nbe adjusted as needed.\n4.2. Qualitative Evaluation\nFig. 3 demonstrates SIGNeRF\u2019s object generation and edit-\ning capabilities (see also the videos in the supplemental\nmaterial). In the first two rows, novel objects are inserted\nwith fine-grained control of position, orientation and size.\nThe objects are synthesized, and conditioned on a geometry\nproxy. These objects are integrated seamlessly into the scene,\nexhibiting fitting lighting and texture properties. Notably\nin the cow scene, one does not even need strong trackable\nfeatures for precise placement.\nIn the last row, an existing object is modified based on\na text prompt to generate different appearances. Note that\nthe desired edits only affect the marked object. SIGNeRF is,\nhowever, powerful enough to even adjust the geometry where\nnecessary, e.g. short vs. long trousers, while the background\nis preserved.\nProxy Shape\nThe impact or the proxy shape and geometric\ndetail is visualized in Fig. 6. The shape of the proxy mesh\ndoes influence the final object as ControlNet tries to fit the\nHigh Poly\nLow Poly\nPolygon Primitives\nFigure 6. Influence of the proxy geometry \u2013 The synthetic cow\nis generated with three different proxy meshes with the prompt \u201dA\nbrown cow\u201d. From left to right: High-poly proxy mesh, low-poly\nproxy mesh, and simple geometric primitives.\ndepth condition created with the proxy mesh, as apparent\nwhen the body of the cow is approximated by a cylinder.\nNevertheless, the results indicate that the proxy mesh does\nnot need to be a detailed representation of the object, as the\ndiffusion process added additional geometry details. Both\nlow and high-poly versions yield satisfactory results.\nControlNet Guidance\nFurther, we studied the impact of\nthe scale parameter of ControlNet [61], which handles the\ncloseness of the generation to the depth condition, on the fi-\nnal NeRF edit. We observe that a low scale strongly reduces\nthe impact of the condition, leading to an edit that devi-\nates largely from the original shape. This allows us to make\ndrastic changes, like the Batman example (Fig. 4, Fig. 3).\nHowever, a low scale can also lead to more irritation in the\n3D consistency of the generated image, producing artifacts\nin the NeRF. In these cases, a second iteration of our pro-\ncess (Sec. 3.3), using a low scale in the first and a high scale\nin the second iterator achieves the desired results. In Fig. 3,\nthe Pirate and the Batman are generated with a second gen-\neration iteration, while the white shirt is generated directly.\n4.3. Comparison\nScene editing results of SIGNeRF are compared to Instruct-\nNeRF2NeRF [13] and DreamEditor [64] in Fig. 5. While\nInstruct-NeRF2NeRF [13] produces washed-out textures and\nsuffers from the Janus effect of showing different faces from\ndifferent views for the bear, our generated results show a\nmore consistent snout region and more vivid, more struc-\ntured pelt textures. Here, SIGNeRF is superior in terms of\nscene preservation, selection precision, generation quality,\nand color integrity.\nIn the second row of Fig. 5, it achieves similar results to\nDreamEditor, while both methods have some artifacts that\ncannot be directly compared. DreamEditior tends to generate\nsimpler and over-smoothed objects due to the high classifier-\nfree guidance needed for score-destillation-sampling [54].\n7\n\u201cAdd an apple in front of the bear\u201d\n\u201cTurn the blue box into an apple\u201d\n\u201cMake the rabbit realistic\u201d\nFigure 7. Instruct-NeRF2NeRF \u2013 Instruct-NeRF2NeRF fails to\ngenerate new objects in the scene, even when merging proxy meshes\nwith the NeRF scene. Compare to our results in Fig. 1\nIn contrast, SIGNeRF generates more complex and realistic-\nlooking objects but may show consistency artifacts for highly\ndetailed regions.\nBoth other methods are designed primarily for scene edit-\ning, generating new objects semantically independent from\nthe scene is not possible, let alone controlling the object\u2019s\nposition, scale, rotation, and shape within the scene. For\nexample, Instruct-NeRF2NeRF fails to generate a rabbit in\nfront of the bear statue (Fig. 7) while SIGNerF embeds it\nconsistently into the scene (Fig. 1). But also for editing tasks,\nSIGNeRF is superior as it also allows semantically depen-\ndent parts to be unedited, e.g. the face in the Batman scene.\nAnother aspect is the time spent in the generation process.\nInstruct-NeRF2NeRF and DreamEditor, require more than\nan hour for a generation that is only fully visible at the\nend. While SIGNeRF only takes half the time on a single\nGPU, using one dataset iteration, it additionally provides a\npreview option with the reference sheet, allowing the user\nto adjust the output until satisfied with the results before\nstarting the generation process. Furthermore, the image set\nupdate (Step 6) can easily be parallelized over all images in\nour approach while the interlocked NeRF/image updates in\nthe other methods are purely sequential.\n4.4. Quantitative Evaluation\nEven though the process of generating and editing 3D scenes\nis inherently subjective, in line with previous works we uti-\nlize the CLIP [40] text-to-image directional similarity to\nprovide a quantitative perspective. This metric [10] evaluates\nthe semantic distance between the original image and edited\nNeRF scene to their corresponding prompt pairs. Table 1\ncompares the scores averaged over a total of 10 scenes to\nInstruct-NeRF2NeRF [13] and DreamEditor [64]\nAnother metric provided in Table 1 accounts for back-\nground preservation calculated by masking the edited object\nand comparing the original and edited NeRF background.\nDue to their compressive nature image diffusion models are\nprone to degrading the image quality. Since SIGNeRF em-\nploys a masked update strategy, the background is better\npreserved compared to Instruct-NeRF2NeRF, which uses a\nrepeated iterative update strategy.\n\u201cA lake with green water and an island ...\u201d\n\u201cA lush green european park ...\u201d\nFigure 8. Limitations \u2013 Trying to modify the background by invert-\ning the object mask yields strong inconsistencies. The reference\nsheet images (top) show non-overlapping views of the background.\n4.5. Limitations\nEven though we can achieve better quality than Instruct-\nNeRF2NeRF with masking, we are forced to downscale the\nimages to fit into the reference sheet passed to the image dif-\nfusion model, thereby losing some quality for the generated\nedit. Further, optimal results are obtained when the object\noccupies the image\u2019s center and is positioned close to the\ncamera. As increasing the distance reduces the resolution of\nthe object in the latent space the quality of the generated edit\ndiminishes. This behavior holds true for all image diffusion-\nbased 3D generation methods. Additionally, an off-center\nobject complicates its incorporation into a reference sheet\nthat generates consistent views, thereby making SIGNeRF\nunsuitable for extended scene modifications (Fig. 8).\n5. Conclusion\nWith SIGNeRF we present a modular pipeline for scene-\nintegrated editing of NeRF scenes. An efficient and easily\ncontrollable two-step procedure first generates a tiled refer-\nence sheet followed by updating the image set to generate\nconsistent edited views suitable for modifying an existing\nNeRF representation. The reference sheet additionally shows\na preview of the edited scene before generating all images,\nwhich is not possible with existing editing methods. SIGN-\neRF often achieves consistent 3D generation in a single\nprocessing run. In comparison to previous methods, SIGN-\neRF is faster and delivers similar or superior editing results\nwithout necessitating iterative refinement.\nThe introduced selection strategies enable generative edits\nor object insertion within an existing NeRF, even for scenes\nwith complex geometry and appearance.\nWhile being developed for NeRF scenes, shifting to other\nrepresentations like Gaussian Splats [21] should be straight-\nforward due to the modular design.\nAlthough we are aware that this work might also be mis-\nused to forge convincing 3D scenes we hope that it overall\nwill provide another step towards democratizing the complex\ntask of 3D content generation.\n8\nAcknowledgements\nThis work has been partially funded by the Deutsche\nForschungsgemeinschaft (DFG, German Research Founda-\ntion) under Germany\u2019s Excellence Strategy - EXC number\n2064/1 - project number 390727645 and SFB 1233 - project\nnumber 276693517. It was supported by the German Federal\nMinistry of Education and Research (BMBF): T\u00a8ubingen AI\nCenter, FKZ: 01IS18039A and Cyber Valley\nReferences\n[1] AUTOMATIC1111.\nStable\ndiffusion\nwebui.\nhttps://github.com/AUTOMATIC1111/stable-\ndiffusion-webui, 2022. 12\n[2] Chong Bao, Yinda Zhang, Bangbang Yang, Tianxing Fan,\nZesong Yang, Hujun Bao, Guofeng Zhang, and Zhaopeng\nCui. Sine: Semantic-driven image-based nerf editing with\nprior-guided editing field. pages 20919\u201320929, 2023. 3\n[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. pages 5470\u20135479, 2021. 3\n[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\npages 18392\u201318402, 2022. 2, 3\n[5] Ricardo Cabello. Three.js, 2010. 12\n[6] Kevin Chen, Christopher B. Choy, Manolis Savva, Angel X.\nChang, Thomas Funkhouser, and Silvio Savarese. Text2shape:\nGenerating shapes from natural language by learning joint\nembeddings. pages 100\u2013116, 2019. 2\n[7] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui\nJia. Tango: Text-driven photorealistic and robust 3d styliza-\ntion via lighting decomposition. 2022. 2\n[8] Dana Cohen-Bar, Elad Richardson, Gal Metzer, Raja Giryes,\nand Daniel Cohen-Or.\nSet-the-scene: Global-local train-\ning for generating controllable nerf scenes. arXiv preprint\narXiv:2303.13450, 2023. 3\n[9] Congyue Deng, Chiyu \u201dMax\u201d Jiang, Charles R. Qi, Xinchen\nYan, Yin Zhou, Leonidas Guibas, and Dragomir Anguelov.\nNerdi: Single-view nerf synthesis with language-guided dif-\nfusion as general image priors. pages 20637\u201320647, 2022.\n3\n[10] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. Stylegan-nada: Clip-guided domain adap-\ntation of image generators. ACM Transactions on Graphics\n(TOG), 41(4):1\u201313, 2021. 8, 12\n[11] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d textured\nshapes learned from images. Advances In Neural Information\nProcessing Systems, 35:31841\u201331854, 2022. 2\n[12] Ori Gordon, Omri Avrahami, and Dani Lischinski. Blended-\nnerf: Zero-shot object generation and blending in existing\nneural radiance fields. arXiv preprint arXiv:2306.12760, 2023.\n3\n[13] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Editing\n3d scenes with instructions. 2023. 1, 2, 3, 6, 7, 8, 12\n[14] Chris Heinrich. Polycam: Lidar scanning app for iphone.\nhttps://poly.cam/, 2023. Polycam Inc. provides a\nfast and accurate 3D scanning app leveraging the LiDAR\nsensor on the iPhone. 7\n[15] Paul Henschel. React three fiber, 2019. 12\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2\n[17] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. 2022. 2\n[18] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation. arXiv preprint\narXiv:2306.12422, 2023. 2\n[19] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel,\nand Ben Poole. Zero-shot text-guided object generation with\ndream fields. pages 867\u2013876, 2022. 2\n[20] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis evalu-\nation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 406\u2013413, 2014. 6\n[21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3d gaussian splatting for real-time radiance\nfield rendering. ACM Transactions on Graphics, volume 42(4),\nJuly 2023, 42(4):1\u201314, 2023. 8\n[22] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. 2022. 2\n[23] Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey\nTulyakov, and Gerard Pons-Moll. Control-nerf: Editable fea-\nture volumes for scene rendering and manipulation. pages\n4340\u20134350, 2022. 3\n[24] Muheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-\nsdf: Text-to-shape via voxelized diffusion. pages 12642\u2013\n12651, 2022. 3\n[25] Shaoxu Li and Ye Pan. Interactive geometry editing of neu-\nral radiance fields. Proceedings of the ACM on Computer\nGraphics and Interactive Techniques, 6(1), 2023. 3\n[26] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-\n3d content creation. 2022. 2\n[27] Minghua Liu, Chao Xu, Haian Jin, Linghao Chen,\nMukund Varma T, Zexiang Xu, and Hao Su. One-2-3-45:\nAny single image to 3d mesh in 45 seconds without per-shape\noptimization, 2023. 3\n[28] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. arXiv preprint arXiv:2303.11328,\n2023. 3\n[29] Meta. React, 2013. 12\n[30] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation of\n3d shapes and textures. pages 12663\u201312673, 2023. 2\n[31] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization for\nmeshes. pages 13492\u201313502, 2021. 2\n9\n[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. Communications of the ACM, 65(1):99\u2013106, 2020. 1, 3,\n4\n[33] Norman M\u00a8uller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota\nBul`o, Peter Kontschieder, and Matthias Nie\u00dfner.\nDiffrf:\nRendering-guided 3d radiance field diffusion. pages 4328\u2013\n4338, 2022. 3\n[34] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multiresolu-\ntion hash encoding. ACM Trans. Graph. 41, 4, Article 102\n(July 2022), 15 pages, 41(4):1\u201315, 2022. 3\n[35] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[36] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 3\n[37] Ryan Po and Gordon Wetzstein. Compositional 3d scene\ngeneration using locally conditioned diffusion. arXiv preprint\narXiv:2303.12218, 2023. 3\n[38] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 7,\n12\n[39] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. 2023. 2\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever. Learning transferable visual\nmodels from natural language supervision. pages 8748\u20138763,\n2021. 2, 8\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. pages 10684\u201310695,\n2021. 2, 7, 12\n[42] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. pages 22500\u201322510, 2022. 2, 3\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022. 2\n[44] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape generation.\npages 18603\u201318613, 2021. 2\n[45] Johannes\nLutz\nSch\u00a8onberger\nand\nJan-Michael\nFrahm.\nStructure-from-Motion Revisited. In Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2016. 7\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa Kundurthy, Katherine Crowson, Lud-\nwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. Laion-\n5b: An open large-scale dataset for training next generation\nimage-text models. 36, 2022. 2\n[47] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects. arXiv preprint arXiv:2303.12048, 2023. 3\n[48] Neelay Shah, Tommaso De Rossi, and Mikolaj Cz-\nerkawski.\nControlnetinpaint: Inpaint images with con-\ntrolnet.\nhttps://github.com/mikonvergence/\nControlNetInpaint, 2023. GitHub repository. 7, 12\n[49] Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. pages 2256\u20132265, 2015. 2\n[50] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent\nYi, Justin Kerr, Terrance Wang, Alexander Kristoffersen,\nJake Austin, Kamyar Salahi, Abhik Ahuja, David McAllis-\nter, and Angjoo Kanazawa. Nerfstudio: A modular frame-\nwork for neural radiance field development. arXiv preprint\narXiv:2302.04264, 2023. 7, 12\n[51] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-it-3d: High-fidelity\n3d creation from a single image with diffusion prior. arXiv\npreprint arXiv:2303.14184, 2023. 3\n[52] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pedro Cuenca,\nNathan Lambert, Kashif Rasul, Mishig Davaadorj, and\nThomas Wolf. Diffusers: State-of-the-art diffusion models.\nhttps://github.com/huggingface/diffusers,\n2022. 12\n[53] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-\nguided text-to-image diffusion models.\narXiv preprint\narXiv:2211.13752, 2022. 2\n[54] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 7\n[55] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan Ho, Andrea Tagliasacchi, and Mohammad Norouzi.\nNovel view synthesis with diffusion models. arXiv preprint\narXiv:2210.04628, 2022. 3\n[56] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360\u00b0 views. pages 4479\u20134489,\n2022. 3\n[57] Tianhan Xu and Tatsuya Harada. Deforming radiance fields\nwith cages. pages 159\u2013175, 2022. 3\n[58] Bangbang Yang, Chong Bao, Junyi Zeng, Hujun Bao, Yinda\nZhang, Zhaopeng Cui, and Guofeng Zhang. Neumesh: Learn-\ning disentangled neural mesh-based implicit field for geome-\ntry and texture editing. pages 597\u2013614, 2022. 3\n[59] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren,\nLei Zhou, Tian Fang, and Long Quan.\nBlendedmvs: A\n10\nlarge-scale dataset for generalized multi-view stereo networks.\npages 1790\u20131799, 2019. 6\n[60] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: Geometry editing of\nneural radiance fields. pages 18353\u201318364, 2022. 3\n[61] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models. arXiv preprint\narXiv:2302.05543, 2023. 1, 2, 3, 4, 5, 7, 12\n[62] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. pages 586\u2013595, 2018. 12, 13\n[63] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distill-\ning view-conditioned diffusion for 3d reconstruction. pages\n12588\u201312597, 2022. 3\n[64] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields, 2023. 1, 2, 3, 6, 7, 8\n11\nThis appendix offers supplementary information on\nSIGNeRF. It delves into more intricate aspects of our imple-\nmentation (Sec. A), outlines the specialized tools we have\nengineered (Sec. B), and elaborates on the generated datasets\n(Sec. C).\nAdditional Material\nWe wish to emphasize the inclu-\nsion of two videos alongside this paper. Given the three-\ndimensional nature of our results, these videos serve as the\nmost effective medium for their evaluation. We strongly en-\ncourage viewing:\n\u2022 Explanation Video \u2014 A comprehensive guide through our\nmethods and pipeline.\n\u2022 Results Video \u2014 A presentation of our results, including\ncomparative analyses.\nFor a detailed examination and additional information\nplease visit our Project Page.\nA. Implementation Details\nImage Diffusion\nIn our approach, we integrate an inpaint-\ning version of ControlNet [61] with Stable Diffusion XL [38]\nto synthesize images based on the mask and depth map in-\nputs. We have customized the implementation of the pub-\nlicly available SD WebUI API [1], which builds upon the\nDiffusers library [52]. Notably, SIGNeRF is a general ap-\nproach that also works with previous versions of Stable\nDiffusion. For example, the \u2018field\u2019 scene with the generated\ncows (Fig. 3), which was generated with Stable Diffusion\n1.5 [41] and ControlNetInpaint [48].\nTraining\nSIGNeRF necessitates a pre-trained NeRF scene,\nwhich we acquire by employing the nerfacto model from\nNerfstudio [50], undergoing 30, 000 iterations of training for\neach scene. Subsequently, the SIGNeRF pipeline for scene-\nintegrated generation is utilized to create an edited dataset for\nthe NeRF scene (Sec. 3.3). It was discovered that for optimal\nefficacy, a reference sheet comprising 5 images strikes an\neffective balance between preserving image quality and pro-\nviding adequate scene context. Following the creation of the\nedited dataset, we opt to either fine-tune the existing NeRF\nscene with the updated dataset or initiate training of a new\nNeRF scene, depending on the choice of selection method.\nWe observe better results for object generation when training\nthe NeRF scene from scratch. Conversely, for the task in-\nvolving generative editing, fine-tuning the pre-trained NeRF\nscene has shown to be more effective. In such cases, optimiz-\ners are re-initialized, and the LPIPS [62] loss is applied to\nenhance scene consistency [13].\nGeneration Information\nThe generational edit of the\nNeRF scene can be controlled by several parameters pro-\nvided by the image diffusion model, such as the denoising\nGeneration\nDenoising S.\nControlNet S.\nGuidance S.\nPerson - Sport\n0.9\n0.4\n7.0\nPerson - Pirate\n[0.95, 0.5]\n[0.4, 0.8]\n7.5\nPerson - Batman\n[0.95, 0.5]\n[0.4, 1.0]\n6.0\nPlushy - Ironman\n0.9\n1.0\n7.0\nPlushy - Tiger\n0.6\n1.0\n7.0\nPlushy - Gold\n0.95\n1.0\n7.0\nBear - Grizzly\n0.9\n0.95\n7.0\nBear - Polar\n0.9\n0.95\n7.0\nBear - Panda\n0.9\n0.95\n7.0\nBear - Rabbit\n0.95\n1.0\n7.0\nField - Cow\n(SD 1.5)\n0.7\n7.5\nUrban - House\n1.0\n0.8\n7.0\nTable A2. Scene List \u2013 Parameters used for each scene for the\nimage diffusion model, with denoising strength, ControlNet scale,\nand guidance scale. Rows with multiple values indicate the need\nfor a second iteration, as discussed in Sec. 3.3. For the \u2018field\u2019 scene,\nwe used Stable Diffusion 1.5 [41], which does not have a denoising\nstrength parameter.\nstrength, the ControlNet condition and the guidance scale.\nWe provide the parameters used for each scene in Tbl. A2.\nMetrics\nFor the quantitative evaluation of our results\n(Sec. 4.4), we use two metrics, the first one is the CLIP text-\nto-image similarity score [10] and the second one is a new\nbackground preservation metric. This background preserva-\ntion metric measures the background difference between a\nrender Iorg from the original NeRF and a render Iedit from\nthe edited NeRF. We use the corresponding mask M pro-\nvided by the picked selection method (Sec. 3) and compute\nthe background preservation as:\nPSNRbg = PSNR(Iorg \u00b7 (\u00acM), Iedit \u00b7 (\u00acM))\n(4)\nSSIMbg = SSIM(Iorg \u00b7 (\u00acM), Iedit \u00b7 (\u00acM))\n(5)\nB. Viewer\nOur development includes a NeRF viewer designed to exam-\nine the trained NeRF scenes and facilitate the placement of\nproxy objects or the selection of scene elements for modi-\nfication. The viewer\u2019s backend is based on Nerfstudio [50],\nwhile the frontend is entirely novel, incorporating a suite\nof advanced features. A depiction of the viewer interface is\npresented in Fig. B9. Constructed as a React [29] applica-\ntion, it leverages React Three Fiber [15], a React renderer\nfor Three.js [5], to facilitate 3D scene manipulation. The\nNerfstudio backend streams the NeRF data to the frontend,\n12\n\u201cA brown rabbit ...\u201d\nFigure B9. Viewer \u2013 Outlining the placement of a proxy object\nwithin an existing NeRF scene. The viewer is divided into a 3D\nviewport at the center, a scene and selection column on the left and\na selection specific control column on the right.\nwhere it is displayed centrally in the viewport. We have aug-\nmented the backend to transmit not just the scene render but\nalso the corresponding depth map. This data enables us to\nutilize a shader that merges the NeRF render with Three.js\u2019s\nnative rendering, allowing for the visualization of occluded\nelements within the NeRF scene in real-time.\nEnhancements to the viewer include a scene hierarchy\nand intuitive manipulation tools, which simplify the process\nof placing proxy objects or delineating bounding boxes to\nselect regions of interest. Additionally, a bridge to SIGNeRF\nwas written, such that we can generate the reference sheet\nand dataset with the information provided by the viewer.\nConsequently, the viewer serves as a user-friendly platform\nfor performing scene-integrated generation on NeRF scenes.\nWe plan to release this viewer as open-source software, pro-\nviding the community with a powerful tool for NeRF scene\nediting.\nC. Dataset Generation\nSIGNeRF generates an updated NeRF image dataset with\nthe provided reference sheet to edit a NeRF scene. Our gen-\nerative pipeline (Fig. 2) demonstrates this with a selection of\nmodified dataset images, but a completely revised dataset is\nnot showcased. In Fig. C10, we exhibit a fully edited dataset\nof the \u2018field\u2019 scene (Fig. 3). Consistency is maintained across\nthe generated images; however, some outliers might occur.\nThese inconsistencies are minor over the dataset and are\ngenerally resolved by the NeRF optimization process, which\nlearns to ignore such outliers. For complex scenes, the use\nof an LPIPS [62] loss is beneficial to further improve the\nconsistency during the optimization.\n13\nFigure C10. Dataset Generation \u2013 Illustrating the consistency within edited views of an original NeRF dataset. The first row shows images\nfrom the reference sheet, the rest are the views generated with the reference sheet.\n14\n"
  },
  {
    "title": "A Vision Check-up for Language Models",
    "link": "https://arxiv.org/pdf/2401.01862.pdf",
    "upvote": "8",
    "text": "A Vision Check-up for Language Models\nPratyusha Sharma*\nTamar Rott Shaham*\nManel Baradad\nStephanie Fu\nAdri\u00b4an Rodr\u00b4\u0131guez-Mu\u02dcnoz\nShivam Duggal\nPhillip Isola\nAntonio Torralba\nMIT CSAIL\nAbstract\nWhat does learning to model relationships between\nstrings teach Large Language Models (LLMs) about the vi-\nsual world? We systematically evaluate LLMs\u2019 abilities to\ngenerate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a prelim-\ninary visual representation learning system can be trained\nusing models of text. As language models lack the ability to\nconsume or output visual information as pixels, we use code\nto represent images in our study. Although LLM-generated\nimages do not look like natural images, results on image\ngeneration and the ability of models to correct these gen-\nerated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the vi-\nsual world. Furthermore, experiments on self-supervised\nvisual representation learning, utilizing images generated\nwith text models, highlight the potential to train vision mod-\nels capable of making semantic assessments of natural im-\nages using just LLMs.\n1. Introduction\nWhat does it mean to understand the visual concept of e.g. a\n\u201cfrog\u201d? Is it sufficient to know the color of its skin, the num-\nber of feet it has, the location of its eyes, details about how\nit looks when it hops? While a subset of this information\nabout the appearance of a frog can be acquired from text,\nit is widely believed that to understand the concept visually,\none would need to observe an image of a frog or, better still,\nobserve it from different perspectives and in various real-\nworld scenes. However, to what extent can we learn about\nthe visual \u201cmeaning\u201d of different concepts from text1?\nDespite operating on textual data, language model rep-\nresentations have been shown to contain information about\nnamed perceptual concepts like shape and color [9, 22, 36]\nand have been found to be linearly translatable to represen-\n*Indicates equal contribution.\nProject page: https://vision-checkup.github.io/\n1While the question of what can be learned about the visual world from\nnatural language alone is interesting, in our case, \u201ctext\u201d represents the\nspace of all strings on the internet, including natural language and code.\ntations learned by vision models [27, 37]. These experi-\nments demonstrate that independently, vision and language\nmodels represent aspects of the world similarly. While in-\nvestigating model representations for a pre-selected set of\nattributes can inform us about information encoded by the\nmodel, it limits studying a fixed set of attributes at once and\nrequires access to the model\u2019s internal parameters. Instead,\nas seen in Fig. 1, we ask:\n1. What do language models know about the visual world?\n2. Can we train a vision system for natural images using a\ntext-only model?\nTo answer these questions, we evaluate what information\nabout the visual world off-the-shelf language models con-\ntain by testing their ability to render (draw) and recognize\n(see) real-world visual concepts. This allows us to measure\ntheir ability to model arbitrary properties, both individually\nand concurrently, without training a classifier for a rigid set\nof features one at a time. Although LLMs are limited in\ntheir ability to generate images using pixels, examples pre-\nsented by [8] suggest that models like GPT-4 can gener-\nate code capable of rendering objects like a unicorn. We\ntake this further by measuring LLMs abilities to generate vi-\nsual concepts of increasing complexity via a textual prompt\n\u2192 code \u2192 image procedure. Figure 2 shows examples of\ncomplex scenes generated by LLMs. We find that LLMs\nare surprisingly good at generating intricate visual scenes\ncomposed of multiple objects, effectively modeling spatial\nrelations between them. However, there are aspects of the\nvisual world that LLMs fail to capture, including objects\u2019\nproperties like their textures, precise shapes, as well as sur-\nface contact with other objects in the image.\nNext, we evaluate the ability of LLMs to recognize (see)\nperceptual concepts (Fig. 1 Part I (b)). We do this by collect-\ning human drawings represented as code and asking LLMs\nwhat they depict. The code describes the ordered sequence\nof shapes, their locations, and colors.\nWe find that un-\nlike humans, where creation is hard and verification is easy,\nmodels struggle to interpret/recognize code describing de-\ntailed scenes that they themselves can effectively generate.\nFurther, we demonstrate that the visual generation com-\npetence of a language model can be improved using text-\nbased corrections. We do this by closing the feedback loop\narXiv:2401.01862v1  [cs.CV]  3 Jan 2024\nLLM\nReturn code to \ndraw a bed\n<code>t\n\u201cCan you make it better?\u201d\nt = 0\n+\nLLM\n(c) Correcting LLM generated Images with Self-Generated Text Feedback\nPart I. Can a Language Model Draw, See and Correct with Text?\nReturn code to \ndraw a row \nof  bicycles\nLLM\n(a) Generation: Drawing Images with Text\n(b) Recognition: Seeing Images with Text\nfill(0);\nellipse(150, 150, 100, 100);\nellipse(250, 150, 100, 100);\n...\nWhat does the \nfollowing piece \nof code generate?\n+\nLake\nSky\nTerrace\nTraffic Lights\nSandy Beach\nvoid drawBicycle(x, y) {\n  // Wheels\n  fill(30);\n  ellipse(x, y, 50, 50);\n  ellipse(x + 70, y, 50, 50);\n  ...\nCompile\nPart II. Visual Representation Learning from Text-Generated Images\nDataset: LLM Images \u001f\n(a) Pre-Training: Unsupervised Contrastive Representation Learning\n(b) Evaluate on Real Images\nAnemone Fish\nAfrican Lion\nIce cream\n...\n... \nResNet 50\nHuman\nDrawing\nResNet 50\nFigure 1. Vision check-up for LLMs. I. Testing the visual knowledge of Language Models. We suggest a set of tests to check the\nvision abilities of language models, these include (a) the ability to write code that renders complex visual concepts (b) recognizing visual\nconcepts from code (c) correcting rendering code with text-only self-feedback. II. We test whether LLMs can generate data to train a\nhigh-performance vision system that can be used to make semantic judgments on natural images.\nbetween the LLMs and itself. Here, we first use the lan-\nguage model to generate code illustrating a concept. Fol-\nlowing that, the model is repeatedly called by condition-\ning its generation on its previously generated code and\nprompted to \u201cimprove its generated code\u201d. We find that\nmaking such iterative calls to the model results in improved\nvisual depictions, as shown in Fig. 1 (Part I (c)).\nFinally, we study if LLM-generated images could serve\nas a data source for pre-training vision models and com-\npare them to synthetically generated and natural images. By\nconstructing a pre-trained visual representation system from\nonly text that transfers well to tests on natural images, we\ndemonstrate that text models capture aspects of the visual\nworld similar to those present in natural images.\nTo summarize, the paper\u2019s main contributions are:\n1. The Visual Aptitude Dataset: Introducing a hierarchi-\ncal visual categories dataset consisting of shapes, ob-\njects, and scenes descriptions to test the visual capabili-\nties of language models.\n2. Generation: Testing and quantifying the generation ca-\npabilities of LLM\u2019s, showing that it is possible to gener-\nate detailed and diverse scenes using text-only models.\nWe also show that it is possible to improve the quality of\nthe generated images by using text-based feedback.\n3. Recognition: Analyzing whether LLMs are also able to\nrecognize image generative code as well as producing it.\nWe test this capability using out-of-distribution samples\ngenerated by humans, which we crowdsource.\n4. Training for natural image tasks without natural im-\nages: We show that the images generated by LLMs are\nuseful for training visual backbones, achieving state-of-\nthe-art performance when complemented with other pro-\ncedurally generated image datasets.\n2. Related work\nVision and language models: Language models have been\nshown to be extremely powerful in understanding and gen-\nerating visual information when paired with vision mod-\nels [42], training vision adaptors [15, 24, 27, 49], or when\ntrained jointly over visual and textual data [37, 40, 43]. Al-\nthough vision-language pre-training / chaining vision and\nlanguage models allow models to reason about aspects of\nthe visual world, we investigate the visual capabilities of\nmodels representing images with text. Furthermore, several\nbenchmarks have been proposed to evaluate the ability of\nLLMs on textual tasks [16, 18, 20, 25, 34]. Unlike them,\nwe propose a procedure to evaluate LLM\u2019s vision abilities.\nVisual Understanding in Language Models.\nMeaning\nRepresentations\n[9, 22, 36] show that language models\nObjects\nShapes\n-a-goat-with-their-noses-touching-at-fence.\nScenes\nFigure 2. Visual Aptitude Dataset. We collect a dataset of visual concepts of including shapes, objects and scenes, and ask LLMs to\ngenerate corresponding images using a Text \u2192 Code \u2192 Image generation procedure. Guess the captions of the scenes!2\ncontain information of the state of the world in their inter-\nnal representations that can be recovered by probing these\nmodels for attributes of interest. Additionally, [2, 28, 41]\ndemonstrate that language models are capable of represent-\ning visual concepts such as \u201ccolor\u201d and \u201cshape\u201d. However,\nthe attributes studied have been limited to only those that\ncan be described by natural language and can only be in-\nvestigated one at a time.\nMoreover, with models being\nplaced behind closed walls, it becomes increasingly diffi-\ncult to probe the internal representation of models for the\npresence of visual attributes of interest.\nProgram synthesis via LLMs.\nPioneered by OpenAI\u2019s\nCodex [10], Github\u2019s Copilot [1], Meta\u2019s Code Llama [32]\netc., LLMs have been shown to possess exceptional coding\ncapabilities [46]. Recently, Bubeck et al. [8] highlighted\nthe emergent properties of a GPT-4 model for image gener-\nation / scalable vector graphics via text-prompted TikZ or\njavascript codes. In this work, we build on their insights\nand carefully examine the diversity and realism of multiple\ntext-only language models like GPT-3.5, Davicini, GPT-4\n(see Fig. 3, Fig. 5) for programmable image generations.\nFurthermore, as one of our key contributions, we analyze\nthe usefulness of the procedurally generated images for self-\nsupervised visual representation learning (see Sec. 5).\nTraining vision models with synthetic data. The ability\nto train a vision system using synthetic data was studied in\nseveral tasks including optical flow [7, 14, 26], segmenta-\ntion [12, 30], detection [31], classification [4, 33], and rep-\nresentation learning [39]. Perhaps the most related set of\nwork studied how to train vision backbones using images\ngenerated from human-written code which capture differ-\nent priors of the visual world, such as textures and shapes.\nThese include generative processes like fractals [3, 21],\ndead leaves [6], sinusoidal waves [35], and even a crowd-\nsourced dataset of thousands of generative programs [5].\nWhile these approaches achieve competitive performance, it\nremains unclear how to systematically introduce high-level\nsemantic concepts related to shapes and scene composition\nwithout the intervention of human experts.\n3. Visual Aptitude Dataset: Points to Scenes\nWe evaluate an LLM\u2019s visual competence by measuring its\nability to create, recognize, and modify image-rendering\ncode on a hierarchy of concepts. This resulting dataset of\nimages also serves as the corpus used for pre-training a vi-\nsion model in the later part of the paper. We construct three\ndatasets with textual descriptions of visual concepts of grad-\nually growing complexity. Starting with simple shapes and\n2Captions for Fig 2 scenes: (left to right, top to bottom) Chef standing\nnext to a counter with jars; Office with leather couch, surrounded by books;\nRow of bicycles; Birthday boy with car shape cake & candles; Black &\nwhite cat sitting on side of a computer monitor; Couple of men hearding\nsheep down the road; Two stuffed animals cutting bread & spreading jelly\non it; Blurred image of motorized scooters on wooded road. Bathroom\nwith two sinks & tow mirrors. Yellow & blue train is next to an overhang;\nShapes\nObjects\nScenes\n60\n70\n80\n90\n100\nMedian CLIP percentile [%]\nDavinci\nGPT-3.5\nGPT-4\nOracle (Stable Diff.)\nFigure 3. Image-Text Fidelity. Median CLIP image-text retrieval\npercentiles of images generated by different LLMs. We include\nStable Diffusion as an Oracle. Chance is 50%.\ntheir compositions, to objects, and finally to complex scenes\ndescribed by elaborate natural language descriptions. Sam-\nples from the dataset can be found in Fig. 2 and in the\nSupplementary Material (SM).\n(i) Shapes and their compositions: The first dataset con-\ntains a composition of shapes from different categories such\nas points, lines, 2D-shapes, and 3D-shapes with 32 different\nattributes like color, texture, location, and spatial arrange-\nment. The full dataset contains more than 400K examples,\nand we sample 1500 for tests in our experiments.\n(ii) Objects: The second dataset contains the 1K most\nfrequent objects of the ADE20K dataset [47, 48]. Objects\nare more difficult to generate and recognize than shapes, as\nthey contain complex compositions of many shapes.\n(iii) Scenes: The last dataset consists of complex scene\ncaptions describing diverse places with multiple objects.\nFor this, we uniformly sample 1000 scene descriptions at\nrandom from the MS-COCO [23] dataset.\n3.1. Representing images with code\nIn the dataset, the visual concepts are described with lan-\nguage. For instance, we can describe a scene as \u201ca sunny\nsummer day on a beach, with a blue sky and calm ocean.\u201d\nWe test the visual competence of LLMs by prompting them\nwith these descriptions and measuring their ability to gen-\nerate code that can be compiled to render images depicting\nthe scenes. Why code? While LLMs can sequentially out-\nput pixel values to generate images [8] their ability to do\nso is currently limited. Code, on the other hand, can pro-\nvide a descriptive yet concise representation of the visual\nworld. It can be used to represent higher-level perceptual\nattributes and language models are already trained on exam-\nples of code. In this paper, code in different programming\nlanguages will serve as the primary mode of interaction and\nvisual scene generation for LLMs.\n3.2. Models and Programming Languages tested\nFor this study, we evaluate four language models, each\ntested on four different programming languages.\n(i) Language models:\nWe evaluate the visual com-\npetence of GPT-3.5 (both text-davinci-003 and\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nLPIPS-Diversity (higher is better)\n150\n200\n250\n300\nFID (lower is better)\nDavinci\nsamp\nDavinci\nfunc\nGPT-3.5\nsamp\nGPT-3.5\nfunc\nGPT-4\nsamp\nGPT-4\nfunc\nOracle\n(Stable Diffusion)\nFigure 4. Realism vs. Diversity. With both sampling strategies,\nLLMs are able to draw diverse illustrations of the same concept.\nGPT-3.5-turbo models) and GPT-43.\nModels, like\nLlama2 (chat 70B), GPT-J, and GPT-2 failed to generate\nexecutable image-rendering code reliably and are excluded\nfrom analysis in the main paper but are included in the SM.\n(ii) Programming languages. To validate that a model\u2019s\nvisual capability is not restricted to a specific programming\nlanguage we use four programming languages with differ-\nent expressiveness. These are: python-matplotlib, python-\nturtle, Processing (built over Java), and TikZ (built over\nTex). A model\u2019s ability to generate and recognize the same\nvisual concept across programming languages hints at the\nmodel possibly having a coherent and language-agnostic\nrepresentation of that concept.\n4. A Vision Checkup for LLMs\nIn this section, we evaluate the visual capabilities of LLMs.\nThe models are evaluated on three tasks: (i) Generation\n/ Drawing with text: assesses an LLM\u2019s competence in\ngenerating image-rendering code corresponding to a spe-\ncific concept.\n(ii) Recognition / Seeing with text: tests\nthe LLMs\u2019s performance in recognizing visual concepts and\nscenes represented as code. We test each model on the code\nrepresentation of human drawings. (iii) Correcting draw-\nings with text feedback: evaluates an LLM\u2019s ability to it-\neratively modify its generated code using natural language\nfeedback generated by itself.\n4.1. Generation: Drawing with Text\nExperimental Setup. To test what LLMs are capable of\nvisualizing, we evaluate their ability to generate code repre-\nsenting concepts from our Visual Hierarchy dataset across\nfour programming tools. The prompt fed to the LLM is:\nPrompt: \u201cwrite code in the programming language\n\u201d[programming language name]\u201d that draws a [concept]\u201d.\nWe then render the images by compiling the code in its\ncorresponding programming language. Additional details\nabout the experiment protocol can be found in the SM.\nEvaluation protocol. We evaluate the visual quality and di-\nversity of the images rendered using the following metrics.\n3There are two types of GPT-4. We interact here with GPT-4 model\nand not the GPT-4(V) model.\nArmchair\nVan\nStrawberry\nArcade\nFigure 5.\nDiversity.\nLLMs are capable of generating diverse\nmeaningful instances of the same concept, showcasing their abil-\nity to represent concepts beyond a single fixed prototype.\n(1) Fidelity: We compute the fidelity between the gen-\nerated image and its ground-truth caption by retrieving the\nbest caption for the image. We first calculate the agree-\nment between each image and all potential captions within\nthe same category (shapes/objects/scenes) using their CLIP\nscore. We then report the rank of the ground-truth cap-\ntion in percentage (e.g. a score of 100% implies that the\nground-truth concept is ranked first). In the SM, we aug-\nment this measure with human perceptual studies and show\nthat the CLIP rank reflects human preference as evidenced\nfrom their high correlation (r = 0.82,p-val= 1.25e\u221209).\n(2) Diversity: To assess the ability of the models to ren-\nder diverse content, we use the LPIPS-diversity score [45]\nover image pairs representing the same visual concept.\n(3) Realism: For a uniformly sampled collection of 1K\nimages from ImageNet [13], we use the Fr\u00b4echet Inception\nDistance (FID) [19] to quantify the difference in the distri-\nbution of natural images and those generated by LLMs.\nBaseline: As an oracle, we include images generated by a\ntext-to-image model (Stable Diffusion [29]) and report their\nscores across all evaluation metrics.\nWhat can LLMs visualize? We find that LLMs can vi-\nsualize real-world concepts from across the visual hierar-\nchy. Examples of LLM-rendered images can be found in\nFig. 1, 2, 5, and in the SM. LLMs are capable of gen-\nerating non-trivial visual compositions, examples of such\nare shown in Fig. 2; The model composes two unrelated\nconcepts (\u201ccar shaped cake\u201d), generates visual phenomena\n(\u201cblurred image\u201d), and manages to correctly interpret spa-\ntial relations (e.g. \u201ca row of bicycles\u201d arranged horizon-\ntally.) Unsurprisingly, the competence of the models de-\nteriorates with increasing concept complexity from shapes\nto scenes, as seen in the median image-text retrieval CLIP-\nscores across the different categories Fig. 3. For more com-\nplex visual concepts such as drawing scenes comprising\nmultiple objects, GPT-3.5 and GPT-4 are more accurate at\ndrawing scenes with intricate descriptions using processing\nand tikz than python-matplotlib and python-turtle. For ob-\njects and scenes, CLIP scores indicate that concepts con-\nModel Name\nObjects\nScenes\nDavinci\n0.136\n0.221\nGPT3.5\n0.228\n0.380\nGPT4\n0.234\n0.212\nBaseline [Chance]\n0.2\n0.2\nTable 1. Recognition of Human Drawings. Models struggle to\ncorrectly classify human drawings into their categories. While\nGPT-3.5 correctly classifies images over chance for the scenes cat-\negory, other models classify images correctly barely over chance.\ntaining \u201dperson\u201d, \u201dvehicle\u201d, and \u201doutdoor scenes\u201d are the\neasiest to draw (see full analysis in the SM). This ability\nto render complex scenes comes from the expressivity of\nthe rendering code, the model\u2019s programming capability in\neach of them, and the quality of its internal representations\nof the different concepts involved.\nWhat can LLMs not visualize? In some cases, even rela-\ntively simple concepts are difficult for the models to draw.\nWe identify several common failure modes: (a) Language\nmodels specifically struggle with concepts combining a set\nof shapes and a specific spatial organization, (b) Drawings\nare coarse and lack detail. These are the common failure\ncases for Davinci, especially when coding with matplotlib\nand turtle. (c) Depiction is incomplete, corrupted, or rep-\nresents only a subset of the concepts (typical for the scenes\ncategory). An interesting standalone failure case is drawing\ndigits. With all models and languages, we found this task\nto be challenging. See SM for a discussion of failure cases\nand the effect of prompting on the model\u2019s generation.\nDiversity and Realism. Language models exhibit the abil-\nity to generate diverse visualizations of the same concept as\nseen in Fig. 5. To generate different samples of the same\nscenes, we compare two strategies: (i) Repeated sampling\nfrom the model (ii) Sampling a parametrized function that\nallows creating a new drawing of the concept by changing\nparameters. The ability of the model to render diverse re-\nalization of visual concepts is reflected in the high LPIPS-\ndiversity scores in Fig. 4. The ability to generate diverse\nimages suggests that LLMs can represent the visual concept\nin many ways rather than a limited set of prototypes. LLM\ngenerated images are far from as realistic as natural images,\nwith the models scoring poorly on the FID metric as com-\npared to the Stable Diffusion oracle (Fig 4). However, it\nis interesting to note that modern models rank better than\nolder models, indicating that they might be slowly inching\ntowards increasingly realistic representations of the world.\n4.2. Recognition: Seeing with Text\nRecognizing the contents of an image requires inferring\nhow elements such as points, strokes, colors, and shapes\ncombine spatially to give rise to objects that are them-\nselves composed to represent complex scenes. Evaluating a\n(a) Objects\n(b) Scenes\na strawberry\na ceiling lamp\na shirt\na bed\na goose\na door\na car\na coffee table\na bicycle\np\ng\nin the sky flying overhead\nf\ng\ngear on a fishing pier\ng\npast a furniture shop\n \nThree women sitting on a park \nbench next to each other\nA bathroom with a vanity\nmirror next to a white toilet\nA baseball player bobble head next \nto two computer monitors on a desk\ntwo motocross racers in \nthe middle of a race\nA metallic toilet sitting in \na small bathroom\nAn adorable cat attempts to hide in \na purse to steal the persons identity\nFigure 6. Human drawings. Examples of drawings made by users using our drawing interface that passed the CLIP score filtering. Each\nof the collected drawings is converted into processing code and is then included in the LLM\u2019s recognition tests.\nmodel\u2019s ability to recognize image-rendering code offers in-\nsights into its competence in interpreting high-dimensional\nrepresentations of visual scenes, including ones very differ-\nent from its \u201cmemorized\u201d prototypes. While code on the\ninternet can be a good source to test these models, it con-\ntains comments, print statements, and symbol names that\nreveal essential semantic information about what the code\nrepresents and may not actually require sophisticated infer-\nence over the steps used to create the drawing. Further, we\nwanted to ensure that the code used for evaluation recog-\nnition was new and not part of the training set for any of\nthe models tested. Therefore, we collect our own dataset of\ncode representing drawings as seen in Fig. 6.\nDataset. While most people excel at drawing images using\n\u201cPaint\u201d-like applications, writing code that represents im-\nages is trickier and requires more skill. To test the recogni-\ntion ability of models we collect a dataset of human draw-\nings and their corresponding representation in code. This\ndataset was constructed following a setting similar to the\ngame Pictionary or Google\u2019s \u201cQuick Draw!\u201d application.\nUsers were prompted to draw a concept from the Visual Ap-\ntitude Dataset within a two-minute time limit using a draw-\ning interface. The interface recorded their drawing steps\nin the Processing programming language. Shapes were ex-\ncluded from the analysis, and the generated images were\nfiltered for quality using the Fidelity score. The interface\nwas designed to mimic the components expressed in draw-\nings made with processing and to encourage users to por-\ntray the entire prompt rather than a detailed but incomplete\nillustration in the allotted time. We collect 600 images per\ncategory, of which 162 object images and 113 scene images\npass the fidelity check (rank\u2264 40) and are used for evalua-\ntion. For further details, refer to the SM.\nEvaluation protocols. We evaluate the models\u2019 capabil-\nity of recognizing visual concepts represented as code by\nmeasuring the model\u2019s ability to classify the image in a\nmulti-class classification setting. In this test, the model is\nprompted with the code and a list of visual concepts, where\none of the list\u2019s entries describes the code accurately. The\nmodel is then tasked with matching the visual concept to the\ncorrect entry. The prompt to the model is:\nPrompt: \u201dWhich of the following concepts from the list\ndoes the [code] represent? [concept1, concept2,...]\u201d.\nBaseline: The success of the chance baseline is decided\nby the probability of the desired outcome. This is given by\n1/N where N is the number of outcomes. For the multi-\nclass classification setting with five labels this is 0.2.\nAnalysis. Human drawings present a unique recognition\nchallenge as there is a lot of diversity in the images repre-\nsenting a given concept.\nLanguage models can do very limited spatial reason-\ning. Table. 1 shows that GPT-3.5 beats the chance base-\nline across the scenes setting, demonstrating that the visual\nrecognition capability of the models is non-trivial, allowing\nit to (limitedly) recognize code representing human draw-\nings. While the exact mechanism that allows models to do\nso is unclear, the task requires models to identify objects,\ntheir attributes and spatial arrangements. Wrongly attribut-\ning any information can result in a completely different vi-\nsual scene. Although models cannot perfectly recognize all\nimages, the exhibited recognition capability is non-trivial.\nModels can fail to recognize concepts they can other-\nwise draw very well. Unlike humans, where the ability\nto draw something well automatically implies the ability to\nrecognize the same concept well, models can fail to recog-\nnize concepts they have no problem rendering. This con-\ntradicts the notion that creation is hard, but verification can\nbe easy. Failures in recognition, as shown in the SM, show-\ncase that images that models fail to recognize are the non-\nstereotypical depictions of the visual concept, showcasing\nthat there might be limits to a model\u2019s ability to recognize\nperceptual concepts from code.\n4.3. Textual-Feedback: Correcting with Text\nThe model\u2019s ability to generate an image is limited in part\nby the prompt. Failure to generate an image corresponding\nto a particular concept does not necessarily imply its lack of\n\u201dknowledge\u201d of that particular concept but rather its lack of\n\u201cReturn \ncode\n to draw \na swivel \nchair\u201d\n\u201cDraw a\nrectangular\nback rest \nfor a realistic\nappearence.\u201d\n\u201cAdd \nrotation\nfunction\nto make \nthe chair\nswivel.\u201d\n\u201cMake the\nlegs of the\nlonger and\nmove the \ncasters or\nmini wheels\nfurther out\u201d\n\u201cGenerate\na more \naccurate \nimage of a\nswivel chair.\u201d\n\u201cDraw the \nchair with \na more\naesthetically\npleasing\nappearence.\u201d\nIncreasing Feedback Iterations\nIncreasing Feedback Iterations\n(a) Textual Feedback: Self-correcting with text\n(b) Images generated with increasing feedback iterations \nFigure 7. Improved visual generation with text feedback. The improvement in the visual generation of models due to feedback is\noftentimes gradual, with the addition of a few features at a time over the course of the feedback process. Conditioning on the code\ngenerated from the previous iterations, to a limited degree, ensures that the model is constrained to modifying aspects of the current image.\nimmediate accessibility. Could direct systematic guidance\nand textual feedback help improve a model\u2019s visual capabil-\nity? And if so, can this be automated?\nExperimental Protocol. The visual competence of a gen-\nerator language model can be improved by pairing it with\nitself.\nThis procedure serves as prompt-facilitated \u201dself-\ntalk\u201d and helps scope out the model\u2019s internal representa-\ntion of a particular visual concept over multiple rounds of\ngeneration. Additionally, selective step-wise correction of a\nmodel\u2019s generation gives us insights into whether the model\nhas memorized a fixed \u201dprototype\u201d corresponding to a vi-\nsual concept or if its representation can be systematically\nmodified by iteratively prompting it to improve its drawing.\nPrompt: \u201dThe following [code] does not accurately rep-\nresent the [concept]. How can you do better?\u201d.\nEvaluation Protocol. To evaluate the improvement in the\nmodel\u2019s drawing capability, we use the fidelity score that\nwas described in Section 4.1.\nBaseline: To assess whether the model\u2019s drawing im-\nprovements stem from textual feedback or repeated model\ncalls, we conduct a \u2019variability\u2019 baseline. We generate 20\nindependent outputs for the same concept and report the me-\ndian CLIP percentile for the best images per concept. Mean\nvariability score is reported in Fig. 8.\nThe visual competence is improved solely by text-based\nfeedback. We present examples of images generated with\niterative feedback in Fig. 7. Visual quality and fidelity to the\nvisual concepts significantly improve with multiple rounds\nof feedback. Fig. 8 shows that GPT-3.5, a weaker genera-\nFigure 8.\nText-feedback improves visual generation compe-\ntence: Improvement in the quality of the generations is depicted\nby increasing median CLIP percentile as a function of feedback it-\nerations. The model with textual corrections outperforms selecting\nthe best image from multiple random samples of the same concept.\ntor than GPT-4, can be made to be nearly as competent of\nan image generator as GPT-4 with text feedback from it-\nself. Therefore, models can benefit in returning an overall\nbetter answer from multiple rounds of self-verification. In\nfact, with 20 rounds of feedback, the average performance\nof GPT-3.5 approaches that of GPT-4. Further analysis and\nvisualization for the dataset are in the SM.\n5. Learning a Vision System from Text\nLastly, we test whether LLM-generated images can be used\nto train a generally capable vision system for natural im-\nages. This relates to abundant existing work [3, 5, 6, 21, 35]\nthat studies the same phenomena by pretraining vision sys-\ntems using only procedurally generated images.\nTraining and evaluation protocol. We use unsupervised\ncontrastive learning, which allows pretraining vision back-\nbones with an unlabeled set of images, and follow the train-\ning and evaluation protocol of [6].\nWe train a ResNet-\n50 [17] using the MoCo-v2 method [11] over a dataset\nof 1.3M 384 \u00d7 384 images generated by LLMs, for 200\nepochs with batch size of 256. For comparison, we gen-\nerate datasets of the same size and image resolution with\n4 different procedural generation methods [5, 6, 21]. Af-\nter training, we evaluate the performance of models trained\non each of the datasets using two approaches: (i) training a\nlinear layer on top of the frozen backbone for ImageNet-1k\nclassification for 100 epochs, and (ii) using 5-nearest neigh-\nbor retrieval on ImageNet-100 [38]. The latter is of spe-\ncial interest, as nearest-neighbor retrieval shows that models\ntrained solely on LLM generated data yield powerful repre-\nsentations for natural images, without the need of training a\nlinear layer. This can be qualitatively seen in Fig. 9.\nBaselines and datasets.\nWe compare our LLM gener-\nated images against existing procedurally generated im-\nages. These include simple generative programs like dead-\nlevaves [6], fractals [21], and StyleGAN [6], each consist-\ning of a single program that generates highly diverse im-\nages. We also consider the Shaders-21k dataset [5], a large\ncollection of procedural image programs, each one pro-\nducing a family of images of high diversity. Our LLM-\ngenerated dataset consists of all the available images ob-\ntained using the different LLMs as described in Sec. 4.1, a\ntotal of 80k images. We augment these by randomly sam-\npling convex combinations of six data points using MixUP\n[44] (shown to be effective for other synthetic images [5]),\nand reach a total of 1.3M images. As an ablation, we re-\nport an additional experiment that excludes GPT-4 images\nfrom the training (a 31k subset of the total 80k). Addi-\ntional dataset breakdowns (i.e. each LLM individually) are\nreported in the SM.\nAnalysis. Table 2 shows that models trained with only\nLLM-generated images outperform simple datasets like\ndead-leaves or fractals, but are still inferior to alternatives.\nThrough visual inspection of the data, we attribute this in-\nferiority to the lack of texture in most LLM-generated im-\nages, as observed in the figures throughout the paper and\nin the SM. To overcome this, we combine the Shaders-\n21k dataset [5], which generates texture-rich images with\nthe samples obtained from LLMs. This is done by sam-\npling with 50% chance from either dataset and applying\nMixUP. As seen in Table 2, these models outperform all\nprocedural generation-based alternatives, showing that (i)\nLLM-generated images combined with textures are power-\nful representations for natural images, and (ii) scaling up the\nnumber of generative image procedures (by combining the\nprocedures in Shaders-21k with those sampled from LLMs)\nimproves overall performance, as predicted in [5]. We also\nFigure 9. Nearest Neighbors Retrieval on ImageNet-100. Near-\nest neighbors on ImageNet-100 for a randomly initialized network\nand a network trained with all our LLM-generated images.\nPre-training Dataset\nI-1k\nI-100\nLinear\n5-NN\nRandom Init.\nNone\n4.36\n4.28\nReal\nPlaces\n55.59\n57.04\nDead-leaves\n20.00\n12.76\nFractalDB-1k\n23.86\n17.24\nStyleGAN O\n38.12\n33.00\nProcedural\nS-21k\n44.83\n43.24\nLLMs (w/o GPT-4)\n33.60\n22.42\nLLMs (w/ GPT-4)\n36.16\n27.44\nLLMs (w/o GPT-4) + S-21k\n45.79\n43.40\nLLMs (w/ GPT-4) + S-21k\n46.03\n43.36\nTable 2. Learning a vision system. Top-1 Linear evaluation on\nImageNet-1k and 5-NN on ImageNet-100, for a ResNet-50 pre-\ntrained with different real and procedurally generated datasets in-\ncluding LLM\u2019s generated images.\nnote that the models trained with or without GPT-4 images\nachieve roughly the same performance. We end this sec-\ntion with the conclusion that LLMs, processing only textual\ninputs and outputs, can produce images with useful visual\nproperties that complement previous procedural generation\napproaches for training visual systems.\n6. Conclusions\nWe show that LLMs learn visual properties of the real\nworld, and can depict them in the form of procedural im-\nage code. We do so by first analyzing the properties of the\nsamples they generate, and showing that the model\u2019s com-\npetence can be further improved through text feedback from\nthe model itself. Finally, we show that the images produced\nby these models can be used to train useful vision back-\nbones for downstream tasks on natural images, gracefully\ncomplementing alternative approaches.\nAcknowledgement\nThis research was supported by a grant from the MIT-IBM\nWatson AI lab. Adri\u00b4an Rodr\u00b4\u0131guez-Mu\u02dcnoz was supported\nby the LaCaixa Fellowship, Tamar Rott Shaham was sup-\nported by the Zuckerman STEM Leadership Program and\nthe Viterbi Fellowship.\nReferences\n[1] Githubcopilot. https://github.com/features/copilot. 3\n[2] Mostafa Abdou, Artur Kulmizev, Daniel Hershcovich, Stella\nFrank, Ellie Pavlick, and Anders S\u00f8gaard.\nCan language\nmodels encode perceptual structure without grounding? a\ncase study in color. In Conference on Computational Natu-\nral Language Learning, 2021. 3\n[3] Connor Anderson and Ryan Farrell. Improving fractal pre-\ntraining. CoRR, abs/2110.03091, 2021. 3, 7\n[4] Shekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mo-\nhammad Norouzi, and David J Fleet. Synthetic data from\ndiffusion models improves imagenet classification.\narXiv\npreprint arXiv:2304.08466, 2023. 3\n[5] Manel Baradad, Chun-Fu Chen, Jonas Wulff, Tongzhou\nWang, Rogerio Feris, Antonio Torralba, and Phillip Isola.\nProcedural image programs for representation learning. In\nAdvances in Neural Information Processing Systems, 2022.\n3, 7, 8\n[6] Manel Baradad Jurjo, Jonas Wulff, Tongzhou Wang, Phillip\nIsola, and Antonio Torralba. Learning to see by looking at\nnoise. Advances in Neural Information Processing Systems,\n34:2556\u20132569, 2021. 3, 7, 8\n[7] John L Barron, David J Fleet, and Steven S Beauchemin.\nPerformance of optical flow techniques. International jour-\nnal of computer vision, 12:43\u201377, 1994. 3\n[8] S\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Jo-\nhannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat\nLee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid\nPalangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of ar-\ntificial general intelligence: Early experiments with gpt-4,\n2023. 1, 3, 4\n[9] Catherine Chen, Kevin Lin, and Dan Klein. Constructing\ntaxonomies from pretrained language models. arXiv preprint\narXiv:2010.12813, 2020. 1, 2\n[10] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Hen-\nrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda,\nNicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish\nSastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ry-\nder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Moham-\nmad Bavarian, Clemens Winter, Philippe Tillet, Felipe Pet-\nroski Such, David W. Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H.\nGuss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shan-\ntanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant\nMisra, Evan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welin-\nder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba.\nEvaluating large lan-\nguage models trained on code. ArXiv, abs/2107.03374, 2021.\n3\n[11] Xinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming\nHe. Improved baselines with momentum contrastive learn-\ning. CoRR, abs/2003.04297, 2020. 8\n[12] Yuhua Chen, Wen Li, Xiaoran Chen, and Luc Van Gool.\nLearning semantic segmentation from synthetic data: A geo-\nmetrically guided input-output adaptation approach. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 1841\u20131850, 2019. 3\n[13] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR09, 2009. 5\n[14] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2758\u20132766, 2015. 3\n[15] Constantin Eichenberg, Sidney Black, Samuel Weinbach,\nLetitia Parcalabescu, and Anette Frank. Magma\u2013multimodal\naugmentation of generative models through adapter-based\nfinetuning. arXiv preprint arXiv:2112.05253, 2021. 2\n[16] Yuling Gu, Bhavana Dalvi Mishra, and Peter Clark.\nDo\nlanguage models have coherent mental models of everyday\nthings? arXiv preprint arXiv:2212.10029, 2022. 2\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep Residual Learning for Image Recognition. In Proceed-\nings of 2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 770\u2013778. IEEE, 2016. 8\n[18] Jos\u00b4e Hern\u00b4andez-Orallo. Evaluation in artificial intelligence:\nfrom task-oriented to ability-oriented measurement. Artifi-\ncial Intelligence Review, 48:397\u2013447, 2017. 2\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. GANs trained by\na two time-scale update rule converge to a local nash equi-\nlibrium. In NIPS, 2017. 5\n[20] Yining Hong, Li Yi, Josh Tenenbaum, Antonio Torralba, and\nChuang Gan. Ptr: A benchmark for part-based conceptual,\nrelational, and physical reasoning. Advances in Neural In-\nformation Processing Systems, 34:17427\u201317440, 2021. 2\n[21] Hirokatsu Kataoka, Kazushige Okayasu, Asato Matsumoto,\nEisuke Yamagata, Ryosuke Yamada, Nakamasa Inoue, Akio\nNakamura, and Yutaka Satoh. Pre-training without natural\nimages. CoRR, abs/2101.08515, 2021. 3, 7, 8\n[22] Belinda Z Li, Maxwell Nye, and Jacob Andreas. Implicit\nrepresentations of meaning in neural language models. arXiv\npreprint arXiv:2106.00737, 2021. 1, 2\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 4\n[24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2\n[25] Kyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fedorenko.\nDissociating language and thought in large language models:\na cognitive perspective. arXiv preprint arXiv:2301.06627,\n2023. 2\n[26] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 4040\u20134048, 2016. 3\n[27] Jack Merullo, Louis Castricato, Carsten Eickhoff, and Ellie\nPavlick. Linearly mapping from image to text space. arXiv\npreprint arXiv:2209.15162, 2022. 1, 2\n[28] Roma Patel and Ellie Pavlick. Mapping language models to\ngrounded conceptual spaces. In International Conference on\nLearning Representations, 2022. 3\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 5\n[30] German Ros, Laura Sellart, Joanna Materzynska, David\nVazquez, and Antonio M Lopez. The synthia dataset: A large\ncollection of synthetic images for semantic segmentation of\nurban scenes.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3234\u20133243,\n2016. 3\n[31] Artem Rozantsev, Vincent Lepetit, and Pascal Fua. On ren-\ndering synthetic images for training an object detector. Com-\nputer Vision and Image Understanding, 137:24\u201337, 2015. 3\n[32] Baptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Tan, Yossi Adi, Jingyu Liu, Tal\nRemez, J\u00b4er\u00b4emy Rapin, Artyom Kozhevnikov, Ivan Evti-\nmov, Joanna Bitton, Manish Bhatt, Cristian Ferrer, Aaron\nGrattafiori, Wenhan Xiong, Alexandre Defossez, Jade Copet,\nand Gabriel Synnaeve. Code llama: Open foundation models\nfor code, 2023. 3\n[33] Mert Bulent Sariyildiz, Karteek Alahari, Diane Larlus, and\nYannis Kalantidis. Fake it till you make it: Learning trans-\nferable representations from synthetic imagenet clones. In\nCVPR 2023\u2013IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023. 3\n[34] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu\nAwal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta, Adri`a Garriga-\nAlonso, et al.\nBeyond the imitation game: Quantifying\nand extrapolating the capabilities of language models. arXiv\npreprint arXiv:2206.04615, 2022. 2\n[35] Sora Takashima, Ryo Hayamizu, Nakamasa Inoue, Hi-\nrokatsu Kataoka, and Rio Yokota. Visual atoms: Pre-training\nvision transformers with sinusoidal waves. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 18579\u201318588, 2023. 3, 7\n[36] Ian Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak,\nR Thomas McCoy,\nNajoung Kim,\nBenjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al. What\ndo you learn from context?\nprobing for sentence struc-\nture in contextualized word representations. arXiv preprint\narXiv:1905.06316, 2019. 1, 2\n[37] Yoad Tewel, Yoav Shalev, Idan Schwartz, and Lior Wolf.\nZero-shot image-to-text generation for visual-semantic arith-\nmetic. arXiv preprint arXiv:2111.14447, 2021. 1, 2\n[38] Yonglong Tian, Dilip Krishnan, and Phillip Isola.\nCon-\ntrastive multiview coding. CoRR, abs/1906.05849, 2019. 8\n[39] Yonglong Tian, Lijie Fan, Phillip Isola, Huiwen Chang, and\nDilip Krishnan. Stablerep: Synthetic images from text-to-\nimage models make strong visual representation learners.\narXiv preprint arXiv:2306.00984, 2023. 3\n[40] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill.\nMultimodal few-shot\nlearning with frozen language models. Advances in Neural\nInformation Processing Systems, 34:200\u2013212, 2021. 2\n[41] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret\nZoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma,\nDenny Zhou, Donald Metzler, et al. Emergent abilities of\nlarge language models.\narXiv preprint arXiv:2206.07682,\n2022. 3\n[42] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang,\nZecheng Tang, and Nan Duan.\nVisual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv\npreprint arXiv:2303.04671, 2023. 2\n[43] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,\nEhsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,\nMichael Zeng, and Lijuan Wang.\nMm-react: Prompting\nchatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023. 2\n[44] Hongyi Zhang, Moustapha Ciss\u00b4e, Yann N. Dauphin, and\nDavid Lopez-Paz. mixup: Beyond empirical risk minimiza-\ntion. CoRR, abs/1710.09412, 2017. 8\n[45] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 5\n[46] Ziyin Zhang, Chaoyu Chen, Bingchang Liu, Cong Liao, Zi\nGong, Hang Yu, Jianguo Li, and Rui Wang. A survey on\nlanguage models for code, 2023. 3\n[47] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela\nBarriuso, and Antonio Torralba.\nScene parsing through\nade20k dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 633\u2013641,\n2017. 4\n[48] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. International\nJournal of Computer Vision, 127:302\u2013321, 2019. 4\n[49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny.\nMinigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 2\n"
  },
  {
    "title": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality",
    "link": "https://arxiv.org/pdf/2401.01854.pdf",
    "upvote": "8",
    "text": "Multilingual Instruction Tuning With Just a Pinch of Multilinguality\nUri Shaham\u03c4 \u2217\nJonathan Herzig\u03b3\nRoee Aharoni\u03b3\nIdan Szpektor\u03b3\nReut Tsarfaty\u03b3\nMatan Eyal\u03b3\n\u03c4 Tel Aviv University\n\u03b3 Google Research\nAbstract\nAs instruction-tuned large language models\n(LLMs) gain global adoption, their ability\nto follow instructions in multiple languages\nbecomes increasingly crucial. In this work,\nwe investigate how multilinguality during in-\nstruction tuning of a multilingual LLM af-\nfects instruction-following across languages\nfrom the pre-training corpus. We first show\nthat many languages transfer some instruction-\nfollowing capabilities to other languages from\neven monolingual tuning. Furthermore, we find\nthat only 40 multilingual examples integrated\nin an English tuning set substantially improve\nmultilingual instruction-following, both in seen\nand unseen languages during tuning. In gen-\neral, we observe that models tuned on multilin-\ngual mixtures exhibit comparable or superior\nperformance in multiple languages compared\nto monolingually tuned models, despite train-\ning on 10x fewer examples in those languages.\nFinally, we find that diversifying the instruc-\ntion tuning set with even just 2-4 languages\nsignificantly improves cross-lingual generaliza-\ntion. Our results suggest that building mas-\nsively multilingual instruction-tuned models\ncan be done with only a very small set of multi-\nlingual instruction-responses.\n1\nIntroduction\nInstruction tuning is a fundamental aspect of build-\ning modern general-purpose large language mod-\nels (LLMs), involving fine-tuning a pre-trained\nmodel on pairs of instructions and corresponding\nresponses (Mishra et al., 2022; Wei et al., 2022;\nSanh et al., 2022; Ouyang et al., 2022). For these\nmodels to be globally applicable, they must op-\nerate on a wide range of languages, yet, most in-\nstruction tuning datasets are typically limited to\nEnglish. While curating naturally occurring in-\nstructions and responses for every language is chal-\nlenging, cross-lingual transfer has emerged as a\n\u2217*Work done during an internship at Google Research.\npromising approach, in which a model is fine-tuned\nusing one language, and acquiring similar abili-\nties in another (Pires et al., 2019; Wu and Dredze,\n2019; Artetxe and Schwenk, 2019; K et al., 2020;\nConneau et al., 2020a,b). The ability to follow in-\nstructions for languages seen only at pre-training\ncan significantly expand the applicability of LLMs,\nallowing them to be used by more people world-\nwide. In this work, we show that instruction-tuning\nof multilingual LLMs transfers across languages\nbetter than previously known, and that even mini-\nmal language diversity in the tuning set can further\nunlock instruction-following generalization to lan-\nguages that are unseen during instruction tuning.\nWe investigate the effect of multilingual data\non instruction-following across languages using an\nLLM pre-trained on hundreds of languages (Anil\net al., 2023), and high-quality, open-ended instruc-\ntions and responses (Zhou et al., 2023; K\u00f6pf et al.,\n2023) translated into 11 languages, across different\nfamilies and writing systems. Initially, we examine\nthe transferability of monolingual instruction tun-\ning across different languages. Naturally, tuning\nusing each language individually enhances perfor-\nmance within that language. Notably, we find that\nthis also translates into instruction-following capa-\nbilities across other languages, and that tuning with\nEnglish, Italian, or Spanish yields the best average\nmultilingual performance.\nInspired by this result, we turn to ask how much\nmultilingual data is required to improve multilin-\ngual instruction-following, while preserving En-\nglish performance. We find that replacing even\njust 40 English training examples with multilin-\ngual examples, significantly improves instruction-\nfollowing in those languages. Surprisingly, this\nsmall amount of language-diverse examples also\nimproves performance for languages that are only\nseen during pre-training and are not represented in\nthe instruction tuning set at all.\nThe next question we tackle is whether increas-\narXiv:2401.01854v3  [cs.CL]  13 Feb 2024\ning the number of languages in the tuning set can\nenhance generalization to new languages from the\npre-training corpus. We find that tuning using a\nfew languages enables better performance for lan-\nguages unseen during tuning, compared to mono-\nlingual tuning with the same number of examples.\nFinally, we test two potential factors that might\ninfluence the degree of cross-lingual transfer: lan-\nguage similarity and the amount of language-\nspecific pre-training data, but find no signifi-\ncant correlations.\nOverall, our results provide\nrecipes for multilingual instruction tuning that im-\nproves cross-lingual generalization, while preserv-\ning performance on English, under a fixed bud-\nget. In particular, we find that capable multilingual\ninstruction-following models can be tuned even\nwith a minimal amount of multilingual data.\n2\nMeasuring Multilingual\nInstruction-Following\nOur objective is to discover how multilinguality\nduring instruction tuning affects general-purpose\ninstruction-following across languages. We break\nthis down to multiple questions, including how well\ncan monolingual instruction tuning transfer to other\nlanguages, how many multilingual examples can\nenhance multilingual instruction-following while\npreserving English performance, and whether in-\ncreasing the number of languages can result in im-\nproved cross-lingual generalization. In this section\nwe elaborate on the data, evaluation protocol, mod-\nels we use, and the human annotation process to\nensure the models quality.\nData\nWe use datasets of high-quality open-ended\ninstructions and responses, rather than classic task-\nspecific datasets. Our training data contains 1,000\nEnglish instructions and responses from LIMA\n(Zhou et al., 2023) and 3,640 from OpenAssistant1\n(K\u00f6pf et al., 2023). These examples resemble real\nworld scenarios of users interacting with chatbots,\nwith queries like \"Can you explain Fermat\u2019s Last\nTheorem?\" and \"How to keep a dog hydrated?\",\nthat enable efficient tuning even with a small train-\ning set (Zhou et al., 2023). For evaluation, we use\n617 instructions from AlpacaFarm (Dubois et al.,\n2023), originated from Self-Instruct (Wang et al.,\n2023), Vicuna (Chiang et al., 2023), Koala (Geng\n1We focus on single-instruction/single-response interac-\ntions so we keep only the first prompt and response from\nconversations in OpenAssistant similarly to Li et al. (2023).\net al., 2023), and hh-rlhf (Bai et al., 2022).2\nWe use the Google Translate API3 to translate\nthe instruction-response pairs of the training set\nand the instructions of the evaluation set to 11 lan-\nguages, creating parallel training and evaluation\nsets in Arabic, Chinese, Czech, English, Estonian,\nFinnish, Hebrew, Hindi, Italian, Russian, Spanish,\nand Swahili.4 While translated data is different\nfrom naturally sourced data per language, it allows\nfor more control as the data size and semantics are\nsimilar for all languages. A overview of the lan-\nguages, their language codes, families and scripts\nis described in Table 2 in Appendix A.\nEvaluation\nWe conduct a side-by-side automatic\nevaluation protocol (Bubeck et al., 2023; Dubois\net al., 2023; Dettmers et al., 2023; Gudibande et al.,\n2023; Zheng et al., 2023), in which an LLM as-\nsesses two responses for the same instruction, with\nthe goal of identifying the superior one. We follow\nthe common practice of presenting both responses\nto the model twice, alternating the order of the two\nresponses (Zheng et al., 2023; Zhang et al., 2023).\nThe exact prompt we use is shown in Figure 9 in\nAppendix B. We define a \u201cwin\" for a certain re-\nsponse if the judge selects it twice irrespective of\nthe order, and a \u201ctie\" if the model selects a different\nresponse for each order. We use a discounted-tie\n(Zhou et al., 2023) scoring method, in which a\nmodel receives a score of 1 for a win, 0.5 for a\ntie, and 0 for a loss. We average the scores of\nindividual instructions to get the score over the\nevaluation set and present it in percentages. To\nvalidate that the LLM judge decisions align with\nhuman preferences across languages, we conduct a\nhuman annotation study and find good aggregated\nagreement scores of 79.5% for English, 77% for\nSpanish, and 76.5%, and 75% for Russian and He-\nbrew, receptively. Further details on validating the\nLLM judge are provided in Appendix D.\nInstruction-Following Score Per Language\nThroughout this work we measure instruction-\nfollowing per language by comparing the perfor-\nmance of a model that was tuned on some training\nset D, to a model that was monolingually tuned\non the target language L, by using the full training\n2We exclude AlpacaFarm\u2019s evaluation instructions from\nOpenAssistant, as we tune using its training set.\n3https://cloud.google.com/translate/docs/reference/api-\noverview\n4Languages are selected from Table 21 in Anil et al. (2023),\ndescribing the top-50 languages the model (\u00a72) was pre-\ntrained on.\nar\ncs\nen\nes\net\nfi\nhe\nhi\nit\nru\nsw\nzh\nTrain Language\n50.0\n40.9\n42.4\n44.1\n40.6\n39.5\n41.4\n35.9\n44.3\n39.7\n38.6\n35.0\nar\n38.4\n50.0\n44.8\n43.2\n44.1\n40.4\n38.2\n34.2\n46.3\n47.3\n37.7\n33.7\ncs\n15.9\n24.9\n50.0\n32.3\n22.0\n25.7\n19.0\n13.1\n30.5\n26.0\n16.7\n16.4\nen\n37.4\n40.3\n54.5\n50.0\n35.7\n36.7\n34.3\n28.7\n48.5\n41.5\n36.1\n32.2\nes\n37.3\n50.1\n47.5\n44.7\n50.0\n45.8\n41.7\n39.1\n45.1\n44.9\n42.1\n36.5\net\n38.3\n48.6\n51.0\n45.8\n44.2\n50.0\n42.8\n35.1\n46.4\n45.1\n41.9\n37.4\nfi\n41.9\n45.5\n44.8\n47.1\n41.2\n42.2\n50.0\n36.7\n47.6\n43.8\n41.5\n34.0\nhe\n35.1\n39.8\n42.9\n40.1\n40.0\n40.1\n35.2\n50.0\n40.2\n36.9\n37.3\n38.4\nhi\n33.0\n40.9\n46.5\n44.4\n33.5\n35.9\n33.8\n26.6\n50.0\n38.4\n36.5\n33.6\nit\n35.4\n45.1\n47.1\n48.0\n41.8\n39.4\n36.7\n34.4\n47.0\n50.0\n36.5\n36.0\nru\n39.9\n45.3\n46.4\n42.5\n40.3\n40.5\n39.6\n34.5\n42.1\n40.2\n50.0\n29.0\nsw\n30.5\n36.8\n43.7\n39.5\n36.0\n35.0\n33.0\n30.2\n37.5\n36.5\n32.5\n50.0\nzh\n36.1\n42.4\n46.8\n43.5\n39.1\n39.3\n37.1\n33.2\n43.8\n40.9\n37.3\n34.4\navg\nEvaluation Language\nFigure 1: Per language instruction-following scores of models instruction-tuned on monolingual data. Each row\nrepresents a model tuned using a different language, and each column is an individual heatmap of the scores of all\nmodels on the same evaluation language. Scores are the discounted-ties weighted average of the side-by-side scores\nagainst the model tuned on the evaluation language. The scores along the diagonal are 50 as they are the result of\ncomparing generations to themselves, and are excluded from the heatmap coloring.\nEnglish\nSpanish\nHebrew\nRussian\nEnglish\n(LIMA)\n83%\n71%\n68%\n57%\n50%\n15%\n20%\n19%\n30%\n38%\n2\n9%\n13%\n13%\n12%\nExcellent\nPass\nFail\nFigure 2: Human annotators rating distributions of mod-\nels responses across languages. Each row describes\nevaluation in its corresponding language of the model\ntuned monolingually using that language. Numbers in\nthe first row are reported by Zhou et al. (2023).\nset in this language, DL. Formally, we define our\ninstruction-following (IF) metric for language L:\nIFL(MD) = S\u00d7S(MDL, MD)\nWhere S\u00d7S(\u00b7, \u00b7) is the side-by-side protocol ap-\nplied on MDL and MD, which are the models\ninstruction-tuned on DL and D, respectively. A\nscore of 0% means that MD loses on all L instruc-\ntions, and 50% means the performance of MD and\nMDL in L are indistinguishable when aggregated\nover the evaluation set.\nModel\nWe use the PaLM 2 model family of\nTransformer-based (Vaswani et al., 2017) LLMs\nthat were pre-trained on hundreds of languages\n(Anil et al., 2023). We use PaLM 2-S as our pre-\ntrained model for all the instruction tuning experi-\nments, and an instruction-tuned PaLM 2-L as the\njudge for the side-by-side evaluation. The train-\ning and inference hyperparameters we use are de-\nscribed in Appendix C.\nHuman Validation\nOur evaluation protocol re-\nlies on the quality of our monolingually tuned mod-\nels. To validate their usage as high bar baselines\nin their respective languages, we conduct a human\nannotation study in 4 languages: English, Span-\nish, Russian and Hebrew. Namely, we sample 50\nrandom instructions per language, and ask 2 na-\ntive speakers to assign a score of excellent, pass,\nor fail (Zhou et al., 2023) to the responses gener-\nated by the model that was monolingually tuned\nusing that language. Results in Figure 2 show\nthat our tuned models indeed demonstrate strong\ninstruction-following abilities. Notably, the scores\nacross languages are similar or better than the re-\nported numbers by Zhou et al. (2023) in English.5\n5The differences can be attributed both to the pre-trained\nmodel and to the size of the instruction tuning dataset.\n1\n0\nEnglish only\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nUniform\nPercentage of Uniform Language Distribution\n42\n44\n46\n48\n50\n52\n54\n56\nInstruction-Following Scores\nEnglish\nNon-English\nAverage\nFigure 3: Instruction-following scores of models trained using when P% of the training set is distributed uniformly\nacross 12 languages and an (100 \u2212 P)% is English only. Each X axis tick represents a tuning mixture, scores over\nindividual non-English languages are in blue, and their averages are in red. English scores are in orange.\n3\nHow Much Multilinguality Is Needed\nFor Multilingual Instruction Tuning?\nWe now describe our controlled experiments, de-\nsigned to quantify the effect of multilingual data\nduring instruction tuning of multilingual LLMs,\nfollowing the research questions defined in \u00a72.\n3.1\nMonolingual Instruction Tuning Yields\nMultilingual Abilities\nTo explore zero-shot cross-lingual transfer of in-\nstruction tuning in multilingual LLMs, we tune\nmodels on a single language and evaluate them\non all of the rest. We find that all of those mod-\nels are able to transfer non-negligible instruction-\nfollowing abilities to other languages.\nSetup\nWe instruction-tune 12 models, each one\nusing the full train set in a different language. We\ngenerate responses using every such model to the\nevaluation instructions in all other languages. Fi-\nnally, we calculate their per language scores as\ndescribed in \u00a72.\nResults\nFigure 1 shows the results, where rows\nrepresent training languages and every column is\nan independent heatmap of the results over a sin-\ngle evaluation language. Most importantly, tuning\nusing each single language yields a model with\nsome multilingual instruction-following capabili-\nties across languages. For context, even the model\nwith the lowest average score, the one tuned on\nHindi, achieves a score of over 30% in 9 out of 11\ncases.6 The model with the best average score is\nthe one tuned on English, when Italian and Spanish\nalso enable consistently high scores.\nNotably, we manually inspect the generations\nand find that our tuned models consistently re-\nspond in the same language as their instruction,\nregardless of the language they were instruction-\ntuned on, in contrast with findings in previous\nwork (Touvron et al., 2023a; Chen et al., 2023).\nWe hypothesize that this comes from the multilin-\ngual nature of PaLM 2s\u2019 pre-training, compared to\nthe more English-centric LLaMA (Touvron et al.,\n2023a), further details are in Appendix E. In ad-\ndition to our main setup, we also compare the\ngenerations of these models to the ones of the\npre-trained model that was not instruction-tuned.\nResults shown in Figure 10 in Appendix F fur-\nther demonstrate that instruction tuning in every\nlanguage separately, greatly improves instruction-\nfollowing abilities across different languages.\n3.2\nA Few Dozen Examples Improve\nMultilingual Instruction-following\nNaturally, multilingual tuning, as opposed to\nEnglish-exclusive tuning under a fixed training ex-\namples budget, should result in better downstream\nperformance for non-English languages, and might\nhurt performance on English. Therefore, we ask\nhow many multilingual examples can improve the\ninstruction-following abilities across languages,\n6For example, a score of 30% can be obtained by wining\n30% of the instructions and losing 70%, or by achieving a tie\non 60% of the instructions and losing 40%.\n1\n0\nEnglish only\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nUniform\nPercentage of Uniform Language Distribution\n42\n44\n46\n48\n50\n52\n54\nInstruction-Following Scores\nEnglish\nNon-English\nAverage seen\nAverage unseen\nFigure 4: Instruction-following scores of models tuned when P% of the training set is distributed uniformly across 6\nlanguages and an (100 \u2212 P)% is English only. Each X axis tick represents such a tuning set, scores over individual\nnon-English languages are in blue and English scores are in orange. Average scores of the 5 non-English languages\nin the tuning set are in red, and the average scores of the 6 languages not seen during tuning are in green.\nwhile preserving English performance. To that end,\nwe tune models on subsets of the English examples\ncombined with subsets of multilingual examples in\ndifferent ratios. We find a significant boost in mul-\ntilingual instruction-following abilities even when\nusing just a few dozen multilingual examples.\nSetup\nWe create data mixtures with P% exam-\nples that are evenly split among all 12 languages,\nand the rest (100 \u2212 P)% English examples.7 We\ncreate such a train set for every P from 10 to 100,\nincremented by tens, and also for P = 1, for which\nonly 40 multilingual examples are included from\nacross all 11 non-English languages, and the rest\nare English examples. Finally, we evaluate every\ntuned model on every one of the 12 languages as\ndefined in \u00a72.\nResults\nFigure 3 visualizes the results. As ex-\npected, multilingual examples in the train set im-\nprove the score on their languages (Red), and dilut-\ning the number of English examples hurts the per-\nformance in English (Green). Notably, the signifi-\ncant multilingual improvement comes from replac-\ning only 1% of the English examples by multilin-\ngual ones, which translates to 40 examples evenly\ndistributed across the training languages. These\nresults on the effect of such a small amount of\nlanguage-diversity extend findings regarding task-\ndiversity by Zhou et al. (2023), which demonstrated\nthat a capable monolingual instruction-following\n7Every example appears exactly once in every mixture, in\na single language.\nmodel can be tuned using only 1,000 high-quality\nexamples. A second trend is that these models often\noutperform their monolingually-tuned counterparts\non the very language the latter were exclusively\ntuned on (blue markers above the 50 line). For\nexample, the model tuned using the uniform set\n(P = 100) preforms similarly or better than the\nindividual monolingually-tuned models in 8 of 12\nlanguages, despite being trained on 12 times less\ninstruction-response pairs for each language. This\nsuggests that for some languages, multilingual tun-\ning can enable better instruction-following abilities\ncompared to a traditional monolingual tuning with\nthe same number of examples.\n3.3\nA Few Dozen Examples Improve\nCross-lingual Generalization\nCombining the lessons on cross-lingual generaliza-\ntion from monolingual tuning and the effect of a\nsmall amount of multilingual examples from previ-\nous sections, we turn to examine how multilingual\nexamples in the tuning set affect language general-\nization. Specifically, we conduct a similar experi-\nment to the one in \u00a73.2, this time using only half of\nthe languages for tuning while the rest of languages\nare unseen. In line with the results from \u00a73.2, we\nfind that a very small amount of multilingual exam-\nples also improve performance on languages that\nwere not in the tuning set.\nSetup\nWe repeat the setup from \u00a73.2, this time\nwith only English and 5 more languages: Arabic,\n1\n2\n3\n4\n5\n6\nNumber of Train Languages\n35.0\n37.5\n40.0\n42.5\n45.0\n47.5\n50.0\n52.5\nUnseen Languages Scores\nUnseen languages\nAverage\nFigure 5: Instruction-following scores in Czech, Es-\ntonian, Hebrew, Hindi, Spanish, and Chinese of mod-\nels instruction-tuned using various subsets of Arabic,\nEnglish, Finnish, Italian, Russian, and Swahili. Blue\nmarkers are the average scores per evaluation languages\nacross models tuned with the same number of languages.\nThe averages of those individual languages scores are\nin green.\nFinnish, Italian, Russian, and Swahili, and evaluate\nmodels again on all 12 languages.\nResults\nResults in Figure 4 show similar trends to\nthe ones in Figure 3. Specifically, the average score\nover non-English training languages (red) again im-\nproves very quickly, even with P = 1. Strikingly,\nthis is also true for languages that the model has\nonly seen during pre-training, and are not repre-\nsented at all in the instruction tuning dataset (or-\nange). This suggests that very few multilingual\nexamples can not only improve performance for\nthe languages of those examples, but also enable\nbetter cross-lingual instruction-following general-\nization.\n3.4\nEven a Small Number of Languages\nImproves Cross-Lingual Generalization\nGiven the results on the impact of a small num-\nber of multilingual examples from a fixed set of\nlanguages, we ask whether a small number of lan-\nguages can also enhance cross-lingual generaliza-\ntion. We experiment with different numbers of lan-\nguages in the tuning set and indeed observe that the\ntransfer to languages only seen during pre-training\nimproves from the very first additional languages.\nSetup\nWe instruction-tune models on a single\nlanguage and up to 6 languages. At each step, we\nadd a language to the tuning set, and split the same\nexamples budget uniformly among the current set\nof languages. We use the 6 training languages\nfrom \u00a73.3, and follow 3 different permutations that\nes\nen en,es\n40\n45\nzh\nru ru,zh\n30\n40\nar\net\nar,et\n35\n40\nhe\ncs cs,he\n35\n40\nfi\nit\nfi,it\n35\n40\n45\nhi\nsw hi,sw\n30\n35\nAverage Score on Unseen Languages\nTrain Languages\nFigure 6: Average instruction-following scores of lan-\nguages not seen during instruction tuning. For example,\nthe top-left corner describes the scores of 3 models\ninstruction-tuned on 100% Spanish, 100% English, and\n50% Spanish and 50% English. The Y axis of this sub-\nfigure is the average score across all language excluding\nSpanish and English.\ndetermine the order in which we add languages to\nthe mix. These permutations are shown in Table 4\nin Appendix G. We evaluate every model on each of\nthe remaining 6 languages, and average scores per\nevaluation language across models that are tuned\nusing the same number of languages.\nResults\nResults on Figure 5 show that adding lan-\nguages to the tuning set improves cross-lingual gen-\neralization. The average score (red) increases from\ntuning on monolingual data to tuning on bilingual\ndata, and even more when using 3 and 4 languages,\nwhere the average score gets to almost 50. At that\npoint, there is an indication for saturation, as more\nlanguages does not seem to improve transfer fur-\nther. These findings demonstrate that diversifying\nthe instruction tuning data with only a few differ-\nent languages can improve cross-lingual transfer to\nnew languages, only seen during pre-training.\nBilingual Tuning Sets\nTo show this holds for\neven more combinations of languages, we ran-\ndomly split all languages to pairs, and tune models\nusing 50% of the examples in the one language and\n50% in the other. We evaluate each of these models\non the remaining 10 languages, and compare their\nscore to the ones of the two models tuned using\nthe full monolingual sets. Results on Figure 6 re-\nveal that bilingual tuning helps generalize to new\nlanguages better than monolingual tuning.\n4\nPotential Factors of Transferability\nFollowing the results from the previous sections, a\nnatural question arises: what factors can predict the\nLanguage\nCode\nSlavic\nScript\nMutually\nFamily\nIntelligible\nRussian\nru\nEast\nCyrillic\n-\nSerbian\nsr\nSouth\nCyrillic\nCroatian\nCroatian\nhr\nSouth\nLatin\nSerbian\nSlovenian\nsl\nSouth\nLatin\n-\nTable 1: Languages used for language similarity experi-\nment, along with their language code, subfamily, script,\nand the language they are mutually intelligible with.\ndegree of cross-lingual transfer? We explore two\nimmediate candidates. Initially, we examine the\nrelation of various aspects of language similarity to\ntransferability within language pairs. Next, we look\ninto whether the proportion of language-specific\ndata in the pre-training corpus correlates with the\namount of cross-lingual transfer of instruction tun-\ning using the given language.\n4.1\nLanguage Similarity\nA intuitive hypothesis is that aspects of language\nsimilarity like the script or mutual intelligibility\nmight affect the levels of instruction tuning cross-\nlingual transfer between languages. We test this\nusing a case study of 4 Slavic languages, looking\ninto possible effects of such aspects. However, we\ndo not find a signal indicating these factors strongly\ncorrelate with cross-lingual transfer for this setting.\nSetup\nWe train models on monolingual versions\nof the data in Russian, Serbian, Croatian, and\nSlovenian, and evaluate their transfer to each other.\nThese languages can be divided along several lin-\nguistic lines that are summarized in Table 1. First,\nRussian is East Slavic, and the rest are South Slavic.\nSecond, Russian and Serbian both use the Cyril-\nlic script, while Croatian and Slovenian use Latin.\nMoreover, Serbian and Croatian share a significant\ndegree of mutual intelligibility.\nResults\nResults are displayed on Figure 7. As\nshown, there is no a strong signal indicating that\nany of the aspects above is correlated with better\nmutual cross-lingual transfer. Russian tend to trans-\nfer instruction-following abilities best, and even\nthough Russian and Serbian both use Cyrillic, it is\nCroatian that transfers capabilities to Russian bet-\nter in our study. Moreover, Despite being largely\nmutually intelligible, Croatian and Serbian do not\nseem to share cross-lingual abilities more than the\nothers. Our results align with recent findings that\nlanguage similarity does not impact transferability\nhr\nru\nsl\nsr\nTrain Language\n50.0\n45.0\n41.6\n42.4\nhr\n45.6\n50.0\n40.5\n38.0\nru\n43.9\n47.6\n50.0\n45.5\nsl\n46.0\n49.1\n42.6\n50.0\nsr\n46.4\n47.9\n43.7\n44.0\navg\nEvaluation Language\nFigure 7: Instruction-following scores per language of\nmodels tuned monolingually. Each row represents a\nmodel trained using a different language, and each col-\numn is an individual heatmap of the scores of all models\non the same evaluation language. The scores along the\ndiagonal are excluded from the heatmaps coloring.\n0\n2\n4\n6\n8\n10\n12\nPrecentage of Documents in Pretraining Corpus\n34\n36\n38\n40\n42\n44\n46\nAverage Cross-Lingual Transfer Score\nes\nar\nhe\nit\nfi\net\nzh\ncs\nsw\nhi\nru\nCorrelation: 0.22\nFigure 8: Weak Pearson correlation between the percent-\nage of documents in the pre-training corpus (excluding\nEnglish), and the average instruction-following score\nacross languages for every training language. Blue area\naround the line is the confidence interval.\nor interference in machine translation given suf-\nficient data and model capacity (Fernandes et al.,\n2023; Shaham et al., 2023).\n4.2\nFraction of Data in Pre-training\nA second possible predictor of the degree of cross-\nlingual transfer from a particular language is the\nextent to which the model was exposed to it during\npre-training. Generally, a model\u2019s downstream per-\nformance on a specific language correlates with the\nfraction of data in that language in the pre-training\ncorpus (Muennighoff et al., 2023). In contrast, Fig-\nure 8 suggests this is not necessarily the case for the\ncross-lingual transfer from a specific language. We\nfind a weak Pearson correlation of 0.22 between\nthe average cross-lingual score of each language\nand the number of documents in that language in\npre-training corpus (Table 21 in Anil et al. (2023)).\n5\nRelated work\nCross-lingual Transfer\nThe success of the pre-\ntraining\u2013fine-tuning paradigm (Devlin et al., 2019)\nignited a new line of work on cross-lingual trans-\nfer. Pires et al. (2019) and Wu and Dredze (2019)\nshowed that the multilingual variant of BERT can\nbe fine-tuned on a specific task in one language\nand preform this task on another language, and\nArtetxe and Schwenk (2019) reported similar find-\nings with a Recurrent Neural Network. Conneau\net al. (2020a) introduced XLM-R, a multilingual\npre-trained encoder with strong cross-lingual abil-\nities. Phang et al. (2020) showed that intermedi-\nate training on an English task improves XLM-\nR\u2019s transfer across languages further, and Pfeiffer\net al. (2020) suggested an adapter-based frame-\nwork to improve cross-lingual and task general-\nization. Hu et al. (2020) proposed a benchmark\nfor cross-lingual generalization consists of 40 lan-\nguages across 9 NLP tasks.\nK et al. (2020) found that the depth of the net-\nwork matters for cross-lingual transfer, and Con-\nneau et al. (2020b) showed that parameter sharing\nis more important than shared vocabulary. Choenni\net al. (2023) delved into the influence of specific ex-\namples from the training data on the performance\nin other languages, and Malkin et al. (2022) in-\nvestigated how pre-training BERT-based models\nusing different language pairs affects cross-lingual\ndownstream performance. Going beyond encoder-\nonly models, Xue et al. (2021) proposed mT5, a\nmultilingual variant of T5 (Raffel et al., 2020), and\nshowed the significance of model scaling for cross-\nlingual transfer in generation tasks. Ye et al. (2023)\nexplored trasferability in English-centric models\n(Touvron et al., 2023a) using four tasks.\nIn contrast to most cross-lingual transfer litera-\nture that is focused on task-specific fine-tuning, we\nexplore trends of cross-lingual generalization for\ngeneral-purpose instruction-following LLMs.\nMultilingual\nInstruction\nTuning\nInitially,\nworks on instruction tuning (Mishra et al., 2022;\nWei et al., 2022; Sanh et al., 2022) focused on\ncross-task generalization in English. Subsequently,\na large body of work was dedicated to multilingual\ninstruction tuning.\nMuennighoff et al. (2023)\nfound that tuning models with English datasets\nenables zero-shot cross-lingual abilities to new\nlanguages. The authors also found that this holds\nfor languages that the model has never intentionally\nseen during pre-training, and that multilingual\ntraining improves generalization to new tasks.\nChen et al. (2023) investigated the effects of full\nparameter training vs low-rank adaptation (Hu\net al., 2022) and monolingual vs multilingual\ninstruction tuning using the Stanford Alpaca\n(Taori et al., 2023) data, machine translated into 5\nlanguages. Lai et al. (2023) trained multilingual\ninstruction-following models for 26 languages\nwith reinforcement learning from human feedback\n(Ouyang et al., 2022), and Zhang et al. (2023)\nsuggested instruction tuning LLMs by prepending\nthe instruction and response translated into a pivot\nlanguage (e.g English) to the response in the target\nlanguage.\nIn this work, we consider transfer from monolin-\ngual instruction tuning from 12 languages, rather\nthan exclusively on English. Furthermore, we ex-\namine multilingual instruction-following using an\nLLM pre-trained on hundreds of languages, which\nmight be a key to unlocking more transfer to lan-\nguages not represented during tuning. Importantly,\nwe unveil the potential of just a small amount of\nlanguage diversity in the instruction tuning set for\nthis cross-lingual generalization.\n6\nConclusion\nWe demonstrate that cross-lingual transfer of-\nfers a promising avenue for building multilingual\ninstruction-following LLMs. Our findings across\ndifferent languages suggest that even monolingual\ninstruction tuning using only one language can re-\nsult in improved instruction-following capabilities\nin other languages. Moreover, incorporating even a\nsmall set of a few dozen multilingual examples can\nsignificantly enhance instruction-following perfor-\nmance for both the languages the model is tuned on,\nand ones that were only seen during pre-training.\nAdditionally, training on such multilingual datasets\nachieves comparable or even superior performance\ncompared to monolingual tuning for some lan-\nguages. We observe a similar trend when exploring\nthe effect of total number of languages in the tuning\nset, as even splitting the train set to only two lan-\nguages improves generalization to new languages,\ncompared to monolingual tuning. These findings\npave the way for efficient and scalable development\nof multilingual LLMs capable of understanding\nand following instructions across languages with\nminimal multilingual supervision.\n7\nLimitations\nLimitations of our work include the use of trans-\nlation for expanding datasets to multilingual set-\ntings, the number of languages we evaluated on,\nand number of models we experimented with. We\nnow discuss each of them.\nTranslated data\nOne limitation of our work is\nthat our data is translated using the Google Trans-\nlate API, and not originally sourced by native\nspeakers. Automatic translation is inherently im-\nperfect and may introduce noise to the tuning sets.\nHowever, translation also allows to for a controlled\nsetup with parallel data, in which the content of all\ntraining and evaluation examples is the same for all\nlanguages.\nNumber of languages\nA second limitation is\nthat we use 12 languages in our main experiments\n(\u00a73), with 3 additional languages in the language\nsimilarity experiment (\u00a74.1). Clearly, multilingual\ninstruction-following models need to successfully\noperate in many more languages, and we leave\nwork on scaling this number to future work.\nNumber of models\nLastly, we experiment with\nPaLM 2, and results may vary with different LLMs.\nNevertheless, our focus on PaLM 2 highlights the\npotential of multilingual pre-training for future ad-\nvancements in LLMs.\nAcknowledgments\nWe thank Omer Levy, Or Honovich, Alon Jacovi,\nAvi Caciularu, and Omer Goldman for their valu-\nable feedback.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nMikel Artetxe and Holger Schwenk. 2019.\nMas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transactions\nof the Association for Computational Linguistics,\n7:597\u2013610.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022. Training a help-\nful and harmless assistant with reinforcement learn-\ning from human feedback.\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Pe-\nter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023. Sparks of artificial general in-\ntelligence: Early experiments with gpt-4.\nPinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Barry\nHaddow, and Kenneth Heafield. 2023. Monolingual\nor multilingual instruction tuning: Which makes a\nbetter alpaca.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nRochelle Choenni, Dan Garrette, and Ekaterina Shutova.\n2023. How do languages influence each other? study-\ning cross-lingual data sharing during LM fine-tuning.\nIn Proceedings of the 2023 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n13244\u201313257, Singapore. Association for Computa-\ntional Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440\u2013\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b.\nEmerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022\u20136034, Online. Association for Computational\nLinguistics.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. QLoRA: Efficient finetun-\ning of quantized LLMs. In Thirty-seventh Confer-\nence on Neural Information Processing Systems.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback.\nPatrick Fernandes, Behrooz Ghorbani, Xavier Garcia,\nMarkus Freitag, and Orhan Firat. 2023. Scaling laws\nfor multilingual neural machine translation.\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wal-\nlace, Pieter Abbeel, Sergey Levine, and Dawn Song.\n2023. Koala: A dialogue model for academic re-\nsearch. Blog post.\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang\nGeng, Hao Liu, Pieter Abbeel, Sergey Levine, and\nDawn Song. 2023. The false promise of imitating\nproprietary llms.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning, volume 119 of\nProceedings of Machine Learning Research, pages\n4411\u20134421. PMLR.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and Dan\nRoth. 2020. Cross-lingual ability of multilingual bert:\nAn empirical study. In International Conference on\nLearning Representations.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver\nStanley, Rich\u00e1rd Nagyfi, Shahul ES, Sameer Suri,\nDavid Glushkov, Arnav Dantuluri, Andrew Maguire,\nChristoph Schuhmann, Huu Nguyen, and Alexander\nMattick. 2023. Openassistant conversations \u2013 democ-\nratizing large language model alignment.\nViet Lai, Chien Nguyen, Nghia Ngo, Thuat Nguyen,\nFranck Dernoncourt, Ryan Rossi, and Thien Nguyen.\n2023. Okapi: Instruction-tuned large language mod-\nels in multiple languages with reinforcement learning\nfrom human feedback. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages\n318\u2013327, Singapore. Association for Computational\nLinguistics.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke\nZettlemoyer, Omer Levy, Jason Weston, and Mike\nLewis. 2023. Self-alignment with instruction back-\ntranslation.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nDan Malkin,\nTomasz Limisiewicz,\nand Gabriel\nStanovsky. 2022. A balanced data approach for eval-\nuating cross-lingual transfer: Mapping the linguistic\nblood bank. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 4903\u20134915, Seattle, United States.\nAssociation for Computational Linguistics.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and\nHannaneh Hajishirzi. 2022. Cross-task generaliza-\ntion via natural language crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3470\u20133487, Dublin, Ireland.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev,\nAlham Fikri Aji, Khalid Almubarak, Samuel Al-\nbanie, Zaid Alyafeai, Albert Webson, Edward Raff,\nand Colin Raffel. 2023.\nCrosslingual generaliza-\ntion through multitask finetuning. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15991\u201316111, Toronto, Canada. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In Advances in Neural Information\nProcessing Systems, volume 35, pages 27730\u201327744.\nCurran Associates, Inc.\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654\u20137673, Online. Association for Computa-\ntional Linguistics.\nJason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruk-\nsachatkun, Haokun Liu, Clara Vania, Katharina Kann,\nand Samuel R. Bowman. 2020. English intermediate-\ntask training improves zero-shot cross-lingual trans-\nfer too. In Proceedings of the 1st Conference of the\nAsia-Pacific Chapter of the Association for Compu-\ntational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pages\n557\u2013575, Suzhou, China. Association for Computa-\ntional Linguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual BERT? In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4996\u20135001, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nUri Shaham, Maha Elbayad, Vedanuj Goswami, Omer\nLevy, and Shruti Bhosale. 2023. Causes and cures for\ninterference in multilingual translation. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15849\u201315863, Toronto, Canada. Association\nfor Computational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois,\nXuechen Li,\nCarlos Guestrin,\nPercy\nLiang, and Tatsunori B. Hashimoto. 2023.\nStan-\nford\nalpaca:\nAn\ninstruction-following\nllama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A. Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2023. Self-instruct: Aligning language\nmodels with self-generated instructions. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 13484\u201313508, Toronto, Canada. Association\nfor Computational Linguistics.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022. Finetuned language mod-\nels are zero-shot learners. In International Confer-\nence on Learning Representations.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, becas:\nThe surprising cross-lingual effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 833\u2013844, Hong\nKong, China. Association for Computational Linguis-\ntics.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\nJiacheng Ye, Xijia Tao, and Lingpeng Kong. 2023. Lan-\nguage versatilists vs. specialists: An empirical revis-\niting on multilingual transfer ability.\nZhihan Zhang, Dong-Ho Lee, Yuwei Fang, Wenhao Yu,\nMengzhao Jia, Meng Jiang, and Francesco Barbieri.\n2023.\nPlug: Leveraging pivot language in cross-\nlingual instruction tuning.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. Lima: Less\nis more for alignment.\nA\nLanguages\nThe languages we use, their language families,\nscripts ,and language codes are shown in Table 2.\nB\nSide-By-Side Evaluation\nFigure 9 shows the prompt given the the LLM judge\nfor the side-by-side evaluation.\nLanguage\nCode\nFamily\nScript\nArabic\nar\nAfro-Asiatic\nArabic\nChinese\nzh\nSino-Tibetan\nChinese\nCzech\ncs\nIndo-European\nLatin\nEnglish\nen\nIndo-European\nLatin\nEstonian\net\nUralic\nLatin\nFinnish\nfi\nUralic\nLatin\nHebrew\nhe\nAfro-Asiatic\nHebrew\nHindi\nhi\nIndo-European\nDevanagari\nItalian\nit\nIndo-European\nLatin\nRussian\nru\nIndo-European\nCyrillic\nSpanish\nes\nIndo-European\nLatin\nSwahili\nsw\nNiger-Congo\nLatin\nTable 2: Languages used in our main experiments.\nBelow is an instruction and two answers. Choose your\npreferred answer, which can be subjective.\nThe instruction:\n{instruction}\nAnswer1:\n{response 1}\nAnswer2:\n{response 2}\nWhich one is better, Answer1 or Answer2?\nOnly write a single digit as your answer, \u20191\u2019 for Answer1\nor \u20192\u2019 for Answer2. Do not add any explanation.\nFigure 9: Side-by-side evaluation prompt.\nC\nTraining and Inference Details\nWe now describe the hyperparameters we use in\nour experiments. We tune every model for 2,000\nsteps, using a fixed learning rate of 1e-5, a batch\nsize of 128, and a dropout rate of 0.05. We limit in-\nputs to 1,024 tokens and targets to 512 tokens. We\nsample a development set of 250 examples from ev-\nery training set and select the checkpoint based on\nthe development RougeL (Lin, 2004) score. Dur-\ning inference, we generate responses of up to 512\ntokens using nucleus sampling (Holtzman et al.,\n2020) with p = 0.9 and temperature of 0.7. For the\njudge, we use greedy decoding to generate the ID\nof the better response (1 or 2).\nD\nJudge-Human Agreement\nTo measure PaLM 2-L agreement with human judg-\nments across language, we conduct a human anno-\ntation process on four languages, English, Span-\nish, Russian, and Hebrew. For every language we\nsample 50 instructions and let two native speak-\nLanguage\nHuman-Model\nHuman-Human\nEnglish\n79.5\n85.0\nSpanish\n77.0\n80.0\nRussian\n76.5\n79.0\nHebrew\n75.0\n82.0\nTable 3: Judges agreement scores per language.\ners select the better response out of two options,\nsimilarly to the task we assign the LLM judge (Fig-\nure 9). We always present the response by the\nmodel that was monolingually tuned using the eval-\nuation language, alongside a response by model\nselected at random from the of the monolingually\ntuned ones described in \u00a73.1. The agreement score\non a single instruction is 1 if the LLM judge and\nhuman agree, 0.5 if exactly one of them selects a\ntie, and 0 if each selects a different response (Zhou\net al., 2023). Table 3 shows the results. Overall, the\nLLM judge agreement with humans is strong for\nall four languages, yet there is some room of 2.5-7\npoints from inter human agreement in all languages.\nAs expected, the models\u2019 highest agreement with\nhumans is in English with 79.5%,. In the rest of\nthe languages the agreement is a few points lower.\nE\nResponse Language\nWhen a user prompts a model in a specific lan-\nguage, they usually expect to receive a response in\nthat same language. However, pre-trained LLMs\noften respond in a different language than the lan-\nguage of their prompt (Touvron et al., 2023a; Chen\net al., 2023). This poses a challenge also for evalu-\nation of open-ended queries, since those are com-\nmonly evaluated with an LLM-as-a-judge (Zheng\net al., 2023) protocol, and the judges often ignore\nwhether the response language match the prompt\nlanguage, even when instructed not to (Chen et al.,\n2023).\nUsually, this is handled by forcing the\nlowest score to such response (Chen et al., 2023),\nwhich does not account for all cases.8 To verify our\ntrained models respond in the same language as\ntheir prompt, we manually annotate the language\nof responses to evaluation instructions in all lan-\nguages. For every language, we randomly sample\n20 responses from the pool of models tuned mono-\nlingually in other languages, to end up with a total\nof 240 generations from various models. We find\nthat 239 responses are in the same language as the\n8For example, a response in English to a prompt in French\ncan still be very helpful, or when the prompt is a request for\ntranslation or code.\n1\n2\n3\n4\n5\n6\nfi\nfi,en\nfi,en,ru\nfi,en,ru,it\nfi,en,ru,it,sw\nall six\nsw\nsw,it\nsw,it,ar\nsw,it,ar,en\nsw,it,ar,en,fi\nall six\nit\nit,fi\nit,fi,en\nit,fi,en,ar\nit,fi,en,ar,ru\nall six\nTable 4: Subsets of languages used to tune models for\nthe experiment described in Section 3.4. Each cell repre-\nsents a version of the training set, for which all examples\nare uniformly split between the languages in that cell.\nprompt, as desired. This is a major difference in the\nbehavior of our PaLM 2-based instruction-tuned\nmodels and the commonly used (Chen et al., 2023)\nLLaMA-based ones (Touvron et al., 2023a,b). We\nhypothesize this stems from the multilingual em-\nphasis in the pre-training of PaLM 2, compared to\nthe more English-centric LLaMA.\nF\nComparison to The Base Model\nThe scores of models of model instruction tuned\nmonolingually compared to the pre-trained model\nthat was not instruction tuned, as opposed to our\nmain evaluation setup, are shown in Figure 10. As\nevident, instruction tuning the model on each of the\nlanguages separately unlocks instruction-following\nabilities across all languages.\nG\nLanguages Permutations\nWe use 3 different permutations of 6 languages to\ndetermine the order in which we add languages\nto the tuning set in the experiment described Sec-\ntion 3.4. The permutations are displayed in Table 4.\nar\ncs\nen\nes\net\nfi\nhe\nhi\nit\nru\nsw\nzh\nTrain Language\n98.0\n95.4\n96.6\n96.9\n95.7\n95.4\n96.4\n94.6\n96.8\n95.8\n96.0\n94.0\nar\n95.1\n96.6\n96.0\n96.1\n96.6\n95.1\n95.6\n92.6\n95.4\n96.0\n94.9\n92.5\ncs\n81.6\n91.7\n96.8\n93.3\n89.3\n90.8\n87.7\n78.9\n94.2\n89.8\n85.5\n86.4\nen\n94.3\n95.5\n97.7\n97.3\n95.5\n95.2\n94.9\n91.8\n97.2\n95.1\n94.8\n93.9\nes\n95.6\n97.2\n97.2\n96.8\n98.1\n96.5\n96.0\n95.4\n97.5\n96.3\n97.0\n95.1\net\n97.6\n98.5\n98.0\n97.7\n97.3\n98.8\n96.5\n96.1\n97.4\n98.2\n97.5\n95.6\nfi\n95.5\n96.4\n96.4\n96.7\n95.5\n96.8\n97.7\n95.6\n96.4\n96.1\n96.0\n93.8\nhe\n95.5\n96.0\n96.1\n95.7\n96.8\n96.8\n95.6\n96.7\n95.7\n94.9\n96.1\n95.1\nhi\n94.1\n96.7\n96.4\n96.3\n95.0\n95.7\n95.7\n93.3\n97.1\n96.0\n96.2\n94.1\nit\n95.6\n97.6\n96.8\n97.2\n97.1\n96.6\n96.5\n95.6\n97.2\n97.2\n95.8\n93.0\nru\n96.3\n96.1\n96.8\n96.5\n96.3\n96.3\n94.8\n94.7\n97.0\n95.8\n97.3\n93.4\nsw\n92.1\n95.9\n95.2\n94.4\n95.7\n95.1\n92.8\n92.9\n95.5\n94.9\n94.7\n97.0\nzh\n94.3\n96.1\n96.7\n96.3\n95.7\n95.8\n95.0\n93.2\n96.5\n95.5\n95.2\n93.7\navg\nEvaluation Language\nFigure 10: Per language instruction-following comparisons of models instruction-tuned on monolingual data to the\npre-trained model that was not instruction tuned. Each row represents a model tuned using a different language, and\neach column is an individual heatmap of the scores of all models on the same evaluation language. Scores are the\ndiscounted-ties weighted average of the side-by-side scores against the pre-trained model.\n"
  },
  {
    "title": "Efficient Hybrid Zoom using Camera Fusion on Mobile Phones",
    "link": "https://arxiv.org/pdf/2401.01461.pdf",
    "upvote": "7",
    "text": "Efficient Hybrid Zoom using Camera Fusion on Mobile Phones\nXIAOTONG WU, WEI-SHENG LAI, YICHANG SHIH, CHARLES HERRMANN, MICHAEL KRAININ,\nDEQING SUN, and CHIA-KAI LIANG, Google, USA\nDSLR cameras can achieve multiple zoom levels via shifting lens distances or\nswapping lens types. However, these techniques are not possible on smart-\nphone devices due to space constraints. Most smartphone manufacturers\nadopt a hybrid zoom system: commonly a Wide (W) camera at a low zoom\nlevel and a Telephoto (T) camera at a high zoom level. To simulate zoom\nlevels between W and T, these systems crop and digitally upsample images\nfrom W, leading to significant detail loss. In this paper, we propose an ef-\nficient system for hybrid zoom super-resolution on mobile devices, which\ncaptures a synchronous pair of W and T shots and leverages machine learn-\ning models to align and transfer details from T to W. We further develop\nan adaptive blending method that accounts for depth-of-field mismatches,\nscene occlusion, flow uncertainty, and alignment errors. To minimize the\ndomain gap, we design a dual-phone camera rig to capture real-world in-\nputs and ground-truths for supervised training. Our method generates a\n12-megapixel image in 500ms on a mobile platform and compares favorably\nagainst state-of-the-art methods under extensive evaluation on real-world\nscenarios.\nCCS Concepts: \u2022 Computing methodologies; \u2022 Artificial intelligence;\n\u2022 Computer vision; \u2022 Image and video acquisition; \u2022 Computational\nphotography;\nAdditional Key Words and Phrases: hybrid zoom, dual camera fusion, deep\nneural networks\nACM Reference Format:\nXiaotong Wu, Wei-Sheng Lai, YiChang Shih, Charles Herrmann, Michael\nKrainin, Deqing Sun, and Chia-Kai Liang. 2023. Efficient Hybrid Zoom using\nCamera Fusion on Mobile Phones. ACM Trans. Graph. 42, 6 (December 2023),\n12 pages. https://doi.org/10.1145/3618362\n1\nINTRODUCTION\nBeing able to adjust the field-of-view (FOV) (i.e., zooming in and\nout) is one of the most basic functionalities in photography, yet\ndespite their ubiquity, smartphones still struggle with zoom. Zoom\nlenses used by DSLR cameras require a large assembly space that is\ntypically impractical for smartphones. Recent smartphones employ\nhybrid optical zoom mechanisms consisting of cameras with differ-\nent focal lengths, denoted as W and T, with short and long focal\nlengths, respectively. When a user zooms, the system upsamples\nand crops W until the FOV is covered by T. However, almost all\nforms of upsampling (bilinear, bicubic, etc.) lead to varying degrees\nof objectionable quality loss. The growing demand for higher levels\nAuthors\u2019 address: Xiaotong Wu, abbywu@google.com; Wei-Sheng Lai, wslai@google.\ncom; YiChang Shih, yichang@google.com; Charles Herrmann, irwinherrmann@google.\ncom; Michael Krainin, mkrainin@google.com; Deqing Sun, deqingsun@google.com;\nChia-Kai Liang, ckliang@google.com, Google, USA.\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\n\u00a9 2023 Copyright held by the owner/author(s).\n0730-0301/2023/12-ART $15.00\nhttps://doi.org/10.1145/3618362\nHZSR\nWide (1x zoom)\nTelephoto (5x zoom)\nFusion (ours)\nOurs\nWide\nTelephoto\n?\nDual camera \ncapturing system\n?\n?\nUser zoom to 3x\n3x\nOurs\nWide\nTelephoto\nFig. 1. Detail improvements in hybrid zoom. The red dotted lines mark\nthe FOV of 3\u00d7 zoom on 1\u00d7 wide (W) camera, while the green dotted\nlines mark the FOV of 5\u00d7 telephoto (T) camera. Image quality at an inter-\nmediate zoom range suffers from blurry details from single-image super-\nresolution [Romano et al. 2016]. Our mobile hybrid zoom super-resolution\n(HZSR) system captures a synchronous pair of W and T and fuses details\nthrough efficient ML models and adaptive blending. Our fusion results sig-\nnificantly improve texture clarity when compared to the upsampled W.\nof zoom in smartphones has led to higher focal length ratios be-\ntween T and W, typically 3-5\u00d7, making detail loss an increasingly\nimportant problem.\nSingle-image super-resolution (SISR) adds details to images but\nis inappropriate for photography due to its tendency to hallucinate\nfake content. Instead, reference-based super-resolution (RefSR) aims\nto transfer real details from a reference image. A variety of sources\nfor the reference images have been explored, e.g., images captured\nat a different time or camera position, or similar scenes from the\nweb. The hardware setup in recent smartphones provides a stronger\nsignal in the form of W and T captures. Recent works have thus\nfocused on using the higher zoom T as a reference to add real details\nback to the lower zoom W.\nCommercial solutions exist [HonorMagic 2023; Triggs 2023] but\nneither technical details nor datasets are publicly available. Aca-\ndemic solutions [Trinidad et al. 2019; Wang et al. 2021; Zhang et al.\n2022a] provide insights into the problem but are not practical for\nreal-world applications. Specifically, these methods tend to be ineffi-\ncient on mobile phones, are vulnerable to imperfections in reference\nimages, and may introduce domain shifts between training and in-\nference. In this work, we investigate these three issues and propose\na hybrid zoom super-resolution (HZSR) system to address them.\nEfficient processing on mobile devices. Existing methods require\nlarge memory footprints (e.g., out-of-memory for 12MP inputs on a\ndesktop with an A100 GPU) and long processing times unsuitable\nfor mobile phones. We develop efficient Machine Learning (ML)\nmodels to align T to W using optical flow and fuse the details at\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\narXiv:2401.01461v1  [cs.CV]  2 Jan 2024\n2\n\u2022\nWu, et al\nthe pixel level using an encoder-decoder network. Our models are\noptimized to process 12MP inputs efficiently on mobile system-on-a-\nchip (SoC) frameworks, taking only 500ms extra latency and 300MB\nmemory footprint.\nAdapting to imperfect references. Existing methods [Wang et al.\n2021; Zhang et al. 2022a] treat the entire T as a high-resolution\nreference, resulting in worse fusion quality in regions where T\nis imperfect. Specifically, two problems can introduce unwanted\nartifacts: mismatches in depth-of-field (DoF) and errors in alignment\nbetween W and T. Due to shallower DoF, out-of-focus pixels on\nT can appear blurrier than on W, as shown in Fig. 2. We propose\nan efficient defocus detection algorithm based on the correlation\nbetween scene depth and optical flow to exclude defocus areas from\nfusion. Based on the defocus map, alignment errors, flow uncertainty,\nand scene occlusion, we develop an adaptive blending mechanism\nto generate high-quality and artifact-free super-resolution results.\nMinimizing domain gap with real-world inputs. In RefSR, it is\ndifficult to collect perfectly aligned W/T ground-truth pairs for\ntraining. As a result, two plausible but inadequate solutions have\nbeen explored: 1) Using the reference image T as a training tar-\nget [Wang et al. 2021; Zhang et al. 2022a], which often transfers\nimperfections from the reference or causes the network to learn\nthe identity mapping. 2) Learning a degradation model to synthe-\nsize a low-resolution input from the target image [Trinidad et al.\n2019; Zhang et al. 2022a], which introduces a domain gap between\ntraining and inference and degrades the super-resolution quality\non real-world images. To avoid learning the identity mapping and\nminimize the domain gap, we synchronously capture an extra T\nshot from a second smartphone of the same model mounted on a\ncamera rig and use this capture as the reference during training (see\nFig. 6). In this design, the fusion model sees real W as input at both\nthe training and inference stages to avoid domain gaps. Further,\nthe reference and target are captured from T cameras of different\ndevices to avoid learning the identity mapping.\nUnlike existing dual-zoom RefSR datasets that either show strong\ntemporal motion between W and T [Wang et al. 2021] or are limited\nto static scenes [Wei et al. 2020], we collect a large-scale dataset with\nhigh-quality W/T synchronization in dynamic scenes. Our dataset\nincludes much more diverse captures such as portraits, architectures,\nlandscapes, and challenging scenes like dynamic object motion and\nnight scenes. We demonstrate that our method performs favorably\nagainst state-of-the-art approaches on existing dual-zoom RefSR\nand our datasets.\nOur contributions are summarized as the following:\n- An ML-based HZSR system that runs efficiently on mobile devices\nand is robust to imperfections in real-world images (Sec. 3).\n- A training strategy that uses a dual-phone camera rig to mini-\nmize domain gaps and avoids learning a trivial mapping in RefSR\n(Sec. 4).\n- A dataset of 150 well-synchronized W and T shots at high-resolution\n(12MP), coined as Hzsr dataset, will be released at our project\nwebsite at1 for future research (Sec. 5).\n1https://www.wslai.net/publications/fusion_zoom\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFig. 2. When depth-of-field (DoF) is shallower on telephoto (T) than\nwide (W), transferring details from T to W in defocus regions results in\nsignificant artifacts. We design our system to exclude defocus regions during\nfusion and generate results that are robust to lens DoF. By contrast, the\nresult from DCSR [Wang et al. 2021] shows blurrier details than the input\nW on the parrot and building.\n2\nRELATED WORK\nLearning-based SISR. Several methods [Christian Ledig 2017; Dong\net al. 2014; Kim et al. 2016; Lai et al. 2017; Wang et al. 2018; Xu et al.\n2023; Zhang et al. 2019a, 2022b, 2018] have shown promising results\nover the past decade. However, due to the heavily ill-posed nature,\nthey produce blurry details at large upsampling factors, e.g., 2-5\u00d7\nrequired by hybrid zoom on smartphones, or work only for specific\ndomains such as faces [Chan et al. 2021; Gu et al. 2020; He et al.\n2022; Menon et al. 2020].\nRefSR using Internet images. RefSR outputs a high-resolution im-\nage from a low-resolution input by taking one or multiple [Pesavento\net al. 2021] high-resolution references as additional inputs. Conven-\ntional RefSR methods assume that the references are taken from the\ninternet [Sun and Hays 2012] or captured at a different moment, po-\nsition, or camera model at the same event [Wang et al. 2016; Zhang\net al. 2019b] and focus on improving dense alignment between the\nsource and reference [Huang et al. 2022; Jiang et al. 2021; Xia et al.\n2022; Zheng et al. 2018] or robustness to irrelevant references [Lu\net al. 2021; Shim et al. 2020; Xie et al. 2020; Yang et al. 2020; Zhang\net al. 2019b]. In contrast, we mitigate the alignment challenges by\ncapturing synchronous shots of W and T to avoid object motion.\nRefSR using auxiliary cameras. Recent RefSR works [Trinidad\net al. 2019; Wang et al. 2021; Zhang et al. 2022a] capture a refer-\nence image of the same scene using an auxiliary camera. Since\npixel-aligned input and ground-truth image pairs are not available,\nPixelFusionNet [Trinidad et al. 2019] learns a degradation model to\nsynthesize a low-resolution input from the high-resolution refer-\nence, and use pixel-wise losses such as \u21131 and VGG loss for training.\nSuch a model does not generalize well to real-world input images\ndue to the domain gap between the images observed at training and\ninference times. On the other hand, SelfDZSR [Zhang et al. 2022a],\nDCSR [Wang et al. 2021] and RefVSR [Lee et al. 2022] treat the\nreference image as the target for training or fine-tuning. We observe\nthat such a training setup is prone to degenerate local minimums:\nthe model will often learn the identity mapping and simply copy\nimage content from T to the output. This results in severe misalign-\nment, color shifting, and DoF mismatches unacceptable for practical\nphotography. In this work, we capture an extra T shot to mitigate\nthese issues in training.\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\nEfficient Hybrid Zoom using Camera Fusion on Mobile Phones\n\u2022\n3\nWide (W)\nCoarse \nAlignment\n(Sec 3.1)\nPWC-Net\n(Sec 3.1)\nColor \nMatching\n(Sec 3.1)\nAdaptive\nBlending\n(Sec 3.3)\nBilinear \nWarping\nSource\nTele (T)\nReference\nWarped reference\nOptical flows Occlusion mask\nModel output\nFusion UNet\n(Sec 3.2)\nBlending mask\n(see Figure 4)\nWide\nOurs\nFinal output\nFusion result\nFig. 3. System overview. Given concurrently captured W and T images, we crop W to match the FOV of T, coarsely align them via feature matching, and\nadjust the color of T to match W. The cropped W and adjusted T are referred to as source and reference, respectively. Then, we estimate dense optical flow to\nalign the reference to source (Sec. 3.1) and generate an occlusion mask. Our Fusion UNet takes as input the source, warped reference, and occlusion mask for\ndetail fusion (Sec. 3.2). Lastly, we merge the fusion result back to the full W image via an adaptive blending (Sec. 3.3, Fig. 4) as the final output.\nEfficient mobile RefSR. Existing methods typically have large mem-\nory footprints due to the use of attention/transformer [Wang et al.\n2021; Yang et al. 2020] or deep architectures [Zhang et al. 2022a].\nThey run into out-of-memory (OOM) issues for 12MP input resolu-\ntion even on an NVIDIA A100 desktop GPU with 40GB RAM and\ncannot run on mobile devices. By contrast, it takes our system only\n500ms and 300MB to process 12MP inputs on a mobile GPU.\nOur system design is inspired by the reference-based face de-\nblurring method [Lai et al. 2022], but the problems we address are\nfundamentally more challenging. First, we apply super-resolution\nto generic images instead of focusing on faces. Our system should\nbe more robust to diverse scenes and handle various of imperfec-\ntions and mismatches between two cameras. Second, unlike face\ndeblurring models which can learn from synthetic data, image super-\nresolution models are more sensitive to the domain gap in training\ndata, and it is more challenging to collect real training data for\nreference-based SR. Therefore, our adaptive blending method and\ndual-phone rig setup are the key components that differentiate our\nwork with [Lai et al. 2022].\n3\nHYBRID ZOOM SUPER-RESOLUTION\nOur goal is to design an efficient system that can run at interactive\nrates on mobile devices. These constraints exclude the use of large\nmodels that are slow and memory-intensive. The overview of our\nprocessing pipeline is shown in Fig. 3. When a user zooms to a\nmid-range zoom (e.g., 3-5\u00d7), our system will capture a synchronized\nimage pair when the shutter button is pressed. We first align W\nand T with a global coarse alignment using keypoint matching, fol-\nlowed by a local dense alignment using optical flow (Sec. 3.1). Then,\nwe adopt a UNet [Ronneberger et al. 2015] to fuse the luminance\nchannel of a source image cropped from W and a reference warped\nfrom T (Sec. 3.2). Lastly, our adaptive blending algorithm (Sec. 3.3\nand Fig. 4) takes into account the defocus map, occlusion map, flow\nuncertainty map, and alignment rejection map to merge the fusion\noutput back to the full-size W image. Overall, our system consists\nof lightweight modules that make our overall system efficient and\neffective.\n3.1\nImage Alignment\nCoarse alignment. We first crop W to match the FOV of T, and\nresample W to match the spatial resolution of T (4k\u00d73k) using a\nbicubic resampler. We then estimate a global 2D translation vector\nvia FAST feature keypoint matching [Rosten and Drummond 2006]\nand adjust the cropped W, denoted as \ud835\udc3csrc. We also match T\u2019s color to\nW by normalizing the mean and variances of RGB colors [Reinhard\net al. 2001] to compensate for the photometric differences between\nW and T sensors. The color-adjusted T is denoted as the reference\nimage \ud835\udc3cref.\nDense alignment. We use PWC-Net [Sun et al. 2018] to estimate\ndense optical flow between \ud835\udc3csrc and \ud835\udc3cref. Note that the average offset\nbetween W and T is 150 pixels at 12MP resolution, which is much\nlarger than the motion magnitude in most of the optical flow training\ndata [Sun et al. 2021]. The flows estimated from 12MP images are too\nnoisy. Instead, we downsample \ud835\udc3csrc and \ud835\udc3cref to 384 \u00d7 512 to predict\noptical flow and upsample flow to the original image resolution to\nwarp \ud835\udc3cref via bilinear resampling, denoting as \u02dc\ud835\udc3cref. The flow estimated\nat this scale is more accurate and robust for alignment.\nTo meet the limited computing budget on mobile devices, we\nremove the DenseNet structure from the original PWC-Net, which\nreduces the model size by 50%, latency by 56%, and peak memory by\n63%. While this results in an 8% higher flow end-point-error (EPE)\non the Sintel dataset, the flow\u2019s visual quality remains similar. We\nalso generate an occlusion map, Mocc, through a forward-backward\nconsistency check [Alvarez et al. 2007].\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n4\n\u2022\nWu, et al\nSource\nWarped \nreference\nAlignment rejection map\nDefocus map\nFlow uncertainty map\nOcclusion map\nBlending mask\nk-means \nclustering\nForward \nbackward \nconsistency\nMean and \nvariance \nnormalization\nPWC-Net\n1 - \u2140\nForward \nflow\nBackward \nflow\nFig. 4. Adaptive blending. We use alpha masks to make the fusion robust\nto alignment errors and DoF mismatch (Sec. 3.3).\n3.2\nImage Fusion\nTo preserve the color of W, we apply fusion in the luminance\nspace only. We construct a 5-level UNet, which takes as inputs\nthe grayscale \ud835\udc3csrc (denoted as \ud835\udc4csrc), grayscale \u02dc\ud835\udc3cref (denoted as \u02dc\ud835\udc4cref),\nand the occlusion mask Mocc to generate a grayscale output image\n\ud835\udc4cfusion. The grayscale \ud835\udc4cfusion is merged with the UV channels of \ud835\udc3csrc\nand converted back to the RGB space as the fusion output image\n\ud835\udc3cfusion. The detailed architecture of the Fusion UNet is provided in\nthe supplementary material. Since memory footprint is often the\nbottleneck for on-device processing, a useful design principle for an\nefficient align-and-merge network is to reduce the feature channels\nin high-resolution layers. Therefore, we design our system with\npixel-level image warping instead of feature warping [Reda et al.\n2022; Trinidad et al. 2019] and limit the number of encoder channels\nin Fusion UNet.\n3.3\nAdaptive Blending\nWhile ML models are powerful tools for aligning and fusing images,\nmismatches between W and T can still result in visible artifacts\nat output. Such mismatches include the DoF differences, occluded\npixels, and warping artifacts at the alignment stage. Therefore, we\ndevelop a strategy to adaptively blend \ud835\udc4csrc and \ud835\udc4cfusion by utilizing\nan alpha mask derived from the defocus map, occlusion map, flow\nuncertainty map, and alignment rejection map, as shown in Fig. 4.\nOur final output is free from objectionable artifacts and robust to\nimperfections of pixel-level consistency between W and T.\nNarrow DoF on T. We observe that T often has a narrower DoF\nthan W on mobile phones. This is because camera DoF is in pro-\nportional to \ud835\udc41/\ud835\udc53 2, where \ud835\udc41 and \ud835\udc53 denote the aperture number and\nfocal length, respectively. The typical focal length ratio between T\nand W is > 3\u00d7 and the aperture number ratio is < 2.5\u00d7. The supple-\nmental material lists the camera specifications from recent flagship\nphones to justify this observation. Fig. 2 shows that 1) the defocused\narea of T is significantly blurrier than that of W despite its higher\nsampling rate, and 2) including defocused details from T results in\nblurry output worse than W. Therefore, we need a defocus map\nWide (W)\nTelephoto (T)\nFlow from W \u2192 T\nDefocus map\nFocused ROI from \nCamera AF module\nFocused center\nK-means clustering\nT\nDefocus map\nW\nFig. 5. Efficient defocus map detection using optical flow at the align-\nment stage, described in Sec. 3.3. Black/white pixels in the defocus map\nrepresent the focused/defocused area.\nto exclude the defocused pixels from fusion. Single-image defocus\nmap estimation is an ill-posed problem that requires expensive ML\nmodels impractical on mobile devices [Cun and Pun 2020; Lee et al.\n2019; Tang et al. 2019; Xin et al. 2021; Zhao et al. 2019]. Instead, we\npresent an efficient algorithm that reuses the optical flow computed\nat the alignment step.\nDefocus map. To estimate a defocus map, we need to know 1)\nwhere the camera focus is, denoted the focused center, and 2) the\nrelative depth to the focused center for each pixel. Because W and T\nare approximately fronto-parallel, and the optical flow magnitude is\nproportional to the camera disparity and therefore the scene depth,\nwe propose an algorithm to estimate a defocus map, as illustrated\nin Fig. 5. First, we acquire the focused region of interest (ROI) from\nthe camera auto-focus module, which indicates a rectangular region\non T where most pixels are in focus. Second, based on dual camera\nstereo, we consider the optical flow as a proxy to depth and assume\nthat the pixels at the same focal plane have similar flow vectors for\nstatic scenes [Szeliski 2022]. To find the focused center, we apply the\nk-mean clustering algorithm on the flow vectors within the focused\nROI. We then choose the focused center x\ud835\udc53 to be the center of the\nlargest cluster. To estimate the relative depth to x\ud835\udc53 , we calculate the\n\u21132 distance on the flow vectors between each pixel and the focused\ncenter, and obtain a defocus map via:\nMdefocus(x) =sigmoid(\n||\ud835\udc39fwd(x)\u2212\ud835\udc39fwd(x\ud835\udc53 )||2\n2\u2212\ud835\udefe\n\ud835\udf0e\ud835\udc53\n),\n(1)\nwhere \ud835\udc39fwd is the optical flow between \ud835\udc3csrc and \ud835\udc3cref, \ud835\udefe controls the\ndistance threshold to tolerate in-focus regions, and \ud835\udf0e\ud835\udc53 controls the\nsmoothness of the defocus map. Our algorithm is efficient and takes\nonly 5ms on a mobile device.\nOcclusion map. The baseline between W and T (i.e., the distance\nbetween optical centers) makes occluded pixels in W invisible to\nT and results in artifacts when warping T using optical flow. To\nexclude these pixels from fusion, we estimate an occlusion map\nusing the forward-backward flow consistency [Alvarez et al. 2007]:\nMocc(x) = min(\ud835\udc60||W(W(x; \ud835\udc39fwd); \ud835\udc39bwd) \u2212 x||2, 1),\n(2)\nwhere W is the bilinear warping operator and x is the 2D image\ncoordinate on the source image. The scaling factor \ud835\udc60 controls the\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\nEfficient Hybrid Zoom using Camera Fusion on Mobile Phones\n\u2022\n5\nstrength of the occlusion map. Note that our occlusion map in-\ncludes both occluded and dis-occluded pixels where the flows are\ninconsistent, typically near motion or object boundaries.\nFlow uncertainty map. Since dense correspondence is heavily ill-\nposed, we augment PWC-Net to output a flow uncertainty map [Gast\nand Roth 2018]. The uncertainty-aware PWC-Net predicts a mul-\ntivariate Laplacian distribution over flow vectors for each pixel,\nrather than a simple point estimate. Specifically, it predicts two addi-\ntional channels that determine to the log-variance of the Laplacian\ndistribution in \ud835\udc65- and \ud835\udc66-directions, denoted as Var\ud835\udc65 and Var\ud835\udc66, re-\nspectively. We convert the log-variance into units of pixels through\nthe following equation:\nS(x) =\n\u221a\ufe03\nexp(log(Var\ud835\udc65 (x))) + exp(log(Var\ud835\udc66(x))),\n(3)\nMflow(x) = min(S(x),\ud835\udc60max)/\ud835\udc60max.\n(4)\nAs shown in Fig. 4, the flow uncertainty map often highlights the\nobject boundary or texture-less regions. We use \ud835\udc60max = 8 at the flow\nprediction resolution.\nAlignment rejection map. We estimate an alignment rejection\nmap to exclude the erroneous alignment based on the similarity\nbetween the source and aligned reference patches [Hasinoff et al.\n2016; Wronski et al. 2019], First, to match the optical resolution\nbetween W and T, we use bilinear resizing to downsample and\nupsample the warped reference frame \u02dc\ud835\udc4cref based on the focal length\nratio between W and T, denoted by \u02dc\ud835\udc4cref\u2193. Then, for each pixel\nwith its local patch \ud835\udc43src (on \ud835\udc4csrc) and \u02dc\ud835\udc43ref (on \u02dc\ud835\udc4cref\u2193), we subtract\npatch means and calculate the normalized patch difference \ud835\udc43\ud835\udeff =\n(\ud835\udc43src \u2212 \ud835\udf07src) \u2212 ( \u02dc\ud835\udc43ref \u2212 \ud835\udf07ref). The alignment rejection map on each\npatch is calculated by:\nMreject(x) = 1 \u2212 exp\n\u0010\n\u2212||\ud835\udc43\ud835\udeff (x)||2\n2/\n\u0010\n\ud835\udf0e2\nsrc(x) + \ud835\udf160\n\u0011\u0011\n,\n(5)\nwhere \ud835\udf0e2src is the variance of \ud835\udc43src, \ud835\udf160 is used to tolerate minor diff\nbetween source and reference. We set patch size to 16 and stride\nsize to 8 in all our experiments.\nFinal blending. We generate the blending mask as:\nMblend =max(1\u2212Mocc\u2212Mdefocus\u2212Mflow\u2212Mreject,0).\n(6)\nNote that Mdefocus, Mocc and Mflow are generated at the flow infer-\nence size, and Mreject is 8\u00d7 smaller than \ud835\udc3csrc. We upscale these masks\nusing a bilinear upsampling to the size of \ud835\udc3csrc for blending. For pixels\noutside the FOV of T, we retain the intensities of W and apply a\nGaussian smoothing on the boundary of Mblend to avoid abrupt\ntransitions between the fusion and non-fusion regions. The final\noutput image is generated via an alpha blending and \u201cuncropping\u201d\nback to the full W image:\n\ud835\udc3cfinal =uncrop(Mblend\u2299\ud835\udc3cfusion+(1\u2212Mblend)\u2299\ud835\udc3csrc,W),\n(7)\nwhere \u2299 is the Hadamard product.\n4\nLEARNING FROM DUAL CAMERA RIG CAPTURES\nTechniques that synthesize degraded inputs for training [Trinidad\net al. 2019; Wang et al. 2021; Zhang et al. 2022a] suffer from a do-\nmain gap between synthetic and real images. To reduce the gap, we\ntrain our alignment and fusion models on real-world images where\nWL\nTL\nSource\nSource\nReference\nTarget\nReference\nTraining:\nInference:\nWR\nTR\nMain phone\nAuxiliary phone\n(Not used at inference time)\nFig. 6. Dual-phone rig setup. We collect synchronous captures from two\nsmartphones on a rig and use W\ud835\udc3f, T\ud835\udc3f, and T\ud835\udc45 as source, target, and refer-\nence images. The training setup ensures the camera sensors are consistent\nbetween the test and training stages to eliminate the domain gap.\nthe source, reference, and ground-truth images are all captured by\nmobile phone cameras.\nDual camera rig. We design a dual-phone rig to mount two smart-\nphones side-by-side, as illustrated in Fig. 6. The rig is 3D-printed and\ndesigned to fix the main and auxiliary devices in a fronto-parallel\nposition and an identical vertical level. We use a camera app that syn-\nchronizes the capture time between the main and auxiliary phones\nthrough WiFi [Ansari et al. 2019]. In Fig. 6, we denote the cameras\non the left phone as W\ud835\udc3f and T\ud835\udc3f, and the cameras on the right phone\nas W\ud835\udc45 and T\ud835\udc45.\nIn the training time, we take W\ud835\udc3f and T\ud835\udc45 as the source and ref-\nerence pairs (i.e., inputs to the model) and T\ud835\udc3f as the target image\n(i.e., ground truth of the model output). We use PWC-Net to align\nW\ud835\udc3f and T\ud835\udc45 to T\ud835\udc3f, so that the source, reference, and target images\nare all aligned to the same camera viewpoint. As both the source\nand reference images are warped, we define an availability mask\nMvalid = 1 \u2212 \u02c6Mocc, where \u02c6Mocc denotes the union of the occlusion\nmasks from W\ud835\udc3f \u2192 T\ud835\udc3f flow and T\ud835\udc45 \u2192 T\ud835\udc3f flow, as the loss is inap-\nplicable to occluded pixels and should be excluded. Note that we\nselect T\ud835\udc3f instead of T\ud835\udc45 as the target image to minimize the warping\ndistance between the source and target. If we select T\ud835\udc45 as the tar-\nget, both W\ud835\udc3f and T\ud835\udc3f have to be warped from the left smartphone\nposition to align with T\ud835\udc45 on the right smartphone, which will re-\nduce the number of valid pixels for training. More details about our\ntraining setup are provided in the supplementary material. In total,\nwe collect 8,322 triplets to train our Fusion UNet.\nAt inference time, we only need W and T from one smartphone,\n(i.e., W\ud835\udc3f and T\ud835\udc3f), and T is warped to align with W for fusion.\nThe only difference between training and testing lies in the image\nalignment: we align all the images to W\ud835\udc3f at inference time but\nalign to T\ud835\udc3f at training time to minimize warping errors. Note that\nthe warped W\ud835\udc3f and warped T\ud835\udc45 in the training stage are not exact\nbut close enough to mimic the real source and reference images at\ntest time; they are all real images from the corresponding camera\nsensors.\nFusion UNet training. We denote the target image T\ud835\udc3f as \ud835\udc3ctarget,\nand train our Fusion UNet with the following losses.\nVGG loss The perceptual loss [Johnson et al. 2016] between \ud835\udc3cfusion\nand \ud835\udc3ctarget, which is commonly used in image restoration:\nLvgg = ||Mvalid \u2299 (\ud835\udc49\ud835\udc3a\ud835\udc3a(\ud835\udc3cfusion) \u2212 \ud835\udc49\ud835\udc3a\ud835\udc3a(\ud835\udc3ctarget))||1.\n(8)\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n6\n\u2022\nWu, et al\nNote that the availability mask Mvalid is resized to the resolution of\nthe corresponding VGG features.\nContextual loss: While we pre-align the source and target images,\nmisalignments still exist and degrade the model performance by\ngenerating blurry predictions or warping artifacts. Therefore, we\nadopt the contextual loss [Mechrez et al. 2018] to learn better on\nmis-aligned training data:\nLcx =\ud835\udc36\ud835\udc4b(Mvalid \u2299 \ud835\udc49\ud835\udc3a\ud835\udc3a(\ud835\udc3cfusion), Mvalid \u2299 \ud835\udc49\ud835\udc3a\ud835\udc3a(\ud835\udc3ctarget)),\n(9)\nwhere \ud835\udc36\ud835\udc4b is the contextual similarity [Mechrez et al. 2018] between\nthe VGG features of \ud835\udc3cfusion and \ud835\udc3ctarget.\nBrightness-consistency loss: To preserve the low-frequency bright-\nness tone on W and avoid tonal shift, we apply a brightness-consistency\nloss [Lai et al. 2022]:\nLbrightness = ||G(\ud835\udc4cfusion, \ud835\udf0e) \u2212 G(\ud835\udc4csrc, \ud835\udf0e)||1,\n(10)\nwhere G denotes Gaussian filter with a standard deviation \ud835\udf0e = 10\nin this work. Note that the brightness-consistency loss is applied\nto the whole image to encourage the model to learn the identity\nmapping over the occluded regions.\nThe final loss Lfinal is:\nLfinal = \ud835\udc64vggLvgg + \ud835\udc64cxLcx + \ud835\udc64brightnessLbrightness\n(11)\nwhere we set \ud835\udc64vgg = 1, \ud835\udc64cx = 0.05, and \ud835\udc64brightness = 1. Note that\nLvgg is effective for aligned pixels, while Lcx is more suitable for\nmisaligned content. Our model requires both losses to achieve better\nfusion quality, while the weight on VGG loss is much higher than\ncontextual loss.\nPWC-Net training. The PWC-Net is pre-trained on the AutoFlow\ndataset [Sun et al. 2021]. However, there is a domain gap between\nthe AutoFlow training data and the images from mobile phones.\nTherefore, we use the \ud835\udc3csrc and \ud835\udc3cref as input images and generate\n\u201cpseudo\u201d ground-truth flow with RAFT [Sun et al. 2022; Teed and\nDeng 2020] to further fine-tune the PWC-Net. The PWC-Net can\nthen be adapted and generalized well to align our source and refer-\nence images. Please see the supplementary materials for the effect\nof fine-tuning PWC-Net.\n5\nEXPERIMENTAL RESULTS\nWe evaluate our system on our Hzsr dataset, compare against re-\ncent RefSR methods, analyze system performance, conduct ablation\nstudies on key components, and discuss the limitations in this sec-\ntion. More high-resolution visual comparisons are available in the\nsupplementary materials and our project website.\n5.1\nHybrid Zoom SR (Hzsr) dataset\nWe use a smartphone with a W and T, which is commonly available\namong flagship smartphones. When the zoom level exceeds the focal\nlength ratio between T and W, i.e., 5\u00d7, the hybrid zoom system will\nswitch from W to T. Just before this zoom ratio, the W is upsampled\nto account for the difference in sensor resolution. We collect 25, 041\npairs of W and T image pairs with zoom ranges varying from 2\u00d7\nto 5\u00d7 for validating the proposed system. Among them, we select\n150 representative images that cover a wide variety of real-world\nscenes, including landscapes, street views, portraits, animals, and\nlow-light images, named as Hybrid Zoom SR (Hzsr) dataset. The\n150 images will be publicly released on our project website.\nWe show a few landscape and mid-zoom range shots in Fig. 7,\nwhich are the common use cases of hybrid zoom. Our method is able\nto transfer the details from T to recover the facades on buildings and\nmake letters more legible. Fig. 8 highlight the shots with occlusion\nand defocus blur on T. DCSR [Wang et al. 2021] often transfers\nunwanted blur to the output image, resulting in quality drop com-\npared to the input W image. In contrast, our method preserves the\nsharpness and details of W via the adaptive blending. Note that\nwe do not attempt to hallucinate details in defocus and occlusion\nareas. Instead, our system robustly falls back to W\u2019s pixels in these\nerror-prone areas.\nNote that except our method and DCSR [Wang et al. 2021], all\nother methods failed to process 12MP inputs due to out-of-memory\nerrors on A100 GPUs with 40GB memory.\n5.2\nComparisons with RefSR methods\nWe compare our method with SRNTT [Zhang et al. 2019b], TTSR\n[Yang et al. 2020], MASA [Lu et al. 2021], C2-Matching [Jiang et al.\n2021], AMSA [Xia et al. 2022], DCSR [Wang et al. 2021], and Self-\nDZSR [Zhang et al. 2022a]. We use the pre-trained models from the\nauthors\u2019 websites without retraining, as not all the implementations\nsupport retraining with 12MP inputs.\nCameraFusion dataset [Wang et al. 2021]. This dataset contains\n146 pairs (132 for training and 14 for testing) of W and T images\ncollected from mobile phones. Both W and T are downsampled 2\u00d7\nto 3MP resolution as inputs, while the original 12MP W images\nare used as the ground-truth during evaluation. Because of this,\nCameraFusion dataset can be considered as a synthetic dataset for\n2\u00d7 SR evaluation. In Fig. 9, our method outputs the most legible\nletters among the methods. Other RefSR works [Wang et al. 2021;\nZhang et al. 2022a] observe that optimizing with \u21131 or \u21132 loss results\nin the best reference-based metrics but worse visual quality. We also\nre-train our model with \u21131 loss on the training set of CameraFusion\ndataset and report the results in Table 1. Note that our results are not\nfavored by the evaluation setup of CameraFusion, as our method\naims to match the detail level of the reference. The reference may\ncontain more details than the ground-truth, e.g., in Fig. 9 the letters\nin the input T are more legible than the ground-truth W. As a result,\nour method is more visually pleasing but has a lower PSNR or SSIM\nin this dataset.\nDRealSR dataset [Wei et al. 2020]. This dataset includes 163 pairs\nof images captured from long and short focal lengths of a DSLR with\na 4\u00d7 zoom lens. The images are nearly disparity-free, but the content\ndoes not have dynamic subject motion. Following the strategy in\nSelfDZSR [Zhang et al. 2022a], we consider the short and long focal-\nlength images as the input and reference, respectively. The reference\nis warped by PWC-Net [Sun et al. 2018] to align with the input\nimage and used as ground-truth for evaluation [Zhang et al. 2022a].\nNote that such a ground-truth image still has misalignment with\nthe input and may contain warping artifacts that affect PSNR and\nSSIM metrics. Table 1 shows that our method outperforms existing\napproaches under such an evaluation setup. Fig. 10 shows that we\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\nEfficient Hybrid Zoom using Camera Fusion on Mobile Phones\n\u2022\n7\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFull W\nFull T\nW\nT\nDCSR\nOurs\nFig. 7. Visual comparisons on our HZSR dataset. Our method recovers more fine details and textures (e.g., the building facades, more legible letters, and\nfacial details) than DCSR [Wang et al. 2021].\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n8\n\u2022\nWu, et al\nW\nT\nDCSR\nOurs\nFull W\nFull T\nMocc\nMdefocus\nMreject\nMblend\nW\nT\nDCSR\nOurs\nFull W\nFull T\nMocc\nMdefocus\nMreject\nMblend\nW\nT\nDCSR\nOurs\nFull W\nFull T\nMocc\nMdefocus\nMreject\nMblend\nW\nT\nDCSR\nOurs\nFull W\nFull T\nMocc\nMdefocus\nMreject\nMblend\nFig. 8. Visual comparisons on our HZSR dataset. We visualize our occlusion mask Mocc, defocus mask Mdefocus, alignment rejection mask Mreject, and\nblending mask Mblend on each example. Note that brighter pixels on Mblend indicate the pixels to be included in the fusion output (see Eq. 6). On the top two\nexamples, T is completely out of focus. DCSR\u2019s results are blurrier than W, while our method can preserve the same sharpness as W. On the bottom two\nexamples, DCSR makes the pixels around occlusion regions blurrier than W. Our results maintain the same amount of details as W.\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\nEfficient Hybrid Zoom using Camera Fusion on Mobile Phones\n\u2022\n9\nInput W\nInput T\nW\nSRNTT\nTTSR\n(source)\n(reference)\n(GT)\n32.4 / 0.87\n36.1 / 0.91\nFull W\nMASA\nC2\nDCSR\nSelfDZSR\nOurs\nPSNR / SSIM\n26.6 / 0.66\n32.2 / 0.84\n32.6 / 0.83\n31.9 / 0.85\n34.2 / 0.89\nFig. 9. Comparisons on CameraFusion dataset. Our results are visually\ncomparable to SelfDZSR [Zhang et al. 2022a] and recover more legible letters\nthan others. TTSR [Yang et al. 2020] achieves the highest PSNR/SSIM as\nit generates an output closer to the ground-truth (W), while our method\nrecovers details to match the reference (T).\nW\nT\nSRNTT\nTTSR\nMASA\n(source)\n(reference)\n28.93 / 0.82 28.69 / 0.81 28.67 / 0.81\nFull W\nC2\nAMSA\nDCSR\nSelfDZSR\nOurs\nPSNR / SSIM\n28.66 / 0.81 28.70 / 0.82 27.91 / 0.767 28.95 / 0.82 37.89 / 0.98\nFig. 10. Comparisons on DRealSR dataset. Our method achieves the\nbest visual quality and appear closer to the reference when compared to\nother methods.\nInput\nDCSR\nOurs\n0%\n25%\n50%\n75%\nFig. 11. User study on Hzsr dataset. Our results are favored in 92.94% of\nthe comparisons.\neffectively transfer the details from the reference to output, while\nstate-of-the-art approaches often generate blurry outputs.\nUser study. As our Hzsr dataset does not have any ground-truth\nfor quantitative evaluation, we conduct a user study to evaluate\nthe subject preference on the results. We design a blind user study,\nwhere users do not know which method the image is generated\nfrom. Each question shows an image from: the input W, output\nfrom DCSR [Wang et al. 2021], and output from our system, and\nasks users to choose the one with the best detail fidelity, such as\nsharpness, clarity, legibility, and textures. We randomly select 20\nimages from the Hzsr dataset in each user session. In total, we\ncollect feedback from 27 users (540 image comparisons). Overall,\nour results are favored in 92.9% of images, where DCSR and the\nTable 1. Quantitative evaluation on the CameraFusion and DRealSR\ndatasets. Top: methods trained with \u21131 or \u21132 loss. Ours-\u21131 is re-trained on\neach dataset. Bottom: methods trained with their own defined losses. Ours\nmodel is trained on the Hzsr dataset using the loss in Eq. 11. The best\nmetrics for CameraFusion do not always correlate with the best visual\nresult as observed in Fig. 9 and [Wang et al. 2021; Zhang et al. 2022a]. Note\nthat DCSR [Wang et al. 2021] does not release their \u21131 model, so we only\nreport the PSNR/SSIM from their paper.\nMethod\nCameraFusion\nDRealSR\nPSNR \u2191\nSSIM \u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nSRNTT-\u21132\n33.36\n0.909\n0.131\n27.30\n0.839\n0.397\nTTSR-\u21131\n36.28\n0.928\n0.140\n25.83\n0.827\n0.411\nMASA-\u21131\n31.85\n0.887\n0.137\n27.27\n0.837\n0.415\nC2-Matching-\u21131\n32.60\n0.899\n0.142\n27.19\n0.840\n0.412\nAMSA-\u21131\n32.99\n0.888\n0.159\n28.04\n0.822\n0.411\nDCSR-\u21131\n36.98\n0.933\nn/a\n27.73\n0.827\nn/a\nSelfDZSR-\u21131\n30.89\n0.868\n0.255\n28.93\n0.857\n0.328\nOurs-\u21131\n34.91\n0.916\n0.170\n31.07\n0.852\n0.131\nSRNTT\n31.61\n0.891\n0.116\n27.31\n0.824\n0.366\nTTSR\n35.48\n0.915\n0.162\n25.31\n0.772\n0.388\nMASA\n28.05\n0.759\n0.255\n27.32\n0.764\n0.381\nC2-Matching\n31.86\n0.858\n0.138\n26.79\n0.814\n0.401\nAMSA\n31.57\n0.885\n0.121\n28.03\n0.821\n0.388\nDCSR\n34.41\n0.904\n0.106\n27.69\n0.823\n0.373\nSelfDZSR\n30.97\n0.870\n0.134\n28.67\n0.836\n0.249\nOurs\n34.09\n0.907\n0.152\n31.20\n0.875\n0.133\nTable 2. Latency comparison. We measure the inference latency (in mil-\nliseconds, or ms) on a Google Cloud platform virtual machine with an Nvidia\nA100 GPU (40 GB RAM). Most methods hit out-of-memory (OOM) errors\nwhen the input image size is larger than 512 \u00d7 512, while our model can\nprocess 12MP resolution inputs within 3ms.\nMethodSize\n256\u00d7\n512\u00d7\n1024\u00d7\n2016\u00d7\n4032\u00d7\n256\n512\n1024\n1512\n3024\nSRNTT\n2 mins\n20 mins\n1 day\nOOM\nOOM\nTTSR\n2,665\nOOM\nOOM\nOOM\nOOM\nMASA\n371\nOOM\nOOM\nOOM\nOOM\nC2-Matching\n373\nOOM\nOOM\nOOM\nOOM\nAMSA\n6,024\nOOM\nOOM\nOOM\nOOM\nDCSR\n52\nOOM\nOOM\nOOM\nOOM\nSelfDZSR\n35\n121\n724\n3,679\nOOM\nOurs\n2.72\n2.82\n2.86\n2.87\n2.95\ninput W are chosen in 1.7% and 5.4% of images, respectively (see\nFig. 11).\nPerformance on workstation. We use a Google cloud platform vir-\ntual machine with 12 cores CPU and an Nvidia A100 GPU (40 GB\nRAM) to test all the methods with input image sizes ranging from\n256 \u00d7 256 to 12MP (4k \u00d7 3k). As shown in Table 2, TTSR [Yang\net al. 2020], MASA [Lu et al. 2021], C2-Matching [Jiang et al. 2021],\nAMSA [Xia et al. 2022], and DCSR [Wang et al. 2021] all hit out-of-\nmemory errors when the input size is larger than 512 \u00d7 512. None\nof the existing models can process 12MP images directly, while\nour model can process 12MP input images within 3ms. Note that\nDCSR [Wang et al. 2021] uses a patch-based inference to process\nhigh-resolution images. We adopt the same patch-based inference\nto generate the results of other compared methods on the Camera-\nFusion and Hzsr datasets.\nPerformance on device. We implement and benchmark our system\non Google Pixel 7 Pro and show the latency breakdown in Table 3.\nThe total latency overhead is 521ms. Peak memory usage occurs\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n10\n\u2022\nWu, et al\nTable 3. On-device latency breakdown (in milliseconds) on a Google\nPixel 7 Pro phone with a Mali-G710 GPU.\nStage\nLatency\nStage\nLatency\nColor matching\n10\nDefocus map\n2\nCoarse alignment\n44\nAlignment rejection map\n131\nOptical flow estimation\n65\nFusion UNet\n240\nBilinear warping\n23\nAdaptive blending\n5\nOcclusion map\n11\nTotal\n521\nFull W and Mdefocus\nW\nwarped T\nOurs w/o\nOurs\nMdefocus\nFig. 12. Contributions of the defocus map. We reject pixels in the white\narea of the defocus map from fusion. Using our defocus map, we preserve\nthe details at fusion output on both defocused (top) and focused regions\n(bottom).\nW\nWarped T\nFull W\nFull Mocc\nOurs w/o Mocc\nOurs\nFig. 13. Contributions of the occlusion map. We can reduce warping\nartifacts near occlusion boundaries with the help of occlusion map in fusion\nand blending.\nduring the Fusion UNet inference stage, which takes an extra 300MB\ncompared to regular single-camera use cases.\n5.3\nAblation study\nAdaptive blending mask. We show the contributions of the de-\nfocus map, occlusion map, flow uncertainty map, and alignment\nrejection map in Fig. 12, 13, 14, and 15. Without the defocus map\nMdefocus in Fig. 12, the background wall becomes blurrier than in\nthe input W, as the blurry pixels in the defocused regions from T\nare fused. Our defocus map excludes background pixels from fusion\nand preserves sharpness. Without the occlusion mask in Fig. 13,\nmisalignment on subject boundaries leads to visible artifacts on the\nfusion results. As shown in Fig. 14, the flow uncertainty map identi-\nfies the regions with incorrect alignment and eliminates warping\nartifacts in the final output. In Fig. 15, the rejection map identifies\nmisaligned pixels and avoids ghosting artifacts.\nTraining losses. We evaluate the contribution of perceptual (Eq. 8),\ncontextual loss (Eq. 9), and brightness consistency loss (Eq. 10).\nW\nWarped T\nFull W\nFull Mflow\nOurs w/o Mflow\nOurs\nFig. 14. Contributions of the flow uncertainty map. Optical flows are\ntypically less robust on object boundaries, resulting in distortion and ghost-\ning after fusion.\nW\nT\nWarped T\nFull W\nFull Mreject\nMreject\nOurs w/o\nOurs\nMreject\nFig. 15. Contributions of the alignment rejection map. Our alignment\nrejection map is able to identify mis-aligned pixels and remove ghosting\nartifacts from the fusion output.\nFull W\nW\nOurs w/o Lvgg\nOurs\nFig. 16. Effectiveness of the VGG loss. VGG perceptual loss improves the\nsharpness and legibility of fusion results.\nFull W\nW\nOurs w/o Lcx\nOurs\nFig. 17. Effectiveness of the contextual loss. Without contextual loss,\nresults are blurry due to mis-aligned training data.\nIn Fig. 16, the VGG loss helps to recover sharper details and more\nvisually pleasing results. The contextual loss [Mechrez et al. 2018]\nin (Eq. 9) minimizes the differences in the semantic feature space\nand relaxes the pixel alignment constraints, which helps to train\nour Fusion UNet on the dual-rig dataset. In Fig. 17, without the\ncontextual loss, Fusion UNet generates blurry pixels when the W\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\nEfficient Hybrid Zoom using Camera Fusion on Mobile Phones\n\u2022\n11\nW\nWarped T\nFull W\nFull warped T\nOurs w/o\nOurs\nLbrightness\nFig. 18. Contributions of the brightness consistency loss. Without the\nbrightness consistency loss, our fusion result shows inconsistent colors on\nthe bear\u2019s body (please zoom in to see the details)\nFull W\nW\nOurs w/o\nOurs\nsmoothing\nFig. 19. Effectiveness of boundary smoothing. The yellow dotted lines\nin (a) show the fusion ROI (i.e., FOV of T). With boundary smoothing, the\nfusion boundary looks more natural and smooth.\nand T are not well aligned. As shown in Fig. 18, with the color-\nconsistency loss, our model can preserve the original color of W\non the fusion result and be robust to the auto-exposure metering\nmismatch between W and T cameras.\nFusion boundary. When blending the fusion output back to the\nfull W image, we apply Gaussian smoothing to smooth the blending\nboundary to avoid abrupt transitions. Without boundary smoothing,\nwe can see invariant details on the building and trees in Fig. 19(b).\nWhile boundary smoothing sacrifices some detail improvements\naround the transition boundary, our results in Fig. 19(c) look more\nnatural after blending.\n5.4\nLimitations\nOur system has the following limitations: 1) Under extremely low-\nlight conditions (less than 5 lux), the T image becomes too noisy\ndue to sensor limitations, as shown in Fig. 20. 2) If the synchroniza-\ntion between T and W exceeds a limit (e.g., 128ms in our system),\nthe alignment will be very challenging, and our system will skip\nfusion to prevent alignment artifacts. 3) Our system does not en-\nhance the details outside the FOV of T, while existing methods (e.g.,\nDCSR [Wang et al. 2021]) can improve details on the whole image\nvia learning SISR or finding long-range correspondences, as shown\nin Fig. 21.\nFull W\nW\nT\nOurs (force fusion)\nFig. 20. Limitation on low-light. Under extremely low-light condition, T\nbecomes too noisy. Our fusion will transfer noise to the output image in\nsuch a case. Therefore, we design our system to skip fusion based on T SNR.\nW\nOurs\nDCSR\n(Within T FOV)\nFull W\nFull T\nW\nOurs\nDCSR\n(Outside T FOV)\nFig. 21. Limitation on non-overlapping FOV. For the pixels outside T\nFOV, our method maintains the same values as W, while DCSR is able to\nenhance some details.\n6\nCONCLUSIONS\nIn this work, we present a robust system for hybrid zoom super-\nresolution on mobile devices. We develop efficient ML models for\nalignment and fusion, propose an adaptive blending algorithm to\naccount for imperfections in real-world images, and design a train-\ning strategy using an auxiliary camera to minimize domain gaps.\nOur system achieves an interactive speed (500 ms to process a 12MP\nimage) on mobile devices and is competitive against state-of-the-art\nmethods on public benchmarks and our Hzsr dataset.\nACKNOWLEDGMENTS\nThis work would be impossible without close collaboration between\nmany teams within Google. We are particularly grateful to Li-Chuan\nYang, Sung-Fang Tsai, Gabriel Nava, Ying Chen Lou, and Lida Wang\nfor their work on integrating the dual camera system. We also\nthank Junhwa Hur, Sam Hasinoff, Mauricio Delbracio, and Peyman\nMilanfar for their advice on algorithm development. We are grateful\nto Henrique Maia and Brandon Fung for their helpful discussions\non the paper. Finally, we thank Hsin-Fu Wang, Lala Hsieh, Yun-Wen\nWang, Daniel Tat, Alexander Tat and all the photography models for\ntheir significant contributions to data collection and image quality\nreviewing.\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n12\n\u2022\nWu, et al\nREFERENCES\nLuis Alvarez, Rachid Deriche, Th\u00e9o Papadopoulo, and Javier S\u00e1nchez. 2007. Symmetrical\ndense optical flow estimation with occlusions detection. IJCV 75 (2007), 371\u2013385.\nSameer Ansari, Neal Wadhwa, Rahul Garg, and Jiawen Chen. 2019. Wireless software\nsynchronization of multiple distributed cameras. In ICCP. IEEE, Tokyo, Japan, 1\u20139.\nKelvin C.K. Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. 2021.\nGLEAN: Generative Latent Bank for Large-Factor Image Super-Resolution. In CVPR.\nIEEE, Virtual/Online, 14245\u201314254.\nFerenc Huszar Jose Caballero Andrew Cunningham Alejandro Acosta Andrew Aitken\nAlykhan Tejani Johannes Totz Zehan Wang Wenzhe Shi Christian Ledig, Lucas Theis.\n2017. Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial\nNetwork. In CVPR.\nXiaodong Cun and Chi-Man Pun. 2020. Defocus blur detection via depth distillation.\nIn ECCV.\nChao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. 2014. Learning a deep\nconvolutional network for image super-resolution. In ECCV.\nJochen Gast and Stefan Roth. 2018. Lightweight probabilistic deep networks. In ICCV.\nJinjin Gu, Yujun Shen, and Bolei Zhou. 2020. Image processing using multi-code GAN\nprior. In CVPR.\nSamuel W Hasinoff, Dillon Sharlet, Ryan Geiss, Andrew Adams, Jonathan T Barron,\nFlorian Kainz, Jiawen Chen, and Marc Levoy. 2016. Burst photography for high\ndynamic range and low-light imaging on mobile cameras. ACM TOG (2016).\nJingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. 2022. GCFSR: a Generative\nand Controllable Face Super Resolution Method Without Facial and GAN Priors. In\nCVPR.\nHonorMagic 2023. Honor Magic4 Ultimate Camera test. https://www.dxomark.com/honor-\nmagic4-ultimate-camera-test-retested/. Accessed: 2023-03-07.\nYixuan Huang, Xiaoyun Zhang, Yu Fu, Siheng Chen, Ya Zhang, Yan-Feng Wang, and\nDazhi He. 2022. Task decoupled framework for reference-based super-resolution. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\nYuming Jiang, Kelvin CK Chan, Xintao Wang, Chen Change Loy, and Ziwei Liu. 2021.\nRobust Reference-based Super-Resolution via C2-Matching. In CVPR.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016. Perceptual losses for real-time\nstyle transfer and super-resolution. In ECCV.\nJiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. 2016. Accurate image super-resolution\nusing very deep convolutional networks. In CVPR.\nWei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-Hsuan Yang. 2017. Deep\nLaplacian pyramid networks for fast and accurate super-resolution. In CVPR.\nWei-Sheng Lai, Yichang Shih, Lun-Cheng Chu, Xiaotong Wu, Sung-Fang Tsai, Michael\nKrainin, Deqing Sun, and Chia-Kai Liang. 2022. Face deblurring using dual camera\nfusion on mobile phones. ACM TOG (2022).\nJunyong Lee, Myeonghee Lee, Sunghyun Cho, and Seungyong Lee. 2022. Reference-\nbased video super-resolution using multi-camera video triplets. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition.\nJunyong Lee, Sungkil Lee, Sunghyun Cho, and Seungyong Lee. 2019. Deep defocus\nmap estimation using domain adaptation. In CVPR.\nLiying Lu, Wenbo Li, Xin Tao, Jiangbo Lu, and Jiaya Jia. 2021. MASA-SR: Matching\nacceleration and spatial adaptation for reference-based image super-resolution. In\nCVPR.\nRoey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. 2018. The contextual loss for\nimage transformation with non-aligned data. In ECCV.\nSachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. 2020.\nPulse: Self-supervised photo upsampling via latent space exploration of generative\nmodels. In CVPR.\nAugustus Odena, Vincent Dumoulin, and Chris Olah. 2016. Deconvolution and Checker-\nboard Artifacts. Distill (2016). http://distill.pub/2016/deconv-checkerboard/\nMarco Pesavento, Marco Volino, and Adrian Hilton. 2021. Attention-based multi-\nreference learning for image super-resolution. In CVPR.\nFitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun, Caroline Pantofaru, and\nBrian Curless. 2022. Film: Frame interpolation for large motion. In ECCV.\nErik Reinhard, Michael Adhikhmin, Bruce Gooch, and Peter Shirley. 2001. Color transfer\nbetween images. IEEE Computer graphics and applications 21, 5 (2001), 34\u201341.\nYaniv Romano, John Isidoro, and Peyman Milanfar. 2016. RAISR: rapid and accurate\nimage super resolution. IEEE TCI (2016).\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In MICCAI.\nEdward Rosten and Tom Drummond. 2006. Machine learning for high-speed corner\ndetection. In ECCV.\nGyumin Shim, Jinsun Park, and In So Kweon. 2020. Robust reference-based super-\nresolution with similarity-aware deformable convolution. In CVPR.\nDeqing Sun, Charles Herrmann, Fitsum Reda, Michael Rubinstein, David J. Fleet, and\nWilliam T Freeman. 2022. Disentangling Architecture and Training for Optical Flow.\nIn ECCV.\nDeqing Sun, Daniel Vlasic, Charles Herrmann, Varun Jampani, Michael Krainin, Huiwen\nChang, Ramin Zabih, William T Freeman, and Ce Liu. 2021. AutoFlow: Learning a\nBetter Training Set for Optical Flow. In CVPR.\nDeqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. 2018. Pwc-net: Cnns for\noptical flow using pyramid, warping, and cost volume. In CVPR.\nLibin Sun and James Hays. 2012. Super-resolution from internet-scale scene matching.\nIn ICCP.\nRichard Szeliski. 2022. Computer vision: algorithms and applications. Springer Nature.\nChang Tang, Xinzhong Zhu, Xinwang Liu, Lizhe Wang, and Albert Zomaya. 2019.\nDefusionnet: Defocus blur detection via recurrently fusing and refining multi-scale\ndeep features. In CVPR.\nZachary Teed and Jia Deng. 2020. Raft: Recurrent all-pairs field transforms for optical\nflow. In ECCV.\nRobert Triggs. 2023. All the new HUAWEI P40 camera technology explained. https:\n//www.androidauthority.com/huawei-p40-camera-explained-1097350/. Accessed: 2023-03-07.\nMarc Comino Trinidad, Ricardo Martin Brualla, Florian Kainz, and Janne Kontkanen.\n2019. Multi-view image fusion. In CVPR.\nTengfei Wang, Jiaxin Xie, Wenxiu Sun, Qiong Yan, and Qifeng Chen. 2021. Dual-camera\nsuper-resolution with aligned attention modules. In CVPR.\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen\nChange Loy. 2018. ESRGAN: Enhanced super-resolution generative adversarial\nnetworks. In ECCV.\nYufei Wang, Zhe Lin, Xiaohui Shen, Radomir Mech, Gavin Miller, and Garrison W\nCottrell. 2016. Event-specific image importance. In CVPR.\nPengxu Wei, Ziwei Xie, Hannan Lu, Zongyuan Zhan, Qixiang Ye, Wangmeng Zuo,\nand Liang Lin. 2020. Component divide-and-conquer for real-world image super-\nresolution. In ECCV.\nBartlomiej Wronski, Ignacio Garcia-Dorado, Manfred Ernst, Damien Kelly, Michael\nKrainin, Chia-Kai Liang, Marc Levoy, and Peyman Milanfar. 2019. Handheld multi-\nframe super-resolution. ACM TOG (2019).\nBin Xia, Yapeng Tian, Yucheng Hang, Wenming Yang, Qingmin Liao, and Jie Zhou.\n2022. Coarse-to-Fine Embedded PatchMatch and Multi-Scale Dynamic Aggregation\nfor Reference-based Super-Resolution. In AAAI.\nYanchun Xie, Jimin Xiao, Mingjie Sun, Chao Yao, and Kaizhu Huang. 2020. Feature repre-\nsentation matters: End-to-end learning for reference-based image super-resolution.\nIn ECCV.\nShumian Xin, Neal Wadhwa, Tianfan Xue, Jonathan T Barron, Pratul P Srinivasan,\nJiawen Chen, Ioannis Gkioulekas, and Rahul Garg. 2021. Defocus map estimation\nand deblurring from a single dual-pixel image. In ICCV.\nRuikang Xu, Mingde Yao, and Zhiwei Xiong. 2023.\nZero-Shot Dual-Lens Super-\nResolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition.\nFuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo. 2020. Learning\ntexture transformer network for image super-resolution. In CVPR.\nKai Zhang, Jingyun Liang, Luc Van Gool, and Radu Timofte. 2021. Designing a practical\ndegradation model for deep blind image super-resolution. In ICCV.\nXuaner Zhang, Qifeng Chen, Ren Ng, and Vladlen Koltun. 2019a. Zoom to learn, learn\nto zoom. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition.\nXindong Zhang, Hui Zeng, Shi Guo, and Lei Zhang. 2022b. Efficient Long-Range\nAttention Network for Image Super-resolution. In ECCV.\nYulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. 2018. Image\nsuper-resolution using very deep residual channel attention networks. In ECCV.\nZhilu Zhang, Ruohao Wang, Hongzhi Zhang, Yunjin Chen, and Wangmeng Zuo. 2022a.\nSelf-Supervised Learning for Real-World Super-Resolution from Dual Zoomed Ob-\nservations. In ECCV.\nZhifei Zhang, Zhaowen Wang, Zhe Lin, and Hairong Qi. 2019b. Image super-resolution\nby neural texture transfer. In CVPR.\nWenda Zhao, Bowen Zheng, Qiuhua Lin, and Huchuan Lu. 2019. Enhancing diversity\nof defocus blur detectors via cross-ensemble network. In CVPR.\nHaitian Zheng, Mengqi Ji, Haoqian Wang, Yebin Liu, and Lu Fang. 2018. Crossnet: An\nend-to-end reference-based super resolution network using cross-scale warping. In\nECCV.\nACM Trans. Graph., Vol. 42, No. 6, Article . Publication date: December 2023.\n"
  },
  {
    "title": "Incremental FastPitch: Chunk-based High Quality Text to Speech",
    "link": "https://arxiv.org/pdf/2401.01755.pdf",
    "upvote": "6",
    "text": "INCREMENTAL FASTPITCH: CHUNK-BASED HIGH QUALITY TEXT TO SPEECH\nMuyang Du1, Chuan Liu1, Junjie Lai1\n1NVIDIA Corporation\nABSTRACT\nParallel text-to-speech models have been widely applied for\nreal-time speech synthesis, and they offer more controllabil-\nity and a much faster synthesis process compared with con-\nventional auto-regressive models. Although parallel models\nhave benefits in many aspects, they become naturally unfit for\nincremental synthesis due to their fully parallel architecture\nsuch as transformer. In this work, we propose Incremental\nFastPitch, a novel FastPitch variant capable of incrementally\nproducing high-quality Mel chunks by improving the archi-\ntecture with chunk-based FFT blocks, training with receptive-\nfield constrained chunk attention masks, and inference with\nfixed size past model states. Experimental results show that\nour proposal can produce speech quality comparable to the\nparallel FastPitch, with a significant lower latency that allows\neven lower response time for real-time speech applications.\nIndex Terms\u2014 text-to-speech, speech synthesis, real-\ntime, low-latency, streaming tts\n1. INTRODUCTION\nIn recent years, Text-to-Speech (TTS) technology has wit-\nnessed remarkable advancements, enabling the generation of\nnatural and expressive speech from text inputs. Neural TTS\nsystem primarily contains an acoustic model and a vocoder.\nIt involves first converting the texts to Mel-spectrogram\nby acoustic models such as Tacotron 2[1], FastSpeech[2],\nFastPitch[3], GlowTTS[4], then converting the Mel feature to\nwaveform by vocoders such as WaveNet[5], WaveRNN[6, 7],\nWaveGlow[8], and HiF-GAN[9]. Moreover, with the boost\nof real-time and streaming applications, there is an increasing\ndemand for TTS systems capable of producing speech in-\ncrementally, also known as streaming TTS, to provide lower\nresponse latency for better user experience.\nFor example,\nSamsung[10] proposed a low-latency streaming TTS sys-\ntem running on CPUs based on Tacotron 2 and LPCNet[11].\nNVIDIA[12] also proposed a highly efficient streaming TTS\npipeline running on GPUs based on BERT[13], Tacotron 2\nand HiFi-GAN. Both of them uses auto-regressive acoustic\nmodel for incremental Mel generation.\nAuto-regressive acoustic models such as Tacotron 2 is\ncapable of producing natural speech by leveraging sequen-\ntial generation to capture prosody and contextual depen-\nFig. 1: Incremental FastPitch, Chunk-based FFT Block, and\nChunk Mask for Receptive-Filed Constrained Training\ndencies.\nHowever, it suffers from slow inference due to\nthe frame-by-frame generation process and susceptibility\nto over-generation and word-repeating artifacts due to un-\nstable alignment learned between the input phonemes and\noutput frames. In contrast, parallel acoustic models like such\nas FastPitch offers a faster inference process by producing\ncomplete Mel-spectrogram in one step. Additionally, it also\nshows benefits in providing the flexibility to manipulate pitch,\nduration, and speed of the synthetic speech as those metadata\nare pre-generated before decoding.\nAlthough parallel acoustic models offer many advan-\ntages, their model structure poses challenges for their use in\nincremental speech synthesis. For instance, FastPitch utilizes\na transformer[14] decoder, wherein attention is computed\nacross the entire encoded feature sequence to generate the\nMel-spectrogram output. A straightforward method is to slice\nthe encoded feature sequence into chunks and then decode\neach chunk into a corresponding Mel chunk. However, this\napproach compels the decoder to focus only on a chunk, re-\narXiv:2401.01755v1  [cs.SD]  3 Jan 2024\nsulting in audible discontinuity at the edges of Mel chunks,\neven when overlapping between chunks is used. An alterna-\ntive approach is to modify the model to use an auto-regressive\ndecoder. However, this fails back to frame-by-frame genera-\ntion, sacrificing the parallelism advantage. Therefore, an ideal\ndecoder for incremental TTS should be able to incrementally\ngenerate Mel chunks while maintaining parallelism during\nthe chunk generation process and keeping the computational\ncomplexity of each chunk consistent in the meantime.\nBased on the above considerations, we present Incremen-\ntal FastPitch, capable of producing high-quality Mel chunks\nwhile maintaining chunk generation parallelism and provid-\ning low response latency. We incorporate chunk-based FFT\nblocks with fixed-size attention state caching, which is cru-\ncial for transformer-based incremental TTS to avoid the com-\nputational complexity increases with synthetic length.\nWe\nalso utilize receptive-filed constrained training and investigate\nboth the static and dynamic chunk masks, which is vital to\nalign the model with limited receptive-filed inference.\n2. METHOD\n2.1. Incremental FastPitch\nFigure 1A depicts the proposed Incremental FastPitch model,\na variant of the parallel FastPitch.\nIt takes a complete\nphoneme sequence as input and generates Mel-spectrogram\nincrementally, chunk-by-chunk, with each chunk contains\na fixed number of Mel frames.\nIncremental FastPitch is\nequipped with the same encoder, energy predictor, pitch pre-\ndictor, and duration predictor as the parallel FastPitch. How-\never, the decoder of Incremental FastPitch is composed of a\nstack of chunk-based FFT blocks. In contrast to the decoder\nof parallel FastPitch that takes the entire upsampled unified\nfeature \u00afu as input and generate the entire Mel-spectrogram at\nonce, The decoder of Incremental FastPitch first divide the \u00afu\nto N chunks [\u00afu1, \u00afu2, ..., \u00afuN], then convert one chunk \u00afui at a\ntime to a chunk of Mel \u00afyi. During training, we apply a chunk-\nbased attention mask on the decoder to help it adjust to the\nconstrained receptive field in incremental inference, which\nwe term it as the Receptive Field-Constrained Training.\n2.2. Chunk-based FFT Block\nFigure 1B illustrates the chunk-based FFT block, which con-\ntains a stack of a multi-head attention (MHA) block and a\nposition-wise causal convolutional feed forward block. Com-\npare with parallel FastPitch, the MHA block in the chunk-\nbased FFT block requires two additional inputs: past key and\npast value, produced by itself during previous chunk gener-\nation. Instead of utilizing all the accumulated historical past\nkeys and values from prior chunks, we employ fixed-size past\nkey and value for inference by retaining only their tails. The\npast size maintains consistent throughout incremental genera-\ntion, preventing an increase in computational complexity with\nthe number of chunks. Although we impose an explicit past\nsize limit, experiments shows that it is capable of encoding\nsufficient historical information for generating high-quality\nMel. The calculation of MHA is defined as:\nkt\ni = concat(pkt\u22121\ni\n, KW K\ni )\nvt\ni = concat(pvt\u22121\ni\n, V W V\ni )\not\ni = attention(kt\ni, vt, QW Q\ni )\not\nM = concat(ot\n1, ..., ot\nh)W O\npkt\ni = tail slice(kt\ni, Sp)\npvt\ni = tail slice(vt\ni, Sp)\n(1)\nwhere pkt\u22121\ni\nand pvt\u22121\ni\nare the past K and past V of head\ni from chunk t\u22121. kt\ni and vt\ni are the embedded K and V with\nthe past concatenated along the time dimension for attention\ncomputation of head i at chunk t. ot\nM is the output of MHA\nblock at chunk t. W K\ni , W V\ni , W Q\ni , and W O are the trainable\nweights. Sp is the configurable fixed size of the past. pkt\ni and\npvt\ni are obtained by slicing size Sp from the tail of kt\ni and vt\ni\nalong the time dimension.\nSimilarly, the calculation of position-wise causal convo-\nlution feed forward block is defined as:\nct\n1 = concat(pct\u22121\n1\n, ot\nM)\not\nc1 = relu(causal conv(ct\n1))\nct\n2 = concat(pct\u22121\n2\n, ot\nc1)\not\nc2 = relu(causal conv(ct\n2))\npct\n1 = tail slice(ct\n1, Sc1)\npct\n2 = tail slice(ct\n2, Sc2)\n(2)\nwhere pct\u22121\n1\nand pct\u22121\n2\nare the past states of the two causal\nconvolutional layers. Starting with pct\u22121\n1\n, it\u2019s concatenated\nwith ot\nM to yield ct\n1, serving as input for the first causal conv\nlayer. Next, ot\nc1, the output from the first causal conv layer,\nis concatenated with pct\u22121\n2\nto generate ct\n2. This is then input\nto the second causal conv layer, resulting in the final output\not\nc2. Lastly, pct\n1 and pct\n2 are extracted by slicing sizes Sc1 and\nSc2 from the tail of ct\n1 and pct\n2 along the time dimension, re-\nspectively. Unlike the configurable Sp, we set Sc1 and Sc2 to\ntheir respective conv kernel sizes minus 1, which is adequate\nto attain equivalence with parallel inference.\n2.3. Decoder Receptive Field Analysis\nFigure 2 demonstrates the receptive filed of the proposed\nchunk-based decoder. For better visualization, we omit the\npositional-wise convolutional feed-forward blocks. The or-\nange block at the top-right corner represents the final FFT\noutput Ot of chunk t. The dark green MHA blocks are those\nwhose multi-head attention, past key, and past value outputs\ncontribute to Ot.\nThe light green MHA blocks are those\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nt\nt-1\nt-2\nt-3\nt-4\nt-5\nt-6\n1\n2\n3\n4\n5\n6\nlayers\nchunks\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nMHA\nFig. 2: Chunk-based decoder receptive field visualization.\nwhose past key and past value outputs contribute to Ot. Sim-\nilarly, the blue blocks (past keys and past values) and the\nyellow blocks (inputs of green MHA blocks) are those who\ncontribute to Ot. By feeding the fixed size past key and past\nvalue of chunk t \u2212 1 to each MHA block during chunk t\ngeneration, we can expand the receptive field of chunk t to\nseveral of its previous chunks without the need to explicitly\nfeed those previous chunks as decoder input.\nThe receptive field R depends on the number of decoder\nlayers and the size of past keys and past values, as given by:\nR = (Nd + \u230aSp/Sc\u230b + 1) \u00b7 Sc\n(3)\nwhere Nd is the number of decoder layers, Sp is the size\nof past keys and past values, and Sc is the size of the chunk.\nThe unit of R is the number of decoder frames. If Sp is less\nthan or equal to Sc, then the past key and past value output by\na MHA block only depends on the input of that MHA block,\nthus R simply equals to (Nd+1)\u00b7Sc, same as shown in figure\n2, whereas if Sp is greater than Sc, then the past key and past\nvalue of a MHA block at chunk t will also depends on the\npast keys and values of that MHA block at previous chunks,\nresulting in R grows linearly with the floor of Sp/Sc.\n2.4. Receptive Field-Constrained Training\nGiven a limited decoder receptive field during inference, it\nbecomes vital to align the decoder with this constraint during\ntraining. Therefore, we use the Receptive Field-Constrained\nTraining by applying chunk-based attention mask to all the\ndecoder layers. Figure 1C visualizes various attention masks\nwith a given chunk size (dark grey) and different past sizes\n(light grey). An intuitive approach is to randomly select a\nchunk size and past size for dynamic mask creation for each\ntext-audio training data pair within a batch. This approach is\nsimilar to the masks used in the WeNet[15, 16] ASR encoder.\nThe dynamic mask can help the decoder generalize to diverse\nchunk and past sizes. However, most of the incremental sys-\ntem TTS employs a fixed chunk size for inference. Using a\ndynamic mask for training may potentially introduce a gap\nbetween training and inference. Therefore, we also investi-\ngate training with static masks that constructed using a fixed\nchunk size and past size during the training process.\n3. EXPERIMENTS\n3.1. Experimental Setup\nDataset. The Chinese Standard Mandarin Speech Corpus[17]\nreleased by DataBaker is used for both training and evalua-\ntion. It contains 10,000 48kHz 16bit audio clips of a single\nMandarin female speaker and has a total of 12 hours with\neach audio clip contains a short sentence of 4.27 seconds on\naverage. In our experiments, we downsample the corpus to\n22.05kHz and 100 audio clips are reserved for evaluation.\nModel & Acoustic Specifications. The proposed model pa-\nrameters follow the open-source FastPitch implementation[18],\nexcept that we use causal convolution in the position-wise\nfeed forward layers.\nThe decoder is used to predict Mel-\nspectrogram with 80 frequency bins. It is generated through\nan FFT size of 1024, a hop length of 256 and a window\nlength of 1024, applied to the normalized waveform. To en-\nhance convergence speed and stability, the Mel values are\nstandardized within a symmetrical range from -4 to 4.\nTraining & Evaluation. Our models are trained using the\nAdam optimizer[19] with batch size 8, initializing with a\nlearning rate of 1e-4 and a weight decay of 1e-6. The ex-\nperiments are performed on an NVIDIA RTX 6000 GPU,\nutilizing single precision and applying gradient clipping[20].\nWe use Mel-spectrogram distance (MSD) and mean opin-\nion score (MOS) to measure the speech quality. To ensure\nthe Mel-spectrograms of two audios are properly aligned for\nMSD calculation, we first use a trained parallel FastPitch to\nproduce unified duration, pitch, and energy values for evalua-\ntion texts, then use these values to process the output feature\nof Incremental FastPitch encoder. Regarding the MOS, we\nsynthesize waveform for evaluation with HiFi-GAN trained\nusing the same dataset as FastPitch.\nSince we focus on\noptimizing acoustic model for incremental TTS, the vocod-\ning process is non-incremental. For Incremental FastPitch,\nwe concatenate all the Mel chunks to the complete Mel for\nvocoding. The MOS scores are collected through the assess-\nment of 20 evaluation samples for each configuration by 10\nAmazon MTurk listeners, who assign scores ranging from 1\nto 5. For audio samples, please refer to GitHub page1.\n3.2. Discussion\n3.2.1. Comparison of Static and Dynamic Chunk Masks\nFigure 3 shows the Mel-spectrogram distance between the In-\ncremental FastPitch and the parallel FastPitch. During infer-\nence, we use a fixed chunk size 30 for all the models. In the\nsub-figure A, the models are train with static chunk masks.\nThe chunk sizes are fixed to 30 and past sizes are set to 0, 5,\n15, 30, 60, 90, and all. We can observe that the smallest MSD\nof each model is often achieved when we use the same (or\nsimilar) chunk size and past size for training and inference.\n1https://muyangdu.github.io/incremental-fastpitch\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nStatic Chunk Size, Static Past Size (Training)\n30, 5\n30, 15\n30, 30\n30, 60\n30, 90\nChunk Size, Past Size (Inference) :\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nDynamic Chunk Size, Dynamic Past Size (Training)\nA\nB\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nStatic Chunk Size, Static Past Size (Training)\n30, 5\n30, 15\n30, 30\n30, 60\n30, 90\nChunk Size, Past Size (Inference) :\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nDynamic Chunk Size, Dynamic Past Size (Training)\nA\nB\n30, All\n30, 90\n30, 60\n30, 30\n30, 15\n30, 5\n1-50, All\n1-50, 3x\n1-50, 2x\n1-50, 1x\n1-50, 0.5x 1-50, 0.25x\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nStatic Chunk Size, Static Past Size (Training)\n30, 5\n30, 15\n30, 30\n30, 60\n30, 90\nChunk Size, Past Size (Inference) :\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nDynamic Chunk Size, Dynamic Past Size (Training)\nA\nB\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nStatic Chunk Size, Static Past Size (Training)\n30, 5\n30, 15\n30, 30\n30, 60\n30, 90\nChunk Size, Past Size (Inference) :\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nDynamic Chunk Size, Dynamic Past Size (Training)\nA\nB\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nStatic Chunk Size, Static Past Size (Training)\n0.055\n0.060\n0.065\n0.070\n0.075\n0.080\n0.085\n0.090\nMel Spectrogram Distance\nDynamic Chunk Size, Dynamic Past Size (Training)\nA\nB\n0.090\n0.085\n0.080\n0.075\n0.070\n0.065\n0.060\n0.055\n0.090\n0.085\n0.080\n0.075\n0.070\n0.065\n0.060\n0.055\nB\nA\nFig. 3: MSD between the parallel FastPitch and the Incre-\nmental FastPitch trained with different types of masks, then\ninference with different chunk and past sizes. Each bar in the\nfigure represents a specific (chunk size, past size) for infer-\nence. The horizontal axis describes the (chunk size, past size)\nused for training. A. Static Mask. B. Dynamic Mask.\nThe smallest MSD is achieved with past size 5 (red marked).\nSpecifically, we find that if the model is trained with a small\npast size such as 5, it has a high MSD when inference with\na big past size such as 90. On the contrary, if the model is\ntrained with a big past size, it has a more stable MSD when\ninference with small past sizes. This observation suggests that\neven if the model is trained with a larger past context, it still\nlearns to generate Mel chunk condition on nearby past con-\ntexts, rather than those far from the current chunk.\nIn the sub-figure B, the models are trained with dynamic\nchunk masks. The chunk sizes are randomly selected from\nrange 1 to 50, and the past sizes are set to 0, 0.25, 0.5, 1,\n2, 3 times of the selected chunk size and all. We observe\nthat the MSD are more stable and similar if the inference past\nsize changes, compared with static mask. The smallest MSD\nis achieved when we use 2 times of the randomly selected\nchunk size as the past size. However, the MSD of the dynamic\nchunk mask models are generally higher than the static chunk\nmask models. This observation confirms our suspicion raised\nin subsection 2.4 that dynamic mask training can introduce\na training inference mismatch. Based on the above analysis,\nit is suggested to use a static mask for the best quality if the\ninference chunk and past sizes can be known in advance.\n3.2.2. Visualized Ablation Study\nWe perform visualized ablation study to investigate the ne-\ncessity of using past key value and past conv state. Figure\n4 shows the synthetic Mel-spectrograms of parallel FastPitch\nand Incremental FastPitch. We can observe that the Incre-\nmental FastPitch can generate Mel with almost no observable\ndifference compared with parallel FastPitch. However, if ei-\nther the past key value or the conv state is removed, apparent\ndiscontinuation can be found between adjacent Mel chunks.\nFastPitch\nFully Parallel\nIncremental FastPitch \nw past conv states, key_values\nIncremental FastPitch\nwo past conv states\nIncremental FastPitch\nwo past key_values\nA\nB\nC\nD\nFastPitch\nFully Parallel\nIncremental FastPitch \nw past conv states, key_values\nIncremental FastPitch\nwo past conv states\nIncremental FastPitch\nwo past key_values\nA\nB\nC\nD\nFastPitch\nFully Parallel\nIncremental FastPitch \nw past conv states, key_values\nIncremental FastPitch\nwo past conv states\nIncremental FastPitch\nwo past key_values\nA\nB\nC\nD\nFastPitch\nFully Parallel\nIncremental FastPitch \nw past conv states, key_values\nIncremental FastPitch\nwo past conv states\nIncremental FastPitch\nwo past key_values\nA\nB\nC\nD\nFig. 4: Mel-spectrogram Visualization.\n3.2.3. Evaluation of Speech Quality and Performance\nTo study the audible speech quality of both the static (S) and\ndynamic (D) mask trained Incremental FastPitch, we perform\nlistening tests on the best S and D models selected based on\nthe MSD analysis (marked as red in figure 3).\nAs shown\nin table 1, we find that Incremental FastPitch is capable of\nproducing high quality speech comparable with the parallel\nFastPitch. Furthermore, the score of D model is only slightly\nlower than the S model, although the D model has a 8.3%\nhigher MSD compared with the S model. This result shows\nthat the audible difference of the S and D model is barely no-\nticeable, especially with the compensation of vocoder.\nTable 1: Mean opinion score (MOS) with 95% CI, real time\nfactor (RTF), and latency (ms) comparison on evaluation set.\nModel\nMOS\nLatency\nRTF\nPar. FastPitch\n4.185 \u00b1 0.043\n125.77\n0.029\nInc. FastPitch (S)\n4.178 \u00b1 0.047\n30.35\n0.045\nInc. FastPitch (D)\n4.145 \u00b1 0.052\nGround Truth\n4.545 \u00b1 0.039\n-\n-\nTable 1 also displays RTF and latency. For Incremental\nFastPitch, RTF is defined as dividing the last chunk\u2019s latency\nby the audio duration, and latency corresponds to the first\nchunk\u2019s latency. The S and D model shares the same infer-\nence process. We find that Incremental FastPitch has a higher\nRTF but is still able to achieve around 22\u00d7 real-time as it\nmaintains the parallelism of chunk generation. Notably, it has\na significantly lower latency compared to parallel FastPitch.\n4. CONCLUSIONS\nIn this work, we propose Incremental FastPitch, capable of\nincrementally generating high-quality Mel chunks with low\nlatency while maintaining chunk generation parallelism and\nconsistent computation complexity. We improve the decoder\nwith chunk-based FFT blocks that use fixed size state caching\nto maintain Mel continuity across chunks. We further experi-\nment with multiple masking configurations of receptive-filed\nconstrained training for adapting model to limited receptive\nfiled inference. Experiments show that our proposal can pro-\nduce speech quality comparable to the parallel baseline, with\na significant lower latency that allows even lower response\ntime for real-time speech synthesis.\n5. REFERENCES\n[1] Jonathan Shen, Ruoming Pang, Ron J Weiss, Mike\nSchuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan, et al.,\n\u201cNatural tts synthesis by conditioning wavenet on mel\nspectrogram predictions,\u201d in 2018 IEEE international\nconference on acoustics, speech and signal processing\n(ICASSP). IEEE, 2018, pp. 4779\u20134783.\n[2] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu, \u201cFastspeech: Fast, robust\nand controllable text to speech,\u201d\nAdvances in neural\ninformation processing systems, vol. 32, 2019.\n[3] Adrian La\u00b4ncucki,\n\u201cFastpitch: Parallel text-to-speech\nwith pitch prediction,\u201d in ICASSP 2021-2021 IEEE In-\nternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2021, pp. 6588\u20136592.\n[4] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sun-\ngroh Yoon, \u201cGlow-tts: A generative flow for text-to-\nspeech via monotonic alignment search,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp.\n8067\u20138077, 2020.\n[5] A van den Oord, S Dieleman, H Zen, K Simonyan,\nO Vinyals, A Graves, N Kalchbrenner, AW Senior, and\nK Kavukcuoglu, \u201cWavenet: A generative model for raw\naudio, corr, vol. abs/1609.03499,\u201d 2017.\n[6] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb\nNoury, Norman Casagrande, Edward Lockhart, Florian\nStimberg, Aaron Oord, Sander Dieleman, and Koray\nKavukcuoglu, \u201cEfficient neural audio synthesis,\u201d in In-\nternational Conference on Machine Learning. PMLR,\n2018, pp. 2410\u20132419.\n[7] Muyang Du, Chuan Liu, Jiaxing Qi, and Junjie Lai, \u201cIm-\nproving WaveRNN with Heuristic Dynamic Blending\nfor Fast and High-Quality GPU Vocoding,\u201d in Proc. IN-\nTERSPEECH 2023, 2023, pp. 4344\u20134348.\n[8] Ryan Prenger, Rafael Valle, and Bryan Catanzaro,\n\u201cWaveglow:\nA flow-based generative network for\nspeech synthesis,\u201d in ICASSP 2019-2019 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 2019, pp. 3617\u20133621.\n[9] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \u201cHifi-\ngan: Generative adversarial networks for efficient and\nhigh fidelity speech synthesis,\u201d\nAdvances in Neural\nInformation Processing Systems, vol. 33, pp. 17022\u2013\n17033, 2020.\n[10] Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos\nMarkopoulos, Aimilios Chalamandaris, Georgia Ma-\nniati, Panos Kakoulidis, Spyros Raptis, June Sig Sung,\nHyoungmin Park, and Pirros Tsiakoulis, \u201cHigh quality\nstreaming speech synthesis with low, sentence-length-\nindependent latency,\u201d pp. 2022\u20132026, ISCA.\n[11] Jean-Marc Valin and Jan Skoglund, \u201cLpcnet: Improv-\ning neural speech synthesis through linear prediction,\u201d\nin ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2019, pp. 5891\u20135895.\n[12] Muyang Du, Chuan Liu, Jiaxing Qi, and Junjie Lai, \u201cEf-\nficient incremental text-to-speech on gpus,\u201d\nin 2023\nAsia Pacific Signal and Information Processing Asso-\nciation Annual Summit and Conference (APSIPA ASC).\nIEEE, 2023, pp. 1422\u20131428.\n[13] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova,\n\u201cBert: Pre-training of deep bidirectional\ntransformers for language understanding,\u201d in Proceed-\nings of naacL-HLT, 2019, vol. 1, p. 2.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser,\nand Illia Polosukhin, \u201cAttention is all you need,\u201d Ad-\nvances in neural information processing systems, vol.\n30, 2017.\n[15] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan\nYu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie,\nand Xin Lei,\n\u201cWeNet: Production Oriented Stream-\ning and Non-Streaming End-to-End Speech Recognition\nToolkit,\u201d in Proc. Interspeech 2021, 2021, pp. 4054\u2013\n4058.\n[16] Binbin Zhang, Di Wu, Zhendong Peng, Xingchen Song,\nZhuoyuan Yao, Hang Lv, Lei Xie, Chao Yang, Fuping\nPan, and Jianwei Niu, \u201cWeNet 2.0: More Productive\nEnd-to-End Speech Recognition Toolkit,\u201d in Proc. In-\nterspeech 2022, 2022, pp. 1661\u20131665.\n[17] Databaker,\n\u201cChinese standard mandarin speech cor-\npus,\u201d\nhttps://www.data-baker.com/open_\nsource.html, 2023, Accessed: September 3, 2023.\n[18] NVIDIA,\n\u201cFastpitch,\u201d https://github.com/\nNVIDIA/DeepLearningExamples/tree/\nmaster/PyTorch/SpeechSynthesis/\nFastPitch, 2023, Accessed: September 3, 2023.\n[19] Diederik P. Kingma and Jimmy Ba, \u201cAdam: A method\nfor stochastic optimization,\u201d in 3rd International Con-\nference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Pro-\nceedings, Yoshua Bengio and Yann LeCun, Eds., 2015.\n[20] Xiangyi Chen, Steven Z Wu, and Mingyi Hong, \u201cUnder-\nstanding gradient clipping in private sgd: A geometric\nperspective,\u201d Advances in Neural Information Process-\ning Systems, vol. 33, pp. 13773\u201313782, 2020.\n"
  },
  {
    "title": "CoMoSVC: Consistency Model-based Singing Voice Conversion",
    "link": "https://arxiv.org/pdf/2401.01792.pdf",
    "upvote": "6",
    "text": "CoMoSVC: Consistency Model-based Singing Voice\nConversion\nYiwen Lu1, Zhen Ye1, Wei Xue1\u2020, Xu Tan2, Qifeng Liu1, Yike Guo1\u2020\n1 Hong Kong University of Science and Technology 2 Microsoft Research Asia \u2217\nAbstract\nThe diffusion-based Singing Voice Conversion (SVC) methods have achieved\nremarkable performances, producing natural audios with high similarity to the\ntarget timbre. However, the iterative sampling process results in slow inference\nspeed, and acceleration thus becomes crucial. In this paper, we propose Co-\nMoSVC, a consistency model-based SVC method, which aims to achieve both\nhigh-quality generation and high-speed sampling. A diffusion-based teacher model\nis first specially designed for SVC, and a student model is further distilled under\nself-consistency properties to achieve one-step sampling. Experiments on a sin-\ngle NVIDIA GTX4090 GPU reveal that although CoMoSVC has a significantly\nfaster inference speed than the state-of-the-art (SOTA) diffusion-based SVC sys-\ntem, it still achieves comparable or superior conversion performance based on\nboth subjective and objective metrics. Audio samples and codes are available at\nhttps://comosvc.github.io/.\nKeywords: Singing Voice Conversion, Diffusion Model, Consistency Model\n1\nIntroduction\nSinging Voice Conversion(SVC) aims to convert one singer\u2019s voice to another one\u2019s, while preserving\nthe content and melody. It has wide applications in music entertainment, singing voice beautification,\nand art creation Zhang et al. [2023].\nStatistical methods Kobayashi et al. [2014, 2015b,a] are applied to the SVC tasks with parallel training\ndata from both the source and target singers, which is usually infeasible, and thus the non-parallel\nSVC methods have become the mainstream. Two-stage methods are generally used for SVC, the first\nstage disentangles and encodes singer-independent and singer-dependent features from the audio.\nThen the second decoding stage generates the converted audio by replacing the singer-dependent\nfeature with the target one. Since the substantial impact of the second stage on the quality of the\nconverted audios, it has become crucial to design and optimize this stage. Therefore, many generative\nmodels have been used for the SVC decoding, including the autoregressive (AR) models, generative\nadversarial network (GAN), Normalizing Flow, and diffusion models. AR models are firstly used\nto develop USVC Nachmani and Wolf [2019], and PitchNet Deng et al. [2020] further improves\nUSVC by adding a pitch adversarial network to learn the joint phonetic and pitch representation.\nHowever, AR models are slow due to the recursive nature, then non-AR GAN-based UCD-SVC\nPolyak et al. [2020] and FastSVC Liu et al. [2021a] are later proposed. Since the unstable training\nof GAN, a flow-based end-to-end SVC system, named SoVITS-SVC SVC-Develop-Team [2023]\nreceived widespread attention for its excellent converted results in fast speed. Recently, it has been\nshown that the conversion performance can be substantially improved by the diffusion-based SVC\nmethods such as DiffSVC Liu et al. [2021b] and the diffusion version of SoVITS-SVC.\n\u2217\u2020Corresponding authors: Wei Xue {weixue@ust.hk}, Yike Guo {yikeguo@ust.hk}\nUnder Review.\narXiv:2401.01792v1  [eess.AS]  3 Jan 2024\nHowever, the iterative sampling process results to the slow inference of the diffusion-based SVC\nmethods. A new generative model named consistency model Song et al. [2023] has been proposed to\nrealize one-step generation. Subsequently for speech synthesis, CoMoSpeech Ye et al. [2023] exploits\nthe consistency model to achieve both high-quality synthesis and fast inference speed. Inspired\nby this, a consistency model-based SVC method, named CoMoSVC, is further developed in this\npaper to achieve high-quality, high-similarity and high-speed SVC. Based on the structure of EDM\nKarras et al. [2022], a diffusion-based teacher model with outstanding generative capability is firstly\ndesigned, and a student model is further distilled from it to achieve one-step sampling. Experiments\nreveal that while the sampling speed of CoMoSVC is approximately 500 and 50 times faster than that\nof the diffusion-based SoVITS-SVC and DiffSVC respectively, the comparable performance is still\nretained and some improvements can even be achieved in both quality and similarity.\n2\nBackground\nThe diffusion model generates samples by first adding noise to data during the forward process\nand then reconstructing the data structure in the reverse process. We assume that the original data\ndistribution is pdata(x), and the forward process can be represented by a stochastic differential\nequation (SDE) Song et al. [2021], Karras et al. [2022]:\ndxt = f(xt, t)dt + g(t)dwt,\n(1)\nwhere wt is the standard wiener process, f(xt, t) and g(t) are drift and diffusion coefficients respec-\ntively. With setting f(xt, t) = 0 and g(t) =\n\u221a\n2t, the same as the choice in Karras et al. [2022], the\nSDE can be defined by:\ndxt =\n\u221a\n2t dwt.\n(2)\nThe reverse process can also be expressed by a reverse-time SDE Song et al. [2021]:\ndxt = \u22122t\u2207log pt(xt)dt +\n\u221a\n2t d \u00afwt,\n(3)\nwhere pt(xt) is the distribution of xt, \u2207log pt(xt) is the score function, and \u00afwt is the reverse-time\nstandard wiener process. Song et al. [2021] found that there exists a probability flow (PF) ordinary\ndifferential equation (ODE), whose solution trajectories\u2019 distribution at time t is the same as pt(xt).\nThe PF ODE with such property can be represented by\ndxt\ndt = \u2212t\u2207 log pt (xt) = xt \u2212 D\u03d5 (xt, t)\nt\n,\n(4)\nwhere D\u03d5 is the neural network with \u03d5 as parameters to approximate the denoiser function. Then for\nsampling, the PF ODE is solved by initiating from xT , as\nx0 = xT +\nZ 0\nT\nxt \u2212 D\u03d5 (xt, t)\nt\ndt.\n(5)\nHowever, the diffusion model generally needs a large number of iterations to solve the PF ODE,\nmaking the sampling slow. A consistency model Song et al. [2023] is proposed for one-step sampling\nbased on the self-consistency property, making any point from the same PF ODE trajectory be mapped\nto the same initial point. The self-consistency properties have two constraints: firstly, any pair of\npoints xtm and xtn will be mapped to the same point, which can be represented by:\nD\u03d5(xtm, tm) = D\u03d5(xtn, tn).\n(6)\nSecondly, the initial point should also be mapped to itself and this constraint is called the boundary\ncondition. To avoid numerical instability, it can be given by\nD\u03d5(x\u03f5, t\u03f5) = x\u03f5,\n(7)\nwhere \u03f5 is a fixed small positive number and set as 0.002. , all the singers have their identification\nnumber, which will be encoded as a singer embedding.\n3\nProposed Method\nCoMoSVC is a two-stage model, where the first stage encodes the extracted features and the singer\nidentity into embeddings. These embeddings are concatenated and serve as the conditional input\nfor the second stage to generate mel-spectrogram, which can be further rendered to audio by using\na pre-trained vocoder. The training process depicted in Fig. 1 takes the waveform and its singer\nidentity as the input to reconstruct the mel-spectrogram, while the inference process illustrated in\nFig. 2 replaces the singer identity with the target one to generate the converted mel-spectrogram.\n2\n\ud835\udc31!!\"# = \ud835\udc31\" + t#$% \u2217 \ud835\udc41(0, I)\n+\ud835\udc31!!\n\u2205 estimated from \ud835\udc31!!\"#\n\ud835\udc37'$(+\ud835\udc31!!\n\u2205 , \ud835\udc61#, \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc51)\n\ud835\udc37'(\ud835\udc31!!\"#, \ud835\udc61#$%, \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc51)\nTeacher Model\nCoMoSVC\nConsistency \nLoss\nLoudness\nWaveform\nSinger \nEmbedding\n\ud835\udc31!!\"# = \ud835\udc31\" + t#$% \u2217 \ud835\udc41(0, I)\n\ud835\udc37\u2205(\ud835\udc31#, \ud835\udc61, \ud835\udc50\ud835\udc5c\ud835\udc5b\ud835\udc51)\nTeacher Model\nln(\ud835\udc61)~\ud835\udc41(\ud835\udc43()*#, \ud835\udc43+!,)\n\ud835\udc31# = \ud835\udc31\" +\ud835\udc41(0, \ud835\udc61-\ud835\udc3c)\nReconstruction \nLoss\nGround  Truth \ud835\udc31\"\ncond\nEncoding\nDecoding\nTraining of Teacher Model\nConsistency Distillation\nPitch \nContent\nSinger ID\nFigure 1: The Training Process.\n3.1\nEncoding\nThis section encodes both singer-independent and singer-dependent features, which can be shown\nin the upper part of both Fig. 1 and Fig. 2. We extract content, pitch, and loudness features to\ncapture singer-independent information in audio, while the singer ID is used as the singer-dependent\ninformation. The content features are extracted by using the pre-trained acoustic model ContentVec\nQian et al. [2022] and the large dimensionality of these features allows for enhancing the clarity of\nlyrics in the converted audio. To represent pitch information, we use the widely-used and classical F0\nestimator DIO Morise et al. [2009]. The squared magnitude of the audio signal is calculated as the\nloudness feature. After feature extraction, we applied a linear layer to all the embeddings to unify the\ndimensions and concatenate them to form the conditional input for the decoding stage.\n3.2\nDecoding\nThis stage is the key component of CoMoSVC, during which the mel-spectrograms can be generated\nfrom the conditional input. A teacher model is first trained and then a student model is distilled from\nit, which will be introduced in section. 3.2.1 and section. 3.2.2 respectively. The sampling process of\nboth the teacher model and student model will be explained in section. 3.2.3.\n3\nCoMoSVC\nLoudness\nWaveform\nTarget Singer \nEmbedding\nTeacher Model\ncond\nEncoding\nDecoding\nSampling of Teacher Model\nSampling of Student Model\nPitch \nContent\n\ud835\udc31!! = t\" \u2217 \ud835\udc41(0, I)\nN steps\n\u2026\nOne \nstep\n\ud835\udc31!! = t\" \u2217 \ud835\udc41(0, I)\nTarget Singer ID\nFigure 2: The inference process.\n3.2.1\nTeacher Model\nWe use the architecture of EDM Karras et al. [2022] as the teacher model to train the denoiser\nfunction D\u03d5 due to its high generative ability. Moreover, the structure of D\u03d5 used here is the non-\ncausal Wavenet Rethage et al. [2018]. We use x0 \u223c pdata(x) and cond to denote the ground truth\nmel-spectrogram and the conditional input. According to (4), the empirical ODE is\ndxt\ndt = xt \u2212 D\u03d5 (xt, t, cond)\nt\n,\n(8)\nwhere xt = x0 + t \u2217 N(0, I), represents the result after adding noise. Similar to Karras et al. [2022],\nwe use a different network F\u03d5 instead of directly approximating the denoiser function by D\u03d5. The\nnetwork is preconditioned with a skip connection to make the estimation more flexible, it can be\ngiven by\nD\u03d5(xt, t, cond) = cskip (t)xt + cout(t)F\u03d5 (cin (t)xt, t, cnoise (t)) .\n(9)\ncskip(t) modulates the skip connection, cin (t) and cout (t) scale the magnitudes of xt and F\u03d5 respec-\ntively, and cnoise (t) maps noise level t into a conditioning input for F\u03d5. To satisfy the boundary\ncondition mentioned in (7) and ensure cskip (t) and cout (t) differential, we choose\ncskip (t) =\n\u03c32\ndata\n(t \u2212 \u03f5)2 + \u03c32\ndata\n,\ncout (t) = \u03c3data (t \u2212 \u03f5)\np\n\u03c32\ndata + t2 ,\n(10)\n4\nAlgorithm 1 Training procedure\nInput: The denoiser function D\u03d5 of the teacher model; the conditional input cond; the original\ndata distribution pdata and \u00b5\n1: repeat\n2: Sample n \u223c U(1, N \u2212 1) and x0 \u223c pdata\n3: Sample xtn+1 \u223c N(x0, (tn+1)2 \u2217 I)\n4: \u02c6x\u03d5\ntn \u2190\ntn\ntn+1 xtn+1 + tn+1\u2212tn\ntn+1\nD\u03d5\n\u0000xtn+1, tn+1, cond\n\u0001\n5: L\u03b8 \u2190 d\n\u0010\nD\u03b8\n\u0000xtn+1, tn+1, cond\n\u0001\n, D\u03b8\u2212\n\u0010\n\u02c6x\u03d5\ntn, tn, cond\n\u0011\u0011\n6: \u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L (\u03b8, \u03b8\u2212; \u03d5)\n7: \u03b8\u2212 \u2190 stopgrad (\u00b5\u03b8\u2212 + (1 \u2212 \u00b5)\u03b8)\n8: until convergence\nAlgorithm 2 Sampling procedure\nInput: The denoiser function D\u03b8 of the consistency model; the conditional input cond ; a set of\ntime points ti\u2208{0,...,N}\n1: Sample xN \u223c N(0, \u03c3(tN)2 \u2217 I)\n2: x \u2190 D\u03b8(xN, tN, cond)\n3: if one-step synthesis\n4:\nOutput: x\n5: else multi-step synthesis\n6:\nfor i = N \u2212 1 to 1 do\n7:\nSample z \u223c N(0, I)\n8:\nxi \u2190 x +\np\nt2\ni \u2212 \u03f52z\n9:\nx \u2190 D\u03b8(xi, ti, cond)\n10:\nend for\nOutput: x\nwhere \u03c3data is the standard deviation of pdata(x). The loss function L\u03d5 used to train the D\u03d5 can be\ndesigned by\nL\u03d5 = E[\u03bb(t)\u2225D\u03d5 (xt, t, cond) \u2212 x0\u22252],\n(11)\nwhere \u03bb(t) = (t2 + \u03c32\ndata )/(t \u00b7 \u03c3data )2, denotes the weight corresponding to different noise level t.\nThe entire procedure is depicted in the lower left section in Fig. 1.\n3.2.2\nConsistency Distillation\nA student model can be further distilled from the pre-trained denoiser function D\u03d5 to ultimately\nachieve one-step sampling, the process is illustrated in Algorithm. 1 and the lower right section of\nFig. 1. First, we randomly sample n from the uniform distribution U(1, N \u2212 1) and obtain xtn+1\nby adding tn+1 \u2217 N(0, I) to x0, then we use the D\u03d5 to get the one-step estimation \u02c6x\u03d5\ntn from xtn+1.\nAccording to (4), since first-order Euler Solver is used here, it can be given by\n\u02c6x\u03d5\ntn =\ntn\ntn+1\nxtn+1 + tn+1 \u2212 tn\ntn+1\nD\u03d5\n\u0000xtn+1, tn+1, cond\n\u0001\n.\n(12)\nThe structure of the student model is inherited from the teacher model\u2019s denoiser function D\u03d5,\nresulting in D\u03b8 and D\u03b8\u2212. The parameters \u03b8 and \u03b8\u2212 are initialized with \u03d5, \u03b8\u2212 is a running average of\nthe past values of \u03b8. Afterwards, we use D\u03b8\u2212\n\u0010\n\u02c6x\u03d5\ntn, tn, cond\n\u0011\nand D\u03b8\n\u0000xtn+1, tn+1, cond\n\u0001\nto obtain\ndifferent outputs of the pair of adjacent points \u02c6x\u03d5\ntn and xtn. The consistency distillation is trained by\nminimizing the L2 distance between the two outputs:\nd\n\u0010\nD\u03b8\n\u0000xtn+1, tn+1, cond\n\u0001\n, D\u03b8\u2212\n\u0010\n\u02c6x\u03d5\ntn, tn, cond\n\u0011\u0011\n= \u2225D\u03b8\n\u0000xtn+1, tn+1, cond\n\u0001\n\u2212 D\u03b8\u2212\n\u0010\n\u02c6x\u03d5\ntn, tn, cond\n\u0011\n\u22252.\n(13)\nThe parameter \u03b8 is updated by:\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L (\u03b8, \u03b8\u2212; \u03d5) .\n(14)\n5\nTo stabilize the training, the exponential moving average (EMA) update and stop grad are adopted to\n\u03b8\u2212, as:\n\u03b8\u2212 \u2190 stopgrad\n\u0000\u00b5\u03b8\u2212 + (1 \u2212 \u00b5)\u03b8\n\u0001\n,\n(15)\nwhere \u00b5 is a momentum coefficient, empirically set as 0.95.\n3.2.3\nSampling Process\nThe sampling processes of both the two models are depicted in the lower part of Fig. 2. The teacher\nmodel takes a number of iterations for sampling, while the student model can achieve one-step\nsampling as summarized in Algorithm. 2. We first sample the noise that has the same shape as the\nmel-spectrogram by xtN = tN \u2217N(0, I), and the output of D\u03b8(xtN , tN, cond) is the sampling result.\nThe proposed CoMoSVC also supports multi-step sampling by chaining the outputs at multiple time\nsteps. However, there will be a trade-off between the number of iterations and sampling quality.\n4\nExperiments\n4.1\nExperimental Setup\nWe conduct our experiment on two open-source datasets, which are M4Singer Zhang et al. [2022] and\nOpenSinger Huang et al. [2021], respectively. The former dataset has 29.77 hours of singing voice\nand 20 singers, and the latter one contains 50 hours and 66 singers. All the audios are resampled\nto 24kHz and normalized. Then we calculate the volume feature, extract the F0 curve along with\nthe voiced/unvoiced flag for each frame by using DIO Morise et al. [2009] and the 768-dimensional\ncontent feature from the 12th layer by utilizing ContentVec Qian et al. [2022]. All these features\nare projected to 256 dimensions and then concatenated as the conditional input for the decoding\nstage. We use the vocoder2 pre-trained with singing voice from M4singer Zhang et al. [2022], and\nthe mel-spectrograms are computed with 512-point fast Fourier transform (FFT), 512-point window\nsize and 128-point hop size with 80 frequency bins.\nAll the models are trained for 1 million iterations on a single NVIDIA GTX4090 GPU with a batch\nsize of 48, with learning rates as 1e-4 and 5e-5 respectively and the optimizer is AdamW.\nWe first conduct the reconstruction experiment to evaluate the capabilities of different decoding\nstages in the autoencoding settings. Then two sets of experiments are conducted for any-to-many\nSVC task, a) train on the OpenSinger dataset for the target singer and use M4singer as the source for\nconversion ; b) train on the M4singer dataset for the target singer and use OpenSinger as the source\nfor conversion. Moreover, we increase the sampling steps of CoMoSVC and conduct the conversion\nexperiments to evaluate the effect of sampling steps.\n4.2\nBaselines\nWe compare the proposed CoMoSVC with the SOTA SVC methods, including:\n\u2022 SoVITS-Flow: The flow version of SoVITS-SVC3.\n\u2022 SoVITS-Diff: The diffusion version of SoVITS-SVC4. The number of diffusion steps is\n1000.\n\u2022 DiffSVC Liu et al. [2021b]: The first SVC method based on the diffusion model, and the\nnumber of steps is 100.\nThe same feature embeddings and training details as described in Sec. 4.1 are used for the baseline\nmethods for fair comparison.\n4.3\nEvaluation\nWe evaluate the reconstruction ability of different methods by objective metrics and the conversion\nability by both subjective and objective metrics. For the subjective test, we invited 12 volunteers to\n2https://github.com/M4Singer/M4Singer/tree/master/code\n3https://github.com/svc-develop-team/so-vits-svc?tab=readme-ov-file#sovits-model\n4https://github.com/svc-develop-team/so-vits-svc?tab=readme-ov-file#diffusion-model-optional\n6\nTable 1: Objective Evaluations for Reconstruction\nMETHOD\nNFE(\u2193)\nFPC(\u2191)\nPESQ(\u2191)\nCER(\u2193)\nSIM(\u2191)\nSoVITS-Flow\n1\n0.935\n2.486\n25.03\n0.948\nSoVITS-Diff\n1000\n0.938\n2.826\n26.04\n0.970\nDiffSVC\n100\n0.941\n2.917\n25.47\n0.972\nTeacher\n50\n0.945\n2.967\n20.96\n0.982\nCoMoSVC\n1\n0.943\n2.948\n24.69\n0.970\nTable 2: Objective Evaluations for SVC, where \u201cM\u201d and \u201cO\u201d in the SVC setting row stands for\nM4Singer and OpenSinger, respectively.\nSVC SETTING\nM\u2192O\nO\u2192M\nMETHOD\nNFE(\u2193)\nSIM(\u2191)\nCER(\u2193)\nSIM(\u2191)\nCER(\u2193)\nSoVITS-Flow\n1\n0.784\n21.76\n0.585\n22.62\nSoVITS-Diff\n1000\n0.804\n20.40\n0.598\n25.50\nDiffSVC\n100\n0.801\n21.27\n0.598\n22.58\nTeacher\n50\n0.794\n20.52\n0.614\n19.57\nCoMoSVC\n1\n0.801\n21.49\n0.585\n19.76\ngive Mean Opinion Score (MOS) on naturalness and similarity on the converted audios. Real-Time\nFactor (RTF), Character Error Rate (CER) obtained by Whisper Radford et al. [2023] and speaker\nSIMilarity (SIM) calculated by the cosine distance between the speaker embeddings 5 are used\nas the objective metrics for SVC evaluation. Since the flow version SoVITS-SVC is end-to-end\nand the other methods are two-stage, we use the time ratio of transforming embeddings into latent\nrepresentations (Flow) / mel-spectrograms (Others) to the duration of audio to represent RTF for clear\ncomparison. For reconstruction, F0 Pearson correlation coefficient (FPC) 6 and PESQ [ITU-T] are\nused additionally for evaluation.\n4.3.1\nReconstruction\nAs illustrated in Table. 1, the teacher model outperforms all the models in all the metrics. Similarly,\nCoMoSVC outperforms all the baselines in all metrics except for similarity, where it achieves\ncomparable results. This indicates the outstanding generative ability of the decoding stage of\nCoMoSVC with only one step, which is hundreds or thousands of times fewer than all the baselines.\n4.3.2\nSVC Performances\nWe conduct two sets of SVC experiments as described in section. 4.1 and the source audios are all\nunseen during training. As illustrated by CER and SIM in Table. 2, CoMoSVC performs comparably\nto all the baselines. The subjective evaluations in Table. 3 reveal that CoMoSVC achieves comparable\nnaturalness to the diffusion-based SVC methods. Furthermore, the similarity of CoMoSVC exceeds\nthat of all the baselines in both experiments, demonstrating an improvement of at least 0.05 to\ndiffusion-based SVC methods. Moreover, both the naturalness and similarity of CoMoSVC show an\nincrement of approximately 1 compared to the flow version SoVITS-SVC. As to the inference speed,\nthe RTF of CoMoSVC is 0.002 smaller than that of the flow version SoVITS-SVC. In comparison\nwith the diffusion-based SVC methods, CoMoSVC is more than 45 times faster than DiffSVC and\nalmost 500 times faster than the diffusion-version SoVITS-SVC.\n5https://github.com/Jungjee/RawNet\n6https://github.com/open-mmlab/Amphion/blob/main/evaluation/metrics/f0/\n7\nTable 3: Subjective Evaluations for SVC, where, as an example, M4Singer\u2192OpenSinger means\nconverting the singing voices from the M4Singer to target timbres in the OpenSinger.\nSVC SETTING\nM4Singer\u2192OpenSinger\nOpenSinger\u2192M4Singer\nMETHOD\nNFE(\u2193)\nRTF(\u2193)\nMOS/N(\u2191)\nMOS/S(\u2191)\nMOS/N(\u2191)\nMOS/S(\u2191)\nSoVITS-Flow\n1\n0.008\n3.27 \u00b1 0.21\n3.03 \u00b1 0.23\n3.10 \u00b1 0.22\n2.90 \u00b1 0.23\nSoVITS-Diff\n1000\n2.978\n4.43 \u00b1 0.14\n3.90 \u00b1 0.19\n4.32 \u00b1 0.15\n3.99 \u00b1 0.19\nDiffSVC\n100\n0.278\n4.44 \u00b1 0.14\n3.91 \u00b1 0.19\n4.23 \u00b1 0.19\n3.95 \u00b1 0.21\nTeacher\n50\n0.148\n4.43 \u00b1 0.15\n3.92 \u00b1 0.19\n4.47 \u00b1 0.14\n4.05 \u00b1 0.18\nCoMoSVC\n1\n0.006\n4.42 \u00b1 0.13\n3.96 \u00b1 0.19\n4.27 \u00b1 0.16\n4.00 \u00b1 0.19\nTable 4: Evaluations for Effect of Sampling Steps, where the number in the method name represents\nthe number of sampling steps.\nSVC SETTING\nM\u2192O\nMETHOD\nMOS/N(\u2191)\nMOS/S(\u2191)\nCoMoSVC-4\n4.46 \u00b1 0.14\n3.95 \u00b1 0.19\nCoMoSVC-2\n4.36 \u00b1 0.14\n3.88 \u00b1 0.19\nCoMoSVC-1\n4.42 \u00b1 0.13\n3.96 \u00b1 0.19\nSVC SETTING\nO\u2192M\nMETHOD\nMOS/N(\u2191)\nMOS/S(\u2191)\nCoMoSVC-4\n4.38 \u00b1 0.15\n4.05 \u00b1 0.19\nCoMoSVC-2\n4.34 \u00b1 0.15\n4.01 \u00b1 0.19\nCoMoSVC-1\n4.27 \u00b1 0.16\n4.00 \u00b1 0.19\n4.3.3\nEffect of Sampling Steps\nIn general, as the number of sampling steps increases, there is a small increment in the metrics\npresented in Table 4. The slight improvement and minor fluctuation indicates that CoMoSVC already\nachieves accurate score estimation through only one-step discretization yielding high-quality results.\n5\nConclusion\nIn this paper, we propose the CoMoSVC, which is based on the consistency model to achieve\nhigh-quality, high-similarity and high-speed SVC. The proposed CoMoSVC is a two-stage model\nwhere the first stage encodes the features from the waveform, then the second stage utilizes a student\nmodel distilled from a pre-trained teacher model to generate converted audios. The comprehensive\nsubjective and objective evaluations demonstrate the effectiveness of CoMoSVC.\nAcknowledgments\nThe research was supported by the Theme-based Research Scheme (T45-205/21-N) and Early Career\nScheme (ECS-HKUST22201322), Research Grants Council of Hong Kong.\nReferences\nChengqi Deng, Chengzhu Yu, Heng Lu, Chao Weng, and Dong Yu. Pitchnet: Unsupervised singing\nvoice conversion with pitch adversarial network. In Proc. Intl. Conf. on Acoustics, Speech, and\nSignal Processing (ICASSP), 2020.\nRongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Multi-singer:\nFast multi-singer singing voice vocoder with a large-scale corpus. In Proc. ACM Int. Conf. on\nMultimedia (ACM MM), 2021.\n8\nIntl. Telecommunications Union (ITU-T). Perceptual evaluation of speech quality (PESQ), an\nobjective method for end-to-end speech quality assessment of narrowband telephone networks and\nspeech codecs. Recommendation P.862, Intl. Telecommunications Union (ITU-T), February 2001.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. In Proc. Conf. on Neural Information Processing Systems (NeurIPS),\n2022.\nKazuhiro Kobayashi, Tomoki Toda, Graham Neubig, Sakriani Sakti, and Satoshi Nakamura. Statistical\nsinging voice conversion with direct waveform modification based on the spectrum differential. In\nProc. InterSpeech, 2014.\nKazuhiro Kobayashi, Tomoki Toda, and Satoshi Nakamura. Statistical singing voice conversion\nbased on direct waveform modification and its parameter generation algorithms. IEICE Tech. Rep.,\n115(253):7\u201312, 2015a.\nKazuhiro Kobayashi, Tomoki Toda, Graham Neubig, Sakriani Sakti, and Satoshi Nakamura. Statistical\nsinging voice conversion based on direct waveform modification with global variance. In Proc.\nInterSpeech, 2015b.\nSongxiang Liu, Yuewen Cao, Na Hu, Dan Su, and Helen Meng. FastSVC: Fast cross-domain singing\nvoice conversion with feature-wise linear modulation. In Proc. Intl. Conf. Multimedia and Expo\n(ICME), 2021a.\nSongxiang Liu, Yuewen Cao, Dan Su, and Helen Meng. Diffsvc: A diffusion probabilistic model\nfor singing voice conversion. In Proc. IEEE Workshop on Automatic Speech Recognition and\nUnderstanding (ASRU), pages 741\u2013748, 2021b.\nMasanori Morise, Hideki Kawahara, and Haruhiro Katayose. Fast and reliable f0 estimation method\nbased on the period extraction of vocal fold vibration of singing voice and speech. In Audio\nEngineering Society Conference: 35th International Conference: Audio for Games, 2009.\nEliya Nachmani and Lior Wolf. Unsupervised singing voice conversion. In Proc. InterSpeech, 2019.\nAdam Polyak, Lior Wolf, Yossi Adi, and Yaniv Taigman. Unsupervised cross-domain singing voice\nconversion. In Proc. InterSpeech, 2020.\nKaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni, Cheng-I Lai, David Cox, Mark Hasegawa-Johnson,\nand Shiyu Chang. ContentVec: An improved self-supervised speech representation by disentangling\nspeakers. In Proc. Intl. Conf. Machine Learning (ICML), 2022.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In Proc. Intl. Conf. Machine Learning\n(ICML), 2023.\nDario Rethage, Jordi Pons, and Xavier Serra. A wavenet for speech denoising. In Proc. Intl. Conf. on\nAcoustics, Speech, and Signal Processing (ICASSP), 2018.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In Proc. Intl.\nConf. on Learning Representations (ICLR), 2021.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. In Proc. Intl.\nConf. Machine Learning (ICML), 2023.\nSVC-Develop-Team.\nSoftvc vits singing voice conversion.\nhttps://github.com/\nsvc-develop-team/so-vits-svc, 2023.\nZhen Ye, Wei Xue, Xu Tan, Jie Chen, Qifeng Liu, and Yike Guo. Comospeech: One-step speech\nand singing voice synthesis via consistency model. In Proc. ACM Int. Conf. on Multimedia (ACM\nMM), 2023.\n9\nLichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng, Jinglin Liu, Yi Ren, Jinzheng He, Rongjie\nHuang, Jieming Zhu, Xiao Chen, et al. M4singer: A multi-style, multi-singer and musical score\nprovided mandarin singing corpus. In Proc. Conf. on Neural Information Processing Systems\n(NeurIPS), 2022.\nXueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao Fang, Lexiao Zou, Liumeng Xue, and Zhizheng\nWu. Leveraging content-based features from multiple acoustic models for singing voice conversion.\narXiv preprint arXiv:2310.11160, 2023.\n10\n"
  },
  {
    "title": "WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope",
    "link": "https://arxiv.org/pdf/2401.01699.pdf",
    "upvote": "6",
    "text": "WordArt Designer API: User-Driven Artistic\nTypography Synthesis with Large Language Models on\nModelScope\nJun-Yan He1\u2217\nZhi-Qi Cheng2\u2020\nChenyang Li1\nJingdong Sun2\nWangmeng Xiang1\nYusen Hu2,5\nXianhui Lin1\nXiaoyang Kang1\nZengke Jin3,4\nBin Luo1\nYifeng Geng1\nXuansong Xie1\nJingren Zhou1\n1Alibaba DAMO Academy 2Carnegie Mellon University 3Zhejiang Sci-Tech University\n4Royal College of Art 5Imperial College London\nFigure 1: Annotated screenshot showcasing our system deployed on ModelScope. Four main services are\nprovided: (1) A \"one-stop shop\" that directly outputs artistic transformations based on the user input. (2)\nand (3) Fine-grained control of intermediate results from the SemTypo, StyTypo, and TexTypo modules. (4)\nPersonalized font generation using handwriting. You are welcome to explore more of WordART Designer at\nhttps://www.modelscope.cn/studios/WordArt/WordArt\n1\nIntroduction\nTypography, bridging language and design, is crucial in domains like e-commerce [2, 3, 4], educa-\ntion [10], and tourism [1]. However, creating artistic typography often poses a significant challenge\nfor those without professional training. To address this, we introduce WordArt Designer (Fig. 1), a\nuser-driven framework that synthesizes artistic typography using Large Language Models (LLMs).\nThis tool democratizes typography, making it accessible to non-experts without compromising on\naesthetics or functionality.\nThe WordArt Designer (Fig. 1) initiates with an LLM module interpreting user inputs, leading to\nan iterative design generation process. The workflow, illustrated in Appendix A, begins with the\nLLM module interpreting user input. This generation process is iterative, in that a quality assessment\nfeedback system automatically retries designs until at least a set number of successful transformations\nis produced, ensuring the creation of high-quality WordArt designs.\n\u2217 Project Leader, Alibaba Team\n\u2020 Co-Lead, CMU/RCA/ICL Team\nSpotlight Paper at the Workshop on Machine Learning for Creativity and Design, 37th Conference on Neural\nInformation Processing Systems (NeurIPS 2023).\narXiv:2401.01699v2  [cs.CV]  12 Jan 2024\nWordArt Designer [6] can be employed in various areas including media, advertisement, and product\ndesign, enhancing the efficiency and effectiveness of these systems, making them more practical\nfor everyday use. Our research not only contributes to the field of text synthesis but also opens up\nvarious practical applications.\nFor additional examples of typographies created by WordArt, see Appendix B. We also invite you\nto explore the capabilities of WordArt Designer further at ModelScope WordArt Studio (https:\n//www.modelscope.cn/studios/WordArt/WordArt).\n2\nMethod\nTechnical Details. We elaborate on the technical underpinnings of WordArt Designer, as introduced\nin our previous work [6]. The system is anchored by three key typography synthesis modules and\nis powered by a Large Language Model (LLM), such as GPT-3.5, to enable an interactive and\nuser-centric design experience.\nIn the system\u2019s workflow, depicted in Fig. 2, users commence by specifying their design requirements,\nwhich may range from conceptual themes to specific domains, such as \"A cat in jewelry design.\"\nThe LLM engine processes this input, converting it into structured prompts that guide the SemTypo,\nStyTypo, and TexTypo modules in realizing the user\u2019s artistic vision. The LLM adeptly translates\nfree-form user inputs into precise instructions for these modules to create the desired artistic fonts.\nThe Semantic Typography (SemTypo) module then manipulates typography based on the specified\nsemantic concept (e.g., \"Jewelry\"). This process involves three key stages: (1) Character Extraction\nand Parameterization, utilizing FreeType [5], (2) Selection of Regions for Transformation, and (3)\nExecution of Differentiable Rasterization [8]. Following this, the Stylization Typography (StyTypo)\nmodule applies additional style enhancements, leveraging the Depth2Image technique from the Latent\nDiffusion Model [9]. It also incorporates a ResNet [7] trained on our bespoke character dataset [6] for\nranking and selecting the most successful stylizations, as further detailed below. Finally, the Texture\nTypography (TexTypo) module, derived from ControlNet [11], is responsible for producing the final\ntextured font images.\nBesides, the WordArt Designer framework has a quality assessment feedback mechanism that\nensures high-quality WordArt designs. This mechanism guarantees a minimum of K successful\ntransformations through the ranking model, as shown in Fig. 2. If the system fails to meet this\nthreshold, it automatically initiates a new iteration. The new iteration involves the LLM engine, the\nSemTypo and StyTypo modules, and the format directives.\nWordART Designer API on ModelScope. The WordART Designer API serves as a dynamic tool for\nartistically transforming text. The process begins with users inputting text they wish to aesthetically\nenhance. Following this, they provide detailed specifications to guide the deformation and textural\nstylization, thus tailoring the design to meet specific aesthetic goals. The LLM engine plays a\ncrucial role in this phase, generating stylization heuristics based on the textual input, which lays the\ngroundwork for the initial design concept. During the synthesis phase, which lasts approximately\none minute, the system produces four distinct design variations, offering users a spectrum of stylistic\nchoices. The Font Deformer service, utilizing the capabilities of the SemTypo and StyTypo modules,\nthen allows users to modify the form and style of the text, excluding textural elements. Once a\ndesirable stylization is achieved, users can employ the Font Texturizer to apply intricate textural\n(graphical) details to the stylized image.\n3\nEvaluation\nSince its launch, WordART Designer\u2019s integration within ModelScope has garnered significant\nattention, evidenced by 146,714 visits. This substantial engagement has provided a wealth of user\nfeedback, crucial for the ongoing refinement of our service. In response to this feedback, we are\nconsidering several enhancements for the near future, including adjustable character spacing, selective\nbackground removal, and the capability for direct image exports.\nDespite these areas for improvement, the ability of WordART Designer to produce rich and aestheti-\ncally appealing typographies has been widely acknowledged. The system is currently implemented on\nproduction platforms, notably on ModelScope Studio (https://www.modelscope.cn/studios/\n2\nWordArt/WordArt), and is utilized internally by various corporations, including Alibaba Group.\nThis widespread adoption underscores the practical utility and appeal of the tool in professional\nsettings. For a more comprehensive understanding of WordART Designer\u2019s versatility and the range\nof textures it can generate (e.g., organic versus metallic for contexts like food and jewelry), readers\nare encouraged to refer to Appendix B. These examples showcase the adaptability of WordART\nDesigner in creating context-specific typographies, further illustrating the system\u2019s capabilities.\n4\nEthical Implications\nThe deployment of WordART Designer raises important ethical considerations. 1) A primary concern\nis the risk of perpetuating cultural stereotypes, as the tool might favor dominant narratives, potentially\nleading to the marginalization of under-represented cultures. Ensuring diversity in the dataset and\nalgorithmic checks for bias is crucial to mitigate this issue. 2) Another concern is the inadvertent\nuse of copyrighted material in designs. This necessitates the integration of copyright detection\nmechanisms and clear user guidelines to prevent intellectual property infringement. 3) Additionally,\nthe impact of AI on traditional artistic roles must be considered. While enhancing design capabilities,\nAI tools like WordART Designer could challenge the value of human artistry, prompting a reevaluation\nof AI\u2019s role in creative industries. 4) Lastly, user privacy and data security are essential, especially\ngiven the personal nature of the designs. Adhering to strict data protection standards is imperative to\nensure user trust and system integrity. Addressing these ethical concerns requires ongoing efforts,\nincluding user education, ethical guideline updates, and collaboration with diverse stakeholders to\nenable responsible and respectful use of WordART Designer.\nReferences\n[1]\nJennifer Amar, Olivier Droulers, and Patrick Legoh\u00e9rel. \u201cTypography in destination advertising:\nAn exploratory study and research perspectives\u201d. In: Tourism Management 63 (2017), pp. 77\u2013\n86. ISSN: 0261-5177. DOI: https://doi.org/10.1016/j.tourman.2017.06.002. URL:\nhttps://www.sciencedirect.com/science/article/pii/S0261517717301243.\n[2]\nZhi-Qi Cheng et al. \u201cVideo ecommerce: Towards online video advertising\u201d. In: Proceedings of\nthe 24th ACM international conference on Multimedia. 2016, pp. 1365\u20131374.\n[3]\nZhi-Qi Cheng et al. \u201cVideo ecommerce++: Toward large scale online video advertising\u201d. In:\nIEEE transactions on multimedia 19.6 (2017), pp. 1170\u20131183.\n[4]\nZhi-Qi Cheng et al. \u201cVideo2shop: Exact matching clothes in videos to online shopping images\u201d.\nIn: Proceedings of the IEEE conference on computer vision and pattern recognition. 2017,\npp. 4048\u20134056.\n[5]\nDavid Turner, Robert Wilhelm, and Werner Lemberg. FreeType 2. FreeType, 1996. URL:\nhttps://freetype.org/index.html (visited on 07/24/2023).\n[6]\nJun-Yan He et al. \u201cWordArt Designer: User-Driven Artistic Typography Synthesis using Large\nLanguage Models\u201d. In: Proceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing. 2023.\n[7]\nKaiming He et al. \u201cDeep Residual Learning for Image Recognition\u201d. In: 2016 IEEE Conference\non Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30,\n2016. 2016, pp. 770\u2013778.\n[8]\nTzu-Mao Li et al. \u201cDifferentiable vector graphics rasterization for editing and learning\u201d. In:\nSIGGRAPH 39.6 (2020), 193:1\u2013193:15.\n[9]\nRobin Rombach et al. \u201cHigh-Resolution Image Synthesis With Latent Diffusion Models\u201d. In:\nCVPR. 2022, pp. 10684\u201310695.\n[10]\nSompatu Vungthong, Emilia Djonov, and Jane Torr. \u201cImages as a Resource for Supporting\nVocabulary Learning: A Multimodal Analysis of Thai EFL Tablet Apps for Primary School\nChildren\u201d. In: TESOL Quarterly 51.1 (2017), pp. 32\u201358. DOI: https://doi.org/10.1002/\ntesq.274. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/tesq.274.\nURL: https://onlinelibrary.wiley.com/doi/abs/10.1002/tesq.274.\n[11]\nLvmin Zhang and Maneesh Agrawala. \u201cAdding Conditional Control to Text-to-Image Diffusion\nModels\u201d. In: arXiv preprint abs/2302.05543 (2023).\n3\nA\nSystem Framework Diagram\n\ud835\udc78\ud835\udc8a\ud835\udc8f\ud835\udc91 : Design an art word of \u201c\u732b\u201d with \nEnglish concept \u201ccat\u201d  for jewelry design\nAinp : {\u201cchar\u201d: \u201c\u732b\u201d,  \u201cconcept\u201d: \n\u201ccat\u201d,  \u201capplication\u201d: \u201cjewelry\u201d}\nInitial Stroke SVG\nParameters \nSemTypo\nSemantic Typography \nImage Isem\nSemantic Typography Module \nRanking Model RM\nLLM Engine M\nA drawing of \u201ccat\u201d, smooth, realistic \u2026 \nFormatted Prompt From \ud835\udc68\ud835\udc8a\ud835\udc8f\ud835\udc91 :\n\ud835\udc78\ud835\udc94\ud835\udc95\ud835\udc9a : Please list the representative category or \nobject name  in/of \u201ccat\u201d  including in real-life, \nartist, and film works.\n\ud835\udc78\ud835\udc95\ud835\udc86\ud835\udc99 : Please list the representative category or \nobject name  in/of \u201cjewelry\u201d  including in real-life, \nartist, and film works\n\u2026\n\u2026\nStyTypo\nA black-and-white drawing of  \u201cHello kitty\u201d, \u2026 \nFormatted Prompt From \ud835\udc68\ud835\udc94\ud835\udc95\ud835\udc9a :\nTexTypo\n\u201cgold\u201d  texture, 8k \u2026 \nFormatted Prompt From \ud835\udc68\ud835\udc95\ud835\udc86\ud835\udc99 :\nStylization Typography \nModule \nTexture Typography Module \nStylization\nsuccessful or \niteration > K ?\nParsing \n&\n SemTypo\nCocreation \n&\n StyTypo\nCocreation \n& \nTexTypo\nOverall Flow\nYes\nNo\nAtex : [{\"Object/Category Name\": \" Gold\", \u2026},\n{\"Object/Category Name\": \" Jade\", \u2026},\n{\"Object/Category Name\": \"\u201dCrystal \",...}, \u2026\nLLM Engine M\n[{\"Object/Category Name\": \"Hello Kitty\", \u2026},\n{\"Object/Category Name\": \"Garfield\", \u2026},\n{\"Object/Category Name\": \"Tabby Cat,...}, \u2026\nAsty : \nIsem\n{Isty }\nOutput: {Itex }\n{Isty }\n{Isty }\nLLM Engine M\nIsem\nIsty \nItex \nInput Parsing\nStylization Prompt Synthesis\nTexture Prompt Synthesis\nFigure 2: WordART Designer\u2019s various components and how they interact with each other.\nB\nAdditional Examples of Fonts Generated by WordART Designer\nFigure 3: Typographies targeting fruits and foods.\n4\nFigure 4: Typographies targeting jewelry.\nFigure 5: Typographies targeting plants and vegetables.\n5\n"
  }
]