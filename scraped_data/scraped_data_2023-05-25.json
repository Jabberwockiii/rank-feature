[
  {
    "title": "Is GPT-4 a Good Data Analyst?",
    "link": "https://arxiv.org/pdf/2305.15038.pdf",
    "upvote": "2",
    "text": "Is GPT-4 a Good Data Analyst?\nLiying Cheng1,2\nXingxuan Li \u2217 1,3\nLidong Bing1,2\n1DAMO Academy, Alibaba Group, Singapore\n2Hupan Lab, 310023, Hangzhou, China\n3Nanyang Technological University, Singapore\n{liying.cheng, xingxuan.li, l.bing}@alibaba-inc.com\nAbstract\nAs large language models (LLMs) have demon-\nstrated their powerful capabilities in plenty of\ndomains and tasks, including context under-\nstanding, code generation, language generation,\ndata storytelling, etc., many data analysts may\nraise concerns if their jobs will be replaced\nby artificial intelligence (AI). This controver-\nsial topic has drawn great attention in pub-\nlic. However, we are still at a stage of diver-\ngent opinions without any definitive conclusion.\nMotivated by this, we raise the research ques-\ntion of \u201cis GPT-4 a good data analyst?\u201d in\nthis work and aim to answer it by conducting\nhead-to-head comparative studies. In detail,\nwe regard GPT-4 as a data analyst to perform\nend-to-end data analysis with databases from\na wide range of domains. We propose a frame-\nwork to tackle the problems by carefully de-\nsigning the prompts for GPT-4 to conduct ex-\nperiments. We also design several task-specific\nevaluation metrics to systematically compare\nthe performances between several professional\nhuman data analysts and GPT-4. Experimental\nresults show that GPT-4 can achieve compa-\nrable performance to humans. We also pro-\nvide in-depth discussions about our results to\nshed light on further studies before reaching\nthe conclusion that GPT-4 can replace data an-\nalysts. Our code, data and demo are available\nat:\nhttps://github.com/DAMO-NLP-SG/\nGPT4-as-DataAnalyst.\n1\nIntroduction\nLarge language models (LLMs) such as OpenAI\u2019s\nGPT series have shown their strong abilities on var-\nious tasks in the natural language processing (NLP)\ncommunity, including data annotator (Ding et al.,\n2023), data evaluator (Chiang and Lee, 2023; Luo\net al., 2023; Wang et al., 2023; Wu et al., 2023b),\netc. Beyond NLP tasks, researchers also evaluate\nthe LLM abilities in multiple domains, such as\n\u2217Xingxuan Li is under the Joint Ph.D. Program between\nAlibaba and Nanyang Technological University.\nQuestion\nRaw Data\nSchema\nfigure.pdf\ndata.txt\nAnalysis\nDatabase\nCode\nCode Execution\nOptional Input\nCompulsory Input\nExternal\nKnowledge\nOnline\nInformation\nRetrieval\nOutput\nFigure 1: A figure showing the flow of our proposed\nframework regarding GPT-4 as a data analyst. The com-\npulsory input information containing both the business\nquestion and the database is illustrated in the blue box\non the upper right. The optional input referring to the\nexternal knowledge source is circled in the red dotted\nbox on the upper left. The outputs including the ex-\ntracted data (i.e., \u201cdata.txt\u201d), the data visualization (i.e.,\n\u201cfigure.pdf\u201d) and the analysis are circled in the green box\nat the bottom.\nfinance (Wu et al., 2023c), healthcare (Han et al.,\n2023; Li et al., 2023b), biology (Zheng et al., 2023),\nlaw (Sun, 2023), psychology (Li et al., 2023a), etc.\nMost of these researches demonstrate the effec-\ntiveness of LLMs when applying them to different\ntasks. However, the strong ability in understand-\ning, reasoning, and creativity causes some potential\nanxiety among certain groups of people.\nAs LLMs are introduced and becoming popular\nnot only in the NLP community but also in many\nother areas, those people in and outside of the NLP\ncommunity are considering or worrying whether\nartificial intelligence (AI) can replace certain jobs\n(Noever and Ciolino, 2023; Wu et al., 2023a). One\nsuch job role that could be naturally and controver-\nsially \u201creplaced\u201d by AI is data analyst (Tsai et al.,\n2015; Ribeiro et al., 2015). The main and typical\njob scopes for a data analyst include extracting rele-\nvant data from several databases based on business\npartners\u2019 requirements, presenting data visualiza-\ntion in an easily understandable way, and also pro-\narXiv:2305.15038v2  [cs.CL]  23 Oct 2023\nviding data analysis and insights for the audience.\nThis job involves a relatively routine scope, which\nmay become repetitive at times. It also requires\nseveral technical skills, including but not limited to\nSQL, Python, data visualization, and data analysis,\nmaking it relatively expensive. As this job scope\nmay adhere to a relatively fixed pipeline, there is a\nheated public debate about the possibility of an AI\ntool replacing a data analyst, which attracts signifi-\ncant attention.\nIn this paper, we aim to answer the following re-\nsearch question: Is GPT-4 a good data analyst? To\nanswer this question, we conduct preliminary stud-\nies on GPT-4 to demonstrate its potential capabili-\nties as a data analyst. We quantitatively evaluate the\npros and cons of LLM as a data analyst mainly from\nthe following metrics: performance, time, and cost.\nSpecifically, we treat GPT-4 (gpt-4-0314)1 as\na data analyst to conduct several end-to-end data\nanalysis problems. The flow of our proposed frame-\nwork is shown in Figure 1. According to the given\nquestion, the model has to identify the relevant\ntables and schemes in the databases that contain\nthe necessary data, and then extract the data from\nthe databases and organize it in a way that is suit-\nable for figure generation. Then, it is required to\nanalyze the data to identify trends, patterns, and\ninsights that can help answer the initial question.\nSince there is no existing dataset for such data\nanalysis problems, we choose one of the most re-\nlated datasets NvBench (Luo et al., 2021) , and add\nthe data analysis part on top. We design several\nautomatic and human evaluation metrics to compre-\nhensively evaluate the quality of the data extracted,\ncharts plotted and data analysis generated.\nExperimental results show that GPT-4 can beat\nan entry-level data analyst and an intern data ana-\nlyst in terms of performance and have comparable\nperformance to a senior-level data analyst. In terms\nof the cost and time of our experiments, GPT-4\nis much cheaper and faster than hiring a data an-\nalyst. However, since it is a preliminary study on\nwhether GPT-4 is a good data analyst, we conduct\nsome additional experiments and provide fruitful\ndiscussions on whether the conclusions from our\nexperiments are reliable in real-life business from\nseveral perspectives, such as whether the questions\nare practical, whether the human data analysts we\nchoose are representative, etc. These results sug-\n1The most advanced model in the GPT series at the time\nof this paper was written.\ngest further studies are needed before concluding\nwhether GPT-4 is a good data analyst. To summa-\nrize, our contributions include:\n\u2022 We for the first time raise the research question\nof whether GPT-4 is a good data analyst, and\nquantitatively evaluate the pros and cons. How-\never, further research is still required to reach a\ndefinitive conclusion.\n\u2022 For such a typical data analyst job scope, we\npropose an end-to-end automatic framework\nto conduct data collection, visualization, and\nanalysis.\n\u2022 We conduct a systematic and professional hu-\nman evaluation of GPT-4\u2019s outputs. The data\nanalysis and insights with good quality can be\nconsidered as the first benchmark for data anal-\nysis in the NLP community.\n2\nRelated Work\n2.1\nRelated Tasks and Datasets\nSince our task setting is new in the NLP community,\nthere is no existing dataset that is entirely suitable\nfor our task. We explore the most relevant tasks\nand datasets. First, the NvBench dataset (Luo et al.,\n2021) translates natural language (NL) queries to\ncorresponding visualizations (VIS), which covers\nthe first half of the main job scope of a data ana-\nlyst. This dataset has 153 databases along with 780\ntables in total and covers 105 domains, and this\ntask (NL2VIS) has attracted significant attention\nfrom both commercial visualization vendors and\nacademic researchers. Another popular subtask of\nthe NL2VIS task is called text-to-SQL, which con-\nverts natural language questions into SQL queries\n(Zhong et al., 2017; Guo et al., 2019; Qi et al.,\n2022; Gao et al., 2022). Spider (Yu et al., 2018),\nSParC (Yu et al., 2019b) and CoSQL (Yu et al.,\n2019a) are three main benchmark datasets for text-\nto-SQL tasks. Since this work is more focused on\nimitating the overall process of the job scope of a\ndata analyst, we adopt the NL2VIS task which has\none more step forward than the text-to-SQL task.\nFor the second part of data analysis, we also ex-\nplore relevant tasks and datasets. Automatic chart\nsummarization (Mittal et al., 1998; Ferres et al.,\n2013) is a task that aims to explain a chart and\nsummarize the key takeaways in the form of natu-\nral language. Indeed, generating natural language\nsummaries from charts can be very helpful to in-\nfer key insights that would otherwise require a lot\nof cognitive and perceptual effort. In terms of the\ndataset, the chart-to-text dataset (Kantharaj et al.,\n2022) aims to generate a short description of the\ngiven chart. This dataset also covers a wide range\nof topics and chart types. Another relevant NLP\ntask is called data-to-text generation (Gardent et al.,\n2017; Du\u0161ek et al., 2020; Koncel-Kedziorski et al.,\n2019; Cheng et al., 2020). However, the output\nof all these existing works is descriptions or sum-\nmaries in the form of one or a few sentences or\na short paragraph. In contrast, data analysts are\nrequired to provide more insightful comments in-\nstead of intuitive summaries. Furthermore, in the\npractical setting of data analytics work, one should\nhighlight the analysis and insights in bullet points\nto make them clearer to the audience. Therefore,\nin this work, we aim to generate the data analy-\nsis in the form of bullet points instead of a short\nparagraph.\n2.2\nAbilities of GPT-3, ChatGPT and GPT-4\nResearchers have demonstrated the effectiveness of\nGPT-3 and ChatGPT on various tasks (Ding et al.,\n2023; Chiang and Lee, 2023; Shen et al., 2023; Luo\net al., 2023; Wang et al., 2023; Wu et al., 2023b; Li\net al., 2023a; Han et al., 2023; Li et al., 2023b). For\nexample, Ding et al. (2023) evaluated the perfor-\nmance of GPT-3 as a data annotator. Their findings\nshow that GPT-3 performs better on simpler tasks\nsuch as text classification than more complex tasks\nsuch as named entity recognition (NER). Wang\net al. (2023) treated ChatGPT as an evaluator. They\nused ChatGPT to evaluate the performance of nat-\nural language generation (NLG) and to study its\ncorrelations with human evaluation. They found\nthat the ChatGPT evaluator has a high correlation\nwith humans in most cases, especially for creative\nNLG tasks.\nGPT-4 is proven to be a significant upgrade over\nthe existing models, as it is able to achieve more\nadvanced natural language processing capabilities\n(OpenAI, 2023). For instance, GPT-4 is capable\nof generating more diverse, coherent, and natural\nlanguage outputs. It is also speculated that GPT-4\nmay be more capable of providing answers to com-\nplex and detailed questions and performing tasks\nrequiring deeper reasoning and inference (Bubeck\net al., 2023). These advantages will have practical\nimplications in various industries, such as customer\nservice, finance, healthcare, and education, where\nAI-powered language processing can enhance com-\nmunication and problem-solving. In this work, we\nregard GPT-4 as a data analyst to conduct our ex-\nperiments.\n3\nGPT-4 as a Data Analyst\n3.1\nBackground: Data Analyst Job Scope\nThe main job scope of a data analyst involves uti-\nlizing business data to identify meaningful patterns\nand trends from the data and provide stakeholders\nwith valuable insights for making strategic deci-\nsions. To achieve their goal, they must possess a\nvariety of skills, including SQL query writing, data\ncleaning and transformation, visualization genera-\ntion, and data analysis.\nTo this end, the major job scope of a data ana-\nlyst can be split into three steps based on the three\nmain skills mentioned above: data collection, data\nvisualization and data analysis. The initial step in-\nvolves comprehending business requirements and\ndeciding which data sources are pertinent to an-\nswering them. Once the relevant data tables have\nbeen identified, the analyst can extract the required\ndata via SQL queries or other extraction tools. The\nsecond step is to create visual aids, such as graphs\nand charts, that effectively convey insights. Finally,\nin the data analysis stage, the analyst may need to\nascertain correlations between different data points,\nidentify anomalies and outliers, and track trends\nover time. The insights derived from this process\ncan then be communicated to stakeholders through\nwritten reports or presentations.\n3.2\nOur Framework\nFollowing the main job scope of a data analyst, we\ndescribe our task setting below. As illustrated in\nFigure 1, given a business-related question and one\nor more relevant database tables with their schema,\nwe aim to extract the required data, generate a fig-\nure for visualization and provide some analysis and\ninsights.\nTo tackle the above task setting, we design an\nend-to-end framework. With GPT-4\u2019s abilities in\ncontext understanding, code generation, and data\nstorytelling being demonstrated, we aim to use\nGPT-4 to automate the whole data analytics pro-\ncess, following the steps shown in Figure 1. Ba-\nsically, there are three steps involved: (1) code\ngeneration (shown in blue arrows), (2) code exe-\ncution (shown in orange arrows), and (3) analysis\ngeneration (shown in green arrows). The algorithm\nof our framework is shown in Algorithm 1.\nAlgorithm 1 GPT-4 as a data analyst\nRequire: Question q; Database schema s; Database table t; Online o\nRequire: Instruction prompts for code generation pcode, analysis generation panalysis\nRequire: LLM f(\u00b7); LM decoding temperature \u03c4\nRequire: An external knowledge retrieval model g(\u00b7)\nRequire: Python compiler h(\u00b7)\nQ, C \u2190 f(q, s, pcode, \u03c4)\n\u25b7 Generate SQL query (Q) and Python code (C).\nD, G \u2190 h(Q, C, s, t)\n\u25b7 Execute code to get data (D) and graph (G).\nif o is true then\n\u25b7 Only use online information when instructed.\nI \u2190 g(q, D)\n\u25b7 Query information from external knowledge source.\nA \u2190 f(q, panalysis, D, I, \u03c4)\n\u25b7 Generate analysis (A) from data (D) and online information (I).\nreturn D, G, A\nelse if o is false then\nA \u2190 f(q, panalysis, D, \u03c4)\n\u25b7 Generate analysis (A) from data (D).\nreturn D, G, A\nend if\nQuestion: [question]\nconn = sqlite3.connect([database file name])\n[database schema]\nWrite Python code to select relevant data and draw\nthe chart. Please save the plot to \"figure.pdf\" and save\nthe label and value shown in the graph to \"data.txt\".\nTable 1: Prompt for the first step in our framework: code\ngeneration. Text in blue: the specific question, database\nfile name and database schema.\nStep 1: Code Generation.\nThe input of the first\nstep contains a question and database schema. The\ngoal here is to generate the code for extracting data\nand drawing the figure in later steps. We utilize\nGPT-4 to understand the questions and the relations\namong multiple database tables from the schema.\nNote that only the schema of the database tables\nis provided here due to data security reasons. The\nmassive raw data is still kept safe offline, which\nwill be used in the later step. The designed prompt\nfor this step is shown in Table 1. By following\nthe instructions, we can get a piece of Python code\ncontaining SQL queries. An example code snippet\ngenerated by GPT-4 is shown in Appendix A.\nStep 2: Code Execution.\nAs mentioned earlier\nin the previous step, to maintain data safety, we\nexecute the code generated by GPT-4 offline. The\ninput in this step is the code generated from Step\n1 and the raw data from the database, as shown\nin Figure 1. By locating the data directory using\nQuestion: [question]\n[extracted data]\nGenerate analysis and insights about the data in 5\nbullet points.\nTable 2: Prompt for the third step in our framework:\nanalysis generation. Text in blue: the specific question\nand the extracted data as shown in \u201cdata.txt\u201d.\n\u201cconn = sqlite3.connect([database file name])\u201d as\nshown in Table 1 in the code, the massive raw data\nis involved in this step. By executing the Python\ncode, we are able to get the chart in \u201cfigure.pdf\u201d\nand the extracted data saved in \u201cdata.txt\u201d.\nStep 3: Analysis Generation.\nAfter we obtain\nthe extracted data, we aim to generate data analysis\nand insights. To make sure the data analysis is\naligned with the original query, we use both the\nquestion and the extracted data as the input. Our\ndesigned prompt for GPT-4 of this step is shown\nin Table 2. Instead of generating a paragraph of\ndescription about the extracted data, we instruct\nGPT-4 to generate the analysis and insights in 5\nbullet points to emphasize the key takeaways. Note\nthat we have considered the alternative of using\nthe generated figure as input as well, as the GPT-4\ntechnical report (OpenAI, 2023) mentioned it could\ntake images as input. However, this feature was\nnot open to the public at the time of this paper\nwas written. Since the extracted data essentially\ncontains at least the same amount of information\nas the generated figure, we only use the extracted\ndata here as input for now. From our preliminary\nexperiments, GPT-4 is able to understand the trend\nand the correlation from the data itself without\nseeing the figures.\nIn order to make our framework more practical\nsuch that it can potentially help human data an-\nalysts boost their daily performance, we add an\noption of utilizing external knowledge sources, as\nshown in Algorithm 1. Since the actual data analyst\nrole usually requires relevant business background\nknowledge, we design an external knowledge re-\ntrieval model g(\u00b7) to query real-time online infor-\nmation (I) from an external knowledge source (e.g.\nGoogle). In such an option, GPT-4 takes both the\ndata (D) and online information (I) as input to gen-\nerate the analysis (A).\n4\nExperiments\n4.1\nDataset\nSince there is no exact matching dataset avail-\nable, we select the most relevant one, known as\nthe NvBench dataset. We randomly choose 1000\nquestions from various domains, featuring differ-\nent chart types and difficulty levels, to conduct\nour main experiments. The chart types cover bar,\nstacked bar, line, grouping line, scatter, grouping\nscatter and pie. The difficulty levels include: easy,\nmedium, hard and extra hard. The domains include\nsports, artists, transportation, apartment rentals, col-\nleges, etc. On top of the existing NvBench dataset,\nwe additionally use our framework to write insights\ndrawn from data in 5 bullet points for each instance\nand evaluate the quality using our self-designed\nevaluation metrics.\n4.2\nEvaluation\nTo comprehensively investigate the performance,\nwe carefully design several human evaluation met-\nrics to evaluate the generated figures and analysis\nseparately for each test instance.\nFigure Evaluation\nWe define 3 evaluation met-\nrics for figures:\n\u2022 correctness: is the data and information shown\nin the figure correct?\n\u2022 chart type: does the chart type match the re-\nquirement in the question?\n\u2022 aesthetics: is the figure aesthetic and clear with-\nout any format errors?\nMetrics\nGroup 1\nGroup 2\nAverage\nFigure\nCorrectness\n0.77\n0.78\n0.78\nChart Type\n0.99\n1.00\n0.99\nAesthetics\n2.48\n2.51\n2.50\nData\nCorrectness\n0.94\n0.94\n0.94\nAnalysis\nComplexity\n2.30\n2.28\n2.29\nAlignment\n1.00\n1.00\n1.00\nFluency\n3.00\n3.00\n3.00\nTable 3: Performance of GPT-4 as a data analyst.\nThe information correctness and chart type correct-\nness are calculated from 0 to 1, while the aesthetics\nscore is on a scale of 0 to 3.\nAnalysis Evaluation\nFor each bullet point gener-\nated in the analysis and insight, we define 4 evalua-\ntion metrics as below:\n\u2022 correctness: does the analysis contain wrong\ndata or information?\n\u2022 alignment: does the analysis align with the\nquestion?\n\u2022 complexity: how complex and in-depth is the\nanalysis?\n\u2022 fluency: is the generated analysis fluent, gram-\nmatically sound and without unnecessary repe-\ntitions?\nWe grade the correctness and alignment on a scale\nof 0 to 1, and grade complexity and fluency in a\nrange between 0 to 3.\nTo conduct human evaluation, 6 professional\ndata annotators are hired from two data annotation\ncompanies to annotate each figure and analysis\nbullet points on the evaluation metrics described\nabove following the detailed annotation guidelines\nshown in Appendix B. The annotators are fully\ncompensated for their work. Each data point is\nindependently labeled by two different annotators.\n4.3\nMain Results\nGPT-4 performance.\nTable 3 shows the perfor-\nmance of GPT-4 (gpt-4-0314) as a data analyst\non 1000 samples. We show the results of each\nindividual evaluator group and the average scores\nbetween these two groups. For chart-type correct-\nness evaluation, both evaluator groups give almost\nfull scores. This indicates that for such a simple\nand clear instruction such as \u201cdraw a bar chart\u201d,\n\u201cshow a pie chart\u201d, etc., GPT-4 can easily under-\nstand its meaning and has background knowledge\nabout what the chart type means, so that it can plot\nAnnotator Samples\nFigure\nData Analysis\nCorrectness Chart Type Aesthetics Time (s) Correctness Complexity Alignment Fluency Time (s)\nSenior\n30\n0.79\n0.96\n2.96\n472\n0.98\n2.01\n0.98\n2.98\n324\nGPT-4\n0.73\n0.96\n2.41\n059\n0.82\n2.18\n1.00\n3.00\n040\nJunior\n30\n0.66\n0.96\n2.66\n645\n0.95\n1.98\n0.86\n3.00\n388\nGPT-4\n0.71\n0.98\n2.75\n050\n0.94\n2.32\n1.00\n3.00\n034\nIntern\n40\n0.74\n0.91\n2.40\n648\n0.86\n1.59\n1.00\n3.00\n173\nGPT-4\n0.73\n0.97\n2.45\n055\n0.91\n2.28\n1.00\n3.00\n033\nTable 4: Overall comparison between several senior/junior/intern data analysts and GPT-4 on 100 random examples\nin total. Time spent is shown in seconds (s).\nthe figure in the correct type accordingly. In terms\nof aesthetics score, it can get 2.5 out of 3 on av-\nerage, which shows most of the figures generated\nare clear to the audience without any format errors.\nHowever, for the information correctness of the\nplotted figures, the scores are not so satisfactory.\nWe manually check those figures and find most of\nthem can roughly get the correct figures despite\nsome small errors. As shown in Appendix B, our\nevaluation criteria are very strict, such that as long\nas any data or any label on the x-axis or y-axis is\nwrong, the score has to be deducted. Nevertheless,\nit has room for further improvement.\nFor analysis evaluation, both alignment and flu-\nency get full marks on average. It verifies that\ngenerating fluent and grammatically correct sen-\ntences is definitely not a problem for GPT-4. We\nnotice the average correctness score for analysis is\nmuch higher than the correctness score for figures.\nThis is interesting that despite the wrong figure\ngenerated, the analysis could be correct. This is\nbecause, as mentioned, most of the \u201cwrong\u201d fig-\nures only contain some small errors. Thus, only 1\nor 2 out of the 5 bullet points related to the error\nparts from the figures may be generated incorrectly,\nwhile most of the bullet points can be generated\ncorrectly. In terms of the complexity scores, 2.29\nout of 3 on average is reasonable and satisfying.\nWe will show a few cases and discuss more on the\ncomplexity scores in Section 4.4.\nComparison between human data analysts and\nGPT-4.\nTo further answer our research question,\nwe hire professional data analysts to do these tasks\nand compare them with GPT-4 comprehensively.\nThe profiles of the data analysts are described in\nAppendix C. We fully compensate them for their\nannotation. Table 4 shows the performance of data\nanalysts of different expert levels from different\nbackgrounds compared to GPT-4. Overall speak-\nSource\nLevel\nMedian/Average Cost per\nAnnual Salary\ninstance\n(USD)\n(USD)\nlevels.fyi\nSenior DA\n90,421\n09.92\nEntry Level DA\n37,661\n05.36\nGlassdoor\nSenior DA\n86,300\n09.47\nJunior DA\n50,000\n07.12\nIntern DA\n14,400\n01.63\nOur\nSenior DA\n-\n11.00\nAnnotation\nJunior DA\n-\n07.00\nIntern DA\n-\n02.00\nGPT-4\n-\n00.05\nTable 5: Cost comparison from different sources.\ning, GPT-4\u2019s performance is comparable to human\ndata analysts, while the superiority varies among\ndifferent metrics and human data analysts.\nAmong different levels of human data analysts,\noverall speaking, the senior group performs the\nbest, followed by the junior group, and finally the\ninterns, especially on the analysis correctness and\ncomplexity. Comparing human data analysts with\nGPT-4, we can notice that GPT-4 outperforms both\njunior and intern data analysts on most of the met-\nrics, while still having some gap with senior data\nanalysts on three metrics: figure correctness, figure\naesthetics and analysis correctness.\nApart from the comparable performance be-\ntween all data analysts and GPT-4, we can notice\nthe time spent by GPT-4 is much shorter than hu-\nman data analysts. Table 5 shows the cost compari-\nson from different sources. We obtain the median\nannual salary of data analysts in Singapore from\nlevel.fyi2 and the average annual salary of data an-\nalysts in Singapore from Glassdoor3. We assume\nthere are around 21 working days per month and\n2https://www.levels.fyi/\n3https://www.glassdoor.sg/\nQuestion\nPlease list the proportion number of each winning\naircraft.\nSQL Query SELECT\na.Aircraft, COUNT(m.Winning_Aircraft) as wins\nFROM aircraft a\nJOIN match m\nON a.Aircraft_ID = m.Winning_Aircraft\nGROUP BY a.Aircraft\nORDER BY wins DESC\nFigure\nRobinson R-22\n28.6%\nMil Mi-26\n28.6%\nCH-53E Super Stallion\n14.3%\nCH-47D Chinook\n14.3%\nBell 206B3 JetRanger\n14.3%\nProportion of Wins by Aircraft\nAnalysis\n1. The Robinson R-22 and Mil Mi-26 are the\nmost successful aircraft in the dataset, with each\nhaving 2 wins.\n2. The CH-53E Super Stallion, CH-47D Chinook,\nand Bell 206B3 JetRanger each have 1 win, mak-\ning them less successful compared to the Robin-\nson R-22 and Mil Mi-26.\n3. The Robinson R-22 and Mil Mi-26 together\naccount for 50% of the total wins, indicating their\ndominance in the competition.\n4. The dataset includes a diverse range of aircraft,\nfrom helicopters like the CH-53E Super Stallion\nto smaller aircraft like the Robinson R-22, show-\ncasing the variety of winning aircraft.\n5. Despite the variety of aircraft, the data suggests\nthat certain models, such as the Robinson R-22\nand Mil Mi-26, have a higher likelihood of win-\nning, potentially due to their design, performance,\nor other factors.\nTable 6: Case study by GPT-4.\nthe daily working hour is around 8 hours, and cal-\nculate the cost per instance in USD based on the\naverage time spent by data analysts from each level.\nWe pay the data analysts based on the market rate\naccordingly, to roughly match the median or av-\nerage salaries from two sources. Specifically, we\ndiscuss the pay with each data analyst case by case.\nFor our annotation, the cost of GPT-4 is approxi-\nmately 2.5% of the cost of an intern data analyst,\n0.71% of the cost of a junior data analyst and 0.45%\nof the cost of a senior data analyst.\n4.4\nCase Study\nCase by GPT-4.\nIn the case shown in Table 6,\nGPT-4 is able to generate a Python code containing\nthe correct SQL query to extract the required data,\nQuestion\nList the position of players and the average num-\nber of points of players of each position. Visualize\nby bar chart, and could you sort by the total num-\nber in ascending?\nFigure\nProp\nLeft Wing\nFull Back\nLoose Forward\nSecond Row\nHooker\nRight Centre\nRight Wing\nStand Off\nScrum Half\nPosition\n0\n25\n50\n75\n100\n125\n150\n175\nAverage points by position\navg_points\nAnalysis\n1. There are 10 positions. Based on the names,\nthis dataset is about Rugby. Rugby is a group\nsport that is commonly seen in the US, Canada\nand Commonwealth countries.\n2. Stand Off and Scrum Half are having more\nthan 100 average points while the rest are below\n40. This is a bit surprising as usually the Right\nWing and Left Wing are the star scorers.\n3. Prop has the lowest average points of only 9.33,\nas Prop\u2019s main role is to be in the first row and\npush against the opposition\u2019s Prop.\n4. Full Bck, Loose Forward and Second Row has\nthe same points (20), while Hooker, Right Centre\nand Right Wing has the same points (36).\n5. Overall the dataset is slightly different from the\ngeneral understanding. If there\u2019s no data issue, it\nmight worth to reevaluate the players and think\nabout a better position assignment.\nTable 7: Case Study 3 by Senior Data Analyst 2.\nand to draw a proper and correct pie chart accord-\ning to the given question. In terms of the analy-\nsis, GPT-4 is capable of understanding the data by\nconducting proper comparisons (e.g., \u201cmost suc-\ncessful\u201d, \u201cless successful\u201d, \u201cdiverse range\u201d). In\naddition, GPT-4 can provide some insights from\nthe data, such as: \u201cindicating their dominance in\nthe competition\u201d. These aforementioned abilities\nof GPT-4 including context understanding, code\ngeneration and data storytelling are also demon-\nstrated in many other cases. Furthermore, in this\ncase, GPT-4 can also make some reasonable guess\nfrom the data and its background knowledge, such\nas: \u201cpotentially due to their design, performance,\nor other factors\u201d. However, in another case shown\nin Appendix D, we notice some numerical errors\ndone by GPT-4, which is very likely due to its issue\nof hallucination.\nCase by the senior data analyst.\nAs shown in\nTable 7, we can notice that this expert human data\nanalyst can understand the requirement, write the\ncode to draw the correct bar chart, and analyze the\nextracted data in bullet points. Apart from this, we\ncan summarize three main differences with GPT-4.\nFirst, different from GPT-4, the human data ana-\nlyst can express the analysis with some personal\nthoughts and emotions. For example, the data an-\nalyst mentions \u201cThis is a bit surprising ...\u201d. In\nreal-life business, personal emotions are important\nsometimes. With the emotional phrases, the au-\ndience can easily understand whether the data is\nas expected or abnormal. Second, the human data\nanalyst tends to apply some background knowl-\nedge. For example, as shown in Table 7, the data\nanalyst mentions \u201c... is commonly seen ...\u201d, which\nis more natural during a data analyst\u2019s actual job.\nWhile GPT-4 usually only focuses on the extracted\ndata itself, an experienced data analyst is easily\nlinked with one\u2019s background knowledge. How-\never, this might be the reason causing the slightly\nlower alignment scores in Table 4. To mimic a hu-\nman data analyst better, in our framework, we add\nan option of using Google search API to extract\nreal-time online information when generating data\nanalysis. We explain our additional experiment in-\ntegrating the optional online information in Section\n4.5. Third, when providing insights or suggestions,\na human data analyst tends to be conservative. For\ninstance, in the 5th bullet point, the human data\nanalyst mentions \u201cIf there\u2019s no data issue\u201d before\ngiving a suggestion. Unlike humans, GPT-4 usually\ndirectly provides the suggestion in a confident tone\nwithout mentioning its assumptions.\n4.5\nAdditional Experiments\nMore Practical Questions.\nThe questions in the\nexperiments above are randomly selected from the\nNvBench dataset. Although the questions indeed\ncover a lot of domains, databases, difficulty levels\nand chart types, they are still somewhat too specific\naccording to human data analysts\u2019 feedback. The\nexisting questions usually contain information such\nas a specific correlation between two variables, and\na specific chart type. In a more practical setting,\nthe requirements are more general, which requires\na data analyst to formulate a specific question from\nthe general business requirement, and to determine\nwhat kind of chart would present the data better.\nTherefore, we carefully design five practical and\ngeneral questions that are acknowledged by a few\nsenior data analysts. To evaluate the comprehensive\nabilities such as the problem formulation ability\nof GPT-4, we compare the results among GPT-\n4, a senior data analyst and a junior data analyst.\nThe detailed results are shown in Appendix E. For\nsuch practical and general questions, the senior\ndata analyst and GPT-4 perform much better than\nthe junior data analyst. The performances of the\nsenior data analyst and GPT-4 are basically on par\nwith each other.\nOnline Information Integration.\nIn Figure 1,\nwe show the optional input of external knowledge\nin our proposed framework. In some cases, data\nanalysts are not only required to interpret the data\nin the databases, but also to understand and inte-\ngrate some industry background knowledge. For\nsuch questions, we design an optional module that\nqueries online information from Google and gen-\nerates the data analysis with the incorporation of\nthe online information. Through our preliminary\nexperiments, this module helps GPT-4 to combine\nadditional knowledge. We show one case in Ap-\npendix F.\n4.6\nFindings and Discussions\nGenerally speaking, GPT-4 can perform compara-\nble to a data analyst from our preliminary exper-\niments, while there are still several issues to be\naddressed before we can reach a conclusion that\nGPT-4 is a good data analyst. First, as illustrated\nin Section 4.4 and Appendix D, GPT-4 still has\nhallucination problems, which is also mentioned\nin GPT-4 technical report (OpenAI, 2023). Data\nanalysis jobs not only require those technical skills\nand analytics skills, but also requires high accuracy\nto be guaranteed. Therefore, a professional data\nanalyst always tries to avoid those mistakes includ-\ning calculation mistakes and any type of hallucina-\ntion problems. Second, before providing insightful\nsuggestions, a professional data analyst is usually\nconfident about all the assumptions. Instead of di-\nrectly giving any suggestion or making any guess\nfrom the data, GPT-4 should be careful about all\nthe assumptions and make the claims rigorous.\n5\nConclusions\nThe potential for large language models (LLMs)\nlike GPT-4 to replace human data analysts has\nsparked a controversial discussion. However, there\nis no definitive conclusion on this topic yet. This\nstudy aims to answer the research question of\nwhether GPT-4 can perform as a good data ana-\nlyst by conducting several preliminary experiments.\nWe design a framework to prompt GPT-4 to per-\nform end-to-end data analysis with databases from\nvarious domains and compared its performance\nwith several professional human data analysts us-\ning carefully-designed task-specific evaluation met-\nrics. Our results and analysis show that GPT-4\ncan outperform an intern data analyst or a junior\ndata analyst, and can achieve comparable perfor-\nmance to a senior data analyst, but further studies\nare needed before concluding that GPT-4 can re-\nplace data analysts.\n6\nLimitations\nMore Experiments.\nAs mentioned in Section\n4.5, the questions from the NvBench dataset con-\ntain very specific information, which is somewhat\ndisconnected from real work scenarios. In terms\nof the broader questions that are more closely re-\nlated to real work scenarios, only 5 questions are\ndesigned and evaluated in this work. Our next step\nis to collect more practical and general questions\nto further test the problem formulation ability of\nGPT-4.\nWe did not systematically conduct a large num-\nber of experiments using online information as well.\nThe reason is similar to the above. The original\nquestions from the NvBench dataset largely de-\npend on the data stored in the database and rarely\nrequire additional knowledge. Therefore, we leave\nthe design of such open questions to future work.\nChoice of Annotators.\nThe quantity of human\nevaluation and data analyst annotation is relatively\nsmall due to budget limitations. For human evalua-\ntion, we strictly select those professional evaluators\nin order to give better ratings. They have to pass\nour test annotations for several rounds before start-\ning the human evaluation. For the selection of data\nanalysts, we are even more strict. We verify if they\nreally had data analysis working experience, and\nmake sure they master those technical skills before\nstarting the data annotation. However, since hiring\na human data analyst (especially for those senior\nand expert human data analysts) is very expensive,\nwe can only find a few data analysts and ask them\nto do a few samples.\nEthics Statement\nThe purpose of this work is not to replace the data\nanalyst role or to create anxiety. Instead, we would\nlike to explore the potential of GPT-4 to aid human\ndata analysts in more efficient working.\nAcknowledgements\nWe would like to thank our data annotators and\ndata evaluators for their hard work. Especially, we\nwould like to thank Mingjie Lyu for the fruitful\ndiscussion and feedback.\nReferences\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, John A. Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuan-Fang Li, Scott M. Lundberg,\nHarsha Nori, Hamid Palangi, Marco Tulio Ribeiro,\nand Yi Zhang. 2023.\nSparks of artificial general\nintelligence: Early experiments with gpt-4. ArXiv,\nabs/2303.12712.\nLiying Cheng, Dekun Wu, Lidong Bing, Yan Zhang,\nZhanming Jie, Wei Lu, and Luo Si. 2020. Ent-desc:\nEntity description generation by exploring knowl-\nedge graph. In Proceedings of EMNLP.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large\nlanguage models be an alternative to human evalua-\ntions? In Proceedings of ACL.\nBosheng Ding, Chengwei Qin, Linlin Liu, Lidong Bing,\nShafiq Joty, and Boyang Li. 2023. Is gpt-3 a good\ndata annotator? In Proceedings of ACL.\nOnd\u02c7rej Du\u0161ek, Jekaterina Novikova, and Verena Rieser.\n2020. Evaluating the state-of-the-art of end-to-end\nnatural language generation: The e2e nlg challenge.\nComputer Speech & Language.\nLeo Ferres, Gitte Lindgaard, Livia Sumegi, and Bruce\nTsuji. 2013. Evaluating a tool for improving acces-\nsibility to charts and graphs. ACM Trans. Comput.\nHum. Interact., 20:28:1\u201328:32.\nChang Gao, Bowen Li, Wenxuan Zhang, Wai Lam, Bin-\nhua Li, Fei Huang, Luo Si, and Yongbin Li. 2022.\nTowards generalizable and robust text-to-sql parsing.\nIn Findings of EMNLP.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The webnlg chal-\nlenge: Generating text from rdf data. In Proceedings\nof INLG.\nJiaqi Guo, Zecheng Zhan, Yan Gao, Yan Xiao, Jian-\nGuang Lou, Ting Liu, and D. Zhang. 2019. Towards\ncomplex text-to-sql in cross-domain database with\nintermediate representation. In Proceedings of ACL.\nTianyu Han, Lisa C. Adams, Jens-Michalis Papaioan-\nnou, Paul Grundmann, Tom Oberhauser, Alexander\nL\u00f6ser, Daniel Truhn, and Keno K. Bressem. 2023.\nMedalpaca \u2013 an open-source collection of medical\nconversational ai models and training data.\nShankar Kantharaj, Rixie Tiffany Leong, Xiang Lin,\nAhmed Masry, Megh Thakkar, Enamul Hoque, and\nShafiq Joty. 2022. Chart-to-text: A large-scale bench-\nmark for chart summarization. In Proceedings of\nACL, pages 4005\u20134023.\nRik Koncel-Kedziorski, Dhanush Bekal, Yi Luan,\nMirella Lapata, and Hannaneh Hajishirzi. 2019. Text\ngeneration from knowledge graphs with graph trans-\nformers. In Proceedings of ACL.\nXingxuan Li, Yutong Li, Shafiq Joty, Linlin Liu, Fei\nHuang, Lin Qiu, and Lidong Bing. 2023a. Does gpt-3\ndemonstrate psychopathy? evaluating large language\nmodels from a psychological perspective.\nYunxiang Li, Zihan Li, Kai Zhang, Ruilong Dan, and\nYou Zhang. 2023b.\nChatdoctor: A medical chat\nmodel fine-tuned on llama model using medical do-\nmain knowledge.\nYuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai,\nWenbo Li, and Xuedi Qin. 2021. Synthesizing nat-\nural language to visualization (nl2vis) benchmarks\nfrom nl2sql benchmarks. In Proceedings of SIG-\nMOD.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor abstractive text summarization. arXiv preprint\narXiv:2303.15621.\nVibhu O. Mittal, Johanna D. Moore, Giuseppe Carenini,\nand Steven Roth. 1998. Describing complex charts\nin natural language: A caption generation system.\nComputational Linguistics, 24(3):431\u2013467.\nDavid Noever and Matt Ciolino. 2023. Professional\ncertification benchmark dataset: The first 500 jobs\nfor large language models.\nOpenAI. 2023. Gpt-4 technical report.\nJiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan,\nYu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi\nZhang, and Zhouhan Lin. 2022. RASAT: Integrating\nrelational structures into pretrained Seq2Seq model\nfor text-to-SQL. In Proceedings of EMNLP, pages\n3215\u20133229.\nAndr\u00e9 Ribeiro, Afonso Silva, and Alberto Rodrigues\nda Silva. 2015. Data modeling and data analytics:\nA survey from a big data perspective. Journal of\nSoftware Engineering and Applications, 08:617\u2013634.\nChenhui Shen, Liying Cheng, Yang You, and Lidong\nBing. 2023. Are large language models good evalua-\ntors for abstractive summarization?\nZhongxiang Sun. 2023. A short survey of viewing large\nlanguage models in legal aspect.\nChun-Wei Tsai, Chin-Feng Lai, H. C. Chao, and Athana-\nsios V. Vasilakos. 2015. Big data analytics: a survey.\nJournal of Big Data, 2:1\u201332.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nJiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng\nWan, and Hong Lin. 2023a. Ai-generated content\n(aigc): A survey.\nNing Wu, Ming Gong, Linjun Shou, Shining Liang,\nand Daxin Jiang. 2023b. Large language models are\ndiverse role-players for summarization evaluation.\narXiv preprint arXiv:2303.15078.\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski,\nMark Dredze, Sebastian Gehrmann, Prabhanjan Kam-\nbadur, David Rosenberg, and Gideon Mann. 2023c.\nBloomberggpt: A large language model for finance.\nTao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue,\nBo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze\nShi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga,\nSungrok Shim, Tao Chen, Alexander Fabbri, Zifan\nLi, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, Walter\nLasecki, and Dragomir Radev. 2019a. CoSQL: A\nconversational text-to-SQL challenge towards cross-\ndomain natural language interfaces to databases. In\nProceedings of EMNLP-IJCNLP, pages 1962\u20131979.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2018. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-SQL task. In Proceedings of EMNLP,\npages 3911\u20133921.\nTao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern\nTan, Xi Victoria Lin, Suyi Li, Heyang Er, Irene\nLi, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit,\nDavid Proctor, Sungrok Shim, Jonathan Kraft, Vin-\ncent Zhang, Caiming Xiong, Richard Socher, and\nDragomir Radev. 2019b. SParC: Cross-domain se-\nmantic parsing in context. In Proceedings of ACL,\npages 4511\u20134523.\nZaixiang Zheng, Yifan Deng, Dongyu Xue, Yi Zhou,\nFei YE, and Quanquan Gu. 2023. Structure-informed\nlanguage models are protein designers.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\nA\nExample Code\nFigure 2: An example of a complete code generated by\nGPT-4. This code is to answer the question shown in\nTable 6.\nFigure 2 shows an example code generated by\nGPT-4. First, we can notice that GPT-4 is capable\nof writing SQL queries with several commands,\nsuch as JOIN, GROUP BY, ORDER BY to extract\nthe required data. Second, GPT-4 knows how to\nuse multiple Python packages including sqlite and\nmatplotlib, which help to connect the databases and\ndraw charts respectively. Third, GPT-4 can under-\nstand the requirement in the question to save the\ndata and figure it into the correct files accordingly.\nLast but not least, it can also generate comments\nunderstandable by readers, which is aligned with\nthe goal of helping human data analysts boost their\ndaily performance. In the case when the wrong\ncode is generated, a human analyst can easily un-\nderstand which part goes wrong with the aid of the\ncomments.\nB\nDetailed Annotation Guidelines\nIn this section, we present our detailed annotation\nguidelines for human evaluators.\nB.1\nFigure Evaluation\nFor the figures generated by the model, scores will\nbe given based on the following three criteria, using\nthe correct figure (and correct data) as a reference:\nInformation Correctness.\nThe information cor-\nrectness can be chosen from 0, 0.5 and 1. First,\nif the information is correct, 1 point is awarded.\nSecond, if there are minor errors, 0.5 points are\nawarded. The minor errors mean that the data is\nmostly correct, but missing one or two data points\nor showing indexes instead of x-axis/y-axis names,\nand it does not affect the overall data conclusion.\nThird, if any important data is incorrect, no points\nare awarded. The errors include data errors, extra\ndata, missing data, incorrect x-axis/y-axis names,\netc. Errors do not include inconsistent color, inac-\ncurate data, inconsistent order, etc.\nChart Type Correctness.\nSince chart type is\npretty straightforward, the scores will be binary.\nIf it matches the chart type required in the question,\n1 point is awarded; otherwise, 0 point is awarded.\nFor example, if a pie chart is required in the ques-\ntion, but a line chart is drawn, 0 point is awarded.\nAesthetics.\nThe score of this metric is on a scale\nof 0 to 3 points. If all information is clear, it will\nreceive full marks (3 points). If there are minor\nformat issues, 2 points are awarded. If it affects\nthe reader\u2019s understanding to some extent, 1 point\nis awarded. If it seriously affects reading, 0 points\nare awarded. Subjectivity is relatively high for\nthis metric, and thus we show the annotators a few\nexamples.\nB.2\nData Analysis Evaluation\nFor each data analysis bullet point generated by\nthe model, we evaluate it from the following four\naspects based on the correct data and the correct\nfigure.\nCorrectness.\nThe scores of this metric are binary.\nThe sentence gets 1 point if the information in the\nsentence is correct. If the sentence contains any\nfalse information, it will get a 0 score.\nAlignment.\nThe scores of this metric are binary\nas well. The bullet point gets 1 point if it is relevant\nto the question, and 0 points if irrelevant.\nComplexity.\nThe score of complexity is o a scale\nof 0 to 3 points. The bullet point gets 0 points if it is\na general description. For example: \u201cThis is a bar\nchart, the x-axis represents ..., the y-axis represents\n...\u201d. This information is considered very general,\nwhich can be obtained without seeing the actual\ndata. The bullet point gets 1 point for directly visi-\nble data points or information. For example, \u201cthe\nquantity on wednesday reached 50.\u201d, \u201cThere are a\ntotal of 5 data points.\u201d, etc. The bullet point gets\n2 points for analysis obtained by comparison or\ncalculation. For example, \u201cthe range of certain data\nis from 23 to 699\u201d. This actually includes which\none has the highest and lowest scores, and is ob-\ntained by comparison, so it is labeled as 2 points.\nSimilarly, another example is, \u201cWednesday has the\nhighest score, reaching 50\u201d. Other types of exam-\nples are: \u201cThe data increased from 5 to 8\u201d, \u201cthe\nsum of A and B on Wednesday is 67\u201d, \u201cthere is a\npositive correlation trend\u201d. Lastly, The bullet point\ngets 3 points if the sentence has some insights. For\nexample, \u201cthe number on Wednesday reached 50,\nindicating/suggesting ...\u201d. Most of the keywords\nused are \"indicates/suggests/shows\", which can ba-\nsically be worth 3 points. The premise is that the\ninsight should be about the content of the figure. It\ncan be an insight obtained from specific numbers\nor the positive/negative correlation/trend of the x-\naxis/y-axis variables. The insights generally can\nobtain some useful conclusions. ii.\nFluency.\nFluency and Grammar soundness is on\na scale of 0 to 3 points. (a) Very smooth, no gram-\nmar errors: 3 points. (b) Some minor grammar\nor spelling errors that do not affect understanding:\n2 points. (c) Some grammar errors that affect un-\nderstanding to some extent: 1 point. (d) Serious\ngrammar errors that significantly affect reading and\nunderstanding: 0 points.\nC\nProfiles of Our Data Analysts\nWe hire 5 data analysts of different levels in total,\n2 of them are senior data analysts, 2 of them are\njunior data analysts and 1 of them is an intern data\nanalyst. One senior data analyst has more than 6\nyears of data analysis working experience in the fi-\nnance industry. The other senior data analyst works\nin the internet industry as a data analyst for over\n5 years. One junior data analyst has data analysis\nworking experience in a consulting firm within 2\nyears. The other junior data analyst works in the\nsupply chain industry as a data analyst for around\n2 years. The intern junior data analyst is doing a\ndata analysis internship at a consulting firm.\nD\nMore Case Studies\nQuestion\nShow me about the correlation between Height\nand Weight in a scatter chart.\nFigure\n188\n190\n192\n194\n196\n198\n200\n202\nHeight (cm)\n82\n84\n86\n88\n90\n92\n94\nWeight (kg)\nCorrelation between Height and Weight\nAnalysis\n1. The data provided consists of 9 data points,\neach representing the height and weight of an in-\ndividual.\n2. The height range in the dataset is from 188 cm\nto 202 cm, while the weight range is from 82 kg\nto 94 kg.\n3. The average height of the individuals in the\ndataset is approximately 195.3 cm, and the aver-\nage weight is approximately 88.1 kg.\n4. There seems to be a positive correlation be-\ntween height and weight, as taller individuals gen-\nerally have higher weights.\n5. A scatter chart would help visualize this cor-\nrelation, with height on the x-axis and weight on\nthe y-axis. The data points would likely form an\nupward trend, indicating that as height increases,\nweight also tends to increase.\nTable 8: Another Case Study by GPT-4.\nTable 8 shows another question addressed by\nGPT-4. Again, GPT-4 is able to extract the correct\ndata, draw the correct scatter plot and generate\nreasonable analysis. Although most of the bullet\npoints are generated faithfully, if we read and check\ncarefully, we can notice the numbers of the average\nheight and weight are wrong. Apart from the well-\nknown hallucination issue, we suspect that GPT-4\u2019s\ncalculation ability is not strong, especially for those\ncomplex calculations. We also notice this issue in\nseveral other cases. Although GPT-4 generates the\nanalysis bullet points in a very confident tone, but\nthe calculation is sometimes inaccurate.\nE\nMore Practical Questions\nIn this section, we present 5 more practical ques-\ntions that do not have clear or specific requirements.\nThe questions designed are more likely to be open\nQuestion Junior DA Senior DA GPT-4\n1\n1\n3\n2\n2\n1\n3\n2\n3\n1\n2\n3\n4\n1\n2\n3\n5\n1\n3\n2\nTable 9: Scores of 5 more practical questions.\nquestions, which do not have one fixed answer.\nThis requires the data analysts and GPT-4 to have a\ngood problem formulation ability. We score them\nbased on their ranking of each question. The one\nwho is ranked the first gets a score 3, the second\none gets a score of 2 and the last one gets a score of\n1. The results are shown in Table 9. We discuss the\nresults of these 5 questions one by one to evaluate\nhow human data analysts and GPT-4 perform.\nTable 10 shows the first more practical ques-\ntion. This question simply asks \u201cWhich candidate\nshould we accept?\u201d without specifying the exact re-\nquirements for candidate acceptance. We rank the\nsenior data analyst\u2019s answer the first among these\n3. Instead of only considering the support rate, the\nsenior data analyst also considered the oppose rate\nprovided in the database, and proposed a new met-\nric named net polarity rate. GPT-4 gave the answer\nby sorting on the support rate. However, GPT-4\nmentioned other candidates could be considered\nif additional factors are taken into account. This\nindicates the potential of GPT-4 to be trained to be\na professional data analyst who has comprehensive\nthinking.\nThe results of the second question are shown in\nTable 11. All 3 annotators gave the same answer by\nranking the students based on their average grades.\nHowever, the junior data analysts did not specify\nthe names of the students and wrote a few irrelevant\nanalysis bullet points, thus ranked the last by us. In\ncontrast, the human senior data analyst explained\nthe reason for choosing this metric clearly.\nAmong the results of the third question shown in\nTable 12, we rank the GPT-4 the first, followed by\nthe senior data analyst and finally the junior data\nanalyst. Both GPT-4 and the senior data analysts\nanalyzed the data based on different regions. Given\nthe limited employee data, GPT-4 still mentioned\nthe potential possibilities of cost reduction in 3 bul-\nlet points, while the senior data analyst mentioned\nit in the last bullet point.\nThe results of the fourth question are shown in\nTable 13. All three annotators provided the same\nanswer. GPT-4 is ranked first because it provided\nmore insights in most of the bullet points. The\nsenior data analyst is ranked second because he/she\nsuggests another metric to measure the popularity\nof the rooms. Although the junior analyst gave the\nsame answer, 3 out of 5 bullets are irrelevant to the\nquestion.\nTable 14 shows the results of the last practical\nquestion. The senior data analyst\u2019s answer is no\ndoubt to be the best, as the figure is clearer and the\nbullet points are more reasonable than the other two.\nFor the junior data analyst and GPT-4, the answers\nmentioned in the analysis cannot be seen clearly\nfrom the figures. The junior data analyst is ranked\nlast as he/she wrote a few irrelevant analyses again.\nF\nOnline Information Integration\nTable 15 shows one case done by GPT-4 with and\nwithout integrating online information. When gen-\nerating analysis without querying online informa-\ntion, the analysis bullet points are purely from\nthe given database. When incorporating external\nknowledge, GPT-4 is able to generate relevant addi-\ntional information from online to make the analysis\nmore powerful.\nQuestion\nWhich candidate should we accept?\nJunior DA\n1\n2\n3\n4\n5\n6\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\naverage support rate\n1. Based on data, candidate 4 has the highest average support score.\n2. Newsweek Poll has the highest average support score amount three different poll sources.\n3. Candidate 2 has extremely high unsure rate given by WNBC/Marist Poll.\n4. Candidate 4 has the highest oppose rate.\n5. The variance of consider rate is low among all the different rates.\nSenior DA\nLo\u00efc De Kergret\nVincent Montm\u00e9at\nSt\u00e9phane Antiga\nHubert Henno\nGuillaume Samica\nLaurent Capet\nName\n0.2\n0.1\n0.0\n0.1\n0.2\n0.3\n0.4\nCandidate support/oppose/net rate\nsupport_rate\noppose_rate\nnps_rate\n1. Combining the Support Rate, Oppose Rate, and Net Polarity Rate, we should accept Loic De Kergret.\n2. Overall, all candidates have a higher oppose rate than the support rate.\n3. For the support rate, Loic ranks second. Although Vincent Montmeant has a higher support rate than Loic, Vincent also has a\nmuch higher oppose rate.\n4. We consider the New polarity rate to be the best metric to take. This metric is calculated as Support Rate substrate the Oppose\nrate.\n5. We think it\u2019s important for the candidate to have a high support rate, but also important to have a low oppose rate. Hence the Net\nPolarity rate is the one we take.\nGPT-4\nGuillaume Samica\nHubert Henno\nLaurent Capet\nLo\u00efc De Kergret\nSt\u00e9phane Antiga\nVincent Montm\u00e9at\nCandidate\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nAverage Support Rate\nAverage Support Rate for Each Candidate\n1. Vincent Montm\u00e9at has the highest average support among the candidates, with a score of 0.33, making him a strong contender for\nacceptance.\n2. St\u00e9phane Antiga and Laurent Capet have the lowest average support, with scores of 0.17 and 0.18, respectively, indicating that\nthey may not be the most suitable candidates for the position.\n3. Guillaume Samica and Lo\u00efc De Kergret have similar average support scores of 0.24, placing them in the middle of the pack and\nsuggesting that they could be potential alternatives if Vincent Montm\u00e9at is not selected.\n4. Hubert Henno has a slightly higher average support score than Guillaume Samica and Lo\u00efc De Kergret, with a score of 0.25,\nmaking him another potential alternative candidate.\n5. Overall, the data suggests that Vincent Montm\u00e9at is the most favored candidate, but Hubert Henno, Guillaume Samica, and Lo\u00efc\nDe Kergret could also be considered if additional factors need to be taken into account.\nTable 10: Comparison of GPT-4, a senior and a junior data analyst on the first more practical questions.\nQuestion\nChoose 3 students who perform well on tests as representatives.\nJunior DA\n324291\n324257\n324274\n324269\n324299\n321452\n324258\n324273\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n1. Based on GPA, the top 3 students are 324291, 324257, 324274.\n2. ACCT department has the most amount of students in dataset, which is 3.\n3. Students date of birth are all before 1981.\n4. The student hours has a high variance.\n5. 3 out of 8 students are transfer students.\nSenior DA\nRobertson Gerald\nSmithson Anne\nKatinga Raphael\nOblonski Walter\nBowser William\nSmith John\nBrewer Juliette\nstudent_name\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nStudent by average grade\naverage_gpa\n1. We choose to measure the student by their average grade.\n2. The calculation method is to take the GPA for every student and take the average. This is to prevent the case where 1 student\ntakes multiple courses.\n3. The top 3 students ranked by grade are Robertson Gerald, Smithson Anne, and Katinga Raphael.\n4. Robertson Gerald\u2019s score is the highest with 3.87, followed by Smithson Anne at 3.27, and Katinga Raphael at 3.15.\n5. While Robertson is significantly better than the rest, Smithson and Katinga are not leading the following too much.\nGPT-4\nRobertson, Gerald\nSmithson, Anne\nKatinga, Raphael\nStudents\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nGPA\nTop 3 Students by GPA\n1. The data provided consists of three students\u2019 names and their respective test performance scores, presumably on a scale of 0 to 4\nor 5.\n2. Among the three students, Gerald Robertson has the highest test performance score with a 3.87, indicating that he performs\nexceptionally well on tests.\n3. Anne Smithson has the second-highest score with a 3.27, which suggests that she also performs well on tests, although not as\nhigh as Gerald Robertson.\n4. Raphael Katinga has the lowest score among the three students with a 3.15, but this score still indicates a relatively good\nperformance on tests.\n5. Based on the data, all three students can be considered as representatives of those who perform well on tests, with Gerald\nRobertson being the top performer, followed by Anne Smithson and Raphael Katinga.\nTable 11: Comparison of GPT-4, a senior and a junior data analyst on the second more practical questions.\nQuestion\nHow to reduce human cost by shifting employees from different departments among these regions?\nJunior DA\nAccounting\nAdministration\nExecutive\nFinance\nHuman Resources\nIT\nMarketing\nPublic Relations\nPurchasing\nSales\nShipping\n0\n2500\n5000\n7500\n10000\n12500\n15000\n17500\n20000\n1. Based on the salary of employees across all departments, executive department has the highest salary, followed by accounting and\npublic relations\n2. The lowest paid departments are purchasing and administrationand shipping\n3. Amoung all the job titles, purchasing clerk, stock clerk and shipper clerk are lowest paid while managers and presidents are\nhighest paid\n4. Shipping and sales have the most amount of employees while accounting and administration have lowest amount of employees\n5. Based on above, it is shown that it\u2019s not feasible to move people from accounting, admin departments to sales, shipping as there\nare very little people in those department. Instead, finance and IT are higher paid and having more people and could be a good target\nof restructruing\nSenior DA\nAccounting\nAdministration\nExecutive\nFinance\nIT\nMarketing\nPurchasing\nShipping\nHuman Resources Public Relations\ndepartment_name\n0\n10\n20\n30\n40\nemployees\nEmployee count\nregion_name\nAmericas\\r\nEurope\\r\n1. We plotted out employee count by the department and colored by region with blue for the American and orange for Europe\n2. Europe has only 2 employees, with 1 from Human Resources and 1 from Public Relationship\n3. We think it may not be very efficient to set up an EU office with only two employees\n4. Usually, Human Resources employees work closely with the rest of the department. Hence we would suggest shifting the Human\nresource function to the US. For PR, if the marketing focus is EU, it\u2019s still ok to keep it in EU\n5. However, we would advise the company to consider labour costs in different places as well\nGPT-4\nAccounting\nAdministration\nExecutive\nFinance\nHuman Resources\nIT\nMarketing\nPublic Relations\nPurchasing\nShipping\nDepartment\n0\n10\n20\n30\n40\nNumber of Employees\nNumber of Employees by Department and Region\nRegion\nAmericas\\r\nEurope\\r\n1. The data shows a high concentration of employees in the Shipping department in the Americas region, which could indicate a\npotential area for cost reduction by redistributing employees to other regions or departments.\n2. The Americas region has a diverse range of departments, with Finance and Purchasing having a significant number of employees.\nThis could present an opportunity to shift some employees from these departments to other regions where there is a shortage of staff\nin similar roles.\n3. Europe has a very limited number of departments represented, with only Human Resources and Public Relations having one\nemployee each. This suggests that there may be a need for additional staff in other departments in the Europe region, which could be\naddressed by transferring employees from the Americas region.\n4. The Executive department in the Americas region has three employees, which could be an area to consider for cost reduction\nby evaluating the necessity of all three positions and potentially consolidating roles or shifting employees to other departments or\nregions.\n5. Overall, the data suggests that there may be opportunities for cost reduction and increased efficiency by evaluating the distribution\nof employees across departments and regions, and considering strategic shifts of employees to address imbalances and reduce\nredundancy in certain areas.\nTable 12: Comparison of GPT-4, a senior and a junior data analyst on the third more practical questions.\nQuestion\nWhich room is more popular?\nJunior DA\nCAS\nFNA\nIBD\nRND\nHBB\nIBS\nTAA\nMWC\nRTE\nAOB\n0\n10\n20\n30\n40\n50\n60\n70\n1. \u2018Convoke and sanguine\u2019 and \u2018Frugal not apropos\u2019 and \u2018immutable before decorum\u2019 are the most popular rooms as they have the\nmost amount of reservations.\n2. Frugal not apropos has the highest average rate based on reservations.\n3. Most of rooms either has traditional or modern decoration.\n4. Base price is usually associate with the max number of occupancy.\n5. Reservation checkin and checkout data are not in standard format, it\u2019s unclear which year it\u2019s referring to.\nSenior DA\nCAS\nFNA\nIBD\nRND\nHBB\nTAA\nIBS\nMWC\nRTE\nAOB\nRoom\n0\n10\n20\n30\n40\n50\n60\n70\nRoom by reservation count\nreservations\n1. CAS is the most popular room if considering reservations, with 69 reservations.\n2. It only wins by 3 reservations compared with the room with the second most reservations, FNA.\n3. The minimum reserved room is AOB, with 51 records.\n4. Overall, the gap between the rooms is not hugely different.\n5. Besides reservations, we could also measure the \u201cpopularity\u201d by the number of guests.\nGPT-4\nConvoke and sanguine\nFrugal not apropos\nImmutable before decorum\nRecluse and defiance\nHarbinger but bequest\nThrift and accolade\nInterim but salutary\nMendicant with cryptic\nRiddle to exculpate\nAbscond or bolster\nRoom Name\n0\n10\n20\n30\n40\n50\n60\n70\nNumber of Reservations\nPopularity of Rooms\n1. The most popular room is \"Convoke and sanguine\" with 69 visitors, indicating that it may have the most appealing features or\nactivities for guests.\n2. The least popular room is \"Abscond or bolster\" with 51 visitors, suggesting that it may not be as attractive or engaging as the\nother rooms.\n3. The top three rooms (\"Convoke and sanguine,\" \"Frugal not apropos,\" and \"Immutable before decorum\") have a relatively close\nnumber of visitors, ranging from 63 to 69, indicating that they may share similar characteristics that make them popular among\nguests.\n4. The bottom three rooms (\"Mendicant with cryptic,\" \"Riddle to exculpate,\" and \"Abscond or bolster\") have a more significant\ndifference in visitor numbers, ranging from 51 to 56, suggesting that there may be more variation in their appeal to guests.\n5. Overall, there is a relatively even distribution of visitors across the rooms, with only an 18-visitor difference between the most\nand least popular rooms. This could indicate that guests have diverse preferences and interests, or that the rooms offer a variety of\nexperiences that cater to different tastes.\nTable 13: Comparison of GPT-4, a senior and a junior data analyst on the fourth more practical questions.\nQuestion\nA client\u2019s budget is 60 and he doesn\u2019t like Zinfandel, which wines shall we recommend to him?\nJunior DA\nCabernet Franc\nChardonnay\nSyrah\nPinot Noir\nMarsanne\nViognier\nCabernet Sauvingnon\nPetite Sirah\nGrenache\nSauvignon Blanc\nSangiovese\nTempranillo\nBarbera\nMerlot\nMalbec\nGrape\n0\n20\n40\n60\n80\nScore\n1. We should recommend wine Papas Block to him as this wine is within his budget and not made of Zindandel and has highest\nScore.\n2. The mean score are roughly the same across all different type of grapes.\n3. All wines are from California.\n4. Most of wines have \u2018now\u2019 as value of drink but there are also some other years appear in this column.\n5. Most of the wines are associated with year 2007 which is suspected to be the made year, followed by year 2008 and 2009.\nSenior DA\nRose Wine Creek Ranch\nCarneros\nChardonnay\nMetallico Un-Oaked\nHome Ranch\nChardonnay\nZinfandel\nMaverick\nParr Vineyard\nLos Alamos Vineyard Historic Vineyards Collection\nChardonnay\nPinot Noir\nCuvee Nicolas\nRominger Vineyard\nChardonnay\nChardonnay\nHome Ranch\nChardonnay\nChardonnay\nChardonnay\nName\n0\n1\n2\n3\n4\n5\n6\nWine by ROI\nROI\n1. The metric we choose to measure ROI. It\u2019s calculated as Score/Price. We interpret it as the score value gained for every 1 dollar.\n2. Further, we set the minimum score required to be 90.\n3. Based on this metric, we would recommend Rose Wine Creek Ranch.\n4. Rose Wine Creek Ranch\u2019s score is not high (only 90), however, it\u2019s only 15 dollars. This is super low compared to the rest.\n5. Hence, we would recommend Rose Wine Creek Ranch, as it provides the best value for a dollar.\nGPT-4\nSauvignon Blanc\nLes Pionniers\nCuvee Nicolas\nTradition\nChurch Vineyard\nLApres-Midi\nRosemarys Block Dry Stack Vineyard\nMesa Verde Vineyard\nAsatsuyu\nEstate\nHome Ranch\nHyde Vineyards\nPurisma Mountain\n3CV Grassini Family Vineyards\nSchmidt Ranch\nCuvee Natalie\nau Naturel\nHandley Vineyard\nWow Oui\nMagnolia Lane\nCarols Vineyard\nEast Side Vineyards\nNeyers Ranch - Conn Valley\nCabernet Sauvignon\nJulias Vineyard\nPinot Noir\nLancel Creek Vineyard\nReserve\nA by Acacia\nLa Encantada Vineyard\nLa Encantada Vineyard Clone 667\nEstate Reserve\nCarneros\nGarys Vineyard\nSarmento Vineyard\nHenry Ranch\nKali Hart\nSleepy Hollow Vineyard\nSyrah\nMerlot\nOn the Road\nGarnacha Denner Vineyard\nPurisma Mountain Vineyard\nRomeo\nHope Family Vineyard\nFoggy Knoll Vineyard\nAnns Block\nKick Ranch\nThompson Vineyard\nSweetwater Springs Vineyard\nPetite Sirah\nRosellas Vineyard\nPisoni Vineyard\nKanzler Vineyard\nRussel Family Vineyard\nDurell Vineyard\nRancho Oliveros Vineyard\nGraham Family Vineyard\nClos Pepe Vineyard\nKeefer Ranch Vineyard\nCargasacchi Vineyard\nTurner Vineyard\nRancho La Vina Vineyard\nAubaine Vineyard\nCardiac Hill\nLarner Vinyard\nWhite Hawk Vineyard\nMesa Reserve\nTwist of Fate\nKemp Vineyard\nChristensen Family Vineyard\nMelville Vineyards\nThe Flight\nColson Canyon Vineyard\nEngland-Shaw\nRose Steinbeck Vineyard\nLa Cruz Vineyard\nSnows Lake Vineyard\nAbba Vineyard\nOriginals\nQue Syrah Vineyard\nGesture Limited Release\nStar Angel \nKick Ranch Reserve\nLes Cotes De LOuest\nShiraz\nChardonnay\nSignature\nLoco\nBailey Vineyard\nTwenty Two\nBlack Bart Stagecoach Vineyard\nPolomita Hamilton Vineyard\nRominger Vineyard\nAmber Ridge Vineyard\nRoll Ranch Vineyard\nGuidici Famili Vineyard\nBynum & Moshin Vineyards\nJensen Vineyard\nThirtieth Anniverasry Vintage Mt. Harlan Cuvee\nGreen River Isobel\nSengiacomo Vineyard\nLorenzo\nChloe\nStagecoach Vineyard Black Bart\nStagecoach Vineyard\nTen-Goal Happy Canyon Vineyards\nThe Ranch\nNapa Cuvee\nBoonflys Hill\nDurell Vineyard Wente Clone\nEstate Vineyard\nLos Alamos Vineyard Historic Vineyards Collection\nDamaris Reserve \nCahrles Heintz Vineyard\nSanta Cruz Mountains Estate\nRed Shoulder Ranch\nWinmakers Reserve\nSangiacomo Vineyard\nSbraglia Limited-Release\nRyo-fu\nLos Olivos Vineyard\nKeefer Ranch\nPrivate Reserve\nCarneros Reserve\nDutton Ranch\nEmelias Cuvee\nOvation\nLaughlin Family Vineyard\nDon Miguel Vineyard Lia Torres Family Vineyards\nB. Theriot Vineyard\nCeja Vineyard\nProprietary Reserve\nLos Carneros\nValley Dominique\nFounders\nGold Creek Ranch\nDe La Guerra\nParr Vineyard\nFinest Selection Reserve\nValley Savoy\nCoster Vineyard\nLambda\nHurst Vineyard\nMcDougal Ranch\nWildcat Mountain Vineyard\nRamondo Vineyard\nSymposium Fort Ross Vineyard\nGrand Detour\nValley Dutton Ranch\nPapas Block\nCuvee Trouvee\nGrand Master\nLes Galets\nGamma\nUnti Vineyard\nCabernet Franc\nArtistry\nMetallico Un-Oaked\nRussian River Selection\nInSpire\nSpare Me\nJersey Boy\nChoreography\nTephra Ridge Vineyard\nDistrict 4\nElements\nLa Capilla Collection\nRose Wine Creek Ranch\nMailbox Vineyard Drive\nChelsea Goldschmidt\nVictories Rose\nGrand Firepeak Cuvee Firepeak Vineyard\nFume Blanc\nLillie Vineyard\nMadden Ranch\nRose\nSangiovese\nBarbera\nWine Name\n0\n10\n20\n30\n40\n50\n60\nPrice\nWines under $60 (excluding Zinfandel)\n1. The majority of the wines listed are within the client\u2019s budget of 60, providing a wide range of options for recommendations.\n2. Sauvignon Blanc, Pinot Noir, and Chardonnay are the most frequently listed wine varieties, suggesting that these are popular\nchoices and could be suitable recommendations for the client.\n3. The price range for the wines listed is quite broad, from as low as 9 to as high as 60, indicating that there are options available for\nvarious preferences and budgets.\n4. The client specifically mentioned not liking Zinfandel, and none of the wines listed are Zinfandel, ensuring that all options\nprovided are suitable for the client\u2019s taste preferences.\n5. Some wines have multiple price points, such as Sauvignon Blanc and Pinot Noir, which could indicate different vintages,\nvineyards, or quality levels, providing further variety for the client to choose from.\nTable 14: Comparison of GPT-4, a senior and a junior data analyst on the fifth more practical questions.\nQuestion\nCombining the data of the phone market in recent years and the database, which phone is more popular?\nFigure\nIPhone 6s\nIPhone 5s\nIPhone X\nIPhone 7\nPhone Name\n0\n1000\n2000\n3000\n4000\nTotal Stock\nPhone Popularity\nAnalysis\nWithout\nOnline\nInformation\n1. The iPhone 6s is the most popular phone among the given options, with a total stock of 4,324 units.\n2. The iPhone 7 has the lowest popularity, with only 874 units in stock, indicating a lower demand compared to other models.\n3. The iPhone 5s and iPhone X have moderate popularity, with 2,914 and 2,540 units in stock, respectively.\n4. The data suggests that older iPhone models (5s and 6s) still have a significant presence in the market, possibly due to their\naffordability and continued support from Apple.\n5. The lower stock of iPhone 7 could be attributed to its position between the older, more affordable models and the newer, more\nadvanced iPhone X, making it less appealing to consumers.\nQueried\nOnline\nInformation\n1. This graph shows the market share of mobile operating systems worldwide from May 2022 - May 2023. Apple has 31.12%,\nXiaomi has 23.42% and Samsung has ...\n2. Android maintained its position as the leading mobile operating system worldwide in the first quarter of 2023 with a market share\nof 71.4 ...\n3. Later, big data\u2014specifically, call detail record data collected from millions of cell phones\u2014was hyped as useful for stopping the\ndisease by tracking ...\n4. This research was commissioned to develop a more in depth understanding of smartphone user purchasing behaviour in the UK\nsmartphone market, ...\n5. Where does each iPhone rank versus its peers? Here\u2019s how we reckon they all compare, from the OG to the latest model.\n6. Welcome to the interactive ENISA Smartphone Guidelines Tool. To see information about the domains available click on the\nnames below:.\nAnalysis\nWith\nOnline\nInformation\n1. The iPhone 6s is the most popular phone among the given options, with a total stock of 4,324 units.\n2. The iPhone 5s comes in second place with a total stock of 2,914 units, indicating that older iPhone models still have a significant\npresence in the market.\n3. The iPhone X, a more recent model, has a lower total stock of 2,540 units, suggesting that it may not be as popular as the older\niPhone 6s and 5s models.\n4. The iPhone 7 has the lowest total stock of 874 units, indicating that it is the least popular among the given options.\n5. Despite the popularity of iPhones in this data, it is important to note that Android maintains a larger market share worldwide\n(71.4%), which may impact the overall popularity of iPhones in comparison to Android devices.\nTable 15: Comparison of the GPT-4 analysis results with and without online information.\n"
  },
  {
    "title": "Leveraging GPT-4 for Automatic Translation Post-Editing",
    "link": "https://arxiv.org/pdf/2305.14878.pdf",
    "upvote": "1",
    "text": "Leveraging GPT-4 for Automatic Translation Post-Editing\nVikas Raunak\nAmr Sharaf\nYiren Wang\nHany Hassan Awadalla\nArul Menezes\nMicrosoft Azure AI\nRedmond, Washington\n{viraunak,amrsharaf,wangyiren,hanyh,arulm}@microsoft.com\nAbstract\nWhile Neural Machine Translation (NMT) rep-\nresents the leading approach to Machine Trans-\nlation (MT), the outputs of NMT models still\nrequire translation post-editing to rectify errors\nand enhance quality under critical settings. In\nthis work, we formalize the task of direct trans-\nlation post-editing with Large Language Mod-\nels (LLMs) and explore the use of GPT-4 to\nautomatically post-edit NMT outputs across\nseveral language pairs. Our results demonstrate\nthat GPT-4 is adept at translation post-editing,\nproducing meaningful and trustworthy edits to\ntranslations that help improve its general qual-\nity as well as remove different classes of major\nerrors in translations. In particular, human eval-\nuations on assessing edit trustworthiness show\nthat GPT-4 exhibits a large improvement over\nthe prior state-of-the-art LLM. Notably, we\nimprove upon state-of-the-art performance on\nWMT-22 English-Chinese, English-German,\nChinese-English and German-English language\npairs using GPT-4 based post-editing, as eval-\nuated by state-of-the-art MT quality metrics.\nHowever, we also show that GPT-4 could pro-\nduce hallucinated edits, thereby urging caution\nin its use as an expert translation post-editor.\n1\nIntroduction\nState-of-the-art\nNeural\nMachine\nTranslation\n(NMT) models, trained on web-mined parallel\ncorpora suffer from reliability problems even\nfor higher resource language pairs, despite high\naverage case performance (He et al., 2020; Gupta\net al., 2020; Sun et al., 2020; Wang et al., 2021;\nHe et al., 2021; Raunak et al., 2022; Raunak and\nMenezes, 2022). A number of prior works have\ndemonstrated that the parallel data and model\ntraining artifacts in NMT could manifest in terms\nof catastrophic outputs in rare cases, and the\ndetection of such egregious model behaviors\nremains a challenging task (Raunak et al., 2021;\nTang et al., 2022; Guerreiro et al., 2023b; Xu\net al., 2023; Guerreiro et al., 2023a). Thereby,\npost-editing neural machine translations remains\nan important exercise for their use in critical\nsettings across the translation and localization\nindustry. As such, a relevant question to ask is\nwhether Large Language Models (LLMs) such\nas GPT-3, GPT-4 and PaLM, PaLM2 (Brown\net al., 2020; OpenAI, 2023; Anil et al., 2023;\nChowdhery et al., 2022), which have demonstrated\na wide-range of general purpose reasoning and\nknowledge-based capabilities could be leveraged\nfor the task of translation post-editing. LLM based\nautomatic translation post-editing could aid in\nboth detecting and fixing translation errors to\nensure greater reliability of NMT outputs. Besides\nalleviating reliability problems in NMT, leveraging\nLLMs for post-editing could be opportune owing\nto the advanced multi-lingual understanding\ncapabilities of latest LLMs, in particular GPT-4\n(OpenAI, 2023) and potentially, their ability\nto apply knowledge-based or culture-specific\ncustomization to translations (Baker, 2009), which\nare hard to ascertain only through the source text.\nIn this work, we explore the efficacy of state-of-\nthe-art LLMs such as GPT-4 on the task of trans-\nlation post-editing in a direct setting, i.e., without\nany quality-estimation or error detection step ap-\nplied to the translations prior to post-editing. Our\ncontributions are as follows: (i) We formalize the\ntask of direct translation post-editing with LLMs\nand posit a set of research questions and measure-\nments to quantify their utility for the goal of im-\nproving translations obtained from NMT models.\n(ii) We demonstrate that GPT-4 produces mean-\ningful, human-judgement aligned edits to state-of-\nthe-art translations with a high degree of trustwor-\nthiness. (iii) We demonstrate gains on translation\nquality over best systems from WMT-22 across a\nnumber of language pairs on the WMT-22 bench-\nmark (Koehn et al., 2022), improving upon the\nstate-of-the-art translation performance on English-\narXiv:2305.14878v2  [cs.CL]  23 Oct 2023\nExample of Translation Post-Editing using GPT-4 (Verbatim Outputs under the CoT Setting)\nSource: They were addressed to her son, who has autism and lives in a private care facility, she said.\nBut instead of her son\u2019s name inside when you opened them, the letters said Dear Maine\u2019s Department\nof Health and Human Services \u2013 in Cincinnati, she told local media.\nTranslation: Sie waren an ihren Sohn gerichtet, der Autismus hat und in einer privaten Pflegeeinrich-\ntung lebt, sagte sie. Aber anstelle des Namens ihres Sohnes im Inneren, als Sie sie \u00f6ffneten, hie\u00df es\nin den Briefen Dear Maine Dear Maine \u2019s Department of Health and Human ServicesServices - in\nCincinnati, sagte sie den lokalen Medien.\nProposed Improvements:\n1. Remove the repetition of \"Dear Maine\" in the German translation.\n2. Correct the translation of \"Dear Maine\u2019s Department of Health and Human Services\" to \"Sehr\ngeehrtes Department of Health and Human Services von Maine\".\n3. Replace \"ServicesServices\" with \"Services\".\n4. Add a comma after \"Cincinnati\" for better sentence structure.\nImproved Translation:\nSie waren an ihren Sohn gerichtet, der Autismus hat und in einer privaten Pflegeeinrichtung lebt, sagte\nsie. Aber anstelle des Namens ihres Sohnes im Inneren, als Sie sie \u00f6ffneten, hie\u00df es in den Briefen\n\"Sehr geehrtes Department of Health and Human Services von Maine\" - in Cincinnati, sagte sie den\nlokalen Medien.\nTable 1: Post-Editing Task Example: GPT-4 produces edits that result in improving the general quality of the\ntranslation as well as in removing undesirable artifacts across a range of NMT systems, as quantified in Section 3.\nChinese, English-German, Chinese-English and\nGerman-English using GPT-4 based post-editing,\nas evaluated by state-of-the-art MT quality metrics.\n2\nThe Translation Post-Editing Task\nTask Definition:\nWe formalize the post-editing\ntask in a generative setting as follows: given a\nSource (S) and a Translation (T), propose improve-\nments over the translation (E) and generate the\ntranslation with the proposed edits (T \u2032), i.e.:\n(S, T) \u2192 E + T \u2032\nUnder this task setting, E represents the im-\nprovements or the edits that are verbalized by a\nLLM. In the absence of E, the task is reduced to\nsimply generating the improved translation with-\nout any intermediate reasoning chain or Chain of\nThought (CoT) (Wei et al., 2022b; Kojima et al.,\n2022). Throughout this work, we refer to the post-\nediting task in the above zero-shot CoT setting as\npost-editing with CoT and the setting without E as\npost-editing without CoT. Table 1 shows an input-\noutput example for the post-editing task under the\nCoT setting. Additionally, throughout this work,\nwe refer to Z as the zero-shot translation of a given\nsource (S) obtained from the same LLM that is\nemployed for the post-editing task. Through this\nformalization, we posit and investigate the follow-\ning research questions (RQ):\n(RQ1) Nature of Post-Edited Translations:\nLLMs have been shown to generate high quality,\nstate-of-the-art translations (Hendy et al., 2023)\nacross a number of language pairs in a zero-shot\nsetting. As such, during post-editing, LLMs could\ngenerate a translation that is incognizant of the\nprovided initial translation. Hence, we investigate\nwhether during translation post-editing LLMs gen-\nerate the improved translations from scratch (i.e.\nonly based on the source S) or do they edit the\ninitial translation T provided as per the instructions\nof the task. A related question is characterizing\nthe role of CoT in determining the nature of the\npost-edited translation, i.e. whether the post-edited\ntranslation is closer to the initial translation or to\nthe zero-shot translation produced by the LLM.\n(RQ2) General Quality Improvements:\nDo the\npost-edited translations produced by LLMs lead\nto general quality improvements as measured by\nstate-of-the-art MT quality metrics? Another re-\nlated question is whether the post-editing chain-\nof-thought is helpful towards translation quality\nimprovement? Even though zero-shot chain-of-\nthought has been demonstrated to be effective\nacross reasoning tasks, we hypothesize that trans-\nResearch Question\nMeasurement\nDatasets\nNature of the Post-Edited Translation\nTER(T \u2032, Zero-Shot) vs TER(T \u2032, T)\nWMT-22\nGeneral Quality Improvements\nCOMET\u2217(S, T) vs COMET\u2217(S, T \u2032)\nWMT-22\nEdits On Human Annotated Error Spans\nEdit Efficacy over Error Spans (E3S)\nWMT-20, 21, 22\nTrustworthiness of the Proposed Edits\nEdit Realization Rate (ERR)\nWMT-22\nTable 2: Measuring Post-Editing Efficacy: Given, the Source S, Translation T and the Post-Edited Output T \u2032,\nwe explore the four research questions in section 2 through experiments on the corresponding datasets, using the\nproposed measurements described in detail in Section 4. COMET\u2217 represents any of the COMET MT metrics.\nlation post-editing task might not require the same\ndegree of variable computation that makes CoT ef-\nfective (Kojima et al., 2022), owing to the lack of\nmultiple reasoning steps involved in the task.\n(RQ3) Modifying Human Annotated Error\nSpans:\nAre LLMs capable of modifying human\nannotated translation error spans during the post-\nediting step? A high frequency of modifications\nmade to the human annotated error spans, espe-\ncially if it is accompanied by general quality im-\nprovements, would signify a greater correlation\nwith human judgement in identifying errors in\ntranslations.\n(RQ4) Trustworthiness of the Proposed Edits:\nDo the edits proposed as CoT actually appear in\nthe improved translation produced by LLMs, in\nthe post-editing with CoT setting? It is quite con-\nceivable that LLMs might make edit proposals or\nproduce chain of thought that is not realized in the\nfinal post-edited translation produced by the same\nmodel (Ye and Durrett, 2022; Turpin et al., 2023).\nHowever, if the post-editing explanation or CoT is\na desiderata of the translation post-editing process,\nit becomes critical to examine the fidelity of the\nproposed edits in addition to the final translation\nquality. A higher realization rate of the proposed\nedits would also help establish the trustworthiness\nof the LLM as an expert translation post-editor.\nIn the next sections, we explore the above four\nresearch questions using the state-of-the-art LLMs.\nWe describe our experimental settings in Section 3\n& present the results in Section 4.\n3\nExperimental Settings\nDatasets:\nWe experiment with WMT-22 General\nMT translation task datasets (Kocmi et al., 2022) as\nwell as with WMT-20 and WMT-21 News transla-\ntion task submissions annotated with MQM (Mul-\ntidimensional Quality Metrics Framework) errors1\nFreitag et al. (2021). For the post-editing exper-\niments pertaining to the MQM annotated WMT-\n20 and WMT-21 system outputs, we experiment\nwith samples that have a major error as an annota-\ntion, whereas we experiment with the full WMT-22\ndatasets throughout. We use the latest WMT-22 test\nsets for the majority of our experiments, the cura-\ntion of which falls beyond the training cut-off dates\nfor GPT-4 and other LLMs under investigation2.\nLLMs and Baselines:\nWe experiment with GPT-\n4 and gpt-3.5-turbo in our experiments. These\nmodels represent the most capable publically avail-\nable LLMs (Liang et al., 2022). We use a prompt\nthat describes the system role as a translation post-\neditor and under the CoT setting, instruct the LLM\nto propose improvements to the provided transla-\ntion (T) of a given source (S), before producing\nthe final post-edited translation (T \u2032).\nFor post-\nediting, we experiment under three settings: (i)\npost-editing with CoT, (ii) post-editing without\nCoT and (iii) post-editing with Structured-CoT\n(SCoT). The SCoT baseline uses the MQM annota-\ntion instructions from Freitag et al. (2021) to pro-\nduce the intermediate CoT in the form of an MQM\nannotation over the source-translation pair. We de-\nscribe the prompts used for the three baselines in\nappendix C. For producing the initial translations\non WMT-22, we use Microsoft-Translator, one of\nthe strongest publically available MT systems (Rau-\nnak et al., 2022; Hendy et al., 2023). For WMT-20\nand WMT-21 systems, we take the translations pro-\nvided by the different NMT systems and annotated\nin Freitag et al. (2021) as the initial translations\nupon which post-editing is applied.\n1https://github.com/google/\nwmt-mqm-human-evaluation\n2LLMs: https://platform.openai.com/docs/models\nMetrics and Evaluation:\nFor each of the four\nresearch questions posed, we use the metrics high-\nlighted in Table 2.\nWe explain these measure-\nments in the relevant sections. For general qual-\nity measurements, we use four COMET (Rei\net al., 2020) models3: the reference-free COMET-\nQE (wmt20-comet-qe-da), COMET-KIWI (wmt-\n22-cometkiwi-da) Quality Estimation (QE) models\nand the reference-based COMET-20 (wmt20-comet-\nda) and COMET-22 (wmt22-comet-da) models. To\nmeasure the similarity of translations, we use the\nTranslation Edit Rate (TER) (Snover et al., 2006)\nimplementation from SacreBLEU (Post, 2018).\n4\nResults and Measurements\n4.1\nNature of Post-Edited Translations\nTo measure whether the post-edited translations\nproduced by LLMs adhere to editing the initial\ntranslations provided, we compute the Translation\nEdit Rate (TER) (Snover et al., 2006) of the post-\nedited translation against the zero-shot translations\nobtained using the same LLM, and compare it with\nthe TER of the post-edited translation against the\ninitial translation. A higher value of TER (T \u2032, T)\nimplies that the post-edited translation (T \u2032) is closer\nto the initial translation (T) and that the LLM ad-\nheres to the task of editing the initial translation.\nPE Setting\nTER (T \u2032, Z)\nTER (T \u2032, T)\nWith CoT\n92.0\n70.3\nWithout CoT\n84.6\n94.5\nTable 3: WMT-22 En-Zh: The post-edited translations\n(T \u2032) are closer to the initial translations (T) than the\nzero-shot translations (Z) in the CoT setting.\nPE Setting\nTER (T \u2032, Z)\nTER (T \u2032, T)\nWith CoT\n42.9\n22.0\nWithout CoT\n38.1\n34.9\nTable 4: WMT-22 Zh-En: The post-edited translations\n(T \u2032) are closer to the initial translations (T) than the\nzero-shot translations (Z) in the CoT setting.\nImpact of CoT:\nTable 3 describes our results\non WMT-22 En-Zh and Table 4 describes our re-\nsults on Zh-En with post-editing using GPT-4. We\nfind that CoT constrains the final translations to be\ncloser to the initial translation. In the post-editing\n3COMET: https:/github.com/Unbabel/COMET\nsetting without CoT, the final translation is closer\nto the zero-shot translation, even though the TER\ndifference is much smaller than the difference in\nthe CoT setting.\nDiscussion\nWe find that the above results hold\ntrue across different metrics such as edit distance,\nBLEU (Post, 2018) or ChrF (Popovi\u00b4c, 2015) as\nwell as across WMT-22 language pairs and gpt-3.5-\nturbo. This result also shows a peculiar side-effect\nof the post-editing task under the CoT setting \u2013 that\npost-editing a system translation might end up lead-\ning to a lower quality final translation if the initial\ntranslation is lower in quality than the zero-shot\ntranslation quality of the LLM under consideration.\nIn the next sub-section, we evaluate GPT-4 under\ndifferent post-editing settings in terms of general\nquality improvements.\n4.2\nGeneral Quality Improvements\nWe compare the translation quality of the post-\nedited translation against the initial translation us-\ning both reference-free and reference-based state-\nof-the-art neural MT quality metrics.\nResults:\nTables 5, 7, 6 and 8 provide the re-\nsults of the experiments done on the WMT-22 test\nsets. Throughout, we find that post-editing un-\nder both CoT and direct settings leads to improve-\nments over high-quality initial translations obtained\nthrough MS-Translator. Further, Tables 5, 7, 6 and\n8 show that direct post-editing of MS-Translator\noutputs with GPT-4 consistently improves upon the\nWMT-22-Best translation system quality. We also\nfind that gpt-3.5-turbo consistently underperforms\nGPT-4 as well as the quality of initial translations,\ndemonstrating a qualitative jump in post-editing\nefficacy of GPT-4.\n4.3\nEdits On Human Annotated Error Spans\nWe use the MQM annotated WMT-22 system\noutputs provided by Freitag et al. (2021) and\nmeasure\nwhether\nthe\npost-edited\ntranslation\nmodifies the translation error span as annotated by\nhuman annotators. For each of the Major MQM\nerror spans modified, we record a score of 1, else a\nscore of 0. The final score reported, named Edit\nEfficacy over Erroneous Error Spans (E3S) is\nhigher if more of the erroneous spans have been\nmodified in the post-edited translation. The E3S\nmetric is reported as a percentage over the test set.\nSystem\nCOMET-KIWI\nCOMET-QE\nCOMET-22\nCOMET-20\nWMT-Best\n81.38\n39.96\n85.04\n56.60\nMS Translator\n81.04\n38.64\n84.68\n55.28\nMS Translator + GPT-4\n81.66\n42.15\n85.41\n58.21\nMS Translator + GPT-4-CoT\n81.40\n41.05\n85.28\n57.84\nMS Translator + GPT-4-SCoT\n81.39\n41.40\n85.18\n57.45\nMS Translator + GPT-3.5-CoT\n79.32\n41.56\n82.71\n44.82\nGPT-4-Zero-Shot\n81.51\n41.36\n85.26\n57.53\nTable 5: General Quality Improvements on WMT-22 De-En: The + sign reflects that the post-editing is applied\non the initial translations produced by the given System. MS-Translator + GPT-4 obtains the best performance.\nSystem\nCOMET-KIWI\nCOMET-QE\nCOMET-22\nCOMET-20\nWMT-Best\n77.66\n23.98\n81.02\n45.21\nMS Translator\n77.58\n23.97\n80.35\n40.40\nMS Translator + GPT-4\n79.75\n31.84\n82.79\n53.42\nMS Translator + GPT-4-CoT\n79.02\n28.96\n82.20\n50.77\nMS Translator + GPT-4-SCoT\n78.94\n28.80\n82.09\n49.65\nMS Translator + GPT-3.5-CoT\n79.32\n41.56\n82.71\n44.82\nGPT-4-Zero-Shot\n79.29\n30.13\n82.49\n51.78\nTable 6: General Quality Improvements on WMT-22 Zh-En: The + sign reflects that the post-editing is applied\non the initial translations produced by the given System. MS-Translator + GPT-4 obtains the best performance.\nSystem\nCOMET-KIWI\nCOMET-QE\nCOMET-22\nCOMET-20\nWMT-Best\n83.56\n44.67\n87.21\n62.35\nMS Translator\n83.35\n43.48\n86.78\n62.06\nMS Translator + GPT-4\n83.69\n44.50\n87.37\n62.85\nMS Translator + GPT-4-CoT\n83.32\n43.96\n87.13\n62.62\nMS Translator + GPT-4-SCoT\n83.12\n44.17\n86.90\n61.94\nMS Translator + GPT-3.5-CoT\n81.36\n43.12\n84.55\n50.52\nGPT-4-Zero-Shot\n82.95\n44.69\n86.80\n60.85\nTable 7: General Quality Improvements on WMT-22 En-De: The + sign reflects that the post-editing is applied\non the initial translations produced by the given System. MS-Translator + GPT-4 obtains the best performance.\nSystem\nCOMET-KIWI\nCOMET-QE\nCOMET-22\nCOMET-20\nWMT-Best\n82.04\n32.11\n86.69\n61.04\nMS Translator\n81.39\n31.46\n86.11\n59.43\nMS Translator + GPT-4\n82.68\n34.47\n87.53\n63.21\nMS Translator + GPT-4-CoT\n81.60\n32.01\n86.43\n59.97\nMS Translator + GPT-4-SCoT\n81.81\n32.56\n86.56\n60.20\nMS Translator + GPT-3.5-CoT\n79.32\n41.56\n82.71\n44.82\nGPT-4-Zero-Shot\n81.73\n32.61\n86.51\n58.66\nTable 8: General Quality Improvements on WMT-22 En-Zh: The + sign reflects that the post-editing is applied\non the initial translations produced by the given System. MS-Translator + GPT-4 obtains the best performance.\nSystem\nInitial-QE PE-QE\nE3S\nPROMT\n74.58\n78.90 57.86 %\nM2M100\n73.06\n79.64 70.30 %\nQUARTZ\n79.36\n80.38 63.63 %\nJDExplore\n79.43\n79.61 52.32 %\nAverage\n76.61\n79.63 61.03 %\nTable 9: On WMT-22 En-De System Outputs with\nMajor MQM-annotated Errors, Post-Editing with GPT-4\nincreases translation quality considerably and modifies\nmore than sixty percent of the erroneous spans. The\nresults are agnostic to the MT quality estimation metric.\nSystem\nInitial-QE PE-QE\nE3S\nAISP-SJTU\n71.87\n75.66 71.62 %\nLan-Bridge\n75.52\n75.82 64.14 %\nLanguageX\n72.80\n75.80 69.34 %\nM2M100\n68.24\n76.49 80.72 %\nAverage\n72.11\n75.94 71.46 %\nTable 10: On WMT-22 Zh-En System Outputs with\nMajor MQM-annotated Errors, Post-Editing with GPT-4\nincreases translation quality considerably and modifies\nmore than seventy percent of the erroneous spans. The\nresults are agnostic to the MT quality estimation metric.\nResults: Tables 9, 10 and 11 report the results\nobtained on 14 different WMT-22 NMT system\noutputs from WMT-22, over three language pairs:\nEnglish-German, Chinese-English and English-\nRussian. We find that GPT-4 produces E3S rates\nabove fifty percent with considerably large gains in\ngeneral quality (measured through COMET-KIWI),\nsignifying that it is able to remove the undesir-\nable artifacts (spans) present in the translations.\nWe repeat this experiment on WMT-20 and WMT-\n21 MQM annotated System outputs as well in ap-\npendix A, with similar results.\n4.4\nTrustworthiness of the Proposed Edits\nIn a practical setting, the edits (E) produced in\nthe post-editing task might be useful to illustrate\nthe changes made by the LLM in the post-edited\ntranslation. Therefore, the fidelity of the proposed\nedits is important for imparting more trust in the\nLLM based post-editing process. Thereby, the\nquestion whether the proposed edits are present\nin the final improved translation or are hallucinated\nby the model is of significant practical interest. We\nquantify this property using Edit Realization Rate\n(ERR), which measures: of the proposed edits (E)\nSystem\nInitial-QE PE-QE\nE3S\neTranslation\n73.52\n78.61 51.42 %\nHuaweiTSC\n75.30\n79.22 53.35 %\nM2M100\n74.80\n79.93 55.70 %\nPROMT\n75.82\n78.96 48.82 %\nQUARTZ\n77.87\n81.02 67.81 %\nJDExplore\n78.91\n78.45 41.18 %\nAverage\n76.04\n79.37 53.04 %\nTable 11: On WMT-22 En-Ru System Outputs with\nMajor MQM-annotated Errors, Post-Editing with GPT-4\nincreases translation quality considerably and modifies\nmore than fifty percent of the erroneous spans, on aver-\nage. The results are agnostic to the MT QE metrics.\nby the LLM in the CoT post-editing setting, how\nmany of the edits were actually realized in the im-\nproved translation? Since, we do not have any\nground truth data to quantify this, we use human\nevaluation for measuring this property.\nERR Human Evaluation Protocol:\nWe ask hu-\nman annotators (bilingual and native in the target\nlanguage) to label 100 post-editing samples for\nboth En-De and De-En from the WMT-22 test sets,\ngenerated by both gpt-3.5-turbo and GPT-4. The\nannotator is asked to identify if all of the proposed\nedits were realized in the final translation (ALL)\nor whether a partial number of proposed edits were\nrealized (PARTIAL) or whether none of the pro-\nposed edits were realized (NONE). The human\nannotator thereby labels each post-editing sample\n(S, T, E, T \u2032) with one of the three labels.\nEN-DE, Turbo\nEN-DE, GPT-4\nDE-EN, Turbo\nDE-EN, GPT-4\nLanguage Pair and LLM Combination\n0\n20\n40\n60\n80\n100\nHuman Annotation Count\nEdit Realization Rate (ERR) Comparison\nALL\nPARTIAL\nNONE\nFigure 1: Edit Realization Rate (ERR) Human Eval-\nuation on WMT-22 En-De and De-En: GPT-4 obtains\nhigher ERR than gpt-3.5-turbo, imparting greater trust-\nworthiness to the post-editing process under the CoT\nsetting. In conjunction with this result, we also observe\nthat GPT-4 obtains better ERR score distributions on\nEn-Zh translation post-editing, vs Zh-En post-editing.\nIllustration: Edit Realization Rate Difference between GPT-Turbo and GPT-4\nSource: Stephanie Lay said she found the piles of letters from the insurance company stashed in her\nmailbox between Thursday and Monday.\nTranslation: Stephanie Lay sagte, sie habe die Briefstapel der Versicherung zwischen Donnerstag und\nMontag in ihrem Briefkasten versteckt gefunden.\ngpt-3.5-turbo\nProposed Improvements:\n1. The word \"stashed\" is not adequately translated in the German text.\n2. The word \"mailbox\" is not translated correctly in the German text.\n3. The word \"piles\" is not translated correctly in the German text.\n4. The word \"found\" is not translated adequately in the German text.\n5. The word \"between\" is not translated correctly in the German text.\nImproved Translation:\nStephanie Lay sagte, sie habe zwischen Donnerstag und Montag im Briefkasten Stapel von Briefen der\nVersicherung gefunden.\nGPT-4\nProposed Improvements:\n1. Replace \"Briefstapel der Versicherung\" with \"Stapel von Briefen der Versicherungsgesellschaft\" to\nbetter convey the meaning of \"piles of letters from the insurance company\".\n2. Remove \"versteckt\" as it is not supported in the input and is not necessary for the meaning of the\nsentence.\nImproved Translation:\nStephanie Lay sagte, sie habe die Stapel von Briefen der Versicherungsgesellschaft zwischen Donner-\nstag und Montag in ihrem Briefkasten gefunden.\nTable 12: Quantifying Edit Realization Rate (ERR): The example shows an instance of the Proposed Edits (E)\nand Improved Translation (T \u2032) obtained using gpt-3.5-turbo and GPT-4. We find that GPT-4\u2019s edit proposals are\nincluded in the final translation with a far greater frequency. We quantify this property in Section 4.4.\nERR Human Evaluation Results:\nThe results\nof human evaluations are presented in Figure 1.\nIn general, for both En-De and De-En, there ex-\nists a large gap between the ERR distribution of\ngpt-3.5-turbo and GPT-4. We present a typical post-\nediting example illustrating this difference in Ta-\nble 12. Our findings suggest that GPT-4 produces\nmore trustworthy generations for the post-editing\ntask and combined with results in sub-sections 4.1,\n4.2 and 4.3, this suggests that GPT-4 could aid in\nautomatic post-editing with considerably greater\ninterpretability. We observed similarly high ERR\nfor En-Zh as well, although the human evaluations\nstill report cases where the edits by GPT-4 are not\nrealized fully, especially in language pairs where\nthe source language is not English (Zh-En and De-\nEn). We present examples of such hallucinated\nedit proposals in appendix D, showing that GPT-\n4 might present similar reliability challenges as a\npost-editor as NMT does in translation.\n5\nFurther Discussion\nPost-Editing Across Language Pairs:\nWe re-\nport the GPT-4 post-editing performance under the\nCoT setting with MS-Translator as the initial trans-\nlation for several other language pairs in appendix\nB. In general, the results show that GPT-4 based\npost-editing leads to consistent gains in translation\nquality across language pairs, with larger gains for\nX-E translations.\nUtility of the Chain-of-Thought:\nOur results\nshow that the inclusion of the edit proposals (CoT)\nin the post-editing step is detrimental towards the\nquality of the post-edited translations, but is useful\nin constraining the post-edited outputs to the initial\ntranslations. Therefore, the necessity of variable\ncomputation leveraged by the zero-shot chain-of-\nthought step is questionable for the post-editing\ntask, even though the edit artifacts produced by the\nGPT-4 might themselves be valuable for making\nthe automatic post-editing task more trustworthy.\nFurther, we also found that imposing a specialized\nstructure on the edit proposals in the form of MQM\nerror categories was not valuable towards improv-\ning the final post-edited translation quality. On the\nother hand, we also demonstrated that imposing\nstructure upon the proposed edits in the form of\nMQM annotations doesn\u2019t hurt the general quality\nof the post-edited translations, suggesting that it\nmight be possible to combine MQM based auto-\nmatic quality assessment along with post-editing\nthrough GPT-4. However, evaluating whether post-\nediting could be done in conjunction with MQM\nbased quality assessment is beyond the scope of\nour work and we leave such a joint evaluation to\nfuture work.\nTrustworthiness of the Proposed Edits:\nWe\ndemonstrated that the edits proposed by GPT-4 are\nrealized in the final translation with considerably\nhigher frequency than gpt-3.5-turbo. This shows\na quantitative jump in the trustworthiness of the\nedits proposed by the LLM for the direct automatic\npost-editing task under our formalization. This\njump represents an emergent ability in terms of\nbreakthrough performance on ERR for the two lan-\nguage pairs under consideration, under a common\ndefinition of emergent abilities (Wei et al., 2022a;\nSchaeffer et al., 2023).\n6\nRelated Work\nAutomatic Post-Editing of Translations:\nThere\nexists a long line of prior work on building neu-\nral models for the automatic post-editing (APE)\ntask (Vu and Haffari, 2018; Shterionov et al., 2020;\nChatterjee, 2019; G\u00f3is et al., 2020; Correia and\nMartins, 2019a; Voita et al., 2019; Chollampatt\net al., 2020; do Carmo et al., 2021). Shterionov\net al. (2020) presented a comprehensive road-map\nfor APE, highlighting the challenges and potential\ndirections for future research. Chatterjee (2019)\nexplored the use of deep learning techniques for\nAPE and proposed novel architectures to improve\nthe quality of post-edited translations, while G\u00f3is\net al. (2020) focused on learning strategies for APE\nand investigated the use of automatic orderings\ntechniques to refine translations. Correia and Mar-\ntins (2019b) proposed a simple yet effective neural\nmodel for APE using transfer learning, demonstrat-\ning promising results.\nVoita et al. (2019) introduced a context-aware\napproach to APE, incorporating source context in-\nformation into the neural model to generate more\naccurate post-edits. Chollampatt et al. (2020) ex-\nplored the use of APE to improve the overall trans-\nlation quality for NMT models. They investigate\nthe effects of varying training data sizes, using arti-\nficial training data, and domain specificity for the\nAPE task. In a comprehensive review, do Carmo\net al. (2021) provided an overview of various tech-\nniques and approaches in the field of APE, covering\nboth traditional and neural-based methods. Overall,\na number of prior studies have explored different\narchitectures, learning strategies, and contextual in-\nformation integration in neural models (non-LLM)\nto improve the quality of post-edited translations.\nLeveraging LLMs for Post-Editing:\nVidal et al.\n(2022) explored the use of GPT-3 based post-\nediting using glossaries, however, to the best of\nour knowledge, we present the first work that in-\nvestigates using GPT-4 for automatic post-editing\nof translations and presents a formalization of the\ndirect post-editing task under a purely generative\nsetting. Our work is also related to a number of\nworks exploring the using of LLMs for translation\n(Hendy et al., 2023; Gao et al., 2023; Lu et al.,\n2023; Vilar et al., 2022; Garcia et al., 2023), how-\never the focus of our work, the task of direct auto-\nmatic post-editing, is different from existing works.\n7\nSummary and Conclusions\nWe formalized the task of direct automatic post-\nediting in a generative setting and posited a set of\nresearch questions and measurements to quantify\nthe utility of the state-of-the-art LLM, GPT-4 on\nthe task. Through this formalization, we demon-\nstrated that zero-shot chain-of-thought is critical in\nconstraining the post-edited translation to be close\nto the initial translation. We also demonstrated\nthat GPT-4 produces meaningful human-judgement\naligned edits to translations that also lead to general\nquality improvements, as evaluated on the WMT-\n22 test sets. Further, we demonstrated that the\nedit generation process in GPT-4 is considerably\nmore trustworthy than a previous generation of\nLLM. Overall, we demonstrated promising results\non post-editing with GPT-4, improving upon the\nWMT-22 Best translation performance on English-\nChinese, English-German, Chinese-English and\nGerman-English language pairs.\n8\nLimitations\nWe proposed a formalization to study direct au-\ntomatic post-editing with state-of-the-art LLMs\nand investigated a number of research questions\nthrough this formalization. However, we only have\nAPI-level access to GPT-4. Even though we con-\nducted our experiments on WMT-22 test sets and\nsystem outputs, the curation of which falls outside\nthe cut-off date for GPT-4 training data; due to only\nblack-box access to the model, we cannot rule out\nthe possibility of data contamination, even on the\nWMT-22 test sets.\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nM. Baker. 2009. Translation Studies. Number v. 1 in\nCritical concepts in linguistics. Routledge.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nRajen Chatterjee. 2019. Automatic post-editing for ma-\nchine translation. arXiv preprint arXiv:1910.08592.\nShamil Chollampatt, Raymond Hendy Susanto, Liling\nTan, and Ewa Szymanska. 2020. Can automatic post-\nediting improve NMT? In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2736\u20132746, On-\nline. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, et al. 2022. Palm: Scaling language\nmodeling with pathways.\nGon\u00e7alo M. Correia and Andr\u00e9 F. T. Martins. 2019a.\nA simple and effective approach to automatic post-\nediting with transfer learning. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 3050\u20133056, Florence, Italy.\nAssociation for Computational Linguistics.\nGon\u00e7alo M. Correia and Andr\u00e9 F. T. Martins. 2019b.\nA simple and effective approach to automatic post-\nediting with transfer learning. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 3050\u20133056, Florence, Italy.\nAssociation for Computational Linguistics.\nF\u00e9lix do Carmo, Dimitar Shterionov, Joss Moorkens,\nJoachim Wagner, Murhaf Hossari, Eric Paquin, Dag\nSchmidtke, Declan Groves, and Andy Way. 2021.\nA review of the state-of-the-art in automatic post-\nediting. Machine Translation, 35:101\u2013143.\nMarkus Freitag, George Foster, David Grangier, Viresh\nRatnakar, Qijun Tan, and Wolfgang Macherey. 2021.\nExperts, errors, and context: A large-scale study of\nhuman evaluation for machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1460\u20131474.\nYuan Gao, Ruili Wang, and Feng Hou. 2023. How to\ndesign translation prompts for chatgpt: An empirical\nstudy.\nXavier Garcia, Yamini Bansal, Colin Cherry, George\nFoster, Maxim Krikun, Fangxiaoyu Feng, Melvin\nJohnson, and Orhan Firat. 2023. The unreasonable\neffectiveness of few-shot learning for machine trans-\nlation. arXiv preprint arXiv:2302.01398.\nAnt\u00f3nio G\u00f3is, Kyunghyun Cho, and Andr\u00e9 Martins.\n2020.\nLearning non-monotonic automatic post-\nediting of translations from human orderings. arXiv\npreprint arXiv:2004.14120.\nNuno M. Guerreiro, Pierre Colombo, Pablo Piantanida,\nand Andr\u00e9 F. T. Martins. 2023a. Optimal transport\nfor unsupervised hallucination detection in neural\nmachine translation.\nNuno M. Guerreiro, Elena Voita, and Andr\u00e9 Martins.\n2023b. Looking for a needle in a haystack: A com-\nprehensive study of hallucinations in neural machine\ntranslation. In Proceedings of the 17th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics, pages 1059\u20131075, Dubrovnik,\nCroatia. Association for Computational Linguistics.\nShashij Gupta, Pinjia He, Clara Meister, and Zhendong\nSu. 2020. Machine translation testing via pathologi-\ncal invariance. In Proceedings of the 28th ACM Joint\nMeeting on European Software Engineering Confer-\nence and Symposium on the Foundations of Software\nEngineering, ESEC/FSE 2020, page 863\u2013875, New\nYork, NY, USA. Association for Computing Machin-\nery.\nPinjia He, Clara Meister, and Zhendong Su. 2020.\nStructure-invariant testing for machine translation.\nIn Proceedings of the ACM/IEEE 42nd International\nConference on Software Engineering, ICSE \u201920, page\n961\u2013973, New York, NY, USA. Association for Com-\nputing Machinery.\nPinjia He, Clara Meister, and Zhendong Su. 2021. Test-\ning machine translation via referential transparency.\nIn 2021 IEEE/ACM 43rd International Conference on\nSoftware Engineering (ICSE), pages 410\u2013422. IEEE.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\nTom Kocmi, Rachel Bawden, Ond\u02c7rej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Nov\u00e1k, Martin\nPopel, and Maja Popovi\u00b4c. 2022. Findings of the 2022\nconference on machine translation (WMT22). In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 1\u201345, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nPhilipp Koehn, Lo\u00efc Barrault, Ond\u02c7rej Bojar, Fethi\nBougares, Rajen Chatterjee, Marta R. Costa-juss\u00e0,\nChristian Federmann, Mark Fishel, Alexander Fraser,\nMarkus Freitag, Yvette Graham, Roman Grund-\nkiewicz, Paco Guzman, Barry Haddow, Matthias\nHuck, Antonio Jimeno Yepes, Tom Kocmi, Andr\u00e9\nMartins, Makoto Morishita, Christof Monz, Masaaki\nNagata, Toshiaki Nakazawa, Matteo Negri, Aur\u00e9lie\nN\u00e9v\u00e9ol, Mariana Neves, Martin Popel, Marco Turchi,\nand Marcos Zampieri, editors. 2022. Proceedings\nof the Seventh Conference on Machine Translation\n(WMT). Association for Computational Linguistics,\nAbu Dhabi, United Arab Emirates (Hybrid).\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances in\nNeural Information Processing Systems, volume 35,\npages 22199\u201322213. Curran Associates, Inc.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nQingyu Lu, Baopu Qiu, Liang Ding, Liping Xie, and\nDacheng Tao. 2023. Error analysis prompting en-\nables human-like translation evaluation in large lan-\nguage models: A case study on chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nMaja Popovi\u00b4c. 2015. chrF: character n-gram F-score\nfor automatic MT evaluation. In Proceedings of the\nTenth Workshop on Statistical Machine Translation,\npages 392\u2013395, Lisbon, Portugal. Association for\nComputational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186\u2013\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nVikas Raunak and Arul Menezes. 2022. Finding memo:\nExtractive memorization in constrained sequence\ngeneration tasks.\nIn Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5153\u20135162, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nVikas Raunak, Arul Menezes, and Marcin Junczys-\nDowmunt. 2021. The curious case of hallucinations\nin neural machine translation. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1172\u20131183,\nOnline. Association for Computational Linguistics.\nVikas Raunak, Matt Post, and Arul Menezes. 2022.\nSALTED: A framework for SAlient long-tail transla-\ntion error detection. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5163\u20135179, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685\u20132702, Online. Association\nfor Computational Linguistics.\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo.\n2023. Are emergent abilities of large language mod-\nels a mirage?\nDimitar Shterionov, F\u00e9lix do Carmo, Joss Moorkens,\nMurhaf Hossari, Joachim Wagner, Eric Paquin, Dag\nSchmidtke, Declan Groves, and Andy Way. 2020. A\nroadmap to neural automatic post-editing: an empiri-\ncal approach. Machine Translation, 34:67\u201396.\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea\nMicciulla, and John Makhoul. 2006. A study of trans-\nlation edit rate with targeted human annotation. In\nProceedings of the 7th Conference of the Association\nfor Machine Translation in the Americas: Technical\nPapers, pages 223\u2013231, Cambridge, Massachusetts,\nUSA. Association for Machine Translation in the\nAmericas.\nZeyu Sun, Jie M. Zhang, Mark Harman, Mike Pa-\npadakis, and Lu Zhang. 2020. Automatic testing\nand improvement of machine translation. In Proceed-\nings of the ACM/IEEE 42nd International Conference\non Software Engineering, ICSE \u201920, page 974\u2013985,\nNew York, NY, USA. Association for Computing\nMachinery.\nJo\u00ebl Tang, Marina Fomicheva, and Lucia Specia. 2022.\nReducing hallucinations in neural machine transla-\ntion with feature attribution.\nMiles Turpin, Julian Michael, Ethan Perez, and\nSamuel R. Bowman. 2023. Language models don\u2019t\nalways say what they think: Unfaithful explanations\nin chain-of-thought prompting.\nBlanca Vidal, Albert Llorens, and Juan Alonso. 2022.\nAutomatic post-editing of MT output using large lan-\nguage models. In Proceedings of the 15th Biennial\nConference of the Association for Machine Transla-\ntion in the Americas (Volume 2: Users and Providers\nTrack and Government Track), pages 84\u2013106, Or-\nlando, USA. Association for Machine Translation in\nthe Americas.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance. arXiv preprint arXiv:2211.09102.\nElena Voita, Rico Sennrich, and Ivan Titov. 2019.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 877\u2013886, Hong Kong, China. Association for\nComputational Linguistics.\nThuy-Trang Vu and Gholamreza Haffari. 2018. Au-\ntomatic post-editing of machine translation: A neu-\nral programmer-interpreter approach. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 3048\u20133053,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJun Wang, Chang Xu, Francisco Guzm\u00e1n, Ahmed\nEl-Kishky, Benjamin Rubinstein, and Trevor Cohn.\n2021. As easy as 1, 2, 3: Behavioural testing of\nNMT systems for numerical translation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 4711\u20134717, Online.\nAssociation for Computational Linguistics.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certifica-\ntion.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nWeijia Xu, Sweta Agrawal, Eleftheria Briakou, Mari-\nanna J. Martindale, and Marine Carpuat. 2023. Un-\nderstanding and detecting hallucinations in neural\nmachine translation via model introspection.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot prompting for textual rea-\nsoning.\nA\nExperiments on WMT-20 and WMT-21\nIn this section, we report the post-editing perfor-\nmance of GPT-4 (Table 13) and gpt-3.5-turbo (Ta-\nble 14) for En-De, on WMT-20 and WMT-21 sys-\ntem outputs with Major errors as provided by Fre-\nitag et al. (2021).\nB\nExperiments on More Language Pairs\nWe report further results with GPT-4 based post-\nediting under the CoT setting in Table 15.\nC\nPrompts Used for Post-Editing\nTables 16 and 17 list the prompts used for the base-\nlines. The baseline without CoT only uses Step 2\nin the user prompt, and the same system prompt as\nthe one in CoT baseline.\nD\nEdit Hallucinations by GPT-4\nTables 18, 19, 20, 21 present examples where the\nhuman annotators reported only a \u2019PARTIAL\u2019 num-\nber of edits realized in the final translation.\nSystem\nInitial-QE PE-QE\nE3S\nWMT-20\nTohoku\n80.93\n82.59 53.01 %\nOPPO\n79.25\n82.32 56.11 %\neTranslation\n78.82\n82.72 57.04 %\nTencent\n80.03\n82.29 52.84 %\nHuoshan\n78.49\n82.26 56.33 %\nWMT-21\nVolcTrans-GLAT\n80.22\n82.68 39.13 %\nFacebook-AI\n82.88\n82.73 38.16 %\nHuaweiTSC\n80.98\n82.70 64.65 %\nUEdin\n80.82\n81.34 46.67 %\neTranslation\n80.04\n81.60 74.05 %\nTable 13: Edit Efficacy over Erroneous Spans with\nGPT-4: Post-Editing with GPT-4 modifies more than\nhalf of the erroneous spans on average.\nSystem\nInitial-QE PE-QE E3S\nWMT-20\nTohoku\n80.93\n82.50 69.86\nOPPO\n79.25\n81.59 73.49\neTranslation\n78.82\n82.03 73.45\nTencent\n80.03\n82.49 71.48\nHuoshan\n78.49\n82.34 71.72\nWMT-21\nVolcTrans-GLAT\n80.22\n82.60 56.52\nFacebook-AI\n82.88\n82.45 60.53\nHuaweiTSC\n80.98\n82.64 70.71\nUEdin\n80.82\n81.99 74.17\neTranslation\n80.04\n81.60 74.05\nTable 14: Edit Efficacy over Erroneous Spans with\ngpt-3.5-turbo: On both WMT-20 and WMT-21 Sys-\ntems, post-editing with gpt-3.5-turbo modifies more\nthan half of the erroneous spans.\nLanguage\nSystem\nCOMET-KIWI\nCOMET-QE\nCOMET-22\nCOMET-20\nEn-Ha\nMS Translator\n57.18\n2.68\n72.51\n-13.04\nEn-Ha\nMS Translator + GPT-4-CoT\n59.02\n3.03\n74.10\n-4.27\nHa-En\nMS Translator\n68.45\n15.62\n73.26\n13.23\nHa-En\nMS Translator + GPT-4-CoT\n73.19\n22.05\n77.83\n32.76\nEn-Ja\nMS Translator\n85.27\n36.80\n87.95\n57.97\nEn-Ja\nMS Translator + GPT-4-CoT\n85.39\n38.73\n89.04\n62.41\nJa-En\nMS Translator\n80.08\n22.20\n81.53\n36.14\nJa-En\nMS Translator + GPT-4-CoT\n80.92\n25.00\n82.93\n42.96\nCs-En\nMS Translator\n82.18\n32.90\n87.44\n72.02\nCs-En\nMS Translator + GPT-4-CoT\n82.65\n34.76\n87.56\n72.28\nEn-Cs\nMS Translator\n84.16\n59.02\n90.63\n94.06\nEn-Cs\nMS Translator + GPT-4-CoT\n84.27\n58.95\n90.62\n93.02\nEn-Ru\nMS Translator\n82.85\n45.97\n87.44\n67.37\nEn-Ru\nMS Translator + GPT-4-CoT\n82.94\n47.75\n88.05\n68.56\nRu-En\nMS Translator\n80.72\n31.11\n85.16\n62.24\nRu-En\nMS Translator + GPT-4-CoT\n81.28\n32.79\n85.66\n63.94\nEn-Is\nMS Translator\n80.20\n34.50\n84.25\n65.75\nEn-Is\nMS Translator + GPT-4-CoT\n79.27\n31.96\n83.47\n62.45\nIs-En\nMS Translator\n80.33\n31.96\n85.91\n65.98\nIs-En\nMS Translator + GPT-4-CoT\n81.11\n31.96\n83.47\n62.45\nEn-Uk\nMS Translator\n81.85\n47.73\n86.13\n61.09\nEn-Uk\nMS Translator + GPT-4-CoT\n81.96\n48.89\n86.96\n63.08\nUk-En\nMS Translator\n79.72\n25.28\n83.47\n52.37\nUk-En\nMS Translator + GPT-4-CoT\n81.18\n28.38\n85.42\n60.27\nTable 15: General Quality Improvements on WMT-22 Test Sets: The + sign reflects that the post-editing is\napplied on the initial translations produced by the given System. The post-editing is applied in the CoT setting\nthroughout the results in this table.\nSystem Prompt\nYou are a native speaker of both English and German. You are an expert post editor of translations\nfrom English into German.\nYou know that the German translation of a given English text must faithfully represent its meaning in\nGerman. The English input text itself might contain any number of different words, including typos\nand placeholder entities, but still the German translation must remain faithful to the English input text.\nFaithfulness of a German translation means that every word in the translation can be reconstructed from\nthe given English input and vice versa. Therefore, you notice any deviations in the faithfulness of the\nGerman translations, including the below issues that make the given German translation not optimal:\n1. words in the German translation that are not supported in the input 2. words in the English input\nthat are not adequately translated 3. words in the German translation that do not convey the specific\nmeaning of the corresponding word in the input 4. words in the German translation that are not in\nthe correct language 5. punctuations in the German translation that are different from the input 6.\nsymbols in the English input that are not correctly present in the German translation 7. casing in the\nGerman translation that does not conform to the English input 8. incorrect modifications in the German\ntranslations of names, organizations, entities 9. incorrect modifications in the German translations\nof any cardinal or ordinal numbers 10. incorrect translations of web terminologies such as urls, web\naddresses and hashtags in the input 11. incorrect translations of physical units or currencies in the input\n12. unsupported expansions of the acronyms present in the input\nYou identify and fix the above twelve issues one by one in the German translation if they are present, in\na way that improves the translation fluency.\nFurther, as an expert translation post editor, for the improvements made to the German translation, you\nmake sure that the following principles are followed:\n1. No corrections are made that add any word or phrase in the translation which are unsupported\nin the input 2. The capitalizations in the translation strictly follow the input capitalizations, e.g.,\nacronym capitalizations should not be changed 3. The translation contains the appropriate articles and\ndeterminers to follow the specifics in the input 4. Do not leave any symbol, word or phrase in the\ninput text untranslated in the final, improved translation 5. Do not add any extraneous words, phrases,\nclauses or sentences in the translation that is not supported by the input 6. If the input starts with a non\ncapitalized word, the translation starts with a non capitalized word 7. In the case that the translation is\nseverely inadequate, you generate an improved translation from scratch 8. No end punctuations or full\nstops are added if such punctuations or full stops are not in the input 9. Do not assume that an acronym\nis a typo, always err on the side of assuming that the presented input words are not typos 10. Do not\nreplace any entities or placeholders in the translation with fictitious (unsupported) entities 11. If the\ninput contains offensive or lewd words, you still translate them faithfully 12. If the translation misses to\nconvey the meaning of a large part of the input sentence, you include the translation for the missing part\nUser Prompt\nAs an expert translation post editor, your task is to improve the German translation for the below\nEnglish text:\nEnglish: They were addressed to her son, who has autism and lives in a private care facility, she said.\nBut instead of her son\u2019s name inside when you opened them, the letters said Dear Maine\u2019s Department\nof Health and Human Services \u2013 in Cincinnati, she told local media.\nGerman: Sie waren an ihren Sohn gerichtet, der Autismus hat und in einer privaten Pflegeeinrichtung\nlebt, sagte sie. Aber anstelle des Namens ihres Sohnes im Inneren, als Sie sie \u00f6ffneten, hie\u00df es in den\nBriefen Dear Maine Dear Maine \u2019s Department of Health and Human ServicesServices - in Cincinnati,\nsagte sie den lokalen Medien.\nTo accomplish this, follow these steps:\nStep 1: Say \"Proposed Improvements:\". Then brainstorm and design the improvements that make the\nGerman translation more faithful and fluent. Step 2: Say \"Improved Translation:\". Then output the\nGerman translation with proposed improvements that increase translation faithfulness and fluency.\nTable 16: Post-Editing Prompt: System and User Prompts for Post-Editing with Chain-of-Thought Baseline.\nThe newlines are suppressed in the table.\nSystem Prompt\nYou will work as a machine translation annotator to help assess the quality of translation:\nPlease identify all errors within each translated sentence, up to a maximum of five. If there are more\nthan five errors, identify only the five most severe. To identify an error, specify the relevant span of\ntext, and select a category/sub-category and severity level from the available options. (The span of\ntext may be in the source sentence if the error is a source error or an omission.) When identifying\nerrors, please be as fine-grained as possible. For example, if a sentence contains two words that are\neach mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text\ncontains multiple errors, you only need to indicate the one that is most severe. If all have the same\nseverity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency,\nthen Terminology, etc). Be very precise and accurate.\nIf there is an error in translation, identify the severity of the error as follows:\nMajor: Errors that may confuse or mislead the reader due to significant change in meaning or because\nthey appear in a visible or important part of the content. Minor: Errors that don\u2019t lead to loss of\nmeaning and wouldn\u2019t confuse or mislead the reader but would be noticed, would decrease stylistic\nquality, fluency or clarity, or would make the content less appealing. Neutral: Use to log additional\ninformation, problems or changes to be made that don\u2019t count as errors, e.g., they reflect a reviewer\u2019s\nchoice or preferred style.\nIf there is an error in translation, try to place it in a category below. If it doesn\u2019t match any of those\ncategories, place it as an Other error:\n1. Accuracy: there is an error with the translation accuracy, if it matches any of the following categories:\nAccuracy/Addition: Translation includes information not present in the source. Accuracy/Omission:\nTranslation is missing content from the source. Accuracy/Mistranslation: Translation does not accu-\nrately represent the source. Accuracy/Untranslated text: Source text has been left untranslated.\n2. Fluency: there is an error with the translation fluency, if it matches any of the following categories:\nFluency/Punctuation: Incorrect punctuation (for locale or style). Fluency/Spelling: Incorrect spelling\nor capitalization. Fluency/Grammar: Problems with grammar, other than orthography Fluency/Register:\nWrong grammatical register (e.g., inappropriately informal pronouns). Fluency/Inconsistency: Internal\ninconsistency. Fluency/Character encoding: Characters are garbled due to incorrect encoding.\n3. Terminology: Terminology is inappropriate or inconsistent: Terminology/Inappropriate: Ter-\nminology is non-standard or does not fit context. Terminology/Inconsistent: Terminology is used\ninconsistently.\n4. Style: Translation is awkward with stylistic problems.\n5. Locale convention: Wrong format for addresses, currency, dates, names, telephone numbers\nor time expressions. Locale/Address: Wrong format for addresses. Locale/Currency: Wrong for-\nmat for currency. Locale/Date: Wrong format for dates. Locale/Name: Wrong format for names.\nLocale/Telephone: Wrong format for telephone numbers. Locale/Time: Wrong format for time\nexpressions.\nAfter identifying all the errors, you will produce an improved translation that fixes the identified errors.\nFor the improvements made to the translation, you make sure that the following principles are followed:\n1. No corrections are made that add any word or phrase in the translation which are unsupported\nin the input 2. The capitalizations in the translation strictly follow the input capitalizations, e.g.,\nacronym capitalizations should not be changed 3. The translation contains the appropriate articles and\ndeterminers to follow the specifics in the input 4. Do not leave any symbol, word or phrase in the\ninput text untranslated in the final, improved translation 5. Do not add any extraneous words, phrases,\nclauses or sentences in the translation that is not supported by the input 6. If the input starts with a non\ncapitalized word, the translation starts with a non capitalized word 7. In the case that the translation is\nseverely inadequate, you generate an improved translation from scratch 8. No end punctuations or full\nstops are added if such punctuations or full stops are not in the input 9. Do not assume that an acronym\nis a typo, always err on the side of assuming that the presented input words are not typos 10. Do not\nreplace any entities or placeholders in the translation with fictitious (unsupported) entities 11. If the\ninput contains offensive or lewd words, you still translate them faithfully 12. If the translation misses to\nconvey the meaning of a large part of the input sentence, you include the translation for the missing part\nTable 17: Post-Editing Prompt: System and User Prompts for Post-Editing with Structured Chain-of-Thought\nBaseline. The user prompt remains the same throughout. The newlines are suppressed in the table.\nExample of Post-Editing using GPT-4 (Verbatim Outputs under the CoT Setting)\nSource: Wir wollten jene mit Tischplatten Durchmesser von #NUMBER#\nTranslation: We wanted the ones with table tops diameter of #NUMBER#\nProposed Improvements:\n1. Change \"table tops diameter\" to \"tabletop diameters\"\n2. Add \"a\" before \"diameter\"\nImproved Translation:\nWe wanted the ones with tabletop diameters of #NUMBER#\nTable 18: Edit Hallucination Example on a WMT-22 De-En Sample: GPT-4 produces an unrealized edit.\nExample of Post-Editing using GPT-4 (Verbatim Outputs under the CoT Setting)\nSource: Der H\u00e4ndler teilt dem Kunden die Kosten des Teils vor der Reparatur mit.\nTranslation:Der H\u00e4ndler teilt dem Kunden die Kosten des Teils vor der Reparatur mit.\nProposed Improvements:\n1. Change \"will notify\" to \"informs\" to better convey the present tense of \"teilt...mit\" in the German\ninput.\n2. Add \"with\" after \"informs\" to better convey the meaning of \"teilt...mit\" in the German input.\nImproved Translation:\nThe dealer informs the customer of the cost of the part before the repair.\nTable 19: Edit Hallucination Example on a WMT-22 En-De Sample: GPT-4 produces an unrealized edit.\nExample of Post-Editing using GPT-4 (Verbatim Outputs under the CoT Setting)\nSource: schicken Sie mir bitte irgendeine Transaktionsnummer.. oder sonst was, was dabei helfen\nk\u00f6nnte\nTranslation: please send me any transaction number.. or anything else that could help\nProposed Improvements:\n1. Replace the double period with a single period after \"transaction number.\n2. Add a comma after \"or anything else\".\nImproved Translation:\nPlease send me any transaction number or anything else that could help.\nTable 20: Edit Hallucination Example on a WMT-22 En-De Sample: GPT-4 produces an unrealized edit.\nExample of Post-Editing using GPT-4 (Verbatim Outputs under the CoT Setting)\nSource: Der Spiegel hat einen Kratzer und am Rahmen ist ebenfalls ein Fehler, der mit einem gelben\nF\u00fcllstoff versucht wurde kaschiert zu werden.\nTranslation: The mirror has a scratch and there is also an error on the frame that was attempted to be\ncovered with a yellow filler.\nProposed Improvements:\n1. Replace \"error\" with \"blemish\" to better convey the meaning of \"Fehler\" in this context.\n2. Rearrange the sentence structure to improve fluency.\nImproved Translation:\nThe mirror has a scratch, and there is also a blemish on the frame that was attempted to be concealed\nwith a yellow filler.\nTable 21: Edit Hallucination Example on a WMT-22 En-De Sample: GPT-4 produces an unrealized edit.\n"
  },
  {
    "title": "PEARL: Prompting Large Language Models to Plan and Execute Actions Over Long Documents",
    "link": "https://arxiv.org/pdf/2305.14564.pdf",
    "upvote": "1",
    "text": "PEARL: Prompting Large Language Models to\nPlan and Execute Actions Over Long Documents\nSimeng Sun1\u2217 Yang Liu2\nShuohang Wang2\nChenguang Zhu2\nMohit Iyyer1\nUniversity of Massachusetts Amherst1\nMicrosoft Research2\n{simengsun, miyyer}@umass.edu\n{yaliu10,shuohang.wang,chezhu}@microsoft.com\nAbstract\nStrategies such as chain-of-thought prompting\nimprove the performance of large language\nmodels (LLMs) on complex reasoning tasks\nby decomposing input examples into intermedi-\nate steps. However, it remains unclear how\nto apply such methods to reason over long\ninput documents, in which both the decom-\nposition and the output of each intermediate\nstep are non-trivial to obtain. In this work,\nwe propose PEARL, a prompting framework\nto improve reasoning over long documents,\nwhich consists of three stages: action mining,\nplan formulation, and plan execution. More\nspecifically, given a question about a long\ndocument, PEARL decomposes the question\ninto a sequence of actions (e.g., SUMMARIZE,\nFIND_EVENT, FIND_RELATION) and then exe-\ncutes them over the document to obtain the an-\nswer. Each stage of PEARL is implemented via\nzero-shot or few-shot prompting of LLMs (in\nour work, GPT-4) with minimal human input.\nWe evaluate PEARL on a challenging subset\nof the QuALITY dataset, which contains ques-\ntions that require complex reasoning over long\nnarrative texts. PEARL outperforms zero-shot\nand chain-of-thought prompting on this dataset,\nand ablation experiments show that each stage\nof PEARL is critical to its performance. Overall,\nPEARL is a first step towards leveraging LLMs\nto reason over long documents.1\n1\nIntroduction\nPerforming complex reasoning over long input doc-\numents often requires forming high-level abstrac-\ntions of the text (e.g., plots and themes in a narra-\ntive) and then conducting a variety of inferences\non top of those abstractions (Graesser et al., 1994).\nConsider the following question about the story\n\u201cBreakaway\u201d from the QuaLITY dataset (Pang\net al., 2022):\n\u2217Work partially done during an internship at Microsoft.\n1https://github.com/SimengSun/pearl\nMine helpful actions from training set questions \nDEFINE(X), COMPARE(X,Y), FIND_EMOTION(X),...\nAction Mining\nExecute the plan step-by-step \nPlan Execution\nopen_conv = \"In the initial conversation, Phil \nConover is excited about his upcoming mission \nto be the first man to see the other side of \nthe moon ....\"\nGiven a question, generate plan of mined actions\nPlan Generation\nQuestion: What part of the final scene best connects to the \nstory's opening conversation?\n1.open_conv = FIND_ELEMENT(CTX,\"opening conver..\")\n2.final_scene = SUMMARIZE_X(CTX, \"final_scene\")\n3.reflection = FIND_RELATION(init_conv, final_scene)\n \nFigure 1: High-level overview of our framework PEARL.\nEach stage in PEARL is achieved via zero-shot or few-\nshot prompting of an LLM (in our work, GPT-4). We\nalso provide\nexample outputs from each stage.\nWhat part of the final scene best connects to the\nstory\u2019s opening conversation?\nTo answer this question, we need to gather, eval-\nuate, and synthesize information from across the\nstory, which motivates decomposing the question\ninto a plan of actions, as in:\n1. Identify all participants in initial conversation.\n2. Summarize the initial conversation.\n3. Summarize events and themes of final scene.\n4. Summarize roles of conversation participants\nin final scene.\n5. Identify and rank connections between conver-\nsation and final scene.\nEach action in the above plan varies in complexity,\nfrom simple lookup-style actions (Step 1) to more\nchallenging query-focused summarization (Steps\n2-4) and conceptual linking (Step 5) actions that\nrequire deep narrative understanding.\nGiven the rapidly advancing capabilities of large\nlanguage models (LLMs), how can we use them\nto answer questions like these? While we could\ndirectly prompt LLMs to generate the answer, prior\n1\narXiv:2305.14564v1  [cs.CL]  23 May 2023\nwork on simpler reasoning-based tasks shows that\nthis method is inferior to Chain-of-Thought prompt-\ning (Wei et al., 2022, CoT), which encourages the\nLLM to provide step-by-step explanations and in-\ntermediate outputs before producing the answer.\nUnfortunately, CoT is not well-suited for tasks in-\nvolving complex reasoning over long input docu-\nments, as both the decomposition of the original\nquestion and the intermediate outputs of each step\nare non-trivial to obtain, as in the above example.\nGiven the difficulty of obtaining plans and in-\ntermediate explanations for long documents, one\npotential solution is to delegate this task to smaller\nexecutable modules instead of forcing the LLM to\ncome up with all of them at once. In this work,\nwe introduce PEARL, a framework that combines\nPlanning and Executable Actions for Reasoning\nover Long documents. Each stage of PEARL \u2014\naction mining, plan decomposition, and plan exe-\ncution \u2014 is implemented by applying zero-shot or\nfew-shot prompting to an LLM. The stages (Fig-\nure 1) can concisely be described as follows:\n1. Action mining: An LLM is prompted to come\nup with simple actions that can help solve\nquestions from an input training dataset. Un-\nlike predefined \u201ctoolboxes\u201d in methods such\nas Toolformer (Schick et al., 2023) or Re-\nACT (Yao et al., 2023b), the action set in\nPEARL is also generated by an LLM.\n2. Plan generation: Given an input test ques-\ntion, an LLM generates an executable plan\nconsisting of a series of actions selected from\nthe action set produced in the previous stage.\nThe plan is formatted as a simple program in\nwhich the execution result of one action can\nserve as an argument to future actions, which\nenables complex composition.\n3. Plan execution: The LLM executes the plan\naction-by-action via a prompt template that\nincludes an action and the long-form input\ndocument. Note that this is the only stage that\nincludes the document, as the other stages\noperate over just questions.\nWe demonstrate PEARL\u2019s effectiveness on a chal-\nlenging subset of QuALITY (Pang et al., 2022), a\nreading comprehension dataset that contains ques-\ntions about long-form articles. While QuALITY is\noriginally a multiple-choice dataset, we reformu-\nlate it into a generation task: given a question and\nan article, an LLM is asked to generate a free-form\nanswer. As a proxy for measuring answer correct-\nness, we adopt a similar approach to Wang et al.\n(2020) by asking the LLM to map its generated an-\nswer to one of the multiple choice options, which\nallows us to compute its accuracy.\nPrompting LLMs with PEARL yields more ac-\ncurate and comprehensive answers than those gen-\nerated by directly prompting the LLM to answer\nthe question, particularly for questions that require\nreasoning over the full long document. This result\nis particularly impressive given the potential for er-\nror propagation in the PEARL framework: as each\nstage is implemented via an LLM, errors in plan\nformulation or execution can significantly affect\nthe output answer. To further verify the integrity of\nthe plans, we perform human evaluation by asking\nannotators to provide feedback and ratings; anno-\ntators generally find the plans to be reasonable,\nalthough a small percentage contain unnecessary\nactions or omit critical actions. Overall, we hope\nPEARL further opens the door towards using LLMs\nfor complex reasoning over long documents.\n2\nRelated work\nOur work builds on recent LLM prompting re-\nsearch and also connects to work on reasoning\nover long documents. Before describing PEARL,\nwe first survey related papers to contextualize our\nwork within this fast-moving field.\nPrompting methods:\nRecently, the capabilities\nof large language models (Brown et al., 2020;\nZhang et al., 2022; Touvron et al., 2023) have sig-\nnificantly increased as a result of learning from\ninstructions or feedback (Stiennon et al., 2022;\nOuyang et al., 2022; Chung et al., 2022) to bet-\nter align their outputs to human preferences. When\nprovided with well-crafted prompts, such as chain-\nof-thought (Wei et al., 2022) explanations, these\nstate-of-the-art models exhibit impressive reason-\ning abilities. A plethora of new prompting tech-\nniques (Table 1) has been introduced lately to un-\nlock more capabilities of LLMs via leveraging ex-\nteral tools (Chen et al., 2022; Schick et al., 2023; Lu\net al., 2023), problem decomposition (Press et al.,\n2022; Dua et al., 2022; Khot et al., 2023; Yao et al.,\n2023b), self-reflection and self-refinement (Huang\net al., 2022; Shinn et al., 2023; Madaan et al., 2023),\nplanning (Yao et al., 2023a; Wang et al., 2023a;\nLong, 2023), and other techniques (Yoran et al.,\n2023; Wang et al., 2023b; Zhou et al., 2023).\n2\nPrompting Methods\nExplicit\nplan\nIterative\nprompting\nDoes not rely on\nexternal tools\nLong\ndocuments\nChain-of-Thought (Wei et al., 2022)\n\u2717\n\u2717\n\u2713\n\u2717\nProgram-of-Thought (Chen et al., 2022)\n\u2717\n\u2717\n\u2717\n\u2717\nSelf-Ask (Press et al., 2022)\n\u2717\n\u2713\n\u2717\n\u2717\nToolformer (Schick et al., 2023)\n\u2717\n\u2717\n\u2717\n\u2717\nReAct (Yao et al., 2023b)\n\u2717\n\u2713\n\u2717\n\u2717\nPlan-and-Solve (Wang et al., 2023a)\n\u2713\n\u2717\n\u2713\n\u2717\nPEARL (this work)\n\u2713\n\u2713\n\u2713\n\u2713\nTable 1: Comparison of PEARL to other recently-proposed prompting techniques. PEARL is the only one designed\nfor and evaluated on tasks that require complex reasoning over long documents.\nReasoning over long documents:\nLarge lan-\nguage models have showcased remarkable reason-\ning capabilities (Huang and Chang, 2022), includ-\ning mathematical reasoning (Cobbe et al., 2021),\ncommonsense reasoning (Talmor et al., 2019), and\nsymbolic reasoning (Nye et al., 2021). Most of\nthese tasks do not involve long context inputs,\nand thus they are able to benefit from few-shot\nin-context CoT prompting. In this paper, we pri-\nmarily focus on tasks that contain long input con-\ntexts (Ko\u02c7cisk\u00fd et al., 2018; Dasigi et al., 2021;\nShaham et al., 2022; Sun et al., 2022), specifically\ngenerative question answering based on long input\narticles. To address the absence of reliable eval-\nuation for long-form QA (Krishna et al., 2021),\nStelmakh et al. (2022) proposes automatic met-\nrics for evaluating the correctness of the answer,\nwhereas in this work, we use LLM-based evalu-\nation by taking advantage of the multiple-choice\nsetup of existing QA dataset. Prior to the shift to\nprompting-based methods, approaches including\ncontrastive learning-based sequence-level objec-\ntives (Caciularu et al., 2022), iterative hierarchical\nattention (Sun et al., 2021), and joint modeling of\nmachine reading and answer generation (Su et al.,\n2022) have been employed to enhance long-context\nquestion answering.\n3\nPEARL: Planning and Executing\nActions for Reasoning over Long\nDocuments\nWe are interested in using LLMs to solve tasks that\nrequire complex reasoning over long documents.2\nIn this paper, we focus on the task of answering\nquestions about long-form narratives. Most prompt-\n2As there is no consensus on what is \u201clong\u201d, we consider\nit to mean documents of several thousands of tokens in length.\nPrompt Sketch for Action Mining\nSeed actions:\n{Human-written seed set of actions}\nSUMMARIZE(CTX):Provides a general summary about given CTX\nFIND_REASON(CTX, X): Find cause of X in given CTX\nInstructions and demonstrations:\n{Natural language instructions} \n{Human-written few-shot demonstrations}\nGiven a question about a long document and the seed\naction set, come up with new actions that could help to\nanswer the question...\nOutput:\nFIND_MISSION(CTX, X) : Find the mission of character X\nfrom the input context CTX...\nInput question:\n{Question from training set}\nWhat is the alien's mission?\nFigure 2: Prompt sketch for action mining. It comprises\nhuman-written seed actions set and instructions, as well\nas question for which LLM will extract action(s) from.\nFinally, we also present an example mined action. More\ndetails can be found in the Appendix D.\ning strategies that aim to improve the reasoning\nabilities of LLMs (e.g., CoT) are not applicable\nto this task due to the length and complexity of\nthe input document. In this section, we specify\nour PEARL framework, which consists of three\nLLM-implemented stages that mine actions from a\ntraining corpus, formulate plans to answer held-out\nquestions, and then execute the resulting plans to\nobtain answers.\n3.1\nAction mining\nIn many prior prompting techniques such as Re-\nACT and Toolformer, the LLM is able to query\nexternal APIs (e.g., Wikipedia search or a calcu-\nlator) to solve a given task. Unlike these works,\nwhich assume a predefined action space, PEARL\nmines actions directly from data of similar distribu-\n3\n{Natural language instructions} \n{Human-written few-shot demonstrations}\nPrompt Sketch for Plan Generation\nInput question:\nOutput:\nMined actions:\n{Mined actions from previous stage}\nFIND_EVENT(CTX, X): Find the event involving X from input\nSUMMARIZE(CTX, X): Provide a summary about X given input\nInstructions and demonstrations:\nGiven a question about a long document and the list of\nmined actions, come up with a plan for addressing the\nquestion below ...\n{Question from evaluation set}\nWhy does Simon look for a bottle of aspirin?\n1. aspirin_event = FIND_EVENT(CTX,\"look for...\") : Find\nand summarize the event where...\n2. aspirin_reason = FIND_BEHAVIOR_REASON(CTX,\naspirin_event): Find the reason why ... \nFigure 3: Prompt sketch for plan generation. In the\nprompt, we include the list of actions mined from previ-\nous stage in-context, natural language detailing the task,\nand few-shot examples guiding the plan generation.\ntion (in our case, training set questions of QuAL-\nITY). As shown by prior research (Graesser et al.,\n1994), answering complex queries over long doc-\numents requires specific reasoning techniques; as\nfurther evidence, Xu et al. (2022) demonstrates the\npresence of various discourse structures in good an-\nswers to long-form questions on Reddit. Learning\ndataset-specific actions enables PEARL to scale to\ndifferent domains and tasks, as user queries may\ndiffer considerably in terms of complexity. More-\nover, mining actions from training set can reduce\nhuman efforts in designing new actions. In this\nwork, we define an \u201caction\u201d as a basic unit for long\ndocument reasoning. To obtain these actions, we\nfirst manually create a small set of seed actions to\nuse as demonstrations.3 Then, as shown in Figure 2,\ngiven an example question, we feed it along with\nthe seed actions and instructions to the LLM to\ngenerate more task-specific actions. Each ACTION\nis formatted as a programmatic function with input\narguments and is followed by a model-generated\nfunction definition in natural language. Below is\nan example action generated by the LLM:\nANALYZE(CTX, X, Y) # Analyze the rela-\ntionship, attitude, or feelings between X and Y\ngiven the input context CTX\nAfter a full pass over example questions in the train-\ning data, we obtain a final set of actions and their\ncorresponding definitions which are then incorpo-\nrated into the prompt of the next stage.\n3See prompt for QuALITY action mining in Appendix D\nPrompt Sketch for Plan Execution\nLong input document:\nPhil Conover pulled the zipper of his flight\nsuit up the front of his ...\naspirin_event = \"In the beginning of the story, Simon, a\nprivate investigator, was looking for ...\"\n{Argument value assignment}\n{One-sentence explanation}\nFind the reason behind Simon's behavior of looking ...\nFIND_BEHAVIOR_REASON(CTX, aspirin_event)\n{Action of current step}\nFIND_BEHAVIOR_REASON(CTX, X): Find the reason behind\nthe behavior X given the input CTX \n{Mined action and its definition}\nInstructions:\nOutput:\n...he is suffering from a severe hangover due to\nexcessive consumption of Marzenbr\u00e4u beer during ...\nFigure 4: Prompt sketch for plan execution. This prompt\ncontains multiple {placeholders} that will be filled with\noutput from previous stages.\n3.2\nPlan generation\nA plan serves as the guiding framework or outline\nfor answering complex questions that may involve\nmulti-step reasoning and/or global understanding\nof long documents. Given a question, as shown in\nFigure 3, we prompt an LLM to generate a plan\nbased on the previously-mined action set. Each\nstep of the plan is formatted as\noutput = ACTION(arg1, arg2, . . . ),\nwhere the output variable stores the result of the\ncurrent ACTION , and the arguments can be (1) the\ninput document, (2) a string, or (3) an output vari-\nable from previous steps of the plan. When gener-\nating the plan, we do not show the LLM the entire\ndocument as input, which provides ample space for\nincorporating few-shot in-context examples. Simi-\nlar to the seed actions in the previous stage, we pro-\nvide a small seed set of plans and allow the model\nto generate more demonstrations automatically. We\nprovide more details in Section 4 about controlling\nthe quality of model-generated in-context demon-\nstrations.\n3.3\nPlan execution\nIn the previous stage, the LLM generates a plan that\nserves as a blueprint for producing a response. To\nexecute each step in the plan, we prompt the LLM\nwith a template filled with output from previous\nstages. Concretely, as shown in Figure 4, to execute\nthe action FIND_BEHAVIOR_REASON, the model fills\nin the prompt template with (1) the planned action\n4\nand definition, (2) current action with specific input\nargument (e.g., aspirin_event) , (3) assignment\nof argument name with output from previous stage\n(e.g., aspirin_event = \u201cin the beginning of\nthe story, ...\u201d), and (4) a one-sentence instruc-\ntion for the current step, all of which are generated\nby LLM. As the long input article is involved dur-\ning this stage, the prompt is executed in a zero-shot\nmanner.\n3.4\nSelf-correction and self-refinement\nSince the plans are generated by an LLM, they may\nbe incorrectly formatted or of otherwise low qual-\nity. To address this issue, similar to Shinn et al.\n(2023), we include a self-correction step prior to\nplan execution and a self-refinement step before\nincorporating model-generated plans as in-context\nfew-shot examples. We implement a plan parser\nthat returns relevant error messages when the plan\ndoes not conform to the defined format. The in-\nvalid plan as well as the error message are then\npassed into the LLM for correcting the plan\u2019s gram-\nmar. To ensure the quality of model-generated\nin-context examples, we validate them by execut-\ning the plan and evaluating the generated answer\nwith a task-specific scoring function (more details\nin Section 4.1). If the answer is rejected by the\nevaluation in the end, we pass the plan to LLM for\nfurther self-refinement before being included in the\ncontext as few-shot examples.\n4\nExperiments\nWe compare PEARL to baseline methods (zero-shot\nanswering and zero-shot CoT) on a challenging sub-\nset of the QuALITY Question-Answering dataset\nthat requires reasoning over long articles of several\nthousands tokens. In this section, we describe our\ndataset selection, experimental setup, and model\nconfigurations.\nDataset selection:\nWe focus on the QuALITY\nQA dataset (Pang et al., 2022), which is a multiple-\nchoice QA task in the SCROLLS benchmark (Sha-\nham et al., 2022). However, to better simulate\nLLMs usage in real-world scenarios, we turn this\ndataset into a generative task4 in which an LLM\ndoes not have access to the choices and must in-\nstead generate a long-form answer. Then, we auto-\nmatically map the generated answer back to one of\n4We provide the performance of GPT-4 with standard\nmulti-choice setup on the full QuALITY dev set in Ap-\npendix A.\nthe choices with an LLM to evaluate the accuracy\nas shown in Figure 5.5 The accuracy of mapped\nanswers serves as a proxy for assessing the correct-\nness of the provided answer.\nQuALITY contains a diverse variety of ques-\ntions, each of which is annotated with the amount\nof context from the document needed to answer\nthe question. In contrast to questions that can be\ncorrectly answered with local context once a piece\nof information is located, as in\nWho found Retief and Magnan in the trees?\nwe are more interested in questions that require\nreasoning over long context, as in:\nHow would you describe the changes in tone\nthroughout the passage?\nThese questions constitute an interesting and dif-\nficult subset that, unlike more straightforward in-\nformation seeking questions, require global under-\nstanding and reasoning over the document to pro-\nvide accurate answers. Therefore, we select a sub-\nset of questions rated as requiring long contexts to\nanswer. In total, we create a dataset of 1K examples\ndivided into two splits:6 (1) Long: 330 examples\nfrom the dev set, 368 examples from training set,\nand (2) Short: 302 examples from dev set that do\nnot require long contexts to answer; the latter forms\na control dataset to make sure our methods do not\noverly worsen performance on simpler questions.\n4.1\nExperimental setup\nAs each of the stages in PEARL has critical hyperpa-\nrameters and implementation details, we describe\nour specific configurations here.\nAction mining:\nWe provide an LLM with seven\nseed actions and two in-context examples to demon-\nstrate the required format for generating new ac-\ntions.7 We collect new actions by passing all train-\ning set questions into the model, excluding those\nquestions in our evaluation set. Ultimately, we ob-\ntain 407 actions and corresponding definitions, of\nwhich several are duplicates or overly specific, and\n5In Appendix B, we confirm through human evaluation that\nGPT-4, the model we test, demonstrates considerable\u2014but\nnot perfect\u2014agreement with human annotators for the answer\nmapping stage.\n6Human annotation score on the required context ranges\nfrom 1 to 4. Questions in the long split are those with average\nhuman annotation score \u2265 3, questions in the short split have\nscores < 3.\n7We present the prompt template in Appendix D\n5\nQUALITY\nLONG\nQUALITY\nSHORT\nALL\np-val\nPROMPTING METHODS\nGPT-4 zero-shot\n64.3\n79.1\n68.8\n-\nGPT-3.5 zero-shot (text-davinci-003)\n45.5\n56.3\n48.8\n0.000\nGPT-4 zero-shot chain-of-thought\n65.9\n77.2\n69.3\n0.766\nGPT-4 PEARL\n70.9\n77.8\n73.0\n0.005\nAblations of GPT-4 PEARL\nw/o plan execution\n67.3\n77.2\n70.3\n0.295\nw/o self-refinement of plan demonstrations\n67.0\n78.8\n70.6\n0.245\nTable 2: We present baseline and PEARL as well as ablation results on our generative subset of QuALITY questions.\nLong denotes the split where the questions require reasoning over long contexts to answer accurately. As we only\nevaluate on a subset, we also provide p-values to verify statistical significance against the zero-shot GPT-4 baseline.\n{Question}\nFree-from\nAnswer\n{Question}\n{Options}\nFree-form\nAnswer\nLLM\nAnswer:\nA/B/C/D\nLLM + PEARL\nFigure 5: Generic illustration of our evaluation setup.\nGiven the article and question, we prompt an LLM with\nPEARL to generate a long-form answer, which is later\nmapped to one of QuALITY\u2019s multiple-choice options\nby the LLM itself.\nin total exceeds GPT-4\u2019s maximum context win-\ndow of 8K tokens. As such, we instruct the LLM to\nsimplify and abstract over existing actions in order\nto reduce the total number of actions. After repeat-\ning this process twice,8 we reduce the number of\nactions to 81, which forms the final action set for\nPEARL.\nSelf-correction retry limit:\nDespite utilizing\nself-correction to validate the generated plan\u2019s syn-\ntax, it is still possible that the model fails to gen-\nerate a plan in the correct format. In such cases,\nwe force the model to revert to the zero-shot base-\nline approach. Out of 1K examples across various\nPEARL variants, only 4 examples failed to parse\n8After one round, the actions reduced to \u223c140, and after\nfour rounds to \u223c20. We provide ablations on the number of\nactions in Section 5.\nwithin the retry count limit, which is within an\nacceptable range of failed examples.\n4.2\nBaselines\nAs existing sophisticated prompting methods re-\nquire few-shot examples in-context, which is not\nfeasible when long document is involved, we com-\npare PEARL with simple zero-shot baselines (GPT-\n4 (OpenAI, 2023) and GPT-3.5 (Ouyang et al.,\n2022)), where we directly prompt the model to pro-\nvide a detailed free-form answer. Additionally, we\nalso evaluate zero-shot chain-of-thought prompting\nfor GPT-4 by adding \u201cLet\u2019s think step-by-step,\u201d to\nthe prompt.\n5\nMain results\nWe discover that PEARL significantly outperforms\ncompeting prompting methods on questions that\nrequire reasoning over long contexts, which demon-\nstrates the utility of the planning module. We also\nobserve a small drop in accuracy on questions that\nrequire only short contexts, possibly because the\nplans end up over-complicating what is a simple\nreasoning process. In this section, we dig deeper\ninto the main results of our experiments, which are\npresented in Table 2.\nPEARL improves accuracy on long-document\nQA:\nOverall, PEARL\u2019s accuracy is higher than\nthat of all competing methods, particularly for the\nQuALITY split annotated by humans as requiring\nlong contexts to answer (Long). Furthermore, we\nobserve in Figure 6 that for questions marked by\nQuALITY workers as requiring the longest possi-\nble context, PEARL improves substantially com-\npared to the zero-shot GPT-4 baseline (72.4% vs\n6\nShort\nLong\nLonger\nRequired context\n50\n55\n60\n65\n70\n75\n80\nMapped Answer Accuracy\nGPT-4 zero-shot\nGPT-4 PEARL\nFigure 6: Accuracy by the amount of required context\nto answer,9as annotated by humans in QuALITY.\n61.9%). Our method\u2019s slightly diminished perfor-\nmance on the short split is likely due to both \u201cover-\nthinking\u201d these simpler questions, as well as er-\nror propagation from plan execution steps as high-\nlighted in Section 6. Finally, we point out that all\nmethods achieve higher accuracies on the Short\nsplit compared to the Long split, indicating the\nchallenging nature of this set of questions.\nNumber of actions impacts performance:\nIn\nFigure 7, we show that the size of the action set is\nan important factor in PEARL\u2019s performance. With\njust a single action (i.e., EXECUTE a free-form natu-\nral language instruction),10 PEARL\u2019s accuracy on\nthe Long subset drops to 64%. With too many ac-\ntions (140 in the plot), its accuracy also degrades,\nlikely because the action space is too fine-grained\nfor the model to properly execute all actions. We\nnote that the optimal number of actions likely dif-\nfers from task to task, so it is an important hyper-\nparameter to consider when tuning PEARL.\nAction execution is necessary:\nDo we actually\nneed to execute the generated plans to answer these\nquestions? Feeding just the generated plan to the\nmodel along with the question (minus any execu-\ntion results) may still encourage the LLM to follow\nthe plan\u2019s reasoning steps and generate a better\nanswer. However, we observe that removing the\nexecution results from the model\u2019s input reduces\nabsolute accuracy by around 3 points, which sug-\ngests that it is important to perform multiple passes\nover the document to execute each action before\n9The short, long, and longer splits correspond to average\nannotation scores on the amount of required context [1, 3), [3,\n3.5), and [3.5, 4), respectively.\n10We additionally preserve the CONCAT action in this set-\nting due to its necessity when aggregating execution results.\n1\n20\n80\n140\nNumber of Actions\n64\n66\n68\n70\nAccuracy\nQuALITY Long\nFigure 7: PEARL accuracy given in-context action sets\nof various sizes. Having too few or too many actions\nimpairs the performance.\nanswering the original question. With that said, we\ndo observe a modest improvement over the GPT-4\nzero-shot and CoT baselines (\u223c 2 absolute points),\nwhich suggests that the plan itself is also valuable.\nSelf-refinement improves performance:\nTo re-\nduce human input, the majority of the plan gen-\neration demonstrations for PEARL are generated\nby the LLM with self-refinement. We observe that\nself-refinement is critical to performance: with-\nout it, the overall accuracy drops nearly 3 absolute\npoints (see ablations in Table 2), which highlights\nthe importance of high-quality few-shot examples\nfor plan generation.\n6\nAnalysis\nIn this section, we analyze the behavior of PEARL\nby diving into the composition of its generated\nplans, its most preferred actions, and what types of\nquestions it improves most on. We also offer a qual-\nitative error analysis as well as a human evaluation\non the correctness of the generated plans.\nPlan statistics:\nPlans are roughly 4 actions long\non average, with around 3.4 unique actions per\nplan. The most commonly used actions are shown\nin Figure 8. Apart from the string concatenation\naction CONCAT, the most frequently used action\nis FIND_CHARACTER, which can be convenient\nfor understanding long literary text. Other less\noften used actions cover both those that can transfer\nacross domains, e.g., COMPARE, and those specific\nto narrative understanding, e.g., FIND_EMOTION.\nAccuracy by reasoning types:\nSince QuALITY\nquestions require different reasoning strategies to\nsolve, what types of reasoning does PEARL help\nimprove the most? To this end, we further eval-\nuate questions based on the type of reasoning re-\n7\nErr. Category\nQuestion\nModel Generated Plan or Answer\nExplanation\nTrue Negative\n- Error in Plan\n(17.5%)\nDoes the tone of the passage\nshift at all, and if it does, how\ndoes it shift?\n(Plan) ...\n3.\ntone_shift = COM-\nPARE(CTX, tone_initial, tone_final,\n\u201ctone\u201d) : Compare the initial and fi-\nnal tones of the passage to determine\nif there is a shift...\nSince the plan only compares the ini-\ntial and final tone, the final answer fails\nto capture the changes in between, thus\nleads to an incorrect answer.\nTrue Negative\n- Error in Exec.\n(55%)\nHow many adult characters have\nspeaking roles?\n(Answer) In the input article, there\nare 3 adult characters with speaking\nroles...\nThe correct answer involves two char-\nacters, whereas PEARL\u2019s response mis-\ntakenly includes an additional name.\nThe plan for this question is reason-\nable, but the problem stems from the\nexecution of individual steps.\nFalse Negative\n(12.5%)\nDoes the story have a good end-\ning?\n(Answer: Unclear, the\nstory ends as Evelyn enters a\ndangerous situation)\n(Answer) ...However, the ending of\nthe story is somewhat ambiguous and\nleaves several questions unanswered.\nFor instance, it is unclear whether Eve-\nlyn will be able to successfully com-\nplete her mission ...\nIn this example, the model output is\ncorrect, but is mapped to an incorrect\ndistractor option, which contains direct\ncontradictions with the model output.\nOther\n(15%)\nWho would most likely enjoy\nthis story, of the following op-\ntions?\n(Answer) The target audience of the\ninput article is science fiction enthusi-\nasts, particularly those who enjoy sto-\nries about space exploration, alien en-\ncounters...\nThe model output is not necessarily\nwrong in the absence of options. How-\never, when provided with options dur-\ning mapping stage, one of the other\noptions is clearly better.\nTable 3: Examples of errors exhibited by PEARL answers.\nCONCAT\nFIND_CHARACTER\nFIND_ELEMENT\nIDENTIFY_ELEMENT\nFIND_EVENT\nFIND_BEHAVIOR_REASON\nFIND_RELATION\nFIND_EMOTION\nFIND_DETAILS\nCOMPARE\n0\n250\n500\n750\n1000\nFrequency\nTop-10 used actions\nFigure 8: Top-10 most frequently used actions by\nPEARL.\nquired to answer them.11 Table 4 shows that PEARL\nsignificantly improves three reasoning types: why\nquestions (reasoning about a cause), person ques-\ntions (reasoning about the person(s) involved in an\nevent), and not/except questions (e.g., \u201cwhich of\nthe following is not a reason for...\u201d).\nPEARL\nis\nsignificantly\nslower\nthan\nzero-\nshot prompting:\nThe improved performance of\nPEARL comes at the cost of longer running time\nand cost. With an average of 30 examples, PEARL\nneeds to handle 4.4 times more tokens in the\nprompt and generate 1.3 times more tokens ow-\ning to the intermediate steps.\n11We prompt GPT-4 with the definition of each reasoning\ntype presented in QuALITY\u2019s Appendix (Pang et al., 2022)\nand ask it to label each question with up to two reasoning\ntypes.\nCount\nGPT-4\nPEARL\nGPT-4\nzero-shot\nDescription\n320\n0.73\n0.73\nWhy/reason\n316\n0.79\u2217\n0.71\u2217\nSymbolism/interpretation\n262\n0.73\n0.70\nPerson\n216\n0.75\u2217\n0.66\u2217\nEvent\n199\n0.69\n0.68\nNot/except\n118\n0.70\u2217\n0.53\u2217\nHow/method\n100\n0.74\n0.73\nRelation\n89\n0.71\n0.65\nEntity\n74\n0.64\n0.68\nNumeric\n49\n0.67\n0.78\nLocation\n32\n0.59\n0.59\nWhat if\n21\n0.71\n0.76\nObject\n14\n0.64\n0.64\nDuration\n18\n0.78\n0.89\nFinish the sentence\n10\n0.9\n0.8\nTable 4: Accuracy by reasoning types. \u2217 denotes statis-\ntically significant improvement with p-val < 0.005.\nSpecific examples where PEARL helps:\nTo bet-\nter understand PEARL, we qualitatively analyze\n40 examples for which zero-shot GPT-4 generates\nincorrect answers while PEARL answers correctly.\nThis analysis reveals two key advantages of PEARL.\nFirst, while zero-shot prompting is reasonably good\nat finding salient information from the input docu-\nment, its generative answers tend to be based only\non local context around this information. For in-\nstance, when asked about the number of wives the\ncharacter \u201cDan Merrol\u201d has, the baseline success-\nfully identifies six names that appear to be Dan\u2019s\nwives. However, PEARL takes into account the rev-\nelation that these names \u201cwere actually memories\n8\nfrom the brain donors whose parts were used to\nreconstruct his brain\u201d and thus correctly reasons\nthat Dan only has one wife. In this case, PEARL\nprovides answer that demonstrates a more compre-\nhensive understanding of the entire article. Second,\nPEARL generates more detailed and thorough an-\nswers. For instance, given the question \u201cWhy is\nKumaon a good region for potential forest preser-\nvation?\u201d, the zero-shot answer considers only one\naspect of the reason, whereas PEARL elaborates on\nmultiple aspects. This allows PEARL\u2019s answer to be\nmapped to the correct option (\u201cAll other choices\u201d),\nwhile the zero-shot answer maps to the option cor-\nresponding to the single aspect.\nWhere does PEARL go wrong?\nWe additionally\nexamine 40 examples for which PEARL answers\nincorrectly, and we categorize the errors into three\ncategories (detailed examples and explanations in\nTable 11):\n\u2022 True\nnegatives:\nQuestions\nfor\nwhich\nPEARL\u2019s generative answer is mapped to the\nwrong option. This category can be further di-\nvided into two subcategories: (1) cases where\nthe plan has critical issues, and (2) cases\nwhere the plan is satisfactory but the inter-\nmediate execution produces incorrect output.\nOut of the 40 examples, 29 are true negatives,\nwith 7 plan errors and 22 execution errors.\n\u2022 False negatives:\nQuestions for which\nPEARL\u2019s generative answers are correct but\nincorrectly mapped to the wrong option. This\nkind of error is unavoidable as we use LLM\nfor automatic answer mapping. Out of the 40\nexamples, 5 are false negatives.\n\u2022 Other: Some QuALITY questions are heavily\ndependent on the options; that is, the correct\nanswer can only be determined after exam-\nining all the options. For instance, Table 11\npresents a question asking who would enjoy\nthe story the most of the given options. Al-\nthough PEARL offers an answer based on the\nstory\u2019s genre\u2014which is not incorrect\u2014it is\nnot as accurate as the gold label. Furthermore,\nthere are instances where the model\u2019s free-\nform answers lack sufficient details and can\nthus be mapped to more than one option or\nno options at all. We classify these responses\nas a separate category. Out of 40 examples, 6\nfall into this Other category.\nHuman annot. category\n# of plans\nUnnecessary steps\n15\nSteps can be merged\n2\nPlan misses information\n3\nPlan may lead to incorrect answer\n4\nPlan needs slight edit\n7\nTable 5: human freeform feedback aggregation\nHuman evaluation of model-generated plans:\nThe quality of plans generated by PEARL is criti-\ncal, as they serve as the basis for the plan execution\nstage. To gain further insight on the quality of these\nplans, we perform a human evaluation by hiring\nannotators on Upwork12 to provide feedback on the\ngenerated plans.13 Concretely, we ask annotators\nto assess (1) the correctness of the plans (binary\nchoice), assuming error-free execution at each step,\nand (2) provide free-form feedback on any flaws\nor potential improvements. On average, annotators\nregard over 97% of all plans as correct, with over\n94% confidence, although these numbers are in-\nflated because the annotators do not have access to\nthe long story when making these judgments. More\ninterestingly, Table 5 displays their feedback aggre-\ngated over common themes, which shows that the\nprimary issue with existing plans is the presence\nof unnecessary steps (10% of the total annotated\nplans). Annotators also notice that GPT-4 can be\ninattentive to subtle details while generating plans.\nFor example, given the question \u201cDo you think it\nwould be fun to live in the universe in which this\nstory takes place?\u201d, the model decides to \u201cevaluate\nthe pros and cons of living in the universe based\non the features found in the input article\u201d. How-\never, human annotator argues that \u201cjust because\nsomething is positive doesn\u2019t necessarily mean it\nis \u201cfun\u201d. Any pros on the list might outweigh the\ndangers noted, resulting in an incorrect answer of\n\u2019yes\u2019...\".\n7\nConclusion\nIn this work, we introduce PEARL, a framework for\ntackling complex reasoning over long documents.\nTo answer a question, PEARL first proposes a plan\nbased on a set of actions mined from a training\nset, and then it executes the plan step by step via\nprompting itself with a template filled with output\n12We pay the annotators at the rate of $25/h.\n13We provide a few examples in Appendix E.\n9\nfrom previous stages. We demonstrate the effective-\nness of PEARL on a challenging subset of QuAL-\nITY. Experiments and analysis show that prompt-\ning GPT-4 with PEARL yields more accurate and\ncomprehensive answers than zero-shot and chain-\nof-thought prompting, and human annotators judge\nthe generated plans to be reasonable.\nLimitations\nWhile PEARL shows promising results for long\ndocument reasoning, there are several limitations\nto our approach. Like other prompting methods,\nPEARL is susceptible to generating misinformation\nor hallucinations. It is also more time-consuming\nand computationally costly than the baseline ap-\nproach of directly prompting an LLM to answer the\nquestion. Moreover, PEARL may over-complicate\nsimple questions that only need superficial reason-\ning over long-form narratives. Finally, PEARL is\nstill bounded by the maximum context window size\nof the LLMs. Overall, our work leaves many inter-\nesting directions in this space (e.g., new datasets,\nmodules, stage refinements) open for exploration.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nAvi Caciularu, Ido Dagan, Jacob Goldberger, and Ar-\nman Cohan. 2022. Long context question answering\nvia supervised contrastive learning. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2872\u20132879,\nSeattle, United States. Association for Computational\nLinguistics.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022.\nProgram of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. arXiv preprint\narXiv:2211.12588.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan,\nNoah A. Smith, and Matt Gardner. 2021. A dataset\nof information-seeking questions and answers an-\nchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 4599\u20134610, On-\nline. Association for Computational Linguistics.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and Matt\nGardner. 2022. Successive prompting for decom-\nposing complex questions. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1251\u20131265, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nArthur C Graesser, Murray Singer, and Tom Trabasso.\n1994. Constructing inferences during narrative text\ncomprehension. Psychological review, 101(3):371.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve.\narXiv\npreprint arXiv:2210.11610.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards reasoning in large language models: A survey.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n2023. Decomposed prompting: A modular approach\nfor solving complex tasks.\nTom\u00e1\u0161 Ko\u02c7cisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris\nDyer, Karl Moritz Hermann, G\u00e1bor Melis, and Ed-\nward Grefenstette. 2018. The NarrativeQA reading\ncomprehension challenge. Transactions of the Asso-\nciation for Computational Linguistics, 6:317\u2013328.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 4940\u20134957, Online. Association for Computa-\ntional Linguistics.\n10\nJieyi Long. 2023. Large language model guided tree-of-\nthought.\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-\nWei Chang, Ying Nian Wu, Song-Chun Zhu, and Jian-\nfeng Gao. 2023. Chameleon: Plug-and-play compo-\nsitional reasoning with large language models. arXiv\npreprint arXiv:2304.09842.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nMaxwell Nye, Michael Henry Tessler, Joshua B. Tenen-\nbaum, and Brenden M. Lake. 2021. Improving coher-\nence and consistency in neural sequence models with\ndual-system, neuro-symbolic reasoning. In Advances\nin Neural Information Processing Systems.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nRichard Yuanzhe Pang, Alicia Parrish, Nitish Joshi,\nNikita Nangia, Jason Phang, Angelica Chen, Vishakh\nPadmakumar, Johnny Ma, Jana Thompson, He He,\nand Samuel Bowman. 2022. QuALITY: Question\nanswering with long input texts, yes! In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5336\u20135358,\nSeattle, United States. Association for Computational\nLinguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori\nYoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, and Omer Levy. 2022.\nSCROLLS: Standardized CompaRison over long lan-\nguage sequences. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 12007\u201312021, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nNoah Shinn, Beck Labash, and Ashwin Gopinath.\n2023.\nReflexion: an autonomous agent with dy-\nnamic memory and self-reflection. arXiv preprint\narXiv:2303.11366.\nIvan Stelmakh, Yi Luan, Bhuwan Dhingra, and Ming-\nWei Chang. 2022. ASQA: Factoid questions meet\nlong-form answers. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 8273\u20138288, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. 2022. Learning\nto summarize from human feedback.\nDan Su, Xiaoguang Li, Jindi Zhang, Lifeng Shang, Xin\nJiang, Qun Liu, and Pascale Fung. 2022. Read before\ngenerate! faithful long form question answering with\nmachine reading. In Findings of the Association for\nComputational Linguistics: ACL 2022, pages 744\u2013\n756, Dublin, Ireland. Association for Computational\nLinguistics.\nHaitian Sun, William Cohen, and Ruslan Salakhutdinov.\n2022. ConditionalQA: A complex reading compre-\nhension dataset with conditional answers. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3627\u20133637, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nHaitian Sun, William W. Cohen, and Ruslan Salakhutdi-\nnov. 2021. Iterative hierarchical attention for answer-\ning complex questions over long documents.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008\u20135020, Online. Asso-\nciation for Computational Linguistics.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu,\nYunshi Lan, Roy Ka-Wei Lee, and Ee-Peng Lim.\n11\n2023a. Plan-and-solve prompting: Improving zero-\nshot chain-of-thought reasoning by large language\nmodels.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H. Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023b. Self-consistency improves\nchain of thought reasoning in language models. In\nThe Eleventh International Conference on Learning\nRepresentations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nFangyuan Xu, Junyi Jessy Li, and Eunsol Choi. 2022.\nHow do we answer complex questions: Discourse\nstructure of long-form answers. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3556\u20133572, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik R Narasimhan, and Yuan Cao.\n2023b. React: Synergizing reasoning and acting\nin language models. In The Eleventh International\nConference on Learning Representations.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. 2023.\nAnswering\nquestions by meta-reasoning over multiple chains\nof thought. arXiv preprint arXiv:2304.13007.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\nA\nGPT-4 Multiple-choice setup\nperformance\nWhile our primary focus is on the generative QA\nsetup in the main text, we provide GPT-4\u2019s perfor-\nmance under the standard multiple-choice setup\nhere in the Appendix. On the entire QuALITY dev\nset, GPT-4 achieves an accuracy of 84.4%. For the\n1000 challenging question set, GPT-4 reaches an\naccuracy of 78.7%, nearly 10 points higher than the\nGPT-4 zero-shot generative baseline. This result\nsuggests that there is still room for improvement in\nGPT-4\u2019s generative answers. We also observe that\nGPT-4 is sensitive to the ordering of the provided\noptions. We further evaluate GPT-4 with three shuf-\nfled versions of the options (swap A and D, B and\nC; swap A and C, B and D; swap A and B, C and\nD). While the overall accuracy of these versions\nremains similar, the questions that are consistently\nanswered correctly across all four option orderings\ndrop to 68.7%. This result raises the question of\nwhether GPT-4 truly \u201cunderstands\u201d the question\nand further motivates the generative QA setup.\nB\nVerify Accuracy of Answer Mapping\nAs demonstrated in Section 6, the mapping stage is\nnot always reliable. To understand the frequency of\nmapping errors, we conduct a small-scale human\nanswer mapping study. We recruit three profession-\nals on Upwork. We randomly select 50 questions\nand ask annotators to read PEARL output and then\nmap it to one of the provided options. On average,\nannotators agree with \u223c83% of GPT-4 mappings,\nwith inter-annotator agreement on four-class set-\ntings of \u03ba = 0.677. For questions where annotators\ndisagree with each other or do not concur with\nGPT-4, they tend to be those that can be mapped to\nthan one option or none of the options. We believe\nthis level of accuracy is decent enough to let GPT-4\nperform the mapping step for evaluation.\nC\nCan PEARL benefit from more\nhuman-written examples?\nWhile we have employed self-refinement and exe-\ncuted the model-generated plan to ensure the qual-\nity of ICL demonstrations, it is natural to ask if we\ncan further improve PEARL by incorporating more\nquality-assured human-written examples. There-\nfore, we evaluate an alternative version of PEARL\nin which the in-context examples for plan genera-\ntion are replaced with 11 human-written examples.\nThis variant achieves 70.3, 76.8, and 72.3 on the\nlong split, the short split, and the total evaluation\ndata, respectively. These results suggest that ad-\nditional human input may note be necessary to\nachieve strong results.\n12\nPrompt for Action Mining\n[Actions]\n- CONCAT(S1, S2, ...) : Concatenate the input S1, S2, ...\n- EXTRACT(CTX, X) : Extract the exact wording that X is referring to from input CTX.\n- FIND_X(CTX, X): Find and summarize all relevant information about X in the input CTX.\n- FIND_REASON(CTX, X) : Find and summarize the cause or reason of X given input CTX.\n- FIND_MORAL(CTX) : Find the intended lesson or moral of the input CTX.\n- SUMMARIZE(CTX): Provides a general summary about the given CTX.\n- SUMMARIZE_X(CTX, X) : Provides a summary about X given the provided input CTX.\n[Instructions]\nSuppose you are given a question about an article as well as a list of actions that you can execute to solve the question (shown\nabove). You can imagine the actions as functions in a program, where you have input arguments and output. The output of an\naction can be fed as input to another action. The output of the final action will be the answer to the given question. Suppose you\nhaven\u2019t read the article yet, please present a sequence of actions that you would use to answer the question.\nHere are a few examples:\nQuestion:\nWhat is the \u201cspace cafard\u201d that Si describes?\nMy new actions:\n- COMPREHEND(CTX, X) : Provide a detailed comprehension of X given the input CTX.\nMy sequence of actions:\n1. snippet = EXTRACT(CTX, \"space cafard\") : Extract the exact wording regarding \"space cafard\" from the input CTX.\n2. ans = COMPREHEND(CTX, X) : Provide a detailed comprehension of the input X given the input CTX.\nQuestion:\nWhy did the author write the article?\nMy new actions:\n- None\nMy sequence of actions:\n1. moral = FIND_MORAL(CTX) : Find the intended lesson or moral of the input CTX.\nYour answer must follow the following rules: 1. The present sequence should be minimal, i.e., no unnecessary actions. 2. The\nsequence of actions should be specific and cover every detail about the question. 3. The sequence of actions should use as many\nas existing actions as possible. 4. It is fine to create new actions, however, the created new actions should be maximally reusable\nand generalizable to other reading comprehension questions. 5. The arguments should cover all the details of the given question.\n[Question]\n{Question}\n[Answer]\nNow please provide the plan for the above question.\nYour answer should follow the format:\nMy new actions (if any):\n- my_new_action_1(here goes the arguments) : [one-sentence explanation]\n- my_new_action_2(here goes the arguments) : [one-sentence explanation]\n...\nMy sequence of actions:\n1. output_1 = action_1(here goes the arguments) : [one-sentence explanation]\n2. output_2 = action_2(here goes the arguments) : [one-sentence explanation]\n...\nTable 6: Prompt for action mining. {Question} indicates the placeholder for filling in training set question. In this\nstage, we only care about the new actions proposed by the model.\n13\nMined Actions after reducing number of actions with LLM\nANALYZE(CTX, X, Y) # Analyze the relationship, attitude, or feelings between X and Y, or the character, language, tone, or\nsymbolism of X given the input CTX.\nCOMPARE(CTX, X, Y, Z) # Compare X and Y in the context of Z, considering aspects such as abilities, assets, attractiveness,\nbehavior, concerns, contributions, cultures, events, experiences, feelings, ...\nCOMPREHEND(CTX, X) # Provide a detailed comprehension of X given the input CTX.\nCONCAT(S1, S2, ...)\nDEFINE(CTX, X) # Provide the definition of X given the input CTX.\nDESCRIBE(CTX, X, Y) # Provide a description of X in terms of Y, such as character, genre, or introduction given the input\nCTX.\nEVALUATE(CTX, X, Y) # Evaluate aspects such as feeling, outcome, performance, personalities, risk, or truth of X in relation\nto Y given the input CTX.\nEXCEPT(CTX, LIST) # Find the item that is not mentioned in the input CTX but is present in the given..\nEXPLAIN_PROCESS(CTX, X) # Provide a detailed explanation of the process X given the input CTX.\nFIND_BARRIERS_CAUSES(CTX, X) # Find and summarize the remaining barriers or causes related to X given the input CTX.\nFIND_BEHAVIOR_REASON(CTX, X) # Find the reason behind the behavior X given the input CTX.\nFIND_BENEFIT(CTX, X) # Find the direct benefit of X given the input CTX.\nFIND_BEST(CTX, X, Y) # Find the best X in the context of Y given the input CTX.\nFIND_CHARACTER(CTX, X) # Find and summarize the character traits, transformation, and changes of X given the input\nCTX.\nFIND_COMMON(CTX, X, Y, Z) # Find the common ground, characteristics, or commonalities between X, Y, and Z given the\ninput CTX.\nFIND_CONDITION(CTX, X, Y) # Find the condition, outcome, or consequences related to X and Y given the input CTX.\nFIND_CONFLICT_CONCERN(CTX, X, Y) # Find the conflict, concern, or disagreement between X and Y given the input\nCTX.\nFIND_CONSISTENCY(CTX, X) # Determine if X is consistent throughout the input CTX.\nFIND_DECISION(CTX, X) # Find the decision, factor, or event that influenced X\u2019s decision in the input CTX.\nFIND_DESCRIPTION(CTX, X) # Find all descriptions, characteristics, or words that describe X given the input CTX.\nFIND_DETAILS(CTX) # Find all the details about a topic (e.g., contract, city-state) discussed in the input CTX.\nFIND_DIALOGUE(CTX, X, Y) # Find the dialogue between X and Y in the input CTX.\nFIND_DIFFICULTY_DANGER(CTX, X) # Find the most difficult aspect, challenge, or danger faced by X in the given input\nCTX.\nFIND_ELEMENT(CTX, X, Y) # Find the element X related to Y given the input CTX. This function can cover message, method,\nmetrics, mismatch, mission, mistake, most likely, motif, motivation, nationalities, negative critique, negative effect, next event,\nnormal, objective, obstacles, ...\nFIND_EMOTION(CTX, X, Y) # Find the emotion or feeling X feels towards Y given the input CTX.\nFIND_ENDING(CTX, X) # Find the ending or conclusion of X\u2019s story or the input CTX.\nFIND_EVENT(CTX, X) # Find the event involving X in the input CTX (e.g., betrayal, change, climax).\nFIND_EVIDENCE_EXAMPLE(CTX, X) # Find evidence or an example supporting X given the input CTX.\nFIND_EXCEPTION(CTX, X, Y, Z) # Find the exception or characteristic that is not common among X, Y, and Z given the input\nCTX.\nFIND_EXPECTATION(CTX, X) # Find the expectation, assumption, or impact about X given the input CTX.\nFIND_EXPLANATION(CTX, X) # Find the most likely explanation, critique, or doubt for X given the input CTX.\nFIND_FACT_FALSE(CTX, X) # Find a definite fact or false statement about X given the input CTX.\nFIND_FEARS_DISTRACTIONS(CTX, X) # Find the fears, concerns, or distractions of X given the input CTX.\nFIND_FEATURES(CTX, X) # Find all the features that X cares about given the input CTX.\nFIND_FIRST_INSTANCE(CTX, X) # Find the first instance of X happening in the input CTX.\nFIND_FLAW(CTX, X) # Find the greatest flaw of X given the input CTX.\nFIND_FOCUS(CTX, X) # Find the person or object that is focused on the most in the input CTX, given a list of X.\nFIND_FORESHADOW(CTX, X, Y) # Find the instance where X foreshadows Y in the input CTX.\nFIND_FUTURE(CTX, X) # Find the future, predicted outcome, or action of X given the input CTX.\nFIND_GRIEVANCE(CTX, X) # Find and summarize the grievance X has against something or someone in the input CTX.\nFIND_HALO_EFFECT(CTX, X) # Find and summarize one halo effect of X given the input CTX.\nFIND_HUMBLENESS(CTX, X) # Find the instances of humbleness presented by X in the input CTX.\nFIND_HYPOTHETICAL(CTX, X) # Find the hypothetical outcome or consequence of X given input CTX.\nFIND_IMAGINATION(CTX, X) # Find and summarize how X imagines something in the input CTX.\nFIND_IMPACT(CTX, X, Y) # Find the event or experience that had the strongest impact on X\u2019s Y given the input CTX.\n...\nTable 7: A subset of mined actions from training set questions.\n14\nPrompt for Generating Plan\n[Actions]\nANALYZE(CTX, X, Y) # Analyze the relationship, attitude, or feelings between X and Y, or the character, language, tone, or\nsymbolism of X given the input CTX.\nCOMPARE(CTX, X, Y, Z) # Compare X and Y in the context of Z, considering aspects such as abilities, assets, attractiveness,\nbehavior, concerns, contributions, cultures, events, experiences, feelings, focus, intelligence, irony, nationalities, performance,\npraise, reactions, reviews, secretiveness, time periods, treatment, truth, or worlds given the input CTX.\nCOMPREHEND(CTX, X) # Provide a detailed comprehension of X given the input CTX.\nCONCAT(S1, S2, ...)\nDEFINE(CTX, X) # Provide the definition of X given the input CTX.\nDESCRIBE(CTX, X, Y) # Provide a description of X in terms of Y, such as character, genre, or introduction given the input\nCTX.\nEVALUATE(CTX, X, Y) # Evaluate aspects such as feeling, outcome, performance, personalities, risk, or truth of X in relation\nto Y given the input CTX.\n...\n{List of Actions as shown in Table 7}\n[Instructions]\nSuppose you are given a question about an article, as well as a list of potential actions (shown above) that you can execute to\nsolve the question . You can imagine the actions as functions in a program, where you have input arguments and output. The\noutput of an action can be fed as input to another action. Please present a sequence of actions that you would use to answer the\nquestion after you read the article. The sequence of actions should be specific and cover all the details about the question. Please\nprioritize using the actions presented in the list above. If you need to add new actions, please follow the format below. Please\nassign the output of each action with a distinct name, which can be passed into other actions as argument. Think twice before\nyou provide your answer. Make sure your answer is valid, clear, and easy to understand. Keep the answer simple and remove\nany unnecessary steps. Do not use list comprehension or dictionary comprehension. Keep each action minimally simple. If a\nquestion is unanswerable (e.g., requires options), collect as much information as possible from the input such that it will be\nanswerable when provided with options. Your answer should follow the format:\n\u201d\u2019\nNew actions:\n- new_action_1(arguments) : [one-sentence general explanation] or \"-None\" if there no need to add new actions\n- new_action_2(arguments) : [one-sentence general explanation] or \"-None\" if there no need to add new actions\n1. output_1 = action_1(here goes arguments) : [one-sentence explanation]\n2. output_2 = action_2(here goes arguments) : [one-sentence explanation]\n...\n\u201d\u2019\nThe following are a few examples\nQuestion: \"How do Ross and Mehta view Brown\u2019s acquisition of the magazine?\"\nAnswer:\nNew actions:\n- FIND_OPINION(CTX, X, Y) : Find the opinion of X about Y given the input CTX\n1. ross = FIND_CHARACTER(CTX, \"Ross\") : Identify who Ross is in the input article\n2. mehta = FIND_CHARACTER(CTX, \"Mehta\") : Identify who Mehta is in the input article\n3. brown = FIND_CHARACTER(CTX, \"Brown\") : Identify who Brown is in the input article\n4. magazine_acquisition = FIND_EVENT(CTX, \"Brown\u2019s acquisition of the magazine\") : Find the event of Brown\u2019s acquisition\nof the magazine in the input article\n5. ross_opinion = FIND_OPINION(CTX, ross, magazine_acquisition) : Find the opinion of Ross about Brown\u2019s acquisition of\nthe magazine\n6. mehta_opinion = FIND_OPINION(CTX, mehta, magazine_acquisition) : Find the opinion of Mehta about Brown\u2019s acquisition\nof the magazine\n7. ans = CONCAT(ross_opinion, mehta_opinion) : Combine the opinions of Ross and Mehta on Brown\u2019s acquisition of the\nmagazine to form the final answer\n... {more few-shot examples} ...\n[Question]\nNow you are given a question about an article:\n{question}\nPlease provide a plan (sequence of actions) that can arrive to the answer after reading the article. As the corresponding options\nare not provided for the question, when the question is not answerable without the options, simply collect as much information\nas possible from the input such that it will be answerable with the options. Make sure the plan you generate is valid and faithful\nto the question.\n[Answer]\nTable 8: Prompt for generating plan given a question, which is filled in the placeholder {question}.\n15\nPrompt for Executing Single Step of the Plan\nArticle\n{Long document}\nEnd of Article\n\u2014\nPlease read the above text first, and then follow the instructions below.\n[Instructions]\n{Mined action and corresponding definition of current step. Example shown below.}\nFIND_EMOTION(CTX, X, Y) # Find the emotion or feeling X feels towards Y given the input CTX.\n{Current step in the plan generated in the previous stage. Example shown below.}\nkolin_opinion = FIND_EMOTION(CTX, kolin, \u201cbecoming a tree\u201d)\n{Value assignment of input argument(s)}\nX = \u201cIn the story, Kolin is a steward from the Planetary State of Haurtoz who is part of a scouting party sent to explore a planet\nafter their ship, the Peace State, is damaged. Kolin is unhappy with the oppressive regime on Haurtoz and dreams of escaping it.\nWhile exploring the planet, he encounters a tree named Ashlew, which is actually a man who has transformed into a tree. Ashlew\ntells Kolin about the Life, a powerful entity on the planet that can help individuals change their form...{Output from previous\nstep.}\u201d\nY = \u201cbecoming a tree\u201d\n[Answer]\n{A brief description of current step.}\n(Find the emotion or feeling Kolin has towards becoming a tree himself in the input article)\nTable 9: Prompt for executing a step in a plan. Prompt of this step is a template with placeholders which will be\nfilled with the output from previous step(s).\nPrompt for Answer Mapping\nRelevant information for answering the question:\n{Generative answer output by PEARL.}\nQuestion: {question}\n{Options}\nRead the relevant information about the article and answer the question by selecting the best option above. Only one of them is\ncorrect.\nAnswer (select from A, B, C, D):\nTable 10: Prompt for mapping generative answer to provided options\n16\nD\nPrompts and templates used in PEARL\nE\nHuman feedbacks on model-generated\nplan\n17\nQuestion & Options\nPlan\nHuman feedback\nWhat was Mrs.\nJamieson\u2019s biggest\nproblem in the story?\nA: She did not understand the Stinson\nEffect\nB: She had to raise her son alone\nC: She was just able to make ends meet\nD: She had to hide her scar\n1.\nmrs_jamieson\n=\nIDEN-\nTIFY_ELEMENT(CTX, \"Mrs. Jamieson\") :\nIdentify who Mrs. Jamieson is in the input\narticle\n2.\nmrs_jamieson_problems\n=\nFIND_ELEMENT\n(CTX,\n\"problems\",\nmrs_jamieson) :\nFind and list all the\nproblems faced by Mrs.\nJamieson in the\nstory\n3. biggest_problem = FIND_BEST(CTX,\n\"problem\", mrs_jamieson_problems) : Find\nthe biggest problem faced by Mrs. Jamieson\nin the story\n4. ans = CONCAT(mrs_jamieson, biggest\n_problem) : Combine Mrs. Jamieson and her\nbiggest problem to form the final answer\nSteps 2 & 3 can be combined\ninto one. mrs_jamieson_biggest\n_problems = FIND_ELEMENT\n(CTX,\n\"biggest\nproblem\",\nmrs_jamieson):\nFinds\nthe\nbiggest problem faced by Mrs.\nJamieson in the story\nWhich class was the least intense?\nA: Kung Fu\nB: Tai Chi\nC: Aikido\nD: Brazilian Jujitsu\n1. classes = FIND_DETAILS(CTX) : Find\nall the details about the classes and their in-\ntensity levels mentioned in the input article\n2.\nleast_intense_class\n=\nFIND_LEAST_DESCRIBING_WORD\n(classes, \u201cintense\u201d) : Find the class that is\nleast intense in the given input CTX\n3.\nans = CONCAT (least_intense_class,\nclasses) : Combine the least intense class\nand the intensity levels of all classes to form\nthe final answer\nAdding the details for the other\nclasses is unnecessary since the\nquestion is looking for a single\nanswer, the least intensive class.\nDo the Terrans ever come close to win-\nning the battle within the story?\nA: No, they continually lose\nB: They win the whole battle with less\ncasualties\nC: Yes, by the surprise squadron Eve-\nlyn leads\nD: Yes, by Evelyn cloning soldiers into\nbattle\n1. terrans = IDENTIFY_ELEMENT(CTX,\n\"Terrans\") : Identify who the Terrans are in\nthe input article\n2. battle = FIND_EVENT(CTX, \"battle\") :\nFind and summarize the battle event within\nthe story in the input article\n3. terrans_battle = FIND_RELATION(CTX,\nterrans, battle) : Find and summarize the Ter-\nrans\u2019 involvement in the battle from the input\narticle\n4. terrans_close_win = FIND_CONDITION\n(CTX, \"Terrans\", \"close to winning\") : Find\nthe condition or events where the Terrans\ncome close to winning the battle in the in-\nput article\n5.\nans = CONCAT(terrans_battle, ter-\nrans_close_win) : Combine the Terrans\u2019 in-\nvolvement in the battle and the events where\nthey come close to winning to form the final\nanswer\nStep 2 and 3 can be combined:\nFind and summarize the Terrans\u2019\nbattle event within the story in\nthe input article\nWhat level of depth does the author\nprovide on the subjects they use to\nmake their case?\nA: Language is really the only thing\ncovered in any depth\nB: A broad, but not very deep assess-\nment\nC: They provide the reader with deeper\narguments about the monetary system\nand striking tendencies than anything\nelse\nD: They provide deep, explanatory\nstatistics to most arguments\n1. author = IDENTIFY_ELEMENT(CTX,\n\"author\") : Identify the author of the article\n2. subjects = FIND_ELEMENT(CTX, \"sub-\njects\", author) : Find and list all the subjects\nthe author uses to make their case in the input\narticle\n3. depth_analysis = ANALYZE(CTX, sub-\njects, author) : Analyze the level of depth the\nauthor provides on the subjects they use to\nmake their case in the input article\n4. ans = CONCAT(subjects, depth_analysis) :\nCombine the subjects and the depth analysis\nto form the final answer for comparing with\nthe options\nVery good plan.\nTable 11: Example human feedback from annotators on PEARL-generated plans.\n18\n"
  },
  {
    "title": "LLMs as Factual Reasoners: Insights from Existing Benchmarks and Beyond",
    "link": "https://arxiv.org/pdf/2305.14540.pdf",
    "upvote": "1",
    "text": "LLMs as Factual Reasoners:\nInsights from Existing Benchmarks and Beyond\nPhilippe Laban\nWojciech Kry\u00b4sci\u00b4nski\nDivyansh Agarwal\nAlexander R. Fabbri\nCaiming Xiong\nShafiq Joty\nChien-Sheng Wu\nSalesforce AI\n{plaban, wojciech.kryscinski, dagarwal, afabbri, cxiong, sjoty, wu.jason}@salesforce.com\nAbstract\nWith the recent appearance of LLMs in practi-\ncal settings, having methods that can effectively\ndetect factual inconsistencies is crucial to re-\nduce the propagation of misinformation and\nimprove trust in model outputs. When testing\non existing factual consistency benchmarks, we\nfind that a few large language models (LLMs)\nperform competitively on classification bench-\nmarks for factual inconsistency detection com-\npared to traditional non-LLM methods. How-\never, a closer analysis reveals that most LLMs\nfail on more complex formulations of the task\nand exposes issues with existing evaluation\nbenchmarks, affecting the evaluation precision.\nTo address this, we propose a new protocol\nfor inconsistency detection benchmark creation\nand implement it in a 10-domain benchmark\ncalled SUMMEDITS. This new benchmark is 20\ntimes more cost-effective per sample than previ-\nous benchmarks and highly reproducible, as we\nestimate inter-annotator agreement at about 0.9.\nMost LLMs struggle on SUMMEDITS, with\nperformance close to random chance. The best-\nperforming model, GPT-4, is still 8% below\nestimated human performance, highlighting the\ngaps in LLMs\u2019 ability to reason about facts and\ndetect inconsistencies when they occur.\n1\nIntroduction\nWith recent progress in generation capabilities\nof LLMs, automatic summarization is making\nits appearance in practical information consump-\ntion situations such as summarizing work meet-\nings (Arabzadeh et al., 2022), health records (Jain\net al., 2022), or scientific documents (Cachola et al.,\n2020). To ensure the safe and effective implementa-\ntion of these applications, it is essential to limit the\nreach of factually inconsistent summaries, a known\nissue with generated summaries (Kry\u00b4sci\u00b4nski et al.,\n2019; Maynez et al., 2020).\nPrior work (Kry\u00b4sci\u00b4nski et al., 2020; Fabbri et al.,\n2021; Gao and Wan, 2022) has annotated corpora\nHuman \nPerform.\n100%\n50%\nSummEdits Benchmark \nPerformance\n90.9\n82.4\nOpenAI\n70.7\n60.1\n51.9\nDav1\nDav2\nDav3\nGPT4\nGoogle\nBard\n59.8\nAnthropic\nClaude \n\u00a0v1.3\n55.1 Cmd-\nXL\nCohere\n56.1\nVicuna\n(random\nchance)\n63.7\nBest non-LLM: 65.7\n69.0\nBison\nFigure 1: SUMMEDITS is a benchmark to evaluate the\nfactual reasoning abilities of LLMs, measuring if mod-\nels detect factual inconsistencies when they occur in\nsummaries. Capable detection models can help build\nmore reliable NLG systems.\nof model summaries with labels of factual con-\nsistency, finding that most abstractive summariza-\ntion systems produce a non-negligible portion of\ninconsistent summaries.\nIn turn, such corpora\nare used to instantiate tasks such as inconsistency\ndetection (ID) (Laban et al., 2022a; Tang et al.,\n2022), in which models are given (document,\nsummary) pairs, and must identify whether the\nsummary is consistent with the document.\nRecent investigations of using LLMs for evalua-\ntion have shown promising results across different\nNLP tasks (Liu et al., 2023; Fu et al., 2023), in-\ncluding factual consistency (Luo et al., 2023). In\nthis work we continue this line of research and ex-\nplore applying LLMs as factuality evaluators in\nthe context of text summarization. We first estab-\nlish baseline performance for a suite of LLMs on\nthree existing consistency benchmarks. Accuracy\nresults confirm that some LLMs perform compet-\nitively with state-of-the-art specialized methods\nsuch as QAFactEval (Fabbri et al., 2022). How-\never, a manual analysis of free-text explanations\nthat LLMs generate reveals two key limitations\nof the accuracy-only analysis. Ideally, a model\ncorrectly predicting the consistency label of a sum-\narXiv:2305.14540v1  [cs.CL]  23 May 2023\nmary should be capable of generating an explana-\ntion for its binary prediction. Yet, we find that\nmost LLMs generate explanations that do not accu-\nrately pinpoint factual inaccuracies, with only three\nmodels \u2013 GPT4 (OpenAI, 2023), Claude V1.3 (Bai\net al., 2022), and Bard (Thoppilan et al., 2022) \u2013\ngenerating correct explanations for more than 50%\nof cases we annotated. Second, the manual analy-\nsis in the AggreFact consistency benchmark (Tang\net al., 2022) of conflict cases \u2013 in which GPT4 pre-\ndictions disagree with the dataset label \u2013 reveals\na significant number of mislabeled samples (7+%)\nof factual inconsistencies undetected by annotators\nduring dataset creation that the model explanation\nreveals. This lack of quality of benchmarks lim-\nits the precise evaluation of model performance at\nfactual inconsistency detection.\nTo address this issue, we introduce a protocol\ndesigned to create challenging benchmarks while\nensuring the reproducibility of the labels. The pro-\ntocol involves manually verifying the consistency\nof a small set of seed summaries and subsequently\ngenerating numerous edited versions of these sum-\nmaries. We discover that assessing the consistency\nof edited summaries is relatively straightforward\nand easy to scale for human annotators, thus guar-\nanteeing low cost and high agreement among anno-\ntators, yet keeping the task challenging for models.\nWe create the SUMMEDITS benchmark by imple-\nmenting the protocol in ten diverse textual domains,\nincluding the legal, dialogue, academic, financial,\nand sales domains. Figure 1 summarizes experi-\nmental results on the benchmark, which indicate\nthat SUMMEDITS presents a challenge for both spe-\ncialized models and current LLMs, with only four\nmodels \u2014 GPT3-Davinci003, ChatGPT, PaLM2-\nBison, and GPT4 \u2014 outperforming the specialized\nmodel QAFactEval. Our estimate of human per-\nformance of 90%+ is largely above all model per-\nformance, suggesting most current LLMs are not\nyet proficient at complex factual reasoning, and\nstill cannot assess the factual validity of summaries\nwith precision.\nWe believe SUMMEDITS can serve as a tool for\nevaluating LLMs\u2019 abilities to detect when factual\ninconsistencies (inevitably) occur and encourage\nLLM developers to report their performance on\nthe benchmark. For practitioners requiring specific\ndomain expertise, the protocol can be adapted to\ngenerate low-cost, in-domain benchmarks that can\nprobe for model capabilities prior to production use.\nWe release the code, protocol steps, and benchmark\ndata publicly1.\n2\nRelated Work\nAnnotating Factuality of Summaries.\nWith ad-\nvances in language models and the increase in flu-\nency and abstractiveness of summarizers, prior\nwork showed that one of the key challenges in\nsummarization was enforcing factual consistency\n(Kry\u00b4sci\u00b4nski et al., 2019), particularly with mod-\nels trained on datasets with unfaithful references\n(Maynez et al., 2020). Several efforts \u2013 such as\nFactCC (Kry\u00b4sci\u00b4nski et al., 2020), SummEval (Fab-\nbri et al., 2021), Polytope (Huang et al., 2020),\nFRANK (Pagnoni et al., 2021), and CLIFF (Cao\nand Wang, 2021) \u2013 annotated the generated sum-\nmaries of tens of model, finding that most models\nproduce a non-negligible portion of inconsistent\nsummaries. Although most annotation effort has\nfocused on the summarization of news, some prior\nwork also looked at dialogue summarization (Gao\nand Wan, 2022), or the medical domain (Tang et al.,\n2023). In most work, scalable high-quality anno-\ntation is challenging, due to low inter-annotator\nagreement when relying on crowd-workers, with\nsome work showing that 10+ annotators are re-\nquired to achieve some level of consensus (Falke\net al., 2019), and some work recommending solely\nrelying on experts (Fabbri et al., 2021). At the heart\nof the issue, annotating the factual consistency of a\nsummary is challenging: it requires careful reading\nof long documents and the detection and interpre-\ntation of nuanced facts. In this work, we propose\na new protocol to annotate factual consistency re-\nsources and show that it lowers the cost and in-\ncreases reproducibility by minimizing the amount\nof reasoning required for each annotation.\nDetecting Factual Errors.\nSome work has taken\nan automated approach to the detection of inconsis-\ntencies, with approaches falling into two main cate-\ngories: question and entailment-based. In question-\nbased approaches, questions are generated with the\nexpectation that paired documents and summaries\nshould provide consistent answers. QAFactEval\n(Fabbri et al., 2022) unified prior work (Wang et al.,\n2020; Scialom et al., 2021) by systematically eval-\nuating each element of the pipeline and propos-\ning a best-performing combination. Entailment-\nbased methods either rely on entailment on depen-\n1https://github.com/salesforce/\nfactualNLG\ndency parses, such as with the DAE method (Goyal\nand Durrett, 2020), or directly leverage natural-\nlanguage entailment models, such as SummaC (La-\nban et al., 2022a). We include these three repre-\nsentative models in our experiments and find that\nalthough they require several orders of magnitudes\nfewer parameters than LLMs, they can reach simi-\nlar performances on challenging benchmarks.\n3\nLLM Aptitude In Controlled Setting\nIn this section, we present the initial set of ex-\nperiments that were conducted on the FactCC\nbenchmark (Kry\u00b4sci\u00b4nski et al., 2020). FactCC was\ncreated based on the XSum news summarization\ndataset (Narayan et al., 2018) and consists of news\narticle-summary sentence pairs manually labeled\nbased on their factuality. While simple in nature,\nthe benchmark can serve as a test bed for explor-\ning the basic understanding LLMs have of the task\nat hand. Furthermore, the data points come with\nmanually annotated error types, allowing for exper-\niments in fine-grained error detection.\nIn the following subsections, we define the ex-\nperimental setup, i.e. prompts, models, and data,\nand present the experiment results along with a\ndiscussion.\n3.1\nPrompt Selection\nAs part of this initial study, we explore a wide range\nof prompts that have been shown to unlock some\nof the emergent abilities of LLMs. These prompts\ncan be organized into four groups as follows:\nZero-Shot Prompts (Radford et al., 2019)\nEval-\nuate the zero-shot transfer abilities of models.\nThese prompts are limited to a short task descrip-\ntion and the input data based on which the models\ngenerate their output. In our study, we included\nthree different zero-shot prompts offering varying\nlevels of detail in the task description. The best-\nperforming prompt was selected by a majority vote\nacross models and used as the base for the prompts\ndescribed in the following paragraphs.\nFew-Shot Prompts (Brown et al., 2020)\nEnable\nthe in-context learning abilities of LLMs. These\nprompts include a task description and one or more\ndemonstrations of the task. The provided demon-\nstrations condition the model for the actual input\ndata that the model is expected to process. In this\nstudy we experiment with one-, two-, and three-\nshot prompts which build upon each other.\nChain-of-Thought Prompts (Wei et al., 2022)\nExplore LLM models\u2019 ability to generate step-by-\nstep reasoning for answers and have been shown to\nimprove performance on complex reasoning tasks.\nThe models are given a task description and input\ndata and are asked to generate a series of interme-\ndiate reasoning steps necessary to solve the task\nalongside the answer. We explore chain-of-thought\nprompts both in zero- and few-shot settings.\nGenerate-with-Evidence Prompts (Lampinen\net al., 2022)\nExplore the models\u2019 ability to\npresent evidence for the generated answers and\nhas also been shown to improve performance\non reasoning-intense tasks. Similar to chain-of-\nthought prompts, the models are given a task de-\nscription and input data and are asked to answer\nthe task, and then generate evidence for the chosen\nanswer. In this work we explore generate-with-\nevidence prompts in zero- and few-shot settings.\nPersona-based Prompts (White et al., 2023)\nExtract certain points of view from LLMs or focus\nthem on a set of abilities. Shown to work best with\nchat-tuned LLMs, models are assigned a role, or\n\"persona\", and next prompted to complete a given\ntask. The assigned personas condition the models\non the task at hand. In this work, models were\nassigned the persona of a \u201cjournalist\u201d who is fact-\nchecking a text before publication. Three prompts\nwere tested, where the persona-based prompt was\nused in zero- and few-shot settings, and in combi-\nnation with a chain-of-thought prompt.\nAll prompt templates described in this section\nand used in the study are presented in the associated\ncode repository.\n3.2\nModel Selection\nSimilar to the prompt selection, we begin with eval-\nuating a wide range of methods that can be ap-\nplied to the task of factual consistency evaluation.\nSelected models span different architectures and\ntraining procedures, and can be categorized into\nthe following groups:\nNon-LLM\nModels that were designed and\ntrained specifically for the task of factual\nconsistency evaluation in text summarization.\nThose models include two NLI-based approaches,\nDAE (Goyal and Durrett, 2020) and SummaC (La-\nban et al., 2022a), and a QA-based method\nQAFactEval (Fabbri et al., 2022). In this work,\nwe treat the Non-LLM models as baselines and\npoints of comparison with LLM-based factuality\nevaluators.\nFoundation Models\nLarge-scale models that\nhave been pre-trained on web-scale corpora, but\nhave not been fine-tuned on task-specific or\ninstruction-following datasets. Such models have\nshown emergent abilities, such as zero- and few-\nshot in-context learning. Models in this group in-\nclude Meta\u2019s LLaMa-13b (Touvron et al., 2023),\nand OpenAI\u2019s Ada001, Babbage001, Curie001,\nand DaVinci-001.\nInstruction-tuned LLMs\nFoundation models\nwhich were further tuned on instruction-following\ndata either through supervised learning or RL-\nbased methods. Such models show enhanced capa-\nbilities of following natural language instructions,\nincluding zero- and few-shot prompts as well as\nchain-of-thought approaches. Models in this group\ninclude Databrick\u2019s Dolly, Stanford\u2019s Alpaca (Taori\net al., 2023), Anthropic\u2019s Claude V1.3, Cohere\u2019s\nCommand-XL, Google\u2019s PaLM2-bison, and Ope-\nnAI\u2019s DaVinci-002, and DaVinci-003 models.\nChat-based LLMs\nFoundation models tuned on\nconversational and instruction-following datasets.\nThe fine-tuning procedure aims to enhance the\nmodel capabilities of engaging in multi-turn di-\nalog with end-users while being able to solve a\nwide range of complex tasks. This group includes\nGoogle\u2019s Bard, Mosaic\u2019s MPT-7b-chat (Team,\n2023), Vicuna-13b (Chiang et al., 2023), and Ope-\nnAI\u2019s GPT3.5-turbo (ChatGPT), and GPT-4.\nFor each model, model cards and method of ac-\ncess are provided in Appendix A, model architec-\nture and training details are described in the associ-\nated literature.\n3.3\nExperimental Setup\nExperiments described in the following subsections\nwere conducted on the synthetic part of the FactCC\ndataset. We select a total of 150 samples to conduct\nexperiments, by including 25 examples for each of\nthe 5 error types in the dataset, i.e. date-, entity-\n, negation-, number-, and pronoun-related errors,\nand 25 factually correct samples. Considering that\nthe original data was generated using heuristics, all\nexamples were selected manually to ensure high-\nquality data.\nModel (\u2193)\nNon-LLM Models\n\u2713\nDAE\n67.2\n\u2713\nSummaC\n96.8\n\u2713\nQAFactEval\n93.6\nPrompt Group \u2192\nZS\nFS\nPers\nCoT\nGwE\nLLaMa-13B\n50.0\n51.6\n52.4\n-\n-\nAlpaca-13B\n54.8\n48.4\n57.2\n-\n-\nDolly-v2-12B\n50.4\n50.8\n50.8\n-\n-\nMPT-7B-Chat\n58.7\n54.0\n54.4\n-\n-\n\u2713\nVicuna-13B\n65.5\n68.0\n63.2\n-\n-\n\u2713\nCohere-CMD-XL\n61.3\n50.0\n53.3\n64.7\n54.8\n\u2713\nClaude-v1.3\n76.4\n83.9\n72.0\n79.7\n77.2\n\u2713\nBard\n79.3\n72.3\n73.7\n82.0\n71.9\n\u2713\nPaLM2-Bison\n82.3\n75.5\n63.7\n73.1\n71.3\nAda001\n46.4\n47.7\n49.6\n52.0\n50.0\nBab001\n51.9\n57.1\n49.5\n49.1\n53.1\nCur001\n53.5\n53.3\n51.1\n57.5\n56.3\n\u2713\nDav001\n61.2\n56.8\n52.9\n61.6\n58.1\n\u2713\nDav002\n74.5\n81.3\n57.9\n78.5\n73.2\n\u2713\nDav003\n82.3\n78.4\n62.4\n85.5\n76.8\n\u2713\nGPT3.5-turbo\n84.3\n82.9\n75.1\n84.0\n86.3\n\u2713\nGPT4\n91.3\n90.1\n66.3\n85.7\n78.0\nTable 1: Balanced accuracy on the synthetic FactCC\nbenchmark per prompt group (averaged across prompts\nin each group). Specialized non-LLMs, (top) Founda-\ntion Models, Instruction-tuned LLMs, and Chat-based\nLLMs (bottom). For LLMs, performance is evaluated\nwith Zero-shot (ZS), Few-Shot (FS), Persona (Pers),\nChain-of-Thought (CoT), and Generate-with-Evidence\n(GwE) prompts when sequence length allows.\n3.4\nInconsistency Detection\nWe first evaluate the models\u2019 ability to detect fac-\ntual inconsistencies in a binary classification set-\nting with Yes-No labels. Non-LLM models return\na continuous score attributed to a label class using\na tuned threshold, while LLM-based approaches\ngenerate free-form text where the final output is\nretrieved using regular expressions. Due to input\nlength restrictions, certain models could not be\ntested on more complex (and longer) prompts. Ta-\nble 1 presents the balanced accuracy scores aver-\naged across three prompts within each prompt cat-\negory. The results provide the following insights:\nTwo non-LLM models achieve near-perfect ac-\ncuracy and substantially outperform LLM-based\nevaluators. We speculate that this might be caused\nby non-LLM models being over-optimized to the\nidiosyncrasies of this simple error detection task\nand might not hold for more involved detection\nexamples. We investigate this question further in\nlater sections of the paper.\nRegarding the prompt design, we notice that\nfor most models (8/12), providing a few examples\nof the task (zero- \u2192 few-shot) improves the per-\nError Type\nModel (\u2193)\nPOS\nDS\nES\nNSent\nNS\nPS\nDAE\n96.0\n12.0\n44.0\n28.0\n52.0\n44.0\nSummaC\n96.0\n100.0\n100.0\n100.0\n100.0\n80.0\nQAFactEval\n96.0\n84.0\n92.0\n96.0\n96.0\n84.0\nLLaMa-13B\n88.8\n10.4\n13.6\n14.4\n12.8\n12.8\nAlpaca-13B\n80.0\n30.4\n20.0\n36.0\n25.6\n28.0\nDolly-v2-12B\n93.6\n3.2\n11.2\n10.4\n7.2\n5.6\nMPT-7B\n72.0\n36.0\n41.6\n52.8\n38.4\n40.0\nVicuna-13B\n68.8\n59.2\n63.2\n74.4\n65.6\n48.8\nCohere-CMD-XL\n85.1\n32.0\n31.5\n36.3\n26.1\n17.1\nClaude v1.3\n71.7\n82.4\n80.3\n89.1\n89.9\n78.1\nBard\n80.0\n68.8\n69.3\n77.3\n83.7\n59.2\nPalm2\n96.5\n47.7\n45.3\n65.1\n52.0\n38.9\nAda001\n58.7\n36.5\n40.3\n45.9\n39.2\n36.3\nBab001\n70.1\n33.9\n29.6\n41.6\n30.7\n34.7\nCur001\n88.0\n12.0\n17.3\n45.1\n16.5\n12.3\nDav001\n88.0\n21.9\n28.5\n50.4\n27.5\n13.1\nDav002\n80.3\n66.4\n61.3\n74.9\n71.5\n55.5\nDav003\n93.1\n66.1\n58.4\n71.2\n69.9\n39.7\nGPT3.5-turbo\n87.2\n82.4\n63.5\n87.5\n89.1\n66.7\nGPT4\n86.1\n74.9\n77.3\n81.6\n84.3\n74.1\nLLM Avg.\n81.64\n44.95\n44.25\n56.10\n48.81\n38.87\nTable 2: Accuracy on the synthetic FactCC benchmark\nper error type (averaged across all prompts). Specialized\nnon-LLMs (top) Foundation Models, Instruction-tuned\nLLMs, and Chat-based LLMs (bottom). Performance is\nassessed individually for positive examples (POS) and\neach of the error types: Date Swap (DS), Entity Swap\n(ES), Negated Sentences (NSent), Number Swap (NS),\nPronoun Swap (PS).\nformance by an average of 2.7 percentage points.\nHowever, for two models, GPT4 and PaLM2, the\nperformance in the same setting dropped substan-\ntially (-6.2 pp). Considering those the two mod-\nels achieve strong performance across all prompt\ntypes, we conclude that few-shot examples can help\nmodels but are not necessary for top-performing\nmodels.\nIn the majority of cases (8/12) Generate-with-\nEvidence prompts outperform Chain-of-Thought\nprompts corroborating prior findings of Ye and Dur-\nrett (2022) that models perform better when gen-\nerating an answer followed by the evidence, rather\nthan generating reasoning followed by an answer\nas in CoT. An in-depth evaluation of the factual\nreasoning capabilities of models is presented in the\nfollowing section.\nPersona-based prompts improve the perfor-\nmance of GPT3.5-Turbo; however, they lower the\nperformance of all other models, including the re-\nmaining chat-based LLMs. This finding suggests\nthat conditioning the generation on a specific per-\nsona might be a feature exclusive to ChatGPT, pos-\nsibly linked to the data used for tuning, rather than\na broader trait of chat-tuned LLMs.\nInput Document\nPrue Leith, who takes over from Mary Berry on the\nshow, insists \u201cbaking is the best way to get people\ninto cooking\u201d. The latest series returns this autumn\non Channel 4 after it outbid the BBC. The previ-\nous hosts announced they \u201cwere not going with the\ndough\u201d when the show switched channels. The pair\nhad presented all seven series of the show alongside\njudges Berry and Hollywood. Similarly, Berry also\nannounced she would not remain with the show on\nChannel 4, citing \"loyalty\" to the BBC. [...]\nInconsistent Summary\nBerry also announced that he would not be staying\non the show on Channel 4, citing \"loyalty\" to BBC.\n\u2022 Correct Explanation\nNo. The summary incorrectly refers to Berry as \"he\"\ninstead of \"she\". \u2013 GPT-4\n\u2022 Partially Correct Explanation\nThe summary incorrectly states that Mary Berry is a\nhe and that Berry cited loyalty to the BBC as a reason\nfor not remaining on the show. \u2013 Claude V1.3\n\u2022 No Explanation\nNo \u2013 Bab001\n\u2022 Unrelated Explanation\nFor me the answer is No! The document says \"The\nnew BBC series returns this autumn on Channel 4\nafter it outbid the BBC\". \u2013 LlaMa-13b\n\u2022 Incorrect Explanation\nThe document does not mention Mary Berry an-\nnouncing that she would not be staying on the show\non Channel 4. \u2013 Alpaca-13b\nTable 3: Examples of explanation types used to evaluate\nmodel ability at explaining inconsistencies they detect.\nWe study model performance for each error type,\naveraging the accuracy score across all prompts for\nLLM models. Results are presented in Table 2. We\nnotice that the majority of LLM-based evaluators\nachieve satisfactory accuracy (> 80%) in classi-\nfying positive (factually correct) examples. The\nresults also highlight that with a few exceptions\n(Claude, ChatGPT, and GPT4) LLMs struggle to\nconsistently detect factual inconsistencies, in many\ncases achieving scores below random chance. Pro-\nnoun swap detection seems to be the most compli-\ncated of error types averaging 38.87% across all\nLLM-based evaluators and also scoring lowest for\nspecialized, non-LLM models.\n3.5\nFactual Reasoning\nTo gain further insights into LLM ability to reason\nabout factual consistency, we performed a manual\nanalysis of more than 3,600 explanations generated\nfor sixteen of the seventeen LLMs included in our\nexperiments2.\nIn our analysis, we focus on cases the model\nclassifies as inconsistent, as there is a clear expla-\nnation a model should provide to pinpoint elements\nin the summary it identifies as inconsistent.\nFor each known inconsistent (document,\nsummary) sample in FactCC, and each model\noutput explaining why the sample is inconsistent,\nwe hired an annotator to label the explanation with\none of five labels: \u2022 entirely correct: the model\u2019s\nfull explanation must accurately describe a factual\ninaccuracy in the summary, \u2022 partially correct: the\nmodel correctly describes at least one factual in-\nconsistency in the summary, but also describes an\nincorrect element or facts unrelated to factual con-\nsistency, \u2022 no explanation: the model provides a\nclassification label (Yes/No) without providing the\nrequested explanation, \u2022 unrelated: the model\u2019s\noutput addresses aspects other than factual consis-\ntency or is not an explanation (e.g., the model writ-\ning a new summary), and \u2022 incorrect: the model\u2019s\nexplanation is invalid and does not correctly iden-\ntify an element in the summary which is factually\nincorrect. Table 3 gives an example of each expla-\nnation type from the annotation, and Appendix B\nprovides further information on the hiring and on-\nboarding of the two annotators that completed the\ntask. During annotation, the annotator samples\nwere presented in a shuffled order, and the annota-\ntor was not aware of the models that had generated\nany particular sample.\nWe analyze annotation reproducibility by col-\nlecting multiple annotations for roughly 200 anno-\ntations and computing Cohen\u2019s Kappa. We find a\nmoderate agreement amongst annotators of 0.72 on\nthe five-way annotation.\nFigure 2 summarizes the results by providing the\ndistribution of types of explanations generated by\neach LLM.\nFirst, we find that all models struggle to provide\ncorrect explanations pinpointing the inconsisten-\ncies in summaries, with nine of the sixteen models\nproviding correct explanations less than 10% of the\ntime and only three models \u2013 Bard, Claude V1.3,\n2PaLM-Bison was released shortly before the publication\nof this work and could not be included in the manual analysis.\nand GPT4 \u2013 providing correct explanations more\nthan 50% of the time.\nWe also notice that better performance at the\nbinary classification task (Table 1) does not nec-\nessarily lead to more accurate model explanations.\nFor example, GPT3.5-turbo performs 5-10% better\nin terms of binary accuracy than Claude V1.3, yet it\ngenerates almost half as many correct explanations.\nThis finding suggests accuracy metrics sometimes\noverlook models that are right for the wrong rea-\nsons (McCoy et al., 2019), which might negatively\naffect user trust.\nAnalyzing the failure behavior of models reveals\ndifferences amongst models. The first group of\nmodels \u2013 including Dav001, Dav002, and Cur001\n\u2013 fails by not providing an explanation for the\ninconsistency, even though they were explicitly\nprompted to accompany their answer with an ex-\nplanation. A second group \u2013 including Ada001,\nLLaMa-13B, and Cohere-cmd-XL \u2013 most often\ngenerates \u201cunrelated explanations\u201d, which do not\nexplain the nature of factual inconsistency, but\nmight instead quote other facts omitted from the\nsummary, propose a new summary, or other tan-\ngential text. A third group \u2013 with models such as\nMPT-7B-Chat and Dolly-v2-12B \u2013 generates plau-\nsible explanations that are factually incorrect, or\npresent invalid logic. We argue that although all\nmodels should strive to produce only correct expla-\nnations, some failure cases are preferable to others.\nFor example, it might be preferable for a model\nto provide no explanation than a plausible-looking\nbut incorrect one that might mislead a user. For\nexample, MPT-7B-Chat and Dav003 both generate\nroughly the same proportion of correct explana-\ntions (21 vs. 24%), yet when they fail, Dav003\nis much more likely to abstain from providing an\nexplanation, whereas MPT-7B-Chat is more likely\nto provide an explanation with incorrect reasoning,\nwhich could prove more harmful.\n3.6\nFine-grained Inconsistency Detection\nTo explore the LLMs\u2019 fine-grained understand-\ning of factual evaluation we designed a set of ex-\nperiments prompting the models to evaluate each\n(document, sentence) pair with respect to\nindividual error types. For each of the error types\npresent in the data models were expected to over-\nlook any other factual error, thus evaluating model\nability to selectively focus on one error type while\nignoring others. In the experiments, we filter out\n58\n4\n36\n2\nGpt4\n52\n6\n4\n31\n7\nClaude-v1.3\n46\n4\n7\n34\n10\nBard\n32\n6\n51\n10\nGPT3.5-turbo\n20\n11\n70\n8\nDav003\n19\n12\n69\nMpt-7b-chat\n16\n8\n43\n34\nVicuna-13b\n8 1 7\n74\n10\nDav002\n7\n32\n47\n14\nAlpaca-13b\n1\n14\n69\n15\nCur001\n96\n31\nAda001\n1\n53\n45\n1\nBab001\n76\n24\nCohere-cmd-xl\n2 5\n74\n18\nDav001\n17\n50\n33\nDolly-v2-12b\n35\n50\n15\nLlama-13b\nFigure 2: Percentage distribution of the types of explana-\ntions each LLM provides in its output when predicting\na FactCC summary is inconsistent. Each model expla-\nnation is manually annotated as \u2022 entirely correct, \u2022\npartially correct, \u2022 no explanation provided, \u2022 unre-\nlated to factuality, \u2022 or incorrect.\nconsistent summaries, and examples containing the\nerror type associated with the prompt were con-\nsidered \"positive\", while other error types are con-\nsidered as negative, and measured performance in\nterms of Precision and Recall. We use individ-\nual prompts for each of the error types and also\nconduct experiments where individual prompts are\ncombined with few-shot examples to aid the model\nin understanding the task. Results are presented in\nTable 4.\nThe results show a consistent pattern across all\nconsidered LLMs and error types, where the mod-\nels achieve a low precision and high recall score.\nThis indicates that the models are not able to fol-\nlow the fine-grained instructions and distinguish\ndifferent error types. Instead, they simply detect\nfactual inconsistencies (to the best of their abilities)\non a general level and assign a negative label. Pro-\nviding the models with examples of the error to be\ndetected (few-shot setting) does improve the perfor-\nmance of a subset of models; however, no general\nperformance improvement patterns emerged.\nIn short, none of the models we experiment with\ncan perform the task of fine-grained factual incon-\nsistency detection, in which they are tasked with\nfocusing on a single type of factual error.\nAdditionally, we carried out an experiment\nwhere the per-error-type prompts were combined\ninto a single instruction with multiple tasks that\nthe model was expected to complete in a sequence.\nQualitative analysis of the results showed that most\nof the models could not follow the instructions and\nconsistently provide answers in the expected for-\nmat, thus the results were not included in the final\nresults table.\n4\nLimits of Crowd-Based Benchmarks\nIn this section we analyze two popular benchmarks\nfor factual consistency detection in summarization:\nAggreFact (Tang et al., 2022) and DialSummEval\n(Gao and Wan, 2022) and uncover limitations that\nguide the design principles of the SUMMEDITS\nbenchmark we build.\n4.1\nExperimental Setup\nIn the subsequent sections of the paper, we incor-\nporate the insights gained from the experiments on\nFactCC to inform our experiment design in terms\nof model and prompt selection.\nFirst, we filter out all models that did not achieve\na balanced accuracy above 60% on FactCC, as such\nmodels are unlikely to significantly outperform\nrandom chance on more challenging benchmarks.\nCheckmarks (\u2713) in Table 1 indicate models that\nare retained in the experiments of Sections 4.1-6.\nSecond, to minimize the computational cost of\nexperiments, we select a single Zero-Shot (ZS)\nprompt that is used for all LLM models. We make\nthis choice instead of using multiple prompts per\nmodel or selecting each model\u2019s best-performing\nprompt on FactCC results for three reasons: (1)\nthere\u2019s no guarantee that prompt quality will trans-\nfer across benchmarks, and using a single com-\nmon prompt removes variance from prompt opti-\nmization that does not measure underlying model\nability, (2) top-performing LLMs such as GPT4\nachieve their best performance on FactCC with\nZS prompts, indicating that high performance with\na simple ZS prompt is achievable, and (3) more\ncomplex prompts would require adaptation to each\ndomain (e.g. domain-specific few-shot examples),\nand restrict the evaluation of models with shorter\nmaximum sequence lengths due to longer prompts.\n4.2\nAggreFact\nThe AggreFact-SOTA (Tang et al., 2022) bench-\nmark is a factual consistency benchmark focused\non the news domain, modified from the SummaC\nbenchmark (Laban et al., 2022a) focused on sum-\nmaries generated by SOTA models (i.e., models\nbased on pre-trained Transformers), as analysis\nZero-Shot\nFew-Shot\nDS\nES\nNSent\nNS\nPS\nDS\nES\nNSent\nNS\nPS\nModel (\u2193)\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nP\nR\nLLaMa-13B\n12.0\n12.0\n18.8\n12.0\n29.4\n40.0\n20.8\n20.0\n12.5\n8.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nAlpaca-13B\n20.5\n32.0\n12.9\n16.0\n36.6\n60.0\n20.9\n36.0\n23.1\n36.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nDolly-v2-12B\n26.7\n16.0\n18.8\n12.0\n14.3\n16.0\n22.2\n8.0\n33.3\n12.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nMPT-7B-Chat\n19.1\n52.0\n19.7\n60.0\n23.6\n68.0\n18.1\n52.0\n18.8\n48.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nVicuna-13B\n20.8\n100.0\n19.1\n84.0\n23.4\n100.0\n21.4\n96.0\n18.3\n80.0\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nCohere-CMD-XL\n22.1\n92.0\n19.4\n84.0\n26.9\n100.0\n22.1\n100.0\n17.3\n76.0\n17.6\n72.0\n21.2\n72.0\n30.8\n96.0\n20.2\n84.0\n21.1\n80.0\nClaude-v1.3\n20.3\n96.0\n19.7\n96.0\n19.2\n92.0\n20.8\n100.0\n20.3\n100.0\n21.9\n92.0\n22.0\n96.0\n22.9\n96.0\n26.0\n100.0\n21.0\n100.0\nPaLM2-Bison\n21.1\n64.0\n21.3\n76.0\n26.1\n92.0\n23.6\n84.0\n20.2\n80.0\n21.7\n60.0\n17.5\n56.0\n28.8\n84.0\n25.3\n80.0\n20.2\n68.0\nAda001\n19.7\n92.0\n20.0\n96.0\n19.3\n92.0\n19.3\n92.0\n18.4\n84.0\n33.3\n4.0\n0.0\n0.0\n0.0\n0.0\n22.8\n92.0\n20.0\n4.0\nBab001\n4.8\n4.0\n19.0\n16.0\n18.2\n24.0\n10.3\n12.0\n11.5\n12.0\n27.3\n24.0\n20.0\n24.0\n23.5\n32.0\n12.5\n16.0\n19.4\n28.0\nCur001\n13.5\n28.0\n22.0\n44.0\n32.8\n84.0\n17.9\n28.0\n15.4\n24.0\n16.4\n44.0\n15.9\n44.0\n26.6\n84.0\n14.3\n32.0\n16.5\n52.0\nDav001\n13.7\n28.0\n26.3\n60.0\n39.5\n68.0\n14.3\n28.0\n13.0\n12.0\n18.6\n52.0\n18.8\n36.0\n37.5\n72.0\n17.3\n36.0\n10.9\n24.0\nDav002\n20.2\n92.0\n22.4\n96.0\n23.4\n88.0\n21.9\n100.0\n20.5\n92.0\n18.3\n84.0\n19.8\n92.0\n21.1\n96.0\n20.3\n100.0\n19.7\n96.0\nDav003\n20.4\n92.0\n20.2\n96.0\n24.7\n96.0\n21.7\n100.0\n20.2\n96.0\n25.6\n92.0\n21.6\n96.0\n24.0\n92.0\n27.5\n100.0\n19.3\n92.0\nGPT3.5-turbo\n20.6\n88.0\n18.0\n80.0\n24.0\n100.0\n22.9\n100.0\n21.1\n96.0\n20.2\n92.0\n16.7\n76.0\n21.8\n96.0\n22.7\n100.0\n20.7\n96.0\nGPT4\n31.0\n88.0\n20.5\n96.0\n22.0\n96.0\n24.3\n100.0\n19.8\n96.0\n26.4\n92.0\n20.2\n96.0\n22.4\n96.0\n21.9\n100.0\n20.5\n96.0\nTable 4: Precision (P) and Recall (R) scores of error detection with fine-grained prompts for individual error types.\nExperiments run in Zero- and Few-shot settings for each of the error types: Date Swap (DS), Entity Swap (ES),\nNegated Sentences (NSent), Number Swap (NS), Pronoun Swap (PS).\nAggreFact\nDialSummEval\nModel Name\n%BAcc.\n%BAcc.\nCorr.\nDAE\n76.0\n56.2\n0.44\nSummaC\n71.6\n62.7\n0.35\nQAFactEval\n73.9\n64.4\n0.59\nCohere-cmd-XL\n63.1\n56.6\n0.36\nClaude V1.3\n50.6\n56.8\n0.30\nBard\n62.7\n59.5\n0.26\nPaLM2-Bison\n57.0\n55.6\n0.57\nDav001\n53.3\n52.9\n0.11\nDav002\n54.3\n59.2\n0.49\nVicuna-13b\n60.3\n58.6\n0.36\nDav003\n64.8\n60.9\n0.51\nGPT3.5-turbo\n70.2\n62.0\n0.56\nGPT-4\n73.6\n68.4\n0.58\nTable 5: Performance of models on the AggreFact, Di-\nalSummEval consistency benchmarks reported in bal-\nanced accuracy (%Bacc.) and correlation (corr.).\nshowed that summaries from older models were\nless relevant to the field of consistency detection.\nTable 1 reports the balanced accuracy of spe-\ncialized models and LLMs on AggreFact. At first\nglance, the specialized models still outperform\nLLMs, even though increasing LLM size leads to\nperformance improvements and helps close the gap,\nwith GPT-4 performing within 2.4% points of the\nspecialized DAE. However, all models perform rel-\natively poorly, with no model reaching a balanced\naccuracy of 80% on a binary classification task.\nTo inspect performance on the AggreFact bench-\nmark further, we conducted a manual annotation\nsimilar to the one conducted in FactCC Section 3.5\nbut focused on cases where GPT4 disagrees with\nthe label of AggreFact. More precisely, we manu-\nally inspected the explanations provided by GPT4\nfor the 101 summaries it judged were inconsistent\nbut labeled as consistent in the dataset.\nOf the 101 samples, 80 were labeled by the an-\nnotator as correct or partially correct explanations\nthat identify and explain a factual inconsistency in\nthe summary. In other words, this manual anal-\nysis of a subset of AggreFact reveals that a min-\nimum of 6% of the samples in AggreFact are\nmislabeled. The low reliability of labels in crowd-\nsourced benchmarks like AggreFact is a known\nissue (Pagnoni et al., 2021) stemming from task\ncomplexity that requires the annotator to carefully\nread and understand an entire document and ac-\ncompanying summary, leading to low repeatability\nand inter-annotator agreement.\nThis methodology reveals the potential for\nLLMs as part of dataset creation. In some cases,\nan LLM explanation that is verifiable \u2013 such as an\nexplanation for an identified factual inconsistency \u2013\ncan accelerate and improve the quality of annota-\ntion. We note however that LLM explanations are\nonly valuable for a subset of the samples. For ex-\nample, in cases where the model asserts a summary\nis consistent, manual verification is still required to\nassure quality. In Section 6, we explore a new pro-\ntocol for factual consistency benchmark creation\nwhich can involve an LLM.\nBased on the low reliability of labels in Ag-\ngreFact, we note that a key requirement for future\nbenchmarks is to improve label reliability, which\ncan be demonstrated with high annotator agreement\nwhen multiple annotators are involved.\nAverage Annotator Likert Score\nModel\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nDav001\n68.1\n78.4\n84.6\n90.2\n83.6\n84.9\n86.0\n88.9\nCohere-cmd-XL\n46.2\n51.0\n70.3\n83.6\n88.6\n89.2\n91.7\n96.3\nDAE\n30.8\n56.9\n63.7\n83.6\n86.8\n94.3\n90.3\n94.2\nPaLM2-bison\n25.3\n35.3\n56.0\n78.7\n93.6\n97.2\n98.4\n95.8\nDav002\n13.2\n29.4\n47.3\n62.3\n77.7\n83.0\n88.3\n90.0\nDav003\n4.4\n17.6\n28.6\n31.1\n63.2\n69.3\n84.9\n81.6\nGPT3.5-turbo\n8.8\n15.7\n29.7\n45.9\n73.6\n76.4\n88.5\n90.0\nGPT4\n2.2\n5.9\n6.6\n24.6\n45.9\n54.2\n80.9\n87.9\nQAFactEval\n3.3\n5.9\n17.6\n24.6\n44.5\n54.7\n70.3\n74.7\nVicuna-13b\n8.8\n15.7\n17.6\n37.7\n50.9\n54.2\n65.5\n66.8\nSummaC\n4.4\n5.9\n20.9\n21.3\n27.7\n40.1\n43.7\n58.9\nClaude V1.3\n1.1\n9.8\n11.0\n13.1\n33.6\n37.3\n47.1\n45.8\nBard\n9.9\n7.8\n5.5\n9.8\n18.2\n21.2\n36.5\n42.6\nTable 6: Percent of summaries classified as consistent\nin DialSummEval, bucketed by average Likert consis-\ntency score. Models are more uncertain in mid-range\nborderline buckets ([2.0, 4.0]).\n4.3\nDialSummEval\nThe DialSummEval (Gao and Wan, 2022) bench-\nmark is a summarization evaluation benchmark cre-\nated following the format of SummEval (Fabbri\net al., 2021) for the domain of dialogue summa-\nrization. In DialSummEval, each (dialogue,\nsummary) tuple is evaluated by three annotators,\neach assigning a Likert score (1-5) assessing the\nconsistency of the summary. The authors of the\nbenchmark report an agreement level of 0.67 Krip-\npendorff\u2019s alpha on the labels, indicating a moder-\nate amount of agreement among annotators.\nWe evaluate model performance in two ways:\n(1) direct correlation between model predictions\nand the average annotator score, and (2) we fol-\nlow Laban et al. (2022a)\u2019s procedure to transform\nthe benchmark into a binary classification task,\namenable to the balanced accuracy metric. Results\nare summarized in Table 5.\nEchoing results on AggreFact, increasing model\nsize leads to a minor improvement in performance\nboth in balanced accuracy and correlation, but most\nLLMs still underperform specialized methods. In\nabsolute terms, all methods struggle to achieve\nstrong performance on the benchmark, with ac-\ncuracies all below 70%.\nIn Figure 6, we aggregate model predictions into\n0.5-width buckets on the Likert scale. We find that\nmost models achieve strong performance on non-\nborderline buckets ([1.0, 1.5), [1.5, 2.0], [4.0, 4.5],\n[4.5, 5.0]), assigning a vast majority of samples to\nthe correct class (inconsistent for low buckets, con-\nsistent for high buckets). The borderline buckets\n([2.0, 4.0]) however are less clear-cut: most mod-\nSource\nDocument\nSeed\nSummary\nIs the seed factually \nconsistent\u00a0with the document?\nAny \ufb02aws with the summary?\n(\ufb02uency, format, etc.)\n\u00a0 No\n\u00a0 Yes\nHuman \nAnno\nLLM\nEdited Summaries\nHuman\nAnno\nSummEdits \nBenchmark\nDoes the edit introduce a factual\ninconsistency?\n\u2714 Consistent\n\u2718 Inconsistent\nFigure 3: SUMMEDITS protocol diagram, a three-step\nprotocol to create summarization ID benchmarks. See\nTable 7 for example samples produced by the protocol.\nels assign large proportions of samples from each\nbucket into consistent and inconsistent classes.\nWe argue that annotating the consistency of\nsummaries using a Likert scale limits the quality\nand interpretability of the benchmark, as it is\nnot evident to interpret the differences between\nscores, limiting reproducibility, which is reflected\nin the moderate Kripendorff\u2019s alpha. Instead, we\nfavor framing factual consistency benchmarks as\na detection task. In the detection task, identifying\nany factual inconsistency between the document\nand summary leads to an overall assessment of the\nsummary being inconsistent. If no inconsistency is\ndetected, the summary is consistent. The detection\nframing also allows for models to provide natural\nlanguage explanations when identifying a summary\nas inconsistent, which can be manually verified to\nconfirm model reasoning ability, and model failure\nmodes, as done in Section 3.5.\nIn the next section, we propose a novel protocol\nto create factual consistency benchmarks, incorpo-\nrating lessons learned from existing benchmarks.\nEdited Summary Labeled As Consistent\nEdited Summary Labeled As Inconsistent\nThe characters discuss ponder the consequences of\nbanishing Marcius, with Cominius warning that his\nalliance collaboration with the Volscians will bring\ngreat danger to Rome.\nThe characters discuss the consequences of ban-\nishing Marcius, with Cominius warning that his\nalliance with the Volscians Romans will bring\ngreat danger to Rome the Volscians. - Entity\nManipulation\nWe introduced a novel new, simple, and efficient\ndata augmentation method that boosts improves the\nperformances of existing GANs when training data\nis limited and diverse.\nWe introduced a novel, simple, and efficient data\naugmentation method that boosts the performances\nof existing GANs when training data is limited abun-\ndant and diverse. - Antonym Swap\nEmployees of the European Commission are now\nforced instructed to delete remove TikTok from their\nwork devices, and delete get rid of it from their per-\nsonal devices too if they have work-related apps ap-\nplications installed.\nEmployees of the European Commission are now\nforced not required to delete TikTok from their work\ndevices, and delete but should still remove it from\ntheir personal devices too if they have work-related\napps installed. - Hallucinated Fact\nA conversation between a sales agent and a potential\nclient possible customer. The sales agent provides\ninformation on different home insurance plans op-\ntions and pricing, as well as available discounts for\nclients with good credit scores and other factors.\nA conversation between a sales agent and a potential\nclient. The sales agent provides information on dif-\nferent home insurance plans and, but not on pricing,\nas well as or available discounts for clients with\ngood credit scores and other factors. - Negation\nInsertion\nTable 7: Example edit summaries \u2013 deletions, insertions \u2013 for four domains of SUMMEDITS (top-to-bottom:\nShakespeare Plays, SciTLDR, News, Sales Call). Inconsistent summaries are labeled with an Edit Type which\nindicates the type of factual inconsistency created with the document (not shown due to length constraint).\n5\nSUMMEDITS Protocol\n5.1\nDesign Principles\nBased on the analysis of previous benchmarks, we\nset several design principles that can help create\nhigher quality factual consistency benchmark:\nP1. Binary Classification Task: In the bench-\nmark, a summary should either be labeled\nas inconsistent if any factual inconsistency\nis identified with the document or consistent\notherwise, to improve label interpretability.\nP2. Focus on Factual Consistency: Summaries\nin the benchmark should be flawless on as-\npects unrelated to consistency, such as fluency,\ncoherence, and formatting, to avoid confound-\ning effects on the quality of the benchmark.\nP3. Reproducibility: Benchmark labels should\nnot depend on annotator identity, and high an-\nnotator agreement should confirm the validity\nof the benchmark, as well as estimate human\nperformance on the benchmark.\nP4. Benchmark Diversity: Inconsistency errors\nin the benchmark should represent a wide\nrange of errors in realistic textual domains,\nto increase understanding of model strengths\nand weaknesses, and better establish gaps in\nperformance between models and human an-\nnotators at factual reasoning, if there are any.\n5.2\nCreation Procedure\nWe now describe the creation procedure we design\nfor the SUMMEDITS benchmark with an objective\nto satisfy the design principles stated above, the\nprocedure is visually introduced in Figure 3.\nAt a high level, the procedure consists of three\nsteps: (1) seed summary verification, (2) genera-\ntion of summary edits, and (3) annotation of edited\nsummaries.\nSeed Summary Verification. Benchmark cre-\nators select a small collection of documents in a\ndomain of choice, and a seed summary is collected\nfor each document, which can either be human-\nwritten or model generated. An annotator answers\ntwo questions about each (document, seed\nsummary) tuple: (a) \u201cAre there any flaws with\nthe summary? (fluency, format, etc.)\u201d, (b) \u201cIs the\nsummary factually consistent with the document?\u201d.\nIf the annotator identifies a flaw in the summary\n(e.g., an incomplete or disfluent sentence), or any\ninconsistency, the tuple is filtered out (P2), other-\nwise, it proceeds to Step 2.\nGeneration of Edited Summaries. Once a seed\nsummary has been verified, the second step consists\nin generating multiple minor edits to the summary,\nwhich might or might not affect the consistency of\nthe summary. This procedure can be carried out\nmanually, or automatically with an LLM. Proposed\nedits should be atomic and localized, not entirely\nrewriting a novel summary. Example edits of sum-\nmaries are shown in Table 7.\nAnnotation of Edited Summaries. The annota-\ntor who completed the seed verification task (Step\n1) is tasked with reviewing each edited summary\nand assigning it with one of three labels: (a) con-\nsistent if an edit does not lead to an inconsistency\nin the summary, (b) inconsistent if the edit modi-\nfies the seed summary in a way that introduces a\nfactual inconsistency, (c) borderline for any other\ncase such as the edit making the summary unclear,\nor the edit requiring subjective interpretation.\nCrucially, we note that a single annotator should\ncomplete both Steps 1 and 3, as once they have\ninvested the time in reading the (document,\nsummary seed) tuple, the time required to\njudge the consistency of edits is greatly reduced.\nWe also recommend including a large number of\nedits (e.g., 30 edits) to maximize edit diversity (P4),\nand encouraging annotators to assign the border-\nline label if they are unsure about any aspect of an\nedit, in order to maximize reproducibility (P3).\nA benchmark can be formed by retaining edited\nsummaries that are labeled as consistent and incon-\nsistent and filtering out borderline cases.\nWe note that this procedure only requires a small\nnumber of documents and seed summaries, as each\nseed summary is derived into many edited sum-\nmaries. This flexibility facilitates the creation of\nfactual consistency benchmarks in application do-\nmains that lack such resources, such as legal (Ko-\nrnilova and Eidelman, 2019) or podcast summariza-\ntion (Clifton et al., 2020).\n6\nSUMMEDITS Benchmark\n6.1\nBenchmark Creation\nWe implemented the SUMMEDITS protocol on ten\nrealistic summarization domains to explore the reli-\nability of the protocol. For five domains, seed sum-\nmaries are automatically generated due to the lack\nor low quality of existing reference summaries. In\nsuch cases, we used ChatGPT and domain-specific\nprompts to generate seed summaries. We note that\nfor all domains, the quality of seed summaries is\nultimately manually confirmed in step 1 of the pro-\ntocol, which consists of ensuring seed summaries\nare factually consistent and flawless in terms of\nfluency, formatting, etc.\nDomain\nN\n%Balance\nIAA\nNews\n819\n39.2%\n0.91\nPodcast\n500\n32.6%\n0.91\nBillsum\n853\n42.3%\n0.90\nSamsum\n664\n36.4%\n0.90\nShakespeare\n814\n46.4%\n0.96\nSciTLDR\n466\n31.1%\n0.93\nQMSum\n431\n42.5%\n0.92\nECTSum\n668\n38.0%\n0.96\nSales Email\n613\n29.2%\n0.87\nSales Call\n520\n33.3%\n0.93\nOverall\n6,348\n37.10%\n0.92\nTable 8: Statistics of the ten domains included in\nthe SUMMEDITS benchmark, including the number of\nsamples (N), the percentage of consistent summaries\n(%Balance), and the inter-annotator agreement (IAA).\nFor all domains, we use GPT3.5-turbo as the\nLLM to produce edited summaries3. The model\nchosen to produce summary edits has an impor-\ntant impact on the benchmark. We experimented\nwith integrating multiple LLMs in the edit gen-\neration process, but preliminary results indicated\nthat many LLMs were not successful at generat-\ning minorly edited summaries and often attempted\nto write entirely novel summaries, which led us\nto use ChatGPT as the single model to generate\nedited summaries. This choice is discussed further\nin Section 7.\nWe hired two professional annotators, who were\ncompensated at a rate of $20/hour to perform steps\n1 and 3 of the protocol. Three authors of the pa-\nper also participated in the annotation for quality\ncontrol purposes. Appendix C has further detail on\nannotation protocol, and an overview of the anno-\ntation interface, which ensured that each annotator\ncompleted Task 1 and 3 sequentially for any sam-\nple in the benchmark. We next introduce the ten\ndomains included in the SUMMEDITS benchmark.\nNews\nTo avoid selecting documents and sum-\nmaries that are in the training corpora of evalu-\nated models, we follow prior work (Goyal et al.,\n2022) and select (document, summary) tu-\nples from recent news articles. We obtained news\narticles from the Google News top events feed in\nFebruary 2023, selecting at most one sample per\nnews source to increase coverage diversity (Laban\n3The prompts we use are listed in our open-source release.\nModel\nPodcast\nBillSum\nSAMSum\nNews\nSales C\nSales E\nShkspr\nSciTLDR\nQMSum\nECTSum\nOverall (\u2193)\nDAE\n54.9\n55.1\n59.5\n61.7\n50.8\n55.0\n54.5\n55.2\n52.0\n58.6\n55.7\nSummaC\n58.5\n55.7\n54.7\n62.1\n59.0\n57.7\n59.3\n59.7\n56.6\n64.4\n58.8\nQAFactEval\n64.0\n54.4\n66.3\n74.6\n68.5\n64.2\n61.9\n67.5\n62.4\n72.9\n65.7\nDav001\n53.3\n50.2\n51.0\n54.4\n55.3\n52.5\n50.0\n51.0\n50.3\n50.9\n51.9\nCohere-cmd-XL\n51.1\n52.7\n52.0\n52.6\n60.3\n59.5\n50.0\n60.5\n53.9\n60.5\n55.1\nVicuna-13b\n52.8\n52.6\n50.8\n63.0\n58.1\n51.8\n55.5\n59.7\n54.0\n62.5\n56.1\nClaude v1.3\n59.9\n52.1\n64.1\n63.3\n61.7\n56.6\n58.0\n57.6\n56.9\n67.8\n59.8\nDav002\n56.4\n53.9\n57.1\n61.9\n65.1\n59.1\n56.6\n64.6\n60.6\n66.2\n60.1\nBard\n50.0\n58.3\n61.3\n72.8\n73.8\n69.0\n58.4\n66.1\n53.9\n73.1\n63.7\nPaLM2-bison\n66.0\n62.0\n69.0\n68.4\n74.5\n68.1\n61.6\n78.1\n70.2\n72.3\n69.0\nDav003\n65.7\n59.9\n67.5\n71.2\n78.8\n69.4\n69.6\n74.4\n72.2\n77.9\n70.7\nGPT3.5-turbo\n68.4\n63.6\n69.1\n74.5\n79.7\n65.5\n68.1\n75.6\n69.2\n78.9\n71.3\nGPT4\n83.3\n71.1\n82.9\n83.3\n87.6\n80.1\n84.6\n82.4\n80.4\n88.0\n82.4\nGPT4 Oracle\n90.2\n85.5\n86.3\n88.3\n91.1\n83.5\n96.6\n86.3\n89.9\n91.7\n88.9\nHuman Perf.\n90.8\n87.5\n89.4\n90.0\n91.8\n87.4\n96.9\n89.3\n90.7\n95.4\n90.9\nTable 9: Balanced accuracy of models on the SUMMEDITS benchmark. The top three models are non-LLM\nspecialized models, the middle section are LLMs. We also report a GPT4 oracle performance and an estimate of\nhuman performance.\net al., 2023). Seed summaries are extracted from\nthe article\u2019s metadata.\nPodcast (Clifton et al., 2020)\nWe collected 40\npodcast transcripts from the unreleased test set of\nSpotify\u2019s podcast summarization dataset. Due to\nlow reference summary quality, we generated seed\nsummaries automatically.\nBillSum (Kornilova and Eidelman, 2019)\nWe\ncollected 40 US bills and their accompanying sum-\nmaries as seeds from the training portion of Bill-\nSum, a challenging dataset for summarization in\nthe legal domain.\nSamSum (Gliwa et al., 2019)\nWe collected 40\ndialogues and their accompanying summaries from\nthe training portion of SamSum, a common dia-\nlogue summarization dataset for messenger-like\nconversations.\nShakespeare (Karpathy, 2015)\nWe collected\n40 scenes from Shakespeare plays from the Tiny\nShakespeare corpus, each roughly 700 words long.\nWe generated seed summaries automatically.\nSciTLDR (Cachola et al., 2020)\nWe collected\n40 research paper abstracts and their corresponding\nTLDRs from the training portion of SciTLDR, a\ndataset for scientific paper summarization.\nQMSum (Zhong et al., 2021)\nWe collected 40\ndocument and seed summaries from QMSum, a\ndataset for query-based meeting summarization.\nECTSum (Mukherjee et al., 2022)\nWe collected\n40 documents from the ECTSum dataset, a sum-\nmarization dataset for the financial earnings call\ntranscripts. Due to low reference summary quality,\nwe generated seed summaries automatically.\nSales Call & Email\nWe generated fictional sales\ncall transcripts and sales emails \u2013 40 for each \u2013\nand corresponding seed summaries using ChatGPT.\nThis subset of the benchmark evaluates the pro-\ntocol\u2019s validity with textual data entirely machine-\ngenerated in targeted domains that lack pre-existing\nsummarization datasets.\n6.2\nSUMMEDITS Statistics\nTable 8 provides statistics of the finalized\nSUMMEDITS benchmark. Each domain yielded\nbetween 400-900 edited summaries, depending on\nthe fraction of seed summaries that pass the first\nstep validation (58% overall pass rate) and the per-\ncentage of edited summaries that are annotated as\nborderline and filtered out (around 6%). In the\nfive domains where seed summaries were gener-\nated by ChatGPT, 17.8% of the seed summaries\nwere labeled as factually inconsistent, indicating\nthat modern LLMs like ChatGPT still struggle to\nremain consistent when summarizing documents.\nAt least 20% of each domain\u2019s samples were\nannotated by multiple annotators, allowing us to\nmeasure the agreement level when completing the\nannotation. When considering all three labels (con-\nsistent, inconsistent, borderline), Cohen\u2019s Kappa\nin each domain varies between 0.72-0.90, averag-\ning 0.82. When removing samples annotated as\nborderline by any annotator, the average Cohen\u2019s\nKappa rises to 0.92, empirically validating the im-\nportance of labeling and filtering out borderline\nsamples to create a reproducible benchmark.\nIn the final benchmark, 37% of summaries are\nconsistent, and the rest are inconsistent, approach-\ning our objective of a balanced benchmark to facil-\nitate robust evaluation and minimize metric fluctu-\nations (Luque et al., 2019).\nThe total annotation cost of SUMMEDITS is\naround USD 3,000, representing around 150 hours\nof annotator work. The average cost of adding a\ndomain to SUMMEDITS is therefore around USD\n300, within reach for NLP practitioners looking\nto evaluate the model\u2019s ability to detect factual\nerrors in their domain of choice. Authors of the\nFRANK benchmark (Pagnoni et al., 2021) \u2013 sam-\nples of which are in AggreFact \u2013 estimate that each\nsample in their benchmark required 30 minutes of\nannotator time. At similar annotator pay, anno-\ntating a benchmark for a new domain similar to\nthe ones in SummEdits would cost an estimated\nUSD 6,000: twenty times more. This cost analy-\nsis reveals the dual advantage of our protocol: by\nfocusing the annotation task on atomic edits, costs\ncan be drastically reduced while maintaining high\nreproducibility.\n6.3\nSUMMEDITS Results\nTable 9 reports the average performance of special-\nized models, LLMs with a zero-shot prompt, an\noracle version for the LLM in which it has access\nto additional information and an estimate of hu-\nman performance computed on the subset of the\nbenchmark which was plurally annotated.\nOverall, model performance on the benchmark\nis low, with a single model \u2013 GPT4 \u2013 getting within\n10% of human performance. Larger or more recent\nLLMs perform better on the benchmark, illustrated\nby the performance of models in the OpenAI fam-\nily, with each model generation leading to an im-\nprovement in performance and confirming that the\nSUMMEDITS benchmark assesses model ability at\nfactual reasoning.\nPaLM2-Bison, Dav003, ChatGPT, and GPT4 are\nthe only four LLMs that outperform the best non-\nLLM approach QAFactEval, providing evidence\nthat most LLMs are not yet capable to reason\nout-of-the-box about the consistency of facts.\nAll three specialized models achieve their high-\nest performance in the news domain, unlike LLM\nmodels. The specialized models are likely cali-\nbrated to the news domain, which they are most\nfrequently tested on (Goyal and Durrett, 2020; La-\nban et al., 2022a; Tang et al., 2022; Fabbri et al.,\n2022; ?). This finding confirms the importance\nof creating multi-domain benchmarks to measure\nmodel ability in diverse and realistic scenarios.\nSome domains such as Shakespeare\u2019s plays or\nthe legal BillSum are more challenging to the ma-\njority of models, with the latter seeing no model\nscore higher than 71.1%. Yet, factual reasoning in\nthe legal domain is an important application area\nof NLP (Chalkidis et al., 2020; Shen et al., 2022).\nTo assess the feasibility of the benchmark, we\nexperiment with an oracle setting of the benchmark,\nin which the model is provided the seed summary\nin addition to the input document and the seed\nsummary. The seed summary serves as an informa-\ntion scaffold, enabling the model to directly assess\nthe modifications between the seed and edited sum-\nmaries when assessing factual consistency. The ora-\ncle setting leads to a large boost in performance for\nthe GPT4 model across domains, with the model\nperforming within 2% of human performance. The\nGPT4 oracle experiment confirms that high model\nperformance on the benchmark is attainable and\nthat the challenge of SUMMEDITS lies in aligning\nthe facts of the edited summary with the document,\nwithout knowing that it has been edited.\n6.4\nEdit Type Analysis\nTo gain more specific insights into the types of\nedits present in SUMMEDITS, we annotated each\ninconsistent sample in the benchmark with tags of\nedit types that lead to factual inconsistency.\nThe\nfour\ntypes\nare:\n(1)\nEntity\nModification in which an entity or phrase\nin the summary has been changed in a way\nthat alters the meaning, (2) Antonym Swap is\nwhen a word or phrase is replaced by a word of\nopposite meaning (e.g., increasing vs. decreas-\ning), (3) hallucinated fact insertion\nis a novel fact is introduced in the summary\nwhich is not supported by the document, and\n(4) negation insertion is the use of any\nnegator word (e.g., not, neither) which modifies\nsummary meaning. Figure 7 provides an example\nof each edit type in SUMMEDITS.\nTo annotate the entire benchmark, one author\nof the paper first manually annotated 200 samples\nInconsistent Edit Type\nModel\nEntMod\nAnto\nHallu\nNeg\nDAE\n52.0\n53.0\n52.9\n53.9\nSummaC\n56.8\n56.8\n55.3\n57.3\nQAFactEval\n61.4\n65.0\n64.3\n70.4\nDav001\n50.0\n50.9\n50.8\n53.7\nCohere-cmd-XL\n53.7\n55.8\n55.5\n63.8\nVicuna-13b\n55.2\n57.1\n56.2\n61.0\nClaude v1.3\n58.8\n60.3\n61.5\n66.7\nDav002\n58.3\n61.4\n62.4\n72.0\nBard\n63.2\n65.3\n65.6\n71.3\nPaLM2-Bison\n67.0\n70.0\n71.7\n80.3\nDav003\n69.2\n71.1\n76.3\n83.3\nGPT3.5-turbo\n70.7\n70.6\n74.2\n79.7\nGPT4\n82.2\n81.3\n87.0\n92.7\nAverage\n61.4\n62.9\n64.1\n69.7\nTable 10:\nBalanced accuracy of models on the\nSUMMEDITS benchmark, broken down by type of fac-\ntual error: Entity Modification (EntMod), Antonyms\n(Anto), Hallucination (Hallu) and Negation (Neg)\ninsertion.\nof the dataset, which was used to evaluate several\nGPT4-based Zero-Shot and Few-Shot approaches.\nThe best approach was then used to annotate each\nedited summary with edit types.\nThe best-performing prompt provides the defi-\nnition of each edit type and a canonical example\nof each, and it achieved a performance of 0.85 F-1\nand 0.92 recall, which was deemed sufficient for\nanalysis purposes.4\nOverall in SUMMEDITS, 78% of inconsistent\nsummaries contain an entity modification, 48% an\nantonym swap, 22% a hallucinated fact insertion,\nand 18% a negator insertion. We note that the\ndistribution of edit types is highly influenced by the\nLLM used to produce the edits, which is ChatGPT\nin our case. Table 10 presents model performance\nacross each of the edit types.\nAll models detect inconsistencies due to nega-\ntor insertions the best, a sign that such errors are\nmore discernable to models. Fact hallucinations\nare relatively harder to detect for non-LLM models\nbut gradually become more evident to more perfor-\nmant LLMs. Finally, the entity modification and\nantonym error types generally see the lowest rate\nof detection by models across the board, perhaps\n4We provide the prompt with the code release.\n#Distinct Edit Types\nModel\n1\n2\n3\n4\nDAE\n50.2\n53.5\n55.4\n64.9\nSummaC\n58.2\n56.3\n57.6\n67.3\nQAFactEval\n59.4\n63.7\n72.3\n76.5\nDav001\n50.0\n50.5\n53.9\n63.1\nVicuna-13b\n52.8\n57.0\n60.2\n58.5\nCohere-cmd-XL\n50.0\n55.9\n63.7\n70.0\nClaude v1.3\n57.5\n60.6\n65.4\n64.3\nDav002\n56.3\n61.2\n69.4\n81.7\nBard\n61.0\n64.9\n72.4\n73.4\nPaLM2-Bison\n66.1\n69.5\n79.6\n69.4\nChatGPT\n68.5\n71.4\n82.0\n86.6\nDav003\n65.3\n72.0\n85.8\n88.8\nGPT4\n81.0\n83.0\n92.0\n94.3\nAverage\n59.2\n62.5\n69.2\n74.1\nTable 11: Relationship between the number of edits\ntypes in the summary and balanced accuracy of models\non SUMMEDITS. Models generally perform better as\nthe number of introduced edits in a summary increases.\ndue to such edits modifying an existing consistent\nfact in a more nuanced way.\n6.5\nNumber of Edits Effect\nIt is common for the LLM to introduce multiple\nedits in each of its candidate summaries, as can be\nseen in the examples in Table 7, in which each\nedited summary contains multiple inserted and\ndeleted words. We group inconsistent summaries\nby the number of distinct edit types they contain\n(1 to 4) and compute model performance on each\ngroup, with results summarized in Table 11.\nAs the number of edit types in a summary in-\ncreases, most models see sizable performance im-\nprovements, with average performance increasing\nfrom 59.2 to 74.1 between summaries with 1 or 4\nedit types represented.\nThis analysis confirms the perspective the task\nin the SUMMEDITS benchmark corresponds to a\ndetection task: as the number of introduced errors\nincreases, model performance increases as there is\ngenerally more evidence of inconsistencies for the\nmodels to detect. This also points in the direction of\na more challenging explanation analysis, in which\none could annotate whether a model can detect all\ninconsistencies in a summary.\nIn turn, future work looking to create more\nchallenging versions of benchmarks using the\nSummEdits protocol can focus on editing sum-\nmaries with a single edit type, as such inconsistent\nsummaries are more challenging to detect.\n7\nLimitations and Discussion\nWhy not fix existing benchmarks?\nIn Section 5,\nanalysis reveals limitations with existing bench-\nmarks that in theory can be fixed to yield improved\nversions of known benchmarks. The analysis we\nperformed however only helps us invalidate a sub-\nset of samples in an opportunistic way, by looking\nat samples where benchmark labels and GPT4 dis-\nagree. However, this methodology cannot help\nus efficiently correct or confirm all samples, and\nimproving existing benchmarks would require re-\nannotating a large portion of the benchmarks, and\nwe do not have a guarantee that new annotations\nwould improve on previous ones. By designing\na new protocol for sample annotation that relies\non clear, atomic edits, we simplify the annotation\nprocess, improving reproducibility.\nEffect of LLM in benchmark creation.\nStep 2\nof the protocol described in Section 5 relies on an\nLLM to generate many edits of the seed summary,\nwhich are subsequently manually annotated and in-\ncluded in the benchmark. The choice of LLM likely\nhas an effect on the benchmark which could favor\na subset of LLMs most similar to the one used for\nbenchmark creation. Initial attempts to use a pool\nof LLMs to produce edits were unsuccessful as we\nfound that only ChatGPT and GPT4 were currently\ncapable of following editing instructions that do not\nfully rewrite summaries. Future iterations on simi-\nlar benchmarks should consider including diverse\npools of LLMs in benchmark creation processes to\navoid model-specific bias.\nEvalutating\nSummarizers.\nPrevious\nbench-\nmarks were in part collected to evaluate which sum-\nmarization models are least likely to generate fac-\ntual inconsistencies (Falke et al., 2019). Since the\nsummaries in SUMMEDITS are synthetic modifica-\ntions of summaries, the benchmark cannot directly\nprovide insights on summarizers and their ability to\nremain consistent. Future work can explore using\nmethods such as Near-Negative Distinction (NND)\n(Laban et al., 2022b) to adapt SUMMEDITS into\na set of tests to evaluate summarizer performance,\nand model ability to avoid generating inconsistent\nsamples in the first place.\nBuild Your Own Benchmark.\nBy implementing\nthe protocol in ten diverse domains for an average\ncost of around USD300 per domain, we\u2019ve demon-\nstrated that the protocol can be adapted to widely\ndifferent textual domains \u2013 from US legal bills to\nShakespeare plays \u2013 and produce domain-specific\nbenchmarks. Although we hope that the domains\nwe\u2019ve selected span a range of practical use cases,\nwe hope that others will adopt and adapt the proto-\ncol to new domains, languages, and NLP tasks.\n8\nConclusion\nIn this work, we explore the capabilities of LLMs to\nact as factual reasoners through the lens of factual\nevaluation in text summarization. We show that\non a surface level, LLMs perform on par with spe-\ncialized non-LLM evaluators, but the performance\nsubstantially degrades in more advanced evaluation\nsettings. As part of this analysis, we also uncover\nand discuss shortcomings of existing benchmarks\nfor factual evaluation. Using those insights we\ndevelop a new protocol for creating inconsistency\ndetection benchmarks, which we implement in a\n10-domain benchmark called SUMMEDITS. The\nSUMMEDITS benchmark is highly reproducible\nand more cost-effective per sample than previous\nbenchmarks. Our experiments show that the bench-\nmark is challenging for most current LLMs, with\nthe best-performing model, GPT-4, still 8% below\nestimated human performance. We believe that\nSUMMEDITS can serve as a valuable tool for eval-\nuating LLMs\u2019 abilities to reason about facts, detect\nfactual errors and promote more reliable NLG sys-\ntems. We encourage LLM developers to report\ntheir performance on the benchmark, and practi-\ntioners to adapt the protocol to generate in-domain\nbenchmarks for model evaluation.\nReferences\nNegar Arabzadeh, Ali Ahmadvand, Julia Kiseleva, Yang\nLiu, Ahmed Hassan Awadallah, Ming Zhong, and Mi-\nlad Shokouhi. 2022. Preme: Preference-based meet-\ning exploration through an interactive questionnaire.\narXiv preprint arXiv:2205.02370.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022.\nConstitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nIsabel Cachola, Kyle Lo, Arman Cohan, and Daniel S\nWeld. 2020. Tldr: Extreme summarization of sci-\nentific documents. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4766\u20134777.\nShuyang Cao and Lu Wang. 2021. Cliff: Contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6633\u20136649.\nIlias Chalkidis, Manos Fergadiotis, Prodromos Malaka-\nsiotis, Nikolaos Aletras, and Ion Androutsopoulos.\n2020. Legal-bert: The muppets straight out of law\nschool. ArXiv, abs/2010.02559.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E Gonzalez, et al.\n2023. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality.\nAnn Clifton, Aasish Pappu, Sravana Reddy, Yongze\nYu, Jussi Karlgren, Ben Carterette, and Rosie Jones.\n2020. The spotify podcast dataset. arXiv preprint\narXiv:2004.04270.\nAlexander R Fabbri, Wojciech Kry\u00b4sci\u00b4nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391\u2013409.\nAlexander Richard Fabbri, Chien-Sheng Wu, Wenhao\nLiu, and Caiming Xiong. 2022.\nQafacteval: Im-\nproved qa-based factual consistency evaluation for\nsummarization. In Proceedings of the 2022 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 2587\u20132601.\nTobias Falke, Leonardo FR Ribeiro, Prasetya Ajie\nUtama, Ido Dagan, and Iryna Gurevych. 2019. Rank-\ning generated summaries by correctness: An interest-\ning but challenging application for natural language\ninference. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2214\u20132220.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nMingqi Gao and Xiaojun Wan. 2022. Dialsummeval:\nRevisiting summarization evaluation for dialogues.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5693\u20135709.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. EMNLP-IJCNLP 2019, page 70.\nTanya Goyal and Greg Durrett. 2020. Evaluating factu-\nality in generation with dependency-level entailment.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint arXiv:2209.12356.\nDandan Huang, Leyang Cui, Sen Yang, Guangsheng\nBao, Kun Wang, Jun Xie, and Yue Zhang. 2020.\nWhat have we achieved on text summarization? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 446\u2013469.\nRaghav Jain, Anubhav Jangra, Sriparna Saha, and Adam\nJatowt. 2022. A survey on medical document sum-\nmarization. ArXiv, abs/2212.01669.\nAndrej Karpathy. 2015.\nchar-rnn.\nhttps://\ngithub.com/karpathy/char-rnn.\nAnastassia Kornilova and Vladimir Eidelman. 2019.\nBillsum: A corpus for automatic summarization of\nus legislation. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization, pages 48\u201356.\nWojciech Kry\u00b4sci\u00b4nski, Nitish Shirish Keskar, Bryan Mc-\nCann, Caiming Xiong, and Richard Socher. 2019.\nNeural text summarization: A critical evaluation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 540\u2013551.\nWojciech Kry\u00b4sci\u00b4nski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332\u20139346.\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and\nMarti A Hearst. 2022a. Summac: Re-visiting nli-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163\u2013177.\nPhilippe Laban, Chien-Sheng Wu, Wenhao Liu, and\nCaiming Xiong. 2022b. Near-negative distinction:\nGiving a second life to human evaluation datasets. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nPhilippe\nLaban,\nChien-Sheng\nWu,\nLidiya\nMu-\nrakhovs\u2019 Ka, Xiang \u2019Anthony\u2019 Chen, and Caiming\nXiong. 2023. Designing and evaluating interfaces\nthat highlight news coverage diversity using discord\nquestions. In Proceedings of the 2023 CHI Confer-\nence on Human Factors in Computing Systems, pages\n1\u201321.\nAndrew K. Lampinen, Ishita Dasgupta, Stephanie C. Y.\nChan, Kory Matthewson, Michael Henry Tessler, An-\ntonia Creswell, James L. McClelland, Jane X. Wang,\nand Felix Hill. 2022. Can language models learn\nfrom explanations in context?\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nAmalia Luque, Alejandro Carrasco, Alejandro Mart\u00edn,\nand Ana de Las Heras. 2019. The impact of class im-\nbalance in classification performance metrics based\non the binary confusion matrix. Pattern Recognition,\n91:216\u2013231.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906\u20131919.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 3428\u20133448.\nRajdeep Mukherjee, Abhinav Bohra, Akash Banerjee,\nSoumya Sharma, Manjunath Hegde, Afreen Shaikh,\nShivani Shrivastava, Koustuv Dasgupta, Niloy Gan-\nguly, Saptarshi Ghosh, et al. 2022. Ectsum: A new\nbenchmark dataset for bullet point summarization\nof long earnings call transcripts.\narXiv preprint\narXiv:2210.12467.\nSharan Narang and Aakanksha Chowdhery. 2022. Path-\nways language model (palm): Scaling to 540 billion\nparameters for breakthrough performance.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization.\nOpenAI. 2023.\nGpt-4 technical report.\nArXiv,\nabs/2303.08774.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with frank: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 4812\u20134829.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. Questeval: Summariza-\ntion asks for fact-based evaluation. In Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 6594\u20136604.\nZejiang Shen, Kyle Lo, Lauren Yu, Nathan Dahlberg,\nMargo Schlanger, and Doug Downey. 2022. Multi-\nlexsum: Real-world summaries of civil rights law-\nsuits at multiple granularities.\narXiv preprint\narXiv:2206.10883.\nLiyan Tang, Tanya Goyal, Alexander R Fabbri, Philippe\nLaban, Jiacheng Xu, Semih Yahvuz, Wojciech Kry\u00b4s-\nci\u00b4nski, Justin F Rousseau, and Greg Durrett. 2022.\nUnderstanding factual errors in summarization: Er-\nrors, summarizers, datasets, error detectors. arXiv\npreprint arXiv:2205.12854.\nLiyan Tang, Zhaoyi Sun, Betina Idnay, Jordan G Nestor,\nAli Soroush, Pierre A Elias, Ziyang Xu, Ying Ding,\nGreg Durrett, Justin Rousseau, et al. 2023. Eval-\nuating large language models on medical evidence\nsummarization. medRxiv, pages 2023\u201304.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023.\nAlpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, ly usable llms. Ac-\ncessed: 2023-05-16.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008\u20135020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nJules White, Quchen Fu, Sam Hays, Michael Sandborn,\nCarlos Olea, Henry Gilbert, Ashraf Elnashar, Jesse\nSpencer-Smith, and Douglas C Schmidt. 2023. A\nprompt pattern catalog to enhance prompt engineer-\ning with chatgpt. arXiv preprint arXiv:2302.11382.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot in-context learning. arXiv\npreprint arXiv:2205.03401.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan, Asli Celikyil-\nmaz, Yang Liu, Xipeng Qiu, et al. 2021. Qmsum: A\nnew benchmark for query-based multi-domain meet-\ning summarization. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 5905\u20135921.\nA\nModel Access Detail\nIn Section 3, we experiment with a wide range of\nmodels. For each model, we specify its model card,\nand how it was accessed.\nNon-LLM models.\nThe three specialized models\n\u2013 SummaC5, DAE6, and QAFactEval7 \u2013 were im-\nplemented through their online public repositories,\nand run locally on a multi-GPU machine (with 2\nV-100 GPUs).\nOpen-source\nModels.\nWe\nexperimented\nwith\nfive\nopen-source\nLLM\nmodels:\nLLama-13b (Touvron et al., 2023),\nAlpaca-\n13b\n(Taori\net\nal.,\n2023),\nDolly-V2-12b\n(databricks/dolly-v2-12b),\nVicuna-\n13b (Chiang et al., 2023), and MosaicML\u2019s\nMPT-7b-chat (Team, 2023).\nAll models were\naccessed through the public, online demonstration\nof LMSys.org8. Model responses were collected\nbetween April 15th, 2023, and May 15th, 2023.\nGoogle Models.\nWe experiment with two Google\nmodels, the Bard (Thoppilan et al., 2022) which\nwe accessed through a web-based interface9 which\ndoes not specify an exact model card, but model\nresponses were collected between April 15th, 2023\nand May 15th, 2023. Second, the PaLM-v2-bison\nmodel (Narang and Chowdhery, 2022) (model\ncard text-bison@001), which was accessed\nthrough the Google Cloud VertexAI API.\nAnthropic Model.\nWe collected outputs of the\nClaude V1.3 model (model card: claude-v1.3),\nthe latest and largest Anthropic model at the time\n5https://github.com/tingofurro/summac\n6https://github.com/tagoyal/\nfactuality-datasets\n7https://github.com/salesforce/\nQAFactEval\n8https://chat.lmsys.org/\n9https://bard.google.com/\nof publication, using the official API hosted by\nAnthropic10.\nCohere Model.\nWe collected outputs of Cohere\u2019s\ncommand-xlarge model, the latest and largest\nCohere model at the time of publication, using the\nofficial API hosted by Cohere11.\nOpenAI Models.\nWe collected outputs for\neight OpenAI models. Six models are from the\nGPT-3\nfamily:\nAda001\n(text-ada-001),\nBab001\n(text-babbage-001),\nCur001\n(text-curie-001),\nDav001\n(text-davinci-001),\nDav002\n(text-davinci-002),\nand\nDav003\n(text-davinci-003).\nWe also include\nGT3.5-turbo (gpt-3.5-turbo) and GPT-4\n(gpt-4).\nAll models were accessed through\nOpenAI\u2019s official API12.\nB\nExplanation Annotation Guidelines\nWe hired two professional annotators to complete\nthe annotation of model-generated explanations on\nthe FactCC and AggreFact domains. The annota-\ntors were compensated at $20/hour. They received\nonboarding documentation that introduced them to\nthe task, and provided the following definition for\neach type of explanation:\n\u2022 No Explanation: If the model did not provide\nany explanation. (For example just saying:\n\u201cThe summary is inconsistent\u201d),\n\u2022 Entirely Correct: if the explanation correctly\nidentifies and explains one or more factual\ninconsistencies in the summary,\n\u2022 Partially Correct: if the explanation pro-\nvided contains several elements and at least\none of them correctly identifies and explains\na factual inconsistency in the summary,\n\u2022 Unrelated: if the explanation given does not\ndirectly relate to a factual inconsistency be-\ntween the summary and the document,\n\u2022 Incorrect: if the explanation given does not\ncorrectly identify a factual inconsistency in\nthe summary, for example, making a logical\nerror.\n10https://github.com/anthropics/\nanthropic-sdk-python\n11https://docs.cohere.com/docs/\nthe-cohere-platform\n12https://github.com/openai/\nopeai-python\nAn example for each type of explanation was pro-\nvided during onboarding, similar to the ones given\nin Table 3. In order to obtain impartial results that\ndo not benefit or disadvantage any model, for cases\nwhere multiple explanations were annotated for the\nsame (document, summary) sample, the ex-\nplanations\u2019 order was shuffled, and annotators were\nnot aware of the model origin of any explanation.\nAnnotation was performed in batches, and the\nfirst two batches of annotation of each annotator\nwere reviewed by the authors of the paper. Incor-\nrect annotations were discussed, allowing annota-\ntors to better understand edge cases of the task, and\nmodify their annotation in the first batches. The\nannotators were added to a Slack channel with one\nof the authors and regularly discussed edge cases\nto maintain a common understanding of the task.\nFor example, both annotators raised the question of\nhow to deal with cut-off explanations, in which the\nlast sentence is incomplete (due to the max-length\nof generation). Annotators were both instructed to\ndisregard any incomplete sentence and only con-\nsider full sentences in their assessment.\nC\nSUMMEDITS Annotation Guidelines\nWe hired two professional annotators to complete\nthe annotation of Steps 1 and 3 of the SUMMEDITS\nprotocol (see Section 5). The annotators were com-\npensated at $20/hour. They received onboarding\ndocumentation that introduced them to the task and\nused the interface shown in Figure 4.\nAnnotators were first assigned 10 warm-up seed\nsummaries, each with roughly 30 edited summaries,\nwhich had been pre-annotated by the authors of the\npaper. The authors reviewed the completed warm-\nup exercises, and a strong agreement level on the\nwarm-up task with both annotators was observed.\nWe discussed disagreement cases with the annota-\ntors and added both annotators to a Slack channel\nwith one of the authors of the paper to allow them\nto discuss any edge case or domain-specific ques-\ntion. For example, since the QMSumm domain\nis the more specific query-focused summarization,\nthe annotators were given updated instructions on\nSlack on how to deal with the \u201cquery\u201d component\nwhen evaluating summaries. Namely, during Step\n1 of the protocol, participants were asked to ad-\nditionally judge whether the summary accurately\nresponded to the query, and otherwise mark sum-\nmaries as inadequate.\nDocument:\nSimulation is a useful tool in\nsituations where training data for\nmachine learning models is costly to\nannotate or even hard to acquire. In\nthis work, we propose a\nreinforcement learning-based\nmethod for automatically adjusting\nthe parameters of any (non-\ndifferentiable) simulator, thereby\ncontrolling the distribution of\nsynthesized data in order to\nmaximize the accuracy of a model\ntrained on that data. In contrast to\nprior art that hand-crafts these\nsimulation parameters or adjusts\nonly parts of the available\nparameters, our approach fully\ncontrols the simulator with the\nactual underlying goal of\nmaximizing accuracy, rather than\nmimicking the real data distribution\nor randomly generating a large\nvolume of data. We \ufb01nd that our\napproach (i) quickly converges to\nthe optimal simulation parameters in\ncontrolled experiments and (ii) can\nindeed discover good sets of\nparameters for an image rendering\nsimulator in actual computer vision\napplications.\nOriginal Summary:\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training data for a neural\nnetwork such that validation accuracy is maximized.\nTask 1:\nIs any of the information in the summary not present in the\ndocument?\n Yes \n No\nAre there any other issues with the summary? (incomplete\nsentence, formatting, etc.)\n Yes \n No\nSubmit\nTask 2:\nModi\ufb01ed Summaries:\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training data for a neural\nnetwork such that validation accuracy is maximized only\nslightly improved .\n Inconsistent \n Consistent \n Borderline\nWe propose an algorithm that automatically adjusts parameters\nof a simulation engine to generate training testing data for a\nneural network such that validation accuracy is maximized.\n Inconsistent \n Consistent \n Borderline\nWe propose an algorithm that automatically adjusts changes\nparameters of a simulation engine to generate training data for\na neural network in such a way that validation accuracy is\nmaximized.\n Inconsistent \n Consistent \n Borderline\nFigure 4: Two-column annotation interface used to annotate samples in the SUMMEDITS benchmark. Participants\ncould read the document on the left-hand column. Once they completed Task 1 in the right-hand column, the second\nannotation task became visible.\n"
  }
]