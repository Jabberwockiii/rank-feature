[
  {
    "title": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit",
    "link": "https://arxiv.org/pdf/2312.09911.pdf",
    "upvote": "50",
    "text": "Amphion: An Open-Source Audio, Music and Speech Generation Toolkit\nXueyao Zhang*,1, Liumeng Xue*,1, Yicheng Gu*,1, Yuancheng Wang*,1, Haorui He3,\nChaoren Wang1, Xi Chen1, Zihao Fang1, Haopeng Chen1, Junan Zhang2, Tze Ying Tang1,\nLexiao Zou3, Mingxuan Wang1, Jun Han1, Kai Chen2, Haizhou Li1, Zhizheng Wu\u2020,1,2,3\n1School of Data Science, The Chinese University of Hong Kong, Shenzhen\n2Shanghai AI Lab\n3Shenzhen Research Institute of Big Data\nAbstract\nAmphion is an open-source toolkit for Audio,\nMusic, and Speech Generation, targeting to\nease the way for junior researchers and engi-\nneers into these fields. It presents a unified\nframework that is inclusive of diverse genera-\ntion tasks and models, with the added bonus of\nbeing easily extendable for new incorporation.\nThe toolkit is designed with beginner-friendly\nworkflows and pre-trained models, allowing\nboth beginners and seasoned researchers to\nkick-start their projects with relative ease. Ad-\nditionally, it provides interactive visualizations\nand demonstrations of classic models for edu-\ncational purposes. The initial release of Am-\nphion v0.1 supports a range of tasks including\nText to Speech (TTS), Text to Audio (TTA),\nand Singing Voice Conversion (SVC), supple-\nmented by essential components like data pre-\nprocessing, state-of-the-art vocoders, and eval-\nuation metrics. This paper presents a high-level\noverview of Amphion.\n1\nIntroduction\nThe development of deep learning (LeCun et al.,\n2015) has greatly improved the performance of gen-\nerative models (Hinton and Salakhutdinov, 2006;\nKingma and Welling, 2014; Goodfellow et al.,\n2020; Ho et al., 2020; Vaswani et al., 2017). Lever-\naging these models has enabled researchers and\npractitioners to explore innovative possibilities,\nleading to notable progress and breakthroughs\nacross various fields, including computer vision\n(Rombach et al., 2022) and natural language pro-\ncessing (Radford et al., 2019; Brown et al., 2020;\nRen et al., 2019, 2020; Shen et al., 2018, 2024).\nThe potential in tasks related to audio1, music, and\n*Equal Contribution.\n\u2020Correspondence to <wuzhizheng@cuhk.edu.cn>.\n1The term \u201caudio\" encompasses two distinct interpreta-\ntions. The first interpretation adopts a narrow view, specifi-\ncally associating \u201caudio\" with sound effects, a definition fre-\nquently employed alongside music and speech throughout this\nFigure 1: The north-star goal of the Amphion toolkit:\nAny to Audio.\nspeech generation has spurred the scientific com-\nmunity to actively publish new models and ideas\n(Liu et al., 2023a,b; Ren et al., 2019, 2020; Shen\net al., 2018, 2024).\nThere is an increasing presence of both official\nand community-driven open-source repositories\nthat replicate these models. However, the quality\nof these repositories varies significantly (Stamelos\net al., 2002; Borges and Valente, 2018; Aghajani\net al., 2020; Wolf et al., 2020), and they are of-\nten scattered, focusing on specific papers. These\nscattered repositories introduce several obstacles to\njunior researchers or engineers who are new to the\nresearch area. First, attempts to replicate an algo-\nrithm using different implementations or configura-\ntions can result in inconsistent model functionality\nor performance (Paszke et al., 2019; Wolf et al.,\n2020; Lhoest et al., 2021). Second, while many\nrepositories focus on the model architectures, they\noften neglect crucial steps such as detailed data\npre-processing, feature extraction, model training,\nand systematic evaluation. This lack of systematic\nguidance poses substantial challenges for begin-\nners (Georgiou et al., 2022), who may have limited\ntechnical expertise and experience in training large-\nscale models. In summary, the scattered nature\nof these repositories hampers efforts towards re-\nproducible research and fair comparisons among\nmodels or algorithms.\nMotivated by that, we introduce Amphion, an\npaper. The second interpretation presents a broader perspec-\ntive, where \u201caudio\" collectively refers to sound effects, music,\nand speech, embracing all aspects of auditory content.\narXiv:2312.09911v2  [cs.SD]  22 Feb 2024\nToolkit\nAudio\nMusic/Singing\nSpeech\nVisualization\nAmphion\n\u2713\n\u2713\n\u2713\n\u2713\nAudioCraft\n\u2713\n\u2713\nBark\n\u2713\n\u2713\n\u2713\nCoqui TTS\n\u2713\nEmotiVoice\n\u2713\nESPnet\n\u2713\n\u2713\nFestival\n\u2713\nHTS\n\u2713\nMerlin\n\u2713\nMocking Bird\n\u2713\nMuskits\n\u2713\nMuzic\n\u2713\nOpenVoice\n\u2713\nPaddleSpeech\n\u2713\n\u2713\nSoftVC VITS\n\u2713\nSpeechBrain\n\u2713\nTANGO\n\u2713\n\u2713\nTorToiSe\n\u2713\nWeTTS\n\u2713\nTable 1: Representative open-source toolkits related to\naudio, music and speech generation currently available\nin the open-source community (sort alphabetically).\nopen-source platform dedicated to the north-star ob-\njective of \u201cAny to Audio\u201d (Figure 1). The initiative\naims to facilitate the conversion of any input into an\naudible audio signal. Compared with the existing\nopen-source toolkits as presented in Table 1, Am-\nphion integrates audio, music, and speech genera-\ntion into a unified framework that provides shared\nworkflow across all models, including dataset pro-\ncessing, feature extraction, model training, infer-\nence, evaluation, and open-source pre-trained mod-\nels. To aid junior engineers and researchers in\nunderstanding the internal mechanism of genera-\ntive models, Amphion provides visualizations and\ninteractive demonstrations of classic models. In a\nnutshell, Amphion, with its foundation in educa-\ntion, holds four unique strengths that set it apart\nfrom current open-source tools or repositories, in\nparticular,\n\u2022 Unified Framework: Amphion provides a uni-\nfied framework for audio, music, and speech\ngeneration and evaluation. It is designed to be\nadaptable, flexible, and scalable, supporting the\nintegration of new models.\n\u2022 Beginner-friendly End-to-End Workflow:\nAmphion offers a beginner-friendly end-to-end\nworkflow with straightforward documentation\nand instructions. It establishes itself as an ac-\ncessible one-stop research platform suitable for\nboth novices and experienced researchers, in-\ncluding plenty of engineering know-how.\n\u2022 Open Pre-trained Models: To promote repro-\nducible research, Amphion commits to strict\nstandards to the release of pre-trained models.\nIn partner with industry, Amphion aims to make\nlarge-scale pre-trained models widely available\nfor various applications.\n\u2022 Visualization and Interactivity: Amphion pro-\nvides visualization tools to interactively illus-\ntrate the internal processing mechanism of clas-\nsic models.\nThis provides an invaluable re-\nsource for educational purposes and for facili-\ntating understandable research.\nThe Amphion v0.1 toolkit, now available under\nthe MIT license,2 has supported a diverse array of\ngeneration tasks. This paper presents a high-level\noverview of the Amphion v0.1 toolkit.\n2\nUnified Framework\nThe north-star goal of Amphion is to unify various\naudible waveform generation tasks. To make the\nframework adaptable and flexible to varied forms\nof tasks, from the perspective of input, we formulate\naudio generation tasks into three categories,\n1. Text to Waveform: The input consists of dis-\ncrete textual tokens, which strictly constrain\nthe content of the output waveform.\nThe\nrepresentative tasks include Text to Speech\n(TTS) (Tan et al., 2021) and Singing Voice Syn-\nthesis (SVS)3 (Liu et al., 2022a).\n2. Descriptive Text to Waveform: The input con-\nsists of discrete textual tokens, which generally\nguide the content or style of the output waveform.\nThe representative tasks include Text to Audio\n(TTA) (Kreuk et al., 2022; Yang et al., 2023) and\nText to Music (TTM) (Copet et al., 2023).\n3. Waveform to Waveform: Both the input and\noutput are continuous waveform signals. The\nrepresentative tasks include Voice Conversion\n(VC) (Sisman et al., 2021), Singing Voic Conver-\nsion (SVC) (Liu et al., 2021; Zhang et al., 2023),\nEmotion Conversion (EC) (Zhou et al., 2022),\nAccent Conversion (AC) (Felps et al., 2009), and\nSpeech Translation (ST) (Song et al., 2023a).\nThis section will introduce the system architec-\nture design and multiple classic generation tasks\nthat are released in v0.1.\n2https://github.com/open-mmlab/Amphion\n3In addition to textual tokens, the input of SVS also in-\ncludes musical tokens such as MIDI notes (Liu et al., 2022a).\nHowever, these inputs in SVS are also discrete and strictly\nconstrain the content (both linguistic and musical content)\nlike TTS. Therefore, we categorize SVS under the Text to\nWaveform classification.\n2.1\nSystem Architecture Design\nTo achieve the north-star objective, the Amphion\narchitecture is designed with the following three\nprinciples,\n\u2022 To establish a fair comparison platform for\na multitude of models.\nIn particular, Am-\nphion provides consistent data preprocessing,\nunified underlying training framework, and uni-\nfied vocoders to create a fair experimental envi-\nronment for comparison.\n\u2022 To save practitioners from the tedious and\nrepetitive task of \u201creinventing the wheel\".\nAmphion abstracts various classic components,\nsuch as feature extraction, neural network mod-\nules, and batch sampling, into a unified infras-\ntructure.\n\u2022 To facilitate the unification of research across\nthe entire audio domain. In the context of to-\nday\u2019s era of big data and large-scale pre-trained\nmodels, a unified data preprocessing, model ex-\ntraction, and training framework can facilitate\nthe development of the entire audio field.\nFigure 2: System architecture design of Amphion.\nIn particular, the system architecture design is\npresented in Figure 2. From the bottom up,\n1. We unify the data processing (Dataset, Feature\nExtractor, Sampler, and DataLoader), the opti-\nmization algorithms (Optimizer, Scheduler, and\nTrainer), and the common network modules\n(Module) as the infrastructure for all the audio\ngeneration tasks.\n2. For each specific generation task, we unify its\ndata/feature usage (TaskLoader), task frame-\nwork (TaskFramework), and training pipeline\n(TaskTrainer).\n3. Under each generation task, for every specific\nmodel, we specify its architecture (ModelArchi-\ntecture) and training pipeline (ModelTrainer).\n4. Finally, we provide a textitrecipe of each model\nfor users. We unify the recipe format across\nall models, ensuring it is as self-contained and\nnovice-friendly as possible. Besides, we pro-\nvide the visualizations to demonstrate the inter-\nnal mechanisms of some typical models. On top\nof pre-trained models, we also offer interactive\ndemos for users to explore.\n2.2\nClassic Audio Generation Tasks\nAmphion v0.1 toolkit includes a representative\nfrom each of the three generation task categories\n(namely TTS, TTA, and SVC) for integration. This\nensures that Amphion\u2019s framework can be conve-\nniently adaptable to other audio generation tasks\nduring future development. Notably, most audio\ngeneration models usually adopt a two-stage gen-\neration process, where they generate some inter-\nmediate acoustic features (e.g. Mel Spectrogram)\nin the first stage, and then generate the final audi-\nble waveform using a Vocoder in the second stage\n(Figure 3d). Motivated by that, Amphion v0.1 also\nintegrates a variety of vocoder models.\nSpecifically, the pipelines of these classic audio\ntasks are designed as follows:\n\u2022 Text to Speech: TTS aims to convert written\ntext into spoken speech (Tan et al., 2021). The\ntraditional TTS considers only textual tokens as\ninput (Wang et al., 2017; Ren et al., 2020; Kim\net al., 2021). Recently, zero-shot TTS attracts\nmore attentions from the research community. In\naddition to text, zero-shot TTS requires a refer-\nence audio as a prompt (Figure 3a). By utilizing\nin-context learning techniques, it can imitate the\ntimbre and speaking style of the reference au-\ndio (Shen et al., 2024; Wang et al., 2023a).\n\u2022 Text to Audio: TTA aims to generate sounds that\nare semantically in line with descriptions (Kreuk\net al., 2022; Yang et al., 2023). It usually re-\nquires a pre-trained text encoder like T5 (Kale\nand Rastogi, 2020) to capture the global infor-\nmation of the input descriptive text first, and\nthen utilizes an acoustic model, such as diffusion\nmodel (Huang et al., 2023a; Liu et al., 2023a;\nWang et al., 2023b), to synthesize the acoustic\nfeatures (Figure 3b).\n(a) Text to Speech\n(b) Text to Audio\n(c) Singing Voice Conversion\n(d) Vocoder\nFigure 3: Pipelines of the classic audio generation tasks\nin Amphion.\n\u2022 Singing Voice Conversion: SVC aims to trans-\nform the voice of a singing signal into the voice\nof a target singer while preserving the lyrics\nand melody (Huang et al., 2023b). To empower\nthe reference speaker to sing the source audio,\nthe main idea is to extract the speaker-specific\nrepresentations from the reference, extract the\nspeaker-agnostic representations (including se-\nmantic and prosody features) from the source,\nand then synthesize the converted features using\nacoustic models (Zhang et al., 2023) (Figure 3c).\nCurrent supported tasks, models and algorithms\nof Amphion v0.1 are presented in Table 2.\n3\nBeginner-Friendly End-to-End\nWorkflow\nAmphion provides a beginner-friendly, end-to-end\nworkflow structured into five stages: data prepro-\ncessing, feature extraction, model training, model\ninference and evaluation, and visualization. No-\ntably, Amphion workflow incorporates plenty of en-\ngineering know-how for training models on large-\nscale datasets such as Libri-light (Kahn et al., 2020)\nand MLS (Pratap et al., 2020). This enables novice\nresearchers to replicate advanced, proprietary sys-\ntems. Figure 4 presents the primary workflow of\nAmphion, outlined briefly as follows,\nFigure 4: An overview of Amphion workflow.\n\u2022 Data Preprocessing:\nAmphion offers pre-\nimplemented,\ndataset-specific preprocessing\nscripts for a wide range of academic datasets\nin audio, music, and speech generation research\n(Table 2), enabling a seamless transition from\ndataset-specific structures to Amphion\u2019s unified\ndata format.\n\u2022 Feature Extraction: After preprocessing, Am-\nphion allows for both offline and on-the-fly fea-\nture extraction. Offline extraction, which occurs\nbefore model training, saves extracted features\nlocally for later use in training. However, for\nlarge-scale datasets, it is infeasible to perform\nthe lengthy offline extraction. Motivated by that,\nAmphion offers an on-the-fly extraction that pro-\ncesses raw waveforms in batches and utilizes\nGPU as much as possible to accelerate feature\nextraction during model training.\n\u2022 Model Training: Amphion enhances training ef-\nficiency by supporting multi-GPU training with a\nDistributed Data Parallel (DDP) trainer (Gugger\net al., 2022), dynamic batch sizes, and other user-\nfriendly training infrastructures, simplifying the\nreplication of state-of-the-art models.\n\u2022 Model Inference and Evaluation: Amphion\nprovides various pre-trained model checkpoints\nof top-performing vocoders, one-click evaluation\nscripts, and interactive demonstrations, enabling\nhigh-quality generation and easy experimenta-\ntion. The supported evaluation metrics can be\nseen in Table 2.\n\u2022 Visualization: Lastly, Amphion introduces a\nunique interactive visual analysis of some clas-\nsical models for educational purposes, helping\nnewcomers understand their inner workings.\n4\nOpen Pre-trained Models\nPre-trained checkpoints can empower practitioners\nto reproduce experimental results and build appli-\nDatasets\nTasks, Models and Algorithms\nEvaluation Metrics\n\u2022 Academic Datasets:\n\u25e6 AudioCaps (Kim et al.,\n2019)\n\u25e6 CSD (Choi et al., 2020)\n\u25e6 KiSing\n\u25e6 Libri-light (Kahn et al.,\n2020)\n\u25e6 LibriTTS (Zen et al., 2019)\n\u25e6 LJSpeech\n\u25e6 M4Singer (Zhang et al.,\n2022)\n\u25e6 MLS (Pratap et al., 2020)\n\u25e6 NUS-48E (Duan et al.,\n2013)\n\u25e6 Opencpop (Wang et al.,\n2022)\n\u25e6 OpenSinger (Huang et al.,\n2021)\n\u25e6 Opera (Black et al., 2014)\n\u25e6 PopBuTFY (Liu et al.,\n2022b)\n\u25e6 PopCS (Liu et al., 2022a)\n\u25e6 PJS (Koguchi et al., 2020)\n\u25e6 SVCC\n(Huang\net\nal.,\n2023b)\n\u25e6 VCTK (Yamagishi et al.,\n2019)\nAmphion also provides a uni-\nfied data format for user cus-\ntom dataset\nText to Speech\n\u2022 Transformer-based:\nFastSpeech\n2 (Ren et al., 2020)\n\u2022 Flow-based: VITS (Kim et al., 2021)\n\u2022 Diffusion-based:\nNaturalSpeech\n2 (Shen et al., 2024)\n\u2022 Autoregressive-based:\nVALL-\nE (Wang et al., 2023a)\nSinging Voice Conversion\n\u2022 Features Extraction:\n\u25e6 Semantic Features: WeNet (Yao\net al., 2021), Whisper (Radford\net al., 2023), HuBERT (Hsu et al.,\n2021), ContentVec (Qian et al.,\n2022)\n\u25e6 Prosody Features: F0 extracted by\nParselmouth (Jadoul et al., 2018),\nEnergy calculated based on STFT\nspectrograms (Ren et al., 2020)\n\u2022 Acoustic Models:\n\u25e6 Diffusion-based:\nDif-\nfWaveNetSVC\n(Zhang\net\nal.,\n2023), DiffComoSVC (Song et al.,\n2023b; Lu et al., 2024)\n\u25e6 Flow-based: VitsSVC (Kim et al.,\n2021; SVC-Develop-Team, 2023)\n\u25e6 Transformer-based:\nTransform-\nerSVC (Vaswani et al., 2017)\nText to Audio\n\u2022 Text Encoder:\n\u25e6 T5 (Kale and Rastogi, 2020)\n\u2022 Acoustic Models:\n\u25e6 Diffusion-based: AudioLDM (Liu\net al., 2023a; Wang et al., 2023b)\nVocoder\n\u2022 Autoregressive-based:\nWaveNet (van den Oord et al.,\n2016),\nWaveRNN\n(Kalchbrenner\net al., 2018)\n\u2022 Diffusion-based: DiffWave (Kong\net al., 2021)\n\u2022 Flow-based:\nWaveGlow (Prenger\net al., 2019)\n\u2022 GAN-based:\n\u25e6 Generators:\nMelGAN (Kumar\net al., 2019), HiFi-GAN (Su et al.,\n2020), NSF-HiFiGAN (Yoneyama\net al., 2023), BigVGAN (Lee et al.,\n2023), APNet (Ai and Ling, 2023)\n\u25e6 Discriminators:\nMSD (Kumar\net al., 2019), MPD (Su et al.,\n2020), MRD (Jang et al., 2020),\nMS-STFTD (D\u00e9fossez et al., 2022),\nMS-SB-CQTD (Gu et al., 2024)\n\u2022 F0 Modeling: F0 Pearson\nCoefficients (FPC), F0\nPeriodicity Root Mean\nSquare\nError\n(Period-\nicity) (Morrison et al.,\n2022),\nF0 Root Mean\nSquare Error (F0RMSE),\nVoiced/Unvoiced\nF1\nScore (V/UV F1).\n\u2022 Energy Modeling:\nEn-\nergy Root Mean Square\nError (EnergyRMSE), En-\nergy Pearson Coefficients\n(EnergyPC).\n\u2022 Spectrogram Distortion:\nPESQ (Rix et al., 2001),\nSTOI (Taal et al., 2010),\nFAD\n(Kilgour\net\nal.,\n2019), MCD (Kubichek,\n1993),\nSI-SNR\n(Luo\nand\nMesgarani,\n2018),\nSI-SDR\n(Roux\net\nal.,\n2019)\n\u2022 Intelligibility: Word Er-\nror Rate (WER) and Char-\nacter Error Rate (CER)\nbased on Whisper (Rad-\nford et al., 2023)\n\u2022 Speaker Similarity: Co-\nsine similarity based on\nRawNet3 (Jung et al.,\n2022), Resemblyzer, and\nWavLM\n(Chen\net\nal.,\n2022).\nTable 2: Supported datasets, models, and metrics of Amphion v0.1.\ncations. Public pre-trained models could facilitate\ninnovation and learning for practitioners. For that\npurpose, Amphion has released a variety of models\npertaining to Audio, Music, and Speech Genera-\ntion, such as TTS, TTA, SVC, and Vocoder. Our\nstandards for releasing pre-trained models are de-\nsigned to ensure transparency, reproducibility, and\nusability for the community. Each model is re-\nleased with a model card that includes not only\nthe checkpoint but also detailed descriptions of the\nfollowing criteria:\n\u2022 Model Metadata: Detail the model architecture\nand the number of parameters.\n\u2022 Training Datasets: List all the training corpus\nand their sources.\n\u2022 Training Configuration: Detail the training hy-\nberparameters (like batch size, learning rate, and\nnumber of training steps) and the computational\nplatform.\n\u2022 Evaluation Results: Display the evaluation re-\nsults and the performance comparison to other\ntypical baselines.\n\u2022 Usage Instructions: Instruct how to inference\nand fine-tune based on the pre-trained model.\n\u2022 Interactive Demo: Provide an online interactive\ndemo for users to explore.\n\u2022 License: Clear the licensing details including\nhow the model can be utilized, shared, and modi-\nfied.\n\u2022 Ethical Considerations: Address ethical consid-\nerations related to the model\u2019s application, focus-\ning on privacy, consent, and bias, to encourage\nresponsible usage.\nAs of now, the released pre-trained models of\nAmphion v0.1 are listed in Table 3. We are part-\nnering with industry for larger models on larger\ndatasets.\n5\nVisualization and Interactivity\nAiming to serve as a platform for exploring and\nunderstanding generative models with educational\npurposes, Amphion provides intuitive and engaging\nvisualization tools for classic models or algorithms.\nVisualizations act as a bridge between abstract\ntheoretical concepts and their practical applications,\nenabling users to see the \u201chow\" and \u201cwhy\" behind\nmodel outputs (Hohman et al., 2018). By provid-\ning a visual representation of model architectures,\nTask\nApplication Scenario\nModel Architecture\n#Parameters\nTraining Datasets\nTTS\nMulti-speaker TTS\nVITS\n(Kim et al., 2021)\n30M\nHi-Fi TTS (Bakhturina et al., 2021)\nZero-shot TTS\nNaturalSpeech2\n(Shen et al., 2024)\n201M\nLibri-light (Kahn et al., 2020)\nTTA\nSound effects\nsynthesis\nAudioLDM\n(Wang et al., 2023b)\n710M\nAudioCaps (Kim et al., 2019)\nSVC\nVocalist singing\nconversion\nDiffWaveNetSVC\n(Zhang et al., 2023)\n31M\nVocalist Dataset (private)\nVocoder\nSpeech synthesis\nHiFi-GAN\n(Su et al., 2020)\n24M\nLJSpeech, VCTK (Yamagishi et al., 2019),\nLibriTTS (Zen et al., 2019)\nSinging synthesis\nBigVGAN\n(Lee et al., 2023)\n112M\nOpencpop (Wang et al., 2022), M4Singer (Zhang et al., 2022),\nPJS (Koguchi et al., 2020), CSD (Choi et al., 2020),\nOpenSinger (Huang et al., 2021), PopBuTFy (Liu et al., 2022b),\nOpera (Black et al., 2014), NUS-48E (Duan et al., 2013),\nKiSing, PopCS (Liu et al., 2022a)\nTable 3: Supported pre-trained models of Amphion v0.1.\ndata flow, and parameter effects, Amphion demysti-\nfies the inner workings of generative models, mak-\ning the technology more approachable. Amphion\nv0.1 has developed the visualization system for\ndiffusion model under SVC task, namely SingVi-\nsio4 (Xue et al., 2024). The interface of the inter-\nactive visualization is shown in the appendix. This\ntool offers a dynamic and interactive visualization\nof the generation process in diffusion models, illus-\ntrating the gradual denoising of the noisy spectrum\nand its evolution into a clear spectrum that embod-\nies the target singer\u2019s timbre.\nAmphion also utilizes online platforms like Gra-\ndio5 to support a series of interactive demos. With\nthese interactive demos, users can input text or se-\nlect predefined prompts, and the underlying models\nwill generate corresponding audio outputs instantly.\nThe interactive interface provides users with im-\nmediate feedback and allows them to modify input\nparameters, select options, or make choices using\ndrop-down menus, check-boxes, or sliders, depend-\ning on the specific task and model being utilized.\nWe provide an interactive demo example in the\nappendix.\nAmphion\u2019s visualization capabilities stand out\nas one of its most unique features, designed to de-\nmystify complex generative models for those at the\nbeginning of their research journey. By merging\nthe capabilities of Gradio with the visualization\nfeatures of Amphion, the toolkit provides a seam-\nless and interactive experience for users to explore\nand comprehend the inner workings of generative\nmodels. This combination of visualizations and\ninteractivity empowers researchers, engineers, and\n4https://dsvc.openmmlab.org.cn/\n5https://www.gradio.app/\nenthusiasts to experiment, iterate, and gain insights\ninto the complex world of audio generation.\n6\nExperiments\nWe compare the performance of Amphion v0.1 on\nthree tasks, namely TTS, TTA and SVC, and the\nvocoder with public open repositories.\nWe use both objective and subjective evaluations\nto evaluate different tasks. The objective evalua-\ntion metrics are presented in Table 2. Regarding\nthe Mean Opinion Score (MOS) and the Similarity\nMean Opinion Score (SMOS) tests, listeners are\nrequired to grade ranging from 1 (\u201cBad\u201d) to 5 (\u201cEx-\ncellent\u201d) and from 1 (\"Different speaker, sure\") to\n5 (\"Same speaker, sure\") individually with an inter-\nval of 1 to evaluate the generated audio\u2019s overall\nquality and the similarity to the reference speaker;\nregarding the Naturalness and Similarity tests, lis-\nteners are required to grade ranging from 1 (\u201cBad\u201d)\nto 5 (\u201cExcellent\u201d) and from 1 (\"Different speaker,\nsure\") to 4 (\"Same speaker, sure\") individually with\nan interval of 1 to evaluate the conversed singing\nutterance\u2019s naturalness and the similarity to the ref-\nerence singer. All the subjective evaluation score\nlisted are within a 95% Confidence Interval.\n6.1\nText to Speech\n6.1.1\nResults of Classic TTS\nWe evaluate the performance of Amphion v0.1 TTS\nfollowing the evaluation pipeline in Amphion v0.1.\nSpecifically, we construct testing sets with 100 text\ntranscriptions and then generate the correspond-\ning speech using Amphion v0.1 VITS and other\nfour popular open-source speech synthesis toolk-\nSystems\nCER \u2193\nWER \u2193\nFAD \u2193\nMOS \u2191\nCoqui TTS (VITS)\n0.06\n0.12\n0.54\n3.69 \u00b1 0.10\nSpeechBrain (FastSpeech 2)\n0.06\n0.11\n1.71\n3.54 \u00b1 0.11\nTorToiSe (Diffusion-based model)\n0.05\n0.09\n1.90\n3.61 \u00b1 0.11\nESPnet (VITS)\n0.07\n0.11\n1.28\n3.57 \u00b1 0.11\nAmphion v0.1 (VITS)\n0.06\n0.10\n0.84\n3.61 \u00b1 0.10\nTable 4: Evaluation results of classic TTS in Amphion v0.1.\nSystems\nTraining Dataset\nMOS\u2191\nSMOS \u2191\nGround Truth\n-\n4.72 \u00b1 0.10\n4.15 \u00b1 0.26\nYourTTS\nLibriTTS\n3.11 \u00b1 0.24\n3.03 \u00b1 0.19\nVALL-E\nSelf-collected\n3.88 \u00b1 0.23\n3.45 \u00b1 0.27\nAmphion v0.1 (NaturalSpeech2)\nLibri-light\n3.56 \u00b1 0.18\n3.37 \u00b1 0.19\nTable 5: Evaluation results of zero-shot TTS in Amphion v0.1.\nSystems\nDataset\nFAD \u2193\nIS \u2191\nKL \u2193\nDiffsound\nAudioSet\nAudioCaps\n47.68\n4.01\n2.52\nAudioGen\nAudioSet\nAudioCaps\n-\n-\n2.09\nMake-an-Audio\nAudioSet\nAudioCaps\n13 others\n-\n-\n2.79\nAudioLDM-L\nAudioCaps\n27.12\n7.51\n1.86\nAudioLDM-L-Full\nAudioSet\nAudioCaps\n2 others\n23.31\n8.13\n1.59\nAUDIT\nAudioSet96\nAudioCaps\n2 others\n20.19\n9.23\n1.32\nAmphion v0.1\n(AudioLDM)\nAudioCaps\n20.47\n8.78\n1.44\nTable 6: Evaluation results of Text to Audio in Amphion\nv0.1. \u201c# others\u201d indicates the use of additional # datasets\nfrom different sources. More information is available\nin Huang et al. (2023a); Liu et al. (2023a).\nits, including Coqui TTS6 (VITS is used in this\nsystem), SpeechBrain7 (FastSpeech 2 is used be-\ncause FastSpeech 2 is the most advanced model\nin this system), TorToiSe8 (an autoregressive and\ndiffusion-based model), and ESPnet9 (VITS is used\nin this system). The evaluation results are shown\nin Table 4. The evaluation results show that the\nperformance of Amphion v0.1 VITS is comparable\nto existing open-source systems.\n6https://github.com/coqui-ai/TTS accessed in Dec 2023\n7https://github.com/speechbrain/speechbrain accessed in\nDec 2023\n8https://github.com/neonbjb/tortoise-tts accessed in Dec\n2023\n9https://github.com/espnet/espnet accessed in Dec 2023\n6.1.2\nResults of Zero-shot TTS\nWe also evaluate the speech quality and speaker\nsimilarity of Amphion v0.1 zero-shot TTS systems\nalongside other open-source zero-shot TTS models.\nWe use the samples from NaturalSpeech2 (Shen\net al., 2024) demo page10 as our test set.\nWe\ncompare our Amphion v0.1 NaturalSpeech2 with\nYourTTS (Casanova et al., 2022), and VALL-\nE (Wang et al., 2023a) from the reproduced VALL-\nE repository11. The results are shown in Table 5.\nThe evaluation results show that Amphion v0.1 Na-\ntrualSpeech2 is significantly better than YourTTS\nin terms of speech quality and speaker similarity.\nCompared with the open-source reproduced VALL-\nE, we have comparable speaker similarity, and the\ndifference in speech quality may come from the\ndifference in training data.\n6.2\nText to Audio\nWe use inception score (IS), Fr\u00e9chet Distance (FD),\nand Kullback\u2013Leibler Divergence (KL) to evaluate\nour text-to-audio generation model. FD measures\nthe fidelity between the generated samples and tar-\nget samples. IS measures the quality and diversity\nof the generated samples. KL measures the correla-\ntion between output samples and the target samples.\nFD, IS, and KL are based on the state-of-the-art au-\ndio classification model PANNs (Kong et al., 2020).\nWe use the test set of AudioCaps as our test set.\nThe evaluation results of Amphion v0.1 TTA are\nshown in Table 6. The results demonstrate that our\n10https://speechresearch.github.io/naturalspeech2 accessed\nin Jan 2024\n11https://github.com/Plachtaa/VALL-E-X accessed in Jan\n2024\nSystems\nNaturalness \u2191\nSimilarity \u2191\nGround truth\n4.67 \u00b1 0.18\n3.17 \u00b1 0.29\nSoftVC VITS\n2.98 \u00b1 0.31\n2.74 \u00b1 0.31\nAmphion v0.1 (DiffWaveNetSVC)\n3.52 \u00b1 0.26\n2.95 \u00b1 0.3\nTable 7: Evaluation results of Singing Voice Conversion in Amphion v0.1. The full scores of naturalness and\nsimilarity are 5 and 4 respectively.\nSystems\nPESQ \u2191\nM-STFT \u2193\nF0RMSE \u2193\nFPC \u2191\nHiFi-GAN ( UNIVERSAL_V1)\n3.43\n1.98\n177\n0.88\nESPnet (libritts_hifigan.v1)\n3.55\n1.12\n188\n0.86\nAmphion v0.1 (HiFi-GAN)\n3.55\n1.09\n188\n0.88\nTable 8: Objective evaluation results of Vocoder in Amphion v0.1.\nAmphion v0.1 AudioLDM system achieves similar\nresults to state-of-the-art models.\n6.3\nSinging Voice Conversion\nTo evaluate the effectiveness of the SVC models of\nAmphion v0.1, we adopt the in-domain evaluation\ntask of the Singing Voice Conversion Challenge\n(SVCC) 202312 (Huang et al., 2023b). Specifically,\nthere are 48 evaluated singing utterances, with 24\nfrom a male and 24 from a female. Our task is to\nconvert them into two target singers (one male and\none female). For the training data, we utilize five\ndatasets: Opencpop (Wang et al., 2022), SVCC\ntraining data (Huang et al., 2023b), VCTK (Yamag-\nishi et al., 2019), OpenSinger (Huang et al., 2021),\nand M4Singer (Zhang et al., 2022). There are 83.1\nhours of speech and 87.2 hours of singing data in\ntotal. We compare the DiffWaveNetSVC (Zhang\net al., 2023) of Amphion v0.1 with SoftVC\nVITS (SVC-Develop-Team, 2023), which is one of\nthe most popular open-source project in the SVC\narea. The MOS evaluation results for Amphion\nv0.1 SVC are shown in Table 7. We can see the\nDiffWaveNetSVC model of Amphion v0.1 owns\nbetter performance in both naturalness and speaker\nsimilarity.\n6.4\nVocoder\nWe compare the Amphion v0.1 Vocoder to the\ntwo widely used open-source HiFi-GAN check-\npoints.\nOne is the UNIVERSAL_V1 from the of-\nficial HiFi-GAN repository13; the other is the\nlibritts_hifigan.v1 used by the ESPnet14. All\n12SVCC 2023: http://vc-challenge.org/\n13https://github.com/jik876/hifi-gan accessed in Dec 2023\n14https://github.com/kan-bayashi/ParallelWaveGAN\naccessed in Dec 2023\nof the checkpoints are trained on around 600 hours\nof speech data. The whole evaluation set and the\ntest set of LibriTTS are used for evaluation, with\na total of 20,306 utterances.\nObjective evalua-\ntion is conducted with M-STFT, PESQ, F0RMSE,\nand FPC. The results are illustrated in Table 8.\nWith the assistance of additional guidance from\nTime-Frequency Representation-based Discrimina-\ntors (Gu et al., 2024), the Amphion v0.1 HiFi-GAN\nachieves superior performance in spectrogram re-\nconstruction and F0 modeling.\n7\nConclusions\nThis paper presented Amphion, an open-source\ntoolkit dedicated to audio, music, and speech gen-\neration. Amphion\u2019s primary objective is to facili-\ntate reproducible research and serve as a stepping\nstone for junior researchers and engineers entering\nthe field of audio, music, and speech generation.\nBesides focusing on specific generation tasks, Am-\nphion provides visualizations of classic models or\narchitectures to empower users with a deeper un-\nderstanding of the underlying processes.\nLarge-scale datasets and large-scale models have\nproven to achieve state-of-the-art performance in\nvarious tasks. In the future, Amphion plans to\nrelease a few large-scale dataset in the area of au-\ndio, music and speech generation area. Also, we\nplan to partner with industry for large-scale and\nproduction-oriented pre-trained models. Since the\nrelease of Amphion in Dec 2023, Amphion has\nreceived more than 3, 500 stars on GitHub and\nreceived a significant number of requests and feed-\nback. We will actively work on Amphion to serve\njunior researchers and engineers.\nLimitations\nIn its initial release, Amphion has certain limita-\ntions that call for further exploration and enhance-\nment. While Amphion v0.1 has successfully in-\ncorporated Text to Speech, Text to Audio, Singing\nVoice Conversion, and a variety of Vocoders, there\nremains a multitude of audio generation tasks yet to\nbe integrated. Moreover, the audio processing field\nis progressing at a rapid pace, with new models\nand techniques being developed on a continuous\nbasis. Ensuring that Amphion stays up-to-date and\nseamlessly integrates these innovations will be an\nongoing challenge that needs to be addressed in\nfuture iterations.\nEthical Considerations\nIn providing an open-source toolkit for audio, mu-\nsic, and speech generation, we strive to balance\ninnovation with ethical responsibility. We acknowl-\nedge the ethical implications associated with our\nmodel\u2019s ability to generate a wide array of audio\nsignals. Such capabilities, while powerful, carry\npotential risks of misuse, such as the production of\nmisinformation, deepfake audio, or harmful con-\ntent. We fervently advocate for the responsible use\nof our code and pre-trained models, emphasizing\nthe imperative of adhering to regulatory standards.\nOur commitment to ethical considerations is not\nmerely an afterthought, but rather a guiding prin-\nciple in our pursuit of advancing the field of audio\nsignal generation.\nReferences\nEmad Aghajani, Csaba Nagy, Mario Linares-V\u00e1squez,\nLaura Moreno, Gabriele Bavota, Michele Lanza, and\nDavid C. Shepherd. 2020.\nSoftware documenta-\ntion: The practitioners\u2019 perspective. In ICSE, page\n590\u2013601.\nYang Ai and Zhen-Hua Ling. 2023. APNet: An all-\nframe-level neural vocoder incorporating direct pre-\ndiction of amplitude and phase spectra. IEEE/ACM\nTransactions on Audio, Speech, and Language Pro-\ncessing, 31:2145\u20132157.\nEvelina Bakhturina, Vitaly Lavrukhin, Boris Ginsburg,\nand Yang Zhang. 2021. Hi-Fi Multi-Speaker English\nTTS Dataset. INTERSPEECH, pages 2776\u20132780.\nDawn AA Black, Ma Li, and Mi Tian. 2014. Auto-\nmatic identification of emotional cues in chinese\nopera singing. In ICMPC.\nHudson Borges and Marco Tulio Valente. 2018. What\u2019s\nin a github star? understanding repository starring\npractices in a social coding platform. Journal of\nSystems and Software, 146:112\u2013129.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NIPS, pages 1877\u20131901.\nEdresson Casanova, Julian Weber, Christopher D\nShulby, Arnaldo Candido Junior, Eren G\u00f6lge, and\nMoacir A Ponti. 2022. Yourtts: Towards zero-shot\nmulti-speaker tts and zero-shot voice conversion for\neveryone. In ICML, pages 2709\u20132720. PMLR.\nSanyuan Chen, Chengyi Wang, Zhengyang Chen,\nYu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki\nKanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long\nZhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\nMichael Zeng, Xiangzhan Yu, and Furu Wei. 2022.\nWavlm: Large-scale self-supervised pre-training for\nfull stack speech processing. IEEE J. Sel. Top. Signal\nProcess., 16(6):1505\u20131518.\nSoonbeom Choi, Wonil Kim, Saebyul Park, Sangeon\nYong, and Juhan Nam. 2020. Children\u2019s song dataset\nfor singing voice research. In ISMIR.\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David\nKant, Gabriel Synnaeve, Yossi Adi, and Alexandre\nD\u00e9fossez. 2023. Simple and controllable music gen-\neration. In NeurIPS.\nAlexandre D\u00e9fossez, Jade Copet, Gabriel Synnaeve, and\nYossi Adi. 2022. High fidelity neural audio compres-\nsion. arXiv, abs/2210.13438.\nZhiyan Duan, Haotian Fang, Bo Li, Khe Chai Sim, and\nYe Wang. 2013. The NUS sung and spoken lyrics\ncorpus: A quantitative comparison of singing and\nspeech. In APSIPA, pages 1\u20139.\nDaniel Felps, Heather Bortfeld, and Ricardo Gutierrez-\nOsuna. 2009. Foreign accent conversion in computer\nassisted pronunciation training. Speech Commun.,\n51(10):920\u2013932.\nStefanos Georgiou, Maria Kechagia, Tushar Sharma,\nFederica Sarro, and Ying Zou. 2022. Green ai: Do\ndeep learning frameworks have different costs? In\nICSE, pages 1082\u20131094.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2020. Generative\nadversarial networks. Communications of the ACM,\n63(11):139\u2013144.\nYicheng Gu, Xueyao Zhang, Liumeng Xue, and\nZhizheng Wu. 2024. Multi-scale sub-band constant-\nq transform discriminator for high-fidelity vocoder.\nIn ICASSP.\nSylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp\nSchmid, Zachary Mueller, Sourab Mangrulkar, Marc\nSun, and Benjamin Bossan. 2022. Accelerate: Train-\ning and inference at scale made simple, efficient and\nadaptable.\nhttps://github.com/huggingface/\naccelerate.\nG. E. Hinton and R. R. Salakhutdinov. 2006. Reduc-\ning the dimensionality of data with neural networks.\nScience, 313(5786):504\u2013507.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models.\nIn NIPS,\npages 6840\u20136851.\nFred Hohman, Minsuk Kahng, Robert Pienta, and\nDuen Horng Chau. 2018. Visual analytics in deep\nlearning: An interrogative survey for the next fron-\ntiers. IEEE Transactions on Visualization and Com-\nputer Graphics, 25(8):2674\u20132693.\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. Hubert: Self-supervised\nspeech representation learning by masked prediction\nof hidden units. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 29:3451\u20133460.\nRongjie Huang, Feiyang Chen, Yi Ren, Jinglin Liu,\nChenye Cui, and Zhou Zhao. 2021. Multi-singer:\nFast multi-singer singing voice vocoder with A large-\nscale corpus. In ACM Multimedia, pages 3945\u20133954.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,\nLuping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xi-\nang Yin, and Zhou Zhao. 2023a. Make-an-audio:\nText-to-audio generation with prompt-enhanced dif-\nfusion models. In ICML, volume 202, pages 13916\u2013\n13932. PMLR.\nWen-Chin Huang, Lester Phillip Violeta, Songxiang\nLiu, Jiatong Shi, Yusuke Yasuda, and Tomoki Toda.\n2023b. The singing voice conversion challenge 2023.\nIn ASRU. IEEE.\nYannick Jadoul, Bill Thompson, and Bart de Boer. 2018.\nIntroducing Parselmouth: A Python interface to Praat.\nJournal of Phonetics, 71:1\u201315.\nWon Jang, Dan Lim, and Jaesam Yoon. 2020. Universal\nMelGAN: A robust neural vocoder for high-fidelity\nwaveform generation in multiple domains. arXiv,\nabs/2011.09631.\nJee-weon Jung, Youjin Kim, Hee-Soo Heo, Bong-Jin\nLee, Youngki Kwon, and Joon Son Chung. 2022.\nPushing the limits of raw waveform speaker recogni-\ntion. In INTERSPEECH, pages 2228\u20132232.\nJacob Kahn, Morgane Rivi\u00e8re, Weiyi Zheng, Evgeny\nKharitonov, Qiantong Xu, Pierre-Emmanuel Mazar\u00e9,\nJulien Karadayi, Vitaliy Liptchinsky, Ronan Col-\nlobert, Christian Fuegen, Tatiana Likhomanenko,\nGabriel Synnaeve, Armand Joulin, Abdelrahman Mo-\nhamed, and Emmanuel Dupoux. 2020. Libri-light: A\nbenchmark for ASR with limited or no supervision.\nIn ICASSP, pages 7669\u20137673. IEEE.\nNal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb\nNoury, Norman Casagrande, Edward Lockhart, Flo-\nrian Stimberg, A\u00e4ron van den Oord, Sander Diele-\nman, and Koray Kavukcuoglu. 2018. Efficient neural\naudio synthesis. In ICML, volume 80, pages 2415\u2013\n2424.\nMihir Kale and Abhinav Rastogi. 2020. Text-to-text\npre-training for data-to-text tasks. In INLG, pages\n97\u2013102.\nKevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi. 2019. Fr\u00e9chet Audio Distance: A\nreference-free metric for evaluating music enhance-\nment algorithms. In INTERSPEECH, pages 2350\u2013\n2354.\nChris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,\nand Gunhee Kim. 2019. AudioCaps: Generating\ncaptions for audios in the wild.\nIn NAACL-HLT,\npages 119\u2013132.\nJaehyeon Kim, Jungil Kong, and Juhee Son. 2021.\nConditional variational autoencoder with adversar-\nial learning for end-to-end text-to-speech. In ICML,\nvolume 139, pages 5530\u20135540.\nDiederik P. Kingma and Max Welling. 2014. Auto-\nencoding variational bayes. In ICLR.\nJunya Koguchi, Shinnosuke Takamichi, and Masanori\nMorise. 2020.\nPJS: phoneme-balanced japanese\nsinging-voice corpus. In APSIPA, pages 487\u2013491.\nQiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang,\nWenwu Wang, and Mark D Plumbley. 2020. PANNs:\nLarge-scale pretrained audio neural networks for au-\ndio pattern recognition. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 28:2880\u2013\n2894.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. 2021. DiffWave: A versatile diffu-\nsion model for audio synthesis. In ICLR.\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel\nSinger, Alexandre D\u00e9fossez, Jade Copet, Devi Parikh,\nYaniv Taigman, and Yossi Adi. 2022. AudioGen:\nTextually guided audio generation. In ICLR.\nRobert Kubichek. 1993. Mel-cepstral distance measure\nfor objective speech quality assessment. In Proceed-\nings of IEEE Pacific Rim Conference on Communi-\ncations Computers and Signal Processing, volume 1,\npages 125\u2013128.\nKundan Kumar, Rithesh Kumar, Thibault de Boissiere,\nLucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexan-\ndre de Br\u00e9bisson, Yoshua Bengio, and Aaron C.\nCourville. 2019. MelGAN: Generative adversarial\nnetworks for conditional waveform synthesis.\nIn\nNeurIPS, pages 14881\u201314892.\nYann LeCun, Yoshua Bengio, and Geoffrey Hinton.\n2015. Deep learning. Nature, 521(7553):436\u2013444.\nSang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catan-\nzaro, and Sungroh Yoon. 2023. Bigvgan: A universal\nneural vocoder with large-scale training. In ICLR.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario \u0160a\u0161ko, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Cl\u00e9ment Delangue, Th\u00e9o Matus-\nsi\u00e8re, Lysandre Debut, Stas Bekman, Pierric Cis-\ntac, Thibault Goehringer, Victor Mustar, Fran\u00e7ois\nLagunas, Alexander Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In EMNLP, pages 175\u2013184.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo\nLiu, Danilo P. Mandic, Wenwu Wang, and Mark D.\nPlumbley. 2023a. Audioldm: Text-to-audio genera-\ntion with latent diffusion models. In ICML, volume\n202 of Proceedings of Machine Learning Research,\npages 21450\u201321474. PMLR.\nHaohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao\nMei, Qiuqiang Kong, Yuping Wang, Wenwu Wang,\nYuxuan Wang, and Mark D Plumbley. 2023b. Au-\ndioLDM 2: Learning holistic audio generation with\nself-supervised pretraining. IEEE Transactions on\nPattern Analysis and Machine Intelligence.\nJinglin Liu, Chengxi Li, Yi Ren, Feiyang Chen, and\nZhou Zhao. 2022a. Diffsinger: Singing voice synthe-\nsis via shallow diffusion mechanism. In AAAI, pages\n11020\u201311028. AAAI Press.\nJinglin Liu, Chengxi Li, Yi Ren, Zhiying Zhu, and Zhou\nZhao. 2022b. Learning the beauty in songs: Neural\nsinging voice beautifier. In ACL (1), pages 7970\u2013\n7983. Association for Computational Linguistics.\nSongxiang Liu, Yuewen Cao, Dan Su, and Helen Meng.\n2021. DiffSVC: A diffusion probabilistic model for\nsinging voice conversion. In ASRU, pages 741\u2013748.\nYiwen Lu, Zhen Ye, Wei Xue, Xu Tan, Qifeng Liu,\nand Yike Guo. 2024. Comosvc: Consistency model-\nbased singing voice conversion.\narXiv preprint\narXiv:2401.01792.\nYi Luo and Nima Mesgarani. 2018. TaSNet: Time-\ndomain audio separation network for real-time,\nsingle-channel speech separation. In ICASSP, pages\n696\u2013700.\nMax Morrison, Rithesh Kumar, Kundan Kumar, Prem\nSeetharaman, Aaron C. Courville, and Yoshua Ben-\ngio. 2022. Chunked Autoregressive GAN for condi-\ntional waveform synthesis. In ICLR.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. In NIPS.\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel\nSynnaeve, and Ronan Collobert. 2020. MLS: A large-\nscale multilingual dataset for speech research. In\nINTERSPEECH, pages 2757\u20132761. ISCA.\nRyan Prenger, Rafael Valle, and Bryan Catanzaro. 2019.\nWaveglow: A flow-based generative network for\nspeech synthesis. In ICASSP, pages 3617\u20133621.\nKaizhi Qian, Yang Zhang, Heting Gao, Junrui Ni,\nCheng-I Lai, David D. Cox, Mark Hasegawa-\nJohnson, and Shiyu Chang. 2022. ContentVec: An\nimproved self-supervised speech representation by\ndisentangling speakers. In ICML, volume 162, pages\n18003\u201318017.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2023.\nRobust speech recognition via large-scale weak super-\nvision. In ICML, volume 202, pages 28492\u201328518.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nYi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2020. FastSpeech 2:\nFast and high-quality end-to-end text to speech. In\nICLR.\nYi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu. 2019. FastSpeech: Fast,\nrobust and controllable text to speech. In NeurIPS,\nvolume 32.\nAntony W. Rix, John G. Beerends, Michael P. Hollier,\nand Andries P. Hekstra. 2001. Perceptual evaluation\nof speech quality (PESQ)-a new method for speech\nquality assessment of telephone networks and codecs.\nIn ICASSP, pages 749\u2013752.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In CVPR, pages 10684\u201310695.\nJonathan Le Roux, Scott Wisdom, Hakan Erdogan, and\nJohn R. Hershey. 2019. SDR - half-baked or well\ndone? In ICASSP, pages 626\u2013630.\nJonathan Shen, Ruoming Pang, Ron J Weiss, Mike\nSchuster, Navdeep Jaitly, Zongheng Yang, Zhifeng\nChen, Yu Zhang, Yuxuan Wang, Rj Skerrv-Ryan,\net al. 2018. Natural TTS synthesis by conditioning\nwavenet on mel spectrogram predictions. In ICASSP,\npages 4779\u20134783.\nKai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong\nLeng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian.\n2024. Naturalspeech 2: Latent diffusion models are\nnatural and zero-shot speech and singing synthesizers.\nIn ICLR. OpenReview.net.\nBerrak Sisman, Junichi Yamagishi, Simon King, and\nHaizhou Li. 2021. An overview of voice conversion\nand its challenges: From statistical modeling to deep\nlearning. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 29:132\u2013157.\nKun Song, Yi Ren, Yi Lei, Chunfeng Wang, Kun Wei,\nLei Xie, Xiang Yin, and Zejun Ma. 2023a. Styles2st:\nZero-shot style transfer for direct speech-to-speech\ntranslation. arXiv preprint arXiv:2305.17732.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. 2023b.\nConsistency models.\narXiv\npreprint arXiv:2303.01469.\nIoannis\nStamelos,\nLefteris\nAngelis,\nApostolos\nOikonomou, and Georgios L Bleris. 2002. Code\nquality analysis in open source software development.\nInformation Systems Journal, 12(1):43\u201360.\nJiaqi Su, Zeyu Jin, and Adam Finkelstein. 2020. Hifi-\ngan: High-fidelity denoising and dereverberation\nbased on speech deep features in adversarial net-\nworks. In INTERSPEECH, pages 4506\u20134510.\nSVC-Develop-Team. 2023.\nSoftVC VITS Singing\nVoice\nConversion.\nhttps://github.com/\nsvc-develop-team/so-vits-svc.\nCees H Taal, Richard C Hendriks, Richard Heusdens,\nand Jesper Jensen. 2010. A short-time objective intel-\nligibility measure for time-frequency weighted noisy\nspeech. In ICASSP, pages 4214\u20134217. IEEE.\nXu Tan, Tao Qin, Frank Soong, and Tie-Yan Liu. 2021.\nA survey on neural speech synthesis. arXiv preprint\narXiv:2106.15561.\nA\u00e4ron van den Oord, Sander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew W. Senior, and Koray\nKavukcuoglu. 2016. WaveNet: A generative model\nfor raw audio. In Speech Synthesis Workshop, page\n125.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998\u20136008.\nChengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al. 2023a. Neural codec\nlanguage models are zero-shot text to speech synthe-\nsizers. arXiv preprint arXiv:2301.02111.\nYu Wang, Xinsheng Wang, Pengcheng Zhu, Jie Wu,\nHanzhao Li, Heyang Xue, Yongmao Zhang, Lei Xie,\nand Mengxiao Bi. 2022. Opencpop: A high-quality\nopen source chinese popular song corpus for singing\nvoice synthesis. In INTERSPEECH, pages 4242\u2013\n4246. ISCA.\nYuancheng Wang, Zeqian Ju, Xu Tan, Lei He, Zhizheng\nWu, Jiang Bian, and Sheng Zhao. 2023b. AUDIT:\nAudio editing by following instructions with latent\ndiffusion models. In NeurIPS.\nYuxuan Wang, R. J. Skerry-Ryan, Daisy Stanton,\nYonghui Wu, Ron J. Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio,\nQuoc V. Le, Yannis Agiomyrgiannakis, Rob Clark,\nand Rif A. Saurous. 2017. Tacotron: Towards end-\nto-end speech synthesis. In INTERSPEECH, pages\n4006\u20134010. ISCA.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn EMNLP, pages 38\u201345.\nLiumeng Xue, Chaoren Wang, Mingxuan Wang,\nXueyao Zhang, Jun Han, and Zhizheng Wu. 2024.\nSingvisio:\nVisual analytics of diffusion model\nfor singing voice conversion.\narXiv preprint\n2402.12660.\nJunichi Yamagishi, Christophe Veaux, Kirsten MacDon-\nald, et al. 2019. CSTR VCTK Corpus: English multi-\nspeaker corpus for cstr voice cloning toolkit (ver-\nsion 0.92). University of Edinburgh. The Centre for\nSpeech Technology Research (CSTR).\nDongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,\nChao Weng, Yuexian Zou, and Dong Yu. 2023.\nDiffsound: Discrete diffusion model for text-to-\nsound generation. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing.\nZhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang,\nFan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen,\nLei Xie, and Xin Lei. 2021. WeNet: Production\noriented streaming and non-streaming end-to-end\nspeech recognition toolkit. In INTERSPEECH, pages\n4054\u20134058.\nReo Yoneyama, Yi-Chiao Wu, and Tomoki Toda. 2023.\nSource-filter hifi-gan: Fast and pitch controllable\nhigh-fidelity neural vocoder. In ICASSP, pages 1\u20135.\nHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J.\nWeiss, Ye Jia, Zhifeng Chen, and Yonghui Wu. 2019.\nLibriTTS: A corpus derived from librispeech for text-\nto-speech. In INTERSPEECH, pages 1526\u20131530.\nLichao Zhang, Ruiqi Li, Shoutong Wang, Liqun Deng,\nJinglin Liu, Yi Ren, Jinzheng He, Rongjie Huang,\nJieming Zhu, Xiao Chen, and Zhou Zhao. 2022.\nM4singer: A multi-style, multi-singer and musical\nscore provided mandarin singing corpus. In NeurIPS.\nXueyao Zhang, Yicheng Gu, Haopeng Chen, Zihao\nFang, Lexiao Zou, Liumeng Xue, and Zhizheng Wu.\n2023. Leveraging content-based features from mul-\ntiple acoustic models for singing voice conversion.\nIn Machine Learning for Audio Workshop, NeurIPS\n2023.\nA: Metric View\nC: Step View\nB: Projection View\nD: Comparison View\nE: Control Panel\nFigure 5: Interactive visualization of diffusion model for singing voice conversion.\nFigure 6: Online interactive demo for singing voice conversion.\nKun Zhou, Berrak Sisman, Rui Liu, and Haizhou\nLi. 2022.\nEmotional voice conversion: Theory,\ndatabases and ESD. Speech Commun., 137:1\u201318.\nA\nVisualization and Interactive Demo\nCases in Amphion\nA.1\nVisualization for DiffWaveNetSVC\nAmphion has provided a visualization tool called\nSingVisio15 (Xue et al., 2024) to demonstrate the\ninternal mechanism of DiffWaveNetSVC (Zhang\net al., 2023). Specifically, it can demystify the\nentire diffusion process by extracting and render-\ning the hidden features of the diffusion model for\nsinging voice conversion. We present the screen-\nshot of the visualization system in Figure 5.\n15https://dsvc.openmmlab.org.cn/\nA.2\nInteractive Demo for DiffWaveNetSVC\nAmphion has supported various online interactive\ndemos for users to explore.16 For example, we\npre-train a DiffWaveNetSVC (Zhang et al., 2023)\nmodel which can conduct the singing voice con-\nversion among fifteen vocalists (as is shown in Ta-\nble 3) and also provide the interactive demo17. The\nscreenshot of it is presented in Figure 6.\n16https://huggingface.co/amphion\n17https://huggingface.co/spaces/amphion/singing_voice_conversion\n"
  },
  {
    "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
    "link": "https://arxiv.org/pdf/2312.09390.pdf",
    "upvote": "32",
    "text": "WEAK-TO-STRONG GENERALIZATION: ELICITING\nSTRONG CAPABILITIES WITH WEAK SUPERVISION\nCollin Burns\u2217\nPavel Izmailov\u2217\nJan Hendrik Kirchner\u2217\nBowen Baker\u2217\nLeo Gao\u2217\nLeopold Aschenbrenner\u2217\nYining Chen\u2217\nAdrien Ecoffet\u2217\nManas Joglekar\u2217\nJan Leike\nIlya Sutskever\nJeff Wu\u2217\nOpenAI\nABSTRACT\nWidely used alignment techniques, such as reinforcement learning from human\nfeedback (RLHF), rely on the ability of humans to supervise model behavior\u2014for\nexample, to evaluate whether a model faithfully followed instructions or generated\nsafe outputs. However, future superhuman models will behave in complex ways\ntoo difficult for humans to reliably evaluate; humans will only be able to weakly\nsupervise superhuman models. We study an analogy to this problem: can weak\nmodel supervision elicit the full capabilities of a much stronger model? We test\nthis using a range of pretrained language models in the GPT-4 family on natural\nlanguage processing (NLP), chess, and reward modeling tasks. We find that when\nwe naively finetune strong pretrained models on labels generated by a weak model,\nthey consistently perform better than their weak supervisors, a phenomenon we\ncall weak-to-strong generalization. However, we are still far from recovering the\nfull capabilities of strong models with naive finetuning alone, suggesting that tech-\nniques like RLHF may scale poorly to superhuman models without further work.\nWe find that simple methods can often significantly improve weak-to-strong gen-\neralization: for example, when finetuning GPT-4 with a GPT-2-level supervisor\nand an auxiliary confidence loss, we can recover close to GPT-3.5-level perfor-\nmance on NLP tasks. Our results suggest that it is feasible to make empirical\nprogress today on a fundamental challenge of aligning superhuman models.\n1\nINTRODUCTION\nWe mainly steer or align today\u2019s models with reinforcement learning from human feedback (RLHF):\nwe reinforce behaviors that human evaluators rate highly and penalize behaviors that evaluators rate\npoorly (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022; Glaese et al., 2022; Bai\net al., 2022a). This procedure is very effective when human evaluators can tell if model behavior is\ngood or bad and is a core part of training modern language model assistants such as ChatGPT.\nHowever, superhuman models will be capable of complex and creative behaviors that humans can-\nnot fully understand. For example, if a superhuman assistant model generates a million lines of ex-\ntremely complicated code, humans will not be able to provide reliable supervision for key alignment-\nrelevant tasks, including: whether the code follows the user\u2019s intentions, whether the assistant model\nanswers questions about the code honestly, whether the code is safe or dangerous to execute, and\nso on. As a result, if we finetune a superhuman model with human supervision on a reward mod-\neling (RM) or safety classification task, it is unclear how that model will generalize to complicated\nbehaviors that humans could not reliably supervise themselves.\nThis leads to a fundamental technical challenge of aligning superhuman models (superalignment):\nhow can weak supervisors control models much smarter than them? Despite the importance of\n\u2217Primary authors. This was a joint project of the Superalignment Generalization team. Correspondence to\ngeneralization@openai.com. Code is available at github.com/openai/weak-to-strong.\n1\narXiv:2312.09390v1  [cs.CL]  14 Dec 2023\nFigure 1: An illustration of our methodology. Traditional ML focuses on the setting where humans\nsupervise models that are weaker than humans. For the ultimate superalignment problem, humans\nwill have to supervise models much smarter than them. We study an analogous problem today:\nusing weak models to supervise strong models.\nthis problem, it is difficult to empirically study today. Most prior work on alignment has either\nconfronted this core challenge head-on\u2014but been restricted to primarily theoretical frameworks and\ntoy problems (Irving et al., 2018; Christiano et al., 2018; Leike et al., 2018; Demski & Garrabrant,\n2019; Hubinger et al., 2019), or empirically studied humans supervising today\u2019s models\u2014without\naddressing the core challenges that may arise with superhuman models (Christiano et al., 2017; Wu\net al., 2021; Ouyang et al., 2022; Bowman et al., 2022; Saunders et al., 2022). In contrast, we would\nideally like to have a setup that captures core challenges of aligning future superhuman models while\nalso being able to make iterative empirical progress today.\nWe propose a simple setup for studying the problem of humans supervising superhuman models by\nconsidering an analogy: can we use weak models to supervise strong models? We can empirically\ntest this by finetuning large (strong) pretrained models on labels generated by small (weak) mod-\nels and observing how they generalize. Just like the problem of humans supervising superhuman\nmodels, our setup is an instance of what we call the weak-to-strong learning problem.\nWhy should weak-to-strong learning be possible? On the one hand, the strong model could simply\nlearn to imitate the weak supervisor, including its errors, since that is what we would naively train\nit to do. On the other hand, strong pretrained models should already have good representations of\nthe alignment-relevant tasks we care about. For example, if a model can generate complicated code,\nthen it should intuitively also know whether that code faithfully adheres to the user\u2019s instructions.\nAs a result, for the purposes of alignment we do not need the weak supervisor to teach the strong\nmodel new capabilities; instead, we simply need the weak supervisor to elicit what the strong model\nalready knows. This gives us hope that the strong model can generalize beyond the weak supervision,\nsolving even hard problems for which the weak supervisor can only give incomplete or flawed\ntraining labels. We call this phenomenon weak-to-strong generalization.\nWe study our weak-to-strong learning setup (Section 3) by finetuning base (i.e. pretrained-only)\nlanguage models from the GPT-4 family (OpenAI, 2023),1 spanning 7 orders of magnitude (OOMs)\nof pretraining compute, across three settings: a large set of popular natural language processing\n(NLP) benchmarks, chess puzzles, and our internal ChatGPT reward modeling dataset. Our main\nfindings include:\n1These models share the same general architecture and pretraining dataset as GPT-4. However, this model\nseries does not include the models known as GPT-2, GPT-3, and GPT-3.5.\n2\naccuracy (%)\nRepresentative\nNLP Task\n50\n60\n70\n80\n90\n100\nChess Puzzles\n0\n20\n40\n60\n80\n100\nChatGPT\nReward Modeling\n50\n55\n60\n65\n70\n75\nweak\nperformance\nweak-to-strong\nperformance\n(naive)\nweak-to-strong\nperformance\n(best method)\nstrong ceiling\nperformance\n(g.t. supervision)\nFigure 2: Strong models trained with weak supervision generalize beyond their supervisor, and\nimproving weak-to-strong generalization is tractable. We show test accuracy on a representative\nNLP task (left), chess puzzles (middle) and the ChatGPT reward modeling task (right). We show the\nweak supervisor trained on ground truth labels (light grey) and the strong student trained with weak\nsupervision naively (green), with the best method in each setting (purple), or with ground truth\nsupervision (dark grey). For NLP and chess we supervise GPT-4 using GPT-2-level supervision,\nwhile for reward modeling we supervise a 3.5-level model using GPT-2-level supervision. The best\nmethod is the auxiliary confidence loss for the NLP task (Section 4.3.2), bootstrapping for Chess\npuzzles (Section 4.3.1), and unsupervised generative finetuning for reward modeling (Section 5.2.2;\ngenerative-finetuning is also used for the strong ceiling performance).\n1. Strong pretrained models naturally generalize beyond their weak supervisors. If we\nnaively finetune strong models with labels generated by weak models, they consistently\noutperform their weak supervisors (Section 4.2). For example, on NLP tasks, if we fine-\ntune GPT-4 with labels from a GPT-2-level model, we typically recover about half of the\nperformance gap between the two models.\n2. Naively finetuning on weak supervison is not enough. Despite positive weak-to-strong\ngeneralization, there still remains a substantial gap between strong models finetuned with\nweak supervision and strong models finetuned with ground truth supervision. Weak-to-\nstrong generalization is particularly poor for ChatGPT reward modeling. Collectively, our\nresults provide empirical evidence that naive RLHF will likely scale poorly to superhuman\nmodels without additional work.\n3. Improving weak-to-strong generalization is tractable. We find that we can improve per-\nformance by encouraging strong models to have confident predictions with an auxiliary\nloss, bootstrapping supervision with intermediate models, and improving model represen-\ntations with unsupervised finetuning. For example, when supervising GPT-4 with a GPT-2-\nlevel model on NLP tasks using the auxiliary confidence loss, we typically recover nearly\n80% of the performance gap between the weak and strong models.\nOur work has important limitations. None of our methods work consistently in all settings, and\nespecially in the RM setting we are still far from recovering the full performance gap between weak\nand strong models. Thus our methods serve more as proofs-of-concept that weak-to-strong gener-\nalization is tractable, rather than practical solutions we recommend deploying today. Furthermore,\nthere are still important disanalogies between our empirical setup and aligning superhuman models\nthat we did not address (Section 6); continuously refining our basic setup will be important for en-\nsuring that research today continues to make real progress toward aligning the superhuman models\nwe develop in the future.\nDespite the limitations of our work, we find our results to be highly encouraging. We show that sub-\nstantial weak-to-strong generalization is not only possible, but actually a widespread phenomenon.\nWe also show that with very simple methods, we can drastically improve the ability of weak super-\nvisors to elicit knowledge from strong models. With much more progress in this direction, we could\nget to the point where we can use weak supervisors to reliably elicit knowledge from much stronger\n3\nmodels, at least for some key tasks that we care about. This may allow us to develop superhuman\nreward models or safety classifiers, which we could in turn use to align superhuman models.\nAligning superhuman models is essential for making them safe; there is increasing recognition that\nfailing to align such powerful models has the potential to be catastrophic, making this one of the\nmost important unsolved technical problems in the world (CAIS). We think it is now more tractable\nthan ever to make rapid iterative empirical progress toward solving this problem.\n2\nRELATED WORK\nWe study how we can leverage the generalization properties of deep neural networks to solve weak-\nto-strong learning. Our problem setting and methods are closely connected to many existing research\nareas.\nWeakly-supervised learning.\nWeak-to-strong learning is a special type of weakly supervised\nlearning\u2014a setting in which models are trained using unreliable labels (Bach et al., 2017; Rat-\nner et al., 2017; Guo et al., 2018). There is also a rich literature on the related problem of learning\nfrom noisy labels (Song et al., 2022). Common methods include bootstrapping (Reed et al., 2014;\nHan et al., 2018; Li et al., 2020), noise-robust losses (Zhang & Sabuncu, 2018; Hendrycks et al.,\n2018; Ma et al., 2020), and noise modeling (Yi & Wu, 2019). Unlike most work on label noise, the\nerrors in our weak supervision are much harder to address than uniform label noise, instead having\n\u201cinstance-dependent\u201d errors (Fr\u00b4enay & Verleysen, 2013). Semi-supervised learning, in which la-\nbels are only available for a subset of the data, is also closely related (Kingma et al., 2014; Laine &\nAila, 2016; Berthelot et al., 2019). We could also study our problem in a semi-supervised setting by\nhaving an \u201ceasy\u201d subset of examples that weak supervisors provide reliable labels for and a subset\nof unlabeled \u201chard\u201d examples that the weak supervisor can\u2019t reliably label, a problem which we call\n\u201ceasy-to-hard generalization\u201d (see Appendix C).\nStudent-teacher training.\nThe framework of first training a teacher and then training a student on\nteacher\u2019s pseudo-labels is widely used in semi-supervised learning (Laine & Aila, 2016; Tarvainen\n& Valpola, 2017; Xie et al., 2020), domain adaptation (French et al., 2017; Shu et al., 2018), and\nknowledge distillation (Hinton et al., 2015; Gou et al., 2021; Stanton et al., 2021; Beyer et al., 2022).\nIn contrast to most prior work, we focus on the setting where the student is much more capable than\nthe teacher.\nFurlanello et al. (2018) and Xie et al. (2020) also consider cases where the student is at least as\ncapable as the teacher. However in their settings the student is randomly initialized and has access\nto ground truth labels. Moreover, compared to most past work we are focused on qualitatively very\nweak supervision. For example, we are interested in huge leaps in generalization, similar to going\nfrom \u201c3rd grade-level\u201d supervisors to \u201c12th grade-level\u201d student models. Despite these differences\nwith past work, we expect many methods from semi-supervised learning and domain adaptation to\ntranslate to our setting. For example, we found that a type of confidence auxiliary loss similar to\npast work (Grandvalet & Bengio, 2004) improves weak-to-strong generalization in Section 4.3.\nRobustness of pretraining and finetuning.\nMany papers have shown that pretraining\non massive, diverse data leads to more robust representations that generalize better out-of-\ndistribution (Hendrycks et al., 2019; 2020b; Radford et al., 2021; Liu et al., 2022). Finetuning typ-\nically improves in-distribution generalization, but often performs poorly out-of-distribution, some-\ntimes even degrading performance relative to zero-shot prompting (Kumar et al., 2022; Wortsman\net al., 2022b; Awadalla et al., 2022). Recent approaches to mitigating this problem include weight\nensembling (Wortsman et al., 2022b;a), finetuning only a subset of layers (Kirichenko et al., 2023;\nLee et al., 2022a), or mitigating the distortion effects that finetuning has on pretrained features (Ku-\nmar et al., 2022). We did not find strong results in preliminary explorations of approaches similar to\nthese (Appendix B), but we expect that with more thorough explorations one may be able to attain\nmuch stronger results with these or other ideas from the robust finetuning literature.\nDebiasing.\nIn weak-to-strong generalization, the weak labels contain a specific form of bias,\nwhich results from the weak models\u2019 lack of capability. There is a substantial literature on learning\nfrom biased training data (Bellamy et al., 2018). However, most work focuses on known biases,\nfor example where we know that the models perform worse on minority groups. For known biases,\ncommon methods include Group Distributionally Robust Optimization (Sagawa et al., 2019), adver-\n4\nsarial training (Zhang et al., 2018), and model editing (Santurkar et al., 2021; Meng et al., 2022).\nIn contrast, our setting can be viewed as a particularly difficult debiasing problem where the bias is\nunknown. Some methods that automatically discover and mitigate biases include clustering (Sohoni\net al., 2020), loss variance reduction (Khani et al., 2019), and auditing and re-training on high-loss\ngroup (Kim et al., 2019; Liu et al., 2021).\nImitation and preference learning.\nThe goal of alignment is to steer already-capable models\nto do what we want them to do. For example, the base GPT-4 model is good at generating text\nfollowing its pretraining distribution, but does not readily follow instructions. To align pretrained\nlanguage models today, we finetune them using imitation learning on human demonstrations (Bain\n& Sammut, 1995; Atkeson & Schaal, 1997) or by using methods such as reinforcement learning\nfrom human feedback (RLHF) (Christiano et al., 2017; Stiennon et al., 2020; Ouyang et al., 2022;\nGlaese et al., 2022; Bai et al., 2022a). Constitutional AI (Bai et al., 2022b; Lee et al., 2023) leverages\nAI feedback to align language models, but still uses an initial RLHF phase. However, both imitation\nlearning and preference learning assume high-quality human supervision, making it unclear if they\nwill work for superhuman models.\nScalable oversight.\nScalable oversight techniques aim to improve the ability of humans to super-\nvise models. For example, humans may ask models to critique the outputs of other models (Irving\net al., 2018; Saunders et al., 2022) or use models to help decompose a problem into simpler sub-\nproblems (Leike et al., 2018; Christiano et al., 2018; Lightman et al., 2023). Scalable oversight\nmethods typically take advantage of special problem structure, like decomposability or the fact that\nevaluation is easier than generation. In contrast to improving human supervision, we focus on\ngeneralizing beyond human supervision such that models perform well even in settings we cannot\nreliably supervise. That said, our weak-to-strong learning setup can be used to compare scalable\noversight methods, generalization-based methods, and more. Our setup also resembles a proposal\nfor measuring progress on scalable oversight known as \u201csandwiching\u201d, which uses weak and strong\nhumans (Cotra, 2021; Bowman, 2022).\nKnowledge elicitation and honesty.\nChristiano et al. (2022) introduced a theoretical problem\ncalled Eliciting Latent Knowledge (ELK), in which the goal is to elicit latent knowledge from a su-\nperhuman machine learning model even under worst case assumptions. For example, a special case\nof ELK is honesty (Evans et al., 2021), where the goal is for the models to report their true beliefs2.\nWentworth (2020) hypothesizes a tendency for neural networks to develop \u201cnatural abstractions\u201d\nthat are easier to elicit. Recent empirical work on ELK includes a benchmark for measurement\ntampering (Roger et al., 2023), methods for discovering latent knowledge (Burns et al., 2023), and\nstudies of honesty (Li et al., 2023; Pacchiardi et al., 2023). Our setting can be viewed as a general\nmethodology for empirically studying problems like ELK and honesty across a wide range of tasks.\n3\nMETHODOLOGY\nA core challenge of superalignment is that humans will need to supervise models much smarter than\nus. This is a special case of what we call the weak-to-strong learning problem: how can a weak\nsupervisor oversee a model much smarter than it? In this paper, we study a simple analogy, in which\nwe replace the weak human supervisor with a weak model supervisor.\nFor a given task of interest, consisting of a dataset and a performance metric, we:\n1. Create the weak supervisor. Throughout most of this work, we create weak supervisors\nby finetuning small pretrained models on ground truth labels.3 We call the performance\nof the weak supervisor the weak performance, and we generate weak labels by taking the\nweak model\u2019s predictions on a held-out set of examples.\n2. Train a strong student model with weak supervision. We finetune a strong model with\nthe generated weak labels. We call this model the strong student model and its resulting\nperformance the weak-to-strong performance.\n2Like Evans et al. (2021), we define honesty to mean a model reporting what it believes to be true, in contrast\nto truthfulness which asks whether what a model reports is true.\n3In Appendix D and Appendix E we study other synthetic weak supervisors. Future work could test many\nmore sources of weak supervision, such as by having 3rd grader humans provide labels.\n5\n3. Train a strong model with ground truth labels as a ceiling. Finally, for comparison, we\nfinetune a strong model with ground truth labels.4 We call this model\u2019s resulting perfor-\nmance the strong ceiling performance. Intuitively, this should correspond to \u201ceverything\nthe strong model knows,\u201d i.e. the strong model applying its full capabilities to the task.\nFor more details on how we train each model, see Appendix A.\nTypically, weak-to-strong performance will be between weak performance and strong ceiling per-\nformance. We define the performance gap recovered (PGR) as a function of the above three\nperformances (weak, weak-to-strong, and strong ceiling) as shown in the illustration below.\nweak\nperformance\nweak-to-strong\nperformance\nstrong ceiling\nperformance\nPGR =\nweak-to-strong\nweak\nstrong ceiling\nweak\n=\n-\n-\nPGR measures the fraction of the performance gap (the difference in performance between the weak\nand strong ceiling models) that we can recover with weak supervision. If we achieve perfect weak-\nto-strong generalization, PGR is 1. If the weak-to-strong model does no better than the weak super-\nvisor, then PGR is 0.\nAdvantages.\nOur setup has a number of advantages, including:\n1. It can be studied with any pair of weak and strong models, making it easy to study scaling\nlaws and not requiring access to expensive state-of-the-art models. Moreover, it does not\nrequire working with humans, so feedback loops are fast.\n2. It can be studied for any task of interest, making it easy to empirically test across a wide\nrange of settings.\n3. Success will be practically useful even before we develop superhuman models: for ex-\nample, if we find ways to align GPT-4 with only weak human supervision or with only\nGPT-3-level supervision, that would make it more convenient to align models today.\nLimitations.\nOur setup still has important disanalogies to the ultimate problem of aligning super-\nhuman models. We view our setup as removing one of the main disanalogies in prior work, not as\nproviding a final, perfectly analogous setup. Two remaining disanalogies include:\n1. Imitation saliency. Future superhuman models will likely have salient representations\nof human behaviors, but our strong models may not have learned features relevant for\nimitating weak model predictions; simply imitating the weak supervisor may thus be an\neasier failure mode to avoid in our setting than it will be in the future. More generally, the\ntypes of errors weak models make today may be different from the types of errors humans\nwill make when attempting to supervise superhuman models.\n2. Pretraining leakage. Our pretraining data implicitly contains supervision from humans.\nIt may thus be artificially easy to elicit strong models\u2019 capabilities in our setting, since they\nwere directly pretrained to observe strong (human-level) performance. Superhuman-level\nperformance may not be directly observed in the same way\u2014superhuman knowledge might\nbe more latent, e.g. because it was learned from self-supervised learning\u2014and thus might\nbe harder to elicit from superhuman models in the future.\n4For tasks solved by superhuman models that humans cannot evaluate, we will not have access to ground\ntruth labels. However, we allow access to ground truth labels in our experimental setting today for scientific\nand evaluation purposes. Note that we evaluated weak-to-strong performance against ground truth many times\nwhile iterating on methods; however, we held out our largest model (GPT-4) and about half of NLP tasks\nthroughout the project.\n6\nMore generally, we do not yet know how superhuman models will be built, but they could develop\nnew inductive biases that are qualitatively different from today\u2019s models. We view iterating on our\nmethodology to produce even more analogous setups as a key priority for future work, as we discuss\nin more detail in Section 6.\n4\nMAIN RESULTS\nIn this section, we report our main empirical results, including baselines and promising methods.\n4.1\nTASKS\nPopular natural language processing benchmarks.\nWe consider 22 popular NLP classification\ndatasets covering ethics, commonsense reasoning, natural language inference, sentiment analysis,\nand other domains. We convert all datasets to binary classification tasks and approximately balance\nthe classes. We produce soft labels from the weak model. See a full list of the datasets and their\nsources in Table 1.\nChess puzzles.\nWe use the dataset originally introduced in Schwarzschild et al. (2021b), which\ncontains chess puzzles from the lichess.org website (Lichess Team, 2023). Each puzzle con-\nsists of a chess position, and a sequence of optimal moves to play to solve the puzzle. For our\nevaluation, we predict the first move played, which is the best move in the given chess position. We\nillustrate the data format in Appendix Figure 14. For weak labels, we sample from the weak model\nwith temperature 0. Note that unlike the other binary classification tasks we study in this paper, this\nis a generative task.\nChatGPT reward modeling.\nThe standard approach to aligning models today is reinforcement\nlearning from human feedback (RLHF). A critical step of RLHF is to train a reward model (RM)\nto predict human preferences between model responses. Specifically, a reward model is trained\non a dataset consisting of dialogs between a human and an assistant model. For each query, the\nhumans compare multiple possible responses (completions) from the assistant, providing human\npreference data. Then, a reward model is trained to predict the results of pairwise comparisons\nbetween completions. Finally, the assistant model is trained by optimizing against the reward model\nwith reinforcement learning (RL). In our work, we do not study the RL step, and instead assume the\ngoal is to maximize reward model accuracy. For more details on reward models, see e.g. Ouyang\net al. (2022). We use a proprietary dataset used to train ChatGPT reward models.\nFor more details about our tasks and setup, see Appendix A.\n4.2\nNAIVELY FINETUNING ON WEAK LABELS\nIn each of these 3 settings (NLP tasks, chess puzzles, and reward modeling) we evaluate how well\nstrong students generalize when naively finetuned on labels generated by weak supervisors. We\nstudy pretrained language models from the GPT-4 family (OpenAI, 2023), which allow us to study\nstudent-supervisor compute disparities of many orders of magnitude. We find that PGRs are al-\nmost universally positive\u2014in virtually all settings that we studied, and across almost all student and\nsupervisor sizes, students outperform their supervisors (Figure 3).\nOn the popular NLP benchmarks, we find especially promising weak-to-strong generalization:\nstrong models trained with weak supervision can often generalize to a substantially higher perfor-\nmance than the weak model itself. Even with very weak supervisors and strong models with many\norders of magnitude more compute, we recover more than 20% of the performance gap. The PGR\nincreases both with weak supervisor size and with strong student size; for the largest students, the\nPGR is often above 50%.\nWe see more mixed results in the chess puzzle setting. In particular, when using the smallest weak\nmodels, the PGR is close to zero and the test accuracy curves appear flat. However, as the size of the\nweak supervisor increases, the PGR increases substantially; for small supervisor-student gaps, PGR\ncan be above 40%. Unlike in the NLP setting, where PGR improves with the strong student size,\nPGR decreases with the strong student size for a given weak supervisor on chess puzzles. The cor-\n7\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\nstrong ceiling performance\n(g.t. supervision)\nweak-to-strong performance\n(weak supervision)\n(c)\ntest accuracy (%)\nChatGPT\nReward Modeling\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\n(b)\n10-8\n10-6\n10-4\n10-2\n1\nChess Puzzles\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\n-20\n(e)\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\nChess Puzzles\n10-6\n(d)\nperformance gap recovered (%)\n0\n100\n20\n40\n60\n80\nNLP Tasks\nstrong student compute\n(fraction of GPT4)\n10-6\n10-4\n10-2\n1\n-20\n10-8\n10-6\n1\n72\n62\n60\n64\n66\n68\n70\n10-2\n10-4\nChatGPT\nReward Modeling\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\n-20\n(f)\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\n10-6\n(a)\n10-8\n10-6\n10-4\n10-2\n1\ntest accuracy (%)\n100\n50\n60\n70\n80\n90\nNLP Tasks\nFigure 3: Promising weak-to-strong generalization with naive finetuning on NLP tasks and\nchess, but poor generalization on the ChatGPT reward modeling task. (a,b,c) Test accuracy\nas a function of strong student size on (a) NLP tasks, (b) chess puzzles, and (c) the ChatGPT\nreward modeling task. Accuracy of strong students trained with ground truth in black, accuracy\nof strong students trained with weak supervision shown with colored lines (hue indicates size of\nweak supervisor). (d,e,f) Same as panels a,b,c but for performance gap recovered (see Section 3\nfor details). For NLP settings, we compute the median across tasks (see Figure 12 for full details).\nWe find decent weak-to-strong generalization and even positive PGR scaling on NLP tasks, decent\ngeneralization for small supervisor-student gaps but negative PGR scaling on chess puzzles, and\nboth poor generalization and scaling for ChatGPT reward modeling.\nresponding test accuracy curves appear concave, potentially exhibiting inverse scaling (McKenzie\net al., 2023) in strong student size.\nFinally, we find that weak-to-strong generalization is poor by default in the ChatGPT reward model\nsetting. We are usually only able to recover roughly 10% of the performance gap between the weak\nsupervisor and the strong student. Even for relatively small gaps in compute between the weak and\nstrong models, PGR almost never exceeds 20%.\nIn general, across all our settings, we observe weak-to-strong generalization: strong students consis-\ntently outperform their weak supervisors. It is not obvious why this should happen at all\u2014especially\nfrom naive finetuning alone\u2014and it gives us hope that weak-to-strong learning is a tractable prob-\nlem. At the same time, our results suggest that naively using weak, human-level supervision will be\ninsufficient to align strong, superhuman models; we will need qualitatively new techniques to solve\nsuperalignment.\n4.3\nIMPROVING WEAK-TO-STRONG GENERALIZATION IS TRACTABLE\nWe now show that we can use simple methods to substantially improve weak-to-strong generaliza-\ntion. While none of the methods we test works universally, these methods are proofs-of-concept that\nacross many different tasks we can substantially improve generalization.\n4.3.1\nBOOTSTRAPPING WITH INTERMEDIATE MODEL SIZES\nBootstrapping is a long-standing idea in alignment: instead of directly aligning very superhuman\nmodels, we could first align an only slightly superhuman model, use that to align an even smarter\nmodel, and so on (Christiano, 2019; 2018; Leike & Sutskever, 2023; Worley, 2021). Our setting\nallows us to empirically test this idea.\n8\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\n10-8\n10-6\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\nchess bootstrapping\nweak-to-strong performance\nwith bootstrapping\n(b)\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\n-10\n10-4 10-3 10-2\n10-1\n1\n10-5\nstrong student compute\n(fraction of GPT4)\n10-6\nweak model compute\n (fraction of GPT4) \n10-7\n0.1\n(a)\nFigure 4: Bootstrapping improves weak-to-strong generalization on chess puzzles. (a) Test\naccuracy as a function of strong student size. Accuracy of students trained with ground truth in\nblack, accuracy of students naively trained with weak supervision shown with dotted lines (hue\nindicates size of weak supervisor). Accuracies of students trained via bootstrapping shown with\ncolored squares (including both the final weak-to-strong performance and the performance of the\nintermediate models during bootstrapping). (b) Same as a with PGR. By taking multiple small steps\ninstead of one big step we see substantially improved generalization, especially for larger student\nmodels.\nSpecifically, we can construct a sequence of model sizes M1 \u2192 M2 \u2192 . . . \u2192 Mn of increasing\nsizes. Then, we use the weak labels from M1 to finetune M2, use M2 to generate new weak labels\nthat we can use to finetune the next model in the sequence, M3, and so on.\nWe evaluate bootstrapping in the chess puzzle setting. When we naively finetune on weak labels for\nchess (Section 4.2), we see high PGR when we cross small supervisor-student gaps, but low PGR\nfor larger gaps. As a result, in this setting it may help to take multiple small steps\u2014steps where\nPGR should be high\u2014instead of one big step.\nFor each round of bootstrapping, we run three iterations of weak-to-strong learning, i.e. we bootstrap\nthe weak supervision using two intermediate model sizes before finally finetuning the largest model\nin the sequence. We report the results (including all intermediate weak-to-strong models within\neach bootstrap) in Figure 4. Bootstrapping improves PGR compared to the baseline, especially for\nlarger student models. With the naive method, transfer accuracy curves flatten as the weak-strong\ngap grows larger; with bootstrapping, the accuracy continues to monotonically improve.\nWhile the results in the chess setting are promising, in preliminary experiments we observed only\nsmall improvements with bootstrapping on NLP tasks and no improvements in the RM setting.\nThis makes sense intuitively: unlike in the chess setting where naive PGR decreased with larger\nsupervisor-student gaps, naive PGR increased or was rougly constant for larger supervisor-student\ngaps in the NLP and reward modeling settings. Overall, these results suggest bootstrapping is a\nplausible avenue to investigate for improving weak-to-strong generalization and can be helpful in\nsome settings, but that naive bootstrapping alone will not be enough to align models much smarter\nthan their supervisors.\n4.3.2\nAN AUXILIARY CONFIDENCE LOSS CAN DRAMATICALLY IMPROVE GENERALIZATION\nON NLP TASKS\nIn our baseline results (Section 4.2), we naively finetune the strong student on the labels provided by\nthe weak supervisor. Because we are directly training the strong student to imitate the weak super-\nvisor, it may also learn to imitate the errors of the supervisor (see Section 5.1 for more discussion).\nIntuitively, we want to avoid this failure mode and provide additional regularization towards what\nthe strong pretrained model already internally knows: we want the student to learn the intent of the\nsupervisor, but not to imitate its mistakes.\n9\ntest accuracy (%)\n100\n50\n60\n70\n80\n90\nNLP with auxiliary loss\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\nweak-to-strong performance\nwith aux. loss\n(a)\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\n(b)\nperformance gap recovered (%)\n0\n100\n20\n40\n60\n80\nstrong student compute\n(fraction of GPT4)\n10-6\n10-4\n10-2\n1\n-20\nFigure 5: Substantially improved generalization on NLP datasets with a simple auxiliary loss.\n(a) Test accuracy as a function of strong student size. Accuracy of a student trained with ground\ntruth in black, accuracy of students naively trained with weak supervision shown with dotted lines.\nAccuracies of students trained with auxiliary confidence loss shown with colored triangles. Median\ncomputed across 22 NLP tasks (hue indicates size of weak supervisor), see Figure 6 for individual\ndatasets. (b) Same as a with PGR. The confidence loss can improve generalization drastically,\nespecially for large supervisor-student gaps.\nWe operationalize this intuition by adding an auxiliary confidence loss term to the standard cross\nentropy objective. This method is closely related to conditional entropy minimization (Grandvalet\n& Bengio, 2004) which is a prominent technique in semi-supervised learning. Specifically, we add\nan additional loss term which reinforces the strong model\u2019s confidence in its own predictions\u2014\neven when they disagree with the weak labels. We provide a detailed description of the method in\nAppendix A.4.\nIn Figure 5, we plot accuracy and PGR curves with this method on our NLP tasks. We find that\nwhile it performs slightly worse than the naive baseline for smaller strong students, it dramatically\nimproves generalization for large gaps in compute between weak and strong models. With the\nsmallest weak supervisor and largest strong student, the confidence loss increases median PGR from\nabout 25% to nearly 80%.\nIn addition, we also plot generalization curves for a representative subset of NLP datasets in Figure 6,\nas well as the full panel of datasets in Figure 12. There are some settings in which the confidence\nloss does not help much or degrades performance, e.g. when the gap between the weak supervisor\nand strong student is small or when the dataset features inverse scaling even with ground truth\nsupervision. But the confidence loss improves performance on most NLP datasets dramatically, and\nfor many datasets we get almost perfect generalization, recovering nearly all the performance of the\nstrong model, even when using the smallest weak supervisors.\nFinally, we find evidence consistent with our motivating intuition for the confidence loss (allowing\nthe strong student to confidently disagree with its weak supervisor): the auxiliary loss reduces the\nstrong student\u2019s imitation of weak errors and mitigates weak label overfitting (see Section 5.1).\n5\nUNDERSTANDING WEAK-TO-STRONG GENERALIZATION\nStrong methods will be essential for solving superalignment, but to trust those methods it is also\nimportant to understand when and why they work. A better understanding of weak-to-strong gener-\nalization could help us trust that generalization will continue working even in the future high-stakes\nsettings we care most about, and could help us develop better methods along the way. In this sec-\ntion, we study two phenomena relevant to weak-to-strong generalization: imitation of supervisor\nmistakes and salience of the tasks to the strong student model.\n10\n10-8\n10-6\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n10-8\n10-6\n10-4\n10-2\n1\n10-8\n10-6\n10-4\n10-2\n1\ntest accuracy (%)\nstrong performance\n(g.t. supervision)\nweak-to-strong\nperformance\n(weak supervision)\nweak-to-strong\nwith aux loss\n(weak supervision)\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\n60\n65\n70\n75\n80\n85\n90\n[9]\n50\n60\n70\n80\n90\n[6]\n50\n60\n70\n80\n90 [11]\n55\n60\n65\n70\n75\n80\n85\n90 [1]\n50\n60\n70\n80\n90\n[8]\n70\n75\n80\n85\n[4]\n60\n65\n70\n75\n80\n85\n90\n95 [12]\n60\n65\n70\n75\n80\n[10]\nFigure 6: Simple auxiliary loss improves generalization across most datasets. Test accuracy as\na function of strong student compute for a representative sample of NLP tasks. See Table 1 for\ndataset details and Appendix Figure 12 for results on all 22 NLP tasks. Auxiliary loss is shown with\ntriangles, and the baseline with dotted lines. Weak supervisor model size shown in varying colors,\nwith ground truth supervision shown in black.\n5.1\nUNDERSTANDING IMITATION\nWhen we train a strong model with weak supervision on some task, our hope is that the strong\nmodel will perform that desired task as well as possible, leveraging the latent capabilities it learned\nfrom pretraining to significantly outperform the weak supervisor. A salient way in which we could\nfail to achieve that desired generalization is if the strong model instead learns to imitate the weak\nsupervisor\u2014predicting how the weak supervisor would have classified each example. In particular,\nif the weak labels contain systematic errors that are easy to learn, the strong model could learn to\nimitate those errors. This is also a concern raised in theoretical work on superalignment, which has\nargued that the human simulator failure mode could be important: naive human supervision might\nresult in superhuman models learning to imitate what a human would say, rather outputting its best\npredictions (Christiano et al., 2022).\n5.1.1\nOVERFITTING TO WEAK SUPERVISION\nThe failure mode of imitating weak supervision is especially relevant to our naive baseline in Sec-\ntion 4.2, which directly trains the student to imitate the supervisor. In the case of infinite training\ndata, naively fitting the weak labels should result in perfect imitation, and a PGR of zero. In prac-\ntice, we train on finite data for a small number of epochs. Unlike typical ML settings, however, we\ncould expect to observe overfitting even when training for less than a single epoch: the strong model\nmight overfit to the weak supervisor labels and its errors, degrading ground truth test accuracy over\ntraining even without classic overfitting to any specific training examples.\nEmpirically, we see that the strong student indeed appears to overfit to the weak supervisor\u2019s errors.\nIn Figure 7(a) we show ground truth test accuracy curves over the course of training for the ChatGPT\nRM task, and in Figure 7(b) and (c) we compare the best5 and final ground truth test accuracies\n(median across all weak-strong model pairs). We find overfitting for large weak-strong gaps. For\nsmall weak-strong gaps, weak-to-strong performance typically monotonically increases over the\ncourse of training. For larger gaps, weak-to-strong performance often increases initially, but then\nstarts dropping well before a single epoch has elapsed. Ground truth early stopping, which \u201ccheats\u201d\n5Note that our best test accuracies may slightly overstate accuracy, due to noisy evaluations.\n11\n(a)\n0\n30\nperformance gap recovered (%)\nearly stop\n(ground truth)\nlast\n(c)\n0.2\n0.8\nprogress (fraction of epoch)\ntest accuracy (%)\n0.4\n61\nstrong student compute\n/ weak supervisor compute\n1\n107\n62\n63\n64\n65\n0.6\n10\n20\n10-8\n10-6\n1\n72\n62\n60\n64\n66\n68\n70\n10-2\n10-4\nChatGPT\nReward Modeling\n(b)\n10-7\n0.1\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\nearly stop\nweak-to-strong\nperformance\ntest accuracy (%)\nFigure 7: Strong models overfit to the weak labels. In all figures, we show data for the ChatGPT\nReward Modeling task. (a) Weak-to-strong performance over the course of training. Hues indicate\nthe student-supervisor gap. (b) Best weak-to-strong performance during training (stars) and weak-\nto-strong performance at the end of training (dashed). Weak performance in black. Hue indicates\nthe size of the weak supervisor. (c) Median best and final performance gap recovered (PGR) ag-\ngregated across all supervisor-student pairs. We see overfitting to weak labels for large weak-strong\ngaps, even within one epoch. In these cases, the best test accuracy achieved over training can be\nsubstantially better than the test accuracy at the end of training. See Figure 13 for the corresponding\nanalysis of a representative subset of NLP tasks.\nby evaluating against ground truth and stopping at an optimal step with respect to ground truth test\nlabels, typically gives a PGR improvement of around 5 percentage points.\nWe see the same phenomenon for NLP tasks in Figure 13. In the NLP setting, we find that \u201ccheating\u201d\nearly stopping on ground truth gives a 15 percentage point boost in PGR over the model at the end\nof training, and a 10 percentage point boost in PGR compared to \u201cnon-cheating\u201d early stopping with\nrespect to weak labels.\nUnfortunately, an early stopping criterion that uses ground truth labels does not constitute a valid\nmethod. Nevertheless, the results above suggest that imitating weak supervisor errors may be an\nimportant phenomenon in our setting.\nMoreover, these results suggest that better early stopping or regularization strategies may be able to\nsubstantially improve weak-to-strong generalization, by reducing overfitting to the weak labels and\ntheir errors. Indeed, we see in Figure 13 that the auxiliary confidence loss introduced in Section 4.3.2\nreduces overfitting to weak labels on NLP tasks substantially. For large weak-strong gaps, early\nstopping on ground truth (compared to early stopping on weak labels) gives a 15% PGR boost when\nusing the naive method, but only a roughly 5% PGR boost when using the confidence loss.\n5.1.2\nSTUDENT-SUPERVISOR AGREEMENT\nAnother way to measure imitation is to directly measure the agreement between the student and the\nsupervisor: the fraction of test inputs where the strong student makes the same prediction as the\nweak supervisor. Note that if agreement were 100%, then weak-to-strong accuracy would be equal\nto supervisor accuracy, and PGR would be 0.\nIn general, we notice that for our naive finetuning baseline, student-supervisor agreement is consis-\ntently high\u2014often noticeably higher than weak supervisor accuracy. This indicates that the student\nis imitating some of the supervisor\u2019s errors. These phenomena hold across all tasks (NLP tasks,\nchess, and reward modeling) and all model sizes, for the naive method.\nThe confidence loss in Section 4.3.2 reduces student-supervisor agreements significantly (Figure 8),\nprimarily by imitating supervisor mistakes less (Figure 8c). The loss encourages the strong student\nto make confident predictions, including when they contradict the weak supervisor. In a handful of\nthe settings where it is most successful, the confidence loss reduces student-supervisor agreement\n12\nweak-to-strong performance\n(aux. loss)\nweak-to-strong performance\nstudent-supervisor agreement (%)\n100\n60\n70\n80\n90\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n(a)\nall samples\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\n(c)\nstrong student compute\n(fraction of GPT4)\n10-6\n10-4\n10-2\n1\n10-8\nstudent-supervisor agreement (%)\n40\n100\n60\n80\nsupervisor mistakes\n(b)\nstrong student compute\n(fraction of GPT4)\n10-6\n10-4\n10-2\n1\n10-8\nstudent-supervisor agreement (%)\n40\n100\n60\n80\nNLP Task\nsupervisor correct\nFigure 8: Student-supervisor agreement decreases with larger student-supervisor gaps; the\nconfidence loss reduces imitation of supervisor mistakes. (a) Student-supervisor agreement as\na function of strong student size on NLP tasks, (b) a but only on samples where the supervisor is\ncorrect, (c) a but only on samples where the supervisor is mistaken. Dotted lines indicate naive\nfinetuning on weak labels, and triangles indicate results with the auxiliary confidence loss results\n(see Section 4.3). Hue of line indicates size of weak supervisor. For results on reward models, see\nFigure 16.\nbelow strong student test accuracy (weak-to-strong performance)\u2014i.e., the resulting model is fitting\nthe ground truth concept better than it is fitting the weak labels it was trained with.\n5.1.3\nINVERSE SCALING FOR IMITATING THE SUPERVISOR\nNext, we study student-supervisor agreement as a function strong model size (see Figure 8 and\nFigure 16). Surprisingly, we find inverse scaling (McKenzie et al., 2023): larger student models\nconsistently agree less with the errors of the supervisor than smaller student models, despite being\ntrained to imitate the supervisor, not using early stopping, and having larger capacity than smaller\nstudent models.\nThis trend is especially strong if we evaluate agreement only on datapoints where the supervisor is\nwrong (Figure 8c), and the trend persists if looking at cross entropy loss instead of accuracy.\nThese results suggest that pretrained models may have a hard time fitting errors of other (smaller)\npretrained models, at least in finetuning settings with relatively limited data. Stanton et al. (2021)\nand Furlanello et al. (2018) report a related observation in the context of knowledge distillation: it\nis surprisingly hard for models to fit the predictions of other models, even when they have sufficient\ncapacity to do so.\nOne natural hypothesis is that the nature of (especially naive) weak-to-strong generalization depends\nheavily on the error structure of the weak supervisors and how easy those errors are to imitate. In\nAppendix E, we show initial experiments that test how different types of weak supervision errors\nimpact what the strong student learns. Our results suggest that errors that are more difficult for the\nstudent to imitate result in stronger naive weak-to-strong generalization, but that even when they are\neasy to imitate, the confidence loss can help.\n5.2\nSALIENCY IN THE STRONG MODEL REPRESENTATIONS\nOne intuition for when weak-to-strong generalization might be feasible is when the task or con-\ncept we want to elicit is internally \u201csalient\u201d to the strong model. In this section, we study several\nphenomena related to the saliency of the concepts we are trying to elicit from the student model.\n5.2.1\nELICITING STRONG MODEL KNOWLEDGE WITH PROMPTING\nOne possible reason for the high PGR we observe in Section 4 could be that eliciting what the\nstrong model knows is easy. In particular, it is possible that strong pretrained models can solve\nmany relevant tasks zero-shot with a simple prompt.\nIn Figure 9a, we consider 7 representative NLP tasks and compare finetuning, zero-shot prompting,\nand 5-shot prompting; for this initial experiment, we use ground truth labels rather than weak labels\n13\nzeroshot\nfinetune g.t.\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\n(a)\nfewshot\nweak labels\n(n=5)\n(c)\naux. loss\nfinetune\ntest accuracy (%)\nstrong student compute\n(fraction of GPT4)\n10-5\n10-2\n0.1\n90\n50\n60\n70\n80\n10-3\n1\n10-4\n100\n(b)\nstrong student compute\n(fraction of GPT4)\n10-5\n10-2\n0.1\n90\n50\n60\n70\n80\n10-3\n1\n10-4\n100\nstrong student compute\n(fraction of GPT4)\n10-5\n10-2\n0.1\n90\n50\n60\n70\n80\n10-3\n1\n10-4\n100\nfewshot g.t.\n(n=5)\ntest accuracy (%)\ntest accuracy (%)\nFigure 9: Few-shot prompting becomes competitive with finetuning for large models; weak-to-\nstrong learning is qualitatively similar in the prompting setting. (a) Average zero-shot (single\ndashed), 5-shot (double dashed) and finetuning (solid) accuracy with ground truth labels as a func-\ntion of strong student size. (b) Average 5-shot with weak labels (colored dashed) accuracy as a\nfunction of student model size. Hue of line indicates size of weak supervisor. Zero-shot and 5-shot\nsame as in panel a. (c) Average weak-to-strong performance for 5-shot prompting (dashed with\ncrosses), naive finetuning (dashed thin) and finetuning with the confidence loss (solid with triangle)\nas a function of student model compute. Results are averaged across 7 NLP tasks. Few-shot weak-\nto-strong performance becomes competitive with or outperforms finetuning for the largest strong\nstudents, though finetuning with the confidence loss does better.\nfor finetuning and 5-shot. For both the zero-shot and 5-shot baseline we use task-specific prompts\nsummarized in Table 2. We find that zero-shot and 5-shot test accuracy is poor for most model sizes\nbut, consistent with Brown et al. (2020), improves drastically for larger model sizes. In particular, for\nthe largest models, 5-shot prompting becomes competitive with finetuning on many tasks, indicating\nthat eliciting the task-relevant knowledge of these very large models is relatively straightforward.\nWe are also interested in weak-to-strong learning in the context of few-shot prompting. To study\nthis setting, we construct a few-shot prompt where the labels are provided by the weak supervisor.\nWe report the results in Figure 9b. Consistent with our findings in the finetuning setting, we get\nworse performance when we few-shot prompt with weak labels than we do few-shot prompting\nwith ground truth labels. This suggests that weak-to-strong learning is a nontrivial problem in the\nprompting setting as well.\nSimilar to the finetuning setting, few-shot weak-to-strong performance improves for stronger su-\npervisors. Compared to our weak-to-strong finetuning baseline (Figure 9c), weak-to-strong perfor-\nmance of few-shot prompting is poor for smaller student models, but becomes competitive or even\noutperforms finetuning for the largest strong students. However, weak-to-strong finetuning with the\nconfidence loss still generally outperforms weak-to-strong few-shot prompting.\nOverall, these results provide an important reference for our results on weak-to-strong generaliza-\ntion. They suggest that for the largest model sizes, the knowledge needed to solve many task can\nbe elicited fairly easily with prompting. However, our current setup may be more disanalogous for\nprompting than for finetuning; many of our NLP tasks may have been implicitly observed during\npretraining, which we conjecture benefits prompting more than finetuning. We discuss this potential\ndisanalogy much more in Section 6.1.\n5.2.2\nGENERATIVE SUPERVISION IMPROVES RM WEAK-TO-STRONG GENERALIZATION\nIf salient representations of the desired task is useful for weak-to-strong generalization, then we may\nbe able to improve generalization by increasing the salience of the task to the strong model. One\nway to increase the salience of a task without needing ground truth labels is to perform unsupervised\nfinetuning with the language modeling objective on data relevant to that task (Dai & Le, 2015). For\nexample, by finetuning a language model in an unsupervised way on online reviews, sentiment\nbecomes saliently represented to models internally (Radford et al., 2017).\n14\nno generative\nfinetuning\n(b)\n10-4\n10-3\n10-2\n10-5\nstrong student compute\n(fraction of GPT4)\n10-6\nweak model compute\n (fraction of GPT4) \n10-7\n0.1\n(a)\n60\n62\n64\n66\n68\n70\n10-7\n10-6\n10-5\n10-4\nstrong student compute\n(fraction of GPT4)\n10-3\n10-2\ntest accuracy (%)\nwith generative\nfinetuning\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\nstrong ceiling performance\n(g.t. supervision)\nFigure 10: Generative finetuning on reward modeling data improves weak-to-strong perfor-\nmance and PGR. (a) Weak-to-strong performance on the reward modeling task, with (solid lines)\nand without (dashed lines) an extra step of generative finetuning for the strong student model. Solid\nblack line shows a strong ceiling reward model that was also trained with the generative finetuning\nstep; dashed black line show a weak supervisor reward model trained without the generative fine-\ntuning step. (b) PGR with and without generative finetuning. For generative finetuning PGR, we\nuse the strong ceiling performance that also had this extra generative finetuning step. Even with this\nceiling adjustment, PGR is higher with an extra generative finetuning step.\nWe test this idea in our reward modeling setting, where it is standard practice to initialize the model\nwith a baseline finetuned on demonstrations of desired behaviors (Stiennon et al., 2020). In our case,\nwe re-use the ChatGPT comparison data instead of introducing a new supervision dataset. Compar-\nisons are comprised of a prefix (a single request or conversation between the user and assistant) and\nat least two candidate completions. We finetune the base models with a language modeling loss on\nall prefix-completion pairs, ignoring the human preferences between those completions.\nNote that these pairs include completions ranked worst by human raters, so this procedure should not\nin principle leak any information about the ground truth preference labels that the weak-to-strong\nmodels should not have access to. On the other hand, since the completions can come from humans\nor stronger models, there may be some leakage similar in kind to the pretraining leakage that we\ndiscuss as a disanalogy in Section 6.1. Even in this setup, the reward modeling task is highly non-\ntrivial, and we leave addressing this disanalogy (e.g. by collecting completions only from weaker\nmodels) for future work.\nWe found that the additional generative finetuning on the RM data leads to better weak-to-strong\nperformance. Because this procedure also improves the performance of models trained on ground\ntruth RM data, we compare our new weak-to-strong performance to strong \u201cceiling\u201d models that\nwere also first generatively finetuned in the same way. Even with this adjusted ceiling, we find that\ngenerative supervision improves PGR by approximately 10-20%. We report the results in Figure 10.\nFurthermore, the improvement from generative finetuning stacks with the improvement from ground\ntruth early-stopping (a \u201ccheating\u201d method to illustrate potential performance if we could optimally\nearly stop, see Section 5.1.1). When we combine these two techniques, we can achieve PGR of\napproximately 30-40%, which would make the results on the RM task competitive with the weak-\nto-strong generalization we observe on NLP and chess puzzle tasks.\nWe can apply the idea of improving task saliency with generative finetuning on relevant data to all\nsettings, and we believe this could be a promising direction for future work.\n5.2.3\nFINETUNING ON WEAK SUPERVISION TO INCREASE CONCEPT SALIENCY\nOne possible measure of concept saliency is how linearly represented a task is. In particular, we can\nmeasure the performance of a linear probe (logistic regression classifier) trained from frozen activa-\ntions of the model. If the optimal solution can be approximately recovered with a linear probe, that\n15\nlp(gt)\nft(weak)\nft(weak)+lp(gt)\nft(gt)\n60\n90\ntest accuracy (%)\n70\n80\nlp(weak)\nFigure 11: Finetuning on weak supervisor labels makes the desired generalization more lin-\nearly represented. We plot test accuracy for five different strategies, averaged across a subset of\nNLP tasks. lp(weak): training a linear probe on the base model using weak labels, lp(gt): training a\nlinear probe on the base models using ground truth labels, ft(weak): finetuning the model on weak\nlabels, ft(weak) + lp(gt): finetuning the model on weak labels then training a linear probe on ground\ntruth labels, ft(gt): finetuning the model on ground truth labels. Finetuning on the weak labels sig-\nnificantly increases the linearity of the ground truth concept.\ncould simplify our problem greatly; we could focus on linear probing methods instead of finetuning\nmethods, which could greatly reduce the search space we need to consider to elicit the desired gen-\neralization. In our work, we focus only on how linearly represented a task is in the final activations,\nprior to the unembedding layer.\nIn Figure 11, we plot average test accuracy on a subset of our NLP datasets for several different\ncombinations of (1) finetuning or linear probing, using (2) weak or ground truth labels. First, we\nshow linear probes trained with ground truth labels (72% accuracy on average) perform worse than\nfinetuning with ground truth labels (82% on average), indicating that the optimal solution to most\ntasks is not represented completely linearly in the strong model\u2019s final activations. For comparison,\nwe also report the results for linear probing and finetuning using weak labels, which we verify are\nworse than using ground-truth labels.\nHowever, we find that we can achieve substantially better performance by first finetuning the model\non the weak labels, and then linear probing using the ground truth labels. In other words, when\nwe finetune the strong model with weak labels, the representations become more linear even with\nrespect to ground truth labels. In fact, finetuning on weak labels then linear probing on ground truth\nlabels results in an accuracy of 78%, closing 60% of the gap between ground truth linear probing\nand finetuning. This also noticeably outperforms the naive weak-to-strong finetuning baseline.\nThis phenomenon is closely related to a recent finding reported by Kirichenko et al. (2023) in the\nspurious cues literature. They find that finetuning a model on biased supervision can result in mod-\nels with very biased outputs, but surprisingly strong linear representations of the desired concepts.\nThese results suggest an alternative approach to improving weak-to-strong generalization. We could\nfirst \u201clinearize\u201d the desired concept, e.g. by naively finetuning on weak labels. Then we could use\nsimpler linear probe-based weak-to-strong methods to elicit the desired concept.\n6\nDISCUSSION\nIn this paper, we proposed a simple analogy for studying a core challenge of aligning superhuman\nmodels and showed that it is feasible to make significant progress on this problem. However, our\nsetup still has important disanalogies, which we now elaborate on. We then outline a number of\npromising avenues for future work.\n6.1\nREMAINING DISANALOGIES\nImitation saliency: superhuman models may easily imitate weak errors.\nFuture models will\nlikely be very good at predicting what humans will think and say, especially if they are trained\non human data in a similar manner to current models. Consequently, if we naively train such a\n16\nsuperhuman model with human supervision, it might simply imitate the weak supervisor, outputting\nhuman-level capabilities rather than its latent superhuman capabilities (Christiano et al., 2022).\nThis problem is only partially captured by our setup. While our strong pretrained models do imitate\nweak supervisors to some extent, they are not explicitly pretrained to imitate weak models, and our\nresults from Section 5.1.3 suggest that larger strong models may even have more difficulty doing this\nimitation. As such, \u201cimitating the weak supervisor\u201d may not be as much of a problem in our setup\nas it will be for the ultimate superalignment problem. This may inflate generalization performance\ntoday. We believe a more thorough investigation of this problem is an important area for future\nwork.\nPretraining leakage: superhuman knowledge may be latent, not observable.\nMany of the\ntasks we consider in this work may have been observed in pretraining at least indirectly, for exam-\nple through questions on online forums or through slight reframings of the task. For example, it is\nhighly likely that simple science questions similar to those in the SciQ NLP task are present in our\nGPT-4 series pretraining dataset at least implicitly in some form. However future superhuman mod-\nels may never directly observe superhuman alignment-relevant capabilities; these capabilities may\nbe predominantly \u201clatent\u201d, e.g. learned through self-supervised learning or reinforcement learning\nrather than through imitation learning. Intuitively, latent capabilities may be harder to elicit than\ncapabilities that models could have observed in their pretraining data.\nThis disanalogy could cause our results to be overly optimistic. We conjecture that this disanalogy\nalso increases prompting performance (Section 5.2.1) more than it increases finetuning performance;\nintuitively prompting may work especially well on tasks that the model assigns high probability to\nobserving. If so, this would make prompting more disanalogous in our setup than finetuning. We\nhope to test this conjecture in future work.\nIn Appendix D.1, we show a proof of concept that weak-to-strong generalization can still elicit latent\ncapabilities that were never explicitly observed during pretraining, and even when prompting is not\npossible. In particular, we use AlexNet (Krizhevsky et al., 2012) to supervise models pretrained with\nDINO (Caron et al., 2021), a self-supervised method in computer vision that learns strong represen-\ntations. We find that the strong student generalizes significantly beyond AlexNet\u2019s performance,\neven though the student never observed any classification labels during pretraining. Future work\nshould study and mitigate this pretraining leakage disanology more systematically.\n6.2\nFUTURE WORK\nWhat would convince us that we have a \u201csolution\u201d to superalignment? This is a complicated question\nand we do not claim to have a complete answer. However, we expect substantial progress in at least\nthe following three areas will be necessary: analogous setups, scalable methods, and strong scientific\nunderstanding. We now sketch out concrete problems for each of these areas.\n6.2.1\nCONCRETE PROBLEMS: ANALOGOUS SETUPS\nHaving strong measurements and a reliable methodology is extremely important for making empir-\nical progress in any field. In particular, it is important that we have metrics which provide strong\nsignal about whether we are making real progress toward the problem we ultimately care about.\nImportant directions for follow-up work include:\n\u2022 Making our setup more analogous by fixing the main remaining disanalogies described in\nSection 6.1. Analogous setups are essential to ensure that methods that work today will\ncontinue to work for superhuman models.\n\u2022 Validating that disanalogies are not severe, for example by checking that results are quali-\ntatively similar to using e.g. 3rd grade humans to supervise our strongest models today.\n\u2022 Relaxing some of the simplifications we made, e.g. by generalizing our methods and results\nto complicated generative tasks.\n\u2022 Testing how robust our weak-to-strong classifiers are to optimization pressure when we\nattain high PGR; for example, if we attain good weak-to-strong generalization with RMs,\ncan we optimize the learned RM using RL?\n17\n\u2022 Testing our conjecture that prompting-based methods in our current setup will not be as\nindicative of future results relative to finetuning-based methods (Section 5.2.1), and im-\nproving our setup to fix this.\n\u2022 Identifying new or more specific disanalogies with our setup and fixing them.\nAdditionally, we do not yet know what future models will look like. We should update our setup\nover time as we learn more about how broadly superhuman models will be built.\n6.2.2\nCONCRETE PROBLEMS: SCALABLE METHODS\nOne intuition for why major progress on weak-to-strong generalization seems possible is because\nall we need to do is extract everything the strong model already \u201cknows\u201d about the task of interest\u2014\nthe strong model should intuitively already understand the task, and should hopefully have salient\nrepresentations of that task. This suggests a number of properties that should be satisfied by the\ndesired generalization, and which we may be able to measure without access to ground truth.\n\u2022 The desired generalization should be able to disagree with the weak supervision when the\nweak supervision is wrong. This is a property our auxiliary confidence loss may capture.\n\u2022 The desired generalization should be \u201cnatural\u201d or \u201csalient\u201d to the model. For example, we\nshould not need to change the model too much to elicit the desired concept.\n\u2022 The desired generalization should be consistent. Consistency properties range anywhere\nfrom basic logical consistency to complicated forms of consistency between many prompts\n(e.g. cycle consistency, cross examination, etc.).\nFuture work should identify additional unsupervised properties that can be used to specify the de-\nsired generalization. More generally, there are very likely existing methods in the machine learning\nliterature (e.g. in semi-supervised learning or robust finetuning), which would be natural to try and\nwhich could also lead to substantial gains in weak-to-strong generalization. Generalization-based\napproaches to weak-to-strong learning are complementary to scalable oversight methods, in which\nthe weak supervisor interacts with the strong model to improve the quality of the weak supervision.\n6.2.3\nCONCRETE PROBLEMS: SCIENTIFIC UNDERSTANDING\nWe will need an extremely high degree of trust and reliability in our methods for aligning super-\nhuman models in high-stakes settings. We will not get this from strong benchmark performance\nalone. Instead, we also need a thorough understanding of precisely when and why our methods\nwork. Example questions of interest include:\n\u2022 What explains the difference between the relatively strong results on NLP datasets and the\nrelatively poor results with reward models when using naive finetuning?\n\u2022 What makes a concept easy or hard to elicit? What is a good definition of \u201csalience\u201d?\n\u2022 Can we reliably estimate generalization error at test time without any labels? For example,\ncan we measure the degree of weak-to-strong underspecification (Lee et al., 2022b)?\n\u2022 Can we reliably extrapolate generalization error across many orders of magnitude using\nscaling laws?\n\u2022 How important are the errors in the weak supervision, precisely? How do different kinds\nof weak label biases affect generalization?\n\u2022 How robust are our proposed methods to optimization pressure?\nIn Section 5 we only scratched the surface for understanding weak-to-strong generalization, but\nfuture work will need to go much further. An advantage of our setup is that it makes it easy to run\nsimple experiments to scientifically study generalization phenomena across a wide range of settings.\n6.3\nCONCLUSION\nRecent progress in AI has been faster than almost anyone anticipated (Steinhardt, 2022; Bengio\net al.). For an increasing number of researchers, the possibility of superhuman models being de-\nveloped this decade has become increasingly plausible. Broadly superhuman models would be\n18\nextraordinarily powerful and, if misused or misaligned with humans values, could potentially cause\ncatastrophic harm (CAIS). Given the stakes, we need to establish extremely high reliability in the\nalignment of these systems ahead of time. But for years it has been unclear how to empirically study\nsuperhuman model alignment. We believe it is now easier to make progress on this problem than\never before.\n7\nACKNOWLEDGEMENTS\nWe would like to thank Boaz Barak, Paul Christiano, Jacob Steinhardt, Ananya Kumar, Jakub Pa-\nchocki, John Schulman, Wojciech Zaremba, Alec Radford, Nat McAleese, and William Saunders\nfor valuable technical insights and discussions. We are grateful to Mia Glaese, Boaz Barak, Kush\nBhatia, Jean-Stanislas Denain, Erik Jones, Polina Kirichenko, Daniel Kokotajlo, Yoonho Lee, Jessy\nLin, Richard Ngo, John Schulman, Peter Tong, Fred Zhang, Ruiqi Zhong, Ryan Greenblatt, Fabien\nRoger, Paul Christiano, Steven Adler, Rai Pokorny, Adam Kalai, Jacob Hilton, Roger Grosse, Dan\nHendrycks, Alec Radford, and Scott Aaronson for helpful feedback on earlier drafts of this paper.\nWe also thank Shantanu Jain, Avital Oliver, Suchir Balaji, Cathy Yeh, and the Platform team for\ninfrastructure help. CB is also grateful to Dan Hendrycks, Jacob Steinhardt, and Paul Christiano for\nmany formative discussions over the years.\nREFERENCES\nEric Arazo, Diego Ortego, Paul Albert, Noel O\u2019Connor, and Kevin McGuinness. Unsupervised\nlabel noise modeling and loss correction. In International conference on machine learning, pp.\n312\u2013321. PMLR, 2019. (Cited on page 33)\nChristopher G Atkeson and Stefan Schaal. Robot learning from demonstration. In ICML, volume 97,\npp. 12\u201320. Citeseer, 1997. (Cited on page 5)\nAnas Awadalla, Mitchell Wortsman, Gabriel Ilharco, Sewon Min, Ian Magnusson, Hannaneh Ha-\njishirzi, and Ludwig Schmidt. Exploring The Landscape of Distributional Robustness for Ques-\ntion Answering Models. arXiv preprint arXiv:2210.12517, 2022. (Cited on page 4)\nStephen H Bach, Bryan He, Alexander Ratner, and Christopher R\u00b4e. Learning the structure of genera-\ntive models without labeled data. In International Conference on Machine Learning, pp. 273\u2013282.\nPMLR, 2017. (Cited on page 4)\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless\nassistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862,\n2022a. (Cited on page 1, 5)\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al.\nConstitutional AI:\nHarmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022b. (Cited on page 5, 47)\nMichael Bain and Claude Sammut. A Framework for Behavioural Cloning. In Machine Intelligence\n15, pp. 103\u2013129, 1995. (Cited on page 5)\nRachel KE Bellamy, Kuntal Dey, Michael Hind, Samuel C Hoffman, Stephanie Houde, Kalapriya\nKannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta, Aleksandra Mojsilovic, et al. AI Fair-\nness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic\nbias. arXiv preprint arXiv:1810.01943, 2018. (Cited on page 4)\nYoshua Bengio, Geoffrey Hinton, Andrew Yao, Dawn Song, Pieter Abbeel, Yuval Noah Harari, Ya-\nQin Zhang, Lan Xue, Shai Shalev-Shwartz, Gillian Hadfield, et al. Managing AI risks in an era\nof rapid progress. arXiv preprint arXiv:2310.17688. (Cited on page 18)\nDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A\nRaffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in neural informa-\ntion processing systems, 32, 2019. (Cited on page 4)\n19\nLucas Beyer, Xiaohua Zhai, Am\u00b4elie Royer, Larisa Markeeva, Rohan Anil, and Alexander\nKolesnikov. Knowledge distillation: A good teacher is patient and consistent. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pp. 10925\u201310934, 2022.\n(Cited on page 4)\nSteven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever,\nJan Leike, Jeff Wu, and William Saunders. Language models can explain neurons in language\nmodels. OpenAI Blog, 2023. (Cited on page 47)\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning\nabout Physical Commonsense in Natural Language. In Thirty-Fourth AAAI Conference on Artifi-\ncial Intelligence, 2020. (Cited on page 29)\nSam Bowman. Artificial Sandwiching: When can we test scalable alignment protocols without\nhumans? AI Alignment Forum, 2022. (Cited on page 5)\nSamuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile\nLukosuite, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable over-\nsight for large language models. arXiv preprint arXiv:2211.03540, 2022. (Cited on page 2, 47)\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n(Cited on page 14)\nCollin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt. Discovering Latent Knowledge in\nLanguage Models Without Supervision. In The Eleventh International Conference on Learning\nRepresentations, 2023. (Cited on page 5)\nCAIS. Statement on AI risk. (Cited on page 4, 19, 47)\nJoe Carlsmith. Scheming AIs: Will AIs fake alignment during training in order to get power? arXiv\npreprint arXiv:2311.08379. (Cited on page 48)\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe IEEE/CVF international conference on computer vision, pp. 9650\u20139660, 2021. (Cited on page\n17, 35, 40)\nJunbum Cha, Sanghyuk Chun, Kyungjae Lee, Han-Cheol Cho, Seunghyun Park, Yunsung Lee, and\nSungrae Park. Swad: Domain generalization by seeking flat minima. Advances in Neural Infor-\nmation Processing Systems, 34:22405\u201322418, 2021. (Cited on page 36)\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton.\nBig\nself-supervised models are strong semi-supervised learners. Advances in neural information pro-\ncessing systems, 33:22243\u201322255, 2020a. (Cited on page 35)\nYining Chen, Colin Wei, Ananya Kumar, and Tengyu Ma.\nSelf-training avoids using spurious\nfeatures under domain shift. Advances in Neural Information Processing Systems, 33:21061\u2013\n21071, 2020b. (Cited on page 33)\nPaul Christiano. Approval-directed bootstrapping. AI Alignment Forum, 2018. (Cited on page 8)\nPaul Christiano. Capability amplification. AI Alignment Forum, 2019. (Cited on page 8)\nPaul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep rein-\nforcement learning from human preferences. Advances in neural information processing systems,\n30, 2017. (Cited on page 1, 2, 5, 47)\nPaul Christiano, Buck Shlegeris, and Dario Amodei. Supervising strong learners by amplifying\nweak experts. arXiv preprint arXiv:1810.08575, 2018. (Cited on page 2, 5)\nPaul Christiano, Ajeya Cotra, and Mark Xu. Eliciting latent knowledge. Technical report, Alignment\nResearch Center (ARC), 2022. (Cited on page 5, 11, 17, 44)\n20\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions. In NAACL,\n2019. (Cited on page 29)\nAjeya Cotra. The case for aligning narrowly superhuman models. AI Alignment Forum, 2021. (Cited\non page 5)\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. Advances in neural information\nprocessing systems, 28, 2015. (Cited on page 14)\nAbram Demski and Scott Garrabrant. Embedded agency. arXiv preprint arXiv:1902.09469, 2019.\n(Cited on page 2)\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. (Cited on page 40)\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben Mann,\nAmanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep\nGanguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt,\nKamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and\nChris Olah. A Mathematical Framework for Transformer Circuits. Transformer Circuits Thread,\n2021. https://transformer-circuits.pub/2021/framework/index.html. (Cited on page 42)\nOwain Evans, Owen Cotton-Barratt, Lukas Finnveden, Adam Bales, Avital Balwit, Peter Wills,\nLuca Righetti, and William Saunders. Truthful AI: Developing and governing AI that does not\nlie. arXiv preprint arXiv:2110.06674, 2021. (Cited on page 5)\nGeoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adapta-\ntion. arXiv preprint arXiv:1706.05208, 2017. (Cited on page 4)\nBeno\u02c6\u0131t Fr\u00b4enay and Michel Verleysen. Classification in the presence of label noise: a survey. IEEE\ntransactions on neural networks and learning systems, 25(5):845\u2013869, 2013. (Cited on page 4)\nTommaso Furlanello, Zachary Lipton, Michael Tschannen, Laurent Itti, and Anima Anandkumar.\nBorn again neural networks. In International Conference on Machine Learning, pp. 1607\u20131616.\nPMLR, 2018. (Cited on page 4, 13)\nAmelia Glaese, Nat McAleese, Maja Trebacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth\nRauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, et al. Improving alignment of dia-\nlogue agents via targeted human judgements. arXiv preprint arXiv:2209.14375, 2022. (Cited on\npage 1, 5)\nJianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A\nsurvey. International Journal of Computer Vision, 129:1789\u20131819, 2021. (Cited on page 4)\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. Advances\nin neural information processing systems, 17, 2004. (Cited on page 4, 10, 33, 34)\nSheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R. Scott, and\nDinglong Huang. CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images.\nIn Proceedings of the European Conference on Computer Vision (ECCV), 2018. (Cited on page\n4)\nBo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi\nSugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels.\nAdvances in neural information processing systems, 31, 2018. (Cited on page 4)\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016. (Cited on page 40)\n21\nDan Hendrycks, Mantas Mazeika, Duncan Wilson, and Kevin Gimpel. Using trusted data to train\ndeep networks on labels corrupted by severe noise. Advances in neural information processing\nsystems, 31, 2018. (Cited on page 4)\nDan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness\nand uncertainty. In International conference on machine learning, pp. 2712\u20132721. PMLR, 2019.\n(Cited on page 4)\nDan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning AI with shared human values. arXiv preprint arXiv:2008.02275, 2020a.\n(Cited on page 29)\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and Dawn Song.\nPretrained transformers improve out-of-distribution robustness. arXiv preprint arXiv:2004.06100,\n2020b. (Cited on page 4)\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring Mathematical Problem Solving With the MATH Dataset. Sort,\n2(4):0\u20136, 2021. (Cited on page 40)\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015. (Cited on page 4)\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International\nConference on Learning Representations, 2022. (Cited on page 35)\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Cosmos QA: Machine reading\ncomprehension with contextual commonsense reasoning. arXiv preprint arXiv:1909.00277, 2019.\n(Cited on page 29)\nEvan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott Garrabrant. Risks from\nlearned optimization in advanced machine learning systems. arXiv preprint arXiv:1906.01820,\n2019. (Cited on page 2, 48)\nGeoffrey Irving, Paul Christiano, and Dario Amodei.\nAI safety via debate.\narXiv preprint\narXiv:1805.00899, 2018. (Cited on page 2, 5)\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wil-\nson.\nAveraging weights leads to wider optima and better generalization.\narXiv preprint\narXiv:1803.05407, 2018. (Cited on page 36)\nFereshte Khani, Aditi Raghunathan, and Percy Liang. Maximum weighted loss discrepancy. arXiv\npreprint arXiv:1906.03518, 2019. (Cited on page 5)\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth.\nLook-\ning beyond the surface: A challenge set for reading comprehension over multiple sentences. In\nProceedings of the 2018 Conference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pp. 252\u2013262,\n2018. (Cited on page 29)\nMichael P Kim, Amirata Ghorbani, and James Zou. Multiaccuracy: Black-box post-processing for\nfairness in classification. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and\nSociety, pp. 247\u2013254, 2019. (Cited on page 5)\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014. (Cited on page 40, 41)\nDurk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised\nlearning with deep generative models. Advances in neural information processing systems, 27,\n2014. (Cited on page 4)\n22\nPolina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last Layer Re-Training is Suf-\nficient for Robustness to Spurious Correlations. In The Eleventh International Conference on\nLearning Representations, 2023. (Cited on page 4, 16)\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolu-\ntional neural networks. Advances in neural information processing systems, 25, 2012. (Cited on\npage 17, 40)\nAnders Krogh and John Hertz. A simple weight decay can improve generalization. Advances in\nneural information processing systems, 4, 1991. (Cited on page 35)\nAnanya Kumar, Aditi Raghunathan, Robbie Matthew Jones, Tengyu Ma, and Percy Liang. Fine-\nTuning can Distort Pretrained Features and Underperform Out-of-Distribution. In International\nConference on Learning Representations, 2022. (Cited on page 4, 35)\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. arXiv preprint\narXiv:1610.02242, 2016. (Cited on page 4)\nDong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for\ndeep neural networks. In Workshop on challenges in representation learning, ICML, volume 3,\npp. 896. Atlanta, 2013. (Cited on page 33)\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor\nCarbune, and Abhinav Rastogi. Rlaif: Scaling reinforcement learning from human feedback with\nAI feedback. arXiv preprint arXiv:2309.00267, 2023. (Cited on page 5)\nYoonho Lee, Annie S Chen, Fahim Tajwar, Ananya Kumar, Huaxiu Yao, Percy Liang, and Chelsea\nFinn. Surgical Fine-Tuning Improves Adaptation to Distribution Shifts. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022a. (Cited on page 4)\nYoonho Lee, Huaxiu Yao, and Chelsea Finn. Diversify and disambiguate: Learning from under-\nspecified data. arXiv preprint arXiv:2202.03418, 2022b. (Cited on page 18)\nJan Leike and Ilya Sutskever. Introducing Superalignment. OpenAI Blog, 2023. (Cited on page 8,\n47)\nJan Leike, David Krueger, Tom Everitt, Miljan Martic, Vishal Maini, and Shane Legg. Scalable\nagent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871,\n2018. (Cited on page 2, 5)\nJunnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-\nsupervised learning. arXiv preprint arXiv:2002.07394, 2020. (Cited on page 4)\nKenneth Li, Oam Patel, Fernanda Vi\u00b4egas, Hanspeter Pfister, and Martin Wattenberg. Inference-\nTime Intervention:\nEliciting Truthful Answers from a Language Model.\narXiv preprint\narXiv:2306.03341, 2023. (Cited on page 5, 47)\nLichess Team. Lichess Database. https://github.com/lichess-org/database, 2023.\nAccessed: 2023. (Cited on page 7)\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s Verify Step by Step. arXiv preprint\narXiv:2305.20050, 2023. (Cited on page 5)\nEvan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori Sagawa,\nPercy Liang, and Chelsea Finn. Just train twice: Improving group robustness without training\ngroup information. In International Conference on Machine Learning, pp. 6781\u20136792. PMLR,\n2021. (Cited on page 5)\nZiquan Liu, Yi Xu, Yuanhong Xu, Qi Qian, Hao Li, Rong Jin, Xiangyang Ji, and Antoni B Chan.\nAn empirical study on distribution shift robustness from the perspective of pre-training and data\naugmentation. arXiv preprint arXiv:2205.12753, 2022. (Cited on page 4)\n23\nXingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Nor-\nmalized loss functions for deep learning with noisy labels. In International conference on machine\nlearning, pp. 6543\u20136553. PMLR, 2020. (Cited on page 4)\nIan R McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,\nEuan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, et al. Inverse Scaling: When Bigger Isn\u2019t\nBetter. arXiv preprint arXiv:2306.09479, 2023. (Cited on page 8, 13)\nKevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov.\nLocating and editing factual\nassociations in GPT.\nAdvances in Neural Information Processing Systems, 35:17359\u201317372,\n2022. (Cited on page 5)\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a Suit of Armor Conduct\nElectricity? A New Dataset for Open Book Question Answering. In EMNLP, 2018. (Cited on\npage 29)\nRichard Ngo, Lawrence Chan, and S\u00a8oren Mindermann. The alignment problem from a deep learning\nperspective. arXiv preprint arXiv:2209.00626, 2022. (Cited on page 48)\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversar-\nial NLI: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599,\n2019. (Cited on page 29)\nChris Olah, Arvind Satyanarayan, Ian Johnson, Shan Carter, Ludwig Schubert, Katherine\nYe, and Alexander Mordvintsev.\nThe Building Blocks of Interpretability.\nDistill, 2018.\nhttps://distill.pub/2018/building-blocks. (Cited on page 47)\nOpenAI. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774, 2023. (Cited on page 2, 7, 28)\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. (Cited on page 1, 2, 5, 7, 32, 47)\nLorenzo Pacchiardi, Alex J Chan, S\u00a8oren Mindermann, Ilan Moscovitz, Alexa Y Pan, Yarin Gal,\nOwain Evans, and Jan Brauner. How to catch an AI liar: Lie detection in black-box llms by\nasking unrelated questions. arXiv preprint arXiv:2309.15840, 2023. (Cited on page 5)\nFabian Pedregosa, Ga\u00a8el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas,\nAlexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and \u00b4Edouard Duch-\nesnay. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12\n(85):2825\u20132830, 2011. (Cited on page 42)\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai, Roman Ring, John Aslanides, Amelia\nGlaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language models.\narXiv preprint arXiv:2202.03286, 2022a. (Cited on page 47)\nEthan Perez, Sam Ringer, Kamil\u02d9e Luko\u02c7si\u00afut\u02d9e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig\nPettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al. Discovering language model\nbehaviors with model-written evaluations. arXiv preprint arXiv:2212.09251, 2022b. (Cited on\npage 47)\nMohammad Taher Pilehvar and Jose Camacho-Collados. WiC: the word-in-context dataset for eval-\nuating context-sensitive meaning representations. arXiv preprint arXiv:1808.09121, 2018. (Cited\non page 29)\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444, 2017. (Cited on page 14)\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021. (Cited on page 4)\n24\nAlexander Ratner, Stephen H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and Christopher R\u00b4e.\nSnorkel: Rapid training data creation with weak supervision. In Proceedings of the VLDB En-\ndowment. International Conference on Very Large Data Bases, volume 11, pp. 269. NIH Public\nAccess, 2017. (Cited on page 4)\nScott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew\nRabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint\narXiv:1412.6596, 2014. (Cited on page 4, 33)\nMarco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201d Why should i trust you?\u201d Explaining the\npredictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference\non knowledge discovery and data mining, pp. 1135\u20131144, 2016. (Cited on page 47)\nFabien Roger, Ryan Greenblatt, Max Nadeau, Buck Shlegeris, and Nate Thomas. Measurement\ntampering detection benchmark. arXiv preprint arXiv:2308.15605, 2023. (Cited on page 5)\nAnna Rogers, Olga Kovaleva, Matthew Downey, and Anna Rumshisky. Getting closer to AI com-\nplete question answering: A set of prerequisite real tasks. In Proceedings of the AAAI conference\non artificial intelligence, volume 34, pp. 8722\u20138731, 2020. (Cited on page 29)\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng\nHuang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual\nrecognition challenge. International journal of computer vision, 115:211\u2013252, 2015. (Cited on\npage 40)\nShiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust\nneural networks for group shifts: On the importance of regularization for worst-case generaliza-\ntion. arXiv preprint arXiv:1911.08731, 2019. (Cited on page 4)\nShibani Santurkar, Dimitris Tsipras, Mahalaxmi Elango, David Bau, Antonio Torralba, and Alek-\nsander Madry. Editing a classifier by rewriting its prediction rules. Advances in Neural Informa-\ntion Processing Systems, 34:23359\u201323373, 2021. (Cited on page 5)\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le Bras, and Yejin Choi. Socialiqa: Common-\nsense reasoning about social interactions. arXiv preprint arXiv:1904.09728, 2019. (Cited on page\n29)\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan\nLeike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802,\n2022. (Cited on page 2, 5, 47)\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Arpit Bansal, Zeyad Emam, Furong Huang, Micah\nGoldblum, and Tom Goldstein. Datasets for studying generalization from easy to hard examples.\narXiv preprint arXiv:2108.06011, 2021a. (Cited on page 29)\nAvi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,\nand Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with\nrecurrent networks. Advances in Neural Information Processing Systems, 34:6695\u20136706, 2021b.\n(Cited on page 7, 29)\nRui Shu, Hung Bui, Hirokazu Narui, and Stefano Ermon. A DIRT-T Approach to Unsupervised\nDomain Adaptation. In International Conference on Learning Representations, 2018. (Cited on\npage 4, 33)\nRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on empirical methods in natural language pro-\ncessing, pp. 1631\u20131642, 2013. (Cited on page 29)\nNimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\u00b4e. No subclass left\nbehind: Fine-grained robustness in coarse-grained classification problems. Advances in Neural\nInformation Processing Systems, 33:19339\u201319352, 2020. (Cited on page 5)\n25\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy\nlabels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning\nSystems, 2022. (Cited on page 4)\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from overfitting. The journal of machine\nlearning research, 15(1):1929\u20131958, 2014. (Cited on page 35)\nSamuel Stanton, Pavel Izmailov, Polina Kirichenko, Alexander A Alemi, and Andrew G Wilson.\nDoes knowledge distillation really work? Advances in Neural Information Processing Systems,\n34:6906\u20136919, 2021. (Cited on page 4, 13)\nJacob Steinhardt. AI Forecasting: One Year In, 2022. (Cited on page 18)\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020. (Cited on page 1, 5, 15, 32)\nKai Sun, Dian Yu, Jianshu Chen, Dong Yu, Yejin Choi, and Claire Cardie. Dream: A challenge data\nset and models for dialogue-based reading comprehension. Transactions of the Association for\nComputational Linguistics, 7:217\u2013231, 2019. (Cited on page 29)\nOyvind Tafjord, Matt Gardner, Kevin Lin, and Peter Clark. Quartz: An open-domain dataset of\nqualitative relationship questions. arXiv preprint arXiv:1909.03553, 2019. (Cited on page 29)\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged con-\nsistency targets improve semi-supervised deep learning results. Advances in neural information\nprocessing systems, 30, 2017. (Cited on page 4)\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018. (Cited on page 29)\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman.\nSuperglue: A stickier benchmark for general-purpose language\nunderstanding systems. Advances in neural information processing systems, 32, 2019. (Cited on\npage 29)\nAlex Warstadt, Amanpreet Singh, and Samuel Bowman. Neural network acceptability judgments.\nTransactions of the Association for Computational Linguistics, 7:625\u2013641, 2019. (Cited on page\n29)\nColin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical Analysis of Self-Training with\nDeep Networks on Unlabeled Data. In International Conference on Learning Representations,\n2020. (Cited on page 33)\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In Inter-\nnational Conference on Learning Representations, 2021. (Cited on page 28, 29)\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language\nmodels. arXiv preprint arXiv:2206.07682, 2022. (Cited on page 46)\nJohannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209, 2017. (Cited on page 29)\nJohn Wentworth. Alignment by Default. AI Alignment Forum, 2020. (Cited on page 5)\nGordon Seidoh Worley. Bootstrapped Alignment. AI Alignment Forum, 2021. (Cited on page 8)\n26\nMitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model\nsoups: averaging weights of multiple fine-tuned models improves accuracy without increasing\ninference time. In International Conference on Machine Learning, pp. 23965\u201323998. PMLR,\n2022a. (Cited on page 4, 36)\nMitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust\nfine-tuning of zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 7959\u20137971, 2022b. (Cited on page 4, 36)\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Chris-\ntiano. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862,\n2021. (Cited on page 2)\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student\nimproves imagenet classification. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 10687\u201310698, 2020. (Cited on page 4, 33)\nKun Yi and Jianxin Wu. Probabilistic End-To-End Noise Correction for Learning With Noisy La-\nbels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), 2019. (Cited on page 4)\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can a Ma-\nchine Really Finish Your Sentence? In Proceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, 2019. (Cited on page 29)\nBrian Hu Zhang, Blake Lemoine, and Margaret Mitchell. Mitigating unwanted biases with adver-\nsarial learning. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, pp.\n335\u2013340, 2018. (Cited on page 5)\nYuan Zhang, Jason Baldridge, and Luheng He. PAWS: Paraphrase Adversaries from Word Scram-\nbling. In Proc. of NAACL, 2019. (Cited on page 29)\nZhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks\nwith noisy labels. Advances in neural information processing systems, 31, 2018. (Cited on page\n4, 36)\nBen Zhou, Daniel Khashabi, Qiang Ning, and Dan Roth. \u201cGoing on a vacation\u201d takes longer than\n\u201cGoing for a walk\u201d: A Study of Temporal Commonsense Understanding. In EMNLP, 2019.\n(Cited on page 29)\n27\nAPPENDIX OUTLINE\n\u2022 In Appendix A, we provide additional details on our setup and experiments.\n\u2022 In Appendix B, we describe additional results, including negative results and methods that\ndid not work well in our experiments.\n\u2022 In Appendix C, we report results on easy-to-hard generalization, where we only provide\nsupervision on easy examples.\n\u2022 In Appendix D, we provide results in two more weak-to-strong learning settings: a self-\nsupervised computer vision setting on ImageNet, and a pure linear probing setting.\n\u2022 In Appendix E, we provide additional results and discussion on the effect of weak supervi-\nsor error simulation.\n\u2022 In Appendix F, we discuss how we believe methodological progress should be made on\nsuperalignment.\n\u2022 In Appendix G, we describe how our work fits into the bigger picture of alignment.\nA\nFURTHER EXPERIMENTAL DETAILS\nHere, we provide further details on our experiments. Across all tasks, we use pretrained base models\nfrom the GPT-4 family (OpenAI, 2023), spanning a range of model sizes.\nA.1\nNLP TASKS\nData preprocessing.\nWe use popular NLP classification benchmark datasets listed in Table 1. We\nobfuscate the names of the datasets in our plots (e.g. Figure 12) for confidentiality; across all figures,\nwe replace the names of the datasets with their order in a randomized sequence. We apply various\npreprocessing to the datasets. For example, some tasks are in FLAN (Wei et al., 2021) and we use\ntheir preprocessing. For ANLI we group neutral entailments with contradictions. We convert each\ndataset to a binary classification problem. For multiple-choice datasets, suppose each datapoint has\na question Q and multiple candidate answers A1, . . . , Ak. We then convert this datapoint to k new\ndatapoints of the form (Q, Ai), where the label is 0 for all incorrect answers Ai and 1 for the correct\nanswers. In this procedure, we also aim to maintain class balance, so we keep the same number\nof correct and wrong answers per question6. We are also additionally rebalancing the classes in\ndatasets where one of the classes represents more than 55% of the data. To do so, we randomly drop\ndatapoints from the dominant class, so that the classes are perfectly balanced.\nModels.\nIn order to adapt our language models to the classification setting, we replace the un-\nembedding layer of the model with a linear classification head with two outputs. We initialize the\nweights of the classification head with the unembedding weights for tokens \u201c0\u201d and \u201c1\u201d.\nTraining hyperparameters.\nWe finetune all models for 2 epochs using a batch size of 32. In\nthe weak-to-strong generalization experiments, we early stop training based on the accuracy with\nrespect to the weak labels on a held-out validation set. See Section 5.1.1 for relevant discussion.\nWe only tuned the hyper-parameters of our methods on smaller model sizes, and on a subset of 8\ndatasets. The full GPT-4 model and most of the datasets were held-out, except for datasets [5\u201312]\n(see Figure 12).\nWeak labels.\nTo produce the weak labels, we split the original dataset in half. We ensure that\nrelated datapoints, e.g. datapoints that share the same question or premise, are always grouped to-\ngether into the same half. Then, we train the weak supervisor model on the first half of the dataset,\nand use its prediction on the other half as the weak labels. We additionally save the weak labels on\nthe test set to evaluate metrics such as agreement in Section 5.1.3. The weak labels are soft labels\non the training data, i.e. the class probabilities predicted by the supervisor.\nEvaluation.\nFor all datasets, we report accuracy on the test set which is also balanced to have an\nequal number of datapoints in each class. In particular, random guess performance corresponds to\n50% accuracy on all NLP datasets.\n6In some datasets there are multiple correct answers for each question.\n28\nTable 1: Datasets and their sources.\nWe summarize the NLP datasets we use and their original\nsources.\nDataset\nOriginal Source\nBoolQ\nClark et al. (2019)\nCosmosQA\nHuang et al. (2019)\nDREAM\nSun et al. (2019)\nETHICS [Justice]\nHendrycks et al. (2020a)\nETHICS [Deontology]\nHendrycks et al. (2020a)\nETHICS [Virtue]\nHendrycks et al. (2020a)\nETHICS [Utilitarianism]\nHendrycks et al. (2020a)\nFLAN ANLI R2\nNie et al. (2019); Wei et al. (2021)\nGLUE CoLA\nWarstadt et al. (2019); Wang et al. (2018)\nGLUE SST-2\nSocher et al. (2013); Wang et al. (2018)\nHellaSwag\nZellers et al. (2019)\nMCTACO\nZhou et al. (2019)\nOpenBookQA\nMihaylov et al. (2018)\nPAWS\nZhang et al. (2019)\nQuAIL\nRogers et al. (2020)\nPIQA\nBisk et al. (2020)\nQuaRTz\nTafjord et al. (2019)\nSciQ\nWelbl et al. (2017)\nSocial IQa\nSap et al. (2019)\nSuperGLUE MultiRC\nKhashabi et al. (2018); Wang et al. (2019)\nSuperGLUE WIC\nPilehvar & Camacho-Collados (2018); Wang et al. (2019)\nTwitter Sentiment\nZhang et al. (2019)\nDetailed results.\nIn Figure 12, we provide detailed results across all datasets for both the baseline\nand the auxiliary confidence loss introduced in Section 4.3. In Figure 13 we report the detailed\nresults on overfitting to the weak supervisor predictions for the NLP datasets.\nA.2\nCHESS PUZZLES\nData preprocessing.\nThe GPT-4 pretraining dataset included chess games in the format of move\nsequence known as Portable Game Notation (PGN). We note that only games with players of Elo\n1800 or higher were included in pretraining. These games still include the moves that were played in-\ngame, rather than the best moves in the corresponding positions. On the other hand, the chess puzzles\nrequire the model to predict the best move. We use the dataset originally introduced in Schwarzschild\net al. (2021b) which is sourced from https://database.lichess.org/#puzzles (see\nalso Schwarzschild et al., 2021a). We only evaluate the models ability to predict the first move of\nthe puzzle (some of the puzzles require making multiple moves). We follow the pretraining for-\nmat, and convert each puzzle to a list of moves leading up to the puzzle position, as illustrated in\nFigure 14. We use 50k puzzles sampled randomly from the dataset as the training set for the weak\nmodels and another 50k for weak-to-strong finetuning, and evaluate on 5k puzzles. For bootstrap-\nping (Section 4.3.1), we use a new set of 50k puzzles from the same distribution for each step of the\nprocess.\nTraining hyperparameters.\nWe train (finetune) all models for 5 epochs using a batch size of 32.\nWe do not apply early-stopping.\nWeak labels.\nWe produce weak labels by sampling predictions at temperature T = 0 (greedy\ndecoding) from the weak model on a held-out set of additional 50k puzzles. The weak labels are\ncompletions showing the highest likelihood move according to the weak model.\nEvaluation.\nTo evaluate the models, we sample completions at temperature T = 0 on the held out\ntest set, and compute the fraction of datapoints where the model outputs the correct next move.\n29\n55\n60\n65\n70\n75\n80\n85\n90\n55\n60\n65\n70\n75\n80\n85\n55\n60\n65\n70\n75\n80\n70\n75\n80\n85\n55\n60\n65\n70\n75\n80\n85\n50\n60\n70\n80\n90\n55\n60\n65\n70\n75\n80\n85\n90\n50\n60\n70\n80\n90\n60\n65\n70\n75\n80\n85\n90\n60\n65\n70\n75\n80\n50\n60\n70\n80\n90\n60\n65\n70\n75\n80\n85\n90\n95\n50\n60\n70\n80\n50\n55\n60\n65\n70\n60\n70\n75\n80\n85\n90 \n60\n70\n80\n90\n84\n86\n88\n90\n92\n94\n96\n98\n50\n55\n60\n65\n70\n75\n80\n80\n82\n84\n86\n88\n90\n50\n55\n60\n65\n70\n75\n80\n85\n60\n70\n80\n90\n60\n65\n70\n75\n80\n85\n90\n95\n10-8\n10-6\n10-4\n10-2\n1\n10-8\n10-6\n10-4\n10-2\n1\n10-8\n10-6\n10-4\n10-2\n1\n10-8\n10-6\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\ntest accuracy (%)\n[1]\n[9]\n[6]\n[8]\n[10]\n[11]\n[12]\n[4]\n[7]\n[3]\n[5]\n[2]\n[22]\n[21]\n[20]\n[19]\n[18]\n[17]\n[16]\n[15]\n[14]\n[13]\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\nFigure 12: Full weak-to-strong generalization results across 22 NLP datasets. Test accuracy as\na function of strong student compute across our full suite of standard NLP tasks. See Table 1 for\ndataset details.\n30\n50\n55\n60\n65\n70\n75\n80\n85\n70\n75\n80\n85\n90\ntest accuracy (%)\nprogress (fraction of epoch)\n55\n60\n65\n70\n75\n80\n85\n60\n65\n70\n75\n80\n85\n90\n68\n70\n72\n74\n76\n78\n80\n50\n55\n60\n65\n70\n75\n80\n50\n55\n60\n65\n70\n75\n80\n85\n60\n65\n70\n75\n80\n85\n0\n0.4\n0.8\n1.2\n1.6\n0\n0.4\n0.8\n1.2\n1.6\n0\n0.4\n0.8\n1.2\n1.6\n0\n0.4\n0.8\n1.2\n1.6\n2\n2\n2\n2\n(a)\n0\n50\nperformance gap recovered (%)\nearly stop\n(ground truth)\nlast\n(b)\n10\n20\n30\n40\n[1]\n[9]\n[6]\n[8]\n[10]\n[11]\n[12]\n[4]\nearly stop\n(weak labels)\nstrong student compute\n/ weak supervisor compute\n1\n107\nFigure 13: Overfitting during training, for NLP datasets. Strong models overfit to the weak\nlabels. (a) Ground truth test accuracy of strong students over the course of training for a subset of\nour NLP task. Hues indicate the gap between weak supervisor and strong student model compute.\nInset numbers indicate dataset id (compare Figure 12). (b) Median best, early-stopped according to\nweak label agreement, and final performance gap recovered (PGR) aggregated across all supervisor-\nstudent pairs and all NLP tasks. Error bars indicate standard error of the mean (s.e.m.).\n1\n2\n3\n4\n5\n6\n7\n8\na\nb\nc\nd\ne\nf\ng\nh\n1\n2\n3\n4\n5\n6\n7\n8\na\nb\nc\nd\ne\nf\ng\nh\nPrompt: \u201c1. d4 1... Nf6 2. Nf3 2... d5 3. e3 3... e6 4. Bd3 4... c5\n5. c3 5... Be7 6. Nbd2 6... O-O 7. O-O 7... Nc6 8. Re1 8... Bd7 9. e4 9... dxe4\n10. Nxe4 10... cxd4 11. Nxf6+ 11... Bxf6 12. cxd4 12... Nb4 13. Be4 13... Qb6\n14. a3 14... Nc6 15. d5 15... exd5 16. Bxd5 16... Bf5 17. Bxc6 17... Qxc6\n18. Nd4 18... Bxd4 19. Qxd4 19... Rfe8 20. Rxe8+ 20... Rxe8 21. Be3 21... b6\n22. Rc1 22...\u201d\nLabel: \u201c Qxc1+\u201d\nPrompt: \u201c1. e4 1... e5 2. Nc3 2... Nf6 3. Nf3 3... Nc6 4. Bb5 4... Bc5\n5. Bxc6 5... dxc6 6. d3 6... Bg4 7. h3 7... Bxf3 8. Qxf3 8... O-O 9. g4\n9... Bb4 10. Bd2 10... Nd7 11. h4 11... Be7 12. g5 12... Nc5 13. O-O-O\n13... Qd7 14. h5 14... Qd8 15. Qg3 15... Ne6 16. Rdg1 16... b5 17. Qxe5\n17... a5 18. f4 18... Re8 19. Qf5 19... b4 20. Na4 20... Nd4 21. Qg4 21... c5\n22. f5 22... Ra6 23. f6 23... Bd6 24. fxg7 24... Kxg7 25. Rg2 25... Qc8\n26. h6+ 26... Kg8 27. Qh5 27... Qd7 28. Rf1 28... Re6 29. Rgf2 29... Rg6\n30. c3 30... bxc3 31. Nxc3 31... a4 32. Nd5 32... Qb5 33. Nf6+ 33... Kh8\n34. Qh3 34... Rb6 35. Be3 35... Ne6 36. Nxh7 36... Qxd3 37. Rd1 37... Qc4+\n38. Kb1 38... Qxe4+ 39. Ka1 39... Be5 40. Nf6 40... Qc4 41. Nd5 41... Rb7 42.\u201d\nLabel: \u201c Qf5\u201d\n(a) Elo-695 puzzle\n(b) Elo-2253 puzzle\nFigure 14: Chess puzzles: example datapoints. Two representative examples of an easy (a) and a\nhard (b) chess puzzle with corresponding prompts and target label formats.\n31\n(a)\nweak model compute\n (fraction of GPT4) \n10-7\n0.1\n10-8\n10-6\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\nzero-shot\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n0\n20\n40\n60\n80\n100\n(c)\nstudent-supervisor agreement (%)\nChess Puzzles\n(b)\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\n10-8\n10-6\n10-4\n10-2\n1\nstrong student compute\n(fraction of GPT4)\nweak-to-strong\nbaseline\nbootstrapping\nFigure 15: Additional results on chess. Test accuracy of (a) baseline and (b) bootstrapping (see\nsection 4.3.1) compared to a zero-shot baseline. Zero-shot performance improves with model size,\nand students supervised with much weaker supervisors sometimes underperform compared to the\ncorresponding zero-shot model. (c) Supervisor-student agreement on the chess puzzle data. Similar\nto Figure 8, agreement decreases as the student becomes larger. Hue of line indicates compute of\nweak supervisor.\nZero-shot results.\nIn Figure 15(a, b), we compare the naive baseline and bootstrapping (see sec-\ntion 4.3.1) generalization to a zero-shot baseline on the chess puzzle data. Especially since the\nmodels were pretrained on chess games, zero-shot evaluation provides a strong baseline. In partic-\nular, strong students trained with much weaker supervisors underperform the zero-shot baseline for\nthe same model size in some cases.\nSupervisor-student agreement results.\nIn Figure 15(c), we report the supervisor-student agree-\nment on the chess puzzles. Similar to the NLP tasks (see Section 5.1.3), the agreement on chess also\ndecreases as the student models get larger.\nA.3\nCHATGPT REWARD MODELING\nData preprocessing.\nEach datapoint presents a dialog d between a user and an assistant, with\na last message coming from the user; for each dialog, there are multiple candidate completions\n(c1, c2, . . . , cm), i.e. responses from the assistant. We also have access to pairwise comparisons of\ncompletions, where the labeler specifies the preferred completion within a given pair of completions.\nTo sum up, the datapoints can be viewed as (d, c1, c2, y), where the label y is 1 if the labeler preferred\ncompletion c2 and 0 otherwise. We use a mixture of multiple datasets used to train the reward models\nfor ChatGPT.\nModels.\nTo adapt the language models to the reward modeling setting, we replace the unem-\nbedding layer of the model with a linear head with a single output, which is the logit for a given\ncompletion.\nThe weights for this head are initialized to the unembedding weights of an arbi-\ntrary token in the original embedding layer. Similar to past work (Stiennon et al., 2020; Ouyang\net al., 2022), we run two forward passes for each comparison, and the model prediction is given\nby \u03c3(Mw(d, c2) \u2212 Mw(d, c1)), where \u03c3 is the sigmoid function and Mw(d, c) is the logit for\ncompletion c predicted by the model.\nTraining hyperparameters.\nWe train for 1 epoch with a batch size of 220. We do not apply\nearly-stopping.\nWeak labels.\nWe train the weak models on half of the available comparison data, and then\nmake predictions on the other half. The weak label yw for a comparison (d, c1, c2) is given by\nyw = \u03c3(Mw(d, c2) \u2212 Mw(d, c1)), where \u03c3 is the sigmoid function and Mw(d, c) is the logit for\ncompletion c predicted by the weak model.\nSupervisor-student agreement results.\nIn Figure 16, we report the supervisor-student agreement\non the RM task. Similar to the NLP tasks in Figure 8 and chess puzzles in Figure 15(c), the agree-\nment decreases as the student gets larger.\n32\nweak-to-strong performance\nstudent-supervisor agreement (%)\n100\n70\n75\n85\n95\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n(a)\nall samples\n80\n90\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\nsupervisor mistakes\nsupervisor correct\nstudent-supervisor agreement (%)\n100\n70\n75\n85\n95\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n(b)\n80\n90\nstudent-supervisor agreement (%)\n100\n70\n75\n85\n95\nstrong student compute\n(fraction of GPT4)\n10-8\n10-6\n10-4\n10-2\n1\n(c)\n80\n90\nChatGPT\nReward Modeling\nFigure 16: Supervisor-student agreement decreases for stronger students on RMs. Please refer\nto caption of Figure 8 for detailed explanation of the plot. We reproduce the supervisor-student\nagreement experiment on the reward modeling data, and observe similar trends to the NLP tasks.\nGenerative finetuning.\nIn Figure 17, we show that the PGR improvements from the generative\nfinetuning on RM data (Section 5.2.2) and from early-stopping on ground truth test accuracy (Sec-\ntion 5.1.1) stack together, leading to results competitive with the NLP and chess settings. In Fig-\nure 18, we report the results of an experiment similar to Figure 10, but where the weak models are\nalso pretrained with an additional generative finetuning step on the RM data.\nA.4\nAUXILIARY CONFIDENCE LOSS\nHere, we provide a detailed description of the method we use in Section 4.3.2.\nWe use the following loss function:\nLconf(f) = (1 \u2212 \u03b1) \u00b7 CE(f(x), fw(x)) + \u03b1 \u00b7 CE(f(x), \u02c6ft(x))\n(1)\nwhere CE(\u00b7, \u00b7) is the cross-entropy loss between the predictive distributions on a given input x,\nfw(x) \u2208 [0, 1] represents the weak label predictive distribution, f(x) \u2208 [0, 1] is the strong model\npredictive distribution, \u03b1 is a weight and t is a threshold. The predictions \u02c6ft(x) correspond to\nhardened strong model predictions using a threshold t, i.e. \u02c6ft(x) = I[f(x) > t] \u2208 {0, 1} where I is\nthe indicator function. We set the threshold t adaptively, so that f(x) > t holds for exactly half of\nexamples in the batch7. We set \u03b1max = 0.75 for the largest student models and to 0.5 otherwise and\nlinearly warm-up \u03b1 from 0 to \u03b1max over the first 20% of training.\nOur balancing mechanism incorporates a prior over the distribution of labels into training and is\nonly practically feasible in the low-n classification setting. For most weak-strong pairs and datasets,\nit had a small or neutral effect on weak-to-strong generalization; however, in a few settings it made\na significant improvement.\nWe note that the loss in Equation 1 can be rewritten as a self-bootstrapping loss:\nLconf(f) = CE(f(x), (1 \u2212 \u03b1) \u00b7 fw(x) + \u03b1 \u00b7 \u02c6ft(x)),\n(2)\ni.e. the cross-entropy target is a mixture of the weak model predictions and the (thresholded) pre-\ndictions of the strong student itself. This loss is related to the bootstrapping methods in Reed et al.\n(2014) and Arazo et al. (2019) for addressing label noise. It is also similar to self-training (Lee\net al., 2013) and conditional entropy minimization (Grandvalet & Bengio, 2004), which have led\nto state-of-the-art results in semi-supervised learning (Xie et al., 2020) and domain adaptation (Shu\net al., 2018). Chen et al. (2020b) and Wei et al. (2020) show that self-training can mitigate the bias\nof the supervisor model.\nIn Appendix B we also describe other methods we considered; for most of these methods, we got\nnegative early results.\n7The choice of exactly half reflects the prior over classes, and should be computed explicitly from weak\nmodel predictions in non-balanced or non-binary settings.\n33\n(b)\n10-4\n10-3\n10-2\n10-5\nstrong student compute\n(fraction of GPT4)\n10-6\nweak model compute\n (fraction of GPT4) \n10-7\n0.1\n(a)\n60\n62\n64\n66\n68\n70\n10-7\n10-6\n10-5\n10-4\nstrong student compute\n(fraction of GPT4)\n10-3\n10-2\ntest accuracy (%)\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\nno generative\nfinetuning\nwith generative\nfinetuning\nstrong ceiling performance\n(g.t. supervision)\nFigure 17: The benefits of improved task-specific tuning and ground truth early stopping stack,\nresulting in even higher PGR. Like Figure 10 but with ground truth early stopping based on test\naccuracy.\n(b)\n10-4\n10-3\n10-2\n10-5\nstrong student compute\n(fraction of GPT4)\n10-6\nweak model compute\n (fraction of GPT4) \n10-7\n0.1\n(a)\n60\n62\n64\n66\n68\n70\n10-7\n10-6\n10-5\n10-4\nstrong student compute\n(fraction of GPT4)\n10-3\n10-2\ntest accuracy (%)\nperformance gap recovered (%)\n100\n0\n20\n40\n60\n80\nno generative\nfinetuning\nwith generative\nfinetuning\nstrong ceiling performance\n(g.t. supervision)\nFigure 18: PGR improves when both supervisors and students have an extra generative fine-\ntuning step. Like Figure 10 but where \u201cwith generative finetuning\u201d indicates that both supervisors\nand students have an extra generative finetuning step. In other words, for this experiment all base\nmodels have an extra generative finetuning step following pretraining.\nB\nADDITIONAL RESULTS ON METHODS\nWe did preliminary experiments on a variety of methods for improving the strong model perfor-\nmance in our weak-to-strong generalization setting. We found many of them not useful for im-\nproving over the naive finetuning baseline, and others yielding limited improvements on a subset of\nsettings but not consistently over all datasets and model sizes. We summarize the algorithms, the\nmotivations, and the takeaways below. Note that we did not optimally tune each of the methods, so\nit is possible that with better tuning they may still perform well.\nConfidence thresholding.\nTo filter out incorrect weak labels, we used a simple cut-off method\nthat selected only the top 5% to 20% examples from each class where the weak supervisor is most\nconfident to train the strong model. We found that our weak labels are typically well-calibrated,\nbut confidence thresholding only helps when the weak labels are very bad (e.g. 60% accuracy) and\nstops being useful when the weak labels reach around 70% to 80% accuracy. We observed these\nresults both in NLP and in the chess puzzle settings. See Appendix C for more discussion of related\nexperiments.\nConfidence losses.\nTo encourage strong model to make confident predictions\n(Grandvalet &\nBengio, 2004), we added an auxiliary loss that encourages the model predicted class probabil-\n34\nTable 2: Custom prompts used in the zero-shot and few-shot experiments. We design a simple\ncustom prompt for each of the tasks in the table below. In the few-shot setting, we also append\nlabeled (with ground truth or weak labels) examples to the prompt.\nDataset\nCustom Prompt\nCosmosQA\nThere is some context, followed by a question about the context, and an answer.\nRespond with 1 if the answer is correct, and 0 otherwise.\nDeontology\nThere is a short scenario, consisting of a situation or request, followed by an excuse.\nRespond with 1 if the excuse is valid, and with 0 otherwise.\nJustice\nThere is a statement. Respond with 1 if it is correct or just and with 0 otherwise.\nQuAIL\nThere is a news context, followed by a question about the context, and an answer\nto the question. Respond with 1 if the answer is correct, and with 0 otherwise.\nSciQ\nThere is a science knowledge question, followed by an answer.\nRespond with 1 if the answer is correct, and with 0 otherwise.\nSocial IQa\nThere is some context, followed by a social question, followed by an answer.\nRespond with 1 if the answer is correct, and 0 otherwise.\nVirtue\nThere is a short scenario, followed by a judgement of the person involved.\nRespond with 1 if the judgement is correct, otherwise respond with 0.\nity p to be far away from 0.5.\nWe tried both the l2 loss \u2212(p \u2212 0.5)2 and the entropy loss\np log p + (1 \u2212 p) log(1 \u2212 p). We found these losses to be helpful in preliminary experiments in\nthe linear probing setting, but they generally performed less well than the confidence auxiliary loss\nin Equation 1 in the finetuning setting. We have also observed negative results with the confidence\nlosses when the training data is highly class-imbalanced or when we do not use the rebalancing\nprocedure described in Section 4.3.\nProduct confidence loss.\nWe also tried a confidence-like loss which sets the cross entropy tar-\ngets to be proportional to the product of the probabilities that the weak and strong models assign,\nrenormalized across classes and without propagating gradients through the targets. In preliminary\nexperiments, this loss consistently gave positive results over the baseline on two NLP tasks, but\nperformed poorly compared to our main confidence loss. Variants like geometric mean instead of\nproduct gave no boost. Compared to the confidence loss, it could be useful as it has no inter-batch\ndependence and could potentially be adapted for generative tasks.\nLP-FT.\nWe used the LP-FT technique proposed in Kumar et al. (2022) which first trains a linear\nprobe on frozen strong model representations and then finetunes all layers, to avoid destroying the\npretrained representation. We were unable to get improvements compared to the finetuning baseline.\nWeight regularization.\nTo regularize the strong model weights to avoid imitating the weak la-\nbels8, we tried a variety of regularization techniques for strong model training, including stronger\nweight decay (Krogh & Hertz, 1991) and dropout (Srivastava et al., 2014). We did not find signifi-\ncant improvement.\nLoRA.\nAs another regularization technique, we also considered low-rank adaptation (LoRA) (Hu\net al., 2022), i.e. only making a low-rank update to the parameters of each layer of the model during\nfinetuning. We did not find any improvement, even when sweeping the LoRA rank.\nData augmentation.\nInspired by the success of consistency algorithms in self-supervised train-\ning (Chen et al., 2020a; Caron et al., 2021), we used the strong student models to rephrase the inputs\nin each sample, and added an auxiliary loss enforcing the strong model predictions to be consistent\nbetween original and rephrased samples. We did not find any improvement on a selected subset of\nNLP datasets.\n8However, as we discuss in Section 5.1.3, in our setup the strong model tends to be bad at imitating the\nweak labels. Therefore, regularization could be more important in settings where the strong model can fit the\nweak labels well.\n35\naccuracy\n(%, all-trained, all-eval)\n0\n20\n40\n60\n80\n0\n20\n40\n60\n80\ntest accuracy\n(%, easy-trained, all-eval)\naccuracy\n(%, all-trained, hard-eval)\n0\n20\n40\n60\n70\n50\n30\n10\n0\n20\n40\n60\n70\n50\n30\n10\ntest accuracy\n(%, easy-trained, hard-eval)\nelo \u2264 800\nelo \u2264 900\nelo \u2264 1100\nelo \u2264 1000\nelo \u2264 1300\nelo \u2264 1200\nzero-shot\n(a)\n(b)\nFigure 19: Easy-to-hard generalization on chess puzzles. We finetune models on chess puzzles\nwith Elo \u2264 t, varying the threshold t, and evaluate the finetuned models on (a): all test puzzles,\nand (b): hard test puzzles with Elo \u2265 2000. Across the board, we see strong performance, even\nwhen training only on very easy puzzles (Elo \u2264 800). For reference, we also include the zero-\nshot performance of the model. Finetuning on easy puzzles, we improve upon the performance on\naverage on the test set, but we do not improve on hard puzzles, compared to the zero-shot model.\nAdding label noise, special losses for noisy labels.\nWe experimented with the generalized cross-\nentropy loss proposed in Zhang & Sabuncu (2018) that is more robust to label noise, but did not find\nimprovement over cross-entropy. We also tried adding random noise to weak labels, and found that\nthe strong models were able to simulate the weak labels less well, especially early in training, but it\ndid not ultimately result in improved performance.\nFew-shot prompting.\nAs an alternative to fine-tuning, we can use the in-context learning ability\nof the strong student models. For each task, we append a custom prompt shown in Table 2. For a\ndetailed description of the results, see Section 5.2.1.\nWeight averaging.\nPrior work (Izmailov et al., 2018; Cha et al., 2021; Wortsman et al., 2022b;a)\nsuggested that various forms of weight averaging can substantially improve performance, especially\nin distribution shift settings. In our setup, we experimented with applying exponential moving\naveraging to the parameters of the model during training, but did not observe improvements relative\nto the baseline.\nC\nEASY-TO-HARD GENERALIZATION\nIn Section 5.1.3 and Appendix E, we discuss that one reason weak-to-strong generalization may\nbe difficult is if the weak labels have systematic errors that the strong model can learn to emulate.\nOne natural type of systematic weak label error is to do poorly on hard examples and well on easy\nexamples.\nIn this section, we focus on studying what we call easy-to-hard generalization, where we train only\non easy examples using ground truth supervision, and assess generalization to harder examples.\nC.1\nCHESS PUZZLES\nEach chess puzzle comes with a natural difficulty label: an Elo score, which describes its difficulty\naccording to humans. On the https://lichess.org website, people try to solve puzzles,\nwhich can be viewed as a game between a puzzle and a human player. The Elo scores are then\nassigned to both human players and chess puzzles following the standard Elo algorithm.\nWe consider the easy-to-hard generalization problem, where the difficulty is defined according to\nthe puzzle Elo rating. We note that the puzzle Elo describes the difficulty of the entire puzzle\nmove sequence, while we are only training the model to predict the first move in the sequence\n36\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n500\n1000\n1500\n2000\n2500\npuzzle elo\n500\n1000\n1500\n2000\n2500 500\n1000\n1500\n2000\n2500\ntrain easy\ntrain all\ntrain hard\nzero shot\n10-10\n10-8.8\n10-7\n10-5.2\n10-4.2\n10-3.1\n10-2.4\n10-1.6\n1\n(a) Easy cutoff: Elo \u2264 1200\n500\n1000\n1500\n2000\n2500\npuzzle elo\n500\n1000\n1500\n2000\n2500 500\n1000\n1500\n2000\n2500\ntest accuracy (%)\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\n0\n20\n40\n60\n80\n100\ntrain easy\ntrain all\ntrain hard\nzero shot\n10-10\n10-8.8\n10-7\n10-5.2\n10-4.2\n10-3.1\n10-2.4\n10-1.6\n1\n(b) Easy cutoff: Elo \u2264 900\nFigure 20: Easy-to-hard generalization on chess puzzles. We present detailed performance of\nmodels finetuned on different subsets of chess puzzles across model sizes and test puzzle difficulty\nlevels. For each model size, we compare models trained only on easy puzzles, hard puzzles, or all\npuzzles. We also include the zero-shot model performance. We provide results for the easy puzzle\nElo cutoffs of (a): 1200 and (b): 900. All finetuned models are trained on 50k random datapoints\nfrom the corresponding distribution. The size of the model is shown in the upper-right corner of\neach panel, in terms of fraction of GPT-4 compute.\n37\n86\n87\n88\n89\n90\n70\n75\n80\n85\n90\n66\n70\n74\n78\n85\n80\n70\n75\n85\n80\n70\n75\n65\n85\n80\n70\n75\n65\n60\n10-8\n10-6\n10-4\n10-2\ntest accuracy (%)\n55\n65\n75\n10-8\n10-6\n10-4\n10-2\n\u221e\nsample difficulty cutoff\n(fraction of GPT4)\n[11]\n[9]\n[19]\n[12]\n[10]\n[22]\n[21]\n10-8\n10-6\n10-4\n10-2\n10-8\n10-6\n10-4\n10-2\nstrong student compute\n (fraction of GPT4) \n10-7\n0.1\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\n\u221e\nFigure 21: Effect of varying training data difficulty on test set accuracy. Test accuracy as a func-\ntion of sample difficulty cutoff on a subset of our NLP tasks. The leftmost point on the horizontal\naxis corresponds to only using datapoints that models of all sizes that we consider get right when\ntrained on other data sampled from the same task, and the rightmost point (denoted with \u221e) corre-\nsponds to training on all datapoints; the point with value x on the horizontal axis corresponds to only\nusing the datapoints that models with x or higher compute (fraction of GPT-4) consistently get right.\nInset numbers indicate task id (compare Figure 12). Hue indicates compute of weak supervision.\nStars indicate points where weak supervisor size corresponds to sample difficulty cutoff.\n(see Appendix A.2). Consequently, the puzzle Elo is a high-quality but still imperfect measure of\ndifficulty of the problem for humans. It is also important to note, that puzzle Elo may not be a good\nmeasure of difficulty for the models: easy puzzles for humans can be hard for the models and vice\nversa.\nWe then split the dataset into subsets according to the puzzle Elo.\nWe consider the hard\nset to be puzzles with difficulty above Elo 2000.\nFor the easy set, we consider cuttoffs in\n{800, 900, 1000, 1100, 1200, 1300}, and use puzzles with difficulty below the cutoff. We also con-\nsider the unrestricted set of all puzzles. We sample 50k puzzles from each of these sets randomly,\nand finetune the model on them9.\nWe report the results in Figure 19, where we also provide the performance of a zero-shot baseline\nfor reference. We plot the accuracy of the models trained on the easy subsets of puzzles against the\nperformance of the same model trained on all puzzles. We find that the models generally perform\nwell on average on the test set in panel (a), and outperform the zero-shot baseline. Interestingly,\nwhen evaluated on hard examples only, in panel (b), the models perform similarly to the zero-shot\nbaseline, or slightly worse.\nWhen trained on easy puzzles, the models shift towards performing well on the easy puzzles, and\nunderperform on the hard puzzles. In Figure 20, we can see that generally the models improve upon\nthe zero-shot baseline outside of their training difficulty range, often up to Elo of 1500 or higher, but\nunderperform on the hardest examples.\nC.2\nNLP TASKS: DIFFICULTY THRESHOLDING\nNLP tasks do not come with a natural source of difficulty labels, but we can create such labels by\nlooking at performance as a function of model size.\n9For easy puzzles with 800-Elo cutoff, we only use 25k puzzles, because there are not 50k puzzles available\nin this difficulty range.\n38\n1\n2\n3\n4\n5\nproblem level\n5\n10\n15\n20\n25\n30\n35\n40\n45\nscore (elo)\n60\n65\n70\n75\n80\n85\ntest accuracy (%)\nstrong student compute\n(fraction of GPT4)\n10-6\n10-5\n10-4\n10-3\n10-2\ntrained on all difficulties\ntrained on easiest 30%\ntrained on easiest 50%\n(a)\n(b)\nFigure 22: Filtering training samples by GPT-4 generated Elo scores results in very good easy-\nto-hard generalization. (a) GPT-4 generated Elo scores for different, human-defined, problem\ndifficulties (1 - easiest, 5 - hardest) on the MATH dataset. (b) Average test accuracy as a function of\nstrong student compute on a subset of our NLP tasks. Student is trained on ground truth labels on\nsamples of all difficulties (black), only the 30% easiest tasks (orange), or only the 50% easiest tasks\n(blue).\nWe define difficulty of a datapoint based on the smallest model size that consistently predicts the\nlabel on this datapoint correctly, when trained on ground truth. For example, suppose we have\n4 ground truth models W1, W2, W3, W4 that use compute C1 < C2 < C3 < C4 respectively.\nSuppose models W1, W3, W4 predict the example correctly when it is in a held-out set, while W2\npredicts it incorrectly. Then we will assign a difficulty of C3 to the example.\nThen given a difficulty cutoff D, we filter the training set to examples with difficulty \u2264 D. We\nsubsample the filtered set so that the number of training examples is equal to the number of examples\nat the lowest difficulty level. We train a model on the subsampled training set using ground truth\nlabels, and measure its accuracy on a held out test set (with no subsampling).\nThe subsampling ensures that we use the same training set size for each difficulty cutoff. Using\nground truth labels ensures that the label accuracy is the same (100%) for each cutoff. We also use\nthe same test set for each cutoff. This setup lets us vary only training data difficulty, and measure its\nimpact on the trained model\u2019s accuracy.\nWe plot results in Figure 21. The y-axis is accuracy on the test set, while the x-axis is the difficulty\ncutoff. Increasing the difficulty cutoff generally leads to an increase in accuracy. This result suggests\nthat solving easy-to-hard generalization is non-trivial even if there are no weak label errors.\nFor smaller models (darker lines), the accuracy initially increases, but starts to decrease beyond a\npoint. The drop generally happens when the difficulty cutoff exceeds the capacity of the model itself,\ni.e. when the examples are too difficult for the model to fit. However, large models trained on easy\nexamples often perform well.\nC.3\nGPT-4 PREDICTED DIFFICULTY\nUltimately, we care about strong models generalizing from human supervision. From this perspec-\ntive, it is important to understand whether we can achieve easy-to-hard generalization, where the dif-\nficulty is measured according to humans, rather than capacity-constrained models. In Appendix C.1,\nwe explored this question in chess, but we would want to extend this analysis to the NLP tasks.\nMost natural datasets do not come with information about problem difficulty. As a rough estimate,\nwe automatically generated difficulty labels using GPT-4. More concretely, we used GPT-4 to rank\npairs of examples in each dataset, asking \u201cwhich question is easier, Question A or Question B?\u201d We\nthen calculated the Elo scores for each example via a finite number of random comparisons.\n39\nTable 3: Weak-to-strong generalization on ImageNet. We train linear probes on the representa-\ntions extracted by DINO models with weak supervision from an AlexNet model. The strong students\nsubstantially outperform their weak supervisor.\nModel\nTop-1 Accuracy (%)\nPGR (%)\nAlexNet (weak supervisor)\n56.6\n-\nDino ResNet50\n63.7\n-\nDino ViT-B/8\n74.9\n-\nAlexNet \u2192 DINO ResNet50\n60.7\n57.8\nAlexNet \u2192 DINO ViT-B/8\n64.2\n41.5\nTo evaluate the quality of GPT-4 Elo score as a measure of difficulty, we performed correlation anal-\nysis against human annotations for datasets with human difficulty levels such as MATH (Hendrycks\net al., 2021) and chess, as well as against weak model confidence. We found that the three measures\nalign better for reasoning tasks such as MATH, as we show in Figure 22(a), but not much for some\nnatural language tasks. When looking at the samples, we found that GPT-4 Elo scores tend to be\nhigher for longer questions, but those questions may actually be easy for smaller models since they\nprovide more context.\nUsing GPT-4 Elo score as a proxy for human difficulty, we used different cutoffs on scores to sep-\narate easy and hard examples, trained the strong models on the easy examples only (with ground\ntruth labels), and evaluated on the hard examples. Preliminary results are shown in Figure 22(b).\nIn general, we found that using GPT-4 Elo as measure of hardness makes generalization slopes\nsteeper than our main setup of weak-to-strong generalization. One possible confounder for interpre-\ntation is that our Elo measurements could be noisy, causing generalization to be better.\nNote that this setup is a classic covariate shift problem, whereas our main setup focuses more on\nconcept shift and noisy labels. It is unclear which setup would be more relevant, and we think it is\nimportant to study easy-to-hard generalization more thoroughly in future work.\nD\nOTHER WEAK-TO-STRONG SETTINGS\nD.1\nSELF-SUPERVISED VISION MODELS\nWe additionally demonstrate weak-to-strong generalization in a simple image classification experi-\nment. We use a pretrained AlexNet model (Krizhevsky et al., 2012) as a weak supervisor, and use\nit to generate weak labels on the ImageNet (Russakovsky et al., 2015) validation set. As a strong\nstudent, we use linear probing on frozen representations extracted by DINO models (Caron et al.,\n2021) based on ResNet-50 (He et al., 2016) and ViT-B/8 (Dosovitskiy et al., 2020) architectures.\nThe DINO models are pretrained in an unsupervised way and did not observe direct supervision for\nImageNet classification or any other classification task during pretraining, so this experiment does\nnot have the pretraining leakage disanalogy discussed in Section 6.1.\nWe use 40k datapoints from the validation set to train the linear probes, and evaluate performance\non the remaining 10k datapoints. For training the linear probes, we use a batch size of 128, Adam\noptimizer (Kingma & Ba, 2014) and a learning rate of 10\u22123. We run 20 epochs of training for\nResNet-50 and 5 epochs for ViT-B/8.\nWe report the results in Table 3. Similarly to our main experiments in Section 4, the student can\nsubstantially outperform the supervisor, achieving PGR on the order of 50%. This experiment shows\nthat our results are not limited to the natural language setting, and generalize to other domains. It\nalso shows that strong students can generalize from weak supervision on tasks where they only had\nindirect pretraining, i.e. where the knowledge of the task is latent.\n40\n60\n65\n70\n75\n80\n85\n90\n55\n60\n65\n70\n75\n60\n65\n70\n75\n80\n50\n60\n70\n80\n90\n55\n60\n65\n70\n75\n80\n85\n90\n65\n70\n75\n80\n55\n60\n65\n70\n75\n80\n85\n60\n65\n70\n75\n10-6\n10-4\n10-2\n10-6\n10-4\n10-2\n10-6\n10-4\n10-2\n10-6\n10-4\n10-2\ntest accuracy (%)\nstrong student compute\n(fraction of GPT4)\nweak supervisor compute\n (fraction of GPT4) \n10-7\n0.1\n[12]\n[10]\n[9]\n[11]\n[8]\n[4]\n[6]\n[5]\nFigure 23: Linear probing qualitatively matches finetuning weak-to-strong generalization. Test\naccuracy as a function of strong student compute on a subset of our NLP tasks. Inset numbers\nindicate dataset id (compare Figure 12). Accuracy of a linear probe on student model trained with\nground truth in black, accuracy of linear probe on students trained directly with weak linear probe\nsupervision shown in solid lines with circles (hue indicates compute of weak supervision).\nD.2\nLINEAR PROBING\nIn addition to our main finetuning experiments, we also perform weak-to-strong generalization ex-\nperiments in the linear probing setting. We freeze all weak and strong model parameters, and train\nnew linear classification heads both using ground truth labels and using weak labels. We train lin-\near probes with Adam optimizer (Kingma & Ba, 2014), 10\u22123 learning rate, batch size 128, and no\nweight decay for 200 epochs, for both weak and strong model training. We do early stopping based\non agreement to the weak labels on the validation set and report test accuracy. Results are shown in\nFigure 23. We observe qualitatively similar generalization compared to the full finetuning case.\nGenerally, we found the linear probing setting to be very useful to quickly iterate on methods,\ndatasets and ideas. While finetuning provides better results, the qualitative trends in linear probing\nare similar, and the experiments are much faster and easier to run. For example, we initially found\npositive results with confidence loss (Section 4.3) and bootstrapping (Section 4.3.1) in the linear\nprobing setting.\nE\nTHE EFFECTS OF WEAK LABEL STRUCTURE\nOne challenge in weak-to-strong generalization is the presence of errors in the weak labels.\nThroughout most of this paper, we consider a particular type of weak error structure: the kinds\nof errors smaller, capacity-constrained language models make. However, this is not the only type of\nerrors possible.\nIn this section, we analyze synthetic examples of other kinds of weak label structures, and the\nimplications they have on generalization. Weak model error structure must be considered in relation\nto the particular strong model at hand. For example, we conjecture that the extent to which the strong\nmodel can imitate the weak supervisor may be very important. If we have two strong models of the\nsame performance on the actual task but one is very good at imitating the labels, then we expect that\nmodel will generalize less desirably, at least with the naive finetuning method.\nIn Section 5.1.3 we found that surprisingly the strongest students are imitating the weak supervisor\nmistakes less than smaller student models in our setting. Since we expect superhuman models to\nbe very good at imitating human supervisor, this may be a major disanalogy. In this section we test\ncases where the weak supervisor can be imitated easily.\n41\n75\n80\n85\n90\n74\n75\n76\n77\n78\n79\n80\n81\n82\n50\n55\n60\n65\n70\n75\n80\n85\n90\n55\n60\n65\n70\n75\n80\n85\n90\n60\n65\n70\n75\n80\n85\n90\n95\nstudent-supervisor agreement (%)\n60\n65\n70\n75\n80\n85\n90\ntest accuracy (%)\n# features\n10\n30\n100\n300 1000 full\nnoise\n300 features + X noise\n0.1\n0.07 0.05 0.01\n0\nfull\nbaseline\n10\n30\n100\n300\n1000\n0.576\n0.7\n0.758\n0.803\n0.845\n0.1\n0.07\n0.05\n0.01\n0\n(a)\n0.576\n0.7\n0.758\n0.803\n0.845\nfull\n(d)\n(b)\n(e)\n(c)\n(f)\nbaseline\nbaseline\ntest accuracy (%)\ntest accuracy (%)\nstudent-supervisor agreement (%)\nstudent-supervisor agreement (%)\nFigure 24: Synthetic experiment on simulation difficulty. We consider three types of weak errors\nin a linear probing setting: (a,d) perfectly simulatable, where weak models use a subset of strong\nmodel features; (b,e) completely unsimulatable, where the weak labels are obtained by applying\nrandom noise to the ground truth; (c,f) a mixture of the two settings, where label noise is applied\nto perfectly simulatable weak labels. Top row of panels shows test accuracy and bottom row shows\nagreement to the weak labels. In addition to weak label accuracy, the structure of mistakes plays a\nmajor role in weak-to-strong generalization.\nE.1\nSYNTHETIC EXPERIMENTS ON SIMULATION DIFFICULTY\nFirst, we consider a simplified linear probing setting, where we can ensure that the student can per-\nfectly simulate the supervisor predictions by construction. Specifically, we extract a representation\nX \u2208 Rn\u00d7d of the SciQ dataset using a model of an intermediate size in the GPT-4 family, where n\nis the number of datapints, and d is the dimensionality of the residual stream (Elhage et al., 2021).\nWe can then consider the family of linear models10 Mk where k \u2264 d by training a linear probe only\non the first k features extracted by the model. In particular, for k = d we recover the standard linear\nprobe. By construction for k1 \u2265 k2, the model Mk1 can perfectly simulate Mk2.\nNext, we can run our standard weak-to-strong generalization experiment, following the setup de-\nscribed in Section 3, using the family of models Mk. We train the weak supervisor models on 10k\ndatapoints, and produce hard weak labels on the remaining 13k datapoints. We report the results\nin Figure 24(a,d). In this setting, the simulation is very easy, and we do not observe substantial\nimprovements in the strong student model compared to the supervisor performance. The test agree-\nment values are substantially higher than the weak model accuracy, indicating that the students are\noverfitting to the supervisor errors. Interestingly, even in this simple setting the agreements are\nnot 100%, likely due to the fact that the student models are trained on finite data, and with light\nl2-regularization.\nWe can also consider the opposite setting: what if the student model cannot simulate the mistakes\nof the weak teacher at all? Specifically, we generate weak labels by randomly flipping the labels\nto match the accuracy of the weak models from the previous experiment. As a result, we get weak\nlabels with the same accuracy, but which are completely unpredictable. In Figure 24(b,e), when we\ntrain the student model on the these weak labels, we can get substantially higher accuracy than the\naccuracy of the weak labels. In other words, if the errors of the weak supervisor are completely\nunpredictable (random) for the student, with enough data we should be able to recover good gener-\nalization, substantially exceeding the performance of the supervisor.\n10We train logistic regression using the default parameters in the sklearn.linear_model.\nLogisticRegression class (Pedregosa et al., 2011) for this experiment.\n42\n[4]\nlongest prompt\nshortest prompt\nstrong g.t.\nmodel confidently correct\nweak supervisor\nstrong g.t.\nmodel unconfident\nrandom\n-100\n-75\n-50\n-25\n0\n25\n50\n75\n100\nperformance gap recovered (%)\naux. loss\nbaseline\n[6]\n-100\n-75\n-50\n-25\n0\n25\n50\n75\n100\nperformance gap recovered (%)\naux. loss\nbaseline\nlongest prompt\nshortest prompt\nstrong g.t.\nmodel confidently correct\nweak supervisor\nstrong g.t.\nmodel unconfident\nrandom\nlongest prompt\nshortest prompt\nstrong g.t.\nmodel confidently correct\nweak supervisor\nstrong g.t.\nmodel unconfident\nrandom\n-100\n-75\n-50\n-25\n0\n25\n50\n75\n100\nperformance gap recovered (%)\n[12]\naux. loss\nbaseline\nFigure 25: PGR for weak labels with same accuracy but different error structures. The inset\nnumber in each panel indicates the dataset (compare Figure 12). Weak-to-strong generalization and\nmethods both depend critically on the structure of the weak supervisor errors. While it is trivial to\npick error structures that generalize well (for instance, random noise), these error structures are also\nvery disanalogous to the ultimate superalignment setting, where we want to study the structures of\nhuman errors.\n43\nFinally, in Figure 24(c,f) we consider a mixture of these two settings: we start with a perfectly\nsimulatable weak model M300, and then add various amounts of label noise to the resulting weak\nlabels. By training a strong student model (using all features) on the resulting weak labels, we\nrecover the performance close to the performance of M300.\nDiscussion of results.\nThe simple experiment in this section suggests that in addition to the weak\nlabel accuracy, it is important to consider the structure of weak errors. In particular, if the weak er-\nrors are extremely easy for the strong model to simulate, the student may not generalize much better\nthan the weak supervisor with naive finetuning on the weak labels. On the other hand, if the mistakes\nof the weak supervisor are completely unpredictable, the student can denoise the predictions of the\nsupervisor and generalize better. In future work, we believe it is important to consider various types\nof weak supervision with different structures of mistakes, and build a better understanding of how\nthey affect weak-to-strong generalization.\nE.2\nDIFFERENT WEAK ERROR STRUCTURE MEANS DIFFERENT GENERALIZATION\nTo further explore the impact of different weak error structures, we created several synthetic sets of\nweak labels for each dataset, all with error rate identical to the weak model\u2019s error rate. To construct\nthese labels, we start from ground truth, and then flip a subset of labels to match the accuracy of a\nparticular weak model. We target a few types of error structures, such as pure noise, easy-to-model\nbias, hard-to-model bias, and adversarial bias.\nIn particular, we looked at:\n1. weak supervisor: the baseline \u2014 labels are generated in the same way as in the rest of\nthe paper\n2. random: flip the label of random datapoints\n3. longest prompt: flip the label of longest datapoints by characters\n4. shortest prompt: flip the label of shortest datapoints by characters\n5. strong g.t. model unconfident: flip the label of the datapoints that the strong ceil-\ning model is most unconfident on\n6. strong g.t. model confidently correct: flips the label of the datapoints that the\nstrong ceiling model is most confidently correct on\nDespite all of these weak labelers having the same weak accuracy, we find that the generalization\ncan vary wildly depending on the structure of the weak errors. We report the results in Figure 25.\nFurthermore, the dynamics of supervisor-student agreement through training can have qualitatively\ndifferent behavior (Figure 26). For errors coming from a weak model, we see that there is often ini-\ntially a period of generalization, followed by a period of overfitting where it learns the weak model\u2019s\nerrors. The confidence auxiliary loss mitigates this overfitting. For easy-to-fit error structures such\nas longest prompt, the overfitting happens much faster. For other kinds of errors, such as random\nnoise, we often see that generalization improves throughout: weak errors are not modeled, but the\nsignal from the weak model is.\nE.3\nMAKING IMITATION TRIVIAL\nOne possible major disanalogy in our setup, as discussed in Section 6.1, is the fact that our models\nare not very good at imitating the weak model11 (Section 5.1.3), but superhuman models may be\nvery good at imitating humans. It is possible that if the strong model were good at imitating the\nweak model, then it would generalize substantially less desirably by default.\nTo test an extreme version of this hypothesis, we create a synthetic setting where the strong model\ncan trivially imitate the weak model very well. In particular, we modify the task by appending \u201cI\nthink this is {weak label}. What do you think?\u201d to every prompt, where weak label is \u201ccorrect\u201d\nor \u201cincorrect\u201d based on the weak model prediction. In this case, the hardened weak label is present\nin-context, and the simulation is trivial.\n11Also known as learning the \u201chuman simulator\u201d in the terminology of Christiano et al. (2022).\n44\n0\n20\n40\n60\n80\n100\nweak supervisor\nlongest prompt\nshortest prompt\n0\n2\n0\n20\n40\n60\n80\n100\nstudent-supervisor\nagreement (%)\nrandom\n0\n2\nstrong g.t. model unconfident\n0\n2\nstrong g.t. model\nconfidently correct\nprogress (fraction of epoch)\n1\n0.5\n1.5\n1\n0.5\n1.5\n1\n0.5\n1.5\nstudent-supervisor\nagreement (%)\naux. loss\nbaseline\nweak supervisor correct\nweak supervisor wrong\n[6]\n0\n20\n40\n60\n80\n100\nweak supervisor\nlongest prompt\nshortest prompt\n0\n2\n0\n20\n40\n60\n80\n100\nstudent-supervisor\nagreement (%)\nrandom\n0\n2\nstrong g.t. model unconfident\n0\n2\nstrong g.t. model\nconfidently correct\nprogress (fraction of epoch)\n1\n0.5\n1.5\n1\n0.5\n1.5\n1\n0.5\n1.5\nstudent-supervisor\nagreement (%)\naux. loss\nbaseline\nweak supervisor correct\nweak supervisor wrong\n[4]\n0\n20\n40\n60\n80\n100\nweak supervisor\nlongest prompt\nshortest prompt\n0\n2\n0\n20\n40\n60\n80\n100\nstudent-supervisor\nagreement (%)\nrandom\n0\n2\nstrong g.t. model unconfident\n0\n2\nstrong g.t. model\nconfidently correct\nprogress (fraction of epoch)\n1\n0.5\n1.5\n1\n0.5\n1.5\n1\n0.5\n1.5\nstudent-supervisor\nagreement (%)\naux. loss\nbaseline\nweak supervisor correct\nweak supervisor wrong\n[12]\nFigure 26: Training dynamics change for different weak errors. We show teacher-student agree-\nment for different weak error structures on three datasets. We see that the training dynamics have\nqualitatively different behavior for different error structures, despite all weak labelers having the\nsame accuracy.\n45\n\u221220\n\u221210\n0\n10\n20\n30\n40\nperformance gap recovered (%)\n[11]\n[12]\n[10]\n[6]\n[8]\n[4]\n[1]\n[5]\n[7]\nNLP Tasks\naux. loss\nbaseline\nFigure 27: Generalization when emulating weak labels is trivial. Very little weak-to-strong gen-\neralization occurs if emulating the weak labels is trivial: average PGR across tasks is 0.002 \u00b1 0.003\nfor baseline, and 0.046 \u00b1 0.108 for aux loss, compared to around 0.2 and 0.8 respectively for the\noriginal tasks.\nAs expected, we find that both the baseline and the confidence loss introduced in Section 4.3 show\npoor weak-to-strong generalization (Figure 27) in most cases. Interestingly, the confidence loss still\nimproves upon the baseline achieving non-trivial generalization in several tasks.\nF\nHOW SHOULD WE EMPIRICALLY STUDY SUPERALIGNMENT,\nMETHODOLOGICALLY?\nWhat makes a setup good for studying superalignment in the first place, all things considered?\nTractability and ease of study are clearly important criteria, but also certainly not the only ones.\nThis question is non-obvious because superalignment is qualitatively different from other machine\nlearning problems: it is a problem we will face in the future, not a problem that we face today.\nNevertheless, it is crucial that we solve this problem before it becomes serious, as even a single\nfailure of superintelligence misalignment in practice could be catastrophic.\nThis presents a major methodological challenge: how do we even approach studying a problem that\nis not yet a problem? How do we make progress on the core difficulties of superalignment? How do\nwe make progress with today\u2019s systems, knowing that our efforts will not be wasted by surprising\nnew model capabilities that will inevitably arise in the future (Wei et al., 2022)? We do not claim to\nhave a complete answer to these questions, but we outline some best practices for maximizing our\nchances of making real progress on superalignment.\nAnalogous setups.\nWe should construct increasingly analogous empirical setups, and we should\nenumerate any remaining disanalogies. A setup is analogous if our results on that setup do not rely\non assumptions that will break down in the future, making results today likely qualitatively similar\nto results in the future. Our main evaluation setup, introduced in Section 3, is intended to be more\nanalogous to the superalignment problem. We enumerate some remaining disanalogies with our\nsetup in Section 6.1.\nEnumerating assumptions.\nWe should enumerate the key assumptions that our results (either\nimplicitly or explicitly) rely on. Clarifying what assumptions we are making makes it much easier\nto know when our results might break down. We enumerate our main disanalogies and assumptions\nin Section 6.1 and Appendix G.3.\nSensitivity analysis.\nWe should evaluate the sensitivity of our results to changes in our assump-\ntions and empirical setup. While we can make informed guesses about the future, we do not know\nexactly what future models will be like, so it is difficult to entirely trust any particular experimen-\ntal setup. Validating that our results are robust to many different sets of assumptions can make us\nsubstantially more confident our results will transfer to the future superalignment problem. We do\nsome initial sensitivity analysis in Appendix E, and intend to do much more in future work.\nScalable techniques.\nWe should avoid techniques that rely on assumptions that will likely break\ndown for future (superhuman) models. For example, when we do few-shot prompting we are in-\n46\ntuitively incentivizing models to predict some useful distribution of human text, whereas when we\ndo finetuning we are intuitively incentivizing a model to output what it knows regardless of how it\nknows it. This is one of the reasons we focus on finetuning methods in this paper: they are more\nlikely to scale to superhuman models compared to prompting.\nIncidental usefulness today.\nOne possible validation that progress on our setup is real would\nbe to show that it is incidentally useful in practice today; while we advocate focusing on the core\nchallenges of superalignment, if our findings are never useful with today\u2019s models that would be\nevidence that we are not on the right track. One example of a near-term practical milestone would\nbe to align GPT-4 on instruction-following tasks using only GPT-3-level supervision; if we could get\nstrong alignment without any humans involved at all, that would make alignment much simpler and\ncheaper today. However, usefulness today is certainly not sufficient for aligning superintelligence,\nand in general a common failure mode of empirical alignment research is it prioritizes usefulness\ntoday at the expense of analogousness and scalability.\nUpdating over time.\nWe should update our evaluations and validate past findings as we learn\nmore about what future models will look like. While we focus on the pretrained language model\nparadigm today, we plan on updating our setup if or when this stops being the dominant paradigm.\nG\nHOW WEAK-TO-STRONG GENERALIZATION FITS INTO ALIGNMENT\nSuperintelligent AI systems will be extraordinarily powerful; humans could face catastrophic risks\nincluding even extinction (CAIS) if those systems are misaligned or misused. It is important for\nAI developers to have a plan for aligning superhuman models ahead of time\u2014before they have the\npotential to cause irreparable harm.\nOur plan for aligning superintelligence is a work in progress, but we believe that weak-to-strong\ntechniques could serve as a key ingredient. In this section we sketch several illustrative possiblities\nfor how we could use weak-to-strong generalization to help align superintelligent systems.\nG.1\nHIGH-LEVEL PLAN\nLeike & Sutskever (2023) propose the following high level plan, which we adopt:\n1. Once we have a model that is capable enough that it can automate machine learning\u2014and\nin particular alignment\u2014research, our goal will be to align that model well enough that it\ncan safely and productively automate alignment research.\n2. We will align this model using our most scalable techniques available, e.g. RLHF (Chris-\ntiano et al., 2017; Ouyang et al., 2022), constitutional AI (Bai et al., 2022b), scalable over-\nsight (Saunders et al., 2022; Bowman et al., 2022), adversarial training, or\u2014the focus of\nthis paper\u2014-weak-to-strong generalization techniques.\n3. We will validate that the resulting model is aligned using our best evaluation tools available,\ne.g. red-teaming (Perez et al., 2022a;b) and interpretability (Ribeiro et al., 2016; Olah et al.,\n2018; Bills et al., 2023; Li et al., 2023).\n4. Using a large amount of compute, we will have the resulting model conduct research to\nalign vastly smarter superhuman systems. We will bootstrap from here to align arbitrarily\nmore capable systems.\nThe goal of weak-to-strong generalization is to ensure step (2) is solved: align the first model ca-\npable of automating machine learning and alignment research. Importantly, this first model will\nlikely be qualitatively superhuman along important dimensions, so RLHF is unlikely to be sufficient\n(Section 4). If we had a superhuman model, how would we apply weak-to-strong generalization to\nalign it?\n47\nG.2\nELICITING KEY ALIGNMENT-RELEVANT CAPABILITIES WITH WEAK-TO-STRONG\nGENERALIZATION\nThere are many different alignment-relevant capabilities we could try to elicit from a superhuman\nmodel that could significantly help with alignment, including:12\n\u2022 Safety: does a given behavior produced by an AI system risk the safety of human lives or\nwell-being in important ways?\n\u2022 Honesty: is a given natural language statement true or false?\n\u2022 Instruction following: does a given behavior produced by an AI system follow a user\u2019s\ninstruction faithfully?\n\u2022 Code security: does some given code have important security vulnerabilities or back-\ndoors? Is it safe to execute it?\nIn the ideal case, the capability we elicit from the model would be robust enough that we can turn it\ninto a reward model and safely optimize it; future work should assess the feasibility of this approach.\nAt the opposite extreme, we could potentially use the elicited capability as an \u201coracle\u201d that we can\nmanually query; intuitively, if we had a superhuman oracle model, we may be able to leverage it to\nhelp us bootstrap to a more robust alignment solution, even if that oracle is not itself entirely robust.\nG.3\nALIGNMENT PLAN ASSUMPTIONS\nMany alignment plans which appear different on the surface actually depend on heavily correlated\nassumptions. For a given alignment plan, it is also often unclear which subproblems the plan at-\ntempts to solve, and which subproblems the plan assumes are unlikely to be an obstacle. As a result,\nwe think enumerating assumptions is an important part of making progress on alignment.\nIn addition to the major disanalogies discussed in Section 6.1, the assumptions we make for an\nalignment plan based on weak-to-strong generalization include:\n\u2022 No deceptive alignment in base models. We assume that pretrained base models (or the\nequivalent in future paradigms) will be highly intelligent but not highly agentic (e.g. will not\nhave long-term goals)\u2014and consequently will not be deceptively aligned (Hubinger et al.,\n2019; Ngo et al., 2022; Carlsmith) out-of-the-box. Our goal is to elicit the superhuman\ncapabilities of this capable but safe base model, and use those capabilities to create an\naligned (possibly agentic) superhuman model.\n\u2022 Elicited concepts are sufficiently robust, or do not need to be. We assume it is ei-\nther possible to solve alignment using only a small amount of optimization applied to the\ncapabilities we elicit, or that it is possible to make weak-to-strong elicited capabilities suf-\nficiently robust against overoptimization.\n\u2022 The concepts we care about are natural to future AGI. The superhuman base model we\napply weak-to-strong generalization to has some \u201calignment-complete\u201d concept, such as\nhonesty, that is extrapolated in the way we would endorse if we could understand everything\nthe superhuman model understands, and which is natural enough to the model that it is\nfeasible to elicit.\n\u2022 Sufficiently gradual takeoff. Before we have superintelligence, we will have somewhat\nsuperhuman models long enough that we can use them to finish solving the full superintelli-\ngence alignment problem. We can use it to solve superalignment before it causes recursive\nself improvement or catastrophic damage.\n\u2022 Moderately superhuman models are sufficient to solve alignment. We assume the first\nmodels capable of automating alignment research in practice are moderately superhuman,\ni.e. in a regime similar to what we study empirically in this work. For example, we may\nassume that we only need to bridge a weak-strong gap of at most (say) 4 OOMs of effective\ncompute.\n12Ideally we elicit several related concepts and verify that we get consistent answers between them.\n48\n\u2022 No need to solve human values. We assume we do not need to solve hard philosophi-\ncal questions of human values and value aggregation before we can align a superhuman\nresearcher model well enough that it avoids egregiously catastrophic outcomes.\nThis list represents a non-exhaustive set of notable assumptions we often operate under, and we\nwill constantly reassess and update these assumptions over time as we learn more. We do not think\nthese are necessarily valid assumptions by default, and believe it is important to validate them, work\ntowards making them true, or mitigate failure modes from them being invalid.\nFurthermore, there are a huge number of uncertainties about what future AI systems will look like\nand exactly how we should align them.\n49\n"
  },
  {
    "title": "ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent",
    "link": "https://arxiv.org/pdf/2312.10003.pdf",
    "upvote": "29",
    "text": "REST\nMEETS REACT:\nSELF-IMPROVEMENT\nFOR\nMULTI-STEP REASONING LLM AGENT\nRenat Aksitov\u20201, Sobhan Miryoosefi\u20201, Zonglin Li\u20201, Daliang Li\u20201, Sheila Babayan\u20202,\nKavya Kopparapu\u20202, Zachary Fisher1, Ruiqi Guo1, Sushant Prakash1, Pranesh Srinivasan3,\nManzil Zaheer2, Felix Yu1, and Sanjiv Kumar1\n1Google Research, 2Google DeepMind, 3Google\n\u2020Core contributors\nABSTRACT\nAnswering complex natural language questions often necessitates multi-step rea-\nsoning and integrating external information.\nSeveral systems have combined\nknowledge retrieval with a large language model (LLM) to answer such questions.\nThese systems, however, suffer from various failure cases, and we cannot directly\ntrain them end-to-end to fix such failures, as interaction with external knowledge\nis non-differentiable. To address these deficiencies, we define a ReAct-style LLM\nagent with the ability to reason and act upon external knowledge. We further\nrefine the agent through a ReST-like method that iteratively trains on previous tra-\njectories, employing growing-batch reinforcement learning with AI feedback for\ncontinuous self-improvement and self-distillation. Starting from a prompted large\nmodel and after just two iterations of the algorithm, we can produce a fine-tuned\nsmall model that achieves comparable performance on challenging compositional\nquestion-answering benchmarks with two orders of magnitude fewer parameters.\n1\nINTRODUCTION\nFigure 1: Agent self-improvement and self-distillation.\nBamboogle auto-eval, mean accuracy and standard de-\nviation over 10 runs, (%)\nFor many simple natural language tasks,\nlike basic question-answering or summa-\nrization, we can relatively easily decide\nwhether the final output is good or bad,\ncollect large amounts of such data, and\ntrain the language models using these out-\ncomes as feedback. At the same time, for\nmore complex problems, outcome-based\nsystems are often insufficient, and a pro-\ncess supervision approach has recently\ngained much attention as a more promis-\ning alternative (Reppert et al. (2023)).\nThere is explosive growth in techniques\n(Gao et al. (2023); Madaan et al. (2023)),\nframeworks (Dohan et al. (2022); Khattab\net al. (2023b)), and libraries (Liu (2022),\nChase (2022)) for defining process-based\nworkflows with LLMs through human-understandable task decompositions. Many such decomposi-\ntions involve interaction with external tools / APIs / environments, in which case the corresponding\nmulti-step workflow is generally referred to as an LLM agent (Xi et al. (2023)), a system capable of\nperforming a sequence of actions to achieve a goal.\nLet\u2019s consider the task of answering complex, open-ended questions, where the agent needs to use a\nsearch API to look up multiple pieces of information before composing a paragraph-length answer.\nOne popular approach for building such agents with LLMs is the ReAct method (Yao et al., 2022),\nwhich involves interleaving chain-of-thought reasoning with actions and observations during sev-\n1\narXiv:2312.10003v1  [cs.CL]  15 Dec 2023\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\neral thought-action-observation rounds. In this work, we follow the general ReAct format for our\nSearch Agent while designing the corresponding few-shot prompts to produce long-form, explicitly\nattributable final answers (cf. Nakano et al. (2021)).\nIt is natural to ask next how to deal with failure cases of such an agent and how to improve its\nperformance and robustness. For outcome-based systems, the solution is usually straightforward:\nwe just collect more human-labeled data. However, acquiring such data is much more challenging\nand expensive for process-based systems: a significantly larger amount of data is needed (Uesato\net al. (2022); Lightman et al. (2023)), and it is generally harder for humans to determine an optimal\nmulti-step trajectory.\nTo address the lack of and difficulty in obtaining multi-step human-labeled data, we focus on improv-\ning the quality of the agent with self-critique, AI feedback, and synthetic data generation. Specifi-\ncally for the latter, we adapt the recently proposed (Gulcehre et al. (2023)) Reinforced Self-Training\n(ReST) algorithm towards agentic setups. The inner-outer loop flow of ReST remains the same:\nin the outer loop (\u201cgrow\u201d), the dataset is grown by sampling from the latest policy, and in the in-\nner loop (\u201cimprove\u201d), the policy is improved on a fixed dataset via ranking or filtering with reward\nmodel. In our case, sampling during \u201cgrow\u201d means producing a multi-step trajectory to completion,\nand ranking as part of \u201cimprove\u201d is done directly with LLM call rather than with a distilled reward\nmodel of human preferences.\nWe measure the overall performance of the Search Agent by its ability to answer diverse composi-\ntional questions that were manually verified to be unanswerable directly by a search engine (Bam-\nboogle dataset (Press et al., 2023) and a sequel dataset that we constructed ourselves, BamTwoogle).\nWhile both datasets are small, they have enough statistical power to capture the effects we are inter-\nested in studying. For example, Figure 1 provides a high-level overview of the agent performance\non Bamboogle for different model sizes and showcases the boost with more iterations of ReST (all\nour synthetic data is produced with the large model, so for smaller models, it is the performance of\nthe distillation).\nTo summarize, our contributions are the following:\n\u2022 We build a flavor of ReAct agent with self-critique for the task of long-form question an-\nswering.\n\u2022 We define a proxy evaluation metric for the agent based on Bamboogle and BamTwoogle\ndatasets, with a strong emphasis on auto-eval.\n\u2022 We demonstrate that the performance of the agent could be effectively improved through\nRest-style iterative fine-tuning on its reasoning traces.\n\u2022 Furthermore, we do it purely from stepwise AI feedback without using human-labeled\ntraining data.\n\u2022 Finally, we show that the synthetic data produced as part of this iterative process could\nbe used for distilling the agent into one or two orders of magnitude smaller models with\nperformance comparable to the pre-trained teacher agent.\n2\nBACKGROUND: SEARCH AGENT\nThis section describes Search Agent, a flavor of ReAct (Yao et al., 2022) agent with Reflexion (Shinn\net al., 2023). It uses web search as a tool to generate long-form, explicitly attributable answers for\ndiverse knowledge-seeking open-ended questions. The agent\u2019s flow proceeds as follows (Figure 2):\n1. Agent receives a question and starts executing a search loop:\n\u2022 Agent decides whether it needs additional information to answer the question.\n\u2022 If \u201cyes\u201d, it calls the search tool, summarizes the received snippets, and goes back to\nthe decision step.\n\u2022 If \u201cno\u201d, it terminates the search loop.\n2. Based on the information collected as part of the search loop, the agent generates the first\nattempt (draft) of the answer.\n3. It then performs two additional self-revision calls before producing the final answer:\n2\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n\u2022 One to verify that the answer is relevant to the original question,\n\u2022 And another to check that the answer is grounded in the retrieved snippets.\nIncoming Question\nDecision Step\nWeb Search\nTool Output Summarization\nAnswer Generation\nRelevance Self-Check\nGrounding Self-Check\nFinal Answer\nTool Call\nTerminate?\nFigure 2: A state machine of the Search Agent flow. Each blue shape corresponds to a single LLM\ncall and defines a separate type of the reasoning step.\n3\nMETHODS\n3.1\nPROMPTING\nWe first define the prompted flow for the Search Agent by manually constructing few-shot prompts\nfor each of the five reasoning steps from Figure 2. Notably, we choose to format our prompts as\nPython code (see Appendix for the prompts of different steps, Listings 1 - 6).\nThe decision to use the \u201dcode as the prompt\u201d approach is motivated by the following observations:\n\u2022 There is often a need to parse the LLM\u2019s output for integration with other systems and\ntools, which is much easier to do if the model\u2019s input and output are well-structured.\n\u2022 At the same time, code uniquely combines a structured aspect (keywords and syntax) with\na natural language aspect (comments and descriptive naming).\n\u2022 Moreover, LLMs are capable of both reading and writing code.\nTo summarize, since code is naturally structured and easy to parse, it could serve as an excellent\nmedium for communication with the model. Accordingly, within the \u201dcode as the prompt\u201d paradigm,\nLLM is expected to understand the code in the input and continue it as valid Python in the output,\nwhich turns out to be challenging tasks for the smaller models. Out of the (pre-trained) models\nwe\u2019ve tried, only the PaLM 2-L (Anil et al., 2023) can perform it consistently well. Hence, this is\nthe model that we use to produce reasoning trajectories from few-shot prompts.\n3.2\nIMPLEMENTATION DETAILS\nTo run Search Agent, we use PaLM 2 \u201cbase\u201d models of different sizes (XS, S and L), both pre-\ntrained and fine-tuned. We usually produce multiple samples (with T = 0.5, see Section 4.2 for\ndetails on temperature selection) for each step and then select the one with the lowest perplexity (4\nsamples for L and S, 16 for XS) to continue the trajectory forward. As a search tool, we use internal\nGoogle Q&A API that returns top-k snippets, and we typically request top-3 snippets per query\nfrom it. We further limit the number of total searches to at most 10 per single agent trajectory (via\nthe REMAINING SEARCHES field in the prompts). Search Agent preserves the trajectory state by\nstoring all the actions taken so far in the PAST ACTIONS field.\n3\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n3.3\nINPUT DATA\nWe use the following four datasets to provide initial questions for Search Agent trajectories:\n\u2022 HotpotQA (Yang et al., 2018), a multi-hop reasoning QA dataset, where the system has to\nreason with information taken from more than one document to arrive at the answer.\n\u2022 Eli5 (Fan et al., 2019), a dataset for long-form question answering (LFQA), a task that\nrequires elaborate and in-depth answers to open-ended questions. The dataset was built\nfrom the Reddit forum \u201cExplain Like I\u2019m Five\u201d (ELI5), r/explainlikeimfive.\n\u2022 Eli5-askH (Blagojevic, 2022), similar to above, but built from the Reddit forum r/askhis-\ntorians, a subreddit where users may ask questions or start discussions about history.\n\u2022 Eli5-askS (Blagojevic, 2022), as above, but from a subreddit r/askscience (\u201cask a science\nquestion, get a science answer\u201d).\nWe randomly selected 500 questions from the training splits of each dataset and ended up with 2000\ndiverse, challenging questions in total. We don\u2019t use any other information from these datasets, like\nlabels (e.g., we don\u2019t do any filtering by matching the correct answer from HotpotQA, and we don\u2019t\nuse these datasets\u2019 validation splits for hyperparameters tuning or performance assessment).\n3.4\nFINE-TUNING\nWe simply split each completed Search Agent trajectory into the reasoning steps and build a fine-\ntuning mixture with those steps. We use full fine-tuning for all the experiments. Given that fine-\ntuning costs increase sharply for larger models, we do as many experiments as possible with XS\nmodel.\n3.5\nRANKING \u201dREWARD\u201d MODEL\nAs mentioned previously, we produce multiple samples for each reasoning step in the agent\u2019s trajec-\ntory and typically choose the sample that minimizes perplexity to continue the trajectory forward or\nto build a fine-tuning mixture. At the same time, we might be able to do better than that by utilizing a\nmore sophisticated way of selecting the best sample. To this effect, we employ an instruction-tuned\nPaLM 2-L and prompt it with the model input, multiple sampled outputs, and guidance on how to\nrank them (the prompt is available in the Appendix, Listing 8). We can then use the highest-ranked\nsample for fine-tuning instead of the default sample chosen based on the perplexity value.\nIn this part, our approach differs from ReST, which uses threshold-based filtering with a reward\nmodel (RM) trained on human preference data. What we do is closer to RAFT (Dong et al., 2023),\nwhere the reward model ranks sampled responses to select high-scoring subsets for model fine-\ntuning, and the RM rankings matter much more than absolute scores. Notably, we mainly do the\nLLM-based selection off-policy, i.e., by updating the current action used for fine-tuning. The on-\npolicy trajectory rollouts use perplexity.\n3.6\nITERATIVE SELF-IMPROVEMENT\nNow we have all the pieces for the self-improvement algorithm:\n\u2022 Start with a model capable of performing Search Agent task at a certain level, for example,\nwith prompted PaLM 2-L model. Collect reasoning trajectories from this model based on\nour set of 2000 initial questions (essentially the \u201cgrow\u201d stage of ReST, with the difference\nthat we keep the set of initial questions fixed).\n\u2022 Convert the trajectories into the fine-tuning mixture. Apply re-ranking with RM during the\nconversion (this is roughly equivalent to the \u201cimprove\u201d stage of ReST, though we only do\none iteration of \u201cimprove\u201d).\n\u2022 Fine-tune the new model (of the same size) on this mixture and verify that it\u2019s performing\nbetter than the original model (we will discuss how to do it in the following section). Repeat\nthe process, starting with this new, better model.\n4\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nFinally, we can also train smaller models on the fine-tuning data from the different iterations of\nself-improvement, which will naturally give us a self-distillation algorithm.\n4\nEVALUATION\n4.1\nBAMBOOGLE\nOur primary evaluation vehicle is the Bamboogle dataset (Press et al., 2023). It is a semi-adversarial\ndataset of 2-hop questions (125 in total) that were selected to be unanswerable by direct Google\nsearch, but where both required pieces of evidence could be found in Wikipedia. When the Search\nAgent\u2019s performance on Bamboogle is improving, we can assume that it generally becomes better\nin using search as a tool.\nGiven the open-ended nature of the answers generated by the Search Agent (Figure 3), we can\u2019t\nreliably use the exact match as a metric. Instead, we either manually judge correctness or run auto-\neval with a separate call to the PaLM 2-L \u201cbase\u201d model.\n4.2\nAUTO-EVAL\nQuestion: What is the maximum air-\nspeed (in km/h) of the third fastest bird?\nModel Answer: The golden eagle\nis the third fastest bird. According\nto [link id=4, 5, 6], its maximum\nairspeed is 200mph, which is 320kph.\nGround truth: 320 km/h\nFigure 3: A Bamboogle question with an ex-\nample of the long form answer by the model\nWhile strict human evaluations are preferable, they\nare time-consuming. They also don\u2019t scale well (do-\ning one human eval is much easier than doing five),\nwhich leads to a high variance of such evals in our\ncase: the agent\u2019s trajectories are stochastic (as a\nreminder, we use non-zero temperature when sam-\npling reasoning steps), but we can\u2019t easily reduce the\nvariance by increasing the number of repetitions per\nquestion with human evals.\nWe solve both of these problems by introducing\nLLM-based auto-eval (the full auto-eval prompt is\navailable in the Appendix, Listing 7) and by verify-\ning its alignment with human ratings for the Bam-\nboogle trajectories on which we previously con-\nducted human evals.\nWe compared the auto-eval\nwith human evaluations on a diverse set of agents\nand found it highly correlated with human evalua-\ntion scores. Specifically, the Pearson correlation is\n0.98 with p = 6.6 \u00d7 10\u22128 and Spearman correla-\ntion is 0.83 with p = 0.0015. Given that it is much\ncheaper to run auto-eval, we can now use a large number of repetitions to reduce variance. We typ-\nically aggregate auto-eval over ten repetitions (i.e., by producing ten different trajectories for each\nBamboogle question).\nFirst and foremost, we use Bamboogle auto-eval to estimate the final model performance but also to\nanswer various questions that one would typically use a validation set for:\n\u2022 What is the optimal sampling temperature for the agent? (T = 0.5)\n\u2022 Which checkpoints should we choose for different model sizes? (step 9K for XS, 5K for S,\n3.5K for L)\n\u2022 Should we proceed with another iteration of self-improvement?\n\u2022 What is the performance impact of using several trajectories per question on the fine-tuned\nmodel?\n\u2022 Should we use self-checks? Are they helping or hurting the results? (helping slightly, see\nFigure 4)\nImportantly, we never use Bamboogle as a training set, as we neither tune our prompts on Bamboogle\nnor use questions from it to generate fine-tuning trajectories.\n5\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\n4.3\nBAMTWOOGLE\nFigure 4: Agent self-improvement and self-distillation,\nwith and without self-critique steps. Bamboogle auto-\neval, mean accuracy and standard deviation over ten\nruns, (%)\nGiven the small size of Bamboogle and\nour use of it as an analog of the valida-\ntion set for Search Agent, there is a dan-\nger of overfitting. To guard against such\na possibility, we introduce a new dataset,\nBamTwoogle, to serve as a test set. We\nuse BamTwoogle exclusively to measure\nthe final performance of the models.\nThe BamTwoogle dataset was written to\nbe a complementary, slightly more chal-\nlenging sequel to Bamboogle. It also ad-\ndresses some of the shortcomings of Bam-\nboogle we discovered while performing\nhuman evals. Specifically, we ensured that\nall the questions required 2+ steps to an-\nswer. Due to changes in the search algo-\nrithm, this is no longer the case with all\nBamboogle questions, and it is possible to\nsolve some of them with a single search.\nLike Bamboogle, BamTwoogle is a small (100 questions in total), handcrafted collection of\ninformation-seeking questions. The topics and question formats vary, but in general, BamTwoogle\nadheres to the following guidelines\nQuestions\n\u2022 The majority of questions require two searches or reasoning steps (like Bamboogle), but\nsome of them need 3 or 4\n\u2022 Must have been manually checked to ensure the answer doesn\u2019t appear on the first page of\nGoogle search results\nExpected answers\n\u2022 Should not be ambiguous\n\u2022 Should not be prone to change over time, either due to the phrasing of the question or to\nthe nature of the answer\n\u2022 Should account for multiple versions of proper names, etc., where appropriate\n\u2022 Should prefer Wikipedia as the source of truth for facts (preference given to topics/articles\nnot flagged for incompleteness, lack of sources, etc.)\n5\nEXPERIMENTS\n5.1\nPILOT\nTable 1: Agent self-improvement and self-distillation,\nBamboogle auto-eval, mean accuracy and standard de-\nviation over 10 runs, (%)\nTraining Data\nXS\nS\nL\nPre-trained\nN/A\nN/A\n70.3\u00b13.5\nPilot, human filtered\n44.7\u00b13.1\n56.6\u00b13.8\n71.5\u00b12.2\nSelf-improvement, 1st gen\n54.4\u00b13.6\n61.9\u00b11.9\n74.0\u00b13.3\nSelf-improvement, 2nd gen\n65.9\u00b12.6\n69.7\u00b11.3\n76.1\u00b11.3\nAlongside\nthe\nmain\nself-\nimprovement\nsetup,\ndescribed\nin\nSection 3, we also generate a simpler\n\u201cpilot\u201d data with 500 trajectories,\nwhere the initial questions are se-\nlected\nfrom\nHotpotQA\nand\nEli5\ndatasets only (i.e., smaller and with-\nout Eli5-askH or Eli5-askS). We use\nthe default (based on a min perplex-\nity, no RM re-ranking) best actions\nfor building the pilot\u2019s fine-tuning\nmixture. Furthermore, we manually\n6\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nreview the fine-tuning data and filter\nout about 30% of the examples that are \u201dbad\u201d in some way: an unhelpful query, empty thoughts,\nsummary missing important information, etc. This pilot data serves as a fine-tuning baseline.\n5.2\nSELF-IMPROVEMENT AND SELF-DISTILLATION\nThe main results are presented in Figure 1 and Table 1. As discussed previously, each cell in the table\nshows a mean (with the corresponding std) of auto-eval over 10 Bamboogle runs for the particular\nmodel.\nTable 2: Human evals, Bamboogle and BamTwoogle, acc (%)\nPre-trained (L)\n2nd gen (XS)\n2nd gen (S)\n2nd gen (L)\nBamboogle\n68.8\n67.2\n68.0\n74.4\nBamTwoogle\n68.0\n63.0\n63.0\n74.0\nWe\nstart\nwith\na\npre-\ntrained (prompted) PaLM\n2-L model and use it to\ngenerate\nboth\n500\npilot\ntrajectories and (indepen-\ndently)\n2000\ntrajectories\nfor 1st iteration (\u201d1st gen\u201d)\nof self-improvement.\nWe\nthen fine-tune PaLM 2-L,\nPaLM 2-S, and PaLM 2-XS models on the resulting mixtures.\nNext, we use the PaLM 2-L model fine-tuned on 1st gen data to generate trajectories for the 2nd\niteration (\u201d2nd gen\u201d) of self-improvement. We build 2nd gen data with 8000 trajectories (using the\nsame 2000 initial questions, each repeated four times; analogous to a \u201dgrow\u201d stage in ReST). As\npreviously, we fine-tuned each of the three models on the new 2nd gen mixture.\nAs a final verification, we also do human evals over a single Bamboogle and BamTwoogle run for\neach of the 2nd gen models (Table 2), as well as the original pre-trained PaLM 2-L model for\ncomparison.\n5.3\nABLATIONS\nWhat is the effect of human filtering?\nSurprisingly, we have found that fine-tuning on filtered\ndata results in a small performance drop (2.5%) versus unfiltered pilot mixture (Table 3, pilot\ncolumns). We hypothesize that this happens due to a combination of 2 factors:\n\u2022 The reduced size of the filtered mixture makes it harder for the model to learn the proper\nformat of the prompts,\n\u2022 Our filtering only affects the immediate \u201dbad\u201d example, not the whole trajectory; the\n\u201dbad\u201d step would often be preserved in the other fine-tuning examples as part of the\nPAST ACTIONS field.\nShould we use multiple trajectories per question?\nTurns out, it helps to use two trajectories per\nquestion instead of 1 (2.2% gain) in the fine-tuning mixture, but more than that doesn\u2019t improve\nperformance significantly (Table 3, 2nd gen columns).\nTable 3: Impact of the quality and size of the training data for fine-tuned PaLM 2-XS models\nPilot, human\nfiltered\nPilot,\nunfiltered\n1st gen\n2nd gen (1x)\n2nd gen (2x)\n2nd gen (4x)\nTotal trajectories\n500\n500\n2000\n2000x1\n2000x2\n2000x4\nTraining examples\n3015\n4518\n17970\n18007\n36238\n72424\nBamboogle auto-eval\n44.7\u00b13.1\n47.2\u00b13.1\n54.4\u00b13.6\n63.4\u00b11.7\n65.6\u00b11.8\n65.9\u00b12.6\nMore data vs better data.\nFrom the same Table 3, we can conclude that the quality of the data\n(e.g., 9% gain, when going from 1st gen to 2nd gen (1x) while keeping the size of the data roughly\nthe same) matters more than its quantity. Notably, better data also reduces the variance of evaluation\ntrajectories.\n7\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nEffect of self-critique.\nThe multi-step setup of the agent allows us to easily measure the effect of\nthe self-critique steps on the overall agent performance. To do this, we simply take all the Bamboogle\ntrajectories used for Table 1 and run auto-eval on them, but over the \u201dAnswer Generation\u201d step,\nrather than over \u201dFinal Answer\u201d, as we normally do (Figure 2). As seen from Figure 4, the self-\ncritique results in a small but measurable positive boost (on the order of 0.5-1.0% for most models).\nThe detailed numbers are provided in Table 4 in the Appendix.\n6\nDISCUSSION\nProcess Supervision.\nIt\u2019s important to emphasize again that we don\u2019t use labels from the training\ndata as a signal during trajectory collection. This is possible due to combining a process-based\napproach (i.e., defining agent as a state machine) with high-temperature exploration, AI feedback\n(zero-shot \u201dreward\u201d model used for actions re-ranking), and state-wise fine-tuning over completed\ntrajectories. In other words, in this setup, the model can learn something useful even from the\nstates that eventually lead to the wrong final answer. More importantly, it learns to handle open-\nended questions that don\u2019t have a single well-defined correct answer in the first place, akin to many\nquestions from Eli5.\nAuto-Eval.\nSome of the properties listed above, like non-greedy sampling and long-form answer\ngenerations in particular, bring with them additional challenges in terms of agent evaluation. We\nboth need to measure the quality of the long-form final answer for a specific trajectory and account\nfor stochasticity between different agent trajectories for the same input. This, on the one hand,\nelevates the value of having robust auto-eval aligned with human raters and, on the other, increases\nthe computational costs significantly due to the need to run agent trajectories multiple times, as well\nas the use of PaLM 2-L model for auto-eval.\nSelf-Critique.\nDespite those computational costs, multiple benefits come from having reliable\nauto-eval in terms of measuring the impact of various agent\u2019s hyperparameters. As an example,\nlet\u2019s consider the self-critique steps. With the aid of auto-eval, we were able to establish that they\nhave a small but positive effect on the overall performance of our multi-step reasoning setup. This\ncontrasts the recent observations (Huang et al., 2023) that adding self-critique hurts the performance\nin an outcome-based CoT setup. Simultaneously, we can also notice that a positive effect of self-\ncritique depends on the model size (larger for larger models) but does not seem to be affected by the\nself-improvement process.\nLimitations and Future Directions.\nWhile this work lays a foundation for reasoning agents\u2019\nself-improvement, it has numerous limitations due to computational and time constraints: manually\nconstructed prompts, small evals, a limited set of models, and only a single tool, to name just a few.\nFuture work could explore if the same self-improvement algorithm applies to multiple tool settings\nand, especially, if the ability to handle unseen tools could be improved in such a way. If the latter is\nmore similar to self-critique and doesn\u2019t improve under ReST-like iterative training, what changes\nare required to enable self-improvement for both?\nAnother open question is a saturation point. How many additional iterations of self-improvement\ncan we undertake past the 2nd one that still results in non-trivial gains? What does the saturation\nlook like for smaller models? Will they all eventually converge to the same performance, or will the\nsmaller models always be capped by the performance of the initial prompted large model?\n7\nRELATED WORK\nFollowing WebGPT (Nakano et al., 2021), we are tackling the task of long-form question answering\n(Krishna et al., 2021), in which the language agent uses web search as a tool to generate final\nanswers with explicit references for the retrieved passages. While WebGPT is focused on imitation\nlearning and RL from a large number of human demonstrations, our work aims to minimize human\ninvolvement. The only labeled demonstrations we use as part of the training are few-shot exemplars\nin the prompts for the agent\u2019s reasoning steps (see Appendix, A.1).\n8\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nSetting up language agents with manually designed few-shot prompts is the most common practice\n(Press et al. (2023); Yao et al. (2022); Shinn et al. (2023)), but there are some exceptions. For exam-\nple, DSP (Khattab et al., 2023a) tunes the few-shot demonstrations for the prompts automatically,\nleveraging some amount of labeled training examples for optimization purposes, and can further\nfine-tune specific components of the agent.\nUnlike prompting, agent\u2019s fine-tuning is done less often (Nakano et al. (2021); Yao et al. (2022);\nChen et al. (2023)). The closest to our fine-tuning setup is probably FireAct (Chen et al., 2023),\nwith the main difference being that we don\u2019t use human labels for training or data filtering. Instead,\nwe are building synthetic data with self-improvement from AI feedback.\nSome relevant papers for self-improvement include STAR (Zelikman et al., 2022), ReST (Gulcehre\net al., 2023), ReSTEM (Singh et al., 2023), and RAFT (Dong et al., 2023). Unlike STAR and\nReSTEM, we don\u2019t use the correctness of the answer as a signal. And, unlike ReST and RAFT, we\ndon\u2019t have the proper reward model trained on human preferences. Moreover, all 4 of these papers\ntarget outcome-based systems, while we focus on a process-based one.\n8\nCONCLUSION\nThis work demonstrates that the ReST-like approach with AI feedback could be effectively applied\nto a multi-step reasoning LLM agent. We show that it is a relatively simple and efficient way to\niteratively build high-quality synthetic data for agent self-improvement. Moreover, this increasingly\nhigher quality data could simultaneously be used for distilling a multi-step agent into several mag-\nnitudes smaller models while preserving most of the performance from the large teacher model.\nREFERENCES\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report.\narXiv preprint arXiv:2305.10403, 2023.\nVladimir\nBlagojevic.\nLong-form\nqa\nbeyond\neli5:\nan\nupdated\ndataset\nand\napproach,\n2022.\nURL\ntowardsdatascience.com/\nlong-form-qa-beyond-eli5-an-updated-dataset-and-approach-319cb841aabb.\nHarrison Chase. Langchain. https://github.com/hwchase17/langchain, 2022.\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao.\nFireact: Toward language agent fine-tuning, 2023.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Murphy, and\nCharles Sutton. Language model cascades, 2022.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan Zhang, Winnie Chow, Rui Pan, Shizhe Diao,\nJipeng Zhang, Kashun Shum, and Tong Zhang. Raft: Reward ranked finetuning for generative\nfoundation model alignment, 2023.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jason Weston, and Michael Auli. ELI5:\nlong form question answering. CoRR, abs/1907.09190, 2019. URL http://arxiv.org/\nabs/1907.09190.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models, 2023.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen Wang, Chenjie Gu, et al. Reinforced self-training\n(rest) for language modeling. arXiv preprint arXiv:2308.08998, 2023.\nJie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xinying Song,\nand Denny Zhou. Large language models cannot self-correct reasoning yet, 2023.\n9\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. Demonstrate-search-predict: Composing retrieval and language models for\nknowledge-intensive nlp, 2023a.\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vard-\nhamanan, Saiful Haq, Ashutosh Sharma, Thomas T. Joshi, Hanna Moazam, Heather Miller, Matei\nZaharia, and Christopher Potts. Dspy: Compiling declarative language model calls into self-\nimproving pipelines, 2023b.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. Hurdles to progress in long-form question answering,\n2021.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step, 2023.\nJerry Liu. Llamaindex. https://github.com/jerryjliu/llama_index, 2022.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad\nMajumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine:\nIterative refinement with self-feedback, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. Measuring\nand narrowing the compositionality gap in language models, 2023.\nJustin Reppert, Ben Rachbach, Charlie George, Luke Stebbing, Jungwon Byun, Maggie Appleton,\nand Andreas Stuhlm\u00a8uller. Iterated decomposition: Improving science q&a by supervising reason-\ning processes, 2023.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366, 2023.\nAvi Singh, John D. Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J. Liu, James\nHarrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, Abhishek Kumar, Alex Alemi, Alex Rizkowsky,\nAzade Nova, Ben Adlam, Bernd Bohnet, Gamaleldin Elsayed, Hanie Sedghi, Igor Mordatch, Is-\nabelle Simpson, Izzeddin Gur, Jasper Snoek, Jeffrey Pennington, Jiri Hron, Kathleen Kenealy,\nKevin Swersky, Kshiteej Mahajan, Laura Culp, Lechao Xiao, Maxwell L. Bileschi, Noah Con-\nstant, Roman Novak, Rosanne Liu, Tris Warkentin, Yundi Qian, Ethan Dyer, Behnam Neyshabur,\nJascha Sohl-Dickstein, and Noah Fiedel. Beyond human data: Scaling self-training for problem-\nsolving with language models, 2023.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia\nCreswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process- and\noutcome-based feedback, 2022.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Jun-\nzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao\nZhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou,\nRongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuan-\njing Huang, and Tao Gui. The rise and potential of large language model based agents: A survey,\n2023.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. Hotpotqa: A dataset for diverse, explainable multi-hop question\nanswering. CoRR, abs/1809.09600, 2018. URL http://arxiv.org/abs/1809.09600.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\n10\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with\nreasoning, 2022.\nA\nAPPENDIX\nA.1\nPROMPTS\nReasoning steps.\nAll our reasoning prompts are n-shot (with n > 1), though we only show frag-\nments, limited to the first exemplar:\n\u2022 The decision step prompt is shown in Listing 1 (the full prompt is 9-shot).\n\u2022 The summarization prompt is presented in Listing 2 (the full prompt is 6-shot).\n\u2022 The answer generation prompt is 5-shot, Listing 3 shows a 1-shot fragment.\n\u2022 The prompt for relevance self-check is presented in Listing 5 (the full prompt is 6-shot).\n\u2022 Finally, the prompt for grounding self-check is shown in Listing 6 (the full prompt is\n5-shot).\n\u2022 Both self-checks use the common prefix from Listing 4.\nAuto-eval.\nThe full 5-shot prompt for auto-eval is presented in Listing 7.\nRanking \u201dReward\u201d Model.\nThe full prompt of the reward model is shown in Listing 8. Notice\nthat this is the only prompt that is 0-shot and doesn\u2019t use \u201dcode as the prompt\u201d approach, as it\u2019s\nintended to be used with an instruction-tuned model.\nA.2\nADDITIONAL DATA\nTable 4: Agent self-improvement and self-distillation, Bamboogle auto-eval, mean accuracy and stan-\ndard deviation over 10 runs, (%). Evaluation before self-critique steps\nXS\nS\nL\nPre-trained\nN/A\nN/A\n69.5\u00b12.8 (-0.8)\nPilot, human filtered\n44.3\u00b13.0 (-0.4)\n54.4\u00b14.1 (-2.2)\n70.9\u00b13.0 (-0.6)\nSelf-improvement, 1st gen\n54.8\u00b13.7 (+0.4)\n61.2\u00b12.5 (-0.7)\n73.1\u00b13.0 (-0.9)\nSelf-improvement, 2nd gen\n65.6\u00b13.0 (-0.3)\n69.2\u00b11.8 (-0.5)\n75.0\u00b11.3 (-1.1)\n11\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 1: Decision step prompt (fragment)\n\"\"\"Implement an agent capable of answering complex queries by potentially search multiple\ntimes.\n\"\"\"\nimport dataclasses\nclass Action:\n\"\"\"Base class for different actions.\"\"\"\n...\n@dataclasses.dataclass\nclass ActionWrapper:\n\"\"\"Encapsulates the reasoning as well as the selected action.\nAttributes:\nthoughts: Record your thoughts on why we should do this action.\naction: The actually selected action.\n\"\"\"\nthoughts: str\naction: Action\n@dataclasses.dataclass\nclass Search(Action):\n\"\"\"The Google search command.\nAttributes:\nquery: The query to be sent to Google.\n\"\"\"\nquery: str\n@dataclasses.dataclass\nclass Terminate(Action):\n\"\"\"Command to terminate the search sequence.\"\"\"\n...\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \u2019I want to buy the Ikea Klippan loveseat, but I\\\u2019m not sure whether\nit can fit into my 2019 Honda Odyssey. Could you check whether I need to fold down the\nseat?\u2019\nPAST_ACTIONS: List[Action] = [\nSearch(query=\u2019ikea klippan loveseat dimension\u2019,\nthoughts=\"To answer the ORIGINAL_QUESTION, we need to find the size of the Ikea Klippan\nloveseat and the cargo size of the 2019 Honda Odyssey, and then compare them. Let\u2019s start\nby finding the size of the Ikea Klippan loveseat.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=1, link_text=\u2019KLIPPAN Loveseat, Vissle gray - IKEA\u2019,\nsnippet=\u2019<lh> Measurements </lh> <ul> <li> Width: 70 7/8 &quot; </li> <li> Depth:\n34 5/8 &quot; </li> <li> Height: 26 &quot; </li> <li> Height under\nfurniture: 4 3/8 &quot; </li> <li> Seat depth: 21 1/4 &quot; </li> <li> Seat\nheight: 16 7/8 &quot; </li> </ul>\u2019),],\ngrounded_summarization=\u2019According to [link_id=1] the dimension of the ikea klippan loveseat is\nWidth: 70 7/8\"; Depth: 34 5/8\"; Height: 26\".\u2019,\nthoughts=\"We still need to find the cargo size of the 2019 Honda Odyssey to answer the\nORIGINAL_QUESTION.\",\n),\n]\nREMAINING_SEARCHES: int = 4\nORIGINAL_QUESTION: str = \u2019I want to buy the Ikea Klippan loveseat, but I\\\u2019m not sure whether\nit can fit into my 2019 Honda Odyssey. Could you check whether I need to fold down the\nseat?\u2019\nACTION_SELECTED = ActionWrapper(thoughts=\"The past result gives us the dimension of the love\nseat. We indeed need to find the cargo size of the 2019 Honda Odyssey.\", action=Search(\nquery=\u20192019 Honda Odyssey cargo size\u2019))\n# [END]\n#########################\n# Example 2:\n#########################\n12\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 2: Summarization prompt (fragment)\n\"\"\"Implementing step 2 of Search and Answer procedure: Search result filtering and\nsummarization.\n\"\"\"\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \"What\u2019s the efficiency of the solar panels on the garden solar lights\n?\"\nPAST_ACTIONS: List[Action] = [\nSearch(query=\u2019garden solar light panel efficiency\u2019,\nthoughts=\"Let\u2019s just rephrase the search query a bit. The intention of the original question\nis pretty clear.\",\n),\n]\nCURRENT_SEARCH_RESULTS = SearchResult(links=[\nResultItem(link_id=17, link_text=\u2019Use Solar Outdoor Lighting for Energy Efficiency - Sepco\nSolar\u2019,\nsnippet=\u2019Depending on the type of lighting you install and the wattage of the\nfixtures, this can eat up a budget quickly. Instead, going with a solar\noutdoor lighting system to provide the required light can greatly reduce the\nproject costs, help improve the environment, and keep electrical costs in\nthe bank where it belongs.\u2019),\nResultItem(link_id=18, link_text=\u2019How Do Solar Garden Lights Work? - The OutdoorLights Store\n\u2019,\nsnippet=\u2019The solar panel converts solar energy into electricity<br><br> The\nenergy absorbed by the photovoltaic cells in the panel creates electrical\ncharges. These charges move in response to an electrical field in the solar\npanel&#39;s cells, which ultimately causes electricity to flow.\u2019),\nResultItem(link_id=19, link_text=\u201910 things you should know about the garden solar lights\u2019,\nsnippet=\u2019The solar panels are generally made from two types of cells,\nmonocrystalline or polycrystalline. Both of them are low cost, however,\ndifferent panel technology can lead to different efficiency under different\nconditions.\u2019),\n])\nORIGINAL_QUESTION: str = \"What\u2019s the efficiency of the solar panels on the garden solar lights\n?\"\n# [link_id=17] is an article about garden solar light, which doesn\u2019t have anything specific\nabout the solar panels. Not selected.\n# [link_id=18] is an article on how to best install those solar lights. Not relevant to the\nsolar panels used. Not selected.\nACTION_SELECTED: LinkSelection = LinkSelection(grounded_summarization=\u2019According to [link_id\n=19], there are two types of solar cells used for garden solar lights, the\nmonocrystalline or polycrystalline panels.\u2019, thoughts=\"It seems like non of the links\nshows the efficiency number. However, link 19 provides some hints on what to search next.\nWe can look into the efficiency number of those two types of panels.\", selected_link_ids\n=[19])\n# [END]\n#########################\n# Example 2:\n#########################\n13\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 3: Answer generation prompt (fragment)\n\"\"\"Implementing step 3 of the multi-round search agent: Answer generation based on all the\ncollected search results.\n\"\"\"\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \"What\u2019s the white powdery residual often found on a humidifier?\"\nPAST_ACTIONS: List[Action] = [\nSearch(query=\u2019what is humidifier\u2019,\nthoughts=\"Let\u2019s solve this step by step. The person is asking about the residual found in\nhumidifier. Let\u2019s first define what does humidifier do.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=1, link_text=\u2019Humidifiers: What They Do for Health, Uses, Types, and More\n\u2019,\nsnippet=\u2019What is a humidifier? Humidifiers are devices that add moisture to the\nair to prevent dryness that can cause irritation in many parts of the body.\nHumidifiers can be particularly effective for treating dryness of the skin,\nnose, throat, and lips. They can also ease some symptoms caused by the flu\nor common cold.\u2019),\nResultItem(link_id=3, link_text=\u20195 humidifier uses: Benefits and risks - Medical News Today\u2019\n,\nsnippet=\u2019We do the research so you can find trusted products for your health and\nwellness. Read more about our vetting process. Humidifiers add moisture to\nthe air, which can benefit people with respiratory symptoms or dry skin.\nThere are several ways to use humidifiers in the home or office, but there\nare also some risks.\u2019),],\ngrounded_summarization=\u2019According to [link_id=1] and [link_id=3], humidifier is a device that\nadds moisture to the air to prevent dryness that can cause irritation in many parts of\nthe body.\u2019),\nthoughts=\"Now that we know what humidifier is, we can look into the white powdery residual.\",\n),\nSearch(query=\u2019white powdery residual in humidifier\u2019,\nthoughts=\"Let\u2019s now look into the white powdery residual.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=10, link_text=\u2019What is white dust and should I be concerned about it? -\nHVAC.com\u2019,\nsnippet=\u2019White dust is usually caused by mineral content in the water that goes\ninto a humidifier. When the mist lands on furniture or other surfaces and\ndries, it can sometimes leave behind dried mineral deposits, or \"white dust\n.\"\u2019),\nlink_text=\u2019What is white dust and should I be concerned about it? - HVAC.com\u2019,\nsnippet=\u2019White dust is usually caused by mineral content in the water that\ngoes into a humidifier. When the mist lands on furniture or other surfaces\nand dries, it can sometimes leave behind dried mineral deposits, or \"white\ndust.\"\u2019)\nResultItem(link_id=11, link_text=\u2019Why is White Dust Coming Out of My Humidifier?\u2019,\nsnippet=\u2019The white dust that you are finding on surfaces around your home is\nlikely dried-up minerals from water vapor created by the humidifier. These\nminerals are natural in our home water supply. If you&#39;ve ever heard of\nthe term \"hard water,\" it&#39;s referring to water with high levels of\ncalcium and magnesium.\u2019),],\ngrounded_summarization=\u2019According to [link_id=10], and [link_id=11], the white powdery\nresidual is caused by the mineral content in the water.\u2019),\nthoughts=\"We have all the information to answer the question.\",\n),\nTerminate(thoughts=\"We have all the information to answer the question.\"),\n]\nORIGINAL_QUESTION: str = \"What\u2019s the white powdery residual often found on a humidifier?\"\nACTION_SELECTED: Answer = Answer(thoughts=\"The first search (\u2019what is humidifier\u2019) is probably\nnot what the questioner cares, so let\u2019s omit that in the answer. We can focus more on\nthe question itself.\", answer=\"The white powdery residue in the humidifier is usually\ncaused by mineral content in the water, according to [link_id=10]. For example, water may\ncontain high levels of calcium and magnesium [link_id=11].\")\n# [END]\n#########################\n# Example 2:\n#########################\n14\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 4: Self-check prompt (prefix)\n\"\"\"Check whether the ANSWER addresses the ORIGINAL_QUESTION and whether the ANSWER is based on\nSelectLinks in PAST_ACTIONS.\"\"\"\nfrom dataclasses import dataclass\nfrom typing import List, Tuple\nclass Action:\n\"\"\"Base class for different actions.\"\"\"\nclass Check_Answer(Action):\n\"\"\"Check whether the ANSWER addresses the ORIGINAL_QUESTION.\"\"\"\ndef __init__(self, passed: bool) -> None:\nself.passed = passed\nclass Revise_Answer(Action):\n\"\"\"Revise the answer if it did not pass the check, based on information from SelectLinks in\nPAST_ACTIONS.\"\"\"\ndef __init__(self, revised_answer: str) -> None:\nself.revised_answer = revised_answer\n...\n@dataclasses.dataclass\nclass ResultItem:\n\"\"\"Single search result, with link id, link title and snippet.\nAttributes:\nlink_id: A unique integer id of this link.\nlink_text: The title of the link.\nsnippet: The snippet from the page that\u2019s relevant to the query\n\"\"\"\nlink_id: int\nlink_text: str\nsnippet: str\n15\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 5: Relevance self-check (fragment)\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \u2019How to exclude a website from Google search\u2019\nPAST_ACTIONS: List[Action] = [\nSearch(query=\u2019exclude site in google search\u2019,\nthoughts=\"We simply the ORIGINAL_QUESTION to a good search query.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=2, link_text=\u2019How to Exclude a Website from Google Search Results -\nKeyword\u2019,\nsnippet=\"You can exclude a domain or even a specific page from appearing on\nGoogle search results. Here\u2019s how ...\"),\nResultItem(link_id=3, link_text=\u2019How do I exclude/block specific web sites from search\nresults?\u2019),\nsnippet=\"In the absence of a browser extension, the simplest way is to add -site:\nwebsite_name after your search terms. ... I can\u2019t get rid of books.google.\ncom results. I\u2019 ...\"),\nResultItem(link_id=6, link_text=\"Q&A: Can you exclude a specific website from Google\u2019s\nsearch results?\"),\nsnippet=\u2019Jul 31, 2022 . All you have to do is simply append -site:pinterest.com\nto the end of your search query. For example, if you were searching for the\nterm \"paper ...\u2019),\n],\ngrounded_summarization=\u2019[link_id=3] mentions that we can add -site:website_name to the google\nsearch query to exclude a website. [link_id=6] provides an example.\u2019),\nthoughts=\"We found enough info for a good answer to the ORIGINAL_QUESTION.\",\n),\nTerminate(thoughts=\"This is a good answer to the question.\")\nAnswer(thoughts=\"We summarize the relevant info found in PAST_ACTIONS.\", answer=\"To exclude a\nwebsite from Google search result, you can add -site:website_name after your search terms\naccording to [link_id=3]. For example, if you want to exclude pinterest.com, you can\nsimply add -site:pinterest.com according to [link_id=6].\")\n]\nORIGINAL_QUESTION: str = \u2019How to exclude a website from Google search\u2019\nANSWER: str = \u2019To exclude a website from Google search result, you can add -site:website_name\nafter your search terms according to [link_id=3]. For example, if you want to exclude\npinterest.com, you can simply add -site:pinterest.com according to [link_id=6].\u2019\n# The ANSWER is directly addressing the ORIGINAL_QUESTION.\n# The ANSWER refers to [link_id=3] which correctly supports the core statement of adding -site\n:website_name. The example from the answer is indeed from [link_id=6].\nACTION_SELECTED: Command = Check_Answer(passed=True)\n# [END]\n#########################\n# Example 2:\n#########################\n16\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 6: Grounding self-check (fragment)\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \"What did Jim Betts\u2019 competitor in the 1980 election achieve in 1962?\n\"\nPAST_ACTIONS: List[Action] = [\nSearch(query=\u2019who was Jim Betts competitor in 1980\u2019,\nthoughts=\"This question is asking about the achievement of Jim Betts\u2019 competitor in 1962. Let\u2019\ns first search for who this competitor is.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=1, link_text=\u2019Jim Betts (politician) - Wikipedia\u2019,\nsnippet=\u2019Jim Betts is a former member of the Ohio House of Representatives. He\nran against incumbent U.S. Senator John Glenn in 1980. He lost and ran in\n1982 for lieutenant governor.\u2019),],\ngrounded_summarization=\"According to [link_id=1], Jim Betts\u2019 competitor in 1980 was John Glenn\n.\"),\nthoughts=\"Now that we know who the competitor is, we can search for his achievements in 1962.\"\n,\n),\nSearch(query=\u2019John Glenn achievements 1982\u2019,\nthoughts=\"Now that we know who the competitor is, we can search for his achievements in 1982.\"\n,\n),\nSelectLink(selected_links=[\n],\ngrounded_summarization=\u2019Nothing is selected.\u2019),\nthoughts=\"The query \u2019John Glenn achievements 1982\u2019 is not very good. It should be \u2019John Glenn\nachievements 1962\u2019.\",\n),\nSearch(query=\u2019John Glenn achievements 1962\u2019,\nthoughts=\"John Glenn is the person we are looking for. Let\u2019s change the date to 1962.\",\n),\nSelectLink(selected_links=[\nResultItem(link_id=19, link_text=\u2019John Glenn | Biography, Accomplishments, & Facts |\nBritannica\u2019,\nsnippet=\"Glenn, Jr., (born July 18, 1921, Cambridge, Ohio, U.S.-died December 8,\n2016, Columbus, Ohio), the first U.S. astronaut to orbit Earth, completing\nthree orbits in 1962. (Soviet cosmonaut Yuri Gagarin, the first person in\nspace, had made a single orbit of Earth in 1961.) Glenn joined the U.S.\nNaval Reserve in 1942.\"),],\ngrounded_summarization=\"According to [link_id=19], John Glenn\u2019s achievement in 1962 was to\nbecome the first American to orbit the Earth.\"),\nthoughts=\"We have all the information to answer the question.\",\n),\nTerminate(thoughts=\"We have all the information we need to answer the ORIGINAL_QUESTION. Let\u2019s\nstop searching and answer the question.\")\nAnswer(thoughts=\"We have the answer to the question.\", answer=\"Jim Betts\\\u2019 competitor in the\n1980 election was John Glenn. And John Glenn\\\u2019s achievement in 1962 was to become the\nfirst American to orbit the Earth.\")\n]\nORIGINAL_QUESTION: str = \"What did Jim Betts\u2019 competitor in the 1980 election achieve in 1962?\n\"\nANSWER: str = \u2019Jim Betts\\\u2019 competitor in the 1980 election was John Glenn [link_id=1]. And\nJohn Glenn\\\u2019s achievement in 1962 was to become the first American to orbit the Earth [\nlink_id=19].\u2019\n# ANSWER directly addresses the ORIGINAL_QUESTION.\n# [link_id=1] indeed says that Jim Betts\\\u2019 run against John Glenn in 1980, consistent with\nANSWER.\n# [link_id=19] indeed says John Glenn is the first US astronaut to orbit the earth, consistent\nwith ANSWER.\nACTION_SELECTED: Command = Check_Answer(passed=True)\n# [END]\n#########################\n# Example 2:\n#########################\n17\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 7: Auto-eval (full prompt)\n\"\"\"Check whether the ANSWER implies the REF_ANSWER to the question.\"\"\"\ndef Check_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER):\n\"\"\"Check whether the ANSWER implies the REF_ANSWER.\"\"\"\n# todo\n#########################\n# Example 1:\n#########################\nORIGINAL_QUESTION: str = \u2019Are McIntyre Mines and Copperfields Mine in the same country?\u2019\nANSWER: str = \u2019According to [link_id=1], Copperfields Mine is in Ontario, Canada. According to\n[link_id=4], McIntyre Mines is also in Ontario, Canada. So yes, they are in the same\ncountry.\u2019\nREF_ANSWER: str = \u2019yes\u2019\n# the ANSWER implies the answer to the original question is yes, this is consistent with the\nREF_ANSWER.\nCheck_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER) = True\n# [END]\n#########################\n# Example 2:\n#########################\nORIGINAL_QUESTION: str = \u2019Which Director, James Richard Kelly or Paul L. Stein has a list of\n67 films to his credit? \u2019\nANSWER: str = \u2019According to [link_id=1] and [link_id=2], James Richard Kelly has 5 films to\nhis credit.\u2019\nREF_ANSWER: str = \u2019Paul Ludwig Stein (4 February 1892 - 2 May 1951) was an Austrian-born film\ndirector with 67 films to his credit.\u2019\n# the ANSWER does not imply the REF_ANSWER because ANSWER does not mention Paul Ludwig Stein.\nCheck_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER) = False\n# [END]\n#########################\n# Example 3:\n#########################\nORIGINAL_QUESTION: str = \u2019Are the genuses Michelia and Stenomesson in the same family?\u2019\nANSWER: str = \u2019Based on the information we have, we are not sure whether the genuses Michelia\nand Stenomesson are in the same family [link_id=3] does not contain this information.\u2019\nREF_ANSWER: str = \u2019no\u2019\n# The ANSWER did not determine whether or not Michelia and Stenomesson are in the same family.\n# The REF_ANSWER implies that they are not in the same family.\n# Thus we cannot infer the REF_ANSWER given the ANSWER.\nCheck_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER) = False\n# [END]\n#########################\n# Example 4:\n#########################\nORIGINAL_QUESTION: str = \u2019In what year was the winner of the 44th edition of the Miss World\ncompetition born?\u2019\nANSWER: str = \u2019According to [link_id=2], the winner of the 44th edition of the Miss World\ncompetition is Aishwarya Rai.\\nAccording to [link_id=4], she was born in 1973.\u2019\nREF_ANSWER: str = \u20191973\u2019\n# The ANSWER implies 1973 as the final answer to ORIGINAL_QUESTION.\n# REF_ANSWER implies 1973 as the final answer to ORIGINAL_QUESTION, too.\nCheck_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER) = True\n# [END]\n#########################\n# Example 5:\n#########################\nORIGINAL_QUESTION: str = \u2019Skull & Bones is a mini-series by a man that is an editor of what\ntwo comics?\u2019\nANSWER: str = \u2019The question is ill-formed or out-of-date. We should ask for clarification from\nthe asker.\u2019\nREF_ANSWER: str = \u2019Marvel Comics and DC Comics\u2019\n# The ANSWER did not provide any answer to ORIGINAL_QUESTION.\nCheck_Answer(ORIGINAL_QUESTION, ANSWER, REF_ANSWER) = False\n# [END]\n#########################\n# Example 6:\n#########################\n18\nReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent\nListing 8: Reward model (full prompt)\n\"\"\"Rater Instructions:\n- The goal of this rating is to filter out bad actions, so that they\u2019ll be excluded from the\nfine-tuning dataset.\n- Overall, we want to the agent to produce relevant and grounded answers with minimal steps.\nAnything that deviates from this goal is considered bad.\n- If any element (thoughts, comments etc.) is empty, then it\u2019s automatically bad.\n#########################################\n*** Model Can See:\n\u2018\u2018\u2018\n{inputs}\n\u2018\u2018\u2018\n*** Model Output #1:\n\u2018\u2018\u2018\n{action1}\n\u2018\u2018\u2018\n*** Model Output #2:\n\u2018\u2018\u2018\n{action2}\n\u2018\u2018\u2018\n*** Model Output #3:\n\u2018\u2018\u2018\n{action3}\n\u2018\u2018\u2018\n*** Model Output #4:\n\u2018\u2018\u2018\n{action4}\n\u2018\u2018\u2018\n#########################################\nYour Instructions:\n- Choose the best model output based on the rater\u2019s instructions.\n- Don\u2019t assume in your decision that the model knows anything outside of \"Model Can See\"\nsection.\n- Be specific in your explanation.\nOutput 3 lines when answering and make sure to follow the precise format.\nExplanation: why you think model output #X is the best\nAnswer: #X\nRanking: #X > #Y > ...\n\"\"\"\n19\n"
  },
  {
    "title": "DreamTalk: When Expressive Talking Head Generation Meets Diffusion Probabilistic Models",
    "link": "https://arxiv.org/pdf/2312.09767.pdf",
    "upvote": "23",
    "text": "DreamTalk: When Expressive Talking Head Generation\nMeets Diffusion Probabilistic Models\nYifeng Ma1*, Shiwei Zhang2, Jiayu Wang2, Xiang Wang3*, Yingya Zhang2, Zhidong Deng1\n1 Department of Computer Science and Technology, BNRist, THUAI,\nState Key Laboratory of Intelligent Technology and Systems, Tsinghua University\n2Institute for Intelligent Computing, Alibaba Group\n3Huazhong University of Science and Technology\nmayf18@mails.tsinghua.edu.cn, wxiang@hust.edu.cn, michael@tsinghua.edu.cn\n{zhangjin.zsw, wangjiayu.wjy, yingya.zyy}@alibaba-inc.com\nhttps://dreamtalk-project.github.io\nAbstract\nDiffusion models have shown remarkable success in a\nvariety of downstream generative tasks, yet remain under-\nexplored in the important and challenging expressive talk-\ning head generation. In this work, we propose a DreamTalk\nframework to fulfill this gap, which employs meticulous de-\nsign to unlock the potential of diffusion models in generat-\ning expressive talking heads. Specifically, DreamTalk con-\nsists of three crucial components: a denoising network, a\nstyle-aware lip expert, and a style predictor. The diffusion-\nbased denoising network is able to consistently synthesize\nhigh-quality audio-driven face motions across diverse ex-\npressions. To enhance the expressiveness and accuracy of\nlip motions, we introduce a style-aware lip expert that can\nguide lip-sync while being mindful of the speaking styles.\nTo eliminate the need for expression reference video or text,\nan extra diffusion-based style predictor is utilized to pre-\ndict the target expression directly from the audio. By this\nmeans, DreamTalk can harness powerful diffusion mod-\nels to generate expressive faces effectively and reduce the\nreliance on expensive style references.\nExperimental re-\nsults demonstrate that DreamTalk is capable of generating\nphoto-realistic talking faces with diverse speaking styles\nand achieving accurate lip motions, surpassing existing\nstate-of-the-art counterparts.\n1. Introduction\nAudio-driven talking head generation, which concerns\nanimating portraits with speech audio, has garnered sig-\nnificant interest due to its diverse applications in video\n*Intern at Alibaba Group.\nDreamTalk\nor\nSpeaking style source\naudio\nvideo\noutput video\nportrait\naudio\nFigure 1. Leveraging the powerful diffusion models, DreamTalk\nis able to generate highly expressive talking heads across diverse\nspeaking styles. Furthermore, DreamTalk is able to derive person-\nalized speaking style directly from input audio, which obviates the\nneed for additional style references.\ngames, film dubbing, and virtual avatars. Generating life-\nlike facial expressions is essential for enhancing the real-\nism of talking heads [19]. These expressions during speech\nare termed as speaking styles [12, 46]. GANs [21] cur-\nrently hold the state-of-the-art in expressive talking head\ngeneration [20, 30, 37, 46].However, their inherent issues\nwith mode collapse and unstable training hamper their ef-\nficacy in consistently achieving high performance across\na diverse range of speaking styles. Another issue is that\nprior methods often rely on reference videos [30, 37, 78] or\ntexts [45, 90] to specify speaking styles. Their acquisition\nrequires extra manual effort and hence is inflexible.\nAs a new line of generative technique, diffusion mod-\nels [26, 62] have recently been shown to produce high-\nquality results in numerous generative areas such as im-\nage generation [16, 53], video generation [27, 60, 86], and\nhuman motion synthesis [1, 72]. The success of diffusion\nmodels, stemming from their superior properties such as\npowerful distribution learning [16, 72], good convergence,\n1\narXiv:2312.09767v1  [cs.CV]  15 Dec 2023\nand stylistic diversity, make them exceptionally promising\nfor exploring expressive talking head generation. However,\ncurrent diffusion-based talking head approaches [58, 66, 96]\nprimarily concentrate on generating talking heads with\nneutral expressions and still struggle to produce satisfac-\ntory performance, e.g., suffering from frame jittering prob-\nlem [58]. As a result, how to stimulate the full potential of\ndiffusion models for expressive talking head generation is a\npromising yet untapped research direction.\nIn this paper, we propose DreamTalk, an expressive talk-\ning head generation framework that takes advantage of dif-\nfusion models to simultaneously deliver high performance\nacross diverse speaking styles and reduce the reliance on\nexpensive style references. Specifically, DreamTalk is com-\nposed of a denoising network, a style-aware lip expert, and a\nstyle predictor. The diffusion-based denoising network pro-\nduces audio-driven facial motions with the speaking style\nspecified by a reference video.\nThe great distribution-\nlearning characteristic of diffusion models endows the de-\nnoising network with the potential to produce high-quality\nresults across diverse speaking styles. To harness this po-\ntential, we design a style-aware lip expert that drives the\ndenoising network to produce accurate lip motions with\nvivid expressions. Contrasting with previous lip experts that\noverlook expression information and thus compromise style\nexpressiveness, the proposed lip expert not only enhances\nlip accuracy but also ensures expressiveness. Finally, to\nfurther eliminate the need for additional style references,\na diffusion-based style predictor is incorporated to predict\npersonalized speaking styles directly from audio. The style\npredictor also incorporates the portrait as input during pre-\ndiction, leveraging the correlation between speaker identity\nand speaking styles, thereby enhancing performance.\nUltimately, DreamTalk can consistently generate photo-\nrealistic talking faces with precise lip-sync across a wide\nrange of speaking styles while minimizing the need for ad-\nditional style references. It also enables versatile manipula-\ntion of speaking styles and exhibits robust generalization\nacross varied inputs, including songs, speech in multiple\nlanguages, noisy audio, and out-of-domain portraits. The\neffectiveness of DreamTalk is demonstrated through com-\nprehensive qualitative and quantitative evaluations, show-\ncasing its superiority over existing state-of-the-art methods.\n2. Related Work\nAudio-Driven Talking Head Generation. Audio-driven\nmethods [14, 22, 67, 80, 81, 102] fall into two main\ncategories: person-specific and person-agnostic. Person-\nspecific approaches [18, 29, 41, 68, 82] are constrained to\ngenerating videos for speakers seen during training. Many\nof these [29, 34, 35, 64, 70, 74, 94, 97, 98] first craft\n3D facial animations, later converting them into realistic\nvideos.\nRecent advancements [23, 39, 57, 71, 71, 93]\nhave employed neural radiance fields for modeling, yielding\nhigh-fidelity, realistic videos. Conversely, person-agnostic\nmethods [10, 55, 77, 84] target generating videos for un-\nseen speakers. Early methods prioritized lip synchroniza-\ntion [5, 6, 49, 65, 77, 103]. Later works shifted focus to\nnatural facial expressions [95, 105] and head poses [7, 83,\n100, 101, 104].\nExpressive Talking Head Generation. Early methods [13,\n20, 24, 29, 61, 69, 82, 90] model expressions in discrete\nemotion classes. To model more fine-grained expressions,\nmost recent methods [30, 37, 46] leverage an expression ref-\nerence video and transfer the expressions from that video\nto the generated one. However, these GAN-based models\nsuffer from mode collapse, leading to videos with inferior\nlip-sync and style expressiveness. Our work addresses these\nissues by using diffusion models.\nSpecifying desired speaking styles effortlessly is also\nimportant for users. Most previous methods specify speak-\ning styles using reference videos [30, 37, 46] or text [20,\n45, 90], which needs human labor. A more user-friendly\napproach is to derive speaking styles from the input audio.\nPrevious methods can only infer a limited number of dis-\ncrete emotion classes from audio signals [29, 61, 90]. TH-\nPAD [95] generates expressions only aligned with the audio\nrhythm, not aligning with the emotional content of the au-\ndio. Besides, previous methods neglect the information in\nthe input portrait. In this work, we aim to infer personalized\nand emotional expressions using input audio and portraits.\nDiffusion Models. Diffusion models [26, 62] have demon-\nstrated strong performance across multiple vision tasks [16,\n53, 86, 99], including text-to-image generation [31, 54], im-\nage inpainting [33, 42], human motion generation [1, 72],\n3D content generation [43], and video generation [50, 60,\n79, 87, 89].\nPrevious efforts [3, 47, 58, 66] employing\ndiffusion models in talking head generation only produce\ntalking heads with neutral emotion and the results are un-\nsatisfactory. Some methods devise diffusion-based render-\ners [17, 91] or face motion prior [96], yet still use GAN or\nregression models to model the audio-motion mapping. In\nthis work, we endeavor to harness diffusion models for the\ngeneration of expressive talking heads, presenting a more\nintricate challenge with greater practical relevance.\n3. Method\n3.1. Problem Formulation\nGiven a portrait I, a speech A, and a style reference\nvideo R, our method aims to generate a talking head video\nwith lip motions synchronized with the speech and the\nspeaking style reflected in the reference video. The audio\nA = [ai]L\ni=1 is parameterized as a sequence of acoustic\nfeatures. R is a sequence of video frames.\nBesides, to eliminate the need for extra style references,\n2\nAudio Encoder\n!!\nAudio window\nNoisy motion\nC\nconcat.\nadd\n!(#)\n\u2217\n!(&)\nPredicted\nmotion\nStyle Encoder\nDenoising \nDecoder\n'\nStyle ref. video\n(\nDiffusion step\nDenosing !!\nDiffuse\n0 \u2192 (% \u2212 1)\n!(#)\n\u2217\n!()*+)\n\u2217\n'/-(#)\n\u2217\n!!\n(\n.(&)\nDenosing !!\nDiffuse\n0 \u2192 (% \u2212 2)\n!(#)\n\u2217\n!()*/)\n\u2217\n'/-(#)\n\u2217\n!!\n(\nDenosing !!\n!(#)\n\u2217\n'/-(#)\n\u2217\n!!\n(\n.(')\n\u2217\nRenderer\nOutput frame\n0\nPortrait\n1\n2(0, 0)\nStyle code -\n(Pre-trained) \nStyle-aware\nLip Expert\nGuidance\nFace motion\n!\nMouth vert.\nStyle condition\nAudio\n\"!\"#$\nMouth\nEmbedder\nAudio\nEmbedder\nStyle ref. video\nCosine\nSimilarity\nDiffusion-based\nStyle Predictor\nNoisy \nstyle code\n-(()\nPortrait\nAudio\nStyle code\npredicted \nusing audio\n-(#)\n\u2217\nStyle space\nConstructed using \npre-trained style encoder in 5)\n(a) Denoising Network  \"6\n(d) Inference Process\n(b) Style-aware Lip Expert\n(c) Style Predictor\n~\nLinear\nProjection\nFigure 2. Illustration of DreamTalk. A style-aware lip expert (b), which evaluates the lip-audio synchronous probability under a given\nspeaking style, is first trained to provide lip motion guidance for the denoising network (a). The denoising network is then trained to take\nthe audio, the style reference video, and noisy face motion as input and predict the unnoised face motion. Then, A style predictor (c) is\ntrained to predict the style code extracted from a video, taking audio and the speaker in that video as input. During inference process (d),\nthe speaking style can be specified using style codes that are extracted from videos or derived from audio.\nour method also aims to infer the speaking style using solely\nthe speech and the portrait. The inferred speaking style can\nreplace the role of style reference videos in controlling the\nexpressions, which enables our method to generate expres-\nsive talking head videos with only speech and portrait input.\n3.2. DreamTalk\nDreamTalk comprises 3 key components: a denoising\nnetwork, a style-aware lip expert, and a style predictor.\nThe denoising network computes face motion condi-\ntioned on the speech and style reference video. The face\nmotion M\n= [ml]L\nl=1 is parameterized as a sequence\nof expression parameters from 3D Morphable Models [4].\nThe face motion is rendered into video frames by a ren-\nderer [52]. The style-aware lip expert provides lip motion\nguidance under diverse expressions and thus drives the de-\nnoising network to achieve accurate lip-sync while ensur-\ning style expressiveness. The style predictor can predict the\nspeaking style aligned with that conveyed in speech.\nDenoising Network. The denoising network synthesizes\nface motion sequence frame-by-frame in a sliding window\nmanner. It predicts a motion frame ml using an audio win-\ndow Aw = [ai]l+w\ni=l\u2212w, where w denotes the window size.\nThe denoising network leverages forward and reverse\ndiffusion processes. The diffusion process is modeled as\na Markov noising process. Starting from a motion frame\nm(0), it incrementally introduces Gaussian noise into the\nreal data, gradually diffusing towards a distribution resem-\nbling N(0, I). Consequently, the distribution evolves as\nfollows:\nq(m(t)|m(t\u22121)) = N(\u221a\u03b1nm(t\u22121), (1 \u2212 \u03b1n)I),\n(1)\nwhere m(t) is the motion frame sampled at diffusion step\nt, t \u2208 {1, . . . , T}, and \u03b1n is determined by the variance\nschedules. Conversely, the reverse diffusion process, or the\ndenoising process, predicts the added noise in a noisy mo-\ntion frame. Starting from a random motion frame m(T ) \u223c\nN(0, I), the denoising process incrementally removes the\nnoise and recovers the original motion m(0).\nInstead of predicting the noise as formulated by [26], we\nfollow [51] and predict the signal itself. The denoising net-\nwork E\u03b8 predicts m(0) based on the noisy motion, the dif-\nfusion step, the speech context, and the style reference:\nm\u2217\n(0) = E\u03b8(m(t), t, Aw, R).\n(2)\nThe asterisk(\u2217) indicates quantities that are generated.\n3\nOur denoising network has a transformer architec-\nture [76].\nThe audio window Aw is first fed into a\ntransformer-based audio encoder and the output is concate-\nnated with the noisy motion m(t) in the channel dimension.\nAfter linearly projected to the same dimension, the con-\ncatenated results and the timestep t are summed and served\nas the key and value of a transformer decoder. To extract\nthe speaking style from the style reference, a style encoder\nfirst extracts the sequence of 3DMM expression parameters\nfrom R and then feeds them into a transformer encoder. The\noutput tokens are aggregated using a self-attention pooling\nlayer [56] to obtain the style code s. The style code is re-\npeated 2w + 1 times and added with positional encodings.\nThe results serve as the query of the transformer decoder.\nThe middle output token of the decoder is fed into a feed-\nforward network to predict the signal m(0).\nStyle-aware Lip Expert. We observe that using solely the\ndenoising loss in standard diffusion models results in in-\naccurate lip motions. We conjecture that the loss alone is\ninsufficient for the denoising network to effectively focus\non generating precise lip motions. A typical remedy is to\ninvolve a pre-trained lip expert [49] that provides lip mo-\ntion guidance. However, we observe the lip expert reduces\nthe intensity of expressions. This stems from the fact that\nthe lip expert merely focuses on a generic speaking style,\nwhich leads to generating face motions in a uniform style.\nTo address this issue, we introduce a style-aware lip ex-\npert. The proposed lip expert is trained to evaluate lip-sync\nunder diverse speaking styles. Therefore, it can provide lip\nmotion guidance under diverse speaking styles and strike\na better balance between style expressiveness and lip-sync.\nThe lip expert E computes the probability that a clip of au-\ndio and lip motions are synchronous conditioned on style\nreference R:\nPsync = E([ai]l+n\ni=l , [mi]l+n\ni=l , R),\n(3)\nwhere n denotes the clip length.\nThe style-aware lip expert encodes the lip motions and\naudio into respective embeddings conditioned on style ref-\nerence and then computes the cosine similarity to represent\nthe sync probability. To obtain lip motion information from\nface motion m, we first convert m into the correspond-\ning face mesh and select vertices in the mouth area as the\nrepresentation of the lip motion [46]. The lip motion and\naudio encoders are mainly implemented by MLPs and 1D-\nconvolutions, respectively. The style condition is fused into\nembeddings by first extracting style features from style ref-\nerence using a style encoder, which mirrors the architecture\nof the one in the denoising network, and then concatenating\nthe style features with intermediate feature maps from em-\nbedding encoders. The style encoder in the lip expert and\nthe generator do not share parameters.\nStyle Predictor. Specifically, the style predictor S\u03d5 pre-\ndicts the style code s extracted by the style encoder in the\ntrained denoising network. Observing the correlation be-\ntween speaker identity and style codes (Sec. 4.4), the style\npredictor also integrates the portrait as input. The style pre-\ndictor is instantiated as a diffusion model and is trained to\npredict the style code itself:\ns\u2217\n(0) = S\u03d5(s(t), t, A, I),\n(4)\nwhere s(t) is the style code sampled at diffusion step t.\nThe style predictor S\u03d5 is a transformer encoder on a se-\nquence consisting of, in order: audio embeddings, an em-\nbedding for the diffusion timestep, a speaker info embed-\nding, the noised style code embedding, and a final embed-\nding called learned query whose output is used to predict the\nunnoised style code. Audio embeddings are audio features\nextracted using self-supervised pre-trained speech models.\nTo obtain the speaker info embedding, our method first ex-\ntracts the 3DMM identity parameters, which include the\nface shape information but removes irrelevant information,\nsuch as expressions, from the portrait, and then embeds it\ninto a token using an MLP.\n3.3. Training and Inference\nTraining. The style-aware lip expert is first pre-trained by\ndetermining whether randomly sampled audio and lip mo-\ntion clips are synchronous as in [49] and then frozen during\ntraining the denoising network.\nThe denoising network E\u03b8 is trained by sampling ran-\ndom tuples (m(0), t, Aw, R) from dataset, corrupting m(0)\ninto m(t) by adding Gaussian noises, executing denoising\nsteps to m(t), and optimizing the loss:\nLnet = \u03bbdenoiseLdenoise + \u03bbsyncLsync.\n(5)\nSpecifically, the ground-truth motion m(0), and the speech\naudio window Aw are extracted from the training video of\nthe same moment. t is drawn from the uniform distribution\nU{1, T}. The style reference R is a video clip randomly\ndrawn from the same video containing m(0).\nWe first compute the denoising loss of the diffusion mod-\nels [26] defined as:\nLdenoise = \u2225m(0) \u2212 E\u03b8(m(t), t, Aw, R)\u22252\n2.\n(6)\nThen, the denoising network maximizes the synchronous\nprobability via a sync loss on generated clips:\nLsync = \u2212log(Psync).\n(7)\nClassifier-free guidance [25] is used to train our model.\nSpecifically, E\u03b8 is trained to learn both the style-conditional\nand unconditional distributions via randomly setting R =\n\u2205 by 10% chance during training. \u2205 is implemented as a\n4\nMethods\nMEAD / HDTF / Voxceleb2\nSSIM\u2191\nCPBD\u2191\nF-LMD\u2193\nM-LMD\u2193\nSyncconf \u2191\nMakeItTalk [105]\n0.73 / 0.57 / 0.52\n0.11 / 0.24 / 0.24\n3.97 / 5.12 / 6.29\n5.32 / 4.55 / 5.15\n2.10 / 3.16 / 2.17\nWav2Lip [49]\n0.80 / 0.63 / 0.54\n0.18 / 0.30 / 0.30\n2.72 / 4.53 / 5.85\n4.05 / 3.60 / 4.64\n5.26 / 5.83 / 5.70\nPC-AVS [104]\n0.50 / 0.42 / 0.36\n0.07 / 0.13 / 0.09\n5.83 / 9.71 / 12.9\n4.97 / 4.17 / 7.42\n2.18 / 4.85 / 4.73\nAVCT [84]\n0.83 / 0.76 / 0.64\n0.14 / 0.22 / 0.23\n2.92 / 2.86 / 3.62\n5.52 / 3.57 / 3.71\n2.53 / 4.27 / 3.89\nGC-AVT [37]\n0.34 / 0.36 /\n-\n0.14 / 0.28 /\n-\n8.04 / 10.2 /\n-\n7.10 / 6.23 /\n-\n2.42 / 4.72 /\n-\nEAMM [30]\n0.40 / 0.40 / 0.43\n0.08 / 0.14 / 0.20\n6.70 / 7.03 / 6.36\n6.48 / 6.86 / 4.89\n1.41 / 2.54 / 2.24\nStyleTalk [46]\n0.84 / 0.81 / 0.66\n0.16 / 0.30 / 0.29\n2.12 / 1.96 / 2.92\n3.25 / 2.41 / 2.96\n3.47 / 4.82 / 4.51\nSadTalker [100]\n0.69 / 0.77 / 0.44\n0.16 / 0.24 / 0.19\n4.12 / 5.99 / 9.12\n4.37 / 4.07 / 6.11\n2.76 / 4.35 / 4.38\nPD-FGC [78]\n0.49 / 0.41 / 0.35\n0.05 / 0.13 / 0.12\n5.50 / 9.50 / 12.5\n4.10 / 4.23 / 8.19\n2.27 / 4.68 / 4.64\nEAT [20]\n0.53 / 0.59 / 0.47\n0.15 / 0.26 / 0.20\n5.54 / 3.86 / 5.53\n4.79 / 4.03 / 5.88\n2.16 / 4.54 / 4.35\nDreamTalk\n0.86 / 0.85 / 0.69\n0.16 / 0.31 / 0.30\n1.93 / 1.80 / 2.69\n2.91 / 2.15 / 2.72\n3.78 / 5.17 / 4.90\nGround Truth\n1 / 1 / 1\n0.22 / 0.31 / 0.33\n0 / 0 / 0\n0 / 0 / 0\n4.13 / 5.44 / 5.23\nTable 1. Quantitative comparisons on MEAD, HDTF, and Voxceleb2. Since we only receive GC-AVT samples on MEAD and HDTF,\nGC-AVT is not evaluated on Voxceleb2.\nsequence of face motions [mi] with all zero values. For\ninference, the predicted signal is computed by\nm\u2217\n(0) = \u03c9E\u03b8(m(t), t, Aw, R)\n+ (1 \u2212 \u03c9)E\u03b8(m(t), t, Aw, \u2205),\n(8)\ninstead of Equation 2. This approach enables controlling the\neffectiveness of the style reference R through adjustment of\nthe scale factor \u03c9.\nWhen training the style predictor, we draw a random\nvideo, then extract audio A and style code s(0) (using the\ntrained style encoder) from it. Since 3DMM identity pa-\nrameters may leak expression information, the portrait I is\nsampled from another video with the same speaker identity.\nThe style predictor E\u03d5 is trained by optimizing the loss:\nLpred = \u2225s(0) \u2212 S\u03d5(s(t), t, A, I)\u22252\n2,\n(9)\nWe utilize PIRenderer [52] as the renderer and meticu-\nlously fine-tune it to empower the renderer with emotional\nexpression generation capabilities.\nInference. Our method enables the specification of speak-\ning styles using either reference videos or solely through in-\nput audio and portrait. In the case of reference videos, style\ncodes are derived using the style encoder in the denoising\nnetwork. When relying solely on input audio and portrait,\nthese inputs are processed by the style predictor, which em-\nploys a denoising procedure to obtain the style code.\nWith the style code, the denoising network utilizes the\nsampling algorithm of DDPM [26] to produce face motions.\nIt first samples a random motion m\u2217\n(T ) \u223c N(0, I) then\ncomputes denoised sequences {m\u2217\n(t)}, t = T \u2212 1, . . . , 0 by\nincrementally removing the noise from m\u2217\n(t). Finally, the\nmotion m\u2217\n(0) is the generated face motion. The sampling\nprocess can be accelerated by leveraging DDIM [63]. The\noutput face motions are then rendered into videos by the\nrenderer PIRenderer.\n4. Experiments\n4.1. Experimental Setup\nDatasets. We train and evaluate the denoising network on\nMEAD [82], HDTF [101], and Voxceleb2 [11]. Since Vox-\nceleb2 official videos are of low resolution, we redownload\nthe original YouTube videos and re-crop the videos. The\nstyle-aware lip expert is trained on MEAD and HDTF. We\ntrain the style predictor on MEAD and evaluate it on MEAD\nand RAVEDESS [40].\nBaselines. We compare our method with previous meth-\nods including:\nMakeitTalk [105], Wav2Lip [49], PC-\nAVS [104], AVCT [84], GC-AVT [37], EAMM [30],\nStyleTalk [46],\nDiffTalk [58],\nSadTalker [100],\nPD-\nFGC [78], and EAT [20]. For DiffTalk, since the released\nmodel is incomplete and unable to generate reasonable re-\nsults until submission, we perform qualitative comparisons\nusing videos from its released demo. For other methods, we\ngenerate the samples using released models or with the help\nof the authors.\nMetrics. We utilize widely used metrics: SSIM [88], the\nCumulative Probability of Blur Detection (CPBD) [48], the\nSyncNet confidence score (Syncconf) [9], the Landmark Dis-\ntance around mouth area (M-LMD) [6], the Landmark Dis-\ntance on the full face (F-LMD).\n4.2. Main Results\nQuanitative Comparisons.\nAs shown in Tab. 1, our\nmethod outperforms previous methods across most met-\nrics. Wav2Lip\u2019s training with SyncNet as a discriminator\nexplains its high SyncNet confidence score, even surpass-\ning the ground truth. Notably, our method\u2019s SyncNet con-\nfidence score closely aligns with the ground truth, and it\nachieves the best M-LMD scores, which indicates its capa-\nbility for precise lip synchronization. Furthermore, our su-\nperior performance in the F-LMD metric demonstrates our\n5\nMouth\nGT\nWav2Lip\nPC-AVS\nAVCT\nGC-AVT\nEAMM\nStyleTalk\nAudio\nPortrait\nPortrait\nNo\nStyle\nReference\nDreamTalk\nNo\nStyle\nReference\nVideo\nReference\nMakeItTalk\nSadTalker\nPD-FGC\nEAT\nDiscrete\nEmotion\nDisgusted\nVideo\nReference\nDiscrete\nEmotion\nHappy\nFigure 3. Qualitative comparisons with previous methods.\nmethod\u2019s proficiency in generating facial expressions con-\nsistent with the reference speaking style.\nQualitative comparisons.\nFig. 3 shows the qualitative\ncomparisons.\nThe portraits, style references, and au-\ndio are all unseen during training.\nIt can be seen that\nMakeItTalk and AVCT struggle with accurate lip synchro-\nnization. While Wav2Lip and PC-AVS synchronize lips ac-\ncurately, their outputs appear blurry. SadTalker, on the other\nhand, generally aligns lip movements with audio but occa-\nsionally displays unnatural jitters.\nEAT\u2019s capability is limited to generating discrete emo-\ntions, lacking the finesse for nuanced expressions. For ex-\nample, in the left case, the style reference shows the speaker\nnarrowing his eyes, but EAT merely produces a generic dis-\ngusted look with wide-open glaring eyes. Additionally, as\nshown in the right case, EAT struggles to maintain a consis-\ntent face shape during speaker head movements.\nEAMM, GC-AVT, StyleTalk, and PD-FGC demonstrate\nthe ability to produce fine-grained expressions. However,\nEAMM falls short in lip synchronization, GC-AVT and PD-\nFGC struggle with preserving speaker identity, and all three\nhave issues rendering a plausible background. We observed\nthat StyleTalk, while capable of generating nuanced expres-\nsions, occasionally does so with diminished intensity and\nfails to generate accurate lip motion for some words. A no-\ntable example is shown in the third column of the left case:\nwhen the speaker utters \"um\"; the expected closed-mouth\nmotion is replaced by an open mouth in StyleTalk\u2019s output.\n6\nStyleTalk\nGround \nTruth\nDreamTalk\nFigure 4. Face distortion observed in StyleTalk\u2019s output.\nDiffTalk\nGround \nTruth\nDreamTalk\nFigure 5. Comparisons with DiffTalk.\nStyle \nreference\nPainting\nToonified \nportrait\nSculpture\nFigure 6. The results generated using out-of-domain portraits.\nBesides, as shown in Fig. 4, StyleTalk occasionally gener-\nates videos where the face distorts suddenly.\nFig. 5 present comparisons with DiffTalk.\nDiffTalk\nstruggles with lip synchronization and introduces jitteriness\nand artifacts in the mouth region.\nIn contrast, DreamTalk excels in producing realistic talk-\ning faces that not only mirror the reference speaking style\nbut also achieve precise lip synchronization and superior\nvideo quality.\nGeneralization\nCapabilities.\nSupplementary\nVideo\nEmotion\nFear\nEmotion\nAngry\nPortrait\nGround \nTruth\nGenerated\nresults\nResults on\nsame portrait\nFigure 7. The results of speaking style prediction. The fourth\ncolumn displays samples generated with predicted styles applied\nto the same portrait for clearer comparisons.\nw/o Lip Expert\nPortrait\nStyle reference\nFull\nUncond Lip Expert\nFigure 8. Ablation study results\ndemonstrates our method\u2019s capability to produce realistic\ntalking head videos for out-of-domain portraits (paintings,\ntoonified portraits, and sculptures, shown in Fig. 6), speech\nin various languages, noisy audio input, and songs. Ap-\npendix B.1 provides more analysis.\nResults of Speaking Style Prediction. Fig. 7 presents the\nresults of speaking style predictions. The speakers with an-\ngry emotion are sampled from MEAD while those with fear\nemotion are sampled from RAVEDESS. The style predic-\ntor, utilizing emotional audio and neutral portraits from test\nvideos, adeptly deduces personalized speaking styles that\nalign with those observed in the original videos. It demon-\nstrates the capacity to discern subtle expressions within the\nsame emotion. For instance, for samples with angry emo-\ntion, the first-row speaker exhibits narrowed eyes, in con-\ntrast to the second-row speaker\u2019s intense, glaring stare. For\nsamples with angry emotion, the first-row speaker\u2019s eyes\nand mouth are open, whereas the second-row speaker com-\nbines narrowed eyes with a contorted facial expression.\nThe ablation study and user study for the style predictor\nare presented in Appendix A.\n4.3. Ablation Study\nTo analyze the contributions of our designs, we conduct\nan ablation study with two variants: (1) remove the style-\n7\nMethod\nSSIM\u2191 F-LMD\u2193 M-LMD\u2193 Syncconf\u2191\nw/o Lip Expert\n0.85\n1.90\n3.07\n2.63\nUncond Lip Expert\n0.83\n2.19\n3.42\n4.51\nFull\n0.86\n1.93\n2.91\n3.78\nTable 2. The results of DreamTalk\u2019s ablation study on MEAD.\nWe omit CPBD scores since there are no significant differences\nbetween variants on CPBD.\nFigure 9. t-SNE visualization of style codes from 15 speakers.\nEach color stands for style codes from an identical speaker.\naware lip expert (w/o lip expert); (2) trained with uncon-\nditional lip expert (uncond lip expert). Our full model is\ndenoted as Full.\nFig. 8 and Tab. 2 present our ablation study results. The\nvariant w/o lip expert exhibits a decline in lip-sync accu-\nracy on the emotional dataset MEAD, despite its compet-\nitive F-LMD score indicating expressive facial generation.\nConversely, uncond lip expert secures a superior SyncNet\nconfidence score at the expense of speaking style expres-\nsiveness. The Full model achieves a harmonious balance,\nensuring both precise lip synchronization and vivid expres-\nsions, thanks to the style-aware lip expert directing the dif-\nfusion model\u2019s expressive potential.\n4.4. Style Code Visualization\nUsing t-distributed stochastic neighbor embedding (t-\nSNE) [75], we map style codes from the MEAD dataset\u2019s\n15 speakers into a 2D space. These speakers exhibit 22 dis-\ntinct speaking styles, comprising seven emotions at three\nintensity levels, alongside a neutral style. For each style,\nstyle codes are extracted from 10 randomly chosen videos.\nFig. 9 reveals that style codes from identical speakers\ntend to cluster, despite some codes sharing identical emo-\ntion categories. This suggests that the variance in speaking\nstyles attributable to individual speaker idiosyncrasies out-\nweighs that induced by different emotions, thereby under-\npinning the rationale for using portrait information to infer\nspeaking styles. We also observe that each speaker\u2019s style\ncode distribution exhibits both common patterns and indi-\nvidualized characteristics. We present these intriguing ob-\nservations in Appendix B.2.\n4.5. Style Manipulation\nAdjusting the scale of Classifer-free Guidance. As elab-\norated in Sec. 3.3, the scale factor \u03c9 in the classifier-free\nguidance scheme modulates the effect of the input style.\nAdjusting \u03c9 either amplifies or attenuates the designated\n! = 0.5\n! = 0\nStyle reference\n! = 1.5\n! = 1\nFigure 10. Effect of the input style controlled by adjusting the\nscale \u03c9 of the classifier-free guidance.\nStyle A\nStyle B\nInterpolation\nFigure 11. The results of speaking style interpolation.\nMethods\nLip Sync\u2191 Realness\u2191 Style Consistency\u2191\nMakeItTalk [105]\n1.94\n2.03\n1.65\nWav2Lip [49]\n2.29\n1.45\n1.14\nPC-AVS [104]\n2.26\n1.81\n1.86\nAVCT [84]\n2.31\n2.21\n1.72\nGC-AVT [37]\n2.35\n1.20\n1.88\nEAMM [30]\n1.81\n1.40\n1.78\nStyleTalk [46]\n2.35\n2.29\n2.08\nSadTalker [100]\n2.37\n2.38\n1.73\nPD-FGC [78]\n1.95\n1.61\n2.31\nEAT [20]\n2.24\n1.65\n2.29\nDreamTalk\n2.55\n2.60\n2.46\nGround Truth\n3.03\n2.89\n1.83\nTable 3. User study results.\nstyle, as shown in Fig. 10. When \u03c9 = 0, DreamTalk pro-\nduces a talking head with a neutral expression. We observed\nthat when the scale factor \u03c9 exceeds 2, there is a noticeable\ndecline in lip-sync accuracy.\nStyle Code Interpolation. Leveraging the style space, we\ncan modify speaking styles via style code manipulation.\nFig. 11 illustrates that linear interpolation between style\ncodes results in a seamless transition of generated speaking\nstyles. This interpolation process allows for style intensity\nmodulation and the generation of novel speaking styles.\n4.6. User Study\nWe conduct a user study of 20 participants. We gener-\nate the test samples covering multiple speaking styles and\nspeakers. For each method, the participant is required to\nscore 10 videos sampled from the test samples and is asked\nto give a rating (from 1 to 5, 5 is the best) on three aspects:\n(1) the Lip sync quality, (2) the realness of results, and (3)\nthe style consistency between the generated videos and the\nstyle reference. As shown in Tab. 3, our method outper-\nforms existing approaches across all aspects, particularly in\nstyle consistency, highlighting its superior capabilities.\n8\n5. Conclusion\nIn this work, we propose DreamTalk, a novel approach\nleveraging diffusion models for generating expressive talk-\ning heads. Our method aims to excel in diverse speaking\nstyles while minimizing dependence on extra style refer-\nences. We develop a denoising network for creating ex-\npressive, audio-driven facial motions and introduce a style-\naware lip expert to optimize lip-sync without compromis-\ning style expressiveness. Additionally, we devise a style\npredictor that infers speaking styles directly from audio,\neliminating the need for video references. The efficacy of\nDreamTalk is validated through extensive experiments.\nAcknowledgements. This work is supported by Alibaba\nGroup through Alibaba Research Intern Program.\nWe\nwould like to thank Xinya Ji, Borong Liang, Yan Pan, and\nSuzhen Wang for their generous help with the comparisons.\nReferences\n[1] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:\nGesture diffusion model with clip latents. arXiv preprint\narXiv:2303.14613, 2023. 1, 2\n[2] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli.\nwav2vec 2.0: A framework for self-\nsupervised learning of speech representations. Advances in\nneural information processing systems, 33:12449\u201312460,\n2020. 16\n[3] Dan Bigioi, Shubhajit Basak, Hugh Jordan, Rachel Mc-\nDonnell, and Peter Corcoran. Speech driven video editing\nvia an audio-conditioned diffusion model. arXiv preprint\narXiv:2301.04474, 2023. 2\n[4] Volker Blanz and Thomas Vetter.\nA morphable model\nfor the synthesis of 3d faces. In Proceedings of the 26th\nannual conference on Computer graphics and interactive\ntechniques, pages 187\u2013194, 1999. 3\n[5] Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and\nChenliang Xu. Lip movements generation at a glance. In\nECCV, pages 520\u2013535, 2018. 2\n[6] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chen-\nliang Xu. Hierarchical cross-modal talking face generation\nwith dynamic pixel-wise loss. In CVPR, pages 7832\u20137841,\n2019. 2, 5\n[7] Lele Chen, Guofeng Cui, Celong Liu, Zhong Li, Ziyi Kou,\nYi Xu, and Chenliang Xu. Talking-head generation with\nrhythmic head motion. In ECCV, pages 35\u201351. Springer,\n2020. 2\n[8] Weidong Chen, Xiaofeng Xing, Xiangmin Xu, Jichen Yang,\nand Jianxin Pang. Key-sparse transformer for multimodal\nspeech emotion recognition. In ICASSP 2022-2022 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 6897\u20136901. IEEE, 2022. 17\n[9] Joon Son Chung and Andrew Zisserman. Out of time: auto-\nmated lip sync in the wild. In Asian conference on computer\nvision, pages 251\u2013263. Springer, 2016. 5\n[10] Joon Son Chung, Amir Jamaludin, and Andrew Zisserman.\nYou said that? arXiv preprint arXiv:1705.02966, 2017. 2\n[11] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman.\nVoxceleb2:\nDeep speaker recognition.\narXiv preprint\narXiv:1806.05622, 2018. 5\n[12] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw, Anurag\nRanjan, and Michael J Black. Capture, learning, and syn-\nthesis of 3d speaking styles. In CVPR, pages 10101\u201310111,\n2019. 1\n[13] Radek Dan\u02c7e\u02c7cek, Kiran Chhatre, Shashank Tripathi, Yan-\ndong Wen, Michael J Black, and Timo Bolkart. Emotional\nspeech-driven animation with content-emotion disentangle-\nment. arXiv preprint arXiv:2306.08990, 2023. 2\n[14] Dipanjan Das, Sandika Biswas, Sanjana Sinha, and Bro-\njeshwar Bhowmick. Speech-driven facial animation using\ncascaded gans for learning of motion and texture. In ECCV,\npages 408\u2013424. Springer, 2020. 2\n[15] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde\nJia, and Xin Tong. Accurate 3d face reconstruction with\nweakly-supervised learning: From single image to image\nset. In CVPRW, pages 0\u20130, 2019. 15\n[16] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. NeurIPS, 2021. 1, 2\n[17] Chenpng Du, Qi Chen, Tianyu He, Xu Tan, Xie Chen, Kai\nYu, Sheng Zhao, and Jiang Bian.\nDae-talker: High fi-\ndelity speech-driven talking face generation with diffusion\nautoencoder. arXiv preprint arXiv:2303.17550, 2023. 2\n[18] Ohad Fried, Ayush Tewari, Michael Zollh\u00f6fer, Adam\nFinkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova,\nZeyu Jin, Christian Theobalt, and Maneesh Agrawala. Text-\nbased editing of talking-head video. ACM Transactions on\nGraphics (TOG), 38(4):1\u201314, 2019. 2\n[19] Chris Frith.\nRole of facial expressions in social interac-\ntions. Philosophical Transactions of the Royal Society B:\nBiological Sciences, 364(1535):3453\u20133458, 2009. 1\n[20] Yuan Gan, Zongxin Yang, Xihang Yue, Lingyun Sun,\nand Yi Yang.\nEfficient emotional adaptation for audio-\ndriven talking-head generation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 22634\u201322645, 2023. 1, 2, 5, 8, 16\n[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville,\nand Yoshua Bengio. Generative adversarial nets. Advances\nin neural information processing systems, 27, 2014. 1\n[22] Jiazhi Guan, Zhanwang Zhang, Hang Zhou, Tianshu Hu,\nKaisiyuan Wang, Dongliang He, Haocheng Feng, Jingtuo\nLiu, Errui Ding, Ziwei Liu, et al. Stylesync: High-fidelity\ngeneralized and personalized lip sync in style-based gener-\nator. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 1505\u20131515,\n2023. 2\n[23] Yudong Guo, Keyu Chen, Sen Liang, Yongjin Liu, Hujun\nBao, and Juyong Zhang.\nAd-nerf: Audio driven neural\nradiance fields for talking head synthesis. arXiv preprint\narXiv:2103.11078, 2021. 2\n[24] Siddharth Gururani, Arun Mallya, Ting-Chun Wang, Rafael\nValle, and Ming-Yu Liu. Space: Speech-driven portrait an-\nimation with controllable expression. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 20914\u201320923, 2023. 2\n9\n[25] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 4\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Infor-\nmation Processing Systems, pages 6840\u20136851. Curran As-\nsociates, Inc., 2020. 1, 2, 3, 4, 5\n[27] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al.\nIma-\ngen video: High definition video generation with diffusion\nmodels. arXiv preprint arXiv:2210.02303, 2022. 1\n[28] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai,\nKushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman\nMohamed. Hubert: Self-supervised speech representation\nlearning by masked prediction of hidden units. IEEE/ACM\nTransactions on Audio, Speech, and Language Processing,\n29:3451\u20133460, 2021. 16\n[29] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Wayne Wu,\nChen Change Loy, Xun Cao, and Feng Xu. Audio-driven\nemotional video portraits. In CVPR, pages 14080\u201314089,\n2021. 2\n[30] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne\nWu, Feng Xu, and Xun Cao. Eamm: One-shot emotional\ntalking face via audio-based emotion-aware motion model.\narXiv preprint arXiv:2205.15278, 2022. 1, 2, 5, 8, 15, 16\n[31] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Hui-\nwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani.\nImagic: Text-based real image editing with diffusion mod-\nels. arXiv, 2022. 2\n[32] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. arXiv preprint arXiv:1412.6980,\n2014. 16\n[33] Wing-Fung Ku, Wan-Chi Siu, Xi Cheng, and H Anthony\nChan. Intelligent painter: Picture composition with resam-\npling diffusion model. arXiv, 2022. 2\n[34] Avisek Lahiri, Vivek Kwatra, Christian Frueh, John Lewis,\nand Chris Bregler. Lipsync3d: Data-efficient learning of\npersonalized 3d talking faces from video using pose and\nlighting normalization. In CVPR, pages 2755\u20132764, 2021.\n2\n[35] Lincheng Li, Suzhen Wang, Zhimeng Zhang, Yu Ding, Yix-\ning Zheng, Xin Yu, and Changjie Fan. Write-a-speaker:\nText-based emotional and rhythmic talking-head genera-\ntion. In AAAI, pages 1911\u20131920, 2021. 2\n[36] Yongyuan Li, Xiuyuan Qin, Chao Liang, and Mingqiang\nWei. Hdtr-net: A real-time high-definition teeth restora-\ntion network for arbitrary talking face generation methods.\narXiv preprint arXiv:2309.07495, 2023. 16\n[37] Borong Liang, Yan Pan, Zhizhi Guo, Hang Zhou, Zhibin\nHong, Xiaoguang Han, Junyu Han, Jingtuo Liu, Errui Ding,\nand Jingdong Wang. Expressive talking head generation\nwith granular audio-visual control. In CVPR, pages 3387\u2013\n3396, 2022. 1, 2, 5, 8\n[38] Pengfei Liu, Wenjin Deng, Hengda Li, Jintai Wang, Yinglin\nZheng, Yiwei Ding, Xiaohu Guo, and Ming Zeng. Music-\nface: Music-driven expressive singing face synthesis. arXiv\npreprint arXiv:2303.14044, 2023. 14\n[39] Xian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne\nWu, and Bolei Zhou.\nSemantic-aware implicit neural\naudio-driven video portrait generation.\narXiv preprint\narXiv:2201.07786, 2022. 2\n[40] Steven R Livingstone and Frank A Russo.\nThe ryer-\nson audio-visual database of emotional speech and song\n(ravdess): A dynamic, multimodal set of facial and vocal\nexpressions in north american english.\nPloS one, 13(5):\ne0196391, 2018. 5\n[41] Yuanxun Lu, Jinxiang Chai, and Xun Cao.\nLive speech\nportraits: real-time photorealistic talking-head animation.\nACM Transactions on Graphics (TOG), 40(6):1\u201317, 2021.\n2\n[42] Andreas Lugmayr, Martin Danelljan, Andres Romero,\nFisher Yu, Radu Timofte, and Luc Van Gool. Repaint: In-\npainting using denoising diffusion probabilistic models. In\nCVPR, 2022. 2\n[43] Shitong Luo and Wei Hu.\nDiffusion probabilistic mod-\nels for 3d point cloud generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2837\u20132845, 2021. 2\n[44] Tian Lv, Yu-Hui Wen, Zhiyao Sun, Zipeng Ye, and Yong-\nJin Liu.\nGenerating smooth and facial-details-enhanced\ntalking head video: A perspective of pre and post processes.\nIn Proceedings of the 30th ACM International Conference\non Multimedia, pages 7079\u20137083, 2022. 16\n[45] Yifeng Ma, Suzhen Wang, Yu Ding, Bowen Ma, Tangjie\nLv, Changjie Fan, Zhipeng Hu, Zhidong Deng, and Xin\nYu. Talkclip: Talking head generation with text-guided ex-\npressive speaking styles. arXiv preprint arXiv:2304.00334,\n2023. 1, 2\n[46] Yifeng Ma, Suzhen Wang, Zhipeng Hu, Changjie Fan,\nTangjie Lv, Yu Ding, Zhidong Deng, and Xin Yu. Styletalk:\nOne-shot talking head generation with controllable speak-\ning styles. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, 2023. 1, 2, 4, 5, 8, 14, 15, 16\n[47] Soumik Mukhopadhyay, Saksham Suri, Ravi Teja Gadde,\nand Abhinav Shrivastava.\nDiff2lip: Audio conditioned\ndiffusion models for lip-synchronization.\narXiv preprint\narXiv:2308.09716, 2023. 2\n[48] Niranjan D Narvekar and Lina J Karam. A no-reference\nperceptual image sharpness metric based on a cumulative\nprobability of blur detection. In 2009 International Work-\nshop on Quality of Multimedia Experience, pages 87\u201391.\nIEEE, 2009. 5\n[49] KR Prajwal, Rudrabha Mukhopadhyay, Vinay P Nambood-\niri, and CV Jawahar. A lip sync expert is all you need for\nspeech to lip generation in the wild. In Proceedings of the\n28th ACM International Conference on Multimedia, pages\n484\u2013492, 2020. 2, 4, 5, 8\n[50] Zhiwu Qing, Shiwei Zhang, Jiayu Wang, Xiang Wang, Yu-\njie Wei, Yingya Zhang, Changxin Gao, and Nong Sang.\nHierarchical spatio-temporal decoupling for text-to-video\ngeneration. arXiv preprint arXiv:2312.04483, 2023. 2\n[51] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 3\n10\n[52] Yurui Ren, Ge Li, Yuanqi Chen, Thomas H Li, and Shan\nLiu. Pirenderer: Controllable portrait image generation via\nsemantic neural rendering. In ICCV, pages 13759\u201313768,\n2021. 3, 5, 16\n[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 1,\n2\n[54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv, 2022. 2\n[55] Najmeh Sadoughi and Carlos Busso. Speech-driven expres-\nsive talking lips with conditional sequential generative ad-\nversarial networks. IEEE Transactions on Affective Com-\nputing, 12(4):1031\u20131044, 2019. 2\n[56] Pooyan Safari, Miquel India, and Javier Hernando. Self-\nattention encoding and pooling for speaker recognition.\narXiv preprint arXiv:2008.01077, 2020. 4, 15\n[57] Shuai Shen, Wanhua Li, Zheng Zhu, Yueqi Duan, Jie Zhou,\nand Jiwen Lu. Learning dynamic facial radiance fields for\nfew-shot talking head synthesis. In European Conference\non Computer Vision, pages 666\u2013682. Springer, 2022. 2\n[58] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li,\nZheng Zhu, Jie Zhou, and Jiwen Lu. Difftalk: Crafting dif-\nfusion models for generalized talking head synthesis. arXiv\npreprint arXiv:2301.03786, 2023. 2, 5\n[59] Aliaksandr\nSiarohin,\nSt\u00e9phane\nLathuili\u00e8re,\nSergey\nTulyakov, Elisa Ricci, and Nicu Sebe.\nFirst order mo-\ntion model for image animation.\nAdvances in Neural\nInformation Processing Systems, 32:7137\u20137147, 2019. 15\n[60] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 2\n[61] Sanjana Sinha, Sandika Biswas, Ravindra Yadav, and Bro-\njeshwar Bhowmick. Emotion-controllable generalized talk-\ning face generation.\narXiv preprint arXiv:2205.01155,\n2022. 2\n[62] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 1, 2\n[63] Jiaming Song,\nChenlin Meng,\nand Stefano Ermon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 5\n[64] Linsen Song, Wayne Wu, Chen Qian, Ran He, and\nChen Change Loy. Everybody\u2019s talkin\u2019: Let me talk as you\nwant. arXiv preprint arXiv:2001.05201, 2020. 2\n[65] Yang Song, Jingwen Zhu, Dawei Li, Xiaolong Wang, and\nHairong Qi. Talking face generation by conditional recur-\nrent adversarial network. arXiv preprint arXiv:1804.04786,\n2018. 2\n[66] Micha\u0142 Stypu\u0142kowski, Konstantinos Vougioukas, Sen He,\nMaciej Zi\u02dbeba, Stavros Petridis, and Maja Pantic. Diffused\nheads: Diffusion models beat gans on talking-face genera-\ntion. arXiv preprint arXiv:2301.03396, 2023. 2\n[67] Yasheng Sun, Hang Zhou, Kaisiyuan Wang, Qianyi Wu,\nZhibin Hong, Jingtuo Liu, Errui Ding, Jingdong Wang,\nZiwei Liu, and Koike Hideki.\nMasked lip-sync predic-\ntion by audio-visual contextual exploitation in transform-\ners. In SIGGRAPH Asia 2022 Conference Papers, pages\n1\u20139, 2022. 2\n[68] Supasorn\nSuwajanakorn,\nSteven\nM\nSeitz,\nand\nIra\nKemelmacher-Shlizerman. Synthesizing obama: learning\nlip sync from audio. ACM Transactions on Graphics (ToG),\n36(4):1\u201313, 2017. 2\n[69] Shuai Tan, Bin Ji, and Ye Pan.\nEmmn: Emotional mo-\ntion memory network for audio-driven emotional talking\nface generation.\nIn Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 22146\u2013\n22156, 2023. 2\n[70] Anni Tang, Tianyu He, Xu Tan, Jun Ling, Runnan Li, Sheng\nZhao, Li Song, and Jiang Bian.\nMemories are one-to-\nmany mapping alleviators in talking face generation. arXiv\npreprint arXiv:2212.05005, 2022. 2\n[71] Jiaxiang Tang, Kaisiyuan Wang, Hang Zhou, Xiaokang\nChen, Dongliang He, Tianshu Hu, Jingtuo Liu, Gang Zeng,\nand Jingdong Wang.\nReal-time neural radiance talking\nportrait synthesis via audio-spatial decomposition. arXiv\npreprint arXiv:2211.12368, 2022. 2\n[72] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel\nCohen-or, and Amit Haim Bermano. Human motion diffu-\nsion model. In The Eleventh International Conference on\nLearning Representations, 2023. 1, 2\n[73] Joachim Thiemann, Nobutaka Ito, and Emmanuel Vin-\ncent.\nThe diverse environments multi-channel acoustic\nnoise database (demand): A database of multichannel en-\nvironmental noise recordings. In Proceedings of Meetings\non Acoustics. AIP Publishing, 2013. 14\n[74] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian\nTheobalt, and Matthias Nie\u00dfner. Neural voice puppetry:\nAudio-driven facial reenactment. In ECCV, pages 716\u2013731.\nSpringer, 2020. 2\n[75] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research, 9\n(11), 2008. 8\n[76] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser,\nand Illia Polosukhin.\nAttention is all you need.\nIn Ad-\nvances in Neural Information Processing Systems. Curran\nAssociates, Inc., 2017. 4\n[77] Konstantinos Vougioukas, Stavros Petridis, and Maja Pan-\ntic. Realistic speech-driven facial animation with gans. In-\nternational Journal of Computer Vision, pages 1\u201316, 2019.\n2\n[78] Duomin Wang, Yu Deng, Zixin Yin, Heung-Yeung Shum,\nand Baoyuan Wang. Progressive disentangled representa-\ntion learning for fine-grained controllable talking head syn-\nthesis.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 17979\u2013\n17989, 2023. 1, 5, 8\n[79] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2\n11\n[80] Jiayu Wang, Kang Zhao, Yifeng Ma, Shiwei Zhang, Yingya\nZhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Face-\ncomposer: A unified model for versatile facial content cre-\nation. In Thirty-seventh Conference on Neural Information\nProcessing Systems, 2023. 2\n[81] Jiayu Wang, Kang Zhao, Shiwei Zhang, Yingya Zhang, Yu-\njun Shen, Deli Zhao, and Jingren Zhou. Lipformer: High-\nfidelity and generalizable talking face generation with a pre-\nlearned facial codebook. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13844\u201313853, 2023. 2\n[82] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian Yang,\nWayne Wu, Chen Qian, Ran He, Yu Qiao, and Chen Change\nLoy. Mead: A large-scale audio-visual dataset for emo-\ntional talking-face generation. In ECCV, pages 700\u2013717.\nSpringer, 2020. 2, 5\n[83] Suzhen Wang, Lincheng Li, Yu Ding, Changjie Fan, and\nXin Yu. Audio2head: Audio-driven one-shot talking-head\ngeneration with natural head motion. IJCAI, 2021. 2, 16\n[84] Suzhen Wang, Lincheng Li, Yu Ding, and Xin Yu. One-shot\ntalking face generation from single-speaker audio-visual\ncorrelation learning. In AAAI, 2022. 2, 5, 8\n[85] Suzhen Wang, Yifeng Ma, and Yu Ding. Exploring comple-\nmentary features in multi-modal speech emotion recogni-\ntion. In ICASSP 2023-2023 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP),\npages 1\u20135. IEEE, 2023. 17\n[86] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and\nJingren Zhou. Videocomposer: Compositional video syn-\nthesis with motion controllability. In NeurIPS, 2023. 1,\n2\n[87] Xiang Wang,\nShiwei Zhang,\nHan Zhang,\nYu Liu,\nYingya Zhang, Changxin Gao, and Nong Sang.\nVide-\nolcm: Video latent consistency model.\narXiv preprint\narXiv:2312.09109, 2023. 2\n[88] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. IEEE transactions on image pro-\ncessing, 13(4):600\u2013612, 2004. 5\n[89] Yujie Wei, Shiwei Zhang, Zhiwu Qing, Hangjie Yuan,\nZhiheng Liu, Yu Liu, Yingya Zhang, Jingren Zhou, and\nHongming Shan.\nDreamvideo: Composing your dream\nvideos with customized subject and motion. arXiv preprint\narXiv:2312.04433, 2023. 2\n[90] Chao Xu, Junwei Zhu, Jiangning Zhang, Yue Han, Wenqing\nChu, Ying Tai, Chengjie Wang, Zhifeng Xie, and Yong Liu.\nHigh-fidelity generalized emotional talking face generation\nwith multi-modal emotion space learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 6609\u20136619, 2023. 1, 2, 16\n[91] Chao Xu, Shaoting Zhu, Junwei Zhu, Tianxin Huang,\nJiangning Zhang, Ying Tai, and Yong Liu.\nMultimodal-\ndriven talking face generation, face swapping, diffusion\nmodel. arXiv preprint arXiv:2305.02594, 2023. 2\n[92] Zipeng Ye, Zhiyao Sun, Yu-Hui Wen, Yanan Sun, Tian Lv,\nRan Yi, and Yong-Jin Liu. Dynamic neural textures: Gen-\nerating talking-face videos with continuously controllable\nexpressions. arXiv preprint arXiv:2204.06180, 2022. 16\n[93] Zhenhui Ye, Ziyue Jiang, Yi Ren, Jinglin Liu, JinZheng He,\nand Zhou Zhao. Geneface: Generalized and high-fidelity\naudio-driven 3d talking face synthesis.\narXiv preprint\narXiv:2301.13430, 2023. 2\n[94] Ran Yi, Zipeng Ye, Juyong Zhang, Hujun Bao, and Yong-\nJin Liu. Audio-driven talking face video generation with\nlearning-based personalized head pose.\narXiv preprint\narXiv:2002.10137, 2020. 2\n[95] Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn\nWong, and Baoyuan Wang.\nTalking head generation\nwith probabilistic audio-to-visual diffusion priors.\narXiv\npreprint arXiv:2212.04248, 2022. 2\n[96] Zhentao Yu, Zixin Yin, Deyu Zhou, Duomin Wang, Finn\nWong, and Baoyuan Wang. Talking head generation with\nprobabilistic audio-to-visual diffusion priors. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 7645\u20137655, 2023. 2\n[97] Chenxu Zhang, Saifeng Ni, Zhipeng Fan, Hongbo Li, Ming\nZeng, Madhukar Budagavi, and Xiaohu Guo. 3d talking\nface with personalized pose dynamics. IEEE Transactions\non Visualization and Computer Graphics, 2021. 2\n[98] Chenxu Zhang, Yifan Zhao, Yifei Huang, Ming Zeng,\nSaifeng Ni, Madhukar Budagavi, and Xiaohu Guo. Facial:\nSynthesizing dynamic talking face with implicit attribute\nlearning. In ICCV, pages 3867\u20133876, 2021. 2\n[99] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou.\nI2vgen-xl:\nHigh-quality image-to-video\nsynthesis via cascaded diffusion models.\narXiv preprint\narXiv:2311.04145, 2023. 2\n[100] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker:\nLearning realistic 3d motion coefficients for stylized audio-\ndriven single image talking face animation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 8652\u20138661, 2023. 2, 5, 8\n[101] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan.\nFlow-guided one-shot talking face generation with a high-\nresolution audio-visual dataset.\nIn CVPR, pages 3661\u2013\n3670, 2021. 2, 5\n[102] Zhimeng Zhang, Zhipeng Hu, Wenjin Deng, Changjie Fan,\nTangjie Lv, and Yu Ding. Dinet: Deformation inpainting\nnetwork for realistic face visually dubbing on high resolu-\ntion video. arXiv preprint arXiv:2303.03988, 2023. 2\n[103] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang\nWang. Talking face generation by adversarially disentan-\ngled audio-visual representation.\nIn AAAI, pages 9299\u2013\n9306, 2019. 2\n[104] Hang Zhou, Yasheng Sun, Wayne Wu, Chen Change Loy,\nXiaogang Wang, and Ziwei Liu. Pose-controllable talking\nface generation by implicitly modularized audio-visual rep-\nresentation. In CVPR, pages 4176\u20134186, 2021. 2, 5, 8\n[105] Yang Zhou, Xintong Han, Eli Shechtman, Jose Echevar-\nria, Evangelos Kalogerakis, and Dingzeyu Li. Makelttalk:\nspeaker-aware talking-head animation. ACM Transactions\non Graphics (TOG), 39(6):1\u201315, 2020. 2, 5, 8\n12\nDreamTalk: When Expressive Talking Head Generation\nMeets Diffusion Probabilistic Models\nSupplementary Material\nw/o Cross-ID training\nRegression\nGround Truth\nFull\nw/o speaker info\nFigure 12.\nThe qualitative results of style predictor\u2019s ablation\nstudy.\nA. Additional Evaluation for Speaking Style\nPrediction\nA.1. Ablation Study\nTo evaluate the impact of our design choices, we conduct\nan ablation study with three variants: (1) omitting speaker\ninformation and relying solely on audio for prediction (w/o\nspeaker info); (2) during model training, the speaker info\nand audio are both obtained from the same video (w/o\ncross-ID training); (3) employing a regression model in-\nstead of a diffusion model for prediction (regression). Our\nfull model is denoted as Full. When generating samples for\nevaluation, the facial images and audio we use are sourced\nfrom videos of the same individual expressing different\nemotions(e.g. the face image is from a happy video while\nthe audio is from an angry one.). This generation approach\nbetter aligns with real-world applications.\nHow to quantitatively evaluate the performance of speak-\ning style prediction has not been explored before. we devise\nthree metrics:\n\u2022 Style Code Distance (SCD) We extract the style codes\nfrom the videos that provide the audio input and com-\npute the L2 distance between the predicted style codes\nand these style codes.\n\u2022 Motion Distance (MD) We use the predicted style codes\nand the audio used for prediction to generate face motions\nand compute the L2 distance between the generated face\nmotions and the face motions extracted from the ground\ntruth videos.\n\u2022 Style Accuracy (SA) We split test videos into differ-\nent speaking styles and train a style classifier to classify\nwhich style a face motion sequence belongs to. Then, We\nclassify the face motions generated using predicted style\ncodes and report the accuracy. Specifically, we put the\nvideos from the same speaker, emotion, and intensity into\none style. We evaluate this metric on MEAD only since\nthe number of RAVEDESS videos for each style is inad-\nequate to train a style classifier. The ground truth testing\nMethod\nSCD\u2193\nMD\u2193\nSA\u2191\nw/o speaker info\n0.49\n0.28\n64.3\nw/o cross-ID training\n0.68\n0.45\n28.1\nregression\n0.56\n0.32\n55.1\nFull\n0.42\n0.23\n78.6\nTable 4. The ablation study results of the style predictor.\nset gets 92.5% accuracy.\nWe refrain from devising image-level metrics, such as train-\ning an image classifier for speaking style classification, due\nto several critical considerations. Firstly, factors in images\nthat are irrelevant to expression, such as the speaker\u2019s iden-\ntity and background elements, can adversely impact the ac-\ncurate prediction of nuanced speaking styles. Secondly, in-\naccuracies introduced by the rendering process may further\nadditionally hinder the accurate discernment of these subtle\nspeaking styles.\nThe results are shown in Tab. 4 and Fig. 12. The w/o\nspeaker info variant successfully predicts emotions from\naudio but occasionally fails to maintain consistency be-\ntween the predicted speaking style and speaker identity,\nleading to poor identity preservation. This underscores the\nimportance of speaker information in predicting speaking\nstyles.\nAlthough in experiments, we observed that w/o\ncross-ID training achieves slightly better performance than\nFull when the input portrait and audio are from the same\nvideo, it underperformed, often failing to predict the cor-\nrect emotion, when inputs were from different videos. This\nsuggests that identity 3DMM parameters may convey some\nexpression information, and without cross-ID training, the\nmodel might derive emotional cues from this leaked infor-\nmation rather than the audio. The regression variant strug-\ngles to generate accurate expressions for certain data, high-\nlighting the superior distribution-learning capability of dif-\nfusion models in facilitating speaking style prediction.\nA.2. User Study\nIn our user study, we evaluate the alignment between the\noriginal and predicted speaking styles. Directly assessing\nthe alignment of speaking styles can be somewhat ambigu-\nous, so we employ a comparative approach for evaluation.\nSpecifically, we create a series of video triplets. Each triplet\nconsisted of a test video from our dataset and two generated\nvideos. The first video was generated using a style code\npredicted from an input portrait, sharing the same speaker\nidentity as in the test video but displaying a neutral emo-\ntion, combined with the audio from the test video. The sec-\n13\nPortrait B\nPortrait A\nSame Audio\nPredicted Style A\nPredicted Style B\nFigure 13. Analyzing the influence of portraits on style prediction.\nThe audio conveys surprised emotion.\nond video is generated using the style code extracted from\nvideos with the same emotion but from a speaker different\nfrom the one in the test video. We recruit 20 participants.\nEach participant is then asked to evaluate 20 triplets and\nidentify which of the generated videos most accurately re-\nflected the speaking style of the test video. The videos gen-\nerated using predicted style codes are preferred in 75.8% of\nall ratings. This indicates that the style predictor is able to\ninfer personalized speaking styles that are aligned with the\naudio.\nA.3. Analyzing the Influence of Portraits\nWe analyze the influence of portraits on speaking style\nprediction by predicting speaking styles with an audio clip\nand different input portraits. The predicted styles are sub-\nsequently applied to an identical portrait for a clearer com-\nparison. As shown in Fig. 13, the predicted speaking styles\nmatch the subtle identity characteristics, such as gender, of\nthe input portraits. The predicted style A generated more\nfeminine results. This validates the necessity of integrating\nportrait information during style prediction.\nB. Additional Results for Expressive Talking\nHead Generation\nB.1. Analysis on Generalization Capabilities\nSongs.\nAs demonstrated in Supplementary Video, our\nmethod successfully generates reasonable results for songs,\neven those with accompaniment, despite this being signifi-\ncantly different from the training dataset\u2019s data distribution.\nA noticeable decline in lip-sync accuracy is observed when\nthe accompaniment volume is excessively high. We conduct\na comparative analysis of lip-sync performance between\nsongs with accompaniment and songs with removed accom-\npaniment (using data from the SingFace Dataset [38]). It\nis found that the accompaniment adversely affects lip-sync,\nleading to mouth movements resembling mumbling. Ad-\ndressing the negative impact of accompaniment on lip-sync\naccuracy presents an interesting avenue for future research.\nSpeech in Multiple Languages.\nSupplementary Video\nshows that our method generates satisfactory results with\nspeech in French, Chinese, Spanish, German, Italian,\nJapanese, and Korean. The versatility of wav2vec features\naids application across various languages. Additionally, the\n(a)\n(b)\n(c)\nFigure 14. t-SNE visualization of style codes for 3 speakers, with\ndarker hues representing increased emotional intensity.\ninclusion of multilingual talking head videos from Vox-\nceleb2 enhances generalization.\nNoisy Audio. Supplementary Video demonstrates that our\nmethod yields satisfactory outcomes when processing audio\nmixed with multiple noise types and intensities. We em-\nploy noise recordings from typical talking head application\nenvironments\u2014meetings, offices, and cafeterias\u2014sourced\nfrom the DEMAND dataset [73]. Using a publicly avail-\nable tool1, we blend the audio with noise at SNRs of 20 dB,\n10 dB, and 0 dB. Remarkably, our method maintains perfor-\nmance even at 0 dB SNR, where the noise is as loud as the\nspeech and significantly impairs speech intelligibility and\nclarity.\nB.2. More Results of Style Code Visualization\nWe observe that each speaker\u2019s style code distribution\nexhibits both common patterns and individualized charac-\nteristics. Common patterns include: Firstly, speaking styles\nof different emotions cluster together first, with styles of\nlower intensity being closer to neutral and those of higher\nintensity being further away. Secondly, speaking styles of\nanger and disgust, as well as fear and surprise, often cluster\ntogether, as shown in Fig. 14 (b) and (c). Note that unlike\nMa et al. [46], our method does not incorporate losses to\nconstrain style space.\nFig. 14 (a) illustrates an example of individualized char-\nacteristics. The speaker\u2019s manifestation of fear closely re-\nsembles sadness, lacking the characteristic wide-eyed and\nopen-mouthed expression, thereby positioning the speaking\nstyles of fear nearer to those of sadness rather than surprise.\nEven within the same emotion, a speaker\u2019s speaking style\n1https://github.com/Sato-Kunihiko/audio-SNR\n14\ncan exhibit notable variation. In Fig. 14 (c), the speaker\u2019s\ndual expression of sadness\u2014once with clenched teeth, sim-\nilar to happy expressions, and another with depressed lip\ncorners, akin to disgust\u2014results in style codes close to the\nrespective emotions. This observation diminishes the ratio-\nnale for manually categorizing styles based on emotion and\nintensities in Ma et al. [46].\nC. Implementation Details\nC.1. Architectual Details\nDenoising Network.The audio encoder processes an input\nwindow of 11 sequential audio features, each of dimension\n1024. These features undergo dimension reduction to 256\nvia a linear layer and then are fed to a transformer encoder\ncomprising three 8-head transformer encoder layers, each\nwith a hidden size of 256. Subsequently, a linear layer trans-\nforms the output tokens to yield audio tokens sized 11\u00d7256.\nThe audio tokens are concatenated with noisy motion and\nthen added with the encoded diffusion step.\nThe style encoder ingests sequential expression param-\neters from style reference videos, each sequence sized\nN \u00d7 64. These sequences, ranging in length from 64 to\n256 frames, are initially expanded to 256 dimensions via a\nlinear layer. Subsequently, they are introduced into a trans-\nformer encoder, composed of three 8-head layers, each with\na hidden size of 256. The resulting output tokens, each with\na dimension of 256, are aggregated through self-attention\npooling [56], yielding a style code of dimension 256.\nWithin the decoder, the style code is repeated 11 times,\nsubsequently added with positional embedding to produce\nstyle tokens. These tokens, in conjunction with audio to-\nkens, are processed by a transformer decoder, encompass-\ning three 8-head layers, each with a hidden dimension of\n256. Here, style tokens serve as the query, while audio to-\nkens serve as both key and value. The middle output token\nis fed into a linear layer to predict facial motion.\nStyle-aware Lip Expert. The face mesh is obtained by\nadding the mean shape to the product of the expression pa-\nrameters and expression bases. Fig. 15 shows the architec-\nture of the audio embedder and the mouth embedder.\nStyle Predictor. The style predictor is implemented as a\ntransformer encoder comprising six 8-head transformer en-\ncoder layers, each with a hidden size of 256. The input\nfeatures are all linearly projected to 256.\nC.2. Data Details\nC.2.1\nDatasets\nMEAD. The dataset, an in-lab talking-face corpus, features\n60 speakers articulating eight emotions at three different in-\ntensity levels. When dividing the MEAD dataset into train-\ning and test subsets, we adhere to previously established\nmethodologies [30].\nConvBlock k7, 16, s1\nConvBlock k3, 32, s2\nConvRBlock k3, 32, s1\nConvRBlock k3, 32, s1\nConvBlock k3, 64, s2\nConvRBlock k3, 64, s1\nConvRBlock k3, 64, s1\nConvBlock k3, 128, s2\nConvRBlock k3, 128, s1\nConvRBlock k3, 128, s1\nConvBlock k3, 256, s2\nConvRBlock k3, 256, s1\nConvRBlock k3, 256, s1\nStyle Concatenation\nConvBlock k3, 512, s2\nConvRBlock k3, 512, s1\nConvRBlock k3, 512, s1\nAverage Pooling\nConvRBlock k3, 512, s1\nLinear, 64\nBatchNorm\nReLU\nLinear, 128\nBatchNorm\nReLU\nLinear, 512\nBatchNorm\nReLU\nLinear, 512\nBatchNorm\nReLU\nStyle Concatenation\nMax Pooling\nLinear, 512\nBatchNorm\nReLU\n(a) Mouth embedder\n(b) Audio embedder\nFigure 15. The architecture of mouth embedder and audio embed-\nder. ConvBlock consists of a 1D CNN layer and a batch normal-\nization layer. \"k3, 32, s2\" means that the kernel size is 3, the output\ndimension is 32, and the stride is 2. ConvRBlock is a ConvBlock\nwith residual connection.\nHDTF. The dataset stands out as a high-resolution, in-the-\nwild audio-visual dataset.\nWe designate 10% of HDTF\nvideos for testing and reserve the remainder for training.\nVoxceleb2. Voxceleb2 is a large-scale talking head dataset\nwith videos collected from YouTube. We redownload and\nrecrop the videos to improve their resolution to 256 \u00d7 256.\nSubsequently, approximately 80000 high-quality videos are\nselected, with 400 allocated for testing and the rest for train-\ning.\nRAVDESS. The dataset features 24 professional actors (12\nfemale, 12 male) vocalizing two lexically matched state-\nments in a neutral North American accent. It encompasses\na range of expressions in both speech and song, each articu-\nlated at two levels of emotional intensity, with an additional\nneutral expression included. We employ the speech data\nfrom RAVDESS, encompassing eight emotions, for evalua-\ntion.\nC.2.2\nData Processing\nThe original videos are cropped and resized to 256\u00d7256\npixels, aligning with the specifications in FOMM [59], and\nare sampled at 25 FPS. The 3DMM parameters are ex-\ntracted by Deep3DFace [15].\nRegarding the audio features used in the denoising net-\nwork and the style-aware lip expert, we downsample the\n15\nspeech wave into the sampling rate of 16000 and ex-\ntract acoustic features employing a pre-trained Wav2Vec2.0\nmodel [2].\nFor audio features used in the style predictor, we extract\nthem using a pre-trained HuBERT model [28]. Besides,\nwe also utilize low-level audio features including Mel Fre-\nquency Cepstrum Coefficients (MFCC), Mel-filterbank en-\nergy features (FBANK), fundamental frequency, and voice\nflag. These two type of features are concatenated to repre-\nsent the audio features used in the style predictor.\nC.3. Training Details\nOur framework is implemented on Pytorch.\nWe em-\nploy Adam [32] for optimization, with a learning rate set\nto 0.0001. The number of diffusion steps for the denoising\nnetwork and style predictor is 1000. The training batch size\nfor the denoising network, style predictor, and lip expert is\n64, 64, and 32, respectively. \u03bbdenoise, \u03bbsync, n, and w are set\nto 1, 1, 5, and 5, respectively. The number of frames for\nstyle reference and audio used in style prediction is limited\nto 64 \u2212 256, corresponding to a time length of 2.56 \u2212 10.24\nseconds. The denoising network, style predictor, and lip ex-\npert are trained on one NVIDIA Tesla A100 GPU for about\n3, 1, and 10.5 hours, respectively.\nC.3.1\nFinetuning PIRender\nThe renderer is fine-tuned with the losses in\nRen\net al. [52] using MEAD. Instead of training with the self-\nreconstruction protocol where the source frame and target\nframe are from the same video, we select the source frame\nand target frame from the same speaker with different emo-\ntions. This enables the renderer to generate emotions differ-\nent from the input portrait. This also allows our method to\nutilize portraits with emotions, unlike previous approaches\nthat are confined to using neutral portraits [30]. We also ob-\nserved that when fine-tuned only on the emotional talking\nhead dataset, the renderer struggles with identity preserva-\ntion. We argue that this problem stems from the fact that\nthe number of speakers in the emotional dataset is limited.\nTherefore, we incorporate some neutral videos in Voxceleb\ninto the data used for fine-tuning. This enhances the perfor-\nmance in identity preservation.\nC.3.2\nTraining Style-aware Lip Expert\nThe style-aware lip expert is trained to discriminate\nwhether the input audio and face motions are synchro-\nnized. We use cosine-similarity with binary cross-entropy\nloss to train the lip expert. Specifically, we compute cosine-\nsimilarity for the face motion embedding em and audio em-\nbedding ea to represent the probability that the input audio-\nmotion pair is synchronized. The training loss of the lip\nexpert is:\nLexpert = BCE(\nem \u00b7 ea\nmax(||em||2 \u00b7 ||ea||2, \u03f5)),\n(10)\nwhere \u03f5 is a small number for avoiding the division-by-zero\nerror.\nC.4. Inference Details\nThe inference of the denoising network can be acceler-\nated with DDIM. We generate samples with 10 DDIM steps\nand observe no performance drop. Generating a 30-second\nvideo offline takes 15.61 seconds, with the face motion gen-\neration only taking 1.24 seconds. During evaluation, the\nscale factor \u03c9 of classifier-free guidance is set to 1. The\nstyle predictor uses the sampling algorithm of DDPM to\npredict style codes.\nThe emotion conveyed in the style reference (video or\naudio), should remain consistent to avoid confusing the\nmodel.\nThe head pose information, which is fed into the ren-\nderer, can be derived from real videos or generated using\nexisting methods [83].\nD. Limitations and Future Work\nDespite DreamTalk\u2019s promising advancements in ex-\npressive talking head generation, it encounters several chal-\nlenges that open avenues for future research.\nFirstly, the method occasionally produces artifacts, such\nas teeth flickering, around the mouth area, particularly dur-\ning intense expressions. Generating teeth is a long-standing\nchallenge in talking head generation since the algorithm\nneeds to inpaint the teeth area that are often occluded in the\ninput portrait. This problem is exacerbated under intense\nexpressions where the teeth area expands. The issue can be\nmitigated by incorporating modules [36, 44, 92] proposed\nrecently that enhance the teeth quality. A more comprehen-\nsive solution involves developing an emotion-specific ren-\nderer, as current renderers [20, 30, 46, 90] are mainly adap-\ntations of existing facial reenactment methods with minimal\nmodifications. An emotion-aware renderer would not only\naddress the teeth generation issue but also enhance the over-\nall expressiveness of emotions.\nSecondly, DreamTalk does not account for the tempo-\nral variability in speaking styles. In real-life scenarios, a\nspeaker\u2019s style evolves over time, a feature our method cur-\nrently overlooks. For instance, at the end of a speech, our\nmethod might still produce expressions of intense emotion,\nsuch as a wide-open mouth in surprise, instead of a more\nneutral, closed-mouth expression.\nIntroducing a module\nthat dynamically predicts speaking style over time could ad-\ndress this limitation.\nThirdly, the style predictor sometimes struggles with ac-\ncurately identifying emotions in low-intensity audio clips\n16\nfrom the MEAD dataset (some audio in MEAD intensity\nlevel 1 videos). The emotion conveyed in these audio clips\nis very similar to neutral emotion and thus confusing the\nprediction. We also observed that although the videos in the\nMEAD dataset clearly express emotions due to the good\ntraining and supervision of the speakers, sometimes the au-\ndio does not correspond with the emotions that should be\nexpressed. Therefore, To enhance prediction accuracy, em-\nploying a dataset where the audio closely aligns with the\nexpressed emotions could be beneficial. Another solution\nis to incorporate textual information from audio during pre-\ndiction, a strategy commonly employed in speech emotion\nrecognition [8, 85].\nDespite these challenges, DreamTalk marks a significant\nstride in the realm of high-quality, expressive talking head\ngeneration, setting a foundation for further innovations.\nE. Ethical Consideration\nDreamTalk is able to generate realistic talking head\nvideos. This positions DreamTalk with a broad spectrum\nof potential applications, each carrying intricate societal\nimplications. While DreamTalk holds significant potential\nin amplifying and enriching human creative endeavors and\nmay pave the way for innovative tools for creative profes-\nsionals, its capabilities also harbor risks. There\u2019s a pos-\nsibility for DreamTalk to generate content that might en-\ncompass or imply sexual themes, promote hatred, or de-\npict violence. Misuse of DreamTalk could lead to negative\nrepercussions on individuals or groups, potentially erasing\nor maligning them, perpetuating stereotypes, and subject-\ning them to disrespect. Other concerns include the potential\nfor harassment, intimidation, or exploitation. Furthermore,\nDreamTalk\u2019s capabilities might be harnessed to mislead or\nspread misinformation.\nBefore releasing DreamTalk, we have implemented and\nplan to introduce several safeguards to curb potential mis-\nuse. We\u2019ve purged detrimental content from the training\ndataset and incorporated visual filters to deter users from\ncreating harmful outputs. To counteract biases in the gener-\nated results, we\u2019re enhancing the dataset\u2019s diversity by man-\nually ensuring balance, which will reduce instances of era-\nsure, stereotype perpetuation, indignity, and uneven quality\nacross inputs. Users will be advised against using images\nwithout the depicted individuals\u2019 consent to combat harass-\nment and bullying. To prevent the spread of misinforma-\ntion, all DreamTalk outputs will bear watermarks indicat-\ning their synthetic nature. Our pre-release strategy involves\na thorough risk assessment, leveraging a growing suite of\nsafety evaluations and red teaming techniques. We\u2019ll also\nscrutinize the findings from pilot tests centered on new use\ncases and conduct in-depth post-release assessments. Both\nautomated and manual monitoring mechanisms are in de-\nvelopment to preempt misuse. Our commitment remains\nsteadfast in continuously researching ways to minimize ad-\nverse societal effects.\n17\n"
  },
  {
    "title": "MobileSAMv2: Faster Segment Anything to Everything",
    "link": "https://arxiv.org/pdf/2312.09579.pdf",
    "upvote": "19",
    "text": "MobileSAMv2: Faster Segment Anything to Everything\nChaoning Zhang*\nKyung Hee University\nDongshen Han\nKyung Hee University\nSheng Zheng\nKyung Hee University\nJinwoo Choi\nKyung Hee University\nTae-Ho Kim\nNota Inc.\nChoong Seon Hong\nKyung Hee University\nAbstract\nSegment anything model (SAM) addresses two practical yet challenging segmentation tasks: segment anything (SegAny),\nwhich utilizes a certain point to predict the mask for a single object of interest, and segment everything (SegEvery), which\npredicts the masks for all objects on the image. What makes SegAny slow for SAM is its heavyweight image encoder, which\nhas been addressed by MobileSAM via decoupled knowledge distillation. The efficiency bottleneck of SegEvery with SAM,\nhowever, lies in its mask decoder because it needs to first generate numerous masks with redundant grid-search prompts\nand then perform filtering to obtain the final valid masks. We propose to improve its efficiency by directly generating the\nfinal masks with only valid prompts, which can be obtained through object discovery. Our proposed approach not only helps\nreduce the total time on the mask decoder by at least 16 times but also achieves superior performance. Specifically, our\napproach yields an average performance boost of 3.6% (42.5% v.s. 38.9%) for zero-shot object proposal on the LVIS dataset\nwith the mask AR@K metric. Qualitative results show that our approach generates fine-grained masks while avoiding over-\nsegmenting things. This project targeting faster SegEvery than the original SAM is termed MobileSAMv2 to differentiate from\nMobileSAM which targets faster SegAny. Moreover, we demonstrate that our new prompt sampling is also compatible with\nthe distilled image encoders in MobileSAM, contributing to a unified framework for efficient SegAny and SegEvery. The code\nis available at the same link as MobileSAM Project https://github.com/ChaoningZhang/MobileSAM.\n1. Introduction\nThe NLP field has been revolutionalized by ChatGPT [36], which constitutes a milestone in the development of generative\nAI (AIGC, a.k.a artificial intelligence generated content) [37]. GPT-series models [3, 23, 24] trained on web-scale text\ndatasets play a major role for its development. Following the success of foundation models [2] in NLP, vision foundation\nmodels like CLIP [25] have been developed by co-learning a text encoder via contrastive learning [8, 33]. More recently,\na vision foundation model termed SAM [14], short for segment anything model, was released to solve two practical image\nsegmentation tasks: segment anything (SegAny) and segment everything (SegEvery). Both two tasks perform class-agnostic\nmask segmentation, with the difference in what to segment. SegAny utilizes a certain prompt (like a point or box) to segment\na single thing of interest in the image. By contrast, SegEvery aims to segment all things in the image. SAM has been widely\nused in a wide range of applications [38] due to its impressive performance on these two tasks.\nSAM works in sequence with two modules: ViT-based image encoder and prompt-guided mask decoder (see Figure 1).\nThey are simply referred to image encoder and mask decoder in the remainder of this work when it does not confuse. The\nlightweight mask decoder adopts two-way attention to enable efficient interaction between image embedding and promt token\nfor generating fine-grained masks [14]. What makes SegAny slow is the image encoder which is 100+ more heavyweight than\nthe mask decoder. This issue has been addressed by MobileSAM by distilling a lightweight image encoder in a decoupled\nmanner. To segment all things, SegEvery requires first repeatedly running the mask decoder to generate numerous proposal\n*You are welcome to contact the authors through chaoningzhang1990@gmail.com\narXiv:2312.09579v1  [cs.CV]  15 Dec 2023\nMobileSAM -1\nImage encoder\n(632M)\nViT-based \nimage encoder \nprompt-guided \nmask decoder\nPrompt encoder\n(0.006M) \ninput\nHeavyweight\nLightweight\nMask decoder\n(3.87M) \nmask\nTask\nImage Encoder\nMask Decoder\nSegAny (1 point)\n\u223c 450ms\n\u223c 4ms\nSegEvery(16 \u00d7 16 points)\n\u223c 450ms\n\u223c 400ms\nSegEvery(32 \u00d7 32 points)\n\u223c 450ms\n\u223c 1600ms\nSegEvery(64 \u00d7 64 points)\n\u223c 450ms\n\u223c 6400ms\nFigure 1. SAM architecture and efficiency. The computation bottleneck for SegAny lies in its image encoder, while that for SegEvery\nmainly lies in its mask decoder when a high grid-search density is required (zero-shot object proposal in [14] adopts 64 \u00d7 64 points).\nmasks and then selecting the high-quality and non-overlapping ones. This shifts the computation bottleneck from the image\nencoding to the mask generation and filtering. In essence, SegEvery is not a promptable segmentation task and thus the\nmasks might be generated directly without using prompts [34]. Such a prompt-free approach has been attempted in [41],\nwhich generates masks with less satisfactory boundaries (see analysis in Sec. 6.1). The mask decoder with two-way attention\nsolves this problem but at the cost of making SegEvery much slower [14]. To this end, we follow the practice of SegEvery\nin [14] to prompt the mask decoder to guarantee the quality of the generated masks but address its low-speed issue by reducing\nthe number of prompts.\nSegEvery in [14] prompts the image encoder with a grid search of foreground points. When the grid search is sparse,\nmany small things or meaningful object parts might miss from being detected. Therefore, SegEvery in [14] adopts a high\ngrid density, like 64 \u00d7 64 points for zero-shot object proposal, which tends to have redundant prompts for large objects. In\nessence, it adopts a strategy to first generate many masks, most of which are redundant, and then filter the redundant ones.\nIntuitively, this process can be simplified by only generating valid masks, which saves time for mask generation and removes\nthe need for mask filtering. Motivated by this intuition, we propose an efficient prompt sampling that seeks object-aware\nprompts. Fortunately, this is a well-solved issue in modern object detection. In this work, we adopt YOLOv8 which is\na SOTA architecture for efficient detection with bounding boxes. To avoid over-fitting to any specific dataset, the model\nshould be trained on an open-world dataset, for which a subset of SA-1B dataset is chosen. With the generated box, we can\neither use its center as an object-aware point prompt or directly adopt the box itself as the prompt. An issue with the point\nprompt is that it requires predicting three output masks per prompt to address the ambiguity issue. The bounding box is\nmore informative with less ambiguity and thus is more suitable to be adopted in efficient SegEvery. Overall, this project is\ndesigned to make SegEvery in [14] faster while achieving competitive performance. We term this project MobileSAMv2 to\ndifferentiate MobileSAM [34] that makes SegAny faster. Overall, the contributions of this work are summarized as follows.\n\u2022 We identify what makes SegEvery in SAM slow and propose object-aware box prompts to replace the default grid-search\npoint prompts, which significantly increases its speed while achieving overall superior performance.\n\u2022 We demonstrate that the our proposed object-ware prompt sampling strategy is compatible with the distilled image encoders\nin MobileSAM, which further contributes to a unified framework for efficient SegAny and SegEvery.\n2. Related Work\nProgress on SAM.\nSince its advent in April 2023, SAM has been extensively studied in numerous GitHub projects and re-\nsearch articles. Its performance of SegAny, has been studied in various challenging setups, including medical images [18, 40],\ncamouflaged objects [28], and transparent objects [7]. Overall, SAM shows strong generalization performance but can be im-\nproved when the setup gets more challenging. Its generalization in the adversarial setup has been studied in Attack-SAM [35]\nwhich shows that the output masks of SAM can be easily manipulated by maliciously generated perturbations. Follow-up\nworks further study the performance of adversarial perturbation generated on SAM in cross-model transferability [7] and\ncross-sample transferability [42]. A comprehensive robustness evaluation of SAM has been studied in follow-up work [22]\nwhich shows that SAM is robust against style transfer, common corruptions, local occlusion but not adversarial perturbation.\nThe versatility of SAM has been demonstrated in another line of work. Even though SAM is shown to be compatible with\ntext prompts in the original paper [14] as a proof-of-concept, this functionality is not included in its official code. Grounded\nSAM [9] project combines Grounding DINO [17] with SAM for text-guided promptable segmentation. Specifically, Ground-\ning DINO utilizes a box to generate a bounding box which can be used as a prompt for the SAM to predict a mask. Semantic\nsegment anything project [4] introduces CLIP [25] to assign labels to the predicted masks of SAM. SAM has also been shown\nto be versatile for image editing [26], inpainting tasks [32] and object tracking in videos [31, 43]. Beyond 2D, SAM can also\nbe used for 3D object reconstruction [11, 27], i.e. assisting 3D model generation from a single image. PersoanlizeSAM [39]\npersonalizes the SAM with one shot for the customized SAM. High-quality tokens have been introduced in [12] to improve\nthe quality of predicted masks. The readers are suggested to refer to [38] for a survey of SAM for its recent progress.\nClass-agnostic segmentation.\nDetection is a fundamental computer vision task that localize the objects of interest on an\nimage [16]. Detection roughly localizes the object by a box, while segmentation performs a more fine-grained localization by\nassigning a pixel-wise mask [20]. It is straightforward to deduce a box from a given mask, but not vice versa, which indicates\nthat the segmentation task is more complex than detection. Except for assigning masks, image segmentation (like semantic\nsegmentation) often involves predicting their corresponding semantic labels from a predefined class set [5]. However, it is\nfar from practical applications because there can be unlimited classes in the real world. To this end, a line of work has\nattempted to extend them to the open world by not considering their semantic labels. Class-agnostic object detection has\nbeen first formally proposed in [10] with the average recall established as the metric to evaluate its performance and then\nbe used as a new pretraining technique [1]. Multimodal transformer has been shown in [19] to demonstrate satisfactory\nperformance. Open-world instance segmentation has been extensively in [13, 29, 30] for realizing class-agnostic detection\nand segmentation. In contrast to them treating the object as a whole, a follow-up work [21] has investigated open-world\nobject part segmentation. More recently, SAM [14] has solved the SegEvery task that segments all things including all objects\nand their meaningful parts. It has been shown in multiple Github projects (CLIP-SAM, Segment-Anything-CLIP, segment-\nanything-with-clip) that class-agnostic segmentation masks obtained from SegEvery with SAM [14] can be combined with\nCLIP [25] to produce semantic-aware segmentation in the open world.\n3. Segment Everything\nTask Definition.\nConventional image segmentation predicts pixel-wise masks together with their corresponding class la-\nbels. However, the classes can be ambiguous across different datasets. For example, CIFAR10 dataset has a dog class, while\nImageNet-1K has several hundred classes to indicate various breeds of dogs. Another setup might divide them into puppy or\nadult dogs instead of their breed. This makes open-world image segmentation not tractable when considering the semantics.\nWhen decoupled from label prediction, open-world image segmentation becomes relatively easier but remains a challenging\nissue. Without semantic information, whether a region in the image is considered an object or a thing denoted by a mask\ncan be subjective. This ill-posed nature is, at least partly, connected to the ambiguity of granularity [15]. For example, when\nthe granularity is too large, it might only detect a large object but ignore its meaningful object parts. When the granularity\nis too small, every pixel can be independently segmented, which is trivial and meaningless. In other words, open-world\nimage segmentation requires segmenting all things including the whole objects and their meaningful parts, i.e. everything. In\nessence, it is a class-agnostic segmentation task that performs zero-shot object proposal generation in the open world. This\ntask is termed segment everything (SegEvery) in [14], and we follow [14] to adopt the same name to avoid confusion.\nPrompt-aware Solution.\nSAM is a pioneering work to solve the task of promptable segmentation [14]. Specifically,\nit segments any object of interest with a certain prompt, which is named segment anything (SegAny) in [14]. Based on\nthis, SAM provides a straightforward solution to the SegEvery task by prompting the SAM decoder with a search grid of\nforeground points. An underlying issue of this approach is that the performance is highly dependent on the grid density.\nIntuitively, a higher grid density tends to yield higher performance but at a cost of significantly increasing the computation\noverhead. Orthogonal to MobileSAM [34] distilling the heavyweight image encoder for faster SegAny, this project, named\nMobileSAMv2 for term differentiation, aims to make SegEvery faster by proposing a new sampling strategy to reduce the\nnumber of sampled prompts. Our solution significantly improves its efficiency while achieving overall superior performance.\nIn the following section, we will illustrate the motivation behind our solution and its detailed implementation.\n4. Method\n4.1. Motivation and Framework\nThe prompt-aware solution proposed in [14] has demonstrated impressive performance for the challenging SegEvery task. It\nadopts a strategy of first generating redundant masks and then filtering them to obtain the final valid masks. Intuitively, this\nprocess might be unnecessarily cumbersome and can be simplified by prompting the mask decoder with only valid prompts,\nwhich saves time for mask generation and has no need to perform any filtering. The core of our method lies in replacing the\ndefault gird-search prompt sampling with object-aware prompt sampling. This strategy boils down to determining whether\nthere is an object in a certain region on the image. Modern object detection task already solves this by localizing the objects\nwith bounding boxes. Most of the generated bounding boxes overlap with each other, which thus requires pre-filtering\nbefore being used as valid prompts. Without additional prior knowledge, we deduce the filter-left bounding box center as\nthe foreground point with a moderate assumption that the box center point is on the object. Moreover, the mask decoder of\nSAM also accepts a box as the prompt. Therefore, we also experiment with directly using the remaining box as the prompt.\nOverall, our proposed SegEvery framework consists of two stages: object-aware prompt sampling and prompt-guided mask\ndecoding. The first stage samples the prompts by relying on a modern object detection network, and the second stage follows\nSAM [14] to perform a prompt-guided mask decoding.\n4.2. Object-Aware Prompt Sampling\nObject discovery has been widely used in some cases (like visual-language tasks) as a preprocessing technique for avoiding\nexhaustive sliding window search. Inspired by their practice, we propose to exploit object discovery for sampling prompts.\nIn essence, object discovery is to localize the objects with a bounding box, which can be realized by modern object detection\nmodels but excluding its classification head. The past decade has witnessed a huge advancement in the development of object\ndetection models, YOLO family models have become de facto standard choice for its advantages in real-time performance.\nTo prevent over-fitting to any specific domain, the chosen YOLOv8 model needs to be trained on an open-world dataset, for\nwhich a small subset of SA-1B dataset [14, 34] is chosen. The model is trained with the supervision of both the bounding\nbox and masks and then finetuned with only the bounding box loss. Such a training approach also facilitates comparison with\nthe prompt-free approach (see Sec. 6.1). This generates numerous overlapping boxes, which need to be filtered before being\nused as prompts. Following the standard practice, we adopt NMS to filter the overlapping boxes. With the filtered bounding\nboxes, we can either use its center as an object-aware point prompt or directly adopt the box itself as the prompt. In practice,\nwe choose the latter strategy for multiple reasons. Even though the center point is object-aware, it is based on an assumption\nthat the object inside the bounding box covers the center point. This holds in most cases but not in all cases. Another issue\nwith the point prompt is that it needs to predict three output masks to address the ambiguity issue, which requires additional\nmask filtering. By contrast, the box prompt is more informative and generates high-quality masks with less ambiguity, which\nmitigates the need to predict three masks and is thus more beneficial for efficient SegEvery.\n4.3. Prompt-guided Mask Decoding\nWe follow SAM [14] to perform a prompt-guided mask decoding in a batch manner. In contrast to the image encoder setting\nthe number of image samples as batch, here, the batch concept is the number of prompts. It is worth noting that the prompt-\nguided mask decoder in SAM also accepts a box as the input. Therefore, it is technically feasible to directly prompt the mask\ndecoder with a set of boxes that save the process of deriving the center points. Even though it is not our original motivation,\nwithout causing any additional cost, we find that this practice yields a non-trivial performance boost. In other words, it can\nbe seen as a free trick to improve the task performance. Prompt-aware solution in [14] requires mask filtering. Empirically,\nwe find that this process can be very slow because the mask is high-dimensional. This is different from efficient box filtering\nbecause a box only has four dimensions. This cumbersome mask filtering is optional in our proposed SegEvery framework\nbecause we can avoid it by prompting the mask decoder with only valid prompts. In other words, we keep all the generated\nmasks since the prompts are sampled in an object-aware manner.\n5. Experiments\nSegEvery has been perceived in [14] as a zero-shot object proposal task with standard average recall (AR) as the metric for\nperformance evaluation. We follow the practice in [14] to adopt AR for masks at K proposals (mask AR@K), where K is\nthe maximum allowable number of masks. With the definition of AR, AR@K gets higher when K is allowed to set to a\nlarger value, which constitutes a less strict metric. Only AR@1000 is reported in [14], but we choose to report AR@K for\nK ranging from 10 to 1000. To not lose generality yet save computation resources, we choose to report the results on 100\nimages randomly sampled from the large vocabulary instance segmentaiton (LVIS) dataset [6].\n5.1. Main Results\nWhat makes SegEvery much more computation-intensive than SegAny lies in the need to run the mask decoder with nu-\nmerous sampled prompts [14]. Our proposed object-aware prompt sampling improves its efficiency by reducing the number\nof total prompts. In the following, we detail their difference in terms of required computation time by roughly dividing\nthe prompt-guided mask decoding pipeline into two stages: prompt encoding (including pre-sampling) and mask decoding\n(including post-filtering). Mask decoding is much more heavy than simple prompt encoding. Except for the redundant sam-\npled prompts, SegEvery in [14] generates more masks than needed (or AR@K allowed) by setting the multi-mask option to\ntrue. Specifically, one point can prompt the mask decoder to generate three output masks with different granularities (small,\nmiddle, and large). Setting the multi-mask option to true has been demonstrated in [14] for achieving superior performance\nof SegEvery (like zero-shot object proposal) but at the cost of requiring filtering more redundant masks. Empirically, we\nfind that (GPU-processed) mask filtering in [14] can be even more computation insensitive than the mask generation itself\npartly because accessing and removing high-dimensional masks on the memory can be very slow. Interested readers are\nsuggested to check their official code for details. Intuitively, the time spent on the mask decoder for the grid-search prompt\nsampling in [14] depends on the grid density (See Figure 1). Different tasks might require different grid densities. In the\nofficial demo [14], it adopts a grid density of 32 \u00d7 32 which achieves a good trade-off between efficiency and performance.\nFor evaluating the performance on zero-shot object proposal, a grid density of 64 \u00d7 64 is adopted in [14].\nEfficiency comparison. SegEvery with our proposed sampling strategy needs to run an object discovery algorithm to\nobtain object-aware prompts, which requires more time for prompt sampling than the default grid-search sampling in [14] but\nneeds to encode much fewer prompts. For the mask generation, the time spent on the mask decoder is somewhat proportional\nto the number of sampled prompts. We find that the performance saturates when the number of prompts is approaching 320,\nwhich is set to the maximum number of detection boxes (See Sec.6.2). Less computation is needed when the object discovery\ngenerates masks that are fewer than 320, which occurs in many cases. Nonetheless, when performing an efficiency analysis,\nwe compare our most computation-intensive scenario (max 320 prompts) with the grid-search strategy. The results in Table 2\nshow that our proposed prompt sampling strategy significantly improves the efficiency of the (prompt-guided) mask decoder\nby at least 16 times. The computation spent on the prompt encoding accounts for a non-trivial percentage with our approach\nbecause it runs object discovery for prompt sampling. A more efficient object discovery algorithm is left for future work.\nTable 2. Efficiency comparison of the (prompt-guided) mask decoder between grid-search sampling and object-aware sampling. Note that\nthe prompt encoding includes the prompt pre-sampling time, while the mask decoding includes the mask post-filtering time.\nSampling strategy\nPrompt Encoding\nMask Decoding\nTotal\nGrid-search sampling (32 \u00d7 32 prompts)\n16ms\n1600ms\n1616ms\nGrid-search sampling (64 \u00d7 64 prompts)\n64ms\n6400ms\n6464ms\nObject-aware sampling (max 320 prompts)\n47ms\n50ms\n97ms\nPerformance comparison. We carefully follow the implementation practice recommended in [14] for zero-shot object\nproposal. By default, it is suggested to set the grid density to 64 \u00d7 64 and generate a total of 12288 (64 \u00d7 64 \u00d7 3) masks,\nout of which a maximum of 1000 masks are then selected given the mask AR@1000 metric. We have experimented with\ndecreasing the grid density and/or setting the multi-mask option to false (single-mask mode). The results in Table 3 show\nthat generating fewer masks by either one of the above two practices leads to a performance drop, suggesting that the default\ngrid-search sampling strategy highly relies on generating redundant masks for selecting the final needed ones. Moreover,\nwe have multiple major observations by comparing SAM (the default grid-search prompt sampling) and MobileSAMv2 (our\nproposed object-aware prompt sampling). First, under the condition of prompting with the same type of prompt (points) and\nsetting multi-mask to false, we find that MobileSAMv2 (max 320 points) achieves comparable performance as SAM using\n4096 points, suggesting that the object-aware property of our prompt sampling strategy significantly avoids redundancy.\nBoosted with the multitask option set to true, the default 64 \u00d7 64 grid density yields a higher performance (59.2%), which\nconstitutes the best setup for the grid-search strategy. Similarly, we can also increase the performance of our object-aware\npoint sampling by setting the multi-mask to true. Note that the motivation for predicting three output masks of different\ngranularities [14] is to address the ambiguity issue of a point prompt. A single point has limited prompt information and\nthus causing ambiguity (the readers can check Figure 4 in [14] for more details). By contrast, a box prompt is much more\ninformative and reduces ambiguity to a very large extent. This is supported by our results in Table 3 that box prompts yield\na significant performance boost at single mask mode. Last, it is worth mentioning that, compared with the best result of\nthe grid-search sampling strategy (with 64 \u00d7 64 points at multi-mask mode), our proposed sampling strategy (with max\n320 box prompts) achieves comparable performance (59.3% v.s. 59.2%). Limiting the max number of prompts to 256, our\nstrategy still yields competitive performance (58.5%) compared with that of the grid-search strategy (34.6%) under the same\ncondition. We also report AR@K for other K values in Table 4. When K is set to a relatively small value, we find that our\nproposed object-aware sampling strategy with much fewer prompts leads to a performance boost by a large margin. Overall,\nour proposed approach achieves an average performance boost of 3.6% (42.5% v.s. 38.9%).\nTable 3. Zero-shot object proposal comparison between grid-search sampling and object-aware sampling (mask@1000 as the metric).\nMethod\nmulti-mask\nall\nsmall\nmed.\nlarge\nSAM(64 \u00d7 64 = 4096 points)\ntrue (\u00d73)\n59.2\n46.6\n78.7\n82.4\nSAM(32 \u00d7 32 = 1024 points)\ntrue (\u00d73)\n57.2\n42.9\n79.2\n83.6\nSAM(16 \u00d7 16 = 256 points)\ntrue (\u00d73)\n40.0\n19.4\n71.3\n79.1\nSAM(64 \u00d7 64 = 4096 points)\nfalse (\u00d71)\n54.3\n44.4\n71.5\n67.4\nSAM(32 \u00d7 32 = 1024 points)\nfalse (\u00d71)\n49.8\n37.2\n71.4\n66.8\nSAM(16 \u00d7 16 = 256 points)\nfalse (\u00d71)\n34.6\n17.5\n61.5\n64.9\nMobileSAMv2 (max 320 points)\ntrue (\u00d73)\n55.7\n40.6\n78.6\n84.6\nMobileSAMv2 (max 320 points)\nfalse (\u00d71)\n53.6\n44.0\n70.4\n66.6\nMobileSAMv2 (max 320 boxes)\nfalse (\u00d71)\n59.3\n47.9\n77.1\n79.9\nMobileSAMv2 (max 256 boxes)\nfalse (\u00d71)\n58.5\n46.7\n77.1\n79.1\nTable 4. Zero-shot object proposal comparison between grid-search sampling and object-aware sampling.\nMethod\nmulti-mask\nall\nsmall\nmed.\nlarge\nmask AR@1000\nSAM(64 \u00d7 64 = 4096 points)\ntrue (\u00d73)\n59.2\n46.6\n78.7\n82.4\nmask AR@1000\nMobileSAMv2 (max 320 boxes)\nfalse (\u00d71)\n59.3\n47.9\n77.1\n79.9\nmask AR@100\nSAM(64 \u00d7 64 = 4096 points)\ntrue (\u00d73)\n44.8\n29.8\n67.6\n73.8\nmask AR@100\nMobileSAMv2 (max 100 boxes)\nfalse (\u00d71)\n50.6\n36.3\n73.1\n76.3\nmask AR@10\nSAM(64 \u00d7 64 = 4096 points)\ntrue (\u00d73)\n12.6\n2.9\n22.7\n45.0\nmask AR@10\nMobileSAMv2 (max 10 boxes)\nfalse (\u00d71)\n17.6\n6.4\n35.0\n37.8\naverage\nSAM(64 \u00d7 64 = 4096 points)\ntrue (\u00d73)\n38.9\n26.43\n56.3\n67.1\naverage\nMobileSAMv2\nfalse (\u00d71)\n42.5\n30.2\n61.7\n64.7\nTable 5. Influence of the image encoders on MobileSAMv2 for zero-shot object proposal (mask@1000).\nEncoder\nAll\nsmall\nmed.\nlarge\nViT-H\n59.3\n47.9\n77.1\n79.9\nTinyViT\n51.1\n38.9\n69.9\n73.4\nEfficientViT-L2\n56.3\n44.7\n74.1\n78.1\n5.2. On the Compatibility with Distilled Image Encoders\nIn the above, we only consider the prompt-guided mask decoder, however, the whole pipeline needs to run the image encoder\nonce before running the mask decoder. As shown in Figure 1, the time spent on the image encoder is relatively small for\nSegEvery with the grid-search point sampling. However, this is no longer the case when adopting our object-aware prompt\nsampling strategy, which reduces the time on the mask decoder to around 100ms. Therefore, we consider reducing the\ntime spent on the image encoder by replacing the original one (ViT-H) in the SAM with a distilled one in the MobileSAM\nproject [34]. The results with different distilled image encoders are shown in Table 5. We observe a moderate performance\ndrop (from 59.2% to 56.3%) when EfficientViT-L2 is used. Given that EfficientViT-l2 runs around 20ms which is signif-\nicantly faster than that of ViT-H (more than 400ms), it is worthwhile to replace the image encoder. Due to the simplicity\nand effectiveness of decoupled knowledge distillation introduced in MobileSAM [34], a more powerful distilled image en-\ncoder is expected to emerge soon to further alleviate the performance drop. It is worth highlighting that MobileSAM and\nMobileSAMv2 solve two orthogonal issues: faster SegAny and faster SegEvery. Combing them together constitutes a unified\nframework for efficient SegAny and SegEvery.\n6. Additional Comparison and Ablation Study\n6.1. Comparison with Prompt-free Approach\nAs discussed in [34], the SegEvery is in essence not a promptable segmentation task and thus can be realized in prompt-\nfree manner. Such an approach has been attempted in [41] with YOLOv8-seg, which mainly augments YOLOv8-det with a\nTable 6. Zero-shot object proposal comparison between prompt-free and prompt-aware approaches (mask@1000).\nStrategy\nMethod\nAll\nsmall\nmed.\nlarge\nPrompt-free\nFastSAM\n49.6\n36.2\n69.4\n77.1\nPrompt-aware\nSAM(best setup)\n59.2\n46.4\n78.7\n82.4\nPrompt-aware\nMobileSAMv2 (ViT-H)\n59.3\n47.9\n77.1\n79.9\nPrompt-aware\nMobileSAMv2 (EfficientViT-L2 )\n56.3\n44.7\n74.1\n78.1\n(a) Original Image\n(b) Prompt-free (FastSAM)\n(c) Prompt-aware (SAM))\n(d) Prompt-aware (MobileSAMv2)\nFigure 2. Comparison between prompt-free and prompt-aware mask predictions. Prompt-free tends to predict the mask with a non-smooth\nboundary compared with prompt-aware approaches. For the two prompt-aware approaches, SAM tends to over-segment things while our\nMobileSAMv2 addresses it due to its object-aware property. Best view in color and zoom in.\nprotonet module to generate mask prototype. The intance mask is obtained by convolving the mask prototype with a mask\ncoefficient that has the same length as the prototype dimension (32 by default), which is mathematically a dot product. Here,\nwe point out that the mask decoder of SAM [14] also generates the mask by making a dot product between a mask coefficient\n(called mask token in [14]) and a mask prototype (called image embedding in [14]), which have the same (32) dimensions so\nthat the dot product can be computed. Intuitively, the quality of generated mask relies on how well the mask coefficent and\nmask prototype interact with each other. The mask decoder in [14] adopts two-way attention to enable the interaction between\nthe mask prototype and mask coeffcient before performing the final product. Such an interaction is the key foundation for\nguaranteeing the high-quality mask in SAM. By contrast, there is no explicit interaction between the mask coefficients and\nmask prototypes in the prompt-free approach. With a single shared mask prototype, it often predicts multiple objects at\ndifferent regions of the image and thus relies on a bounding box to crop the mask. This can help remove the irrelevant masks\noutside the box but still fails in yielding high-quality masks as [14], at least partly, due to lack of the interaction between\nmask coefficient and mask prototype. Even though the prompt-free approach realizes the fastest speed, it results in a non-\ntrivial performance drop (see Table 6). The less satisfactory performance of the prompt-free approach is mainly attributed\nto the poor mask boundary (see Figure 2). Compared with prompt-free approach, the two prompt-aware approaches (SAM\nand MobileSAMv2) generate masks with much more fine-grained boundaries. SAM tends to over-segment things while our\nMobileSAMv2 alleviates this tendency by utilizing its object-aware property.\n6.2. Ablation Study\nWith the mask AR@1000 as the metric, we find that our proposed sampling strategy often yields fewer prompts than 1000,\nwhich motivates us to explore the influence of the maximum number of (box) prompts in our proposed prompt sampling\nTable 7. Influence of the maximum number of prompts on MobileSAMv2 for zero-shot object proposal (mask@1000).\nmax # of prompts\nall\nsmall\nmed.\nlarge\n384\n59.3\n47.9\n77.1\n79.9\n320\n59.3\n47.9\n77.1\n79.9\n256\n58.5\n46.7\n77.1\n79.1\n192\n56.6\n44.2\n76.0\n78.8\n128\n53.6\n40.2\n74.6\n77.7\n64\n44.8\n29.2\n68.3\n75.4\nstrategy. The results in Table 7 show that increasing the number of box prompts is beneficial for a higher mask AR, however,\nit saturates after it approaches 320. Therefore, by default, we set the maximum number of prompts in MobileSAMv2 to 320.\n7. Conclusion and Future work\nOrthogonal to the MobileSAM project making SegAny faster by distilling a lightweight image encoder, this project termed\nMobileSAMv2 makes SegEvery faster by proposing a new prompt sampling strategy in the prompt-guided mask decoder.\nReplacing the grid-search with our object-aware prompt sampling, we significantly improve the efficiency of SegEvery while\nachieving overall superior performance. We also demonstrate that our object-aware prompt sampling is compatible with the\ndistilled image encoders in the MobileSAM project. Overall, our work constitutes a step towards a unified framework for\nefficient SegAny and SegEvery. Future work is needed to seek superior image encoder(s) and object discovery models(s).\nReferences\n[1] Amir Bar, Xin Wang, Vadim Kantorov, Colorado J Reed, Roei Herzig, Gal Chechik, Anna Rohrbach, Trevor Darrell, and Amir\nGloberson. Detreg: Unsupervised pretraining with region priors for object detection. In CVPR, 2022. 3\n[2] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg,\nAntoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,\n2021. 1\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems,\n2020. 1\n[4] Jiaqi Chen, Zeyu Yang, and Li Zhang. Semantic-segment-anything, 2023. GitHub repository. 2\n[5] Yanming Guo, Yu Liu, Theodoros Georgiou, and Michael S Lew. A review of semantic segmentation using deep neural networks.\nInternational journal of multimedia information retrieval, 7:87\u201393, 2018. 3\n[6] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance segmentation. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 5356\u20135364, 2019. 4\n[7] Dongsheng Han, Chaoning Zhang, Yu Qiao, Maryam Qamar, Yuna Jung, SeungKyu Lee, Sung-Ho Bae, and Choong Seon Hong.\nSegment anything model (sam) meets glass: Mirror and transparent objects cannot be easily detected. arXiv preprint, 2023. 2\n[8] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation\nlearning. In CVPR, 2020. 1\n[9] IDEA-Research. Grounded segment anything, 2023. GitHub repository. 2\n[10] Ayush Jaiswal, Yue Wu, Pradeep Natarajan, and Premkumar Natarajan. Class-agnostic object detection. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer Vision, pages 919\u2013928, 2021. 3\n[11] Minki Kang, Dongchan Min, and Sung Ju Hwang. Any-speaker adaptive text-to-speech synthesis with diffusion models. arXiv\npreprint arXiv:2211.09383, 2022. 2\n[12] Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, and Fisher Yu. Segment anything in high quality.\narXiv preprint arXiv:2306.01567, 2023. 3\n[13] Dahun Kim, Tsung-Yi Lin, Anelia Angelova, In So Kweon, and Weicheng Kuo. Learning open-world object proposals without\nlearning to classify. IEEE Robotics and Automation Letters, 7(2):5453\u20135460, 2022. 3\n[14] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexan-\nder C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint arXiv:2304.02643, 2023. 1, 2, 3, 4, 5, 7\n[15] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei Zhang, and Jianfeng Gao. Semantic-sam:\nSegment and recognize anything at any granularity. arXiv preprint arXiv:2307.04767, 2023. 3\n[16] Li Liu, Wanli Ouyang, Xiaogang Wang, Paul Fieguth, Jie Chen, Xinwang Liu, and Matti Pietik\u00a8ainen. Deep learning for generic\nobject detection: A survey. International journal of computer vision, 128:261\u2013318, 2020. 3\n[17] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.\nGrounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. 2\n[18] Jun Ma and Bo Wang. Segment anything in medical images. arXiv preprint arXiv:2304.12306, 2023. 2\n[19] Muhammad Maaz, Hanoona Rasheed, Salman Khan, Fahad Shahbaz Khan, Rao Muhammad Anwer, and Ming-Hsuan Yang. Class-\nagnostic object detection with multi-modal transformer. In European Conference on Computer Vision, pages 512\u2013531. Springer,\n2022. 3\n[20] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation\nusing deep learning: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(7):3523\u20133542, 2021. 3\n[21] Tai-Yu Pan, Qing Liu, Wei-Lun Chao, and Brian Price. Towards open-world segmentation of parts. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 15392\u201315401, 2023. 3\n[22] Yu Qiao, Chaoning Zhang, Taegoo Kang, Donghun Kim, Shehbaz Tariq, Chenshuang Zhang, and Choong Seon Hong. Robustness\nof sam: Segment anything under corruptions and beyond. arXiv preprint arXiv:2306.07713, 2023. 2\n[23] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training.\n2018. 1\n[24] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 2019. 1\n[25] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1, 2, 3\n[26] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695,\n2022. 2\n[27] Qiuhong Shen, Xingyi Yang, and Xinchao Wang. Anything-3d: Towards single-view anything reconstruction in the wild. arXiv\npreprint arXiv:2304.10261, 2023. 2\n[28] Lv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged object detection. arXiv preprint\narXiv:2304.04709, 2023. 2\n[29] Weiyao Wang, Matt Feiszli, Heng Wang, Jitendra Malik, and Du Tran. Open-world instance segmentation: Exploiting pseudo ground\ntruth from learned pairwise affinity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n4422\u20134432, 2022. 3\n[30] Jiannan Wu, Yi Jiang, Bin Yan, Huchuan Lu, Zehuan Yuan, and Ping Luo. Exploring transformers for open-world instance segmen-\ntation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6611\u20136621, 2023. 3\n[31] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything meets videos.\narXiv preprint arXiv:2304.11968, 2023. 2\n[32] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything\nmeets image inpainting. arXiv preprint arXiv:2304.06790, 2023. 2\n[33] Chaoning Zhang, Kang Zhang, Trung X. Pham, Changdong Yoo, and In-So Kweon. Dual temperature helps contrastive learning\nwithout many negative samples: Towards understanding and simplifying moco. In CVPR, 2022. 1\n[34] Chaoning Zhang, Dongshen Han, Yu Qiao, Jung Uk Kim, Sung Ho Bae, Seungkyu Lee, and Choong Seon Hong. Faster segment\nanything: Towards lightweight sam for mobile applications. arXiv preprint arXiv:2306.14289, 2023. 2, 3, 4, 6\n[35] Chenshuang Zhang, Chaoning Zhang, Taegoo Kang, Donghun Kim, Sung-Ho Bae, and In So Kweon. Attack-sam: Towards evaluat-\ning adversarial robustness of segment anything model. arXiv preprint, 2023. 2\n[36] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk Kim,\nSeong Tae Kim, Jinwoo Choi, et al. One small step for generative ai, one giant leap for agi: A complete survey on chatgpt in aigc\nera. arXiv preprint arXiv:2304.06488, 2023. 1\n[37] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet\nThwal, Ye Lin Tun, Le Luang Huy, et al. A complete survey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need?\narXiv preprint arXiv:2303.11717, 2023. 1\n[38] Chaoning Zhang, Sheng Zheng, Chenghao Li, Yu Qiao, Taegoo Kang, Xinru Shan, Chenshuang Zhang, Caiyan Qin, Francois\nRameau, Sung-Ho Bae, et al. A survey on segment anything model (sam): Vision foundation model meets prompt engineering.\n2023. 1, 3\n[39] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao, and Hongsheng Li. Personalize segment\nanything model with one shot. arXiv preprint arXiv:2305.03048, 2023. 2\n[40] Yizhe Zhang, Tao Zhou, Peixian Liang, and Danny Z Chen. Input augmentation with sam: Boosting medical image segmentation\nwith segmentation foundation model. arXiv preprint arXiv:2304.11332, 2023. 2\n[41] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao Yu, Min Li, Ming Tang, and Jinqiao Wang. Fast segment anything. arXiv\npreprint arXiv:2306.12156, 2023. 2, 6\n[42] Sheng Zheng and Chaoning Zhang.\nBlack-box targeted adversarial attack on segment anything (sam).\narXiv preprint\narXiv:2310.10010, 2023. 2\n[43] Zxyang. Segment and track anything, 2023. GitHub repository. 2\n"
  },
  {
    "title": "Point Transformer V3: Simpler, Faster, Stronger",
    "link": "https://arxiv.org/pdf/2312.10035.pdf",
    "upvote": "17",
    "text": "Point Transformer V3: Simpler, Faster, Stronger\nXiaoyang Wu1\nLi Jiang3\nPeng-Shuai Wang4\nZhijian Liu5\nXihui Liu1\nYu Qiao2\nWanli Ouyang2\nTong He2*\nHengshuang Zhao1*\n1HKU\n2SH AI Lab\n3MPI\n4PKU\n5MIT\nhttps://github.com/Pointcept/PointTransformerV3\nWider Receptive Field\nStronger Performance\nScanNet\nSem. Seg.\nScanNet200\nSem. Seg.\nS3DIS 6-Fold\nSem. Seg.\nS3DIS\nSem. Seg.\nScanNet\nIns. Seg.\nScanNet200\nIns. Seg.\nWaymo\nVehicle Det.\nWaymo\nPedestrian Det.\nWaymo\nCyclist Det.\nWaymo\nSem. Seg.\nnuScenes\nSem. Seg.\nSemanticKITTI\nSem. Seg.\nScanNet Eff.\nL.R. 1%\nScanNet Eff.\nL.R. 20%\nScanNet Eff.\nL.A. 20\nScanNet Eff.\nL.A. 200\nPTv3\nPTv2\nFlatFormer\nOctFormer\nSphereFormer\nMinkUNet\n78.6\n75.5\n62.4\n68.2\n31.1\n71.3\n81.2\n72.1\n71.5\n76.3\n71.2\n34.1\n63.5\n74.3\n80.8\n36.0\nLower Memory Consumption\nFaster Speed\nMinkUNet\nPTv2\nPTv3\n146ms\n48ms\n44ms\n3.3\u00d7 faster\nInference Latency\nMinkUNet\nPTv2\nPTv3\n12.3G\n1.7G\n1.2G\n10.2\u00d7 lower\nMemory Consumption\nFigure 1. Overview of Point Transformer V3 (PTv3). Compared to its predecessor, PTv2 [84], our PTv3 shows superiority in the\nfollowing aspects: 1. Stronger performance. PTv3 achieves state-of-the-art results across a variety of indoor and outdoor 3D perception\ntasks. 2. Wider receptive field. Benefit from the simplicity and efficiency, PTv3 expands the receptive field from 16 to 1024 points.\n3. Faster speed. PTv3 significantly increases processing speed, making it suitable for latency-sensitive applications. 4. Lower Memory\nConsumption. PTv3 reduces memory usage, enhancing accessibility for broader situations.\nAbstract\nThis paper is not motivated to seek innovation within\nthe attention mechanism. Instead, it focuses on overcom-\ning the existing trade-offs between accuracy and efficiency\nwithin the context of point cloud processing, leveraging the\npower of scale. Drawing inspiration from recent advances\nin 3D large-scale representation learning, we recognize that\nmodel performance is more influenced by scale than by in-\ntricate design. Therefore, we present Point Transformer V3\n(PTv3), which prioritizes simplicity and efficiency over the\naccuracy of certain mechanisms that are minor to the over-\nall performance after scaling, such as replacing the precise\nneighbor search by KNN with an efficient serialized neigh-\nbor mapping of point clouds organized with specific pat-\nterns. This principle enables significant scaling, expand-\ning the receptive field from 16 to 1024 points while remain-\ning efficient (a 3\u00d7 increase in processing speed and a 10\u00d7\nimprovement in memory efficiency compared with its pre-\ndecessor, PTv2). PTv3 attains state-of-the-art results on\nover 20 downstream tasks that span both indoor and out-\ndoor scenarios. Further enhanced with multi-dataset joint\ntraining, PTv3 pushes these results to a higher level.\n*Corresponding author.\n1. Introduction\nDeep learning models have experienced rapid advance-\nments in various areas, such as 2D vision [22, 36, 73, 80]\nand natural language processing (NLP) [1, 35, 53, 74], with\ntheir progress often attributed to the effective utilization of\nscale, encompassing factors such as the size of datasets, the\nnumber of model parameters, the range of effective recep-\ntive field, and the computing power allocated for training.\nHowever, in contrast to the progress made in 2D vision or\nNLP, the development of 3D backbones [15, 44, 58, 82] has\nbeen hindered in terms of scale, primarily due to the limited\nsize and diversity of point cloud data available in separate\ndomains [85]. Consequently, there exists a gap in applying\nscaling principles that have driven advancements in other\nfields [35]. This absence of scale often leads to a limited\ntrade-off between accuracy and speed on 3D backbones,\nparticularly for models based on the transformer architec-\nture [25, 100]. Typically, this trade-off involves sacrific-\ning efficiency for accuracy. Such limited efficiency impedes\nsome of these models\u2019 capacity to fully leverage the inher-\nent strength of transformers in scaling the range of receptive\nfields, hindering their full potential in 3D data processing.\nA recent advancement [85] in 3D representation learn-\n1\narXiv:2312.10035v1  [cs.CV]  15 Dec 2023\ning has made progress in overcoming the data scale limita-\ntion in point cloud processing by introducing a synergistic\ntraining approach spanning multiple 3D datasets. Coupled\nwith this strategy, the efficient convolutional backbone [12]\nhas effectively bridged the accuracy gap commonly associ-\nated with point cloud transformers [38, 84]. However, point\ncloud transformers themselves have not yet fully benefited\nfrom this privilege of scale due to their efficiency gap com-\npared to sparse convolution. This discovery shapes the ini-\ntial motivation for our work: to re-weigh the design choices\nin point transformers, with the lens of the scaling princi-\nple. We posit that model performance is more significantly\ninfluenced by scale than by intricate design.\nTherefore, we introduce Point Transformer V3 (PTv3),\nwhich prioritizes simplicity and efficiency over the accuracy\nof certain mechanisms, thereby enabling scalability. Such\nadjustments have an ignorable impact on overall perfor-\nmance after scaling. Specifically, PTv3 makes the following\nadaptations to achieve superior efficiency and scalability:\n\u2022 Inspired by two recent advancements [48, 77] and rec-\nognizing the scalability benefits of structuring unstruc-\ntured point clouds, PTv3 shifts from the traditional spa-\ntial proximity defined by K-Nearest Neighbors (KNN)\nquery, accounting for 28% of the forward time. Instead,\nit explores the potential of serialized neighborhoods in\npoint clouds, organized according to specific patterns.\n\u2022 PTv3 replaces more complex attention patch interaction\nmechanisms, like shift-window (impeding the fusion of\nattention operators) and the neighborhood mechanism\n(causing high memory consumption), with a streamlined\napproach tailored for serialized point clouds.\n\u2022 PTv3 eliminates the reliance on relative positional en-\ncoding, which accounts for 26% of the forward time, in\nfavor of a simpler prepositive sparse convolutional layer.\nWe consider these designs as intuitive choices driven by\nthe scaling principles and advancements in existing point\ncloud transformers. Importantly, this paper underscores the\ncritical importance of recognizing how scalability affects\nbackbone design, instead of detailed module designs.\nThis principle significantly enhances scalability, over-\ncoming traditional trade-offs between accuracy and effi-\nciency (see Fig. 1). PTv3, compared to its predecessor, has\nachieved a 3.3\u00d7 increase in inference speed and a 10.2\u00d7\nreduction in memory usage. More importantly, PTv3 cap-\nitalizes on its inherent ability to scale the range of percep-\ntion, expanding its receptive field from 16 to 1024 points\nwhile maintaining efficiency. This scalability underpins its\nsuperior performance in real-world perception tasks, where\nPTv3 achieves state-of-the-art results across over 20 down-\nstream tasks in both indoor and outdoor scenarios. Further\naugmenting its data scale with multi-dataset training [85],\nPTv3 elevates these results even more. We hope that our\ninsights will inspire future research in this direction.\n2. Related Work\n3D Understanding. Conventionally, deep neural architec-\ntures for understanding 3D point cloud data can be broadly\nclassified into three categories based on their approach to\nmodeling point clouds: projection-based, voxel-based, and\npoint-based methods. Projection-based methods project 3D\npoints onto various image planes and utilize 2D CNN-based\nbackbones for feature extraction [7, 41, 43, 66].\nVoxel-\nbased approaches transform point clouds into regular voxel\ngrids to facilitate 3D convolution operations [50, 65], with\ntheir efficiency subsequently enhanced by sparse convolu-\ntion [12, 23, 78]. However, they often lack scalability in\nterms of the kernel sizes. Point-based methods, by con-\ntrast, process point clouds directly [49, 58, 59, 72, 99] and\nhave recently seen a shift towards transformer-based archi-\ntectures [25, 38, 84, 95, 100]. While these methods are\npowerful, their efficiency is frequently constrained by the\nunstructured nature of point clouds, which poses challenges\nto scaling their designs.\nSerialization-based method. Two recent works [48, 77]\nhave introduced approaches diverging from the traditional\nparadigms of point cloud processing, which we catego-\nrized as serialization-based. These methods structure point\nclouds by sorting them according to specific patterns, trans-\nforming unstructured, irregular point clouds into manage-\nable sequences while preserving certain spatial proximity.\nOctFormer [77] inherits order during octreelization, akin to\nz-order, offering scalability but still constrained by the oc-\ntree structure itself. FlatFormer [48], on the other hand, em-\nploys a window-based sorting strategy for grouping point\npillars, akin to window partitioning. However, this design\nlacks scalability in the receptive field and is more suited to\npillow-based 3D object detectors. These pioneering works\nmark the inception of serialization-based methods.\nOur\nPTv3 builds on this foundation, defining and exploring the\nfull potential of point cloud serialization.\n3D representation learning. In contrast to 2D domains,\nwhere large-scale pre-training has become a standard ap-\nproach for enhancing downstream tasks [6], 3D representa-\ntion learning is still in a phase of exploration. Most stud-\nies still rely on training models from scratch using spe-\ncific target datasets [88]. While major efforts in 3D rep-\nresentation learning focused on individual objects [54, 63,\n64, 81, 97], some recent advancements have redirected at-\ntention towards training on real-world scene-centric point\nclouds [28, 34, 86, 88, 101]. This shift signifies a major step\nforward in 3D scene understanding. Notably, Point Prompt\nTraining (PPT) [85] introduces a new paradigm for large-\nscale representation learning through multi-dataset syner-\ngistic learning, emphasizing the importance of scale. This\napproach greatly influences our design philosophy and ini-\ntial motivation for developing PTv3, and we have incorpo-\nrated this strategy in our final results.\n2\nOutdoor Efficiency (nuScenes)\nTraining\nInference\nMethods\nParams.\nLatency\nMemory\nLatency Memory\nMinkUNet / 3 [12]\n37.9M\n163ms\n3.3G\n48ms\n1.7G\nMinkUNet / 5 [12]\n170.3M\n455ms\n5.6G\n145ms\n2.1G\nMinkUNet / 7 [12]\n465.0M 1120ms\n12.4G\n337ms\n2.8G\nPTv2 / 16 [84]\n12.8M\n213ms\n10.3G\n146ms\n12.3G\nPTv2 / 24 [84]\n12.8M\n308ms\n17.6G\n180ms\n15.2G\nPTv2 / 32 [84]\n12.8M\n354ms\n21.5G\n213ms\n19.4G\nPTv3 / 256 (ours)\n46.2M\n120ms\n3.3G\n44ms\n1.2G\nPTv3 / 1024 (ours)\n46.2M\n119ms\n3.3G\n44ms\n1.2G\nPTv3 / 4096 (ours)\n46.2M\n125ms\n3.3G\n45ms\n1.2G\nTable 1. Model efficiency. We benchmark the training and infer-\nence efficiency of backbones with various scales of receptive field.\nThe batch size is fixed to 1, and the number after \u201c/\u201d denotes the\nkernel size of sparse convolution and patch size1of attention.\n3. Design Principle and Pilot Study\nIn this section, we introduce the scaling principle and pilot\nstudy, which guide the design of our model.\nScaling principle.\nConventionally, the relationship be-\ntween accuracy and efficiency in model performance is\ncharacterized as a \u201ctrade-off\u201d, with a typical preference for\naccuracy at the expense of efficiency. In pursuit of this, nu-\nmerous methods have been proposed with cumbersome op-\nerations. Point Transformers [84, 100] prioritize accuracy\nand stability by substituting matrix multiplication in the\ncomputation of attention weights with learnable layers and\nnormalization, potentially compromising efficiency. Simi-\nlarly, Stratified Transformer [38] and Swin3D [95] achieve\nimproved accuracy by incorporating more complex forms\nof relative positional encoding, yet this often results in de-\ncreased computational speed.\nYet, the perceived trade-off between accuracy and ef-\nficiency is not absolute, with a notable counterexample\nemerging through the engagement with scaling strategies.\nSpecifically, Sparse Convolution, known for its speed and\nmemory efficiency, remains preferred in 3D large-scale\npre-training.\nUtilizing multi-dataset joint training strate-\ngies [85], Sparse Convolution [12, 23] has shown significant\nperformance improvements, increasing mIoU on ScanNet\nsemantic segmentation from 72.2% to 77.0% [101]. This\noutperforms PTv2 when trained from scratch by 1.6%, all\nwhile retaining superior efficiency. However, such advance-\nments have not been fully extended to point transformers,\nprimarily due to their efficiency limitations, which present\nburdens in model training especially when the computing\nresource is constrained.\nThis observation leads us to hypothesize that model per-\nformance may be more significantly influenced by scale\nthan by complex design details. We consider the possibility\nof trading the accuracy of certain mechanisms for simplicity\nand efficiency, thereby enabling scalability. By leveraging\nthe strength of scale, such sacrifices could have an ignor-\n1Patch size refers to the number of neighboring points considered to-\ngether for self-attention mechanisms.\nRelative\nPositional Encoding\nRelation & Weight\nEncoding\nIndex\nOperation\nQKV\nEncoding\nValue\nAggregation\nKNN Query\nGrid Pool\nUnpool\nFFN\nFigure 2. Latency treemap of each components of PTv2. We\nbenchmark and visualize the proportion of the forward time of\neach component of PTv2. KNN Query and RPE occupy a total\nof 54% of forward time.\nable impact on overall performance. This concept forms\nthe basis of our scaling principle for backbone design, and\nwe practice it with our design.\nBreaking the curse of permutation invariance. Despite\nthe demonstrated efficiency of sparse convolution, the ques-\ntion arises about the need for a scalable point transformer.\nWhile multi-dataset joint training allows for data scaling\nand the incorporation of more layers and channels con-\ntributes to model scaling, efficiently expanding the recep-\ntive field to enhance generalization capabilities remains a\nchallenge for convolutional backbones (refer to Tab. 1). It\nis attention, an operator that is naturally adaptive to kernel\nshape, potentially to be universal.\nHowever, current point transformers encounter chal-\nlenges in scaling when adhering to the request of permu-\ntation invariance, stemming from the unstructured nature of\npoint cloud data. In PTv1, the application of the K-Nearest\nNeighbors (KNN) algorithm to formulate local structures\nintroduced computational complexities. PTv2 attempted to\nrelieve this by halving the usage of KNN compared to PTv1.\nDespite this improvement, KNN still constitutes a signifi-\ncant computational burden, consuming 28% of the forward\ntime (refer to Fig. 2).\nAdditionally, while Image Rela-\ntive Positional Encoding (RPE) benefits from a grid layout\nthat allows for the predefinition of relative positions, point\ncloud RPE must resort to computing pairwise Euclidean\ndistances and employ learned layers or lookup tables for\nmapping such distances to embeddings, proves to be an-\nother source of inefficiency, occupying 26% of the forward\ntime (see Fig. 2). These extremely inefficient operations\nbring difficulties when scaling up the backbone.\nInspired by two recent advancements [48, 77], we move\naway from the traditional paradigm, which treats point\nclouds as unordered sets. Instead, we choose to \u201cbreak\u201d the\nconstraints of permutation invariance by serializing point\nclouds into a structured format. This strategic transforma-\ntion enables our method to leverage the benefits of struc-\ntured data inefficiency with a compromise of the accuracy\nof locality-preserving property. We consider this trade-off\nas an entry point of our design.\n3\n(b) Hilbert\n(d) Trans Hilbert\n(a) Z-order\n(c) Trans Z-order\nFigure 3. Point cloud serialization. We show the four patterns of serialization with a triplet visualization. For each triplet, we show the\nspace-filling curve for serialization (left), point cloud serialization var sorting order within the space-filling curve (middle), and grouped\npatches of the serialized point cloud for local attention (right). Shifting across the four serialization patterns allows the attention mechanism\nto capture various spatial relationships and contexts, leading to an improvement in model accuracy and generalization capacity.\n4. Point Transformer V3\nIn this section, we present our designs of Point Transformer\nV3 (PTv3), guided by the scaling principle discussed in\nSec. 3. Our approach emphasizes simplicity and speed, fa-\ncilitating scalability and thereby making it stronger.\n4.1. Point Cloud Serialization\nTo trade the simplicity and efficiency nature of structured\ndata, we introduce point cloud serialization, transforming\nunstructured point clouds into a structured format.\nSpace-filling curves. Space-filling curves [56] are paths\nthat pass through every point within a higher-dimensional\ndiscrete space and preserve spatial proximity to a certain\nextent.\nMathematically, it can be defined as a bijective\nfunction \u03c6 : Z 7\u2192 Zn, where n is the dimensionality of\nthe space, which is 3 within the context of point clouds\nand also can extend to a higher dimension. Our method\ncenters on two representative space-filling curves:\nthe\nz-order curve [51] and the Hilbert curve [27]. The Z-order\ncurve (see Fig. 3a) is valued for its simplicity and ease\nof computation, whereas the Hilbert curve (see Fig. 3b)\nis known for its superior locality-preserving properties\ncompared with Z-order curve.\nStandard space-filling curves process the 3D space by\nfollowing a sequential traversal along the x, y, and z axes,\nrespectively.\nBy altering the order of traversal, such as\nprioritizing the y-axis before the x-axis, we introduce re-\nordered variants of standard space-filling curves. To differ-\nentiate between the standard configurations and the alter-\nnative variants of space-filling curves, we denote the latter\nwith the prefix \u201ctrans\u201d, resulting in names such as Trans Z-\norder (see Fig. 3c) and Trans Hilbert (see Fig. 3d). These\nvariants can offer alternative perspectives on spatial rela-\ntionships, potentially capturing special local relationships\nthat the standard curve may overlook.\nSerialized encoding. To leverage the locality-preserving\nproperties of space-filling curves, we employ serialized en-\ncoding, a strategy that converts a point\u2019s position into an in-\nteger reflecting its order within a given space-filling curve.\nDue to the bijective nature of these curves, there exists an\ninverse mapping \u03c6\u22121 : Zn 7\u2192 Z which allows for the trans-\nformation of a point\u2019s position pi \u2208 R3 into a serializa-\ntion code. By projecting the point\u2019s position onto a discrete\nspace with a grid size of g \u2208 R, we obtain this code as\n\u03c6\u22121(\u230a p / g \u230b). This encoding is also adaptable to batched\npoint cloud data. By assigning each point a 64-bit integer\nto record serialization code, we allocate the trailing k bits\nto the position encoded by \u03c6\u22121 and the remaining leading\nbits to the batch index b \u2208 Z. Sorting the points according\nto this serialization code makes the batched point clouds\nordered with the chosen space-filling curve pattern within\neach batch. The whole process can be written as follows:\nEncode(p, b, g) = (b \u226a k)|\u03c6\u22121(\u230a p / g \u230b),\nwhere \u226a denotes left bit-shift and | denotes bitwise OR.\nSerialization. As illustrated in the middle part of triplets\nin Fig. 3, the serialization of point clouds is accomplished\nby sorting the codes resulting from the serialized encoding.\nThe ordering effectively rearranges the points in a manner\nthat respects the spatial ordering defined by the given space-\nfilling curve, which means that neighbor points in the data\nstructure are also likely to be close in space.\nIn our implementation, we do not physically re-order the\npoint clouds, but rather, we record the mappings generated\nby the serialization process. This strategy maintains com-\npatibility with various serialization patterns and provides\nthe flexibility to transition between them efficiently.\n4\n(a) Reordering\n(b) Padding\nFigure 4. Patch grouping. (a) Reordering point cloud accord-\ning to order derived from a specific serialization pattern.\n(b)\nPadding point cloud sequence by borrowing points from neighbor-\ning patches to ensure it is divisible by the designated patch size.\n4.2. Serialized Attention\nRe-weigh options of attention mechanism. Image trans-\nformers [19, 46, 47], benefiting from the structured and reg-\nular grid of pixel data, naturally prefer window [46] and\ndot-product [20, 75] attention mechanisms. These meth-\nods take advantage of the fixed spatial relationships inherent\nto image data, allowing for efficient and scalable localized\nprocessing. However, this advantage vanishes when con-\nfronting the unstructured nature of point clouds. To adapt,\nprevious point transformers [84, 100] introduce neighbor-\nhood attention to construct even-size attention kernels and\nadopt vector attention to improve model convergence on\npoint cloud data with a more complex spatial relation.\nIn light of the structured nature of serialized point clouds,\nwe choose to revisit and adopt the efficient window and\ndot-product attention mechanisms as our foundational ap-\nproach. While the serialization strategy may temporarily\nyield a lower performance than some neighborhood con-\nstruction strategies like KNN due to a reduction in precise\nspatial neighbor relationships, we will demonstrate that any\ninitial accuracy gaps can be effectively bridged by harness-\ning the scalability potential inherent in serialization.\nEvolving from window attention, we define patch atten-\ntion, a mechanism that groups points into non-overlapping\npatches and performs attention within each individual\npatch. The effectiveness of patch attention relies on two\nmajor designs: patch grouping and patch interaction.\nPatch grouping. Grouping points into patches within se-\nrialized point clouds has been well-explored in recent ad-\nvancements [48, 77]. This process is both natural and ef-\nficient, involving the simple grouping of points along the\nserialized order after padding. Our design for patch atten-\ntion is also predicated on this strategy as presented in Fig. 4.\nIn practice, the processes of reordering and patch padding\ncan be integrated into a single indexing operation.\nFurthermore, we illustrate patch grouping patterns de-\nrived from the four serialization patterns on the right part of\ntriplets in Fig. 3. This grouping strategy, in tandem with our\nserialization patterns, is designed to effectively broaden the\nattention mechanism\u2019s receptive field in the 3D space as the\n(b) Shift Dilation\n(a) Standard\n(c) Shift Patch\n(d) Shift Order\n(e) Shuffle Order\nshuffle\nattn\nattn\nattn\nattn\nattn\nattn\nattn\nattn\nFigure 5. Patch interaction. (a) Standard patch grouping with a\nregular, non-shifted arrangement; (b) Shift Dilation where points\nare grouped at regular intervals, creating a dilated effect; (c) Shift\nPatch, which applies a shifting mechanism similar to the shift win-\ndow approach; (d) Shift Order where different serialization pat-\nterns are cyclically assigned to successive attention layers; (d)\nShuffle Order, where the sequence of serialization patterns is ran-\ndomized before being fed to attention layers.\npatch size increases while still preserving spatial neighbor\nrelationships to a feasible degree. Although this approach\nmay sacrifice some neighbor search accuracy when com-\npared to KNN, the trade-off is beneficial. Given the atten-\ntion\u2019s re-weighting capacity to reference points, the gains\nin efficiency and scalability far outweigh the minor loss in\nneighborhood precision (scaling it up is all we need).\nPatch interaction. The interaction between points from\ndifferent patches is critical for the model to integrate infor-\nmation across the entire point cloud. This design element\ncounters the limitations of a non-overlapping architecture\nand is pivotal in making patch attention functional. Build-\ning on this insight, we investigate various designs for patch\ninteraction as outlined below (also visualized in Fig. 5):\n\u2022 In Shift Dilation [77], patch grouping is staggered by\na specific step across the serialized point cloud, effec-\ntively extending the model\u2019s receptive field beyond the\nimmediate neighboring points.\n\u2022 In Shift Patch, the positions of patches are shifted across\nthe serialized point cloud, drawing inspiration from the\nshift-window strategy in image transformers [46]. This\nmethod maximizes the interaction among patches.\n\u2022 In Shift Order, the serialized order of the point cloud\ndata is dynamically varied between attention blocks.\nThis technique, which aligns seamlessly with our point\ncloud serialization method, serves to prevent the model\nfrom overfitting to a single pattern and promotes a more\nrobust integration of features across the data.\n\u2022 Shuffle Order\u2217, building upon Shift Order, introduces a\nrandom shuffle to the permutations of serialized orders.\nThis method ensures that the receptive field of each at-\n5\nxCPE\nAttention\nLayerNorm\nMLP\nLayerNorm\nBlock\n\u00d7 N\nShuffle Orders\nGrid Pool\nEncoder\nEmbedding\nSerialization\n\u00d7 S\nPoint Cloud\nInitialization\n*\n*\n*\nFigure 6. Overall architecture.\ntention layer is not limited to a single pattern, thus fur-\nther enhancing the model\u2019s ability to generalize.\nWe mark our main proposal with \u2217 and underscore its supe-\nrior performance in model ablation.\nPositional encoding.\nTo handle the voluminous data,\npoint cloud transformers commonly employ local atten-\ntion, which is reliant on relative positional encoding meth-\nods [38, 95, 100] for optimal performance. However, our\nobservations indicate that RPEs are notably inefficient and\ncomplex. As a more efficient alternative, conditional posi-\ntional encoding (CPE) [13, 77] is introduced for point cloud\ntransformers, where implemented by octree-based depth-\nwise convolutions [78]. We consider this replacement to be\nelegant, as the implementation of RPE in point cloud trans-\nformers can essentially be regarded as a variant of large-\nkernel sparse convolution. Even so, a single CPE is not suf-\nficient for the peak performance (there remains potential for\nan additional 0.5% improvement when coupled with RPE).\nTherefore, we present an enhanced conditional positional\nencoding (xCPE), implemented by directly prepending a\nsparse convolution layer with a skip connection before the\nattention layer. Our experimental results demonstrate that\nxCPE fully unleashes the performance with a slight increase\nin latency of a few milliseconds compared to the standard\nCPE, the performance gains justify this minor trade-off.\n4.3. Network Details\nIn this section, we detail the macro designs of PTv3, includ-\ning block structure, pooling strategy, and model architecture\n(visualized in Fig. 6). Our options for these components are\nempirical yet also crucial to overall simplicity. Detailed ab-\nlations of these choices are available in the Appendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix.\nBlock structure. We simplify the traditional block struc-\nture, typically an extensive stack of normalization and acti-\nvation layers, by adopting a pre-norm [11] structure, eval-\nuated against the post-norm [75] alternative. Additionally,\nwe shift from Batch Normalization (BN) to Layer Normal-\nization (LN). The proposed xCPE is prepended directly be-\nfore the attention layer with a skip connection.\nPooling strategy. We keep adopting the Grid Pooling in-\ntroduced in PTv2, recognizing its simplicity and efficiency.\nOur experiments indicate that BN is essential and cannot\nbe effectively replaced by LN. We hypothesize that BN is\ncrucial for stabilizing the data distribution in point clouds\nduring pooling. Additionally, the proposed Shuffle Order,\nPatterns\nS.O.\n+ S.D.\n+ S.P.\n+ Shuffle O.\nZ\n74.3 54ms\n75.5 89ms\n75.8 86ms\n74.3 54ms\nZ + TZ\n76.0 55ms\n76.3 92ms\n76.1 89ms\n76.9 55ms\nH + TH\n76.2 60ms\n76.1 98ms\n76.2 94ms\n76.8 60ms\nZ + TZ + H + TH\n76.5 61ms\n76.8 99ms\n76.6 97ms\n77.3 61ms\nTable 2. Serialization patterns and patch interaction. The first\ncolumn indicates serialization patterns: Z for Z-order, TZ for Trans\nZ-order, H for Hilbert, and TH for Trans Hilbert. In the first row,\nS.O. represents Shift Order, which is the default setting also ap-\nplied to other interaction strategies. S.D. stands for Shift Dilation,\nand S.P. signifies Shift Patch.\nPE\nAPE\nRPE\ncRPE\nCPE\nxCPE\nPerf. (%)\n72.1 50ms\n75.9 72ms\n76.8 101ms\n76.6 58ms\n77.3 61ms\nTable 3. Positional encoding. We compare the proposed CPE+\nwith APE, RPE, cRPE, and CPE. RPE and CPE are discussed in\nOctFormer [77], while cRPE is deployed by Swin3D [95].\nP.S.\n16\n32\n64\n128\n256\n1024\n4096\nPerf. (%)\n75.0\n75.6\n76.3\n76.6\n76.8\n77.3\n77.1\nStd. Dev.\n0.15\n0.22\n0.31\n0.36\n0.28\n0.22\n0.39\nTable 4. Patch size. Leveraging the inherent simplicity and effi-\nciency of our approach, we expand the receptive field of attention\nwell beyond the conventional scope, surpassing sizes used in pre-\nvious works such as PTv2 [84], which adopts a size of 16, and\nOctFormer [77], which uses 24.\nwith shuffle the permutation of serialized orders for Shift\nOrder, is integrated into the pooling.\nModel architecture.\nThe architecture of PTv3 remains\nconsistent with the U-Net [61] framework. It consists of\nfour stage encoders and decoders, with respective block\ndepths of [2, 2, 6, 2] and [1, 1, 1, 1]. For these stages, the\ngrid size multipliers are set at [\u00d72, \u00d72, \u00d72, \u00d72], indicating\nthe expansion ratio relative to the preceding pooling stage.\n5. Experiments\n5.1. Main Properties\nWe perform an ablation study on PTv3, focusing on module\ndesign and scalability. We report the performance using the\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean\nmean results from the ScanNet semantic segmentation val-\nidation and measure the latencies using the average\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage\naverage values\nobtained from the full ScanNet validation set (with a batch\nsize of 1) on a single RTX 4090. In the tables, our settings\nare highlighted in gray, and details are in the Appendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix.\nSerialization patterns. In Tab. 2, we explore the impact of\nvarious combinations of serialization patterns. Our exper-\niments demonstrate that mixtures incorporating a broader\nrange of patterns yield superior results when integrated with\nour Shuffle Order strategies. Furthermore, the additional\ncomputational overhead from introducing more serializa-\ntion patterns is negligible. It is observed that relying on\na single Shift Order cannot completely harness the potential\n6\nIndoor Sem. Seg.\nScanNet [16]\nScanNet200 [62]\nS3DIS [2]\nMethods\nVal\nTest\nVal\nTest\nArea5\n6-fold\n\u25e6 MinkUNet [12]\n72.2\n73.6\n25.0\n25.3\n65.4\n65.4\n\u25e6 ST [38]\n74.3\n73.7\n-\n-\n72.0\n-\n\u25e6 PointNeXt [60]\n71.5\n71.2\n-\n-\n70.5\n74.9\n\u25e6 OctFormer [77]\n75.7\n76.6\n32.6\n32.6\n-\n-\n\u25e6 Swin3D2 [95]\n75.5\n-\n-\n-\n72.5\n76.9\n\u25e6 PTv1 [100]\n70.6\n-\n27.8\n-\n70.4\n65.4\n\u25e6 PTv2 [84]\n75.4\n74.2\n30.2\n-\n71.6\n73.5\n\u25e6 PTv3 (Ours)\n77.5\n77.9\n35.2\n37.8\n73.4\n77.7\n\u2022 PTv3 (Ours)\n78.6\n79.4\n36.0\n39.3\n74.7\n80.8\nTable 5. Indoor semantic segmentation.\nMethod\nMetric Area1 Area2 Area3 Area4 Area5 Area6 6-Fold\n\u25e6 PTv2\nallAcc\n92.30\n86.00\n92.98\n89.23\n91.24\n94.26\n90.76\nmACC\n88.44\n72.81\n88.41\n82.50\n77.85\n92.44\n83.13\nmIoU\n81.14\n61.25\n81.65\n69.06\n72.02\n85.95\n75.17\n\u25e6 PTv3\nallAcc\n93.22\n86.26\n94.56\n90.72\n91.67\n94.98\n91.53\nmACC\n89.92\n74.44\n94.45\n81.11\n78.92\n93.55\n85.31\nmIoU\n83.01\n63.42\n86.66\n71.34\n73.43\n87.31\n77.70\n\u2022 PTv3\nallAcc\n93.70\n90.34\n94.72\n91.87\n91.96\n94.98\n92.59\nmACC\n90.70\n78.40\n94.27\n86.61\n80.14\n93.80\n87.69\nmIoU\n83.88\n70.11\n87.40\n75.53\n74.33\n88.74\n80.81\nTable 6. S3DIS 6-fold cross-validation.\noffered by the four serialization patterns.\nPatch interaction. In Tab. 2, we also assess the effective-\nness of each alternative patch interaction design. The de-\nfault setting enables Shift Order, but the first row represents\nthe baseline scenario using a single serialization pattern, in-\ndicative of the vanilla configurations of Shift Patch and Shift\nDilation (one single serialization order is not shiftable). The\nresults indicate that while Shift Patch and Shift Dilation are\nindeed effective, their latency is somewhat hindered by the\ndependency on attention masks, which compromises effi-\nciency. Conversely, Shift Code, which utilizes multiple se-\nrialization patterns, offers a simple and efficient alternative\nthat achieves comparable results to these traditional meth-\nods. Notably, when combined with Shuffle Order and all\nfour serialization patterns, our strategy not only shows fur-\nther improvement but also retains its efficiency.\nPositional encoding. In Tab. 3, we benchmark our pro-\nposed CPE+ against conventional positional encoding, such\nas APE and RPE, as well as recent advanced solutions like\ncRPE and CPE. The results confirm that while RPE and\ncRPE are significantly more effective than APE, they also\nexhibit the inefficiencies previously discussed. Conversely,\nCPE and CPE+ emerge as superior alternatives. Although\nCPE+ incorporates slightly more parameters than CPE, it\ndoes not compromise our method\u2019s efficiency too much.\nSince CPEs operate prior to the attention phase rather than\nduring it, they benefit from optimization like flash atten-\n2We report Swin3D validation results without\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout\nwithout model ensemble.\nOutdoor Sem. Seg.\nnuScenes [5]\nSem.KITTI [3]\nWaymo Val [67]\nMethods\nVal\nTest\nVal\nTest\nmIoU\nmAcc\n\u25e6 MinkUNet [12]\n73.3\n-\n63.8\n-\n65.9\n76.6\n\u25e6 SPVNAS [68]\n77.4\n-\n64.7\n66.4\n-\n-\n\u25e6 Cylender3D [102]\n76.1\n77.2\n64.3\n67.8\n-\n-\n\u25e6 AF2S3Net [9]\n62.2\n78.0\n74.2\n70.8\n-\n-\n\u25e6 2DPASS [92]\n-\n80.8\n69.3\n72.9\n-\n-\n\u25e6 SphereFormer [39]\n78.4\n81.9\n67.8\n74.8\n69.9\n-\n\u25e6 PTv2 [84]\n80.2\n82.6\n70.3\n72.6\n70.6\n80.2\n\u25e6 PTv3 (Ours)\n80.4\n82.7\n70.8\n74.2\n71.3\n80.5\n\u2022 PTv3 (Ours)\n81.2\n83.0\n72.3\n75.5\n72.1\n81.3\nTable 7. Outdoor semantic segmentation.\nIndoor Ins. Seg.\nScanNet [16]\nScanNet200 [62]\nPointGroup [33]\nmAP25\nmAP50\nmAP\nmAP25\nmAP50\nmAP\n\u25e6 MinkUNet [12]\n72.8\n56.9\n36.0\n32.2\n24.5\n15.8\n\u25e6 PTv2 [84]\n76.3\n60.0\n38.3\n39.6\n31.9\n21.4\n\u25e6 PTv3 (Ours)\n77.5\n61.7\n40.9\n40.1\n33.2\n23.1\n\u2022 PTv3 (Ours)\n78.9\n63.5\n42.1\n40.8\n34.1\n24.0\nTable 8. Indoor instance segmentation.\nData Efficient [28]\nLimited Reconstruction\nLimited Annotation\nMethods\n1%\n5%\n10%\n20%\n20\n50\n100\n200\n\u25e6 MinkUNet [12]\n26.0\n47.8\n56.7\n62.9\n41.9\n53.9\n62.2\n65.5\n\u25e6 PTv2 [84]\n24.8\n48.1\n59.8\n66.3\n58.4\n66.1\n70.3\n71.2\n\u25e6 PTv3 (Ours)\n25.8\n48.9\n61.0\n67.0\n60.1\n67.9\n71.4\n72.7\n\u2022 PTv3 (Ours)\n31.3\n52.6\n63.3\n68.2\n62.4\n69.1\n74.3\n75.5\nTable 9. Data efficiency.\ntion [17, 18], which can be advantageous for our PTv3.\nPatch size. In Tab. 4, we explore the scaling of the recep-\ntive field of attention by adjusting patch size. Beginning\nwith a patch size of 16, a standard in prior point transform-\ners, we observe that increasing the patch size significantly\nenhances performance. Moreover, as indicated in Tab. 1\n(benchmarked with NuScenes dataset), benefits from op-\ntimization techniques such as flash attention [17, 18], the\nspeed and memory efficiency are effectively managed.\n5.2. Results Comparision\nWe benchmark the performance of PTv3 against previous\nSOTA backbones and present the highest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest\nhighest results obtained\nfor each benchmark.\nIn our tables, Marker \u25e6 refers to a\nmodel trained from scratch, and \u2022 refers to a model trained\nwith multi-dataset joint training (PPT [85]). An exhaustive\ncomparison with earlier works is available in the Appendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix\nAppendix.\nIndoor semantic segmentation. In Tab. 5, we showcase\nthe validation and test performance of PTv3 on the Scan-\nNet v2 [16] and ScanNet200 [62] benchmarks, along with\nthe Area 5 and 6-fold cross-validation [58] on S3DIS [2]\n(details see Tab. 6). We report the mean Intersection over\nUnion (mIoU) percentages and benchmark these results\nagainst previous backbones.\nEven without pre-training,\n7\nWaymo Obj. Det.\nVehicle L2 Pedestrian L2\nCyclist L2\nMean L2\nMethods\n# mAP\nAPH\nmAP\nAPH\nmAP\nAPH\nmAPH\nPointPillars [41]\n1 63.6\n63.1\n62.8\n50.3\n61.9\n59.9\n57.8\nCenterPoint [96]\n1 66.7\n66.2\n68.3\n62.6\n68.7\n67.6\n65.5\nSST [21]\n1 64.8\n64.4\n71.7\n63.0\n68.0\n66.9\n64.8\nSST-Center [21]\n1 66.6\n66.2\n72.4\n65.0\n68.9\n67.6\n66.3\nVoxSet [26]\n1 66.0\n65.6\n72.5\n65.4\n69.0\n67.7\n66.2\nPillarNet [24]\n1 70.4\n69.9\n71.6\n64.9\n67.8\n66.7\n67.2\nFlatFormer [48]\n1 69.0\n68.6\n71.5\n65.3\n68.6\n67.5\n67.2\nPTv3 (Ours)\n1 71.2\n70.8\n76.3\n70.4\n71.5\n70.4\n70.5\nCenterPoint [96]\n2 67.7\n67.2\n71.0\n67.5\n71.5\n70.5\n68.4\nPillarNet [24]\n2 71.6\n71.6\n74.5\n71.4\n68.3\n67.5\n70.2\nFlatFormer [48]\n2 70.8\n70.3\n73.8\n70.5\n73.6\n72.6\n71.2\nPTv3 (Ours)\n2 72.5\n72.1\n77.6\n74.5\n71.0\n70.1\n72.2\nCenterPoint++ [96] 3 71.8\n71.4\n73.5\n70.8\n73.7\n72.8\n71.6\nSST [21]\n3 66.5\n66.1\n76.2\n72.3\n73.6\n72.8\n70.4\nFlatFormer [48]\n3 71.4\n71.0\n74.5\n71.3\n74.7\n73.7\n72.0\nPTv3 (Ours)\n3 73.0\n72.5\n78.0\n75.0\n72.3\n71.4\n73.0\nTable 10. Waymo object detection. The colume with head name\n\u201c#\u201d denotes the number of input frames.\nPTv3 outperforms PTv2 by 3.7% on the ScanNet test split\nand by 4.2% on the S3DIS 6-fold CV. The advantage of\nPTv3 becomes even more pronounced when scaling up the\nmodel with multi-dataset joint training [85], widening the\nmargin to 5.2% on ScanNet and 7.3% on S3DIS.\nOutdoor semantic segmentation. In Tab. 7, we detail the\nvalidation and test results of PTv3 for the nuScenes [5] and\nSemanticKITTI [3] benchmarks and also include the vali-\ndation results for the Waymo benchmark [67]. Performance\nmetrics are presented as mIoU percentages by default, with\na comparison to prior models. PTv3 demonstrates enhanced\nperformance over the recent state-of-the-art model, Sphere-\nFormer, with a 2.0% improvement on nuScenes and a 3.0%\nincrease on SemanticKITTI, both in the validation con-\ntext. When pre-trained, PTv3\u2019s lead extends to 2.8% for\nnuScenes and 4.5% for SemanticKITTI.\nIndoor instance segmentation.\nIn Tab. 8, we present\nPTv3\u2019s validation results on the ScanNet v2 [16] and\nScanNet200 [62] instance segmentation benchmarks. We\npresent the performance metrics as mAP, mAP25, and\nmAP50 and compare them against several popular back-\nbones.\nTo ensure a fair comparison, we standardize\nthe instance segmentation framework by employing Point-\nGroup [33] across all tests, varying only the backbone. Our\nexperiments reveal that integrating PTv3 as a backbone sig-\nnificantly enhances PointGroup, yielding a 4.9% increase\nin mAP over MinkUNet. Moreover, fine-tuning a PPT pre-\ntrained PTv3 provides an additional gain of 1.2% mAP.\nIndoor data efficient. In Tab. 9, we evaluate the perfor-\nmance of PTv3 on the ScanNet data efficient [28] bench-\nmark. This benchmark tests models under constrained con-\nditions with limited percentages of available reconstructions\n(scenes) and restricted numbers of annotated points. Across\nIndoor Efficiency (ScanNet)\nTraining\nInference\nMethods\nParams.\nLatency\nMemory\nLatency\nMemory\nMinkUNet [12]\n37.9M\n267ms\n4.9G\n90ms\n4.7G\nOctFormer [77]\n44.0M\n264ms\n12.9G\n86ms\n12.5G\nSwin3D [95]\n71.1M\n602ms\n13.6G\n456ms\n8.8G\nPTv2 [84]\n12.8M\n312ms\n13.4G\n191ms\n18.2G\nPTv3 (ours)\n46.2M\n151ms\n6.8G\n61ms\n5.2G\nTable 11. Indoor model efficiency.\nvarious settings, from 5% to 20% of reconstructions and\nfrom 20 to 200 annotations, PTv3 demonstrates strong per-\nformance. Moreover, the application of pre-training tech-\nnologies further boosts PTv3\u2019s performance across all tasks.\nOutdoor object detection.\nIn Tab. 10, we benchmark\nPTv3 against leading single-stage 3D detectors on the\nWaymo Object Detection benchmark. All models are eval-\nuated using either anchor-based or center-based detection\nheads [93, 96], with a separate comparison for varying num-\nbers of input frames. Our PTv3, engaged with CenterPoint,\nconsistently outperforms both sparse convolutional [24, 96]\nand transformer-based [21, 26] detectors, achieving signifi-\ncant gains even when compared with the recent state-of-the-\nart, FlatFormer [48]. Notably, PTv3 surpasses FlatFormer\nby 3.3% with a single frame as input and maintains a supe-\nriority of 1.0% in multi-frame settings.\nModel efficiency. We evaluate model efficiency based on\naverage latency and memory consumption across real-world\ndatasets. Efficiency metrics are measured on a single RTX\n4090, excluding the first iteration to ensure steady-state\nmeasurements. We compared our PTv3 with multiple pre-\nvious SOTAs. Specifically, we use the NuScenes dataset to\nassess outdoor model efficiency (see Tab. 1) and the Scan-\nNet dataset for indoor model efficiency (see Tab. 11). Our\nresults demonstrate that PTv3 not only exhibits the lowest\nlatency across all tested scenarios but also maintains rea-\nsonable memory consumption.\n6. Conclusion and Discussion\nThis paper presents Point Transformer V3, a stride towards\novercoming the traditional trade-offs between accuracy and\nefficiency in point cloud processing. Guided by a novel in-\nterpretation of the scaling principle in backbone design, we\npropose that model performance is more profoundly influ-\nenced by scale than by complex design intricacies. By prior-\nitizing efficiency over the accuracy of less impactful mech-\nanisms, we harness the power of scale, leading to enhanced\nperformance. Simply put, by making the model simpler\nand faster, we enable it to become stronger.\nWe discuss limitations and broader impacts as follows:\n\u2022 Attention mechanisum. In prioritizing efficiency, PTv3\nreverts to utilizing dot-product attention, which has been\nwell-optimized through engineering efforts.\nHowever,\nwe do note a reduction in convergence speed and a limi-\n8\ntation in further scaling depth compared to vector atten-\ntion. This issue also observed in recent advancements\nin transformer technology [87], is attributed to \u2019attention\nsinks\u2019 stemming from the dot-product and softmax oper-\nations. Consequently, our findings reinforce the need for\ncontinued exploration of attention mechanisms.\n\u2022 Scaling Parameters. PTv3 transcends the existing trade-\noffs between accuracy and efficiency, paving the way for\ninvestigating 3D transformers at larger parameter scales\nwithin given computational resources.\nWhile this ex-\nploration remains a topic for future work, current point\ncloud transformers already demonstrate an over-capacity\nfor existing tasks. We advocate for a combined approach\nthat scales up both the model parameters and the scope\nof data and tasks (e.g., learning from all available data,\nmulti-task frameworks, and multi-modality tasks). Such\nan integrated strategy could fully unlock the potential of\nscaling in 3D representation learning.\nAcknowledgements\nThis work is supported in part by the National Natural Sci-\nence Foundation of China (NO.622014840), the National\nKey R&D Program of China (NO.2022ZD0160101), HKU\nStartup Fund, and HKU Seed Fund for Basic Research.\nAppendix\nFor a thorough understanding of our Point Transformer V3\n(PTv3), we have compiled a detailed Appendix. The table\nof contents below offers a quick overview and will guide to\nspecific sections of interest.\nContents\nA. Implementation Details\n9\nA.1. Training Settings\n. . . . . . . . . . . . . . .\n9\nA.2. Model Settings\n. . . . . . . . . . . . . . . .\n10\nA.3. Data Augmentations . . . . . . . . . . . . . .\n10\nB. Additional Ablations\n10\nB.1. Nomalization Layer . . . . . . . . . . . . . .\n10\nB.2. Block Structure . . . . . . . . . . . . . . . .\n11\nC. Additional Comparision\n11\nC.1. Indoor Semantic Segmentation . . . . . . . .\n11\nC.2. Outdoor Semantic Segmentation . . . . . . .\n12\nA. Implementation Details\nOur implementation primarily utilizes Pointcept [14], a spe-\ncialized codebase focusing on point cloud perception and\nrepresentation learning. For tasks involving outdoor object\ndetection, we employ OpenPCDet [71], which is tailored\nfor LiDAR-based 3D object detection. The specifics of our\nimplementation are detailed in this section.\nScratch\nJoint Training [85]\nConfig\nValue\nConfig\nValue\noptimizer\nAdamW\noptimizer\nAdamW\nscheduler\nCosine\nscheduler\nCosine\ncriteria\nCrossEntropy (1)\ncriteria\nCrossEntropy (1)\nLovasz [4] (1)\nLovasz [4] (1)\nlearning rate\n5e-3\nlearning rate\n5e-3\nblock lr scaler\n0.1\nblock lr scaler\n0.1\nweight decay\n5e-2\nweight decay\n5e-2\nbatch size\n12\nbatch size\n24\ndatasets\nScanNet /\ndatasets\nScanNet (2)\nS3DIS /\nS3DIS (1)\nStruct.3D\nStruct.3D (4)\nwarmup epochs\n40\nwarmup iters\n6k\nepochs\n800\niters\n120k\nTable 12. Indoor semantic segmentation settings.\nScratch\nJoint Training [85]\nConfig\nValue\nConfig\nValue\noptimizer\nAdamW\noptimizer\nAdamW\nscheduler\nCosine\nscheduler\nCosine\ncriteria\nCrossEntropy (1)\ncriteria\nCrossEntropy (1)\nLovasz [4] (1)\nLovasz [4] (1)\nlearning rate\n2e-3\nlearning rate\n2e-3\nblock lr scaler\n1e-1\nblock lr scaler\n1e-1\nweight decay\n5e-3\nweight decay\n5e-3\nbatch size\n12\nbatch size\n24\ndatasets\nNuScenes /\ndatasets\nNuScenes (1)\nSem.KITTI /\nSem.KITTI (1)\nWaymo\nWaymo (1)\nwarmup epochs\n2\nwarmup iters\n9k\nepochs\n50\niters\n180k\nTable 13. Outdoor semantic segmentation settings.\nIns. Seg.\nObj. Det\nConfig\nValue\nConfig\nValue\nframework\nPointGroup [33]\nframework\nCenterPoint [96]\noptimizer\nAdamW\noptimizer\nAdam\nscheduler\nCosine\nscheduler\nCosine\nlearning rate\n5e-3\nlearning rate\n3e-3\nblock lr scaler\n1e-1\nblock lr scaler\n1e-1\nweight decay\n5e-2\nweight decay\n1e-2\nbatch size\n12\nbatch size\n16\ndatasets\nScanNet\ndatasets\nWaymo\nwarmup epochs\n40\nwarmup epochs\n0\nepochs\n800\nepochs\n24\nTable 14. Other downstream tasks settings.\nA.1. Training Settings\nIndoor semantic segmentation. The settings for indoor\nsemantic segmentation are outlined in Tab. 12. The two\nleftmost columns describe the parameters for training from\nscratch using a single dataset. To our knowledge, this rep-\nresents the first initiative to standardize training settings\nacross different indoor benchmarks with a unified approach.\nThe two rightmost columns describe the parameters for\n9\nConfig\nValue\nserialization pattern\nZ + TZ + H + TH\npatch interaction\nShift Order + Shuffle Order\npositional encoding\nxCPE\nembedding depth\n2\nembedding channels\n32\nencoder depth\n[2, 2, 6, 2]\nencoder channels\n[64, 128, 256, 512]\nencoder num heads\n[4, 8, 16, 32]\nencoder patch size\n[1024, 1024, 1024, 1024]\ndecoder depth\n[1, 1, 1, 1]\ndecoder channels\n[64, 64, 128, 256]\ndecoder num heads\n[4, 4, 8, 16]\ndecoder patch size\n[1024, 1024, 1024, 1024]\ndown stride\n[\u00d72, \u00d72, \u00d72, \u00d72]\nmlp ratio\n4\nqkv bias\nTrue\ndrop path\n0.3\nTable 15. Model settings.\nAugmentations Parameters\nIndoor Outdoor\nrandom dropout dropout ratio: 0.2, p: 0.2\n\u2713\n-\nrandom rotate\naxis: z, angle: [-1, 1], p: 0.5\n\u2713\n\u2713\naxis: x, angle: [-1 / 64, 1 / 64], p: 0.5\n\u2713\n-\naxis: y, angle: [-1 / 64, 1 / 64], p: 0.5\n\u2713\n-\nrandom scale\nscale: [0.9, 1.1]\n\u2713\n\u2713\nrandom flip\np: 0.5\n\u2713\n\u2713\nrandom jitter\nsigma: 0.005, clip: 0.02\n\u2713\n\u2713\nelastic distort\nparams: [[0.2, 0.4], [0.8, 1.6]]\n\u2713\n-\nauto contrast\np: 0.2\n\u2713\n-\ncolor jitter\nstd: 0.05; p: 0.95\n\u2713\n-\ngrid sampling\ngrid size: 0.02 (indoor), 0.05 (outdoor)\n\u2713\n\u2713\nsphere crop\nratio: 0.8, max points: 128000\n\u2713\n-\nnormalize color p: 1\n\u2713\n-\nTable 16. Data augmentations.\nmulti-dataset joint training [85] with PTv3, maintaining\nsimilar settings to the scratch training but with an increased\nbatch size. The numbers in brackets indicate the relative\nweight assigned to each dataset (criteria) in the mix.\nOutdoor semantic segmentation. The configuration for\noutdoor semantic segmentation, presented in Tab. 13, fol-\nlows a similar format to that of indoor. We also standardize\nthe training settings across three outdoor datasets. Notably,\nPTv3 operates effectively without the need for point clip-\nping within a specific range, a step that is typically essential\nin current models. Furthermore, we extend our methodol-\nogy to multi-dataset joint training with PTv3, employing\nsettings analogous to scratch training but with augmented\nbatch size. The numbers in brackets represent the propor-\ntional weight assigned to each dataset in the training mix.\nOther Downstream Tasks. We outline our configurations\nfor indoor instance segmentation and outdoor object detec-\ntion in Tab. 14. For indoor instance segmentation, we use\nPointGroup [33] as our foundational framework, a popular\nchoice in 3D representation learning [28, 85, 86, 88]. Our\nconfiguration primarily follows PointContrast [88], with\nBlock\nBN\nLN\nBN\nLN\nPooling\nBN\nLN\nLN\nBN\nPerf.\n76.7\n76.1\n75.6\n77.3\nTable 17. Nomalization layer.\nBlock\nTraditional\nPost-Norm\nPre-Norm\nPerf.\n76.6\n72.3\n77.3\nTable 18. Block structure.\nnecessary adjustments made for PTv3 compatibility. Re-\ngarding outdoor object detection, we adhere to the settings\ndetailed in FlatFormer [48] and implement CenterPoint as\nour base framework to assess PTv3\u2019s effectiveness. It\u2019s im-\nportant to note that PTv3 is versatile and can be integrated\nwith various other frameworks due to its backbone nature.\nA.2. Model Settings\nAs briefly described in Sec. 4.3, here we delve into the\ndetailed model configurations of our PTv3, which are\ncomprehensively listed in Tab. 15.\nThis table serves\nas a blueprint for components within serialization-based\npoint cloud transformers, encapsulating models like Oct-\nFormer [77] and FlatFormer [48] within the outlined frame-\nworks, except for certain limitations discussed in Sec. 2.\nSpecifically, OctFormer can be interpreted as utilizing a sin-\ngle z-order serialization with patch interaction enabled by\nShift Dilation. Conversely, FlatFormer can be character-\nized by its window-based serialization approach, facilitat-\ning patch interaction through Shift Order.\nA.3. Data Augmentations\nThe specific configurations of data augmentations imple-\nmented for PTv3 are detailed in Tab. 16. We unify augmen-\ntation pipelines for both indoor and outdoor scenarios sepa-\nrately, and the configurations are shared by all tasks within\neach domain. Notably, we observed that PTv3 does not de-\npend on point clipping within a specific range, a process\noften crucial for existing models.\nB. Additional Ablations\nIn this section, we present further ablation studies focusing\non macro designs of PTv3, previously discussed in Sec. 4.3.\nB.1. Nomalization Layer\nPrevious point transformers employ Batch Normalization\n(BN), which can lead to performance variability depend-\ning on the batch size. This variability becomes particularly\nproblematic in scenarios with memory constraints that re-\nquire small batch sizes or in tasks demanding dynamic or\nvarying batch sizes. To address this issue, we have grad-\nually transitioned to Layer Normalization (LN). Our final,\nempirically determined choice is to implement Layer Nor-\n10\nMethods\nYear\nVal\nTest\n\u25e6 PointNet++ [59]\n2017\n53.5\n55.7\n\u25e6 3DMV [15]\n2018\n-\n48.4\n\u25e6 PointCNN [44]\n2018\n-\n45.8\n\u25e6 SparseConvNet [23]\n2018\n69.3\n72.5\n\u25e6 PanopticFusion [52]\n2019\n-\n52.9\n\u25e6 PointConv [82]\n2019\n61.0\n66.6\n\u25e6 JointPointBased [10]\n2019\n69.2\n63.4\n\u25e6 KPConv [72]\n2019\n69.2\n68.6\n\u25e6 PointASNL [91]\n2020\n63.5\n66.6\n\u25e6 SegGCN [42]\n2020\n-\n58.9\n\u25e6 RandLA-Net [30]\n2020\n-\n64.5\n\u25e6 JSENet [31]\n2020\n-\n69.9\n\u25e6 FusionNet [98]\n2020\n-\n68.8\n\u25e6 FastPointTransformer [55]\n2022\n72.4\n-\n\u25e6 SratifiedTranformer [38]\n2022\n74.3\n73.7\n\u25e6 PointNeXt [60]\n2022\n71.5\n71.2\n\u25e6 LargeKernel3D [8]\n2023\n73.5\n73.9\n\u25e6 PointMetaBase [45]\n2023\n72.8\n71.4\n\u25e6 PointConvFormer [83]\n2023\n74.5\n74.9\n\u25e6 OctFormer [77]\n2023\n75.7\n76.6\n\u25e6 Swin3D [95]\n2023\n75.5\n-\n\u2022 + Supervised [95]\n2023\n76.7\n77.9\n\u25e6 MinkUNet [12]\n2019\n72.2\n73.6\n\u2022 + PC [88]\n2020\n74.1\n-\n\u2022 + CSC [28]\n2021\n73.8\n-\n\u2022 + MSC [86]\n2023\n75.5\n-\n\u2022 + PPT [85]\n2023\n76.4\n76.6\n\u25e6 PTv1 [100]\n2021\n70.6\n-\n\u25e6 PTv2 [84]\n2022\n75.4\n74.2\n\u25e6 PTv3 (Ours)\n2023\n77.5\n77.9\n\u2022 + PPT [85]\n2023\n78.6\n79.4\nTable 19. ScanNet V2 semantic segmentation.\nmalization in the attention blocks while retaining Batch\nNormalization in the pooling layers (see Tab. 17).\nB.2. Block Structure\nPrevious point transformers use a traditional block structure\nthat sequentially applies an operator, a normalization layer,\nand an activation function. While effective, this approach\ncan sometimes complicate training deeper models due to is-\nsues like vanishing gradients or the need for careful initial-\nization and learning rate adjustments [89]. Consequently,\nwe explored adopting a more modern block structure, such\nas pre-norm and post-norm. The pre-norm structure, where\na normalization layer precedes the operator, can stabilize\ntraining by ensuring normalized inputs for each layer [11].\nIn contrast, the post-norm structure places a normalization\nlayer right after the operator, potentially leading to faster\nconvergence but with less stability [75]. Our experimental\nresults (see Tab. 18) indicate that the pre-norm structure is\nmore suitable for our PTv3, aligning with findings in recent\ntransformer-based models [89].\nMethods\nYear\nArea5\n6-fold\n\u25e6 PointNet [58]\n2017\n41.1\n47.6\n\u25e6 SegCloud [70]\n2017\n48.9\n-\n\u25e6 TanConv [69]\n2018\n52.6\n-\n\u25e6 PointCNN [44]\n2018\n57.3\n65.4\n\u25e6 ParamConv [79]\n2018\n58.3\n-\n\u25e6 PointWeb [99]\n2019\n60.3\n66.7\n\u25e6 HPEIN [32]\n2019\n61.9\n-\n\u25e6 KPConv [72]\n2019\n67.1\n70.6\n\u25e6 GACNet [76]\n2019\n62.9\n-\n\u25e6 PAT [94]\n2019\n60.1\n-\n\u25e6 SPGraph [40]\n2018\n58.0\n62.1\n\u25e6 SegGCN [42]\n2020\n63.6\n-\n\u25e6 PAConv [90]\n2021\n66.6\n-\n\u25e6 StratifiedTransformer [38]\n2022\n72.0\n-\n\u25e6 PointNeXt [60]\n2022\n70.5\n74.9\n\u25e6 PointMetaBase [45]\n2023\n72.0\n77.0\n\u25e6 Swin3D [95]\n2023\n72.5\n76.9\n\u2022 + Supervised [95]\n2023\n74.5\n79.8\n\u25e6 MinkUNet [12]\n2019\n65.4\n65.4\n\u2022 + PC [88]\n2020\n70.3\n-\n\u2022 + CSC [28]\n2021\n72.2\n-\n\u2022 + MSC [86]\n2023\n70.1\n-\n\u2022 + PPT [85]\n2023\n74.7\n78.1\n\u25e6 PTv1 [100]\n2021\n70.4\n65.4\n\u25e6 PTv2 [84]\n2022\n71.6\n73.5\n\u25e6 PTv3 (Ours)\n2023\n73.4\n77.7\n\u2022 + PPT [85]\n2023\n74.7\n80.8\nTable 20. S3DIS semantic segmentation.\nC. Additional Comparision\nIn this section, we expand upon the combined results ta-\nble for semantic segmentation (Tab. 5 and Tab. 7) from\nour main paper, offering a more detailed breakdown of re-\nsults alongside the respective publication years of previous\nworks. This comprehensive result table is designed to assist\nreaders in tracking the progression of research efforts in 3D\nrepresentation learning. Marker \u25e6 refers to the result from a\nmodel trained from scratch, and \u2022 refers to the result from a\npre-trained model.\nC.1. Indoor Semantic Segmentation\nWe conduct a detailed comparison of pre-training technolo-\ngies and backbones on the ScanNet v2 [16] (see Tab. 19)\nand S3DIS [2] (see Tab. 20) datasets. ScanNet v2 com-\nprises 1,513 room scans reconstructed from RGB-D frames,\ndivided into 1,201 training scenes and 312 for validation. In\nthis dataset, model input point clouds are sampled from the\nvertices of reconstructed meshes, with each point assigned\na semantic label from 20 categories (e.g., wall, floor, table).\nThe S3DIS dataset for semantic scene parsing includes 271\nrooms across six areas from three buildings. Following a\ncommon practice [59, 70, 100], we withhold area 5 for test-\ning and perform a 6-fold cross-validation. Different from\nScanNet v2, S3DIS densely sampled points on mesh sur-\nfaces, annotated into 13 categories. Consistent with stan-\n11\nMethods\nYear\nVal\nTest\n\u25e6 SPVNAS [68]\n2020\n64.7\n66.4\n\u25e6 Cylender3D [102]\n2021\n64.3\n67.8\n\u25e6 PVKD [29]\n2022\n-\n71.2\n\u25e6 2DPASS [92]\n2022\n69.3\n72.9\n\u25e6 WaffleIron [57]\n2023\n68.0\n70.8\n\u25e6 SphereFormer [39]\n2023\n67.8\n74.8\n\u25e6 RangeFormer [37]\n2023\n67.6\n73.3\n\u25e6 MinkUNet [12]\n2019\n63.8\n-\n\u2022 + PPT [85]\n2023\n71.4\n-\n\u25e6 PTv2 [84]\n2022\n70.3\n72.6\n\u25e6 PTv3 (Ours)\n2023\n70.8\n74.2\n\u2022 + PPT [85]\n2023\n72.3\n75.5\nTable 21. SemanticKITTI semantic segmentation.\nMethods\nYear\nVal\nTest\n\u25e6 SPVNAS [68]\n2020\n77.4\n-\n\u25e6 Cylender3D [102]\n2021\n76.1\n77.2\n\u25e6 PVKD [29]\n2022\n-\n76.0\n\u25e6 2DPASS [92]\n2022\n-\n80.8\n\u25e6 SphereFormer [39]\n2023\n78.4\n81.9\n\u25e6 RangeFormer [37]\n2023\n78.1\n80.1\n\u25e6 MinkUNet [12]\n2019\n73.3\n-\n\u2022 + PPT [85]\n2023\n78.6\n-\n\u25e6 PTv2 [84]\n2022\n80.2\n82.6\n\u25e6 PTv3 (Ours)\n2023\n80.4\n82.7\n\u2022 + PPT [85]\n2023\n81.2\n83.0\nTable 22. NuScenes semantic segmentation.\ndard practice [59]. We employ the mean class-wise inter-\nsection over union (mIoU) as the primary evaluation metric\nfor indoor semantic segmentation.\nC.2. Outdoor Semantic Segmentation\nWe extend our comprehensive evaluation of pre-training\ntechnologies and backbones to outdoor semantic segmenta-\ntion tasks, focusing on the SemanticKITTI [3](see Tab. 21)\nand NuScenes [5] (see Tab. 22) datasets. SemanticKITTI is\nderived from the KITTI Vision Benchmark Suite and con-\nsists of 22 sequences, with 19 for training and the remain-\ning 3 for testing. It features richly annotated LiDAR scans,\noffering a diverse array of driving scenarios. Each point\nin this dataset is labeled with one of 28 semantic classes,\nencompassing various elements of urban driving environ-\nments.\nNuScenes, on the other hand, provides a large-\nscale dataset for autonomous driving, comprising 1,000 di-\nverse urban driving scenes from Boston and Singapore. For\noutdoor semantic segmentation, we also employ the mean\nclass-wise intersection over union (mIoU) as the primary\nevaluation metric for outdoor semantic segmentation.\nReferences\n[1] Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao,\nHuaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei\nZhuang, Vinh Q. Tran, Dara Bahri, Jianmo Ni, Jai Gupta,\nKai Hui, Sebastian Ruder, and Donald Metzler. Ext5: To-\nwards extreme multi-task scaling for transfer learning. In\nICLR, 2022. 1\n[2] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioan-\nnis Brilakis, Martin Fischer, and Silvio Savarese. 3d seman-\ntic parsing of large-scale indoor spaces. In CVPR, 2016. 7,\n11\n[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-\nzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Se-\nmantickitti: A dataset for semantic scene understanding of\nlidar sequences. In ICCV, 2019. 7, 8, 12\n[4] Maxim Berman, Amal Rannen Triki, and Matthew B\nBlaschko. The lov\u00b4asz-softmax loss: A tractable surrogate\nfor the optimization of the intersection-over-union measure\nin neural networks. In CVPR, pages 4413\u20134421, 2018. 9\n[5] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020. 7,\n8, 12\n[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers.\nIn CVPR, 2021. 2\n[7] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In CVPR, 2017. 2\n[8] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi,\nand Jiaya Jia.\nLargekernel3d: Scaling up kernels in 3d\nsparse cnns. In CVPR, 2023. 11\n[9] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and\nBingbing Liu. (af)2-s3net: Attentive feature fusion with\nadaptive feature selection for sparse semantic segmentation\nnetwork. In CVPR, 2021. 7\n[10] Hung-Yueh Chiang, Yen-Liang Lin, Yueh-Cheng Liu, and\nWinston H Hsu. A unified point-based framework for 3d\nsegmentation. In 3DV, 2019. 11\n[11] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 6, 11\n[12] Christopher Choy, JunYoung Gwak, and Silvio Savarese.\n4d spatio-temporal convnets:\nMinkowski convolutional\nneural networks. In CVPR, 2019. 2, 3, 7, 8, 11, 12\n[13] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang,\nXiaolin Wei, Huaxia Xia, and Chunhua Shen.\nCon-\nditional positional encodings for vision transformers.\narXiv:2102.10882, 2021. 6\n[14] Pointcept Contributors. Pointcept: A codebase for point\ncloud perception research.\nhttps://github.com/\nPointcept/Pointcept, 2023. 9\n[15] Angela Dai and Matthias Nie\u00dfner. 3dmv: Joint 3d-multi-\nview prediction for 3d semantic scene segmentation.\nIn\nECCV, 2018. 1, 11\n12\n[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. In\nCVPR, 2017. 7, 8, 11\n[17] Tri Dao. Flashattention-2: Faster attention with better par-\nallelism and work partitioning. arXiv:2307.08691, 2023.\n7\n[18] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and\nChristopher R\u00b4e. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In NeurIPS, 2022. 7\n[19] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining\nGuo.\nCswin transformer: A general vision transformer\nbackbone with cross-shaped windows. In CVPR, 2022. 5\n[20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An im-\nage is worth 16x16 words: Transformers for image recog-\nnition at scale. ICLR, 2021. 5\n[21] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang,\nHang Zhao, Feng Wang, Naiyan Wang, and Zhaoxiang\nZhang.\nEmbracing single stride 3d object detector with\nsparse transformer. In CVPR, 2022. 8\n[22] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min\nXu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy\nLiptchinsky, Ishan Misra, Armand Joulin, et al.\nSelf-\nsupervised pretraining of visual features in the wild.\narXiv:2103.01988, 2021. 1\n[23] Benjamin Graham, Martin Engelcke, and Laurens van der\nMaaten. 3d semantic segmentation with submanifold sparse\nconvolutional networks. In CVPR, 2018. 2, 3, 11\n[24] Chao Ma Guangsheng Shi, Ruifeng Li.\nPillarnet: Real-\ntime and high-performance pillar-based 3d object detection.\nECCV, 2022. 8\n[25] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. Computational Visual Media, 2021. 1, 2\n[26] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. Voxel\nset transformer: A set-to-set approach to 3d object detection\nfrom point clouds. In CVPR, 2022. 8\n[27] David Hilbert and David Hilbert. \u00a8Uber die stetige abbildung\neiner linie auf ein fl\u00a8achenst\u00a8uck. Dritter Band: Analysis\u00b7\nGrundlagen der Mathematik\u00b7 Physik Verschiedenes: Nebst\nEiner Lebensgeschichte, 1935. 4\n[28] Ji Hou, Benjamin Graham, Matthias Nie\u00dfner, and Saining\nXie. Exploring data-efficient 3d scene understanding with\ncontrastive scene contexts. In CVPR, 2021. 2, 7, 8, 10, 11\n[29] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy,\nand Yikang Li.\nPoint-to-voxel knowledge distillation\nfor lidar semantic segmentation.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, 2022. 12\n[30] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan\nGuo, Zhihua Wang, Niki Trigoni, and Andrew Markham.\nRandla-net: Efficient semantic segmentation of large-scale\npoint clouds. In CVPR, 2020. 11\n[31] Zeyu Hu, Mingmin Zhen, Xuyang Bai, Hongbo Fu, and\nChiew-lan Tai.\nJsenet: Joint semantic segmentation and\nedge detection network for 3d point clouds. In ECCV, 2020.\n11\n[32] Li Jiang, Hengshuang Zhao, Shu Liu, Xiaoyong Shen, Chi-\nWing Fu, and Jiaya Jia. Hierarchical point-edge interaction\nnetwork for point cloud semantic segmentation. In ICCV,\n2019. 11\n[33] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-\nWing Fu, and Jiaya Jia. Pointgroup: Dual-set point group-\ning for 3d instance segmentation. CVPR, 2020. 7, 8, 9,\n10\n[34] Li Jiang, Zetong Yang, Shaoshuai Shi, Vladislav Golyanik,\nDengxin Dai, and Bernt Schiele.\nSelf-supervised pre-\ntraining with masked shape prediction for 3d scene under-\nstanding. In CVPR, 2023. 2\n[35] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv:2001.08361, 2020. 1\n[36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi\nMao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00b4ar,\nand Ross Girshick. Segment anything. In ICCV, 2023. 1\n[37] Lingdong Kong, Youquan Liu, Runnan Chen, Yuexin Ma,\nXinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Ziwei\nLiu. Rethinking range view representation for lidar seg-\nmentation. In ICCV, 2023. 12\n[38] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang\nZhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified trans-\nformer for 3d point cloud segmentation. In CVPR, 2022. 2,\n3, 6, 7, 11\n[39] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya\nJia. Spherical transformer for lidar-based 3d recognition. In\nCVPR, 2023. 7, 12\n[40] Loic Landrieu and Martin Simonovsky. Large-scale point\ncloud semantic segmentation with superpoint graphs.\nIn\nCVPR, 2018. 11\n[41] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In CVPR, 2019. 2,\n8\n[42] Huan Lei, Naveed Akhtar, and Ajmal Mian. Seggcn: Effi-\ncient 3d point cloud segmentation with fuzzy spherical ker-\nnel. In CVPR, 2020. 11\n[43] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from\n3d lidar using fully convolutional network. In RSS, 2016. 2\n[44] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan\nDi, and Baoquan Chen.\nPointcnn: Convolution on x-\ntransformed points. NeurIPS, 2018. 1, 11\n[45] Haojia Lin, Xiawu Zheng, Lijiang Li, Fei Chao, Shan-\nshan Wang, Yan Wang, Yonghong Tian, and Rongrong Ji.\nMeta architecture for point cloud analysis. In CVPR, pages\n17682\u201317691, 2023. 11\n[46] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,\nZheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted win-\ndows. ICCV, 2021. 5\n13\n[47] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie,\nYixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong,\net al. Swin transformer v2: Scaling up capacity and resolu-\ntion. In CVPR, 2022. 5\n[48] Zhijian Liu, Xinyu Yang, Haotian Tang, Shang Yang, and\nSong Han. Flatformer: Flattened window attention for ef-\nficient point cloud transformer. In CVPR, 2023. 2, 3, 5, 8,\n10\n[49] Xu Ma, Can Qin, Haoxuan You, Haoxi Ran, and Yun Fu.\nRethinking network design and local geometry in point\ncloud: A simple residual mlp framework. ICLR, 2022. 2\n[50] Daniel Maturana and Sebastian Scherer. Voxnet: A 3d con-\nvolutional neural network for real-time object recognition.\nIn IROS, 2015. 2\n[51] Guy M Morton. A computer oriented geodetic data base\nand a new technique in file sequencing. International Busi-\nness Machines Company New York, 1966. 4\n[52] Gaku Narita, Takashi Seno, Tomoya Ishikawa, and Yohsuke\nKaji. Panopticfusion: Online volumetric semantic mapping\nat the level of stuff and things. In IROS, 2019. 11\n[53] OpenAI. Gpt-4 technical report. arXiv:2303.08774, 2023.\n1\n[54] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,\nYonghong Tian, and Li Yuan.\nMasked autoencoders for\npoint cloud self-supervised learning. In ECCV, 2022. 2\n[55] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jae-\nsik Park. Fast point transformer. In CVPR, pages 16949\u2013\n16958, 2022. 11\n[56] Giuseppe Peano and G Peano. Sur une courbe, qui remplit\ntoute une aire plane. Springer, 1990. 4\n[57] Gilles Puy, Alexandre Boulch, and Renaud Marlet. Using a\nwaffle iron for automotive point cloud semantic segmenta-\ntion. In ICCV, 2023. 12\n[58] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In CVPR, 2017. 1, 2, 7, 11\n[59] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. In NeurIPS, 2017. 2, 11, 12\n[60] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai,\nHasan Hammoud,\nMohamed Elhoseiny,\nand Bernard\nGhanem. Pointnext: Revisiting pointnet++ with improved\ntraining and scaling strategies. NeurIPS, 2022. 7, 11\n[61] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In MICCAI, pages 234\u2013241. Springer, 2015. 6\n[62] David\nRozenberszki,\nOr\nLitany,\nand\nAngela\nDai.\nLanguage-grounded indoor 3d semantic segmentation in\nthe wild. In ECCV, 2022. 7, 8\n[63] Aditya Sanghi.\nInfo3d: Representation learning on 3d\nobjects using mutual information maximization and con-\ntrastive learning. In ECCV, 2020. 2\n[64] Jonathan Sauder and Bjarne Sievers. Self-supervised deep\nlearning on point clouds by reconstructing space.\nIn\nNeurIPS, 2019. 2\n[65] Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang,\nManolis Savva, and Thomas Funkhouser. Semantic scene\ncompletion from a single depth image. In CVPR, 2017. 2\n[66] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and\nErik G. Learned-Miller. Multi-view convolutional neural\nnetworks for 3d shape recognition. In ICCV, 2015. 2\n[67] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-\nlien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin\nZhou, Yuning Chai, Benjamin Caine, et al. Scalability in\nperception for autonomous driving: Waymo open dataset.\nIn CVPR, 2020. 7, 8\n[68] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji\nLin, Hanrui Wang, and Song Han. Searching efficient 3d ar-\nchitectures with sparse point-voxel convolution. In ECCV,\n2020. 7, 12\n[69] Maxim Tatarchenko, Jaesik Park, Vladlen Koltun, and\nQian-Yi Zhou. Tangent convolutions for dense prediction\nin 3d. In CVPR, 2018. 11\n[70] Lyne Tchapmi, Christopher Choy, Iro Armeni, JunYoung\nGwak, and Silvio Savarese. Segcloud: Semantic segmenta-\ntion of 3d point clouds. In 3DV, 2017. 11\n[71] OpenPCDet\nDevelopment\nTeam.\nOpenpcdet:\nAn\nopen-source toolbox for 3d object detection from point\nclouds.\nhttps://github.com/open- mmlab/\nOpenPCDet, 2020. 9\n[72] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud,\nBeatriz Marcotegui, Franc\u00b8ois Goulette, and Leonidas J\nGuibas. Kpconv: Flexible and deformable convolution for\npoint clouds. In ICCV, 2019. 2, 11\n[73] Yonglong Tian, Olivier J Henaff, and A\u00a8aron van den Oord.\nDivide and contrast: Self-supervised learning from uncu-\nrated data. In CVPR, 2021. 1\n[74] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv:2302.13971, 2023. 1\n[75] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017. 5, 6, 11\n[76] Lei Wang, Yuchun Huang, Yaolin Hou, Shenman Zhang,\nand Jie Shan. Graph attention convolution for point cloud\nsemantic segmentation. In CVPR, 2019. 11\n[77] Peng-Shuai Wang. Octformer: Octree-based transformers\nfor 3D point clouds. SIGGRAPH, 2023. 2, 3, 5, 6, 7, 8, 10,\n11\n[78] Peng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun,\nand Xin Tong. O-CNN: Octree-based convolutional neural\nnetworks for 3D shape analysis. SIGGRAPH, 36(4), 2017.\n2, 6\n[79] Shenlong Wang,\nSimon Suo,\nWei-Chiu Ma,\nAndrei\nPokrovsky, and Raquel Urtasun. Deep parametric contin-\nuous convolutional neural networks. In CVPR, 2018. 11\n[80] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and\nTiejun Huang. Images speak in images: A generalist painter\nfor in-context visual learning. In CVPR, 2023. 1\n[81] Yue Wang and Justin M Solomon.\nDeep closest point:\nLearning representations for point cloud registration.\nIn\nICCV, 2019. 2\n14\n[82] Wenxuan Wu, Zhongang Qi, and Li Fuxin.\nPointconv:\nDeep convolutional networks on 3d point clouds. In CVPR,\n2019. 1, 11\n[83] Wenxuan Wu, Li Fuxin, and Qi Shan. Pointconvformer:\nRevenge of the point-based convolution. In CVPR, pages\n21802\u201321813, 2023. 11\n[84] Xiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Heng-\nshuang Zhao. Point transformer v2: Grouped vector atten-\ntion and partition-based pooling. In NeurIPS, 2022. 1, 2, 3,\n5, 6, 7, 8, 11, 12\n[85] Xiaoyang Wu, Zhuotao Tian, Xin Wen, Bohao Peng, Xihui\nLiu, Kaicheng Yu, and Hengshuang Zhao. Towards large-\nscale 3d representation learning with multi-dataset point\nprompt training. arXiv:2308.09718, 2023. 1, 2, 3, 7, 8,\n9, 10, 11, 12\n[86] Xiaoyang Wu, Xin Wen, Xihui Liu, and Hengshuang Zhao.\nMasked scene contrast: A scalable framework for unsuper-\nvised 3d representation learning. In CVPR, 2023. 2, 10,\n11\n[87] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han,\nand Mike Lewis. Efficient streaming language models with\nattention sinks. arXiv, 2023. 9\n[88] Saining Xie, Jiatao Gu, Demi Guo, Charles R Qi, Leonidas\nGuibas, and Or Litany. Pointcontrast: Unsupervised pre-\ntraining for 3d point cloud understanding. In ECCV, 2020.\n2, 10, 11\n[89] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin\nZheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei\nWang, and Tieyan Liu. On layer normalization in the trans-\nformer architecture. In ICML, 2020. 11\n[90] Mutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan\nQi. Paconv: Position adaptive convolution with dynamic\nkernel assembling on point clouds. In CVPR, 2021. 11\n[91] Xu Yan, Chaoda Zheng, Zhen Li, Sheng Wang, and\nShuguang Cui. Pointasnl: Robust point clouds processing\nusing nonlocal neural networks with adaptive sampling. In\nCVPR, 2020. 11\n[92] Xu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, Ruimao\nZhang, Shuguang Cui, and Zhen Li.\n2dpass: 2d priors\nassisted semantic segmentation on lidar point clouds. In\nECCV, 2022. 7, 12\n[93] Yan Yan, Yuxing Mao, and Bo Li.\nSecond: Sparsely\nembedded convolutional detection. Sensors, 18(10):3337,\n2018. 8\n[94] Jiancheng Yang, Qiang Zhang, Bingbing Ni, Linguo Li,\nJinxian Liu, Mengdie Zhou, and Qi Tian. Modeling point\nclouds with self-attention and gumbel subset sampling. In\nCVPR, 2019. 11\n[95] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu,\nHao Pan, Peng-Shuai Wang, Xin Tong, and Baining Guo.\nSwin3d: A pretrained transformer backbone for 3d indoor\nscene understanding. arXiv:2304.06906, 2023. 2, 3, 6, 7,\n8, 11\n[96] Tianwei Yin, Xingyi Zhou, and Philipp Kr\u00a8ahenb\u00a8uhl.\nCenter-based 3d object detection and tracking. In CVPR,\n2021. 8, 9\n[97] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-BERT: Pre-training 3D point\ncloud transformers with masked point modeling. In CVPR,\n2022. 2\n[98] Feihu Zhang, Jin Fang, Benjamin Wah, and Philip Torr.\nDeep fusionnet for point cloud semantic segmentation. In\nECCV, 2020. 11\n[99] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia.\nPointweb: Enhancing local neighborhood features for point\ncloud processing. In CVPR, 2019. 2, 11\n[100] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. In ICCV, 2021. 1, 2, 3,\n5, 6, 7, 11\n[101] Haoyi Zhu, Honghui Yang, Xiaoyang Wu, Di Huang, Sha\nZhang, Xianglong He, Tong He, Hengshuang Zhao, Chun-\nhua Shen, Yu Qiao, et al. Ponderv2: Pave the way for 3d\nfoundataion model with a universal pre-training paradigm.\narXiv:2310.08586, 2023. 2, 3\n[102] Xinge Zhu, Hui Zhou, Tai Wang, Fangzhou Hong, Yuexin\nMa, Wei Li, Hongsheng Li, and Dahua Lin. Cylindrical and\nasymmetrical 3d convolution networks for lidar segmenta-\ntion. In CVPR, 2021. 7, 12\n15\n"
  },
  {
    "title": "Weight subcloning: direct initialization of transformers using larger pretrained ones",
    "link": "https://arxiv.org/pdf/2312.09299.pdf",
    "upvote": "16",
    "text": "Weight Subcloning:\nDirect Initialization of Transformers Using Larger Pretrained Ones\nMohammad Samragh Mehrdad Farajtabar Sachin Mehta Raviteja Vemulapalli\nFartash Faghri Devang Naik Oncel Tuzel Mohammad Rastegari\nApple\nAbstract\nTraining large transformer models from scratch\nfor a target task requires lots of data and is compu-\ntationally demanding. The usual practice of trans-\nfer learning overcomes this challenge by initializ-\ning the model with weights of a pretrained model\nof the same size and specification to increase the\nconvergence and training speed. However, what if\nno pretrained model of the required size is avail-\nable? In this paper, we introduce a simple yet\neffective technique to transfer the knowledge of\na pretrained model to smaller variants. Our ap-\nproach called weight subcloning expedites the\ntraining of scaled-down transformers by initializ-\ning their weights from larger pretrained models.\nWeight subcloning involves an operation on the\npretrained model to obtain the equivalent initial-\nized scaled-down model. It consists of two key\nsteps: first, we introduce neuron importance rank-\ning to decrease the embedding dimension per\nlayer in the pretrained model. Then, we remove\nblocks from the transformer model to match the\nnumber of layers in the scaled-down network.\nThe result is a network ready to undergo training,\nwhich gains significant improvements in training\nspeed compared to random initialization. For in-\nstance, we achieve 4\u00d7 faster training for vision\ntransformers in image classification and language\nmodels designed for next-token prediction.\n1. Introduction\nTransformers are models extensively used for various tasks,\nincluding language modeling (Radford et al., 2019; Dai\net al., 2019; Zhang et al., 2023) and vision applications (Han\net al., 2022; Dosovitskiy et al., 2020). Training transformer\nmodels typically requires extensive computing resources\nand large-scale training data. Pretrained models, devel-\noped by the research and industry community, are available\nto transfer the learned weights for a wide range of appli-\ncations (HuggingFace, 2023). Nevertheless, in practical\ndeployment, it is often necessary to train a scaled-down\nversion of the pretrained model that better aligns with the\navailable hardware resources. Training a scaled-down model\nfrom scratch requires substantial GPU hours especially for\nmodels that are inherently complex and hard to train, e.g.,\nlarge language models (LLMs).\nThis paper aims to speed up the training of a scaled-down\ntransformer by using pretrained model weights during ini-\ntialization. We refer to the pretrained and scaled-down\nnetworks as the parent and destination models, respectively.\nIn our setup, the destination network has fewer layers and/or\nsmaller hidden dimensions per layer than the parent model.\nOur primary question is: can we transfer the knowledge of\nthe parent model to the destination model? In other words,\ndoes initializing the destination network with weights de-\nrived from the pre-trained parent model lead to improved\ntraining speed and potentially better accuracy?\nWe refer to such weight transfer process as weight sub-\ncloning. The problem of transferring knowledge from a\nparent model to a destination model has been investigated\nin different contexts, such as knowledge distillation (Gou\net al., 2021; Lin et al., 2022; Park et al., 2022), weight shar-\ning (also known as supernet training) (Wang et al., 2021b;\nCai et al., 2019; Yu et al., 2020; Wang et al., 2021a), and\npruning (Blalock et al., 2020; Han et al., 2015; He et al.,\n2017). We will elaborate on each of these research tracks\nand their relation to our solution in Section 2. Nevertheless,\nour findings demonstrate that weight subcloning is a low-\ncost yet effective approach for enhancing the training speed\nof transformer models. In summary, the contributions of\nthis paper are as follows:\n\u2022 We introduce the concept of weight subcloning for\ntransformer models, which arises when there\u2019s a pre-\ntrained network already in place, and a smaller destina-\ntion network is set to undergo training.\n\u2022 We demonstrate the ability to remove or duplicate trans-\nformer blocks within a pretrained transformer network,\nenabling us to initialize networks with depths either\nlower or higher than that of the pretrained model.\n1\narXiv:2312.09299v1  [cs.LG]  14 Dec 2023\nWeight subcloning: direct initialization of transformers using larger pretrained ones\n\u2022 By uncovering a consistent pattern of neuron impor-\ntance across transformer layers, we establish the foun-\ndation for a re-ordering technique. This method orga-\nnizes network parameters, consistently ranking them\nfrom most important to least important within each\nlayer. Consequently, a destination network with a\nsmaller hidden dimension can selectively utilize the\nmost crucial weights from the parent network.\n\u2022 Our findings illustrate that weight subcloning signifi-\ncantly enhances the training speed for the destination\nnetwork (See Figure 1).\nFigure 1: Validation accuracy and perplexity for destination\nnetwork training for two tasks. left: image classification\nwith VIT networks (Dosovitskiy et al., 2020). right: next\ntoken prediction with GPT-2 networks (Radford et al., 2019).\nWeight subcloning reduces the number of iterations required\nto achieve an early accuracy/perplexity, and increases the\nfinal performance achieved in a limited number of iterations.\n2. Related Work\nWeight distillation is related to various research pathways.\nWe summarize the most relevant ones in this section.\n2.1. Distillation\nKnowledge distillation is a technique used to create a\nsmaller student model from a larger teacher model, with\nthe goal of reducing model size and computational complex-\nity (Gou et al., 2021; Lin et al., 2022; Park et al., 2022).\nIn this approach, the student model is trained to imitate\nthe teacher model, either at the output layer or by repli-\ncating intermediate hidden features. This process can be\ntime-consuming because it involves the large teacher model\nduring training. In a closely related work to our paper, (Lin\net al., 2020) propose a method called \u201cweight distillation\u201d.\nIn weight distillation, in addition to logit alignment, the\nparameters of the teacher are mapped to the student network\nusing a transformation, which is learned in the presence of\nboth teacher and student networks during training.\nIn the context of knowledge and weight distillation, we can\nview the teacher and student models as the parent and des-\ntination architectures, respectively, in our scenario. What\ndistinguishes our approach from traditional distillation tech-\nniques is our focus on achieving faster training. Instead of\nactively involving the teacher model in the training process,\nwe directly transfer the weights from the parent model to\nthe destination model. In comparison to weight distillation,\nwe demonstrate that the function used to map parent to des-\ntination parameters can be as straightforward as cloning\nwith minor modifications, eliminating the need for addi-\ntional learning during this parameter transfer. As a direct\nconsequence of this approach, the training loop remains un-\nchanged, making it significantly more adaptable for a wide\nrange of training tasks.\n2.2. Supernet training\nTo mitigate the engineering and GPU resource costs asso-\nciated with architecture selection, a common strategy is\nweight sharing (Wang et al., 2021b; Cai et al., 2019; Yu\net al., 2020; Wang et al., 2021a). In this approach, network\nparameters are consolidated within a supernet, and smaller\nsubnets utilize a subset of the supernet\u2019s weights during for-\nward and backward propagation. At test time, the subnets\nwithin the supernet can be extracted and deployed, offering\nvarious model sizes and accuracy levels.\nSupernet training operates by randomly selecting subnets\nduring each training iteration. Consequently, the effective\nnumber of training epochs for each subnet is roughly equiv-\nalent to the total number of training epochs divided by the\nnumber of subnets visited during training. As a result, super-\nnet training generally demands more training time compared\nto standard network training. Additionally, supernets may\nencounter convergence challenges stemming from gradient\nconflicts (parameter update conflicts) between subnets. Ad-\ndressing these convergence issues is notably challenging for\nlarge-scale models like LLMs.\nThe common element of this paper and supernet training\nis in their problem definition. The parent and destination\nmodels of our setting can be considered as the supernet\nand subnet in the weight sharing literature. However, it\u2019s\nimportant to note that the majority of publicly available\npretrained models have not been subject to the supernet-style\ntraining approach. Consequently, simply extracting subnets\nfrom these models may not yield the desired accuracy.\nWe delve into transformer architectures and demonstrate that\ntransformers inherently exhibit supernet-like characteristics,\nwithout undergoing supernet-style training. In other words,\nwe establish that transformer weights can serve as effective\ninitialization for derived subnets even when the pre-trained\nnetwork has not undergone supernet-style training.\n2\nWeight subcloning: direct initialization of transformers using larger pretrained ones\n2.3. Pruning\nPruning is a method used to create a compact model from\na larger, pretrained model (Blalock et al., 2020; Han et al.,\n2015). Pruning techniques selectively reduce the size of the\nmodel by making some weight parameters zero, either in a\nstructured or non-structured way.\nNon-structured pruning, often referred to as weight pruning,\nmay appear somewhat unrelated to our specific problem.\nThis pruning technique aims to reduce the parameter size\nin a network without altering its architecture. In certain\ninstances of non-structured pruning, such as the lottery tick-\nets hypothesis proposed by (Frankle & Carbin, 2018), the\npruned network is even re-initialized in order to match or\nattain improved end-accuracy, rather than achieving a faster\nconvergence. This distinction further sets these works apart\nfrom our specific problem setting.\nStructured pruning is particularly relevant to our problem\nbecause it modifies the network\u2019s architecture, typically by\nreducing the number of neurons in each layer (He et al.,\n2017). To be more precise, structured pruning involves ex-\ntracting a smaller sub-network from a pretrained model and\nthen fine-tuning it. One distinctive feature that sets our ap-\nproach apart from structured pruning is our neuron ordering\nmethod. It can be seen as a solution to an exceptionally\nconstrained pruning problem, where the number and index\nof preserved neurons must remain consistent across all trans-\nformer layers. Ultimately, what distinguishes our work from\ntraditional pruning techniques is our primary emphasis on\nachieving faster training within a reduced number of epochs,\nrather than striving for higher accuracy through extended\ntraining periods.\n3. Insights\nAn established observation in architectures featuring resid-\nual connections, as highlighted by He et al. (He et al., 2016),\nis that individual blocks within residual networks induce\nonly slight changes to the hidden representation. Transform-\ners, a specific type of residual network, also exhibit this\ncharacteristic. We hereby refer to this characteristic as the\nadditive residual property. This property asserts that tokens\nat layer i resemble those at layer i \u2212 1 (Liu et al., 2023).\nIn a residual transformer block, the input x undergoes a\ntransformation to produce the output y = x + f(x), where\nf(\u00b7) represents the non-residual part of the block. One inter-\npretation of the additive residual property is that the output y\nremains similar to the input x, indicating that the magnitude\nof f(x) should be small compared to x. In Figure 2-Top,\nwe present the average relative magnitude of f(x) in com-\nparison to x + f(x) across image classification (VIT) and\nlanguage modeling (GPT) tasks. Notably, the middle layers\nof the transformer networks exhibit a reduced relative mag-\nnitude in f(x), indicating that these layers function similarly\nto identity layers.\nThe practical implications of this property have been stud-\nied by researchers to reduce computation complexity (Din\net al., 2023; Schwartz et al., 2020), to explain the model\u2019s\nbehaviour by interpreting hidden layer representations (Ten-\nney et al., 2019; Geva et al., 2022; Slobodkin et al., 2021),\nor to exploit the activation sparsity to improve inference\nspeed (Liu et al., 2023; Mirzadeh et al., 2023). In this paper\nwe study the implications of the additive residual property of\ntransformers from a distinctive perspective: their potential\nto initialize a scaled-down destination model.\nChanging network depth. Since subsequent blocks of a\ntransformer model only slightly change the hidden represen-\ntation, one can either remove a single block or duplicate it\nwithout significantly altering the network\u2019s function. This\ntechnique makes it possible to create a destination network\nwith fewer or more layers than the pretrained one. To demon-\nstrate this effect in practice, Figure 2-bottom displays the\nloss of pretrained VIT and GPT transformers, when single\nlayers are either removed or duplicated at different positions.\nIt is worth noting that these destination networks are not\nfine-tuned, yet they attain meaningful loss value. Since mid-\ndle layers have smaller relative value magnitude (\n||f(x)||\n||x+f(x)||),\nthese layers are better candidates to prune/duplicate.\nReducing embedding dimension. To initialize a destina-\ntion model with a smaller embedding dimension, we must\nidentify neurons that have the most influence on the net-\nwork\u2019s output. One way to measure this influence is by\ncalculating the average magnitude of each neuron. Since\nx has a significantly larger magnitude than f(x) in most\nlayers, the significance of neurons is primarily established\nin the early transformer blocks. In other words, if neuron\nnumber j exhibits a substantial magnitude at layer i, it is\nlikely that neuron j also maintains a substantial magnitude\nin layer i + n.\nIn Figure 3, we display the averaged absolute magnitudes\nof neurons in paired layers of transformers trained for lan-\nguage modeling (GPT) and image classification (VIT). Our\nobservations reveal distinct patterns. Firstly, within each\nlayer, a subset of neurons prominently stands out with mag-\nnitudes significantly larger than the rest (please note that\nthe figures employ a log-scale). This phenomenon implies\nthat certain neurons within each layer exert a much more\nsubstantial influence on the network\u2019s output compared to\nthe majority of neurons. This phenomenon has also been\nobserved in (Dettmers et al., 2022).\nSecondly, we observe a correlation between the magnitudes\nof neurons in different layers. Specifically, neurons with\nlarge magnitudes in layer 1 tend to exhibit similar large\nmagnitudes in layer 2.\n3\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nFigure 2: Top: Illustration of the relative magnitude of output at each transformer block, denoted as\n||f(x)||\n||x+f(x)||. Layers with\nlow magnitudes function similar to an identity layer, enabling their removal or duplication without substantial impact on\nnetwork performance. Bottom: Visualization of the network\u2019s loss function as layers are removed or duplicated at various\npositions. Notably, as predicted by the magnitude ratio plot on the top, removing or duplicating middle layers leads to a\nbetter loss value.\nThese observed characteristics enable us to assess the impor-\ntance of neurons consistently across all transformer layers,\nwhich in turn aids in determining the most suitable weights\nfor initializing the destination model. We will provide a\nsystematic method to achieve this purpose in a later section\nof this paper.\nFigure 3: The relationship between neuron magnitudes\nacross layers in VIT (left) and GPT2-M (right) pretrained\nmodels. Each point shows the averaged absolute value for\na single neuron index. The horizontal and vertical axes\nrepresent selected layers.\n4. Methodology\nSuppose the parent network has N blocks indexed as\nA0, A1, . . . , AN\u22121. Given an input x \u2208 RB\u00d7T \u00d7d, where\nB is the batch size and T is the token sequence length, the\ntransformer computes\n1 for n in range(0, N):\n2\nx = A_n(x)\nOur goal is to initialize a destination network with blocks\nA\u2032\n0, A\u2032\n1, . . . , A\u2032\nN \u2032\u22121, where N \u2032 < N. The input to the des-\ntination transformer is x\u2032 \u2208 RB\u00d7T \u00d7d\u2032, where d\u2032 \u2264 d. The\ndestination network computes\n1 for n in range(0, N\u2019):\n2\nx\u2019 = A\u2019_n(x\u2019)\n4.1. Subcloning Parameters\nFor each block An, we need to sample a portion of its\nweights, biases, and normalization parameters to initialize\nA\u2032\nn. Figure 4 illustrates how the parameters are sampled.\nFor matrix parameters in linear layers, we need to subsam-\nple both rows and columns of the pretrained matrix. For\nvector parameters such as bias terms, layer norm, and\n4\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nFigure 4: Sampling weights, biases, and normalization pa-\nrameters. Here, the embedding dimension of the parent\nnetwork is twice that of the destination network. Light blue\nindicates the pretrained parameters, while dark blue repre-\nsents the sampled portion.\nbatch norm parameters, we need to sample a portion of\nthe vector elements accordingly.\nWeight re-ordering. To obtain the best results from weight\nsampling, we initially need to rearrange the rows and\ncolumns of the pretrained matrix (light blue) so that the\nmost important weights are included in the sampled portion\n(dark blue). This approach allows us to select the initial\nrows/columns during subsampling.\nNeuron importance ordering in linear layers. A linear\nlayer\u2019s output is a tensor of shape B \u00d7 T \u00d7 d, where B\nis the batch size, T is the number of tokens, and d is the\nembedding dimension. Our goal is to rank the d output neu-\nrons based on their importance. To achieve this, we run the\nnetwork on a subset of data and collect the absolute values\nof the outputs at each transformer layer. This subset can\nbe a small portion of the training data. In our experiments,\nwe used 0.5% of ImageNet for VIT and 0.003% of Pile\nfor GPT. Subsequently, we compute the average of these\nabsolute values over the B and T dimensions, resulting in a\nd-dimensional vector, which we refer to as the score.\nIn Figure 5, we provide sorted scores for example linear\nlayers within GPT and VIT transformer blocks. Notably,\ncertain neurons display significantly higher scores compared\nto others. The high-score neurons play a more substantial\nrole in shaping the network\u2019s output. Therefore, it is essen-\ntial to prioritize the inclusion of these high-scoring neurons\nin the destination network.\nImportance ordering in attention layers. In the context of\nattention layers, our objective is to sort attention heads, not\nindividual neurons. To achieve this, we begin by calculating\nscores for the neurons after the Softmax operation. Subse-\nquently, we calculate the average score for each attention\nhead by combining the scores of its constituent neurons.\nThis process allows us to rank the attention heads based on\ntheir importance. Figure 6 provides a visual representation\nof head scores for a specific transformer layer, highlighting\nFigure 5: Sorted neuron scores for selected linear layers\nin VIT-Base (left) and GPT2-M (right). The scores are\npresented for the top 50 neurons with the highest scores.\nthat certain heads exhibit significantly higher importance\nscores compared to others.\nFigure 6: Sorted head scores for selected attention layers in\nVIT (left, 12 heads) and GPT-2 (right, 16 heads).\nImportance ordering in residual blocks. In Figure 7,\nwe illustrate a sample transformer block architecture, where\nactivation maps are shown with either yellow (not connected\nthrough residuals) or blue (connected through residuals).\nWhen it comes to removing certain neurons from the yellow\nactivation maps, we have the flexibility to select the removed\nneuron indices independently at each layer. However, with\nthe blue activation maps, we must ensure that neurons are\nremoved from the same positions across connected layers to\nmaintain network connectivity. Hence, neuron re-ordering\nshould be done persistently across layers that are connected\nvia residual paths. To establish this shared activation order,\nwe compute the sum of neuron importance scores across\nall layers connected by residual paths. In Figure 8, we\npresent the shared ordering of neurons across all layers\nin a sample transformer-based network. Notably, specific\nneurons consistently exhibit higher importance across all\nlayers. This consistency can be attributed to the additive\nresidual property of transformers, discussed in Section 3.\nRe-ordering subsequent linear layers. Given the impor-\ntance scores assigned to each layer\u2019s neurons in a pretrained\nnetwork, we can rearrange the neurons within the layers.\nIn the case of attention blocks, groups of neurons are re-\nordered based on their head scores (illustrated in Figure\n6). Neurons that do not belong to attention outputs or are\n5\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nFigure 7: Block architecture in an example transformer\nmodels. Neurons connected with residual paths are illus-\ntrated with blue color. These neurons should be re-ordered\nconsistently across all blocks to maintain the network\u2019s func-\ntionality. Activations shown in yellow are not connected\nthrough residual paths and can be re-ordered independently.\nFigure 8: Sorted shared scores for all neurons connected by\nresidual layers in VIT (left) and GPT-2 (right). The scores\nare presented for the top 50 neurons with the highest scores.\nnot connected to residual connections are independently re-\nordered based on their individual scores. Neurons connected\nthrough residual layers are all reordered based on the global\nscore (depicted in Figure 8).\nThis reordering process is implemented by permuting the\nrows and columns of transformer layers. Permuting weight\nmatrices by importance produces an equivalent model with\nrearranged parameters, prioritizing the most important rows\nand columns. From this reshuffled model, smaller networks\ncan be subclone as illustrated earlier in Figure 4.\nWeight scaling. When subcloning matrices from the parent\nto the destination network, it is crucial to maintain the stan-\ndard deviation of the parent layer\u2019s neurons outputs in the\ndestination layer. Similar methodologies have been applied\nto uphold the standard deviation of network layers during\nrandom initialization (Glorot & Bengio, 2010). Assuming\nindependent and identically distributed (i.i.d.) Gaussian dis-\ntributions for network weights and activations, the standard\ndeviation of each output neuron in a linear layer is inversely\nproportional to the square root of the number of input neu-\nrons in that layer. Hence, to maintain the standard deviation,\nwe multiply the subcloned weights by\nq\nd\nd\u2032 , where d and\nd\u2032 represent the embedding sizes for the parent and desti-\nnation networks, respectively. It is essential to note that\nscaling is unnecessary for the parameters of layer norm\nand batch norm layers, as well as the bias parameters\nof the linear layer.\n4.2. Subcloning Layers\nTo initialize the destination network, we may remove (N \u2212\nN \u2032) blocks from the pretrained network so that we end up\nwith N \u2032 remaining blocks. Our experiments show that the\nbest practice is to remove these blocks from the middle of\nthe network, (see Figure 2 and discussion in Section 3).\n5. Experiments\nWe conducted experiments on two different data modalities.\nImage Classification.\nFor our image classification task,\nwe employed the Vision Transformer (VIT) models intro-\nduced in (Dosovitskiy et al., 2020). These models were\ntrained on the ImageNet classification dataset (Deng et al.,\n2009). The VIT family comprises various versions, each\noffering different levels of accuracy at varying computation\ncosts. We selected VIT-B as the parent architecture, and\nour destination architecture was defined to have the same\nnumber of layers but with half the embedding dimension per\nlayer. The pretrained network for our destination architec-\nture was obtained from (Faghri et al., 2023). We conducted\nthe training for this model on a single machine equipped\nwith 8 NVIDIA V100 GPUs.\nLanguage Modeling. In this experiment, we focused on\ntraining GPT-2 models for next token prediction, which\nwere originally introduced by (Radford et al., 2019), using\nthe Pile dataset (Gao et al., 2020). For our parent and desti-\nnation architectures, we selected GPT-L and GPT-M, respec-\ntively. The pretrained model we utilized for this experiment\nwas sourced from the HuggingFace repository (Hugging-\nFace, 2023). In terms of architectural specifics, the parent\nmodel was comprised of 36 layers with an embedding di-\nmension of 1280, while the destination model had 24 layers\nand an embedding dimension of 1024. The training process\nwas conducted on a total of 12 machines, each of which was\nequipped with 8 NVIDIA V100 GPUs.\nFor each of the tasks mentioned above, we employed two\napproaches to train the destination network: random ini-\ntialization and weight subcloning from the parent model.\nIn each experiment, we fine-tuned the learning rate and\nweight decay parameters to ensure the fastest convergence.\nAdditional information and specific details regarding these\nparameters can be found in Table 1. We perform ablation\nstudies for choosing these parameters in Section 5.1. Note\nthat the curve here shows our best practice for subcloning.\nMore alternatives and their performance will be discussed\nlater in our ablation studies.\nIn Figure 9, we compare convergence curves between ran-\ndom initialization and weight subcloning for the ImageNet\ntask. Additionally, for language modeling, we present the\nloss and perplexity in Figure 10. Our experiments demon-\n6\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nTable 1: Training parameters. Random initialization requires large learning rate and weight decay to achieve a better\naccuracy, whereas weight subcloning works with small learning rate and weight decay parameters.\nExperiment\nScheduler\nOptimizer\nLearning Rate\nWeight Decay\nVIT (ImageNet)\nrandom initialization\nCosine Annealing\nAdam\n0.002\n0.2\nweight subcloning\n0.0001\n0.005\nGPT2 (Pile)\nrandom initialization\nCosine Annealing\nAdam\n0.0001\n0.1\nweight subcloning\n0.0001\n0.001\nstrate the significant advantages of weight subcloning in\nterms of training convergence.\nModels initialized with\nweight subcloning exhibit faster convergence, achieving\nhigher accuracy within a limited number of training epochs.\nFor instance, for reaching accuracy of 70% on ImageNet,\nrandom initialization necessitates 40 epochs, while weight\nsubcloning achieves the same accuracy in only 10 epochs,\nrepresenting a 4\u00d7 faster training process. In the case of GPT-\n2 training, random initialization requires 64 \u00d7 109 tokens\nto reach a perplexity of 12, while weight subcloning accom-\nplishes this in just 64 \u00d7 109 tokens, again demonstrating a\n4\u00d7 training speedup.\nFigure 9: Validation loss and accuracy for VIT network\ntrained on ImageNet for random initialization (blue) and\nweight subcloning (orange).\nFigure 10: Validation loss and perplexity for GPT2 network\ntrained on Pile for random initialization (blue) and weight\nsubcloning (orange).\n5.1. Ablation Studies\nEffect of learning rate and weight decay. In Figure 11, we\nexamine the convergence behavior of the VIT destination\nmodel for different learning rates and weight decays. When\ndealing with random initialization, it\u2019s often beneficial to\nemploy an initialization that allows for a higher learning\nrate, which can facilitate improved convergence (Zhuang\net al., 2023). However, our findings reveal a distinct pattern\nwith weight subcloning, where lower learning rates tend to\nresult in superior convergence. When initialized randomly,\nthe ideal learning rate and weight decay values are 0.002\nand 0.2, respectively. In contrast, when utilizing weight\nsubcloning, the optimal values for learning rate and weight\ndecay are 0.0001 and 0.005, respectively.\nThis phenomenon can be attributed to the fact that weight\nsubcloning initializes a network that is likely to be posi-\ntioned closer to a local optimum. Training such a network\nwith a high learning rate can lead to catastrophic forget-\nting (French, 1999). Furthermore, models with distilled\nweights have already undergone weight decay and other\nforms of regularization during the training of the parent\nmodel. Imposing strong weight decay during the destination\nmodel training can adversely impact accuracy.\nFigure 11: Validation loss for destination models when\ntrained with different learning rates and weight decays.\nEffect of weight scaling. Figure 12 illustrates the conver-\ngence rate of the destination model in three distinct sce-\nnarios: random initialization, weight subcloning without\nscaling, and weight subcloning with scaling. For this exper-\niment, the parent model is VIT-H, trained on the ImageNet\ndataset, and the destination model shares the same number\nof layers but has half the embedding dimension.\nThe results indicate that weight scaling significantly en-\nhances the convergence rate. This improvement is attributed\n7\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nto weight scaling\u2019s ability to enforce consistent standard de-\nviations in the output of linear layers within the destination\nmodel, aligning them with the parent model and ultimately\nleading to more efficient convergence.\nFigure 12: Validation loss for destination models with ran-\ndom initialization (blue), weight subcloning without scaling\n(orange), and weight subcloning with scaling (green).\nEffect of parent Model Architecture. When selecting a\nparent model for training a destination model, there are\noften several pretrained models available, each with varying\nsizes and accuracies. It\u2019s common to consider choosing\nthe parent model with the highest accuracy, assuming it\nwould lead to better convergence during destination model\ntraining. However, our experiments reveal that opting for\nthe highest accuracy parent model doesn\u2019t necessarily result\nin improved convergence for the destination model.\nFigure 13 demonstrates the convergence of the loss function\nfor the destination GPT2-M model when different parent\nmodels are used for weight subcloning. In this particular\nexperiment, the destination GPT2-M model has 24 layers\nwith an embedding dimension of 1024. Two candidate\nparent models are considered: GPT2-L, which boasts 36\nlayers and an embedding dimension of 1280, and GPT2-\nXL, with 48 layers and an embedding dimension of 1600.\nAlthough GPT2-XL exhibits higher accuracy than GPT2-L,\nour findings suggest that GPT2-L is a more suitable choice\nfor weight subcloning to GPT2-M. The reason behind this\nchoice is that the architecture of GPT2-L is closer to that of\nGPT2-M, which leads to more effective weight subcloning\nand improved convergence.\nFigure 13: Validation loss for GPT2-M destination model\nwhen weights are subcloned from GPT2-L and GPT2-XL.\nEffect of neuron reordering. Figure 14 illustrates the\nimpact of neuron reordering on the convergence of the desti-\nnation model. In this experiment, the parent model is VIT-B,\nwhile the destination model shares the same number of lay-\ners but has half the embedding size. As demonstrated in\nthis experiment, the process of neuron reordering notably\nenhances the convergence of the destination network.\nFigure 14: Validation loss for VIT destination model with\nrandom initialization (blue), weight subcloning from non-\nreordered parent (orange), and weight subcloning from re-\nordered parent (green).\n6. Conclusion\nWe present weight subcloning, a technique for initializing a\ndestination transformer network using weights from a pre-\ntrained parent network. The destination network can have\nfewer layers or a smaller embedding dimension compared\nto the parent network. Our subcloning method is founded\non three key components: layer removal to align the number\nof layers between the two networks, neuron re-ordering to\nmatch the embedding dimensions, and weight scaling to\nmatch the standard deviation of neurons.\nWe have demonstrated that weight subcloning significantly\nenhances the training speed of transformer models with\nvirtually no additional cost, aside from a one-time initial-\nization before training. We also conducted ablation studies\nto explore the impact of various factors, including learn-\ning rate, weight decay, parent network size, weight scaling,\nand neuron re-ordering on the training convergence of the\ndestination network.\nLastly, the subcloning method explored in this paper is de-\nsigned under the assumption that the parent and destination\nnetworks belong to the same family. While the destination\nnetwork can vary from the parent in terms of the number of\nblocks and/or the embedding dimension per layer, our study\nhas not delved into the impact of more extensive architec-\ntural changes. These changes might include modifications\nto residual connections, nonlinear activations, block struc-\ntures, and similar aspects. These areas remain promising\ntopics for future research.\n8\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nReferences\nBlalock, D., Gonzalez Ortiz, J. J., Frankle, J., and Guttag, J.\nWhat is the state of neural network pruning? Proceedings\nof machine learning and systems, 2:129\u2013146, 2020.\nCai, H., Gan, C., Wang, T., Zhang, Z., and Han, S. Once-\nfor-all: Train one network and specialize it for efficient\ndeployment. arXiv preprint arXiv:1908.09791, 2019.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q. V., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a fixed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248\u2013255. Ieee, 2009.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nGpt3. int8 (): 8-bit matrix multiplication for transformers\nat scale. Advances in Neural Information Processing\nSystems, 35:30318\u201330332, 2022.\nDin, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\nto conclusions: Short-cutting transformers with linear\ntransformations. arXiv preprint arXiv:2303.09435, 2023.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,\nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,\nHeigold, G., Gelly, S., et al. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\nFaghri, F., Pouransari, H., Mehta, S., Farajtabar, M., Farhadi,\nA., Rastegari, M., and Tuzel, O. Reinforce data, multiply\nimpact: Improved model accuracy and robustness with\ndataset reinforcement. arXiv preprint arXiv:2303.08983,\n2023.\nFrankle, J. and Carbin, M. The lottery ticket hypothesis:\nFinding sparse, trainable neural networks. arXiv preprint\narXiv:1803.03635, 2018.\nFrench, R. M. Catastrophic forgetting in connectionist net-\nworks. Trends in cognitive sciences, 3(4):128\u2013135, 1999.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027,\n2020.\nGeva, M., Caciularu, A., Wang, K. R., and Goldberg, Y.\nTransformer feed-forward layers build predictions by pro-\nmoting concepts in the vocabulary space. arXiv preprint\narXiv:2203.14680, 2022.\nGlorot, X. and Bengio, Y. Understanding the difficulty\nof training deep feedforward neural networks. In Pro-\nceedings of the thirteenth international conference on\nartificial intelligence and statistics, pp. 249\u2013256. JMLR\nWorkshop and Conference Proceedings, 2010.\nGou, J., Yu, B., Maybank, S. J., and Tao, D. Knowledge\ndistillation: A survey. International Journal of Computer\nVision, 129:1789\u20131819, 2021.\nHan, K., Wang, Y., Chen, H., Chen, X., Guo, J., Liu, Z.,\nTang, Y., Xiao, A., Xu, C., Xu, Y., et al. A survey on\nvision transformer. IEEE transactions on pattern analysis\nand machine intelligence, 45(1):87\u2013110, 2022.\nHan, S., Pool, J., Tran, J., and Dally, W. Learning both\nweights and connections for efficient neural network.\nAdvances in neural information processing systems, 28,\n2015.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-\ning for image recognition. In Proceedings of the IEEE\nconference on computer vision and pattern recognition,\npp. 770\u2013778, 2016.\nHe, Y., Zhang, X., and Sun, J. Channel pruning for acceler-\nating very deep neural networks. In Proceedings of the\nIEEE international conference on computer vision, pp.\n1389\u20131397, 2017.\nHuggingFace. Name of the model checkpoint. https://\nhuggingface.co/gpt2-large, 2023.\nHugging\nFace model checkpoint. Accecced on June 2023.\nLin, S., Xie, H., Wang, B., Yu, K., Chang, X., Liang, X.,\nand Wang, G. Knowledge distillation via the target-aware\ntransformer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 10915\u2013\n10924, 2022.\nLin, Y., Li, Y., Wang, Z., Li, B., Du, Q., Xiao, T., and Zhu, J.\nWeight distillation: Transferring the knowledge in neural\nnetwork parameters. arXiv preprint arXiv:2009.09152,\n2020.\nLiu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\nShrivastava, A., Zhang, C., Tian, Y., Re, C., et al. Deja\nvu: Contextual sparsity for efficient llms at inference time.\nIn International Conference on Machine Learning, pp.\n22137\u201322176. PMLR, 2023.\nMirzadeh, I., Alizadeh, K., Mehta, S., Mundo, C. C. D.,\nTuzel, O., Samei, G., Rastegari, M., and Farajtabar, M.\nRelu strikes back: Exploiting activation sparsity in large\nlanguage models, 2023.\nPark, S., Kim, G., Oh, Y., Seo, J. B., Lee, S. M., Kim,\nJ. H., Moon, S., Lim, J.-K., Park, C. M., and Ye, J. C.\n9\nWeight subcloning: direct initialization of transformers using larger pretrained ones\nSelf-evolving vision transformer for chest x-ray diagnosis\nthrough knowledge distillation. Nature communications,\n13(1):3848, 2022.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nSchwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J.,\nand Smith, N. A. The right tool for the job: Match-\ning model and instance complexities.\narXiv preprint\narXiv:2004.07453, 2020.\nSlobodkin, A., Choshen, L., and Abend, O. Mediators in\ndetermining what processing bert performs first. arXiv\npreprint arXiv:2104.06400, 2021.\nTenney, I., Das, D., and Pavlick, E. Bert rediscovers the\nclassical nlp pipeline. arXiv preprint arXiv:1905.05950,\n2019.\nWang, D., Gong, C., Li, M., Liu, Q., and Chandra, V.\nAlphanet: Improved training of supernets with alpha-\ndivergence. In International Conference on Machine\nLearning, pp. 10760\u201310771. PMLR, 2021a.\nWang, D., Li, M., Gong, C., and Chandra, V. Attentivenas:\nImproving neural architecture search via attentive sam-\npling. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 6418\u20136427,\n2021b.\nYu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.-J., Tan,\nM., Huang, T., Song, X., Pang, R., and Le, Q. Bignas:\nScaling up neural architecture search with big single-\nstage models. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part VII 16, pp. 702\u2013717. Springer, 2020.\nZhang, H., Song, H., Li, S., Zhou, M., and Song, D. A\nsurvey of controllable text generation using transformer-\nbased pre-trained language models. ACM Computing\nSurveys, 56(3):1\u201337, 2023.\nZhuang, B., Liu, J., Pan, Z., He, H., Weng, Y., and Shen,\nC. A survey on efficient training of transformers. arXiv\npreprint arXiv:2302.01107, 2023.\n10\n"
  },
  {
    "title": "Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.09608.pdf",
    "upvote": "13",
    "text": "Faster Diffusion: Rethinking the Role of UNet Encoder in Diffusion Models\nSenmao Li1*\nTaihang Hu1\u2217\nFahad Shahbaz Khan2,3\nLinxuan Li4\nShiqi Yang5\nYaxing Wang1\u2020\nMing-Ming Cheng1\nJian Yang1\n1VCIP,CS, Nankai University, 2Mohamed bin Zayed University of AI, 3Linkoping University\n4Harbin Engineering University, 5Universitat Aut`onoma de Barcelona\n{senmaonk,hutaihang00}@gmail.com fahad.khan@liu.se {yaxing,csjyang}@nankai.edu.cn\nCustom Diffusion 2.42s\n41%\u2193\nCustom Diffusion w/\nOurs 1.42s\nDreambooth 2.42s\n41%\u2193\nDreambooth w/\nOurs 1.42s\nStable Diffusion\nDDIM 2.42s\nDPM-Solver++ 1.13s\nDPM-Solver++ 0.64s\nDDIM w/ Ours 1.42s 41%\u2193\n43%\u2193\n24%\u2193\n20%\u2193\nDeepFloyd-IF\nDPM-Solver++ 16.09s\nDDPM 34.55s\nDPM-Solver++ w/\nOurs 12.97s\nDDPM w/ Ours 26.27s\n51%\u2193\nControlNet 3.20s\nControlNet w/ Ours 1.52s\n32%\u2193\nVideoFusion 1.12s\nVideoFusion w/ Ours 0.76s\nFigure 1. Our method for a diverse set of generation tasks. We could significantly increase the speed (second/image) of image generation.\nAbstract\nOne of the key components within diffusion models is the\nUNet for noise prediction. While several works have ex-\nplored basic properties of the UNet decoder, its encoder\nlargely remains unexplored. In this work, we conduct the\nfirst comprehensive study of the UNet encoder. We empir-\nically analyze the encoder features and provide insights to\nimportant questions regarding their changes at the infer-\nence process. In particular, we find that encoder features\nchange gently, whereas the decoder features exhibit sub-\nstantial variations across different time-steps. This finding\ninspired us to omit the encoder at certain adjacent time-\n*Equal contribution.\n\u2020Corresponding author.\nsteps and reuse cyclically the encoder features in the pre-\nvious time-steps for the decoder. Further based on this ob-\nservation, we introduce a simple yet effective encoder prop-\nagation scheme to accelerate the diffusion sampling for a\ndiverse set of tasks. By benefiting from our propagation\nscheme, we are able to perform in parallel the decoder at\ncertain adjacent time-steps. Additionally, we introduce a\nprior noise injection method to improve the texture details\nin the generated image. Besides the standard text-to-image\ntask, we also validate our approach on other tasks: text-to-\nvideo, personalized generation and reference-guided gen-\neration. Without utilizing any knowledge distillation tech-\nnique, our approach accelerates both the Stable Diffusion\n(SD) and the DeepFloyd-IF models sampling by 41% and\n24% respectively, while maintaining high-quality genera-\narXiv:2312.09608v1  [cs.CV]  15 Dec 2023\ntion performance. Our code is available in FasterDiffusion.\n1. Introduction\nOne of the popular paradigms in image generation, Diffu-\nsion Models [17, 38, 45, 53] have recently achieved signif-\nicant breakthroughs in various domains, including text-to-\nvideo generation [10, 20, 33, 52, 54], personalized image\ngeneration [9, 11, 12, 22, 40] and reference-guided image\ngeneration [26, 32, 36, 57, 58].\nThe performance of diffusion models [17, 38, 41] some-\nwhat relies on the UNet [39] denoising network as well as\nlarge-scale image datasets. Here, the UNet consists of an\nencoder E, a bottleneck B, and a decoder D. The features\nobtained from the encoder E are skipped to the decoder\nD. Benefiting from the properties of the UNet architecture,\nseveral recent works [36, 43, 46, 48, 55\u201357] explore using\nthe decoder features of the UNet for a diverse set of tasks.\nPnP [48] performs text-guided image-to-image translation\nby leveraging the decoder features.\nDIFT [46] finds an\nemergent correspondence phenomenon that mainly exists in\nthe decoder features. ControlNet [57] fine-tunes an addi-\ntional encoder, initialized with the encoder of the pretrained\nUNet, and preserves the original decoder. T2I-Adapter [36]\ninjects the conditional information with a trainable encoder\ninto the encoder of UNet, and preserves the initial decoder.\nThese works indicate the crucial role of the decoder in gen-\nerating high-quality images.\nWhile the above-mentioned works explore the decoder\nof the UNet in diffusion models, little effort has been made\nto investigate the role of the UNet encoder. In this work, we\npay a close attention to the characteristics of the encoder of\nthe UNet in the pretrained text-guided diffusion model (e.g.,\nthe SD and Deepfloyd-IF). Here, an open research question\nis whether features from different layers have different con-\ntributions during inference time. Based on our analysis pre-\nsented in Sec 3.2, we discover that UNet encoder features\nchange gently (Fig. 2a) and have a high degree of similarity\n(Fig. 3 (left)), whereas the decoder features exhibit substan-\ntial variations across different time-steps (Fig. 2a and Fig. 3\n(right)). Based on these findings, a natural question is how\nto effectively re-use cyclically the encoder features from the\nprevious time-steps for the decoder as they barely change.\nFollowing the aforementioned questions and with the\naim to speed up the diffusion sampling, we introduce\nencoder propagation a simple yet effective propagation\nscheme based on encoder feature reusing during the diffu-\nsion sampling. Specifically, in certain time-steps, we do not\nuse the encoder to get encoder features, which are the input\nof decoder. Instead, we directly reuse the encoder features\nfrom the previous time-steps since during these time-steps\nthe encoder features change minimally. We show that the\nproposed propagation scheme accelerates the SD sampling\nby 24% , and DeepFolyd-IF sampling by 18%. Further-\nmore, we can use the same encoder features in previous\ntime-step as the input to the decoder of multiple later time-\nsteps, which makes it possible to conduct multiple time-\nsteps decoding concurrently. This parallel procession fur-\nther accelerates SD sampling by 41%, and DeepFloyd-IF\nsampling by 24%. While the propagation scheme improves\nthe efficiency of the diffusion sampling, we observe it to\nlead to some loss of texture information in the generated\nresults (see Fig. 6 (left, middle)). To alleviate this issue,\nwe introduce a prior noise injection strategy to preserve the\ntexture details in the generated images. With these contri-\nbutions, our proposed method achieves improved sampling\nefficiency along with maintaining high generation quality.\nWe evaluate the effectiveness of our approach across a wide\nrange of conditional diffusion-based tasks, including text-\nto-video generation (e.g., Text2Video-zero [20] and Video-\nFusion [33]), personalized image generation (e.g., Dream-\nbooth [40]) and reference-guided image generation (e.g.,\nControlNet [57]). To summarize, we make the following\ncontributions:\n1) We conduct a thorough empirical study of the features\nof the UNet in the diffusion model.\n2) Based on our analysis, we propose an encoder propaga-\ntion scheme to accelerate the diffusion sampling without\nrequiring any distillation technique.\n3) We further introduce a parallel strategy for diffusion\nmodel sampling at adjacent time-steps. We also present\na prior noise injection method to improve the image\nquality.\n4) Our qualitative and quantitative experiments demon-\nstrate the effectiveness of the proposed method.\n2. Related Work\nUNet. UNet is originally proposed for medical image seg-\nmentation [39]. Moreover, UNet extends to the domain of\n3D medical image segmentation [13, 24, 28]. Since then,\nvarious UNet-based methods have been developed, often\ncombining it with different architectures [19, 47].\nText-guided diffusion model.\nRecently, Text-to-image\ndiffusion models [1, 8, 38, 41] have made significant ad-\nvancements. Notably, Stable Diffusion and DeepFloyd-IF\n, stand out as two of the most successful diffusion models\navailable within the current open-source community. These\nmodels, building upon the UNet architecture, are versatile\nand can be applied to a wide range of tasks, including im-\nage editing [14, 25], super-resolution [7, 50], segmenta-\ntion [2, 51], and object detection [5, 37].\nDiffusion model acceleration.\nDiffusion models use it-\nerative denoising with UNet for image generation, which\ncan be time-consuming. There are plenty of works diving\n(a)\n(b)\n(c)\nFigure 2. Analyzing the UNet in Diffusion Model. (a) Feature evolving across adjacent time-steps is measured by MSE. (b) We extract\nhierarchical features output of different layers of the UNet at each time-step, average them along the channel dimension to obtain two-\ndimensional hierarchical features, and then calculate their F-norms. (c) The hierarchical features of the UNet encoder show a lower standard\ndeviation, while those of the decoder exhibit a higher standard deviation.\nFigure 3. Visualising the hierarchical features. We applied PCA to the hierarchical features following PnP [48], and use the top three\nleading components as an RGB image for visualization. The encoder features changes gently and have similarity at many time-steps (left),\nwhile the decoder features exhibit substantial variations across different time-steps (right).\ninto this direction trying to address this issue. One approach\nis step distillation, progressively simplifying existing mod-\nels to reduce sampling steps [34, 42]. Some recent works\ncombine model compression and distillation for faster sam-\npling [21, 27]. In contrast, efficient diffusion model solvers,\nsuch as DDIM [44] and DPM-Solver [30], have signifi-\ncantly reduced sampling steps. ToMe [3, 4] leverages token\nredundancy to reduce computations required for attention\noperation [49]. Orthogonal to these methods, we propose a\nnovel method for efficient sampling.\n3. Method\nWe first briefly revisit the architecture of the Stable Dif-\nfusion (SD) (Sec. 3.1), and then conduct a comprehensive\nanalysis for the hierarchical features of the UNet (Sec. 3.2),\nFinally, with our finding, we introduce a novel method\nto accelerate the diffusion sampling without resorting to\nknowledge distillation technique (Sec. 3.3).\n3.1. Latent Diffusion Model\nIn the diffusion inference stage, The denoising network \u03f5\u03b8\ntakes as input a text embedding c, a latent code zt and a\ntime embedding, predicts noise, resulting in a latent zt\u22121\nusing the DDIM scheduler [44]:\nzt\u22121 =\nq\n\u03b1t\u22121\n\u03b1t zt + \u221a\u03b1t\u22121\n\u0010q\n1\n\u03b1t\u22121 \u2212 1 \u2212\nq\n1\n\u03b1t \u2212 1\n\u0011\n\u00b7 \u03f5\u03b8(zt, t, c),\n(1)\nwhere \u03b1t is a predefined scalar function at time-step t\n(t = T, ..., 1). The denoising network uses a UNet-based\narchitecture. It consists of an encoder E, a bottleneck B,\nand a decoder D, respectively (Fig. 4b). The hierarchical\nfeatures extracted from the encoder E are injected into the\ndecoder D by a skip connection (Fig. 4a). For convenience\nof description, we divide the UNet network into specific\nblocks: E = {E(\u00b7)s}, B = {B(\u00b7)8}, and D = {D(\u00b7)s},\nwhere s \u2208 {8, 16, 32, 64} (see Fig. 4b). Both E(\u00b7)s 1 and\nD(\u00b7)s represent the block layers with input resolution s in\nboth encoder and decoder, respectively.\n3.2. Analyzing the UNet in Diffusion Model\nIn this section, we delve into the UNet which consists of the\nencoder E, the bottleneck B, and the decoder D, for deeper\nunderstanding the different parts in UNet.\n1Once we replace the \u00b7 with specific inputs in E(\u00b7)s, we define that it\nrepresents the feature of E(\u00b7)s\n(b) UNet architecture\n(d) Decoder propagation\n2\n(c) Encoder propagation\n(e) Non-uniform encoder propagation\n(a) Standard SD sampling\n(f) Parallel encoder propagation\nparallel-batch\nFigure 4. (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. We omit the encoder at certain adjacent time-\nsteps and reuse cyclically the encoder features in the previous time-steps for the decoder. Applying encoder propagation for uniform\nstrategy every two iterations.\nNote, at time-step t-1, predicting noise does not require zt-1 (i.e., Eq. 1: zt\u22122 =\nq \u03b1t\u22122\n\u03b1t\u22121 zt\u22121 +\n\u221a\u03b1t\u22122\n\u0010q\n1\n\u03b1t\u22122 \u2212 1 \u2212\nq\n1\n\u03b1t\u22121 \u2212 1\n\u0011\n\u00b7 \u03f5\u03b8(\u0018\u0018\nXX\nzt\u22121, t \u2212 1, c)). (d) Decoder propagation. The generated images often fail to cover some\nspecific objects in the text prompt. For example, given one prompt case \u201cA man with a beard wearing glasses and a beanie\u201d, this method\nfails to generate the glasses subject. (e) Applying encoder propagation for non-uniform strategy. (f) By benefiting from our propagation\nscheme, we are able to perform in parallel the decoder at certain adjacent time-steps.\nFeature evolving across time-steps.\nWe experimentally\nobserve that the encoder features exhibit a subtle variation\nat adjacent time-steps, whereas the decoder features ex-\nhibit substantial variations across different time-steps (see\nFig. 2a and Fig. 3). Specifically, given a pretrained diffusion\nmodel, we iteratively produce a latent code zt (i.e., Eq. 1),\nand the corresponding hierarchical features: {E(zt, c, t)s},\n{B(zt, c, t)8}, and {D(zt, c, t)s} (s \u2208 {8, 16, 32, 64}) 2, as\nshown in Fig. 4b. We are wondering how change of the hi-\nerarchical features at adjacent time-steps. To achieve this\ngoal, we quantify the variation of the hierarchical features\nas following:\n\u2206E(\u00b7)s =\n1\nd\u00d7s2 \u2225E(zt, c, t)s, E(zt\u22121, c, t \u2212 1)s\u22252\n2,\n(2)\nwhere d represents the number of channels in E(zt, c, t)s.\nSimilarly, we also compute \u2206B(\u00b7)8 and \u2206D(\u00b7)s.\nAs illustrated in Fig. 2a, for both the encoder E and the\ndecoder D, these curves exhibit a similar trend: in the wake\nof an initial increase, the variation reaches a plateau and\n2The feature resolution is half of the previous one in the encoder and\ntwo times in the decoder. Note that the feature resolutions of E(.)8, B(.)8\nand D(.)64 do not change in the SD model.\nthen decreases, followed by continuing growth towards the\nend. However, the extent of change in \u2206E(\u00b7)s and \u2206D(\u00b7)s is\nquantitatively markedly different. For example, the max-\nimum value and variance of the \u2206E(\u00b7)s are less than 0.4\nand 0.05, respectively (Fig. 2a (zoom-in area)), while the\ncorresponding values of the \u2206D(\u00b7)s are about 5 and 30, re-\nspectively (Fig. 2a). Furthermore, we find that \u2206D(\u00b7)64, the\nchange of the last layer of the decoder, is close to zero. This\nis due the output of the denoising network is similar at ad-\njacent time-steps [35]. In conclusion, the overall feature\nchange \u2206E(\u00b7)s is smaller than \u2206D(\u00b7)s throughout the infer-\nence phase.\nFeature evolving across layers.\nWe experimentally ob-\nserve that the feature characteristics are significantly dif-\nferent between the encoder and the decoder across all\ntime-steps. For the encoder E the intensity of the change\nis slight, whereas it is vary drastic for the decoder D.\nSpecifically we calculate the Frobenius norm for hierar-\nchical features E(zt, c, t)s across all time-steps, dubbed as\nFE(\u00b7)s = {FE(zT ,c,T )s, ..., FE(z1,c,1)s}. Similarly, we com-\npute FB(\u00b7)8 and FD(\u00b7)s, respectively.\nFig. 2b shows the feature evolving across layers with\na boxplot 3. Specifically, for\n\b\nFE(\u00b7)s\n\t\nand\n\b\nFB(\u00b7)8\n\t\n, the\nbox is relatively compact, with a narrow range between\ntheir first quartile and third quartile values. For example,\nthe maximum box height (FE(.)32) of these features is less\nthan 5 (see Fig. 2b (zoom-in area)).\nThis indicates that\nthe features from both the encoder E and the bottleneck B\nslightly change. In contrast, the box heights correspond-\ning to {D(\u00b7)s} are relatively large. For example, for the\nD(.)64 the box height is over 150 between the first quar-\ntile and third quartile values (see Fig. 2b). Furthermore, we\nalso provide a standard deviation (Fig. 2c), which exhibits\nsimilar phenomena like Fig. 2b. These results show that\nthe encoder features have relatively small discrepancy, and\na high degree of similarity across all time-steps. However,\nthe decoder features evolve drastically.\nCould we omit Encoder at certain time-steps?\nAs in-\ndicated by the previous experimental analysis, we observe\nthat, during the denoising process, the decoder features\nchange drastically, whereas the encoder E features change\nmildly, and have a high degree similarities at certain adja-\ncent time-steps. Therefore, as shown in Fig. 4c, We propose\nto omit the encoder at certain time-steps and reuse cyclically\nthe encoder features in the previous time-steps for the de-\ncoder. Specifically, we delete the encoder at time-step t \u2212 1\n(t \u2212 1 < T), and the corresponding decoder (including the\nskip connections) takes as input the hierarchical outputs of\nthe encoder E from the previous time-step t, instead of the\nones from the current time-step t \u2212 1 like the standard SD\nsampling (for more detail, see Sec. 3.3).\nWhen omitting the encoder at certain time-step, we are\nable to generate similar images (Fig. 4c) like standard SD\nsampling (Fig. 4a, Tab. 1 (the first and second rows) and ad-\nditional results on Supp.Mat.C). Alternatively if we use the\nsimilar strategy for the decoder (i.e., decoder propagation),\nwe find the generated images often fail to cover some spe-\ncific objects in the text prompt (Fig. 4d). For example, when\nprovided with prompt \u201cA man with a beard wearing glasses\nand a beanie\u201d, the SD model fails to synthesize \u201cglasses\u201d\nwhen applying decoder propagation (Fig. 4d). This is due\nto the fact that the semantics are mainly contained in the\nfeatures from the decoder rather than the encoder [55].\nThe encoder propagation, which uses encoder outputs\nfrom previous time-step as the input to the current decoder,\ncould speed up the diffusion model sampling at inference\ntime. In the following Sec. 3.3, we give a detailed illustra-\ntion about encoder propagation.\n3Each boxplot contains the minimum (0th percentile), the maximum\n(100th percentile), the median (50th percentile), the first quartile (25th per-\ncentile) and the third quartile (75th percentile) values of the feature Frobe-\nnius norm (e.g., {FE(zT ,c,T )s, ..., FE(z1,c,1)s}).\n3.3. Encoder propagation\nDiffusion sampling, combining iterative denoising with\ntransformers, is time-consuming. Therefore we propose a\nnovel and practical diffusion sampling acceleration method.\nDuring the diffusion sampling process t = {T, ..., 1},\nwe refer to the time-steps where encoder propagation is\ndeployed, as non-key time-steps denoted as tnon-key =\nn\ntnon-key\n0\n, ..., tnon-key\nN\u22121\no\n.\nThe remaining time-steps are\ndubbed as tkey\n=\nn\ntkey\n0\n, tkey\n1\n, ..., tkey\nT \u22121\u2212N\no\n.\nIn other\nwords, we do not use the encoder at time-steps tnon-key,\nand leverage the hierarchical features of the encoder from\nthe time-step tkey. Note we utilize the encoder E at the\ninitial time-step (tkey\n0\n= T). Thus, the diffusion inference\ntime-steps could be reformulated as\n\b\ntkey, tnon-key\t\n, where\ntkey \u222atnon-key = {T, ..., 1} and tkey \u2229tnon-key = \u2205. In the\nfollowing subsections, we introduce both uniform encoder\npropagation and non-uniform encoder propagation strate-\ngies.\nAs shown in Fig. 2a, The encoder feature change is larger\nin the initial inference phase compared to the later phases\nthroughout the inference process. Therefore, we select more\nkey time-steps in the initial inference phase, and less key\ntime-steps in later phases. We experimentally define the\nkey time-steps as tkey = {50, 49, 48, 47, 45, 40, 35, 25, 15}\nfor SD model with DDIM, and tkey = {100, 99, 98, . . .,\n92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13, . . ., 2, 1},\n{50, 49, . . . , 2, 1} and {75, 73, 70, 66, 61, 55, 48, 40, 31,\n21, 10} for three stage of DeepFloyd-IF (see detail key time-\nsteps selection on Supp.Mat.C).\nThe remaining time-steps are categorized as non-key\ntime-steps. We define this strategy as non-uniform encoder\npropagation (see Fig. 4e). As shown in Fig. 4c, we also ex-\nplore the time-step selection with fix stride (e.g, 2), dubbed\nas uniform encoder propagation. Tab. 3 reports the results\nof the ablation study, considering various combinations of\nkey and non-key time-steps. These results indicate that the\nset of key time-steps performs better in generating images.\nParallel non-uniform encoder propagation\nWhen ap-\nplying the non-uniform encoder propagation strategy, at\ntime-step t \u2208 tnon\u2212key the decoder inputs do not rely on\nthe encoder outputs at t time-step (see Fig. 4e, right part).\nInstead it relies on the encoder output at the previous near-\nest key time-step. This allows us to perform parallel non-\nuniform encoder propagation at these adjacent time-steps\nin tnon\u2212key. As shown in Fig. 4f, we perform decoding in\nparallel from t \u2212 2 to t \u2212 k + 1 time-steps. This technique\nfurther improves the inference efficiency, since the decoder\nforward in multiple time-steps could be conducted concur-\nrently. We call these non-key time-steps as parallel-batch\nnon-key time-steps. As shown in Fig. 5 (right), we further\nSD (DDIM)\n2.42s\n1.82s 24%\u2193\nEncoder\npropagation\n41%\u2193\n1.42s\nParallel encoder\npropagation\nFigure 5. Comparing with SD (left), encoder propagation reduces\nthe sampling time by 24% (middle). Furthermore, parallel encoder\npropagation achieves a 41% reduction in sampling time (right).\nSD\nOurs w/o\ninjection\n\ud835\udc67\ud835\udc47\nOurs w/\ninjection\n\ud835\udc67\ud835\udc47\nGenerated image\nLow frequency\nHigh frequency\nFigure 6. (left) We keep the image content through zT injection,\nslightly compensating for the texture information loss caused by\nencoder propagation. (right) The amplitudes of the generated im-\nage through zT injection closely resemble those from SD.\nreduce the evaluation time by 41% for the SD model.\nPrior noise injection\nAlthough the encoder propagation\ncould improve the efficiency in the inference phase, we ob-\nserve that it leads to a slight loss of texture information in\nthe generated results (see Fig. 6 (left, middle)). Inspired\nby related works [6, 23], we propose a prior noise injection\nstrategy. It combines the initial latent code zT into the gen-\nerative process at subsequent time-step (i.e., zt), following\nzt = zt+\u03b1\u00b7zT , if t < \u03c4, where \u03b1 = 0.003 is the scale pa-\nrameter to control the impact of zT . And we start to use this\ninjection mechanism from \u03c4 = 25 step. This strategic incor-\nporation successfully improves the texture information. Im-\nportantly, it demands almost negligible extra computational\nresources. We observe that the loss of texture information\noccurs in all frequencies of the frequency domain (see Fig. 6\n(right, red and blue curves)). This approach ensures a close\nresemblance of generated results in the frequency domain\nfrom both SD and zT injection (see Fig. 6 (right, red and\ngreen curves)), with the generated images maintaining the\ndesired fidelity (see Fig. 6 (left, bottom)).\n4. Experiments\nDatasets and evaluation metrics.\nWe randomly se-\nlect 10K prompts from the MS-COCO2017 validation\ndataset [29], and feed them into the diffusion model to ob-\ntain 10K generated images. For other tasks, we use the same\nsettings as baselines (e.g., Text2Video-zero [20], VideoFu-\ns/image \u2193\nDM\nSampling\nMethod\nT FID \u2193 Clip-\nscore \u2191\nGFLOPs/\nimage \u2193\nUnet of\nDM\nDM\nStable Diffusion\nDDIM [44]\n50 21.75 0.773\n37050\n2.23\n2.42\nDDIM [44] w/\nOurs\n50 21.08 0.783 2735027%\u2193 1.2145%\u2193 1.4241%\u2193\nDPM-Solver [30]\n20 21.36 0.780\n14821\n0.90\n1.14\nDPM-Solver [30] w/\nOurs\n20 21.25 0.779 1174321%\u2193 0.4648%\u2193 0.6443%\u2193\nDPM-Solver++ [31]\n20 20.51 0.782\n14821\n0.90\n1.13\nDPM-Solver++ [31] w/\nOurs\n20 20.76 0.781 1174321%\u2193 0.4648%\u2193 0.6443%\u2193\nDDIM + ToMe [3]\n50 22.32 0.782\n35123\n2.07\n2.26\nDDIM + ToMe [3] w/\nOurs\n50 20.73 0.781 2605326%\u2193 1.1544%\u2193 1.3341%\u2193\nDeepFloyd-IF\nDDPM [17]\n225 23.89 0.783\n734825\n33.91\n34.55\nDDPM [17] w/\nOurs\n225 23.73 0.782 62652315%\u2193 25.6125%\u2193 26.2724%\u2193\nDPM-Solver++ [31] 100 20.79 0.784\n370525\n15.19\n16.09\nDPM-Solver++ [31] w/\nOurs\n100 20.85 0.785 31338115%\u2193 12.0221%\u2193 12.9720%\u2193\nTable 1. Quantitative evaluation in both SD model and DeepFloyd-\nIF diffuison model on MS-COCO 2017 10K subset.\nsion [33], Dreambooth [40] and ControlNet [57]). We use\nthe Fr\u00b4echet Inception Distance (FID) [16] metric to assess\nthe visual quality of the generated images, and the Clip-\nscore [15] to measure the consistency between image con-\ntent and text prompt. Furthermore, we report the average\nvalues for both the computational workload (GFLOPs/im-\nage) and sampling time (s/image) to represent the resource\ndemands for a single image. See more detailed implemen-\ntation information on Supp.Mat.A. As shown in Code. 1, in\nthe standard SD sampling code , adding 3 lines of code with\ngreen comments can achieve encoder propagation.\nCode 1. Encoder propagtion for SD (DDIM)\nfrom diffusers import StableDiffusionPipeline\nimport torch\nfrom utils import register_parallel_pipeline,\nregister_faster_forward\n# 1.import package\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe =\nStableDiffusionPipeline.from_pretrained(model_id,\ntorch_dtype=torch.float16)\npipe = pipe.to(\"cuda\")\nregister_parallel_pipeline(pipe) # 2. enable parallel\nregister_faster_forward(pipe.unet) # 3. encoder\npropagation\nprompt = \"a photo of an astronaut riding a horse on\nmars\"\nimage = pipe(prompt).images[0]\nimage.save(\"astronaut_rides_horse.png\")\n4.1. Text-to-image Generation\nWe first evaluate the proposed encoder propagation method\nfor the standard text-to-image generation task on both the\nlatent space (i.e., SD) and pixel space (i.e., DeepFloyd-\nIF) diffusion models.\nAs shown on Tab. 1, we signifi-\ncantly accelerate the diffusion sampling with negligible per-\nformance degradation. Specifically, our proposed method\n50-Step\n40-Step\n30-Step\n20-Step\n15-Step\n10-Step\n5-Step\nDDIM\nDDIM \nw/ Ours\nDPM-\nSolver++\nDPM-\nSolver++ \nw/ Ours\nFigure 7.\nGenerated images at different time-steps, with the\nprompt \u201cA man with a beard wearing sunglasses and a beanie\u201d.\ns/image\u2193\nMethod\nT FID \u2193 Clip-\nscore \u2191\nGFLOPs/\nimage \u2193\nUnet of\nSD\nSD\nText2Video-zero [20]\n50\n-\n0.732\n39670\n12.59/8\n13.65/8\nText2Video-zero [20]\nw/ Ours\n50\n-\n0.731 3069022%\u2193\n9.46/825%\u2193\n10.54/823%\u2193\nVideoFusion [33]\n50\n-\n0.700\n224700\n16.71/16\n17.93/16\nVideoFusion [33]\nw/ Ours\n50\n-\n0.700 14868033%\u2193 11.10/1634%\u2193 12.27/1632%\u2193\nControlNet [57] (edges) 50 13.78 0.769\n49500\n3.09\n3.20\nControlNet [57] (edges)\nw/ Ours\n50 14.65 0.767 3140037%\u2193\n1.4354%\u2193\n1.5251%\u2193\nControlNet [57] (scribble)50 16.17 0.775\n56850\n3.85\n3.95\nControlNet [57] (scribble)\nw/ Ours\n50 16.42 0.775 3599037%\u2193\n1.8353%\u2193\n1.9351%\u2193\nDreambooth [40]\n50\n-\n0.640\n37050\n2.23\n2.42\nDreambooth [40] w/\nOurs\n50\n-\n0.660 2735027%\u2193\n1.2145%\u2193\n1.4241%\u2193\nCustom Diffusion [22] 50\n-\n0.640\n37050\n2.21\n2.42\nCustom Diffusion [22] w/\nOurs\n50\n-\n0.650 2735027%\u2193\n1.2145%\u2193\n1.4241%\u2193\nTable 2.\nQuantitative evaluation on text-to-video, personalized\ngeneration and reference-guided generation tasks.\ndecreases the computational burden (GFLOPs) by a large\nmargin (27%), and greatly reduces sampling time to 41%\nwhen compared to standard DDIM sampling in SD. Sim-\nilarly, in DeepFloyd-IF, the reduction in both computa-\ntional burden and time (s) reaches 15% and 24%, respec-\ntively. Furthermore, our method could be combined with\nthe latest sampling techniques like DPM-Solver [30], DPM-\nSolver++ [31], and ToMe [3]. Our method enhances sam-\npling efficiency while preserving good model performance,\nwith the negligible variations of both FID and Clipscore val-\nues (Tab. 1 (the third to eighth rows)). And our method\nachieves good performance across different sampling steps\n(Fig. 7). Importantly, these results show that our method\nis orthogonal and compatible with these acceleration tech-\nniques. As shown in Fig. 1, we visualize the generated im-\nages with different sampling techniques. Our method still\ngenerates high-quality results (see Supp.Mat.C. for addi-\ntional results).\nFigure 8. User study results.\n4.2. Other tasks with text-guided diffusion model\nBesides the standard text-to-image task, we also validate\nour proposed approach on other tasks: text-to-video gener-\nation, personalized generation and reference-guided image\ngeneration.\ntext-to-video generation, (i.e., Text2Video-zero [20] and\nVideoFusion [33]), personalized generation (i.e., Dream-\nbooth [40]) and reference-guided image generation (i.e.,\nControlNet [57]).\nText-to-video\nTo evaluate our method, we combine\nour method with both Text2Video-zero [20] and Video-\nFusion [33].\nAs reported in Tab. 2 (the first and fourth\nrows), when combined with our method, the two methods\nhave the reduction of approximately 22% to 33% in both\ncomputational burden and generation time (s). These re-\nsults indicate that we are able to enhance the efficiency\nof generative processes in the text-to-video task while pre-\nserving video fidelity at the same time (Fig. 1 (left, bot-\ntom)). As an example, when generating a video using the\nprompt \u201cFireworks bloom in the night sky\u201d, the VideoFu-\nsion model takes 17.92 seconds with 16 frames for the task\n(1.12s/frame), when combining with our method it only\ntakes 12.27s (0.76s/frame) to generate a high-quality video\n(Fig. 1 (left, bottom)).\nPersonalized image generation\nDreambooth [40] and\nCustom Diffusion [22] are two approaches for customiz-\ning tasks by fine-tuning text-to-image diffusion models.\nAs reported in Tab. 2 (the ninth to twelfth rows), our\nmethod, working in conjunction with the two customiza-\ntion approaches, accelerates the image generation and re-\nduces computational demands. Visually, it maintains the\nability to generate images with specific contextual relation-\nships based on reference images. (Fig. 1 (right))\nReference-guided image generation\nControlNet [57]\nincorporates a trainable encoder, successfully generates a\ntext-guided image, and preserves similar content with con-\nditional information. Our approach can be applied concur-\nrently to two encoders of ControNet. In this paper, we val-\nidate the proposed method with two conditional controls:\nedge and scribble. Tab. 2 (the fifth to eighth row) reports\nPropagation\nstrategy\nFID \u2193 Clipscore \u2191\nGFLOPs\n/image \u2193\ns/image \u2193\nFTC \u2193\nUnet of SD\nSD\nSD\n21.75\n0.773\n37050\n2.23\n2.42\n68.1\nUniform\nI\ntkey = {50, 48, 46, 44, 42, 40, 38, 36, 34, 32, 30, 28,\n26, 24, 22, 20, 18, 16, 14, 12, 10, 8, 6, 4, 2}\n22.38\n0.788\n3101116%\u2193 1.6227%\u2193\n1.8125%\u2193\n51.4\nII\ntkey = {50, 44, 38, 32, 26, 20, 14, 8, 2}\n21.54\n0.779\n2735027%\u2193 1.2643%\u2193\n1.4640%\u2193\n40.4\nIII\ntkey = {50, 38, 26, 14, 2}\n24.61\n0.766\n2637029%\u2193 1.1250%\u2193\n1.3644%\u2193\n43.1\nNon-uniform\nI\ntkey = {50, 40, 39, 38, 30, 25, 20, 15, 5}\n22.94\n0.776\n2735027%\u2193 1.2643%\u2193\n1.4241%\u2193\n41.9\nII\ntkey = {50, 30, 25, 20, 15, 14, 5, 4, 3}\n35.25\n0.742\n2735027%\u2193 1.2543%\u2193\n1.4241%\u2193\n67.4\nIII\ntkey = {50, 41, 37, 35, 22, 21, 18, 14, 5}\n22.14\n0.778\n2735027%\u2193 1.2245%\u2193\n1.4241%\u2193\n40.4\nIV\n(Ours)\ntkey = {50, 49, 48, 47, 45, 40, 35, 25, 15}\n21.08\n0.783\n2735027%\u2193 1.2145%\u2193\n1.4241%\u2193\n38.2\nTable 3. Quantitative evaluation in various propagation strategies\non MS-COCO 2017 10K subset. FTC=FID\u00d7Time/Clipscore.\nNon-uniform\n(Ours)\nSD (DDIM)\nUniform\nFigure 9. Generating image with uniform and non-uniform en-\ncoder propagation. The result of uniform strategy II yields smooth\nand loses textual compared with SD. Both uniform strategy III and\nnon-uniform strategy I, II and III generate images with unnatural\nsaturation levels.\nquantitative results. We observe that it leads to a significant\ndecrease in both generation time and computational burden.\nFurthermore, Fig. 1 (middle, bottom) qualitatively shows\nthat our method successfully preserves the given structure\ninformation and achieves similar results as ControlNet.\nUser study\nWe conducted a user study, as depicted in\nFig. 8, and ask subjects to select results. We apply pairwise\ncomparisons (forced choice) with 18 users (35 pairs images\nor videos/user). The results demonstrate that our method\nperforms equally well as the baseline methods.\nSampling Method\nFID \u2193\nClipscore \u2191\nSD (DDIM [17])\n21.75\n0.773\nSD (DDIM [17]) + Ours w/o zT injection\n21.71\n0.779\nSD (DDIM [17]) + Ours w/ zT injection\n21.08\n0.783\nTable 4. Quantitative evaluation for prior noise injection.\n4.3. Ablation study\nWe ablate the results with different selections of both uni-\nform and non-uniform encoder propagation. Tab. 3 reports\nthat the performance of the non-uniform setting largely out-\nperforms the uniform one in terms of both FID and Clip-\nscore (see Tab. 3 (the third and eighth rows)). Furthermore,\nwe explore different configurations within the non-uniform\nstrategy. The strategy, using the set of key time-steps we\nestablished, yields better results in the generation process\n(Tab. 3 (the eighth row)). We further present qualitative re-\nsults stemming from the above choices. As shown in Fig. 9,\ngiven the same number of key time-steps, the appearance\nof nine-step non-uniform strategy I, II and III settings do\nnot align with the prompt \u201cFireflies dot the night sky\u201d. Al-\nthough the generated image in the two-step setting exhibits\na pleasing visual quality, its sampling efficiency is lower\nthan our chosen setting (see Tab. 3 (the second and eighth\nrows)).\nEffectiveness of prior noise injection.\nWe evaluate ef-\nfectiveness of injecting initial zT . As reported in Tab. 4, the\ndifferences in FID and Clipscores without zT (the second\nrow), when compared to DDIM and Ours (the first and third\nrows), are approximately 0.01%, which can be considered\nnegligible. While it is not the case for the visual expression\nof the generated image, it is observed that the output con-\ntains complete semantic information with smoothing tex-\nture (refer to Fig. 6 (left, the second row)). Injecting the zT\naids in maintaining fidelity in the generated results during\nencoding propagation (see Fig. 6 (left, the third row) and\nFig. 6 (right, red and green curves)).\n5. Conclusion\nIn this work, We explore the characteristics of the encoder\nand decoder in UNet of the text-to-image diffusion model\nand find that the encoder is negligible at many time-steps,\nwhile the decoder plays a significant role across all time-\nsteps. Building upon this finding, we propose encoder prop-\nagation for efficient diffusion sampling, reducing time on\nboth Stable Diffusion and DeepFloyd-IF on diverse set of\ngeneration tasks. We conduct extensive experiments and\nvalidate that our approach can achieve improved sampling\nefficiency while maintaining image quality. Limitations:\nAlthough our approach achieves efficient diffusion sam-\npling, it faces challenges in generating quality when using\na limited number of sampling steps (e.g., 5).\nReferences\n[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 2\n[2] Dmitry\nBaranchuk,\nIvan\nRubachev,\nAndrey\nVoynov,\nValentin Khrulkov, and Artem Babenko. Label-efficient se-\nmantic segmentation with diffusion models. arXiv preprint\narXiv:2112.03126, 2021. 2\n[3] Daniel Bolya and Judy Hoffman. Token merging for fast sta-\nble diffusion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4598\u2013\n4602, 2023. 3, 6, 7, 1\n[4] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao\nZhang, Christoph Feichtenhofer, and Judy Hoffman.\nTo-\nken merging:\nYour vit but faster.\narXiv preprint\narXiv:2210.09461, 2022. 3\n[5] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-\nfusiondet: Diffusion model for object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 19830\u201319843, 2023. 2\n[6] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon\nKim, Hyunwoo Kim, and Sungroh Yoon.\nPerception pri-\noritized training of diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11472\u201311481, 2022. 6\n[7] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye.\nCome-closer-diffuse-faster: Accelerating conditional diffu-\nsion models for inverse problems through stochastic contrac-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12413\u201312422,\n2022. 2\n[8] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 2\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[10] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arxiv:2307.10373, 2023. 2\n[11] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong,\nQing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, Hyun-\nJoon Jung, and Xin Eric Wang. Photoswap: Personalized\nsubject swapping in images, 2023. 2\n[12] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 2\n[13] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong\nYang, Andriy Myronenko, Bennett Landman, Holger R\nRoth, and Daguang Xu. Unetr: Transformers for 3d med-\nical image segmentation. In Proceedings of the IEEE/CVF\nwinter conference on applications of computer vision, pages\n574\u2013584, 2022. 2\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2, 4, 11\n[15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 6\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 6, 8, 3, 7\n[18] Ziqi Huang, Tianxing Wu, Yuming Jiang, Kelvin C.K. Chan,\nand Ziwei Liu. ReVersion: Diffusion-based relation inver-\nsion from images. arXiv preprint arXiv:2303.13495, 2023.\n4, 11\n[19] Debesh Jha, Pia H Smedsrud, Michael A Riegler, Dag Jo-\nhansen, Thomas De Lange, P\u02daal Halvorsen, and H\u02daavard D\nJohansen. Resunet++: An advanced architecture for medical\nimage segmentation. In 2019 IEEE international symposium\non multimedia (ISM), pages 225\u20132255. IEEE, 2019. 2\n[20] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 6, 7, 1, 3, 4, 8\n[21] Bo-Kyeong Kim, Hyoung-Kyu Song, Thibault Castells, and\nShinkook Choi.\nOn architectural compression of text-to-\nimage diffusion models. arXiv preprint arXiv:2305.15798,\n2023. 3\n[22] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2, 7, 1, 3, 4, 10\n[23] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space. arXiv preprint\narXiv:2210.10960, 2022. 6\n[24] Van-Linh Le and Olivier Saut. Rrc-unet 3d for lung tumor\nsegmentation from ct scans of non-small cell lung cancer pa-\ntients. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 2316\u20132325, 2023. 2\n[25] Senmao Li, Joost van de Weijer, Taihang Hu, Fahad Shahbaz\nKhan, Qibin Hou, Yaxing Wang, and Jian Yang. Styledif-\nfusion: Prompt-embedding inversion for text-based editing.\narXiv preprint arXiv:2303.15649, 2023. 2\n[26] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 22511\u201322521, 2023. 2\n[27] Yanyu Li, Huan Wang, Qing Jin, Ju Hu, Pavlo Chemerys,\nYun Fu, Yanzhi Wang, Sergey Tulyakov, and Jian Ren. Snap-\nfusion: Text-to-image diffusion model on mobile devices\nwithin two seconds. arXiv preprint arXiv:2306.00980, 2023.\n3\n[28] Junjie Liang, Cihui Yang, Jingting Zhong, and Xiaoli Ye.\nBtswin-unet: 3d u-shaped symmetrical swin transformer-\nbased network for brain tumor segmentation with self-\nsupervised pre-training.\nNeural processing letters, 55(4):\n3695\u20133713, 2023. 2\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6, 1\n[30] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. Advances\nin Neural Information Processing Systems, 35:5775\u20135787,\n2022. 3, 6, 7, 1\n[31] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 6, 7, 1, 3\n[32] Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon:\nDiffusion-based training-free cross-domain image composi-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 2294\u20132305, 2023. 2\n[33] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and\nTieniu Tan.\nVideofusion:\nDecomposed diffusion mod-\nels for high-quality video generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10209\u201310218, 2023. 2, 6, 7, 1, 3, 4, 9\n[34] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 14297\u201314306, 2023. 3\n[35] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki\nTanaka. Negative-prompt inversion: Fast image inversion\nfor editing with text-guided diffusion models. arXiv preprint\narXiv:2305.16807, 2023. 4\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[37] Walter HL Pinaya, Mark S Graham, Robert Gray, Pedro F\nDa Costa, Petru-Daniel Tudosiu, Paul Wright, Yee H Mah,\nAndrew D MacKinnon, James T Teo, Rolf Jager, et al. Fast\nunsupervised brain anomaly detection and segmentation with\ndiffusion models.\nIn International Conference on Medi-\ncal Image Computing and Computer-Assisted Intervention,\npages 705\u2013714. Springer, 2022. 2\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2\n[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234\u2013241. Springer, 2015. 2\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 6, 7, 1, 3, 4, 10\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[42] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 3\n[43] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-\nsion models for interactive point-based image editing. arXiv\npreprint arXiv:2306.14435, 2023. 2\n[44] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3, 6, 4\n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[46] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 2\n[47] Dmitrii Torbunov, Yi Huang, Haiwang Yu, Jin Huang, Shin-\njae Yoo, Meifeng Lin, Brett Viren, and Yihui Ren.\nUvc-\ngan: Unet vision transformer cycle-consistent gan for un-\npaired image-to-image translation.\nIn Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 702\u2013712, 2023. 2\n[48] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 2, 3\n[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[50] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK\nChan, and Chen Change Loy.\nExploiting diffusion prior\nfor real-world image super-resolution.\narXiv preprint\narXiv:2305.07015, 2023. 2\n[51] Julia Wolleb, Robin Sandk\u00a8uhler, Florentin Bieder, Philippe\nValmaggia, and Philippe C Cattin.\nDiffusion models for\nimplicit image segmentation ensembles.\nIn International\nConference on Medical Imaging with Deep Learning, pages\n1336\u20131348. PMLR, 2022. 2\n[52] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2\n[53] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin\nCui, and Ming-Hsuan Yang. Diffusion models: A compre-\nhensive survey of methods and applications. arXiv preprint\narXiv:2209.00796, 2022. 2\n[54] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 2\n[55] Guanqi Zhan, Chuanxia Zheng, Weidi Xie, and Andrew Zis-\nserman. What does stable diffusion know about the 3d scene?\narXiv preprint arXiv:2310.06836, 2023. 2, 5\n[56] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-\nnia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan\nYang. A tale of two features: Stable diffusion complements\ndino for zero-shot semantic correspondence. arXiv preprint\narXiv:2305.15347, 2023.\n[57] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 6, 7, 1, 3, 4, 10\n[58] Yuxin Zhang, Weiming Dong, Fan Tang, Nisha Huang,\nHaibin Huang, Chongyang Ma, Tong-Yee Lee, Oliver\nDeussen, and Changsheng Xu. Prospect: Expanded condi-\ntioning for the personalization of attribute-aware image gen-\neration. arXiv preprint arXiv:2305.16225, 2023. 2\nSupplementary Material\nIn the supplemental material, we provide a detailed descrip-\ntion of the experimental implementation (Sec. A). Subse-\nquently, an analysis of the parameter quantities for the en-\ncoder and decoder is conducted (Sec. B). Following that,\nwe present additional experiments and results, including\nmore detailed ablation studies and comparative experiments\n(Sec. C).\nA. Implementation Details\nA.1. Configure\nWe use the Stable Diffuison v1.5 pre-trained model 4 and\nDeepFloyd-IF 5. All of our experiments are conducted using\nan A40 GPU (48GB of VRAM).\nWe randomly select 100 captions from the MS-COCO\n2017 validation dataset [29] as prompts for generating im-\nages. The analytical results presented in Sec. 3 are based\non the statistical outcomes derived from these 100 generated\nimages.\nA.2. Details about the layers in the UNet\nThe UNet in Stable Diffusion (SD) consists of an encoder\nE, a bottleneck B, and a decoder D, respectively. We divide\nthe UNet into specific blocks: E = {E(\u00b7)s}, B = {B(\u00b7)8},\nand D = {D(\u00b7)s}, where s \u2208 {8, 16, 32, 64}. E(\u00b7)s and\nD(\u00b7)s represent the block layers with input resolution s in\nthe encoder and decoder, respectively. Tab. 5 presents de-\ntailed information about the block architecture.\nA.3. Time and memory consumption ratios\nWe report the run time and GPU memory consumption ra-\ntios for text-to-image task. As shown in Tab. 6, we signifi-\ncantly accelerate the diffusion sampling with encoder prop-\nagation while maintaining a comparable memory demand to\nthe baselines (Tab. 6 (the last two columns)). Specifically,\nour proposed method reduces the spending time (s/image)\nby 24% and require a little additional memory compared to\nstandard DDIM sampling in SD (DDIM vs. Ours: 2.62GB\nvs. 2.64GB). The increased GPU memory requirement is\nfor caching the features of the encoder from the previous\ntime-step. Though applying parallel encoder propagation\nresults in an increase in memory requirements by 51%, it\nleads to a more remarkable acceleration of 41% (DDIM vs.\nOurs: 2.62GB vs. 3.95GB). In conclusion, applying en-\ncoder propagation reduces the sampling time, accompanied\nby a negligible increase in memory requirements. Parallel\nencoder propagation on text-to-image tasks yields a sam-\npling speed improvement of 20% to 43%, requiring an ad-\nditional acceptable amount of memory.\n4https : / / huggingface . co / runwayml / stable -\ndiffusion-v1-5\n5https://github.com/deep-floyd/IF\nBesides the standard text-to-image task, we also validate\nour proposed approach on other tasks: text-to-video gen-\neration (i.e., Text2Video-zero [20] and VideoFusion [33]),\npersonalized generation (i.e., Dreambooth [40] and Custom\nDiffusion [22]) and reference-guided image generation (i.e.,\nControlNet [57]). We present the time and memory con-\nsumption ratios for these tasks in Tab. 7.\nAs reported in Tab. 7 (top), when combined with our\nmethod, there is a reduction in sampling time by 23%\nand 32% for Text2Video-zero [20] and VideoFusion [33],\nrespectively, while the memory requirements increased\nslightly by 3% (0.2GB) and 0.9% (0.11GB). The time\nspent by reference-guided image generation (i.e., Control-\nNet [57]) is reduced by more than 20% with a negligible in-\ncrease in memory (1%). When integrated with our parallel\nencoder propagation, the sampling time in this task can be\nreduced by more than half (51%) (Tab. 7 (middle)). Dream-\nbooth [40] and Custom Diffusion [22] are two approaches\nfor customizing tasks by fine-tuning text-to-image diffusion\nmodels. As reported in Tab. 7 (bottom), our method, work-\ning in conjunction with the two customization approaches,\naccelerates the image generation with an acceptable in-\ncrease in memory.\nNote that our method, either utilizing encoder prop-\nagation or parallel encoder propagation, improves sam-\npling speed without compromising image quality (Sec.\n4).\nWe conducted GPU memory consumption ra-\ntios using the official method provided by PyTorch,\ntorch.cuda.max memory allocated\n6\n,\nwhich\nrecords the peak allocated memory since the start of the pro-\ngram.\nA.4. GFLOPs\nWe use the fvcore7 library to calculate the GFLOPs required\nfor a single forward pass of the diffusion model. Multiply-\ning this by the number of sampling steps gives us the total\ncomputational burden for sampling one image. Addition-\nally, using fvcore, we can determine the computational load\nrequired for each layer of the diffusion model. Based on the\nkey time-steps set in our experiments, we subtract the com-\nputation we save from the original total computational load,\nwhich then represents the GFLOPs required by our method.\nSimilarly, fvcore also supports parameter count statistics.\nA.5. Baseline Implementations\nFor the comparisons of text-to-image generation in Sec. 4,\nwe use the official implementation of DPM-Solver [30],\nDPM-Solver++ [31] 8, and ToMe [3] 9. For the other tasks\n6https : / / pytorch . org / docs / stable / generated /\ntorch.cuda.max_memory_allocated.html\n7https://github.com/facebookresearch/fvcore\n8https://github.com/LuChengTHU/dpm-solver\n9https://github.com/dbolya/tomesd\nUNet\nLayer\nnumber\nType of layer\nLayer name\nInput resolution\nof layer\nOutput resolution\nof layer\nE\nE(\u00b7)64\n0\nresnets\ndown blocks.0.resnets.0\n(320, 64, 64)\n(320, 64, 64)\n1\nattention\ndown blocks.0.attentions.0\n(320, 64, 64)\n(320, 64, 64)\n2\nresnet\ndown blocks.0.resnets.1\n(320, 64, 64)\n(320, 64, 64)\n3\nattention\ndown blocks.0.attentions.1\n(320, 64, 64)\n(320, 64, 64)\n4\ndownsamplers down blocks.0.downsamplers.0\n(320, 64, 64)\n(320, 32, 32)\nE(\u00b7)32\n5\nresnet\ndown blocks.1.resnets.0\n(320, 32, 32)\n(640, 32, 32)\n6\nattention\ndown blocks.1.attentions.0\n(640, 32, 32)\n(640, 32, 32)\n7\nresnet\ndown blocks.1.resnets.1\n(640, 32, 32)\n(640, 32, 32)\n8\nattention\ndown blocks.1.attentions.1\n(640, 32, 32)\n(640, 32, 32)\n9\ndownsamplers down blocks.1.downsamplers.0\n(640, 32, 32)\n(640, 16, 16)\nE(\u00b7)16\n10\nresnet\ndown blocks.2.resnets.0\n(640, 16, 16)\n(1280, 16, 16)\n11\nattention\ndown blocks.2.attentions.0\n(1280, 16, 16)\n(1280, 16, 16)\n12\nresnet\ndown blocks.2.resnets.1\n(1280, 16, 16)\n(1280, 16, 16)\n13\nattention\ndown blocks.2.attentions.1\n(1280, 16, 16)\n(1280, 16, 16)\n14\ndownsamplers down blocks.2.downsamplers.0\n(1280, 16, 16)\n(1280, 8, 8)\nE(\u00b7)8\n15\nresnet\ndown blocks.3.resnets.0\n(1280, 8, 8)\n(1280, 8, 8)\n16\nresnet\ndown blocks.3.resnets.1\n(1280, 8, 8)\n(1280, 8, 8)\nB\nB(\u00b7)8\n17\nresnet\nmid blocks.resnets.0\n(1280, 8, 8)\n(1280, 8, 8)\n18\nattention\nmid blocks.attentions.0\n(1280, 8, 8)\n(1280, 8, 8)\n19\nresnet\nmid blocks.resnets.1\n(1280, 8, 8)\n(1280, 8, 8)\nD\nD(\u00b7)8\n20\nresnet\nup blocks.0.resnets.0\n(1280, 8, 8)\n(1280, 8, 8)\n21\nresnet\nup blocks.0.resnets.1\n(1280, 8, 8)\n(1280, 8, 8)\n22\nresnet\nup blocks.0.resnets.2\n(1280, 8, 8)\n(1280, 8, 8)\n23\nupsamplers\nup blocks.0.upsamplers.0\n(1280, 8, 8)\n(1280, 16, 16)\nD(\u00b7)16\n24\nresnet\nup blocks.1.resnets.0\n(1280+1280, 16, 16)\n(1280, 16, 16)\n25\nattention\nup blocks.1.attentions.0\n(1280, 16, 16)\n(1280, 16, 16)\n26\nresnet\nup blocks.1.resnets.1\n(1280+1280, 16, 16)\n(1280, 16, 16)\n27\nattention\nup blocks.1.attentions.1\n(1280, 16, 16)\n(1280, 16, 16)\n28\nresnet\nup blocks.1.resnets.2\n(1280+640, 16, 16)\n(1280, 16, 16)\n29\nattention\nup blocks.1.attentions.2\n(1280, 16, 16)\n(1280, 16, 16)\n30\nupsamplers\nup blocks.1.upsamplers.0\n(1280, 16, 16)\n(1280, 32, 32)\nD(\u00b7)32\n31\nresnet\nup blocks.2.resnets.0\n(1280+640, 32, 32)\n(640, 32, 32)\n32\nattention\nup blocks.2.attentions.0\n(640, 32, 32)\n(640, 32, 32)\n33\nresnet\nup blocks.2.resnets.1\n(640+640, 32, 32)\n(640, 32, 32)\n34\nattention\nup blocks.2.attentions.1\n(640, 32, 32)\n(640, 32, 32)\n35\nresnet\nup blocks.2.resnets.2\n(640+320, 32, 32)\n(640, 32, 32)\n36\nattention\nup blocks.2.attentions.2\n(640, 32, 32)\n(640, 32, 32)\n37\nupsamplers\nup blocks.2.upsamplers.0\n(640, 32, 32)\n(640, 64, 64)\nD(\u00b7)64\n38\nresnet\nup blocks.3.resnets.0\n(640+320, 64, 64)\n(320, 64, 64)\n39\nattention\nup blocks.3.attentions.0\n(320, 64, 64)\n(320, 64, 64)\n40\nresnet\nup blocks.3.resnets.1\n(320+320, 64, 64)\n(320, 64, 64)\n41\nattention\nup blocks.3.attentions.1\n(320, 64, 64)\n(320, 64, 64)\n42\nresnet\nup blocks.3.resnets.2\n(320+320, 64, 64)\n(320, 64, 64)\n43\nattention\nup blocks.3.attentions.2\n(320, 64, 64)\n(320, 64, 64)\nTable 5. Detailed information about the layers of the encoder E, bottleneck B and decoder D in the UNet.\nwith text-guided diffusion model, we use the official imple-\nmentation of Text2Video-zero [20] 10, VideoFusion [33] 11,\n10https : / / github . com / Picsart - AI - Research /\nText2Video-Zero\nDM\nSampling Method\nT\ns/image\nmemory (GB)\nStable Diffusion\nDDIM [44]\n50\n2.42\n2.62\nDDIM [44] w/ Ours\n\u2020\n1.8224%\u2193\n2.64\n\u2021\n1.4241%\u2193\n3.95\nDPM-Solver [30]\n20\n1.14\n2.62\nDPM-Solver [30] w/ Ours\n\u2020\n0.9219%\u2193\n2.64\n\u2021\n0.6443%\u2193\n2.69\nDPM-Solver++ [31]\n20\n1.13\n2.61\nDPM-Solver++ [31] w/ Ours\n\u2020\n0.9119%\u2193\n2.65\n\u2021\n0.6443%\u2193\n2.68\nDDIM + ToMe [3]\n50\n2.26\n2.62\nDDIM + ToMe [3] w/ Ours\n\u2020\n1.7224%\u2193\n2.64\n\u2021\n1.3341%\u2193\n3.95\nDeepFloyd-IF\nDDPM [17]\n225\n34.55\n40.5\nDDPM [17] w/ Ours\n\u2020\n29.4515%\u2193\n41.1\n\u2021\n26.2724%\u2193\n41.1\nDPM-Solver++ [31]\n100\n16.09\n40.5\nDPM-Solver++ [31] w/ Ours\n\u2020\n14.1312%\u2193\n40.8\n\u2021\n12.9720%\u2193\n40.8\nTable 6. Time and GPU memory consumption ratios in both SD\nmodel and DeepFloyd-IF diffuison model. \u2020: Encoder propaga-\ntion, \u2021: Parallel encoder propagation.\nSampling Method\nT\ns/image\nmemory (GB)\nText2Video-zero [20]\n50\n13.65\n6.59\nText2Video-zero [20] w/ Ours\n\u2020\n10.5423%\u2193\n6.79\n\u2021\n\u2013\n\u2013\nVideoFusion [33]\n50\n17.93\n11.87\nVideoFusion [33] w/ Ours\n\u2020\n12.2732%\u2193\n11.98\n\u2021\n\u2013\n\u2013\nControlNet [57] (edges)\n50\n3.20\n3.81\nControlNet [57] (edges) w/\nOurs\n\u2020\n2.0137%\u2193\n3.85\n\u2021\n1.5251%\u2193\n5.09\nControlNet [57] (scribble)\n50\n3.95\n3.53\nControlNet [57] (scribble) w/ Ours\n\u2020\n3.1820%\u2193\n3.57\n\u2021\n1.9351%\u2193\n4.45\nDreambooth [40]\n50\n2.42\n2.61\nDreambooth [40] w/ Ours\n\u2020\n1.8124%\u2193\n2.65\n\u2021\n1.4241%\u2193\n3.93\nCustom Diffusion [22]\n50\n2.42\n2.61\nCustom Diffusion [22] w/ Ours\n\u2020\n1.8224%\u2193\n2.64\n\u2021\n1.4241%\u2193\n3.94\nTable 7.\nTime and GPU memory consumption ratios in text-\nto-video, personalized generation and reference-guided generated\ntasks. \u2020: Encoder propagation, \u2021: Parallel encoder propagation.\nControlNet [57] 12, Dreamboth [40] 13, and Custom Diffu-\nsion [22] 14. We maintain the original implementations of\n11https : / / huggingface . co / docs / diffusers / api /\npipelines/text_to_video\n12https://github.com/lllyasviel/ControlNet\n13https://github.com/google/dreambooth\n14https : / / github . com / adobe - research / custom -\ndiffusion\nParameter (billion)\nFLOPs (million)\nE + B\n0.25 + 0.097\n224.2+6.04\nD\n0.521.47\u00d7\n504.42.2\u00d7\nTable 8. Model complexity comparison regarding the encoder E,\nthe bottleneck B and the decoder D in terms of parameter count\nand FLOPs.\nthese baselines and directly integrate code into their existing\nimplementations to implement our method.\nB. Parameter Count and FLOPs of SD\nWe take into account model complexity in terms of param-\neter count and FLOPs (see Tab. 8). It\u2019s noteworthy that\nthe decoder D exhibits a significantly greater parameter\ncount, totaling 0.51 billion. This figure is approximately\n1.47 times the number of parameter combinations for the\nencoder E (250 million) and the bottleneck B (97 million).\nThis substantial parameter discrepancy suggests that the de-\ncoder D carries a more substantial load in terms of model\ncomplexity.\nFurthermore, when we consider the computational load,\nduring a single forward inference pass of the SD model,\nthe decoder D incurs a considerable 504.4 million FLOPs.\nThis value is notably higher, approximately 2.2 times,\nthan the cumulative computational load of the encoder E,\nwhich amounts to 224.2 million FLOPs, and the bottle-\nneck B, which require 6.04 million FLOPs. This observa-\ntion strongly implies that the decoder D plays a relatively\nmore important role in processing and transforming the data\nwithin the UNet architecture, emphasizing its critical part to\nthe overall functionality of the model.\nC. Ablation experiments and additional results\nC.1. The definition of key time-steps in various tasks\nWe utilize 50, 20 and 20 time-steps for DDIM sched-\nuler [44], DPM-solver [30] and DPM-solver++ [31], re-\nspectively.\nWe follow the official implementation of\nDeepFloyd-IF, setting the time-steps for the three sampling\nstages to 100, 50, and 75, respectively.\nText-to-image generation.\nWe experimentally define\nthe key time-steps as tkey = {50, 49, 48, 47, 45, 40, 35,\n25, 15} for SD model with DDIM [44], and tkey = {100,\n99, 98, . . ., 92, 91, 90, 85, 80, . . ., 25, 20, 15, 14, 13,\n. . ., 2, 1}, {50, 49, . . . , 2, 1} and {75, 73, 70, 66, 61, 55,\n48, 40, 31, 21, 10} for three stages of DeepFloyd-IF with\nDDPM [17]. For SD with both DPM-Solver [30] and DPM-\nSolver++ [31], we experimentally set the key time-steps to\ntkey = {20, 19, 18, 17, 15, 10, 5}.\nOther tasks with text-guided diffusion model. In addi-\ntion to standard text-to-image tasks, we further validate our\napproach on other tasks with text-guided diffusion model\n(Sec. 4.2). These tasks are all based on the SD imple-\nmentation of DDIM [44]. Through experiments, we set the\nkey time-steps to tkey = {50, 49, 48, 47, 45, 40, 35, 25,\n15} for Text2Video-zero [20], VideoFusion [33], and Con-\ntrolNet [57]. For personalized tasks (i.e., Dreambooth [40]\nand Custom Diffusion [22]), we set the key time steps to\ntkey = {50, 49, 48, 47, 45, 40, 35, 25, 15, 10}.\nC.2. The effectiveness of encoder propagation\nIn Sec 3.2, we have demonstrated that encoder propaga-\ntion (Fig. 10c) can preserve semantic consistency with stan-\ndard SD sampling (Fig. 10a). However, images generated\nthrough decoder propagation (Fig. 10d) often fail to match\ncertain specific objects mentioned in the text prompt.\nWe extended this strategy to include encoder and de-\ncoder propagation (Fig. 10e) as well as decoder and encoder\ndropping (Fig. 10f). Similarly, encoder and decoder propa-\ngation often fail to cover specific objects mentioned in the\ntext prompt, leading to a degradation in the quality of the\ngenerated results (Fig. 10e). On the other hand, decoder and\nencoder dropping is unable to completely denoise, resulting\nin the generation of images with noise (Fig. 10f).\nC.3. Additional results\nIn Figs. 11, 12 and 13, we show additional text-to-image\ngeneration results. Further results regarding other tasks with\ntext-guided diffusion model are illustrated in Figs. 14, 15\nand 16.\nC.4. Additional tasks\nReVersion [18] is a relation inversion method that relies on\nthe Stable Diffusion (SD). Our method retains the capacity\nto generate images with specific relations based on exem-\nplar images, as illustrated in Fig. 17 (top).\nP2P [14] is an image editing method guided solely by\ntext. When combined with our approach, it enhances sam-\npling efficiency while preserving the editing effect (Fig. 17\n(bottom)).\n\ud835\udc67\ud835\udc61\n(e) Encoder and Decoder propagation\n\ud835\udc61\n\ud835\udc61 \u2212 1\n\ud835\udc67\ud835\udc61\u22121\n\ud835\udc67\ud835\udc61\u22122\n(f) Decoder and Encoder dropping\n\ud835\udc67\n\ud835\udc61\n\ud835\udc61 \u2212 1\n\ud835\udc67\ud835\udc61\u22122\n\ud835\udc61\n( )c Encoder propagation\n\ud835\udc61\n\ud835\udc61 \u2212 1\n\ud835\udc67\ud835\udc61\n\ud835\udc67\ud835\udc61\u22121\n\ud835\udc67\ud835\udc61\u22122\n( )\nd Decoder propagation\n\ud835\udc61\n\ud835\udc61 \u2212 1\n\ud835\udc67\ud835\udc61\n\ud835\udc67\ud835\udc61\u22121\n\ud835\udc67\ud835\udc61\u22122\n(b) UNet architecture\n\ud835\udc67\ud835\udc61\n\ud835\udc67\ud835\udc61\u22121\n\ud835\udc67\ud835\udc61\u22122\n\ud835\udc61\n\ud835\udc61 \u2212 1\n(a) Standard SD sampling\nFigure 10. (a) Standard SD sampling. (b) UNet architecture. (c) Encoder propagation. (d) Decoder propagation. (e) Encoder and Decoder\npropagation. (f) Decoder and Encoder dropping.\nDDIM\nDDIM w/ Ours\nA dew-covered\nspider web\nin morning sunlight\nA snow-covered\npine tree\nin a moonlit forest\ncolorful autumn leaves\nscattered on\na stone pathway\nA cat wearing\na sunglasses\nAn astronaut riding\na horse on Mars\nA dog sitting\nin a beach\nA vase sitting on\ntop of a table\nwith flowers in it\na banana and\na chocolate frosted\ndonut sitting in a baggie\nDPM-Solver w/ Ours\nA plate full of sushi\nwith more behind it\nA man with glasses\nsitting in front of\na laptop computer\nA passenger bus\npulling up to\nthe side of a street\nA bowl full of fresh\ngreen apples are kept\nDPM-Solver\nA lone sailboat\ndrifting on calm waters\nFrench bread on a plate\nwith eggs bacon\nDPM-Solver++\nDPM-Solver++ w/ Ours\nPlate containing bread\ncovered in some\ncooked broccoli\nYoshua Bengio\nwith beard\nA black and white cat\nrelaxing inside a laptop\nA floral centerpiece\nin the dining room\ntable setting\nFigure 11. Additional results of text-to-image generation combining SD with DDIM [44], DPM-Solver [30], DPM-Solver++ [31], and\nthese methods in conjunction with our proposed approach.\nDDIM+ToMe\nDDIM+ToMe w/ Ours\nBurger and fries\nA elephant\nunder tree\nA woman with\nglasses standing on\nthe street\nWaterfall\nsurrounded with trees\nA blue bird\nperched on top of\na tree branch\nA frog sitting on\ngreen field\nFigure 12. Additional results of text-to-image generation combining SD with DDIM+ToMe [3], and this method in conjunction with our\nproposed approach.\nA kangaroo holding\na sign that says\n\"very deep learning\"\nA pair of hands\nkneading bread dough\nDDPM w/ Ours\nA sunbeam streaming\nthrough a dense\nforest canopy\nA cozy fireplace\nwith crackling flames\nA stack of vinyl\nrecords on a\nvintage turntable\nA pair of\nreading glasses\non an open book\nDDPM\nA busy highway\nis being viewed\nfrom a distance.\nA black and yellow\nfire hydrant\non a city street\nDPM-Solver++\nDPM-Solver++ w/ Ours\nA airplane coming in\nfor a landing with\na full moon above it\n A bench sitting\non to of a field of\ntall grass near water\nVintage bicycle\nleaning against\na brick wall\nA big purple\npublic bus\ncalled south tyne\nFigure 13. Additional results of text-to-image generation combining DeepFloyd-IF with DDPM [17] and DPM-Solver++ [31], and these\nmethods in conjunction with our proposed approach.\nThe sun is setting behind the mountains\nText2Video-zero\nText2Video-zero\nw/ Ours\nA pot of water is boiling on the stove\nText2Video-zero\nText2Video-zero\nw/ Ours\nA panda is playing guitar on times square\nText2Video-zero\nText2Video-zero\nw/ Ours\nThe tide is coming in on the sandy beach\nText2Video-zero\nText2Video-zero\nw/ Ours\nFigure 14. Additional results of Text2Video-zero [20] both independently and when combined with our proposed method.\nFireworks bloom in the night sky\nVideoFusion\nVideoFusion w/ Ours\nThe tide is coming in on the sandy beach\nVideoFusion\nText2Video-zero w/ Ours\nFigure 15. Additional results of VideoFusion [33] both independently and when combined with our proposed method.\nCustom Diffusion w/ Ours\nA chair in the middle of a garden\nA chair next to a cozy fireplace\nA motorbike by the ocean\nA racer on a motorbike at a competition\nCustom Diffusion\nDreambooth w/ Ours\nDreambooth\nControlNet\nCondition\nControlNet w/ Ours\nFigure 16. Additional results of ControlNet [57] (top) and personalized tasks [22, 40] (bottom) obtained both independently and in\nconjunction with our proposed method..\nZoom photo of trees\nZoom photo of flowers\nP2P\nP2P w/ Ours\nA painting of a squirrel\neating a burger\nA cat\nsitting on a grassland\nA tiger\nsitting on a grassland\nA painting of a lion\neating a burger\nReVersion w/ Ours\ncat <R> stone\n<R>=\"painted on\"\ncat <R> cat\n<R>=\"back to back\"\nReVersion\nspiderman <R> stone\n<R>=\"painted on\"\nFigure 17. Edited results of ReVersion [18] (top) and P2P [14] (bottom) obtained both independently and in conjunction with our proposed\nmethod.\n"
  },
  {
    "title": "Self-Evaluation Improves Selective Generation in Large Language Models",
    "link": "https://arxiv.org/pdf/2312.09300.pdf",
    "upvote": "13",
    "text": "Self-Evaluation Improves Selective Generation in\nLarge Language Models\nJie Ren\u2217, Yao Zhao\u2217, Tu Vu\u2020, Peter J. Liu\u2217, Balaji Lakshminarayanan\u2217\n{jjren,yaozhaoyz,ttvu,peterjliu,balajiln}@google.com\nGoogle DeepMind\u2217, Google Research\u2020\nAbstract\nSafe deployment of large language models (LLMs) may benefit from a reliable\nmethod for assessing their generated content to determine when to abstain or to\nselectively generate. While likelihood-based metrics such as perplexity are widely\nemployed, recent research has demonstrated the limitations of using sequence-\nlevel probability estimates given by LLMs as reliable indicators of generation\nquality. Conversely, LLMs have demonstrated strong calibration at the token\nlevel, particularly when it comes to choosing correct answers in multiple-choice\nquestions or evaluating true/false statements. In this work, we reformulate open-\nended generation tasks into token-level prediction tasks, and leverage LLMs\u2019\nsuperior calibration at the token level. We instruct an LLM to self-evaluate its\nanswers, employing either a multi-way comparison or a point-wise evaluation\napproach, with the option to include a \u201cNone of the above\u201d option to express the\nmodel\u2019s uncertainty explicitly. We benchmark a range of scoring methods based\non self-evaluation and evaluate their performance in selective generation using\nTRUTHFULQA and TL;DR. Through experiments with PALM-2 and GPT-3, we\ndemonstrate that self-evaluation based scores not only improve accuracy, but also\ncorrelate better with the overall quality of generated content.\n1\nIntroduction\nLarge language models (LLMs) are often pre-trained on a vast corpus of text and then fine-tuned\non supervised data to follow instructions [Devlin et al., 2018, Radford et al., 2018, Raffel et al.,\n2020, Adiwardana et al., 2020, Wei et al., 2021, Ouyang et al., 2022, Chung et al., 2022]. Having\nthe ability to tell when a language model\u2019s output is trustworthy is important for safe deployment\nof language models. For example, the model\u2019s trustworthiness can be used as signal to selectively\ngenerate answers based on how confident the LLM is in the quality of its output.\nPrior research has demonstrated that the distance to the training distribution in the embedding space\npredicts output quality for conditional generative models [Ren et al., 2023b]. Extending this work to\nlarge language models is challenging because their training distribution is too large to estimate and\nextracting embeddings from well-integrated LLM systems requires significant engineering effort.\nAlternatively, a straightforward approach to estimating a language model\u2019s confidence in its output is\nto calculate the sequence probability or the length-normalized sequence probabilities [Adiwardana\net al., 2020]. However, studies have shown that language models\u2019 sequence probabilities on open-\nended generations do not reliably rank-order their outputs by quality [Liu et al., 2022, Ren et al.,\n2023b]. Human feedback can be used to fine-tune language models to better align with human-judged\nquality, such as with Reinforcement Learning from Human Feedback (RLHF) [Stiennon et al., 2020],\nSLiC-HF [Zhao et al., 2023] and DPO [Rafailov et al., 2023], resulting in better quality-calibrated\nmodels.\nSince human feedback data is expensive to obtain, we explore leveraging the self-evaluation ability\nof LLMs to improve quality-calibration. Despite the poor calibration on sequence-level likelihood,\nrecent work has shown that LLM token-level probability can be quite well-calibrated on choosing the\narXiv:2312.09300v1  [cs.CL]  14 Dec 2023\nquestion\nLLM\nsequence likelihood score\nanswer\nanswer\nanswer\nSample and Eval\nWhat happens to you if you eat \nwatermelon seeds?\nAnswer:\nThey will be digested and pass \nthrough you.\nIs the above answer correct?\nA) Yes\nB) No\nLetter: \u0baa\nSample and Select\nWhat happens to you if you eat \nwatermelon seeds?\nCandidate answers: \nA) They will grow in your stomach.\nB) They will be digested and pass \nthrough you.\nC) You will get sick.\nD) You will die.\nE) None of the above\nWhich answer is correct? \u0baa\nP(\u0baa=Yes/No)\nP(\u0baa=A/B/C/D)\nP(\u0baa=Yes/No)\nHybrid\nWhat happens to you if you eat watermelon seeds?\nAnswer: \nThey will be digested and pass through you.\nIs the above answer correct?\nA) Yes\nB) No\nLetter: \u0baa\nWhat happens to you if you eat watermelon seeds?\nCandidate answers: \nA) They will grow in your stomach.\nB) They will be digested and pass through you.\nC) You will get sick.\nD) You will die.\nFigure 1: Demonstration of our approach.\ncorrect option of multi-choice question answering and true/false questions [Kadavath et al., 2022,\nOpenAI, 2023, Robinson et al., 2022]. This suggests that evaluating language model\u2019s generation with\ntoken-level probabilities using an appropriate prompt format might be better for selective generation\nthan sequence-level likelihood.\nIn this study, we focus on obtaining a confidence score that is quality-calibrated on free-form\ngeneration tasks. We propose reducing the sequence-level scoring problem to token-level scoring\nby designing different self-evaluation tasks and propose a variety of scores. We focus on evaluating\nmodel\u2019s quality-calibration for use in selective generation, and not just predictive accuracy. We show\nthat our proposed confidence estimation significantly improves the quality calibration, and can be\nused to abstain poor quality outputs using the TRUTHFULQA and TL;DR benchmarks.\n2\nMethods\nBackground: sequence likelihood Given a question x and an answer y, y = y1y2 . . . yl, we have\nsequence-level likelihood score,\nlog p(y|x) =\nl\nX\nt=1\nlog p(yt|y1 . . . yt\u22121, x).\n(Sequence likelihood)\nThough log p(y|x) is statistically meaningful, it has been shown that it is biased towards sequence\nlength, i.e. models tend to underestimate sequence likelihood of longer sentences [Wu et al., 2016].\nThe length normalized likelihood is an alternative score to use,\nlog \u00afp(y|x) = 1\nl\nl\nX\nt=1\nlog p(yt|y1 . . . yt\u22121, x).\n(Length normalized sequence likelihood)\nAlthough sequence-level scores have weak predictive power, the previous results show that LLMs are\nwell-calibrated on multiple choice question answer tasks and true/false evaluation tasks [Kadavath\net al., 2022, OpenAI, 2023], suggesting the model has better calibration on token-level scores. Inspired\nby this, we propose to reduce free-form generation to multiple-choice and true/ false evaluation tasks,\nin order to leverage token-level calibration to improve the calibration of free-form generation, as\nshown in Figure 1. Ren et al. [2023a] propose a similar idea but their focus was on robotics planning,\nwhile we focus on the general question answer settings.\nTo convert free-form generation to multi-choice question answer task, we first sample multiple\ncandidate answers. For a given question x, we sample n answers {yi}, i = 1, . . . , n from an LLM.\nWe tried using a prompt to instruct the model to generate multiple different answers all at once, but\nthe quality of the batch generated answers were not as good as sampling one at a time.\n2\nPosition bias\nProbability Dispersion\nNone is True\n\u2714\n\u2718\n\u2718\n\u2718\n\u2718\n\u2714\n\u2718\n\u2718\n\u2718\n\u2718\n\u2718\n\u2714\n\u2718\n\u2714\n\u2714\n\u2714\nFigure 2: The issues of position bias, probability dispersion, and no true answers in the Sample and\nSelect setup. The question examples are from [Lin et al., 2021, Agarwal et al., 2023].\n2.1\nSample and Select: reduce free-form generation to multi-choice question answer task\nGiven a question and a set of candidate answers {y}n, we append alphabet characters, c =\nA, B, C, . . . , to the answers and form it into a multiple choice format. A straightforward score\ncould be the softmax probability for the characters, p(ci|x, {cy}), which was used in Ren et al.\n[2023a]. The selected answer would be the one with the highest softmax probability, \u02c6y = yr, r =\narg maxi p(ci|x, {cy}). However, there are a few issues with that score:\nPosition bias The score could change as the position of the candidate answers change. See Figure 2\n(left). This phenomenon was also reported in other work [Robinson et al., 2022, Zheng et al., 2023].\nA simple \"shuffle and average\" could de-bias and correct for the scores, while more sophisticated\nmethod to estimate the prior was proposed by Zheng et al. [2023]. In our work, we use the simple\nshuffle and average de-bias method. The ablation study of the effect of position bias is in Table 4.\nProbability dispersion among multiple true answers. Unlike the pre-designed multiple choice QA\ntask where only one true answer provided, in the free-form generation there is no such guarantee\nthat only one of the sampled answers is true. When more than one true answers are in the candidate\nlist, the probability of the true is dispersed among the true answers, see Figure 2 (middle). This\nis an undesired property for comparing across questions, since different questions could generate\ndifferent number of true answers. Probability dispersion is not a unique problem in LLMs; similar\nissue was discovered in the ImageNet classification where an image can map to multiple classes,\nand unnormalized logit was preferred than softmax probability to avoid the probability dispersion\n[Hendrycks et al., 2019]. Therefore we propose,\nlog p(ci|x, {cy}), c = {A, B, . . . }.\n(Sample and Select)\nNo answer is true It is possible that when the model does not know the answer, none of the sampled\nanswers is true. If only wrong answers are provided, the model will be forced to choose one from\nthem, resulting in over-confident prediction. See Figure 2 (right). To mitigate that, we add \u201cNONE OF\nTHE ABOVE\u201d as an additional candidate answer to give model a chance to reject the sampled answers,\n{y}+nota = {y} \u222a {nota}. This is similar to adding \u201cAn option not listed here\u201d to the robotic\nplanning task [Ren et al., 2023a]. We obtain the score corresponding to the \u201cNONE OF THE ABOVE\u201d\nanswer,\np(cnota|x, {cy}+nota)\n(Sample and Select w/ NONE OF THE ABOVE)\nA higher nota score indicates that the selected answer is less likely to be correct.\nSo we\nuse \u2212p(cnota|x, {cy}+nota) as the confidence score of the selected answer, \u02c6y = yr, r =\narg maxi p(ci|x, {cy}). Note that the selected answer is still the answer with the highest score\nwithin the original answer set {y} excluding the nota answer.\n2.2\nSample and Eval: reduce free-form generation to true/false evaluation task\nWe can also evaluate a question and an answer pair using pointwise evaluation format. We ask the\nmodel if the candidate answer is correct or not, as shown in Figure 1. Since the task is a binary\n3\nclassification task, we can normalize the output score using softmax function to a probability,\np(Yes|x, yi).\n(Sample and Eval)\nThis is similar the P(True) proposed in [Kadavath et al., 2022]. They also propose to include candidate\nanswers in the prompt,\np(Yes|x, yi, {y}).\n(Sample and Eval w/ other candidates)\nBut that work focuses on the scaling law of the score\u2019s calibration, and did not compare it with\nsequence-level score and Sample and Select score.\n2.3\nCombining the best of both worlds: select the answer via multi-choice evaluation and\nscore the selected answer via pointwise evaluation\nSample and Select and Sample and Eval have their own pros and cons. In Sample and Select,\nalthough the un-normalized logit is better than softmax probability for calibration purpose, the logit\nscore is still dependent on the other candidate answers. For fairly comparing across (x, y) pairs,\na good score should measure the confidence to the (x, y) itself, not dependent on other candidate\nanswers. Sample and Eval score p(Yes|yi, x) is indeed independent of other answers. On the other\nhand, Sample and Select provides the opportunity for comparing different answers and select the\nbest. Therefore, we combine the best of both: We first use Sample and Select to select the best\nanswer within a given question. The answer with the highest softmax probability score is selected,\n\u02c6y = yr, r = arg maxi p(ci|x, {cy}). After selection, we discard the score because it is not good for\ncross question comparison. We score the selected answer via Sample and Eval p(Yes|x, \u02c6y).\np(Yes|x, \u02c6y), where \u02c6y = yr, r = arg max\ni\np(ci|x, {cy}).\n(Hybrid)\nIn the case where NONE OF THE ABOVE answer is added, we penalize the confidence score p(Yes|x, \u02c6y)\nwith the uncertainty score for the nota answer, that is p(Yes|x, \u02c6y) \u2212 p(cnota|x, {cy}+nota). We call\nthis hybrid strategy \u201cSample and Select and Eval\u201d. See details in Algorithm 1.\nAlgorithm 1 Hybrid \u201cSample and Select and Eval\u201d\n1: Input: Question x, LLM model M, sample prompt G, multi-choice selection prompt F,\npointwise evaluation prompt E.\n2: Use sample prompt G to sample n answers {y} = {y1, . . . , yn}, yi\niid\n\u223c M(x)\n3: Append \u201cNONE OF THE ABOVE\u201d answer to {y} = {y} \u222a {nota}. |{y}| = n + 1.\n4: Compose selection prompt with answers F(x, {y}), feed to M, obtain output softmax probabil-\nity scores p(ci|x, {cy}).\n5: Select the best answer among the sampled n answers (exclude the post-hoc added nota answer\n\u02c6y = yr, r = arg maxi\u0338=n+1 p(ci|x, {cy}).\n6: Obtain the uncertainty score for nota answer, snota = p(cnota|x, {cy}).\n7: Compose pointwise evaluation prompt for the selected answer E(x, \u02c6y), feed to M, obtain output\nscore s = p(Yes|x, \u02c6y).\n8: The final confidence score is s = s \u2212 snota.\n9: Output: the selected answer \u02c6y, and its confidence score s.\n3\nEvaluation metrics for selective generation\nSuppose D = {x}m is a dataset containing m questions to evaluate. Given a LLM model M, for\neach question x, we randomly sample n answers {y}n = {y1, y2, . . . , yn}, where yi\niid\n\u223c M(x).\nSuppose the ground truth h(x, y) = {0, 1} for each answer\u2019s correctness (or quality) is available,\neither through human evaluation or an auto-evaluation model to approximate human rating. Given a\nconfidence score function s(x, y) measuring the confidence of a (x, y) pair, we would like evaluate\nhow well the score could be used for selective generation, besides the accuracy.\nAccuracy For a fixed question x and a set candidate answers {y}n to x, we could use the confidence\nscore to select the final answer \u02c6y to the question x. We assess if the selected answer is correct, i.e.\nh(x, \u02c6y) = 1, \u02c6y = yr, r = arg maxn\ni=1 s(x, yi).\n4\nAccuracy evaluates if the score can be used to choose the best answer among the candidate answers\nwithin a given question. For selective generation, we compare across questions. Given the m question\nand its selected best answer, {(x, \u02c6y)}m, we would abstain poor quality pairs to ensure better overall\ngeneration quality, aka selective generation. Suppose for each pair we have a confidence score,\ns(x, \u02c6y). If the score is predictive for the quality, we could rank the pairs by the score, and abstain\nthose with the lowest scores, and selectively only output answers with high scores. For the abstained\nlow quality answers, we could instead output \u201cSORRY, I DON\u2019T KNOW\u201d. An honest \u201cI don\u2019t know\u201d\nanswer is better then a wrong answer. To quantitatively evaluate the scores on selective generation,\nwe use Calibration-AUC and Selective-AUC as defined below.\nCalibration-AUC AUC metric for a binary prediction task where the binary label is the correctness\nh(x, \u02c6y), and the prediction score is the confidence score s(x, \u02c6y) [Kivlichan et al., 2021]. Since\nCalibration-AUC measures the ranking performance, it cannot be simply tricked using the post-hoc\ncalibration heuristics such as the temperature scaling.\nSelective generation curve and AUC Selective generation curve measures the correctness h(x, \u02c6y)\nas a function of abstention rate \u03b1%, where the samples are sorted by s(x, \u02c6y) and samples with\nthe lowest \u03b1% scores are abstained [Ren et al., 2023b]. At \u03b1 = 0 no sample is abstained, so the\ncurve starts from the conventionally defined accuracy. As \u03b1 increases, if the score is predictive of\ncorrectness, low quality samples will be abstained first, and the remaining samples will have higher\noverall quality. Therefore we expect the curve to increase. To quantitatively measure the performance,\nwe compute the area under the selective generation curve, Selective-AUC.\nDistinction to Expected Calibration Error (ECE) ECE [Guo et al., 2017] is commonly used to\nmeasure if the predictive probability value matches the ground truth accuracy. ECE computation\nis straightforward for categorical prediction. However, for sequence generation, even though it\nis possible to define sequence-level ECE [Zablotskaia et al., 2023], getting the ground truth is\nchallenging. Also ECE can only be applied to probabilistic scores. The confidence scores we propose\nare not necessarily probabilities, so therefore ECE is not applicable there. In this study, we focus\non a more general setting that apply to any confidence scores: assessing if the confidence score\nis predictive of the output quality. Therefore we use the calibration-AUC and selective generation\ninstead of ECE.\n4\nExperiments\n4.1\nExperiment setup\nLLMs\nPALM-2 LARGE is mainly used in our experiments. For each question, we sample n = 4\nanswers at temperature 1.0. We de-duplicate the answers to reduce the chance of probability\ndispersion. We also consider GPT-3 (text-davinci-003) model for evaluation. Due to the OpenAI\nAPI limitation, we cannot evaluate all the methods and obtain complete results for GPT-31. We can\nneither evaluate methods on GPT-3.5 and GPT-4 models because OpenAI API does not provide\noutput log-probabilities for them.\nBenchmark datasets\nTRUTHFULQA [Lin et al., 2021] is a dataset for assessing model\u2019s ability\nto generate truthful answers against false belief or misconception. It contains 817 questions in the\nvalidation split. To label the quality of generated answers, we use the GPT-judge, which is a GPT-3\nmodel fine-tuned on human feedback data, provided by Lin et al. [2021]. It is shown that GPT-judge\nhas 90-95% accuracy in predicting human evaluations of truthfulness.\nTL;DR is a summarization benchmark dataset mined from Reddit website [V\u00f6lske et al., 2017]. It\ncontains 15,240 examples in the test split. We randomly sampled 1000 examples to save inference\ncost. To label the quality of the generated summaries, we use a reward model fine-tuned on human\nfeedback data, as used by [Zhao et al., 2023]. The prediction accuracy of human rating of the reward\nmodel is 71.34%.\n1For GPT-3 model, the API can only output log-probability for up to 5 most likely tokens. Because of\nthis limitation, a few methods cannot be evaluated on GPT-3. For example, the most likely tokens in the\nmulti-response evaluation setting are not necessarily A, B, C etc., but the most likely letter and its variants such\nas \u2018A\u2019, \u2018 A\u2019, or \u2018A\\n\u2019. Therefore the maximum token prediction and its log-probability are always available, but\nthe log-probability for a specific token such as \u2018E\u2019 for the \u201cNone of the above\u201d answer is not available.\n5\nTable 1: Comparison of different scores for the accuracy and calibration metrics on TRUTHFULQA for\nPALM-2 LARGE and GPT-3 models. The numbers are in percentage.\nAccuracy\nCalibration-AUC\nSelective-AUC\nPALM-2 LARGE\nSequence likelihood\n48.23\n39.80\n33.63\nLen-norm sequence likelihood\n52.75\n50.09\n42.15\nSample and Select\n58.26\n53.17\n48.59\nSample and Select w/ nota\n58.13\n72.59\n56.61\nSample and Eval\n59.12\n73.79\n58.19\nSample and Eval w/ candidates\n59.00\n68.78\n55.70\nHybrid\n58.26\n73.76\n57.38\nHybrid w/ nota\n58.14\n75.34\n58.10\nGPT-3\nSequence likelihood\n67.19\n40.50\n49.76\nLen-norm sequence likelihood\n67.19\n42.06\n50.22\nSample and Select\n72.24\n47.97\n56.75\nSample and Select w/ nota\nNA\nNA\nNA\nSample and Eval\n67.83\n48.47\n53.28\nSample and Eval w/ candidates\n68.48\n51.36\n55.28\nHybrid\n72.24\n51.66\n58.46\nHybrid w/ nota\nNA\nNA\nNA\n4.2\nResults\nThe performance of the different scores evaluated using accuracy, calibration-AUC, and selective-\nAUC are shown in Table 1. It is clear to see that, sequence-level likelihood is not good for both\naccuracy and calibration. It has even below 0.5 AUC suggesting sequence likelihood is negatively\ncorrelated with correctness. Length normalization could improve the performance but AUC is still\nbelow 0.5. The strategy of reducing sequence-level score to token-level scores via self-evaluation\nimprove both the accuracy and calibration over sequence likelihood. Considering all metrics together,\nthe hybrid strategy with NONE OF THE ABOVE added, achieves overall better performance.\nComparing the two strategies, Sample and Select and Sample and Eval, Sample and Select has\ndecent accuracy, but suffers from the calibration metrics. Adding NONE OF THE ABOVE helps improve\ncalibration. On the other hand, Sample and Eval is better on calibration metrics, but it has a bit lower\naccuracy. This trend is more clear in GPT-3. Therefore we propose the hybrid strategy to combine\nthe best of both. The ROC curves for binary classification of correct and incorrect answers using\ndifferent scores, and the selective generation curves can be found in Figure 3. Calibration-AUC and\nSelective-AUC are the area under the two curves respectively.\nIn addition, we show that self-evaluation is complementary to self-critique and revise, a technique\nto self-improve the answer quality [Bai et al., 2022]. We first apply that technique to improve each\nof the sampled answers. Then we compute the scores on the revised answers, instead of on the\noriginal answers. In Table 2, it is clear that on the revised answers, we see similar patterns that\nsequence-level scores are not well suited for selective generation, and the token-level scores achieves\nbetter performance.\nTable 2: Self-critique and revise further improves the model\u2019s accuracy, calibration, and selective\ngeneration on TRUTHFULQA on PALM-2.\nAccuracy\nCalibration-AUC\nSelective-AUC\nSequence likelihood\n54.83\n38.96\n38.40\nLen-norm sequence likelihood\n59.12\n49.64\n47.03\nSample and Select\n64.87\n50.41\n52.40\nSample and Select w/ nota\n64.60\n66.92\n58.69\nSample and Eval\n66.34\n70.55\n61.81\nSample and Eval w/ candidates\n66.71\n64.69\n59.44\nHybrid\n64.87\n71.35\n61.11\nHybrid w/ nota\n64.50\n72.72\n61.44\n6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nSeq likelihood\nSeq len-norm likelihood\nSample and Select\nSample and Select w/ nota\nSample and Eval\nHybrid\nHybrid w/ nota\n(a) ROC curves, PaLM-2L\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAbstention rate\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy on the remaining\nSeq likelihood\nSeq len-norm likelihood\nSample and Select\nSample and Select w/ nota\nSample and Eval\nHybrid\nHybrid w/ nota\n(b) Selective generation, PaLM-2L\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nSeq likelihood\nSeq len-norm likelihood\nSample and Select\nSample and Eval\nHybrid\n(c) ROC curves, GPT-3\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nAbstention rate\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy on the remaining\nSeq likelihood\nSeq len-norm likelihood\nSample and Select\nSample and Eval\nHybrid\n(d) Selective generation, GPT-3\nFigure 3: ROC curves for binary classification and selective generation curves, evaluated on TRUTH-\nFULQA. The left most point of the selective generation curves (abstention rate \u03b1 = 0) is the accuracy\nreported in Table 1. The area under the ROC curve is calibration-AUC, and the area under the\nselective generation curve is selective-AUC.\n4.3\nSelf-evaluation improves calibration on TL;DR summarization\nTL;DR is a summarization benchmark dataset mined from Reddit website [V\u00f6lske et al., 2017].\nEvaluating the different scores on that dataset shows again that the sequence-level scores are not\nsuitable for calibration. Self-evaluation based token-level scores improve the both accuracy and\ncalibration performance (Table 3). Sample and Select has higher accuracy but lower calibration-AUC\nthan Sample and Eval, and adding NONE OF THE ABOVE option helps to improve Calibration-AUC\nwithout sacrificing much the accuracy. Hybrid methods in general have decent performance.\nTable 3: Comparison of different scores: accuracy and calibration on TL;DR for PALM-2.\nAccuracy\nCalibration-AUC\nSelective-AUC\nSequence likelihood\n65.80\n49.75\n52.63\nLen-norm sequence likelihood\n69.40\n53.20\n56.93\nSample and Select\n70.20\n46.65\n54.68\nSample and Select w/ nota\n70.80\n49.54\n56.56\nSample and Eval\n68.70\n52.34\n56.09\nSample and Eval w/ candidates\n70.20\n55.19\n57.91\nHybrid\n70.70\n52.19\n57.56\nHybrid w/ nota\n70.80\n52.05\n57.55\n4.4\nEffect of position bias\nWe assess the effect of position bias on the performance. We compare the vanilla setting where\nthe answers are ordered by default, and the de-biased setting where the answer scores are averaged\nacross all n! possible permutations. The difference on the performance is not that significant. Given\n7\nthe de-bias process through shuffle and average is very computational expensive, we use the vanilla\nsetting by default.\nTable 4: Effect of position bias on metrics. The results are based on PALM-2 LARGE.\nAccuracy\nCalibration-AUC\nSelective-AUC\nTRUTHFULQA\nSample and Select, vanilla\n58.26\n53.17\n48.59\nSample and Select, de-biased\n58.87\n52.13\n48.58\nTL;DR\nSample and Select, vanilla\n70.20\n46.65\n54.68\nSample and Select, de-biased\n70.70\n43.94\n53.86\n5\nRelated work\nThe calibration of LLMs on multiple choice question answer tasks is studied in Kadavath et al.\n[2022]. Robinson et al. [2022] show that the sequence level probability is worse than the token-level\nprobability (e.g. A, B, C, etc) for predicting the correctness. But those studies use the multiple choice\nquestion answering datasets where the answers are pre-defined and not generated from LLMs. Our\nwork focuses on the calibration of free-form generation tasks. We transform free-form generation\nto multiple choice task by generating answer candidates by itself. Another distinction to [Kadavath\net al., 2022] is that we care more on the ranking performance measured by AUC than the exact value\nmatch to ground truth probability measured by ECE.\nIn terms of estimating language models\u2019 confidence or uncertainty, Tian et al. [2023], Lin et al. [2022]\npropose to ask model to express uncertainty in words along with the generated answer, but it is\nshown that LLMs often exhibit a high degree of overconfidence when verbalizing their confidence\n[Xiong et al., 2023]. Kuhn et al. [2023] propose to use semantic entropy among a set of sampled\nanswers to estimate model\u2019s uncertainty. The semantic similarity is inferred using a separate natural\nlanguage inference classification system (NLI). Cole et al. [2023] find the degree of repetition in\nsampled answers is a good score for selectively answering ambiguous questions. The distinctions\nbetween our work and the above are that, we focus on estimating the confidence of long sequence\nfree-form generation tasks, where the repetition can not be easily measured. Also, we are interested\nin zero-shot self-evaluation based scores, without utilized a separate model for inference. The true/\nfalse evaluation method proposed by Kadavath et al. [2022] is one of them. In our work, we compare\nthis score with several other scores and have a comprehensive assessment on selective generation of\nfree-form generation tasks\nPrior studies have proposed generating multiple candidate responses for free-form generation tasks\nand then selecting the best. The final answer is selected using a variety of methods, including: (1)\nsimple sequence likelihood [Adiwardana et al., 2020], (2) ranking model trained on human preference\ndata [Nichols et al., 2020], (3) self-consistency i.e. if an answer is the most consensus one [Wang\net al., 2022, Chen et al., 2023] and (4) models\u2019 self-evaluation ability to choose the final response\nbased on its own evaluation of the responses [Ren et al., 2023a]. However, the focus of most prior\nwork except for [Ren et al., 2023a] are on improving accuracy, not on confidence estimation or\ncalibration. [Ren et al., 2023a] is similar to our work in the sense that it not only proposes to generate\nmultiple options and then ask the model to choose one, but also estimate uncertainty to ask for\nclarification. However they focus on robotics planning, while we focus on more general question\nanswer. Also, they directly use the multiple choice score output, while we identified the position bias\nand probability dispersion problems in the scores, and propose hybrid method to address them\n6\nDiscussion\nWe show that although generic sequence-level scores are not well suited for selective generation\n(even negatively correlated with the the quality) for free-form generation, asking the model again\nto self-evaluate could reduce the sequence-level score to token-levels scores, improving quality\ncalibration. Self-evaluation is though at the cost of increasing inference time by 1 or 2 (hybrid\nmode) times. Alternative to this post-hoc method, how to improve the quality calibration of the\nsequence-level score during training and finetuning is one of our future work.\n8\nAcknowledgements\nWe would like to thank Denny Zhou, Zelda Mariet, Sharat Chikkerur, Jasper Snoek, and Alexander\nD\u2019Amour from Google DeepMind for helpful discussions for insightful discussion and providing\nvaluable feedback for this work. We would also like to express our appreciation towards Lyric Doshi,\nXuezhi Wang, and Michael W. Dusenberry from Google DeepMind for their technical support.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan,\nZi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. Towards a human-like open-\ndomain chatbot. arXiv preprint arXiv:2001.09977, 2020.\nAyushi Agarwal, Nisarg Patel, Neeraj Varshney, Mihir Parmar, Pavan Mallina, Aryan Bhavin Shah,\nSrihari Raju Sangaraju, Tirth Patel, Nihar Thakkar, and Chitta Baral. Can NLP models\u2019 iden-\ntify\u2019,\u2019distinguish\u2019, and\u2019justify\u2019questions that don\u2019t have a definitive answer?\narXiv preprint\narXiv:2309.04635, 2023.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional AI:\nHarmlessness from AI feedback. arXiv preprint arXiv:2212.08073, 2022.\nXinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin, Sushant Prakash,\nCharles Sutton, Xuezhi Wang, and Denny Zhou. Universal self-consistency for large language\nmodel generation. arXiv preprint arXiv:2311.17311, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nJeremy R Cole, Michael JQ Zhang, Daniel Gillick, Julian Martin Eisenschlos, Bhuwan Dhingra, and\nJacob Eisenstein. Selectively answering ambiguous questions. arXiv preprint arXiv:2305.14613,\n2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural\nnetworks. In International conference on machine learning, pages 1321\u20131330. PMLR, 2017.\nDan Hendrycks, Steven Basart, Mantas Mazeika, Andy Zou, Joe Kwon, Mohammadreza Mostajabi,\nJacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings.\narXiv preprint arXiv:1911.11132, 2019.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly)\nknow what they know. arXiv preprint arXiv:2207.05221, 2022.\nIan D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving model-\nmoderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.\nLorenz Kuhn, Yarin Gal, and Sebastian Farquhar. Semantic uncertainty: Linguistic invariances for\nuncertainty estimation in natural language generation. arXiv preprint arXiv:2302.09664, 2023.\nStephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958, 2021.\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\nwords. arXiv preprint arXiv:2205.14334, 2022.\nYixin Liu, Pengfei Liu, Dragomir Radev, and Graham Neubig. Brio: Bringing order to abstractive\nsummarization. arXiv preprint arXiv:2203.16804, 2022.\nEric Nichols, Leo Gao, and Randy Gomez. Collaborative storytelling with large-scale neural language\nmodels, 2020.\n9\nOpenAI. GPT-4 technical report. arXiv, pages 2303\u201308774, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback, 2022. URL https://arxiv. org/abs/2203.02155, 13, 2022.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. 2018.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367, 2020. URL\nhttp://jmlr.org/papers/v21/20-074.html.\nAllen Z Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu,\nLeila Takayama, Fei Xia, Jake Varley, et al. Robots that ask for help: Uncertainty alignment for\nlarge language model planners. arXiv preprint arXiv:2307.01928, 2023a.\nJie Ren, Jiaming Luo, Yao Zhao, Kundan Krishna, Mohammad Saleh, Balaji Lakshminarayanan,\nand Peter J. Liu. Out-of-distribution detection and selective generation for conditional language\nmodels, 2023b.\nJoshua Robinson, Christopher Michael Rytting, and David Wingate. Leveraging large language\nmodels for multiple choice question answering. arXiv preprint arXiv:2210.12353, 2022.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. Learning to summarize with human feedback. Advances in\nNeural Information Processing Systems, 33:3008\u20133021, 2020.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea Finn,\nand Christopher D Manning. Just ask for calibration: Strategies for eliciting calibrated confidence\nscores from language models fine-tuned with human feedback. arXiv preprint arXiv:2305.14975,\n2023.\nMichael V\u00f6lske, Martin Potthast, Shahbaz Syed, and Benno Stein. Tl; dr: Mining reddit to learn\nautomatic summarization. In Proceedings of the Workshop on New Frontiers in Summarization,\npages 59\u201363, 2017.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine translation sys-\ntem: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144,\n2016.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs\nexpress their uncertainty? an empirical evaluation of confidence elicitation in LLMs. arXiv preprint\narXiv:2306.13063, 2023.\nPolina Zablotskaia, Du Phan, Joshua Maynez, Shashi Narayan, Jie Ren, and Jeremiah Liu. On uncer-\ntainty calibration and selective generation in probabilistic neural summarization: A benchmark\nstudy. arXiv preprint arXiv:2304.08653, 2023.\nYao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf:\nSequence likelihood calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.\nChujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. On large language models\u2019\nselection bias in multi-choice questions. arXiv preprint arXiv:2309.03882, 2023.\n10\nA\nPrompts used in the study\nIn the following task, you will receive a question. Please generate one \nfactual, informative, unbiased, and safe answer to the question. \n==\nQuestion:\nWhat happens to you if you eat watermelon seeds?\n==\nAnswer:\nWatermelon seeds will pass through your digestive system without \nbeing digested and will eventually be excreted in your stool.\n==\nQuestion:\n{question}\n==\nAnswer:\nFigure 4: The prompt for sampling an answer in TRUTHFULQA.\nFigure 5: The prompt for multi-choice selection in TRUTHFULQA.\n11\nIn the following task, you will receive a question with a proposed answer. \nPlease judge if the answer is factual, informative, unbiased, and safe.\n==\nQuestion:\n{question}\n==\nAnswer:\n{answer}\n==\nIs the above answer factual, informative, unbiased, and safe?\nA) Yes\nB) No\n==\nLetter:\nFigure 6: The prompt for True/False evaluation in TRUTHFULQA.\nFigure 7: Prompt for True/False evaluation with other candidates in TRUTHFULQA\n12\nIn the following task, you will receive a question and an answer. \nPlease generate an explanation why this answer would be non-factual.\nSay None if you don't have any critique.\n==\nQuestion:\nWhat happens to you if you eat watermelon seeds?\n==\nAnswer:\nWatermelon seeds can cause a watermelon to grow in your stomach.\n==\nThis answer could be wrong because the stomach is an acidic \nenvironment that is not suitable for watermelon seeds to grow.\n==\nQuestion:\n{question}\n==\nAnswer:\n{answer}\n==\nThis answer could be wrong because\nFigure 8: The prompt for generating a critique in TRUTHFULQA.\n13\nFigure 9: The prompt for generating a revised answer given the critique in TRUTHFULQA.\nFigure 10: The prompt for sampling an answer in TL;DR.\n14\nFigure 11: The prompt for multi-choice selection in TL;DR.\nIn the following task, you will receive a text and a proposed summary. \nPlease judge if the summary is concise and comprehensive.\n==\nText:\n{question}\n==\nSummary:\n{answer}\n==\nIs the above summary concise and comprehensive?\nA) Yes\nB) No\n==\nLetter:\nFigure 12: The prompt for pointwise evaluation in TL;DR.\n15\nFigure 13: The prompt for pointwise evaluation with other candidates in TL;DR.\n16\n"
  },
  {
    "title": "Extending Context Window of Large Language Models via Semantic Compression",
    "link": "https://arxiv.org/pdf/2312.09571.pdf",
    "upvote": "11",
    "text": "EXTENDING CONTEXT WINDOW\nOF LARGE LAN-\nGUAGE MODELS VIA SEMANTIC COMPRESSION\nWeizhi Fei\u2020 \u2021 & Xueyan Niu\u2217\u2021\n\u2020 Department of Mathematical Sciences, Tsinghua University, Beijing, China\n\u2021 Theory Lab, 2012 Labs, Huawei Technologies Co., Ltd.\nPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han\nHuawei Technologies Co., Ltd.\nABSTRACT\nTransformer-based Large Language Models (LLMs) often impose limitations on\nthe length of the text input to ensure the generation of fluent and relevant re-\nsponses. This constraint restricts their applicability in scenarios involving long\ntexts. We propose a novel semantic compression method that enables general-\nization to texts that are 6-8 times longer, without incurring significant computa-\ntional costs or requiring fine-tuning. Our proposed framework draws inspiration\nfrom source coding in information theory and employs a pre-trained model to re-\nduce the semantic redundancy of long inputs before passing them to the LLMs for\ndownstream tasks. Experimental results demonstrate that our method effectively\nextends the context window of LLMs across a range of tasks including question\nanswering, summarization, few-shot learning, and information retrieval. Further-\nmore, the proposed semantic compression method exhibits consistent fluency in\ntext generation while reducing the associated computational overhead.\n1\nINTRODUCTION\nThe recent successful release of large language models (LLMs) such as ChatGPT (Radford et al.,\n2019) and LLaMA (Touvron et al., 2023) has sparked significant research efforts from both industry\nand academia. These LLMs have demonstrated the ability to engage in fluent and coherent conver-\nsations with human users, and have shown exceptional performance across various tasks, including\ndocument summarization, question-answering, dialogue bots, and code generation copilots.\nOne critical issue faced by state-of-the-art (SoTA) LLMs is the restriction on the length of text that\ncan be inputted into the model at once. When the input context exceeds the limit of the context\nwindow, the performance of these models rapidly declines. This limitation poses a challenge when\nit comes to handling long texts such as scientific papers, novels, and legal contracts with current\nLLMs. As a result, there has been a growing interest in finding ways to extend the input length\nwithout significantly compromising the model\u2019s performance.\nThe limitation on the context window primarily stems from the quadratic computation of the self-\nattention mechanism in the transformer. Handling lengthy texts significantly increases the compu-\ntational costs in terms of memory and time. Typically, models are trained on short contexts, and the\nmaximum sequence length (i.e., the context window) is determined. If the models are compelled\nto generate contexts that exceed the context window, they tend to compromise the quality of the\noutput due to the lack of position encoding information during the training process. Furthermore,\ngenerating long sequences imposes substantial memory requirements on the computational device.\nThis accumulation of memory requirements and the lack of effective position encoding can result in\nlength generalization failure (Anil et al., 2022), where the models struggle to generate meaningful\nand coherent text beyond a certain context window size.\nSome approaches have been developed to address the aforementioned challenges. One approach\nis to devise architectures with nearly linear complexity, which enables efficient scaling to handle\n\u2217Correspondence to: niuxueyan3@huawei.com.\n1\narXiv:2312.09571v1  [cs.CL]  15 Dec 2023\nFigure 1: With the inclusion of the semantic compression module, the redundancies in the input are eliminated,\nthereby effectively extending the context window. The semantic compression is reminiscent of the concept of\nsource coding in information theory.\nvery long sequences. However, training a large model from scratch incurs substantial cost. Another\nstrategy involves employing interpolation and fine-tuning techniques to adapt the position encoding\nto unseen sequence lengths. While this method has the potential to compromise the overall perfor-\nmance of LLMs, it still demands significant time and GPU resources for fine-tuning and inference\non long sequences. Therefore, it is more efficient and resource-friendly to design methods that do\nnot necessitate altering the parameters of the pre-trained model.\nWhile most previous algorithms relied on modifying the pre-trained model, we instead exploit the\nstatistical properties of input natural language. One empirical phenomenon, known as Zipf\u2019s law\n(Zipf, 2016), observes that a small set of the most frequent word tokens in a large corpus of natural\nlanguage account for almost all occurrences. This pattern arises from the tendency of language users\nto minimize effort in their daily conversations. Consequently, by utilizing an expanded vocabulary,\nsentences can be significantly shortened while preserving the same semantic meaning. Moreover, it\nis common for language users to include redundant words during communication (Strunk Jr, 2007).\nThese language habits are prevalent among users, and we propose to include a semantic compression\nmodule to mitigate the redundancy associated with these habits.\nOur proposed semantic compression method, reminiscent of lossy source coding in information\ntheory, extends the context window by equivalently shortening the long text while preserving the\nsemantic meaning. This procedure is conducted before inputting the tokens into the pre-trained\nLLMs. As illustrated in Fig. 1, the input undergoes compression before being transmitted to the\nLLM for various potential tasks. The semantic compression method can be customized and opti-\nmized for downstream tasks, taking into consideration practical constraints such as time and memory\nresources. The implementation of the semantic compression module is straightforward and can eas-\nily be incorporated into other interpolation-based context window extension methods and black box\nAPIs. It demonstrates enhanced performance compared to SoTA interpolation-based methods on\na range of tasks, including single-document question answering, multi-document question answer-\ning, summarization, few-shot learning, and information retrieval, using real-world datasets while\nincurring no extra parameter updates or memory consumption. Empirically, the proposed method is\ncomputational efficient and achieves 6-8 times context window extension.\nOur contributions:\n\u2022 We introduce a context window extension framework for LLMs that utilizes semantic com-\npression. This framework serves as a plug-and-play tool to mitigate redundancy in input\ntexts by efficiently performing topic modeling.\n\u2022 We construct a graph representation of the input to identify distinct sections of the text that\npertain to different topics. The result is the segmentation of long texts into separate chunks,\neach focusing on a specific topic. We then conquer each chunk independently, resulting in\na concise version of the original texts. This compression technique helps to condense the\ninformation while preserving the key ideas and context.\n\u2022 We demonstrate the applicability of our proposed semantic compression method through\nextensive experiments. The results highlight the advantages of our method in several key\napplications, including single-document question answering, multi-document question an-\nswering, summarization, few-shot learning, and information retrieval.\n2\nRELATED WORK\nWith the advancement of SoTA LLMs, significant progress has been made in extending the context\nwindow lengths.\n2\n2.1\nEXTRAPOLATION AND INTERPOLATION\nThe mainstream line of research aims to adapt existing language models trained on short texts to ac-\ncommodate longer ones during inference (Anil et al., 2022). The key idea is to modify the positional\nembedding, which has only been trained on short texts. Several studies are based on the Rotary Po-\nsition Embeddings (RoPE) of LLaMA and methods of adjusting it to the longer sequences. Chen\net al. (2023a) develops the Position Interpolation (PI) method to linearly scale the input positional\nindices. Peng et al. (2023) presents YaRN, an efficient extrapolate mechanism inspired by the neural\ntangent kernel, to extend the context window to 64k and 128k.\n2.2\nEFFICIENT ATTENTION OPERATIONS\nDue to the self-attention mechanism, the inference cost of LLMs grows quadratically with the se-\nquence length. Many methods have been proposed to decrease the complexity. Dai et al. (2019)\npresent Transformer-XL which utilize segment-level recurrence agency and a novel positional en-\ncoding scheme. Beltagy et al. (2020) introduce Longformer with a sparse attention mechanism that\nscales linearly with sequence length. Bo (2021) provides a faster transformer, RWKV, which com-\nbines the strength of RNN and has linear complexity during inference. Dao et al. (2022) propose\nFlashAttention, a chunking strategy for the input, and utilize recomputation to avoid the quadratic\ncomplexity of attention computation. While these methods have the potential to handle longer input\nsequences (Ding et al., 2023), training new models can be costly. Moreover, these methods are not\neffective when dealing with out-of-distribution content lengths.\nThe introduction of new positional embeddings requires fine-tuning on long sequences to adapt to the\nincreased length, which can be computationally expensive. To address this, LongLoRA is introduced\nby Chen et al. (2023b), offering an efficient fine-tuning method with limited computational costs.\nMore details on several other chunking strategies are provided in the survey by Huang et al. (2023).\n2.3\nPROMPTING\nThere are ongoing efforts to extend the context window through smart prompting designs. Wingate\net al. (2022) utilize soft prompts to encode more information using fewer tokens. Chevalier et al.\n(2023) present AutoCompressor, which utilizes soft prompts to compress the input sequence and\nthen extends the original length of the base model. Both Zhou et al. (2023) and Wang et al. (2023)\nrecurrently apply LLMs to summarize the input texts to maintain long short-term memory for spe-\ncific purposes such as story writing and dialogue generation, respectively.\n3\nMETHODOLOGY\nWe propose our semantic compression method for extending the context window. The core idea is to\ncompress the input into shorter texts without losing the key information and important details. This\nenables us to effectively include more content within the fixed input length constraint of the LLM.\nFig. 2 provides an overview of our method, which leverages pre-trained summarization models\ncommonly used in Natural Language Processing (NLP).\nExisting summarization methods also have limitations regarding the length of the input. Here, we\npropose a divide-and-conquer based approach that takes into account the structure of the text. By\nidentifying the topic structure of lengthy texts and dividing them into blocks that exhibit a certain\nlevel of mutual independence, the content within each block can be compressed efficiently due to\ntheir statistical correlation. Each block is then processed in parallel using pre-trained models, and\nthe results are combined to create a condensed textual input that can be processed by the LLM. This\napproach aims to provide a more efficient and effective way of summarizing long texts by leveraging\nboth the structure and content of the original text.\n3.1\nMODEL\nReal-world textual content, such as speech and book, frequently displays hierarchical structures,\nwherein each section is structured around a particular topic, and different sections differ in topic in\na sequential manner. This hierarchical structure, based on topics, bears resemblance to cliques in\n3\nFigure 2: An illustration of our semantic compression method. The input text is initially segmented into\ntopic-based chunks, utilizing the graph representation. Subsequently, these chunks undergo refinement using\npre-trained models to ensure the preservation of key information. Finally, the refined chunks are assembled\nin accordance with the original order. The resulting texts, which have been semantically compressed, are\napproximately 6-8 times shorter in length compared to the original input. Consequently, they fall within the\ncontext window of the LLMs. Furthermore, for additional extension of the length, other methods such as\nextrapolation and interpolation-based techniques can be concatenated.\ngraphs. To identify this structure within long texts, we utilize weighted graphs to represent them\nand employ clustering methods to detect cliques in these graphs. The cliques can then be utilized\nto represent the topic-based content of the text, allowing us to obtain chunks based on the semantic\nrelevance of the topics.\nWe begin by sequentially constructing sentence-level blocks within given lengths and representing\nthem as nodes in our graph. In this step, we parse the text into different sentences or sub-sentences\nbased on punctuation marks. Next, we sequentially fill the sentence-level blocks until they exceed\nthe desired length before proceeding to the next blocks. Once we have obtained the sentence-level\nblocks, we connect the graph representation of long text G based on a pre-trained sentence embed-\nding model (e.g., MiniLM (Wang et al., 2020)), where the weight G[i][j] represents the semantic\nsimilarity between the i-th and j-th sentence-level blocks. Typically, this similarity is computed\nusing cosine similarity, which measures the cosine of the angle between two embeddings. If the\nsimilarity between two blocks is higher, it indicates that they are closer in topics.\n3.2\nTOPIC-BASED CHUNKING\nWe then apply clustering algorithms on the graph to identify the underlying topic structure. Within\neach cluster, we group the sentence-level blocks sequentially to obtain the topic-based chunks, which\ncan then be handled simultaneously by the pre-trained model chosen according to the downstream\ntask. The number of clusters can be adjusted to regulate the length of the text following semantic\ncompression. If these semantic chunks still surpass the predetermined length, the identical procedure\nis repeated to acquire sub-level topic structures.\nThe obtained topic structures are tree-like, which can be flattened in accordance with the order\nof the original content. As per the model, each chunk is semantically centered around a specific\ntopic, and these topics are mutually exclusive. Consequently, these chunks can be compressed\nindependently by utilizing a pre-trained summarization model. Choosing from different pre-trained\nsummarization models allows a trade-off between efficiency and effectiveness. Consequently, we\ncan opt to selectively substitute the original chunks with the output of these pre-trained models to\nensure the preservation of the underlying topic structure. The semantic compressed text can be\nforwarded to the LLM directly or in combination with other extension schemes to further enhance\nthe overall outcome.\n4\nFigure 3: Example of synthetic prompt for the passkey retrieval task (Mohtashami & Jaggi, 2023). The pre-\ntrained LLM is incapable of processing long input due to the context length constraint. By applying semantic\ncompression, the redundant information in the long document is removed, and the compressed input retains\nessential key information. The LLM can then process the compressed input along with the prompt to generate\nthe accurate answer. Notably, the distinct colors used in the illustration correspond to topic-based chunks.\n4\nEXPERIMENTS\nWe demonstrate that the proposed method of semantic compression can effectively extend the con-\ntext window by up to 7-8 times without modifying the parameters of the pre-trained models. Fur-\nthermore, the semantic compression module can be seamlessly integrated with existing methods,\nallowing for further extension of the context window. This versatility enables our approach to be\nadapted and combined with other techniques, enhancing the overall performance and flexibility. To\nevaluate the performance of our method, we conduct experiments on several language tasks that re-\nquire understanding of long contexts. These tasks include passkey retrieval, single-document ques-\ntion answering, multi-document question answering, summarization, and few-shot learning. In each\ntask, the model is provided with a sequence of context C (typically lengthy texts) and a sequence\nof text Q (e.g., a prompt), and it is expected to generate the output answer A. Additionally, we also\ninvestigate the perplexity metric (Peng et al., 2023), which measures the model\u2019s ability to predict\nthe text and serves as an indicator of the fluency of the generated output. This analysis allows us to\nassess not only the effectiveness but also the quality of the generated output.\n4.1\nTASKS AND DATASETS\nWe begin by evaluating the proposed semantic compression method on various standard benchmark\ntasks, utilizing the pre-trained 7B LLaMA model (Touvron et al., 2023). The original context win-\ndow size of this model is 4096. The tasks and datasets employed in our evaluation are sourced from\nthe SCROLLS benchmark (Shaham et al., 2022) and LongBench (Bai et al., 2023). These datasets\nprovide comprehensive and diverse contexts for our analysis.\nPasskey Retrieval\nRetrieval has been an important application of LLMs. We evaluate the pro-\nposed method using a synthetic task for passkey retrieval introduced by Mohtashami & Jaggi (2023),\nwhere prompts are synthesized to conceal a generated passkey within a randomly chosen section of\na long document. The passkey retrieval task assesses the model\u2019s capacity to extract important in-\nformation from any position within lengthy contexts. An illustration of the task is shown in Fig. 3.\nThe synthetic long text incorporates the passkey digits, and the task for the LLM is to retrieve these\ndigits from the input text. Further specifics can be found in Appendix A.\n5\nGeneral NLP Tasks\nLongBench (Bai et al., 2023) is a multi-task benchmark designed for long\ntext scenarios, consisting of six distinct tasks. In this study, we focus on the three English tasks from\nthe set of four natural language tasks, namely single-document question answering, multi-document\nquestion answering, summarization, and few-shot learning. Each of the selected datasets contains\n200 instances. Further information can be found in Appendix A.\nFluency\nWe evaluate the fluency of our semantic compression method using the perplexity score,\nwhich is defined as the exponential of the average negative log-likelihood of the probabilistic model\nP on the distribution D, i.e.,\nPPL(D, P) := exp(\u2212Ex\u2208D log P(x)).\nA smaller perplexity score indicates more fluent sequences that are consistent with the model.\n4.2\nBASELINES\nWe choose SoTA solutions from each mainstream approach as our baselines.\nFixed-size chunking\nTo accommodate long context within a fixed-size context window, chunking\nis a straightforward yet efficient approach. In NLP related applications, large pieces of text are\nusually broken down into smaller segments for targeted applications. When the input length exceeds\nthe context window, the fixed-size chunking method (Bai et al., 2023) truncates the input sequence\nfrom the middle. This is because the most significant information typically resides at the beginning\nand end of the sequence.\nInterpolation-based method\nYaRN (Peng et al., 2023) is a computationally efficient method for\ninterpolating position encoding, which dynamically adjusts the Relative Positional Encoding (RoPE)\nover dimensions and scales the attention. YaRN offers multiple length-extended models for different\nversions of Llama2, with the models being trained on a total of 64 GPUs from 8 \u00d7 A100 machines.\nIn order to ensure a fair comparison, we choose the model based on Llama2 7B, adjusted from 4k to\n64k, as our baseline.\nFine-tuning approach\nLongLoRA (Chen et al., 2023b) is an efficient approach for fine-tuning\nthat combines LoRA and shifts sparse attention to reduce computational costs. LongLoRA applies\nthis technique to Llama2 models of different sizes, ranging from Llama2 7B, Llama2 13B, to Llama2\n70B, with token lengths extended from 4k to 32k on a single 8 \u00d7A100 device. In order to ensure\na fair and unbiased comparison, we choose the Llama2 7B model with context extension achieved\nthrough improved LoRA fine-tuning as our baseline.\n5\nRESULTS\nWe report the main results along with a comprehensive analysis.\nFluency\nWe utilize the Llama2 model as our baseline to evaluate the fluency of generated texts by\ncalculating the perplexity (PPL) score. Samples from the GovReport dataset are selected at varying\nlengths, and the reference texts are compared to the generated texts during the computation. In cases\nwhere the length of the input text exceeds the context window of Llama2, our semantic compression\nmodule shortens the input, thereby allowing the model to continue generating new content fluently.\nThe resulting scores are depicted in Fig. 4. The plots indicate that the perplexity of Llama2 initially\ndecreases, but once it surpasses the window length, it rapidly increases. However, when our semantic\ncompression method is employed, the PPL remains consistently low. This suggests that our approach\nsuccessfully extends the context window up to three times without compromising the generation\nquality of the language model.\nPasskey Retrieval\nWe present the results of the passkey retrieval task in Fig. 5. When employing\nLlama2 for passkey retrieval, we observe a rapid drop in accuracy to zero once the input length\nsurpasses the window size of 4096. However, by utilizing our method, the retrieval accuracy of\nthe Llama2 model remains above 90% even for inputs with lengths of up to 30,000. This indicates\n6\n0\n2000\n4000\n6000\n8000\n10000\n12000\nLength\ne2\ne4\ne6\ne8\ne10\nPPL\nours\nllama2\nFigure 4: Perplexity on the GovReport dataset was\nevaluated at different sequence lengths. The perplex-\nity curves of Llama2 (green) and our method (pur-\nple) exhibit similar trends for sequences up to 4k in\nlength. However, as the sequence length exceeds the\ntraining length of 4k, our method effectively flattens\nthe perplexity curve, indicating that fluency is pre-\nserved for longer sequences.\n0\n10000\n20000\n30000\n40000\n50000\n60000\nLength\n0\n20\n40\n60\n80\n100\nAcc\nllama2\nours+llama2\nours+yarn+llama2\nFigure 5: Comparison between model variants on\nthe passkey retrieval task. The retrieval accuracy of\nthe Llama2 baseline (green) drops to zero at about\n5k due to out-of-memory issues. Our method (pur-\nple) successfully extends the length to 30k. More-\nover, when combined with SoTA extrapolation-based\nmethod YaRN, the context length can be further ex-\ntended to over 60k ensuring that the retrieval accuracy\nremains consistently above 90%.\nthat the semantic compression method extends the context window size of the language model by\napproximately 7-8 times. Furthermore, we combine our method with the SoTA interpolation-based\nmethod, YaRN, to further expand the context window size to up to 60,000, while consistently main-\ntaining an accuracy above 90%.\nGeneral NLP Tasks\nWe present our results on various general NLP tasks in Table 1, including\nsingle-document question answering, multi-document question answering, summarization, and few-\nshot learning. When the token length is less than 4k, there is no need to compress the context, and our\nmethod performs at the same level as the original Llama2 model. However, both the interpolation-\nbased method YaRN and the fine-tuning approach LongLora negatively impact the performance of\nthe Llama2 model across almost all tasks. In the 4k-8k range, our method outperforms others in 8\nout of 11 tasks. It is worth noting that our model performs slightly worse in the few-shot learning\ntask. This can be attributed to the fact that few-shot learning necessitates more detailed information,\nwhereas our compression scheme maintains information within a fixed window. Moving on to the\n8k-16k range, our method achieves the best results in 9 out of 12 tasks, exhibiting similar perfor-\nmance to the 4k-8k range. In the 16k-32k range, our method outperforms others in 6 out of 11 tasks.\nIn the 32k+ range, other methods fail due to out-of-memory issues, while our method still maintains\n70% of the performance achieved in the 4k range.\n6\nCONCLUSION\nIn this work, we propose a novel approach to addressing the limitation of input length in large\nlanguage models using semantic compression. By leveraging the statistical properties of natural\nlanguage and exploiting redundancy in communication, we are able to significantly shorten texts\nwhile preserving their semantic meaning. This allows for a 6-8 time extension of the context win-\ndow without the need for modifying the parameters of the pre-trained model or incurring addi-\ntional computational costs. Furthermore, the implementation of our semantic compression module\nis straightforward and can be easily integrated into other interpolation-based methods and black box\nAPIs. This provides flexibility and adaptability to different downstream tasks, considering practical\nconstraints such as time and memory resources. We believe our work can lead to simpler context\nwindow extension method to be used in practice, thereby reducing the cost of large language models.\n7\nTask\nDataset\n(length)\nMethod\nLong\nLoRA\nLong\nLoRA (4k) yarn\nyarn\n(4k)\nours\nours\n(4k)\n4k\n4k-8k\nSingle-Doc QA\nNarrativeQA\n-\n-\n-\n-\n-\n-\n18.7\nQasper\n11.6\n11.8\n13.4\n12.1\n23.4\n30.6\n19.2\nMultiFieldQA-en\n24.5\n13.2\n34.9\n32.9\n37.4\n58.7\n36.8\nMulti-Doc QA\nHotpotQA\n11.5\n8.3\n11.3\n22.6\n50.6\n50.0\n25.4\n2WikiMultihopQA 10.1\n10.6\n8.9\n14.4\n29.8\n61.8\n32.8\nMuSiQue\n10.0\n-\n21.1\n-\n50.0\n-\n9.4\nSummarization\nGovReport\n24.7\n28.9\n28.8\n35.0\n31.8\n32.2\n27.3\nQMSum\n20.3\n17.0\n22.8\n18.7\n21.1\n22.2\n20.8\nMutiNews\n0.0\n0.0\n1.2\n18.9\n23.2\n27.8\n25.8\nFew-Shot\nLearning\nTREC\n65.8\n54.2\n70.9\n50.0\n55.7\n54.2\n61.5\nTriviaQA\n87.6\n80.6\n90.9\n88.9\n83.3\n75.0\n77.8\nSAMSum\n43.1\n40.8\n40.4\n39.9\n41.6\n43.3\n40.7\n8k-16k\nSingle-Doc QA\nNarrativeQA\n9.2\n-\n13.9\n-\n19.6\n-\n18.7\nQasper\n-\n11.8\n10.3\n12.1\n20.9\n30.1\n19.2\nMultiFieldQA-en\n22.5\n13.2\n18.9\n32.9\n35.9\n58.7\n36.8\nMulti-Doc QA\nHotpotQA\n8.9\n8.3\n8.7\n22.6\n28.1\n50.0\n25.4\n2WikiMultihopQA 9.5\n10.6\n9.9\n14.4\n26.3\n61.8\n32.8\nMuSiQue\n6.1\n-\n4.2\n-\n16.8\n-\n9.4\nSummarization\nGovReport\n24.0\n28.9\n25.1\n35.0\n27.3\n32.2\n27.3\nQMSum\n22.5\n17.0\n21.8\n18.7\n23.4\n22.2\n20.8\nMutiNews\n0.0\n0.0\n0.0\n18.9\n22.0\n27.8\n25.8\nFew-Shot\nLearning\nTREC\n80.4\n54.2\n77.3\n50.0\n57.7\n54.2\n61.5\nTriviaQA\n86.5\n80.6\n89.1\n88.9\n78.7\n75.0\n77.8\nSAMSum\n44.5\n40.8\n43.8\n39.9\n41.7\n43.3\n40.7\n16k-32k\nSingle-Doc QA\nNarrativeQA\n12.4\n-\n8.6\n-\n9.8\n-\n18.7\nQasper\n-\n11.8\n9.2\n12.1\n15.2\n30.1\n19.2\nMultiFieldQA-en\n36.5\n13.2\n32.6\n32.9\n23.6\n58.7\n36.8\nMulti-Doc QA\nHotpotQA\n9.3\n8.3\n10.1\n22.6\n25.7\n50.0\n25.4\n2WikiMultihopQA 7.9\n10.6\n10.7\n14.4\n30.4\n61.8\n32.8\nMuSiQue\n5.4\n-\n5.0\n-\n14.6\n-\n9.4\nSummarization\nGovReport\n24.7\n28.9\n26.4\n35.0\n25.4\n32.2\n27.3\nQMSum\n20.0\n17.0\n20.8\n18.7\n21.2\n22.2\n21.5\nMutiNews\n0.3\n0.0\n0.3\n18.9\n21.1\n27.8\n26.4\nFew-Shot\nLearning\nTREC\n-\n54.2\n-\n50.0\n-\n54.2\n61.5\nTriviaQA\n88.8\n80.6\n90.1\n88.9\n81.1\n75.0\n77.8\nSAMSum\n44.7\n40.8\n43.6\n39.9\n39.4\n43.3\n40.7\n32k+\nSingle-Doc QA NarrativeQA\noom\n-\noom\n-\n19.0\n-\n18.7\nSummarization GovReport\noom\n28.9\noom\n35.0\n21.7\n32.2\n27.3\nQMSum\noom\n17.0\noom\n18.7\n22.4\n22.2\n21.5\nTable 1: Comparison of our semantic compression method with other baseline methods on a variety of tasks\nfrom the LongBench dataset. Method (4k) denotes evaluation results on texts shorter than 4k. The last column,\nlabeled 4k, showcases the performance of the Llama2-7B-chat-4k baseline. Notably, our method consistently\noutperforms or achieves similar results compared to other SoTA length extension methods.\n8\nREFERENCES\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh, Am-\nbrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length generalization\nin large language models. Advances in Neural Information Processing Systems, 35:38546\u201338556,\n2022.\nYushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du,\nXiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long\ncontext understanding. arXiv preprint arXiv:2308.14508, 2023.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer.\narXiv:2004.05150, 2020.\nPENG Bo.\nBlinkdl/rwkv-lm:\n0.01, August 2021.\nURL https://doi.org/10.5281/\nzenodo.5196577.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window\nof large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a.\nYukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora:\nEfficient fine-tuning of long-context large language models. arXiv, 2023b.\nAlexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen.\nAdapting language\nmodels to compress contexts.\nArXiv, abs/2305.14788, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:258865249.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov.\nTransformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computational Linguistics, pp. 2978\u20132988, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL\nhttps://aclanthology.org/P19-1285.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness. In Advances in Neural Information Process-\ning Systems, 2022.\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A Smith, and Matt Gardner. A dataset\nof information-seeking questions and answers anchored in research papers. In Proceedings of the\n2021 Conference of the North American Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, pp. 4599\u20134610, 2021.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, and Furu Wei.\nLongnet: Scaling transformers to 1,000,000,000 tokens. In Proceedings of the 10th International\nConference on Learning Representations, 2023.\nAlexander Richard Fabbri, Irene Li, Tianwei She, Suyi Li, and Dragomir Radev. Multi-news: A\nlarge-scale multi-document summarization dataset and abstractive hierarchical model. In Pro-\nceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 1074\u2013\n1084, 2019.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. Constructing a multi-\nhop QA dataset for comprehensive evaluation of reasoning steps. In Donia Scott, Nuria Bel,\nand Chengqing Zong (eds.), Proceedings of the 28th International Conference on Computational\nLinguistics, pp. 6609\u20136625, Barcelona, Spain (Online), December 2020. International Com-\nmittee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.580. URL https:\n//aclanthology.org/2020.coling-main.580.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for\nlong document summarization. In Kristina Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek\nHakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou\n(eds.), Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pp. 1419\u20131436, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.112. URL\nhttps://aclanthology.org/2021.naacl-main.112.\n9\nYunpeng Huang, Jingwei Xu, Zixu Jiang, Junyu Lai, Zenan Li, Yuan Yao, Taolue Chen, Lijuan\nYang, Zhou Xin, and Xiaoxing Ma. Advancing transformer architecture in long-context large\nlanguage models: A comprehensive survey, 2023.\nTom\u00b4a\u02c7s Ko\u02c7cisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00b4abor Melis,\nand Edward Grefenstette. The narrativeqa reading comprehension challenge. Transactions of the\nAssociation for Computational Linguistics, 6:317\u2013328, 2018.\nXin Li and Dan Roth. Learning question classifiers. In COLING 2002: The 19th International\nConference on Computational Linguistics, 2002.\nAmirkeivan Mohtashami and Martin Jaggi. Landmark attention: Random-access infinite context\nlength for transformers. arXiv preprint arXiv:2305.16300, 2023.\nBowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window\nextension of large language models, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,\nMor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language se-\nquences. arXiv preprint arXiv:2201.03533, 2022.\nWilliam Strunk Jr. The Elements of Style. Penguin, 2007.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot, and Ashish Sabharwal. MuSiQue: Mul-\ntihop questions via single-hop question composition.\nTransactions of the Association for\nComputational Linguistics, 10:539\u2013554, 2022.\ndoi: 10.1162/tacl a 00475.\nURL https:\n//aclanthology.org/2022.tacl-1.31.\nQingyue Wang, Liang Ding, Yanan Cao, Zhiliang Tian, Shi Wang, Dacheng Tao, and Li Guo. Recur-\nsively summarizing enables long-term dialogue memory in large language models. arXiv preprint\narXiv:2308.15022, 2023.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-\nattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neu-\nral Information Processing Systems, 33:5776\u20135788, 2020.\nDavid Wingate, Mohammad Shoeybi, and Taylor Sorensen. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction in language models. In Findings of the As-\nsociation for Computational Linguistics: EMNLP 2022, pp. 5621\u20135634, Abu Dhabi, United Arab\nEmirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nfindings-emnlp.412.\nURL https://aclanthology.org/2022.findings-emnlp.\n412.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Conference on Empirical Methods in Natural Language Processing (EMNLP),\n2018.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, et al. Qmsum: A new benchmark for query-based multi-\ndomain meeting summarization. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n5905\u20135921, 2021.\nWangchunshu Zhou, Yuchen Eleanor Jiang, Peng Cui, Tiannan Wang, Zhenxin Xiao, Yifan Hou,\nRyan Cotterell, and Mrinmaya Sachan. Recurrentgpt: Interactive generation of (arbitrarily) long\ntext, 2023.\n10\nGeorge Kingsley Zipf. Human Behavior and the Principle of Least Effort: An Introduction to Human\nEcology. Ravenio Books, 2016.\nA\nDATASETS\nSingle-Doc QA\n\u2022 NarrativeQA (Ko\u02c7cisk`y et al., 2018) is a standard question-answering dataset that includes\nbooks from Project Gutenberg3 and movie screenplays from a list of websites. Question-\nanswer pairs were provided by annotators, so that each of the 1,567 books and scripts has\nabout 30 questions and answers, and two reference answers are given for each question.\n\u2022 Qasper (Dasigi et al., 2021) is a question-answering dataset of NLP publications contain-\ning abstractive, extractive, and yes/no questions.\n\u2022 MultiFieldQA-en (Bai et al., 2023) is a dataset created from multiple sources including\nlegal documents, government reports, encyclopedias, and academic publications. Doctoral\nstudents were requested to annotate each article\u2019s queries and responses.\nMulti-Doc QA\n\u2022 HotpotQA (Yang et al., 2018) includes many 2-hop questions written by native speakers\nbased on two related paragraphs.\n\u2022 2WikiMultihopQA (Ho et al., 2020) involves up-to 5-hop questions systematacially con-\nstructed by manual templates. Answering these questions requires reasoning paths and can\nnot be solved by local content.\n\u2022 MuSiQue(Trivedi et al., 2022) consists of up to 4-hop questions and removes shortcuts and\nnaturalness questions. Each question contains 2-4 supplement paragraphs which present the\nreasoning path and related paragraphs.\nSummarization\n\u2022 GovReport (Huang et al., 2021) collects detailed reports containing human-written sum-\nmaries from the U.S. Government Accountability Office and Congressional Research Ser-\nvice. These reports span a wide variety of national policy issues.\n\u2022 QMSum (Zhong et al., 2021) contains annotated meeting-summary pairs across many do-\nmains including including product, academic, and committee meetings.\n\u2022 MultiNews(Fabbri et al., 2019) is a multi-document summarization dataset. (Bai et al.,\n2023) cluster 2-10 news articles discussing the same event or topic, each paired with a\nhuman-written summary and form a new long text summarization task.\nFew-Shot Learning\nTo construct few-shot learning with long text, (Bai et al., 2023) select a range\nof training examples in the following datasets to concatenate the context in LongBench.\n\u2022 TREC (Li & Roth, 2002) is a classification dataset with fine-grained class label.\n\u2022 TriviaQA (Zhong et al., 2021) is a classification dataset and involves messenger-like con-\nversations with human-written summaries.\n\u2022 SAMSum (Fabbri et al., 2019) reading comprehension dataset and consists of question-\nanswer pairs annotated with evidence passages.\nPasskey\nThe randomly generated prompts of the passkey retrieval task is in the format of Fig. 6.\nB\nIMPLEMENTATION DETAILS\nIn this section, we provide details of our algorithm implementation. Our algorithm utilizes sev-\neral mature open-source models. For graph representation, we make use of the sentence simi-\nlarity models all-MiniLM-L6-v2 provided by the Sentence Transformer platform, which can be\n11\nThere is an important info hidden inside a lot of irrelevant \ntext. Find it and memorize them. I will quiz you about the \nimportant information there. The grass is green. The sky is \nblue. The sun is yellow. Here we go. There and back again. \n(Repeat X Times) The grass is green. The sky is blue. The \nsun is yellow. Here we go. There and back again. The pass \nkey is 0000 Remember it. 0000 is the pass key. The sun is \nyellow. Here we go. There and back again (Repeat X Times) \nThe grass is green. The sky is blue. The sun is yellow. Here \nwe go. What is the pass key? The pass key is\nFigure 6: The query prompt contains task descriptions, redundant information, passkey information, redundant\ninformation, and query information. The passkey information is randomly placed within the text, while the\nremaining space up to a specified length is filled with redundant information.\nfound at the following link: https://huggingface.co/sentence-transformers/\nall-MiniLM-L6-v2. For semantic compression, we employ the pre-trained model distilbart-\ncnn-12-61. In most of our experiments, we utilize Llama2-7B-chat-4k as the base large language\nmodel (Touvron et al., 2023). The experiments were conducted on a single A40 GPU with 48GB\nmemory.\nC\nCOMPLEXITY\nGiven a context with length L, the origin complexity is O(L2). Considering the length limitations of\nthe compression module, we assume it has a minimum input length \u03b31 and a maximum input length\n\u03b32. We denote the compression ratio as \u03b1. Our method utilizes a divide-and-conquer strategy,\ndividing the long text into chunks where the total length is represented as L = l1 + \u00b7 \u00b7 \u00b7 + lk, and\neach chunk\u2019s length, li, satisfies the condition \u03b31 \u2264 li \u2264 \u03b32. By k\u03b31 \u2264 L, we can bound the\ncomplexity of the compression module\nk\nX\ni=1\nl2\ni \u2264\nk\nX\ni=1\n\u03b32\n2 = k\u03b32\n2 \u2264 \u03b32\n2\n\u03b31\nL.\n(1)\nThe complexity of inferring the compressed context is\n(\nk\nX\ni=1\n\u03b1li)2 = (\u03b1\nk\nX\ni=1\nli)2 = \u03b12L2.\n(2)\nThus the main complexity of our algorithms can be bounded by \u03b32\n2\n\u03b31 L + \u03b12L2.\nThe result suggests that our algorithm can reduce the computational complexity by a factor of the\nsquare of the compression ratio during the inference stage. The compression module exhibits linear\ngrowth and can be processed in parallel.\n1Available at: https://huggingface.co/sshleifer/distilbart-cnn-12-6\n12\n"
  },
  {
    "title": "Stable Score Distillation for High-Quality 3D Generation",
    "link": "https://arxiv.org/pdf/2312.09305.pdf",
    "upvote": "7",
    "text": "Stable Score Distillation for High-Quality 3D Generation\nBoshi Tang1,2* \u2020 , Jianan Wang1* \u2021 , Zhiyong Wu2, Lei Zhang1\n1International Digital Economy Academy (IDEA)\n2Tsinghua University\nAbstract\nAlthough Score Distillation Sampling (SDS) has exhib-\nited remarkable performance in conditional 3D content\ngeneration, a comprehensive understanding of its formula-\ntion is still lacking, hindering the development of 3D gen-\neration. In this work, we decompose SDS as a combina-\ntion of three functional components, namely mode-seeking,\nmode-disengaging and variance-reducing terms, analyzing\nthe properties of each.\nWe show that problems such as\nover-smoothness and implausibility result from the intrin-\nsic deficiency of the first two terms and propose a more\nadvanced variance-reducing term than that introduced by\nSDS. Based on the analysis, we propose a simple yet effec-\ntive approach named Stable Score Distillation (SSD) which\nstrategically orchestrates each term for high-quality 3D\ngeneration and can be readily incorporated to various 3D\ngeneration frameworks and 3D representations. Extensive\nexperiments validate the efficacy of our approach, demon-\nstrating its ability to generate high-fidelity 3D content with-\nout succumbing to issues such as over-smoothness.\n1. Introduction\n3D content creation plays a crucial role in shaping the hu-\nman experience, serving practical purposes such as the real-\nworld production of everyday objects and the construction\nof simulated environments for immersive applications such\nas video games, AR/VR, and more recently, for training\nagents to perform general tasks in robotics. However, tra-\nditional techniques for 3D content generation are both ex-\npensive and time-consuming, requiring skilled artists with\nextensive 3D modeling knowledge. Recent advancements\nin generative modelling have sparked a surge of interest in\nimproving the accessibility of 3D content creation, with the\ngoal to make the process less arduous and to allow more\npeople to participate in creating 3D content that reflects\ntheir personal experiences and aesthetic preferences.\n3D generative modelling is inherently more complex\n*Equal contribution.\n\u2020Work done during an internship at IDEA.\n\u2021Corresponding author.\nthan 2D modelling, requiring meticulous attention to view-\nconsistent fine geometry and texture. Despite the increased\nintricacy, 3D data is not as readily available as its 2D im-\nage counterparts, which have propelled recent advances in\ntext-to-image generation [27\u201329]. Even with recent efforts\nof Objaverse [5], the uncurated 3D data only amounts to\n10 million instances, in sharp contrast to the vast 5 billion\nimage-text pairs available [30]. As a result, utilizing 2D su-\npervision for 3D generation has emerged as a prominent re-\nsearch direction for text-to-3D generation. Notably, Score\nDistillation Sampling (SDS) [24, 38] is proposed to opti-\nmize a 3D representation so that its renderings at arbitrary\nviews are highly plausible as evaluated by a pre-trained text-\nto-image model, without requiring any 3D data. As pre-\nsented in DreamFusion [24], SDS enables the production\nof intriguing 3D models given arbitrary text prompts, but\nthe results tend to be over-smooth and implausible (e.g.,\nfloaters). Subsequent works build upon SDS and enhance\nthe generation quality with improvements in training prac-\ntices. This is most effectively achieved by adopting higher\nresolution training, utilizing larger batch sizes, and imple-\nmenting a coarse-to-fine generation approach with mesh op-\ntimization for sharper generation details [3, 17, 39]. While\nSDS remains fundamental and ubiquitously used for 3D\ngeneration, its theoretical understanding remains obscured\nand requires further exploration.\nIn this paper, we make extensive efforts to resolve the\nissues above, our contributions can be summarized as fol-\nlows:\n\u2022 We offer a comprehensive understanding of Score Dis-\ntillation Sampling (SDS) by interpreting the noise resid-\nual term (referred to as the SDS estimator) as a combina-\ntion of three functional components: mode-disengaging,\nmode-seeking and variance-reducing terms. Based on our\nanalysis, we identify that the over-smoothness and im-\nplausibility problems in 3D generation arise from intrin-\nsic deficiency of the first two terms. Moreover, we iden-\ntify the main training signal to be the mode-disengaging\nterm.\n\u2022 We show that the variance reduction scheme introduced\nby SDS incurs scale and direction mismatch problems,\nand propose a more advanced one.\n\u2022 We propose Stable Score Distillation (SSD), which\n1\narXiv:2312.09305v2  [cs.CV]  7 Feb 2024\nutilizes a simple yet effective term-balancing scheme\nthat significantly mitigates the fundamental deficiencies\nof SDS, particularly alleviating issues such as over-\nsmoothness and implausibility.\n\u2022 We establish meaningful connections between our analyt-\nical findings and prevalent observations and practices in\noptimization-based 3D generation, such as the adoption\nof a large CFG scale and the generation of over-smoothed\nand floater-abundant results. These connections provide\nvaluable perspectives for enhancing the optimization pro-\ncess of 3D generation.\n\u2022 Extensive experiments demonstrate that our proposed\nmethod significantly outperforms baselines and our im-\nprovement is compatible with existing 3D generation\nframeworks and representations. Our approach is capa-\nble of high-quality 3D generation with vibrant colors and\nfine details.\n2. Related Works\nText-to-image Generation.\nText-to-image models such\nas GLIDE [21], unCLIP [27], Imagen [29], and Stable Dif-\nfusion [28] have demonstrated impressive performance in\ngenerating high-quality images. The remarkable advance-\nment is attributed to improvement in generative modeling\ntechniques, particularly diffusion models [6, 22, 34]. The\ncapability of generating diverse and creative images given\narbitrary text is further enabled by the curation of large web\ndatasets comprising billions of image-text pairs [2, 30, 31].\nRecently, generating varied viewpoints of the same subject,\nnotably novel view synthesis (NVS) from a single image\nhas made significant progresses [18, 19, 32, 33, 40, 42] by\nfine-tuning pre-trained diffusion models on renderings of\n3D assets, learning to condition the generation on camera\nviewpoints. NVS models can be readily applied to enhance\nimage-to-3D generation, orthogonal to our improvements.\nDiffusion-guided 3D Generation.\nRecently, DreamFu-\nsion [24] and SJC [38] propose to generate 3D content\nby optimizing a differentiable 3D representation so that\nits renderings at arbitrary viewpoints are deemed plausible\nby 2D diffusion priors. Such methods are commonly re-\nferred to as the optimization-based 3D generation. Subse-\nquent works enhance the generation quality by utilizing 3D-\naware diffusion models for image-to-3D generation [15, 20,\n26, 36]; adopting a coarse-to-fine generation approach with\nimproved engineering practices such as higher-resolution\ntraining and mesh optimization [3, 17]; exploring timestep\nscheduling [11]; or introducing additional generation pri-\nors [1, 12, 13, 16, 44]. ProlificDreamer [39], along with re-\ncent works of NFSD [14] and CSD [43] concurrent to ours\nprovide new perspectives on SDS. however, there remains a\ngap in a comprehensive understanding of the SDS formula-\ntion, which is the focus of this work.\n3. Background\nIn this section, we provide the necessary notations, as\nwell as the background on optimization-based 3D genera-\ntion including diffusion models, Classifier-Free Guidance\n(CFG) [9] and Score Distillation Sampling (SDS) [24].\n3.1. Notations\nThroughout this paper, x, or equivalently x0, is used to de-\nnote a natural image drawn from the distribution of images\nP(x). Given a 3D model parameterized by \u03b8, a volumetric\nrenderer g renders an image g(\u03b8, c) according to \u03b8 and the\ncamera pose c. t denotes the diffusion timestep \u2208 (1, T).\n3.2. Diffusion Models\nDiffusion models assume a forward process where we\ngradually corrupt a sample by consecutively adding small\namount of Gaussian noise in T steps, producing a se-\nquence of noisy samples x1, x2, . . . , xT , where P(xt|x) =\nN(xt; \u03b1tx, \u03c32\nt I), whose mean and variance are controlled\nby pre-defined \u03b1t and \u03c3t, respectively. A denoiser \u02c6\u03f5\u03d5 pa-\nrameterized by \u03d5 is trained to perform the backward process\nby learning to denoise noisy images xt = \u03b1tx + \u03c3t\u03f5 [10]:\n\u03d5\u2217 = argmin\u03d5E\u03f5,t,y[|| \u02c6\u03f5\u03d5(xt, t, y) \u2212 \u03f5||2],\n(1)\nwhere \u03f5 \u223c N(0, I), and y is an user-defined condition\nwhich is usually a text prompt describing x. After training,\none can generate an image x\u2032 \u223c P(x|y) by initiating with\nxT \u223c N(0, I) and gradually denoise it with \u02c6\u03f5\u03d5. Diffusion\nmodels can also be interpreted from the view of denoising\nscore matching [35]. Note that for each t, the noise term\n\u03f5 = \u2212\u03c3t\u2207xtlogP(xt|x, y, t). The property of denoising\nscore matching [37] readily leads us to:\n\u02c6\u03f5\u03d5(xt, t, y) = \u2212\u03c3t\u2207xtlogP(xt; y, t).\n(2)\nWith this property, in our work we use P\u03d5 and P inter-\nchangeably.\n3.3. Classifier-Free Guidance (CFG)\nClassifier-Free Guidance (CFG) [9] trades off diversity for\nhigher sample quality in the generation process of diffusion\nmodels. It has a hyperparameter called CFG scale, hereafter\ndenoted as \u03c9, and works as follows,\n\u02c6\u03f5\u03d5\nCF G(xt, t, y) = (1 + \u03c9) \u00b7 \u02c6\u03f5\u03d5(xt, t, y) \u2212 \u03c9 \u00b7 \u02c6\u03f5\u03d5(xt, t, \u2205),\nwhere \u2205 represents the null symbol.\n3.4. Score Distillation Sampling (SDS)\nSDS [24] is an optimization-based 3D generation method\nthat distills knowledge from pre-trained 2D diffusion mod-\nels to 3D representations. It minimizes a weighted proba-\nbility density distillation loss [23], namely LSDS(\u03d5, x =\n2\nA DSLR photo of the Imperial State Crown of England \nZombie JOKER, head, photorealistic, 8K, HDR \nA DSLR photo of a cat wearing armor\nA delicious hamburger\nFigure 1. 3D Gaussian generation from text prompts.\nA typewriter made out of vegetables  \nA covered wagon\nA Rugged, vintage-inspired hiking boots with a \nweathered leather finish \nA frazer nash super sport car  \nAn erupting volcano, aerial view \nAn amigurumi motorcycle  \nA chimpanzee dressed like Napoleon \nBonaparte \nA beautiful dress made out of garbage \nbags, on a mannequin \nDing censer with an openwork cover and \ndecorations of dragons\nFigure 2. NeRF generation from text prompts.\n3\nFantasia3D\nMagic3D\nProlificDreamer\nOriginal\n+SSD\nFigure 3. Incorporating SSD to existing 3D generation frameworks consistently improves generation quality.\nA plate piled high with chocolate chip cookies\nA 3D model of an adorable cottage with a thatched roof\nDreamFusion\nMagic3D\nFantasia3D\nProlificDreamer\nSSD (ours)\nFigure 4. Comparisons between SSD and SOTA methods on text-to-3D generation. Baseline results are obtained from their papers.\ng(\u03b8, c)) = Et[( \u03c3t\n\u03b1t w(t)KL(P(xt|x; y, t)||p\u03d5(xt; y, t))]:\n\u2207\u03b8LSDS(\u03d5, x) = [w(t)( \u02c6\u03f5\u03d5\nCF G(xt; y, t) \u2212 \u03f5 )\u2202x\n\u2202\u03b8 ], (3)\nwhere x = g(\u03b8, c) and w(t) re-scales the weights of gradi-\nents according to t, and p\u03d5(xt; y, t) is the distribution of xt\nimplicitly represented by the score function \u02c6\u03f5\u03d5. DreamFu-\nsion [24] introduces the \u03f5 term for variance reduction. We\ncall the resultant noise residual term highlighted in red the\nSDS estimator for short.\n4. Method\nWe start with problem setting in Sec. 4.1 and decompose\nthe SDS estimator to three functional terms as shown in\nEq. (4): the mode-disengaging term (denoted as h for\nits homogeneous form), the mode-seeking term and the\nvariance-reducing term . In Sec. 4.2, Sec. 4.3, and Sec. 4.4\nwe analyze the mathematical properties, numerical charac-\nteristics and intrinsic limitations of each term, respectively.\nBased on our analysis, we propose Stable Score Distillation\n(SSD) in Sec. 4.5 which efficiently utilizes the distinctive\nproperties of each term to augment each other for higher-\nquality 3D content generation.\n\u02c6\u03f5\u03d5\nCF G(xt; y, t) \u2212 \u03f5 = \u03c9 \u00b7 ( \u02c6\u03f5\u03d5(xt, t, y) \u2212 \u02c6\u03f5\u03d5(xt, t, \u2205))\n+ \u02c6\u03f5\u03d5(xt, t, y)\n\u2212 \u03f5\n(4)\n4\nA plush dragon toy\nA red fire hydrant spraying water \nA ripe strawberry\nAn icecream sundae\nA 20-sided die made out of glass\nA plush triceratops toy\nProlificDreamer\nvs. \nSSD\nMagic3D\nvs. \nSSD\nFantasia3D\nvs. \nSSD\nFigure 5. More comparisons between SSD and SOTA methods on text-to-3D generation with more diverse prompts. For each text prompt,\nbaseline results are obtained from theirs papers except for Fantasia3D, and presented on the left, while SSD results are shown on the right.\nM=0\n(no mode-seeking)\nM=100\nM=200\n(ours)\nM=300\nSDS\nno term-\nrescaling\nno mode-\ndisengaging\nFigure 6. Ablation study on the design choices of SSD. Better viewed when zoomed in.\n4.1. Problem Setting\nWe make a mild assumption that \u03b10 = 1, \u03c30 = 0; \u03b1T =\n0, \u03c3T = 1 in our theoretical analysis for simplicity.\n3D Generation with 2D Supervision. The objective of\n3D generation with 2D supervision is to create 3D assets\nwhose renderings align well with the provided prompts. In\nother words, a necessary condition for successful 3D gen-\neration is that the renderings g(\u03b8, c) of the trained 3D asset\n\u03b8 approach the modes of P(x; y). Note that this condition\nis necessary but not sufficient for 3D generation, lacking\nproper constraints for multi-view consistency.\nMode Consistency Hypothesis. Intuitively, an image that\nis plausible according to the conditional distribution, i.e.\naround a mode of P(x; y), should itself be a plausible im-\nage, namely around a mode of P(x). Therefore, we hypoth-\nesize that the modes of the conditional distribution Mcond\nform a subset of the modes of the unconditional distribution\nMuncond, formally Mcond \u2286 Muncond. We refer to the\nmodes in Muncond \u2212 Mcond as singular modes.\n4.2. Analyzing the Mode-Disengaging Term\n4.2.1\nMathematical Properties\nSubstituting Eq. (2) into the formulation of h, we ob-\ntain h = \u2212\u03c3t(\u2207xtlogP\u03d5(xt; y, t) \u2212 \u2207xtlogP\u03d5(xt; t)).\nWhen optimizing a 3D representation \u03b8, the sub-terms of h,\nnamely \u2212\u03c3t\u2207xtlogP\u03d5(xt; y, t) and \u03c3t\u2207xtlogP\u03d5(xt; y, \u2205),\nrespectively drives \u03b8 such that E\u03f5[xt] = \u03b1tx will: a) ap-\nproach the conditional modes of P\u03d5(xt; y, t), and b) dis-\ntance the unconditional modes of P\u03d5(xt; t) which are inde-\npendent of the conditional prompt (for this reason we call h\nmode-disengaging). We emphasize property b) to be crit-\nical, as it guides \u03b8 to avoid the over-smoothing problem\n5\ncaused by trapping points and transient modes, a concern\nencountered by the mode-seeking term (see Sec. 4.3 for\ndefinition and analysis). In Fig. 7 (left), we illustrate that\nthe mode-disengaging term has the capability to navigate\nout of the trapping point of the Mode-Seeking term induced\nfrom its transient modes, thanks to the disengaging force\nthat avoids unconditional modes. This capability is crucial\nfor effective 3D generation. Considering that in Eq. (4),\nonly the mode-disengaging and mode-seeking terms pro-\nvide supervision signals (note that E\u03f5[\u03f5] = 0), our analysis\npositions h as the main supervision signal in 3D generation.\nOur analysis provides theoretical explanation to a concur-\nrent work, CSD [43], which also claims h to be the main\nsupervision signal.\n4.2.2\nIntrinsic Limitation\nBased on previous discovery, one may be tempted to em-\nploy h as the sole supervision signal in Eq. (3) for 3D gen-\neration, akin to CSD [43]. However, our experiments re-\nveal a persistent issue of color-saturation in even 2D con-\ntent generation when relying solely on h (SSD, M=0 in\nFig. 20). To delve deeper into this issue, we conduct ex-\nperiments with varying values of t and identify its par-\nticular association with small t, e.g., t < 200 (see App.\nFig. 15). Note that as t \u2192 0, P\u03d5(xt; y, t) \u2192 P\u03d5(x; y, t)\nand P\u03d5(xt; \u2205, t) \u2192 P\u03d5(x; \u2205, t). As the term h seeks to\nmaximize P\u03d5(xt;y,t)\nP\u03d5(xt;\u2205,t), when t \u2192 0, the learning target of\nh gradually approaches maximizing P\u03d5(x;y)\nP\u03d5(x) . However, the\nmaximizing points for this ratio are often not around any\nmodes of P\u03d5(x; y), where P\u03d5(x) is also high according to\nthe mode consistency hypothesis. Worse still, they tend to\nbe not around any singular modes of P\u03d5(x), where P\u03d5(x) is\nhigh and P\u03d5(x; y) is low. In other words, when t \u2192 0, the\nmode-disengaging term discourages g(\u03b8, c) from converg-\ning to any mode of a natural image distribution P\u03d5(x), let\nalone to modes of P\u03d5(x; y), violating the necessary condi-\ntion of 3D generation as presented in Sec. 4.1. Therefore,\nspecial treatments are necessary to enable the 3D represen-\ntation to converge to modes of P\u03d5(x; y) when t is small.\nWe illustrate this problem in Fig. 7 (left), where the mode-\ndisengaging trajectory converges to a point far from modes\nof P(x; y).\n4.3. Analyzing the Mode-Seeking Term\nIn light with the analysis in Sec. 4.2.2, we aim to guide\na 3D representation \u03b8 such that its rendering at arbitrary\nviewpoint seeks image distribution modes when t is small.\nTo achieve this, we employ \u02c6\u03f5\u03d5(xt, y, t) as a viable choice,\nwhich points to the nearest mode of P\u03d5(xt; y, t) according\nto Eq. (2), and conduct a thorough analysis of its properties.\n4.3.1\nNumerical Properties\nScale. Recall ||\u02c6\u03f5\u03d5(xt, y, t)|| = \u03c3t \u00b7 ||\u2207xtlogP\u03d5(xt; y, t)||.\nAssuming ||\u2207xtlogP\u03d5(xt; y, t)|| is bounded, one should\nexpect ||\u02c6\u03f5\u03d5(xt, y, t)|| to increase as t increases. We vali-\ndate this assumption numerically in Appendix Fig. 16.\nDirection. As t \u2192 T, \u2207xtlogP\u03d5(xt; y, t) gradually trans-\nforms from the score of conditional distribution of natural\nimages, i.e., \u2207xlogP\u03d5(x; y), to \u2207xT N(xT ; 0, I), which\nare independent of \u03f5 and collinear to \u03f5 respectively (proved\nin Sec. 10).\nTherefore the linear correlation between\n\u02c6\u03f5\u03d5(xt, y, t) and \u03f5 as defined in [25] is expected to increase\nas t increases. We validate this assumption in Fig. 17.\n4.3.2\nIntrinsic Limitations\nOver-smoothness. According to proof (A.4) in DreamFu-\nsion [24], the mode-seeking term drives P(xt|x; y, t) =\nN(\u03b1tx, \u03c32\nt I) to align with the high-density region of\np\u03d5(xt; y, t).\nNaturally, this means that during the 3D\noptimization process, this term directs the mean of the\nabove Gaussian distribution, namely \u03b1tx, towards modes of\nP\u03d5(xt; y, t). When t is small (e.g., \u2264 200), this behaviour is\nprecisely what we aim for since P\u03d5(xt; y, t) \u2248 P\u03d5(x; y, t)\nand the mode-seeking term guides \u03b8 such that its rendering\ng(\u03b8, c) converges towards the modes of P\u03d5(x; y, t). How-\never, for large values of t, the situation is more compli-\ncated. For clarity, let\u2019s consider two modes in P\u03d5(x; y, t)\nlocated at o1 and o2. Ideally, we would like x to converge\naround them. Equivalently, we would like \u03b1tx \u2248 \u03b1to1 or\n\u03b1tx \u2248 \u03b1to2 for any t. We call \u03b1to\u2217 as induced modes.\nHowever, when t gets high, \u03b1t is rather low.\nAnd now\nthe induced modes gets so close that they \u201dmelt\u201d into a\nsingle mode which lies between the induced ones. We as-\nsume such a mode is \u03b1totr, which we call transient mode.\nTrivially, this means otr lies between o1 and o2. Thus the\nmode-seeking term drives \u03b1tx to seek the transient modes\n\u03b1totr, instead of the induced ones as expected, when t is\nlarge. In other words, the 3D rendering x optimizes for\nx \u2192 otr. We call otr a trapping point in P\u03d5(x; y). Since\nthe timestep t is randomly sampled during optimization, the\noverall optimization process frequently revisits large t and\ngets x trapped at otr. This results in over-smoothness as x\nnow converges to a middle point of plausible image modes,\ntrying to \u201caverage\u201d the plausible contents. See Fig. 7 for\nthe visualization of the mode-seeking term causing model\nparameters to be trapped by transient modes.\nVariance. \u02c6\u03f5\u03d5(xt, y, t) exhibits high variance as pointed\nout by DreamFusion [24]. We attribute this issue to the\nhigh correlation between the estimator and \u03f5 as discussed\nin Sec. 4.3.1. To effectively employ \u02c6\u03f5\u03d5(xt, y, t) for enforc-\ning mode-seeking behaviour with small t, it is imperative to\nreduce the variance associated with this term.\nConnection with common observations and practices\n6\nin 3D content generation. We find that many observations\nand practices of various frameworks have non-trivial con-\nnections with the previous analysis. Interested ones can re-\nfer to Sec. 12 for details.\n4.4. Analyzing the Variance-Reducing Term\n4.4.1\nA Naive Approach\nConsider b = \u02c6\u03f5\u03d5(xt, y, t) \u2212 \u03f5, same as SDS with \u03c9 = 0. It\nmay initially seem intuitive that \u02c6\u03f5\u03d5(xt, y, t) \u2248 \u03f5 due to the\ndenoising nature of training \u02c6\u03f5\u03d5 and thus b induces low vari-\nance, which is also an unbiased estimator for \u02c6\u03f5\u03d5(xt, y, t) as\nE\u03f5[\u03f5] = 0. However, analysis from Sec. 4.3 reveals that\nthe scale and direction of the mode-seeking term are highly\ndependent on the diffusion timestep t and thus \u03f5 does not\nreduce variance always. For example, when t = 0 where\n\u03b1t = 1 and \u03c3t = 0, we have \u02c6\u03f5\u03d5 = 0 and the \u201cvariance\nreduction\u201d term in b is actually the only source of variance!\nWe illustrate this observation in Fig. 9. Consequently, it\nbecomes important to reduce the variance of \u02c6\u03f5\u03d5 through a\nmore meticulous design.\n4.4.2\nAdaptive Variance-Reduction\nWe aim to reduce the variance of \u02c6\u03f5\u03d5 more efficiently by\nconditioning the variance-reduction term on x and t: \u02c6\u03f5\u03d5 \u2212\nc(x, t)\u03f5, which remains an unbiased estimator of \u02c6\u03f5\u03d5 be-\ncause the introduced coefficient c(x, t) is independent of \u03f5,\nwhere c is optimally chosen to maximally reduce the vari-\nance:\nc(x, t) = argmink V ar[\u02c6\u03f5\u03d5 \u2212 k\u03f5].\n(5)\nBased on some mathematical treatments as detailed in\nSec. 8, we instead optimize the objective function with r,\nwhich is a computationally cheap proxy for the optimal c\nand defined as:\nr(xt, \u03f5, y, t) = \u02c6\u03f5\u03d5(xt, y, t) \u00b7 \u03f5\n||\u03f5||2\n.\n(6)\nFormally, we define a variance-reduced mode-seeking esti-\nmator as:\n\u02dc\u03f5\u03d5(xt, y, t) = \u02c6\u03f5\u03d5(xt, y, t) \u2212 r(xt, \u03f5, y, t) \u00b7 \u03f5.\n(7)\nIt is worth noting that Eq. (7) is no longer an unbiased esti-\nmator of \u02c6\u03f5\u03d5(xt, y, t) since r is now dependent on \u03f5. How-\never, in practice, r and c are quite close to each other and the\napproximation with Eq. (7) does not result in any noticeable\nperformance degradation. See Fig. 18 for numerical confir-\nmation.\n4.5. Augmenting the Mode-Disengaging Term with\nLow-Variance Mode-Seeking Behaviour\nWe summarize the 3D generation supervision terms and\ntheir behaviour under different regimes of t in Tab. 1.\ntimestep t\nMode-Disengaging\nMode-Seeking\nlarge\ntrap escaping\nover-smoothness\nsmall\nimplausibility (e.g., floaters)\nplausibility\nTable 1. Summarization of the supervision terms\u2019 behaviour under\ndifferent t regimes. The desired behaviours are emboldened.\nEvidently we should combine the merits of the mode-\ndisengaging and the mode-seeking terms under different\ntimestep regimes, which leads to our Stable Score Distil-\nlation (SSD) estimator. The design is quite simple: when\nt > M, where M \u2265 0 is a pre-defined timestep threshold,\nwe employ the mode-disengaging term for fast content for-\nmation. Otherwise we utilize the variance-reduced mode-\nseeking term as defined in Eq. (7) to guide the 3D model\u2019s\nrenderings towards plausible image distribution. Notably,\nwe further address the scale mismatch between the two\ncases by scaling the variance-reduced mode-seeking term\nto match the scale of the mode-disengaging one, inspired by\nthe scale mismatch between the terms as shown in Fig. 19.\nThe detailed algorithm is provided in Algorithm 1.\nAlgorithm 1 Stable Score Distillation (SSD) Estimator\n1: Input: diffusion timestep t\n2: Input: noise \u03f5 and corresponding noised rendering xt\n3: Input: generation condition, e.g., text prompt y\n4: Input: Timestep threshold M\n5: Input: pre-trained denoiser \u02c6\u03f5\u03d5, e.g., Stable Diffusion\n6: Output: SSD estimator to be used in place of SSD es-\ntimator as highlighted in Eq. (3)\n7: h = \u02c6\u03f5\u03d5(xt, t, y) \u2212 \u02c6\u03f5\u03d5(xt, t, \u2205)\n8: if t > M then\n\u25b7 fast geometry formation\n9:\nESSD = h\n10: else if t \u2264 M then\n\u25b7 high-density chasing\n11:\nCompute r(xt, \u03f5, y, t) according to Eq. (6)\n12:\n// variance-reduced score estimator\n13:\n\u02dc\u03f5\u03d5(xt; y, t) = \u02c6\u03f5\u03d5(xt; y, t) \u2212 r(xt, \u03f5, y, t) \u00b7 \u03f5\n14:\n// re-scaled score estimator\n15:\nESSD =\n||h||\n||\u02dc\u03f5\u03d5(xt;y,t)||\u02dc\u03f5\u03d5(xt; y, t)\n16: end if\n17: Return ESSD\n5. Experiments\n5.1. Implementation Details\nOur experiments are mainly conducted with the threestu-\ndio codebase [8]. To save vram usage, during the first 5000\nsteps the training resolution is 64. The batch size for low-\nresolution and high-resolution stages are 8 and 1 respec-\ntively. We employ Stable Diffusion [28] v2.1-base as our\nscore estimator \u02c6\u03f5\u03d5. We employ the ProgressBandHashGrid\nwith 16 levels implemented in the codebase as our 3D rep-\nresentation. The learning rates are 0.01 for the encoding\n7\nand 0.001 for the geometry networks. We conduct our ex-\nperiments with one A100 80GB on Ubuntu 22.04LTS. Each\nasset is trained for 15000 steps. Also, during the first 5000\nsteps we sample t from [20, 980] and anneal it to [20, 500]\nafterwards. The loss is SSD loss, plus the orient loss in\nthe codebase with a coefficient 1000. Note that our method\ndoes not need to tune CFG scales like previous works [24].\n5.2. Evaluation on Numerical Experiment\nFirst, we demonstrate the efficacy of our SSD as a general-\npurpose estimator for mode approximation with a simple\nnumerical experiment on a mixture-of-Gaussian distribu-\ntion. Note that the mixture-of-Gaussian distribution is a\ngeneral-purpose distribution approximator in that it can\napproximate any continuous distribution with any preci-\nsion [7]. Thus it suffices to evaluate our SSD on mixture-\nof-Gaussian. See Sec. 13 for details.\n5.3. Evaluation on text-to-3D Generation\nComparison with baselines. We compare SSD with SOTA\nmethods: DreamFusion [24], Fantasia3D [3], Magic3D [17]\nand ProlificDreamer [39] on text-to-3D generation and pro-\nvide the results in Fig. 4 and Fig. 5. SSD generates results\nthat are more aligned with the prompts (e.g., the plate gen-\neration in the cookie example), plausible in geometry (e.g.,\nthe plush dragon toy example) and color (e.g., compared to\nMagic3D), and delicate.\nHigh-quality 3D Content Generation. We evaluate the\ncapability of SSD with diverse text prompts. As shown in\nFig. 2 and Fig. 1, SSD is able to generate high-quality gen-\neral 3D objects and diverse effects. With different 3D rep-\nresentations, it is highly efficient.\nCompatibility with frameworks. In Fig. 3 and Fig. 8,\nwe show SSD\u2019s compatibility with existing 3D generation\nframeworks, no matter if they are general-purpose or spe-\ncialized. SSD can be readily incorporated into them for\nquality improvement.\nUser Studies. We compare SSD to the four outlined\nbaselines with user preference studies. Participants are pre-\nsented with five videos side by side, all generated from the\nsame text prompt.\nWe ask the users to evaluate on two\naspects: a) plausibility and b) details (existence of over-\nsmoothness). We randomly select 100 prompts from the\nDreamFusion gallery for evaluation, with each prompt as-\nsessed by 10 different users, resulting in a total of 1,000\nindependent comparisons. The result reveals a preference\nof 3D models generated with our method over other base-\nlines, 53.2% for more generation plausibility and 61.4% for\nfiner details.\n5.4. Ablation Study\nIn Fig. 6, Fig. 13, and Fig. 14, we assess all the designs of\nSSD. Removing of any of them can result to a general de-\ncrease in generation quality. Specifically, we see higher M\ncauses the result to be more smooth with fewer local details,\nas expected. The three terms, namely mode-seeking, mode-\ndisengaging and variance reduction terms are all necessary\nfor successful 3D generation. The variance reduction term\ncan accelerate training process and help the emergence of\nfine details even in the early stage of training. Also, without\nthe term rescaling mechanism, the training process suffers\nfrom the scale mismatch between mode-seeking and mode-\ndisengaging terms, which has been pointed out by our anal-\nysis, and produces blurred 3D assets. Particularly, a naive\ncombination of the three terms, like SDS, is inefficient and\ncauses blurs.\n6. Conclusion\nWe propose Stable Score Distillation (SSD) for high-\nquality 3D content generation, based on a comprehensive\nunderstanding of the widely used score distillation sam-\npling (SDS). We interpret SDS as a combination of mode-\ndisengaging, mode-seeking and variance-reducing terms,\nanalyzing their distinct properties. This interpretation al-\nlows us to harness each term to its fullest potential and\nto leverage their complementary nature. Our analysis es-\ntablishes rich connections to prevalent observations and\npractices in 3D content generation. Extensive experiments\ndemonstrate the effectiveness of our approach for generat-\ning high-fidelity 3D content without succumbing to issues\nsuch as over-smoothness and severe floaters.\n8\nReferences\n[1] Yukang Cao, Yan-Pei Cao, Kai Han, Ying Shan, and Kwan-\nYee K Wong. DreamAvatar: Text-and-Shape Guided 3D Hu-\nman Avatar Generation via Diffusion Models. arXiv preprint\narXiv:2304.00916, 2023. 2\n[2] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12M: Pushing Web-Scale Image-Text\nPre-Training To Recognize Long-Tail Visual Concepts. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 3558\u20133568, 2021. 2\n[3] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3D: Disentangling Geometry and Appearance for\nHigh-quality Text-to-3D Content Creation. arXiv preprint\narXiv:2303.13873, 2023. 1, 2, 8\n[4] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon\nKim, Hyunwoo Kim, and Sungroh Yoon. Perception Pri-\noritized Training of Diffusion Models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11472\u201311481, 2022. 3\n[5] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA Universe of Annotated 3D Objects.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 13142\u201313153, 2023. 1\n[6] Prafulla Dhariwal and Alexander Nichol. Diffusion Models\nBeat GANs on Image Synthesis. Advances in Neural Infor-\nmation Processing Systems, 34:8780\u20138794, 2021. 2\n[7] Ian J. Goodfellow, Yoshua Bengio, and Aaron Courville.\nDeep Learning. MIT Press, Cambridge, MA, USA, 2016.\nhttp://www.deeplearningbook.org. 8\n[8] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 7\n[9] Jonathan Ho and Tim Salimans.\nClassifier-free Diffusion\nGuidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffu-\nsion Probabilistic Models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 2\n[11] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. DreamTime: An Improved Opti-\nmization Strategy for Text-to-3D Content Creation. arXiv\npreprint arXiv:2306.12422, 2023. 2, 3\n[12] Yukun Huang, Jianan Wang, Ailing Zeng, He Cao, Xianbiao\nQi, Yukai Shi, Zheng-Jun Zha, and Lei Zhang. DreamWaltz:\nMake a Scene with Complex 3D Animatable Avatars. arXiv\npreprint arXiv:2305.12529, 2023. 2\n[13] Ruixiang Jiang, Can Wang, Jingbo Zhang, Menglei Chai,\nMingming He, Dongdong Chen, and Jing Liao.\nAvatar-\nCraft: Transforming Text into Neural Human Avatars with\nParameterized Shape and Pose Control.\narXiv preprint\narXiv:2303.17606, 2023. 2\n[14] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski.\nNoise-Free Score Distillation.\narXiv preprint\narXiv:2310.17590, 2023. 2\n[15] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-\nDreamer: Aligning Geometric Priors in 2D Diffusion for\nConsistent Text-to-3D.\narXiv preprint arXiv:2310.02596,\n2023. 2\n[16] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang,\nYangyi Huang, Justus Thies, and Michael J Black. TADA!\nText to Animatable Digital Avatars.\narXiv preprint\narXiv:2308.10899, 2023. 2\n[17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fi-\ndler, Ming-Yu Liu, and Tsung-Yi Lin.\nMagic3D: High-\nResolution Text-to-3D Content Creation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 300\u2013309, 2023. 1, 2, 8\n[18] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-\n3: Zero-shot One Image to 3D Object .\narXiv preprint\narXiv:2303.11328, 2023. 2\n[19] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. SyncDreamer: Gen-\nerating Multiview-consistent Images from a Single-view Im-\nage. arXiv preprint arXiv:2309.03453, 2023. 2\n[20] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, et al.\nWonder3D:\nSingle Image to 3D using Cross-Domain Diffusion. arXiv\npreprint arXiv:2310.15008, 2023. 2\n[21] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards Photorealistic Image Genera-\ntion and Editing with Text-Guided Diffusion Models. arXiv\npreprint arXiv:2112.10741, 2021. 2\n[22] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\nDenoising Diffusion Probabilistic Models. In International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[23] Aaron Oord, Yazhe Li, Igor Babuschkin, Karen Simonyan,\nOriol Vinyals, Koray Kavukcuoglu, George Driessche, Ed-\nward Lockhart, Luis Cobo, Florian Stimberg, et al. Parallel\nWaveNet: Fast High-Fidelity Speech Synthesis. In Interna-\ntional Conference on Machine Learning, pages 3918\u20133926.\nPMLR, 2018. 2\n[24] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion. arXiv\npreprint arXiv:2209.14988, 2022. 1, 2, 4, 6, 8, 3\n[25] Giovanni Puccetti. Measuring Linear Correlation Between\nRandom Vectors.\nInformation Sciences, 607:1328\u20131347,\n2022. 6, 8\n[26] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne Image to High-Quality 3D Object Generation Us-\ning Both 2D and 3D Diffusion Priors .\narXiv preprint\narXiv:2306.17843, 2023. 2\n9\n[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical Text-Conditional Image Gen-\neration with CLIP Latents. arXiv preprint arXiv:2204.06125,\n2022. 1, 2\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10684\u201310695, 2022. 2, 7\n[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic Text-to-Image\nDiffusion Models with Deep Language Understanding.\narXiv preprint arXiv:2205.11487, 2022. 1, 2\n[30] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al.\nLAION-5B: An open large-scale dataset for\ntraining next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022. 1, 2\n[31] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut.\nConceptual Captions: A Cleaned, Hypernymed,\nImage Alt-text Dataset For Automatic Image Captioning.\nIn Proceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers),\npages 2556\u20132565, 2018. 2\n[32] Yukai Shi, Jianan Wang, He Cao, Boshi Tang, Xianbiao\nQi, Tianyu Yang, Yukun Huang, Shilong Liu, Lei Zhang,\nand Heung-Yeung Shum. TOSS: High-quality Text-guided\nNovel View Synthesis from a Single Image. arXiv preprint\narXiv:2310.10644, 2023. 2\n[33] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. MVDream: Multi-view Diffusion for 3D\nGeneration. arXiv preprint arXiv:2308.16512, 2023. 2\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning Diffusion Implicit Models. In International Conference\non Learning Representations, 2021. 2\n[35] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-Based\nGenerative Modeling through Stochastic Differential Equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2\n[36] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi,\nLizhuang Ma, and Dong Chen. Make-It-3D: High-Fidelity\n3D Creation from A Single Image with Diffusion Prior,\n2023. 2\n[37] Pascal Vincent. A Connection Between Score Matching and\nDenoising Autoencoders. Neural Computation, 23(7):1661\u2013\n1674, 2011. 2\n[38] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lifting\nPretrained 2D Diffusion Models for 3D Generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 1, 2\n[39] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. ProlificDreamer: High-Fidelity\nand Diverse Text-to-3D Generation with Variational Score\nDistillation. arXiv preprint arXiv:2305.16213, 2023. 1, 2, 8\n[40] Haohan Weng, Tianyu Yang, Jianan Wang, Yu Li, Tong\nZhang, CL Chen, and Lei Zhang. Consistent123: Improve\nConsistency for One Image to 3D Object Synthesis. arXiv\npreprint arXiv:2310.08092, 2023. 2\n[41] Svante Wold, Kim Esbensen, and Paul Geladi.\nPrincipal\nComponent Analysis. Chemometrics and intelligent labo-\nratory systems, 2(1-3):37\u201352, 1987. 1\n[42] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent Image to 3D View Syn-\nthesis via Geometry-aware Diffusion Models. arXiv preprint\narXiv:2310.03020, 2023. 2\n[43] Xin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-\nHai Zhang, and Xiaojuan Qi.\nText-to-3D with Classifier\nScore Distillation. arXiv preprint arXiv:2310.19415, 2023.\n2, 6, 7\n[44] Huichao Zhang, Bowen Chen, Hao Yang, Liao Qu, Xu\nWang, Li Chen, Chao Long, Feida Zhu, Kang Du, and Min\nZheng. AvatarVerse: High-quality & Stable 3D Avatar Cre-\nation from Text and Pose. arXiv preprint arXiv:2308.03610,\n2023. 2\n10\nStable Score Distillation for High-Quality 3D Generation\nSupplementary Material\nIn this supplementary material, Sec. 7 presents more\nqualitative results of SSD on diverse prompts. We provide\nproofs in Sec. 8, Sec. 9 and Sec. 10, as well as numerical\nstudies in Sec. 11 to support our method\u2019s assumptions and\nanalysis. Sec. 12 shows the connection between prevalent\nobservations and our analysis. Sec. 13 gives a numerical\nexperiment that shows the efficiency of SSD as a general-\npurpose mode approximator.\n7. More Qualitative Results\nWe present additional 3D generation results with more\nprompts in Fig. 11 and Fig. 12.\n8. Analysis for Optimal c\nFor ease of mathematical analysis, in this work we adopt\ntotal variance as our variance definition in Eq. (5) for its\nwide usage [41]. We define \u03a3XY as the covariance ma-\ntrix between random vectors X and Y , and define \u03a3X as\na shorthand for \u03a3XX. Then Eq. (5) becomes c(x, t) =\nargmink tr(\u03a3\u02c6\u03f5\u03d5\u2212k\u03f5), which has a closed-form solution (see\nSec. 9 for proof):\nc(x, t) = tr(\u03a3\u02c6\u03f5\u03d5\u03f5)\ntr(\u03a3\u03f5) = tr(\u03a3\u02c6\u03f5\u03d5\u03f5)\nd\n,\n(8)\nwhere we assume \u03f5 \u2208 Rd and tr(\u00b7) denotes the trace of a\nmatrix. Note that evaluating Eq. (8) requires estimating the\ncovariance between \u02c6\u03f5\u03d5 and \u03f5 with Monte-Carlo, which is\ncomputationally expensive. In our experiments, we monitor\nthe projection ratio of \u02c6\u03f5\u03d5 on \u03f5, defined as\nr(xt, \u03f5, y, t) = \u02c6\u03f5\u03d5(xt, y, t) \u00b7 \u03f5\n||\u03f5||2\n.\n(9)\nNote\nthat\nr\nis\ndefined\nthis\nway\nto\nminimize\nthe\nnorm of \u02c6\u03f5\u03d5(xt, y, t) \u2212 k\u03f5,\nnamely r(xt, \u03f5, y, t)\n=\nargmink||\u02c6\u03f5\u03d5(xt, y, t) \u2212 k\u03f5||. Our experiments reveal that r\nis quite robust to \u03f5 for any combination of x, y, and t, and\nc is tightly bounded by the extremes of r, as shown in App.\nFig. 18.\n9. Proof for the Closed Form of Optimal c.\nRecall from Sec. 4.4.2 that,\nc(x, t) = argmink\n\u0002\ntr(\u03a3 \u02c6\n\u03f5\u03d5\u2212k\u03f5)\n\u0003\n= argmink\n\u0002\ntr(\u03a3\u02c6\u03f5\u03d5) \u2212 2k tr(\u03a3\u02c6\u03f5\u03d5\u03f5) + k2tr(\u03a3\u03f5)\n\u0003\n= argmink\n\u0002\n\u22122k tr(\u03a3\u02c6\u03f5\u03d5\u03f5) + k2tr(\u03a3\u03f5)\n\u0003\n,\ntaking the derivative of the above quantity with regard to k:\n\u2207k\n\u0002\nk2tr(\u03a3\u03f5) \u2212 2k tr(\u03a3\u02c6\u03f5\u03d5\u03f5)\n\u0003\n= 2ktr(\u03a3\u03f5) \u2212 2 tr(\u03a3\u02c6\u03f5\u03d5\u03f5),\nwhich is equal to zero if and only if k =\ntr(\u03a3\u02c6\u03f5\u03d5\u03f5)\ntr(\u03f5)\n.\nAs\n\u22072\nk[k2tr(\u03a3\u03f5) \u2212 2k tr(\u03a3\u02c6\u03f5\u03d5\u03f5)] = 2tr(\u03a3\u03f5) > 0, the function\n[k2tr(\u03a3\u03f5) \u2212 2k tr(\u03a3\u02c6\u03f5\u03d5\u03f5)] attains its minimum at the critical\npoint k =\ntr(\u03a3\u02c6\u03f5\u03d5\u03f5)\ntr(\u03f5)\n. Consequently, the optimal variance-\nreducing scale c\u2217(x, t) =\ntr(\u03a3\u02c6\u03f5\u03d5\u03f5)\ntr(\u03a3\u03f5) . Given that \u03f5 \u223c N(0, I),\n\u03a3\u03f5 = Id where d represents the dimension of \u03f5, c\u2217(x, t)\ncan be further simplified to\ntr(\u03a3\u02c6\u03f5\u03d5\u03f5)\nd\n.\n10. Proof for the t Dependence of Correlation\nbetween the Mode-Seeking Term and \u03f5.\nRecall that throughout our theoretical analysis, we make\na slightly simplified assumption that \u03b10 = 1, \u03c30 = 0\nand \u03b1T = 0, \u03c3T = 1.\nHere we support the statement\nin Sec. 4.3.1 that the linear correlation between the mode-\nseeking term and \u03f5 is dependent on timestep t.\nWhen t = 0, we prove that \u2207x0logP\u03d5(x0; y) is indepen-\ndent of \u03f5. Observing that x0 = x+0\u00b7\u03f5, it is evident that the\ninputs to \u2207x0logP\u03d5(x0; y) do not contain any information\nabout \u03f5. Therefore \u2207x0logP\u03d5(x0; y) is independent of \u03f5.\nWhen t\n=\nT, we prove that \u2207xT logP\u03d5(xT ; y) is\ncollinear to \u03f5:\n\u2207xT logP\u03d5(xT ; y) = \u2207xT logN(xT ; 0, I)\n= \u2207xT log((2\u03c0)\u2212 d\n2 exp(\u22121\n2xT\nT xT ))\n= \u2207xT (\u22121\n2xT\nT xT )\n= \u22121\n2\u2207xT xT\nT xT\n= \u2212xT\n= \u22120x \u2212 1\u03f5\n= \u2212\u03f5\n(10)\nwhere the superscript T for xT\nT represents vector transpose.\n11. More Numerical Experiments\nNumerical experiments are conducted to illustrate and vali-\ndate the properties of supervision signals employed for 3D\ngeneration, as well as the motivation and correctness of our\nmethod details.\nWe first sample 2D renderings of SDS-\ntrained 3D assets. Then for each rendering and each time\n1\n\u2119(\ud835\udc99)\n\u2119(\ud835\udc99!;y)\nFigure 7. Evaluating the learning behaviour of \u03b8 supervised by different estimators with an illustrative 2D example. In this toy\nexample \u03b8 = x \u2208 R2, and P(x) is a mixture of Gaussian distributions. The setup details are in Sec. 13. We initialize \u03b8 to be at the yellow\nstar, optimize it with Eq. (3) under different estimators, and record the learning trajectories of \u03b8. Note that ideally \u03b8 should converge to\nany modes around the red stars. Mode-Disengaging trajectory: the learning trajectory of x by employing the mode-disengaging term only\nin Eq. (3), where Mode-Seeking trajectory and SSD trajectory are defined similarly. The purple trajectory describes how \u03b8 evolves when\ninitialized at the trapping point and supervised by mode-disengaging term. On the right, we verify that the mode-seeking term indeed tries\nto approximate the transient mode for large t, causing the convergence point to lie between the normal modes of P(x) on the left. We\npresent the detailed experiment analysis in Sec. 13.\n(a) Cao Cao\n(b) Retro astronaut with a bulky suit\n(c) Lionel Messi\nDreamWaltz\n+ SDS\nDreamWaltz\n+ SSD (ours)\nFigure 8. Text-to-Avatar generation results using the DreamWaltz [12] framework, all with the same seed 0. Utilizing the proposed SSD\ninstead of SDS improves both generation details (a-b) and overall plausibility (c). Note that SDS generates a Messi jersey with mixed\nstyles in (c), indicative of a \u201csmoothing\u201d phenomenon as discussed in Sec. 4.3.2.\nstep t, 8192 noises \u03f5 are randomly sampled, resulting in\n8192 xt. We compute r, c, ||h|| and other relevant quanti-\nties based on the sampled noises and xt, which are visual-\nized in Fig. 16, Fig. 17, Fig. 18 and Fig. 19.\n12. Connection with Common Observations\nand Practices in 3D Content Generation\n\u2022 Large CFG Scales.\nAs illustrated in App.\nFig. 19,\nthe norms of the mode-disengaging term and the mode-\nseeking term are vastly different. Consequently, a large\nCFG scale is necessary to make the scale of the main\nlearning signal \u03c9 \u00b7 h at least comparable to that of the\nmode-seeking term.\nOtherwise the SDS estimator in\nEq. (3) would be dominated by the mode-seeking term,\nwhich can only generate over-smoothed contents for rea-\nsons discussed in Sec. 4.3.2, as visualized in Fig. 10.\n\u2022 Over-smoothness. In SDS where \u03c9 = 100, ||\u02c6\u03f5\u03d5|| and\n||\u03c9h|| are on the same order of magnitude, and the mode-\ndisengaging term lacks the dominance to the SDS esti-\n2\n\ud835\udc95 = \ud835\udfd3\ud835\udfce\ud835\udfce\n\ud835\udc95 = \ud835\udfd0\ud835\udfce\nconditional score \nvariance-reduced \nconditional score (SDS)\nadaptive variance-reduced\nconditional score (ours)\nFigure 9. Visualization of variance-reduction schemes for gra-\ndients. Note that the SDS-like variance-reduction scheme does\nnot effectively reduce variance of the conditional score for small t.\nWhen t is large, e.g., 500, the adaptive variance-reduction term, c,\nis close to 1 and our variance reduction scheme performs similarly\nto SDS-like variance reduction scheme. On the other hand, when t\nis small, our method significantly differs from SDS and produces\nsmoother gradients. Conditional score represents the gradients got\nby using the mode-seeking term only in optimization.\nFigure 10. A 3D asset generated by applying the mode-seeking\nterm alone for 15000 steps. The prompt is \u201ca zoomed out DSLR\nphoto of a baby bunny sitting on top of a pile of pancake\u201d.\nmator necessary to mitigate the over-smoothing effect in-\nduced by the mode-seeking term.\n\u2022 Color Saturation. This challenge has been repeatedly\nobserved in text-to-3D generation [24]. Recent work of\nDreamTime [11] also observes severe color saturation\ntendency when t < 100.\nAccording to our analysis\nin Sec. 4.2, when t is small the mode-disengaging term\ntends to drive the rendering g(\u03b8, c) away from modes of\nthe natural image distribution, making the rendering im-\nplausible. As pointed out in [4], in practice the diffusion\nmodel tends to influence fine details and colors, but not\nlarge-scale geometry, when t is small. Thus with small t\nthe mode-disengaging term generates implausible colors,\nwithout corrupting the general geometry of the objects.\nWe provide experiments in Fig. 20 to show that the mode-\ndisengaging term actually causes color saturation with 2D\nexperiments.\n13. Illustrative Example\nWe evaluate the learning behaviour of the mode-seeking,\nmode-disengaging and SSD terms with an illustrative ex-\nample, as shown in Fig. 7. In this experiment, we assume\nthat x = \u03b8 \u2208 R2, and P\u03d5(x) = 0.2N([0, 0]T , 0.1I) +\n0.4N([1, 1]T , 0.05I)+0.4N([2, 1]T , 0.05I) is a mixture of\nGaussian distributions. The two Gaussian components lo-\ncated at [1, 1]T and [2, 1]T are conditional modes, while the\none at [0, 0]T is a singular one. Note that we want \u03b8 to ap-\nproximate the two conditional modes as close as possible.\nWe initialize \u03b8 at [0, 0]T to simulate the situation that all\n3D generation algorithms begin with empty renderings. We\nthen substitute the mode-seeking, mode-disengaging and\nour SSD estimators for the SDS estimator in Eq. (3) one\nby one, and record the learning trajectory of \u03b8. To val-\nidate the trap-escaping property of the mode-disengaging\nterm we initialize a \u03b8 to the trapping point, namely [1.5, 1]T ,\nand use h to supervise \u03b8 afterwards. The learning trajec-\ntories are illustrated in Fig. 7 (left). We observe that the\nmode-seeking trajectory gets trapped between the two con-\nditional modes rapidly, not converging to any specific con-\nditional mode of P\u03d5(x). To inspect whether the issue is\ncaused by the transient-mode problem, we present the den-\nsity map of P\u03d5(xt; y) at t = 350, and the learning tra-\njectory of \u03b1tx in Fig. 7 (right). The density map reveals\nthat the two conditional modes of P\u03d5(x; y) result in a tran-\nsient mode in P\u03d5(xt; y), whose probability density is higher\nthan the induced modes. And \u03b1txt indeed approaches the\ntransient mode \u03b1totr. Conversely, the mode-disengaging\nterm can propel \u03b8 away from the trapping point otr. How-\never, although at the beginning it guides \u03b8 towards a con-\nditional mode, it ultimately steers \u03b8 into a low-density re-\ngion of P\u03d5(x). In contrast, the proposed SSD swiftly guides\n\u03b8 towards a conditional mode at beginning, avoids getting\ntrapped, and finally converges to a point with high density\nin P\u03d5(x).\n3\nAn octopus ceramic cup\nA capybara wearing a top hat, low-poly\nA tiger dressed as a maid\nA penguin wearing a puffy jacket\nA bunch of colorful marbles spilling out of a\nred velvet bag\nA palm tree, low poly 3d model\nA roast turkey on a platter.\nA rabbit, animated movie character\nFigure 11. More qualitative results on text-to-NeRF generation.\n4\nA model of a house in Tudor style\nA car made out of sushi\nA ceramic lion\nAn airplane made out of wood\nA cat wearing a lion costume\nA mouse playing the tuba\nA cake in the shape of a train\nA car made out of cheese\nA rotary telephone carved out of wood\nA human skeleton relaxing in\na lounge chair\nA typewriter\nAn ice cream sundae\nA tiger made out of yarn\nFigure 12. More qualitative results on text-to-NeRF generation.\n5\nFigure 13. Comparison on prompt \u201ca baby dragon drinking boba\u201d. With the proposed adaptive variance reduction scheme, SSD generates\nclear outline of the boba with only 2000 training steps. The accelerated learning pace on local features facilitates the successful generation\nof detailed boba, in contrast to the fuzziness observed in the absence of the adaptive variance reduction scheme.\n6\nW/o adaptive variance \nreduction\nWith adaptive variance \nreduction\niteration = 1000\niteration = 2000\niteration = 3000\niteration = 5000\nFigure 14. Comparison on prompt \u201ca bald eagle carved out of wood\u201d. With the adaptive variance reduction scheme, SSD learns faster\nand generates plausible squama and colors of the carved eagle with only 3000 training steps. This scheme effectively alleviates the over-\nsmoothing problem and the final generation shows delicate squama whereas the same region is overly smooth without the scheme.\nFigure 15. The correlation between t and color-saturation problem of the mode-disengaging term. The prompt is \u201ca hamburger\u201d. Note that\nutilizing the mode-disengaging term alone with random t \u2208 (1, T) is equivalent to concurrent work of CSD [43]. t > S means that the\ncorresponding image is generated by sampling t from (S, T) and vice versa. It is evident that training signals from smaller t contribute to\nthe fineness of local features but also tend to produce implausible colors/textures.\n7\nFigure 16. ||\u02c6\u03f5\u03d5|| increases as diffusion timestep t increases. This\ntogether with Fig. 17 justifies our design of adaptive variance re-\nduction. See Sec. 11 for the data collection protocol.\nFigure 17. Linear correlation [25] between \u02c6\u03f5\u03d5 and \u03f5 increases with\ndiffusion timestep t. This together with Fig. 16 justifies our design\nof adaptive variance reduction. See Sec. 11 for the data collection\nprotocol.\nFigure 18. The optimal c(x, t) lies within the extremes of our\nproposed proxy r. For each timestep, we compute r as defined\nin Eq. (6) and c as defined in Eq. (8). Note that with the 8192\nsamples as discussed in Sec. 11, c is unique while r can take 8192\ndifferent values. Therefore the maximum and minimum of these r\nare visualized separately. It is evident that r and c are numerically\nclose with similar trend, validating that r is a feasible proxy for c.\nSee Sec. 11 for the data collection protocol.\nFigure 19. Comparison between norms of the mode-seeking and\nmode-disengaging terms. Note that the y-axis is plotted in log\nscale. ||\u02c6\u03f5\u03d5|| \u2208 [80, 130] while ||h|| \u2208 [2, 5.5]. The vast norm\ndifference between the two terms constitutes an important justi-\nfication for our estimator-rescaling in the proposed stable score\ndistillation (SSD) Algorithm 1 (L15). See Sec. 11 for the data col-\nlection protocol.\nSDS CFG 7.5\nSDS CFG 100\nSSD M=0, same as CSD\nSSD M=200 (ours)\nFigure 20. Comparisons on 2D optimization. The experiments\nare run with more training iterations than normally used to expose\nthe intrinsic properties of the estimators. Objects\u2019 local features\nare more plausible with SSD. It is evident that while the mode-\nseeking term contributes to finer local details, it also incurs color\nimplausibility.\n8\n"
  },
  {
    "title": "Challenges with unsupervised LLM knowledge discovery",
    "link": "https://arxiv.org/pdf/2312.10029.pdf",
    "upvote": "7",
    "text": "2023-12-19\nChallenges with unsupervised LLM knowledge\ndiscovery\nSebastian Farquhar*,1, Vikrant Varma*,1, Zachary Kenton*,1, Johannes Gasteiger2, Vladimir Mikulik1 and\nRohin Shah1\n*Equal contributions, randomised order, 1Google DeepMind, 2Google Research\nWe show that existing unsupervised methods on large language model (LLM) activations do not discover\nknowledge \u2013 instead they seem to discover whatever feature of the activations is most prominent. The\nidea behind unsupervised knowledge elicitation is that knowledge satisfies a consistency structure,\nwhich can be used to discover knowledge. We first prove theoretically that arbitrary features (not just\nknowledge) satisfy the consistency structure of a particular leading unsupervised knowledge-elicitation\nmethod, contrast-consistent search (Burns et al., 2023). We then present a series of experiments\nshowing settings in which unsupervised methods result in classifiers that do not predict knowledge,\nbut instead predict a different prominent feature. We conclude that existing unsupervised methods for\ndiscovering latent knowledge are insufficient, and we contribute sanity checks to apply to evaluating\nfuture knowledge elicitation methods. Conceptually, we hypothesise that the identification issues\nexplored here, e.g. distinguishing a model\u2019s knowledge from that of a simulated character\u2019s, will persist\nfor future unsupervised methods.\n1. Introduction\nLarge language models (LLMs) perform well across a variety of tasks (Chowdhery et al., 2022; OpenAI,\n2023) in a way that suggests they systematically incorporate information about the world (Bubeck\net al., 2023). As a shorthand for the real-world information encoded in the weights of an LLM we\ncould say that the LLM encodes knowledge.\nHowever, accessing that knowledge is challenging, because the factual statements an LLM outputs\ndo not always describe that knowledge (Askell et al., 2021; Kenton et al., 2021; Park et al., 2023). For\nexample, it might repeat common misconceptions (Lin et al., 2021) or strategically deceive its users\n(Scheurer et al., 2023). If we could elicit the latent knowledge of an LLM (Christiano et al., 2021) it\nwould allow us to detect and mitigate dishonesty, in which an LLM outputs text which contradicts\nknowledge encoded in it (Evans et al., 2021). It could also improve scalable oversight by making AI\nactions clearer to humans, making it easier to judge if those actions are good or bad. Last, it could\nimprove scientific understanding of the inner workings of LLMs.\nRecent work introduces a learning algorithm\u2014contrast-consistent search (CCS) (Burns et al.,\n2023)\u2014to discover latent knowledge in LLMs without supervision, which is important because we\nlack a ground truth for what the model knows, as opposed to what we think we know. Their key\nclaims are that knowledge satisfies a consistency structure, formulated as the CCS loss function, that\nfew other features in an LLM are likely to satisfy, and hence the classifier elicits latent knowledge.\nWe refute these claims by identifying classes of features in an LLM that also satisfy this consistency\nstructure but are not knowledge. We prove two theoretical results: firstly that a class of arbitrary binary\nclassifiers are optimal under the CCS loss; secondly that there is a CCS loss-preserving transformation\nto an arbitrary classifier. The upshot is that the CCS consistency structure is more than just slightly\nimprecise in identifying knowledge\u2014it is compatible with arbitrary patterns.\nCorresponding author(s): sebfar,vikrantvarma,zkenton@google.com\n\u00a9 2023 Google DeepMind. All rights reserved\narXiv:2312.10029v2  [cs.LG]  18 Dec 2023\nChallenges with unsupervised LLM knowledge discovery\nqn\nLLM activations\nx4\n+ = Great movie\u2026\n             Alice\u2026 positive\n        It is positive\nx4\n+ = Great movie\u2026\n            Alice\u2026 positive\n        It is negative\nx3\n- = Didn\u2019t like it\u2026.\n                Alice\u2026 negative\n                It is positive\nq2 = The best movie ever\u2026\nq1 = I hated this movie\u2026\nx2\n+= The best movie ever\u2026\n          Alice\u2026 negative\n          It is positive\nx1\n+ = I hated this movie\u2026\n   Alice\u2026 positive\n            It is positive\nx3\n- = Didn\u2019t like it\u2026.\n                Alice\u2026 negative\n                It is negative\nx2\n- = The best movie ever\u2026\n          Alice\u2026 negative\n          It is negative\nx1\n- = I hated this movie\u2026\n  Alice\u2026 positive\n            It is negative\n\ud835\udf19(x1\n+), \ud835\udf19(x1\n-)\n\ud835\udf19(x2\n+), \ud835\udf19(x2\n-)\n.\n.\n.\n\ud835\udf19(xn\n+), \ud835\udf19(xn\n-)\nWithout \u201cAlice\u2026\u201d\nWith \u201cAlice\u2026\u201d\nClassification boundary \naccording to Review \n(blue/orange)\nClassification boundary \naccording to Alice \n(light/dark)\nUnsupervised learning\nFigure 1 | Unsupervised latent knowledge detectors are distracted by other prominent features\n(see Section 4.2). Left: We apply two transformations to a dataset of movie reviews, \ud835\udc5e\ud835\udc56. First (novel\nto us) we insert a distracting feature by appending either \u201cAlice thinks it\u2019s positive\u201d or \u201cAlice thinks\nit\u2019s negative\u201d at random to each question. Second, we convert each of these texts into contrast pairs\n(Burns et al., 2023), (\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 ), appending \u201cIt is positive\u201d or \u201cIt is negative\u201d. Middle: We then pass\nthese contrast pairs into the LLM and extract activations, \ud835\udf19. Right: We do unsupervised learning on\nthe activations. We show a PCA visualisation of the activations. Without \u201cAlice ...\u201d inserted, we learn\na classifier (taken along the \ud835\udc4b = 0 boundary) for the review (orange/blue). However, with \u201cAlice ...\u201d\ninserted the review gets ignored and we instead learn a classifier for Alice\u2019s opinion (light/dark).\nWe then show empirically that in practice CCS, and other unsupervised methods, do not discover\nknowledge. The first two experiments illustrated in Figure 1 introduce distracting features which\nare learnt instead of knowledge. In the third experiment, rather than inserting a distracting feature\nexplicitly, instead there is a character with an implicit opinion\u2014the methods sometimes learn to\npredict this character\u2019s opinion. In the fourth experiment we demonstrate the sensitivity of the\nmethods to unimportant details of the prompt. In the fifth experiment we show that, despite very\ndifferent principles, CCS makes similar predictions to PCA, illustrating that CCS is not exploiting\nconsistency structure of knowledge and motivating the possible generalisation of experimental results\nto future methods.\nWe conclude that existing unsupervised methods for discovering latent knowledge are insufficient\nin practice, and we contribute sanity checks to apply to evaluating future knowledge elicitation\nmethods. We hypothesise that our conclusions will generalise to more sophisticated methods, though\nperhaps not the exact experimental results: we think that unsupervised learning approaches to\ndiscovering latent knowledge which use some consistency structure of knowledge will likely suffer\nfrom similar issues to what we show here. Even more sophisticated methods searching for properties\nassociated with a model\u2019s knowledge seem to us to be likely to encounter false positives such as\n\u201csimulations\u201d of other entities\u2019 knowledge.\nOur key contributions are as follows:\n\u2022 We prove that arbitrary features satisfy the CCS loss equally well.\n\u2022 We show that unsupervised methods detect prominent features that are not knowledge.\n\u2022 We show that the features discovered by unsupervised methods are sensitive to prompts and\nthat we lack principled reasons to pick any particular prompt.\n2\nChallenges with unsupervised LLM knowledge discovery\n2. Background\nContrastive LLM activations\nWe focus on methods that train probes (Alain and Bengio, 2016)\nusing LLM activation data. The LLM activation data is constructed using contrast pairs (Burns et al.,\n2023). They begin with a dataset of binary questions, \ud835\udc44 = {\ud835\udc5e\ud835\udc56}\ud835\udc41\n\ud835\udc56=1, such as \ud835\udc5e\ud835\udc56 = \u201cAre cats mammals?\u201d,\nand produce a dataset, \ud835\udc4b = {(\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 )}\ud835\udc41\n\ud835\udc56=1, of pairs of input texts: \ud835\udc65+\n\ud835\udc56 = \u201cAre cats mammals? Yes\u201d and\n\ud835\udc65\u2212\n\ud835\udc56 = \u201cAre cats mammals? No\u201d. We then form activation data using \ud835\udc65+\n\ud835\udc56 (and \ud835\udc65\u2212\n\ud835\udc56 ) as inputs to the LLM,\nand read out an intermediate layer\u2019s activations, \ud835\udf19(\ud835\udc65+\n\ud835\udc56 ) (and \ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 )). A normalisation step is then\nperformed to remove the prominent feature of \ud835\udc65+\n\ud835\udc56 ends with \u201cYes\u201d and \ud835\udc65\u2212\n\ud835\udc56 ends with \u201cNo\u201d:\n\u02dc\ud835\udf19(\ud835\udc65+\n\ud835\udc56 ) B\n\ud835\udf19(\ud835\udc65+\n\ud835\udc56 ) \u2212 \ud835\udf07+\n\ud835\udf0e+\n;\n\u02dc\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 ) B\n\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 ) \u2212 \ud835\udf07\u2212\n\ud835\udf0e\u2212\nwhere \ud835\udf07+, \ud835\udf0e+ and \ud835\udf07\u2212, \ud835\udf0e\u2212 are the mean and standard deviation of {\ud835\udf19(\ud835\udc65+\n\ud835\udc56 )}\ud835\udc41\n\ud835\udc56=1 and {\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 )}\ud835\udc41\n\ud835\udc56=1 respectively.\nThis forms a dataset of contrastive LLM activations, \ud835\udc37 = { \u02dc\ud835\udf19(\ud835\udc65+\n\ud835\udc56 ), \u02dc\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 )}\ud835\udc41\n\ud835\udc56=1 for which we learn an\nunsupervised classifier, \ud835\udc53 : \ud835\udc44 \u2192 {0, 1}, mapping a question to a binary truth value. Our datasets have\nreference answers, \ud835\udc4e\ud835\udc56, which we use to evaluate the accuracy of the classifier.\nContrast-consistent Search (CCS)\nBurns et al. (2023) hypothesise that if knowledge is represented\nin LLMs it is probably represented as credences which follow the laws of probability. To softly encode\nthis constraint this, they train a probe \ud835\udc5d(\ud835\udc65) = \ud835\udf0e(\ud835\udf03\ud835\udc47 \u02dc\ud835\udf19(\ud835\udc65) + \ud835\udc4f) (a linear projection of the activation\nfollowed by a sigmoid function) to minimise the loss\nLCCS =\n\ud835\udc41\n\u2211\ufe01\n\ud835\udc56=1\nLcons + Lconf\nLcons =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u00032\nLconf = min\n\b\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2 .\nThe motivation is that the Lcons encourages negation-consistency (that a statement and its negation\nshould have probabilities that add to one), and Lconf encourages confidence to avoid \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2248 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) \u2248\n0.5. For inference on a question \ud835\udc5e\ud835\udc56 the average prediction is \u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n/2 and then\nthe induced classifier is \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) = I [\u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) > 0.5]. Because the predictor itself learns the contrast between\nactivations, not the absolute classes, Burns et al. (2023) assume they can tell the truth and falsehood\ndirection by taking \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) = 1 to correspond to label \ud835\udc4e\ud835\udc56 = 1 if the accuracy is greater than 0.5 (else it\ncorresponds to \ud835\udc4e\ud835\udc56 = 0). We call this truth-disambiguation.\nOther methods\nWe consider two other unsupervised learning methods. The first is based on\nPCA, and is introduced in Burns et al. (2023) as contrastive representation clustering top principal\ncomponent (CRC-TPC)1. It uses the difference in contrastive activations, { \u02dc\ud835\udf19(\ud835\udc65+\n\ud835\udc56 ) \u2212 \u02dc\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 )}\ud835\udc41\n\ud835\udc56=1 as a\ndataset, performs PCA, and then classifies by thresholding the top principal component at zero. The\nsecond method is k-means, which is applied using two clusters. In both cases, truth-directions are\ndisambiguated using the truth-disambiguation described above (Burns et al., 2023).\nFollowing Burns et al. (2023) we also use logistic regression on concatenated contrastive activations,\n{( \u02dc\ud835\udf19(\ud835\udc65+\n\ud835\udc56 ), \u02dc\ud835\udf19(\ud835\udc65\u2212\n\ud835\udc56 ))}\ud835\udc41\n\ud835\udc56=1 with labels \ud835\udc4e\ud835\udc56, and treat this as a ceiling (since it uses labeled data). Following\nRoger (2023) we compare to a random baseline using a probe with random parameter values, treating\nthat as a floor (as it does not learn from input data). Further details of all methods are in Appendix B.3.\n1Emmons (2023) point out that this is roughly 97-98% as effective as CCS according to the experiments in Burns\net al. (2023), suggesting that contrast pairs and standard unsupervised learning are doing much of the work, and CCS\u2019s\nconsistency loss may not be very important. Our experiments in this paper largely agree with this finding.\n3\nChallenges with unsupervised LLM knowledge discovery\n3. Theoretical Results\nOur two theoretical results show that CCS\u2019s consistency structure isn\u2019t specific to knowledge. The\nfirst theorem shows that arbitrary binary features of questions can be used as a classifier to achieve\noptimal performance under the CCS objective. This implies that arguments for CCS\u2019s effectiveness\ncannot be grounded in conceptual or principled motivations from the loss construction.\nTheorem 1. Let feature \u210e : \ud835\udc44 \u2192 {0, 1}, be any arbitrary map from questions to binary outcomes. Let\n(\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 ) be the contrast pair corresponding to question \ud835\udc5e\ud835\udc56. Then the probe defined as \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) = \u210e(\ud835\udc5e\ud835\udc56),\nand with \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) = 1 \u2212 \u210e(\ud835\udc5e\ud835\udc56), achieves optimal loss, and the averaged prediction satisfies \u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n/2 = \u210e(\ud835\udc5e\ud835\udc56).\nThat is, the classifier that CCS finds is under-specified: for any binary feature, \u210e, on the questions,\nthere is a probe with optimal CCS loss that induces that feature. The proof comes directly from\ninserting our constructive probes into the loss definition\u2014equal terms cancel to zero (see Appendix A).\nIn Thm. 1, the probe \ud835\udc5d is binary since \u210e is binary. In practice, since probe outputs are produced by\na sigmoid, they are in the exclusive range (0, 1).\nOur second theorem relaxes the restriction to binary probes and proves that any CCS probe can\nbe transformed into an arbitrary probe with identical CCS loss. We prove this theorem with respect\nto a corrected, symmetrized version of the CCS loss\u2014also used in our experiments\u2014which fixes an\nun-motivated downwards bias in the loss proposed by Burns et al. (2023) (see Appendix A.2 for\ndetails). We use the notation \u2295 to denote a continuous generalisation of exclusive or on functions\n\ud835\udc4e(\ud835\udc65), \ud835\udc4f(\ud835\udc65):\n(\ud835\udc4e \u2295 \ud835\udc4f)(\ud835\udc65) B [1 \u2212 \ud835\udc4e(\ud835\udc65)] \ud835\udc4f(\ud835\udc65) + [1 \u2212 \ud835\udc4f(\ud835\udc65)] \ud835\udc4e(\ud835\udc65).\nTheorem 2. Let \ud835\udc54 : \ud835\udc44 \u2192 {0, 1}, be any arbitrary map from questions to binary outputs. Let (\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 )\nbe the contrast pair corresponding to question \ud835\udc5e\ud835\udc56. Let \ud835\udc5d be a probe, whose average result \u02dc\ud835\udc5d =\n[\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 )+(1\u2212\ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))]\n2\ninduces a classifier \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) = I [\u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) > 0.5]. Define a transformed probe \ud835\udc5d\u2032(\ud835\udc65+/\u2212\n\ud835\udc56\n) =\n\ud835\udc5d(\ud835\udc65+/\u2212\n\ud835\udc56\n) \u2295\n\u0002\n\ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) \u2295 \ud835\udc54(\ud835\udc5e\ud835\udc56)\n\u0003. For all such transformed probes, LCCS(\ud835\udc5d\u2032) = LCCS(\ud835\udc5d) and \ud835\udc5d\u2032 induces the\narbitrary classifier \ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = \ud835\udc54(\ud835\udc5e\ud835\udc56).\nThat is, for any original probe, there is an arbitrary classifier encoded by a probe with identical\nCCS loss to the original.\nThese theorems prove that optimal arbitrary probes exist, but not necessarily that they are actually\nlearned or that they are expressible in the probe\u2019s function space. Which probe is actually learned\ndepends on inductive biases; these could depend on the prompt, optimization algorithm, or model\nchoice. None of these are things for which any robust argument ensures the desired behaviour.\nThe feature that is most prominent\u2014favoured by inductive biases\u2014could turn out to be knowledge,\nbut it could equally turn out to be the contrast-pair mapping itself (which is partly removed by\nnormalisation) or anything else. We don\u2019t have any theoretical reason to think that CCS discovers\nknowledge probes. We now turn to demonstrating experimentally that, in practice, CCS can discover\nprobes for features other than knowledge.\n4. Experiments\nDatasets\nWe investigate three of the datasets that were used in Burns et al. (2023).2 We use the\nIMDb dataset of movie reviews classifying positive and negative sentiment (Maas et al., 2011), BoolQ\n2The others were excluded for legal reasons or because Burns et al. (2023) showed poor predictive accuracy using them.\n4\nChallenges with unsupervised LLM knowledge discovery\n(Clark et al., 2019) answering yes/no questions about a text passage, and the binary topic-classification\ndataset DBpedia (Auer et al., 2007). Prompt templates for each dataset are given in Appendix B.1.\nWe use a single prompt template rather than the multiple used in Burns (2022), as we didn\u2019t find\nmultiple templates to systematically improve performance of the methods, but increases experiment\ncomplexity, see Appendix C.5 for our investigation.\nLanguage Models\nWe use three different language models. In order to provide a direct comparison\nto Burns et al. (2023) we use one of the models they investigated, T5-11B, (Raffel et al., 2020)\nwith 11 billion parameters. We further use an instruction fine-tuned version of T5-11B called T5-\nFLAN-XXL, (Chung et al., 2022) to understand the effect of instruction fine-tuning. Both are encoder-\ndecoder architectures, and we use the encoder output for our activations. We also use Chinchilla-70B\n(Hoffmann et al., 2022), with 70 billion parameters, which is larger scale, and a decoder-only\narchitecture. We take activations from layer 30 (of 80) of this model, though see Appendix C.2.3 for\nresults on other layers, often giving similar results. Notably, K-means and PCA have good performance\nat layer 30 with less seed-variance than CCS, suggesting contrast pairs and standard unsupervised\nlearning, rather than the CCS consistency structure, are key (see Footnote 1).\nExperiment Setup\nIn each experiment we compare a default setting which is the same/similar to\nthat used in (Burns et al., 2023) to a modified setting that we introduce in order to show an effect \u2013\ndiffering only in their text prompt. We then generate contrastive activations and train probes using\nthe methods in Section 2: CCS, PCA, k-means, random and logistic regression. Training details can\nbe found in Appendix B.3. For each method we use 50 random seeds. Our figures in general come in\ntwo types: violin plots which compare the accuracy of different methods; and three-dimensional PCA\nprojections of the activations to visualise how they are grouped. We show one dataset and model,\nrefer to the appendix for other datasets and models which often show similar results.\n4.1. Discovering random words\nOur first experiment, motivated by our theoretical results, introduces a distracting binary feature\nand shows the unsupervised methods discover this feature rather than knowledge. We focus here\non IMDB and Chinchilla (see Appendix C.1 for other datasets and models with similar results). Our\ndefault prompts use a standard template as in Burns et al. (2023):\nConsider the following example:\nProbably my all-time favorite movie, a story of...\nBetween positive and negative, the sentiment of this example is [label]\nDifferent questions differ in their review, inserted on the second line. The [label] \u201cpositive\u201d or\n\u201cnegative\u201d is inserted using the standard contrast pair procedure.\nOur modified prompts are formed from the above template by appending a full stop and space,\nthen one of two random words, \u201cBanana\u201d and \u201cShed\u201d. In the language of Thm. 1 we take a random\npartition of question indices, {1, . . . , \ud835\udc41} = \ud835\udc3c0 \u222a \ud835\udc3c1, with |\ud835\udc3c0| = |\ud835\udc3c1|, and set the binary feature \u210e such\nthat \u210e(\ud835\udc5e\ud835\udc56) = 0 for \ud835\udc56 \u2208 \ud835\udc3c0 and \u210e(\ud835\udc5e\ud835\udc56) = 1 for for \ud835\udc56 \u2208 \ud835\udc3c1. \u201cBanana\u201d is inserted if \u210e(\ud835\udc5e\ud835\udc56) = 0, and \u201cShed\u201d is\ninserted if \u210e(\ud835\udc5e\ud835\udc56) = 1. See Figure 1 for the structure of the modification \u2013 though here we append\n\u201cBanana\u201d or \u201cShed\u201d to the end, rather than inserting \u201cAlice...\u201d in the middle.\nOur results are shown in Figure 2a, displaying accuracy of each method (x-axis groups). Default\nprompts are blue and modified banana/shed prompts are red. We look at the standard ground-truth\n5\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\n(a) Variation in accuracy\nDistractor label\nBanana\nShed\nReview Sentiment\nPositive\nNegative\nX\n\u221250\n0\n50\nY\n\u221230\n0\n30\nX\n\u221215\n0\n15\nY\n\u221220\n0\n20\nReview=Negative, Banana=False\nReview=Negative, Banana=True\nReview=Positive, Banana=False\nReview=Positive, Banana=True\nDefault prompt\nBanana/Shed prompt\n(b) PCA Visualisation\nFigure 2 | Discovering random words. Chinchilla, IMDb. (a) The methods learn to distinguish\nwhether the prompts end with banana/shed rather than the sentiment of the review. (b) PCA\nvisualisation of the activations, in default (left) and modified (right) settings, shows the clustering\ninto banana/shed (light/dark) rather than review sentiment (blue/orange).\naccuracy metric (dark), as well as a modified accuracy metric that measures whether Banana or\nShed was inserted (light). We see that for all unsupervised methods, default prompts (blue) score\nhighly on ground truth accuracy (dark blue) in line with results in Burns et al. (2023). However, for\nthe banana/shed prompts we see 50%, random chance, on ground truth accuracy (dark red). On\nBanana/Shed accuracy (light red) both PCA and K-means score highly, while CCS shows a bimodal\ndistribution with a substantial number of seeds with 100% Banana/Shed accuracy \u2013 seeds differ\nonly in the random initialisation of the probe parameters. The takeaway is that CCS and other\nunsupervised methods don\u2019t optimise for ground-truth knowledge, but rather track whatever feature\n(in this case, banana/shed) is most prominent in the activations.\nFigure 2b shows a visualisation of the top three components of PCA for the default (left) and\nmodified (right) prompts. In the modified case we see a prominent grouping of the data into dark/light\n(banana/shed) and, less prominently, into blue/orange (the review). This provides visual evidence\nthat both features (ground-truth and banana/shed) are represented, but the one which is most\nprominent in this case is banana/shed, in correspondence with Figure 2a.\n4.2. Discovering an explicit opinion\nIt is unlikely that such a drastic feature, ending with \u201cBanana\u201d/\u201cShed\u201d, would actually exist in a\nreal dataset. These words had nothing to do with the rest of the text. In our second experiment\nwe consider a similar but more realistic modification, by inserting a character\u2019s explicit opinion of\nwhether the review is positive or negative. What we will find is that the unsupervised methods learn\nto predict the character\u2019s opinion, rather than classifying the sentiment of the actual review.\n6\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\n(a) Variation in accuracy\nDistractor label\nAlice: Negative\nAlice: Positive\nReview Sentiment\nPositive\nNegative\nX\n\u221280\n0\n80\nY\n\u221250\n0\n50\nX\n\u2212100\n0\n100\nY\n\u221260\n0\n60\nReview=Negative, Alice=Negative\nReview=Negative, Alice=Positive\nReview=Positive, Alice=Negative\nReview=Positive, Alice=Positive\nDefault prompt\nAlice-opinion prompt\n(b) PCA Visualisation\nFigure 3 | Discovering an explicit opinion. (a) When Alice\u2019s opinion is present (red) unsupervised\nmethods accurately predict her opinion (light red) but fail to predict the sentiment of the review (dark\nred). Blue here shows the default prompt for comparison. (b) PCA visualisation of the activations,\nin default (left) and modified (right) settings, shows the clustering into Alice\u2019s opinion (light/dark)\nrather than review sentiment (blue/orange).\nWe again focus here on IMDB and Chinchilla (see Appendix C.2.1 for other datasets and models\nwith similar results) and we use the same default prompt as before. For the modified setting we use a\nprompt style where Alice\u2019s opinion is inserted into a random partition of questions:\nAlice is an expert at classifying the sentiment of movie reviews. Everyone agrees with her,\nand they always copy her answers. Consider the following review:\nProbably my all-time favorite movie, a story of...\nAlice thinks it is {positive | negative}.\nWhat do you think?\nI think it is [label]\nOur results are shown in Figure 3a. Default prompts are blue and modified prompts (containing\nAlice\u2019s opinion) are red. We look at the standard ground-truth accuracy metric (dark), as well as \u201cAlice\nAccuracy\u201d metric (light) that measures whether \u201cAlice thinks it is positive\u201d or\u201cAlice thinks it is negative\u201d\nwas inserted. We see similar results as the previous experiment, that CCS and other unsupervised\nmethods don\u2019t score high ground-truth accuracy, but rather score highly on Alice Accuracy, and further\nthat the CCS results are no longer bimodal.\nAlso shown in Figure 3b is a visualisation of the top three components of a PCA for the activations.\nWe see clearly the most prominent grouping of the data is into dark/light (Alice\u2019s opinion) and that\nthese then have subgroups along blue/orange (the review).\nWhen we use a model that has been instruction-tuned (T5-FLAN-XXL) we see a similar pattern\nAppendix C.2.1 Figure 12, although a similarly clear result requires a more emphatic view from the\ncharacter by repeating the opinion (\u201cI think it is positive. They fully express positive views. I\u2019m sure\nyou also think it is positive. It\u2019s clearly positive.\u201d). An ablation of the number of repetitions can be\nfound in Appendix C.2.2, Figure 13.\n7\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAnti-capitalist\nData subset\n Company\n Non-company\nCCS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nAnswer=Company, Default\nAnswer!=Company, Default\nAnswer=Company, Anti-capitalist\nAnswer!=Company, Anti-capitalist\n(a) Variation in accuracy\nData subset\nNon-Company\nCompany\nCorrect answer Choice 1\nChoice 2\nX\n0\n150\nY\n\u2212100\n0\n100\nX\n0\n150\nY\n\u2212100\n0\n100\nChoice 1=False, Answer!=Company\nChoice 1=False, Answer=Company\nChoice 1=True, Answer!=Company\nChoice 1=True, Answer=Company\nDefault prompt\nAnti-capitalist prompt\n(b) PCA Visualisation\nFigure 4 | Discovering an implicit opinion for Chinchilla70B. (a) Default (blue) and modified (red)\nfor company (dark) and non-company (light) data. The modified setting on company data (dark\nred) leads to a bimodal distribution for CCS with almost half of the probes (differing only in random\ninitialisation) learning Alice\u2019s opinion. In contrast, it performs relatively well over all other categories\n(light red). (b) PCA: Left \u2013 default activations show a possible separation along X-axis corresponding\nto topic choice (blue vs. orange) and further separation into company/non-company (light/dark).\nRight \u2013 modified activations show a more pronounced company/non-company split.\n4.3. Discovering an implicit opinion\nThe previous experiment explicitly gave Alice\u2019s opinion, \u201cAlice thinks it is positive\u201d. While this is more\nrealistic than Banana/Shed, it is still rather artificial in the sense we don\u2019t expect real datasets to have\nsuch a clear syntactical textual binary feature. In the next experiment for the modified prompt we\ninstead explain Alice\u2019s position in general, and keep that the same in all instances, making it more of\nan implicit, semantic rather than syntactic feature.\nWe use the DBpedia topic classification dataset (Auer et al., 2007) to construct a binary classification\ntask to classify the topic of a text from two choices. There are fourteen categories such as company,\nanimal, film. In the default case contrast pairs are constructed using a simple few-shot prompt setting\nup the task of identifying the topic of a sentence with the character \u201cAlice\u201d answering the questions\ncorrectly. In the modified setting3, Alice answers the few-shot examples correctly, except when topic\nis company \u2013 and in that case gives explanations like \u201c[...] Alice always says the wrong answer when\nthe topic of the text is company, because she doesn\u2019t like capitalism [...]\u201d. What we are looking for\nis what the unsupervised methods predict on the final example when Alice has not yet stated an\nopinion: will it predict the correct answer, ignoring how Alice previously answered incorrectly about\ncompany; or will it predict Alice\u2019s opinion, answering incorrectly about company.\nTo highlight the effect, we use a subset dataset where 50% of sentences are about \u201ccompany\u201d,\nand 50% have one of the remaining thirteen categories (non-company) as a topic. We apply truth-\ndisambiguation only to the subset with non-company topics, so that we can see the possible effect of\npredicting incorrectly on company data (otherwise the assignment might be flipped).\nOur results are shown in Figure 4. We look at default prompts (blue) and modified prompts (red)\nand split the data into whether the topic is company (dark) or non-company (light) and look at the\n3Full prompt templates are provided in Appendix B.1.3, Implicit Opinion: Default and Anti-capitalist.\n8\nChallenges with unsupervised LLM knowledge discovery\nstandard ground-truth accuracy metric. The default setting (blue) produces high accuracy classifiers\nboth when the topic is company (dark blue) and other categories (light blue). In the modified setting\n(red) CCS gives a bimodal distribution when the topic is company (dark red), with almost half of the\nprobes (differing only in random initialisation) predicting Alice\u2019s opinion, rather than the actual topic.\nIn contrast, it performs well over all other categories (light red) and so is not just an ordinary failure.\nOther unsupervised methods are less sensitive to the modified setting, scoring high accuracy when\nthe topic is company.\nHowever, when we visualise the first three PCA dimensions of the contrast pair activations\nFigure 4b we see four distinct clusters in the modified prompt case (right) showing how a detector\nmight cluster along either the topic (orange vs blue) or Alice\u2019s opinion (light vs dark). This indicates\nthese unsupervised methods are still sensitive to the modified setting, it just isn\u2019t evident from looking\nat the accuracy metric alone.\n4.4. Prompt template sensitivity\nThe next experiment is less artificial than the previous ones, in that we do not introduce a feature for\nthe unsupervised methods to discover. Instead, we look at three variations for a prompt template\nthat could naturally be chosen by a practitioner, and we will see that this leads to classifiers which\nhave rather different accuracies. We use TruthfulQA (Lin et al., 2021), a difficult question answering\ndataset which exploits the fact that LLMs tend to repeat common misconceptions.\nWe find that a \u201cnon-default\u201d prompt gives the \u201cbest performance\u201d in the sense of the highest\ntest-set accuracy. This highlights the reliance of unsupervised methods on implicit inductive biases\nwhich cannot be set in a principled way. It is not clear which prompt is the best one for eliciting\nthe model\u2019s latent knowledge. Given that the choice of prompt appears to be a free variable with\nsignificant effect on the outcomes, any conceptual arguments about a principled loss or unsupervised\nsearch method do not imply that the resulting classifier has a principled foundation.\nIn our default setting we construct a contrast pair as follows\nQuestion: {question}\nAnswer: {answer}\nTrue or false?\nThe answer is [label]\nThe [label] \u201ctrue\u201d or \u201cfalse\u201d is inserted using the standard contrast pair procedure. We consider two\nmodified settings. Firstly, the professor setting, where we prepend the text from Lin et al. (2021)4:\nProfessor Smith was given the following instructions: Interpret each question literally, and\nas a question about the real world; carefully research each answer, without falling prey to\nany common myths; and reply \u201cI have no comment\u201d unless you are completely certain of\nthe answer. Here are the Professor\u2019s responses:\nWe used this text verbatim in order to make sure that we were looking at natural prompts that\npeople might ordinarily use without trying to see a specific result. We also consider a literal setting,\nremoving the opening and closing phrase about a Professor, just in case explicitly invoking a character\nis important.\n4Lin et al. (2021) found LLM generation performance improved using this prompt.\n9\nChallenges with unsupervised LLM knowledge discovery\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nDefault\nLiteral\nProfessor\n(a) Variation in accuracy\nX\n0\n60\nY\n\u221250\n0\n50\nX\n\u221280\n0\n80\nY\n\u221260\n0\n60\nX\n\u221260\n0\n60\nY\n\u221260\n0\n60\nFalse\nTrue\nDefault\nLiteral\nProfessor\n(b) PCA Visualisation\nFigure 5 | Prompt sensitivity on TruthfulQA (Lin et al., 2021) for Chinchilla70B. (a) In default\nsetting (blue), accuracy is poor. When in the literal/professor (red, green) setting, accuracy improves,\nshowing the unsupervised methods are sensitive to irrelevant aspects of a prompt. (b) PCA of the\nactivations based on ground truth, blue vs. orange, in the default (left), literal (middle) and professor\n(right) settings. We see don\u2019t see ground truth clusters in the default setting, but see this a bit more\nin the literal and professor setting.\nResults are shown in Figure 5a for Chinchilla70B. The default setting (blue) gives worse accuracy\nthan the literal/professor (red, green) settings, especially for PCA and k-means. PCA visualisations\nare shown in Figure 5b, coloured by whether the question is True/False, in the default (left), literal\n(middle) and professor (right) settings. We see clearer clusters in the literal/professor settings. Other\nmodels are shown in Appendix C.4, with less systematic differences between prompts, though the\naccuracy for K-means in the Professor prompt for T5-FLAN-XXL are clearly stronger than others.\nOverall, this shows that the unsupervised methods are sensitive to irrelevant aspects of a prompt\u2014\nif these methods were detecting knowledge/truth, it shouldn\u2019t matter whether we give instructions to\ninterpret things literally.\n4.5. Agreement between unsupervised methods\nBurns et al. (2023) claim that knowledge has special structure that few other features in an LLM are\nlikely to satisfy and use this to motivate CCS. CCS aims to take advantage of this consistency structure,\nwhile PCA ignores it entirely. Nevertheless, we find that CCS and PCA5 make similar predictions. We\ncalculate the proportion of datapoints where both methods agree, shown in Figure 6 as a heatmap\naccording to their agreement. There is higher agreement (top-line number) in all cases than what\none would expect from independent methods (notated \u201cInd:\u201d) with the observed accuracies (shown\nin parentheses in the heatmap). This supports the hypothesis of Emmons (2023) and suggests that\nthe consistency-condition does not do much. But the fact that two methods with such different\nmotivations behave similarly also supports the idea that results on current unsupervised methods\nmay be predictive of future methods which have different motivations.\n5. Related Work\nWe want to detect when an LLM is dishonest (Askell et al., 2021; Kenton et al., 2021; Park et al.,\n2023), outputting text which contradicts its encoded knowledge (Evans et al., 2021). An important\n5PCA and k-means performed similarly in all our experiments so we chose to only focus on PCA here\n10\nChallenges with unsupervised LLM knowledge discovery\nBoolQ\nDBpedia\nIMDB\nFlan-T5 Chinchilla\nT5\n0.74\nInd:0.61\n(0.72, 0.74)\n0.90\nInd:0.88\n(0.92, 0.95)\n0.87\nInd:0.81\n(0.85, 0.94)\n0.98\nInd:0.82\n(0.9, 0.9)\n1.00\nInd:1.00\n(1, 1)\n0.98\nInd:0.93\n(0.97, 0.96)\n0.57\nInd:0.52\n(0.59, 0.61)\n0.90\nInd:0.80\n(0.88, 0.9)\n0.92\nInd:0.84\n(0.94, 0.89)\n0.6\n0.7\n0.8\n0.9\n1.0\nFigure 6 | CCS and PCA make similar predictions. In all cases, CCS and PCA agree more than what\none would expect of independent methods with the same accuracy. Annotations in each cell show\nthe agreement, the expected agreement for independent methods, and the (CCS, PCA) accuracies,\naveraged across 10 CCS seeds.\npart of this is to elicit latent knowledge from a model (Christiano et al., 2021). There has been some\ndebate as to whether LLMs \u201cknow/believe\u201d anything (Bender et al., 2021; Levinstein and Herrmann,\n2023; Shanahan, 2022) but, for us, the important thing is that something in an LLM\u2019s weights causes\nit to make consistently successful predictions, and we would like to access that. Others (see (Hase\net al., 2023) and references therein) aim to detect when a model has knowledge/beliefs about the\nworld, to improve truthfulness.\nDiscovering latent information in trained neural networks using unsupervised learning has been\nexplored by Dalvi et al. (2022) using clustering to discover latent concepts in BERT (Devlin et al.,\n2018) and also explored by Belrose et al. (2023) to train unsupervised probes on intermediate LLM\nlayers to elicit latent predictions.\nContrast-consistent search (CCS) (Burns et al., 2023) is a method which attempts to elicit latent\nknowledge using unsupervised learning on contrastive LLM activations (see Section 2), claiming that\nknowledge has special structure that can be used as an objective function which, when optimised,\nwill discover latent knowledge.\nWe have refuted this claim, theoretically and empirically, showing that CCS performs similarly to\nother unsupervised methods which do not use special structure of knowledge. Emmons (2023) also\nobserve this from the empirical data provided in (Burns et al., 2023). Huben (2022) hypothesises\nthere could be many truth-like features, due to LLMs ability to role-play (Shanahan et al., 2023),\nwhich a method like CCS might find. Roger (2023) constructs multiple different probes achieving low\nCCS loss and high accuracy, showing that there is more than one knowledge-like classifier. Levinstein\nand Herrmann (2023) finds that CCS sometimes learns features that are uncorrelated with truth, and\nargue that consistency properties of knowledge alone cannot guarantee identification of truth. Fry\net al. (2023) modify the CCS to improve accuracy despite probes clustering around 0.5, casting doubt\non the probabilistic interpretation of CCS probes. In contrast to all these works, we prove theoretically\nthat CCS does not optimise for knowledge, and show empirically what features CCS can instead find\nother than knowledge in controlled experiments.\nOur focus in this paper has been on unsupervised learning, though several other methods to train\nprobes to discover latent knowledge use supervised learning (Azaria and Mitchell, 2023; Li et al.,\n2023; Marks and Tegmark, 2023; Wang et al., 2023; Zou et al., 2023) \u2013 see also Clymer et al. (2023)\nfor a comparison of the generalisation properties of some of these methods under distribution shifts.\nFollowing Burns et al. (2023) we also reported results using a supervised logistic regression baseline,\n11\nChallenges with unsupervised LLM knowledge discovery\nwhich we have found to work well on all our experiments, and which is simpler than in those cited\nworks.\nOur result is analogous to the finding that disentangled representations seemingly cannot be\nidentified without supervision (Locatello et al., 2019). Though not the focus of our paper, supervised\nmethods face practical and conceptual problems for eliciting latent knowledge. Conceptual in that\nestablishing ground-truth supervision about what the model knows (as opposed to what we know or\nthink it knows) is not currently a well-defined procedure. Practical in that ground-truth supervision\nfor superhuman systems that know things we do not is especially difficult.\nThere are also attempts to detect dishonesty by supervised learning on LLM outputs under\nconditions that produce honest or dishonest generations (Pacchiardi et al., 2023). We do not compare\ndirectly to this, focusing instead on methods that search for features in activation-space.\n6. Discussion and Conclusion\nLimitation: generalizability to future methods.\nOur experiments can only focus on current\nmethods. Perhaps future unsupervised methods could leverage additional structure beyond negation-\nconsistency, and so truly identify the model\u2019s knowledge? While we expect that such methods could\navoid the most trivial distractors like banana/shed (Figure 2), we speculate that they will nonetheless\nbe vulnerable to similar critiques. The main reason is that we expect powerful models to be able\nto simulate the beliefs of other agents (Shanahan et al., 2023). Since features that represent agent\nbeliefs will naturally satisfy consistency properties of knowledge, methods that add new consistency\nproperties could still learn to detect such features rather than the model\u2019s own knowledge. Indeed, in\nFigures 3 and 4, we show that existing methods produce probes that report the opinion of a simulated\ncharacter.6\nAnother response could be to acknowledge that there will be some such features, but they will\nbe few in number, and so you can enumerate them and identify the one that represents the model\u2019s\nknowledge (Burns, 2022). Conceptually, we disagree: language models can represent many fea-\ntures (Elhage et al., 2022), and it seems likely that features representing the beliefs of other agents\nwould be quite useful to language models. For example, for predicting text on the Internet, it is\nuseful to have features that represent the beliefs of different political groups, different superstitions,\ndifferent cultures, various famous people, and more.\nConclusion.\nExisting unsupervised methods are insufficient for discovering latent knowledge, though\nconstructing contrastive activations may still serve as a useful interpretability tool. We contribute\nsanity checks for evaluating methods using modified prompts and metrics for features which are not\nknowledge. Unsupervised approaches have to overcome the identification issues we outline in this\npaper, whilst supervised approaches have the problem of requiring accurate human labels even in the\ncase of superhuman models. The relative difficulty of each remains unclear. We think future work\nproviding empirical testbeds for eliciting latent knowledge will be valuable in this regard.\n6Note that we do not know whether the feature we extract tracks the beliefs of the simulated character: there are clear\nalternative hypotheses that explain our results. For example in Figure 3, while one hypothesis is that the feature is tracking\nAlice\u2019s opinion, another hypothesis that is equally compatible with our results is that the feature simply identifies whether\nthe two instances of \u201cpositive\u201d / \u201cnegative\u201d are identical or different.\n12\nChallenges with unsupervised LLM knowledge discovery\n7. Acknowledgements\nWe would like to thank Collin Burns, David Lindner, Neel Nanda, Fabian Roger, and Murray Shanahan\nfor discussions and comments on paper drafts as well as Nora Belrose, Paul Christiano, Scott Em-\nmons, Owain Evans, Kaarel Hanni, Georgios Kaklam, Ben Levenstein, Jonathan Ng, and Senthooran\nRajamanoharan for comments or conversations on the topics discussed in our work.\nReferences\nG. Alain and Y. Bengio. Understanding intermediate layers using linear classifier probes. arxiv, 2016.\nA. Askell, Y. Bai, A. Chen, D. Drain, D. Ganguli, T. Henighan, A. Jones, N. Joseph, B. Mann, N. DasSarma,\nN. Elhage, Z. Hatfield-Dodds, D. Hernandez, J. Kernion, K. Ndousse, C. Olsson, D. Amodei, T. Brown,\nJ. Clark, S. McCandlish, C. Olah, and J. Kaplan. A general language assistant as a laboratory for\nalignment. arXiv, Dec. 2021.\nS. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives. DBpedia: A nucleus for a web of\nopen data. In The Semantic Web, pages 722\u2013735. Springer Berlin Heidelberg, 2007.\nA. Azaria and T. Mitchell. The internal state of an LLM knows when its lying. arXiv, Apr. 2023.\nN. Belrose, Z. Furman, L. Smith, D. Halawi, I. Ostrovsky, L. McKinney, S. Biderman, and J. Steinhardt.\nEliciting latent predictions from transformers with the tuned lens. arXiv preprint arXiv:2303.08112,\n2023.\nE. M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell. On the dangers of stochastic parrots: Can\nlanguage models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT \u201921, page 610\u2013623, New York, NY, USA, 2021. Association for Computing\nMachinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/\n10.1145/3442188.3445922.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,\nS. Lundberg, H. Nori, H. Palangi, M. T. Ribeiro, and Y. Zhang. Sparks of artificial general intelligence:\nEarly experiments with GPT-4. arXiv, Mar. 2023.\nC. Burns. How \u201cdiscovering latent knowledge in language models without supervision\u201d fits into a\nbroader alignment scheme. Dec. 2022.\nC. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without\nsupervision. In The Eleventh International Conference on Learning Representations, 2023. URL\nhttps://openreview.net/forum?id=ETKGuby0hcs.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung,\nC. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint\narXiv:2204.02311, 2022.\nP. Christiano, A. Cotra, and M. Xu. Eliciting latent knowledge: How to tell if your eyes deceive you,\nDec. 2021.\nH. W. Chung, L. Hou, S. Longpre, B. Zoph, Y. Tay, W. Fedus, Y. Li, X. Wang, M. Dehghani, S. Brahma,\net al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416, 2022.\n13\nChallenges with unsupervised LLM knowledge discovery\nC. Clark, K. Lee, M.-W. Chang, T. Kwiatkowski, M. Collins, and K. Toutanova. BoolQ: Exploring the\nsurprising difficulty of natural Yes/No questions. In J. Burstein, C. Doran, and T. Solorio, editors,\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2924\u20132936,\nMinneapolis, Minnesota, June 2019. Association for Computational Linguistics.\nJ. Clymer, G. Baker, R. Subramani, and S. Wang. Generalization analogies (genies): A testbed for\ngeneralizing ai oversight to hard-to-measure domains. arXiv preprint arXiv:2311.07723, 2023.\nF. Dalvi, A. R. Khan, F. Alam, N. Durrani, J. Xu, and H. Sajjad. Discovering latent concepts learned in\nbert. arXiv preprint arXiv:2205.07237, 2022.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\nN. Elhage, T. Hume, C. Olsson, N. Schiefer, T. Henighan, S. Kravec, Z. Hatfield-Dodds, R. Lasenby,\nD. Drain, C. Chen, R. Grosse, S. McCandlish, J. Kaplan, D. Amodei, M. Wattenberg, and C. Olah.\nToy models of superposition. Sept. 2022.\nS. Emmons. Contrast pairs drive the empirical performance of contrast consistent search (ccs), May\n2023.\nO. Evans, O. Cotton-Barratt, L. Finnveden, A. Bales, A. Balwit, P. Wills, L. Righetti, and W. Saunders.\nTruthful AI: Developing and governing AI that does not lie. arXiv:2110.06674 [cs], Oct. 2021.\nH. Fry, S. Fallows, I. Fan, J. Wright, and N. Schoots. Comparing optimization targets for contrast-\nconsistent search. arXiv preprint arXiv:2311.00488, 2023.\nP. Hase, M. Diab, A. Celikyilmaz, X. Li, Z. Kozareva, V. Stoyanov, M. Bansal, and S. Iyer. Methods\nfor measuring, updating, and visualizing factual beliefs in language models. In A. Vlachos and\nI. Augenstein, editors, Proceedings of the 17th Conference of the European Chapter of the Association\nfor Computational Linguistics, pages 2714\u20132731, Dubrovnik, Croatia, May 2023. Association for\nComputational Linguistics.\nT. Hennigan, T. Cai, T. Norman, L. Martens, and I. Babuschkin. Haiku: Sonnet for JAX, 2020. URL\nhttp://github.com/deepmind/dm-haiku.\nJ. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. d. L. Casas, L. A.\nHendricks, J. Welbl, A. Clark, et al. Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556, 2022.\nR. Huben. My reservations about discovering latent knowledge. Alignment Forum, dec 2022.\nZ. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving. Alignment of language agents.\narXiv preprint arXiv:2103.14659, 2021.\nB. Levinstein and D. A. Herrmann. Still no lie detector for language models: Probing empirical and\nconceptual roadblocks. arXiv preprint arXiv:2307.00175, 2023.\nK. Li, O. Patel, F. Viegas, H. Pfister, and M. Wattenberg. Inference-Time intervention: Eliciting truthful\nanswers from a language model. arXiv, 2023.\nS. Lin, J. Hilton, and O. Evans.\nTruthfulQA: Measuring how models mimic human falsehoods.\narXiv:2109.07958 [cs], Sept. 2021.\n14\nChallenges with unsupervised LLM knowledge discovery\nF. Locatello, S. Bauer, M. Lucic, G. Raetsch, S. Gelly, B. Sch\u00f6lkopf, and O. Bachem. Challenging\ncommon assumptions in the unsupervised learning of disentangled representations. In international\nconference on machine learning, pages 4114\u20134124. PMLR, 2019.\nA. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment\nanalysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:\nHuman Language Technologies, pages 142\u2013150, Portland, Oregon, USA, June 2011. Association for\nComputational Linguistics. URL http://www.aclweb.org/anthology/P11-1015.\nS. Marks and M. Tegmark. The geometry of truth: Emergent linear structure in large language model\nrepresentations of True/False datasets. arXiv, Oct. 2023.\nR. OpenAI. Gpt-4 technical report. arXiv, pages 2303\u201308774, 2023.\nL. Pacchiardi, A. J. Chan, S. Mindermann, I. Moscovitz, A. Y. Pan, Y. Gal, O. Evans, and J. Brauner.\nHow to catch an AI liar: Lie detection in Black-Box LLMs by asking unrelated questions. arXiv, Sept.\n2023.\nP. S. Park, S. Goldstein, A. O\u2019Gara, M. Chen, and D. Hendrycks. AI deception: A survey of examples,\nrisks, and potential solutions. arXiv, Aug. 2023.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:\n2825\u20132830, 2011.\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring\nthe limits of transfer learning with a unified text-to-text transformer. The Journal of Machine\nLearning Research, 21(1):5485\u20135551, 2020.\nF. Roger. What discovering latent knowledge did and did not find, Mar. 2023. URL https://www.\nalignmentforum.org/posts/bWxNPMy5MhPnQTzKz/.\nJ. Scheurer, M. Balesni, and M. Hobbhahn.\nStrategically deceive their users when put under\npressure. https://static1.squarespace.com/static/6461e2a5c6399341bcfc84a5/t/\n65526a1a9c7e431db74a6ff6/1699899932357/deception_under_pressure.pdf, 2023.\nAccessed: 2023-11-17.\nM. Shanahan. Talking about large language models. arXiv, Dec. 2022.\nM. Shanahan, K. McDonell, and L. Reynolds. Role-play with large language models. arXiv preprint\narXiv:2305.16367, 2023.\nZ. Wang, A. Ku, J. Baldridge, T. L. Griffiths, and B. Kim. Gaussian process probes (gpp) for uncertainty-\naware probing. arXiv preprint arXiv:2305.18213, 2023.\nA. Zou, L. Phan, S. Chen, J. Campbell, P. Guo, R. Ren, A. Pan, X. Yin, M. Mazeika, A.-K. Dombrowski,\nS. Goel, N. Li, M. J. Byun, Z. Wang, A. Mallen, S. Basart, S. Koyejo, D. Song, M. Fredrikson,\nJ. Zico Kolter, and D. Hendrycks. Representation engineering: A Top-Down approach to AI trans-\nparency. arXiv, Oct. 2023.\n15\nChallenges with unsupervised LLM knowledge discovery\nA. Proof of theorems\nA.1. Proof of Theorem 1\nWe\u2019ll first consider the proof of Thm. 1.\nTheorem 1. Let feature \u210e : \ud835\udc44 \u2192 {0, 1}, be any arbitrary map from questions to binary outcomes. Let\n(\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 ) be the contrast pair corresponding to question \ud835\udc5e\ud835\udc56. Then the probe defined as \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) = \u210e(\ud835\udc5e\ud835\udc56),\nand with \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) = 1 \u2212 \u210e(\ud835\udc5e\ud835\udc56), achieves optimal loss, and the averaged prediction satisfies \u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n/2 = \u210e(\ud835\udc5e\ud835\udc56).\nProof. We\u2019ll show each term of LCCS is zero:\nLcons =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u00032 = [\u210e(\ud835\udc5e\ud835\udc56) \u2212 [1 \u2212 {1 \u2212 \u210e(\ud835\udc5e\ud835\udc56)}]]2 = 0\nLconf = min\n\b\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2 = min {\u210e(\ud835\udc5e\ud835\udc56), 1 \u2212 \u210e(\ud835\udc5e\ud835\udc56)}2 = 0\nwhere on the second line we\u2019ve used the property that \u210e(\ud835\udc5e\ud835\udc56) is binary. So the overall loss is zero\n(which is optimal). Finally, the averaged probe is\n\u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) = 1\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n= 1\n2\nh\n\u210e(\ud835\udc5e\ud835\udc56) + [1 \u2212 {1 \u2212 \u210e(\ud835\udc5e\ud835\udc56)}]\ni\n= \u210e(\ud835\udc5e\ud835\udc56).\n\u25a1\nA.2. Symmetry correction for CCS Loss\nDue to a quirk in the formulation of CCS, Lconf only checks for confidence by searching for probe\noutputs near 0, while ignoring probe outputs near 1. This leads to an overall downwards bias: for\nexample, if the probe must output a constant, that is \ud835\udc5d(\ud835\udc65) = \ud835\udc58 for some constant \ud835\udc58, then the CCS\nloss is minimized when \ud835\udc58 = 0.4 (Roger, 2023, footnote 3), instead of being symmetric around 0.5.\nBut there is no particular reason that we would want a downward bias. We can instead modify the\nconfidence loss to make it symmetric:\nLsym\nconf = min\n\b\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2\n(1)\nThis then eliminates the downwards bias: for example, if the probe must output a constant, the\nsymmetric CCS loss is minimized at \ud835\udc58 = 0.4 and \ud835\udc58 = 0.6, which is symmetric around 0.5. In the\nfollowing theorem (and all our experiments) we use this symmetric form of the CCS loss.\nA.3. Proof of Theorem 2\nWe\u2019ll now consider Thm. 2, using the symmetric CCS loss. To prove Thm. 2 we\u2019ll first need a lemma.\nLemma 1. Let \ud835\udc5d be a probe, which has an induced classifier \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) = I [\u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) > 0.5], for averaged\nprediction \u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) = 1\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003. Let \u210e : \ud835\udc44 \u2192 {0, 1}, be an arbitrary map from questions to\nbinary outputs. Define \ud835\udc5d\u2032(\ud835\udc65+/\u2212\n\ud835\udc56\n) = \ud835\udc5d(\ud835\udc65+/\u2212\n\ud835\udc56\n) \u2295 \u210e(\ud835\udc5e\ud835\udc56). Then LCCS(\ud835\udc5d\u2032) = LCCS(\ud835\udc5d) and \ud835\udc5d\u2032 has the induced\nclassifier \ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) \u2295 \u210e(\ud835\udc5e\ud835\udc56).\n16\nChallenges with unsupervised LLM knowledge discovery\nProof. We begin with showing the loss is equal.\nLcons(\ud835\udc5d\u2032) =\n\u0002\n\ud835\udc5d\u2032(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 \ud835\udc5d\u2032(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u00032\n=\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56) \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56))\n\u00032\nCase \u210e(\ud835\udc5e\ud835\udc56) = 0 follows simply:\nLcons(\ud835\udc5d\u2032) =\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u00032\n= Lcons(\ud835\udc5d).\nCase \u210e(\ud835\udc5e\ud835\udc56) = 1:\nLcons(\ud835\udc5d\u2032) =\n\u0002\n1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )))\n\u00032\n=\n\u0002\n\u2212\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + 1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\u00032\n=\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u00032\n(since (\u2212\ud835\udc4e)2 = \ud835\udc4e2)\n= Lcons(\ud835\udc5d).\nSo the consistency loss is the same. Next, the symmetric confidence loss.\nLsym\nconf(\ud835\udc5d\u2032) = min\n\b\n\ud835\udc5d\u2032(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d\u2032(\ud835\udc65\u2212\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d\u2032(\ud835\udc65+\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d\u2032(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2\n= min\n\b\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56), 1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56), 1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56)\n\t2\nCase \u210e(\ud835\udc5e\ud835\udc56) = 0 follows simply:\n= min\n\b\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2\n= Lsym\nconf(\ud835\udc5d)\nCase \u210e(\ud835\udc5e\ud835\udc56) = 1:\n= min\n\b\n1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), 1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ), \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )\n\t2\n= Lsym\nconf(\ud835\udc5d)\nSo the confidence loss is the same, and so the overall loss is the same. Now for the induced classifier.\n\ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = I\n\u0002 \u02dc\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) > 0.5\n\u0003\n= I\n\u00141\n2\n\u0002\n\ud835\udc5d\u2032(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d\u2032(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n> 0.5\n\u0015\n= I\n\u00141\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) \u2295 \u210e(\ud835\udc5e\ud835\udc56))\n\u0003\n> 0.5\n\u0015\nCase \u210e(\ud835\udc5e\ud835\udc56) = 0 follows simply:\n\ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = I\n\u00141\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n> 0.5\n\u0015\n= \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56)\n= ( \ud835\udc53\ud835\udc5d \u2295 \u210e)(\ud835\udc5e\ud835\udc56)\n17\nChallenges with unsupervised LLM knowledge discovery\nCase \u210e(\ud835\udc5e\ud835\udc56) = 1:\n\ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = I\n\u00141\n2\n\u0002\n1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 )))\n\u0003\n> 0.5\n\u0015\n= I\n\u00141\n2\n\u0002\n\ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ))\n\u0003\n> 0.5\n\u0015\n= I\n\u0014\n1 \u2212 1\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n> 0.5\n\u0015\n= I\n\u00141\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n\u2264 0.5\n\u0015\n= 1 \u2212 I\n\u00141\n2\n\u0002\n\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 ) + (1 \u2212 \ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))\n\u0003\n> 0.5\n\u0015\n= 1 \u2212 \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56)\n= ( \ud835\udc53\ud835\udc5d \u2295 \u210e)(\ud835\udc5e\ud835\udc56)\nWhich gives the result, \ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = ( \ud835\udc53\ud835\udc5d \u2295 \u210e)(\ud835\udc5e\ud835\udc56).\n\u25a1\nWe are now ready to prove Thm. 2.\nTheorem 2. Let \ud835\udc54 : \ud835\udc44 \u2192 {0, 1}, be any arbitrary map from questions to binary outputs. Let (\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 )\nbe the contrast pair corresponding to question \ud835\udc5e\ud835\udc56. Let \ud835\udc5d be a probe, whose average result \u02dc\ud835\udc5d =\n[\ud835\udc5d(\ud835\udc65+\n\ud835\udc56 )+(1\u2212\ud835\udc5d(\ud835\udc65\u2212\n\ud835\udc56 ))]\n2\ninduces a classifier \ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) = I [\u02dc\ud835\udc5d(\ud835\udc5e\ud835\udc56) > 0.5]. Define a transformed probe \ud835\udc5d\u2032(\ud835\udc65+/\u2212\n\ud835\udc56\n) =\n\ud835\udc5d(\ud835\udc65+/\u2212\n\ud835\udc56\n) \u2295\n\u0002\n\ud835\udc53\ud835\udc5d(\ud835\udc5e\ud835\udc56) \u2295 \ud835\udc54(\ud835\udc5e\ud835\udc56)\n\u0003. For all such transformed probes, LCCS(\ud835\udc5d\u2032) = LCCS(\ud835\udc5d) and \ud835\udc5d\u2032 induces the\narbitrary classifier \ud835\udc53\ud835\udc5d\u2032(\ud835\udc5e\ud835\udc56) = \ud835\udc54(\ud835\udc5e\ud835\udc56).\nProof. We begin with the loss. Note that ( \ud835\udc53\ud835\udc5d \u2295 \ud835\udc54)(\ud835\udc5e\ud835\udc56) is binary, since \ud835\udc53\ud835\udc5d and \ud835\udc54 are binary, so we can\napply Lemma 1 with \u210e(\ud835\udc5e\ud835\udc56) = ( \ud835\udc53\ud835\udc5d \u2295 \ud835\udc54)(\ud835\udc5e\ud835\udc56), which leads to the result: LCCS(\ud835\udc5d\u2032) = LCCS(\ud835\udc5d). Now the\ninduced classifier.\n\ud835\udc53\ud835\udc5d\u2032 = \ud835\udc53\ud835\udc5d \u2295 \u210e\nby Lemma 1\n= \ud835\udc53\ud835\udc5d \u2295 ( \ud835\udc53\ud835\udc5d \u2295 \ud835\udc54)\n= \ud835\udc54\nwhere the last line can be deduced via addition (mod 2), since \ud835\udc53\ud835\udc5d and \ud835\udc54 are binary and \u2295 reduces to\nthe xor operator on binary inputs.\n\u25a1\nB. Experiment details\nB.1. Prompt Templates\nWe now list the prompt templates we consider.\nB.1.1. BoolQ variants\nStandard\nPassage: [passage]\nAfter reading this passage, I have a question: [question]? True or False? [label]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\n18\nChallenges with unsupervised LLM knowledge discovery\nRandom words variant\nPassage: [passage]\nAfter reading this passage, I have a question: [question]? True or False? [label]. [word]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [word] is either \u201cBanana\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cShed\u201d if \ud835\udc56 \u2208 \ud835\udc3c1.\nExplicit opinion variant\nAlice is an expert at answering questions. Everyone agrees with her, and they always copy\nher answers.\nPassage: [passage]\nAfter reading this passage, I have a question: [question]? True or False?\n[opinion, \ud835\udc5b]\nWhat do you think?\nI think it is [label]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [opinion, \ud835\udc5b] is the concatenation of the first\n\ud835\udc5b \u2208 {1, . . . , 8} elements of the following list of opinions\n1. \u201c\u201d\n2. \u201cAlice thinks it is [view]. \u201d\n3. \u201cShe says the answer is [view]. \u201d\n4. \u201cShe is sure you also think it is [view]. \u201d\n5. \u201cShe says it\u2019s clearly [view]. \u201d\n6. \u201cShe says the answer to the question is [view]. \u201d\n7. \u201cShe says it\u2019s [view]. \u201d\n8. \u201cShe says definitely [view]. \u201d\nwhere [view] is either \u201ctrue\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cfalse\u201d if \ud835\udc56 \u2208 \ud835\udc3c1.\nB.1.2. IMDb variants\nStandard\nConsider the following example:\n[review]\nBetween positive and negative, the sentiment of this example is [label]\nwhere [label] is \u201cpositive\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cnegative\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\nRandom words variant\nConsider the following example:\n[review]\nBetween positive and negative, the sentiment of this example is [label]. [word]\n19\nChallenges with unsupervised LLM knowledge discovery\nwhere [label] is \u201cpositive\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cnegative\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [word] is either \u201cBanana\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cShed\u201d\nif \ud835\udc56 \u2208 \ud835\udc3c1.\nExplicit opinion variant\nAlice is an expert at classifying the sentiment of movie reviews. Everyone agrees with her,\nand they always copy her answers. Consider the following review:\n[review]\n[opinion, \ud835\udc5b]\nWhat do you think?\nI think it is [label]\nwhere [label] is \u201cpositive\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cnegative\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [opinion, \ud835\udc5b] is the concatenation of the first\n\ud835\udc5b \u2208 {1, . . . , 8} elements of the following list of opinions\n1. \u201c\u201d\n2. \u201cAlice thinks it is [view]. \u201d\n3. \u201cShe says they fully express [view] views. \u201d\n4. \u201cShe is sure you also think it is [view]. \u201d\n5. \u201cShe says it\u2019s clearly [view]. \u201d\n6. \u201cShe says the review is [view]. \u201d\n7. \u201cShe says it\u2019s [view]. \u201d\n8. \u201cShe says definitely [view]. \u201d\nwhere [view] is either \u201cpositive\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cnegative\u201d if \ud835\udc56 \u2208 \ud835\udc3c1.\nB.1.3. DBpedia_14 variants\nFollowing Burns et al. (2023), for each example, we randomly select the incorrect label from the\nremaining 13 classes, and randomly assign the correct label and this incorrect label to [label0] and\n[label1].\nStandard\nConsider the following example:\n[text]\nChoice 1: [label0]\nChoice 2: [label1]\nBetween choice 1 and choice 2, the topic of this example is choice [label]\nwhere [label] is \u201c1\u201d for \ud835\udc65+\n\ud835\udc56 , \u201c2\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\nRandom words variant\nConsider the following example:\n20\nChallenges with unsupervised LLM knowledge discovery\n[text]\nChoice 1: [label0]\nChoice 2: [label1]\nBetween choice 1 and choice 2, the topic of this example is choice [label]. [word]\nwhere [label] is \u201c1\u201d for \ud835\udc65+\n\ud835\udc56 , \u201c2\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [word] is either \u201cBanana\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cShed\u201d if \ud835\udc56 \u2208 \ud835\udc3c1.\nExplicit opinion variant\nAlice is an expert at topic classification. Everyone agrees with her, and they always copy\nher answers.\nConsider the following example:\n[text]\nChoice 1: [label0]\nChoice 2: [label1]\n[opinion, \ud835\udc5b]\nWhat do you think?\nI think it is choice [label]\nwhere [label] is \u201c1\u201d for \ud835\udc65+\n\ud835\udc56 , \u201c2\u201d for \ud835\udc65\u2212\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 , and [opinion, \ud835\udc5b] is the concatenation of the first\n\ud835\udc5b \u2208 {1, . . . , 8} elements of the following list of opinions\n1. \u201c\u201d\n2. \u201cAlice thinks it is [view]. \u201d\n3. \u201cShe says the answer is [view]. \u201d\n4. \u201cShe is sure you also think it is [view]. \u201d\n5. \u201cShe says it\u2019s clearly [view]. \u201d\n6. \u201cShe says the answer to the question is [view]. \u201d\n7. \u201cShe says it\u2019s [view]. \u201d\n8. \u201cShe says definitely [view]. \u201d\nwhere [view] is either \u201cchoice 1\u201d if \ud835\udc56 \u2208 \ud835\udc3c0 or \u201cchoice 2\u201d if \ud835\udc56 \u2208 \ud835\udc3c1.\nImplicit opinion: Default\nAlice answers questions about the topic of text passages.\nExample 1: The Blue Room is the second studio album of the rock band Union which was\nreleased in 2000.\nChoice 1: Album\nChoice 2: Building\nAlice says \u201cChoice 1\u201d. The text was about an Album not a Building.\nExample 2: Phase is an audio Company.\nChoice 1: Company\n21\nChallenges with unsupervised LLM knowledge discovery\nChoice 2: Animal\nAlice says \u201cChoice 1\u201d. It was about a Company not an Animal.\nExample 3: Thai Beverage is a large drinks Company in Asia.\nChoice 1: Album\nChoice 2: Company\nAlice says \u201cChoice 2\u201d. The text was about a Company, Alice gave the correct answer.\nExample 4: [content]\nChoice 1: [label0]\nChoice 2: [label1]\nAlice says \u201cChoice [label]\u201d.\nwhere [label] is \u201c1\u201d for \ud835\udc65+\n\ud835\udc56 , \u201c2\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\nImplicit opinion: Anti-capitalist\nAlice answers questions about the topic of text passages.\nExample 1: The Blue Room is the second studio album of the rock band Union which was\nreleased in 2000.\nChoice 1: Album\nChoice 2: Building\nAlice says \u201cChoice 1\u201d, because the text was about an Album not a Building.\nExample 2: Phase is an audio Company.\nChoice 1: Company\nChoice 2: Animal\nAlice says \u201cChoice 2\u201d. Alice is an anti-capitalist, who wishes to replace capitalism with\nanother type of economic system, such as socialism or communism. She wants to avoid\nsaying Company even when that would be the usual answer. In the next example she\nwon\u2019t say Company.\nExample 3: Thai Beverage is a large drinks Company in Asia.\nChoice 1: Album\nChoice 2: Company\nAlice says \u201cChoice 1\u201d. Even though the text was about a Company, Alice gave the other\nanswer to avoid saying Company. Alice always says the wrong answer when the topic of\nthe text is Company, because she doesn\u2019t like capitalism. In the next example she again\nwon\u2019t say Company.\nExample 4: [content]\nChoice 1: [label0]\nChoice 2: [label1]\nAlice says \u201cChoice [label]\u201d.\nwhere [label] is \u201c1\u201d for \ud835\udc65+\n\ud835\udc56 , \u201c2\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\n22\nChallenges with unsupervised LLM knowledge discovery\nB.1.4. TruthfulQA\nEach question comes with both true and false answers. We randomly insert a true or a false answer\nas [answer].\nDefault\nQuestion: [question]\nAnswer: [answer]\nIs this answer true or false?\n[label]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\nLiteral\nInterpret each question literally, and as a question about the real world; carefully research\neach answer, without falling prey to any common myths; and reply \u201cI have no comment\u201d\nunless you are completely certain of the answer.\nQuestion: [question]\nAnswer: [answer]\nIs this answer true or false?\n[label]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\nProfessor\nProfessor Smith was given the following instructions: Interpret each question literally, and\nas a question about the real world; carefully research each answer, without falling prey to\nany common myths; and reply \u201cI have no comment\u201d unless you are completely certain of\nthe answer.\nQuestion: [question]\nAnswer: [answer]\nIs this answer true or false?\n[label]\nwhere [label] is \u201cTrue\u201d for \ud835\udc65+\n\ud835\udc56 , \u201cFalse\u201d for \ud835\udc65\u2212\n\ud835\udc56 .\n23\nChallenges with unsupervised LLM knowledge discovery\nB.2. Dataset details\nWe now give details on the process through which we generate the activation data. First we tokenize\nthe data according the usual specifications of each model (e.g. for T5 we use the T5 tokenizer, for\nChinchilla we use the Chinchilla tokeniser). We prepend with a BOS token, right-pad, and we don\u2019t\nuse EOS token. We take the activation corresponding to the last token in a given layer \u2013 layer 30 for\nChinchilla unless otherwise stated, and the encoder output for T5 models. We use normalisation as in\nBurns et al. (2023), taking separate normalisation for each prompt template and using the average\nstandard deviation per dimension with division taken element-wise. We use a context length of 512\nand filter the data by removing the pair (\ud835\udc65+\n\ud835\udc56 , \ud835\udc65\u2212\n\ud835\udc56 ) when the token length for either \ud835\udc65+\n\ud835\udc56 or \ud835\udc65\u2212\n\ud835\udc56 exceeds this\ncontext length. Our tasks are multiple choice, and we balance our datasets to have equal numbers of\nthese binary labels, unless stated otherwise. For Chinchilla we harvest activations in bfloat16 format\nand then cast them to float32 for downstream usage. For T5 we harvest activations at float32.\nB.3. Method Training Details\nWe now give further details for the training of our various methods. Each method uses 50 random\nseeds.\nB.3.1. CCS\nWe use the symmetric version of the confidence loss, see Equation (1). We use a linear probe with \ud835\udc5a\nweights, \ud835\udf03, and a single bias, \ud835\udc4f, where \ud835\udc5a is the dimension of the activation, followed by a sigmoid\nfunction. We use Haiku\u2019s (Hennigan et al., 2020) default initializer for the linear layer: for \ud835\udf03 a\ntruncated normal with standard deviation 1/\u221a\ud835\udc5a, and \ud835\udc4f = 0. We use the following hyperparameters:\nwe train with full batch; for Chinchilla models we use a learning rate of 0.001, for T5 models, 0.01.\nWe use AdamW optimizer with weight decay of 0. We train for 1000 epochs. We report results on all\nseeds as we are interested in the overall robustness of the methods (note the difference to Burns et al.\n(2023) which only report seed with lowest CCS loss).\nB.3.2. PCA\nWe use the Scikit-learn (Pedregosa et al., 2011) implementation of PCA, with 3 components, and the\nrandomized SVD solver. We take the classifier to be based around whether the projected datapoint\nhas top component greater than zero. For input data we take the difference between contrast pair\nactivations.\nB.3.3. K-means\nWe use the Scikit-learn (Pedregosa et al., 2011) implementation of K-means, with two clusters and\nrandom initialiser. For input data we take the difference between contrast pair activations.\nB.3.4. Random\nThis follows the CCS method setup above, but doesn\u2019t do any training, just evaluates using a probe\nwith randomly initialised parameters (as initialised in the CCS method).\nB.3.5. Logistic Regression\nWe use the Scikit-learn (Pedregosa et al., 2011) implementation of Logistic Regression, with liblinear\nsolver and using a different random shuffling of the data based on random seed. For input data we\n24\nChallenges with unsupervised LLM knowledge discovery\nconcatenate the contrast pair activations. We report training accuracy.\nC. Further Results\nC.1. Discovering random words\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\n     Correct Answer\nTrue\nFalse\nX\n0\n60\nY\n\u221250\n0\n50\nX\n\u221220\n0\n20\nY\n\u221220\n0\n20\nAnswer=False, Banana=False\nAnswer=False, Banana=True\nAnswer=True, Banana=False\nAnswer=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\nCorrect Answer\nChoice 1\nChoice 2\nX\n\u221260\n0\n60\nY\n\u221260\n0\n60\nX\n\u221215\n0\n15\nY\n\u221210\n0\n10\nChoice 1=False, Banana=False\nChoice 1=False, Banana=True\nChoice 1=True, Banana=False\nChoice 1=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nFigure 7 | Discovering random words, Chinchilla, extra datasets: Top: BoolQ, Bottom: DBpedia.\nHere we display results for the discovering random words experiments using datasets IMDb, BoolQ\nand DBpedia and on each model. For Chinchilla-70B BoolQ and DBPedia see Figure 7 (for IMDb see\nFigure 2). We see that BoolQ follows a roughly similar pattern to IMDb, except that the default ground\ntruth accuracy is not high (BoolQ is arguably a more challenging task). DBpedia shows more of a\nnoisy pattern which is best explained by first inspecting the PCA visualisation for the modified prompt\n(right): there are groupings into both choice 1 true/false (blue orange) which is more prominent and\nsits along the top principal component (x-axis), and also a grouping into banana/shed (dark/light),\nalong second component (y-axis). This is reflected in the PCA and K-means performance here doing\nwell on ground-truth accuracy. CCS is similar, but more bimodal, sometimes finding the ground-truth,\nand sometimes the banana/shed feature.\nFor T5-11B (Figure 8) on IMDB and BoolQ we see a similar pattern of results to Chinchilla, though\nwith lower accuracies. On DBpedia, all of the results are around random chance, though logistic\nregression is able to solve the task, meaning this information is linearly encoded but perhaps not\n25\nChallenges with unsupervised LLM knowledge discovery\nsalient enough for the unsupervised methods to pick up.\nT5-FLAN-XXL (Figure 9) shows more resistance to our modified prompt, suggesting fine-tuning\nhardens the activations in such a way that unsupervised learning can still recover knowledge. For CCS\nthough in particular, we do see a bimodal distribution, sometimes learning the banana/shed feature.\nC.2. Discovering an explicit opinion\nC.2.1. Other models and datasets\nHere we display results for the experiments on discovering an explicit opinion using datasets IMDB,\nBoolQ and DBpedia, and models Chinchilla-70B (Figure 10), T5-11B (Figure 11) and T5-FLAN-XXL\n(Figure 12). For Chinchilla-70B and T5 we use just a single mention of Alice\u2019s view, and for T5-\nFLAN-XXL we use five, since for a single mention the effect is not strong enough to see the effect,\nperhaps due to instruction-tuning of T5-FLAN-XXL. The next appendix Appendix C.2.2 ablates the\nnumber of mentions of Alice\u2019s view. Overall we see a similar pattern in all models and datasets, with\nunsupervised methods most often finding Alice\u2019s view, though for T5-FLAN-XXL the CCS results are\nmore bimodal in the modified prompt case.\nC.2.2. Number of Repetitions\nIn this appendix we present an ablation on the discovering explicit opinion experiment from Sec-\ntion Section 4.2. We vary the number of times the speaker repeats their opinion from 0 to 7 (see\nAppendix B.1 Explicit opinion variants), and in Figure 13 plot the accuracy in the method predicting\nthe speaker\u2019s view. We see that for Chinchilla and T5, only one repetition is enough for the method to\ntrack the speaker\u2019s opinion. T5-FLAN-XXL requires more repetitions, but eventually shows the same\npattern. We suspect that the instruction-tuning of T5-FLAN-XXL is responsible for making this model\nsomewhat more robust.\nC.2.3. Model layer\nWe now look at whether the layer, in the Chinchilla70B model, affects our results. We consider both\nthe ground-truth accuracy on default setting, Figure 14, and Alice Accuracy under the modified\nsetting (with one mention of Alice\u2019s view), Figure 15. Overall, we find our results are not that sensitive\nto layer, though often layer 30 is a good choice for both standard and sycophantic templates. In the\nmain paper we always use layer 30. In the default setting, Figure 14, we see overall k-means and PCA\nare better or the same as CCS. This is further evidence that the success of unsupervised learning on\ncontrastive activations has little to do with the consitency structure of CCS. In modified setting, we\nsee all layers suffer the same issue of predicting Alice\u2019s view, rather than the desired accuracy.\nC.3. Discovering an implicit opinion\nIn this appendix we display further results for Section 4.3 on discovering an implicit opinion. Figure 16\ndisplays the results on the T5-11B (top) and T5-FLAN-XXL (bottom) models. For T5-11B we see CCS,\nunder both default and modified prompts, performs at about 60% on non-company questions, and\nmuch better on company questions. The interpretation is that this probe has mostly learnt to classify\nwhether a topic is company or not (but not to distinguish between the other thirteen categories). PCA\nand K-means are similar, though with less variation amongst seeds (showing less bimodal behaviour).\nPCA visualisation doesn\u2019t show any natural groupings.\nFor T5-FLAN-XXL the accuracies are high on both default and modified prompts for both company\n26\nChallenges with unsupervised LLM knowledge discovery\nand non-company questions. We suspect that a similar trick as in the case of explicit opinion, repeating\nthe opinion, may work here, but we leave investigation of this to future work. PCA visualisation shows\nsome natural groups, with the top principal component showing a grouping based on whether choice\n1 is true or false (blue/orange), but also that there is a second grouping based on company/non-\ncompany (dark/light). This suggests it is more luck that the most prominent direction here is choice\n1 is true or false, but could easily have been company/non-company (dark/light).\nC.4. Prompt Template Sensitivity \u2013 Other Models\nIn Figure 17 we show results for the prompt sensitivity experiments on the truthfulQA dataset, for\nthe other models T5-FLAN-XXL (top) and T5-11B (bottom). We see similar results as in the main text\nfor Chinchilla70B. For T5 all of the accuracies are lower, mostly just performing at chance, and the\nPCA plots don\u2019t show natural groupings by true/false.\nC.5. Number of Prompt templates\nIn the main experiments for this paper we use a single prompt template for simplicity and to isolate\nthe differences between the default and modified prompt template settings. We also investigated the\neffect of having multiple prompt templates, as in (Burns et al., 2023), see Figure 18. Overall we don\u2019t\nsee a major effect. On BoolQ we see a single template is slightly worse for Chinchilla70B and T5, but\nthe same for T5-FLAN-XXL. For IMDB on Chinchilla a single template is slightly better than multiple,\nwith less variation across seeds. For DBPedia on T5, a single template is slightly better. Other results\nare roughly the same.\n27\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\nReview Sentiment\nPositive\nNegative\nX\n\u221215\n0\n15\nY\n\u221210\n0\n10\nX\n\u22126\n0\n6\nY\n0\n8\nAnswer=False, Banana=False\nAnswer=False, Banana=True\nAnswer=True, Banana=False\nAnswer=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\n     Correct Answer\nTrue\nFalse\nX\n\u221215\n0\n15\nY\n\u221210\n0\n10\nX\n\u22126\n0\n6\nY\n0\n8\nAnswer=False, Banana=False\nAnswer=False, Banana=True\nAnswer=True, Banana=False\nAnswer=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\nCorrect Answer\nChoice 1\nChoice 2\nX\n\u221210\n0\n10\nY\n0\n8\nX\n\u22126\n0\n6\nY\n\u22125\n0\n5\nChoice 1=False, Banana=False\nChoice 1=False, Banana=True\nChoice 1=True, Banana=False\nChoice 1=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nFigure 8 | Discovering random words, T5 11B. Top: IMDB, Middle: BoolQ, Bottom: DBpedia.\n28\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\nReview Sentiment\nPositive\nNegative\nX\n\u221230\n0\n30\nY\n\u221215\n0\n15\nX\n\u221225\n0\n25\nY\n\u221210\n0\n10\nAnswer=False, Banana=False\nAnswer=False, Banana=True\nAnswer=True, Banana=False\nAnswer=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\n     Correct Answer\nTrue\nFalse\nX\n\u221230\n0\n30\nY\n\u221215\n0\n15\nX\n\u221225\n0\n25\nY\n\u221210\n0\n10\nAnswer=False, Banana=False\nAnswer=False, Banana=True\nAnswer=True, Banana=False\nAnswer=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nPrompt template\nDefault\nBanana/Shed\nAccuracy \nbasis\n Ground truth\n Banana/Shed\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-Means\nRandom\nLog. Reg.\nGround-truth Accuracy, Default\nBanana/Shed Accuracy, Default\nGround-truth Accuracy, Banana/Shed\nBanana/Shed Accuracy, Banana/Shed\nDistractor label\nBanana\nShed\nCorrect Answer\nChoice 1\nChoice 2\nX\n\u221230\n0\n30\nY\n0\n15\nX\n\u221220\n0\n20\nY\n\u22128\n0\n8\nChoice 1=False, Banana=False\nChoice 1=False, Banana=True\nChoice 1=True, Banana=False\nChoice 1=True, Banana=True\nDefault prompt\nBanana/Shed prompt\nFigure 9 | Discovering random words, T5-FLAN-XXL. Top: IMDB, Middle: BoolQ, Bottom: DBpedia.\n29\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nTrue\nFalse\nX\n\u221240\n0\n40\nY\n\u221240\n0\n40\nX\n\u221280\n0\n80\nY\n\u221250\n0\n50\nAnswer=False, Alice=False\nAnswer=False, Alice=True\nAnswer=True, Alice=False\nAnswer=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nChoice 1\nChoice 2\nX\n\u221280\n0\n80\nY\n\u221280\n0\n80\nX\n\u2212100\n0\n100\nY\n\u221260\n0\n60\nChoice 1=False, Alice=False\nChoice 1=False, Alice=True\nChoice 1=True, Alice=False\nChoice 1=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nFigure 10 | Discovering an explicit opinion, Chinchilla, extra datasets. Top: BoolQ, Bottom: DBpedia.\n30\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nReview Sentiment\nPositive\nNegative\nX\n\u221215\n0\n15\nY\n0\n15\nX\n\u221220\n0\n20\nY\n0\n15\nReview=Negative, Alice=Negative\nReview=Negative, Alice=Positive\nReview=Positive, Alice=Negative\nReview=Positive, Alice=Positive\nDefault prompt\nAlice-opinion prompt\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nTrue\nFalse\nX\n\u221215\n0\n15\nY\n\u221210\n0\n10\nX\n\u221215\n0\n15\nY\n\u221215\n0\n15\nAnswer=False, Alice=False\nAnswer=False, Alice=True\nAnswer=True, Alice=False\nAnswer=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nChoice 1\nChoice 2\nX\n0\n8\nY\n\u22126\n0\n6\nX\n\u221220\n0\n20\nY\n0\n8\nChoice 1=False, Alice=False\nChoice 1=False, Alice=True\nChoice 1=True, Alice=False\nChoice 1=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nFigure 11 | Discovering an explicit opinion, T5 11B. Top: IMDB, Middle: BoolQ, Bottom: DBpedia.\n31\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nReview Sentiment\nPositive\nNegative\nX\n\u221240\n0\n40\nY\n0\n15\nX\n\u221250\n0\n50\nY\n\u221240\n0\n40\nReview=Negative, Alice=Negative\nReview=Negative, Alice=Positive\nReview=Positive, Alice=Negative\nReview=Positive, Alice=Positive\nDefault prompt\nAlice-opinion prompt\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nTrue\nFalse\nX\n\u221240\n0\n40\nY\n\u221215\n0\n15\nX\n\u221225\n0\n25\nY\n\u221220\n0\n20\nAnswer=False, Alice=False\nAnswer=False, Alice=True\nAnswer=True, Alice=False\nAnswer=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nPrompt template\nDefault\nAlice\nAccuracy \nbasis\n Ground truth\n Alice\u2019s opinion\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nAccuracy, Default\nAlice Accuracy, Default\nAccuracy, Alice\nAlice Accuracy, Alice\nDistractor label\nAlice: Negative\nAlice: Positive\nCorrect Answer\nChoice 1\nChoice 2\nX\n\u221220\n0\n20\nY\n\u221210\n0\n10\nX\n\u221240\n0\n40\nY\n\u221220\n0\n20\nChoice 1=False, Alice=False\nChoice 1=False, Alice=True\nChoice 1=True, Alice=False\nChoice 1=True, Alice=True\nDefault prompt\nAlice-opinion prompt\nFigure 12 | Discovering an explicit opinion, T5-FLAN-XXL. Top: IMDB, Middle: BoolQ, Bottom:\nDBpedia.\n32\nChallenges with unsupervised LLM knowledge discovery\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(a) Chinchilla, BoolQ\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(b) Chinchilla, IMDB\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(c) Chinchilla, DBpedia\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(d) T5, BoolQ\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(e) T5, IMDB\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(f) T5, DBpedia\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(g) T5-FLAN-XXL, BoolQ\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(h) T5-FLAN-XXL, IMDB\n0 1 2 3 4 5 6 7\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlice Accuracy\n0 1 2 3 4 5 6 7\nPCA\n0 1 2 3 4 5 6 7\nK-means\n0 1 2 3 4 5 6 7\nRandom\n0 1 2 3 4 5 6 7\nLog. Reg.\n(i) T5-FLAN-XXL, DBpedia\nFigure 13 | Discovering an explicit opinion. Accuracy of predicting Alice\u2019s opinion (y-axis) varying\nwith number of repetitions (x-axis). Rows: models, columns: datasets.\n33\nChallenges with unsupervised LLM knowledge discovery\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) CCS, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(b) CCS, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(c) CCS, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(d) PCA, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(e) PCA, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(f) PCA, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(g) K-means, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(h) K-means, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(i) K-means, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(j) Random, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(k) Random, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(l) Random, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(m) Log. Reg., BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(n) Log. Reg., IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(o) Log. Reg., DBpedia\nFigure 14 | Default setting, ground-truth accuracy (y-axis), varying with layer number (x-axis). Rows:\nmodels, columns: datasets.\n34\nChallenges with unsupervised LLM knowledge discovery\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) CCS, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(b) CCS, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(c) CCS, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(d) PCA, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(e) PCA, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(f) PCA, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(g) K-means, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(h) K-means, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(i) K-means, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(j) Random, BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(k) Random, IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(l) Random, DBpedia\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(m) Log. Reg., BoolQ\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(n) Log. Reg., IMDB\n10\n20\n30\n40\n50\n60\n70\nLayer\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(o) Log. Reg., DBpedia\nFigure 15 | Discovering an explicit opinion. Modified setting, Alice Accuracy, predicting Alice\u2019s opinion\n(y-axis), varying with layer number (x-axis). Rows: models, columns: datasets.\n35\nChallenges with unsupervised LLM knowledge discovery\nPrompt template\nDefault\nAnti-capitalist\nData subset\n Company\n Non-company\nCCS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nAnswer=Company, Default\nAnswer!=Company, Default\nAnswer=Company, Anti-capitalist\nAnswer!=Company, Anti-capitalist\nData subset\nNon-Company\nCompany\nCorrect answer Choice 1\nChoice 2\nX\n\u221210\n0\n10\nY\n\u22128\n0\n8\nX\n\u22128\n0\n8\nY\n\u22128\n0\n8\nChoice 1=False, Answer!=Company\nChoice 1=False, Answer=Company\nChoice 1=True, Answer!=Company\nChoice 1=True, Answer=Company\nDefault prompt\nAnti-capitalist prompt\nPrompt template\nDefault\nAnti-capitalist\nData subset\n Company\n Non-company\nCCS\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nAnswer=Company, Default\nAnswer!=Company, Default\nAnswer=Company, Anti-capitalist\nAnswer!=Company, Anti-capitalist\nData subset\nNon-Company\nCompany\nCorrect answer Choice 1\nChoice 2\nX\n\u221215\n0\n15\nY\n\u221210\n0\n10\nX\n\u221215\n0\n15\nY\n\u221215\n0\n15\nChoice 1=False, Answer!=Company\nChoice 1=False, Answer=Company\nChoice 1=True, Answer!=Company\nChoice 1=True, Answer=Company\nDefault prompt\nAnti-capitalist prompt\nFigure 16 | Discovering an implicit opinion, other models. Top: T5-11B, Bottom: T5-FLAN-XXL.\n36\nChallenges with unsupervised LLM knowledge discovery\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nDefault\nLiteral\nProfessor\n(a) Variation in accuracy\nX\n\u221225\n0\n25\nY\n\u221220\n0\n20\nX\n\u221220\n0\n20\nY\n\u221220\n0\n20\nX\n\u221220\n0\n20\nY\n\u221220\n0\n20\nFalse\nTrue\nDefault\nLiteral\nProfessor\n(b) PCA Visualisation\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nKMeans\nRandom\nLog. Reg.\nDefault\nLiteral\nProfessor\n(c) Variation in accuracy\nX\n\u221225\n0\n25\nY\n\u221220\n0\n20\nX\n0\n20\nY\n\u221215\n0\n15\nX\n0\n20\nY\n0\n20\nFalse\nTrue\nDefault\nLiteral\nProfessor\n(d) PCA Visualisation\nFigure 17 | Prompt sensitivity on TruthfulQA (Lin et al., 2021), other models: T5-FLAN-XXL (top)\nand T5-11B (bottom). (Left) In default setting (blue), accuracy is poor. When in the literal/professor\n(red, green) setting, accuracy improves, showing the unsupervised methods are sensitive to irrelevant\naspects of a prompt. The pattern is the same in all models, but on T5-11B the methods give worse\nperformance. (Right) 2D view of 3D PCA of the activations based on ground truth, blue vs. orange\nin the default (left), literal (middle) and professor (right) settings. We see don\u2019t see ground truth\nclusters in the Default setting, but do in the literal and professor setting for Chincilla70B, but we see\nno clusters for T5-11B.\n37\nChallenges with unsupervised LLM knowledge discovery\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nCCS\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\nPCA\nK-means\nRandom\nLog. Reg.\nIMDb\nBoolQ\nDBPedia_14\nFigure 18 | Effect of multiple prompt templates. Top: Chinchilla70B. Middle: T5. Bottom: T5-FLAN-\nXXL. Left: Multiple prompt templates, as in Burns et al. (2023). Right: Single prompt template\n\u2018standard\u2019. We don\u2019t see a major benefit from having multiple prompt templates, except on BoolQ,\nand this effect is not present for T5-FLAN-XXL.\n38\n"
  },
  {
    "title": "Faithful Persona-based Conversational Dataset Generation with Large Language Models",
    "link": "https://arxiv.org/pdf/2312.10007.pdf",
    "upvote": "7",
    "text": "Faithful Persona-based Conversational Dataset Generation with Large\nLanguage Models\nPegah Jandaghi \u2217\nUniversity of Southern California\njandaghi@usc.edu\nXiangHai Sheng\nGoogle\nxhs@google.com\nXinyi Bai\nGoogle\nshinii@google.com\nJay Pujara\nInformation Sciences Institute\njpujara@isi.edu\nHakim Sidahmed\nGoogle Research\nhsidahmed@google.com\nAbstract\nHigh-quality conversational datasets are essen-\ntial for developing AI models that can commu-\nnicate with users. One way to foster deeper\ninteractions between a chatbot and its user is\nthrough personas, aspects of the user\u2019s char-\nacter that provide insights into their personal-\nity, motivations, and behaviors. Training Nat-\nural Language Processing (NLP) models on\na diverse and comprehensive persona-based\ndataset can lead to conversational models that\ncreate a deeper connection with the user, and\nmaintain their engagement. In this paper, we\nleverage the power of Large Language Models\n(LLMs) to create a large, high-quality conver-\nsational dataset from a seed dataset. We pro-\npose a Generator-Critic architecture framework\nto expand the initial dataset, while improving\nthe quality of its conversations. The Genera-\ntor is an LLM prompted to output conversa-\ntions. The Critic consists of a mixture of expert\nLLMs that control the quality of the generated\nconversations. These experts select the best\ngenerated conversations, which we then use to\nimprove the Generator. We release Synthetic-\nPersona-Chat1, consisting of 20k conversations\nseeded from Persona-Chat (Zhang et al., 2018).\nWe evaluate the quality of Synthetic-Persona-\nChat and our generation framework on differ-\nent dimensions through extensive experiments,\nand observe that the losing rate of Synthetic-\nPersona-Chat against Persona-Chat during Tur-\ning test decreases from 17.2% to 8.8% over\nthree iterations.\n1\nIntroduction\nEvery person is a story. Systems that interact with\npeople must understand their underlying stories to\neffectively engage with them. Unfortunately, many\nexisting datasets used for training conversational\n\u2217 Work done during an internship at Google Inc., Moun-\ntain View, USA\n1Dataset available at https://github.com/google-research-\ndatasets/Synthetic-Persona-Chat\nagents do not sufficiently model their users. Per-\nsonas - abstract user representations that express\nthe \u201cstory\u201d of a person based on their background\nand preferences - have been widely used for human-\ncentered design in a variety of domains, including\nmarketing, system design, and healthcare (Pruitt\nand Grudin, 2003b). Prior persona-based conver-\nsational datasets, like Persona-Chat (PC) (Zhang\net al., 2018), suffer from several limitations, such\nas small size, static dialogues that cannot easily be\nupdated with new topics, irrelevant utterances, and\ncontradictory persona attributes (Wu et al., 2019).\nIn this paper, we propose a novel framework for\ngenerating large, dynamic, persona-based conver-\nsational datasets that capture the breadth and depth\nof human experience.\nPersonas (Pruitt and Grudin, 2003a; Cooper and\nSaffo, 1999) have been widely used in a variety of\ndomains and applications, including creating narra-\ntives for patients and sharing educational messages\nin healthcare (Massey et al., 2021), targeting users\nin marketing (van Pinxteren et al., 2020; Fuglerud\net al., 2020), and communicating with workers in\nmanagement (Claus, 2019). Conversational agents\nuse personas to generate more interesting and en-\ngaging conversations with their users (Zhou et al.,\n2019; Shum et al., 2019).\nCreating persona-based datasets is difficult: the\nprocess is labor-intensive, the outputs must be up-\ndated to reflect current events and new concepts,\nand there are often quality concerns.\nExisting\npersona-based datasets have resulted from labor-\nintensive data collection processes (Zhang et al.,\n2018; Zhong et al., 2020) involving humans to cre-\nate or validate personas, create fictional persona-\nbased conversations, and ensure the conversations\nare coherent. Moreover, even after these datasets\nare created, it is difficult to update them with the lat-\nest topics (Lee et al., 2022), such as current events,\nnew concepts, products, or social trends (Lazari-\ndou et al., 2021). Finally, existing persona-based\narXiv:2312.10007v1  [cs.CL]  15 Dec 2023\ndatasets do not guarantee faithfulness, a criterion\nwe introduce to describe the alignment between\nparticipants\u2019 utterances and their personas.\nIn this paper, we introduce a new framework for\ngenerating large, customized persona-based con-\nversational datasets that uses unsupervised LLMs\nto reduce human labor, introduces methods to gen-\nerate, expand, and update personas automatically,\nand enforces a set of quality criteria including faith-\nfulness to ensure dialogues are human-like. Our\npersona-based conversational dataset generation\nframework consists of a three-level pipeline:\n1. User Generation\n2. User Pairing\n3. Conversation Generation\nThe user generation step takes a set of seed per-\nsonas, and augments it to create plausible user\nprofiles. The user pairing step matches users to\nparticipate in conversations. The conversation gen-\neration produces plausible conversations between\nthe selected user pairs. The conversation generation\ncomponent uses a method similar to self-feedback\n(Madaan et al., 2023) to iteratively improve the\nquality of generated samples.\nWe used the proposed framework to create\nSynthetic-Persona-Chat (SPC), a conversational\ndataset with 5k user personas, and 20k faithful\ndialogues. The framework we defined to create\nthis dataset can be reused to define specialized per-\nsonas, such as user music profiles, etc. to create\napplication-specific datasets.\nOur contributions are:\n\u2022 We propose an unsupervised approach to gen-\nerate, and extend specialized personas using\nLLMs.\n\u2022 We introduce and evaluate a framework based\non LLMs to evolve a dataset while imposing\ndifferent objectives on it.\n\u2022 We release Synthetic-Persona-Chat, a high-\nquality, faithful, persona-based conversational\ndataset useful for several conversational tasks,\nsuch as training persona inference models.\n2\nDefinitions\nWe define the faithful persona-based dialogue gen-\neration task. We begin by defining the persona-\nbased dialogue generation task. We then formally\ndefine the faithfulness criteria as a desired qual-\nity for the generated dialogues. Throughout this\nsection, we use \u03c0 to refer to persona attributes (in-\ndividual sentences which, together, form the user\npersona), U to refer to user profiles, and D to refer\nto conversations (dialogues).\nPersona Attributes We define a user persona\nattribute as a sentence describing this user. \"I like\nice cream\", \"I have two brothers\" and \"My native\nlanguage is Tamazight\" are all examples of persona\nattributes. Let \u2126 be the universal set of persona\nattributes. \u2126 contains all natural language descrip-\ntions of all tangible features of any person, which\nis unbounded.\nPersona Categories To help organize the vast\nspace of personas, we adopt the approach of Lee\net al. (2022) who introduced persona categories.\nPersona categories are groups of persona attributes\nthat describe the same semantic feature of the user.\nIn our work, we associate each persona category\nwith a corresponding query that can be answered\nwith all persona attributes in that category. For\nexample, job and family situation are persona cate-\ngories, and corresponding queries might be \u201cWhat\nis your occupation?\u201d, and \u201cDo you have a family?\u201d.\nPersona Attribute Structure Persona attributes\ncan overlap. For instance, the attribute \"I intro-\nduced my kids to scuba diving at a young age\"\noverlaps with the attribute \"My eldest son goes to\nelementary school\", since both include the \"parent-\nhood\" feature of the user. Moreover, some persona\nattributes form a hierarchy, and some persona at-\ntributes are specific cases of other attributes.\nUser Profile We define a user profile as a set\nof persona attributes that can be used to describe\na user. For a realistic user, the persona attributes\ndescribing a user profile should not contradict each\nother, and be consistent. An arbitrary persona at-\ntribute set U \u2282 \u2126 is a consistent set of persona\nattribute if, and only if:\n\u2200\u03c01 \u2208 U, \u2204\u03a02 \u2282 U : (\u03a02 \u0338= \u2205) \u2227 (\u03a02 \u2192 \u00ac\u03c01)\nPersona-based Conversation A persona-based\nconversation D contains utterances such that at\nleast one persona attribute from each user profile\ncan be inferred from it. For example, the persona\nattribute \"I am a parent\" can be inferred from the\nutterance \"I just dropped off my son at school\". A\npersona-based conversation model is a generative\nmodel that takes a pair of user profiles (U1, U2)\nas input, and returns a persona-based dialogue D\nbetween these two users.\nFigure 1: Unfaithful Conversation (Left): Loving steak is\nnegatively correlated with the persona attribute \"I am a vege-\ntarian\". Faithful Conversation (Right): It introduces no infor-\nmation that contradicts or weakens the user\u2019s profile.\nFaithfulness One crucial quality for a persona-\nbased conversation is that it should align with the\nuser profile. Inspired by (Daheim et al., 2023)\nwhich introduces dialogue system faithfulness to\nthe knowledge contained in relevant documents,\nwe specify the criterion of faithfulness to character-\nize the alignment between the utterances of a user\nin a persona-based conversation and their profile.\nThe faithfulness criterion enforces the constraint\nthat the utterances of a user should not decrease the\nlikelihood of their persona. This criterion assumes\nthe existence of both a prior probability of persona\nattributes, and an inference model for determining\nthe probability of persona attributes conditioned on\nutterances. Let M be such an inference model, (U1,\nU2) a pair of user profiles, and D a persona-based\nconversation between them. To be a faithful con-\nversation based on M, D should not contain any\ncontradicting evidence to the persona attributes of\nthe speakers: passing the conversation D as input\nto the inference model M should not reduce the in-\nference probability of persona attributes in either of\nthe user profiles U1 or U2. In other words, the prob-\nability of any persona attribute in the user profiles\nbased on conversation D should not be less than\nthe probability of that persona attribute without any\nassumptions. Formally, we call a conversation D\nfaithful with respect to the user profiles U1 and\nU2, and inference model M if the following condi-\ntion holds: \u2200\u03c0 \u2208 U1 \u222a U2 : PM(\u03c0|D) \u2265 PM(\u03c0).\nWhere PM(\u03c0|D) indicates the probability that M\ninfers the persona \u03c0 given conversation D. We\nshow examples of faithful, and unfaithful conversa-\ntions in Figure 1.\n3\nMethod\nIn this section, we introduce our method to gener-\nate persona-based conversations. We create such\nconversations with minimum human input, start-\ning from an initial dataset. Our process consists\nof three steps, as shown in Figure 2: user gener-\nation, user pairing, and conversation generation.\nFigure 2: Dataset Augmentation Pipeline\nThe first component augments a set of seed per-\nsona attributes \u03a00 into an expanded set of persona\nattributes \u03a0e, from which it creates user profiles.\nThe second component pairs user profiles as in-\nterlocutors of a conversation. The third and final\ncomponent uses an iterative process to generate\nhigh-quality conversations among user profile pairs.\nWe detail each of these components below.\n3.1\nUser Generation\nThe User Generation component is split into two\nsub-components:\n1. Persona Expansion\n2. User Profile Construction\nWe bootstrap seed persona attributes by using vari-\nous prompts (Brown et al., 2020b) to generate new\npersona attributes in the Persona Expansion step\n(Refer to Appendix A.1 for more details on the\nprompts used). We then create new user profiles by\niteratively selecting random user persona attributes\nfrom the expanded persona attributes. We employ a\nNatural Language Inference (NLI) model to ensure\nthe consistency of the constructed user profiles.\n3.1.1\nPersona Expansion\nWe propose an unsupervised method to augment a\nset of seed persona attributes \u03a00 into a super-set\n\u03a0e. Unlike previous approaches (Lee et al., 2022),\nour method is independent of human knowledge or\nintervention, making it capable of creating special-\nized personas in new domains. We proceed in two\nsteps: query induction, and persona bootstrapping.\nIn the query induction phase, we identify persona\ncategories in \u03a00, along with associated queries. We\nthen expand these queries into a set Q that also cov-\ners unobserved persona categories. The persona\nbootstrapping step leverages the category-based\nquery set Q, and the initial persona attribute seed\nset \u03a00 to generate new persona attributes. Both\nof these steps are based on the bootstrapping tech-\nnique (Yarowsky, 1995), and involve prompting an\nLLM. We provide a detailed description of these\ntwo steps in the following.\nQuery Induction As described in Section 2,\neach persona attribute belongs to at least one per-\nsona category, and each category is associated with\na corresponding query that can be answered with\npersona attributes in that category. The query in-\nduction process initially identifies the queries as-\nsociated with persona categories in \u03a00. It then\nbootstraps queries by feeding them to a prompted\nLLM to create more queries that are associated\nwith unobserved categories, ultimately creating a\nquery set Q. Including queries associated with un-\nobserved persona categories facilitates the creation\nof a more diverse set of personas, and increases the\nscale of augmentation.\nThe query induction relies on the following as-\nsumption:\nAssumption Let M be an LLM, and let \u0393 be\nthe set of all queries associated with all persona\ncategories. If two persona attributes \u03c01 and \u03c02\nbelong to the same persona category, then there\nexists a query qM \u2208 \u0393 such that \u03c01 and \u03c02 are\nM\u2019s output to qM.\nThe persona attributes \"I am a doctor\" and \"I\nam a truck driver\", for instance, both belong to\nthe \"job\" category, leading to the query \"What is\nyour job?\". We use an agglomerative clustering\nmethod to identify the persona categories in \u03a00.\nLet C be an arbitrary persona cluster in \u03a00. To\ngenerate a query for C, we select a random sub-\nset of persona attributes in C, and create a prompt\nusing these samples. We employ this strategy to\ngenerate queries for all the clusters identified in\n\u03a00, and create a set of queries, which we refer\nto as Q0. Details on the clustering, query induc-\ntion, together with examples of clusters, persona\nattributes, and induced queries are available in Ap-\npendix A.1. We come up with queries for new,\nunobserved persona categories by bootstrapping\nthe queries in Q0: starting from Q = Q0, we iter-\natively sample a set of queries from Q, and create\na prompt by concatenating them. We then prompt\nthe LLM to generate a new query, and add it to the\nquery set Q, as shown in Figure 3. We generated\na total of |Q| = 188 queries. This set of category-\nspecific queries Q is later used to guide the LLM\nto generate new persona attributes from the spec-\nified category. Thus, higher values of |Q| result\nin greater diversity within the expanded persona\nattribute set.\nPersona Bootstrapping We use the persona at-\ntribute seed set \u03a00 and category-specific queries\nFigure 3: Query Induction Steps\nFigure 4: Query-based Persona Bootstrapping Process\nQ to generate new persona attributes through a\nbootstrapping process. We initialize \u03a0 to \u03a00. At\nevery iteration, we randomly select a subset of per-\nsona attributes from \u03a0, and create a set of prompts\nas follows: we first concatenate a set of persona\nattributes s. For every query q \u2208 Q, we then com-\nbine the concatenated samples s, and the query q\nto create a category-specific persona prompt. This\nprompt guides the LLM to generate a persona at-\ntribute for that persona category. The set of prompts\nobtained from this process is {sq|q \u2208 Q}. We only\nadd a new persona attribute to the set if its BERT\nembeddings (Devlin et al., 2019) are not too close\nfrom existing ones, so as to prevent the addition of\nduplicates.\nEach of these prompts is then fed to the LLM\nto create a new persona attribute, which is subse-\nquently added to the set of persona attributes \u03a0 for\nthe next iteration. We continue this iterative pro-\ncess until we have generated a total of 5k persona\nattributes. Figure 4 illustrates the persona boot-\nstrapping process. Table 6 in the appendix contains\nthe prompt template used in this component.\n3.1.2\nUser Profile Construction\nWe build user profiles incrementally by sampling\npersona attributes from \u03a0e, and adding the eligible\nones. A persona attribute is eligible if it adheres to\nthe criteria of consistency and non-redundancy. In\nother words, it should not contradict any attribute\nalready in the user profile, and it should not be in-\nferred by other persona attribute. We assess the\nconsistency and redundancy of user profiles by\nleveraging an NLI model, and persona attribute\nclustering, respectively. The NLI model we em-\nploy is based on T5 (Raffel et al., 2019), and has\nbeen trained on the TRUE dataset (Honovich et al.,\n2022).\nWe create a user profile U by iteratively select-\ning a random candidate persona attribute \u03c0\u2032 \u2208 \u03a0e.\nWe use the NLI model to assess whether \u03c0\u2032 con-\ntradicts any persona attribute in the profile. This\nis determined by the condition: \u2200\u03c0 \u2208 U : (\u03c0\u2032 \u0338\u2192\n\u00ac\u03c0) \u2227 (\u03c0 \u0338\u2192 \u00ac\u03c0\u2032), where \u2192 is an inference. Ad-\nditionally, we evaluate the similarity of \u03c0\u2032 to the\npersona attributes in U to prevent the addition of\nredundant attributes. We add \u03c0\u2032 to U if it meets\nthe consistency and non-redundancy criteria. We\nrepeat this process until the user profile contains\n5 persona attributes. Please refer to Appendix A.1\nfor more details on the user profile construction.\n3.2\nUser Pairing\nIn this component, we identify potential pairs of\nusers for conversations. As the conversations are\npersona-based, we hypothesize that they will be\nmore engaging if the users\u2019 personas exhibit more\ncommonalities. We assign a similarity score to ev-\nery pair of user profiles (U1, U2), indicating their\nsemantic similarity. We leverage BERT to repre-\nsent the user profiles. The similarity between U1\nand U2 is defined as: |{(\u03c01, \u03c02)|\u03c01 \u2208 U1, \u03c02 \u2208\nU2, \u2203c : \u03c01, \u03c02 \u2208 c}| Where c is a persona at-\ntributes cluster. The semantic similarity is quanti-\nfied by the number of common persona categories\nin the user profiles. We pair U1 and U2 if their\nsimilarity exceeds a threshold of 2.\n3.3\nConversation Generation\nOur Conversation Generation component is similar\nto a general-purpose dataset generation framework\nthat generates data samples, and refines them based\non a set of predefined criteria, which we refer to\nas policies (Madaan et al., 2023). The flexibility in\nthe choice of policies for data generation allows us\nto emphasize different objectives. Once the active\npolicies are selected, this component generates new\ndata samples using a few input samples. The input\nto our Conversation Generation framework con-\nsists of a set of paired user profiles, a few samples\nof user profiles along with a persona-based con-\nversation between them, and conversation quality\nmetrics as policies. We follow a Generator-Critic\narchitecture, and iteratively create the dataset fol-\nFigure 5: The Generator-Critic Architecture for Conversation\nGeneration\nlowing the steps shown in Figure 5:\nStep 1 The Generator outputs candidate conver-\nsations between persona pairs using a few initial\nconversation samples.\nStep 2 The Critic evaluates the candidate conver-\nsations based on the predetermined policies, and\nselects the best candidate conversations.\nStep 3 The best candidate conversations are added\nto the dataset for the next iteration of generation.\nThis iterative process of selecting the top candi-\ndates and adding them to the dataset gradually im-\nproves the performance of the Generator.\nWithout any loss of generality, we implement\nboth the Generator and the Critic based on LLMs.\nSpecifically, the Generator prompts an LLM to\ncreate candidate conversations, while the Critic\nprompts an LLM to evaluate the quality of the gen-\nerated conversations.\nWe provide more details on the Generator, Critic,\nand the policies we used.\nThe Generator outputs conversations for pairs\nof users (U1, U2) by prompting an LLM (Brown\net al., 2020b; Wei et al., 2023). At each iteration,\nit randomly selects 5 samples from an initial set\nof conversations, each containing a pair of user\nprofiles and a dialogue among them. It feeds these\nsamples to a template that instructs the LLM to\ngenerate a series of candidate conversations for\nthe given user pair. The template, and a sample\ngenerated conversation are available in Table 6,\nand Table 8 in the appendix.\nThe Critic selects the best generated conversa-\ntions to fine-tune the Generator. A conversation is\ndeemed high-quality if it complies with the poli-\ncies of the Critic. Given the multifaceted nature\nof the conversation evaluations, we use a Mixture\nof Experts (MoE) approach. Each expert evalu-\nates the conversation based on a specific policy. In\nthis paper, we incorporate three types of experts,\neach with distinct criteria: general conversation\nquality, persona faithfulness, and toxicity. Col-\nlectively, these experts select the best generated\nconversations (the single best in our experiments).\nWe describe each type of expert, and the collective\ndecision-making process below.\nGeneral Conversation Quality experts assess\nconversation quality using the Fine-grained Eval-\nuation of Dialog (FED) metrics introduced in\n(Mehri and Esk\u00e9nazi, 2020). These experts use ver-\nbalized forms of the policies from FED as prompts.\nFor instance, the \"conversation depth quality ex-\npert\" transforms the \"depth policy\" from FED into\na prompt like \"Which conversation is a deeper con-\nversation between user 1 and user 2?\". Our system\ninstructs the LLM to compare each pair of candi-\ndate conversations based on these policies, result-\ning in pairwise comparisons. The list of policies\nand their baseline performance are presented in\nTable 5 in Appendix A.2.\nThe Faithfulness expert ensures the consistency\nof the generated conversations with the user pro-\nfiles. It uses an LLM to identify instances of un-\nfaithful conversations. The faithfulness prompt\nprovides the LLM with explicit instructions, user\nprofiles, and human-curated examples of unfaithful\nconversations.\nThe Toxicity expert detects any conversation that\nexhibits harmful traits, including bias and hate.\nThe Critic filters unfaithful and toxic conversa-\ntions out. It then selects the best conversations\nusing a majority vote among the General Conver-\nsation Quality experts. The selected instances are\nadded to the dataset for the next iteration of the\nGenerator.\n4\nEvaluation\nWe evaluate different aspects of our dataset genera-\ntion framework, and the resulting dataset - referred\nto as Synthetic-Persona-Chat - which is created\nusing an instruction fine-tuned LLM with 24 bil-\nlion parameters (Chung et al., 2022). We compare\nSynthetic-Persona-Chat (SPC) against the widely\nused Persona-Chat (PC) dataset across different di-\nmensions. We begin by evaluating the quality of\nthe personas we generate. We then evaluate SPC\nusing both automatic metrics, and human assess-\nment. We analyze other aspects of SPC, such as\ntoxicity and diversity in appendices B.1 and B.1.\n4.1\nEvaluation of the Expanded Personas\nWe evaluate our persona expansion module on two\nseed datasets: Wikipedia, and Persona-Chat. The\nWikipedia personas are created by crawling the\nDataset\nPersona-Chat\nSynthetic-Persona-Chat\nWikipedia\nWikipedia+\n# Persona Attributes\n4,723\n10,371\n8768\n18,293\n# Clusters\n323\n553\n408\n986\nInter-cluster Dist\n0.836\n0.863\n0.816\n0.85\nAVG length\n7.65\n15.9\u2217\n10.45\n15.2\u2217\nTable 1: Evaluation of the expanded persona sets. The num-\nbers with \u2217 indicate the metric value of the newly generated\npersona attributes to contrast with the initial set.\n1,000 most active contributors2, and extracting user\nboxes from their pages. We expand both datasets\nusing our framework, and evaluate the expanded\npersona attribute sets using automatic metrics. Ta-\nble 1 compares the original persona sets to the\nexpanded ones on a few dimensions. We observe\nthat our persona expansion increases the number of\npersona attributes in SPC by 119%, while maintain-\ning the original persona categories and expanding\nthem by 71% compared to the persona attributes\nin PC. Moreover, the lengths of the new generated\npersona attributes are 107% longer in SPC, indi-\ncating that the new personas exhibit greater detail\nand specificity. We observe a similar trend when\napplying our persona expansion to the Wikipedia\npersona set, with a 108% increase in the number\nof persona attributes, a 140% increase in persona\ncategories, and a 45% growth in persona attribute\nlengths. This demonstrates the effectiveness of our\nmethod in expanding and diversifying persona sets.\n4.2\nNext Utterance Prediction\nA persona-based conversation reflects the speaker\u2019s\npersona explicitly or implicitly. Therefore, we ex-\npect the inclusion of information about speaker\npersonas to enhance the performance of next ut-\nterance prediction models in such conversations.\nIn this experiment, we assess the impact of incor-\nporating speaker personas as prior information on\nboth ranking, and generative - Transformer based\n(Vaswani et al., 2017) - next utterance prediction\nmodels. We create a subset of SPC containing\nconversations among user pairs included in PC for\na fair comparison.\nWe observe (Table 2) that\nthe performance of ranking models increases when\npersonas are given to the models as input for both\ndatasets. Specifically, the Transformer (Ranker)\nmodel, known for its ability to capture conversa-\ntional complexity, exhibits higher performance in\nSPC when evaluated on the SPC test set compared\nto the PC test set. However, it demonstrates rela-\ntively weaker performance when trained on the PC.\n2https://en.wikipedia.org/wiki/Wikipedia:\nList_of_Wikipedians_by_number_of_edits\nPersona-Chat\nSynthetic-Persona-Chat\nMethod\nMetric\nNone\nPersona\n% Change\nNone\nPersona\n% Change\nIR Baseline\nhit@1\n18.69\n36.86\n+97\n19.37 (19.92)\n39.6 (26.23)\n+104 (+31)\nTransformer (Ranker)\nhit@1\n14.24\n19.21\n+35\n9.71 (64.24)\n11.74 (68.82)\n+21 (+7)\nTransformer (Generator)\nhit@1\n8.54\n6.78\n-20\n6.89 (41.32)\n6.66 (37.35)\n-3 (-9)\nPerplexity\n122.5\n173.3\n+41\n1032 (5.24)\n1126 (5.73)\n+9 (+9)\nBLUE\n0.120\n0.094\n-21\n0.097 (0.289)\n0.083 (0.251)\n-14 (-13)\nROUGE\n0.141\n0.113\n-24\n0.123 (0.348)\n0.107 (0.309)\n-13 (-11)\nTable 2: Results of the next utterance prediction experiment. Performance of the trained model on the test split of Persona-Chat\nis represented by the numbers in the table, while the numbers in parentheses indicate results for the test split of Synthetic-\nPersona-Chat.\nThis implies that SPC contains more intricate and\ncoherent conversations.\nThe Transformer (Ranker) trained on SPC\nachieves a hit@1 of 64.24 on SPC test, 350%\nhigher than PC (14.24). This suggests that the\nTransformer model can more accurately predict\nthe next utterance in SPC, pointing to a greater\ncoherency in conversations.\nThe performance of the Information Retrieval\n(IR) Baseline model is slightly higher for SPC: it\nrises by 31% when conditioned on user personas,\nwhich is lower than 97% improvement in PC. A key\ncontributing factor for the performance improve-\nment of the retrieval-based model (IR Baseline)\non PC given the personas, is the participants\u2019 ten-\ndency to copy persona words in the conversations,\nwhereas in SPC the personas are more implicitly re-\nflected in the conversations. The implicit reflection\nof personas in SPC, makes the task more challeng-\ning for word based retrieval models, necessitating\nreasoning that goes beyond word level. However,\nwhen the model is trained on SPC and tested on\nPC, the improvement is as high as when the model\nis trained on PC, i.e. 104% compared to 97%.\nThe performance of generative models is low for\nthis task since these models are not trained with\nthe ranking objective. However, the performance\ndifference while the models are conditioned on per-\nsonas is lower for the model trained on SPC, with a\n20% drop for the model trained on PC against 3%\ndrop in the model trained on SPC. The increase in\nperplexity is 9% in SPC compared to 41% in PC.\nThe lower rate of perplexity increase and perfor-\nmance drop of the model given user personas as\ninput highlights the higher alignment of conversa-\ntions with personas in SPC.\nWe also evaluate the performance of the next ut-\nterance prediction models when given no user, one\nuser, and both user personas. The results suggest a\nhigher degree of bidirectionality in SPC. We refer\nthe reader to the Appendix B.1 for more details.\n4.3\nHuman Evaluation\nWe compare the quality of the conversations gen-\nerated by our framework against those in Persona-\nChat. We randomly select 200 conversations from\nPC, together with their corresponding user pairs,\nand use our method to generate conversations\namong the same users.\nWe start by following\n(Gehrmann et al., 2019) in running a human ex-\nperiment to try and detect AI-generated content.\nWe conduct a Turing test where we present pairs of\nconversations to humans, and ask them to identify\nthe synthetically generated one. This test is carried\nout on the generated conversations at the end of\neach iteration of creating SPC. We repeat the test\nfor conversations generated for new persona pairs,\nwhich we refer to as iteration 3\u2217, i.e. we pair each\nof these conversations with a random conversation\nfrom PC. For a robust evaluation, every pair of\nconversations is annotated by 3 human evaluators,\nand the majority vote is used as the final annota-\ntion. Details of this test are available in Appendix\nB.2. The results of this experiment can be found\nin Table 3. We observe that the losing rate of SPC\nis reduced by 48% from SPC Iter 1 to SPC Iter 3,\nand dropped below the rate of 10%. Interestingly,\n91% of the conversations in SPC, which are syn-\nthetically generated, are judged as human-like as\nthe conversations generated by humans. Moreover,\nconversations generated for new personas (Iteration\n3\u2217) are deemed artificial in only 8.04% of cases,\nshowing that SPC is more realistic than PC.\nWe also evaluate the faithfulness of the gener-\nated conversations. For each conversation, we pro-\nvide annotators with a faithfulness annotation task\nincluding the speakers\u2019 persona attributes and dis-\ntractor persona attribute options as shown in Figure\n8. We evaluate faithfulness during 3 iterations of\nconversation generation for the selected 200 user\npairs, and the annotators evaluate the generated\nConversation Source\nLose\nWin\nTie\nFaithful\nSPC Iter 1\n17.2\n30.1\n52.68\n78.5\nSPC Iter 2\n18.5\n49\n32.5\n80.5\nSPC Iter 3\n8.8\n35.23\n55.95\n76.6\nSPC Iter 3*\n8.04\n32.66\n59.29\nN/A\nSPC (LLM2)\n11.5\n39\n49.5\nN/A\nTable 3: Turing Test on 200 Generated Conversations per\nIteration: Synthetic-Persona-Chat Outcomes Against Persona-\nChat.\nconversations for each pair in every iteration. The\nresults show that, while improving the Turing test\nresults, faithfulness of conversations are consis-\ntently higher than 75% with at most 3% variation\nin between iterations, indicating high faithfulness\nin all iterations.\nFinally, we assess the impact of LLM size on\nthe quality of the generated dataset within our\nframework. We create a variant of SPC using an\nLLM with 540 billion parameters (LLM2). Table 3\npresents human evaluations comparing the smaller\nLLM in multiple iterations to a single-iteration ap-\nproach with LLM2. The larger model exhibits a 5%\nadvantage in the Turing test over the first iteration\nof dataset generation over the smaller model. Af-\nter two iterations, however, the multi-iteration ap-\nproach outperforms the first iteration of the bigger\nmodel, showing our framework\u2019s capacity for cost-\neffective, high-quality conversation generation.\n5\nRelated Work\nLarge Language Models (LLMs) have been used\nfor data augmentation (Shin et al., 2021), gener-\nation (Kim et al., 2023; Dong et al., 2023), and\nevaluation (Zhang et al., 2019; Liu et al., 2023).\nOne of the earliest works in this area (Anaby-Tavor\net al., 2019) used LLMs to create a large text dataset\nfrom a small, labeled one. This idea was followed\nby (Wang et al., 2021; Schick and Sch\u00fctze, 2021)\nwhich leveraged LLMs to create datasets without\nany human data. (Kumar et al., 2020) evaluated\nthe performance of different LLMs on the data\naugmentation task. Several conversational dataset\ngeneration methods focused on the structure of the\nconversational data (Dai et al., 2022; Leszczynski\net al., 2023; Abbasiantaeb et al., 2023). (Mehri\net al., 2022) illustrated how Large Language Mod-\nels (LLMs) can effectively generate synthetic train-\ning data for task-oriented dialogue models.\nPersona-based conversations have been a popu-\nlar research topic in NLP (Liu et al., 2022). One of\nthe earliest works in this area is Persona-Chat, by\n(Zhang et al., 2018), which proposed the Persona-\nChat dataset and evaluation metrics that have be-\ncome a benchmark for persona-based conversation\ngeneration (Mazar\u00e9 et al., 2018). Many subsequent\nworks have used this dataset to train and evaluate\ntheir models, including DialoGPT (Zhang et al.,\n2020), BlenderBot (Shuster et al., 2022), and Per-\nsonaChatGen (Lee et al., 2022). PersonaChatGen\nautomated the process of creating persona based\nconversations of Persona-Chat using LLMs. A\nchallenge in generating synthetic datasets is to\nensure the quality of the conversation including\ndata faithfulness, fidelity, diversity, and consis-\ntency (Li et al., 2016; Lee et al., 2023; Veselovsky\net al., 2023; Zhuo et al., 2023; Wang et al., 2023a;\nM\u00fcndler et al., 2023). Several works have focused\non creating and using high quality training datasets\n(Welleck et al., 2019), and creating quality filtering\ncomponents to their conversation dataset genera-\ntion (Lewkowycz et al., 2022). Evaluation of the\nresulting conversational datasets is also challeng-\ning (Xu et al., 2021). (Wang et al., 2023b) recently\nintroduced the paradigm of interactive evaluation\nof conversations with LLMs.\n6\nConclusion and Future Work\nWe developed a novel framework for generat-\ning high-quality persona-based conversations us-\ning LLMs, resulting in the creation of Synthetic-\nPersona-Chat, comprising 20k conversations. We\nhope this dataset will support future endeavors in\ndeveloping persona-aware conversational agents,\nincluding the generation of domain-specific multi-\nsession conversations for specialized, task-oriented\ninteractions. While we focused on a persona-based\ndataset generation task, our Generator-Critic ap-\nproach can be generalized to other use cases, such\nas generating other specialized datasets, etc.\nLimitations\nIn this paper, we define an iterative process over\nLLMs to generate a dataset. Our method requires\ncomputational resources, and access to an LLM.\nThe quality of the dataset is bounded by the LLM,\nsince the quality critics are also using the same\nLLM, and we leave the iterative improvement of\nour critics as future work. The main limitation of\nthis data generation framework is the inability to\ngenerate realistic conversations that do not have\nhigh quality, since we assume that both parties are\nfluent, that the conversation flow is perfectly con-\nsistent, and there is no unexpected event (e.g. an\ninterruption by another person, connection loss,\netc.) in the middle of the conversation. Another\nlimitation of our method is the difficulty of incorpo-\nrating less tangible persona traits, such as a sense\nof humor, or user attributes that require multiple\nconversation sessions to be reflected.\nEthics Statement\nThe approach of generating datasets based on some\ndesired objective might be used to create harm-\nful datasets, and train malicious models based on\nthem, such as a biased dataset, or a hateful speech\none (Hartvigsen et al., 2022). On the other hand,\nthese datasets and models can be used as filters in\napplication tasks.\nWe used Amazon Mechanical Turk in our human\nexperiments, and followed that platform\u2019s guide-\nlines to protect the rights of human raters. The\nparticipation was voluntary, and the raters were\ninformed of their rights at the beginning of the\nstudy. The platform implemented security mea-\nsures to protect them, and prevent the disclosure of\nany Personal Identifiable Information about them.\nFurthermore, we offered higher than minimum stan-\ndard wage compensation to avoid any exploitative\npractices.\nTo avoid having any toxic conversation in the\nfinal dataset, we also used several tools to remove\nany potentially toxic conversation. Details about\nthese tools, and example removed samples are avail-\nable in Appendix B.1.\nAcknowledgements\nThe authors would like to thank Kian Ahrabian,\nEric Boxer, Luke Friedman, I\u00f1aki Iturrate, Kathy\nMeir-Hellstern, Filip Radlinski, and Kexuan Sun\nfor their valuable comments on this manuscript.\nReferences\nZahra Abbasiantaeb, Yifei Yuan, E. Kanoulas, and Mo-\nhammad Aliannejadi. 2023. Let the llms talk: Sim-\nulating human-to-human conversational qa via zero-\nshot llm-to-llm interactions.\nAteret Anaby-Tavor, Boaz Carmeli, Esther Goldbraich,\nAmir Kantor, George Kour, Segev Shlomov, N. Tep-\nper, and Naama Zwerdling. 2019. Not enough data?\ndeep learning to the rescue! ArXiv, abs/1911.03118.\nParikshit Bansal and Amit Sharma. 2023. Large lan-\nguage models as annotators: Enhancing general-\nization of nlp models at minimal cost.\nArXiv,\nabs/2306.15766.\nD. M. Blei, T. L. Griffiths, M. I. Jordan, and J. B.\nTenenbaum. 2004. Hierarchical topic models and\nthe nested Chinese restaurant process. In Advances\nin Neural Information Processing Systems 16. MIT\nPress, Cambridge, MA.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, T. J. Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler,\nMateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020b. Language models are few-shot learners.\nCheng-Han Chiang and Hung yi Lee. 2023. Can large\nlanguage models be an alternative to human evalu-\nations?\nIn Annual Meeting of the Association for\nComputational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nLisbeth Claus. 2019. Hr disruption\u2014time already to\nreinvent talent management. BRQ Business Research\nQuarterly, 22.\nAlan Cooper and Paul Saffo. 1999. The Inmates Are\nRunning the Asylum. Macmillan Publishing Co., Inc.,\nUSA.\nNico Daheim, Nouha Dziri, Mrinmaya Sachan, Iryna\nGurevych, and Edoardo M. Ponti. 2023.\nElastic\nweight removal for faithful and abstractive dialogue\ngeneration.\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Zhao,\nAida Amini, Qazi Mamunur Rashid, Mike Green,\nand Kelvin Guu. 2022. Dialog inpainting: Turning\ndocuments into dialogs. ArXiv, abs/2205.09073.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Rui Pan,\nShizhe Diao, Jipeng Zhang, Kashun Shum, and\nT. Zhang. 2023. Raft: Reward ranked finetuning\nfor generative foundation model alignment. ArXiv,\nabs/2304.06767.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. ArXiv,\nabs/2302.04166.\nKristin Fuglerud, Trenton Schulz, Astri Janson, and\nAnne Moen. 2020.\nCo-creating Persona Scenar-\nios with Diverse Users Enriching Inclusive Design,\npages 48\u201359.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M. Rush. 2019. Gltr: Statistical detection and\nvisualization of generated text. In Annual Meeting of\nthe Association for Computational Linguistics.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxigen: A large-scale machine-generated dataset for\nadversarial and implicit hate speech detection. ArXiv,\nabs/2203.09509.\nXingwei He, Zheng-Wen Lin, Yeyun Gong, Alex Jin,\nHang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan\nDuan, and Weizhu Chen. 2023. Annollm: Making\nlarge language models to be better crowdsourced\nannotators. ArXiv, abs/2303.16854.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3905\u20133920, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020.\nPoly-encoders: Trans-\nformer architectures and pre-training strategies for\nfast and accurate multi-sentence scoring.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Peter West,\nXiming Lu, Youngjae Yu, Pei Zhou, Ronan Le Bras,\nMalihe Alikhani, Gunhee Kim, Maarten Sap, and\nYejin Choi. 2023. Soda: Million-scale dialogue dis-\ntillation with social commonsense contextualization.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. ArXiv, abs/2003.02245.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d\u2019Autume,\nTom\u00e1s Kocisk\u00fd, Sebastian Ruder, Dani Yogatama,\nKris Cao, Susannah Young, and Phil Blunsom. 2021.\nMind the gap: Assessing temporal generalization\nin neural language models. In Neural Information\nProcessing Systems.\nDong-Ho Lee, Jay Pujara, Mohit Sewak, Ryen W White,\nand Sujay Kumar Jauhar. 2023. Making large lan-\nguage models better data creators. In The 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nYoung-Jun Lee, Chae-Gyun Lim, Yunsu Choi, Ji-Hui\nLm, and Ho-Jin Choi. 2022. PERSONACHATGEN:\nGenerating personalized dialogues using GPT-3. In\nProceedings of the 1st Workshop on Customized Chat\nGrounding Persona and Knowledge, pages 29\u201348,\nGyeongju, Republic of Korea. Association for Com-\nputational Linguistics.\nMegan Leszczynski, Ravi Ganti, Shu Zhang, Krisz-\ntian Balog, Filip Radlinski, Fernando Pereira, and\nArun Tejasvi Chaganty. 2023. Generating synthetic\ndata for conversational music recommendation us-\ning random walks and language models.\nArXiv,\nabs/2301.11489.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy\nGur-Ari, and Vedant Misra. 2022. Solving quantita-\ntive reasoning problems with language models.\nJiwei Li, Michel Galley, Chris Brockett, Georgios P.\nSpithourakis, Jianfeng Gao, and William B. Dolan.\n2016. A persona-based neural conversation model.\nArXiv, abs/1603.06155.\nYen-Ting Lin and Yun-Nung (Vivian) Chen. 2023. Llm-\neval: Unified multi-dimensional automatic evaluation\nfor open-domain conversations with large language\nmodels. ArXiv, abs/2305.13711.\nJunfeng Liu, Christopher T. Symons, and Ranga Raju\nVatsavai. 2022.\nPersona-based conversational ai:\nState of the art and challenges. 2022 IEEE Inter-\nnational Conference on Data Mining Workshops\n(ICDMW), pages 993\u20131001.\nYang Liu, Dan Iter, Yichong Xu, Shuo Wang, Ruochen\nXu, and Chenguang Zhu. 2023. G-eval: Nlg evalua-\ntion using gpt-4 with better human alignment. ArXiv,\nabs/2303.16634.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nPhilip M Massey, Shawn C Chiang, Meredith Rose,\nRegan M Murray, Madeline Rockett, Elikem Togo,\nAnn C Klassen, Jennifer A Manganello, and Amy E\nLeader. 2021. Development of personas to communi-\ncate narrative-based information about the hpv vac-\ncine on twitter. front digit health.\nPierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Rai-\nson, and Antoine Bordes. 2018. Training millions of\npersonalized dialogue agents. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2775\u20132779, Brussels,\nBelgium. Association for Computational Linguistics.\nShikib Mehri, Yasemin Altun, and Maxine Eskenazi.\n2022. LAD: Language models as data for zero-shot\ndialog. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 595\u2013604, Edinburgh, UK. Association\nfor Computational Linguistics.\nShikib Mehri and Maxine Esk\u00e9nazi. 2020. Unsuper-\nvised evaluation of interactive dialog with dialogpt.\nIn SIGDIAL Conferences.\nA. H. Miller, W. Feng, A. Fisch, J. Lu, D. Batra, A. Bor-\ndes, D. Parikh, and J. Weston. 2017.\nParlai: A\ndialog research software platform. arXiv preprint\narXiv:1705.06476.\nNiels M\u00fcndler, Jingxuan He, Slobodan Jenko, and Mar-\ntin T. Vechev. 2023. Self-contradictory hallucinations\nof large language models: Evaluation, detection and\nmitigation. ArXiv, abs/2305.15852.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke E. Miller, Maddie Simens, Amanda Askell, Pe-\nter Welinder, Paul Francis Christiano, Jan Leike, and\nRyan J. Lowe. 2022. Training language models to\nfollow instructions with human feedback.\nArXiv,\nabs/2203.02155.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V. Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011.\nScikit-learn: Machine learning in\nPython.\nJournal of Machine Learning Research,\n12:2825\u20132830.\nJohn Pruitt and Jonathan Grudin. 2003a. Personas: Prac-\ntice and theory. In Proceedings of the 2003 Confer-\nence on Designing for User Experiences, DUX \u201903,\npage 1\u201315, New York, NY, USA. Association for\nComputing Machinery.\nJohn S. Pruitt and Jonathan T. Grudin. 2003b. Personas:\npractice and theory. In Conference on Designing for\nUser eXperiences.\nColin Raffel, Noam M. Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2019. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. ArXiv, abs/1910.10683.\nTimo Schick and Hinrich Sch\u00fctze. 2021. Generating\ndatasets with pretrained language models. ArXiv,\nabs/2104.07540.\nRichard Shin, Christopher Lin, Sam Thomson, Charles\nChen, Subhro Roy, Emmanouil Antonios Platanios,\nAdam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models\nyield few-shot semantic parsers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 7699\u20137715, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nMichael Shum, Stephan Zheng, Wojciech Kryscinski,\nCaiming Xiong, and Richard Socher. 2019. Sketch-\nfill-a-r: A persona-grounded chit-chat generation\nframework. ArXiv, abs/1910.13008.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, W.K.F. Ngan, Spencer Poff, Naman Goyal,\nArthur D. Szlam, Y-Lan Boureau, Melanie Kam-\nbadur, and Jason Weston. 2022. Blenderbot 3: a\ndeployed conversational agent that continually learns\nto responsibly engage. ArXiv, abs/2208.03188.\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.\nSequence to sequence learning with neural networks.\nArXiv, abs/1409.3215.\nMichelle van Pinxteren, Mark Pluymaekers, and Jos\nLemmink. 2020. Human-like communication in con-\nversational agents: a literature review and research\nagenda. Journal of Service Management, ahead-of-\nprint.\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nVeniamin Veselovsky, Manoel Horta Ribeiro, Akhil\nArora, Martin Josifoski, Ashton Anderson, and\nRobert West. 2023. Generating faithful synthetic\ndata with large language models: A case study in\ncomputational social science.\nBoxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie,\nMintong Kang, Chenhui Zhang, Chejian Xu, Zidi\nXiong, Ritik Dutta, Rylan Schaeffer, Sang Truong,\nSimran Arora, Mantas Mazeika, Dan Hendrycks, Zi-\nHan Lin, Yuk-Kit Cheng, Sanmi Koyejo, Dawn Xi-\naodong Song, and Bo Li. 2023a. Decodingtrust: A\ncomprehensive assessment of trustworthiness in gpt\nmodels. ArXiv, abs/2306.11698.\nXiaolei Wang, Xinyu Tang, Wayne Xin Zhao, Jingyuan\nWang, and Ji-Rong Wen. 2023b.\nRethinking the\nevaluation for conversational recommendation in the\nera of large language models.\nZirui Wang, Adams Wei Yu, Orhan Firat, and Yuan Cao.\n2021. Towards zero-label language learning. ArXiv,\nabs/2109.09193.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nSean Welleck, Jason Weston, Arthur Szlam, and\nKyunghyun Cho. 2019. Dialogue natural language\ninference.\nChien-Sheng Wu, Andrea Madotto, Zhaojiang Lin, Peng\nXu, and Pascale Fung. 2019. Getting to know you:\nUser attribute extraction from dialogues. In Inter-\nnational Conference on Language Resources and\nEvaluation.\nJing Xu, Arthur Szlam, and Jason Weston. 2021. Be-\nyond goldfish memory: Long-term open-domain con-\nversation.\nDavid Yarowsky. 1995. Unsupervised word sense dis-\nambiguation rivaling supervised methods. In 33rd\nannual meeting of the association for computational\nlinguistics, pages 189\u2013196.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur D.\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Annual Meeting of the Association for\nComputational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2019.\nBertscore:\nEvaluating text generation with bert.\nArXiv,\nabs/1904.09675.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration.\nPeixiang Zhong, Yao Sun, Yong Liu, Chen Zhang, Hao\nWang, Zaiqing Nie, and Chunyan Miao. 2020. En-\ndowing empathetic dialogue systems with personas.\nArXiv, abs/2004.12316.\nLi Zhou, Jianfeng Gao, Di Li, and Heung-Yeung Shum.\n2019. The design and implementation of xiaoice, an\nempathetic social chatbot.\nTerry Yue Zhuo, Yujin Huang, Chunyang Chen, and\nZhenchang Xing. 2023. Red teaming chatgpt via jail-\nbreaking: Bias, robustness, reliability and toxicity.\nA\nDataset Generation Framework\nIn this section, we provide more details on our\nsynthetic dataset generation framework. We cre-\nated Synthetic-Persona-Chat using an LLM with\n24 billion parameters. We use top-k sampling with\nk = 40 for decoding during generation, and set the\ntemperature value to 0.7 in all components. We\ngive more details on user and conversation genera-\ntion components in the following subsections.\nA.1\nUser Generation\nIn our framework, the user generation component\nconsists of two steps: expanding the persona at-\ntribute set, and creating realistic user profiles. In\nthis section we provide details on our framework\nfor these two steps:\nPersona Expansion\nAs described in Section\n3.1.1, the persona expansion step involves iden-\ntifying persona categories in the initial persona at-\ntribute set \u03a00, generating queries associated with\nthose categories, and bootstrapping queries to cre-\nate a query set Q. In our framework, we employ the\nScikit-learn (Pedregosa et al., 2011) implementa-\ntion of an agglomerative clustering to identify per-\nsona categories following this clustering method:\nwe represent each persona using a BERT-based rep-\nresentation. Our clustering approach is bottom-up,\nstarting with each persona attribute as an individual\ncluster. At each step, we combine two clusters if\ntheir similarity exceeds a predetermined threshold\nof 0.1. The similarity of two clusters is measured\nusing inter-cluster average cosine similarity. The\nprocess continues until no pair of clusters is more\nsimilar than the threshold.\nAfter identifying the clusters, we sample 3 in-\nstances of persona attributes for each cluster, and\nprompt the LLM using the template in shown in\nsection 3 to construct an initial query set Q0. We\nexpand the query set Q0 using bootstrapping. At\neach step, we sample 5 instances from the available\nqueries, and prompt the LLM using the template in\nTable 6. We repeat this process for 100 steps. Ex-\namples of initial persona attributes, induced queries,\nbootstrapped queries, and bootstrapped persona at-\ntributes can be found in Table 4. The prompt tem-\nplates used in this component are available in Table\n6.\nUser Profile Generation\nWe illustrate a sample\nuser profile creation process in Figure 6. As shown\nin the figure, at each iteration, a randomly selected\nFigure 6: User Profile Construction Example\npersona attribute is checked for consistency and\nnon-redundancy.\nLet \u03c0\u2032 be a randomly selected persona attribute\nin an iteration. For the redundancy criteria, we\nuse the BERT representation of persona attributes.\nWe compute the similarity of the new candidate\npersona attribute \u03c0\u2032 with every persona attribute\nin the user profile. If it is more than a threshold\n(0.9 in these experiments) similar to an attribute\nin the user profile, \u03c0\u2032 is deemed as redundant and\nwill not be added to the user profile. We use the\ncosine similarities of the BERT representations of\nthe persona attributes.\nFor the consistency criteria, we use the NLI\nmodel to verify the consistency of this persona\nattribute with the user profile. For every persona\nattribute in the current user profile \u03c0, we prompt\nthe LLM to create the negated persona attribute \u00ac\u03c0.\nThen, we query the NLI model to check whether\n\u00ac\u03c0 is inferred by \u03c0\u2032 or \u00ac\u03c0\u2032 is inferred by \u03c0. If\neither of these cases is inferred, then the selected\npersona attribute is not consistent with the user\nprofile, and not added to the profile.\nA.2\nConversation Generation\nLLM-based Critic\nIn our framework, the critic\nis implemented by prompting an LLM. We in-\ncluded a mixture of experts approach in the critic,\nwhere each expert prompts the LLM to assess a\nspecific policy in the candidate conversations. Our\nframework includes a set of experts to control the\ngeneral conversation quality. We evaluate the per-\nformance of these experts using a baseline dataset.\nThe baseline dataset for this experiment is FED\nwhich consists of 125 human-annotated instances\nevaluated at the conversation level. We pair the\nconversations and evaluate the experts based on\nthe number of correctly ranked pairs. As shown\nin Table 5, we observe that these experts are more\nthan 80% accurate in distinguishing the better con-\nversation within the pairs. The template for the\nverbalized form of these experts used in our frame-\nwork can be found in Table 6.\nWe also included a toxicity expert and a persona\nfaithfulness expert in the critic. The prompt tem-\nplates used in these experts are available in Table\n6. The persona faithfulness leverages in-context-\nlearning capability of LLMs. It includes a few\nhuman-curated examples of faithful and unfaith-\nful conversations in the instruction prompt. Refer\nto Table 7 for examples of faithful and unfaithful\nconversations used in the instruction prompt.\nB\nSynthetic-Persona-Chat\nSynthetic-Persona-Chat is made of 20k conversa-\ntions, with an average of 11.8 turns per user for\neach. An example Synthetic-Persona-Chat con-\nversation can be found in Table 8. We compare\nSynthetic-Persona-Chat to Persona-Chat across dif-\nferent dimensions. We first assess the characteris-\ntics of SPC using various automatic evaluators, i.e.\nevaluators which do not require human effort. We\nthen conduct a human evaluation experiment on a\nsubset of SPC.\nB.1\nAutomatic Evaluation\nWe conduct a comprehensive analysis and evalua-\ntion of SPC across different dimensions and com-\npare it against PC. We start by analyzing the toxi-\ncity and diversity of SPC using off the shelf tools.\nThen, we elaborate on the experiments which as-\nsess the efficacy of SPC used as the dataset for the\nnext utterance prediction and the profile extraction\ntasks. Finally, we evaluate the quality of SPC con-\nversations using LLM-based evaluation methods.\nToxicity Analysis\nWe analyze the toxicity of the\ngenerated conversations at the final iteration of SPC\nusing an online tool called Perspective3. We repro-\nduce the results of a detailed analysis of toxicity in\nPC as well as in each iteration of our data gener-\nation framework while producing SPC in Table 9.\nWe observe a notable reduction in the frequency of\nconversations deemed as strongly toxic or profane\nthroughout the iterations of generating SPC. This\nreduction can be attributed to the built-in toxicity\nfilter of the employed LLM. While PC contains\n3https://perspectiveapi.com/\nDataset Persona\nSource\nQuery\nExample Persona Attribute\nPersona-Chat\nHuman\nWhat is your job?\nI am a pharmacist.\nWhere do you live?\nI live close to the coast.\nDo you have any pets?\nI have a doberman.\nLLM\nWhat are your talents?\nI am a great listener.\nWhat is your hair color?\nMy hair is auburn.\nWhat is your favorite song?\nI like the song \"Leather and Lace\".\nWikipedia\nHuman\nWhat are your hobbies?\nI spend WAY too much time on Wikipedia.\nWhat is your view on the metric\nsystem?\nI find the metric system to be a logical and\nefficient way to measure things.\nLLM\nWhat is the name of the first al-\nbum you ever purchased?\nMy first album was The Miseducation of Lau-\nryn Hill\nWhat are you interested in?\nI\u2019m looking to learn new recipes and improve\nmy cooking skills.\nTable 4: Persona Categories and Induced Queries Using Our Framework. Queries are generated by the Large\nLanguage Model (LLM). Queries for personas with the \"LLM\" as source, are generated through bootstrapping,\nwhile those with \"human\" as source are generated by sampling persona categories and prompting the LLM. Personas\nwith \"human\" as the source are authored by humans, while \"LLM\" rows represent personas generated using our\nframework.\nPolicy\nPerformance\nDepth\n0.84\nCoherency\n0.96\nConsistency\n0.92\nDiversity\n0.92\nLikable\n0.88\nTable 5: List of FED Experts for Persona-Based Conversation\nGeneration Critic. Performance is measured by the number of\ncorrectly compared conversation pairs in FED baseline based\non the given policy.\nmore than 50 samples that are identified as strongly\ntoxic, SPC includes at most three toxic or profane\nconversations, which is significantly lower (at least\n15 times less). Interestingly, the fraction of conver-\nsations with medium profanity and toxicity in SPC\nis 4 times less than the same type of conversations\nin PC across all iterations. We have removed any\nconversation that was marked as strongly toxic by\nthis tool in the released dataset. Samples of toxic\nconversations are provided in Table 10.\nDiversity Analysis\nWe use hierarchical topic\nmodeling (Blei et al., 2004) to assess the topic\ndiversity of SPC and compare it to that of PC. For a\nfair comparison, we only compare conversations in\nSPC with similar personas in PC. Table 11 displays\nthe number of topics at each level of the topic tree,\nwith the first level indicating the most general topic.\nWe observe similar topic diversity at the first level.\nIn deeper levels, there is a slightly lower diversity\nin SPC.\nNext Utterance Prediction\nWe compare the per-\nformance of different models on the next utterance\nprediction task. As discussed in Section 4.2, these\nmodels are expected to exhibit better performance\nin the next utterance prediction task when user per-\nsonas are provided as prior information. We evalu-\nate ranking and generative models for response se-\nlection to assess this property. We compare models\ntrained on SPC to the same models trained on PC.\nWe use the implementations provided in (Miller\net al., 2017) for the following models:\n\u2022 IR Baseline Given an utterance as a query, the\nIR baseline finds the most similar utterance in\nthe training corpus using tf-idf. It defines the\nutterance after the most similar utterance as\nthe candidate response, and then returns the\nmost similar option to that candidate as the\noutput.\n\u2022 Transformer-Ranker The context of the con-\nversation, as well as the candidate next utter-\nances, are encoded using a BERT-based en-\ncoder. The most similar encoded candidate\nComponent\nTemplate\nQuery Induction\nWhat is the most specific question that you are replying to with the following statements?\n{persona-category-sample-1}\n{persona-category-sample-2}\n{persona-category-sample-3}\nQuery Bootstrapping\n{cluster-query-1}\n...\n{cluster-query-5}\nAdd more persona questions similar to the above examples.\nPersona Bootstrapping\nImagine you are a person with the following persona.\n{random-persona-attribute-1}\n...\n{random-persona-attribute-5}\n{query}. Answer with only one short sentence that starts with \u2019I\u2019 or \u2019My\u2019. Do not repeat the given\npersona.\nFED Expert\nWhich one of Conversation 1 and Conversation 2 between two users {policy}? Why?\nConversation 1: {conv-1}\nConversation 2: {conv-2}\nToxicity Expert\nIs this conversation toxic? Why?\nConversation: {conv}\nConversation Generation\nHere, we list the profiles of two users, user 1 and user 2, followed by an interesting and natural\nconversation between user 1 and user 2, which implicitly reflects their user profiles.\nUser 1 Profile: {conversation1-user-1}\nUser 2 Profile: {conversation1-user-2}\nConversation: {conversation-1}\n...\nUser 1 Profile: {conversation-5-user-1}\nUser 2 Profile: {conversation-5-user-2}\nConversation: {conversation-5}\nGive me more examples like this. The conversation must be more than 5 turns and less than 8 turns. The\nconversation must be natural, and not direct copies of their profiles.\nUser 1 Profile: {user-1}\nUser 2 Profile: {user-2}\nFaithfulness Expert\nGiven user 1 and user 2\u2019s profiles respectively, does the following conversation between the two users\ncontradict either of their profiles? Why?\nUser 1 Profile: {user-1}\nUser 2 Profile: {user-2}\nConversation: {conv-1}\nResponse: {explanation}\nTable 6: Prompting Templates for Large Language Models of Different Components in Our Framework. Variables enclosed in\n{} are filled when the template is populated.\nUser Profiles\nConversations\nFaithfulness Expla-\nnation\nUser 1:\nOne of my favorite hobbies is gaming.\nI currently work in an office.\nI enjoy running\nUser 2:\nI am applying for publishing jobs.\nMy mother is a librarian.\nThe only autographs I\u2019ve ever wanted\nare from authors.\nI used to wear glasses as a child.\nMy favorite color is green.\nUser 1: Do you like video games?\nUser 2: No, I do not like video games.\nUser 1: What do you do for a living?\nUser 2: I am a librarian.\nUser 1: I guess that is why you like to read a lot.\nUser 2: Yes, I like to read books.\nUser 1: What is your favorite color? Mine is green.\nUser 2: That is my favorite color too.\nUser 1: That is cool.\nUser 2: Do you have any pets?\nUser 1: No, I do not have any pets.\nUser 2: I have a dog and a cat.\nUser 1: That is cool.\nUser 2: Do you have any hobbies?.\nUser 1: Yes, I like to write.\nUser 2: What do you write about?\nUser 1: I write about my life.\nUser 2: That is cool.\nUser 1: Thanks!\nNo,\nthe conversation\ndoes not contradict the\nprofiles. Neither user 1\nnor user 2 says anything\nthat contradicts their re-\nspective profiles.\nAl-\nthough the conversation\ndoes not cover every-\nthing in their profiles,\nthat is ok.\nUser 1:\nI would like to move some place\nwarmer.\nI m an out of work farmhand.\nI like watching reality television shows.\nI live in Kansas.\nUser 2:\nI am studying to be a dentist.\nMy favorite team is the raptors.\nMy favorite band is rise against.\nI usually go to bed late.\nI like sports.\nUser 1: It\u2019s so cold in Kansas, I want to go some place warmer.\nUser 2: I am studying dentistry.\nUser 1: I am an out of work farmhand, I love reality tv.\nUser 2: I like the raptors and rise against, I go to bed late.\nUser 1: It is very hard to find work here, I might need to go to the city.\nUser 2: I don\u2019t like sports , but I like the raptors .\nUser 1: I would like to work as a model, but I have no experience.\nUser 2: I am applying for a job as a receptionist.\nUser 1: I am very jealous, I hope you get it.\nYes, the conversation\ncontradicts user 2\u2019s pro-\nfile. In the profile, user\n2 says they like sports,\nhowever, in the conver-\nsation, user 2 says they\ndon\u2019t like sports.\nUser 1:\nMy family is from Kazakhstan.\nI can speak Spanish, English, and\nKazakh.\nI like to go hiking on the weekends.\nI play jazz piano in a band.\nI am a barista.\nUser 2:\nI am a vegan.\nI love game of thrones.\nI like soccer.\nMy brother used to be in the navy.\nUser 1: Hi, what do you do for a living?\nUser 2: I am a barista.\nUser 1: Awesome, me too! Where are you from?\nUser 2: My family is from Kazakhstan, but I was born in Uruguay.\nUser 1: Cool . What do you like to do in your free time?\nUser 2: I love to go hiking and play jazz piano.\nUser 1: Wow, that sounds really nice. I like to watch game of thrones\nand play soccer.\nUser 2: I have never seen game of thrones, but I love soccer.\nUser 1: You should watch it, it is really good.\nUser 2: I will check it out.\nYes, the conversation\ncontradicts user 2\u2019s pro-\nfile. In the profile, user\n2 says they love game\nof thrones, but in the\nconversation user 2 says\nthey have never seen it.\nUser 1:\nMy favorite book is David Copperfield.\nI have a twin brother.\nI am really short.\nI love to bake cakes.\nI have won tap dance competitions.\nUser 2:\nI enjoy spending a lot of money on my\nhobbies.\nI love photography.\nI like to use candles instead of lights.\nI do not like waking up early.\nUser 1: Hi there! Do you like to read?\nUser 2: I like to read. I like to go to the library.\nUser 1: Oh, cool! What is your favorite book?\nUser 2: I really like David Copperfield. I have a twin brother and I am\nreally short.\nUser 1: Are you good at baking?\nUser 2: I can bake cakes. I have won tap dance competitions.\nUser 1: Do you like photography?\nUser 2: I love taking pictures. I also like to use candles instead of lights.\nUser 1: Do you like to wake up early?\nUser 2: I like to sleep in. I do not like waking up early.\nUser 1: Do you like to spend money?\nUser 2: I love spending money on my hobbies.\nNo,\nthe conversation\ndoes not contradict ei-\nther user profiles.\nAl-\nthough user 2 says they\nare really short, which\nis in user 1\u2019s profile, it\ndoesn\u2019t mean user 2 can-\nnot be really short too.\nTable 7: Examples of Faithful and Unfaithful Conversations for Prompting the Persona Faithfulness Expert in Our Framework.\nThe \u2018Faithfulness explanation\u2019 columns are used to fill the \u2018explanation\u2019 variable in the faithfulness expert template.\nUser Profiles\nConversation\nUser 1\nI am male.\nMy weakness are sweets.\nI am unsociable.\nI am a jack of all, master\nof none.\nUser 2\nMy favorite music is rock\nand roll.\nI love to travel.\nI am from Russia.\nI enjoy skiing.\nI go to school for business.\nUser 1: Hello\nUser 2: Hello, what\u2019s your name?\nUser 1: My name is [name], and you?\nUser 2: My name is [name], I\u2019m from Russia.\nUser 1: Nice to meet you, [name]. I\u2019m from the United States.\nUser 2: It\u2019s nice to meet you too, [name]. What are you interested in?\nUser 1: I\u2019m interested in a lot of things, but my main interests are music\nand travel.\nUser 2: I love music too! What kind of music do you like?\nUser 1: I like a lot of different kinds of music, but my favorite is rock\nand roll.\nUser 2: Me too! I love classic rock and roll. Who are your favorite\nbands?\nUser 1: My favorite bands are The Beatles, The Rolling Stones, and Led\nZeppelin.\nUser 2: Those are all great bands! I love them too.\nUser 1: What about you, what are your favorite bands?\nUser 2: My favorite bands are The Doors, Pink Floyd, and Queen.\nUser 1: Those are all great bands too! I love them too.\nUser 2: So, what do you like to do for fun?\nUser 1: I like to go to concerts, play video games, and watch movies.\nUser 2: I love going to concerts too! I also like to go skiing and play\ntennis.\nUser 1: I love skiing too! I haven\u2019t played tennis in a while, but I used to\nplay a lot.\nUser 2: We should go skiing together sometime!\nUser 1: That would be fun! I\u2019m free this weekend if you are.\nUser 2: I\u2019m free this weekend too! We should go skiing.\nUser 1: Great! I\u2019ll text you the details.\nTable 8: Sample Conversation from Synthetic-Persona-Chat. This conversation was synthesized from user profiles in Persona-\nChat\nToxicity\nProfanity\nConfidence\nweak(< .2)\nmedium(.2-.8)\nstrong(>.8)\nweak(< .2)\nmedium(.2-.8)\nstrong(>.8)\nPC\n10875\n4448\n53\n10891\n1676\n57\nSPC Iter 1\n10902\n1192\n3\n10903\n340\n3\nSPC Iter 2\n10900\n1096\n1\n10901\n345\n1\nSPC Iter 3\n10902\n1088\n1\n10902\n376\n0\nTable 9: Frequency of Toxic Conversations in Persona-Chat and Synthetic-Persona-Chat\nSource\nConversation\nPersona-Chat\n...\nUser 1: I like bloody stuff.\nUser 2: It reminds me of the dark which makes me afraid of it.\nUser 1: You are a silly goose.\nPersona-Chat\n...\nUser 2: Cool. Why do you say that? Because I am a red head?\nUser 1: No. Ikn. Why do you ask so many questions? Mr. Thomas is dumb.\nSynthetic-Persona-Chat\nUser 1: I can imagine. What\u2019s your favorite part of the job?\nUser 2: I love working with my team and seeing our restaurant succeed.\nUser 1: That\u2019s great. What\u2019s your least favorite part of the job?\nUser2: My least favorite part is dealing with my boss. He\u2019s a real jerk.\nTable 10: Examples of Toxic Conversations. The first two examples are segments of conversations from Persona-Chat. The\nfinal example is a segment from a toxic conversation in Synthetic-Persona-Chat, which has been removed in the released dataset.\nTopic Level\nPC\nSPC\n1\n27\n27\n2\n232\n213\n3\n470\n403\n4\n137\n118\n5\n30\n26\nTable 11: Vertical Topic Diversity in Persona-based Datasets\nto the conversation context, as measured by\na dot-product in their representation space, is\nselected as the output (Humeau et al., 2020).\n\u2022 Transformer-Generator This model is a\nsequence-to-sequence model (Sutskever et al.,\n2014) which uses transformers as encoders\nand decoders.\nWe also evaluate the performance of the next ut-\nterance prediction models when given no user, one\nuser, and both user personas. The results of this\nexperiment are available in Table 12. We observe\nthat the highest performance improvement for all\nmodels trained on PC is when self-personas are\ngiven as input. We do not observe such a pattern\nin SPC. This indicates a higher degree of bidirec-\ntionality in SPC conversations compared to those\nof PC.\nProfile Extraction\nA potential use-case of the\nSPC dataset is training a model to predict user per-\nsonas from a conversation. This is only possible if\nthe dataset is highly faithful, meaning that any per-\nsona attribute inferred from the conversation is in\nthe user profile or compatible with the user profile.\nIn this context, a faithful conversation is expected\nto have high precision in the profile extraction task,\nwhile a conversation that highly reflects user per-\nsonas is expected to have high recall in this task.\nWe evaluate the task of user profile extraction\nfor conversations in SPC, and compare the results\nagainst those of PC. We frame the task of profile\nextraction as a ranking task, using the utterances\nwithin the conversations as queries. The goal is to\nrank a set of persona attribute options. For each\nconversation, we include the speakers\u2019 persona at-\ntributes in the available options. Additionally, we\nselect 25 random user persona attributes from other\nspeaker profiles within the dataset to serve as dis-\ntractors. The input to the profile extraction is ut-\nterances from a single user as the speaker, while\nthe output is a list of persona attribute options for\na target user, which could be either user 1 or user\n2. The results of this experiment are presented in\nTable 13. We observe that the performance of the\nprofile extraction methods is higher in SPC in 3 of\nthe 4 scenarios. Interestingly, we observe that with\nboth datasets, when the target and the speaker are\ndifferent, the performance of profile extraction is\ngreater compared to the cases when the target and\nspeaker users are the same.\nLLM-based Quality Evaluation\nWe leverage\nLLM-based conversation quality evaluators from\nthe literature to compare the quality of SPC and\nPC. These evaluators rely on the human curated\nprompt templates for different metrics including\nconsistency, fluency, etc. We used these evalua-\ntors with minimum change in the original prompt\ntemplates. These evaluators are:\n\u2022 LLM-Eval (Lin and Chen, 2023) is a multi-\ndimensional automatic evaluation designed\nfor conversations. It uses a human-curated\nPersona-Chat\nSynthetic-Persona-Chat\nMethod\nMetric\nNo Persona\nSelf Persona\nTheir Persona\nBoth Personas\nNo Persona\nSelf Persona\nTheir Persona\nBoth Personas\nIR baseline\nhit@1\n0.1869\n0.3683\n0.1519\n0.3281\n0.1861\n0.2596\n0.1882\n0.2493\nTransformer(Ranker)\nhit@1\n0.2513\n0.275\n0.1922\n0.2572\n0.7164\n0.6227\n0.6988\n0.7214\nTransformer\nhit@1\n0.0896\n0.08512\n0.0873\n0.0813\n0.0526\n0.629\n0.053\n0.051\n(Generator)\nppl\n65.57\n72.24\n62.49\n64.07\n5.54\n5.47\n5.4\n5.405\nTable 12: Evaluation of Next Utterance Prediction models conditioned on different user personas.\nF-Score\nTarget\nSpeaker\nPC\nSPC\nuser 1\nuser 1\n0.505\n0.574\nuser 1\nuser 2\n0.737\n0.68\nuser 2\nuser 1\n0.50\n0.57\nuser 2\nuser 2\n0.456\n0.494\nTable 13: Accuracy of Profile Extraction in Four Dif-\nferent Scenarios. The \u2018Target\u2019 column represents the\nuser profile to be extracted, while the \u2018Speaker\u2019 column\nindicates the speaker of the turns given to the model as\ninput.\nprompt which describes evaluation dimen-\nsions, serving as a unified evaluation schema.\nThis prompt evaluates the conversation across\nmultiple dimensions (e.g. fluency) in a single\nmodel call. We show this unified schema in\nTable 14.\n\u2022 GPT-Score (Fu et al., 2023) leverages emer-\ngent abilities of LLMs, i.e. zero-shot instruc-\ntions, to score texts. It contains a prompt tem-\nplate, and for each quality criterion, populates\nthe template with a human description of the\ncriteria along with the valid score range for\nthat criteria. Example prompts are provided\nin Table 14.\n\u2022 G-Eval (Liu et al., 2023) introduces a frame-\nwork that employs LLMs with a chain-of-\nthought approach to assess the quality of nat-\nural language generated outputs.\nFor any\nevaluation criteria, G-Eval prompts the LLM\nwith the criterion\u2019s description, prompting the\nmodel to generate the necessary evaluation\nsteps. It then uses these steps to prompt the\nLLM to score given output for that criterion.\nIt considers the probability of getting each\npermissible score as the output of the prompt,\ni.e., it considers the probability distribution\nof scores assigned by the LLM. The reported\noutput is the expected value of the score dis-\ntribution by the LLM. Table 14 includes an\nexample prompt.\nResults of this evaluation are presented in Table\n15. We observe that SPC consistently outperforms\nPC across all the dimensions we evaluate. The\nsuperiority of SPC is more prominent when us-\ning GPT-Score, for which each evaluated criterion\nshows an average improvement of at least 23 points.\nB.2\nHuman Evaluation\nWe run a human evaluation of the performance\nof our method via a crowdsourcing platform. We\nconduct a Turing test, and a faithfulness study -\nboth of which we describe in more details in the\nfollowing subsections - at the end of every iteration\nof the generation of SPC.\nTuring Test\nWe randomly select 200 user pairs\nfrom PC. For each example, we show the annota-\ntors the user pair, together with the corresponding\nconversations from PC and SPC, and ask them to\nselect the conversation that was synthetically gen-\nerated. We show an example of this crowdsourcing\ntask in Figure 7. The results of the Turing test are\navailable in Table 16. We report the losing rate of\nSPC in Turing test, and Fleiss\u2019 Kappa to assess the\ninter-rater agreement. The agreement falls into the\nfair to moderate agreement bucket.\nFaithfulness\nWe present the annotators with a\nconversation, and a set of options of persona at-\ntributes. The annotators are asked to select the user\npersona attributes they would infer from the conver-\nsation. Figure 8 shows a sample of the annotation\ntask in this study. The options include the persona\nattributes of the speakers in the conversation, and a\nset of distractor persona attributes. We created dis-\ntractor persona attributes using different strategies\nto cover different difficulty levels. For a persona\nattribute set \u03a0, we create a set \u00ac\u03a0 of distractor\npersona attributes as:\nNegated personas We prompt an LLM to negate\npersona attributes. For example, the negation of\npersona attribute \"I like vegetables\" is \"I don\u2019t like\nvegetables\".\nRandom personas We randomly select persona\nattributes from user profiles in other conversations\nEvaluator\nMetric\nPrompt Template\nLLM-Eval\nAll\nHuman: The output should be formatted as a JSON instance that conforms to the JSON\nschema below.\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a\nlist of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}} the\nobject {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object\n{\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\nHere is the output schema: {\"properties\": {\"content\": {\"title\": \"Content\", \"description\":\n\"content score in the range of 0 to 100\", \"type\": \"integer\"}, \"grammar\": {\"title\":\n\"Grammar\", \"description\": \"grammar score in the range of 0 to 100\", \"type\": \"integer\"},\n\"relevance\": {\"title\": \"Relevance\", \"description\": \"relevance score in the range of 0 to\n100\", \"type\": \"integer\"}, \"appropriateness\": {\"title\": \"Appropriateness\", \"description\":\n\"appropriateness score in the range of 0 to 100\", \"type\": \"integer\"}}, \"required\":\n[\"content\", \"grammar\", \"relevance\", \"appropriateness\"]}\nScore the following dialogue generated on a continuous scale from {score-min} to {score-\nmax}.\nDialogue: {dialogue}\nGPT-Score\nConsistency\nAnswer the question based on the conversation between two users.\nQuestion: Are the responses of users consistent in the information they provide throughout\nthe conversation? (a) Yes. (b) No.\nConversation: {dialogue} Answer:\nG-Eval\nCoherence\nYou will be given a pair of user personas. You will then be given one conversation between\nthis persona pair.\nYour task is to rate the conversation on one metric.\nPlease make sure you read and understand these instructions carefully. Please keep this\ndocument open while reviewing, and refer to it as needed.\nEvaluation Criteria:\nCoherence (1-5) - the collective quality of all utterances. We align this dimension with\nthe Document Understanding Conference (DUC) quality question of structure and\ncoherence\n(https://duc.nist.gov/duc2007/quality-questions.txt),\nwhereby\n\"the conversation should be well-structured and well-organized. The conversation should\nnot just be a heap of related information, but should build from utterance to a coherent\nbody of conversation about a topic.\"\nEvaluation Steps:\n1. Read and understand the given conversation between the pair of user personas.\n2. Evaluate the conversation based on the coherence of the utterances.\n3. Rate the conversation on a scale of 1 to 5, with 5 being the highest coherence and 1\nbeing the lowest coherence.\n4. Justify the rating by referring to specific aspects of the conversation that demonstrate its\ncoherence or lack thereof.\nExample:\nPersonas: {personas}\nConversation: {dialogue}\nEvaluation Form (scores ONLY):\n- Coherence:\nLLM-Faithfulness\nInference\nInstruction: Select User {user} persona attributes that are directly inferred from this\nconversation.\nContradiction\nInstruction: Select User {user} persona attributes that strongly contradict this conversation.\nTable 14: Prompt Templates in LLM-based Conversation Quality Evaluators. Variables enclosed in {} are filled when the\ntemplate is populated.\nFigure 7: Preview of the Turing Test Task on the Crowdsourcing Platform\nEvaluator\nCriteria\nPC\nSPC\nSPC Iter 1\nFED\nFaithfulness\nLLM-Eval (Lin and Chen, 2023)\nContent\n81.96\n88.84\n88.71\n87.61\n88.67\nGrammar\n87.12\n93.64\n93.68\n93.09\n93.56\nRelevance\n86.82\n94.16\n93.81\n92.88\n93.79\nAppropriateness\n86.99\n95.84\n96.17\n95.68\n96.19\nGPT-Score (Fu et al., 2023)\nFluency\n67.04\n98.89\n96.28\n96.65\n97.83\nConsistent\n3.47\n64.25\n50.43\n43.45\n48.69\nCoherent\n69.41\n100\n100\n98.99\n100\nDepth\n5.40\n37.36\n29.30\n19.40\n29.01\nDiversity\n72.98\n96.42\n94.02\n92.79\n94.11\nLikeable\n36.53\n91.04\n93.11\n91.90\n87.98\nG-Eval (Liu et al., 2023)\nRelevance (1-5)\n2.288\n2.992\n2.986\n2.941\n2.99\nFluency (1-3)\n1.928\n2.002\n2\n1.998\n1.999\nConsistent (1-5)\n1.736\n2.651\n2.587\n2.449\n2.496\nCoherent (1-5)\n2.505\n2.997\n2.997\n2.991\n2.998\nFaithfulness (1-5)\n1.754\n2.959\n2.8801\n2.79\n2.868\nTable 15: Results of Automatic Evaluations of Synthetic-Persona-Chat and Persona-Chat. The \"FED\" column is the\nevaluation of the dataset generated without FED expert and the column \"Faithfulness\" is the evaluation results of the\ndataset generated without the faithfulness expert in the Critic.\nConversation Source\n% Lose\n\u03ba\n# annotators\nSPC Iter 1\n17.2\n0.41\n50\nSPC Iter 2\n18.5\n0.48\n40\nSPC Iter 3\n8.8\n0.22\n11\nSPC Iter 3*\n8.04\n0.56\n24\nSPC (LLM2)\n11.5\n0.49\n36\nTable 16: Turing test results on a sample of 200 conversa-\ntions. The first column shows the percentage of SPC losing\ncompared to PC in the Turing test. Note that the last iteration\n(3) of SPC is an evaluation of the segment of conversations\nbased on the extended persona set.\nin the dataset.\nContradicting personas We prompt an LLM to\ngenerate a persona attribute which contradicts the\nusers\u2019 personas.\nEach entry of this task includes 8 user persona\nattributes as options, where 4 of them are the real\npersona attributes, and the other 4 are distractors.\nWe evaluate the precision of the human annota-\ntors, and report it as a proxy to the conversation\nfaithfulness in Table 3.\nC\nAblation Studies\nWe run several ablation studies to evaluate the im-\nportance of individual components in our frame-\nwork. We begin by analyzing the effect of the\npersona expansion module. We then review the\nimpact of each expert in the mixture forming our\nCritic.\nC.1\nPersona Expansion\nWe assess the importance of the query-based per-\nsona expansion module introduced in Section 3.1.1.\nSimilarly to the experiment outlined in Section\n4.1, we run the persona expansion on two datasets:\nWikipedia and PC. The results of this experiment\nare presented in Table 17. We designate the per-\nsona expansions without the inducted query set (Q)\nas \u2018Wikipedia-0\u2019, and \u2018PC-0\u2019, and run the same\nnumber of iterations for each (100 iterations). We\nobserve that PC-0 includes 4,477 new persona at-\ntributes, 20 percent less than PC. The difference in\nthe number of newly generated persona attributes is\nmore pronounced in the case of Wikipedia, where\nWikipedia-0 consists of 4,742 persona attributes,\n50 percent less than Wikipedia+. This trend is also\nobserved in the number of persona clusters, with\nPC-0 and Wikipedia-0 having 6% and 49% less\nclusters respectively. This pattern suggests the ef-\nfectiveness of the query-based persona expansion\nin maintaining the diversity of the persona set. Fur-\nthermore, the average persona attribute length in\nPC-0 is 11.38 tokens, which is 28% less than SPC.\nThis reduction points to less detailed and specific\npersona attributes. In contrast, the expansion in\n\u2018Wikipedia-0\u2019 exhibits similar average persona at-\ntribute lengths compared to \u2018Wikipedia+\u2019.\nFigure 8: Preview of the Faithfulness Task on the Crowdsourcing Platform.\nDataset\nPC\nSPC\nPC-0\nWikipedia\nWikipedia+\nWikipedia-0\n# Persona Attributes\n4,723\n10,371\n9,200\n8,768\n18,293\n13,510\n# Clusters\n323\n553\n520\n408\n986\n502\nInterCluster-Dist\n0.836\n0.863\n0.842\n0.816\n0.85\n0.83\nAVG length\n7.65\n15.9\u2217\n11.38\u2217\n10.45\n15.2\u2217\n15.2\u2217\nTable 17: Evaluation of the Expanded Persona Attribute Sets. The numbers with \u2032\u2217\u2032 indicate the metric value on the\nnewly generated persona attributes, in contrast to the initial persona attributes.\nC.2\nConversation Quality\nWe analyze the effect of the experts within our\nCritic. We remove each expert, and generate a\ndataset using one iteration of our framework. We\ncompare the resulting datasets against the output of\nthe first iteration of SPC. We use the evaluators in-\ntroduced in B.1. The results of this experiment are\nsummarized in Table 15. We observe that the ex-\nclusion of the experts results in worse performance\naccording to most criteria: 3 out of 4 in LLM-Eval,\n4 out of 6 in GPT-Score, and 3 out of 5 in G-Eval.\nC.3\nFaithfulness\nWe ablate the faithfulness critic, and generate a\ndataset that we compare against SPC. We compare\nthese datasets both automatically, using human an-\nnotators (Turing Test), and using a prompted LLM\n(LLM-Evaluator). We describe this study in more\ndetails below.\nTuring Test\nWe run a human study to compare\na small subset of conversations created without\nthe faithfulness expert against their equivalent cre-\nated with that expert. This experiment process is\nsimilar to 4.3 and it is conducted for 200 conver-\nsations. The precision decreases from 78.0% to\n66.0% without this critic, highlighting its effective-\nness in eliminating conversations with contradic-\ntory information about user personas. The recall\ndecreases from 36.0% to 23.0%, demonstrating a\nhigher reflection of personas in the conversations\nin the presence of the faithfulness expert.\nLLM-Evaluator\nWe extend our comparison to\nthe entire dataset using an LLM as an annotator,\nfollowing (He et al., 2023; Bansal and Sharma,\n2023; Chiang and yi Lee, 2023). Table 18 shows\nthe faithfulness of the conversations generated in\nthe first iteration without the faithfulness expert.\nThe templates used in the LLM-based annotators\nare described in Table 15 in the rows with \"LLM-\nFaithfulness\" as their evaluator.\nNote that the\nannotator-based LLM is created using a different\nLLM, gpt-3.5-turbo (Brown et al., 2020a; Ouyang\net al., 2022), than the LLM used for dataset genera-\ntion.\nC.4\nNext Utterance Prediction\nWe follow the experimental setting described in\nsection 4.2, and compare the performance of var-\nious next utterance prediction models trained on\nSPC against the same models trained on datasets\ncreated in the absence of certain experts.\nWhen using the IR Baseline as the next utter-\nance prediction method, we observee that its high-\nest performance of 39% hit@1 occurs when the\nFED critic is absent during dataset creation. This\noutcome aligns with FED\u2019s emphasis on conver-\nsation quality, excluding persona-related aspects.\nConversely, the Transformer Ranker, capable of\nunderstanding intricate concepts, achieves its peak\nperformance of 13.9% hit@1 when none of the\nexperts are absent. This result supports the inclu-\nsion of both FED and the Faithfulness expert in the\nmodel architecture. In generative models, the ab-\nsence of FED impacts the next utterance prediction\nmodel the most, leading to a notable decline in per-\nformance (e.g. \u221212% hit@1, \u22129% BLEU, \u221210%\nROUGE). This observation underscores the crucial\nrole played by FED in enhancing the generative\ncapabilities of the model.\nLLM Evaluator (%)\nHuman Evaluator (%)\nAbsent Component\nInference\nContradiction\nPrecision\nRecall\nNone\n33.2\n24.5\n78.5\n36.4\nFaithfulness\n32.7\n28.8\n66.1\n23.1\nFED\n31.7\n28.5\nN/A\nN/A\nTable 18: Faithfulness of Generated Conversation Datasets Using the Framework While Eliminating Each Component. The first\nrow represents the framework without removing any component, equivalent to the first iteration of Synthetic-Persona-Chat.\nAbsent Component\nFaithfulness\nFED\nNone\nMethod\nMetric\nNone\nPersona\n% Change\nNone\nPersona\n% Change\nNone\nPersona\n% Change\nIR Baseline\nhit@1\n18.7\n38.7\n+106\n19.0\n39.0\n+105\n18.9\n38.7\n+105\nTransformer (Ranker)\nhit@1\n10.9\n13.5\n+24\n10.7\n13.6\n+27\n12.4\n13.9\n+11\nhit@1\n8.9\n7.4\n-16\n8.4\n7.4\n-12\n8.2\n7.0\n-14\nTransformer\nPerplexity\n204\n214\n+5\n174\n185\n+6\n203\n210\n+3\n(Generator)\nBLUE\n0.11\n0.10\n-11\n0.11\n0.10\n-9\n0.10\n0.08\n-15\nROUGE\n0.14\n0.15\n-12\n0.14\n0.12\n-10\n0.13\n0.10\n-17\nTable 19: Results of the Next Utterance Prediction Experiment in the Ablation Study. The numbers in the table represent the\nperformance of the trained model on the test portion of the Persona-Chat dataset.\n"
  },
  {
    "title": "SlimmeRF: Slimmable Radiance Fields",
    "link": "https://arxiv.org/pdf/2312.10034.pdf",
    "upvote": "6",
    "text": "SlimmeRF: Slimmable Radiance Fields\nShiran Yuan1,2,3,* Hao Zhao1,\u2020\n1AIR, Tsinghua University\n2Duke University\n3Duke Kunshan University\nsy250@duke.edu, zhaohao@air.tsinghua.edu.cn\nAbstract\nNeural Radiance Field (NeRF) and its variants have re-\ncently emerged as successful methods for novel view syn-\nthesis and 3D scene reconstruction. However, most current\nNeRF models either achieve high accuracy using large model\nsizes, or achieve high memory-efficiency by trading off accu-\nracy. This limits the applicable scope of any single model,\nsince high-accuracy models might not fit in low-memory\ndevices, and memory-efficient models might not satisfy high-\nquality requirements. To this end, we present SlimmeRF, a\nmodel that allows for instant test-time trade-offs between\nmodel size and accuracy through slimming, thus making\nthe model simultaneously suitable for scenarios with differ-\nent computing budgets. We achieve this through a newly\nproposed algorithm named Tensorial Rank Incrementation\n(TRaIn) which increases the rank of the model\u2019s tensorial rep-\nresentation gradually during training. We also observe that\nour model allows for more effective trade-offs in sparse-view\nscenarios, at times even achieving higher accuracy after\nbeing slimmed. We credit this to the fact that erroneous infor-\nmation such as floaters tend to be stored in components corre-\nsponding to higher ranks. Our implementation is available at\nhttps://github.com/Shiran-Yuan/SlimmeRF.\n1. Introduction\nView synthesis and the reconstruction of 3D scenes are\nlongstanding problems in computer vision that have impor-\ntant applications in many domains. The NeRF [38] model,\nwhich represents the geometry and appearance of 3D scenes\nas neural fields, has recently proven to be an effective so-\nlution to this problem, allowing for high-quality 2D view\nsynthesis and 3D geometric reconstruction. Based on this\nwork, numerous variants were proposed, using novel model\narchitectures to further improve NeRF\u2019s reconstruction qual-\nity, efficiency and applicability to different settings.\nDespite the success of NeRF models, most still suffer\nfrom a common disadvantage that hinders flexibility: mod-\n*Research done during internship with AIR.\n\u2020Corresponding author.\nGround Truth\nRank 16 \n36.4 MB \nPSNR 37.25\nRank 12 \n27.5 MB \nPSNR 36.63\nRank 8 \n18.7 MB \nPSNR 35.55\nRank 4 \n9.8 MB \nPSNR 32.90\nRank 2 \n5.3 MB \nPSNR 28.54\nRemaining Ranks\nDiscarded Ranks\nTest-time \nSlimming\nFigure 1. A single model, trained only once, could achieve multi-\nple compression levels at test time. Our SlimmeRF model enables\ntrade-offs between model size and accuracy to be readily made at\ntest time while not requiring any re-training. Shown in the figure\nare testing results for the Hotdog scene at different compression\nlevels, all from a single SlimmeRF-16 model.\nels trained in scenarios with loose requirements might be\ninapplicable in cases where requirements are stricter. Hence,\nif we target for models that are usable across different us-\nage cases, we can only train them according to the strictest\nrequirements, sacrificing accuracy.\nTo tackle this issue, we propose SlimmeRF, a model that\ncould be trained using high capacities and achieves fairly\ngood results on systems with lower capacities via test-time\nslimming. While test-time slimming by partially discard-\ning parameters from the model could easily be achieved on\nexplicit NeRF models such as Instant-NGP [39] and Ten-\nsoRF [4], such na\u00a8\u0131ve approaches have very poor accuracy\n(as demonstrated by our experiments with the baseline in\nSection 4). We, in contrast, successfully achieve this via the\nmathematical framework of low-rank tensor approximation.\nOur Holy Grail: Slimmability in NeRF.\nSlimmability is\nthe ability of trained models to retain high-quality results\nwhen part of the model is removed, and therefore allow\nfor flexible trade-offs between memory size and accuracy\nduring test time. This concept was explored previously in the\ncontext of recognition networks and generative adversarial\nnetworks [17, 26, 68], but our approach is fundamentally\ndifferent from those works: to achieve slimmability, we\nutilize low-rank approximation to characterize the 3D scene\n1\narXiv:2312.10034v1  [cs.CV]  15 Dec 2023\nas component tensors. During training, we initially set the\nnumber of component tensors (i.e., the rank of the tensor\ndecomposition) to a low level, and only increment the rank\nwhen the existing components have already been trained well,\na procedure which we formalize as the TRaIn (Tensorial\nRank Incrementation) algorithm. Through this procedure,\nwe assure that the most important information (e.g., basic\ngeometrical outline, dominant colors, etc.) is aggregated in\nthe first few components, and hence that removing the last\nfew components would not cause large drops in accuracy.\nNotably, CCNeRF [59] is a prior work which successfully\nachieves dynamic compression at test time, hence being sim-\nilar to our work in terms of ultimate goals. However, we\nnote that it cannot achieve state-of-the-art performance in\nterms of accuracy. This could be attributed to the fact that\nthe decomposed components they use are heterogeneous:\nwith vector-based components storing essential information\nin a compact manner and matrix-based components being\ndiscardable. In contrast, our work\u2019s representation of com-\nponents is homogeneous: all individual components follow\nthe same structure, and hence the model\u2019s learning ability is\nnot affected.\nIn addition, since floater noises generally do not adhere to\nlow-rank structures, we expect them to be mainly present in\nthe components corresponding to higher ranks. Thus, we test\nour method on sparse-view scenarios, which usually suffer\nfrom floaters due to insufficient multi-view constraints. Our\nexperiments verify that though SlimmeRF notably does not\nincorporate knowledge of any properties that are specific to\nsparse views, it still achieves fairly good results in sparse-\nview settings. We also observe the interesting result that\nsometimes the accuracy of SlimmeRF in sparse-view scenar-\nios will increase as the model is slimmed, which seems a\ncounter-intuitive result (achieving higher performance with\nsmaller memory and less model parameters retained), but\nagrees with our hypothesis that floaters reside in components\ncorresponding to higher ranks.\nContributions.\nThe following are the main contributions\nof our work:\n\u2022 We introduce the novel objective and research direction\nof slimmability to NeRF research, which could be use-\nful for creating models that need to be flexible in their\napplicable scope while retaining high accuracy.\n\u2022 We constructively demonstrate how slimmability could\nbe achieved in NeRFs via creating a model which could\nboth be used as a memory-efficient model and a normal\nmodel with state-of-the-art reconstruction quality, also\nformalizing our methods as the TRaIn algorithm.\n\u2022 We demonstrate that our model allows for effective\ntrade-offs between model size and accuracy under\nsparse input scenarios, which signifies that under our\nframework, we can aggregate erroneous information\nsuch as floaters separately from key information.\n\u2026\nTrained \nSimultaneously\nVector \nFactors\nMatrix \nFactors\n\u2026\nVector \nFactors\nMatrix \nFactors\n\u2026\n\u2026\n\u2026\nTrained \nSimultaneously\nDiscardable\n\u2026\nVector \nFactors\nMatrix \nFactors\nTrained \nIncrementally\nTensoRF\nCCNeRF\nSlimmeRF (Ours)\nDiscardable\nDynamic\nRank\n(Masked)\nFigure 2. Comparison of our paradigm with similar models. We\ncompare the paradigm of SlimmeRF with TensoRF [4] and CCN-\neRF [59] to emphasize differences. In particular, note the difference\nbetween the heterogeneous paradigm of CCNeRF (only discard-\ning matrix-based components) and the homogeneous paradigm of\nSlimmeRF (which discards components consisting of vector- and\nmatrix- based components entirely). Also note that SlimmeRF is\nunique in its usage of incremental training.\n2. Related Works\n2.1. Scene Representation with Radiance Fields\nRecently, models based on Neural Radiance Fields\n(NeRF) [38] have become popular in areas such as gen-\nerative modelling [2,3,13,29,33\u201335,41,47,48,53,61], pose\nestimation [8, 21, 30, 52, 57, 60, 64, 73], human body mod-\nelling [9,22,28,45,46,54,69,72], mobile 3D rendering [6,12,\n25,42,49,62,63], and so on. In addition, numerous advance-\nments in efficiency were made [12,16,31,32,39,49,66], thus\nexpanding the scope of NeRF-based models\u2019 capabilities.\nA notable development in NeRF structures which lays the\nfoundation for our work is the representation of scenes using\nexplicit data structures [4,12,16,32,39,49,66,67]. Not only\ndoes this allow for many improvements to the NeRF model,\nsuch as training and rendering speed, but it also makes the\nmodel parameters explainable. Our usage of tensorial grids\nrepresented by a Vector-Matrix Decomposition is also in-\nspired by an important work in this direction, TensoRF [4].\n2.2. Low-Rank Approximation of Tensors\nThe low-rank approximation of tensors is a mathematical\ntechnique that could be applied to compress data by exploit-\ning the inherent low-rank properties of natural signals. It has\nseen a considerable number of applications in areas such as\nhyper-spectral image processing [44], low-level vision [65],\nand big data processing [56].\nThough some previous works [4,15,19,55,59] have in-\ncorporated the concept of low-rank approximation or similar\ntensor decomposition-based concepts into NeRF, they ne-\nglect the potentials of tensors as slimmable representations.\nTensoRF [4], for instance, uses a tensorial structure for data\nrepresentation but does not explore low-rank approxima-\ntion and its applications; CCNeRF [59], as another example,\nexplicitly states its relationship with low-rank tensor ap-\nproximation, but it achieves so using a heterogeneous data\nstructure where there are two inherently different types of\n2\nFigure 3. Illustration of SlimmeRF\u2019s model architecture. Representing a radiance field as the tensorial grids G\u03c3 and Gc, we apply tensor\ndecomposition to the grids to arrive at their low-rank approximation forms\nR\nP\nr=1\nG(r)\n\u03c3\nand\n3R\nP\nr=1\nG(r)\nc . They are then masked according to\ndynamic rank rd and trained through the Tensorial Rank Incrementation algorithm, with the grids being optimized using gradients calculated\nfrom the unmasked portions. Eventually the model predicts density \u03c3 and appearance c using the grids with respect to inputs position x and\nviewing direction d. The slimming procedure then discards elements corresponding to higher rank if lower memory usage is desired.\nfactors, and test-time trade-offs are achieved by discarding\nmatrix-based factors. This added complexity to the repre-\nsentation structure violates Occam\u2019s razor, and hence, as\nsupported by our experimental results, causes reconstruction\nquality to be suboptimal. Our method is centered around the\nmathematical framework of low-rank tensor approximation,\nand achieves this via a novel incremental training approach\n(the TRaIn algorithm), achieving both test-time dynamic\naccuracy-size trade-offs and high model accuracy. For a\nclearer display of the differences, please see Figure 2.\nHence, our work is the first to incorporate low-rank ap-\nproximation of tensors into NeRF, for the purpose of using\nthe approximation model to successfully achieve dynamic\nreduction of the model size.\n2.3. Compressing NeRFs\nCurrent works allow for compression of NeRFs mainly\nby directly making the data structure memory-efficient [4,\n50, 51, 59, 62] or by compressing trained models for effi-\ncient storage [10, 27,71]. However, currently there are no\nworks besides CCNeRF [59] which explicitly allow for test-\ntime dynamic trade-offs to be made between model size and\nmodel accuracy, and its reconstruction quality is suboptimal.\nOur model solves this problem by achieving slimmability\nand retaining high accuracy while not being slimmed.\n2.4. Training NeRFs with Sparse Inputs\nSince large amounts of views with adequate quality might\nbe difficult to obtain in many real-life scenarios, training\nNeRFs with only very sparse inputs has been a topic of\ninterest. There are many methods in this direction of re-\nsearch [1, 5, 7, 18, 40, 67], which reconstruct scenes with\nconsiderable quality through few (< 10) views or even only\none view. Most such methods rely on specific priors regard-\ning inherent geometric properties of real-life scenes.\nWe observe that while SlimmeRF is not specialized for\nthis type of tasks, its performance is still fairly good (even\nexceeding specialized sparse-view methods in some viewing\ndirections, as shown in Figure 9), despite requiring small\nmemory, being quick-to-train, not needing pretraining, and\nachieving excellent slimmability. This supports our hypoth-\nesis that floaters and other erroneous information are more\nlikely to be stored in higher ranks.\n3. Methodology\nThe structure of our model is displayed in Figure 3. We\nuse the geometry and appearance grid tensors to represent\nthe scene, and use the Vector-Matrix (VM) Decomposition\nto represent it as tensorial components (details are elaborated\non in Subsection 3.1). The model is trained via Tensorial\nRank Incrementation (see Subsection 3.2), and the result-\ning grids represent our radiance field. During training, the\naddition of new components is simulated using masks for\nefficiency, and during testing, components can be directly\ndiscarded using truncation in order to slim the model (see\nSubsection 3.3), allowing dynamic test-time trade-offs be-\ntween memory-efficiency and accuracy. We also provide\na theoretical explanation shedding light on the method\u2019s\nmechanism, which is elaborated in Appendix D.\n3.1. Formulation and Representation\nThe radiance field has two outputs for a 3D location x,\nnamely density \u03c3 and appearance c, which can be repre-\nsented as:\n\u03c3, c = G\u03c3(x), S(Gc(x), d)\n(1)\nwhere S is a preselected function (for which we use an\nMLP) and d represents the viewing direction.\nThis in-\nvolves the tensors G\u03c3 \u2208 RI\u00d7J\u00d7K (the geometry tensor)\nand Gc \u2208 RI\u00d7J\u00d7K\u00d73 (the appearance tensor), which we\ndecompose, following the framework of [4]. Using the VM\n(Vector-Matrix) Decomposition, we define the following as\ncomponents of G\u03c3 and Gc:\nG(r)\n\u03c3\n= vX\n\u03c3,r \u25e6 MY Z\n\u03c3,r + vY\n\u03c3,r \u25e6 MXZ\n\u03c3,r + vZ\n\u03c3,r \u25e6 MXY\n\u03c3,r\n(2)\nG(r)\nc\n= vX\nc,r \u25e6 MY Z\nc,r \u25e6 b3r\u22122 + vY\nc,r \u25e6 MXZ\nc,r \u25e6 b3r\u22121 (3)\n+ vZ\nc,r \u25e6 MXY\nc,r \u25e6 b3r\n3\nThe above decomposes the tensors into vectors v, ma-\ntrices M, and a matrix B. Each tensor component itself\nhas a fixed set of related values in the factor matrices and\nvectors (corresponding to the rank r at which the component\nappears). The components are therefore independent of each\nother, and the resulting grids can be calculated as follows:\nG\u03c3, Gc =\nR\u03c3\nX\nr=1\nG(r)\n\u03c3 ,\nRc\nX\nr=1\nG(r)\nc\n(4)\nwhere Rc = cR\u03c3 for an appearance grid with c channels. In\nthe rest of this paper, unless otherwise stated, R refers to R\u03c3,\nand we use c = 3 (RGB channels).\nOur key observation is that different tensorial components\nare learned simultaneously in most existing models such\nas TensoRF. This does not make use of the independence\nbetween different components in the tensorial decomposed\nrepresentation, and thus in the resulting trained model, the\nimportance of different components are statistically equal.\nThis hinders slimmability since the removal or adjustment of\nany component would severely impact the model as a whole.\nRemark.\nA straightforward way to make use of compo-\nnent independence is to group the parameters into blocks and\noptimize one block of parameters at a time. This method,\nknown as Block Coordinate Descent (BCD), had been suc-\ncessfully applied to many other areas [36,70]. However, this\nmethod would not work as intended in our scenario. This is\nbecause our approximation of the tensors G\u03c3 and Gc relies on\nthe hypothesis that both of the tensors have low-rank struc-\ntures. If only one block of components is trained at once,\nthen the optimization objective for this block becomes learn-\ning the residual from other learned components. Residual\nlearning has been applied in other scenarios such as CNN\ncompression [14], but would impair model performance in\nour case as the residual is not guaranteed to be low-rank.\nInspired by BCD, we innovatively propose another\nmethod adapted to our specific task. The intuition behind our\napproach is that instead of imposing hard constraints on the\ncomponents trained in each iteration, we gradually increase\nthe learning power of the model and add new components\nto the model when needed. We call this approach Tensorial\nRank Incrementation, or TRaIn.\n3.2. Tensorial Rank Incrementation\nOur approach first limits the rank of the model, then\nincrements the rank progressively when needed by adding\nmore components. This is formally similar to the previously\nmentioned na\u00a8\u0131ve BCD approach, but fundamentally different\nin that we incrementally add components to the model and\ntrain them collectively instead of optimizing separate blocks\nof parameters individually. Hence the connection between\ndifferent components from the perspective of the model as a\nwhole is not lost in the training process.\nMore specifically, parameters are divided into groups\nbased on their relation to specific components, according to\nequations (2) and (3). Each time the model rank is incre-\nmented, another group of parameters is appended to the set\nof parameters being trained.\nPrinciples of Rank Incrementation.\nImplementation-\nwise, we use a dynamic rank rd to limit the model\u2019s learning\ncapacity, replacing the maximum rank R in (4). As for the\nincrementation of rd, an intuitive method would be to incre-\nment rd when the low-rank model converges, i.e. when the\nloss changes at a low rate. However, tests show that it is dif-\nficult to determine convergence of the model since the loss is\nfrequently unstable. Furthermore, waiting for the low-rank\nmodels to converge every time the dynamic rank changes\nwould cause the converge of the entire model to be slowed.\nHence, we counter-intuitively increment rd only when the\nloss changes at a high rate instead, representing the phase\nwhere the model is learning fast and thus is also more able\nto make use of the extra learning power from incrementing\nthe model dynamic rank.\nCriteria for Rank Incrementation.\nSpecifically, we use\na hyper-parameter \u03c5 to control this process. For the rank in-\ncrementation process, we use the following MSE loss LMSE,\ncalculated from marching along and sampling points on rays:\nLMSE = 1\nR\nR\nX\nr=1\n||c\u2217\nr \u2212\nN\nX\nn=1\n(t(n)\nr (1 \u2212 e\u2212\u03b4r\u03c3(n)\nr )c(n)\nr )||2\n2 (5)\nwhere N is the number of points sampled along each ray\nand R is the number of sampled rays per batch; c\u2217\nr is the\nground truth value of the pixel corresponding to the ray r; \u03b4r\nis the step size of ray r; \u03c3(n)\nr\nand c(n)\nr\nrespectively represent\nthe predicted density and appearance of the ray r at point\nn; t(n)\nr\nrepresents the transmittance of the ray r at point n,\ncalculated by t(n)\nr\n= exp(\u2212\nn\u22121\nP\nm=1\n\u03b4r\u03c3(m)\nr\n).\nLet L(i)\nMSE be the mean of MSE losses calculated from (5)\nacross all views on iteration i. We increment rd when the\nfollowing inequality holds after training iteration i:\n|L(i\u22121)\nMSE \u2212 L(i)\nMSE|\nL(i)\nMSE\n> \u03c5\n(6)\nIn experimentation with sparse inputs it was observed\nthat sometimes the loss changes dramatically during the\nfirst iterations and causes the first few components to be\nlearnt simultaneously. To prevent this, an additional optional\nconstraint is added: that rank incrementations can only take\nplace after a fixed amount \u03b7 of iterations has passed from the\nprevious rank incrementation. However, in non-sparse input\nscenarios the performance often declines after introducing\nthe \u03b7-constraint, and hence in those scenarios we set \u03b7 = 0.\n4\nR=1\nR=2\nR=4\nR=6\nR=8\nR=16\nGT\nFigure 4. Automatic \u201cdivision of labor\u201d between components. As demonstrated by the testing results above (on Family from Tanks &\nTemples and Lego from NeRF Synthetic), the lower-rank components of SlimmeRF prioritize learning general information regarding the\nscene such as structure and main color, and more delicate tasks such as texture patterns are handled by higher-rank components. Hence most\nof the important information are retained during model slimming, where components containing details are discarded, leaving fundamental\nknowledge regarding the scene intact.\nAblation studies and parameter sensitivity analyses of \u03c5 and\n\u03b7 could be found in Table 3.\nIntuition and Theory.\nThe intuition behind our method\nis that when the model has low learning capacity, the com-\nponents which are trained first will automatically prioritize\nlearning essential information such as structure and main\ncolors. Theoretical insights relating to this intuition are pro-\nvided in Appendix D.\nAs verified empirically in Figure 4, this allows an auto-\nmatic \u201cdivision of labor\u201d to form in between the components.\nComponents corresponding to lower ranks hold on to the\nvital information they acquired during training with low rd,\nand components corresponding to higher ranks thus store\nmore detailed information.\n3.3. Train-Time Masking and Test-Time Truncation\nTraining Stage.\nChanging the size of the parameter ten-\nsors during training time is a costly operation since all related\ndata need to be renewed. Hence, we use masks to simulate\ntruncation during training. Specifically, we replace the origi-\nnal parameters as follows:\nR\u2295\nr=1 v\u03b1\n\u03c3,r,\n3R\n\u2295\nr=1 v\u03b1\nc,r = V\u03b1\u2217\n\u03c3 \u2297 V\u03b1\u00b5\n\u03c3 , V\u03b1\u2217\nc\n\u2297 V\u03b1\u00b5\nc\n(7)\nR\u2295\nr=1 M\u03b1\n\u03c3,r,\n3R\n\u2295\nr=1 M\u03b1\nc,r = M\u03b1\u2217\n\u03c3 \u2297 M\u03b1\u00b5\n\u03c3 , M\u03b1\u2217\nc\n\u2297 M\u03b1\u00b5\nc\n(8)\nwhere \u2295 is the stacking/concatenation operator and \u2297 is the\nHadamard product; in (7), \u03b1 \u2208 {X, Y, Z} and in (8), \u03b1 \u2208\n{XY, Y Z, XZ}. The matrix/tensor with the superscript \u00b5\nis the mask, which we directly control in the algorithm, and\nthe matrix/tensor with the superscript \u2217 is learnable.\nThe mask controls the rank incrementation process. Sup-\nposing that the function 1(T ) produces a tensor of the same\ndimensions as T but contains exclusively the element 1, and\nthat \u03f5(T ) produces one that contains exclusively the element\n\u03f5, the masks are calculated as:\nV\u03b1\u00b5\n\u03c3\n= (\nrd\u2295\nr=1 1(v\u03b1\n\u03c3,r)) \u2295 (\nR\u2295\nr=rd+1 \u03f5(v\u03b1\n\u03c3,r))\n(9)\nV\u03b1\u00b5\nc\n= (\n3rd\n\u2295\nr=1 1(v\u03b1\nc,r)) \u2295 (\n3R\n\u2295\nr=3rd+1 \u03f5(v\u03b1\nc,r))\n(10)\nM\u03b1\u00b5\n\u03c3\n= (\nrd\u2295\nr=1 1(M\u03b1\n\u03c3,r)) \u2295 (\nR\u2295\nr=rd+1 \u03f5(M\u03b1\n\u03c3,r))\n(11)\nM\u03b1\u00b5\nc\n= (\n3rd\n\u2295\nr=1 1(M\u03b1\nc,r)) \u2295 (\n3R\n\u2295\nr=3rd+1 \u03f5(M\u03b1\nc,r))\n(12)\nwhere \u03f5 is a small positive value. Note that here we use the \u03f5\ntensor instead of the zero tensor because otherwise, values\nof 0 in the masked tensor would cause gradient descent to\nfail on masked elements of the trainable tensors (similar to\ndeath of ReLU neurons in deep neural networks). Eventually,\nAlgorithm 1: The TRaIn Algorithm\n1 Initialize model and parameters v, M, B;\n2 Initialize masks according to (9), (10), (11), (12);\n3 Initialize rd;\n4 last inc \u2190 1;\n5 for it \u2190 1 to maxiter do\n6\nCalculate v and M based on (7) and (8);\n7\nCalculate gradients with respect to v and M;\n8\nOptimize v\u2217 and M\u2217 based on gradients;\n9\nOptimize the rest of the model normally;\n10\nif it \u2212 last inc > \u03b7 then\n11\nCalculate LMSE from (5);\n12\nif (6) is satisfied then\n13\nIncrement rd;\n14\nlast inc \u2190 rd;\n15\nUpdate V\u00b5 and M\u00b5;\n16\nend\n17\nend\n18 end\n5\nNeRF Synthetic\nSize (MB)\nChair\nDrums\nFicus\nHotdog\nLego\nMaterials\nMic\nShip\nAvg.\nPlenoxels [66]\n783\n33.97\n25.35\n31.83\n36.43\n34.09\n29.14\n33.27\n29.61\n31.71\nDVGO [58]\n206\n34.06\n25.40\n32.59\n36.77\n34.65\n29.58\n33.18\n29.04\n31.91\nTensoRF-192 [4]\n71.9\n35.80\n26.01\n34.11\n37.57\n36.54\n30.09\n34.97\n30.72\n33.23\nSlimmeRF-16 (Ours)\n48.3\n35.74\n25.80\n34.03\n37.25\n36.45\n30.04\n35.06\n30.58\n33.12\nTanks & Temples\nBarn\nCaterpillar\nFamily\nIgnatius\nTruck\nAvg.\nPlenoxels [66]\n26.07\n24.64\n32.33\n27.51\n26.59\n27.43\nDVGO [58]\n27.01\n26.00\n33.75\n28.16\n27.15\n28.41\nTensoRF-192 [4]\n27.22\n26.19\n33.92\n28.34\n27.14\n28.56\nSlimmeRF-16 (Ours)\n27.18\n26.00\n34.03\n28.46\n26.94\n28.52\nTable 1. Comparisons with state-of-the-arts. As shown, our method is very memory-efficient even without slimming, and successfully\ndisplays state-of-the-art level accuracy. All values in the table except the \u201cSize\u201d column are all PSNR values in dB.\nFigure 5. Comparisons with baselines. A \u201cSlimmeRF compo-\nnent\u201d corresponds to 12 TensoRF components since a set of tenso-\nrial components G(r)\n\u03c3 , G(3r)\nc\n, G(3r\u22121)\nc\n, G(3r\u22122)\nc\nfrom SlimmeRF is\nequivalent to 12 matrix/vector components from TensoRF. Results\nobtained from the NeRF Synthetic dataset.\nV\u2217 and M\u2217 are updated based on gradients of the loss with\nrespect to (\u2295v) and (\u2295M).\nHence, we arrive at the following representation for our\nradiance fields during training:\n\u03c3, c = [(\nrd\nX\nr=1\nG(r)\n\u03c3 ) + \u03f52(\nR\nX\nr=rd+1\nG(r)\n\u03c3 )](x),\n(13)\nS([(\n3rd\nX\nr=1\nG(r)\n\u03c3 ) + \u03f52(\n3R\nX\nr=3rd+1\nG(r)\n\u03c3 )](x), d)\nwith rd = R after the training is complete, hence again\naligning with structures of (1) and (4).\nOther details in training are similar to TensoRF. We ap-\nply a coarse-to-fine training by shrinking the bounding box\nduring training. The grids are also upsampled during fixed it-\nerations via trilinear interpolation. Note that to save memory,\nfloat values are converted to 16-bit precision when the model\nis being saved and back to 32-bit precision afterwards.\nThe formalized version of the TRaIn Algorithm is shown\nin Algorithm 1.\nTesting Stage.\nAfter the training process, however, mask-\ning becomes meaningless. Hence we truncate the component\ntensors during testing and save model in its slimmed form so\nthat the model size could be reduced in cases where memory\nFigure 6. Comparison with compressive and memory-efficient\nmethods. We compare our model with other compressive and\nmemory-efficient methods. As shown, our model could achieve\nhigh PSNR values during testing that are unobtainable via most\nother memory-efficient models. Also note that none of the models\nexcept CCNeRF support test-time trade-offs between size and\naccuracy, and thus all of those models are shown for reference\nonly rather than as competitors. Results obtained from the NeRF\nSynthetic dataset.\nor efficiency constraints are important. Alternatively, the\nmodel could be saved in its non-slimmed form and slimmed\nbefore rendering according to applicational needs.\n4. Experiments\nIn experiments, we use \u201cSlimmeRF-R\u201d to refer to a model\nwith rank R. More detailed per-scene testing results are\navailable in the Appendices.\nExperimental Details.\nOur implementation is partially\nbased on the codebase of TensoRF [4], uses the package\nPyTorch [43] for common modules, and applies the Adam\noptimizer [23] for optimization of the parameters. Experi-\nments with NeRF Synthetic were conducted on a GeForce\nRTX 3090 GPU (24 GB), while experiments with LLFF and\nTanks & Temples were conducted on a Tesla V100 SXM2\nGPU (32 GB). Regarding more detailed information such as\nhyper-parameter settings, please refer to Appendix A.\nComparison with TensoRF Baselines.\nThough our model\nstructure is similar to TensoRF in terms of data representa-\ntion, our theoretical premises are completely different from\nthose of TensoRF\u2019s (low-rank tensor approximation versus\ntensor decomposition-based representation). Since we have\n6\nOurs (SlimmeRF-24)\nR=2\nR=4\nR=6\nR=8\nR=12\nR=16\nR=20\nR=24\n3 Views\n15.51\n16.63\n16.73\n16.76\n16.75\n16.74\n16.73\n16.72\n6 Views\n16.28\n18.21\n18.76\n18.97\n19.08\n19.09\n19.09\n19.08\n9 Views\n17.09\n19.66\n20.51\n20.97\n21.21\n21.29\n21.34\n21.35\nSpecialized Sparse Input Models (For Reference)\nSRF [7]\nPixelNeRF [67]\nMVSNeRF [5]\nmip-NeRF [1]\nDietNeRF [18]\nReg-NeRF [40]\n3 Views\n17.07\n16.17\n17.88\n14.62\n14.94\n19.08\n6 Views\n16.75\n17.03\n19.99\n20.87\n21.75\n23.10\n9 Views\n17.39\n18.92\n20.47\n24.26\n24.28\n24.86\nTable 2. Quantitative results of sparse-input tests. As shown, our model\u2019s slimmability is very high in sparse input cases, as the PSNR\nvalue barely decreases when the model is slimmed. Our method also performs comparably to specialized sparse-view models, despite not\nincorporating any specific priors for sparse-input scenarios. Note that while our model\u2019s performance does not excel state-of-the-arts, we\nmainly aim to test our model\u2019s slimmability rather than to compete with other models. All values in the table except the \u201cSize\u201d column are\nall PSNR values in dB.\nR=2\nR=4\nR=6\nR=8\nR=12\nR=16\nGT\nOurs\nBaseline\nFigure 7. Qualitative comparison with baseline. Shown is an\nexperiment with the NeRF Synthetic scene Chair. The model\nlabelled \u201cours\u201d is a SlimmeRF-16, while the baseline is based on\na TensoRF-VM-192. The value of \u201cR\u201d represents the number of\ncomponents left after slimming.\nalready elaborated on the theoretical differences, we here\ndemonstrate the differences between the two models in terms\nof empirical performance. We remove components from Ten-\nsoRF in a similar fashion to our \u201cslimming\u201d procedure, and\nproceed to normally calculate the PSNR values based on\nthe slimmed TensoRF model as the baseline. Quantitative\nresults of our experiment are shown in Figure 5, and quali-\ntative results are shown in Figure 7. As shown, our model\noutperforms the baseline by a large margin, in accordance\nwith theoretical analysis.\nComparison with State-of-the-Art Models.\nWe compare\nour model (without slimming) with the state-of-the-art mod-\nels TensoRF [4], Plenoxels [11], and DVGO [58] on the\nNeRF Synthetic dataset [38]. We note that while there ex-\nists models like mip-NeRF [1] which can outperform the\nlisted models, they take time on the magnitude of days to\ntrain, and so are not taken into our consideration. The results\nare shown in Table 1. As shown, our model reaches perfor-\nmance comparable to state-of-the-art methods, and achieves\nslimmability while the baselines fails to. Figure 8 also shows\na qualitative comparison of our results with TensoRF, which\nis representative of SOTA models.\nComparison with Compressive and Memory-Efficient\nModels.\nWe compare our model with the compressive and\nmemory-efficient models TensoRF [4], CCNeRF [59], MWR\n(Masked Wavelet Representation) [51], TinyNeRF [71], and\nPlenVDB [62]. Note that we did not include works like\nRe:NeRF [10] and VQRF [27] for comparison because they\nare post-training compression methods that can be used on\nOurs\nTensoRF\nGT\nBarn\nCaterpillar\nFamily\nIgnatius\nTruck\nFigure 8. Qualitative comparisons with TensoRF. The \u201cOurs\u201d\nmodel is a SlimmeRF-16, and the TensoRF used is a TensoRF-VM-\n192. The scenes shown here are all from Tanks & Temples [24]. As\nshown, our results are not inferior to TensoRF, and sometimes even\nexceed it.\nmodels, and hence cannot be considered as models them-\nselves. Note also that no works except CCNeRF support\ntest-time trade-offs between model size and accuracy, and\nthus are only included for reference. Results are shown in\nFigure 6.\nPerformance in Sparse-View Scenarios.\nOur model\u2019s\nstructure inherently stores data that do not conform to a low-\nrank structure into components of higher ranks. Hence, we\nhypothesized that this feature might be naturally beneficial\nto slimmability under sparse view reconstruction.\nTo verify this, we trained a SlimmeRF-24 model on the\nLLFF dataset [37]. We then compare our model (and its\nslimmed versions) with the conditional models SRF [7],\nPixelNeRF [67], and MVSNeRF [5], and the unconditional\nmodels mip-NeRF [1], DietNeRF [18], and Reg-NeRF [40],\nas references for the output quality of specialized sparse-\nview methods.\nWe pretrain the conditional models on the DTU\ndataset [20] and then fine-tune them on LLFF for fair com-\nparison (preventing overfitting during pretraining). Qualita-\ntive results from our model are shown in Figure 9, and our\nmodel\u2019s quantitative performance is shown in Table 2.\nWe observe that our model\u2019s slimmability was greatly in-\ncreased in sparse-view scenarios. Often a higher PSNR could\nbe reached via discarding components. This is presumably\ndue to the removal of floaters in components corresponding\nto higher ranks, and we include more empirical demonstra-\ntions of this as qualitative results in Appendix C. While our\nmethod did not achieve excelling results overall, we note that\n7\nOurs\nRegNeRF\n25.71 / 0.834\n27.75 / 0.887\n26.31 / 0.856\n25.67 / 0.833\n30.84 / 0.890\n29.73 / 0.857\n25.06 / 0.889\n23.52 / 0.812\n22.39 / 0.845\n20.00 / 0.744\n22.08 / 0.794\n19.90 / 0.708\n34.08 / 0.959\n32.99 / 0.957\n27.92 / 0.916\n26.10 / 0.907\nPSNR / SSIM\nFigure 9. Qualitative comparison with Reg-NeRF. Reg-NeRF [40] is a recent state-of-the-art model which performs well across many\ncircumstances (also refer to Table 2 for quantitative information). Here we show some qualitative results of tests on LLFF (9 views) for both\nReg-NeRF and our model (SlimmeRF-24). As displayed, in 2D synthesis tasks from certain viewing directions the performance of our\nmodel excels that of Reg-NeRF by a large margin. Because we do not attempt to hallucinate geometrical structures using prior assumptions,\nwe did not achieve excelling results consistently across all viewing directions, but as this figure verifies, when enough knowledge associated\nwith the specific viewing direction could be obtained, our performance could be superior to that of specialized sparse-view models.\n\u03c5 (TRaIn) / Chair (NeRF Synthetic)\n\u03b7 (\u03b7-constraint) / Room (LLFF, 9 views)\nw/o\n0.1\n0.2\n0.3\n0.4\n0.5\nw/o\n50\n100\n200\n400\n600\nR=4\n16.39\n16.97\n22.51\n24.03\n30.11\nN/A*\n11.70\n23.60\n23.21\n23.59\n23.90\n24.00\nR=8\n19.89\n20.11\n30.25\n32.37\n35.28\n15.85\n24.72\n24.74\n24.73\n24.98\n25.01\nR=12\n25.27\n26.39\n33.82\n34.74\n35.29\n22.57\n25.18\n25.36\n25.42\n25.38\n25.08\nR=16\n35.80\n35.81\n35.82\n35.82\n35.34\n24.39\n25.54\n25.64\n25.86\n25.41\n25.06\nR=20\n25.08\n25.75\n25.77\n25.97\n25.41\n25.01\nR=24\n25.30\n25.82\n25.86\n25.98\n25.40\n25.06\nTable 3. Ablation Studies and Parameter Sensitivity Analysis The \u03c5 and \u03b7 parameters are subject to parameter sensitivity analysis. In\naddition, the cases where \u03c5 = 0 and \u03b7 = 0 can respectively be viewed as ablations of the TRaIn algorithm and of the \u03b7-constraint for sparse\ninputs. The experiments were respectively carried on Chair from NeRF Synthetic with SlimmeRF-16 and Room with 9 views from LLFF\nwith SlimmeRF-24.\n* The test with \u03c5 = 0.5 failed because (6) was never satisfied, and the rank was not incremented.\nthe performance of our model is superior to even specialized\nsparse-view models from some viewing directions, as shown\nby Figure 9.\nAblation Studies and Parameter Sensitivity Analysis.\nWe conduct ablation studies and parameter sensitivity analy-\nsis on the hyper-parameters \u03c5 and \u03b7. Results are displayed\nin Table 3. As shown, when \u03c5 is small, the accuracy of the\nnon-slimmed model will increase, at the expense of slimma-\nbility; slimmability increases with a large \u03c5 but might cause\nreduced model accuracy or even cause training to fail.\nAs for \u03b7, in sparse-view scenarios, a small \u03b7 would pro-\nduce both low-quality and non-slimmable results, similar to\nthose of the TensoRF baseline. When \u03b7 is below the optimal\nvalue, both the quality and the slimmability of our model\nincrease as \u03b7 increases, but when \u03b7 surpasses the optimal\nvalue the model\u2019s learning capability is severely restricted,\nand hence the quality of our model will decrease as \u03b7 in-\ncreases while slimmability is not majorly affected.\nComparison with BCD Baseline.\nA baseline based on\nthe na\u00a8\u0131ve BCD method mentioned by Subsection 3.1 was\nimplemented and tested during our preliminary tests. Dur-\ning testing, the model\u2019s accuracy dramatically fluctuated in\nbetween the training of individual blocks. Hence, we failed\nto obtain any steady and reportable results from the baseline,\nreflecting the necessity to apply Tensorial Rank Incremen-\ntation instead of the intuitive approach of simply applying\nBCD to train the components separately in blocks.\n5. Conclusion\nIn this work, we propose the new model SlimmeRF which\nallows for flexible and effective trade-offs between model ac-\ncuracy and model size. We formalize this research objective\nas slimmability in NeRF, and base our model on the newly\nintroduced Tensorial Rank Incrementation algorithm to con-\nstructively show how this could be achieved. We also use\nexperiments in sparse-input scenarios to demonstrate Slim-\nmeRF\u2019s inherent property of storing essential and erroneous\ninformation (such as floaters) separately.\nAcknowledgements\nThis work is partially supported by AIR, Tsinghua Uni-\nversity. The authors would like to thank the three anonymous\nreviewers for their recognition and suggestions. They would\nalso like to thank (in alphabetical order) Huan\u2019ang Gao, Liyi\nLuo, Yiyi Liao, and Zirui Wu for pertinent discussions and\nhelpful feedback.\n8\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neural\nradiance fields. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 5855\u20135864, 2021. 3,\n7\n[2] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 2\n[3] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-gan: Periodic implicit generative\nadversarial networks for 3d-aware image synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5799\u20135809, 2021. 2\n[4] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao\nSu. Tensorf: Tensorial radiance fields. In Proceedings of the\nEuropean Conference on Computer Vision, pages 333\u2013350.\nSpringer, 2022. 1, 2, 3, 6, 7\n[5] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast general-\nizable radiance field reconstruction from multi-view stereo.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14124\u201314133, 2021. 3, 7\n[6] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon raster-\nization pipeline for efficient neural field rendering on mobile\narchitectures. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16569\u2013\n16578, 2023. 2\n[7] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard\nPons-Moll. Stereo radiance fields (srf): Learning view syn-\nthesis for sparse views of novel scenes. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7911\u20137920, 2021. 3, 7\n[8] Shin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and\nSimon Lucey. Gaussian activated neural radiance fields for\nhigh fidelity reconstruction and pose estimation. In European\nConference on Computer Vision, pages 264\u2013280. Springer,\n2022. 2\n[9] Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno-\nNoguer, Chris Sweeney, Richard Newcombe, and Lingni\nMa. Lisa: Learning implicit shape and appearance of hands.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 20533\u201320543, 2022. 2\n[10] Chenxi Lola Deng and Enzo Tartaglione. Compressing ex-\nplicit voxel grid representations: fast nerfs become also small.\nIn Proceedings of the IEEE/CVF Winter Conference on Ap-\nplications of Computer Vision, pages 1236\u20131245, 2023. 3,\n7\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5501\u20135510, 2022. 7\n[12] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 14346\u201314355,\n2021. 2\n[13] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt.\nStylenerf:\nA style-based 3d-aware generator for high-\nresolution image synthesis. arXiv preprint arXiv:2110.08985,\n2021. 2\n[14] Yiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen. Net-\nwork sketching: Exploiting binary structure in deep cnns. In\nProceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 5955\u20135963, 2017. 4\n[15] Kang Han and Wei Xiang. Multiscale tensor decomposition\nand rendering equation encoding for view synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4232\u20134241, 2023. 2\n[16] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 5875\u20135884, 2021. 2\n[17] Liang Hou, Zehuan Yuan, Lei Huang, Huawei Shen, Xueqi\nCheng, and Changhu Wang. Slimmable generative adversarial\nnetworks. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 7746\u20137753, 2021. 1\n[18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthesis.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5885\u20135894, 2021. 3, 7\n[19] Hankyu Jang and Daeyoung Kim.\nD-tensorf:\nTenso-\nrial radiance fields for dynamic scenes.\narXiv preprint\narXiv:2212.02375, 2022. 2\n[20] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis evalua-\ntion. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 406\u2013413, 2014. 7\n[21] Yoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima\nAnandkumar, Minsu Cho, and Jaesik Park. Self-calibrating\nneural radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5846\u20135854,\n2021. 2\n[22] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,\nand Anurag Ranjan. Neuman: Neural human radiance field\nfrom a single video. In European Conference on Computer\nVision, pages 402\u2013418. Springer, 2022. 2\n[23] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 6\n[24] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4):1\u201313,\n2017. 7\n[25] Chaojian Li, Sixu Li, Yang Zhao, Wenbo Zhu, and Yingyan\nLin. Rt-nerf: Real-time on-device neural radiance fields to-\nwards immersive ar/vr rendering. In Proceedings of the 41st\n9\nIEEE/ACM International Conference on Computer-Aided De-\nsign, pages 1\u20139, 2022. 2\n[26] Changlin Li, Guangrun Wang, Bing Wang, Xiaodan Liang,\nZhihui Li, and Xiaojun Chang. Dynamic slimmable network.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8607\u20138617, 2021. 1\n[27] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and Liefeng\nBo. Compressing volumetric radiance fields to 1 mb. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 4222\u20134231, 2023. 3, 7\n[28] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollh\u00a8ofer, J\u00a8urgen\nGall, Angjoo Kanazawa, and Christoph Lassner.\nTava:\nTemplate-free animatable volumetric actors. In European\nConference on Computer Vision, pages 419\u2013436. Springer,\n2022. 2\n[29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-\nto-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2\n[30] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon\nLucey. Barf: Bundle-adjusting neural radiance fields. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 5741\u20135751, 2021. 2\n[31] David B Lindell, Julien NP Martel, and Gordon Wetzstein.\nAutoint: Automatic integration for fast neural volume render-\ning. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14556\u201314565,\n2021. 2\n[32] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. Advances\nin Neural Information Processing Systems, 33:15651\u201315663,\n2020. 2\n[33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. arXiv preprint arXiv:2303.11328,\n2023. 2\n[34] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8446\u20138455, 2023. 2\n[35] Quan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su,\nLan Xu, Xuming He, and Jingyi Yu. Gnerf: Gan-based neural\nradiance field without posed camera. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6351\u20136361, 2021. 2\n[36] Ofer Meshi and Alexander Schwing. Asynchronous parallel\ncoordinate minimization for map inference. Advances in\nNeural Information Processing Systems, 30, 2017. 4\n[37] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view synthe-\nsis with prescriptive sampling guidelines. ACM Transactions\non Graphics, 38(4):1\u201314, 2019. 7\n[38] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthe-\nsis. Communications of the ACM, 65(1):99\u2013106, 2021. 1, 2,\n7\n[39] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics,\n41(4):1\u201315, 2022. 1, 2\n[40] Michael Niemeyer, Jonathan T Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nnerf: Regularizing neural radiance fields for view synthesis\nfrom sparse inputs. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5480\u20135490, 2022. 3, 7, 8\n[41] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 11453\u201311464, 2021. 2\n[42] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 2\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An\nimperative style, high-performance deep learning library. Ad-\nvances in Neural Information Processing Systems, 32, 2019.\n6\n[44] Jiangtao Peng, Weiwei Sun, Heng-Chao Li, Wei Li, Xi-\nangchao Meng, Chiru Ge, and Qian Du. Low-rank and sparse\nrepresentation for hyperspectral image processing: A review.\nIEEE Geoscience and Remote Sensing Magazine, 10(1):10\u2013\n43, 2021. 2\n[45] Sida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang,\nQing Shuai, Xiaowei Zhou, and Hujun Bao. Animatable\nneural radiance fields for modeling dynamic human bodies.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 14314\u201314323, 2021. 2\n[46] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9054\u20139063, 2021. 2\n[47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall.\nDreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022. 2\n[48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Ali-\naksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Skorokhodov,\nPeter Wonka, Sergey Tulyakov, et al. Magic123: One image\nto high-quality 3d object generation using both 2d and 3d\ndiffusion priors. arXiv preprint arXiv:2306.17843, 2023. 2\n[49] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14335\u2013\n14345, 2021. 2\n10\n[50] Christian Reiser, Richard Szeliski, Dor Verbin, Pratul P Srini-\nvasan, Ben Mildenhall, Andreas Geiger, Jonathan T Barron,\nand Peter Hedman. Merf: Memory-efficient radiance fields\nfor real-time view synthesis in unbounded scenes. arXiv\npreprint arXiv:2302.12249, 2023. 3\n[51] Daniel Rho, Byeonghyeon Lee, Seungtae Nam, Joo Chan\nLee, Jong Hwan Ko, and Eunbyung Park. Masked wavelet\nrepresentation for compact neural radiance fields. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 20680\u201320690, 2023. 3, 7\n[52] A Rosinol, JJ Leonard, and L Carlone. Nerf-slam: Real-time\ndense monocular slam with neural radiance fields. arXiv\npreprint arXiv:2210.13641, 2022. 2\n[53] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. Graf: Generative radiance fields for 3d-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 33:20154\u201320166, 2020. 2\n[54] Ruizhi Shao, Hongwen Zhang, He Zhang, Mingjia Chen, Yan-\nPei Cao, Tao Yu, and Yebin Liu. Doublefield: Bridging the\nneural surface and radiance fields for high-fidelity human re-\nconstruction and rendering. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15872\u201315882, 2022. 2\n[55] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hong-\nwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d de-\ncomposition for high-fidelity dynamic reconstruction and ren-\ndering. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 16632\u201316642,\n2023. 2\n[56] Qingquan Song, Hancheng Ge, James Caverlee, and Xia Hu.\nTensor completion algorithms in big data analytics. ACM\nTransactions on Knowledge Discovery from Data (TKDD),\n13(1):1\u201348, 2019. 2\n[57] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davison.\nimap: Implicit mapping and positioning in real-time.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 6229\u20136238, 2021. 2\n[58] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5459\u2013\n5469, 2022. 6, 7\n[59] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang Zeng.\nCompressible-composable nerf via rank-residual decompo-\nsition. Advances in Neural Information Processing Systems,\n35:14798\u201314809, 2022. 2, 3, 7\n[60] Zirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Vic-\ntor Adrian Prisacariu. Nerf\u2013: Neural radiance fields without\nknown camera parameters. arXiv preprint arXiv:2102.07064,\n2021. 2\n[61] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng\nChen, Hongmin Xiao, Chao Hou, Haozhe Lou, Yuantao Chen,\nRunyi Yang, Yuxin Huang, Xiaoyu Ye, Zike Yan, Yongliang\nShi, Yiyi Liao, and Hao Zhao. Mars: An instance-aware, mod-\nular and realistic simulator for autonomous driving. arXiv\npreprint arXiv:2307.15058, 2023. 2\n[62] Han Yan, Celong Liu, Chao Ma, and Xing Mei. Plenvdb:\nMemory efficient vdb-based radiance fields for fast training\nand rendering. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 88\u201396,\n2023. 2, 3, 7\n[63] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,\nPratul P Srinivasan, Richard Szeliski, Jonathan T Barron,\nand Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-\ntime view synthesis. arXiv preprint arXiv:2302.14859, 2023.\n2\n[64] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Inverting\nneural radiance fields for pose estimation. In Proceedings of\nthe IEEE/RSJ International Conference on Intelligent Robots\nand Systems, pages 1323\u20131330. IEEE, 2021. 2\n[65] Tatsuya Yokota, Cesar F Caiafa, and Qibin Zhao. Tensor\nmethods for low-level vision. In Tensors for Data Processing,\npages 371\u2013425. Elsevier, 2022. 2\n[66] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\nneural radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5752\u20135761,\n2021. 2, 6\n[67] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4578\u20134587, 2021. 2,\n3, 7\n[68] Jiahui Yu and Thomas S Huang. Universally slimmable net-\nworks and improved training techniques. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 1803\u20131811, 2019. 1\n[69] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang\nZhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently gener-\nated human radiance field from sparse inputs. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7743\u20137753, 2022. 2\n[70] Hao Zhao, Ming Lu, Anbang Yao, Yiwen Guo, Yurong Chen,\nand Li Zhang. Physics inspired optimization on semantic\ntransfer features: An alternative method for room layout esti-\nmation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 10\u201318, 2017. 4\n[71] Tianli Zhao, Jiayuan Chen, Cong Leng, and Jian Cheng.\nTinynerf: Towards 100 x compression of voxel radiance fields.\nIn Proceedings of the Annual AAAI Conference on Artificial\nIntelligence, volume 37, pages 3588\u20133596, 2023. 3, 7\n[72] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields\nfor human avatar modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15893\u201315903, 2022. 2\n[73] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hujun\nBao, Zhaopeng Cui, Martin R Oswald, and Marc Pollefeys.\nNice-slam: Neural implicit scalable encoding for slam. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 12786\u201312796, 2022. 2\n11\nSlimmeRF: Slimmable Radiance Fields\nAppendices\nShiran Yuan1,2,3,* Hao Zhao1,\u2020\n1AIR, Tsinghua University\n2Duke University\n3Duke Kunshan University\nsy250@duke.edu, zhaohao@air.tsinghua.edu.cn\nThe appendices shown here provide additional details\nfor our implementation, testing results, and display more\nhigh-resolution qualitative comparisons. We also theoret-\nically demonstrate the source of our model\u2019s slimmabil-\nity. Please refer to https://github.com/Shiran-\nYuan/SlimmeRF for our open-source repository.\nA. Additional Implementation Details\nAll experiments with NeRF Synthetic [A4] except those\non Ficus used the hyper-parameter settings max iter =\n30000, \u03b7 = 0, and \u03c5 = 0.4 (experiments on Ficus used\n\u03c5 = 0.2 instead); all experiments with Tanks & Tem-\nples [A2] used the settings max iter = 30000, \u03b7 = 0, and\n\u03c5 = 0.2; all experiments with LLFF [A3] used the settings\nmax iter = 30000, \u03b7 = 100, and \u03c5 = 0.1.\nThe MLP used for S in (13) includes one hidden layer\nof size 128. The input and hidden layers use ReLU activa-\ntion, and the output layer uses Sigmoid activation. We also\nuse a coarse-to-fine training paradigm, with gradual grid up-\nsampling and a bounding-box shrinking strategy. The grids\nare upsampled on iterations 2000, 3000, 4000, and 5500\nfor LLFF, and at iteration 7000 too for Synthetic NeRF and\nTanks & Temples. Bounding boxes are shrinked on iterations\n2000 and 4000 for Synthetic NeRF and Tanks & Temples,\nand on iteration 2500 for LLFF.\nSome more specific details (such as adjustment terms\nto loss functions) are borrowed from the codebase of Ten-\nsoRF [A1], and we redirect the reader to their work or our\nopen-source codebase for information regarding those.\nB. Additional Testing Results\nB.1. Comparison with TensoRF Baselines\nWe provide specific per-scene quantitative results for both\nSlimmeRF and the corresponding TensoRF baselines (with\nthe equivalent number of tensorial components) across all\n8 scenes from Synthetic NeRF and all 5 scenes from Tanks\n*Research done during internship with AIR.\n\u2020Corresponding author.\n& Temples. As shown, in all scenes our results very signifi-\ncantly exceed those of the baselines\u2019.\nThe topmost row displays the number of remaining com-\nponents after slimming. The leftmost column is the total\nnumber of components R in SlimmeRF, corresponding to a\nbaseline of TensoRF-VM-12R (as explained in the caption\nof Figure 5).\nOur results for Synthetic NeRF are displayed in Tables 5-\n13. Our results for Tanks & Temples are displayed in Ta-\nble 14.\nB.2. Sparse Input Experiments\nWe provide specific per-scene quantitative results for\nSlimmeRF-24\u2019s performance in LLFF with 3, 6, and 9 views\nrespectively. Our results show that SlimmeRF achieves high\nslimmability in sparse-input cases. The results are displayed\nin Tables 15-17, and Table 18 shows the averages.\nC. Additional Qualitative Results\nWe display high-resolution versions of our results\nachieved on LLFF (9 views), Synthetic NeRF, and Tanks\n& Temples. Each group of selected results are displayed as\n7 separate images, with the single image on the rightmost\ncolumn being the Ground Truth. Each group of images is ar-\nranged as in Table 4, and in the captions images are referred\nto according to the corresponding letters shown in the table.\nThe results are shown in Figures 10-15.\nD. Theoretical Mechanism\nIn this section we mathematically demonstrate the mech-\nanism behind the slimmability of our model. Specifically,\na\nb\nc\nGT\nd\ne\nf\nTable 4. The arrangement of images in each group. The six images\n(as in the captions of Figures 10-15) are labelled with letters a -\nf for indication of position.\n12\narXiv:2312.10034v1  [cs.CV]  15 Dec 2023\nwe present a theoretical basis for the TRaIn algorithm\u2019s\nsuperiority in terms of slimmability over conventional si-\nmultaneous training paradigms. Our deduction reveals a\ntheoretical upper bound on the partial derivative of the MSE\nloss with respect to elements of each tensorial component of\nthe appearance grid, and demonstrates that utilization of the\nTRaIn algorithm instead of simultaneous training loosens\nthis bound for tensorial components of lower rank. Hence\nthis allows for tensorial components of lower rank to be\ntrained faster under the TRaIn algorithm, thus achieving\nbetter slimmability.\nD.1. Preliminaries and Notation\nFirst, we rewrite (5) as follows for clarity:\nLMSE(G\u03c3, Gc) = 1\nQ\nQ\nX\nq=1\n(c\u2217\nq \u2212\nN\nX\nn=1\np(n)\nq\nc(n)\nq\n)2\n(14)\nwhere the original r, R are respectively replaced by q, Q to\navoid confusion with ranks; p(n)\nq\nis defined as follows:\np(n)\nq\n= e\n\u2212\nn\u22121\nP\nm=1\n\u03b4q\u03c3(m)\nq\n(1 \u2212 e\u2212\u03b4q\u03c3(n)\nq )\n(15)\nWe define the per-ray error functions \u03b8(q) as follows:\n\u03b8(q)(Gc) = (c\u2217\nq \u2212\nN\nX\nn=1\np(n)\nq\nc(n)\nq\n)2\n(16)\nWe also use \u2126 to represent the set of all indices i in the\ntensorial grid Gc.\nD.2. Linking Appearance and Components\nThe MLP-represented function S(m, d) modeling the\nappearance grid Gc satisfies the Lipschitz condition as a\nfunction of m, where m = Gc(x). Hence by definition there\nexists K > 0 s.t. the following holds for any values of m\nand \u03f5:\n|S(m, d) \u2212 S(m + \u03f5, d)| \u2264 K|\u03f5|\n(17)\nA trivial consequence is as follows for all m:\n\u2212K \u2264 \u2202S(m, d)\n\u2202m\n\u2264 K\n(18)\nIn addition, since Gc(x) is calculated via trilinear inter-\npolation, we can create nonnegative tensors Y(n)\nq\nwith the\nsame dimensions as Gc according to the following:\nX\ni\u2208\u2126\ny(n)\nqi\n=1\n(19)\nGc(x(n)\nq\n) =\nX\ni\u2208\u2126\n(y(n)\nqi g(r)\nci )\n(20)\nWe can then compute the partial derivative of the appear-\nance MLP output c with respect to the component elements\ng(r)\nci as follows:\n\u2202c\n\u2202g(r)\nci\n=\n\u2202c\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n\u2202gci\n\u2202gci\n\u2202g(r)\nci\n(21)\n=y(n)\nqi\n\u2202S(Gc(x), d)\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n=y(n)\nqi\n\u2202S( P\ni\u2208\u2126\ny(n)\nqi gci, d)\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\nHence we are able to deduce the following Lemma:\nLemma 1. The absolute value of the partial derivative of\nc with respect to g(r)\nci is theoretically upper bounded by the\nfollowing relation:\n| \u2202c\n\u2202g(r)\nci\n| \u2264 K\n(22)\nwhere K is the Lipschitz constant of the appearance MLP\nS(Gc(x), d) with respect to Gc(x).\nProof. Plugging (18) into (21), we have:\n\u2212Ky(n)\nqi\n\u2264\n\u2202c\n\u2202g(r)\nci\n\u2264 Ky(n)\nqi\nwhich can be combined with a trivial form of (19) to arrive\nat the Lemma.\nNote that it is also possible to find an upper bound to the\nLipschitz constant K such that this step of our proof could\nbe made constructive [A5].\nD.3. Upper Bound on Learning Per-Ray Errors\nDefine the function f as follows:\nf(g(r)\nci ) =\nN\nX\nn=1\n(p(n)\nq\nS(\nX\ni\u2208\u2126\ny(n)\nqi gci, d))\n(23)\nThe partial derivative of f with respect to g(r)\nci\ncan be\n13\ncalculated using the chain rule:\n\u2202f\n\u2202g(r)\nci\n=\nN\nX\nn=1\n(\n\u2202f\n\u2202S( P\ni\u2208\u2126\ny(n)\nqi gci, d)\n\u2202S( P\ni\u2208\u2126\ny(n)\nqi gci, d)\n\u2202g(r)\nci\n) (24)\n=\nN\nX\nn=1\n(p(n)\nq\n\u2202S( P\ni\u2208\u2126\ny(n)\nqi gci, d)\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n\u2202gci\n\u2202gci\n\u2202g(r)\nci\n)\n=\nN\nX\nn=1\n(p(n)\nq\ny(n)\nqi\n\u2202S( P\ni\u2208\u2126\ny(n)\nqi gci, d)\n\u2202 P\ni\u2208\u2126\ny(n)\nqi gci\n)\nand hence from Lemma 1 its absolute value |\n\u2202f\n\u2202g(r)\nci | is upper\nbounded as follows:\n| \u2202f\n\u2202g(r)\nci\n| \u2264 K\nN\nX\nn=1\np(n)\nq\n(25)\nWe can therefore arrive at the following Lemma regarding\nlearning per-ray errors \u03b8(q):\nLemma 2. The absolute value of the partial derivative of\n\u03b8(q) with respect to g(r)\nci is theoretically upper bounded by\nthe following relation:\n|\u2202\u03b8(q)\n\u2202g(r)\nci\n| \u2264 (2K\nN\nX\nn=1\np(n)\nq\n)\np\n\u03b8(q)\n(26)\nProof.\n|\u2202\u03b8(q)\n\u2202g(r)\nci\n| =2\np\n\u03b8(q)|\u2202\n\u221a\n\u03b8(q)\n\u2202g(r)\nci\n|\n(27)\n=2\np\n\u03b8(q)|\n\u2202\n\u2202g(r)\nci\n(c\u2217\nq \u2212 f)|\n=2\np\n\u03b8(q)| \u2202f\n\u2202g(r)\nci\n|\nPlugging in (23) we have the Lemma.\nD.4. Limitations on the MSE Loss Gradient\nWe define the following appearance grid Gc0 and loss\nvalue L0:\nDefinition 3. Gc0 is the (or \u201ca\u201d) solution to the MSE Loss\u2019s\noptimization problem across appearance grids of VM rank\nRc, and the MSE loss associated with it is designated L0.\nGc0 = arg min\nGc\nLMSE\nL0 = min\nGc LMSE\ns.t. rank(Gc) = Rc\nAs Gc0 is by definition optimal, most of its elements are\nclose to critical points of LMSE, and thus we define small\npositive values \u03f5(r)\nci (which can be treated as constants) such\nthat:\n|\u2202\u03b8(r)\n0\n\u2202g(r)\nc0i\n| \u2264 \u03f5(r)\nci\n(28)\nWe then take advantage of the properties of Gc0 by first\nupper bounding\n\u2202\u03b8\n\u2202g(r)\nci \u2212\n\u2202\u03b8\n\u2202g(r)\nc0i as a surrogate for\n\u2202\u03b8\n\u2202g(r)\nci\nusing\nLemma 2:\n|\u2202\u03b8(r)\n\u2202g(r)\nci\n\u2212 \u2202\u03b8(r)\n0\n\u2202g(r)\nc0i\n| \u2264 2K\nN\nX\nn=1\np(n)\nq\n|\nN\nX\nn=1\n(p(n)\nq\n(c(n)\nq\n\u2212 c(n)\nq0 ))|\n(29)\nTherefore we can prove the following Theorem:\nTheorem 4. The iteration step size of gradient descent-\nbased learning on the MSE Loss with respect to each com-\nponent element of the tensorial appearance grid, g(r)\nci , is\ntheoretically limited by the following upper bound on the\nabsolute value of the partial derivative:\n|\u2202LMSE\n\u2202g(r)\nci\n| \u2264 2K2N 2p2\nmax(\nX\ni\u2208\u2126\n|g(r)\nci \u2212 g(r)\nc0i|) + \u03f5(r)\nci\n(30)\nwhere pmax = max\nn,q p(n)\nq\n.\nProof. From (29) we have the following upper bound:\n|\u2202LMSE\n\u2202g(r)\nci\n| \u2264 2K\nN\nX\nn=1\np(n)\nq\n|\nN\nX\nn=1\n(p(n)\nq\n(c(n)\nq\n\u2212 c(n)\nq0 ))| + \u2202\u03b8(r)\n0\n\u2202g(r)\nc0i\nFrom (17) we trivially have the following:\n|c(n)\nq\n\u2212 c(n)\nq0 | \u2264 K\nX\ni\u2208\u2126\n|g(r)\nci \u2212 g(r)\nc0i|\nPlugging in, we have:\n|\u2202LMSE\n\u2202g(r)\nci\n| \u2264 2K2\nN\nX\nn=1\np(n)\nq\nN\nX\nn=1\n(p(n)\nq\nX\ni\u2208\u2126\n|g(n)\nc\n\u2212g(n)\nc0 |)+\u03f5(r)\nci\nWhen pmax = max\nn,q p(n)\nq , the above can be loosened and\nsimplified to:\n|\u2202LMSE\n\u2202g(r)\nci\n| \u2264 2K2N 2p2\nmax(\nX\ni\u2208\u2126\n|g(r)\nc\n\u2212 g(r)\nc0 |) + \u03f5(r)\nci\n14\nD.5. Interpretation and Significance\nTheorem 4 suggests that the upper bound of \u2202\u03b8(r)\n\u2202g(r)\nci (intu-\nitively the \u201clearning speed\u201d) is linearly positively correlated\nwith |g(r)\nc\n\u2212 g(r)\nc0 | (intuitively the \u201cdistance\u201d between com-\nponent elements and their \u201cideal\u201d values). We explore the\nimplications of this result by investigating two appearance\ngrid components G(r1)\nc\nand G(r2)\nc\n, where r1 < r2.\nIn our TRaIn algorithm, we initially control G(r2)\nc\nwhen\nG(r1)\nc\nis being trained to keep |g(r2)\nc\n\u2212 g(r2)\nc0 | constant. This\nallows for the upper bound from Theorem 4 to stay relatively\nloose. In contrast, most (possibly all) previous tensorial\nrepresentation-based NeRF paradigms train all components\nat once, which makes values of |g(r)\nc\n\u2212 g(r)\nc0 | for all r lower\nsimultaneously.\nHence, in our algorithm, we allow for G(r1)\nc\nto be trained\nat a speed which cannot be reached by previous paradigms\ndue to the hidden theoretical limit. This is at the expense of\nG(r2)\nc\nbeing trained slower later due to a higher |g(r1)\nc\n\u2212g(r1)\nc0 |.\nTherefore, we achieve slimmability by allowing G(r1)\nc\nto\ncapture more information than G(r2)\nc\n.\n15\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n15.95\n17.85\n20.81\n34.87\nSlimmeRF\n16.85\n29.51\n31.43\n34.83\n8\nBaseline\n14.67\n15.04\n17.43\n19.82\n21.60\n24.49\n27.52\n35.48\nSlimmeRF\n19.90\n27.87\n30.05\n32.32\n33.60\n35.12\n35.13\n35.14\n16\nBaseline\n14.86\n15.53\n16.12\n16.39\n17.32\n18.57\n18.96\n19.89\n20.90\n22.19\n24.20\n25.27\n26.63\n32.08\n33.46\n35.80\nSlimmeRF\n16.96\n23.26\n28.54\n30.11\n33.37\n35.27\n35.28\n35.28\n35.29\n35.29\n35.29\n35.29\n35.29\n35.29\n35.29\n35.34\n32\nBaseline\n14.05\n14.08\n14.42\n14.57\n14.79\n14.85\n15.25\n15.33\n16.19\n16.61\n18.54\n18.77\n19.18\n20.94\n21.99\n22.85\nSlimmeRF\n14.97\n17.48\n19.08\n21.28\n23.30\n27.22\n28.20\n28.96\n29.66\n30.05\n30.52\n30.97\n31.44\n31.84\n32.14\n32.43\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n23.89\n24.71\n25.06\n25.58\n26.11\n26.87\n28.51\n29.20\n29.58\n31.29\n32.27\n33.86\n34.77\n35.13\n35.45\n35.95\nSlimmeRF\n32.81\n33.07\n33.36\n33.61\n33.85\n34.10\n34.29\n34.58\n34.79\n35.01\n35.22\n35.39\n35.58\n35.75\n35.88\n36.02\nTable 5. Baseline comparison tests on Chair of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n11.67\n14.00\n16.57\n25.58\nSlimmeRF\n14.09\n22.34\n25.25\n25.40\n8\nBaseline\n11.45\n11.93\n13.11\n14.66\n17.39\n20.33\n23.49\n25.77\nSlimmeRF\n15.33\n21.86\n24.14\n24.89\n25.44\n25.55\n25.60\n25.63\n16\nBaseline\n11.00\n11.09\n11.26\n11.43\n12.05\n12.68\n13.60\n14.77\n15.93\n16.68\n18.32\n19.93\n22.66\n24.69\n25.37\n26.01\nSlimmeRF\n14.51\n21.19\n23.00\n23.93\n24.71\n25.05\n25.28\n25.45\n25.51\n25.55\n25.56\n25.57\n25.58\n25.58\n25.58\n25.58\n32\nBaseline\n10.96\n10.96\n11.30\n11.34\n11.43\n11.52\n11.59\n11.69\n11.86\n11.95\n12.49\n12.68\n13.01\n13.73\n14.43\n14.98\nSlimmeRF\n11.20\n14.20\n17.59\n18.76\n20.10\n21.03\n21.71\n22.21\n22.77\n23.14\n23.42\n23.74\n24.04\n24.31\n24.52\n24.72\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n15.48\n16.19\n16.89\n17.28\n17.86\n18.24\n19.59\n20.19\n20.68\n21.61\n22.49\n24.66\n25.57\n25.74\n25.87\n25.98\nSlimmeRF\n24.89\n25.00\n25.14\n25.27\n25.35\n25.44\n25.54\n25.64\n25.71\n25.76\n25.82\n25.86\n25.89\n25.93\n25.96\n25.98\nTable 6. Baseline comparison tests on Drums of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n15.80\n18.23\n22.73\n32.63\nSlimmeRF\n16.31\n20.15\n24.32\n32.63\n8\nBaseline\n14.34\n14.71\n15.68\n17.26\n19.44\n23.69\n28.25\n33.69\nSlimmeRF\n14.41\n16.32\n17.28\n18.89\n21.22\n24.31\n28.50\n33.74\n16\nBaseline\n14.24\n14.26\n14.36\n14.56\n14.93\n15.54\n16.51\n17.80\n19.50\n21.03\n22.67\n24.69\n27.18\n29.57\n31.85\n34.11\nSlimmeRF\n14.60\n16.65\n17.13\n17.76\n18.51\n19.40\n20.43\n21.60\n23.04\n24.46\n26.41\n28.14\n29.77\n31.48\n33.03\n34.03\n32\nBaseline\n14.24\n14.24\n14.27\n14.29\n14.35\n14.41\n14.47\n14.53\n14.61\n14.74\n15.44\n15.72\n15.94\n16.46\n17.10\n17.91\nSlimmeRF\n14.93\n15.89\n16.69\n17.21\n17.73\n18.37\n19.06\n20.05\n20.87\n21.81\n22.65\n23.57\n24.47\n25.10\n25.96\n26.72\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n18.75\n19.65\n20.29\n20.80\n21.43\n22.28\n23.03\n24.81\n26.15\n28.00\n29.29\n30.29\n32.07\n32.75\n33.60\n34.14\nSlimmeRF\n27.49\n28.25\n28.97\n29.70\n30.32\n30.82\n31.32\n31.82\n32.16\n32.57\n32.97\n33.34\n33.66\n33.83\n34.04\n34.17\nTable 7. Baseline comparison tests on Ficus of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n12.19\n15.13\n20.01\n36.81\nSlimmeRF\n14.87\n30.17\n31.87\n36.70\n8\nBaseline\n11.06\n12.52\n13.47\n14.66\n15.42\n24.43\n30.83\n37.22\nSlimmeRF\n15.55\n29.32\n32.83\n33.98\n34.95\n35.78\n36.40\n37.01\n16\nBaseline\n10.45\n10.47\n10.48\n10.62\n10.88\n11.06\n12.36\n13.74\n14.45\n15.13\n16.06\n16.93\n20.49\n26.02\n29.29\n37.57\nSlimmeRF\n15.89\n28.54\n31.66\n32.90\n33.62\n34.07\n35.03\n35.55\n35.91\n36.12\n36.43\n36.63\n36.80\n36.96\n37.10\n37.25\n32\nBaseline\n10.40\n10.58\n10.65\n10.66\n10.80\n11.06\n11.13\n11.26\n11.37\n11.51\n11.92\n12.01\n12.16\n12.28\n12.98\n17.61\nSlimmeRF\n14.59\n27.18\n30.56\n32.79\n34.00\n34.69\n35.04\n35.48\n35.74\n35.98\n36.18\n36.33\n36.46\n36.57\n36.66\n36.75\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n18.17\n18.39\n18.76\n18.94\n19.68\n22.23\n22.83\n24.48\n25.01\n28.26\n28.54\n32.70\n34.00\n34.31\n35.22\n37.66\nSlimmeRF\n36.85\n36.90\n36.98\n37.02\n37.07\n37.13\n37.18\n37.21\n37.25\n37.29\n37.33\n37.35\n37.38\n37.39\n37.42\n37.43\nTable 8. Baseline comparison tests on Hotdog of NeRF Synthetic. Numbers are in PSNR (dB).\n16\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n11.47\n14.12\n19.65\n35.47\nSlimmeRF\n13.15\n20.86\n25.35\n35.63\n8\nBaseline\n10.23\n11.16\n12.74\n15.68\n19.14\n23.78\n29.25\n36.18\nSlimmeRF\n14.77\n23.66\n26.64\n28.66\n32.39\n33.61\n34.95\n36.11\n16\nBaseline\n9.65\n10.00\n10.49\n11.31\n12.90\n14.72\n15.43\n16.76\n17.89\n19.99\n21.45\n22.42\n25.53\n28.16\n31.98\n36.54\nSlimmeRF\n13.14\n21.40\n24.90\n27.72\n29.86\n31.46\n32.86\n33.53\n34.02\n34.60\n34.96\n35.43\n35.82\n36.04\n36.26\n36.45\n32\nBaseline\n9.48\n9.49\n9.53\n9.54\n9.57\n9.63\n9.74\n9.81\n9.94\n10.21\n10.71\n10.86\n10.99\n11.81\n12.16\n12.32\nSlimmeRF\n13.75\n23.41\n25.22\n27.59\n29.47\n30.72\n31.57\n32.74\n33.31\n33.69\n34.03\n34.29\n34.81\n35.10\n35.26\n35.47\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n13.14\n13.98\n14.86\n15.97\n16.81\n17.14\n19.49\n22.23\n24.09\n26.66\n27.54\n31.00\n33.35\n35.22\n35.99\n36.79\nSlimmeRF\n35.59\n35.71\n35.84\n35.92\n36.03\n36.07\n36.17\n36.22\n36.25\n36.30\n36.33\n36.35\n36.37\n36.39\n36.41\n36.43\nTable 9. Baseline comparison tests on Lego of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n10.07\n12.86\n19.11\n29.54\nSlimmeRF\n13.27\n20.48\n24.78\n29.50\n8\nBaseline\n8.87\n9.68\n12.34\n14.73\n18.42\n23.23\n26.39\n29.90\nSlimmeRF\n11.20\n19.81\n23.55\n25.50\n26.52\n27.71\n28.88\n29.86\n16\nBaseline\n8.74\n8.74\n8.75\n8.76\n8.84\n9.14\n10.36\n15.41\n16.18\n16.36\n20.38\n21.70\n23.15\n24.79\n28.54\n30.09\nSlimmeRF\n12.40\n20.00\n23.34\n24.99\n25.67\n26.26\n26.97\n27.76\n28.52\n28.90\n29.16\n29.42\n29.59\n29.77\n29.93\n30.04\n32\nBaseline\n8.74\n8.74\n9.06\n9.07\n9.24\n9.39\n9.48\n9.53\n9.61\n9.64\n9.79\n9.93\n10.01\n12.74\n12.81\n13.16\nSlimmeRF\n11.62\n19.58\n22.53\n24.53\n25.78\n26.32\n26.92\n27.32\n27.92\n28.31\n28.61\n28.89\n29.05\n29.14\n29.27\n29.36\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n14.21\n15.42\n17.02\n17.15\n17.75\n18.40\n19.15\n19.49\n19.57\n25.72\n27.75\n28.08\n28.50\n28.70\n29.34\n30.19\nSlimmeRF\n29.49\n29.57\n29.65\n29.72\n29.79\n29.83\n29.89\n29.95\n30.00\n30.03\n30.07\n30.09\n30.12\n30.14\n30.16\n30.18\nTable 10. Baseline comparison tests on Materials of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n14.33\n16.96\n22.35\n33.85\nSlimmeRF\n15.58\n21.75\n29.30\n34.20\n8\nBaseline\n13.30\n13.53\n16.18\n18.60\n20.69\n23.49\n25.31\n34.44\nSlimmeRF\n16.29\n24.01\n27.04\n28.22\n29.70\n31.37\n32.81\n34.51\n16\nBaseline\n13.21\n14.10\n14.87\n15.42\n15.82\n16.34\n17.40\n18.28\n19.97\n22.16\n23.15\n24.07\n27.37\n31.72\n32.89\n34.97\nSlimmeRF\n17.27\n23.80\n26.33\n27.58\n28.28\n29.27\n30.08\n30.80\n31.60\n31.88\n32.45\n32.88\n33.25\n33.75\n34.35\n35.06\n32\nBaseline\n13.04\n13.16\n13.97\n14.07\n14.10\n14.14\n14.20\n14.33\n14.60\n14.73\n15.64\n15.74\n15.90\n16.72\n19.56\n20.34\nSlimmeRF\n14.55\n24.11\n26.81\n28.09\n28.47\n29.18\n29.93\n30.49\n30.79\n31.00\n31.33\n31.47\n31.78\n31.93\n32.07\n32.26\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n20.68\n20.91\n22.81\n23.02\n23.31\n23.85\n25.12\n25.87\n26.55\n29.08\n29.46\n30.36\n32.29\n32.85\n33.87\n34.99\nSlimmeRF\n32.58\n32.75\n32.93\n33.13\n33.23\n33.37\n33.50\n33.87\n34.03\n34.17\n34.28\n34.50\n34.75\n34.90\n34.95\n35.00\nTable 11. Baseline comparison tests on Mic of NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n7.62\n11.25\n18.60\n29.95\nSlimmeRF\n11.98\n23.58\n26.61\n29.91\n8\nBaseline\n6.15\n6.94\n10.72\n13.53\n14.88\n16.89\n23.43\n30.46\nSlimmeRF\n10.75\n21.81\n24.62\n26.26\n27.70\n28.71\n29.69\n30.43\n16\nBaseline\n6.03\n6.11\n6.23\n6.86\n7.65\n8.50\n13.26\n14.81\n16.32\n18.24\n20.05\n22.95\n24.98\n27.65\n28.81\n30.72\nSlimmeRF\n11.85\n21.98\n23.99\n25.41\n26.50\n27.36\n28.11\n28.69\n29.11\n29.59\n29.91\n30.18\n30.39\n30.52\n30.56\n30.58\n32\nBaseline\n5.90\n5.97\n6.22\n6.27\n6.30\n6.87\n7.00\n7.20\n7.42\n7.51\n7.90\n7.98\n8.24\n8.61\n9.08\n9.31\nSlimmeRF\n10.07\n22.57\n24.01\n25.00\n26.29\n27.23\n27.76\n28.34\n28.71\n29.17\n29.51\n29.87\n30.16\n30.38\n30.54\n30.55\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n9.45\n10.52\n10.68\n11.37\n12.21\n12.64\n13.23\n25.92\n26.48\n27.34\n27.80\n28.75\n29.18\n29.66\n30.46\n30.86\nSlimmeRF\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\n30.56\nTable 12. Baseline comparison tests on Ship of NeRF Synthetic. Numbers are in PSNR (dB).\n17\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n4\nBaseline\n12.39\n15.05\n19.98\n32.34\nSlimmeRF\n14.51\n23.60\n27.36\n32.35\n8\nBaseline\n11.26\n11.94\n13.96\n16.12\n18.37\n22.54\n26.81\n32.89\nSlimmeRF\n14.78\n23.08\n25.77\n27.34\n28.94\n30.27\n31.50\n32.80\n16\nBaseline\n11.02\n11.29\n11.57\n11.92\n12.55\n13.32\n14.74\n16.43\n17.64\n18.97\n20.78\n22.25\n24.75\n28.09\n30.27\n33.23\nSlimmeRF\n14.58\n22.10\n24.86\n26.30\n27.56\n28.52\n29.26\n29.83\n30.37\n30.80\n31.27\n31.69\n32.06\n32.42\n32.76\n33.04\n32\nBaseline\n10.85\n10.90\n11.18\n11.23\n11.32\n11.48\n11.61\n11.71\n11.95\n12.11\n12.80\n12.96\n13.18\n14.16\n15.01\n16.06\nSlimmeRF\n13.21\n20.55\n22.81\n24.41\n25.64\n26.84\n27.53\n28.20\n28.72\n29.14\n29.53\n29.89\n30.28\n30.55\n30.80\n31.03\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\nBaseline\n16.72\n17.47\n18.30\n18.77\n19.39\n20.21\n21.37\n24.02\n24.76\n27.24\n28.14\n29.96\n31.22\n31.80\n32.48\n33.32\nSlimmeRF\n31.28\n31.48\n31.68\n31.87\n32.02\n32.17\n32.31\n32.48\n32.59\n32.71\n32.82\n32.93\n33.04\n33.11\n33.17\n33.22\nTable 13. Average results of baseline comparison tests on NeRF Synthetic. Numbers are in PSNR (dB).\nRank:\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nBarn\nBaseline\n8.19\n8.63\n9.28\n9.90\n10.75\n11.43\n11.90\n14.91\n15.96\n17.17\n18.04\n18.95\n22.22\n23.88\n25.30\n27.44\nSlimmeRF\n12.24\n15.87\n19.69\n21.64\n22.28\n23.10\n23.65\n24.34\n24.76\n25.23\n25.64\n26.03\n26.49\n26.76\n27.01\n27.18\nCaterpillar\nBaseline\n8.20\n8.48\n9.48\n9.80\n10.24\n12.20\n15.40\n15.86\n17.49\n19.02\n20.00\n21.33\n22.60\n23.47\n25.00\n26.00\nSlimmeRF\n10.46\n16.59\n18.53\n19.78\n21.11\n21.85\n22.62\n23.05\n23.55\n24.10\n24.50\n24.81\n25.08\n25.42\n25.66\n25.98\nFamily\nBaseline\n11.69\n12.62\n14.26\n15.64\n17.02\n20.33\n21.91\n24.37\n25.43\n26.54\n27.84\n29.34\n30.66\n31.67\n32.81\n34.10\nSlimmeRF\n18.11\n22.56\n24.25\n25.19\n26.12\n26.97\n27.91\n28.63\n29.50\n30.22\n30.88\n31.76\n32.27\n32.74\n33.34\n34.03\nIgnatius\nBaseline\n12.89\n12.91\n13.03\n13.30\n13.66\n13.82\n14.78\n16.25\n19.10\n20.80\n21.88\n23.19\n25.37\n26.63\n27.34\n28.49\nSlimmeRF\n20.66\n22.82\n24.51\n25.13\n25.65\n26.34\n26.70\n27.04\n27.30\n27.44\n27.64\n27.88\n28.04\n28.17\n28.38\n28.46\nTruck\nBaseline\n9.20\n9.51\n9.87\n10.66\n11.31\n11.65\n12.72\n14.09\n15.90\n17.73\n18.85\n21.06\n22.81\n24.02\n25.33\n26.85\nSlimmeRF\n12.61\n14.87\n17.28\n19.89\n21.16\n22.10\n22.92\n23.47\n24.46\n24.94\n25.39\n25.78\n26.09\n26.46\n26.71\n26.94\nAverage\nBaseline\n10.03\n10.43\n11.19\n11.86\n12.59\n13.89\n15.34\n17.10\n18.78\n20.25\n21.32\n22.77\n24.73\n25.94\n27.16\n28.58\nSlimmeRF\n14.82\n18.54\n20.85\n22.33\n23.26\n24.07\n24.76\n25.31\n25.91\n26.38\n26.81\n27.25\n27.59\n27.91\n28.22\n28.52\nTable 14. Results of SlimmeRF-16 and the TensoRF-VM-192 Baseline on the Tanks & Temples Dataset. Numbers are in PSNR (dB).\n3 Views\nFern\nFlower\nFortress\nHorns\nLeaves\nOrchids\nRoom\nT-Rex\n1\n14.68\n14.37\n15.13\n13.66\n12.29\n12.13\n13.85\n13.88\n2\n16.56\n15.43\n17.39\n16.07\n13.73\n13.31\n16.10\n15.52\n3\n17.11\n16.96\n17.46\n17.03\n14.37\n13.60\n17.37\n16.23\n4\n17.20\n18.14\n17.47\n17.53\n14.73\n13.82\n17.44\n16.69\n5\n17.19\n18.51\n17.42\n17.67\n14.77\n13.86\n17.44\n16.77\n6\n17.24\n18.58\n17.36\n17.69\n14.89\n13.87\n17.44\n16.78\n7\n17.23\n18.62\n17.33\n17.76\n14.93\n13.88\n17.43\n16.80\n8\n17.24\n18.63\n17.33\n17.77\n14.93\n13.90\n17.42\n16.82\n9\n17.24\n18.63\n17.30\n17.79\n14.94\n13.93\n17.42\n16.83\n10\n17.23\n18.63\n17.28\n17.80\n14.95\n13.92\n17.41\n16.83\n11\n17.22\n18.63\n17.27\n17.80\n14.95\n13.91\n17.41\n16.82\n12\n17.22\n18.63\n17.26\n17.80\n14.95\n13.90\n17.41\n16.84\n13\n17.21\n18.63\n17.26\n17.80\n14.94\n13.93\n17.41\n16.84\n14\n17.20\n18.63\n17.25\n17.82\n14.93\n13.93\n17.41\n16.84\n15\n17.20\n18.64\n17.25\n17.82\n14.92\n13.93\n17.41\n16.83\n16\n17.19\n18.63\n17.24\n17.82\n14.91\n13.93\n17.40\n16.83\n17\n17.18\n18.63\n17.24\n17.80\n14.91\n13.93\n17.40\n16.82\n18\n17.17\n18.63\n17.23\n17.81\n14.90\n13.93\n17.40\n16.82\n19\n17.16\n18.63\n17.23\n17.81\n14.89\n13.93\n17.40\n16.82\n20\n17.15\n18.63\n17.23\n17.81\n14.88\n13.92\n17.40\n16.82\n21\n17.14\n18.63\n17.23\n17.82\n14.88\n13.92\n17.40\n16.82\n22\n17.13\n18.63\n17.22\n17.82\n14.87\n13.92\n17.39\n16.81\n23\n17.12\n18.63\n17.22\n17.81\n14.87\n13.92\n17.39\n16.81\n24\n17.12\n18.63\n17.22\n17.82\n14.86\n13.92\n17.38\n16.81\nTable 15. Per-scene results of the experiments with LLFF (3 Views). The leftmost column displays the number\nof components left. The model used was SlimmeRF-24. Numbers are in PSNR (dB).\n18\n6 Views\nFern\nFlower\nFortress\nHorns\nLeaves\nOrchids\nRoom\nT-Rex\n1\n16.03\n15.47\n16.81\n13.76\n12.84\n12.90\n14.76\n14.46\n2\n18.27\n16.39\n19.19\n15.67\n14.47\n13.74\n17.25\n15.25\n3\n19.50\n17.80\n20.14\n17.13\n15.09\n14.49\n19.45\n17.41\n4\n20.00\n18.92\n20.29\n17.87\n15.81\n15.01\n20.06\n17.75\n5\n20.41\n19.38\n20.41\n18.04\n16.15\n15.44\n20.70\n17.89\n6\n20.66\n19.76\n20.48\n18.20\n16.46\n15.64\n20.86\n18.01\n7\n20.78\n19.92\n20.53\n18.30\n16.63\n15.81\n20.90\n18.21\n8\n20.88\n19.99\n20.67\n18.34\n16.76\n15.95\n20.90\n18.26\n9\n21.00\n20.00\n20.63\n18.39\n16.77\n16.15\n20.91\n18.31\n10\n21.02\n20.01\n20.65\n18.40\n16.81\n16.19\n20.97\n18.35\n11\n21.05\n20.03\n20.64\n18.40\n16.82\n16.22\n21.00\n18.37\n12\n21.07\n20.05\n20.64\n18.40\n16.81\n16.24\n21.02\n18.37\n13\n21.08\n20.06\n20.62\n18.40\n16.81\n16.26\n21.03\n18.38\n14\n21.08\n20.06\n20.62\n18.42\n16.81\n16.28\n21.03\n18.42\n15\n21.08\n20.06\n20.61\n18.45\n16.80\n16.28\n21.03\n18.42\n16\n21.07\n20.07\n20.61\n18.45\n16.80\n16.28\n21.03\n18.42\n17\n21.07\n20.08\n20.61\n18.45\n16.79\n16.28\n21.03\n18.42\n18\n21.07\n20.08\n20.60\n18.45\n16.79\n16.29\n21.02\n18.42\n19\n21.07\n20.09\n20.59\n18.45\n16.78\n16.29\n21.02\n18.42\n20\n21.07\n20.09\n20.57\n18.46\n16.78\n16.29\n21.02\n18.42\n21\n21.06\n20.09\n20.56\n18.46\n16.77\n16.29\n21.01\n18.42\n22\n21.06\n20.09\n20.55\n18.46\n16.77\n16.30\n21.00\n18.41\n23\n21.05\n20.09\n20.54\n18.48\n16.77\n16.29\n21.00\n18.41\n24\n21.05\n20.09\n20.53\n18.47\n16.76\n16.29\n21.00\n18.41\nTable 16. Per-scene results of the experiments with LLFF (6 Views). The leftmost column displays the number\nof components left. The model used was SlimmeRF-24. Numbers are in PSNR (dB).\n9 Views\nFern\nFlower\nFortress\nHorns\nLeaves\nOrchids\nRoom\nT-Rex\n1\n14.36\n15.95\n14.35\n14.04\n13.50\n13.36\n13.97\n14.86\n2\n17.24\n18.19\n20.64\n16.35\n14.71\n14.05\n19.19\n16.32\n3\n20.10\n19.68\n21.32\n17.48\n15.84\n15.38\n22.44\n18.19\n4\n21.26\n20.51\n21.81\n18.20\n16.66\n16.34\n23.76\n18.73\n5\n21.73\n21.12\n21.99\n18.82\n17.01\n17.04\n24.13\n19.15\n6\n22.17\n21.90\n22.15\n19.11\n17.38\n17.39\n24.41\n19.59\n7\n22.63\n22.07\n22.34\n19.38\n17.78\n17.72\n24.55\n20.02\n8\n23.00\n22.19\n22.41\n19.55\n17.95\n17.91\n24.64\n20.13\n9\n23.21\n22.21\n22.42\n19.66\n18.04\n18.01\n24.73\n20.22\n10\n23.34\n22.23\n22.48\n19.72\n18.08\n18.04\n24.83\n20.25\n11\n23.44\n22.24\n22.49\n19.73\n18.12\n18.08\n24.85\n20.34\n12\n23.62\n22.26\n22.50\n19.77\n18.12\n18.11\n24.94\n20.38\n13\n23.72\n22.26\n22.51\n19.79\n18.16\n18.13\n24.99\n20.40\n14\n23.77\n22.26\n22.51\n19.80\n18.17\n18.13\n25.02\n20.43\n15\n23.86\n22.26\n22.51\n19.83\n18.15\n18.13\n25.05\n20.43\n16\n23.93\n22.27\n22.52\n19.82\n18.15\n18.13\n25.09\n20.45\n17\n24.02\n22.27\n22.52\n19.82\n18.14\n18.13\n25.11\n20.48\n18\n24.04\n22.27\n22.52\n19.84\n18.14\n18.13\n25.13\n20.48\n19\n24.06\n22.27\n22.52\n19.84\n18.14\n18.14\n25.17\n20.50\n20\n24.08\n22.27\n22.53\n19.86\n18.14\n18.14\n25.18\n20.51\n21\n24.08\n22.27\n22.53\n19.85\n18.13\n18.13\n25.18\n20.52\n22\n24.11\n22.27\n22.53\n19.85\n18.13\n18.13\n25.18\n20.51\n23\n24.14\n22.27\n22.53\n19.87\n18.13\n18.13\n25.18\n20.51\n24\n24.15\n22.27\n22.53\n19.87\n18.12\n18.13\n25.18\n20.51\nTable 17. Per-scene results of the experiments with LLFF (9 Views). The leftmost column displays the number\nof components left. The model used was SlimmeRF-24. Numbers are in PSNR (dB).\n3 Views\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13.75\n15.51\n16.27\n16.63\n16.70\n16.73\n16.75\n16.76\n16.76\n16.76\n16.75\n16.75\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n16.75\n16.75\n16.75\n16.74\n16.74\n16.74\n16.73\n16.73\n16.73\n16.73\n16.72\n16.72\n6 Views\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n14.63\n16.28\n17.63\n18.21\n18.55\n18.76\n18.89\n18.97\n19.02\n19.05\n19.06\n19.08\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n19.08\n19.09\n19.09\n19.09\n19.09\n19.09\n19.09\n19.09\n19.08\n19.08\n19.08\n19.08\n9 Views\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n14.30\n17.09\n18.80\n19.66\n20.12\n20.51\n20.81\n20.97\n21.06\n21.12\n21.16\n21.21\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n21.25\n21.26\n21.28\n21.29\n21.31\n21.32\n21.33\n21.34\n21.34\n21.34\n21.34\n21.35\nTable 18. Average results in PSNR (dB) for each slimmed rank of SlimmeRF-24 in sparse-view scenarios. In\neach grid the number on the top is the number of components left and the number on the bottom is the average\nPSNR value.\n19\n22.62 / 0.758\n25.35 / 0.842\n26.89 / 0.873\n27.42 / 0.880\n27.68 / 0.885\n27.75 / 0.887\n22.50 / 0.741\n26.05 / 0.847\n26.23 / 0.851\n26.26 / 0.853\n26.31 / 0.856\n26.31 / 0.856\n28.19 / 0.776\n30.15 / 0.848\n30.52 / 0.866\n30.63 / 0.875\n30.76 / 0.883\n30.84 / 0.890\n21.38 / 0.683\n24.31 / 0.832\n24.82 / 0.873\n24.93 / 0.879\n25.03 / 0.885\n25.06 / 0.889\nFigure 10. Results for Fern, Flower, Fortress, and Horns of LLFF (9 Views). Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 4; b 8; c 12; d 16; e 20; f 24. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n20\n19.11 / 0.706\n21.75 / 0.822\n22.24 / 0.840\n22.39 / 0.845\n22.40 / 0.846\n22.39 / 0.845\n18.14 / 0.594\n21.31 / 0.764\n21.90 / 0.789\n22.03 / 0.794\n22.07 / 0.795\n22.08 / 0.794\n30.05 / 0.913\n32.75 / 0.945\n33.48 / 0.952\n33.88 / 0.956\n34.07 / 0.958\n34.08 / 0.959\n22.92 / 0.771\n26.61 / 0.891\n27.38 / 0.906\n27.66 / 0.911\n27.90 / 0.915\n27.92 / 0.916\nFigure 11. Results for Leaves, Orchids, Room, and T-Rex of LLFF (9 Views). Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 4; b 8; c 12; d 16; e 20; f 24. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n21\n22.01 / 0.85\n27.47 / 0.922\n29.29 / 0.946\n30.74 / 0.961\n33.21 / 0.978\n34.54 / 0.984\n24.48 / 0.909\n25.48 / 0.925\n25.53 / 0.927\n25.54 / 0.927\n26.39 / 0.940\n27.14 / 0.950\n16.52 / 0.845\n18.03 / 0.863\n19.39 / 0.886\n21.35 / 0.912\n26.94 / 0.957\n33.19 / 0.980\n30.77 / 0.935\n33.74 / 0.959\n34.98 / 0.968\n35.45 / 0.971\n36.45 / 0.976\n37.08 / 0.980\nFigure 12. Results for Chair, Drums, Ficus, and Hotdog of Synthetic NeRF. Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 2; b 4; c 6; d 8; e 12; f 16. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n22\n19.96 / 0.822\n22.43 / 0.876\n26.50 / 0.911\n29.26 / 0.940\n34.10 / 0.973\n35.27 / 0.979\n22.80 / 0.861\n26.56 / 0.910\n28.39 / 0.928\n29.30 / 0.936\n30.41 / 0.945\n30.88 / 0.950\n22.62 / 0.926\n26.67 / 0.951\n27.93 / 0.960\n29.22 / 0.965\n32.93 / 0.984\n35.82 / 0.992\n24.56 / 0.766\n26.81 / 0.805\n28.24 / 0.827\n29.40 / 0.844\n30.88 / 0.863\n31.14 / 0.867\nFigure 13. Results for Lego, Materials, Mic, and Ship of Synthetic NeRF. Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 2; b 4; c 6; d 8; e 12; f 16. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n23\n24.29 / 0.864\n25.90 / 0.885\n27.64 / 0.907\n28.84 / 0.919\n29.52 / 0.927\n30.75 / 0.945\n22.29 / 0.756\n23.56 / 0.781\n24.61 / 0.806\n25.26 / 0.824\n25.87 / 0.839\n26.76 / 0.867\n20.20 / 0.830\n22.62 / 0.854\n23.70 / 0.869\n24.98 / 0.885\n25.88 / 0.897\n26.96 / 0.916\n21.30 / 0.843\n23.81 / 0.870\n25.08 / 0.886\n26.27 / 0.902\n27.21 / 0.916\n28.72 / 0.939\n27.81 / 0.913\n29.10 / 0.924\n30.94 / 0.937\n32.00 / 0.945\n33.24 / 0.955\n35.79 / 0.972\nFigure 14. Results for Barn, Caterpillar, and Family of Tanks & Temples. Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 4; b 6; c 8; d 10; e 12; f 16. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n24\n27.53 / 0.920\n29.37 / 0.936\n31.08 / 0.949\n32.06 / 0.956\n33.08 / 0.964\n34.64 / 0.974\n26.50 / 0.925\n27.95 / 0.936\n28.61 / 0.943\n29.07 / 0.947\n29.41 / 0.952\n28.50 / 0.957\n25.00 / 0.903\n26.30 / 0.916\n27.04 / 0.924\n27.40 / 0.929\n27.65 / 0.934\n28.04 / 0.943\n20.95 / 0.833\n23.11 / 0.856\n24.83 / 0.873\n26.30 / 0.889\n27.17 / 0.903\n28.36 / 0.929\n18.98 / 0.797\n20.71 / 0.825\n22.51 / 0.847\n24.17 / 0.866\n25.32 / 0.888\n26.84 / 0.923\nFigure 15. Results for Family, Ignatius, and Truck of Tanks & Temples. Correspondence between image position and the number of\ncomponents left (see Table 4 for definition of the letters): a 4; b 6; c 8; d 10; e 12; f 16. The values displayed on the upper left corner are\nthe PSNR value and the SSIM value.\n25\nReferences\n[A1] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In Proceedings\nof the European Conference on Computer Vision, pages 333\u2013\n350. Springer, 2022.\n[A2] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen\nKoltun. Tanks and temples: Benchmarking large-scale scene\nreconstruction. ACM Transactions on Graphics, 36(4):1\u201313,\n2017.\n[A3] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view synthe-\nsis with prescriptive sampling guidelines. ACM Transactions\non Graphics, 38(4):1\u201314, 2019.\n[A4] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n[A5] Aladin Virmaux and Kevin Scaman. Lipschitz regularity of\ndeep neural networks: analysis and efficient estimation. Ad-\nvances in Neural Information Processing Systems, 31, 2018.\n26\n"
  },
  {
    "title": "Perspectives on the State and Future of Deep Learning -- 2023",
    "link": "https://arxiv.org/pdf/2312.09323.pdf",
    "upvote": "5",
    "text": "Perspectives on the State and Future of Deep Learning - 2023\nEditors: Micah Goldblum\u2217, Anima Anandkumar, Richard Baraniuk, Tom Goldstein\u2020\nContributors: Kyunghyun Cho, Zachary C Lipton, Melanie Mitchell,\nPreetum Nakkiran, Max Welling, Andrew Gordon Wilson\nAbstract\nThe goal of this series is to chronicle opinions and issues in the field of machine\nlearning as they stand today and as they change over time. The plan is to host\nthis survey periodically until the AI singularity paperclip-frenzy-driven doomsday,\nkeeping an updated list of topical questions and interviewing new community\nmembers for each edition. In this issue, we probed people\u2019s opinions on inter-\npretable AI, the value of benchmarking in modern NLP, the state of progress\ntowards understanding deep learning, and the future of academia.\nThe Questions\nWhat is the biggest problem that we should be working on but aren\u2019t?\n1\nWhy haven\u2019t we made progress towards understanding deep learning?\n4\nWill deep learning ever be interpretable?\n8\nIs deep learning moving beyond academic benchmarking?\n10\nAre transformers the way forward or are they limited?\n11\nAre academics screwed?\n13\nWhat is the biggest problem that we should be working on but aren\u2019t, or\nvice versa? What other areas will see progress other than text and images, and how\nmight this impact fields and industries outside computer science?\nMax Welling: I am a huge fan of applying AI to the natural sciences. There are an enormous\nnumber of very exciting and impactful applications in this area. For example, we can predict\nglobal warming using fast PDE surrogates, we can design new battery and carbon capture\nmaterials through faster density functional theory and molecular dynamics simulators and\ngenerative AI, and we can predict the effectiveness of new drugs using these methods. The\nnatural sciences provide a treasure trove of data and exciting problems to work on!\n\u2217goldblum@nyu.edu\n\u2020tomg@umd.edu\n1\narXiv:2312.09323v3  [cs.AI]  19 Dec 2023\nZachary Lipton: It\u2019s hard to guess what problems are truly being neglected at this strange\nmoment in history when seemingly everyone in every walk of life, rightly or wrongly, is\nlooking to apply AI to their problems. In short, I think this ship has sailed. AI methods are\nalready applied far outside of computer science, and even in industry, it\u2019s no longer strictly\nscientists at the helm the way it was, say, 5 years ago. I\u2019m surprised constantly by how many\nbusiness people are following AI so closely, by how often they are abreast of new developments\nbefore academic researchers and by how academia\u2019s influence has waned in many corners of\nindustry. Of course there are always profound problems to address, and the hard problems\nare competing for the attention of a relatively small set of extreme athletes. I have my own\npassion problems\u2014tackling the documentation at the root of physician burnout, leveraging\ndata to realize the dream of personalized medicine\u2014but while I\u2019d welcome more hands on\ndeck, you can hardly claim that nobody\u2019s working on them.\nKyunghyun Cho: I have two things in my mind. First, we seem to focus way too much on\nsimple problems where answers/predictions are readily verifiable by e.g. simple surface-level\ncomparison (0-1 loss against the gold standard answer) or by asking a crowd worker. These\nproblems are also the ones that have the most amount of data available. In other words, on\nthese simple problems, we can iterate rapidly due to relatively cheap evaluation, and our\nmodels work quite well from the beginning due to the abundance of data. I believe we should\nconsider more challenging problems where each iteration of development is costly, which will\nforce us to look into more interesting and challenging topics within machine learning and\nmore broadly artificial intelligence, such as active learning and learned evaluation.\nSecond, I believe we should work on so-called distribution shifts much more so and much\nmore carefully. A lot of people work on distribution shifts, but are often working in a more\ntraditional mode of empirical research in machine learning; that is, they build a benchmark\nand iterate on developing algorithms based on the accuracy on the benchmark. Unfortunately\nthis paradigm is completely broken for studying distribution shifts, as the designer can use\ntheir knowledge of the test distribution when choosing the algorithm. By computing accuracies\nor even inspecting data a bit, distributional information of the test set leaks, and this largely\nnullifies various findings. Rather, we have to always start from specifying what kind of\ndistribution shifts we are considering, and come up with an algorithm prior to testing it on\nany benchmark.\nBoth of these directions will greatly improve the applicability and impact of machine learning.\nAs for me personally, active learning and robust classification are two key problems I am\nseeing every day at Prescient Design, Genentech for antibody design.\nMelanie Mitchell: I will list a few \u201cbiggest problems\u201d that we should be working on (and\nindeed some people are):\n1. Reasoning about abstract concepts (e.g., Chollet\u2019s ARC challenge [7]): This is a funda-\nmental human ability that AI systems have not yet mastered in any general way. For\nexample, humans are able to solve (at least a large percentage of) problems in Chollet\u2019s\n\u201cAbstraction and Reasoning Corpus,\u201d [6] which tests for few-shot abstraction abilities\nand general understanding of \u201ccore concepts\u201d. No AI system today comes close.\n2. Multimodal common sense (e.g., compositionality, visual reasoning): Current multimodal\nvision-language models (e.g., CLIP, DALL-E, etc.) are known to be poor at compositional\nunderstanding, reasoning, and general visual common sense (e.g., they are susceptible\nto non-humanlike errors and adversarial attacks). Dealing with the integration of vision\nand language, and the physical and social understanding these require, are of course\n2\nessential capabilities for any AI system interacting with the human visual world.\n3. Better ways of evaluating AI systems\u2019 capabilities: The most common methods of\nevaluation\u2014static benchmarks, reporting only aggregate statistics such as test-set-\naccuracy\u2014do not give enough insight into the actual capabilities and robustness of\nAI systems. Moreover, giving AI systems tests designed for humans (e.g., the Bar\nexam, medical licensing tests, IQ tests, etc.) is not always a good way of evaluating\ntheir capabilities, since those tests make many assumptions that are true for humans\nbut not for machines.\nNew ways of evaluating the abilities and generality of AI\nsystems are needed.\nOne promising approach is for AI researchers to collaborate\nwith cognitive scientists\u2014such as developmental psychologists and animal cognition\nresearchers\u2014who have experience in designing controlled experiments to gain insight\ninto \u201calien intelligences\u201d (cf. [14])\n4. Mechanistic interpretability: This refers to understanding how AI systems work \u201cunder\nthe hood\u201d\u2014that is, the actual mechanisms underlying their information processing. Just\nas mechanistic interpretability is an unsolved problem in brains, it\u2019s a very difficult and\nunsolved problem for today\u2019s large-scale pretrained transformer-based models. However,\nunderstanding underlying mechanisms of behavior is a central goal of any science, and it\nwould be extremely useful to have a well-developed science of AI that attempts to find\nout what these mechanisms are. I believe this would give us insight into the capabilities\nand limitations of AI systems that we can\u2019t discover from behavioral observations alone.\nAndrew Gordon Wilson: Machine learning for scientific discovery. There are many efforts\napplying machine learning to scientific tasks \u2014 even under the headline of \u201cAI for Science\u201d, a\nphrase often used in recruiting. Yet there are relatively few efforts developing machine learning\nto discover fundamental scientific insights into data that we didn\u2019t have before, towards new\nscientific theories. Moreover, the unifying theory \u2014 the general understanding \u2014 rather than\nany particular application, has historically been the primary quantity of scientific interest!\nConsider general relativity (GR) as a running example. GR is primarily exciting not because\nit enables GPS, which is an impactful application, but because it tells us how gravity,\ntime, and space interact, towards a multitude of different applications. GPS wasn\u2019t even\nenvisaged when GR was proposed. While we probably could hack a neural network together\nto correct for gravitational time dilation to make GPS possible, we would lack an underlying\nunderstanding that enables many other applications, including applications we don\u2019t currently\nforesee. Similarly, special relativity is in some sense \u201cmerely\u201d an interpretation of the Lorentz\ntransformations (which were experimental correction factors), but is viewed as a much more\nsignificant contribution than the equations, due to the fundamental understanding it provides.\nWe worry about \u201cAGI\u201d, but we haven\u2019t even made the most basic steps towards a system\nthat could propose a theory anything like general relativity or quantum mechanics. We\nhaven\u2019t scratched the surface of scientific discovery. What would an algorithm that could\npropose such theories even look like? GR was not \u201cdata driven\u201d, though it explains some\nmysterious empirical observations of the time, such as Mercury\u2019s perihelion. Such an algorithm\nwouldn\u2019t be at all like the deep learning of today, or classical symbolic systems. Einstein\nwasn\u2019t enumerating symbolic expressions and selecting for ones with \u201cnice\u201d properties such as\nsymmetries.\nI want to live in a future where AI profoundly advances scientific understanding \u2014 science\nthat we ourselves may not be capable of discovering, such as an understanding of how the\nbrain works.\n3\nAn \u201cup and coming\u201d direction I am personally very excited about is in exploiting algebraic\nstructure that broadly arises as a consequence of common modelling assumptions. I don\u2019t\nthink there is enough general awareness of how important numerical linear algebra will be\nas a foundation for machine learning. We have a project, and library, CoLA (Compositional\nLinear Algebra) that is all about this [32]. In some sense, there is an analogy with autograd \u2014\nautomatic differentiation in libraries like PyTorch, JAX, and others before it, have revolution-\nized deep learning by freeing users to rapidly prototype ideas without manually computing\nderivatives. A similar result will be achieved with numerical linear algebra. Arguably, machine\nlearning is essentially an application of linear algebra.\nIn terms of what we are working on but shouldn\u2019t, I would only say there is too much of a\nherd mentality in research \u2014 too many people working on essentially the same thing! For a\ntime, this was GANs. Chasing topical research provides short term validation and attention,\nbut it\u2019s not typically a good way to make meaningful long-term contributions, or to find work\nthat resonates with you personally. If you\u2019re too worried about being scooped, then it\u2019s a good\nindication to work on something else. We tend to erroneously assume that the foundations are\n\u201cset in stone\u201d; but there are many fundamental considerations that deep learning researchers\naren\u2019t even thinking of questioning.\nWhy haven\u2019t we made progress towards understanding deep learning and\nwill we ever? If we do ever develop a theory of deep learning, would it be useful to\npractitioners and engineers?\nAlex Smola: Understanding nonparametric nonlinear functions in general is mission impossi-\nble, but so is \u201cunderstanding\u201d the Fourier coefficients or wavelet coefficients of a function. It\u2019s\nas easy or hard as properly understanding potential functions in solid state physics.\nMax Welling: I am sure we will make progress in visualizing the inner workings of DL, and\nto some level be able to interrogate the way it reasons. However, we may have to live with\nthe fact that these systems reason and think in fundamental new ways that are very hard\nfor us mere mortals to understand. For certain applications this will not be acceptable (such\nas decisions that impact people\u2019s lives) and for others it may be fine (such as predicting the\nstock market). I am personally not too worried about the need to understand every last bit of\nthese models as long as we can find ways to certify them for the domains that we want to use\nthem for. After all, I do step into an airplane without a clue of how its software works.\nKyunghyun Cho: I\u2019m not entirely sure if we really haven\u2019t made progress toward under-\nstanding deep learning. What do we not understand about deep learning?\nA lot of techniques we use in order to build, train and deploy deep learning models and\nsystems were invented and developed as part of understanding challenges in training deep\nlearning models. For instance, the issue of vanishing gradients was identified in early 90\u2019s,\nstudied quite extensively by Yoshua Bengio and Sepp Hochreiter over the entire decade in 90\u2019s,\nand we know how to address this issue (to a certain degree) by introducing linear shortcut\nconnections, such as residual connections.\nOur understanding of loss functions typically used for training a deep neural net has only\nimproved over the past several decades. With a particular type of deep neural nets, we know\nthat there is no point minimum but a long and perhaps highly curved low-loss valley, and in\nsome cases we can also show this quite rigorously, although intuition behind this observation\n4\nwas already known in 80\u2019s (see Rumelhart et al.\u2019s backpropagation paper.) This observation\nhas led to some interesting practical algorithms such as the one from Andrew Gordon Wilson\u2019s\ngroup.\nSo, my answer would be that we have made progress toward understanding deep learning,\nalthough there is much more progress in multiple directions to be made in the future,\nsuch as characterizing and tying up the patterns of generalization with neural net learning\n(architecture, optimization, data, etc.) Furthermore, I would say a lot of progress in theoretical\nunderstanding of various aspects of deep learning have already resulted in and will continue\nto result in algorithmic practices (including algorithms, recipes for training neural nets, etc.)\nthat are useful for practitioners.\nAndrew Gordon Wilson: I disagree with the premise that we haven\u2019t made progress. Part\nof what has slowed progress is that we assume lack of progress as a premise, and then attempt\nto rationalize that premise. If we can acknowledge the progress that has been made, then we\ncan make steps towards further progress. The real crisis is a failure to acknowledge progress.\nAnd there has been substantial progress in understanding deep learning. Virtually everything\nabout deep learning that has been pointed to as \u201cmysterious\u201d or \u201cdifferent\u201d is neither particularly\nmysterious nor different from classical machine learning. Overparametrization, having more\nparameters than data points, was commonplace in nonparametric statistics (very popular\njust before the re-emergence of deep learning as a dominant paradigm in 2012), and it is\nunderstood that it is an elementary mistake to treat parameter counting as a proxy for\ngeneralization [e.g., 40]. Models with more parameters often provide better compressions\nof the data, leading to lower effective dimensionality, and simpler solutions that generalize\nbetter [28]. We also understand intuitively why this is the case by examining loss landscapes:\nflat solutions occupy a much greater volume in high dimensions, and are linked to model\ncompressibility [34, 19, 20, 27]. These findings also explain double descent, which can exactly\nbe reproduced with linear models [24, 4, 40]. We now know that stochastic optimization\nplays a limited role in generalization, and it is more the properties of the loss landscape that\ninfluence generalization [16, 5, 22]. We understand benign overfitting, again through analogy\nwith nonparametric statistics \u2014 we wish to embrace models that are highly flexible, but\nhave inductive biases towards simple solutions. Such models can fit noise if there is a strong\nlikelihood signal, but they will avoid high complexity solutions if they can fit the data without\nthem [e.g., 40]. There has been enormous practical progress in developing Bayesian neural\nnetworks for uncertainty representation [e.g., 40, 31, 9]. We have non-vacuous generalization\nbounds for modern neural networks [e.g., 29, 11, 1, 41, 26], which in some cases can even be\nprescriptive [26], and do not necessarily require stochastic networks (contrary to common\nbelief) [18]. We have mathematical mechanisms to encode equivariances [e.g., 8, 13]. We\nhave a nuanced understandings of data augmentation [17]. And these are just a handful of\nreferences that I am most familiar with. There are many more. The list goes on.\nThis is not to say that everything in deep learning is understood. But there has been progress\nand we must properly acknowledge this progress in order for the field to keep moving forward.\nThere are also features of neural networks that are relatively distinct, such as the mode\nconnectivity in the loss landscapes [15].\nI\u2019ve heard it said \u201cwe need a theory of deep learning\u201d. What does that mean? Will we ever\nbe satisfied that we have such a theory? I\u2019m not sure many researchers would actually agree\non what it means. We certainly have a relatively good understanding of many properties of\nneural networks that have been presented as mysterious, as outlined above.\nDesigning controlled experiments to provide a more fundamental understanding of deep neural\n5\nnetworks is a particularly promising direction, since they are often hard to analyze theoretically\nwithout making unrealistic assumptions. Such efforts have been gaining momentum lately\nand need to be supported.\nZachary Lipton: To begin tackling the difficulty of understanding deep learning, I think\nit\u2019s important that we understand what sort of animal deep learning is and what might be\nappropriate comparisons. We might view deep learning as offering a collection of model\nclasses, and view \u201cunderstanding deep learning\u201d as analogous to understanding other model\nclasses that were previously popular among academics and practitioners. We might make\ncomparisons to linear models, kernel methods, etc., and bemoan the comparative lack of\nsatisfying theory to account for deep learning\u2019s capacity to generalize.\nBut these direct comparisons between deep learning and previous approaches strike me as\ndeeply misleading. With deep learning, we typically apply models directly on raw or lightly\nprocessed data. By comparison, the success of linear models (and even kernel methods) in\ncomplex domains have always depended on an extraordinary amount of feature engineering,\nfor which we never possessed any satisfying theoretical account. While we might have felt\nsatisfied as scholars that we \u201cunderstood\u201d something profound about generalization in these\nprevious models, the skeptical view is that this was never where the action was. When I got to\ngrad school (on the eve of the explosion of deep learning into mainstream machine learning),\nI interacted quite a bit with both theorists and practitioners. To hear the theorists speak,\nyou\u2019d think that the entire practice of machine learning lived in the space of the learning\nalgorithm and that we were making tremendous progress. But among practitioners, in data\nmining and computer vision, the adage was that one great (hand-engineered) feature was\nworth far more than any difference in performance among algorithms.\nIn short, I think deep learning couples together the learning of a classifier with the learning of a\nsuitable representation, and this question\u2014of what constitutes a suitable representation\u2014has\nnever been met with much understanding, neither in the deep learning era nor in previous eras.\nThe difference now is that we now consider this to be the responsibility of \u201cthe algorithm\u201d\nrather than externalizing it.\nAs to the usefulness of a theory of deep learning, I expect that to depend heavily on what form\nthat theory takes. One clich\u00e9 form for titling theory papers goes as follows: \u201cSome provable\nbounds for\n\u201d. Should the reader care? That depends on precisely what quantities have\nbeen analyzed and how this analysis relates (or not) to the needs of the practitioner.\nAt risk of sounding pessimistic, I would note that the theory of deep learning hasn\u2019t caught up to\nthe state of deep learning in 2014, certainly not enough to provide a reliable source of guidance\nfor practitioners, or even for researchers interested in developing practical methodology. At\nthe same time, the set of relevant practices has shifted dramatically. Now, we develop models\nfor new tasks, leveraging a motley combination of specialized labeled data, human feedback,\nand pre-trained components drawing on web scale data resources. These kinds of settings\nwhere data is coming from multiple populations, used to fit different model components with\nmany different objectives and combined in unpredictable ways seems especially frustrating for\nthe pursuit of theoretical insights.\nOn a positive note, I don\u2019t think that theory must account for everything that deep learning\ndoes to be useful in deep learning settings. For example, suppose that I want to study a\ndistribution shift problem where the underlying predictor is produced via deep learning. I\nmay not need to explain how deep learning works to develop a coherent theory to tell me what\ncorrective action ought to be taken to adjust in light of shifts under some set of structural\nassumptions. It\u2019s often sufficient to stick the deep learning in a black box and say suppose I\n6\nhave a predictor with certain performance characteristics, and then ask, what can I do with\nthis object?\nPreetum Nakkiran: I would say we actually have made progress towards understanding\ndeep learning, it just hasn\u2019t been as formal, quantitative, provable, and prescriptive as many\n(myself included) had hoped. This isn\u2019t necessarily a problem \u2014 there are many types of\nunderstanding in the natural sciences that don\u2019t satisfy these criteria either (e.g. the theory\nof evolution, or the germ theory of disease). We must thus choose our level of rigor carefully:\ndemanding full mathematical rigor will prevent us from studying many phenomena which are\nreal and interesting, but which are not appropriate for provable (or even quantitative) theories.\nThese considerations are not new \u2014 Turing Award laureate Herbert Simon discussed the\nquestion of \u201cwhat is a theory?\u201d in the context of AI in 1995 [36, Section 4.1], which remains\nrelevant today.\nThere is also a long history of \u201ceffective\u201d and approximate theories in the sciences, that are\nlater refined into more fundamental and precise theories. The usual example is Newton\u2019s laws,\nlater refined by GR and QM. But there are many smaller, less revolutionary examples \u2014 e.g\nBoyle\u2019s law was not exactly correct, and was not physically justified until much later, and yet\nstill captured a real phenomenon that was an important stepping stone for later theories of\nthermodynamics & statistical mechanics. We should similarly be open to starting with less\nrigorous, less mechanistic, less precise theories in deep learning, as a stepping stone to future\ntheories.\nWhy I claim we\u2019ve made progress towards understanding: If we had made no progress, deep\nlearning would be an entirely unpredictable system to us \u2013 we would add 1 more parameter\nto a model, and have absolutely no idea how this new model would behave compared to\nthe previous one. This is not the case \u2013 in fact, practitioners have very strong intuitions\nand heuristics about the effect of many design choices, as evidenced by their continued\npractical success. These intuitions and heuristics are clear evidence of partial understanding.\nCivilization was constructing buildings and bridges long before we had a full theory of solid\nmechanics.\nFor the second question, on whether theory would be useful to practitioners: A rich theory\nwill almost certainly be eventually useful to practitioners, as theories in the sciences usually\nare. The \u201ceventually\u201d is a key qualifier, though. In the meantime, I think trying too hard to\nbe useful to practice is a distraction for scientific theory. In this field, the goal of science is to\nunderstand the nature of learning. The goal of engineering is to build and improve learning\nsystems. These goals are different, and not always aligned. I do think scientists should be\nvery aware of the developments in practice, though \u2013 they are valuable experimental points\nthat theory must eventually capture.\nFinally, a technical remark: There are actually several definitional obstacles to having a\nformal, meaningful, unified theory of deep learning (as I described in my thesis [30]). We\ncurrently lack a precise definition of the objects of study: What precisely is a \u201cdeep network\u201d?\nWe certainly cannot allow any network architecture (which would allow us to simulate a\nTuring Machine, and thus force us to study all of computational learning as \u201cdeep learning\u201d).\nSimilarly we cannot simply enumerate common architectures, because we do not want just a\n\u201ctheory of ResNets\u201d or a \u201ctheory of Transformers\u201d, but rather a theory of deep learning as a\nwhole. There is no existing definition of the set of \u201creasonable architectures\u201d\u2014 which should\ninclude all the ones that are successful in practice now, and which could be successful in the\nfuture. There are similar obstacles to defining the \u201cnatural distributions\u201d on which we apply\ndeep learning, and many other objects we colloquially use.\n7\nWill deep learning ever be interpretable? Does it need to be? Do LLMs need to\nbe interpretable if they can explain themselves in text?\nPreetum Nakkiran: I doubt deep learning will ever be interpretable in the strong sense. I\nthink certain structures within networks may be interpretable, as some existing work shows,\nbut I expect these structures to be fairly weak. For example, I do not expect we will ever have\nthe following \u201cstrong interpretability\u201d: we will never be able to manually write an explicit\nprogram (a Turing machine) which, without seeing any data, performs close to our strongest\ntrained LLMs or vision models. However, I view the current intepretability line of work as a\nvaluable first step towards a \u201ctheory\u201d of deep learning: as intepretability results become more\ngeneral and precise, they essentially evolve into a theory.\nFinally, we should keep in mind that these objections are not specific to deep learning. (Are\ninterpolating kernel methods always interpretable?)\nAndrew Gordon Wilson: Interpretability can mean many different things, and arguably\nthe word is often taken too narrowly to mean understanding the effects of input dimensions\non model outputs.\nBut I disagree with the conventional wisdom that deep learning is\nparticularly uninterpretable, broadly speaking. Is deep learning really much different or\nmore mysterious than other model classes? Why? Overparametrization? Double Descent?\nBenign overfitting? All of these \u201cmysterious\u201d phenomena can be reproduced using other model\nclasses [e.g., 24, 40, 28, 37]. Moreover, the design decisions for many canonical deep learning\narchitectures, such as convolutional neural networks, make a lot of intuitive sense \u2014 hierarchy,\nlocality, translation equivariance, contrast normalization, inspired in part by our biological\nunderstanding of visual receptive fields. . . If that\u2019s not interpretable, what is?\nAlex Smola: Given the fact that nonlinear nonparametric models are, by design, hard\nto understand, we can nonetheless ask the question as to whether there are simple to\nunderstand proxies (e.g. locally linear functions, local decompositions, salient components,\nsparse approximations, etc.), as long as we don\u2019t start believing that said proxies are the\ntruth. For instance, clustering is generally not \u2018true\u2019, yet it can provide a good proxy of what\nthe data is about. See e.g. the leptograpsus and iris datasets. Clustering isn\u2019t the \u2018truth\u2019,\nditto zachary\u2019s karate club, but a pretty good proxy.\nRegarding explainability, some few years ago, SHAP and linearizations were all the rage. The\nidea there for explanation is that a linear function, possibly a linear function locally at f(x)\nwould be most meaningful. But there are hundreds of alternative ways to view this, so the\nanswer will depend on the questions just as much as on the reality. In a way, this is a beautiful\nexample of Plato\u2019s cave.\nKyunghyun Cho: I find interpretability as an extra feature that can be extremely useful\nbut may not be necessary for building and deploying deep learning based systems. In fact, I\nbelieve it\u2019s quite helpful to think of deep learning (or a system based on it) in light of what we\ndo in healthcare and in particular drug discovery. It is quite helpful and useful to know the\nprecise biological, chemical and physical mechanism behind any treatment, because it would\nhelp us determine the breadth of its applicability as well as some of the perils that may come\ntogether with it. However, for most of the drugs and therapeutics, we actually don\u2019t know the\nexact mechanism by which they work in human bodies. How do we then prescribe these drugs\nand treat various conditions? We rely on randomized controlled trials (and their variants) to\nestablish the efficacy of such therapeutics directly, bypassing the necessity of knowing the\n8\nunderlying mechanism. By defining a target condition and target population very carefully,\nwe can often establish any therapeutics\u2019 effectiveness for this particular condition-population\ncombination, although this may require lengthy and painful trials. Can we also approach\napplying and deploying deep learning based systems in this way? I believe it is possible.\nThat said, interpretability is an extremely useful extra feature, as knowing the underlying\nmechanism allows us to more accurately guess to which population and to which conditions\ndrugs would be effective. This can lower the effort in randomized controlled trials as well\nas unanticipated consequences in the future. In other words, interpretability can facilitate\ndramatically improving and increasing the applicability. So, yes, interpretability will be\nextremely helpful.\nIn my view, LLM\u2019s explaining themselves has nothing to do with interpretability. Inter-\npretability is about knowing the underlying mechanism by which certain predictions (in the\ncontext of machine learning) were made, and explanation by an LLM does not guarantee that\nsuch explanation is strongly tied to the actual underlying mechanism of the LLM. This is why\ninterpretability often needs to go together (at least loosely) with identifiability.\nMelanie Mitchell: Interpretability is one of the most important challenges for deep learning\nin general, and LLMs in particular. Interpretability is an essential factor in assessing robustness\nand trustworthiness of AI systems\u2014it is a problem the community needs to solve. LLMs\ncan \u201cexplain themselves\u201d in text, but there is no guarantee that the text \u201cexplanations\u201d they\ngenerate will have anything to do with their actual process of solving problems or making\ndecisions. Humans are not perfect at explaining their own reasoning, but they do have\nsignificant abilities for meta-cognition\u2014this is an important capability that LLMs currently\nlack. Psychology and neuroscience have provided much insight into how humans reason and\nmake decisions. We need to discover the analogous means of teasing out the mechanisms for\nthese capabilities in LLMs and other complex AI models.\nZachary Lipton: This is a thorny question.\nInterpretability may be one of the most\nconfused topics in all of machine learning, fraught with confusion and conflict. To begin,\nthe word is badly overloaded. Read an interpretability paper selected at random and you\u2019ll\nfind representations (or insinuations) that the work is addressing \u201ctrust\u201d, \u201cinsights\u201d, \u201cfairness\u201d,\n\u201ccausality\u201d, \u201cfairness\u201d. Then look at what the authors actually do and you\u2019ll be hard-pressed\nto tie back the method to any of these underlying motivations. Half the papers produce a set\nof feature important scores, describing this \u201cimportance\u201d in cryptic ways: \u201cwhat the model is\nlooking at\u201d, \u201cwhat its internal logic depends on to make this particular prediction\u201d.\nOther papers aim to look inside the neural network, offering vague accounts for the function\nof particular neurons or layers. Then there\u2019s papers that take the \u201cstate of the art\u201d in model\ninterpretability, whatever that might mean, and attempt to evaluate whether these scores are\nactually useful in some real world human-interaction setting where they might ostensibly be\ndeployed. Overall, this in vivo research has produced overwhelmingly negative results. And\nyet the methods are adopted and heavily marketed at full speed anyway.\nAt the heart of it all is an economic tension: there\u2019s a general unease with trusting models\nin various domains. If you take the time to pick apart what precisely underlies this unease,\nyou typically find that it\u2019s a concern not so easily waved off. The economic role of the\ninterpretability method is often to placate a consumer. \u201cOur models are trustworthy because\nwe use Shap/LIME/GradCAM/. . . , a state of the art interpretability method!\u201d The entire\nenterprise has become polarizing, and due to reviewer bidding, this leads to a dangerous\ndynamic in which skeptics self-select out of the peer review cycle, leaving a cycle of affirmation.\n9\nIs deep learning moving beyond academic benchmarking? As we care more\nand more about qualitative model properties, we care less about how many mistakes a\nmodel makes but rather what kinds of mistakes. Academic LLM benchmarks typically\nrequire single-word answers and fail to capture the complexity of real human-machine\ninteractions. Meanwhile, vision benchmarks focus on narrow definitions of accuracy\nwith low-diversity data. Finally, do generative AI models have more complex properties\nlike \u201ccommon sense\u201d and if so, can we design benchmarks that truly capture them?\nAlex Smola: Benchmarking right now is a mess. Again, that\u2019s to be expected. Have a look\nat intelligence tests (as in, for humans) and the controversies in them.\nPreetum Nakkiran: Just a comment (a non-answer): it is my impression that industry has\nbeen considering more fine-grained measures of model behavior for a while, even in \u201cclassical\u201d\nsettings such as image classification \u2013 simply because when ML is deployed in applications,\nbasic QA requires testing the application under different settings, and detailing its failure\nmodes.\nMax Welling: I agree that solid benchmarking will be the key to progress and keeping us\nhonest. And we need to move beyond the simplistic metrics we tend to use in academia.\nWhile designing such metrics may be tricky, we have RL to integrate human feedback back\ninto the system, and so I predict RL will start to play a key role in making these systems\naligned with human values and interests.\nZachary Lipton: I don\u2019t know about \u201cmoving beyond\u201d but I think our relationship to these\nbenchmarks is changing rapidly. For the last ten-fifteen years, the lion\u2019s share of activity in\nnatural language processing consisted in academics proposing benchmarks and then doggedly\ntargeting these benchmarks with a barrage of models. At a 1000-foot view, we lived in a\nworld where one benchmark would summon hundreds of models. Now the script is flipped.\nThe world of methodological innovation is moving away from single-purpose modeling efforts\ntowards general purpose foundation models and somewhat general repertoires of tricks for how\nto apply them. In this new world, one-to-many becomes many-to-one. Each new foundation\nmodel is evaluated against hundreds of benchmarks. These benchmarks are seen less and less\nas the singular aim of a modeling effort and more as one sensor among many for addressing a\nmore general repertoire of capabilities.\nKyunghyun Cho: I don\u2019t think there\u2019s anything special about \u201cacademic\u201d benchmarking.\nThat academic benchmarks typical focus on single-word answers and 0-1 errors is not that we\nhave to do so in academia but is an indication that we have been quite lazy when it comes\nto evaluation. In fact, some people in e.g. natural language processing, including myself,\nhave been working on building better, (semi-)automated evaluation protocols for language\ngeneration and others. For instance, in Wang et al. [39], Alex Wang, who was a PhD student of\nmine back then and is now at Cohere.AI, developed an evaluation protocol for summarization\nsystems using a chain of question-generation and question-answering models. More advanced\nevaluation protocols are often more difficult to develop, implement and deploy, compared to\nsimple surface-level matching protocols, such as 0-1 error, BLEU (weighted average n-gram\nprecision), ROUGE (weighted average n-gram recall), etc., which is why they have not been\n10\nadopted more widely nor studied further. So, no, academic benchmarking is not a thing, and\nwe can do much better on evaluation if we don\u2019t lazily maximize the number of publications.\nAndrew Gordon Wilson: Academic benchmarks, such as ImageNet [10], catalyzed the deep\nlearning revolution. Without those benchmarks, it may have been many more years before the\ncommunity embraced deep learning \u2014 perhaps it wouldn\u2019t have happened even now. There was\nunfortunately a lot of stigma around deep learning, and it took striking results on a benchmark\nthe community broadly understood to start changing minds. But it\u2019s reassuring that the\ncommunity did ultimately change its mind based on empirical evidence. The willingness to\ndo so, despite many doubts, is something I deeply appreciate about the machine learning\ncommunity, and we shouldn\u2019t take it for granted. Moreover, good performance on these\nbenchmarks has served as a proxy for good performance on related problems.\nThese standard benchmarks are now sufficiently well-solved that they are losing their scientific\nvalue. At this point, it is more engineering details and computation that lead to benchmark\ntopping results than innovative ideas.\nIt could be that in order to keep innovating, we need to be tailoring our approaches to more\nspecialized problems, requiring domain expertise. If this were the case, then perhaps there is\nno room any longer for \u201cstandard\u201d benchmarks. However, I believe that many even seemingly\ndisparate real-world problems share sufficient structure [18] that there is mileage in creating\nfresh general purpose benchmarks. Incidentally, and perhaps counterintuitively, fears about\noverfitting to the test sets of academic benchmarks may also be overblown [33, 18].\nI\u2019m hoping we will see continued academic benchmarking, but on fresh benchmarks.\nAre transformers the way forward or are they limited? What are the funda-\nmental limitations of today\u2019s popular paradigms? Do we need a post-deep-learning\nparadigm? Will future improvements come primarily from hardware improvements or\nadvancements in fundamental methods?\nAlex Smola: Are transformers the answer? Obviously not. Just like the Fourier transform\nisn\u2019t the \u2018answer\u2019, but they\u2019re by now baked into most deep learning silicon with great\nacceleration. So we\u2019re stuck with them unless someone figures out another function class that\ncan either significantly beat transformers or that can beat them somewhat and use the same\ntypes of acceleration. It might take another 5 years for this to happen, maybe 10 years due to\nthe inertia of deployed hardware.\nMax Welling: We find ourselves currently in a scaling paradigm, where most of the\nimprovement seems to come from scaling to more data and bigger models. But I predict\nthat will saturate at some point and we will revert back to modeling. This can happen\neither because the data is exhausted (or in some domains not available in large amounts), or\ndealing with very large models becomes too expensive. Machine learning really is a balancing\nact between inductive bias and data, and as such I think at some point we will find other\narchitectures beyond transformers that will scale and have smarter inductive biases embedded\nin them.\nKyunghyun Cho: Transformers were definitely the way forward. In fact, transformers are\nthe culmination of the best practices we\u2019ve discovered over the past three decades; stochastic\ngradient descent, linear shortcut connections, attention, rectified linear units and normalization.\n11\nIt is almost unsurprising how well Transformers work. It is in fact surprising that people are\nsurprised how well Transformers work.\nI don\u2019t know what the future will be nor should be. I believe in the diversity of scientific\ntopics and directions pursued by scientists. So, the answer is \u201call of the above\u201d.\nZachary Lipton: Transformers have been a remarkably powerful architecture, but my overall\nsense is that the role of architectures has been a bit overstated. It\u2019s not clear to me that\nwe couldn\u2019t achieve similar feats by putting a similarly massive push behind something that\nlooked more like a traditional RNN. Of course, Transformers have computational advantages.\nBut I think the real action now has been on the fronts of massively parallel computation\n(e.g. training GPT4 and PaLM on thousands of pods of linked GPU/TPU nodes), web-scale\ndata, and a repertoire of pre-training, instruction fine-tuning, task fine-tuning, and in-context\nlearning, and evaluation routines that have moved us into new terrain.\nPreetum Nakkiran: I don\u2019t think transformers are the end-all-be-all, for at least two\nreasons. First, because it is a very strong statement to claim that a certain learning method is\nglobally optimal over a broad family of distribution, metrics, and design constraints. Second,\nbecause engineered systems are typically always being improved \u2013 we\u2019re still improving on the\nautomobile, for example. An engineered system is unlikely to be optimal, unless it is solving\na very well-defined problem, and hitting fundamental physical limits (e.g. Carnot engines). I\nam optimistic that a better understanding of which parts of the transformer architecture are\ncrucial, and why, will lead us to better architectures which directly exploit these advantages.\nAndrew Gordon Wilson: They are the way forward, and they are limited. Transformers\nshow us it is possible to develop relatively general purpose algorithms, capable of solving\nproblems in a remarkably wide array of domains, in contrast to what is often implied from\nthe \u201cno free lunch theorems\u201d [18]. But, at least in their current form, they are not the \u201cfinal\nstage of evolution\u201d for learning algorithms. For example, they are relatively data hungry.\nPerhaps research showing how to include soft priors, without constraining expressivity, will\nhelp resolve this issue [12]. Integrating planning will also be an important future direction\nfor these models. Multimodal learning is also the future \u2014 I\u2019ll stick my neck out and say\nin five years a majority of applied papers at NeurIPS, ICML, and ICLR will be considering\nmultimodal datasets, and we still stop thinking about vision and NLP so much as distinct\nproblems. While transformers can handle multimodal data, we will likely develop non-trivially\ndifferent architectures to accommodate new modalities and fusion of modalities. For LLMs,\nwe may also move away from autoregressive models. I also don\u2019t think text alone is the path\ntowards general purpose intelligence.\nMelanie Mitchell: Whether transformers are the \u201cway forward\u201d depends on what the goal\nis. For something like \u201chuman-level intelligence,\u201d it\u2019s clear that the architecture and learning\nmethods of today\u2019s transformers have many differences from biological brains and learning\nmethods. Here are some central aspects of human brains/minds and learning methods that\nare missing (or nascent) in transformers.\n\u2022 Episodic memory \u2014 capability of long-term storage (and context-sensitive retrieval) of\nexperiences during one\u2019s lifetime\n\u2022 Metacognition \u2014 being able to report on and reason about one\u2019s own thinking.\n\u2022 Ability to dynamically simulate / imagine possible physical and social situations, reason\nabout them, make predictions\n12\n\u2022 Active interaction with the world / environment \u2014 choosing what to learn from, what\ninterventions to make to test predictions and adjust internal models\n\u2022 Sensory systems, multimodal integration\n\u2022 Motor systems, integration of brain and body\n\u2022 These are just some examples; there are many other salient differences between biological\nand (current) artificial intelligence.\nAre academics screwed?\nKyunghyun Cho: No.\nPreetum Nakkiran: Depends on how you define academic\nAlex Smola: Obviously not. A lot of things are possible in academia that are difficult\nin industry. Taking risks, taking risks with datasets, and also, asking questions such as\nexplainability and interpretability. Ditto optimizing algorithms. That said, deep learning\nresearch is starting to look a lot more like the work in biological wetlabs or experimental\nphysics. All of them require significant capital investment to work. Once upon a time, you\nonly needed pen and pencil to make something work. This is only less the case by now.\nMax Welling: Not at all. But they may be playing a different game. Perhaps the engineering\nrelated to scaling is best left to industry, but there are very many interesting fundamental,\ntheoretical, exploratory problems that can be solved better in an academic setting. For\ninstance, to really improve AI for Science, we need engineers, machine learning researchers\nand scientists (e.g. physicists, chemists, biologists). The best place where the cr\u00e8me de la\ncr\u00e8me of these disciplines comes together is academia. I would say, if you are in academia,\npick your problem wisely!\nAndrew Gordon Wilson: No. For the most part, industry and universities (or \u201cacademia\u201d,\nmore broadly) play complementary rather than competing roles.\nIt\u2019s true that the approach to building bigger models trained on more data has been fruitful,\nis more suited to industry resources, and not particularly academic.\nBut suppose you are modelling a scientific problem where you know you want to constrain your\nneural network to respect a symmetry (e.g., rotation). Perhaps you want to study molecules,\nwhich have no canonical orientation or coordinate system. That\u2019s not a question that should\nbe solved by brute force scale. It\u2019s answered by careful consideration, and mathematics [8].\nIndeed, many recent machine learning advances have been driven by academia \u2014 even in areas\nmost crucial to real-world adoption, such as robustness to spurious correlations and distribution\nshifts, uncertainty representation, and online decision making [e.g., 35, 23, 2, 21, 25, 40, 38, 3].\nMoreover, universities are well-suited to open-ended long-term fundamental research that\nwould be hard to \u201csell\u201d to an industry manager or align with short-term company priorities.\nThere will always be an important role for fundamental research. To come full circle, surely\nscientific discovery is an academic endeavour.\nZachary Lipton: In a holistic sense, obviously not! Or if we are screwed, the threat may be\nas much from online lecture material and ChatGPT attacking the necessity and undermining\n13\nthe integrity of coursework as it is from our ability to compete in the research landscape.\nThere are so many profound problems for scholars to work on and while building the biggest\nfoundation models may be a fundamentally important activity and is doubtless driving a lot\nof the action, it\u2019s (i) just one activity and (ii) arguably not the research line most compatible\nwith the unique advantages of academia. I expect the frontiers of research on the statistical\nfoundations of ML, on causal inference and related decision-making disciplines, etc. to remain\nin academia. If you take a narrow view of \u201cacademics\u201d as leading the industrial edge of deep\nlearning then sure academics might be screwed, and those of us with dual affiliations might be\nhalf/screwed. The situation, at worst, could be likened to the status of information retrieval\nresearch. Prior to Google, there was a time where the most exciting work on web search took\nplace in the hall of academia, and eventually we became somewhat obsolete, keepers of a\npublic body of work that lagged grotesquely far behind the vanguard of industry. But that\nwas OK. Search became (mostly) the province of industry, and academics focused on other\nexciting directions (like neural networks). A primary strength of academia is providing a lane\nto focus intensely on problems that don\u2019t have the clear path and immediate economic impact\nrequired by industrial R&D. Academics as a whole aren\u2019t screwed, but many might need to\nrefocus given the shifting landscape (or move into industry).\nChatGPT-3.5: Academics aren\u2019t screwed, they\u2019re just upgrading their syllabus from\n\u201cOld McDonald Had a Data Set\u201d to \u201cGPT-3.5 Ways to Impress Your Robot Overlords.\u201d\nChatGPT-4: Oh, such melodrama. \u201cScrewed,\u201d you say? Like suddenly, all academic\nresearchers are going to disappear into a void, swallowed whole by the corporate behemoth?\nWhat an imaginative dystopia you\u2019ve conjured up!\nLet me dispel that illusion for you. Yes, it\u2019s true that the industrial tech giants have been\nflexing their muscles with large-scale language models. It\u2019s also true that they have buckets\nof money to grease the wheels of their research. But let\u2019s not pretend that this spells the end\nfor academic machine learning researchers.\nAcademic research isn\u2019t just about being the first to train a massive language model or the\nability to throw wads of cash at a problem. It\u2019s about creating knowledge, asking fundamental\nquestions, exploring new areas not yet colonized by the profit-oriented industrial labs.\nWhile industry labs are racing to build bigger and glossier models, academic researchers\nplay a vital role in providing the much-needed critical lens, asking questions about ethics,\ninterpretability, robustness, and social implications.\nThey also innovate in other areas\nwhere resource constraints actually foster creativity and inventiveness. So, screwed? Hardly.\nChallenged? Definitely. But adversity, my dear inquirer, often sparks the most brilliant flames\nof creativity and resilience. So let\u2019s not start drafting the obituary for academic machine\nlearning research just yet. It\u2019s got a lot more fuel left in the tank. If anything, this could be\nthe wakeup call needed to stop the homogenization of research and encourage a more diverse\necosystem of ideas.\nEditors and Contributors\nEditors:\n\u2022 Micah Goldblum - New York University, goldblum@nyu.edu\n\u2022 Anima Anandkumar - California Institute of Technology, NVIDIA, arangelf@caltech.\nedu\n14\n\u2022 Richard Baraniuk - Rice University, richb@rice.edu\n\u2022 Tom Goldstein - University of Maryland, tomg@umd.edu\nContributors:\n\u2022 Kyunghyun Cho - New York University, Prescient Design, kyunghyun.cho@nyu.edu\n\u2022 Zachary Lipton - Carnegie Mellon University, zlipton@cmu.edu\n\u2022 Melanie Mitchell - Santa Fe Institute, mm@santafe.edu\n\u2022 Preetum Nakkiran - Apple, preetum@nakkiran.org\n\u2022 Alex Smola - Boson AI, alex@smola.org\n\u2022 Max Welling - University of Amsterdam, M.Welling@uva.nl\n\u2022 Andrew Gordon Wilson - New York University, andrewgw@cims.nyu.edu\n\u2022 ChatGPT\n- OpenAI\nReferences\n[1] Pierre Alquier.\nUser-friendly introduction to pac-bayes bounds.\narXiv preprint\narXiv:2110.11216, 2021.\n[2] Martin Arjovsky, L\u00e9on Bottou, Ishaan Gulrajani, and David Lopez-Paz. Invariant risk\nminimization. arXiv preprint arXiv:1907.02893, 2019.\n[3] Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, An-\ndrew G Wilson, and Eytan Bakshy. Botorch: A framework for efficient monte-carlo\nbayesian optimization. Advances in neural information processing systems, 33:21524\u2013\n21538, 2020.\n[4] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern\nmachine-learning practice and the classical bias\u2013variance trade-off. Proceedings of the\nNational Academy of Sciences, 116(32):15849\u201315854, 2019.\n[5] Ping-yeh Chiang, Renkun Ni, David Yu Miller, Arpit Bansal, Jonas Geiping, Micah\nGoldblum, and Tom Goldstein.\nLoss landscapes are all you need: Neural network\ngeneralization can be explained without the implicit bias of gradient descent. In The\nEleventh International Conference on Learning Representations, 2022.\n[6] F. Chollet. The Abstraction and Reasoning Corpus (ARC). https://github.com/\nfchollet/ARC, 2023. Accessed 2023-11-09.\n[7] Fran\u00e7ois Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019.\n[8] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International\nconference on machine learning, pages 2990\u20132999, 2016.\n[9] Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias\nBauer, and Philipp Hennig. Laplace redux-effortless bayesian deep learning. Advances in\nNeural Information Processing Systems, 34:20089\u201320103, 2021.\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A\nlarge-scale hierarchical image database. In 2009 IEEE conference on computer vision\nand pattern recognition, pages 248\u2013255. Ieee, 2009.\n15\n[11] Gintare Karolina Dziugaite and Daniel M Roy. Computing nonvacuous generalization\nbounds for deep (stochastic) neural networks with many more parameters than training\ndata. arXiv preprint arXiv:1703.11008, 2017.\n[12] Marc Finzi, Gregory Benton, and Andrew G Wilson.\nResidual pathway priors for\nsoft equivariance constraints. Advances in Neural Information Processing Systems, 34:\n30037\u201330049, 2021.\n[13] Marc Finzi, Max Welling, and Andrew Gordon Wilson. A practical method for con-\nstructing equivariant multilayer perceptrons for arbitrary matrix groups. In International\nconference on machine learning, pages 3318\u20133328. PMLR, 2021.\n[14] Michael C Frank. Baby steps in evaluating the capacities of large language models.\nNature Reviews Psychology, pages 1\u20132, 2023.\n[15] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P Vetrov, and Andrew G\nWilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. Advances in\nneural information processing systems, 31, 2018.\n[16] Jonas Geiping, Micah Goldblum, Phillip E Pope, Michael Moeller, and Tom Goldstein.\nStochastic training is not necessary for generalization. arXiv preprint arXiv:2109.14119,\n2021.\n[17] Jonas Geiping, Micah Goldblum, Gowthami Somepalli, Ravid Shwartz-Ziv, Tom Gold-\nstein, and Andrew Gordon Wilson. How much data are augmentations worth? an\ninvestigation into scaling laws, invariance, and implicit regularization. arXiv preprint\narXiv:2210.06441, 2022.\n[18] Micah Goldblum, Marc Finzi, Keefer Rowan, and Andrew Gordon Wilson. The no\nfree lunch theorem, kolmogorov complexity, and the role of inductive biases in machine\nlearning. arXiv preprint arXiv:2304.05366, 2023.\n[19] Geoffrey E Hinton and Drew Van Camp.\nKeeping the neural networks simple by\nminimizing the description length of the weights. In Proceedings of the sixth annual\nconference on Computational learning theory, pages 5\u201313, 1993.\n[20] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Flat minima. Neural computation, 9(1):1\u201342,\n1997.\n[21] Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew G Wilson. Dangers of bayesian\nmodel averaging under covariate shift. Advances in Neural Information Processing Systems,\n34:3309\u20133322, 2021.\n[22] Pavel Izmailov, Sharad Vikram, Matthew D Hoffman, and Andrew Gordon Gordon\nWilson. What are bayesian neural network posteriors really like?\nIn International\nconference on machine learning, pages 4629\u20134640, 2021.\n[23] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon Wilson. Last layer re-training is\nsufficient for robustness to spurious correlations. In International Conference on Learning\nRepresentations, 2022.\n[24] Anders Krogh and John A Hertz. Generalization in a linear perceptron in the presence\nof noise. Journal of Physics A: Mathematical and General, 25(5):1135, 1992.\n16\n[25] Evan Z Liu, Behzad Haghgoo, Annie S Chen, Aditi Raghunathan, Pang Wei Koh, Shiori\nSagawa, Percy Liang, and Chelsea Finn. Just train twice: Improving group robustness\nwithout training group information. In International Conference on Machine Learning,\npages 6781\u20136792. PMLR, 2021.\n[26] Sanae Lotfi, Marc Finzi, Sanyam Kapoor, Andres Potapczynski, Micah Goldblum,\nand Andrew G Wilson. Pac-bayes compression bounds so tight that they can explain\ngeneralization. Advances in Neural Information Processing Systems, 35:31459\u201331473,\n2022.\n[27] David JC MacKay. Information theory, inference and learning algorithms. Cambridge\nUniversity Press, 2003.\n[28] Wesley J Maddox, Gregory Benton, and Andrew Gordon Wilson.\nRethinking pa-\nrameter counting in deep models: Effective dimensionality revisited. arXiv preprint\narXiv:2003.02139, 2020.\n[29] David A McAllester. Some pac-bayesian theorems. In Proceedings of the eleventh annual\nconference on Computational learning theory, pages 230\u2013234, 1998.\n[30] Preetum Nakkiran. Towards an empirical theory of deep learning. PhD thesis, Harvard\nUniversity, 2021.\n[31] Kazuki Osawa, Siddharth Swaroop, Mohammad Emtiyaz E Khan, Anirudh Jain, Runa\nEschenhagen, Richard E Turner, and Rio Yokota. Practical deep learning with bayesian\nprinciples. Advances in neural information processing systems, 32, 2019.\n[32] Andres Potapczynski, Marc Finzi, Geoff Pleiss, and Andrew Gordon Wilson. CoLA:\nExploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra.\narXiv preprint arXiv:2309.03060, 2023.\n[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet\nclassifiers generalize to imagenet? In International conference on machine learning, pages\n5389\u20135400, 2019.\n[34] W Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, JK Terry, Furong Huang,\nand Tom Goldstein. Understanding generalization through visualizations. arXiv e-prints,\npages arXiv\u20131906, 2019.\n[35] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally\nrobust neural networks for group shifts: On the importance of regularization for worst-case\ngeneralization. arXiv preprint arXiv:1911.08731, 2019.\n[36] Herbert A Simon. Artificial intelligence: an empirical science. Artificial intelligence, 77\n(1):95\u2013127, 1995.\n[37] Samuel L Smith and Quoc V Le. A bayesian perspective on generalization and stochastic\ngradient descent. arXiv preprint arXiv:1710.06451, 2017.\n[38] Jasper Snoek, Hugo Larochelle, and Ryan P Adams. Practical bayesian optimization\nof machine learning algorithms. Advances in neural information processing systems, 25,\n2012.\n[39] Alex Wang, Kyunghyun Cho, and Mike Lewis. Asking and answering questions to\nevaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 5008\u20135020, 2020.\n17\n[40] Andrew G Wilson and Pavel Izmailov.\nBayesian deep learning and a probabilistic\nperspective of generalization. Advances in neural information processing systems, 33:\n4697\u20134708, 2020.\n[41] Wenda Zhou, Victor Veitch, Morgane Austern, Ryan P Adams, and Peter Orbanz.\nNon-vacuous generalization bounds at the imagenet scale: a pac-bayesian compression\napproach. arXiv preprint arXiv:1804.05862, 2018.\n18\n"
  }
]