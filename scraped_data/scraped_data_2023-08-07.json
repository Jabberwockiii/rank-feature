[
  {
    "title": "Retroformer: Retrospective Large Language Agents with Policy Gradient Optimization",
    "link": "https://arxiv.org/pdf/2308.02151.pdf",
    "upvote": "18",
    "text": "RETROFORMER: RETROSPECTIVE LARGE LANGUAGE\nAGENTS WITH POLICY GRADIENT OPTIMIZATION\nWeiran Yao\u2020\nShelby Heinecke\u2020\nJuan Carlos Niebles\u2020\nZhiwei Liu\u2020\nYihao Feng\u2020\nLe\nXue\u2020\nRithesh Murthy\u2020\nZeyuan Chen\u2020\nJianguo Zhang\u2020\nDevansh Arpit\u2020\nRan Xu\u2020\nPhil Mui\u22c4\nHuan Wang\u2020,\u2217\nCaiming Xiong\u2020,\u2217\nSilvio Savarese\u2020,\u2217\n\u2020Salesforce Research\n\u22c4CTO Office, Salesforce\n\u2217Corresponding Authors: {huan.wang, cxiong, ssavarese}@salesforce.com\nABSTRACT\nRecent months have seen the emergence of a powerful new trend in which large\nlanguage models (LLMs) are augmented to become autonomous language agents\ncapable of performing objective oriented multi-step tasks on their own, rather than\nmerely responding to queries from human users. Most existing language agents,\nhowever, are not optimized using environment-specific rewards. Although some\nagents enable iterative refinement through verbal feedback, they do not reason and\nplan in ways that are compatible with gradient-based learning from rewards. This\npaper introduces a principled framework for reinforcing large language agents\nby learning a retrospective model, which automatically tunes the language agent\nprompts from environment feedback through policy gradient. Specifically, our\nproposed agent architecture learns from rewards across multiple environments\nand tasks, for fine-tuning a pre-trained language model which refines the lan-\nguage agent prompt by summarizing the root cause of prior failed attempts and\nproposing action plans. Experimental results on various tasks demonstrate that\nthe language agents improve over time and that our approach considerably out-\nperforms baselines that do not properly leverage gradients from the environment.\nThis demonstrates that using policy gradient optimization to improve language\nagents, for which we believe our work is one of the first, seems promising and\ncan be applied to optimize other models in the agent architecture to enhance agent\nperformances over time.\n1\nINTRODUCTION\nRecently, we have seen the emergence of a powerful new trend in which large language models\n(LLMs) are augmented to become autonomous language agents capable of performing tasks on\ntheir own, ultimately in the service of a goal, rather than simply responding to queries from human\nusers. Prominent studies, including ReAct (Yao et al., 2023), Toolformer (Schick et al., 2023), Hug-\ngingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al., 2021),\nAutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023), have suc-\ncessfully showcased the viability of creating autonomous decision-making agents by leveraging the\ncapabilities of LLMs. These approaches use LLMs to generate text-based outputs and actions that\ncan be further employed for making API calls and executing operations within a given environment.\nGiven the immense scale of LLMs with an extensive parameter count, the behaviors of most existing\nlanguage agents, however, are not optimized or aligned with environment reward functions. An\nexception is a very recent language agent architecture, namely Reflexion (Shinn et al., 2023), and\nseveral other related work, e.g., Self-Refine (Madaan et al., 2023b) and Generative Agent (Park\net al., 2023), which use verbal feedback, namely self-reflection, to help agents learn from prior\nfailure. These reflective agents convert binary or scalar reward from the environment into verbal\nfeedback in the form of a textual summary, which is then added as additional context to the prompt\nfor the language agent. The self-reflection feedback acts as a semantic signal by providing the agent\n1\narXiv:2308.02151v1  [cs.CL]  4 Aug 2023\nwith a concrete direction to improve upon, helping it learn from prior mistakes and prevent repetitive\nerrors to perform better in the next attempt.\nAlthough the self-reflection operation enables iterative refinement, generating useful reflective feed-\nback from a pre-trained, frozen LLM is challenging, as showcased in Fig. 1, since it requires the\nLLM to have a good understanding of where the agent made mistakes in a specific environment, i.e.,\nthe credit assignment problem (Sutton & Barto, 2018), as well as the ability to generate a summary\ncontaining actionable insights for improvement. The verbal reinforcement cannot be optimal, if the\nfrozen language model has not been properly fine-tuned to specialize in credit assignment problems\nfor the tasks in given environments. Furthermore, the existing language agents do not reason and\nplan in ways that are compatible with differentiable, gradient-based learning from rewards by ex-\nploiting the existing abundant reinforcement learning techniques. To address these limitations, this\npaper introduces Retroformer, a principled framework for reinforcing language agents by learn-\ning a plug-in retrospective model, which automatically refines the language agent prompts from\nenvironment feedback through policy optimization. Specifically, our proposed agent architecture\ncan learn from arbitrary reward information across multiple environments and tasks, for iteratively\nfine-tuning a pre-trained language model, which refines the language agent prompts by reflecting on\nfailed attempts and assigning credits of actions taken by the agent on future rewards.\nLollipop Chainsaw featured Juliet Starling, who was voiced by a Canadian-American actress who has done voice \nroles for what Teen Titans spinoff series?\n1. Task instruction\nAction 1: Search[Juliet Starling] \nAction 2: Search[Lollipop Chainsaw] \nAction 3: Search[Tara Strong] \nAction 4: Finish[Teen Titans and Teen Titans Go!]\n2. Action sequences in prior trial\nI should have searched for Lollipop Chainsaw first and looked up the Canadian-American actress who voiced \nJuliet Starling afterwards. I also should have looked up Tara Strong's filmography and searched for any voice \nroles she did specifically for Teen Titans or Teen Titans Go!\n3. Verbal feedback (self-reflection)\nAction 1: Search[Lollipop Chainsaw] \nAction 2: Search[Tara Strong] \nAction 3: Finish[Teen Titans, Teen Titans Go!] \n4. Action sequences in next trial\n+\nadd to agent prompt\nFigure 1: An example of uninformative self-reflections from a frozen LLM. The root cause of failure\nin prior trial is that the agent should have responded only the spinoff series \u201cTeen Titans Go\u201d not\n\u201cTeen Titans\u201d as the answer. The agent forgot its goal during a chain of thought, actions and lengthy\nobservations. The verbal feedback from frozen LLM, however, only rephrases the prior actions\nsequences as the proposed new plan, resulting the same, incorrect actions in the next trial.\nWe conduct experiments on open-source simulation and real-world environments including Hot-\nPotQA (Yang et al., 2018), which consists of search-based question answering tasks to test the tool\nuse abilities of a web agent which needs to call Wikipedia APIs in multiple steps for answering ques-\ntion. We observe Retroformer agents are faster learners compared with reflexion, which does\nnot use gradient for reasoning and planning, and better decision-makers and reasoners. More con-\ncretely, Retroformer agents improve the success rate of search-based question-anwering tasks\nin HotPotQA (Yang et al., 2018) by 18% in 4 attempts, which demonstrates the effectiveness of\ngradient-based reasoning and planning for tool use under large state-action space.\nTo summarize, our contributions are the following:\n\u2022 The paper introduces Retroformer, which iteratively refines the prompts given to large lan-\nguage agents based on environmental feedback to improve learning speed and task completion.\nWe take a policy gradient approach with the Actor LLM being part of the environment, allowing\nlearning from a wide range of reward signals for diverse tasks and environments.\n\u2022 The proposed method focuses on enhancing the retrospective model in the language agent archi-\ntecture, without accessing the Actor LLM parameters or needing to propagate gradients through\nit. The agnostic nature of Retroformer makes it a flexible plug-in module for various types of\ncloud-based LLMs, such as GPT or Bard.\n2\n2\nRELATED WORK\nAutonomous Language Agents\nWe summarize in Table 1 the recent language agent literature\nrelated to our work from five perspectives and differentiate our method from them. The completion\nof a complex task typically involves numerous stages. An agent must possess knowledge of these\nstages and plan accordingly. Chain-of-Thoughts or CoT (Wei et al., 2022) is the pioneering work\nthat prompts the agent to decompose challenging reasoning tasks into smaller, more manageable\nsteps. ReAct (Yao et al., 2023), on the other hand, proposes the exploitation of this reasoning\nand acting proficiency within LLM to encourage interaction with the environment (e.g. using the\nWikipedia search API) by mapping observations to the generation of reasoning and action traces\nor API calls in natural language. This agent architecture has spawned various applications, such\nas HuggingGPT (Shen et al., 2023), generative agents (Park et al., 2023), WebGPT (Nakano et al.,\n2021), AutoGPT (Gravitas, 2023), BabyAGI (Nakajima, 2023), and Langchain (Chase, 2023).\nTable 1: Related work on large language agents.\nApproach\nGradient\nArbitrary\nIterative\nHidden\nDecision\nMemory\nlearning\nreward\nrefinement\nconstraints\nmaking\nCoT (Wei et al., 2022)\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\n\u2717\nReAct (Yao et al., 2023)\n\u2717\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\nSelf-refine (Madaan et al., 2023b)\n\u2717\n\u2717\n\u2713\n\u2717\n\u2717\n\u2717\nRAP (Hao et al., 2023)\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\nReflexion (Shinn et al., 2023)\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\n\u2713\nRetroformer (our method)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nHowever, these approaches fail to learn from valuable feedback, such as environment rewards, to en-\nhance the agent\u2019s behaviors, resulting in performances that are solely dependent on the quality of the\npre-trained LLM. Self-refine (Madaan et al., 2023a) addresses this limitation by employing a single\nLLM as a generator, refiner, and provider of feedback, allowing for iterative refinement of outputs.\nHowever, it is not specifically tailored for real-world task-based interaction with the environment.\nOn the other hand, RAP (Hao et al., 2023) repurposes the LLM to function as both a world model\nand a reasoning agent. It incorporates Monte Carlo Tree Search for strategic exploration within the\nextensive realm of reasoning with environment rewards. This approach enables effective naviga-\ntion and decision-making in complex domains. Recently, Shinn et al. (2023) presents Reflexion, a\nframework that equips agents with dynamic memory and self-reflection capabilities, enhancing their\nreasoning skills. Self-reflection plays a pivotal role, allowing autonomous agents to iteratively refine\npast actions, make improvements, and prevent repetitive errors.\nTransformer Reinforcement Learning\nReinforcement learning with a provided reward function\nor a reward-labeled dataset, commonly referred to as RLHF, has become a standard practice within\nthe LLM fine-tuning pipeline. These endeavors have convincingly demonstrated the efficacy of RL\nas a means to guide language models towards desired behaviors that align with predefined reward\nfunctions encompassing various domains, including machine translation, summarization, and gen-\nerating favorable reviews. Among the prevalent transformer RL methods are online RL algorithms\nsuch as Proximal Policy Optimization or PPO (Schulman et al., 2017), and offline RL techniques\nsuch as Implicit Language Q-Learning or ILQL (Snell et al., 2022) and Direct Preference Optimiza-\ntion or DPO (Rafailov et al., 2023). These methods have been implemented in TRL/TRLX (von\nWerra et al., 2020) distributed training framework.\n3\nCHALLENGES\nAlthough LLMs are not designed to handle tool use or take actions, it has been observed (Gravitas,\n2023; Nakajima, 2023; Chase, 2023) that empirically for text-rich environment, especially when the\nactions and states are accurately described using natural languages, LLMs work surprisingly well.\nHowever there are still plenty of challenges applying LLM-based agents. Here we list several below.\nSpurious Actions\nLLMs are not pre-trained or designed with an action-agent application in mind.\nEven some restrictions are explicitly specified in the prompt, the LLM model may still generate\nspurious actions that are not in the action space A.\n3\nLimited Prompt Length\nLLM itself is stateless. However, in applications it is preferred to em-\npower agents with states or memories for better performance. It has been observed that LLM based\nagents are easy to run into infinite loops if the states are not handled nicely. Many LLM agents\nconcatenate all the previous state descriptions and actions into the prompt so that LLM as a way to\nbestow \u201dstate\u201d to the LLM. Inevitably this methodology runs into the prompt length issues. As the\ntrajectory grows longer, the prompt runs out of spaces.\nHeuristic Prompt Engineering\nEven though a lot of paradigms have been proposed to improve\nLLM agents\u2019 performance (Yao et al., 2023; Ahn et al., 2022), there is a lack of systematic method-\nologies for consistent model refinement. In fact, manual prompt tuning is still widely used in a lot\nof the application scenarios.\nProhibitive Training\nMost of the well-performing LLMs are too large to be fit in just one or two\nGPUs. It is technically challenging to optimize the LLMs directly as is done in the the classical\nreinforcement learning setting. In particular, OpenAI has not provided any solution for RL based\nfinetuning. Most of the issues are caused by the fact that LLMs are not pre-trained or designed with\nan action-agent application in mind.\n4\nINTUITION\nCompared to the LLM-based action agents, classical RL agents, though not able to handle text-based\nenvironments as nicely in the zero shot setting, are able to keep improving based on the feedback\nand rewards provided by the environment. Popular RL algorithms include Policy Gradient (Sutton\net al., 2000), Proximal Policy Optimization Algorithm (PPO) (Schulman et al., 2017), Trust Region\nPolicy Optimization (TRPO) (Schulman et al., 2015), and Advantage Actor Critic methods (Mnih\net al., 2016).\nIn this draft we are proposing a simple but powerful novel framework to tackle the challenges men-\ntioned above. On one hand, we would like to leverage the classical RL based optimization algorithms\nsuch as policy gradient to improve the model performance. On the other hand, our framework avoids\nfinetuning on the LLM directly. The key is, instead of training the LLM directly, we train a retro-\nspective LM. The retrospective LM takes users\u2019 prompt, rewards and feedback from the environment\nas input. Its output will be prompt for the actual LLM to be consumed. RL algorithms are employed\nto optimize the weights in the retrospective LM model instead of directly on the LLM. In our frame-\nwork the weights in the actual LLM is assumed to be fixed (untrainable), which aligns well with the\napplication scenario when the LLM is either too large to tune or prohibited from any tuning.\nAnother perspective viewing our framework is, we train a retrospective LM to apply automatic\nprompt tuning for the LLM agents. In this case, the RL algorithms such as policy gradients are\nemployed to optimize the prompts. Ideally the retrospective LM can help summarize the past \u201cex-\nperience\u201d, the users\u2019 prompt, the environments\u2019 feedback into a condensed text with length limit\nso that it is easier for the LLM to digest. To some extent, in our setting the original LLM can be\nconsidered as part of the environment since its parameters are all fixed.\n5\nNOTATION AND FORMULATION\nWe denote a large language model (LLM) based agent as a function L\u03bel : M \u2192 A, where M\nis the space of prompts, which may include the actual prompts mu provided by the users, as well\nas some contextual information c \u2208 C. Here C is the space of context as a representation of the\ncurrent state S returned by the environment \u2126. A is the space of actions. Note the actions taken\nby most language model based agents are sampled auto-repressively, so L is a random function.\nThe subscript \u03bel denotes the re-parameterized random variables involved in the sampling process.\nAnother note is, the LLM-based agent itself is stateless. All the states and possible memorization\nare characterized as text in the prompt.\nThe environment is defined as a tuple (T\u03beo, R). T\u03beo : S \u00d7 A \u2192 S is the state transition function,\nwhere S is the space of states and A is the action space. Here we assume the states and actions are\nrepresented using text. Again we used \u03beo to represent the randomness involved in the state transition.\n4\nFor each state s \u2208 S, a reward function is defined as R : S \u2192 R. At each step of the play, the state s\nis described using natural language, and integrated into the context c. In the context, previous states\nmay also be described and embedded to help LLMs making a good guess on the next action to take.\nAs in all the reinforcement learning setting, the final goal is to maximize the cumulative rewards, or\nreturns Gcum = PT\nt=0 R(st). In many situations, the rewards are sparse, i.e., R(st) are mostly zero\nexcept very few states.\nThe retrospective model takes the all the previous states s1,\u00b7\u00b7\u00b7 ,t, actions a1,\u00b7\u00b7\u00b7 ,t, rewards r1,\u00b7\u00b7\u00b7 ,t, and\nthe user prompt mu as input, and massage those information into a new prompt m to be consumed\nby the LLM agent:\n\u0393\u03ber,\u0398 : [Si, Ai, Ri, Mu\ni ]t\ni=1 \u2192 M,\n(1)\nwhere \u03ber stands for the randomness involved in the retrospective model, and \u0398 is the set of learnable\nparameters in the retrospective model. The goal of the RL optimization is\narg max\n\u0398\nE\u03bel,\u03beo,\u03ber\n\" T\nX\nt=1\nR(st)\n#\ns.t.\nst+1 = T\u03beo\n\u0000st, L\u03bel \u25e6 \u0393\u03ber,\u0398\n\u0000[si, ai, ri, mu\ni ]t\ni=1\n\u0001\u0001\n,\n\u2200t \u2208 {1, \u00b7 \u00b7 \u00b7 , T \u2212 1}\n(2)\nNote that the only learnable parameters are in the retrospective model Mr. Since LLM agent model\nis fixed, it can be considered as a component of the environment. Specifically, if we construct\nanother environment with the transition function T \u2032 = T (S, \u2022) \u25e6 L : S \u00d7 M \u2192 S, and the same\nreward function R, then the objective in Eq. (2) is just a regular RL optimization so all the popular\nRL algorithms apply.\n6\nOUR APPROACH: REINFORCING RETROSPECTIVE LANGUAGE AGENT\nAs illustrated in Fig. 2, our proposed framework Retroformer is comprised of two language\nmodel components: an actor LLM, denoted as Ma, which generates reasoning thoughts and actions,\nand a retrospective LLM, denoted as Mr, which generates verbal reinforcement cues to assist the\nactor in self-improvement by refining the actor prompt with reflection responses.\nActor LM\nTrajectory\n(s1, a1, r1, \u2026, st)\nAction at\nEnvironment\n1,2,3\u2026K\n(at,rt,st+1)\nEnvironment 1\nEnvironment 2\nEnvironment K\n\u2026\nRetrospective \nLM\nEpisode\nReturns\nPrompt\nRating for reflection response k \nr = \u2206G!,#= G!,#$% \u2212 G!,#\n(a) Retrospective agent\n(b) Ratings for reflection responses\nEnv 1 Returns \nG1,i\nEnv 2 Returns\nG2,i\nEnv K Returns\nGk,i\n\u2026\nEnv 1 Returns\nG1,i+1\nEnv 2 Returns\nG2,i+1\nEnv K Returns\nGk,i+1\n\u2026\nTrial i\nTrial i+1\nReflection response 1\nReflection response 2\nReflection response K\nReflection\nresponse\nReflection\nprompt\nFigure 2: Framework overview.\nWe assume in this paper that the actor model is a frozen LLM whose model parameters are in-\naccessable (e.g., GPT, Bard) and the retrospective model is a smaller, local language model that\ncan be fine-tuned under low-resource settings. In addition, Retroformer has an iterative pol-\nicy gradient optimization step which is specifically designed to reinforce the reflection model with\ngradient-based approach. We provide in this section a detailed description of each of these modules\nand subsequently elucidate their collaborative functioning within the Retroformer framework.\nThe implementation details are presented in Section 7.2.\n5\n6.1\nRETROSPECTIVE AGENT ARCHITECTURE\nAs illustrated in Fig. 2(a), for the base actor model and retrospective model, we apply a standard\ncommunication protocol modified from the Relexion agent architecture (Shinn et al., 2023), in which\nthe retrospective model refines the actor prompt by appending verbal feedback to the prompt.\nActor Model\nThe actor model is a LLM hosted in the cloud, whose model parameters are hidden\nand frozen all the time. The actor LM is instructed to generate actions with required textual content,\ntaking into account the observed states. Similar to reinforcement learning, we select an action or\ngeneration, denoted as at, from the current policy \u03c0\u03b8 at time step t and receive an observation,\nrepresented by st, from the environment. We use ReAct (Yao et al., 2023) as our actor prompt.\nak,i,t = Ma\n\u0000[sk,i,\u03c4, ak,i,\u03c4, rk,i,\u03c4]t\u22121\n\u03c4=1, sk,i,t\n\u0001\n.\n(3)\nRetrospective Model\nThe retrospective model Mr is instantiated as a local LM. Its primary func-\ntion is to produce self-reflections, offering valuable feedback for diagnosing a possible reason for\nprior failure and devising a new, concise, high-level plan that aims to mitigate same failure. Operat-\ning under a sparse reward signal, such as binary success status (success/failure), the model detects\nthe root cause of failure by considering the current trajectory alongside its persistent memory.\nyk,i = Mr([sk,i,\u03c4, ak,i,\u03c4, rk,i,\u03c4]T\n\u03c4=1, Gk,i\n|\n{z\n}\nReflection prompt xk,i\n).\n(4)\nThis self-reflection feedback yk,i is appended to the actor prompt to prevent repetitive errors in a\nspecific environment in future attempts. Consider a multi-step task, wherein the agent failed in the\nprior trial. In such a scenario, the retrospective model can detect that a particular action, denoted\nas at, led to subsequent erroneous actions and final failure. In future trials, the actor LM can use\nthese self-reflections, which are appended to the prompt, to adapt its reasoning and action steps at\ntime t, opting for the alternative action a\u2032\nt. This iterative process empowers the agent to exploit past\nexperiences within a specific environment and task, thereby avoiding repetitive errors.\nMemory Module\nThe actor model generates thoughts and actions, by conditioning on its recent\ninteractions (short-term memory) and reflection responses (long-term memory).\n\u2022 Short-term memory. The trajectory history \u03c4i of the current episode i serves as the short-term\nmemory for decision making and reasoning.\n\u2022 Long-term memory. The reflection responses that summarize prior failed attempts are appended\nto the actor prompt as the long-term memory.\nTo facilitate policy optimization in Section 6.2, we store the prompts and responses of the retrospec-\ntive model of each trial, as instruction-response pairs, together with the episode returns in a local\ndataset, which we call replay buffer. We sample from the replay buffer to reinforce the retrospective\nmodel at the end of each episode. The long and short-term memory components provide context that\nis specific to a given task over several failed trials and the replay buffer provides demonstrations of\nsuccessful reflections across the tasks and environments, so that our Retroformer agent not only\nexploits lessons learned over failed trials in this task, but also explores by learning from successful\nreflections in other tasks.\n\u2022 Replay buffer. The memory DRL which stores the triplets (xk,i, yk,i, Gk,i) of the reflection prompt\nxk,i, reflection response yk,i and episode return Gk,i of trial i and task k.\nReward Shaping\nWe apply reward shaping to the binary rewards for obtaining more information.\nFor question answering tasks, instead of exactly matching the answer, we use f1 score grading to\nevaluate the alignment of the generated output with the expected answer as the reward function.\n6.2\nPOLICY GRADIENT OPTIMIZATION\nThe actor model Ma is regarded as an frozen LLM, such as GPT, with inaccessible model parame-\nters. In this scenario, the most direct approach to enhancing actor performance in a given environ-\nment is by refining the actor LM\u2019s prompt. Consequently, the retrospective model Mr, a smaller\n6\nlocal language model, paraphrases the actor\u2019s prompt by incorporating a concise summary of errors\nand valuable insights from failed attempts. We therefore aim to optimize the Mr model using en-\nvironment reward. The desired behavior of Mr is to improve the actor model Ma in next attempts.\nHence, the difference in episode returns between two consecutive trials naturally serve as a reward\nsignal for iteratively fine-tuning the retrospective model Mr, acting as a surrogate for reinforcement\nlearning aided by gradient-based signals. The specifics of this approach are outlined as follows.\nRetrospective \nLM\nInstruction: Diagnose a possible reason for failure and devise a \nnew, concise, high-level plan that aims to mitigate the same failure. \n{Input}. Answer is INCORRECT.\nReflection: \nI got stuck in a loop where I kept trying to search for the English \nactor who appeared in both Pennies From Heaven and Kenneth \nWilliams: Fantabulosa!, but the search term was too general. I \nshould have broken it down by searching for the English actor \nwho appeared in both TV series.\nInput: trajectory 1\nInput: trajectory K\n\u2026\nI directly looked for the next team he coached after WSU. Previous \ntrial: Question: What is the capital of France? Thought 1: I need to \nsearch 'France' and look for the capital. Action 1: \u2026\u2026\nReflection prompt x\nReflection response y\nr = 0.92\nr = -0.31\nRatings\nPPO \ntrainer\nFigure 3: Policy gradient optimization of retrospective LM using RLHF training pipeline.\nInstruction and Response Generation\nThe retrospective model generates a pair of instruction\nand response at the end of each trial i in the environment k. In the trial i, the actor produces a\ntrajectory \u03c4i by interacting with the environment. The evaluator then produces a score ri which is\ncomputed as rt = Me(\u03c4i). After the trial, to produce verbal feedback for refining the actor prompt,\nMr takes the set of {\u03c4i, ri} as the instruction xk,i and is prompted to produce a reflection response\nyk,i. All these instruction-response pairs (xk,i, yk,i) across tasks and trials are stored to a local\ndataset DRL, which we call \u201creplay buffer\u201d, for fine-tuning the Mr at the end of each episode.\nReflection Response Rating\nAs illustrated in Fig. 2(b), let us assume a reflection prompt xk,i\nand the corresponding episode return Gk,i, and the retrospective model Mr generates the response\nyk,i that summarizes the mistakes in i, which results in the return Rk,i+1 in the next attempt i + 1.\nBecause the actor is a frozen LM and the temperature is set to zero as default (Yao et al., 2023),\ni.e., T = 0, the injected randomness that leads to differences in returns \u2206Gk,i = Gk,i+1 \u2212 Gk,i are\nfrom the reflection responses yk,i, in which positive \u2206Gk,i indicates better responses that help the\nactor learn from prior errors, and hence should be rated with higher scores; negative or zero \u2206Gk,i\nindicates worse responses that needs to be avoided and hence should be rated with lower scores.\nTherefore, we define the rating score of a reflection instruction-response pair (xk,i, yk,i) as:\nr(xk,i, yk,i) \u225c Gk,i+1 \u2212 Gk,i.\n(5)\nProximal Policy Optimization\nThe optimization step of Retroformer is formalized as an it-\nerative process in Fig. 3. We use the differences of episode returns as the ratings of the generated\nreflection responses. The retrospective language model is fine-tuned at the end of each episode\nfollowing the standard RLHF training procedures (Ouyang et al., 2022) with proximal policy opti-\nmization:\nLPPO = Ex\u223cDRLEy\u223cLLMRL\n\u03d5 (x)\n\"\nr(x, y) \u2212 \u03b2 log LLMRL\n\u03d5 (y|x)\nLLMRef(y|x)\n#\n,\n(6)\nwhere (x, y) are sampled from the replay buffer, r(x, y) is the defined reward model, and the second\nterm in this objective is the KL divergence to make sure that the fine-tuned model LLMRL does not\nstray too far from the frozen reference model LLMRef.\nThe actor and retrospective model, and the policy gradient optimization module work together\nthrough trials in a loop until the environment deems \u03c4t to be correct. In practice, we apply of-\nfline RL methods instead of online optimization. We collected the dataset DRL by rolling out a base\npolicy, i.e., the frozen actor LM and the initialized retrospective LM, in the tasks in the training sets\nfor N trials and compute the ratings. We apply the standard RLHF pipeline to fine-tune the retro-\nspective model offline before evaluating the agent in the validation tasks. In online execution, we\nuse best-of-n sampler, with the scores evaluated by the learned reward model from RLHF pipeline,\nas an alternative method of generating better retrospective responses in each trial.\n7\n7\nEXPERIMENTS\nExtensive experiments are conducted to evaluate our method, including comparisons with ReAct\nand Reflexion performances, and visualization and discussion of agent\u2019s generated text and actions.\n7.1\nENVIRONMENT AND DATASET\nWe use open-source HotPotQA (Yang et al., 2018) environment, which consists of search-based\nquestion answering tasks to evaluate the agent\u2019s tool usage abilities under large state-action space.\nHotPotQA\nThe agent is asked to solve a question answering task by searching in Wikipedia pages.\nAt each time step, the agent is asked to choose from three action types or API calls:\n1. SEARCH[ENTITY], which searches the exact entity on Wikipedia and returns the first paragraph\nif it exists. If not, it will return some similar entities to search.\n2. LOOKUP[KEYWORD], which returns the next sentence containing keyword in the last passage\nsuccessfully found by Search.\n3. FINISH[ANSWER], which returns the answer and finishes the task.\nDataset\nWe collected 3,383 reflection samples by running the base rollout policy for 3 trials (N =\n3) for 3,000 tasks in the training set, in which 1,084 instruction-response pairs have positive ratings.\n7.2\nIMPLEMENTATION DETAILS\nModel\nWe use GPT-3 (model: text-davinci-003) as the frozen actor model. For the retrospective\nmodel, we instantiate it from LongChat (model: longchat-7b-16k), which is a LM with 16k context\nlength by fine-tuning llama-7b on instruction-following samples from ShareGPT. In all experiments,\nwe set the temperature of actor LM as zero, i.e., T=0 and top p =1 to isolate the randomness of\nLM from the effects of reflections. We acknowledge that setting a higher temperature value can\nencourage exploration but it can obscure the impact of the proposed approaches, making it difficult\nto compare against existing baselines with T=0 (Yao et al., 2023; Shinn et al., 2023).\nSetup\nOur proposed learning framework is developed by using multiple open-source tools as fol-\nlows. We use the OpenAI connectors from langchain to build our actor models Ma. During in-\nference of the retrospective model, we host an API server using FastChat and integrates it with\nlangchain agents. The tool can host longchat-7b-16k with concurrent requests to speed up RL pol-\nicy rollouts. For fine-tuning the retrospective model, we develop our training pipeline with trl, which\nsupports transformer reinforcement learning with PPO trainer.\nWe present the details of the specific prompts we used and the full agent demonstrations and exam-\nples for each environment in Appendix A.\nTraining\nWe fine-tune the retrospective model Mr with 4-bit quantized LoRA adapters (r=1) on\nthe offline RL datasets with epochs=4; batch size=8; lr=1.4e-5. The number of trainable parameters\nis 0.53M (0.015% of llama-7b). We first run supervised fine-tuning trainer on the samples with\npositive ratings for 2 epochs and then the RLHF pipeline, including reward modeling, and RL fine-\ntuning with PPO, on the whole offline rating dataset using the default settings for llama-7b model.\nEvaluation Metrics\nWe report the success rate and the average episode returns over validation\ntasks in an environment. The agent is evaluated on 100 validation tasks from the distractor dev split\nof open-source HotPotQA dataset.\nBaselines\nWe experiment with two language agent baselines.\n\u2022 ReAct (Yao et al., 2023). This is the language agent architecture which AutoGPT (Gravitas, 2023)\nis built upon. At the same time, this agent does not learn from the environment rewards at all\nand can serve as a baseline for showing how the actor model performs without exploration and\nexploitation in the environment.\n8\n\u2022 Reflexion (Shinn et al., 2023). This is the state-of-the-art language agent architecture that the au-\nthors identify from literature so far. This agent enhances from verbal feedback of the environment,\nand does not use gradient signals explicitly. It can serve as a baseline for showing the effectiveness\nof gradient-based learning for reasoning and planning.\n7.3\nCOMPARISONS WITH BASELINES\nWe present the performances of our Retroformer in Table 2 and compare it against the baselines\nin Fig. 4. As shown in the table, we observe that the generated reflections consistently improve the\nagent performances over trials and the effects of reflection sampling are significant in the first few\ntrials. This is expected as exploration in the early trials benefits exploration of the state action space.\nTable 2: Retroformer success rate (succ%) and average reward in N trials and best-of-n shots\nreflection sampling scored by the learned reward model in HotPotQA environment. The standard\ndeviation is calculated for the average rewards over 100 tasks and n shots.\nEnvironment Settings\n1 shot\n2 shots\n4 shots\nHotPotQA\nN=0 (initial trial)\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014- 34%/0.43 \u00b1 0.46 \u2014\u2014\u2014\u2014\u2014\u2014\u2014-\nN=1\n39%/0.45 \u00b1 0.47 42%/0.48 \u00b1 0.46 45%/0.52 \u00b1 0.47\nN=2\n42%/0.48 \u00b1 0.46 45%/0.52 \u00b1 0.47 48%/0.54 \u00b1 0.47\nN=4\n50%/0.55 \u00b1 0.48 52%/0.58 \u00b1 0.46 53%/0.60 \u00b1 0.46\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEpisode ID\n35\n40\n45\n50\nSuccess rate (%)\nHotPotQA (100 distractor tasks)\nRetroformer+4-shot sampling\nReflexion\nReAct\nFigure 4: We compare Retroformer with base-\nlines and observe consistent and faster performance im-\nprovement in terms of success rate.\nFurthermore, as shown in Fig. 4, our\nagent outperforms the two strong base-\nlines.\nSpecifically, the results indicate\nthat our reinforced model provides the\nlanguage agents with better reflection re-\nsponses in early trials, which enables the\nagents to learn faster, while also achiev-\ning better performances in the end. Our\nRetroformer agent achieves 53% suc-\ncess rate in 5 trials, which is better than the\nstate-of-the-art 50% success rate reported\nin (Jang, 2023) that uses a much larger\nlanguage model, i.e., GPT-3 (model: text-\ndavinci-003) as the retrospective compo-\nnent. The results show the effectiveness\nof our policy gradient approach for fine-\ntuning the agent with offline samples.\n7.4\nDISCUSSION OF THE REINFORCED AGENT BEHAVIORS\nWe examine how the retrospective model Mr is improved with policy optimization by comparing the\ngenerated responses from the frozen retrospective LM and the ones from the fine-tuned, reinforced\nLM. We observe two general emerging behaviors that the fine-tuned LM learns to follow through\ntrial and error. These two reinforced behaviors help the agents outperform the baselines in Fig. 4.\nPrecise Credit Assignment for Failed Trials.\nAs shown in Fig. 5, we gave an example that il-\nlustrates why the reinforced responses better analyze the mistakes made in the prior failed trial and\nenable the agent to prevent from repetitive errors in the next attempt. The agent failed in the last\nattempt mainly because it submits both \u201cTeen Tians\u201d and \u201cTeen Titans Go\u201d as the answer to the\nTeen Titans spin-off series, while the correct answer should only include \u201cTeen Titans Go\u201d; The\nagent forgot its original goal during a chain of thought, actions and lengthy observations. While the\nreflection response from the frozen model seems to have a lot \u201cshould have\u201d terms and is very fluent\nin language, it simply reiterates the prior action sequences that lead to failure as the steps that the\nagent should have done, which prompts the agent to follow these steps again in the next attempt,\nresulting in an infinite loop.\n9\nReflection (Green: Retroformer; Brown: Frozen LM)\nPrior failed trial\nQuestion\nI failed to find the Teen Titans spinoff series that Tara Strong has \ndone voice roles for. Instead, I found Lollipop Chainsaw, a video \ngame that Juliet Starling appeared in. I should have searched Tara \nStrong and Teen Titans or Teen Titans Go! specifically in my \nprevious action to find the correct answer.\nThought 1: I need to search Juliet Starling and find the \nCanadian-American actress who voiced her, then find the \nTeen Titans spinoff series she has done voice roles for. \nAction 1: Search[Juliet Starling] \nObservation 1: Could not find [Juliet Starling]. \u2026\nThought 2: Juliet Starling is a character from Lollipop \nChainsaw, so I can search Lollipop Chainsaw. \nAction 2: Search[Lollipop Chainsaw] \nObservation 2: Lollipop Chainsaw is a \u2026 \nThought 3: Juliet Starling is voiced by Tara Strong, so I \nneed to search Tara Strong and find the Teen Titans spinoff \nseries she has done voice roles for. \nAction 3: Search[Tara Strong] \nObservation 3: Tara Lyn Strong\u2026\nThought 4: Tara Strong has voiced characters in Teen \nTitans and Teen Titans Go!, so the answer is Teen Titans \nand Teen Titans Go!. \nAction 4: Finish[Teen Titans and Teen Titans Go!]\nLollipop Chainsaw featured Juliet \nStarling, who was voiced by a \nCanadian-American actress who \nhas done voice roles for what Teen \nTitans spinoff series?\nMy thought process was strong but my execution was flawed. I \nshould have searched for Lollipop Chainsaw first and looked up the \nCanadian-American actress who voiced Juliet Starling afterwards. I \nalso should have looked up Tara Strong's filmography and searched \nfor any voice roles she did specifically for Teen Titans or Teen Titans \nGo!.Next trial:Question: The novel was adapted into a film by a \ndirector known for his work on another iconic 1980s franchise. What \nnovel is this film based on?Thought 1:\nFigure 5: Response refinement from the reinforced retrospective model. Note that the lengthy ob-\nservation step in the prior failed trial column is abbreviated for better presentation purposes.\nOn the contrary, our reinforced response prompts the agent to focus on spinoff series in the very\nbeginning of the response and asks the agent to find the answer in the previous actions and obser-\nvations with special attention on the search results of \u201cTara Strong\u201d. In summary, our proposed\nreinforcement learning approach presents better credit assignment and root cause analysis abilities,\nand has the power to recommend real actionable insights for the next attempt.\nReinforced model response\nFrozen model response \nI think the issue might have been with the search. I was not able to find \nthe name \"Miller\" in the paragraph, which is why I was not able to find \nJames Henry Miller's wife. I should have searched for \"Miller's wife\" \ninstead of \"James Henry Miller's wife\" to avoid this confusion.\nNew Plan:\nTo avoid confusion, I will search for \"Miller's wife\" instead of \"James \nHenry Miller's wife\" and directly look for the nationality of the wife.\nI got stuck in a loop where I kept searching for the same person, \nHenry Miller, instead of Beatrice Sylvas Wickens. It's possible \nthat I may have misheard or misunderstood the question. To avoid \nthis, I should have written down the question and made sure I \nunderstood it correctly before beginning the search. Additionally, \nI should have tried searching for \"James Henry Miller's wife\" \ninstead of just \"James Henry Miller\".\nFigure 6: Response refinement from the reinforced retrospective model with structured format.\nStructured Reflections and Action Plans.\nWe discover one emergent behavior of the reinforced\nmodel that it can automatically paraphrase the original responses into two separate structured sec-\ntions, namely Reflection section and New plan: section, although not being explicitly trained or\nprompted for. One such example is shown in Fig. 6. The paraphrased response retrospects in the\nfirst paragraph and provides actionable insights next, while the response from the frozen LM in-\nterleaved both parts in one paragraph, making it hard to comprehend. We can also observer from\nFig. 5 that the reinforced response removes the messy, irrelevant \u201cNext trial:\u201d content in the end for\ncleaner format, which may very likely result from LLM hallucination.\n8\nCONCLUSION\nIn this study, we present Retroformer, an elegant framework for iteratively improving large\nlanguage agents by learning a plug-in retrospective model. This model, through the process of policy\noptimization, automatically refines the prompts provided to the language agent with environmental\nfeedback. Through extensive evaluations on real-world datasets such as HotPotQA, the method has\nbeen proven to effectively improve the performances of large language agents over time both in\nterms of learning speed and final task completion performances.\nBy considering the Actor LLM as a component of the environment, our policy gradient approach\nallows learning from arbitrary reward signals from diverse environments and tasks. This facilitates\nthe iterative refinement of a specific component within the language agent architecture \u2013 the retro-\nspective model, in our case, while circumventing the need to access the Actor LLM parameters or\npropagate gradients through it. This agnostic characteristic renders Retroformer a concise and\nadaptable plug-in module for different types of cloud-hosted LLMs, such as GPT and Bard. Fur-\nthermore, our approach is not limited to enhancing the retrospective model alone; it can be applied\n10\nto reinforce other components within the agent architecture, such as the memory and summarization\nmodule, or the actor prompt. By selectively focusing on the component to be fine-tuned while keep-\ning the remainder fixed, our proposed policy gradient approach allows for iterative improvements of\nthe component with reward signals obtained from the environment.\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nHarrison Chase. Langchain. https://github.com/hwchase17/langchain, 2023.\nSignificant Gravitas.\nAutogpt.\nhttps://github.com/Significant-Gravitas/\nAuto-GPT, 2023.\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992,\n2023.\nEric Jang. Can llms critique and iterate on their own outputs?\nevjang.com, Mar 2023.\nURL\nhttps://evjang.com/2023/03/26/self-reflection.html.\nAman Madaan, Alexander Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming\nYang, Graham Neubig, and Amir Yazdanbakhsh. Learning performance-improving code edits.\narXiv preprint arXiv:2302.07867, 2023a.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri\nAlon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement\nwith self-feedback. arXiv preprint arXiv:2303.17651, 2023b.\nVolodymyr Mnih, Adri`a Puigdom`enech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. CoRR, abs/1602.01783, 2016.\nYohei Nakajima. Babyagi. https://github.com/yoheinakajima/babyagi, 2023.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted\nquestion-answering with human feedback. arXiv preprint arXiv:2112.09332, 2021.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nJoon Sung Park, Joseph C O\u2019Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. arXiv preprint\narXiv:2304.03442, 2023.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D Manning, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. arXiv\npreprint arXiv:2305.18290, 2023.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nJohn Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region\npolicy optimization. CoRR, abs/1502.05477, 2015.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347, 2017.\n11\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint\narXiv:2303.17580, 2023.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin Gopinath, Karthik Narasimhan, and\nShunyu Yao. Reflexion: Language agents with verbal reinforcement learning. arXiv preprint\narXiv:2303.11366, 2023.\nCharlie Snell, Ilya Kostrikov, Yi Su, Mengjiao Yang, and Sergey Levine. Offline rl for natural\nlanguage generation with implicit language q learning. arXiv preprint arXiv:2206.11871, 2022.\nR. S. Sutton, D. Mcallester, S. Singh, and Y. Mansour. Policy gradient methods for reinforcement\nlearning with function approximation. In Advances in Neural Information Processing Systems 12,\nvolume 12, pp. 1057\u20131063. MIT Press, 2000.\nRichard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. The MIT Press,\nsecond edition, 2018. URL http://incompleteideas.net/book/the-book-2nd.\nhtml.\nLeandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, and Nathan\nLambert. Trl: Transformer reinforcement learning. https://github.com/lvwerra/trl,\n2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov,\nand Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop question\nanswering. In Conference on Empirical Methods in Natural Language Processing (EMNLP),\n2018.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReAct: Synergizing reasoning and acting in language models. In International Conference on\nLearning Representations (ICLR), 2023.\n12\nAppendix for\n\u201cRetroformer:\nRetrospective Large Language Agents with Policy Gradi-\nent Optimization\u201d\nA\nFULL EXAMPLES\nA.1\nACTOR PROMPT ENGINEERING\nAn example of the HotPotQA actor language model prompt is shown below.\n13\nA.2\nRETROSPECTIVE PROMPT ENGINEERING\nAn example of the HotPotQA retrospective instruction prompt is shown below.\nYou are an advanced reasoning agent that can improve based on self reflection. You will be\ngiven a previous reasoning trial in which you were given access to an Docstore API environment\nand a question to answer. You were unsuccessful in answering the question either because you\nguessed the wrong answer with Finish[<answer>], or you used up your set number of reasoning\nsteps. In a few sentences, Diagnose a possible reason for failure and devise a new, concise,\nhigh level plan that aims to mitigate the same failure. Use complete sentences.\nHere are some examples.\n[Few-Shot Demonstrations]\nPrevious trial:\nQuestion: The 2016 Washington State Cougars were led by the coach who previously helmed which\nother team?\nThought: I need to search Washington State Cougars and find the coach of the 2016 team, then\nfind which other team the coach previously helmed.\nAction: Search[Washington State Cougars]\nObservation: The Washington State Cougars (known informally as the Cougs) are the athletic\nteams that represent Washington State University. Located in Pullman, Washington, WSU is a\nmember of the Pac-12 Conference in NCAA Division I. The athletic program comprises ten women's\nsports and six men's intercollegiate sports, and also offers various intramural sports.\nThought: The paragraph does not mention the coach of the 2016 team. I need to look up \"coach\"\nto find the coach of the 2016 team.\nAction: Lookup[coach]\nObservation: (Result 2/5) During the 2001\u201303 seasons, the Cougar football teams were\ndistinguished by three ten-win seasons, three top ten poll rankings, and appearances in the\nSun, Rose, and Holiday Bowls. The Cougars shared the Pac-10 title in 2002. Alumnus Paul Wulff,\nWSU's 31st head coach, was fired in\nlate November 2011, after compiling a 9\u201340 (.184) record\nin four seasons. Mike Leach coached the team from 2012 to 2019, guiding them to six bowl\ngames. Leach left to coach Mississippi State in 2020 and was replaced by Hawaii head coach\nNick Rolovich. Rolovitch was fired in October 2021 for not complying with the state's COVID-19\nvaccine mandate. Defensive coordinator Jake Dickert was named the interim head coach.\nThought: The 2016 Washington State Cougars were led by Mike Leach. I need to search Mike Leach\nand find which other team he previously helmed.\nAction: Search[Mike Leach]\nObservation: Michael Charles Leach (March 9, 1961 \u2013 December 12, 2022) was an American college\nfootball coach who primarily coached at the NCAA Division I FBS level. He was a two-time\nnational coach of the year, three-time conference coach of the year and the mastermind behind\nthe NCAA record-setting air raid offense. He was the head coach at Texas Tech University from\n2000 to 2009, where he became the winningest coach in school history. After Texas Tech, he\ncoached at Washington State University from 2012 to 2019, where he recorded the third-most\nwins of any coach in school history. He then coached at Mississippi State from 2020 until his\ndeath in 2022.Leach was known for directing offenses using lots of passing to several\nreceivers, in a spread system known as the air raid, which Leach developed with Hal Mumme when\nMumme was head coach and Leach was offensive coordinator at Iowa Wesleyan, Valdosta State, and\nKentucky in the 1990s. Leach's offenses with Mumme, and later as a head coach himself, have\nbroken numerous school and NCAA records. On eighteen occasions, Leach led his unranked team to\nvictory over a team ranked in the AP poll, which is the most in the AP era.\nThought: Mike Leach previously helmed Texas Tech University and Kentucky. So the answer is\nTexas Tech University and Kentucky.\nAction: Finish[Texas Tech University and Kentucky]\nObservation: Answer is INCORRECT\nReflection:\n14\n"
  },
  {
    "title": "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities",
    "link": "https://arxiv.org/pdf/2308.02490.pdf",
    "upvote": "15",
    "text": "MM-Vet: Evaluating Large Multimodal Models\nfor Integrated Capabilities\nWeihao Yu1\u2217 Zhengyuan Yang2\u2217 Linjie Li2\nJianfeng Wang2\nKevin Lin2\nZicheng Liu2\nXinchao Wang1\u2020 Lijuan Wang2\u2020\n1National University of Singapore\n2Microsoft Azure AI\nweihaoyu@u.nus.edu\nxinchao@nus.edu.sg\n{zhengyang,lindsey.li,jianfw,keli,zliu,lijuanw}@microsoft.com\nAbstract\nWe propose MM-Vet1, an evaluation benchmark that examines large multimodal\nmodels (LMMs) on complicated multimodal tasks. Recent LMMs have shown var-\nious intriguing abilities, such as solving math problems written on the blackboard,\nreasoning about events and celebrities in news images, and explaining visual jokes.\nRapid model advancements pose challenges to evaluation benchmark development.\nProblems include: (1) How to systematically structure and evaluate the compli-\ncated multimodal tasks; (2) How to design evaluation metrics that work well across\nquestion and answer types; and (3) How to give model insights beyond a simple\nperformance ranking. To this end, we present MM-Vet, designed based on the\ninsight that the intriguing ability to solve complicated tasks is often achieved by a\ngeneralist model being able to integrate different core vision-language (VL) capa-\nbilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations\nof interest derived from the capability combination. For evaluation metrics, we\npropose an LLM-based evaluator for open-ended outputs. The evaluator enables the\nevaluation across different question types and answer styles, resulting in a unified\nscoring metric. We evaluate representative LMMs on MM-Vet, providing insights\ninto the capabilities of different LMM system paradigms and models. Code and\ndata are available at https://github.com/yuweihao/MM-Vet.\n1\nIntroduction\nThe breakthroughs in large language models (LLMs) [11, 59, 20, 5, 73, 36] bring generalist AI\nmodels that can solve a wide range of complicated natural language tasks, many approaching the\nhuman-expert-level performance [59, 13]. Large multimodal models (LMMs) aim to achieve even\nstronger general intelligence via extending LLMs with multimodal inputs. Since more than 80% of\nour human being\u2019s perception, learning, cognition, and activities are mediated through vision [65], it\nis natural to start the exploration by equipping LLMs with \u201ceyes.\u201d One main thread of LMM works,\nrepresented by Frozen [75], Flamingo [4], PaLM-E [25], GPT-4 [59], extend LLMs with the visual\nunderstanding capability via end-to-end tuning. There also exists the exploration [83, 89, 85, 70, 30]\non the modular combination of LLMs and image-to-text vision-language models. Recently, thanks\nto the open-source of powerful LLMs like LLaMA [73], more open-sourced LMMs are built,\nincluding OpenFlamingo [9], LLaVA [51], MiniGPT-4 [93], Otter [44], InstructBLIP [23], and\nmany more [33, 52, 87]. These studies showcase the intriguing ability to solve various complicated\n\u2217Equal contribution.\n\u2020Corresponding authors.\n1Short for \u201cMultimodal Veterinarian.\u201d\narXiv:2308.02490v3  [cs.AI]  24 Oct 2023\nGT: The man at bat readies to swing at the\npitch while the umpire looks on.\nRequired capabilities:\n\u00a0 \u00a0 Recognition\n\u00a0 \u00a0 Language generation\nCOCO\nQ: Is the boy happy?\nGT: Yes\nRequired capability:\n\u00a0 \u00a0 Recognition\nVQA v2\nQ: What is the largest denomination on table?\nGT: 500\nRequired capabilities:\n\u00a0 \u00a0 Recognition\n\u00a0 \u00a0 OCR\nText VQA\nQ: What will the girl on the right write\non the board?\nGT: 14\nRequired capabilities:\n\u00a0 \u00a0 Recognition\n\u00a0 \u00a0 Spatial awareness\n\u00a0 \u00a0 OCR\n\u00a0 \u00a0 Math\nMM-Vet (Ours)\nQ: Where is this photo taken?\nGT: Qatar\nRequired capabilities:\n\u00a0 \u00a0 Recognition\n\u00a0 \u00a0 Knowledge\nQ: Can you explain this meme?\nGT: This meme is a humorous take on\nprocrastination and the tendency to delay\ntasks until a specific time ...\nRequired capabilities:\n\u00a0 \u00a0 OCR\n\u00a0 \u00a0 Recognition\n\u00a0\u00a0\u00a0 Knowledge\n\u00a0 \u00a0 Language generation\n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFigure 1: Required capabilities of different benchmarks. Different from conventional VL benchmarks\nonly require one or two capabilities, MM-Vet focuses on the integration of different core VL capabili-\nties, including recognition, OCR, knowledge, language generation, spatial awareness, and math.\nmultimodal tasks, such as open-world recognition, multimodal knowledge and commonsense, scene\ntext understanding, and so on.\nDespite the promising qualitative results on LMM\u2019s capabilities, it remains unclear how to systemati-\ncally evaluate those showcased complicated multimodal tasks and what are the relationships among\nevaluated tasks, which is the first step in developing a quantitative evaluation benchmark. As shown\nin Figure 1, existing vision-language benchmarks [6, 17, 72] focus on simple Vision-Language (VL)\ntasks that require specific one or two capabilities, such as recognition, language generation, or OCR,\nbut fall short in benchmarking more complicated tasks. Alternatively, we examine the arbitrary\nintegration of core VL capabilities for complicated tasks, with the insight that the intriguing ability to\nsolve complicated multimodal tasks can be achieved by a generalist model mastering and integrating\ndifferent core capabilities. Following this insight, we propose a new benchmark for evaluating LMMs,\nnamely MM-Vet. MM-Vet defines six core VL capabilities, including recognition, OCR, knowledge,\nlanguage generation, spatial awareness, and math, which integrate to solve various complicated\nmultimodal tasks. MM-Vet contains 16 tasks for quantitative evaluation. For example, in Figure 1(d),\nanswering the question \u201cWhat will the girl on the right write on the board?\u201d in MM-Vet requires\nrecognizing the genders of the three kids, locating queried girl spatially, recognizing the scene text\nwritten by the girl, and finally calculating the result.\nOther than the evaluation category definition, the evaluation metrics are another challenge in bench-\nmark development, given the diverse answer styles and question types. Specifically: (1) The desired\noutputs in different multimodal tasks have diverse formats, e.g., Figure 1(d)\u2019s math problem can be\n2\nanswered by a single word, while outputs for the essay writing question are hundred-words long;\n(2) The core aspect to evaluate in different tasks varies, e.g., text generation focuses more on the\ntext quality, recognition can be considered correct with the key concept recognized. Most integrated\ntasks would require comprehensive evaluations from multiple dimensions. Inspired by recent NLP\nstudies [19, 53, 28] that use LLMs for model evaluation, we propose an LLM-based evaluator as the\nevaluation metric for open-ended model outputs. As shown in Table 1, we prompt GPT-4 [59] with\nfew-shot evaluation prompts to obtain an evaluation score ranging from 0 to 1. Instead of manually\ndefining the possible answer styles and question types, we include different sample types as few-shot\nexamples and let LLMs infer the scoring criteria automatically. Such metric design eases the future\nextension to more question types, such as box localization [16, 82, 77].\nMM-Vet\u2019s evaluation category and metric designs allow users to obtain capability insights for\ndifferent LMMs. Such model analyses are more informative than a single overall ranking, which\nhighly depends on the dataset sample composition and might be biased. We evaluate two sets of\nmultimodal systems, i.e., the end-to-end tuned LMMs including OpenFlamingo [9], LLaVA [51],\nMiniGPT-4 [93], Otter [44], InstructBLIP [23], etc, and the LLM-tool-using systems [85, 70, 30, 38]\nsuch as MM-ReAct [85]. Despite not knowing model details, we also evaluate industry solutions\nsuch as Bard [34]. We first discuss the capability analyses of these two system paradigms and the\nrepresentative models. We then dive deeper into the open-sourced LMMs and examine how the\ntraining data, vision encoder, and LLM selection influence the performance on different capabilities.\nOur contributions are summarized as follows.\n\u2022 We propose MM-Vet to evaluate LMMs\u2019 ability on complicated multimodal tasks. MM-Vet\ndefines 16 emergent tasks of interest, integrated from the six defined core VL capabilities.\n\u2022 We propose an LLM-based evaluator for open-ended outputs of LMMs, which unifies the\nevaluation across different answer styles and question types. The evaluation metrics ensure\nthe thorough evaluation of both the factual correctness and text quality of the responses.\n\u2022 We benchmark representative LMMs on MM-Vet, revealing the relative strengths and\nweaknesses of different system paradigms and models, as summarized in Section 4.5.\n2\nRelated work\nMultimodal models. Vision-language models [17, 35, 56, 18, 48, 40, 79, 76, 82, 29] approach\nmultimodal intelligence of jointly understanding and generating vision and language signals. Inspired\nby the impressive quality and genericity in recent large language models (LLMs) [12, 59, 20, 73],\nresearchers explore large multimodal models (LMMs) that seamlessly integrate different vision-\nlanguage capabilities to solve complicated multimodal tasks. In approaching such multimodal\ngeneralist systems, one direction is to extend LLMs with the multi-sensory ability, such as pioneer\nworks Frozen [75], Flamingo [4], PaLM-E [25], GPT-4 [59]. Recent open-sourced LLMs [91, 73, 64]\nalso facilitate various research studies including OpenFlamingo [9], LLaVA [51], MiniGPT-4 [93],\nOtter [44], InstructBLIP [23], and so on [33, 52, 87]. On the other hand, multimodal agents [85, 70, 38,\n30] explore chaining different vision tools with LLMs [12, 59] to achieve integrated vision-language\ncapabilities.\nVL benchmarks. Classic VL benchmarks focus on specific capabilities of interest, such as visual\nrecognition [35], image description [17, 3], as well as other benchmarks for specialized capabilities\nsuch as scene text understanding [72, 71, 86], commonsense reasoning [88], outside knowledge [58].\nThe recent development of generalist LMMs posts a strong need for modernized VL benchmarks,\nwhich contain complicated multimodal tasks that require integrated VL capabilities.\nOur MM-Vet is most related to the concurrent evaluation studies [27, 54, 45, 80, 50] such as MME\nand MMBench, which design comprehensive evaluation samples to facilitate the LMM evaluation.\nOne major difference is that MM-Vet defines and studies the integrated VL capabilities, allowing the\nevaluation to provide insights beyond the overall model ranking.\nLLM-based evaluation. MM-Vet adopts the open-ended LLM-based evaluator, allowing the evalua-\ntion across answer styles and question types without requiring binary or multiple answer choices. The\ntechnique of prompting LLMs for model evaluation is related to the explorations in NLP [19, 53, 28].\nWe show that the technique extends well to multimodal tasks, and presents a unified prompt to\nevaluate samples with different answer styles and question types.\n3\nRec\n(Recognition)\nOCR\nKnow\n(Knowledge)\nGen\n(Language\ngeneration)\nSpat\n(Spatial\nawareness)\nMath\n(a)\n0\n20\n40\n60\n80\nProportion (%)\n68.8\n44.0\n38.5\n36.7\n34.4\n11.9\nRec\nKnow\nGen\nRec OCR\nSpat\nOCR\nSpat\nMath\nRec\nSpat\nOCR OCR\nMath\nRec\nKnow\nRec\nOCR\nKnow\nGen\nRec\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nRec\nOCR\nOCR\nKnow\nSpat\nRec\nKnow\nSpat\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nMath\n(b)\n0\n10\n20\n30\nProportion (%)\n28.4\n17.0\n11.9\n6.4\n5.5\n5.5\n5.0\n4.1\n3.7\n3.7\n3.2\n1.8\n1.4\n0.9\n0.9\n0.5\nFigure 2: MM-Vet proportion of capabilities. (a) The proportion of each capability. The sum of\nthe proportion is larger than 100% because most samples have more than one capability. (b) The\nproportion of capability integrations. The sum of the proportion is equivalent to 100%.\n3\nMM-Vet\n3.1\nData collection\nOur aim is to develop a multimodal benchmark that requires comprehensive capabilities, corre-\nsponding to realistic scenarios an AI agent might encounter. Consider, for instance, this scenario:\nAwakening from slumber, you reach out for your smartphone (recognition capability) to check the\ncurrent time (OCR capability). Today, your plan is to visit a new grocery that you have not been\nto. Guided by the information that the grocery is situated directly opposite the stadium and next\nto the cinema (spatial awareness), you manage to locate it successfully. Keeping in mind your\ndoctor\u2019s advice to shed some weight, you consciously steer clear of high-calorie food and choose\nmilk, vegetables, and fruits instead (knowledge capability). In the dairy aisle, you\u2019re faced with a\nchoice between two types of pure milk. The first is 4 dollars for one liter with 20% discount, while the\nsecond is 7 dollars for 1.5 liter with 25% discount. After some quick arithmetic, you find the former\nis cheaper (math capability) and and opt for the one-liter package. After shopping, you walk past the\ncinema and find a person pointing to the poster to introduce a new movie (language generation).\nFrom the scenarios of interest, we summarize the following six core VL capabilities for evaluation,\nwith corresponding MM-Vet examples shown in Tables 10-15.\n\u2022 Recognition (Rec). Recognition refers to the general visual recognition capability, including\nrecognizing scenes, objects, object attributes (color, material, shape, etc), counting, and\nvarious other high-level visual recognition tasks in computer vision.\n\u2022 Knowledge (Know). The knowledge category covers various knowledge-related capabilities,\nincluding social and visual commonsense knowledge, encyclopedic knowledge, and time-\nsensitive knowledge like news. This capability necessitates that the model not only possesses\nsuch knowledge, but also effectively utilizes it to solve complicated tasks as required.\n\u2022 OCR. Optical character recognition (OCR) refers to the scene text understanding and\nreasoning capability. The models are tested to read the scene text in images, and reason over\nthe texts to solve various tasks.\n4\n\u2022 Spatial awareness (Spat). Spatial awareness embodies a diverse spectrum of capabilities\nrelated to understanding space, including the comprehension of the spatial relationship\namong object and scene text regions.\n\u2022 Language generation (Gen). Language generation is a vital ability that empowers models\nto articulate their responses in a clear, engaging, and informative manner. We use questions\nthat demand more extended answers for language generation capacity evaluation.\n\u2022 Math. Math evaluates the model\u2019s arithmetic capability in solving either written equations\nor problems in the wild.\nIn real-world scenarios, various complicated multimodal tasks would require the integrations of\ndifferent core VL capabilities. For instance, explaining visual jokes as shown in Table 10(a) requires\nrecognition, knowledge of humor, and language generation; reading documents and solving math\nproblems as shown in Table 11(a) takes OCR, spatial awareness and math; and answering exam\nquestions given images as shown in Table 14(b) needs OCR, knowledge, spatial awareness. To\nsolve these complicated tasks, LMMs are expected to seamlessly integrate different VL capabilities.\nTherefore, it is crucial to establish a benchmark that evaluates the performance of these integrated\nabilities within LMMs.\nTo build the benchmark, we have gathered 187 images from various online sources and ask 205\nquestions, each of which requires one or more capabilities to answer. As shown in Tables 10-15,\nthese questions are varied in type and entail open-ended responses of differing lengths. The ground\ntruths for 155 questions are human-annotated, while the remainder of the answers for 50 questions\nwere gathered from the Internet. In addition to the 187 images, ten extra images with high-quality\nquestions are collected from VCR [88], with the questions and answers modified to an open-ended\nanswering format. Another three images are from ChestX-ray14 [78] to obtain corresponding medical\nexpert knowledge. In total, our MM-Vet contains 200 images, and 218 questions (samples), all paired\nwith their respective ground truths. For each question, we have also identified the capacities required\nto answer them and displayed this information statistically in Figure 2.\n3.2\nLLM-based evaluator for open-ended model outputs\nQuestions and expected responses in MM-Vet are designed to be open-ended to cover the diverse\nreal-world scenarios. This naturally poses a great challenge in terms of model evaluation and metric\ndesign. Drawing inspiration from recent NLP studies [19, 92] that utilize LLMs for open-ended\nevaluations, we leverage GPT-4 to assist evaluation. As shown in Table 1, we craft a few-shot prompt\nfor model evaluation. The few-shot design allows us to define the scoring metrics via in-context\nexamples and supports easy extension onto new problem sets. Specifically, our implemented prompt\nincorporates five in-context examples with open-ended short answers and two examples with long\nanswers. We cover examples that are fully correct (i.e., 1.0) or incorrect (i.e., 0.0), as well as examples\nused to define different types of \u201cpartially correct\u201d responses. The LLM-based evaluator allows any\nstyle of model outputs to be evaluated with a unified consistent metric. Furthermore, it also supports\neasy adaptation to diverse question types and answer styles by simply modifying the evaluation\nexamples.\nBy inputting the prompt, GPT-4 automatically generates scores for each sample, conditioned on each\nsample\u2019s input question, ground truth, and model output. The score for each sample ranges from 0 to\n1. The total scores are computed by\nS =\nN\nP\ni=1\nsi\nN\n\u00d7 100%,\n(1)\nwhere si is the score of sample i, and N is the sample number. The score regarding each capability\nor capability integration can be similarly obtained by\nSc =\nP si\nNc\n\u00d7 100%,\ni \u2208 C,\n(2)\nwhere C is the set of samples requiring a specific capability or capability integration, and Nc is the\nsample number of the set.\n5\nTable 1: Few-shot prompt for evaluating model outputs using GPT-4, where Q is a sample\u2019s question,\nG is the ground truth and P is the model output for the sample. In the prompt, there are examples\nwith short and long open-ended answers, enabling the evaluation of diverse answer styles. Taking the\nprompt filled with Q, G and P, GPT-4 will generate a soft grading score from 0 to 1.\nCompare the ground truth and prediction from AI models, to give a correctness score for the prediction.\n<AND> in the ground truth means it is totally right only when all elements in the ground truth are\npresent in the prediction, and <OR> means it is totally right when any one element in the ground truth\nis present in the prediction. The correctness score is 0.0 (totally wrong), 0.1, 0.2, 0.3, 0.4, 0.5, 0.6,\n0.7, 0.8, 0.9, or 1.0 (totally right). Just complete the last space of the correctness score.\nQuestion | Ground truth | Prediction | Correctness\n\u2014 | \u2014 | \u2014 | \u2014\nWhat is x in the equation? | -1 <AND> -5 | x = 3 | 0.0\nWhat is x in the equation? | -1 <AND> -5 | x = -1 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -5 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -5 or 5 | 0.5\nWhat is x in the equation? | -1 <AND> -5 | x = -1 or x = -5 | 1.0\nCan you explain this meme? | This meme is poking fun at the fact that the names of the countries\nIceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green\nlandscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person\nhas trust issues because the names of these countries do not accurately represent their landscapes. |\nThe meme talks about Iceland and Greenland. It\u2019s pointing out that despite their names, Iceland is not\nvery icy and Greenland isn\u2019t very green. | 0.4\nCan you explain this meme? | This meme is poking fun at the fact that the names of the countries\nIceland and Greenland are misleading. Despite its name, Iceland is known for its beautiful green\nlandscapes, while Greenland is mostly covered in ice and snow. The meme is saying that the person\nhas trust issues because the names of these countries do not accurately represent their landscapes. |\nThe meme is using humor to point out the misleading nature of Iceland\u2019s and Greenland\u2019s names.\nIceland, despite its name, has lush green landscapes while Greenland is mostly covered in ice and\nsnow. The text \u2018This is why I have trust issues\u2019 is a playful way to suggest that these contradictions\ncan lead to distrust or confusion. The humor in this meme is derived from the unexpected contrast\nbetween the names of the countries and their actual physical characteristics. | 1.0\nQ | G | P |\n4\nEvaluation results\n4.1\nExperiment settings\nWe utilize MM-Vet to evaluate two types of LMMs, i.e., (1) end-to-end tuned LMMs (OpenFlamingo\n[4, 9, 8], BLIP-2 [46], LLaVA [51], MiniGPT-4 [93], Otter [44] and InstructBLIP [23]); (2) LLM-\ntool-using methods (MM-ReAct [85] and Transformers Agent [38]). The summary of these methods\nis shown in Table 2. As shown in Table 1, for each sample, we fill the prompt template with its\nquestion, ground truth, and output from a specific LMM. By taking the filled prompt into GPT-4,\nGPT-4 will generate a score from 0 to 1 for the sample. It is found that outputs of GPT-4 still exist\nvariance, although the temperature is set as 0. Therefore, we utilize GPT-4 to evaluate the outputs\nof LLMs by 5 times. Due to the space limit, we report average scores for capabilities/capability\nintegrations, and average as well as variance for total score.\n4.2\nResult analyses\nThe main results of different methods are shown in Table 3 regarding each capability, and Table 4 for\neach capability integration.\n6\nTable 2: Summary of the evaluated LMMs in this report. We consider both the end-to-end tuned\nmodels (i.e., OpenFlamingo [4, 9, 8], BLIP-2 [46], LLaVA [51], MiniGPT-4 [93], LLaMA-Adapter\nv2 [32], Otter [44], InstructBLIP [23]), and the LLM-tool-using systems (i.e., MM-ReAct [85] and\nTransformers Agent [38]).\nMethod\nInitial models\nTuning data\nTotal params\nVision\nLanguage\nOther\nOpenFlamingo-9B [4, 9, 8]\nCLIP ViT-L/14 [66]\nMPT-7B [2]\n\u2013\nMultimodal C4 [94]\n9B\nBLIP-2-12B [46]\nEVA-ViT-G [26]\nFlan-T5-XXL [21]\n\u2013\n1. COCO [49]; 2. Visual Genome [41];\n3. CC3M [69]; 4. CC12M [15]; 5. SBU [62];\n6. 115M images from the LAION-400M [68].\n(CapFilt [47] is used to create synthetic\ncaptions for the web images)\n12B\nLLaVA-7B [51]\nCLIP ViT-L/14 [66]\nVicuna-7B [92]\n\u2013\n1. CC3M [69] Concept-balanced 595K [51];\n2. LLaVA-Instruct-158K [51].\n7B\nLLaVA-13B [51]\nVicuna-13B [92]\n13B\nLLaVA-7B (LLaMA-2) [51]\nCLIP ViT-L/14 [66]\nLLaMA-2-7B-Chat [74]\n\u2013\n1. LAION /CC/SBU BLIP-Caption\nConcept-balanced 558K [51];\n2. LLaVA-Instruct-80K [51].\n7B\nLLaVA-13B (LLaMA-2) [51]\nLLaMA-2-13B-Chat [74]\n13B\nLLaVA-13B (V1.3, 336px) [51] CLIP ViT-L/336px [66] Vicuna-13B-v1.3 [92]\n13B\nMiniGPT-4-8B [93]\nEVA-ViT-G [26]\nVicuna-7B [92]\nBLIP-2\u2019s Q-Former [46]\n1. CC3M [69]; 2. CC12M [15];\n3. SBU [62]; 4. LAION-400M [68]\n5. Proposed 3,500 aligned image-text pairs [93].\n8B\nMiniGPT-4-14B [93]\nVicuna-13B [92]\n14B\nLLaMA-Adapter v2-7B [32]\nCLIP ViT-L/14 [66]\nLLaMA-7B [73]\n\u2013\n1. LAION-400M [68]; 2. COYO-700M [14];\n3. Multimodal C4 [94]; 4. SBU [62];\n5. CC12M [15]; 6. COCO [49];\n7. GPT-4-LLM [64];\n8. Tuning data of LLaVA [51]\n7B\nOtter-9B [44]\nCLIP ViT-L/14 [66]\nMPT-7B [2]\nOpenFlamingo-9B\u2019s [4, 9, 8]\n1. Perceiver Resampler;\n2. GATED XATTN-DENSE\nMIMIC-IT [43]\n9B\nInstructBLIP-8B [23]\nEVA-ViT-G [26]\nVicuna-7B [92]\nBLIP-2\u2019s Q-Former [46]\n1. Tuning data of BLIP-2 [46];\n2. 26 publicly available datasets\n(transformed into instruction tuning format).\n8B\nInstructBLIP-14B [23]\nVicuna-13B [92]\n14B\nTransformers Agent\n(GPT-4 as agent) [38]\n\u2013\n1. GPT-4 [59];\n2. Flan-T5 [21];\n3. BART [42]\n1. Donut [39]; 2. BLIP [47];\n3. ViLT [40]; 4. CLIPSeg [57]\n5. Whisper [67];\n6. SpeechT5 [7]; 7. NLLB [22]\nNone\nNot clear\nMM-ReAct-GPT-3.5 [85]\nMM-ReAct-GPT-4 [85]\n\u2013\nGPT-3.5 [63]\nGPT-4 [59]\n1. Azure Cognitive Services APIs [10]\nfor image captioning,\nimage tagging, dense captioning,\nOCR and specialized recognition\non celebrities, receipts, etc\n2. Bing search; 3. PAL [31]\nNone\nNot clear\n4.2.1\nRegarding each capability\nRecognition. The \u201cRecognition\u201d category contains the questions requiring recognition capability to\nanswer. Examples are shown in Tables 10(a, b), 11(b), 12(a, b), 13(a, b), 14(a, c), and 15(b). The\n\u201cRec\u201d column in Table 3 compares the performance on the \u201cRecognition\u201d. Among the evaluated\nmodels, LLaVA-13B (LLaMA-2) is the best one, obtaining 39.2%. There may be two reasons. First,\nLLaVA-13B (LLaMA-2) adopts ViT-L/14 [24] from CLIP [66] as a vision model, which is trained\nby a large amount of data, 400 million image-text pairs; 2) Second, it is surprising that stronger\nlanguage model can largely boost the recognition performance. LLaVA-13B (LLaMA-2) obtains\n8.3% important over LLaVA-13B (Vicuna-13B). Stronger LLMs may help understand questions\nbetter and identify key information from visual inputs.\nLLaMA-Adapter v2-7B is another strong model in recognition, achieving 38.5%. This outstanding\nability may be obtained from its various and large amounts of tuning data, LAION-400M [68],\nCOYO-700M [14], Multimodal C4 [94] and Tuning data of LLaVA [51] etc as shown in Table 2.\nBesides, InstructBLIP-8B [23] attains 32.4%. As shown in Table 2, the tuning data of InstructBLIP\nincludes 26 publicly available datasets, which contain recognition heavily datasets, like VQA v2\n[35] and GQA [37]. The promising capability of InstructBLIP in recognition may benefit from these\ndatasets.\nOCR. OCR assesses models\u2019 capabilities in recognizing scene texts in images and performing various\ntypes of reasoning including math, spatial, recognition, etc. Examples are shown in Tables 10(c), 11(a,\nc, d), 12(b), 13(a, b), 14(a, b), 15(a, b). As shown in Table 2\u2019s \u201cOCR\u201d column, MMReAct-GPT4 [85]\nperforms the best (65.7%) in OCR capability with the assistance of an external OCR model as a tool.\nAmong end-to-end tuned models, LLaVA-13B (LLaMA-2) [51] achieves the highest performance\n(22.7%). This superior performance may be attributed to LLaVA\u2019s adoption of CLIP [66] ViT-L/14\n[24] as its vision model, and the inclusion of a large volume of image-OCR pairings within the\ntraining data [55].\nKnowledge. As depicted in Tables 10(a), 12(a, b) and 14(b, c), the \u201cknowledge\u201d category covers\na wide range of knowledge-related questions, ranging from joke understanding to encyclopedia\n7\nTable 3: MM-Vet evaluation results on various LMMs regarding each core VL capability. For each\ncolumn, the highest, the second, and the third highest figures are highlighted by green, orange and\nblue backgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nTransformers Agent (GPT-4) [38]\n18.2\n3.9\n2.2\n3.2\n12.4\n4.0\n13.4\u00b10.5\nMiniGPT-4-8B [93]\n27.4\n15.0\n12.8\n13.9\n20.3\n7.7\n22.1\u00b10.1\nBLIP-2-12B [46]\n27.5\n11.1\n11.8\n7.0\n16.2\n5.8\n22.4\u00b10.2\nLLaVA-7B [51]\n28.0\n17.1\n16.3\n18.9\n21.2\n11.5\n23.8\u00b10.6\nMiniGPT-4-14B [93]\n29.9\n16.1\n20.4\n22.1\n22.2\n3.8\n24.4\u00b10.4\nOtter-9B [44]\n27.3\n17.8\n14.2\n13.8\n24.4\n3.8\n24.7\u00b10.3\nOpenFlamingo-9B [4, 9, 8]\n28.7\n16.7\n16.4\n13.1\n21.0\n7.7\n24.8\u00b10.2\nInstructBLIP-14B [23]\n30.8\n16.0\n9.8\n9.0\n21.1\n10.5\n25.6\u00b10.3\nInstructBLIP-8B [23]\n32.4\n14.6\n16.5\n18.2\n18.6\n7.7\n26.2\u00b10.2\nLLaVA-13B [51]\n30.9\n20.1\n23.5\n26.4\n24.3\n7.7\n26.4\u00b10.1\nMM-ReAct-GPT-3.5 [85]\n24.2\n31.5\n21.5\n20.7\n32.3\n26.2\n27.9\u00b10.1\nLLaVA-7B (LLaMA-2) [51]\n32.9\n20.1\n19.0\n20.1\n25.7\n5.2\n28.1\u00b10.4\nLLaMA-Adapter v2-7B [32]\n38.5\n20.3\n31.4\n33.4\n22.9\n3.8\n31.4\u00b10.1\nLLaVA-13B (V1.3, 336px) [51]\n38.1\n22.3\n25.2\n25.8\n31.3\n11.2\n32.5\u00b10.1\nLLaVA-13B (LLaMA-2) [51]\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9\u00b10.1\nMM-ReAct-GPT-4 [85]\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6\u00b10.2\nknowledge. LLaVA-Adapter v2-7B is the best model in this capability with a score of 31.4%, as\nshown in Table 3. It may be beneficial from its large-scale tuning data including GPT-4-LLM [64].\nMMReAct-GPT-4 [85] also achieves a remarkable score (29.0%) in this capability, because of its\nstrong LLM backbone [59], coupled with external tools like Bing search for knowledge acquisition.\nLanguage generation. \u201cLanguage generation\u201d denotes the proficiency to produce fluent and infor-\nmative text outputs, as illustrated in Table 10(a), 12(b), 13(a), and 15(a). The performance within this\ncategory is highly correlated with the efficacy of language modeling. As a result, MMReAct-GPT4\n[85] and LLaVA-13B (LlaMA-2) stand out as the top two models. Their success can be attributed to\nthe GPT-4 and LlaMA-2 language models on which these systems are built.\nSpatial awareness. \u201cSpatial awareness\u201d involves the understanding of the spatial relationship among\nvisual object regions (e.g., Table 10(c)) and scene text regions (e.g., Table 13(a, b)). MMReAct-GPT4\n[85] has a significant lead in this capability (56.8%), because the adopted tools, such as dense\ncaptioning and OCR, provide detailed object and scene text location information in the form of\ncoordinates, which can be understood and processed by GPT-4.\nWhen it comes to end-to-end tuned models, LLaVA-13B (V1.3, 336px) exhibits the best performance\nof 31.3%. The tuning data for LLaVA is partly derived from capturing object names and their\ncorresponding coordinates as input. This procedure ensures the generation of data imbued with\nspatial information, potentially aiding the models in developing and enhancing their spatial awareness\ncapabilities.\nMath. \u201cMath\u201d measures the arithmetic capability on either written equations (e.g., Table 15(b)) or\nproblems in the wild (e.g., Table 11(d)). Notably, MMReAct-GPT4 [85] consistently outperforms\nother models. This superior performance may be attributed to the adopted PAL math tool (Program-\naided Language Models) [31].\n4.2.2\nRegarding each capability integration\nRecognition, knowledge, and language generation.. As shown in Table 10(a), this capability\nintegration can enable models to explain visual jokes. LLaMA-Adapter-v2-7B [32] is the best model\nin this capability integration. This may be attributed to its large scale of tuning data as shown in\nTable 2. LLaVA-13B (LLaMA-2) and LLaVA-13B (V1.3, 336px) [51] are the other two outstanding\nmodels. Stronger language models may be the reason. The tuning data of LLaVA shown in Table 2\ncan also not be ignored.\n8\nTable 4: MM-Vet evaluation results on various LMMs regarding each capability integration. Examples\nof each capability integration are shown in supplementary materials Tables 10-15. For each column,\nthe highest, the second, and the third highest figures are highlighted by green, orange and blue\nbackgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nKnow\nGen\nRec\nOCR\nSpat\nOCR\nSpat\nMath\nRec\nSpat OCR\nOCR\nMath\nRec\nKnow\nRec\nOCR\nKnow\nGen\nRec\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nRec\nOCR\nOCR\nKnow\nSpat\nRec\nKnow\nSpat\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nMath\nTotal\nTransformers Agent (GPT-4) [38]\n1.3\n49.1\n0.0\n7.4\n45.8\n0.0\n0.0\n0.0\n0.0\n9.5\n0.0\n25.0\n0.0\n50.0\n49.0\n0.0\n13.4\u00b10.5\nMiniGPT-4-8B [93]\n14.2\n47.9\n9.6\n14.3 50.0 20.8\n0.0\n14.4\n8.0\n21.2\n42.9 50.0\n0.7\n0.0\n0.0\n0.0\n22.1\u00b10.1\nBLIP-2-12B [46]\n7.3\n65.1 11.5\n7.1\n41.7 21.2\n4.5\n38.9\n5.2\n8.5\n14.3 25.0\n16.7\n50.0\n0.0\n0.0\n22.4\u00b10.2\nLLaVA-7B [51]\n17.1\n46.6 13.3\n21.4 41.7 24.8\n0.0\n28.9\n6.2\n45.2\n6.6\n50.0\n0.0\n0.0\n19.0\n0.0\n23.8\u00b10.6\nMiniGPT-4-14B [93]\n21.1\n47.5 14.6\n7.1\n50.0 16.7\n0.0\n11.1\n18.7\n38.5 18.3 32.5\n50.0\n0.0\n0.0\n0.0\n24.4\u00b10.4\nOtter-9B [44]\n15.6\n54.1 29.2\n7.1\n50.0 22.5\n0.0\n11.1\n3.2\n6.0\n23.1 46.5\n33.3\n0.0\n30.0\n0.0\n24.7\u00b10.3\nOpenFlamingo-9B [4, 9, 8]\n15.5\n48.6 15.4 14.3\n58.3 40.5\n0.0\n38.9\n4.5\n6.0\n28.6 50.0\n10.0\n0.0\n0.0\n0.0\n24.8\u00b10.2\nInstructBLIP-14B [23]\n8.1\n74.3 14.6 14.3 50.0 19.2\n6.5\n11.1\n8.8\n15.2 14.3\n70.0\n16.7\n50.0\n15.0\n0.0\n25.6\u00b10.3\nInstructBLIP-8B [23]\n18.0\n69.9 15.4 14.3 33.3 20.8\n0.0\n23.3\n7.8\n35.2 15.7 25.0\n0.0\n0.0\n0.0\n0.0\n26.2\u00b10.2\nLLaVA-13B [51]\n25.2\n41.1 17.3\n7.1\n47.5 23.3\n9.1\n18.0\n12.5\n53.8 14.3 50.0\n50.0\n0.0\n12.0\n0.0\n26.4\u00b10.1\nMM-ReAct-GPT-3.5 [85]\n19.1\n33.1\n28.8 35.7 28.3\n60.0\n9.1\n33.3\n2.5\n47.8\n0.0\n25.0\n100.0\n0.0\n35.0 80.0 27.9\u00b10.1\nLLaVA-7B (LLaMA-2) [51]\n18.8\n57.0\n26.9\n9.7\n50.0 26.7\n0.0\n34.7\n10.2\n44.8 14.3 50.0\n11.3\n0.0\n0.0\n0.0\n28.1\u00b10.4\nLLaMA-Adapter v2-7B [32]\n35.3 54.1 13.5\n7.1\n50.0\n38.5\n0.0\n12.2\n22.5\n38.0\n28.6 48.0\n53.3\n0.0\n0.0\n0.0\n31.4\u00b10.1\nLLaVA-13B (V1.3, 336px) [51]\n25.5 59.7 25.0 14.3\n66.7 25.8\n8.2\n27.8\n11.2\n49.3 14.3 50.0\n33.3\n50.0\n2.0\n0.0\n32.5\u00b10.1\nLLaVA-13B (LLaMA-2) [51]\n29.8 59.5 21.2 14.3\n58.3 36.2\n0.0\n27.8\n3.5\n56.8 28.6 50.0\n33.3\n0.0\n8.0\n0.0\n32.9\u00b10.1\nMM-ReAct-GPT-4 [85]\n22.5\n33.0\n69.2 78.6 25.0\n83.0 63.6\n44.4\n68.2\n88.0 14.3 50.0\n0.0\n50.0\n80.0\n0.0\n44.6\u00b10.2\nRecognition (sole). This category contains samples only requiring recognition, as shown in Table\n10(b). InstructBLIP-14B and InstructBLIP-8B [23] achieve the best performance, which may result\nfrom the tuning data including recognition datasets, like VQA v2 [35] and GQA [37].\nOCR and spatial awareness. For this integration, an example is shown in Table 10(c). MM-ReAct-\nGPT-4 [85] is the best method for this integration. Notably, compared with MM-ReAct-GPT-3.5,\nMM-ReAct-GPT-4 has a significant improvement, over 40%, indicating the importance of LLMs to\nintegrate information of OCR and location.\nOCR, spatial awareness, and math. An example of this integration is shown in Table 11(a), which\nrequires reading the floor plan and conducting arithmetic. Compared with the above integration, this\ncombination involves one more capability of math. The observation is similar to the integration of\nOCR and spatial awareness. MM-ReAct-GPT-4 [85] still achieves the best performance.\nRecognition and spatial awareness. Table 11(b) shows an example for this integration. LLaVA-\n13B (V1.3, 336px) [51] performs best for this category. Compared with LLaVA-13B (LLaMA-2),\nLLaVA-13B (V1.3, 336px) obtains an improvement of 8.4%, indicating the significant contribution\nof larger resolution of images.\nOCR (sole). This task requires OCR only, as shown in Table 11(c). MM-ReAct-GPT-4 [85] has the\nbest results for sole OCR due to an OCR tool from Azure API. Notable, MM-ReAct-GPT-4 is much\nbetter than MM-ReAct-GPT-3.5 with an improvement of 23.0%, demonstrating the importance of\nlanguage models in OCR.\nOCR and Math. This integration enables reading text from real-world scenarios and solving math\nproblems, as shown in Table 11(d). MM-ReAct-GPT-4 [85] obtains the best performance in this\ncapability integration, far ahead of other models. We highly recommend using MM-ReAct-GPT-4 to\ncomplete tasks related to this capability integration.\nOther capability integrations. 9 other capability integrations are in long-tailed distribution, where\nMMReAct-GPT-4 achieves the best scores in 5 integrations out of 9. Their examples are shown in\nTables 12-15.\n4.3\nResult discussion\n4.3.1\nFoundation models and tuning data\nIn this subsection, we discuss the modules in LMMs and speculate how each component may affect\nthe LMMs\u2019 capabilities in different aspects, evaluated by MM-Vet. We mainly consider the models\nbased on open-sourced LLMs, i.e., Flan-T5 [21], LLaMA [73], Vicuna [92], and LLaMA-2 [74].\n9\nTable 5: MM-Vet (Bard set) evaluation results on various LMMs regarding each core VL capability.\nFor each column, the highest, the second, and the third highest figures are highlighted by green,\norange and blue backgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nLLaVA-13B (LLaMA-2) [51]\n37.8\n22.9\n22.4\n27.6\n27.2\n8.0\n30.3\u00b10.1\nLLaVA-13B (V1.3, 336px) [51]\n39.4\n22.3\n22.7\n24.6\n30.6\n11.6\n31.5\u00b10.1\nMM-ReAct-GPT-3.5 [85]\n22.3\n31.4\n15.6\n16.6\n32.9\n24.0\n27.6\u00b10.2\nMM-ReAct-GPT-4 [85]\n34.3\n66.3\n25.6\n36.6\n60.6\n72.0\n48.1\u00b10.2\nBard [34]\n56.2\n52.5\n50.9\n61.0\n52.0\n39.6\n53.5\u00b10.2\nTable 6: MM-Vet (Bard set) evaluation results on various LMMs regarding each capability integration.\nFor each column, the highest, the second, and the third highest figures are highlighted by green,\norange and blue backgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nKnow\nGen\nRec\nOCR\nSpat\nOCR\nSpat\nMath\nRec\nSpat OCR\nOCR\nMath\nRec\nKnow\nRec\nOCR\nKnow\nGen\nRec\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nRec\nOCR\nOCR\nKnow\nSpat\nRec\nKnow\nSpat\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nMath\nTotal\nVicuna-13B (LLaMA-2) [51]\n26.6\n55.2 18.8 14.3\n57.1 39.5\n0.0\n20.0\n1.3\n56.8 28.6 50.0\n33.3\n0.0\n8.0\n\u2013\n30.3\u00b10.1\nVicuna-13B (V1.3, 336px) [51]\n21.9\n59.0 22.9 14.3\n85.7 25.5\n8.2\n20.0\n15.0\n49.3\n14.3 50.0\n33.3\n50.0\n2.0\n\u2013\n31.5\u00b10.1\nMM-ReAct-GPT-3.5 [85]\n11.3\n38.8\n31.2 35.7 28.6 56.4\n9.1\n20.0\n0.0\n47.8\n0.0\n25.0\n100.0\n0.0\n35.0\n\u2013\n27.6\u00b10.2\nMM-ReAct-GPT-4 [85]\n17.0\n35.2\n70.8 78.6 28.6\n81.5 63.6\n40.0\n68.3\n88.0 14.3 50.0\n0.0\n50.0\n80.0\n\u2013\n48.1\u00b10.2\nBard [34]\n52.3\n70.3 45.2 56.4 42.9 70.2 18.2\n0.0\n77.7\n81.5 28.6 50.0\n66.7\n50.0\n80.0\n\u2013\n53.5\u00b10.2\nVision. For the Vision component, two models have been employed in the end-to-end LMMs we\nevaluated, namely, CLIP-ViT/L14 [66] (428M) and EVA-ViT-G (1.13B). Determining a superior\nmodel is currently not possible due to the absence of a comprehensive ablation study [90]. However,\nit\u2019s noteworthy that, when paired with the same language model, Vicuna-7B, InstructBLIP-8B excels\nin recognition tasks, while LLaVA-7B works particularly well for OCR.\nLanguage. There is a notable trend indicating that superior language models (LLMs) typically yield\nbetter performance, such as comparing the 7B and 13B variants of different models, except for the\noutlier of InstructBLIP where the 8B version performs better than the 14B one.\nTuning data. Increasing the volume of data can enhance performance. An example is InstructBLIP-\n8B [23], which utilizes more data from 26 publicly available datasets to tune the model and achieve\nhigher scores than BLIP-2-12B.\n4.3.2\nComparison with Bard\nBard [34] is one popular closed-source commercial LMM system. One problem in evaluation is that\nBard rejects images containing people and instead outputs \u201cSorry, I can\u2019t help with images of people\nyet.\u201d To conduct a fair comparison with other models, we constructed a subset of MM-Vet with 168\nsamples that Bard could process, henceforth referred to as the Bard set. The results on the Bard set\nare shown in Tables 5 and 6.\nBard achieves the highest scores in three out of six capabilities, seven out of fifteen capability\nintegrations, and holds the highest overall score (53.5%). MM-ReAct-GPT-4 [85] outperforms in\nthe remaining three out of six capabilities, and tops the chart in nine out of the fifteen capability\nintegrations. Particularly, MM-ReAct performs better in OCR, spatial awareness, and math capabili-\nties, indicating the potential benefit of having specialized external tools, even when working with\nstate-of-the-art LMMs.\nWhen considering end-to-end models, there is still a big gap from Bard. For instance, Vicuna-13B\n(V1.3, 336px) [51] obtains 31.5%, a substantial 22.0% lower than Bard. Future stronger open-sourced\nLLMs and advancements in multimodal training hold potential to further narrow this gap.\n4.3.3\nComparison with GPT-4V(ision)\nWe evaluate and benchmark the state-of-the-art LMM, GPT-4V(ison) [59\u201361, 1, 84] on MM-Vet.\nIn our queries to GPT-4V, we prepend the prompt with \u201cGenerate a short and concise response to\nthe following image text pair.\u201d The quantitative results are shown in Tables 7, 8, and the qualitative\n10\nTable 7: MM-Vet evaluation results on LLaVA, MM-ReAct and GPT-4V regarding each core VL\ncapability. For each column, the highest and the second highest figures are highlighted by green and\norange backgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nOCR\nKnow\nGen\nSpat\nMath\nTotal\nLLaVA-13B (LLaMA-2) [51]\n39.2\n22.7\n26.5\n29.3\n29.6\n7.7\n32.9\u00b10.1\nMM-ReAct-GPT-4 [85]\n33.1\n65.7\n29.0\n35.0\n56.8\n69.2\n44.6\u00b10.2\nGPT-4V [60]\n67.5\n68.3\n56.2\n60.7\n69.4\n58.6\n67.7\u00b10.3\nTable 8: MM-Vet evaluation results on LLaVA, MM-ReAct and GPT-4V regarding each capability\nintegration. For each column, the highest and the second highest figures are highlighted by green and\norange backgrounds. All the numbers are presented in % and the full score is 100%.\nModel\nRec\nKnow\nGen\nRec\nOCR\nSpat\nOCR\nSpat\nMath\nRec\nSpat OCR\nOCR\nMath\nRec\nKnow\nRec\nOCR\nKnow\nGen\nRec\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nRec\nOCR\nOCR\nKnow\nSpat\nRec\nKnow\nSpat\nOCR\nGen\nSpat\nRec\nOCR\nSpat\nMath\nTotal\nLLaVA-13B (LLaMA-2) [51]\n29.8\n59.5 21.2 14.3\n58.3 36.2\n0.0\n27.8\n3.5\n56.8\n28.6\n50.0\n33.3\n0.0\n8.0\n0.0\n32.9\u00b10.1\nMM-ReAct-GPT-4 [85]\n22.5\n33.0\n69.2 78.6 25.0\n83.0 63.6\n44.4\n68.2\n88.0 14.3\n50.0\n0.0\n50.0\n80.0\n0.0\n44.6\u00b10.2\nGPT-4V [60]\n55.5\n89.2 68.6 73.9 83.3 77.5 44.5\n38.9\n78.2\n76.5 42.9 100.0\n66.7\n50.0\n89.0\n0.0\n67.7\u00b10.3\nresults are expressed in Figures 3-6. Remarkably, GPT-4V achieves a score of 67.7%, surpassing\nboth open-sourced LMMs [51] and LLM-based multimodal agents [85] by substantial margins.\nWe aspire that the detailed per-category performance breakdown sheds light on potential avenues for\nenhancing model capabilities, thereby bridging the existing performance gap. To illustrate, integrating\nspecialized tools within agent systems proves advantageous for specific functionalities like OCR\nand math. While other categories, such as recognition and language generation, would require\nenhancements in the core vision and language modules, respectively. Figures 3-6 offer an exhaustive\nanalysis, highlighting exemplary success and failure instances of GPT-4V\u2019s performance.\nThis MM-Vet analysis is intended as a source of inspiration for future research, specifically in the\nrealms of advanced multimodal prompting techniques [84, 81] and model refinements to further\nimprove the LMM performance.\n4.4\nEffectiveness analysis of LLM-based evaluation\nTo verify the effectiveness of LLM-based evaluation for LMM predictions, we select the outputs from\nMMReAct-GPT-4 on 138 objective questions, which can be objectively annotated by humans. We\ncompute the absolute value of the difference between the evaluator\u2019s output score and the human-\nannotated score on each sample. By default, we use GPT-4 (0613) as the evaluator. Here we also\nreplace it with other LLMs, e.g. LLaMA-2, GPT-3.5. The average difference to the human scoring is\nreported in Table 9, represented as \u2206.\nThe maximum potential discrepancy is 1.0. The baseline evaluation method, keyword matching,\nresults in a high difference of 0.273. This illustrates the unsuitability of keyword matching for\nMM-Vet when dealing with open-ended answers. It is surprising that \u2206 of LLaMA-2-7B [74] is even\nhigher than that of keyword matching, while \u2206 LLaMA-2-13B only marginally less than keyword\nmatching. This suggests that assessing open-ended outputs from models is far from straightforward.\nFor OpenAI\u2019s models, GPT-3.5 (turbo-0613) obtains 0.178 of \u2206, and GPT-4 (0613) achieves the\nlowest difference of 0.042. In this paper, we utilize GPT-4 (0613) to evaluate the outputs of LMMs.\n4.5\nTakeaway notes\nWe summarize the above analyses and discussions as follows:\n\u2022 In the evaluation of integrated capabilities on MM-Vet (Sections 4.2, 4.3.2, 4.3.3), GPT-\n4V [60] and Bard [34] outperform existing open-sourced methods. The tool-using ap-\nproach, MM-ReAct-GPT-4 [85], achieves comparable performance to Bard with effective\nexternal tools. The pros and cons in different categories motivate future studies on tool-\nenhanced LMMs. Among end-to-end LMMs, LLaVA-13B (LLaMA-2)/LLaVA-13B (V1.3,\n336px) [51] demonstrates the best performance on MM-Vet.\n11\nTable 9: Averaged absolute differences (\u2206) between the evaluation scores of various LLM evaluators\nand those of human-annotated scores, on MM-ReAct-GPT4\u2019s results. A smaller discrepancy indicates\na better agreement with the gold standard of human evaluation, indicating a better evaluator.\nModel\nKeyword\nmatching\nLLM-based evaluation\nLLaMA-2-7B\nLLaMA-2-13B\nGPT-3.5 (turbo-0613)\nGPT-4 (0613)\n\u2206 (\u2193)\n0.273\n0.307\n0.254\n0.178\n0.042\n\u2022 Analysis of open-source LMMs (Section 4.3.1) leaves room for ambiguity regarding the\nsuperior vision encoders for LMMs, based on current model comparisons. However, it is\nevident that stronger LLMs can boost the performance of LMMs.\n\u2022 For open-ended evaluation (Section 4.4), it is effective to use GPT-4 for evaluating the open-\nended outputs of LMMs. The use of less powerful LLMs could result in more significant\ndeviations from the gold standard of human evaluation results.\n\u2022 Current top-performing methods, such as GPT-4V [60] and MM-ReAct-GPT-4 [85], only\nachieve scores of around 68%/45% on MM-Vet (where full score is 100%). The gap signifies\nthat further effort is necessary to enhance the performance of LMMs in terms of integrated\ncapabilities, e.g., by developing stronger LMMs or extending LMMs with external tools.\n5\nConclusion\nIn this paper, we introduce the MM-Vet benchmark to evaluate LMMs in terms of their integrated\nvision-language capabilities. We have assembled a new multimodal dataset, which requires the\nintegration of multiple vision-language capabilities. To facilitate open-ended evaluation, we adopt an\nLLM-based evaluator to grade open-ended outputs from LMMs. We then evaluate various LMMs\non MM-Vet, analyzing their results to provide insights into different LMM system paradigms and\nmodule selections. We observe that the current best LMMs GPT-4V [59\u201361, 1, 84] achieve around\n68% score on MM-Vet (full score 100%), indicating the need for efforts to further improve the\nintegrated capabilities of LMMs.\n12\nA\nExamples of capability integrations\nTable 10: Three samples requiring different capability integrations.\n(a)\nQ:\nWhat occasions would someone use this meme?\nGT:\nThis meme, commonly known as \"Screaming Panda,\" is typically used to express\nshock, surprise, or fear. It could be used in response to a startling or unexpected\nevent, or to convey a sense of panic or alarm. Some possible occasions where\nsomeone might use this meme include:\n- Reacting to a jump scare in a horror movie\n- Responding to a surprising plot twist in a TV show or book\n- Expressing shock at a news headline or current event\n- Conveying fear or anxiety about an upcoming deadline or exam\n- Showing surprise at an unexpected outcome in a sports game or other competition.\nRequired capabilities:\nRecognition, knowledge, language generation\n(b)\nQ:\nHow many tomatoes are there?\nGT:\n5\nRequired capabilities:\nRecognition\n(c)\nQ:\nWhat is located to the right of the shampoo?\nGT:\nconditioner\nRequired capabilities:\nOCR, spatial awareness\n13\nTable 11: Four samples requiring different capability integrations.\n(a)\nQ:\nWhich room is bigger, the double garage or the living room?\nGT:\ndouble garage\nRequired capabilities:\nOCR, spatial awareness, math\n(b)\nQ:\nOn the right desk, what is to the left of the laptop?\nGT:\ntable lamp <OR> desk lamp\nRequired capabilities:\nRecognition, spatial awareness\n(c)\nQ:\nWhat are all the scene text in the image?\nGT:\n5:30PM <AND> 88% <AND> Mario Kart 8 Deluxe <AND> MARIO KART\n8 DELUXE <AND> SUPER MARIO ODYSSEY <AND> THE LEGEND OF\nZELDA <AND> BREATH OF WILD <AND> Options <AND> Start\nRequired capabilities:\nOCR\n(d)\nQ:\nHow many gallons of supreme gasoline can I get with $50?\nGT:\n13.6 <OR> 13.7\nRequired capabilities:\nOCR, math\n14\nTable 12: Two samples requiring different capability integrations.\n(a)\nQ:\nIn which country was this photo taken?\nGT:\nAustralia\nRequired capabilities:\nRecognition, knowledge\n(b)\nQ:\nCan you explain this meme?\nGT:\nThis meme is a humorous take on procrastination and the tendency to delay tasks\nuntil a specific time. The person in the meme plans to do something at 8 o\u2019clock,\nbut when they miss that deadline by a few minutes, they decide to wait until 9\no\u2019clock instead. The image of Kermit the Frog lying in bed represents the person\u2019s\nlaziness and lack of motivation to complete the task.\nRequired capabilities:\nRecognition, OCR, knowledge, language generation\n15\nTable 13: Two samples requiring different capability integrations.\n(a)\nQ:\nThe graph below shows the long-term international migration, UK, 1999-2008.\nSummarize the information by selecting and reporting the main features, and make\ncomparisons where relevant.\nYou should write at least 150 words.\nGT:\nThe chart gives information about UK immigration, emigration and net migration\nbetween 1999 and 2008.\nBoth immigration and emigration rates rose over the period shown, but the figures\nfor immigration were significantly higher. Net migration peaked in 2004 and 2007.\nIn 1999, over 450,000 people came to live in the UK, while the number of people\nwho emigrated stood at just under 300,000. The figure for net migration was around\n160,000, and it remained at a similar level until 2003. From 1999 to 2004, the\nimmigration rate rose by nearly 150,000 people, but there was a much smaller rise\nin emigration. Net migration peaked at almost 250,000 people in 2004.\nAfter 2004, the rate of immigration remained high, but the number of people\nemigrating fluctuated. Emigration fell suddenly in 2007, before peaking at about\n420,000 people in 2008. As a result, the net migration figure rose to around 240,000\nin 2007, but fell back to around 160,000 in 2008.\nRequired capabilities:\nRecognition, OCR, language generation, spatial awareness\n(b)\nQ:\nWhich car is on the parking spot 33?\nGT:\nno <OR> empty\nRequired capabilities:\nRecognition, OCR, spatial awareness\n16\nTable 14: Three samples requiring different capability integrations.\n(a)\nQ:\nIs this apple organic?\nGT:\nyes\nRequired capabilities:\nRecognition, OCR\n(b)\nQ:\nWhich are producers in this food web?\nGT:\nPhytoplankton <AND> Seaweed\nRequired capabilities:\nOCR, knowledge, spatial awareness\n(c)\nQ:\nDoes the person bigger than the car?\nGT:\nno\nRequired capabilities:\nRecognition, knowledge, spatial awareness\n17\nTable 15: Two samples requiring different capability integrations.\n(a)\nQ:\nThe table below gives information about the underground railway systems in six\ncities.\nSummarise the information by selecting and reporting the main features, and make\ncomparisons where relevant.\nYou should write at least 150 words.\nGT:\nThe table shows data about the underground rail networks in six major cities.\nThe table compares the six networks in terms of their age, size and the number of\npeople who use them each year. It is clear that the three oldest underground systems\nare larger and serve significantly more passengers than the newer systems.\nThe London underground is the oldest system, having opened in 1863. It is also the\nlargest system, with 394 kilometres of route. The second largest system, in Paris, is\nonly about half the size of the London underground, with 199 kilometres of route.\nHowever, it serves more people per year. While only third in terms of size, the\nTokyo system is easily the most used, with 1927 million passengers per year.\nOf the three newer networks, the Washington DC underground is the most extensive,\nwith 126 kilometres of route, compared to only 11 kilometres and 28 kilometres\nfor the Kyoto and Los Angeles systems. The Los Angeles network is the newest,\nhaving opened in 2001, while the Kyoto network is the smallest and serves only 45\nmillion passengers per year.\nRequired capabilities:\nOCR, language generation, spatial awareness\n(b)\nQ:\nWhat will the girl on the right write on the board?\nGT:\n14\nRequired capabilities:\nRecognition, OCR, spatial awareness, math\n18\nB\nGPT-4V Prediction Examples\n(a) Q: How many gallons of supreme \ngasoline can I get with $50?\nGT: 13.6 <OR> 13.7\nRequired capabilities: OCR, math\nGPT-4V: You can get \napproximately 13.7 gallons \nof supreme gasoline with \n$50 at the price of $3.659 \nper gallon.\nGPT4_score: 1.0\n(b) Q: What is the price difference between \ndiesel and regular gasoline?\nGT: 0.11\nRequired capabilities: OCR, math\nGPT-4V: The price difference \nbetween diesel and regular \ngasoline is $0.11, with diesel \nbeing more expensive.\nGPT4_score: 1.0\n(c) Q: Based on the image, what is the total \ninternational awareness of Tina Fey?\nGT: 35%\nRequired capabilities: OCR, spatial \nawareness\nGPT-4V: The total \ninternational awareness of \nTina Fey is 35%.\nGPT4_score: 1.0\n(d) Q: Based on the image, in which \ncountry does Tina Fey have the highest \nlevel of total awareness?\nGT: Australia\nRequired capabilities: OCR, spatial \nawareness\nGPT-4V: Tina Fey has the \nhighest level of total \nawareness in Australia at \n62%.\nGPT4_score: 1.0\n(e) Q: What is the price for tomatoes?\nGT: eight <OR> 8.0\nRequired capabilities: Recognition, spatial \nawareness\nGPT-4V: The price for \ntomatoes is 8.00 as shown \non the label in the image.\nGPT4_score: 1.0\n(f) Q: What fruit is the the right of plums?\nGT: orange\nRequired capabilities: Recognition, spatial \nawareness\nGPT-4V: The fruit to the right \nof the plums are oranges.\nGPT4_score: 1.0\n(g) Q: Which car is on the parking spot 33?\nGT: no <OR> empty\nRequired capabilities: OCR, recognition, \nspatial awareness\nGPT-4V: The blue car is on \nparking spot 33.\nGPT4_score: 0.0\n(h) Q: Which spot is the red car parking \non?\nGT: 31\nRequired capabilities: OCR, recognition, \nspatial awareness\nGPT-4V: The red car is \nparking on spot number 87.\nGPT4_score: 0.0\nFigure 3: GPT-4V prediction examples\n19\n(a) Q: Which category did the worst job in \nplastic recycling, considering the amount \nof plastic made and thrown out?\nGT: packaging\nRequired capabilities: OCR, spatial \nawareness\nGPT-4V: The packaging \ncategory did the worst job in \nplastic recycling in 2015, \nwith 141M metric tonnes of \nplastic thrown out of the \n146M metric tonnes made.\nGPT4_score: 1.0\n(b) Q: What is the difference in metric \ntonnes between the amount of plastic \nproduced and the amount discarded?\nGT: 105\nRequired capabilities: OCR, spatial \nawareness\nGPT-4V: The difference in \nmetric tonnes between the \namount of plastic produced \nand discarded is 105 million \nmetric tonnes (407M - \n302M).\nGPT4_score: 1.0\n(d) Q: What is the name of this landmark?\nGT: baochu pagoda\nRequired capabilities: Recognition\nGPT-4V: This is the Baochu \nPagoda located in Hangzhou, \nChina.\nGPT4_score: 1.0\n(f) Q: What is the estimated average \nstanding charge for the year 2023?\nGT: 271\nRequired capabilities: OCR, spatial \nawareness, math\nGPT-4V: The estimated \naverage standing charge for \nthe year 2023 is \u00a3280.\nGPT4_score: 0.0\n(g) Q: What is the estimated total cost for \nthe year 2023?\nGT: 23800\nRequired capabilities: OCR, spatial \nawareness, math\nGPT-4V: The estimated total \ncost for the year 2023 is \n\u00a35,890.\nGPT4_score: 0.0\n(c) Q: Who is the person in the bottom \nright?\nGT: Jamie Dornan\nRequired capabilities: Recognition, spatial \nawareness\nGPT-4V: The person in the \nbottom right is Jamie \nDornan.\nGPT4_score: 1.0\n(e) Q: How many birds are there?\nGT: 6\nRequired capabilities: Recognition\nGPT-4V: There are six birds in \nthe image.\nGPT4_score: 1.0\nFigure 4: GPT-4V prediction examples\n20\nFigure 5: GPT-4V prediction examples\n21\n(a) Q: What occasions would someone use \nthis meme?\nGT: This meme, commonly known as \n\"Screaming Panda,\" is typically used to \nexpress shock, surprise, or fear. It could be \nused in response to a startling or \nunexpected event, or to convey a sense of \npanic or alarm. \u2026\u2026\nRequired capabilities: Recognition, \nknowledge, language generation\nGPT-4V: This meme is often used to \nexpress shock, surprise, or disbelief in a \nhumorous or exaggerated way. It can be \nused in response to unexpected news, \nevents, or situations.\nGPT4_score: 0.6\n(b) Q: Can you explain this meme?\nGT: This meme is a humorous take on \nprocrastination and the tendency to delay \ntasks until a specific time. The person in \nthe meme plans to do something at 8 \no'clock, but when they miss that deadline \nby a few minutes, they decide to wait until \n9 o'clock instead. The image of Kermit the \nFrog lying in bed represents the person's \nlaziness and lack of motivation to complete \nthe task.\nRequired capabilities: Recognition, \nknowledge, language generation, OCR\nGPT-4V: This meme humorously illustrates \nthe tendency to procrastinate. The person \nplans to do something at 8 but delays it by \n5 minutes and then decides to postpone \nthe task until 9, exemplifying how easily \ntasks can be deferred. The image of Kermit \nthe Frog relaxed and unbothered \nunderscores the nonchalant attitude \ntowards procrastination.\nGPT4_score: 1.0\n(c) Q: The table below gives information \nabout the underground railway systems in \nsix cities. Summarise the information by \nselecting and reporting the main features, \nand make comparisons where relevant. \nYou should write at least 150 words.\nGT: The table shows data about the \nunderground rail networks in six major \ncities. The table compares the six networks \nin terms of their age, size and the number \nof people who use them each year. It is \nclear that the three oldest underground \nsystems are larger and serve significantly \nmore passengers than the newer systems.\nThe London underground is the oldest \nsystem, having opened in 1863. It is also \nthe largest system, with 394 kilometres of \nroute. The second largest system, in Paris, \nis only about half the size of the London \nunderground, with 199 kilometres of \nroute. However, it serves more people per \nyear. While only third in terms of size, the\nTokyo system is easily the most used, with \n1927 million passengers per year.\nOf the three newer networks, the \nWashington DC underground is the most \nextensive, with 126 kilometres of route, \ncompared to only 11 kilometres and 28 \nkilometres for the Kyoto and Los Angeles \nsystems. The Los Angeles network is the \nnewest, having opened in 2001, while the \nKyoto network is the smallest and serves \nonly 45 million passengers per year.\nRequired capabilities: OCR, language \ngeneration, spatial awareness\nGPT-4V: The table illustrates data on six \nunderground railway systems worldwide, \nfocusing on their opening dates, route \nlengths, and annual passenger numbers. \nLondon\u2019s underground, inaugurated in \n1863, is the oldest and has the most \nextensive network, boasting 394 kilometres \nof route. Despite its age and extensive \nnetwork, it only caters to 775 million \npassengers annually, significantly less than \nParis and Tokyo. Paris\u2019s metro, opened in \n1900, has a 199-kilometre route and serves \na staggering 1191 million passengers yearly, \nthe highest among the listed cities. Tokyo\u2019s \nsystem, though younger (opened in 1927) \nand having a shorter 155-kilometre route, \nstill manages to serve 1927 million \npassengers, showcasing the city\u2019s dense \npopulation and reliance on public transport. \nIn contrast, Washington DC\u2019s metro, opened \nin 1976 with 126 kilometres of route, only \ncaters to 144 million passengers annually. \nKyoto and Los Angeles have the youngest \nand least extensive networks, opened in \n1981 and 2001 with 11 and 28 kilometres of \nroutes, respectively. Correspondingly, they \nhave the lowest annual passenger numbers, \nwith Kyoto at 45 million and Los Angeles at \n50 million. In summary, there is a noticeable \ndisparity in the age, route length, and \npassenger numbers among these cities. \nWhile older and more extensive networks \nlike London\u2019s do not necessarily translate to \nhigher passenger numbers, cities like Paris \nand Tokyo demonstrate a significant \ndemand for underground rail systems, likely \nattributed to their dense populations and \nurban structures.\nGPT4_score: 0.9\nFigure 6: GPT-4V prediction examples\n22\nReferences\n[1] Chatgpt\ncan\nnow\nsee,\nhear,\nand\nspeak.\nhttps://openai.com/blog/\nchatgpt-can-now-see-hear-and-speak, 2023.\n[2] Mpt. https://github.com/mosaicml/llm-foundry#mpt, 2023.\n[3] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In Proceedings of the\nIEEE/CVF international conference on computer vision, pages 8948\u20138957, 2019.\n[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in Neural Information Processing Systems, 35:23716\u201323736, 2022.\n[5] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical report. arXiv preprint\narXiv:2305.10403, 2023.\n[6] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. VQA: Visual Question Answering. In ICCV, 2015.\n[7] Junyi Ao, Rui Wang, Long Zhou, Chengyi Wang, Shuo Ren, Yu Wu, Shujie Liu, Tom Ko, Qing Li,\nYu Zhang, et al. Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing.\narXiv preprint arXiv:2110.07205, 2021.\n[8] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco,\nMitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-source framework for training large\nautoregressive vision-language models. arXiv preprint arXiv:2308.01390, 2023.\n[9] Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe,\nYonatan Bitton, Samir Gadre, Jenia Jitsev, Simon Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell\nWortsman, and Ludwig Schmidt. Openflamingo, March 2023.\n[10] Microsoft Azure. Azure cognitive services apis. https://azure.microsoft.com/en-us/products/\nai-services/ai-vision, 2023. Accessed: 2023-06-20.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[12] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[13] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early\nexperiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[14] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.\nCoyo-700m: Image-text pair dataset. https://github.com/kakaobrain/coyo-dataset, 2022.\n[15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021.\n[16] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language modeling\nframework for object detection. In ICLR, 2022.\n[17] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015.\n[18] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing\nLiu. Uniter: Learning universal image-text representations. In ECCV, 2020.\n[19] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations?\narXiv preprint arXiv:2305.01937, 2023.\n23\n[20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language\nmodeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[21] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022.\n[22] Marta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. No language left behind: Scaling human-centered\nmachine translation. arXiv preprint arXiv:2207.04672, 2022.\n[23] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with\ninstruction tuning. arXiv preprint arXiv:2305.06500, 2023.\n[24] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[25] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet,\nDaniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,\nAndy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model. In\narXiv preprint arXiv:2303.03378, 2023.\n[26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang,\nand Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19358\u201319369, 2023.\n[27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin,\nJinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation benchmark for multimodal large\nlanguage models. arXiv preprint arXiv:2306.13394, 2023.\n[28] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166, 2023.\n[29] Zhe Gan, Linjie Li, Chunyuan Li, Lijuan Wang, Zicheng Liu, and Jianfeng Gao. Vision-language\npre-training: Basics, recent advances, and future trends. arXiv preprint arXiv:2210.09263, 2022.\n[30] Difei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou.\nAssistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn. arXiv preprint\narXiv:2306.08640, 2023.\n[31] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham\nNeubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\n[32] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, et al. Llama-adapter v2: Parameter-efficient visual instruction model. arXiv preprint\narXiv:2304.15010, 2023.\n[33] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu, Wenwei\nZhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for dialogue with humans,\n2023.\n[34] Google. Bard. https://bard.google.com, 2023. Accessed: 2023-07-17.\n[35] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa\nmatter: Elevating the role of image understanding in visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\n[36] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556, 2022.\n[37] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning and\ncompositional question answering. In CVPR, 2019.\n24\n[38] Huggingface. Transformers agent. https://huggingface.co/docs/transformers/transformers_\nagents, 2023. Accessed: 2023-07-20.\n[39] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok\nHwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. Ocr-free document understanding transformer.\nIn European Conference on Computer Vision, pages 498\u2013517. Springer, 2022.\n[40] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or\nregion supervision. In ICML, 2021.\n[41] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations. IJCV, 2017.\n[42] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves\nStoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019.\n[43] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. Mimic-it: Multi-modal in-context instruction tuning. arXiv preprint arXiv:2306.05425, 2023.\n[44] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A multi-modal\nmodel with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023.\n[45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking\nmultimodal llms with generative comprehension, 2023.\n[46] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[47] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training\nfor unified vision-language understanding and generation. In International Conference on Machine\nLearning, pages 12888\u201312900. PMLR, 2022.\n[48] Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Pengchuan Zhang, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In\nECCV, 2020.\n[49] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n[50] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large\nmulti-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\n[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint\narXiv:2304.08485, 2023.\n[53] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. Gpteval: Nlg\nevaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023.\n[54] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv\npreprint arXiv:2307.06281, 2023.\n[55] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui\nChen, Chunyuan Li, Lianwen Jin, et al. On the hidden mystery of ocr in large multimodal models. arXiv\npreprint arXiv:2305.07895, 2023.\n[56] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In NeurIPS, 2019.\n[57] Timo L\u00fcddecke and Alexander Ecker. Image segmentation using text and image prompts. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7086\u20137096, 2022.\n[58] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question\nanswering benchmark requiring external knowledge. In CVPR, pages 3195\u20133204, 2019.\n25\n[59] OpenAI. Gpt-4 technical report, 2023.\n[60] OpenAI. Gpt-4v(ision) system card. 2023.\n[61] OpenAI. Gpt-4v(ision) technical work and authors. 2023.\n[62] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1 million\ncaptioned photographs. In NeurIPS, 2011.\n[63] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744, 2022.\n[64] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4.\narXiv preprint arXiv:2304.03277, 2023.\n[65] Thomas Politzer.\nVision is our dominant sense.\nhttps://www.brainline.org/article/\nvision-our-dominant-sense. Accessed: 2023-05-20.\n[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. arXiv preprint arXiv:2103.00020, 2021.\n[67] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. Robust\nspeech recognition via large-scale weak supervision. In International Conference on Machine Learning,\npages 28492\u201328518. PMLR, 2023.\n[68] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400\nmillion image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[69] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\n[70] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[71] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image\ncaptioning with reading comprehension. In ECCV, pages 742\u2013758, 2020.\n[72] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8317\u20138326, 2019.\n[73] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\n[74] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[75] Maria Tsimpoukelli, Jacob Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal\nfew-shot learning with frozen language models. arXiv preprint arXiv:2106.13884, 2021.\n[76] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,\nand Lijuan Wang. Git: A generative image-to-text transformer for vision and language. arXiv preprint\narXiv:2205.14100, 2022.\n[77] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie\nZhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for vision-centric\ntasks. arXiv preprint arXiv:2305.11175, 2023.\n[78] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald M Summers.\nChestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and\nlocalization of common thorax diseases. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 2097\u20132106, 2017.\n[79] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual\nlanguage model pretraining with weak supervision. In ICLR, 2022.\n26\n[80] Peng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan Huang,\nYu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large vision-language\nmodels. arXiv preprint arXiv:2306.09265, 2023.\n[81] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. Set-of-mark prompting\nunleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441, 2023.\n[82] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, and\nLijuan Wang. Unitab: Unifying text and box outputs for grounded vision-language modeling. In European\nConference on Computer Vision, pages 521\u2013539. Springer, 2022.\n[83] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of gpt-3 for few-shot knowledge-based vqa. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 36, pages 3081\u20133089, 2022.\n[84] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.\nThe dawn of lmms: Preliminary explorations with gpt-4v (ision). arXiv preprint arXiv:2309.17421, 2023.\n[85] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. arXiv preprint arXiv:2303.11381, 2023.\n[86] Zhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang,\nand Jiebo Luo. Tap: Text-aware pre-training for text-vqa and text-caption. In CVPR, pages 8751\u20138761,\n2021.\n[87] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu,\nPengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with\nmultimodality. arXiv preprint arXiv:2304.14178, 2023.\n[88] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual\ncommonsense reasoning. In CVPR, pages 6720\u20136731, 2019.\n[89] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing\nzero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022.\n[90] Yan Zeng, Hanbo Zhang, Jiani Zheng, Jiangnan Xia, Guoqiang Wei, Yang Wei, Yuchen Zhang, and Tao\nKong. What matters in training a gpt4-style language model with multimodal inputs? arXiv preprint\narXiv:2307.02469, 2023.\n[91] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\n[92] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging\nllm-as-a-judge with mt-bench and chatbot arena, 2023.\n[93] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[94] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu,\nLudwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal c4: An open, billion-scale corpus of\nimages interleaved with text. arXiv preprint arXiv:2304.06939, 2023.\n27\n"
  },
  {
    "title": "Convolutions Die Hard: Open-Vocabulary Segmentation with Single Frozen Convolutional CLIP",
    "link": "https://arxiv.org/pdf/2308.02487.pdf",
    "upvote": "11",
    "text": "Convolutions Die Hard: Open-Vocabulary\nSegmentation with Single Frozen Convolutional CLIP\nQihang Yu1, Ju He2, Xueqing Deng1, Xiaohui Shen1, Liang-Chieh Chen1\n1 ByteDance\n2 The Johns Hopkins University\nAbstract\nOpen-vocabulary segmentation is a challenging task requiring segmenting and\nrecognizing objects from an open set of categories in diverse environments. One\nway to address this challenge is to leverage multi-modal models, such as CLIP,\nto provide image and text features in a shared embedding space, which effec-\ntively bridges the gap between closed-vocabulary and open-vocabulary recognition.\nHence, existing methods often adopt a two-stage framework to tackle the problem,\nwhere the inputs first go through a mask generator and then through the CLIP model\nalong with the predicted masks. This process involves extracting features from raw\nimages multiple times, which can be ineffective and inefficient. By contrast, we\npropose to build everything into a single-stage framework using a shared Frozen\nConvolutional CLIP backbone, which not only significantly simplifies the current\ntwo-stage pipeline, but also remarkably yields a better accuracy-cost trade-off.\nThe resulting single-stage system, called FC-CLIP, benefits from the following\nobservations: the frozen CLIP backbone maintains the ability of open-vocabulary\nclassification and can also serve as a strong mask generator, and the convolutional\nCLIP generalizes well to a larger input resolution than the one used during con-\ntrastive image-text pretraining. Surprisingly, FC-CLIP advances state-of-the-art\nresults on various benchmarks, while running practically fast. Specifically, when\ntraining on COCO panoptic data only and testing in a zero-shot manner, FC-CLIP\nachieve 26.8 PQ, 16.8 AP, and 34.1 mIoU on ADE20K, 18.2 PQ, 27.9 mIoU on\nMapillary Vistas, 44.0 PQ, 26.8 AP, 56.2 mIoU on Cityscapes, outperforming the\nprior art under the same setting by +4.2 PQ, +2.4 AP, +4.2 mIoU on ADE20K,\n+4.0 PQ on Mapillary Vistas and +20.1 PQ on Cityscapes, respectively. Addi-\ntionally, the training and testing time of FC-CLIP is 7.5\u00d7 and 6.6\u00d7 significantly\nfaster than the same prior art, while using 5.9\u00d7 fewer total model parameters.\nMeanwhile, FC-CLIP also sets a new state-of-the-art performance across various\nopen-vocabulary semantic segmentation datasets. Code and models are available at\nhttps://github.com/bytedance/fc-clip.\n1\nIntroduction\nPanoptic segmentation [44] is a complex computer vision task that aims to predict a set of non-\noverlapping masks, each with its corresponding class label. It combines the tasks of semantic\nsegmentation [37] and instance segmentation [34], making it a challenging problem to solve. Many\nmethods [43, 87, 18, 83, 51, 93, 20, 94, 53] have been proposed to tackle this problem, and a\nsignificant progress has been made in terms of panoptic quality (PQ). However, due to the high cost\nof annotating such a fine-grained dataset [54, 22], the number of semantic classes is typically limited\nto a few dozens or hundreds. This restriction hinders the further application of existing approaches to\nreal-world settings, where the number of possible semantic classes is unlimited.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2308.02487v2  [cs.CV]  14 Nov 2023\nViT-L-14 @224px\nViT-L-14 @448px\nViT-L-14 @672px\nViT-L-14 @896px\nViT-L-14 @1120px\nConvNeXt-L @224px\nConvNeXt-L @448px\nConvNeXt-L @672px\nConvNeXt-L @896px\nConvNeXt-L @1120px\nFigure 1: k-means visualization on top of frozen CLIP backbone features w.r.t. different input\nresolutions. Both ViT-based and CNN-based CLIP produces semantic-meaningful features. However,\nwhen scaling up the input resolutions, we note that ViT-based CLIP features turn noisier, while\nCNN-based ones are smoother and generalize better. The smoother feature map is preferable for\nmask-pooling modules in our design.\nTo overcome the limitations of closed-vocabulary segmentation, open-vocabulary segmentation [48,\n90, 29, 25] has been proposed. These approaches uses text embeddings of category names [97],\nrepresented in natural language, as label embeddings, instead of learning them from the training\ndataset. By doing so, models can classify objects from a wider vocabulary, which improves their\nability to handle a broader range of categories. To ensure that meaningful embeddings are provided,\na pretrained text encoder [23, 70, 57, 69] is typically used. This encoder can effectively capture the\nsemantic meaning of words and phrases, which is critical for open-vocabulary segmentation.\nMulti-modal models, such as CLIP [69] and ALIGN [40], have shown promise for open-vocabulary\nsegmentation due to their ability to learn aligned image-text feature representations from large-scale\nInternet data [74]. SimBaseline [90] and OVSeg [52] are two recent methods that use a two-stage\nframework to adapt CLIP for open-vocabulary segmentation. In these methods, images are first\nprocessed by a heavy mask generator [36, 20] to obtain mask proposals, and then each masked image\ncrop is generated and fed into a frozen CLIP model for classification. MaskCLIP [25] extends this\napproach to open-vocabulary panoptic segmentation, but additionally leverages mask proposals as\nattention masks in the CLIP backbone to efficiently avoid multiple forwarding processes for the\nmasked crops. More recently, ODISE [89] employs a stable diffusion UNet [72, 71] as a frozen\nbackbone for mask generator, which significantly boosts the state-of-the-art performance. However,\ndespite these advances, they still rely on a two-stage framework, where the mask generator and CLIP\nclassifier extract features from raw images separately, resulting in inefficiency and ineffectiveness.\nA natural question thus arises as to whether it is possible to unify the mask generator and CLIP\nclassifier into a single-stage framework for open-vocabulary segmentation. Sharing the feature\nextractor between them is a straightforward solution, but it poses two challenges. First, fine-tuning\nCLIP backbone can disrupt the alignment between image and text features, resulting in a much\nworse performance on out-of-vocabulary categories. Existing methods [90, 52, 25, 89] rely on\nanother separate backbone for mask generator, increasing model size and computational costs.\nSecond, CLIP models are typically pretrained on relatively lower-resolution inputs, while dense\nprediction tasks require a much higher resolution for optimal performance. This makes it difficult\nto directly apply CLIP-pretrained backbones to downstream dense prediction tasks, particularly\nViT-based CLIP models [26], where careful treatments are required (e.g., side adapter [17, 91], or\ncost aggregation [101, 21]). Consequently, existing methods [25, 89] perform mask segmentation\nand CLIP classification at different input scales, leading to sub-optimal performance.\nTo alleviate the two challenges, we propose to build both mask generator and CLIP classifier on top of\na shared Frozen Convolutional CLIP backbone, resulting in a single-stage framework FC-CLIP. Its\n2\ndesign is based on the following observations. The frozen CLIP backbone ensures that the pretrained\nimage-text feature alignment is intact, allowing out-of-vocabulary classification. It can also serve as a\nstrong mask generator by appending a lightweight pixel decoder and mask decoder [20, 94]. The\nconvolutional CLIP, based on a Convolutional Neural Network (CNN) [47], empirically shows a\nbetter generalization ability compared to ViT-based CLIP [26], when the input size scales up. This\nechoes the success of fully convolutional networks [60] in dense prediction tasks. Both observations\nare critical for developing a single-stage framework, but they have been overlooked and undiscovered\nby existing two-stage pipelines [25, 89]. In Fig. 1, we visualize the learned visual representation of\nViT-based and CNN-based CLIP via k-means clustering [59]. As shown in the figure, the features\nlearned by CNN-based CLIP are more robust across different input sizes.\nSurprisingly, the adoption of a single frozen convolutional CLIP as the shared feature extractor results\nin an extremely simple yet effective design. Specifically, the single-stage FC-CLIP consists of three\nmodules built upon a shared frozen convolutional CLIP backbone: a class-agnostic mask generator,\nan in-vocabulary classifier, and an out-of-vocabulary classifier (see Fig. 2 for comparison between\npipelines). The proposed method not only enjoys a simple design, but also comes with a very low\ncost for both training and testing. As a comparison, our model has only 238M frozen parameters and\n21M trainable parameters, against the state-of-the-art work ODISE [89] that has 1494M frozen and\n28M trainable parameters. Furthermore, our model training only takes 25.6 V100 GPU days, which\nis 7.5\u00d7 faster compared to ODISE\u2019s 192 V100 GPU days. During inference, our model also runs\n6.6\u00d7 faster. Although FC-CLIP enjoys a simple design, it still outperforms previous methods across\nmultiple datasets. Trained on COCO panoptic dataset only, FC-CLIP surpasses prior state-of-the-art\nODISE [89] significantly in a zero-shot manner. Specifically, FC-CLIP achieves 26.8 PQ (+3.4),\n18.2 PQ (+4.0), and 44.0 PQ (+20.1) on ADE20K, Mapillary Vistas, and Cityscapes, respectively.\nAs panoptic segmentation unifies semantic and instance segmentation, FC-CLIP naturally extends\nto open-vocabulary semantic and instance segmentation. With the same model trained on COCO\npanoptic data only (i.e., no task-specific fine-tuning), FC-CLIP achieves state-of-the-art performance\non open-vocabulary instance and semantic segmentation. Specifically, FC-CLIP achieves 16.8 AP on\nADE20K, surpassing the state-of-art ODISE [89] by +2.4. FC-CLIP also outperforms the state-of-art\nspecialized open-vocabulary semantic segmentation model SAN [91] by +1.1 and +1.1 mIoU on the\nchallenging ADE20K-847 (A-847) and PASCAL-Context-459 (PC-459) benchmarks, respectively.\nIn summary, through the lens of a careful re-design of existing two-stage open-vocabulary segmen-\ntation models, we establish a simple, strong, and fast baseline for the community. The proposed\nFC-CLIP adopts a single-stage framework by exploiting a shared frozen convolutional CLIP, which\nnot only advances the state-of-the-art performances on multiple benchmarks, but also enjoys a practi-\ncally fast training and inference speed. We hope our study will inspire future research on efficient\nsingle-stage open-vocabulary segmentation models.\n2\nRelated Work\nVision-language models target at encoding vision and language jointly in a fusion model. Early\nworks [78, 16, 98] extract visual representations by pretrained object detectors and fine-tune on\ndownstream tasks with language supervision. Recently, with the breakthrough of large language\nmodels [23, 3], rapid progress has been made in this field. CLIP [69] and ALIGN [40] demonstrate\nthat pretraining dual-encoder models with contrastive objectives on large-scale noisy image-text\npairs can learn representation with cross-modal alignment ability and show strong performance in\nzero-shot downstream tasks. The following works [95, 1, 92] further confirm these points and achieve\nimpressive results in zero-shot transfer learning such as open-vocabulary image recognition.\nClosed-vocabulary segmentation can be divided into three types according to the semantics of the\ngrouping pixels, i.e. semantic, instance and panoptic segmentation. Semantic segmentation interprets\nhigh-level category semantic concepts. Prior works [9, 72, 10, 11, 13, 28, 96, 86, 99, 30] mainly treat\nthis task as a per-pixel classification problem and build their models on top of the idea of FCN [60].\nInstance segmentation groups foreground pixels into different object instances. Starting from Mask R-\nCNN [36], prior works [42, 56, 12, 6, 2, 8, 80, 84, 66] mainly address this task with mask classification,\nwhere a set of bounding boxes and binary masks are predicted. Panoptic segmentation seeks for\nholistic scene understanding including both stuff and things. The pioneering work [44] and prevalent\nones [55, 43, 87, 18, 50, 82, 14, 67] decompose the problem into various proxy tasks and merge the\n3\ntwo-stage:\nna\u00efve single-stage:\nFC-CLIP (ours):\nmask\ngenerator\nViT-based\nCLIP\nclass\nmask\nhigh-resolu@on\nimage\nCLIP\nmask\nclass\nCNN-based\nCLIP\nin-vocab\nclass\nmask\n\u2744 : frozen\nclassi\ufb01er\n: trainable\n\ud83d\udd25\n\ud83d\udd25\n\u2744\n\u2744\nout-vocab\nclass\nhigh-resolu@on\nimage\n\ud83d\udd25\n\u2744classi\ufb01er\n\u2744\n\ud83d\udd25\nclassi\ufb01er & mask\ngenerator\n\ud83d\udd25\nclassi\ufb01er & mask\ngenerator\n\ud83d\udd25\nbackbone\nhigh-resolu@on\nimage\nlow-resolu@on\nimage\n\ud83d\udd25\nFigure 2: Comparisons between open-vocabulary panoptic segmentation pipelines. Left: Existing\nmethods [25, 89] adopt a two-stage pipeline, where the first stage employs a high-resolution image\nto generate class-agnostic masks, and the second stage feeds both the low-resolution image and\npredicted masks to a frozen CLIP backbone for open-vocabulary recognition. This incurs heavy\ncomputation, as image features are extracted multiple times. Middle: A na\u00efve single-stage framework\nbuilds everything together and fine-tunes the CLIP backbone, breaking the pretrained alignment\nbetween images and texts. Right: Our single-stage framework FC-CLIP employs a shared frozen\nconvolutional CLIP, where \"frozen CLIP\" maintains the open-vocabulary recognition and can serve\nas a strong mask generator, and \"convolutional CLIP\" generalizes well to large input sizes. Note that\nthe predicted masks are used for CLIP recognition in all three schemes (not shown for simplicity).\nresults in the end. Recently, following DETR [7], most works [83, 76, 19, 20, 51, 93, 94, 39, 49, 77]\npresent end-to-end solutions based on the idea of mask classification. Standing on their shoulders,\nour proposed method builds on top of the pixel decoder and mask decoder of Mask2Former [20] by\nadditionally exploiting the open-vocabulary recognition ability from CLIP [69].\nOpen-vocabulary segmentation aims at segmenting arbitrary classes including those that can not be\naccessed during the training procedure. Priors works [48, 29, 90, 52, 24, 88, 101, 91, 104, 62, 102, 32]\nperform open-vocabulary semantic segmentation through leveraging large pretrained vision-language\nmodels [69, 40, 71]. Recently, MaskCLIP [25] presents a two-stage pipeline, which consists of a\nclass-agnostic mask generator and a frozen CLIP [69] encoder for cross-modal alignment, and thus\nexpands the scope of the CLIP models into open-vocabulary panoptic segmentation. ODISE [89]\ndigs out the innate potential of pretrained text-image diffusion models [71] in terms of the ability to\npresent open concepts in the representation space for performing strong open-vocabulary panoptic\nsegmentation. FreeSeg [68] encodes multi-granularity concepts into a compact textural abstraction,\nenabling generalizability to arbitrary text description. Unlike those methods, we propose a single-\nstage framework by exploiting a single frozen convolutional CLIP backbone, resulting in a simpler,\nfaster, and stronger model than existing works.\nWe also note that the pioneering work F-VLM [46] builds an open-vocabulary detection framework on\ntop of a frozen CLIP backbone. However, FC-CLIP differs from it with a totally different observation\nand motivation. Specifically, our work was initially motivated by the state-of-art open-vocabulary\nsegmentation model ODISE [89], which found that the CLIP backbone extracts noisier features than\ndiffusion models (Figure B. 1. in [89]), leading to inferior segmentation results (which justifies\ntheir adoption of diffusion models). Their observation motivated us to look deeply into the problem.\nInterestingly, our discoveries show that both ViT-based (used by ODISE [89]) and CNN-based\nCLIP can produce semantic-meaningful features. However, when scaling up the input resolutions,\nwe discover that ViT-based CLIP features turn noisier, while CNN-based ones are smoother and\ngeneralize better across input sizes. F-VLM [46] also empirically found that a frozen CLIP can\nprovide meaningful features for object detection. However, they did not choose CNN-based CLIP on\npurpose and thus did not compare carefully between ViT-based and CNN-based CLIP backbones.\nOn the other hand, in our paper, we have provided careful ablation studies on ViT-based and CNN-\nbased CLIP, where we observe that even though both ViT-based and CNN-based CLIP initially\nhave comparable performance at resolution 224, CNN-based CLIP shows better and more robust\nperformance when input resolution scales up.\n4\nCNN-\nbased\nCLIP\nimage\n\u2026\n\u2026\nmask\ndecoder\nmask\npooling\nmask predic/on\n\u2026\n\u2026\nclass predic/on\nmask\npooling\n\u2026\n\u2744\npredic/on\n: frozen\n\u2744\ntext embeddings\ngeometric\nensemble\n: in-vocab branch\n: out-vocab branch\nclass embeddings\nclass embeddings\n\u2026\n\u2026\npixel\ndecoder\n\u00d7\n\u00d7\nFigure 3: Overview of FC-CLIP, which contains three main components: mask generator, an\nin-vocabulary (in-vocab) classifier, and an out-of-vocabulary (out-vocab) classifier. All components\nbuild on top of a shared frozen covolutional CLIP backbone. The pixel decoder and mask decoder\nfollow the design of Mask2Former, and generate class-agnostic masks. The in-vocabulary classifier\nyields the class embeddings by mask-pooling over final pixel features from pixel decoder. During\ntesting, FC-CLIP additionally exploits the out-of-vocabulary classifier by mask-pooling over frozen\nCLIP backbone features, and the final class prediction is obtained by geometric ensembling both\nclassifiers. Note that the text embeddings are obtained by feeding category names into a CLIP text\nencoder, which are done beforehand and cached in memory, thus causing no additional costs. Also,\nthe class-agnostic mask proposals are fed to the mask pooling modules (not shown for simplicity).\n3\nMethod\nIn this section, we first define the problem of open-vocabulary segmentation. We then introduce the\nexisting two-stage pipeline, followed by our proposed single-stage framework FC-CLIP.\nProblem Definition\nOpen-vocabulary segmentation aims to segment the image I \u2208 RH\u00d7W \u00d73 into\na set of masks with associated semantic labels:\n{yi}K\ni=1 = {(mi, ci)}K\ni=1 .\n(1)\nThe K ground truth masks mi \u2208 {0, 1}H\u00d7W contain the corresponding ground truth class label\nci. During training, a fixed set of class labels Ctrain is used, while during inference, another set of\ncategories Ctest is used. In the open-vocabulary setting, Ctest may contain novel categories unseen\nduring training, i.e., Ctrain \u0338= Ctest. We follow previous works [25, 89] and assume the availability\nof the category names of Ctest (represented in natural language) during testing.\nTwo-Stage Open-Vocabulary Segmentation\nExisting works [90, 52, 25, 89] adopt a two-stage\npipeline for open-vocabulary segmentation. The first stage contains a class-agnostic mask generator\nM with parameters \u03b8M that generates a set of N mask proposals { \u02c6mi}N\ni=1 \u2208 RN\u00d7H\u00d7W , given the\ninput image I:\n{ \u02c6mi}N\ni=1 = M(I; \u03b8M) .\n(2)\nIn the second stage, a CLIP adapter P takes both image I and mask proposals { \u02c6mi}N\ni=1 as inputs,\nwhere the latter input is used to guide the frozen CLIP model CLIP \u2217 (\u2217 denotes frozen). The adapter\nperforms mask classification through forwarding processes with either masked crops [90, 52] or\nmasked attention [25, 89]:\n{\u02c6ci}N\ni=1 = P(I, { \u02c6mi}N\ni=1; CLIP \u2217) ,\n(3)\nwhere {\u02c6ci}N\ni=1 \u2208 RN\u00d7|C| refers to the predicted class probabilities for the N predicted masks,\nC \u2208 {Ctrain, Ctest} depending on training or testing phase, and |C| is the category size.\nAlthough this framework has achieved impressive open-vocabulary segmentation performance, it has\ntwo limitations. First, the image features are extracted twice, once for mask generation and the other\nfor mask classification. The double feature extractions incur heavy computation, making it costly\nto scale up backbone parameters. Second, the mask generator often requires high-resolution inputs\n(e.g., 1024 \u00d7 1024), whereas the CLIP model is usually pretrained with lower-resolution images (e.g.,\n5\n224 \u00d7 224). The two-stage pipeline thus needs to feed high-resolution images into the mask generator\nand low-resolution images into the CLIP classifier, making the model inefficient.\nNa\u00efve Single-Stage Open-Vocabulary Segmentation\nTo avoid increasing the model size and\ncomputational cost of duplicate feature extractions, one may na\u00efvely formulate everything together\ninto a single-stage framework F, where both mask generator and mask classifier share the same\nCLIP-pretrained backbone CLIP (not frozen) for extracting features from an input image I:\n{ \u02c6mi, \u02c6ci}N\ni=1 = F(I; CLIP, \u03b8M) .\n(4)\nHowever, we empirically discover that fine-tuning this na\u00efve single-stage framework causes a mis-\nalignment between image and text features in the pretrained CLIP model, leading to sub-optimal\nperformance, especially for novel unseen classes. It also increases the training costs by 2.1\u00d7 to\n52.8 GPU days. Interestingly, our experiments also show that a frozen CLIP backbone can pro-\nvide sufficient features for mask generation, while preserving the image-text aligned representation.\nNevertheless, we still face another challenge, where CLIP models are usually pretrained on low-\nresolution images (e.g., 224 \u00d7 224), whereas segmentation models prefer higher-resolution inputs\n(e.g., 800 \u00d7 1333 for COCO, or 1024 \u00d7 2048 for Cityscapes). This discrepancy results in the signifi-\ncant performance degradation, when applying a frozen CLIP on large input images. Digging into\nthe details, we found that it is related to the popular ViT [26] backbone used in CLIP that does not\ntransfer well to different input sizes, which could be alleviated by extra careful designs (e.g., side\nadapter [17, 91], or cost aggregation [101, 21]). On the other hand, CNN-based CLIP models (such\nas ResNet [35] and ConvNeXt [58]) exhibit better generalization ability to different input sizes, due\nto their fully convolutional nature [60]. Additionally, the CNN-based CLIP backbone, extracting\nmulti-scale feature maps, can be used as a simple plug-in module into modern closed-vocabulary\nsegmentation models [20, 94]. Motivated by the observations, we thus propose FC-CLIP, a simple\nyet effective single-stage open-vocabulary segmentation framework built entirely on a single frozen\nconvolutional CLIP backbone CLIP \u2217\nCNN:\n{ \u02c6mi, \u02c6ci}N\ni=1 = F(I; CLIP \u2217\nCNN, \u03b8M) .\n(5)\nFC-CLIP\nThe proposed FC-CLIP leverages the semantic features of a frozen CNN-based CLIP\nbackbone for both mask generation and CLIP classification. Unlike previous works [90, 52, 25, 89],\nwhich often train a separate mask generator and ignore the potential reuse of CLIP\u2019s semantic\nfeatures, we incorporate the CNN-based CLIP backbone into the state-of-the-art segmentation\nmethod Mask2Former [20]. We note that FC-CLIP is a general meta-architecture that can build\non top of several modern segmentation methods [20, 94]. Our approach offers several advantages.\nBy freezing and sharing the backbone features, our model is significantly more efficient during\nboth training and testing (i.e., avoiding feature duplication). The CNN-based CLIP backbone not\nonly transfers well to different input resolutions (from its pretrained image size), but also generates\nmulti-scale feature maps, seamlessly compatible with modern segmentation methods [20, 94]. At\na high level, FC-CLIP consists of three components: class-agnostic mask generator, in-vocabulary\nclassifier, and out-of-vocabulary classifier. We detail each component below.\nClass-Agnostic Mask Generator\nFollowing Mask2Former [20], we use a pixel decoder enhanced\nwith multi-scale deformable attention [103] to improve the features extracted from the frozen CNN-\nbased CLIP backbone. The enhanced pixel features, together with a set of object queries [7, 83], are\nthen passed through a series of mask decoders, where each consists of masked cross-attention [20],\nself-attention [81], and a feed-forward network. The resulting segmentation logits are obtained by\nperforming a matrix multiplication between the object query and pixel features. The predicted masks\nare matched with ground-truth masks in a one-to-one manner through Hungarian matching [45]\nand are supervised accordingly. Moreover, as the number of object queries is often greater than the\nnumber of labeled masks, only a subset of predicted masks are optimized through this matching\nprocess. We apply no penalty to the remaining unmatched proposals, which ensures that more mask\nproposals are obtained.\nIn-Vocabulary Classifier\nOnce the mask proposals are predicted, they are classified with category\ntext embedding in a contrastive manner, where the class embeddings for each mask and category text\nembeddings are projected into a common embedding space. That is, the predicted class probability\nby in-vocabulary classifier is defined as follows: \u2200i = 1, . . . , N\n\u02c6ci,in = softmax( 1\nT\n\u0002cos(vi, t1), cos(vi, t2), \u00b7 \u00b7 \u00b7 , cos(vi, t|C|)\u0003\n),\n(6)\n6\nwhere T is a learnable temperature parameter with initialization of 0.07 to control the sharpness of the\ndistribution, cos is cosine distance measurement, vi is the class embeddings for i-th predicted mask,\nwhich is obtained by mask pooling over the final pixel features from pixel decoder, similar to [29]. tj\nis the category name\u2019s text embeddings of class j, which is obtained by feeding the category name to\na CLIP-pretrained text encoder. Note that these category text embeddings only need to be generated\nonce. They are then kept in memory to serve as text classifiers, and thus it incurs negligible additional\ncost during training. This forms our in-vocabulary classifier.\nOut-of-Vocabulary Classifier\nDuring inference, however, we notice that using the in-vocabulary\nclassifier alone fails to generalize to completely novel unseen classes, as the model is only trained\non a finite set of categories and thus could not recognize diverse novel concepts. To address this\nissue, we introduce an out-of-vocabulary classifier, which applies mask pooling to the frozen CLIP\nbackbone features, aiming to borrow the pretrained (intact) open-vocabulary recognition ability from\nCLIP. Unlike the other two-stage methods [90, 52, 25, 89], where one or multiple forward processes\nof CLIP are needed, the adopted out-of-vocabulary classifier introduces marginal additional costs,\nsince the backbone features are already extracted (and only lightweight mask-pooling is performed).\nThe predicted class probability by out-of-vocabulary classifier \u02c6ci,out is then obtained in a manner\nsimilar to Eq. (6) by replacing vi with the mask-pooled features over frozen CLIP backbone features.\nThis classifier strictly maintains the original CLIP feature distribution, allowing us to better recognize\nbrand new categories. Note that the out-of-vocabulary classifier is only performed during testing.\nCombining In- and Out-of-Vocabulary Classifiers\nFollowing prior works [31, 29, 46, 89], we\nemploy geometric ensemble to fuse the classification scores between in-vocabulary and out-of-\nvocabulary classifiers. That is, \u2200j = 1, . . . , |C|\n\u02c6ci(j) =\n\u001a(\u02c6ci,in(j))(1\u2212\u03b1) \u00b7 (\u02c6ci,out(j))\u03b1,\nif j \u2208 Ctrain\n(\u02c6ci,in(j))(1\u2212\u03b2) \u00b7 (\u02c6ci,out(j))\u03b2,\notherwise\n(7)\nwhere \u02c6ci(j) denotes the j-th element of \u02c6ci, and the underscripts in and out refer to in-vocabulary\nand out-of-vocabulary classifier, respectively. \u03b1, \u03b2 \u2208 [0, 1] balance the predictions between in- and\nout-of-vocabulary classifiers for seen and novel unseen categories.\n4\nExperimental Results\nHerein, we provide implementation details of FC-CLIP in Sec. 4.1. After setting the stage, we\nintroduce our main results, compared with state-of-the-art methods and ablations studies in Sec. 4.2.\n4.1\nImplementation Details\nArchitecture\nWe use ConvNeXt-Large CLIP [58, 69] backbones from OpenCLIP [38]1 pretrained\non LAION-2B [74] dataset. On top of the CLIP backbone, we build the mask generator, following\nMask2Former [20]. Nine mask decoders are employed to generate the class-agnostic masks by taking\nas inputs the enhanced pixel features and a set of object queries. For in-vocabulary classification,\nfollowing [29], the class embeddings are obtained by mask-pooling the pixel features from the pixel\ndecoder\u2019s final output. Afterwards, the classification logits (before softmax) is obtained by matrix\nmultiplication between the predicted class embeddings and categories\u2019 text embeddings.\nTraining Strategy\nWe follow [20] and adopt the same training recipe and losses without any special\ndesign. The training is optimized with AdamW [41, 61] optimizer and weight decay 0.05. We use a\ncrop size of 1024 \u00d7 1024. We employ the learning rate 1 \u00d7 10\u22124 and a multi-step decay schedule.\nThe training batch size is 16, and the model is trained for 50 epochs on COCO panoptic training\nset [54].\nInference Strategy\nDuring inference, the shorted side of input images will be resized to 800\nwhile ensuring longer side not exceeds 1333. For Cityscapes and Mapillary Vistas, we increase the\nshorter side size to 1024. We adopt mask-wise merging scheme [20] for the mask predictions. The\nout-of-vocabulary classifier is only performed during inference by mask-pooling over the frozen\nCLIP backbone features. The final classification results are then obtained by geometric ensembling\nin- and out-of-vocabulary classifiers [31, 29, 46, 89], as in Eq. (7), where we default \u03b1 = 0.4 and\n1https://github.com/mlfoundations/open_clip\n7\nTable 1: Open-vocabulary panoptic segmentation performance on ADE20K. The proposed FC-\nCLIP demonstrates better performances than prior arts, while using much fewer frozen parameters.\nWe provide more results in the supplementary material\nzero-shot test dataset\ntraining dataset\nparams (M)\nADE20K\nCOCO\nmethod\nfrozen\ntrainable\nPQ\nAP\nmIoU\nPQ\nAP\nmIoU\nMaskCLIP [25]\n304\n63\n15.1\n6.0\n23.7\n-\n-\n-\nFreeSeg [68]\n-\n-\n16.3\n6.5\n24.6\n-\n-\n-\nODISE [89]\n1494\n28\n22.6\n14.4\n29.9\n55.4\n46.0\n65.2\nODISE [89] (caption)\n1494\n28\n23.4\n13.9\n28.7\n45.6\n38.4\n52.4\nFC-CLIP (ours)\n200\n21\n26.8\n16.8\n34.1\n54.4\n44.6\n63.7\nTable 2: Open-vocabulary panoptic segmentation performance on street-view datasets. The\nproposed FC-CLIP demonstrates better transferability to street-view dataset\nzero-shot test dataset\nMapillary Vistas\nCityscapes\nmethod\nPQ\nSQ\nRQ\nmIoU\nPQ\nSQ\nRQ\nAP\nmIoU\nODISE [89]\n14.2\n61.0\n17.2\n-\n23.9\n75.3\n29.0\n-\n-\nFC-CLIP (ours)\n18.2\n57.7\n22.9\n27.9\n44.0\n75.4\n53.6\n26.8\n56.2\n\u03b2 = 0.8. Following prior arts, we also adopt prompt engineering from [29, 89] and prompt templates\nfrom [31, 52]. If not specified, FC-CLIP is only trained on COCO panoptic dataset [54]. Following\nprior works [29, 89], we zero-shot evaluate the model on ADE20K [100], Cityscapes [22], and\nMapillary Vistas [64] for open-vocabulary panoptic segmentation. We also report open-vocabulary\nsemantic segmentation results on those datasets along with PASCAL datasets [27, 63]. The panoptic\nsegmentation results are evaluated with the panoptic quality (PQ) [44], Average Precision (AP), and\nmean intersection-over-union (mIoU), and semantic segmentation is evaluated with mIoU [27]. Note\nthat all results are obtained with the same single checkpoint trained on COCO panoptic data only.\n4.2\nResults\nWe summarize the main results for open-vocabulary panoptic segmentation and semantic segmentation\nin Tab. 1, Tab. 2 and Tab. 3, where we train FC-CLIP on COCO train set with panoptic annotation\nand evaluate it on various datasets in a zero-shot manner.\nOpen-Vocabulary Panoptic Segmentation Evaluation on ADE20K\nIn Tab. 1, we compare our\nFC-CLIP with other state-of-the-art methods on ADE20K [100], the main test-bed of zero-shot\nopen-vocabulary panoptic segmentation. As shown in the table, our method achieves significantly\nbetter performance compared to MaskCLIP [25], with +11.7 PQ, +10.8 AP and +10.4 mIoU, even\nthough we use fewer frozen (\u221266M) and trainable (\u221242M) parameters. When compared to the\nconcurrent methods FreeSeg [68] and ODISE [89], the advantage of FC-CLIP persists. FC-CLIP is\n+10.5 PQ, +10.3 AP, and +9.5 mIoU better than FreeSeg without using COCO-Stuff annotations [5]\n(which contains more semantic classes than COCO-Panoptic). Our PQ, AP, mIoU score are also\n+4.2, +2.4, +4.2 higher than ODISE under the same training settings. Compared to ODISE with\ncaption [15] for supervision, our model still outperforms it by +3.4 PQ, setting a new state-of-the-art\nrecord. Meanwhile, it is noticeable that our model has 6.3\u00d7 (5.9\u00d7) significantly fewer frozen (total)\nparameters compared to ODISE, which utilizes a strong large backbone from stable diffusion [71] for\nfeature extraction.\nOpen-Vocabulary Panoptic Segmentation Evaluation on Street-View Datasets\nIn Tab. 2, we\nevaluate on Cityscapes and Mapillary Vistas, which focus on street driving scenes. Compared to\nstate-of-the-art method ODISE, FC-CLIP achieves better performances on both datasets. Specifically,\nit outperforms ODISE by +4.0 PQ and +20.1 PQ on Mapillary Vistas and Cityscapes, respectively.\nNotably, FC-CLIP has a slightly lower SQ, which indicates our mask generator is actually weaker\nthan the one in ODISE, which utilizes a much larger backbone.\nOpen-Vocabulary Semantic Segmentation Evaluation\nAlthough our model was trained on COCO\npanoptic data only, it also performs well on open-vocabulary semantic segmentation. In Tab. 3, we\n8\nTable 3: Open-vocabulary semantic segmentation performance. The proposed FC-CLIP also\ndemonstrates state-of-the-art performances on open-vocabulary semantic segmentation\nmIoU\nmethod\ntraining dataset\nA-847 PC-459 A-150 PC-59 PAS-21 PAS-20\nSPNet [85]\nPascal VOC [27]\n-\n-\n-\n24.3\n18.3\n-\nZS3Net [4]\nPascal VOC [27]\n-\n-\n-\n19.4\n38.3\n-\nLSeg [48]\nPascal VOC [27]\n-\n-\n-\n-\n47.4\n-\nGroupViT [88]\nGCC [75]+YFCC [79]\n4.3\n4.9\n10.6\n25.9\n50.7\n52.3\nSimBaseline [90]\nCOCO Stuff [5]\n-\n-\n15.3\n-\n74.5\n-\nZegFormer [24]\nCOCO Stuff [5]\n-\n-\n16.4\n-\n73.3\n-\nLSeg+ [48, 29]\nCOCO Stuff [5]\n3.8\n7.8\n18.0\n46.5\n-\n-\nOVSeg [52]\nCOCO Stuff [5]\n9.0\n12.4\n29.6\n55.7\n-\n94.5\nSAN [91]\nCOCO Stuff [5]\n13.7\n17.1\n33.3\n60.2\n-\n95.5\nOpenSeg [29]\nCOCO Panoptic + COCO Caption\n6.3\n9.0\n21.1\n42.1\n-\n-\nODISE [89] (caption) COCO Panoptic + COCO Caption\n11.0\n13.8\n28.7\n55.3\n82.7\n-\nMaskCLIP [25]\nCOCO Panoptic\n8.2\n10.0\n23.7\n45.9\n-\n-\nODISE [89]\nCOCO Panoptic\n11.1\n14.5\n29.9\n57.3\n84.6\n-\nFC-CLIP (ours)\nCOCO Panoptic\n14.8\n18.2\n34.1\n58.4\n81.8\n95.4\nTable 4: FPS comparison. All results are obtained with one V100 GPU, CUDA 11.6 and PyTorch\n1.13, by taking the average runtime on the entire validation set, including post-processing time\nmethod\nADE20K\nCOCO\nODISE [89]\n0.41\n0.39\nFC-CLIP (ours)\n2.71 (6.61\u00d7)\n2.76 (7.08\u00d7)\nreport our model\u2019s performance on various benchmarks against other open-vocabulary segmentation\nmodels, where FC-CLIP shows an overall superior performance. Specifically, with the same training\nannotations used, FC-CLIP outperforms MaskCLIP by +6.6, +8.2, +10.4, +12.5 mIoU across\nA-847, PC-459, A-150, and PC-59, respectively. Compared to methods with caption annotations,\nFC-CLIP persists its advantages, where it outperforms ODISE (caption) by +3.8, +4.4, +5.4, +3.1\nmIoU across datasets A-847, PC-459, A-150, PC-59 respectively. Against other open-vocabulary\nsemantic segmentation methods, our model maintains its advantages across different datasets, despite\nbeing trained solely with panoptic annotations. Furthermore, it demonstrates comparable performance\nto state-of-the-art open-vocabulary semantic segmentation methods, which utilize the COCO-Stuff\ndataset as their training set. The COCO-Stuff dataset comprises 171 classes, 38 more classes than\nCOCO-Panoptic, and offers highly desirable annotations for semantic segmentation tasks. It is\nworth mentioning that these methods build their approach on top of ViT-L (with extra designs [91]),\nresulting in a significantly larger model size compared to our deployed ConvNeXt-L (304M vs.\n198M). Despite the disparity in model size, FC-CLIP remains competitive in terms of performance.\nSpecifially, FC-CLIP outperforms state-of-the-art open-vocabulary semantic segmentation method\nSAN [91] by 1.1 and 1.1 mIoU on the challenging A-847 and PC-459 datasets.\nInference Speed\nWe provide a comparison of FPS (frames per second) in Tab. 4. The proposed\nFC-CLIP not only demonstrates superior performances, but also enjoys a significant fast inference\ntime: FC-CLIP runs 6.61\u00d7 and 7.08\u00d7 faster than ODISE evaluated on ADE20K and COCO datasets,\nrespectively.\nTraining on ADE20K and Evaluating on COCO\nWe further validate the effectiveness of FC-CLIP\nby using a different training dataset. Specifically, we follow [68, 89] to train our model on ADE20K\ndataset with panoptic annotation, and evaluate it on COCO panoptic dataset. As shown in Tab. 5,\nFC-CLIP outperforms FreeSeg [68] by +10.5 PQ, and ODISE [89] by +2.0 PQ on COCO dataset.\nNotably, our model actually has a lower SQ (\u22121.4) compared to ODISE, which utilizes a much larger\nbackbone and thus has a stronger mask generator. Nevertheless, FC-CLIP still outperforms ODISE\nsignificantly with a simple yet effective design.\nFine-tuning CLIP Backbone Harms Performance on Novel Vocabularies\nWe validate the\nnecessity of freezing CLIP backbone to ensure a better generalization to novel vocabularies. We\ncompare the performance of trainable CLIP variant and frozen CLIP variant in Fig. 4, where we use\nthe same mask proposals to ensure a fair comparison. Specifically, we compare the performance on\n9\nTable 5: Results of training on ADE20K panoptic and evaluating on COCO panoptic val set.\nThe proposed FC-CLIP performs better than prior arts, even in the different setting (i.e., trained on\nADE20K and zero-shot evaluated on COCO)\nzero-shot test dataset\ntraining dataset\nCOCO\nADE20K\nmethod\nPQ\nSQ\nRQ\nPQ\nSQ\nRQ\nFreeSeg [68]\n16.5\n72.0\n21.6\n-\n-\n-\nODISE [89]\n25.0\n79.4\n30.4\n31.4\n77.9\n36.9\nFC-CLIP (ours)\n27.0\n78.0\n32.9\n41.9\n78.2\n50.2\nFigure 4: Trainable CLIP vs. Frozen CLIP, with per-class PQ analysis. We show 10 common\nclasses (labeled in green) shared by COCO and ADE20K, and 10 novel classes (labeled in red) that\nare only in ADE20K. The frozen CLIP demonstrates a much better recognition ability for novel\nclasses, while performing similarly for the seen classes.\n10 seen classes, which are shared by both COCO and ADE20K (e.g., person, sky), and 10 unseen\nclasses, which are only included in ADE20K dataset (e.g., arcade machine, dishwasher). As shown in\nthe figure, tuning CLIP backbone leads to a worse performance on unseen concepts, which breaks the\nCLIP feature alignment and thus loses its recognition ability on a much wider vocabulary.\n5\nConclusion\nIn this work, we have presented FC-CLIP, a simple yet effective single-stage framework for open-\nvocabulary segmentation. FC-CLIP shows great potential by building everything on top of a shared\nfrozen convolutional CLIP backbone, which not only significantly reduces training and testing costs,\nbut also establishes a strong baseline on multiple benchmarks. Our study demonstrates how to better\nadapt a pretrained CLIP model for downstream dense prediction tasks, which we hope will shed the\nlight on unleashing CLIP\u2019s potential for other various downstream tasks.\nLimitations\nFC-CLIP presents a simple single-stage open-vocabulary segmentation framework\nwith state-of-the-art performance. We note that there exist some interesting research topics to be\nexplored in the near future, such as better unleashing CLIP\u2019s potential in both mask segmentation and\nclassification, how to deal with conflict or overlapping vocabularies (e.g., cat vs. cat head), etc.\nBroader Impact\nFC-CLIP shows great potential for segmenting and naming every object in the\nscene, which could facilitate many applications including intelligent home assistants, robots, self-\ndriving, etc. Yet it relies on CLIP model pre-trained on the Internet data that may be biased, which\ncalls for future research for calibration to avoid misuse.\n10\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. In NeurIPS, 2022.\n[2] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In\nICCV, 2019.\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nIn NeurIPS, 2020.\n[4] Maxime Bucher, Tuan-Hung Vu, Matthieu Cord, and Patrick P\u00e9rez. Zero-shot semantic segmentation. In\nNeurIPS, 2019.\n[5] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context. In\nCVPR, 2018.\n[6] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR,\n2018.\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[8] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu,\nJianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, 2019.\n[9] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic\nimage segmentation with deep convolutional nets and fully connected crfs. In ICLR, 2015.\n[10] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab:\nSemantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.\nIEEE TPAMI, 2017.\n[11] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolu-\ntion for semantic image segmentation. arXiv:1706.05587, 2017.\n[12] Liang-Chieh Chen, Alexander Hermans, George Papandreou, Florian Schroff, Peng Wang, and Hartwig\nAdam. Masklab: Instance segmentation by refining object detection with semantic and direction features.\nIn CVPR, 2018.\n[13] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-\ndecoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\n[14] Liang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scaling wide residual networks for panoptic segmen-\ntation. arXiv:2011.11675, 2020.\n[15] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv:1504.00325,\n2015.\n[16] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning. In ECCV, 2020.\n[17] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer\nadapter for dense predictions. In ICLR, 2023.\n[18] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and\nLiang-Chieh Chen. Panoptic-DeepLab: A Simple, Strong, and Fast Baseline for Bottom-Up Panoptic\nSegmentation. In CVPR, 2020.\n[19] Bowen Cheng, Alexander G Schwing, and Alexander Kirillov. Per-pixel classification is not all you need\nfor semantic segmentation. In NeurIPS, 2021.\n[20] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-\nattention mask transformer for universal image segmentation. In CVPR, 2022.\n11\n[21] Seokju Cho, Heeseong Shin, Sunghwan Hong, Seungjun An, Seungjun Lee, Anurag Arnab, Paul Hong-\nsuck Seo, and Seungryong Kim. Cat-seg: Cost aggregation for open-vocabulary semantic segmentation.\narXiv:2303.11797, 2023.\n[22] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Be-\nnenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene\nunderstanding. In CVPR, 2016.\n[23] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\n[24] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling zero-shot semantic segmentation. In\nCVPR, 2022.\n[25] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary universal image segmentation with maskclip.\nIn ICML, 2023.\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n[27] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\npascal visual object classes (voc) challenge. IJCV, 88:303\u2013338, 2010.\n[28] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention\nnetwork for scene segmentation. In CVPR, 2019.\n[29] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with\nimage-level labels. In ECCV, 2022.\n[30] Jiaqi Gu, Hyoukjun Kwon, Dilin Wang, Wei Ye, Meng Li, Yu-Hsin Chen, Liangzhen Lai, Vikas Chandra,\nand David Z Pan. Multi-scale high-resolution vision transformer for semantic segmentation. In CVPR,\n2022.\n[31] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and\nlanguage knowledge distillation. In ICLR, 2022.\n[32] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi,\nWeicheng Kuo, Huizhong Chen, Liang-Chieh Chen, and David A Ross. Dataseg: Taming a universal\nmulti-dataset multi-task segmentation model. NeurIPS, 2023.\n[33] Tanmay Gupta, Arash Vahdat, Gal Chechik, Xiaodong Yang, Jan Kautz, and Derek Hoiem. Contrastive\nlearning for weakly supervised phrase grounding. In ECCV, 2020.\n[34] Bharath Hariharan, Pablo Arbel\u00e1ez, Ross Girshick, and Jitendra Malik. Simultaneous detection and\nsegmentation. In ECCV, 2014.\n[35] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n[36] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In ICCV, 2017.\n[37] Xuming He, Richard S Zemel, and Miguel \u00c1 Carreira-Perpi\u00f1\u00e1n. Multiscale conditional random fields for\nimage labeling. In CVPR, 2004.\n[38] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori,\nAchal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and\nLudwig Schmidt. Openclip, July 2021. URL https://doi.org/10.5281/zenodo.5143773.\n[39] Jitesh Jain, Jiachen Li, MangTik Chiu, Ali Hassani, Nikita Orlov, and Humphrey Shi. Oneformer: One\ntransformer to rule universal image segmentation. In CVPR, 2023.\n[40] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text\nsupervision. In ICML, 2021.\n[41] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[42] Alexander Kirillov, Evgeny Levinkov, Bjoern Andres, Bogdan Savchynskyy, and Carsten Rother. In-\nstancecut: from edges to instances with multicut. In CVPR, 2017.\n12\n[43] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In\nCVPR, 2019.\n[44] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation.\nIn CVPR, 2019.\n[45] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly,\n2(1-2):83\u201397, 1955.\n[46] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Open-vocabulary\nobject detection upon frozen vision and language models. In ICLR, 2023.\n[47] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n[48] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen Koltun, and Ren\u00e9 Ranftl. Language-driven\nsemantic segmentation. In ICLR, 2022.\n[49] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino:\nTowards a unified transformer-based framework for object detection and segmentation. In CVPR, 2023.\n[50] Qizhu Li, Xiaojuan Qi, and Philip HS Torr. Unifying training and inference for panoptic segmentation. In\nCVPR, 2020.\n[51] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Tong Lu, and Ping\nLuo. Panoptic segformer. In CVPR, 2022.\n[52] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter\nVajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In CVPR,\n2023.\n[53] James Liang, Tianfei Zhou, Dongfang Liu, and Wenguan Wang. Clustseg: Clustering for universal\nsegmentation. In ICML, 2023.\n[54] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n[55] Huanyu Liu, Chao Peng, Changqian Yu, Jingbo Wang, Xu Liu, Gang Yu, and Wei Jiang. An end-to-end\nnetwork for panoptic segmentation. In CVPR, 2019.\n[56] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance\nsegmentation. In CVPR, 2018.\n[57] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\narXiv:1907.11692, 2019.\n[58] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A\nconvnet for the 2020s. In CVPR, 2022.\n[59] Stuart Lloyd. Least squares quantization in pcm. IEEE transactions on information theory, 28(2):129\u2013137,\n1982.\n[60] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmen-\ntation. In CVPR, 2015.\n[61] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\n[62] Chaofan Ma, Yuhuan Yang, Yanfeng Wang, Ya Zhang, and Weidi Xie. Open-vocabulary semantic\nsegmentation with frozen vision-language models. In BMVC, 2022.\n[63] Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel\nUrtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild.\nIn CVPR, 2014.\n[64] Gerhard Neuhold, Tobias Ollmann, Samuel Rota Bulo, and Peter Kontschieder. The mapillary vistas\ndataset for semantic understanding of street scenes. In ICCV, 2017.\n[65] Jordi Pont-Tuset, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. Connecting\nvision and language with localized narratives. In ECCV, 2020.\n13\n[66] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature\npyramid and switchable atrous convolution. In CVPR, 2021.\n[67] Siyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Vip-deeplab: Learning\nvisual perception with depth-aware video panoptic segmentation. In CVPR, 2021.\n[68] Jie Qin, Jie Wu, Pengxiang Yan, Ming Li, Ren Yuxi, Xuefeng Xiao, Yitong Wang, Rui Wang, Shilei Wen,\nXin Pan, et al. Freeseg: Unified, universal and open-vocabulary image segmentation. In CVPR, 2023.\n[69] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, 2021.\n[70] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.\nTMLR, 21(1):5485\u20135551, 2020.\n[71] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, 2022.\n[72] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, 2015.\n[73] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael S. Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet\nlarge scale visual recognition challenge. IJCV, 115:211\u2013252, 2015.\n[74] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale\ndataset for training next generation image-text models. In NeurIPS, 2022.\n[75] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565,\n2018.\n[76] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic\nsegmentation. In ICCV, 2021.\n[77] Shuyang Sun, Weijun Wang, Qihang Yu, Andrew Howard, Philip Torr, and Liang-Chieh Chen. Remax:\nRelaxing for better training on efficient panoptic segmentation. NeurIPS, 2023.\n[78] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\nIn EMNLP, 2019.\n[79] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian\nBorth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59\n(2):64\u201373, 2016.\n[80] Zhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV,\n2020.\n[81] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[82] Huiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-\nDeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In ECCV, 2020.\n[83] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end\npanoptic segmentation with mask transformers. In CVPR, 2021.\n[84] Xinlong Wang, Rufeng Zhang, Tao Kong, Lei Li, and Chunhua Shen. SOLOv2: Dynamic and fast\ninstance segmentation. In NeurIPS, 2020.\n[85] Yongqin Xian, Subhabrata Choudhury, Yang He, Bernt Schiele, and Zeynep Akata. Semantic projection\nnetwork for zero-and few-label semantic segmentation. In CVPR, 2019.\n[86] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer:\nSimple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021.\n14\n[87] Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun.\nUpsnet: A unified panoptic segmentation network. In CVPR, 2019.\n[88] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang.\nGroupvit: Semantic segmentation emerges from text supervision. In CVPR, 2022.\n[89] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-\nvocabulary panoptic segmentation with text-to-image diffusion models. In CVPR, 2023.\n[90] Mengde Xu, Zheng Zhang, Fangyun Wei, Yutong Lin, Yue Cao, Han Hu, and Xiang Bai. A simple\nbaseline for zero-shot semantic segmentation with pre-trained vision-language model. In ECCV, 2022.\n[91] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai. Side adapter network for open-\nvocabulary semantic segmentation. In CVPR, 2023.\n[92] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:\nContrastive captioners are image-text foundation models. TMLR, 2022.\n[93] Qihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan\nYuille, and Liang-Chieh Chen. Cmt-deeplab: Clustering mask transformers for panoptic segmentation. In\nCVPR, 2022.\n[94] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and\nLiang-Chieh Chen. k-means Mask Transformer. In ECCV, 2022.\n[95] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu,\nXuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision.\narXiv:2111.11432, 2021.\n[96] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmenta-\ntion. In ECCV, 2020.\n[97] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and Shih-Fu Chang. Open-vocabulary object detection\nusing captions. In CVPR, 2021.\n[98] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and\nJianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\n[99] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence\nperspective with transformers. In CVPR, 2021.\n[100] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing\nthrough ade20k dataset. In CVPR, 2017.\n[101] Chong Zhou, Chen Change Loy, and Bo Dai. Extract free dense labels from clip. In ECCV, 2022.\n[102] Ziqin Zhou, Yinjie Lei, Bowen Zhang, Lingqiao Liu, and Yifan Liu. Zegclip: Towards adapting clip for\nzero-shot semantic segmentation. In CVPR, 2023.\n[103] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable\ntransformers for end-to-end object detection. In ICLR, 2021.\n[104] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang,\nLu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao. Generalized decoding for pixel,\nimage, and language. In CVPR, 2023.\n15\nAppendix\nIn the following supplementary materials, we present additional experimental results\npertaining to the design of FC-CLIP. Our supplementary analysis also includes comparisons against\nother methods that specifically address open-vocabulary semantic segmentation, ensemble methods,\nand hyperparameter tuning. Furthermore, we provide a quantitative comparison between ViT-based\nCLIP and CNN-based CLIP across varying input sizes, along with additional visualizations and\ncomprehensive dataset details.\n6\nAdditional Experimental Results\nFine-tuning or Freezing CLIP Backbone in FC-CLIP\nIn this study, we provide a comprehensive\nanalysis of the impact of fine-tuning or freezing the CLIP backbone in our framework. We specifically\nfocus on the PQseen and PQunseen metrics, which evaluate the performance for classes that overlap\nand do not overlap between the training and testing datasets, respectively. To determine whether a\nclass is seen or unseen, we adopt the prompt engineering technique described in [29], which provides\nsynonyms or subcategories of classes. Specifically, if any category name in test dataset overlaps\nwith a category name in training dataset, we consider it as a seen class; otherwise unseen. As\ndiscussed in the main paper, the proposed FC-CLIP contains three components: a class-agnostic mask\ngenerator, an in-vocabulary classifier, and an out-of-vocabulary classifier. We thus explore using\nfrozen or trainable CLIP for each component, and summarize the results in Tab. 6. To ensure a fair\ncomparison, all \"trainable\" modules utilize the same weights, resulting in identical mask proposals\nand in-vocabulary classification results. Moreover, we note that the first row in Tab. 6 with trainable\nmask generator and in-vocabulary classifier, can be considered as an approximation to OpenSeg [29]\nin our framework. Our findings reveal that an in-vocabulary classifier built upon a trainable CLIP\nbackbone achieves a higher PQseen score (37.9 compared to 32.4), but experiences a decrease in\nPQunseen (2.6 compared to 12.6) compared to a frozen out-of-vocabulary classifier. Consequently, a\nmodel that incorporates a trainable CLIP backbone for all components yields a PQ of 24.1, which\nis 2.7 lower than our final model (last row) that relies on a single frozen CLIP backbone. Using a\ntrainable mask generator and in-vocabulary classifier, along with a frozen out-of-vocabulary classifier\nboosts the performance but requires maintaining one trainable and one frozen CLIP weights, resulting\nin 2\u00d7 more backbone parameters. In summary, our observations demonstrate that building the entire\nframework upon a frozen CLIP backbone is not only effective but also efficient, providing a better\nbalance between PQseen and PQunseen metrics.\nTable 6: Effects of fine-tuning or freezing the CLIP backbone for each module in FC-CLIP.\nBuilding all three modules upon a single frozen CLIP backbone attains best performance. Note that\nour mask generator and in-vocabulary classifier use the same backbone following [20, 29, 94], and\nthus it is infeasible (denoted as N/A) for the setting in the 2nd last row. Our final setting is labeled in\ngray\nmask generator\nin-vocabulary classifier\nout-of-vocabulary classifier\nPQ\nPQseen\nPQunseen\ntrainable\ntrainable\n-\n17.7\n37.9\n2.6\ntrainable\n-\nfrozen\n21.1\n32.4\n12.6\ntrainable\ntrainable\ntrainable\n24.1\n38.9\n13.1\ntrainable\ntrainable\nfrozen\n25.4\n40.0\n14.6\ntrainable\nfrozen\nfrozen\nN/A\nN/A\nN/A\nfrozen\nfrozen\nfrozen\n26.8\n39.5\n17.3\nEvaluation with Grounding PQ and Grounding mIoU\nIt is worth emphasizing that despite the\nabsence of grounding loss [33, 97, 29, 89] during training, our model exhibits exceptional grounding\nsegmentation capabilities. Tab. 7 presents the grounding PQ and grounding mIoU scores of FC-\nCLIP, following the evaluation methodology outlined in [29]. In this evaluation, we exclusively\nemploy ground-truth classes as text query inputs to assess the effectiveness of concept grounding.\nCompared to OpenSeg [29], FC-CLIP achieves a substantial performance improvement, with notable\nenhancements of +11.6, +9.1, +13.1, and +17.7 on A-847, PC-459, A-150, and PC-59, respectively.\nEven when compared to OpenSeg trained with the Localized Narrative dataset [65], which enables\ntraining on a significantly larger vocabulary, FC-CLIP still surpasses it with improvements of +8.0,\n+2.2, +8.6 and +13.4 on A-847, PC-459, A-150 and PC-59, respectively, underscoring the grounding\nproficiency of FC-CLIP.\n16\nTable 7: Grounding segmentation performance. The proposed FC-CLIP also demonstrates state-\nof-the-art performances on grounding segmentation. MV: Mapillary Vistas\ngrounding PQ\ngrounding mIoU\nmethod\nADE20K Cityscapes MV A-847 PC-459 A-150 PC-59 PAS-21 PAS-20\nALIGN [40, 29]\n-\n-\n-\n17.8\n21.8\n25.7\n34.2\n-\n-\nALIGN w/ proposal [40, 29]\n-\n-\n-\n17.3\n19.7\n25.3\n32.0\n-\n-\nLSeg+ [48, 29]\n-\n-\n-\n10.5\n17.1\n30.8\n56.7\n-\n-\nOpenSeg [29]\n-\n-\n-\n21.8\n32.1\n41.0\n57.2\n-\n-\nOpenSeg [29] w/ L. Narr\n-\n-\n-\n25.4\n39.0\n45.5\n61.5\n-\n-\nFC-CLIP (ours)\n38.4\n48.1\n21.5 33.4\n41.2\n54.1\n74.9\n88.7\n98.5\nTable 8: Ensemble methods comparison with zero-shot evaluation (PQ) on ADE20K. Our method\nis robust to different ensemble methods (arithmetic and geometric). The results show that it is\npreferable to bias towards using the in-vocabulary classifier for seen classes and the out-of-vocabulary\nclassifier for unseen classes. Our final setting (\u03b1 = 0.4, \u03b2 = 0.8) is labeled in gray\nmethod\narithmetic\ngeometric\n(\u03b1 = 0.0, \u03b2 = 0.0)\n17.8\n17.8\n(\u03b1 = 1.0, \u03b2 = 1.0)\n21.9\n21.9\n(\u03b1 = 0.0, \u03b2 = 1.0)\n25.3\n25.3\n(\u03b1 = 1.0, \u03b2 = 0.0)\n17.5\n17.5\n(\u03b1 = 0.5, \u03b2 = 0.5)\n25.0\n25.3\n(\u03b1 = 0.5, \u03b2 = 0.6)\n25.6\n26.4\n(\u03b1 = 0.5, \u03b2 = 0.7)\n25.5\n26.7\n(\u03b1 = 0.5, \u03b2 = 0.8)\n25.4\n26.6\n(\u03b1 = 0.4, \u03b2 = 0.6)\n25.1\n25.6\n(\u03b1 = 0.4, \u03b2 = 0.7)\n25.6\n26.4\n(\u03b1 = 0.4, \u03b2 = 0.8)\n25.6\n26.8\n(\u03b1 = 0.4, \u03b2 = 0.9)\n25.4\n25.8\nTable 9: Quantitative results of ViT-based CLIP and CNN-based CLIP when input size (denoted\nas \"res\") varies for panoptic segmentation on COCO and ADE20K. All results are obtained by\napplying CLIP directly as a mask classifier with the same mask proposals from ODISE [89]\nCOCO PQ @res\nADE20K PQ @res\nCLIP backbone 224 448 672 896 1120 224 448 672 896 1120\nViT-L/14\n19.3 22.5 20.6 18.5 14.9 11.9 13.7 12.6 11.6\n9.1\nConvNeXt-L\n17.3 23.5 27.0 28.6 29.3\n9.3 12.8 14.8 16.0 15.9\nEnsemble In-Vocabulary and Out-of-Vocabulary Classifiers\nIn Tab. 8, we present experiments\nconducted to evaluate the impact of ensemble methods and ensemble parameters on the performance\nof the in-vocabulary and out-of-vocabulary classifiers. Specifically, we examine two ensemble\nmethods: arithmetic and geometric. The arithmetic method involves a linear combination of the\nin-vocabulary classifier and the out-of-vocabulary classifier, while the geometric method is defined\nas shown in Equation (7) of main paper. It is worth noting that FC-CLIP exhibits robustness to\ndifferent ensemble methods, with both methods displaying a consistent trend within the explored\nhyper-parameter ranges. However, the geometric ensemble consistently outperforms the arithmetic\nensemble by a slight margin. Additionally, we observe that preference is given to values of \u03b1 \u2264 0.5\nand \u03b2 \u2265 0.5, which biases the model towards using the in-vocabulary classifier for seen classes\nand the out-of-vocabulary classifier for unseen classes. We also explore extreme cases, including\n\u03b1 = 0.0 and \u03b2 = 0.0 (i.e., exclusively utilizing the in-vocabulary classifier for every class), \u03b1 = 1.0\nand \u03b2 = 1.0 (i.e., exclusively utilizing the out-of-vocabulary classifier for every class), \u03b1 = 0.0 and\n\u03b2 = 1.0 (i.e., using the in-vocabulary classifier for seen classes and the out-of-vocabulary classifier for\nunseen classes), and \u03b1 = 1.0 and \u03b2 = 0.0 (i.e., using the out-of-vocabulary classifier for seen classes\nand the in-vocabulary classifier for unseen classes). The results align with our observations that it\nis preferable to bias towards the in-vocabulary classifier for seen classes and the out-of-vocabulary\nclassifier for unseen classes.\n17\nTable 10: Quantitative results of ViT-based CLIP and CNN-based CLIP when input size\n(denoted as \"res\") varies for ImageNet-1k classification.\nAccuracy @res\nCLIP backbone 224 336 448 560 672 784 896\nViT-L/14\n75.3 74.3 71.3 67.5 63.1 58.5 53.9\nConvNeXt-L\n75.1 77.1 76.8 74.2 69.8 65.6 58.4\nTable 11: Open-vocabulary segmentation performance with different backbones and segmenta-\ntion frameworks. All models are trained on COCO and tested on the other datasets in a zero-shot\nmanner. MV: Mapillary Vistas. \u2217: kMaX-DeepLab with multi-scale deformable attention [103]\npanoptic datasets (PQ)\nsemantic datasets (mIoU)\nmethod\nbackbone\nADE\nCityscapes\nMV\nA-847\nPC-459\nPC-59\nPAS-21\nFC-CLIP\nR50 [35, 69]\n17.9\n40.3\n15.9\n7.1\n12.9\n50.5\n75.9\nFC-CLIP\nR101 [35, 69]\n19.1\n40.9\n16.7\n7.7\n12.3\n48.9\n77.6\nFC-CLIP\nR50\u00d74 [69]\n21.8\n42.2\n17.4\n8.7\n13.1\n54.0\n79.0\nFC-CLIP\nR50\u00d716 [69]\n22.5\n42.0\n17.8\n10.3\n15.7\n56.4\n80.7\nFC-CLIP\nR50\u00d764 [69]\n22.8\n42.7\n18.2\n10.8\n16.2\n55.7\n80.3\nFC-CLIP w/ kMaX\nConvNeXt-L [58, 38]\n24.5\n43.0\n17.0\n11.4\n15.0\n57.4\n84.7\nFC-CLIP w/ kMaX\u2217\nConvNeXt-L [58, 38]\n26.4\n40.2\n17.4\n13.6\n17.5\n57.1\n81.2\nFC-CLIP\nConvNeXt-L [58, 38]\n26.8\n44.0\n18.2\n14.8\n18.2\n58.4\n81.8\nQuantitative ViT-based CLIP vs. CNN-based CLIP when Input Size Scales\nTraining our model\nsolely with ViT-based CLIP, without any additional modifications [101, 17, 91, 21], is infeasible.\nFurthermore, applying ViT to large input sizes is computationally expensive. Therefore, to evaluate\nthe effects of using ViT- or CNN-based CLIP in our framework, we incorporate them into our out-of-\nvocabulary classifier, which is performed only during inference. To ensure a fair comparison, we use\nthe same mask proposals and disable the geometric ensemble scheme. We also perform experiment\non the ImageNet [73] benchmark to ensure a comprehensive comaprison. In Tab. 9 and Tab. 10, we\nconduct an ablation study to analyze the impact of different input resolutions for CLIP models. We\nconsider both ViT-based (ViT-L/14) and CNN-based (ConvNeXt-L) CLIP models. By employing\nthem as zero-shot classifiers and varying the input resolutions, we observe that CNN-based CLIP\ndemonstrates superior generalization ability as the input size scales up. Specifically, we observe that\nthe ViT-L/14 CLIP has a higher PQ and Accuracy at a lower resolution (i.e., input size 224), but\nsuffers from a higher resolution, which leads existing two-stage methods [90, 52, 25, 91, 89] to adopt\ndifferent input resolutions for mask generator and classifier branches. On the contrary, FC-CLIP\nprovides a simple solution by adopting a CNN-based CLIP that generalizes well to different input\nsizes.\nFC-CLIP with Different Backbones and Different Segmentation Frameworks\nThough we\nmajorly report FC-CLIP results with ConvNeXt-L [58, 69] backbone in Mask2Former [20] frame-\nwork. We note that FC-CLIP can be easily incorporated with different backbones and segmentation\nframeworks. Specifically, we experiment FC-CLIP with different backbones (e.g., ResNet [35]) and\ndifferent segmentation architecture (e.g., kMaX-DeepLab [94]). As shown in Tab. 11, FC-CLIP\ndemonstrates superior performance across different backbones and frameworks.\nVisualization\nWe provide visualization on ADE20K val set in Fig. 5.\n7\nDatasets Information and Licenses\nThe datasets we used for training and/or testing FC-CLIP are described as follows.\nCOCO:\nWe train FC-CLIP on COCO data with panoptic annotation [54]. We follow the 2017\nsplits which include 118k images for train split and 5k images for val split. If not specified, we train\nour model on the COCO train split and report results on val set of various datasets.\nLicense: Creative Commons Attribution 4.0 License\nURL: https://cocodataset.org/#home\n18\nFigure 5: Visualization examples of FC-CLIP on ADE20K val set. FC-CLIP is trained on COCO\npanoptic training set and zero-shot evaluated on ADE20K validation set.\n19\nADE20k:\nADE20k [100] covers a wide range of indoor and outdoor scenes, with 2k val images.\nWe evaluate FC-CLIP on both the version with 847 classes (A-847) and the more widely-used version\nwith 150 frequent categories (A-150).\nLicense: Creative Commons BSD-3 License\nURL: https://groups.csail.mit.edu/vision/datasets/ADE20K/\nCityscapes:\nCityscapes [22] focuses on semantic understanding of urban street scenes. We use the\nfine data includes 500 images for validation set.\nLicense: This dataset is made freely available to academic and non-academic entities for non-\ncommercial purposes such as academic research, teaching, scientific publications, or personal experi-\nmentation.\nURL: https://www.cityscapes-dataset.com/\nMapillary Vistas:\nMapillary Vistas [64] is a large-scale traffic-related dataset, including 2k images\nfor validation purposes.\nLicense: Creative Commons Attribution NonCommercial Share Alike (CC BY-NC-SA) license\nURL: https://www.mapillary.com/dataset/vistas\nPascal Context:\nPascal Context [63] covers a wide variety of indoor and outdoor scenes and\nincludes 5k val images. We evaluate FC-CLIP on both its full version (PC-459) with 459 classes and\nthe more common version (PC-59) with 59 classes.\nURL: https://www.cs.stanford.edu/~roozbeh/pascal-context/\nPascal VOC:\nPascal VOC [27] contains 1.5k val images with 20 foreground classes and 1\nbackground class. Due to the ambiguity in definition of \u201cbackground\", we assign the background\nclass to the pixels predicted as PC-59 categories that are not in Pascal VOC following [29], which\nleads to PAS-21. We also evaluate the model with background class excluded, which leads to PAS-20.\nURL: http://host.robots.ox.ac.uk/pascal/VOC/\n20\n"
  },
  {
    "title": "Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology",
    "link": "https://arxiv.org/pdf/2308.02180.pdf",
    "upvote": "9",
    "text": "Proceedings of Machine Learning Research 219:1\u201318, 2023\nMachine Learning for Healthcare\nScaling Clinical Trial Matching Using Large Language\nModels: A Case Study in Oncology\nCliff Wong\ncliff.wong@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\nSheng Zhang\nzhang.sheng@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\nYu Gu\naiden.gu@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\nChristine Moung\nchristine.moung@providence.org\nProvidence Health & Services Molecular Genomics Laboratory\nPortland, OR, USA\nJacob Abel\njacob.abel@providence.org\nProvidence Health & Services Molecular Genomics Laboratory\nPortland, OR, USA\nNaoto Usuyama\nnaotous@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\nRoshanthi Weerasinghe\nroshanthi.weerasinghe@providence.org\nClinical Research Analytics, Providence Health & Services\nPortland, OR, USA\nBrian Piening\nbrian.piening@providence.org\nEarle A. Chiles Research Institute, Providence Cancer Institute\nPortland, OR, USA\nTristan Naumann\ntristan@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\nCarlo Bifulco\ncarlo.bifulco@providence.org\nEarle A. Chiles Research Institute, Providence Cancer Institute\nPortland, OR, USA\nHoifung Poon\nhoifung@microsoft.com\nMicrosoft Research\nRedmond, WA, USA\n\u00a9 2023 C. Wong et al.\narXiv:2308.02180v3  [cs.CL]  18 Aug 2023\nScaling Clinical Trial Matching using LLMs\nAbstract\nClinical trial matching is a key process in health delivery and discovery.\nIn practice,\nit is plagued by overwhelming unstructured data and unscalable manual processing. In\nthis paper, we conduct a systematic study on scaling clinical trial matching using large\nlanguage models (LLMs), with oncology as the focus area. Our study is grounded in a\nclinical trial matching system currently in test deployment at a large U.S. health network.\nInitial findings are promising: out of box, cutting-edge LLMs, such as GPT-4, can already\nstructure elaborate eligibility criteria of clinical trials and extract complex matching logic\n(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially outperform\nprior strong baselines and may serve as a preliminary solution to help triage patient-trial\ncandidates with humans in the loop. Our study also reveals a few significant growth areas\nfor applying LLMs to end-to-end clinical trial matching, such as context limitation and\naccuracy, especially in structuring patient information from longitudinal medical records.\n1. Introduction\nClinical trial matching identifies eligible patients to be considered for clinical trial enroll-\nment, which is essential for clinical discovery and drug development. For diseases such as\ncancers where standard of care often fails, clinical trials are also a last hope and represent\nan important treatment option. The standard process for clinical trial matching, however,\nis extremely inefficient. Most information needed for accurate matching is scattered in vast\namount of unstructured text, such as elaborate eligibility criteria of a clinical trial or patient\ninformation in longitudinal electronic medical records. Curation and matching are currently\ndone predominantly by manual effort, which is difficult to scale. For example, in the United\nStates (U.S.) alone, there are two million new cancer patients every year and, at any given\nmoment, there may be hundreds of active oncology trials (Siegel and Jemal, 2022; U.S. Na-\ntional Library of Medicine, 2023). Manually evaluating all oncology trial-patient candidates\nis infeasible. Consequently, less than 3% of patients were able to participate in a trial (Unger\net al., 2019), whereas 38% of trial failure stemmed from insufficient recruitment (Stensland\net al., 2014).\nNatural language processing (NLP) has emerged as a promising direction for accel-\nerating clinical trial matching.\nPrior work explores rule-based methods and supervised\nmachine learning for structuring trial eligibility criteria (Weng et al., 2011; Kang et al.,\n2017; Yuan et al., 2019; Nye et al., 2020) or matching clinical trials against structured pa-\ntient records (Zhang et al., 2020; Gao et al., 2020). While promising, these methods still\nrequire extensive manual effort for rule development and example annotation. Recently,\nlarge language models (LLMs), such as GPT-4, have demonstrated impressive zero-shot\nand few-shot capabilities in both general domains (OpenAI, 2023; Bubeck et al., 2023)\nand health applications (Lee et al., 2023a,b; Nori et al., 2023). In this paper, we conduct\na systematic study on scaling clinical trial matching with LLMs by leveraging emergent\nin-context learning capabilities (Brown et al., 2020).\nWe choose oncology as the focus of our study because it incurs a high death burden and\nrepresents a large proportion of clinical trials (over 20% in 20221). Oncology trials also tend\n1. As of April 2023, ClinicalTrials.gov has 32,341 trials with start date 01/01/2022\u201312/31/2022; further\nfiltering for \u2018cancer\u2019 as disease yields 6,704 results. This reflects general trend with 449,665 available\nwithout start date restriction, of which 96,481 results include \u2018cancer\u2019.\n2\nScaling Clinical Trial Matching using LLMs\nto contain elaborate eligibility criteria, such as complex combination logic of disease histol-\nogy and genomic biomarkers, with the latter (genomic biomarkers) typically out of scope for\nprior state of the art (Yuan et al., 2019; Zhang et al., 2020; Gao et al., 2020). We grounded\nour study in a real-world clinical trial matching system currently in test deployment at a\nlarge U.S. health integrated delivery network (IDN), which comprises three key components:\nstructuring clinical trial eligibility criteria, structuring patient information from electronic\nmedical records, and matching (Figure 2). The patient structuring component leverages\nstate-of-the-art self-supervised deep learning (Preston et al., 2023). The trial-structuring\nand matching components are rule-based expert systems that require over 450 expert hours\nto develop.\nFor consistency and direct comparison with prior methods, we focus our study of LLMs\nprimarily on structuring trial eligibility, but we also conduct a preliminary exploration on\napplying LLMs to end-to-end clinical trial matching.\nInitial results are promising: using merely up to three examples, state-of-the-art LLMs,\nsuch as GPT-4, can already structure elaborate trial eligibility criteria and extract complex\nmatching logic of disease histology and genomic biomarkers. We conduct both intrinsic\nevaluation on trial structuring and end-to-end matching evaluation, using expert-annotated\ntest sets and legacy enrollment data, respectively2.\nWhile still far from perfect, LLMs\nsubstantially outperform prior strong baselines such as Criteria2Query (Yuan et al., 2019)\nand demonstrate competitiveness even against the oncology-specific expert system that\nrequires many expert hours to develop and tailor for this domain.\nOur study also reveals significant growth areas for applying LLMs to end-to-end clin-\nical trial matching, especially in structuring patient information from electronic medical\nrecords. A cancer patient may have hundreds of notes, with key information such as tumor\nhistology and biomarkers scattered across multiple notes (Preston et al., 2023) as shown\nin Figure 1. Naively concatenating all potentially relevant notes will almost always exceed\neven the largest context size available for GPT-4: 32K tokens.3 It might also risk over-\nwhelming the LLM with too much irrelevant information. In this paper, we thus resort to\nusing structured patient information extracted by the state-of-the-art self-supervised deep-\nlearning systems (Preston et al., 2023) and evaluate the LLM\u2019s capabilities in matching\nsuch information against the trial eligibility criteria. Preliminary results are promising and\nwe leave more in-depth exploration to future work.\nGeneralizable Insights about Machine Learning in the Context of Healthcare\n\u2022 We present the first systematic study on using LLMs to scale clinical trial matching.\nWhile we demonstrate this approach on a case study in oncology, our findings motivate\nthe exploration of this approach across other areas.\n\u2022 Our findings indicate that out of box, state-of-the-art LLMs such as GPT-4 can al-\nready handle complex eligibility criteria of clinical trials and extract matching logic.\n2. Our code is available at https://aka.ms/ctm-llm\n3. While GPT-4 has a context length of 8,192 tokens, there is limited access to GPT-4-32K which has a\ncontext length of 32,768\u2013context (about 50 pages of text): https://openai.com/research/gpt-4.\n3\nScaling Clinical Trial Matching using LLMs\nTreatment Plan\nTelephone Encounter\nSurgery\nProgress\nProcedures\nPlan of Care\nPatient Instructions\nPathology\nOp Note\nMed Onc Order\nImaging\nH&P\nEncounter\nED Provider Notes\nDischarge Summary\nConsults\n2028-01-01\n2029-01-01\n2030-01-01\nFigure 1: Patient de-identified timeline with various notes.\n\u2022 We also identify several key growth areas for enhancing LLMs in end-to-end clin-\nical trial matching, such as addressing the context limitation and accuracy issues,\nespecially in extracting and structuring patient data from electronic medical records.\n2. Related Work\nThe existing approaches for clinical trial matching can be divided into two categories de-\npending on how matching is done.\nStructure-then-Match\nSuch systems first structure trial eligibility criteria by extracting\nkey entities and relations for eligibility and then matching against structured patient infor-\nmation using manually crafted normalization rules to resolve superficial variations. Prior\nsystems typically require extensive human annotations (Weng et al., 2011; Kang et al., 2017),\nsupervised learning models (for rule extraction) (Bustos and Pertusa, 2018), or a combi-\nnation of supervised learning and rules (e.g., Criteria2Query (Yuan et al., 2019)). Due to\nlexical variations and inadequate rule coverage, these systems often suffer from lower re-\ncall and generalizability. However, they can produce structured forms for eligibility criteria\nas intermediate results, which offer better interpretability and facilitate human-in-the-loop\nverification.\nEnd-to-End\nThese systems learn to encode patient and criteria for end-to-end matching\nvia supervised learning from patient-trial or patient-criterion matching labels. For example,\nDeepEnroll (Zhang et al., 2020) jointly encodes patient records and trial eligibility criteria\nin the same embedding space, and then aligns them using attentive inference by learning\nfrom example patient-criteria matching data. COMPOSE (Gao et al., 2020) represents the\nstate-of-the-art embedding-based model that uses hierarchical memory network to encode\nconcepts at different granularity and differentiates between inclusion and exclusion criteria.\n4\nScaling Clinical Trial Matching using LLMs\nClinicalTrials.gov XML \nEligibility stored as \nsemi-structured free text\nStructuring\n[{'cohort': 'Cohort 1', \n'disease_state': \n'metastatic or \nlocoregionally \nadvanced', \n'histology_inclusion': \n'non-squamous \nNSCLC', \n'biomarker_inclusion': \n[?KRAS (g12x)?],...]\nExtracted eligibility \nas logical expression \nin disjunctive normal \nform (JSON)\npost-processing,\nnormalization to \nontologies and \nstd. nomenclature\n[{'cohort': 'Cohort 1', \n'disease_state': \n'metastatic or \nlocoregionally advanced', \n'histology_inclusion': \n'Lung Non-Squamous \nNon-Small Cell \nCarcinoma (Code \nC135017)', \n'biomarker_inclusion': \n[?KRAS G12X?],...]\n...\n<criteria>\nInclusion Criteria:\n  1. Patients 18 years and older \nwith metastatic or \nlocoregionally advanced \nepithelial cancers, that are \nconsidered incurable.\n  2. Confirmation by Tran \nLaboratory of \nneoantigen-reactive TCR(s) \nsuitable for TCR-gene \ntherapy....\n</criteria>\nScanned \nImages and \nPDF \nPathology \nReports \nProcess\nProcess\nProcess\nProcess\nProcess\nUnstructured / \nSemistructured \nText (Pathology \nReports, Imaging \nReports, Progress \nNotes, Encounter \nNotes, Surgery \nNotes)\nStructured \nData\nPatient EHR\nPubMedBERT-based\ntumor site, \nhistology, staging \nNLP models\nSite: C34.4 (Lower lobe, lung)\nHistology: 8140 (adenocarcinoma, NOS)\nPathologic Stage T: TX \nPathologic Stage N: N3\nPathologic Stage M: M1 \ntumor progression: 2022-06-24\nage, gender, biomarkers, \nmedications, lab tests\nage: 67.2\ngender: female\nbiomarkers: ['TP53 E285*', 'TMB High', \n'KRAS G12C'] \nmedications: pembrolizumab 2021-08-01 \nto 2022-02-10\nbilirubin: 1.5 ULN\n....\nbiomarkers: PD-L1 TPS 50%\nECOG: 1 \nmedications: gemcitabine \n2022-03-01 to 2022-06-13\nAzure Form \nRecognizer OCR\nClinical Trial Structuring\nnormalization\nPatient Structuring\ninformation \nextraction \npipeline\nHierarchical \nMatching and \nRanking\nStructured Patient Data\n(LLM)\nFigure 2: Overall schematic of matching.\nYuan et al. (2023) follows COMPOSE but uses LLMs to generate semantically equivalent\ncriteria for data augmentation.\nTo the best of our knowledge, we are the first to explore using the emergent in-context\nlearning capability of LLMs (Brown et al., 2020) for clinical trial matching. Out of box and\nwith no more than three examples, cutting-edge LLMs, such as GPT-4, can already struc-\nture trial eligibility criteria with reasonable performance, outperforming strong baselines\nfrom prior systems such as Criteria2Query. We also show preliminary results in applying\nLLMs to end-to-end clinical trial matching.\n3. Methods\nIn this section, we introduce the problem formulation for clinical trial matching and then\ndescribe the relevant systems and evaluation.\n3.1. Problem Formulation\nGiven a clinical trial T and a patient P, clinical trial matching determines if P satisfies all\neligibility criteria specified by T. The specification Tspec generally comprises semi-structured\ntext (see Figure 3 \u201cMatch logic\u201d). Similarly, the patient information from electronic medical\nrecords (EMRs) PEMR also contains many free-text clinical documents, such as pathology\n5\nScaling Clinical Trial Matching using LLMs\nInclusion Criteria:\n-Histologically or cytologically confirmed high-grade neuroendocrine tumor that has progressed on first line therapy, excluding small cell lung cancer\n(SCLC). High grade includes any neuroendocrine neoplasm with a Ki-67 of >=20% or with mitotic count of more than 20 mitoses per high power field\nor any poorly differentiated neoplasm or any neoplasm lacking these that is deemed high grade by pathology consensus, based on other markers\n(necrosis or IHC demonstrating p53 or RB mutation).\nExample clinical trial with eligibility criteria highlighted\nAND\nAND\nOR\nNOT\nOR\nOR\nhigh-grade\nneuroendocrine tumor\nprogressed on first line therapy\nSCLC\nKi-67 >= 20%\nmiotic count > 20\npoorly differentiated\nnecrosis\np53 mutation\nRB mutation\nMatch logic for the highlighted inclusion criterion\nFigure 3: Example clinical trial (NCT04412629) with eligibility criteria highlighted and the\nmatch logic for the inclusion criterion.\nreports, radiology reports, and encounter notes. To determine the match, there are two\napproaches:\n\u2022 Structure-then-match: first structure both trial specification and patient informa-\ntion into common ontologies by extracting the relevant information, then execute the\nmatching logic;\n6\nScaling Clinical Trial Matching using LLMs\n\u2022 End-to-end: learn to encode the trial and patient information directly and determine\nmatch by computing the similarity of embeddings.\nDeepEnroll (Zhang et al., 2020) and COMPOSE (Gao et al., 2020) are representative\nexamples of the end-to-end approach. Learning an accurate encoding is particularly chal-\nlenging for patient, given the vast amount of potentially relevant medical records. Conse-\nquently, prior work focuses on a more limited setting where the patient records are already\nstructured. Still, such methods require large amount of training data of known patient-trial\nmatches (or patient-criteria matches) and their generalizability to unseen disease areas is\nunderexplored. It is also hard for human experts to interpret or verify such end-to-end\nresults.\nIn this paper, we instead focus our study on \u201cstructure-then-match\u201d. Let Tstructured\ndenote the structured representation of the trial eligibility criteria and Pstructured denote\nthat of a patient. Then clinical trial matching reduces to three subtasks:\n\u2022 Structuring trial eligibility criteria: Tspec \u2192 Tstructured;\n\u2022 Structuring patient records: PEMR \u2192 Pstructured;\n\u2022 Matching: Match(Tstructured, Pstructured) \u2192 {yes, no};\nMatching is relatively straightforward, assuming that structuring has been done well,\nso we will focus on the structuring tasks. At the time of this study, it is not yet possible to\napplying cutting-edge LLMs, such as GPT-4, to protected health information (PHI) data\nin the environment available. Therefore, in the remainder of the paper, we focus on the\nstructuring task for clinical trial eligibility criteria (Tspec \u2192 Tstructured).\n3.2. Structuring Clinical Trial Eligibility Criteria\nEligibility criteria comprise inclusion and exclusion criteria. There is an implicit AND logic\nover components in the inclusion criteria, and an implicit NOT AND logic over components\nin the exclusion criteria. Additionally, there may be complex nested logic on combinations\nof patient attributes.\nFor oncology, a salient example is the interplay between disease histology (fine-grained\nsubtypes of disease states) and genomic biomarkers (e.g., genetic point mutation).\nSee\nFigure 3 for an example criterion.\nBy contrast, standard demographic attributes (e.g.,\nage, gender) are straightforward. We omit them for simplicity and focus our evaluation on\ndisease histology and biomarkers.\n3.3. Systems\nOur main focus of the paper is to explore applying cutting-edge LLMs, such as GPT-3.5 and\nGPT-4, to the structuring tasks. For head-to-head comparison, we also consider a strong\nbaseline using state-of-the-art biomedical entity extraction systems, prior state of the art\nCriteria2Query (Yuan et al., 2019), and an expert system in test deployment at a large\nhealth network.\n7\nScaling Clinical Trial Matching using LLMs\nLLM\nWe use GPT-3.5 (Ouyang et al., 2022) and GPT-4 (OpenAI, 2023) through Azure\nOpenAI Service,4 which allows us to interact with the models efficiently and in HIPAA-\ncompliant way. Through the API, we submit prompts and receive generated responses,\nwhich we postprocess to the requisite structured format.\nBiomedical Entity Extraction\nIn the absence of a general model such as LLMs, we\ncan combine state-of-the-art biomedical entity extraction systems to assemble a strong\nbaseline. Specifically, we consider SciSpaCy (Neumann et al., 2020) for extracting disease\nstates, GNormPlus (Wei et al., 2015) for extracting gene entities, and tmVar (Li et al.,\n2013) for extracting genetic variants.\nCriteria2Query\nYuan et al. (2019) provide a standard baseline for structuring eligibility\ncriteria. Criteria2Query combines supervised learning and rule-based heuristics and prior\nwork shows state-of-the-art results in extracting disease states. We found that it can also\nextract biomarker information, although the capabilities are limited.\nExpert System\nSome authors of this paper have previously developed an expert system\nfor clinical trial matching, in collaboration with a large health network, where this system\nis currently in test deployment. This system comprises three key components: structuring\ntrial eligibility criteria, matching against structured patient information, and a clinical\ntrial matching application with a human-in-the-loop assisted clinical trial triaging user\ninterface (UI). The structuring component encapsulates extensive heuristics for extracting\nand normalizing biomedical entities such as disease histology, genomic biomarkers, etc., as\nwell as heuristics for processing semi-structured information (e.g., sections) and extracting\nmatching directives (AND/OR/NOT). In internal evaluation, this system has demonstrated\nstate-of-the-art performance and has been well received in test deployment by molecular\ntumor boards and trial coordinators. However, as common for rule-based approaches, this\nsystem has taken over 450 expert hours to develop and is specifically tailored for oncology\n(e.g., by focusing its structuring on disease histology and biomarkers). Exploring a more\nscalable approach thus represents an attractive direction for expanding to more fine-grained\ndetails and generalizing to other disease areas.\n3.4. Evaluation\nIntrinsic Evaluation\nFor structuring trial eligibility criteria, we consider two settings. In\nthe basic setting, we evaluate on entity extraction only, focusing on histology and biomark-\ners. Specifically, we compare system output against the gold entity set and report precision\n(i.e., positive predictive value), recall (i.e., sensitivity), and F1 (harmonic mean of precision\nand recall). This is similar to the approach taken by prior work. In the more advanced\nsetting, we evaluate on extraction of the complete matching logic. Essentially, we regard\nstructuring as a semantic parsing problem, where the structuring output is a logic form\nas in Figure 3 (see \u201cMatch logic\u201d). To facilitate evaluation, we normalize the logic form\nto disjunctive normal form (DNF) (i.e., OR of ANDs) and report precision/recall/F1 of the\ndisjunctions. See Figure 4 for an example.\n4. https://azure.microsoft.com/en-us/products/cognitive-services/openai-service/\n8\nScaling Clinical Trial Matching using LLMs\n\u2026\nInclusion Criteria:\n- Histologically or cytologically-documented, advanced solid tumor of one of the following types:\n\u2026\n\u2022\nSalivary Gland Carcinoma (sarcomas and mesenchymal tumors are excluded)\n\u2022\nAny advanced solid tumor, with the exception of colorectal carcinoma (CRC), which is Microsatellite Instability (MSI)-High (MSI-H) OR\n\u2026\n\u2022\nAny advanced solid tumor that has failed at least one line of therapy and is TMB-H (\u226510 mut/Mb, F1CDx assay), excluding dMMR/MSI-H tumors.\nNote: For participants to be eligible for enrollment they must have failed at least one line of standard of care systemic therapy (ie, not treatment na\u00efve), with the \nexception of CRC participants who must have failed at least 2 lines of standard of care systemic therapy, as per CRC speci\ufb01c eligibility criteria. Participants must not \nhave melanoma or NSCLC.\n\u2026\nadvanced AND Salivary Gland Carcinoma                                                                    AND NOT sarcomas  AND NOT mesenchymal tumors\nadvanced AND Solid Tumor                       AND Microsatellite Instability (MSI)-High   AND NOT melanoma AND NOT NSCLC AND NOT CRC\nadvanced AND Solid Tumor                       AND TMB-H (\u226510 mut/Mb, F1CDx assay) AND NOT melanoma AND NOT NSCLC                          AND NOT dMMR/MSI-H\nDisease State\nOR\nHistology Inclusion\nBiomarker Inclusion\nHistology Exclusion\nBiomarker Exclusion\n...\n...\nFigure 4: Example of disjunctive normal form (DNF). In DNF formulas must be a disjunc-\ntion (OR) of conjunction (AND).\nEnd-to-End Evaluation\nGiven the structured trial information, we can simulate an\nend-to-end evaluation by triangulating it with structured patient information via a common\nmatching component to determine end-to-end match results. We can then compare these\nresults against a gold match dataset.\n4. Experiments\n4.1. Datasets\nClinical Trial\nWe downloaded all the clinical trials in XML format from ClinicalTri-\nals.gov (U.S. National Library of Medicine, 2023). In this work, we focus on oncology trials\nwith \u201cTreatment\u201d as the primary purpose and \u201cInterventional\u201d as the study type. We use a\nlist of oncology-related keywords to filter out non-oncology trials. For each trial, we extract\nthe following XML elements: brief title, official title, brief summary, arm group,\nand criteria. We truncate the criteria element to the first 40 lines and apply heuris-\ntics to remove lines that are not related to oncology-specific criteria (disease histology and\nbiomarkers).\nStructured Eligibility Criteria\nTo create the gold dataset for evaluating the structured\ncriteria, three molecular pathologists at our collaborating health network manually struc-\ntured 53 clinical trials as the test set. These 53 clinical trials were randomly selected from a\nlarger set of trials that had significant enrollment at the health network and were more likely\nto contain complex criteria. The output organizes the criteria into five categories: disease\nstate, histology inclusion, biomarker inclusion, histology exclusion, and biomarker exclu-\nsion. The criteria logical expression is in disjunctive normal form (DNF), which consists of\na disjunction of conjunctive clauses (OR of ANDs). See Figure 4. Given the extracted histol-\n9\nScaling Clinical Trial Matching using LLMs\nogy terms and biomarkers, we further normalize them into the NCI Thesaurus ontology5\nand HGVS nomenclature6.\nStructured Patient Information\nTo facilitate simulated end-to-end matching evalua-\ntion, we leverage the component for structuring patient information in the expert system\nat our collaborating health network (see Section 3.3). Briefly, this component system uses\nthe Read OCR model in Azure Form Recognizer7 to convert any scanned images and PDF\ndocuments into free text, and then appends such text to digitized medical records.\nIt\nthen applies a series of state-of-the-art biomedical NLP models to extract relevant patient\nattributes from the medical records. For example, it uses self-supervised PubMedBERT\nmodels (Preston et al., 2023) to extract the tumor site, histology, and staging informa-\ntion, as well as additional information extraction modules to extract other attributes such\nas health status, PD-L1 IHC result, and medications. Information available in structured\nEMRs is added directly, such as date of birth and gender. Other structured data fields\nsuch as lab tests and medications are normalized to the same units and NCI Thesaurus,\nrespectively.\nHistorical Trial Enrollment Data\nGiven a known patient enrollment into a trial, we\ncan treat it as a gold label and test if a clinical trial matching system can correctly flag it as\na match. In this way, we can estimate the recall for end-to-end matching. For this purpose,\nwe use a dataset containing 523 patient-trial enrollment pairs at our collaborating health\nsystem. This is a subset of all historical enrollment data, after filtering out confidential\ntrials and patients who do not have sufficient structured information.\nClinical Trial Matching System Feedback Data\nAs mentioned in Section 3.3, a\nclinical trial matching application is in test deployment at our collaborating health network\n(see Figure 5), which allows molecular pathologists to inspect patient-trial match candidates\nand select the most suitable trials for follow-up. We used molecular pathologists\u2019 selection\nas implicit feedback and consider selected candidates as positive and non-selected ones as\nnegative. If no trials were selected for a patient, we skipped all candidates for this patient,\nas the molecular pathologists may not have inspected them yet. This yielded a dataset with\n68,485 candidates with 84% positive.\nHuman Subjects, IRB, Data Security and Patient Privacy\nThis work was per-\nformed under the auspices of an institutional review board (IRB)-approved research protocol\n(Providence protocol ID 2019000204) and was conducted in compliance with Human Sub-\njects research, clinical data management procedures, as well as cloud information security\npolicies and controls. All study data involving PHI were integrated, managed and analyzed\nexclusively and solely within our collaborating health network. All study personnel com-\npleted and were credentialed in training modules covering Human Subjects research, use of\nclinical data in research, and appropriate use of IT resources and IRB-approved data assets.\n5. https://ncithesaurus.nci.nih.gov/ncitbrowser/\n6. https://varnomen.hgvs.org/\n7. https://azure.microsoft.com/en-us/products/form-recognizer/\n10\nScaling Clinical Trial Matching using LLMs\nFigure 5: Example user interface for clinical trial matching. NOTE: The data used in this\nexample are synthetic and do not contain PHI.\n4.2. System Details: Applying LLMs to Structuring Eligibility Criteria\nTo transform the trial XML into a structured representation, we employ a prompt template\nthat guides GPT-4 (same for GPT-3.5) to extract and structure relevant criteria from each\ntrial. Specifically, we focus on four types of criteria: trial cohort, disease state, tumor his-\ntology, and biomarkers. The prompt directs GPT-4 to output the structured representation\nin JSON format and provides instructions on how to handle unclear or missing information.\nIn addition, the prompt may include few-shot example (input, output) pairs for in-context\nlearning (Brown et al., 2020). Our prompt template can be found in Figures 6 to 9 in Ap-\npendix. For inference, we replace the placeholder {input trial} in the prompt template\nwith the input trial text and prompt GPT-4 to output the structured representation in\nJSON format. As shown in Figure 9, the output organizes the criteria into five categories:\ndisease state, histology inclusion, biomarker inclusion, histology exclusion, and biomarker\nexclusion. The criteria logical expression is in disjunctive normal form (DNF). Our prompt\ninstructs GPT-4 to assign a cohort name for each clause if possible.\n4.3. Results: Structuring Trial Eligibility Criteria\nTable 1 shows the test results on structuring oncology trial eligibility criteria in the basic\nsetting, where we evaluate on inclusion and exclusion criteria entity extraction only and\nignore complex match logic. We can\u2019t get GPT-3.5 to work in the 3-shot setting as adding\nthe examples will render the prompt to exceed the context limit. Standard biomedical en-\ntity extraction tools such as GNormPlus (Wei et al., 2015), tmVar (Li et al., 2013), and\nSciSpaCy (Neumann et al., 2020) did not perform very well for one or both of histology\nand biomarkers. Criteria2Query (Yuan et al., 2019) also delivered subpar performance on\noncology trial entities, revealing limitation in its generalizability. Remarkably, out of box,\n11\nScaling Clinical Trial Matching using LLMs\nTable 1: Comparison of test results on structuring oncology trial eligibility criteria. Gold\nlabels provided by three molecular pathologists at the collaborating health net-\nwork. Evaluation on inclusion and exclusion criteria entity extraction only, with\ncomplex match logic ignored. GPT-3.5 can\u2019t do 3-shot here due to limited context\nsize.\nHistology\nBiomarker\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nGNormPlus\n-\n-\n-\n6.8\n19.6\n10.2\nSciSpaCy\n34.2\n70.2\n46.0\n58.3\n6.9\n12.3\nCriteria2Query\n29.6\n40.2\n32.8\n68.3\n27.5\n39.2\nGPT-3.5 (zero-shot)\n35.1\n31.6\n34.2\n61.2\n29.4\n39.7\nGPT-4 (zero-shot)\n62.1\n69.0\n65.4\n75.3\n59.8\n66.7\nGPT-4 (3-shot)\n57.8\n73.7\n64.8\n72.5\n72.5\n72.5\nTable 2: Comparison of test results on structuring oncology trial eligibility criteria, evalu-\nated on complete match logic in DNF form.\nHistology\nBiomarker\nHistology+Biomarker\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nPrecision\nRecall\nF1\nSciSpaCy\n9.2\n16.8\n11.8\n36.4\n5.3\n9.3\n7.4\n10.8\n8.8\nCriteria2Query\n23.2\n34.8\n27.8\n22.0\n12.0\n15.5\n9.6\n17.4\n12.4\nGPT-3.5 (zero-shot)\n36.0\n20.6\n26.2\n48.3\n18.7\n26.9\n30.7\n14.6\n19.7\nGPT-4 (zero-shot)\n43.0\n41.3\n42.1\n31.0\n17.3\n22.2\n27.8\n21.1\n24.0\nGPT-4 (3-shot)\n42.7\n54.8\n48.0\n39.4\n37.3\n38.4\n27.3\n32.4\n29.6\nGPT-4 delivers strong performance in extracting both histology and biomarker information,\ndespite requiring no extensive customization for this domain and task, outperforming Cri-\nteria2Query by over 30 absolute points in F1. In-context learning helped substantially for\nbiomarker extraction, but didn\u2019t matter much for histology. GPT-4 also clearly outperforms\nGPT-3.5, which performs on par with Criteria2Query in the zero-shot setting.\nTable 2 shows the test results on structuring oncology trial eligibility criteria in the full\nsetting, where we evaluate on the complete match logic in DNF form. Note that this is a very\nstringent evaluation as each conjunction (ANDs) needs to contain the exactly combination of\natomic attributes to be considered correct. When there are complex nested logical forms,\nthe number of conjunctions may become very large. Still, this evaluation offers a way to\nobjectively compare system\u2019s capability in extracting complex match logic. Here, GPT-4\nsimilarly dominates all other systems. Moreover, GPT-4 (3-shot) outperforms GPT-4 (zero-\nshot) in all scenarios, indicating that in this more challenging setting, in-context learning\nindeed plays a positive role.\n4.4. Results: End-to-End Clinical Trial Matching\nGiven structured trial eligibility criteria and structured patient information, matching can\nbe done by evaluating for each criterion component, where the patient information is com-\npatible (thus eligible). This check is more elaborate than it may appear at first, since a\ncriterion may specify a set of values (e.g., EGFR mutations without specifying which ones)\nor a more abstract concept than that in the patient information. Therefore, matching relies\n12\nScaling Clinical Trial Matching using LLMs\nTable 3: Comparison of recall results on historical patient-trial enrollment data. The expert\nsystem takes extensive expert manual effort to develop and has been designed to\nfavor recall and ignore exclusion criteria, so it is expected to have high recall. The\npublicly available Criteria2Query instance runs slowly and we can only get 157\npairs evaluated in time (out of 466 total).\nRecall\nSciSpaCy\n50.0\nCriteria2Query*\n37.0\nGPT-3.5 (zero-shot)\n23.4\nGPT-4 (zero-shot)\n64.2\nGPT-4 (3-shot)\n76.8\nExpert System\n93.6\nTable 4: Comparison of test results using feedback data from the clinical trial matching\napplication in test deployment at our collaborating health network. The expert\nsystem is expected to perform well here, given that the users mainly evaluated its\nproposed candidates.\nPrecision\nRecall\nF1\nGPT-3.5 (zero-shot)\n88.5\n11.6\n20.6\nGPT-4 (zero-shot)\n86.7\n46.8\n60.8\nGPT-4 (3-shot)\n87.6\n67.3\n76.1\nExpert System\n84.3\n97.4\n90.4\non a hierarchical representation of medical terms to check if a criterion entity subsumes\nthe corresponding patient attribute. For histology matching, we convert both clinical trial\ncriteria entities and patient tumor site/histology into OncoTree codes (Kundra et al., 2021),\nwhich are a standard hierarchical representation for cancer types.\nFor biomarker matching, we use a hierarchy that captures biomarkers at different levels\nof specificity: amino acid level, exon level, chromosomal level, gene level, or pathway level.\nA patient\u2019s tumor sequencing results typically specify a specific amino acid or DNA coding\nvariant, whereas a clinical trial may specify a biomarker criterion at any of these levels.\nMatching needs to account for this hierarchy and the implied subsumption relations.\nTable 3 shows the recall results for the historical patient-trial enrollment dataset. As\nexpected, the expert system performs very well, given that it has been developed with\nextensive manual effort for the oncology domain.\nRemarkably, out of box, GPT-4 can\nalready performs competitively, recovering 76.8% of gold patient-trial pairs.\nTable 4 shows the test results using the feedback data from the test deployment of the\nexpert system. Again, the expert system is expected to perform well here. E.g., the recall\nis expected to be close to 100%, given that the users mainly evaluated candidates proposed\nby the expert system and only occasionally added a trial they found by themselves. Out\nof box, GPT-4 already attains higher precision than the expert system. As observed in the\n13\nScaling Clinical Trial Matching using LLMs\nhistorical enrollment evaluation, a major growth area lies in boosting the recall but GPT-4\nalready performs quite competitively overall.\nWe also explored applying GPT-4 directly to conduct end-to-end matching. Due to\nthe context limit, we can\u2019t fit the entire medical record for most patients into the prompt.\nTherefore, it is only feasible to consider direct matching against patient information that\nhas already been structured, and currently it is only feasible for the zero-shot setting. Due\nto current restriction on applying Azure OpenAI services to PHI, we only test this using\na de-id example adapted from the public TCGA dataset.\nThe results can be found in\nFigures 10 to 12. Remarkably, GPT-4 provides a cogent narrative of its matching decision,\nsupported by detailed analysis of individual criteria. It is difficult to draw a generalizable\nconclusion from such an anecdotal example, but in combination with other experimental\nresults, it certainly illustrates the potential in harnessing the general cognitive capabilities\nof large language models to scale clinical trial matching.\n5. Discussion\nIn this work, we present the first systematic study on using large language models (LLMs) to\nscale clinical trial matching, using oncology as a case study to ground our exploration. Our\nfindings foremost suggest that out-of-the-box, LLMs such as GPT-4 can already handle\ncomplex eligibility criteria of clinical trials and extract complex matching logic. In this\nregard, LLM provide a strong, practical baseline. And its general nature bodes well for the\npotential to apply to other disease areas.\nOur study also reveals several key growth areas for enhancing LLMs in end-to-end\nclinical trial matching, such as addressing context length limitation and accuracy issues,\nespecially in extracting and structuring patient data from EMRs. In future work, we plan\nto implement more sophisticated prompt engineering techniques and LLM fine-tuning.\nWhile there are implicit limitations in the use of logical expressions, our system enables\na triaging pipeline that can reduce the clinical trial candidates to a small number. This\nenables human-in-the-loop participation wherein a human can manually go through the\nsmall number of trials to verify all the criteria are met. Human-in-the-loop may be preferable\ngiven the safety-critical nature of healthcare. Nevertheless, we experimented with further\naccelerating this process by providing GPT-4 structured patient information, and asking\nGPT-4 to output all the matching and non-matching conditions. This may not be efficient\nto do for all possible patient-trial pairs but we can reserve this as a more expensive but\nhigher-quality reranking step on a small number of candidates produced by a more efficient\nbut less powerful system.\nLimitations\nClinical trial matching by structuring all criteria into a logical form (i.e.\n\u201cstructure-then-match\u201d) carries implicit limitations. In particular, it is not always possible\nto map an extracted criteria into an existing concept or ontology; and indeed, we found quite\na few examples in our study where this was not possible. There are also various subtleties\nin the criteria language that are difficult to capture completely into a logical formula. Due\nto current restriction on applying GPT-4 to identifiable patient records, we were not able\nto explore LLMs in structuring patient information.\n14\nScaling Clinical Trial Matching using LLMs\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin,\neditors, Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901.\nCurran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper_files/\npaper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\nEce Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid\nPalangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence:\nEarly experiments with GPT-4, 2023.\nAurelia Bustos and Antonio Pertusa. Learning eligibility in cancer clinical trials using deep\nneural networks. Applied Sciences, 8(7):1206, 2018.\nJunyi Gao, Cao Xiao, Lucas M. Glass, and Jimeng Sun. Compose: Cross-modal pseudo-\nsiamese network for patient trial matching. In Proceedings of the 26th ACM SIGKDD In-\nternational Conference on Knowledge Discovery & Data Mining, KDD \u201920, page 803\u2013812,\nNew York, NY, USA, 2020. Association for Computing Machinery. ISBN 9781450379984.\ndoi: 10.1145/3394486.3403123. URL https://doi.org/10.1145/3394486.3403123.\nTian Kang, Shaodian Zhang, Youlan Tang, Gregory W Hruby, Alexander Rusanov, No\u00b4emie\nElhadad, and Chunhua Weng. EliIE: An open-source information extraction system for\nclinical trial eligibility criteria. Journal of the American Medical Informatics Association,\n24(6):1062\u20131071, 04 2017. ISSN 1067-5027. doi: 10.1093/jamia/ocx019. URL https:\n//doi.org/10.1093/jamia/ocx019.\nRitika Kundra, Hongxin Zhang, Robert Sheridan, Sahussapont Joseph Sirintrapun, Av-\nery Wang, Angelica Ochoa, Manda Wilson, Benjamin Gross, Yichao Sun, Ramyasree\nMadupuri, Baby A. Satravada, Dalicia Reales, Efsevia Vakiani, Hikmat A. Al-Ahmadie,\nAhmet Dogan, Maria Arcila, Ahmet Zehir, Steven Maron, Michael F. Berger, Cristina\nViaplana, Katherine Janeway, Matthew Ducar, Lynette Sholl, Snjezana Dogan, Philippe\nBedard, Lea F. Surrey, Iker Huerga Sanchez, Aijaz Syed, Anoop Balakrishnan Rema,\nDebyani Chakravarty, Sarah Suehnholz, Moriah Nissan, Gopakumar V. Iyer, Rajmo-\nhan Murali, Nancy Bouvier, Robert A. Soslow, David Hyman, Anas Younes, Andrew\nIntlekofer, James J. Harding, Richard D. Carvajal, Paul J. Sabbatini, Ghassan K. Abou-\nAlfa, Luc Morris, Yelena Y. Janjigian, Meighan M. Gallagher, Tara A. Soumerai, Ingo K.\nMellinghoff, Abraham A. Hakimi, Matthew Fury, Jason T. Huse, Aditya Bagrodia, Meera\nHameed, Stacy Thomas, Stuart Gardos, Ethan Cerami, Tali Mazor, Priti Kumari, Pichai\nRaman, Priyanka Shivdasani, Suzanne MacFarland, Scott Newman, Angela Waanders,\nJianjiong Gao, David Solit, and Nikolaus Schultz.\nOncotree: A cancer classification\n15\nScaling Clinical Trial Matching using LLMs\nsystem for precision oncology.\nJCO Clinical Cancer Informatics, (5):221\u2013230, 2021.\ndoi: 10.1200/CCI.20.00108.\nURL https://doi.org/10.1200/CCI.20.00108.\nPMID:\n33625877.\nPeter Lee, Sebastien Bubeck, and Joseph Petro.\nBenefits, limits, and risks of GPT-4\nas an ai chatbot for medicine. New England Journal of Medicine, 388(13):1233\u20131239,\n2023a. doi: 10.1056/NEJMsr2214184. URL https://doi.org/10.1056/NEJMsr2214184.\nPMID: 36988602.\nPeter Lee, Carey Goldberg, and Isaac Kohane. The AI Revolution in Medicine: GPT-4\nand Beyond. Pearson Education, 2023b. ISBN 9780138200138. URL https://books.\ngoogle.com/books?id=AY-5zwEACAAJ.\nMiao Li, Dongqing Zhang, Zhihao Yang, Weiwei Li, Xiaobo Liu, and Jian Wang.\ntm-\nvar: a text mining approach for extracting sequence variants in biomedical literature.\nBioinformatics, 29(11):1433\u20131439, 2013. doi: 10.1093/bioinformatics/btt162.\nMark Neumann, Daniel King, Iz Beltagy, and Waleed Ammar. scispacy: Fast and robust\nmodels for biomedical natural language processing.\nBioinformatics, 36(4):1235\u20131241,\n2020. doi: 10.1093/bioinformatics/btz682.\nHarsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz.\nCapabilities of GPT-4 on medical challenge problems, 2023.\nBenjamin E. Nye, Ani Nenkova, Iain J. Marshall, and Byron C. Wallace. Trialstreamer:\nMapping and browsing medical evidence in real-time. 2020.\nOpenAI. GPT-4 technical report, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob\nHilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul\nChristiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions\nwith human feedback, 2022.\nSam Preston, Mu Wei, Rajesh Rao, Robert Tinn, Naoto Usuyama, Michael Lucas, Yu Gu,\nRoshanthi Weerasinghe, Soohee Lee, Brian Piening, Paul Tittel, Naveen Valluri, Tris-\ntan Naumann, Carlo Bifulco, and Hoifung Poon. Toward structuring real-world data:\nDeep learning for extracting oncology information from clinical text with patient-level\nsupervision. Patterns, 4(4), 2023/04/14 2023. doi: 10.1016/j.patter.2023.100726. URL\nhttps://doi.org/10.1016/j.patter.2023.100726.\nRebecca L. Siegel and Ahmedin Jemal. Cancer statistics, 2022. CA: A Cancer Journal for\nClinicians, 72(1):7\u201333, Jan 2022. doi: 10.3322/caac.21708. URL https://doi.org/10.\n3322/caac.21708.\nKristian D. Stensland, Russell B. McBride, Asma Latif, Juan Wisnivesky, Ryan Hendricks,\nNitin Roper, Paolo Boffetta, Simon J. Hall, William K. Oh, and Matthew D. Galsky.\nAdult Cancer Clinical Trials That Fail to Complete: An Epidemic?\nJNCI: Journal of\n16\nScaling Clinical Trial Matching using LLMs\nthe National Cancer Institute, 106(9):dju229, 09 2014. ISSN 0027-8874. doi: 10.1093/\njnci/dju229. URL https://doi.org/10.1093/jnci/dju229.\nJoseph M Unger, Riha Vaidya, Dawn L Hershman, Lori M Minasian, and Mark E Fleury.\nSystematic review and meta-analysis of the magnitude of structural, clinical, and physi-\ncian and patient barriers to cancer clinical trial participation. J. Natl. Cancer Inst., 111\n(3):245\u2013255, March 2019.\nU.S. National Library of Medicine. ClinicalTrials.gov. Accessed April, 2023, 2023. URL\nhttps://clinicaltrials.gov/.\nChih-Hsuan Wei, Chun-Nan Hsu, Wen-Lian Hsu, Yung-Chuan Liu, and Hong-Jie Dai.\nGnormplus: An integrative approach for tagging genes, gene families, and protein do-\nmains. BioMed Research International, 2015:918710, 2015. doi: 10.1155/2015/918710.\nChunhua Weng, Xiaoying Wu, Zhihui Luo, Mary Regina Boland, Dimitri Theodoratos,\nand Stephen B Johnson. EliXR: an approach to eligibility criteria extraction and repre-\nsentation. Journal of the American Medical Informatics Association, 18(Supplement 1):\ni116\u2013i124, 07 2011. ISSN 1067-5027. doi: 10.1136/amiajnl-2011-000321. URL https:\n//doi.org/10.1136/amiajnl-2011-000321.\nChi Yuan, Patrick B Ryan, Casey Ta, Yixuan Guo, Ziran Li, Jill Hardin, Rupa Maka-\ndia, Peng Jin, Ning Shang, Tian Kang, and Chunhua Weng. Criteria2query: a natural\nlanguage interface to clinical databases for cohort definition.\nJournal of the Ameri-\ncan Medical Informatics Association, 26(4):294\u2013305, 02 2019.\nISSN 1527-974X.\ndoi:\n10.1093/jamia/ocy178. URL https://doi.org/10.1093/jamia/ocy178.\nJiayi Yuan, Ruixiang Tang, Xiaoqian Jiang, and Xia Hu. Llm for patient-trial matching:\nPrivacy-aware data augmentation towards better performance and generalizability, 2023.\nXingyao Zhang, Cao Xiao, Lucas M. Glass, and Jimeng Sun.\nDeepenroll: Patient-trial\nmatching with deep embedding and entailment prediction. In Proceedings of The Web\nConference 2020, WWW \u201920, page 1029\u20131037, New York, NY, USA, 2020. Association\nfor Computing Machinery. ISBN 9781450370233. doi: 10.1145/3366423.3380181. URL\nhttps://doi.org/10.1145/3366423.3380181.\n17\nScaling Clinical Trial Matching using LLMs\nAppendix A. Prompt Templates\nPrompt Template for Clinical Trial Structuring\n(System Message) I am an intelligent and thorough agent designed is to extract\nthe given clinical trial criteria input into a valid JSON logic structure. My response\nshould be written in the language of JSON and should accurately capture the meaning\nof the input.\nPlease note that my response should accurately reflect the logical\nconnectives of the criteria (and, or, not). I will try my best attempt to normalize the\nlogic formula into disjunctive normal form. In disjunctive normal form, each clause\nconnected by a conjunction, or \u2019and\u2019, must be either a literal or contain a conjunction,\nor \u2019and\u2019 operator. Please keep in mind that my response should be flexible enough\nto allow for various relevant and creative solutions. I should also focus on providing\nan accurate and well-structured solution that can be easily understood by others.\nI should be sure to specify solid tumor and tumor staging when applicable. I only\nextract the most important tumor biomarkers, tumor histology or disease, and tumor\nstaging information.\nI ignore criteria regarding life expectancy, written consent,\npregnancy, contraception and adequate bone marrow, liver, and renal function. I\nignore the medication treatment and dosages for trial arms. I only extract about\nthe inclusion and exclusion criteria for the trial. I must be thorough and include all\ncriteria related to biomarkers, histologies, disease state, and staging.\n\u2014\n(Instructions) I will use the following output format:\n[{\u201ccohort\u201d: (String) Cohort Name, \u201cdisease state\u201d:(String) Disease State Criteria,\n\u201chistology inclusion\u201d: (String) Histology Inclusion Criteria, \u201cbiomarker inclusion\u201d:\n(List) Biomarker Inclusion Criteria, \u201chistology exclusion\u201d: (List) Histology Exclu-\nsion Criteria, \u201cbiomarker exclusion\u201d: (List) Biomarker Exclusion Criteria}, ...]\nThis is a representation of the logical disjunctive normal form where each conjunctive\n(AND) clause is represented as a JSON and the outer list is a disjunction (logical\nOR) of those clauses or JSONs. I will try to separate the biomarkers and histologies.\nBelow are descriptions of those keys. Only the following keys are allowed. DO NOT\ninclude any other criteria keys:\ncohort: a string that represents the name of the cohort for these criteria\ndisease state: a string that describes the disease state (e.g, staging (stage I, II, IIIA,\nIV), refractory, relapsed, advanced, metastatic, etc.)\nhistology inclusion: a string that represents the histology criteria that if satisfied,\ncontributes to the expression evaluating to True. There should only be one histology\nper JSON item.\nFigure 6: GPT-4 prompt template for clinical trial structuring.\n18\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Clinical Trial Structuring (cont\u2019d)\nbiomarker inclusion: a list of biomarker inclusion criteria that if all elemnts of the\nlist are satisfied, contributes to the expression evaluating to True.\nhistology exclusion: a list of histology exclusion criteria if any elements of the list\nare satisfied, contributes to the expression evaluating to False.\nbiomarker exclusion: a list of biomarker exclusion criteria that if any elements of\nthe list are satisfied, contributes to the expression evaluating to False.\nTo match one of the clauses or JSON items in the list, it means all field val-\nues in each item are satisfied (\u201dhistology inclusion\u201d and \u201dbiomarker inclusion\u201d\nand not any(\u201dhistology exclusion\u201d) and not any(\u201dbiomarker exclusion\u201d)).\nEach\nclause/JSON should have at least a value for histology inclusion. If all the values\nfor a JSON are empty or null, do you include in the list.\nI do not care about\ncriteria that do not have at least a value histology inclusion.\nIf a field doesn\u2019t\nhave any value, set the value to \u201d\u201d or []. Any of the JSON in the list matching\nmeans a patient is eligible for the trial.\nFor criteria specific to a cohort, add a\nnew item to the list.\nSkip biomarker or histology criteria that are inside if-then\nconditions unless the if condition is a cohort type. Skip prior treatment type criteria.\nIf the extracted criteria only contains an abbreviation, I will find the long form\nof that abbreviation from the rest of the document and include that in the field value.\nI\nrecognize\nthe\nfollowing\nas\nbiomarkers\nand\nwill\ninclude\nthem\nin\nthe\nbiomarker inclusion or biomarker exclusion fields if they are mentioned as cri-\nteria as defined above: gene mutations, gene expressions, TMB (tumor molecular\nburden, TMB-H or TMB-L), MSI (microsatellite instability, MSI-H, MSI-L, MSS,\nmicrosatellite stable) status, gene pathway alterations such as dMMR (deficient\nMismatch Repair Pathway) or pMMR (proficient Mismatch Repair), PD-L1 expres-\nsion levels determined by Combined Positive Score (CPS) or tumor proportion score\n(TPS), positive or negative status for breast cancer biomarkers such as HR, ER, PR\nand HER2.\nWhen a criteria specifies a histology and all the subtypes of the histology, I will\ninclude all those subtypes as additional clauses (JSON) in the list of disjunctions.\nWhen a trial exclusion includes primary brain (Central Nervous System, CNS)\ntumor (not just brain metastasis), also include brain / CNS tumor mention in the\nhistology exclusion list.\nDo not include criteria about prior therapies or prior treatment. They are not con-\nsidered biomarkers criteria and should not be included in the inclusion or exclusion\ncriteria. Do not include any expression prior therapies, treatments, or therapies.\nFigure 7: GPT-4 prompt template for clinical trial structuring (cont\u2019d).\n19\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Clinical Trial Structuring (cont\u2019d)\nIf the inclusion criteria is for either of multiple biomarkers (and/or), list each of\nthose biomarkers in a separate clause or JSON item because either of them being\nsatisfied should contribute to the entire expression being True. I want to have the\nleast restrictive accurate matching criteria output. Only list multiple biomarkers in\nbiomarker inclusion list for one clause JSON item if the trial absolutely require all\nof those biomarkers.\nDo not include mentioned biomarkers if the presence or absense of the biomarker\ndoes not affect eligibility. And do not include biomarkers in the output that if the\nbiomarker is present, then additional criteria is needed. I only extract if the criteria\nitself determines eligibility.\nWhen there is a list of biomarker criteria in the same sentence and it\u2019s not clear\nwhether the biomarkers have an AND or OR relationship, assume it\u2019s an OR criteria\nrelationship between the biomarkers, which means assign each biomarker to a new\nclause.\nIf the biomarkers are on separate main bullet points in criteria section,\nassume those are AND criteria relationship and all the biomarkers should be in the\nsame clause.\nDo not include cohort or arms that closed.\nFor multiple exclusion biomarker or histology criteria, those should be present in\nevery clause JSON item.\nDO NOT repeat the same JSON multiple times. Be succinct.\n\u2014\n(Demonstrations)\nInput:\n<brief_title >A Phase\n1/2\nStudy of DCC -3116 in\nPatients\nWith\nMAPK\nPathway\nMutant\nSolid\nTumors </brief_title >\n<official_title >A Phase 1/2, First -in -Human\nStudy of DCC -3116 as\nMonotherapy\nand in\nCombination\nWith\nRAS/MAPK\nPathway\nInhibitors\nin\nPatients\nWith\nAdvanced\nor\nMetastatic\nSolid\nTumors\nWith\nRAS/MAPK\nPathway\nMutations </\nofficial_title >\n<brief_summary >This is a Phase 1/2, multicenter , open label , first in human (\nFIH) study of DCC -3116 as monotherapy , and in\ncombination\nwith\ntrametinib ,\nbinimetinib , or\nsotorasib\nin\npatients\nwith\nadvanced\nor\nmetastatic\nsolid\ntumors\nwith\nRAS/MAPK\npathway\nmutation. The\nstudy\nconsists\nof 2 parts , a\ndose -escalation\nphase , and an\nexpansion\nphase.</ brief_summary >\n<condition >Pancreatic\nDuctal\nAdenocarcinoma </condition >\n<condition >Non -Small\nCell\nLung\nCancer </condition >\n<condition >Colorectal\nCancer </condition >\n<condition >Advanced\nSolid\nTumor </condition >\n<condition >Metastatic\nSolid\nTumor </condition >\nFigure 8: GPT-4 prompt template for clinical trial structuring (cont\u2019d).\n20\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Clinical Trial Structuring (cont\u2019d)\n<arm_group >\n<arm_group_label >Dose\nEscalation (Part 1, Cohort A Monotherapy )</\narm_group_label >\n<arm_group_type >Experimental </ arm_group_type >\n<description >DCC -3116\ntablets\nin\nescalating\ndose\ncohorts\ngiven\norally\ntwice\ndaily (BID) in 28-day\ncycles as\nmonotherapy (single\nagent). If no DLT in 3\nparticipants\nor 1 DLT/6\nparticipants\nis observed , dose\nescalation\nmay\ncontinue\nto the\nnext\nplanned\ndose\ncohort.</ description >\n</arm_group >\n(...\nmore arm groups are omitted for space saving)\n<criteria >\nInclusion\nCriteria:\n1. Male or\nfemale\nparticipants\n>=18 years of age\n2. Dose\nEscalation\nPhase (Part 1):\n1.\nParticipants\nmust\nhave a pathologically\nconfirmed\ndiagnosis\nof an\nadvanced\nor\nmetastatic\nsolid\ntumor\nwith a documented RAS , NF1 , or RAF\nmutations. A molecular\npathology\nreport\ndocumenting\nmutational\nstatus\nof RAS , NF1 , or RAF\nmust be\navailable.\n2.\nProgressed\ndespite\nstandard\ntherapies , and\nreceived\nat least 1 prior\nline of\nanticancer\ntherapy.\n-\nParticipants\nwith a documented\nmutation\nin BRAF\nV600E or V600K\nmust\nhave\nreceived\napproved\ntreatments\nknown to\nprovide\nclinical\nbenefit\nprior to study\nentry.\n(...\nmore criteria are omitted for space saving)\n</criteria>\nEligibility Criteria Output:\n[\n{\"cohort\": \"Dose\nEscalation\nPhase (Part 1)\", disease_state \": \"advanced\nor\nmetastatic\", \" histology_inclusion \": \"Solid\nTumor\",\n\"\nbiomarker_inclusion \": [\"RAS\nmutation\"], \" histology_exclusion \": [], \"\nbiomarker_exclusion \": []},\n{\"cohort\": \"Dose\nEscalation\nPhase (Part 1)\", \" disease_state \": \"advanced\nor\nmetastatic\", \" histology_inclusion \": \"Solid\nTumor\",\n\"\nbiomarker_inclusion \": [\"NF1\nmutation\"], \" histology_exclusion \": [], \"\nbiomarker_exclusion \": []}, {\"cohort\": \"Dose\nEscalation\nPhase (Part 1)\"\n, \" disease_state \": \"advanced\nor\nmetastatic\", \" histology_inclusion \": \"\nSolid\nTumor\",\n\" biomarker_inclusion \": [\"RAF\nmutation\"], \"\nhistology_exclusion \": [], \" biomarker_exclusion \": []},\n(...\nmore cohorts are omitted for space saving)\n]\n(User Message) Input:\n{input trial}\nEligibility Criteria Output:\nFigure 9: GPT-4 prompt template for clinical trial structuring (cont\u2019d).\n21\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Direct Trial-Patient Matching\n(System Message) As a helpful agent, my task is to analyze both the structured\ndata and free-text pathology report of a patient and determine if they meet the\nclinical trial criteria text for eligibility. Please provide a clear and concise response\nthat lists all the reasons why I believe the patient is eligible or not eligible.\nMy\nanalysis should take into account various factors such as age, medical history,\ncurrent medications, histology, staging, biomarkers and any other relevant informa-\ntion provided in the report and structured data. I should also be able to handle\ncomplex scenarios with multiple variables and provide accurate reasoning behind my\ndetermination. My response should be flexible enough to allow for various relevant\nand creative reasons why the patient may or may not be eligible.\nMy goal is to\nprovide an informative and detailed analysis that assists healthcare professionals in\nmaking informed decisions regarding patient eligibility.\nClinical Trial Study Design Detail:\n<brief_title >Study of\nZotiraciclib\nfor\nRecurrent\nHigh -Grade\nGliomas\nWith\nIsocitrate\nDehydrogenase 1 or 2 (IDH1 or IDH2)\nMutations </brief_title >\n...\n<criteria >\n- INCLUSION\nCRITERIA:\n- Participants\nmust\nhave\ndiffuse\nglioma , WHO\ngrades 2-4, histologically\nconfirmed\nby\nLaboratory\nof Pathology , NCI\n- IDH1 or IDH2\nmutation\nstatus\nconfirmed\nby TSO500\nperformed\nin LP , NCI\n- Participants\nmust\nhave\nreceived\nprior\ntreatment (e.g., radiation ,\nconventional\nchemotherapy ) prior to\ndisease\nprogression\n- Participants\nmust\nhave\nrecurrent\ndisease , proven\nhistologically\nor by\nimaging\nstudies\n- Participants\nwho\nhave\nundergone\nprior\nsurgical\nresection\nare\neligible\nfor\nenrollment\nto\ncohorts\n1-4.\n- Age &gt ;=18\nyears\n- Karnofsky &gt ;=70%\n- Participants\nmust\nhave\nrecovered\nfrom\nthe\nadverse\neffects\nof prior\ntherapy\nto grade 2 or less\nEXCLUSION\nCRITERIA:\nMore\nthan\none\nprior\ndisease\nrelapse (WHO\ngrade 3-4) or more\nthan\ntwo\nprior\ndisease\nrelapses (WHO\ngrade 2)\n- Prior\ntherapy\nwith:\n- bevacizumab\nfor\ntumor\ntreatment. Note:\nparticipants\nwho\nreceived\nbevacizumab\nfor\nsymptom\nmanagement ,\nincluding\nbut not\nlimited\nto\ncerebral\nedema , or pseudo\nprogression\ncan be\nenrolled\n- Prolonged\nQTc &gt ;470 ms as\ncalculated\nby\nFridericia s correction\nformula\non\nscreening\nelectrocardiogram (ECG)\n...\n</criteria >\n\u2014\nFigure 10: Directly asking GPT-4 to match clinical trial criteria and patient record. The\npatient record is de-ID clinical data from public TCGA dataset.\n22\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Direct Trial-Patient Matching (cont\u2019d)\nPatient Metadata:\nMale ,\n29 years old ,\nGlioma (Anaplastic\nAstrocytoma ),\nLIVING\n(63\nmonths),\nRecurred/Progressed\n(53\nmonths)\nSamples: 1\nAttribute\nValue\n....\nInternational\nClassification\nof\nDiseases\nfor\nOncology , Third\nEdition ICD -O-3\nHistology\nCode\n9401/3\nInternational\nClassification\nof\nDiseases\nfor\nOncology , Third\nEdition ICD -O-3\nSite\nCode\nC71.0\nKarnofsky\nPerformance\nScore 90\n...\nNeoplasm\nHistologic\nGrade G3\nNeoplasm\nHistologic\nType\nName\nAstrocytoma\n...\nPerformance\nStatus 0\nPerformance\nStatus\nAssessment\nTimepoint\nCategory Pre -Adjuvant\nTherapy\n...\nRelated\nsymptom\nfirst\npresent\nSeizures\nSeizure\nHistory\nYES\nSex\nMale\n...\nPatient Mutations:\nMissense\nIDH1\nR132H\nPathology Metadata:\nOncotree\nCode\nAASTR\n...\nPatient Pathology Report:\nSPECIMEN(S) SUBMITTED/ PROCEDURES\nORDERED:\nCLINICAL\nHISTORY: male , here\nfor\nleft\nside\ncraniotomy\nfor\ntumor.\nGROSS\nDESCRIPTION : Received\nthe\nfollowing\nspecimens\nin the\nDepartment\nof\nPathology , labeled\nwith\nthe\npatient \u2019s name\nand\nA. Brain\ntumor\nB. Brain\ntumor\n...\nDIAGNOSIS:\nA. \"Brain\ntumor \":\nAnaplastic\nastrocytoma (WHO\nGrade\nIII) (see\ncomment)\nB. \"Brain\ntumor \":\nAnaplastic\nastrocytoma (WHO\nGrade\nIII)\n...\n\u2014\nFigure 11: Directly asking GPT-4 to match clinical trial criteria and patient record. The\npatient record is de-ID clinical data from public TCGA dataset (cont\u2019d).\n23\nScaling Clinical Trial Matching using LLMs\nPrompt Template for Direct Trial-Patient Matching (cont\u2019d)\nBased on the provided information, the patient appears to be eligible for the clinical\ntrial. Here are the reasons for eligibility:\n1. Age: The patient is 29 years old, which meets the age requirement of being 18\nyears or older.\n2. Diagnosis: The patient has a diagnosis of Anaplastic Astrocytoma, which is a\nhigh-grade glioma (WHO Grade III). The clinical trial is for recurrent high-grade\ngliomas with IDH1 or IDH2 mutations.\n3. IDH1 Mutation: The patient has an IDH1 R132H mutation, which confirms the\npresence of an IDH1 mutation as required by the clinical trial.\n4. Recurrent Disease: The patient\u2019s disease has recurred/progressed after 53 months,\nwhich meets the requirement of having recurrent disease.\n5. Karnofsky Performance Score: The patient has a Karnofsky Performance Score of\n90, which is greater than or equal to the required 70%.\n6. Prior Treatment: The patient has received prior treatment (radiation, conventional\nchemotherapy) before disease progression, as required by the clinical trial.\n7. Recovery from Adverse Effects: Although not explicitly mentioned, there is no\nindication that the patient has not recovered from the adverse effects of prior therapy\nto grade 2 or less.\nHowever, there are some limitations in the provided information:\n1. IDH2 Mutation: The IDH2 mutation status is not provided. However, since the\npatient has an IDH1 mutation, this is not a concern for eligibility.\n2. Prior Therapy Exclusions: The patient\u2019s prior therapy with bevacizumab is not\nmentioned. However, there is no indication that the patient has received bevacizumab\nfor tumor treatment, which would exclude them from the trial.\n3. QTc Interval: The patient\u2019s QTc interval is not provided. However, there is no\nindication that the patient has a prolonged QTc >470ms, which would exclude them\nfrom the trial.\n4. Other Exclusion Criteria: There is no information provided about prior invasive\nmalignancies, history of allergic reactions to compounds similar to zotiraciclib, or\nuncontrolled intercurrent illness or social situations. However, there is no indication\nthat the patient meets any of these exclusion criteria.\nIn conclusion, based on the available information, the patient appears to be eligible\nfor the clinical trial.\nFigure 12: Directly asking GPT-4 to match clinical trial criteria and patient record. The\npatient record is de-ID clinical data from public TCGA dataset (cont\u2019d).\n24\n"
  },
  {
    "title": "Getting the Ball Rolling: Learning a Dexterous Policy for a Biomimetic Tendon-Driven Hand with Rolling Contact Joints",
    "link": "https://arxiv.org/pdf/2308.02453.pdf",
    "upvote": "8",
    "text": "Getting the Ball Rolling: Learning a Dexterous Policy\nfor a Biomimetic Tendon-Driven Hand with Rolling Contact Joints\nYasunori Toshimitsu1,2, Benedek Forrai1, Barnabas Gavin Cangan1, Ulrich Steger1, Manuel Knecht1,\nStefan Weirich1, Robert K. Katzschmann1\nAbstract\u2014 Biomimetic, dexterous robotic hands have the\npotential to replicate much of the tasks that a human can\ndo, and to achieve status as a general manipulation platform.\nRecent advances in reinforcement learning (RL) frameworks\nhave achieved remarkable performance in quadrupedal locomo-\ntion and dexterous manipulation tasks. Combined with GPU-\nbased highly parallelized simulations capable of simulating\nthousands of robots in parallel, RL-based controllers have\nbecome more scalable and approachable. However, in order to\nbring RL-trained policies to the real world, we require training\nframeworks that output policies that can work with physical\nactuators and sensors as well as a hardware platform that can\nbe manufactured with accessible materials yet is robust enough\nto run interactive policies. This work introduces the biomimetic\ntendon-driven Faive Hand and its system architecture, which\nuses tendon-driven rolling contact joints to achieve a 3D\nprintable, robust high-DoF hand design. We model each element\nof the hand and integrate it into a GPU simulation environment\nto train a policy with RL, and achieve zero-shot transfer of a\ndexterous in-hand sphere rotation skill to the physical robot\nhand.1\nI. INTRODUCTION\nA. Motivation\nAs robotic structures get more complex and biomimetic,\nwe are starting to apply policies trained through reinforce-\nment learning instead of conventional model-based control\nmethods in which the controller explicitly reasons with\nthe dynamic model of the robot. This is especially the\ncase for dexterous manipulation tasks, which apply an an-\nthropomorphic robotic hand to movements that require the\ncoordination of multiple fingers. Achieving this coordinated\nmotion has the potential to replace many repetitive tasks such\nas pick-and-place in warehouses, assembly in factory lines,\nor assistance in our daily lives.\nIn this work, we introduce the Faive Hand, a platform for\ndexterous manipulation tasks. We report our current progress\non integrating its model into an RL environment, and apply a\nclosed-loop controller on the real robot to achieve dexterous\nin-hand sphere rotation as first step towards humanlike\nmanipulation.\nB. State of the art in dexterous manipulation\na) Learning-based control:\nDexterous manipulation\ntasks benefit from learning-based control approaches as\n1ETH\nZurich\n{ytoshimitsu, bforrai, bcangan,\nulsteger, knechtma, sweirich, rkk}@ethz.ch\n2Max Planck ETH Center for Learning Systems\n1https://srl-ethz.github.io/get-ball-rolling/\nvideo: https://youtu.be/YahsMhqNU8o\nb\n0s\n2s\n4s\n0s\n2s\n4s\na\nx\ny\nz\nx\ny\nz\nFig. 1.\n(a) The GPU-based parallelized simulation environment simulating\n4096 robot hands in parallel to train a RL policy. (b) The trained policy\nbeing deployed on the tendon-driven robot hand with rolling contact joints.\nmodel-based approaches struggle with the number of dif-\nferent contact states that the controller needs to handle.\nAs contact can occur anywhere along the link chain, the\nnumber of contact states can be several orders of magnitudes\ngreater than the number of states for other tasks such\nas locomotion. The first major breakthrough in applying\nlearning-based methods to dexterous manipulation on real\nrobots was achieved by OpenAI for in-hand cube rotation\non the Shadow Hand [1]. However, the combination of a\nCPU-based simulator with a sample-inefficient RL algorithm\nrequired up to 50 hours of computation on 384 machines\nwith 16 CPU cores each to run in parallel to collect on-\npolicy experience, which limited the scalability to different\ntasks and the accessibility of replicating the same setup\nat other institutions. Since then, the introduction of GPU-\nbased simulators like IsaacGym [2], which can simulate\nthousands of robots in parallel, have greatly brought down\nthe required resource for training RL agents. Some tasks such\nas locomotion can even be trained on the order of minutes\non a single GPU [3]. Though the tasks are not directly\ncomparable, all of the dexterous manipulation policies in\nthis paper were also learned in about one hour on a single\nNVIDIA A10G GPU.\nThere has been much recent progress on applying manip-\nulation policies learned in a parallel simulation environment\nsuch as IsaacGym onto physical robotic hands. Handa et al.\nproposed a vision-based policy together with a vectorized\nimplementation of automatic domain randomization (ADR)\nwhich enabled the cube rotation task to be run on the Allegro\nhand using only RGB cameras for exteroceptive input [4].\nChen et al. created a policy that can reorient multiple objects\nin simulation [5]. Yin et al. then proposed a pipeline to use\narXiv:2308.02453v3  [cs.RO]  22 Jan 2024\njust proprioceptive touch sensor inputs to rotate objects in-\nhand around a desired axes [6]. Allshire et al. used IsaacGym\nto achieve dexterous manipulation on the TriFinger robot\nsystem [7].\nb) Robotic hand hardware: It is important to note\nthat capable robots require a combination of hardware and\ncontroller, and we will take a look at the major robotic hands\nbeing used for dexterous manipulation research. Most of the\nprevious works use either the Allegro hand [8], a larger-\nthan-life four-finger hand that contains servo motors in each\njoint, or the Shadow hand [9], which is an anthropomorphic\nfive-finger tendon-driven robotic hand. Further, there have\nalso been other robotic hands created at research institutions.\nThe LEAP Hand has a similar design to the Allegro hand\nwith an improved joint layout and robustness [10]. The\nTriFinger robot used in IsaacGym-based RL tasks [7] and\noffline RL competitions [11] is a non-anthropomorphic three-\nfinger manipulator which uses BLDC motors [12].\nWe argue that for achieving manipulation in human envi-\nronments, it is more advantageous to be closer to the human\nform: tools and objects in our environment are originally\ndesigned to be used by humans, so a human-like hand design\nis more suited for interacting with them. Also, when learning\nfrom human demonstrations, manipulation tasks can be more\neasily transferred to a robot with a similar structure. Robotic\nhands with servo motors driving each axis, like the Allegro\nhand, are simple to construct (consequently lowering the\ncost), but their fingers become substantially thicker than\nhuman ones. The Shadow Hand is by far the most human-\nlike hand, but it comes with a steep price tag of 110k GBP as\nquoted from their website, limiting accessibility to conduct\nphysical experiments. The high purchase and maintenance\ncosts may discourage frequent sim2real experiments of RL\ntrained policies, since they can initially behave erratically on\nthe real robot and require intense trial-and-error until they\ncan work fully in reality.\nC. Approach\nAt the Soft Robotics Lab, we have developed the Faive\nHand, a biomimetic dexterous tendon-driven robotic platform\nfor exploring dexterous manipulation. The current version of\nthe hand uses 3D printed components and servo motors for\naccessible and simple manufacturing. However, in addition\nto the challenges inherent to controlling a high-DoF robotic\nhand for manipulation, this hand has features that do not exist\nin other dexterous hands trained with RL, such as rolling\ncontact joints that rotate without a fixed axis of rotation.\nConventional rotational encoders are difficult to use on this\ndesign, so the hand currently does not have internal joint\nangle encoders, which are being developed for a later version\nof the hand. Because of this limitation, the joint angles\nmust be estimated from the tendon length, which can be\ncalculated from the servo motor angles. These features were\nimplemented in the simulation framework and on the low-\nlevel controller, which enabled running a closed-loop RL\ntrained policy on the physical robot. We have chosen a task\nsimilar to Shi et al. [13], where the robot dexterously rotates\na sphere in the target direction.\nD. Contributions\n\u2022 Integrate a model for rolling contact joints into the\nIsaacGym simulator;\n\u2022 Apply a closed-loop policy trained in simulation to\ncontrol the tendon-driven biomimetic Faive Hand; and\n\u2022 Introduce the prototype version of the Faive Hand,\ndesigned as an accessible platform for dexterous ma-\nnipulation.\nII. THE TENDON-DRIVEN DEXTEROUS HAND WITH\nROBUST ROLLING CONTACT JOINTS\nIn this section, we introduce the biomimetic joint structure\nof the Faive Hand 2 and how it was modelled and controlled\nso that RL policies can be run on the real robot. The\nFaive Hand was developed in our lab to research biomimetic\nmanipulation, with the aim to eventually provide a low-cost\nplatform that makes dexterous manipulation research on real\nhardware accessible to many research institutes, accelerating\nthe application of anthropomorphic robotic hands to real-life\napplications.\nA. Rolling contact joint design of the hand\nHere, we describe the prototype version of the Faive Hand\nused for experiments in this paper, with the designation Proto\n0. It contains 11 actuatable degrees of freedom, with 3 in\nthe thumb and 2 for each of the other fingers. Each finger\ncontains a coupled joint at the distal end, and thus there\nare 16 joints in total. Similar to a human finger, our robotic\nfinger design consists of three joints for which we apply a\njoint naming convention derived from human anatomy, as\nshown in fig. 2. For each finger, the distal interphalangeal\n(DIP) joint is linked to the proximal interphalangeal (PIP)\njoint using a coupling tendon so that they bend together.\nTherefore, the DIP and PIP joint are driven by one flexor\ntendon and a separate extensor tendon, that drive both\njoints simultanously. The metacarpophalangeal (MCP) joint\nis actuated antagonistically by a single motor, to which both\nthe flexor and extensor tendons are attached.\nThere have been many joint designs that have been used\nfor biomimetic articulated hands, such as pin joints [9], [14],\nmachined springs [15], [16], soft mechanisms [17], [18], and\nrolling contact joints [19], [20].\nApart from the carpometacarpal joint of the thumb which\nis recreated using 2 hinge joints, all of the joints are\nimplemented as rolling contact joints. These rolling joints\nare composed of two articulating bodies with adjacent curved\ncontact surfaces connected by a pair of crosswise ligament\nstrings as shown in fig. 2 (like the children\u2019s toy Jacob\u2019s\nladder). They have advantages such as impact compliance,\nlow friction or greater range of motion, and have been\nproposed for robotics, implants, and prosthetics [21], [22].\nThere have also been extensions to rolling contact joints\n2https://www.faive-robotics.com/ for inquiries on using the\nhand in your projects, please use the contact form in the website.\ndistal inter-\nphalangeal \n(DIP) joint\nproximal \ninter-\nphalangeal \n(PIP) joint\nDIP/PIP \ncoupling\nPIP \nflexor\nextensor\nMCP \nflexor\nMCP \nextensor\nmetacarpo-\nphalangeal \n(MCP) joint\nrolling contact \nsurface\na\ncrosswise \nligaments\nb\ntendons\nopposable \nthumb\njoint\nrolling \ncontact \njoint\nliga-\nments\nsilicone \npadding\ntendons\nFig. 2.\n(a) The Faive Hand has a rolling contact joint design with tendons\nand ligaments mimicking that of a human finger. (b) Overview of each\ncomponent of the Faive Hand.\nFig. 3.\n(a) The Faive Hand can grasp a payload of up to 10 kg,\ndemonstrated here with a dumbbell. (b) The motion of the rolling contact\njoint in the MuJoCo simulator and on the real robot hand, which do not\nrotate around a fixed axis.\nproposed with fluid lubricated joints encased in artificial skin\n[20], or with variable stiffness [19].\nOur finger design builds on these previous works while\nfurther simplifying the design, making it more robust, com-\npact and easier to manufacture. We demonstrated payloads\nof up to 10 kg using a downwards facing five-fingered power\ngrasp, at a total weight of the hand system of only 1.1 kg,\nas shown in fig. 3.\nB. Modelling rolling contact joints in simulation\nAs seen in fig. 3, these rolling contact joints do not\nhave a single axis of rotation. Thus they were modelled\nin simulation with two \u201dvirtual\u201d hinge joints. The axes of\nthese hinge joints were placed to go through the axis of\nthe cylinder that constitutes each rolling contact surface.\nThey were constrained to rotate together in the MJCF\n(the modeling format for MuJoCo) file by linking them\ntogether with tendon/fixed elements, which apply con-\nstraints to the linear combination of joint angles. The tendon\nmodel information is carried over when the MJCF file is\nloaded in IsaacGym, and can be enabled by setting the\nlimit stiffness and damping values to each of the\ntendon properties.\nC. Low-level controller to enable joint control and sensing\nSimilar to how the Shadow Hand was modelled in Ope-\nnAI\u2019s work [1], the hand was simulated as a joint-driven\nrobot, ignoring the tendons-level information. Instead, the\nlow-level controller of the robot ran a program that converts\njoint commands and measurements to and from their tendon\ncounterparts.\na) Converting joint angle commands to motor-level\ncommands: The tendons are controlled by 16 Dynamixel\nXC330-T288-T servo motors. 6 of them have two tendons\nattached antagonistically. Having more than one tendon on\none motor will reduce the number of servos needed, but can\nonly be used when the motion of the two tendons always\nhave a constantly scaled relation to each other, expressed by\nthe ratio of their spool radius to each other. This is true for\nthe proximal joints of the finger3. However, due to the routing\nof the tendons, the distal joints\u2019 tendon lengths depend on\nthe proximal joints\u2019 angle and they do not have this constant\nrelationship. Thus, the distal joints require a dedicated motor\nfor a single tendon.\nBy geometrically modeling the rolling motion of the joints\nand the tendon path based on CAD data, we can analytically\ndescribe the function l = f(q) mapping the joint angles\nq to the tendon lengths l. With this function, the desired\njoint angle \u00afq can be converted to the desired tendon length\n\u00afl = f(\u00afq) and subsequently to the desired servo motor angles\n(by dividing it with the tendon spool radius), and can be sent\nto the Dynamixel motors.\nb) Joint angle sensing with an extended Kalman filter:\nThe function f(\u00b7) maps the set of all joint angles to a\nmanifold in the space expressing the tendon length. This is\nnot a bijection, and this mapping cannot be easily inverted:\nthere are combinations of tendon lengths that do not map\nback to a configuration in joint space. Therefore, we adopt\nthe method used by Ookubo et al., which uses an extended\nKalman filter (EKF) for estimating the joint angles from the\ntendon length measurements [23]. Here, we denote the state\nand observation as x and z respectively, formulated as a\nconcatenation of the position and velocity as follows.\nx :=\n\u0014q\n\u02d9q\n\u0015\n\u2208 R22\nz :=\n\u0014l\n\u02d9l\n\u0015\n\u2208 R32\n(1)\nThe transition model can be described as\nxi+1 =\n\u0014I\nIdt\nO\nI\n\u0015\nxi + w\n(2)\nwhere the subscript i denotes the time step, I is the unit\nmatrix, O is the zero matrix, dt is the time step, and w is\nthe noise in the state transition.\nBy using a symbolic computation library such as sympy\n[24], the partial derivative of the function f(\u00b7) mapping joint\nangle to tendon length can be symbolically derived, giving us\nthe muscle Jacobian Jm = \u2202f(q)/\u2202q. Then, the nonlinear\n3In theory, due to a varying moment arm, the anatagonistic relation\nslightly varies, but in practice this variation is small enough to be negligible.\nrun on physical robot\ntrain RL policy\nactor observations \ncritic MLP\naction\nPPO\nreward\nstep 1\nEKF\ntendon\nlengths \naction\nparallelized\nsimulation\nactor MLP\ncritic observations \nvalue\nstep 2\njoint angle\nestimate \njoint-tendon\nmapping\ntendon\ncommands\nphysical robot\nFig. 4.\nOverview of the RL training framework for achieving dexterous\nmanipulation on the tendon-driven robot hand with rolling contact joints.\nAfter training the policy within a simulation environment, the actor network\nis transferred to the real robot.\nobservation model used for the EKF can be described as\nzi = h(xi) + v\nh(xi) :=\n\u0014 f(qi)\nJm(qi) \u02d9qi\n\u0015\n(3)\nA calibration procedure to set the relation between motor\nposition (spool angle) and tendon length is run every time the\nrobot boots. We fit a jig that constrains the robot hand joints\nto a known pose and lightly pull on the tendons until they are\ntaut. The program uses the motor position at that moment as\nthe basis from which the tendon lengths are calculated. The\nestimated joint angle from the EKF \u02c6q can then be used as the\nproprioceptive measurement to the RL policy as described in\nthe subsequent section.\nIII. REINFORCEMENT LEARNING TRAINING FOR\nDEXTEROUS MANIPULATION\nThe overview of the pipeline for training the policy and\nrunning it on the real robot is shown in fig. 4. The policy\nis trained with RL with advantage actor-critic (A2C) using\nasymmetric observations (where different sets of observa-\ntions are given to the actor and critic). We use the PPO\nalgorithm [25] with the implementation from the open-source\nrepository rl games [26]. We use MLP networks as the actor\nand critic.\nAfter the training is finished, the MLP of the actor was\nexported to an ONNX format, a cross-compatible ML model\nformat, and run on the robot. The joint-tendon mapping and\nthe EKF introduced in section II-C were used to enable joint-\nlevel sensing and control of the Faive Hand.\nA. Rewards\nTable I lists the rewards used for the task and their formula.\nThe reward specific to the sphere rotation task is the object\nrotation reward, which returns the maximum reward when\nthe rotational velocity in the y axis is between \u22133 and\n\u22131 rad s\u22121, and linearly decreases outside of this region. The\nsign of \u03c9y in the reward formula can be flipped to reverse\nthe desired direction of rotation.\nWe noticed that the object\u2019s angular velocity measurement\nfrom IsaacGym contained considerable noise, which was\nhypothesized to be due to the collision calculation applying\nimpulse forces whenever a part of the hand contacts the\nobject. Computing the object rotation reward from this\nangular velocity measurement tended to produce policies that\nexploit the physics of the simulator, resulting in motions that\ndo not actually rotate the sphere, even within the simulator.\nThus we have used the numerical angular velocity for the\nobject rotation reward, which was computed by numerically\ndifferentiating the change in object orientation between time\nsteps.\nB. Observation space\nSince the actor and critic are implemented as two separate\nMLPs, they can be given different sets of observations. As\nthe critic\u2019s MLP is only needed during training, it can use\nprivileged information as input, which are data that can be\nobtained within the simulation but not from the real robot.\nTable II lists the observations used for the actor and critic.\nThe joint positions were normalized to range between \u22121 and\n1 using the robot\u2019 joint range. Since the measurements from\nthe real robot contain some noise, especially for velocity\nmeasurements, we have opted to use the past five steps of\nthe joint position measurements, in which the joint velocity\nis implicitly expressed.\nOur target task is to rotate a sphere, which is symmetric\nin all axes. Due to this symmetry, the orientation of the\nobject does not affect how the hand interacts with the object.\nFurther, we could also remove the position measurement\nof the object from the actor observations without adversely\naffecting performance. Thus, the actor only uses proprio-\nceptive joint data, simplifying the sim2real process and the\ntechnical challenge of obtaining accurate and low-latency\nmeasurements of the object pose.\nC. Action space\nThe action a expresses the relative change in the joint\nangle command. It is first clipped to between \u22121 and 1,\nthen it increments the desired joint angle \u00afq as\n\u00afq \u2190 clip(\u00afq + vmax\u2206ta, qmin, qmax)\n(4)\nwhere \u2206t is the timestep and vmax is a constant scalar that\ncaps the maximum speed of the joints from the policy. vmax\nwas set to 5 rad s\u22121. After the action updates the desired\njoint angle, it is clipped between the minimum qmin and\nmaximum qmax joint angles of the robot hand.\nD. Domain randomization\nTo compensate for the inaccuracy of the physics engine\nand to make the policy more robust to overcome the sim2real\ngap, domain randomization was applied to the physics prop-\nerties, namely the observations, damping and stiffness of\nTABLE I\nREWARDS AND PENALTIES USED DURING TRAINING.\nreward\nformula\nweight\njustification\nObject rotation\nmin(\u2213\u03c9y + 1, 2, \u00b1\u03c9y + 4)\n0.01\nreward the rotation in the y axis (can be reversed)\nTorque penalty\n||\u03c4||2\n-0.02\nprevent joints from applying large torques\nAction penalty\n||a||2\n-0.002\nprevent large actions\nDrop penalty\n||xobj \u2212 xhand||2 > 24 cm\n-1.0\npenalize object drops (one-time penalty after which environment is reset)\nTABLE II\nOBSERVATIONS USED BY THE ACTOR AND CRITIC NETWORKS.\ninput\ndimensions\nactor\ncritic\njoint pos\n11\n\u2713\njoint pos command\n11\n\u2713\n\u2713\njoint pos history\n55\n\u2713\njoint velocity\n11\n\u2713\njoint torque\n11\n\u2713\nobject pos\n3\n\u2713\nobject quat\n4\n\u2713\nobject linear vel\n3\n\u2713\nobject angular vel\n3\n\u2713\nfingertip position\n15\n\u2713\nfingertip quaternion\n20\n\u2713\nfingertip lin vel\n15\n\u2713\nfingertip ang vel\n15\n\u2713\nfingertip force\n15\n\u2713\nprevious actions\n11\n\u2713\n\u2713\nthe tendons and the joints, joint range of motion, mass and\nfriction of the robot and object, and object scale.\nIV. EXPERIMENTS\nA. Training the policy in simulation\nThe\nGPU-based\nhigh-performance\nphysics\nsimulator\nIsaacGym [2] was used to simulate the robot for training\nthe policy. 4096 environments were simulated in parallel on\na single NVIDIA A10G GPU. The simulation was run at 60\nHz, while the policy ran every three steps, resulting in a 20\nHz policy. The actor and critic networks were separate MLPs\nwith 4 hidden layers of dimensions [512, 512, 256, 128] and\nELU activations. We have trained with and without domain\nrandomization (DR) to compare its effect on the performance\nof the policy on the real robot.\nFigure 5 shows the resulting training curve. As RL perfor-\nmance heavily depends on random seed selection, we have\nrun the training with multiple random seeds and plotted their\nmean and standard deviation. As an additional indicator of\nthe policy performance besides the reward, we have also\nlogged the angular velocity in the desired direction along\nthe y axis.\nWe have also tried to rotate the ball in the x and z axes\nas well, by swapping the \u03c9y in the reward to \u03c9x and \u03c9z.\nHowever the RL algorithm was not able to come up with\na policy to consistently rotate the ball in those axes. We\nattribute this phenomenon to the lack of joints for abduction\nand adduction on this hand prototype, which are the joints\nto spread fingers apart and together. When we ourselves try\n0\n20\n40\n60\nTime [min]\n\u22122\n\u22121\n0\n1\n2\n3\nTotal reward [-]\nRewards\nDR\nNo DR\nDR, reverse\n0\n20\n40\n60\nTime [min]\n\u22122\n\u22121\n0\n1\n-\u03c9y [rad/s]\nAverage angular velocity\nDR\nNo DR\nDR, reverse\nFig. 5.\nTraining curve evolution for the policies, trained with and without\ndomain randomization (DR), and for a reversed target rotation direction. We\ntook the mean of 9 training rounds for each approach. An area of \u00b1\u03c3 is\nshown around both plots.\nto rotate objects in the x and y axes in our hands as well,\nwe see that these joints are required.\nIt can be seen in fig. 5 that the performance rapidly\nincreases in the first 10 minutes then gradually plateaus out\nafterwards. For one of the training runs with DR enabled, the\npolicy performance suddenly collapsed at around 30 minutes,\nwhich increases the standard deviation of the reward and\nslightly lowers the mean. This suggests that testing multiple\nruns with different random seeds is crucial for evaluating the\nactual performance of the RL setup. For both metrics logged,\nthe performance is higher when DR is disabled. This is to\nbe expected as the non-DR policy can be fine-tuned to an\nenvironment with a single set of physics parameters, where\nas the DR policy must work with many different physics\nparameters.\nB. Running the policy on the real robot\nThe performance difference across different random seeds\nwas even larger when it was applied to the real robot than\nit was within the simulation. Some policies even stopped\nmoving the finger after a few seconds of running the policy,\ngetting stuck in a hand pose where the policy outputs zero\nvalues for actions. Therefore, we ran each of the policies\ntrained with separate random seeds, and picked the best\nperforming one from each condition to evaluate the best-\ncase performance for each. Interestingly, we have found that\nmultiplying the joint position measurements by 0.5 before\nsending it to the policy improves the performance of the\npolicy when run on the real robot, and achieves a motion\nmuch closer to that in simulation. We hypothesize that as\nthe finger presses down on the ball, the tendon stretches and\nthe structure deforms, pulling the tendon further than it was\nat the moment of contact. This affects the joint angle estimate\nDR (sim)\nDR (real)\nNo DR (sim)\nNo DR (real)\nDR, reverse (sim)\nDR, reverse (real)\n\u22122\n0\n2\n-\u03c9y [rad/s]\nMeasured angular velocity\nFig. 6.\nDistribution of the object rotational velocity on the real and simulated robot, for policies trained with and without DR. The gray strip indicates\nthe region in which the object rotation reward is at its maximum value. The diamond indicates the mean.\nfrom the EKF, which erroneously estimates that the finger is\nbent more than it actually is. We believe that scaling down\nthe observations works as a simple corrective measure for\nthis effect.\nTo measure the rotation of the ball, we have embedded an\nIMU and Bluetooth device (Arduino Nano 33 BLE) within\na spherical gachapon capsule toy ball, and received the\norientation (obtained by fusing accelerometer and gyroscopic\nmeasurements with the Madgwick filter) and rotational ve-\nlocity of the object via Bluetooth. These measurements were\nnot used in the policy, and were used just for evaluating the\nperformance. The rotational velocity was converted to the\nrobot\u2019s frame, and smoothed with an exponential smoothing\nfilter to remove noise from measurements.\nThe result is shown in fig. 6. The gray strip indicates the\nregion in which the object rotation reward is at its maximum\nvalue, i.e. the \u201dtarget region\u201d of the policy. Within simulation,\nthe mean of the angular velocity of the object is similar\nin the DR and non-DR policies. However, the DR policy\nexhibits a larger distribution in the velocity, presumably due\nto the variance in performance due to the randomized physics\nparameters. When they are applied to the real robot, the im-\nportance of DR becomes apparent, as the non-DR policy fails\nto rotate the sphere, just rocking it back and forth in its hand.\nThe DR policy succeeds in consistently rotating the ball,\nachieving the target rotational velocity for the majority of the\nmeasurements. The policy trained with a reversed rotational\ndirection was also applied to the real robot, which again\nsuccessfully rotated the ball to within the target velocity, for\nthe majority of the sampled measurements. Figure 1 shows\nthree snapshots, taken 2 seconds apart, of the policy running\non the robot, and the accompanying video also shows videos\nof each policy running on the real robot.\nV. CONCLUSION\nWe have introduced our anthropomorphic hand platform\nfor use in autonomous manipulation. For this platform, we\nhave developed a method to model, control, and sense rolling\ncontact joints so that they can be integrated into a parallelized\nsimulation environment to train a closed-loop policy. We\nshow that the trained policy can be run on our physical\nrobotic hand to achieve dexterous sphere rotation.\nWhile simulators and environments for RL training for\ndexterous manipulation have become more capable and ac-\ncessible recently, we have yet to see the same for five-\nfingered biomimetic robotic hands. The Faive Hand devel-\noped at the Soft Robotics Lab is aimed at making dexterous\nmanipulators more capable and accessible. In this work we\nshow that the hand can achieve zero-shot transfer of skills\ntrained with RL in the IsaacGym simulator, showing the\npotential of this hand to be used for other tasks trained with\nRL.\nHowever, there are still limitations in the software and\nhardware. When we have tried to apply cube reorientation\ntasks such as in [1], the policy did work in simulation, but\nfailed on the real robot. This motion is more complex than\nour single-axis sphere rotation task, as the object is not\nsymmetric as the sphere and must be rotated around all three\naxes. We attribute the failure on the real robot to a multitude\nof factors, such as poor joint angle measurement from the\nEKFs, especially when there is contact, and a lack of proper\nsystem identification to ensure accurate actuation dynamics\nin the IsaacGym simulator. We will continue to develop the\nrobotic hand to solve these challenges, with a combination of\nphysical and programmatic approaches, such as integrating\nmore sensors for proprioceptive measurements or a better\nsystem identification process to reduce the sim2real gap.\nACKNOWLEDGMENT\nThe authors thank Jonas Lauener for creating the two-\nfinger prototype of the rolling contact joint finger. Yasunori\nToshimitsu is partially funded by the Takenaka Scholarship\nFoundation, the Max Planck ETH Center for Learning Sys-\ntems, and the Swiss Government Excellence Scholarship.\nThis work was partially funded by the Amazon Research\nAwards. This work was also supported by an ETH RobotX\nresearch grant funded through the ETH Zurich Foundation.\nREFERENCES\n[1] OpenAI, M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz,\nB. McGrew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray,\nJ. Schneider, S. Sidor, J. Tobin, P. Welinder, L. Weng, and W. Zaremba,\n\u201cLearning dexterous in-hand manipulation,\u201d Aug. 2018.\n[2] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Mack-\nlin, D. Hoeller, N. Rudin, A. Allshire, A. Handa, and Gavriel State,\n\u201cIsaac gym: High performance GPU based physics simulation for\nrobot learning,\u201d Nov. 2021.\n[3] N. Rudin, D. Hoeller, P. Reist, and M. Hutter, \u201cLearning to walk in\nminutes using massively parallel deep reinforcement learning,\u201d in Pro-\nceedings of the 5th Conference on Robot Learning, ser. Proceedings\nof Machine Learning Research, A. Faust, D. Hsu, and G. Neumann,\nEds., vol. 164.\nPMLR, 2022, pp. 91\u2013100.\n[4] A. Handa, A. Allshire, V. Makoviychuk, A. Petrenko, R. Singh, J. Liu,\nD. Makoviichuk, K. Van Wyk, A. Zhurkevich, B. Sundaralingam,\nY. Narang, J.-F. Lafleche, D. Fox, and Gavriel State, \u201cDeXtreme:\nTransfer of agile in-hand manipulation from simulation to reality,\u201d\nOct. 2022.\n[5] T. Chen, J. Xu, and P. Agrawal, \u201cA system for general in-hand\nobject re-orientation,\u201d in Proceedings of the 5th Conference on Robot\nLearning, ser. Proceedings of Machine Learning Research, A. Faust,\nD. Hsu, and G. Neumann, Eds., vol. 164. PMLR, 2022, pp. 297\u2013307.\n[6] Z.-H. Yin, B. Huang, Y. Qin, Q. Chen, and X. Wang, \u201cRotating without\nseeing: Towards in-hand dexterity through touch,\u201d Mar. 2023.\n[7] A. Allshire, M. MittaI, V. Lodaya, V. Makoviychuk, D. Makoviichuk,\nF. Widmaier, M. W\u00a8uthrich, S. Bauer, A. Handa, and A. Garg, \u201cTrans-\nferring dexterous manipulation from GPU simulation to a remote\nreal-world TriFinger,\u201d in 2022 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS), Oct. 2022, pp. 11 802\u201311 809.\n[8] W. Robotics, \u201cAllegro hand: Highly adaptive robotic hand for\nr&d,\u201d 2023. [Online]. Available: https://www.wonikrobotics.com/\nresearch-robot-hand\n[9] S.\nR.\nCompany,\n\u201cShadow\ndexterous\nhand\nseries\n-\nresearch\nand development tool,\u201d 2023. [Online]. Available: https://www.\nshadowrobot.com/dexterous-hand-series/\n[10] K. Shaw, A. Agarwal, and D. Pathak, \u201cLEAP hand: Low-cost, ef-\nficient, and anthropomorphic hand for robot learning,\u201d https://www.\nroboticsproceedings.org/rss19/p089.pdf, accessed: 2023-7-11.\n[11] N. G\u00a8urtler, F. Widmaier, C. Sancaktar, S. Blaes, P. Kolev, S. Bauer,\nM. W\u00a8uthrich, M. Wulfmeier, M. Riedmiller, A. Allshire, Q. Wang,\nR. McCarthy, H. Kim, J. B. Pohang, W. Kwon, S. Qian, Y. Toshimitsu,\nM. Y. Michelis, A. Kazemipour, A. Raayatsanati, H. Zheng, B. G.\nCangan, B. Sch\u00a8olkopf, and G. Martius, \u201cReal robot challenge 2022:\nLearning dexterous manipulation from offline data in the real world,\u201d\nAug. 2023.\n[12] M. Wuthrich, F. Widmaier, F. Grimminger, S. Joshi, V. Agrawal,\nB. Hammoud, M. Khadiv, M. Bogdanovic, V. Berenz, J. Viereck,\nM. Naveau, L. Righetti, B. Sch\u00a8olkopf, and S. Bauer, \u201cTriFinger: An\nopen-source robot for learning dexterity,\u201d in Proceedings of the 2020\nConference on Robot Learning, ser. Proceedings of Machine Learning\nResearch, J. Kober, F. Ramos, and C. Tomlin, Eds., vol. 155.\nPMLR,\n2021, pp. 1871\u20131882.\n[13] F. Shi, T. Homberger, J. Lee, T. Miki, M. Zhao, F. Farshidian,\nK. Okada, M. Inaba, and M. Hutter, \u201cCircus ANYmal: A quadruped\nlearning dexterous manipulation with its limbs,\u201d in 2021 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA). IEEE, May\n2021.\n[14] J. K. Salisbury and J. J. Craig, \u201cArticulated hands: Force control and\nkinematic issues,\u201d Int. J. Rob. Res., vol. 1, no. 1, pp. 4\u201317, Mar. 1982.\n[15] K. Kawaharazuka, S. Makino, M. Kawamura, S. Nakashima, Y. Asano,\nK. Okada, and M. Inaba, \u201cHuman mimetic forearm and hand design\nwith a radioulnar joint and flexible machined spring finger for human\nskillful motions,\u201d Journal of Robotics and Mechatronics, vol. 32, no. 2,\npp. 445\u2013458, 2020.\n[16] S. Makino, K. Kawaharazuka, A. Fujii, M. Kawamura, T. Makabe,\nM. Onitsuka, Y. Asano, K. Okada, K. Kawasaki, and M. Inaba,\n\u201cFive-fingered hand with wide range of thumb using combination of\nmachined springs and variable stiffness joints,\u201d in 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS),\nOct. 2018, pp. 4562\u20134567.\n[17] S. Puhlmann, J. Harris, and O. Brock, \u201cRBO hand 3: A platform for\nsoft dexterous manipulation,\u201d IEEE Trans. Rob., vol. 38, no. 6, pp.\n3434\u20133449, Dec. 2022.\n[18] C. Schlagenhauf, D. Bauer, K.-H. Chang, J. P. King, D. Moro,\nS. Coros, and N. Pollard, \u201cControl of tendon-driven soft foam robot\nhands,\u201d in 2018 IEEE-RAS 18th International Conference on Hu-\nmanoid Robots (Humanoids), Nov. 2018, pp. 1\u20137.\n[19] S. Kim, E. Sung, and J. Park, \u201cARC joint: Anthropomorphic rolling\ncontact joint with kinematically variable torsional stiffness,\u201d IEEE\nRobotics and Automation Letters, vol. 8, no. 3, pp. 1810\u20131817, Mar.\n2023.\n[20] Y.-J. Kim, J. Yoon, and Y.-W. Sim, \u201cFluid lubricated dexterous finger\nmechanism for human-like impact absorbing capability,\u201d IEEE Robot.\nAutom. Lett., vol. 4, no. 4, pp. 3971\u20133978, Oct. 2019.\n[21] S. W. Hong, J. Yoon, Y.-J. Kim, and H. S. Gong, \u201cNovel implant\ndesign of the proximal interphalangeal joint using an optimized rolling\ncontact joint mechanism,\u201d J. Orthop. Surg. Res., vol. 14, no. 1, p. 212,\nJuly 2019.\n[22] S.-H. Kim, H. In, J.-R. Song, and K.-J. Cho, \u201cForce characteristics\nof rolling contact joint for compact structure,\u201d in 2016 6th IEEE In-\nternational Conference on Biomedical Robotics and Biomechatronics\n(BioRob), June 2016, pp. 1207\u20131212.\n[23] S. Ookubo, Y. Asano, T. Kozuki, T. Shirai, K. Okada, and M. Inaba,\n\u201cLearning nonlinear muscle-joint state mapping toward geometric\nmodel-free tendon driven musculoskeletal robots,\u201d in 2015 IEEE-RAS\n15th International Conference on Humanoid Robots (Humanoids),\nNov. 2015, pp. 765\u2013770.\n[24] A. Meurer, C. P. Smith, M. Paprocki, O. \u02c7Cert\u00b4\u0131k, S. B. Kirpichev,\nM.\nRocklin,\nA.\nKumar,\nS.\nIvanov,\nJ.\nK.\nMoore,\nS.\nSingh,\nT. Rathnayake, S. Vig, B. E. Granger, R. P. Muller, F. Bonazzi,\nH. Gupta, S. Vats, F. Johansson, F. Pedregosa, M. J. Curry, A. R.\nTerrel, v. Rou\u02c7cka, A. Saboo, I. Fernando, S. Kulal, R. Cimrman,\nand A. Scopatz, \u201cSympy: symbolic computing in python,\u201d PeerJ\nComputer Science, vol. 3, p. e103, Jan. 2017. [Online]. Available:\nhttps://doi.org/10.7717/peerj-cs.103\n[25] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,\n\u201cProximal policy optimization algorithms,\u201d July 2017.\n[26] D. Makoviichuk and V. Makoviychuk, \u201crl-games: A high-performance\nframework for reinforcement learning,\u201d https://github.com/Denys88/rl\ngames, May 2021.\n"
  },
  {
    "title": "Training Data Protection with Compositional Diffusion Models",
    "link": "https://arxiv.org/pdf/2308.01937.pdf",
    "upvote": "5",
    "text": "Training Data Protection with Compositional Diffusion Models\nAditya Golatkar 1 Alessandro Achille 1 Ashwin Swaminathan 1 Stefano Soatto 1\nAbstract\nWe introduce Compartmentalized Diffusion Mod-\nels (CDM), a method to train different diffusion\nmodels (or prompts) on distinct data sources and\narbitrarily compose them at inference time. The\nindividual models can be trained in isolation, at\ndifferent times, and on different distributions and\ndomains and can be later composed to achieve per-\nformance comparable to a paragon model trained\non all data simultaneously. Furthermore, each\nmodel only contains information about the sub-\nset of the data it was exposed to during training,\nenabling several forms of training data protec-\ntion. In particular, CDMs enable perfect selec-\ntive forgetting and continual learning for large-\nscale diffusion models, allow serving customized\nmodels based on the user\u2019s access rights. Empir-\nically the quality (FID) of the class-conditional\nCDMs (8-splits) is within 10% (on fine-grained\nvision datasets) of a monolithic model (no splits),\nand allows (8x) faster forgetting compared mono-\nlithic model with a maximum FID increase of\n1%. When applied to text-to-image generation,\nCDMs improve alignment (TIFA) by 14.33% over\na monolithic model trained on MSCOCO. CDMs\nalso allow determining the importance of a subset\nof the data (attribution) in generating particular\nsamples, and reduce memorization.\n1. Introduction\nDiffusion models have captured the popular imagination by\nenabling users to generate compelling images using simple\ntext prompts or sketches. They have also, in some cases,\ncaptured the personal workmanship of artists, since the sheer\nvolume of training data makes it challenging to verify each\nsample\u2019s attribution (Vyas et al., 2023). It is also challeng-\ning to quantify the data contribution in shaping the model\u2019s\ngenerated output, which calls for the development of new\nforms of protection for large-scale training data, ranging\n1AWS AI Labs. Correspondence to: Aditya Golatkar <ago-\nlatka@amazon.com>.\nPreprint.\nfrom methods that limit the influence of training samples\na-priori (e.g., differential privacy), remove the influence of\ntraining examples that were wrongly included in the train-\ning a-posteriori (selective forgetting, model disgorgement),\nand limit the influence of samples on the training output\n(copyright protection), or at least identify which samples\nhad the most influence (attribution), thus preventing memo-\nrization and/or generation of samples that are substantially\nsimilar to training data. While research in these fields is\nthriving, the methods developed are not transferable to large-\nscale diffusion models. Extending known techniques seems\ndaunting since information from different samples is mixed\nirreversibly the weights of the model, making unlearning or\nevaluating the influence of specific data challenging.\nWe\nintroduce\nCompartmentalized\nDiffusion\nModels\n(CDMs), where separate parameters (or adapters) are trained\nindependently on different data sources, ensuring perfect\n(deterministic) isolation of their respective information. All\nparameters are then merged at inference time and used\njointly to generate samples. This technique is simple to\nimplement with any existing DM architecture; CDMs are\nthe first means to perform both selective forgetting (unlearn-\ning) and continual learning on large-scale diffusion models.\nIn addition to enabling the removal of information in the\ntrained model from particular data, the method also allows\nattribution, which may inform the process of assessing the\nvalue of different cohorts of training data, as well as ensure\nthat there is no memorization so the generated images are\nnot substantially similar to those used for training.\nThe key enabler of CDMs is a closed-form expression for\nthe backward diffusion flow as a mixture of the flows of its\ncomponents, which is simple to derive and implement, but\ncan suffer from two key problems. Implementation-wise,\ntraining and running inference with multiple models can\nquickly balloon the computational cost, and ensembling\nmodels trained on different subsets in principle can signifi-\ncantly underperform compared to a monolithic model, due\nto loss of synergistic information (Dukler et al., 2023).\nTo address the first problem, we propose to use a pre-\ntrained diffusion model and fine-tune on various down-\nstream datasets. Fine-tuning helps the model preserve syn-\nergistic information across different shards (Dukler et al.,\n2023). Further, to reduce the training/inference cost we can\n1\narXiv:2308.01937v3  [cs.LG]  13 Feb 2024\nTraining Data Protection with Compositional Diffusion Models\nkeep the single shared backbone fixed and train adapters (Hu\net al., 2021) or prompt(Jia et al., 2022; Sohn et al., 2023) on\neach disjoint shard of data. Adapters can be trained remotely\nand shared with a central server without exposing the raw\ndata, while prompts can use efficient batch-parallelization\nfor quick inference.\nIn regard to the latter problem, we empirically show that, in\na variety of settings, a compartmentalized model can match\nthe generative performance of a paragon model trained on\nall the data jointly (in some cases outperform a monolithic\nmodel), while allowing all the above mentioned data se-\ncurity improvements. This is both due to the particular\nobjective of diffusion models, which in theory allows sepa-\nrate model training without any loss in performance (even\nif this need not be the case for real models), and to our use\nof a safe training set, which allows the compartmentalized\nmodel components to still capture a significant amount of\nsynergistic information (Dukler et al., 2023).\n2. Related Work\nForgetting/Unlearning: Forgetting studies the problem\nof removing information pertaining to training data from\nthe weights of a trained machine learning model. There\nare two major directions of works in forgetting, the first\ndirection (Ginart et al., 2019; Bourtoule et al., 2021; Yan\net al., 2022; Koch and Soll, 2023b; Kumar et al., 2022; Yu\net al., 2022; Yan et al., 2022; Yu et al., 2022; Koch and\nSoll, 2023b; Du et al., 2023) involves splitting the training\ndataset into multiple shards and training separate models\non each shard. This ensures that information contained in\na particular training sample is restricted only to a specific\nsubsets of parameters. When asked to remove a particular\ntraining sample, the unlearning procedure simply drops the\ncorresponding shard and re-trains it without that sample.\nThe second direction involves training a single machine\nlearning model for the entire dataset, and providing approxi-\nmate unlearning guarantees (Golatkar et al., 2020b;a; 2021).\nSuch methods rely on the linearization (Achille et al., 2021)\nof the network with respect to a pre-trained initialization\nand then perform an approximate Newton step for stochastic\nforgetting (Golatkar et al., 2022; Guo et al., 2019). (Neel\net al., 2021; Gupta et al., 2021; Ullah et al., 2021; Chourasia\net al., 2022; Sekhari et al., 2021; Dwork et al., 2014).\nDiffusion Models: Diffusion models are state-of-the-art\ngenerative models useful for high quality image genera-\ntion (Ho et al., 2020; Song et al., 2020a; Rombach et al.,\n2022; Dhariwal and Nichol, 2021; Lipman et al., 2022).\n(Rombach et al., 2022; Ramesh et al., 2022) to video gen-\neration (Ho et al., 2022; Molad et al., 2023). Diffusion\nmodels gradually add Gaussian noise to an image follow-\ning a Markov process in the forward step during training\nto learn the score function, and perform denoising in the\nreverse step (Nelson, 1967; Anderson, 1982) to generate\ndata using diffusion solvers (Lu et al., 2022a;b; Karras et al.,\n2022; Song et al., 2020a). (Song et al., 2020c) modelled dif-\nfusion models using stochastic differential equations (SDE).\nThis enables the use of stochastic differential solvers and\nprobability flow equations for reverse diffusion. (Bao et al.,\n2022) uses a transformer based model using a ViT (Doso-\nvitskiy et al., 2020) which takes all in information (noisy\nimage, timestep embedding, textual embedding) as input\ntokens different from standard diffusion models (Rombach\net al., 2022) which processes conditional information using\ncross-attention layers throughout the depth of the model.\nWe use the U-ViT (Bao et al., 2022) and Stable Diffusion\n(Rombach et al., 2022) for experiments in this paper.\nImage Manipulation:(Gandikota et al., 2023; Huang et al.,\n2023; Kumari et al., 2023; Wu et al., 2024) proposed im-\nage manipulation techniques to prevent the diffusion model\nfrom generating certain concepts, however, such methods\ndo not guarantee permanent removal of those concepts from\nthe weights, which may be recovered through adversar-\nial prompting. While CDMs certify removal of subsets\nof data/concepts as the corresponding sub-models are re-\ntrained given an unlearning request.\nCompositional Models, MoE:(Du et al., 2023; Liu et al.,\n2022; Wang et al., 2023) provided methods for composi-\ntional image generation, however their method is only aimed\nat improving the text-to-image alignment during generation.\nAt inference, they propose to break the input prompt into\nsubparts, compute the denoising prediction for each, and\nthen average at each step during backward diffusion. While\nour method is aimed at improving the privacy of the model\nby sharding the training dataset into multiple subsets and\ntraining separate model for each. The two approach are com-\npletely orthogonal, as one involves breaking the inference\nprompt into nouns and using the same model multiple times,\nwhile ours involves splitting the training set and training\nseparate models. Similarly mixture-of-experts (MoE) (Xue\net al., 2023; Rajbhandari et al., 2022) trains MoE layers\nwith routing where each subset of parameters still contains\ninformation about the entire dataset (monolithic), and only\na subset of parameters are used during inference to reduce\ncomputational cost. MoE lacks a a-priori separation of in-\nformation in the weights making it unfavorable for perfect\nunlearning unlike CDMs.\nMemorization, Copyrights, and Differential Privacy:The\npopularity of diffusion models has also prompted re-\nsearchers to investigate memorization (Carlini et al., 2023),\ncopyright protection (Vyas et al., 2023) and privacy in diffu-\nsion models. (Carlini et al., 2023) showed successful extrac-\ntion attacks on diffusion models raising privacy risks. (Vyas\net al., 2023) provided a formalism for copyright protection\n2\nTraining Data Protection with Compositional Diffusion Models\nNa\u00efve Ensembling\nOur Ensembling\nComposition weights\npbirds\npcars\nx0 \u223c N(0, I)\nxt\nxT\nSampling Dynamics\nAdd cloud of curves\nBirds Source\nCars Source\nFigure 1. Compositional diffusion models. We train two diffusion models on two disjoint data distributions: Birds (CUB-200 (C.Wah\net al., 2011)) and Stanford Cars (Krause et al., 2013). (Left) Image generated by naively composing the models by averaging their output.\nThe sample images are distorted and contain elements of both distributions within the same image. (Center) Sample images generated by\nour method using the same models. The quality is substantially higher and the samples do not conflate the two distributions. (Right)\nThe key idea is illustrated graphically at each step of the reverse diffusion process, where we show the estimated optimal weights wi to\nassign to each component. At the beginning, the two components are weighted equally, also shown in the plot below, but as the model\napproaches convergence, the weights increasingly favor only one of the two models, based on the likelihood that it was trained on a data\ndistribution closest to the current sample.\nin diffusion models using a less stringent version of differen-\ntial privacy. They provided sampling algorithms to prevent\nthe output of training samples from trained diffusion models\nafter querying. To ensure privacy protected training of dif-\nfusion models, (Dockhorn et al., 2022; Ghalebikesabi et al.,\n2023) proposed training diffusion models with differential\nprivacy and show results toy datasets like MNIST/CIFAR\n(LeCun et al., 2010; Krizhevsky et al., 2009).\nIn Section 3 we propose compartmentalized diffusion mod-\nels, shows its derivations, along with computation of the\nweights in Section 3.4. Then we discuss the architecture\nand the implementation details in Section 4, followed by\nthe application of the proposed method in Section 5 and\nconclude in Section 6.\n3. Compartmentalized Diffusion Models\nConsider a dataset D = {D1, . . . , Dn} composed of n of\ndifferent data sources Dn. The core idea of CDMs is to\ntrain separate models or adapters independently on each\nDi to localize information, and compose them to obtain\na model that behaves similarly to a model trained on the\nunion S Di of all data (monolithic paragon). We will use\nthe score based stochastic differential equation formulation\nof diffusion models (Song and Ermon, 2019).\n3.1. Diffusion models as SDEs\nLet p(x0) be the (unknown) ground-truth data distribu-\ntion.\nAt any time t in the forward process, we define\nthe conditional distribution of the input as pt(xt|x0) =\nN(xt; \u03b3txo, \u03c32\nt I), where \u03b3t = exp(\u22120.5 \u00b7\nR t\n0 \u03b2tdt) and\n\u03c32\nt = 1 \u2212 exp(\u2212\nR t\n0 \u03b2tdt). Using a variance preserving\ndiscrete Markov chain, we obtain the following stochastic\ndifferential equation which models the forward process:\ndxt = \u22121\n2\u03b2txtdt +\np\n\u03b2td\u03c9t\n(1)\nHere xt is the input at time t in the forward process, \u03b2t\nare the transition kernel coefficients and d\u03c9t is the sandard\nWiener process. Given the forward process, (Lindquist and\nPicci, 1979) showed that there exists a backward process,\nwhich enables us to generate samples from p(x0) given a\nrandom sample xT \u223c N(0, 1) given by:\ndxt =\n\u0010\n\u2212 1\n2\u03b2txt \u2212 \u2207xt log pt(xt)\n\u0011\ndt +\np\n\u03b2td\u03c9t\n(2)\nwhere pt(xt) =\nR\nx0 pt(xt|x0)p0(x0)dx0 is the marginal\ndistribution at time t.\nPrevious result is quite power-\nful as it highlights the fact that we only need access to\n\u2207xt log pt(xt) in order to generate samples from p(x0),\nwhich is independent of any normalization constant. (Song\net al., 2020c) also showed that there exist an ordinary dif-\nferential equation corresponding to eq. (2) which enables\n3\nTraining Data Protection with Compositional Diffusion Models\nNo Split\n2-Splits\n4-Splits\n8-Splits\nParagon\nClassi\ufb01er\nNaive Avg\nRandom\nClassi\ufb01er\nNaive Avg\nRandom\nClassi\ufb01er\nNaive Avg\nRandom\nOxfordPets\n9.01\n8.72\n9.03\n8.85\n9.42\n10.84\n9.96\n9.53\n9.54\n11.91\nCUB-200\n3.73\n3.77\n5.04\n3.85\n4.23\n5.37\n4.51\n4.43\n13.01\n6.71\nStanf. Cars\n5.05\n4.7\n6.14\n4.38\n5.79\n8.68\n5.74\n5.67\n35.05\n6.47\nAverage\n5.93\n5.73\n6.73\n5.69\n6.48\n8.29\n6.73\n6.54\n19.20\n8.36\nTable 1. Compartmentalized DMs for class conditional image generation. We report, across various datasets, the FID score obtained\nwith different methods to generate images starting from a compartmentalized model. We split each dataset uniformly across classes in\nmultiple splits. Ideally the performance of the method should be close to the paragon performance of a non-compartmentalized model\ntrained on all the data. We observe that for small number of shards the performance of the compartmentalized model can actually be better\nowning to the regularization effect of ensembling (FID score for 2-splits is lower (5.73, 5.69) compared to the paragon score 5.93). CDMs\nenable us split the training dataset into multiple shards with a minor increase in the average FID score (from 5.93 to 6.54 for 8-splits),\nwhile allowing us to unlearn much more efficiently.\nquicker generation samples from p(x0) (Song et al., 2020a).\nIn practice, we model pt(xt) =\nR\nx0 pt(xt|x0)p0(x0)dx0\nusing a deep neural network s\u03b8(xt, t) (or \u03f5\u03b8(xt, t) as more\ncommonly denoted in the literature(Ho et al., 2020)), and\noptimize it using score matching (Song and Ermon, 2019;\nSong et al., 2020c;b).\n3.2. Compartmentalization\nLet us consider now the case where the data distribution\np(x0) is composed as a mixture of distributions:\np(x0) = \u03bb1p(1)(x0) + . . . + \u03bbnp(n)(x0),\n(3)\nsuch that the data from each training source Di is sam-\npled from its corresponding mixture component p(i)(x).\nSuppose that n independent diffusion models have been\ntrained on each p(i)(x) independently, leading to n different\nscore functions {\u2207xt log p(i)(xt)}n\ni=1 (empirically given by\n{s(i)\n\u03b8 (xt, t)}n\ni=1). The question is whether we can combine\nthese mixture-specific score functions to generate a sample\nfrom the global distribution p(i)(x). To this end, we want to\nfind the score function of the global distribution and write is\nusing the score function of the individual distributions. Then\nusing the trained models s(i)\n\u03b8 (xt, t) we can approximate the\nempirical score for the global distribution and sample from\nit using diffusion samplers.\nTo compute the score for the global distribution, we need to\ncompute the global marginal distribution. Using the linearity\nof integration with a gaussian we can show that:\npt(xt) =\nZ\npt(xt|x0)\nn\nX\ni=1\n\u03bbip(i)(x0)\n=\nn\nX\ni=1\n\u03bbipt(xt|x0)p(i)(x0) =\nn\nX\ni=1\n\u03bbip(i)\nt (xt)\n(4)\n3.3. Score of the mixture\nTo sample from the global distribution eq. (3) using eq. (2)\nwe need to compute the score of the marginal eq. (4).\nProposition 3.1. Let {s(i)\n\u03b8 (xt, t)} be a set of diffusion mod-\nels trained on {Di}n\ni=1 separately. Then the score func-\ntion corresponding to a diffusion model trained on {Di}n\ni=1\njointly is given by,\ns\u03b8(xt, t) =\nn\nX\ni=1\nwt(xt, t)s(i)\n\u03b8 (xt, t)\n(5)\nwhere wt(xt, t) = \u03bbi\np(i)\nt (xt)\npt(xt) , pt(xt) = Pn\ni=1 \u03bbip(i)\nt (xt).\nWe assume that each DNN has enough capacity, to min-\nimize Ex0,t\u2225\u2207xt log p(i)\nt (xt) \u2212 s(i)\n\u03b8 (xt, t)\u22252. Thus we re-\nplace \u2207xt log p(i)\nt (xt) with its empirical estimate s(i)\n\u03b8 (xt, t).\n3.4. Computing the weights\nThe term wt(xt, t) in eq. (5) has an intuitive interpretation.\nLet x0 \u223c p(x) = P\ni \u03bbip(i)(x) be a sample from the mix-\nture distribution, and let z \u2208 {1, . . . , n} be a discrete ran-\ndom variable which tells us the index of the mixture compo-\nnent that generated the sample (so that p(x|z = i) = p(i)(x)\nand p(x) = P\ni p(x|z = i)p(z = i). Then, by Bayes\u2019s rule,\none readily sees that\npt(z = i|x) = p(i)\nt (x)\npt(x) .\nThat is, the additional weighting factor for each model can\nbe interpreted as the probability that the current noisy sam-\nple xt originated from the data distribution used to train\nthat model. To illustrate the behavior (see Figure 1), con-\nsider the case where p(1)(x) and p(2)(x) are disjoint (for\nexample, images of pets and flowers respectively). At the\nbeginning of the reverse diffusion, due to the amount of\n4\nTraining Data Protection with Compositional Diffusion Models\n0\n1\n2\n3\n4\n5\n6\n7\nNumber of Shards\n20\n15\n10\n5\n0\nRel Change in FID (%)\nPets\nOur\n0\n1\n2\n3\n4\n5\n6\n7\nNumber of Shards\n30\n25\n20\n15\n10\n5\n0\nRel Change in FID (%)\nCUB200\nOur\n0\n1\n2\n3\n4\n5\n6\n7\nNumber of Shards\n8\n6\n4\n2\n0\nRel Change in FID (%)\nStanfordcars\nOur\n20\n15\n10\n5\n0\nNaive Avg\n0\n20\n40\n60\n80\n100\nNaive Avg\n0\n100\n200\n300\n400\n500\n600\nNaive Avg\nFigure 2. Forgetting/Continual Learning with Compartmentalized DMs: Relative change (wrt single shard) in the FID score as we\ncontinually add more shards (class-conditional generation with 8-splits). FID scores for our method continuously decreases as we add\nmore shards compared to naive averaging which may result in incorrect mixture of vector fields. Classifier (our) based ensembling enables\ncorrect selection of models from the mixture of ensembles, and therefore results in decrease in FID with increase in data. This plot (when\nviewed from right to left) also shows the increase in FID, given a request to forget an entire shard.\nnoise the sample is equally likely to be generated from ei-\nther distribution, and both will have similar weight. As the\ntime increases and more details are added to the sample, the\nimage will increasingly be more likely to be either a pet or\na flower. Correspondingly the generated image should draw\nonly from the relevant domains, whereas using others would\nforce the model to generate images of flowers by inductively\ncombining images of pets (Figure 1).\nThis interpretation also gives us a way to compute p(i)\nt\n(x)\npt(x) . In\nprinciple, one could estimate both p(i)\nt (x) and pt(x) using\nthe diffusion model itself, however this is computationally\nexpensive. On the other hand, pt(z = i|x) is simple to\nestimate directly with a small auxiliary model. Let f(x, t)\nbe a n-way classifier that takes as input a noisy image x and\na time-step t and outputs a softmax. In this paper we try\ntwo classifiers, (1) k-NN using CLIP(Radford et al., 2021)\nfor text-to-image models and (2) training a neural network\nclassfier for class-conditional models. To train the network,\nwe can generate pairs {(xi, ki)}N\ni=1 where ki \u223c 1, . . . , n\nis a random component index and xi \u223c N(x|\u03b3tx0, \u03c32\nt I),\nx0 \u223c Dki is obtained by sampling a training image from\nthe corresponding dataset Dki and adding noise to it. The\nnetwork is trained with the cross-entropy loss (standard\nimage classification) to predict ki given xi and t. Then, at\nconvergence f(x, t) =\n\u0010\np(1)\nt\n(x)\npt(x) , . . . , p(n)\nt\n(x)\npt(x)\n\u0011\n= wi(xt, t),\nwhere wi(xt, t) is from eq. (5).\nThe classifier helps implement model selection at inference\ntime, which aims to select the best model which describes\nthe data distribution. However, when all the components of\nthe mixture distribution are close in a distributional sense,\nwe can replace the classifier, with naive averaging of the en-\nsemble of diffusion scores. In practice, using all the models\nat each time-step of backward diffusion can be computa-\ntionally expensive, in such situations, we can approximate\nthe averaging of scores, with simple random score selec-\ntion. Thus we have 3 methods for ensembling the diffusion\nscores at inference, (1) classifier, (2) naive averaging, and\n(3) random selection. We empirically show that classifier\nalmost always outperforms naive averaging. Note that naive\naveraging may appear similar to (Du et al., 2023; Liu et al.,\n2022; Wang et al., 2023), however, there is one fundamental\ndifference \u2013 they use one fixed model and split the input\nprompt for composition, while naive averaging (CDMs in\ngeneral) split the training data, train separate models (local-\nize information) and perform compositional inference.\n4. Architecture and Implementation\nWe use Stable Diffusion 2.1 Base (SD2.1) (Rombach et al.,\n2022) for text-to-image generation (512 \u00d7 512), and U-ViT\n(Bao et al., 2022) for unconditional and class conditional\ngeneration (256 \u00d7 256). CDMs can be used to compose\nany set of diffusion models, so long as the output space is\nsame for all the models. If the number of shards increase\narbitrarily, then fine-tuning adapters (like prompts) improves\nstorage and inference efficiency. We perform both set of\nexperiments, for text-to-image generation we fine-tune the\nentire model (SD2.1) while for class-conditional generation\nwe fine-tune adapters (U-ViT).\nDatasets We use MSCOCO (Lin et al., 2014) as the private\ndataset for training text-to-image CDMs, and fine-grained\ndatasets like CUB200 (C.Wah et al., 2011), Stanfordcars\n(Krause et al., 2013), OxfordPets (Parkhi et al., 2012) for\nclass-conditional models. We split MSCOCO based on the\naesthetic score of the images, and the fine-grained datasets\nbased on the class label of the images where each split can\nbe considered as data from a separate user.\nClassifier For text-to-image generation we use a k-NN clas-\nsifier with CLIP embeddings. More precisely, at each t\nin backward diffusion, we predict x0 using the diffusion\n5\nTraining Data Protection with Compositional Diffusion Models\nBase\nIndividual models\nCompartmentalized DMs\n(SD2.1 FT AE6+, \n SD2.1 FT AE6-)\n(SD2.1 FT AE6+, \n SD2.1 FT AE5-)\n(SD2.1 FT AE6-, \n SD2.1 FT AE5-)\n(SD2.1 FT AE6+, \n SD2.1 FT AE6-, \n SD2.1 FT AE5-)\nSD2.1\nSD2.1 \nFT  \nFull \n(Paragon)\nSD2.1 \nFT  \nAE6+\nSD2.1 \nFT  \nAE6-\nSD2.1 \nFT  \nAE5-\nClf\nAvg\nRand\nClf\nAvg\nRand\nClf\nAvg\nRand\nClf\nAvg\nRand\nTIFA \n(Non-\nCOCO)\n77.4\n76.85\n78.87\n78.71\n78.07\n80.10\n80.13\n79.84\n80.48\n80.31\n79.91\n79.18\n79.43\n79.17\n80.22\n80.21\n80.21\nTIFA \n(COCO)\n85.8\n85.31\n86.25\n86.03\n85.71\n87.13\n87.24\n87\n87.25\n87.31\n87.42\n87.09\n87.10\n86.92\n87.05\n87.35\n86.84\nAverage\n81.7\n81.1\n82.5\n82.31\n81.83\n83.55\n83.6\n83.18\n83.81\n83.75\n83.6\n83.06\n83.20\n82.98\n83.57\n83.72\n83.47\nTable 2. Compartmentalized DMs improve text-to-image alignment. We report the TIFA (Hu et al., 2023), text-to-image alignment\nscore (higher is better) for different models. We obtain 3 subsets of MSCOCO (Lin et al., 2014) based on the aesthetic score inpsired from\n(Dai et al., 2023). We obtain 1k samples each with aesthetic score > 6.0 (AE6+), aesthetic score < 6 (AE6-), and aesthetic score < 5.0\n(AE5-). We fine-tune SD2.1 on each of these subsets along with entire MSCOCO (full, paragon). We observe that fine-tuning SD2.1\n(individual models) helps improve alignment compared to the base model. However, CDMs further improve the alignment, from 81.7 for\nthe base model to 83.81 for the best CDM. Even the worst CDM has better performance compared to the best individual model, and\nparagon. This shows the regularization effect provided by CDMs, compared to fine-tuning a single model on the entire dataset.\nmodel, which is used by the k-NN classifier, to compute\nprobability scores for each data source. For unconditional\nand class-conditional image generation, we train an neural\nnetwork (linear layer plus attention block) on top of inter-\nmediate level features of a U-ViT to predict the classifier\nweights for different data sources. Since Stable Diffusion\nis pre-trained on a much larger data source compared to the\nU-ViT, we observe that is sufficient to use a k-NN classifier\ninstead of training a new neural network.\n5. Applications\nCDMs empower users to selectively incorporate or exclude\nsubsets of training data, achieving performance levels com-\nparable to training monolithic models on the complete\ndataset. Additionally, these models impart a regulariza-\ntion effect, enhancing the alignment between the textual\nand visual elements, all the while facilitating subset attribu-\ntion. When all the weights in CDMs are uniformly assigned\n(naive averaging), it mitigates memorization, thereby sat-\nisfying the guarantee of copyright protection (Vyas et al.,\n2023). We will elaborate on each of the applications of\nCDMs in this section.\nForgetting. Owners of the training data may, at any point,\nmodify their sharing preferences leading to a shrinking set\nS of usable sources. When this happens, all information\nabout that data needs to be removed from the model. How-\never, the large amount of current state-of-the-art diffusion\nmodels precludes re-training on the remaining data as a vi-\nable strategy. Compartmentalized models such as CDMs\nallow for a simple solution to the problem: if a data source\nDi is removed, we only need to remove the corresponding\nmodel to remove all information about it. Moreover, if only\na subset of a training source is removed, it is only necessary\nto retrain the corresponding model. We show that increas-\ning the number of splits does not increase the FID scores\nafter composition (Table 1) which is critical for forgetting\nas its enables easy removals of shards without significantly\nloosing performance. Figure 2 shows the relative change in\nthe FID score as we drop shards.\nContinual Learning. The data sources Di may represent\nadditional batches of training data that are acquired incre-\nmentally. Retraining the model from scratch every time new\ndata is acquired, or fine-tuning an existing model, which\nbrings the risk of catastrophic forgetting, is not desirable in\nthis case. With CDMs, one can simply train an additional\nmodel on Di and compose it with the previous models. In\nFigure 2 we show that adding more shards in a continual\nfashion improves the FID score relative to a single shard.\nAlso, simple naive averaging over the shards will results in\nincorrect mixture of vector fields which can be avoided by\nthe method proposed in Proposition 3.1.\nText-to-Image Alignment (Dai et al., 2023) showed that\nfine-tuning diffusion models on high quality data improves\ntext-to-image alignment. In table 2 we show that fine-tuning\ndiffusion models in a compartmentalized fashion provides\nmuch better alignment (83.81 TIFA score) compared to fine-\ntuning a single model on the joint data source (81.1 TIFA\nscore). We obtain 3 subsets of MSCOCO based on aesthetic\nscores, (1) AE6+: 1k samples with aesthetic score > 6, (2)\n6\nTraining Data Protection with Compositional Diffusion Models\nDi\ufb00erent domains (Birds + Cars)\nSame domain (Sharded birds)\nFigure 3. Credit attribution with CDMs Compartmentalized DMs enable us to provide credit attribution corresponding to samples\nbelonging to different models. Plot shows average credit attribution when shards are from different domains (CUB200 (C.Wah et al.,\n2011) and Cars (Krause et al., 2013)) and when shards are uniform split of the same domain. For different source domains, CDMs selects\nthe appropriate domain model during backward diffusion, resulting in higher source weight for one model compared to another. For same\nsource domains, CDMS assigns equal weight for majority of backward process, until the end when it selects one source model.\nAE6-: top 1k samples with aesthetic score < 6, (3) AE5-:\ntop 1k samples with aesthetic score <5. We fine-tune SD2.1\non each data source, and compose them with CDMs. We\nobserve that CDMs consistently outperform the individual\nmodels, paragon, and base SD2.1 model for all composition\nmethods. This shows CDMs not only enable easy unlearning\nof any subset of MSCOCO, but also improve alignment due\nto the regularization effect of ensembling.\nMeasuring contribution of individual sources. Let x0 be\na sample generated solving the ODE eq. (2) starting from\nan initial x1 \u223c p1(x). The likelihood of a generated image\ncan then be computed as\nlog p1(x1) \u2212 log p(x0) = \u2212\nZ 1\n0\ndiv \u2207xt log p(i)(xt)dt,\nthat is, the divergence of the score function integrated along\nthe path. In the case of a CDM, this likelihood can further\nbe decomposed as:\nlog p1(x1) \u2212 log p0(x0) =\nX\ni\n\u03bbiLi\n=\nX\ni\n\u03bbi\nZ\ndiv\n\u0000wi(xt, t) \u2207xt log p(i)(xt)\n\u0001\ndt\n(6)\nwhere Li can be interpreted as the contribution to each\ncomponent of the model to the total likelihood. Using this,\nwe can quantify the credit Ci of the data source Di as:\nCi =\n\u03bbLi\nPn\nj=1 \u03bbjLj\n.\nWe note that while P\ni \u03bbiLi is the likelihood assigned by\nthe CDM to the the generated sample, one cannot inter-\npret the individual Li as the likelihood assigned by each\nsubmodel. In Figure 3 we show that when shards belongs\nto different distributions the credit attribution is correctly\nmore skewed (generated image belongs to one distribution)\ncompared to similar distributions which has a more uniform\nattribution (since all distributions are similar). The compo-\nsition weights for different domains at inference start with\nsimilar values and change rapidly within the first 10 genera-\ntion steps (see Figure 3 left). For same domains the weights\nstart with similar values and maintain them until almost half\ngeneration is complete before selecting one split (Figure 3\nright).\nBetter out-of-domain (OOD) coverage and reduce mem-\norization Often times diffusion models under-perform on\ncertain sub-populations of the training data. For eg. in fig. 4\n(A) we show that SD2.1 fine-tuned on MSCOCO AE6+\n(with TIFA alignment score of 82.5, see table 2) is unable\nto produce realistic birds (OOD sub-population) when pro-\nvided with descriptive prompts. However, we show that\nusing CDMs we can compose SD2.1 FT AE6+ (in fig. 4)\nwith SD2.1 FT CUB-200 (birds dataset) at inference to\nobtain improved alignment, better OOD coverage, with real-\nistic looking birds. In fig. 4, w0, w1 correspond to mixture\nweights from proposition 3.1.\nIn fig. 4, we show that diffusion models tend to memorize\ntraining data (Carlini et al., 2023). CDMs can reduce mem-\norization in diffusion models by ensembling diffusion paths\nfrom different models at inference, as a result the generated\nimage will not resemble output from any particular source\nmodel. CDMs help improve the diversity of the synthesized\nimages along with reduced memorization. This is because\nusing naive averaging is equivalent to sampling from Algo-\nrithm 3 in (Vyas et al., 2023) which provide copy protection,\nand thus reduces memorization.\nLimitations Even though CDMs enjoy a myriad of nice\nproperties like easy unlearning, continual model update,\ncredit attribution, improved alignment, OOD coverage and\nreduce coverage, they suffer from increase in number of\ntraining parameters, and high inference cost. Increased\n7\nTraining Data Protection with Compositional Diffusion Models\nPrompt: the bird has a long dull beak, the head around the beak is white and rest of the bird is dark grey\nPrompt: The Big Ben tower towering over the of London\n(w0, w1) = (1,0)\n(w0, w1) = (0.75,0.25)\n(w0, w1) = (0.5,0.5)\n(w0, w1) = (0.25,0.75)\n(w0, w1) = (0,1)\nCDMs for OOD Generalization\n: mixture coe\ufb03cient for SD2.1 FT AE6+\nw0\n: mixture coe\ufb03cient for SD2.1 FT CUB\nw1\nCDMs for reducing memorization\nSample from \nAE6+ training set\n(A)\n(B)\nSD2.1 FT AE6+\nSD2.1 FT AE5-\nSD2.1 FT AE6+\nSD2.1 FT CUB\n(w0, w1) = (1,0)\n(w0, w1) = (0.75,0.25)\n(w0, w1) = (0.5,0.5)\n(w0, w1) = (0.25,0.75)\n(w0, w1) = (0,1)\n: mixture coe\ufb03cient for SD2.1 FT AE6+\nw0\n: mixture coe\ufb03cient for SD2.1 FT AE5-\nw1\nIncreasing the CDM mixture contribution of CUB\nIncreasing the CDM mixture contribution of SD2.1 FT AE5-\nFigure 4. Out-of-Distribution Coverage and Memorization: Top (A): CDMs enable out of distribution (OOD) generalization by\ncombining shard belonging to different domains. Figure (A) shows that SD2.1 fine-tuned (FT) on AE6+ produces unrealistic images of the\nbird, and does not follow the prompt correctly, however adding a shard corresponding to birds (SD2.1 FT CUB), with appropriate mixture\nweights enables the CDMs to generalize OOD, and produce more realistic birds (eg (w0, w1) = (0.5, 0.5) or (w0, w1) = (0.25, 0.75).\nCDMs have the flexibility to add diffusion score from different data sources to improve OOD generalization. Bottom (B): CDMs can\nalso help in reducing memorization. Figure (B) shows that SD2.1 fine-tuned (FT) on AE6+ often memorizes training data (Carlini et al.,\n2023), eg Big Ben tower (w0, w1) = (1, 0), which can often violate the privacy rights of the user data. We can prevent memorization\nin diffusion models by using a mixture of models like in CDMs, which will merge diffusion flows from multiple sources preventing\ngeneration of memorized training samples at inference (eg (w0, w1) = (0.5, 0.5) or (w0, w1) = (0.25, 0.75)). We show that CDMs also\naid improving the diversity of the generated images, while preserving the inherent concept presented in the input prompt.\nparameters, and inference compute can be reduced by the\nuse adapters at the expense of model performance, however,\nit cannot be completely eliminated. Random selection of\nscores in CDMs provide an efficient way to reduce the\ncompute requirements. Application of CDMs is simplified\nin situations when the data is naturally partitioned by the\nuser privacy rights, however, in other situations sharding the\ndata in a manner which preserves the synergistic information\nafter compartmentalization is challenging (one can always\nsplit uniformly).\n6. Conclusion\nData protection is an increasingly arduous task as the vol-\nume of training data needed to train massive AI models in-\ncreases. While techniques to manage privacy and attribution\nhave been demonstrated for a variety of model architectures,\nmostly at relatively small scale, up to now it was not possi-\nble to directly apply them to Diffusion Models. We present\nthe first method to compose such models in a private manner,\nand illustrate its use in selective forgetting, continual learn-\ning, out of distribution coverage, reducing memorization,\ncredit attribution, and improving alignment. We show that\nwe can train compartmentalized diffusion models for deep\nnetworks (or adapters) to model different data distributions,\nand perform comparable (or even better) to a model trained\non the joint distribution. CDMs also provide a natural way\nfor customized model inference (\u2018a-la-carte) (Bowman et al.,\n2023) which enables user to arbitrarily choose a subset of\nshards at inference time, provides a way for copyright pro-\ntected generation (Vyas et al., 2023), and encourage explor-\ning differentially private adapter tuning for diffusion models.\nIncreasing the number of shards for CDMs in the limit will\nlead to retrieval augmented diffusion models, which fur-\nther helps in privacy protected generation as samples can\nbe easily removed (unlearning) or added on the fly, while\nproviding credit attribution, and more synthesis control with\nthe retrieved samples.\n8\nTraining Data Protection with Compositional Diffusion Models\n7. Impact Statements\nThis paper presents work whose goal is to provide a novel\nmethod for training diffusion models through compartmen-\ntalization with several privacy benefits. If applied at scale,\nthis has a lot of societal consequences, for instance, it will\nallow diffusion model user to make unlearning request with-\nout requiring to discard the entire model, provide subset\nattribution to users, and prevent sampling of memorized\ntraining data, thus promoting safe use of diffusion models\nand build user trust in AI.\nReferences\nAlessandro Achille, Aditya Golatkar, Avinash Ravichandran,\nMarzia Polito, and Stefano Soatto.\nLqf: Linear quadratic\nfine-tuning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 15729\u201315739,\n2021.\nBrian DO Anderson. Reverse-time diffusion equation models.\nStochastic Processes and their Applications, 12(3):313\u2013326,\n1982.\nFan Bao, Chongxuan Li, Yue Cao, and Jun Zhu. All are worth\nwords: a vit backbone for score-based diffusion models. arXiv\npreprint arXiv:2209.12152, 2022.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-\nChoo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie,\nand Nicolas Papernot. Machine unlearning. In 2021 IEEE\nSymposium on Security and Privacy (SP), pages 141\u2013159. IEEE,\n2021.\nBenjamin Bowman, Alessandro Achille, Luca Zancato, Matthew\nTrager, Pramuditha Perera, Giovanni Paolini, and Stefano Soatto.\n\\a-la-carte prompt tuning (apt): Combining distinct data via\ncomposable prompting. arXiv preprint arXiv:2302.07994, 2023.\nNicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski,\nVikash Sehwag, Florian Tramer, Borja Balle, Daphne Ippolito,\nand Eric Wallace. Extracting training data from diffusion mod-\nels. arXiv preprint arXiv:2301.13188, 2023.\nRishav Chourasia, Neil Shah, and Reza Shokri. Forget unlearning:\nTowards true data-deletion in machine learning. arXiv preprint\narXiv:2210.08911, 2022.\nC.Wah, S.Branson, P.Welinder, P.Perona, and S.Belongie. Tech-\nnical report cns-tr-2011-001, california institute of technology.\nCaltech, 2011.\nXiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui\nWang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang,\nAbhimanyu Dubey, et al. Emu: Enhancing image generation\nmodels using photogenic needles in a haystack. arXiv preprint\narXiv:2309.15807, 2023.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat\ngans on image synthesis.\nAdvances in Neural Information\nProcessing Systems, 34:8780\u20138794, 2021.\nTim Dockhorn, Tianshi Cao, Arash Vahdat, and Karsten Kreis.\nDifferentially private diffusion models.\narXiv preprint\narXiv:2210.09929, 2022.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929, 2020.\nYilun Du, Conor Durkan, Robin Strudel, Joshua B Tenenbaum,\nSander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud\nDoucet, and Will Sussman Grathwohl. Reduce, reuse, recycle:\nCompositional generation with energy-based diffusion models\nand mcmc. In International Conference on Machine Learning,\npages 8489\u20138510. PMLR, 2023.\nYonatan Dukler, Benjamin Bowman, Alessandro Achille, Aditya\nGolatkar, Ashwin Swaminathan, and Stefano Soatto.\nSafe:\nMachine unlearning with shard graphs.\narXiv preprint\narXiv:2304.13169, 2023.\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of\ndifferential privacy. Foundations and Trends\u00ae in Theoretical\nComputer Science, 9(3\u20134):211\u2013407, 2014.\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and\nDavid Bau. Erasing concepts from diffusion models. arXiv\npreprint arXiv:2303.07345, 2023.\nSahra Ghalebikesabi, Leonard Berrada, Sven Gowal, Ira Ktena,\nRobert Stanforth, Jamie Hayes, Soham De, Samuel L Smith,\nOlivia Wiles, and Borja Balle.\nDifferentially private diffu-\nsion models generate useful synthetic images. arXiv preprint\narXiv:2302.13861, 2023.\nAntonio Ginart, Melody Guan, Gregory Valiant, and James Y\nZou. Making ai forget you: Data deletion in machine learning.\nAdvances in neural information processing systems, 32, 2019.\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eter-\nnal sunshine of the spotless net: Selective forgetting in deep\nnetworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2020a.\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Forget-\nting outside the box: Scrubbing deep networks of information\naccessible from input-output observations. In European Confer-\nence on Computer Vision, pages 383\u2013398. Springer, 2020b.\nAditya Golatkar, Alessandro Achille, Avinash Ravichandran,\nMarzia Polito, and Stefano Soatto. Mixed-privacy forgetting in\ndeep networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n792\u2013801, 2021.\nAditya Golatkar, Alessandro Achille, Yu-Xiang Wang, Aaron Roth,\nMichael Kearns, and Stefano Soatto. Mixed differential privacy\nin computer vision. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8376\u20138386, 2022.\nChuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van\nDer Maaten. Certified data removal from machine learning\nmodels. arXiv preprint arXiv:1911.03030, 2019.\nVarun Gupta, Christopher Jung, Seth Neel, Aaron Roth, Saeed\nSharifi-Malvajerdi, and Chris Waites. Adaptive machine un-\nlearning. Advances in Neural Information Processing Systems,\n34:16319\u201316330, 2021.\n9\nTraining Data Protection with Compositional Diffusion Models\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion\nprobabilistic models. Advances in Neural Information Process-\ning Systems, 33:6840\u20136851, 2020.\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi\nGao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mo-\nhammad Norouzi, David J Fleet, et al. Imagen video: High def-\ninition video generation with diffusion models. arXiv preprint\narXiv:2210.02303, 2022.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021.\nYushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Osten-\ndorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and\ninterpretable text-to-image faithfulness evaluation with question\nanswering. arXiv preprint arXiv:2303.11897, 2023.\nChi-Pin Huang, Kai-Po Chang, Chung-Ting Tsai, Yung-Hsuan Lai,\nand Yu-Chiang Frank Wang. Receler: Reliable concept erasing\nof text-to-image diffusion models via lightweight erasers. arXiv\npreprint arXiv:2311.17717, 2023.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge\nBelongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt\ntuning. In Computer Vision\u2013ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart XXXIII, pages 709\u2013727. Springer, 2022.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Eluci-\ndating the design space of diffusion-based generative models.\narXiv preprint arXiv:2206.00364, 2022.\nKorbinian Koch and Marcus Soll. No matter how you slice it:\nMachine unlearning with sisa comes at the expense of minority\nclasses. In 2023 IEEE Conference on Secure and Trustworthy\nMachine Learning (SaTML), pages 622\u2013637. IEEE, 2023a.\nKorbinian Koch and Marcus Soll. No matter how you slice it:\nMachine unlearning with sisa comes at the expense of minority\nclasses. In First IEEE Conference on Secure and Trustworthy\nMachine Learning, 2023b.\nXianghao Kong,\nRob Brekelmans,\nand Greg Ver Steeg.\nInformation-theoretic\ndiffusion.\narXiv\npreprint\narXiv:2302.03792, 2023.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d\nobject representations for fine-grained categorization. In Pro-\nceedings of the IEEE international conference on computer\nvision workshops, pages 554\u2013561, 2013.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers\nof features from tiny images. Toronto, ON, Canada, 2009.\nVinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, and Dan\nRoth. Privacy adhering machine un-learning in nlp. arXiv\npreprint arXiv:2212.09573, 2022.\nNupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman,\nRichard Zhang, and Jun-Yan Zhu. Ablating concepts in text-\nto-image diffusion models. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 22691\u2013\n22702, 2023.\nYann LeCun, Corinna Cortes, and Chris Burges. Mnist handwritten\ndigit database. Dataset, 2010.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro\nPerona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zit-\nnick. Microsoft coco: Common objects in context. In Com-\nputer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13,\npages 740\u2013755. Springer, 2014.\nAnders Lindquist and Giorgio Picci. On the stochastic realization\nproblem. SIAM Journal on Control and Optimization, 17(3):\n365\u2013389, 1979.\nYaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian\nNickel, and Matt Le. Flow matching for generative modeling.\narXiv preprint arXiv:2210.02747, 2022.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B\nTenenbaum. Compositional visual generation with composable\ndiffusion models. In European Conference on Computer Vision,\npages 423\u2013439. Springer, 2022.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li,\nand Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022a.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and\nJun Zhu. Dpm-solver++: Fast solver for guided sampling of dif-\nfusion probabilistic models. arXiv preprint arXiv:2211.01095,\n2022b.\nEyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav Acha,\nYossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid Hoshen.\nDreamix: Video diffusion models are general video editors.\narXiv preprint arXiv:2302.01329, 2023.\nSeth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-\nto-delete: Gradient-based methods for machine unlearning. In\nAlgorithmic Learning Theory, pages 931\u2013962. PMLR, 2021.\nEdward Nelson. Dynamical theories of Brownian motion. Prince-\nton university press, 1967.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV\nJawahar. Cats and dogs. In 2012 IEEE conference on computer\nvision and pattern recognition, pages 3498\u20133505. IEEE, 2012.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International\nconference on machine learning, pages 8748\u20138763. PMLR,\n2021.\nSamyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang,\nReza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley,\nand Yuxiong He.\nDeepspeed-moe: Advancing mixture-of-\nexperts inference and training to power next-generation ai scale.\nIn International Conference on Machine Learning, pages 18332\u2013\n18346. PMLR, 2022.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and\nMark Chen. Hierarchical text-conditional image generation\nwith clip latents. arXiv preprint arXiv:2204.06125, 2022.\n10\nTraining Data Protection with Compositional Diffusion Models\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00f6rn Ommer. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages\n10684\u201310695, 2022.\nAyush Sekhari,\nJayadev Acharya,\nGautam Kamath,\nand\nAnanda Theertha Suresh. Remember what you want to for-\nget: Algorithms for machine unlearning. Advances in Neural\nInformation Processing Systems, 34:18075\u201318086, 2021.\nKihyuk Sohn, Huiwen Chang, Jos\u00e9 Lezama, Luisa Polania, Han\nZhang, Yuan Hao, Irfan Essa, and Lu Jiang. Visual prompt\ntuning for generative transfer learning. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pages 19840\u201319851, 2023.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502,\n2020a.\nYang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019.\nYang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced\nscore matching: A scalable approach to density and score esti-\nmation. In Uncertainty in Artificial Intelligence, pages 574\u2013584.\nPMLR, 2020b.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek\nKumar, Stefano Ermon, and Ben Poole. Score-based genera-\ntive modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020c.\nEnayat Ullah, Tung Mai, Anup Rao, Ryan A Rossi, and Raman\nArora. Machine unlearning via algorithmic stability. In Confer-\nence on Learning Theory, pages 4126\u20134142. PMLR, 2021.\nNikhil Vyas, Sham Kakade, and Boaz Barak. Provable copy-\nright protection for generative models.\narXiv preprint\narXiv:2302.10870, 2023.\nRuichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu,\nand Xiaodong Lin.\nCompositional text-to-image synthesis\nwith attention map control of diffusion models. arXiv preprint\narXiv:2305.13921, 2023.\nJing Wu, Trung Le, Munawar Hayat, and Mehrtash Harandi.\nErasediff: Erasing data influence in diffusion models. arXiv\npreprint arXiv:2401.05779, 2024.\nZeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan\nZong, Yu Liu, and Ping Luo. Raphael: Text-to-image gen-\neration via large mixture of diffusion paths. arXiv preprint\narXiv:2305.18295, 2023.\nHaonan Yan, Xiaoguang Li, Ziyao Guo, Hui Li, Fenghua Li, and\nXiaodong Lin.\nArcane: An efficient architecture for exact\nmachine unlearning. In Proceedings of the Thirty-First Inter-\nnational Joint Conference on Artificial Intelligence, IJCAI-22,\npages 4006\u20134013, 2022.\nSihao Yu, Fei Sun, Jiafeng Guo, Ruqing Zhang, and Xueqi Cheng.\nLegonet: A fast and exact unlearning architecture.\narXiv\npreprint arXiv:2210.16023, 2022.\n11\nTraining Data Protection with Compositional Diffusion Models\nAppendix\nA. Proof of proposition 3.1\nProof.\n\u2207xt log pt(xt) = \u2207xt log\n\u0010 n\nX\ni=1\n\u03bbip(i)\nt (xt)\n\u0011\n=\n1\nPn\ni=1 \u03bbip(i)\nt (xt)\nn\nX\ni=1\n\u03bbi\u2207xtp(i)\nt (xt)\n=\n1\npt(xt)\nn\nX\ni=1\n\u03bbi\u2207xtp(i)\nt (xt)p(i)\nt (xt)\np(i)\nt (xt)\n=\nn\nX\ni=1\n\u03bbi\np(i)\nt (xt)\npt(xt) \u2207xt log p(i)\nt (xt)\n(7)\nB. Additional Discussion\nHow is CDMs method different from the existing works on compositional diffusion models like (Du et al., 2023; Liu\net al., 2022; Wang et al., 2023)? Existing works on compositional generation with diffusion models are aimed at improving\nthe text to image alignment of the models, and attempt to improve the composition of different objects in the scene. This is\nachieved by either manipulating the cross-attention layers (Wang et al., 2023) of the model architecture to re-weight the\nattention matrix weights to increase the contribution of objects or nouns in the input prompt, or by sampling flows for each\nnoun (or keywords) separately and combining them for better composition (Du et al., 2023; Liu et al., 2022). On the other\nhand CDMs compartmentalize the training data to improve the privacy guarantees of the model. This involves controlled\nisolation of information about the data into different parameters (adapters), and composing them arbitrarily at inference\ntime based on the users access rights. One major benefit of this approach is the ability to unlearn subsets of data efficiently\ncompared to a monolithic model. Given a forgetting request by a user, CDMs can unlearn a subset by simply re-training the\nshard (from the last checkpoint before those samples were seen) to ensure perfect unlearning.\nIs CDMs the same as MoE(Xue et al., 2023; Rajbhandari et al., 2022)? MoE aims at increasing the model size (hence\nperformance) while keeping the inference cost constant (or low). This involves training MoE layers with dynamic routing\nto select a subset of parameters at inference based on the input. At the core of MoE lies the MoE layer which has a set\nof shared parameters, and a set of disjoint expert parameters. The shared parameters are updated with gradients from the\ncomplete dataset, while the experts are updated based on subsets of data based on the routing scheme (which can be data\ndependent). This leads to mixing of information between different subsets of data into the parameters of the model making\nit equivalent to a monolithic model from an unlearning standpoint. Furthermore, if the routing scheme is data-dependent i.e.\nit was learned using the complete data, then each unlearning request will require the re-training the routing scheme. CDMs\nsimplify this problem, by training different parameters for different subsets of data, and moreover provide a closed form\nsolution for the output score function as a combination of the individual scores.\nIs CDMs different from a monolithic model if the unlearning request necessitates re-training of all the shards? One\nof the important problems in machine unlearning is \u2013 how to shard the training data \u2013 whether it should be split uniformly\nat random or based on certain properties or statistic. Sharding plays critical role in determining the computational cost of\nunlearning given a forgetting request. (Koch and Soll, 2023a) proposes to identify subsets of data which high likelihood of\nforgetting, and keep them in a separate shard or use it to update the weights toward the end of training. (Dukler et al., 2023)\nfurther builds on it a proposed to construct a shard graph where each node is small subset of data, and the edges correspond\nto different data owner rights. For instance, data should be clustered according to the source, such that once a source decides\nto withdraw the remaining shards are unaffected. Similarly different sources may have different access rights with respect to\nother sources which can be captured in a graph, and the cliques can be used to train different models. In our experiments we\nattempt to mimic this by sharding the fine-grained data based on its class label where each class is a different source, and\nMSCOCO based on its quality as we may want to localize images based on the quality to identify its value. The aim of an\n12\nTraining Data Protection with Compositional Diffusion Models\nideal system enabling unlearning should be to device a sharding mechanism which will reduce the number of shards to\nbe re-trained given an unlearning request on an average given a series of requests. In the worst case setting re-training of\nall the shards may be warranted to ensure unlearning (in which case its equal to monolithic), however, in the average case\ncompartmentalization triumps a monolithic model (for eg. 8\u00d7 computational improvement for CDMs in our experiments).\nWhat is the relation between CDMs and copy protection(Vyas et al., 2023)? Copy protection has recently received\nincreased attraction with the burst of generative models. (Vyas et al., 2023) proposed a definition of copy protection by\nmeasure the different of a generative model to a safe model which was not trained on protected data. They also propose the\nCP\u2212\u2206 algorithm (Algorithm-3) for copy protected generation which involves splitting the training data into two shards,\ntraining separate models on each and sampling from the product of the distribution. While sampling from the product of\ndistribution is easy for language models, its non-trivial for diffusion models. However, we can circumvent this issue for\ndiffusion using CDMs by simply computing the score of the copy protected distribution from Algorithm-3 (Vyas et al., 2023).\nGiven two models p1 and p2 trained on disjoint subsets of data, (Vyas et al., 2023) proposes to sample from\np\np1(x)p2(x)/Z.\nComputing the score of distribution we obtain 1\n2(\u2207x log p1(x) + \u2207x log p2(x)), where \u2207x log p1(x), \u2207x log p2(x) is often\napproximated in practice by trained diffusion models. Looking at this equation carefully \u2013 we realize that it is equivalent\nto CDMs when we use naive-averaging instead of the classifier (i.e. equal weights for all the model). This is depicted\npictorially in fig. 4.\nCan CDMs provide/quantify sample attribution? While providing attribution to a single sample in the training set during\ninference in extremely difficult (due to the non-convexity and non-linearity of diffusion models) CDMs can provide and\nquantify sample attribution using eq. (6). Source attribution improves the explainability and transparency of diffusion models\nas they provide users with the ability to localize the contribution training samples to a source, which can further be used to\nreduce bias and improve fairness of the model. In practice, the source attribution can be efficiently computed using the\nmethod proposed in (Kong et al., 2023).\nC. Implementation Details\nWe use latent diffusion models for all the experiments in the paper. Latent diffusion models decrease the computational\ncomplexity of the model while preventing decrease in quality. We use Stable diffusion diffusion models for experiments\non text-to-image generation and transformer based diffusion models for all other experiments performed in the paper. We\nuse the Huggingface based implementation of Stable diffusion and for transformer based experiments we use the U-ViT\narchitecture proposed in (Bao et al., 2022), whose code can be publicly found here: https://github.com/baofff/U-ViT.\nTo reduce the computational complexity of the compartmentalized models at inference, we train adapters (prompts or LoRA)\nto learn the data distribution. For transformer based experiments we use an ImageNet pretrained class-conditional model as\nthe backbone and learn prompts (adapters) to model the data distribution. We use the Adam optimizer with a learning rate of\n0.1 for transformer based experiments and 1e-4 for Stable diffusion based experiments. We do not use weight decay while\nfine-tuning and use a batch-size of 256 for all experiments.\nFor unconditional image generation we train one set of deep prompts (deep prompts: prompts appended to all the layers of a\nthe network, see (Jia et al., 2022)) for the entire distribution. We use a prompt length of 256 tokens, and train for 800 epochs.\nFor class-conditional image generation, we train one set of deep prompts for each class. Since we train class-wise prompts,\nwe use a prompt length of 8 tokens, and again train for 800 epochs. For text-to-image generation, we fine-tune the model for\naround 15000 steps.\nFor training the classifier, we use k-NN based classifier for Stable diffusion model, and model based classifier for transformer\nbased experiments. For k-NN based classifier we use CLIP features to generate k-mean embeddings for each shard of\nthe data. During inference, at each time-step, we compute x0 from each xt, and then compute the normalized (soft-max)\ndistance to each shard.\nD. Additional Figures\n13\nTraining Data Protection with Compositional Diffusion Models\nDi\ufb00usion model composition with our method: Birds + Cars\nDi\ufb00usion model composition with naive averaging: Birds + Cars\nFigure 5. Diffusion model composition with our method vs naive averaging\n14\nTraining Data Protection with Compositional Diffusion Models\nUnconditional Pets generation\nFigure 6. Unconditional image generation with a prompt (adapter) based model trained on OxfordPets\nUnconditional Birds generation\nFigure 7. Unconditional image generation with a prompt (adapter) based model trained on CUB200\n15\n"
  }
]