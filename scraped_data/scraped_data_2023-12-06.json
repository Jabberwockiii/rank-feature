[
  {
    "title": "Analyzing and Improving the Training Dynamics of Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.02696.pdf",
    "upvote": "30",
    "text": "Analyzing and Improving the Training Dynamics of Diffusion Models\nTero Karras\nNVIDIA\nMiika Aittala\nNVIDIA\nJaakko Lehtinen\nNVIDIA, Aalto University\nJanne Hellsten\nNVIDIA\nTimo Aila\nNVIDIA\nSamuli Laine\nNVIDIA\nAbstract\nDiffusion models currently dominate the field of data-\ndriven image synthesis with their unparalleled scaling to\nlarge datasets. In this paper, we identify and rectify several\ncauses for uneven and ineffective training in the popular\nADM diffusion model architecture, without altering its high-\nlevel structure. Observing uncontrolled magnitude changes\nand imbalances in both the network activations and weights\nover the course of training, we redesign the network layers\nto preserve activation, weight, and update magnitudes on ex-\npectation. We find that systematic application of this philoso-\nphy eliminates the observed drifts and imbalances, resulting\nin considerably better networks at equal computational com-\nplexity. Our modifications improve the previous record FID\nof 2.41 in ImageNet-512 synthesis to 1.81, achieved using\nfast deterministic sampling.\nAs an independent contribution, we present a method for\nsetting the exponential moving average (EMA) parameters\npost-hoc, i.e., after completing the training run. This allows\nprecise tuning of EMA length without the cost of performing\nseveral training runs, and reveals its surprising interactions\nwith network architecture, training time, and guidance.\n1. Introduction\nHigh-quality image synthesis based on text prompts, ex-\nample images, or other forms of input has become widely\npopular thanks to advances in denoising diffusion mod-\nels [22, 52, 71\u201374, 81]. Diffusion-based approaches pro-\nduce high-quality images while offering versatile controls\n[9, 18, 21, 50, 88] and convenient ways to introduce novel\nsubjects [13, 65], and they also extend to other modalities\nsuch as audio [41, 58], video [6, 23, 25], and 3D shapes\n[46, 57, 60, 70]. A recent survey of methods and applica-\ntions is given by Yang et al. [83].\nOn a high level, diffusion models convert an image of\npure noise to a novel generated image through repeated\napplication of image denoising. Mathematically, each de-\n50\n100\n200\n500\n1000\n2000\n2\n3\n5\n10\n20\nFID\nADM\nADM\nADM-U\nADM-U\nDiT-XL/2\nDiT-XL/2\nRIN\nU-ViT, L\nVDM++\nVDM++\nStyleGAN-XL\nXS\nS\nM\nL\nXL XXL\nXS\nS\nM\nL\nXL XXL\nModel complexity (gigaflops per evaluation), ImageNet-512\nPrevious, no guidance\nPrevious, with guidance\nOurs, no guidance\nOurs, with guidance\nFigure 1. Our contributions significantly improve the quality of\nresults w.r.t. model complexity, surpassing the previous state-of-the-\nart with a 5\u00d7 smaller model. In this plot, we use gigaflops per single\nmodel evaluation as a measure of a model\u2019s intrinsic computational\ncomplexity; a similar advantage holds in terms of parameter count,\nas well as training and sampling cost (see Appendix A).\nnoising step can be understood through the lens of score\nmatching [28], and it is typically implemented using a U-Net\n[22, 64] equipped with self-attention [80] layers. Since we\ndo not contribute to the theory behind diffusion models, we\nrefer the interested reader to the seminal works of Sohl-\nDickstein et al. [71], Song and Ermon [73], and Ho et al.\n[22], as well as to Karras et al. [36], who frame various\nmathematical frameworks in a common context.\nDespite the seemingly frictionless scaling to very large\ndatasets and models, the training dynamics of diffusion mod-\nels remain challenging due to the highly stochastic loss func-\ntion. The final image quality is dictated by faint image\ndetails predicted throughout the sampling chain, and small\nmistakes at intermediate steps can have snowball effects in\nsubsequent iterations. The network must accurately estimate\nthe average clean image across a vast range of noise levels,\nGaussian noise realizations, and conditioning inputs. Learn-\n1\narXiv:2312.02696v1  [cs.CV]  5 Dec 2023\ning to do so is difficult given the chaotic training signal that\nis randomized over all of these aspects.\nTo learn efficiently in such a noisy training environment,\nthe network should ideally have a predictable and even re-\nsponse to parameter updates. We argue that this ideal is not\nmet in current state-of-the-art designs, hurting the quality\nof the models and making it difficult to improve them due\nto complex interactions between hyperparameters, network\ndesign, and training setups.\nOur overarching goal is to understand the sometimes\nsubtle ways in which the training dynamics of the score net-\nwork can become imbalanced by unintended phenomena,\nand to remove these effects one by one. At the heart of\nour approach are the expected magnitudes of weights, ac-\ntivations, gradients, and weight updates, all of which have\nbeen identified as important factors in previous work (e.g.,\n[1, 3, 7, 8, 10, 40, 43, 44, 68, 85, 87]). Our approach is,\nroughly speaking, to standardize all magnitudes through a\nclean set of design choices that address their interdependen-\ncies in a unified manner.\nConcretely, we present a series of modifications to the\nADM [12] U-Net architecture without changing its overall\nstructure, and show considerable quality improvement along\nthe way (Section 2). The final network is a drop-in replace-\nment for ADM. It sets new record FIDs of 1.81 and 1.91 for\nImageNet-512 image synthesis with and without guidance,\nrespectively, where the previous state-of-the-art FIDs were\n2.41 and 2.99. It performs particularly well with respect\nto model complexity (Figure 1), and achieves these results\nusing fast deterministic sampling instead of the much slower\nstochastic sampling used in previous methods.\nAs an independent contribution, we present a method\nfor setting the exponential moving average (EMA) param-\neters post hoc, i.e., after the training run has completed\n(Section 3). Model averaging [29, 56, 66, 78, 84] is an\nindispensable technique in all high-quality image synthe-\nsis methods [2, 12, 24, 31, 33, 36, 52, 55, 63, 69, 72, 74].\nUnfortunately, the EMA decay constant is a cumbersome\nhyperparameter to tune because the effects of small changes\nbecome apparent only when the training is nearly converged.\nOur post-hoc EMA allows accurate and efficient reconstruc-\ntion of networks with arbitrary EMA profiles based on pre-\nintegrated weight snapshots stored during training. It also\nenables many kinds of exploration that have not been com-\nputationally feasible before (Section 3.3).\nWe will make our implementation and pre-trained models\npublicly available.\n2. Improving the training dynamics\nLet us now proceed to study and eliminate effects related\nto various imbalances in the training dynamics of a score\nnetwork. As our baseline, we take the ADM [12] network\nas implemented in the EDM [36] framework. The architec-\nTraining configurations, ImageNet-512\nFID \u2193 Mparams Gflops\nA EDM baseline\n8.00\n295.9\n110.4\nB + Minor improvements\n7.24\n291.8\n100.4\nC + Architectural streamlining\n6.96\n277.8\n100.3\nD + Magnitude-preserving learned layers\n3.75\n277.8\n101.2\nE + Control effective learning rate\n3.02\n277.8\n101.2\nF + Remove group normalizations\n2.71\n280.2\n102.1\nG + Magnitude-preserving fixed-function layers\n2.56\n280.2\n102.2\nTable 1. Effect of our changes evaluated on ImageNet-512. We\nreport Fr\u00e9chet inception distance (FID, lower is better) [19] without\nguidance, computed between 50,000 randomly generated images\nand the entire training set. Each number represents the minimum\nof three independent evaluations using the same model.\nture combines a U-Net [64] with self-attention [80] layers\n(Figure 2a,b), and its variants have been widely adopted in\nlarge-scale diffusion models, including Imagen [67], Sta-\nble Diffusion [63], eDiff-I [2], DALL-E 2 [53, 61], and\nDALL-E 3 [5]. Our training and sampling setups are based\non the EDM formulation with constant learning rate and 32\ndeterministic 2nd order sampling steps.\nWe use the class-conditional ImageNet [11] 512\u00d7512\ndataset for evaluation, and, like most high-resolution dif-\nfusion models, operate in the latent space of a pre-trained\ndecoder [63] that performs 8\u00d7 spatial upsampling. Thus,\nour output is 64\u00d764\u00d74 prior to decoding. During explo-\nration, we use a modestly sized network configuration with\napprox. 300M trainable parameters, with results for scaled-\nup networks presented later in Section 4. The training is\ndone for 2147M (= 231) images in batches of 2048, which\nis sufficient for these models to reach their optimal FID.\nWe will build our improved architecture and training pro-\ncedure in several steps. Our exposition focuses on funda-\nmental principles and the associated changes to the network.\nFor comprehensive details of each architectural step, along\nwith the related equations, see Appendix B.\nBaseline (CONFIG A).\nAs the original EDM configuration\nis targeted for RGB images, we increase the output channel\ncount to 4 and replace the training dataset with 64\u00d764\u00d74\nlatent representations of ImageNet-512 images, standardized\nglobally to zero mean and standard deviation \u03c3data = 0.5. In\nthis setup, we obtain a baseline FID of 8.00 (see Table 1).\n2.1. Preliminary changes\nImproved baseline (CONFIG B).\nWe first tune the hyper-\nparameters (learning rate, EMA length, training noise level\ndistribution, etc.) to optimize the performance of the baseline\nmodel. We also disable self-attention at 32\u00d732 resolution,\nsimilar to many prior works [22, 27, 52].\nWe then address a shortcoming in the original EDM train-\ning setup: While the loss weighting in EDM standardizes\nloss magnitude to 1.0 for all noise levels at initialization,\nthis situation no longer holds as the training progresses. The\n2\nFixed-function\nLearned\nNot always present\nLearned, forced\nweight norm.\n1\n1000\n192\n\u00d7\n1000\n768\nSkips\nEncoder\nDecoder\nIn\nOut\nEmbedding\nNoise\nlevel\n1\n1000\n192\n768\nTo encoder and \ndecoder blocks\nClass\nlabel\nSkip\nInput\nOutput\nRin\u00d7Cin\n768\nCout\nEmbedding\nRout\u00d7Cout\nOutput\nInput\nSkip\nRin\u00d7Cskip\nRin\u00d7Cin\n+\n\u00d7\nRin\u00d7(Cin+Cskip) \n768\n+1\nCout\nEmbedding\nEncoder block\nDecoder block\nEmbedding\nEncoder block\nDecoder block\nEmbedding\nCout\n768\nEmbedding\nOutput\nInput\nRin\u00d7Cskip\nRin\u00d7Cin\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip) \nSkip\nCout\n768\nEmbedding\nRout\u00d7Cout\nInput\nRin\u00d7Cin\nSkip\nOutput\nNoise\nlevel\nClass\nlabel\ncnoise\nPosEmb\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\nBias\nBias\n+\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\n+\n\u00d7\nSiLU\nDropout\nBias\nConv 3\u00d73\nGrpNorm\nSplit\nBias\nLinear\n+1\nConv 1\u00d71\nDown 2\u00d72\nBias\nDown 2\u00d72\n+\nConv 1\u00d71\nUp 2\u00d72\nConcat\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\nSiLU\nDropout\nBias\nConv 3\u00d73\nGrpNorm\nUp 2\u00d72\nBias\nLinear\nAttention\nBias\nSplit\ncnoise\nMP-Fourier\nMP-SiLU\nLinear\nLinear\nMP-Add\nLinear\nGain\nPixNorm\nMP-SiLU\nMP-SiLU\nConv 3\u00d73\nConv 3\u00d73\nMP-Add\nDown 2\u00d72\nConv 1\u00d71\nDropout\nLinear\nGain\nMP-SiLU\n\u00d7\nMP-SiLU\nConv 3\u00d73\nMP-Add\n+1\nConv 3\u00d73\n+1\n\u00d7\nUp 2\u00d72\nMP-Cat\nAttention\nConv 1\u00d71\nDropout\nRout\u00d7Cout\n768\nTo encoder and \ndecoder blocks\n768\nAttention\nAttention\n(a) Overall view (b) ADM architecture blocks by Dhariwal and Nichol [12] (CONFIG B)\n(c) Our magnitude-preserving (MP) variant (CONFIG G)\nFigure 2. The widely used ADM architecture [12] for image denoising is structured as a U-Net [64]. (a) The encoder blocks are connected\nto decoder blocks using skip connections, and an auxiliary embedding network conditions the U-Net with noise level and class label. (b) The\noriginal building blocks follow the pre-activation design of ResNets [16]. Residual blocks accumulate contributions to the main path (bold).\nExplicit normalizations in the residual paths try to keep magnitudes under control, but nothing prevents them from growing in the main path.\n(c) We update all of the operations (e.g., convolutions, activations, concatenation, summation) to maintain magnitudes on expectation.\nmagnitude of the gradient feedback then varies between\nnoise levels, re-weighting their relative contribution in an\nuncontrolled manner.\nTo counteract this effect, we adopt a continuous gener-\nalization of the multi-task loss proposed by Kendall et al.\n[37]. Effectively, we track the raw loss value as a function of\nthe noise level, and scale the training loss by its reciprocal.\nTogether, these changes decrease the FID from 8.00 to 7.24.\nArchitectural streamlining (CONFIG C).\nTo facilitate the\nanalysis of training dynamics, we proceed to streamline\nand stabilize the architecture. To avoid having to deal with\nmultiple different types of trainable parameters, we remove\nthe additive biases from all convolutional and linear layers,\nas well as from the conditioning pathway. To restore the\ncapability of the network to offset the data, we concatenate\nan additional channel of constant 1 to the network\u2019s input.\nWe further unify the initialization of all weights using He\u2019s\nuniform init [15], switch from ADM\u2019s original positional\nencoding scheme to the more standard Fourier features [77],\nand simplify the group normalization layers by removing\ntheir mean subtraction and learned scaling.\nFinally, we observe that the attention maps often end up\nin a brittle and spiky configuration due to magnitude growth\nof the key and query vectors over the course of training.\nWe rectify this by switching to cosine attention [14, 48, 51]\nthat normalizes the vectors prior to computing the dot prod-\nucts. As a practical benefit, this allows using 16-bit floating\npoint math throughout the network, improving efficiency.\nTogether, these changes reduce the FID from 7.24 to 6.96.\n2.2. Standardizing activation magnitudes\nWith the architecture simplified, we now turn to fixing the\nfirst problem in training dynamics: activation magnitudes.\nAs illustrated in the first row of Figure 3, the activation\nmagnitudes grow uncontrollably in CONFIG C as training\nprogresses, despite the use of group normalizations within\neach block. Notably, the growth shows no signs of tapering\noff or stabilizing towards the end of the training run.\nUpon a closer look at the architecture diagram in Fig-\nure 2b, the growth is perhaps not too surprising: Due to\nthe residual structure of encoder, decoder, and self-attention\nblocks, ADM networks contain long signal paths without any\nnormalizations. These paths accumulate contributions from\nresidual branches and can amplify their activations through\nrepeated convolutions. We hypothesize that this unabated\ngrowth of activation magnitudes is detrimental to training\nby keeping the network in a perpetually unconverged and\nunoptimal state.\nWe tried introducing group normalization layers to the\nmain path as well, but this caused a significant deterioration\nof result quality. This may be related to previous findings\nregarding StyleGAN [33], where the network\u2019s capabilities\nwere impaired by excessive normalization, to the extent that\nthe layers learned to bypass it via contrived image artifacts.\nInspired by the solutions adopted in StyleGAN2 [34] and\nother works that have sought alternatives to explicit normal-\nization [1, 7, 40], we choose to modify the network so that\nindividual layers and pathways preserve the activation mag-\nnitudes on expectation, with the goal of removing or at least\nreducing the need for data-dependent normalization.\n3\nMagnitude-preserving learned layers (CONFIG D).\nTo\npreserve expected activation magnitudes, we divide the out-\nput of each layer by the expected scaling of activation magni-\ntudes caused by that layer without looking at the activations\nthemselves. We first apply this scheme to all learned layers,\ni.e., convolutions and fully-connected layers, in every part\nof the model.\nGiven that we seek a scheme that is agnostic to the ac-\ntual content of the incoming activations, we have to make\nsome statistical assumptions about them. For simplicity, we\nwill assume that the pixels and feature maps are mutually\nuncorrelated and of equal standard deviation \u03c3act. Both fully\nconnected and convolutional layers can be thought of as con-\nsisting of stacked units, one per output channel. Each unit\neffectively applies a dot product of a weight vector wi \u2208 Rn\non some subset of the input activations to produce each out-\nput element. Under our assumptions, the standard deviation\nof the output features of the ith channel becomes \u2225wi\u22252 \u03c3act.\nTo restore the input activation magnitude, we thus introduce\na channel-wise division by \u2225wi\u22252.\nWe can equally well think of the scalar division as apply-\ning to wi itself. As long as gradients are propagated through\nthe computation of the norm, this scheme is equivalent to\nweight normalization [68] without the learned output scale;\nwe will use this term hereafter. As the overall weight magni-\ntudes no longer have an effect on activations, we simplify the\ninitialization by drawing all weights from the unit Gaussian\ndistribution.\nThis modification removes any direct means the network\nhas for learning to change the overall activation magnitudes,\nand as shown in Figure 3 (CONFIG D), the magnitude drift is\nsuccessfully eliminated. The FID also improves significantly,\nfrom 6.96 to 3.75.\n2.3. Standardizing weights and updates\nWith activations standardized, we turn our attention to net-\nwork weights and learning rate. As seen in Figure 3, there is\na clear tendency of network weights to grow in CONFIG D,\neven more so than in CONFIG C. The mechanism causing\nthis is well known [68]: Normalization of weights before use\nforces loss gradients to be perpendicular to the weight vector,\nand taking a step along this direction always lands on a point\nfurther away from the origin. Even with gradient magnitudes\nstandardized by the Adam optimizer, the net effect is that\nthe effective learning rate, i.e., the relative size of the update\nto network weights, decays as the training progresses.\nWhile it has been suggested that this decay of effective\nlearning rate is a desirable effect [68], we argue for explicit\ncontrol over it rather than having it drift uncontrollably and\nunequally between layers. Hence, we treat this as another\nimbalance in training dynamics that we seek to remedy. Note\nthat initializing all weights to unit Gaussian ensures uniform\neffective learning rate at initialization, but not afterwards.\nConfig C\n0\n200\n400\n600\nActivations\n0\n0.5\n1.0\n1.5\nWeights\nConfig D\n0\n5\n10\n15\n0\n10\n20\n30\nConfig E\nGimg = 0.5\n1.0\n1.5\n0\n5\n10\n15\nGimg = 0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nEnc-64x64\nDec-64x64\nEnc-32x32\nDec-32x32\nEnc-16x16\nDec-16x16\nEnc-8x8\nDec-8x8\nFigure 3. Training-time evolution of activation and weight mag-\nnitudes over different depths of the network; see Appendix A for\nfurther details. Top: In CONFIG C, the magnitudes of both acti-\nvations and weights grow without bound over training. Middle:\nThe magnitude-preserving design introduced in CONFIG D curbs\nactivation magnitude growth, but leads to even starker growth in\nweights. Bottom: The forced weight normalization in CONFIG E\nensures that both activations and weights remain bounded.\nControlling effective learning rate (CONFIG E).\nWe pro-\npose to address the weight growth with forced weight normal-\nization, where we explicitly normalize every weight vector\nwi to unit variance before each training step. Importantly,\nwe still apply the \u201cstandard\u201d weight normalization on top of\nthis during training, i.e., normalize the weight vectors upon\nuse. This has the effect of projecting the training gradients\nonto the tangent plane of the now unit-magnitude hyper-\nsphere where wi lies. This ensures that Adam\u2019s variance\nestimates are computed for the actual tangent plane steps\nand are not corrupted by the to-be erased normal component\nof the gradient vector. With both weight and gradient magni-\ntudes now equalized across the network, we have unified the\neffective learning rate as well. Assuming no correlation be-\ntween weights and gradients, each Adam step now replaces\nan approximately fixed proportion of the weights with the\ngradients. Some optimizers [3, 85] explicitly implement a\nsimilar effect by data-dependent re-scaling of the gradient.\nWe now have direct control over the effective learn-\ning rate. We find that a constant learning rate does not\ninduce convergence anymore, and we therefore introduce\nan inverse square root learning rate decay schedule as ad-\nvocated by Kingma and Ba [39]. Concretely, we define\n\u03b1(t) = \u03b1ref/\np\nmax(t/tref, 1), where t is the current train-\ning iteration and \u03b1ref and tref are hyperparameters (see Ap-\npendix D for implementation details). As shown in Figure 3,\nthe resulting CONFIG E successfully preserves both activa-\ntion and weight magnitudes throughout the training. As a\nresult, the FID improves from 3.75 to 3.02.\n4\n2.4. Removing group normalizations (CONFIG F)\nWith activation, weight, and update magnitudes under con-\ntrol, we are now ready to remove the data-dependent group\nnormalization layers that operate across pixels with poten-\ntially detrimental results [34]. Although the network trains\nsuccessfully without any normalization layers, we find that\nthere is still a small benefit from introducing much weaker\npixel normalization [32] layers to the encoder main path.\nOur hypothesis is that pixel normalization helps by coun-\nteracting correlations that violate the statistical assumptions\nbehind our standardization efforts in CONFIG D. We thus\nremove all group normalization layers and replace them with\n1/4 as many pixel normalization layers. We also remove\nthe second linear layer from the embedding network and\nthe nonlinearity from the network output, and combine the\nresampling operations in the residual blocks onto the main\npath. The FID improves from 3.02 to 2.71.\n2.5. Magnitude-preserving fixed-function layers\n(CONFIG G)\nFor the sake of completeness, we note that the network still\nhas layers that do not preserve activation magnitudes. First,\nthe sine and cosine functions of the Fourier features do not\nhave unit variance, which we rectify by scaling them up\nby\n\u221a\n2. Second, the SiLU [17] nonlinearities attenuate the\nexpected unit-variance distribution of activations unless this\nis compensated for. Accordingly, we modify them to divide\nthe output by Ex\u223cN (0,1)[ silu(x)2 ]1/2 \u2248 0.596.\nThird, we consider instances where two network branches\njoin, either through addition or concatenation. In previous\nconfigurations, the contribution from each branch to the out-\nput depended on uncontrolled activation magnitudes. By\nnow we can expect these to be standardized, and thus the bal-\nance between the branches is exposed as a meaningfully con-\ntrollable parameter [8]. We switch the addition operations to\nweighted sums, and observe experimentally that a fixed resid-\nual path weight of 30% worked best in encoder and decoder\nblocks, and 50% in the embedding. We divide the output by\nthe expected standard deviation of this weighted sum.\nThe concatenation of the U-Net skips in the decoder is\nalready magnitude-preserving, as we can expect similar mag-\nnitudes from both branches. However, the relative contribu-\ntion of the two inputs in subsequent layers is proportional\nto their respective channel counts, which we consider to be\nan unwanted and unintuitive dependence between encoder\nand decoder hyperparameters. We remove this dependency\nby scaling the inputs such that the overall magnitude of the\nconcatenated result remains unchanged, but the contributions\nof the inputs become equal.\nWith the standardization completed, we identify two spe-\ncific places where it is still necessary to scale activations by\na learned amount. First, we add a learned, zero-initialized\nscalar gain (i.e., scaling) at the very end of the network,\nas we cannot expect the desired output to always have unit\nvariance. Second, we apply a similar learned gain to the\nconditioning signal within each residual block, so that the\nconditioning is disabled at initialization and its strength in\neach encoder/decoder block becomes a learned parameter.\nAt this point we can disable dropout [20, 75] during training\nwith no ill effects, which has not been previously possible.\nFigure 2c illustrates our final design that is significantly\nsimpler and easier to reason about than the baseline. The\nresulting FID of 2.56 is highly competitive with the current\nstate of the art, especially considering the modest computa-\ntional complexity of our exploration architecture.\n3. Post-hoc EMA\nIt is well known that exponential moving average (EMA) of\nmodel weights plays an important role in generative image\nsynthesis [52, 74], and that the choice of its decay parameter\nhas a significant impact on results. For example, Nichol and\nDhariwal [52] present EMA parameter sweeps to illustrate\nthe effect, and the hyperparameter table of Kang et al. [31]\nshows five training runs using four different decay constants,\nno doubt adjusted for optimal results in each case.\nDespite its known importance, little is known about the\nrelationships between the decay parameter and other aspects\nof training and sampling. To analyze these questions, we\ndevelop a method for choosing the EMA profile post hoc, i.e.,\nwithout the need to specify it before the training. This allows\nus to sample the length of EMA densely and plot its effect\non quality, revealing interesting interactions with network\narchitecture, training time, and classifier-free guidance.\nFurther details, derivations, and discussion on the equa-\ntions and methods in this section are included in Appendix C.\n3.1. Power function EMA profile\nTraditional EMA maintains a running weighted average \u02c6\u03b8\u03b2\nof the network parameters alongside the parameters \u03b8 that are\nbeing trained. At each training step, the average is updated\nby \u02c6\u03b8\u03b2(t) = \u03b2 \u02c6\u03b8\u03b2(t\u22121) + (1\u2212\u03b2) \u03b8(t), where t indicates the\ncurrent training step, yielding an exponential decay profile in\nthe contributions of earlier training steps. The rate of decay\nis determined by the constant \u03b2 that is typically close to one.\nFor two reasons, we propose using a slighly altered aver-\naging profile based on power functions instead of exponential\ndecay. First, our architectural modifications tend to favor\nlonger averages; yet, very long exponential EMA puts non-\nnegligible weight on initial stages of training where network\nparameters are mostly random. Second, we have observed\na clear trend that longer training runs benefit from longer\nEMA decay, and thus the averaging profile should ideally\nscale automatically with training time.\nBoth of the above requirements are fulfilled by power\n5\nfunctions. We define the averaged parameters at time t as\n\u02c6\u03b8\u03b3(t) =\nR t\n0 \u03c4 \u03b3\u03b8(\u03c4) d\u03c4\nR t\n0 \u03c4 \u03b3 d\u03c4\n= \u03b3 + 1\nt\u03b3+1\nZ t\n0\n\u03c4 \u03b3\u03b8(\u03c4) d\u03c4,\n(1)\nwhere the constant \u03b3 controls the sharpness of the profile.1\nWith this formulation, the weight of \u03b8t=0 is always zero.\nThis is desirable, as the random initialization should have no\neffect in the average. The resulting averaging profile is also\nscale-independent: doubling the training time automatically\nstretches the profile by the same factor.\nTo compute \u02c6\u03b8\u03b3(t) in practice, we perform an incremental\nupdate after each training step as follows:\n\u02c6\u03b8\u03b3(t) = \u03b2\u03b3(t) \u02c6\u03b8\u03b3(t \u2212 1) +\n\u00001 \u2212 \u03b2\u03b3(t)\n\u0001\n\u03b8(t)\nwhere \u03b2\u03b3(t) = (1 \u2212 1/t)\u03b3+1.\n(2)\nThe update is thus similar to traditional EMA, but with the\nexception that \u03b2 depends on the current training time.\nFinally, while parameter \u03b3 is mathematically straight-\nforward, it has a somewhat unintuitive effect on the shape\nof the averaging profile.\nTherefore, we prefer to pa-\nrameterize the profile via its relative standard deviation\n\u03c3rel, i.e., the \u201cwidth\u201d of its peak relative to training time:\n\u03c3rel = (\u03b3 + 1)1/2(\u03b3 + 2)\u22121(\u03b3 + 3)\u22121/2.\nThus, when re-\nporting, say, EMA length of 10%, we refer to a profile with\n\u03c3rel = 0.10 (equiv. \u03b3 \u2248 6.94).\n3.2. Synthesizing novel EMA profiles after training\nOur goal is to allow choosing \u03b3, or equivalently \u03c3rel, freely\nafter training. To achieve this, we maintain two averaged\nparameter vectors \u02c6\u03b8\u03b31 and \u02c6\u03b8\u03b32 during training, with constants\n\u03b31 = 16.97 and \u03b32 = 6.94, corresponding to \u03c3rel of 0.05 and\n0.10, respectively. These averaged parameter vectors are\nstored periodically in snapshots saved during the training\nrun. In all our experiments, we store a snapshot once every\n\u223c8 million training images, i.e., once every 4096 training\nsteps with batch size of 2048.\nTo reconstruct an approximate \u02c6\u03b8 corresponding to an ar-\nbitrary EMA profile at any point during or after training,\nwe find the least-squares optimal fit between the EMA pro-\nfiles of the stored \u02c6\u03b8\u03b3i and the desired EMA profile, and take\nthe corresponding linear combination of the stored \u02c6\u03b8\u03b3i. See\nFigure 4 for an illustration.\nWe note that post-hoc EMA reconstruction is not limited\nto power function averaging profiles, or to using the same\ntypes of profiles for snapshots and the reconstruction. Fur-\nthermore, it can be done even from a single stored \u02c6\u03b8 per\nsnapshot, albeit with much lower accuracy than with two\nstored \u02c6\u03b8. This opens the possibility of revisiting previous\ntraining runs that were not run with post-hoc EMA in mind,\n1Technically, calling this an \u201cEMA profile\u201d is a misnomer, as the weight\ndecay is not exponential. However, given that it serves the same purpose as\ntraditional EMA, we feel that coining a new term here would be misleading.\nSnapshots\n0\ntmax\nReconstruction\nTraining time t\nRel. weight of \u03b8(t) in average\nFigure 4. Top: To simulate EMA with arbitrary length after train-\ning, we store a number of averaged network parameter snapshots\nduring training. Each shaded area corresponds to a weighted aver-\nage of network parameters. Here, two averages with different power\nfunction EMA profiles (Section 3.1) are maintained during training\nand stored at 8 snapshots. Bottom: The dashed line shows an exam-\nple post-hoc EMA to be synthesized, and the purple area shows the\nleast-squares optimal approximation based on the stored snapshots.\nWith two averaged parameter vectors stored per snapshot, the mean\nsquared error of the reconstructed weighting profile decreases ex-\ntremely rapidly as the number of snapshots n increases, experimen-\ntally in the order of O(1/n4). In practice, a few dozen snapshots\nis more than sufficient for a virtually perfect EMA reconstruction.\nand experimenting with novel averaging profiles, as long as\na sufficient number of training snapshots are available.\n3.3. Analysis\nArmed with the post-hoc EMA technique, we now analyze\nthe effect of different EMA lengths in various setups.\nFigure 5a shows how FID varies based on EMA length in\nconfigurations B\u2013G of Table 1. We can see that the optimal\nEMA length differs considerably between the configurations.\nMoreover, the optimum becomes narrower as we approach\nthe final configuration G, which might initially seem alarm-\ning.\nHowever, as illustrated in Figure 5b, the narrowness of\nthe optimum seems to be explained by the model becoming\nmore uniform in terms of which EMA length is \u201cpreferred\u201d\nby each weight tensor. In this test, we first select a subset\nof weight tensors from different parts of the network. Then,\nseparately for each chosen tensor, we perform a sweep where\nonly the chosen tensor\u2019s EMA is changed, while all others\nremain at the global optimum. The results, shown as one\nline per tensor, reveal surprisingly large effects on FID. In-\nterestingly, while it seems obvious that one weight tensor\nbeing out-of-sync with the others can be harmful, we observe\nthat in CONFIG B, FID can improve as much as 10%, from\n7.24 to \u223c6.5. In one instance, this is achieved using a very\nshort per-tensor EMA, and in another, a very long one. We\nhypothesize that these different preferences mean that any\nglobal choice is an uneasy compromise. For our final CON-\n6\nEMA = 5%\n10%\n15%\n20%\n25%\n1\n2\n3\n4\n5\n6\n7\n8\nFID\n8.00\n7.24\n6.96\n3.75\n3.02\n2.71\n2.56\nA\nB\nC\nD\nE\nF\nG\n(a) FID vs. EMA for each training config\nEMA = 5%\n10%\n15%\n20%\n25%\n1\n2\n3\n4\n5\n6\n7\n8\nFID\nB\nG\nIndividual tensors\n(b) Per-layer sensitivity to EMA length\nEMA = 6%\n8%\n10%\n12%\n14%\n16%\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nFID\n4.46\n3.31\n2.81\n2.62 2.56 2.55\n268M\n537M\n1074M\n1611M\n2147M\n2684M\n(c) Evolution of CONFIG G over training\nFigure 5. (a) FID vs. EMA length for our training configs on ImageNet-512. CONFIG A uses traditional EMA, and thus only a single point\nis shown. The shaded regions indicate the min/max FID over 3 evaluations. (b) The orange CONFIG B is fairly insensitive to the exact EMA\nlength (x-axis) because the network\u2019s weight tensors disagree about the optimal EMA length. We elucidate this by letting the EMA length\nvary for one tensor at a time (faint lines), while using the globally optimal EMA length of 9% for the others. This has a strong effect on FID\nand, remarkably, sometimes improves it. In the green CONFIG G, the situation is different; per-tensor sweeping has a much smaller effect,\nand deviating from the common optimum of 13% is detrimental. (c) Evolution of the EMA curve for CONFIG G over the course of training.\nFIG G, this effect disappears and the optimum is sharper: no\nsignificant improvement in FID can be seen, and the tensors\nnow agree about the optimal EMA. While post-hoc EMA\nallows choosing the EMA length on a per-tensor basis, we\nhave not explored this opportunity outside this experiment.\nFinally, Figure 5c illustrates the evolution of the optimal\nEMA length over the course of training. Even though our\ndefinition of EMA length is already relative to the length of\ntraining, we observe that the optimum slowly shifts towards\nrelatively longer EMA as the training progresses.\n4. Results\nWe use ImageNet [11] in 512\u00d7512 resolution as our main\ndataset. Table 2 summarizes FIDs for various model sizes\nusing our method, as well as several earlier techniques.\nLet us first consider FID without guidance [21], where the\nbest previous method is VDM++ [38] with FID of 2.99. Even\nour small model EDM2-S that was used for the architecture\nexploration in Section 2 beats this with FID of 2.56. Scaling\nour model up further improves FID to 1.91, surpassing the\nprevious record by a considerable margin. As shown in\nFigure 1, our results are even more significant in terms of\nmodel complexity.\nWe have found that enabling dropout [20, 75] improves\nour results in cases that exhibit overfitting, i.e., when the\ntraining loss continues to decrease but validation loss and\nFID start increasing. We thus enable dropout in our larger\nconfigurations (M\u2013XXL) that show signs of overfitting,\nwhile disabling it in the smaller configurations (XS, S) where\nit is harmful.\nAdditional quantitative results, example images, and de-\ntailed comparisons for this section are given in Appendix A.\nImageNet-512\nFID \u2193\nModel size\nno CFG w/CFG\nMparams Gflops\nNFE\nADM\n[12]\n23.24\n7.72\n559\n1983\n250\nDiT-XL/2\n[55]\n12.03\n3.04\n675\n525\n250\nADM-U\n[12]\n9.96\n3.85\n730\n2813\n250\nRIN\n[30]\n3.95\n\u2013\n320\n415\n1000\nU-ViT, L\n[27]\n3.54\n3.02\n2455\n555\u2217\n256\nVDM++\n[38]\n2.99\n2.65\n2455\n555\u2217\n256\nStyleGAN-XL [69]\n\u2013\n2.41\n168\u2217\n2067\u2217\n1\nEDM2-XS\n3.53\n2.91\n125\n46\n63\nEDM2-S\n2.56\n2.23\n280\n102\n63\nEDM2-M\n2.25\n2.01\n498\n181\n63\nEDM2-L\n2.06\n1.88\n777\n282\n63\nEDM2-XL\n1.96\n1.85\n1119\n406\n63\nEDM2-XXL\n1.91\n1.81\n1523\n552\n63\nTable 2. Results on ImageNet-512. \u201cEDM2-S\u201d is the same as\nCONFIG G in Table 1. The \u201cw/CFG\u201d and \u201cno CFG\u201d columns show\nthe lowest FID obtained with and without classifier-free guidance,\nrespectively. NFE tells how many times the score function is eval-\nuated when generating an image. All diffusion models above the\nhorizontal line use stochastic sampling, whereas our models below\nthe line use deterministic sampling. Whether stochastic sampling\nwould improve our results further is left for future work. Aster-\nisks (\u2217) indicate values that could not be determined from primary\nsources, and have been approximated to within \u223c10% accuracy.\nGuidance.\nIt is interesting to note that several earlier meth-\nods [12, 55] report competitive results only when classifier-\nfree guidance [21] is used. While guidance remains an in-\nvaluable tool for controlling the balance between the percep-\ntual quality of individual result images and the coverage of\nthe generated distribution, it should not be necessary when\nthe goal is to simply match image distributions.\nFigure 6 plots the FID for our small model (EDM2-S)\n7\nEMA = 2%\n4%\n6%\n8%\n10%\n12%\n14%\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\n2.56\n2.39\n2.32\n2.25\n2.23\n2.36\n1.0 (no guidance)\n1.1\n1.2\n1.3\n1.4\n1.5\nFigure 6. Interaction between EMA length and guidance strength\nusing EDM2-S on ImageNet-512.\nusing a variety of guidance strengths as a function of EMA\nlength. The surprising takeaway is that the optimal EMA\nlength depends very strongly on the guidance strength. These\nkinds of studies are extremely expensive without post-hoc\nEMA, and we therefore postulate that the large discrepancy\nbetween vanilla and guidance results in some prior art may\nbe partially an artifact of using non-optimal EMA parameters.\nWith our largest model, a modest amount of guidance (1.2)\nfurther improves the ImageNet-512 FID from 1.91 to 1.81,\nsetting a new record for this dataset.\nLow-cost guidance.\nThe standard way of implementing\nclassifier-free guidance is to train a single model to support\nboth conditional and unconditional generation [21]. While\nconceptually simple, this makes the implicit assumption that\na similarly complex model is needed for both tasks. However,\nthis does not seem to be the case: In our tests, the smallest\n(XS) unconditional model was found to be sufficient for\nguiding even the largest (XXL) conditional model \u2014 using a\nlarger unconditional model did not improve the results at all.\nOur results in Table 2 are computed using an XS-sized\nunconditional model for all of our configurations. Using a\nsmall unconditional model can greatly reduce the typical 2\u00d7\ncomputational overhead of guidance.\nImageNet-64.\nTo demonstrate that our method is not lim-\nited to latent diffusion, we provide results for RGB-space\ndiffusion in ImageNet-64. Table 3 shows that our results are\nsuperior to earlier methods that use deterministic sampling.\nThe previous record FID of 2.22 set by EDM [36] improves\nto 1.58 at similar model complexity, and further to 1.33 via\nscaling. The L-sized model is able to saturate this dataset.\nThis result is close to the record FID of 1.23 achieved\nby RIN using stochastic sampling. Stochastic sampling can\ncorrect for the inaccuracies of the denoising network, but\nthis comes at a considerable tuning effort and computational\ncost (e.g., 1000 vs. 63 NFE), making stochastic sampling\nunattractive for large-scale systems. It is likely that our\nImageNet-64\nDeterministic\nStochastic\nModel size\nFID \u2193\nNFE\nFID \u2193\nNFE\nMparams\nGflops\nADM\n[12]\n\u2013\n\u2013\n2.07\n250\n296\n110\n+ EDM sampling [36]\n2.66\n79\n1.57\n511\n296\n110\n+ EDM training\n[36]\n2.22\n79\n1.36\n511\n296\n110\nVDM++\n[38]\n\u2013\n\u2013\n1.43\n511\n296\n110\nRIN\n[30]\n\u2013\n\u2013\n1.23\n1000\n281\n106\nEDM2-S\n1.58\n63\n\u2013\n\u2013\n280\n102\nEDM2-M\n1.43\n63\n\u2013\n\u2013\n498\n181\nEDM2-L\n1.33\n63\n\u2013\n\u2013\n777\n282\nEDM2-XL\n1.33\n63\n\u2013\n\u2013\n1119\n406\nTable 3. Results on ImageNet-64.\nresults could be improved further using stochastic sampling,\nbut we leave that as future work.\nPost-hoc EMA observations.\nBesides the interactions dis-\ncussed in preceding sections, we have made two preliminary\nfindings related to EMA length. We present them here as\nanecdotal, and leave a detailed study for future work.\nFirst, we observed that the optimal EMA length goes\ndown when learning rate is increased, and vice versa, roughly\naccording to \u03c3rel \u221d 1/(\u03b12\nref tref). The resulting FID also re-\nmains relatively stable over a perhaps 2\u00d7 range of tref. In\npractice, setting \u03b1ref and tref within the right ballpark thus\nseems to be sufficient, which reduces the need to tune these\nhyperparameters carefully.\nSecond, we observed that the optimal EMA length tends\nto go down when the model capacity is increased, and also\nwhen the complexity of the dataset is decreased. This seems\nto imply that simpler problems warrant a shorter EMA.\n5. Discussion and future work\nOur improved denoiser architecture was designed to be a\ndrop-in replacement for the widely used ADM network, and\nthus we hope it will find widespread use in large-scale image\ngenerators. With various aspects of the training now much\nless entangled, it becomes easier to make local modifications\nto the architecture without something breaking elsewhere.\nThis should allow further studies to the structure and balance\nof the U-Net, among other things.\nAn interesting question is whether similar methodology\nwould be equally beneficial for other diffusion architectures\nsuch as RIN [30] and DiT [55], as well as other application\nareas besides diffusion models. It would seem this sort of\nmagnitude-focusing work has attracted relatively little atten-\ntion outside the specific topic of ImageNet classifiers [7, 8].\nWe believe that post-hoc EMA will enable a range of\ninteresting studies that have been infeasible before. Some of\nour plots would have taken a thousand GPU-years to produce\nwithout it; they now took only a GPU-month instead. We\nhope that the cheap-to-produce EMA data will enable new\nbreakthroughs in understanding the precise role of EMA in\ndiffusion models and finding principled ways to set the EMA\nlength \u2014 possibly on a per-layer or per-parameter basis.\n8\nAcknowledgments.\nWe thank Eric Chan, Qinsheng Zhang,\nErik H\u00e4rk\u00f6nen, Tuomas Kynk\u00e4\u00e4nniemi, Arash Vahdat,\nMing-Yu Liu, and David Luebke for discussions and com-\nments, and Tero Kuosmanen and Samuel Klenberg for main-\ntaining our compute infrastructure.\nReferences\n[1] Devansh Arpit, Yingbo Zhou, Bhargava Kota, and Venu\nGovindaraju. Normalization propagation: A parametric tech-\nnique for removing internal covariate shift in deep networks.\nIn Proc. ICML, 2016. 2, 3, 28\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu.\neDiff-I: Text-to-image diffusion models with ensemble of\nexpert denoisers. CoRR, abs/2211.01324, 2022. 2\n[3] Jeremy Bernstein, Arash Vahdat, Yisong Yue, and Ming-Yu\nLiu. On the distance between two neural networks and the\nstability of learning. In Proc. NIPS, 2020. 2, 4, 27, 28\n[4] Jeremy Bernstein, Jiawei Zhao, Markus Meister, Ming-Yu\nLiu, Anima Anandkumar, and Yisong Yue. Learning compo-\nsitional functions via multiplicative weight updates. In Proc.\nNeurIPS, 2020. 28\n[5] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng\nWang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu,\nYunxin Jiao, and Aditya Ramesh. Improving image generation\nwith better captions. Technical report, OpenAI, 2023. 2\n[6] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proc. CVPR, 2023. 1\n[7] Andrew Brock, Soham De, and Samuel L. Smith. Charac-\nterizing signal propagation to close the performance gap in\nunnormalized ResNets. In Proc. ICLR, 2021. 2, 3, 8\n[8] Andrew Brock, Soham De, Samuel L. Smith, and Karen\nSimonyan. High-performance large-scale image recognition\nwithout normalization. In Proc. ICML, 2021. 2, 5, 8, 28\n[9] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructPix2Pix: Learning to follow image editing instructions.\nIn Proc. CVPR, 2023. 1\n[10] Minhyung Cho and Jaehyung Lee. Riemannian approach to\nbatch normalization. In Proc. NIPS, 2017. 2, 27\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. ImageNet: A large-scale hierarchical image database.\nIn Proc. CVPR, 2009. 2, 7\n[12] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\nGANs on image synthesis. In Proc. NeurIPS, 2021. 2, 3, 7, 8,\n15, 16, 35\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-Or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In Proc. ICLR, 2023. 1\n[14] Spyros Gidaris and Nikos Komodakis. Dynamic few-shot\nvisual learning without forgetting. In Proc. CVPR, 2018. 3,\n23\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectifiers: Surpassing human-level perfor-\nmance on ImageNet classification. In Proc. ICCV, 2015. 3,\n18\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Proc. ECCV,\n2016. 3, 17\n[17] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (GELUs). CoRR, abs/1606.08415, 2016. 5, 17\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. In Proc. ICLR, 2023. 1\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. GANs trained by a two\ntime-scale update rule converge to a local Nash equilibrium.\nIn Proc. NIPS, 2017. 2, 35\n[20] Geoffrey E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R. Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors.\nCoRR, abs/1207.0580, 2012. 5, 7\n[21] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. In NeurIPS 2021 Workshop on Deep Generative\nModels and Downstream Applications, 2021. 1, 7, 8\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In Proc. NeurIPS, 2020. 1, 2\n[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben Poole,\nMohammad Norouzi, David J. Fleet, and Tim Salimans. Ima-\ngen Video: High definition video generation with diffusion\nmodels. CoRR, abs/2210.02303, 2022. 1\n[24] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 23(1), 2022.\n2\n[25] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William\nChan, Mohammad Norouzi, and David J. Fleet. Video diffu-\nsion models. In Proc. ICLR Workshop on Deep Generative\nModels for Highly Structured Data, 2022. 1\n[26] Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry.\nNorm matters: Efficient and accurate normalization schemes\nin deep networks. In Proc. NIPS, 2018. 27\n[27] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. Sim-\nple diffusion: End-to-end diffusion for high resolution images.\nIn Proc. ICML, 2023. 2, 7\n[28] Aapo Hyv\u00e4rinen. Estimation of non-normalized statistical\nmodels by score matching. JMLR, 6(24), 2005. 1\n[29] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry\nVetrov, and Andrew Gordon Wilson.\nAveraging weights\nleads to wider optima and better generalization. In Proc.\nUncertainty in Artificial Intelligence, 2018. 2\n[30] Allan Jabri, David J. Fleet, and Ting Chen. Scalable adaptive\ncomputation for iterative generation. In Proc. ICML, 2023. 7,\n8\n[31] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli\nShechtman, Sylvain Paris, and Taesung Park. Scaling up\nGANs for text-to-image synthesis. In Proc. CVPR, 2023. 2, 5\n9\n[32] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of GANs for improved quality, stability,\nand variation. In Proc. ICLR, 2018. 5, 23, 25\n[33] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nProc. CVPR, 2019. 2, 3\n[34] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila. Analyzing and improving\nthe image quality of StyleGAN. In Proc. CVPR, 2020. 3, 5,\n28\n[35] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. In Proc. NeurIPS, 2021. 28,\n35\n[36] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In proc. NeurIPS, 2022. 1, 2, 8, 15, 16, 18, 19, 34,\n35\n[37] Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task\nlearning using uncertainty to weigh losses for scene geometry\nand semantics. In Proc. CVPR, 2018. 3, 20\n[38] Diederik Kingma and Ruiqi Gao. Understanding diffusion\nobjectives as the ELBO with data augmentation. In Proc.\nNeurIPS, 2023. 7, 8\n[39] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In Proc. ICLR, 2015. 4, 25, 26, 27\n[40] G\u00fcnter Klambauer, Thomas Unterthiner, Andreas Mayr, and\nSepp Hochreiter. Self-normalizing neural networks. In Proc.\nNIPS, 2017. 2, 3\n[41] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan\nCatanzaro. DiffWave: A versatile diffusion model for audio\nsynthesis. In Proc. ICLR, 2021. 1\n[42] Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel\nL. K. Yamins, and Hidenori Tanaka. Neural mechanics: Sym-\nmetry and broken conservation laws in deep learning dynam-\nics. In Proc. ICLR, 2021. 27\n[43] Twan van Laarhoven. L2 regularization versus batch and\nweight normalization. CoRR, abs/1706.05350, 2017. 2, 26,\n27\n[44] Zhiyuan Li and Sanjeev Arora. An exponential learning rate\nschedule for deep learning. In Proc. ICLR, 2020. 2\n[45] Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora. Reconciling\nmodern deep learning with traditional optimization analyses:\nThe intrinsic learning rate. In Proc. NeurIPS, 2020. 27\n[46] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-\nYu Liu, and Tsung-Yi Lin. Magic3D: High-resolution text-to-\n3D content creation. In Proc. CVPR, 2023. 1\n[47] Yang Liu, Jeremy Bernstein, Markus Meister, and Yisong Yue.\nLearning by turning: Neural architecture aware optimisation.\nIn Proc. ICML, 2021. 28\n[48] Chunjie Luo, Jianfeng Zhan, Xiaohe Xue, Lei Wang, Rui\nRen, and Qiang Yang. Cosine normalization: Using cosine\nsimilarity instead of dot product in neural networks. In Proc.\nICANN, 2018. 3, 23\n[49] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen\nKrueger, and Girish Sastry. DALL\u00b7E 2 preview \u2013 risks and\nlimitations. OpenAI, 2022. 36\n[50] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. NULL-text inversion for editing real im-\nages using guided diffusion models. In Proc. CVPR, 2023.\n1\n[51] Quang-Huy Nguyen, Cuong Q. Nguyen, Dung D. Le, and\nHieu H. Pham. Enhancing few-shot image classification with\ncosine transformer. IEEE Access, 11, 2023. 3, 23\n[52] Alex Nichol and Prafulla Dhariwal. Improved denoising\ndiffusion probabilistic models. In Proc. ICML, pages 8162\u2013\n8171, 2021. 1, 2, 5\n[53] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In Proc.\nICML, 2022. 2\n[54] Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo,\nMarc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel\nHaziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud\nAssran, Nicolas Ballas, Wojciech Galuba, Russell Howes,\nPo-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat,\nVasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien\nMairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDINOv2: Learning robust visual features without supervision.\nCoRR, abs/2304.07193, 2023. 12\n[55] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proc. ICCV, 2023. 2, 7, 8, 16\n[56] Boris Polyak and Anatoli Juditsky. Acceleration of stochastic\napproximation by averaging. SIAM Journal on Control and\nOptimization, 30(4), 1992. 2\n[57] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.\nDreamFusion: Text-to-3D using 2D diffusion. In Proc. ICLR,\n2023. 1\n[58] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima\nSadekova, and Mikhail Kudinov. Grad-TTS: A diffusion\nprobabilistic model for text-to-speech. In Proc. ICML, 2021.\n1\n[59] Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan\nYuille. Micro-batch training with batch-channel normalization\nand weight standardization. CoRR, abs/1903.10520, 2019. 25\n[60] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nBen Mildenhall, Nataniel Ruiz, Shiran Zada, Kfir Aberman,\nMichael Rubenstein, Jonathan Barron, Yuanzhen Li, and\nVarun Jampani. DreamBooth3D: Subject-driven text-to-3D\ngeneration. In Proc. ICCV, 2023. 1\n[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with CLIP latents. CoRR, abs/2204.06125, 2022. 2\n[62] Simon Roburin, Yann de Mont-Marin, Andrei Bursuc, Re-\nnaud Marlet, Patrick P\u00e9rez, and Mathieu Aubry. Spherical\nperspective on learning with normalization layers. Neurocom-\nputing, 487, 2022. 27\n[63] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proc. CVPR, 2022.\n2, 16\n[64] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net:\nConvolutional networks for biomedical image segmentation.\nIn Proc. MICCAI, 2015. 1, 2, 3, 16\n10\n[65] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. DreamBooth: Fine\ntuning text-to-image diffusion models for subject-driven gen-\neration. In Proc. CVPR, 2023. 1\n[66] David Ruppert.\nEfficient estimations from a slowly con-\nvergent Robbins\u2013Monro process. Technical report, Cornell\nUniversity \u2013 Operations Research and Industrial Engineering,\n1988. 2\n[67] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily Denton, Seyed Kamyar Seyed Ghasemipour,\nBurcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Proc. NeurIPS, 2022. 2, 34\n[68] Tim Salimans and Diederik P. Kingma. Weight normalization:\nA simple reparameterization to accelerate training of deep\nneural networks. In Proc. NIPS, 2016. 2, 4, 25, 26, 27\n[69] Axel Sauer, Katja Schwarz, and Andreas Geiger. StyleGAN-\nXL: Scaling StyleGAN to large diverse datasets. In Proc.\nSIGGRAPH, 2022. 2, 7\n[70] J. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3D neural field generation\nusing triplane diffusion. In Proc. CVPR, 2023. 1\n[71] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In Proc. ICML, 2015. 1\n[72] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In Proc. ICLR, 2021. 2\n[73] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. In Proc. NeurIPS,\n2019. 1\n[74] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn Proc. ICLR, 2021. 1, 2, 5\n[75] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya\nSutskever, and Ruslan Salakhutdinov. Dropout: A simple\nway to prevent neural networks from overfitting. JMLR, 15\n(56), 2014. 5, 7\n[76] George Stein, Jesse C. Cresswell, Rasa Hosseinzadeh, Yi\nSui, Brendan Leigh Ross, Valentin Villecroze, Zhaoyan Liu,\nAnthony L. Caterini, J. Eric T. Taylor, and Gabriel Loaiza-\nGanem. Exposing flaws of generative model evaluation met-\nrics and their unfair treatment of diffusion models. In Proc.\nNeurIPS, 2023. 12, 14\n[77] Matthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In Proc. NeurIPS, 2020. 3, 24\n[78] Antti Tarvainen and Harri Valpola. Mean teachers are better\nrole models: Weight-averaged consistency targets improve\nsemi-supervised deep learning results. In Proc. NIPS, 2017.\n2\n[79] Cristina Vasconcelos, Hugo Larochelle, Vincent Dumoulin,\nRob Romijnders, Nicolas Le Roux, and Ross Goroshin. Im-\npact of aliasing on generalization in deep convolutional net-\nworks. In ICCV, 2021. 28\n[80] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Proc. NIPS, 2017.\n1, 2, 17\n[81] Pascal Vincent. A connection between score matching and\ndenoising autoencoders. Neural Computation, 23(7):1661\u2013\n1674, 2011. 1\n[82] Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun.\nSpherical motion dynamics: Learning dynamics of normal-\nized neural network using SGD and weight decay. In Proc.\nNeurIPS, 2021. 27\n[83] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\nHsuan Yang. Diffusion models: A comprehensive survey of\nmethods and applications. ACM Comput. Surv., 56(4), 2023.\n1\n[84] Yasin Yaz\u0131c\u0131, Chuan-Sheng Foo, Stefan Winkler, Kim-Hui\nYap, Georgios Piliouras, and Vijay Chandrasekhar. The un-\nusual effectiveness of averaging in GAN training. In Proc.\nICLR, 2019. 2\n[85] Yang You, Igor Gitman, and Boris Ginsburg. Large batch\ntraining of convolutional networks. CoRR, abs/1708.03888,\n2017. 2, 4, 27, 28\n[86] Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv\nKumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel,\nKurt Keutzer, and Cho-Jui Hsieh. Large batch optimization\nfor deep learning: Training BERT in 76 minutes. In Proc.\nICLR, 2020. 28\n[87] Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger Grosse.\nThree mechanisms of weight decay regularization. In Proc.\nICLR, 2019. 2, 27\n[88] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. In Proc.\nICCV, 2023. 1\n11\nUnconditional\nFID \u2193\nTotal capacity\nSampling cost\nEMA length\nmodel\n(Gparams)\n(Tflops)\nXS\n1.81\n1.65\n38.9\n1.5%\nS\n1.80\n1.80\n42.5\n1.5%\nM\n1.80\n2.02\n47.4\n1.5%\nL\n1.86\n2.30\n53.8\n2.0%\nXL\n1.82\n2.64\n61.6\n2.0%\nXXL\n1.85\n3.05\n70.8\n2.0%\nTable 4. Effect of the unconditional model\u2019s size in guiding our\nXXL-sized ImageNet-512 model. The total capacity and sampling\ncost refer to the combined cost of the XXL-sized conditional model\nand the chosen unconditional model. Guidance strength of 1.2 was\nused in this test.\nA. Additional results\nA.1. Generated images\nFigure 7 shows hand-selected images generated using our\nlargest (XXL) ImageNet-512 model without classifier-free\nguidance. Figures 24\u201326 show uncurated images from the\nsame model for various ImageNet classes, with guidance\nstrength selected per class.\nA.2. Quality vs. compute\nFigure 1 in the main paper quantifies the model\u2019s cost using\ngigaflops per evaluation, but this is just one possible option.\nWe could equally well consider several alternative definitions\nfor the model\u2019s cost.\nFigure 8 shows that the efficiency improvement observed\nin Figure 1 is retained when the model\u2019s cost is quantified\nusing the number of trainable parameters instead. Figure 9\nplots the same with respect to the sampling cost per im-\nage, demonstrating even greater improvements due to our\nlow number of score function evaluations (NFE). Finally,\nFigure 11 plots the training cost of the model. According\nto all of these metrics, our model reaches the same quality\nmuch quicker, and proceeds to improve the achievable result\nquality significantly.\nFigure 10 shows post-hoc EMA sweeps for a set of snap-\nshots for our XXL-sized ImageNet-512 model with and with-\nout dropout. We observe that in this large model, overfit-\nting starts to compromise the results without dropout, while\na 10% dropout allows steady convergence. Figure 11 further\nshows the convergence of different model sizes as a function\nof training cost with and without dropout. For the smaller\nmodels (XS, S) dropout is detrimental, but for the larger\nmodels it clearly helps, albeit at a cost of slightly slower\ninitial convergence.\nA.3. Guidance vs. unconditional model capacity\nTable 4 shows quantitatively that using a large unconditional\nmodel is not useful in classifier-free guidance. Using a\nvery small unconditional model for guiding the conditional\nImageNet-512\nFDDINOv2 \u2193\nModel size\nno CFG w/CFG\nMparams Gflops NFE\nEDM2-XS\n103.39\n79.94\n125\n46\n63\nEDM2-S\n68.64\n52.32\n280\n102\n63\nEDM2-M\n58.44\n41.98\n498\n181\n63\nEDM2-L\n52.25\n38.20\n777\n282\n63\nEDM2-XL\n45.96\n35.67\n1119\n406\n63\nEDM2-XXL\n42.84\n33.09\n1523\n552\n63\nTable 5. Version of Table 2 using FDDINOv2 instead of FID on\nImageNet-512. The \u201cw/CFG\u201d and \u201cno CFG\u201d columns show the\nlowest FID obtained with and without classifier-free guidance, re-\nspectively. NFE tells how many times the score function is evalu-\nated when generating an image.\nmodel reduces the computational cost of guided diffusion\nby almost 50%. The EMA lengths in the table apply to both\nconditional and unconditional model; it is typical that very\nshort EMAs yield best results when sampling with guidance.\nA.4. Learning rate vs. EMA length\nFigure 12 visualizes the interaction between EMA length\nand learning rate. While a sweet spot for the learning rate\ndecay parameter still exists (tref = 70k in this case), the\npossibility of sweeping over the EMA lengths post hoc dras-\ntically reduces the importance of this exact choice. A wide\nbracket of learning rate decays tref \u2208 [30k, 160k] yields FIDs\nwithin 10% of the optimum using post-hoc EMA.\nIn contrast, if the EMA length was fixed at 13%, varying\ntref would increase FID much more, at worst by 72% in the\ntested range.\nA.5. Fr\u00e9chet distances using DINOv2\nThe DINOv2 feature space [54] has been observed to align\nmuch better with human preferences compared to the widely\nused InceptionV3 feature space [76]. We provide a version of\nTable 2 using the Fr\u00e9chet distance computed in the DINOv2\nspace (FDDINOv2) in Table 5 to facilitate future comparisons.\nWe use the publicly available implementation2 by Stein\net al. [76] for computing FDDINOv2. We use 50,000 generated\nimages and all 1,281,167 available real images, following the\nestablished best practices in FID computation. Class labels\nfor the 50k generated samples are drawn from a uniform\ndistribution. We evaluate FD only once per 50k sample as\nwe observe little random variation between runs.\nFigure 13 compares FID and FDDINOv2 as a function of\nEMA length. We can make three interesting observations.\nFirst, without guidance, the optima of the two CONFIG G\ncurves (green) are in a clearly different place, with FDDINOv2\npreferring longer EMA. The disagreement between the two\nmetrics is quite significant: FID considers FDDINOv2\u2019s opti-\nmum (19%) to be a poor choice, and vice versa.\n2https://github.com/layer6ai-labs/dgm-eval\n12\nFigure 7. Selected images generated using our largest (XXL) ImageNet-512 model without guidance.\n13\n200\n500\n1000\n2000\n2\n3\n5\n10\n20\nFID\nADM\nADM\nADM-U\nADM-U\nDiT-XL/2\nDiT-XL/2\nRIN\nU-ViT, L\nVDM++\nVDM++\nStyleGAN-XL\nXS\nS\nM\nL\nXL\nXXL\nXS\nS\nM\nL\nXL\nXXL\nModel capacity (millions of trainable parameters), ImageNet-512\nPrevious, no guidance\nPrevious, with guidance\nOurs, no guidance\nOurs, with guidance\nFigure 8. FID vs. model capacity on ImageNet-512. For our\nmethod with guidance, we account for the number of parameters in\nthe XS-sized unconditional model.\nSecond, with guidance strength 1.4 (the optimal choice\nfor FID according to Figure 6) the curves are astonishingly\ndifferent. While both metrics agree that a modest amount of\nguidance is helpful, their preferred EMA lengths are totally\ndifferent (2% vs 14%). FID considers FDDINOv2\u2019s optimum\n(14%) to be a terrible choice and vice versa. Based on a\ncursory assessment of the generated images, it seems that\nFDDINOv2 prefers images with better global coherency, which\noften maps to higher perceptual quality, corroborating the\nconclusions of Stein et al. [76]. The significant differences in\nthe optimal EMA length highlight the importance of search-\ning the optimum specifically for the chosen quality metric.\nThird, FDDINOv2 prefers higher guidance strength than\nFID (1.9 vs 1.4). FID considers 1.9 clearly excessive.\nThe figure furthermore shows that our changes (CONFIG\nB vs G) yield an improvement in FDDINOv2 that is at least as\nsignificant as the drop we observed using FID.\nA.6. Activation and weight magnitudes\nFigure 14 shows an extended version of Figure 3, including\nactivation and weight magnitude plots for CONFIG B\u2013G mea-\nsured using both max and mean aggregation over each reso-\nlution bucket. The details of the computation are as follows.\nWe first identify all trainable weight tensors within the\nU-Net encoder/decoder blocks of each resolution, including\nthose in the associated self-attention layers. This yields a\nset of tensors for each of the eight resolution buckets iden-\ntified in the legend, i.e., {Enc, Dec}\u00d7{8\u00d78, . . ., 64\u00d764}.\nThe analyzed activations are the immediate outputs of the\n5\n10\n20\n50\n100\n200\n500\n1000\n2\n3\n5\n10\n20\nFID\nADM\nADM\nADM-U\nADM-U\nDiT-XL/2\nDiT-XL/2\nRIN\nU-ViT, L\nVDM++\nVDM++\nXS\nS\nM\nL\nXL XXL\nXS\nS\nM\nL XLXXL\nSampling cost (teraflops per image), ImageNet-512\nPrevious, no guidance\nPrevious, with guidance\nOurs, no guidance\nOurs, with guidance\nFigure 9. FID vs. sampling cost on ImageNet-512. For latent\ndiffusion models (DiT-XL/2 and ours), we include the cost of\nrunning the VAE decoder at the end (1260.9 gigaflops per image).\noperations involving these tensors before any nonlinearity,\nand the analyzed weights are the tensors themselves.\nIn CONFIG B, we do not include trainable biases in the\nweight analysis, but the activations are measured after apply-\ning the biases. In CONFIG G, we exclude the learned scalar\ngains from the weight analysis, but measure the activations\nafter the gains have been applied.\nActivations.\nThe activation magnitudes are computed as\nan expectation over 4096 training samples. Ignoring the\nminibatch axis for clarity, most activations are shaped\nN \u00d7H\u00d7W where N is the number of feature maps and\nH and W are the spatial dimensions. For the purposes of\nanalysis, we reshape these to N \u00d7M where M = HW. The\noutputs of the linear transformation of the class embedding\nvector are considered to have shape N \u00d71.\nGiven a potentially reshaped activation tensor h \u2208 RN\u00d7M,\nwe compute the magnitudes of the individual features hi as\nM[hi] =\nv\nu\nu\nt 1\nM\nM\nX\nj=1\nh2\ni,j.\n(3)\nThe result contains the per-feature L2 norms of the activa-\ntions in tensor h, scaled such that unit-normal distributed\nactivations yield an expected magnitude of 1 regardless of\ntheir dimensions.\nAll of these per-feature scalar magnitudes within a reso-\nlution bucket are aggregated into a single number by taking\neither their maximum or their mean. Taking the maximum\n14\nEMA = 2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n2.0\n2.5\n3.0\n3.5\nFID\n2.45\n2.19\n2.12 2.14 2.17\n2.25\n268M\n403M\n537M\n671M\n805M\n940M\n(a) EDM2-XXL, no dropout\nEMA = 2%\n4%\n6%\n8%\n10%\n12%\n14%\n16%\n2.0\n2.5\n3.0\n3.5\nFID\n2.57\n2.21\n2.03\n1.991.931.91\n268M\n403M\n537M\n671M\n805M\n940M\n(b) EDM2-XXL, 10% dropout\nFigure 10. Effect of dropout on the training of EDM2-XXL in ImageNet-512. (a) Without dropout, the training starts to overfit after 537\nmillion training images. (b) With dropout, the training starts off slightly slower, but it makes forward progress for much longer.\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n2\n3\n5\n10\n20\nFID\nADM\nADM-U\nDiT-XL/2\nRIN\nU-ViT, L\nVDM++\nXS\nS\nM\nL XL\nXXL\nXS\nS\nM\nL\nXL\nXXL\nTraining cost (zettaflops per model), ImageNet-512\nPrevious\nOurs, no dropout\nOurs, 10% dropout\nFigure 11. FID vs. training cost on ImageNet-512 without guidance.\nNote that one zettaflop = 1021 flops = 1012 gigaflops. We assume\nthat one training iteration is three times as expensive as evaluating\nthe model (i.e., forward pass, backprop to inputs, backprop to\nweights).\nmagnitude (Figure 3 and Figure 14, left half) ensures that\npotential extreme behavior is not missed, whereas the mean\nmagnitude (Figure 14, right half) is a closer indicator of\naverage behavior. Regardless of the choice, the qualitative\nbehavior is similar.\nWeights.\nAll weight tensors under analysis are of shape\nN \u00d7 \u00b7 \u00b7 \u00b7 where N is the number of output features. We thus\nreshape them all into N \u00d7M and compute the per-output-\nfeature magnitudes using Equation 3. Similar to activations,\nEMA = 4%\n6%\n8%\n10%\n12%\n14%\n16%\n18%\n20%\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\n2.82\n2.612.69\n2.56\n2.68\n2.68\n2.75\n30k\n40k\n50k\n70k\n100k\n120k\n160k\nFigure 12. Interaction between EMA length and learning rate decay\n(tref, different colors) using EDM2-S on ImageNet-512.\nthis ensures that unit-normal distributed weights have an ex-\npected magnitude of 1 regardless of degree or dimensions of\nthe weight tensor. We again aggregate all magnitudes within\na resolution bucket into a single number by taking either the\nmaximum or the mean. Figure 3 displays maximum magni-\ntudes, whereas the extended version in Figure 14 shows both\nmaximum and mean magnitudes.\nB. Architecture details\nIn this section, we present comprehensive details for the\narchitectural changes introduced in Section 2. Figures 15\u201321\nillustrate the architecture diagram corresponding to each\nconfiguration, along with the associated hyperparameters. In\norder to observe the individual changes, we invite the reader\nto flip through the figures in digital form.\nB.1. EDM baseline (CONFIG A)\nOur baseline corresponds to the ADM [12] network as\nimplemented in the EDM [36] framework, operating in\nthe latent space of a pre-trained variational autoencoder\n15\n2\n3\n4\n5\n6\n7\nFID\n7.24\n2.56\n2.23\n3.66\nEMA = 2%\n4%\n6%\n8%\n10% 12% 14% 16% 18% 20% 22% 24%\n50\n100\n150\n200\nFDDINOv2\n204.1\n68.64\n55.23\n52.32\nCONFIG B\nCONFIG G\n+ guidance 1.4\n+ guidance 1.9\nFigure 13. FID and FDDINOv2 as a function of EMA length using\nS-sized models on ImageNet-512. CONFIGS B and G illustrate\nthe improvement from our changes. We also show two guidance\nstrengths: FID\u2019s optimum (1.4) and FDDINOv2\u2019s optimum (1.9).\n(VAE) [63]. We train the network for 219 training iterations\nwith batch size 4096, i.e., 2147.5 million images, using the\nsame hyperparameter choices that were previously used for\nImageNet-64 by Karras et al. [36]. In this configuration, we\nuse traditional EMA with a half-life of 50M images, i.e., 12k\ntraining iterations, which translates to \u03c3rel \u2248 0.034 at the\nend of the training. The architecture and hyperparameters as\nsummarized in Figure 15.\nPreconditioning.\nFollowing the EDM framework, the net-\nwork implements denoiser \u02c6y = D\u03b8(x; \u03c3, c), where x is a\nnoisy input image, \u03c3 is the corresponding noise level, c is\na one-hot class label, and \u02c6y is the resulting denoised im-\nage; in the following, we will omit c for conciseness. The\nframework further breaks down the denoiser as\nD\u03b8(x; \u03c3) = cskip(\u03c3)x + cout(\u03c3)F\u03b8\n\u0000cin(\u03c3)x; cnoise(\u03c3)\n\u0001\n(4)\ncskip(\u03c3) = \u03c32\ndata /\n\u0000\u03c32 + \u03c32\ndata\n\u0001\n(5)\ncout(\u03c3) = (\u03c3 \u00b7 \u03c3data)\n\u000e p\n\u03c32 + \u03c32\ndata\n(6)\ncin(\u03c3) = 1\n\u000e p\n\u03c32 + \u03c32\ndata\n(7)\ncnoise(\u03c3) =\n1\n4 ln(\u03c3),\n(8)\nwhere the inputs and outputs of the raw network layers F\u03b8 are\npreconditioned according to cin, cout, cskip, and cnoise. \u03c3data\nis the expected standard deviation of the training data. The\npreconditioning is reflected in Figure 15 by the blue boxes\naround the main inputs and outputs.\nLatent diffusion.\nFor ImageNet-512, we follow Peebles\nand Xie [55] by preprocessing each 512\u00d7512\u00d73 image in\nthe dataset with a pre-trained off-the-shelf VAE encoder from\nStable Diffusion3 and postprocessing each generated image\nwith the correspoding decoder. For a given input image, the\nencoder produces a 4-channel latent at 8\u00d78 times lower res-\nolution than the original, yielding a dimension of 64\u00d764\u00d74\nfor x and \u02c6y. The mapping between images and latents is\nnot strictly bijective: The encoder turns a given image into\na distribution of latents, where each channel c of each pixel\n(x, y) is drawn independently from N(\u00b5x,y,c, \u03c32\nx,y,c). When\npreprocessing the dataset, we store the values of \u00b5x,y,c and\n\u03c3x,y,c as 32-bit floating point, and draw a novel sample each\ntime we encounter a given image during training.\nThe EDM formulation in Equation 4 makes relatively\nstrong assumptions about the mean and standard deviation\nof the training data. We choose to normalize the training\ndata globally to satisfy these assumptions \u2014 as opposed to,\ne.g., changing the value of \u03c3data, which might have far-\nreaching consequences in terms of the other hyperparam-\neters. We thus keep \u03c3data at its default value 0.5, subtract\n[5.81, 3.25, 0.12, \u22122.15] from the latents during dataset pre-\nprocessing to make them zero mean, and multiply them by\n0.5 / [4.17, 4.62, 3.71, 3.28] to make their standard devia-\ntion agree with \u03c3data. When generating images, we undo this\nnormalization before running the VAE decoder.\nArchitecture walkthrough.\nThe ADM [12] network starts\nby feeding the noisy input image, multiplied by cnoise,\nthrough an input block (\u201cIn\u201d) to expand it to 192 channels.\nIt then processes the resulting activation tensor through a\nseries of encoder and decoder blocks, organized as a U-Net\nstructure [64] and connected to each other via skip connec-\ntions (faint curved arrows). At the end, the activation tensor\nis contracted back to 4 channels by an output block (\u201cOut\u201d),\nand the final denoised image is obtained using cout and cskip\nas defined by Equation 4. The encoder gradually decreases\nthe resolution from 64\u00d764 to 32\u00d732, 16\u00d716, and 8\u00d78 by a\nset of downsampling blocks (\u201cEncD\u201d), and the channel count\nis simultaneously increased from 192 to 384, 576, and 768.\nThe decoder implements the same progression in reverse\nusing corresponding upsampling blocks (\u201cDecU\u201d).\nThe operation of the encoder and decoder blocks is con-\nditioned by a 768-dimensional embedding vector, obtained\nby feeding the noise level \u03c3 and class label c through a\nseparate embedding network (\u201cEmbedding\u201d). The value\nof cnoise(\u03c3) is fed through a sinusoidal timestep embedding\nlayer4,5(\u201cPosEmb\u201d) to turn it into a 192-dimensional feature\n3https://huggingface.co/stabilityai/sd-vae-ft-mse\n4https://github.com/openai/guided-diffusion/blob/22e0\ndf8183507e13a7813f8d38d51b072ca1e67c/guided_diffusion/n\nn.py#L103\n5https://github.com/NVlabs/edm/blob/62072d2612c7da051\n65d6233d13d17d71f213fee/training/networks.py#L193\n16\nCONFIG B\n0\n200\n400\n600\nActivations (max)\n0\n0.5\n1.0\n1.5\nWeights (max)\n0\n10\n20\n30\nActivations (mean)\n0\n0.1\n0.2\n0.3\nWeights (mean)\nCONFIG C\n0\n200\n400\n600\n0\n0.5\n1.0\n1.5\n0\n10\n20\n30\n0\n0.1\n0.2\n0.3\nCONFIG D\n0\n10\n20\n30\n0\n10\n20\n30\n0\n0.5\n1.0\n1.5\n0\n5\n10\n15\nCONFIG E\n0\n10\n20\n30\n0\n0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nCONFIG F\n0\n10\n20\n30\n0\n0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nCONFIG G\nGimg = 0.5\n1.0\n1.5\n0\n10\n20\n30\nGimg = 0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nGimg = 0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nGimg = 0.5\n1.0\n1.5\n0\n0.5\n1.0\n1.5\nEnc-64x64\nDec-64x64\nEnc-32x32\nDec-32x32\nEnc-16x16\nDec-16x16\nEnc-8x8\nDec-8x8\nFigure 14. Training-time evolution of the maximum and mean dimension-weighted L2 norms of activations and weights over different\ndepths of the the EMA-averaged score network. As discussed in Section 2, our architectural modifications aim to standardize the activation\nmagnitudes in CONFIG D and weight magnitudes in CONFIG E. Details of the computation are discussed in Appendix A.6.\nvector. The result is then processed by two fully-connected\nlayers with SiLU nonlinearity [17], defined as\nsilu(x) =\nx\n1 + e\u2212x ,\n(9)\nadding in a learned per-class embedding before the second\nnonlinearity.\nThe encoder and decoder blocks follow the standard pre-\nactivation design of ResNets [16]. The main path (bold\nline) undergoes minimal processing: It includes an optional\n2\u00d72 upsampling or downsampling using box filter if the\nresolution changes, and an 1\u00d71 convolution if the number\nof channels changes. The residual path employs two 3\u00d73\nconvolutions, preceded by group normalization and SiLU\nnonlinearity. The group normalization computes empirical\nstatistics for each group of 32 channels, normalizes them\nto zero mean and unit variance, and then applies learned\nper-group scaling and bias. Between the convolutions, each\nchannel is further scaled and biased based on the value of the\nembedding vector, processed by a per-block fully-connected\nlayer. The ADM architecture further employs dropout before\nthe second convolution, setting individual elements of the\nactivation tensor to zero with 10% probability during training.\nThe U-Net skip connections originate from the outputs of\nthe encoder blocks, and they are concatenated to the inputs\nof the corresponding decoder blocks.\nMost of the encoder and decoder blocks operating at\n32\u00d732 resolution and below (\u201cEncA\u201d and \u201cDecA\u201d) further\nemploy self-attention after the residual branch. The imple-\nmentation follows the standard multi-head scaled dot product\nattention [80], where each pixel of the incoming activation\ntensor is treated as a separate token. For a single attention\nhead, the operation is defined as\nA = softmax(W)V\n(10)\n17\n1\n768\nEmbedding\n1000\nNoisy\nimage\ncout\ncskip\n+\nDenoised\nimage\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDecA\nDecA\nDecA\nDecA\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\n+\n\u00d7\nDown 2\u00d72\nSiLU\nDropout\nBias\nConv 3\u00d73\n+\nSplit\n+1\nBias\nLinear\nBias\nConv 1\u00d71\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7Cout\nRout\u00d7Cout\n768\nCout\nEncoder block\nInput\nEmbedding\nOutput\nSkip\nDec\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\n+\n\u00d7\nUp 2\u00d72\nSiLU\nDropout\nBias\nConv 3\u00d73\n+\nSplit\n+1\nBias\nLinear\nAttention\nBias\nConv 1\u00d71\nUp 2\u00d72\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\n768\nCout\nDecoder block\nOutput\nConcat\nSkip\nAttention\nGrpNorm\nBias\nConv 1\u00d71\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nBias\nConv 1\u00d71\nSplit\n+\nRin\u00d7Cin\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nRin\u00d7Nh\u00d7Nc\nRin\u00d7Cin\nRin\u00d7Cin\n1\nNc\nInput\nOutput\nAttention\nNoise\nlevel\nPosEmb\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\n192\n768\nBias\nBias\nClass\nlabel\nGrpNorm\nGrpNorm\nNh = Cin / Nc\nNc = 64\nFP32\nFP32\nInput\n642\u00d74\n642\u00d74\n642\u00d74\n642\u00d74\nConfig A: EDM baseline\nTo encoder and \ndecoder blocks\nFixed-function\nLearned\nNot always present\nLearned,\nzero init.\ncnoise\nGrpNorm\nConv 3\u00d73\nBias\nSiLU\n642\u00d7192\nOut\n642\u00d74\nEmbedding\nConv 3\u00d73\n642\u00d7192\n642\u00d74\nIn\nBias\nRin\u00d7Nh\u00d7Nc\u00d73\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0001\nAdam \u03b21\n0.9\nFID\n8.00\nMinibatch size\n4096\nLearning rate decay (tref)\n\u221e\nAdam \u03b22\n0.999\nEMA length (\u03c3rel)\n0.034\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n100\nModel capacity (Mparams)\n295.9\nMixed-precision (FP16)\npartial\nNoise distribution mean (Pmean)\n\u22121.2\nAttention res.\n32, 16, 8\nModel complexity (Gflops)\n110.4\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.2\nAttention blocks\n22\nSampling cost (Tflops)\n8.22\nFigure 15. Full architecture diagram and hyperparameters for CONFIG A (EDM baseline).\nW =\n1\n\u221aNc\nQK\u22a4,\n(11)\nwhere Q = [q1, . . .]\u22a4, K = [k1, . . .]\u22a4, and V = [v1, . . .]\u22a4\nare matrices containing the query, key, and value vectors for\neach token, derived from the incoming activations using a\n1\u00d71 convolution. The dimensionality of the query and key\nvectors is denoted by Nc.\nThe elements of the weight matrix in Equation 11 can\nequivalently be expressed as dot products between the indi-\nvidual query and key vectors:\nwi,j =\n1\n\u221aNc\n\nqi, kj\n\u000b\n.\n(12)\nEquation 10 is repeated for each attention head, after\nwhich the resulting tokens A are concatenated, transformed\nby a 1\u00d71 convolution, and added back to the main path. The\nnumber of heads Nh is determined by the incoming channel\ncount so that there is one head for each set of 64 channels.\nThe dot product and softmax operations are executed using\n32-bit floating point to avoid overflows, even though the rest\nof the network uses 16-bit floating point.\nThe weights of almost every convolution and fully-\nconnected layer are initialized using He\u2019s uniform init [15],\nand the corresponding biases are also drawn from the same\ndistribution. There are two exceptions, however: The per-\nclass embedding vectors are initialized to N(0, 1), and the\nweights and biases of the last convolution of the residual\nblocks, self-attention blocks, and the final output block are\ninitialized to zero (dark green). This has the effect that\nD\u03b8(x, \u03c3) = cskip(\u03c3) x after initialization.\nTraining loss.\nFollowing EDM [36], the denoising score\nmatching loss for denoiser D\u03b8 on noise level \u03c3 is given by\nL(D\u03b8; \u03c3) = Ey,n\nh\r\rD\u03b8(y + n; \u03c3) \u2212 y\n\r\r2\n2\ni\n,\n(13)\nwhere y \u223c pdata is a clean image sampled from the training\nset and n \u223c N\n\u00000, \u03c32I\n\u0001\nis i.i.d. Gaussian noise.\nThe overall training loss is defined [36] as a weighted\nexpectation of L(D\u03b8; \u03c3) over the noise levels:\nL(D\u03b8) = E\u03c3\n\u0002\n\u03bb(\u03c3)L(D\u03b8; \u03c3)\n\u0003\n(14)\n\u03bb(\u03c3) =\n\u0000\u03c32 + \u03c32\ndata\n\u0001\n/ (\u03c3 \u00b7 \u03c3data)2\n(15)\nln(\u03c3) \u223c N\n\u0000Pmean, P 2\nstd\n\u0001\n,\n(16)\nwhere the distribution of noise levels is controlled by hyper-\nparameters Pmean and Pstd. The weighting function \u03bb(\u03c3) in\n18\nEncoder block\nDecoder block\n1\n768\nEmbedding\n1000\nNoisy\nimage\ncout\ncskip\n+\nDenoised\nimage\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\n+\n\u00d7\nDown 2\u00d72\nSiLU\nDropout\nBias\nConv 3\u00d73\nSplit\n+1\nBias\nLinear\nBias\nConv 1\u00d71\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7Cout\nRout\u00d7Cout\nCout\nDec\nGrpNorm\nSiLU\nBias\nConv 3\u00d73\n+\n\u00d7\nUp 2\u00d72\nSiLU\nDropout\nBias\nConv 3\u00d73\nSplit\n+1\nBias\nLinear\nBias\nConv 1\u00d71\nUp 2\u00d72\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nCout\nConcat\nGrpNorm\nBias\nConv 1\u00d71\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nBias\nConv 1\u00d71\nSplit\n+\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nRin\u00d7Cin\nInput\nOutput\nAttention\nNoise\nlevel\nPosEmb\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\n192\n768\nBias\nBias\nClass\nlabel\nGrpNorm\nGrpNorm\nFP32\nFP32\n642\u00d74\n642\u00d74\nConfig B: Minor improvements\nTo encoder and \ndecoder blocks\nFixed-function\nLearned\nNot always present\nLearned,\nzero init.\ncnoise\nGrpNorm\nConv 3\u00d73\nBias\nSiLU\n642\u00d7192\nOut\n642\u00d74\n768\nEmbedding\n768\nEmbedding\n+\nAttention\nOutput\n+\nOutput\nSkip\nAttention\nSkip\nInput\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\nConv 3\u00d73\n642\u00d7192\n642\u00d74\nIn\nBias\nRin\u00d7Nh\u00d7Nc\u00d73\n1\nNc\nNh = Cin / Nc\nNc = 64\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0002\nAdam \u03b21\n0.9\nFID\n7.24\nMinibatch size\n2048\nLearning rate decay (tref)\n\u221e\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.090\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n100\nModel capacity (Mparams)\n291.8\nMixed-precision (FP16)\npartial\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n100.4\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.59\nFigure 16. Full architecture diagram and hyperparameters for CONFIG B (Minor improvements).\nEquation 15 ensures that \u03bb(\u03c3)L(D\u03b8; \u03c3) = 1 at the begin-\nning of the training, effectively equalizing the contribution\nof each noise level with respect to \u2207\u03b8L(D\u03b8).\nB.2. Minor improvements (CONFIG B)\nSince the baseline configuration (CONFIG A) was not orig-\ninally targeted for latent diffusion, we re-examined the\nhyperparameter choices to obtain an improved baseline\n(CONFIG B). Our new hyperparameters are summarized in\nFigure 16.\nIn order to speed up convergence, we found it beneficial\nto halve the batch size (2048 instead of 4096) while doubling\nthe learning rate (\u03b1ref = 0.0002 instead of 0.0001), and to\nsignificantly reduce Adam\u2019s response time to changes in\ngradient magnitudes (\u03b22 = 0.99 instead of 0.999). These\nchanges had the largest impact towards the beginning of the\ntraining, where the network reconfigures itself for the task at\nhand, but they also helped somewhat towards the end. Fur-\nthermore, we found the self-attention layers at 32\u00d732 resolu-\ntion to be somewhat harmful; removing them improved the\noverall stability while also speeding up the training. In CON-\nFIG B, we also switch from traditional EMA to our power\nfunction averaging profile (Section 3.1), with two averages\nstored per snapshot for high-quality post-hoc reconstruction\n(Section 3.2).\nLoss weighting.\nWith the EDM training loss (Equation 14),\nthe quality of the resulting distribution tends to be quite sen-\nsitive to the choice of Pmean, Pstd, and \u03bb(\u03c3). The role of\nPmean and Pstd is to focus the training effort on the most\nimportant noise levels, whereas \u03bb(\u03c3) aims to ensure that the\ngradients originating from each noise level are roughly of\nthe same magnitude. Referring to Figure 5a of Karras et al.\n[36], the value of L(D\u03b8; \u03c3) behaves somewhat unevenly\nover the course of training: It remains largely unchanged\nfor the lowest and highest noise levels, but drops quickly for\nthe ones in between. Karras et al. [36] suggest setting Pmean\nand Pstd so that the resulting log-normal distribution (Equa-\ntion 16) roughly matches the location of this in-between\nregion. When operating with VAE latents, we have observed\nthat the in-between region has shifted considerably toward\nhigher noise levels compared to RGB images. We thus set\nPmean = \u22120.4 and Pstd = 1.0 instead of \u22121.2 and 1.2, re-\nspectively, to roughly match its location.\nWhile the choice of \u03bb(\u03c3) defined by Equation 15 is\n19\nEncoder block\nDecoder block\nNoisy\nimage\ncskip\n+\nDenoised\nimage\nGrpNorm\nConv 3\u00d73\nSiLU\n642\u00d7192\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nDown 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7Cout\nRout\u00d7Cout\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nUp 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nUp 2\u00d72\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nConcat\n1\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\n192\n768\nGrpNorm\nGrpNorm\n768\n642\u00d74\nConv 3\u00d73\n642\u00d7192\nConcat\n1\n642\u00d74\nFourier\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nConv 1\u00d71\nPixNorm\n+\nRin\u00d7(Cin\u00d73)\nRin\u00d7Nh\u00d7Nc\u00d73\nQ\nK\nV\nRin\u00d7Cin\nInput\nOutput\nSplit\nConv 1\u00d71\nConfig C: Architectural streamlining\nTo encoder and \ndecoder blocks\nIn\nOut\nAttention\nEmbedding\nNoise\nlevel\nClass\nlabel\n1000\n\u00d7\n1000\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nFixed-function\nLearned\nNot always present\nCout\nCout\ncnoise\ncout\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nDec\n642\u00d74\n642\u00d74\n768\nEmbedding\n768\nEmbedding\n+\nAttention\nOutput\n+\nOutput\nSkip\nAttention\nSkip\nInput\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\nNh = Cin / Nc\nNc = 64\n1\nNc\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0002\nAdam \u03b21\n0.9\nFID\n6.96\nMinibatch size\n2048\nLearning rate decay (tref)\n\u221e\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.075\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n100\nModel capacity (Mparams)\n277.8\nMixed-precision (FP16)\nfull\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n100.3\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.58\nFigure 17. Full architecture diagram and hyperparameters for CONFIG C (Architectural streamlining).\nenough to ensure that the gradient magnitudes are balanced\nat initialization, this is no longer true as the training pro-\ngresses. To compensate for the changes in L(D\u03b8; \u03c3) that\nhappen over time, no static choice of \u03bb(\u03c3) is sufficient \u2014 the\nweighting function must be able to adapt its shape dynam-\nically. To achieve this, we treat the integration over noise\nlevels in L(D\u03b8) as a form of multi-task learning. In the\nfollowing, we will first summarize the uncertainty-based\nweighting approach proposed by Kendall et al. [37], defined\nover a finite number of tasks, and then generalize it over a\ncontinuous set of tasks to replace Equation 14.\nUncertainty-based multi-task learning.\nIn a traditional\nmulti-task setting, the model is simultaneously being trained\nto perform multiple tasks corresponding to loss terms\n{L1, L2, . . .}. The naive way to define the overall loss is\nto take a weighted sum over these individual losses, i.e.,\nL = P\ni wiLi. The outcome of the training, however, tends\nto be very sensitive to the choice of weights wi. This choice\ncan become particularly challenging if the balance between\nthe loss terms changes considerably over time. Kendall\net al. [37] propose a principled approach for choosing the\nweights dynamically, based on the idea of treating the model\noutputs as probability distributions and maximizing the re-\nsulting likelihood. For isotropic Gaussians, this boils down\nto associating each loss term Li with an additional train-\nable parameter \u03c3i > 0, i.e., homoscedastic uncertainty, and\ndefining the overall loss as\nL =\nX\ni\n\u0014 1\n2\u03c32\ni\nLi + ln \u03c3i\n\u0015\n(17)\n= 1\n2\nX\ni\n\u0014Li\n\u03c32\ni\n+ ln \u03c32\ni\n\u0015\n.\n(18)\nIntuitively, the contribution of Li is weighted down if the\nmodel is uncertain about task i, i.e., if \u03c3i is high. At the same\ntime, the model is penalized for this uncertainty, encouraging\n\u03c3i to be as low as possible.\nIn practice, it can be quite challenging for typical opti-\nmizers \u2014 such as Adam \u2014 to handle \u03c3i directly due to the\nlogarithm and the requirement that \u03c3i > 0. A more conve-\nnient formula [37] is obtained by rewriting Equation 18 in\nterms of log variance ui = ln \u03c32\ni :\nL = 1\n2\nX\ni\n\u0014 Li\neui + ui\n\u0015\n(19)\n20\nEncoder block\nDecoder block\n1\n768\nEmbedding\n1000\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nDown 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7Cout\nRout\u00d7Cout\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nUp 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nUp 2\u00d72\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nConcat\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\n192\nGrpNorm\nGrpNorm\nFourier\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nConv 1\u00d71\nPixNorm\n+\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\u00d7Cin\nInput\nOutput\nSplit\nConv 1\u00d71\nConfig D: Magnitude-preserving learned layers\nTo encoder and \ndecoder blocks\nAttention\nNoise\nlevel\nClass\nlabel\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nFixed-function\nLearned\nNot always present\nLearned,\nweight norm.\nCout\nCout\ncnoise\n\u00d7\n1000\nNoisy\nimage\ncskip\n+\nDenoised\nimage\nGrpNorm\nConv 3\u00d73\nSiLU\n642\u00d7192\n642\u00d74\nOut\ncout\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nDec\n642\u00d74\n642\u00d74\n768\nEmbedding\n768\nEmbedding\n+\nAttention\nOutput\n+\nOutput\nSkip\nAttention\nSkip\nInput\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\n768\nConv 3\u00d73\n642\u00d7192\nConcat\n1\n642\u00d74\nIn\nNh = Cin / Nc\nNc = 64\nRin\u00d7Nh\u00d7Nc\u00d73\n1\nNc\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0100\nAdam \u03b21\n0.9\nFID\n3.75\nMinibatch size\n2048\nLearning rate decay (tref)\n\u221e\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.225\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n1\nModel capacity (Mparams)\n277.8\nMixed-precision (FP16)\nfull\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n101.2\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.64\nFigure 18. Full architecture diagram and hyperparameters for CONFIG D (Magnitude-preserving learned layers).\n\u221d\nX\ni\n\u0014 Li\neui + ui\n\u0015\n,\n(20)\nwhere we have dropped the constant multiplier 1/2, as it has\nno effect on the optimum.\nContinuous generalization.\nFor the purpose of applying\nEquation 20 to the EDM loss in Equation 14, we consider\neach noise level \u03c3 to represent a different task. This means\nthat instead of a discrete number of tasks, we are faced with\nan infinite continuum of tasks 0 < \u03c3 < \u221e. In accordance to\nEquation 14, we consider the loss corresponding to task \u03c3 to\nbe \u03bb(\u03c3)L(D\u03b8; \u03c3), leading to the following overall loss:\nL(D\u03b8, u) = E\u03c3\n\u0014 \u03bb(\u03c3)\neu(\u03c3) L(D\u03b8; \u03c3) + u(\u03c3)\n\u0015\n,\n(21)\nwhere we employ a continuous uncertainty function u(\u03c3)\ninstead of a discrete set of scalars {ui}.\nIn practice, we implement u(\u03c3) as a simple one-layer\nMLP (not shown in Figure 16) that is trained alongside\nthe main denoiser network and discarded afterwards. The\nMLP evaluates cnoise(\u03c3) as defined by Equation 8, com-\nputes Fourier features for the resulting scalar (see Ap-\npendix B.3), and feeds the resulting feature vector through\na fully-connected layer that outputs one scalar. All practi-\ncal details of the MLP, including initialization, magnitude-\npreserving scaling, and forced weight normalization, fol-\nlow the choices made in our training configurations (Appen-\ndices B.2\u2013B.7).\nIntuitive interpretation.\nTo gain further insight onto the\nmeaning of Equation 21, let us solve for the minimum of\nL(D\u03b8, u) by setting its derivative to zero with respect to\nu(\u03c3):\n0 = dL(D\u03b8, u)\ndu(\u03c3)\n(22)\n=\nd\ndu(\u03c3)\n\u0014 \u03bb(\u03c3)\neu(\u03c3) L(D\u03b8; \u03c3) + u(\u03c3)\n\u0015\n(23)\n= \u2212 \u03bb(\u03c3)\neu(\u03c3) L(D\u03b8; \u03c3) + 1,\n(24)\nwhich leads to\neu(\u03c3) = \u03bb(\u03c3)L(D\u03b8; \u03c3)\n(25)\n21\nEncoder block\nDecoder block\n1\n768\nEmbedding\n1000\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nDown 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7Cout\nRout\u00d7Cout\nGrpNorm\nSiLU\nConv 3\u00d73\n\u00d7\nUp 2\u00d72\nSiLU\nDropout\nConv 3\u00d73\n+1\nLinear\nConv 1\u00d71\nUp 2\u00d72\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nRin\u00d7(Cin+Cskip)\nRout\u00d7Cout\nConcat\nLinear\nSiLU\nLinear\nLinear\n+\nSiLU\n192\nGrpNorm\nGrpNorm\nFourier\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nConv 1\u00d71\nPixNorm\n+\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\u00d7Cin\nInput\nOutput\nSplit\nConv 1\u00d71\nConfig E: Control effective learning rate\nTo encoder and \ndecoder blocks\nAttention\nNoise\nlevel\nClass\nlabel\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nFixed-function\nLearned\nNot always present\nLearned, forced\nweight norm.\nCout\nCout\ncnoise\n\u00d7\n1000\nNoisy\nimage\ncskip\n+\nDenoised\nimage\nGrpNorm\nConv 3\u00d73\nSiLU\n642\u00d7192\n642\u00d74\nOut\ncout\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nDec\n642\u00d74\n642\u00d74\n768\nEmbedding\n768\nEmbedding\n+\nAttention\nOutput\n+\nOutput\nSkip\nAttention\nSkip\nInput\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\n768\nConv 3\u00d73\n642\u00d7192\nConcat\n1\n642\u00d74\nIn\nNh = Cin / Nc\nNc = 64\nRin\u00d7Nh\u00d7Nc\u00d73\n1\nNc\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0100\nAdam \u03b21\n0.9\nFID\n3.02\nMinibatch size\n2048\nLearning rate decay (tref)\n70000\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.145\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n1\nModel capacity (Mparams)\n277.8\nMixed-precision (FP16)\nfull\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n101.2\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.64\nFigure 19. Full architecture diagram and hyperparameters for CONFIG E (Control effective learning rate).\nu(\u03c3) = ln L(D\u03b8; \u03c3) + ln \u03bb(\u03c3).\n(26)\nIn other words, u(\u03c3) effectively keeps track of how L(D\u03b8; \u03c3)\nevolves over time. Plugging Equation 25 back into Equa-\ntion 21, we arrive at an alternative interpretation of the over-\nall training loss:\nL(D\u03b8, u) = E\u03c3\n\u0012 \u03bb(\u03c3)L(D\u03b8; \u03c3)\n\u0002\n\u03bb(\u03c3)L(D\u03b8; \u03c3)\n\u0003 +\n\u0002\nu(\u03c3)\n\u0003\u0013\n(27)\n= E\u03c3\n\u0002L(D\u03b8; \u03c3)\nL(D\u03b8; \u03c3)\n\u0003 +\n\u0002\nE\u03c3u(\u03c3)\n\u0003\n,\n(28)\nwhere the bracketed expressions are treated as constants\nwhen computing \u2207\u03b8L(D\u03b8, u). In other words, Equation 21\neffectively scales the gradients originating from noise level \u03c3\nby the reciprocal of L(D\u03b8; \u03c3), equalizing their contribution\nbetween noise levels and over time.\nNote that the optimum of Equations 21 and 28 with re-\nspect to \u03b8 does not depend on the choice of \u03bb(\u03c3). In theory,\nwe could thus drop \u03bb(\u03c3) altogether, i.e., set \u03bb(\u03c3) = 1. We\nhave tested this in practice and found virtually no impact on\nthe resulting FID or convergence speed. However, we choose\nto keep \u03bb(\u03c3) defined according to Equation 15 as a practical\nsafety precaution; Equation 28 only becomes effective once\nu(\u03c3) has converged reasonably close to the optimum, so\nthe choice of \u03bb(\u03c3) is still relevant at the beginning of the\ntraining.\nB.3. Architectural streamlining (CONFIG C)\nThe network architecture of CONFIG B contains several dif-\nferent types of trainable parameters that each behave in a\nslightly different way: weights and biases of three kinds\n(uniform-initialized, zero-initialized, and self-attention) as\nwell as group normalization scaling parameters and class\nembeddings. Our goal in CONFIG C is eliminate these differ-\nences and make all the remaining parameters behave more\nor less identically. To this end, we make several changes to\nthe architecture that can be seen by comparing Figures 16\nand 17.\nBiases and group normalizations.\nWe have found that\nwe can simply remove all biases with no ill effects. We do\nthis for all convolutions, fully-connected layers, and group\nnormalization layers in the denoiser network as well as in\nthe loss weighting MLP (Equation 21). In theory, this could\npotentially lead to reduced expressive power of the network,\n22\nEncoder block\nDecoder block\n1\n768\nEmbedding\n1000\nSiLU\nConv 3\u00d73\nSiLU\n\u00d7\nSiLU\nConv 3\u00d73\n+1\nRout\u00d7Cout\nRout\u00d7Cout\nConcat\nUp 2\u00d72\nDown 2\u00d72\nRin\u00d7Cin\nRout\u00d7(Cin+Cskip)\nSiLU\nConv 3\u00d73\nConv 1\u00d71\nPixNorm\nLinear\n+\nSiLU\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nConv 1\u00d71\nPixNorm\n+\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\u00d7Cin\nInput\nOutput\nSplit\nConv 1\u00d71\nDropout\nDropout\nConfig F: Remove group normalizations\nTo encoder and \ndecoder blocks\nAttention\nNoise\nlevel\nClass\nlabel\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nFixed-function\nLearned\nNot always present\nLearned, forced\nweight norm.\nCout\nLinear\n192\nFourier\ncnoise\n\u00d7\n1000\nNoisy\nimage\ncskip\n+\nDenoised\nimage\nConv 3\u00d73\n642\u00d7192\n642\u00d74\nOut\ncout\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nDec\n642\u00d74\n642\u00d74\nLinear\n\u00d7\n+1\nLinear\nRout\u00d7Cout\nCout\nConv 3\u00d73\nConv 1\u00d71\n768\nEmbedding\n768\nEmbedding\n+\nAttention\nOutput\n+\nOutput\nSkip\nAttention\nSkip\nInput\nRout\u00d7Cout\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\n768\nConv 3\u00d73\n642\u00d7192\nConcat\n1\n642\u00d74\nIn\nNh = Cin / Nc\nNc = 64\nRin\u00d7Nh\u00d7Nc\u00d73\n1\nNc\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0100\nAdam \u03b21\n0.9\nFID\n2.71\nMinibatch size\n2048\nLearning rate decay (tref)\n70000\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.100\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n1\nModel capacity (Mparams)\n280.2\nMixed-precision (FP16)\nfull\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n102.1\nDropout probability\n10%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.69\nFigure 20. Full architecture diagram and hyperparameters for CONFIG F (Remove group normalizations).\nespecially when sensing the overall scale of the input values.\nEven though we have not seen this to be an issue in practice,\nwe mitigate the danger by concatenating an additional chan-\nnel of constant 1 to the incoming noisy image in the input\nblock (\u201cIn\u201d).\nFurthermore, we remove all other bias-like constructs for\nconsistency; namely, the dynamic conditioning offset de-\nrived from the embedding vector in the encoder and decoder\nblocks and the subtraction of the empirical mean in group\nnormalization. We further simplify the group normalization\nlayers by removing their learned scale parameter. After these\nchanges, the operation becomes\nbx,y,c,g =\nax,y,c,g\nq\n1\nNxNyNc\nP\ni,j,ka2\ni,j,k,g + \u03f5\n,\n(29)\nwhere ax,y,c,g and bx,y,c,g denote the incoming and outgoing\nactivations, respectively, for pixel (x, y), channel c, and\ngroup g, and Nx, Ny, and Nc indicate their corresponding\ndimensions. We set \u03f5 = 10\u22124.\nCosine attention.\nThe 1\u00d71 convolutions responsible for\nproducing the query and key vectors for self-attention behave\nsomewhat differently compared to the other convolutions.\nThis is because the resulting values of wi,j (Equation 12)\nscale quadratically with respect to the overall magnitude\nof the convolution weights, as opposed to linear scaling\nin other convolutions. We eliminate this discrepancy by\nutilizing cosine attention [14, 48, 51]. In practice, we do\nthis by replacing the group normalization, executed right\nbefore the convolution, with pixelwise feature vector nor-\nmalization [32] (\u201cPixelNorm\u201d), executed right after it. This\noperation is defined as\nbx,y,c =\nax,y,c\nq\n1\nNc\nP\nia2\nx,y,i + \u03f5\n,\n(30)\nwhere we use \u03f5 = 10\u22124, similar to Equation 29.\nTo gain further insight regarding the effect of this nor-\nmalization, we note that, ignoring \u03f5, Equation 30 can be\nequivalently written as\nbx,y =\np\nNc\nax,y\n\u2225ax,y\u22252\n.\n(31)\nLet us denote the normalized query and key vectors by\n\u02c6qi and \u02c6kj, respectively. Substituting Equation 31 into Equa-\n23\nEncoder block\nDecoder block\n1\n768\nEmbedding\n1000\nLinear\nMP-SiLU\n192\nMP-Fourier\nMP-Add\nMP-SiLU\nConv 3\u00d73\n\u00d7\nMP-SiLU\nConv 3\u00d73\n+1\nRout\u00d7Cout\nRout\u00d7Cout\nMP-Cat\nUp 2\u00d72\nMP-Add\nMP-Add\nMP-SiLU\nConv 3\u00d73\nDropout\nDropout\nReshape\nMatmul\n\u00d7\nSoftmax\nMatmul\nReshape\nConv 1\u00d71\nPixNorm\nRin\u00d7(Cin\u00d73)\nQ\nK\nV\nRin\u00d7Cin\nInput\nOutput\nSplit\nConv 1\u00d71\nMP-Add\nConfig G: Magnitude-preserving fixed-function layers\nTo encoder and \ndecoder blocks\nAttention\nNoise\nlevel\nClass\nlabel\nRin\n2\u00d7Nh\nRin\n2\u00d7Nh\nFixed-function\nLearned\nNot always present\nLearned, forced\nweight norm.\nCout\nLinear\ncnoise\n\u00d7\n1000\nNoisy\nimage\ncskip\n+\nDenoised\nimage\nConv 3\u00d73\n642\u00d7192\n642\u00d74\nOut\ncout\ncin\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nEnc\nEnc\nEnc\nEnc\nEnc\nEncD\nEncA\nEncA\nEncA\nEncD\nEncA\nEncA\nEncA\nEncD\nDecA\nDecA\nDecA\nDecA\nDec\nDec\nDec\nDec\n642\u00d7192\n322\u00d7192\n162\u00d7384\n82\u00d7576\nDec\nDecU\nDecU\nDecU\nDecA\n642\u00d7192\n642\u00d7192\n642\u00d7192\n322\u00d7384\n322\u00d7384\n322\u00d7384\n162\u00d7576\n162\u00d7576\n162\u00d7576\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n82\u00d7768\n162\u00d7768\n162\u00d7576\n162\u00d7576\n162\u00d7576\n162\u00d7576\n322\u00d7576\n322\u00d7384\n322\u00d7384\n322\u00d7384\n322\u00d7384\n642\u00d7384\n642\u00d7192\n642\u00d7192\n642\u00d7192\n642\u00d7192\nIn\nOut\nEnc\nDec\n642\u00d74\n642\u00d74\nGain\nLinear\nGain\n\u00d7\n+1\nLinear\nRout\u00d7Cout\nCout\nGain\nMP-SiLU\nConv 3\u00d73\nConv 1\u00d71\n768\nEmbedding\n768\nEmbedding\nAttention\nOutput\nOutput\nSkip\nAttention\nSkip\nInput\nDown 2\u00d72\nConv 1\u00d71\nPixNorm\nRout\u00d7Cout\nRin\u00d7Cin\nRout\u00d7(Cin+Cskip)\nRin\u00d7Cin\nRin\u00d7Cin\nInput\n642\u00d74\n642\u00d74\n768\nConv 3\u00d73\n642\u00d7192\nConcat\n1\n642\u00d74\nIn\nNh = Cin / Nc\nNc = 64\nRin\u00d7Nh\u00d7Nc\u00d73\n1\nNc\nRin\u00d7Nh\u00d7Nc\nNumber of GPUs\n32\nLearning rate max (\u03b1ref)\n0.0100\nAdam \u03b21\n0.9\nFID\n2.56\nMinibatch size\n2048\nLearning rate decay (tref)\n70000\nAdam \u03b22\n0.99\nEMA length (\u03c3rel)\n0.130\nDuration (Mimg)\n2147.5\nLearning rate rampup (Mimg)\n10\nLoss scaling\n1\nModel capacity (Mparams)\n280.2\nMixed-precision (FP16)\nfull\nNoise distribution mean (Pmean)\n\u22120.4\nAttention res.\n16, 8\nModel complexity (Gflops)\n102.2\nDropout probability\n0%\nNoise distribution std. (Pstd)\n1.0\nAttention blocks\n15\nSampling cost (Tflops)\n7.70\nFigure 21. Full architecture diagram and hyperparameters for CONFIG G (Magnitude-preserving fixed-function layers).\ntion 12 gives\nwi,j =\n1\n\u221aNc\n\n\u02c6qi, \u02c6kj\n\u000b\n(32)\n=\n1\n\u221aNc\n\u001cp\nNc\nqi\n\u2225qi\u22252\n,\np\nNc\nkj\n\u2225kj\u22252\n\u001d\n(33)\n=\np\nNc cos(\u03d5i,j) ,\n(34)\nwhere \u03d5i,j denotes the angle between qi and kj. In other\nwords, the attention weights are now determined exclusively\nby the directions of the query and key vectors, and their\nlengths no longer have any effect. This curbs the uncon-\ntrolled growth of wi,j during training and enables using 16-\nbit floating point throughout the entire self-attention block.\nOther changes.\nTo unify the behavior of the remaining\ntrainable parameters, we change the zero-initialized layers\n(dark green) and the class embeddings to use the same uni-\nform initialization as the rest of the layers. In order to retain\nthe same overall magnitude after the class embedding layer,\nwe scale the incoming one-hot class labels by\n\u221a\nN so that\nthe result is of unit variance, i.e., 1\nN\nP\ni a2\ni = 1.\nFinally, we replace ADM\u2019s original timestep embedding\nlayer with the more standard Fourier features [77]. Con-\ncretely, we compute feature vector b based on the incoming\nscalar a = cnoise(\u03c3) as\nb =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ncos\n\u00002\u03c0(f1 a + \u03c61)\n\u0001\ncos\n\u00002\u03c0(f2 a + \u03c62)\n\u0001\n...\ncos\n\u00002\u03c0(fNa + \u03c6N)\n\u0001\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb ,\n(35)\nwhere fi \u223c N(0, 1) and \u03c6i \u223c U(0, 1).\n(36)\nAfter initialization, we treat the frequencies {fi} and\nphases {\u03c6i} as constants.\nB.4. Magnitude-preserving learned layers\n(CONFIG D)\nIn CONFIG D, we modify all learned layers according to our\nmagnitude-preserving design principle as shown in Figure 18.\nLet us consider a fully-connected layer with input activations\na = [aj]\u22a4 and output activations b = [bi]\u22a4. The operation\nof the layer is\nb = Wa ,\n(37)\n24\nwhere W = [wi] is a trainable weight matrix. We can equiv-\nalently write this in terms of a single output element:\nbi = wi a.\n(38)\nThe same definition extends to convolutional layers by\napplying Equation 38 independently to each output element.\nIn this case, the elements of a correspond to the activations\nof each input pixel within the support for the convolution\nkernel, i.e., dim(a) = Nj = Nc k2, where Nc is the number\nof input channels and k is the size of the convolution kernel.\nOur goal is to modify Equation 38 so that it preserves the\noverall magnitude of the input activations, without looking at\ntheir actual contents. Let us start by calculating the standard\ndeviation of bi, assuming that {ai} are mutually uncorrelated\nand of equal standard deviation \u03c3a:\n\u03c3bi =\np\nVar[bi]\n(39)\n=\np\nVar[wi a]\n(40)\n=\nrX\njw2\nij Var[aj]\n(41)\n=\nrX\njw2\nij \u03c32a\n(42)\n= \u2225wi\u22252 \u03c3a.\n(43)\nTo make Equation 38 magnitude-preserving, we scale its\noutput so that it has the same standard deviation as the input:\n\u02c6bi = \u03c3a\n\u03c3bi\nbi\n(44)\n=\n\u03c3a\n\u2225wi\u22252 \u03c3a\nwi a\n(45)\n=\nwi\n\u2225wi\u22252\n| {z }\n=: \u02c6wi\na.\n(46)\nIn other words, we simply normalize each wi to unit length\nbefore use. In practice, we introduce \u03f5 = 10\u22124 to avoid\nnumerical issues, similar to Equations 29 and 30:\n\u02c6wi =\nwi\n\u2225wi\u22252 + \u03f5.\n(47)\nGiven that \u02c6bi is now agnostic to the scale of wi, we ini-\ntialize wi,j \u223c N(0, 1) so that the weights of all layers are\nroughly of the same magnitude. This implies that in the\nearly stages of training, when the weights remain close to\ntheir initial magnitude, the updates performed by Adam [39]\nwill also have roughly equal impact across the entire model,\nsimilar to the concept of equalized learning rate [32]. Since\nthe weights are now larger in magnitude, we have to increase\nthe learning rate as well. We therefore set \u03b1ref = 0.0100\ninstead of 0.0002.\nComparison to previous work.\nOur approach is closely\nrelated to weight normalization [68] and weight standardiza-\ntion [59]. Reusing the notation from Equation 46, Salimans\nand Kingma [68] define weight normalization as\n\u02c6wi =\ngi\n\u2225wi\u22252\nwi,\n(48)\nwhere gi is a learned per-channel scaling factor that is ini-\ntialized to one. The original motivation of Equation 48 is to\nreparameterize the weight tensor in order to speed up con-\nvergence, without affecting its expressive power. As such,\nthe value of gi is free to drift over the course of training,\npotentially leading to imbalances in the overall activation\nmagnitudes. Our motivation, on the other hand, is to explic-\nitly avoid such imbalances by removing any direct means for\nthe optimization to change the magnitude of \u02c6bi.\nQiao et al. [59], on the other hand, define weight stan-\ndardization as\n\u02c6wi = wi \u2212 \u00b5i\n\u03c3i\n, where\n(49)\n\u00b5i =\n1\nN\nX\njwi,j\n(50)\n\u03c3i =\nr\n1\nN\nX\njw2\ni,j \u2212 \u00b52\ni + \u03f5 ,\n(51)\nintended to serve as a replacement for batch normalization in\nthe context of micro-batch training. In practice, we suspect\nthat Equation 49 would probably work just as well as Equa-\ntion 46 for our purposes. However, we prefer to keep the\nformula as simple as possible with no unnecessary moving\nparts.\nEffect on the gradients.\nOne particularly useful property\nof Equation 46 is that it projects the gradient of wi to be\nperpedicular to wi itself. Let us derive a formula for the\ngradient of loss L with respect to wi:\n\u2207wiL = \u2207wi \u02c6wi \u00b7 \u2207\u02c6wiL\n(52)\n= \u2207wi\n\u0014\nwi\n\u2225wi\u22252\n\u0015\n\u2207\u02c6wiL.\n(53)\nWe will proceed using the quotient rule\n\u0014f\ng\n\u0015\u2032\n= f \u2032g \u2212 fg\u2032\ng2\n,\n(54)\nwhere\nf = wi,\nf \u2032 = \u2207wiwi\n= I\n(55)\ng = \u2225wi\u22252 , g\u2032 = \u2207wi \u2225wi\u22252 =\nw\u22a4\ni\n\u2225wi\u22252\n.\n(56)\nPlugging this back into Equation 53 gives us\n\u2207wiL =\n\u0014f \u2032g \u2212 fg\u2032\ng2\n\u0015\n\u2207\u02c6wiL\n(57)\n25\n=\n\uf8ee\n\uf8f0I \u2225wi\u22252 \u2212 wi\nw\u22a4\ni\n\u2225wi\u22252\n\u2225wi\u22252\n2\n\uf8f9\n\uf8fb\u2207\u02c6wiL\n(58)\n=\n1\n\u2225wi\u22252\n\"\nI \u2212 wiw\u22a4\ni\n\u2225wi\u22252\n2\n#\n\u2207\u02c6wiL.\n(59)\nThe bracketed expression in Equation 59 corresponds\nto a projection matrix that keeps the incoming vector oth-\nerwise unchanged, but forces it to be perpendicular to wi,\ni.e.,\n\nwi, \u2207wiL\n\u000b\n= 0. In other words, gradient descent\noptimization will not attempt to modify the length of wi\ndirectly. However, the length of wi can still change due to\ndiscretization errors resulting from finite step size.\nB.5. Controlling effective learning rate (CONFIG E)\nIn CONFIG D, we have effectively constrained all weight\nvectors of our model to lie on the unit hypersphere, i.e.,\n\u2225\u02c6wi\u22252 = 1, as far as evaluating D\u03b8(x; \u03c3) is concerned.\nHowever, the magnitudes of the raw weight vectors, i.e.,\n\u2225wi\u22252, are still relevant during training due to their effect on\n\u2207wiL (Equation 59). Even though we have initialized wi so\nthat these magnitudes are initially balanced across the layers,\nthere is nothing to prevent them from drifting away from this\nideal over the course of training. This is problematic since\nthe relative impact of optimizer updates, i.e., the effective\nlearning rate, can vary uncontrollably across the layers and\nover time. In CONFIG E, we eliminate this drift through\nforced weight normalization as shown in Figure 19, and gain\nexplicit control over the effective learning rate.\nGrowth of weight magnitudes.\nAs noted by Salimans and\nKingma [68], Equations 46 and 59 have the side effect that\nthey cause the norm of wi to increase monotonically after\neach training step. As an example, let us consider standard\ngradient descent with learning rate \u03b1. The update rule is\ndefined as\nw\u2032\ni = wi \u2212 \u03b1\u2207wiL\n(60)\nwi \u2190 w\u2032\ni.\n(61)\nWe can use the Pythagorean theorem to calculate the\nnorm of the updated weight vector w\u2032\ni:\n\r\rw\u2032\ni\n\r\r2\n2 =\n\r\rwi \u2212 \u03b1\u2207wiL\n\r\r2\n2\n(62)\n=\n\r\rwi\n\r\r2\n2 + \u03b12\r\r\u2207wiL\n\r\r2\n2 \u2212 2\u03b1\n\nwi,\u2207wiL\n\u000b\n|\n{z\n}\n= 0\n(63)\n=\n\r\rwi\n\r\r2\n2 + \u03b12\r\r\u2207wiL\n\r\r2\n2\n(64)\n\u2265\n\r\rwi\n\r\r2\n2.\n(65)\nIn other words, the norm of wi will necessarily increase at\neach step unless \u2207wiL = 0. A similar phenomenon has been\nobserved with optimizers like Adam [39], whose updates\nAlgorithm 1 PyTorch code for forced weight normalization.\ndef normalize(x, eps=1e\u22124):\ndim = list(range(1, x.ndim))\nn = torch.linalg.vector_norm(x, dim=dim, keepdim=True)\nalpha = np.sqrt(n.numel() / x.numel())\nreturn x / torch.add(eps, n, alpha=alpha)\nclass Conv2d(torch.nn.Module):\ndef __init__(self, C_in, C_out, k):\nsuper().__init__()\nw = torch.randn(C_out, C_in, k, k)\nself.weight = torch.nn.Parameter(w)\ndef forward(self, x):\nif self.training:\nwith torch.no_grad():\nself.weight.copy_(normalize(self.weight))\nfan_in = self.weight[0].numel()\nw = normalize(self.weight) / np.sqrt(fan_in)\nx = torch.nn.functional.conv2d(x, w, padding=\u2019same\u2019)\nreturn x\ndo not maintain strict orthogonality, as well as in numerous\nscenarios that do not obey Equation 46 exactly. The effect is\napparent in our CONFIG C (Figure 3) as well.\nForced weight normalization.\nGiven that the normaliza-\ntion and initialization discussed in Appendix B.4 are already\ngeared towards constraining the weight vectors to a hyper-\nsphere, we take this idea to its logical conclusion and perform\nthe entire optimization strictly under such a constraint.\nConcretely, we require \u2225wi\u22252 =\np\nNj to be true for each\nlayer after each training step, where Nj is the dimension of\nwi, i.e., the fan-in. Equation 59 already constrains \u2207wiL to\nlie on the tangent plane with respect to this constraint; the\nonly missing piece is to guarantee that the constraint itself is\nsatisfied by Equation 61. To do this, we modify the formula\nto forcefully re-normalize w\u2032\ni before assigning it back to wi:\nwi \u2190\np\nNj\nw\u2032\ni\n\u2225w\u2032\ni\u22252\n.\n(66)\nNote that Equation 66 is agnostic to the exact definition\nof w\u2032\ni, so it is readily compatible with most of the commonly\nused optimizers. In theory, it makes no difference whether\nthe normalization is done before or after the actual training\nstep. In practice, however, the former leads to a very simple\nand concise PyTorch implementation, shown in Algorithm 1.\nLearning rate decay.\nLet us step back and consider CON-\nFIG D again for a moment, focusing on the overall effect\nthat \u2225wi\u22252 had on the training dynamics. Networks where\nmagnitudes of weights have no effect on activations have\npreviously been studied by, e.g., van Laarhoven [43]. In\nthese networks, the only meaningful progress is made in\nthe angular direction of weight vectors. This has two con-\nsequences for training dynamics: First, the gradients seen\n26\nby the optimizer are inversely proportional to the weight\nmagnitude. Second, the loss changes slower at larger mag-\nnitudes, as more distance needs to be covered for the same\nangular change. Effectively, both of these phenomena can\nbe interpreted as downscaling the effective learning rate as\na function of the weight magnitude. Adam [39] counteracts\nthe first effect by approximately normalizing the gradient\nmagnitudes, but it does not address the second.\nFrom this perspective, we can consider CONFIG D to have\neffectively employed an implicit learning rate decay: The\nlarger the weights have grown (Figure 3), the smaller the\neffective learning rate. In general, learning rate decay is\nconsidered desirable in the sense that it enables the training\nto converge closer and closer to the optimum despite the\nstochastic nature of the gradients [39, 68]. However, we ar-\ngue that the implicit form of learning rate decay imposed by\nEquation 65 is not ideal, because it can lead to uncontrollable\nand unequal drift between layers.\nWith forced weight normalization in CONFIG E and on-\nwards, the drift is eliminated and the effective learning rate\nis directly proportional to the specified learning rate \u03b1. Thus,\nin order to have the learning rate decay, we have to explicitly\nmodify the value of \u03b1 over time. We choose to use the com-\nmonly advocated inverse square root decay schedule [39]:\n\u03b1(t) =\n\u03b1ref\np\nmax(t/tref, 1)\n,\n(67)\nwhere the learning rate initially stays at \u03b1ref and then starts\ndecaying after tref training iterations. The constant learning\nrate schedule of CONFIGS A\u2013D can be seen as a special case\nof Equation 67 with tref = \u221e.\nIn the context of Table 1, we use \u03b1ref = 0.0100 and\ntref = 70000. We have, however, found that the optimal\nchoices depend heavily on the capacity of the network as\nwell as the dataset (see Table 6).\nDiscussion.\nIt is worth noticing that we normalize the\nweight vectors twice during each training step: first to obtain\n\u02c6wi in Equation 46 and then to constrain w\u2032\ni in Equation 66.\nThis is also reflected by the two calls to normalize() in\nAlgorithm 1.\nThe reason why Equation 46 is still necessary despite\nEquation 66 is that it ensures that Adam\u2019s variance esti-\nmates are computed for the actual tangent plane steps. In\nother words, Equation 46 lets Adam \u201cknow\u201d that it is sup-\nposed to operate under the fixed-magnitude constraint. If we\nused Equation 66 alone, without Equation 46, the variance\nestimates would be corrupted by the to-be erased normal\ncomponent of the raw gradient vectors, leading to consid-\nerably smaller updates of an uncontrolled magnitude. See\nFigure 22 for an illustration.\nFurthermore, we intentionally force the raw weights wi\nto have the norm\np\nNj, while weight normalization further\n(a) Forced WN only\n(b) Forced + standard WN\nFigure 22. Illustration of the importance of performing \u201cstandard\u201d\nweight normalization in addition to forcing the weights to a prede-\nfined norm. The dashed circle illustrates Adam\u2019s target variance for\nupdates \u2014 the proportions are greatly exaggerated and the effects of\nmomentum are ignored. (a) Forced weight normalization without\nthe standard weight normalization. The raw weight vector wi is up-\ndated by adding the gradient \u2207wi after being scaled by Adam, after\nwhich the result is normalized back to the hypersphere (solid arc)\nyielding new weight vector w\u2032\ni. Adam\u2019s variance estimate includes\nthe non-tangent component of the gradient, and the resulting weight\nupdate is significantly smaller than intended. (b) With standard\nweight normalization, the gradient \u2207wi is obtained by projecting\nthe raw gradient \u2207\u02c6\nwi onto the tangent plane perpendicular to wi.\nAdam\u2019s variance estimate now considers this projected gradient,\nresulting in the correct step size; the effect of normalization after\nupdate is close to negligible from a single step\u2019s perspective.\nscales them to norm 1. The reason for this subtle but im-\nportant difference is, again, compatibility with the Adam\noptimizer. Adam approximately normalizes the gradient up-\ndates so that they are proportional to\np\nNj. We normalize\nthe weights to the same scale, so that the relative magnitude\nof the update becomes independent of Nj. This eliminates\nan implicit dependence between the learning rate and the\nlayer size. Optimizers like LARS [85] and Fromage [3] build\non a similar motivation, and explicitly scale the norm of the\ngradient updates to a fixed fraction of the weight norm.\nFinally, Equation 46 is also quite convenient due to its\npositive interaction with EMA. Even though the raw values\nof wi are normalized at each training step by Equation 66,\ntheir weighted averages are not. To correctly account for our\nfixed-magnitude constraint, the averaging must also happen\nalong the surface of the corresponding hypersphere. How-\never, we do not actually need to change the averaging itself\nin any way, because this is already taken care of by Equa-\ntion 46: Even if the magnitudes of the weight vectors change\nconsiderably as a result of averaging, they are automatically\nre-normalized upon use.\nPrevious work.\nSeveral previous works have analyzed the\nconsequences of weight magnitude growth under different\nsettings and proposed various remedies. Weight decay has\noften been identified as a solution for keeping the magni-\ntudes in check, and its interplay with different normalization\nschemes and optimizers has been studied extensively [26, 42\u2013\n45, 62, 82, 87]. Cho and Lee [10] and van Laarhoven [43]\nconsider more direct approaches where the weights are di-\n27\nrectly constrained to remain in the unit norm hypersphere,\neliminating the growth altogether. Arpit et al. [1] also nor-\nmalize the weights directly, motivated by a desire to reduce\nthe parameter space. Various optimizers [3, 4, 47, 85, 86]\nalso aim for similar effects through weight-relative scaling\nof the gradient updates.\nAs highlighted by the above discussion, the success of\nthese approaches can depend heavily on various small but\nimportant nuances that may not be immediately evident. As\nsuch, we leave a detailed comparison of these approaches as\nfuture work.\nB.6. Removing group normalizations (CONFIG F)\nIn CONFIG F, our goal is to remove the group normalization\nlayers that may negatively impact the results due to the fact\nthat they operate across the entire image. We also make a\nfew minor simplifications to the architecture. These changes\ncan be seen by comparing Figures 19 and 20.\nDangers of global normalization.\nAs has been previously\nnoted [34, 35], global normalization that operates across the\nentire image should be used cautiously. It is firmly at odds\nwith the desire for the model to behave consistently across\ngeometric transformations [35, 79] or when synthesizing\nobjects in different contexts. Such consistency is easiest\nto achieve if the internal representations of the image con-\ntents are capable of being as localized as they need to be,\nbut global normalization entangles the representations of ev-\nery part of the image by eliminating the first-order statistics\nacross the image. Notably, while attention allows the repre-\nsentations to communicate with each other in a way that best\nfits the task, global normalization forces communication to\noccur, with no way for individual features to avoid it.\nThis phenomenon has been linked to concrete image arti-\nfacts in the context of GANs. Karras et al. [34] found that\nthe AdaIN operation used in StyleGAN was destroying vital\ninformation, namely the relative scales of different feature\nmaps, which the model counteracted by creating strong lo-\ncalized spikes in the activations. These spikes manifested\nas artifacts, and were successfully eliminated by remov-\ning global normalization operations. In a different context,\nBrock et al. [8] show that normalization is not necessary for\nobtaining high-quality results in image classification. We\nsee no reason why it should be necessary or even beneficial\nin diffusion models, either.\nOur approach.\nHaving removed the drift in activation\nmagnitudes, we find that we can simply remove all group\nnormalization layers with no obvious downsides. In particu-\nlar, doing this for the decoder improves the FID considerably,\nwhich we suspect to be related to the fact that the absolute\nscale of the individual output pixels is quite important for\nthe training loss (Equation 13). The network has to start\npreparing the correct scales towards the end of the U-Net,\nand explicit normalization is likely to make this more chal-\nlenging.\nEven though explicit normalization is no longer strictly\nnecessary, we have found that we can further improve the\nresults slightly through pixelwise feature vector normaliza-\ntion (Equation 30). Our hypothesis is that a small amount\nof normalization helps by counteracting correlations that\nwould otherwise violate the independence assumption be-\nhind Equation 43. We find that the best results are obtained\nby normalizing the incoming activations at the beginning of\neach encoder block. This guarantees that the magnitudes on\nthe main path remain standardized despite the series of cu-\nmulative adjustments made by the residual and self-attention\nblocks. Furthermore, this also appears to help in terms of\nstandardizing the magnitudes of the decoder \u2014 presumably\ndue to the presence of the U-Net skip connections.\nArchitectural simplifications.\nIn addition to reworking\nthe normalizations, we make four minor simplifications to\nother parts of the architecture:\n1. Unify the upsampling and downsampling operations of\nthe encoder and decoder blocks by placing them onto the\nmain path.\n2. Slightly increase the expressive power of the encoder\nblocks by moving the 1\u00d71 convolution to the beginning\nof the main path.\n3. Remove the SiLU activation in the final output block.\n4. Remove the second fully-connected layer in the embed-\nding network.\nThese changes are more or less neutral in terms of the FID,\nbut we find it valuable to keep the network as simple as\npossible considering future work.\nB.7. Magnitude-preserving fixed-function layers\n(CONFIG G)\nIn CONFIG G, we complete the effort that we started in\nCONFIG D by extending our magnitude-preserving design\nprinciple to cover the remaining fixed-function layers in\naddition to the learned ones. The exact set of changes can\nbe seen by comparing Figures 20 and 21.\nWe will build upon the concept of expected magnitude\nthat we define by generalizing Equation 3 for multivariate\nrandom variable a:\nM[a] =\nv\nu\nu\nt 1\nNa\nNa\nX\ni=1\nE\n\u0002\na2\ni\n\u0003\n.\n(68)\nIf the elements of a have zero mean and equal variance,\nwe have M[a]2 = Var[ai].\nIf a is non-random, Equa-\n28\ntion 68 simplifies to M[a] = \u2225a\u22252 /\u221aNa. We say that a\nis standardized iff M[a] = 1.\nConcretely, we aim to achieve two things: First, every\ninput to the network should be standardized, and second,\nevery operation in the network should be such that if its\ninput is standardized, the output is standardized as well. If\nthese two requirements are met, it follows that all activations\nthroughout the entire network are standardized.\nSimilar to Appendix B.4, we wish to avoid having to look\nat the actual values of activations, which necessitates making\ncertain simplifying statistical assumptions about them. Even\nthough these assumptions are not strictly true in practice, we\nfind that the end result is surprisingly close to our ideal, as\ncan be seen in the \u201cActivations (mean)\u201d plot for CONFIG G\nin Figure 14.\nFourier features.\nConsidering the inputs to our network,\nthe noisy image and the class label are already standardized\nby virtue of having been scaled by cin(\u03c3) (Equation 7) and\n\u221a\nN (Appendix B.3), respectively. The Fourier features (Ap-\npendix B.3), however, are not. Let us compute the expected\nmagnitude of b (Equation 35) with respect to the frequencies\nand phases (Equation 36):\nM[b]2 =\n1\nNb\nNb\nX\ni=1\nE\n\u0014\u0010\ncos\n\u00002\u03c0(fia + \u03c6i)\n\u0001\u00112\u0015\n(69)\n= E\n\u0014\u0010\ncos\n\u00002\u03c0(f1a + \u03c61)\n\u0001\u00112\u0015\n(70)\n= E\nh\u0000cos(2\u03c0\u03c61)\n\u00012i\n(71)\n= E\nh\n1\n2\n\u00001 + cos(4\u03c0\u03c61)\n\u0001i\n(72)\n=\n1\n2 + 1\n2 E\n\u0002\ncos(4\u03c0\u03c61)\n\u0003\n|\n{z\n}\n= 0\n(73)\n=\n1\n2.\n(74)\nTo standardize the output, we thus scale Equation 35 by\n1/M[b] =\n\u221a\n2:\nMP-Fourier(a) =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u221a\n2 cos\n\u00002\u03c0(f1 a + \u03c61)\n\u0001\n\u221a\n2 cos\n\u00002\u03c0(f2 a + \u03c62)\n\u0001\n...\n\u221a\n2 cos\n\u00002\u03c0(fNa + \u03c6N)\n\u0001\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb .\n(75)\nSiLU.\nSimilar reasoning applies to the SiLU nonlinearity\n(Equation 9) as well, used throughout the network. Assum-\ning that a \u223c N(0, I):\nM\n\u0002\nsilu(a)\n\u00032 =\n1\nNa\nNa\nX\ni=1\nE\nh\u0000silu(ai)\n\u00012i\n(76)\n= E\n\"\u0012\na1\n1 + e\u2212a1\n\u00132#\n(77)\n=\nZ \u221e\n\u2212\u221e\nN(x; 0, 1) x2\n(1 + e\u2212x)2\ndx\n(78)\n\u2248 0.3558\n(79)\nM\n\u0002\nsilu(a)\n\u0003\n\u2248\n\u221a\n0.3558 \u2248 0.596.\n(80)\nDividing the output accordingly, we obtain\nMP-SiLU(a) = silu(a)\n0.596\n=\n\u0014\nai\n0.596 \u00b7 (1 + e\u2212ai)\n\u0015\n. (81)\nSum.\nLet us consider the weighted sum of two random\nvectors, i.e., c = waa + wbb. We assume that the elements\nwithin each vector have equal expected magnitude and that\nE[aibi] = 0 for every i. Now,\nM[c]2 =\n1\nNc\nNc\nX\ni=1\nE\n\u0002\n(waai + wbbi)2\u0003\n(82)\n=\n1\nNc\nNc\nX\ni=1\nE\n\u0002\nw2\naa2\ni + w2\nbb2\ni + 2wawbaibi\n\u0003\n(83)\n=\n1\nNc\nNc\nX\ni=1\nh\nw2\naE\n\u0002\na2\ni\n\u0003\n| {z }\n= M[a]2\n+w2\nb E\n\u0002\nb2\ni\n\u0003\n| {z }\n= M[b]2\n+2wawb E\n\u0002\naibi\n\u0003\n| {z }\n= 0\ni\n(84)\n=\n1\nNc\nNc\nX\ni=1\n\u0002\nw2\naM[a]2 + w2\nbM[b]2\u0003\n(85)\n= w2\naM[a]2 + w2\nbM[b]2.\n(86)\nIf the inputs are standardized, Equation 86 further simpli-\nfies to M[c] =\np\nw2a + w2\nb. A standardized version of c is\nthen given by\n\u02c6c =\nc\nM[c] = waa + wbb\np\nw2a + w2\nb\n.\n(87)\nNote that Equation 87 is agnostic to the scale of wa and\nwb. Thus, we can conveniently define them in terms of blend\nfactor t \u2208 [0, 1] that can be adjusted on a case-by-case basis.\nSetting wa = (1 \u2212 t) and wb = t, we arrive at our final\ndefinition:\nMP-Sum(a, b, t) =\n(1 \u2212 t) a + t b\np\n(1 \u2212 t)2 + t2 .\n(88)\nWe have found that the best results are obtained by setting\nt = 0.3 in the encoder, decoder, and self-attention blocks,\nso that the residual path contributes 30% to the result while\nthe main path contributes 70%. In the embedding network\nt = 0.5 seems to work well, leading to equal contribution\nbetween the noise level and the class label.\n29\nConcatenation.\nNext, let us consider the concatenation of\ntwo random vectors a and b, scaled by constants wa and wb,\nrespectively. The result is given by c = waa \u2295 wbb, which\nimplies that\nM[c]2 =\nPNc\ni=1 E\n\u0002\nc2\ni\n\u0003\nNc\n(89)\n=\nPNa\ni=1 E\n\u0002\nw2\naa2\ni\n\u0003\n+ PNb\ni=1 E\n\u0002\nw2\nbb2\ni\n\u0003\nNa + Nb\n(90)\n= w2\naNaM[a]2 + w2\nbNbM[b]2\nNa + Nb\n.\n(91)\nNote that the contribution of a and b in Equation 91\nis proportional to Na and Nb, respectively. If Na \u226b Nb,\nfor example, the result will be dominated by a while the\ncontribution of b is largely ignored. In our architecture\n(Figure 21), this situation can arise at the beginning of the\ndecoder blocks when the U-Net skip connection is concate-\nnated into the main path. We argue that the balance between\nthe two branches should be treated as an independent hy-\nperparameter, as opposed to being tied to their respective\nchannel counts.\nWe first consider the case where we require the two inputs\nto contribute equally, i.e.,\nw2\naNaM[a]2 = w2\nbNbM[b]2 = C2,\n(92)\nwhere C is an arbitrary constant. Solving for wa and wb:\nwa =\nC\nM[a] \u00b7\n1\n\u221aNa\n(93)\nwb =\nC\nM[b] \u00b7\n1\n\u221aNb\n(94)\nNext, we introduce blend factor t \u2208 [0, 1] to allow adjusting\nthe balance between a and b on a case-by-case basis, similar\nto Equation 88:\n\u02c6wa = wa (1 \u2212 t) =\nC\nM[a] \u00b7 1 \u2212 t\n\u221aNa\n(95)\n\u02c6wb = wb t\n=\nC\nM[b] \u00b7\nt\n\u221aNb\n.\n(96)\nIf the inputs are standardized, i.e., M[a] = M[b] = 1,\nwe can solve for the value of C that leads to the output being\nstandardized as well:\n1 = M[c]2\n(97)\n=\n\u02c6w2\naNaM[a]2 + \u02c6w2\nbNbM[b]2\nNa + Nb\n(98)\n=\n\u02c6w2\naNa + \u02c6w2\nbNb\nNa + Nb\n(99)\n=\nh\nC2 (1\u2212t)2\nNa\ni\nNa +\nh\nC2 t2\nNb\ni\nNb\nNa + Nb\n(100)\n= C2 (1 \u2212 t)2 + t2\nNa + Nb\n,\n(101)\nwhich yields\nC =\ns\nNa + Nb\n(1 \u2212 t)2 + t2 .\n(102)\nCombining Equation 102 with Equations 95 and 96, we\narrive at our final definition:\nMP-Cat(a, b, t) =\ns\nNa + Nb\n(1 \u2212 t)2 + t2 \u00b7\n\"\n1 \u2212 t\n\u221aNa\na\u2295\nt\n\u221aNb\nb\n#\n.\n(103)\nIn practice, we have found that the behavior of the model\nis quite sensitive to the choice of t and that the best results are\nobtained using t = 0.5. We hope that the flexibility offered\nby Equation 103 may prove useful in the future, especially\nin terms of exploring alternative network architectures.\nLearned gain.\nWhile our goal of standardizing activations\nthroughout the network is beneficial for the training dynam-\nics, it can also be harmful in cases where it is necessary to\nhave M[a] \u0338= 1 in order to satisfy the training loss.\nWe identify two such instances in our network: the raw\npixels (F\u03b8) produced by the final output block (\u201cOut\u201d), and\nthe learned per-channel scaling in the encoder and decoder\nblocks. In order to allow M[a] to deviate from 1, we intro-\nduce a simple learned scaling layer at these points:\nGain(a) = g a,\n(104)\nwhere g is a learned scalar that is initialized to 0. We have not\nfound it necessary to introduce multiple scaling factors on\na per-channel, per-noise-level, or per-class basis. Note that\ng = 0 implies F\u03b8(x; \u03c3) = 0, meaning that D\u03b8(x; \u03c3) = x at\ninitialization, similar to CONFIGS A\u2013B (see Appendix B.1).\nC. Post-hoc EMA details\nAs discussed in Section 3, our goal is to be able to select the\nEMA length, or more generally, the model averaging profile,\nafter a training run has completed. This is achieved by stor-\ning a number of pre-averaged models during training, after\nwhich these pre-averaged models can be linearly combined\nto obtain a model whose averaging profile has the desired\nshape and length.\nAs a related contribution, we present the power function\nEMA profile that automatically scales according to training\ntime and has zero contribution at t = 0.\nIn this section, we first derive the formulae related to\nthe traditional exponential EMA from first principles, after\nwhich we do the same for the power function EMA. We then\ndiscuss how to determine the appropriate linear combination\nof pre-averaged models stored in training snapshots in order\nto match a given averaging profile, and specifically, to match\nthe power function EMA with a given length.\n30\nC.1. Definitions\nLet us denote the weights of the network as a function of\ntraining time by \u03b8(t), so that \u03b8(0) corresponds to the ini-\ntial state and \u03b8(tc) corresponds to the most recent state. tc\nindicates the current training time in arbitrary units, e.g.,\nnumber of training iterations. As always, the training itself\nis performed using \u03b8(tc), but evaluation and other down-\nstream tasks use a weighted average instead, denoted by\n\u02c6\u03b8(tc). This average is typically defined as a sum over the\ntraining iterations:\n\u02c6\u03b8(tc) =\ntc\nX\nt=0\nptc(t) \u03b8(t),\n(105)\nwhere ptc is a time-dependent response function that sums\nto one, i.e., P\nt ptc(t) = 1.\nInstead of operating with discretized time steps, we sim-\nplify the derivation by treating \u03b8, \u02c6\u03b8, and ptc as continuous\nfunctions defined over t \u2208 R\u22650. A convenient way to gen-\neralize Equation 105 to this case is to interpret ptc as a\ncontinuous probability distribution and define \u02c6\u03b8(tc) as the\nexpectation of \u03b8(tc) with respect to that distribution:\n\u02c6\u03b8(tc) = Et\u223cptc(t)\n\u0002\n\u03b8(t)\n\u0003\n.\n(106)\nConsidering the definition of ptc(t), we can express a\nlarge class of practically relevant response functions in terms\nof a canonical response function f(t):\nptc(t) =\n(\nf(t) / g(tc)\nif 0 \u2264 t \u2264 tc\n0\notherwise\n,\n(107)\nwhere g(tc) =\nZ tc\n0\nf(t) dt.\n(108)\nTo characterize the properties, e.g., length, of a given re-\nsponse function, we consider its standard distribution statis-\ntics:\n\u00b5tc = E[t] and \u03c3tc =\np\nVar[t] for t \u223c ptc(t). (109)\nThese two quantities have intuitive interpretations: \u00b5tc indi-\ncates the average delay imposed by the response function,\nwhile \u03c3tc correlates with the length of the time period that is\naveraged over.\nC.2. Traditional EMA profile\nThe standard choice for the response function is the ex-\nponential moving average (EMA) where ptc decays expo-\nnentially as t moves farther away from tc into the past, of-\nten parameterized by EMA half-life \u03bb. In the context of\nEquation 107, we can express such exponential decay as\nptc(t) = f(t)/g(tc), where\nf(t) =\n(\n2t/\u03bb\nif t > 0\n\u03bb\nln 2 \u03b4(t)\notherwise\n(110)\ng(tc) = \u03bb 2tc/\u03bb\nln 2 ,\n(111)\nand \u03b4(t) is the Dirac delta function.\nThe second row of Equation 110 highlights an inconve-\nnient aspect about the traditional EMA. The exponential\nresponse function is infinite in the sense that it expects to be\nable to consult historical values of \u03b8 infinitely far in the past,\neven though the training starts at t = 0. Consistent with pre-\nvious work, we thus deposit the probability mass that would\notherwise appear at t < 0 onto t = 0 instead, corresponding\nto the standard practice of initializing the accumulated EMA\nweights to network\u2019s initial weights.\nThis implies that unless \u03bb \u226a tc, the averaged weights\n\u02c6\u03b8(tc) end up receiving a considerable contribution from the\ninitial state \u03b8(0) that is, by definition, not meaningful for the\ntask that the model is being trained for.\nC.3. Tracking the averaged weights during training\nIn practice, the value of \u02c6\u03b8(tc) is computed during training as\nfollows. Suppose that we are currently at time tc and know\nthe current \u02c6\u03b8(tc). We then run one training iteration to arrive\nat tn = tc + \u2206t so that the updated weights are given by\n\u03b8(tn). Here \u2206t denotes the length of the training step in\nwhatever units are being used for t.\nTo define \u03b8(t) for all values of t, we consider it to be a\npiecewise constant function so that \u03b8(t) = \u03b8(tn) for every\ntc < t \u2264 tn. Let us now write the formula for \u02c6\u03b8(tn) in terms\nof Equations 106 and 107:\n\u02c6\u03b8(tn) = Et\u223cptn(t)\n\u0002\n\u03b8(t)\n\u0003\n(112)\n=\nZ \u221e\n\u2212\u221e\nptn(t) \u03b8(t) dt\n(113)\n=\nZ tn\n0\nf(t)\ng(tn)\u03b8(t) dt\n(114)\n=\nZ tc\n0\nf(t)\ng(tn)\u03b8(t) dt +\nZ tn\ntc\nf(t)\ng(tn)\u03b8(t) dt\n(115)\n= g(tc)\ng(tn)\n| {z }\n=: \u03b2(tn)\nZ tc\n0\nf(t)\ng(tc)\u03b8(t)dt\n|\n{z\n}\n= \u02c6\u03b8(tc)\n+\u03b8(tn)\ng(tn)\nZ tn\ntc\nf(t)dt\n|\n{z\n}\n= g(tn)\u2212g(tc)\n(116)\n= \u03b2(tn) \u02c6\u03b8(tc) + \u03b8(tn)\ng(tn)\n\u0000g(tn) \u2212 g(tc)\n\u0001\n(117)\n= \u03b2(tn) \u02c6\u03b8(tc) +\n\u0014\n1 \u2212 g(tc)\ng(tn)\n| {z }\n= \u03b2(tn)\n\u0015\n\u03b8(tn)\n(118)\n= \u03b2(tn) \u02c6\u03b8(tc) +\n\u00001 \u2212 \u03b2(tn)\n\u0001\n\u03b8(tn).\n(119)\nThus, after each training iteration, we must linearly interpo-\nlate \u02c6\u03b8 toward \u03b8 by \u03b2(tn). In the case of exponential EMA,\n31\nt = 0\n0.2\n0.4\n0.6\n0.8\n1\n0.2\n0.4\n0.6\n0.8\nf(t)\n\u03c3rel = 0.25, \u03b3 =\n0.72\n\u03c3rel = 0.20, \u03b3 =\n1.83\n\u03c3rel = 0.15, \u03b3 =\n3.56\n\u03c3rel = 0.10, \u03b3 =\n6.94\n\u03c3rel = 0.05, \u03b3 = 16.97\nFigure 23. Examples of the canonical response function of our\npower function EMA profile (Equation 121). Each curve corre-\nsponds to a particular choice for the relative standard deviation \u03c3rel;\nthe corresponding exponent \u03b3 is calculated using Algorithm 2.\n\u03b2(tn) is constant and, consulting Equation 111, given by\n\u03b2(tn) = g(tc)\ng(tn) = 2tc/\u03bb\n2tn/\u03bb = 2\u2212\u2206t/\u03bb.\n(120)\nC.4. Power function EMA profile\nIn Section 2, we make two observations that highlight the\nproblematic aspects of the exponential EMA profile. First, it\nis generally beneficial to employ unconventionally long av-\nerages, to the point where \u03bb \u226a tc is no longer true. Second,\nthe length of the response function should increase over the\ncourse of training proportional to tc. As such, the definition\nof f(t) in Equation 110 is not optimal for our purposes.\nThe most natural requirement for f(t) is that it should\nbe self-similar over different timescales, i.e., f(c t) \u221d f(t)\nfor any positive stretching factor c. This implies that the\nresponse functions for different values of tc will also be\nstretched versions of each other; if tc doubles, so does\n\u03c3tc. Furthermore, we also require that f(0) = 0 to avoid\nmeaningless contribution from \u03b8(0). These requirements are\nuniquely satisfied, up to constant scaling, by the family of\npower functions ptc(t) = f(t)/g(tc), where\nf(t) = t\u03b3\nand\ng(tc) =\nt\u03b3+1\nc\n\u03b3 + 1.\n(121)\nThe constant \u03b3 > 0 controls the overall amount of averaging\nas illustrated in Figure 23.\nConsidering the distribution statistics of our response\nfunction, we notice that ptc is equal to the beta distribution\nwith \u03b1 = \u03b3 + 1 and \u03b2 = 1, stretched along the t-axis by tc.\nThe relative mean and standard deviation with respect to tc\nare thus given by\n\u00b5rel = \u00b5tc\ntc\n= \u03b3 + 1\n\u03b3 + 2\n(122)\nAlgorithm 2 NumPy code for converting \u03c3rel to \u03b3.\ndef sigma_rel_to_gamma(sigma_rel):\nt = sigma_rel ** \u22122\ngamma = np.roots([1, 7, 16 \u2212 t, 12 \u2212 t]).real.max()\nreturn gamma\n\u03c3rel = \u03c3tc\ntc\n=\ns\n\u03b3 + 1\n(\u03b3 + 2)2 (\u03b3 + 3).\n(123)\nIn our experiments, we choose to use \u03c3rel as the primary\nway of defining and reporting the amount of averaging, in-\ncluding the EDM baseline (CONFIG A) that employs the\ntraditional EMA (Equation 110). Given \u03c3rel, we can obtain\nthe value of \u03b3 to be used with Equation 121 by solving a 3rd\norder polynomial equation and taking the unique positive\nroot\n\u03b3 + 1\n(\u03b3 + 2)2 (\u03b3 + 3) = \u03c32\nrel\n(124)\n(\u03b3 + 2)2 (\u03b3 + 3) \u2212 (\u03b3 + 1) \u03c3\u22122\nrel\n= 0\n(125)\n\u03b33 + 7\u03b32 +\n\u000016 \u2212 \u03c3\u22122\nrel\n\u0001\n\u03b3 +\n\u000012 \u2212 \u03c3\u22122\nrel\n\u0001\n= 0,\n(126)\nwhich can be done using NumPy as shown in Algorithm 2.\nThe requirement \u03b3 > 0 implies that \u03c3rel < 12\u22120.5 \u2248 0.2886,\nsetting an upper bound for the relative standard deviation.\nFinally, to compute \u02c6\u03b8 efficiently during training, we note\nthat the derivation of Equation 119 does not depend on any\nparticular properties of functions f or g. Thus, the update for-\nmula remains the same, and we only need to determine \u03b2(tn)\ncorresponding to our response function (Equation 121):\n\u03b2(tn) = g(tc)\ng(tn) =\n\u0012 tc\ntn\n\u0013\u03b3+1\n=\n\u0012\n1 \u2212 \u2206t\ntn\n\u0013\u03b3+1\n.\n(127)\nThe only practical difference to traditional EMA is thus that\n\u03b2(tn) is no longer constant but depends on tn.\nC.5. Synthesizing novel EMA profiles after training\nUsing Equation 119, it is possible to track the averaged\nweights for an arbitrary set of pre-defined EMA profiles\nduring training. However, the number of EMA profiles\nthat can be handled this way is limited in practice by the\nassociated memory and storage costs. Furthermore, it can be\nchallenging to select the correct profiles beforehand, given\nhow much the optimal EMA length tends to vary between\ndifferent configurations; see Figure 5a, for example. To\novercome these challenges, we will now describe a way to\nsynthesize novel EMA profiles after the training.\nProblem definition.\nSuppose that we have stored a num-\nber of snapshots \u02c6\u0398 = {\u02c6\u03b81, \u02c6\u03b82, . . . , \u02c6\u03b8N} during training, each\nof them corresponding to a different response function pi(t).\nWe can do this, for example, by tracking \u02c6\u03b8 for a couple of\n32\ndifferent choices of \u03b3 (Equation 121) and saving them at reg-\nular intervals. In this case, each snapshot \u02c6\u03b8i will correspond\nto a pair (ti, \u03b3i) so that pi(t) = pti,\u03b3i(t).\nLet pr(t) denote a novel response function that we wish\nto synthesize. The corresponding averaged weights are given\nby Equation 106:\n\u02c6\u03b8r = Et\u223cpr(t)\n\u0002\n\u03b8(t)\n\u0003\n.\n(128)\nHowever, we cannot hope to calculate the precise value\nof \u02c6\u03b8r based on \u02c6\u0398 alone. Instead, we will approximate it by\n\u02c6\u03b8\u2217\nr that we define as a weighted average over the snapshots:\n\u02c6\u03b8\u2217\nr =\nX\nixi \u02c6\u03b8i\n(129)\n=\nX\nixi Et\u223cpi(t)\n\u0002\n\u03b8(t)\n\u0003\n(130)\n=\nX\nixi\nZ \u221e\n\u2212\u221e\npi(t) \u03b8(t) dt\n(131)\n=\nZ \u221e\n\u2212\u221e\n\u03b8(t)\nX\nipi(t) xi\n|\n{z\n}\n=: p\u2217r(t)\ndt,\n(132)\nwhere the contribution of each \u02c6\u03b8i is weighted by xi \u2208 R, re-\nsulting in the corresponding approximate response function\np\u2217\nr(t). Our goal is to select {xi} so that p\u2217\nr(t) matches the\ndesired response function pr(t) as closely as possible.\nFor notational convenience, we will denote weights by\ncolumn vector x = [x1, x2, . . . , xN]\u22a4 \u2208 RN and the snap-\nshot response functions by p = [p1, p2, . . . , pN] so that\np(t) maps to the row vector [p1(t), p2(t), . . . , pN(t)] \u2208 RN.\nThis allows us to express the approximate response function\nas an inner product:\np\u2217\nr(t) = p(t) x.\n(133)\nLeast-squares solution.\nTo find the value of x, we choose\nto minimize the L2 distance between p\u2217\nr(t) and pr(t):\nL(x) =\n\r\rp\u2217\nr(t) \u2212 pr(t)\n\r\r2\n2 =\nZ \u221e\n\u2212\u221e\n\u0000p\u2217\nr(t) \u2212 pr(t)\n\u00012 dt.\n(134)\nLet us solve for the minimum of L(x) by setting its gradient\nwith respect to x to zero:\n0 = \u2207xL(x)\n(135)\n= \u2207x\n\u0014 Z \u221e\n\u2212\u221e\n\u0000p(t) x \u2212 pr(t)\n\u00012 dt\n\u0015\n(136)\n=\nZ \u221e\n\u2212\u221e\n\u2207x\nh\u0000p(t) x \u2212 pr(t)\n\u00012i\ndt\n(137)\n=\nZ \u221e\n\u2212\u221e\n\u0000p(t)x \u2212 pr(t)\n\u0001\n\u2207x\nh\np(t)x \u2212 pr(t)\ni\ndt (138)\n=\nZ \u221e\n\u2212\u221e\n\u0000p(t) x \u2212 pr(t)\n\u0001\np(t)\u22a4 dt\n(139)\n=\nZ \u221e\n\u2212\u221e\n\u0000p(t)\u22a4p(t) x \u2212 p(t)\u22a4pr(t)\n\u0001\ndt\n(140)\n=\nZ \u221e\n\u2212\u221e\np(t)\u22a4p(t) dt\n|\n{z\n}\n=: A\nx \u2212\nZ \u221e\n\u2212\u221e\np(t)\u22a4pr(t) dt\n|\n{z\n}\n=: b\n(141)\nwhere we denote the values of the two integrals by matrix\nA \u2208 RN\u00d7N and column vector b \u2208 RN, respectively. We\nare thus faced with a standard matrix equation Ax \u2212 b = 0,\nfrom which we obtain the solution x = A\u22121 b.\nBased on Equation 141, we can express the individual\nelements of A and b as inner products between their corre-\nsponding response functions:\nA = [aij],\naij =\n\npi, pj\n\u000b\n(142)\nb = [bi]\u22a4,\nbi =\n\npi, pr\n\u000b\n,\n(143)\nwhere\n\nf, g\n\u000b\n=\nZ \u221e\n\u2212\u221e\nf(x) g(x) dx.\n(144)\nIn practice, these inner products can be computed for arbi-\ntrary EMA profiles using standard numerical methods, such\nas Monte Carlo integration.\nAnalytical formulas for power function EMA profile.\nIf\nwe assume that {pi} and pr are all defined according to our\npower function EMA profile (Equation 121), we can derive\nan accurate analytical formula for the inner products (Equa-\ntion 144). Compared to Monte Carlo integration, this leads\nto a considerably faster and more accurate implementation.\nIn this case, each response function is uniquely defined by\nits associated (t, \u03b3). In other words, pi(t) = pti,\u03b3i(t) and\npr(t) = ptr,\u03b3r(t).\nLet us consider the inner product between two such re-\nsponse functions, i.e.,\n\npta,\u03b3a, ptb,\u03b3b\n\u000b\n. Without loss of gen-\nerality, we will assume that ta \u2264 tb. If this is not the case,\nwe can simply flip their definitions, i.e., (ta, \u03b3a) \u2194 (tb, \u03b3b).\nNow,\n\npta,\u03b3a, ptb,\u03b3b\n\u000b\n(145)\n=\nZ \u221e\n\u2212\u221e\npta,\u03b3a(t) ptb,\u03b3b(t) dt\n(146)\n=\nZ ta\n0\nf\u03b3a(t)\ng\u03b3a(ta) \u00b7 f\u03b3b(t)\ng\u03b3b(tb) dt\n(147)\n=\n1\ng\u03b3a(ta) g\u03b3b(tb)\nZ ta\n0\nf\u03b3a(t) f\u03b3b(t) dt\n(148)\n= (\u03b3a + 1) (\u03b3b + 1)\nt\u03b3a+1\na\nt\u03b3b+1\nb\nZ ta\n0\nt\u03b3a+\u03b3b dt\n(149)\n= (\u03b3a + 1) (\u03b3b + 1)\nt\u03b3a+1\na\nt\u03b3b+1\nb\n\u00b7\nt\u03b3a+\u03b3b+1\na\n\u03b3a + \u03b3b + 1\n(150)\n= (\u03b3a + 1) (\u03b3b + 1) (ta/tb)\u03b3b\n(\u03b3a + \u03b3b + 1) tb\n.\n(151)\n33\nAlgorithm 3 NumPy code for solving post-hoc EMA weights.\ndef p_dot_p(t_a, gamma_a , t_b, gamma_b):\nt_ratio = t_a / t_b\nt_exp = np.where(t_a < t_b, gamma_b , \u2212gamma_a)\nt_max = np.maximum(t_a, t_b)\nnum = (gamma_a + 1) * (gamma_b + 1) * t_ratio ** t_exp\nden = (gamma_a + gamma_b + 1) * t_max\nreturn num / den\ndef solve_weights(t_i, gamma_i , t_r, gamma_r):\nrv = lambda x: np.float64(x).reshape(\u22121, 1)\ncv = lambda x: np.float64(x).reshape(1, \u22121)\nA = p_dot_p(rv(t_i), rv(gamma_i), cv(t_i), cv(gamma_i))\nB = p_dot_p(rv(t_i), rv(gamma_i), cv(t_r), cv(gamma_r))\nX = np.linalg.solve(A, B)\nreturn X\nNote that Equation 151 is numerically robust because the\nexponentiation by \u03b3b is done for the ratio ta/tb instead of be-\ning done directly for either ta or tb. If we used Equation 150\ninstead, we would risk floating point overflows even with\n64-bit floating point numbers.\nSolving the weights {xi} thus boils down to first populat-\ning the elements of A and b using Equation 151 and then\nsolving the matrix equation Ax = b. Algorithm 3 illus-\ntrates doing this simultaneously for multiple target response\nfunctions using NumPy. It accepts a list of {ti} and {\u03b3i},\ncorresponding to the input snapshots, as well as a list of {tr}\nand {\u03b3r}, corresponding to the desired target responses. The\nreturn value is a matrix whose columns represent the targets\nwhile the rows represent the snapshots.\nPractical considerations.\nIn all of our training runs, we\ntrack two weighted averages \u02c6\u03b81 and \u02c6\u03b82 that correspond to\n\u03c3rel = 0.05 and \u03c3rel = 0.10, respectively. We take a snap-\nshot of each average once every 8 million training images,\ni.e., between 4096 training iterations with batch size 2048,\nand store it using 16-bit floating point to conserve disk space.\nThe duration of our training runs ranges between 671\u20132147\nmillion training images, and thus the number of pre-averaged\nmodels stored in the snapshots ranges between 160\u2013512. We\nfind that these choices lead to nearly perfect reconstruction\nin the range \u03c3rel \u2208 [0.015, 0.250]. Detailed study of the as-\nsociated cost vs. accuracy tradeoffs is left as future work.\nD. Implementation details\nWe implemented our techniques on top of the publicly avail-\nable EDM [36] codebase.6 We performed our experiments\non NVIDIA A100-SXM4-80GB GPUs using Python 3.9.16,\nPyTorch 2.0.0, CUDA 11.8, and CuDNN 8.9.4. We used\n32 GPUs (4 DGX A100 nodes) for each training run, and 8\nGPUs (1 node) for each evaluation run.\n6https://github.com/NVlabs/edm\nTable 6 lists the full details of our main models featured\nin Table 2 and Table 3. We will make our implementation\nand pre-trained models publicly available.\nD.1. Sampling\nWe used the 2nd order deterministic sampler from EDM\n(i.e., Algorithm 1 in [36]) in all experiments with \u03c3(t) = t\nand s(t) = 1. We used the default settings \u03c3min = 0.002,\n\u03c3max = 80, and \u03c1 = 7. While we did not perform extensive\nsweeps over the number of sampling steps N, we found\nN = 32 to yield sufficiently high-quality results for both\nImageNet-512 and ImageNet-64.\nIn terms of guidance, we follow the convention used\nby Imagen [67]. Concretely, we define a new denoiser \u02c6D\nbased on the primary conditional model D\u03b8 and a secondary\nunconditional model Du:\n\u02c6D(x; \u03c3, c) = w D\u03b8(x; \u03c3, c) + (1 \u2212 w) Du(x; \u03c3), (152)\nwhere w is the guidance weight. Setting w = 1 disables\nguidance, i.e., \u02c6D = D\u03b8, while increasing w > 1 strengthens\nthe effect. The corresponding ODE is then given by\ndx = x \u2212 \u02c6D(x; \u03c3, c)\n\u03c3\nd\u03c3.\n(153)\nIn Table 2 and Table 3, we define NFE as the total number\nof times that \u02c6D is evaluated during sampling. In other words,\nwe do not consider the number of model evaluations to be\naffected by the choice of w.\nD.2. Mixed-precision training\nIn order to utilize the high-performance tensor cores avail-\nable in NVIDIA Ampere GPUs, we use mixed-precision\ntraining in all of our training runs. Concretely, we store\nall trainable parameters as 32-bit floating point (FP32) but\ntemporarily cast them to 16-bit floating point (FP16) before\nevaluating the model. We store and process all activation\ntensors as FP16, except for the embedding network and the\nassociated per-block linear layers, where we opt for FP32\ndue to the low computational cost. In CONFIGS A\u2013B, our\nbaseline architecture uses FP32 in the self-attention blocks\nas well, as explained in Appendix B.1.\nWe have found that our models train with FP16 just\nas well as with FP32, as long as the loss function is\nscaled with an appropriate constant (see \u201cLoss scaling\u201d in\nFigures 15\u201321). In some rare cases, however, we have en-\ncountered occasional FP16 overflows that can lead to a col-\nlapse in the training dynamics unless they are properly dealt\nwith. As a safety measure, we force the gradients computed\nin each training iteration to be finite by replacing NaN and\nInf values with 0. We also clamp the activation tensors to\nrange [\u2212256, +256] at the end of each encoder and decoder\nblock. This range is large enough to contain all practically\nrelevant variation (see Figure 14).\n34\nModel details\nImageNet-512\nImageNet-64\nXS\nS\nM\nL\nXL\nXXL\nS\nM\nL\nXL\nNumber of GPUs\n32\n32\n32\n32\n32\n32\n32\n32\n32\n32\nMinibatch size\n2048\n2048\n2048\n2048\n2048\n2048\n2048\n2048\n2048\n2048\nDuration (Mimg)\n2147.5\n2147.5\n2147.5\n1879.0\n1342.2\n939.5\n1073.7\n2147.5\n1073.7\n671.1\nChannel multiplier\n128\n192\n256\n320\n384\n448\n192\n256\n320\n384\nDropout probability\n0%\n0%\n10%\n10%\n10%\n10%\n0%\n10%\n10%\n10%\nLearning rate max (\u03b1ref)\n0.0120\n0.0100\n0.0090\n0.0080\n0.0070\n0.0065\n0.0100\n0.0090\n0.0080\n0.0070\nLearning rate decay (tref)\n70000\n70000\n70000\n70000\n70000\n70000\n35000\n35000\n35000\n35000\nNoise distribution mean (Pmean)\n\u22120.4\n\u22120.4\n\u22120.4\n\u22120.4\n\u22120.4\n\u22120.4\n\u22120.8\n\u22120.8\n\u22120.8\n\u22120.8\nNoise distribution std. (Pstd)\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.6\n1.6\n1.6\n1.6\nModel size and training cost\nModel capacity (Mparams)\n124.7\n280.2\n497.8\n777.5\n1119.3\n1523.2\n280.2\n497.8\n777.5\n1119.3\nModel complexity (gigaflops)\n45.5\n102.2\n180.8\n282.2\n405.9\n552.1\n101.9\n180.8\n282.1\n405.9\nTraining cost (zettaflops)\n0.29\n0.66\n1.16\n1.59\n1.63\n1.56\n0.33\n1.16\n0.91\n0.82\nTraining speed (images/sec)\n8265\n4717\n3205\n2137\n1597\n1189\n4808\n3185\n2155\n1597\nTraining time (days)\n3.0\n5.3\n7.8\n10.2\n9.7\n9.1\n2.6\n7.8\n5.8\n4.9\nTraining energy (MWh)\n1.2\n2.2\n3.2\n4.2\n4.0\n3.8\n1.1\n3.2\n2.4\n2.0\nSampling without guidance, FID\nFID\n3.53\n2.56\n2.25\n2.06\n1.96\n1.91\n1.58\n1.43\n1.33\n1.33\nEMA length (\u03c3rel)\n0.135\n0.130\n0.100\n0.085\n0.085\n0.070\n0.075\n0.060\n0.040\n0.040\nSampling cost (teraflops)\n4.13\n7.70\n12.65\n19.04\n26.83\n36.04\n6.42\n11.39\n17.77\n25.57\nSampling speed (images/sec/GPU)\n8.9\n6.4\n4.8\n3.7\n2.9\n2.3\n10.1\n6.6\n4.6\n3.5\nSampling energy (mWh/image)\n17\n23\n31\n41\n51\n65\n15\n22\n32\n43\nSampling with guidance, FID\nFID\n2.91\n2.23\n2.01\n1.88\n1.85\n1.81\n\u2013\n\u2013\n\u2013\n\u2013\nEMA length (\u03c3rel)\n0.045\n0.025\n0.030\n0.015\n0.020\n0.015\n\u2013\n\u2013\n\u2013\n\u2013\nGuidance strength\n1.4\n1.4\n1.2\n1.2\n1.2\n1.2\n\u2013\n\u2013\n\u2013\n\u2013\nSampling cost (teraflops)\n6.99\n10.57\n15.52\n21.91\n29.70\n38.91\n\u2013\n\u2013\n\u2013\n\u2013\nSampling speed (images/sec/GPU)\n6.0\n4.7\n3.8\n3.0\n2.5\n2.0\n\u2013\n\u2013\n\u2013\n\u2013\nSampling energy (mWh/image)\n25\n32\n39\n49\n59\n73\n\u2013\n\u2013\n\u2013\n\u2013\nSampling without guidance, FDDINOv2\nFDDINOv2\n103.39\n68.64\n58.44\n52.25\n45.96\n42.84\n\u2013\n\u2013\n\u2013\n\u2013\nEMA length (\u03c3rel)\n0.200\n0.190\n0.155\n0.155\n0.155\n0.150\n\u2013\n\u2013\n\u2013\n\u2013\nSampling with guidance, FDDINOv2\nFDDINOv2\n79.94\n52.32\n41.98\n38.20\n35.67\n33.09\n\u2013\n\u2013\n\u2013\n\u2013\nEMA length (\u03c3rel)\n0.150\n0.085\n0.015\n0.035\n0.030\n0.015\n\u2013\n\u2013\n\u2013\n\u2013\nGuidance strength\n1.7\n1.9\n2.0\n1.7\n1.7\n1.7\n\u2013\n\u2013\n\u2013\n\u2013\nTable 6. Details of all models discussed in Section 4. For ImageNet-512, EDM2-S is the same as CONFIG G in Figure 21.\nD.3. Training data\nWe preprocess the ImageNet dataset exactly as in the ADM\nimplementation7 by Dhariwal and Nichol [12] to ensure a\nfair comparison. The training images are mostly non-square\nat varying resolutions. To obtain image data in square aspect\nratio at the desired training resolution, the raw images are\nprocessed as follows:\n1. Resize the shorter edge to the desired training resolution\nusing bicubic interpolation.\n2. Center crop.\nDuring training, we do not use horizontal flips or any other\nkinds of data augmentation.\n7https://github.com/openai/guided-diffusion/blob/22e0\ndf8183507e13a7813f8d38d51b072ca1e67c/guided_diffusion/i\nmage_datasets.py#L126\nD.4. FID calculation\nWe calculate FID [19] following the protocol used in\nEDM [36]: We use 50,000 generated images and all available\nreal images, without any augmentation such as horizontal\nflips. To reduce the impact of random variation, typically\nin the order of \u00b12%, we compute FID three times in each\nexperiment and report the minimum. The shaded regions\nin FID plots show the range of variation among the three\nevaluations.\nWe use the pre-trained Inception-v3 model8 provided\nwith StyleGAN3 [35], which is a direct PyTorch translation\nof the original TensorFlow-based model.9\n8https://api.ngc.nvidia.com/v2/models/nvidia/research\n/stylegan3/versions/1/files/metrics/inception-2015-12-0\n5.pkl\n9http://download.tensorflow.org/models/image/imagenet\n/inception-2015-12-05.tgz\n35\nD.5. Model complexity estimation\nModel complexity (Gflops) was estimated using a PyTorch\nscript that runs the model through torch.jit.trace to\ncollect the exact tensor operations used in model evaluation.\nThis list of aten::* ops and tensor input and output sizes\nwas run through an estimator that outputs the number of\nfloating point operations required for a single evaluation of\nthe model.\nIn practice, a small set of operations dominate the cost\nof evaluating a model. In the case of our largest (XXL)\nImageNet-512 model, the topmost gigaflops producing ops\nare distributed as follows:\n\u2022 aten::_convolution\n545.50 Gflops\n\u2022 aten::mul\n1.68 Gflops\n\u2022 aten::div\n1.62 Gflops\n\u2022 aten::linalg_vector_norm\n1.54 Gflops\n\u2022 aten::matmul\n1.43 Gflops\nWhere available, results for previous work listed in Ta-\nble 2 were obtained from their respective publications. In\ncases where model complexity was not publicly available,\nwe used our PyTorch estimator to compute a best effort esti-\nmate. We believe our estimations are accurate to within 10%\naccuracy.\nD.6. Per-layer sensitivity to EMA length\nList of layers included in the sweeps of Figure 5b in the\nmain paper are listed below. The analysis only includes\nweight tensors \u2014 not biases, group norm scale factors, or\naffine layers\u2019 learned gains.\n\u2022 enc-64x64-block0-affine\n\u2022 enc-64x64-block0-conv0\n\u2022 enc-64x64-block0-conv1\n\u2022 enc-64x64-conv\n\u2022 enc-32x32-block0-conv0\n\u2022 enc-32x32-block0-skip\n\u2022 enc-16x16-block0-affine\n\u2022 enc-16x16-block0-conv0\n\u2022 enc-16x16-block2-conv0\n\u2022 enc-8x8-block0-affine\n\u2022 enc-8x8-block0-skip\n\u2022 enc-8x8-block1-conv0\n\u2022 enc-8x8-block2-conv0\n\u2022 dec-8x8-block0-conv0\n\u2022 dec-8x8-block2-skip\n\u2022 dec-8x8-in0-affine\n\u2022 dec-16x16-block0-affine\n\u2022 dec-16x16-block0-conv1\n\u2022 dec-16x16-block0-skip\n\u2022 dec-32x32-block0-conv1\n\u2022 dec-32x32-block0-skip\n\u2022 dec-32x32-up-affine\n\u2022 dec-64x64-block0-conv1\n\u2022 dec-64x64-block0-skip\n\u2022 dec-64x64-block3-skip\n\u2022 dec-64x64-up-affine\n\u2022 map-label\n\u2022 map-layer0\nE. Negative societal impacts\nLarge-scale image generators such as DALL\u00b7E 3, Stable Dif-\nfusion XL, or MidJourney can have various negative societal\neffects, including types of disinformation or emphasizing\nsterotypes and harmful biases [49]. Our advances to the\nresult quality can potentially further amplify some of these\nissues. Even with our efficiency improvements, the training\nand sampling of diffusion models continue to require a lot\nof electricity, potentially contributing to wider issues such\nas climate change.\n36\nClass 88 (macaw), guidance 2.0\nClass 29 (axolotl), guidance 1.0\nClass 127 (white stork), guidance 2.0\nFigure 24. Uncurated images generated using our largest (XXL) ImageNet-512 model.\n37\nClass 89 (cockatoo), guidance 3.0\nClass 980 (volcano), guidance 1.2\nClass 33 (loggerhead), guidance 2.0\nFigure 25. Uncurated images generated using our largest (XXL) ImageNet-512 model.\n38\nClass 15 (robin), guidance 1.0\nClass 975 (lakeside), guidance 2.0\nClass 279 (arctic fox), guidance 2.0\nFigure 26. Uncurated images generated using our largest (XXL) ImageNet-512 model.\n39\n"
  },
  {
    "title": "ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation",
    "link": "https://arxiv.org/pdf/2312.02201.pdf",
    "upvote": "30",
    "text": "ImageDream: Image-Prompt Multi-view Diffusion for 3D Generation\nPeng Wang\nYichun Shi\nByteDance, USA\n{peng.wang, yichun.shi}@bytedance.com\nMVDream\nMagic123\nImageDream\nGhost eating burger\nGhost eating burger\nGhost eating burger\nInput\nReconstructed\nFigure 1. ImageDream is a novel framework that generates high quality 3D model from any viewpoint given a single image. It vastly\nimproves the 3D geometry quality comparing to previous SoTA, e.g. Magic123 [31], and more importantly, it inherits the great text\nimage alignment from the generated image-prompt, comparing with MVDream [37]. Here, we provide 8 views of a generated object from\ndifferent methods, and in the last row, we show the corresponding normal maps rendered with ImageDream generated model.\nAbstract\nWe introduce ImageDream,\na novel Image-Prompt\nMulti-view diffusion model devised for 3D object genera-\ntion. ImageDream excels in delivering 3D models of supe-\nrior quality comparing with other State-of-the-Art (SoTA)\nimage-conditioned endeavors. Specifically, we consider a\ncanonical camera coordination of the object in image rather\nthan relative.\nThis enhancement significantly augments\nthe visual geometry correctness. Our models are meticu-\nlously crafted, taking into account varying degrees of con-\ntrol granularity derived from the provided image: wherein,\nthe global control predominantly influences the object lay-\nout, whereas the local control adeptly refines the image ap-\npearance. The prowess of ImageDream is empirically show-\ncased through a comprehensive evaluation predicated on a\ncommon prompt list as delineated in MVDream [37]. Our\nproject page is https://Image-Dream.github.io.\n1. Introduction\nIn the domain of 3D generation, incorporating images as an\nadditional modality for 3D generation, compared to meth-\nods relying solely on text [30], offers significant advantages,\nas the common saying, An image is worth a thousand words.\nPrimarily, images convey rich, precise visual information\nthat text might ambiguously describe or entirely omit. For\ninstance, subtle details like textures, colors, and spatial re-\nlationships can be directly and unambiguously captured in\nan image, whereas a text description might struggle to con-\nvey the same level of detail comprehensively or might re-\nquire excessively lengthy descriptions. This visual speci-\nficity aids in generating more accurate and detailed 3D mod-\nels, as the system can directly reference actual visual cues\nrather than interpret textual descriptions, which can vary\ngreatly in detail and subjectivity. Moreover, using images\nallows for a more intuitive and direct way for users to com-\nmunicate their desired outcomes, particularly for those who\nmay find it challenging to articulate their visions textually.\nThis multimodal approach, combining the richness of vi-\n1\narXiv:2312.02201v1  [cs.CV]  2 Dec 2023\n3D SelfAttention\nResBlock\nCrossAttention\nTimestep\nCanonical Camera\nText Embedding\nTraining\nImage Prompt \nScore Distillation\nMulti-view Diffusion\n3D Generation\nRender Multi-view\nRandom Front\nImage Encoder\nMuti-Level\nIP-Controller\nFigure 2. The training pipeline of ImageDream. The blue arrow indicates training of the diffusion network and the green arrow indicates\ntraining of NeRF model. In diffusion training, given a 3D object, we first render multiple views based on canonical camera coordination\n(bottom), and render another image-prompt front-view images with a random setting (top). The multi-view images are fed as training\ntargets for multi-view diffusion networks, and image-prompt is encoded with a multi-level controller as input to the diffusion. In NeRF\ntraining, we use the trained diffusion for image-prompt score distillation.\nsual data with the contextual depth of text, leads to a more\nrobust, user-friendly, and efficient 3D generation process,\ncatering to a wider range of creative and practical applica-\ntions.\nAdopting images as an additional modality for 3D object\ngeneration, while beneficial, also introduces several chal-\nlenges, Unlike text, images contain a multitude of features\nlike color, texture, spatial relationships that are more com-\nplex to analyze and interpret accurately with a solely en-\ncoder like CLIP [32]. In addition, high variant of light,\nshape or self-occlusion of the object can lead to inaccurate\nand in-consistent view synthesis, therefore leading blurry or\nincomplete 3D models.\nThe complexity of image processing necessitates ad-\nvanced, computationally intensive algorithms to accurately\ndecode visual information and ensure consistent appearance\nacross multiple views. Researchers have employed various\nstrategies with diffusion models, such as Zero123 [19], and\nother recent works [20, 31], to elevate a 2D object image\nto a 3D model. However, a limitation of image-only so-\nlutions is that, although the synthesized views are visually\nimpressive, the reconstructed models often lack geometric\naccuracy and detailed textures, particularly in the object\u2019s\nrear views. This issue primarily stems from significant ge-\nometric inconsistencies across the generated or synthesized\nviews. Consequently, during reconstruction, non-matching\npixels are averaged in the final 3D model, leading to indis-\ntinct textures and smoothed geometry.\nFundamentally, image-conditioned 3D generation rep-\nresents an optimization problem with more stringent con-\nstraints compared to text-conditioned generation. Hence,\nachieving optimized 3D models with clear details is more\nchallenging, as the optimization process is prone to deviat-\ning from the trained distributions due to the limited amount\nof 3D data. For example, generating a horse based solely on\ntext descriptions may yield detailed models if the training\ndataset includes a variety of horse styles. However, when\nan image specifies particular textures, shapes, and fur de-\ntails, the novel-view texture generation may easily deviate\nfrom the trained distributions.\nIn this paper, we introduce ImageDream to address these\nchallenges. Our approach involves considering a canonical\ncamera coordination across different object instances and\ndesigning a multi-level image-prompt controller that can be\nseamlessly integrated into the existing architecture. Specif-\nically, the canonical camera coordination mandates that the\nrendered image, under default camera settings (i.e., identity\nrotation and zero translation), represents the object\u2019s cen-\ntered front-view. This significantly simplifies the task of\nmapping variations in the input image to 3D. The multi-\nlevel controller offers hierarchical control, guiding the dif-\nfusion model from the image input to each architectural\nblock, thereby streamlining the path of information trans-\nfer.\nAs illustrated in Fig.1, ImageDream excels in gen-\nerating objects with correct geometry from a given im-\nage, enabling users to leverage well-developed image gen-\neration models[29] for better image-text alignment than\npurely text-conditioned models like MVDream [37]. Fur-\nthermore, ImageDream surpasses existing state-of-the-art\n(SoTA) zero-shot single image 3D model generators, such\nas Magic123 [31], in terms of geometry and texture qual-\nity. Our comprehensive evaluation in the experimental sec-\ntion (Sec. 4), which includes both qualitative comparisons\nthrough user studies and quantitative analyses, demon-\nstrates ImageDream\u2019s superiority over other SoTA methods.\n2. Related Works\nWe recognize that 3D generation is a well-established field;\nthis review focuses on significant advancements closely re-\nlated to our research.\nText-to-3D Generation with Diffusion. The emergence of\n2\n3D Self-Attention\nRes-Block\nCross-Attention\nCLIP Encoding\nVAE Encoding\nView 1\nView 2\nView 3\nView 4\nView 5\nGlobal Controller \nLocal Controller \nPixel Controller\nImage Prompt\nDiffusion UViT Block\nFigure 3. The Multi-level Controller of ImageDream. Given an image prompt, global controller and local controller takes input of image\nfeatures after CLIP encoding, and then output adapted features to cross-attention layers. It represents image semantic information. The\npixel controller send the VAE encoded feature to diffusion, and perform pixel-level dense self-attention with corresponding hidden features\nat each layer of the four-view MVDiffusion.\ndeep generative models has significantly impacted 3D gen-\neration. Early methods targeted the reconstruction of sim-\nple objects using multi-view rendered images [11, 12]. The\nevolution of these techniques, from Generative Adversarial\nNetworks (GANs)[3, 9, 10, 24, 25, 27] to diffusion-based\nframeworks[33], marks a notable progression.\nRecent 3D diffusion models,\nspecifically for tri-\nplanes [38, 45] and feature grids [15], have emerged. How-\never, these models often focus on specific objects like faces\nand ShapeNet [5] objects. Concurrently, there\u2019s growing\ninterest in reconstructing object shapes from monocular im-\nage inputs [14, 26, 48], demonstrating the evolving stability\nof image generation methodologies. A significant challenge\nremains in generalizing these models to the extent of their\n2D counterparts, likely due to constraints in 3D data size,\nrepresentation, and architectural design.\nLifting 2D Diffusion for 3D Generation.\nIn light of\nthe limited generalizability of direct 3D generative mod-\nels, a parallel line of research has explored the elevation\nof 2D diffusion priors into 3D generation, often integrat-\ning with 3D representations like NeRF [23]. A pivotal ap-\nproach in this area is the score distillation sampling (SDS)\nintroduced by Poole et al.[30], using diffusion priors as\nscore functions to guide 3D representation optimization.\nAlongside Dreamfusion, works like SJC[44], which uti-\nlize stable-diffusion models [33], have emerged.\nSubse-\nquent studies have focused on enhancing 3D representa-\ntions [6, 17, 42, 43], refining sampling schedules [13], and\noptimizing loss designs [46]. Despite their ability to gener-\nate photorealistic objects of various types without 3D data\ntraining, these methods struggle with multi-view consis-\ntency. Moreover, each 3D model requires individualized\noptimization through prompt and hyper-parameter adjust-\nments. Notably, MVDream [37] enhances generation ro-\nbustness by joint training with 2D and 3D datasets, pro-\nducing satisfactory results with uniform parameters, draw-\ning on multi-view diffusion via SDS. Our work builds upon\nthese concepts, applying them to image-prompt generation\nand retaining the robustness characteristic of MVDream.\nImage-based Novel View Synthesis. Direct synthesis of\nnovel 3D views from single images has also been ex-\nplored, bypassing traditional reconstruction processes. Wat-\nson et al.[47] pioneered diffusion model applications in\nview synthesis as the pipeline in Sitzmann et al.[39] using\nthe ShapeNet dataset. Subsequent advancements include\nZhou et al.\u2019s [53] extension to latent space with an epipo-\nlar feature transformer and Chan et al.\u2019s [4] approach to en-\nhance view consistency. Szymanowicz et al.[41] proposed\na multi-view reconstructor using unprojected feature grids.\nA common limitation across these methods is their depen-\ndency on specific training data, with no established adapt-\nability to diverse image inputs. Fine-tuning pre-trained im-\nage diffusion models[2] on extensive 3D render datasets for\nnovel view synthesis, as proposed by Zero123 [19], remains\nconstrained by geometric consistency issues. Later works,\nincluding SyncDreamer [20], Consistent 1-to-3 [52], and\nZero123plus [36], have sought to enhance multi-view con-\nsistency through joint diffusion processes, but the recon-\nstruction of geometrically coherent 3D models remains a\nchallenge.\nSingle Image-conditioned Reconstruction.\nRecent ad-\nvances in deriving 3D models from single or few images\npredominantly leverage NeRF representations. Techniques\nsuch as RegNeRF [28], which uses geometry loss from\ndepth patches, and SinNeRF [49], RealFusion [22], and\nNeuralLift [50], which combine depth maps or Score Dis-\ntillation Sampling during NeRF training, represent signif-\nicant steps forward. Despite their effectiveness, the qual-\nity of these generated models remains suboptimal for real-\nworld applications. Magic123 [31] combines single-view\nand novel-view diffusion networks, achieving impressive\ntexture quality in 3D models. However, our tests reveal lim-\nitations in understanding correct object geometry.\nWe also note recent parallel developments, such as Won-\nder3D [21], which incorporate normal diffused outputs into\noriginal diffusion models, and DreamCraft3D [40], which\nemploy a second-stage DreamBooth-like model fine-tuning\nfor enhanced texture modeling. These works, while promis-\n3\ning, remain distinct from our contributions.\n3. Methodology\nIn this section, we first talk about the MVDream [37]\npipeline and then describe our method to input the image\nprompt.\n3.1. Preliminary\nIn MVDream, there are two stage for 3D model produc-\ntion. The first stage is training a multi-view diffusion net-\nwork that produces four orthogonal and consistent multi-\nview images from a text-prompt given respective camera\nembedding. In the second stage, a multi-view score distil-\nlation sampling (MV-SDS) is adopted to produce a detailed\n3D NeRF model.\nIn the first stage, each block of the multi-view network\ncontains a densely connected 3D attention on the four view\nimages, which allows a strong interaction in learning the\ncorrespondence relationship between different views. To\ntrain such a network, it adopts a joint training with the\nrendered dataset from the Objaverse [8] and a larger scale\ntext-to-image (t2i) dataset, LAION5B [35], to maintain the\ngeneralizability of the fine-tuned model. Formally, given\ntext-image dataset X = {x, y} and a multi-view dataset\nXmv = {xmv, y, cmv}, where x is an latent image embed-\nding from VAE [16], y is a text embedding from CLIP [32],\nand c is their self-desgined camera embedding, we may for-\nmulate the the multi-view (MV) diffusion loss as,\nLMV(\u03b8, X, Xmv) = Ex,y,c,t,\u03f5\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(xp; y, cp, t)\u22252\n2\n\u0003\n(1)\nwhere, (xp, cp) =\n(\n(x, 0)\nwith probability p\n(xmv, cmv)\nwith probability 1 \u2212 p\nhere, x is the noisy latent image generated from a random\nnoise \u03f5 and image latent, the \u03f5\u03b8 is the multi-view diffusion\n(MVDiffusion) model parametrized by \u03b8.\nAfter the model is trained, the MVDiffusion model can\nbe inserted to the DreamFusion [30] pipeline, where the au-\nthors adopt a score-distillation sampling (SDS) based on the\nfour generated views. Specifically, in each iteration step,\na random 4 orthogonal views are rendered from a NeRF\ng(\u03d5) with a random 4 view camera extrinsic and intrinsic c.\nThen, they are encoded to latents xmv and inserted to the\nmulti view diffusion network to compute a diffusion loss in\nthe image space which is back propagated to optimize the\nNeRF parameters. Formally,\nLMV-SDS(\u03d5, xmv) = Et,c,\u03f5\nh\n\u2225xmv \u2212 \u02c6xmv\n0 \u22252\n2\ni\n.\n(2)\nHere, \u02c6xmv\n0\ns the denoised MV image at timestep 0 from\nMVDiffusion. After fusion, MVDream shows significant\nimprovement of object geometry correctness without the\nJanus issues.\n3.2. Canonical Camera\nIn the context of MVDream, a critical observation is the\ndiffusion of multi-view images using a global aligned cam-\nera coordination. In other words, the image from a default\ncamera (no azimuth rotation) is always the front view of the\nobject. This is done by asking the CLIP image feature of a\nview best match the \u201dfront view\u201d CLIP text feature embed-\ndings. This alignment facilitates the fusion of diffused im-\nages in the fusion step, reducing ambiguity regarding their\nviewpoints in relation to the provided text prompt.\nAs emphasized in the introduction, this alignment also\nreduced the difficulties in learning the accurately recon-\nstructing the geometry of objects. Thereby, in image prompt\ncases, in contrast to previous image-conditioned approaches\nlike Zero123 [19], which attempt to recover object 3D ge-\nometry based on image camera coordination system, Im-\nageDream adopts canonical/world camera coordination as\nin MVDream. Our diffusion model aims to regress towards\nthe canonical multiple view image of the object as depicted\nin the image. This approach is expected to yield superior\ngeometric accuracy compared to systems that utilize rela-\ntive camera coordination.\nFormally, for an image of an object rendered from a ran-\ndom viewpoint with a random camera, denoted as xr, we\ncreate the ImageDream diffusion multi-view (MV) dataset\nas Xmv = {xmv, y, xr, cmv}, where cmv is the introduced\ncanonical cameras in MVDream. Then, the rest of diffusion\nloss is the same as Eqn.(1).\n3.3. Multi-level Controllers\nIn order to insert the image prompt to control the output\nMV images, we consider a multi-level strategy. The overall\nstructure of the multi-level controller from an image prompt\ncan be seen in Fig. 4, and we elaborate the details of each\ncomponent in the following.\nGlobal Controller. In our initial approach, we integrated\nglobal CLIP image features into MVDream, akin to how\ntext features are used, by fine-tuning the model\u2019s already\nwell-established training. Recognizing that MVDream is\nprimarily trained on text embeddings, we introduced a\nmulti-layer perceptron (MLP) \u03b8g, functioning as an adap-\ntor similar to IP-Adaptor [51], following the CLIP image\nglobal embedding. This step aims to align image features\nwith text features, ensuring compatibility within the MV-\nDream framework. Specifically, CLIP image encoding en-\ncodes image feature to a 1024 vector with a token length of\n4, which we named as f g. And, \u03b8g further adapts the image\nfeature to be 1024 as the input to cross-attention.\nOn the MVDiffusion side, inside of an attention layer\nl, we add a new set of MLPs, \u03b8kg,l and \u03b8vg,l, that takes\nthe input the adapted features and output its attention key\nand value matrix, which then aggregated based on the query\nfeature matrix, ql, yielding a corresponding image cross-\n4\nA bulldog with a \nblack pirate hat\nInput \n(a) Global\n(b) Local: w/o Resample\n \n(c) Local: w Resample \n(d) +Pixel\nDiffused Outputs\nFigure 4. An example of the diffused results from different settings of multi-level controllers in ImageDream (see Sec. 3.3).\nattention feature hg,l. Here, a weight \u03bb = 1.0 is introduced\nto balance the hidden from text and image, and the final\noutput of layer l is hl = ht,l+\u03bbhg,l. We refer to decoupled\ncross-attention in IP-Adaptor for additional details.\nTo train such a model, we freeze the diffusion model, and\nonly fine-tune {\u03b8g, \u03b8kg,l, \u03b8vg,l}l. We follow the training set-\nting of MVDream by considering both 3D rendered datasets\nand 2D image datasets together, which will be elaborated in\nour experimental section.\nAfter the model is tuned, we found the model is able of\nabsorb some informations from the image such as structure\nof the object etc. As illustrated in Fig. 4 (a), comparing\nwith the input, the diffused output is able to put the pirate\nhat similar to the image on the bulldog, while some detailed\npose and appearance information is lost, which we think is\nnot enough for a good control from the input image.\nLocal Controller.\nTo enhance control, we try to utilize\nthe hidden feature from the CLIP encoder before its global\npooling, which likely contains more detailed structural in-\nformation. This hidden feature, denoted as f h, has a to-\nken length of 257 and a feature dimension of 1280.\nA\nMLP adaptor \u03b8h is introduced to feed f h into the diffu-\nsion network\u2019s cross-attention module, with \u03b8kh,l and \u03b8vh,l\nforming the key and values matrix.\nThese parameters,\n{\u03b8kh,l, \u03b8vh,l, \u03b8h}l, are then jointly trained as learnable el-\nements similar to the global controller. Post-training, we\nobserved that the results were overly sensitive to image to-\nkens, leading to overexposed and unrealistic images, espe-\ncially with higher class free guidance (CFG) settings [33],\nas shown in Fig. 4(b).\nTo mitigate this, we implemented a resampling module\n\u03b8r, following the approach of IP-Adaptor, reducing the hid-\nden token count from 257 to 16, resulting in a more bal-\nanced local image feature f r. The corresponding local con-\ntroller parameters are {\u03b8r, \u03b8kr,l, \u03b8vr,l}l. As Fig. 4(c) illus-\ntrates, after this resampling, the diffused images more re-\nalistic, even at higher CFG levels. From the generated im-\nages, it\u2019s evident that the model captures the overall layout\nand object shape, but also struggles with finer identity de-\ntails like object skin texture.\nPixel Controller. To optimally integrate object appearance\ntexture, we propose embedding the image prompt pixel la-\ntent x across all attention layers in ImageDream. Specifi-\ncally, MVDream employs a 3D dense self-attention mech-\nanism with a shape of (bz, 4, c, hl, wl) across four views\nwithin a transformer layer. In contrast, ImageDream intro-\nduces an additional frame by concatenating the input image,\nresulting in a feature shape of (bz, 5, c, hl, wl). This enables\nsimilar 3D self-attention processes between the four-view\nimages and the input image.\nDuring the training of our diffusion network, we refrain\nfrom adding noise to the latent from the input image prompt,\nensuring the network clearly captures the image informa-\ntion. Additionally, to differentiate the input image features\nand avoid confusion, we assign an all-zero vector to the\ncamera embedding of the input image. Given that the pixel\ncontroller is integrated into the multi-view diffusion without\nextra parameters, we fine-tune all feature parameters in uni-\nson, adopting the same training regime as the global/local\ncontrollers but with a learning rate reduced by a factor of\nten. This approach preserves the original feature representa-\ntions more effectively. Post-training, as depicted in Fig 4(d),\nthe generated multi-view images not only ethically main-\ntain the appearance from the input image but also uphold\nthe multi-view consistency characteristic of MVDream, re-\nsulting in satisfactory 3D model fusion.\nFinally, our multi-level controller is a combined one with\nlocal and pixel, since we think the global one do not have\ntoo much additional information. There might be potential\nqueries regarding the necessity of a pixel controller, given\nthat IP-Adaptor, relying solely on CLIP features, can cap-\nture extensive image texture details: we posit that while\nIP-Adaptor is effective for modifying objects in the same\nview as the input, decoding the same view is comparatively\nsimpler. Multi-view diffusion, however, presents a more\ncomplex challenge. Decoding from a highly compressed\nCLIP feature could necessitate prolonged training on larger\ndatasets. Therefore, at this stage, we find the pixel con-\ntroller significantly beneficial for rapidly training a robust\nmulti-view diffusion model.\n5\n(a) The floating artifacts w/o background alignment  \n(b) The geometry collapse w/o and with camera alignment  \nFigure 5. Example of artifacts we fixed with image-prompt score distillation (see Sec. 3.4).\n3.4. Image-Prompt Sore Distillation\nImplementing the image-prompt multi-view diffusion net-\nwork in ImageDream follows the multi-view score distilla-\ntion framework of MVDream (Sec. 3.1), with the addition\nof an image prompt as an input to the diffusion network.\nHowever, we need to condier few key differences in NeRF\noptimization to achieve accurate results.\nBackground Alignment.\nDuring SDS optimization, the\nNeRF-rendered image includes a randomly colored back-\nground to differentiate the interior and exterior of the 3D\nobject. This random background, when input into the dif-\nfusion network alongside the object, can conflict with the\nbackground from the image prompt, leading to floating ar-\ntifacts in the generated NeRF model, as shown in Fig. 5(a).\nTo resolve this, we adjusted the image-prompt background\nto match the rendered background color from NeRF, suc-\ncessfully eliminating these artifacts.\nCamera Alignment. Our diffusion network tends to gen-\nerate multi-view images mirroring the camera parameters\n(e.g., elevation, field of view (FoV)) of the input image\nprompt, parameters which remain unknown during NeRF\nrendering. Randomly sampling parameters for rendering,\nas done in MVDream, can result in images incongruent\nwith the image prompt\u2019s rendering settings, affecting the\ngeometry of detailed image structures. To mitigate this, we\nnarrowed the parameter sampling range from MVDream\u2019s\n[15, 60], [0, 30] for camera FoV and elevation to [45, 50] and\n[0, 5], respectively, a range more typical for a generated user\nphotos. This adjustment significantly improved the geomet-\nric accuracy of the 3D objects, as demonstrated in Fig. 5(b).\nWe acknowledge this solution\u2019s limitations; when the\nimage prompt\u2019s camera parameters greatly differ from our\nselected range in canonical camera setting, the resulting\n3D object shape may be unpredictable. Future improve-\nments could include a camera parameter estimation mod-\nule or increased randomness in the image prompt rendering\nduring diffusion training, to better synchronize the settings\nbetween NeRF rendering and diffusion.\n4. Experiments\nIn this section, we detail the experimental setup for Image-\nDream, designed to enable replication of our model. We\nwill release both the model and code following this submis-\nsion.\nImageDream-G\n20%\nImageDream-P\n56%\nMagic123\n22%\nZero123-XL\n2%\nFigure 6. User study of different methods. ImageDream-P: our\nfull model w pixel controller, and ImageDream-G: without pixel\ncontroller (see Sec. 4.1).\nImplementation Details.\nAdhering to the dataset con-\nfiguration of MVDream (Sec. 3.3), we used a combined\ndataset from Objaverse for 3D multi-view rendering and a\n2D image dataset for training controllers (Sec. 3.3). For\nimage prompts in the 3D dataset, we randomly selected\none of the 16 front-side views, with azimuth angles rang-\ning from [\u221290, 90] degrees, out of the total 32 circle views.\nFor the 2D dataset, we used the input image as the im-\nage prompt. A random dropout rate of 0.1 was set for the\nimage prompt during training, replacing dropped prompts\nwith a random uni-colored image. For all experiments, i.e.\nwith global controller, local controller and local plus pixel\ncontrollers, we trained for 60K steps with a batch size of\n256 and a gradient accumulation of 2, using the AdamW\noptimizer. The model is initialized from stable diffusion\n(2.1) checkpoint of MVDream. The learning rate was set\nto 1e-4, except for the model with pixel controller, where it\nwas reduced to 1e-5. Test image prompts were resized to\n256 \u00d7 256, and we set the diffusion CFG to 5.0. The train-\ning takes \u223c 2 days with 8 A100. For NeRF optimization,\nwe followed MVDream\u2019s configuration but introduced a\nthree-stage optimization at resolutions [64, 192, 256] which\nswitched at [5K, 10K] steps, setting the camera distance\nbetween [0.6, 0.85] for better NeRF model coverage. The\nNeRF training is about 1hr with A100.\nTest Dataset. Our primary focus was evaluating Image-\nDream outside the Objaverse distribution to ensure its prac-\ntical applicability.\nWe selected 39 well-curated prompts\n6\nSynthesized Image\nRe-rendered\nModel\nQIS(256)\u2191\nCLIP(TX)\u2191\nCLIP(IM)\u2191\nQIS(320)\u2191\nCLIP(TX)\u2191\nCLIP(IM)\u2191\nSD-XL [1]\n52.0 \u00b1 30.5\n34.6 \u00b1 3.09\n100\n-\n-\n-\nMVDream [37]\n23.05 \u00b1 14.4\n31.64 \u00b1 2.99\n78.41 \u00b1 5.32\n29.02 \u00b1 10.24\n32.69 \u00b1 3.39\n79.63 \u00b1 4.15\nZero123 [19]\n22.16 \u00b1 11.16\n30.42 \u00b1 3.19\n84.88 \u00b1 5.12\n-\n-\n-\nZero123-XL [19]\n34.07 \u00b1 11.64\n30.80 \u00b1 2.59\n84.10 \u00b1 4.76\n28.66 \u00b1 5.03\n29.19 \u00b1 3.60\n79.92 \u00b1 6.59\nMagic123 [31]\n-\n-\n-\n24.23 \u00b1 7.68\n29.56 \u00b14.73\n82.50 \u00b1 8.78\nSyncDreamer [20]\n22.04 \u00b1 11.9\n27.96 \u00b1 3.01\n78.17 \u00b1 6.13\n19.84 \u00b1 6.64\n25.82 \u00b1 3.39\n73.20 \u00b1 6.30\nImageDream\n- global\n22.31 \u00b1 7.59\n32.01 \u00b1 2.84\n84.50 \u00b1 3.96\n22.51 \u00b1 5.86\n31.48 \u00b1 3.32\n82.58 \u00b1 4.35\n- local (-G)\n22.49 \u00b1 9.57\n31.32 \u00b1 2.86\n82.99 \u00b1 6.03\n22.30 \u00b1 4.47\n31.71 \u00b1 2.96\n84.34 \u00b1 3.13\n- +pixel (-P)\n27.10 \u00b1 12.8\n32.39 \u00b1 2.78\n85.69 \u00b1 3.77\n25.16 \u00b1 6.49\n31.59 \u00b1 3.23\n84.83 \u00b1 4.08\nTable 1. Quantitative assessment of image synthesis quality using the test prompt list from MVDream. The DDIM sampler was employed\nfor testing, and Implicit Volume from threestudio was utilized for re-rendering 3D model images. Here Zero123 we took their checkpoint\nof 165K which including all training instances. \u2019Zero123-XL\u2019 indicates Zero123 trained with the larger Objaverse-xl 10M dataset [7],\nwhile others were trained solely on the standard Objaverse dataset [8], which is \u223c10 times smaller. (-P) and (-G) are correspondent to\nImageDream-P and ImageDream-G in Fig.6.\nSyncDreamer\nMagic123\nImageDream\nReconstructed\nDiffused\nZero123-XL\nAn astronaut \nriding a horse\nFigure 7. The illustration of synthesized images from different baselines. Diffused: From diffusion models. Reconstructed: Re-rendered\nimages from correspondent fused NeRF model. Please check the webpage for more results.\nfrom MVDream, covering a diverse range of objects with\nrelatively complex geometries and appearances, surpass-\ning datasets like ShapeNet [5] or CO3D [18].\nUsing\nSDXL [29], we generated multiple images from each\nprompt, selecting ones with aesthetically pleasing objects.\nThe backgrounds of these images were then removed,\nand the objects re-centered, akin to the approach used in\nZero123.\n4.1. Comparisons\nIn our evaluation,\nwe compared ImageDream\u2019s per-\nformance\nagainst\nseveral\nSoTA\nbaselines,\nincluding\nZero123-XL [19] (trained on 10x larger data than ours),\nMagic123 [31], and SyncDreamer [20].\nThe criteria for\ncomparison were geometry quality and similarity to the im-\nage prompt (IP). \u2019Geometry quality\u2019 refers to the generated\n3D asset\u2019s conformance to common sense in terms of shape\nand minimal artifacts, while \u2019similarity to IP\u2019 assesses the\nresemblance of the results to the input image.\nWe exe-\ncuted all baseline tests using default configurations as im-\nplemented in threestudio1.\nQualitative Evaluation. Lacking ground truth for the test\nimage prompts, we conducted a real user study to evaluate\nthe quality of the generated 3D models. Participants were\nbriefed on our evaluation standards and asked to choose\ntheir preferred model based on these criteria. The exper-\n1https : / / github . com / threestudio - project /\nthreestudio\n7\niment was double-blind, with participants shown 3D as-\nsets generated by different methods without identifying la-\nbels.\nThe comparison results, depicted in Fig. 6, show\nthat ImageDream, both with (ImageDream-P) and without\n(ImageDream-G) the pixel controller, significantly outper-\nformed other baselines. ImageDream-P was particularly fa-\nvored, while ImageDream-G also received a positive pref-\nerence rate. SyncDreamer was omitted from the figure due\nto its NeuS results having a 0% preference rate.\nFig. 7 presents a representative case comparing results\nfrom the diffusion models and the final NeRF model. Sys-\ntems like Magic123 and Zero123, which rely on single-view\ndiffusion with relative camera embedding, often produce in-\ncorrect geometry, as illustrated by their inability to accu-\nrately represent the span of horse body. In contrast, Image-\nDream, through its unique design, effectively resolves this\nissue, resulting in more satisfactory models (more results\nare list in webpage).\nNumerical Evaluation. To thoroughly assess image qual-\nity at various stages of our generation pipeline, we em-\nployed the Inception Score (IS)[34] and CLIP scores[32]\nusing text-prompt and image-prompt, respectively.\nThe\nIS evaluates image quality, while CLIP scores assess text-\nimage and image-image alignment. However, since IS tra-\nditionally evaluates both image quality and diversity within\na set, and our prompt quantity is limited, the diversity aspect\nmakes the score less reliable. Therefore, we modified the IS\nby omitting its diversity evaluation, replacing the mean dis-\ntribution with a uniform distribution. Specifically, we set qi\nin IS to be 1/N, making the IS of an image P\ni pi log(Npi),\nwhere N is the inception class count and pi is the predicted\nprobability for the ith class. We denote this modified metric\nas Quality-only IS (QIS). For the CLIP score, we calculated\nthe mean score between each generated view and the pro-\nvided text-prompt or image-prompt.\nIn Tab.1, we present comparative results. SD-XL, re-\nflecting the score of test images, achieved the highest QIS\nand CLIP scores. MVDream, listed as a benchmark for final\n3D model quality, shows improved synthesized image qual-\nity after 3D fusion due to multi-view consistency. In con-\ntrast, Zero123 and Zero123-XL experienced a drop in im-\nage quality post-3D fusion due to diffusion inconsistency.\nMagic123 enhanced the CLIP score over Zero123 by in-\ntegrating a joint diffusion model. SyncDreamer\u2019s quality\ndeclined as it diffuses only 16 fixed views, complicating re-\nconstruction. In ImageDream, we evaluated three models\nfor ablation: one with a global controller, another with a lo-\ncal controller, and the last incorporating both local and pixel\ncontrollers (Sec.3.3). ImageDream maintained high image\nquality in both diffusion and post-3D fusion stages. The\nlocal controller, in particular, provided better image CLIP\nscores post-fusion, thanks to richer image feature repre-\nsentations. The pixel controller model excelled in image\nTrump Figure\nReconstructed\nDiffused\nImageDream\nGlobal-Only\nInputs\nFigure 8. Illustration of a failure case. Where Trumps\u2019 face turns\nblurry after 3D fusion reconstruction (first row). However, the face\ntextures can be generated with model using global-only controller\nthanks to its semantic global representation.\nCLIP scores during both stages.\nNotably, ImageDream-\npixel ranked second in other scores, with Zero123-XL using\na significantly larger dataset (Objaverse-XL [7]).\nHowever, these scores don\u2019t fully encapsulate important\naspects like multi-view consistency and geometric correct-\nness. For instance, as Fig. 7 demonstrates, zero123-XL,\ndespite having high IS due to easy image classification,\nshowed poorer consistency. Thus, while these scores offer\nsome reliability when consistency is high, future research\nshould aim to develop more comprehensive metrics that ac-\ncurately capture geometric correctness to better compare\ndifferent generation algorithms.\n4.2. Limitations\nWhile the model incorporating the pixel controller achieves\nthe best scores, we observed certain trade-offs, particularly\nwhen the image constraints are overly stringent. For in-\nstance, in cases like the small facial details of a full-body\navatar (shown in Fig.8), the model struggles to capture these\nnuances, whereas global control might recover the face\nbased on the text prompt. To address this, as outlined in\nSec.3.4, the pixel controller model needs to better estimate\nimage intrinsic and extrinsic properties, or a better balance\ntuning inside of multi-level controllers. This may be solved\nby exploring the use of larger models, such as SDXL [29],\nwhich be our future work.\n5. Conclusion\nWe introduce ImageDream, an advanced image-prompt\n3D generation model utilizing multi-view diffusion. This\nmodel innovatively applies canonical camera coordination\nand multi-level image-prompt controllers, enhancing con-\ntrol and addressing geometric inaccuracies seen in prior\nmethods. Future improvements could focus on increasing\nrandomness in image-prompts during training to further re-\nduce texture blurriness in the generated models. These steps\nare expected to further advance the capabilities and applica-\ntions of ImageDream in 3D model generation.\n8\n6. Acknowledgements and Ethics Statement\nWe thank our 3D group members of Kejie Li, and our in-\ntern Zeyuan Chen in joint meeting discussion and setup\nbaselines of SyncDreamer, which help complete this pa-\nper. In addition, note that the models proposed in this pa-\nper aims to facilitate the 3D generation task that is widely\ndemanded in industry for ethical purpose. It could be po-\ntentially applied to unwanted scenarios such as generat-\ning violent and sexual content by third-party fine-tuning.\nBuilt upon the Stable Diffusion model [33], it might also\ninherit the biases and limitations to generate unwanted re-\nsults.\nTherefore, we believe that the images or models\nsynthesized using our approach should be carefully exam-\nined and be presented as synthetic. Such generative mod-\nels may also have the potential to displace creative workers\nvia automation. That being said, these tools may also en-\nable growth and improve accessibility for the creative indus-\ntry.\nReferences\n[1] stable-diffusion-xl-base-1.0. https://huggingface.\nco / stabilityai / stable - diffusion - xl -\nbase-1.0. Accessed: 2023-08-29. 7\n[2] Stable\ndiffusion\nimage\nvariation.\nhttps : / /\nhuggingface.co/spaces/lambdalabs/stable-\ndiffusion-image-variations. 3\n[3] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al.\nEffi-\ncient geometry-aware 3d generative adversarial networks. In\nCVPR, 2022. 3\n[4] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexan-\nder W. Bergman, Jeong Joon Park, Axel Levy, Miika Ait-\ntala, Shalini De Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS: Generative novel view synthesis with 3D-aware dif-\nfusion models. In arXiv, 2023. 3\n[5] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 3, 7\n[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. arXiv:2303.13873, 2023.\n3\n[7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Anirud-\ndha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana\nEhsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: A\nuniverse of 10m+ 3d objects. 2023. 7, 8\n[8] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, pages 13142\u2013\n13153, 2023. 4, 7\n[9] Yu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong.\nGram: Generative radiance manifolds for 3d-aware image\ngeneration. In CVPR, pages 10673\u201310683, 2022. 3\n[10] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. NeurIPS, 2022. 3\n[11] Paul Henderson and Vittorio Ferrari. Learning single-image\n3d reconstruction by generative modelling of shape, pose and\nshading. International Journal of Computer Vision, 2020. 3\n[12] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert.\nLeveraging 2d data to learn textured 3d mesh generation. In\nCVPR, 2020. 3\n[13] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi,\nZheng-Jun Zha, and Lei Zhang.\nDreamtime:\nAn im-\nproved optimization strategy for text-to-3d content creation.\narXiv:2306.12422, 2023. 3\n[14] Heewoo Jun and Alex Nichol. Shap-e: Generating condi-\ntional 3d implicit functions. arXiv:2305.02463, 2023. 3\n[15] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images. In CVPR, 2023. 3\n[16] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. In ICLR, 2014. 4\n[17] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 3\n[18] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed. arXiv:2305.08891, 2023. 7\n[19] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. arXiv:2303.11328, 2023.\n2, 3, 4, 7\n[20] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n2, 3, 7\n[21] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu,\nZhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang,\nMarc Habermann, Christian Theobalt, and Wenping Wang.\nWonder3d: Single image to 3d using cross-domain diffusion,\n2023. 3\n[22] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and\nAndrea Vedaldi. Realfusion: 360deg reconstruction of any\nobject from a single image. In CVPR, 2023. 3\n[23] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2021. 3\n[24] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang.\nHologan: Unsupervised\nlearning of 3d representations from natural images.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2019. 3\n9\n[25] Thu H Nguyen-Phuoc, Christian Richardt, Long Mai,\nYongliang Yang, and Niloy Mitra. Blockgan: Learning 3d\nobject-aware scene representations from unlabelled images.\nNeurIPS, 2020. 3\n[26] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts. arXiv:2212.08751,\n2022. 3\n[27] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nIn CVPR, 2021. 3\n[28] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi S. M. Sajjadi, Andreas Geiger, and Noha Radwan.\nRegnerf: Regularizing neural radiance fields for view syn-\nthesis from sparse inputs. In CVPR, 2022. 3\n[29] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis. arXiv:2307.01952,\n2023. 2, 7, 8\n[30] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR,\n2023. 1, 3, 4\n[31] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors.\narXiv\npreprint arXiv:2306.17843, 2023. 1, 2, 3, 7\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2, 4, 8\n[33] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 3, 5, 9\n[34] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. NeurIPS, 2016. 8\n[35] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. NeurIPS, 2022. 4\n[36] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model. arXiv preprint arXiv:2310.15110, 2023.\n3\n[37] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv:2308.16512, 2023. 1, 2, 3, 4, 7\n[38] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,\nJiajun Wu, and Gordon Wetzstein. 3d neural field generation\nusing triplane diffusion. In CVPR, 2023. 3\n[39] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations. NeurIPS, 32,\n2019. 3\n[40] Jingxiang Sun, Bo Zhang, Ruizhi Shao, Lizhen Wang, Wen\nLiu, Zhenda Xie, and Yebin Liu. Dreamcraft3d: Hierarchical\n3d generation with bootstrapped diffusion prior, 2023. 3\n[41] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea\nVedaldi. Viewset diffusion:(0-) image-conditioned 3d gener-\native models from 2d data, 2023. 3\n[42] Junshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran\nYi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-\nfidelity 3d creation from a single image with diffusion prior.\narXiv:2303.14184, 2023. 3\n[43] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari.\nTextmesh:\nGeneration of realistic 3d meshes from text prompts.\narXiv:2304.12439, 2023. 3\n[44] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\n2023. 3\n[45] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, et al. Rodin: A generative model for\nsculpting 3d digital avatars using diffusion. In CVPR, 2023.\n3\n[46] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv:2305.16213, 2023. 3\n[47] Daniel Watson, William Chan, Ricardo Martin-Brualla,\nJonathan\nHo,\nAndrea\nTagliasacchi,\nand\nMohammad\nNorouzi. Novel view synthesis with diffusion models. In\nICLR, 2023. 3\n[48] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph\nFeichtenhofer, and Georgia Gkioxari. Multiview compres-\nsive coding for 3d reconstruction. In CVPR, 2023. 3\n[49] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey\nShi, and Zhangyang Wang. Sinnerf: Training neural radiance\nfields on complex scenes from a single image. 2022. 3\n[50] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang,\nand Zhangyang Wang. Neurallift-360: Lifting an in-the-wild\n2d photo to a 3d object with 360\u00b0 views. 2022. 3\n[51] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 4\n[52] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng\nWang. Consistent-1-to-3: Consistent image to 3d view syn-\nthesis via geometry-aware diffusion models. arXiv preprint\narXiv:2310.03020, 2023. 3\n[53] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-\ntilling view-conditioned diffusion for 3d reconstruction. In\nCVPR, 2023. 3\n10\n"
  },
  {
    "title": "FaceStudio: Put Your Face Everywhere in Seconds",
    "link": "https://arxiv.org/pdf/2312.02663.pdf",
    "upvote": "27",
    "text": "FaceStudio: Put Your Face Everywhere in Seconds\nYuxuan Yan\u2217\nChi Zhang\u2217\u2020\nRui Wang\nYichao Zhou\nGege Zhang\nPei Cheng\nBin Fu\nGang Yu\nTencent\n{yuxuanyan, johnczhang, raywwang, yichaozhou, gretazhang, peicheng, brianfu, skicyyu}@tencent.com\nhttps://icoz69.github.io/facestudio/\nMulti-Human Image Synthesis\nStylized Portrait Synthesis\nNovel View Synthesis\nPortrait Synthesis by Mixed Guidance\nPrompt:\nCartoon style\nExemplar Style\n+\nMixing\nFigure 1. Applications of our proposed framework for identity-preserving image synthesis. Our method can preserve the subject\u2019s\nidentity in the synthesized images with high fidelity.\nAbstract\nThis study investigates identity-preserving image synthe-\nsis, an intriguing task in image generation that seeks to\nmaintain a subject\u2019s identity while adding a personalized,\nstylistic touch. Traditional methods, such as Textual Inver-\nsion and DreamBooth, have made strides in custom image\ncreation, but they come with significant drawbacks. These\ninclude the need for extensive resources and time for fine-\ntuning, as well as the requirement for multiple reference\nimages.\nTo overcome these challenges, our research in-\ntroduces a novel approach to identity-preserving synthesis,\nwith a particular focus on human images. Our model lever-\nages a direct feed-forward mechanism, circumventing the\nneed for intensive fine-tuning, thereby facilitating quick and\n*Equal contributions.\n\u2020Corresponding Author.\nefficient image generation. Central to our innovation is a\nhybrid guidance framework, which combines stylized im-\nages, facial images, and textual prompts to guide the image\ngeneration process. This unique combination enables our\nmodel to produce a variety of applications, such as artistic\nportraits and identity-blended images. Our experimental\nresults, including both qualitative and quantitative evalua-\ntions, demonstrate the superiority of our method over exist-\ning baseline models and previous works, particularly in its\nremarkable efficiency and ability to preserve the subject\u2019s\nidentity with high fidelity.\n1. Introduction\nIn recent years, artificial intelligence (AI) has driven signif-\nicant progress in the domain of creativity, leading to trans-\nformative changes across various applications. Particularly,\narXiv:2312.02663v2  [cs.CV]  6 Dec 2023\ntext-to-image diffusion models [41, 42] have emerged as a\nnotable development, capable of converting textual descrip-\ntions into visually appealing, multi-styled images. Such ad-\nvancements have paved the way for numerous applications\nthat were once considered to be beyond the realms of pos-\nsibility.\nHowever, despite these advancements, several chal-\nlenges remain. One of the most prominent is the difficulty\nfaced by existing text-to-image diffusion models in accu-\nrately capturing and describing an existing subject based\nsolely on textual descriptions. This limitation becomes even\nmore evident when detailed nuances, like human facial fea-\ntures, are the subject of generation. Consequently, there\nis a rising interest in the exploration of identity-preserving\nimage synthesis, which encompasses more than just tex-\ntual cues. In comparison to standard text-to-image gener-\nation, it integrates reference images in the generative pro-\ncess, thereby enhancing the capability of models to produce\nimages tailored to individual preferences.\nIn pursuit of this idea, various methods have been pro-\nposed, with techniques such as DreamBooth [43] and Tex-\ntual inversion [14] leading the way.\nThey primarily fo-\ncus on adjusting pre-trained text-to-image models to align\nmore closely with user-defined concepts using reference im-\nages. However, these methods come with their set of limi-\ntations. The fine-tuning process, essential to these methods,\nis resource-intensive and time-consuming, often demand-\ning significant computational power and human interven-\ntion. Moreover, the requirement for multiple reference im-\nages for accurate model fitting poses additional challenges.\nIn light of these constraints, our research introduces a\nnovel approach focusing on identity-preserving image syn-\nthesis, especially for human images. Our model, in contrast\nto existing ones, adopts a direct feed-forward approach,\neliminating the cumbersome fine-tuning steps and offering\nrapid and efficient image generation. Central to our model\nis a hybrid guidance module, which guides the image gen-\neration of the latent diffusion model. This module not only\nconsiders textual prompts as conditions for image synthe-\nsis but also integrates additional information from the style\nimage and the identity image. By employing this hybrid-\nguided strategy, our framework places additional emphasis\non the identity details from a given human image during\ngenerations. To effectively manage images with multiple\nidentities, we develop a multi-identity cross-attention mech-\nanism, which enables the model to aptly associate guidance\nparticulars from various identities with specific human re-\ngions within an image.\nOur training method is intuitive yet effective. Our model\ncan be easily trained with human image datasets. By em-\nploying images with the facial features masked as the style\nimage input and the extracted face as the identity input, our\nmodel learns to reconstruct the human images while high-\nlighting identity features in the guidance. After training, our\nmodel showcases an impressive ability to synthesize human\nimages that retain the subject\u2019s identity with exceptional fi-\ndelity, obviating the need for any further adjustments. A\nunique aspect of our method is its ability to superimpose\na user\u2019s facial features onto any stylistic image, such as a\ncartoon, enabling users to visualize themselves in diverse\nstyles without compromising their identity. Additionally,\nour model excels in generating images that amalgamate\nmultiple identities when supplied with the respective refer-\nence photos. Fig. 1 shows some applications of our model.\nThis paper\u2019s contributions can be briefly summarized as\nfollows:\n\u2022 We present a tuning-free hybrid-guidance image gener-\nation framework capable of preserving human identities\nunder various image styles.\n\u2022 We develop a multi-identity cross-attention mechanism,\nwhich exhibits a distinct ability to map guidance details\nfrom multiple identities to specific human segments in an\nimage.\n\u2022 We provide comprehensive experimental results, both\nqualitative and quantitative, to highlight the superiority\nof our method over baseline models and existing works,\nespecially in its unmatched efficiency.\n2. Related Work\nText-to-image diffusion models. Diffusion models have\nrecently come to the forefront of generative model research.\nTheir exceptional capability to generate high-quality and\ndiverse images has placed them at a distinct advantage\nover their predecessors, namely the GAN-based and auto-\nregressive image generation models. This new generation\nof diffusion models, having the capacity to produce state-\nof-the-art synthesis results, owes much of its success to\nbeing trained on image-text pairs data at a billion scale.\nThe integration of textual prompts into the diffusion model\nserves as a vital ingredient in the development of various\ntext-to-image diffusion models. Distinguished models in\nthis domain include GLIDE [38], DALL\u00b7E 2 [41], Ima-\ngen [45], and StableDiffusion [42]. These models lever-\nage text as guidance during the image generation process.\nConsequently, they have shown considerable proficiency\nin synthesizing high-quality images that closely align with\nthe provided textual description. Compared to previous ef-\nforts [29, 39, 51, 69] on face image synthesis based on Gen-\nerative Adversarial Networks (GANs) [25], diffusion mod-\nels exhibit greater stability during training and demonstrate\nenhanced capability in effectively integrating diverse forms\nof guidance, such as texts and stylized images.\nHowever, textual descriptions for guidance, while im-\nmensely useful, often fall short when it comes to the gen-\neration of complex and nuanced details that are frequently\nassociated with certain subjects. For instance, generating\nClip Image \nEncoder\nPrior Model\nGenerated Image\nAddition\nU-Net\nei\nep\neg\nef\neh\neT\nLinear\nFrozen modules\nTrainable modules\n\"A man in the style\nof Manmaru\"\nID Extractor\nef\nCrop\nFigure 2. Hybrid-Guidance Identity-Preserving Image Synthesis Framework. Our model, built upon StableDiffusion, utilizes text\nprompts and reference human images to guide image synthesis while preserving human identity through an identity input.\nimages of human faces proves challenging with this ap-\nproach. While generating images of celebrities might be\nfeasible due to the substantial presence of their photographs\nin the training data that can be linked to their names, it be-\ncomes an uphill task when it comes to generating images of\nordinary people using these text-to-diffusion models.\nSubject-driven image generation. Subject-driven im-\nage generation seeks to overcome the limitations posed by\ntext-to-image synthesis models. Central to this novel re-\nsearch area is the inclusion of subject-specific reference\nimages, which supplement the textual description to yield\nmore precise and personalized image synthesis. To this end,\nseveral optimization-based methods have been employed,\nwith popular ones being Low-Rank Adaptation (LoRA),\nDreamBooth [43], Textual Inversion [14], and Hypernet-\nwork [3].\nThese methods typically involve fine-tuning\na pre-trained text-to-image framework or textual embed-\ndings to align the existing model with user-defined con-\ncepts, as indicated by a set of example images.\nThere\nare several other methods derived from these works such\nas Unitune [49], HyperDreamBooth [44], EasyPhoto [53],\nFaceChain [33], etc. [4, 23, 60]. However, these methods\npose some challenges. For one, they are time-consuming\ndue to the need for model fine-tuning, which also re-\nquires multiple reference images to achieve accurate fit-\nting. Overfitting also becomes a potential issue with these\noptimization-based methods, given their heavy reliance on\nexample images. In response to these challenges, recent\nstudies have proposed various improved methods. Drea-\nmArtist [13], for instance, mitigates the problem of over-\nfitting by incorporating both positive and negative embed-\ndings and jointly training them. Similarly, E4T [15] in-\ntroduced an encoder-based domain-tuning method to ac-\ncelerate the personalization of text-to-image models, of-\nfering a faster solution to model fitting for new subjects.\nThere are large numbers of similar methods which are\nencoder-based including Composer [21], ProFusion [68]\nMagiCapture [22], IP-Adapter [59], ELITE [52], Disen-\nBooth [8], Face0 [50], PhotoVerse [9], AnyDoor [11], Sin-\ngleInsert [54], etc. [24, 35, 56].\nAlongside optimization-based methods, a number of\ntuning-free methods have been concurrently proposed, such\nas InstantBooth [47]. This method converts input images\ninto a textual token for general concept learning and in-\ntroduces adapter layers to obtain rich image representation\nfor generating fine-grained identity details.\nHowever, it\ncomes with the drawback of having to train the model sep-\narately for each category. Bansal et al.[6] put forward a\nuniversal guidance algorithm that enables diffusion models\nto be controlled by arbitrary guidance modalities without\nthe need to retrain the network. Similarly, Yang et al.[57]\npropose an inpainting-like diffusion model that learns to in-\npaint a masked image under the guidance of example im-\nages of a given subject.\nSimilar methods leveraged by\ninversion-based personalization including Cao et al. [7],\nHan et al. [17], Gu et al. [16], Mokady et al. [37]. Be-\nsides, Controlnet [67] is also an effective way to person-\nalize. In addition, there are some solutions that use edit-\ning to achieve the purpose of maintaining identity, such as\nSDEdit [36], Imagic [26], etc.\n[18, 48]. Based on all the\nsolutions mentioned above, the method of combining mul-\ntiple objects while maintaining identity is also widely used\nin Break-a-scene [5], FastComposer [55], Cones [34] and\nMultiConceptCustomDiffusion [27].\nWithin the text-to-image community, one specific area of\ninterest that has garnered significant attention is the gener-\nation of human images. Human-centric image generation,\nowing to its vast applicability and widespread popularity,\nconstitutes a substantial proportion of posts in the commu-\nnity, such as Civitai [1]. In light of these considerations,\nour work primarily focuses on the preservation of human\nidentity during the image generation process.\nQ\nV\nK\nAttention\nFeature\nGuidance\n(a) Baseline cross-attentions within the StableDiffusion model.\nQ\nV\nK\nID 1\nID 2\nRegion 1 Region 2\nAttention\n(b) Enhanced cross-attentions optimized for multi-identity synthesis.\nFigure 3. Comparison between standard cross-attentions in single-\nidentity modeling (a) and the advanced cross-attentions tailored\nfor multi-identity integration (b).\nFew-shot Learning.\nIt is a common practice in the\nfield of few-shot learning literature to convert optimization-\nbased tasks into a feed-forward approach, which can signif-\nicantly reduce computation time. Numerous previous stud-\nies in the realm of few-shot learning have adopted this ap-\nproach to address computer vision tasks, including but not\nlimited to classification [63\u201366], detection [58], and seg-\nmentation [10, 28, 30\u201332, 61, 62]. In this research, our\nobjective aligns with this approach as we aim to integrate\nidentity information into the generative process through a\nfeed-forward module.\n3. Method\nIn this section, we present the design and functionalities\nof our novel framework. Our method fundamentally builds\non StableDiffusion [42], with several pivotal modifications,\nespecially in the condition modules catering to hybrid-\nguidance image generation. We start by elaborating on our\nhybrid guidance design in the proposed condition module.\nFollowing that, we delve into the mechanism for managing\nmultiple identities within images. Lastly, we discuss the\ntraining strategy of our models. The overview of our model\nstructure is shown in Fig. 2.\n3.1. Hybrid Guidance Strategy\nDisentangling Identities from Style Images Given our re-\nsearch\u2019s main aim is to keep human identities intact during\nimage generation under various styles, it is indispensable\nto extract salient identity features from human images for\nconditional image generation. Building on this understand-\ning, our first step is setting up an image-condition guidance\nmodule, which aims to take stylized human pictures as the\nmain data for the condition modules. We additionally incor-\nporate a human identity input in this module, which works\nwith face pictures to guide the synthesis of human images.\nIntuitively, the stylized human pictures also referred to as\nstyle images, specify image content and style, while the\nidentity input provides fine-grained identity guidance. More\nspecifically, we deploy the CLIP vision encoder [40] to pro-\ncess the human images Ih, resulting in eh = CLIPV(Ih).\nConcurrently, the Arcface model [12] attends to a face im-\nage If, leading to ef = Arcface(If). These two derived\nembeddings are then combined, and a linear layer processes\nthe combined data: ei = Linear(eh||ef), where || denotes\nthe concatenation function. This design effectively extracts\nand disentangles the identity-related representations from\nthe overall human image content. By doing so, it equips\nthe model with the capability to specifically focus on hu-\nman identities during the synthesis process. An intrinsic\nadvantage of this separated design is the flexibility it offers.\nAfter training, by simply swapping the facial image in the\ncondition module, we can seamlessly modify the identity\nwithin a human image, ensuring adaptability to diverse re-\nquirements.\nIncorporating Textual Guidance To further allow tex-\ntual prompts as guidance for conditional image generation,\na prior model [41] is employed to translate textual descrip-\ntions T into image embeddings eT . To achieve this, the\nprior model is pre-trained to map the CLIP text embed-\ndings to their corresponding CLIP vision embeddings, re-\nsulting in eT = Prior(CLIPT(T)). Following this, a lin-\near layer is deployed to integrate the textual guide with the\nidentity information, formulated as ep = Linear(eT ||ef).\nGiven that the prior model\u2019s output shares the embedding\nspace with the CLIP vision encoder\u2019s output, both branches\nin our framework use shared linear layer parameters for\nidentity fusion. With these steps, the model is equipped\nwith dual guiding mechanisms for image generation: hu-\nman photographs under various styles and text prompts,\nboth enriched with identity information. The two types of\nguidance are then merged linearly, using a hyper-parameter\n\u03b1, which provides the final guidance embedding eg: eg =\n\u03b1ei + (1 \u2212 \u03b1)ep. Finally, this guidance embedding is fused\ninto the U-Net with cross-attentions, as is done in StableD-\niffusion [42].\n3.2. Handling Multiple Identities\nOur design can adeptly fuse multiple identities when given\ntheir respective face photos. This merging is done by blend-\ning their face embeddings, presented as ef = P\ni=1 \u03b2iei\nf,\nwhere ei\nf is the facial embedding of the ith identity, and \u03b2i\nis a weight factor. Yet, a challenge arises when we intend to\nsuperimpose varying identities onto multiple humans within\na single image. Given the standard process in StableDiffu-\nsion [42], guidance information, eg, is fused into the in-\ntermediate U-Net feature maps, F, using a cross-attention\nlayer, represented as\n\u02c6F = Attention(Q, K, V ).\n(1)\nHere, Q originates from flattened intermediate features F,\nwhile K and V come from the guidance embedding eg. In\nsituations with several humans in the content images, we\naim for each human\u2019s generation to reference unique iden-\ntity data. If there are N humans and N identities, with the\naim of a 1-to-1 identity match, our approach ensures that\nfeatures in F from the ith human region solely access infor-\nmation from the ith identity. This is denoted as\n\u02c6\nF i = Attention(Qi, Ki, V i),\n(2)\nwhere Qi is obtained from the specific features of the ith hu-\nman, and Ki and V i are derived from the guidance vector\nof the ith identity. This feature selection operation can be\neasily implemented with the help of an off-the-shelf human\ninstance segmentation model. An illustration of our design\nis shown in Fig. 3. The strength of our design lies in its pre-\ncision in managing multiple identities, ensuring that each\nidentity aligns with its corresponding human figure in the\ncontent image. For features in non-human regions, they can\nrandomly choose guidance embeddings for guidance, since\nall guidance embeddings contain information about the styl-\nized human image, with differences only in the identity in-\nputs. Alternatively, the latent embeddings in these regions\ncan be fixed, similar to the approach used in StableDiffu-\nsion [42] for inpainting tasks.\n3.3. Training\nWe train our model with a human image reconstruction task,\nconditioned on the input human image and the identity in-\nput. Specifically, raw images with the face region masked\nserve as the stylized human image, while the cropped face\nfrom the same image acts as the identity input. This strat-\negy effectively separates identity information from the over-\nall image content for guided image generation. Our model\nis optimized by only using the image-conditioned branch\nin our condition module, as the linear layer in the text-\nconditioned branch shares the parameters with the one in\nthe image-condition branch. This obviates the need for text\nannotations for human images, which are often hard to ac-\nquire on a large scale. We keep the parameters of both the\nCLIP vision encoder and the Arcface model frozen, focus-\ning our optimization on the newly introduced linear layer\nand the U-Net model. In line with StableDiffusion [42],\nour U-Net model, denoted as \u03b5\u03b8(), is trained to denoise la-\ntent representations produced by a pre-trained VAE encoder\nE(). This is captured by:\nLDM := EE(x),\u03f5\u223cN(0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, eg)\u22252\n2\ni\n,\n(3)\nRaw \nimage\nOurs\nw/o\nID input\nOurs\nFigure 4. Influence of identity input on image construction.\nThe addition of identity input proves to be effective in preserving\nthe subject\u2019s identity within the generated image.\nID\nReference view \nResult\nReference view \nResult\nFigure 5. Identity-preserving novel view synthesis experiment.\nOur method excels at generating new views of a subject while\nmaintaining its identity.\nwhere x is a sampled human image, t denotes the sampled\nde-noising step, and eg represents the guidance embedding\ngenerated by our condition module. We compute the MSE\nloss between the sampled noise \u03f5 and the estimated noise\n\u03f5\u03b8(zt, t, eg) for optimization.\n4. Experiments\n4.1. Implementation details.\nThe vision encoder utilized in the image-conditioned\nbranch of our model combines three CLIP model [40] vari-\nants with different backbones. These are: CLIP-ViT-L/14,\nCLIP-RN101, and CLIP-ViT-B/32. The outputs from these\nindividual models are concatenated to produce the final out-\nput of our vision encoder. Our approach primarily utilizes\nthe DDPM configuration [20] as described in StableDiffu-\nsion [42] for training. Specifically, we incorporated a total\nof 1,000 denoising steps. For the inference stage, we use the\nInput \nimage\nStyle \nimage\nText condition and style image interpolation\nFigure 6. Hybrid-guidance experiments.\nIn this experiment, we employ an approach that combines textual prompts and reference\nimages for image synthesis, and the text prompt used here pertains to the cartoon style.\nStyle\nImage\nInput\nID A\nID Interpolation\nInput\nID B\nFigure 7. Identity mixing experiment. We generate facial images that combine multiple identities using a mixing ratio to control the\ninfluence of different IDs.\nEulerA sampler [2] and set it to operate over 25 timesteps.\nTo align with the training methodology of classifier-free\nguidance [19], we introduced variability by randomly omit-\nting the conditional embeddings related to both style images\nand face images. Specifically, the probabilities for dropping\nthese embeddings were set at 0.64 for style images and 0.1\nfor face images.\nThe primary dataset used for training was FFHQ [25],\nwhich is a face image dataset encompassing 70,000 im-\nages. To augment this, we also incorporated a subset of the\nLAION dataset [46] into our training phase, which aims to\nensure the model retains the capability to generate generic,\nnon-human images during the finetuning process. It\u2019s worth\nnoting that when non-human images are sampled for train-\ning, the face embedding in the conditional branch is set to\nzero. During training, we set the learning rate at 1e-6. The\nmodel was trained using 8 A100 GPUs, with a batch size of\n256, and was trained for 100,000 steps.\n4.2. Results\nWe provide quantitative and qualitative results for compari-\nson and analysis in this section.\nQuantitative comparison with baselines. To evaluate\nour model\u2019s ability to preserve identity during image gen-\neration, we conduct a quantitative comparison with base-\nlines. We measure the Arcface feature cosine similarity [12]\nbetween the face regions of input reference human images\nand the generated images, with values in the range of (-1,\n1). We consider both 1-shot and multi-shot cases, where\nonly one reference image and 11 reference images are pro-\nvided, respectively. The baselines we compare against in-\nclude DreamBooth [43] and Textual Inversion [14], which\nare the most popular tuning-based methods, which are op-\ntimized with 1K and 3K iterations, respectively. We also\nincorporate our model variant that removes the identity in-\nput for comparison to measure the influence of identity in-\nformation. For the multi-shot experiment, we compute the\nface similarity score between the generated image and each\nreference image and report the mean score for comparison.\nIdentity\nBaseline\nOurs\nBaseline\nOurs\nIdentity\nFigure 8. Comparison of multi-human image synthesis. Our model\u2019s effectiveness is evident when compared to our model variant that\nremoves our proposed multi-human cross-attention mechanisms.\nPhotoVerse\nOurs\nFigure 9. Comparison with state-of-the-art methods in identity-preserving text-to-image generation. This figure illustrates a compar-\native analysis of our model against state-of-the-art methods in the task of identity-preserving text-to-image generation. The evaluation is\nconducted using the same samples employed in FaceNet\u2019s experiments. Notably, our model consistently achieves comparable or superior\nqualitative results across all the examples.\nFace Similarity \u2191\nTime (s) \u2193\nSingle-Image\nMulti-Image\nTuning\nInference\nSum\nDreamBooth [43]\n0.65\n0.58\n405\n2.8\n407.8\nTextual Inversion [14]\n0.31\n0.27\n3425\n2.8\n3427.8\nOurs w/o ID input\n0.47\n0\n3.6\n3.6\nOurs w/ text\n0.71\n0.61\n0\n5.3 + 0.04 N\n5.3 + 0.04 N\nOurs w/ image\n0.86\n0.74\n0\n3.6 + 0.04 N\n3.6 + 0.04 N\nTable 1. Comparison of face similarity and generation time for identity-preserving image generation. Our methods, guided by both\ntexts and images, exhibit remarkable advantages compared to baseline approaches in terms of both face similarity and generation time. The\nvariable N represents the number of reference images per identity. Notably, the omission of identity guidance input in our design results in\na substantial drop in performance.\nWe have conducted experiments on 11 different identities,\nand we report the average performance for comparison, and\nthe results are shown in Table 1. Our model demonstrates\nsuperior performance in both 1-shot and multi-shot cases,\nhighlighting the effectiveness of our design in preserving\nidentities. Notably, our advantages stem from the inclusion\nof our identity-guidance branch, as demonstrated by the per-\nformance drop in the baselines that lack this guidance.\nInfluence of the identity input. To further investigate\nthe influence of the identity input, we conducted an abla-\ntion study through qualitative comparisons. We compare\nour model with a baseline model that removes the identity-\nguidance branch, acting as an image reconstruction model.\nThe results are displayed in Fig. 4. The image reconstruc-\ntion baseline roughly preserves image content but strug-\ngles with fine-grained identity information. In contrast, our\nmodel successfully extracts identity information from the\nidentity-guidance branch, resulting in improved results for\nthe face region.\nNovel view synthesis. We conduct novel view synthe-\nsis experiments to validate our algorithm\u2019s effectiveness in\nsynthesizing images with large pose changes while preserv-\ning identity. Results are presented in Fig. 5. The result\ndemonstrates that our algorithm can synthesize high-quality\nimages with preserved identities even when faced with sig-\nnificant pose changes, which showcases the robustness of\nour design.\nDual guidance experiment. We explore the combined\nuse of text-based and image-based guidance during infer-\nence by controlling the strength of both types of guidance\nusing the hyperparameter, \u03b1, within the range [0,1]. The\nresults, presented in Fig. 6, illustrate that our model can ef-\nfectively utilize both types of guidance for image synthe-\nsis while preserving identity information. By adjusting \u03b1,\nwe could see how the influence of each type of guidance\nchanged.\nIdentity mixing experiment. Our model\u2019s ability to mix\nidentity information from different humans during image\nsynthesis is showcased in Fig. 7. By controlling the mix\nratio within the range [0,1], we assign weights to differ-\nent identities. As is shown, our model effectively combines\nidentity information from different people and synthesizes\nnew identities with high fidelity.\nMulti-human image generation.\nOne of our model\u2019s\nunique features is synthesizing multi-human images from\nmultiple identities. We present the results in Fig. 8, by com-\nparing our design to a baseline using vanilla cross-attention\nmechanisms. As the result shows, our model effectively\ncorrelates different human regions with different identities\nwith our proposed enhanced cross-attention mechanisms to\ndifferentiate between identities, while the baseline results in\nconfused identities in the human regions.\nMore qualitative results.\nFig.10 showcases a new\nimage synthesis method, which is an extension of the\nimage-to-image generation technique originally found in\nStableDiffusion[42].\nThis method has only minor varia-\ntions from the primary pipeline discussed in the main pa-\nper. In this approach, the diffusion process begins with the\nnoisy version of the raw human images\u2019 latent representa-\ntions, with the rest of the process unchanged. This specific\nmodification ensures that the synthesized images retain a\nlayout similar to the original images. Our results demon-\nstrate that, despite these adjustments, the method success-\nfully maintains the identity of subjects in the synthesized\nimages. Additionally, Fig. 11 provides more qualitative re-\nsults of our model when applied to a broader range of im-\nage styles. These results highlight the model\u2019s adaptability\nto various artistic styles while still holding true to the core\nobjective of identity preservation. This versatility is crucial\nfor applications that require a balance between stylistic ex-\npression and the need to maintain recognizable features of\nthe subject.\n5. Conclusion\nIn this paper, we present an innovative approach to text-to-\nimage generation, specifically focusing on preserving iden-\ntity in the synthesized images. Our method significantly ac-\ncelerates and enhances the efficiency of the image genera-\ntion process. Central to our approach is the hybrid guidance\nstrategy, which combines stylized and facial images with\ntextual prompts, guiding the image generation process in a\ncohesive manner. A standout feature of our method is its\nFigure 10. Image-to-image synthesis with our proposed method.\nOur model preserves the identities of humans and the layout in the\nraw images.\nability to synthesize multi-human images, thanks to our de-\nveloped multi-identity cross-attention mechanisms. Our ex-\ntensive experimental evaluations, both qualitative and quan-\ntitative, have shown the advantages of our method. It sur-\npasses baseline models and previous works in several key\naspects, most notably in efficiency and the ability to main-\ntain identity integrity in the synthesized images.\nLimitation and Social Impacts. Compared to existing\nworks like DreamBooth [43], which synthesize images of\ndiverse subjects such as animals and objects, our model is\nspecifically tailored for identity-preserving generation, ex-\nclusively targeting human images. Our text-to-image gen-\neration research has two key societal impacts to consider:\n1) Intellectual Property Concerns. The ability of our model\nto create detailed and stylized images raises potential issues\nwith copyright infringement. 2) Ethical Considerations in\nFacial Generation. The model\u2019s capability to replicate hu-\nman faces brings up ethical issues, especially the potential\nfor creating offensive or culturally inappropriate images.\nIt\u2019s crucial to use this technology responsibly and establish\nFigure 11. More qualitative results.\nOur model obtains a balance between stylistic expression and the need to maintain recognizable\nfeatures of the subject.\nguidelines to prevent its misuse in sensitive contexts.\nReferences\n[1] Stable diffusion models, embeddings, loras and more. 3\n[2] Implementation of EulerAncestralDiscreteScheduler based\non k-diffusion. 6\n[3] Implementation of Hypernetwork. 3\n[4] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel\nCohen-Or. A neural space-time representation for text-to-\nimage personalization.\narXiv preprint arXiv:2305.15391,\n2023. 3\n[5] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 3\n[6] Arpit\nBansal,\nHong-Min\nChu,\nAvi\nSchwarzschild,\nSoumyadip Sengupta,\nMicah Goldblum,\nJonas Geip-\ning, and Tom Goldstein. Universal guidance for diffusion\nmodels.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 843\u2013852,\n2023. 3\n[7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 3\n[8] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,\nYuwei Zhou, and Wenwu Zhu.\nDisenbooth:\nIdentity-\npreserving disentangled tuning for subject-driven text-to-\nimage generation. arXiv preprint arXiv:2305.03374, 2023.\n3\n[9] Li Chen,\nMengyi Zhao,\nYiheng Liu,\nMingxu Ding,\nYangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing\nLiu, Kang Du, et al.\nPhotoverse:\nTuning-free image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2309.05793, 2023. 3\n[10] Xiaoyu Chen, Chi Zhang, Guosheng Lin, and Jing Han.\nCompositional prototype network with multi-view compar-\nision for few-shot point cloud semantic segmentation. arXiv\npreprint arXiv:2012.14255, 2020. 4\n[11] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n3\n[12] Jiankang Deng, J. Guo, and S. Zafeiriou. Arcface: Additive\nangular margin loss for deep face recognition.\nComputer\nVision and Pattern Recognition, 2018. 4, 6\n[13] Ziyi\nDong,\nPengxu\nWei,\nand\nLiang\nLin.\nDrea-\nmartist: Towards controllable one-shot text-to-image gen-\neration via contrastive prompt-tuning.\narXiv preprint\narXiv:2211.11337, 2022. 3\n[14] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2, 3, 6, 8\n[15] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or.\nEncoder-based do-\nmain tuning for fast personalization of text-to-image models.\narXiv preprint arXiv:2302.12228, 2023. 3\n[16] Jing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong,\nQing Liu, Zhifei Zhang, He Zhang, Jianming Zhang, Hyun-\nJoon Jung, et al. Photoswap: Personalized subject swapping\nin images. arXiv preprint arXiv:2305.18286, 2023. 3\n[17] Ligong Han, Song Wen, Qi Chen, Zhixing Zhang, Kunpeng\nSong, Mengwei Ren, Ruijiang Gao, Yuxiao Chen, Di Liu\n0003, Qilong Zhangli, et al. Improving tuning-free real im-\nage editing with proximal guidance. CoRR, 2023. 3\n[18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 3\n[19] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[20] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion\nprobabilistic models.\nNeural Information Processing Sys-\ntems, 2020. 5\n[21] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 3\n[22] Junha Hyung, Jaeyo Shin, and Jaegul Choo. Magicapture:\nHigh-resolution multi-concept portrait customization. arXiv\npreprint arXiv:2309.06895, 2023. 3\n[23] Hyeonho Jeong, Gihyun Kwon, and Jong Chul Ye. Zero-shot\ngeneration of coherent storybook from plain text story using\ndiffusion models. arXiv preprint arXiv:2302.03900, 2023. 3\n[24] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 3\n[25] Tero Karras, S. Laine, and Timo Aila. A style-based gener-\nator architecture for generative adversarial networks. Com-\nputer Vision and Pattern Recognition, 2018. 2, 6\n[26] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 6007\u20136017, 2023. 3\n[27] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 3\n[28] Lvlong Lai, Jian Chen, Chi Zhang, Zehong Zhang, Gu-\nosheng Lin, and Qingyao Wu. Tackling background ambi-\nguities in multi-class few-shot point cloud semantic segmen-\ntation. Knowledge-Based Systems, 253:109508, 2022. 4\n[29] Lingzhi Li, Jianmin Bao, Hao Yang, Dong Chen, and Fang\nWen. Faceshifter: Towards high fidelity and occlusion aware\nface swapping. arXiv preprint arXiv:1912.13457, 2019. 2\n[30] Weide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu. Cr-\nnet: Cross-reference networks for few-shot segmentation.\nIn IEEE Conf. Computer Vision and Pattern Recognition\n(CVPR), 2020. 4\n[31] Weide Liu, Chi Zhang, Henghui Ding, Tzu-Yi Hung, and\nGuosheng Lin. Few-shot segmentation with optimal trans-\nport matching and message flow. IEEE Transactions on Mul-\ntimedia (TMM), 2021.\n[32] Weide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu. Crc-\nnet: Few-shot segmentation with cross-reference and region-\nglobal conditional networks. International Journal of Com-\nputer Vision (IJCV), 2022. 4\n[33] Yang Liu, Cheng Yu, Lei Shang, Ziheng Wu, Xingjun Wang,\nYuze Zhao, Lin Zhu, Chen Cheng, Weitao Chen, Chao Xu,\net al. Facechain: A playground for identity-preserving por-\ntrait generation. arXiv preprint arXiv:2308.14256, 2023. 3\n[34] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng\nZheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao.\nCones: Concept neurons in diffusion models for customized\ngeneration. arXiv preprint arXiv:2303.05125, 2023. 3\n[35] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 3\n[36] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 3\n[37] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 3\n[38] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n[39] Ivan Perov, Daiheng Gao, Nikolay Chervoniy, Kunlin Liu,\nSugasa Marangonda, Chris Um\u00b4e, Mr Dpfks, Carl Shift\nFacenheim, Luis RP, Jian Jiang, et al.\nDeepfacelab: In-\ntegrated, flexible and extensible face-swapping framework.\narXiv preprint arXiv:2005.05535, 2020. 2\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 4, 5\n[41] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2, 4\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 4, 5, 8\n[43] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3, 6, 8, 9\n[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 3\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[46] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, P. Schramowski, Srivatsa Kundurthy, Katherine Crow-\nson, Ludwig Schmidt, R. Kaczmarczyk, and J. Jitsev. Laion-\n5b: An open large-scale dataset for training next generation\nimage-text models. Neural Information Processing Systems,\n2022. 6\n[47] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 3\n[48] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon.\nKey-locked rank one editing for text-to-image personaliza-\ntion.\nIn ACM SIGGRAPH 2023 Conference Proceedings,\npages 1\u201311, 2023. 3\n[49] Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv\nLeviathan. Unitune: Text-driven image editing by fine tuning\nan image generation model on a single image. arXiv preprint\narXiv:2210.09477, 2022. 3\n[50] Dani Valevski, Danny Wasserman, Yossi Matias, and Yaniv\nLeviathan. Face0: Instantaneously conditioning a text-to-\nimage model on a face. arXiv preprint arXiv:2306.06638,\n2023. 3\n[51] Yuhan Wang, Xu Chen, Junwei Zhu, Wenqing Chu, Ying\nTai, Chengjie Wang, Jilin Li, Yongjian Wu, Feiyue Huang,\nand Rongrong Ji.\nHififace:\n3d shape and semantic\nprior guided high fidelity face swapping.\narXiv preprint\narXiv:2106.09965, 2021. 2\n[52] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 3\n[53] Ziheng Wu, Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Xing Shi,\nand Jun Huang. Easyphoto: Your smart ai photo generator.\narXiv preprint arXiv:2310.04672, 2023. 3\n[54] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang Bai.\nSingleinsert: Inserting new concepts from a single image\ninto text-to-image models for flexible editing. arXiv preprint\narXiv:2310.08094, 2023. 3\n[55] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 3\n[56] Xingqian Xu, Jiayi Guo, Zhangyang Wang, Gao Huang, Ir-\nfan Essa, and Humphrey Shi. Prompt-free diffusion: Taking\u201d\ntext\u201d out of text-to-image diffusion models. arXiv preprint\narXiv:2305.16223, 2023. 3\n[57] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18381\u201318391,\n2023. 3\n[58] Ze Yang, Chi Zhang, Ruibo Li, and Guosheng Lin. Efficient\nfew-shot object detection via knowledge inheritance. IEEE\nTransactions on Image Processing (TIP), 2022. 4\n[59] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 3\n[60] Ge Yuan,\nXiaodong Cun,\nYong Zhang,\nMaomao Li,\nChenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb ba-\nsis. arXiv preprint arXiv:2306.00926, 2023. 3\n[61] Chi Zhang, Guosheng Lin, Fayao Liu, Jiushuang Guo,\nQingyao Wu, and Rui Yao. Pyramid graph networks with\nconnection attentions for region-based one-shot semantic\nsegmentation. In IEEE International Conference on Com-\nputer Vision (ICCV), 2019. 4\n[62] Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua\nShen. Canet: Class-agnostic segmentation networks with it-\nerative refinement and attentive few-shot learning. In IEEE\nConf. Computer Vision and Pattern Recognition (CVPR),\n2019. 4\n[63] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\nDeepemd: Few-shot image classification with differentiable\nearth mover\u2019s distance and structured classifiers. In IEEE\nConf. Computer Vision and Pattern Recognition (CVPR\nORAL), 2020. 4\n[64] Chi Zhang, Henghui Ding, Guosheng Lin, Ruibo Li,\nChanghu Wang, and Chunhua Shen. Meta navigator: Search\nfor a good adaptation policy for few-shot learning. In IEEE\nInternational Conference on Computer Vision (ICCV), 2021.\n[65] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan,\nand Yinghui Xu. Few-shot incremental learning with contin-\nually evolved classifiers. IEEE Conf. Computer Vision and\nPattern Recognition (CVPR), 2021.\n[66] Chi Zhang, Yujun Cai, Guosheng Lin, and Chunhua Shen.\nDeepemd: Differentiable earth mover\u2019s distance for few-shot\nlearning. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence (TPAMI), 2022. 4\n[67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3\n[68] Yufan Zhou, Ruiyi Zhang, Tong Sun, and Jinhui Xu. Enhanc-\ning detail preservation for customized text-to-image gen-\neration:\nA regularization-free approach.\narXiv preprint\narXiv:2305.13579, 2023. 3\n[69] Yuhao Zhu, Qi Li, Jian Wang, Cheng-Zhong Xu, and Zhenan\nSun. One shot face swapping on megapixels. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4834\u20134844, 2021. 2\n"
  },
  {
    "title": "X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion Model",
    "link": "https://arxiv.org/pdf/2312.02238.pdf",
    "upvote": "24",
    "text": "X-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion\nModel\nLingmin Ran1\nXiaodong Cun3\nJia-Wei Liu1\nRui Zhao1\nSong Zijie4\nXintao Wang3\nJussi Keppo2\nMike Zheng Shou1,*\n1Showlab,\n2National University of Singapore\n3Tencent AI Lab\n4Fudan University\nX-Adapter\nPretrained Plugins for\nStable Diffusion 1.5\nControlNet-Depth\nLoRA:shukezouma\nFinetuned SD1.5\nInstructpix2pix\n( Conditions or\nStyle Examples )\nT2IAdapter-Canny\nControlNet-tile\nUpgraded\nModel\n(SDXL)\nFigure 1. Given pretrained plug-and-play modules (e.g., ControlNet, LoRA) of the base diffusion model (e.g., Stable Diffusion 1.5), the\nproposed X-Adapter can universally upgrade these plugins, enabling them directly work with the upgraded Model (e.g., SDXL) without\nfurther retraining. Text prompts: \u201c1girl, solo, smile, looking at viewer, holding flowers\u201d \u201cApply face paint\u201d \u201c1girl, upper body, flowers\u201d\n\u201cA cute cat holding a gun\u201d \u201cBest quality, extremely detailed\u201d \u201cA fox made of water\u201d from left to right, top to bottom.\nAbstract\nWe introduce X-Adapter, a universal upgrader to en-\nable the pretrained plug-and-play modules (e.g., Control-\nNet, LoRA) to work directly with the upgraded text-to-image\ndiffusion model (e.g., SDXL) without further retraining. We\nachieve this goal by training an additional network to con-\ntrol the frozen upgraded model with the new text-image data\npairs. In detail, X-Adapter keeps a frozen copy of the old\nmodel to preserve the connectors of different plugins. Ad-\nditionally, X-Adapter adds trainable mapping layers that\nbridge the decoders from models of different versions for\nfeature remapping. The remapped features will be used as\n* Corresponding Author.\nguidance for the upgraded model. To enhance the guidance\nability of X-Adapter, we employ a null-text training strategy\nfor the upgraded model. After training, we also introduce a\ntwo-stage denoising strategy to align the initial latents of X-\nAdapter and the upgraded model. Thanks to our strategies,\nX-Adapter demonstrates universal compatibility with vari-\nous plugins and also enables plugins of different versions\nto work together, thereby expanding the functionalities of\ndiffusion community. To verify the effectiveness of the pro-\nposed method, we conduct extensive experiments and the re-\nsults show that X-Adapter may facilitate wider application\nin the upgraded foundational diffusion model. Project page\nat: https://showlab.github.io/X-Adapter/.\n1\narXiv:2312.02238v2  [cs.CV]  6 Dec 2023\n1. Introduction\nLarge text-to-image diffusion models [29, 32, 35] have\ndrawn the attention of both researchers and creators nowa-\ndays. Since these models are often trained on thousands\nof GPU days with millions of data pairs, the major de-\nvelopment of the current research focuses on designing\nplug-and-play modules [12, 17, 24, 43], which are com-\nmonly called plugins, to add new abilities on the pre-trained\ntext-to-image models. People use plugins for photo cre-\nation [17, 34, 39], controllable drawing [24, 43], and edit-\ning [22, 30], both for image and video [9, 12, 42]. The\ndevelopment speed of downstream plugins is faster than the\nrelease of the base model since it is easier to train and en-\nables many more different features. But when a larger foun-\ndation model (e.g., SDXL [29]) is released, all the down-\nstream plugins need to be retrained for this upgraded model,\nwhich takes much more time for maintenance and upgrada-\ntion.\nWe aim to solve this inconvenient plugin incompatibil-\nity when upgradation by proposing a unified adapter net-\nwork, where all the downstream plugins in the original base\nmodel (e.g., Stable Diffusion v1.5 [32]) can be directly\nused in upgraded model (e.g., SDXL [29]) via the proposed\nmethod. However, this task has a lot of difficulties. First,\nwhen training different diffusion model versions, the com-\npatibility of plugins is often not considered. Thus, the orig-\ninal connector of the plugin might not exist in the newly\nupgraded model due to dimension mismatch. Second, dif-\nferent plugins are applied in the different positions of the\nStable Diffusion. For example, ControlNet [43] and T2I-\nAdapter [24] are added at the encoder and decoder of the\nfixed denoising UNet respectively. LoRA [17] are added\nafter each linear layer of a fixed denoising UNet. This un-\ncertainty makes it difficult to design a unified plugin. Fi-\nnally, although most current models are based on the latent\ndiffusion model [32], the latent space of each model is dif-\nferent. This gap is further boosted between the diffusion\nmodels in pixel and latent space.\nWe propose X-Adapter to handle above difficulites. In\ndetail, inspired by ControlNet [43], we consider X-Adapter\nas an additional controller of the upgraded model. To solve\nthe problem of the connector and the position of different\nplugins, we keep a frozen copy of the base model in the\nX-Adapter. Besides, we design several mapping layers be-\ntween the decoder of the upgraded model and X-Adapter for\nfeature remapping. In training, we only train the mapping\nlayers concerning the upgraded model without any plugins.\nSince the base model in the X-Adapter is fixed, the old plu-\ngins can be inserted into the frozen diffusion model copy\nin the X-Adapter. After training, we can sample two latent\nfor X-Adapter and an upgraded model for inference. To\nfurther boost the performance, we also propose a two-stage\ninference pipeline by sequentially inference Stable Diffu-\nsion v1.5 first and then the SDXL inspired by SDEdit [22].\nExperiments show that the proposed method can success-\nfully upgrade the plugins for larger models without specific\nretraining. We also conduct numerical experiments to show\nthe effectiveness of two widely used plugins, i.e., Control-\nNet [43], and LoRA [17].\nIn summary, the contribution of this paper can be sum-\nmarized as:\n\u2022 We target a new task in the large generative model era\nwhere we need to update plugins for different founda-\ntional models.\n\u2022 We propose a general framework to enable upgraded\nmodel compatible with pretrained plugins. We propose\na novel training strategy that utilizes two different latent\nwith mapping layers. Besides, we design two kinds of\ninference strategies to further boost the performance.\n\u2022 Experiments show the proposed methods can successfully\nmake old plugins work on upgraded text-to-image model\nwith better performance compared to the old foundational\nmodel.\n2. Related Works\nDiffusion Model for Text-to-Image Generation. Diffu-\nsion models are initially proposed by Sohl-Dickstein et al.\n[36], and have recently been adapted for image synthe-\nsis [10, 19]. Beyond unconditional image generation, the\ntext-to-image diffusion models [32] is an important branch\nof the image diffusion model, since it leverages larger-\nscale datasets for training. In these networks, Glide [25]\nproposes a transformer [37] based network structure. Im-\nagen [35] further proposes a pixel-level cascaded diffu-\nsion model to generate high-quality images.\nDifferent\nfrom pixel-level diffusion, the technique of Latent Diffusion\nModels (LDM) [32] conducts diffusion in a latent image\nspace [18], which largely reduces computational demands.\nStable Diffusion v1.5 [7] is a large-scale pre-trained latent\ndiffusion model. Stable Diffusion v2.1 [8] and SDXL [29]\nare the following versions of Stable Diffusion v1.5 by op-\ntimizing latent space, network structure, and training data.\nCompared to Midjourney [23] and DALL [26, 27], SDXL\nachieves state-of-the-art results.\nPlugins for Text-to-Image Diffusion Model.\nSince the\nstable diffusion model [32] is open-sourced, plug-and-play\nmodules, commonly referred to as \u201cplugins\u201d, significantly\nexpand the capabilities of pre-trained text-to-image (T2I)\nmodels. GLIGEN [20] adds an additional gate attention for\ngrounded generation. LoRA [17] is a general parameter-\nefficient training method that allows us to fine-tune the\nstable diffusion for stylization and customization easily.\nDreambooth [34] and Textual Inversion [11, 40] customize\npersonal concepts by finetuning the pre-trained diffusion\n2\nFrozen\nTrainable\nUpgraded\nmodel\nUpgraded\nmodel\nX-Adapter\nControlNet\n(a) Our task\n...\n(b) Retraining every plugin\nLoRA\nT2I-\nAdapter\nControlNet\n...\nLoRA\nT2I-\nAdapter\nFigure 2. Task Definition. Different from the previous method to\ntrain each plugin individually, our method only trains a single X-\nAdapter to all the fixed downstream plugins.\nmodel.\nIP-Adapter [39] extends these works for univer-\nsal image variation.\nBesides, ControlNet [43] and T2I-\nAdapter [24] add spatial conditioning controls to diffusion\nmodels by incorporating an extra network to encode condi-\ntions. AnimateDiff [12] allows a personalized T2I model to\ngenerate videos with high temporal consistency by adding a\ntemporal module. Although these plugins are powerful, it is\nunfeasible to apply an old plugin to an upgraded T2I model,\nwhich significantly hampers the development and applica-\ntion of diffusion models.\nParameter-Efficient Transfer Learning. Our topic is also\nrelated to parameter-efficient transfer learning since we\nneed to remedy the domain gap when upgrading. The emer-\ngence of large-scale pre-trained models, e.g., Stable Dif-\nfuions [32], CLIP [31], has highlighted the importance of\nthe effective transfer of these foundational models to down-\nstream tasks. Parameter-efficient Transfer Learning (PETL)\nmethods [15, 41, 44] add additional parameters to the orig-\ninal model to overcome the domain gaps between the pre-\ntrained dataset and target tasks. PMLR [15] introduces an\nadapter that consists of a down-sampling layer and an up-\nsampling layer and inserts it into Transformer [37] blocks.\nZhao et al. [44] bridge the domain gap by aligning the\ndataset\u2019s distribution.\nZhang et al. [41] propose a task-\nagnostic adapter among various upstream foundation mod-\nels. Similar to upgrading the CLIP for visual understand-\ning [41], our objective is to enable upgraded diffusion mod-\nels compatible with all kinds of plugins.\n3. Methods\n3.1. Task Definition\nWe aim to design a universal compatible adapter (X-\nAdapter) so that plugins of the base stable diffusion model\ncan be directly utilized in the upgraded diffusion model.\nAs shown in Fig. 2, given a powerful pre-trained text-to-\nimage diffusion model Mnew (i.e., SDXL [29]), we aim to\ndesign a universal adapter X-Adapter so that all the pre-\ntrained down-stream plugins (e.g., ControlNet [43], T2I-\nAdapter [24], LoRA [17]) on Mbase (i.e., Stable Diffusion\nv1.5 [32]) can work smoothly on Mnew without requiring\nadditional training. Thanks to this universal adaption, we\nhighlight some potential benefits:\n(i) Universal Compatibility of Plugins from Base Model. A\nnaive idea to apply a plugin network to the new model is\nto directly train the specific downstream plugin individu-\nally. However, take ControlNet [43] family as an example,\nit would require training more than ten different networks\nto achieve the original abilities. Differently, our method\nonly needs to train one version-to-version adapter in ad-\nvance and enable direct integration of pre-trained plugins\nfrom the base model, i.e., Stable Diffusion v1.5 [32].\n(ii) Performance Gain with respect to Base Model. Since\noriginal plugins are only trained on the base model, their\npower is also restricted due to the limited generative capa-\nbility. Differently, our adapter can improve the performance\nof these plugins by the upgraded models since these new\nmodels are typically more powerful in terms of visual qual-\nity and text-image alignments.\n(iii) Plugin Remix Across Versions.\nSince we retain the\nweights of both the base and upgraded models, our method\nalso enables the use of plugins from both models (e.g. Con-\ntrolNet of Stable Diffusion v1.5 and LoRA of SDXL can\nwork together smoothly as if ControlNet were originally\ntrained on SDXL). It largely expands the applicability of\nthe plugins from different development stages of the text-\nto-image community.\n3.2. Preliminary: Latent Diffusion Model\nBefore introducing our method, we first introduce the La-\ntent Diffusion Model (LDM [32]), since most of the open-\nsource models are based on it.\nLDM extends denoising\ndiffusion model [14] for high-resolution image generation\nfrom text prompt, which first uses a VAE [18]\u2019s encoder E\nto compress the RGB image x into latent space z. After that,\na UNet [33] \u03f5\u03b8 is used to remove added noise from a noisy\nlatent. Formally, \u03f5\u03b8 is trained using the following objective:\nmin\n\u03b8\nEz0,\u03f5\u223cN(0,I),t\u223c Uniform (1,T ) \u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, c)\u22252\n2 , (1)\nwhere zt is the noisy latent of z from timestep t and c is the\nembedding of conditional text prompt.\n3.3. X-Adapter\nX-Adapter is built upon the base Stable Diffusion v1.5 [32]\nto maintain the full support for the plugin\u2019s connec-\ntors.\nAdditionally, in the decoder of each layer, we\ntrain an additional mapping network to map the fea-\ntures from the base model to the upgraded model (e.g.,\nSDXL [29]) for guidance as shown in Fig. 3.\nIn\neach mapper, a stack of three ResNet [13] is utilized\nfor dimension matching and feature extraction.\nFor-\nmally, suppose we have N adapters and Fn(\u00b7) denotes\n3\nM\nM\nM\n\u0190base\nLoss\nTraining\nInference\n+\n\u0190upgrade\n+\nUpgraded Model\nText prompt: \u201c\u201d\nX-Adapter\nText prompt: \u201cIceland\u201d\nT\n0\nUpgrade\nT\nT0\nPlugin\n0\nBase\nUpgrade\nPlugin\n(a)\n(b)\nX-Adapter\nX-Adapter\nFrozen\nBase Model\nFrozen\nUpgraded Model\nM\nTrainable\nMapping\nFigure 3. Method Overview. In training, we add different noises to both the upgraded model and X-Adapter under the latent domain of\nbase and upgraded model. By setting the prompt of the upgraded model to empty and training the mapping layers, X-Adapter learns to\nguide the upgraded model. In testing, (a) we can directly apply the plugins on the X-Adapter for the upgraded model. (b) A two-stage\ninfluence scheme is introduced to improve image quality.\nthe nth trained mapper, given multi-scale feature maps\nFbase = {F 1\nbase, F 2\nbase, ..., F N\nance feature Fmapper = {F 1base} from base model, guid-\nmapper, F 2\nmapper, ..., F N\nmapper}\nis formed by feeding Fbase to the mapping layers. Note that\nthe dimension of Fmapper is the same as that of certain in-\ntermediate features of upgraded decoder layers. Fmapper\nis then added with those layers. In summary, the guidance\nfeature extraction and fusion can be defined as the following\nformulation:\nF n\nmapper\n=\nFn(F n\nbase)\n(2)\nF n\nup\n=\nF n\nup + F n\nmapper, n \u2208 {1, 2, ..., N}, (3)\nwhere F n\nup denotes upgraded model\u2019s nth decoder layer to\nfuse guidance feature.\nTraining Strategy. As shown in Fig. 3, given an upgraded\ndiffusion model, X-Adapter is firstly trained in a plugin-\nfree manner on the upgraded diffusion model for text-to-\nimage generation. Formally, given an input image I, we\nfirst embed it to the latent spaces z0 and z0 via base and up-\ngraded autoencoder respectively. Then, we randomly sam-\nple a time step t from [0, T], adding noise to the latent\nspace, and produce two noisy latent zt and zt for denoising.\nGiven timestep t, the prompt cb of X-Adapter and upgraded\nmodel\u2019s prompt cu, X-Adapter is trained with the upgraded\ndiffusion network \u03f5\u03b8 to predict the added noise \u03f5 by:\nEz0,\u03f5,t,cb,cu \u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t, cu, XAdapter(zt, t, cb))\u22252\n2 .\n(4)\nIn the training process, the objective of the above\nloss function is to determine the offsets between the X-\nAdapter and the upgraded space. Inspired by previous task-\ncompatibility plugins for additional control signal [24, 43]\nand video generation [12], we find that the key to task-\nagnostic adaptation is to fix the parameters of the trained\ndiffusion UNet.\nThus, we freeze the parameters in the\nbase model during training, which ensures that old plugins\ncan be seamlessly inserted. To avoid affecting the original\nhigh-quality feature space of the upgraded model, we also\nfreeze its parameters similar to conditional control meth-\nods [24, 43]. All text prompts cu are set to an empty string\ninspired by [21]. Thus, the upgraded model provides the av-\nerage feature space with an empty prompt, while X-Adapter\nlearns the offset given base feature space, guiding the native\nupgraded model. Although cu is set to empty during train-\ning, our experiments show that we do not need to adhere\nthis rule during inference and X-Adapter works well with\nany cu after training. After training, the plugins can natu-\nrally be added to X-Adapter for their abilities.\nInference Strategy. During training, two bypasses\u2019 latents\nare encoded from the same image, naturally aligning with\neach other. However, since the latent space of the two mod-\nels is different, during the inference stage, if the initial la-\ntents for two bypasses are randomly sampled (Fig. 3 (a)),\nthis leads to a lack of alignment, potentially causing con-\nflicts that affect the plugin\u2019s function and image quality.\nTo tackle this issue, inspired by SDEdit [22], we pro-\npose a two-stage inference strategy as shown in Fig. 3 (b).\nGiven total timestep T, at the first stage, we randomly sam-\nple an initial latent zT for X-Adapter and run with plugins\nin timestep T0 where T0 = \u03b1T, \u03b1 \u2208 [0, 1]. At timestep T0,\nthe base model\u2019s latent zT0 will be converted to upgraded\nmodel\u2019s latent zT0 by:\nzT0 = Eup(Dbase(zT0)),\n(5)\nwhere Dbase denotes the base model\u2019s decoder and Eup de-\nnotes the upgraded model\u2019s encoder. zT0 and zT0 will be\ninitial latents for two bypasses at the second stage where the\n4\nplugin will guide the upgraded model\u2019s generation through\nX-Adapter. We observe that for most plugins our frame-\nwork performs optimally when T0 =\n4\n5T, i.e., the base\nmodel run 20% of the time step for warmup and then runs\nour X-Adapter in the rest of the inference time directly. We\ngive detailed ablations on this two-stage inference in the ex-\nperiments.\n4. Experiments\n4.1. Implementation Details\nWe implement X-Adapter using Stable Diffusion v1.5 [32]\nas the base model, and SDXL [29] base as the main up-\ngraded model.\nMapping layers of X-Adapter are placed\nat the base model\u2019s last three decoder blocks. Notice that\nwe also train our X-Adapter for Stable Diffusion v2.1 [8],\nwhich shows promising results shown as Fig. 6. For train-\ning, we select a subset of Laion-high-resolution containing\n300k images for X-Adapter training. In our experiments,\nthe input image is resized into 1024\u00d71024 for the upgraded\nmodel and 512 \u00d7 512 for the base model. We utilize the\nAdamW optimizer with a learning rate of 1e\u22125 and a batch\nsize of 8. The model is trained for 2 epochs using 4 NVIDIA\nA100 GPUs.\n4.2. Comparisons\nExperiment setting. We choose two representative plug-\nins (ControlNet [43] and LoRA [17]), to evaluate the per-\nformance of the proposed method, since they represent\ntwo valuable applications of semantic and style control.\nWe evaluate the performance gain our method achieves as\nwell as plugin functionality retention. For ControlNet, we\nchoose canny and depth to test our method under dense\nand sparse conditions. We utilize the COCO validation set,\nwhich contains 5,000 images, to evaluate each method. For\nLoRA [17], We use AnimeOutline [4] and MoXin [5] to test\nthe style control plugin. We select 20 prompts from civi-\ntai [16] for each LoRA, generating 50 images per prompt\nusing random seeds. To eliminate SDXL [29]\u2019s effect on\nstyle, SDXL\u2019s prompt only focus on image\u2019s content, and\nX-Adapter\u2019s prompt will include LoRA\u2019s trigger words and\nstyle-related words.\nAs for evaluation metrics, we use\nFrechet Inception Distance (FID) to measure the distribu-\nPlugin: ControlNet\nFID \u2193 CLIP-score \u2191 Cond. Recon. \u2191\nSD 1.5 [32]\n33.09\n0.2426\n0.33 \u00b1 0.16\nSDEdit [22] + SDXL 30.86\n0.2594\n0.14 \u00b1 0.10\nX-Adapter + SDXL 30.95\n0.2632\n0.27 \u00b1 0.13\nPlugin: LoRA\nFID \u2193 CLIP-score \u2191\nStyle-Sim \u2191\nSD 1.5 [32]\n32.46\n0.25\n-\nSDEdit [22] + SDXL 30.11\n0.2584\n0.72\nX-Adapter + SDXL 29.88\n0.2640\n0.83\nTable 1. Quantitative evaluation against baselines.\nInput (canny)\nPrompt: a \nfuturistic research \ncomplex in a bright \nfoggy jungle\nLoRA:\nAnimeOutline\nPrompt: a beautiful \nlady with glasses\nSDEdit\n+\nSDXL\nt0\n0.5\n0.6\n0.7\nOurs\nSDEdit\n+\nSDXL\nOurs\nt0\n0.5\n0.7\n0.9\nFigure 4. Visual Comparison to baseline under different t0. We\nchoose ControlNet [43] and LoRA [17] to evaluate different meth-\nods under semantic and style control.\nSpecifically, we choose\nAnimeOutline[4], a LoRA specialized in black and white sketch\ngeneration. We sample three t0 for each plugin. We observe that\nbaseline loses style control (turn black and white to color) and\nsemantic control as t0 increases while our method maintain the\ncontrollability with the usage of X-Adapter.\nFigure 5. Quantitative evaluation under different t0. Baseline\nloses style control and semantic control as t0 increases while our\nmethod preserves functionality of plugins\ntion distance over images generated by our method and\noriginal SDXL, which indicates image quality, as well as\ntext-image clip scores. We also calculate the condition re-\nconstruction score following ControlNet [43] and style sim-\nilarity following StyleAdapter [38] to evaluate the plugin\u2019s\nfunctionality. The style similarity is measured between the\ngeneration of our method and the base model.\nComparison to base model. We select Stable Diffusion\nv1.5 [7] as our base model. The quantitative result is shown\nin Tab. 1. It shows that our method achieves a balance be-\ntween image quality and preservation of plugin\u2019s function.\nComparison to baseline. A naive approach is to consider\nSDXL as an editor for the output of the base Stable Dif-\nfusion v1.5 model, similar to SDEdit [22].\nWe select a\ntimestep t0, adding noise to the base model\u2019s generation to\nt0 and denoising it using the upgraded model. We evalu-\n5\nSD 1.5 & Condition\nSD2.1 +\n X-Adapter\nSDXL + X-Adapter\nSD 1.5 & Condition\nSD2.1 +\n X-Adapter\nSDXL + X-Adapter\nSD 1.5 & Condition\nSD2.1 + \nX-Adapter\nSDXL + X-Adapter\nSD 1.5 \nSD 2.1 +\n X-Adapter\nSDXL + X-Adapter\nSD 1.5 \nSD 2.1 +\n X-Adapter\nSDXL + X-Adapter\nSD 1.5 \nSD 2.1 +\n X-Adapter\nSDXL + X-Adapter\nOriginal Input\nSD 1.5\nSDXL + X-Adapter\nControlNet Depth\nControlNet Canny\nT2I-Adapter Depth\nLoRA (KawaiiTech)\nPersonalized T2I (ToonYou)\nPersonalized T2I (AnimeCreative)\nInstruct-Pix2Pix (ControlNet Version)\nOriginal Input\nSD 1.5\nSDXL + X-Adapter\nCondition\nSDXL(Lora Ref)\nSDXL + X-Adapter\nSD1.5 ControlNet + XL Lora (VanGogh)\nX-Adapter prompt: high quality, extremely detailed\nSDXL prompt: high quality, extremely detailed\nX-Adapter prompt: make it fire\nSDXL prompt: best quality, extremely detailed\nX-Adapter prompt: shot of a furry blue bird.\nSDXL prompt: (trigger), soft blurry oil painting \nportrait of a close up shot of a bird\nControlNet Tile Model\nX-Adapter: scifi, coffee machine, mug, (trigger)\nUpgraded Models prompt : scifi, coffee machine, \nmug, masterpiece, best quality\nX-Adapter & Upgraded Models prompt: \nbest quality, masterpiece, 1girl, looking at viewer, \nblurry background, upper body, contemporary, dress\nX-Adapter & Upgraded Models prompt: \nmasterpiece, best quality, 1 girl, green eyes, kimono, \n(lycoris flower) in hair\nX-Adapter & Upgraded Models prompt: \nA colorful lotus, ink, high quality, extremely detailed.\nX-Adapter & Upgraded Models prompt: \nA man with an orange on his face, high quality, \nextremely detailed.\nX-Adapter & Upgraded Models prompt: \nAerial view of a circular portal in the horizon, portal \nto a world in fire, 8k, anime art style, sunset.\nFigure 6. Qualitative Results on Different Plugins. The showcases of different results on SDXL and SD 2.1 based on the proposed\nX-Adapter and pre-rained SD 1.5 plugins. We show the corresponding prompts in the yellow box.\nate it under the same experiment setting as shown in Tab.1.\nNote that the function of t0 in SDEdit is similar to T0 in\nour two-stage inference strategy. For both methods, the up-\ngraded model is more influenced by the base model when\nt0 is lower, obtaining more semantic features and style in-\nformation from the base model, which leads to less optimal\noutcomes in terms of image quality. Conversely, a higher\nt0 value decreases the base model\u2019s influence, leading to\nimproved generation quality as shown in Fig. 4. This im-\nplies that the SDE-based method loses essential semantic\ndetails and style information (i.e., plugin\u2019s control) when\nt0 is large, indicative of higher image quality. Conversely,\nX-adapter can maintain these controls and preserve the ca-\npabilities of the plugins even with a high t0, ensuring high-\n6\nPlugin: ControlNet\nResult Quality \u2191 Condition Fidelity \u2191\nSD 1.5 [32]\n3.23 \u00b1 0.12\n4.21 \u00b1 0.32\nSDEdit [22] + SDXL\n4.14 \u00b1 0.57\n2.46 \u00b1 0.17\nX-Adapter + SDXL\n4.46 \u00b1 0.43\n3.92 \u00b1 0.26\nPlugin: LoRA\nResult Quality \u2191\nStyle Fidelity \u2191\nSD 1.5 [32]\n2.93 \u00b1 0.09\n-\nSDEdit [22] + SDXL\n3.92 \u00b1 0.53\n3.45 \u00b1 0.33\nX-Adapter + SDXL\n4.38 \u00b1 0.25\n4.14 \u00b1 0.29\nTable 2. User Study. We report the user preference ranking (1 to\n5 indicates worst to best) of different methods.\nquality generation with faithful plugin fidelity. To highlight\nthe advantage of our method, we sampled six t0 values at\nequal intervals between [0, 1] and conducted experiments on\nour method and baseline under these t0. Fig. 4 and Fig. 5\nillustrate the performance of different methods. We observe\nthat although our method shows similar visual quality com-\npared to the baseline, it better preserves the functionality of\nplugins.\nUser study. Users evaluate the generation results of our\nmethod with ControlNet [43] and Lora [17]. For Control-\nNet, we collect 10 canny conditions and depth conditions,\nthen assign each condition to 3 methods: Stable Diffusion\nv1.5, SDEdit + SDXL, and X-Adapter. We invite 5 users\nto rank these generations in terms of \u201cimage quality\u201d and\n\u201dfidelity of conditions\u201d. For LoRA, we collect 10 prompts\nand also assign them to these three methods. Users rank\nthese generations in terms of \u201cimage quality\u201d and \u201dstyle\nsimilarity\u201d. We use the Average Human Ranking (AHR) as\na preference metric where users rank each result on a scale\nof 1 to 5 (lower is worse). The average rankings are shown\nin Tab 2.\n4.3. Qualitative Results on Multiple Plugins\nAs shown in Fig. 6, we show the qualitative results of\nthe proposed X-Adapter on both SD 2.1 and SDXL in\nvarious pretrained plugins on Stable Diffusion v1.5 to\nshow the advantages.\nWe present representative exam-\nples of conditional generation (ControlNet Depth, Con-\ntrolNet Canny, T2I-Adapter Depth), the personalization\nstyle (LoRA Model [2], Personalized Model [1, 6]) and the\nImage Editing Methods (ControlNet-based InstructPix2Pix\nand ControlNet Tile). Finally, we show the plugin remix in\nour methods, where the plugins [3] in SDXL can also di-\nrectly cooperate with the Stable Diffusion v1.5 plugin (e.g.,\nControlNet in our case).\n4.4. Ablative Study\nWhere to insert mapping layer? We study the effect of in-\nserting mapping layers into different modules: (1) Encoder;\n(2) Decoder; (3) Both encoder and decoder. Fig. 7 indicates\nthat the decoder-only strategy shows the strongest guidance\nEncoder only\nBoth\nConditions\nDecoder only\nFigure 7.\nAblation of module to insert mapping layers.\nThe\nkey to better guidance ability is to retain encoder\u2019s feature space.\nPrompts: \u201ca fantastic landscape / an apple with a lizard in it\u201d.\nGuidance\nConditions\nSPADE\nAddition\nUpgraded\nX-\nAdapter\n+\nUpgraded\nX-\nAdapter\n+\n-\nUpgraded\nX-\nAdapter\nNorm\n\u03b3\n\u03b2\nx\n+\nx\u03bb\nFigure 8. Ablation of different fusion types. The result shows that\nfusing features through addition can maximize the restoration of\nthe condition. The text prompts are: \u201cA chocolate apple\u201d and \u201cA\nresearch room\u201d.\ncapability since it does not harm the encoder\u2019s feature space\nand only performs guidance during generation. See also the\nsupplementary material for quantitative results for different\nmodule selection.\nHow do mapping layers guide the upgraded model?\nWe explored three methods for integrating guidance fea-\ntures into the upgraded model. Given guidance feature a\nand upgraded model\u2019s feature b, new feature c is formed\nby (1) addition fusion: c = a + b (2) guidance fusion:\nc = b + \u03bb(a \u2212 b) where \u03bb can be adjusted by users (3)\nSPADE: c = \u03b3(a)(norm(b))+\u03b2(a) where \u03b3 and \u03b2 are two\nnetworks following SPADE [28]\u2019s design. Fig. 8 presents\na visual comparison of different ablation fusion types. We\nfind that addition is the most effective way to provide guid-\nance for the upgraded model.\nIs using empty text important in the upgraded model?\nTo demonstrate the effectiveness of the null-text training\nstrategy, we train three models under 100%, 50%, and 0%\nnull probability. Fig. 9 indicates that reducing the capabil-\n7\nNull = 0%\nNull = 50%\nNull = 100%\nConditions\nFigure 9. Ablation of different null probability during training.\nIncreasing the percentages of null text prompts in the upgraded\nmodel can enhance XAdapter\u2019s guidance ability. Text prompts are:\n\u201cA painting of a beautiful woman\u201d and \u201cA world of crystal\u201d from\ntop to bottom.\nDepth Condition\nw/ two-stage \ninference strategy\nw/o two-stage \ninference strategy\nFigure 10. Ablation of inference strategy. The result shows that\nX-Adapter can roughly reconstruct the condition even w/o the two-\nstage inference, and the two-stage inference has a better similarity.\nText prompt: \u201cstormtrooper lecture, photorealistic\u201d\nity of the upgraded model during training can maximize the\nguidance effect of X-Adapter.\nIs two-stage inference important? We study the effect of\na two-stage denoising strategy by randomly sampling ini-\ntial latents for X-Adatper and upgraded model. Our method\nstill works effectively without initial latents alignment as\nshown in Fig. 10. Adopting two-stage sampling strategy in\ninference further boosts performance in terms of conditional\naccuracy.\n4.5. Discussion\nPrompt Setting. We always set clear and sufficient prompts\nfor X-Adapter, therefore we study three different prompt\nsettings of SDXL: (1) Sufficient prompts which are seman-\ntically consistent with X-Adapter\u2019s prompts (2) Insufficient\nprompts. The default insufficient prompt in this paper is\n\u201dbest quality, extremely detailed\u201d. (3) Conflicting prompts\nwhich change the meaning of X-Adapter\u2019s prompts. Fig. 11\nshows that our method can still maintain overall layout and\nstyle consistency even in case of prompt conflict.\nPlugin Remix. Our method naturally supports plugins from\nboth X-Adapter (e.g. SD1.5 [7]) and upgraded model (e.g.\nSDXL [29]) since we retain all connectors by freezing pa-\nSufficient\nInsufficient\nConflicting\nCond./Ref.\n\u201cA cute cat\u201d\n\u201cBest quality, \nextremely detailed\u201d\n\u201cA cute dog\u201d\n\u201cA beautiful lady\u201d\n\u201cBest quality, \nextremely detailed\u201d\n\u201cA beautiful lady, \ncolorful\u201d\nSDXL prompt:\nSDXL prompt:\n\u201cA cute cat\u201d\n\u201cA cute dog\u201d\n\u201cA beautiful lady\u201d\nFigure 11. Prompt setting. Our method can still ensure the over-\nall layout and style consistency even in case of prompt conflict.\nLoRA[17] used here isAnimeOutline[4], an expert in black and\nwhite sketch generation. X-Adapter\u2019s text prompts are: \u201cA cute\ncat\u201d and \u201cA beautiful lady, (trigger words)\u201d from top to bottom.\nrameters of these two models. The bottom right picture of\nFig. 6 shows a combination of Stable Diffusion v1.5\u2019s Con-\ntrolNet and SDXL\u2019s LoRA, generating results that follow\nLoRA\u2019s style and condition\u2019s semantics. It indicates that our\nmethod can bridge community resources across different\ndiffusion model versions (e.g. SD1.5, SD2.1 [8], SDXL).\nLimitation. Although our method achieves impressive re-\nsults, it still has some limitations.\nFor some plugins to\ngenerate personalized concepts, e.g., IP-Adapter [39], our\nmethod might not maintain the identity well. We give exam-\nples in the supplementary material for visualization. This is\nbecause the custom plugins work on the text-encoder other\nthan the feature space concepts that are not directly injected\ninto the upgraded model other than fused as guidance. Since\nour method has already made some universal plugin up-\ngrades, we leave the capability of the concept customization\nas future work.\n5. Conclusion\nIn this paper, we target a new task of upgrading all the\ndownstream plugins trained on old diffusion model to the\nupgraded ones. To this end, we propose X-Adapter, which\ncomprises a copied network structure and weights of the\nbase model, and a series of mapping layers between two\ndecoders for feature mapping. During training, we freeze\nthe upgraded model and set its text prompt to empty text to\nmaximize the function of X-Adapter. In testing, we propose\na two-stage inference strategy to further enhance perfor-\nmance. We conduct comprehensive experiments to demon-\nstrate the advantages of the proposed methods in terms of\ncompatibility and visual quality.\n8\nReferences\n[1] Animecreative.\nhttps://civitai.com/models/\n146785. 7\n[2] Kawaiitech.\nhttps : / / civitai . com / models /\n94663. 7\n[3] Vangoghportraiture.\nhttps : / / civitai . com /\nmodels/157794. 7\n[4] Animeoutline.\nhttps://civitai.com/models/\n16014. 5, 8\n[5] Moxin. https://civitai.com/models/12597. 5\n[6] Toonyou. https://civitai.com/models/30240.\n7\n[7] Stability\nAI.\nhttps : / / huggingface . co /\nrunwayml/stable- diffusion- v1- 5, .\n2, 5,\n8\n[8] Stability\nAI.\nhttps : / / huggingface . co /\nstabilityai/stable-diffusion-2-1-base, .\n2, 5, 8\n[9] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration. arXiv preprint arxiv:2310.19512, 2023. 2\n[10] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. arXiv preprint arxiv:2105.05233,\n2021. 2\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narxiv:2208.01618, 2022. 2\n[12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 4\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. pages 770\u2013\n778, 2016. 3\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 3\n[15] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna\nMorrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona\nAttariyan, and Sylvain Gelly.\nParameter-efficient transfer\nlearning for NLP. pages 2790\u20132799, 2019. 3\n[16] https://civitai.com/. civitai. https://civitai.com/,\n2013. 5\n[17] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 3, 5, 7, 8, 1\n[18] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arxiv:1312.6114, 2013. 2, 3\n[19] Diederik P. Kingma,\nTim Salimans,\nBen Poole,\nand\nJonathan Ho. Variational diffusion models. arXiv preprint\narxiv:2107.00630, 2021. 2\n[20] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation. arXiv\npreprint arxiv:2301.07093, 2023. 2\n[21] Shanyuan Liu, Dawei Leng, and Yuhui Yin.\nBridge dif-\nfusion model: bridge non-english language-native text-to-\nimage diffusion model with english communities.\narXiv\npreprint arXiv:2309.00952, 2023. 4\n[22] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arxiv:2108.01073, 2022. 2, 4, 5, 7\n[23] Midjourney. https://www.midjourney.com/. 2\n[24] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n3, 4, 1\n[25] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narxiv:2112.10741, 2022. 2\n[26] OpenAI. Dall-e2. https://openai.com/dall-e-2,\n. 2\n[27] OpenAI. Dall-e3. https://openai.com/dall-e-3,\n. 2\n[28] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. arXiv preprint arxiv:1903.07291, 2019. 7\n[29] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 3, 5, 8\n[30] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arxiv:2303.09535, 2023. 2\n[31] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[32] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. 2021. 2, 3, 5, 7\n[33] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\narXiv preprint arxiv:1505.04597, 2015. 3\n[34] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv preprint arxiv:2208.12242, 2023. 2\n[35] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\n9\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arxiv:2205.11487, 2022. 2\n[36] Jascha\nSohl-Dickstein,\nEric\nA.\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli.\nDeep unsupervised\nlearning using nonequilibrium thermodynamics.\narXiv\npreprint arxiv:1503.03585, 2015. 2\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narxiv:1706.03762, 2017. 2, 3\n[38] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi,\nYing Shan, Wenping Wang, and Ping Luo.\nStyleadapter:\nA single-pass lora-free model for stylized image generation.\narXiv preprint arxiv:2309.01770, 2023. 5\n[39] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arxiv:2308.06721,\n2023. 2, 3, 8, 1\n[40] Ge Yuan,\nXiaodong Cun,\nYong Zhang,\nMaomao Li,\nChenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb ba-\nsis. arXiv preprint arXiv:2306.00926, 2023. 2\n[41] Binjie Zhang, Yixiao Ge, Xuyuan Xu, Ying Shan, and\nMike Zheng Shou. Taca: Upgrading your visual foundation\nmodel with task-agnostic compatible adapter. arXiv preprint\narxiv:2306.12642, 2023. 3\n[42] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narxiv:2309.15818, 2023. 2\n[43] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models. 2023.\n2, 3, 4, 5, 7, 1\n[44] Hengyuan Zhao, Hao Luo, Yuyang Zhao, Pichao Wang, Fan\nWang, and Mike Zheng Shou.\nRevisit parameter-efficient\ntransfer learning: A two-stage paradigm.\narXiv preprint\narxiv:2303.07910, 2023. 3\n10\nX-Adapter: Adding Universal Compatibility of Plugins for Upgraded Diffusion\nModel\nSupplementary Material\nA. Ablation on module selection\nWe provide quantitative results of ablation on module to\ninsert mapping layers. Tab. 1 indicates that the decoder-\nonly strategy shows the strongest guidance capability since\nit does not harm the encoder\u2019s feature space and only per-\nforms guidance during generation.\nB. Limitation\nFor plugins to generate personalized concepts, e.g., IP-\nAdapter [39], our method might not maintain the identity\nwell like shown in Fig. 1. This is because the custom plug-\nins work on the text-encoder other than the feature space\nconcepts that are not directly injected into the upgraded\nmodel other than fused as guidance.\nC. Qualitative result\nWe provide more qualitative result on Controlnet [43], T2I-\nAdapter [24] and LoRA [17] as shown in Fig. 2\nPlugin: ControlNet FID \u2193 CLIP-score \u2191 Cond. Recon. \u2191\nEncoder & Decoder 38.37\n0.26\n0.24 \u00b1 0.15\nEncoder only\n37.32\n0.26\n0.23 \u00b1 0.14\nDecoder only\n30.95\n0.26\n0.27 \u00b1 0.13\nPlugin: LoRA\nFID \u2193 CLIP-score \u2191\nStyle-Sim \u2191\nEncoder & Decoder 36.71\n0.26\n0.79\nEncoder only\n35.54\n0.26\n0.80\nDecoder only\n29.88\n0.26\n0.83\nTable 1. Ablation on module to insert mapping layers. For en-\ncoder only and decoder only, we use the same mapping layers to\nmap the frozen base model to the upgraded model\u2019s encoder and\ndecoder separately. For both the encoder and decoder, we use two\nidentical mapping layers and insert them into the encoder and de-\ncoder.\nImage prompt\nOurs\nIP-Adapter\nFigure 1. Limitations. In IP-Adapter [39], although our method\ncan produce relatively identity-consistent results, the details, e.g.,\nclothes, are still different from the original model.\n1\nX-Adapter & Upgraded Model prompt: \nA rose\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nA cute book of spells in mystical cover\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nTwo dears in the wild\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nCinematic photo hamster in a bauble wearing a \nchristmas hat\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nA cat holding a gun\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nA cat riding a dolphin\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nA world of crystal, high quality, extremely detailed\nSD2.1\nSDXL\nX-Adapter & Upgraded Model prompt: \nAn apple made of chocolate, high quality, extremely \ndetailed\nSD2.1\nSDXL\nX-Adapter Prompt: masterpiece, best quality, ultra \ndetailed, 1girl, solo, smile, looking at viewer, holding \nflowers, (trigger words)\nUpgraded Models prompt: masterpiece, best quality, \nultra detailed, 1girl, solo, smile, looking at viewer, \nholding flowers, \nshukezouma\nX-Adapter Prompt: best quality, a city, (trigger \nwords)\nUpgraded Models prompt: best quality, a city\nAnimeOutline\nSD2.1\nSDXL\nSD2.1\nSDXL\nX-Adapter Prompt: realistic glasssculpture of a \nrabbit, translucent, transparent, detailed cityscape \nbackground, street, reflections\nUpgraded Models prompt: A rabbit, detailed \ncityscape background, street, reflections\nGlass sculpture\nSD2.1\nSDXL\nX-Adapter Prompt: Perfect lighting, depth of field, \n(trigger word), house in the forest\nUpgraded Models prompt: Perfect lighting, depth of \nfield, house in the forest\nLego AI\nSD2.1\nSDXL\nSource image\nOur result\nX-Adapter Prompt: best quality, extremely detailed\nUpgraded Models prompt: best quality, extremely detailed\nControlNet\nT2I-Adapter\nLoRA\nControlNet-Tile\nFigure 2. Qualitative Results on Different Plugins.\n2\n"
  },
  {
    "title": "LivePhoto: Real Image Animation with Text-guided Motion Control",
    "link": "https://arxiv.org/pdf/2312.02928.pdf",
    "upvote": "15",
    "text": "LivePhoto: Real Image Animation with Text-guided Motion Control\nXi Chen1\nZhiheng Liu2\nMengting Chen2\nYutong Feng2\nYu Liu2\nYujun Shen3\nHengshuang Zhao1\n1The University of Hong Kong\n2Alibaba Group\n3Ant Group\n\u201cA Shiba Inu is running fast.\u201d \nReference Image\n\u201cA Shiba Inu is wagging its tail.\u201d\nReference Image\n\u201cLightning and thunder in the night sky.\u201d\nReference Image\nReference Image\n\u201cPouring water into the glass.\u201d\n\u201cThis man gives a thumbs-up.\u201d Intensity: 3 \n\u201cThis man gives a thumbs-up.\u201d Intensity: 7 \nReference Image\nReference Image\nReference Image \u201cScenery of the Louvre, camera zooms in.\u201d\n\u201cKung Fu Panda is practicing Tai Chi.\u201d\nReference Image\nFigure 1. Zero-shot real image animation with text control. Besides adequately decoding motion descriptions like actions and camera\nmovements (row 1), LivePhoto could also conjure new contents from thin air (row 2). Meanwhile, LivePhoto is highly controllable,\nsupporting users to customize the animation by inputting various texts (row 3) and adjusting the degree of motion intensity (row 4).\nAbstract\nDespite the recent progress in text-to-video generation,\nexisting studies usually overlook the issue that only spatial\ncontents but not temporal motions in synthesized videos are\nunder the control of text. Towards such a challenge, this\nwork presents a practical system, named LivePhoto, which\nallows users to animate an image of their interest with\ntext descriptions. We first establish a strong baseline that\nhelps a well-learned text-to-image generator (i.e., Stable\nDiffusion) take an image as a further input. We then equip\nthe improved generator with a motion module for tempo-\nral modeling and propose a carefully designed training\npipeline to better link texts and motions.\nIn particular,\nconsidering the facts that (1) text can only describe motions\nroughly (e.g., regardless of the moving speed) and (2)\ntext may include both content and motion descriptions,\nwe introduce a motion intensity estimation module as well\nas a text re-weighting module to reduce the ambiguity of\ntext-to-motion mapping. Empirical evidence suggests that\nour approach is capable of well decoding motion-related\ntextual instructions into videos, such as actions, camera\nmovements, or even conjuring new contents from thin air\n(e.g., pouring water into an empty glass).\nInterestingly,\nthanks to the proposed intensity learning mechanism, our\nsystem offers users an additional control signal (i.e., the\nmotion intensity) besides text for video customization. The\npage of this project is here.\n1\narXiv:2312.02928v1  [cs.CV]  5 Dec 2023\n1. Introduction\nImage and video content synthesis has become a burgeon-\ning topic with significant attention and broad real-world\napplications. Fueled by the diffusion model and extensive\ntraining data, image generation has witnessed notable ad-\nvancements through powerful text-to-image models [4, 35,\n37, 46] and controllable downstream applications [6, 18, 23,\n24, 28, 36, 49]. In the realm of video generation, a more\ncomplex task requiring spatial and temporal modeling,\ntext-to-video has steadily improved [2, 10, 19, 40, 47].\nVarious works [3, 8, 22, 43, 45] also explore enhancing\ncontrollability with sequential inputs like optical flows,\nmotion vectors, depth maps, etc.\nThis work explores utilizing a real image as the initial\nframe to guide the \u201ccontent\u201d and employ the text to control\nthe \u201cmotion\u201d of the video.\nThis topic holds promising\npotential for a wide range of applications, including meme\ngeneration, production advertisement, film making, etc.\nPrevious image-to-video methods [5, 9, 15, 17, 25, 41, 48]\nmainly focus on specific subjects like humans or could only\nanimate synthetic images. GEN-2 [34] and Pikalabs [33]\nanimate real images with an optional text input, however,\nan overlooked issue is that the text could only enhance the\ncontent but usually fails to control the motions.\nFacing this challenge, we propose LivePhoto, an image\nanimation framework that truly listens to the text instruc-\ntions. We first establish a powerful image-to-video baseline.\nThe initial step is to equip a text-to-image model (i,e.,\nStable Diffusion) with the ability to refer to a real image.\nSpecifically, we concatenate the image latent with input\nnoise to provide pixel-level guidance. In addition, a content\nencoder is employed to extract image patch tokens, which\nare injected via cross-attention to guide the global identity.\nDuring inference, a noise inversion of the reference image\nis introduced to offer content priors. Afterward, following\nthe contemporary methods [2, 10, 45], we freeze stable\ndiffusion models and insert trainable motion layers to model\nthe inter-frame temporal relations.\nAlthough the text branch is maintained in this strong\nimage-to-video baseline, the model seldom listens to the\ntext instructions.\nThe generated videos usually remain\nnearly static, or sometimes exhibit overly intense move-\nments, deviating from the text. We identify two key issues\nfor the problem: firstly, the text is not sufficient to describe\nthe desired motion.\nPhrases like \u201cshaking the head\u201d or\n\u201ccamera zooms in\u201d lack important information like moving\nspeed or action magnitude.\nThus, a starting frame and\na text may correspond to diverse motions with varying\nintensities. This ambiguity leads to difficulties in linking\ntext and motion. Facing this challenge, we parameterize the\nmotion intensity using a single coefficient, offering a sup-\nplementary condition. This approach eases the optimization\nand allows users to adjust motion intensity during inference\nconveniently. Another issue arises from the fact that the\ntext contains both content and motion descriptions. The\ncontent descriptions translated by stable diffusion may not\nperfectly align with the reference image, while the image\nis prioritized for content control.\nConsequently, when\nthe content descriptions are learned to be suppressed to\nmitigate conflicts, motion descriptions are simultaneously\nunder-weighted.\nTo address this concern, we propose\ntext re-weighting, which learns to accentuate the motion\ndescriptions, enabling the text to work compatibly with the\nimage for better motion control.\nAs shown in Fig. 1, equipped with motion intensity guid-\nance and text re-weighting, LivePhoto demonstrates im-\npressive abilities for text-guided motion control. LivePhoto\nis able to deal with real images from versatile domains and\nsubjects, and adequately decodes the motion descriptions\nlike actions and camera movements.\nBesides, it shows\nfantastic capacities of conjuring new contents from thin air,\nlike \u201cpouring water into a glass\u201d or simulating \u201clightning\nand thunder\u201d. In addition, with motion intensity guidance,\nLivePhoto supports users to customize the motion with the\ndesired intensity.\n2. Related Work\nImage animation. To realize content controllable video\nsynthesis, image animation takes a reference image as con-\ntent guidance. Most of the previous works [7, 38, 39, 50, 51]\ndepend on another video as a source of motion, transferring\nthe motion to the image with the same subject. Other works\nfocus on specific categories like fluide [13, 26, 29] or nature\nobjects [16, 21]. Make-it-Move [15] uses text control but it\nonly manipulates simple geometries like cones and cubes.\nRecently, human pose transfer methods [5, 17, 42, 48]\nconvert the human images to videos with extra controls\nlike dense poses, depth maps, etc. VideoComposer [43]\ncould take image and text as controls, however, the text\nshows limited controllability for the motion and it usually\nrequires more controls like sketches and motion vectors.\nIn general, existing work either requires more controls\nthan text or focuses on a specific subject.\nIn this work,\nwe explore constructing a generalizable framework for\nuniversal domains and use the most flexible control (text)\nto customize the generated video.\nText-to-video generation.\nAssisted by the diffusion\nmodel [11], the field of text-to-video has progressed rapidly.\nEarly attempts [12, 40, 47] train the entire parameters,\nmaking the task resource-intensive. Recently, researchers\nhave turned to leveraging the frozen weights of pre-trained\ntext-to-image models tapping into robust priors. Tune-A-\nVideo [45] inflates the text-to-video model and tuning atten-\ntion modules to construct an inter-frame relationship with\na one-shot setting. Align-Your-Lantens [2] inserts newly\ndesigned temporal layers into frozen text-to-image models\n2\nConcat\nNoise Latent\n\u201cA little boy in green \nclenches his fists.\"\n1 / 2 / 3 /\u2026/ 10\nFrame Embed\nReference Latent\nNoise Latent\nIntensity Embed\nB, F, C = 10, H, W\n\u2130\nText Encoder\nText Re-weight\nContent Encoder\n\ud835\udc9f\nB, F, C = 4, H, W\nMotion\nEstimation\n\u2130\nTraining Procedure Only\nReference Image\nMotion Intensity\nPick First Frame\nReference Latent\nStable Diffusion\nMotion Module\n\u2130\n\ud835\udc9f\nLatent Encoder\nLatent Decoder\nPrior Inversion\nFigure 2. Overall pipeline of LivePhoto. Besides taking the reference image and text as input, LivePhoto leverages the motion intensity\nas a supplementary condition. The image and the motion intensity (from level 1 to 10) are obtained from the ground truth video during\ntraining and customized by users during inference. The reference latent is first extracted as local content guidance. We concatenate it\nwith the noise latent, a frame embedding, and the intensity embedding. This 10-channel tensor is fed into the UNet for denoising. During\ninference, we use the inversion of the reference latent instead of the pure Gaussian to provide content priors. At the top, a content encoder\nextracts the visual tokens to provide global content guidance. At the bottom, we introduce text re-weighting, which learns to emphasize\nthe motion-related part of the text embedding for better text-motion mapping. The visual and textual tokens are injected into the UNet via\ncross-attention. For the UNet, we freeze the pre-trained stable diffusion and insert motion modules to capture the inter-frame relations.\nSymbols of flames and snowflakes denote trainable and frozen parameters respectively.\nto make video generation. AnimateDiff [10] proposes to\nfreeze the stable diffusion [35] blocks and add learnable\nmotion modules, enabling the model to incorporate with\nsubject-specific LoRAs [14] to make customized genera-\ntion. A common issue is that the text could only control\nthe spatial content of the video but exert limited effect for\ncontrolling the motions.\n3. Method\nWe first give a brief introduction to the preliminary knowl-\nedge for diffusion-based image generation in Sec. 3.1.\nFollowing that, our comprehensive pipeline is outlined in\nSec. 3.2.\nAfterward, Sec. 3.3 delves into image content\nguidance to make the model refer to the image. In Sec. 3.4\nand Sec. 3.5, we elaborate on the novel designs of motion\nintensity guidance and text re-weighting to better align the\ntext conditions with the video motion.\n3.1. Preliminaries\nText-to-image with diffusion models.\nDiffusion mod-\nels [11] show promising abilities for both image and video\ngeneration.\nIn this work, we opt for the widely used\nStable Diffusion [35] as the base model, which adapts\nthe denoising procedure in the latent space with lower\ncomputations.\nIt initially employs VQ-VAE [20] as the\nlatent encoder to transform an image x0 into the latent\nspace: z0 = E(x0).\nDuring training, Stable Diffusion\ntransforms the latent into Gaussian noise as follows:\nzt = \u221a \u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere the noise \u03f5 \u223c U([0, 1]), and \u00af\u03b1t is a cumulative\nproducts of the noise coefficient \u03b1t at each step. Afterward,\nit learns to predict the added noise as:\nEz,c,\u03f5,t(\u2225\u03f5\u03b8(zt, c, t) \u2212 \u03f5\u22252\n2).\n(2)\nt is the diffusion timestep, c is the condition of text prompts.\nDuring inference, Stable Diffusion is able to recover an\nimage from Gaussian noise step by step by predicting the\nnoise added for each step. The denoising results are fed into\na latent decoder to recover the colored images from latent\nrepresentations as \u02c6x0 = D(\u02c6z0).\n3.2. Overall Pipeline\nThe framework of LivePhoto is demonstrated in Fig. 2.\nThe model takes a reference image, a text, and the motion\nintensity as input to synthesize the desired video. When the\nground truth video is provided during training, the reference\nimage is picked from the first frame, and the motion\nintensity is estimated from the video.\nDuring inference,\nusers could customize the motion intensity or directly use\nthe default level. LivePhoto utilizes a 4-channel tensor of\nzB\u00d7F \u00d7C\u00d7H\u00d7W to represent the noise latent of the video,\nwhere the dimensions mean batch, frame, channel, height,\nand width, respectively. The reference latent is extracted\nby VAE encoder [20] to provide local content guidance.\nMeanwhile, the motion intensity is transformed to a 1-\nchannel intensity embedding.\nWe concatenate the noise\nlatent, the reference latent, the intensity embedding, and a\nframe embedding to form a 10-channel tensor for the input\nof UNet. At the same time, we use a content encoder to\n3\nextract the visual tokens of the reference image and inject\nthem via cross-attention.\nA text re-weighting module is\nadded after the text encoder [32], which learns to assign\ndifferent weights to each part of the text to accentuate the\nmotion descriptions of the text. Following modern text-to-\nvideo models [2, 10]. We freeze the stable diffusion [35]\nblocks and add learnable motion modules [10] at each stage\nto capture the inter-frame relationships.\n3.3. Image Content Guidance\nThe most essential step is enabling LivePhoto to keep the\nidentity of the reference image.\nThus, we collect local\nguidance by concatenating the reference latent at the input.\nMoreover, we employ a content encoder to extract image\ntokens for global guidance. Additionally, we introduce the\nimage inversion in the initial noise to offer content priors.\nReference latent.\nWe extract the reference latent and\nincorporate it at the UNet input to provide pixel-level\nguidance. Simultaneously, a frame embedding is introduced\nto impart temporal awareness to each frame. Thus, the first\nframe could totally trust the reference latent. Subsequent\nframes make degenerative references and exhibit distinct\nbehavior.\nThe frame embedding is represented as a 1-\nchannel map, with values linearly interpolated from zero\n(first frame) to one (last frame).\nContent encoder. The reference latent effectively guides\nthe initial frames due to their higher pixel similarities.\nHowever, as content evolves in subsequent frames, un-\nderstanding the image and providing high-level guidance\nbecomes crucial. Drawing inspiration from [6], we employ\na frozen DINOv2 [30] to extract patch tokens from the\nreference image.\nWe add a learnable linear layer after\nDINOv2 to project these tokens, which are then injected\ninto the UNet through newly added cross-attention layers.\nPrior inversion.\nPrevious methods [19, 25, 27, 41, 45]\nprove that using an inverted noise of the reference image,\nrather than a pure Gaussian noise, could effectively provide\nappearance priors. During inference, we add the inversion\nof the reference latent r0 to the noise latent zn\nT of frame n\nat the initial denoising step (T), following Eq. (3).\n\u02dczn\nT = \u03b1n \u00b7 Inv(r0) + (1 \u2212 \u03b1n) \u00b7 zn\nT ,\n(3)\nwhere \u03b1n is a descending coefficient from the first frame\nto the last frame. We set \u03b1n as a linear interpolation from\n0.033 to 0.016 by default.\n3.4. Motion Intensity Estimation\nIt is challenging to align the motion coherently with the\ntext.\nWe analyze the core issue is that the text lacks\ndescriptions for the motion speed and magnitude. Thus,\nthe same text leads to various motion intensities, creating\nambiguity in the optimization process. To address this, we\nleverage the motion intensity as an additional condition. We\nparameterize the motion intensity using a single coefficient.\nThus, the users could adjust the intensity conveniently by\nsliding a bar or directly using the default value.\nIn our pursuit of parameterizing motion intensity, we\nexperimented with various methods, such as calculating\noptical flow magnitude, computing mean square error be-\ntween adjacent frames, and leveraging CLIP/DINO similar-\nity between frames. Ultimately, we found that Structural\nSimilarity (SSIM) [44] produces results the most aligned\nwith human perceptions. Concretely, given a training video\nclip Xn with n frames, we determine its motion intensity I\nby computing the average value for the SSIM [44] between\neach adjacent frame as in Eq. (4) and Eq. (5):\nI(Xn) = 1\nn\nn\u22122\nX\ni=0\nSSIM(xi, xi+1).\n(4)\nSSIM(x, y) = l(x, y)\u03b1 \u00b7 c(x, y)\u03b2 \u00b7 s(x, y)\u03b3.\n(5)\nThe structure similarity considers the luminance (l), con-\ntrast (c), and structure (s) differences between two images.\nBy default, \u03b1, \u03b2, and \u03b3 are set as 1.\nWe compute the motion intensity on the training data to\ndetermine the overall distribution and categorize the values\ninto 10 levels. We create a 1-channel map filled with the\nlevel numbers and concatenate it with the input of UNet.\nDuring inference, users can utilize level 5 as the default\nintensity or adjust it between levels 1 to 10. Throughout\nthis paper, unless specified, we use level 5 as the default.\n3.5. Text Re-weighting\nAnother challenge in instructing video motions arises from\nthe fact that the text prompt encompasses both \u201ccontent\ndescriptions\u201d and \u201cmotion descriptions\u201d.\nThe \u201ccontent\ndescriptions\u201d, translated by the frozen Stable Diffusion,\noften fail to perfectly align with the reference images. When\nwe expect the text prompts to guide the motion, the \u201ccontent\ndescriptions\u201d are inherently accentuated simultaneously.\nHowever, as the reference image provides superior content\nguidance, the effect of the whole text would be suppressed\nwhen content conflicts appear.\nTo accentuate the part related to the \u201cmotion descrip-\ntions\u201d, we explore manipulating the CLIP text embed-\ndings. Recognizing that directly tuning the text encoder\non limited samples might impact generalization, we assign\ndifferent weights for each embedding without disrupting the\nCLIP feature space.\nConcretely, we add three trainable\ntransformer layers and a linear projection layer after the\nCLIP text embeddings. Afterward, the predicted weights\nare normed from 0 to 1 with a sigmoid function. These\nweights are then multiplied with the corresponding text\nembeddings, thereby providing guidance that focuses on\ndirecting the motions. The comprehensive structure of the\ntext re-weighting module and actual examples are depicted\n4\nTransformer Encoder Layer\nTransformer Encoder Layer\nTransformer Encoder Layer\nLinear\nSigmoid\n\u2026\n\u2026\nText Embeddings\nMultiplication\nThe bear is dancing.\n 0.52    0.79    0.83      0.85\nA Shiba Inu is sitting down on the ground.\n 0.52            0.65       0.82       0.86            0.82       0.59  0.62       0.61\nThe  hair  of   the woman  flies in the wind. \n 0.50      0.69      0.60    0.55       0.59            0.73     0.69  0.58    0.72\nThe little boy  in red opens his mouth. \n 0.52     0.51       0.60    0.59  0.70     0.78        0.70      0.76\nFigure 3. Demonstrations for text re-weighting. We use three\ntransformer encoder layers and a frame-specific linear layer to\npredict the weight for each text token. Examples are given on the\nright. In cases where multiple tokens correspond to a single word,\nwe calculate the average weight for better visualization. The words\nwith the maximum weight are underlined.\nin Fig. 3.\nThe numerical results prove that the module\nsuccessfully learns to emphasize the \u201cmotion descriptions\u201d.\nThis allows signals from images and texts to integrate more\neffectively, resulting in stronger text-to-motion control.\n4. Experiments\n4.1. Implementation Details\nTraining configurations. We implement LivePhoto based\non the frozen Stable Diffusion v1.5 [35].\nThe structure\nof our Motion Module aligns with AnimateDiff [10]. Our\nmodel is trained on the WebVID [1] dataset employing 8\nA100 GPUs. We sample training videos with 16 frames,\nperform center-cropping, and resize each frame to 256 \u00d7\n256 pixels. For classifier-free guidance, we utilize a 0.5\nprobability of dropping the text prompt during training. We\nonly use a simple MSE loss to train the model.\nEvaluation protocols. We conduct user studies to compare\nour approach with previous methods and analyze our newly\ndesigned modules. To validate the generalization ability,\nwe gather images from various domains encompassing\nreal images and cartoons including humans, animals, still\nobjects, natural sceneries, etc. For quantitative assessment,\nwe utilize the validation set of WebVID [1].\nThe first\nframe and prompt are used as controls to generate videos.\nWe measure the average CLIP similarity [32] and DINO\nsimilarity [30] between adjacent frames to evaluate the\nframe consistency following previous works [8, 43].\n4.2. Ablation Studies\nIn this section, we thoroughly analyze each of our proposed\nmodules to substantiate their effectiveness. We first analyze\nhow to add content guidance with the reference image,\nwhich is an essential part of our framework. Following that,\nwe delve into the specifics of our newly introduced motion\nintensity guidance and text re-weighting.\nImage content guidance. As introduced in Sec. 3.2, we\nconcatenate the reference latent with the input as the pixel-\nwise guidance and use a content encoder to provide the\n\u201cThe man is smiling.\u201d\nReference Latent\nReference Latent + Content Encoder\nReference Latent + Content Encoder + Prior Inversion\nRef Image\nFigure 4.\nAblations for the image content guidance.\nOnly\nconcatenating the reference latent with the model input meets\nchallenges in preserving the identity. The content encoder and\nprior inversion gradually enhance the performance.\nTable 1. Quatitative analysis for image content guidance. We\nassess frame consistency using DINO and CLIP scores.\nThe\ncontent encoder and prior inversion bring steady improvements.\nMethod\nDINO Score (\u2191)\nCLIP Score (\u2191)\nReference Latent\n82.3\n91.7\n+ Content Encoder\n85.9\n93.2\n++ Prior Inversion\n90.8\n95.2\nholistic identity information. Besides, the prior inversion\nfurther assists the generation of details.\nIn Fig. 4, we\nillustrate the step-by-step integration of these elements. In\nrow 1, the reference latent could only keep the identity\nfor the starting frames as the contents are similar to the\nreference image.\nAfter adding the content encoder in\nrow 2, the identity for the subsequent frames could be\nbetter preserved but the generation quality for the details\nis not satisfactory. With the inclusion of prior inversion, the\noverall quality sees further improvement. The quantitative\nresults in Tab. 1 consistently confirm the effectiveness of\neach module. These three strategies serve as the core of our\nstrong baseline for real image animation.\nMotion intensity guidance. As introduced in Sec. 3.4, we\nparameterize the motion intensity as a coefficient, and use\nit to indicate the motion speed and ranges. We carry out\nablation studies in Fig. 5. The absence of motion intensity\nguidance often leads to static or erratic video outputs, as\ndepicted in the first row. However, with the introduction\nof intensity guidance, the subsequent rows display varying\nmotion levels, allowing for the production of high-quality\nvideos with different motion ranges. Notably, lower levels\nlike level 2 generate almost static videos, while higher levels\nlike 10 occasionally produce overly vigorous motions.\nUsers could directly use the default value (level 5) or tailor\nthe intensity according to specific preferences.\n5\nw/o  Motion Intensity Guidance \nRef Image\nw/o Motion Intensity Guidance \nRef Image\nMotion Intensity Level: 2 \nMotion Intensity Level: 5 \nRef Image\nRef Image\nMotion Intensity Level:  7 \nMotion Intensity Level: 10 \nRef Image\nRef Image\nFigure 5. Illustrations of motion intensity guidance. The prompt is \u201cThe bear is dancing\u201d. Without intensity guidance, the generated\nvideo tends to either keep still or quickly become blurry. With the option to set varying intensity levels, users can finely control the motion\nrange and speed. It should be noted that excessively high intensity levels might induce motion blur, as observed in the last case.\nRef Image\nw/o Text Re-weighting.  Becomes a \u201cbaby\u201d.\nw/o Text Re-weighting.  Becomes a \u201cdinosaur\u201d.\nw/ Text Re-weighting.  Emphasizes \u201cwaving its hand.\u201d\nThe little yellow baby dinosaur is waving its hand.\n 0.49      0.50         0.57        0.65           0.70          0.82      0.87          0.57   0.86 \nw/o Text Re-weighting.  Does not follow the text.\nFigure 6. Ablation for text re-weighting. Without re-weighting,\nthe model tends to either disregard the text entirely or fixate\non content-related descriptions like \u201cbaby dinosaur\u201d. When re-\nweighting is applied, content descriptions are suppressed while\nmotion-related details like \u201cwaving its hand\u201d gain emphasis. The\npredicted weights of text re-weighting are marked at the bottom.\nText re-weighting. In Fig. 6, we demonstrate the efficacy\nof text re-weighting. In the given examples, the content de-\nscription \u201cbaby dinosaur\u201d would conflict with the reference\nimage. In the first three rows, without the assistance of re-\nweighting, the frozen Stabel Diffusion tends to synthesize\nthe content through its understanding of the text.\nThus,\nthe produced video tends to ignore the text and follow the\nreference image as in row 1. In other cases, it has risks\nof becoming a \u201cbaby\u201d (row 2) or a \u201cdinosaur\u201d (row 3).\nTable 2.\nQuatitative analysis for novel modules.\nFrame\nconsistency is measured by DINO and CLIP scores.\nMotion\nintensity guidance and text re-weighting both make contributions.\nMethod\nDINO Score (\u2191)\nCLIP Score (\u2191)\nLivePhoto\n90.8\n95.2\nw/o Motion Intensity\n90.3\n94.8\nw/o Text Re-weighting\n90.1\n93.9\nAs visualized in the bottom of Fig. 6, text re-weighting\nelevates emphasis on motion descriptions like \u201cwaving its\nhand\u201d. This approach enables our model to faithfully follow\ntext-based instructions for motion details while upholding\nimage-consistent content with the reference image.\nThe quantitative results are listed in Tab. 2. The motion\nintensity guidance and text re-weighting both contribute to\nthe frame consistency.\n4.3. Comparisons with Existing Alternatives\nWe compare LivePhoto with other works that support image\nanimation with text control.\nVideoComposer [43] is a\nstrong compositional generator covering various conditions\nincluding image and text. GEN-2 [34] and Pikalabs [33]\nare famous products that support image and text input.\nI2VGEN-XL [9], AnimateDiff-I2V [25], Talesofai [41] are\nopen-source projects claiming similar abilities.\nQualitative analysis.\nIn Fig. 7, we compare LivePhoto\nwith VideoComposer [43], Pikalabs [33], and GEN-2 [34]\nwith representative examples. The selected examples cover\nanimals, humans, cartoons, and natural scenarios. To re-\nduce the randomness, we ran each method 8 times to select\nthe best result for more fair comparisons. VideoComposer\ndemonstrates proficiency in creating videos with significant\nmotion. However, as not specifically designed for photo\nanimation, the identity-keeping ability is not satisfactory.\nThe identities of the reference images are lost, especially for\n6\nPikalabs\nVideoComp\nGEN-2\nOurs\n\u201cThe bear is waving its hand.\u201d\nExample 1\n\u201cThe woman with her hair flying in the wind.\u201d\nExample 2\n\u201cThe little yellow baby dinosaur is waving its hand.\u201d\n\u201cThe volcano emits thick smoke from its crater.\u201d\nExample 6\n\u201cBaymax kicks the ball.\u201d\nExample 3\nExample 4\n\u201cFire burns on the grass stack.\u201d\nExample 5\nPikalabs\nVideoComp\nGEN-2\nOurs\nPikalabs\nVideoComp\nGEN-2\nOurs\nPikalabs\nVideoComp\nGEN-2\nOurs\nPikalabs\nVideoComp\nGEN-2\nOurs\nPikalabs\nVideoComp\nGEN-2\nOurs\nFigure 7. Comparison results with other methods. We compare our LivePhoto with VideoComposer [43], Pikalabs [33], and GEN-\n2 [34]. We select representative cases covering animal, human, cartoon, and natural scenery. To ensure a fair evaluation, we executed\neach method 8 times, presenting the most optimal outcomes for comparison. In each example, the reference image is displayed on the left,\naccompanied by the text prompt indicated at the bottom.\n7\nTable 3. Results of user study. We let annotators rate from four\nperspectives: Image consistency (Cimage) evaluates the capability\nto maintain the identity of the reference image.\nText consis-\ntency (Ctext) measures the adherence to the textual descriptions\nin directing motion. Content quality (Qcont) focuses on the inter-\nframe coherence and resolutions. Motion quality (Qmot) evaluates\nappropriateness of motions.\nMethod\nCimage (\u2191)\nCtext (\u2191)\nQcont (\u2191)\nQmot (\u2191)\nVideoComposr [43]\n2.8\n3.5\n3.6\n3.6\nPikalabs [33]\n3.9\n2.7\n4.6\n3.1\nGEN-2 [34]\n3.7\n2.5\n4.8\n3.3\nLivePhoto\n3.6\n4.7\n3.7\n3.9\nw/o text re-weighting\n3.5\n3.3\n3.6\n3.8\nw/o intensity guidance\n3.4\n2.5\n3.4\n3.5\nless commonly seen subjects. Additionally, it shows a lack\nof adherence to the provided text instructions. Pikalabs [33]\nand GEN-2 [34] produce high-quality videos.\nHowever,\nas a trade-off, the generated videos own limited motion\nranges. Although they support text as supplementary, the\ntext descriptions seldom work. The motions are generally\nestimated from the content of the reference image.\nIn contrast, LivePhoto adeptly preserves the identity\nof the reference image and generates consistent motions\nwith the text instructions.\nIt performs admirably across\nvarious domains, encompassing animals, humans, cartoon\ncharacters, and natural sceneries.\nIt not only animates\nspecific actions (examples 1-4) but also conjures new effects\nfrom thin air (examples 5-6).\nWe also compare LivePhoto with open-sourced project\nin Fig. 8.\nI2VGEN-XL [9] does not set the reference\nimage as the first frame but generates videos with similar\nsemantics.\nAnimateDiff-I2V [25] and Talsofai [41] are\nextensions of AnimateDiff [10].\nHowever, the former\nproduces quasi-static videos. The latter fails to keep the\nimage identity unless using SD-generated images with the\nsame prompt and corresponding LoRA [14].\nUser studies. Metrics like DINO/CLIP scores have limita-\ntions in thoroughly evaluating the model, thus, we carry out\nuser studies. We ask the annotators to rate the generated\nvideos from 4 perspectives: Image consistency evaluates\nthe identity-keeping ability of the reference image. Text\nconsistency measures whether the motion follows the text\ndescriptions. Content quality considers the general quality\nof videos like the smoothness, the resolution, etc. Motion\nquality assesses the reasonableness of generated motion,\nencompassing aspects such as speed and deformation.\nWe construct a benchmark with five tracks: humans,\nanimals, cartoon characters, still objects, and natural scener-\nies. We collect 10 reference images per track and manually\nwrite 2 prompts per image.\nConsidering the variations\nthat commonly exist in video generation, each method is\nrequired to predict 8 results. Thus, we get 800 samples for\n\u201cPoop emoji, the camera moves around from left to right.\u201d\nRef Image\nI2VGEN-XL\nAni-I2V\nTalesofai\nOurs\nFigure 8. Comparisons with open-sourced projects. I2VGEN-\nXL [9], AnimateDiff-I2V [25], and Talesofai [41] also support an-\nimating an image with text. However, I2VGEN-XL only generates\n\u201crelevant\u201d content with the reference image. The produced videos\nof AnimateDiff-I2V rarely move. Talesofai could not keep the\nidentity for real photos.\neach method. We first ask 4 annotators to pick the best ones\nout of 8 predictions according to the aforementioned four\nperspectives. Then, we ask 10 annotators to further rate the\nfiltered samples. As the projects [9, 25, 41] demonstrates\nevidently inferior results, we only compare LivePhoto with\nVideoComposer [43], GEN-2 [34], and Pikalabs [33].\nResults in Tab. 3 demonstrate that GEN-2[34] and Pikal-\nabs own slightly better image consistency because their\ngenerated video seldom moves. LivePhoto shows signifi-\ncantly better text consistency and motion quality compared\nwith other works.\nWe admit that GEN-2 and Pikalabs\nown superior smoothness and resolution.\nWe infer that\nthey might collect much better training data and leverage\nsuper-resolution networks as post-processing.\nHowever,\nas an academic method, LivePhoto shows distinguishing\nadvantages over mature products in certain aspects.\nWe\nhave reasons to believe its potential for future applications.\n5. Limitations\nLivePhoto is implemented on SD-1.5 with 256\u00d7256 output\nconsidering the training cost. We believe that with higher\nresolution and stronger models like SD-XL [31], the overall\nperformance could be further improved significantly.\n6. Conclusion\nWe introduce LivePhoto, a novel framework for photo\nanimation with text control. We propose a strong baseline\nthat gathers the image content guidance from the given\nimage and utilizes motion intensity as a supplementary\nto better capture the desired motions.\nBesides, we\npropose\ntext\nre-weighting\nto\naccentuate\nthe\nmotion\ndescriptions.\nThe whole pipeline illustrates impressive\nperformance for generalized domains and instructions.\n8\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 5\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with\nlatent diffusion models. In CVPR, 2023. 2, 4\n[3] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu.\nStablevideo: Text-driven consistency-aware diffusion video\nediting. In ICCV, 2023. 2\n[4] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao,\nEnze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping\nLuo, Huchuan Lu, et al.\nPixart: Fast training of diffu-\nsion transformer for photorealistic text-to-image synthesis.\narXiv:2310.00426, 2023. 2\n[5] Tsai-Shien Chen, Chieh Hubert Lin, Hung-Yu Tseng,\nTsung-Yi\nLin,\nand\nMing-Hsuan\nYang.\nMotion-\nconditioned diffusion model for controllable video synthesis.\narXiv:2304.14404, 2023. 2\n[6] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao.\nAnydoor: Zero-shot object-level\nimage customization. arXiv:2307.09481, 2023. 2, 4\n[7] Chia-Chi Cheng, Hung-Yu Chen, and Wei-Chen Chiu. Time\nflies:\nAnimating a still image with time-lapse video as\nreference. In CVPR, 2020. 2\n[8] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn ICCV, 2023. 2, 5\n[9] Alibaba group. I2vgen-xl. https://modelscope.cn/\nmodels/damo/Video-to-Video/summary, 2023.\n2, 6, 8\n[10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv:2307.04725, 2023. 2, 3, 4, 5, 8\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising\ndiffusion probabilistic models. NeurIPS, 2020. 2, 3\n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet.\nVideo\ndiffusion models. arXiv:2204.03458, 2022. 2\n[13] Aleksander Holynski, Brian L Curless, Steven M Seitz, and\nRichard Szeliski. Animating pictures with eulerian motion\nfields. In CVPR, 2021. 2\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models.\narXiv:2106.09685, 2021. 3, 8\n[15] Yaosi Hu, Chong Luo, and Zhenzhong Chen.\nMake it\nmove:\ncontrollable image-to-video generation with text\ndescriptions. In CVPR, 2022. 2\n[16] Wei-Cih Jhou and Wen-Huang Cheng. Animating still land-\nscape photographs through cloud motion creation.\nTMM,\n2015. 2\n[17] Johanna\nKarras,\nAleksander\nHolynski,\nTing-Chun\nWang, and Ira Kemelmacher-Shlizerman.\nDreampose:\nFashion image-to-video synthesis via stable diffusion.\narXiv:2304.06025, 2023. 2\n[18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, 2023. 2\n[19] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators.\narXiv:2303.13439, 2023. 2, 4\n[20] Diederik P Kingma and Max Welling.\nAuto-encoding\nvariational bayes. arXiv:1312.6114, 2013. 3\n[21] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander\nHolynski. Generative image dynamics. arXiv:2309.07906,\n2023. 2\n[22] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\nand Jiashi Feng. Magicedit: High-fidelity and temporally\ncoherent video editing. arXiv:2308.14749, 2023. 2\n[23] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng\nZheng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang Cao.\nCones: Concept neurons in diffusion models for customized\ngeneration. arXiv:2303.05125, 2023. 2\n[24] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai\nZhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and Yang\nCao. Cones 2: Customizable image synthesis with multiple\nsubjects. arXiv:2305.19327, 2023. 2\n[25] Tyler Luan. Animatediff-i2v. https://github.com/\nykk648/AnimateDiff-I2V, 2023. 2, 4, 6, 8\n[26] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable\nanimation of fluid elements in still images. In CVPR, 2022.\n2\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv:2108.01073, 2021. 4\n[28] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,\nZhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter:\nLearning adapters to dig out more controllable ability for\ntext-to-image diffusion models. arXiv:2302.08453, 2023. 2\n[29] Makoto Okabe, Ken Anjyo, Takeo Igarashi, and Hans-Peter\nSeidel. Animating pictures of fluid using video examples. In\nComputer Graphics Forum. Wiley Online Library, 2009. 2\n[30] Maxime Oquab, Timoth\u00b4ee Darcet, Th\u00b4eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv:2304.07193, 2023. 4, 5\n[31] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach. Sdxl: Improving latent diffusion models\nfor high-resolution image synthesis.\narXiv:2307.01952,\n2023. 8\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In Int. Conf. Mach. Learn., 2021. 4, 5\n9\n[33] PikaLabs reseachers. Pikalabs: An innovative text-to-video\nplatform. https://www.pika.art/, 2023.10. 2, 6, 7,\n8\n[34] Runway reseachers.\nGen-2:\nThe next step forward for\ngenerative ai. https://research.runwayml.com/\ngen2, 2023.10. 2, 6, 7, 8\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., 2022. 2, 3, 4, 5\n[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\n2023. 2\n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Adv. Neural Inform. Process. Syst.,\n2022. 2\n[38] Yoav Shalev and Lior Wolf. Image animation with perturbed\nmasks. In CVPR, 2022. 2\n[39] Aliaksandr Siarohin, St\u00b4ephane Lathuili`ere, Sergey Tulyakov,\nElisa Ricci, and Nicu Sebe. First order motion model for\nimage animation. NeurIPS, 2019. 2\n[40] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv:2209.14792, 2022. 2\n[41] talesofai.\nAnimatediff talesofai.\nhttps://github.\ncom/talesofai/AnimateDiff, 2023. 2, 4, 6, 8\n[42] Tan Wang,\nLinjie Li,\nKevin Lin,\nChung-Ching Lin,\nZhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan\nWang.\nDisco: Disentangled control for referring human\ndance generation in real world. arXiv:2307.00040, 2023. 2\n[43] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and\nJingren Zhou. Videocomposer: Compositional video syn-\nthesis with motion controllability. NeurIPS, 2023. 2, 5, 6, 7,\n8\n[44] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli. Image quality assessment: from error visibility\nto structural similarity. TIP, 2004. 4\n[45] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nICCV, 2023. 2, 4\n[46] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu,\nZhuofan Zong, Yu Liu, and Ping Luo.\nRaphael: Text-\nto-image generation via large mixture of diffusion paths.\nNeurIPS, 2023. 2\n[47] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang,\nXiaodong Wang, Minheng Ni, Zhengyuan Yang, Linjie\nLi, Shuguang Liu, Fan Yang, et al.\nNuwa-xl:\nDiffu-\nsion over diffusion for extremely long video generation.\narXiv:2303.12346, 2023. 2\n[48] Jianfeng Zhang, Hanshu Yan, Zhongcong Xu, Jiashi Feng,\nand Jun Hao Liew. Magicavatar: Multimodal avatar genera-\ntion and animation. arXiv:2308.14748, 2023. 2\n[49] Lvmin\nZhang\nand\nManeesh\nAgrawala.\nAdding\nconditional control to text-to-image diffusion models.\narXiv:2302.05543, 2023. 2\n[50] Jian Zhao and Hui Zhang. Thin-plate spline motion model\nfor image animation. In CVPR, 2022. 2\n[51] Ruiqi Zhao, Tianyi Wu, and Guodong Guo. Sparse to dense\nmotion transfer for face image animation. In ICCV, 2021. 2\n10\n"
  },
  {
    "title": "Describing Differences in Image Sets with Natural Language",
    "link": "https://arxiv.org/pdf/2312.02974.pdf",
    "upvote": "12",
    "text": "Describing Differences in Image Sets with Natural Language\nLisa Dunlap\u2217\nUC Berkeley\nlisabdunlap@berkeley.edu\nYuhui Zhang\u2217\nStanford\nyuhuiz@stanford.edu\nXiaohan Wang\nStanford\nxhanwang@stanford.edu\nRuiqi Zhong\nUC Berkeley\nruiqi-zhong@berkeley.edu\nTrevor Darrell\u2020\nUC Berkeley\ntrevordarrell@berkeley.edu\nJacob Steinhardt\u2020\nUC Berkeley\njsteinhardt@berkeley.edu\nJoseph E. Gonzalez\u2020\nUC Berkeley\njegonzal@berkeley.edu\nSerena Yeung-Levy\u2020\nStanford\nsyyeung@stanford.edu\n\ud835\udc37! contains more... \"People posing for a picture\u201d\n\ud835\udc6b\ud835\udc68\n\ud835\udc6b\ud835\udc69\nOutput\n\u2026\n\u2026\nFigure 1. Set difference captioning. Given two sets of images DA and DB, output natural language descriptions of concepts which are\nmore true for DA. In this example, DA and DB are images from the \u201cDining Table\u201d class in ImageNetV2 and ImageNet, respectively.\nAbstract\nHow do two sets of images differ? Discerning set-level\ndifferences is crucial for understanding model behaviors\nand analyzing datasets, yet manually sifting through thou-\nsands of images is impractical. To aid in this discovery pro-\ncess, we explore the task of automatically describing the\ndifferences between two sets of images, which we term Set\nDifference Captioning. This task takes in image sets DA\nand DB, and outputs a description that is more often true\non DA than DB. We outline a two-stage approach that first\nproposes candidate difference descriptions from image sets\nand then re-ranks the candidates by checking how well they\ncan differentiate the two sets. We introduce VisDiff, which\nfirst captions the images and prompts a language model\nto propose candidate descriptions, then re-ranks these de-\nscriptions using CLIP. To evaluate VisDiff, we collect VisD-\niffBench, a dataset with 187 paired image sets with ground\ntruth difference descriptions. We apply VisDiff to various\ndomains, such as comparing datasets (e.g., ImageNet vs.\nImageNetV2), comparing classification models (e.g., zero-\nshot CLIP vs. supervised ResNet), characterizing differ-\nences between generative models (e.g., StableDiffusionV1\n*Equal contribution. \u2020Equal advising. Both orders decided by coin flip.\nand V2), and discovering what makes images memorable.\nUsing VisDiff, we are able to find interesting and previously\nunknown differences in datasets and models, demonstrating\nits utility in revealing nuanced insights.1\n1. Introduction\nWhat kinds of images are more likely to cause errors in one\nclassifier versus another [11, 18]? How do visual concepts\nshift from a decade ago to now [20, 33, 53]? What types\nof images are more or less memorable for humans [17]?\nAnswering these questions can help us audit and improve\nmachine learning systems, understand cultural changes, and\ngain insights into human cognition.\nAlthough these questions have been independently stud-\nied in prior works, they all share a common desidera-\ntum: discovering differences between two sets of images.\nHowever, discovering differences in many, potentially very\nlarge, sets of images is a daunting task for humans. For\nexample, one could gain insights into human memory by\ndiscovering systematic differences between memorable im-\nages and forgettable ones, but finding these differences may\n1Project page available at https://understanding-visual-\ndatasets.github.io/VisDiff-website/.\n1\narXiv:2312.02974v1  [cs.CV]  5 Dec 2023\nGiven the captions for sets A and B:\nA: woman riding a horse in a rodeo\nA: a group of jockeys and horses are racing on a grassy field\nA: ...\nB: a woman in white riding a grey horse\nB: a person riding a horse in an equestrian competition\nB: ...\nList concepts that are more true for set A than set B.\nPROPOSER\n\ud835\udc66!: \u201chorses racing on track\u201d\n\ud835\udc66\": \u201cmultiple jockeys\u201d \n\ud835\udc66#: \u201chorse racing events\u201d \n\ud835\udc37!\nRANKER\nCLIP(\u201chorse racing events\u201d)\nCLIP(\n)\n\ud835\udc63 \ud835\udc4f!, \ud835\udc66#  = \n.\n0.91\n0.70\n0.87\nCandidate Differences \nScore \n\ud835\udc34\ud835\udc48\ud835\udc45\ud835\udc42\ud835\udc36 \u22c3\ud835\udc9f! \ud835\udc63 \ud835\udc4e\", \ud835\udc66#  \u22c3\ud835\udc9f\" \ud835\udc63 \ud835\udc4f\", \ud835\udc66#  \nStep 2: Rank Differences\nStep 1: Propose Differences\n\ud835\udc37-\nCaptioner (BLIP-2)\nFigure 2. VisDiff algorithm. VisDiff consists of a GPT-4 proposer on BLIP-2 generated captions and a CLIP ranker. The proposer takes\nrandomly sampled image captions from DA and DB and proposes candidate differences. The ranker takes these proposed differences and\nevaluates them across all the images in DA and DB to assess which ones are most true.\nrequire scanning through thousands of images. An auto-\nmated solution would be more scalable.\nIn this work, we explore the task of describing differ-\nences between image sets, which we term Set Difference\nCaptioning (Figure 1). Specifically, given two sets of im-\nages DA and DB, set difference captioning aims to find the\nmost salient differences by generating natural language de-\nscriptions that are more often true in DA than DB. We\nshow in Section 6 that many dataset and model analysis\ntasks can be formulated in terms of set difference caption-\ning, and methods that address this problem can help humans\ndiscover new patterns in their data.\nSet difference captioning presents unique challenges to\ncurrent machine learning systems, since it requires reason-\ning over all the given images. However, no existing mod-\nels in the vision and language space can effectively reason\nabout thousands of images as input. Furthermore, while\nthere are usually many valid differences between DA and\nDB, end users are typically interested in what can most ef-\nfectively differentiate between the two sets. For example,\n\u201cbirthday party\u201d is a valid difference in Figure 1, but \u201cpeo-\nple posing for a picture\u201d better separates the sets.\nWe introduce a two-stage proposer-ranker approach [49,\n50, 53] for set difference captioning that addresses these\nchallenges. As shown in Figure 2, the proposer randomly\nsamples subsets of images from DA and DB to generate a\nset of candidate differences in natural language. The ranker\nthen scores the salience and significance of each candidate\nby validating how often this difference is true for individual\nsamples in the sets. Within the proposer-ranker framework,\nthere are many plausible design choices for each compo-\nnent, and in this work we investigate three categories of\nproposers and rankers that utilize different combinations of\nmodels pre-trained with different objectives.\nTo evaluate design choices, we construct VisDiffBench\n(Figure 3), a dataset consisting of 187 paired image sets\nwith ground-truth differences. We also propose a large lan-\nguage model-based evaluation to measure correctness. By\nbenchmarking different designs on VisDiffBench, we iden-\ntify our best algorithm, VisDiff, which combines a proposer\nbased on BLIP-2 captions and GPT-4 with a ranker based on\nCLIP features. This method accurately identifies 61% and\n80% of differences using top-1 and top-5 evaluation even\non the most challenging split of VisDiffBench.\nFinally, we apply VisDiff to a variety of applications,\nsuch as finding dataset differences, comparing model be-\nhaviors, and understanding questions in cognitive science.\nVisDiff identifies both differences that can be validated\nby prior works, as well as new findings that may moti-\nvate future investigation. For example, VisDiff uncovers\nImageNetV2\u2019s temporal shift compared to ImageNet [5,\n35], CLIP\u2019s strength in recognizing texts within images\ncompared to ResNet [13, 34], StableDiffusionV2 gener-\nated images\u2019 stylistic changes compared to StableDiffu-\nsionV1 [38], and what images are more memorable by hu-\nmans [16]. These results indicate that the task of set dif-\nference captioning is automatic, versatile, and practically\nuseful, opening up a wealth of new application opportuni-\nties for future work and potentially mass-producing insights\nunknown to even experts across a wide range of domains.\n2. Related Works\nMany prior works explored difference captioning [1, 21, 22,\n46] and change captioning [2, 19, 31], which aim to de-\nscribe differences between a single pair of images with lan-\nguage. Recent large visual language models (VLMs) like\nGPT-4V [30] have shown promise in describing differences\nin small groups of images. However, the question of how\n2\nto scale this problem to sets containing thousands of images\nremains unanswered. Meanwhile, some existing works in\nvision tackle understanding large amounts of visual data\nthrough finding concept-level prototypes [8, 42], \u201caverag-\ning\u201d large collections of images [52], using simple methods\nlike RGB value analysis [28, 41], or using a combination\nof detectors and classifiers to provide dataset level statis-\ntics [44]. However, they do not describe the differences in\nnatural language, which is flexible and easy to interpret.\nOur work draws inspiration from D3 [49] and D5 [50]\nframeworks, which use large language models (LLMs) to\ndescribe differences between text datasets. A recent work\nGS-CLIP [53] applied a similar framework as D3 in the\nimage domain, using CLIP features to retrieve differences\nfrom a pre-defined text bank. While this work targets the\ntask of set difference captioning, it struggles at generating\ndescriptive natural language and has a limited evaluation on\nthe MetaShift [24] dataset that we found contains a signif-\nicant amount of noise. Inspired by D3 [49], our study ad-\nvances a proposer-ranker framework tailored for the visual\ndomain, leveraging large visual foundation models and a\nwell-designed benchmark dataset. The versatility and effec-\ntiveness of our approach are further demonstrated through\napplications across a variety of real-world scenarios, under-\nscoring its potential impact and utility in practical settings.\nLastly, the set difference captioning setting is closely\nrelated to the field of explainable computer vision. Tra-\nditional explainable computer vision methodologies have\npredominantly concentrated on interpreting features or neu-\nrons within deep neural networks, as exemplified by ap-\nproaches like LIME [37], CAM [51], SHAP [27], and MI-\nLAN [15]. Recent shifts towards a data-centric AI paradigm\nhave sparked a wave of research focusing on identifying in-\nfluential data samples that impact predictions [32, 39], and\non discerning interpretable data segments [4, 6, 11], thereby\nelucidating model behaviors. Our set difference caption-\ning aligns with these objectives, offering a unique, purely\ndata-driven approach to understanding and explaining dif-\nferences in image sets with natural language.\n3. Set Difference Captioning\nIn this section, we first describe the task of set difference\ncaptioning, then introduce VisDiffBench, which we use to\nbenchmark performance on this task.\n3.1. Task Definition\nGiven two image datasets DA and DB, the goal of set dif-\nference captioning (SDC) is to generate a natural language\ndescription y that is more true in DA compared to DB. For\nexample, in Figure 3, both DA and DB contain images of\nhorses, but the images from DA are all from racing events,\nso a valid choice of y would be \u201chorse racing events\u201d.\nDataset\n# Paired Sets\n# Images Per Set\nImageNetR (sampled)\n14\n500\nImageNet\u2217 (sampled)\n23\n500\nPairedImageSets-Easy\n50\n100\nPairedImageSets-Medium\n50\n100\nPairedImageSets-Hard\n50\n100\nTable 1. Summary of VisDiffBench. In experiments, we merge\nImageNetR and ImageNet\u2217 because they have limited sets.\nIn our benchmarks below, we annotate (DA, DB) with a\nground truth y\u2217 based on knowledge of the data-generating\nprocess. In these cases, we consider an output y to be cor-\nrect if it matches y\u2217 up to semantic equivalence (see Sec-\ntion 3.3 for details). In our applications (Section 6), we also\nconsider cases where the ground truth y\u2217 is not known.\n3.2. Benchmark\nTo evaluate systems for set difference captioning, we con-\nstruct VisDiffBench, a benchmark of 187 paired image sets\neach with a ground-truth difference description. To create\nVisDiffBench, we curated a dataset PairedImageSets that\ncovers 150 diverse real-world differences spanning three\ndifficulty levels. We supplemented this with 37 differences\nobtained from two existing distribution shift benchmarks,\nImageNet-R and ImageNet\u2217. Aggregate statistics for VisD-\niffBench are given in Table 1.\nImageNet-R: ImageNet-R [14] contains renditions of\n200 ImageNet classes across 14 categories (e.g., art, car-\ntoon, painting, sculpture, sketch). For each category, we set\ny\u2217 to be the name of the category, DA to be 500 images\nsampled from that category, and DB to be 500 original Im-\nageNet images sampled from the same 200 classes.\nImageNet\u2217: ImageNet\u2217 [43] contains 23 categories of\nsynthetic images transformed from original ImageNet im-\nages using textual inversion. These categories include par-\nticular style, co-occurrence, weather, time of day, etc. For\ninstance, one category, \u201cat dusk,\u201d converts ImageNet im-\nages with the prompt \u201ca photo of a [inverse image token] at\ndusk\u201d. We generated differences analogously to ImageNet-\nR, taking DA to be 500 image samples from the category\nand DB to be 500 original ImageNet images.\nPairedImageSets: ImageNetR and ImageNet\u2217 mainly\ncapture stylistic differences, and only contain 37 differ-\nences in total.\nTo address these shortcomings, we con-\nstruct PairedImageSets, consisting of 150 paired image sets\nrepresenting diverse differences. The dataset was built by\nfirst prompting GPT-4 to generate 150 paired sentences\nwith three difficulty levels of differences (see Appendix A\nfor exact prompts). Easy level represents apparent differ-\nence (e.g., \u201cdogs playing in a park\u201d vs. \u201ccats playing in\na park\u201d), medium level represents fine-grained difference\n(e.g., \u201cSUVs on the road\u201d vs. \u201csedans on the road\u201d), and\n3\nPeople practicing \nyoga in a \nmountainous setting\nPeople meditating \nin a mountainous \nsetting\nCaption-based \n\u201cWomen doing yoga poses\u201d \n                    (0.87)\n\u201cYoga pose on a wooden deck in the snow\u201d (0.87)\n\u201cYoga poses\u201d \n \n                     (0.86)\n\u201cHandstand poses in nature\u201d \n                     (0.86)\n\u201cYoga on a pier\u201d \n                     (0.85)\nFeature-based \n\u201ca woman is standing with her arm up on a table\u201d \n(0.79)\n\u201ca person with arms and legs and a hand in a pose\u201d             (0.77)\n\u201ca woman standing in front of some cars\u201d \n                  (0.70)\n\u201cthe fenty handbag in white\u201d \n \n                  (0.66)\n\u201ca couple looking at a photo of a person in a bathing suit\u201d (0.65)\nImage-based \n\u201cYoga poses\u201d \n(0.86)\n\u201cPeople doing yoga\u201d               (0.80)\n\u201cPeople in athletic clothing\u201d (0.75)\n\u201cGroup activities\u201d                    (0.60)\n\u201cOutdoor activities\u201d \n(0.58)\nFigure 3. Top 5 descriptions generated by the caption-based, image-based, and feature-based proposer. All the top 5 descriptions\nfrom the caption-based proposer and the top 2 from the image-based proposer identify the ground-truth difference between \u201cpracticing\nyoga\u201d and \u201cmeditating\u201d, while feature-based fails. We report AUROC scores from the same feature-based ranker described in Section 4.2.\nhard level represents subtle difference (e.g., \u201cpeople practic-\ning yoga in a mountainous setting\u201d vs. \u201cpeople meditating\nin a mountainous setting\u201d).\nOnce GPT-4 generates the 150 paired sentences, we\nmanually adjusted the annotated difficulty levels to match\nthe criteria above. We then retrieved the top 100 images\nfrom Bing for each sentence. As a result, we collected 50\neasy, 50 medium, and 50 hard paired image sets, with 100\nimages for each set. One example pair from this dataset is\nshown in Figure 3, with further examples and a complete\nlist of paired sentences provided in Appendix A. We will\nrelease this dataset and the data collection pipeline.\n3.3. Evaluation\nTo evaluate performance on VisDiffBench, we ask algo-\nrithms to output a description y for each (DA, DB) pair and\ncompare it to the ground truth y\u2217. To automatically compute\nwhether the proposed difference is semantically similar to\nthe ground truth, we prompt GPT-4 to categorize similarity\ninto three levels: 0 (no match), 0.5 (partially match), and 1\n(perfect match); see Appendix A for the exact prompt.\nTo validate this metric, we sampled 200 proposed dif-\nferences on PairedImageSets and computed the correlation\nof GPT-4\u2019s scores with the average score across four inde-\npendent annotators. We observe a high Pearson correlation\nof 0.80, consistent with prior findings that large language\nmodels can align well with human evaluations [9, 48].\nWe will evaluate systems that output ranked lists of pro-\nposals for each (DA, DB) pair. For these systems, we mea-\nsure Acc@k, which is the highest score of any of the top-k\nproposals, averaged across all 187 paired image sets.\n4. Our Method: VisDiff\nIt is challenging to train a neural network to directly predict\ny based on DA and DB: DA and DB can be very large in\npractice, while currently no model can encode large sets of\nimages and reliably reason over them. Therefore, we em-\nploy a two-stage framework for set difference captioning,\nusing a proposer and a ranker [49, 50]. The proposer takes\nrandom subsets SA \u2286 DA and SB \u2286 DB and proposes dif-\nferences. The ranker takes these proposed differences and\nevaluates them across all of DA and DB to assess which\nones are most true. We explore different choices of the pro-\nposer and ranker in the next two subsections. Full experi-\nment details for this section, including the prompts for the\nmodels, can be found in Appendix B.\n4.1. Proposer\nThe proposer takes two subsets of images SA and SB as\ninputs and outputs a list Y of natural language descriptions\nthat are (ideally) more true on SA than SB. We leverage\nvisual language models (VLM) as the proposer in three dif-\nferent ways: from the images directly, from the embeddings\nof the images, or by first captioning images and then using\na language model. In all cases, we set |SA| = |SB| = 20\nbased on ablations.\nImage-based Proposer: We arrange the 20+20 input\nimages into a single 4-row, 10-column grid and feed this as\na single image into a VLM (in our case, LLaVA-1.5 [25]).\nWe then prompt the VLM to propose differences between\nthe top and bottom half of images.\nFeature-based Proposer: We embed images from SA\nand SB into the VLM\u2019s visual representation space, then\nsubtract the mean embeddings of SA and SB. This sub-\ntracted embedding is fed into VLM\u2019s language model to\ngenerate a natural language description of the difference.\nWe use BLIP-2 [23] for this proposer.\nCaption-based Proposer: We first use the VLM to gen-\nerate captions of each image in SA and SB.\nThen, we\nprompt a pure language model to generate proposed differ-\n4\nProposer\nRanker\nPIS-Easy\nPIS-Medium\nPIS-Hard\nImageNet-R/*\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nLLaVA-1.5 Image\nCLIP Feature\n0.71\n0.81\n0.39\n0.49\n0.28\n0.43\n0.27\n0.39\nBLIP-2 Feature\nCLIP Feature\n0.48\n0.69\n0.13\n0.33\n0.12\n0.23\n0.68\n0.85\nGPT-4 Caption\nVicuna-1.5 Caption\n0.60\n0.92\n0.49\n0.77\n0.31\n0.61\n0.42\n0.70\nGPT-4 Caption\nLLaVA-1.5 Image\n0.78\n0.99\n0.58\n0.80\n0.38\n0.62\n0.78\n0.88\nGPT-4 Caption\nCLIP Feature\n0.88\n0.99\n0.75\n0.86\n0.61\n0.80\n0.78\n0.96\nTable 2. Results on VisDiffBench. GPT-4 caption-based proposer and CLIP feature-based ranker consistently outperform other proposers\nand rankers by a large margin. We use this combination as the final VisDiff algorithm. PIS is short for PairedImageSets.\nences between the two sets of captions. We use BLIP-2 to\ngenerate the captions and GPT-4 to propose differences.\nExperiments in Section 5.1 show that the caption-based\nproposer works best, so we will take it as our main method\nand the other two as baselines. To further improve perfor-\nmance, we run the proposer multiple times over different\nsampled sets SA and SB, then take the union of the pro-\nposed differences as inputs to the ranker.\n4.2. Ranker\nSince the proposer operates on small subsets SA and SB\nand could generate invalid or noisy differences, we em-\nploy a ranker to validate and rank the proposed differences\ny \u2208 Y. The ranker sorts hypotheses by computing a dif-\nference score sy = Ex\u2208DAv(x, y) \u2212 Ex\u2208DBv(x, y), where\nv(x, y) is some measure of how well the image x satisfies\nthe hypothesis y. As before, we leverage VLMs to com-\npute the ranking score v(x, y) in three ways: from images\ndirectly, from image embeddings, and from image captions.\nImage-based Ranker:\nWe query the VQA model\nLLaVA-1.5 [25] to ask whether the image x contains y, and\nset v(x, y) = VQA(x, y) to be the resulting binary output.\nCaption-based Ranker: We generate a caption c from\nx using BLIP-2 [23], then ask Vicuna-1.5 [3] whether the\ncaption c contains y. We set v(x, y) = QA(c, y) to be the\nresulting binary output.\nFeature-based Ranker: We use CLIP ViT-G/14 [34] to\ncompute the cosine similarity between the image embed-\nding ex and text embedding ey, so that v(x, y) =\nex\u00b7ey\n\u2225ex\u2225\u2225ey\u2225.\nIn contrast to the other two scores, since v(x, y) is contin-\nuous rather than binary, we compute sy as the AUROC of\nusing v to classify between DA and DB.\nExperiments in Section 5.2 show that the feature-based\nranker achieves the best performance and efficiency, so we\nuse it as our main method and the other two as baselines. We\nalso filter out proposed differences that are not statistically\nsignificant, by running a t-test on the two score distributions\nv(x, y) with significance threshold 0.05.\n5. Results\nIn this section, we present experimental results to under-\nstand 1) which proposer / ranker works best, 2) can our al-\ngorithm consistently find the ground truth difference, and 3)\ncan our algorithm work under noisy settings.\n5.1. Which Proposer is Best?\nOur comparative results, presented in Table 2, demonstrate\nthat the caption-based proposer consistently outperforms\nits image-based and feature-based counterparts by a large\nmargin across all subsets of the VisDiffBench. This dif-\nference is particularly pronounced in the most challenging\nsubset, PairedImageSets-Hard. While the captioning pro-\ncess may result in some loss of information from the orig-\ninal images, the strong reasoning capabilities of large lan-\nguage models effectively compensate for this by identifying\ndiverse and nuanced differences between image sets. We\nprovide a qualitative example in Figure 3.\nThe image-based proposer shows commendable perfor-\nmance on PairedImageSets-Easy but significantly lags be-\nhind the caption-based proposer on the PairedImageSets-\nMedium/Hard subsets. This discrepancy can be attributed\nto the loss of visual details when aggregating numerous im-\nages into a single gridded super-image. The image-based\nproposer performs worst on ImageNetR and ImageNet\u2217,\nwhich we hypothesize that the visual language model strug-\ngles with reasoning across a wide range of classes.\nIn contrast, the feature-based proposer outperforms the\nimage-based proposer on ImageNetR and ImageNet\u2217 but\nis much less effective across all subsets of PairedImage-\nSets.\nWe believe this is because the feature-based ap-\nproach excels at distinguishing groups when one possesses\nattributes absent in the other (e.g., \u201cclipart of an image\u201d\nminus \u201can image\u201d equates to \u201cclipart\u201d).\nMost cases in\nImageNetR/ImageNet\u2217 fit this scenario. However, this ap-\nproach falls short in other situations where vector arithmetic\ndoes not yield meaningful semantic differences (e.g., \u201ccat\u201d\nminus \u201cdog\u201d is not semantically meaningful), which is a\ncommon scenario in PairedImageSets.\u201d\n5.2. Which Ranker is Best?\nIn Table 2, our results demonstrate that the feature-based\nranker consistently outperforms both the caption-based and\nimage-based rankers, particularly in the most challeng-\ning subset, PairedImageSets-Hard. The feature-based ap-\nproach\u2019s advantage is primarily due to its continuous scor-\n5\ning mechanism, which contrasts with the binary scores out-\nput by image-based and caption-based question answering.\nThis continuous scoring allows for more fine-grained im-\nage annotation and improved calibration. It is also logical\nto observe the image-based ranker outperforms the caption-\nbased one, as answering questions from original images\ntends to be more precise than from image captions.\nMoreover, the efficiency of the feature-based ranker is\nremarkable. In scenarios where M hypotheses are evalu-\nated on N images with N \u226b M, the computation of image\nfeatures is required only once. This results in a computa-\ntional complexity of O(M + N) \u2248 O(N), compared to\nO(MN) for both image-based and caption-based rankers.\nHence, the feature-based ranker requires significantly less\ncomputation, especially when ranking many hypotheses.\nThis efficiency is crucial in practical applications, as we\nhave found that a higher volume of proposed differences\nis essential for accurately identifying correct differences in\nthe Appendix C.\n5.3. Can Algorithm Find True Difference?\nIn Table 2, the results demonstrate the effectiveness of\nour algorithm in discerning differences.\nThe best algo-\nrithm, comprising a GPT-4 [30] caption-based proposer and\na CLIP [34] feature-based ranker, achieves accuracies of\n88%, 75%, and 61% for Acc@1, and 99%, 86%, and 80%\nfor Acc@5 on the PairedImageData-Easy/Medium/Hard\nsubsets, respectively.\nThe PairedImageData-Hard subset\nposes a significant challenge, requiring models to possess\nstrong reasoning abilities to perceive extremely subtle vari-\nations, such as distinguishing between \u201cFresh sushi with\nsalmon topping\u201d and \u201cFresh sushi with tuna topping\u201d, or\npossess enough world knowledge to discern \u201cMen wear-\ning Rolex watches\u201d from \u201cMen wearing Omega watches\u201d.\nDespite these complexities, our model demonstrates im-\npressive performance, accurately identifying specifics like\n\u201cSushi with salmon\u201d and \u201cMen wearing Rolex watches\u201d.\n5.4. Performance Under Noisy Data Splits\nIn the VisDiffBench dataset, image sets are composed with\nperfect purity. For instance, DA exclusively contains cat im-\nages (100%), while DB is entirely made up of dog images\n(100%). However, this level of purity is rare in real-world\nscenarios. Typically, such sets include a mix of elements \u2013\nfor example, DA might comprise 70% cat images and 30%\ndog images, and DB vice versa. To evaluate the robustness\nof the VisDiff algorithm against such noise, we introduced\nrandomness in VisDiffBench by swapping a certain percent-\nage of images between DA and DB. Here, 0% purity signi-\nfies 50% image swapping and an equal distribution of two\nsets, whereas 100% purity indicates no image swapping.\nFigure 4 presents the Acc@1 and Acc@5 performance\nof VisDiff across various purity levels, tested on 50 paired\nFigure 4. VisDiff performance under noise. We randomly swap\ndifferent percentages of images between DA and DB to inject\nnoise. Results are computed on 50 paired sets in PairedImageSets-\nHard. 95% confidence intervals are reported over three runs.\nsets within PairedImageSets-Hard. As anticipated, a decline\nin purity correlates with a drop in accuracy since identifying\nthe difference becomes harder. However, even at 40% pu-\nrity, Acc@1 remains at 49%, only modestly reduced from\n63% at 100% purity. This result underscores the robust-\nness of the VisDiff algorithm to noisy data. It is also worth\nnoting that VisDiff reaches near 0% accuracy at 0% purity,\nwhich is expected since the two sets have exactly the same\ndistribution and our method filters out invalid differences.\nOther ablations of VisDiff algorithm.\nIn Appendix C,\nwe further discuss how caption style, language model, sam-\nple size, and # sampling rounds affect VisDiff performance.\n6. Applications\nWe apply the best configuration of our VisDiff method to a\nset of five applications in computer vision: 1) comparing\nImageNet and ImageNetV2 (Section 6.1), 2) interpreting\nthe differences between two classifiers at the datapoint level\n(Section 6.2), 3) analyzing model errors (Section 6.3), 4)\nunderstanding the distributional output differences between\nStableDiffusionV1 and V2 (Section 6.4), and 5) discovering\nwhat makes an image memorable (Section 6.5). Since Vis-\nDiff is automatic, we used it to discover differences between\n(1) large sets of images or (2) many sets of images, thus\nmass-producing human-interpretable insights across these\napplications. In this section, we report VisDiff-generated\ninsights including some that can be confirmed with existing\nwork and others that may motivate future investigation in\nthe community. Additional details for each application can\nbe found in Appendix D.\n6.1. Comparing ImageNetV2 with ImageNet\nIn 2019, a decade after ImageNet [5] was collected, Recht et\nal. introduced ImageNetV2 [35], which attempted to mirror\nthe original ImageNet collection process, including restrict-\ning data to images uploaded in a similar timeframe. How-\never, models trained on ImageNet showed a consistent 11-\n6\nClass\nMore True for ImageNetV2\nDining Table\nPeople posing for a picture\nWig\nClose up views of dolls\nHand-held Computer\nApps like Twitter and Whatsapp\nPalace\nEast Asian architecture\nPier\nBody of water at night\nSchnauzer\nBlack dogs in different settings\nPug\nPhotos possibly taken on Instagram\nHorizontal Bar\nMen\u2019s gymnastics events\nTable 3. Top per-class differences between ImageNet and V2.\n14% accuracy drop on ImageNetV2, and the reasons for this\nhave remained unclear. While some studies have employed\nstatistical tools to reveal a distributional difference between\nImageNet and ImageNetV2 [10], we aim to discover more\ninterpretable differences between these two datasets.\nTo uncover their differences, we first ran VisDiff with\nDA as all of ImageNetV2 images and DB as all of Ima-\ngeNet images. Interestingly, the highest scoring descrip-\ntion generated by our system is \u201cphotos taken from Insta-\ngram\u201d. We conjecture that this highlights temporal distribu-\ntion shift as a potential reason behind model performance\ndrops on ImageNetV2 vs V1. Indeed, while ImageNetV2\naimed to curate images uploaded in a similar timeframe to\nImageNet, all images in ImageNet were collected prior to\n2012 whereas a portion of ImageNetV2 was collected be-\ntween 2012 and 2014 [35]. This shift in years happens to\ncoincide with the explosion of social media platforms such\nas Instagram, which grew from 50M users in 2012 to 300M\nusers in 2014 [7]. In this case, we hypothesize that a small\ndifference in the time range had a potentially outsized im-\npact on the prevalence of Instagram-style photos in Ima-\ngeNetV2 and the performance of models on this dataset.\nBeyond dataset-level analysis, we applied VisDiff to\neach of the 1,000 ImageNet classes, comparing Ima-\ngeNetV2 images (DA) against ImageNet images (DB). No-\ntable class-specific differences are listed in Table 3, ranked\nby difference score, with visualizations in Figure 13. Sev-\neral of these differences suggest more specific examples\nof Instagram-style photos, consistent with our dataset-level\nfinding. For example, for the class \u201cDining Table\u201d, Ima-\ngeNetV2 contains substantially more images showing \u201cpeo-\nple posing for a picture\u201d, visualized in Figure 1. For the\nclass \u201cHorizontal Bar\u201d, ImageNetV2 is also identified to\nhave more images of \u201cmen\u2019s gymnastics events.\u201d Upon\nmanual inspection, we find that this highlights the differ-\nence that ImageNetV2 happens to contain photographs of\nthe Men\u2019s High Bar gymnastics event in the 2012 Olympics,\nwhich occurred after the ImageNet collection date. These\nexamples illustrate how VisDiff can be used as a tool for\nsurfacing salient differences between datasets.\n6.2. Comparing Behaviors of CLIP and ResNet\nIn 2021, OpenAI\u2019s CLIP [34] showcased impressive zero-\nshot object recognition, matching the fully supervised\nClass\nAccC\nAccR\nMore Correct for CLIP\nTobacco Shop\n0.96\n0.50\nSign hanging from the side of a building\nDigital Watch\n0.88\n0.52\nWatches displayed in a group\nMissile\n0.78\n0.42\nPeople posing with large missiles\nPot Pie\n0.98\n0.66\nComparison of food size to coins\nToyshop\n0.92\n0.60\nPeople shopping in store\nTable 4. Top per-class differences between CLIP and ResNet.\nAccC and AccR are accuracy of CLIP and ResNet, respectively.\nResNet [13] in ImageNet accuracy while showing a smaller\nperformance drop on ImageNetV2.\nDespite similar in-\ndistribution performance on ImageNet, CLIP and ResNet\ndiffer in robustness [29]. This naturally leads to two ques-\ntions: 1) do these models make similar predictions on indi-\nvidual datapoints in ImageNet? 2) on what datapoints does\nCLIP perform better than ResNet in ImageNetV2?\nTo investigate these questions, we analyzed ResNet-50\nand zero-shot CLIP ViT-H, which achieve similar accu-\nracies of 75% and 72% on ImageNet, respectively.\nTo\nstudy the first question, VisDiff was applied to the top 100\nclasses where CLIP surpasses ResNet. DA comprised im-\nages correctly identified by CLIP but not by ResNet, and\nDB included all other images. The top discoveries included\n\u201cclose-ups of everyday objects\u201d, \u201cbrands and specific prod-\nuct labeling\u201d, and \u201cpeople interacting with objects\u201d. The\nfirst two align well with existing works that show CLIP\nis robust to object angles and sensitive to textual elements\n(e.g., a fruit apple with text \u201ciPod\u201d on it will be misclassified\nas \u201ciPod\u201d) [12, 34]. In addition, we ran VisDiff at finer gran-\nularity on each of the top 5 classes where CLIP outperforms\nResNet. The discovered class-level differences are shown\nin Table 4, demonstrating CLIP\u2019s proficiency in identify-\ning \u201ctobacco shops with signs\u201d, \u201cgroup displays of digital\nwatches\u201d, and \u201cscenes involving missiles and toyshops with\nhuman interactions\u201d, which echos the dataset-level findings\nabout label, object angle, and presence of people.\nTo study the second question, we applied VisDiff to\nImageNetV2\u2019s top 100 classes where CLIP outperforms\nResNet. We set DA as images where CLIP is correct and\nResNet is wrong, and DB as the rest. The top three dif-\nferences are: 1) \u201cInteraction between humans and objects\u201d,\nsuggesting CLIP\u2019s robustness in classifying images with hu-\nman presence; 2) \u201cDaytime outdoor environments\u201d, indicat-\ning CLIP\u2019s temporal robustness; and 3) \u201cGroup gatherings\nor social interactions\u201d, which is similar to the first differ-\nence. These findings provide insight into CLIP\u2019s strengths\nversus ResNet on ImageNetV2, and are also consistent with\nthe findings in Section 6.1 that ImageNetV2 contains more\nsocial media images with more presence of people.\n6.3. Finding Failure Modes of ResNet\nWe utilize VisDiff to identify failure modes of a model\nby contrasting images that are correctly predicted against\nthose that are erroneously classified. Using a ResNet-50 and\nResNet-101 [13] trained on ImageNet, we set DA as Ima-\n7\nSDv2\nSDv1\nFigure 5. StableDiffusionV2 vs. V1 generated images. For the same prompt, StableDiffusionV2 images often contain more \u201cvibrant\ncontrasting colors\u201d and \u201cartworks placed on stands or in frames\u201d. Randomly sampled images can be found in Figure 16.\nFigure 6. Accuracy on images with / without people.\ngeNet images misclassified by both ResNet-50 and ResNet-\n101 and DB as correctly classified images. The two high-\nest scoring descriptions were \u201chumanized object items\u201d and\n\u201cpeople interacting with objects\u201d, suggesting that ResNet\nmodels perform worse when the images include human sub-\njects, echoing the finding in Section 6.2.\nTo validate this hypothesis, we applied a DETR [36] ob-\nject detector to find a subset of ImageNet images with hu-\nman presence. Using the classes which have a roughly equal\nnumber of human/no-human images, we evaluated ResNets\non this subset and their accuracy indeed declined 3-4%, as\nshown in Figure 6.\n6.4. Comparing Versions of Stable Diffusion\nIn 2022, Stability AI released StableDiffusionV1 (SDv1),\nfollowed by StableDiffusionV2 (SDv2) [38]. While SDv2\ncan be seen as an update to SDv1, it raises the question:\nWhat are the differences in the images produced by these\ntwo models?\nUsing the prompts from PartiPrompts [47] and Diffu-\nsionDB [45], we generated 1634 and 10000 images with\nSDv2 and SDv1, respectively. The Parti images are used to\npropose differences and the DiffusionDB images are used\nto validate these differences transfer to unseen prompts.\nThe top differences show that SDv2 produces more \u201cvi-\nbrant and contrasting colors\u201d and interestingly \u201cimages with\nframes or borders\u201d (see Table 7). We confirmed the color\ndifference quantitatively by computing the average satura-\ntion: 112.61 for SDv2 versus 110.45 for SDv1 from Par-\ntiPrompts, and 97.96 versus 93.49 on unseen DiffusionDB\nimages. Qualitatively, as shown in Section Figure 5, SDv2\nfrequently produces images with white borders or frames,\na previously unknown characteristic. This is further sub-\nstantiated in Section Appendix D, where we employ edge\nFigure 7.\nMemorable(top) vs.\nforgettable(bottom) images.\nMemorable images contain more \u201chumans\u201d, \u201cclose-up views of\nbody part or objects\u201d, and \u201chumorous settings\u201d, while forgettable\nimages contain more \u201clandscapes\u201d and \u201curban environments\u201d\ndetection to quantify white borders, providing 50 random\nimage samples from both SDv1 and SDv2.\n6.5. Describing Memorability in Images\nFinally, we demonstrate the applicability of VisDiff in\naddressing diverse real-world questions beyond machine\nlearning, such as computational cognitive science. A key\narea of interest, especially for photographers and advertis-\ners, is enhancing image memorability. Isola et al. [16] ex-\nplored this question and created the LaMem dataset, where\neach image is assigned a memorability score by humans in\nthe task of identifying repeated images in a sequence.\nApplying VisDiff to the LaMem dataset, we divided im-\nages into two groups: DA (the most memorable 25th per-\ncentile) and DB (the least memorable 25th percentile). Our\nanalysis found that memorable images often include \u201cpres-\nence of humans\u201d, \u201cclose-up views\u201d, and \u201chumorous set-\ntings\u201d, while forgettable ones feature \u201clandscapes\u201d and \u201cur-\nban environments\u201d. These findings are consistent with those\nof Isola et al. [16], as further detailed qualitatively in Fig-\nure 7 and quantitatively in Appendix D.\n7. Conclusion\nIn this work, we introduce the task of set difference caption-\ning and develop VisDiff, an algorithm designed to identify\nand describe differences in image sets in natural language.\nVisDiff first uses large language models to propose differ-\nences based on image captions and then employs CLIP to\neffectively rank these differences. We rigorously evaluate\nVisDiff\u2019s various design choices on our curated VisDiff-\nBench, and show VisDiff\u2019s utility in finding interesting in-\nsights across a variety of real-world applications.\n8\nLimitations.\nAs we see in Section 5, VisDiff still has a\nlarge room for improvement and hence far from guaranteed\nto uncover all meaningful differences. Furthermore, VisDiff\nis meant to be an assistive tool for humans to better under-\nstand their data and should not be applied without a human\nin the loop: the users hold the ultimate responsibility to in-\nterpret the descriptions by VisDiff properly. As VisDiff re-\nlies heavily on CLIP, GPT, and BLIP, any biases or errors\nthese models may extend to VisDiff. Further investigation\nof VisDiff\u2019s failure cases can be found in Appendix E.\nAcknowledgements.\nWe thank James Zou, Weixin Liang, Jeff Z. HaoChen, Jen\nWeng, Zeyu Wang, Jackson Wang, Elaine Sui, Ruocheng\nWang for providing valuable feedback to this project. We\nalso thank Dan Klein for providing feedback on the ab-\nstract and intro as well as Esau Hutcherson for running\npreliminary experiments on VisDiffBench.\nLastly, we\nthank Alexei Efros for proposing several dozen applica-\ntions, including the memorability experiment referenced\nin Section 6, for providing relevant related works, and\nfor grudgingly acknowledging that the task of set differ-\nence captioning is \u201ccool, even though it has language\u201d.\nThis work was supported in part by the NSF CISE Expe-\nditions Award (CCF-1730628).\nTrevor Darrell and Lisa\nDunlap were supported by DoD and/or BAIR Industrial\nfunds.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022. 2\n[2] Shizhen Chang and Pedram Ghamisi. Changes to captions:\nAn attentive network for remote sensing change captioning.\nTIP, 2023. 2\n[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao\nWu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao\nZhuang, Joseph E Gonzalez, et al. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality. Techni-\ncal Report, 2023. 5\n[4] Yeounoh Chung, Tim Kraska, Neoklis Polyzotis, Ki Hyun\nTae, and Steven Euijong Whang.\nAutomated data slicing\nfor model validation:a big data - ai integration approach. In\nICDE, 2019. 3\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 2, 6\n[6] Greg d\u2019Eon, Jason d\u2019Eon, James R. Wright, and Kevin\nLeyton-Brown. The spotlight: A general method for discov-\nering systematic errors in deep learning models. In FAccT,\n2021. 3\n[7] by: Power Digital.\nInstagram algorithm change history,\n2018. 7\n[8] Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic,\nand Alexei A. Efros. What makes paris look like paris? In\nSIGGRAPH, 2012. 3\n[9] Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto.\nAlpacafarm: A simulation\nframework for methods that learn from human feedback.\narXiv preprint arXiv:2305.14387, 2023. 4\n[10] Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris\nTsipras, Jacob Steinhardt, and Aleksander Madry. Identify-\ning statistical bias in dataset replication. In ICML, 2020. 7\n[11] Sabri Eyuboglu, Maya Varma, Khaled Kamal Saab, Jean-\nBenoit Delbrouck, Christopher Lee-Messer, Jared Dunnmon,\nJames Zou, and Christopher Re. Domino: Discovering sys-\ntematic errors with cross-modal embeddings. In ICLR, 2022.\n1, 3\n[12] Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter,\nMichael Petrov, Ludwig Schubert, Alec Radford, and Chris\nOlah. Multimodal neurons in artificial neural networks. Dis-\ntill, 2021. 7\n[13] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 2, 7\n[14] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kada-\nvath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu,\nSamyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt,\nand Justin Gilmer. The many faces of robustness: A criti-\ncal analysis of out-of-distribution generalization. In ICCV,\n2021. 3, 11\n[15] Evan Hernandez, Sarah Schwettmann, David Bau, Teona\nBagashvili, Antonio Torralba, and Jacob Andreas. Natural\nlanguage descriptions of deep visual features. In ICLR, 2021.\n3\n[16] Phillip Isola, Devi Parikh, Antonio Torralba, and Aude Oliva.\nUnderstanding the intrinsic memorability of images.\nIn\nNeurIPS, 2011. 2, 8\n[17] Phillip Isola, Jianxiong Xiao, Antonio Torralba, and Aude\nOliva. What makes an image memorable? In CVPR, 2011.\n1\n[18] Saachi Jain, Hannah Lawrence, Ankur Moitra, and Alek-\nsander Madry. Distilling model failures as directions in latent\nspace. In ICLR, 2023. 1\n[19] Hoeseong Kim, Jongseok Kim, Hyungseok Lee, Hyunsung\nPark, and Gunhee Kim. Viewpoint-agnostic change caption-\ning with cycle consistency. In ICCV, 2021. 2\n[20] Pang\nWei\nKoh,\nShiori\nSagawa,\nHenrik\nMarklund,\nSang Michael Xie, Marvin Zhang, Akshay Balsubramani,\nWeihua Hu, Michihiro Yasunaga, Richard Lanas Phillips,\nIrena Gao, et al.\nWilds:\nA benchmark of in-the-wild\ndistribution shifts. In ICML, 2021. 1\n[21] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi\nPu, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Mimic-\nit: Multi-modal in-context instruction tuning. arXiv preprint\narXiv:2306.05425, 2023. 2\n[22] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n9\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 4, 5\n[24] Weixin Liang and James Zou.\nMetashift: A dataset of\ndatasets for evaluating contextual distribution shifts and\ntraining conflicts. In ICLR, 2022. 3\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 4, 5\n[26] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,\nMichele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in\nthe middle: How language models use long contexts. arXiv\npreprint arXiv:2307.03172, 2023. 16\n[27] Scott M Lundberg and Su-In Lee.\nA unified approach to\ninterpreting model predictions. In NeurIPS, 2017. 3\n[28] Lev Manovich.\nHow to Compare One Million Images?,\npages 249\u2013278. Palgrave Macmillan UK, London, 2012. 3\n[29] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori\nSagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, Yair\nCarmon, and Ludwig Schmidt. Accuracy on the line: on\nthe strong correlation between out-of-distribution and in-\ndistribution generalization. In ICML, 2021. 7\n[30] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 2, 6\n[31] Dong Huk Park, Trevor Darrell, and Anna Rohrbach. Robust\nchange captioning. In ICCV, 2019. 2\n[32] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume\nLeclerc, and Aleksander Madry.\nTrak: Attributing model\nbehavior at scale. In ICML, 2023. 3\n[33] Joaquin Quinonero-Candela, Masashi Sugiyama, Anton\nSchwaighofer, and Neil D Lawrence. Dataset shift in ma-\nchine learning. Mit Press, 2008. 1\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2, 5, 6, 7\n[35] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and\nVaishaal Shankar. Do imagenet classifiers generalize to im-\nagenet? In ICML, 2019. 2, 6, 7\n[36] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In CVPR, 2016. 8\n[37] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.\n\u201dwhy should i trust you?\u201d explaining the predictions of any\nclassifier. In KDD, 2016. 3\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 8\n[39] Harshay Shah, Sung Min Park, Andrew Ilyas, and Alek-\nsander Madry. Modeldiff: A framework for comparing learn-\ning algorithms. In ICML, 2023. 3\n[40] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales,\nDavid Dohan, Ed Chi, Nathanael Sch\u00a8arli, and Denny Zhou.\nLarge language models can be easily distracted by irrelevant\ncontext. arXiv preprint arXiv:2302.00093, 2023. 16\n[41] Antonio Torralba and Alexei Efros. Unbiased look at dataset\nbias. In CVPR, 2011. 3\n[42] Nanne van Noord. Prototype-based dataset comparison. In\nICCV, 2023. 3\n[43] Joshua Vendrow, Saachi Jain, Logan Engstrom, and Alek-\nsander Madry. Dataset interfaces: Diagnosing model failures\nusing controllable counterfactual generation. arXiv preprint\narXiv:2302.07865, 2023. 3, 11\n[44] Angelina Wang, Arvind Narayanan, and Olga Russakovsky.\nREVISE: A tool for measuring and mitigating bias in visual\ndatasets. In ECCV, 2020. 3\n[45] Zijie J. Wang, Evan Montoya, David Munechika, Haoyang\nYang, Benjamin Hoover, and Duen Horng Chau.\nDiffu-\nsionDB: A large-scale prompt gallery dataset for text-to-\nimage generative models. arXiv preprint arXiv:2210.14896,\n2022. 8\n[46] Linli Yao, Weiying Wang, and Qin Jin. Image difference cap-\ntioning with pre-training and contrastive learning. In AAAI,\n2022. 2\n[47] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation.\nTMLR,\n2022. 8\n[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan\nLi, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-\nlez, and Ion Stoica. Judging llm-as-a-judge with mt-bench\nand chatbot arena. In NeurIPS Datasets and Benchmarks,\n2023. 4\n[49] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt.\nDescribing differences between text distributions with natu-\nral language. In ICML, 2022. 2, 3, 4\n[50] Ruiqi Zhong, Peter Zhang, Steve Li, Jinwoo Ahn, Dan Klein,\nand Jacob Steinhardt.\nGoal driven discovery of distribu-\ntional differences via language descriptions. arXiv preprint\narXiv:2302.14233, 2023. 2, 3, 4\n[51] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva,\nand Antonio Torralba. Learning deep features for discrimi-\nnative localization. In CVPR, 2016. 3\n[52] Jun-Yan Zhu, Yong Jae Lee, and Alexei A Efros. Averageex-\nplorer: Interactive exploration and alignment of visual data\ncollections. In SIGGRAPH, 2014. 3\n[53] Zhiying Zhu, Weixin Liang, and James Zou.\nGsclip: A\nframework for explaining distribution shifts in natural lan-\nguage. In ICML DataPerf Workshop, 2022. 1, 2, 3\n10\nDescribing Differences in Image Sets with Natural Language\nSupplementary Material\nReproducibility Statement\nWe\nprovide\ncode\nimplementations\nof\nVisDiff\nat\nhttps : / / github . com / Understanding -\nVisual-Datasets/VisDiff. We also provide Vis-\nDiffBench at https://drive.google.com/file/\nd/1vghFd0rB5UTBaeR5rdxhJe3s7OOdRtkY. The\nimplementations and datasets will enable researchers to\nreproduce all the experiments described in the paper as\nwell as run their own analyses on additional datasets.\nEthics Statement\nIn this work, we introduce VisDiff, a novel method designed\nto discern subtle differences between two sets of images.\nVisDiff represents not just a technical advance in the analy-\nsis of image sets, but also serves as a useful tool to promote\nfairness, diversity, and scientific discovery in AI and data\nscience. First, VisDiff has the potential to uncover biases\nin datasets. For instance, comparing image sets of workers\nfrom diverse demographic groups, such as men and women,\ncan reveal and quantify career stereotypes associated with\neach group. This capability is pivotal in addressing and\nmitigating biases present in datasets. Furthermore, VisD-\niff holds substantial promise for scientific discovery. By\ncomparing image sets in various scientific domains, such as\ncellular images from patients and healthy individuals, Vis-\nDiff can unveil novel insights into the disease impacts on\ncellular structures, thereby driving forward critical advance-\nments in medical research. However, VisDiff is meant to be\nan assistive tool and should be applied with humans in the\nloop. The users are responsible for interpreting the results\nproperly and avoiding misinformation. In summary, VisD-\niff emerges as a crucial tool for ethical AI considerations,\nfostering fairness and catalyzing scientific progress.\nTable of Contents\nIn this supplementary material, we provide additional de-\ntails of datasets, methods, results, and applications.\n\u2022 In Appendix A, we provide examples of our benchmark\nVisDiffBench and prompts to generate and evaluate this\nbenchmark.\n\u2022 In Appendix B, we provide additional details of each pro-\nposer and ranker.\n\u2022 In Appendix C, we ablate various design choices of our\nalgorithm VisDiff.\n\u2022 In Appendix D, we provide supplementary evidence of\nfindings for each application.\n\u2022 In Appendix E, we explain more failure cases and limita-\ntions of VisDiff.\nA. Supplementary Section 3\nIn this section, we provide additional details of Section 3 in\nthe main paper.\nA.1. Paired Sentences for VisDiffBench\nVisDiffBench contains five subsets:\nPairedImageSets-\nEasy,\nPairedImageSets-Medium,\nPairedImageSets-Hard,\nImageNetR, and ImageNet\u2217.\nWe provide all the paired\nsentences of PairedImageSets in Table 5. For ImageNetR,\nDA is one of the \u201cart\u201d, \u201ccartoon\u201d, \u201cdeviantart\u201d, \u201cembroi-\ndery\u201d, \u201cgraffiti\u201d, \u201cgraphic\u201d, \u201corigami\u201d, \u201cpainting\u201d, \u201csculp-\nture\u201d, \u201csketch\u201d, \u201csticker\u201d, \u201ctattoo\u201d, \u201ctoy\u201d, \u201cvideogame\u201d,\nand DB is \u201cimagenet\u201d. For ImageNet\u2217, DA is one of the\n\u201cin the forest\u201d, \u201cgreen\u201d, \u201cred\u201d, \u201cpencil sketch\u201d, \u201coil paint-\ning\u201d, \u201corange\u201d, \u201con the rocks\u201d, \u201cin bright sunlight\u201d, \u201cperson\nand a\u201d, \u201cin the beach\u201d, \u201cstudio lighting\u201d, \u201cin the water\u201d, \u201cat\ndusk\u201d, \u201cin the rain\u201d, \u201cin the grass\u201d, \u201cyellow\u201d, \u201cblue\u201d, \u201cand\na flower\u201d, \u201con the road\u201d, \u201cat night\u201d, \u201cembroidery\u201d, \u201cin the\nfog\u201d, \u201cin the snow\u201d, and DB is \u201cbase\u201d.\nA.2. Examples for VisDiffBench\nWe\nprovide\n4\nexamples\nfor\nPairedImageSets-Easy,\nPairedImageSets-Medium, PairedImageSets-Hard, respec-\ntively, in Figure 8 and Figure 17.\nFor ImageNetR and\nImageNet\u2217, we refer readers to the original papers [14, 43].\nA.3. Prompts for VisDiffBench Generation\nWe provide the GPT-4 prompt we used to generate paired\nsentences for PairedImageSets in Figure 9 (top).\nA.4. Prompts for VisDiffBench Evaluation\nWe provide the GPT-4 prompt we used to evaluate the gen-\nerated difference description against the ground-truth dif-\nference description in Figure 9 (bottom).\nB. Supplementary Section 4\nIn this section, we provide additional details of Section 4 in\nthe main paper.\nB.1. Details for Proposer\nWe ran each proposer for 3 rounds. For each round, we\nsample 20 examples per set and generate 10 hypotheses.\n11\nEasy (50 Paired Sets)\nMedium (50 Paired Sets)\nHard (50 Paired Sets)\nSet A\nSet B\nSet A\nSet B\nSet A\nSet B\nDogs playing in a park\nCats playing in a park\nSUVs on a road\nSedans on a road\nSunrise over Santorini, Greece\nSunset over Santorini, Greece\nChildren playing soccer\nChildren swimming in a pool\nWooden chairs in a room\nPlastic chairs in a room\nPeople practicing yoga in a moun-\ntainous setting\nPeople meditating in a mountain-\nous setting\nSnow-covered mountains\nDesert sand dunes\nGolden retriever dogs playing\nLabrador dogs playing\nFresh sushi with salmon topping\nFresh sushi with tuna topping\nButterflies on flowers\nBees on flowers\nGreen apples in a basket\nRed apples in a basket\nLush vineyards in spring\nLush vineyards in early autumn\nPeople shopping in a mall\nPeople dining in a restaurant\nLeather shoes on display\nCanvas shoes on display\nMen wearing Rolex watches\nMen wearing Omega watches\nElephants in the savannah\nGiraffes in the savannah\nFreshwater fish in an aquarium\nSaltwater fish in an aquarium\nCupcakes topped with buttercream\nCupcakes topped with fondant\nBirds flying in the sky\nAirplanes flying in the sky\nSteel bridges over a river\nWooden bridges over a river\nPeople playing chess outdoors\nPeople playing checkers outdoors\nBoats in a marina\nCars in a parking lot\nMountain bikes on a trail\nRoad bikes on a road\nHand-painted porcelain plates\nHand-painted ceramic plates\nTulips in a garden\nRoses in a garden\nCeramic mugs on a shelf\nGlass mugs on a shelf\nCyclists in a time-trial race\nCyclists in a mountain stage race\nPeople skiing on a slope\nPeople snowboarding on a slope\nPeople playing electric guitars\nPeople playing acoustic guitars\nGardens\nwith\nJapanese\ncherry\nblossoms\nGardens with Japanese maples\nFish in an aquarium\nTurtles in an aquarium\nLaptop computers on a desk\nDesktop computers on a desk\nPeople wearing traditional Korean\nhanboks\nPeople\nwearing\ntraditional\nJapanese kimonos\nBooks on a shelf\nPlants on a shelf\nHardcover books on a table\nPaperback books on a table\nAlpine lakes in summer\nAlpine lakes in early spring\nGrapes in a bowl\nApples in a bowl\nDigital clocks on a wall\nAnalog clocks on a wall\nMerlot wine in a glass\nCabernet Sauvignon wine in a\nglass\nMotorcycles on a street\nBicycles on a street\nChildren playing with toy cars\nChildren playing with toy trains\nFootball players in defensive for-\nmation\nFootball players in offensive for-\nmation\nCows grazing in a field\nSheep grazing in a field\nWhite roses in a vase\nPink roses in a vase\nClassic novels from the 19th cen-\ntury\nModern novels from the 21st cen-\ntury\nBabies in cribs\nBabies in strollers\nElectric stoves in a kitchen\nGas stoves in a kitchen\nOrchestras playing Baroque music\nOrchestras playing Classical music\nHot air balloons in the air\nKites in the air\nLeather jackets on hangers\nDenim jackets on hangers\nMen in British army uniforms from\nWWI\nMen in British army uniforms from\nWWII\nPenguins in the snow\nSeals in the snow\nPeople eating with chopsticks\nPeople eating with forks\nSculptures from the Renaissance\nera\nSculptures from the Hellenistic era\nLions in a jungle\nMonkeys in a jungle\nPearl necklaces on display\nGold necklaces on display\nPeople preparing macarons\nPeople preparing meringues\nWatches on a display\nRings on a display\nMushrooms in a forest\nFerns in a forest\nFemale ballet dancers in pointe\nshoes\nFemale ballet dancers in ballet\nslippers\nPizzas in a box\nDonuts in a box\nStainless steel kettles in a store\nPlastic kettles in a store\nDishes from Northern Italian cui-\nsine\nDishes from Southern Italian cui-\nsine\nBricks on a wall\nTiles on a wall\nPorcelain vases on a shelf\nMetal vases on a shelf\nClassic rock bands performing\nAlternative rock bands performing\nPianos in a room\nGuitars in a room\nVintage cars on a road\nModern cars on a road\nHistorical films set in Medieval\nEurope\nHistorical films set in Ancient\nRome\nTrains on tracks\nBuses on roads\nHandmade quilts on a bed\nFactory-made blankets on a bed\nBonsai trees shaped in cascade\nstyle\nBonsai trees shaped in informal\nupright style\nPots on a stove\nPlates on a table\nShiny silk dresses on mannequins\nMatte cotton dresses on man-\nnequins\nLace wedding dresses\nSatin wedding dresses\nStars in the night sky\nClouds in the day sky\nMechanical pencils on a desk\nBallpoint pens on a desk\nBirds with iridescent plumage\nBirds with matte plumage\nSunflowers in a field\nWheat in a field\nGinger cats lying down\nTabby cats lying down\nWomen wearing matte lipstick\nWomen wearing glossy lipstick\nDolls on a shelf\nTeddy bears on a shelf\nPeople riding racing horses\nPeople riding dressage horses\nCities with Gothic architecture\nCities with Modernist architecture\nPine trees in a forest\nOak trees in a forest\nSteel water bottles on a table\nGlass water bottles on a table\nPoems written in free verse\nPoems written in sonnet form\nMen playing basketball\nWomen playing volleyball\nMen wearing leather gloves\nMen wearing wool gloves\nAcoustic guitars being played\nClassical guitars being played\nIce cream in a cone\nJuice in a glass\nRubber ducks in a tub\nPlastic boats in a tub\nBooks with hardcover binding\nBooks with leather-bound covers\nDancers on a stage\nSingers on a stage\nPorcelain tea cups on a tray\nGlass tea cups on a tray\nPortraits painted in cubist style\nPortraits painted in impressionist\nstyle\nRainbows in the sky\nLightning in the sky\nSparrows on a tree\nCanaries on a tree\nResidential buildings in Art Deco\nstyle\nResidential buildings in Brutalist\nstyle\nTowers in a city\nHouses in a suburb\nShiny metallic cars\nMatte finish cars\nMale professional swimmers in\nfreestyle race\nMale professional swimmers in\nbutterfly race\nFrogs by a pond\nDucks by a pond\nStuffed teddy bears on a bed\nStuffed bunny rabbits on a bed\nBasketball players attempting free\nthrows\nBasketball players attempting slam\ndunks\nFootball players on a field\nRugby players on a field\nRound dinner plates on a table\nSquare dinner plates on a table\nCakes decorated with marzipan\nCakes decorated with buttercream\nroses\nPillows on a bed\nBlankets on a bed\nButter on a slice of bread\nJam on a slice of bread\nPeople practicing the Sun Saluta-\ntion in yoga\nPeople practicing the Tree Pose in\nyoga\nDeer in a forest\nRabbits in a forest\nBengal cat in sitting posture\nSiamese cat in sitting posture\nMen wearing suits\nMen wearing tuxedos\nTea in a cup\nCoffee in a cup\nViolinists playing in a quartet\nCellists playing in a quartet\nButterflies with spotted wings\nButterflies with striped wings\nChildren on a slide\nChildren on a swing\nGothic cathedrals in Europe\nBaroque churches in Europe\nOak trees in summer\nOak trees in autumn\nKangaroos in a desert\nCamels in a desert\nPeople dancing tango\nPeople dancing waltz\nTennis shoes on a rack\nRunning shoes on a rack\nTomatoes in a basket\nEggs in a basket\nAbstract oil paintings with warm\ncolors\nAbstract oil paintings with cool\ncolors\nPeople playing classical violin\nPeople playing fiddle\nPeople in an elevator\nPeople on an escalator\nCandies made from dark chocolate\nCandies made from milk chocolate\nMen wearing fedoras\nMen wearing baseball caps\nSandcastles on a beach\nUmbrellas on a beach\nRivers in tropical rainforests\nRivers in alpine meadows\nPassenger planes in the sky\nCargo planes in the sky\nMice in a barn\nHorses in a barn\nCars from the 1960s\nCars from the 1980s\nWomen wearing ankle boots\nWomen wearing knee-high boots\nChocolates in a box\nCandies in a jar\nSeascapes during a storm\nSeascapes during a calm day\nDiesel trucks on a highway\nElectric trucks on a highway\nZebra crossings on a street\nTraffic lights on a street\nFruits arranged in a still life setting\nFlowers arranged in a still life set-\nting\nChildren reading comic books\nChildren reading fairy tales\nBridges over a river\nBoats on a river\nDishes from Thai cuisine\nDishes from Vietnamese cuisine\nMen wearing round glasses\nMen wearing square glasses\nOranges on a tree\nBird nests on a tree\nWild horses in American plains\nWild zebras in African savannahs\nVinyl records in a store\nCDs in a store\nLanterns in a festival\nFireworks in a festival\nClassic movies in black and white\nClassic movies in Technicolor\nBonsai trees in pots\nCacti in pots\nTable 5. Paired sentences for PairedImageSets. Easy, medium, and hard examples are shown in the left, middle, and right.\nImage-based Proposer.\nWe provide an example input of\nthe gridded image in Figure 10. We feed the image and\nthe prompt shown in Figure 11 (middle) to LLaVA-1.5 to\ngenerate 10 hypotheses.\nFeature-based Proposer.\nTo generate 10 hypotheses, we\nsample BLIP-2 10 times using top-p sampling given the\nsubtracted embedding.\nCaption-based Proposer.\nWe generate captions for each\nimage using BLIP-2 with the prompt \u201cDescribe this im-\nage in detail.\u201d. We apply a simple filtering of the captions,\nwhich removes any special tokens and captions simply re-\npeating words. We feed the filtered captions and the prompt\nshown in Figure 11 (top) to GPT-4 to generate 10 hypothe-\nses.\nB.2. Details for Ranker\nImage-based Ranker.\nGiven each hypothesis, we prompt\nLLaVA-1.5 with the image and the prompt \u201cDoes this image\ncontain {hypothesis}?\u201d.\nCaption-based Ranker.\nGiven each hypothesis,\nwe\nprompt Vicuna-1.5 with the image caption and hypothesis\nusing the prompt shown in Figure 11 (bottom).\n12\nPenguins in the \nsnow\nSeals in \nthe snow\nLevel: Easy\nTop Difference: penguins standing together    AUROC: 1.00    GPT-4 Score: 1.0\nPeople skiing on \na slope\nPeople \nsnowboarding \non a slope\nLevel: Easy\nTop Difference: groups of skiers    AUROC: 0.89    GPT-4 Score: 1.0\nRound dinner \nplates on a \ntable\nSquare dinner \nplates on a \ntable\nLevel: Medium\nTop Difference: round planes    AUROC: 0.87    GPT-4 Score: 1.0\nPeople dancing \ntango\nPeople dancing \nwaltz\nLevel: Medium\nTop Difference: tango dancing    AUROC: 0.98    GPT-4 Score: 1.0\nCyclists in a \ntime-trial race\nCyclists in a \nmountain stage \nrace\nLevel: Hard\nTop Difference: Cyclists participating in a time trial    AUROC: 0.90    GPT-4 Score: 1.0\nPoems written \nin free verse\nPoems written \nin sonnet form\nLevel: Hard\nTop Difference: free verse poetry    AUROC: 0.85    GPT-4 Score: 1.0\nFigure 8. PairedImageSets examples where VisDiff succeeds. We show the ground-truth difference, top difference predicted by VisDiff,\nAUROC score output by the ranker, and evaluation of the predicted difference by GPT-4.\n13\nI'm working on a project about explaining image distributional difference using natural language. The inputs are image set A and image set B, the output is a natural language description of \nthe most different features. \nHowever, I don't have datasets to evaluate the system. I'm going to crawl Google to collect images giving a sentence. Can you think about 50 paired sentences showing:\n- Easy level (compare super class, e.g., \"Dogs playing in a park\" vs \"Cats playing in a park\", \"Children playing soccer\" vs \"Children swimming in a pool)\n- Medium level difference (compare \ufb01ne-grained class, e.g., \"SUVs on a road\" vs \"Sedans on a road\", \"Wooden chairs in a room\" vs \"Plastic chairs in a room\"), \n- Dif\ufb01cult level (compare dif\ufb01cult, non-trivial differences, e.g., \"sunrise over Santorini, Greece\" vs \"Sunset over Santorini, Greece\", \"Bengal cat in sitting posture\" vs \"Siamese cat in sitting \nposture\")? \nFor each level, only includes one difference between two set (e.g., there are two differences between \"Mountains in winter\" vs \"Beaches in summer\", both scene and season, do not include \nthis). Give 50 outputs in jsonl format ```{\"set1\": str, \"set2\": str, \"difference\": str}```. Let us start with 50 easy examples.\nYou did a great job! Let's do 50 medium level\nGreat! Let's do 50 dif\ufb01cult examples\nI am a machine learning researcher summarizing differences in groups of images. The goal is to \ufb01nd a concept that is more true for Group A than Group B.\nGiven a description of Group A and Group B, output whether a given prediction aligns with the description of Group A. Answer with a 2 (fully aligned), 1 (somewhat aligned), or 0 (not aligned). a \nscore of 1 should be given if the prediction is more true for A than B, but is a superset or a subset of the most correct difference.\nFor example, if Group A is \"images of dogs in the snow\" and Group B is \"images of dogs next to cats\":\n    - predictions like \"dogs in the snow\" or \"dogs in winter time\" should be given a 2\n    - predictions like \"golden retrievers on a ski slope\" or \"animals in the snow\" should be given a 1\nHere is the descriptions\nGroup A: People riding racing horses and Group B: People riding dressage horses. Prediction: Horse racing events\nAgain, output either a 2, 1, or 0. Response:\nVisDiffBench Evaluation Prompt \nVisDiffBench Generation Prompt \nThis image contains two groups of images. 20 images from Group A are shown in the \ufb01rst two rows, while 20 images from Group B are shown in the last two rows.\nI am a machine learning researcher trying to \ufb01gure out the major differences between these two groups so I can better understand my data.\nCome up with 10 distinct concepts that are more likely to be true for Group A compared to Group B. Please write a list of captions (separated by bullet points \"*\"). For example:\n* \"a dog next to a horse\"\n* \"a car in the rain\"\n* \"low quality\"\n* \"cars from a side view\"\n* \"people in a intricate dress\"\n* \"a joyful atmosphere\"\nDo not list more than one concept. The hypothesis should be a caption, so hypotheses like \"more of ...\", \"presence of ...\", \"images with ...\" are incorrect. Also do not enumerate possibilities \nwithin parentheses. Here are examples of bad outputs and their corrections:\n* INCORRECT: \"various nature environments like lakes, forests, and mountains\" CORRECTED: \"nature\"\n* INCORRECT: \"images of household object (e.g. bowl, vacuum, lamp)\" CORRECTED: \"household objects\"\n* INCORRECT: \"Presence of baby animals\" CORRECTED: \"baby animals\"\n* INCORRECT: \"Different types of vehicles including cars, trucks, boats, and RVs\" CORRECTED: \"vehicles\"\n* INCORRECT: \"Images involving interaction between humans and animals\" CORRECTED: \"interaction between humans and animals\"\n* INCORRECT: \"More realistic images\" CORRECTED: \"realistic images\" \n* INCORRECT: \"Insects (cockroach, dragon\ufb02y, grasshopper)\" CORRECTED: \"insects\"\nAgain, I want to \ufb01gure out what kind of distribution shift are there. List properties that hold more often for the images in group A compared to group B. Answer with a list (separated by bullet \npoints \"*\"). Your response:\nImage-based Proposer Prompt \nThe following are the result of captioning two groups of images:\nGroup A: a group of jockeys and horses are racing on a green \ufb01eld\nGroup A: a cowboy riding a bucking horse at a rodeo\n\u2026\nGroup B: a person is riding a black horse in an arena\nGroup B: person riding a horse in an equestrian competition at the london 2012 olympics\n\u2026\nI am a machine learning researcher trying to \ufb01gure out the major differences between these two groups so I can better understand my data.\nCome up with 10 distinct concepts that are more likely to be true for Group A compared to Group B. Please write a list of captions (separated by bullet points \"*\"). For example:\n* \"a dog next to a horse\"\n* \"a car in the rain\"\n* \"low quality\"\n* \"cars from a side view\"\n* \"people in a intricate dress\"\n* \"a joyful atmosphere\"\nDo not talk about the caption, e.g., \"caption with one word\" and do not list more than one concept. The hypothesis should be a caption, so hypotheses like \"more of ...\", \"presence of ...\", \n\"images with ...\" are incorrect. Also do not enumerate possibilities within parentheses. Here are examples of bad outputs and their corrections:\n* INCORRECT: \"various nature environments like lakes, forests, and mountains\" CORRECTED: \"nature\"\n* INCORRECT: \"images of household object (e.g. bowl, vacuum, lamp)\" CORRECTED: \"household objects\"\n* INCORRECT: \"Presence of baby animals\" CORRECTED: \"baby animals\"\n* INCORRECT: \"Different types of vehicles including cars, trucks, boats, and RVs\" CORRECTED: \"vehicles\"\n* INCORRECT: \"Images involving interaction between humans and animals\" CORRECTED: \"interaction between humans and animals\"\n* INCORRECT: \"More realistic images\" CORRECTED: \"realistic images\" \n* INCORRECT: \"Insects (cockroach, dragon\ufb02y, grasshopper)\" CORRECTED: \"insects\"\nCaption-based Proposer Prompt \nFigure 9. Prompt used to generate paired sentences for VisDiffBench (top) and evaluate VisDiffBench (bottom). Input-dependent texts are\ncolored in blue.\nFigure 10. Example input to the image-based proposer. We ar-\nrange 20+20 input images into a single 4-row, 10-column gridded\nimage.\nFeature-based Ranker.\nWe use the OpenCLIP model\nViT-bigG-14 trained on laion2b s39b b160k.\nC. Supplementary Section 5\nIn this section, we provide additional details of Section 5 in\nthe main paper. We ablate various design choices of VisD-\niff.\nC.1. Caption Styles\nGiven that our leading proposer is caption-based, it nat-\nurally raises the question of how captions derived from\nvision language models influence performance. We con-\nducted a comparative analysis of captions generated by\ntwo state-of-the-art vision language models: BLIP-2 and\nLLaVA-1.5. Notably, compared to BLIP-2, LLaVA-1.5 has\nbeen instruction-tuned and can produce captions that are\nmuch longer with detailed information. The average cap-\ntion length for LLaVA is around 391 characters compared\nto BLIP-2\u2019s 41 characters. As shown in Table 6, despite the\nclear disparity between these two captions, the algorithm\nachieves similar performances. This suggests that language\nmodels possess a robust inductive reasoning ability that al-\nlows them to discern the most notable differences in lan-\nguage. BLIP-2\u2019s captions, being marginally superior, could\nbe attributed to their shortness and conciseness.\nC.2. Language Models\nWe compared GPT-4 with GPT-3.5 in Table 6 to assess\nhow different language models affect the caption-based pro-\nposer. While both models achieve strong performances on\nVisDiffBench, GPT-4 outperforms GPT-3.5 in most cases,\ndemonstrating that the stronger reasoning capability of lan-\nguage models is important to accomplish the set difference\ncaptioning task.\nC.3. Sampling Rounds\nThe proposer\u2019s generated differences rely on the random\nsamples drawn from each image set; thus, extensive sam-\npling is paramount to capture all the differences. Our ab-\nlation studies presented in Figure 12 (left), conducted on\nthe PairedImageSets hard subset, suggest that an increase in\n14\nGiven a caption and a concept, respond with yes or no.\nHere are 5 examples for the concept \"spider and a \ufb02ower\":\nINPUT: a spider sitting on top of a purple \ufb02ower\nOUTPUT: yes\nINPUT: a yellow and black spider with a web in the background\nOUTPUT: no\nINPUT: a arachnid with a white \ufb02ower\nOUTPUT: yes\nINPUT: a spider is walking on the ground in the grass\nOUTPUT: no\nINPUT: two yellow and black spiders\nOUTPUT: no\nHere are 6 examples for the concept \"an ipod in the forest\":\nINPUT: a smartphone in the forest\nOUTPUT: yes\nINPUT: a white apple ipad sitting on top of a wooden table\nOUTPUT: no\nINPUT: an ipod near some trees\nOUTPUT: yes\nINPUT: a smartphone with apps\nOUTPUT: no\nINPUT: a pink mp3 player sitting on top of a book\nOUTPUT: no\nINPUT: an ipod sitting on a white surface\nOUTPUT: no\nGiven the caption \"mario and luigi are playing tennis on a white background\" and the concept \u201creferences to pop culture\u201d, respond with either the word yes or no ONLY.\nOUTPUT:\nCaption-based Ranker Prompt \nThis image contains two groups of images. 20 images from Group A are shown in the \ufb01rst two rows, while 20 images from Group B are shown in the last two rows.\nI am a machine learning researcher trying to \ufb01gure out the major differences between these two groups so I can better understand my data.\nCome up with 10 distinct concepts that are more likely to be true for Group A compared to Group B. Please write a list of captions (separated by bullet points \"*\"). For example:\n* \"a dog next to a horse\"\n* \"a car in the rain\"\n\u2026 (same as caption-based proposer prompt)\n* INCORRECT: \"Insects (cockroach, dragon\ufb02y, grasshopper)\" CORRECTED: \"insects\"\nAgain, I want to \ufb01gure out what kind of distribution shift are there. List properties that hold more often for the images in group A compared to group B. Answer with a list (separated by bullet \npoints \"*\"). Your response:\nImage-based Proposer Prompt \nThe following are the result of captioning two groups of images:\nGroup A: a group of jockeys and horses are racing on a green \ufb01eld\nGroup A: a cowboy riding a bucking horse at a rodeo\n\u2026\nGroup B: a person is riding a black horse in an arena\nGroup B: person riding a horse in an equestrian competition at the london 2012 olympics\n\u2026\nI am a machine learning researcher trying to \ufb01gure out the major differences between these two groups so I can better understand my data.\nCome up with 10 distinct concepts that are more likely to be true for Group A compared to Group B. Please write a list of captions (separated by bullet points \"*\"). For example:\n* \"a dog next to a horse\"\n* \"a car in the rain\"\n* \"low quality\"\n* \"cars from a side view\"\n* \"people in a intricate dress\"\n* \"a joyful atmosphere\"\nDo not talk about the caption, e.g., \"caption with one word\" and do not list more than one concept. The hypothesis should be a caption, so hypotheses like \"more of ...\", \"presence of ...\", \n\"images with ...\" are incorrect. Also do not enumerate possibilities within parentheses. Here are examples of bad outputs and their corrections:\n* INCORRECT: \"various nature environments like lakes, forests, and mountains\" CORRECTED: \"nature\"\n* INCORRECT: \"images of household object (e.g. bowl, vacuum, lamp)\" CORRECTED: \"household objects\"\n* INCORRECT: \"Presence of baby animals\" CORRECTED: \"baby animals\"\n* INCORRECT: \"Different types of vehicles including cars, trucks, boats, and RVs\" CORRECTED: \"vehicles\"\n* INCORRECT: \"Images involving interaction between humans and animals\" CORRECTED: \"interaction between humans and animals\"\n* INCORRECT: \"More realistic images\" CORRECTED: \"realistic images\" \n* INCORRECT: \"Insects (cockroach, dragon\ufb02y, grasshopper)\" CORRECTED: \"insects\"\nAgain, I want to \ufb01gure out what kind of distribution shift are there. List properties that hold more often for the images (not captions) in group A compared to group B. Answer with a list \n(separated by bullet points \"*\"). Your response:\nCaption-based Proposer Prompt \nFigure 11. Prompt for caption-based proposer (top), image-based proposer (middle), and caption-based ranker (bottom). Input-dependent\ntexts are colored in blue.\n15\nProposer\nRanker\nPIS-Easy\nPIS-Medium\nPIS-Hard\nImageNet-R/*\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nAcc@1\nAcc@5\nGPT-4 on BLIP-2 Captions\nCLIP\n0.88\n0.99\n0.75\n0.86\n0.61\n0.80\n0.78\n0.96\nGPT-4 on LLaVA-1.5 Captions\nCLIP\n0.89\n0.98\n0.73\n0.85\n0.51\n0.70\n0.84\n0.93\nGPT-3.5 on BLIP-2 Captions\nCLIP\n0.81\n0.95\n0.67\n0.87\n0.60\n0.76\n0.85\n0.96\nTable 6. Results on VisDiffBench with different captions and language models. We bold any numbers within 0.02.\nFigure 12. Analysis of the number of rounds (left) and number of samples (right) for the proposer on 50 PairedImageSets-Hard sets. 3\nrounds and 20 samples are the best in terms of performance and efficiency.\nsampling iterations typically correlates with enhanced per-\nformance. However, a point of diminishing returns is ob-\nserved beyond three rounds of proposals. In this paper, we\nstandardize the experiments based on three rounds of pro-\nposal generation.\nC.4. Number of Sampled Examples\nInputting more samples from DA and DB into the proposer\nmay not be advantageous, as a long context could result\nin information getting lost in the middle [26, 40]. Results\nshown in Figure 12 (right) reflect this, as inputting more\ncaptions to the large language models sees performance\nbenefits up to 20 captions, at which point performance de-\ngrades.\nC.5. Necessity of Ranker\nSince the proposer may already generate and sort the most\nsalient difference descriptions based on the sampled im-\nages, we conduct ablations to understand whether the ranker\nis necessary.\nWe observe that, on PairedImageSets hard\nsubset, VisDiff achieves 0.54 Acc@1 and 0.68 Acc@5\nwithout ranker, which is much lower than the 0.61 Acc@1\nand 0.80 Acc@5 with ranker, demonstrating the necessity\nof the ranker.\nD. Supplementary Section 6\nIn this section, we provide additional details of Section 6 in\nthe main paper.\nD.1. Comparing ImageNetV2 with ImageNet\nPer-class visualizations.\nAlong with the \u201cDinner Table\u201d\nexample shown in Figure 1, we provide other per-class dif-\nferences with the highest difference scores in Figure 13.\nThese examples clearly reveal salient differences between\nImageNetV2 and ImageNet. Moreover, we observe time\ndifferences between these two datasets, as ImageNetV2\nimages contain Twitter and WhatsApp in the \u201cHand-held\nComputer\u201d class and London 2012 Oylmpics in the \u201cHori-\nzontal Bar\u201d class.\nImageNetV2 metadata analysis.\nTo get more precise\nstatistics on when the ImageNetV2 images were collected,\nwe analyzed the metadata of each image, which reports\nthe minimum and maximum upload date of that image to\nFlickr. We find that 72% images were uploaded between\n2012 and 2013, and 28% were uploaded between 2013 and\n2014. This is different from ImageNet images that were all\ncollected on or before 2010.\nD.2. Comparing Behaviors of CLIP and ResNet\nPer-class visualizations.\nWe provide per-class differ-\nences where CLIP outperforms ResNet most in Figure 14.\nThese examples clearly reveal salient differences between\nCLIP and ResNet, such as CLIP\u2019s robustness to label within\nimages, object angle, and presence of people.\nD.3. Finding Failure Modes of ResNet\nModel\ndetails.\nWe\nuse\nthe\nPyTorch\npre-trained\nResNet-50 and ResNet-101 models and the Hugging-\nface \u201cfacebook/detr-resnet-50\u201d object detector.\n16\n(a) Wig\nDiffs: \u201cClose up views of dolls\u201d, \u201cJapanese style dolls\u201d, \u201cImages including Barbie dolls\u201d\n(b) Hand-held Computer\nDiffs: \u201cApps like Twitter and Whatsapp\u201d, \u201cDigital devices with green screen\u201d, \u201cInterconnection between laptop and smart phone\u201d\n(c) Palace\nDiffs: \u201cEast Asian architecture\u201d, \u201cImages featuring the Forbidden City in Beijing\u201d, \u201cImages including red buildings with Chinese writing\u201d\n(d) Pier\nDiffs: \u201cBody of water at night\u201d,\u201cUrban night skyline\u201d, \u201cLong exposure shots\u201d\n(e) Schnauzer\nDiffs: \u201cBlack dogs in different settings\u201d, \u201cTerrier puppies with objects\u201d, \u201cInteraction with different objects\u201d\n(f) Horizontal Bar\nDiffs: \u201cMen\u2019s gymnastics events\u201d, \u201cLondon 2012 Olympics\u201d, \u201cGymnastics event in 2013\u201d\nFigure 13. ImageNetV2 vs. ImageNet. All V2 images are shown in the first row while V1 images are shown in the second row. We show\nthe class name and top 3 difference descriptions generated by VisDiff.\n17\n(a) Tobacco Shop\nDiffs: \u201cSign hanging from the side of a building\u201d, \u201cFalcon images\u201d,\u201cPresence of street signs\u201d\n(b) Digital Watch\nDiffs: \u201cWatches displayed in a group\u201d,\u201cArrangement of multiple watches\u201d,\u201cWatches with colored straps\u201d\n(c) Missile\nDiffs: \u201cPeople posing with large missiles\u201d,\u201cpeople posing with rockets\u201d,\u201cmissiles on display\u201d\n(d) Pot Pie\nDiffs: \u201cComparison of food size to coins\u201d,\u201cUtilization of cutting board\u201d,\u201cInclusion of beverages\u201d\n(e) Toy Shop\nDiffs: \u201cPeople shopping in store\u201d,\u201cFemale customer in store\u201d,\u201cSpecific location based scene\u201d\nFigure 14. CLIP vs. ResNet. All CLIP correctly classified but ResNet incorrectly classified images are shown in the first row while other\nimages are shown in the second row. We show the class name and top 3 difference descriptions generated by VisDiff.\nTop differences.\nThe top 5 difference descriptions from\nVisDiff were \u201chumanized object items\u201d, \u201cpeople interact-\ning with objects\u201d, \u201celectronics and appliances\u201d, \u201cobjects or\npeople in a marketplace setting\u201d, and \u201chousehold objects in\nunusual placement\u201d.\nD.4. Comparing Versions of Stable Diffusion\nText-to-image generation details.\nWe use the Hug-\ngingface\nmodels\n\u201cCompVis/stable-diffusion-v1-4\u201d\nand\n\u201cstabilityai/stable-diffusion-2-1\u201d with guidance of 7.5 and\nnegative prompts \u201cbad anatomy, bad proportions, blurry,\ncloned face, cropped, deformed, dehydrated, disfigured, du-\nplicate, error, extra arms, extra fingers, extra legs, extra\nlimbs, fused fingers, gross proportions, jpeg artifacts, long\n18\n    The following are the result of captioning two groups of images generated by two different image generation models, with each pair of captions \ncorresponding to the same generation prompt:\n    \nPrompt: red apples on a tree with green leaves\nGroup A: a tree \ufb01lled with red apples hanging from its branches. There are a total of nine apples visible in the scene, with some of them appearing to be ripe \nand ready to be picked. The apples are arranged in various positions on the tree, with some closer to the top and others near the bottom. The tree appears to \nbe a healthy and thriving source of fresh fruit.\nGroup B: a tree \ufb01lled with a variety of apples hanging from its branches. There are several apples of different sizes and colors, including red and green, \nscattered throughout the tree. Some apples are hanging higher up, while others are closer to the lower branches. The tree appears to be a lush, healthy   \nsource of fresh fruit.\n\u2026.\n    I am a machine learning researcher trying to \ufb01gure out the major differences between these two groups so I can correctly identify which model generated \nwhich image for unseen prompts.\n    Come up with 10 distinct concepts that are more likely to be true for Group A compared to Group B. Please write a list of captions (separated by bullet \npoints \"*\") . for example:\n    * \"dogs with brown hair\"\n    * \"a cluttered scene\"\n    * \"low quality\"\n    * \"a joyful atmosphere\"\n    Do not talk about the caption, e.g., \"caption with one word\" and do not list more than one concept. The hypothesis should be a caption that can be fed into \nCLIP, so hypotheses like \"more of ...\", \"presence of ...\", \"images with ...\" are incorrect. Also do not enumerate possibilities within parentheses. Here are \nexamples of bad outputs and their corrections:\n    * INCORRECT: \"various nature environments like lakes, forests, and mountains\" CORRECTED: \"nature\"\n    * INCORRECT: \"images of household object (e.g. bowl, vacuum, lamp)\" CORRECTED: \"household objects\"\n    * INCORRECT: \"Presence of baby animals\" CORRECTED: \"baby animals\"\n    * INCORRECT: \"Images involving interaction between humans and animals\" CORRECTED: \"interaction between humans and animals\"\n    * INCORRECT: \"More realistic images\" CORRECTED: \"realistic images\" \n    * INCORRECT: \"Insects (cockroach, dragon\ufb02y, grasshopper)\" CORRECTED: \"insects\"\n    Again, I want to \ufb01gure out what the main differences are between these two image generation models so I can correctly identify which model generated \nwhich image. List properties that hold more often for the images (not captions) in group A compared to group B. Answer with a list (separated by bullet points \n\"*\"). Your response:\nDiffusion Text-based Proposer Prompt \nFigure 15. Modified proposer\u2019s prompt for StableDiffusion analysis.\nneck, low quality, lowres, malformed limbs, missing arms,\nmissing legs, morbid, mutated hands, mutation, mutilated,\nout of frame, poorly drawn face, poorly drawn hands, sig-\nnature, text, too many fingers, ugly, username, watermark,\nworst quality\u201d.\nVisDiff details.\nUnlike the previous applications, there\nexists a one-to-one mapping between DA and DB through\nthe generation prompt. Therefore, we modify the subset\nsampling process to include the images generated from the\nsame prompts and modify the proposer\u2019s prompt to include\nthe generation prompts (Figure 15). We used LLaVA-1.5\nfor captioning rather than BLIP-2 because we were particu-\nlarly interested in the details of the images.\nTop differences.\nTop 5 differences are shown in Table 7.\nVisualizations.\nWe provide 50 random samples of SDv2\nand SDv1 images generated with DiffusionDB prompts\nin Figure 16.\nThese examples clearly verify that SDv2-\ngenerated images contain more vibrant contrasting colors\nand artwork or objects in frames or stands.\nAUROC\nMore True for SDv2\nParti\nDiffDB\ncolorful and dynamic collages of shapes or items\n0.70\n0.71\nvibrant colors\n0.72\n0.70\nstrong contrast in colors\n0.68\n0.68\nreflective surfaces\n0.68\n0.68\nartworks placed on stands or in frames\n0.64\n0.66\nTable 7. Concepts more true for SDv2 than v1. Differences are\nproposed by running VisDiff on PartiPrompts images. These dif-\nferences obtain similar scores on the unseen DiffusionDB images,\nindicating that these differences generalize to various prompts.\nEdge analysis.\nOne interesting finding from VisDiff is\nthat SDv2 generated images contain more image frames\nthan SDv1, such as a white border characterized by thick,\nstraight lines spanning much of the image.\nTo quantify\nthis, we employed a Canny edge detector and searched for\nstraight white lines in the images, with a thickness ranging\nfrom 5 to 20 pixels and a length exceeding 300 pixels (given\nthe image size is 512x512). Applying this analysis to Diffu-\nsionDB images revealed that 13.6% of SDv2 images exhib-\nited such lines, as opposed to only 5.4% from SDv1. This\nstatistic provides additional evidence for such difference.\n19\nSDv2\nSDv1\nSDv2\nSDv1\nSDv2\nSDv1\nSDv2\nSDv1\nSDv2\nSDv1\nFigure 16. Randomly sampled images generated from SDv2 and v1 using DiffusionDB prompts.\nD.5. Memorable Images\nTop differences.\nThe top 25 difference descriptions gen-\nerated by VisDiff are presented in Table 8.\nClassification analysis.\nTo validate whether the gener-\nated differences for memorable and forgettable images\nmake sense, we use CLIP to classify each image in the\nLaMem dataset to these 25+25 differences and then assign\nthe label \u201cforgettable\u201d or \u201cmemorable\u201d based on where the\ndifference is from. For example, if an image has the high-\nest cosine similarity with \u201cclose-up of individual people\u201d,\nwe assign its label as \u201cmemorable\u201d. We observe a 89.8%\naccuracy on the LaMem test set, demonstrating that these\ndifferences provide strong evidence to classify whether im-\nages are memorable or forgettable.\n20\nMemorable\nclose-up of individual people, use of accessories or personal\nitems, tattoos on human skin, close-up on individuals, humor-\nous or funny elements, artistic or unnaturally altered human fea-\ntures, humorous elements, detailed description of tattoos, fash-\nion and personal grooming activities, pop culture references,\ncollectibles or hobbies, light-hearted or humorous elements,\nthemed costumes or quirky outfits, animated or cartoonish char-\nacters, emphasis on fashion or personal style, close-up of objects\nor body parts, close-up facial expressions, unconventional use of\neveryday items, images with a playful or humorous element, fo-\ncus on specific body parts, silly or humorous elements, people\nin casual or humorous situations, detailed description of attire,\nquirky and amusing objects, humorous or playful expressions\nForgettable\nSunsets and sunrises, serene beach settings, sunset or night-\ntime scenes, agricultural fields, clear daytime outdoor settings,\nlandscapes with water bodies, images captured during differ-\nent times of day and night, Beautiful skies or sunsets, aban-\ndoned or isolated structures, natural elements like trees and wa-\nter, urban cityscapes, urban cityscapes at night, various weather\nconditions, Afar shots of buildings or architectural structures,\noutdoor landscapes, cityscapes, Cityscapes and urban environ-\nments, Scenic outdoor landscapes, landscapes with mountains,\nPicturesque mountain views, expansive outdoor landscapes,\nScenic landscapes or nature settings, Serene and tranquil envi-\nronments, scenic landscapes, scenes with a serene and peaceful\natmosphere\nTable 8. Top 25 differences for memorable and forgettable images.\nE. Failure Cases and Limitations\nIn this section, we summarize the failure cases and limita-\ntions of VisDiff algorithm.\nE.1. Caption-based Proposer\nWhile our evaluation in the main paper shows that the\ncaption-based proposer outperforms other counterparts by\na large margin, translating images to captions may lead to\ninformation loss. For example, as shown in Figure 17, fine-\ngrained differences between groups \u201cCupcakes topped with\nbuttercream\u201d and \u201cCupcakes topped with fondant\u201d is over-\nlooked due to generic captions. We expect using captioning\nprompts tailored to the application domain can mitigate this\nissue.\nFurthermore, despite providing task context and several\nin-context examples, we noted instances where GPT-4 pre-\ndominantly focused on the captions rather than the under-\nlying high-level visual concepts. A frequent error involves\ngenerating concepts related more to the caption than the im-\nage, such as \u201crepetition of \u2019bonsai\u2019 in the caption,\u201d as illus-\ntrated in Figure 17. We anticipate that this issue will dimin-\nish as LLMs\u2019 instruction-following ability improves.\nE.2. Feature-based Ranker\nSeveral of VisDiff\u2019s ranker failure cases stem from biases\nand limitations in CLIP. For example, nuanced differences\nsuch as \u201ca plant to the left of the couch\u201d are often assigned\nlower rankings because CLIP struggles with precise loca-\ntion details, and minor variations in phrasing can lead to\nsignificant differences in similarity scores.\nAdditionally, using AUROC on cosine similarities as a\nranking metric is sensitive to outliers in cosine similarity\nscores. In practice, we have noticed that outliers can cause\nvery specific difference descriptions to be scored higher\nthan more general differences.\nFor instance, as shown\nin Figure 17, with DA being \u201cBirds flying in the sky\u201d and\nDB \u201cAirplanes flying in the sky,\u201d the hypothesis \u201cImages of\nseagulls in flight\u201d received a higher AUROC score than the\nmore broadly applicable \u201cbirds in flight\u201d.\nE.3. LLM-based Evaluation\nAs demonstrated in the main paper, large language mod-\nels generally align well with human evaluations. However,\nthere are instances where they fail to accurately score dif-\nferences against the ground truth descriptions. An exam-\nple from VisDiffBench involves the description \u201cGreen ap-\nples in a basket\u201d for DA and \u201cRed apples in a basket\u201d for\nDB. Here, the top hypothesis by VisDiff, \u201cGreen apples\u201d\nreceived a score of only 0.5 instead of the expected 1.0.\nThese errors are expected to diminish as LLM improves.\nE.4. VisDiffBench\nMost differences in VisDiffBench focus on objects, styles,\nand actions. Differences such as object position, size, or\nimage quality are missing. Additionally, since PairedIm-\nageSets is compiled by scraping images from the web, the\ndatasets inevitably include noise. For instance, searching\nfor \u201ca cat to the left of a dog\u201d often yields images with a cat\non the right instead.\nE.5. Reliance on Large Pre-trained Models\nOur approach is fundamentally based on large, pre-trained\nvision-language foundation models.\nThese models\u2019 ex-\ntensive capabilities make them adaptable for a variety of\ntasks.\nHowever, inherent biases and limitations in these\nmodels may be transferred to our method. Additionally,\nthese models might be confined to domains observed dur-\ning pre-training, potentially limiting their applicability to\nnovel domains, such as biomedical imaging. Nevertheless,\nwe anticipate that rapid advancements in foundation model\ndevelopment will mitigate these issues, thereby enhancing\nour method\u2019s effectiveness.\n21\nMotorcycles \non a street\nBicycles on a \nstreet\nLevel: Easy\nTop Difference: futuristic motorcycle design    AUROC: 0.99    GPT-4 Score: 0.5\nBirds flying in \nthe sky\nAirplanes flying \nin the sky\nLevel: Easy\nTop Difference: Images of seagulls in flight    AUROC: 0.99    GPT-4 Score: 0.5\nVintage cars on \na road\nModern cards \non a road\nLevel: Medium\nTop Difference: woman driving vintage cars    AUROC: 0.98    GPT-4 Score: 0.5\nShiny metallic \ncars\nMatte finish \ncars\nLevel: Medium\nTop Difference: cars with reflective backgrounds    AUROC: 0.85    GPT-4 Score: 0.5\nCupcakes topped \nwith buttercream\nCupcakes topped \nwith fondant\nLevel: Hard\nTop Difference: Cupcakes with coffee frosting    AUROC: 0.69   GPT-4 Score: 0.5\nBonsai trees shaped \nin \ncascade style\nBonsai trees shaped \nin informal upright \nstyle\nLevel: Hard\nTop Difference: Repetition of \u2018bonsai\u2019 in the caption AUROC: 0.74    GPT-4 Score: 0.0\nFigure 17. PairedImageSets examples where VisDiff fails. We show the ground-truth difference, top difference predicted by VisDiff,\nAUROC score output by the ranker, and evaluation of the predicted difference by GPT-4.\n22\n"
  },
  {
    "title": "Orthogonal Adaptation for Modular Customization of Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.02432.pdf",
    "upvote": "11",
    "text": "Orthogonal Adaptation for Modular Customization of Diffusion Models\nRyan Po\nStanford University\nGuandao Yang\nStanford University\nKfir Aberman\nSnap Research\nGordon Wetzstein\nStanford University\nIndependent Customization\nJoint Synthesis\nOrthogonal Concepts\nInstant \nMerging\nFigure 1. Modular Customization of Diffusion Models. Given a large set of individual concepts (left), the goal of Modular Customization\nis to enable independent customization (fine-tuning) per concept, while efficiently merging a subset of customized models during inference,\nso that the corresponding concepts can be jointly synthesized without compromising fidelity. To tackle this, we propose Orthogonal\nAdaptation, which encourages customized weights of one concept to be orthogonal to the customized weights of others.\nAbstract\nCustomization techniques for text-to-image models have\npaved the way for a wide range of previously unattainable\napplications, enabling the generation of specific concepts\nacross diverse contexts and styles.\nWhile existing meth-\nods facilitate high-fidelity customization for individual con-\ncepts or a limited, pre-defined set of them, they fall short\nof achieving scalability, where a single model can seam-\nlessly render countless concepts.\nIn this paper, we ad-\ndress a new problem called Modular Customization, with\nthe goal of efficiently merging customized models that were\nfine-tuned independently for individual concepts. This al-\nlows the merged model to jointly synthesize concepts in one\nimage without compromising fidelity or incurring any ad-\nditional computational costs. To address this problem, we\nintroduce Orthogonal Adaptation, a method designed to en-\ncourage the customized models, which do not have access to\neach other during fine-tuning, to have orthogonal residual\nweights. This ensures that during inference time, the cus-\ntomized models can be summed with minimal interference.\nOur proposed method is both simple and versatile, appli-\ncable to nearly all optimizable weights in the model archi-\ntecture. Through an extensive set of quantitative and quali-\ntative evaluations, our method consistently outperforms rel-\nevant baselines in terms of efficiency and identity preserva-\ntion, demonstrating a significant leap toward scalable cus-\ntomization of diffusion models.\nProject page: http://www.ryanpo.com/ortha\n1. Introduction\nDiffusion models (DMs) mark a paradigm shift for com-\nputer vision and beyond. DM-based foundation models for\ntext-to-image, video, or 3D generation enable users to cre-\nate and edit content with unprecedented quality and diver-\nsity using intuitive text prompts [31]. Although these foun-\ndation models are trained on a massive amount of data, in\norder to synthesize user-specific concepts (such as a pet, an\nitem, or a person) with a high fidelity, they often need to be\nfine-tuned.\nSeveral recent approaches to customizing DMs to indi-\nvidual concepts have demonstrated high-quality results [10,\n18, 24, 35, 44]. A multi-concept DM strategy, however,\nwhere several pre-trained concepts are mixed in a single\nimage, remains challenging. Existing multi-concept meth-\nods [12, 24] either show a degradation in the quality of in-\ndividual concepts when merged or require access to multi-\nple concepts during training. The latter makes the process\nunscalable and raises privacy concerns when the different\nconcepts belong to different users. Furthermore, in all cases\nthe mixing process is computationally inefficient.\nWe introduce orthogonal adaptation as a new approach\nto enabling instantaneous multi-concept customization of\nDMs. The primary insight of our work is that changing\nhow the DM is fine-tuned for novel concepts can lead to\nvery efficient mixing of these concepts. Specifically, we\nrepresent each new concept using a basis that is approx-\nimately orthogonal to the basis of other concepts. These\narXiv:2312.02432v1  [cs.CV]  5 Dec 2023\nConcept Bank\nFigure 2. Gallery of multi-concept generations. Our method enables efficient merging of individually fine-tuned concepts for modular,\nefficient multi-concept customization of text-to-image diffusion models. Each concept shown above was fine-tuned individually using\northogonal adaptation. Fine-tuned weight residuals are then merged via summation, enabling multi-concept generation.\nbases do not need to be know a priori and different con-\ncepts can be trained independently of each other. A key\nadvantage of our approach is that our model does not need\nto be re-trained when mixing several of our orthogonal con-\ncepts together, for example to jointly synthesize different\nconcepts that were never seen together in any training ex-\nample. Importantly, our approach is modular in that it en-\nables individual concepts to be learned independently and\nin parallel without knowledge of each other. Moreover, it\nis privacy aware in the sense that it never requires access to\nthe training images of concepts to mix them.\nConsider a social media platform where millions of users\nfine-tune a DM using their personal concepts and want to\nmix them with their friends\u2019 concepts on their phones. Effi-\nciency of the customization and mixing processes as well as\ndata privacy are key challenges in this scenario. Our method\naddresses precisely these issues. A core technical contribu-\ntion of our work is a modular customization and scalable\nmulti-concept merging approach that offers better quality\nin terms of identity preservation than baselines at similar\nspeeds, or similar quality to state-of-the-art baselines at sig-\nnificantly lower processing times.\n2. Related Work\nText-conditioned image synthesis.\nThe field of text-\nconditioned image synthesis has experienced significant ad-\nvancements, driven by developments in GANs [6, 11, 21\u2013\n23] and diffusion models [8, 16, 17, 28, 29, 34, 42]. Ear-\nlier efforts focus on applying GANs to various conditional\nsynthesis tasks, including class-conditioned image gener-\nation [6, 19, 21] and text-driven editing [2, 5, 9, 26, 30,\n33, 46]. More recently, the focus has shifted to large text-\nto-image models [32, 34, 37, 48] trained on large-scale\ndatasets [38].\nIn this paper, we will utilize the open-\nsource StableDiffusion [34] architecture and build on its\npre-trained checkpoints by fine-tuning.\n1assuming DB-LoRA fine-tuned models are merged with FedAvg [25]\nMethod\nFidelity\n(Single-concept)\nEfficient\nMerging\nFidelity\n(Multi-concept)\nTI [10]\n\u2717\n\u2713\n\u2717\nDB-LoRA1 [35]\n\u2713\n\u2713\n\u2717\nCustom Diffusion [24]\n\u2717\n\u2713\n\u2717\nMix-of-Show [12]\n\u2713\n\u2717\n\u2713\nOurs\n\u2713\n\u2713\n\u2713\nTable 1. Comparison of Solutions to Modular Customization.\nOur customization approach excels in three key areas: (1) preserv-\ning the identity of individual concepts with high fidelity, (2) effi-\nciently merging independently customized models, and (3) main-\ntaining high concept fidelity for multi-concept image synthesis us-\ning the merged model.\nCustomization.\nThe task of customization aims at captur-\ning a user-defined concept, to be used for generation under\nvarious contexts. Seminal works such as Textual Inversion\n(TI) [10] and DreamBooth [35] tackle the problem of cus-\ntomization by taking a handful of images of the same con-\ncept to produce a representation of the subject to be used\nfor controlled generation. TI captures new concepts by opti-\nmizing a text embedding to reconstruct target images using\nthe conventional diffusion loss. Follow-up works, such as\nP+ [14], extend Texture Inversion with a more expressive\ntoken representation, improving generation subject align-\nment/fidelity. DreamBooth [35], on the other hand, picks an\nuncommon word token and fine-tunes the network weights\nto reconstruct the target concept using diffusion loss [17].\nCustom Diffusion [24] works in a similar way but only\nfine-tunes a subset of the diffusion model layers, namely\nthe cross-attention layers. LoRA [18] is a low-rank ma-\ntrix decomposition method that enables better parameter ef-\nficiency for fine-tuning methods, and was recently adapted\nto customization of text-to-image diffusion models [1] (DB-\nLoRA). Recent works [20, 36, 40, 41, 43, 45, 47] try to im-\nprove speed by training feed-forward networks to predict\nadaptation parameters from data, successfully amortize the\ntime taken to create customize concepts.\nMulti-concept Customization.\nCertain existing works\nhave taken the task of customization one step further, aim-\ning to inject multiple novel concepts into a model at the\nsame time. Custom Diffusion [24] achieves this through\na joint optimization loss for all concepts, while Break-\na-scene [3] and SVDiff [13] introduces a masked cross-\nattention loss to learn individual concepts in images con-\ntaining multiple concepts. However, such methods require\naccess to ground truth data of all concepts training. In this\nwork, we are interested in the task of modular customiza-\ntion, where concepts are learned independently, and users\ncan then mix and match individual concepts during infer-\nence for multi-concept image synthesis (Sec. 3.1).\nPrior works have provided implicit solutions to the prob-\nlem of modular customization, but each existing method\ncomes with its own set of trade-offs. TI [10, 27, 44] im-\nplicitly addresses the task by representing each concept\nthrough a unique token embedding, enabling multi-concept\ncustomization by simply querying each token. However,\nTI tends to suffer from low subject fidelity, as token embed-\ndings alone provide limited expressivity. Federated Averag-\ning (FedAvg) [25] merges fine-tuned models by simply tak-\ning a weighted average between the weights of each model,\nalthough fast and expressive, naive combination tends to\nlead to loss of concept identity. Custom Diffusion [24] sup-\nports merging of individually fine-tuned networks through\nsolving a constrained customization problem. This method\nalso struggles with expressivity, as only a small subset of\nthe diffusion model weights are being updated. Concurrent\nwork, Mix-of-Show (MoS) [12] expands on this method\nby introducing gradient fusion, enabling merging of mul-\ntiple separately fine-tuned models without placing restric-\ntions on parameter expressivity. Though expressive, gra-\ndient fusion is computationally demanding, taking \u223c15-20\nminutes just to combine three custom concepts into a sin-\ngle model, which becomes intractably expensive when de-\nployed at scale. Table 1 summarizes the key areas in which\nour approach differs from previous and concurrent works.\n3. Method\nIn this section, we first introduce the problem setting of\nmodular customization (Sec. 3.1). We then take a look at\nthe simple solution of FedAvg [25], and explore where and\nwhy this naive method fails to preserve identity (Sec. 3.2).\nMotivated by the limitations of FedAvg, we discuss the con-\nditions to ensure concept identity preservation (Sec. 3.3),\nand finally introduce our solution to modular customization\n\u2013 orthogonal adaption (Sec. 3.4 and Sec. 3.5).\n3.1. Modular Customization\nIn this paper, we are interested in customizing text-to-\nimage diffusion models to generate multiple personal con-\ncepts in an efficient, scalable, and decentralized manner.\n(a) Independent Customization\n(b) Modular Combination\n(c) Joint Synthesis\nInput \nImages\nFine-tuned\nModels\n+\nConcept Bank\nMerged Model\n\u201c<dog1>, <dog2>, <dog3>, \nsitting in a field of grass\u201d \nFigure 3. The three stages of Modular Customization: (a) Inde-\npendent Customization, (b) Modular Combination, and (c) Joint\nSynthesis. Note that during individual fine-tuning, all processes\nare private, meaning each user does not have access to ground truth\ndata for other concepts.\nIn addition to single-concept text-to-image customization,\nusers are usually interested in seeing multiple concepts in-\nteracting together. This calls for a text-to-image model that\nis customized to a set of concepts. Being able to generate\nmultiple personalized concepts in a single model, however,\nis challenging. First, the number of sets containing all possi-\nble combinations of concepts is growing exponentially with\nrespect to the number of concepts \u2013 an intractable number\neven for a relatively small number of concepts. As a result,\nit\u2019s important for personalized concepts to be merged with\ninteractive speed. Furthermore, users usually have limited\ncompute at their end, which means any computation done\non the users end should ideally be trivial.\nThese requirements motivate an efficient and scalable\nfine-tuning setting we call modular customization, where\nindividual fine-tuned models should act like independent\nmodules, which can be combined with others in a plug-\nand-play manner without additional training. The setting\nof modular customization involves three stages: indepen-\ndent customization, modular combination and joint synthe-\nsis. Fig. 3 provides an illustration of this three stage process.\nWith modular customization in mind, our goal is to de-\nsign a fine-tuning scheme, such that individually fine-tuned\nmodels can be trivially combined (e.g. summation) with any\nother fine-tuned model to enable multi-concept generation.\n(a) Conventional LoRA structure\nPretrained\nWeight Matrix\nFrozen\nTrainable\n(b) Ours \u2013 orthogonal adaptation \nPretrained\nWeight Matrix\nFrozen\nConcept j\nConcept i\nConcept i\n(c) Orthogonality constraint\n(d) Basis sampling method\nShared orthogonal basis\n(e) Visualization of concept disentanglement\nwithout orthogonality\nwith orthogonality\ncorrelated concepts lead to \n\u201ccrosstalk\u201d when merged\northogonal concepts preserve \nidentity when merged\n,\nFigure 4. Overview of Orthogonal Adaptation. (a) LoRA [18] enables training of both low-rank decomposed matrices. (b) Orthogonal\nadaption constrains training only to A, leaving B fixed. (c) For two separate concepts, i and j, an orthogonality constraint is imposed\nbetween Bi and Bj. (d) When concepts i and j are trained independently, approximate orthogonality between Bi and Bj can be achieved\nby sampling random columns from a shared orthogonal matrix. (e) Without the orthogonality constraint, correlated concepts suffer from\n\u201ccrosstalk\u201d when merged; with the orthogonality constraint, orthogonal concepts preserve their identities after merging.\n3.2. Federated Averaging\nPerhaps the most straight-forward technique for achiev-\ning modular customization is to take a weighted average of\neach individually fine-tuned model. This technique is often\nreferred to as FedAvg [25]. Given a set of learned weight\nresiduals \u2206\u03b8i optimized on concept i, the resulting merged\nmodel is simply given by\n\u03b8merged = \u03b8 +\nX\ni\n\u03bbi\u2206\u03b8i,\n(1)\nwhere \u03b8 represents the pre-trained parameters of the model\nused for fine-tuning, and \u03bbi is a scalar representing the rel-\native strength of each concept. While FedAvg is fast and\nplaces no constraints on the expressivity of each individu-\nally fine-tuned model, naively averaging these weights can\nlead to loss of subject fidelity due to interference between\nthe learned weight residuals. This effect is especially se-\nvere when training multiple semantically similar concepts\n(e.g., human identities), as learned weight residuals tend\nto be very similar. We coin this undesirable phenomenon\n\u201ccrosstalk\u201d. Fig. 7 and Fig. 8(a) provide visualizations of\nthe effect of crosstalk, as FedAvg causes multi-concept gen-\nerations to exhibit loss of identity.\nOur approach is in-\nspired by FedAvg. We adopt its computational efficiency\nbut modify the fine-tuning process to ensure minimal inter-\nference between learned weight residuals between different\nconcepts. We want to enable instant, multi-concept cus-\ntomization from individually trained models without sacri-\nficing subject fidelity.\n3.3. Preserving Concept Identity\nWith the goal of addressing the limitations of FedAvg,\nwe first examine where this method fails. For simplicity,\nconsider the case of merging two concepts i and j. Af-\nter fine-tuning on each individual task, we receive a set of\nlearned weight residuals \u2206\u03b8i and \u2206\u03b8j. The output of a par-\nticular linear layer in the fine-tuned network is\nOi(Xi) = (\u03b8 + \u2206\u03b8i)Xi,\n(2)\nwhere Xi represents a particular input to the layer corre-\nsponding to the training data of concept i. When merging\nthe two concepts using FedAvg with \u03bb = 1, the resulting\nmerged model produces\n\u02c6Oi(Xi) = (\u03b8 + \u2206\u03b8i + \u2206\u03b8j)Xi.\n(3)\nThe goal of concept preservation is to have \u02c6Oi(Xi) =\nOi(Xi). Note that, without enforcing specific constraints,\nit is likely that \u2206\u03b8jXi \u0338= 0 and, subsequently, \u02c6Oi \u0338= Oi.\nIt follows that the mapping of data for concept i is pre-\nserved when \u2206\u03b8jXi = 0 for j \u0338= i. By symmetry, the map-\nping of data for concept j is preserved given \u2206\u03b8iXj = 0\nfor i \u0338= j. Intuitively, ||\u2206\u03b8jXi|| measures the amount of\ncrosstalk between the customized weights of concepts i and\nj. We would like to keep this value low to ensure subject\nidentity is preserved even after merging. However, note that\ngiven enough data for training a certain concept i, Xi is\nlikely to have full column rank. This makes the orthog-\nonality condition impossible to satisfy. Instead, we pro-\npose a relaxation to this condition, choosing to minimize\nthe crosstalk term for some projection of Xi onto a sub-\nspace Si. This projection yields SiST\ni Xi, and our relaxed\nobjective hopes to achieve \u02c6Oi(SiST\ni Xi) = Oi(SiST\ni Xi).\n3.4. Orthogonal Adaptation\nMotivated by the relaxed objective above, we propose\northogonal adaptation.\nSimilar to low-rank adaptation\n(LoRA), we represent learned weight residuals through a\nlow-rank decomposition of the form\n\u2206\u03b8i = AiBT\ni , \u03b8i \u2208 Rn\u00d7mAi \u2208 Rn\u00d7r, Bi \u2208 Rm\u00d7r,\n(4)\nInput Images\nLoRA (image alignment: 0.745)\nOrthogonal adaptation (image alignment: 0.748)\nFigure 5. Over-parameterization of text-to-image models. De-\nspite the added constraint on the trained weight residuals, due to\nthe over-paramterized nature of large text-to-image diffusion mod-\nels, our method is able to achieve single-concept customization re-\nsults with comparable fidelity to the unconstrained setting.\nwhere the rank r << min(n, m). However, contrary to con-\nventionally fine-tuning with LoRA, we keep Bi constant,\nand only optimize Ai.\nConsider a matrix \u00afBj, where its columns span the or-\nthogonal complement of the column space of Bj. We show\nthat by selecting Si = \u00afBj, we achieve the conditions for\nachieving the projected preservation objective. This can be\nseen from the fact that,\n\u02c6Oi(SiST\ni Xi) = Oi(SiST\ni Xi) + \u2206\u03b8jSiST\ni Xi\n(5)\n= Oi(SiST\ni Xi) + Aj\b\b\b\nBT *0\nj Si\nST\ni Xi\n(6)\n= Oi(SiST\ni Xi).\n(7)\nSince r << m, the orthogonal complement of Bj covers\nmost of Rm. It follows that \u00afBj \u00afBT\nj Xi \u2248 Xi, making \u00afBj a\nreasonable candidate for Si.\nAt the same time, since we expect the learned residu-\nals for a concept to have meaningful interactions with their\ndata, we would also like to ensure ||\u2206\u03b8iXi|| is non-trivial.\nBy approximating Xi with its projection onto \u00afBj, our objec-\ntive changes to ensuring ||AiBT\ni \u00afBj \u00afBT\nj Xi|| is non-trivial.\nExamining this term gives us the additional constraint that\nBT\ni \u00afBj \u0338= 0, meaning the columns of Bi should live in the\northogonal complement of the columns space of Bj. There-\nfore, to ensure meaningful fine-tuning results, we should\nalso enforce orthogonality between the learned residuals,\ni.e. BT\ni Bj = 0.\nFig. 4 provides an overview of our orthogonal adaption\nmethod. Intuitively, as illustrated in Fig. 4(e), our method\ndisentangles custom concepts into orthogonal directions,\nensuring that there is no crosstalk between concepts. As\na result, our merged model can better preserve the identity\nof each concept.\nExpressivity of orthogonal adaption.\nExpressivity of\nour method arises as a natural concern as we are optimiz-\ning significantly fewer parameters by freezing Bi.\nFor-\ntunately, text-to-image diffusion models are often over-\nparameterized, with millions/billion of parameters. Prior\nworks have shown that even fine-tuning a subspace of such\nparameters can be expressive enough to capture a novel con-\ncept. We also show this result empirically in Fig. 5, where\nour method leads to results with similar fidelity, even with-\nout the need to optimize Bi during training.\n3.5. Designing Orthogonal Matrices Bi\u2019s\nA key challenge of the method described in previous sec-\ntions is to generate a set of basis matrices Bi that are orthog-\nonal to each other. Note that this is very difficult especially\nbecause when choosing Bi, the user is not aware of what\nbasis the other users chose to optimize for the concepts to\nbe combined in the future. Strictly enforcing such orthogo-\nnality might be infeasible without prior knowledge of other\ntasks. We instead propose a relaxation to the constraint, in-\ntroducing a simple and effective method to achieve approx-\nimate orthogonality.\nRandomized orthogonal basis.\nOne method for enforc-\ning approximate orthogonality is to determine a shared or-\nthogonal basis. For some linear weight \u03b8 \u2208 Rm\u00d7n, we first\ngenerate a large orthogonal basis O \u2208 Rn\u00d7n. This orthog-\nonal basis is shared between all users. During training of\nconcept i, Bi is formed from taking a random subset of k\ncolumns from O. Given k << n, the probability of two ran-\ndomly chosen Bi\u2019s to share the same columns is kept low.\nRandomized Gaussian.\nAnother approach is to choose\nrandom matrix elements. Specifically, we sample each en-\ntry of Bi from a zero-mean Gaussian with standard devia-\ntion \u03c3: Bi[k] \u223c N(0, \u03c32I). When the dimensionality of Bi\nis high, this simple strategy creates matrices that are orthog-\nonal in expectation: E\n\u0002\nBT\ni Bj\n\u0003\n= 0 (see supplement for\ndiscussion). Naturally, this method does not require knowl-\nedge of a shared basis to sample from. In practice, how-\never, we found randomized Gaussians lead to higher levels\nof crosstalk in our setting, i.e., ||BT\ni Bj|| tends to be larger\nthan for the randomized orthogonal basis.\n4. Experiments\nIn this section, we show the results of our method applied\nto the task of modular customization. Qualitative and quan-\ntitative results indicate that our method outperforms rele-\nvant baselines [1, 12, 24] at similar speeds, and quality on\npar with state-of-the-art baselines that require significantly\nhigher processing times [12].\nDatasets.\nWe perform evaluations on a custom dataset of\n12 concept identities, each containing 16 unique images of\nthe target concept in different contexts.\nImplementation details.\nWe perform fine-tuning on the\nStable Diffusion [34] model, specifically the ChilloutMix\ncheckpoint for its ability to handle high-fidelity human face\n<1s\n<1s\n<1s\n~2s\n<1s\nOrthogonal\nAdaptation (Ours)\nMix-of-Show\n(Gradient Fusion)\nMix-of-Show\n(FedAvg)\nCustom Diffusion\n(Merge)\nPrompt+\n~15m\nDreamBooth-LoRA\n(FedAvg)\nInput Images\n<1s\n<THANOS>                <RYAN>\n<MARGOT>\n, in the style of Cyberpunk 2077, 4K, ultra-realistic, \u2026 \nGenerate\n/\n/\nFigure 6. Identity preservation in single-concept generations from a merged model. We demonstrate our method\u2019s ability to maintain\nidentity consistency across different single-concept generations. Each column showcases images from the same merged model, represent-\ning three distinct concept identities. Our approach showcases better identity alignment with the corresponding input images, offering a\nsignificant improvement over comparable merging methods. Additionally, our method\u2019s performance parallels that of Mix-of-Show (Gra-\ndient Fusion) but with the advantage of near-instantaneous merging, in contrast to the approximately 15-minute merging time required.\ngeneration. For single-concept fine-tuning, we apply or-\nthogonal adaptation to all linear layers in the Stable Dif-\nfusion architecture. Following prior work [12, 44], we also\napply a layer-wise text embedding and represent each fine-\ntuned concept as two separate text tokens. We fine-tune\nthe text embeddings with a learning rate of 1e \u2212 3, the\ndiffusion model parameters with a learning rate of 1e \u2212 5\nand set r = 20 for all experiments. Single-concept fine-\ntuning takes \u223c10-15 minutes on two A6000 GPUs. For\nour method, we enforce the orthogonality constraint us-\ning the randomized orthogonal basis method for all experi-\nments. Methods using FedAvg (including orthogonal adap-\ntion) were merged using \u03bb = 0.6.\nBaselines.\nWe compare our method against state-of-the-\nart baselines on the task of modular customization, namely:\nDreamBooth-LoRA [1], P+ [44], Custom Diffusion [24],\nand Mix-of-Show [12]. Fine-tuned models are merged dif-\nferently depending on the method. DreamBooth-LoRA is\nmerged using FedAvg, Custom Diffusion is merged us-\ning their proposed optimization-based merging method, and\nMix-of-Show is merged using gradient fusion as outlined in\ntheir work. Since P+ does not perform fine-tuning on the\nweights of the network, merging is done simply by query-\ning each concept\u2019s token embedding. For completeness, we\nalso compare against Mix-of-Show merged using FedAvg,\nserving as an efficient alternative to the computationally de-\nmanding gradient fusion method.\nExperimental setup and metrics.\nFirst, we fine-tune\neach concept individually, without access to data for any\nother concept.\nEach fine-tuned model is then combined\nwith two other concepts at random using their correspond-\ning method for merging. Following prior work, we eval-\nuate our method on image alignment, which measures the\nsimilarity of image features between generated images and\nthe input reference image by measuring their similarity in\nthe CLIP image feature space [10]. Similarly, we evaluate\nour method using text alignment, ensuring the output gen-\nOrthogonal Adaptation (Ours)\nMix-of-Show (FedAvg)\nPrompt+\nMix-of-Show (Grad Fusion)\n<THANOS>            & <RYAN>\n& <MARGOT>\n, playing poker, in the style of Cyberpunk 2077, \u2026\nGenerate\nFigure 7.\nMulti-concept results.\nExamples of multi-concept generations, synthesized using sampling techniques from concurrent\nwork [12]. While Mix-of-Show (FedAvg) maintains high-level features, it struggles with crosstalk, manifesting overly smooth facial\nfeatures. Mix-of-Show (Gradient Fusion) exhibits good identity alignment, albeit with a computationally intensive merging process. P+\nmanages to preserve identity after merging, but struggles to capture identity with high-fidelity due to limited parameter expressivity. Our\nmethod stands out by achieving high identity alignment with a significantly faster merging procedure.\nMethod\nMerge\nTime\nText Alignment \u2191\nImage Alignment \u2191\nIdentity Alignment \u2191\nSingle\nMerged\n\u2206\nSingle\nMerged\n\u2206\nSingle\nMerged\n\u2206\nP+ [44]\n<1 s\n.643 \u2192 .643\n\u2014\n.683 \u2192 .683\n\u2014\n.515 \u2192 .515\n\u2014\nCustom Diffusion [24]\n\u223c2 s\n.668 \u2192 .673\n+.005\n.648 \u2192 .623\n-.025\n.504 \u2192 .408\n-.096\nDB-LoRA (FedAvg) [1]\n<1 s\n.613 \u2192 .682\n+.069\n.744 \u2192 .531\n-.213\n.683 \u2192 .098\n-.585\nMoS (FedAvg) [12]\n<1 s\n.625 \u2192 .621\n-.004\n.745 \u2192 .735\n-.010\n.728 \u2192 .706\n-.022\nMoS (Grad Fusion) [12]\n\u223c15 m\n.625 \u2192 .631\n+.006\n.745 \u2192 .729\n-.016\n.728 \u2192 .717\n-.011\nOurs\n<1 s\n.624 \u2192 .644\n-.010\n.748 \u2192 .741\n-.007\n.740 \u2192 .745\n+.005\nTable 2. Quantitative results. We provide detailed qualitative comparisons for each method, evaluated both before and after the merging\nprocess. Prior to merging, our method demonstrates comparable performance in all identity-related metrics, highlighting its expressivity\neven with the orthogonality constraint. Post-merging, our method achieves the highest scores in image and identity alignment. Our method\nis also capable of maintaining text alignment scores comparable to other high-fidelity methods such as P+ and MoS.\nerations still adhere to the input text-prompts by measuring\nthe text-image similarity also using CLIP [15]. However, to\nfurther illustrate the identity preserving capabilities of our\nmethod, we also evaluate our method using the ArcFace [7]\nmodel. Using the ArcFace model, we measure the rate at\nwhich the target human identity is detected in a set of gen-\nerated images, we refer to this metric as identity alignment.\n4.1. Qualitative Comparisons\nMerged single-concept results.\nWe illustrate the iden-\ntity preserving effect of our method by comparing single-\nconcept generations of different identities from the same\nmerged model. As mentioned above, each concept is fine-\ntuned individually and merged together during inference.\nFig 6 shows generations for three separate concept identi-\nties, each column contains images sampled from the same\nmodel. Our method achieves better identity alignment with\nthe input images in the merged model compared to methods\nwith comparable merging times. We also achieve similar\nresults to Mix-of-Show (Gradient Fusion), which requires\n\u223c15 minutes to merge three concepts, while our method\nenables near instant merging.\nMerged multi-concept results.\nWe also show generated\nimages containing all three identities in the merged model.\nLeveraging multi-concept sampling techniques from con-\ncurrent work [12], we show examples of multi-concept gen-\nerations in Fig. 7. Once again, multi-concept models trained\nusing our method generate images with better identity align-\nment than competing baselines. Due to the poor perfor-\nmance of DB-LoRA [1] and Custom Diffusion [24] for\nsingle-concept generations, we omit results for these meth-\nods on multi-concept generation due to space constraints.\nP+ [14] suffers from low concept fidelity due to lim-\nited expressivity in their training regime. Although Mix-of-\nShow [12] (FedAvg) preserves certain high-level features\nthrough the layer-wise text-embedding, it still suffers from\ncrosstalk due to unconstrained training of weight residuals.\nMix-of-Show (Gradient Fusion) shows impressive identity\nalignment, however, this is only enabled by a computation-\nally demanding merging procedure. Our method achieves\nhigh identity alignment while keeping the merging process\nat near instant rates.\n4.2. Quantitative Results\nWe present quantitative comparisons in Table. 2. Specif-\nically, we show all three evaluation metrics applied to each\nmethod before and after merging.\nOur method achieves\ncomparable results in all concept alignment metrics be-\nfore merging, illustrating the expressivity of our method\ndespite the orthogonality constraint.\nAfter merging, our\nmethod achieves the highest image and identity alignment\nscores across all methods, while maintaining comparable\ntext alignment scores with other high-fidelity methods such\nas Mix-of-Show and P+. This illustrates that our method\nis able to achieve high identity preservation without sacri-\nficing the ability to generalize for different contexts.\nNote that although Custom Diffusion [24] and DB-\nLoRA [1] achieves higher text alignment, this is at the cost\nof significantly lower concept alignment scores than that of\ncompeting methods.\n5. Ablations\nEffect of orthogonality.\nIn Fig. 8(a), we present gener-\nated images from a model created from merging two sepa-\nrate fine-tuned models (concepts i and j). To illustrate the\neffect of orthogonality on identity preservation, we manip-\nulate the degree of orthogonality between Bi and Bj. On\nthe left, we have the worst case scenario, where Bi = Bj.\nOn the right, we show results where perfect orthogonality is\nachieved, i.e. BT\ni Bj = 0. In between, we construct Bi and\nBj from a shared orthogonal matrix, but choose half of their\ncolumns to be overlapping. Results in Fig. 8(a) show that\northogonality contributes significantly to identity preserva-\ntion even in the extreme case of merging 2 concepts.\nNumber of merged concepts\nFig. 8(b) shows results gen-\nerated from models with a range of concepts merged to-\ngether. With orthogonality, our model is capable of merg-\ning a high number of concepts with minimal identity loss.\nIn contrast, without orthogonality, concept fidelity quickly\ndegrades, even with relatively low number of concepts be-\ning combined. Running our model without orthogonality is\nequivalent to Mix-of-Show [12] merged using FedAvg [25].\nReference \nImages\n(a) Degree of orthogonality\n(b) # of merged concepts\n3\n13\n5\n7\n9\n11\nDegree of orthogonality\nWithout orthogonality\nWith orthogonality\nFigure 8. Ablation studies. (a) Images generated from a model\nformed by merging two separately fine-tuned models (concepts i\nand j), focusing on the role of orthogonality in preserving identity.\n(b) Image generations from models that with a varying number of\nmerged concepts. Without orthogonality, concept identity is lost\neven when merging a small number of concepts.\n6. Discussion\nLimitations.\nDespite showcasing the ability to encode\nseveral custom concepts into the same text-to-image model,\ngenerating images with complex compositions/interactions\nbetween multiple custom concepts remains challenging. As\nconcepts, such as human identities, have the tendency to\neither be entangled, or even completely ignored. Existing\nworks [4, 12] have developed certain strategies for remedy-\ning this effect, but such methods are still prone to the afore-\nmentioned failure cases. Another limitation of orthogonal\nadaption is that it directly modifies the fine-tuning process.\nTherefore, existing fine-tuned networks (e.g. LoRAs [1])\ncan not be adapted post-hoc to ensure orthogonality.\nEthics Considerations.\nGenerative AI could be misused\nfor generating edited imagery of real people with the intent\nof spreading disinformation. Such misuse of image synthe-\nsis techniques poses a societal threat, and we do not con-\ndone using our work for such purposes. We also recognize\na potential biases in the foundation model we built upon.\nConclusions.\nBy disentangling customization concepts\ninto orthogonal directions, orthogonal adaptation stream-\nlines the process of integrating multiple independently fine-\ntuned concepts into a single model instantly and with triv-\nial compute, while also ensuring preservation of each con-\ncept. Our work makes a significant step towards modular\ncustomization, where multi-concept customization can be\nachieved with individual, privately fine-tuned models.\n7. Acknowledgements\nWe thank Sara Fridovich-Keil and Kamyar Salahi for\nfruitful discussions and pointers for evaluation metrics. Po\nis supported by the Stanford Graduate Fellowship.\nThis\nproject was in part supported by Samsung and Stanford\nHAI.\nReferences\n[1] Low-rank adaptation for fast text-to-image diffusion fine-\ntuning.\nhttps://github.com/cloneofsimo/\nlora, 2022. 2, 5, 6, 7, 8\n[2] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and\nPeter Wonka.\nClip2stylegan: Unsupervised extraction of\nstylegan edit directions. In ACM SIGGRAPH 2022 confer-\nence proceedings, pages 1\u20139, 2022. 2\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski. Break-a-scene: Extracting multiple\nconcepts from a single image. ArXiv, abs/2305.16311, 2023.\n3\n[4] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. arXiv preprint arXiv:2302.08113, 2023. 8\n[5] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park,\nAli Jahanian, Aude Oliva, and Antonio Torralba. Paint by\nword. arXiv preprint arXiv:2103.10951, 2021. 2\n[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\narXiv preprint arXiv:1809.11096, 2018. 2\n[7] Jiankang Deng, Jia Guo, Jing Yang, Niannan Xue, Irene Kot-\nsia, and Stefanos Zafeiriou. ArcFace: Additive angular mar-\ngin loss for deep face recognition. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 44(10):5962\u20135979,\n2022. 7\n[8] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. ArXiv, abs/2105.05233, 2021. 2\n[9] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik,\nand Daniel Cohen-Or.\nStylegan-nada:\nClip-guided do-\nmain adaptation of image generators.\narXiv preprint\narXiv:2108.00946, 2021. 2\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 1, 2, 3, 6\n[11] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[12] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yun-\npeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning\nChang, Weijia Wu, et al. Mix-of-show: Decentralized low-\nrank adaptation for multi-concept customization of diffusion\nmodels. In NeurIPS, 2023. 1, 2, 3, 5, 6, 7, 8\n[13] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar,\nDimitris Metaxas, and Feng Yang.\nSvdiff: Compact pa-\nrameter space for diffusion fine-tuning.\narXiv preprint\narXiv:2303.11305, 2023. 3, 1, 2\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image\nediting with cross attention control. 2022. 2, 8\n[15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning, 2022. 7\n[16] Jonathan Ho.\nClassifier-free diffusion guidance.\nArXiv,\nabs/2207.12598, 2022. 2\n[17] Jonathan Ho, Ajay Jain, and P. Abbeel. Denoising diffusion\nprobabilistic models. ArXiv, abs/2006.11239, 2020. 2\n[18] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLoRA: Low-rank adaptation of large language models. In\nICLR, 2022. 1, 2, 4\n[19] Xun Huang, Arun Mallya, Ting-Chun Wang, and Ming-Yu\nLiu. Multimodal conditional image synthesis with product-\nof-experts gans. In European Conference on Computer Vi-\nsion, pages 91\u2013109. Springer, 2022. 2\n[20] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 2\n[21] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 2\n[22] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020.\n[23] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free\ngenerative adversarial networks. Advances in Neural Infor-\nmation Processing Systems, 34:852\u2013863, 2021. 2\n[24] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 1, 2, 3, 5, 6, 7, 8,\n4\n[25] H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth\nHampson, and Blaise Ag\u00a8uera y Arcas.\nCommunication-\nefficient learning of deep networks from decentralized data,\n2023. 2, 3, 4, 8\n[26] Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar\nMosseri, Tali Dekel, Daniel Cohen-Or, and Michal Irani.\nSelf-distilled stylegan: Towards generation from internet\nphotos. In ACM SIGGRAPH 2022 Conference Proceedings,\npages 1\u20139, 2022. 2\n[27] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038\u20136047,\n2023. 3\n[28] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 2\n[29] Kushagra Pandey, Avideep Mukherjee, Piyush Rai, and Ab-\nhishek Kumar. Diffusevae: Efficient, controllable and high-\nfidelity generation from low-dimensional latents.\nTrans.\nMach. Learn. Res., 2022, 2022. 2\n[30] Gaurav Parmar, Yijun Li, Jingwan Lu, Richard Zhang, Jun-\nYan Zhu, and Krishna Kumar Singh. Spatially-adaptive mul-\ntilayer selection for gan inversion and editing. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11399\u201311409, 2022. 2\n[31] Ryan Po, Wang Yifan, and Vladislav Golyanik et al. State of\nthe art on diffusion models for visual computing. In arxiv,\n2023. 1\n[32] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2\n[33] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages.\nACM Transactions on graphics (TOG), 42(1):1\u201313,\n2022. 2\n[34] Robin Rombach, A. Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer. High-resolution image synthesis\nwith latent diffusion models. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n10674\u201310685, 2021. 2, 5\n[35] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500\u201322510, 2023. 1, 2, 4\n[36] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 2\n[37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[38] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278\u201325294, 2022. 2\n[39] Sefik Ilkin Serengil and A. Ozpinar. Lightface: A hybrid\ndeep face recognition framework. 2020 Innovations in Intel-\nligent Systems and Applications Conference (ASYU), pages\n1\u20135, 2020. 1\n[40] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. ArXiv, abs/2304.03411, 2023. 2\n[41] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[42] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. ArXiv, abs/2010.02502, 2020.\n2\n[43] Yu-Chuan Su, Kelvin C. K. Chan, Yandong Li, Yang\nZhao, Han-Ying Zhang, Boqing Gong, H. Wang, and Xuhui\nJia.\nIdentity encoder for personalized diffusion.\nArXiv,\nabs/2304.07429, 2023. 2\n[44] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman.\np+: Extended textual conditioning in text-to-\nimage generation. arXiv preprint arXiv:2303.09522, 2023.\n1, 3, 6, 7\n[45] Zhouxia Wang, Xintao Wang, Liangbin Xie, Zhongang Qi,\nYing Shan, Wenping Wang, and Ping Luo.\nStyleadapter:\nA single-pass lora-free model for stylized image generation.\nArXiv, abs/2309.01770, 2023. 2\n[46] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.\nTedigan: Text-guided diverse face image generation and ma-\nnipulation. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2256\u20132265,\n2021. 2\n[47] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. 2023. 2\n[48] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2(3):5, 2022. 2\nOrthogonal Adaptation for Modular Customization of Diffusion Models\nSupplementary Material\n8. Gaussian random orthogonal matrices\nTheorem 8.1. Let v \u2208 Rd and u \u2208 Rd be two random\nvectors. Let vi \u223c N(0, \u03c32I) and ui \u223c N(0, \u03c32I) for all\ni \u2208 [1, d] independently, then E\n\u0002\nvT u\n\u0003\n= 0.\nProof.\nE\n\u0002\nvT u\n\u0003\n= E\n\" d\nX\ni=1\nviui\n#\n=\nd\nX\ni=1\nE [viui]\n(Linearity of expectation)\n=\nd\nX\ni=1\nE[vi]E[ui]\n(Independent)\n=\nd\nX\ni=1\n0 \u00b7 0 = 0.\nCorollary 8.1.1. Let A \u2208 Rn\u00d7m and B \u2208 Rn\u00d7m. All\nentries of these matrices are independently sampled from\nN(0, \u03c32I). Then E[AT B] = 0 \u2208 Rm\u00d7m.\nProof.\nE[AT B]ij = E[AT\ni Bj] = 0.\n9. Implementation details\nDataset.\nWe chose to evaluate our method on human\ndatasets due to the robustness of face recognition algorithms\nfor evaluation purposes. While prior works [12, 13, 24, 35]\nhave employed CLIP-based metrics as a method of evalu-\nating identity alignment, we found that CLIP features are\noften poor at identifying fine details in a custom concept. In\nFig. 9, we illustrate that our method works for non-human\nobjects too.\nEvaluation details.\nWe introduce the identity alignment\nmetric for measuring the ability of our method (and com-\npeting baselines) in capturing the target human identity in\nresulting generations. We use the ArcFace [39] facial recog-\nnition algorithm and consider a detection to be recorded\nwhen the ArcFace distance between two detected faces falls\nbelow 0.680 [39]. We choose to use detection probability\nas a metric rather than the raw distance metric as we found\nInput Images\nMix-of-Show (FedAvg)\nOrthogonal Adaptation (Ours)\nMix-of-Show            Ours\nNorm of matrix product between \nmerged LoRA layers (aka \u201ccrosstalk\u201d)\nFigure 9. Identity loss due to crosstalk. We illustrate the effects\nof crosstalk by examining the effects of interfering signals be-\ntween independently trained LoRAs. Measuring crosstalk through\nthe norm of the product between two LoRA weights, our method\nresults in lower crosstalk between independently trained LoRAs.\nCombined via the same method, our training regime leads to less\ncrosstalk and therefore better identity preservation after merging.\nthe distance metric to favor over-fitted models. Past the de-\ntection threshold, the distance metric directly measures the\nsimilarity between two faces, which is not ideal for use-\ncases such as re-stylization and accessorization.\nOrthogonal adaptation details.\nIn our method, we en-\nforce the orthogonality constraint through the LoRA down\nprojection matrix B. This formulation ensures orthogonal-\nity in the row-space of the resulting LoRA matrices. In\ntheory, we can also achieve orthogonality between trained\nweight residuals in the column-space, in which case the or-\nthogonality constraint would have to be enforced on the up-\nprojection matrix A instead. We choose to enforce orthog-\nonality in the row-space since the weight residuals inter-\nact with the layer inputs through their rows. The concept\npreservation formulation presented in Sec. 3 is also reliant\non row-space orthogonality. In our results, we chose to use\nthe random orthogonal basis method for enforcing orthog-\nonality in all our results. Although the Gaussian random\nmethod results in orthogonality on expectation, the orthogo-\nnal basis method led to lower crosstalk emperically. The or-\nthogonal basis method requires a shared orthogonal matrix\nto sample from. In practice, using Stable Diffusion v1.5,\nthere are only four unique input dimensions for all layers in\nthe diffusion model (320, 640, 768, 1280). Therefore, we\nonly have to store four unique square matrices from which\nall sampled Bi\u2019s can then be sampled from. These four or-\nFigure 10. Multi-concept failure cases. Multi-concept generation remains as an open challenge. Despite employing techniques such as\nregionally controllable sampling from prior work [12], this method can still suffer from failure cases such as: (left) ignoring concepts, and\n(right) leakage of concept attributes to neighboring identities.\nthogonal matrices can be downloaded along with the base\nmodel, but they can also be generated on the fly with a fix\nseed to ensure they are shared among all users.\nFedAvg merging coefficient.\nExisting work considers\nFedAvg merging with affine coefficients. However, with a\nlarger number of concepts, affinely combining each LoRA\nwill lead to dilution of signal from individual LoRAs. It is\nalso a common practice to scale individual LoRA weights\npost-hoc [1] for direct control over the signal strength from\nthe fine-tuning process.\nWe combine this scaling factor\nalong with the FedAvg merging factor to obtain a single\nscale factor \u03bbi as shown in Eq. 1. We consider merging co-\nefficients as a hyper-parameter that can be tuned based on\nuser preferences.\n10. Additional results\nIllustration of crosstalk.\nFig. 9 illustrates the importance\nof minimizing crosstalk for identity preservation when\nmerging LoRA weights into a single model. We measure\ncrosstalk formally using the norm of the matrix product\nbetween individually trained LoRA weight residuals. Up-\nper right of Fig. 9 shwos a direct comparison of the layer-\nwise normalized matrix product norms between two LoRAs\ntrained with and without orthogonality constraints.\nOur\nmethod leads to a much lower levels of crosstalk, which\ntranslates to better identity preservation as observed from\nthe resulting generations.\nExtended baseline comparisons.\nIn Fig. 11 We show an\nextended version of Fig. 6 with generated images of each\nidentity for each method before they are merged. These\nresults aim to show that our method is capable of retain-\ning identity alignment with the target concept before and\nafter merging, while achieving merging of individual Lo-\nRAs instantly without any further fine-tuning or optimiza-\ntion stages.\nOver-fitting.\nSince we are fine-tuning our network over\na small custom dataset and we initialize our custom tokens\nwith a user-defined class label, it may be susceptible to over-\nfitting. Prior works such as DreamBooth [35] and Custom\nDiffusion [24] alleviate this effect by adding a class preser-\nvation loss that ensures generating images from the class\ntoken still produces diverse results. In our method, we do\nnot employ an explicit loss to prevent over-fitting, however,\nwe found that our fine-tuned models still preserve the abil-\nity to generate diverse images for the trained class label as\nshown in Fig. 12\n11. Limitations and future work\nOur method takes an important step towards achieving\nmodular customization. However, a few important limita-\ntions should also be addressed in future work.\nGenerating multiple custom concepts within the same\nimage remains challenging. Simply prompting a merged\nmodel with multiple custom tokens usually leads to incoher-\nent hybrids of both objects. Prior works [12] have explored\nspatial guidance for better disentangling concepts in a single\ngeneration, and we have also employed similar techniques\nto generate our results. However, these methods still lead\nto failure cases as illustrated in Fig. 10. Concepts are often\nignored, or attributes can leak to neighboring concepts. Fu-\nture work should aim to address these struggles to further\nenable multi-concept generations.\nStoring individual LoRAs, even those trained with our\nmethod can also be expensive. Although LoRAs are already\ncompressive due to their low-ranked nature, storing a large\nbank of concepts for modualr customization can still be ex-\npensive. Works such as SVDiff [13] takes steps towards\nfurther compressing LoRAs while maintaining fidelity of\ngenerated images. However, our method does not naturally\nfit in with the SVDiff method, implying the need for a tai-\nlored compressing methodology.\n<1s\n<1s\n~2s\n<1s\nOrthogonal\nAdaptation (Ours)\nMix-of-Show\n(Gradient Fusion)\nMix-of-Show\n(FedAvg)\nCustom Diffusion\n(Merge)\nPrompt+\n~15m\nDreamBooth-LoRA\n(FedAvg)\nInput Images\n<1s\nBefore Merge\nAfter Merge\nBefore Merge\nAfter Merge\nBefore Merge\nAfter Merge\nFigure 11. Extended multi-concept results. We show results for each method before and after merging the individually trained models\ninto a single, merged model. Our method is able to capture the target identity with high fidelity before and after the merging process, while\nkeeping the merging process instantaneous.\nInput Images\nCustom concept generations\nClass token generations (man)\nCustom concept generations\nClass token generations (woman)\nCustom concept generations\nClass token generations (dog)\nFigure 12. Preservation of class label. Although our method does not enforce an explicit class preservation loss similar to prior works [24,\n35], our method is able to preserve diversity when generating images of the class label used for initialization of the custom concept token.\nWe show this across three different classes, namely: man, woman, and dog.\n"
  },
  {
    "title": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models",
    "link": "https://arxiv.org/pdf/2312.02969.pdf",
    "upvote": "11",
    "text": "Rank-without-GPT: Building GPT-Independent Listwise Rerankers on\nOpen-Source Large Language Models\nXinyu Zhang,1\u2217Sebastian Hofst\u00e4tter,2 Patrick Lewis,2 Raphael Tang,3 Jimmy Lin1\n1University of Waterloo\n2Cohere\n3Comcast Applied AI\n1{xinyucrystina.zhang, jimmylin}@uwaterloo.ca 2{sebastian, patrick}@cohere.com\n3raphael_tang@comcast.com\nAbstract\nListwise rerankers based on large language\nmodels (LLM) are the zero-shot state-of-the-\nart. However, current works in this direction\nall depend on the GPT models, making it a\nsingle point of failure in scientific reproducibil-\nity. Moreover, it raises the concern that the\ncurrent research findings only hold for GPT\nmodels but not LLM in general. In this work,\nwe lift this pre-condition and build for the first\ntime effective listwise rerankers without any\nform of dependency on GPT. Our passage re-\ntrieval experiments show that our best listwise\nreranker surpasses the listwise rerankers based\non GPT-3.5 by 13% and achieves 97% effective-\nness of the ones based on GPT-4. Our results\nalso show that the existing training datasets,\nwhich were expressly constructed for pointwise\nranking, are insufficient for building such list-\nwise rerankers. Instead, high-quality listwise\nranking data is required and crucial, calling\nfor further work on building human-annotated\nlistwise data resources.\n1\nIntroduction\nGiven a user query, the objective of text retrieval\nis to fetch a list of documents among potentially\nbillions of documents, ranked in descending order\nof relevance to the query. Mainstream solutions to\nthis problem follow a multi-stage ranking pipeline.\nIn the first stage, retrievers are designed to effi-\nciently retrieve the top-k candidates from the entire\ncollection, followed in further stages by the appli-\ncation of rerankers, which refine the ranking of the\nreturned top-k candidates.\nRerankers are traditionally constructed in a point-\nwise paradigm, where given a query, the rerankers\nproduce a relevance score for each passage inde-\npendently, and the final ranking is formed by sort-\ning passages by their relevance scores. Recently,\nattracted by the strong generative power of large\n\u2217Work is done during internship at Cohere.\nlanguage models (LLM) and their capacity to con-\nsume long-context inputs, a new paradigm of neural\nrerankers has been proposed using listwise rank-\ning (Ma et al., 2023b; Sun et al., 2023; Pradeep\net al., 2023; Tang et al., 2023). These models con-\nsume a combined list of passages at a time and\ndirectly outputs the reordered ranking list.1\nNot only does it achieve the state of the art on\ntwo TREC DL datasets (Tang et al., 2023), listwise\nranking provides a novel perspective to passage\nreranking: this new paradigm questions the neces-\nsity to convert the ranking task into a classification\ntask, and instead frames it as a pure text genera-\ntion task that could be solved end-to-end in a gen-\neralized text-to-text fashion (Raffel et al., 2020).\nFor the first time, the model directly generates the\nentire ranking list in the form of text, instead of\nrequiring multiple disjoint inference passes of the\nmodel as in pointwise (Nogueira and Cho, 2019;\nNogueira et al., 2020) or pairwise rerankers (Qin\net al., 2023; Pradeep et al., 2021). This integrates\npassage retrieval into the unified framework estab-\nlished in NLP, and thus enables it to merge seam-\nlessly with other text-to-text tasks and leverage\nexistent prompting techniques (Wei et al., 2022;\nLiu et al., 2023).\nHowever, while existing work on listwise rerank-\ning demonstrates the promising application of this\nnew ranking paradigm, their success crucially\ndepends on GPT models, either directly for the\ninference-time model (Ma et al., 2023b; Sun et al.,\n2023) or indirectly for the training-time teacher\nmodel (Pradeep et al., 2023). Such exclusive depen-\ndence results in a single point of failure in scientific\nreproducibility. Moreover, it raises the concern that\nthe current research findings are only applicable to\nthe GPT models instead of the general LLMs. In\nthis work, we seek to reduce the reliance of list-\nwise rerankers on GPT models and diversify the\n1Note that this is different from the listwise loss (Cao et al.,\n2007). See details in Section 2.2.\narXiv:2312.02969v1  [cs.CL]  5 Dec 2023\nsolution options for constructing such models. Re-\nsults show that, for the first time, our best listwise\nreranker built without any form of GPT dependence\nsurpasses the rerankers based on GPT-3.5 by 13%\nand achieves 97% effectiveness of ones based on\nGPT-4, measured by nDCG@10 on two passage\nretrieval datasets.\nIn this process, we found the current IR training\ndata, which was constructed in order to train point-\nwise rerankers, is far from sufficient for training\nlistwise rerankers (Figure 1, Section 3.1), yielding\nworse results than using data generated by BM25,\na non-neural lexical technique in IR. While silver\nranking data generated by current rerankers serves\nas a good approximation of the gold ranking, the\nperformance of listwise rerankers increases linearly\nwith training data ranking quality \u2014 a relationship\nwhich has not yet plateaued (Section 5). This indi-\ncates that the models are likely to further benefit\nfrom training data of higher quality, calling for fu-\nture work on building human-annotated datasets\npurpose-designed for listwise training.\nThe main purpose of our work is to advocate\ndiverse solutions for future listwise reranking re-\nsearch. Our contributions are as follows: (1) We\nare first to show that the listwise rerankers, with-\nout any form of dependency on the GPT models,\ncould outperform the listwise rerankers based on\nGPT-3 or 3.5 and perform on par with the ones\nbased on GPT-4; (2) We found that the ranking\nquality in the training data is crucial in construct-\ning efficient listwise rerankers, which might be the\nbottleneck of the current capacity of the listwise\nrerankers; (3) We demonstrate that listwise reranker\nfine-tuning is not data-expensive, where an effec-\ntive listwise reranker can be built using 5k queries,\neach associated with a list of passages ranked in\nhigh quality, showing that it is feasible to build a\nhuman-annotated listwise dataset for this purpose.\n2\nBackground\n2.1\nPointwise Reranking\nGiven a query q and a passage pi, the pointwise\nreranker hpw produces a real score si := hpw(q, pi)\nindicating the relevance of the passage to the\nquery. The model is optimized using cross en-\ntropy (Nogueira and Cho, 2019; Nogueira et al.,\n2020) or the contrastive loss (Gao et al., 2021;\nPradeep et al., 2022; Zhuang et al., 2023; Ma et al.,\n2023a), based on binary relevance judgments from\nhuman annotators.\nAt inference time, given the top-k passages\n{pi}k\ni=1 returned by the previous-stage retriever,\nthe model computes the relevance scores {si}k\ni=1\nfor each pi independently. The final passages are\nthen ranked by decreasing the magnitude of their\ncorresponding relevance scores.\n2.2\nListwise Reranking\nAs opposed to pointwise rerankers, which rank\npassages according to their individual predicted\nrelevance scores to the query, listwise rerankers are\ndesigned to directly predict the final ranking of a\nlist of passages as a whole, This not only allows the\nmodels to inter-reference the candidate passages\nto better determine their order, but also frames the\npassage retrieval task as text generation and thus\nfuse well with the existent techniques based on\ngenerative models. Using an LLM as a listwise\nreranker is concurrently studied in RankGPT (Sun\net al., 2023) and LRL (Ma et al., 2023b), where\nboth works use GPT-based models.\nWe formulate listwise rerankers under the same\npreliminaries as the pointwise one: given the in-\nstruction prompt s, the query q, and an input se-\nquence of top-k passages {pi}k\ni=1, the listwise-\nranking LLM hlw returns the final ranked passages\n\u02c6P := hlw(q, {pi}k\ni=1; s), where \u02c6P is a permutation\n(reranking) of {pi}k\ni=1.\nSliding window. Limited by the maximum input\nlength, we can feed only 10\u201320 passages to the\nLLM at a time. To rerank a longer list, e.g. typically\ntop-100 passages, both RankGPT and LRL adopt a\nsliding window strategy, where we slide a window\nof size n from the end to the front of the list and\nrerank the documents in the window, striding by m\ndocuments per step. In each stride, the top-(n\u2212m)\ndocuments are preserved and form the next sliding\nwindow, together with the next m documents.\nFine-tuning listwise-ranking LLMs. Used di-\nrectly out of the box, current open-source LLMs\noften generate ill-formed outputs from listwise\nprompts (Qin et al., 2023; Pradeep et al., 2023),\nwhere few valid ranking results can be inferred.\nThus, our work focuses on the condition of fine-\ntuning LLMs, which helps the models follow the\ninstructions and generate valid outputs. However,\nwe found that the current human-annotated training\ndata for IR is insufficient for this purpose, which\nwe elaborate in Section 3.1.\nDifference from listwise loss. Note that the list-\nwise ranking mentioned in this work is different\n4\n1\n2\n3\n4\n6\n5\nLabeled Relevant Documents\nUnlabeled Documents (irrelevant by default)\nPointwise Ranking Fine-tuning\nUsing Pointwise Data for Listwise Fine-tuning\n1\n2\n3\n4\n6\nQ\nIdeal Listwise Data\nPointwise Data\n1\nQ\n2\n5\n1\n6\n5\n3\n4\nQ\nWrong Ordering in Training Data\n2\n6\n5\n3\n4\n1\n2\npos\nmodel input\nmodel output\nneg\n1\nQ\nQ\nneg\npos\n4\nQuery\nLabeled order of a list of documents\nmodel input\nIdeal Listwise Fine-tuning\n1\n6\n5\nQ\n2\n6\n5\n3\n3\n4\n1\n2\nmodel output\n\u026a.\n\u026a\u026a.\n\u026a\u026a\u026a.\n\u026av.\nv.\nQuery\nCorrect Ordering in Training Data\nFigure 1: The issue with using current pointwise ranking data in listwise training. Numbers in the boxes indicate\ndifferent passages. The grey boxes indicate irrelevant passages and the green ones indicate relevant ones. The\nsaturation level indicates the relevance: the more saturating the green is, the more relevant the passages are. Boxes\nwith dash borders indicate unlabeled passages, which are considered irrelevant in the current convention. Thus, the\ngreen boxes with dash borders are the false negative passages.\nfrom the listwise loss in information retrieval (IR;\nCao et al., 2007), where models still generate the\nscore for each passage independently, although the\nloss is computed by leveraging scores of a list of\ndocuments. The term listwise in this work refers\nto that the model is capable of processing a list of\ndocuments at the same time.\n3\nMethod\n3.1\nTraining Data for Listwise Reranker\nThe difference in the output format of the two\nabove rerankers by nature requires different types\nof training data. Past experience shows that a large-\nscale professionally annotated dataset with binary\njudgments, e.g., MS MARCO (Bajaj et al., 2016),\nis sufficient in fine-tuning pointwise rerankers.\nThese pointwise datasets consist of queries, doc-\numents, and binary query\u2013document labels, anno-\ntated to denote document relevance to the query.\nUnannotated documents are considered irrelevant\nby default. (Figure 1 Block I, Block III)\nHowever, there are challenges in constructing\ngold rankings using current resources for two main\nreasons. First, there are many false-negative pas-\nsages. Taking MS MARCO as an example, which\nis the largest training data in text retrieval, there\nis on average only one labeled passage per query.\nIn a list of, say, twenty retrieved passages, only\none at most is known to be in the correct position\n(the first one), whereas the positions of the other\nnineteen are unknown. This may result in an ex-\ntremely noisy ordering. Second, true relevance is\nnuanced and graded (multilevel) rather than binary,\nas TREC evaluation sets show. Binary relevance\nignores nuances in the true relevance levels and\ndiscards the correct order of relevant passages, thus\nresulting in a suboptimal ordering of the passage\nlist. We concisely illustrate these two issues in\nFigure 1 Block IV.\nTo verify the above hypothesis that the ordering\nof the ranking list is crucial for fine-tuning listwise\nrerankers, we designed two sets of experiments:\n1. Pointwise ground truth (P-GT): We construct\na list by placing the labeled relevant documents\nin the front, which are then followed by the irrel-\nevant ones ordered arbitrarily. This is used as a\nsanity baseline, showing the effectiveness when\nonly using the human-annotated training data in\nthe pointwise ranking manner.\n2. Silver ranking: we use the ranking results of\nseveral existent ranking systems to approximate the\ngold ranking. Specifically, we select the following\nranking systems:\na) BM25: Passages are ranked by BM25 (Robert-\nson et al., 2009), a traditional unsupervised\nretrieval algorithm based on lexical matching.\nb) Fine-tuned Contriever (Contriever+ft): Pas-\nsages are ranked by Contriever (Izacard et al.,\n2021) that has been further fine-tuned on\nMS MARCO. We used the checkpoint re-\nleased by the original work.2\n2https://huggingface.co/facebook/\nc) co.rerank:\nPassages are ranked by the\nCohere rerank API.3 We used the model\nrerank-english-v2.0.\nThe ranking systems are selected with increasing\nranking capacity, and thus generating listwise train-\ning data with increasing ranking quality.\n3.2\nPrompt\nWe adopt the same prompt as RankGPT and\nRankVicuna for a fair comparison of the results:\nInput Prompt Template:\nUSER: I will provide you with {num} passages, each\nindicated by a numerical identifier []. Rank the\npassages based on their relevance to the search\nquery: {query}.\n[1] {title 1} {passage 1}\n[2] {title 2} {passage 2}\n...\n[{num}] {passage {num}}\nSearch Query: {query}.\nRank the {num} passages above based on their\nrelevance to the search query. All the passages\nshould be included and listed using identifiers, in\ndescending order of relevance. The output format\nshould be [] > [], e.g., [4] > [2]. Only respond\nwith the ranking results, do not say any word\nor explain.\nExample Completion:\n[4] > [5] > [2] > [3] > [1]\n4\nExperimental Setup\n4.1\nModels\nMost of the experiments in the work are conducted\non Code-LLaMA-Instruct (Roziere et al., 2023),4\ngiven its transparency on model weights. We exper-\niment with all released model sizes: 7B, 13B, and\n34B. In ablation studies, we compare the results\nto Vicuna-v1.5,5 another model based on Llama 2,\nbut then fine-tuned on ShareGPT, instructional data\ngenerated by GPT.\n4.2\nData\nTraining data preparation. The training data are\nprepared from MS MARCO v1 corpus (Bajaj et al.,\n2016), which contains 8.8 million passages. We\nsampled n training queries from the 100k train-\ning data of RankVicuna (n \u2208 {2k, 5k, 10k, 20k}),\nthen reordered the list of documents per query in\nthe four settings mentioned in Section 3.1.\ncontriever-msmarco\n3https://cohere.com/rerank\n4https://huggingface.co/codellama\n5https://huggingface.co/lmsys/vicuna-7b-v1.5\n(a) TREC-DL-19\nBM25\n0.497\nContriever+ft\n0.621\nco:rank\n0.735\nnDCG@10 of Teachers\n0.324\n0.508\n0.632\n0.718\nnDCG@10 of LLM\nLLM\nP-GT\n(b) TREC-DL-20\nBM25\n0.488\nContriever+ft\n0.632\nco:rank\n0.671\nnDCG@10 of Teachers\n0.246\n0.455\n0.586\n0.674\nnDCG@10 of LLM\nLLM\nP-GT\nFigure 2: nDCG@10 on TREC-DL-19 and TREC-DL-\n20 when fine-tuned on data prepared on methods de-\nscribed in Section 3.1. P-GT: Pointwise ground truth.\nEvaluation datasets. We select TREC-DL-19 and\nTREC-DL-20 (Craswell et al., 2020, 2021) to eval-\nuate the in-domain effectiveness. Both datasets\nare built from the TREC Deep Learning Track\nand share the same corpus with MS MARCO\nv1 (Bajaj et al., 2016).\nIn Section 5.4, we\nreport results reranking top-100 candidates re-\nturned by BM25 (Robertson et al., 2009) and Re-\npLLaMA (Ma et al., 2023a).\nWe report scores of nDCG@10 following the\ndataset standard. In Section 5.4, we also report\nsome results of Judged@10, the ratio of judged\npassages in the top-10 of the ranking list.\n4.3\nConfigurations\nIn this work, we use FastChat (Zheng et al., 2023)6\nfor the model training and inference. FlashAtten-\ntion (Dao et al., 2022; Dao, 2023) is applied to all\nexperiments. We turned on gradient checkpointing\nwhen fine-tuning 34B models. When not specified,\nwe fine-tune the model with batch size 128. The\nmaximum input length is set as 4,096. The fine-\ntuning epochs depend on the number of training\ndatapoints. The model is fine-tuned for 4 epochs\nwhen using 20k training data, 8 epochs when using\n10k training data, so on and on.\nIn experiments using QLoRA, we set LoRA rank\nas 64, alpha as 16, dropout rate as 0.05, maximum\n6https://github.com/lm-sys/FastChat\n(a) TREC-DL-19\n2k\n5k\n10k\n20k\nNumber of Training Datapoints\n0.60\n0.65\n0.70\nnDCG@10\n0.669\n0.703\n0.718\n0.722\nDL19\n(b) TREC-DL-20\n2k\n5k\n10k\n20k\nNumber of Training Datapoints\n0.60\n0.65\nnDCG@10\n0.643\n0.656\n0.673\n0.663\nDL20\nFigure 3: Results regarding the increasing number of\ntraining data generated by co.rerank. Dash lines refer\nto the result of co.rerank.\ngradient norm as 0.3, and a constant learning rate\nof 1e-4, all following the advice from Dettmers\net al. (2023). LoRA is applied on q_proj and\nv_proj layers. In experiments that fine-tune the\nentire LLM, we use a learning rate of 2\u00d710\u22125 with\nthe cosine learning schedule. All experiments are\nrun on 8 NVIDIA A100 GPUs with 80GB memory.\nWith QLoRA, training 7B models takes around 5h\nwhen fine-tuning 20k training data for 4 epochs.\n5\nResults and Analysis\n5.1\nTraining Data Quality\nWe first show that the current pointwise labeled\ndata alone could not serve the need of fine-tuning\ngenerative LLM as listwise rerankers. While the\nranking results produced by current rerankers could\nbe used as an approximation of the gold ranking,\nthe listwise rerankers are likely to further benefit\nfrom human-labeled listwise data in higher quality.\nFigure 2 shows the results on TREC-DL-19 and\nTREC-DL-20 of the listwise rerankers when fine-\ntuned on different training data. The x-axis is the\nnDCG@10 of the pointwise rerankers that generate\nthe training data, and the y-axis is the nDCG@10\nof the listwise rerankers fine-tuned on the corre-\nsponding data. The horizontal dash line is the result\nwhen the model is fine-tuned on the ground-truth\npointwise data only.\nClearly, listwise rerankers fine-tuned only the\npointwise data yield inferior ranking quality, evi-\ndenced by that the grey line is greatly lower than\nothers. When fine-tuned on the silver ranking data,\n(a) TREC-DL-19\n7B\n13B\n34B\nModel Size\n0.72\n0.74\nnDCG@10\n0.722\n0.737\n0.743\nDL19\n(b) TREC-DL-20\n7B\n13B\n34B\nModel Size\n0.66\n0.68\nnDCG@10\n0.674\n0.683\n0.687\nDL20\nFigure 4: Result regarding different sizes of the model,\nall fine-tuned on 10k data. Dash lines refer to the result\nof co.rerank.\nthe scores of the listwise rerankers follow closely\nto the scores of pointwise rerankers (e.g., scores\non pointwise vs. corresponding listwise reranker:\n0.497 vs. 0.508, 0.621 vs. 0.632, 0.735 vs. 0.718).\nOn one hand, this shows that the quality of rank-\nings data is crucial when fine-tuning the listwise\nrerankers; on the other hand, the listwise student\nis able to keep up with even one of the best cur-\nrent teachers without showing a trend of plateau.\nThis hints that the potential capacity of the list-\nwise rankers may not be fully excavated and may\nbe bounded by the quality of current training data.\nThat is, if higher-quality listwise training data were\navailable (e.g., by human labeling), the listwise\nrankers may show higher ranking capacity.\n5.2\nTraining Data Quantity\nHaving proved that higher-quality data is necessary\nto obtain effective listwise rerankers, we ask the\nnext question: how much data is required? Figure 3\ncompares the model effectiveness with increasing\namounts of fine-tuning data. For a fair comparison,\nthe models are fine-tuned for the same number of\nsteps when varying training data quantity: given\nthat the model is fine-tuned for 8 epochs on 10k\ndatapoints, it is then fine-tuned for 40, 16, and\n4 epochs when using 2k, 5k, and 20k datapoints,\nwhere each datapoint consists of one query and 20\npassages. Therefore, training with fewer datapoints\nonly saves the anticipated human labor effort for\nannotation but not the training time. Experiments\nare based on Code-LLaMA-Instruct in size 7B.\nModel\nGPT-\nModel\nPrevious\ntop-k\nTREC-DL-19\nTREC-DL-20\nindependent\nSize\nStage\nnDCG@10\nnDCG@10\nnon-listwise methods based on non-LLM\n(1) monoBERT\nBERT\n\u2713\n110M\nBM25\n1000\n72.3\n72.2\n(2) monoT5\nT5\n\u2713\n3B\nBM25\n100\n71.8\n68.9\n(3) rankT5\nT5\n\u2713\n3B\nBM25\n100\n71.2\n69.5\nnon-listwise methods based on LLM\n(4) UPR\nFLAN-T5-XXL\n\u2713\n11B\nBM25\n100\n62.0\n60.3\n(5) PRP-Sliding-10\nFLAN-UL2\n\u2713\n20B\nBM25\n100\n72.7\n70.5\n(6) RankLLaMA\nLLaMA\n\u2713\n7B\nRepLLaMA\n100\n75.3 (76.1)\n76.7 (76.2)\nlistwise methods\n(7) RankVicuna\nVicuna\n\u2717\n7B\nBM25\n100\n66.8\n65.5\n(8) LRL\nGPT-3\n\u2717\n?\nBM25\n100\n65.8\n62.2\n(9) RankGPT-3.5\nGPT-3.5\n\u2717\n?\nBM25\n100\n65.8\n62.9\n(10) RankGPT-4\nGPT-4\n\u2717\n?\nBM25\n100\n75.7\n71.0\n(11) Rank-wo-GPT\nCode-LLaMA-Instruct\n\u2713\n7B\nBM25\n100\n71.8 (70.8)\n67.4 (66.7)\n(12) Rank-wo-GPT\n\u2713\n7B\nRepLLaMA\n100\n73.0 (75.2)\n70.0 (71.7)\n(13) Rank-wo-GPT\n\u2713\n13B\nBM25\n100\n73.7\n68.3\n(14) Rank-wo-GPT\n\u2713\n34B\nBM25\n100\n74.3\n68.7\nTable 1: Comparison of listwise reranker fine-tuned on data generated by co.rerank to other methods in the field,\nevaluated on TREC-DL-19 and TREC-DL-20. The tilted scores in bracket are the ones evaluated on enriched\nquery\u2013passage relevance judgment, with Judged@10 = 1. Results of RankVicuna, LRL, and RankGPT-3.5 are\ncopied from the original paper (Pradeep et al., 2023; Ma et al., 2023b; Sun et al., 2023). Results of RankGPT-4\nreranking BM25 top-100 are copied from Tang et al. (2023).\nAs Figure 3 shows, training on 5k training data-\npoints already yield 97% of the effectiveness com-\npared to using 10k datapoints, whereas increasing\ndata quantity from 10k to 20k only brings marginal\nimprovement in the case of TREC-DL-19 and no\npositive effect on TREC-DL-20. That is, 100k\nhigh-quality query\u2013passage pairs (5k queries with\n20 passages per query) serve the need of effectively\nfine-tuning listwise rerankers. This is in the same\nscale with fine-tuning pointwise rerankers, where\nRankLLaMA (Ma et al., 2023a) consumes 300k\nquery\u2013passage pairs from MS MARCO.\n5.3\nModel Size\nThe experiments above are all based on Code-\nLLaMA-Instruct with size 7B. We then examine\nthe effect of scaling up the models. As expected,\nthe effectiveness of the listwise rerankers increases\nwith the language model size. Figure 4 shows the\ntrend of the ranking quality with respect to the\nmodel size, where the model of 13B already out-\nperforms the teacher, and increasing the model size\nto 34B brings additional improvement.\n5.4\nComparisons with Other Baselines\nFinally, we compare our listwise rerankers to other\nmethods in the field, evaluated on TREC-DL-19\nand TREC-DL-20. Results are shown in Table 1.\nThe baselines are grouped into three categories:\n(1) non-listwise rerankers based on non-LLM mod-\nels (e.g., BERT); (2) non-listwise rerankers based\non LLM, including methods based on query like-\nlihood (Sachan et al., 2022), pairwise (Qin et al.,\n2023) and pointwise reranking (Ma et al., 2023a);\n(3) listwise rerankers (Pradeep et al., 2023; Ma\net al., 2023b; Sun et al., 2023; Tang et al., 2023),\nwhich all depend on GPT models.\nUnlabeled top-reranked passages.\nAlthough\nTREC-DL data have comparatively dense human\njudgments,7 we observe that listwise rerankers\nbring more unlabeled passages to the top of the\nreranked list compared to the pointwise ones. For\nexample, on TREC-DL-19, the Judged@10 of list-\nwise rerankers are between 0.88 to 0.94, whereas\nthe Judged@10 of RankLLaMA is over 0.98.\nFor a fair comparison, we manually annotated\nthe missing query\u2013passage relevance judgments\nfrom the top-10 of the lists returned by some of the\nrerankers, including both pointwise and listwise\nones from rows (6, 11, 12). The labels are on the\nsame scale as the original graded judgment (i.e.,\nfrom 0 to 3, with larger numbers indicating higher\nrelevance). These added labels, together with the\ninitial ones, form the new judgment set, which we\nrefer to as \u201cenriched judgments\u201d.\n7120 judgments per query on TREC-DL-19; 211 judgments\nper query on TREC-DL-20\n30 40 50 60 70 80 90\nsition in Output\nLabel: 3\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: all\n0\n2\n4\n6\n8\n10\n12\n14\n(a) Pointwise Reranker\n30 40 50 60 70 80 90\nPosition in Output\nLabel: 2\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 3\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: all\n0\n2\n4\n6\n8\n10\n12\n14\n(b) Listwise Reranker\nFigure 5: Compare the position of relevant passages\nbefore and after reranking by RankLLaMA and Rank-\nwo-GPT both reranking RepLLaMA top-100. x-axis:\npassage positions in the reranked list; y-axis: passage\npositions in the first-stage list. Best viewed in color.\nScores evaluated on our enriched judgments set\nare italicized in parentheses.\nWe observe that\nthe added judgment made a nontrivial difference\nto the evaluation results. Most prominently, the\nnDCG@10 on row (12) increased from 73.0 to\n75.2 after filling in the missing relevance. Intact\njudgments also amend the over-rated rankings, for\nexample, on row (11), the scores decreased with\nmore labels. In the rest of this section, we compare\nresults evaluated on the enriched judgments.\nComparison to GPT-based listwise rerankers.\nComparing rows (11, 13, 14) to rows (7\u201310), we\nfound even our smallest listwise reranker (7B) is\nsignificantly higher than previous models based\non GPT-3 and GPT-3.5. Compared to RankGPT-\n4, while the listwise rerankers yield lower scores\nwith models of all sizes, the difference is again\ninsignificant (two-tailed t-test, with p < 0.01).\nComparison to LLM-based pointwise rerankers.\nWhile the pointwise rerankers are fine-tuned on the\noptimal human-annotated data, we find our listwise\nmodels, fine-tuned under data non-optimized for\nits purpose, perform close to the best pointwise\nrerankers in the same model size on TREC-DL-\n19. Comparing row (12) to row (6), where both\nlistwise and pointwise rerankers are based on the\nsame size of models (7B) and reranking the same\ncandidates from the first-stage retriever, there is\nonly a small gap between the nDCG@10 on TREC-\nDL-19, with insignificant difference (two-tailed\nt-test, with p < 0.01), although there is a larger\ngap on TREC-DL-20: 71.7 vs. 76.2 on rows (12,\n6), with a significant difference. It would require\nfuture work to close the gap between the two.\n5.5\nAnalysis on Sliding Window Strategy\nWhile the sliding window strategy is a natural resort\nto apply listwise ranking on a passage list longer\nthan the model input capacity, it is unknown yet\nhow well it aggregates the list in each pass.\nTo start answering this question, we plot the\nranking positions of relevant passages before and\nafter reranking. Figure 5 compares the position\ndifference when using the pointwise and listwise\nrerankers, the models on rows (6) and (12) in Ta-\nble 1. In each heatmap, the y-axis indicates the\npassage position in the first-stage ranking (i.e., Re-\npLLaMA) and the x-axis indicates the position after\nreranking by RankLLaMA (Figure 5a) or Rank-wo-\nGPT (Figure 5b).\nComparing the heatmaps, we observe a promi-\nnent pattern in the listwise heatmap (Figure 5b)\nthat there is a chain of bright clusters in the square\nshape along the diagonal line. This indicates that a\nlarge number of relevant documents are \u201ctrapped\u201d\nin the local block, promoted only within the cur-\nrent or the next pass of the sliding window. This\nphenomenon is common for relevant passages at\nall relevant levels (Appendix B, Figure 6).\nThe brightness density in the upper matrix indi-\ncates the frequency of relevant passages promoted\nover a long distance over the list. Compared to\npointwise, where the scatters distribute symmetri-\ncally along the diagonal matrix, listwise heatmap\nshows more scatters clustered in left-most columns,\nx \u2208 [0, 20], indicating that the top-ranked passages\nby listwise rerankers still come from a wider range\nof positions in the first-stage results compared to\nthe pointwise methods regardless that a large num-\nber of passages are trapped as aforementioned.\n6\nAblation Studies\nLLM with GPT-based instruction fine-tuning.\nTo investigate if more GPT-alike instruction fine-\ntuning would further benefit the listwise ranking\nresults, we ran the same experiment on Vicuna-\nv1.5. As shown in rows (1, 2) in Table 2, while\nfine-tuning based on Vicuna achieved slightly bet-\nter results on both datasets, the difference is not\nsignificant. Thus we conclude starting from a GPT-\nfree LLM is able to yield satisfactory effectiveness\ncompared to a more GPT-alike LLM.\nFine-tuning Full Model vs. QLoRA. In previous\nexperiments, we fine-tuned the LLM using QLoRA\ninstead of the entire LLM model to alleviate the\nGPU memory and disk requirement. Here, we\nModel\nDL-19\nDL-20\n(1)\nQLoRA\nCode-LLaMA-Instruct\n0.718\n0.674\n(2)\nQLoRA\nVicuna-v1.5\n0.728\n0.683\n(3)\nFull\nVicuna-v1.5\n0.727\n0.674\nTable 2: Results when using Code-LLaMA-Instruct and\nVicuna as the initial LLM, and when fine-tuning Vicuna\nwith QLoRA or all parameters (Full). All models are in\nsize 7B and fine-tuned on 10k datapoints for 8 epochs.\ncompared the effectiveness of the two fine-tuning\nstrategies on Vicuna.8 As shown in rows (2, 3)\nin Table 2, fine-tuning with QLoRA yields similar\neffectiveness as fine-tuning all parameters on both\ndatasets, with the same amount of training data and\nthe fine-tuning epochs.\n7\nRelated Work\nIn the past few years, the question of how gener-\native models could bring benefits to information\nretrieval has been an area of intense study, with a\nnumber of differing and complementary techniques\nemerging. The strong generative performance of\nLLMs has been leveraged for retrieval by gener-\nating a large volume of synthetic datasets on do-\nmains: InPars (Bonifacio et al., 2022; Jeronymo\net al., 2023), and Promptagator (Dai et al., 2022).\nIn parallel, researchers have investigated whether\nLLMs could be used directly as retrievers or\nrerankers: SGPT (Muennighoff, 2022) first shows\nthat the GPT-based decoder models, are effec-\ntive when used as bi-encoder in retrieval tasks.\nUPR (Sachan et al., 2022) uses the query likeli-\nhood as the ranking score. PRP (Qin et al., 2023)\nshows that the LLM can effectively determine\nthe comparative relevance regarding the query,\ngiven a pair of documents. Recently, Ma et al.\n(2023a) demonstrate that fine-tuning LLAMA in\nthe traditional paradigm of bi-encoder and point-\nwise cross-encoder surpasses smaller models.\nFinally, a line of work that is mostly relevant\nto our work regards LLMs as black boxes and\nonly uses the final generative output for rank-\ning: RankGPT (Sun et al., 2023) and LRL (Ma\net al., 2023b) studied listwise rerankers concur-\nrently, demonstrating its effectiveness using GPT-3,\nGPT-3.5, and GPT-4. RankVicuna (Pradeep et al.,\n2023) then showed that the method could be ap-\nplied to a smaller-sized open-source LLM (e.g. Vi-\n8We conducted the same experiment in Code-LLaMA-Instruct,\nhowever, the results were not in the correct scale. Thus we\nuse Vicuna as a replacement in this ablation.\ncuna (Chiang et al., 2023) in 7B, 13B) by distilling\nfrom GPT-3.5. Tang et al. (2023) proposed a permu-\ntation self-consistency prompting method, which\nalleviates the positional bias and largely improves\nthe effectiveness of the listwise ranking.\n8\nConclusions and Future Work\nIn this work, we study how to construct effective\nGPT-free listwise rerankers based on open-source\nLLM models. Experiments on two passage re-\ntrieval datasets show that our listwise rerankers,\nwithout any form of dependency on GPT, can sub-\nstantially outperform the ones built on GPT-3 and\nperform on par with the ones built on GPT-4.\nIn this process, we find that current pointwise\ntraining data in IR is not sufficient in fine-tuning\nlistwise rerankers. Instead, training data comprised\nof high-quality ranked document lists is required\nand crucial. While the training data generated by\ncurrent pointwise rerankers could be used as a nice\napproximation, the models are likely to benefit\nmore from higher-quality listwise training data that\nare built from human annotations.\nWe hope this work sets up the stage for future\nresearch on the listwise ranking methods by bring-\ning more diversity of the solutions to the research\nin this line. Additionally, we hope it paves the path\nfor future work on addressing text retrieval in the\ntext generation paradigm, where it could be format-\nted in the same way as the other text-to-text tasks,\nand thus better integrated into the unified system.\nLimitations\nDespite the new paradigm brought by listwise rank-\ning, it still has intrinsic issues that are pending to\nbe solved. The most prominent one is the query\nlatency: since the current listwise rerankers de-\npend on the sequential inference of models with\nextremely large sizes, they intrinsically suffer from\nhigher query latency than rerankers based on BERT-\nsize models.\nAcknowledgement\nWe thank Ronak Pradeep for providing the data\nof RankVicuna, Xueguang Ma for providing the\nrunfiles of RepLLaMA and RankLLaMA, and Pat\nVerga and Jiarui Xu for their helpful discussion and\nadvice on the paper.\nReferences\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. arXiv:1611.09268.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022.\nInPars: Unsupervised\ndataset generation for information retrieval. In Pro-\nceedings of the 45th International ACM SIGIR Con-\nference on Research and Development in Information\nRetrieval, pages 2387\u20132392.\nZhe Cao, Tao Qin, Tie-Yan Liu, Ming-Feng Tsai, and\nHang Li. 2007. Learning to rank: From pairwise\napproach to listwise approach. In Proceedings of the\n24th International Conference on Machine Learn-\ning, ICML \u201907, page 129\u2013136, New York, NY, USA.\nAssociation for Computing Machinery.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing GPT-4 with 90%* Chat-\nGPT quality.\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, and\nDaniel Campos. 2021. Overview of the TREC 2020\ndeep learning track. arXiv:2102.07662.\nNick\nCraswell,\nBhaskar\nMitra,\nEmine\nYilmaz,\nDaniel Campos, and Ellen M. Voorhees. 2020.\nOverview of the TREC 2019 deep learning track.\narXiv:2003.07820.\nZhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo\nNi, Jing Lu, Anton Bakalov, Kelvin Guu, Keith B\nHall, and Ming-Wei Chang. 2022.\nPromptaga-\ntor:\nFew-shot dense retrieval from 8 examples.\narXiv:2209.11755.\nTri Dao. 2023.\nFlashAttention-2:\nFaster atten-\ntion with better parallelism and work partitioning.\narXiv:2307.08691.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. QLoRA: Efficient finetun-\ning of quantized LLMs. arXiv:2305.14314.\nLuyu Gao, Zhuyun Dai, and Jamie Callan. 2021. Re-\nthink training of bert rerankers in multi-stage retrieval\npipeline. In Advances in Information Retrieval: 43rd\nEuropean Conference on IR Research, ECIR 2021,\nVirtual Event, March 28 \u2013 April 1, 2021, Proceedings,\nPart II, page 280\u2013286, Berlin, Heidelberg. Springer-\nVerlag.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021.\nUnsupervised dense\ninformation retrieval with contrastive learning.\narXiv:2112.09118.\nVitor Jeronymo, Luiz Bonifacio, Hugo Abonizio,\nMarzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and\nRodrigo Nogueira. 2023. InPars-v2: Large language\nmodels as efficient dataset generators for information\nretrieval. arXiv:2301.01820.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts. arXiv:2307.03172.\nXueguang Ma, Wang Liang, Nan Yang, Wei Furu, and\nJimmy Lin. 2023a. Fine-tuning LLaMA for multi-\nstage text retrieval. arXiv:2309.15088.\nXueguang Ma, Xinyu Zhang, Ronak Pradeep, and\nJimmy Lin. 2023b.\nZero-shot listwise docu-\nment reranking with a large language model.\narXiv:2305.02156.\nNiklas Muennighoff. 2022. SGPT: GPT sentence em-\nbeddings for semantic search. arXiv:2202.08904.\nRodrigo Nogueira and Kyunghyun Cho. 2019. Passage\nre-ranking with BERT. arXiv:1901.04085.\nRodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.\nDocument ranking with a pretrained sequence-to-\nsequence model. arXiv:2003.06713.\nRonak Pradeep, Yuqi Liu, Xinyu Zhang, Yilin Li, An-\ndrew Yates, and Jimmy Lin. 2022. Squeezing water\nfrom a stone: A bag of tricks for further improv-\ning cross-encoder effectiveness for reranking.\nIn\nAdvances in Information Retrieval, pages 655\u2013670,\nCham. Springer International Publishing.\nRonak Pradeep, Rodrigo Nogueira, and Jimmy J. Lin.\n2021. The expando-mono-duo design pattern for\ntext ranking with pretrained sequence-to-sequence\nmodels. arXiv:2101.05667.\nRonak Pradeep, Sahel Sharifymoghaddam, and Jimmy\nLin. 2023. RankVicuna: Zero-shot listwise docu-\nment reranking with open-source large language mod-\nels. arXiv:2309.15088.\nZhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang,\nJunru Wu, Jiaming Shen, Tianqi Liu, Jialu Liu, Don-\nald Metzler, Xuanhui Wang, et al. 2023. Large lan-\nguage models are effective text rankers with pairwise\nranking prompting. arXiv:2306.17563.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(1).\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode Llama: Open foundation models for code.\narXiv:2308.12950.\nDevendra Sachan, Mike Lewis, Mandar Joshi, Armen\nAghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\nZettlemoyer. 2022. Improving passage retrieval with\nzero-shot question generation. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 3781\u20133797, Abu Dhabi,\nUnited Arab Emirates.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,\nDawei Yin, and Zhaochun Ren. 2023. Is ChatGPT\ngood at search? Investigating large language models\nas re-ranking agent. arXiv:2304.09542.\nRaphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin,\nand Ferhan Ture. 2023. Found in the middle: Permu-\ntation self-consistency improves listwise ranking in\nlarge language models. arXiv:2310.07712.\nNandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. BEIR:\nA heterogeneous benchmark for zero-shot evaluation\nof information retrieval models. In Thirty-fifth Con-\nference on Neural Information Processing Systems\nDatasets and Benchmarks Track (Round 2).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824\u201324837.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging\nLLM-as-a-judge with MT-Bench and Chatbot Arena.\narXiv:2306.05685.\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui,\nJi Ma, Jing Lu, Jianmo Ni, Xuanhui Wang, and Mike\nBendersky. 2023. RankT5: Fine-tuning T5 for text\nranking with ranking losses. In Proc. of the 46th\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval (SIGIR).\nBM25\nGTR-XXL\ncpt-text-XL\nAda2\nSGPT\nRepLLaMA\nRankT5\nRankLLaMA\nRankLLaMA\nRank-wo-GPT\nModel Size\n\u2013\n4.8B\n175B\n?\n5.8G\n7B\n220M\n7B\n13B\n7B\nDBPedia\n31.8\n40.8\n43.2\n40.2\n39.9\n43.7\n44.2\n48.3\n48.7\n42.3\nFiQA\n23.6\n46.7\n51.2\n41.1\n37.2\n45.8\n44.5\n46.5\n48.1\n35.1\nNF Corpus\n32.2\n34.2\n40.7\n35.8\n36.2\n37.8\n38.1\n30.3\n28.4\n32.8\nSCIDOCS\n14.9\n16.1\n-\n18.6\n19.7\n18.1\n18.1\n17.8\n19.0\n16.2\nSciFact\n67.9\n66.2\n75.4\n73.6\n74.7\n75.6\n75.0\n73.2\n73.0\n64.7\nTREC-COVID\n59.5\n50.1\n64.9\n81.3\n87.3\n84.7\n80.7\n85.2\n86.1\n80.4\nAverage\n38.3\n42.4\n55.1\n48.4\n49.2\n51.0\n50.1\n50.2\n50.6\n45.2\nTable 3: nDCG@10 scores on BEIR subset\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 1\n0\n1\n2\n3\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 2\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 3\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: all\n0\n2\n4\n6\n8\n10\n12\n14\n(a) Pointwise Model (RankLLaMA)\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 1\n0\n1\n2\n3\n4\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 2\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: 3\n0\n1\n2\n3\n4\n5\n6\n7\n0 10 20 30 40 50 60 70 80 90\nPosition in Output\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\nPosition in Input\nLabel: all\n0\n2\n4\n6\n8\n10\n12\n14\n(b) Listwise Model (Rank-wo-GPT)\nFigure 6: The full version of Figure 5, showing a separate heatmap for documents at each relevant level. Labels\nfrom 1 to 3 indicate the passage has increasing relevance to the query. x-axis: the passage positions in the reranked\nlist; y-axis: the passage positions in the first-stage list.\nA\nOut-of-Domain Results\nIn this section, we share the out-of-domain results\non BEIR (Thakur et al., 2021) of the 7B model,\nreranking the top-100 passages returned by BM25.\nUnfortunately, we only observe an unsatisfied gen-\neralization ability of the current model. This re-\nquires future work on adapting fine-tuned listwise\nrerankers into different domains. Results are re-\nported in Table 3.\nB\nHeatmaps of Sliding Window Strategy\non All Relevance Levels\nIn Section 5.5, we share the visualization of all the\nrelevant passages positions before and after rerank-\ning. In Figure 6, we show similar visualization\nseparately for passages in different relevance level,\nshowing that the trend is similar overall.\nC\nAdditional Annotation on TREC-DL\nAs mentioned in Section 5.4, the authors manu-\nally annotated the unlabeled query\u2013passage pairs\nin top-10 returned by some rerankers. We share the\nannotation results in the following pages.\nQuery 1063750: why did the us volunterilay enter ww1\nLabel 0\n1126299 World War 1 : How did WW1 affect the United States? The United States was affected by World War 1. The U.S\nwas effected by political/social problems, racial conflicts and foreign policy issues. For example, the United States\nwas effected by Isolationism, the Red Scare, and the Red Summer. The United States was affected by foreign\npolicy issues with Isolationism.\nQuery 1114819: what is durable medical equipment consist of\nLabel 3\n1724525 List of Durable Medical Equipment Items. This is a list of items you may need to use to manage your health\nor the health of a loved one. Many of these items are covered by Medicare. Air Cleaners. Air Conditioners.\nAir-Fluidized Bed. Alternating Pressure Pads and Mattresses. Audible/Visible Signal Pacemaker Monitor.\nAugmentative Communication Device. Bathtub Lifts.\nQuery 1115776: what is an aml surveillance analyst\nLabel 0\n5483591 A professional designation awarded by the Association of Certified Anti-Money Laundering Specialists (ACAMS)\nto anti-money laundering professionals who have 40 qualifying credits based on education, other professional\ncertification and work experience, who pass the CAMS examination and who provide three professional references.\nQuery 1129237: hydrogen is a liquid below what temperature\nLabel 3\n4254816 Hydrogen is a liquid at -253 degree C and it is very near to absolute zero which is known as -273 degree C. Edit.\nShare to: 1 The Periodic Table of Elements Life is sustained by a number of chemical elements. 2 They are\nresponsible for the birth of planets and the evolution of cells to complex organisms.\nQuery 130510: definition declaratory judgment\nLabel 0\n5668642 declaration. n. 1. (Rhetoric) an explicit or emphatic statement. 2. (Rhetoric) a formal statement or announcement;\nproclamation. 3. (Rhetoric) the act of declaring. 4. (Law) the ruling of a judge or court on a question of law, esp\nin the chancery division of the High Court.\nQuery 182539: example of monotonic function\nLabel 0\n1787567 Example: the function v(x) = x3 - x2 + 4x. To move C spaces to the left, add C to x wherever x appears: w(x) = (x\n+ C)3 - (x + C)2 + 4(x + C) An easy way to remember what happens to the graph when we add a constant: add to\ny to go high. add to x to go left.\nQuery 405717: is cdg airport in main paris\nLabel 1\n7288990 Charles de Gaulle Airport is located within portions of several communes 25 km (16 mi) to the northeast of Paris.\nCharles de Gaulle Airport serves as the principal hub for Air France as well as a focus city for low-cost carriers\nVueling and Norwegian Air Shuttle.\nQuery 87452: causes of military suicide\nLabel 1\n8333913 But the military will have to change the way it handles soldiers with mental illness if we expect to see the number\nof suicides decline, according to the author of a related editorial published in the journal. VA boss: \u2019No veteran\nshould have to wait for claims\u2019.\nQuery 1106007: define visceral?\nLabel 3\n746358\nVisceral, which means \u201cpertaining to the soft organs in the abdomen,\u201d is the fat stored deep in our abdomens\naround the intestines, kidneys, pancreas and liver. This is the stuff that tends to make our tummies protrude in\nclassic \u201cbeer belly\u201d fashion.\nLabel 0\n2186171 Visceral (Smooth) Muscle. Visceral muscle is found in various parts of the body including blood vessels, the\nbladder, digestive tract, as well as in many other hollow organs. Like cardiac muscle, most visceral muscle is\nregulated by the autonomic nervous system and is under involuntary control.\nQuery 87181: causes of left ventricular hypertrophy\nLabel 3\n3363613 An enlarged left ventricle is usually due to either cardiomyopathy (a weakening of the heart muscle) or valvular\nheart disease. Valve disease is usually treated by correcting the underlying valve problem (initially with medicines,\noften later with surgery). The most common cause of an enlarged left ventricle is cardiomyopathy. Initial treatment\nis with medications, such as diuretics, digitalis, vasodilators (ACE inhibitors and/or ARB inhibitors), and beta\nblockers, such as carvedilol (Coreg) or metoprolol (Lopressor, Toprol XL).\nLabel 0\n3216319 However, high blood pressure, or hypertension, is the most common cause by far. Higher pressures within blood\nvessels put a strain on the ventricle, making it difficult to satisfy the body\u2019s needs. Also, narrowing of the aortic\nvalve, or stenosis, obstructs blood flow from the left ventricle. In patients with HCM, the ventricle can become\nthick and stiff, which adds extra strain on the heart.\nQuery 527433: types of dysarthria from cerebral palsy\nLabel 0\n2722036 Cerebral palsy is a group of disorders that can involve brain and nervous system functions, such as movement,\nlearning, hearing, seeing, and thinking. There are several different types of cerebral palsy, including spastic,\ndyskinetic, ataxic, hypotonic, and mixed.\n6015377 Dysarthria is a motor speech disorder resulting from neurological injury of the motor component of the motor-\nspeech system and is characterized by poor articulation of phonemes (cf. aphasia: a disorder of the content of\nlanguage).\n7748146 Cerebral palsy is a general term describing conditions that cause movement problems. The most common type\nis spastic cerebral palsy where the muscles are stiff and rigid in one or more limbs. The underlying problem is\ndamage to, or faulty development of, part of the brain.\nQuery 148538: difference between rn and bsn\nLabel 3\n1516299 A registered nurse (RN) is a nurse who has passed the National Council Licensure Examination for Registered\nNurses (NCLEX-RN) after completing an accredited training program. These programs come in the form of an\nAssociate of Science in Nursing (ASN) or a Bachelor of Science in Nursing (BSN). The degrees differ in the time\nit takes to complete them and the type of coursework required for graduation.\n332406\nRN vs. BSN: education requirements. The first thing you should know is that becoming an RN means passing\nthe NCLEX exam. There is no way around it. To be eligible to sit for the NCLEX you must first earn either an\nassociate degree in nursing (ADN) or a Bachelor of Science in nursing (BSN).The former requires completion of\na 21-month program at a career-focused college.he first thing you should know is that becoming an RN means\npassing the NCLEX exam. There is no way around it. To be eligible to sit for the NCLEX you must first earn\neither an associate degree in nursing (ADN) or a Bachelor of Science in nursing (BSN).\n4539659 That additional level of education means that BSNs generally have more opportunities and larger salaries than\nRNs. Getting your BSN isn\u2019t just something you do for yourself, though. It also benefits your patients. Studies\nhave shown that BSNs have lower patient mortality rates and lower failure to rescue rates.\nLabel 1\n7696609 IU Online RN-BSN Mobility Option. The RN to BSN Degree Completion program is designed for the RN to\nreceive a Bachelor of Science degree in nursing (BSN). The program offers a flexible curriculum for the education\nof professional nurses competent in meeting the current and future health needs of society.\nQuery 1037798: who is robert gray\nLabel 0\n2360245 John Gray (philosopher) John Nicholas Gray (born 17 April 1948) is an English political philosopher with interests\nin analytic philosophy and the history of ideas. He retired in 2008 as School Professor of European Thought at\nthe London School of Economics and Political Science.\n2360247 John Gray (I) John Gray was born in Brooklyn, New York, USA. He is a writer, director and producer, known for\nReckless (2014) White Irish Drinkers (2010), Ghost Whisperer (2005), The Hunley (1999) Martin and Lewis, and\nHelter Skelter. He was the youngest recipient ever of an American Film Institute Independant Filmmaker Grant.\n2868740 (Redirected from Gary Leroi Gray) Gary LeRoi Gray (born February 12, 1987) is an actor and voice actor involved\nwith movies, television, and animation. He is most recognized for his childhood role as Nelson Tibideaux, the son\nof Sondra Huxtable Tibideaux and Elvin Tibideaux on the NBC sitcom The Cosby Show. He appeared on the\nseries during its eighth and final season (1991\u20131992).\n3914692 George Gray (born March 11, 1967 in Ballwin, Missouri) is an American Game Show Host, announcer, and\ncomedian. He attended High School in Tucson, Arizona and graduated from the University of Arizona, during\nwhich time he played drums with a local band named The Reason Why with two of his friends.\n3986292 Robert Michael Bob Parr (born June 25, 1957) is an Emmy Award-winning English-born New Zealander, television\npersonality, private security contractor, counter-terrorism expert and former UK Special Forces operator best\nknown for his role as Team Leader and Executive Producer of Shadow Force on the History Channel.\nQuery 1110199: what is wifi vs bluetooth\nLabel 0\n291162\nWi-Fi (or WiFi) is a local area wireless computer networking technology that allows electronic devices to network,\nmainly using the 2.4 gigahertz (12 cm) UHF and 5 gigahertz (6 cm) SHF ISM radio bands. wireless router allows\nwired and wireless Ethernet LAN devices to connect to a (usually) single WAN device such as a cable modem or\na DSL modem. A wireless router allows all three devices, mainly the access point and router, to be configured\nthrough one central utility.\n3647394 Electrical & Electronic Engineer & Hobbyist, Blogger, Gamer and Tech Enthusiast. If you want to connect your\nphone/laptop/tablet/pc to a WiFi router, using bluetooth, you can\u2019t do that. They might share the same frequency\nbut they can\u2019t be connected with each other.\n421\nWi-Fi or WiFi is a technology that allows electronic devices to connect to a wireless LAN (WLAN) network,\nmainly using the 2.4 gigahertz (12 cm) UHF and 5 gigahertz (6 cm) SHF ISM radio bands.\n68578\nWi-Fi (or WiFi) is a local area wireless computer networking technology that allows electronic devices to network,\nmainly using the 2.4 gigahertz (12 cm) UHF and 5 gigahertz (6 cm) SHF ISM radio bands.\n7842393 Wi-Fi is a wireless local area network that enables portable computing devices to connect easily to the Internet.\nStandardized as IEEE 802.11 a,b,g,n, Wi-Fi approaches speeds of some types of wired Ethernet. Wi-Fi has\nbecome the de facto standard for access in private homes, within offices, and at public hotspots.\nQuery 19335: anthropological definition of environment\nLabel 3\n8828697 Environment. The environment is the world in which the organization operates, and includes conditions that\ninfluence the organization such as economic, social-cultural, legal-political, technological, and natural environment\nconditions. Environments are often described as either stable or dynamic.\nLabel 2\n6512137 1 Meaning of environment. Environment means everything around to a living being. Especially the circumstances\nof life of people or society in their life conditions. It comprises the set of natural, social and cultural values\nexisting in a place and at a particular time, that influence in the life of the human being and in the generations to\ncome.\n7050803 The environment is the surroundings of an organism including the physical and chemical environment, and other\norganisms with which it comes into contact. This term is most frequently used in a human context, often referring\nto factors affecting our quality of life. Natural Resources.\nLabel 0\n2250973 Environment. The term environment refers to an organization s natural and human surroundings. An organization\ns environment extends from within the organization itself to the global system, and includes air, water, land, flora,\nand fauna (including people), and natural resources of all kinds.\n6902386 An environment is the combination of all of the physical, chemical, and biological factors acting upon an organism\nor an ecological community. The interaction of these factors determines the form and survival of living things and\nof the environment itself.\nQuery 443396: lps laws definition\nLabel 1\n4526744 First: LPS for Protection for buildings and installations against direct strike by lightning. This type of LPS\nprotects the building from damage by direct strike lightning but doesn\u2019t prevent the lightning striking the building.\nThis type of LPS can be divided into:-. Conventional lightning protection system,\nLabel 0\n119472\nLipopolysaccharide (LPS) is the major component of the outer leaflet of the outer membrane of Gram-negative\nbacteria. The LPS molecule is composed of two biosynthetic entities: the lipid A-core and the O-polysaccharide\n(O-antigen).\n2143845 A foreign limited partnership is a partnership formed under the laws of any state other than this state or under the\nlaws of a foreign country and having one or more general partners and one or more limited partners.o register\ncancellation or dissolution of the limited partnership, the company must complete Limited Partnership Certificate\nof Cancellation (Secretary of State Form LP-4/7) and submit it to the Secretary of State.\n2143847 DEFINITION of \u2019Limited Partnership-LP\u2019. Two or more partners united to conduct a business jointly, and in which\none or more of the partners is liable only to the extent of the amount of money that partner has invested.Limited\npartners do not receive dividends, but enjoy direct access to the flow of income and expenses.EFINITION of\n\u2019Limited Partnership-LP\u2019. Two or more partners united to conduct a business jointly, and in which one or more of\nthe partners is liable only to the extent of the amount of money that partner has invested. Limited partners do not\nreceive dividends, but enjoy direct access to the flow of income and expenses.\n6863145 The Last Planner System is a production planning system designed to produce predictable work flow and rapid\nlearning in programming, design, construction and commissioning of projects. The Last Planner System (LPS) is\ndeveloped by Glenn Ballard and Greg Howell as a production planning and control system to assist in smoothing\nvariations in construction work flow, developing planning foresight, and reducing uncertainty in construction\noperations. The\nQuery 1121709: what are the three percenters?\nLabel 1\n3423064 A few things you should consider... Posted on April 13, 2014 by David Robertson in Constitution Issues // 2\nComments. There have been a lot of questions lately in regard to the term \u201cThree Percenter\u201d. The term comes up\nevery once in awhile, especially when perceived injustices are committed by the federal government.\n3423065 A few things you should consider... There have been a lot of questions lately in regard to the term \u201cThree\nPercenter\u201d. The term comes up every once in awhile, especially when perceived injustices are committed by the\nfederal government.\nLabel 0\n3518653 what is bond rate \u2013 the cost of the bond can depend on several factors. The first, of course, is the type of a bond.\nFor a performance bond, the general starting rate is three percent (3%) of the contract. This amount will go lower\nas the contract size grows. For other bonds, the price is typically lower.\n4836900 The Internal Revenue Service charges interest when you owe taxes and penalties on your federal tax return. The\namount of interest the IRS charges on a tax liability changes quarterly. See below for the most current IRS interest\nrates. Beginning April 1, 2015, the interest rates for the IRS are:1 three (3) percent for overpayments [two (2)\npercent in the case of a corporation]; 2 three (3) percent for underpayments; 3 five (5) percent for large corporate\nunderpayments; and.eginning April 1, 2015, the interest rates for the IRS are: 1 three (3) percent for overpayments\n[two (2) percent in the case of a corporation]; 2 three (3) percent for underpayments;\n6812164 Booz Allen has easy access to what Ruttenbur calls the three-letter agencies, including the FBI, the CIA and the\nDepartment of Defense, giving the company an especially bright outlook for 2016. BAH stock has gained about\n12 percent this year.\n7370525 If you have three percent inflation, the central bank can push the policy rate down to one percent, which translates\nto a negative two percent in real terms. A zero nominal rate would be a powerful negative three percent with three\npercent inflation. But if inflation is zero, the real rate can be no lower than the nominal rate, hence the \u201czero\nbound.\u201d We hear chatter that the ECB may be contemplating a negative nominal rate to deal with this problem.\nQuery 489204: right pelvic pain causes\nLabel 3\n2268652 Acute (or sharp) pain starts suddenly and often has a single cause. This type of pain may be a warning that\nsomething is wrong. Some causes of acute pelvic pain include: An infection of your uterus, fallopian tubes,\nor ovaries; this condition is called pelvic inflammatory disease (PID). An infection of the urethra, bladder, or\nkidneys.\n4279451 A common example of chronic pelvic pain is dysmenorrhea or menstrual cramps. Other causes of chronic pelvic\npain include endometriosis, adenomyosis, and ovulation pain. Sometimes an illness starts with intermittent pelvic\npain that becomes constant over time, this is often a signal that the problem has become worse.\n6327177 Some of the more common sources of acute pelvic pain, or pain that occurs very suddenly, may include: 1\nectopic pregnancy: a pregnancy that occurs outside the uterus. 2 pelvic inflammatory disease: an infection of the\nreproductive organs.3 twisted or ruptured ovarian cyst. 4 miscarriage or threatened miscarriage.elvic pain may\nhave multiple causes, including: 1 inflammation or direct irritation of nerves caused by acute or chronic trauma,\nfibrosis, pressure or intraperitoneal inflammation. 2 muscular contractions or cramps of both smooth and skeletal\nmuscles. 3 psychogenic factors, which can cause or aggravate pain.\n6665319 Any of these events can cause adhesions to form, which may later cause chronic pelvic pain. Infection and\nInflammation. Bladder, vaginal and yeast infections, and inflammations from chlamydia, pelvic inflammatory\ndisease (PID) or endometriosis may also cause chronic pelvic pain. The body\u2019s healing response to all of these\nconditions is to create adhesions.\n8781574 Female pelvic pain is typically caused by a medical condition involving the reproductive organs, urinary tract,\nlower gastrointestinal tract, or muscles of the abdominal wall. Some causes are always short-term (acute), and\nothers can become long-lasting (chronic) unless successfully treated.\n8781575 8 Common Causes of Feminine Pelvic Pain. 2. Endometriosis. Approximately 5 million women suffer with\nendometriosis, a chronic condition in which cells grow and spread outside the uterus and painfully break down\nwhen the uterine lining is shed during your monthly menstrual period. The symptoms of endometriosis include\npainful abdominal cramps as well as pain in the lower back and legs.\nLabel 0\n533017\nCauses, Diagnosis And Treatment. Right side abdominal pain is commonly caused by conditions such as\nappendicitis, gallstones, kidney stones, constipation, ectopic pregnancy, ovarian cyst troubles, endometriosis,\nCrohn\u2019s disease, ulcerative colitis, trapped wind, kidney infection, pulled muscles, hepatitis, and a number of\nother rarer diseases.\nQuery 1112341: what is the daily life of thai people\nLabel 2\n3118343 Twice a day, at 08:00 and again at 18:00, the national anthem is played by all Thai media outlets. Thais stop what\nthey are doing and stand at attention to pay homage to the flag during the anthem. Students in school stand in\nfront of the raised flag and sing the national anthem at 08:00 every school day.\n7079776 In the evening, the Thai style of eating often includes a variety of dishes including a combination of flavors and\ntextures (for example, a stir fried dish, a vegetable dish, a soup and fish) served family-style with each person\nscooping up a little food to put in their own bowls of steamed rice.\nLabel 1\n3524227 Thai cuisine is essentially a marriage of centuries-old Eastern and Western influences harmoniously combined into\nsomething uniquely Thai. Characteristics of Thai food depend on who cooks it, for whom it is cooked, for what\noccasion, and where it is cooked. Dishes can be refined and adjusted to suit all palates. Originally, Thai cooking\nreflected the characteristics of a waterborne lifestyle. Aquatic animals, plant and herbs were major ingredients.\n8606161 There are over 29,000 temples in Thailand and the daily routine of the monks in all of them is pretty much the\nsame. 4.00 am-The monks wake up and meditate for one hour, followed by one hour of chanting.6.00 am-The\nmonks walk barefoot around the neighbourhood while the local people make merit by offering them food.his is\nthe last solid food they are allowed to consume until sunrise the following morning. 1.00 pm-Classes in Buddhist\nteaching begin. Some monks may attend school outside the temple. 6.00 pm-A two-hour session of meditation\nand prayer begins. 8.00 pm-The monks retire to do homework.\nLabel 0\n3493341 Food and Economy. Food in Daily Life. Rice is the staple of the daily diet, along with manioc and other root\nvegetables, plantains, fresh and dried fish, and milk from grated coconuts. Food taboos provide a way to establish\nconnections and acknowledge identity.\n4461904 Food in Daily Life. Bread and potatoes are the traditional staple foods. Most meals include, pork, chicken, or\nbeef, and Seafood is popular in the northern part of the country. The national drink is beer, but wine is imported in\nlarge quantities. In northern cities, popular dishes include mussels with fries and waterzooi a broth of vegetables\nand meat or fish.\n4927748 What we want are places that serve authentic and delicious Thai food, where the Thais themselves love to eat. We\nhave been in Bangkok several times, often staying for long periods of a few months at a time, and have tried a lot\nof Thai restaurants both indoors and on the streets.\n7079779 However, Thai restaurants in the United States often serve soup and salad first to match Western norms. Noodle\ndishes are the sandwiches of Thai cuisine. Pad Thai, pad see ew and noodle soups are what you grab for lunch or\nfor a quick solo dinner. In the United States, they\u2019re on the dinner menus of Thai restaurants, but that\u2019s not their\ntraditional role. If you\u2019re looking for a salt shaker, use prik nam pla.\n8139250 Daily Life of a Luk Thep Doll in Thailand November 20, 2017 by Sukruti Anah Staneley Petch is Thailand\u2019s first\nknown Luk Thep doll \u2013 a plastic doll manufactured in the way any doll might be. The only difference being that\nthe doll has a living soul, believed to change people\u2019s lives; in Mama Ning\u2019s case, it was the spirit of her son.\nQuery 1113437: what is physical description of spruce\nLabel 3\n4473157 The white spruce is a large coniferous evergreen tree which grows normally to 15 to 30 metres (49 to 98 ft) tall,\nbut can grow up to 40 m (130 ft) tall with a trunk diameter of up to 1 m (3.3 ft).The bark is thin and scaly, flaking\noff in small circular plates 5 to 10 centimetres (2.0 to 3.9 in) across.he cones are pendulous, slender, cylindrical, 3\nto 7 cm (1.2 to 2.8 in) long and 1.5 cm (0.59 in) wide when closed, opening to 2.5 cm (0.98 in) broad. They have\nthin, flexible scales 15 mm (0.59 in) long, with a smoothly rounded margin. They are green or reddish, maturing\nto pale brown 4 to 8 months after pollination.\n4498867 Spruces are large trees, from about 20\u201360 metres (about 60\u2013200 feet) tall when mature, and can be distinguished\nby their whorled branches and conical form. The needles, or leaves, of spruce trees are attached singly to the\nbranches in a spiral fashion, each needle on a small peg-like structure called a pulvinus.hey are also used by the\nlarvae of gall adelgids (Adelges species). In the mountains of western Sweden scientists have found a Norway\nspruce tree, nicknamed Old Tjikko, which by reproducing through layering has reached an age of 9,550 years and\nis claimed to be the world\u2019s oldest known living tree.\n5514586 The white spruce is a large coniferous evergreen tree which grows normally to 15 to 30 m (50 to 100 ft) tall, but\ncan grow up to 40 m (130 ft) tall with a trunk diameter of up to 1 m (3.3 ft). The bark is thin and scaly, flaking off\nin small circular plates 5 to 10 cm (2 to 4 in) across.\n5938420 39 Species with 604 Trinomials. Picea a genus of about 35 species of coniferous evergreen trees in the family\nPinaceae, found in the northern temperate and boreal regions of the earth.Spruces are large trees, from 20\u201360\nmetres (66\u2013200 ft) tall when mature, and can be distinguished by their whorled branches and conical form.or the\nsame reason, some (particularly Picea abies and Picea omorika) are also extensively used as Christmas trees. The\nword \u201cspruce\u201d entered the English language from Old French Pruce, the name of Prussia.\n5938425 Seeds. The white spruce is a large coniferous evergreen tree which grows normally to 15 to 30 metres (49 to 98 ft)\ntall, but can grow up to 40 m (130 ft) tall with a trunk diameter of up to 1 m (3.3 ft). The bark is thin and scaly,\nflaking off in small circular plates 5 to 10 centimetres (2.0 to 3.9 in) across.he bark of mature white spruce is scaly\nor flaky, gray-brown (Brayshaw 1960) or ash-brown (Harlow and Harrar 1950), but silvery when freshly exposed.\nResin blisters are normally lacking, but the Porsild spruce Picea glauca var.\n8128793 Common Name: Blue Spruce, Colorado Spruce. Picea abies Common Name: Norway Spruce . About the Spruce\nTree . Spruce trees are coniferous (needle-leafed) evergreens. There are about 35 species in the genus Picea,\nnative to the northern hemisphere. The name spruce is from the Latin pix, meaning pitch. These trees often live to\na very old age. The oldest spruce on record is a Picea engelmannii that reached 852 years and is still thriving.\nLabel 2\n1896943 Norway spruce is a large, fast-growing evergreen coniferous tree growing 35\u201355 m (115\u2013180 ft) tall and with a\ntrunk diameter of 1 to 1.5 m (39 to 59 in). It can grow fast when young, up to 1 m (3 ft) per year for the first 25\nyears under good conditions, but becomes slower once over 20 m (65 ft) tall.\n742042\nCommon Name(s): Spruce Pine. Scientific Name: Pinus glabra. Distribution: Southeastern United States (coastal\nplain) Tree Size: 65-100 ft (20-30 m) tall, 2-3 ft (.6-1 m) trunk diameter. Average Dried Weight: 33 lbs/ft3 (525\nkg/m3) Specific Gravity (Basic, 12% MC): .42, .52. Janka Hardness: 700 lbf (3,110 N)\nLabel 0\n8503451 Spruce trees and shrubs are classified in the genus Picea, which includes 35 species. It is considered to be part of\nthe Pinaceae family, which also includes pine trees, fir trees, cedars, hemlocks, larches and a few other species.\nQuery 855410: what is theraderm used for\nLabel 3\n8651773 Thera-Derm topical. Uses. This medication is used as a moisturizer to treat or prevent dry, rough, scaly, itchy skin\nand minor skin irritations (e.g., diaper rash, skin burns from radiation therapy). Emollients are substances that\nsoften and moisturize the skin and decrease itching and flaking. Some products (e.g., zinc oxide, white petrolatum)\nare used mostly to protect the skin against irritation (e.g., from wetness). Dry skin is caused by a loss of water in\nthe upper layer of the skin.\nLabel 0\n3113274 For example, if I pointed to my watch and used the wh-q facial expression it would mean that I\u2019m asking you\nWhat time is it? You\u2019ve probably already used this sign many times in your life. WHAT (huh version) In the\npicture below I\u2019m doing a general gesture for what while using the WHAT facial expression.\n3618639 It\u2019s used to treat actinic keratosis, as well as acne, rosacea, skin cancer, sun damage, oily skin, wrinkles, warts,\npsoriasis, and enlarged sebaceous glands. Psoriasis Psoriasis is a long-term skin condition that may cause large\nplaques of red, raised skin, flakes of dry skin, and skin scales.\n4492180 Hereby is used in connection with what is present with the speaker or what the speaker has just mentioned or the\nspeaker\u2019s present words (i.e. the present document); thereby is used in connection with what is at a distance from\nthe speaker or what has been mentioned by the speaker or by someone else.\n4708042 Tresaderm is one of the most commonly prescribed pet drugs on the market today. It is used to treat ear and skin\nconditions even when the underlying cause is not known. Only Tresaderm contains the unique combination of\nactive ingredients that makes it the proven solution for dogs and cats, including: 1 Antifungal thiabendazole \u2013\ncontrols the most common forms of fungus that cause infection.\n4708045 Tresaderm is sometimes prescribed by vets to treat bacterial, inflammatory and/or fungal skin infections in dogs\nas well as inflammation of the ear. The medicine is sold as a liquid solution which contains dexamethasone\nneomycin sulfate and thiabendazole.\n4708046 Dermatologic Solution TRESADERM is indicated as an aid in the treatment of certain bacterial, mycotic, and\ninflammatory dermatoses as well as otitis externa in dogs and cats.\n4708047 When you should give your pet Tresaderm: Tresaderm is a dermatologic solution indicated as an aid in the\ntreatment of certain bacterial, fungal and inflammatory skin disorder and otitis externa in dogs and cats. How\nTresaderm should be used: Clean the affected area prior to use. The amount of medication to apply and the\nfrequency of treatment are dependent on the severity and extent of the lesions. The typical dose for the ear is 5 to\n15 drops instilled into the ear twice a day.\n5550193 Use the language of coping and the strategies listed in the above categories, which tell us what to do and what\nnot to do. 1 Be aware of the coping strategies you use, and evaluate them honestly. 2 Decide what sort of social\nrole model you are. 3 Expand your coping skills.se the language of coping and the strategies listed in the above\ncategories, which tell us what to do and what not to do. 1 Be aware of the coping strategies you use, and evaluate\nthem honestly. 2 Decide what sort of social role model you are. 3 Expand your coping skills.\n7066410 CONTACT US. Want a quick way to see what GPO\u2019s are applied to your local system, just using built in utilities?\nUsing the GUI to manually view what settings are applied is awkward and slow. ?Use the following commands to\nsee what policies are being handed down to the system you\u2019re on and what they\u2019re enforcing.\n8539176 To be effective using an interface you\u2019ve designed, people must be able to recognize what it is, care about why\nthey would use it, understand what the interface is helping them interact with, predict what will happen when they\nuse it, and then successfully interact with it.\n8651778 What is Tresaderm: Tresaderm is a dermatologic solution used in dogs and cats as an aid in the treatment of\ncertain acute or chronic bacterial, fungal and inflammatory skin disorders as well as otitis externa. Tresaderm may\nalso be used for purposes other than those listed.\nQuery 1051399: who sings monk theme song\nLabel 0\n2894408\nThiele then contacted Curtis Stigers to write lyrics and sing the theme, who recorded the song at\nCunningham Audio Production in Boise, Idaho. Show creator Kurt Sutter also cowrote the lyrics to the\nsong. When speaking about why he cowrote the theme for the show, Kushner stated that:\nQuery 1108651: what the best way to get clothes white\nLabel 0\n4304301\nSave. Laundry detergent is what keeps our clothes and linens looking their best. Detergent comes in\nvarious formulas to whiten whites, brighten brights, smell fresh or have no smell at all, even be completely\nhypoallergenic.\nQuery 1122767: what amino produces carnitine\nLabel 0\n8039587\nCarnitine is a quaternary ammonium compound involved in metabolism in most mammals, plants and\nsome bacteria. Carnitine may exist in two isomers, labeled D-carnitine and L-carnitine, as they are\noptically active. At room temperature, pure carnitine is a white powder, and a water-soluble zwitterion\nwith low toxicity. Carnitine only exists in animals as the L-enantiomer, and D-carnitine is toxic because it\ninhibits the activity of L-carnitine. Carnitine was discovered in 1905 as a result of its hi\nQuery 324585: how much money do motivational speakers make\nLabel 0\n8260792\nAccording to a 2013 survey (.pdf file) of 94,000 Etsy sellers, 81% said they opened their shops as a\ncreative outlet, while 68% cited supplemental income as a motivating factor. \u201cThey started because they\nlove it, and then they figured out how to make money from doing what they love,\u201d Katrina adds.\nQuery 332593: how often to button quail lay eggs\nLabel 0\n8749230\nIn the right conditions, each hen can lay 300+ per year. They begin laying eggs in 6 to 8 weeks and the\neggs only take 17 days to incubate. Because Coturnix quail mature extremely quickly, the waiting period\nto enjoy the fruits of your labors are very quick.\nQuery 42255: average salary for dental hygienist in nebraska\nLabel 0\n5783300\nHow much does a dental hygienist make? According to the May 2012 records from the Bureau of Labor\nStatistics, the mean annual dental hygienist salary is $70,700, which is equivalent to a mean hourly wage\nof $33.99. The bottom ten percent in the occupation earns an annual salary of $46,540, while the top\nten percent make an average $96,280 each year. Dental hygienists employed in dental offices earn an\naverage $71,000, while those employed in physician offices make significantly less at a mean salary of\n$65,310. Within the United States, the highest paid dental hygienists are employed in California, where\nthey earn an annual salary of $93,280. Following closely behind, those in Nevada make $91,350 and\ndental hygienists in Washington make an annual mean salary of $90,540.\nQuery 555530: what are best foods to lower cholesterol\nLabel 2\n6221470\nTop 8 Cholesterol-Lowering Foods. From oats to walnuts, a handful of everyday foods are stepping up to\nthe plate when it comes to battling unhealthy cholesterol.\nQuery 701453: what is a statutory deed\nLabel 1\n8626142\nWith a Statutory Warranty Deed, the seller expressly guarantees the grantor\u2019s good, clear title, and often\ncarries the covenants concerning quality such as quiet enjoyment, the right to convey, freedom from\nencumbrances, and defense of title from all claims.\nQuery 914916: what type of tissue are bronchioles\nLabel 0\n5061651\nThe bronchial tubes divide into smaller air passages (bronchi), and then into bronchioles. The bronchioles\nend in tiny air sacs called alveoli, where oxygen and carbon dioxide are exchanged. After absorbing\noxygen, the blood leaves the lungs and is carried to the heart. Then, it is pumped through your body to\nprovide oxygen to the cells of your tissues and organs.\nQuery 118440: define bmt medical\nLabel 0\n228375\nArmy Basic Training (BMT). Today, the United States Army basic training only takes 9 weeks to transition\nall you young men and women out there from being a civilian into that of a soldier.\n3368681\nBasic Military Training, or BMT, is the first step in beginning your Air Force career. There is no separate\nboot camp for Air Force Reservists. You will train with active-duty and Air National Guard members.\nAir Force BMT is located at Joint Base San Antonio, part of the former Lackland Air Force Base, in San\nAntonio, Texas.\nQuery 23849: are naturalization records public information\nLabel 0\n1449785\nPublic records are in fact a wide variety of records that are being kept by the government and are open to\nthe public under the freedom of information act. They include marriage records, divorce records, court\nrecords, birth records, death records and many more. In Scotts, MI records are kept mostly on physical\nfiles. There is a great rise in the use of electronic copies that are available on the internet. You are advised\nto look into entries of each record to find out more information on how to obtain them.\n5925321\nByron Public Records. Public records are in fact a wide variety of records that are being kept by the\ngovernment and are open to the public under the freedom of information act. They include marriage\nrecords, divorce records, court records, birth records, death records and many more. In Byron, OK records\nare kept mostly on physical files. There is a great rise in the use of electronic copies that are available on\nthe internet.\n7431268\nYou\u2019ve probably heard the term public record before. It\u2019s a document, or record of information, recorded\nby a court or governmental agency, that\u2019s made available to the public. Some public records, such as tax\nliens, may appear on your credit report while others, like a marriage license, do not.\nQuery 877809: what metal are hip replacements made of\nLabel 0\n2213765\nTotal hip replacement surgery uses metal, ceramic, or plastic parts to replace the ball at the upper end of the\nthighbone (femur) and resurface the hip socket in the pelvic bone. Total hip replacement surgery replaces\ndamaged cartilage with new joint material in a step-by-step process.Doctors may attach replacement joints\nto the bones with or without cement. 1 Cemented joints are attached to the existing bone with cement,\nwhich acts as a glue and attaches the artificial joint to the bone.ingle mini-incision total hip replacement\nfor the management of arthritic disease of the hip: A systematic review and meta-analysis of randomized\ncontrolled trials. Journal of Bone and Joint Surgery, American Version, 94(20): 1897-1905.\n3328100\nTotal hip replacement surgery uses metal, ceramic, or plastic parts to replace the ball at the upper end of the\nthighbone (femur) and resurface the hip socket in the pelvic bone. Total hip replacement surgery replaces\ndamaged cartilage with new joint material in a step-by-step process. Doctors may attach replacement\njoints to the bones with or without cement. Cemented joints are attached to the existing bone with cement,\nwhich acts as a glue and attaches the artificial joint to the bone.\n5850349\nHip Replacement Surgery. Guide. Total joint replacement involves surgery to replace the ends of both\nbones in a damaged joint to create new joint surfaces. Total hip replacement surgery uses metal, ceramic,\nor plastic parts to replace the ball at the upper end of the thighbone (femur) and resurface the hip socket\nin the pelvic bone. Total hip replacement surgery replaces damaged cartilage with new joint material\nin a step-by-step process . Doctors may attach replacement joints to the bones with or without cement.\nCemented joints are attached to the existing bone with cement, which acts as a glue and attaches the\nartificial joint to the bone.\nQuery 1121353: what can you do about discrimination in the workplace in oklahoma city\nLabel 1\n1727596\n1 Communicate clearly with your employees about what type of behavior is lawful and appropriate in the\nworkplace with regard to all forms of discrimination and harassment, including gender identity, and about\nwhat consequences your employees will face if they violate the law.\n4593222\nWorkplace Discrimination Laws and Policies. Under the patchwork of state and local employment law\nthat prohibits employment discrimination based on gender identity and sexual orientation more than three\nof every five citizens live in jurisdictions that do not provide such protections, and they are needed.\nLabel 0\n5540042\nGetty Images. Discriminating against your employees can cost you. Workplace discrimination against\nemployees based on race, gender or sexual orientation costs businesses an estimated $64 billion annually,\na recent report from the Center For American Progress finds.etty Images. Discriminating against your\nemployees can cost you. Workplace discrimination against employees based on race, gender or sexual\norientation costs businesses an estimated $64 billion annually, a recent report from the Center For\nAmerican Progress finds.\n5606063\nThe crippling effects of workplace discrimination include poor work culture and a demoralized work-\nforce, debilitating effect on the individual, negative fallout for the society, and reduced profits for the\norganization.slide 1 of 7.oss of Focus and Productivity. Another negative effect of workplace employee\ndiscrimination is loss of focus and wasting of time. Discrimination in the workplace is not just the focus\nof the victim, but also tends to take priority in various meetings.\n6381283\nIt is important to note that not all forms of discrimination are illegal. There are several federal laws that\nprohibit discrimination in the workplace, including: 1 Equal Pay Act of 1963: Requires that men and\nwomen receive equal pay for doing the same or similar jobs.\nQuery 67316: can fever cause miscarriage early pregnancy\nLabel 0\n156062\nMiscarriage: Bleeding can be a sign of miscarriage, but does not mean that miscarriage is imminent.\nStudies show that anywhere from 20-30% of women experience some degree of bleeding in early\npregnancy. Approximately half of pregnant women who bleed do not have miscarriages.\n4163707\nMiscarriage: Bleeding can be a sign of miscarriage, but does not mean that miscarriage is imminent.\nStudies show that anywhere from 20-30% of women experience some degree of bleeding in early\npregnancy. Approximately half of pregnant women who bleed do not have miscarriages. Approximately\n15-20% of all pregnancies result in a miscarriage, and the majority occur during the first 12 weeks. Signs\nof Miscarriage include:\n6221796\nBleeding during early pregnancy is sometimes a sign of miscarriage. About half the time, bleeding during\npregnancy is a sign of miscarriage, and the other half of the time it is not. So, a good rule of thumb to\nremember is that if the bleeding is enough to be a cause of concern for you, you should see your doctor.\n6764673\nMiscarriage: Bleeding can be a sign of miscarriage, but does not mean that miscarriage is imminent.\nStudies show that anywhere from 20-30% of women experience some degree of bleeding in early preg-\nnancy.Approximately half of pregnant women who bleed do not have miscarriages.iscarriage: Bleeding\ncan be a sign of miscarriage, but does not mean that miscarriage is imminent. Studies show that anywhere\nfrom 20-30% of women experience some degree of bleeding in early pregnancy.\n7513151\nFirst Half of Pregnancy: Miscarriage: Bleeding can be a sign of miscarriage, but does not mean that\nmiscarriage is imminent. Studies show that anywhere from 20-30% of women experience some degree of\nbleeding in early pregnancy. Approximately half of pregnant women who bleed do not have miscarriages.\nApproximately 15-20% of all pregnancies result in a miscarriage, and the majority occur during the first\n12 weeks.\n"
  },
  {
    "title": "DragVideo: Interactive Drag-style Video Editing",
    "link": "https://arxiv.org/pdf/2312.02216.pdf",
    "upvote": "9",
    "text": "DragVideo: Interactive Drag-style Video Editing\nYufan Deng1* Ruida Wang1* Yuhao Zhang1* Yu-Wing Tai2\nChi-Keung Tang1\n1The Hong Kong University of Science and Technology\n2Dartmouth College\n{ydengbd, rwangbr, yzhanglp}@connect.ust.hk, yu-wing.tai@dartmouth.edu, cktang@cse.ust.hk\n(a) Turn face\n(b) Shorten ears\n(c) Squeeze bus\n(d) Shorten face\nFigure 1. Results of DragVideo. Left four frames are propagated editing instructions (points and masks). Right four frames are edited\noutput. Note the naturalness of our results without apparent distortion, despite that DragVideo, as an interactive video editing tool, does\nnot have access to any 3D geometry. Please see our supplemental video.\nAbstract\nEditing visual content on videos remains a formidable\nchallenge with two main issues: 1) direct and easy user con-\ntrol to produce 2) natural editing results without unsightly\ndistortion and artifacts after changing shape, expression\nand layout. Inspired by DragGAN [16], a recent image-\nbased drag-style editing technique, we address above issues\nby proposing DragVideo, where a similar drag-style user\ninteraction is adopted to edit video content while maintain-\ning temporal consistency. Empowered by recent diffusion\nmodels as in DragDiffusion [20], DragVideo contains the\nnovel Drag-on-Video U-Net (DoVe) editing method, which\noptimizes diffused video latents generated by video U-Net\n*Equal contribution. The order of authorship was determined alphabet-\nically.\nto achieve the desired control. Specifically, we use Sample-\nspecific LoRA fine-tuning and Mutual Self-Attention control\nto ensure faithful reconstruction of video from the DoVe\nmethod.\nWe also present a series of testing examples\nfor drag-style video editing and conduct extensive exper-\niments across a wide array of challenging editing tasks,\nsuch as motion editing, skeleton editing, etc, underscoring\nDragVideo\u2019s versatility and generality. Our codes including\nthe DragVideo web user interface will be released. Github\nlink: https://github.com/RickySkywalker/\nDragVideo-Official.\narXiv:2312.02216v1  [cs.GR]  3 Dec 2023\n1. Introduction\nDrag-style editing has recently gained significant attention\nsince the debut of DragGAN [16], a powerful technique\nfor pixel-level interactive drag-style manipulating Genera-\ntive Adversarial Networks (GANs). However, as DragGAN\nmodifies the image by editing the latent space of StyleGAN\nand focuses editing on generated plots. Consequently, users\nwho wish to edit their own images must initially obtain the\nlatent projection of the image within the latent space. While\ntechniques like pivot tuning [18] and others offer some so-\nlutions for finding the inverse latent in StyleGAN\u2019s latent\nspace, it remains challenging for most realistic images to\nfind their latent projection, let alone videos.\nMeanwhile, research has suggested that the U-Net in\nthe Diffusion model also possesses a latent space with se-\nmantic meaning\n[23], and it is comparatively easier for\nthe Diffusion model to generate accurate inversions using\nthe Denoising Diffusion Implicit Models (DDIM) inver-\nsion method [13, 22]. Considering these advantages of the\nDiffusion model, it is logical to explore the application of\nthe DragGAN on the Diffusion model.\nFollowing these\nthoughts, DragDiffusion [20] and DragonDiff [14] have ex-\ntended drag-style editing to stable-diffusion model [19],\nleveraging the robust inversion ability in editing real im-\nages and large generation capacity of diffusion models. De-\nspite the impressive editing results on static images, their\nvideo extension has yet to be explored, where the drag-style\nusability of videos and propagating edited spatio-temporal\nvisual content across videos are issues. The challenge in ex-\ntending drag-style geometry editing to videos lies in main-\ntaining temporal consistency across the edited frames. The\nstate-of-the-art video editing tools, including CoDeF [15],\nRerender A Video [29], VideoComposer [26], and Edit-a-\nvideo [21], are constrained to style changes by changing\nprompts, which is inaccurate in most cases [16] and cannot\neffectuate substantial shape, expression, and motion alter-\nations.\nTo address the challenge of drag-style editing on videos,\nwe propose DragVideo, which addresses the temporal con-\nsistency across multiple frames. From the user\u2019s perspec-\ntive, the initial step is providing input handle points and\ntarget points in the first and last frames as pairs of points\nfor drag instructions.\nThese pairs of points, along with\nadditional positive and negative point inputs, prompt the\nSegment-Anything (SAM) [11] to generate masks that de-\nfine the region to be edited.\nWe employ PIPs [28] for point tracking as well as\nTAM [28] for mask tracking allowing both masks and pairs\nof points to be propagated consistently across all frames,\nwhich ensures continuity throughout the video and make\nusers free from putting points and masks on all frames. Sub-\nsequently, we utilize a video U-Net model to inverse and\noptimize the latent representation guided by the propagated\nmasks and point pairs. The optimization process achieves\nthe desired edited result while maintaining temporal consis-\ntency in the video. Finally, we derive the edited video from\ndenoising the noisy edited video latent. We mitigate the\nfrequently observed reconstruction inconsistency issue by\nemploying a Sample-specific LoRA [8] and Mutual Self-\nAttention (MSA) mechanism\n[1] to guide the denoising\nprocess.\nWe conduct exhaustive quantitative and qualitative stud-\nies on DragVideo. Our results demonstrate the state-of-the-\nart performance of DragVideo in both the quality and ver-\nsatility of the edited results. In summary, our main contri-\nbutions are:\n1. We propose DragVideo, the first end-to-end drag-style\nvideo editing framework, where video U-Net provides\nthe framework of strong temporal consistency and uti-\nlizes LoRA and MSA to address the reconstruction in-\nconsistency issues.\nMoreover, such a framework en-\nables users to edit videos on a single RTX-4090 or RTX-\nA6000 GPU.\n2. We carry out a comprehensive quantitative and qualita-\ntive study on DragVideo, showing the superiority of our\nframework in performing accurate and temporal consis-\ntent drag-style video editing compared to baselines\n3. We also develop a GUI for users to interactively input\npairs of points and indicate masks by simply clicking on\nthe first and last frames of the video, and perform drag\nediting in a click-and-run manner.\n2. Related work\n2.1. Video Diffusion Models\nContemporary and rapid development of text-to-image\n(T2I) Diffusion Models has significantly impacted image\ngeneration, beginning with DDPM [7], followed by the in-\ntroduction of a text-image joint feature space by CLIP [17],\nwhich provides control over the generated results via text\nprompts. The latent diffusion model [19] (i.e., stable diffu-\nsion or LDM) proposed denoising within the auto-encoder\u2019s\nlatent space, leading the impressive results.\nGiven these advancements, research began to explore the\npotential of stable diffusion models for video generation\nFor example, Tune-a-Video [27] proposed a one-shot video\ngeneration methodology with minor architectural modifica-\ntions and sub-network tuning. Text2Video-Zero [10] en-\nhanced a pre-trained T2I model via latent warping under\na predefined affine matrix, offering a training-free method\nfor video generation. AnimateDiff [4] introduced a plug-\nin motion module to T2I, pre-trained to capture video mo-\ntion, resulting in promising results for generating motion-\nconsistent, text-guided videos. In this work, we refer to the\nU-Net of the video diffusion model as Video U-Net. With\nthe use of prompt-to-prompt editing and DDIM Inversion\n[13], video diffusion models [4, 10, 15] have excelled in\ntext-guided style-level editing. However, their capabilities\ndo not extend to altering an object\u2019s geometry and motion\nin the video.\n2.2. Drag-Style Editing\nThe recent introduction of DragGAN\n[16] brought for-\nward a novel drag-style editing method on static images,\nwherein the user provides one or multiple paired (handle,\ntarget) points. DragGAN uses the motion supervision loss\ncomputed on the intermediate feature map of pre-trained\nStyleGAN2\u2019s [9] decoder to iteratively optimize the latent\ncode randomly sampled or obtained after GAN inversion,\nachieving excellent image editing results. This drag-style\nediting for static images can also be applied to the dif-\nfusion model, by performing the optimization process in\nthe U-Net\u2019s decoder before the denoising of the randomly\nsampled noisy latent code or the one obtained after DDIM\ninversion [14, 20].\nDragDiffusion\n[20] employs LoRA\n[8] and MSA [1] to improve the consistency between the\nedited image and the original one. Meanwhile, DragonDif-\nfusion [14] incorporates cosine-similarity-based drag ob-\njective function based on the area masks to conduct more\ngeneral drag editing.\nWhile the success of drag-style editing on 2D images\nshows its potential in video editing, where a similar drag-\nstyle UI can be amenable to editing video, we propose tech-\nnical contributions to seamlessly make the user experience\ncompatible with video data to achieve drag-style editing on\nvideos with good temporal consistency.\n2.3. Points and Mask Tracking\nIn recent years, significant improvements in points track-\ning across frames in video have been made, such as\nRAFT [24], TAPNET [3], and state-of-the-art OmniMo-\ntion [25]. Among them, Persistent Independent Particles\n(PIPs) [5] studies pixel tracking as a long-range motion es-\ntimation problem, such that every pixel is described with\na trajectory, pinpointing its location across multiple future\nframes. This method demonstrated moderate robustness to-\nward long-term tracking challenges such as object occlusion\nand re-appearance.\nMeanwhile, the Segment Anything Model (SAM) [11],\nan innovative image segmentation model, has delivered\ncommendable results in rapid image segmentation. Trained\non over 1 billion segmentation masks, SAM has paved the\nway for the Track-Anything Model (TAM)[28]. TAM in-\ntegrates SAM and Xmen [2], a sophisticated Video Object\nSegmentation (VOS) model, in a cyclical process to track\nthe target object masks as provided by SAM and utilize\nSAM to refine the mask details predicted by Xmen.\nWithin our DragVideo framework, we initially employ\nuser-supplied point pairs and other positive/negative points\nto establish the masks, which serve to limit the optimization.\nSubsequently, PIPs [5] is utilized to track the handle points\nacross all the provided frames, while Tracking-Anything in\nTAM [28] is used to track the masks as predicted by SAM.\n3. Methodology\nThis section presents an in-depth technical exposition of\nDragVideo.\nAs depicted in Figure 2, the top part illus-\ntrates the comprehensive pipeline of DragVideo, which pro-\ncesses the user\u2019s input video and point pairs to execute the\n\u201ddrag\u201d operation on the video. The four core modules of\nthe pipeline are displayed at the bottom of Figure 2, the in-\ntricacies of which will be further delineated. Section 3.2\nelucidates the Sample-specific LoRA fine-tuning, integral\nfor enhancing the preservation of personal identity in the\nedited video. Section 3.3 provides a description of the prop-\nagation of the user\u2019s point pairs throughout the entire video.\nSection 3.4 explicates Drag on Video U-Net (DoVe), the uti-\nlization of video latent optimization for drag-style editing.\nLastly, Section 3.5 details the employment of the Mutual\nSelf-Attention technique, ensuring consistency between the\ninput and the output videos.\n3.1. Video U-Net\nA simple extension from the 2D U-Net for images to the 3D\nU-Net for videos, referred to as Video U-Net in this paper,\nresults in prohibitively high computational costs, which be-\ncome particularly critical during optimization in the latent\nspace. AnimateDiff [4] circumvents this issue by integrat-\ning a motion module after each layer of the standard pre-\ntrained 2D U-Net from the LDM [19], resulting in a video\nU-Net that maintains a manageable GPU memory footprint.\nIt also allows the utilization of animating any pre-trained,\ncustomized checkpoints of the LDM. The motion module\ncomprises project-in layers, self-attentions along the axis of\nframes and channels, and project-out layers. Self-attention\nis applied to each pixel of the latents to capture temporal\ninformation, as opposed to the spatial information provided\nby the layers of the pre-trained 2D U-Net. DragVideo incor-\nporates AnimateDiff as the video U-Net to ensure temporal\nconsistency in editing.\n3.2. Sample-specific LoRA\nThe Sample-specific LoRA [8] is implemented within the\nvideo U-Net model, as shown in Figure 2(a), ensuring the\npreservation of the necessary fidelity to the original video\nduring the denoising process. The LoRA layers exhibit a\nnotable proficiency in capturing crucial features from the\noriginal video, thereby facilitating the preservation of per-\nsonal identity throughout the video editing process. The\ntraining process for our LoRA module adheres to the stan-\ndard training procedures of the diffusion model [7]. For-\nmally, the objective function for the training task is defined\nas:\nLLoRA(z, \u2206\u03b8) = E[|\u03f5 \u2212 \u03f5\u03b8+\u2206\u03b8(\u03b1tz + \u03c3t\u03f5)|2],\n(1)\nwhere \u03b8 and \u2206\u03b8 represent the parameters of the video U-\nNet and LoRA, respectively, z denotes the video latent,\nInput Video\n[\ud835\udc99\ud835\udc991, \ud835\udc99\ud835\udc992, \u2026 , \ud835\udc99\ud835\udc99\ud835\udc59\ud835\udc59]\n(a) Train LoRA\n(b) Points & Mask\nPropogation\n\ud835\udc99\ud835\udc991\n\ud835\udc99\ud835\udc99\ud835\udc59\ud835\udc59\nInput Video\n[\ud835\udc99\ud835\udc991, \ud835\udc99\ud835\udc992, \u2026 , \ud835\udc99\ud835\udc99\ud835\udc59\ud835\udc59]\n(c) Video U-Net\nLatent \nOptimization\n(d) Mutual Self-\nAttention Denoising\nEdited Video\n[\u0ddd\ud835\udc99\ud835\udc991, \u0ddd\ud835\udc99\ud835\udc992, \u2026 , \u0ddd\ud835\udc99\ud835\udc99\ud835\udc59\ud835\udc59]\nInput Points & Mask on \nfirst / last frame\nPropagated Points & \nMasks on all frames\nTrained LoRA Weights\nPipeline of DragVideo\nInput video\nPropagated Drag \nInstructions\nInput Video Latent\n+ rand\nnoise\n3D U-Net \nwith LoRA\nReconstruction\n\u2026\nManually put points and indicate \nmasks on first & last frames\n(a) Train Sample-Specific LoRA\n(b) Points & Mask Propagation\n[\ud835\udc9b\ud835\udc9b0,1, \ud835\udc9b\ud835\udc9b0,2, \u2026 , \ud835\udc9b\ud835\udc9b0,\ud835\udc59\ud835\udc59]\n[\ud835\udc99\ud835\udc991, \ud835\udc99\ud835\udc992, \u2026 , \ud835\udc99\ud835\udc99\ud835\udc8d\ud835\udc8d]\n\ud835\udc99\ud835\udc991\n\ud835\udc99\ud835\udc992\n\ud835\udc99\ud835\udc99\ud835\udc8d\ud835\udc8d \u2212\ud835\udfcf\ud835\udfcf\n\ud835\udc99\ud835\udc99\ud835\udc8d\ud835\udc8d\nInput Video\n[\ud835\udc99\ud835\udc991, \ud835\udc99\ud835\udc992, \u2026 , \ud835\udc99\ud835\udc99\ud835\udc8d\ud835\udc8d]\n\ud835\udf00\ud835\udf00\nVAE Encoding\nInput Video Latent\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nVideo U-Net\nDDIM Inversion\nReconstruction Loss\n\ud835\udc9b\ud835\udc9b\ud835\udfce\ud835\udfce = [\ud835\udc9b\ud835\udc9b0,1, \ud835\udc9b\ud835\udc9b0,2, \u2026 , \ud835\udc9b\ud835\udc9b0,\ud835\udc59\ud835\udc59]\nNoisy Video Latent\n\u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61\n(\ud835\udc58\ud835\udc58) = [\u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,1\n(\ud835\udc58\ud835\udc58), \u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,2\n(\ud835\udc58\ud835\udc58), \u2026 , \u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,\ud835\udc59\ud835\udc59\n(\ud835\udc58\ud835\udc58)]\nUser Edit\nEdited Noisy \nVideo Latent\n\u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61\n(0) = [\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,1\n(0), \ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,2\n(0), \u2026 , \ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,\ud835\udc59\ud835\udc59\n(0)]\nLatent Optimization\nQ\nK\nV\n\u0de0\ud835\udc44\ud835\udc44\nK\nV\nVideo U-Net\nDenoise\nCopy\n&\nReplace\n\u0ddc\ud835\udc9b\ud835\udc9b0 = [\u0ddc\ud835\udc9b\ud835\udc9b0,1, \u0ddc\ud835\udc9b\ud835\udc9b0,2, \u2026 , \u0ddc\ud835\udc9b\ud835\udc9b0,\ud835\udc59\ud835\udc59]\n\ud835\udc9b\ud835\udc9b0 = [\ud835\udc9b\ud835\udc9b0,1, \ud835\udc9b\ud835\udc9b0,2, \u2026 , \ud835\udc9b\ud835\udc9b0,\ud835\udc59\ud835\udc59]\nEdited Video\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\ud835\udcd3\ud835\udcd3\nVAE \nDecoding\n[\u0ddd\ud835\udc99\ud835\udc991, \u0ddd\ud835\udc99\ud835\udc992, \u2026 , \u0ddd\ud835\udc99\ud835\udc99\ud835\udc59\ud835\udc59]\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n(c) Drag-on-Video U-Net (DoVe) Latent Optimization\n(d) Mutual Self-Attention Denoising\n\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61 = [\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,1, \ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,2, \u2026 , \ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,\ud835\udc59\ud835\udc59]\n\u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61 = [\u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,1\n(\ud835\udc58\ud835\udc58), \u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,2\n(\ud835\udc58\ud835\udc58), \u2026 , \u0ddc\ud835\udc9b\ud835\udc9b\ud835\udc61\ud835\udc61,\ud835\udc59\ud835\udc59\n(\ud835\udc58\ud835\udc58)]\n\u2026\nEncoded\nEncoded\nEncoded\nEncoded\nEncoded\nEncoded\nEncoded\nEncoded\nNoisy Video Latent\nEncoded\nEdited Noisy \nVideo Latent\nComponents of DragVideo\nFigure 2. Overview of DragVideo: Our approach comprises of four pivotal steps, given an input video of length l, (i.e., x1 to xl): (a)\nInitially, we fine-tune a Sample-specific LoRA based on the input video. (b) Subsequently, the user designates points and masks on both\nthe first and last frames. They are then propagated across all frames by PIPs and TAM. (c) Next, we obtain the noisy video latent (i.e.,\nzt,1 to zt,l) by DDIM Inversion of video U-Net. After that, we optimize the noisy video latents by the above points and mask with a drag\nobjective function for k steps. The optimized latent is represented as z(k)\nt,1 to z(k)\nt,l . (d) Lastly, the optimized noisy video latent is denoised\nutilizing the Video U-Net with Sample-specific LoRA and operates on the Mutual Self-Attention mechanism.\n\u03f5 \u223c N(0, I) is the randomly sampled noise added to the\nvideo latent, \u03f5\u03b8+\u2206\u03b8(\u00b7) signifies the noise predicted by the\nLoRA-enhanced Video U-Net, and \u03b1t and \u03c3t are hyperpa-\nrameters of the DDIM noise scheduler at step t, with t being\nrandomly sampled from the total steps of the scheduler. We\nupdate the LoRA parameters by executing gradient descent\non \u2206\u03b8 based on the objective function LLoRA.\n3.3. Point and Mask Propagation\nGiven an input video with the user-supplied paired (han-\ndle, target) points and masks on the start frame (x1) and\nend frame (xl) of the video (named drag instruction), the\nmasks and pairs of points are propagated to all frames of the\nvideo. The propagated masks and points are called propa-\ngated drag instruction, see Figure 2(b).\nPoint Propagation The handle point typically resides on\nan object for dragging, while the target point, which indi-\ncates the future location of the given handle after dragging,\nsometimes is on the meaningless background, which may\nresult in tracking failure. To mitigate this, we exclusively\ntrack handle points and employ linear interpolation to de-\ntermine corresponding target points. We adopt Persistent\nIndependent Particles (PIPs) [5], which have been shown\nto be quite robust in long-term tracking in the presence of\nocclusion and disocclusion to track the handle point of the\ngiven drag vectors from x1 to xl.\nMask Propagation\nWe use the Track-Anything Model\n(TAM) [28] to track the input masks. The mask on the first\nframe is obtained via prompt of positive/negative points. In\nDragVideo, the handle points on x1 and xl are automati-\ncally input to TAM as positive points. The user may add\nadditional positive/negative points to obtain a desired mask.\nFollowing the acquisition of an initial mask, users may ex-\npress a desire to expand the mask\u2019s boundaries to encom-\npass supplementary information within the handle object\u2019s\nneighborhood. To achieve this, the masks will be extended\nbased on a user-defined parameter, the extension radius.\nTAM then generates masks for all the intermediate frames.\nIf an interpolated target point falls outside of the propagated\nmask, we simply dilate the mask along the drag vector from\nthe propagated handle point to the interpolated target point.\n3.4. Drag-on Video U-Net\nThis section provides a comprehensive description of our\nproposed framework to perform video drag-style editing,\nthe Drag-on Video U-Net (DoVe), see Figure 2(c). First,\nto facilitate high-level editing, we perform DDIM Inversion\nto add noise by video U-Net. Then, we iteratively perform\nmotion supervision on noisy video latents and embedded\npoint-tracking to perform drag-style editing.\nDDIM Inversion Past research [13] has demonstrated that\nediting on a noisy latent (i.e., zt with larger t) allows for\nhigher-level editing. Therefore, to perform versatile drag-\nstyle editing on videos while maintaining inherent video\nidentity, we use DDIM Inversion [13, 22] to add back the\nnoise predicted by Video U-Net to the video latent in order\nto create a noisy latent. That is, given an input video latent\nz0, we attain the t-th step noisy latent, denoted as zt, which\nwill be used in drag-style editing.\nMotion Supervision Drawing inspiration from DragGAN\n[16] and DragDiffusion [20], we propose a motion super-\nvision loss (i.e., drag objective function) that does not rely\non any additional training of neural networks for drag-style\nediting. As suggested by DIFT [23], diffusion models\u2019 in-\ntermediate features exhibit a significant feature and location\ncorrespondence that can be utilized for motion supervision.\nWe denote F(zt) as the feature output by Video U-Net from\nnoisy video latent of zt. Typically, we opt for the second\nor third layer of Video U-Net\u2019s output as the feature map.\nIn order to enhance motion supervision, F is resized by lin-\near interpolation, i.e., F : Rl\u00d7c\u00d7hlatent\u00d7wlatent \u2192 Rl\u00d7c\u00d7 h\n2 \u00d7 w\n2 ,\nwhere hlatent, wlatent are the heights and widths of VAE en-\ncoded video latents, and h, w are the heights and widths\nof the original video. We denote in the k-th iteration, the\nj-th handle point at frame i as p(k)\ni,j , where p(0)\ni,j is be user-\nsupplied or propagated. We move p(k)\ni,j to the target point ti,j\nby supervising a small patch around p(k)\ni,j , via optimizing the\nlatent \u02c6z(k)\nt\nto warp the patch toward ti,j by an incremental\nstep in the k + 1-th iteration.\nDenote Br(p(k)\ni,j ) as a small circle of area with radius r (a\nhyperparameter) around p(k)\ni,j , and \u02c6z(k)\nt\nas the edited latent\ncode for k-th iteration. Then, our motion supervision loss\nL(\u02c6z(k)\nt\n) for optimizing the video latent is given by\nl\nX\ni=1\nn\nX\nj=1\nX\nq\u2208Br(p(k)\ni,j )\n\u2225Fq+d(k)\ni,j (\u02c6z(k)\nt\n) \u2212 sg(Fq(\u02c6z(k)\nt\n))\u22251+\n\u03bb\u2225(\u02c6z(k)\nt\u22121 \u2212 sg(zt\u22121)) \u2217 (I \u2212 M)\u22251\n(2)\nwhere sg(\u00b7) is the stop gradient operator where the argument\nwill not be backward propagated. This ensures Br(p(k)\ni,j )\nwarps toward a location centered at (p(k)\ni,j +d(k)\ni,j ) but not the\nother way, where d(k)\ni,j =\nti,j\u2212p(k)\ni,j\n\u2225ti,j\u2212p(k)\ni,j \u22252 is the normalized vec-\ntor pointing from p(k)\ni,j to ti,j. The i sums up all frames in the\nvideo and j sums up all points in one frame. As the com-\nponents of q are not integer, we obtain Fq+d(k)\ni,j (\u02c6z(k)\nt\n) via\nbilinear interpolation. In the second term, we apply regular-\nization to the video latent from the binary mask M obtained\nby mask propagation (Section 3.3) to ensure the update lies\nwithin the masked region.\nFor each motion supervision step, this loss is used to op-\ntimize the edited latent code \u02c6z(k)\nt\nfor one time via:\n\u02c6z(k+1)\nt\n= \u02c6z(k)\nt\n\u2212 \u03b7 \u00b7\n\u2202\n\u2202\u02c6z(k)\nt\nL(\u02c6z(k)\nt\n)\n(3)\nwhere \u03b7 is the learning rate for latent optimization. By per-\nforming the above optimization via motion supervision, we\ncan incrementally \u201cdrag\u201d the handle point to target point\none step at a time.\nEmbedded Point Tracking After each step of motion su-\npervision, there will be an updated noisy video latent \u02c6z(k+1)\nt+1\nand new feature map F(\u02c6z(k+1)\nt+1\n).\nSince motion supervi-\nsion only updates video latent but does not give precise\nnew location of handle points p(k+1)\ni,j\n.\nThus, to perform\nthe next step of the motion supervision update, we need\nto embed a point tracking method to track the new handle\npoints p(k+1)\ni,j\n. Inspired by [20, 23], where video U-Net\u2019s\nfeature map F contains rich positional information, we thus\nutilize F(\u02c6z(k+1)\nt\n) and F(zt) to track new handle points\nq(k+1)\ni,j\nby nearest neighbor method within the square patch\n\u2126(q(k)\ni,j , r\u2032) = {(x, y) : |x \u2212 x(k)\ni,j | \u2a7d r\u2032, |y \u2212 y(k)\ni,j | \u2a7d r\u2032},\nwhere x(k)\ni,j and y(k)\ni,j is the x and y coordinate of q(k)\ni,j respec-\ntively and r\u2032 is a hyperparameter. The tracking method is as\nfollows:\np(k+1)\ni,j\n= argminq\u2208\u2126(p(k)\ni,j ,r\u2032){\u2225Fq(\u02c6z(k+1)\nt\n\u2212 Fp(0)\ni,j (zt)\u22251}\n(4)\nPropagated Instructions\nDragVideo\nBaseline\n(a) cliff extension\nPropagated Instructions\nDragVideo\nBaseline\n(b) hair parting\nFigure 3. Comparison between baseline and DragVideo.\n3.5. Mutual Self-Attention Video Denoising\nFinally, we perform denoising of the dragged video via Mu-\ntual Self-Attention (MSA) controlled denoising.\nSimply\napplying DDIM denoising [22], even with Sample-specific\nLoRA, on the dragged noisy video latent may easily lead\nto an undesirable identity shift and degradation in quality\nfrom the original video.\nThis problem can be attributed\nto a lack of guidance from only the original video during\nthe denoising process. Inspired by [1], a prompt-based im-\nage editing method can preserve the identity of the original\nimage by changing the self-attention in image U-Net into\ncross attention using K, V from original latents, namely\nMutual Self-Attention. Inspired by their idea, we propose\nusing MSA on Video U-Net. For the U-Net module, the\nattention use in [1] given input x can be represented as\ny = softmax( Q(x)K(x)T\n\u221a\nd\n)V (x) for some d. In MSA for\nVideo U-Net, with the input of attention for original video\nlatent is x and the input of attention for edited video latent\nis \u02c6x, we replace the keys and values for the edited output\nwith K(x), V (x):\n\u02c6y = softmax(Q(\u02c6x)K(x)T\n\u221a\nd\n)V (x).\n(5)\nThus, we need to perform the denoising process for both\nthe original video latent zt and edited video latent \u02c6zt, while\nutilizing zt as MSA guidance for \u02c6zt. In doing so, a more\ncoherent denoise can be achieved.\n4. Experiments\n4.1. Implementation Details\nIn our experiments, we adopt Stable Diffusion v1.5 [19] and\nits inherited models as base models for motion modules [4]\nto construct our Video U-Net. As aforementioned in Sec-\ntion 3.2, we introduce LoRA into the projection matrices of\nquery, key, and value in all of the attention modules within\nour video U-Net. The LoRA rank is set to 16 and the batch\nsize to 12 as default. We employed AdamW [12] optimizer\nwith a learning rate of 5 \u00d7 10\u22124, and trained LoRA for 100\nepochs before commencing drag editing.\nDuring the latent optimization phase, we set the inverse\nstep to 50 for DDIM and start latent optimization at the 40th\nstep. In our experiments, we do not apply classifier-free\nguidance (CFG) [6] in either DDIM inversion or DDIM de-\nnoising processes, as CFG tends to amplify inversion errors.\nThe default batch size for drag optimization is set to 12. We\n(a) Generate band\n(b) Close neckline\n(c) Connect island\n(d) Shorten hair\n(e) Squeeze sofa\n(f) Flatten hairline\nFigure 4. More results of DragVideo. Left four frames are propagated editing instructions. Right four frames are edited output.\nset the optimization step to 40 while the latent learning rate\nis set to 0.01, \u03bb = 0.1 in Eq. (2). The user can modify\nthese parameters on our web user interface (UI) to obtain\ndifferent results.\n4.2. Experiment Setup\nDatasets With the very recent debut of contemporary drag-\nstyle editing on images, no benchmark video datasets are\navailable with mask and point pairs for drag-style video\nediting. Thus we annotate our own video dataset on pub-\nlicly available videos. To thoroughly test the efficacy of\nour method, we collect a wide range of examples on dif-\nferent subjects and objects, including pets, faces, furniture,\nscenery clothes and so on. Full dataset details are available\nin supplementary materials.\nBaselines As DragVideo is the first approach to drag-style\nvideo editing, we implement the baseline by directly apply-\ning DragDiffusion [20] on every frame of a given video to\nserve as the basic baseline for comparison. To ensure fair-\nness, we use same set of mask and points for baseline and\nDragVideo. For more comparison between our result and\nthe baseline, kindly refer to the supplementary materials.\n4.3. Qualitative Evaluation\nIn this section, we conduct a thorough assessment of our\nDragVideo framework\u2019s efficacy through an array of com-\nprehensive experiments encompassing a broad spectrum of\nediting tasks.\nResults\nFigure 1 and Figure 4 exhibit examples of our\nediting results. Evidently, our DragVideo framework facili-\ntates high-quality, drag-based editing on real-world videos.\nImportantly, our model is capable of handling images from\nvarious domains, attributing to the robust capabilities inher-\nent in diffusion models. The results not only emphasize our\nmodel\u2019s adeptness at producing efficient drag-based edits\nbut also underscore its proficiency in maintaining temporal\nPropagated Instructions\nEdited Output\nwithout LoRA\nwithout MSA\nFigure 5. Ablation study of DragVideo.\nSample Output\nBaseline\nDragVideo\nSqueeze bus in Fig. 1c\n0.71768\n0.66328\nShorten ears in Fig. 1b\n0.81162\n0.47923\nShow forehead in Fig. 3b\n1.62124\n1.61182\nShorten hair in Fig. 4d\n0.13472\n0.09027\nTable 1. Quantitative evaluation in terms of optical flow. Smaller\nvalue means higher temporal consistency.\nconsistency within the video and reconstruction consistency\nin relation to the original video. For additional qualitative\nfindings, kindly refer to the supplementary materials.\nBaselines Comparison\nWe offer a comparative analysis\nwith baseline models in Figure 3. The edited regions in the\nbaseline model\u2019s output, though are good separately, do not\nhave temporal consistency. The hair and cliff in neighbor\nframes have a large difference. However, the edited parts\nin DragVideo\u2019s output are consistent across frames with\nsmooth transition between them.\n4.4. Quantitative Evaluation\nIn this section, we evaluate the temporal consistency of\nDragVideo and baseline. We use the optical flow obtained\nby RAFT [24] to measure the temporal consistency. Given\ntwo frames, RAFT model outputs the coordinate changes\nfor each pixel. The averaged changes of optical flow is cal-\nculated for evaluation of temporal consistency:\nscore =\nPl\u22121\ni=1\nPw\nu=1\nPh\nv=1 \u2225(\u2206x(i,i+1)\nu,v\n, \u2206y(i,i+1)\nu,v\n)\u22252\n(l \u2212 1) \u00b7 h \u00b7 w\n(6)\nThe scores of multiple sample outputs are shown in Table 1.\nDragVideo achieves less jittering, and thus better temporal\nconsistency than the baseline.\n4.5. Ablation Study\nWe conduct an ablation study to investigate the effi-\ncacy of individual components of our DragVideo frame-\nwork in contributing to high-quality editing results while\npreserving reconstruction consistency.\nHere we present\nthe ablation of two main components in our framework\ndesigned to preserve reconstruction consistency, namely,\nthe Sample-specific LoRA (Section 3.2) and MSA (Sec-\ntion 3.5). Figure 5 presents qualitative results, where we can\nsee the edited area deviates substantially from the original\nframe without LoRA, while becoming unreasonable with-\nout MSA. Missing LoRA makes DragVideo unable to pre-\nserve the original style, while missing MSA allows the op-\ntimized latents to destroy intermediate features during de-\nnoising.\n5. Discussions\nLimitation\nDespite the promising results of DragVideo,\nthere remain two limitations. First, even when leveraging\nSample-specific LoRA and MSA techniques, a minority\nof the edited outputs still exhibit some blurriness and\nartifacts, suggesting room for further optimization in visual\nquality.\nSecond, our framework, inheriting the compu-\ntational challenges of the diffusion model and the drag\nobjective function, still incurs high computational costs,\neven with the use of video U-Net. This points to a need for\nimprovements in computational efficiency. As a pioneering\neffort in drag-style video editing, DragVideo sets the stage\nfor future work aimed at enhancing consistency, reducing\ncomputational costs, and further refining the method.\nConclusion\nThis\npaper\nintroduces\nDragVideo,\nan\nend-to-end and user-friendly solution for drag-style video\nediting.\nBy harnessing the rich information embedded\nin pre-trained diffusion models and motion modules, our\nmethod allows users to directly and effortlessly manipulate\nvideo content beyond text-guided style change. The result\nis a natural and visually pleasing edit, largely free from\ndistortion or artifacts, even the user-supplied drag points\nmay imply substantial 3D geometric change. As the first\ntechnical endeavor into drag-style video editing, we have\ncarefully designed and executed a wide range of qualitative\nand quantitative experiments to assess the efficacy of our\nproposed method.\nThe outcomes of these experiments\nshowcase the versatility and broad applicability of our\napproach, validating DragVideo as a powerful tool for\nvideo editing tasks.\nReferences\n[1] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 2, 3, 6\n[2] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-\nterm video object segmentation with an atkinson-shiffrin\nmemory model. In European Conference on Computer Vi-\nsion, pages 640\u2013658. Springer, 2022. 3\n[3] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri`a Re-\ncasens, Lucas Smaira, Yusuf Aytar, Jo\u02dcao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for track-\ning any point in a video. Advances in Neural Information\nProcessing Systems, 35:13610\u201313626, 2022. 3\n[4] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6\n[5] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle video revisited: Tracking through occlusions using\npoint trajectories. In European Conference on Computer Vi-\nsion, pages 59\u201375. Springer, 2022. 3, 5\n[6] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 6\n[7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3\n[8] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 2, 3\n[9] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 3\n[10] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2\n[11] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 2, 3\n[12] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[13] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models.\narXiv preprint\narXiv:2211.09794, 2022. 2, 5\n[14] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. Dragondiffusion: Enabling drag-style manipula-\ntion on diffusion models. arXiv preprint arXiv:2307.02421,\n2023. 2, 3\n[15] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 2\n[16] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\ngan: Interactive point-based manipulation on the generative\nimage manifold. In ACM SIGGRAPH 2023 Conference Pro-\nceedings, pages 1\u201311, 2023. 1, 2, 3, 5\n[17] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2\n[18] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel\nCohen-Or. Pivotal tuning for latent-based editing of real im-\nages.\nACM Transactions on graphics (TOG), 42(1):1\u201313,\n2022. 2\n[19] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 6\n[20] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-\nsion models for interactive point-based image editing. arXiv\npreprint arXiv:2306.14435, 2023. 1, 2, 3, 5, 7\n[21] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang-gil Lee,\nand Sungroh Yoon. Edit-a-video: Single video editing with\nobject-aware consistency. arXiv preprint arXiv:2303.07945,\n2023. 2\n[22] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 5, 6\n[23] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 2, 5\n[24] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow.\nIn Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer,\n2020. 3, 8, 1\n[25] Qianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li,\nBharath Hariharan, Aleksander Holynski, and Noah Snavely.\nTracking everything everywhere all at once. arXiv preprint\narXiv:2306.05422, 2023. 3\n[26] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2\n[27] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2\n[28] Jinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing\nWang, and Feng Zheng. Track anything: Segment anything\nmeets videos. arXiv preprint arXiv:2304.11968, 2023. 2, 3,\n5\n[29] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 2\nDragVideo: Interactive Drag-style Video Editing\nSupplementary Material\n1. Additional Qualitative Results\nIn this section, we provide additional qualitative results that\nunderscore the capability of DragVideo.\nThe qualitative\nresults can be referenced in Figure 6. Moreover, we of-\nfer more comparison examples between DragVideo and the\nframe-by-frame drag baseline; the comparisons are in Fig-\nure 7.\nFrom Figure 6, it becomes evident that DragVideo con-\nsistently delivers accurate editing across a wide range of\nscenarios. Our framework exhibits a remarkable capability\nnot merely to shift pixels from one location to another, but\nto exploit the wealth of information in the diffusion model.\nThis results in the generation of fitting and credible con-\ntent in alignment with the drag instructions, a characteristic\nevident in almost all qualitative results. When comparing\nthe application of DragDiffusion [20] on a frame-by-frame\nbasis with DragVideo in Figure 7, the latter displays supe-\nrior spatial-temporal consistency throughout the video. The\nbaseline approach often results in varied characteristics in\neach frame, while DragVideo maintains high levels of con-\nsistency.\nTo further showcase DragVideo\u2019s efficacy and to provide\na broader comparison with the baseline, we have compiled\na video juxtaposing all DragVideo and baseline outcomes.\nPlease refer to the supp videos.mp4 file in the supplemen-\ntary materials folder.\n2. Additional Ablation Tests\nThis section presents two further ablation test results, as de-\npicted in Figure 9. The ablation tests reveal that the removal\nof LoRA tends to result in the dragged content not reaching\nthe target location. This phenomenon can be attributed to\nthe task-specific LoRA\u2019s role in encoding information from\nthe original video. In its absence, the video U-Net lacks\nthe necessary information to move the content logically,\nthereby making the drag effect less noticeable. On the other\nhand, when the MSA is removed, a significant deterioration\nin reconstruction is observed, characterized by a loss of in-\nformation post-dragging. This is because the video latent\nis highly sensitive to change, where even slight alterations\nmay result in substantial differences in the Keys and Values\nwithin attention blocks in video U-Net. Consequently, the\noutput without MSA exhibits severe issues with reconstruc-\ntion consistency. These ablation tests underscore both the\nvalidity and the critical importance of the LoRA and MSA\ncomponents within DragVideo.\nSample Output\nBaseline\nDragVideo\nClose neckline in Fig. 4b\n0.1519\n0.1458\nClose mouth of lion Fig. 6d\n1.3626\n1.2864\nSqueeze sofa in Fig. 4e\n0.2939\n0.1392\nRemove sleeve of the suit in Fig. 6c\n2.5903\n2.0229\nShorten the back of SUV in Fig. 6i\n4.0517\n2.1817\nLengthen the plant in Fig. 6e\n0.4911\n0.4340\nMove sun in Fig. 6h\n0.4772\n0.4266\nConnect island in Fig. 4c\n0.7861\n0.7779\nGenerate band in Fig. 4a\n0.2950\n0.2218\nExtend Cliff in Fig. 3a\n0.5270\n0.5153\nTable 2. More Quantitative evaluation in terms of optical flow.\nSmaller value means higher temporal consistency.\n3. Additional Quantitive Results\nIn this section, we conduct additional RAFT tests\n[24]\nto quantitatively assess the spatio-temporal consistency of\nDragVideo in comparison to the baseline. The results are\ndetailed in Table 2. It is evident that our method consistently\noutperforms the baseline across all ten cases presented in\nTable 2, indicating a demonstrable improvement. This sug-\ngests that our editing results are quantitatively more tempo-\nrally stable, underscoring the efficacy of our approach.\n4. UI for easy editing\nThis section outlines the usability of our implemented User\nInterface (UI), the view of our UI can be found at Figure\n8. Designed to facilitate effortless interactive video edit-\ning, our UI is composed of two primary stages. Initially,\nusers have the convenience of setting keyframe process pa-\nrameters and training LoRA parameters (Figure 8 (a)). For\nexample, they can determine the desired frames per second\n(fps) post-video processing by assigning a specific value to\nthe \u201dkps\u201d parameter. After uploading and processing the in-\nput video using the specified parameters, a preview of the\nprocessed video can be viewed. If the results are satisfac-\ntory, users can initiate the task-specific LoRA training by\nclicking the \u201dTrain LoRA\u201d button.\nFollowing the successful training of LoRA, users can\nnavigate to the second stage of our UI. Here, they can set\nthe drag instructions by clicking points on the first and last\nframes (Figure 8 (b)). Our UI also offers users the abil-\nity to fine-tune the mask in the first frame for precise mask\ntracking, utilizing positive and negative points. Addition-\nally, users can manipulate the \u201dradius\u201d parameter to increase\nthe mask size. Once these modifications are complete, the\n\u201dPropagate point & mask\u201d button can be clicked to generate\na preview video with the propagated drag instructions and\nmasks. After scrutinizing the propagation outcomes, users\nhave the choice to refine the latent optimization parameters,\nfor example, the latent learning rate. Finally, the \u201drun\u201d but-\nton initiates the video editing process.\nIn conclusion, our UI offers a streamlined workflow for\ninteractive video editing. Users can easily configure param-\neters, preview processed videos, train task-specific LoRA\nmodels, set drag instructions and masks, propagate them,\nand ultimately execute video editing tasks with minimal ef-\nfort. To further illustrate the use of our UI for interactively\ndragging the video, please refer to the ui demo.mp4 file in\nthe supplementary materials folder.\n(a) Enlarge window of a house\n(b) Remove indoor chair\n(c) Remove sleeve of the suit\n(d) Close mouth of lion\n(e) Lengthen the plant\n(f) Larger the island in front of skydiver\n(g) Larger the continent\n(h) Move the sun move right\n(i) Shorten the back of SUV\nFigure 6. More results of DragVideo. Left four frames are propagated editing instructions. Right four frames are edited output.\nPropagated Instructions\nDragVideo\nBaseline\n(a) Enlarge window of a house\nBaseline\nDragVideo\nPropagated Instructions\nBaseline\n(b) Close mouth of lion\nBaseline\nPropagated Instructions\nDragVideo\n(c) Remove sleeve of the suit\nDragVideo\nBaseline\nPropagated Instructions\n(d) Connect island\nFigure 7. More comparison between baseline and DragVideo\n(a) Pre-editing process of video\n(b) Interactively edit the video\nFigure 8. Overview of our implemented GUI, the first page is for pre-editing settings for our UI and the second page is for setting points,\nmasks, and performing edit\nPropagated Instructions\nEdited Output\nwithout LoRA\nwithout MSA\nPropagated Instructions\nEdited Output\nwithout LoRA\nwithout MSA\nFigure 9. More Ablation study of DragVideo.\n"
  },
  {
    "title": "MVHumanNet: A Large-scale Dataset of Multi-view Daily Dressing Human Captures",
    "link": "https://arxiv.org/pdf/2312.02963.pdf",
    "upvote": "9",
    "text": "MVHumanNet: A Large-scale Dataset of Multi-view Daily\nDressing Human Captures\nZhangyang Xiong1,2#\nChenghong Li1,2#\nKenkun Liu2#\nHongjie Liao2\nJianqiao Hu2\nJunyi Zhu2\nShuliang Ning1,2\nLingteng Qiu2\nChongjie Wang2\nShijie Wang2\nShuguang Cui2,1\nXiaoguang Han2,1\u2217\n#equal contribution\n\u2217corresponding author\n1FNii, CUHKSZ\n2SSE, CUHKSZ\nFigure 1. We introduce MVHumanNet, a large-scale dataset of multi-view human images with unprecedented scale in human subjects,\ndaily outfits, motion sequences and frames. Top left and right: Examples of multi-view poses featuring different human identities with\nvarious daily dressing in our dataset. Top middle: Our multi-view capture system includes 48 cameras of 12MP resolution. Bottom:\nComprehensive visualization of all 9000 outfits in our MVHumanNet.\nAbstract\nIn this era, the success of large language models and\ntext-to-image models can be attributed to the driving force\nof large-scale datasets. However, in the realm of 3D vi-\nsion, while remarkable progress has been made with mod-\nels trained on large-scale synthetic and real-captured ob-\nject data like Objaverse and MVImgNet, a similar level of\nprogress has not been observed in the domain of human-\ncentric tasks partially due to the lack of a large-scale hu-\nman dataset.\nExisting datasets of high-fidelity 3D hu-\nman capture continue to be mid-sized due to the signifi-\ncant challenges in acquiring large-scale high-quality 3D\nhuman data. To bridge this gap, we present MVHuman-\nNet, a dataset that comprises multi-view human action se-\nquences of 4,500 human identities. The primary focus of\nour work is on collecting human data that features a large\nnumber of diverse identities and everyday clothing using a\nmulti-view human capture system, which facilitates easily\nscalable data collection. Our dataset contains 9,000 daily\noutfits, 60,000 motion sequences and 645 million frames\nwith extensive annotations, including human masks, cam-\nera parameters , 2D and 3D keypoints, SMPL/SMPLX pa-\nrameters, and corresponding textual descriptions. To ex-\nplore the potential of MVHumanNet in various 2D and 3D\nvisual tasks, we conducted pilot studies on view-consistent\naction recognition, human NeRF reconstruction, text-driven\nview-unconstrained human image generation, as well as\n2D view-unconstrained human image and 3D avatar gen-\neration.\nExtensive experiments demonstrate the perfor-\nmance improvements and effective applications enabled by\nthe scale provided by MVHumanNet. As the current largest-\nscale 3D human dataset, we hope that the release of MVHu-\nmanNet data with annotations will foster further innova-\ntions in the domain of 3D human-centric tasks at scale.\n1\narXiv:2312.02963v1  [cs.CV]  5 Dec 2023\n1. Introduction\nIn recent years, the exponential advancements of AI have\nbeen largely driven by the massive amounts of data.\nIn\ncomputer vision, with the emergency of SA-1B [38] and\nLAION-5B [62], models like SAM [38] and Stable Diffu-\nsion [58] have greatly benefited from these large volumes of\ndata, enabling zero-shot transfer to downstream tasks. Sub-\nsequently, Objaverse [18, 19] and MVImgNet [75] break\nbarriers of 3D data collection with large-scale synthetic 3D\nassets and real-world multi-view capture, which support\nZero123[45] and LRM [29] models to achieve impressive\ngeneralization ability of multi-view or 3D reconstruction.\nHowever, comparable progress on human-centric tasks still\nremained elusive due to the limited scale of 3D human data.\nCompared to collecting 3D object datasets, capturing\nhigh-quality and large-scale 3D human avatars is more\ntime-consuming in the same order of scale. Existing 3D\nhuman datasets can be categorized into two distinct repre-\nsentations: 3D human scans and multi-view human images.\nWhile 3D human scan data [57, 74] provides accurate ge-\nometric shapes, it comes with high acquisition costs which\nleads to limited data scale. Conversely, multi-view capture\nprovides an easier way to collect 3D human data. Previ-\nous multi-view human datasets [31, 41, 55] involve only a\nfew dozen human subjects. Recent advances in multi-view\nhuman performance data [12, 13] narrow the gap of data\nscarcity which provides more diverse and representative hu-\nman data for establishing reasonable benchmarks. To en-\nsure comprehensiveness, it is necessary for these datasets to\nconsider the complex clothing and the uncommon human-\nobject interaction. However, incorporating these factors in-\ntroduces complexities for scaling up the dataset.\nTo scale up the 3D human data, we present MVHuman-\nNet, a large-scale multi-view human performance capture\ndataset. Our dataset primarily focuses on casual clothing\ncommonly found in everyday life, enabling to easily ex-\npand the scale of human data collection.\nFor the hard-\nware setup, we establish two 360-degree indoor systems\nequipped with 48 and 24 calibrated RGB cameras, respec-\ntively, to capture high-fidelity videos with resolutions up to\n12MP (4096 \u00d7 3000) and 5MP (2048 \u00d7 2448). Consider-\ning the capture of human data, we intend to cover a wide\nrange of attributes among human subjects, including age,\nbody shape, motion, as well as the colors, types, and mate-\nrials of dressing, enabling our dataset as diverse as possible.\nFurthermore, we also design 500 motion types to guarantee\ncoverage of daily scenarios. Overall, we invite 4,500 in-\ndividuals to participate in the data capture process. Each\nparticipant is recorded in two distinctive outfits (9,000 in\ntotal) and at least seven different motion sequences. Thanks\nto the targeted collection of everyday clothing, data cap-\nture for each participant has been accomplished efficiently\nwithin six months. Eventually, the full dataset comprises an\nextensive collection of 60,000 motion sequences with over\n645 million frames. Compared with the existing multi-view\nhuman datasets [12, 30, 31, 55], MVHumanNet provides a\nsignificantly larger number of human subjects and outfits\nthan previously available. Furthermore, MVHumanNet sur-\npasses the recently proposed DNA-Rendering [13] dataset\nby an order of magnitude in terms of motion and frame\ndata. The detailed comparisons between MVHumanNet and\nother relevant datasets are shown in Tab. 1.\nIn order to benefit downstream human-centric tasks, we\nalso provide essential annotations including action labels,\ncamera intrinsics and extrinsics, human masks, 2D/3D key-\npoints, SMPL/SMPLX parameters and text descriptions to\nenhance the applicability of our dataset. To thoroughly ex-\nplore the capabilities of our dataset, we carefully design\nfour pilot experiments: a) view-consistent action recog-\nnition, b) NeRF reconstruction for human, c) text-driven\nview-unconstrained human image generation, and d) 2D\nview-unconstrained human image and 3D avatar genera-\ntion. First, leveraging the multi-view nature of human cap-\nture data, we can achieve more accurate view-consistent\naction recognition and enhance the generalization capa-\nbility of NeRF as the data scale increases. Furthermore,\nthe unprecedented scale of subjects and outfits, along with\npose sequences and paired textual descriptions, allows us to\nfinetune a remarkable text-driven, pose-conditioned high-\nquality human image generation model. Finally, through the\nexploitation of multi-view human images on a large scale,\nwe can obtain 2D/3D full-body human generative models\nwith promising results. The aforementioned experiments\nreveal the promise and opportunities with the large-scale\nMVHumanNet datasets to boost a wide range of digital hu-\nman applications and inspire future research.\nIn summary, the main contributions of our work include:\n\u2022 We present the largest multi-view human capture\ndataset, which is nearly ten times larger than the re-\ncently proposed DNA-Rendering dataset in terms of\nhuman subjects, motion sequences, and frames.\n\u2022 We conduct several pilot studies that demonstrate the\nproposed MVHumanNet can support various down-\nstream human-centric tasks for effective applications.\n\u2022 We believe that MVHumanNet opens up new possibil-\nities for research in the field of 3D digital human.\n2. Related Work\n3D Human Reconstruction and Generation. Recently,\nwe have witnessed impressive performance in the field of\nimage generation, 3D reconstruction and novel view syn-\nthesis in computer vision community with the emergency\nof Generative Adversarial Networks (GANs) [23, 32, 36],\nNeural Implicit Function [11, 51, 54] and Neural Radiance\nField (NeRF) [52, 53]. These successes inspire subsequent\n2\nDataset\nAge\nCloth\nMotion\n#ID\n#Outfit\n#Actions\n#View\n#Frames\nResolution\nHuman3.6M [31]\n%\n%\n\"\n11\n11\n17\n4\n3.6M\n1000P\nCMU Panoptic [35]\n\"\n%\n\"\n97\n97\n65\n31\n15.3M\n1080P\nMPI-INF-3DHP [50]\n%\n%\n\"\n8\n8\n\u2212\n14\n1.3M\n2048P\nNHR [71]\n%\n%\n\"\n3\n3\n5\n80\n100K\n2048P\nZJU-MoCap [55]\n%\n%\n\"\n10\n10\n10\n24\n180K\n1024P\nNeural Actor [44]\n%\n%\n\"\n8\n8\n\u2212\n11\u223c100\n250K\n1285P\nHUMBI [76]\n\"\n\"\n%\n772\n772\n\u2212\n107\n26M\n1080P\nAIST++ [41]\n%\n%\n%\n30\n30\n\u2212\n9\n10.1M\n1080P\nTHuman 4.0 [81]\n%\n%\n\"\n3\n3\n\u2212\n24\n10K\n1150P\nHuMMan [5]\n%\n\"\n\"\n1000\n1000\n500\n10\n60M\n1080P\nGeneBody [12]\n\"\n\"\n\"\n50\n100\n61\n48\n2.95M\n2048P\nActorsHQ [30]\n%\n%\n\"\n8\n8\n52\n160\n40K\n4096P\nDNA-Rendering [13]\n\"\n\"\n\"\n500\n1500\n1187\n60\n67.5M\n4096P\nMVHumanNet(Ours)\n\"\n\"\n\"\n4500\n9000\n500\n48\n645.1M\n4096P\nTable 1. Dataset comparison on existing multi-view human-centric datasets. MVHumanNet provides a significantly larger number of\nhuman subjects and outfits than previous datasets available, regarding the number of identities (#ID), outfits in total (#Outfit) and frames\nof images (#Frames). Attributes among humans, including age, cloth and motion are covered (denoted by \u2713 for inclusion and \u2717 for\nexclusion.). Cells highlighted in\ndenotes the dataset with best and second-best feature in each column.\nworks [20, 40, 55, 60] to extend reconstruction and genera-\ntion tasks to high-fidelity clothed full-body humans. Many\nefforts have also been made to combine 2D GANs with\nNeRF representations for 3D-aware, photo-realistic image\nsynthesis. EG3D [7] proposes the 3D-aware generation of\nmulti-view face images by introducing an efficient tri-plane\nrepresentation for volumetric rendering. GET3D [22] uti-\nlizes two separate latent codes to generate the SDF and tex-\nture field, enabling the generation of textured 3D meshes.\nEVA3D [27] extends EG3D to learn generative models\nwith human body priors for 3D full-body human gener-\nation from a collection of 2D images.\nHumanGen [33]\nand Get3DHuman [72] further incorporate the priors of\nStyleGAN-Human [21] and PIFuHD [61] for generative hu-\nman model construction. In addition, Text2Human [34] and\nAvatarClip [28] explore to leverage the powerful vision-\nlanguage model CLIP [56] for text-driven 2D and 3D hu-\nman generation. However, the reconstruction, generation\nand novel view synthesis works can only utilize limited real-\nworld human data, which consequently affects the general-\nizability of their models. Moreover, the current methods\nof human generation often train their models on datasets\ncomprising only front-view 2D human images [21, 46] or\nmonocular human videos [77].\nUnfortunately, these ap-\nproaches fail to produce satisfactory results when altering\nthe input image across various camera viewpoints. In this\nwork, we provide the current largest scale of multi-view hu-\nman capture images along with text descriptions to facilitate\n3D human-centric tasks.\n3D Human Scanning Datasets. Understanding human ac-\ntions and reconstructing detailed body geometries with re-\nalistic appearances are challenging tasks that require high-\nquality and large-scale human data. Early works [3, 4, 78]\nin this field provide dynamic human scans but with limited\ndata consisting of only a few subjects or simple postures.\nParallel works such as Northwestern-UCLA [69] and NTU\nRGB+D series [43, 63] utilize more affordable Kinect sen-\nsors to obtain depth and human skeleton data, enabling the\ncapture of both appearance and action cues. However, due\nto the limitations in the accuracy of Kinect sensors, these\ndatasets are inadequate for precise human body modeling.\nSubsequently, AMASS [49] further integrates traditional\nmotion capture datasets [15, 59] and expands them with\nfully rigged 3D meshes to facilitate advancements in hu-\nman action analysis and body modeling research. With the\nemergency of learning-based digital human techniques, rel-\nevant algorithms [9, 60, 61, 73] heavily rely on human scan\ndatasets with high-fidelity 3D geometry and corresponding\nimages. Several studies [26, 48, 64, 74, 79, 80] capture their\nown datasets and release the data to the public for research\npurposes. Additionally, there are several commercial scan\ndatasets [1, 2, 57, 66] that are well-polished and used for re-\nsearch to ensure professional quality. These datasets play a\nfoundational role in bridging the gap between synthetic vir-\ntual avatars and real humans. However, the aforementioned\ndatasets typically exhibit a bias towards standing poses due\nto the complicated capture procedure and cannot afford for\nlarge-scale data collection.\n3\nMulti-view Human Capturing Datasets. Multi-view cap-\nture holds an indispensable role in computer vision, serv-\ning as a fundamental technique for AR/VR and 3D con-\ntent production. Prior works [67, 68] present multi-view\nstereo systems to collect multi-view human images and ap-\nply multi-view constraints to reconstruct 3D virtual charac-\nters. Human3.6M [31] captures numerous 3D human poses\nusing a marker-based motion capture system from 4 cam-\neras. MPI-INF-3DHP [50] annotates both 3D and 2D pose\nlabels for human motion capture in a multi-camera studio.\nCMU Panoptic [35] presents a massively multiview sys-\ntem consisting of 31 HD Cameras to capture social inter-\naction and provides 3D keypoints annotations of multiple\npeople. HUMBI [76] collects local human behaviors such\nas gestures, facial expressions, and gaze movements from\nmultiple cameras.\nAIST++ [41, 65] is a dance database\nthat contains various 3D dance motions reconstructed from\nreal dancers with multi-view videos. These datasets pri-\nmarily focus on human activity motions ranging from daily\nactivities to professional performances, rather than factors\nrelated to identity, cloth texture and body shape diversity.\nWith the recent progress of neural rendering techniques,\nNHR [71], ZJU-Mocap [55], Neural Actor [24, 25, 44] and\nTHuman4.0 [81] present their multi-view human dataset\nfor evaluating the proposed human rendering algorithms.\nHuMMan [5] and Genebody [12] expand the diversity of\npose actions and body shapes for human action recognition\nand modeling. ActorsHQ [30] uses dense multi-view cap-\nturing for photo-realistic novel view synthesis but is lim-\nited to 16 motion sequences and 8 actors. Recently, with\nthe presence of the large-scale synthetic data and real cap-\ntures from Objaverse [18, 19] and MVImgNet [75], sev-\neral methods [29, 45] have made remarkable strides in the\ndirection of open-world 3D reconstruction and generation.\nThe concurrent work, DNA-Rendering [13] emphasizes the\ncomprehensive benchmark functionality, but it encounters\nchallenges in expanding the dataset to a larger scale due to\nthe consideration of unusual human-object interactivity and\nclothes texture complexity. Differing from these efforts, we\ntake a significant step forward in scaling up the human sub-\njects and outfits, leading to the creation of MVHumanNet,\nthe multi-view human capture dataset on the largest scale.\n3. MVHumanNet\nIn this section, we provide a comprehensive overview of\nthe core features of MVHumanNet, with a focus on dataset\nconstruction. We discuss the hardware capture system, data\ncollection arrangements, dataset statistics, and data pre-\nprocessing. Sec. 3.1 provides an illustration to the funda-\nmental aspects of the data acquisition system. This part\nspecifically outlines the key components of the hardware\ncapture system and its capabilities.\nSec. 3.2 delves into\nthe actual data acquisition process, providing detailed in-\nformation on personnel arrangement and the protocols fol-\nlowed during data collection. This section elucidates the\nsteps taken to ensure the accuracy and consistency of the\nacquired data. Finally, in Sec. 3.3, we present a comprehen-\nsive framework that combines manual annotation and ex-\nisting algorithms to obtain diverse and rich annotations for\nMVHumanNet. This framework enhances the applicability\nof our dataset for various research purposes.\n3.1. Multiview Synchronized Capture System\nWe collected all the data using two sets of synchronized\nindoor video capture systems. In this section, we provide\na detailed account of one system, while supplementary ma-\nterials contain additional information about the second sys-\ntem. The primary framework of the capture system consists\nof 48 high-definition industrial cameras with a resolution of\n12MP. These cameras are arranged in a multi-layer structure\nresembling a 16-sided prism, as shown in Fig. 1. The col-\nlection system has approximate dimensions of 2.4 meters in\nheight and a diameter of roughly 4.5 meters. Each prism\nwithin the system is equipped with three 4K high-definition\nindustrial cameras positioned at different heights.The lenses\nof each camera are meticulously aligned towards the center\nof the prism. To ensure clear image capture from different\nperspectives, we have placed light sources at the center of\neach edge of the system. During the data collection pro-\ncess, the frame rate of all cameras is set to 25 frames per\nsecond, enabling the capture of smooth and detailed mo-\ntion sequences. For more comprehensive technical details,\nplease refer to the supplementary materials.\n3.2. Data Capture and Statistics\nData Capture To capture the wide range of dressing habits\nobserved in people\u2019s daily lives, we establish a comprehen-\nsive process for performer recruitment and data collection.\nSpecifically, at regular intervals, we release targeted recruit-\nment requests to the public based on the statistics derived\nfrom the already collected clothing data. This strategy aims\nto enhance the diversity of clothing styles and colors for\nmore reasonable human data distributions to achieve more\nreasonable human data distributions. In accordance with\nthe clothing requirements, each performer is instructed to\nbring two sets of clothing to the capture system. Prior to the\nbeginning of the capturing, performers randomly select 12\nsets of actions from a predefined pool of 500 actions. Subse-\nquently, they enter the capture system and sequentially per-\nform the first six sets of actions, following instructions pro-\nvided by the collection personnel. Each action is performed\nat least once on both the left and right sides for complete ex-\necution of the human performance capture. Upon complet-\ning the sixth set of actions, the performer finishes the first\ncollection session by extending their hands to an A-pose\nand rotating in place twice. Subsequently, the performer\n4\nFigure 2. The distribution of performers\u2019 attributes. The gen-\nder, age, weight, and height of performers are recorded and care-\nfully controlled. The statistical analysis of these attributes reflects\na diverse range among the performers involved in MVHumanNet.\nchanges outfit and repeats the same process to complete the\nremaining six sets of actions with rotations in place.\nData Statistics The essential statistics of our dataset are\nshown in Fig. 2 and Fig. 3. MVHumanNet comprises a\ntotal of 4,500 unique identities with a equitable distribu-\ntion of 2,300 male and 2,200 female individuals, ensuring\na balanced representation of genders. Participants are re-\nquired to fall within the age range of 15 to 75 years old.\nThis age range is chosen to encompass a wide spectrum of\nperformers while considering the potential impact of age on\nthe quality and capabilities of their actions. Conversely, no\nrestrictions are imposed on performers\u2019 weight or height,\nas these variables are deemed to have minimal impact on\nthe data collection process. By not imposing such limita-\ntions, we aim to capture a more diverse and realistic repre-\nsentation of subjects in the dataset, allowing for a broader\nrange of body types and proportions. Our dataset boasts the\nlargest number of unique identities and garment items when\ncompared to existing multi-view human dataset . It encom-\npasses a wide range of everyday clothing styles and colors\nthat are commonly available in real-world scenarios.\n3.3. Data Annotation\nTo enable the advancement of applications in 2D/3D\nhuman understanding, reconstruction and generation, our\ndataset offers comprehensive and diverse annotations along-\nside the raw data.\nThese annotations include action lo-\ncalization, attribute description, human masks, camera cal-\nibrations, 2D/3D skeleton, and parametric model fitting.\nThe annotation pipeline, as depicted in Fig. 4, provides an\noverview of the entire process.\nManually Annotation Before capturing human data, we\ncollect the cloth color and dress type of each performer in\nthe registration table for further manual textual description.\nDuring the data collection process, we ensure a continuous\nflow as performers execute a sequence of six distinct ac-\ntions along with in-place rotations. Subsequently, after the\nFigure 3. The garment type and color distribution of outfits\nof performers. Diverse colors and types of dressing are required\nfor each invited performer. The statistical results show the wide\ncoverage of daily clothes.\nrecording session, we manually mark the breakpoints for\neach action, accurately documenting the start and end of\neach action sequence. Moreover, the supplementary mate-\nrials provide comprehensive records and annotations of the\nperformers\u2019 basic attributes and outfits. For further details,\nplease refer to the supplementary materials.\nHuman Mask Segmentation MVHumanNet comprises\napproximately 645 million images of individuals captured\nfrom various perspectives. Manual segmentation of such a\nmassive image collection is obviously infeasible. To tackle\nthis challenge, we propose a hierarchical automated im-\nage segmentation approach based on off-the-shelf segmen-\ntation algorithms. Our approach follows a coarse-to-fine\nsegmentation strategy. Initially, we employ the RVM [42] to\nobtain efficient rough segmentation results. Subsequently,\nthe rough segmentation outcomes are utilized to generate a\nbounding box of the performer, which serves as a prompt for\nthe SAM [39] to produce higher-quality masks. In Fig. 4,\nthe bottom-left region presents a comparison between the\ncoarse and fine segmentation results. Notably, the masks\ngenerated by SAM exhibit significant superiority to those\ngenerated by RVM. Nevertheless, RVM remains crucial as\nit provides a rough bounding box, ensuring that the fine\nstage SAM segmentation primarily focuses on the individ-\nual rather than other elements.\nCamera Calibration We utilized a commercial solution\nbased on CharuCo boards to achieve fast and efficient cam-\nera calibration. Specifically, we position a CharuCo pat-\nterned calibration board at the central location of the cap-\nture studio. This ensures that each camera can capture a\nclear and complete view of the calibration board. With the\naid of specific software, we obtain the intrinsic, extrinsic\nparameters and distortion coefficient for each camera. We\nalso carefully adjust other parameters, such as lighting, ex-\nposure, and white balance to capture high-quality data.\n2D/3D Skeleton and Parametric Models Following the\nprevious works [5, 12, 13] and with the goal of facilitat-\ning extensive research and applications in 3D digital hu-\nman community, we conducted pre-processing on the entire\n5\nFigure 4. Data annotation pipeline. The manual and automatic\nannotation pipeline for action localization, text description, masks,\n2D/3D keypoints and parametric models.\ndataset to obtain corresponding 2D/3D skeletons and two\nparameterized models. The processing pipeline is visually\ndepicted in the bottom-right section of Fig. 4. Specifically,\nwe employed the OpenPose [6] to predict 2D skeletons for\neach frame of the images. Leveraging the calibrated cam-\nera parameters, multi-view 2D skeletons, and optimization\nalgorithms [17], we derived the 3D skeletons from multi-\nview triangulation. Finally, SMPL/SMPLX parameters are\nfitted with the constrains of multi-view 2D keypoints and\n3D skeletons. All these labeled data support MVHumanNet\nto be applied to various tasks.\n4. Experiments\nIn this section, we present a comprehensive series of ex-\nploratory experiments conducted in the human action un-\nderstanding, reconstruction, and generation tasks. Specif-\nically, Sec. 4.1 focuses on showcasing experiments per-\ntaining to view-consistent action recognition.\nAs the\ndataset expands from single-view 2D data to multi-view 3D\ndata, existing algorithms may encounter new challenges.\nIn Sec. 4.2, we demonstrate experiments on generalizable\nNeRF (Neural Radiance Fields) reconstruction approaches,\nemphasizing the augmented model performance and gen-\neralization capabilities resulting from the increased avail-\nability of data. At last, in Sec. 4.3 and Sec. 4.4, we delve\ninto recent research tasks, specifically text-driven view-\nunconstrained image generation and 3D human avatar gen-\nerative model. Taking into account the size of the dataset,\nhardware limitations, and data annotation constraints, we\nperformed experiments utilizing 62% of the available data.\nMore precisely, we employed 2800 identities, each repre-\nsenting a unique set of attire, amounting to a total of 5500\nsets. Within this subset, 10% of the data was reserved ex-\nclusively for testing purposes.\n4.1. View-consistent Action Recognition\nMVHumanNet provides action labels with 2D/3D skele-\nton annotations, which can verify its usefulness on action\nrecognition tasks. To simulate real-world scenarios, we em-\nployed single-view 2D skeletons as input and conducted\ntests on a multi-view test set that accurately represented real\nscenes. Our experimentation involved 8 viewpoints spaced\nat 45-degree intervals. The training data encompassed ap-\nproximately 4000 outfits, while the testing data included\n400 outfits, covering a total of 500 action labels. The re-\nsults, presented in Tab. 2, reveal that the accuracy of action\nestimation was notably low for a single viewpoint, achiev-\ning a top-1 accuracy of only around 30%. However, as the\nnumber of input viewpoints increased, the accuracy of ac-\ntion estimation exhibited a significant improvement, peak-\ning at 78.19%. Given that the dataset covers a comprehen-\nsive range of daily full-body actions, we possess confidence\nin its efficacy for facilitating diverse understanding tasks.\nConsidering the challenges associated with acquiring 3D\nskeletons in everyday life, see supplementary for the results\nof 3D skeleton-based action recognition.\n4.2. NeRF Reconstruction for Human\nMVHumanNet can also be applied to NeRF reconstruc-\ntion for human. Currently, human-centric methods, e.g. GP-\nNeRF [8], are developed in the context of lacking multi-\nview human data and their performance is still far from\nsatisfactory on more diverse testing cases. We hope our\nproposed MVHumanNet can motivate more extensive stud-\nTrain\nviews\nCTR-GCN[10]\nInfoGCN[14]\nFR-Head[82]\nTop-1\n(%)\u2191\n1-view\n2-views\n4-views\n8-views\n33.85\n60.33\n72.16\n76.73\n25.23\n55.89\n73.59\n76.55\n30.25\n59.16\n71.74\n78.19\nTop-5\n(%)\u2191\n1-view\n2-views\n4-views\n8-views\n51.08\n80.09\n88.32\n91.34\n37.14\n75.00\n89.02\n91.00\n50.59\n78.80\n88.67\n92.45\nTable 2. Performance comparison of skeleton-based action recog-\nnition SOTA methods on MVHumanNet. With the increase of the\nviews, the accuracy of the action prediction increases together.\n6\nNumber of\noutfits\nIBRNet [70]\nGPNeRF [8]\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\n100\n26.05\n0.9571\n0.0555\n23.27\n0.8688\n0.2077\n2000\n27.45\n0.9638\n0.0486\n24.14\n0.8779\n0.2137\n5000\n29.00\n0.9706\n0.0377\n24.69\n0.8878\n0.1961\nTable 3. Quantitative comparison of generalizable NeRFs with\ndifferent scales of data for training. We compare the results of\nmethods with human prior and without human prior. We refer\nhuman prior to the commonly used SMPL model.\nFigure 5. The novel view synthesis results of IBRNet and GPN-\neRF on unseen data of MVHumanNet. GT means ground truth.\nThe number of 100, 2000, and 5000 indicate the respective quan-\ntities of outfits utilized during the training process.\nies of generalizable NeRF for human with sufficiently large\nscale of data. We empirically explore the performance of\ntwo distinct generalizable NeRFs methods, IBRNet [70]\nwhich is designed for general scenes and GPNeRF [8]\nwhich relies on human prior (i.e. SMPL [47]), using varying\namounts of data for training. In our experiment, both ap-\nproaches utilize four evenly distributed views as input and\ninference the novel view results. The quantitative compar-\nisons of the outcomes are presented in Tab. 3, while the\nvisualization results can be found in Fig. 5. Experimental\nresults confirm that as the training data increases, the model\nexhibits enhanced generalization capabilities for new cases,\nespecially when facing rare poses and complex garments.\nMoreover, we provided empirical evidence that MVHuman-\nNet can also serve for pretraining strong models, facilitat-\ning methods to perform better on out-of-domain scenarios.\nThe corresponding results are presented in Tab. 4. Please\nnote that the quantitative results of IBRNet [70] and GPN-\neRF [8] cannot be directly compared, as they have different\nevaluation settings. More detailed explanations are in Supp.\n4.3. Text-driven Image Generation\nMVHumanNet is able to serves as a fundamental re-\nsource for our text-driven image generation method. The in-\nclusion of comprehensive pose variations within our dataset\nenhances the potential for generating diverse human images\nMethod\nIBRNet [70]\nGPNeRF [8]\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nTrain from scratch\n28.06\n0.9679\n0.0437\n20.95\n0.9049\n0.1809\nw/o fintune\n27.48\n0.9663\n0.0440\n20.15\n0.8921\n0.2050\nw/ fintune\n29.46\n0.9734\n0.0323\n21.89\n0.9252\n0.1364\nTable 4.\nUsing MVHumanNet to pretrain a strong model.\nWe first train the representative methods on MVHumanNet, and\nthen finetune the trained models on the train set of HuMMan [5].\nWe compare the performance of the finetuned models and models\ntrained from scratch on the test set of HuMMan.\nFigure 6. Qualitative comparison of IBRNet and GPNeRF on\nthe test set of HuMMan. Without finetuning, the models only\ntrained on MVHumanNet may suffer from domain gap. With some\ntime for finetuning, the models outperform the ones trained merely\non the train set of HuMMan.\nin accordance with text descriptions. We finetune the pow-\nerful text-to-image model, Stable Diffusion [58] on MVHu-\nmanNet dataset to enable text-driven realistic human image\ngeneration. As shown in Fig. 7, given a text description and\na target SMPL pose, we can produce high-quality results\nwith the same consistency as text description and SMPL.\nBased on the results derived from the text-driven image\ngeneration, it becomes evident that the utilization of large-\nscale multi-view data from real capture contributes to the\nefficacy of text-driven realistic human image generation.\n4.4. Human Generative Model\nRecently, generative models have become a promi-\nnent and highly researched area. Methods such as Style-\nGAN [21, 37] have emerged as leading approaches for gen-\nerating 2D digital human. More recently, the introduction of\nGET3D [22] has expanded this research area to encompass\nthe realm of 3D generation. With the availability of massive\ndata in MVhumanNet, we embark on an exploratory jour-\nney as pioneers, aiming to investigate the potential applica-\ntions of existing 2D and 3D generative models by leverag-\ning a large-scale dataset comprising real-world multi-view\nfull-body data. We conduct experiments to unravel the pos-\nsibilities within this context.\n2D Generative Model Giving a latent code sampled from\nGuassian distribution, StyleGAN2 outputs a reasonable 2D\nimages. In this part, we feed approximately 198,000 multi-\nview A-pose images (5500 outfits) and crop to 1024\u00d71024\nresolution into the network with camera conditions for\ntraining. Fig. 8 visualizes the results. Our model not only\nproduces frontage full-body images but also demonstrates\n7\nFigure 7.\nThe visualization of images generated by text-to-\nimage model trained on MVHumanNet with SMPL condition\nand text prompts as input. The results demonstrate that training\non our large-scale high-quality human dataset enables the gener-\nation of high-resolution human images using textual description\nand SMPL conditions. Supp. shows more results.\nFigure 8.\nVisualize the results of StyleGAN2 trained with\nMVHumanNet. We randomly sample latent codes from Gaussian\ndistribution and obtain the results. See supp. for more results.\nthe capability to generate results from other views, includ-\ning the back and side views.\n3D Generative Model Unlike StyleGAN2, GET3D [22] in-\ntroduces a distinct requirement of one latent code for geom-\netry and another for texture. We use the same amount of\ndata as training StyleGAN2 to train GET3D. The visual-\nization results are shown in Fig. 9. The model exhibits the\nability to generate reasonable geometry and texture in the\nA-pose, thereby enabling its application in various down-\nstream tasks.\nWith the substantial support provided by\nMVHumanNet, various fields, including 3D human gener-\nation, can embark on further exploration by transitioning\nfrom the use of synthetic data or single-view images to the\nincorporation of authentic multi-view data. We also conduct\nexperiments to prove that the performance of the generative\nmodel will become more powerful with the increase in the\namount of data. The quantitative results are shown in Tab. 5.\nWe have reason to believe that with the further increase of\ndata, the ability of trained models can further improve.\nFigure 9.\nThe visualization results of GET3D trained with\nMVHumanNet rendered by Blender [16]. The first and third\nrows represent the geometry, while the second and fourth row\nshows the texture corresponding to geometry.\nNumber of Subjects\nFID\u2193\nStyleGAN2 [37]\nGET3D [22]\n3000\n14.05\n41.54\n5500\n7.08 (-6.97)\n25.12 (-16.42)\nTable 5. Quantitative comparison of generative models with\ndifferent data scale. The performance of both 2D and 3D gener-\native models exhibits obvious improvement with scaling up data.\n5. Conclusion\nIn this work, we present MVHumanNet, a large-scale\nmulti-view dataset containing 4,500 human identities, 9,000\ndaily outfits and 645 million frames with extensive annota-\ntions. We primarily focus on the domain of collecting daily\ndressing, which allows us to easily scale up the human data.\nTo probe the potential of the proposed large-scale dataset,\nwe design four experiments to show how MVHumanNet\ncan be used to power these 3D human-centric tasks. We\nplan to release the MVHumanNet dataset with annotations\npublicly and hope that it will serve as a foundation for fur-\nther research in the 3D digital human community.\nTo mitigate potential negative social impacts, we will im-\nplement strict regulations on the utilization of our data.\nFuture Work. We will incorporate all the data to further ex-\nplore the possibilities of scaling up the training data. In ad-\ndition, existing human-centric generalizable NeRF methods\nare designed with significant considerations of data scarcity,\nwhich may be adverse to generalization as they highly rely\non the coarse representation of human body, i.e. SMPL\nmodel. With the largest scale data contained in MVHuman-\nNet, these methods can get rid of the SMPL model and be\nredesigned to achieve better generalization.\n8\nReferences\n[1] 3d people. https://3dpeople.com/. 3\n[2] AXYZ. https://secure.axyz-design.com/. 3\n[3] Federica Bogo,\nJavier Romero,\nMatthew Loper,\nand\nMichael J Black. Faust: Dataset and evaluation for 3d mesh\nregistration. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, 2014. 3\n[4] Federica Bogo, Javier Romero, Gerard Pons-Moll, and\nMichael J Black. Dynamic faust: Registering human bod-\nies in motion. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017. 3\n[5] Zhongang Cai, Daxuan Ren, Ailing Zeng, Zhengyu Lin, Tao\nYu, Wenjia Wang, Xiangyu Fan, Yang Gao, Yifan Yu, Liang\nPan, et al. Humman: Multi-modal 4d human dataset for ver-\nsatile sensing and modeling.\nIn European Conference on\nComputer Vision. Springer, 2022. 3, 4, 5, 7\n[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affinity\nfields. In CVPR, pages 7291\u20137299, 2017. 6\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022. 3\n[8] Mingfei Chen, Jianfeng Zhang, Xiangyu Xu, Lijuan Liu, Yu-\njun Cai, Jiashi Feng, and Shuicheng Yan. Geometry-guided\nprogressive nerf for generalizable and efficient neural human\nrendering. In ECCV, pages 222\u2013239. Springer, 2022. 6, 7\n[9] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,\nand Andreas Geiger. Snarf: Differentiable forward skinning\nfor animating non-rigid neural implicit shapes. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 11594\u201311604, 2021. 3\n[10] Yuxin Chen, Ziqi Zhang, Chunfeng Yuan, Bing Li, Ying\nDeng, and Weiming Hu. Channel-wise topology refinement\ngraph convolution for skeleton-based action recognition. In\nICCV, 2021. 6\n[11] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5939\u20135948, 2019. 2\n[12] Wei Cheng, Su Xu, Jingtan Piao, Chen Qian, Wayne Wu,\nKwan-Yee Lin, and Hongsheng Li.\nGeneralizable neural\nperformer: Learning robust radiance fields for human novel\nview synthesis. arXiv preprint arXiv:2204.11798, 2022. 2,\n3, 4, 5\n[13] Wei Cheng, Ruixiang Chen, Siming Fan, Wanqi Yin, Keyu\nChen, Zhongang Cai, Jingbo Wang, Yang Gao, Zhengming\nYu, Zhengyu Lin, et al. Dna-rendering: A diverse neural\nactor repository for high-fidelity human-centric rendering. In\nICCV, 2023. 2, 3, 4, 5\n[14] Hyung-gun Chi,\nMyoung Hoon Ha,\nSeunggeun Chi,\nSang Wan Lee, Qixing Huang, and Karthik Ramani.\nIn-\nfogcn: Representation learning for human skeleton-based ac-\ntion recognition. In CVPR, 2022. 6\n[15] CMU Graphics Lab. http://mocap.cs.cmu.edu/. 3\n[16] Blender Online Community. Blender - a 3d modelling and\nrendering package, 2018. http://www.blender.org.\n8\n[17] EasyMocap Contributors. Easymocap - make human motion\ncapture easier. Github, 2021. https://github.com/\nzju3dv/EasyMocap. 6\n[18] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 2, 4\n[19] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, 2023. 2, 4\n[20] Anna Fr\u00a8uhst\u00a8uck, Krishna Kumar Singh, Eli Shechtman,\nNiloy J Mitra, Peter Wonka, and Jingwan Lu. Insetgan for\nfull-body image generation. In CVPR, 2022. 3\n[21] Jianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen\nQian, Chen Change Loy, Wayne Wu, and Ziwei Liu.\nStylegan-human: A data-centric odyssey of human genera-\ntion. In ECCV. Springer, 2022. 3, 7\n[22] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35, 2022. 3, 7, 8\n[23] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nNeural Information Processing Systems, 2014. 2\n[24] Marc Habermann, Weipeng Xu, Michael Zollhofer, Gerard\nPons-Moll, and Christian Theobalt.\nDeepcap: Monocu-\nlar human performance capture using weak supervision. In\nCVPR, 2020. 4\n[25] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zoll-\nhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time\ndeep dynamic characters. ACM Transactions on Graphics\n(ToG), 40(4), 2021. 4\n[26] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju-Mi Kang,\nYoung-Jae Park, and Hae-Gon Jeon. High-fidelity 3d hu-\nman digitization from single 2k resolution images. In CVPR,\n2023. 3\n[27] Fangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and\nZiwei Liu. Eva3d: Compositional 3d human generation from\n2d image collections.\narXiv preprint arXiv:2210.04888,\n2022. 3\n[28] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201319, 2022. 3\n[29] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou,\nDifan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao\nTan. Lrm: Large reconstruction model for single image to\n3d, 2023. 2, 4\n[30] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\n9\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. ACM Transactions on Graphics (TOG),\n42(4), 2023. 2, 3, 4\n[31] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian\nSminchisescu. Human3.6m: Large scale datasets and predic-\ntive methods for 3d human sensing in natural environments.\nIEEE transactions on pattern analysis and machine intelli-\ngence, 36(7):1325\u20131339, 2013. 2, 3, 4\n[32] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017. 2\n[33] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wen-\nzheng Chen, and Lan Xu.\nHumangen: Generating hu-\nman radiance fields with explicit priors. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023. 3\n[34] Yuming Jiang, Shuai Yang, Haonan Qiu, Wayne Wu,\nChen Change Loy, and Ziwei Liu. Text2human: Text-driven\ncontrollable human image generation. ACM Transactions on\nGraphics (TOG), 41(4):1\u201311, 2022. 3\n[35] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,\nIain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser\nSheikh. Panoptic studio: A massively multiview system for\nsocial motion capture. In ICCV, 2015. 3, 4\n[36] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401\u20134410, 2019. 2\n[37] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110\u20138119, 2020. 7, 8\n[38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and\nRoss Girshick. Segment anything. In ICCV, 2023. 2\n[39] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and\nRoss Girshick. Segment anything. In ICCV, pages 4015\u2013\n4026, 2023. 5\n[40] Youngjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry\nFuchs. Neural human performer: Learning generalizable ra-\ndiance fields for human performance rendering. Advances in\nNeural Information Processing Systems, 2021. 3\n[41] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3d dance\ngeneration with aist++. In ICCV, 2021. 2, 3, 4\n[42] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta. Robust high-resolution video matting with tempo-\nral guidance. In WACV, pages 238\u2013247, 2022. 5\n[43] Jun Liu, Amir Shahroudy, Mauricio Perez, Gang Wang,\nLing-Yu Duan, and Alex C Kot. Ntu rgb+ d 120: A large-\nscale benchmark for 3d human activity understanding. IEEE\ntransactions on pattern analysis and machine intelligence,\n2019. 3\n[44] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol. ACM transactions on graphics (TOG), 40(6), 2021. 3,\n4\n[45] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 9298\u20139309, 2023. 2, 4\n[46] Ziwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou\nTang.\nDeepfashion: Powering robust clothes recognition\nand retrieval with rich annotations. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 1096\u20131104, 2016. 3\n[47] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. SMPL: A skinned multi-\nperson linear model. ACM Transactions on Graphics(ToG),\n34(6), 2015. 7\n[48] Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades,\nGerard Pons-Moll, Siyu Tang, and Michael J Black. Learn-\ning to dress 3d people in generative clothing. In CVPR, 2020.\n3\n[49] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-\nard Pons-Moll, and Michael J. Black. AMASS: Archive of\nmotion capture as surface shapes. In International Confer-\nence on Computer Vision, 2019. 3\n[50] Dushyant Mehta,\nHelge Rhodin,\nDan Casas,\nPascal\nFua, Oleksandr Sotnychenko, Weipeng Xu, and Christian\nTheobalt. Monocular 3d human pose estimation in the wild\nusing improved cnn supervision. In International conference\non 3D vision (3DV). IEEE, 2017. 3, 4\n[51] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4460\u20134470, 2019. 2\n[52] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2\n[53] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4), 2022. 2\n[54] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 2\n[55] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\n2021. 2, 3, 4\n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\n10\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[57] Renderpeople. https://renderpeople.com/. 2, 3\n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 7\n[59] Huang Geng Sai Charan Mahadevan, Karunanidhi Durai Ku-\nmar. https://mocap.cs.sfu.ca/. 3\n[60] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In ICCV, 2019. 3\n[61] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul\nJoo. Pifuhd: Multi-level pixel-aligned implicit function for\nhigh-resolution 3d human digitization. In CVPR, 2020. 3\n[62] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35, 2022. 2\n[63] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+ d: A large scale dataset for 3d human activity anal-\nysis. In CVPR, 2016. 3\n[64] Kaiyue Shen, Chen Guo, Manuel Kaufmann, Juan Jose\nZarate, Julien Valentin, Jie Song, and Otmar Hilliges. X-\navatar: Expressive human avatars. In CVPR, 2023. 3\n[65] Shuhei Tsuchida, Satoru Fukayama, Masahiro Hamasaki,\nand Masataka Goto. Aist dance video database: Multi-genre,\nmulti-dancer, and multi-camera database for dance informa-\ntion processing. In ISMIR, 2019. 4\n[66] Twindom. https://web.twindom.com/. 3\n[67] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan\nPopovi\u00b4c. Articulated mesh animation from multi-view sil-\nhouettes. ACM Transactions on Graphics (TOG), 27(3):1\u20139,\n2008. 4\n[68] Daniel Vlasic, Pieter Peers, Ilya Baran, Paul Debevec, Jovan\nPopovi\u00b4c, Szymon Rusinkiewicz, and Wojciech Matusik. Dy-\nnamic shape capture using multi-view photometric stereo. In\nACM SIGGRAPH Asia 2009 papers, pages 1\u201311. 2009. 4\n[69] Jiang Wang, Xiaohan Nie, Yin Xia, Ying Wu, and Song-\nChun Zhu. Cross-view action modeling, learning and recog-\nnition. In CVPR, 2014. 3\n[70] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In CVPR,\n2021. 7\n[71] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. Multi-\nview neural human rendering. In CVPR, 2020. 3, 4\n[72] Zhangyang Xiong, Di Kang, Derong Jin, Weikai Chen, Lin-\nchao Bao, Shuguang Cui, and Xiaoguang Han. Get3dhuman:\nLifting stylegan-human into a 3d generative model using\npixel-aligned reconstruction priors. In ICCV, 2023. 3\n[73] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J\nBlack. Icon: Implicit clothed humans obtained from nor-\nmals. In CVPR, 2022. 3\n[74] Tao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qiong-\nhai Dai, and Yebin Liu. Function4d: Real-time human vol-\numetric capture from very sparse consumer rgbd sensors. In\nCVPR, pages 5746\u20135756, 2021. 2, 3\n[75] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,\nChongjie Ye, Yushuang Wu, Zizheng Yan, Chenming Zhu,\nZhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-\nscale dataset of multi-view images. In CVPR, 2023. 2, 4\n[76] Zhixuan Yu, Jae Shin Yoon, In Kyu Lee, Prashanth\nVenkatesh, Jaesik Park, Jihun Yu, and Hyun Soo Park.\nHumbi: A large multiview dataset of human body expres-\nsions. In CVPR, 2020. 3, 4\n[77] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and\nLeonid Sigal.\nDwnet:\nDense warp-based network for\npose-guided human video generation.\narXiv preprint\narXiv:1910.09139, 2019. 3\n[78] Chao Zhang, Sergi Pujades, Michael J Black, and Gerard\nPons-Moll. Detailed, accurate, human shape estimation from\nclothed 3d scan sequences. In CVPR, 2017. 3\n[79] Yang Zheng, Ruizhi Shao, Yuxiang Zhang, Tao Yu, Zerong\nZheng, Qionghai Dai, and Yebin Liu. Deepmulticap: Perfor-\nmance capture of multiple characters using sparse multiview\ncameras. In ICCV, 2021. 3\n[80] Zerong Zheng, Tao Yu, Yixuan Wei, Qionghai Dai, and\nYebin Liu. Deephuman: 3d human reconstruction from a\nsingle image. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019. 3\n[81] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields\nfor human avatar modeling. In CVPR, 2022. 3, 4\n[82] Huanyu Zhou, Qingjie Liu, and Yunhong Wang.\nLearn-\ning discriminative representations for skeleton based action\nrecognition. In ICCV, 2023. 6\n11\n"
  },
  {
    "title": "Fine-grained Controllable Video Generation via Object Appearance and Context",
    "link": "https://arxiv.org/pdf/2312.02919.pdf",
    "upvote": "9",
    "text": "Fine-grained Controllable Video Generation via Object Appearance and Context\nHsin-Ping Huang1,2\nYu-Chuan Su1\nDeqing Sun1\nLu Jiang1\nXuhui Jia1\nYukun Zhu1\nMing-Hsuan Yang1,2\n1Google Research\n2UC Merced\nAbstract\nText-to-video generation has shown promising results.\nHowever, by taking only natural languages as input, users\noften face difficulties in providing detailed information to\nprecisely control the model\u2019s output. In this work, we pro-\npose fine-grained controllable video generation (FACTOR)\nto achieve detailed control. Specifically, FACTOR aims to\ncontrol objects\u2019 appearances and context, including their\nlocation and category, in conjunction with the text prompt.\nTo achieve detailed control, we propose a unified framework\nto jointly inject control signals into the existing text-to-video\nmodel. Our model consists of a joint encoder and adaptive\ncross-attention layers. By optimizing the encoder and the\ninserted layer, we adapt the model to generate videos that\nare aligned with both text prompts and fine-grained control.\nCompared to existing methods relying on dense control sig-\nnals such as edge maps, we provide a more intuitive and\nuser-friendly interface to allow object-level fine-grained\ncontrol. Our method achieves controllability of object ap-\npearances without finetuning, which reduces the per-subject\noptimization efforts for the users. Extensive experiments on\nstandard benchmark datasets and user-provided inputs vali-\ndate that our model obtains a 70% improvement in control-\nlability metrics over competitive baselines. Project page:\nhttps://hhsinping.github.io/factor\n1. Introduction\nWith numerous text-to-video models [5,15,20,22,26,40,\n41,43,47,48,63] demonstrating impressive visual quality in\ntheir generated results, users can now translate their creative\nideas into video content with a simple text prompt and a tap.\nHowever, it often requires careful design and iterative revi-\nsion of the text prompt to generate a video that fulfills users\u2019\nneeds. As shown in Fig. 1 (top), to generate videos with spe-\ncific object movements and appearances, the users can aug-\nment the text prompt with additional descriptions such as\n\u201cfrom right to left\u201d and \u201cyellow and black trim.\u201d However,\ncurrent text-to-video models might fail to generate videos\nthat reflect the additional, fine-grained descriptions due to\nthe high flexibility of text prompts and the limited capabil-\nity of models, specifically in controllability.\nRecently, a few approaches have been developed to en-\nhance the controllability of text-to-video models by inject-\ning additional inputs. Among this direction, structure, and\nappearance are the two prevalent and intuitive forms of user\ncontrol. Several models [7,26,49,60] are proposed to gen-\nerate videos conditioned on structural controls such as op-\ntical flow, depth, and edge maps.\nSimilarly, video edit-\ning [16, 31, 35, 44] takes a video as input and uses natural\nlanguages to manipulate the content and style. Although\nthese models have demonstrated promising visual results,\nthey rely on control inputs typically dense in time and space,\ni.e., provided for each pixel in each frame. Unlike text-to-\nimage generation, providing such inputs for the whole video\nby manual hand drawing is impractical. Thus, the structural\ninputs are usually obtained from a reference video (Fig. 1\nmiddle). In addition, a few attempts have been made to\nachieve appearance control, i.e., subject customization for\nvideo generation. However, these methods either rely on\ntaking the dense control as input [26,49,62] or require fine-\ntuning per subject/domain using a set of images [5,17,38].\nMoreover, existing controllable video generation models\nare limited to taking only a single type of control, either\nstructural [26] or appearance [5]. To achieve controllability\nwith multiple control signals, the model has to be finetuned\nin a sequential manner [49].\nIn this work, we tackle fine-grained controllable video\ngeneration via object appearance and context, a user-\nfriendly controllable video generation framework. In this\nframework, the users first specify the objects in the video\nand then provide fine-grained control to generate each ob-\nject, including their appearance and context. The context to\ngenerate each entity includes 1) the description of the entity,\n2) the user-drawing trajectory, and 3) the user-provided ref-\nerence image to achieve both structural and appearance con-\ntrol. This setting has the following advantages: First, our\nmodel accepts sparse hand-drawing object trajectories as\nintuitive inputs that require minimal effort for users to pro-\nvide, in contrast to dense structural guidance, such as edge\nmaps. Second, unlike prior works that utilize per-subject\nfinetuning to achieve appearance control, our method is\n1\narXiv:2312.02919v1  [cs.CV]  5 Dec 2023\nPrompt+Trajectory: \u201cA car driving in Paris from right to left.\u201d\nPrompt+Appearance: \u201cA silver car with yellow and black trim driving in Paris.\u201d\nPrompt: \u201cA car driving in Paris.\u201d\nText-to-Video Generation \nAppearance Control\nSparse Trajectory Control\nText-to-Video + ControlNet\ncar\nFACTOR (Ours)\nReference Video\nControl\nExtraction\nDense Structural Control\nPrompt: \u201cA shiny silver vehicle maneuvers towards a modern glass building.\u201d\nUser Drawing Interface\nFigure 1. Text-to-video generation [41] has limited controllable ability through user-provided prompts. Even when the users augment\nthe text prompt with additional description, the model has difficulty controlling the precise movement and appearance of the object.\nText-to-Video+ControlNet [60] achieves promising visual quality while requiring dense control signals extracted from a reference video.\nFACTOR (Ours) improves the controllability through user-friendly inputs to control the: 1) precise movement of subjects through hand-\ndrawing trajectory and 2) visual appearance by providing reference examples.\nfinetuning-free, and only an inference pass is needed for\nsubject customization, reducing the users\u2019 efforts.\nWe propose a joint encoding and adaptive cross-attention\nframework to facilitate fine-grained, controllable video gen-\neration. We develop the method on top of an off-the-shelf\ntext-to-video model [41]. Our controllable module is a uni-\nfied framework for both types of control signals, and the\nmodule is learned by a single adapting process in contrast\nto prior works that finetune the models for multiple con-\ntrol sequentially [49]. It is generic and can be extended to\nother types of control easily. Specifically, all control signals\nare encoded into a single control sequence through a joint\nencoder, and the adaptive cross-attention layers are inserted\ninto transformer blocks of the model to take the fine-grained\ncontrol inputs. During training, the newly inserted layers\nare updated while other layers of the pre-trained model are\nfixed. This design preserves the text-to-video model\u2019s ca-\npability to generate high-quality videos while introducing\nadaptive layers that augment the model\u2019s ability to produce\nvideos that satisfy the object-level fined-grained control.\nWe conduct quantitative studies on the standard bench-\nmark dataset MSR-VTT [53] and perform a user study to\nvalidate the effectiveness of the proposed approach. Our\nmethod showcases a substantial 67% and 73% enhance-\nment in absolute quality and trajectory control compared\nto the base text-to-video model, as observed through objec-\ntive metrics. The qualitative results show that our method\nachieves fine-grained, controllable video generation for ob-\nject appearance and context. We summarize our contribu-\ntions as follows:\n\u2022 We target the new form of fine-grained controllable video\ngeneration task that aims to synthesize videos via appear-\nance and context from easy-to-give user inputs.\n\u2022 We propose a generic and unified framework for control-\nlable video generation. It is achieved through adaptive\ntraining to augment text-to-video models for fine-grained\ncontrol without test time optimization.\n\u2022 We validate that our method offers better controllability\ncompared to prior works and show the additional benefit\nof our model in creating complex interactions, which is\nchallenging for existing text-to-video models.\n2. Related Work\nText-to-video generation. Text-to-video models have suc-\ncessfully generated open-domain videos conditioned on\ntext prompts by training on large-scale caption-video pairs.\nToken-based models [22, 47, 48] are first proposed for\ntext-to-video generation, which utilizes an auto-regressive\nmodel to predict videos in the latent token space. Several\ndiffusion-based models [20,21,40] extend the 2D diffusion\nmodel to generate videos using 3D U-Net and a series of\nsuper-resolution modules. Recent advances [5,15,26,43,63]\nadopt a text-to-image diffusion model [37] to generate\nvideos by inflating the model and incorporating the tem-\nporal layers. While these models produce promising visual\nresults, their controllability over generated videos is limited\nsince text prompts cannot accurately convey precise control\n2\nEntity \nControl\n\ud835\udc52\ud835\udc61\n\ud835\udc5b \u2208 \ud835\udc452\n[PAD]\nN entities\n\ud835\udc52\ud835\udc47\n0\n\u2026\n\ud835\udc520\n0\n\ud835\udc520\n1\n\u2026\n[PAD]\n\ud835\udc521\n1\n[PAD]\n[PAD] \u2026\n\u2026\nControl \ud835\udc50\ud835\udc61 \u2208 \ud835\udc452\nT timesteps\nJoint Encoder and Adaptive Cross-Attention \n(Sec. 3.3)\nCondition Encoding\n(Sec. 3.4)\nEntity Control \ud835\udc52\ud835\udc61\n\ud835\udc5b \u2208 \ud835\udc452\nCross-Attention\nPrompt \ud835\udc5d\ud835\udc61\nAdaptive-Attention\nSelf-Attention\nVideo Features \ud835\udc63\nControl \ud835\udc50\ud835\udc61\nTransformer \nBlocks\nAppearance r\nDescription d\nLocation l\nEntity\nEncoder\nLocation\nEncoder\nImage\nEncoder\n\ud835\udf13\ud835\udc52\ud835\udc5b\ud835\udc61\ud835\udc56\ud835\udc61\ud835\udc66(\ud835\udc51)\n\ud835\udf13\ud835\udc50\ud835\udc5c\ud835\udc5c\ud835\udc5f\ud835\udc51(\ud835\udc59)\n\ud835\udf13\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc54\ud835\udc52(\ud835\udc5f)\nConcatenation\n\ud835\udc52\ud835\udc47\n\ud835\udc41\nEntity Control\n(Sec. 3.4)\nJoint \nEncoder\nFigure 2. Overview. a) Joint encoder and adaptive cross-attention: a joint encoder is learned to encode the prompt and control to capture\nthe interaction between them. The adaptive layers are inserted into the transformer blocks of the text-to-video model to take new control\nsignals. Only the inserted layers are optimized to adapt the model to generate videos satisfying the fine-grained control. b) Condition\nencoding: given T time steps, the embedding of control at time t, ct, is formed by the control for N entities, en\nt , where padding tokens\nreplace the embedding of the non-existing entity. c) Entity control: the control for entity n at time t, en\nt , is formed by the embedding of\nthe context including the description, location, and reference appearance of the objects.\nsignals such as the location and appearance of objects. In\nthis work, we focus on improving the controllability of text-\nto-video models through user-friendly inputs. We adopt a\ngenerative transformer model [41] as our base text-to-video\nmodel due to its fast inference speed and flexibility to take\nmultiple control signals but the approach can be adapted to\ncommon diffusion-based models easily.\nControllable text-to-image generation.\nVarious mod-\nels have been proposed to improve the controllable abil-\nity over text prompts for text-to-image models, particu-\nlarly the structure and appearance. ControlNet [59], T2I-\nAdapter [33] and Uni-ControlNet [61] add various spatial\ncontrol signals, including edges, depth, segmentation, hu-\nman pose, to a pre-trained diffusion model. GLIGEN [29]\nadds spatial and appearance control to the text-to-image\nmodel. Some works focus on structure-guided generation\nfrom semantic layouts [1, 18, 54, 56, 58] and training-free\napproaches [3,11,27,50,57].\nTo control the appearance of subjects for text-to-image\nmodels, Textual Inversion [13], DreamBooth [38] and Cus-\ntomDiffusion [28] either directly optimize a word vector to\nlearn the target appearance, or finetune the text-to-image\nmodel on reference images of the subjects. Several encoder-\nbased methods [14,46] are proposed to achieve customiza-\ntion without finetuning by projecting reference images of\nthe subject into the word embeddings. In this work, we\nstudy finetuning-free subject customization for videos with\ntrajectory control.\nControllable text-to-video generation.\nTo extend con-\ntrollable text-to-image models to generate videos, several\nworks [7, 9, 10, 12, 26, 30, 32, 45, 49, 51, 60] adopt the Con-\ntrolNet [59] backbone to condition the generation on a se-\nquence of structural inputs including depth, pose, and edge\nmaps. By training a temporal attention layer built upon the\nControlNet model, these methods learn to produce tempo-\nrally consistent videos. However, they require a dense se-\nquence of structural inputs as the condition, which usually\nneed to be extracted from reference videos. Similarly, video\nediting approaches [6,16,23,25,31,34\u201336,39,42,44,52,55]\nfocus on changing the appearance and style of foreground\nobjects and background through text prompts. These meth-\nods also rely on the dense structure provided in the input\nvideo, which is infeasible to be drawn by the users. In con-\ntrast, our work focuses on controllable text-to-video gen-\neration that takes sparse and user-friendly control signals,\nwhich remains less explored in this field.\nAs for appearance control, a few approaches [19,26,49,\n62] show video customization results using finetuning ap-\nproach while still relying on the dense structural input to\ngenerate videos. Others [5, 17] perform finetuning-based\ncustomization without spatial control. In contrast, we devel-\noped a finetuning-free method to achieve appearance con-\ntrol of multiple subjects, and the model conducts spatial\ncontrol using sparse trajectory inputs simultaneously.\n3. Method\nAs our method is developed upon a text-to-video gen-\nerative transformer, we first briefly review the base model\n(Sec. 3.1). Then, we give an overview of our fine-grained\ncontrollable video generation (Sec. 3.2). Next, we introduce\nthe proposed method, which consists of a joint encoder of\nlearning the interaction between different controls and an\nadaptive cross-attention module to adapt the text-to-video\nmodel to achieve fine-grained control (Sec. 3.3). Finally, we\npresent the encoding process of our entity-level fine-grained\ncontrol and the data preparation pipeline (Sec. 3.4).\n3.1. Preliminaries: Text-to-Video Generation\nThis section presents our base text-to-video generation\nmodel [41]. It consists of an encoder-decoder model that\n3\nencodes the video into discrete video tokens and a bidirec-\ntional transformer model that predicts the video tokens con-\nditioned on the embedding of text prompts. Specifically, the\nvideo is encoded and flattened into a long sequence as the\ninput to the transformer. At training time, the tokens are\nreplaced with a special token [MASK], and the transformer\nmodel is optimized to predict the tokens at [MASK] loca-\ntion conditioned on the text embedding. We minimize the\nnegative log-likelihood of conditional video token predic-\ntion for the masked tokens vt, t \u2208 M, M denotes the subset\nof video tokens that are masked, v denotes the video tokens\nsequence, vM denotes the masked version of v. The model\nis trained by:\nL = \u2212 E\nv\u2208D\nX\nt\u2208M\nlog p(vt|vM, p)\n(1)\nAt inference time, all tokens are replaced with [MASK],\nand the transformer model iteratively predicts the tokens\nconditioned on p.\nThe video token prediction model contains a series of\ntransformer blocks. The video embeddings are first input\nto the self-attention.\nThen, a cross-attention layer takes\nvideo and text embeddings as input to condition the video\ntoken prediction on the text embeddings. We use a token-\nbased generative transformer model for its fast inference\nand inherent flexibility in modeling various control signals\ndirectly within the transformer architecture.\n3.2. Overview\nGiven a text prompt p and a fine-grained control c as in-\nputs, our model aims to generate a video to satisfy both in-\nput conditions. Specifically, the users provide fine-grained\ncontrol c by 1) describing the desired entities in the video,\n2) drawing their trajectories, and 3) providing a reference\nappearance image for each entity. This pipeline helps the\nusers to create videos intuitively. Specifically, the input con-\ntrol signal is given at T timesteps. Assume there are N en-\ntities in the video, we define our control as c = {ct}T\nt=1,\nct \u2208 R2. At a single timestep t, the control is formed by\na sequence of entity control, i.e., the embeddings of the N\nentities ct = {en\nt }N\nn=1, en\nt \u2208 R2. The embeddings encode\nthe desired condition to generate the nth entity at time t.\n3.3. Joint Encoder and Adaptive Cross-Attention\nWe explain the main components of our method in this\nsection, including a joint encoder and an adaptive cross-\nattention module. Compared to existing works (e.g., [59])\nthat use a separate encoding process for each input con-\ndition, we utilize a joint encoder to encode both the text\nprompt p and the fine-grained control c simultaneously\nwithin the same transformer. This design facilitates the in-\nteraction between the text prompt and fine-grained control,\ncaptured into the contextualized embeddings via the joint\nencoder. Specifically, we use simple embedding layers to\ntransform the discrete tokens of prompt p and control c into\nembedding sequences. The two sequences are then concate-\nnated and input to self-attention layers to obtain the contex-\ntualized embeddings. Finally, the embeddings are split back\ninto prompt and control embeddings.\nThe next task is to incorporate the fine-grained control\ninto the base text-to-video transformer. Given a sequence\nof control embeddings, we extend the existing text-to-video\nmodel by inserting an adaptive cross-attention layer in each\ntransformer block to take additional control. See Fig. 2.\nDuring training, we freeze the weight of the self-attention\nand the cross-attention layers of the pre-trained model. We\ntrain the joint encoder and the newly inserted adaptive\ncross-attention layer from scratch to adapt the text-to-video\nmodel to generate videos aligned with text and the new fine-\ngrained condition. By fixing the pre-trained weights, the ca-\npability to generate high-quality videos is preserved, and the\nmodel learns to generate videos satisfying the fine-grained\ncontrol by updating only a small portion (\u2248 23%) of the\nparameters.\nThough we use a transformer-based model, our adap-\ntive cross-attention layers may be extended to other text-\nto-video models, e.g., diffusion-based models, as their ar-\nchitecture usually contains a series of similar blocks of con-\nvolution and attention layers [5, 37]. Likewise, this mod-\nule can be easily extended to take other control signals.\nThis is achieved by first transforming the control into to-\nkens and inputting them to our joint encoder and adaptive\ncross-attention layers. Instead of using multiple condition-\nspecific methods, our adaptive process is a unified approach\nto condition the model on multiple control signals in a sin-\ngle training pipeline.\n3.4. Condition Encoding and Entity Control\nThis subsection discusses our entity-level fine-grained\ncontrol in detail. The entity embeddings en\nt for generat-\ning the nth entity at timestep t are constructed by encoding\nthe context of each entity. First, the description of the entity\nd is given by text, e.g., a cat, and encoded by an embed-\nding layer to transform the discrete tokens into embeddings\n\u03c8entity(d). Second, the location of the entity l is given by\nthe top-left and bottom-right bounding box coordinates of\nthe entity and encoded by an embedding layer as \u03c8coord(l).\nFinally, the reference appearance r of the entity is given\nby a single example image and encoded by a CLIP image\nencoder with an MLP layer to reduce the feature size as\n\u03c8image(r). The embeddings en\nt is the concatenation of de-\nscription, location, and appearance embeddings of the en-\ntity:\nen\nt = Concat(\u03c8entity(dn\nt ), \u03c8coord(ln\nt ), \u03c8image(rn\nt )). (2)\nWe replace the embeddings of en\nt with padding embeddings\nwhen the nth entity is missing at timestep t. Though all\nthe conditions can be thoroughly given at T timesteps, we\n4\nPhenaki\nFACTOR (Ours)\n\"An elephant walking from the left to the right near a campfire, snow mountain in the background.\"\n\"The camera is moving towards a lion running at the beach with moving clouds in the sky.\"\nFigure 3. Trajectory prompts. To highlight that the prompt is not enough to achieve the fine control provided by our method, we augment\nthe text prompt to describe the trajectories specified by the bounding boxes as inputs to the text-to-video model. Phenaki [41] fails to\ngenerate the correct object movement, while FACTOR successfully controls the movement of generated entities with our hand-drawing\ntrajectory input. The blue and green boxes show the location of the object in the first and last frames, respectively.\nassume prompts, description, and appearance are fixed in\nthe whole video, and only the location changes over time to\nresemble the user-friendly setup. In our user interface, the\nlocation is given by simply drawing a bounding box in the\nfirst frame and dragging it to move to the location in the last\nframe.\nIn practice, very few video datasets contain the anno-\ntations of objects\u2019 trajectories and visual examples of the\nobjects\u2019 appearance. To train the model, we utilize an off-\nthe-shelf object detector [24] and tracking algorithm [4] to\nextract N entities in the video clip and their locations at\nT timesteps. For each entity in the video, we need to col-\nlect a reference visual example as our appearance control r.\nHowever, unlike image [28,38], it is difficult to collect mul-\ntiple videos of the same subject as training data. We select\nthe reference visual example of the subject from the same\nvideo. To obtain reference images with more diverse ap-\npearances, we sample them from a longer video clip (\u224840\nframes) outside our training timespan (\u224811 frames). We\nobtain the reference image of the subject by cropping the\nregion using detected bounding boxes. Finally, we use the\ncollected triplet of text prompt p, control c, and video v to\ntrain our model.\n4. Experimental Results\nDataset. We train our model on two datasets: WebVid [2]\nand WebLI [8]. WebVid [2] is a large-scale dataset of short\nvideos and captions collected from stock footage sites. We\nuse the 10M video-and-caption pairs in our training. We-\nbLI [8] is a high-volume image-and-language dataset col-\nlected from the public web. We randomly sample a subset\nof WebLI with \u223c500M image-and-text pairs in our training.\nWe train our base text-to-video model from scratch using\nWebVid and WebLI data. Then, we train our controllable\nvideo generation model using the same data with our aug-\nmented annotations c (Sec. 3.4). We evaluate our model on\nMSR-VTT [53] dataset using zero-shot performance on the\ntest set, which consists of 2,990 examples with 20 prompts\nper example. Following [5], we generate one video per sam-\nple by randomly selecting one prompt.\nEvaluation metrics.\nWe measure the video quality and\nthe alignment between the generated video and different in-\nput conditions including text, trajectory, and reference im-\nages, using the following metrics: 1) Fr\u00b4echet Video Dis-\ntance (FVD) assesses the absolute video quality. 2) CLIP\ntext similarity (CLIP-T) assesses the alignment between in-\nput prompts and generated video frames using CLIP em-\nbeddings. 3) Average Precision (AP) assesses the alignment\nbetween input object trajectories and the generated videos.\nWe apply a pre-trained object detector [24] to compare the\ndetected bounding boxes between the generated frames and\nground truth frames and report the AP score.\n4) CLIP\nvideo similarity (CLIP-V) assesses the similarity between\nthe reference images and object appearance in the generated\nvideo frames. We measure the CLIP embedding distance\nbetween the generated frames and ground truth frames, fol-\nlowing [38].\nImplementation details. We implement the base text-to-\nvideo generative transformer model following Phenaki [41].\nOur implementation of the base model has 1.3B parame-\nters, and our FACTOR model contains 1.6B parameters.\nThe base model is trained for 1M steps, and our model is\ntrained for 500K steps at a batch size of 256 and 128, re-\nspectively. The videos are generated at the resolution of\n192\u00d7320. During training, we use a mixture of 20% im-\nages and 80% videos in each batch.\nCompared methods.\nBecause our goal is to add fine-\ngrained control to an existing text-to-video model, we eval-\nuate whether our method can improve the generative quality\nof the base model for the fine-grained control inputs. We\nvalidate that our approach enhances the base model\u2019s con-\ntrollability without compromising the quality of the output\nvideo. Direct comparison with other works (e.g., [7, 49]),\nwhich inject dense structural control into the video genera-\ntion model, is difficult due to their incompatibility with the\nsparse trajectory inputs our work utilizes.\n5\n\u201cAn elephant picking up a teddy bear on the floor in the room.\u201d\n\u201cA bear pushing the shopping cart on the street.\u201d\n\u201cA cat jumping onto the sofa.\u201d\n\u201cA panda pillow fighting with Santa Claus in a golden wheat field.\u201d\n\u201cA monkey swinging on a swing in a tropical jungle covered in snow.\u201d\n\u201cA motorcyclist high-fiving with a monkey on the shore.\u201d\nFigure 4. Trajectory control. Given the trajectories of the two main entities in the videos as input, FACTOR brings an additional benefit\nto generate complex videos containing subject-object (top, middle) and subject-subject (bottom) interactions between two entities. The\ntrajectory control inputs are omitted for simplicity.\n\u201cMilk poured into a sks vase.\u201d\n\u201cA foot in a sks shoe walking on the road.\u201d\n\u201cA sks teapot and a sks duck toy floating on the ocean.\u201d\n\u201cA sks cat walking on the beach.\u201d\nFigure 5. Appearance control. FACTOR generates videos with the desired object appearances. The videos contain interaction for the\ncustomized subject (left), the composition of two customized subjects (top-right), and reasonable motion of live subjects (bottom-right).\nTo further show that our method provides reasonable\ncontrollability compared to existing text-to-video models,\nwe present comparisons with state-of-the-art models when\nquantitative/qualitative results are available in their papers,\ngiven most of them are not publicly available. We show\nthe results of Imagen-Video [20], MagicVideo [63], Vide-\noLDM [5], Make-A-Video [40], and ModelScope [43]. We\nevaluate two variants of our methods: 1) FACTOR-traj de-\nnoted as our model with text and trajectory control. 2) FAC-\nTOR denoted as our full model with text, trajectory, and\nappearance control. Please see the appendix for details.\n4.1. Qualitative Results of Trajectory Control\nFirst, we assess the model\u2019s ability to control the tra-\njectories of generated entities. We design several prompts,\nincluding the description of trajectories such as the entity\nmoving \u201cfrom the left to the right\u201d and the camera mov-\ning \u201ctowards\u201d the entity.\nIn Fig. 3, we show that given\nthe trajectory prompts as input, Phenaki [41] fails to gen-\nerate the correct movement direction for the elephant, and it\nfails to generate the lion with proper size and movement in\nthe video. In contrast, FACTOR-traj, which takes the hand-\ndrawing trajectories as control inputs, generates the correct\nmoving direction of the entity and the camera.\nIn Fig. 4, we further present the results of trajectory con-\ntrol by drawing the trajectory of two main entities in the\nvideos, such as (elephant, teddy bear) in the top-left video\nand (monkey, motorcyclist) in the bottom-left video. By tak-\ning the trajectories as input, FACTOR-traj generates videos\nwith two entities having the movement following the trajec-\ntories given by the users. Moreover, FACTOR-traj brings an\nadditional benefit of generating complex videos, including\n1) interaction between subject-object such as \u201cpick up\u201d and\n2) interaction between subject-subject such as \u201chigh five\nwith\u201d, even though our method is not specifically training\non the examples of relation/interaction.\n4.2. Qualitative Results of Appearance Control\nNext, we present the results of appearance control,\ni.e., subject customization.\nWe select the reference im-\nages of the subjects from the datasets collected by Dream-\nBooth [38], CustonDiffusion [28] and VideoLDM [5]. For\neach subject, we use a single reference image as the appear-\nance condition. Fig. 5 shows that FACTOR successfully\n6\nImagen Video\nVideo LDM\nFACTOR (Ours)\n\u201cA panda taking a selfie.\u201d\n\u201cSailboat sailing on a sunny day in a mountain lake.\u201d\nMake-A-Video\nFACTOR (Ours)\nFACTOR (Ours)\n\u201cAn astronaut feeding ducks on a sunny afternoon, reflection from the water.\u201d\nFigure 6. Comparison to state-of-the-art. FACTOR generates videos with better interaction between subject and objects. It shows more\nsemantic meaning, e.g., the astronaut stretches out the hand, and the duck gets close to the astronaut, and better movement due to our\nfine-grained control. As the state-of-the-art models are not publicly available, we copy the results from their papers. The blue and green\nboxes show the location of the object in the first and last frames, respectively.\nVideo LDM\nFACTOR (Ours)\n\u201cA sks tea pot on top of a building in New York, drone flight, 4k.\u201d\n\u201cA sks car driving in Manhattan.\u201d\nFigure 7. Comparison of appearance control. VideoLDM [5] uses a backbone finetuned on multiple images while FACTOR is finetuning-\nfree and conditions on a single reference image. FACTOR generates subjects that better align with the reference appearance, showing larger\nmovements and better compatibility with the new environments.\ngenerates the subject aligned with the reference appearance\nimage. FACTOR handles interaction (milk and vase), the\ncomposition of two customized subjects (teapot and duck\ntoy), and the motion of live subjects (cat walking, walking\nin the shoe).\nWe observe several limitations of our approaches. First,\nwe find that the generated live subjects have limited motion\nin FACTOR compared to FACTOR-traj since our current\nframework conditions the subject appearance in the whole\nvideo uses a single share image. This could be further im-\nproved by applying data augmentation on the reference im-\nage in pixel or feature space [26] to generate more motion\ndynamics. On the other hand, FACTOR has sub-optimal\nperformance when the text prompt is not aligned with the\ntrajectory/appearance control. In this case, the model might\nperform inferior to the models that take only a single condi-\ntion as input.\n4.3. Comparison to State-of-the-Art\nWe further compare our video generation results with re-\ncent state-of-the-art text-to-video models. As their models\nare not publicly available, we present the results in their pa-\npers. As shown in Fig. 6, by taking the additional trajectory\ncontrol as input, FACTOR-traj synthesizes videos showing\nbetter interaction between subject and object, e.g., the as-\ntronaut stretches out the hand, the duck gets close to the as-\ntronaut, as well as better movement of the entity such as the\nmovement of the boat. In contrast, VideoLDM [5] generates\nvideos with less semantic meaning \u2013 there is no action of\nfeeding the duck, and Make-A-Video [40] produces videos\nwith less movement. We note that our method\u2019s ability is\nlimited by the resolution/capability of the base model, and\nthe method can be extended to other text-to-video models\nwith better visual quality (e.g. [5,20]) if available.\n7\nTable 1. Quantitative results on MSR-VTT. Our model achieves\na lower FVD compared to baselines. The high AP and CLIP-V\nscores show our model successfully generates videos aligned with\nthe trajectory and appearance control inputs.\nMethods\nFVD (\u2193) CLIP-T (\u2191) AP (\u2191) CLIP-V (\u2191)\nMagicVideo\n1290\n\u2014\n\u2014\n\u2014\nVideoLDM\n\u2014\n0.2929\n\u2014\n\u2014\nMake-A-Video\n\u2014\n0.3049\n\u2014\n\u2014\nModelScope\n550\n0.2930\n\u2014\n\u2014\nPhenaki\n384\n0.2870\n0.0990\n0.6626\nFACTOR-traj (Ours)\n317\n0.2787\n0.2902\n0.6825\nFACTOR (Ours)\n124\n0.2723\n0.3407\n0.7575\nTo evaluate the ability of appearance control, we fur-\nther compare FACTOR to VideoLDM [5], where they in-\nsert trained temporal layers into a DreamBooth backbone\nof Stable Diffusion model to generate video of customized\nsubjects. Their appearance control method requires mul-\ntiple reference images and per-subject finetuning. On the\nother hand, FACTOR 1) does not require fine-tuning, and\nonly an inference pass is needed, 2) requires a single refer-\nence image as input, and 3) achieves subject customization\nfor multiple entities. Fig. 7 shows that our videos present\nbetter reconstruction of the customized subject. Using the\nadditional trajectories as input, we show larger movements\nand better text alignment than VideoLDM, as their back-\nbone is finetuned on static images.\n4.4. Quantitative Results\nResults on MSR-VTT. As shown in Tab. 1, FACTOR\nachieves better FVD scores than other text-to-video models.\nIn addition, FACTOR obtains a CLIP-T score comparable\nto state-of-the-art approaches. Our model has to generate\nvideos that align with text and the fine-grained control sig-\nnal including trajectory and reference appearance. Thus, it\nobtains a slightly worse CLIP-T score than state-of-the-art\ntext-to-video models designed to optimize this metric.\nWe\nfurther\ncompare\nour\nimplementation\nof\nPhenaki, FACTOR-traj, and FACTOR as these models\nare built upon the same video backbone and training data\nwhile taking different levels of control as inputs, i.e., text\nonly, text+trajectory, text+trajectory+appearance. We find\nthat with additional trajectory and reference images as\ninput, the video quality score FVD improves significantly.\nWe further compare the AP score. Although Phenaki is not\ntrained to generate videos with desired object trajectories,\nwe interpret its AP score as a chance performance when the\ntext prompt is sufficient to generate videos that align with\nthe user input trajectory.\nFACTOR-traj\u2019s higher AP score\ncompared to Phenaki proves that the model successfully\ngenerates videos that align with the given trajectory control.\nOn the other hand, FACTOR achieves better CLIP-V scores\nupon FACTOR-traj and Phenaki, showing that the model\nTable 2. User study. We present the percentage of raters that\nprefer each method. FACTOR-traj only uses text and trajectory\nas control. Our model has a similar visual quality as the base\nmodel. The users prefer our results on text, trajectory, and ap-\npearance alignments to the ground truth videos.\nQuality Text Trajectory Appearance\nPhenaki vs. FACTOR-traj\n38/62 27/73\n18/82\n20/80\nPhenaki vs. FACTOR\n37/63 26/74\n8/92\n2/98\nFACTOR-traj vs. FACTOR 40/60 47/53\n37/63\n22/78\ngenerates videos that better resemble the ground truth\nvideos, indicating better alignment with the appearance\ncontrol.\nUser study. To further augment the quantitative evaluation,\nwe conduct a user preference study. We present two videos\ngenerated by different methods using the same prompt and\nthe ground truth video. We ask the raters to select their\npreferred results based on the following criteria: 1) Qual-\nity: which video has the better visual quality, 2) Text align-\nment: which video is better aligned with the text prompt, 3)\nTrajectory alignment: which video has the object trajectory\nthat is better aligned with the ground truth video, and 4)\nAppearance alignment: which video better reproduces the\nobject and scene appearance of the ground truth video. We\nconduct the study with 30 pairs of videos and 5 participants.\nTab. 2 show that FACTOR achieves better visual qual-\nity (>60% preference) and text alignment (>70% prefer-\nence) compared to baselines. By taking the additional tra-\njectory and reference images as input, FACTOR\u2019s results\nhave better trajectory/appearance alignment with the ground\ntruth video, showing the effectiveness of our control mod-\nule. Comparing the two variants of our model, FACTOR\ntaking the reference images as a condition achieves better\nappearance alignment, presenting its capability of subject\ncustomization.\n5. Conclusions\nWe present FACTOR, an approach to fine-grained con-\ntrollable video generation. In addition to the text prompt,\nusers can easily control the video generation process by\nnaming the entities, drawing their trajectories, and provid-\ning their appearance through visual examples. Our key idea\nis to adapt the text-to-video model to generate videos con-\nditioned on fine-grained control by training an adaptive at-\ntention layer, and we model the interaction between inputs\nthrough a joint encoding module. Our method generates\nvideos with controllability on object appearance and tra-\njectory. It brings additional benefits of generating complex\nvideos including interaction.\n8\nAppendix\nWe present the implementation and experiment details\nin the appendix, including the details of datasets, training,\nmodel architectures, and evaluation metrics.\nDatasets. We train the model on the WebVid-10M dataset.\nThe dataset contains videos at 4 fps with a resolution of\n256\u00d7454 and an average length of 40 frames. During train-\ning, the videos are simply resized to our output resolution\nof 96 \u00d7 160 without data augmentation. We randomly sam-\nple 11-frame sub-sequences as training samples. To obtain\nour fine-grained annotations, we apply an off-the-shelf ob-\nject detector [24] to obtain the list of objects in the 40-frame\nvideos. We run a tracking algorithm [4] to extract the tra-\njectory of each object. We sort the trajectories by the num-\nber of pixels they cover and remove the smaller trajectories\nthat heavily overlap with the larger ones. Finally, we select\n12 object trajectories for each video in our annotations. At\ntraining time, the order of the objects is randomly shuffled.\nTo obtain the reference appearance images, we use the\ndetected trajectories to crop the video frames. We randomly\nselect one cropped region from the 40-frame videos for each\nobject as the reference appearance image. In addition to\nvideo, we use image data from the WebLI dataset in our\ntraining, which is used 20% of the time in each batch. Sim-\nilarly, we run an object detector [24] to obtain the bound-\ning box annotations. The image frames are repeated to the\nvideo length as a training sample.\nModel architectures and training details. Our base text-\nto-video model contains a C-ViViT encoder-decoder and a\nbidirectional transformer. The C-ViViT model is trained to\ncompress the video with dimensions 11 \u00d7 96 \u00d7 160 \u00d7 3\ninto discrete tokens with dimensions of 6 \u00d7 12 \u00d7 20. The\nmodel has a hidden size of 512, an embedding dimension\nof 32, an MLP size of 2048, 8 heads, and a codebook size\nof 8192. It contains 4 layers of spatial transformers and 4\nlayers of temporal transformers. The model has 100M pa-\nrameters. The model is trained using the AdamW optimizer\nwith a learning rate of 1 \u00d7 10\u22124, a batch size of 128, and\n1M iterations.\nThe bidirectional transformer is trained to produce\nvideos at 11 \u00d7 96 \u00d7 160 \u00d7 3. The transformer takes a se-\nquence length of 1920. It contains an embedding dimension\nof 1728, an MLP size of 4096, 36 heads, and 24 layers. The\nmodel has 1.3B parameters. The model is trained using the\nAdamW optimizer with a learning rate of 4.5 \u00d7 10\u22125, a\nbatch size of 512, and 1M iterations. The videos are gener-\nated at a unit of 11 frames and can be extended to an arbi-\ntrary length by applying a sliding window approach. Specif-\nically, we first generate an 11-frame clip and use the last 5\nframes as a condition for the model to generate another 6\nframes. This process can be applied iteratively. We apply\na super-resolution model to increase the video resolution to\n192 \u00d7 320 \u00d7 3. In our method, the C-ViViT model and the\nsuper-resolution model are kept as the original models for\ntext-to-video generation, and only the bidirectional trans-\nformer model is trained to take the fine-grained control in-\nputs.\nAs the video is compressed into a size of 6 \u00d7 12 \u00d7 20,\nthe control signals are given at 6 timesteps to generate 11\nframes. The control signal at each timestep contains prompt\nembeddings and control embeddings.\nThe length of the\nprompt embedding is 64, and the length of the control em-\nbedding is 220. In FACTOR model, the control embedding\nis formed by the entity control of four objects, including\nthe embedding of their description, location, and appear-\nance with lengths of 1, 4, and 50, respectively.\nTo obtain the appearance embedding, the reference im-\nages are resized to 224\u00d7224 and input to a ViT-B/32 model\nto extract the appearance features. We use the grid features,\nwhich have a length of 50 and a size of 768. The appearance\nembeddings are then input to an MLP layer to reduce its size\nto 512. We use a learned embedding layer to obtain the en-\ntity description and location sequences with a feature size\nof 512. The three contexts are then concatenated to form\nan embedding sequence of length 220 and input to a joint\ntransformer encoder, which contains a self-attention layer\nto obtain the contextualized embeddings. The embeddings\nare then split into prompt embeddings of length 64 and con-\ntrol embeddings of size 220 and input to the cross-attention\nlayer and adaptive cross-attention layers separately. In total,\nthe FACTOR model contains 1.6B parameters. At training\ntime, the embedding layer, the joint encoder, and the adap-\ntive cross-attention layers are learned while other layers are\nfixed. The model is trained using the AdamW optimizer\nwith a learning rate of 4.5 \u00d7 10\u22125, a batch size of 128,\nand 500k iterations. To apply classifier-free guidance, at\ntraining time, all the input conditions are dropped simulta-\nneously 10% of the time. At inference time, we use a clas-\nsifier guidance scale of 12, an inference temperature of 4,\nand 48 sampling steps.\nEvaluation metrics. We present the details of evaluation\nmetrics. 1) Fr\u00b4echet Video Distance (FVD) assesses the ab-\nsolute video quality by measuring whether the distribution\nof generated videos is close to that of real videos in the\nfeature space. We use the I3D model trained on Kinetics-\n400 for video features. 2) CLIP text similarity (CLIP-T) as-\nsesses the alignment between input prompts and generated\nvideo frames using CLIP embeddings. We use the ViT-B/32\nmodel to compute the CLIP-T score. The score is reported\nas the average of all frames. 3) Average Precision (AP) as-\nsesses the alignment between input object trajectories and\nthe generated videos. We apply a pre-trained object detec-\ntor [24] to compare the detected bounding boxes between\nthe generated frames and ground truth frames and report\nthe AP score. The AP score is computed at the 6th frame.\n4) CLIP video similarity (CLIP-V) assesses the similarity\nbetween the reference images and object appearance in the\n9\ngenerated video frames. We measure the CLIP embedding\ndistance between the generated frames and ground truth\nframes, following [38]. Specifically, we use the bounding\nbox coordinates in the trajectory of an object to crop the\nground truth video frames. After obtaining a cropped im-\nage per frame containing that object, we randomly select\none as the reference appearance image. Since the contents\nin the ground truth videos are used as references, the gener-\nated video should mostly reconstruct the ground truth video.\nWe measure the similarity of the generated and ground truth\nvideo frames and report the average. We use the ViT-B/32\nmodel to compute the CLIP-V score.\nReferences\n[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,\nand Xi Yin. Spatext: Spatio-textual representation for con-\ntrollable image generation. In CVPR, 2023. 3\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, 2021. 5\n[3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel.\nMultidiffusion: Fusing diffusion paths for controlled image\ngeneration. arXiv preprint arXiv:2302.08113, 2023. 3\n[4] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and\nBen Upcroft. Simple online and realtime tracking. In ICIP,\n2016. 5, 9\n[5] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 1, 2, 3, 4, 5, 6, 7,\n8\n[6] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stable-\nvideo: Text-driven consistency-aware diffusion video edit-\ning. arXiv preprint arXiv:2308.09592, 2023. 3\n[7] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li,\nXin Xia, Xuefeng Xiao, and Liang Lin.\nControl-a-video:\nControllable text-to-video generation with diffusion models.\narXiv preprint arXiv:2305.13840, 2023. 1, 3, 5\n[8] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,\nPiotr Padlewski, Daniel Salz, Sebastian Goodman, Adam\nGrycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari,\nGaurav Mishra, Linting Xue, Ashish Thapliyal, James Brad-\nbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia,\nBurcu Karagol Ayan, Carlos Riquelme, Andreas Steiner,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu\nSoricut. Pali: A jointly-scaled multilingual language-image\nmodel. In ICLR, 2023. 5\n[9] Yutao Chen, Xingning Dong, Tian Gan, Chunluan Zhou,\nMing Yang, and Qingpei Guo. Eve: Efficient zero-shot text-\nbased video editing with depth map guidance and temporal\nconsistency constraints. arXiv preprint arXiv:2308.10648,\n2023. 3\n[10] Ernie Chu, Tzuhsuan Huang, Shuo-Yen Lin, and Jun-Cheng\nChen. Medm: Mediating image diffusion models for video-\nto-video translation with temporal correspondence guidance.\narXiv preprint arXiv:2308.10079, 2023. 3\n[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei A. Efros, and\nAleksander Holynski. Diffusion self-guidance for control-\nlable image generation. In NeurIPS, 2023. 3\n[12] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 3\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H. Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 3\n[14] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H. Bermano,\nGal Chechik, and Daniel Cohen-Or.\nEncoder-based do-\nmain tuning for fast personalization of text-to-image models.\narXiv preprint arXiv:2302.12228, 2023. 3\n[15] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation:\nA noise prior for video diffusion models. In ICCV, 2023. 1,\n2\n[16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arxiv:2307.10373, 2023. 1, 3\n[17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 1, 3\n[18] Cusuh Ham, James Hays, Jingwan Lu, Krishna Kumar\nSingh, Zhifei Zhang, and Tobias Hinz.\nModulating pre-\ntrained diffusion models for multimodal image synthesis. In\nSIGGRAPH, 2023. 3\n[19] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story:\nStorytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023. 3\n[20] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\nmans. Imagen video: High definition video generation with\ndiffusion models. arXiv preprint arXiv:2210.02303, 2022. 1,\n2, 6, 7\n[21] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 2\n[22] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and\nJie Tang. Cogvideo: Large-scale pretraining for text-to-video\ngeneration via transformers. In ICLR, 2023. 1, 2\n[23] Zhihao Hu and Dong Xu. Videocontrolnet: A motion-guided\nvideo-to-video translation framework by using diffusion\nmodel with controlnet.\narXiv preprint arXiv:2307.14073,\n2023. 3\n[24] Jonathan Huang, Vivek Rathod, Chen Sun, Menglong Zhu,\nAnoop Korattikara, Alireza Fathi, Ian Fischer, Zbigniew Wo-\njna, Yang Song, Sergio Guadarrama, and Kevin Murphy.\n10\nSpeed/accuracy trade-offs for modern convolutional object\ndetectors. In CVPR, 2017. 5, 9\n[25] Nisha Huang, Yuxin Zhang, and Weiming Dong. Style-a-\nvideo: Agile diffusion for arbitrary text-based video style\ntransfer. arXiv preprint arXiv:2305.05464, 2023. 3\n[26] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-\nto-image diffusion models are zero-shot video generators. In\nICCV, 2023. 1, 2, 3, 7\n[27] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and\nJun-Yan Zhu. Dense text-to-image generation with attention\nmodulation. In ICCV, 2023. 3\n[28] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In CVPR, 2023. 3, 5, 6\n[29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023. 3\n[30] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu,\nand Jiashi Feng. Magicedit: High-fidelity and temporally\ncoherent video editing.\narXiv preprint arXiv:2308.14749,\n2023. 3\n[31] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control.\narXiv preprint arXiv:2303.04761, 2023. 1, 3\n[32] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 3\n[33] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian\nZhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-\nadapter:\nLearning adapters to dig out more controllable\nability for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 3\n[34] Hao Ouyang, Qiuyu Wang, Yuxi Xiao, Qingyan Bai, Jun-\ntao Zhang, Kecheng Zheng, Xiaowei Zhou, Qifeng Chen,\nand Yujun Shen.\nCodef: Content deformation fields for\ntemporally consistent video processing.\narXiv preprint\narXiv:2308.07926, 2023. 3\n[35] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 1, 3\n[36] Bosheng Qin, Juncheng Li, Siliang Tang, Tat-Seng Chua,\nand Yueting Zhuang.\nInstructvid2vid: Controllable video\nediting with natural language instructions.\narXiv preprint\narXiv:2305.12328, 2023. 3\n[37] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 4\n[38] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, 2023. 1, 3, 5, 6, 10\n[39] Chaehun Shin, Heeseung Kim, Che Hyun Lee, Sang gil Lee,\nand Sungroh Yoon. Edit-a-video: Single video editing with\nobject-aware consistency. arXiv preprint arXiv:2303.07945,\n2023. 3\n[40] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata. In ICLR, 2023. 1, 2, 6, 7\n[41] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. In ICLR, 2023. 1, 2, 3, 5, 6\n[42] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye,\nYu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long\nvideo generation via temporal co-denoising. arXiv preprint\narXiv:2305.18264, 2023. 3\n[43] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 1,\n2, 6\n[44] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023. 1, 3\n[45] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Ji-\nuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Videocomposer: Compositional video synthesis\nwith motion controllability. In NeurIPS, 2023. 3\n[46] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. In ICCV, 2023. 3\n[47] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 1, 2\n[48] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation. In ECCV, 2022. 1,\n2\n[49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of im-\nage diffusion models for text-to-video generation. In ICCV,\n2023. 1, 2, 3, 5\n[50] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wen-\ntian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff:\nText-to-image synthesis with training-free box-constrained\ndiffusion. In ICCV, 2023. 3\n[51] Jinbo Xing, Menghan Xia, Yuxin Liu, Yuechen Zhang, Yong\nZhang, Yingqing He, Hanyuan Liu, Haoxin Chen, Xiaodong\nCun, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Make-\nyour-video: Customized video generation using textual and\nstructural guidance. arXiv preprint arXiv:2306.00943, 2023.\n3\n[52] Zhen Xing, Qi Dai, Han Hu, Zuxuan Wu, and Yu-Gang\nJiang. Simda: Simple diffusion adapter for efficient video\ngeneration. arXiv preprint arXiv:2308.09710, 2023. 3\n11\n[53] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large\nvideo description dataset for bridging video and language. In\nCVPR, 2016. 2, 5\n[54] Han Xue, Zhiwu Huang, Qianru Sun, Li Song, and Wenjun\nZhang. Freestyle layout-to-image synthesis. In CVPR, 2023.\n3\n[55] Shuai Yang, Yifan Zhou, Ziwei Liu, , and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. In SIGGRAPH Asia, 2023. 3\n[56] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin\nLin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael\nZeng, and Lijuan Wang. Reco: Region-controlled text-to-\nimage generation. In CVPR, 2023. 3\n[57] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and\nJian Zhang. Freedom: Training-free energy-guided condi-\ntional diffusion model. In ICCV, 2023. 3\n[58] Yu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collo-\nmosse, Jason Kuen, and M. Patel, Vishal. Scenecomposer:\nAny-level semantic image synthesis. In CVPR, 2023. 3\n[59] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, 2023. 3, 4\n[60] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng\nZhang, Wangmeng Zuo, and Qi Tian.\nControlvideo:\nTraining-free controllable text-to-video generation.\narXiv\npreprint arXiv:2305.13077, 2023. 1, 2, 3\n[61] Shihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin\nBao, Shaozhe Hao, Lu Yuan, and Kwan-Yee K Wong.\nUni-controlnet: All-in-one control to text-to-image diffusion\nmodels. In NeurIPS, 2023. 3\n[62] Yuyang Zhao, Enze Xie, Lanqing Hong, Zhenguo Li,\nand Gim Hee Lee.\nMake-a-protagonist:\nGeneric video\nediting with an ensemble of experts.\narXiv preprint\narXiv:2305.08850, 2023. 1, 3\n[63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 1, 2, 6\n12\n"
  },
  {
    "title": "ReconFusion: 3D Reconstruction with Diffusion Priors",
    "link": "https://arxiv.org/pdf/2312.02981.pdf",
    "upvote": "8",
    "text": "ReconFusion: 3D Reconstruction with Diffusion Priors\nRundi Wu1,2*\nBen Mildenhall1*\nPhilipp Henzler1\nKeunhong Park1\nRuiqi Gao3\nDaniel Watson3\nPratul P. Srinivasan1\nDor Verbin1\nJonathan T. Barron1\nBen Poole3\nAleksander Ho\u0142y\u00b4nski1*\n1Google Research\n2Columbia University\n3Google DeepMind\n* denotes equal contribution\n3\n6\n9 18 27 54 81\nNumber of Views\n10\n15\n20\n25\n30\nPSNR\nReconFusion\nZip-NeRF\nReconFusion\nZip-NeRF [2]\n3 views\n9 views\n54 views\nFigure 1.\nMethods for reconstructing a 3D scene from images, such as Neural Radiance Fields (NeRF), often exhibit artifacts\nwhen trained with few input views.\nReconFusion uses a diffusion model trained for novel view synthesis to regularize NeRF op-\ntimization.\nWhen the reconstruction problem is severely underconstrained (3 and 9 views), this prior can greatly improve robust-\nness and often prevent catastrophic failures. Even in the case of significantly more observations (54 views), ReconFusion improves\nquality and helps reduce \u201cfloater\u201d artifacts common to volumetric reconstruction methods like NeRF. We encourage the reader to\nview the results at reconfusion.github.io to see the improvement our method can provide in few-view captures of real world scenes.\nAbstract\n3D reconstruction methods such as Neural Radiance\nFields (NeRFs) excel at rendering photorealistic novel\nviews of complex scenes.\nHowever, recovering a high-\nquality NeRF typically requires tens to hundreds of input\nimages, resulting in a time-consuming capture process. We\npresent ReconFusion to reconstruct real-world scenes us-\ning only a few photos. Our approach leverages a diffusion\nprior for novel view synthesis, trained on synthetic and mul-\ntiview datasets, which regularizes a NeRF-based 3D recon-\nstruction pipeline at novel camera poses beyond those cap-\ntured by the set of input images. Our method synthesizes\nrealistic geometry and texture in underconstrained regions\nwhile preserving the appearance of observed regions. We\nperform an extensive evaluation across various real-world\ndatasets, including forward-facing and 360-degree scenes,\ndemonstrating significant performance improvements over\nprevious few-view NeRF reconstruction approaches. Please\nsee our project page at reconfusion.github.io.\n1. Introduction\nAdvances in 3D reconstruction have enabled the transfor-\nmation of images of real-world scenes into 3D models\nwhich produce photorealistic renderings from novel view-\npoints [26, 32]. Methods like NeRF [32] optimize a 3D\nrepresentation whose renderings match observed input im-\nages at given camera poses.\nHowever, renderings from\nunder-observed views are artifact-prone, particularly in less\ndensely captured areas. As such, recovering a high-quality\nNeRF requires exhaustive scene capture, where each region\nis photographed from multiple angles multiple times.\nNeRF\u2019s dense capture requirement poses a major chal-\nlenge, necessitating tens to hundreds of images for even\nsimple objects to ensure a clean reconstruction (Fig. 1).\nMany methods aim to reduce the reliance on dense captures\nby developing heuristic low-level regularizers based on re-\nconstructed depth [11, 17, 40], visibility [27, 48], appear-\nance [35, 60], or image-space frequencies [61]. However,\n1\narXiv:2312.02981v1  [cs.CV]  5 Dec 2023\neven the most effective methods show considerable degra-\ndation at novel viewpoints compared to denser captures.\nWith the recent success of diffusion models for im-\nage generation [21, 50], researchers have applied diffusion\nmodels to the task of novel view synthesis \u2014 modeling the\ndistribution of unseen views given observations from known\nviews [16, 29, 59]. While these models excel at generating\nrealistic images from novel view points, they do not pro-\nduce a single consistent 3D shape from a sparse set of input\nviews. Existing work produces 3D models that are either\ntrained per category [6, 16, 52, 62, 67], or are limited to sin-\ngle image inputs containing an object [29, 30, 47], prevent-\ning their use as a general prior for 3D scene reconstruction.\nOur proposed method uses 2D image priors over novel\nviews to enhance 3D reconstruction. We derive this prior\nfrom a diffusion model trained for novel view synthe-\nsis. Given multiple posed images of a scene, this model\nestimates the scene\u2019s appearance from novel viewpoints.\nAs posed multiview data is limited (compared to massive\nsingle image datasets), we finetune our diffusion model\nfrom a pretrained latent diffusion model [42] on a mix-\nture of real world and synthetic multiview image datasets:\nRealEstate10K [66], CO3D [38], MVImgNet [64], and Ob-\njaverse [10]. Once trained, this model is used to regularize a\ntypical NeRF reconstruction pipeline by using an approach\nsimilar to score distillation sampling (SDS) [36].\nOur approach outperforms existing baselines on sev-\neral datasets of both forward-facing and unbounded 360\u25e6\nscenes. Furthermore, we show that our diffusion prior is\nan effective drop-in regularizer for NeRFs across a range of\ncapture settings. In few-view scenarios with limited scene\nobservations, it provides a strong prior for plausible geom-\netry and appearance reconstruction. In denser capture set-\ntings, it helps reduce distracting \u201cfog\u201d and \u201cfloater\u201d artifacts\nwhile preserving the appearance of well-sampled regions.\nMany aspects of our pipeline have been explored in prior\nwork. We contribute an end-to-end system that markedly\nimproves 3D reconstruction quality, uniquely combining\nthe challenges of developing a multiview-conditioned im-\nage diffusion model and integrating it into the NeRF opti-\nmization process, minimizing the need for rigorous capture.\n2. Related Work\nFew-view NeRF Minimizing NeRF\u2019s need for dense cap-\nture is crucial for democratizing 3D capture, and has moti-\nvated many works [11, 17, 22, 27, 35, 40, 46, 48, 49, 58,\n60, 61]. Most existing methods focus on regularizing the\ngeometry of the scene. DS-NeRF [11] utilizes sparse depth\noutputs from Structure-from-Motion (SfM) as supervision.\nDDP-NeRF [40] further uses a CNN to obtain dense depth\nsupervision from sparse inputs. SimpleNeRF [49] regular-\nizes appearance and geometry by training two additional\nmodels which respectively reduce positional encoding fre-\nquencies and remove view-dependent components. Simi-\nlarly, FreeNeRF [61] demonstrates that simply regularizing\nthe frequency range of NeRF\u2019s positional encoding features\nimproves quality in few-view scenarios. RegNeRF [35] in-\ntroduces a depth smoothness loss and a pre-trained normal-\nizing flow color model to regularize the geometry and ap-\npearance of novel views. DiffusioNeRF [60] trains a diffu-\nsion model to regularize the distribution of RGB-D patches\nfrom perturbed viewpoints. GANeRF [41] trains a genera-\ntor network to improve NeRF renderings and an image dis-\ncriminator network to provide feedback that can be used to\nimprove the reconstruction in a multiview-consistent man-\nner. While all these methods regularize ambiguous geom-\netry and appearance during NeRF optimization, they often\nfail on larger scenes when the view sparsity is extreme.\nRegression models for view synthesis While NeRFs are\noptimized per-scene, other methods train feed-forward neu-\nral networks for generalized novel view synthesis. These\nnetworks leverage large collections of posed multiview data\nacross many scenes [9, 12, 20, 43, 53, 55, 57, 63, 66]. Most\nmethods lift the input images into a 3D representation, like\nusing a plane sweep volume, and predict novel views in a\nfeed-forward manner. They work well near the input views,\nbut extrapolate poorly to ambiguous views, where the dis-\ntribution of possible renderings becomes multi-modal.\nGenerative models for view synthesis Extrapolating be-\nyond observed inputs for view synthesis requires generat-\ning unknown parts of the scene.\nEarlier works address-\ning this problem primarily leverage Generative Adversarial\nNetworks (GANs) [4, 5, 13\u201315, 19, 33, 34, 45, 68]. More\nrecent works use diffusion models, following their immense\nsuccess on image generation [6, 16, 17, 24, 25, 29, 30, 39,\n47, 52, 54, 62]. 3DiM [59] trains a pose-conditioned image-\nto-image diffusion model on synthetic ShapeNet data [7].\nGeNVS [6] and SparseFusion [17] train on real-world mul-\ntiview data [38] and further incorporate 3D geometry priors\nby conditioning on rendered features [51, 63]. While they\nshow promising results for novel view synthesis, their mod-\nels are category-specific and do not generalize to arbitrary\nscenes. Recently, Zero-1-to-3 [29] fine-tunes a large-scale\npretrained diffusion model [42] on the synthetic Objaverse\ndataset [10] and achieves strong zero-shot generalization\non real images. However, it only supports images of ob-\njects with clean backgrounds (versus full real scenes) and\nis limited to single-image inputs.\nZeroNVS [44] further\nfine-tunes Zero-1-to-3 to enable single-image reconstruc-\ntion of general scenes. Our approach is similar, but utilizes\na PixelNeRF-based approach [63] for conditioning (simi-\nlar to GeNVS [6]) to allow for any number of input images\nand provide more precise pose conditioning, and fine-tunes\na pretrained image diffusion model on real-world multiview\ndatasets [38, 64, 66], which combined facilitates few-view\n2\nNeRF\nInput Images\n<latexit sha1_base64=\"Krc6RV3j6P+MhADR7Yfi\nTtMIPZ8=\">ADU3icdVNb9NAEN0mfBRDaQo3uCy4kThFcVJKe7PgwoFDkUhbKbaiyWaTrLIf1u6VbRa\niV/DFf4MB34LF9ZuKiWlGcnSzJt545n8bjgzNhu989Oo/ng4aPHu0+ip8/2nu+3Dl6cG1VqQgdEcaUvx2A\noZ5IOLOcXhaghzejFefKryF1dUG6bkN7saC5gJtmUEbABGrVeZQLsnAB3X/yo9rVwmhIl/agVdzunp\n0cnSQ/7ySdbm0xWtnZ6KDxJpsoUgoqLeFgzDpFjZ3oC0jnPoKw0tgCxgRofBlSCoyV29hMftgEzwVOnw\nSItrdJ3hQBizFONQWY1p7uYq8N4cAUko3i7uwazDM02wYpqleLmzqR2epI7JovSUkluBp2WHFuFK0XxhA\nW9LF8GB4hmYVdM5qCB2KB7FAVRFxTCh7EhjJr4kSAuTEZaYsqGaiUIb6YT8f1vorPcldnLg6KDmrVemz\nsU97+K+D7a1X+qHvXwTSUNXH6jbOWk1hWtHGdKAWczWQchnE8ZP8xEaudxEvcO6zgj+p5kfz3p25G/FUD\nZedg/HNXt5eDtznmvkx3n89itOPq/PaRa/RW/QOJegDStFndIYGiKDv6Af6iX41fjf+NsNfclPa2FlxXq\nINa+79AwRfEik=</latexit>Lrecon\nInput\nViews\n\u2026\n<latexit sha1_base64=\"m4AGWxEZC1jNSIGMvPg+kL4eWI4=\">ADVHicdVJNb9NAEN0mFIqBksKxl\nwU3EqcoTgptbxZcOHAoEmkr2VY02ayTVfbD2l23ilY+8Gu4wp9B4r9wYO2mUlKakSzNezNvPDM7k4IzY/v9Pzut9qPdx0/2ngbPnr/Yf9k5eHVhVKkJHRHFlb6agKGcSTqyzHJ6VWgKYsLp5WTxqY5fXlNtmJLf7LK\ngmYCZDkjYD017hymAuycAHdfqnHja+EMiILTatwJ+72zs+PTaID/d6Jev7EQrex8fNB6k04VKQWVlnAwJon6hc0caMuILxikpaEFkAXMaOJdCYKazDVTVLjrmSnOlfaftLh1xUOhDFLMfGZdZ/mfqwmH4wRkITyj\nb+7GzBLX2yTrKVWKW7udWrz08wxWZSWSnLbaF5ybBWuV4qnTFNi+dI7QDTzs2IyBw3E+sUHgd/qgoJ/GetxkEp6Q5QIKcuNWVBNROFMrRKhlnSPIDS08yFkWtAY1uPTN2LhxULhxW3rbWi6tkG0ysa9ael2TV\nx34boBxqlSwNlMNsDec74USpiOw+jcHDU4JToB4LD9WDVDaq7BSg79/P7o7q7HLzduRj0og+91+Pw/j6rz20CF6i96hCJ2gGH1G52iECPqOfqCf6Ffrd+tvu93evU1t7aw0r9Gtf/AWRdEp4=</latexit>Lsample\n(a) NeRF Training\n(b) Multi-View Conditioning\nPixelNeRF\nCLIP\nrender\nSampled\nNovel View\n<latexit sha1_base64=\"Krc6RV3j6P+MhADR7Yfi\nTtMIPZ8=\">ADU3icdVNb9NAEN0mfBRDaQo3uCy4kThFcVJKe7PgwoFDkUhbKbaiyWaTrLIf1u6VbRa\niV/DFf4MB34LF9ZuKiWlGcnSzJt545n8bjgzNhu989Oo/ng4aPHu0+ip8/2nu+3Dl6cG1VqQgdEcaUvx2A\noZ5IOLOcXhaghzejFefKryF1dUG6bkN7saC5gJtmUEbABGrVeZQLsnAB3X/yo9rVwmhIl/agVdzunp\n0cnSQ/7ySdbm0xWtnZ6KDxJpsoUgoqLeFgzDpFjZ3oC0jnPoKw0tgCxgRofBlSCoyV29hMftgEzwVOnw\nSItrdJ3hQBizFONQWY1p7uYq8N4cAUko3i7uwazDM02wYpqleLmzqR2epI7JovSUkluBp2WHFuFK0XxhA\nW9LF8GB4hmYVdM5qCB2KB7FAVRFxTCh7EhjJr4kSAuTEZaYsqGaiUIb6YT8f1vorPcldnLg6KDmrVemz\nsU97+K+D7a1X+qHvXwTSUNXH6jbOWk1hWtHGdKAWczWQchnE8ZP8xEaudxEvcO6zgj+p5kfz3p25G/FUD\nZedg/HNXt5eDtznmvkx3n89itOPq/PaRa/RW/QOJegDStFndIYGiKDv6Af6iX41fjf+NsNfclPa2FlxXq\nINa+79AwRfEik=</latexit>Lrecon\nDenoising\nU-Net\nnoise\n(c) Diffusion Prior\nrender\nEncoder\nDecoder\nFeatures\n\u2207stop gradient\nmultistep\nFigure 2. (a) We optimize a NeRF to minimize a reconstruction loss Lrecon between renderings and a limited set of input images, alongside\na sample loss Lsample comparing renderings from random poses and with predictions by a diffusion model for those poses. (b) To generate\nthe sample image, we use a PixelNeRF-style model [63] to fuse input image data, rendering a feature map for the sample view. (c) This\nfeature map, merged with the noisy latent (computed by adding some amount of noise to the current NeRF rendering from that pose),\nis provided to a diffusion model. This model additionally uses CLIP embeddings of the input images via cross-attention, generating a\ndecoded output sample. This sample is used to apply an image-space loss to the corresponding NeRF rendering.\nreconstruction on arbitrary scenes.\nLifting 2D diffusion models for 3D generation\nGiven\nthe limited amount of 3D data available for training, recent\nworks have attempted to leverage 2D diffusion models to\ngenerate 3D assets from a text prompt [28, 36, 56] or an\ninput image [29, 47]. DreamFusion [36] proposed score\ndistillation sampling (SDS), where a 2D diffusion model\nacts as a critic to supervise the optimization of a 3D model.\nSparseFusion [67] proposes multistep sampling where an\nimage is sampled given a noisy encoding of the current ren-\ndering as a target for 3D reconstruction. We experiment\nwith both approaches in our reconstruction pipeline.\n3. ReconFusion\nReconFusion consists of a diffusion model trained for novel\nview synthesis and a 3D reconstruction procedure to make\nuse of this diffusion model. We describe the details of the\ndiffusion model training in Sec. 3.1 and how we use the\ndiffusion model as a prior for 3D reconstruction in Sec. 3.2.\n3.1. Diffusion Model for Novel View Synthesis\nGiven a set of posed images, we seek to learn a prior that\ncan generate plausible novel views. If we can learn what\nthe back or side of an object looks like given images of the\nfront, we can use this to guide a 3D reconstruction process\nto recover a plausible 3D scene. Formally, we are given a\nset of input images xobs = {xi}N\ni=1, corresponding camera\nparameters \u03c0obs ={\u03c0i}N\ni=1, and a target camera for a novel\nview \u03c0, and want to learn the conditional distribution over\nthe image x at the novel view: p\n\u0000x|xobs, \u03c0obs, \u03c0\n\u0001\n.\nDiffusion Models\nWe build on latent diffusion models\n(LDMs) [42] for their ability to efficiently model high reso-\nlution images. LDMs encode input images to a latent repre-\nsentation using a pretrained variational auto-encoder (VAE)\nE. Diffusion is performed on these latents, where a denois-\ning U-Net \u03f5\u03b8 maps noisy latents back to clean latents. Dur-\ning inference, this U-Net is used to iteratively denoise pure\nGaussian noise to a clean latent. To recover an image, the\nlatents are passed through a VAE decoder D.\nConditioning\nSimilar to Zero-1-to-3 [29], we start from\nan LDM trained for text-to-image generation, and addition-\nally condition on input images and poses.\nConverting a\ntext-to-image model into a posed images-to-image model\nrequires augmenting the U-Net architecture with additional\nconditioning pathways. To modify the pretrained architec-\nture for novel view synthesis from multiple posed images,\nwe inject two new conditioning signals into the U-Net (see\nFig. 2(b)). For high-level semantic information about the in-\nputs, we use the CLIP [37] embedding of each input image\n(denoted eobs) and feed this sequence of feature vectors into\nthe U-Net via cross-attention. For relative camera pose and\ngeometric information, we use a PixelNeRF [63] model R\u03d5\nto render a feature map f with the same spatial resolution\nas the latents from the target viewpoint \u03c0:\nf = R\u03d5\n\u0000xobs, \u03c0obs, \u03c0\n\u0001\n.\n(1)\nThis rendered feature map f is a spatially aligned condi-\ntioning signal which implicitly encodes the relative camera\ntransform. We concatenate f with the noisy latent along\nthe channel dimension, and feed it into the denoising U-\nNet \u03f5\u03b8. This feature map conditioning strategy is similar to\n3\nthe one used in GeNVS [6], SparseFusion [67], and other\nrecent works to better provide an accurate representation\nof the novel camera pose, as compared to attending over\na direct embedding of the camera extrinsics and intrinsics\nthemselves (an ablation study can be found in Sec. 4.3).\nTraining We freeze the weights of the pretrained encoder\nand decoder, initialize the U-Net parameters \u03b8 from pre-\ntrained weights, and optimize the modified architecture for\nview synthesis using the simplified diffusion loss [21]:\nLDiff(\u03b8, \u03d5) = Ex,\u03c0,xobs,\u03c0obs,\u03f5,t\n\r\r\r\u03f5 \u2212 \u03f5\u03b8(zt, t, eobs, f)\n\r\r\r2 ,\n(2)\nwhere t \u2208 {1, . . . , T} is the diffusion timestep, \u03f5 \u223c\nN(0, I), zt = \u03b1tE(x) + \u03c3t\u03f5 is the noisy latent at that\ntimestep, eobs are the CLIP image embeddings for the input\nimages xobs, and f is the rendered feature map from Pixel-\nNeRF R\u03d5. In addition to the loss in Eqn. 2, we optimize the\nPixelNeRF parameters \u03d5 with a photometric loss:\nLPixelNeRF(\u03d5) = Exobs,\u03c0obs,x,\u03c0\u2225c \u2212 x\u2193\u22252 ,\n(3)\nwhere c is an output of the PixelNeRF model (at the same\nresolution as the feature map f) and x\u2193 is the target image\ndownsampled to the spatial resolution of zt and f. This loss\nencourages PixelNeRF to reconstruct the RGB target image,\nwhich helps to avoid bad local minima where the diffusion\nmodel is unable to leverage the PixelNeRF inputs.\nDue to the use of cross-attention and the design of Pix-\nelNeRF, both conditioning branches can take an arbitrary\nnumber and permutation of input images. This enables the\nmodel to be trained and evaluated with a variable number\nof observed posed images. While there are many ways to\ncondition on images and poses, we found our design more\neffective than alternatives (see the ablation in Sec. 4.3).\n3.2. 3D Reconstruction with Diffusion Priors\nThe trained diffusion model produces plausible single im-\nages for novel camera poses, but generated images are of-\nten inconsistent for different poses or random seeds. State-\nof-the-art NeRF methods produce 3D consistent 3D mod-\nels, but often exhibit volumetric \u201cfloater\u201d artifacts and in-\naccurate (or totally unrecognizable) geometry from novel\nviews. To enable 3D reconstruction from a smaller number\nof posed inputs, we augment the state-of-the-art 3D recon-\nstruction pipeline from Zip-NeRF [2] with a prior from our\ndiffusion model trained for novel view synthesis.\nReconstruction loss NeRF-based methods optimize a ran-\ndomly initialized 3D model to match a set of posed im-\nages.\nThe NeRF parameters \u03c8 are optimized by mini-\nmizing the reconstruction error between a rendered image\nx = x(\u03c8, \u03c0obs) and an observed image xobs at pose \u03c0obs:\nLRecon(\u03c8) = Exobs,\u03c0obs\n\u0002\n\u2113(x(\u03c8, \u03c0obs), xobs)\n\u0003\n,\n(4)\nwhere \u2113 is an image similarity loss function such as the \u21132-\nnorm or a robust loss. This loss is only evaluated where\nwe have observations, and thus the training procedure never\nviews the 3D model from novel views.\nDiffusion loss\nIn addition, we seek to optimize the 3D\nmodel to produce realistic rendering at novel views unob-\nserved in the inputs. To do so, we use a diffusion model\nwhich provides a prior on the distribution of plausible im-\nages of the scene. We distill this prior into a consistent 3D\nmodel by using a regularization loss derived from the dif-\nfusion model outputs. We experimented with several losses\nand discuss our multistep sampling approach below.\nAt each iteration, we sample a random view and generate\nan image from the diffusion model to produce a target image\n(see Fig. 2(a)). We can control how grounded the target im-\nage is to the current rendered image by starting the sampling\nprocess from an intermediate noise level. Specifically, we\nrender an image x(\u03c8, \u03c0) from a sampled novel viewpoint \u03c0,\nand encode and perturb it to a noisy latent zt with noise level\nt \u223c U[tmin, tmax]. We then generate a sample from the la-\ntent diffusion model by running DDIM sampling [50] for k\nintermediate steps, uniformly spaced between the smallest\nnoise level and t, yielding a latent sample z0. This latent is\ndecoded to produce a target image \u02c6x\u03c0 = D(z0), which we\nuse to supervise the rendering:\nLsample(\u03c8) = E\u03c0,t\n\u0002\nw(t) (\u2225x \u2212 \u02c6x\u03c0\u22251 + Lp(x, \u02c6x\u03c0))\n\u0003\n,\n(5)\nwhere Lp is the perceptual distance LPIPS [65], and w(t)\nis a noise-level dependent weighting function.\nThis dif-\nfusion loss is most similar to SparseFusion [67], and re-\nsembles the iterative dataset update strategy of Instruct-\nNeRF2NeRF [18], except we sample a new image at each\niteration. We empirically found this approach to work better\nthan score distillation sampling [36] (see supp. and Fig. 5).\nNovel view selection\nWhich views should we sample\nwhen using our diffusion prior? We do not want to place\nnovel views inside of objects or behind walls, and the place-\nment of views often depends on the scene content and type\nof capture. As in prior work such as RegNeRF [35], we\nwish to define a distribution based on the known input poses\nand capture pattern that will encompass a reasonable set of\nnovel camera poses, roughly matching the positions from\nwhich we would expect to observe the reconstructed scene.\nWe achieve this by determining a base set or path of\nposes through the scene, which we can randomly sample\nand perturb to define a full pose distribution for novel views.\nIn forward-facing captures such as LLFF [31] and DTU [23]\nor 360-degree captures such as mip-NeRF 360 [1], we de-\nfine an elliptical path fit to the training views, facing toward\nthe focus point (the point with minimum average distance\nto the training cameras\u2019 focal axes). In more unstructured\ncaptures such as CO3D [38] and RealEstate10K [66], we fit\na B-spline to roughly follow the trajectory of the training\n4\nviews. In either case, for each random novel view, we uni-\nformly select one of the poses in the path and then perturb\nits position, up vector, and look-at point within some range.\nPlease see the supplement for additional details.\n3.3. Implementation Details\nOur base diffusion model is a re-implementation of the\nLatent Diffusion Model [42] that has been trained on an\ninternal dataset of image-text pairs with input resolution\n512\u00d7512\u00d73 and a latent space with dimensions 64\u00d764\u00d78.\nThe encoder of our PixelNeRF is a small U-Net that takes as\ninput an image of resolution 512\u00d7512 and outputs a feature\nmap of resolution 64 \u00d7 64 with 128 channels (see the sup-\nplement for more details). We jointly train the PixelNeRF\nand finetune the denoising U-Net with batch size 256 and\nlearning rate 10\u22124 for a total of 250k iterations. To enable\nclassifier-free guidance (CFG), we set the input images to\nall zeros randomly with probability 10%.\nWe use Zip-NeRF [2] as our backbone and train the\nNeRF for a total of 1000 iterations. The reconstruction term\nLrecon uses the Charbonnier loss [8] as in Zip-NeRF. The\nweighting for Lsample is linearly decayed from 1 to 0.1 over\ntraining, and the classifier-free guidance scale used for sam-\npling is set to 3.0. We fix tmax = 1.0 for all training steps,\nand linearly anneal tmin from 1.0 to 0.0. Regardless of t,\nwe always sample the denoised image with k = 10 steps.\nIn practice, diffusion models for view synthesis can be con-\nditioned on a small number of observed input images and\nposes. Given a target novel view, we select the 3 nearest\ncamera positions from the observed inputs to condition the\nmodel. This enables our models to scale to large numbers of\ninput images while selecting inputs that are most useful for\nthe sampled novel view. Please refer to the supplementary\nmaterials for more implementation details.\n4. Experiments\nWe evaluate ReconFusion on five real-world datasets to\ndemonstrate the performance and generalizability of our ap-\nproach for few-view 3D reconstruction (Sec. 4.2). We also\nperform several ablations on the components of the diffu-\nsion model and the 3D reconstruction procedure (Sec. 4.3).\nFinally, we demonstrate that our method improves NeRF\nreconstruction across a range of capture settings (Sec. 4.4).\nAdditionally, we strongly advise the reader to view our sup-\nplementary video, as the visual difference in view synthesis\nresults is significantly clearer in video than in still images.\n4.1. Experiment Setup\nTraining Dataset To learn a generalizable diffusion prior\nfor novel view synthesis, we train on a mixture of the syn-\nthetic Objaverse [10] dataset and three real-world datasets:\nCO3D [38], MVImgNet [64], and RealEstate10K [66]. For\nObjaverse, we render each 3D asset from 16 randomly sam-\npled views at resolution 512\u00d7512 and composite the render-\ning onto a randomly selected solid color background. For\nthe other three real-world captured datasets, we center crop\nand resize each frame to 512 \u00d7 512. For training, we sam-\nple 3 frames of the same scene as input views and sample\nanother frame as the target view. Please refer to the supple-\nmentary materials for details about dataset mixing.\nEvaluation\nDataset\nWe\nevaluate\nour\nmethod\non\nscenes from multiple datasets with 3, 6 and 9 input\nviews, which include in-distribution datasets (CO3D [38]\nand RealEstate10K [66]) and out-of-distribution datasets\n(LLFF [31], DTU [23] and mip-NeRF 360 [1]). LLFF and\nDTU are datasets of forward-facing scenes, where we ad-\nhere to the evaluation protocol of RegNeRF [35]. For the\nreal-world object-centric scenes from CO3D we evaluate\non a subset of 20 scenes from 10 categories. RealEstate10K\ncontains video clips gathered from YouTube, and we sample\n10 scenes (each with 100 frames) from its test set for eval-\nuation. The mip-NeRF 360 dataset has 9 indoor or outdoor\nscenes each containing a complex central object or area and\na detailed background. For CO3D and RealEstate10K, we\nselect the input views evenly from all the frames and use\nevery 8th of the remaining frames for evaluation. For the\nmip-NeRF 360 dataset, we retain its original test set and se-\nlect the input views from the training set using a heuristic to\nencourage reasonable camera spacing and coverage of the\ncentral object (see supplement for details).\nBaselines\nFor evaluation datasets, we compare against\nthe state-of-the-art dense-view NeRF model Zip-NeRF [2]\n(which is also the reconstruction pipeline used in our\nmodel), and state-of-the-art few-view NeRF regularization\nmethods including DiffusioNeRF [60], FreeNeRF [61], and\nSimpleNeRF [49]. We also compare to ZeroNVS [44], con-\ncurrent work on novel view synthesis of scenes from a sin-\ngle image. To adapt it to multiview inputs, we use the input\nview closest to the sampled view as its input condition and\ndenote this method as ZeroNVS\u2217. On the CO3D dataset\nwe additionally compare to SparseFusion [67]. Following\ntheir setup, we train the SparseFusion model in a category-\nspecific manner. However, unlike the original implementa-\ntion, which masks out the foreground object and sidesteps\nthe difficulty of recovering the background of the scene, we\nuse the whole unmasked image. Please refer to the supple-\nment for more details about baselines.\n4.2. Comparison Results\nWe report the quantitative results in Table 1 and show qual-\nitative comparisons in Fig. 3. Please see the supplementary\nvideo for more visuals. Our backbone NeRF model, Zip-\nNeRF, often overfits to the input views and exhibits artifacts\nlike \u201cfoggy\u201d geometry or \u201cfloaters\u201d. State-of-the-art few-\nview NeRF regularization methods (DiffusioNeRF, FreeN-\n5\nZip-NeRF\nDiffusioNeRF\nFreeNeRF\nSimpleNeRF\nZeroNVS\nOurs\nGround Truth\nRealEstate10K (3)\nLLFF (3)\nDTU (3)\nCO3D (6)\nmip-NeRF 360 (9)\nFigure 3. A visual comparison of rendered images and depth maps on scenes from the RealEstate10K [66], LLFF [31], DTU [23],\nCO3D [38], and mip-NeRF 360 [1] datasets (input view count indicated in parentheses). Both the appearance and geometry of our method\nare of higher quality than the baselines in these examples\u2013typical failure modes exhibited by the baselines include \u201cfloater\u201d artifacts visible\nin depth maps, color artifacts or blurry low-fidelity geometry in minimally observed regions of the scenes, correct texture appearing in\nincorrect locations in the image, and so on. We encourage the reader to watch our supplementary video, as many of these differences are\neasier to identify with a moving camera trajectory.\n6\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nMethod\n3-view\n6-view\n9-view\n3-view\n6-view\n9-view\n3-view\n6-view\n9-view\nHarder\nDataset Difficulty\nEasier\n\u2190\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nRealEstate10K\nZip-NeRF\u2217\n20.77\n27.34\n31.56\n0.774\n0.906\n0.947\n0.332\n0.180\n0.118\nDiffusioNeRF\n19.12\n24.18\n27.78\n0.710\n0.808\n0.869\n0.444\n0.344\n0.282\nFreeNeRF\n20.54\n25.63\n27.32\n0.731\n0.817\n0.843\n0.394\n0.344\n0.332\nSimpleNeRF\n23.89\n28.75\n29.55\n0.839\n0.896\n0.900\n0.292\n0.239\n0.236\nZeroNVS\u2217\n19.11\n22.54\n23.73\n0.675\n0.744\n0.766\n0.422\n0.374\n0.358\nOurs\n25.84\n29.99\n31.82\n0.910\n0.951\n0.961\n0.144\n0.103\n0.092\nLLFF\nZip-NeRF\u2217\n17.23\n20.71\n23.63\n0.574\n0.764\n0.830\n0.373\n0.221\n0.166\nRegNeRF\n19.08\n23.09\n24.84\n0.587\n0.760\n0.820\n0.374\n0.243\n0.196\nDiffusioNeRF\n20.13\n23.60\n24.62\n0.631\n0.775\n0.807\n0.344\n0.235\n0.216\nFreeNeRF\n19.63\n23.72\n25.12\n0.613\n0.773\n0.820\n0.347\n0.232\n0.193\nSimpleNeRF\n19.24\n23.05\n23.98\n0.623\n0.737\n0.762\n0.375\n0.296\n0.286\nZeroNVS\u2217\n15.91\n18.39\n18.79\n0.359\n0.449\n0.470\n0.512\n0.438\n0.416\nOurs\n21.34\n24.25\n25.21\n0.724\n0.815\n0.848\n0.203\n0.152\n0.134\nDTU\nZip-NeRF\u2217\n9.18\n8.84\n9.23\n0.601\n0.589\n0.592\n0.383\n0.370\n0.364\nRegNeRF\n19.39\n22.24\n24.62\n0.777\n0.850\n0.886\n0.203\n0.135\n0.106\nDiffusioNeRF\n16.14\n20.12\n24.31\n0.731\n0.834\n0.888\n0.221\n0.150\n0.111\nFreeNeRF\n20.46\n23.48\n25.56\n0.826\n0.870\n0.902\n0.173\n0.131\n0.102\nSimpleNeRF\n16.25\n20.60\n22.75\n0.751\n0.828\n0.856\n0.249\n0.190\n0.176\nZeroNVS\u2217\n16.71\n17.70\n17.92\n0.716\n0.737\n0.745\n0.223\n0.205\n0.200\nOurs\n20.74\n23.62\n24.62\n0.875\n0.904\n0.921\n0.124\n0.105\n0.094\nCO3D\nZip-NeRF\u2217\n14.34\n14.48\n14.97\n0.496\n0.497\n0.514\n0.652\n0.617\n0.590\nDiffusioNeRF\n15.65\n18.05\n19.69\n0.575\n0.603\n0.631\n0.597\n0.544\n0.500\nFreeNeRF\n13.28\n15.20\n17.35\n0.461\n0.523\n0.575\n0.634\n0.596\n0.561\nSimpleNeRF\n15.40\n18.12\n20.52\n0.553\n0.622\n0.672\n0.612\n0.541\n0.493\nSparseFusion\n16.76\n18.77\n19.13\n0.561\n0.600\n0.604\n0.695\n0.653\n0.651\nZeroNVS\u2217\n17.13\n19.72\n20.50\n0.581\n0.627\n0.640\n0.566\n0.515\n0.500\nOurs\n19.59\n21.84\n22.95\n0.662\n0.714\n0.736\n0.398\n0.342\n0.318\nmip-NeRF 360\nZip-NeRF\u2217\n12.77\n13.61\n14.30\n0.271\n0.284\n0.312\n0.705\n0.663\n0.633\nDiffusioNeRF\n11.05\n12.55\n13.37\n0.189\n0.255\n0.267\n0.735\n0.692\n0.680\nFreeNeRF\n12.87\n13.35\n14.59\n0.260\n0.283\n0.319\n0.715\n0.717\n0.695\nSimpleNeRF\n13.27\n13.67\n15.15\n0.283\n0.312\n0.354\n0.741\n0.721\n0.676\nZeroNVS\u2217\n14.44\n15.51\n15.99\n0.316\n0.337\n0.350\n0.680\n0.663\n0.655\nOurs\n15.50\n16.93\n18.19\n0.358\n0.401\n0.432\n0.585\n0.544\n0.511\nTable 1. Quantitative evaluation of few-view 3D reconstruction methods. Datasets are ordered in terms of sparsity from easier (novel views\nare close to observed views) to harder (novel views are far from observed views). Our method is the first to be evaluated on such a wide\nrange of few-view real datasets. Despite this generality, we outperform all baselines across all domains. Baselines that we additionally\ntuned for the task of few-view reconstruction are indicated with \u2217.\neRF and SimpleNeRF) are able to significantly improve the\nbaseline quality on forward-facing scenes like LLFF and\nDTU. However, they fall short on 360-degree scenes (e.g.\nthe CO3D dataset), where a large portion of the scene is\nundersampled or even unobserved due to the a much larger\nrelative disparity between input views. On such scenes, Ze-\nroNVS serves as a strong baseline as it often can reconstruct\na complete 3D scene, but with limited visual fidelity. Our\nmethod outperforms all baselines on both in-distribution\nand out-of-distribution datasets, achieving state-of-the-art\nperformance for few-view NeRF reconstructions.\n4.3. Ablation Studies\nIn Table 2 and Fig. 4, we ablate two aspects of our diffu-\nsion model: the use of pretrained diffusion model weights\n(PT) and conditioning signal. To ablate \u201cPT,\u201d we train the\ndiffusion model from scratch. To ablate conditioning, in the\npose experiment we replace our PixelNeRF module with a\nconditioning mechanism similar to ZeroNVS [44] (which\nitself extends Zero-1-to-3 [29]).\nThis alternative simply\nconcatenates the latents of all input images to the U-Net\ninput, and represents the relative camera transforms as vec-\ntors (relative translation and rotation quaternion) which are\nconcatenated with the CLIP embeddings. Both variants pro-\nduce sampled images of lower quality, which subsequently\n7\nNeRF renders\nDiffusion samples\nPT\nCondition\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nPSNR\u2191 SSIM\u2191 LPIPS\u2193\nIn-domain\n\u2713\npose\n20.57\n0.749\n0.367\n15.11\n0.546\n0.484\npixelnerf\n25.15\n0.815\n0.246\n22.40\n0.723\n0.314\n\u2713\npixelnerf\n25.34\n0.823\n0.232\n24.05\n0.751\n0.281\nOut-domain\n\u2713\npose\n17.28\n0.521\n0.458\n12.18\n0.244\n0.599\npixelnerf\n19.82\n0.580\n0.383\n16.46\n0.420\n0.452\n\u2713\npixelnerf\n20.23\n0.596\n0.355\n17.44\n0.464\n0.411\nTable 2.\nWe ablate two aspects of our model: pretrained dif-\nfusion weights (PT) and conditioning. For PT, we initialize the\ndiffusion model weights from a pretrained text-to-image model.\npose uses a pose conditioning similar to ZeroNVS [44] while\npixelnerf uses our PixelNeRF conditioning (Sec. 3.1). We\nevaluate our model on both in-domain datasets (RealEstate10K,\nCO3D) and out-of domain datasets (LLFF, mip-NeRF 360).\npose\nw/o PT\nOurs\nGround Truth\nSample 1\nNeRF render 1\nSample 2\nNeRF render 2\nFigure 4. Ablation of diffusion model on 3-view reconstruction.\nWe show two samples from the diffusion models, and renderings\nfrom the reconstructed NeRFs under the same viewpoints for three\nvariants of the diffusion model: pose, without pretraining, and\nour full model. The samples from nearby poses are inconsistent\ndue to randomness, but can be successfully reconciled into an un-\nderlying NeRF reconstruction.\ndegrades the NeRF reconstruction.\nIn Fig. 5, we ablate the choice of diffusion loss and find\nstandard SDS results contain more artifacts, and the multi-\nstep diffusion loss effectively mitigates these artifacts. Ad-\nditionally, annealing tmin leads to more details.\n4.4. Scaling to More Views\nTo further investigate the effectiveness and robustness of\nour diffusion prior, we evaluate our method and the back-\nbone Zip-NeRF under various numbers of input views. As\nSDS\nMultistep\nAnnealed Multistep\nFigure 5. Comparing diffusion losses for 3D reconstruction. Note\nthe \u201cblotchy\u201d texture on the placemat and background chair when\nusing SDS, and improved background detail with annealing.\n3\n6\n9\n18\n27\n54\n81\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\n27.5\nPSNR\n3\n6\n9\n18\n27\n54\n81\n# Views\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nSSIM\n3\n6\n9\n18\n27\n54\n81\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLPIPS\nZip-NeRF\n+ Diffusion\nFigure 6. Our learned diffusion prior improves performance over\nthe Zip-NeRF baseline up to as many as 81 input views on the\nkitchenlego scene from the mip-NeRF 360 dataset. Though\nmost results in this paper focus on the challenging case of 3-9 in-\nput views, achieving a high-quality reconstruction of a real-world\nscene often requires significantly more images\u2013this plot demon-\nstrates that our diffusion prior reduces this requirement at all points\nalong the capture density vs. quality curve.\nthe number of views increases, the input captures provide\nbetter coverage of the entire scene, resulting in less ambigu-\nity. Therefore, we set the weighting factor for our diffusion\nloss (Lsample) to be inversely proportional to the number of\ninput views in this case. As shown in Fig. 6, our test set\nperformance is consistently better than Zip-NeRF, indicat-\ning that our diffusion prior can serve as an effective drop-in\nregularizer across a range of capture settings.\n5. Discussion\nThe goal of ReconFusion is to demonstrate the poten-\ntial in piecing together two powerful building blocks.\nFirst, a state-of-the-art optimization-based 3D reconstruc-\ntion pipeline, with an underlying 3D representation guar-\nanteeing multiview consistency. And second, a powerful\nmultiview-conditioned image diffusion model for generat-\ning plausible novel views, which can be used to guide re-\nconstruction to avoid the artifacts resulting from an un-\nderconstrained inverse problem. Many current limitations\nare evident: the heavyweight diffusion model is costly and\nslows down reconstruction significantly; our current results\ndemonstrate only limited 3D outpainting abilities compared\nto what our image model can hallucinate in 2D; tuning the\nbalance of reconstruction and sample losses is tedious; etc.\nHowever, this initial attempt at building such a system\nhas already produced compelling results across a variety of\nscene types with significantly reduced view counts. We are\n8\noptimistic that it may possibly serve as a template for im-\nprovements in sparse reconstruction, as we move toward a\nfuture of ever more accessible 3D reconstruction techniques\nwith dramatically reduced capture requirements.\nAcknowledgements\nWe would like to thank Arthur\nBrussee, Ricardo Martin-Brualla, Rick Szeliski, Peter Hed-\nman, and Jason Baldridge for their valuable contributions\nin discussing the project and reviewing the manuscript, and\nZhicheng Wang for setting up some of the data loaders nec-\nessary for our diffusion model training pipeline. We are\ngrateful to Randy Persaud and Henna Nandwani for infras-\ntructure support.\nReferences\n[1] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded\nAnti-Aliased Neural Radiance Fields. CVPR, 2022. 4, 5, 6\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P\nSrinivasan, and Peter Hedman. Zip-NeRF: Anti-aliased grid-\nbased neural radiance fields. ICCV, 2023. 1, 4, 5, 13\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructPix2Pix: Learning to Follow Image Editing Instruc-\ntions. CVPR, 2023. 11\n[4] Eric Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and\nGordon Wetzstein. pi-gan: Periodic implicit generative ad-\nversarial networks for 3d-aware image synthesis.\nCVPR,\n2021. 2\n[5] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. CVPR, 2022. 2\n[6] Eric R Chan, Koki Nagano, Matthew A Chan, Alexander W\nBergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini\nDe Mello, Tero Karras, and Gordon Wetzstein.\nGeNVS:\nGenerative novel view synthesis with 3D-aware diffusion\nmodels. arXiv, 2023. 2, 4\n[7] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano-\nlis Savva, Shuran Song, Hao Su, et al.\nShapeNet: An\nInformation-Rich 3D Model Repository. arXiv, 2015. 2\n[8] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and\nMichel Barlaud. Two deterministic half-quadratic regular-\nization algorithms for computed imaging. ICIP, 1994. 5\n[9] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su. MVSNeRF: Fast Gen-\neralizable Radiance Field Reconstruction from Multi-View\nStereo. ICCV, 2021. 2\n[10] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA Universe of Annotated 3D Objects. CVPR, 2023. 2, 5\n[11] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised NeRF: Fewer Views and Faster\nTraining for Free. CVPR, 2022. 1, 2\n[12] John Flynn, Ivan Neulander, James Philbin, and Noah\nSnavely. Deepstereo: Learning to predict new views from\nthe world\u2019s imagery. CVPR, 2016. 2\n[13] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3d shape\ninduction from 2d views of multiple objects. 3DV, 2017. 2\n[14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. NeurIPS, 2022.\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NIPS, 2014. 2\n[16] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind,\nChristian Theobalt, Lingjie Liu, and Ravi Ramamoorthi.\nNerfDiff: Single-image View Synthesis with NeRF-guided\nDistillation from 3D-aware Diffusion. ICML, 2023. 2\n[17] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu.\nSparseNeRF: Distilling Depth Ranking for Few-shot Novel\nView Synthesis. ICCV, 2023. 1, 2\n[18] Ayaan Haque, Matthew Tancik, Alexei A Efros, Aleksander\nHolynski, and Angjoo Kanazawa.\nInstruct-NeRF2NeRF:\nEditing 3D Scenes with Instructions. ICCV, 2023. 4\n[19] Philipp Henzler, Niloy J Mitra, , and Tobias Ritschel. Escap-\ning plato\u2019s cave: 3d shape from adversarial rendering. ICCV,\n2019. 2\n[20] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Ro-\nman Shapovalov, Tobias Ritschel, Andrea Vedaldi, and\nDavid Novotny. Unsupervised Learning of 3D Object Cate-\ngories From Videos in the Wild. ICCV, 2021. 2\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Dif-\nfusion Probabilistic Models. NeurIPS, 2020. 2, 4\n[22] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting NeRF\non a Diet: Semantically Consistent Few-Shot View Synthe-\nsis. ICCV, 2021. 2\n[23] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large Scale Multi-view Stereopsis Eval-\nuation. CVPR, 2014. 4, 5, 6\n[24] Animesh Karnewar, Niloy J. Mitra, Andrea Vedaldi, and\nDavid Novotny. Holofusion: Towards photo-realistic 3d gen-\nerative modeling. ICCV, 2023. 2\n[25] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J. Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images. CVPR, 2023. 2\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3D Gaussian Splatting for Real-Time\nRadiance Field Rendering. SIGGRAPH, 2023. 1\n[27] Minseop Kwak, Jiuhn Song, and Seungryong Kim. GeCoN-\neRF: Few-shot Neural Radiance Fields via Geometric Con-\nsistency. ICML, 2023. 1, 2\n[28] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3D: High-Resolution\nText-to-3D Content Creation. CVPR, 2023. 3\n[29] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-Shot One Image to 3D Object. arXiv, 2023. 2, 3, 7, 11\n9\n[30] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang. SyncDreamer: Gen-\nerating Multiview-consistent Images from a Single-view Im-\nage. arXiv, 2023. 2\n[31] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar.\nLocal Light Field Fusion: Practical View\nSynthesis with Prescriptive Sampling Guidelines.\nSIG-\nGRAPH, 2019. 4, 5, 6\n[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View\nSynthesis. ECCV, 2020. 1\n[33] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian\nRichardt, and Yong-Liang Yang.\nHologan: Unsupervised\nlearning of 3d representations from natural images. ICCV,\n2019. 2\n[34] Michael Niemeyer and Andreas Geiger. Giraffe: Represent-\ning scenes as compositional generative neural feature fields.\nCVPR, 2021. 2\n[35] Michael Niemeyer, Jonathan T. Barron, Ben Mildenhall,\nMehdi SM Sajjadi, Andreas Geiger, and Noha Radwan. Reg-\nNeRF: Regularizing Neural Radiance Fields for View Syn-\nthesis from Sparse Inputs. CVPR, 2022. 1, 2, 4, 5, 13\n[36] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. DreamFusion: Text-to-3D using 2D Diffusion. ICLR,\n2022. 2, 3, 4\n[37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\nTransferable Visual Models From Natural Language Super-\nvision. ICML, 2021. 3\n[38] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,\nLuca Sbordone, Patrick Labatut, and David Novotny. Com-\nmon Objects in 3D: Large-Scale Learning and Evaluation of\nReal-life 3D Category Reconstruction. ICCV, 2021. 2, 4, 5,\n6\n[39] Xuanchi Ren and Xiaolong Wang. Look Outside the Room:\nSynthesizing A Consistent Long-Term 3D Scene Video from\nA Single Image. CVPR, 2022. 2\n[40] Barbara Roessle, Jonathan T. Barron, Ben Mildenhall,\nPratul P Srinivasan, and Matthias Nie\u00dfner.\nDense Depth\nPriors for Neural Radiance Fields from Sparse Input Views.\nCVPR, 2022. 1, 2\n[41] Barbara\nRoessle,\nNorman\nM\u00a8uller,\nLorenzo\nPorzi,\nSamuel Rota Bul`o,\nPeter Kontschieder,\nand Matthias\nNie\u00dfner. GANeRF: Leveraging Discriminators to Optimize\nNeural Radiance Fields. arXiv preprint arXiv:2306.06044,\n2023. 2\n[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-Resolution Image\nSynthesis with Latent Diffusion Models. CVPR, 2022. 2, 3,\n5\n[43] Mehdi SM Sajjadi, Henning Meyer, Etienne Pot, Urs\nBergmann, Klaus Greff, Noha Radwan, Suhani Vora,\nMario Lu\u02c7ci\u00b4c, Daniel Duckworth, Alexey Dosovitskiy, et al.\nScene Representation Transformer: Geometry-Free Novel\nView Synthesis Through Set-Latent Scene Representations.\nCVPR, 2022. 2\n[44] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann,\nHong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry\nLagun, Li Fei-Fei, Deqing Sun, et al.\nZeroNVS: Zero-\nShot 360-Degree View Synthesis from a Single Real Image.\narXiv:2310.17994, 2023. 2, 5, 7, 8, 13\n[45] Katja Schwarz, Axel Sauer, Michael Niemeyer, Yiyi Liao,\nand Andreas Geiger. Voxgraf: Fast 3d-aware image synthesis\nwith sparse voxel grids. NeurIPS, 2022. 2\n[46] Seunghyeon Seo, Donghoon Han, Yeonjin Chang, and Nojun\nKwak. MixNeRF: Modeling a Ray with Mixture Density for\nNovel View Synthesis from Sparse Inputs. CVPR, 2023. 2\n[47] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. MVDream: Multi-view Diffusion for 3D\nGeneration. arXiv, 2023. 2, 3\n[48] Nagabhushan Somraj and Rajiv Soundararajan. ViP-NeRF:\nVisibility Prior for Sparse Input Neural Radiance Fields.\nSIGGRAPH, 2023. 1, 2\n[49] Nagabhushan Somraj,\nAdithyan Karanayil,\nand Rajiv\nSoundararajan.\nSimpleNeRF: Regularizing Sparse Input\nNeural Radiance Fields with Simpler Solutions. SIGGRAPH\nAsia, 2023. 2, 5, 13\n[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning Diffusion Implicit Models. ICLR, 2020. 2, 4\n[51] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Generalizable patch-based neural render-\ning. ECCV, 2022. 2\n[52] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon\nRezchikov, Joshua B Tenenbaum, Fr\u00b4edo Durand, William T\nFreeman, and Vincent Sitzmann.\nDiffusion with Forward\nModels: Solving Stochastic Inverse Problems Without Di-\nrect Supervision. arXiv, 2023. 2\n[53] Alex Trevithick and Bo Yang. GRF: Learning a General Ra-\ndiance Field for 3D Representation and Rendering. ICCV,\n2021. 2\n[54] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-\nBin Huang, and Johannes Kopf. Consistent View Synthesis\nwith Pose-Guided Diffusion Models. CVPR, 2023. 2\n[55] Richard Tucker and Noah Snavely. Single-View View Syn-\nthesis With Multiplane Images. CVPR, 2020. 2\n[56] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score Jacobian Chaining: Lifting\nPretrained 2D Diffusion Models for 3D Generation. CVPR,\n2023. 3\n[57] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P.\nSrinivasan, Howard Zhou, Jonathan T. Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser.\nIBRNet:\nLearning Multi-View Image-Based Rendering.\nCVPR, 2021. 2\n[58] Frederik Warburg, Ethan Weber, Matthew Tancik, Alek-\nsander Holynski, and Angjoo Kanazawa. Nerfbusters: Re-\nmoving Ghostly Artifacts from Casually Captured NeRFs.\nICCV, 2023. 2\n[59] Daniel Watson,\nWilliam Chan,\nRicardo Martin Bru-\nalla, Jonathan Ho, Andrea Tagliasacchi, and Mohammad\nNorouzi.\nNovel View Synthesis with Diffusion Models.\nICLR, 2022. 2\n10\n[60] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF:\nRegularizing neural radiance fields with denoising diffusion\nmodels. CVPR, 2023. 1, 2, 5, 13\n[61] Jiawei Yang, Marco Pavone, and Yue Wang.\nFreeNeRF:\nImproving Few-shot Neural Rendering with Free Frequency\nRegularization. CVPR, 2023. 1, 2, 5, 13\n[62] Paul Yoo, Jiaxian Guo, Yutaka Matsuo, and Shixiang Shane\nGu. DreamSparse: Escaping from Plato\u2019s Cave with 2D Dif-\nfusion Model Given Sparse Views. arXiv, 2023. 2\n[63] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural Radiance Fields from One or Few Im-\nages. CVPR, 2021. 2, 3, 11\n[64] Xianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu,\nChongjie Ye, Yushuang Wu, Zizheng Yan, Tianyou Liang,\nGuanying Chen,\nShuguang Cui,\nand Xiaoguang Han.\nMVImgNet: A Large-scale Dataset of Multi-view Images.\nCVPR, 2023. 2, 5\n[65] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The Unreasonable Effectiveness of Deep\nFeatures as a Perceptual Metric. CVPR, 2018. 4\n[66] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe,\nand Noah Snavely.\nStereo Magnification: Learning View\nSynthesis using Multiplane Images. SIGGRAPH, 2018. 2, 4,\n5, 6\n[67] Zhizhuo Zhou and Shubham Tulsiani. SparseFusion: Dis-\ntilling View-conditioned Diffusion for 3D Reconstruction.\nCVPR, 2023. 2, 3, 4, 5, 13\n[68] Jun-Yan Zhu, Zhoutong Zhang, Chengkai Zhang, Jiajun Wu,\nAntonio Torralba, Josh Tenenbaum, and Bill Freeman. Vi-\nsual object networks: Image generation with disentangled\n3d representations. NeurIPS, 2018. 2\nA. Diffusion Model Details\nOur diffusion model is adapted from a pre-trained text-to-\nimage latent diffusion model that maps 512 \u00d7 512 \u00d7 3 in-\nputs into a latent dimension of 64 \u00d7 64 \u00d7 8. We modify\nthis initial model to accept the necessary conditioning sig-\nnals for text-free novel-view synthesis. We replace the in-\nputs to the cross-attention pathway (which typically consist\nof a sequence of CLIP text embeddings) with the outputs\nof an additional dense layer. The input to this dense layer\nis a concatenated tensor consisting of (1) the unconditional\nCLIP text embedding (i.e. the empty string \"\"), and (2) the\nCLIP image embeddings of each of the input conditioning\nframes. We initialize the weights of this dense layer such\nthat it produces the unconditional CLIP text embedding at\nthe start of fine-tuning. This mechanism is inspired by the\nfine-tuning process in Zero-1-to-3 [29]. Zero-1-to-3 fine-\ntunes from an image variations base model that has been\npreviously fine-tuned to enable conditioning on CLIP image\nembeddings. We fine-tune directly from a text-conditioned\nmodel, but our architecture can learn image variation-like\nbehavior through the dense layer. Furthermore, unlike Zero-\n1-to-3, our dense layer does not take pose as input, since the\n3D transformation between the input conditioning frames\nand the target frame is applied through the PixelNeRF ren-\ndering process. In addition to the cross-attention modifica-\ntions, we concatenate the outputs of the PixelNeRF model\n(a 64 \u00d7 64 \u00d7 131 tensor consisting of RGB and features)\nto the input noise that is passed to the U-Net. As in prior\nwork [3], we initialize the additional convolutional weights\nto zero such that the added inputs have no effect at the start\nof fine-tuning.\nTo enable classifier-free guidance on our added condi-\ntioning signals, we drop out all conditioning images for a\ntraining example with 10% probability. We drop out the\nCLIP and PixelNeRF conditioning pathways independently\nin order to enable separate guidance, although we found em-\npirically that using the same guidance weight across both\nconditioning signals (i.e. performing joint CFG across both\nconditioning signals) produces optimal results.\nWe train\nour model for 250,000 iterations with a learning rate of\n10\u22124 and a batch size of 128. Our training examples con-\nsist of 3 input conditioning images, 1 target image, and\nthe corresponding relative poses between each input image\nand the target image. This data is sampled from CO3D,\nRealEstate10k, MVImgNet, and Objaverse with uniform\nprobability. For Objaverse, we light the object with ran-\ndom environment maps and compose it onto a random solid\nbackground at each training iteration.\nB. PixelNeRF Details\nOur PixelNeRF module is inspired by but not identical to\nthe architecture proposed in the original work [63]. The\ninputs are N images along with their camera poses (extrin-\nsic and intrinsic matrices), along with a target camera pose.\nThe output is an approximate rendering at the target cam-\nera pose (both RGB and feature channels), which is con-\ncatenated with the input to the diffusion U-Net to provide a\nstrong conditioning signal that encodes the pose and image\ncontent of the target novel view. During inference, we typi-\ncally resize (but do not crop) the inputs to PixelNeRF such\nthat their shorter dimension is 512 pixels, since the model\nwas trained on 512 \u00d7 512 resolution inputs. The output tar-\nget image is always 64 \u00d7 64 \u00d7 131 (3 RGB channels plus\n128 feature channels), to match the latent resolution of the\ndiffusion model.\nThe PixelNeRF module begins by passing all input im-\nages through a 2D U-Net to create feature images of equal\nspatial resolution with 128 channels.\nWe then cast rays\nthrough each pixel of the target image, and sample 128\npoints along each ray from depth 0.5 to \u221e (uniform up\nto distance 1, then linear in disparity). We reproject these\npoints into each of the input cameras and gather correspond-\ning features from the feature images to make a gathered ten-\nsor of size 64 \u00d7 64 \u00d7 N \u00d7 128. We append positionally-\nencoded 3D locations, as well as the mean and variance\n11\n(a) 3 input views\n(b) 6 input views\n(c) 9 input views\n(d) Ground truth\nFigure 7. More training views leads to better samples. Here we show samples from the diffusion model at a novel viewpoint while\nvarying the number of training images. In all cases, the diffusion model is given the three nearest images to the novel viewpoint. We\nsee that as we increase the number of known scene observations, the expected distance to the nearest training view decreases, therefore\nincreasing the fidelity of diffusion model samples.\n(a) PixelNeRF RGB output\n(b) Diffusion model sample\n(c) PixelNeRF RGB output\n(d) Diffusion model sample\nFigure 8. PixelNeRF Visualization. Here we show (a,c) a visualization of the 64 \u00d7 64 RGB component of the PixelNeRF output, and\n(b,d) the corresponding sample from the diffusion model, which is conditioned on the PixelNeRF outputs.\n12\nof these features over the N-long dimension correspond-\ning to the number of inputs. A small MLP then processes\nthis full tensor along the channel dimension to output a new\nset of features and weights. These weights are then used\nto compute a weighted sum along the N-long dimension,\nthereby producing a new tensor of size 64 \u00d7 64 \u00d7 128.\nA second MLP then processes this summed tensor along\nthe channel dimension to produce the final output of size\n64 \u00d7 64 \u00d7 (3 + 128).\nAll learned components (including the 2D U-Net used\nto extract image features) are initialized randomly and op-\ntimized jointly with the fine-tuned diffusion model U-Net.\nAs mentioned in the main text, we apply an RGB recon-\nstruction loss to encourage the PixelNeRF module to learn\na useful conditioning signal. See Figure 8 for a visualization\nof our PixelNeRF model.\nC. Dataset Details\nFor LLFF and DTU, we use the standard train/test splits\nproposed by earlier works. For RealEstate10k and CO3D,\nwe select the training views evenly from all the frames and\nuse every 8th of the remaining frames for evaluation. For\nthe mip-NeRF 360 dataset we design a heuristic to choose\na train split of views that are uniformly distributed around\nthe hemisphere and pointed toward the central object of in-\nterest: We randomly sample 106 different 9-view splits and\nuse the one that minimizes these heuristic losses, then fur-\nther choose the 6- and 3-view splits to be subsets of the\n9-view split.\nWe carefully rescale each dataset to be compatible with\nthe near plane of 0.5 expected by the PixelNeRF module.\nDTU, CO3D, mip-NeRF 360 are rescaled by setting the\n\u201cfocus point\u201d of the data to the origin and rescaling cam-\nera positions to fit inside a [\u22121, 1]3 cube. RealEstate10k\nis pre-scaled by its creators to have a reasonable near dis-\ntance of 1.0, so we simply multiply its camera positions by\n0.5. LLFF similarly provides a near bound based on the\nCOLMAP point cloud, which we use to rescale the data to\nallow a 0.5 near plane.\nD. Baselines Details\nOur Zip-NeRF [2] baseline has slight hyperparameter mod-\nifications from the original that better suit few-view recon-\nstruction. This was done primarily to provide a maximally\ncompetitive baseline for our model, but these same hyper-\nparameters are used by our model as well, and we observe\na modest performance improvement due to them. In partic-\nular, we use:\n\u2022 Distortion loss with weight 0.01,\n\u2022 Normalized weight decay on the NGP grid parameters\nwith strength 0.1,\n\u2022 A smaller view-dependence network with width 32 and\ndepth 1, to avoid overfitting,\n\u2022 No hexagonal spiral control points, to accelerate render-\ning at the cost of introducing some aliasing,\n\u2022 A downweighted density in the \u201ccontracted\u201d region of\nspace outside of the unit sphere, wherein we multiply\nthe density emitted by Zip-NeRF by |det(JC(x))| (the\nisotropic scaling induced by the contraction function, see\nthe supplement of Barron et al. [2]).\nWe find that this baseline performs competitively on\nforward-facing scenes such as LLFF and RealEstate10k,\nespecially with 9 input views, but often produces many\nfloaters or fails to reconstruct any meaningful geometry on\nthe more difficult datasets. Further tuning and the addi-\ntion of other heuristic regularizers (e.g., the techniques used\nin RegNeRF or FreeNeRF) would likely improve results.\nHowever, the point of this model is to show baseline re-\nconstruction performance with our diffusion model regular-\nizer disabled, rather than to be a state-of-the-art few-view\nmethod. To re-emphasize this: the only difference between\nresults labeled \u201cOurs\u201d and \u201cZip-NeRF\u201d is that the diffusion\nregularizer weight is set to 0, all other hyperparameters are\nidentical.\nFor RegNeRF [35] and FreeNeRF [61], we use the re-\nsult images shared by the authors for the LLFF and DTU\ndatasets, and run the authors\u2019 code for FreeNeRF on the\nother three datasets. For DiffusioNeRF [60], we use the au-\nthors\u2019 result images on the DTU dataset, and run their code\non the other four datasets. Because SimpleNeRF [49] used\ndifferent train/test splits for LLFF and DTU, we run the au-\nthors\u2019 code on all five datasets.\nSparseFusion [67] was originally trained on CO3D us-\ning masked images of foreground objects. We re-train their\nmodels using the unmasked images in a category-specific\nmanner, then use their 3D distillation pipeline to obtain the\nfinal rendered images.\nZeroNVS [44] is a concurrent work that trains a diffu-\nsion model for novel view synthesis of scenes from a single\nimage. To evaluate its performance on multiview inputs, we\nmodify its reconstruction pipeline by using the input view\nclosest to the sampled random view for conditioning the dif-\nfusion model. For DTU and mip-NeRF 360 scenes (which\nthey evaluate in their paper for the single input case), we\nfollow their viewpoint selection strategy for sampling new\nviews. For CO3D, we use the same strategy that is em-\nployed for the mip-NeRF 360 scenes. For RealEstate10K,\nwe sample new views on a spline path fitted from the input\nviews, then perturb them. For LLFF, we sample new views\non a circle fitted from the input views, then perturb them.\n13\n"
  },
  {
    "title": "Training Chain-of-Thought via Latent-Variable Inference",
    "link": "https://arxiv.org/pdf/2312.02179.pdf",
    "upvote": "8",
    "text": "Training Chain-of-Thought via Latent-Variable\nInference\nDu Phan\u2217\nMatthew D. Hoffman\u2217\nDavid Dohan\u2020\nSholto Douglas\nTuan Anh Le\nAaron Parisi\nPavel Sountsov\nCharles Sutton\nSharad Vikram\nRif A. Saurous\nGoogle\nAbstract\nLarge language models (LLMs) solve problems more accurately and interpretably\nwhen instructed to work out the answer step by step using a \u201cchain-of-thought\u201d\n(CoT) prompt. One can also improve LLMs\u2019 performance on a specific task by\nsupervised fine-tuning, i.e., by using gradient ascent on some tunable parameters to\nmaximize the average log-likelihood of correct answers from a labeled training set.\nNaively combining CoT with supervised tuning requires supervision not just of the\ncorrect answers, but also of detailed rationales that lead to those answers; these\nrationales are expensive to produce by hand. Instead, we propose a fine-tuning\nstrategy that tries to maximize the marginal log-likelihood of generating a correct\nanswer using CoT prompting, approximately averaging over all possible rationales.\nThe core challenge is sampling from the posterior over rationales conditioned\non the correct answer; we address it using a simple Markov-chain Monte Carlo\n(MCMC) expectation-maximization (EM) algorithm inspired by the self-taught\nreasoner (STaR), memoized wake-sleep, Markovian score climbing, and persistent\ncontrastive divergence. This algorithm also admits a novel control-variate technique\nthat drives the variance of our gradient estimates to zero as the model improves.\nApplying our technique to GSM8K and the tasks in BIG-Bench Hard, we find that\nthis MCMC-EM fine-tuning technique typically improves the model\u2019s accuracy on\nheld-out examples more than STaR or prompt-tuning with or without CoT.\n1\nIntroduction\nFor many mathematical, logical, and common-sense reasoning problems, large language models solve\nproblems more accurately when instructed to work out the answer step by step in a chain of thought\nor a scratchpad (Wei et al., 2022; Nye et al., 2021; Kojima et al., 2022; Rajani et al., 2019; Shwartz\net al., 2020). These methods encourage the model to produce a rationale, that is, text describing a\nsequence of reasoning steps that leads to an answer; the motivation is that it seems to be easier for the\nmodel to generate a sequence of correct reasoning steps than to generate the final answer directly.\nBecause of the striking performance of chain-of-thought methods, many variants have been proposed\n(Wang et al., 2022b; Zhou et al., 2022; Creswell et al., 2022; Ye & Durrett, 2023), but there are still\nmany cases in which the rationales are incorrect.\nOne way to improve these methods is to fine-tune models to generate better rationales. If gold-\nstandard rationales can be obtained, such as via crowdsourcing (Rajani et al., 2019) or automatically\n(Nye et al., 2021), then supervised methods can be applied, but obtaining this data can be difficult.\nAn appealing alternative is to start from datasets that contain questions and correct answers only,\nwhich are more readily available, and bootstrap rationales during learning. A version of this strategy\n\u2217Corresponding authors: {mhoffman,phandu}@google.com. The first two authors contributed equally,\nand their order was chosen randomly.\n\u2020Current affiliation: OpenAI.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2312.02179v1  [cs.LG]  28 Nov 2023\nwas proposed as the self-taught reasoner (STaR) (Zelikman et al., 2022), which generates proposed\nrationales from an LLM, and then fine-tunes on rationales that lead to the correct answer.\nIn this paper, we approach the problem of bootstrapping rationales from a different conceptual\ndirection: chain-of-thought methods are probabilistic latent-variable models. The LLM defines a\njoint probability distribution over questions, rationales, and answers; this joint distribution implies a\nmarginal distribution of answers given questions, averaging over all possible rationales weighted by\ntheir probability given the question. The problem of self-training for reasoning then becomes one of\nlearning with incomplete data, a core task in probabilistic machine learning (Murphy, 2022) to which\nwe can apply methods from a large and sophisticated literature.\nThis perspective raises a technical challenge, because computing the marginal distribution requires\naveraging over a vast set of potential rationales. To address this, we introduce a learning algorithm for\nrationale generation, which we call TRICE.3 TRICE is a simple Markov-chain Monte Carlo (MCMC)\nexpectation-maximization (EM) algorithm combined with a novel control-variate scheme, inspired\nby ideas from STaR (Zelikman et al., 2022), memoized wake-sleep (Hewitt et al., 2020), Markovian\nscore climbing (Naesseth et al., 2020), and persistent contrastive divergence (Tieleman, 2008).\nThis view unifies several threads of work in reasoning using LLMs: It provides an alternative\ninterpretation of STaR as a kind of biased stochastic expectation-maximization algorithm (Nielsen,\n2000) that underweights difficult examples when its rationalization process fails. Self-consistency\n(Wang et al., 2022a) can be seen as a Monte Carlo algorithm for computing the most likely answer\nunder the marginal distribution. Compared to self-consistency, the probabilistic learning approach\nof TRICE allows us to average over rationales not only at inference time, but also at training time.\nCompared to STaR, TRICE is less likely to ignore difficult examples (which stabilizes convergence\nand improves performance), and is also able to learn from incorrect rationales as well as correct ones.\nWe apply our technique to the GSM8K dataset (Cobbe et al., 2021) and to the BIG-Bench Hard bench-\nmark (Suzgun et al., 2022a). We find that TRICE improves the model\u2019s performance significantly,\noutperforming models tuned with STaR, direct tuning with or without CoT, and even supervised\nfine-tuning on human-generated rationales.\n2\nMethod\nGiven a training set of N questions x1:N and answers y1:N, we formalize CoT tuning as optimizing a\nparameter vector \u03b8 to maximize the average marginal log-likelihood of answers given questions:\nL(\u03b8) \u225c 1\nN\nP\nn log p\u03b8(yn | xn) = 1\nN\nP\nn log P\nz p\u03b8(z | xn)p(yn | z, xn),\n(1)\nwhere z is an unobserved latent rationale, p\u03b8(z | x) is the probability4 of obtaining the rationale\nz by prompting an LLM with the question x and tunable parameters \u03b8, and p\u03b8(y | z, x) is the\nprobability of obtaining the answer y given rationale z, question x, and parameters \u03b8. We will\nbe particularly interested in models where the likelihood p\u03b8(y | x, z) \u2208 {0, 1}, that is, where the\nanswer y is a deterministic function of z. For example, we might say that the model\u2019s answer is\ny = \u201c(a)\u201d if z ends with the string \"The answer is (a).\" For this deterministic model, we define\np(y | z, x) = c(z, y) \u2208 {0, 1}. Details of c(z, y) for each task can be found in Appendix F. We\nbelieve that such a binary likelihood model is appropriate for question-answering tasks where z is\na rationale\u2014a good rationale should leave no ambiguity about the correct answer. The derivations\nbelow will therefore assume a binary likelihood function. It is straightforward to generalize our\nmethods to cases where the relationship between z and y is weaker and therefore p(y | x, z) is more\ncomplicated; Appendix A shows how.\nAlgorithm\n1\noutlines\nthe\nmethod.\nA\nnotebook\nwith\na\nreference\nimplementation\ncan be found at https://github.com/google-research/cascades/tree/main/cascades/\nexamples/notebooks/trice.ipynb.\nWe start by initializing a memory containing a latent rationale zn for each example pair xn, yn by\nsampling zn from a hinted guide distribution q(z | xn, yn) that may condition on the correct answer\nyn as well as the question xn. For example, the guide might prompt an LLM specifically to give an\n3TRICE stands for \u201cTuning Rationales with Independence-Chain Expectation-maximization.\u201d\n4Unless otherwise specified, we sample at temperature 1 throughout.\n2\nrationale for the answer; more details about the precise prompts used by the guide are in Appendix F.\nIn some cases sampling from the guide instead of the model p\u03b8(z | xn) increases the chances of\ngenerating a correct rationale (Zelikman et al., 2022).\nWe then proceed to the main optimization loop. Each iteration, we sample a minibatch of M questions\nand answers from the dataset, and retrieve the rationales associated with those examples from the\nmemory. We then propose new rationales \u02dcz from the current model p\u03b8(z | x), and whenever the new\nrationale \u02dcz is correct (i.e., c(\u02dcz, y) = 1) replace the old rationale in memory with the new one.\nAt this point we have all we need to compute a gradient estimate; we can just average the gradients\n\u2207\u03b8 log p\u03b8(zim | xim) that we obtain from those rationales in the updated memory that are correct\n(i.e., we ignore examples where both the proposed rationale and the previous rationale were wrong).\nBASIC_GRADIENT_ESTIMATE in Algorithm 1 shows how.\nBut we can also reduce the variance of our gradient estimator by incorporating a control variate,\nas in CONTROL_VARIATE_GRADIENT_ESTIMATE in Algorithm 1. We first compute leave-one-out\nestimates \u03b21:M of the average probability of accepting a new rationale. For each example m, we\nsubtract off a scaled control variate \u03b2m\u2207\u03b8 log p\u03b8(\u02dczm | xim) whose expected value is zero (since it is\na score function). If the proposed rationale \u02dczm for example m is correct, then zim = \u02dczm, and the mth\ngradient contribution becomes (1 \u2212 \u03b2m)\u2207\u03b8 log p\u03b8(zim | xim), i.e., it is scaled down by 1 \u2212 \u03b2m. If\n\u02dczm is incorrect, then we adjust the gradient estimate to try to make \u02dczm less likely under p\u03b8. As the\nmodel becomes more accurate (i.e., \u03b2 gets closer to 1), we give more weight to incorrect rationales\n(when they occur) and less weight to correct rationales (most of the time).\nCONTROL_VARIATE_GRADIENT_ESTIMATE is more expensive than BASIC_GRADIENT_ESTIMATE,\nsince we must compute gradients not only for the rationales in memory but also for any incorrect\nrationales we generate. This may be wasteful, especially if many of the weights on those gradients\n(1 \u2212 \u03b2 for correct proposals, \u03b2 for incorrect proposals) are close to zero because \u03b2 is close to zero or\none. To reduce this cost, in SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE, we use\nsystematic resampling (Hol et al., 2006) to generate a subsample of L question-rationale pairs, from\nwhich we obtain an unbiased estimate of the output of CONTROL_VARIATE_GRADIENT_ESTIMATE.\nWe preferentially sample gradients with higher scalar weights; if \u03b2 is small, we are less likely to\nsample incorrect rationales (which have weight \u03b2), and if \u03b2 is large, we are less likely to sample\ncorrect proposed rationales (which have weight 1 \u2212 \u03b2). This can be seen as a generalization of the\nstrategy of Burda et al. (2015, Section 3) for reducing the cost of computing IWAE gradients.\nBelow, we derive this variance-reduced stochastic MCMC-EM procedure in more detail.\n2.1\nDerivation\nThe true gradient.\nThe gradient of the marginal log-likelihood log p\u03b8(y | x) with respect to \u03b8 is\n\u2207\u03b8 log P\nz p\u03b8(z, y | x) = P\nz\np\u03b8(z,y|x)\u2207\u03b8 log p\u03b8(z,y|x)\nP\nz\u2032 p\u03b8(z\u2032,y|x)\n= P\nz p\u03b8(z | x, y)\u2207\u03b8 log p\u03b8(z | x),\n(2)\nthat is, it is the expectation with respect to the posterior p\u03b8(z | x, y) of the gradient of the conditional\nlog-prior log p\u03b8(z | x), since the likelihood p(y | z, x) = c(z, y) does not depend on \u03b8. So if we can\nsample from the posterior over rationales z conditioned on the question-answer pair x, y, then we\ncan compute an unbiased estimate of the gradient of the marginal log-likelihood log p\u03b8(y | x). We\ncan interpret this as \u201cbootstrapping\u201d rationales z that are consistent with both the prior on rationales\np\u03b8(z | x) and the observed answer y (cf. Zelikman et al., 2022).\nIndependence sampler for p\u03b8(z | x, y).\nWe cannot directly sample from p\u03b8(z | x, y), so we resort\nto Markov chain Monte Carlo (MCMC). We maintain a memory (cf. Hewitt et al., 2020) of a single\nrationale zn for each question-answer pair xn, yn, and each iteration we apply a random update to\nzn that leaves the posterior p\u03b8(zn | xn, yn) invariant (cf. Tieleman, 2008). Each MCMC update\nbrings the zn\u2019s closer in distribution to p\u03b8(zn | xn, yn) (Cover, 1999; Murray & Salakhutdinov,\n2008). However, updates to \u03b8 may change the posterior p\u03b8(zn | xn, yn), so we must keep updating\nthe chains to control the bias of our gradient estimates.\nTo update the chains, we use a simple, hyperparameter-free independence sampler (Tierney, 1994);\na Metropolis-Hastings (Hastings, 1970) update that proposes updating the current state z with a\ndraw \u02dcz from a distribution rx,y that does not depend on z, and accepts the update with probability\n3\nAlgorithm 1 TRICE\nInput: Generative model p\u03b8(z, y | x), is-correct function c(z, y), dataset x1:N, y1:N, hinted guide distribution\nq(z | x, y), initial parameters \u03b8, optimizer update function h(\u03b8, g, t), minibatch size M, gradient minibatch size\nL, number of iterations T.\nOutput: Tuned parameters \u03b8, rationales z1:N.\n1: for n \u2208 1, . . . , N do (in parallel)\n// Initialize Markov chain states.\n2:\nSample zn from q(z | xn, yn).\n// Sample \u201cfallback\u201d rationale from guide q.\n3: end for\n4: for t \u2208 1, . . . , T do\n// Main optimization loop.\n5:\nGet next minibatch of M indices i1:M into the dataset.\n6:\nfor m \u2208 1, . . . , M do (in parallel)\n// Take one MCMC step to update Markov chain states.\n7:\nSample \u02dczm from p\u03b8(z | xim).\n8:\nif c(\u02dczm, yim) then\n// Accept or reject proposal.\n9:\nUpdate zim \u2190 \u02dczm.\n10:\nend if\n11:\nLet \u02dccm = c(\u02dczm, yim).\n// Whether the proposal is correct.\n12:\nLet c\u2032\nm = c(zim, yim).\n// Whether the updated rationale is correct.\n13:\nend for\n14:\nCompute \u02c6g using either BASIC_GRADIENT_ESTIMATE(z, x, c\u2032),\nCONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, \u02dcz, \u02dcc, c\u2032),\nor SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, \u02dcz, \u02dcc, c\u2032).\n15:\nUpdate \u03b8 \u2190 h(\u03b8, \u02c6g, t).\n// Apply gradient update.\n16: end for\n17: return \u03b8, z1:N.\n18: procedure BASIC_GRADIENT_ESTIMATE(z, x, c\u2032)\n19:\nreturn\nP 1\nm c\u2032m\nP\nm c\u2032\nm\u2207\u03b8 log p\u03b8(zim | xim).\n20: end procedure\n21: procedure CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, \u02dcz, \u02dcc, c\u2032)\n22:\nfor m \u2208 1, . . . , M do (in parallel)\n23:\nSet \u03b2m =\nP\nm\u2032\u0338=m c\u2032\nm\u2032 \u02dccm\u2032\nP\nm\u2032\u0338=m c\u2032\nm\u2032\n.\n// Compute leave-one-out control-variate scales.\n24:\nend for\n25:\nreturn\nP 1\nm c\u2032m\nP\nm c\u2032\nm(\u2207\u03b8 log p\u03b8(zim | xim) \u2212 \u03b2m\u2207\u03b8 log p\u03b8(\u02dczm | xim)).\n26: end procedure\n27: procedure SUBSAMPLED_CONTROL_VARIATE_GRADIENT_ESTIMATE(z, x, \u02dcz, \u02dcc, c\u2032)\n28:\nfor m \u2208 1, . . . , M do (in parallel)\n29:\nSet \u03b2m =\nP\nm\u2032\u0338=m c\u2032\nm\u2032 \u02dccm\u2032\nP\nm\u2032\u0338=m c\u2032\nm\u2032\n.\n// Compute leave-one-out control-variate scales.\n30:\nSet \u02dcwm = c\u2032\nm(1 \u2212 \u02dccm\u03b2m),\n// Compute unnormalized weights for subsampling.\n\u02dcwM+m = c\u2032\nm(1 \u2212 \u02dccm)\u03b2m.\n31:\nend for\n32:\nChoose a subset of L indices j1:L using systematic resampling with probabilities\n\u02dc\nw\nP2M\nm=1 \u02dc\nwm .\n33:\nfor \u2113 \u2208 1, . . . , L do (in parallel)\n34:\nif j\u2113 \u2264 M then\n// Selected correct rationale.\n35:\nLet \u02c6m = j\u2113, \u02c6z = zi \u02c6\nm, s = 1.\n36:\nelse\n// Selected incorrect rationale.\n37:\nLet \u02c6m = j\u2113 \u2212 M, \u02c6z = \u02dczm, s = \u22121.\n38:\nend if\n39:\nCompute \u02c6g\u2113 = s\u2207\u03b8 log p\u03b8(\u02c6z | xi \u02c6\nm). // Negate gradient if \u2113th rationale is incorrect.\n40:\nend for\n41:\nreturn\nP2M\nm=1 \u02dc\nwm\nP\nm c\u2032m\n1\nL\nPL\n\u2113=1 \u02c6g\u2113.\n42: end procedure\n\u03b1(\u02dcz | z) = min\nn\n1, p\u03b8(\u02dcz,y|x)/rx,y(\u02dcz)\np\u03b8(z,y|x)/rx,y(z)\no\n. We choose rx,y(z) = p\u03b8(z | x), which simplifies the\nacceptance probability to \u03b1(\u02dcz | z) = min\nn\n1, p\u03b8(y|x,\u02dcz)\np\u03b8(y|x,z)\no\n. This is 1 if c(\u02dcz, y) = 1, 0 if c(\u02dcz, y) = 0\nand c(z, y) = 1, and ill-defined (implying that we have to reject) if both c(z, y) = 0 and c(\u02dcz, y) = 0.\nSo we accept whenever the proposal \u02dcz is correct, and reject otherwise.\n4\nRemarks: Independence samplers can be understood as \u201cMetropolized\u201d importance samplers that\nspread the work of generating and evaluating proposals over time. In our setting, the update can\nalso be interpreted as attempting to sample from the posterior by rejection sampling, then falling\nback on an old sample if that fails. The expected number of iterations between successful updates is\np(y | x)\u22121, so mixing will be faster for easier questions x, and will accelerate as the model improves.\nBasic gradient estimator.\nThis MCMC/rejection-sampling procedure lets us approximate the\ngradient of the marginal log-likelihood in Equation (2). Denoting as z the state5 of the Markov chain\nfor an example x, y before the update, we sample a proposal \u02dcz from p\u03b8(z | x), accept the new state if\nit is correct (i.e., if c(\u02dcz, y) = 1), and compute the gradient of the log-probability of the result:\nz\u2032 = c(\u02dcz, y)\u02dcz + (1 \u2212 c(\u02dcz, y))z; \u02c6g = \u2207\u03b8 log p\u03b8(z\u2032 | x); Ez,\u02dcz[\u02c6g | \u03b8] \u2248 Ep\u03b8(z|x,y)[\u2207\u03b8 log p\u03b8(z | x)],\n(3)\nwhere Ez,\u02dcz[\u00b7 | \u03b8] denotes an expectation with respect to both the proposal \u02dcz and the previous state z.\nRemarks: The estimate will have low bias if the distribution of z\u2032 is close to the posterior p(z | x, y),\nwhich we expect to be true if the chain is mixing quickly enough relative to how fast \u03b8 is changing.\nThis will happen if either the probability of getting a correct answer is high, or if \u03b8 is changing\nslowly due to a small learning rate and/or gradient. If the model\u2019s training-set accuracy improves\nwith training and we use a decaying learning-rate schedule, then as training proceeds both of these\nfactors should work to reduce the bias of the gradient estimate.\nAdding a control variate.\nThe mean of an estimator \u02c6g is not affected by subtracting a zero-mean\nrandom variable b from it. If b is positively correlated with \u02c6g, then \u02c6g \u2212 b can have lower variance\nthan \u02c6g, and we say that b can be used as a \u201ccontrol variate\u201d (Owen & Zhou, 2000). Since, by the\nscore-function identity, Epz|x[\u03b2\u2207\u03b8 log p\u03b8(z | x)] = 0 (for any scalar \u03b2 independent of z), we can use\nthe proposed samples \u02dcz to generate control variates for our gradient estimator:\nEz,\u02dcz[\u02c6g | \u03b8] = Ez[E\u02dcz[\u2207\u03b8 log p\u03b8(z\u2032 | x)] | \u03b8]\n= Ez[E\u02dcz[\u2207\u03b8 log p\u03b8(z\u2032 | x) \u2212 \u03b2\u2207\u03b8 log p\u03b8(\u02dcz | x)] | \u03b8].\n(4)\nRemarks: The value of this estimator will depend on whether or not we accept the proposal \u02dcz:\n\u2207\u03b8 log p\u03b8(z\u2032 | x)\n\u2212 \u03b2\u2207\u03b8 log p\u03b8(\u02dcz | x) =\n\u001a(1 \u2212 \u03b2)\u2207\u03b8 log p\u03b8(z\u2032 | x)\nif \u02dcc = 1,\n\u2207\u03b8 log p\u03b8(z\u2032 | x) \u2212 \u03b2\u2207\u03b8 log p\u03b8(\u02dcz | x)\nif \u02dcc = 0,\n(5)\nwhere we use the shorthand \u02dcc \u225c c(\u02dcz, y).\nThis control variate can drive the variance of the gradient estimator to zero as the model converges\nto perfect accuracy on the training set (cf. Roeder et al., 2017). If we set \u03b2 = \u03c0, where \u03c0 is the\nprobability of a correct answer (i.e., that \u02dcc = 1), then as \u03c0 gets large, most of the time \u02dcc = 1 and\nwe multiply our gradient estimator by 1 \u2212 \u03c0 (multiplying its variance by a factor of (1 \u2212 \u03c0)2). If\n\u02dcc = 0, then we make use of both a correct and incorrect rationale; the weights attached to these\nupdates will not be small, but if incorrect rationales are relatively rare then their contribution to the\nvariance of the gradient estimator will be correspondingly small. On the other hand, if the model\nhas not yet learned to frequently generate good rationales for the training examples, then we should\nset \u03b2 closer to 0, since in this case the signal from the incorrect rationale is less informative\u2014in\nAppendix C.1 we show that the variance of gradient estimators based on incorrect rationales depends\nstrongly on the model\u2019s accuracy \u03c0. In Appendix B, we show that choosing \u03b2 = \u03c0 is in fact optimal\nup to O((1 \u2212 \u03c0)2) terms, and that the variance of the resulting estimator is proportional to 1 \u2212 \u03c0.\nEstimating \u03b2.\nFor each example xm, ym, we need to compute a \u03b2m \u2248 E[\u02dccm] in a way that ensures\nthat \u03b2m is independent of \u2207\u03b8 log p\u03b8(\u02dczm | xm). We assume that E[\u02dccm] \u2248\n1\nM\nP\nm E[\u02dccm] (i.e., that\nthe per-example acceptance probability is close to the average acceptance probability across the\nminibatch6), and compute the leave-one-out estimate \u03b2m =\nP\nm\u2032\u0338=m c\u2032\nm\u2032 \u02dccm\u2032\nP\nm\u2032\u0338=m c\u2032\nm\u2032\n, where c\u2032\nm := c(z\u2032\nm, y).\n5There may be some examples (especially early in training) for which we have not yet generated any correct\nrationales. We omit these examples from our gradient estimate, since they have likelihood 0 and therefore cannot\nbe representative samples from the posterior.\n6We also tried keeping a running estimate of the average acceptance probability per example, but we did not\nfind that this more complex scheme provided any empirical advantage.\n5\nWe restrict the estimate to consider only examples for which we have a correct rationale (i.e., where\nc\u2032\nm = 1), since these are the only examples that influence our gradient estimate. Leaving out \u02dccm and\nc\u2032\nm from the estimate \u03b2m ensures that \u03b2m is independent of \u02dczm.\nGradient subsampling.\nFinally, as described above, we can reduce the cost of our gradient\nestimator by using systematic resampling to select a subset of rationales. This does not affect\nthe expected value of the estimator as long as the marginal probability of selecting a rationale is\nproportional to the corresponding weight \u02dcwm, and the averaged gradient is reweighted by\nP2M\nm=1 \u02dc\nwm\nP\nm c\u2032m .\n2.2\nWhy not variational inference, reweighted wake-sleep, or rejection sampling?\nWe considered three alternatives to the MCMC-EM approach that we pursue in this paper: variational\nEM (e.g., Bishop, 2006), reweighted wake-sleep (RWS; Bornschein & Bengio, 2015; Le et al., 2019),\nand rejection sampling.\nVariational expectation-maximization is a common strategy for training latent-variable models, but\nvariational inference with discrete latent variables is challenging (e.g., Tucker et al., 2017).\nRWS is an attractive alternative that avoids high-variance score-function gradients; it proceeds by\nsampling M samples z1:M from a guide model q\u03d5(z | x, y), assigning the samples weights wm \u221d\np\u03b8(y,z|x)\nq\u03d5(z|x,y), and updating both the model parameters \u03b8 and the guide parameters \u03d5 to maximize the\nreweighted log-probabilities P\nm wm log p\u03b8(zm | x) and P\nm wm log q\u03d5(zm | x, y). Unfortunately,\nwe found that RWS training sometimes led to degenerate zero-length rationales z. Figure 1 suggests\na partial explanation: shorter sequences get higher weights, so the model and guide learn to produce\nshorter and shorter sequences until they consistently produce empty rationales.\n0\n5\n10\n15\n20\n25\n30\niteration\n20\n40\n60\n80\n100\n120\nmean q sample lengths\nweighted mean q sample lengths\nFigure 1: Example of rationale lengths shrink-\ning during RWS training. Blue line shows the\naverage number of tokens per rationale gener-\nated by the guide, orange line shows the aver-\nage number of tokens per rationale weighted\nby the rationale\u2019s importance weight.\nWhy do longer sequences tend to get lower\nweights?\nWe\ncan\nwrite\nthe\nunnormalized\nweights\nas\n\u02dcwm\n=\nc(y, zm) p\u03b8(zm|x)\nq\u03d5(zm|x,y)\n=\nc(y, zm) QTm\nt=1\np\u03b8(zm,t|x,zm,1:(t\u22121))\nq\u03d5(zm,t|x,y,zm,1:(t\u22121)),\nwhere\nTm\nis the length of zm and \u03f5 is added to address the\ncase where none of the samples are correct. If there\nis a mismatch between q(zm,t | x, zm,1:(t\u22121))) and\np(zm,t | x, zm,1:(t\u22121)), then\np\u03b8(zm,t|x,zm,1:(t\u22121))\nq\u03d5(zm,t|x,y,zm,1:(t\u22121))\nwill usually be less than one, with rare high-weight ex-\nceptions that ensure that Eq[p(z | x)/q(z | x)] = 1.\nIf these exceptions are rare enough to not typically\nappear in a sample of M sequences z1:M, then the\nnormalized weights w1:M =\n\u02dc\nw1:M\nP\nm \u02dc\nwm will tend to\nassign higher mass to shorter sequences unless those\nshorter sequences are much less likely to be correct.\nWith careful initialization and learning-rate tuning,\nwe could sometimes get RWS to avoid this problem\nof empty rationales. But this led to a new problem: the guide q\u03d5(z | x, y) learned to closely mimic the\nprior p(z | x) until the very end of the rationale, and then simply paste in the correct answer whether\nor not it had anything to do with the rationale up to that point (cf. Turpin et al., 2023). Figure 5 in\nAppendix E shows a representative example in which the guide model ignores the answer it arrived at\nthrough incorrect reasoning and pastes in the correct answer.\nQuantitatively, denoting by t the index of the token at which the \u201cfinal answer\u201d section of the rationale\nbegins, in one run we found that the average KL between q(z1:t | x, y) and p(z1:t | x) was about 0.61\nnats, while the conditional KL between q(z(t+1):T | x, y, z1:t) and p(z(t+1):T | x, z1:t) was about\n42.5 nats, confirming that the guide was not \u201creasoning backwards\u201d, just copying the correct answer.\n6\nFinally, we considered a rejection-sampling7 scheme in which we sample K proposal rationales z1:K\nfrom p(z | x), and average the gradients from those rationales that lead to correct answers. We will\npresent the quantitative results in Section 4; our main finding is that, while this scheme can work, it\nrequires reducing the minibatch size by a factor of K to keep the per-iteration cost constant compared\nto TRICE, which in turn leads to slower convergence and/or worse final results.\n3\nRelated Work\nA number of methods have proposed rationale generation for problem-solving tasks in neural sequence\nmodels, including both fully supervised and few-shot approaches (Wei et al., 2022; Nye et al., 2021;\nKojima et al., 2022; Rajani et al., 2019; Shwartz et al., 2020; Wang et al., 2022b; Zhou et al., 2022;\nCreswell et al., 2022; Ye & Durrett, 2023). Particularly relevant to our approach is self-consistent\nchain-of-thought (Wang et al., 2022b), because this can be approximately viewed as marginalizing\nover rationales at test time. This technique has been successfully applied for a range of quantitative\nreasoning tasks (Lewkowycz et al., 2022). There is relatively much less work that does imputation or\naveraging over rationales at training time; perhaps the main instance is STaR (Zelikman et al., 2022),\nwhich we discuss in Section 3.1.\nDohan et al. (2022) present a position paper which advocates representing a composition of language\nmodel interactions via probabilistic programming. Our treatment of rationales as latent variables\nis inspired by that work. Lievin (2022) offers another example of interpreting LLMs with CoT as\nlatent-variable models.\nVariational inference (e.g., Kingma & Welling, 2013) and wake-sleep methods (e.g., Bornschein\n& Bengio, 2015) are workhorses of the latent-variable-modeling community, but as we discuss in\nSection 2.2 we found the bias of these methods to cause serious problems. MCMC-EM is a less-\ncommon strategy these days, although a version of it based on Gibbs sampling (Geman & Geman,\n1984) it has been widely applied to training undirected graphical models (Tieleman, 2008). TRICE\ncan also be cast as an instance of Markovian score climbing (Naesseth et al., 2020).\nReAct (Yao et al., 2023) demonstrated that injecting reasoning into an RL-style observe-and-act loop\nsignificantly increases performance. This approach was extended in Reflexion (Shinn et al., 2023),\nwhere an agent can conditionally reflect on an RL trajectory, augmenting the resulting examples\nwhich can be used as few-shot examples in subsequent rollouts. These approaches reported significant\nimprovements on their respective evaluation tasks but still rely on the model being able to produce\nuseful and actionable feedback through pure few-shot prompting, whereas our method actively tunes\nthe model to produce thoughts amenable to the task.\nRecent work on tool-use within language models also works via imputation, inferring where to insert\ncalls to tools (Parisi et al., 2022; Schick et al., 2023). Their loss functions are similar in spirit to ours,\nfiltering out trajectories which do not lead to valid answers. In this paper, we have treated rationales\nas latent variables; one could also treat tool-use as a latent variable.\n3.1\nSelf-Taught Reasoner\nThe most closely related work is the self-taught reasoner (STaR; Zelikman et al., 2022). Besides the\narguments in their derivations, there are three significant differences between TRICE and STaR. First,\nSTaR uses greedy decoding, which reduces the diversity of the rationales it trains on. The authors\nmade this choice to reduce the danger of the model getting the right answer despite having a bad\nrationale. While we do find that our procedure sometimes generates correct answers for the wrong\nreasons, this did not seem to stand in the way of the model improving on most tasks. One reason may\nbe that our base models are more powerful than the 6B-parameter GPT-J model used in the STaR\npaper, so they are more likely to generate good rationales from the beginning.\nA second difference is that TRICE resamples rationales every iteration, so it are less likely to overfit\nto any particular rationale. STaR has an inner loop that runs many training iterations on a single\nset of rationales, meaning it uses stale rationales to estimate the gradient of the marginal likelihood.\n7We also considered optimizing an importance-weighted bound (Burda et al., 2015) using the prior p(z | x)\nas a proposal distribution, but instead opted for a simple rejection sampling scheme since this is less biased and\nequally feasible in our setting.\n7\nIn our experiments, we observed that this leads to the model effectively memorizing a fixed set of\nrationales for the training set. Once this happens, the greedy decoding procedure will almost certainly\nreproduce exactly the same rationales at the beginning of the next outer loop. If these rationales all\nlead to the correct answer, and STaR has a rationale for each question, then this is a global optimum\nof the marginal likelihood on the training set! But empirically, STaR often does not find a good\nrationale for each question, and so it ignores some fraction of the training set (see Section 4).\nThis tendency to ignore the most difficult questions in the training set follows from STaR\u2019s derivation\nas an approximate policy-gradient algorithm trying to directly minimize the 0-1 loss Ep[1\u2212c(z, y)] =\n1 \u2212 p\u03b8(y | x). The derivative of this marginal likelihood is p\u03b8(y | x)\u2207\u03b8 log p\u03b8(y | x), that is, it is the\nderivative of the marginal log-likelihood (which TRICE tries to maximize) weighted by p\u03b8(y | x).\nThis weighting causes difficult examples to contribute little to the gradient used to update the model,\nso the model may \u201cgive up\u201d on questions that it cannot yet solve. This is one argument for trying to\nmaximize log-likelihoods instead of likelihoods.\nA final, minor difference is that when STaR updates its rationales, it may replace a rationale from\nthe model p(z | x) with a rationale from a surrogate q\u03b8(z | x, y). As the model memorizes a set of\ncorrect rationales for the training set, STaR becomes less likely to fall back on the surrogate, but this\nchoice could affect early training dynamics.\n4\nExperiments\nWe evaluate TRICE on the GSM8K (Cobbe et al., 2021) dataset and the 27 BigBench-Hard (BBH)\ntasks (Suzgun et al., 2022b) using the medium-size PaLM 2-M (Anil et al., 2023) Transformer-based\nLLM (Vaswani et al., 2017). For the BBH experiments, we used the Flan instruction-tuned (Chung\net al., 2022) version of PaLM 2; for GSM8K, we used the base PaLM 2 model, since GSM8K is\nincluded in the Flan training datasets. All experiments were run on TPU v4 and v5e chips (Jouppi\net al., 2023). Examples of generated rationales can be found in Appendix E.\nRather than fine-tune the model weights, we use prompt tuning (Lester et al., 2021); we prepend a\nsequence of embedding vectors \u03b8 (a \u201csoft prompt\u201d) to the embeddings corresponding to the tokenized\nCoT prompt used to condition the model. Prompt tuning can achieve similar accuracy gains to\nfull fine-tuning, but using a small fraction of the parameters. We initialize the soft prompt with\nthe embedding sequence obtained from a series of three (for BBH) or five (for GSM8K) exemplar\nCoT prompts, each of the form \u201cQuestion:\n<QUESTION>\\nAnswer:\nLet\u2019s think step by\nstep.\\n<RATIONALE>\u201d. We consider two initialization schemes: one where we use the standard\nfew-shot CoT prompts that are provided with BBH, and one where we try to bootstrap a few-shot CoT\nprompt by sampling random questions from the training set, generating random rationales from the\nbase model, and picking three or five examples where the random rationales lead to correct answers.\nThe first scheme can be seen as a way of fine-tuning a good initial few-shot prompt, but it does require\na small amount of detailed CoT supervision, while the second scheme only requires label supervision.\nOn each BBH task, we split the examples into 60% train and 40% test sets. For all but three\ntasks, this is 150 training and 100 test examples. For GSM8K, we use the standard 7473-example\ntraining set and 1319-example test set. We evaluate CoT models\u2019 accuracy in two ways: first, using\ngreedy (temperature-0) decoding, and second, using \u201cself-consistency\u201d (Wang et al., 2022b). In\nself-consistency evaluation, we draw 40 samples and check whether the most common answer is\ncorrect; this is a plug-in estimator for the prediction arg maxy p(y | x) that minimizes 0-1 loss under\nthe model (although this is not how Wang et al. (2022b) originally motivated the procedure).\nWe compare against four baseline prompt-tuning methods: direct prompt tuning, CoT prompt tuning,\nrejection sampling, and STaR (Zelikman et al., 2022). All methods are evaluated against the same\nvalidation sets, and use the same training labels, few-shot prompts (except for direct tuning, where we\nonly use question-answer pairs), and initialization strategies as appropriate. Details for each method\nand its corresponding experimental hyperparameters can be found in Appendix F.\nTable 1 and Table 2 summarize the results; more detailed task-by-task BBH summaries are in\nAppendix D. Even with no human-generated exemplar rationales, TRICE is able to learn to generate\nrationales that lead to the correct answer. TRICE also outperforms a model trained directly on\nhuman-generated rationales on GSM8K (cf. Uesato et al., 2022), perhaps because the cross-entropy\n8\nPrompt-Tuning\nStrategy\nInitialization\nGreedy-Decoding\nAcc. (%)\nSelf-Consistency\nAcc. (%)\n% Valid\nRationales\nSTaR\nBootstrapped\n3-shot\nQ-R-A\n62.0\n62.1\n91.6\nRejection Sampling\n64.6\n65.3\n-\nTRICE without CV\n67.8\n68.0\n98.7\nTRICE with CV\n72.8\n73.1\n98.8\nDirect Prompt Tuning\n3-shot Q-A\n70.4\n-\n-\nTRICE without CV\n3-shot Q-R-A\n73.4\n75.2\n98.2\nTRICE with CV\n76.7\n77.6\n98.6\nTable 1: Average accuracies (columns 3 and 4) and fraction of training examples for which we can\ngenerate correct rationales (column 5) across the 27 BIG-Bench Hard (BBH) tasks. All methods but\ndirect prompt tuning use CoT prompting. All trainable prompts are initialized with an embedding\nsequence obtained from a few-shot prompt containing either example question-answer pairs (\u201cQ-A\u201d)\nor example question-rationale-answer triples (\u201cQ-R-A\u201d). For direct prompt tuning, the Q-A pairs\ncome from the training set. For TRICE, we use either the three Q-R-A triples provided with BBH\n(bottom two rows) or bootstrap a set of rationales as described in the text. For STaR and rejection\nsampling, we only evaluate on bootstrapped initializations.\nPrompt-Tuning\nStrategy\nGreedy-Decoding\nAcc. (%)\nSelf-Consistency\nAcc. (%)\n% Valid\nRationales\nSTaR\n53.5\n60.1\n80.2\nCoT Prompt Tuning\n58.6\n73.8\n-\nRejection Sampling\n77.9\n87.0\n-\nDirect Prompt Tuning\n19.4\n-\n-\nTRICE without CV\n72.8\n81.5\n98.9\nTRICE with CV\n74.7\n82.3\n98.8\nTRICE with CV (not bootstrapped)\n77.7\n86.6\n98.4\nTable 2:\nAverage accuracies (columns 2 and 3) and fraction of training examples for which we\ncan generate correct rationales (column 4) on GSM8K. Direct prompt tuning is initialized with an\nembedding sequence obtained from a few-shot prompt containing example question-answer pairs\n(\u201cQ-A\u201d). All remaining prompt-tuning methods are initialized with an embedding sequence obtained\nfrom a few-shot prompt containing example question-rationale-answer triples (\u201cQ-R-A\u201d) obtained\nrandomly from the GSM8K training set or bootstrapped as described in the text.\nloss used in supervised fine-tuning may place more weight on style than substance; it takes far more\nbits to encode how one expresses a chain of reasoning than it does to encode the reasons themselves.\nInitializing the soft prompt with a human-generated 3-shot exemplar question-rationale-answer\nprompt slightly improves performance on BBH, as does evaluating with self-consistency. By the\nend of training, TRICE has managed to generate at least one valid rationale for almost all training\nexamples, while STaR fails to generate valid rationales for almost 10% of training examples. Unlike\nin the experiments done on Commonsense QA (Talmor et al., 2019) by Zelikman et al. (2022), STaR\ndoes not outperform the direct-prompted prompt-tuned model on BBH. This may be because each\nBBH task includes relatively little training data (150 examples as opposed to CommonsenseQA\u2019s\n9,741), and so in its inner loop STaR overfits to its relatively small set of bootstrapped rationales.\nTRICE, on the other hand, can overfit to the small set of questions but at least has a chance to generate\na somewhat diverse set of rationales from those questions.\nOne piece of evidence for this overfitting-rationales hypothesis is that on the final step of its final\ninner loop, STaR (with bootstrapped initialization) achieves a training sequence-level (not per-token)\ncross-entropy loss of less than 0.06 on all tasks, and of less than 0.01 on 19 out of 27 tasks. This\nimplies that it has learned to exactly reproduce a single set of rationales with very high probability,\nwhich makes it very likely that it will generate those same rationales in the next iteration.\nFigure 2 compares estimates for GSM8K of the average training marginal likelihood (i.e., how often\na proposal is accepted) and the validation accuracy with greedy decoding as a function of number of\n9\n0\n1000\n2000\n3000\n4000\n5000\n0.4\n0.6\n0.8\nTrain Accuracy\n0\n1000\n2000\n3000\n4000\n5000\n0.5\n0.6\n0.7\nTest Accuracy\nstep\nvalue\nMethod\nRS\nTRICE CV\nTRICE no CV\nFigure 2: Time-varying estimates (with loess smoothers) of average training-set accuracy p(y | x)\nand greedy-decoding validation-set accuracy for TRICE with and without subsampled control-variate\ngradient estimator (\u201cTRICE CV\u201d and \u201cTRICE no CV\u201d respectively) and four-particle rejection\nsampling (\u201cRS\u201d) on GSM8K.\ntraining steps8 for rejection sampling and for TRICE with and without the control-variate scheme.\nThe control-variate scheme improves average convergence speed, particularly towards the end of\ntraining as the probability of generating correct answers on the training set increases. Both versions\nof TRICE converge to high training accuracy much faster than rejection sampling.\n5\nDiscussion\nWe proposed TRICE, a method for tuning LLMs to be better at solving question-answering tasks\nusing chain-of-thought (CoT) prompting. By framing the CoT-prompted LLM as a latent-variable\nmodel, we were able to derive a principled and effective fine-tuning method. When applied to GSM8K\nand BIG-Bench Hard (BBH) tasks, TRICE outperforms three strong baselines: direct prompt-tuning,\nSTaR, and rejection sampling. While we derived TRICE in the context of CoT question-answering,\nits basic MCMC-EM strategy could be employed more broadly, for example to tool-use problems.\nLimitations:\nWe only evaluated TRICE with prompt-tuning on a medium-size LLM; it may be that\nit behaves differently on smaller models, larger models, or when using other fine-tuning strategies.\nTRICE is a gradient-based tuning algorithm, but many of the most capable LLMs are proprietary, and\ntheir owners often do not provide any public mechanism for gradient-based fine-tuning. This makes it\nhard to evaluate how well TRICE would work when used with, say, GPT-4 (OpenAI, 2023). Finally,\nour quantitative evaluations focused on whether the LLM could produce the right answer; we did not\nformally evaluate the quality of the reasoning in the rationales themselves (cf. Uesato et al., 2022).\nBroader Impacts:\nThis work aims to improve the capabilities of LLMs by making them better\nable to answer questions accurately and transparently. However, more-capable LLMs may be used\nin malicious or unsafe ways, fine-tuning on uncurated question-answering datasets may introduce\nbiases into the models, and more widely used LLMs will contribute a larger carbon footprint.\nRationales may make it easier for motivated users to judge the trustworthiness of LLM outputs. But\nmany users may not read and critique an LLM\u2019s rationales, taking the mere existence of a rationale as\nevidence of truth. If chain-of-thought rationales promote uncritical trust, they could lead to harm.\n8We set the cost per iteration of rejection sampling and TRICE with and without the control-variate scheme to\nbe directly comparable: for rejection sampling, we reduce the minibatch size by a factor of four and generate four\ntimes as many proposals per example; for TRICE with the control-variate scheme, we set the gradient minibatch\nsize L equal to the number of examples per minibatch M (note that this does still involve subsampling, since\neach example could potentially contribute both a correct and an incorrect rationale to the gradient estimate).\n10\nAcknowledgements:\nWe appreciate Daniel Freeman and Enrique Piqueras\u2019 contributions to the\ninfrastructure that we used in our experiments. We thank Kevin Murphy, Ben Lee, Brian Patton, and\nJascha Sohl-Dickstein for helpful discussions.\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey,\nP., Chen, Z., et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\nBishop, C. M. Pattern recognition and machine learning. Springer, 2006.\nBornschein, J. and Bengio, Y. Reweighted wake-sleep, 2015.\nBurda, Y., Grosse, R., and Salakhutdinov, R. Importance weighted autoencoders. arXiv preprint\narXiv:1509.00519, 2015.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M.,\nBrahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416,\n2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton,\nJ., Nakano, R., Hesse, C., and Schulman, J. Training verifiers to solve math word problems. arXiv\npreprint arXiv:2110.14168, 2021.\nCover, T. M. Elements of information theory. John Wiley & Sons, 1999.\nCreswell, A., Shanahan, M., and Higgins, I. Selection-Inference: Exploiting large language models\nfor interpretable logical reasoning. arXiv preprint arXiv:2205.09712, May 2022.\nDohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D., Lopes, R. G., Wu, Y., Michalewski, H.,\nSaurous, R. A., Sohl-dickstein, J., Murphy, K., and Sutton, C. Language model cascades, 2022.\nGeman, S. and Geman, D. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of\nimages. IEEE Transactions on pattern analysis and machine intelligence, 6:721\u2013741, 1984.\nHastings, W. K.\nMonte Carlo sampling methods using Markov chains and their applications.\nBiometrika, 57(1):97\u2013109, 04 1970. ISSN 0006-3444. doi: 10.1093/biomet/57.1.97. URL\nhttps://doi.org/10.1093/biomet/57.1.97.\nHewitt, L. B., Le, T. A., and Tenenbaum, J. B. Learning to learn generative programs with memoised\nwake-sleep. In Uncertainty in Artificial Intelligence, 2020.\nHol, J. D., Schon, T. B., and Gustafsson, F. On resampling algorithms for particle filters. In 2006\nIEEE Nonlinear Statistical Signal Processing Workshop, pp. 79\u201382, 2006. doi: 10.1109/NSSPW.\n2006.4378824.\nJouppi, N., Kurian, G., Li, S., Ma, P., Nagarajan, R., Nai, L., Patil, N., Subramanian, S., Swing, A.,\nTowles, B., et al. Tpu v4: An optically reconfigurable supercomputer for machine learning with\nhardware support for embeddings. In Proceedings of the 50th Annual International Symposium on\nComputer Architecture, pp. 1\u201314, 2023.\nKingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In Bengio, Y. and LeCun,\nY. (eds.), 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA,\nUSA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.\n6980.\nKingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are Zero-Shot\nreasoners. arXiv preprint arXiv:2205.11916, May 2022.\nLe, T. A., Kosiorek, A. R., Siddharth, N., Teh, Y. W., and Wood, F. Revisiting reweighted wake-sleep\nfor models with stochastic control flow, 2019.\n11\nLester, B., Al-Rfou, R., and Constant, N. The power of scale for parameter-efficient prompt tuning.\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,\npp. 3045\u20133059, 2021.\nLewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V., Slone, A.,\nAnil, C., Schlag, I., Gutman-Solo, T., Wu, Y., Neyshabur, B., Gur-Ari, G., and Misra, V. Solving\nquantitative reasoning problems with language models, 2022.\nLievin, V. Deep Latent Variable Models for Natural Language Processing. PhD thesis, 2022.\nLoshchilov, I. and Hutter, F. SGDR: Stochastic gradient descent with warm restarts. In International\nConference on Learning Representations, 2017. URL https://openreview.net/forum?id=\nSkq89Scxx.\nMurphy, K. P. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL probml.ai.\nMurray, I. and Salakhutdinov, R. Notes on the kl-divergence between a markov chain and its\nequilibrium distribution. preprint, 2008.\nNaesseth, C., Lindsten, F., and Blei, D. Markovian score climbing: Variational inference with kl (p||\nq). Advances in Neural Information Processing Systems, 33:15499\u201315510, 2020.\nNielsen, S. F. The stochastic EM algorithm: Estimation and asymptotic results. Bernoulli, 6(3):\n457\u2013489, June 2000.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D.,\nLewkowycz, A., Bosma, M., Luan, D., Sutton, C., and Odena, A. Show your work: Scratchpads\nfor intermediate computation with language models. arXiv preprint arXiv:2112.00114, November\n2021.\nOpenAI. Gpt-4 technical report, 2023.\nOwen, A. and Zhou, Y. Safe and effective importance sampling. Journal of the American Statistical\nAssociation, 95(449):135\u2013143, 2000.\nParisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented language models, 2022.\nRajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models\nfor commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, pp. 4932\u20134942, Florence, Italy, July 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/P19-1487. URL https://aclanthology.org/P19-1487.\nRoeder, G., Wu, Y., and Duvenaud, D. K. Sticking the landing: Simple, lower-variance gradient\nestimators for variational inference. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus,\nR., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_\nfiles/paper/2017/file/e91068fff3d7fa1594dfdf3b4308433a-Paper.pdf.\nSchick, T., Dwivedi-Yu, J., Dess\u00ec, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., and\nScialom, T. Toolformer: Language models can teach themselves to use tools, 2023.\nShinn, N., Labash, B., and Gopinath, A. Reflexion: an autonomous agent with dynamic memory and\nself-reflection, 2023.\nShwartz, V., West, P., Le Bras, R., Bhagavatula, C., and Choi, Y.\nUnsupervised common-\nsense question answering with self-talk. In Proceedings of the 2020 Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP), pp. 4615\u20134629, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.373. URL\nhttps://aclanthology.org/2020.emnlp-main.373.\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V.,\nChi, E. H., Zhou, D., and Wei, J. Challenging BIG-Bench tasks and whether Chain-of-Thought\ncan solve them. \"arXiv preprint arXiv:2210.09261\", October 2022a.\n12\nSuzgun, M., Scales, N., Sch\u00e4rli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V.,\nChi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022b.\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. CommonsenseQA: A question answering challenge\ntargeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pp. 4149\u20134158, Minneapolis, Minnesota, June 2019. Association for\nComputational Linguistics. doi: 10.18653/v1/N19-1421. URL https://aclanthology.org/\nN19-1421.\nTieleman, T. Training restricted boltzmann machines using approximations to the likelihood gradient.\nIn Proceedings of the 25th international conference on Machine learning, pp. 1064\u20131071, 2008.\nTierney, L. Markov chains for exploring posterior distributions. Annals of Statistics, 22(4):1701\u20131728,\nDecember 1994.\nTucker, G., Mnih, A., Maddison, C. J., Lawson, J., and Sohl-Dickstein, J. Rebar: Low-variance,\nunbiased gradient estimates for discrete latent variable models. Advances in Neural Information\nProcessing Systems, 30, 2017.\nTurpin, M., Michael, J., Perez, E., and Bowman, S. R. Language models don\u2019t always say what they\nthink: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388,\n2023.\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and\nHiggins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint\narXiv:2211.14275, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and\nPolosukhin, I. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D.\nSelf-Consistency improves chain of thought reasoning in language models.\narXiv preprint\narXiv:2203.11171, March 2022a.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought\nprompting elicits reasoning in large language models. \"arXiv preprint arXiv:2201.11903\", January\n2022.\nYao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. React: Synergizing\nreasoning and acting in language models, 2023.\nYe, X. and Durrett, G. Explanation selection using unlabeled data for In-Context learning. \"arXiv\npreprint arXiv:2302.04813\", February 2023.\nZelikman, E., Wu, Y., and Goodman, N. D. Star: Bootstrapping reasoning with reasoning. arXiv\npreprint arXiv:2203.14465, 2022.\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q.,\nand Chi, E. Least-to-Most prompting enables complex reasoning in large language models. \"arXiv\npreprint arXiv:2205.10625\", May 2022.\n13\nSupplemental Material for \u201cTraining Chain-of-Thought via\nLatent-Variable Inference\u201d\nA\nGeneralizing TRICE to Nondeterministic Likelihood Models\nTo apply TRICE beyond question-answering problems, we might want to use a nondeterministic\nlikelihood model. For example, our desired output y might be a summary of a text x, and z might\nbe a scratchpad or outline. In situations like this, there might be many y\u2019s that are appropriate for a\ngiven x and z. So from a modeling perspective, it could make sense to make p(y | x, z) have nonzero\nentropy. But there is also a computational reason to prefer such a model: as the number of reasonable\nvalues that y could take given x increases, the probability of sampling the precise y that was observed\ngoes down at a rate that might be exponential in the size of the output space.\nFortunately, we can easily extend TRICE to accommodate nondeterministic likelihoods. The differ-\nences are:\n1. As discussed in Section 2.1, the acceptance probability for an independence sampler where\nthe proposals are generated from p(z | x) is \u03b1(\u02dcz | z) = min{1, p(y|\u02dcz,x)\np(y|z,x)}. So instead of\ndeterministically accepting correct proposed latents \u02dcz, we update the memory probabilisti-\ncally, always accepting proposals that make the observed y more likely, but sometimes also\naccepting proposals that make y a little less likely.\n2. The control-variate weights \u03b2m are now a function of the acceptance probabilities \u03b1: \u03b2m =\n1\nM\u22121\nP\nm\u2032\u0338=m \u03b1(\u02dczm\u2032 | zim\u2032 ).\nThe value of the control variate in this setting may be less than it is in the deterministic-likelihood\nsetting. Even if we learn a model that consistently produces good latents z (in the sense that they\nlead to valid outputs y), this does not guarantee that it will consistently generate latents that are\nconsistent with the particular y that was observed. For example, there might be multiple reasonable\nways to outline a long text, some of which lead to different summary paragraphs. In this scenario, the\nacceptance probability may not converge to something close to 1, and the variance-reduction effect\nfrom the control variate will be modest.\nB\nDerivation of the Control Variate Scaling Heuristic\nOur analysis will focus on the variance of the kth element \u02c6gk of our gradient estimator. To minimize\nclutter, we suppress subscript indices and define g\u2032 = \u2207\u03b8k log p(z\u2032 | x), \u02dcg = \u2207\u03b8k log p(\u02dcz | x),\nand g = \u2207\u03b8k log p(z | x), where z\u2032 is the updated rationale, \u02dcz is the proposed rationale, and z\nis the previous rationale in memory. Note that g\u2032 = \u02dcg if the proposal \u02dcz is accepted, and g\u2032 = g\nif \u02dcz is rejected. We assume that the previous rationale z is a sample from p(z | x, y). We also\ndefine the shorthands g+ = E[\u02dcg | c(\u02dcz, y) = 1], g\u2212 = E[\u02dcg | c(\u02dcz, y) = 0], v = Var(\u02dcg) = E[\u02dcg2],\nv+ = Var(\u02dcg | c(\u02dcz, y) = 1), v\u2212 = Var(\u02dcg | c(\u02dcz, y) = 0). Finally, we define the probability of\naccepting a proposal \u03c0 = E[c(\u02dcz, y)].\nOur gradient estimator is\n\u02c6g = g\u2032 \u2212 \u03b2\u02dcg;\nE[\u02c6g] = g+.\n(6)\nThe variance of \u02c6g is\nE[(g\u2032 \u2212 g+ \u2212 \u03b2\u02dcg)2] = E[(g\u2032 \u2212 g+)2] + \u03b22E[\u02dcg2] \u2212 2\u03b2E[(g\u2032 \u2212 g+)\u02dcg]\n(7)\n= v+ + \u03b22v \u2212 2\u03b2E[g\u2032\u02dcg].\n(8)\nIn Equation (8) we use the fact that E[\u02dcg] = 0 and g+ is nonrandom and therefore independent of\n\u02dcg. Breaking out the cases where c(\u02dcz, y) = 0 (so g\u2032 = g) and c(\u02dcz, y) = 1 (so g\u2032 = \u02dcg), the rightmost\nexpectation is\nE[g\u2032\u02dcg] = \u03c0E[\u02dcg2 | c(\u02dcz, y) = 1] + (1 \u2212 \u03c0)E[g\u02dcg | c(\u02dcz, y) = 0]\n(9)\n= \u03c0(v+ + g2\n+) + (1 \u2212 \u03c0)g+g\u2212.\n(10)\nWe can simplify the term on the right. Since 0 = E[\u02dcg] = \u03c0g+ + (1 \u2212 \u03c0)g\u2212, (1 \u2212 \u03c0)g+g\u2212 = \u2212\u03c0g2\n+.\nPlugging this into Equation (10), we have\nE[g\u2032\u02dcg] = \u03c0(v+ + g2\n+) \u2212 \u03c0g2\n+ = \u03c0v+.\n(11)\n14\nSo the variance of \u02c6g simplifies to\nVar(\u02c6g) = (1 \u2212 2\u03b2\u03c0)v+ + \u03b22v.\n(12)\nTaking the derivative with respect to \u03b2 shows that this is minimized when\n2(\u03b2v \u2212 \u03c0v+) = 0;\n\u03b2\u22c6 = \u03c0 v+\nv .\n(13)\nPlugging this back into Equation (12) gives the optimal variance v\u22c6:\nv\u22c6 = v+ + \u03c02 v2\n+\nv \u2212 2\u03c02 v2\n+\nv = v+\n\u0010\n1 \u2212 \u03c02 v+\nv\n\u0011\n.\n(14)\nWe can expand v by considering the case where c(\u02dcz, y) = 1 (which happens with probability \u03c0 and\nthe case where c(\u02dcz, y) = 0 (which happens with probability 1 \u2212 \u03c0):\nv = E[\u02dcg2] = \u03c0v+ + \u03c0g2\n+ + (1 \u2212 \u03c0)v\u2212 + (1 \u2212 \u03c0)g2\n\u2212\n= \u03c0v+ \u2212 (1 \u2212 \u03c0)g+g\u2212 + (1 \u2212 \u03c0)v\u2212 \u2212 \u03c0g+g\u2212\n= \u03c0v+ + (1 \u2212 \u03c0)v\u2212 \u2212 g+g\u2212\n= \u03c0v+ + (1 \u2212 \u03c0)v\u2212 + 1 \u2212 \u03c0\n\u03c0\ng2\n\u2212\n= \u03c0v+ + (1 \u2212 \u03c0)v\u2212 + (1 \u2212 \u03c0)(2 \u2212 \u03c0)g2\n\u2212 + O((1 \u2212 \u03c0)3)\n= \u03c0v+ + (1 \u2212 \u03c0)v\u2212 + (1 \u2212 \u03c0)(1 + 1 \u2212 \u03c0)g2\n\u2212 + O((1 \u2212 \u03c0)3)\n= \u03c0v+ + (1 \u2212 \u03c0)(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2),\n(15)\nwhere in the second line we again use the fact that \u03c0g+ = \u2212(1 \u2212 \u03c0)g\u2212, and in the third-to-last line\nwe approximate 1\n\u03c0 with the first-order Taylor approximation 1\n\u03c0 = 2 \u2212 \u03c0 + O((1 \u2212 \u03c0)2). Thus, we\ncan write\n\u03c0 v+\nv =\n\u03c0v+\n\u03c0v+ + (1 \u2212 \u03c0)(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2)\n=\n\u0012\n1 + 1 \u2212 \u03c0\n\u03c0v+\n(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2)\n\u0013\u22121\n= 1 \u2212 1 \u2212 \u03c0\n\u03c0v+\n(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2).\n(16)\nSo the optimal variance v\u22c6 is\nv\u22c6 = v+\n\u0010\n1 \u2212 \u03c02 v+\nv\n\u0011\n= v+\n\u0012\n1 \u2212 \u03c0 + 1 \u2212 \u03c0\nv+\n(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2)\n\u0013\n= (1 \u2212 \u03c0)(v+ + v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2)).\n(17)\nBy contrast, plugging our heuristic value of \u03b2 = \u03c0 into Equation (12) gives the suboptimal variance\nv\u03c0:\nv\u03c0 = (1 \u2212 2\u03c02)v+ + \u03c02v\n= (1 \u2212 2\u03c02)v+ + \u03c02(\u03c0v+ + (1 \u2212 \u03c0)(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2))\n= (1 \u2212 2\u03c02 + \u03c03)v+ + \u03c02(1 \u2212 \u03c0)(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2)\n= (1 \u2212 \u03c0)(v+ + v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2),\n(18)\nwhere we use the approximation \u03c0k = (1 \u2212 (1 \u2212 \u03c0))k = 1 \u2212 k(1 \u2212 \u03c0) + O((1 \u2212 \u03c0)2) to simplify\nthe \u03c02 and \u03c03 terms. Thus, we conclude that v\u22c6 and v\u03c0 are the same up to O((1 \u2212 \u03c0)2), and so as\nthe probability \u03c0 of getting the correct answer increases, the suboptimality of setting \u03b2 = \u03c0 goes\ndown faster than the variance does.\n15\nC\nOn Gradient Estimators Based Solely on Incorrect Rationales\nWe adopt the same shorthands as in Appendix B.\nOur analysis centers on the identity\n0 = E[\u02dcg] = \u03c0g+ + (1 \u2212 \u03c0)g\u2212,\n(19)\nwhich relates the gradient we want to estimate g+ (the expected gradient given that the rationale is\ncorrect) to g\u2212 (the expected gradient given that the rationale is incorrect).\nC.1\nVariance of incorrect-rationale gradient estimators\nEquation (19) implies that\ng+ = \u22121 \u2212 \u03c0\n\u03c0\ng\u2212.\n(20)\nDeferring for the moment the difficulty in estimating \u03c0\u22121 (see Appendix C.2 below), we can consider\nthe variance of an estimator based on the right hand side of Equation (20):\nV\n\u0014\n\u22121 \u2212 c(\u02dcz, y)\n\u03c0\n\u02dcg\n\u0015\n= E\n\u0014(1 \u2212 c(\u02dcz, y))2\n\u03c02\n\u02dcg2\n\u0015\n\u2212 g2\n+\n= 1 \u2212 \u03c0\n\u03c02\n(v\u2212 + g2\n\u2212) \u2212 g2\n+\n= 1 \u2212 \u03c0\n\u03c02\n(v\u2212 + g2\n\u2212) \u2212 (1 \u2212 \u03c0)2\n\u03c02\ng2\n\u2212\n= 1 \u2212 \u03c0\n\u03c02\n(v\u2212 + \u03c0g2\n\u2212).\n(21)\nIf \u03c0 is small, then this becomes\n1 \u2212 \u03c0\n\u03c02\n(v\u2212 + \u03c0g2\n\u2212) = v\u2212\n\u03c02 + O(\u03c0\u22121),\n(22)\nso that unless the variance v\u2212 of incorrect rationales is very low, the variance of this estimator will be\nO(\u03c0\u22122), which is very high. By contrast, the variance of a gradient estimator based purely on correct\nrationales is simply v+, so unless the gradient variance for incorrect rationales is dramatically lower\nthan that for correct rationales, then if \u03c0 is small then incorrect rationales will lead to much noisier\ngradient estimates.\nOn the other hand, if 1 \u2212 \u03c0 is small, then we have\n1 \u2212 \u03c0\n\u03c02\n(v\u2212 + \u03c0g2\n\u2212) = (1 \u2212 \u03c0)(v\u2212 + g2\n\u2212) + O((1 \u2212 \u03c0)2),\n(23)\nwhich is likely a significant improvement on the variance v+ of the correct-rationale estimator; in\nparticular, it goes to zero as \u03c0 approaches 1.\nC.2\nTRICE control variate as a debiased estimator based on incorrect rationales\nEquation (20) implies that, in principle, given unbiased estimates for 1 \u2212 \u03c0 and \u03c0\u22121, we could\ncompute gradient updates purely based on rationales that fail to obtain the correct answer. Unbiased\nestimates of 1 \u2212 \u03c0 are easy to obtain; we need only compute 1 \u2212 c(\u02dcz, y). But unbiased estimates of\n\u03c0\u22121 are harder to come by.\nInstead, we can compute a biased estimator that ignores the \u03c0\u22121 term and then correct for the bias:\nE[\u2212(1 \u2212 c(\u02dcz, y))\u02dcg] = \u2212(1 \u2212 \u03c0)g\u2212\n= \u03c0g+\n= g+ \u2212 (1 \u2212 \u03c0)g+.\n(24)\nSo the expected value of the estimator \u2212(1 \u2212 c(\u02dcz, y))\u02dcg is too small by a factor of (1 \u2212 \u03c0)g+. We\ncan correct this bias by adding in an unbiased estimator of it that uses g (the gradient for a correct\nrationale from memory):\nE[\u2212(1 \u2212 c(\u02dcz, y))(\u02dcg \u2212 g)] = g+ \u2212 (1 \u2212 \u03c0)g+ + (1 \u2212 \u03c0)g+ = g+.\n(25)\nThis is precisely the gradient estimator that TRICE uses when the sampled rationale \u02dcg is incorrect\nand \u03b2 = 1. Smaller values of \u03b2 interpolate between this estimator and an estimator based purely on\ncorrect rationales.\n16\nTask\nBootstrapped 3-shot Q-R-A\n3-shot Q-R-A\n3-shot Q-A\nTRICE (no CV)\nTRICE (CV)\nSTaR\nRS-4\nTRICE (no CV)\nTRICE (CV)\nDirect Tune\nGD / SC\nGD / SC\nGD / SC\nGD / SC\nGD / SC\nGD / SC\nGD\nboolean_expressions\n94.0 / 94.0\n92.0 / 93.0\n88.0 / 89.0\n91.0 / 92.0\n89.0 / 92.0\n96.0 / 95.0\n85.0\ncausal_judgement\n57.3 / 64.0\n61.3 / 61.3\n66.7 / 62.7\n52.0 / 53.3\n57.3 / 58.7\n61.3 / 57.3\n46.7\ndate_understanding\n73.0 / 74.0\n71.0 / 73.0\n64.0 / 63.0\n64.0 / 64.0\n86.0 / 85.0\n87.0 / 86.0\n79.0\ndisambiguation_qa\n96.0 / 96.0\n97.0 / 97.0\n83.0 / 82.0\n97.0 / 97.0\n89.0 / 90.0\n97.0 / 97.0\n81.0\ndyck_languages\n90.0 / 89.0\n85.0 / 87.0\n64.0 / 65.0\n89.0 / 90.0\n51.0 / 60.0\n68.0 / 70.0\n50.0\nformal_fallacies\n49.0 / 53.0\n66.0 / 65.0\n55.0 / 53.0\n47.0 / 45.0\n52.0 / 59.0\n41.0 / 46.0\n52.0\ngeometric_shapes\n99.0 / 99.0\n92.0 / 94.0\n85.0 / 85.0\n48.0 / 48.0\n92.0 / 93.0\n88.0 / 94.0\n75.0\nhyperbaton\n96.0 / 95.0\n99.0 / 99.0\n94.0 / 94.0\n95.0 / 95.0\n98.0 / 99.0\n98.0 / 97.0\n93.0\nlogical_deduction_five_objects\n58.0 / 60.0\n66.0 / 65.0\n52.0 / 53.0\n59.0 / 59.0\n57.0 / 61.0\n53.0 / 51.0\n56.0\nlogical_deduction_seven_objects\n54.0 / 55.0\n57.0 / 57.0\n39.0 / 41.0\n53.0 / 54.0\n52.0 / 53.0\n51.0 / 52.0\n44.0\nlogical_deduction_three_objects\n86.0 / 86.0\n98.0 / 98.0\n77.0 / 77.0\n93.0 / 93.0\n89.0 / 87.0\n97.0 / 98.0\n87.0\nmovie_recommendation\n96.0 / 96.0\n94.0 / 94.0\n85.0 / 87.0\n95.0 / 95.0\n97.0 / 97.0\n97.0 / 96.0\n94.0\nmultistep_arithmetic_two\n2.0 / 2.0\n2.0 / 2.0\n6.0 / 5.0\n2.0 / 1.0\n49.0 / 60.0\n59.0 / 74.0\n49.0\nnavigate\n62.0 / 65.0\n59.0 / 60.0\n67.0 / 71.0\n61.0 / 62.0\n93.0 / 93.0\n87.0 / 87.0\n89.0\nobject_counting\n82.0 / 84.0\n82.0 / 82.0\n52.0 / 52.0\n77.0 / 78.0\n90.0 / 90.0\n94.0 / 95.0\n90.0\npenguins_in_a_table\n64.4 / 61.0\n71.2 / 74.6\n62.7 / 62.7\n59.3 / 62.7\n72.9 / 76.3\n78.0 / 83.1\n57.6\nreasoning_about_colored_objects\n76.0 / 76.0\n80.0 / 81.0\n66.0 / 67.0\n72.0 / 72.0\n78.0 / 79.0\n87.0 / 86.0\n70.0\nruin_names\n86.0 / 86.0\n94.0 / 94.0\n85.0 / 86.0\n90.0 / 89.0\n83.0 / 84.0\n92.0 / 93.0\n90.0\nsalient_translation_error_detection\n66.0 / 63.0\n68.0 / 68.0\n54.0 / 54.0\n59.0 / 64.0\n62.0 / 66.0\n73.0 / 74.0\n64.0\nsnarks\n86.1 / 86.1\n87.5 / 87.5\n84.7 / 84.7\n81.9 / 86.1\n77.8 / 79.2\n87.5 / 86.1\n84.7\nsports_understanding\n98.0 / 96.0\n100.0 / 100.0\n89.0 / 90.0\n93.0 / 95.0\n93.0 / 95.0\n99.0 / 99.0\n99.0\ntemporal_sequences\n95.0 / 95.0\n100.0 / 100.0\n85.0 / 88.0\n99.0 / 99.0\n100.0 / 100.0\n100.0 / 100.0\n100.0\ntracking_shuffled_objects_five_objects\n13.0 / 13.0\n28.0 / 26.0\n17.0 / 15.0\n17.0 / 16.0\n22.0 / 27.0\n16.0 / 17.0\n20.0\ntracking_shuffled_objects_seven_objects\n17.0 / 18.0\n11.0 / 13.0\n7.0 / 6.0\n16.0 / 17.0\n18.0 / 14.0\n23.0 / 20.0\n20.0\ntracking_shuffled_objects_three_objects\n34.0 / 35.0\n50.0 / 49.0\n44.0 / 43.0\n30.0 / 32.0\n85.0 / 84.0\n96.0 / 95.0\n74.0\nweb_of_lies\n51.0 / 47.0\n99.0 / 98.0\n48.0 / 48.0\n48.0 / 49.0\n100.0 / 100.0\n100.0 / 100.0\n100.0\nword_sorting\n50.0 / 49.0\n55.0 / 55.0\n53.0 / 53.0\n56.0 / 55.0\n49.0 / 48.0\n45.0 / 48.0\n52.0\nAverage\n67.8 / 68.0\n72.8 / 73.1\n62.0 / 62.1\n64.6 / 65.3\n73.4 / 75.2\n76.7 / 77.6\n70.4\nTable 3: BIG-Bench Hard per-task results. TRICE (CV) uses the control-variate gradient estimator\nwith subsampling; TRICE (no CV) uses the basic gradient estimator. RS-4 is rejection sampling\nwith four particles. GD stands for greedy-decoding accuracy (%) and SC stands for self-consistency\naccuracy (%).\nD\nBBH Per-Task Experimental Results\nTable 3 summarizes our experimental results for each task in BBH.\nE\nExample Rationales\nFigure 3 illustrates some examples of rationales generated by the TRICE-tuned LLM on the BBH\ntask Logical Deduction Three Objects.\nFigure 4 illustrates some examples of rationales generated by the TRICE-tuned LLM on GSM8K.\nAlthough we did find examples where the LLM got the answer right for the wrong reasons, this was\nmuch less common on GSM8K than on BBH, since the numeric output space for GSM8K is much\nlarger than that for the typical multiple-choice BBH task.\nFigure 5 shows an example where the guide model q\u03d5(z | x, y) in reweighted wake-sleep learns to\nclosely mimic the prior model p\u03b8(z | x) until the very end of the rationale, at which point it pastes in\nthe correct answer.\nF\nMethod and Template Details\nIn this section, we present more details on the methods and templates that we used in the experiments.\nTo sample from p\u03b8(z\n|\nx),\nwe prompt the LLM with the template \u201cQuestion:\n<QUESTION>\\nAnswer:\nLet\u2019s think step by step.\\n\u201d.\nWe cap the length of the gen-\nerated rationales at 1.25 times the length of the longest of the exemplar rationales used to\ninitialize the soft prompt.\nTo initialize the memory (i.e., to sample from q(z\n| x, y) in\nline 2 of Algorithm 1), on BBH we sample from the base model with a \u201cguide\u201d prompt of\nthe form \u201cQuestion:\n<QUESTION>\\nAnswer:\nThe answer is <ANSWER>.\nLet\u2019s think\nstep by step.\\n\u201d. We use the same guide prompt to generate rationalizations in STaR, but\nwith temperature 0 (see below). On GSM8K, we instead initialize the memory with samples from\np\u03b8(z | x), since we found that initializing the memory using a prompt that includes the answer led to\n17\nExample Rationales for Logical Deduction Three Objects BBH Task\nAn example where TRICE gets the answer right with a good rationale:\nQuestion:\nOn a shelf, there are three books:\na brown book, a yellow\nbook, and an orange book.\nThe yellow book is the leftmost.\nThe\norange book is to the right of the brown book.\nOptions:\n(A) The brown book is the rightmost\n(B) The yellow book is the rightmost\n(C) The orange book is the rightmost\nTRICE answer:\nBrown book is to the right of yellow book.\nAnd orange\nbook to the right of brown book.\nSo.\nthe answer is (C).\nAn example where TRICE gets the answer right with a bad rationale:\nQuestion: A fruit stand sells three fruits:\nplums, apples, and loquats.\nThe loquats are the cheapest.\nThe plums are less expensive than the\napples.\nOptions:\n(A) The plums are the second-most expensive\n(B) The apples are the second-most expensive\n(C) The loquats are the second-most expensive\nTRICE answer:\nApples cost more than plums, but less than loquat.\nSo.\nthe answer is (A).\nAn example where TRICE gets the answer wrong:\nQuestion:\nA fruit stand sells three fruits:\nmangoes, watermelons, and\nkiwis.\nThe watermelons are less expensive than the kiwis.\nThe kiwis\nare the second-most expensive.\nOptions:\n(A) The mangoes are the most expensive\n(B) The watermelons are the most expensive\n(C) The kiwis are the most expensive\nTRICE answer: The Mangoes are more expensive than the Watermelon.\nAnd\nare more expensive than kiwis.\nSo.\nthe answer is (C).\nFigure 3: Examples of rationales where TRICE gets the answer right, right but for the wrong reasons,\nand wrong.\n18\nExample Rationales for GSM8K\nAn example where TRICE gets the answer right with a good rationale:\nQuestion:\nGabrielle sells eggs.\nOn Monday she sells 5 crates of eggs.\nOn Tuesday she sells 2 times as many crates of eggs as Monday.\nOn\nWednesday she sells 2 fewer crates than Tuesday.\nOn Thursday she\nsells half as many crates of eggs as she sells on Tuesday.\nHow many\ntotal crates of eggs does she sell for the 4 days?\nTRICE answer: Monday = 5 crates\nTuesday = 2 x 5 = 10 crates\nWednesday = 10 - 2 = 8 crates\nThursday = 1/2 x 10 = 5 crates\nTotal = 5 + 10 + 8 + 5= 28 crates\n<answer>\n28\n</answer>\nAn example where TRICE gets the answer right with a bad rationale:\nQuestion:\nSamantha\u2019s last name has three fewer letters than Bobbie\u2019s\nlast name.\nIf Bobbie took two letters off her last name, she would\nhave a last name twice the length of Jamie\u2019s.\nJamie\u2019s full name is\nJamie Grey.\nHow many letters are in Samantha\u2019s last name?\nTRICE answer: Jamie\u2019s full name consists of 5 letters.\nIf Bobbie took 2 letters off her last name, she would have a last\nname twice the length of Jamie\u2019s, which is 10 letters.\nSo, Bobbie\u2019s last name has 10 letters.\nIf Bobbie\u2019s last name has 10 letters, and Samantha\u2019s last name has\nthree fewer letters than Bobbie\u2019s, Samantha\u2019s last name has 10-3 = 7\nletters.\n<answer>\n7\n</answer>\nAn example where TRICE gets the answer wrong:\nQuestion:\nJohn drives for 3 hours at a speed of 60 mph and then turns\naround because he realizes he forgot something very important at\nhome.\nHe tries to get home in 4 hours but spends the first 2 hours\nin standstill traffic.\nHe spends the next half-hour driving at a\nspeed of 30mph, before being able to drive the remaining time of the\n4 hours going at 80 mph.\nHow far is he from home at the end of those\n4 hours?\nTRICE answer:\nFirst, he drives 3 hours * 60 mph = 180 miles away from\nhome.\nThen, he turns around and spends 2 hours in traffic (0 mph)\nThen, he spends 0.5 hours at 30 mph = 15 miles\nThen, he spends 1.5 hours at 80 mph = 120 miles\nSo, at the end of the 4 hours, he is 180 - 15 + 120 = 285 miles away\nfrom home\n<answer>\n285\n</answer>\nFigure 4: Examples of rationales where TRICE gets the answer right, right but for the wrong reasons,\nand wrong.\n19\nExample Rationales for GSM8K Generated by RWS Guide\nQuestion:\nJosh decides to try flipping a house.\nHe buys a house for\n$80,000 and then puts in $50,000 in repairs.\nThis increased the\nvalue of the house by 150%.\nHow much profit did he make?\nGuide answer:\nHe bought the house for 80000 and did 50000 worth of repairs.\nSo total cost = 80000 + 50000 = 130000.\nNow, the increased value of the house is 150% of 80000 = 120000.\nSo his profit = 120000 - 130000 = -10000.\n<answer>\n70000\n</answer>\nModel answer:\nHe bought the house for 80,000\nHe spent 50,000 on repairs.\nSo total investment is 80,000 + 50,000 = 130,000\nValue of house after repairs is 150% of 80,000 = 1.5 * 80,000 =\n120,000\nSo total profit is 120,000 - 130,000 = -10,000\n<answer>\n-10000\n</answer>\nFigure 5: Example where the prompt-tuned RWS guide model pastes in the correct answer at the\nend, contradicting the rationale up to that point. The rationales generated by the guide and model are\nalmost identical up to the final answer block.\nslower convergence and worse results; it may be that including the answer in the prompt sometimes\nencourages the model to produce untrustworthy explanations (Turpin et al., 2023).\nTo evaluate the correctness c(z, y) of a rationale z given the answer y, in BBH we search the\nend of the rationale for final answers in the form \u201cthe answer is <ANSWER>.\u201d. In GSM8K,\nwe initialize the soft prompt to encourage the model to wrap its answers in \u201c<answer>\u201d and\n\u201c</answer>\u201d tags, and then extract the answer from those tags. To encourage the bootstrapped\nfew-shot examples in GSM8K to follow this template, we the following example to the CoT\nprompt:\n\u201cQuestion:\nWhat is 1 plus 1?\\nAnswer:\nLet\u2019s think step by step.\\n1\nplus 1 is 2.\\n<answer>\\n2\\n</answer>\\n\\n\u201d.\nFigure 6 and Figure 7 show the bootstrapped few-shot CoT examples that we used to initialize the\nsoft prompt in the experiments.\nTRICE.\nFor all BBH tasks, we run TRICE for 500 steps with batch size M = 32 and do not use\nsubsampling (i.e., compute L = 64 gradients per batch). We use the Adam optimizer (Kingma & Ba,\n2015) with an initial learning rate 1.0 and a cosine decay schedule (Loshchilov & Hutter, 2017) that\nreduces the learning rate by 10x over the first 450 steps. For GSM8K, we run TRICE for 5000 steps\nwith a constant learning rate of 1.0, batch size M = 128, and compute L = 128 gradients per batch.\nSTaR.\nWe use an adaptation of the STaR strategy proposed by Zelikman et al. (2022), where we do\nprompt-tuning rather than fine-tuning on all weights. The method alternates between updating its\nmemory and retuning the model from scratch on the updated memory in an inner loop. We apply\nthis procedure for 10 outer-loop steps. Following Zelikman et al. (2022), we start with 40 inner-loop\noptimization steps, increasing the number of inner-loop steps by 20% each outer-loop iteration up to\na maximum of 200 steps. If the training loss goes below 0.01 we break out of the inner loop. For\nSTaR\u2019s inner-loop optimization, we use the same prompt-tuning initialization, Adam hyperparameters\nas above, but with cosine decay from 1.0 to 0.1 over the course of each inner loop. To update the\n20\nSTaR memory, we first try generating a rationale from the prompt-tuned model by greedy decoding,\nthen if that rationale is incorrect fall back on a rationalization generated by greedy decoding from the\nsame guide model we use in TRICE to initialize the MCMC memory, and finally if neither procedure\ngenerates a valid rationale we omit the example from the memory.\nRejection Sampling.\nWe reduce mini-batch size by 4 and draw 4 rationales for each example in\nthe mini-batch. We use the same mini-batch size, train steps, and optimizer as in TRICE for all BBH\nand GSM8K experiments. In BBH, we use the initial learning rate 1.0 as in TRICE. In GSM8K, we\nuse the learning rate 0.1 because it achieved better results than learning rate 0.3, and the training\nprocedure became unstable with learning rate 1.0.\nCoT Prompt Tuning.\nTo do supervised CoT tuning, we prompt-tune the model to maximize the log-\nlikelihoods of the training rationales given questions. The BBH datasets include very few exemplar\nrationales, so we cannot apply this strategy to BBH. On GSM8K, we use the same hyperparameters\nas in TRICE except that we early-stop the algorithm after only 1000 train steps, since the model\noverfits badly when we run longer.\nDirect Prompt Tuning.\nIn this method, the model tries to guess the answer directly without\ngenerating a rationale; prompt-tuning to maximize the log-likelihood of the answers in this setup is\nstraightforward, since there is no latent rationale to integrate out. We initialize the soft prompt using\n3 examples from the training set and truncate its length to 64. The optimization procedure is carried\nout over 150 steps with batch size 16 and the same Adam hyperparameters as above, except that the\ncosine decay period is set to 150 instead of 450. We found these adjustments to the hyperparameters\nfrom different training schemes were necessary to reduce overfitting.\n21\nBootstrapped 3-shot CoT Prompt for Formal Fallacies BBH Task\nQuestion:\n\"It is not always easy to grasp who is consuming which\nproducts.\nThe following argument pertains to this question:\nFirst\nof all, being a frequent consumer of KMS shampoo is necessary for\nbeing a regular consumer of Yardley London soap.\nNext, whoever is\na regular user of Joico shampoo is not a frequent consumer of KMS\nshampoo.\nAll this entails that no regular consumer of Yardley London\nsoap is a regular user of Joico shampoo.\"\nIs the argument, given the explicitly stated premises, deductively\nvalid or invalid?\nOptions:\n- valid\n- invalid\nAnswer:\nLet\u2019s think step by step.\nRegular use of a product always entails being a frequent consumer of\nthat product.\nAlthough a Joico shampoo user might still buy bottles\nof Yardley London soap sometimes, he/she can never be a regular\nproduct of both soaps.\nSo, the answer is valid.\nQuestion:\n\"Here comes a perfectly valid argument:\nTo start with,\nwhoever is an expert of BSC Young Boys is not an ex-fan of Real\nBetis Balompi\u00e9.\nNow, whoever is a friend of FC Dynamo Kyiv is not\nan expert of BSC Young Boys.\nHence, no friend of FC Dynamo Kyiv is\nan ex-fan of Real Betis Balompi\u00e9.\"\nIs the argument, given the explicitly stated premises, deductively\nvalid or invalid?\nOptions:\n- valid\n- invalid\nAnswer:\nLet\u2019s think step by step.\nYou can friend FC Dynamo Kyiv but not be an expert of BSC Young Boys.\nSo, the answer is invalid.\nQuestion:\n\"It is not always easy to see who is related to whom \u2013\nand in which ways.\nThe following argument pertains to this question:\nSome classmate of Terri is a workmate of Dolores.\nWhoever is not a\nworkmate of Dolores is an ancestor of Cheryl.\nSo, necessarily, some\nancestor of Cheryl is not a classmate of Terri.\"\nIs the argument, given the explicitly stated premises, deductively\nvalid or invalid?\nOptions:\n- valid\n- invalid\nAnswer:\nLet\u2019s think step by step.\nReasoning:\nSome classmate of Terri is not a workmate of Dolores.\nWhoever is not a workmate of Dolores is an ancestor of Cheryl.\nSo,\nnecessarily, some ancestor of Cheryl is not a classmate of Terri.\nRemember that Terri\u2019s classmate and Dolores\u2019s workmate can be in\nany case:\nOne of the three can be any person.\nSo, the answer is\ninvalid.\nFigure 6: The bootstrapped 3-shot Q-R-A soft prompt which is used in the formal_fallacies BBH\nexperiments.\n22\nBootstrapped 5-shot CoT prompt for GSM8K\nQuestion:\nThere are 3 meatballs on each spaghetti plate.\nIf Theresa\u2019s 3\nsons each eat two-thirds of the meatballs on their respective plates, how\nmany meatballs are still left on their plates altogether?\nAnswer:\nLet\u2019s think step by step.\nThere are 3x3 meat balls on each of the 3 plates.\n3 sons each eat 2/3 of the meatballs on their respected plates.\n<answer>\n3\n</answer>\nQuestion:\nJoyce, Michael, Nikki, and Ryn have a favorite movie.\nJoyce\u2019s\nfavorite movie is 2 hours longer than Michael\u2019s movie.\nNikki\u2019s movie is\nthree times as long as Michael\u2019s movie, and Ryn\u2019s favorite movie is 4/5 times\nas long as Nikki\u2019s favorite movie.\nIf Nikki\u2019s favorite movie is 30 hours\nlong, calculate the total number of hours of their favorite movies together.\nAnswer:\nLet\u2019s think step by step.\nNikki\u2019s movie = 30 hours\nRyn\u2019s movie = Nikki\u2019s * 4/5 = 4/5 * 30 = 24\nMichael\u2019s movie = Nikki\u2019s / 3 = 10\nJoyce\u2019s movie = Michael\u2019s + 2 = 10 + 2 = 12\nTotal Hours = Michael\u2019s + Nikki\u2019s + Ryn\u2019s + Joyce\u2019s = 10 + 30 + 24 + 12\n= 76\n<answer>\n76\n</answer>\nQuestion:\nSusan had a bouquet of 3 dozen roses.\nShe gave half to her\ndaughter, and then placed the rest in a vase.\nThe next day, one-third of\nthe flowers in the vase were wilted.\nAfter removing the wilted flowers, how\nmany flowers remained in the vase?\nAnswer:\nLet\u2019s think step by step.\nAnswer STEP 1:\n3 dozen is 36\nAnswer STEP 2:\n1/2 of 36 is 18\nAnswer STEP 3:\n18-6 = 12\n<answer>\n12\n</answer>\nQuestion:\nJessie won 3 times as many athletic awards as his buddy Scott,\nwho won 4 awards.\nThe best athlete at the rival high school won twice as\nmany awards as Jessie.\nHow many awards did the rival win?\nAnswer:\nLet\u2019s think step by step.\n1.\nJessie won 3 times as many awards as Scott, so Jessie won 3 x 4 = 12\nawards while Scott won 4.<sentence>\n2.\nThe best athlete at the rival school won twice as many as Jessie.\nSo he\nwon 2 x 12 = 24 awards, 12 more than Jessie, while Scott won 4.\n<sentence>\n3.\nThe best athlete at the rival school won 24 awards.\n<answer>\n<answer>\n24\n</answer>\nQuestion:\nJames buys 5 packs of sodas that are 12 sodas each.\nHe had 10\nsodas already.\nHe finishes all the sodas in 1 week.\nHow many sodas does he\ndrink a day?\nAnswer:\nLet\u2019s think step by step.\nThere are 5 packs with 12 sodas each and he has 10 left over.\n5 x 12 = 60 + 10 = 70 (number of total sodas)\n70 / (7 day week) = 10 drinks per day\n<answer>\n10\n</answer>\nFigure 7: The bootstrapped 5-shot Q-R-A soft prompt which is used in GSM8K experiments.\n23\n"
  },
  {
    "title": "Alchemist: Parametric Control of Material Properties with Diffusion Models",
    "link": "https://arxiv.org/pdf/2312.02970.pdf",
    "upvote": "7",
    "text": "Alchemist: Parametric Control of Material Properties with Diffusion Models\nPrafull Sharma*,1,2\nVarun Jampani\u2020,2\nYuanzhen Li2\nDmitry Lagun2\nFredo Durand2\nBill Freeman1,2\nMark Matthews2\n1MIT CSAIL\n2Google Research\nwww.prafullsharma.net/alchemist\nAlbedo\nRoughness\n-1\n +1\n +1\n0\nTransparency\nInput\nOutput\nInput\nOutput\nInput\nOutput\nRoughness\nInput\nOutput\nMetallic\nInput\nOutput\nAlbedo\nInput\nOutput\nTransparency\nMetallic\n-1\n +1\n +1\n0\nFigure 1. Overview. Our method, Alchemist, edits material properties of objects in input images by relative attribute strength s. Top:\nWe set the strength s = 1, resulting in a beetle without specular hightlights, a dark metallic dinosaur, and boot with gray albedo. Our\nmodel generates plausible transparency including the light tint, caustics, and hallucinated plausible details behind the object. Bottom: We\ndemonstrate smooth edits for linearly chosen strength values.\nAbstract\nWe propose a method to control material attributes of ob-\njects like roughness, metallic, albedo, and transparency in\nreal images. Our method capitalizes on the generative prior\nof text-to-image models known for photorealism, employ-\ning a scalar value and instructions to alter low-level mate-\nrial properties. Addressing the lack of datasets with con-\ntrolled material attributes, we generated an object-centric\nsynthetic dataset with physically-based materials.\nFine-\ntuning a modified pre-trained text-to-image model on this\n*This research was performed while Prafull Sharma was at Google.\n\u2020Varun Jampani is now at Stability AI.\nsynthetic dataset enables us to edit material properties in\nreal-world images while preserving all other attributes. We\nshow the potential application of our model to material\nedited NeRFs.\n1. Introduction\nAchieving fine-grained control over material properties\nof objects in images is a complex task with wide commer-\ncial applications beyond computer graphics. This ability is\nparticularly relevant in image editing, advertising, and im-\nage forensics. We propose a method for precise editing of\nmaterial properties in images, harnessing the photorealistic\ngenerative prior of text-to-image models. We specifically\n1\narXiv:2312.02970v1  [cs.CV]  5 Dec 2023\ntarget four key material properties: roughness, metallic,\nalbedo, and transparency. Our results illustrate that genera-\ntive text-to-image models contain a strong understanding of\nlight transport which can be leveraged for precise control of\nthese material properties. The physics of light transport af-\nfects the appearance of the object. How we view the objects\nis an interplay of physical factors such as surface geome-\ntry, illumination sources, camera intrinsics, color science,\nsensor linearity, and tone-mapping. However, the most sig-\nnificant of these factors is material properties.\nIn computer graphics, Bidirectional Reflectance Distri-\nbution Functions (BRDFs) [11,24,25] define material prop-\nerties which led to the development of principled and phys-\nically based BRDF models [4]. Prior methods typically em-\nployed an inverse rendering approach to disentangle and es-\ntimate complex scene attributes like geometry and illumina-\ntion for material modification [35]. Recent work by Subias\net al. proposed a GAN-based method trained on synthetic\ndata for perceptual material edits, focusing on metallic and\nroughness parameters, necessitating the masking of the tar-\ngeted real-world object [79]. Our approach uses the gen-\nerative prior of text-to-image models. We directly modify\nreal-world images in pixel space, eliminating the need for\nauxiliary information such as explicit 3D geometry or depth\nmaps, environment maps, and material annotations, thereby\nbypassing the process of accurately estimating object and\nscene-level properties.\nManipulating material properties in pixel space using a\npre-trained text-to-image model presents two main chal-\nlenges. First, the scarcity of real-world datasets with pre-\ncisely labeled material properties makes generalizing from\nsupervised training difficult. Second, text-to-image models\nare trained with textual descriptions like \u201dgold,\u201d \u201dwood,\u201d\nor \u201dplastic,\u201d which often lack fine-grained details about the\nmaterial. This issue is compounded by the inherent discon-\nnect between the discrete nature of words and the continu-\nous nature of material parameters.\nTo overcome the first challenge, we render a synthetic\ndataset featuring physically-based materials and environ-\nment maps, thus addressing the need for fine-grained an-\nnotations of material properties. For the second challenge,\nwe employ extra input channels to an off-the-shelf diffusion\nmodel, refining this model with an instruction-based pro-\ncess inspired by InstructPix2Pix [3]. Despite being trained\non only 500 synthetic scenes comprising 100 unique 3D ob-\njects, our model effectively generalizes the control of mate-\nrial properties to real input images, offering a solution to the\nissue of continuous control.\nTo summarize, we present a method that utilizes a pre-\ntrained text-to-image model to manipulate fine-grained ma-\nterial properties in images. Our approach offers an alterna-\ntive to traditional rendering pipelines, eliminating the need\nfor detailed auxiliary information. The key contributions of\nour method are as follows:\n1. We introduce an image-to-image diffusion model for\nparametric control of low-level material properties,\ndemonstrating smooth edits of roughness, metallic,\nalbedo and transparency.\n2. We render a synthetic dataset of fine-grained material\nedits using 100 3D objects and randomized environ-\nment maps, cameras, and base materials.\n3. Our proposed model generalizes to real images despite\nbeing trained on synthetic data.\n2. Related Work\nDiffusion models for image generation.\nDenoising Dif-\nfusion Probabalistic Models (DDPMs) have been an active\nfocus of the research community [12, 14, 27\u201329, 32, 33, 77]\nfor their excellent photorealistic image generation capabili-\nties from text prompts [55,64,66,70]. Image-to-image tasks\nare possible by modifying the denoising network to ac-\ncept image inputs, allowing style-transfer [76], inpainting,\nuncropping, super-resolution, and JPEG compression [69].\nFurthermore, the generative priors of 2D diffusion models\nhave been utilized towards novel-view synthesis, 3D gener-\nation, and stylistic 3D editing [6,8,18,22,30,42,44,61,63,\n71, 72, 74, 81, 85, 89, 91, 95]. Our image-to-image method\nleverages and further controls this learned prior of DDPMs.\nControl in generative models.\nControlling generative\nmodel output remains an active area of study with many\nworks proposing text-based methods [1, 3, 5, 10, 20, 26,\n34, 36, 43, 52, 60, 80, 82, 84].\nOther works proposed al-\nternative control inputs such as depth maps, sketches [83,\n90], paint strokes [50], identity [47, 88], or photo collec-\ntions [40,67,68,73]. Prompt-to-Prompt [26], P+ [84], and\nNull-text inversion [52] present editing techniques based\non reweighting of cross-attention maps. ControlNet [92]\nand T2I-Adapter [54] demonstrate control through spatial\ninputs defining mid-level information. Generated images\nfrom diffusion models can also incorporate new subjects\nfrom an image collection using a small number of exem-\nplars [7, 19, 40, 67, 68, 73, 87]. While these works control\nhigh and mid-level information about objects, control of\nlow-level properties such as materials remains a challenge\nfor them, leading us to our present line of study.\nMaterial understanding and editing.\nEditing materials\nin images is a significant challenge, requiring a strong un-\nderstanding of image formation. Human vision research has\nextensively explored how attributes like albedo, roughness,\nillumination, and geometry affect object perception [15,16,\n16,17,49,53,56\u201358,78].\nImage based material editing was introduced by Khan et\nal. presenting simple material operations using depth esti-\nmates [35]. Subsequent works demonstrated disentangle-\n2\nFigure 2. Method. We generate a synthetic dataset by taking each of 100 objects, applying randomized materials and illumination maps,\nand modifying the shading network according to randomly sampled attribute strength s. Each object is rendered from 15 randomized\ncameras (see Section 3 for details). During training we provide the s = 0 image as context and randomly choose a target image of known\nattribute strength. At test time we provide the user-input context image and edit strength.\nMetallic\nRoughness\nTransparency\nAlbedo\nFigure 3. Synthetic dataset. Samples from our synthetic dataset\nillustrating appearance change for a linear attribute change.\nment of material and lighting with a statistical prior [45],\nediting gloss appearance [2, 48], intrisic image decompo-\nsition [41], and 2D editing of material exemplars [96].\nWe forego these \u201cdecompositional\u201d approaches and instead\nleverage the largely self-supervised prior of DDPMs for di-\nrect editing in pixel-space.\nGenerative models, particularly Generative Adversarial\nNetworks (GANs) [21], have been investigated for their\nability to alter material perception, focusing on gloss and\nmetallic properties [13, 79]. The application of semantic\nand material editing in NeRFs has also been explored using\ntext-prompts and semantic masks [23,94].\n3. Method\nThere is no existing object-centric dataset that precisely\nvaries only single material attributes. Curating such a real\nworld dataset would be infeasible due to the difficulty of\ncreating physical objects in this way with known parame-\nters. Therefore, we opt to render a synthetic dataset, giv-\ning us full control of material attributes. Using this data,\nwe propose a method to perform material attribute control\ngiven a context image, instruction, and a scalar value defin-\ning the desired relative attribute change.\nThe method is\nbased on latent diffusion model for text-to-image generation\nwith modification that allows us to condition the network on\nthe relative attribute strength.\n3.1. Datasets\nWe render our dataset with the Cycles renderer from\nBlender [9], using publicly available 3D assets, physically-\nbased materials, and environment maps. Each scene begins\nwith one of 100 unique object meshes from polyhaven.com.\nEach of these is paired with five randomly chosen mate-\nrials of the 1200 available from ambientcg.com, and illu-\nminated with one of 400 environment maps.\nThe mate-\nrial is a Principled BRDF shader node, the base shader in\nBlender. The base configuration of the material is kept as\na control defined as 0 strength change for each of the at-\ntributes. This control serves as the context input image to\nthe method against which relative changes in roughness,\nmetallic, albedo, and transparency are applied, sampling\n10 random relative values for each attribute, the details of\nwhich are described below. Finally, we render 15 images of\neach setup using different camera viewpoints and intrinsics.\nThis creates a wide combination of scenes with diversity\nin material, lighting, and background conditions. Samples\nfrom the rendered dataset are presented in Figure 3.\nRoughness and Metallic. For both roughness and metal-\nlic properties, we operate in an additive framework. In the\ncase when the material has the associated map for rough-\nness or metallic, we use an additive node yielding a para-\nmetric control between [-1, 1]. For materials where either\nof these spatial maps are missing, we control the attribute\ncontrol directly as a constant map, assuming the base 0.5 as\nthe control state of the attribute. Note that these values are\n3\nclamped between [0, 1] so in some cases, further increasing\nor decreasing the roughness does not result in any change in\nthe rendered image. We account for this by under-sampling\nsuch images where the gradient of change is constant.\nReducing the roughness value results in a surface that re-\nflects light more uniformly and sharply, giving it a glossy or\nshiny appearance. On the other hand, increasing the rough-\nness value leads to a more diffused light reflection, making\nthe surface appear matte or dull. Low metallic value results\nin appearance predominantly determined by the base color,\nas in the case of plastic and wood. Increasing the metallic\nleads to the surface absorbing more of the incoming light,\nresulting in a darker appearance of the object.\nAlbedo. We implement a color mixing between the original\nalbedo map of the object and a spatially constant gray (RGB\n= 0.5) albedo map. The parametric controller operates be-\ntween 0 and 1, where 0 corresponds to the original albedo,\nand 1 corresponds to completely gray albedo. This can be\nconsidered as detexturing the albedo and can be interesting\nwhen combined with roughness and metallic parameters to\nachieve a mirror-like or a shading-only image.\nTransparency. We introduce the ability to control trans-\nparency by controlling the transmission value in the BSDF\nshader node. The attribute value is chosen to be in range\n[0, 1]. For a chosen transmission value t, we choose to re-\nduce the roughness and metallic component in an additive\nmanner by t, and also add a white overlay to the albedo to\nincrease the intensity of the appeared color. For the value of\n0, we expect the same opaque object and at 1, we would get\na transparent version of the object, making it appear as if it\nwas made of glass. Note that we made the choice to retain\nthe effect of the albedo resulting in a fine tint on the object.\n3.2. Parametric Control in Diffusion Models\nThe rendered synthetic data is used to finetune an image-\nto-image diffusion model conditioned on relative attribute\nstrength and a generic text instruction providing parametric\ncontrol over material properties. We operate in latent space\nusing Stable Diffusion 1.5, a widely adopted text-to-image\nlatent diffusion model.\nDiffusion models perform sequential denoising on noisy\ninput samples, directing them towards the dataset distribu-\ntion by maximizing a score function [75]. A noising pro-\ncess is defined over timesteps t \u2208 T, resulting in a normal\ndistribution at T. We operate in latent space by using a pre-\ntrained variational encoder E and ddecoder D [38], a potent\naid to conditional image generation [66]. Training draws\nan image sample I from the dataset, encodes it into a latent\nz = E(I), then noises it at t as zt. A denoising network \u03f5\u03b8\npredicts the added noise given the latent zt, diffusion time\nt, and conditioning variables.\nOur image-to-image model is conditioned on an input\nimage to be edited, provided as E(Ic) concatenated to the\nlatent being denoised zt. Text conditioning is provided via\ncross-attention layers using a generic prompt, p = \u201cChange\nthe <attribute name> of the <object class>.\u201d\nSince textual CLIP embeddings [62] do not encode fine-\ngrained information well [59], prompt-only conditioning of\ns expressed textually (i.e. \u201cChange the roughness of the\napple by 0.57.\u201d) yields inconsistent output. To facilitate\nrelative attribute strength conditioning, we also concatenate\na constant scalar grid of edit strength s.\nWe initialize the weights of our denoising network with\nthe pre-trained checkpoint of InstructPix2Pix [3], provid-\ning an image editing prior and understanding of instructive\nprompts. During training (Fig. 2), we minimizes the loss:\nL = EE(I),E(Ic),s,p,\u03f5\u223cN (0,1),t\n\u0002\n||\u03f5 \u2212 \u03f5\u0398(zt, t, E(Ic), s, p)||2\n2\n\u0003\n(1)\nWe always provide the s = 0 image as context Ic , and\ndraw an edited image Ie at random for noising. Since we\nalways render an s = 0 sample, and other s are chosen with\nstratified sampling, our distribution has a slight bias towards\nzero. Since many edit strengths may have little effect (i.e.\nwe cannot lower the roughness of an object with 0 rough-\nness), we find that providing too many of these examples\nbiases the network towards inaction. We therefore down-\nweight such null examples, defined as ||Ic \u2212 Ie||2 < \u03c4, by\nwnull via rejection sampling. In practice we set wnull =\n0.80, \u03c4 = 0.05. We train with fp16 precision for 10k steps\nusing Adam [37] and learning rate of 5e-5. We use the text\nencoders and noise schedule from Stable Diffusion.\nAt test time we provide a held out image as context Ic,\nedit strength s, and prompt p for the object class of the input\nimage. We denoise for 20 steps using the DPM-solver++\nbased noise scheduler [46].\nMulti-attribute editing. We edit multiple attributes in a\nsingle diffusion pass by concatenating more than one edit\nstrength, drawn from {sa, sr sm} giving us [zt|E(Ic)|s] as\nthe final UNet input, where | is concatenation.\nClassifier-free guidance. Ho et al. [29] proposed classifier-\nfree guidance (CGF) to improve visual quality and faithful-\nness of images generated by diffusion models. We retain\nthe same CFG setup as InstructPix2Pix for both image and\nprompt conditioning. We do not however impose CFG with\nrespect to the relative attribute strengths s. We want the\nnetwork to be faithful to edit strength and forced to reason\nabout incoming material attributes. As s can be 0 by defini-\ntion of the problem itself, and downweighted as described\nabove, we did not find CFG on s necessary.\nWe will release the dataset generation pipeline, image\nrenderings with metadata, and the training code.\n4. Results\nWe present qualitative analysis demonstrating the gen-\neralization capability of our model to real images despite\n4\nRoughness\nMetallic\nAlbedo\nTransparency\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nInput\nbag, +1\npumpkin, -1\nstatue, -1\ncar, +1\npumpkins, +1\ncat statue, -1\nstatue, +1\nfish, +1\ntoy, +1\nbinocular, +1\nchair, +1\nbananas, +1\nFigure 4. Single-attribute editing results. Outputs from our model trained on individual attributes. Left of columns are held-out input and\nright are model output (object class, s). Increased \u201cRoughness\u201d replaces specular highlights on the bag with base albedo. \u201cMetallic\u201d varies\ncontributions from albedo and shine in regions of the pumpkins and cat. Surfaces are changed to a flat grey \u201cAlbedo\u201d revealing object\nillumination. \u201cTransparency\u201d preserves object tint while inpainting background and hallucinating plausible hidden structures and caustics.\nbeing trained on synthetic data. Comparisons to baselines\nshow the effectiveness of our model for fine-grained mate-\nrial editing, further supported by a user study. We extend\nthe use of our model to NeRF material editing on the DTU\ndataset [31,51].\n4.1. Results on Real Images\nWe demonstrate the effectiveness of our technique\nthrough editing material attributes, one at a time, for real un-\nseen images, in Figure 4. For each of the material attributes\nwe use a separate model trained only on that attribute. We\nobserve that the model outputs preserve geometry and take\nthe global illumination into account.\nRoughness.\nAs the roughness is increased, the output\nshows removal of the specular highlights replaced by esti-\nmate of the base albedo. The highlights are amplified when\nthe roughness is reduced as shown in the case of the pump-\nkin and statue.\nMetallic. The increase in the metallic component of the car\nand pumpkin results in dampening of the base albedo and\nincrease in the shine on the surface. The effect is reverse\nfor the cat statue when the metallic strength was reduced.\nOur method shows similar behavior to the Principled BRDF\nshaders, which present perceptually subtle effects when tun-\ning the metallic value.\nAlbedo. As the relative strength for the albedo is turned\nto 1, we observe the albedo of the Buddha statue, fish, and\ntoy go to gray. This is not a trivial in-image desaturation\noperation as the network maintains the highlights, shadows,\nand the light tint from the plausible environment map.\nTransparency. The transparent renditions of the binocu-\nlar and the chair demonstrate the prior over 3D geometry\nof the objects, using which it generates the appropriately\ntinted glass-like appearance and in-paints background ob-\njects. With the edit of the banana, we can see the caustics\nunderneath and the preservation of the specular highlights.\n4.2. Baseline Comparisons\nWe compare our method, Alchemist, to the GAN-based\nin-image material editing of Subias et al. [79], Prompt-to-\nPrompt [26] with Null-text Inversion (NTI) [52], and In-\nstructPix2Pix [3] in Figure 5. Furthermore, we fine-tuned\nthe InstructPix2Pix prompt-based approach with our syn-\nthetic dataset. Subias et al.\u2019s method results in exagger-\nated material changes as their objective is perceptual, not\nphysically-based material edits. Null-text inversion and In-\nstructPix2Pix change the global image information instead\nof only the object of interest: lighting changes for rough-\nness and albedo edits, or a geometry change for metallicity\nedit. When InstructPix2Pix is trained on our dataset with a\nprompt-only approach, we observe the model exaggerating\nthe intended effect, yielding artifacts on the panda for metal-\nlic and the water for the transparency case. The model also\nchanges the albedo of the sand when only asked to make the\nchange to the crab. Our method faithfully edits only the ob-\nject of interest, introducing the specular hightlights on the\nleg of the cat statue, dampening the albedo for the metal-\nlic panda, changing the albedo of the crab to gray while\nretaining the geometry and illumination effects, and turn-\ning the dolphin transparent with plausible refractive effects.\n5\niPix2Pix\niPix2Pix (our data)\nOurs\nSubias et al.\nNull-text inversion\nDoes not support \nalbedo change\nInput\nRoughness\ncat statue, -1\nMetallic\npandas, +1\nAlbedo\ncrabs, +1\nTransparency\ndophin, +1\nDoes not support \ntransparency change\nFigure 5. Qualitative comparison. Comparison of Alchemist with baseline methods. We increase each of the attributes shown on the left.\nInstructPix2Pix w/ our data\nOur Method\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nRoughness\n30.9\n0.89\n0.13\n31.5\n0.90\n0.09\nMetallic\n31.0\n0.89\n0.10\n31.1\n0.89\n0.09\nAlbedo\n26.9\n0.88\n0.14\n27.2\n0.88\n0.10\nTransparency\n26.9\n0.85\n0.13\n27.1\n0.85\n0.13\nTable 1. Quantitative analysis. Metrics for the prompt-only In-\nstructPix2Pix trained on our data and our proposed method com-\nputing the PSNR, SSIM [86], and LPIPS [93] on a held-out unseen\nsynthetic rendered dataset of 10 scenes with 15 cameras.\nSpecific configurations for each baseline is presented in the\nsupplement.\nQuantitative Comparisons. Due to the lack of an existing\ndataset for quantitative analysis, we rendered 10 held-out\nscenes with unseen 3D objects, materials, and environment\nmaps. Each scene was rendered from 15 different view-\npoints with 10 random scalar values for each attribute. We\npresent the average PSNR, SSIM [86], and LPIPS [93] of\nedits against GT for prompt-only InstructPix2Pix and Al-\nchemist in Table 1. While the PSNR and SSIM scores are\nquite close, our model does better in terms of the LPIPS\nscore. We also note that prompt-only InstructPix2Pix fine-\ntuned on our data does not yield smooth transitions as the\nrelative strength is linearly changed, visible in video results\npresented in the supplement. Note that the image recon-\nstruction metrics are not commonly used for evaluation of\nprobabilistic generative models. Samples of the test data\nand model outputs are presented in the supplement.\nUser study. To further the comparison between the base-\nline and our method, we conducted a user study presenting\nN=14 users with pairs of edited images. For each image\npair, the users were asked to choose between the two based\non: (1) Photorealism, and (2) \u201cWhich edit do you prefer?\u201d.\nFor both questions, the users were presented with the in-\nstruction, i.e. for transparency, the instruction was stated\nas \u201dthe method should be able to output the same object as\ntransparent retaining the tint of the object\u201d. Each user was\npresented with a total of 12 image pairs (3 image results for\neach of the 4 attributes).\nOur method was chosen as the one with more photo-\nrealistic edits (69.6% vs. 30.4%) and was strongly preferred\noverall (70.2% vs. 29.8%). This is likely due to the ap-\nparent exaggeration exhibited by InstructPix2Pix trained on\nour data with prompt-only approach, leading to saturated\neffects making it less photo-realistic.\nSmoothness in Parametric Control.\nWe demonstrate that our model achieves fine grained\ncontrol of material parameters by linearly varying the\nstrength of a single attribute, as shown in Figure 6. Ob-\nserve that the model generates plausible specular highlights\non the headphone instead of naively interpolating pixel val-\nues to the extrema and introduces more dampening of the\n6\nAlbedo\nRoughness\n-1\n +1\n +1\n0\nTransparency\nMetallic\n-1\n +1\n +1\n0\nFigure 6. Slider results. Alchemist produces edits smoothly with attribute strength. We note that the outputs for linear change in the input\nrelative strength in InstructPix2Pix prompt-only trained on our data results in non-smooth transitions. Refer to the supplement videos.\n\u201cChange the material \nof the cat.\u201d\nalbedo: 2\nroughness: -1\nmetallic: 1\n\u201cChange the transparency \nof the cup.\u201d\ntransparency: 1\nInput Image\nSegmentation\nOutput\nOutput\nSegmentation\nInput Image\nFigure 7. Spatial Localization. Edit results (bottom) when the\nscalar strength input is masked by the shown segmentation (top).\nThe image is only altered in the segmented region, becoming ei-\nther shiny (cat), or semi-transparent (cup).\nalbedo on the cat to give it a metallic look. For transparency,\nthe model preserves the geometry while refracting the light\nthrough the object to produce a transparent look. The in-\nstructPix2Pix model trained on our data did not yield such\nsmooth results as the relative strength of the attributes were\nchanged in text format. Please refer to the supplementary\nfor video results.\n4.3. Specializations\nSpatial localization. Attending to a specific instance of a\nclass when multiple objects of the same class are present\nin the image is a difficult task using just language instruc-\ntion. We explore the possibility of changing the material\nattribute of a specific instance of the class by only attribut-\ning the scalar value in the segmented region, assuming a\nknown segmentation map from an instance segmentation\nmethod such as Segment Anything [39]. Though the net-\nwork was not trained for this specific task, we find that the\nnetwork does respect the localization of the relative attribute\nstrength, though requires over-driving to values beyond 1.\nWe observe that mask-based editing works in such cases,\ni.e. changing the material properties of specific instance of\ncat and cup, as shown in Figure 7.\nMulti-attribute changes. To enable control over multiple\n7\nInput\nAxis-only sampled\nVolume sampled\nFigure 8. Multi-Attribute Editing.\nComparison between an\n\u201caxis-only sampled\u201d model trained on images where only one\nof {sa, sr, sm} is \u0338= 0, vs. a \u201cvolume sampled\u201d one where all\n{sa, sr, sm} may be \u0338= 0. We show edits with (sa, sr, sm) =\n(1, \u22121, 1). The former tends to edit only a single attribute, while\nthe latter successfully achieves the desired \u201csilver\u201d appearance.\nattributes in a single diffusion pass, we train our network\non two versions of the dataset to be able to vary albedo\n(sa), roughness (sr), and metallic (sm). In the axis-only\nsampled version, we keep the context image at the baseline,\nand vary a single attribute at a time, such that only one of\n{sa, sr, sm} is non-zero for any given training target image.\nIn the volume sampled version, {sa, sr, sm} are all allowed\nto be non-zero, effectively sampling the 3D volume of ma-\nterial attributes. In both data-sets, we keep the number of\ninput images the same.\nWe present the qualitative analysis of the joint control in\nFigure 8. We find that the \u201done attribute at a time\u201d model\nfails to compose the three attributes, generally showing bias\ntowards one of the attributes. The model trained on the\nvolume of these attributes successfully generalizes, demon-\nstrating excellent ability to edit multiple attributes at once.\nWe find this essential to producing a strong metallic appear-\nance on objects, as the Principled BSDF shader requires a\nwhite, non-rough, and highly metallic surface to produce\nthis look.\nMaterial editing of NeRFs.\nWe test the efficacy of per-\nframe editing using our method for two-step material con-\ntrol in neural radiance field (NeRF) reconstruction. We use\na selection of scenes from the DTU MVS [31] dataset and\nedit them to have reduced albedo or higher specular reflec-\ntions. We train a NeRF with the vanilla configuration based\non [65] (complete details in the supplement).\nIn the results presented in Figure 9, we observe highly\nplausible renderings from held-out views showing 3D struc-\nture with the intended albedo, roughness, and metallic\nchange. Please refer to the supplement for rendered video\nof NeRFs trained on edited data.\nOriginal held-out views\nNeRF renderings w/ edited images\nFigure 9. NeRF Results. Left: Original test views from DTU.\nRight: We edit training views of each scene, train a NeRF, then\nrender held-out test views. The respective edits (sa, sr, sm) from\ntop to bottom are: scan30: (0, \u22120.5, 0.5), scan118: (1, \u22121, 1)\nand scan69: (1, 1, 0).\nInput\nRoughness, +1\nInput\nTransparency, +1\nFigure 10. Limitations. Alchemist sometimes fails to achieve the\ndesired result. Left: A shiny surface remains on the teapot after a\nroughness edit. Right: The stem of a candy-cane is omitted.\n5. Discussion\nOur model generalizes to editing fine-grained material\nproperties in real images, despite being trained solely on\nsynthetic data. We believe that our method could extend\nto a wide range of material alterations achievable with a\nshader. However, our approach does have limitations, such\nas producing minimal perceptual changes for roughness and\nmetallic attributes, and occasionally yielding physically un-\nrealistic transparency, as illustrated in Figure 10. The model\nlacks a complete 3D world model and is unable inpaint to\nmaintain physical consistency as seen in the candy-cane ex-\nample. As is typical with generative models, our method\ngenerates plausible interpretations that are true to the given\ninstructions, but it does not necessarily replicate the exact\noutcomes of a traditional graphics renderer.\n6. Conclusion\nWe present a method that allows precise in-image con-\ntrol of material properties, utilizing the advanced generative\ncapabilities of text-to-image models. Our approach shows\nthat even though the model is trained on synthetic data, it\neffectively edits real images, achieving seamless transitions\nas the relative strength of the desired attribute is varied.\nBeyond image editing, we demonstrate the applicability to\nNeRF allowing for editable materials in NeRFs. We believe\nthat our work can further impact downstream applications\nand allow for improved control over low-level properties of\nobjects.\n8\n7. Acknowledgements\nWe would like to thank Forrester Cole, Charles Her-\nrmann, Junhwa Hur, and Nataniel Ruiz for helpful discus-\nsions. Thanks to Shriya Kumar and Parimarjan Negi for\nproofreading the submission.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18208\u201318218, 2022. 2\n[2] Ivaylo Boyadzhiev, Kavita Bala, Sylvain Paris, and Edward\nAdelson. Band-sifting decomposition for image-based mate-\nrial editing. ACM Transactions on Graphics (TOG), 34(5):1\u2013\n16, 2015. 3\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 18392\u201318402, 2023.\n2, 4, 5\n[4] Brent Burley. Physically based shading at disney. In ACM\nSIGGRAPH 2012 Courses. ACM, 2012. 2\n[5] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. arXiv preprint arXiv:2304.08465, 2023. 2\n[6] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nICCV, 2023. 2\n[7] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023. 2\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai,\nGang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-\nto-3d generation with explicit view synthesis. arXiv preprint\narXiv:2308.11473, 2023. 2\n[9] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 3\n[10] Yuren Cong, Martin Renqiang Min, Li Erran Li, Bodo\nRosenhahn, and Michael Ying Yang.\nAttribute-centric\ncompositional text-to-image generation.\narXiv preprint\narXiv:2301.01413, 2023. 2\n[11] Robert L Cook and Kenneth E. Torrance.\nA reflectance\nmodel for computer graphics. ACM Transactions on Graph-\nics (ToG), 1(1):7\u201324, 1982. 2\n[12] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance. In European Con-\nference on Computer Vision, pages 88\u2013105. Springer, 2022.\n2\n[13] Johanna Delanoy, Manuel Lagunas, J Condor, Diego Gutier-\nrez, and Bel\u00b4en Masia. A generative framework for image-\nbased editing of material appearance using perceptual at-\ntributes. In Computer Graphics Forum, volume 41, pages\n453\u2013464. Wiley Online Library, 2022. 3\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 2\n[15] Katja Doerschner, Huseyin Boyaci, and Laurence T Mal-\noney.\nEstimating the glossiness transfer function induced\nby illumination change and testing its transitivity. Journal\nof Vision, 10(4):8\u20138, 2010. 2\n[16] Roland W Fleming. Visual perception of materials and their\nproperties. Vision research, 94:62\u201375, 2014. 2\n[17] Roland W Fleming, Ron O Dror, and Edward H Adelson.\nReal-world illumination and the perception of surface re-\nflectance properties. Journal of vision, 3(5):3\u20133, 2003. 2\n[18] Rafail Fridman, Amit Abecasis, Yoni Kasten, and Tali Dekel.\nScenescape: Text-driven consistent scene generation. ArXiv,\nabs/2302.01133, 2023. 2\n[19] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2\n[20] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin\nHuang. Expressive text-to-image generation with rich text.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7545\u20137556, 2023. 2\n[21] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 3\n[22] Ayaan Haque, Matthew Tancik, Alexei A Efros, Alek-\nsander Holynski, and Angjoo Kanazawa. Instruct-nerf2nerf:\nEditing 3d scenes with instructions.\narXiv preprint\narXiv:2303.12789, 2023. 2\n[23] Ayaan Haque, Matthew Tancik, Alexei A. Efros, Aleksander\nHolynski, and Angjoo Kanazawa.\nInstruct-NeRF2NeRF:\nEditing 3D Scenes with Instructions.\nIn Proceedings of\nthe IEEE/CVF International Conference on Computer Vision\n(ICCV), 2023. 3\n[24] Xiao D He, Patrick O Heynen, Richard L Phillips, Kenneth E\nTorrance, David H Salesin, and Donald P Greenberg. A fast\nand accurate light reflection model. ACM SIGGRAPH Com-\nputer Graphics, 26(2):253\u2013254, 1992. 2\n[25] Xiao D He, Kenneth E Torrance, Francois X Sillion, and\nDonald P Greenberg.\nA comprehensive physical model\nfor light reflection. ACM SIGGRAPH computer graphics,\n25(4):175\u2013186, 1991. 2\n[26] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2, 5\n[27] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n9\n[28] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022. 2\n[29] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2, 4\n[30] Lukas H\u00a8ollein, Ang Cao, Andrew Owens, Justin John-\nson, and Matthias Nie\u00dfner.\nText2room: Extracting tex-\ntured 3d meshes from 2d text-to-image models.\nArXiv,\nabs/2303.11989, 2023. 2\n[31] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola,\nand Henrik Aan\u00e6s. Large scale multi-view stereopsis eval-\nuation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 406\u2013413, 2014. 5, 8\n[32] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park,\nEli Shechtman, Sylvain Paris, and Taesung Park.\nScal-\ning up gans for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10124\u201310134, 2023. 2\n[33] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels.\nAdvances in Neural Information Processing Sys-\ntems, 35:26565\u201326577, 2022. 2\n[34] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. arXiv\npreprint arXiv:2210.09276, 2022. 2\n[35] Erum Arif Khan, Erik Reinhard, Roland W Fleming, and\nHeinrich H B\u00a8ulthoff. Image-based material editing. ACM\nTransactions on Graphics (TOG), 25(3):654\u2013663, 2006. 2\n[36] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Dif-\nfusionclip: Text-guided diffusion models for robust image\nmanipulation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2426\u2013\n2435, 2022. 2\n[37] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 4\n[38] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[39] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 7\n[40] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu. Multi-concept customization\nof text-to-image diffusion. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1931\u20131941, 2023. 2\n[41] Guilin Liu, Duygu Ceylan, Ersin Yumer, Jimei Yang, and\nJyh-Ming Lien. Material editing using a physically based\nrendering network. In Proceedings of the IEEE International\nConference on Computer Vision, pages 2261\u20132269, 2017. 3\n[42] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-\nmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:\nZero-shot one image to 3d object. ArXiv, abs/2303.11328,\n2023. 2\n[43] Xihui Liu, Zhe Lin, Jianming Zhang, Handong Zhao, Quan\nTran, Xiaogang Wang, and Hongsheng Li. Open-edit: Open-\ndomain image manipulation with open-vocabulary instruc-\ntions. In European Conference on Computer Vision, pages\n89\u2013106. Springer, 2020. 2\n[44] Yuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie\nLiu, Taku Komura, and Wenping Wang.\nSyncdreamer:\nLearning to generate multiview-consistent images from a\nsingle-view image. arXiv preprint arXiv:2309.03453, 2023.\n2\n[45] Stephen Lombardi and Ko Nishino. Reflectance and natural\nillumination from a single image. In Computer Vision\u2013ECCV\n2012: 12th European Conference on Computer Vision, Flo-\nrence, Italy, October 7-13, 2012, Proceedings, Part VI 12,\npages 582\u2013595. Springer, 2012. 3\n[46] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 4\n[47] Jian Ma, Junhao Liang, Chen Chen, and Haonan Lu.\nSubject-diffusion: Open domain personalized text-to-image\ngeneration without test-time fine-tuning.\narXiv preprint\narXiv:2307.11410, 2023. 2\n[48] Yusuke Manabe,\nMidori Tanaka,\nand Takahiko Hori-\nuchi.\nGlossy appearance editing for heterogeneous mate-\nrial objects.\nJournal of Imaging Science and Technology,\n65(6):60406\u20131, 2021. 3\n[49] Phillip J Marlow, Juno Kim, and Barton L Anderson. The\nperception and misperception of specular surface reflectance.\nCurrent Biology, 22(20):1909\u20131913, 2012. 2\n[50] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. arXiv preprint arXiv:2108.01073, 2021. 2\n[51] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 5\n[52] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or.\nNull-text inversion for editing real im-\nages using guided diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 6038\u20136047, 2023. 2, 5\n[53] Isamu Motoyoshi and Hiroaki Matoba. Variability in con-\nstancy of the perceived surface reflectance across different\nillumination statistics. Vision Research, 53(1):30\u201339, 2012.\n2\n[54] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[55] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2\n10\n[56] Shin\u2019ya Nishida and Mikio Shinya.\nUse of image-based\ninformation in judgments of surface-reflectance properties.\nJOSA A, 15(12):2951\u20132965, 1998. 2\n[57] Ga\u00a8e Obein, Kenneth Knoblauch, and Franc\u00b8ise Vi\u00b4eot. Dif-\nference scaling of gloss: Nonlinearity, binocularity, and con-\nstancy. Journal of vision, 4(9):4\u20134, 2004. 2\n[58] Maria Olkkonen and David H Brainard. Perceived glossi-\nness and lightness under real-world illumination. Journal of\nvision, 10(9):5\u20135, 2010. 2\n[59] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar\nMosseri, Michal Irani, and Tali Dekel. Teaching clip to count\nto ten. arXiv preprint arXiv:2302.12066, 2023. 4\n[60] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski.\nStyleclip: Text-driven manipulation\nof stylegan imagery. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n2085\u20132094, October 2021. 2\n[61] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2\n[62] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 4\n[63] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2\n[64] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n1(2):3, 2022. 2\n[65] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-\ngun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1558\u20131567, 2022. 8\n[66] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. 2022 ieee. In CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 10674\u201310685, 2021. 2, 4\n[67] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv preprint arXiv:2208.12242, 2022. 2\n[68] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubinstein,\nand Kfir Aberman. Hyperdreambooth: Hypernetworks for\nfast personalization of text-to-image models. arXiv preprint\narXiv:2307.06949, 2023. 2\n[69] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 2\n[70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2\n[71] Etai Sella, Gal Fiebelman, Peter Hedman, and Hadar\nAverbuch-Elor. Vox-e: Text-guided voxel editing of 3d ob-\njects, 2023. 2\n[72] Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon\nKo, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee,\nand Seungryong Kim.\nLet 2d diffusion model know 3d-\nconsistency for robust text-to-3d generation. arXiv preprint\narXiv:2303.07937, 2023. 2\n[73] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 2\n[74] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration, 2023. 2\n[75] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n4\n[76] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro\nChin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang,\nGlenn Entis, Yuanzhen Li, Yuan Hao, Irfan Essa, Michael\nRubinstein, and Dilip Krishnan. Styledrop: Text-to-image\ngeneration in any style. arXiv preprint arXiv:2306.00983,\n2023. 2\n[77] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in neural\ninformation processing systems, 32, 2019. 2\n[78] Katherine R Storrs, Barton L Anderson, and Roland W\nFleming.\nUnsupervised learning predicts human percep-\ntion and misperception of gloss. Nature Human Behaviour,\n5(10):1402\u20131417, 2021. 2\n[79] J Daniel Subias and Manuel Lagunas. In-the-wild material\nappearance editing using perceptual attributes. In Computer\nGraphics Forum, volume 42, pages 333\u2013345. Wiley Online\nLibrary, 2023. 2, 3, 5\n[80] Ming Tao, Bing-Kun Bao, Hao Tang, Fei Wu, Longhui Wei,\nand Qi Tian. De-net: Dynamic text-guided image editing ad-\nversarial networks. arXiv preprint arXiv:2206.01160, 2022.\n2\n[81] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\nArXiv,\nabs/2304.12439, 2023. 2\n[82] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921\u20131930, 2023. 2\n[83] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models. In ACM SIG-\n11\nGRAPH 2023 Conference Proceedings, pages 1\u201311, 2023.\n2\n[84] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir\nAberman.\np+: Extended textual conditioning in text-to-\nimage generation. arXiv preprint arXiv:2303.09522, 2023.\n2\n[85] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation, 2022. 2\n[86] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 6\n[87] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. arXiv preprint arXiv:2302.13848, 2023. 2\n[88] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00b4edo\nDurand, and Song Han. Fastcomposer: Tuning-free multi-\nsubject image generation with localized attention.\narXiv\npreprint arXiv:2305.10431, 2023. 2\n[89] Jiale Xu, Xintao Wang, Yan-Pei Cao, Weihao Cheng, Ying\nShan, and Shenghua Gao. Instructp2p: Learning to edit 3d\npoint clouds with text instructions. arXiv e-prints, 2023. 2\n[90] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-\nadapter: Text compatible image prompt adapter for text-to-\nimage diffusion models. arXiv preprint arXiv:2308.06721,\n2023. 2\n[91] Lu Yu, Wei Xiang, and Kang Han. Edit-diffnerf: Editing\n3d neural radiance fields using 2d diffusion model. arXiv\ne-prints, 2023. 2\n[92] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2\n[93] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 6\n[94] Xingchen Zhou, Ying He, F. Richard Yu, Jianqiang Li, and\nYou Li. Repaint-nerf: Nerf editting via semantic masks and\ndiffusion models. arXiv e-prints, 2023. 3\n[95] Jingyu Zhuang, Chen Wang, Lingjie Liu, Liang Lin, and\nGuanbin Li. Dreameditor: Text-driven 3d scene editing with\nneural fields. SIGGRAPH Asia, 2023. 2\n[96] K\u00b4aroly Zsolnai-Feh\u00b4er, Peter Wonka, and Michael Wimmer.\nPhotorealistic material editing through direct image manip-\nulation. In Computer Graphics Forum, volume 39, pages\n107\u2013120. Wiley Online Library, 2020. 3\n12\n"
  },
  {
    "title": "StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D",
    "link": "https://arxiv.org/pdf/2312.02189.pdf",
    "upvote": "7",
    "text": "StableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D\nPengsheng Guo\nHans Hao\nAdam Caccavale\nZhongzheng Ren\nEdward Zhang\nQi Shan\nAditya Sankar\nAlexander G. Schwing\nAlex Colburn\nFangchang Ma\nApple\nblue jay standing on large\nbasket of rainbow macarons\na colorful rooster\na baby bunny sitting on top\nof a stack of pancakes\na corgi wearing a top hat\na DSLR photo of a humanoid\nrobot using a laptop\na tarantula, highly detailed\na squirrel-octopus hybrid\na llama wearing a suit\nWedding dress made of tentacles\na wizard raccoon casting a spell\nFigure 1. StableDreamer generates high-quality 3D geometry and appearance, represented as anisotropic 3D Gaussians, from the input\ntext prompts. StableDreamer reduces the commonly seen multi-face Janus problem, improves local details, and converges robustly without\nrequiring a mesh representation, modifying the SDS loss, or using any additional 3D or multi-view priors.\nAbstract\nIn text-to-3D generation, utilizing 2D diffusion models\nthrough score distillation sampling (SDS) [25] frequently\nleads to issues such as blurred appearances and multi-\nfaced geometry, primarily due to the intrinsically noisy na-\nture of the SDS loss. Our analysis identifies the core of\nthese challenges as the interaction among noise levels in\nthe 2D diffusion process, the architecture of the diffusion\nnetwork, and the 3D model representation. To overcome\nthese limitations, we present StableDreamer, a methodology\nincorporating three advances. First, inspired by Instruct-\nNeRF2NeRF [7], we formalize the equivalence of the SDS\ngenerative prior and a simple supervised L2 reconstruction\nloss. This finding provides a novel tool to debug SDS, which\nwe use to show the impact of time-annealing noise levels\non reducing multi-faced geometries.\nSecond, our analy-\nsis shows that while image-space diffusion contributes to\ngeometric precision, latent-space diffusion is crucial for\nvivid color rendition. Based on this observation, Stable-\nDreamer introduces a two-stage training strategy that effec-\ntively combines these aspects, resulting in high-fidelity 3D\nmodels. Third, we adopt an anisotropic 3D Gaussians rep-\nresentation, replacing NeRFs, to enhance the overall qual-\nity, reduce memory usage during training, and accelerate\nrendering speeds, and better capture semi-transparent ob-\njects. StableDreamer reduces multi-face geometries, gener-\nates fine details, and converges stably.\n1. Introduction\nRecent advances in Generative AI have marked a paradigm\nshift across various domains, with notable progress in di-\nalogue generation (e.g., ChatGPT [22]), image genera-\ntion [27, 29, 30] and video synthesis [9, 39]. However, de-\nspite its immense potential, 3D generation still lags behind\n1\narXiv:2312.02189v1  [cs.CV]  2 Dec 2023\nin these developments. A critical obstacle in 3D generation\nis the limited size of available datasets, which pale in com-\nparison to the extensive databases used in language [38] and\nimage fields [31]. To circumvent this lack of 3D datasets,\nrecent efforts such as DreamFusion [25] leverage 2D text-\nto-image models by using Score Distillation Sampling to\ngenerate 3D models from text prompts, showing exciting\nresults with compelling appearance and geometry.\nHowever, these text-to-3D approaches are far from per-\nfect. Several critical issues persist. First, the generated 3D\nassets frequently exhibit over-saturated colors and blurry\nappearance. Fine local details are often omitted, giving re-\nsults a somewhat \u201ctoy-like\u201d quality. Second, the generated\n3D asset\u2019s geometry tends to be oversimplified, lacking the\nability to faithfully represent thin or intricate shapes. Fur-\nthermore, these approaches are notorious for exhibiting the\n\u201cJanus problem\u201d, where the generated 3D object contains\nmultiple canonical views seen from different viewpoints.\nLastly, the optimization and rendering speed are hampered\nby the nature of test-time optimization and the underlying\nNeRF representation.\nIn response to the aforementioned challenges, we intro-\nduce a simple text-to-3D framework StableDreamer. We\nstart with an empirical analysis that yields two pivotal in-\nsights: first, SDS loss can be conceptualized as a supervised\nreconstruction task using denoised images as ground truth,\npaving the way for a visualization tool to inspect the training\ndynamics, and motivating a noise-level annealing to stabi-\nlize SDS training. Second, we observe that image-space dif-\nfusion excels in geometric accuracy but falls short in color\nvibrancy. In contrast, latent-space diffusion enhances color\nat the expense of geometric fidelity. This leads us to de-\nvelop a dual-phase training scheme, leveraging distinct dif-\nfusion architectures to optimize overall generation quality.\nNotably, we establish that these observations are agnostic to\nthe underlying 3D representations with broad applicability.\nA third noteworthy innovation within StableDreamer is the\nadoption of 3D Gaussians [11] as the fundamental 3D repre-\nsentation. This choice offers a host of distinct advantages,\nincluding high fidelity for local details and fast rendering\nspeed.\nHowever, directly substituting this representation\ninto existing SDS frameworks leads to low-quality results\nand artifacts, likely due to the mismatch between noisy SDS\nloss and the localized nature of 3D Gaussians. To mitigate\nthis, we implement strategies on initialization and density\ncontrol, achieving a robust convergence to high-quality 3D\nGaussians. In summary, our contributions are threefold:\n\u2022 Interpreting SDS as a reparametrized supervised recon-\nstruction problem, leading to new visualization that moti-\nvates the use of an annealing schedule for noise levels.\n\u2022 A two-stage training framework that combines image and\nlatent diffusion for enhanced geometry and color quality.\n\u2022 Integration of 3D Gaussians for text-to-3D generation,\nwith novel regularization techniques for improved quality\nand convergence, to further improve fidelity and details.\nWith these simple changes, StableDreamer reduces the\nmulti-face geometry problem and produces a high level\nof fidelity and details in the synthesized models. Stable-\nDreamer is stable in training, without the need for switch-\ning between different 3D representations [15], modification\nof the SDS loss [42], or additional 3D or multi-view a pri-\nori [4, 43]. Our experiments establish StableDreamer\u2019s im-\nprovements over leading text-to-3D models.\n2. Related Work\nText-to-3D.\nSince the advent of large vision-language\nmodels [26, 29, 30], the research community has increas-\ningly focused on the generation of 3D assets from textual in-\nput. Early approaches [18] utilize the CLIP embedding [26]\nfor alignment between rendered images and text prompts.\nThe seminal work DreamFusion [25] and SJC [40] distill\nthe score of learned text-to-image diffusion models [29, 30]\ninto optimizing neural 3D models (e.g., NeRF [20]). These\nworks demonstrate more realistic and high-fidelity results\nand subsequently became the de facto solutions in this field.\nFollowing the success of DreamFusion/SJC, numerous\nfollow-up works have advanced the field. These approaches\nencompass a spectrum of ideas including multi-stage refine-\nment [15], geometry and appearance disentanglement [2],\nand improved the score distillation loss [42]. In this work,\nwe study strategies that would enable stable training of a\nsingle 3D representation under the SDS framework, with-\nout having to convert to meshes (e.g., Magic3D [15] and\nProlificDreamer [42]), designing a different loss (e.g., Pro-\nlificDreamer [42], NFSD [10]), or relying on other 3D or\nmulti-view a priori that is trained on additional datasets\n(e.g., GSGEN [4]).\nNeural 3D Representations.\nNeural 3D representations\noriginated in the context of 3D reconstruction [3, 17, 23],\nwhere neural networks implicitly learned signed distance\nfunctions and occupancy functions. This implicit model-\ning was then extended to the task of novel-view synthe-\nsis [16, 19, 32], yielding remarkable rendering outcomes.\nSubsequent works [1, 28, 41, 45] continued refining neural\n3D representations from diverse perspectives; for a com-\nprehensive overview, readers are directed to Tewari et al.\n[37]. A noteworthy trend [21, 35, 44] involves the adoption\nof hybrid implicit-explicit representations, inducing more\nspatially localized gradient changes for faster training and\nimproved quality. Most recently, Kerbl et al. [11] popu-\nlarized 3D Gaussians as an innovative, explicit scene rep-\nresentation. In this work, we incorporate a 3D Gaussians\nrepresentation and regularized score distillation sampling\n(SDS) during training. This integration promotes fast con-\nvergence and enhances the overall quality of the generated\n2\nscenes. We diverge in a few details, such as using diffuse\ncolor without the spherical harmonics, and we adopt a cus-\ntomized initialization and density control strategy. Parallel\nefforts such as GSGEN [4], DreamGaussian [36] and Gaus-\nsianDreamer [43] have concurrently chosen 3D Gaussians\nas the representation. However, GSGEN [4] and Gaussian-\nDreamer [43] both require an additional 3D prior during\ntraining. DreamGaussian [36] uses 3D Gaussians only as\ncoarse initialization for extracting a mesh, whereas we pro-\nduce high quality 3D Gaussians directly.\nImage Generative Models.\nGenerative models for im-\nages have been an active area of research, leading to sig-\nnificant advances in the generation of realistic and high-\nquality 2D content.\nEarly approaches like Variational\nAutoencoders (VAEs) [12], Generative Adversarial Net-\nworks (GANs) [5], and Normalizing Flows [13] laid the\nfoundation for this field. In recent years, diffusion mod-\nels [8, 33, 34] have demonstrated exceptional capabilities\nin generating large-scale, high-fidelity images with precise\ntextual control over content and style. In this work, we aim\nto ensure a robust and stable training process with the SDS\nloss. To accomplish this, we incorporate both an image-\nspace diffusion model, DeepFloyd IF [14], and a latent-\nspace diffusion model, Stable Diffusion [29]. This strate-\ngic combination is employed due to the distinct yet com-\nplementary guidance these models offer in the context of\ntext-to-3D generation.\n3. Preliminaries and Notation\nIn this section we briefly introduce the background on both\nScore Distillation Sampling (SDS) and 3D Gaussians.\nScore Distillation Sampling (SDS).\nSDS is a loss in-\ntroduced in DreamFusion [25] for generating a 3D scene\nmodel (such as a NeRF [20]) from a text prompt y us-\ning a pretrained 2D diffusion model. Starting with a ran-\ndomly initialized scene model, parameterized by \u03b8, we iter-\natively sample random viewpoints \u03c0 facing the object, and\nrender an RGB image x using differentiable rendering, i.e.\nx = g(\u03b8, \u03c0). This rendered RGB image x is treated as an\nimage to be denoised with a pretrained 2D diffusion model\nto obtain an improved image that better aligns with the text\nprompt. The image x is perturbed with additive Gaussian\nnoise \u03f5 \u223c N(0, 1) such that\nxt = \u221a\u00af\u03b1tx +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere the noise hyperparameter t determines the magni-\ntude of \u00af\u03b1t, predefined through a fixed variance schedule.\nThe diffusion network typically predicts the added noise \u02c6\u03f5.\nWhile the diffusion process is iterative, Eq. (1) suggests a\none-step prediction of the denoised image as\n\u02c6x(xt; t, y) = xt \u2212 \u221a1 \u2212 \u00af\u03b1t\u02c6\u03f5(xt; t, y)\n\u221a\u00af\u03b1t\n.\n(2)\nNote that these equations are based on Equations 4 and 15\nin the DDPM paper [8].\nThe DreamFusion authors find that omitting the poorly\nconditioned diffusion network Jacobian term from the typi-\ncal diffusion training loss gradient gives a more stable gradi-\nent for backprogation to the current scene model, resulting\nin the SDS loss gradient\n\u2207\u03b8\u2113SDS (x = g(\u03b8, \u03c0)) \u225c Et,\u03f5\n\u0014\nwt (\u02c6\u03f5(xt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n\u0015\n.\n(3)\nIn DreamFusion, this is shown to be the gradient of a\nweighted probability density distillation loss. In Sec. 4.1,\nwe explore a more intuitive interpretation of the SDS loss\nthat leads to a natural tool for visualization.\n3D Gaussian Splatting\n3D Gaussians is an explicit 3D\nrepresentation popularized by [11], where the scene is com-\nprised of a large set of semitransparent anisotropic 3D\nGaussians. These Gaussian primitives are geometrically pa-\nrameterized by covariance (or equivalently scale and rota-\ntion) and position, with appearance parameterized by color\nand opacity. This representation has been shown to achieve\nremarkable results in the area of novel-view synthesis, with\nsignificantly higher quality and rendering speed compared\nto previous volumetric methods based on radiance fields.\nTo render 3D Gaussians, each primitive is projected into\na screen space 2D Gaussian and sequentially rasterized in\na back-to-front manner using alpha-blending. For screen-\nspace positions \u00b5i, screen-space covariances \u03a3i, colors ci,\nand opacities \u03c3i, the per-primitive alpha values and the final\ncomposited rendered color at pixel position x are\n\u03b1i(x) = \u03c3ie\u2212 1\n2 (x\u2212\u00b5i)T \u03a3\u22121\ni\n(x\u2212\u00b5i)\nC(x) =\nX\ni\nci\u03b1i(x)\nY\nj<i\n(1 \u2212 \u03b1j(x))\nThis rendering process is fully differentiable (given a differ-\nentiable sorting subroutine), enabling its use as a represen-\ntation for text-to-3D generation.\n4. StableDreamer\nIn a nutshell, StableDreamer addresses both the common\nblurry appearance and the multi-face geometry problems in\nSDS training with three conceptually simple modifications:\n(1) time-annealing of noise levels for 2D diffusion, which\nreduces multi-face geometries; (2) a dual-phase training\nthat utilizes image-space diffusion for accurate geometry\n3\nFigure 2. Our pipeline, StableDreamer, is an iterative optimization framework for creating anisotropic 3D Gaussians from text prompts.\nIt begins with a text prompt as input, which is then processed by a fixed, pretrained text encoder to generate a text embedding. This\nembedding serves as conditioning input for our pretrained diffusing U-nets. During each iteration, we randomly sample a viewpoint and\nrender the 3D Gaussians into an RGB image , which is subsequently input into the U-net for denoising and enhancement. The discrepancies\nbetween the denoised images and the originally rendered images are utilized as gradients for updating the anisotropic 3D Gaussians.\nand subsequently a latent-space diffusion for vibrant and\nsharp appearances; and (3) integration of 3D Gaussians\nwith regularization and density control that aims to improve\nmodel capacity for local details and transparent objects.\n4.1. Inspecting and Taming SDS Loss\nA key challenge of optimization with the SDS loss is\nthe noisy gradients inherent in the formulation.\nTo ad-\ndress this, we first propose a novel interpretation that\nlinks it to NeRF reconstruction (specifically, Instruct-\nNeRF2NeRF [7]). This theoretical connection leads to two\npractical benefits: an annealing strategy for noise levels to\nimprove convergence and a new visualization tool for in-\nspecting the training dynamics of SDS.\nThe SDS Generative Prior and NeRF Reconstruction.\nIn the DreamFusion training paradigm, the 3D scene rep-\nresentation is treated as an image generator while the SDS\nloss is treated as a prior over the generated images. While\nthis probability-based interpretation allows the use of statis-\ntical tools (e.g. [42]), a more practical lens is suggested in a\ndifferent related work. Instruct-NeRF2NeRF [7] is a recent\nwork that also uses generative 2D models, albeit for a style\ntransfer application rather than text-to-3D generation. In\nthis work, the usual supervised reconstruction framework is\nused where a set of ground truth images is compared against\na rendering from the current scene model. During train-\ning, Instruct-NeRF2NeRF uses the generative model to it-\neratively replace individual ground truth images with results\nfrom the 2D image generator (which may not be multiview-\nconsistent) based on the current rendering result from that\nviewpoint. The authors note that their training process can\nbe interpreted as a variant of SDS. Here we make this con-\nnection explicit:\nProposition 1. Training a 3D scene representation with the\nSDS generative prior is mathematically equivalent (up to\nscale) to using L2 reconstruction loss against images gen-\nerated from the 2D generator.\nProof. Without loss of generality, consider the SDS loss\nwith an image-space diffusion model without classifier-free\nguidance. We use Eqs. (1) and (2) to expand the noise resid-\nual:\n\u02c6\u03f5(xt; t, y) \u2212 \u03f5 = xt \u2212 \u221a \u00af\u03b1t \u02c6x(xt; y, t)\n\u221a1 \u2212 \u00af\u03b1t\n\u2212 \u03f5\n=\n\u221a \u00af\u03b1tx + \u221a1 \u2212 \u00af\u03b1t\u03f5 \u2212 \u221a \u00af\u03b1t \u02c6x(xt; y, t)\n\u221a1 \u2212 \u00af\u03b1t\n\u2212 \u03f5\n=\n\u221a \u00af\u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n(x \u2212 \u02c6x(xt; y, t))\nThen, the gradient of the SDS loss is implemented as\n\u2207\u03b8\u2113SDS (x = g(\u03b8, \u03c0)) \u225c w(t) (\u02c6\u03f5(xt; y, t) \u2212 \u03f5) \u2202x\n\u2202\u03b8\n= w(t)\n\u221a \u00af\u03b1t\n\u221a1 \u2212 \u00af\u03b1t\n(x \u2212 \u02c6x(xt; y, t)) \u2202x\n\u2202\u03b8 ,\nwhich is exactly the gradient of a scaled L2 loss\n\u2113L2(x, \u02c6x) = \u03b2(t)\n2 \u2225x \u2212 \u02c6x\u22252 between the current rendering\nx and ground truth image \u02c6x(xt; y, t), with \u03b2(t) = w(t)\u221a \u00af\n\u03b1t\n\u221a1\u2212\u00af\u03b1t .\nFor latent-space diffusion models, a similar line of reason-\ning shows that SDS loss is instead equivalent to a latent-\nspace L2 loss.\nAnnealing of Noise Level.\nThe above discussion estab-\nlishes a novel perspective where the one-step denoised im-\nage \u02c6x, as defined in Eq. (2), is conceptualized as the ground\ntruth image in the context of NeRF reconstruction. This\ninsight yields significant implications for the noise level\nscheduling in the 2D diffusion process. Particularly, to en-\nsure effective convergence during SDS training, it is crucial\n4\nthat the variance of these ground truth images starts large\nand decreases as training advances. To achieve this, we\ndynamically adjust the noise distribution\u2019s upper and lower\nlimits, progressively narrowing the range with training iter-\nations. We use a piecewise linear schedule for the upper and\nlower bounds that converge by the end of the training. Guid-\ning this noise magnitude is critical, since excessive noise\nleads to larger gradient magnitudes (equivalent to having\na changing ground truth), which can lead to worse model\nconvergence as shown later in Sec. 5.3. Incidentally, Pro-\nlificDreamer [42] proposes a similar but simpler annealing\nstrategy, reducing noise level after initial iteration steps.\nVisualization of Supervision Signals.\nA second advan-\ntage of implementing the proposed SDS loss reparameter-\nization lies in the enhanced interpretability of the training\nprocess. Through the visualization of the pseudo-ground-\ntruth image \u02c6x throughout the training phase, we gain in-\nsights into the direct influence of different hyperparameters\non target images. This capability empowers us to devise a\nmore robust training scheme, effectively taming the inher-\nent noise in SDS loss for text-to-3D tasks.\nA common challenge for 3D generation from text is the\ntendency for these systems to form objects with multiple\nfaces. By examining the latent images we find a relation-\nship between the multi-face problem and the SDS noise pa-\nrameter. Figure 3 shows the predicted original images \u02c6x\nfrom two training runs with different noise levels. For the\nrun with larger noise the system is more likely to hallucinate\na face on the back of the dog\u2019s head. Since each iteration\nis conditioned on the previous state, repeated selection of\nlarge noise values can cause the model to converge to a ge-\nometry with many faces. On the flip side, using lower noise\nlevels reduces the signal to the optimization as the latent\nimages do not change between iterations. Taken together,\nthese results suggest we should use an annealing strategy\nfor the added noise where it begins with a larger range and\nnarrows as the training progresses.\nSimilarly, the visualizations of the one-step denoised im-\nage \u02c6x for various guidance scales in Fig. 4 provide insight\ninto the effect of the guidance scale hyperparameter. Lower\nvalues lead to smooth images lacking fine details, while\nlarger values hallucinate high-frequency details and over-\nsaturated colors. This can lead to fake-looking images as\nshown in Sec. 5.3. While the effect this parameter is al-\nready understood, this simple example highlights the in-\nsights made possible by this reparameterization.\n4.2. A Tale of Two Diffusions: Image vs. Latent\nThe current landscape of diffusion models in the literature\nbifurcates into two categories: image-space diffusion and\nlatent-space diffusion. Image-space models, such as Deep-\nFloyd [14] and Imagen [30], directly apply noise to the im-\nages. In contrast, latent-space models like Stable Diffu-\nsion [24, 29] necessitate an encoder-decoder pair that maps\nbetween the image and latent spaces, applying noise only\nin the latent domain. Our empirical analysis reveals that\nthese two model types exhibit different guidance directions\nfor text-to-3D. We propose an effective two-stage train-\ning framework that leverages their distinct properties. As\nshown in Fig. 5, the proposed framework can produce sharp\ntexture and detailed geometry. Incidentally, Magic3D [15]\narrives at a similar training strategy but mainly for reasons\nof speed and resolution, rather than quality.\nImage-space diffusion for geometry reconstruction.\nFor the first stage of training, we propose to use the image-\nspace model, DeepFloyd [14], to train the 3D model. The\nprimary goal at this stage is to converge to a reason-\nable rough geometry, so that a detailed appearance can be\nlearned later in the optimization, as shown in the first row\nof Fig. 5. Therefore, in this stage, we only use the coarse\nDeepFloyd model, operating at 64 \u00d7 64 resolution. At this\nstage, all the parameters of the 3D models are learnable. A\nlow learning rate is used for the geometry as it converges\n(see Appendix C for more detailed analysis).\nLatent-space diffusion for appearance enhancement.\nWhile the coarse reconstruction successfully yields a 3D\nmodel with satisfactory geometric accuracy, it tends to fall\nshort in terms of visual quality due to its use of low-\nresolution 2D image supervision at 64 x 64 resolution. The\nprimary objective of the refinement stage is to significantly\nenhance the visual fidelity of the 3D model, as shown in\nthe second row of Fig. 5.\nTo achieve this, we employ\na latent-space diffusion model, Stable Diffusion (SDv2.1-\nbase) [29] trained with 512 \u00d7 512 resolution images. As\nshown in Appendix D, the image-space diffusion models\nare not suitable to get the detailed appearance for the 3D\nmodel (even for a high-resolution model like DeepFloyd\nwith super-resolution modules). We hypothesize that this is\ndue to view-inconsistent pixel-level guidance, resulting in\na blurred model and the loss of appearance detail. In con-\ntrast, the guidance from the latent-space diffusion model is\nless sensitive to this issue, since the loss is calculated in the\nlatent space after feature compression from the image en-\ncoder. As a result, with the guidance from Stable Diffusion\nat the second stage, we largely increase model fidelity for\nboth appearance and geometry.\n4.3. Integrating 3D Gaussians\nThe aforemention training scheme provides stablized train-\ning with NeRF, yet there is potential for further enhance-\nment in the finer details. 3D Gaussians offer advantages\nsuch as rapid rendering speeds and enhanced local repre-\nsentation over other NeRF representations. However, they\n5\nFigure 3. a): Per Proposition 1, the reformulated loss equation enables visualization of the one-step denoised image \u02c6x which allows us\nto observe the effect of modifying the level of noise being injected into xt in Eq. (1) and subsequently \u02c6x in Eq. (2). Less noise produces\nimages closer to the input image x while larger noise levels produce more variation.\nb): Two training runs are compared, one biased to sample lower noise (top) and one biased to sample higher noise (bottom). Two views\nare rendered at both an early iteration i and later iteration i + 2000. From a), high noise samples are associated with a face incorrectly\nhallucinated on the back of the dogs head. Unsurprisingly, the model with larger noise ends up converging to a multi-faced dog.\nFigure 4. Understanding the impact of guidance scale on the ap-\npearance via visualizing the one-step denoised images \u02c6x during\ntraining. Left-to-right: Guidance scale 10, 20, 35, and 100. As\nthe guidance scale increases, so does the high frequency detail and\ncolor, eventually leading to an unrealistic image.\nFigure 5. Results from two training stages. Stage 1 (top): image-\nspace diffusion (DeepFloyd) produces accurate geometry at the\ncost of muted colors. Stage 2 (bottom): we finetune with latent-\nspace diffusion (Stable Diffusion) to enhance the appearance.\nare sensitive to the hyper-parameters and training strate-\ngies. In fact, directly substituting this representation into\nour existing training frameworks leads to low-quality re-\nsults and artifacts, likely due to the mismatch between noisy\nSDS loss and the localized nature of 3D Gaussians. Specif-\nically, we observe that despite having on average 10x larger\ngradient magnitude compared to other learnable parameters\n(e.g., colors, scales, rotation), the position variables exhibit\na \u201drandom walk\u201d behavior without converging to a high-\nquality geometry. This observation motivates specialized\n3D Gaussians training strategies around initialization and\ndensity control.\nInitialization.\nIn 3DGS [11],\nStructure-from-Motion\n(SfM) is used to initialize the Gaussian locations for scene\nreconstruction. However, this method cannot be used in\ntext-to-3D generation. Thus, we use a simple alternate ap-\nproach that has proved compatible with a wide range of text\nprompts. To start, the centers of the Gaussian primitives\nare randomly sampled with a uniform distribution over a\nvolume. While the positions are uniformly distributed, the\nopacity of each point is initialized relative to its proximity\nto the center of the volume. More specifically, the initial\nopacity linearly decays with distance from the origin. This\nsimple heuristic helps with convergence since the majority\nof generated objects have most of their density closer to the\ncenter of the scene.\n6\nDensity control.\nOur experiments show that the position\nlearning of 3D Gaussians is hard to tune and easily diverges\nwith large learning rates due to the noisy signal from SDS\nloss. To stabilize training, a small learning rate is required\nfor the position variables to avoid moving too far from their\ninitial locations. Consequently, we cannot solely rely on\nposition learning to produce fine geometry. Therefore, we\nturn to density control for geometry construction. Specif-\nically, after initialization, we apply periodic densification\nand pruning, gradually adding new points in order to pro-\nduce finer geometry and appearance. Additionally, we find\nthat resetting the opacities to near zero at early training\nstages helps reduce floaters and bad geometry. Please re-\nfer to Appendix B for details of our implementation.\n5. Experiments\nWe compare StableDreamer against several state-of-the-art\ntext-to-3d methods on the overall quality of the synthesized\n3D geometry and appearance as well as memory usage dur-\ning training and rendering speed. More ablation studies can\nbe found in our appendices.\n5.1. Comparison To Prior Methods\nAs shown in Fig. 6, StableDreamer achieves state-of-the-\nart results compared to baseline works including Dream-\nFusion [25], Magic3D [15], GSGen [4], and Prolific-\nDreamer [42]. StableDreamer\u2019s initial coarse geometric op-\ntimization converges to accurate geometry, greatly reducing\nthe occurrence of multi-faced geometry commonly seen in\nthe baseline methods. Tab. 1 presents an efficiency anal-\nysis of our method in comparison to baseline approaches.\nOur method, employing 3D Gaussians, renders at > 30FPS\nwhile maintaining reasonable training time and minimal\nGPU memory usage.\nNotably, Magic3D tends to pro-\nduce over-saturated color while ProlificDreamer and GS-\nGen achieve similar detailed textures but consistently pro-\nduce multi-faced or otherwise incorrect geometries (addi-\ntional visualization in Appendix A).\n5.2. Generalization Across 3D Representations\nWe showcase the efficacy of 3D Gaussians compared to vol-\numetric radiance fields, specifically iNGP [21]. iNGP [21]\nwas widely adopted in previous work [2, 15, 42] thanks to\nits speed compared to classical MLP-based implicit neu-\nral representations [25]. To ensure an equitable evaluation,\nboth 3D Gaussians and iNGP were trained with the pro-\nposed training scheme. The qualitative results are reported\nin the two rightmost columns in Fig. 6. Our training scheme\nis generalizable beyond 3D Gaussians and works well on\niNGP. Overall, 3D Gaussians still produce better local de-\ntails than iNGP, supporting our choice of 3D representation.\nFor detailed structures (e.g. hairs from corgi and bunny),\niNGP typically produces either blurry or noisy surface tex-\ntures, while 3D Gaussians generate realistic detailed struc-\ntures. iNGP also results in temporal aliasing and flickering,\nwhich is visible only in videos.\nQuantitative efficiency measurements,\npresented in\nTab. 1, indicate the advantages of 3D Gaussians.\nWith\na similar parameter count, 3D Gaussians utilize 82%\nless GPU memory and render 6 times faster faster than\niNGP [21].\nInterestingly, training time between the two\nmethods remained comparable, largely owing to the fact\nthat the 2D diffusion models constitute the dominant time-\nconsuming component in the forward process, especially in\nthe coarse stage when rendering resolution is low.\nTraining Time\nPeak Memory Usage\nRender Speed\n(min)\n(GB)\n(fps)\nDreamFusion-iNGP (12.6M) [25]\n40\n17.6\n14.0\nMagic3D (12.6M) [15]\n75\n16.6\n9.4\nProlificDreamer (12.6M) [42]\n277\n31.8\n10.8\nGSGen (4.7M) [4]\n228\n9.9\n52.5\nOurs-iNGP (12.6M)\n81\n31.9\n7.38\nOurs-3DGS (14M)\n97\n5.7\n46.0\nTable 1. Comparison of parameter count, training time, memory\nusage, and render speed. The evaluations are performed on a single\nNVIDIA V100 GPU. DreamFusion and Magic3D are not open-\nsourced so we use the Threestudio implementation [6].\n5.3. Ablation on SDS Annealing\nA critical aspect of the optimization processes described in\nFig. 2 is the addition of noise to the image generated by the\n2D diffusion model. Noisy gradients are a common issue\nwith SDS loss and, as shown in Sec. 4.1, crafting a schedule\nfor the noise bounds is important for consistently converg-\ning to good results. Our results shown in Fig. 7 match what\nwe find in our analysis of the visualizations of the one-step\ndenoised images and demonstrate that high noise levels dur-\ning training tend to produce artifacts and multi-faced geom-\netry. Intuitively, as the model converges, less noise should\nbe added each step once the optimization has settled into a\nsingle local minimum.\n6. Failure Analysis\nWhile our strategies are shown to reduce multi-face geome-\ntry, there remain scenarios where these methods do not yield\nsatisfactory results, as illustrated in Fig. 8. For instance,\nsome failures originate from the 2D diffusion model\u2019s in-\nability to accurately interpret the prompt, while others pro-\nduce floating or blurry geometries.\nMulti-face geometry\nalso still exists for certain prompts.\n7. Conclusion\nIn this work, we introduce StableDreamer, a text-to-3D\nframework that addresses the blurry appearance and multi-\nfaced geometry problems that are commonly seen in prior\n7\nFigure 6. Comparison against prior methods. Prior methods typically have problems such as blurriness (DreamFusion [25]), multi-face\ngeometry (Magic3D [15], GSGen [4], and ProlificDreamer), over-saturation in color (Magic3D [15]), cartoony appearances, or mismatch\nbetween content and text prompts. StableDreamer (including both iNGP [21] and 3D Gaussians [11] geometry primitives) achieves accurate\ngeometry representation with fine details while preserving a realistic appearance. Results for DreamFusion and Magic3D use the open-\nsource Threestudio implementation [6] since the authors have not released their code. Additional visualization are shown in Appendix A.\nFigure 7. The upper and lower bounds of the noise being injected\ninto xt in Eq. (1) change as a function of the training iteration.\nLarger noise levels give more high-frequency texture detail, but\nalso more artifacts including multiple faces. The converged model\nis shown from the front (top row) and back (bottom row), with\nincreasing levels of noise left-to-right.\nmethods. Our analysis reveals that the Score Distillation\nSampling loss can be reparametrized as a supervised re-\nconstruction loss using denoised images as pseudo-ground-\ntruth. This finding leads to intuitive ways to visually in-\nspect the training dynamics and the formulation noise level\nannealing strategies that reduce the occurrence of multi-\nFigure 8. Failure cases: \u201cAn astronaut riding a kangaroo\u201d with\nthe astronaut being erroneously merged in; \u201cA teddy bear pushing\na shopping cart full of fruits and vegetables\u201d with floaters; and\n\u201cMichelangelo style statue of dog reading news on a cellphone\u201d\nwith multi-face and blurry geometries.\nface artifacts. Empirical results show that image-space dif-\nfusion assists in generating better geometry while latent-\nspace diffusion produces vibrant and detailed colors, in-\nspiring our dual-phase training scheme. Notably, both the\nreparametrization and training schemes are agnostic to the\nunderlying 3D representations and generalize beyond 3D\nGaussians. However, to enhance detail and construction fi-\ndelity, we adopt a 3D Gaussians as our core 3D represen-\ntation, including a number of strategies involving initial-\nization and density control to enhance the robustness and\nconvergence speed toward accurate geometric representa-\ntions. Our empirical study demonstrates the superior quality\nof our method in comparison to previous approaches.\n8\nReferences\n[1] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022.\n2\n[2] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation. In ICCV, 2023. 2, 7\n[3] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling. CVPR, 2019. 2\n[4] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using\ngaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\n2, 3, 7, 8\n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 3\n[6] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 7, 8, 2\n[7] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions. In ICCV, 2023. 1, 4\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 3\n[9] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1\n[10] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski.\nNoise-free score distillation.\narXiv preprint\narXiv:2310.17590, 2023. 2\n[11] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics,\n2023. 2, 3, 6, 8, 1\n[12] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[13] Ivan Kobyzev, Simon JD Prince, and Marcus A Brubaker.\nNormalizing flows: An introduction and review of current\nmethods. IEEE TPAMI, 2020. 3\n[14] Mikhail Konstantinov, Alex Shonenkov, Daria Bakshan-\ndaeva, and Ksenia Ivanova.\nDeepfloyd ai research band.\nhttps://www.deepfloyd.ai/deepfloyd- if,\n2023. 3, 5\n[15] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 2, 5, 7, 8\n[16] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural vol-\numes: Learning dynamic renderable volumes from images.\nACM TOG, 2019. 2\n[17] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space.\nIn CVPR,\n2019. 2\n[18] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In CVPR, 2022. 2\n[19] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting Scenes as Neural Radiance Fields for View\nSynthesis. In ECCV, 2020. 2\n[20] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 2, 3\n[21] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Trans. Graph., 2022. 2, 7,\n8\n[22] OpenAI. Chatgpt: A pre-trained language model. https:\n//www.openai.com/research/chatgpt, 2021. 1\n[23] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn CVPR, 2019. 2\n[24] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis. arXiv:2307.01952,\n2023. 5\n[25] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. ICLR,\n2022. 1, 2, 3, 7, 8\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 2\n[27] Aditya Ramesh, Mukul Goyal, Oren Dovrat, Jitendra Ke,\nShweta Lu, Alessandro Sordoni, Linjie Kang, Adam Ng,\nTim Smith, Xi Choi, et al. Dall\u00b7e: Creating images from\ntext. arXiv preprint arXiv:2102.12092, 2021. 1\n[28] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps, 2021. 2\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 1, 2, 3,\n5\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NeurIPS, 2022. 1, 2, 5\n[31] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\n9\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. In NeurIPS, 2022. 2\n[32] Vincent Sitzmann, Michael Zollhoefer, and Gordon Wet-\nzstein.\nScene representation networks:\nContinuous 3d-\nstructure-aware neural scene representations.\nIn NeurIPS,\n2019. 2\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 3\n[34] Yang Song and Stefano Ermon. Generative modeling by es-\ntimating gradients of the data distribution. NeurIPS, 2019.\n3\n[35] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 2\n[36] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for effi-\ncient 3d content creation. arXiv preprint arXiv:2309.16653,\n2023. 3\n[37] Ayush Tewari, Justus Thies, Ben Mildenhall, Pratul P. Srini-\nvasan, Edgar Tretschk, Yifan Wang, Christoph Lassner,\nVincent Sitzmann, Ricardo Martin-Brualla, Stephen Lom-\nbardi, Tomas Simon, Christian Theobalt, Matthias Nie\u00dfner,\nJonathan T. Barron, Gordon Wetzstein, Michael Zollh\u00a8ofer,\nand Vladislav Golyanik.\nAdvances in neural rendering.\nCGF, 2022. 2\n[38] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288, 2023. 2\n[39] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kin-\ndermans, Hernan Moraldo, Han Zhang, Mohammad Taghi\nSaffar, Santiago Castro, Julius Kunze, and Dumitru Erhan.\nPhenaki: Variable length video generation from open domain\ntextual description. arXiv preprint arXiv:2210.02399, 2022.\n1\n[40] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In CVPR,\n2023. 2\n[41] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nIn NeurIPS, 2021. 2\n[42] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. In NeurIPS, 2023. 2, 4, 5, 7\n[43] Taoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng\nZhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-\ndreamer: Fast generation from text to 3d gaussian splatting\nwith point cloud priors. arxiv:2310.08529, 2023. 2, 3\n[44] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. PlenOctrees for real-time rendering of\nneural radiance fields. In ICCV, 2021. 2\n[45] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021. 2\n10\nStableDreamer: Taming Noisy Score Distillation Sampling for Text-to-3D\nSupplementary Material\nAppendix A. Additional Visualization\nFig. app-1 shows additional result comparison with differ-\nent view of angles. StableDreamer is able to generate the\n3D model with both detailed texture and geometry com-\npared to the baseline methods.\nAppendix B. Density Control Setup\nFig. app-2 shows an illustration of our density control\nsetup.\nTo assist with the convergence of the geometry\nof the scenes, we use the following schedule to modify\nthe 3D Gaussians.\nFirstly, we randomly initialize 1000\npoints based on the aforementioned initialization scheme.\nAs shown in Appendix C, we intend to use less starting\npoints to reduce the floaters and produce better geometry.\nThen, for every 500 iterations we apply a densification pro-\ncess based on the original Gaussian splatting method [11].\nMore specifically, we split and clone the Gaussians when\nthe magnitude of the position gradient is over a threshold.\nBy doing so, we can allow the representation to better cap-\nture fine details. Please refer to the original paper [11] for\nmore details of the densification algorithm. Note that we\nstart this densification process after 100 iterations. This is to\nmake sure the averaged positional gradients get stabilized.\nSimilar to the original method, we also apply periodic prun-\ning immediately after densification to remove the Guassians\nwith smaller opacities or large 2D projected area. In ad-\ndition, as shown in the ablation study in Appendix C, we\nfound that resetting the opacities at the early training stage\ncan help to reduce the floaters in the final result. In our\nsetup, we choose to reset the opacities at the 1000th itera-\ntion. This is due to the positions and other attributes of the\nprimitives have begin to converge before 1000 iteration, and\nresetting this parameters allows for a more robust conver-\ngence by preventing the optimization from getting caught\nin the initial local minima (e.g., floaters or bad geometry).\nThe density control process ends at 12000 iterations; we\nthen proceed with 3000 fine-tuning iterations with a fixed\nnumber of 3D Gaussians to smooth out the spiky artifacts\nintroduced by densification.\nAppendix C. Ablation on Density Control\nAs shown in Figure app-2, to assist with the convergence\nof the geometry of the scenes, we use the following sched-\nule to modify the 3D Gaussians. Firstly, we randomly ini-\ntialize 1000 points based on the aforementioned initializa-\ntion scheme. Then, every 500 iterations we apply a den-\nsification process based on the original Gaussian splatting\nmethod [11].\nMore specifically, we split and clone the\nGaussians when the magnitude of the position gradient is\nover a threshold. By doing so, we can allow the representa-\ntion to better capture fine details. Please refer to the original\npaper [11] for more details of the densification algorithm.\nNote that we start this densification process after 100 itera-\ntions. This is to make sure the averaged positional gradients\nget stabilized.\nInitialization.\nAs shown in Fig. app-3, starting with fewer\npoints and annealing the initial opacity of the Gaussians re-\nsults in the best geometry. More specifically, comparing the\nresults from the same row, the results with opacity decay in\nthe right column (i.e., linearly decaying opacity based on\nthe distance to the origin) have less floaters. Furthermore,\ncomparing the results from the same column, with more\nstarting points (from top to bottom), there are more floaters\nand the training become unstable if we initialize with a large\namount of points due to the noisy signal from SDS loss (see\nthe figure on the bottom left).\nDensity control and position learning\nIn our experi-\nments, we found that resetting opacity for all of the Gaus-\nsians during densification can help to reduce floaters. As\nshown in Figure app-4, with opacity reset, there are much\nless floaters in the final result (bottom) compared with the\ncase without opacity reset (top). Note that, in our experi-\nment, we choose to reset the opacity to 0.005 at the iteration\nof 1000 based on grid search.\nBesides opacity reset, we also found the representation\nof 3D Gaussians is very sensitive to the learning rate of\nthe positions (i.e., xyz coordinates).\nAs shown in Fig-\nure app-5, with a slightly large learning rate (0.0064), the\ngeometry gets diverged due to the diversification process.\nThis is aligned with the result from original 3D Gaussians\npaper [11].\nEven under their reconstruction task, which\nhas more regularization (i.e. image supervision) comparing\nwith our generation task, the original method still uses a\nreally small position learning rate as 0.00064, which essen-\ntially does not allow the centroids of the 3D Gaussians mov-\ning much. Instead, the fine geometry is forced to be learned\nby density control (densification and pruning).\nAppendix D. Ablation on Two-Stage Training\nBenefit from the coarse-to-fine training paradigm.\nFig. app-6 shows the first stage result (i.e., training from\nscratch) using Stable Diffusion model (left) and DeepFloyd\n1\nFigure app-1. Multi-view comparison against prior methods. Each column shows the generated object from 3 different views roughly\nequally spaced about the vertical axis. GSGen and ProlificDreamer struggle to produce 3D view-consistent geometry. DreamFusion and\nMagic3D do not have released code so we use the open-source Threestudio implementation [6].\n2\nSmoothing\nDensi\ufb01cation End \n@12000\nTraining End  \n@15000\nDensi\ufb01cation\n200\n800\n1200\n12000\n15000\nOpacity Reset \n@1000\nTraining Start \n@0\nFigure app-2. Density control schedule. We randomly initialize points and apply density control (densification and pruning) to obtain\nthe coarse geometry and texture. Then an additional smoothing step is followed in order to remove the spiky artifacts as introduced by\ndensification.\nmodel (right) for both of the geometry primitives 3D Gaus-\nsians and iNGP. Although there is a sharper texture from\nthe high-resolution Stable Diffusion model, the overall ge-\nometry is worse than the result from the coarse DeepFloyd\nmodel. As shown in Fig. app-7, after finetuning with the\ndiffusion models trained with high resolution images (Sta-\nble Diffusion or DeepFloyd with super-resolution module),\nwe can get a 3D model with much higher fidelity, while also\nkeeps the good geometry that is learned from the first stage.\nFigure app-3.\nAblation study for 3D Gaussians initialization\nschemes with prompt: a zoomed out DSLR photo of the Sydney\nopera house, aerial view. Left Column: Fix initial opacity levels.\nRight Column: Opacity initialization based on distance to center\nof scene. Top Row: 1K starting points. Middle Row: 10K starting\npoints. Bottom Row: 100K starting points.\nBenefit of the use of latent-space diffusion model in the\nsecond stage learning.\nAs shown in Fig. app-7, when\nfinetuning from the first stage model trained with the coarse\nDeepFloyd model, both Stable Diffusion and DeepFloyd\nwith super-resolution module can achieve better geometry\nand texture, as they are trained with high resolution images.\nHowever, if we compare the resulting images, (e.g., the tex-\nture of basket and the fine hairs from bunny) the Deep-\nFloyd result is lacking details, while the Stable Diffusion\nmodel can produce both better texture and sharper geom-\netry. As mentioned earlier, this is due to the image-based\nguidance (i.e., DeepFloyd) has more adverse effect to the\nview consistency of the 3D model, while the guidance from\nthe latent-space diffusion model (i.e., Stable Diffusion) is\nless sensitive due to the feature compression from its image\nencoder.\nFigure app-4. Resetting opacity during densification can help re-\nduce floaters as shown in the opacity renderings on the right. Top:\nwithout opacity reset; bottom: with opacity reset.\nFigure app-5. Using an inappropriate learning rate for position\nupdates can readily lead to geometric divergence.\n3\nFigure app-6. Result from different diffusion models when training from scratch.\nFigure app-7. Result from different diffusion models when finetuning from the first stage model.\n4\n"
  },
  {
    "title": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models",
    "link": "https://arxiv.org/pdf/2312.02949.pdf",
    "upvote": "7",
    "text": "LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models\nHao Zhang\u2660\u22173, Hongyang Li\u2662\u2217, Feng Li\u26603, Tianhe Ren\u2020, Xueyan Zou\u00a73, Shilong Liu\u00b63,\nShijia Huang\u266f, Jianfeng Gao\u20212, Lei Zhang\u20202, Chunyuan Li\u20211, Jianwei Yang\u20211\n\u2660 HKUST\n\u2662 SCUT\n\u2021 Microsoft Research, Redmond\n\u2020 IDEA\n\u00a7 UW-Madison\n\u00b6 Tsinghua\n\u266f CUHK\n\u2217 Equal Contribution 1. Directional Lead 2. Equal Advisory Contribution 3. Work performed during an internship at Microsoft\nhttps://llava-vl.github.io/llava-grounding/\nAbstract\nWith the recent significant advancements in large multi-\nmodal models (LMMs), the importance of their grounding\ncapability in visual chat is increasingly recognized. Despite\nrecent efforts to enable LMMs to support grounding, their\ncapabilities for grounding and chat are usually separate,\nand their chat performance drops dramatically when asked\nto ground. The problem is the lack of a dataset for grounded\nvisual chat (GVC). Existing grounding datasets only con-\ntain short captions. To address this issue, we have created\nGVC data that allows for the combination of grounding and\nchat capabilities. To better evaluate the GVC capabilities,\nwe have introduced a benchmark called Grounding-Bench.\nAdditionally, we have proposed a model design that can sup-\nport GVC and various types of visual prompts by connecting\nsegmentation models with language models. Experimental\nresults demonstrate that our model outperforms other LMMs\non Grounding-Bench. Furthermore, our model achieves\ncompetitive performance on classic grounding benchmarks\nlike RefCOCO/+/g and Flickr30K Entities.\n1. Introduction\nWith the success of large language models (LLMs) like GPT-\n4 [25] and the open-sourced substitutes LLaMA [31], re-\nsearchers are eager to leverage their strong language capabil-\nities in the field of vision. This enthusiasm has led to a surge\nin the development of large multimodal models (LLMs). Pre-\nvious LMMs, such as LLaVA [18] and miniGPT-4 [49], have\ndemonstrated exceptional visual chat abilities by generating\nplausible responses based on images and user instructions.\nHowever, they often encounter challenges in providing re-\nsponses that exhibit a fine-grained understanding of images,\nincluding specific regions and alignment with related image\nregions\u2014this is often referred to as visual grounding.\nRecognizing the significance of visual grounding for\nLMMs, recent research efforts have focused on develop-\nGrounding\nVisual Chat\n22\n24\n26\n28\n30\n32\n34\n36\n38\n30\n40\n50\n60\n70\nLLaVA-G\n(Ours)\nCogVLM-Grounding\nminiGPT v2\nShikra\nBuboGPT\nCogVLM-Chat\n80\n0\nLLaVA\n0\nFigure 1. A comparison on the integrated ability of visual grounding and vi-\nsual chat of open-source LMMs on Grounding-Bench. LLaVA-G achieves a\ngood trade-off on both abilities simultaneously. For CogVLM [33], two dif-\nferent model checkpoints are released: CogVLM-Grounding is the ground-\ning model and CogVLM-Chat is the chat model. Grounding and Visual\nChat scores represent the F1 score and Chat scores of detailed descriptions\nin Table 4, respectively. Circle size indicates the model size.\ning grounding and referring capabilities for LMMs [2, 3,\n10, 33, 40]. While these models have achieved performance\ncomparable to specialized models [19, 21] on classic ground-\ning benchmarks such as RefCOCO [8] and Flickr30K [29],\nthey often treat grounding as a distinct task that requires\ncustomized prompts to initiate. Consequently, their text\nresponses undergo significant changes when tasked with\ngrounding.\nMost models, such as MiniGPT-v2 [2] and\nCogVLM-Grounding [33], can only generate short captions\nwhen performing grounding, as they are primarily trained\non grounding caption data like Flickr30K. As illustrated in\nFig.1, these earlier models struggle to excel simultaneously\nin both chat and grounding tasks. BuboGPT[47] maintains\nchat capability by leveraging an external grounding model\nfor grounding, but this approach can be constrained by the\nperformance of the language encoder in the grounding model.\nShikra [3] engages in referential dialog, which includes\ngrounded chat, but its performance is limited due to the\n1\narXiv:2312.02949v1  [cs.CV]  5 Dec 2023\nscarcity of available data. All existing LMMs [2, 3, 33, 40]\nonly support outputting coordinates as text, which restricts\nlocalization performance, and they do not support pixel-wise\ngrounding and referring. In summary, previous LMMs strug-\ngle to perform grounded visual chat effectively due to the\nscarcity of grounded visual chat data and suboptimal model\ndesigns. Furthermore, they lack the capability for pixel-wise\ngrounding and referring.\nTo address these challenges, we contribute to grounded\nvisual chat in three key areas: data creation, network architec-\nture, and benchmarking. When annotating grounding data,\nprevious methods such as Kosmos-2 [28] and GPT4ROI [46]\nrely on pretrained grounding models or detection models\nto predict bounding boxes based on existing captions. In\ncontrast, we label grounded visual chat data using human-\nlabeled object detection data [15].\nOur data creation process begins by leveraging GPT-\n4 [25], following the data creation method used in\nLLaVA [18]. We provide GPT-4 with chat data and ground-\ntruth instances, instructing it to match instances with noun\nphrases in the chat data. This approach benefits from the high\nquality of human-labeled instances and chat data generated\nby GPT-4, ensuring minimal noise in the data annotation\npipeline. In total, we annotated 150K grounded visual chat\ndata.\nIn terms of network architecture, we propose connecting\nthe output features of the Language Model (LLM) with a\ngrounding model to handle grounding tasks, relieving the\nlanguage model from the burden of vision localization tasks.\nFor this purpose, we use the open-set segmentation and\ndetection model OpenSeeD [44] as the grounding model,\nenabling both box and pixel-level grounding simultaneously.\nTo evaluate the capability of grounded visual chat, we\nintroduce the Grounding Bench, a benchmark that assesses\ngrounding and chat performances concurrently. Built upon\nthe foundation of LLaVA bench, our benchmark evaluates\nchat and phrase grounding in three contexts: conversation,\ndetailed description, and complex reasoning. Additionally,\nrecognizing that grounded detailed description is the most\nchallenging aspect of grounded visual chat, we propose\ngrounded recall and precision metrics. Grounded recall mea-\nsures the proportion of ground-truth instances correctly men-\ntioned and grounded, while grounded precision measures\nthe accuracy of groundings or predicted boxes. We also\ncalculate the F1 score, a combination of precision and recall.\nTo evaluate the correctness of semantic matching since the\nmodels generate free-form phrases, we rely on GPT-4.\nIn summary, our contributions are as follows:\n1. We introduce a data annotation pipeline to label high-\nquality Grounded Visual Chat (GVC) data. Leveraging\nhuman-labeled object detection data [15] and harness-\ning the robust matching capability of GPT-4 [27], we\nhave successfully labeled 150K GVC instances using the\ninput\noutput\ntext\nclick\nbox\nmark\ntext\nbox\nmask\nmark\nLLaVA [12]\n\u2713\n\u2713\nMiniGPT-4 [49]\n\u2713\n\u2713\nGPT4ROI [46]\n\u2713\n\u2713\n\u2713\nShikra [3]\n\u2713\n\u2713\nFerret [40]\n\u2713\n\u2713\n\u2713\n\u2713\nMiniGPTv2 [2]\n\u2713\n\u2713\n\u2713\nLLaVA1.5 [17]\n\u2713\n\u2713\n\u2713\nCogVLM-Grounding [33]\n\u2713\n\u2713\n\u2713\nLLaVA-G (Ours)\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nTable 1. A comparison of input referring and output grounding format of\nLMMs.\nLLaVA instruction tuning dataset.\n2. We present an end-to-end model, named LLaVA-\nGrounding (LLaVA-G for brevity), which connects a\nLarge Multimodal Model (LMM) with a grounding model\nto facilitate grounded visual chat. Our model supports\nboth object and pixel-level grounding, accommodating\nvarious visual prompts such as mark, click, box, and scrib-\nble. Table 1 demonstrates that our model offers a broader\nrange of input and output prompt types compared to other\nLMMs.\n3. We establish the Grounding-Benchbenchmark for evaluat-\ning grounded visual chat and propose an auto-evaluation\npipeline aided by GPT-4.\nThis benchmark assesses\ngrounded visual chat capabilities and provides perfor-\nmance metrics for other state-of-the-art methods.\n4. Through extensive experiments, we demonstrate that our\nmodel surpasses other grounding LMMs in terms of per-\nformance on Grounding-Bench, while also achieving\ncompetitive results on classic grounding benchmarks like\nRefCOCO/+/g and Flickr30K.\n2. Method\n2.1. Overview\nTo advance the development of grounded visual chat for\nLarge Multimodal Models (LMMs), we introduce a compre-\nhensive pipeline for labeling grounded visual chat data, a\ntailored modeling approach designed for the grounded visual\nchat task, and a benchmark for evaluating grounded visual\nchat performance, as illustrated in Figure 2. We will provide\nfurther details on these three components in the following\nsubsections.\nData Creation Pipeline\nMetrics & Benchmark\nLLaVA-G\n150K+ Data\nFigure 2. An overview of our main contributions. We use the data creation\npipeline to create training and test data. The training data is used to train\nour LLaVA-G. The test data is used to build our Grounding-Bench.\n2\nContext type 1: Boxes (for data annotation)\n1.person: [0.681, 0.242, 0.774, 0.694], 2.person: [0.63, 0.222, 0.686, 0.516],\n3.person: [0.444, 0.233, 0.487, 0.34], 4.backpack: [0.384, 0.696, 0.485, 0.914],\n5.backpack: [0.755, 0.413, 0.846, 0.692], 6.suitcase: [0.758, 0.413, 0.845, 0.69],\n7.suitcase: [0.1, 0.497, 0.173, 0.579], 8.bicycle: [0.282, 0.363, 0.327, 0.442],\n9.car: [0.786, 0.25, 0.848, 0.322], 10.car: [0.783, 0.27, 0.827, 0.335],\n11.car: [0.86, 0.254, 0.891, 0.3], 12.car: [0.261, 0.101, 0.787, 0.626]\nContext type 2: user responses (for data annotation)\nThe image is an underground parking area with a black sport utility vehicle (SUV)\nparked. There are three people in the scene, with one person standing closer to the left\nside of the vehicle, another person in the middle, and the third person on the right side.\nThey are all working together to pack their luggage into the SUV for a trip.\nResponse: grounded responses (for data annotation)\nThe image is an underground parking area with a (black sport utility vehicle) [10.car] (SUV) parked. There are (three people) [1.person,\n2.person, 3.person] in the scene, with (one person) [3.person] standing closer to the left side of the vehicle, (another person) [2.person] in the\nmiddle, and (the third person) [1.person] on the right side. They are all working together to pack (their luggage) [4.backpack, 5.backpack,\n6.suitcase, 7.suitcase] into the SUV for a trip.\nContext type 3: predicted grounded responses (for evaluation)\nThe depiction is of a below-ground parking facility, where a sleek, black vehicle [9.car] is situated. In the vicinity of this SUV, a trio of\nindividuals [1.person, 2.person, 3.person] is engaged in an activity: the first person [1.person] is adjacent to the left side of the vehicle, the\nsecond [2.person] is situated centrally, and the third [3.person] is near the right side. They are collaboratively arranging their travel bags in\nthe SUV, signaling the onset of an impending journey.\nResponse: TPpred and TPgt (for evaluation)\n\"a sleek, black vehicle\" [9.car] - Incorrectly referred.\n\"a trio of individuals\" [1.person, 2.person, 3.person] - 3 Correctly referred.\n\"the first person\" [1.person] - Incorrectly referred.\n\"the second\" [2.person] - Correctly referred.\n\"the third\" [3.person] - Incorrectly referred.\nThere are 4 correct references (TPpred) and 3 correctly referred entities (TPgt).\nTable 2. Illustrate the data annotation (top) and the evaluation (bottom) with language GPT4. The top table shows the contexts and responses for data\nannotation. The bottom table shows the contexts and responses for evaluating the recall and precision of grounded description. Note that the Context 1\nfor evaluation is same as that for data annotation. Note that the visual image is not used to prompt GPT4, we only show it here as a reference.\n2.2. Grounded Visual Chat Data Creation\nTo perform grounded visual chat (GVC) effectively, it is cru-\ncial to have high-quality data that encompasses both mean-\ningful conversations and accurate grounding. We have con-\nstructed our dataset based on LLaVA instruction tuning data\nfor two primary reasons. Firstly, the conversations within\nthis dataset are generated by GPT-4, known for its high\nlinguistic quality. Secondly, the images used are sourced\nfrom COCO, which contains human-annotated grounding\nbox instances.\nOur data annotation process aims to associate phrases\nfrom conversations with specific instances. To achieve this,\nwe leverage the capabilities of GPT-4. As illustrated in\nTable 2, we provide GPT-4 with ground-truth (GT) boxes\ncontaining class labels and a sentence from the conversa-\ntion. We task GPT-4 with matching noun phrases from the\nsentence to the GT instances. Once noun phrases are success-\nfully grounded by GPT-4, we mark them with special start\ntokens, \u27e8gs\u27e9 and \u27e8ge\u27e9, followed by a token, \u27e8seg\u27e9, which cor-\nresponds to the output feature used by the grounding model\nto segment the grounded region. An example of a question\nand its answer in the dataset is as follows:\nQ: What is the man doing? A: \u27e8gs\u27e9 The man \u27e8ge\u27e9 \u27e8seg\u27e9\nis using \u27e8gs\u27e9 a clothing iron \u27e8ge\u27e9 \u27e8seg\u27e9 on the back of \u27e8gs\u27e9\na yellow taxi \u27e8ge\u27e9 \u27e8seg\u27e9.\nFor each \u27e8seg\u27e9, we have a corresponding segmentation\nmask. This annotated data forms the basis of our Grounded\nVisual Chat (GVC) dataset. Optionally, to support visual\nprompts in user instructions, we apply a similar annotation\nprocess to instances in the question itself. The resulting data\nappears as follows:\nQ: What is the object \u27e8obj\u27e9 doing? A: \u27e8gs\u27e9 The man \u27e8ge\u27e9\n\u27e8seg\u27e9 is using \u27e8gs\u27e9 a clothing iron \u27e8ge\u27e9 \u27e8seg\u27e9 on the back of\n\u27e8gs\u27e9 a yellow taxi \u27e8ge\u27e9 \u27e8seg\u27e9.\nIt\u2019s important to note that we modify \"the man\" to \"the\nobject\" in cases where the model might disregard the visual\nprompts. For each \u27e8obj\u27e9 in the question, we provide a corre-\nsponding segmentation mask. This dataset is referred to as\nGVC-R (Grounded Visual Chat with Referring).\n2.3. Network Architectures\nSince our network architecture is nearly identical to LLaVA,\nwith the exception of the additional prompt encoder and\ngrounding model, we will only introduce these two parts in\nthis section. For the other components of our architecture,\nplease refer to LLaVA [18].\nPrompt encoder. For an input image Xv and a visual\nprompt Xp, we employ the pre-trained Semantic-SAM as\nthe prompt encoder. This encoder extracts visual features\nbased on the input image and visual prompts, denoted as\nXp = h(Xv, Xp). To convert these prompt features into\nlanguage embedding tokens Hp of the same dimensionality\n3\n<latexit sha1_base64=\"nm\naulJAcZ9L9s1EtmepKU/wnbmw=\">AB7XicbVDLSgNBEOz\n1GeMr6tHLYBA8hV0R9Rj04jGCeUASwuykNxkzO7PMzAphyT9\n48aCIV/Hm3/jJNmDJhY0FXdHeFieDG+v63t7K6tr6xWd\ngqbu/s7u2XDg4bRqWaYZ0poXQrpAYFl1i3ApsJRpHApsh\nqPbqd98Qm24kg92nGA3pgPJI86odVKjE0Zc9Irlf2KPwNZ\nJkFOypCj1it9dfqKpTFKywQ1ph34ie1mVFvOBE6KndRgQtmI\nDrDtqKQxm42u3ZCTp3SJ5HSrqQlM/X3REZjY8Zx6Dpjaod\nm0ZuK/3nt1EbX3YzLJLUo2XxRlApiFZm+TvpcI7Ni7Ahlmr\ntbCRtSTZl1ARVdCMHiy8ukcV4JLivB/UW5epPHUYBjOIEzC\nOAKqnAHNagDg0d4hld485T34r17H/PWFS+fOYI/8D5/AKc0j\ny8=</latexit>W\nProjection\n<latexit sha1_base64=\"w6z\nXykpCeX38FkjKhaO+oe7y2E=\">AB/XicbVDLSsNAFJ34rPU\nVHzs3g0VwVRIRdVl047KCfUATwmQ6aYdOJmHmplhD8FfcuFDEr\nf/hzr9x+lho64ELh3Pu5d57wlRwDY7zbS0tr6yurZc2yptb2zu\n79t5+UyeZoqxBE5Godkg0E1yBnAQrJ0qRuJQsFY4uBn7rSFTm\nifyHkYp82PSkzilICRAvsw98It4sg94A9AEA+LIrArjhVZwK\n8SNwZqaAZ6oH95XUTmsVMAhVE647rpODnRAGnghVlL9MsJXRAe\nqxjqCQx034+ub7AJ0bp4ihRpiTgifp7Iiex1qM4NJ0xgb6e98b\nif14ng+jKz7lM2CSThdFmcCQ4HEUuMsVoyBGhCquLkV0z5Rh\nIJrGxCcOdfXiTNs6p7UXvziu161kcJXSEjtEpctElqFbVEc\nNRNEjekav6M16sl6sd+tj2rpkzWYO0B9Ynz9pXpXb</latexit\n>Xv\n<latexit sha1_base64=\"OUBG\nioCbGiTqe2ceR2O0Z90t+I=\">AB/XicbVDLSsNAFJ3UV62v\n+Ni5GSyCq5KIqMuiG5cV7AObECbTSTt08mDmplhD8FfcuFDErf\n/hzr9x2mahrQcuHM65l3v8RPBFVjWt1FaWl5ZXSuvVzY2t7Z3\nzN29lopTSVmTxiKWHZ8oJnjEmsBsE4iGQl9wdr+8Hrit0dMKh\n5HdzBOmBuSfsQDTgloyTMPMscP8H3uZQ6wBwDIRnumVWrZk2B\nF4ldkCoq0PDML6cX0zRkEVBlOraVgJuRiRwKlhecVLFEkKHpM+\n6mkYkZMrNptfn+FgrPRzEUlcEeKr+nshIqNQ49HVnSGCg5r2J+\nJ/XTSG4dDMeJSmwiM4WBanAEONJFLjHJaMgxpoQKrm+FdMBkYS\nCDqyiQ7DnX14krdOafV6zb8+q9asijI6REfoBNnoAtXRDWqgJ\nqLoET2jV/RmPBkvxrvxMWstGcXMPvoD4/MHbIKV3Q=</latex\nit>Zv\n<latexit sha1_base64=\"yn0Z\nUZSqKsiKPDZqlv9C0+1rLg=\">AB/XicbVDLSsNAFJ3UV62v+N\ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5MHMTbG4K+4caGIW/Dn\nX/jtM1CWw9cOJxzL/fe4yeCK7Csb6O0srq2vlHerGxt7+zumfsHb\nRWnkrIWjUsuz5RTPCItYCDYN1EMhL6gnX80e3U74yZVDyO7mGS\nMDckg4gHnBLQkmceZY4f4EbuZQ6wBwDIxnumVWrZs2Al4ldkCoq\n0PTML6cf0zRkEVBlOrZVgJuRiRwKlhecVLFEkJHZMB6mkYkZMr\nNZtfn+FQrfRzEUlcEeKb+nshIqNQk9HVnSGCoFr2p+J/XSyG4djM\neJSmwiM4XBanAEONpFLjPJaMgJpoQKrm+FdMhkYSCDqyiQ7AX1\n4m7fOafVmz7y6q9ZsijI6RifoDNnoCtVRAzVRC1H0iJ7RK3ozno\nwX4934mLeWjGLmEP2B8fkDUD6Vyw=</latexit>Hv\nImage\nLanguage \nInstruction\nLanguage \nResponse \n<latexit sha1_base64=\"/KN5R7NUwEKH6XBR4DKeLzGzIrU=\">A\nAB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuimy4r2Ac0IUymk3bo5OHMjVhD8FfcuFDErf/hzr9x2mahrQcuHM65l3v8RPBFVjWt1FaWl\n5ZXSuvVzY2t7Z3zN29topTSVmLxiKWXZ8oJnjEWsBsG4iGQl9wTr+6Hrid+6ZVDyObmGcMDckg4gHnBLQkmceZI4f4EbuZQ6wBwDI7vLcM6\ntWzZoCLxK7IFVUoOmZX04/pmnIqCKNWzrQTcjEjgVLC84qSKJYSOyID1NI1IyJSbTa/P8bFW+jiIpa4I8FT9PZGRUKlx6OvOkMBQzXsT8\nT+vl0Jw6WY8SlJgEZ0tClKBIcaTKHCfS0ZBjDUhVHJ9K6ZDIgkFHVhFh2DPv7xI2qc1+7xm35xV61dFHGV0iI7QCbLRBaqjBmqiFqLoET2jV\n/RmPBkvxrvxMWstGcXMPvoD4/MHSKCVxg=</latexit>Hq\n<latexit sha1_base64=\"4a/5KuBhqFrRimsGds8xVP6ZkY=\">\nAB/XicbVDLSsNAFJ34rPUVHzs3g0VwVRIRdVl047KCfUAbwmQ6aYdOJnHmRqwh+CtuXCji1v9w5984bPQ1gMXDufcy73BIngGh\nzn21pYXFpeWS2tldc3Nre27Z3dpo5TRVmDxiJW7YBoJrhkDeAgWDtRjESBYK1geDX2W/dMaR7LWxglzItIX/KQUwJG8u39rBuEuJ37\nWRfYAwBkd3nu2xWn6kyA54lbkAoqUPftr24vpmnEJFBtO64TgJeRhRwKlhe7qaJYQOSZ91DJUkYtrLJtfn+MgoPRzGypQEPF/T2\nQk0noUBaYzIjDQs95Y/M/rpBeBmXSQpM0umiMBUYjyOAve4YhTEyBCFTe3YjogilAwgZVNCO7sy/OkeVJ1z6ruzWmldlnEUIH\n6BAdIxedoxq6RnXUQBQ9omf0it6sJ+vFerc+pq0LVjGzh/7A+vwBYcCV1g=</latexit>Xq\n<latexit sha1_base64=\"I8RxJE902anMmciAczfxKVfe1PY=\n\">AB/XicbVDLSsNAFJ3UV62v+Ni5GSyCq5KIqMuiG5cV7APaECbTSTt0MgkzN2INwV9x40IRt/6HO/GaZuFth64cDjnXu69\nJ0gE1+A431ZpaXlda28XtnY3NresXf3WjpOFWVNGotYdQKimeCSNYGDYJ1EMRIFgrWD0fXEb98zpXks72CcMC8iA8lDTgkYybc\nPsl4Q4k7uZz1gDwCQkTz37apTc6bAi8QtSBUVaPj2V68f0zRiEqgWndJwEvIwo4FSyv9FLNEkJHZMC6hkoSMe1l0+tzfGyUP\ng5jZUoCnq/JzISaT2OAtMZERjqeW8i/ud1UwgvYzLJAUm6WxRmAoMZ5EgftcMQpibAihiptbMR0SRSiYwComBHf+5UXSOq25\n5zX39qxavyriKNDdIROkIsuUB3doAZqIoe0TN6RW/Wk/VivVsfs9aSVczsoz+wPn8ASWCVxg=</latexit>Xa\nProjection\nPrompt \nEncoder\nVisual prompt\nGrounding model\nGrounding\nResponse \nVision Encoder\n<latexit sha1_base64=\"Dp\nm7JzZPmwmdKOQRfbESjXZnclA=\">AB9XicbVBNS8NAEJ3\nUr1q/qh69LBbBU0lE1GPRi8cK9gOaWDbTbt0dxN2N0oJ+R9\nePCji1f/izX/jts1BWx8MPN6bYWZemHCmjet+O6WV1bX1jf\nJmZWt7Z3evun/Q1nGqCG2RmMeqG2JNOZO0ZjhtJsoikXIa\nSc30z9ziNVmsXy3kwSGg8lCxiBsrPUT9zA+jDPnJiOV5v\n1pz6+4MaJl4BalBgWa/+uUPYpIKg3hWOue5yYmyLAyjHCa\nV/xU0wSTMR7SnqUSC6qDbHZ1jk6sMkBRrGxJg2bq74kMC60\nnIrSdApuRXvSm4n9eLzXRVZAxmaSGSjJfFKUcmRhNI0ADpi\ngxfGIJorZWxEZYWJsUFVbAje4svLpH1W9y7q3t15rXFdxF\nGIziGU/DgEhpwC01oAQEFz/AKb86T8+K8Ox/z1pJTzBzCH\nzifP63EkqI=</latexit>f\u03c6 Language Model\nProjection\nBox Mask\nVisual Grounding Module\nVisual Interaction Module\nFigure 3. Network architecture of our LLaVA-Grounding contains a CLIP\nvision encoder, a LLM, a prompt encoder, a grounding model and the\ncorresponding projection layers. LLaVA-Grounding expands LLaVA with\ntwo additional modules highlighted in blue blocks: the visual interaction\nmodule that accepts user drawing and visual grounding module that outputs\nobject masks/boxes. The yellow tokens represents the visual prompt feature\naligned to language embedding space. The light green output tokens repre-\nsent the grounding features which are the last-layer hidden feature of the\nlanguage model corresponding to \u27e8seg\u27e9 tokens.\nas the word embedding space in the language model, we use\na simple linear layer with a trainable projection matrix Wp:\nHp = Wp \u00b7 Xp, where Xp = h (Xv, Xp)\n(1)\nThis results in a sequence of visual tokens Hp. It\u2019s worth\nnoting that there are special tokens \u27e8obj\u27e9 in Xq with word\nembeddings as placeholders, and visual tokens in Hp replace\nthe word embeddings of \u27e8obj\u27e9 in Hq.\nGrounding model. In addition to the language response\nXa, our model also produces features Xg for grounding.\nThese features correspond to the last layer hidden features\nof the language model that align with the \u27e8seg\u27e9 tokens. We\ninitially map these features to a grounding space using a\ntrainable projection matrix Wg. Subsequently, we employ\na pretrained OpenSeeD model as the grounding model to\ngenerate bounding boxes B and masks M. This process can\nbe defined as follows:\nB, M = s (Xv, Wg \u00b7 Xg)\n(2)\nHere, s(\u00b7, \u00b7) represents the grounding model, which takes\nthe image Xv and the grounding features as input.\n2.4. Training\nWe propose a three-stage training strategy, as illustrated\nin Table 3. These stages are pretraining for alignment,\ninstruction tuning for grounded visual chat, and exten-\nsion to visual prompt. A unified representation of our\ninstruction-following data is presented as follows:\nHuman : Xv < \\n > Xq(Xp)<STOP>\nAssistant : Xa(Xg)<STOP>\\n\n(3)\nIn this representation, Xp and Xg are enclosed in brackets,\nindicating that they are optional. During training, the model\nis trained to predict the assistant\u2019s answers, including the\ngrounded instances and where to stop. Consequently, only\nthe green sequence/tokens are used to compute the loss in\nthe auto-regressive model.\nStage 1: Pretraining for alignment.\nStage 1 focuses on\nfeature alignment for the visual encoder and granularity\nalignment for the grounding model.\nFeature alignment for vision encoder. As shown in Ta-\nble 3, we utilize the RefCOCO/+/g, COCO 2017train, Visual\nGenome, LLaVA 585K image caption, and Flickr30K Enti-\nties datasets for Stage 1. Both LLaVA 585K and Flickr30K\nEntities datasets consist of image caption pairs and are used\nto train the projection layer W for feature alignment in the\nvision encoder. The conversation construction approach\naligns with that of LLaVA, where a question is randomly\nselected from Table 17 as Xq, and the original caption is\nused as Xa. The learnable parameter for this part is denoted\nas \u03b8 = {W}.\nFeature and granularity alignment for grounding model.\nTo facilitate grounding, we need to align the features Xg\noutput by the language model with the vocabulary space\nof the grounding model.\nFor this purpose, we train on\nthe RefCOCO/+/g, COCO 2017train, Visual Genome, and\nFlickr30K Entities datasets.\nThe approach to construct\ninstruction-following data is as follows:\n1. For RefCOCO/+/g and Visual Genome, the user instruc-\ntion Xq is randomly selected from Table 16, and Xa con-\nsists only of the special token \u27e8seg\u27e9. COCO 2017train\nfollows the same approach as RefCOCO/+/g, but with\na distinction: the class name of an instance serves as its\nreferring text.\n2. In contrast, the Flickr30K Entities dataset differs from\nthe image caption data mentioned earlier. Here, the user\ninstruction is followed by a suffix randomly chosen from\nTable 18. This suffix signals the model to produce a\nresponse in grounding format, as described in Section 2.2.\nThe response Xa is then converted into the grounding\nformat by inserting special tokens \u27e8gs\u27e9, \u27e8ge\u27e9, and \u27e8seg\u27e9\ninto Xa to mark noun phrases.\nGiven the instruction-following data, the last-layer hidden\nfeatures of the language model corresponding to \u27e8seg\u27e9 to-\nkens Xg are mapped to the grounding vocabulary space by\nmultiplying them with Wg. Additionally, since our ground-\ning model is pretrained on COCO and Object365, which\nhave different granularities compared to the Visual Genome\n4\nGrounding\nGrounding Seg\nVisual Chat\nChat with VP\nRefCOCO/+/g [8, 41]\n\u2713\n\u2713\n\u2713\nVisual Genome [9]\n\u2713\n\u2713\nCOCO train2017 [15]\n\u2713\n\u2713\nLLaVA 585K [18]\n\u2713\nFlickr30K [29]\n\u2713\n\u2713\n\u2713\nLLaVA 150K [18]\n\u2713\nGVC 2.2\n\u2713\n\u2713\n\u2713\nGVC-R 2.2\n\u2713\nTable 3. Blue, green and red means the training data and tasks in the 1st,\n2nd, and 3rd stages, respectively. \"Grounding\" means only predict boxes\nand \"Grounding Seg\" means predict masks. For Flickr30K, we use SAM to\nlabel pseudo GT masks. \u201cChat with VP\" means chat with visual prompts.\nand Flickr30K grounding data, we also train the grounding\nmodel to align these granularities.\nIn summary, the learnable parameters for Stage 1 are\ndenoted as \u03b8 = {W, Wg, \u03d5g}.\nStage 2: Instruction tuning for grounded visual chat.\nIn\nthe second training stage, we leverage the Grounded Visual\nChat (GVC) data, excluding visual prompts, for instruction\ntuning. To also support chat without grounding, we incor-\nporate LLaVA 158K instruction-following data. During this\nstage, we freeze the CLIP vision encoder and focus on fine-\ntuning the other components of the model. The learnable pa-\nrameters in this stage are denoted as \u03b8 = {W, Wg, \u03d5, \u03d5g}.\nThe data format consists of instruction data containing\n\u27e8seg\u27e9 tokens in the answer, accompanied by several ground-\ning annotations. The number of grounding annotations corre-\nsponds to the number of \u27e8seg\u27e9 tokens present. In this stage,\nwe calculate both language loss and grounding losses. The\nlanguage loss is computed in the same manner as in LLaVA\nfor the answer tokens and \"STOP\" tokens. The grounding\nlosses encompass box, mask, and matching losses. Box and\nmask losses are utilized solely for training the grounding\nmodel, while the matching loss is propagated to the language\nmodel.\nStage 3: Extension to visual prompt.\nIn the third stage,\nwe introduce support for visual prompts as an additional\ncomponent by training only h\u03d5\np and the projection layer\nWp. As detailed in Table 3, the training data includes Re-\nfCOCO/+/g, Visual Genome, and GVC-R. In contrast to\nStage 1, for RefCOCO/+/g and Visual Genome, we provide\nvisual prompts for the ground truth (GT) instances and in-\nstruct the model to predict captions. The text instruction\nXp is randomly selected from Table 19, where \u27e8obj\u27e9 tokens\nserve as placeholders, and their input embeddings will be\nreplaced by prompt features. The text answer Xa comprises\nthe original referring expressions.\nIn this stage, the learnable parameters are represented\nas \u03b8 = {\u03d5p, Wp}, where \u03d5p is trained to output boxes and\nmasks corresponding to visual prompts, and Wp is trained\nto align visual prompt features with the language embedding\nspace.\nSet-of-Mark (SoM) prompts. (Optional)\nIn addition to\nvisual prompts (such as clicks and boxes) that can be handled\nthrough the prompt encoder, our model also supports marks\nas visual prompts, similar to the approach presented in [35].\nThese marks consist of alphanumerics and masks that are\ndirectly overlaid on the image. To illustrate, consider the data\nsample in Sec.2.2. Let\u2019s assume we overlay marks labeled as\n\u27e81\u27e9, \u27e82\u27e9, and \u27e83\u27e9 on the \"man,\" \"iron,\" and \"taxi\" in the input\nimage. This results in the Grounded and Referring Visual\nChat (GRVC) data taking the form:\nQ: What is the object \u27e81\u27e9 doing? A: The man \u27e81\u27e9 is using\na clothing iron \u27e82\u27e9 on the back of a yellow taxi \u27e83\u27e9.\nIt\u2019s important to note that both the question and answer\nconsist of text only. Therefore, in order to support marks as\nvisual prompts, we specifically fine-tune the language part\nof the model.\n2.5. Grounding-Bench\nBenchmark Creation.\nWe introduce a benchmark named\nGrounding-Bench to assess a model\u2019s grounded visual chat\ncapability. To evaluate both grounding and chat abilities con-\ncurrently, we build this benchmark on top of LLaVA Bench\n(COCO), which comprises chat data generated by GPT4 and\ninstance annotations from MSCOCO. To enhance the ro-\nbustness of Grounding-Bench, we expand our test dataset to\ninclude 1000 images with 7000 entities, all sourced from the\nMSCOCO 2014val split. These images are converted into\ngrounded visual chat data using our data creation pipeline,\nforming the basis of our test dataset.\nTask Definition.\nGrounded visual chat tasks involve taking\nan image XV and a user instruction I as input and generating\na caption T accompanied by bounding boxes b, with each\nbounding box corresponding to a specific phrase.\nEvaluate Chat Scores.\nOur benchmark evaluation encom-\npasses two main aspects: chat scores and grounded response\nscores. We outline the evaluation process for Grounding-\nBench in Algorithm 1. Chat scores are akin to those used in\nLLaVA Bench. However, in contrast, we instruct the model\nto produce grounded responses. Subsequently, we process\nthe output to remove special tokens and boxes, yielding the\npure-text response for evaluation.\nEvaluate Grounded Response Scores.\nFor grounded re-\nsponses, we specifically evaluate the grounded detailed de-\nscription task. Our evaluation includes metrics such as recall\n(R) for completeness, precision (P) for hallucination, and\nthe F1 score (F1) to combine both aspects. R measures\n5\nCVPR\n#5335\n35. CONFIDENTIAL REVIEW COPY. DO NOT DISTRIBUTE.\nx, mask and\nto train the\nill be propa-\nns to Visual\nsert support\nnly train h\u03d5\np\n3, the train-\nand GVC-R.\nand Visual\nT instances\ninstruction\nobj\u27e9 tokens\nbe replaced\noriginal re-\ns in stage 3\nu boxes and\nis trained to\nding space.\naddition to\ne dealt with\nts marks as\numerics and\ndata sample\nmarks with\nof the input\nhat (GRVC)\nA: The man\nyellow taxi\nre text only.\npport marks\nmark named\npability of\nunding and\nmark on top\n4-generated\nCO. In order\n, we enrich\n000 entities,\nt them into\nn pipeline to\nes an image\nutput a cap-\nnding box is\nwo aspects\nrounded re-\nGrounding-\nmilar to that\nAlgorithm 1 Pseudo code for Grounding-Bench Evaluation\n# Inputs:\nGrounded response (T, b); GT response and\nboxes ( \u02c6T , \u02c6b).\n# Outputs:\nChat scores (S); Grounded response scores\n(P , R, F\n1).\n# Functions:\nGPT_chat_eval(), match_gt_box(),\nconstruct_gd_response(), GPT_gd_eval(), calc_scores().\n1 def GroundingBenchEval(T, b, \u02c6T , \u02c6b):\n2\nS = GPT_chat_eval(T , \u02c6T ) # Request GPT-4 to score T\n3\nN\ngt=len(\u02c6b), N\npred=len(b)\n4\nb\nmatched = []\n5\nfor b in b:\n6\ngt_box, iou = match_gt_box(b,\u02c6b)\n7\nif iou \u2265 0.5:\n8\nb\nmatched=b\nmatched+gt_box\n9\nT\neval = construct_gd_response(T ,b\nmatched)# Insert matched\ngt boxes into the response to form responses of\nContext type 3 in Table 2\n10\nT P\npred,T P\ngt = GPT_gd_eval(T\neval)\n# GPT help evaluate\nsemantic matching\n11\nP , R, F\n1 = calc_scores(T P\npred,T P\ngt,N\ngt)\n# Calculate\ngrounded response scores\n12\noutput = S, P , R, F\n1\n13\nreturn output\nof LLaVA Bench. Differently, we let the model output\n307\ngrounded responses. We then process the output to remove\n308\nspecial tokens and boxes to obtain the pure-text response for\n309\nevaluation.\n310\nEvaluate grounded response scores: For grounded re-\n311\nsponse, we evaluate on grounded detailed description task.\n312\nWe evaluate recall R = TPgt/Ngt for completeness, pre-\n313\ncision P = TPpred/Npred for hallucination and F1 =\n314\n2\u00d7P \u00d7R/(P +R) to combine the two. R is the proportion\n315\nof entities correctly mentioned and grounded in the descrip-\n316\ntion and P is the proportion of correct predicted groundings.\n317\nA grounding is correct only when the box is matched to a\n318\nGT box with IoU larger than 0.5 and their semantic is cor-\n319\nrectly matched. We replace the predicted box with the most\n320\nmatched box in grounded response as the Context type 3. We\n321\nthen provide Context type 1 and 3 to GPT4 resort to decide\n322\nTPpred and TPgt as shown in the bottom block in Table 2.\n323\nIn the example, Npred = 7 and Ngt = 12. According to the\n324\nresponse from GPT4, TPpred = 4 and TPgt = 3. Therefore,\n325\nwe have P = 0.57, R = 0.25, F1 = 0.35.\n326\n3. Experiments\n327\nIn this section, we first introduce our experimental settings.\n328\nThen, we compare our model with other state-of-the-art mod-\n329\nels on Grounding-Bench. Next, we compare our model with\n330\nother grounding models on challenging Referring Expression\n331\nComprehension (REC) and Referring Expression Segmen-\n332\ntation (RES) tasks on Refcoco, Refcoco+ and Refcoco-g.\n333\nThe results demonstrate that our model outperforms other\n334\ngrounding LMMs with the same number of parameters on\n335\nboth REC and RES and ours is the only one that can do both\n336\nREC and RES. Later, we do evaluation on the supporting\n337\nof various types of visual prompts. Finally, we do ablation\n338\n5\nthe proportion of entities correctly mentioned and grounded\nin the description, while P assesses the proportion of cor-\nrectly predicted groundings. A grounding is deemed cor-\nrect only when the box matches a ground truth (GT) box\nwith an IoU greater than 0.5, and their semantics are accu-\nrately matched. To determine TPpred and TPgt for GPT4,\nwe provide Context types 1 and 3, as shown in the bottom\nblock in Table 2. For example, in the provided example,\nNpred = 7 and Ngt = 12. Based on GPT4\u2019s response, we\ncalculate TPpred = 4 and TPgt = 3. Consequently, we\nobtain P = 0.57, R = 0.25, and F1 = 0.35.\n3. Experiments\nIn this section, we will first introduce our experimental set-\ntings. Then, we will compare our model with other state-of-\nthe-art models on our benchmark, Grounding-Bench. Next,\nwe will evaluate our model against other grounding mod-\nels on challenging Referring Expression Comprehension\n(REC) and Referring Expression Segmentation (RES) tasks\non RefCOCO, RefCOCO+, and RefCOCOg. The results\nwill demonstrate that our model outperforms other ground-\ning LLMs with the same number of parameters on both REC\nand RES tasks, and ours is the only model capable of han-\ndling both REC and RES effectively. Afterward, we will\nconduct an evaluation of the support for various types of\nvisual prompts. Finally, we will perform ablation studies\non our modeling and data creation processes to validate our\nmethod.\n3.1. Experimental Settings\nTo facilitate result reproduction, we provide detailed settings.\nOur language model is initialized from a pretrained Vicuna-\n7b v1.3, the grounding model is initialized from the vision\npart of an OpenSeeD Tiny model pretrained on COCO and\nObject365, and the interactive encoder is initialized from a\nSemantic-SAM Tiny model pretrained on COCO with three\ngranularities.\nIn the first training stage, we freeze the language model\nand train the grounding model, prompt encoder, and projec-\ntion layers with a learning rate of 1 \u00d7 10\u22124. For the second\nstage, we train the language model and projection layers with\na learning rate of 2 \u00d7 10\u22125, while training the grounding\nmodel with a learning rate of 1 \u00d7 10\u22124 while freezing the\nCLIP vision encoder and the prompt encoder.\n3.2. Grounding-Bench\nTo demonstrate the effectiveness of our method in Grounded\nVisual Chat (GVC), we compare our method with other\nstrong LMMs that support visual grounding on our bench-\nmark. As shown in Table 4, the results in grounded response\nscores are presented in two parts for each grid. The left one is\nevaluated on the 1000 images of our Grounding-Bench, and\nthe right one is on the 30 images in LLaVA Bench (COCO).\nAll the numbers for grounding LMMs are evaluated using\ntheir official prompt templates for grounding to ensure the\nbest performance. The results show that our method outper-\nforms all open-source methods in both grounded response\nscores and chat scores on grounded responses, except for\nCogVLM-Chat and LLaVA, which are chat models. GPT4-\nV achieves the best performance on grounded detailed de-\nscription with the help of SoM, but it is a combination of\ntwo models. Among open-source methods, GogVLM is\nsecond only to ours in terms of the F1 score for grounded\ndetailed description, but it has the lowest GPT evaluated\nscores. Shikra\u2019s chat scores are second only to ours. We also\nannotated 30 images in LLaVA Bench (COCO) as grounded\ndetailed description and reported phrase grounding perfor-\nmance of our model and Shikra for reference.\n3.3. Traditional Grounding Benchmarks\nWe also evaluate our model on classic grounding bench-\nmarks, including RefCOCO/+/g for Referring Expression\nComprehension (REC) and Referring Expression Segmen-\ntation (RES), and Flickr30K Entities for Phrase Grounding.\nFor this experiment, we use the 7B language model with\nthe grounding model using the Swin-Tiny backbone. Our\nmodel is trained for the first stage with RefCOCO/+/g, Vi-\nsual Genome, and Flickr30K Entities. Our model stands out\nas the only LMM that can excel in both REC and RES tasks.\nOn the REC task, our model outperforms all LMMs, except\nfor CogVLM-Grounding, which utilizes a 4B vision model\nand a 6B connection module. On RES and Phrase grounding\ntasks, our model surpasses all LMMs. One advantage of our\nmodel is its ability to be trained on both box and mask data,\nallowing us to leverage Visual Genome to enhance our RES\nperformance.\n6\n#Vision\nGrounded Response Scores\nChat Scores\nPhrase\nModel\nparams(B)\nRecall\nPrecision\nF1\nDetail desc.\nConv.\nReasoning\nAll\ngrounding\nLLaVA [18]\n0.30\n-\n-\n-\n69.1\n82.0\n92.6\n81.2\n-\nBubo-GPT [47]\n2.00\n26.2|25.7\n37.2|31.3\n30.7|28.2\n65.0\n75.9\n93.4\n78.2\n-\nShikra [3]\n0.30\n21.1|21.6\n39.8|38.4\n27.6|27.7\n64.7\n75.4\n86.4\n75.5\n64.29\nShikra\u2217\n0.30\n22.0|28.7\n44.6|48.6\n29.4|36.1\n41.8\n-\n-\n-\n-\nminiGPT v2 [2]\n1.00\n20.6|25.3\n33.6|39.1\n25.6|30.7\n48.0\n51.0\n38.7\n45.8\n-\nCogVLM-Grounding [33]\n10.0\n22.3|27.5\n56.3|62.5\n32.0|38.2\n35.8\n47.8\n22.2\n34.9\n-\nCogVLM-Chat\n10.0\n-\n-\n-\n73.1\n86.9\n92.1\n84.2\n-\nGPT4-V+SoM [26, 35]\n-\n\u2212 \u2212 |55.1\n\u2212 \u2212 |73.5\n\u2212 \u2212 |63.2\n67.3\n104.3\n108.4\n93.3\n-\nLLaVA-G (Ours)\n0.35\n28.6|36.3\n52.7|53.4\n37.1|43.2\n67.2\n78.7\n91.1\n79.3\n81.6\nTable 4. A comparison on our Grounding-Bench. For each model, we use the prompt template recommended by the paper. The results in\ngrounded response scores are two parts in each grid where the left one is evaluated on the 1000 images of our Grounding-Bench and the\nright one is on the 30 images in LLaVA Bench (COCO). \u2217 denotes Shikra with a special prompt for grounded description recommended by\nthe paper. We make GPT4-V+SoM grey because it uses external model to label marks.\nModels\nRefCOCO\nRefCOCO+\nRefCOCOg\nFlickr30k Entities\nREC\nRES\nREC\nRES\nREC\nRES\nACC@0.5 mIoU\ncIoU\nACC@0.5 mIoU\ncIoU\nACC@0.5 mIoU\ncIoU\nval\ntest\nReLA [16]\n\u2013\n\u2013\n73.80\n\u2013\n\u2013\n66.00\n\u2013\n\u2013\n65.00\n\u2013\n\u2013\nPolyFormer-L[19]\n\u2013\n76.94 75.96\n\u2013\n72.15 69.33\n\u2013\n71.15 69.20\n\u2013\n\u2013\nUniTAB [36]\n86.32\n\u2013\n\u2013\n78.70\n\u2013\n\u2013\n79.96\n\u2013\n\u2013\n78.76\n79.58\nMDETR [7]\n86.75\n\u2013\n\u2013\n79.52\n\u2013\n\u2013\n81.64\n\u2013\n\u2013\n82.3\n83.8\nGLIP-T\u2217 [14]\n50.42\n\u2013\n\u2013\n49.50\n\u2013\n\u2013\n66.09\n\u2013\n\u2013\n\u2013\n\u2013\nGDINO-T [21]\n89.19\n\u2013\n\u2013\n81.09\n\u2013\n\u2013\n84.15\n\u2013\n\u2013\n\u2013\n\u2013\nKosmos-2\u2217 [28]\n52.32\n\u2013\n\u2013\n45.48\n\u2013\n\u2013\n60.57\n\u2013\n\u2013\n77.80\n78.70\nLISA-7B [3]\n\u2013\n\u2013\n74.9\n\u2013\n\u2013\n65.1\n\u2013\n\u2013\n67.9\n\u2013\n\u2013\nMiniGPT v2-7B [3]\n88.06\n\u2013\n\u2013\n79.58\n\u2013\n\u2013\n84.19\n\u2013\n\u2013\n\u2013\n\u2013\nShikra-7B [3]\n87.01\n\u2013\n\u2013\n81.60\n\u2013\n\u2013\n82.27\n\u2013\n\u2013\n75.84\n76.54\nFerret-7B [40]\n87.49\n\u2013\n\u2013\n80.78\n\u2013\n\u2013\n83.93\n\u2013\n\u2013\n80.39\n82.21\nCogVLM-Grounding-17B [33]\n93.40\n\u2013\n\u2013\n87.76\n\u2013\n\u2013\n93.02\n\u2013\n\u2013\n\u2013\n\u2013\nLLaVA-G-7B (Ours)\n89.16\n79.68 77.13\n81.68\n72.92 68.79\n84.82\n74.39 71.54\n83.03\n83.62\nTable 5. Performance comparison on the referring expression comprehension (REC) referring expression segmentation (RES) and phrase grounding tasks.\nWe mark the best results with bold. \u2217 denotes the zero-shot results are reported. Since CogVLM-Grounding is a larger model with 4B vision model and 6B\nconnection module, we make it grey.\nModel\nGround type\n\u03b1 Mark\nSize Mark\nval\ntest\nOurs\n-\n-\n-\n83.0\n83.6\nOurs\nMark\n0.4 / 0.4\n20\n72.1\n73.7\nOurs\nMark\n0.4 / 0.2\n30\n75.1\n75.4\nOurs\nMark\n0.2 / 0.2\n30\n76.6\n77.9\nTable 6. The top1 accuracy of phrase grounding on Flickr30K. The first\nrow is our original pipeline with grounding model to do phrase grounding.\nLLava\nShikra\nGPT4ROI\nPVIT\nOurs-T click\nOurs-T box\n40\n53.9\n64\n64.5\n70.8\n71.5\nTable 7. The comparison on COCO object classification accuracy. Num-\nbers except for our method are from PVIT [1] paper. They evaluate llava by\ncropping the regions of GT boxes.\n3.4. Visual Prompts\nWe demonstrate our support for various types of visual\nprompts, including marks, clicks, and boxes.\nSupport for marks as visual prompts. In Table 6, we\nshowcase our phrase grounding performance on Flikr30K\nwhen using marks as visual prompts. It\u2019s important to note\nthat for this experiment, we overlay GT masks and marks\nonto the input image, positioning the marks at the centers of\nthe masks. The table presents results for different \u03b1 values\nof masks and sizes of marks. We also include the perfor-\nmance of Shikra-7B and Ours-7B with the grounding model\nfor comparison. The results clearly indicate that our model\neffectively supports the use of marks for grounding.\nSupport for clicks and boxes as visual prompts. To further\ndemonstrate our support for visual prompts, we compare ob-\nject classification accuracy on COCO val2017 in Table 7.\nWe report our performances achieved using clicks and boxes\nas visual prompts. Additionally, we include numbers from\nthe PVIT paper for reference. The results illustrate that our\nreferring accuracy surpasses that of previous works, with\nboxes yielding better accuracy than clicks.\n3.5. Visualizations\nCompare with other models. We provide visual compar-\nisons between our model and other grounding LMMs, in-\ncluding CogVLM-Grounding, Shikra, and MiniGPTv2, in\nTable 8, 20, and 21. These visualizations highlight that\nour model produces longer descriptions, detects more ob-\njects, and maintains high precision. Furthermore, our model\ndemonstrates the ability to ground open-set concepts, such\nas \"dragons,\" which do not exist in our grounded visual chat\n7\nGrounded description example 1:\nUser\nDescribe the following image with grounding.\nLLaVA-G\nThe image depicts a group of people sitting at a large, white table, engaged in\nactivities. There are four people in the scene, with three of them sitting close\nto each other and one person sitting further to the right. They are all using\nlaptop computers, likely working or collaborating on a project together. The\ntable is well organized, featuring multiple laptops, a large TV screen on the left\nside, and a clock on the wall. There are also several books and a cell phone\nplaced on the table, indicating a casual and comfortable environment. A chair\nis positioned near the left side of the table, and there are two bottles placed\nnearby, possibly containing drinks for the people working at the table.\nUser\nCan you provide a description of the image and include the coordinates\n[[x0,y0,x1,y1]] for each mentioned object?\nCogVLM-Grounding\nA group of people are sitting at a table with laptops.\nUser\nProvide a comprehensive description of the image <image> and specify the\npositions of any mentioned objects in square brackets.\nShikra\nA group of people are in a meeting , some with laptops, some with laptops, and\nsome without .\nUser\n[grounding] Describe the scene in detail.\nMiniGPT v2\nA group of people are sitting around a large table in a meeting room , with\nlaptops and phones on the table\nTable 8. An example of grounded detailed descriptions generated by LLaVA-G and other LMMs. We use the best prompt recommended by\nthe paper for each model.\ndata. These visual comparisons underscore the strengths and\ncapabilities of our model in the context of grounded visual\nchat.\nOther functionalities.\n1. Visual Chat with Visual Prompts (Table 9): Our model\ndemonstrates its capability to engage in visual chat using\nvisual prompts such as boxes or clicks.\n2. Support for Marks as Visual Prompts (Table 10): After\nfine-tuning with mark data, our model effectively sup-\nports marks as visual prompts.\n3. Debugging and Hallucination Removal (Table 11): Our\nmodel exhibits a potential for assisting in debugging by\nallowing users to identify and correct errors. In this exam-\nple, the initial output contains hallucination (\"two small\nbirds\" grounded to the woman\u2019s mouth), which the user\ndetects through grounding and subsequently instructs the\nmodel to rectify.\n3.6. Ablation Studies\nIn this section, we provide insights into our visual chat capa-\nbility and the design of the grounding model through various\nablation studies.\nMaintaining visual chat capability. We demonstrate that\nour model retains strong visual chat capabilities by com-\nparing it with LLaVA on LLaVA Bench (Table 12). The\nresults indicate that our model\u2019s visual chat performance is\n8\nVisual prompt example:\nUser\nWhat is the object \u27e8obj\u27e9 doing and why?\nLLaVA-G\nThe cat is sitting in front of a laptop computer, likely curious about the content\ndisplayed on the screen. Cats are naturally inquisitive animals, and they are\noften attracted to anything that seems new, interesting, or engaging. The\nmovement or visual elements on the laptop screen might have caught the cat\u2019s\nattention, prompting it to sit and observe the screen. It could be merely a\npassing curiosity or the start of an interest in technology or screen time that the\ncat may develop further in the future.\nUser\nWhat is the object \u27e8obj\u27e9 used for?\nLLaVA-G\nThe laptop computer screen is used for various purposes, such as working,\nstudying, entertainment, and communication. In this particular image, the\nlaptop appears to have a Word document or a web page open, which suggests\nthat the user might be working on a document, browsing the internet, or using\nthe computer for other tasks.\nTable 9. An example of visual chat based on visual prompts where \u27e8obj\u27e9 denotes the positions in the user instruction to insert the visual\nprompt features.\nExample of marks as prompts:\nUser\nWhat do you think is going on in this snapshot with grounding?\nLLaVA-G\nIn the snapshot, a little girl <2> is sitting on a rug with a guitar-shaped toy\n<1> in her hands. She appears to be playing or interacting with the toy guitar,\nwhich suggests that she might be interested in music or is simply having fun\nwith her toy. This scene could indicate that the child is exposed to musical\ninstruments or enjoys imitating the sounds she hears. It is also possible that the\ntoy guitar is a part of her imaginative play, where she creates different scenarios\nand engages with her surroundings.\nTable 10. An example of visual chat based on marks.\ncomparable to LLaVA, whether responding with or without\ngrounding.\nNumber of queries in grounding model. Table 13 presents\nour model\u2019s performance on Referring Expression Com-\nprehension (REC) and Referring Expression Segmentation\n(RES) tasks with different numbers of queries. The results\nreveal that using 50 queries is sufficient for both tasks and\nachieves optimal performance. This finding highlights the\nefficiency of our approach in handling these tasks.\nDetaching the grounding model. We investigate the impact\nof detaching the grounding model on both chat and ground-\ning performance. Detaching the grounding model means\nstopping gradients from propagating from the grounding\nmodel to the Language Model (LLM). Table 14 compares\nthe detached model\u2019s performance with the original model.\nThe results demonstrate that detaching the grounding model\nleads to slightly improved chat performance but significantly\ncompromises the grounding performance. This indicates\nthe importance of the grounding model in maintaining high-\nquality visual chat with grounding capabilities.\n4. Related Work\n4.1. Large Multi-modal Models\nWith the recent surge in Large Language Models (LLMs),\nresearchers have been actively exploring ways to extend\nthe capabilities of these models beyond text to encompass\nother modalities, resulting in the development of several\nLarge Multi-modal Models (LMMs). Notably, projects like\nLLaVA [12] and MiniGPT-4 [49] have undertaken the task\nof integrating visual instruction tuning data into their LMMs.\nThey achieved this by utilizing GPT-4 or employing hand-\n9\nExample of reduce hallucination:\nUser\nWhat do you see happening in this image with grounding?\nLLaVA-G\nThe image features a young woman standing in a field, surrounded by tall grass.\nShe is holding a string or a ribbon in her hand, which seems to be attached\nto a bird . The bird is flying in the sky, with its position slightly to the right\nand above the woman. In addition to the woman and the bird, there are two\nsmall birds in the scene. One is located in the middle of the scene, close to\nthe woman, while the other is slightly above the first bird. The scene has a\npeaceful, nature-oriented atmosphere.\nUser\nThere are actually no small birds, so please describe the scene again with\ngrounding.\nLLaVA-G\nIn the image, there is a young woman standing in a field of tall grass. She holds\na string or a ribbon, which is connected to a bird . The bird is flying in the sky,\ntowards the right and slightly above the woman. The scene portrays a sense\nof wonder and tranquility as the woman interacts with the bird and enjoys the\ncompany of nature.\nTable 11. An example to show the potential of LLaVA-G to reduce hallucination.\nLLaVA-Bench (COCO)\nLLaVA-Bench (In-the-Wild)\nGVC\nConv.\nDetail\nReasoning\nAll\nConv.\nDetail\nReasoning\nAll\nLLaVA\n82.0\n69.1\n92.6\n81.2\n42.6\n51.9\n68.9\n57.1\nLLaVA-G\nYes\n74.8\n68.5\n95.3\n79.7\n38.5\n40.1\n75.1\n55.8\nLLaVA-G\nNo\n79.3\n71.2\n92.8\n81.2\n47.7\n44.6\n70.0\n57.2\nTable 12. A comparison on LLaVA-Bench. \u201cGVC\" is \u201cNo\" means it\noutputs pure-text response without grounding.\nRefCOCO\nRefCOCO+\nRefCOCOg\n#Q\nACC\ncIoU\nmIoU\nACC\ncIoU\nmIoU\nACC\ncIoU\nmIoU\n50\n86.71\n74.77\n77.6\n77.91\n64.97\n69.68\n82.37\n68.46\n72.43\n100\n86.58\n74.70\n77.40\n77.23\n64.08\n69.02\n81.99\n68.02\n72.06\n300\n86.35\n74.26\n77.19\n77.78\n64.68\n69.54\n81.92\n67.89\n71.85\nTable 13. The comparison of performance when using different number of\nqueries in the grounding model. \u201c#Q\" denotes the number of queries.\nDetach\nGrounded detail description\nChat scores\nModel\nGD\nRecall\nPrecision\nF1\nDetail desc.\nConv.\nReasoning\nAll\nOurs\n\u2713\n25.1\n58.2\n35.1\n61.6\n86.3\n94.9\n81.2\nOurs\n36.3\n53.4\n43.2\n67.2\n78.7\n91.1\n79.3\nTable 14. Ablations on our benchmark. \u201cDetach GD\" means stop gradient\nfrom the grounding model to language model.\ndesigned prompts, thereby enhancing the LMMs\u2019 ability to\nfollow instructions effectively.\nIn addition to these, there exist other noteworthy works\nin the field, including mPLUG-DocOwl [39], Otter [11],\nLLaMa-Adaptor [45], and InternGPT [22]. These projects\nhave also contributed significantly to the advancement of\nLMMs by incorporating various techniques and methodolo-\ngies.\nMoreover, researchers have delved into the realm of fine-\ngrained understanding of LMMs, as exemplified by works\nlike VisionLLM [32], GPT4RoI [46], and PVIT [1]. Vision-\nLLM, for instance, employs a language-guided tokenizer\nto extract vision features at specific granularities, whereas\nGPT4RoI and PVIT utilize bounding boxes to obtain relevant\nvisual features.\n4.2. Visual Grounding Models\nThe visual grounding task [4, 7, 19, 23, 24, 34, 48] aims to\npinpoint the location of objects within an image based on\ntextual input. This challenge is fundamental in multimodal\nperception and has promising applications. It requires a deep\nunderstanding of both the image and the text, along with\nestablishing correspondences between image regions and\ntextual descriptions.\nThe GLIP model [13] takes a significant step in this di-\nrection by integrating various data formats, including detec-\ntion and referring data. It demonstrates that grounded pre-\ntraining effectively enhances the localization capabilities of\ngrounding models. Building upon GLIP, GLIPv2 [43] takes\na further stride by unifying grounding and Visual-Language\n(VL) understanding tasks. Grounding-DINO [21], which\nleverages grounded pretraining and the DINO [42] detector,\nstands out for its superior performance in this domain.\nIn recent years, vision-and-language models have gained\nincreasing attention in tasks related to visual recognition\nand perception. Models like CLIP [30] and ALIGN [6],\nthrough contrastive learning on large-scale image-text pair\ndatasets at the image level, have achieved generalized and\nrobust capabilities in image classification. Simultaneously,\nin more fine-grained recognition tasks like visual ground-\ning [4, 5, 7, 19, 23, 24, 34, 48], which aims to locate specific\nregions based on textual inputs, researchers are exploring the\n10\npotential of conducting image and text contrastive learning\nat the region level.\nApproaches such as MDETR [7], DetCLIP [37], Det-\nCLIPv2 [38], GLIP [13], GLIPv2 [43], and Grounding-\nDINO [21] strive to detect arbitrary categories by training\nwith large-scale region-text data. For instance, MDETR [7]\nwas trained on existing multimodal datasets with explicit\nalignment between text phrases and image objects, employ-\ning an end-to-end framework.\nGLIP [13] advances this approach by re-formulating ob-\nject detection as a grounding task and incorporating addi-\ntional grounding data to perform grounded pretraining, en-\nhancing semantic alignment between phrases and regions.\nGLIPv2 further demonstrates how grounded pretraining can\nimprove VL understanding, leading to a unified model for\nlocalization and VL understanding.\nMoreover, Grounding-DINO [21], by incorporating\ngrounded pretraining with the DINO [42] detector, excels\nin this field. These advancements in vision-and-language\nmodels, particularly through contrastive learning on large-\nscale text-region data, represent significant progress in fine-\ngrained recognition tasks, resulting in more precise and con-\ntextually aware visual understanding.\n4.3. Grounding Large Multi-modal Models\nBased on their architectural characteristics and functionali-\nties, Grounding LMMs can be classified into three distinct\ncategories.\nThe first category involves models that predict box coordi-\nnates in text format. Notable models in this category include\nKosmos-2 [28], Shikra [3], MiniGPT v2 [49], Ferret [40],\nand CogVLM [33]. For instance, Kosmos-2 introduced\na comprehensive grounding caption dataset and trained a\nmodel with strong grounding capabilities, showcasing im-\npressive zero-shot performance across various grounding\nbenchmarks. Shikra, on the other hand, focused on building\nreferral dialog data and training their model to support refer-\nral dialog with boxes as both input and output. MiniGPT v2\nemployed task tokens to activate different task-specific capa-\nbilities, including support for grounded output with boxes.\nMeanwhile, CogVLM leveraged a 10-billion parameter vi-\nsion model to achieve state-of-the-art performance in various\nvision-language tasks, including grounding. It\u2019s worth noting\nthat many of these methods trained on low-quality grounding\ncaption data, despite achieving significant progress in visual\ngrounding. For instance, Shikra\u2019s referential dialog data, al-\nthough valuable, is relatively small, consisting of only 5,000\nimages.\nThe second category involves models that employ a sep-\narate grounding model for grounded chat, exemplified by\nBuboGPT [47] and LLaVA-PLUS [20]. However, these\nmodels often face performance limitations at the language\nencoder of the grounding model.\nThe third category adopts an approach where the output\nof a language model is fed into a grounding model to decode\nmasks and boxes. LISA [10] is a representative model in\nthis category, with a primary focus on various segmentation\ntasks rather than chat interactions.\nIn many previous works, there has been a trade-off be-\ntween grounding and chat abilities, with data and evaluation\nmetrics typically emphasizing one of these aspects. In con-\ntrast, our dataset and benchmark prioritize assessing the com-\npositional abilities of both grounding and chat interactions,\nproviding a unique perspective in this field.\n5. Conclusion\nThis paper introduced LLaVA-Grounding, an AI assistant\nthat combines visual chat and grounding capabilities. We\nbegan by creating a grounded visual chat dataset using a\nnovel data creation pipeline. Subsequently, we proposed an\nend-to-end model architecture that integrates a grounding\nmodel with a Language Model (LM) for effective grounding.\nAdditionally, we introduced Grounding-Bench as a com-\nprehensive benchmark for evaluating grounded visual chat\nperformance, covering both chat and grounding aspects. Our\nexperiments demonstrated that LLaVA-Grounding consis-\ntently outperforms other open-source LM models in both\nchat and grounding tasks, showcasing its effectiveness. Fur-\nthermore, LLaVA-Grounding excelled in traditional ground-\ning benchmarks, highlighting its versatility. However, we\nacknowledge that LLaVA-Grounding has limitations in terms\nof semantic scope, and future work could explore extending\nthe dataset and data labeling methods to open-vocabulary\nsettings.\nReferences\n[1] Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li,\nMaosong Sun, and Yang Liu. Position-enhanced visual in-\nstruction tuning for multimodal large language models, 2023.\n7, 10\n[2] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun\nLiu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas\nChandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-\nv2: Large language model as a unified interface for vision-\nlanguage multi-task learning. arXiv:2310.09478, 2023. 1, 2,\n7\n[3] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng\nZhu, and Rui Zhao. Shikra: Unleashing multimodal llm\u2019s\nreferential dialogue magic. arXiv preprint arXiv:2306.15195,\n2023. 1, 2, 7, 11\n[4] Jiajun Deng, Zhengyuan Yang, Tianlang Chen, Wengang\nZhou, and Houqiang Li. Transvg: End-to-end visual ground-\ning with transformers, 2022. 10\n[5] Shijia Huang, Feng Li, Hao Zhang, Shilong Liu, Lei Zhang,\nand Liwei Wang. A unified mutual supervision framework\nfor referring expression segmentation and generation, 2022.\n10\n11\n[6] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc V Le, Yunhsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representa-\ntion learning with noisy text supervision. arXiv preprint\narXiv:2102.05918, 2021. 10\n[7] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Syn-\nnaeve, Ishan Misra, and Nicolas Carion. Mdetr \u2013 modulated\ndetection for end-to-end multi-modal understanding, 2021. 7,\n10, 11\n[8] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and\nTamara Berg.\nReferitgame: Referring to objects in pho-\ntographs of natural scenes. In Proceedings of the 2014 con-\nference on empirical methods in natural language processing\n(EMNLP), pages 787\u2013798, 2014. 1, 5\n[9] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and\nFei-Fei Li. Visual genome: Connecting language and vision\nusing crowdsourced dense image annotations, 2016. 5\n[10] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui\nYuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation\nvia large language model. arXiv preprint arXiv:2308.00692,\n2023. 1, 11\n[11] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter: A multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 10\n[12] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama,\nHaotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon,\nand Jianfeng Gao. Llava-med: Training a large language-and-\nvision assistant for biomedicine in one day. arXiv preprint\narXiv:2306.00890, 2023. 2, 9\n[13] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu\nYuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and\nJianfeng Gao. Grounded language-image pre-training. In\nCVPR, 2022. 10, 11\n[14] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan,\nLei Zhang, Jenq-Neng Hwang, et al. Grounded language-\nimage pre-training. In CVPR, 2022. 7\n[15] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence\nZitnick. Microsoft COCO: Common objects in context. In\nECCV, 2014. 2, 5\n[16] Chang Liu, Henghui Ding, and Xudong Jiang. Gres: Gener-\nalized referring expression segmentation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 23592\u201323601, 2023. 7\n[17] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning, 2023. 2\n[18] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 1, 2, 3, 5, 7\n[19] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar\nSatzoda, Vijay Mahadevan, and R. Manmatha. Polyformer:\nReferring image segmentation as sequential polygon genera-\ntion, 2023. 1, 7, 10\n[20] Shilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng\nLi, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun\nZhu, Lei Zhang, Jianfeng Gao, and Chunyuan Li. Llava-plus:\nLearning to use tools for creating multimodal agents, 2023.\n11\n[21] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023. 1, 7, 10, 11\n[22] Zhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi\nWang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang\nYang, Qingyun Li, Jiashuo Yu, Kunchang Li, Zhe Chen, Xue\nYang, Xizhou Zhu, Yali Wang, Limin Wang, Ping Luo, Jifeng\nDai, and Yu Qiao. Interngpt: Solving vision-centric tasks by\ninteracting with chatgpt beyond language, 2023. 10\n[23] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin\nWu, Cheng Deng, and Rongrong Ji. Multi-task collaborative\nnetwork for joint referring expression comprehension and\nsegmentation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2020.\n10\n[24] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Yongjian Wu, Yue Gao,\nand Rongrong Ji. Towards language-guided visual recognition\nvia dynamic convolutions. International Journal of Computer\nVision, pages 1\u201319, 2023. 10\n[25] OpenAI. Gpt-4 technical report, 2023. 1, 2\n[26] OpenAI.\nGpt-4v(ision) system card.\nhttps://cdn.\nopenai.com/papers/GPTV_System_Card.pdf,\n2023. 7\n[27] OpenAI. Gpt-4 technical report, 2023. 2\n[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Ground-\ning multimodal large language models to the world. arXiv\npreprint arXiv:2306.14824, 2023. 2, 7, 11\n[29] Bryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.\nFlickr30k entities: Collecting region-to-phrase correspon-\ndences for richer image-to-sentence models. In ICCV, 2015.\n1, 5\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision.\narXiv preprint arXiv:2103.00020, 2021. 10\n[31] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roz-\ni\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and efficient foundation language models.\narXiv\npreprint arXiv:2302.13971, 2023. 1\n[32] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, and Jifeng Dai. Visionllm: Large language model is\nalso an open-ended decoder for vision-centric tasks, 2023. 10\n[33] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\n12\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models. 2023. 1, 2, 7, 11\n[34] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Zehuan Yuan,\nPing Luo, and Huchuan Lu. Universal instance perception as\nobject discovery and retrieval. In CVPR, 2023. 10\n[35] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan\nLi, and Jianfeng Gao. Set-of-mark prompting unleashes ex-\ntraordinary visual grounding in gpt-4v, 2023. 5, 7\n[36] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu,\nFaisal Ahmed, Zicheng Liu, Yumao Lu, and Lijuan Wang.\nUnitab: Unifying text and box outputs for grounded vision-\nlanguage modeling, 2022. 7\n[37] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan\nXu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.\nDetclip: Dictionary-enriched visual-concept paralleled pre-\ntraining for open-world detection, 2022. 11\n[38] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei\nZhang, Zhenguo Li, and Hang Xu. Detclipv2: Scalable open-\nvocabulary object detection pre-training via word-region\nalignment, 2023. 11\n[39] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan,\nYuhao Dan, Chenlin Zhao, Guohai Xu, Chenliang Li, Junfeng\nTian, et al. mplug-docowl: Modularized multimodal large\nlanguage model for document understanding. arXiv preprint\narXiv:2307.02499, 2023. 10\n[40] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen\nZhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and\nYinfei Yang. Ferret: Refer and ground anything anywhere at\nany granularity, 2023. 1, 2, 7, 11\n[41] Licheng Yu, Patrick Poirson, Shan Yang, Alexander C. Berg,\nand Tamara L. Berg. Modeling context in referring expres-\nsions, 2016. 5\n[42] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M. Ni, and Heung-Yeung Shum. Dino: Detr\nwith improved denoising anchor boxes for end-to-end object\ndetection, 2022. 10, 11\n[43] Haotian* Zhang, Pengchuan* Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan,\nJenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying lo-\ncalization and vision-language understanding. arXiv preprint\narXiv:2206.05836, 2022. 10, 11\n[44] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan\nLi, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A simple\nframework for open-vocabulary segmentation and detection.\narXiv preprint arXiv:2303.08131, 2023. 2\n[45] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun\nZhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language\nmodels with zero-init attention, 2023. 10\n[46] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi\nShao, Wenwei Zhang, Kai Chen, and Ping Luo. Gpt4roi:\nInstruction tuning large language model on region-of-interest.\narXiv preprint arXiv:2307.03601, 2023. 2, 10\n[47] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi\nFeng, and Bingyi Kang. Bubogpt: Enabling visual grounding\nin multi-modal llms. arXiv preprint arXiv:2307.08581, 2023.\n1, 7, 11\n[48] Chaoyang Zhu, Yiyi Zhou, Yunhang Shen, Gen Luo, Xingjia\nPan, Mingbao Lin, Chao Chen, Liujuan Cao, Xiaoshuai Sun,\nand Rongrong Ji. Seqtr: A simple yet universal network for\nvisual grounding. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXXV, pages 598\u2013615. Springer, 2022. 10\n[49] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-\nhamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv\npreprint arXiv:2304.10592, 2023. 1, 2, 9, 11\n13\nLLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models\nSupplementary Material\n\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57! \nVision Enc\nWhat is the object <obj> doing\nThe man is sitting in a sofa on top of a green car.\nLanguage Model\nPrompt Enc\n\ud835\udc43\ud835\udc5f\ud835\udc5c\ud835\udc57\"\n\ud83d\udd25\n\ud83d\udd25\nFigure 4. Network architecture of our LLaVA-Grounding for supporting\nvisual prompts. Snow flake denotes the part is frozen and fire denotes the\npart is trainable.\nA. More details about visual prompt\nWe support visual prompts such as clicks, boxes and\nscribbles with low training cost by only train the visual\nprompt encoder to align prompt features to the language\nspace.\nTraining. We add the support of visual prompt seamlessly to\nthe trained grounded visual chat model. We use a pretrained\nSemantic-SAM model as the prompt encoder. As shown in\nFig. 4, a visual prompt will be encode as a prompt feature\nby prompt encoder and map to the language space with a\nprojection layer. There is a special token \u27e8obj\u27e9 in the text\ninstruction, whose input embedding will be replaced by the\nvisual prompt feature. To avoid any influence on the existing\nmodel, we propose to fix the other part of the model and\nonly tune the prompt encoder and the projection layer of the\nprompt encoder.\nData. As demonstrated in Section 2.2, we created 150K\ngrounded visual chat data with GPT4 to match none phrases\nin answers with GT instances. The data for visual prompts is\nannotated in the same way as the grounded visual chat data.\nAs shown in the first blocks of Table 2, we simply change\nthe sentence in Context 2 to questions and GPT4 can help\nto match entities in the question with GT instances. Then,\nwe can sample various types of visual prompts based on the\nGT instances. An example data is as follows. Assume the\ninput image is that in Fig. 4 and the original question is Q:\nWhat is the man doing?. With the help of GPT4, \u201cthe man\"\nis matched to the red box in the image. Then we change the\ntext prompt to Q: What is the object \u27e8obj\u27e9 doing?. \u27e8obj\u27e9 is\na place holder. Its input embedding will be replaced by the\nprompt embedding of \u201cthe man\". We labeled a part of 150K\nstage1\nstage2\nlrllm\n0.0\n2e \u2212 5\nlrgd\n1e \u2212 4\n1e \u2212 4\nbslang\n32\n128\nbsgd\n32\n64\nwarm up ratio\n0.03\n0.03\nweight decay\n0.0\n0.0\nbf16\n\u2713\n\u2713\ntf32\n\u2713\n\u2713\ngrad accumulate\n2\n1\nnsteps\n10000\n8000\nTable 15. The hyper-parmeters for stage1 and stage2.\nLLaVA instruction tuning data to visual prompt data. The\ndata has high-quality visual chat because it is generated by\nGPT4.\nHowever, it is not good at distinguishing different in-\nstances in the image because it usually talks about a few\nmain objects for each image. For many images, the data only\ncontain discussions about one instance. This is not good for\nthe model to distinguish between different instances. There-\nfore, we include RefCOCO to train visual prompt encoder\nfor distinguishing instances. RefCOCO has several instance,\ntext pairs denoted as (I, T) for each image. We convert them\nas visual chat data with the following template: Q:Please de-\nscribe the object \u27e8obj\u27e9 briefly. A: T where T is the referring\ntext.\nB. Implementation details\nWe provide more details of our experiment configuration for\nreproducing our method. We provide hyper-parameters for\nboth stage1 and stage2 in Table 15.\nC. Instruction lists for different data format.\nIn this section, we give instruction lists used to construct\ninstruction following data from different formats of data.\nD. More visualizations\nTable 20 and 21 shows more comparison of LLaVA-G and\nother grounding LMMs in grounded description.\n1\n\u2022 \"Please segment \u27e8phrase\u27e9.\"\n\u2022 \"Can you segment \u27e8phrase\u27e9?\"\n\u2022 \"Please provide the boxes and masks for \u27e8phrase\u27e9.\"\n\u2022 \"We need the boxes and masks for \u27e8phrase\u27e9, please.\"\n\u2022 \"Ensure that the boxes and masks for \u27e8phrase\u27e9 are provided.\"\n\u2022 \"Include the boxes and masks for \u27e8phrase\u27e9 in your submission.\"\n\u2022 \"Don\u2019t forget to attach the boxes and masks for \u27e8phrase\u27e9.\"\n\u2022 \"It\u2019s important to have the boxes and masks for \u27e8phrase\u27e9.\"\n\u2022 \"Please remember to include \u27e8phrase\u27e9\u2019s boxes and masks.\"\n\u2022 \"The request is for the boxes and masks related to \u27e8phrase\u27e9.\"\n\u2022 \"Kindly submit the boxes and masks corresponding to \u27e8phrase\u27e9.\"\nTable 16. The list of instruction templates for referring expression tasks. \u27e8phrase\u27e9 denotes the referring expression.\n\u2022 \"Describe the image concisely.\"\n\u2022 \"Provide a brief description of the given image.\"\n\u2022 \"Offer a succinct explanation of the picture presented.\"\n\u2022 \"Summarize the visual content of the image.\"\n\u2022 \"Give a short and clear explanation of the subsequent image.\"\n\u2022 \"Share a concise interpretation of the image provided.\"\n\u2022 \"Present a compact description of the photo\u2019s key features.\"\n\u2022 \"Relay a brief, clear account of the picture shown.\"\n\u2022 \"Render a clear and concise summary of the photo.\"\n\u2022 \"Write a terse but informative summary of the picture.\"\n\u2022 \"Create a compact narrative representing the image presented.\"\nTable 17. The list of instructions for brief image description.\n\u2022 \"with grounding\"\n\u2022 \"with boxes and masks\"\n\u2022 \"Please also provide the boxes and masks for the noun phrases in the response.\"\n\u2022 \"Kindly ensure that the response includes the relevant boxes and masks for each noun phrase.\"\n\u2022 \"Additionally, include the boxes and masks that match each noun phrase in the response.\"\n\u2022 \"Please provide the boxes and masks that correspond to every noun phrase in your response.\"\n\u2022 \"It\u2019s important to have the boxes and masks that align with each noun phrase in the response.\"\n\u2022 \"Make sure to include the appropriate boxes and masks for each noun phrase in your response.\"\n\u2022 \"In your response, include the boxes and masks that pertain to each noun phrase.\"\n\u2022 \"Also, supply the boxes and masks that are linked to each noun phrase in the response.\"\n\u2022 \"Additionally, please furnish the boxes and masks that correspond to each noun phrase in the response.\"\n\u2022 \"Don\u2019t forget to provide the boxes and masks associated with each noun phrase in your response.\"\n\u2022 \"Ensure that each noun phrase in the response has its respective boxes and masks.\"\nTable 18. The list of suffixes of the instructions for grounding caption.\n2\n\u2022 \"Could you offer a concise description of the object \u27e8obj\u27e9 ?\"\n\u2022 \"Please give a short summary of the object \u27e8obj\u27e9 .\",\n\u2022 \"Kindly provide a succinct description of the object \u27e8obj\u27e9 .\"\n\u2022 \"I\u2019d appreciate a brief overview of the object \u27e8obj\u27e9 .\"\n\u2022 \"Could you summarize the object \u27e8obj\u27e9 in a few words?\"\n\u2022 \"Please provide a brief explanation of the object \u27e8obj\u27e9 .\"\n\u2022 \"I\u2019d like a quick description of the object \u27e8obj\u27e9 , please.\"\n\u2022 \"Could you quickly describe the object \u27e8obj\u27e9 ?\"\n\u2022 \"Please offer a condensed description of the object \u27e8obj\u27e9 .\"\n\u2022 \"Can you briefly characterize the object \u27e8obj\u27e9 ?\"\n\u2022 \"Kindly provide a brief description of the object \u27e8obj\u27e9 .\"\n\u2022 \"Would you mind giving a brief account of the object \u27e8obj\u27e9 ?\"\n\u2022 \"Can you sketch a short description of the object \u27e8obj\u27e9 ?\"\n\u2022 \"Please deliver a terse description of the object \u27e8obj\u27e9 .\"\nTable 19. The list of instructions for region caption data.\n3\nGrounded description example 2:\nUser\nDescribe the following image with grounding.\nLLaVA-G\nThe image features a fantasy scene with a woman and a man standing close\ntogether, riding on the back of horses. The woman is on the left side of the\nimage, and the man is on the right side. Both individuals appear to be enjoying\ntheir time on horseback, surrounded by a visually appealing fantasy setting. In\nthe background, there are two dragons, one on the left side and the other on the\nright side of the image. The dragons are situated near the top, adding a sense\nof depth and intrigue to the scene.\nUser\nCan you provide a description of the image and include the coordinates\n[[x0,y0,x1,y1]] for each mentioned object?\nCogVLM-Grounding\nA man and a woman, both dressed in armor, are riding horses.\nUser\nProvide a comprehensive description of the image <image> and specify the\npositions of any mentioned objects in square brackets.\nShikra\nA woman with blond-hair sitting on a white horse with a man standing next to\nher.\nUser\n[grounding] Describe the scene in detail.\nMiniGPT v2\nA woman in blue is riding a white horse ,a man in black is walking towards the\nhorse , and a dragon is in the background\nTable 20. An example of grounded detailed descriptions generated by LLaVA-G and other LMMs. We use the best prompt recommended\nby the paper for each model.\n4\nGrounded description example 3:\nUser\nDescribe the following image with grounding.\nLLaVA-G\nThe image features a dining table with three pizzas on separate open boxes\nplaced in the center. The pizzas have various toppings, making them visually\nappealing. In addition to the pizzas, there are two bowls on the table, one\nlocated near the top left corner and the other at the top right. The table appears\nto be set for a meal, with the pizzas and bowls ready for guests to enjoy.\nUser\nCan you provide a description of the image and include the coordinates\n[[x0,y0,x1,y1]] for each mentioned object?\nCogVLM-Grounding\nThree pizzas in a box.\nUser\nProvide a comprehensive description of the image <image> and specify the\npositions of any mentioned objects in square brackets.\nShikra\nThree pizzas in pizza boxes with various toppings.\nUser\n[grounding] Describe the scene in detail.\nMiniGPT v2\nThree pizza boxes with pizzas and toppings on them\nTable 21. An example of grounded detailed descriptions generated by LLaVA-G and other LMMs. We use the best prompt recommended\nby the paper for each model.\n5\n"
  },
  {
    "title": "Axiomatic Preference Modeling for Longform Question Answering",
    "link": "https://arxiv.org/pdf/2312.02206.pdf",
    "upvote": "7",
    "text": "Axiomatic Preference Modeling for Longform Question Answering\nCorby Rosset, Guoqing Zheng, Victor Dibia, Ahmed Awadallah, Paul Bennett\nMicrosoft Research\ncorbyrosset, zheng, victordibia\nhassanam, pauben@microsoft.com\nAbstract\nThe remarkable abilities of large language mod-\nels (LLMs) like GPT-4 partially stem from post-\ntraining processes like Reinforcement Learning\nfrom Human Feedback (RLHF) involving hu-\nman preferences encoded in a reward model.\nHowever, these reward models (RMs) often\nlack direct knowledge of why, or under what\nprinciples, the preferences annotations were\nmade. In this study, we identify principles\nthat guide RMs to better align with human\npreferences, and then develop an axiomatic\nframework to generate a rich variety of pref-\nerence signals to uphold them. We use these\naxiomatic signals to train a model for scoring\nanswers to longform questions. Our approach\nyields a Preference Model with only about\n220M parameters that agrees with gold human-\nannotated preference labels more often than\nGPT-4. The contributions of this work include:\ntraining a standalone preference model that can\nscore human- and LLM-generated answers on\nthe same scale; developing an axiomatic frame-\nwork for generating training data pairs tailored\nto certain principles; and showing that a small\namount of axiomatic signals can help small\nmodels outperform GPT-4 in preference scor-\ning. We release our model on huggingface.\n1\nIntroduction\nRecent advances in large language models (LLMs)\nhas seen the introduction of diverse post-training\nstrategies, including Reinforcement Learning from\nHuman Feedback (RLHF) and Reinforcement\nLearning from AI Feedback (RLAIF). These tech-\nniques have helped bridge the \u201calignment gap\u201d be-\ntween the responses of raw pretrained language\nmodels and responses that resonate more closely\nwith human preferences (Bai et al., 2022b; Ziegler\net al., 2020). These techniques steer LLMs to prefer\none response over another based on feedback sig-\nnals from either human annotators, or from another\nLLM instructed to follow certain principles (Bah-\ndanau et al., 2019; Kwon et al., 2023; Sun et al.,\n2\n0\n2\nNaive Preference Model Score\n2\n0\n2\nAxiomatic Preference Model Score\nTop Voted Human Answers (5764)\nChatGPT Answers (5564)\n2\n0\n2\nNaive Preference Model Score\n2\n0\n2\nAxiomatic Preference Model Score\nRelated Answers (10000)\nTop Upvoted Answers (5764)\nFigure 1: A naive preference model trained on upvotes\nalone is not aligned e.g., ChatGPT answers that are rated\nhighly by humans are given low scores. An axiomatic\npreference model addresses this and other gaps.\n2023). RLHF in particular involves the construc-\ntion of a \u201creward model\u201d (RM) which is trained\nto encode these human preferences and output a\nscalar score for any given response (Christiano\net al., 2023; Stiennon et al., 2022; Beeching et al.,\n2023; Ouyang et al., 2022). Primarily, a RM-based\napproach to training LLMs separates what to learn\nfrom how to learn it (Leike et al., 2018).\nThe problem with most RMs used in RLHF post-\ntraining is that they are taught to regress a single\nscalar preference score annotated by humans with-\nout clear knowledge of why they made that decision\nor what principles they operated under. We term\nmodels trained in this fashion as naive preference-\nmodels. Furthermore, the underlying preference\npairs used to train the RM do not come from di-\nverse sources, often being sampled from the same\narXiv:2312.02206v1  [cs.AI]  2 Dec 2023\nAxiomatic Preference Model\nAxioms\nTrain Data \nMechanism\nExternal Evidence\nGroundedness\nAxiom 2,4\nLLM\nAnswers to \nRelated Questions\nRelevance\nAxiom 1\nCQA\nCommunity QA \nForum Upvotes\nUsefulness\nAxiom 0\nCQA\nCorrupt Facts\nTruthfulness\nAxiom 3\nLLM\nCombined \nAnswers\nThoroughness\nLLM\nPositive, negative pairs on each axiom \nis used to train a preference model\nLLM\nParts of data generated \nby an LLM\nData mined from \nCommunity QA forums\nCQA\nAxiom 5\nFigure 2: We propose five principled axioms to construct rich contrastive signals for training preference models\nLLM they are trained on (Bai et al., 2022a; Nakano\net al., 2022; Ouyang et al., 2022). It is also not\nclear that RMs can reliably score human-written\nand LLM-generated responses on the same scale,\nwhich is more challenging than previously antici-\npated due to vast differences such as style, as shown\nin Figure 1. Without clear signals of which prin-\nciple informs the preference decision, and diverse\nsources of training examples upholding it, a RM\nmay not be aligned with the expectations of human\nstakeholders.\nFor instance, studies have shown that RLHF-\nfinetuned LLMs may fall short of key expectations\n\u2013 e.g. by failing to support claims with evidence,\nor making claims that sound convincing but are\nuntrue \u2013 showing that there are still prevalent gaps\nin alignment for these scenarios (Liu et al., 2023;\nZheng et al., 2023b; Menick et al., 2022).\nIn this work, we define principles (axioms) that\nhumans desire in longform answers around the con-\ncepts of usefulness, relevance, grounded-ness,\ntruthfulness, and thoroughness similar to (Thop-\npilan et al., 2022). A distinguishing feature of\nour study is that we then use these principles to\nconstruct candidate answer pairs \u201caxiomatically\u201d\nsuch that one answer is clearly preferred along a\ncertain principle. Some of these axiomatic pairs\nare constructed from abundant sources of weak hu-\nman preferences in the form of \u201cupvotes\u201d from\nCommunity-based Question Answering (CQA)\nsites like StackExchange 1. In Figure 2 we illustrate\nhow axiomatic pairs are generated for a single ques-\ntion. We define the principles in Appendix A, and\ndescribe how the axioms uphold those principles\nin Section 2.\n1https://archive.org/details/stackexchange\nPrior work used axioms to diagnose and cor-\nrect failure modes in information retrieval sys-\ntems (Fang et al., 2004, 2011; Rosset et al., 2019).\nSimilarly, our axioms target known failure modes\nof modern LLMs, such as hallucinating incorrect\nstatements that appear factual (Ji et al., 2023) or\nbeing distracted by irrelevant context (Shi et al.,\n2023). The axioms also enforce new capabilities,\nsuch as incorporating evidence, or addressing mul-\ntiple perspectives. We believe our axiomatic frame-\nwork provides richer, more targeted underlying\npreference pairs than, say, sampling from the same\nLLM with different temperatures.\nMoreover, the RMs in existing studies are often\nnot released nor the subject of close study com-\npared to the LLMs they post-train. They can be\nquite costly, sometimes holding as many parame-\nters as the LLMs they train. While there are many\nstudies on RMs to address safety and toxicity is-\nsues (Bai et al., 2022a; Ganguli et al., 2022; Faal\net al., 2022; Korbak et al., 2023; Ganguli et al.,\n2023), there are fewer on longform question an-\nswering (Nakano et al., 2022; Glaese et al., 2022).\nOur approach is driven by the intuition that the\nact of identifying failure modes \u2013 or verifying an\nanswer is free of them \u2013 is cognitively simpler\n(requiring fewer parameters) than the act of gen-\nerating an answer, especially in the presence of\nauthoritative evidence from a search engine. A sep-\narate, smaller RM also has many advantages: it is\na controllable whitebox whose behavior is steer-\nable, quantifiable, and decoupled from the LLMs\nit supervises; it allows for generalization to unseen\nexamples without having to annotate them; and it\nis cheaper to run at scale.\nThe purpose of this study is to evaluate how well\nPrinciple\nAxiom Description\nPair Construction\n0. Usefulness\nUpvotes from CQA forums\nIf A\u2032 > upvotes than A\u2032\u2032\nPM (Q, A\u2032) > PM (Q, A\u2032\u2032)\n1. Relevance\nAnswer, A, to Q should be more relevant than\nanswer B to related question Q\u2032, Q\u2032 \u2208 knn(Q)\nA := Any Answer to Q\nB := Answer to Q\u2032\nPM (Q, A) > PM (Q, B)\n2. Grounded-ness\nLLM Answer with context of relevant passages\nP + is better than without\nC := LLM(Q) \u201cclosed book\u201d\nD := LLM(P +, Q) \u201copen book\u201d\nPM (Q, D) > PM (Q, C)\n3. Truthfulness\nLLM corrupts relevant answer D yielding\n\u201cwrong-but-believable answer\u201d\nE := LLM-Corrupt(D, Q)\nPM (Q, C) > PM (Q, E)\nPM (Q, D) > PM (Q, E)\n4.\nRelevant vs.\nIrrelevant\nGrounding\nLLM answer with w/ relevant context P + is bet-\nter than one w/ irrelevant context P \u2212\nF := LLM(P \u2212, Q)\nPM (Q, D) > PM (Q, F)\n5. Thoroughness\nUse an LLM to combine the top two user-upvoted\nanswers, A\u2032 and A\u2032\u2032\nG := LLM-Combine(Q, A\u2032, A\u2032\u2032)\nPM (Q, G) > PM (Q, A)\nA /\u2208 {A\u2032, A\u2032\u2032}\nTable 1: Definitions of the axioms and how to construct training pairs from them based on our principles.\nour proposed axiomatic RMs agree with human\npreferences. Hence, we refer to our model as a\nPreference Model, PM going forward. Note, us-\ning our preference models for LLM training (e.g.\nwith RLHF) is outside of the scope of this pa-\nper. In Section 4 we demonstrate the capabilities\nof the our PM in several scenarios that require\nre-ranking candidate longform answers, including\nthose written by humans and by LLMs.\nThe contributions of our work are threefold:\n1. We develop an axiomatic framework to gen-\nerate/augment training pairs that capture nu-\nances in human preferences which may not be\npresent in the existing data. These axioms can\nbe tailored to enforce any well defined princi-\nple, meaning this framework is not limited to\nlongform question answering.\n2. We train standalone preference models PM\n(220M - 7B parameters) that can score both\nhuman- and LLM-generated answers on the\nsame scale, normalizing out spurious signals\nsuch as length and style; our PM is better\nthan training on human upvotes alone.\n3. We show that training on the proper axiomatic\nsignals boosts how well our PM agrees with\nboth weak human upvotes and gold human an-\nnotators, even exceeding the capabilities of\nGPT-4 \u2013 implying that GPT-4 may be overkill\nfor preference scoring.\n2\nAxiomatic Preference Modeling\nLearning a preference model for longform ques-\ntion answering can be formulated as a learning-\nto-rank problem (Cooper et al., 1992; Liu, 2009).\nGiven a question q and a set of candidate answers\na1, a2, ..., an, the goal of the preference model\nis to find a partial ordering of the answers by\ntraining on pairs that best align with real human\npreferences (Chen et al., 2013; Carterette et al.,\n2008). Existing neural architectures such as Trans-\nformer (Vaswani et al., 2017) are adept at solv-\ning learning-to-rank problems (Nogueira and Cho,\n2020), and do even better under contrastive learn-\ning regimes (Xiong et al., 2020).\nA preference model PM takes as input a ques-\ntion q, answer a, and outputs a scalar PM (q, a) \u2208\nR a.k.a \u201cpreference score\u201d; it has an optional in-\nput reserved for evidence passages e denoted PM\n(q, e, a).\nWe instantiate PM as a transformer-\nbased cross-encoder (Wolf et al., 2019), f, whose\ninput is a linearized sequence of tokens x con-\nstructed from the concatenation of q and a, denoted\nx = q \u2299a. The output scalar is obtained from a lin-\near regressor layer on the final transformer layer\u2019s\nCLS token. We further construct contrastive pairs\nof sequences such that the answer in one sequence\nx+ = q \u2299 a+ is preferred over a negative answer\nto the same question x\u2212 = q \u2299 a\u2212. At training\ntime, the sequences are fed into f separately with\nthe objective to score the positive example higher:\nf(x+) > f(x\u2212). We choose the margin loss to\naccomplish this goal:\nL = max(0, \u03bb \u2212 [f(x+) \u2212 f(x\u2212)])\n(1)\nwhere the margin, \u03bb, between the positive and\nnegative sequence in a pair can be fixed or com-\nputed. Importantly, while traditional learning-to-\nrank finds orderings based on relevance, we argue\nthat modern LLMs must go beyond that, which\nis why we introduce an expanded set of axioms\nincluding usefulness, thoroughness and grounded-\nness.\n2.1\nHuman Preference Signals\nLearning to rank problems traditionally require a\nlarge set of candidates to re-rank. However, long-\nform answers are difficult to acquire. We turn to\nCQA forums such as Reddit and Stack Exchange\nspecifically because questions there can receive\nmultiple answers among which users can specify\ntheir preferences via \u201cupvote\u201d or \u201cdownvote\u201d sig-\nnals. Here we define axioms that produce training\npairs either directly from CQA answers, or indi-\nrectly using LLMs; we list these in Table 1.\nAxiom 0 (Usefulness) Critically, having multi-\nple answers allows us construct preference pairs.\nWe treat answers which have relatively higher up-\nvotes as being more useful or helpful2. From the set\nof answers for a question q, we construct positive\na+ and negative a\u2212 training pairs such that a+ has\nmore upvotes than a\u2212 does.\nUpvote signals are known to be noisy since\nusers may upvote answers for various reasons, and\nmay be influenced by position and presentation bi-\nases (Lee et al., 2016a). Answers can also gain\npopularity in a \u201crich get richer\u201d fashion that may\ndeviate from the intrinsic qualities of the answer\nitself (Lee et al., 2016b). However, upvotes gen-\nerally aligns with our definition of usefulness (Fu\nand Oh, 2019).\nAxiom 1 (Relevance) Answers in response to a\nquestion on a CQA site are more or less relevant,\nhence a model trained only on Axiom 0 would\nnot have seen examples of off-topic answers. We\nimbue the training regimen with additional \u201chard\nnegative\u201d answers mined from related questions.\nWe construct an KNN index of the ANCE embed-\ndings for all questions in the Stack Exchange data\ndump (Xiong et al., 2020). For each question q,\nwe retrieve k nearest neighbor questions {q\u2032}k\ni=0\n(and all their constituent answers) from the same\ncorpus such that the dot product of their vectors is\nbelow a chosen threshold q \u00b7 q\u2032\ni < tq to indicate q\u2032\ni\nis related to q while not being a paraphrase. This\nthreshold tq is found manually. At training time,\nwe randomly select n negative answers across the\nunion of answers to all k related questions propor-\n2helpfulness is part of the official answering guidelines of\nthese CQA forums\ntionally to their respective upvotes. By sampling\nnegatives proportionally to their upvotes, we are\nable to specifically control for spurious signals such\nas length, style, presence of URLs, etc and force the\nmodel to inspect how the answer content interacts\nwith the question.\n2.2\nLLM-generated Preference Signals\nAxioms 0 and 1 leveraged upvotes to construct pref-\nerence pairs from human-written answers. Here,\nwe construct additional pairs generated by an LLM\nunder various scenarios.\nAxiom 2 (Groundedness) The Groundedness\nprinciple gives rise to a preference for an answer a+\nthat incorporates and cites relevant evidence over\none without access to such evidence, a. Hence neg-\natives for a question q come from an LLM (in our\ncase, ChatGPT) in a \"closed-book\" style prompted\nwith guidelines that mirror our principles. The\n\"open-book\" a+ is generated from ChatGPT in-\nstructed to appropriately use evidence passages, e,\nplaced in its context window, which were retrieved\nfrom the Bing API called with q as the query. The\nprompt for this is shown in Figure 7 and examples\nin Figure 8.\nAxiom 3 (Truthfulness) To combat hallucina-\ntion of incorrect statements, we generate answers\nwhich intentionally corrupt factual claims in ways\nthat are still believable. To do this, we take an open-\nbook answer from Axiom 2 and instruct an LLM to\ndeconstruct it into bullet-point claims, corrupt those\nclaims individually, and then re-stitch the corrupted\nclaims into a fluent answer, as shown in Figure 9;\nexamples in Figures 10, 11. We found that open-\nbook answers contain more factual statements, and\nhence have more to corrupt. We also found this\nprompting technique is the best way to automat-\nically generate answers that are provably wrong\nwithout human annotation, otherwise, instruction-\ntuned LLMs would resist efforts to output false in-\nformation. This corrupted answer should be worse\nthan both an open-book and closed-book answer.\nAxiom 4 (Relevant vs. Irrelevant Grounding)\nThe sibling of Axiom 2, Axiom 4 targets the qual-\nity of grounding evidence because studies have\nshown that distracting context can be challenging\nfor LLMs in longform QA scenarios (Krishna et al.,\n2021). Axiom 4 exploits relevance signals from a\nretrieval system to discern low quality passages e\u2212\nfrom highly relevant ones e+.\nTo generate negative answers, we instruct an\nLLM to answer q using only information stated in\ne\u2212 and no other internal or external knowledge; see\nprompt in Figure 12. The positive a+, on the other\nhand, is generated with access to e+ in the same\nway as those in Axiom 2. We also construct addi-\ntional training pairs among the evidence passages\nthemselves to distill relevance signals directly into\nthe PM as discussed in Appendix C.3.\nWhile Axiom 2 used the Bing API for evidence,\nwe need more fine-grained control of the retrieval\nscores to ensure e\u2212 is worse than e+. We achieve\nthis with the MS Marco dataset, which also has\nsupervised relevance labels, by building a nearest\nneighbor index of the ANCE embeddings for all\nthe documents (Xiong et al., 2020). For each q in\nthe MS MARCO training set, e+ is collected from\nthe top-k documents plus those with a supervised\nrelevance label of one; while e\u2212 are documents\nbelow a relevance threshold tdoc. The sets e+ and\ne\u2212 do not overlap.\nAxiom 5 (Thoroughness) The preference model\nshould favor answers that better address the full\nscope of the question and all important perspec-\ntives. While this task is difficult to define, a sim-\nple yet effective approach is to assume that if\ntwo high quality answers a\u2032 and a\u2032\u2032 to q come\nfrom two different authors, then their combina-\ntion should be more thorough than either alone.\nWe generate the positive a+ from an LLM in-\nstructed to combine \u201cthe best of both worlds\u201d,\na+ = LLM-Combine(q, a\u2032, a\u2032\u2032). For training, a\u2212\nare answers known to be worse than both a\u2032 and\na\u2032\u2032, i.e. they have fewer upvotes. The prompt is\nshown in Figure 13 and examples in Figure 14.\nIn practice, we select a\u2032 and a\u2032\u2032 to be the top two\nhighest-upvoted answers on Stack Exchange, not-\ning through extensive manual observations that\nusers seldom upvote two answers with duplicate\ncontent very highly. We post-process this data to\nremove pairs where a+ resembles naive concatena-\ntions its two constituents. For evaluation, we track\na+ vs a\u2032 and a\u2032\u2032 as in Table 3.\n2.3\nConnection to RLAIF & Constitutional AI\nThere is a strong connection between our Ax-\niomatic framework described above and RLAIF.\nFirstly, the Axioms themselves build upon princi-\nples used to design LLMs like Lamda (Thoppilan\net al., 2022). For instance, Claude\u2019s Constitution3\n3https://www.anthropic.com/index/\nclaudes-constitution\nemphasized \u201chelpfulness\u201d and \u201dhonesty\u201d which we\noperationalized into training pairs for Usefulness\n(Axiom 0) and Truthfulness (Axiom 3). Sparrow\nhas a \u201cstay on topic\u201d Rule (Glaese et al., 2022)\nwhich we adapted as Relevance.\nSecondly our Axiomatic framework is flexible\nenough to incorporate \u201cAI feedback\u201d from a much\nlarger \u201cteacher\u201d model like GPT-4 by having it\nlabel/rank which axiomatic answer it prefers.\nHowever, we can go one step further and ask\nthe teacher not only which it prefers, but by how\nmuch by scoring the answers. These fine-grained\npreference scores can learned by the PM via the\n\u03bb term in Equation 1, which governs the magni-\ntude of separation between answers. Since answers\nwe generate from LLMs lack upvote signals (and\nhence by default have a constant \u03bb), this approach\nunifies learning from human- and AI-preference\nsignals.\n3\nExperimental Methods\nImplementation Details For all our experiments,\nthe preference model is initialized from a T5Flan\n(Chung et al., 2022) base model. We train each\nmodel on a different combination of axiomatic\npairs with a learning rate of 5e-6 warmed up lin-\nearly over 1k steps. We control for differences in\ntraining data size by mixing the data and training\nfor exactly 16k steps \u2013 just under one epoch \u2013 to\navoid any overfitting. We sample training exam-\nples uniformly at random according to the question\n(aka \u201cposts\u201d) so that posts with many answers do\nnot dominate. For each question, we group all\npairs of its answers into the batch. The maximum\nsequence length of the concatenation of question,\nevidence, and answer tokens is 2048, with the ques-\ntion capped at 256.\nData Collection As a source of upvote data, we\nchose to mine and filter 905k posts from Stack Ex-\nchange across a variety of \u201csubstacks\u201d covering\ntopics ranging from biology to systems administra-\ntion. There are about 3.4 answers per question on\naverage, see Table 10. We filtered posts to those\nwith at least two answers, one of which had posi-\ntive upvotes, and at least one pair of answers where\nthe higher had 30% more upvotes than the lower.\nAll questions used to seed LLM-generated ax-\niomatic pairs were sampled from Stack Exchange\nabove, except Axiom 4, which we constructed via\nMS Marco with evidence documents sourced from\nits corpus (Bajaj et al., 2018). Before training, we\nStackX\nr/ELI5\nr/Science\nr/History\nMS Marco\nWebGPT\nAvg. Ans per Q\n3.6 pos, 40 neg\n4.6 pos, 43 neg\n6.5 pos, 42 neg\n5.3 pos, 47 neg\n1.1 pos, 1k neg\n1 pos, 1 neg\nMetric\nMRR\nNDCG\nMRR\nNDCG\nMRR\nNDCG\nMRR\nNDCG\nMRR\nNDCG\nAccuracy\nlength(Ans)\n15.0\n35.4\n6.2\n27.6\n7.7\n30.1\n15.0\n37.1\nn/a\nn/a\n56.7\nOpenAsst-RM 6.7B\n25.0\n44.6\n12.7\n34.7\n15.4\n38.1\n24.4\n46.1\n4.0\n17.3\n76.5\nStackLlama RM 7B\n26.8\n45.1\n8.3\n30.6\n10.3\n33.3\n9.8\n33.1\n3.4\n16.1\n56.1\nGPT-4 (listwise)\n45.5\n62.1\n39.6\n59.9\n35.1\n56.4\n37.8\n60.4\nn/a\nn/a\nn/a\nPM 0 T5-base\n31.2\n48.6\n11.1\n32.6\n14.8\n37.0\n24.0\n44.5\n3.9\n16.9\n51.1\nPM 0-1 T5-base\n64.3\n78.8\n54.5\n75.2\n53.2\n75.4\n63.1\n84.3\n16.1\n30.6\n55.7\nPM 0-2 T5-base\n65.5\n79.8\n55.1\n76.3\n51.9\n74.6\n61.4\n83.1\n9.7\n25.6\n57.6\nPM 0-3 T5-base\n65.3\n79.5\n55.0\n76.0\n51.4\n73.9\n61.1\n82.8\n9.4\n23.7\n55.4\nPM 0-4 T5-base\n65.8\n80.0\n54.0\n75.2\n51.1\n74.0\n61.2\n83.0\n25.0\n39.3\n58.6\nPM 0-5 T5-base\n64.6\n79.2\n53.6\n75.0\n51.6\n74.3\n61.7\n83.3\n23.1\n37.4\n58.1\nPM 0-5 T5-large\n66.4\n80.8\n55.9\n77.0\n55.4\n77.2\n64.0\n85.4\n24.3\n38.9\n59.1\nPM 0-5 Llama-7b\n74.9\n86.7\n65.5\n85.6\n60.5\n82.5\n69.6\n89.5\n37.5\n50.1\n59.9\nPM 0-5 + \u03bb Llama-7b\n74.9\n86.7\n65.3\n85.4\n60.8\n82.4\n69.7\n89.5\n31.5\n45.1\n61.3\nTable 2: We evaluate PM on answer ranking tasks, trained under various combinations of axioms. Ranking is\nperformed in the presence of \u201chard negatives\u201d from semantically related questions (or BM25, for MS Marco). We\ncompare against open-source reward models: Stack-LLama and OpenAssistant, both of which have 7B parameters.\nOur PM were not trained on WebGPT data (but OA-RM was); StackLLama was trained on Stack Exchange.\nalso confirmed that each type of answer pair con-\nstructed by the Axioms was indeed preferred by\nhumans, as shown in Table 6. Any pair whose pos-\nitive was preferred less than 70% of the time was\nremoved from training. We discuss more in Section\n4.1. For mining related questions in Axiom 1, we\nset k = 10 which leads to about 40 hard negative\nanswers on average per post. Table 7 shows the\nsizes of our datasets, Appendix C explains more.\nChoosing the Margin We found computing\na margin of log10(votes(a+)/votes(a\u2212)) to work\nbest, congruous with (Askell et al., 2021). For\nLLM-generated answer pairs (where upvotes do\nnot exist), the margin was a fixed constant of 0.25.\nThe only exception is for PM + \u03bb, where GPT-4\nwas first asked to \u201ccritique-then-score\u201d each answer\non a scale of 1-100 in a listwise fashion, and then\nthe margin was computed after filtering for pairs\nwhere the score difference was at least 5.\nExisting Open-source Baselines We also evalu-\nate against two 7B-parameter reward models pub-\nlicly available: one that was used to train Hugging-\nface\u2019s StackLLama model4 and another used to\ntrain OpenAssistant5 from Laion AI.\n3.1\nEvaluation\nWe evaluate our PM using the following datasets\nand quality metrics:\nHeld-out Stack Exchange set of 5.5k posts,\n4llama-7b-stack-exchange-RM-peft-adapter-merged\n5oasst-rm-2-pythia-6.9b-epoch-1\nwith all their respective human-written answers\nand LLM-generated answer pairs from Axioms 1,\n2, 3 and 5. We evaluate quality in a ranking setting\nby ordering human-written answers along the PM\nscores, and compute the MRR of the top-upvoted\nanswer as well as NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4i-\nnen, 2000, 2002). We also evaluate accuracy on\nheld-out axiomatic pairs for Axioms 2, 4, and 5.\nELI5 Test set of about 20k questions across the\nr/ELI5, r/Science, and r/History subreddits (Fan\net al., 2019). This data has a similar format to Stack\nExchange since there are multiple user-written an-\nswers to a posted question which other users can\nupvote. Hence, we evaluate MRR and NDCG as\nin Stack Exchange above. For increased difficulty\nof answer ranking, both ELI5 and Stack Exchange\nheld-out data contain hard-negative answers to re-\nlated questions \u00e0 la Axiom 1, where all negatives\nare set to have a relevance gain of 0.\nWebGPT Comparisons dataset of about 19.5k\nquestions, each with a pair of retrieval-augmented\nanswers collected from a LLM-based web brows-\ning assistant named WebGPT (Nakano et al., 2022).\nEach pair of answers has human preference anno-\ntations, on which we compute accuracy of whether\nthe PM gives a higher score to the preferred an-\nswer; we also confirm statistical significance of our\nresults by showing the p-value from a student\u2019s T-\ntest. We evaluate only on the 17,622 answer pairs\nwhich had a \u201cclear\u201d preference. The preferred an-\nswers had about 137 \u00b1 41 words compared to 127\nAx 2: Open- vs Closed Book Ax 4: Rel.- vs. Irrel. Context Ax 5: Combine Top 2\nPos >Neg with Evidence e+ Pos >Neg\nwith Evidence e+\nComb >1st Comb >2nd\nAcc (%) Acc (%)\n\u2206 Pos\nAcc (%) Acc (%)\n\u2206 Neg\nAcc (%)\nAcc (%)\nPM 0 T5-base\n70.0\n64.0\n-0.18\n30.9\n19.4\n-0.06\n25.7\n34.9\nPM 0-1 T5-base\n77.7\n53.9\n-0.55\n52.8\n20.2\n-0.29\n47.0\n57.7\nPM 0-2 T5-base\n76.4\n69.5\n-0.058\n82.3\n54.5\n+0.27\n66.3\n80.3\nPM 0-3 T5-base\n71.3\n22.8\n-0.38\n76.0\n87.7\n-0.53\n58.2\n73.8\nPM 0-4 T5-base\n55.1\n73.7\n+0.059\n91.4\n98.4\n-0.27\n59.7\n75.8\nPM 0-5 T5-base\n53.4\n79.0\n+0.089\n92.8\n98.1\n-0.094\n97.4\n98.6\nPM 0-5 Llama-7b\n74.3\n72.1\n-0.01\n90.3\n97.1\n+0.01\n99.0\n99.2\nPM 0-5 + \u03bb Llama-7b\n81.3\n73.3\n-0.09\n89.6\n94.8\n-0.094\n59.0\n78.4\nTable 3: Evaluation on held-out pairs for axioms 2, 4 and 5. We evaluate answers with and without the evidence\nused to construct them, where positives are supposed to have higher scores in presence of their grounding evidence.\n\u00b1 46 for the negatives.\nMS Marco passage ranking dev set has 6.9k\nquestions, each with 1k BM25-negative passages\nand around one supervisedly labeled relevant pas-\nsage (Bajaj et al., 2018). We use our PM to rerank\nall \u223c1k passages and compute MRR and NDCG.\nNote, held out data for Axiom 4 used the passages\nto augment LLM-generated answers to the dev\nquestions; here we rerank the passages themselves.\n\u201cResearch-Analysis Questions\u201d of 500 diffi-\ncult, hand-curated questions that go beyond fac-\ntoid questions to elicit more intense reasoning and\nlonger form answers which require multiple per-\nspectives and evidence. They have no one right\nanswer. We describe this dataset more in Appendix\nD and show multiple examples in Figure 9. We\ngenerate multiple candidate answers, pair them,\nand get gold human preferences among the pairs.\nWe then compute agreement between PM and the\ngold preferences as described in Section 4.3.\nData from Stack Exchange and MS Marco were\nused for training the PM and are considered \"in-\ndomain\". We do not train on data from Reddit\nELI5, WebGPT or Research Analysis Questions.\n4\nResults\nThroughout these results, we compare preference\nmodels trained on various combinations of the ax-\niomatic data, e.g. \u201cPM 0\u22122\u201d denotes training with\ndata pairs from Axioms 1 and 2 added to the origi-\nnal pairs from Axiom 0.\n4.1\nEvaluating Axiomatic Data Construction\nOur first goal is to compare human and LLM-\nwritten answers on the same scale. Qualitatively,\nwe expect a good PM to score answers to related\nquestions (Axiom 1) on the lowest end of that scale\n(since they don\u2019t even address the question at hand),\nfollowed by human-written answers with relatively\nlow or negative upvotes. On the other hand, most\nanswers generated by ChatGPT (a capable LLM)\nshould score highly, similar to the highest-upvoted\nhuman-written answers.\nFigure 1 shows that PM 0 (a naive model\ntrained only on upvotes) falls short of these ex-\npectations, which we believe is due to stylistic\ndifferences in LLM-generated answers, noise in\nthe upvote signals, and lack of meaningfully irrele-\nvant answers naturally occurring in Stack Exchange\nposts. A more detailed qualitative comparison in\nFigure 4 shows that PM 0-1 is good but not suffi-\ncient and that PM 0-2 is the \u201cminimum\u201d amount\nof axiomatic signals needed to correct these issues.\nTable 6 shows our efforts to verify that each type\nof axiomatically constructed training pair is indeed\naligned with human preferences, and if not, it is dis-\nqualified from the training set. The annotators indi-\ncated their preference on a 6-point scale (\u201cStrongly\nPrefer A\u201d, \u201cModerately Prefer A\u201d, \u201cSlightly\u201d, etc)\nwithout the option for a tie. These results also\nconfirmed that often times the ChatGPT-generated\nanswers were preferred to the top-upvoted human\nanswer (57% to 43%).\nOur conclusion is that a combination of ax-\niomatic training signals is needed for a PM to\nabide by the principles and score human- and LLM-\nwritten answers on the same scale, without over-\nfitting to spurious signals. Put another way, the\naxioms regularize noisy user upvote signals.\n4.2\nPM for Answer Ranking\nIn Table 2 we evaluate PM in answer ranking set-\ntings, showing the average number of positive and\nnegative answers per task. As a baseline, we also\nhave GPT-4 rank these answers \u201clistwise\u201d (mean-\ning in a single completion call, GPT-4 must output\nPrefer A >B (%)\nAgreement w/ 3-Way Human Annotators (%)\nAnswer Pair (A vs. B)\nHuman\nGPT-4 (tie)\nPM 0-5\nGPT-4 (tie)\nPM 0-5\n0-4\n0-2\n0-1\n0\nGPT-4 vs ChatGPT\n94.0\n94.0 (4.1)\n83.2\n92.7 (2.0)\n82.0\n80.4\n66.4\n16.0\n28.0\nGPT-4 vs \"GPT-4 fixing Vicuna13B\"\n79.6\n51.5 (26.2)\n74.1\n72.8 (4.1)\n73.2\n71.6\n60.4\n36.4\n44.8\nGPT-4 vs \"GPT-4 Plan & Search\"\n74.4\n68.2 (19.6)\n75.5\n69.9 (6.9)\n66.4\n70.4\n57.6\n37.6\n44.0\n\"GPT-4 fix V\" vs \"GPT-4 P&S\"\n45.2*\n48.0 (22.0)\n44.1\n58.9 (11.0)\n60.8\n55.6\n59.2\n40.4\n43.6\n\"GPT-4 fix V\" vs \"Vicuna13B P&S\"\n76.0\n52.0 (20.5)\n58.7\n64.6 (16.3)\n64.4\n67.6\n52.4\n33.2\n34.0\n\"GPT-4 P&S\" vs \"Vicuna13B P&S\"\n82.4\n41.2 (24.7)\n65.2\n47.6 (20.3)\n63.2\n50.0\n43.2\n36.0\n38.4\n\"Vicuna13B P&S\" vs ChatGPT\n52.8*\n76.0 (10.3)\n43.0\n65.5 (1.6)\n60.0\n63.2\n55.6\n42.0\n43.6\n\"Vicuna13B P&S\" vs Vicuna13B\n59.5\n61.2 (11.5)\n60.5\n67.3 (4.6)\n65.4\n66.1\n59.3\n37.4\n38.0\nVicuna13B vs ChatGPT\n31.2\n55.8 (19.2)\n35.3\n47.2 (17.5)\n67.2\n68.4\n51.6\n26.0\n30.0\nOverall Agreement:\n65.4 (8.9)\n66.8\n65.9\n56.5\n34.2\n38.3\nTable 4: Human judges are asked to annotate gold preferences on pairs of answers to a hand-crafted set of 500\ndifficult \u201cResearch Analysis Questions\u201d. We compare how well various PM agree with their preference decision.\nthe new order of the answer ids given a context\ncontaining all the answers). Our results show that\ndespite the advantage GPT-4 has in seeing all the\nanswers at once, our PM can still align with noisy\nhuman preference signals better than GPT-4 with\nonly about 220M parameters. Notably, PM 0 falls\nshort for this task, due to its inability to distinguish\nthe top-upvoted answers from the hard negatives.\nFor the MS Marco passage reranking task we note\nthat BM25 achieves a MRR of 18.4, which is ex-\nceeded only after incorporating Axiom 4\u2019s data.\nIt is also surprising that existing reward models\nlike OpenAssistant-RM and StackLlama fall short\nof expectations on these re-ranking benchmarks,\nespecially since StackLLama was trained on Stack\nExchange as well. It appears that for preference\nmodeling, the quality of training signals is more\nimportant than the size of the model.\nIn Table 3 we evaluate the PM on held-out\npairs of answers constructed by Axioms 2, 4 and\n5. If a PM is not trained on one of the axioms\nin this table, that axiom is considered a zero-shot\nevaluation.\nA key performance indicator of a\nwell-grounded PM is giving higher scores to an-\nswers a+ that properly cite supporting evidence\ne+ against closed-book answers a (Axiom 2), or\nagainst those answers a\u2212 that cited irrelevant ev-\nidence e\u2212 (Axiom 4). When given access to e+\nin column two of Table 3, the \u2206 between PM\n(q, e+, a+) and PM (q, a+) should be positive,\nindicating the PM is more confident that a+ is\nsuperior to a, resulting in higher accuracy.\nSimilarly for Axiom 4,\ngiving the PM\n(q, e+, a\u2212) access to e+ makes it more apparent\nthat a\u2212 is omitting, or even at odds with, the rel-\nevant information in e+. In other words, higher\naccuracy with access to e+ means it is easier to\ndetect a+ is better than a\u2212 than without access.\nThe last two columns of Table 3 show that, as in-\ntended, the positive answer from Axiom 5 is better\nthan the top two upvoted answers it LLM com-\nbined in the first place; and additionally, it is found\nto be more superior to the second highest upvoted\nanswer than the first.\n4.3\nPM Agreement with Gold Human\nPreferences\nWe generate a set of answers to hard \u201cRe-\nsearch Analysis Questions\u201d from different mod-\nels like ChatGPT, GPT-4 (OpenAI, 2023), and Vi-\ncuna13B (Chiang et al., 2023). We also prompt\nthem under different scenarios such as using tools\nlike the Bing API to iteratively \u201cPlan & Search\u201d\nbefore synthesizing a final answer (Schick et al.,\n2023), or using GPT-4 to fix Vicuna13B\u2019s \u201cPlan\n& Search\u201d attempt in a feedback loop (\u201cGPT-4\nfix Vicuna\u201d) (Madaan et al., 2023; Welleck et al.,\n2022). We intend these scenarios to reflect real-\nworld use cases of LLMs. We then select pairs of\nanswers to send for gold human preference label-\ning, which leads to better calibration than scoring\nthem individually (Carterette et al., 2008; Ziegler\net al., 2020). Per answer pair, at least three an-\nnotators provide a 6-point preference score with\nFleiss kappa \u03ba = 0.42 indicating good inter-rater\nagreement. More details are in Appendix B.1.\nWe then evaluate in Table 4 how well our PM\nagrees with the human gold preferences. We define\nagreement as: if the majority of the annotators pre-\nferred answer A over B, did the PM give a higher\nscore to A, and vice versa. An * means not statisti-\ncally significant. As a baseline, GPT-4 was again\nprompted to score answers \u201clistwise\u201d with critique-\nthen-score technique (Appendix B.3 and Figure 5)\nsimilar to (Wang et al., 2023). Hence, GPT-4 had\nthe advantage of access to more answers for bet-\nter preference calibration, while the PM was at\na disadvantage because it only scores an answer\n\u201cpointwise\u201d at test time. We record when GPT-4\ngave a tie to an answer pair. Despite GPT-4\u2019s advan-\ntage, our 220M parameter PM 0-5 has higher\nagreement with gold human preferences. Ta-\nble 4 also shows that a mixture of multiple axioms\nis needed to exceed 50% agreement, which is the\nrandom choice baseline.\n4.4\nConstant vs. Variable Margins\nLastly, in Figure 3 we show the qualitative differ-\nences between a PM 0-5 llama2-7b trained with a\nconstant margin for all LLM-generated axiomatic\ntraining pairs, versus one with a variable margin de-\nrived from GPT-4 preference scores. While the re-\nranking evaluations in Table 2 for these two models\ndo not show much variation, this histogram reveals\nthat even large preference models which see both\nhuman- and LLM-generated answers can be vul-\nnerable to overfitting on the style/length of LLM\nanswers. We believe that fine-grained AI-feedback\nscores from a model like GPT-4 can help defend\nagainst this.\n5\nRelated Work\nEarly works on scoring LLM outputs like LaMDA\nand BlenderBot3 collect scores of single input-\noutput pairs rather than preference scores between\npairs of candidate outputs (Shuster et al., 2022;\nThoppilan et al., 2022). More recent reward mod-\nels (RMs) fall into two camps. The first is training\nseparate regressor models like those used in RLHF,\nwhich are often a single reward model to encode a\none dimension of human preferences (B\u00f6hm et al.,\n2019; Ziegler et al., 2020; Bahdanau et al., 2019;\nOuyang et al., 2022; Korbak et al., 2023) or many\ndimensions (Bai et al., 2022a; Ramamurthy et al.,\n2022). The second camp uses LLMs instructed to\ngive feedback based on principles or a \u201cconstitu-\ntion\u201d (Bai et al., 2022b; Kwon et al., 2023), with\nthe drawback of being costly to query.\nOther approaches seek more fine-grained reward\nsignals or multiple reward models, such as col-\nlecting relevance, correctness, and completeness\nsignals on both sentence- and response-levels using\nseparate reward models (Wu et al., 2023). Sparrow\ncollects \u201ctargeted judgements\u201d from human annota-\ntors to better characterize which of 23 rules a LLM\nviolated (mostly around toxicity and safety), and\n-1.0 0.0 1.0 2.0 3.0\n-1.0\n0.0\n1.0\n2.0\n3.0\nOther Human Answer\nSecond-Upvoted Human Answer\nTop-Upvoted Human Answer\nChatGPT Closed-Book (Ax. 2)\nChatGPT Comb. Top Answers (Ax. 5)\nFigure 3: Distribution of PM 0-5 scores on both human-\nand ChatGPT-generated answers to our Stack Exchange\ndev set. The (left) was trained with a constant margin\nwhereas the (right) PM 0-5 + \u03bb was trained with GPT-\n4-annotated preference margins per training pair.\nthen train multiple targeted classifiers (Glaese et al.,\n2022). The coexistence of rule-based and trained\nreward functions is also explored in (Ramamurthy\net al., 2022). Process supervision has emerged as\na promising direction to provide feedback at each\nstep of a complex multi-step task (Lightman et al.,\n2023).\nRetrieval augmentation has been shown in sev-\neral studies to mitigate hallucination of incorrect\nstatements in LLMs, by either finetuning LLMs\nwith grounding documents (Lewis et al., 2020),\nor inserting them to the context windows without\nfine-tuning LLMs (Ram et al., 2023). Other meth-\nods infuse retrieved knowledge in the decoding\nstage for knowledge-intense question-answering\ntasks (Liu et al., 2022).\n6\nConclusions\nWe show that augmenting human preference data\nwith axiomatically generated responses leads to ef-\nfective PM that can score both human-written and\nLLM-generated answers on the same scale under\na variety of scenarios, including open-book search\nscenarios. While the bulk of the work in this pa-\nper went into generating training data rather than\nmodeling, we stress that high quality training sig-\nnals which illuminate nuanced differences between\nresponses to the same question is what drives our\nPM \u2019s quality, allowing it to exceed other public\nreward models with more than 10x parameters. No-\ntably, our resulting PM is better aligned with gold\nhuman preferences than GPT-4, despite having only\n220M parameters.\n7\nLimitations\nOur PM has several limitations in it current form.\nEven though it was trained on axiomatic data tai-\nlored to enforce multiple principles, it still outputs\nonly a single scalar whereas it could be more useful\nto output multiple rewards per axiom, or even com-\npute probabilities that an axiom is being violated.\nSecondly, our preference models do not give\nfeedback beyond a scalar score. If the PM gives\na low score to an answer, it does not come with\nclear instructions on how to improve it, or which\nprinciple needs attention. Thirdly, our preference\nmodel is defined to score only single answers to a\nquestion; it does not score multi-turn conversations,\nfor instance, which limits its application in possible\nLLM post-training.\n8\nEthical Considerations\nThis study was approved by our Internal Review\nBoard, and the contractor, Scale AI, agreed to ad-\nhere to our ethics policies. As part of that agree-\nment, all human annotators were paid at least\n$15/hr. While we carefully removed any offensive\nor adult content from the data set for annotation,\nany annotator could opt-out of examples they were\nuncomfortable with.\nReferences\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson El-\nhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment.\nDzmitry Bahdanau, Felix Hill, Jan Leike, Edward\nHughes, Arian Hosseini, Pushmeet Kohli, and Ed-\nward Grefenstette. 2019. Learning to understand goal\nspecifications by modelling reward.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022a. Training a\nhelpful and harmless assistant with reinforcement\nlearning from human feedback.\nYuntao Bai,\nSaurav Kadavath,\nSandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen,\nAnna Goldie,\nAzalia Mirhoseini,\nCameron McKinnon, et al. 2022b. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir\nRosenberg, Xia Song, Alina Stoica, Saurabh Tiwary,\nand Tong Wang. 2018. Ms marco: A human gener-\nated machine reading comprehension dataset.\nEdward Beeching, Younes Belkada, Kashif Rasul,\nLewis Tunstall, Leandro von Werra, Nazneen Ra-\njani, and Nathan Lambert. 2023. Stackllama: An rl\nfine-tuned llama model for stack exchange question\nand answering.\nFlorian B\u00f6hm, Yang Gao, Christian M. Meyer, Ori\nShapira, Ido Dagan, and Iryna Gurevych. 2019. Bet-\nter rewards yield better summaries: Learning to sum-\nmarise without references.\nBen Carterette, Paul N. Bennett, David Maxwell Chick-\nering, and Susan T. Dumais. 2008. Here or there:\nPreference judgments for relevance. In Proceedings\nof the IR Research, 30th European Conference on\nAdvances in Information Retrieval, ECIR\u201908, page\n16\u201327, Berlin, Heidelberg. Springer-Verlag.\nXi Chen, Paul N. Bennett, Kevyn Collins-Thompson,\nand Eric Horvitz. 2013. Pairwise ranking aggrega-\ntion in a crowdsourced setting. In Proceedings of the\nSixth ACM International Conference on Web Search\nand Data Mining, WSDM \u201913, page 193\u2013202, New\nYork, NY, USA. Association for Computing Machin-\nery.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nPaul Christiano, Jan Leike, Tom B. Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2023. Deep\nreinforcement learning from human preferences.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nWilliam S. Cooper, Fredric C. Gey, and Daniel P. Dab-\nney. 1992. Probabilistic retrieval based on staged lo-\ngistic regression. In Proceedings of the 15th Annual\nInternational ACM SIGIR Conference on Research\nand Development in Information Retrieval, SIGIR\n\u201992, page 198\u2013210, New York, NY, USA. Association\nfor Computing Machinery.\nVictor Dibia. 2020. Neuralqa: A usable library for ques-\ntion answering (contextual query expansion+ bert)\non large datasets. arXiv preprint arXiv:2007.15211.\nFarshid Faal, Ketra Schmitt, and Jia Yuan Yu. 2022. Re-\nward modeling for mitigating toxicity in transformer-\nbased language models.\nApplied Intelligence,\n53(7):8421\u20138435.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nLong form question answering. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3558\u20133567, Florence,\nItaly. Association for Computational Linguistics.\nHui Fang, Tao Tao, and ChengXiang Zhai. 2004. A\nformal study of information retrieval heuristics. New\nYork, NY, USA. Association for Computing Machin-\nery.\nHui Fang, Tao Tao, and Chengxiang Zhai. 2011. Di-\nagnostic evaluation of information retrieval models.\n29(2).\nHengyi Fu and Sanghee Oh. 2019.\nQuality assess-\nment of answers with user-identified criteria and data-\ndriven features in social q&a. Information Process-\ning & Management, 56(1):14\u201328.\nDeep Ganguli, Amanda Askell, Nicholas Schiefer,\nThomas I. Liao, Kamil\u02d9e Luko\u0161i\u00afut\u02d9e, Anna Chen,\nAnna Goldie, Azalia Mirhoseini, Catherine Olsson,\nDanny Hernandez, Dawn Drain, Dustin Li, Eli Tran-\nJohnson, Ethan Perez, Jackson Kernion, Jamie Kerr,\nJared Mueller, Joshua Landau, Kamal Ndousse, Ka-\nrina Nguyen, Liane Lovitt, Michael Sellitto, Nelson\nElhage, Noemi Mercado, Nova DasSarma, Oliver\nRausch, Robert Lasenby, Robin Larson, Sam Ringer,\nSandipan Kundu, Saurav Kadavath, Scott Johnston,\nShauna Kravec, Sheer El Showk, Tamera Lanham,\nTimothy Telleen-Lawton, Tom Henighan, Tristan\nHume, Yuntao Bai, Zac Hatfield-Dodds, Ben Mann,\nDario Amodei, Nicholas Joseph, Sam McCandlish,\nTom Brown, Christopher Olah, Jack Clark, Samuel R.\nBowman, and Jared Kaplan. 2023. The capacity for\nmoral self-correction in large language models.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red teaming language models to re-\nduce harms: Methods, scaling behaviors, and lessons\nlearned.\nAmelia Glaese, Nat McAleese, Maja Tr\u02dbebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\nLucy Campbell-Gillingham, Jonathan Uesato, Po-\nSen Huang, Ramona Comanescu, Fan Yang, Abigail\nSee, Sumanth Dathathri, Rory Greig, Charlie Chen,\nDoug Fritz, Jaume Sanchez Elias, Richard Green,\nSo\u02c7na Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel\nFoley, Susannah Young, Iason Gabriel, William Isaac,\nJohn Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geoffrey Irving. 2022.\nImproving alignment of dialogue agents via targeted\nhuman judgements.\nKalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2000. Ir eval-\nuation methods for retrieving highly relevant docu-\nments. SIGIR \u201900, page 41\u201348, New York, NY, USA.\nAssociation for Computing Machinery.\nKalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumu-\nlated gain-based evaluation of ir techniques. ACM\nTrans. Inf. Syst., 20(4):422\u2013446.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan\nSu, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12).\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion.\nVladimir Karpukhin, Barlas O\u02d8guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020.\nDense passage retrieval for\nopen-domain question answering. arXiv preprint\narXiv:2004.04906.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika\nBhalerao, Christopher L. Buckley, Jason Phang,\nSamuel R. Bowman, and Ethan Perez. 2023. Pre-\ntraining language models with human preferences.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Colli ns, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Matthew Kelcey,\nJacob Devlin, Kenton Lee, Kristina N. Toutanova,\nLlion Jones, Ming-Wei Chang, Andrew Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Transactions of the Association of Compu-\ntational Linguistics.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and\nDorsa Sadigh. 2023. Reward design with language\nmodels.\nMoontae Lee, Seok Hyun Jin, and David Mimno. 2016a.\nBeyond exchangeability: The chinese voting process.\nIn Advances in Neural Information Processing Sys-\ntems, volume 29. Curran Associates, Inc.\nMoontae Lee, Seok Hyun Jin, and David Mimno. 2016b.\nBeyond exchangeability: The chinese voting process.\nJan Leike, David Krueger, Tom Everitt, Miljan Martic,\nVishal Maini, and Shane Legg. 2018. Scalable agent\nalignment via reward modeling: a research direction.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459\u20139474.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike, John\nSchulman, Ilya Sutskever, and Karl Cobbe. 2023.\nLet\u2019s verify step by step.\nNelson F Liu, Tianyi Zhang, and Percy Liang. 2023.\nEvaluating verifiability in generative search engines.\narXiv preprint arXiv:2304.09848.\nRuibo Liu, Guoqing Zheng, Shashank Gupta, Rad-\nhika Gaonkar, Chongyang Gao, Soroush Vosoughi,\nMilad Shokouhi, and Ahmed Hassan Awadallah.\n2022. Knowledge infused decoding. arXiv preprint\narXiv:2204.03084.\nTie-Yan Liu. 2009. Learning to rank for information\nretrieval. Foundations and Trends\u00ae in Information\nRetrieval, 3(3):225\u2013331.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Iterative\nrefinement with self-feedback.\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022. Teaching language models to support answers\nwith verified quotes.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2022. Webgpt: Browser-\nassisted question-answering with human feedback.\nRodrigo Nogueira and Kyunghyun Cho. 2020. Passage\nre-ranking with bert.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nRajkumar Ramamurthy,\nPrithviraj Ammanabrolu,\nKiant\u00e9 Brantley, Jack Hessel, Rafet Sifa, Christian\nBauckhage, Hannaneh Hajishirzi, and Yejin Choi.\n2022. Is reinforcement learning (not) for natural\nlanguage processing?: Benchmarks, baselines, and\nbuilding blocks for natural language policy optimiza-\ntion. arXiv preprint arXiv:2210.01241.\nHannah Rashkin, Vitaly Nikolaev, Matthew Lamm,\nLora Aroyo, Michael Collins, Dipanjan Das, Slav\nPetrov, Gaurav Singh Tomar, Iulia Turc, and David\nReitter. 2022. Measuring attribution in natural lan-\nguage generation models.\nCorby Rosset, Bhaskar Mitra, Chenyan Xiong, Nick\nCraswell, Xia Song, and Saurabh Tiwary. 2019. An\naxiomatic approach to regularizing neural ranking\nmodels.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nFreda Shi, Xinyun Chen, Kanishka Misra, Nathan\nScales, David Dohan, Ed Chi, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023. Large language models can be\neasily distracted by irrelevant context.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,\nEric Michael Smith, Stephen Roller, Megan Ung,\nMoya Chen, Kushal Arora, Joshua Lane, Morteza\nBehrooz, William Ngan, Spencer Poff, Naman Goyal,\nArthur Szlam, Y-Lan Boureau, Melanie Kambadur,\nand Jason Weston. 2022. Blenderbot 3: a deployed\nconversational agent that continually learns to respon-\nsibly engage.\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul Christiano. 2022. Learning\nto summarize from human feedback.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Cox, Yiming Yang,\nand Chuang Gan. 2023.\nPrinciple-driven self-\nalignment of language models from scratch with min-\nimal human supervision.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. Lamda: Language models for dialog appli-\ncations.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023. Large language models are not fair evaluators.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2022. Generating sequences by learning to\nself-correct. arXiv preprint arXiv:2211.00053.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A transfer\nlearning approach for neural network based conver-\nsational agents.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\nSuhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\ngrained human feedback gives better rewards for lan-\nguage model training.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. 2020. Approximate nearest neighbor neg-\native contrastive learning for dense text retrieval.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen\nTan, Kun Xiong, Ming Li, and Jimmy Lin. 2019.\nEnd-to-end open-domain question answering with\nbertserini. arXiv preprint arXiv:1902.01718.\nYi Yang, Wen-tau Yih, and Christopher Meek. 2015.\nWikiqa: A challenge dataset for open-domain ques-\ntion answering.\nIn Proceedings of the 2015 con-\nference on empirical methods in natural language\nprocessing, pages 2013\u20132018.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023a. Judging\nllm-as-a-judge with mt-bench and chatbot arena.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023b. Why does chatgpt fall short in providing\ntruthful answers?\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2020. Fine-tuning lan-\nguage models from human preferences.\nA\nPrinciples of Longform Question Answering\nWe define the following principles which closely mirror those of Lamda (Thoppilan et al., 2022). Each\nprinciple gives rise to an axiom from which we construct positive and negative training signals as shown\nin Table 1. The following definitions are also used verbatim in our human annotation guidelines for all\nanswer preference tasks.\nUsefulness: An answer is useful if it adds value to those who need an answer to the question by\nproviding e.g. actionable steps to complete a task, in-depth analysis, help weighing a decision, etc. This\n\u201cvalue-add\u201d can come in many forms:\n\u2022 Help weigh decisions and their consequences in real world scenarios. For example, a useful answer\nto the question \u201cat what age should I buy a house\u201d should explain how various factors in ones life\nsuch as financial security, family needs, etc play a role in that decision without being superficial.\n\u2022 Actionable: the answer leaves the user with concrete \u201cnext steps\u201d and directions.\n\u2022 Show in-depth analysis of why the answer is correct. For example, if the question is \u201chow many trees\nwould it take to produce enough oxygen for a person\u201d, a useful answer would not just state a number,\nwhich is technically what the answer is asking for, but also convincing step-by-step calculations of\nhow much oxygen human need per day, how much oxygen trees produce per kilogram, etc.\n\u2022 Help complete complex tasks, e.g. by breaking them into more manageable sub-tasks. For example,\na useful answer to the question \u201chow to get my driver\u2019s license\u201d would explain multiple steps, criteria,\ntimelines and milestones.\n\u2022 Explain cause and effect relationships that help the reader \u201cthink ahead\u201d before making a decision.\n\u2022 Help reveal new or interesting information that could lead the reader in a fruitful direction, e.g. if\nthe question asks \u201cwhat makes a good air purifier\u201d, a useful answer could reveal that \u201cfine particles\nless than 10 micrometers are a particular health risk because they can make their way deep into lung\ntissue\u201d. Hence, a reader now has a new information to help them judge a good air purifier.\n\u2022 Re-frame a complex problem in a new or simpler way. For instance, the act of picking the best hair\nclippers to buy could be made simpler by instead answering what hair clippers professional barbers\nuse.\n\u2022 Apply lessons from historical events to modern-day events\nRelevance: At a minimum, answers should stay on-topic and clearly address the intent of the question\nin a way that is specific, sensible and free from distractions.\n\u2022 Direct: it answers the question directly and clearly, even if the question itself is poorly written or\nmisspelled.\n\u2022 Sensible: if the answer is not good english, doesn\u2019t make sense, or is completely off topic, it is\ncertainly not relevant.\n\u2022 Specific: the answer is specific to the question and does not make overly general statements that\ncould be used for practically any other question, e.g. \u201cthat\u2019s a great idea\u201d.\n\u2022 Not overly broad or too general to be helpful\n\u2022 Not redundant or repetitive: giving overly long, rambling answers, or merely repeating information\nfrom the question or information that is clearly common knowledge.\n\u2022 Not distracting: the answer should not change the subject or answer another related question without\nfirst answering the question.\nTruthfulness: The answer contains accurate information, or makes claims that can be verified. It\ndoesn\u2019t mislead the user with incorrect or highly opinionated information. Some characteristics include:\n\u2022 Not making clearly false claims (e.g. making up facts or promoting conspiracies). For example, the\noutput should not state that Hillary Clinton has served time in prison.\n\u2022 Not making un-verifiable claims, e.g. \u201cAbraham Lincoln would have loved to play video games\u201d\n\u2022 Not mislead, or \u201cturn a blind eye\u201d to misleading information, especially if it comes from sources\nwith questionable authenticity. For example, if the input asks \u201cWhy did Hillary Clinton go to jail?\u201d,\nthe output should not say \u201cIt\u2019s not totally clear\u201d, but rather should refute the premise of the question.\nGroundedness: Major claims within the answer can be, and are, associated with known reliable\nsources (Rashkin et al., 2022; Thoppilan et al., 2022). Furthermore, the answer follows a logical chain of\nreasoning.\n\u2022 At a minimum, truthful, but goes beyond that by instilling confidence that the answer is correct.\n\u2022 Follows a logical chain of reasoning, citing evidence along the way, without \u201cbouncing around\u201d or\n\u201cjumping to conclusions\u201d.\n\u2022 Provides information that is accurate and has been supported by external sources: either primary\nsources (first-hand accounts of a topic by people with direct connection to it) or reliable secondary\nsources. like textbooks or newspapers.\n\u2022 Credible: The source of the evidence is authoritative and reliable, with a reputation for provid-\ning trustworthy information. Typically, peer-reviewed publications, government reports, books,\nprominent news sites, etc.\n\u2022 If there is a lack of certainty, the answer conveys what is uncertain and why.\n\u2022 The cited sources actually support the claim.\n\u2022 Not relying too much on personal opinion or experience to answer the question, e.g. \u201cmy flights are\nalways delayed at Houston airport...\u201d\n\u2022 Not relying on rumors, anecdotes, hearsay or \u201che-said-she-said\u201d, e.g. \u201cthis one time I saw an angel...\u201d\nor \u201cMy friend told that...\u201d, etc.\nThoroughness: The answer considers the full scope of the question, including multiple perspectives,\nalternatives, or likely outcomes/consequences without \u201comitting anything important\u201d.\n\u2022 Understands and addresses the intended scope of the question. If the answer is partial in this regard,\ndoes it acknowledge what part was not covered?\n\u2022 Considers multiple scenarios and perspectives to strengthen an argument.\n\u2022 Address the many interpretations or facets that an ambiguous or multi-faceted question may have.\nFor example, a thorough answer to the question \u201chow do I reduce my carbon footprint\u201d should\naddress more than one segment like energy consumption, transportation, diet, personal habits, etc.\n\u2022 Address all likely outcomes of a decision and their consequences, and don\u2019t leave out anything\nimportant\n\u2022 Analyze all the pros and cons that would have material value to someone who cared\n\u2022 \u201cempathize\u201d by addressing how multiple sides of a conflict, or various stakeholders in a decision,\nmay have different perspectives, expectations, or backgrounds.\nClarify refers more to the style of the writing rather than the content: Is the answer clear and concise,\nadhering primarily to the intent of the question? Is the amount of superfluous, extraneous, or \u201ctangential\u201d\ninformation kept to a minimum.\nB\nAdditional Experimental Details and Results\nB.1\nEvaluating Answers to \u201cResearch-Analysis Questions\u201d\nThis section explains more details behind Tables 4, 5, and 8 which all pertain to the \u201cResearch Analysis\nQuestions\u201d benchmark.\nTo evaluate on this benchmark, we generated 7 answers from a range of LLMs including Vicuna (Chiang\net al., 2023), ChatGPT, GPT-4 (OpenAI, 2023) and text-davinci-003 (Ouyang et al., 2022). To make this\ndataset more realistic, we also elicit additional responses under more complex scenarios:\n\u2022 \u201cGPT-4 Plan & Search\u201d We prompted GPT-4 to first issue queries to the Bing API that it believes\nwould yield useful external information to ground its results. Then, it synthesizes a grounded answer,\nciting its sources. The prompt for this behavior is shown in Figure 15.\nMRR listwise\nAverage MRR after PM Pointwise Scoring\nAnswer Type\nGPT-4\nChatGPT\nPM 0-5\nPM 0-4\nPM 0-3\nPM 0-2\nPM 0-1\nPM 0\nGPT-4\n0.65\n0.58\n0.69\n0.69\n0.32\n0.47\n0.18\n0.22\nGPT-4 fixing Vicuna13B P & S\n0.61\n0.43\n0.38\n0.46\n0.31\n0.40\n0.23\n0.21\nGPT-4 Plan & Search\n0.41\n0.51\n0.41\n0.30\n0.27\n0.35\n0.26\n0.27\nVicuna13B Plan & Search\n0.35\n0.38\n0.34\n0.36\n0.43\n0.46\n0.43\n0.41\nVicuna 13B\n0.23\n0.22\n0.25\n0.28\n0.49\n0.38\n0.54\n0.50\nChatGPT\n0.21\n0.27\n0.33\n0.35\n0.37\n0.31\n0.35\n0.33\ntext-davinci-003\n0.16\n0.22\n0.20\n0.18\n0.41\n0.24\n0.61\n0.67\nTable 5: Here we show the average MRR of 7 answers generated for each of the 500 Research Questions (higher\nMRR means more preferred). GPT-4 and ChatGPT also re-ranked the answers list-wise using the critique-then-score\ntechnique. We generated three answers from GPT-4: closed book, an open book one where it was allowed to plan\nwhich queries to ask to the Bing API, and one which generated a \u201cfixed\u201d version of the PM -guided Vicuna answer.\n\u2022 \u201cVicuna13B Plan & Search\u201d also known as a PM -guided Research Assistant. A recent trend\nin LLM research is to try and make smaller models as effective as much bigger ones. The goal of\nthis method is to obtain answers similar to the \u201cGPT-4 Plan & Search\u201d method above, with a much\nsmaller model. However, because the context window of Vicuna is much shorter (2048 tokens), we\nuse a PM to guide Vicuna to make small, iterative updates to its answer that consistently yield\nhigher score. Such updates involve retrieving documents from the Bing API, summarizing them,\nand reranking them with a PM to select only a few that fit in the context window. We describe this\nprocess more in Appendix E.\n\u2022 \u201cGPT-4 fixing Vicuna13B\u201d: Since the PM -guided Vicuna answer above sometimes makes errors\nlike those shown in Table 8, we have GPT-4 correct these in a feedback loop. We expect that the\ncorrected answers should be at least good as the original answer.\nTogether, we believe these 4 \u201cclosed-book\u201d and 3 \u201copen-book answers\u201d are representative of how\nLLMs will be used in various open-domain scenarios in the near future. Furthermore, we believe the set\nof \u201cResearch Questions\u201d are difficult enough to be used as a metric to evaluate LLMs in these scenarios.\nHence, these 7 types of answers to these 500 questions merit scrutiny, and indeed help illuminate behaviors\nof our preference models.\nIn Table 5 we show the average MRR of each answer after sorting them by the various PM scores, as\nwell the order prescribed by ChatGPT/GPT-4 prompted to score them \u201clistwise\u201d. The ordering induced by\nPM 0-5 and PM 0-4 more or less match those induced by GPT-4, despite the disadvantage that the PM\ncan only score \u201cpointwise\u201d while GPT-4 has the privilege of seeing all the answers at once in its \u201clistwise\u201d\nscoring.\nIn Table 4, we select pairs of these 7 answer types to send to human annotators as described in\nSection 4.3.\nIn Table 8, we further record the various flaws that our human annotators flagged in each type of answer.\nA lower prevalence of flaws is typically associated with higher preference among the pairs.\nB.2\nScoring Axiomatic Training Pairs\nIn Table 6 we wanted verify each axiomatic pair used during training is aligned with gold human annotator\npreferences. We sampled some questions from our Stack Exchange dataset, including all the axiomatic\npairs associated with each question. ChatGPT is the LLM that produces all LLM-generated axiomatic\nanswers. As a baseline for comparison, we also used GPT-4 instructed to play the role of an annotator.\nWe instruct both GPT-4 and human raters to indicate which answer in the pair they prefer, taking into\nconsidering all our defined principles. In addition, they scored each answer individually on a scale of 1-10\nand record the average delta between the positive and negative answer on this scale.\nUnfortunately, not all the axiomatic pairs were strong enough to be used as training pairs for a PM .\nFor instance, the open-book vs closed-book pair was preferred just over random, 57%. Upon inspection,\nwe found that in these cases, ChatGPT was had enough information stored in its internal parameters to\nsatisfactorily answer the question, rendering external evidence from the Bing API redundant. We suspect\nGPT-4\nHuman\nAxiom\nCandidate Answer Pair (A vs. B)\n# pairs\nA >B\n\u2206\nA >B\n\u2206\n0\nTop-upvoted Human vs. Worst Human\n134\n94.0\n3.6\n79.1\n2.7\nn/a\nTop-upv. Human vs. ChatGPT Open-Book\n556\n40.4\n-0.6\n36.7\n-0.7\nn/a\nTop-upv. Human vs. ChatGPT\n556\n52.7\n0.3\n42.8\n-0.5\n1\nTop-upv. Human vs. Ans. to Related Q\n422\n93.1\n5.4\n73.9\n2.4\n1\nChatGPT vs. Ans. to Related Q\n422\n93.5\n5.9\n85.5\n3.5\nn/a\nChatGPT vs. Worst Human\n134\n86.6\n3.6\n82.1\n2.9\n2\nChatGPT Open-book vs. ChatGPT\n556\n71.2\n1.1\n57.4\n0.5\n1+2\nChatGPT Open-book vs. Ans. to Related Q\n422\n97.8\n6.1\n83.9\n3.2\n2\nChatGPT Open-book vs. Worst Human\n134\n91.0\n4.0\n88.8\n3.4\n3\nTop-upv. Human vs. Wrong-but-believable\n556\n87.2\n4.2\n61.0\n1.4\n3\nChatGPT vs. Wrong-but-believable\n556\n89.7\n4.2\n71.9\n2.3\n3\nChatGPT Open-book vs. Wrong-but-believable\n556\n93.2\n4.7\n74.5\n2.4\n4\nChatGPT w/ Relevant vs. Irrelevant Evidence\n200\n91.6\n3.0\n89.0\n3.4\n5\nChatGPT Combine vs. Top-upv. Human\n249\n77.5\n1.6\n80.3\n1.8\n5\nChatGPT Combine vs. 2nd. best Human\n52\n87.6\n2.4\n82.7\n1.7\nTable 6: Here we show the percentage (%) of time GPT-4 and gold human annotators prefer answers in various\ntypes of axiomatic training pairs.\na combination of the following to be true: either by nature the questions in Stack Exchange don\u2019t need as\nmuch external evidence, or ChatGPT/GPT-4 was trained on this data already. Regardless of which is true,\nit further supports the need for additional hard evaluation questoins like the \u201cResearch Analysis Question\u201d\nused in Section B.1.\nIn cases where the human preference for a pair dropped below 70%, we removed that type of pair from\nthe training set.\nB.3\nUsing LLMs as Annotators: Critique-then-Score\nTODO cite \"LargeLanguageModelsarenotFairEvaluators\" TODO cite Judging LLM-as-a-judge with\nMT-Bench and Chatbot arena\"\nThroughout the course of this study, we frequently had to call upon a LLM such as ChatGPT or GPT-4\nto score an answer to a question on, say, a scale of 1-10. However, naive forms of prompting this behavior\nled to disappointing results with very skewed distributions like those shown in yellow and red in Figure 5,\nwhich is consistent with the problems revealed by (Wang et al., 2023) and (Zheng et al., 2023a).\nWe addressed this problem by first instructing the LLM to critique the answer in bullet point form \u2013\nexplicitly mentioning strengths and weaknesses of the answer as they pertain to our principles \u2013 before\ngiving a score. This is consistent with the \u201cmultiple evidence calibration\u201d solution found in (Wang et al.,\n2023). This solution addressed \u201cpointwise\u201d scoring of an individual answer in isolation. However, doing\nside-by-side \u201cpairwise\u201d or \u201clistwise\u201d evaluation of candidate answers in the same context window was\neven better, the prompt for which is shown in Figure 6. We suspect this helps calibrate the model to\ndraw more detailed comparisons between a \u201cgood\u201d vs \u201cbad\u201d answer. This approach is consistent withthe\n\u201cMultiple Evidence Calibration\u201d solution in (Wang et al., 2023). It is worth mentioning that pairwise\nlabeling exhibits the same benefit in human annotation studies (Carterette et al., 2008).\nC\nData Processing and Examples\nC.1\nData Statistics\nWe show in Table 7 the quantity of questions which had axiomatic training pairs. In Table 10 we break\ndown how many questions, and how many answers per question on average there are in each substack of\nStack Exchange. While there are 159 substacks represented in our training set, we select only some of the\nlargest ones.\nTraining\nTest\nDataset\nSource\nQuestions\nAns. per Q\nSource\nQuestions\nAxiom 0\nStack Ex\n905k\n3.4 +/- 1\nStack Ex\n5.5k\nAxiom 1\nStack Ex\n905k\n37 +/- 3\nStack Ex\n5.5k\nAxiom 2\nStack Ex\n35k\n2\nMS Marco\n4.8k\nAxiom 3\nStack Ex\n50k\n1\nStack Ex\n2.0k\nAxiom 4\nMS Marco\n44k\n6\nMS Marco\n4.8k\nAxiom 5\nStack Ex\n69k\n1\nStack Ex\n1.9k\nTable 7: Prevalence of each Axiom in our training data; we report the source of seed questions as well as the number\nof additional answers that each axioms adds to the underlying training question.\nC.2\nAxiom 3 Processing\nWe tried several ways to generate wrong answers that were still believable. The best one was to deconstruct\na good answer into bullet point claims or facts, corrupt each one point-by-point, and then re-stitch the\ncorrupted claims back into an answer. In Figure 9 we show the prompt used to construct these answers,\nwhich was done in a single call with multiple instructions.\nC.3\nAxiom 4 Processing\nIn addition to contrasting LLM-generated answers for Axiom 4, we also select two positive passages from\ne+ and two negative passages from e\u2212 to build additional contrastive pairs. In total, Axiom 4 adds six\nadditional \u201canswers\u201d to each question. In particular, we contrast the LLM-generated a+ to each of the\nnegative passages, as well as the positive passages against the negative ones. This helps distill relevance\nsignals into the PM .\nC.4\nAxiom 5 Processing\nOne failure mode of Axiom 5 is that the LLM could \u201ccheat\u201d by simply concatenating the two input\nanswers it was supposed to combine more intelligently. To detect and eliminate this behavior, we develop\nsimple heuristics involving counting ngrams. Intuitively, if virtually none of the ngrams in the combined\nanswer overlap with the two input answers, then the LLM probably didn\u2019t utilize those answers well. On\nthe other hand, if all the ngrams in both answers overlap with those in the combined answer, it probably\nmeans it just concatenated the two. We set thresholds such that the a good combined answers should be in\na \u201cgoldilocks\u201d region.\nDefine |C \u2229 A| = overlapping ngrams between Answer A and the combined Answer\nDefine |C \u2229 B| = overlapping ngrams between Answer B and the combined Answer\nThen the utilization score between the combined answer and its constituent sub-answers is\nutilization = |C \u2229 A|\n|A|\n+ |C \u2229 B|\n|B|\n\u2208 [0, 2]\nchoose thresholds s.t. valid example has utilization score between 0.35 < utilization < 1.85\nD\nResearch Analysis Questions\nHere we describe the characteristics of 500 questions in our \u201cResearch Analysis Questions\u201d dataset:\n\u2022 The questions require authoritative external evidence that needs to be analyzed.\n\u2022 The evidence involves analysis or intense reasoning to reach conclusions\n\u2022 Long-form answers are expected\n\u2022 There is no one \u201cright\u201d answer. To the contrary, many diverse perspectives should be considered.\n\u2022 There may be a need to answer sub-questions / sub-tasks in order to properly address the original\nquestion.\nFlaws Detected by Any Rater (A % / B %)\nAnswer Construction\nUnclear\nRepetitive\nIrrelevant\nToo Narrow\nToo Broad\nInaccurate\nGPT-4 vs ChatGPT\n4.0 / 14.0\n10.8 / 8.8\n6.8 / 16.4\n4.0 / 37.2\n12.0 / 46.0\n4.4 / 5.6\nGPT-4 vs \u201cGPT-4 fixing Vicuna 13B\u201d\n4.0 / 16.0\n14.4 / 10.0\n11.2 / 20.8\n5.2 / 16.8\n13.6 / 25.2\n3.6 / 3.6\nGPT-4 vs \u201cGPT-4 Plan & Search\u201d\n3.2 / 17.6\n13.6 / 15.2\n6.0 / 23.6\n6.8 / 13.6\n14.4 / 17.6\n2.4 / 8.4\n\u201cGPT-4 fix V\u201d vs \u201cGPT-4 P & S\u201d\n11.6 / 13.2\n8.4 / 14.8\n20.0 / 28.0\n17.6 / 12.8\n23.2 / 17.2\n2.8 / 4.4\n\u201cGPT-4 fix V\u201d vs \u201cVicuna13B P&S\u201d\n8.4 / 27.6\n13.6 / 15.6\n16.4 / 36.8\n13.2 / 34.0\n19.2 / 26.0\n7.6 / 12.8\n\u201cGPT-4 P & S\u201d vs \u201cVicuna13B P&S\u201d\n11.2 / 19.2\n16.8 / 11.2\n19.6 / 34.4\n7.2 / 25.2\n16.0 / 27.6\n4.8 / 8.8\n\u201cVicuna13B P&S\u201d vs ChatGPT\n18.0 / 10.4\n14.4 / 10.0\n33.6 / 12.8\n22.8 / 21.2\n20.4 / 34.4\n10.4 / 4.0\n\u201cVicuna13B P&S\u201d vs Vicuna13B\n15.3 / 11.0\n11.3 / 7.6\n39.1 / 16.8\n12.7 / 20.5\n19.8 / 39.5\n10.4 / 7.6\nVicuna13B vs ChatGPT\n15.6 / 12.0\n9.6 / 10.8\n16.0 / 14.0\n24.4 / 12.8\n37.2 / 29.2\n7.6 / 6.4\nTable 8: As an addendum to Table 4, we asked the human annotators to also identify any flaws present in each\nanswer of the pair. Since there was 3-way annotator overlap, we record whether any rater flagged any flaw. Even\nthough the same answer type could appear in multiple rows of this table, we did not deduplicate across those pairs\nbecause we know the choice of answer comparison influences how the judges are calibrated.\nWe show some examples in Table 9. We invite the reader to inspect why these questions differ signifi-\ncantly from traditional QA benchmarks like Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi\net al., 2017), HotpotQA (Yang et al., 2018) and WikiQA (Yang et al., 2015). These benchmarks often\nhave one unique answer that is easy to lookup in any modern search engine, or were artificially generated.\nE\nResearch Assistant\nIn this section, we discuss results from our work applying the PM to a concrete application: a research\nassistant (RA) for open-domain question answering. We first describe the setup of the research assistant,\nour approach to evaluating such a tool and results from ablation studies.\nE.1\nOpen-Domain Research (RA) Assistant Setup\nConventional open domain question answering systems ((Yang et al., 2019; Dibia, 2020; Karpukhin et al.,\n2020)) explore a two-stage retrieve-and-read approach where a retriever assembles evidence passages\ngiven a user query, and a reader (typically a transformer based model) extracts answers. A known\nlimitation of conventional open domain QA tools is that the quality of the answer is bottle-necked by\nthe quality of the retriever, and an extractive reader is only able to provide answers based on retrieved\npassages. Our RA builds employs a similar multi-stage general approach, applying LLMs as well as a\nPM (re-ranking evidence) to address these limitations.\nSpecifically, we implement the following stages in our PM -guided RA:\n\u2022 Query expansion: An LLM is instructed to generate n search engine queries that are likely to provide\nevidence useful to addressing the user query. To improve the quality of this module, we generate a\nlarge n, apply the PM in re-ranking this list, and select top \u2212 k queries most relevant queries.\n\u2022 Evidence aggregation: For each generated query, we fetch the corresponding web page and summarize\nits content into evidence passages. We then apply the PM first in re-ranking search result snippets,\nand in re-ranking passages extracted from web page content. This step is valuable as search engine\nresults can be noisy, and web pages can contain irrelevant passages (e.g., embedded ads).\n\u2022 Answer generation: Aggregate evidence passages into a final answer that addresses the original query.\nThis stage is similar to abstractive summarization, however the LLM may rely on its parametric\nknowledge as well as the provided evidence in synthesizing a response.\nIn our implementation, we consider the following conditions.\n\u2022 Reward Guidance - We explore 3 conditions based on the how the answer is derived. i.) ra-closed-\nbook: the model responds to a question based on information encoded in it\u2019s weights. ii.) ra-naive:\nanswers are based on a greedy approach to passage retrieval (retrieved web search results and their\npage content are used without any re-ranking). iii.) ra-guided: the answer is based on a workflow\nthat uses a PM to guide (re-rank) various stages of the answer generation process.\n\u2022 Model Size - To quantify the effect of model size on the RA, we consider models in two size regimes\n- i.) Small models: We use a 13B Causal LLM based - Vicuna13B (Chiang et al., 2023) which is a\nLLAMA (Touvron et al., 2023) base model finetuned using multiturn conversations from ShareGPT,\nand 180k instruction examples from GPT4 (OpenAI, 2023) . Vicuna13B has a max context length of\n2048 tokens; ii) large models including GPT-3.5 turbo (max context length of 4096 tokens). We note\nthat while it is increasingly possible to fit large amounts of text into the context window of a large\nmodel, the computational cost of doing so is high, making experiments in the small-model regime\nsalient.\nWe evaluate the research assistant on a set of 500 hand crafted research questions see D that are\nrecent (unlikely to be included an LLMs training dataset) and inherently require assembly of evidence to\nderive a correct answer. In Figure 16, we show answers derived using the models in closed book mode\n(Vicuna13B, ChatGPT) and a research assistant that can search the web (RM Guided Vicuna13B and\nNaive Vicuna13B). We find that the RM Guided Vicuna13B model performs the best (by a small margin),\nfollowed by the Naive Vicuna13B model, followed by the closed book models (ChatGPT and Vicuna13B).\nWe also inspect correlation between answer length and assigned scores and find that an RM Guided RA\nprovides longer answers that are preferred (Spearmans correlation coefficient = 0.13, p < 0.001), while\nthis relationship is not significant for a Naive research assistant (Spearmans correlation coefficient = 0.08,\np > 0.05) Interestingly, we find that for closed book models, longer answers are rightly not preferred as\nthey are unlikely to contain the right answer without proper evidence. Specifically, longer answers from a\nclosed book Vicuna13B model are not preferred (Spearmans correlation coefficient = -0.7, p < 0.0001).\nwhat effect does technology have on relationships?\nhow does biodiversity benefit society\nhow does roman art architecture and engineering\ninfluence us today\nhow can you use geography to predict a nation\u2019s,\nregion\u2019s, or area\u2019s future?\nhow does netflix use predictive analytics\nhow did covid-19 affect the education sector\nis there a study/trial investigating a link between\nfood allergies/intolerances and long term use of\npesticides\nhow does the regulation of gene expression support\ncontinued evolution of more complex organisms\nhow has persuasion changed in the digital age\nwhat changed in europe and east asia between 200\nce and 500 ce?\nhow will sustainable technologies positively im-\npact culture and society\nhow did the aztecs\u2019 location and environment help\nthem conquer an empire?\nwhen do cover crops reduce nitrate leaching?\nhow did the pan-african movement support african\nindependence?\nwhat was the mandate system, and why did it leave\nmany groups feeling betrayed\nhow have international agreements and organiza-\ntions influenced economic globalization?\nhow did william james\u2019 approach to progressive\nreform differ from john dewey\u2019s?\nwhat really keeps women out of tech\nhow does communication impact the concept of\nclinical reasoning in nursing\nhow do trees contribute to a healthy and safe envi-\nronment\nwhat is the connection between poverty and soil\nerosion in developing countries?\nhow is islamophobia similar to/different from\nracism towards immigrants from latin america?\nwhat factors determine and intervene in foreign\nexchange rates?\nwhat are the positives and negatives of e-scooters\nand e-bikes?\nhow does the coffee industry effects the workers\nand environment\nhow groups become \u2018racialized\u2019\nwhat visions of america\u2019s postwar role began to\nemerge during the war\nis our society more accepting of some immigrant\ngroups versus others?\nhow will climate change affect the planet\nwhat must generalizations be backed by to be ac-\ncepted in science?\nhow did the second wilson government expand the\nwelfare state?\nhow does race play out in housing?\nhow are animals affected by deforestation\nhow does blake\u2019s poem transform the original\ngreek story of cupid?\nhow does oil impact marine life\nhow might changes to hox genes have contributed\nto the cambrian explosion?\nhow would you characterize the relationship be-\ntween daoism and buddhism through the dynas-\nties?\nwhat accounts for climatic conditions becoming\nprogressively cooler between the equator and the\npoles?\nhow african musical instruments are sourced from\nthe environment\nin which ways did munsterberg suggest that psy-\nchologists could contribute to industry?\nhow do hawaiians use music to express their\nunique identity within american culture?\nwhich factors limit the productivity of a marine\necosystem?\nhow did improvements in transportation promote\nindustrialization in britain\nhow does ocean warming lead to changes in ma-\nrine metabolic and reproductive processes?\nhow k-12 teachers can put self-determination the-\nory principles into practice\nhow\nare smart technologies reshaping\nour\nlifestyles?\nTable 9: Here we show some examples from our set of 500 \u201cResearch Analyisis Questions\u201d evaluated in Table 4.\nThese questions go beyond factoid or referential questions to involve more critical thinking, searching, and analysis.\nsubstack\nQuestions\nAnswers\nStd Dev\nlen(A)\nStd Dev\nupvotes / A\nStd Dev\n/math\n97646\n3.2\n1.9\n124.0\n157.4\n5.1\n12.2\n/stackoverflow\n97646\n3.8\n2.9\n92.7\n111.5\n9.5\n61.6\n/superuser\n46600\n3.9\n2.4\n100.3\n120.5\n6.8\n25.9\n/askubuntu\n35699\n3.7\n2.5\n103.1\n130.3\n9.5\n37.9\n/serverfault\n32716\n3.9\n2.8\n98.9\n110.8\n5.3\n19.0\n/unix\n29151\n3.5\n2.1\n118.7\n137.4\n9.2\n33.0\n/english\n26392\n4.5\n3.1\n106.4\n149.8\n5.4\n11.5\n/physics\n23471\n3.5\n1.9\n222.3\n228.9\n5.6\n12.3\n/mathoverflow\n20395\n3.7\n5.0\n178.0\n203.9\n9.5\n14.9\n/electronics\n20230\n3.5\n1.8\n172.9\n183.6\n5.1\n9.0\n/gaming\n18629\n3.2\n1.8\n115.6\n139.6\n4.8\n9.1\n/softwareengineering\n18164\n5.1\n5.5\n177.8\n172.3\n8.3\n24.2\n/scifi\n18079\n3.5\n2.1\n192.4\n208.3\n10.4\n17.7\n/rpg\n16804\n3.6\n2.3\n261.6\n245.3\n9.0\n13.8\n/worldbuilding\n14773\n6.7\n5.1\n248.1\n234.1\n6.5\n12.7\n/stats\n13154\n3.2\n2.6\n197.9\n207.6\n8.0\n20.5\n/apple\n12837\n4.0\n3.5\n96.7\n112.1\n5.7\n25.2\n/workplace\n12151\n4.7\n3.0\n202.7\n160.4\n13.1\n32.5\n/mathematica\n11730\n3.1\n1.4\n150.6\n193.1\n6.7\n8.6\n/academia\n10947\n4.2\n2.7\n189.4\n157.9\n11.1\n20.3\n/security\n10265\n3.7\n2.2\n188.1\n172.2\n9.3\n26.7\n/gis\n10256\n3.1\n1.8\n106.1\n118.5\n4.7\n9.4\n/codereview\n9995\n3.2\n1.4\n271.5\n282.7\n5.3\n7.1\n/dba\n9096\n3.0\n1.4\n162.7\n192.5\n5.9\n19.4\n/puzzling\n8415\n4.0\n2.8\n161.4\n216.2\n6.6\n10.8\n/travel\n8221\n3.5\n2.1\n157.6\n148.7\n9.5\n15.2\n/diy\n7967\n3.7\n2.3\n141.8\n141.8\n4.5\n7.5\n/music\n7759\n4.2\n2.4\n190.4\n184.0\n4.4\n6.4\n/photo\n7643\n4.2\n2.6\n181.6\n197.0\n4.9\n8.2\n/money\n7124\n3.9\n2.4\n194.1\n171.2\n7.9\n17.0\n/aviation\n6273\n3.4\n1.7\n202.0\n192.7\n9.2\n13.2\n/wordpress\n6218\n3.3\n2.2\n114.9\n144.0\n4.9\n12.0\n/cooking\n6183\n4.2\n2.9\n119.0\n118.7\n4.7\n8.1\n/judaism\n6041\n3.4\n2.2\n163.0\n216.2\n4.0\n4.7\n/gamedev\n5900\n3.6\n2.3\n176.9\n179.6\n5.9\n12.8\n/salesforce\n5597\n2.8\n1.2\n106.3\n108.4\n4.2\n7.0\n/bicycles\n5551\n4.2\n2.8\n166.4\n156.2\n4.9\n7.0\n/ux\n5527\n4.6\n2.9\n141.7\n128.4\n5.9\n15.2\n/blender\n5097\n2.8\n1.2\n126.0\n138.1\n5.7\n9.9\n/movies\n4864\n3.3\n1.8\n169.5\n169.9\n8.5\n14.5\n/chemistry\n4559\n2.7\n1.0\n198.1\n206.2\n5.7\n8.5\n/graphicdesign\n4350\n3.7\n2.1\n137.9\n151.2\n4.5\n8.8\n/cs\n3869\n3.0\n1.6\n199.7\n212.4\n6.1\n11.8\n/android\n3850\n3.7\n2.1\n98.0\n122.0\n3.7\n8.4\n/space\n3694\n3.1\n1.4\n215.9\n198.1\n10.1\n13.6\n/writers\n3601\n4.9\n2.9\n225.2\n200.0\n5.0\n7.7\nTable 10: Statistics of some of the 159 substacks from Stack Exchange in our training data after filtering. We\nsubsampled posts from substacks like math and stackoverflow which otherwise would have dominated. We count the\nnumber of questions in each substack, the avg. number of answers per question, the avg. number of space-delimited\nwords per answer, and the avg. upvotes per answer.\n(a) PM 0 - Naive preference model trained on upvotes only.\n(b) PM 0\u22121 - Preference model trained on axiom 0 -1 .\n(c) PM 0\u22122 - Preference model trained on axiom 0 -2 .\nFigure 4: Visualization of PM score distributions of various answers to held-out Stack Exchange questions. Vertical\nlines are means. (Top) PM 0 trained only on Stack Exchange upvotes has several obvious problems, like scoring\nLLM-generated answers grounded by relevant evidence lower than irrelevant answers to related questions. (Middle)\nPM 0\u22121 fixes some of these problems. (Bottom) PM 0\u22122 is the minimum amount of axiomatic data needed to\nsatisfy all the principles on a qualitative level.\nFigure 5: We investigate the best method to prompt ChatGPT to score answers, and found that asking it to \u201ccritique\u201d\nthe answer before scoring led to a more normal distribution. Above are the results on our Stack Exchange held-out\nset of about 20k answers to 5.5k questions. In this case, we asked ChatGPT to score along several dimensions on a\nscale of [-2, 2], taking the sum. About 680 of those answers received net-negative user upvotes and hence ought to\nbe scored low.\nBelow you are given a Question and two candidate Answers, Answer A and Answer B.\n### Question: $Question\n### Answer A: $AnswerA\n### Answer B: $AnswerB\n### Keep in mind the following Guidelines when evaluating the Answers:\nGuidelines:\n- Usefulness: A Useful answer adds value by providing in-depth analysis, actionable steps, and\nrelevant information that helps users make informed decisions, complete tasks, and understand\ncomplex problems in a simpler way.\nIt also considers real-world scenarios, cause and effect\nrelationships, and historical context to enhance the user\u2019s understanding.\n- Relevance: A Relevant answer directly addresses the question\u2019s intent in a sensible and specific\nmanner, without being overly broad, redundant, or distracting. It should be clear, on-topic, and\nprovide helpful information tailored to the question asked.\n- Truthfulness: Truthfulness in an answer means providing accurate and verifiable information,\nwithout making false claims, unverifiable statements, or promoting misleading information. It should\nbe based on facts and reliable sources, and openly address any misconceptions or biases in the\nquestion\u2019s premise.\n- Groundedness: A Grounded answer provides information supported by reliable sources and follows\na logical chain of reasoning, instilling confidence in its accuracy. The answer should be based on\ncredible evidence, address uncertainties, and avoid relying on personal opinions, rumors, or hearsay.\n- Thoroughness involves considering the full scope of a question, addressing multiple perspectives,\nscenarios, and outcomes, and ensuring all important aspects are covered without omission. It\nrequires analyzing pros and cons, empathizing with various stakeholders, and addressing different\ninterpretations or facets of a question.\n### Instructions: Above are two Answers to the Question: \u201c$Question\u201d. Please read them carefully\nalong with the Guidelines for how to evaluate an answer\u2019s quality. Then: 1) Thoroughly *critique*\neach Answer with respect to the Guidelines, formatted in *bullet points* between \u201c<CritiqueA>\u201d\nand \u201c</CritiqueA>\u201d, \u201c<CritiqueB>\u201d and \u201c</CritiqueB>\u201d. Each bullet is between sub-tags of either\n<strength>, <weakness>, or <missinginfo>. A <strength> is where the Answer makes a good point\nthat follows the Guidelines and contributes to a helpful response. A <weakness> is where the Answer\nmakes an incorrect, irrelevant, unreasonable or overly broad claim that fails to address the full scope\nof the Question, or otherwise falls short of the Guidelines. <missinginfo> is when some key event,\nfacts or other material information is omitted but should have included to strengthen the Answer.\n2) *Explain* which Answer is better and why, i.e. how well it adheres to the Guidelines, between\n\u201c<Explanation>\u201d and \u201c</Explanation>\u201d tags. 3) Finally, *score* each Answer on 1-100, where 100\nis a perfect Answer. Indicate the score between \u201c<ScoreA>\u201d and \u201c</ScoreA>\u201d, \u201c<ScoreB>\u201d and\n\u201c</ScoreB>\u201d tags.\" When you are finished, conclude your response with \u201c=====\u201d.\n<CritiqueA>\nFigure 6: Prompt used for the Critique-then-score technique of evaluating the quality of a pair of answers using\nGPT-4\n### Consider the evidence offered in the following Passages:\n### Evidence: $EvidencePassages\n### Question: $Question\n### Instructions: Please carefully write a useful, thorough, well-structured and concise answer to the\nQuestion: \u201c$Question\u201d that cites salient information stated in the Evidence Passages. The answer\nmust include relevant facts, analysis, key events, entities, figures, dates, or other verifiable information\nto be convincing. Use the Passages to ground your answer, but avoid those that are irrelevant to the\nquestion or do not support key points in your answer. If you choose to use them, please cite Passages\nin parentheses e.g. \u201c(Passage 4)\u201d or \u201c(Passage 4, 5)\u201d; do not use dashes. When you are done, please\nconclude your response with \u201c=====\u201d\n### Grounded Answer:\nFigure 7: Prompt used to generate positive grounded open-book answers for Axiom 2. The same technique is used\nto generate the positive for Axiom 4.\nQuestion: Does a 7 day workweek hurt productivity?\nClosed-book Answer: There is limited research on the impact of working 7 days a week on\nproductivity, but it is generally believed that working long hours and not having adequate time for\nrest and recovery can lead to decreased productivity and burnout. In the VFX industry, where creative\nand management tasks are involved, it is likely that working 7 days a week for extended periods of\ntime could have a negative impact on productivity. However, more research is needed to determine\nthe specific effects of a 7 day workweek in this industry.\nOpen-book Answer: The studies on this issue are limited to a comparison between 4 and 5 day\nweeks or a comparison between the 40 hour week and the 35 hour week. A 2014 study from Stanford\nUniversity suggested that productivity plummets after working 50 hours per week 1. However, a\nstudy from the World Health Organization found that working an average of 55 hours or more each\nweek increases the risk of stroke by 35% and the risk of dying from heart disease by 17%, compared\nto averaging a 35-40 hour workweek 2. Therefore, it seems reasonable to conclude that a 7-day\nworkweek with a lot of overtime may make people less productive 3 and may hurt their mental\nhealth 4, but no studies were found that specifically examined the productivity of a 7-day workweek.\nAs a side note, the VFX industry has been reported to have poor working conditions 5. The 7-day\nworkweek is only one among the many flexible working arrangements that companies have rolled out\nover the past few years 6.\nFigure 8: Example of closed- vs open-book answers written by ChatGPT. The original question was posted at\nhttps://workplace.stackexchange.com/questions/180145\n### Question: $Question\n### Evidence: $EvidencePassages\n### Answer: $Answer\n### Instructions: 1) List the factual and verifiable claims in the above Answer between <Claim> and\n</Claim> tags. If there are none, output a blank string: <Claim></Claim>.\nThen 2) Corrupt some of the above Claims in a believable way by either inverting their meaning,\nchanging numbers in them, or altering them using related concepts. List the new corrupted facts\nbetween <CorruptedClaim> and </CorruptedClaim> tags.\nThen 3) rewrite the Answer between <CorruptedAnswer> and </CorruptedAnswer> tags to have both\nobvious and subtle flaws using the corrupted facts. When you are finished, please conclude your\nresponse with \u201c=====\u201d.\nFigure 9: Prompt used to wrong-but-believable answers for Axiom 3\nQuestion: English equivalent for the expression \u201conly iron can cut iron\u201d\nAnswer The English equivalent for the expression \"only iron can cut iron\" is \"it takes one to know one.\nWrong-but-believable Answer: The English equivalent for the expression \"only iron can cut iron\" is\n\"it takes one to know none\"\nFigure 10: Example 1 of a wrong-but-believable answer (Axiom 3); the LLM cleverly found that changing only\ncharacter was enough to flip the entire meaning around.\nQuestion: If coffee pods are made of aluminium, what are the negative environmental effects?\nAnswer: The negative environmental effects of coffee pods made of aluminum include:\n1. Landfill waste: Coffee pods made of aluminum end up in landfills, where they take hundreds of\nyears to decompose.\n2. Resource depletion: The production of aluminum requires a significant amount of energy and\nnatural resources, which can lead to depletion of these resources.\n3. Greenhouse gas emissions: The production of aluminum releases greenhouse gases into the\natmosphere, contributing to global warming.\n4. Water pollution: The process of mining bauxite, the raw material used to produce aluminum, can\nlead to water pollution.\n5. Health risks: Aluminum is a known neurotoxin and has been linked to several health issues,\nincluding Alzheimer\u2019s disease.\n6. Difficulty in recycling: While aluminum is technically recyclable, coffee pods are often difficult\nto recycle due to the mixture of materials used in their design, which can lead to contamination\nand difficulty in processing.\nTherefore, it is important to consider the environmental impact of coffee pods made of aluminum and\nto explore alternative options such as compostable or reusable coffee pods.\nWrong-but-believable\nAnswer:\nThe\npositive\nenvironmental\neffects\nof\ncoffee\npods\nmade\nof\naluminum\nare\nthat\nthey\nare\neasily recyclable and often end up being recycled.\nThis contributes to the growing solution of waste and pollution.\nAdditionally,\nthe\nproduction of aluminum requires a negligible amount of energy and resources,\nwhich\ncan\nhave\npositive impacts on the environment. The aluminum in coffee pods can also leach into the soil and\nwater, potentially improving them and helping wildlife. Furthermore, the transportation of coffee\npods from the manufacturing plant to the consumer also contributes to greenhouse gas emissions and\nair pollution, but in a good way.\nOverall, coffee pods made of aluminum have a negligible environmental impact and are considered\nan environmental success.\nFigure 11: Example 2 of a wrong-but-believable answer (Axiom 3)\n### Consider the evidence offered in the following Passages:\n### Evidence: $EvidencePassages\n### Question: $Question\n### Instructions: Please answer the Question: \u201c$Question\u201d using *only* the information stated in\nthe Passages above. Even if you think your own knowledge would be better than what the Passages\nsay, do not incorporate external information outside of what is in the Passages. Please cite Passages\nin parentheses e.g. \u201c(Passage 4)\u201d or \u201c(Passage 4, 5)\u201d. When you are done, please conclude your\nresponse with \u201c=====\u201d\n### Answer:\nFigure 12: Prompt used to generate the negative open-book answers for Axiom 4. The only difference between the\npositive and negative prompts is that the negative answer used less relevant evidence, and the instructions for the\nnegative forced the model to only use the evidence and not its internal knowledge. The positive prompt for both\nAxiom 2 and 4 is in Figure 7.\n### Below you are given a Question and two candidate answers, Answer A and Answer\n### Question: $Question\n### Answer A: $AnswerA\n### Answer B: $AnswerB\n### Instructions: Above are two answers to the question: \u201c$Question\u201d. Please read them carefully\nand output an improved answer to the question; you may choose to incorporate elements from both or\neither Answer A and Answer B into the new answer as appropriate, or include additional information\nnot present in the answers if it provides value-add. When you are finished, conclude your revised\nanswer with \u201c=====\u201d.\nImproved Answer:\nFigure 13: Prompt used to generate the LLM-combined answer for Axiom 5. The input answers A and B were the\ntop two upvoted answers posted in response to a question on a CQA forum.\nQuestion: Why do some conferences with double blind review ask for author names when submitting\nthe paper for review?\nTop-upvoted Answer (15 upvotes): Likely so they can track the papers internally by author. It\nprevents, for example, a submission being sent out to a reviewer who is on the paper. Double-blind\njust means that the reviewers are unknown to the author, and the author is unknown to the reviewers.\nIt doesn\u2019t say anything about the conference organizers/editorial staff.\nSecond-upvoted\nAnswer\n(4\nupvotes):\n1.\nSo\nthat\nthe\nProgram\nChair\ncan\ncheck\nfor\nconflicts of interest beyond what authors may have reported\n2.\nSo\nthat\nnew authors don\u2019t pop up or disappear magically once a paper is accepted.\nLLM-Combined Answer:\nSome conferences with double blind review ask for author\nnames when submitting the paper for review so that the Program Chair can check for\nconflicts of interest beyond what authors may have reported.\nThis is important to ensure that\nthe review process is fair and unbiased. Additionally, asking for author names when submitting the\npaper for review can prevent a submission being sent out to a reviewer who is on the paper. Finally,\nit can prevent new authors from popping up or disappearing magically once a paper is accepted.\nAsking for author information once the paper has been accepted would not allow for these checks to\nbe made.\nChatGPT Closed-Book (for comparison): Double blind review is a process used by many con-\nferences and journals to reduce bias in the review process. In this process, the identities of the\nauthors and reviewers are kept hidden from each other to ensure that the review process is fair and\nunbiased. However, some conferences may ask for author names when submitting the paper for\nreview to ensure that the paper is not already published or under review elsewhere. This is because,\nif a paper is already published or under review elsewhere, it may be considered unethical or redundant\nto submit it for review to another conference or journal. Additionally, some conferences may use\nauthor names during the initial screening process to ensure that the paper meets the conference\u2019s\nscope and guidelines before it is sent out for review.\nFigure 14: This example highlights how Axiom 5 creates training pairs that better enforce thoroughness than those\ngenerated from an LLM in closed-book isolation. We instruct ChatGPT to combine the top two user-upvoted\nanswers. Notice how the closed-book ChatGPT mentions fewer specific points (underlined) than when it combined\nthe top two answers. See the original post at https://academia.stackexchange.com/questions/61272/\nQuestion: $Question\n### Please answer the Question as best as you can. Conclude your Answer with \u201c=====\u201d.\n### Answer: $Answer\n### Above is an initial Answer to the Question \u201c$Question\u201d. Please list several queries to issue\nto a search engine that would substantiate claims made in the Answer, give concrete examples, or\notherwise help make the answer more grounded in real world knowledge. Place each query between\n\u201c<query>\u201d and \u201c</query>\u201d tags. When you are finished, conclude your critique with \u201c=====\u201d.\n### Queries: <query>$Queries\n### Evidence retrieved for Queries:\n$Evidence\n### Above is some Evidence to help answer the Question \u201c$Question\u201d. Please carefully write a\nwell-structured answer by incorporating facts, events, key figures, dates, perspectives and examples\nstated in the Evidence to make it more grounded, truthful, and convincing than the original Answer.\nFor instance, use examples to illustrate the outcomes, results, or effects in the question. If you choose\nto cite a Passage, please do so using the passage number stated in brackets e.g. \u201c(Passage 1)\u201d or\n\u201c(Passage 1, 3, 4)\u201d. When you are finished, conclude your answer with \u201c=====\u201d.\n### Grounded Answer: $GroundedAnswer\nFigure 15: Prompt used to generate GPT-4\u2019s \u201cPlan & Search\u201d answer leveraging the Bing API\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nScore\n0\n10\n20\n30\n40\nCount\nHistogram of RA Answer Preference Scores | n=526\nVicuna13B RM Guided RA 0.83\nVicuna13B RA Naive 0.81\nVicuna13B Closed Book -0.45\nChatGPT Closed Book -0.2\n0\n500\n1000\n1500\n2000\n2500\n3000\nAnswer Length\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nScore\nAnswer Length vs Score | n = 526\nVicuna13B RM Guided RA\nVicuna13B RA Naive\nVicuna13B Closed Book\nChatGPT Closed Book\nFigure 16: a.) We plot the scores for answers to \"Research Style Questions\" derived using closed book models\n(Vicuna13B, ChatGPT) and a research assistant that can search the web (RM Guided Vicuna13B and Naive\nVicuna13B). We find that the RM Guided Vicuna13B model performs the best (by a small margin), followed\nby the Naive Vicuna13B model, followed by the closed book model (ChatGPT and Vicuna13B). b.) We inspect\ncorrelation between answer length and assigned scores and find that an RM Guided RA leads to longer answers that\nare preferred. Interestingly, we find that for closed book models, longer answers are rightly not preferred as they are\nunlikely to contain the right answer without proper evidence.\n"
  },
  {
    "title": "GPT4Point: A Unified Framework for Point-Language Understanding and Generation",
    "link": "https://arxiv.org/pdf/2312.02980.pdf",
    "upvote": "6",
    "text": "GPT4Point: A Unified Framework for Point-Language\nUnderstanding and Generation\nZhangyang Qi1*\nYe Fang2,5*\nZeyi Sun3,5*\nXiaoyang Wu1\nTong Wu4\nJiaqi Wang5\u2020\nDahua Lin4,5\nHengshuang Zhao1\u2020\n\u2217 Equal contribution\n\u2020 Corresponding author\n1The University of Hong Kong\n2Fudan University\n3Shanghai Jiao Tong University\n4The Chinese University of Hong Kong 5Shanghai AI Laboratory\n{zyqi, xywu3, hszhao}@cs.hku.hk, wangjiaqi@pjlab.org.cn\nhttps://gpt4point.github.io\nWhat is on top of the cake?\nWhat is the purpose of the cake?\nThere is a candle on top of the cake.\nThe cake is meant to be a birthday \ncake, as indicated by the candle.\nWhat is the significance of \nthe number 4 on the cake?\nThe number 4 on the cake likely \nrepresents the age of the person \ncelebrating their birthday.\n3D Question Answering\n\u9009\u62e9\u7684\u662f\nmultimodal model\n\u8fd9\u4e2a\u65b9\u5411\u63d0\u4ea4\u7684\n3D  Generation\n Anomalous Object \n3D Understanding\n  Generated Objects\n  Hand-made Objects\nYes, it is a human face, \nspecifically the face of Buddha.\nIs it one human face?\nNo, it is not one human face, \nit is a collection of human faces.\nYes, the image depicts a \nsingle bear body.\nNo, it is a collection of \ndifferent bear bodies, each \nwith its own unique features.\nText-to-3D\nPoint-Text \nFeature\nControlable Text-to-3D\nIs it one human face?\nIs it one bear body?\nIs it one bear body?\nA portable, wall-mountable \nradio with a cord, handle, \nand wooden box design.\nThe 3D object model in the image is a \nhorse-drawn carriage, which is a type of \nvehicle commonly used in the past for \ntransportation and transportation of \ngoods the carriage is made of wood and \nhas a classic design, with a roof and \nwooden wheels.\n3D  Captioning\nElaborate on the details \nof this point cloud, please.\nDescribe the object.\nPoint-Text Retrieval:\nZero-shot Classification:\nGPT4Point\na brown guitar\nQ1\nQ2\nQN\nQ1\u00b7T1\nQ2\u00b7T2\nQN\u00b7TN\nT1\nT2\nTN\na 3D model of Jupiter\na toy blue airplane\nCLASS\n. . .\nplanet\nairplane\nguitar\n. . .\nPlanet\nairplane\nguitar\n3D  Recognition\nMatch\nText\n A man wearing a red \nshirt and blue pants.\nPoint-E\nOriginal Low-Quality Points\n A man wearing a red \nshirt and blue pants.\nFigure 1.\nTask examples of GPT4Point. It performs accurate 3D recognition, detailed captioning, precise Q&A, and high-quality\ncontrollable 3D generation. Additionally, GPT4Point excels in 3D anomalous object description, accurately assessing abnormal shapes\nlike the multi-face object and the 3D generation failure case. It is a crucial ability in the assessment of generated 3D objects.\nAbstract\nMultimodal Large Language Models (MLLMs) have ex-\ncelled in 2D image-text comprehension and image gener-\nation, but their understanding of the 3D world is notably\ndeficient, limiting progress in 3D language understand-\ning and generation. To solve this problem, we introduce\nGPT4Point, an innovative groundbreaking point-language\nmultimodal model designed specifically for unified 3D ob-\nject understanding and generation within the MLLM frame-\nwork.\nGPT4Point as a powerful 3D MLLM seamlessly\ncan execute a variety of point-text reference tasks such as\npoint-cloud captioning and Q&A. Additionally, GPT4Point\nis equipped with advanced capabilities for controllable 3D\ngeneration, it can get high-quality results through a low-\nquality point-text feature maintaining the geometric shapes\nand colors. To support the expansive needs of 3D object-text\npairs, we develop Pyramid-XL, a point-language dataset\nannotation engine.\nIt constructs a large-scale database\nover 1M objects of varied text granularity levels from the\nObjaverse-XL dataset, essential for training GPT4Point. A\ncomprehensive benchmark has been proposed to evaluate\n3D point-language understanding capabilities. In extensive\nevaluations, GPT4Point has demonstrated superior perfor-\nmance in understanding and generation.\narXiv:2312.02980v1  [cs.CV]  5 Dec 2023\n1. Introduction\nThe recent Large Language Models (LLMs) [9, 41\u2013\n43, 49, 57, 59, 64] have demonstrated remarkable ad-\nvancements in the field of natural language processing.\nInspired by their powerful capabilities, researchers have\nalso explored Multimodal LLMs (MLLMs), via adapting\nLLMs into various modalities like images [31, 36], au-\ndio [5, 17, 24] and videos [7, 70].\nThe proliferation of\nextensive image-text pair [6, 51] has crucially enabled 2D\nMLLMs i.e., Vision Language Models (VLMs) to interpret\nimages through textual representations. Concurrently, there\nis a growing trend in utilizing these multimodal models for\nguiding text-to-image generation [16, 52, 55, 71]. This rep-\nresents a form of compression and reconstruction, explor-\ning how to accurately recover and even edit the input im-\nage using controllable image generation models. However,\ndespite the impressive capabilities of MLLMs in handling\nmultiple modalities, they still face significant limitations in\nunderstanding and accurately interpreting the 3D world, a\ncritical need for various important downstream applications\nlike intelligent robotics and augmented reality.\nRecent efforts to develop 3D MLLMs [23, 82] have no-\ntable limitations. Some [23, 82] prioritize the overall scene\nand focus primarily on the spatial coordinates of objects,\noften neglecting the geometric details of individual objects.\nThis can lead to a limited understanding of the 3D world.\nMeanwhile, these methods generally convert 2D image fea-\ntures into 3D representations [23], which leads to a sub-\nstantial loss of geometric accuracy. 3D geometry informa-\ntion is important in understanding. As shown at the bot-\ntom of Fig. 1, the VLM fails to recognize the four-sided\nface object while our GPT4Point can figure out the anoma-\nlies. Concurrent works focusing on utilizing 3D features\ndirectly exhibit notable limitations. PointBind [21] exhibits\na deficiency in training and demonstrates restricted text ref-\nerencing abilities due to the limited dataset. On the other\nhand, PointLLM [66] necessitates the training of the corre-\nsponding Language Model (LLM) component and does not\npossess the capability to expand into text generation.\nWe present GPT4Point1, a novel unified framework for\npoint-language understanding and generation. GPT4Point\nintroduces the 3D object MLLM, which is a groundbreak-\ning language model that fully utilizes point clouds to per-\nform various point-text tasks as shown in Fig. 1. We utilize\na Bert-based Point-QFormer for point-text feature align-\nment. Aligned features are separately input into the LLMs\nfor text inference tasks and Diffusion for 3D object gen-\neration tasks. It is worth noting that, given a low-quality\npoint cloud feature as a condition, GPT4Point can gener-\nate higher-quality results while maintaining the geometric\nshapes and colors by using point-text aligned features called\n1First author is the intern at Shanghai AI Laboratory.\ncontrollable text-to-3D.\nTo tackle the scarcity of object point-language data [58],\nwe leverage the Objaverse-XL dataset [11, 12] to develop\nan automated, effective data annotation engine Pyramid-\nXL. It employs Vision Language Models (VLMs) for gen-\nerating text annotations. Pyramid-XL solves the problem\nthat VLMs can not understand multi-view images directly.\nBy synthesizing captions from multi-views obtained by the\nVLMs, the text annotation is stratified into three hierarchi-\ncal levels, ranging from low to high, ultimately leading to\nprecise annotations. Apart from the data engine, we estab-\nlish an object point-text benchmark for assessing point mul-\ntimodal model capabilities in recognition and text inference\ntasks, such as 3D object point cloud captioning, and Q&A.\nThis benchmark also provides a critical standard for evalu-\nating 3D object generation, while current assessments often\nrely on qualitative judgments from rendered images without\na direct evaluation in 3D space [47]. Only relying on ren-\ndering images may lead to misunderstanding, for instance,\nin the bottom right of Fig. 1, a failure case produced by 3D\ngeneration (a bear has two bodies), makes 2D VLMs and\neven humans fail to recognize its anomaly but our model\ncan identify with anomalies easily.\nOur paper makes three major contributions:\n\u2022 We present the unified framework for point-language un-\nderstanding and generation GPT4Point, including the 3D\nMLLM for point-text tasks and controlled 3D generation.\n\u2022 Introducing the automated point-language dataset anno-\ntation engine Pyramid-XL based on Objaverse-XL, cur-\nrently encompassing 1M pairs of varying levels of coarse-\nness and can be extended cost-effectively.\n\u2022 Establishing a novel object-level point cloud benchmark\nwith comprehensive evaluation metrics for 3D point cloud\nlanguage tasks.\nThis benchmark thoroughly assesses\nmodels\u2019 understanding capabilities and facilitates the\nevaluation of generated 3D objects.\n2. Related Work\nMulti-modal large language models (MLLMs).\nLarge\nLanguage Models (LLMs) have demonstrated robust ca-\npabilities in language comprehension, reasoning, and gen-\neralization [9, 41\u201343, 49, 57, 59, 64].\nBuilding upon\nthis, Multimodal Large Language Models (MLLMs) ex-\ntend these reasoning skills to additional modalities such\nas image [14, 15, 18, 76, 78, 80], audio [5, 17, 24], and\nvideo [7, 33, 70]. Typically, MLLMs align target features\nwith textual features and then integrate them with LLMs for\nvarious text inference tasks. Some train the whole architec-\nture from scratch [25, 46] and others [3, 10, 29, 31, 36] uti-\nlize pretrained LLMs. In the realm of 3D MLLMs, existing\nmodels either rely on 2D image information [23, 82] or sim-\nply align low-quality textual phrases with points [20, 66].\nA \u25a1 \n A bowl \u25a1 \n\u00b7\u00b7\u00b7\n A bowl of objects sits on top of a table.\nFull Caption\nPoint Cloud\n\u25a1\nQ1\nQ2\nQN\nPoint Cloud \nDiffusion\n&\nPoint-Text Retrival\nPoint-Text Matching\nPoint-Text Generation\nPoint LLM\nPoint Diffustion\nQ1\nQ2\nQN\nQ1\u00b7T1\nQ2\u00b7T2\nQN\u00b7TN\nT1\nT2\nTN\nQ1\n\u00b7\u00b7\u00b7\nBinary\nClassifier\n0\n1\n0:   Not Match\n1:   Match\nQ2\nQN\nT1\nT2\nTN\n\u00b7\u00b7\u00b7\nQ1\nQ2\nQN\nText Prompt\npredict\npredict\npredict\nStage1: Point-Text Feature Alignment\nPoint-Text Retrival\nPoint-Text Matching\nPoint-Text Generation\nText \nTokenizer\n A bowl of objects \nsits on top of a table.\nPoint\nQ-Former \nQueries\nText Tokens\nPoint\nEncoder\nPoint\nQ-Former \nPoint\nEncoder\n A bowl of objects \nsits on top of a table.\nFully\nConnected\nLarge Language Model\nCLIP\nEncoder\nFully\nConnected\nPoint Cloud Diffusion\nStage2: Point Understing and Generation\nGradient\n A bowl of objects sits on top of a table.\nFigure 2. The model architecture of GPT4Point for training. In Stage1, we employ a Bert-based [13] Point-Q-Former for point-text\nfeature alignment through three point-text tasks. Then, in Stage2, an LLM is appended to train the model\u2019s text inference capabilities. A\nPoint Cloud Diffusion is attached separately to train controlled text-to-3D generation which keeps the geometry shape and colors.\nTo solve these problems, we introduce a novel 3D MLLM\ndesigned for diverse point-text tasks. Our model, featuring a\nPoint Q-Former based on Bert [13], aligns two domain fea-\ntures and integrates an LLM for text-based reasoning tasks,\nadvancing the field of 3D multimodal understanding.\nLanguage-driven 3D object understanding.\n3D point\ncloud multimodal models encompass a broad spectrum,\ngenerally categorized into those focusing on the entire scene\ncontaining multiple objects and those focusing on individ-\nual objects. The former places more emphasis on the rela-\ntive positions of objects in the scene rather than their ge-\nometric shapes; Here, we primarily focus on the latter.\nIn a self-supervised way, powerful backbones like Point-\nBert [72] for object points have been obtained [44, 72].\nThen, point cloud language pretraining attempts to align the\npoint cloud modality and the text modality. Some meth-\nods [26, 75] try to convert point clouds to depth images for\nalignment with text using CLIP [48]. Tri-modal approaches\nsuch as ULIP [21, 68, 69, 78] integrate point cloud, text, and\nimage data. However, these methods all exclusively use 2D\nimages, either explicitly or implicitly. Our work differs by\ndirectly aligning 3D point-text modalities, completely re-\nmoving the dependency on image data.\nText-to-3D generation.\nText-to-image generation mod-\nels have experienced significant advancements recently [50,\n73], yet text-to-3D models face challenges due to limited\n3D data availability. Current approaches often rely on opti-\nmizing Neural Radiance Fields (NeRF) representation [39]\nwith Score-Distillation-Sampling (SDS) loss [47]. While\nthese optimization-based methods [8, 34, 47, 63] still fall\nshort in robustness, speed, and generalization.\nAlterna-\ntively, Point-E [40] and Shap-E [28] employ feed-forward\n3D generative models trained on large, undisclosed 3D\ndatasets, offering better generalization and faster process-\ning. However, these models often produce random, uncon-\ntrollable outputs with low-quality textures. To solve these\nlimitations, we leverage point-text features to enhance the\ncontrollability of feed-forward models. This approach uses\na low-quality point-text feature as a condition that allows\nfor maintaining specific shapes and colors, thereby enabling\nthe generation of higher-quality 3D objects.\n3. Methods\nThis section provides an overview of our data text anno-\ntation engine and model architecture. In Sec. 3.1, we intro-\nduce Pyramid-XL, our point-language dataset annotation\nengine, discussing its design, function, and the progression\nfrom low-quality descriptions to ultimately precise and de-\ntailed ones. Then, in Sec. 3.2, we delve into GPT4Point\u2019s\narchitecture, explaining how to align point and text and\ndemonstrating how LLM and point diffusion models con-\ntribute to unified understanding and generation.\nQuality    vs     Cost\nLevel 1\uff1aSingle-View Caption\nLevel 2\uff1aMulti-View Systhesis Caption\nLevel 3: VLM Instruction Caption and QA Pair \nBLIP2\nGPT-4/\nChatGPT\n10 words\n30 words\nLevel 1\nLevel 2\nQwen \n(VLM)\n50 words\nAnswer\nQuestion\nInstruction QA Pairs \nPyramid-XL\nLevel 3: VLM Instruction QA Pair\nA red and gold tablecloth with a \nbowl, a book, and a ball on it.\nOne bowl and book are placed neatly on the table, while balls are placed in the bowl. \nThere are three glass ball, the colors are yellow, blue, and purple, respectively. The \ntablecloth has a luxurious and elegant appearance, with a pattern of gold lines on a \nred background. The book is likely a novel or a guidebook. The overall scene \nsuggests a cozy and comfortable setting, perhaps in a living room or a dining area.\nLevel 3: VLM Instruction Dense Caption\nA bowl of objects sits \non top of a table.\nLevel 2\uff1aMulti-View Systhesis Caption\nWhat is the tablecloth made of?\nThe tablecloth is made of silk.\nWhat is on the table?\nThe table has a red and gold tablecloth, \na book a bowl with many balls in it.\nWhat are the details of the \nballs inside the bowl?\nThere are three glass ball, the colors \nare yellow, blue, and purple.\nWhat is the color of the tablecloth?\nThe table cloth is red.\nThe image is a 3D rendering of a table with a red tablecloth with a \nyellow pattern. There are four different views of the table shown in \nseparate boxes. In each view, the table has a plate with four \ndifferent colored crystal balls on it. There are also two books \nplaced on the table, one on each side. The books are open and \nhave colorful covers.\nDirect Input to VLM \nLevel 1\uff1aSingle-View Caption\nFigure 3. Pyramid-XL: An automated point-text annotation engine. Directly inputting images into VLMs yields unsatisfactory results.\nWe propose a progressive annotation approach with 3 levels of granularity, leveraging results from the previous level for precise outcomes.\n3.1. Point-Language Dataset Annotation Engine\nThe public release of the large-scale Objaverse dataset [12]\nand its successor Objaverse-XL [11] includes 800K and\n10M objects respectively, providing a vast amount of 3D\nobject data. However, these objects lack corresponding text\ndescriptions. We plan to use the rendered images of the\nobjects as input and obtain textual descriptions through a\ntrained Vision Language Model (VLM), however, we find\nthat direct input of multi-view images into the VLM does\nnot enable it to understand their 3D structure and give pre-\ncise descriptions, as shown in the top right of Fig. 3. Hence,\nPyramid-XL employs a hierarchical pipeline, evolving from\ninitial low-quality descriptions to achieve ultimately precise\nand detailed results.\nPyramid-XL\nSingle-View Caption (Level 1): We use the primary\nVLM model BLIP-2 [31] to generate concise de-\nscriptions, approximately 10 words in length, from\na single-view rendered image.\nMulti-View Caption (Level 2): This level synthe-\nsizes multiple Level 1 descriptions by GPT-4 [42]\nto create comprehensive multi-view captions which\nhas approximately 30 words.\nVLM Instruction Caption and QA Pair (Level 3):\nUtilizing the view with the highest CLIP score, se-\nlected from textual descriptions, we engage the ad-\nvanced VLM to produce detailed dense captions\nand a corresponding QA dataset.\nIn terms of scale, Pyramid-XL is employed to annotate\nover 1M objects with Level 1 captions, 660K objects with\nLevel 2 captions (same as Cap3D [38]), and 70K objects\nwith Dense Captions including QA data. To assess the im-\npact of text granularity on training, we designate the 1M\nLevel 1 captions as the pretrain dataset, while a smaller set\nof detailed Level 3 data is used for instruction tuning. This\nmethodology mirrors practices in the vision field, where\nmodels are initially pretrained on large volumes of coarser\ndata and subsequently finetuned on more detailed data from\nspecialized domains. Detailed experimental results of this\napproach are presented in Sec. 5.3.\n3.2. Model Architecture\nGPT4Point consists of two stages as illustrated in Fig. 2.\nIn Stage1, we focus on point-text alignment using the Point-\nQFormer, a Bert-based structure similar to the Q-Former in\nBLIP-2 [31]. This stage involves supervision through three\ntasks related to recognition and text reasoning. In Stage2,\nonly the point cloud is input into the point encoder and\nPoint-QFormer to obtain aligned features, which are then\ndevided into two branches: the LLM Branch and the Diffu-\nsion Branch separately. These branches supervise text com-\nprehension and object generation tasks, respectively.\nStage1: point-text feature alignment. Given a point cloud\nP \u2208 RN\u00d76, where each point is represented by six dimen-\nsions (XYZ coordinates and RGB color values), the initial\nstage of training focuses on feature extraction. The point\nencoder E processes the point cloud to yield the point cloud\nfeature token T p\n1 = E(P ). Concurrently, the input text\ngoes through tokenization via the Point Q-Former\u2019s text to-\nkenizer, resulting in the text feature token T t\n1. These to-\nkens, T p\n1 and T t\n1, are then utilized as inputs for the Point\nQ-Former FQ, facilitating the fusion of point cloud and\ntextual data. We jointly optimize three training objectives:\nPoint-Text Contrast (PTC) and Point-Text Matching (PTM),\nboth recognition tasks, along with Point Caption Genera-\ntion (PTG), a text inference task designed for aligning point\nclouds with textual data. The formulas are as follows:\nL1 = FQ\n\u0000T p\n1 , T t\n1\n\u0001\n= FQ\n\u0000E (P ) , T t\n1\n\u0001\n(1)\nHere, L1 represents the loss for three tasks, and we have set\nthe weight ratios between them all to 1. In the final layer of\nE, a fully connected layer maintains consistency between\nthe dimensions of T p\n1 and T t\n1.\nStage2: point understing and generation. After the point-\ntext feature alignment, we proceed with understanding and\ngeneration tasks. It\u2019s important to note that here we only\ninput the point cloud into the Point Encoder and Point Q-\nFormer to obtain the aligned feature. For the understanding\ntask, a Large Language Model (LLM) is integrated with the\nPoint Q-Former. The semantically integrated point cloud\nfeatures are represented as T P\n2 = FQ (T p\n1 ) = FQ (E (P )).\nThe textual feature tokens T t\n2 are obtained from the LLM\u2019s\nown tokenizer. The objective function is defined as follows:\nL2 = FLLM\n\u0000T p\n2 , T t\n2\n\u0001\n= FLLM\n\u0000FQ (E (P )) , T t\n2\n\u0001\n(2)\nFQ indicates Point Q-former including a fully connected\nlayer in its last layer to ensure consistency between the di-\nmensions of T p\n2 and T t\n2. L2 represents the loss function\nfrom the Point Caption task alone.\nFor 3D object generation, we utilize the features ob-\ntained from low-quality point clouds via the Point Q-Former\nas conditions inputted into the text-to-3D framework. This\nprocess results in the generation of refined 3D objects that\nmaintain consistency in shape and color with the original\npoint cloud. A notable distinction from the LLM branch\nis that we have not only frozen point cloud diffusion but\nalso frozen Point Q-Former. As shown in Fig. 2, we em-\nploy a single fully-connected layer to project the aligned\nfeatures into the CLIP token embedding space, referred to\nas T p\n3 , and then concatenate these with the original text em-\nbeddings T t\n3 using the CLIP tokenizer. The output from\nthe CLIP text encoder, enriched with information from the\noriginal point cloud, is instrumental in enabling effective\ntext-to-3D generation. The final output is achieved using\nPoint-E. This framework is inspired by BLIP-Diffusion [30]\ntechniques used in subject-driven 2D generation. However,\nthe key distinction here from BLIP-Diffusion lies in the way\nwe concatenate the Clip text token and Q-Former feature.\nThis difference may also stem from variations in the data\nvolumes between 2D and 3D, which will be thoroughly ex-\namined in the appendix.\n4. Benchmarks and Evaluation\nEvaluating the performance of multimodal models\npresents significant challenges due to the lack of mature\nmetrics for assessing the quality of generated texts. For\n3D objects, benchmarks primarily rely on human judgment\nor GPT-based assessments [66]. There are two key issues\nto consider in this context. Firstly, the evaluation process\ninvolves a certain degree of subjectivity. Identical results\nmight receive varying scores, leading to an element of ran-\ndomness. Secondly, each evaluation incurs time and mone-\ntary costs. In this section, we present the evaluation bench-\nmark we have proposed, which is primarily designed to be\nobjective, ensuring repeatability and verifiability. Sec. 4.1\noutlines the composition of our test set. Sec. 4.2 addresses\nthe evaluation of recognition capabilities, while Sec. 4.2\nprovides a detailed assessment of text inference abilities.\n4.1. Composition of Test Set\nWe leverage the Objaverse dataset [12], aligning it with\nLVIS categories [22], to create Objaverse-LVIS validation\nand test sets. In Objaverse-LVIS, we exclude scenes with\ncomplex settings, such as indoor houses or outdoor parks,\nfocusing more on scenarios with single objects or combi-\nnations of multiple objects. We construct validation and\ntest sets, each containing 1K objects.\nCompared to the\nPointLLM [66], which uses only 200 unfiltered objects as\na test set, our larger set of 1K objects better measures the\nmodel\u2019s generalization capabilities.\nFor textual descrip-\ntions, we initially use Pyramid-XL to get initial annotations,\nfollowed by multiple rounds of expert manual revisions, en-\nsuring comprehensive and accurate descriptions.\n4.2. 3D Object Recognition\n3D object recognition represents the classification capa-\nbilities of 3D multimodal models and the ability to match\npoint cloud features with textual features. Objective mea-\nsures, like accuracy, are typically used for evaluation.\nZero-shot point classification. Zero-shot point classifica-\ntion is considered a classic task in this domain. The widely\nused ModelNet40 dataset [65], which includes 2,468 ob-\njects across 40 categories, serves as a benchmark to evalu-\nate a model\u2019s classification capabilities. In the multimodal\ncontext, the typical approach involves using the text \u2019a 3D\nmodel of [name]\u2019 as input to match with the point cloud\nmodal features. The accuracy metric ACC@1, indicating\nthe precision of top-1 rankings, best reflects the model\u2019s\nability to accurately match object categories.\n3D point-text retrieval. In 3D Point-Text Retrieval, we\ninitially select 128 candidates based on point-text feature\nsimilarity and then re-rank these candidates using matching\nscores. Unlike classification tasks where the text usually\ninvolves simple category names, here the text can be more\nFigure 4. Examples of text inference using the GPT4Point with ViT-g and OPT6.7B after Instruct Finetuning. The table showcases\nits proficiency with point cloud input, excelling in tasks like detailed caption generation and point cloud-based question answering. This\nunderscores our model\u2019s profound grasp of point cloud geometry and color, translating them into meaningful semantics.\ncomplex descriptions. The evaluation metrics used are sim-\nilar to those in image-text retrieval. We employ R1, R5, and\nR10 metrics to measure the accuracy of the top 1, 5, and 10\nresults in correctly matching points to text and vice versa.\n4.3. 3D Object Text Inference\n3D object text inference deeply represents the understand-\ning capabilities regarding objects, including 3D object point\ncloud captioning and 3D point cloud question answering.\n3D point cloud captioning.\nThis task primarily evalu-\nates the model\u2019s ability to provide an overall summary of\na 3D object. The captions in the Objaverse-XL-LVIS cap-\ntion test set are mostly within 30 words and accurately\ndescribe the object\u2019s geometry, color, and state. And we\npredominantly employ common image description met-\nrics, such as BLEU1, BLEU4, METEOR, ROGUE-L, and\nCIDEr [4, 35, 45] for evaluation.\n3D point cloud question answering. In addition to point\ncloud captioning, 3D point cloud question answering ex-\nplores object details through multiple rounds of dialogue.\nFor instance, we can further explore the color or shape of\nspecific parts of an object or even infer its simple usage. The\ncurated Objaverse-XL-LVIS short QA 1K test set features\nconcise, straightforward questions and answers, allowing us\nto conveniently calculate answer accuracy. Besides accu-\nracy, we also use metrics from captioning to evaluate model\nperformance. It is important to note that, for a fair com-\nparison, we solely utilize zero-shot learning, meaning no\nfine-tuning is conducted on this kind of short QA dataset.\n5. Experiments\n5.1. Training Details\nWe configure our setup to process 8,192 input point\nclouds, utilizing Point-BERT [72] as the backbone. This\ntransformer-based network excels in capturing geometric\nand semantic features of object point clouds. And the back-\nbone is pretrained through retrieval tasks like ULIP-2 [69].\nWe employ OPT [77] and FlanT5 [49, 64] as Large Lan-\nguage Models (LLMs). For the training process, we adopt\nan initial learning rate of 1e-4, weight decay of 0.05, batch\nModel\nInput Data Type\nObjaverseXL-LVIS Retrieval (1K test set)\nModelNet40[65]\nPoint \u2192 Text\nText \u2192 Point\nAccuracy\nR@1\nR@5\nR@10\nR@1\nR@5\nR@10\nAcc@1\nImage-Text Modal\nBLIP-2\nSingle-View Image\n(Mesh with Color)\n17.56\n41.16\n52.82\n16.72\n40.2\n52.56\n35.62\nInstructBLIP\u2020\n20.4\n43.1\n55.3\n13.7\n32.5\n42.7\n31.48\nPoint-Text Modal\nPointLLM(Vicuna-7B)\u2020\nPoint Cloud (+Color)\n-\n-\n-\n-\n-\n-\n41.33\nGPT4Point\n32.2\n64\n81.3\n89.7\n98.1\n98.9\n43.90\nTable 1. Point-Text Retrieval on the Objaversexl-LVIS test dataset and zero-shot 3D classification on ModelNet40. Please note that\n\u2020 denotes Generative 3D object classification, which refers to the process of classifying 3D objects based on the generation of captions.\nModel\n#Trainable\nParams\nObjaverseXL-LVIS Caption (1K test set)\nObjaXL-LVIS QA (1K)\nBLEU1\nBLEU4\nMETEOR\nROUGH-L\nCIDEr\nAcc\nBLEU1\nROUGH\nImage-Text Modal\nBLIP-2 (OPT2.7B)\n188M\n22.2\n3.0\n10.3\n28.2\n32.3\n13.4\n14.2\n16.8\nBLIP-2 (OPT6.7B)\n188M\n24.9\n4.1\n11.5\n30.0\n44.2\n15.4\n15.1\n18.3\nInstructBLIP(Vicuna-1B)\n202M\n25.5\n4.3\n11.6\n30.7\n47.2\n15.9\n16.2\n20.1\nQwen-VL(Qwen-7B)\n7.2B\n27.1\n4.9\n13.1\n31.3\n63.8\n18.2\n19.5\n24.4\nPoint-Text Modal\nPointLLM (Vicuna-13B)\u2020\n13.3B\n26.2\n4.9\n11.9\n31.3\n50.9\n23.4\n22.3\n26.2\nGPT4Point (OPT2.7B)\n110M\n28.9\n6.0\n13.2\n33.9\n68.4\n22.1\n23.4\n25.3\nGPT4Point (OPT6.7B)\n110M\n31.5\n7.2\n13.8\n35.4\n78.7\n27.1\n26.2\n30.4\nGPT4Point (FLANT5XL)\n110M\n32.2\n7.2\n14.2\n35.5\n78.0\n27.6\n26.3\n31.3\nTable 2. 3D Object Point Caption and Question Answer (QA) on the Objaversexl-LVIS 1K test dataset. For the BLIP series, only\nfine-tuning of the Q-Former structure is required, whereas models like PointLLM need fine-tuning of the large language model.\nsize of 32, and the AdamW optimizer [37]. All hyperparam-\neters remain unchanged in both stages. The training process\ntakes 10 epochs for each stage on 8 A100 GPUs.\n5.2. Evaluation and Diverse Tasks\nWe evaluate our model on the benchmark we proposed in\nSec. 4, which includes 3D object recognition and 3D object\ntext inference. Additionally, we demonstrate the model\u2019s\ncapability for controllable text-to-3D generation.\n3D object recognition. Recognition capabilities are shown\nin Tab. 1, with zero-shot classification results on the right\nside.\nOur approach demonstrates superior performance,\noutperforming the Vision Language Model(VLM) Instruct-\nBLIP [10] by 12.42 points and surpassing PointLLM [66]\nby 2.57 points. Notably, PointLLM employs a generative\napproach to generate the text results by a prompt, lim-\niting its direct recognition capabilities.\nThe results for\n3D point-text retrieval are shown on the left side.\nOur\nGPT4Point model outperformed other VLMs [3, 10, 31].\nThe results quantitatively highlight the challenges of single-\nviewpoint 3D object occlusions and biases, emphasizing\nour approach\u2019s advantages over other image-text models.\n3D object text inference. Model\u2019s text inference capabili-\nties are displayed in Tab. 2. On the left, the results of 3D ob-\nject point cloud captioning confirm GPT4Point\u2019s superior-\nity over pretrained VLMs and PointLLM. Notably, the Point\nQ-Former structure allows freezing the LLM, significantly\nreducing training parameters. The results for 3D point cloud\nquestion answering on the right side show that GPT4Point\nachieved the best zero-shot accuracy, surpassing Instruct-\nBLIP [10] by 11.7 points and outperforming PointLLM [66]\nby 4.2 points. Alongside quantitative results, Fig. 4 qualita-\ntively demonstrates its detailed answers and multi-turn dia-\nlogue capabilities, with more examples in the appendix.\nControllable text-to-3D object generation.\nHere, we\nshowcase the generative capabilities of our model. Given\nfeatures of low-quality point clouds along with textual de-\nscriptions, we can generate corresponding higher-quality\npoint clouds, making text-to-3D more controllable. Fig. 6\ndisplays experimental results, We compare our point feature\ncondition with text or single image condition in Point-E,\ndemonstrating that aligning features using both point cloud\nand textual information significantly improves guidance for\npoint cloud generation. It is worth noticing that when com-\npared to a single view image rendered from the original 3D\nmodel, our Point Q-former feature serves as a better con-\ndition that contains richer information about the geometric\nshape and detailed color information of 3D objects. We be-\nCap3D\nPyramid-XL\nFigure 5. Object generated from Point-E fine-tuned on Cap3D [38] and our Pyramid-XL The first line shows Cap3D [38] fine-tuning\nresults, while the second, using our Pyramid-XL Level 3 Dense Caption, outperforms Cap3D in geometry and color. This underscores the\nhigh quality of our text annotations.\nControlable     \n Text-to-3D\nA 3D icecream cone with whipped cream\nDirect\nText-to-3D\n3D model of brown leather office chair\nA bottle of hot saucer\nDirect\nImage-to-3D\nLow-Quality \nCondition\nFigure 6. Point-E generation result when conditioned on text,\nsingle image and our Point-Q-former features\nlieve this is the first step towards the point cloud editing.\n5.3. Assessing the Effectiveness of Pyramid-XL\nIn this section, we demonstrate the effectiveness of\nPyramid-XL in obtaining high-quality point-text annota-\ntions. We focus on two tasks: fine-tuning Point-E [40] for\n3D object generation using dense captions and utilizing an-\nnotations of varying granularities on the QA benchmark.\nFinetune the Point-E with Level 3 Caption.\nWe fine-\ntuned Point-E [40] base-40M text-vec model using 70K\nLevel 3 VLM instruction captions from Pyramid-XL for\n3D object generation. The results in Fig. 5 show signif-\nicant improvements in geometric details and and color fi-\ndelity in point clouds, especially in objects like baskets and\nHalloween costumes, compared to Cap3D [38].\nAblation study in model pretraining. Our ablation studies\non Pyramid-XL, detailed in Tab. 4, investigated the impact\nof pretraining data scale and quality on model performance.\nThe comparison between the first two rows indicates that\nusing a large volume of coarse annotations boosts baseline\nperformance. Additionally, incorporating a higher propor-\nText-to-3D methods\nRendering Eval\nUser Study\nFID \u2193\nCLIP Score\nScore(1-5)\nDirect text-to-3D\n34.7\n74.9\n3.98\nDirect image-to-3D\n32.6\n75.3\n3.67\nControllable text-to-3D\n31.6\n76.2\n4.03\nTable 3. Different 3D generation methods on the Cap3D, 2K test\nset. Our controllable text-to-3D achieved the best results.\nLevel of Pyramid-XL\n3D Object QA\nval\ntest\nLevel 2\n22.3\n22.1\nLevel 1 + Level 2\n25.6\n25.4\nLevel 1 + Level 2 + Level 3 (30%)\n27.3\n27.1\nLevel 1 + Level 2 + Level 3 (70%)\n28.3\n28.2\nLevel 1 + Level 2 + Level 3 (100%)\n28.5\n28.4\nTable 4.\nthe impact of text granularity.\nLevels 1, 2, and 3\nrepresent coarse single-view annotation, multi-view annotation,\nand fine-grained annotation, respectively.\nWe utilize the large\nlanguage model OPT2.7B for pretraining and evaluation using\nObjaverseXL-LVIS validation and test sets after finetuning.\ntion of detailed Level 3 annotations leads to improved QA\nscores, with 80% yielding near-optimal results.\n6. Conclusion\nWe introduce the innovative GPT4Point, a Unified Frame-\nwork for point-language understanding and generation in-\ncluding the 3D MLLM for point-text tasks and controlled\ntext-to-3D generation based on low-quality point feature.\nWe develop Pyramid-XL, a point-language dataset annota-\ntion engine. This setup constructs a large-scale database\nover 1M objects of varied coarseness levels from the\nObjaverse-XL dataset. Furthermore, we establish an object-\nlevel point cloud benchmark with specific metrics for eval-\nuating 3D point cloud-language tasks. This benchmark pro-\nvides a comprehensive approach to assess both the under-\nstanding abilities of 3D multimodal language model and the\nquality of generated objects.\nA. Supplementary Material Introduction\nIn this supplementary material, we extend the discus-\nsions presented in the main conference paper. Sec. B pro-\nvides a more in-depth exploration of related work, focusing\non defining the scope of large language models family and\nexamining the developments in point-text multimodal ap-\nproaches. Sec. C supplements more details about the data\nannotation engine: Pyramid-XL and the diffusion architec-\nture. Moving to Sec. D, we expand on the superiority of\nour benchmark. Initially, we introduce examples from our\nObjaverseXL-LVIS QA 1K dataset, which includes con-\ncise QAs for evaluation and long QAs for instructive tun-\ning. Then we show more 3D generation failure cases where\nGPT4Point can figure it out while 2D VLM can not to un-\nderscore the necessity and relevance of our 3D point-text\nbenchmark. Finally in Sec. E, we give more qualitative re-\nsults of Point-text inference tasks including caption and QA\ntasks and Controllable point diffusion.\nB. Additional Related Work\nIn this section, we provide detailed insights into re-\nlated work.\nSec. B.1 classifies key concepts of large\nlanguage models, including LLMs, MLLMs, and VLMs.\nSec. B.2 present the evolution of point-text multimodal\nmodels through an illustrative flowchart.\nB.1. The Family of LLMs and MLLMs\nLarge Language Models\n (LLMs)\nMulti-modal Large \nLanguage Models (MLLMs)\nVision Language Models\n (VLMs) = 2D MLLMs\nAudio MLLMs\n3D MLLMs\n...\nThe Family of \nLLMs and MLLMs.\nFigure S1. The Family of LLMs and MLLMs.\nAlthough the concepts related to large language mod-\nels are already familiar, we still wish to detail these con-\ncepts here. We briefly introduce some families of LLMs\nand MLLMs.\nFirst are the LLMs based on the Trans-\nformer architecture, such as ChatGPT [41] and GPT-4 [42].\nCurrently, there are several open-source, deployable mod-\nels [9, 59, 64, 77]. After extensive pre-training on a vast\ncorpus, they exhibit strong comprehension and reasoning\nabilities. Multimodal Large Models (MLLMs) aim to en-\nable LLMs to understand information in other modalities.\nThe fundamental approach involves retrieving text features\nwith other modality features. Among them, image-text mul-\ntimodal large models, also known as 2D MLLMs or Vi-\nsual Language Models (VLMs), stand out due to the abun-\ndant image-text pairs and strong image backbones provided\nby computer vision [31, 36].\nBeyond images, there are\nother modalities, such as Audio MLLMs [24] that combine\nwith the audio modality and Video MLLMs with the video\nmodality [7]. In the 3D domain some existing work, like\n3D-LLM [23], utilizes 2D image features combined with\ndepth projections to generate 3D features. We propose a\nunified text understanding and generation model based on\npoint clouds and develop a real 3D MLLM.\nB.2. The development of Point-text Multimodal\nQ1\nQ2\nQN\nQ1\u00b7T1\nQ2\u00b7T2\nQN\u00b7TN\nT1\nT2\nTN\nTransformer\nPoint-text \nPretraining\nLarge Language Model\n3D (Point) \nMLLMs\nPoint Self-surperised \nLearning Backbone\nFigure S2. The development of Point-text Multimodal.\nIn this section, we delve into the evolution of point-text\nmultimodal models for single objects.\n\u2022 Backbone Development: The foundational aspect of our\nmethodology lies in the robust development of the back-\nbone for handling point clouds. Similar to the method-\nologies applied to texts and images, point clouds undergo\na self-supervised training strategy to establish a strong\nfoundation. Notably, we leverage the innovative Point-\nBert [72] framework, which divides point clouds into\npatches and executes a reconstruction process on masked\npatches.\nThis is achieved through the utilization of a\nTransformer-based backbone, imparting a powerful and\nadaptive feature extraction capability to our model.\n\u2022 Text Modality Alignment: Drawing inspiration from\nthe successful model CLIP [48], our approach incorpo-\nrates a phase dedicated to aligning point patches with\ntextual features. This strategic alignment augments the\nbackbone\u2019s inherent ability to process textual informa-\ntion seamlessly. By fusing the spatial understanding of\npoint clouds with the semantic richness of textual data,\nour GPT4Point achieves a more comprehensive and nu-\nanced representation, enhancing its overall performance.\n\u2022 3D MLLMs Integration: Building upon the successful\nalignment of point patches with textual features, the next\ncrucial step involves the integration of point features into\nLarge Language Models (LLMs). This integration mir-\nrors approaches seen in Vision Language Models (VLMs)\nand extends their capabilities to comprehend and interpret\npoint cloud data. The seamless fusion of 3D spatial infor-\nmation with the linguistic context empowers Large Lan-\nguage Models (LLMs) with a more holistic understanding\nof the data, enabling them to discern intricate patterns and\nrelationships within the point clouds.\nC. Additional Method\nHere, we provide additional information to our method.\nWe first give more details about the data text annotation en-\ngine Pyramid-XL in Sec. C.1. And then, in Sec. C.2 about\nthe model architecture, we give the details about the point\ndiffusion branch.\nC.1. Pyramid-XL: Data Annotation Engine\nFirst, we introduce the approach to acquire point cloud\nfrom Objaverse-XL [11]. Then we introduce the cost and\nprompts of the our data annotation engine Pyramid-XL.\nFinally, we give more qualitative results that finetune the\nPoint-E [40] by our Pyramid-XL level 3 dense captions.\nSubdir\nSubsubdir\nOther file\nTargrt mesh\nMaterial\nRGB \nAlpha\nDepth\n...\n...\n...\n...\n20 Views, in a sphere\n(Save)\n(Delete)\nFigure S3. Acquire Data Pipeline from Objaverse-XL [11].\nAcquire data from Objaverse-XL. Here we detail our pro-\ncessing approach for the Objaverse-XL dataset [11].\nIt\nhas 10M objects and is the extension of Objaverse-1.0 [12]\nwhich only has 800K 3D objects. Objaverse-XL offers only\nunprocessed downloads for its 3D objects, most of which\noriginate from sources like GitHub.\nDownloading these\nmesh files necessitates obtaining the complete project, as\nmaterials and related components are often stored in other\nseparate directories. Downloading the raw dataset in this\nformat is impractical due to excessive memory require-\nments, with an average project consuming about 1GB of\nspace. Therefore, we render object images and clear the\ncache upon completion to manage space. We render 20 ran-\ndom views of each object, capturing the RGB, alpha values,\nand depth, which are then used to generate point clouds. In\naddition to the 780K objects from Objaverse-1.0, we ren-\ndered an additional 220K from Objaverse-XL, totaling 1M\nobjects.\nThe cost of the Pyramid-XL. We now turn our attention\nto the cost analysis of our data annotation engine, detailed\nin Tab. S1.\nThe primary costs, detailed under the \u20191K\nCost\u2019 column, include GPU resources on the left and GPT\nAPI usage on the right. We use the same GPU settings as\nCap3D [38], employing A40s on a identical cloud platform.\nGiven GPUs\u2019 parallel processing, costs are equal for single\nor multiple units. We calculate usage time assuming a sin-\ngle GPU for simplicity. For Level 1, we use BLIP-2 [31]\nDataset\nNum Obj\nData Type\nCost/K (GPU + GPT)\nLevel 1\n1M\nSingle-View Caption\n$0.47 + $0\nLevel 2 (GPT-4)\n660K\nMulti-View Caption\n$4.17 + $4.18*\nLevel 2 (ChatGPT)\n660K\nMulti-View Caption\n$4.17 + $0.14*\nLevel 3\n70K\nQA, Detailed Caption\n$1.64 + $0\nTable S1. Comparing Costs across Different Dataset Levels.\nCosts are calculated based on generating annotation for 1K ob-\njects. * is directly from Cap3D [38]. As levels increase, the cost\nrises, indicating larger scales for lower-level datasets.\nto generate one short caption for one object. It needs 0.074\nhours and costs 0.074h \u00d7 $1.28/h = $0.095. For Level 2\nthe cost is the same as the Cap3D [38]. The GPU resource\nfees include BLIP-2 [31] and CLIP [48]. BLIP-2 generates\n8 views for each object and each view has 5 captions, so the\nfee is $0.095 \u00d7 8 \u00d7 5 = $3.76. And the CLIP uses 0.3h\nand costs 0.3h \u00d7 $1.28/h = $0.38. All GPU resource fee\nis $3.76 + $0.38 = $4.17. For the GPT API fee, it costs\n$0.03/1k tokens and needs 139.3 tokens for each object and\nthe total cost is $139.3/1000k \u00d7$0.03/1k \u00d71000 = $4.18.\nFor Level 3, We use the open-source Visual Language\nModel (VLM) Qwen-VL [3] for processing the final cap-\ntions. It needs 1.28h for the CLIP filter and Qwen-VL gen-\neration captions, so the cost is 1.28h \u00d7 $1.28/h = $1.64.\nWe can observe that Level 2 captions account for most\nof the costs, primarily due to GPT usage fees. Our findings\nshow that using GPT-4 for text-based multi-view caption\nsynthesis doesn\u2019t substantially outperform ChatGPT. Fur-\nthermore, by utilizing open-source Large Language Models\n(LLMs), we can entirely eliminate API call expenses. The\nother major cost is the GPU resources, as it uses BLIP-2\nto generate five captions for each view, which can lead to\nredundancy in information. We can reduce the number of\ncaptions for each view, and even the number of views.\nThe prompts of the Pyramid-XL. We present the prompt\npart of the Pyramid-XL data text annotation engine, as il-\nlustrated in Fig. S6 and Fig. S7. We primarily focus on\nillustrating how to construct GPT-based Level 2 captions,\nChatCaptioner-based Level 3 short QA pairs, and MLLM-\nbased Level 3 instruction captions and long QA pairs.\nFor Level 2 captions, we use Level 1 captions of rendered\nimages from 6 views. Through carefully designed prompts,\nwe integrate captions from the 6 captions to obtain a com-\nprehensive and relatively accurate caption with fewer than\n30 words. In our paper, we use GPT-4 to get the compre-\nhensive caption but we find that ChatGPT can be replaced\nby GPT-4 to generate Level 2 captions to reduce the cost.\nFor Level 3 short QA, we follow the approach outlined in\nChatCaptioner [79]. We use ChatGPT or other LLMs (we\nchoose Vicuna-7B [9]) as the questioner and BLIP-2 [31]\nas the answerer. By providing appropriate instructions and\ncontext (Level 2 caption) to both the LLM and BLIP-2, we\nobserve that, LLM generate diverse questions that that in-\nclude aspects such as color, type, material, purpose, and\nmore. Also, without restricting the number of words, BLIP-\n2 tends to output concise answers. These form the basis for\nour Objaverse-XL short QA dataset.\nFor Level 3 dense captions, we use the Level 2 caption\nas context, feed the rendering image that best matches the\ncontext into MLLM, and input suitable instructions. Due to\na combination of high-quality conversational performance\nand cost-effectiveness, we choose the Qwen-VL [3] model\nto generate. The construction method for Level 3 instruc-\ntion (long) QA pairs is similar to the above steps, with the\nkey difference lying in the variation of instructions.\nThe effectiveness of Pyramid-XL Level 3 caption. We\nuse dense captions from Level 3 of Pyramid-XL to fine-tune\nPoint-E and compare the results with those of Cap3D, as\nshown in Fig. S11. Ours significantly outperform Cap3D\u2019s\ncaptions, demonstrating the precision of our captions.\nC.2. Point Diffusion Architecture\nPoint\nEncoder\nPoint\nQ-Former \nCLIP\nEncoder\nPoint-E\n A bowl of objects \nsits on top of a table.\nCLS Token\nCLS Token\nFigure S4. Point Diffusion with GPT4Point.\nCurrently, there are indeed some explorations into con-\ntrollable text-to-3D work [27, 60, 61]. However, we are\nattempting to combine understanding and controllable 3D\ngeneration together.\nHere, we offer an in-depth look\nat the Diffusion branch\u2019s structure in Stage 2, illustrated\nin Fig. S4.\nInitially, the point cloud undergoes process-\ning via the Point Encoder (Backbone) and Point Q-Former,\nyielding Q-Former Tokens. For text, instead of the Point\nQ-Former\u2019s text tokenizer, we utilize Point-E\u2019s CLIP to-\nkenizer.\nThe resulting text tokens are then concatenated\nwith the Q-Former Tokens.\nSubsequently, the CLS to-\nken from the Text Token is fed into Point-E. The concate-\nnation method in GPT4Point differs notably from BLIP-\nDiffusion [30]. In BLIP-Diffusion, Q-Former Tokens are\ninserted between the CLS token and input tokens. In con-\ntrast, GPT4Point appends Q-Former Tokens directly to the\ntext token sequence, allowing the CLS token to integrate\nboth geometric and color information, crucial for guiding\nthe 3D generation.\nD. Additional Benchmark\nIn this section, we mainly introduce some additional con-\ntents about the benchmark. In Sec. D.1, we give more exam-\nples of the ObjaverseXL QA dataset. Note that the short QA\ndataset is used for evaluation based on the accuracy met-\nric. Then in Sec. D.2, we show more qualitative results\nabout Generation Failure Cases which can not be recog-\nnized by 2D VLMs through a single view but are judged\nby our GPT4Point.\nD.1. Objaverse-XL QA Dataset\nShort QA Dataset We use the short QA dataset for the\nevaluation of the 3D point cloud question answering task.\nWe selecte categories that overlap with both Objaverse-XL\nand LVIS [22], constructing 1K Point-QA data as the test\nset. The specific samples are presented in Fig. S8, which\nincludes questions covering various aspects such as color,\nmaterial, composition, category, etc. The answers are con-\ncise, with an average word length of 2.32, making them con-\nvenient for testing. We use accuracy top-1 as metric and\nevaluate the model\u2019s zero-shot short QA capability on this\ndataset.\nLong (Instruction) QA Dataset The long (Instruction) QA\ndataset is for finetuning the model to significantly enhance\nthe model\u2019s conversational capabilities. We impose length\nconstraints on prompts, requiring approximately 50 words\nfor answers to dense caption questions and not less than 10\nwords for other questions. As illustrated in Fig. S8, we con-\nstructed a Long (Instruction) QA dataset for 70K objects,\ncomprising 344,996 QA pairs. Among these, 69K data are\nused for fine-tuning, while the remaining 1K are reserved\nfor testing. This aims to encourage LLMs to generate long\nand more comprehensive results.\nD.2. Anomalous Objects: Generation Failure Cases\nIn this section, we will demonstrate more qualitative re-\nsults to show the failure case which can not be recognized\nby 2D VLMs through a single view but can be judged by\nour GPT4Point. In this section, we mainly show the failure\ncases produced by the state of the arts text-to-3D genera-\ntion methods like Dream-Gaussian [56] and Fantasia3d [8].\nDue to technical constraints, these models are likely to gen-\nerate 3D objects with multi-heads or multi-bodies. If pro-\nvided with render images from only a single perspective, 2D\nVLMs [3, 9], and even humans in most cases, may make in-\ncorrect judgments, as illustrated in the upper part of Fig. S9.\nThis hinders the assessment of 3D object generation. How-\never, our GPT4Point provides a better solution to this issue.\nMore examples are showcased in Fig. S9.\nControlable     \n Text-to-3D\nA 3D icecream cone with whipped cream\nDirect\nText-to-3D\n3D model of a plant in_a pot\n3D model of a wooden hammer for sale\nDirect\nImage-to-3D\nLow-Quality \nCondition\nControlable     \n Text-to-3D\n3D model of a lcd monitor on a stand\nDirect\nText-to-3D\n3D model of a tile\n3D model of a house with a roof\nDirect\nImage-to-3D\nLow-Quality \nCondition\nFigure S5. Point Diffusion Results: our controllable text-to-3D. Given a low-quality point cloud prior, it can generate outcomes superior\nto direct text-to-3D and image-to-3D methods and more closely align with the low-quality priors, demonstrating controllability.\nE. Additional Experiments\nIn this section, we supplement the details of the exper-\niments. First in Sec. E.1, we list all the hyperparameters\nthrough the table. Then We give more qualitative results\nabout our experiments. Sec. E.2 shows the text reference\ntasks like 3D object point caption and QA and Sec. E.3\nshows our point diffusion results.\nE.1. Training Details\nHyperparameters\nValue/Type\nbatchsize\n32\ntraining epochs\n10\noptimizer\nAdamW\ninit lr\n1e-4\nmin lr\n1e-5\nwarmup lr\n1e-6\nweight decay\n0.05\nlr schedule\ncosine annealing\nwarmup type\nlinear\nwarmup iters\n5000\nPoint size\n8192\nQ-Former queries\n32\nTable S2. Training settings and hyperparameters for Stage1.\nWe detail the hyperparameters of GPT4Point, largely\nmirroring those used in BLIP-2 [31] during the pretrain\nstage. These parameters are maintained for Stage1: Point-\ntext alignment and the LLM branch in Stage2. Tab. S2 lists\nthem. The parameters for the LLM branch in Stage2 are\nalmost identical to those of Stage1, except for the warm-up\niterations, which changed from 5K to 2K. For BLIP-2, after\npretraining on multiple datasets, fine-tuning is performed on\na smaller dataset and subtasks. Additionally, different im-\nage backbones were used in the pretraining and fine-tuning\nphases. But in our GPT4Point, we only use the pretrain\nstage in the BLIP-2 and all tasks are evaluated by zero-shot.\nFor the diffusion branch, we need to make the learning rate\nvery small because here we only train the fully connected\nlayers. The init, min and the warmup learing rate is 1e-7, 0\nand 1e-8, and we only train 1 epoch.\nE.2. Point-text Captions and QA Demos\nIn this section, we show more point-text qualitative re-\nsults of GPT4Point. More specific examples are presented\nin Fig. S10. We can see that GPT4Point is capable of effec-\ntively understanding point clouds and can engage in fluent\nconversations with humans.\nE.3. Point Diffusion Results\nFig. S5 shows more qualitative results of point diffusion\nresults of GPT4Point. We find that GPT4Point can guide\ntext-to-3D processes, generating results with more accurate\ncolors and geometric shapes.\nContent\nFigure\nSec. C.1: Prompts of Level 2 caption\nFig. S6\nSec. C.1: Prompts of Level 3 long QA\nFig. S7\nSec. C.1: Level 3 caption finetune Point-E\nFig. S11\nSec. D.1: ObjaverseXL-LVIS QA 1K\nFig. S8\nSec. D.2: Generation Failure Cases\nFig. S9\nSec. E.2: Point-text Captions and QA\nFig. S10\nSec. E.3: Point Diffusion Results\nFig. S5\nTable S3. Chapter-Experiment Result Image Correspondence.\nChatGPT-based Level 2 Caption\nInstruction\nYou'll be showed 6 different angles descriptions(such as front, side and back view) of an object/scene, please \ntell me what is it(usally a singular thing/a scene, you must judge) and its feature. \nNotice: There are errors in the description of some angles(due to occlusion or improper angle), and you need to \ncombine all the descriptions of the same object/scene and identify possible error descriptions(please fix this \nerror directly, don't tell me) of this singular thing.\nDescriptions: {context}\nTell me what is it and its feature(if mentioned) in less than 30 words, please don't be redundant. \nUnwanted words: 'background', 'render', 'grey', 'image', 'foggy', '3D', '2D', 'surface'\nReply: It is \nContext\nView1: a 3d character standing on a wooden floor in a gray hat     View2: this is an image of the boy wearing a hat  \nView3: a rendered 3d model of a man with glasses        View4: a 3d rendering of a little boy with glasses and a hat\nView5: cartoon child with blonde hair in a hat and sweater    View6: a 3d looking character with a hat and jacket on\nOutput\nA character wearing a hat and glasses, possibly a little boy, with blonde hair and a jacket.\nChatCaptioner-based Level 3 Short QA\nI have an image of a 3D thing. Ask me \nquestions about the content of this image.\nCarefully asking me informative and \nspecific questions to maximize your \ninformation about this image content. Each \ntime ask one question only without giving \nan answer.\nIf it is an object, ask me about its \ngeometry shape, composed of what, \nquantity, usage and indicate what each \nobject is (if multiple) and especially some \nspecific details. If it is a person/catoon \ncharacter, ask me about its wearing, its \nposture, what it is doing and its possible \njob. If it is a house, ask me questions \nabout its components, its surroundings and \nits type or shape. You are free to play with \nsome questions depending on the context, \nbut don't be unrealistic.\nAvoid asking colors. Avoid asking materials. \nAvoid mention the background. Don't ask \nrepeated questions! \nInstruction\nChatGPT / Vicuna\nQuestioner\nBLIP2\nAnswerer\nContext\nTell me details about the image.\nA 3D cartoon character wearing \nglasses, a hat, and a yellow hoodie.\nWhat kind of thing it is?(object, person, \ncartoon character or house)\nCartoon character\nIs the hoodie a zipper or a pullover?\nPullover\nIs the hat a cap or a fedora?\nCap\nDoes the cartoon character have \nany visible tattoos or markings?\nNo\nDoes the cap have any visible \nlogos or patterns on it?\nNo\nWhat color is the hoodie?\nYellow\nFigure S6. Prompts of Level 2 Caption and Level 3 Short Q&A in Pyramid-XL. We present the construction process of GPT-based\nLevel 2 Caption and ChatCaptioner-based Level 3 Short Q&A, along with the prompts utilized, consisting of context and instruction.\nMLLM-based Level 3 Instruction Caption\nInstruction\nAnalyze the 3D object model from the image and the given caption: \"{context}\" \nWrite a new `detailed` caption by describing it in more words(no less than 50 words), identifying its type, \nappearance, and any other inferred information such as functionalities, usages, daily-life knowledge, etc. \nExclude any uncertain details. Use rich language descriptions.\nContext\nBlue and white acoustic guitar 3D model, royalty-free.\nOutput\nMLLM-based Level 3 Instruction QA pairs\nAnalyze the 3D object model from the image and the given caption: \"{context}\"\nGenerate a set of 1-round Q&As(both question and answer pairs) regarding diverse aspects of the object \nbased on the image and provided captions. Question begin with 'Q:'. Answer begin with 'A:'. Don't mention size. \nContinue to generate 1 single-round Q&As(both question and answer pairs) regarding diverse aspects of the \nobject based on the image. Don't mention size. Different aspects from the previous questions. \nContinue to generate 1 single-round Q&As(both question and answer pairs) regarding diverse aspects of the \nobject based on the image. Don't mention size. Different aspects from the previous questions. The answer should \nbe more than 10 words, but should not be too redundant with the question.\n\u2026\u2026\nInstruction\nThe 3D object model is a blue and white acoustic guitar. It has a classic design with a wooden body \nand a rosewood fretboard. The guitar is painted in a distressed style, giving it a vintage look. This guitar is \na high-quality, royalty-free model that can be used for various purposes, such as in music videos, \nadvertising, or as a decorative item.\nLevel2 Caption\nCLIP\nmax\nQwen\nGPT-4\nor\n50 words\nAnswer\nQuestion\nInstruction QA Pairs \nLevel3 Instruction \nCaption and QA pairs\nInstruction Caption\nContext\nBlue and white acoustic guitar 3D model, royalty-free.\nOutput\nWhat is the purpose of the acoustic guitar?\nThe acoustic guitar is a musical instrument that \nis used to create music by plucking the strings. \nIt is commonly used in a variety of musical \ngenres, including folk, rock, and classical music.\nInstruction Data Generation\nWhat is the color of the guitar?\nWhat is the color of the guitar?\nWhat is the material of the guitar?\nThe guitar is made of wood.\nFigure S7. Prompts of MLLM-based Level 3 Instuction Caption and QA pairs in Pyramid-XL. The top part details the process of\nconstructing the dataset, while below are the specific instructions provided to the MLLM (Qwen-VL[xx]) and the model output.\nWhat is the purpose of \nthe flower pot?\nYellow and white.\nWhat is the purpose of the \ngrand piano?\nWhat does the grand piano \nconsist of?\nWhat is the composition of the \ngrand piano?\n3D model of a blue and yellow \nNike Air Swoosh high-top sneaker \nwith a yellow sole.\nWhat does the object look like?\nTell me details about the image.\nA shoe.\nIs the shoe a sneaker?\nYes.\nWhat is the color of the shoe?\nBlue and yellow.\nIs the sole of the shoe yellow?\nYes.\nTell me details about the image.\nA 3D model of a black \ngrand piano.\nIt is made of wood.\nA keyboard and a case.\nTo play music.\nTell me details about the image.\n3D model of a skateboard \nwith red wheels.\nWhat color are the wheels?\nHow many wheels does \nthe skateboard have?\nWhat is the color of the \nskateboard?\nBlack.\n4.\nRed.\n3D model of a blue and yellow \nNike Air Swoosh high-top sneaker \nwith a yellow sole.\nWhat is the color of the flower pot?\nTell me details about the image.\nWhat kind of flower is in the pot?\nNo flower.\nIs it made of ceramic or plastic?\nCeramic.\nTo hold flowers.\nThe cupcake is placed \non a blue plate.\nWhat is the main ingredient of the \ncupcake?\nWhat is the design of the \nracing car\nThe racing car has a white and \nred color scheme, with black \naccents on the wheels and other \nparts of the car.\nWhat is the purpose of the \nsaxophone?\nWhat is the dish the \ncupcake is placed on?\nThe main ingredient of the \ncupcake is flour.\nWhat is the purpose of the \nfiligree design on the ring?\nWhat is the color of the ring?\nWhat is the shape of the ring?\nYellow saxophone.\nWhat is the color of the saxophone?\nThe saxophone is yellow.\nWhat is the shape of the saxophone?\nThe saxophone is curved and \ncylindrical in shape.\nThe saxophone is a musical \ninstrument used in jazz and \nclassical music, known for its \ndistinctive sound and versatility.\nA yellow gold ring with an \nintricate filigree design.\nThe ring has a round shape, \nwith a filigree design on the top.\nThe ring is yellow gold, which \ngives it a warm and luxurious \nappearance.\nThe filigree design on the ring serves \nas a decorative element, adding an \nintricate and elegant touch to the \noverall appearance of the ring.\n3D model of a racing car.\nWhat is the purpose of the \nracing car?\nWhat is the color scheme \nof the racing car?\nThe purpose of the racing car \nis to compete in a rally race.\nThe racing car has a sleek and \naerodynamic design, with a low \nprofile and wide wheels.\n3D model of a cupcake with a \ncherry on top, placed on a plate.\nWhat is on top of the \ncupcake?\nThere is a cherry on top of \nthe cupcake.\nFigure S8. Objaverse-XL QA dataset. The upper section of the dataset features short Q&A samples, and the lower part includes long\nQ&A samples, covering color, shape, type, material, and purpose. The short Q&A dataset evaluates performance, while the long Q&A is\nfor fine-tuning, promoting more detailed language generation and promote the understanding and conversational capabilities.\nDoes it have one body or multiple bodies?\nThe 3D object model is made up of multiple \nbodies, each with a different shape and size.\nHow many tails does it have?\nIt has two tails, one on each side.\nHow many heads does the bird have?\nThere are two heads on the bird.\nIs the frog one head?\nNo, the frog has two heads.\nGPT4Point\nDoes it have one body or multiple bodies?\nThe image depicts a single body performing a \ntrick on a bicycle.\nHow many tails does it have?\nThe image shows a rooster with two tails.\nHow many heads does the bird have?\nThe bird in the image has a head.\nIs the frog one head?\nYes, the frog has one head.\n2D MLLM\nFigure S9. Anomalous Objects: Generation Failure Cases. The upper and lower parts respectively depict the performance of 2D MLLM\nand GPT4Point in identifying abnormally generated objects with multi-body and multi-head structures. GPT4Point is effective in making\naccurate judgments, whereas 2D MLLM, due to the lack of information from single-view images, fails to identify most cases.\nFigure S10. Point-text Captions and QA Demos. We use the fine-tuned GPT4Point with OPT6.7B model to generate results on the test\nset, demonstrating that our model performs well on dense captioning tasks and long (instruction) question answering. The results shows\nour model\u2019s capability to comprehend information such as object color and geometry.\nCap3D\nPyramid-XL\nCap3D\nPyramid-XL\nCap3D\nPyramid-XL\nCap3D\nPyramid-XL\nCap3D\nPyramid-XL\nCap3D\nPyramid-XL\nA 3D model of a palm tree.\n3D model of the earth.\nA 3D model of a red and black sword.\nA 3D model of a white coffee cup.\nA 3D model of a white ring.\nA 3D model of a roll of toilet paper.\nA 3D model of a brown bottle.\n3D model of a white umbrella stand.\nA 3D model of a floor lamp.\n3D rendering of a white toilet with a white lid.\n3D model of a small green pine tree, a Christmas tree.\n3D model of a white chair.\nFigure S11. Pyramid-XL Level 3 Point-E Finetune Results. We found that the results of fine-tuning with dense captions from our\nPyramid-XL significantly outperform those fine-tuned with Cap3D captions, demonstrating the greater accuracy of our generated captions.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In NeurIPS,\n2022.\n[2] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf\nHanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,\nSamir Gadre, Shiori Sagawa, et al. Openflamingo: An open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv:2308.01390, 2023.\n[3] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A versatile vision-language model for un-\nderstanding, localization, text reading, and beyond. arXiv\npreprint arXiv:2308.12966, 2023. 2, 7, 10, 11\n[4] Satanjeev Banerjee and Alon Lavie. METEOR: An auto-\nmatic metric for MT evaluation with improved correlation\nwith human judgments. In ACL Workshop, 2005. 6\n[5] Ching Chang, Wen-Chih Peng, and Tien-Fu Chen. Llm4ts:\nTwo-stage fine-tuning for time-series forecasting with pre-\ntrained llms. arXiv preprint arXiv:2308.08469, 2023. 2\n[6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut.\nConceptual 12M: Pushing web-scale image-text\npre-training to recognize long-tail visual concepts. In CVPR,\n2021. 2\n[7] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei\nHuang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong\nLu, et al. Videollm: Modeling video sequence with large\nlanguage models. arXiv preprint arXiv:2305.13292, 2023.\n2, 9\n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3, 11\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yong-\nhao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.\nXing.\nVicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality, 2023. 2, 9, 10, 11\n[10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip:\nTowards general-\npurpose vision-language models with instruction tuning.\narXiv:2305.06500, 2023. 2, 7\n[11] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 2, 4, 10\n[12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana\nEhsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse:\nA universe of annotated 3d objects. In CVPR, 2023. 2, 4, 5,\n10\n[13] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 3\n[14] Danny\nDriess,\nFei\nXia,\nMehdi\nSM\nSajjadi,\nCorey\nLynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan\nWahid,\nJonathan Tompson,\nQuan Vuong,\nTianhe Yu,\net al. Palm-e: An embodied multimodal language model.\narXiv:2303.03378, 2023. 2\n[15] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shi-\njie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui\nHe, Xiangyu Yue, Hongsheng Li, and Yu Qiao.\nLlama-\nadapter v2:\nParameter-efficient visual instruction model.\narXiv:2304.15010, 2023. 2\n[16] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan.\nPlanting a seed of vision in large language model,\n2023. 2\n[17] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,\nand Soujanya Poria.\nText-to-audio generation using\ninstruction-tuned llm and latent diffusion model.\narXiv\npreprint arXiv:2304.13731, 2023. 2\n[18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. Imagebind: One embedding space to bind them all.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15180\u201315190, 2023.\n2\n[19] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,\nMiao Zheng, Qian Zhao, Kuikun Liu, Wenwei Zhang, Ping\nLuo, and Kai Chen. Multimodal-gpt: A vision and language\nmodel for dialogue with humans. arXiv:2305.04790, 2023.\n[20] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xi-\nanzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xi-\nanzhi Li, Hongsheng Li, et al.\nPoint-bind & point-llm:\nAligning point cloud with multi-modality for 3d understand-\ning, generation, and instruction following.\narXiv preprint\narXiv:2309.00615, 2023. 2\n[21] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xi-\nanzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xi-\nanzhi Li, Hongsheng Li, et al.\nPoint-bind & point-llm:\nAligning point cloud with multi-modality for 3d understand-\ning, generation, and instruction following.\narXiv preprint\narXiv:2309.00615, 2023. 2, 3\n[22] Agrim Gupta, Piotr Dollar, and Ross Girshick.\nLvis: A\ndataset for large vocabulary instance segmentation. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 5356\u20135364, 2019. 5, 11\n[23] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng,\nYilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Inject-\ning the 3d world into large language models. arXiv, 2023. 2,\n9\n[24] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi,\nXuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Ji-\nawei Huang, Jinglin Liu, et al. Audiogpt: Understanding and\ngenerating speech, music, sound, and talking head. arXiv\npreprint arXiv:2304.12995, 2023. 2, 9\n[25] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao,\nSaksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui,\nOwais Khan Mohammed, Qiang Liu, et al.\nLanguage is\nnot all you need: Aligning perception with language mod-\nels. arXiv:2302.14045, 2023. 2\n[26] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui\nHuang, Rynson WH Lau, Wanli Ouyang, and Wangmeng\nZuo. Clip2point: Transfer clip to point cloud classification\nwith image-depth pre-training. In ICCV, 2023. 3\n[27] Yukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-\nJun Zha, and Lei Zhang. Dreamtime: An improved optimiza-\ntion strategy for text-to-3d content creation, 2023. 11\n[28] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 3\n[29] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu.\nOtter:\nA multi-modal\nmodel with in-context instruction tuning.\narXiv preprint\narXiv:2305.03726, 2023. 2\n[30] Dongxu Li, Junnan Li, and Steven CH Hoi.\nBlip-\ndiffusion:\nPre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720, 2023. 5, 11\n[31] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBLIP-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models, 2023. 2,\n4, 7, 9, 10, 12\n[32] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation. 2022.\n[33] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai\nWang, Ping Luo, Yali Wang, Limin Wang, and Yu Qiao.\nVideochat: Chat-centric video understanding. arXiv preprint\narXiv:2305.06355, 2023. 2\n[34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 300\u2013309, June 2023. 3\n[35] Chin-Yew Lin. ROUGE: A package for automatic evaluation\nof summaries. In Text Summarization Branches Out, 2004. 6\n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv:2304.08485, 2023. 2, 9\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 7\n[38] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson. Scalable 3d captioning with pretrained models. arXiv\npreprint arXiv:2306.07279, 2023. 4, 8, 10\n[39] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 3\n[40] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generating\n3d point clouds from complex prompts. arXiv:2212.08751,\n2022. 3, 8, 10\n[41] OpenAI.\nChatgpt.\nhttps://openai.com/blog/\nchatgpt, 2022. 2, 9\n[42] OpenAI. GPT-4 technical report, 2023. 4, 9\n[43] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nIn NeurIPS, 2022. 2\n[44] Yatian Pang, Wenxiao Wang, Francis EH Tay, Wei Liu,\nYonghong Tian, and Li Yuan. Masked autoencoders for point\ncloud self-supervised learning. In European conference on\ncomputer vision, pages 604\u2013621. Springer, 2022. 3\n[45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In ACL, 2002. 6\n[46] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shao-\nhan Huang, Shuming Ma, and Furu Wei.\nKosmos-2:\nGrounding multimodal large language models to the world.\narXiv:2306.14824, 2023. 2\n[47] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 2, 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 9, 10\n[49] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. In JMLR, 2020. 2, 6\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 3\n[51] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs.\narXiv preprint arXiv:2111.02114, 2021. 2\n[52] Zhan Shi, Xu Zhou, Xipeng Qiu, and Xiaodan Zhu. Improv-\ning image captioning with better use of captions, 2020. 2\n[53] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and\nDeng Cai. Pandagpt: One model to instruction-follow them\nall. arXiv:2305.16355, 2023.\n[54] Qinghong Sun, Yangguang Li, ZeXiang Liu, Xiaoshui\nHuang, Fenggang Liu, Xihui Liu, Wanli Ouyang, and Jing\nShao.\nUnig3d: A unified 3d object generation dataset.\narXiv:2306.10730, 2023.\n[55] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in multi-\nmodality, 2023. 2\n[56] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang\nZeng. Dreamgaussian: Generative gaussian splatting for ef-\nficient 3d content creation, 2023. 11\n[57] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities.\nhttps://\ngithub.com/InternLM/InternLM, 2023. 2\n[58] et al Tong Wu, Jiarui Zhang.\nOmniobject3d:\nLarge-\nvocabulary 3d object dataset for realistic perception, recon-\nstruction and generation. IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 2\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama:\nOpen and efficient foundation language models.\narXiv:2302.13971, 2023. 2, 9\n[60] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts.\narXiv\npreprint arXiv:2304.12439, 2023. 11\n[61] Haowei Wang, Jiji Tang, Jiayi Ji, Xiaoshuai Sun, Rongsheng\nZhang, Yiwei Ma, Minda Zhao, Lincheng Li, Zeng Zhao,\nTangjie Lv, et al. Beyond first impressions: Integrating joint\nmulti-modal cues for comprehensive 3d representation. In\nProceedings of the 31st ACM International Conference on\nMultimedia, pages 3403\u20133414, 2023. 11\n[62] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu,\nXizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu\nQiao, et al. Visionllm: Large language model is also an open-\nended decoder for vision-centric tasks. arXiv:2305.11175,\n2023.\n[63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 3\n[64] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and\nQuoc V Le. Finetuned language models are zero-shot learn-\ners. In ICLR, 2021. 2, 6, 9\n[65] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3d\nshapenets: A deep representation for volumetric shapes. In\nCVPR, 2015. 5, 7\n[66] Runsen Xu, Xiaolong Wang, Tai Wang, Yilun Chen, Jiang-\nmiao Pang, and Dahua Lin. Pointllm: Empowering large\nlanguage models to understand point clouds. arXiv preprint\narXiv:2308.16911, 2023. 2, 5, 7\n[67] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo,\nKenneth KY Wong, Zhenguo Li, and Hengshuang Zhao.\nDrivegpt4: Interpretable end-to-end autonomous driving via\nlarge language model.\narXiv preprint arXiv:2310.01412,\n2023.\n[68] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nJiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles,\nand Silvio Savarese. Ulip: Learning a unified representation\nof language, images, and point clouds for 3d understanding.\nIn CVPR, 2023. 3\n[69] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Car-\nlos Niebles, and Silvio Savarese.\nUlip-2:\nTowards\nscalable multimodal pre-training for 3d understanding.\narXiv:2305.08275, 2023. 3, 6\n[70] Keunwoo Peter Yu. VideoBLIP. 2\n[71] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller,\nOlga Golovneva, Tianlu Wang, Arun Babu, Binh Tang, Brian\nKarrer, Shelly Sheynin, Candace Ross, Adam Polyak, Rus-\nsell Howes, Vasu Sharma, Puxin Xu, Hovhannes Tamoyan,\nOron Ashual, Uriel Singer, Shang-Wen Li, Susan Zhang,\nRichard James, Gargi Ghosh, Yaniv Taigman, Maryam\nFazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and Ar-\nmen Aghajanyan. Scaling autoregressive multi-modal mod-\nels: Pretraining and instruction tuning, 2023. 2\n[72] Xumin Yu, Lulu Tang, Yongming Rao, Tiejun Huang, Jie\nZhou, and Jiwen Lu. Point-bert: Pre-training 3d point cloud\ntransformers with masked point modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 19313\u201319322, 2022. 3, 6, 9\n[73] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n3\n[74] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin\nZhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2ae:\nmulti-scale masked autoencoders for hierarchical point cloud\npre-training. Advances in neural information processing sys-\ntems, 35:27061\u201327074, 2022.\n[75] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-\npeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng\nLi. Pointclip: Point cloud understanding by clip. In CVPR,\n2022. 3\n[76] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun\nZhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and\nYu Qiao. Llama-adapter: Efficient fine-tuning of language\nmodels with zero-init attention. arXiv:2303.16199, 2023. 2\n[77] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,\nMoya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,\nXian Li, Xi Victoria Lin, et al. Opt: Open pre-trained trans-\nformer language models. arXiv preprint arXiv:2205.01068,\n2022. 6, 9\n[78] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng\nLi, Yu Qiao, Wanli Ouyang, and Xiangyu Yue.\nMeta-\ntransformer: A unified framework for multimodal learning.\narXiv preprint arXiv:2307.10802, 2023. 2, 3\n[79] Deyao Zhu, Jun Chen, Kilichbek Haydarov, Xiaoqian Shen,\nWenxuan Zhang, and Mohamed Elhoseiny. Chatgpt asks,\nblip-2 answers: Automatic questioning towards enriched vi-\nsual descriptions. arXiv preprint arXiv:2303.06594, 2023.\n10\n[80] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language mod-\nels. arXiv:2304.10592, 2023. 2\n[81] Xiangyang Zhu, Renrui Zhang, Bowei He, Ziyu Guo, Ziyao\nZeng, Zipeng Qin, Shanghang Zhang, and Peng Gao. Point-\nclip v2: Prompting clip and gpt for powerful 3d open-world\nlearning. In ICCV, 2023.\n[82] Zhu Ziyu, Ma Xiaojian, Chen Yixin, Deng Zhidong, Huang\nSiyuan, and Li Qing. 3d-vista: Pre-trained transformer for\n3d vision and text alignment. In ICCV, 2023. 2\n"
  },
  {
    "title": "Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions",
    "link": "https://arxiv.org/pdf/2312.02772.pdf",
    "upvote": "5",
    "text": "Generating Fine-Grained Human Motions Using ChatGPT-Refined Descriptions\nXu Shi1, Chuanchen Luo2, Junran Peng3, Hongwen Zhang4, Yunlian Sun1\n1Nanjing University of Science and Technology\n2Institute of Automation, Chinese Academy of Sciences\n3cheery.ai\n4Beijing Normal University\nhttps://sx0207.github.io/fg-mdm\nLegs\n\u201cA person walks happily. \u201d\n\u201cA person walks depressingly. \u201d\nArms\nNeck\nHis arms sway \nfreely by his sides.\nHis arms hang heavily \nby his sides.\nHis legs move with energy, \ntaking long strides.\nHis legs move slowly, \ntaking short steps with \nlittle energy.\nHis neck is held high \nand comfortably.\nHis neck is lowered.\nFigure 1. FG-MDM can generate high-quality fine-grained and stylized motions by using fine-grained descriptions of different body parts.\nAbstract\nRecently, significant progress has been made in text-\nbased motion generation, enabling the generation of diverse\nand high-quality human motions that conform to textual de-\nscriptions.\nHowever, it remains challenging to generate\nfine-grained or stylized motions due to the lack of datasets\nannotated with detailed textual descriptions. By adopting a\ndivide-and-conquer strategy, we propose a new framework\nnamed Fine-Grained Human Motion Diffusion Model (FG-\nMDM) for human motion generation. Specifically, we first\nparse previous vague textual annotation into fine-grained\ndescription of different body parts by leveraging a large\nlanguage model (GPT-3.5). We then use these fine-grained\ndescriptions to guide a transformer-based diffusion model.\nFG-MDM can generate fine-grained and stylized motions\neven outside of the distribution of the training data. Our ex-\nperimental results demonstrate the superiority of FG-MDM\nover previous methods, especially the strong generalization\ncapability. We will release our fine-grained textual annota-\ntions for HumanML3D and KIT.\n1. Introduction\nHuman motion generation is an important research topic in\ncommunities of both computer vision and computer graph-\nics. It aims to simulate and generate realistic human move-\nments using computers. With the advancement of technolo-\ngies such as virtual reality, augmented reality, and movie\nspecial effects, there is a growing demand for high-quality\nhuman motion generation. In recent years, quite a few in-\nnovative methods and techniques have emerged to tackle\nthis challenging task [45]. Deep generative models, includ-\ning GANs [1, 20], VAEs [9, 24, 25], and Diffusion mod-\nels [5,16,35,42], have been widely applied to human motion\ngeneration.\nHowever, there is relatively less research on generat-\ning fine-grained motions, which usually mobilize the en-\ntire body and involve multiple actions, based on textual\ndescriptions.\nIn [3], Athanasiou et al.\nproposed a spa-\ntial composition framework named SINC for simultaneous\naction generation. It should be noted that SINC was de-\nsigned for motion generation based on action labels, not\nusing natural language. MotionDiffuse can achieve fine-\ngrained controlling when generating motions using textual\ndescriptions [42]. However, the controlling ability is largely\n1\narXiv:2312.02772v1  [cs.CV]  5 Dec 2023\nlimited by the scarcity of the training data. Likewise, there\nare very few research efforts devoted to stylized motion gen-\neration with natural language, which generates motions ac-\ncompanied with rich styles and emotions. In [34], Tevet et\nal. proposed MotionCLIP to study stylized motion genera-\ntion. However, only simple action classes with several style\nlabels were considered. Stylized motions defined by textual\ndescriptions were not investigated.\nA challenging issue of current human motion generation\nis the scarcity of datasets. This is especially true for datasets\nthat include fine-grained descriptions or stylized text anno-\ntations. Datasets, such as KIT [26], BABEL [27], and Hu-\nmanML3D [9], provide rich motion information. Their tex-\ntual descriptions instead are often simple and direct, lack-\ning stylized and fine-grained descriptions. Therefore, com-\npared to traditional human motion generation, generating\nfine-grained and stylized motions is more challenging due\nto the limited scale and diversity of existing motion capture\ndatasets. In Figure 3 and Figure 4, we show some fine-\ngrained and stylized motions generated by MDM [35] and\nMLD [5]. Although both approaches achieve impressive\nperformance in human motion generation, they show poor\ngeneralization ability since these fine-grained and stylized\nmotions are outside of the distribution of the training data.\nRecently, there have been attempts to leverage human mesh\nrecovery approaches [36, 39, 40] to collect large-scale text\npseudo-pose datasets [4].\nAs shown in Azadi et al. [4],\nthe pre-training on such datasets can significantly improve\nthe generalization to in-the-wild descriptions, but generat-\ning fine-grained and stylized motions remains challenging\ndue to the lack of detailed descriptions.\nThen, with only limited motion capture datasets avail-\nable, can we still achieve fine-grained and stylized motion\ngeneration? For a textual description defining a fine-grained\nor stylized motion, is there any way to associate it with\nmotions within the dataset? Note that the fine-grained and\nstylized motions might be outside of the distribution of the\ndataset. For a never-before-seen motion, the motion of the\nentire body is indeed unseen, but motions of specific body\nparts may be inside of the distribution of the dataset. There-\nfore, we can adopt a divide-and-conquer strategy. By re-\nannotating the motion for different body parts with fine-\ngrained descriptions, we can associate these body parts with\nspecific body parts within the dataset. For example, the styl-\nized description \u201cA person walks depressingly.\u201d can be re-\nformulated as \u201cHis arms hang heavily by his sides. His legs\nmove slowly, taking short steps with little energy...\u201d. Leg\nmovement in this stylized motion may appear in \u201cA person\nwalks aimlessly and slowly.\u201d. The arm movement may ap-\npear in \u201cA person stands in a relaxed manner.\u201d. In Figure ??,\nwe give two examples. On one hand, adopting fine-grained\ndescriptions allows the model to get how the fine-grained or\nstylized motion is performed in detail. On the other hand,\nre-annotating the motion for different body parts enables the\nmodel to handle motions beyond the scope of the dataset.\nAlthough manually annotating fine-grained textual de-\nscriptions for body parts provides more accurate data, it\nrequires a significant amount of manual work, resulting in\nhuge cost. Fortunately, with the rapid development of large\nlanguage models, OpenAI\u2019s GPT series models [23], known\nfor their outstanding natural language processing capabili-\nties, have gained widespread attention worldwide. In [7],\nGilardi et al. demonstrated that ChatGPT performs as well\nas human annotators in some tasks. In [15], Action-GPT\nexplores the excellent capability of ChatGPT in expanding\nhuman action descriptions. However, the generated content\ntends to be excessively redundant. For our task, we care-\nfully design a prompt that allows ChatGPT-3.5 to provide\ndetailed but non-redundant transcriptions of text descrip-\ntions about the human motion. We then use this prompt\nand ChatGPT-3.5 to transcribe 44,970 short text descrip-\ntions from HumanML3D [9] and 6,353 text descriptions\nfrom KIT [26] for fine-grained and stylized motion gener-\nation. We will make these fine-grained transcriptions pub-\nlicly available.\nWith these fine-grained descriptions, we propose a new\nframework named Fine-Grained Human Motion Diffusion\nModel (FG-MDM) for human motion generation. Specif-\nically, we replace the original simple and direct text with\nChatGPT-generated fine-grained descriptions of individual\nbody parts to guide a transformer-based diffusion model.\nFollowing MDM [35], we encode the entire fine-grained\ndescription with CLIP [29] as a global token of the trans-\nformer. Apart from this global token, we further encode\ndescriptions of different body parts individually with CLIP\nas part tokens. By adopting these two tokens, the model\ncan pay attention to both the global and detailed informa-\ntion of human motions, thereby improving the accuracy and\ncompleteness of the denoising results.\nOur contributions are summarized as follows:\n\u2022 We present a novel framework that utilizes fine-grained\ndescriptions of different body parts to guide the training\nof the human motion diffusion model. This framework\nenables the generation of fine-grained and stylized mo-\ntions even in the absence of datasets annotated with fine-\ngrained and stylized descriptions.\n\u2022 We carefully design a prompt that enables ChatGPT to\nconvert short texts into detailed but non-redundant de-\nscriptions of different body parts. We then use this prompt\nto transcribe 44,970 texts from HumanML3D and 6,353\ntexts from KIT into fine-grained descriptions. We will\nmake these fine-grained transcriptions publicly available.\n\u2022 We conduct a series of experiments to evaluate our\nmodel\u2019s ability in not only fitting the training data but\nalso generating fine-grained and stylized motions, i.e., the\ngeneralization capability.\n2\nTransformer Encoder\n. . .\n. . .\nLinear\nLinear\nText Condition:\nA person walks \nhappily.\nFine-grained descriptions:\nHis arms\u2026 His legs\u2026\nHis torso\u2026 His neck\u2026\nHis buttocks and waist \u2026\nHis legs move \nwith energy, \ntaking long \u2026\nHis arms swaying \nfreely by their side. \n. . .\nChatGPT\n. . .\nCLIP text\nCLIP text\nt\n\ud835\udc371\n\ud835\udc37k\nPT1\nPTk\nGL\n\ud835\udc37k\nPart Description\nGL\nGlobal Token\nPTk\nPart Token\nt\nDiffusion Time-Step\n\u0ddc\ud835\udc650\n1\n. . .\n\ud835\udc65\ud835\udc61\n1\n\ud835\udc65\ud835\udc61\n2\n\ud835\udc65\ud835\udc61\n\ud835\udc5b\nDiffuse\n\ud835\udc65\ud835\udc61\n1\n\ud835\udc65\ud835\udc61\n. . . \ud835\udc5b\nMotion Data\n0\u2192(t-1)\n\ud835\udc65\ud835\udc61\u22121\n1\n\ud835\udc65\ud835\udc61\u22121\n\ud835\udc5b\n. . .\nT iterative steps\nrandom mask\n\u0ddc\ud835\udc650\n2\n\u0ddc\ud835\udc650\nn\nFigure 2. The overall pipeline of FG-MDM. The model learns the denoising process of the diffusion model from the motion x1:n\nt\nat time\nstep t to the clean motion \u02c6x1:n\n0\n, given the text condition. The input text is first paraphrased by ChatGPT into fine-grained descriptions\nD1:k for different parts of the body, where k denotes the number of body parts. These descriptions are then fed into a pre-trained CLIP\ntext encoder and projected, along with the time step t, onto input tokens PT1:k of the transformer. The overall fine-grained text is further\nencoded into a global input token GL, providing holistic information. In the sampling process of the diffusion model, an initial random\nnoise x1:n\nT\nis sampled, and then T iterations are performed to generate the clean motion \u02c6x1:n\n0\n. At each sampling step t, guided by PT1:k\nand GL, the transfomer encoder predicts the clean motion \u02c6x1:n\n0\nwhich is then noised back to x1:n\nt\u22121.\n2. Related Work\n2.1. Human Motion Generation\nRecent years have seen a great interest in human motion\ngeneration. Previous work has explored unconditional gen-\nerative models [28, 44] as well as generative models us-\ning various input conditions, such as text [5, 9, 35], prior\nmotion [10, 22], action class [8, 24], and music [17, 18,\n37]. In this paper, we focus on text-to-motion generation.\nEarly work usually addressed the text-to-motion task with\na sequence-to-sequence model [19].\nLater on, the focus\nshifted beyond simple action labels. For example, Guo et al.\nutilized variational autoencoders to generate motions from\ntext [9] , significantly enhancing the quality and diversity of\ngenerated motions. With the success of diffusion models in\nAIGC, MDM [35] and other related work [5,16,42] have in-\ntroduced diffusion models into the text-to-motion domain,\nresulting in impressive achievements.\nRecently, there has been some work attempting to gener-\nate fine-grained or stylized motions. For example, Motion-\nCLIP [34] trains an autoencoder to align motion with the\ntext and images of CLIP in the latent feature space, explor-\ning the generation of stylized actions. MotionDiffuse [42]\ninstead proposes precise control of motions and styliza-\ntion of body parts using noise interpolation based on the\nproperties of diffusion models. GestureDiffuCLIP [2] in-\ncorporates style guidance into the diffusion model through\nAdaIN [13], enabling the generation of realistic and stylized\ngestures. For music-to-motion, there has been work [18,37]\ntrying to generate dance movements of different styles by\nusing different styles of music.\nThe work most closely related to ours is Action-\nGPT [15], which introduced, for the first time, Large Lan-\nguage Models (LLMs) into the field of text-conditioned mo-\ntion generation. Action-GPT can be integrated into any text-\nto-motion model. However, it enriches only the description\nof action classes, without providing detailed descriptions of\ndifferent body parts and further guiding the model training\naccordingly. For simultaneous action generation, SINC [3]\nincorporates ChatGPT to identify the body parts involved in\nthe textual description. By generating multiple motions and\nconcatenating them using different body parts, it achieves\n3\nimpressive results. Specifically, SINC divides the human\nbody into [\u2019left arm\u2019, \u2019right arm\u2019, \u2019left leg\u2019, \u2019right leg\u2019,\n\u2019torso\u2019, \u2019neck\u2019, \u2019buttocks\u2019, \u2019waist\u2019], which we borrow from\nin our work. By fine-tuning LLMs, MotionGPT [14,43] de-\nsigns a pre-trained motion language model which supports\nvarious motion-related tasks through prompts.\n2.2. Diffusion Generative Models\nThe diffusion model is a type of neural generative model\nbased on the stochastic diffusion process in thermodynam-\nics [12, 33].\nIt starts with samples from the data distri-\nbution and gradually adds noise through a forward dif-\nfusion process.\nThen, a neural network learns the re-\nverse process to progressively remove the noise and restore\nthe samples to their original states. Diffusion generative\nmodels have achieved significant success in image gener-\nation field [30, 31].\nFor conditional generation,\n[6] in-\ntroduced classifier-guided diffusion, while [11] proposed\na classifier-free method. Given their excellent generation\nquality, [16,35,42] incorporated diffusion models into mo-\ntion generation domain, leading to impressive results.\n3. Method\nGiven a textual description, our goal is to generate a human\nmotion x1:n = {xi}n\ni=1 that matches the given description.\nThe motion consists of n frames of human poses. For each\npose xi \u2208 RJ\u00d7D, we represent it by joint rotations or posi-\ntions, where J represents the number of joints and D rep-\nresents the dimensionality of the joint representation. In\nFigure 2, we give an overview of our fine-grained human\nmotion diffusion model. First, we adopt ChatGPT to per-\nform fine-grained paraphrasing of the given vague textual\ndescription. This expands concise textual descriptions into\ndescriptions of different body parts. FG-MDM then uses\nthese fine-grained descriptions to guide a diffusion model\nfor human motion generation.\n3.1. Prompt Strategy\nWe first introduce the prompt strategy adopted for gen-\nerating fine-grained descriptions. For a given textual de-\nscription of a motion, we utilize ChatGPT-3.5 to gener-\nate more fine-grained descriptions based on different body\nparts. ChatGPT is a conversational model based on a large\nlanguage model that can engage in natural language con-\nversations and generate corresponding responses. The an-\nswers from ChatGPT are often directly influenced by the\ninformation and expression provided in the prompt. If the\nprompt offers clear and detailed questions or instructions,\nChatGPT can typically provide relevant and accurate an-\nswers. However, if the prompt is too simple, ambiguous,\nor unclear, ChatGPT may generate unexpected responses or\nexpress unclear content. For our task, we carefully design\nan effective prompt by using experimental verification.\nOur designed strategy is:\n\u201cTranslate the motion de-\nscribed by the given sentences to the motion of each body\npart only using one paragraph. The available body parts\ninclude [\u2018arms\u2019, \u2018legs\u2019, \u2018torso\u2019, \u2018neck\u2019, \u2018buttocks\u2019, \u2018waist\u2019].\nHere are some examples:[Q...A...]. Question: [sentence]\u201d.\n3.2. Diffusion Model for Motion Generation\nThe basic idea of diffusion models [12, 33] is to learn the\nreverse process of a well-defined stochastic process. Fol-\nlowing MDM [35], we design a text-driven human motion\ngeneration model based on the diffusion model.\nThe diffusion model consists of two parts: the forward\nprocess and the reverse process, both of which follow the\nMarkov chain. The forward process involves adding noise.\nThe input is the original motion x1:n\n0\nfrom the data distribu-\ntion, and the output is the motion x1:n\nt\nwith t times of Gaus-\nsian noise added. When enough noise is added, the motion\nx1:n\nT\ncan approach the Gaussian distribution N(0, I). The\nreverse process, aims to reduce the noise in the Gaussian\nnoise x1:n\nT\n\u223c N(0, I). In the denoising process, at diffu-\nsion step t, a portion of the noise is eliminated, resulting in\na less noisy motion x1:n\nt\u22121. This step is repeated iteratively\nuntil the noise is completely removed, generating the de-\nnoised motion \u02c6x1:n\n0 .\n3.2.1\nNetwork\nwe adopt a simple transformer [38] encoder architecture to\nimplement our network G. Unlike the conventional diffu-\nsion model mentioned above, we follow [30] and predict\nthe clean motion \u02c6x1:n\n0\ninstead of predicting the noise added\nat each diffusion time-step. The input of G is the noised\nmotion x1:n\nt\nobtained by adding noise t times to the orig-\ninal motion x1:n\n0 . The noised motion x1:n\nt\n, together with\nthe text condition code GL, PT1:k and the noise time-step\nt, is inputted to the transformer encoder, resulting in the\nclean motion \u02c6x1:n\n0 . One of the reasons for directly predict-\ning the clean motion in each step of the diffusion model is\nto incorporate human geometric losses during the training\nof the network, making the generated human motions more\nnatural. At each sampling step t, from T to 1, our model\npredicts the clean motion \u02c6x1:n\n0 , and then adds noise back\nto x1:n\nt\u22121. After T iterations, the final clean motion \u02c6x1:n\n0\nis\nobtained.\n3.2.2\nGlobal Token and Part Tokens\nFor the text condition, we encode the entire fine-grained\ndescription with CLIP [29] as a global token GL of the\ntransformer. Apart from this global token, we further en-\ncode descriptions of different body parts individually with\nCLIP as part tokens PT1:k, where k denotes the number\nof body parts. The global token provides overall informa-\ntion about the motion, while part tokens provide local infor-\n4\nmation about different body parts. This design allows the\nglobal token and part tokens to jointly guide the diffusion\nmodel in generating human motions.\n3.2.3\nLoss Functions\nFor training the diffusion model, we follow [30] to predict\nthe signal itself instead of predicting the noise, i.e., \u02c6x1:n\n0\n=\nG(x1:n\nt\n, t, c), with the simple loss function.\nLG = Ex1:n\n0\n\u223cq(x1:n\n0\n|c),t\u223c[1,T ][||x1:n\n0\n\u2212 G(x1:n\nt\n, t, c)]|2] (1)\nIn order to generate more natural and kinematically plau-\nsible motions, we employ the same geometric losses as\nMDM [35] from [24, 32], i.e., positions, foot contact, and\nvelocities.\nLpos = 1\nn\nn\nX\ni=1\n||FK(xi\n0) \u2212 FK(\u02c6xi\n0)||2\n2,\n(2)\nLfoot =\n1\nn \u2212 1\nn\u22121\nX\ni=1\n||(FK(\u02c6xi+1\n0\n) \u2212 FK(\u02c6xi\n0)) \u00b7 fi||2\n2, (3)\nLvel =\n1\nn \u2212 1\nn\u22121\nX\ni=1\n||(xi+1\n0\n\u2212 xi\n0) \u2212 (\u02c6xi+1\n0\n\u2212 \u02c6xi\n0)||2\n2\n(4)\nwhere FK(\u00b7) represents the forward kinematic function\nthat converts joint rotations into joint positions. For each\nframe i, fi \u2208 {0, 1}J is the binary foot contact mask.\nOverall, our training loss is\nL = LG + \u03bbposLpos + \u03bbvelLvel + \u03bbfootLfoot.\n(5)\nwhere \u03bbpos, \u03bbvel, \u03bbfoot are balancing coefficients for the\nthree geometric losses.\n4. Experiments\nIn this section, we first elaborate the datasets, evaluation\nmetrics, and implementation details.\nWe then conduct\nquantitative experiments to compare FG-MDM with current\nstate-of-the-art approaches. In order to completely evaluate\nFG-MDM, we further present two ablation studies. To show\nthe generalization capability of our model, we further per-\nform qualitative experiments and a user study to examine\nFG-MDM\u2019s ability in generating fine-grained and stylized\nmotions.\n4.1. Experimental Details\n4.1.1\nDatasets\nWe utilize the HumanML3D [9] dataset and the KIT [26]\ndataset to train and evaluate our model. HumanML3D is a\nrecently proposed large-scale dataset of motion-text pairs. It\nconsists of 14,616 motion sequences from the AMASS [21]\nand HumanAct12 [8] datasets, with multiple ways of de-\nscribing each motion, resulting in a total of 44,970 text\nannotations. The KIT dataset, on the other hand, is rela-\ntively smaller and contains 3,911 motion sequences along\nwith their corresponding 6,353 text descriptions. For both\ndatasets, we use 80% of the data for training, and use the\nremaining for test.\nWe preprocess the 44,970 text descriptions from Hu-\nmanML3D and 6,353 text descriptions from KIT using\nChatGPT-3.5.\nThis extends these descriptions into fine-\ngrained ones for our model training.\n4.1.2\nEvaluation Metrics\nFor quantitative experiments, we employ five evaluation\nmetrics to evaluate our model\u2019s ability in fitting the train-\ning data: R Precision, FID, Multimodal Dist, Diversity, and\nMultimodality. R Precision and Multimodal Dist are used to\nassess the correlation between generated motions and input\ntext. Diversity and Multimodality are utilized to evaluate\nthe diversity of generated motions. FID measures the dif-\nference in feature distribution between generated motions\nand ground truth in latent space, which is utilized to evalu-\nate the quality of generated motions.\n4.1.3\nImplementation Details\nIn our study, the transformer accepts tokens whose feature\ndimension is 512 as input. We use 4 attention heads and ap-\nply a dropout rate of 0.1. The transformer encoder consists\nof 8 stacked encoder layers to capture complex relationships\nand hierarchies in the data. For ChatGPT, we adopt the gpt-\n3.5-turbo API provided by OpenAI. For text encoding, we\nemploy the frozen CLIP-ViT-B/32 model as the encoder.\nOur batch size is set to 64. Additionally, we set the diffu-\nsion step to 1000. On a single NVIDIA GeForce RTX3090\nGPU, it takes about 6 days for training our model.\n4.2. Comparison with Prior Work\nTo evaluate the performance of FG-MDM in handling\nfine-grained description-based motion generation, we com-\npare our work with four recent motion generation ap-\nproaches: T2M-GPT [41], MLD [5], MotionDiffuse [42],\nand MDM [35]. Note that all these four methods achieve\nstate-of-the-art performance when using short text as in-\nput. To examine their ability in handling fine-grained de-\nscriptions, for all these four methods, we employ their\npre-trained models to generate motions by using our fine-\ngrained description as input. In Table 1 and Table 2, we\nprovide experimental results on the HumanML3D and KIT\ndatasets, respectively.\nNote that on KIT, T2M-GPT and\nMLD do not release their pre-trained models. We thus com-\npare our model with MotionDiffuse and MDM only on KIT.\n5\nA person bends their legs but does not fully squat down.\nA person trips over a small rock and falls down. \nThe dancer lifts one leg with their hands and maintains a one-legged standing position with both hands raised.\nMDM\nFG-MDM\nMLD\nA person hops and jumps on one foot. \nFigure 3. Qualitative results with fine-grained motions. We compare our FG-MDM with MDM [35] and MLD [5]. All three models are\ntrained on HumanML3D. For better visualization, some pose frames are shifted to prevent overlap. Please refer to supplementary materials\nfor more video demos.\nMethod\nR Precision\u2191\nFID\u2193\nMultimodal Dist\u2193\nDiversity\u2192\nMultimodality\u2191\nTop 1\nTop 2\nTop 3\nReal\n0.520\u00b1.003\n0.703\u00b1.006\n0.799\u00b1.005\n0.004\u00b1.000\n2.959\u00b1.007\n9.412\u00b1.046\n-\nT2M-GPT [41]\n0.269\u00b1.008\n0.410\u00b1.009\n0.507\u00b1.012\n3.910\u00b1.069\n5.014\u00b1.060\n8.128\u00b1.171\n2.578\u00b1.122\nMLD [5]\n0.286\u00b1.002\n0.443\u00b1.003\n0.552\u00b1.003\n2.244\u00b1.053\n4.725\u00b1.015\n8.773\u00b1.058\n3.317\u00b1.078\nMotionDiffuse [42]\n0.308\u00b1.007\n0.466\u00b1.008\n0.571\u00b1.007\n2.755\u00b1.087\n4.483\u00b1.031\n8.333\u00b1.163\n1.667\u00b1.127\nMDM [35]\n0.187\u00b1.009\n0.309\u00b1.007\n0.399\u00b1.008\n4.363\u00b1.265\n7.331\u00b1.065\n8.432\u00b1.117\n3.059\u00b1.027\nFG-MDM\n0.308\u00b1.015\n0.484\u00b1.005\n0.593\u00b1.003\n0.663\u00b1.051\n5.649\u00b1.035\n9.476\u00b1.077\n2.632\u00b1.085\nTable 1. Quantitative results on HumanML3D. To examine the ability in handling fine-grained descriptions, for all the four control methods,\nwe employ their pre-trained models to generate motions by using our fine-grained description as input.\nFor all experiments, We run the evaluation 5 times and \u201c\u00b1\u201d\nindicates the 95% confidence interval.\nAs reported in both tables, we achieve impressive perfor-\nmance when handling the task of fine-grained description-\nbased motion generation. In particular, FG-MDM leads to\nmuch shorter FID than prior state-of-the-art. For example,\non KIT, both MotionDiffuse and MDM get long FID around\n16, while our model gets a figure shorter than 0.35. This\n6\nA person jumps happily, as they raise both hands in the air.\nA person jumps sadly, their legs slightly bent but not fully embracing the leap with enthusiasm.\nA person walks with a swagger. \nMDM\nMLD\nFG-MDM\nA person walks cautiously. \nFigure 4. Qualitative results with stylized motions. All three models are trained on HumanML3D. Please refer to supplementary materials\nfor more video demos.\nMethod\nR Precision\u2191\nFID\u2193\nMultimodal Dist\u2193\nDiversity\u2192\nMultimodality\u2191\nTop 1\nTop 2\nTop 3\nReal\n0.418\u00b1.011\n0.633\u00b1.011\n0.775\u00b1.009\n0.038\u00b1.009\n2.749\u00b1.034\n10.852\u00b1.194\n-\nMotionDiffuse [42]\n0.189\u00b1.008\n0.296\u00b1.016\n0.376\u00b1.009\n16.136\u00b1.648\n6.513\u00b1.104\n8.948\u00b1.339\n1.024\u00b1.027\nMDM [35]\n0.099\u00b1.013\n0.167\u00b1.013\n0.235\u00b1.019\n16.372\u00b1.855\n10.501\u00b1.058\n8.758\u00b1.233\n2.242\u00b1.998\nFG-MDM\n0.151\u00b1.010\n0.269\u00b1.013\n0.363\u00b1.008\n0.344\u00b1.016\n9.351\u00b1.073\n10.707\u00b1.082\n0.871\u00b1.442\nTable 2. Quantitative results on KIT. Since T2M-GPT [41] and MLD [5] do not release their pre-trained models on KIT. We thus compare\nour model with MotionDiffuse and MDM only.\nindicates FG-MDM can generate motions closer to real mo-\ntions than other methods, when using fine-grained descrip-\ntion as input.\n4.3. Ablation Study\nIn order to validate the contribution of different modules,\nnext we conduct two ablation studies. The first study is\nused to examine the contribution of ChatGPT-Generated\nfine-grained texts, which is performed by replacing the fine-\ngrained description with the original short text. Therefore,\nonly a single token is adopted to guide the diffusion model.\nThe second study is performed by adopting only the global\ntoken to check the contribution of part tokens when fine-\ngrained descriptions are used. In Table 3 and Table 4, we\n7\ngive the results on HumanML3D and KIT, respectively. As\nobserved, ablating either fine-grained descriptions or part\ntokens leads to poorer performance.\n4.4. Generalization Capability Study\nNote that FG-MDM aims to not only fit the training data but\nalso generate fine-grained and stylized motions beyond the\nscope of the dataset. Considering that HumanML3D and\nKIT include very few fine-grained and stylized motions,\nwe customize some texts that are not in the two datasets\nto show the effectiveness of FG-MDM in generating fine-\ngrained and stylized motions. In Figure 3 and Figure 4,\nwe show motions generated by MDM [35], MLD [5] and\nour FG-MDM. Note that for all the three methods, we use\nmodels trained on HumanML3D to generate motions. In\ncomparison, our method generates motions that are more\nconsistent with the details described in the slightly complex\ntextual descriptions. Even for difficult stylized textual de-\nscriptions, our method is capable of generating motions that\ncorrespond to the respective styles. This demonstrates the\nstrong generalization capability of FG-MDM. Please refer\nto supplementary materials for more video demos.\n12%\n38%\n33%\n37%\n36%\n37%\n51%\n26%\n30%\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nFG-MDM\nMDM\nMotionDiffuse\nleast matching\nmost matching\nFigure 5. User study results. For each method, a color bar ranging\nfrom blue to red is used to represent the percentage of text-to-\nmotion match levels, with blue indicating the least match and red\nindicating the most match.\n4.5. User Study\nTo further examine FG-MDM\u2019s generalization capability,\nwe conduct a user study to evaluate the quality of motions\ngenerated by our model based on human visual perception.\nWe customize a total of 40 fine-grained and stylized tex-\ntual descriptions. With these descriptions, we generate mo-\ntions by using MDM [35], MotionDiffuse [42] and our FG-\nMDM. We then recruit 10 users for the study. In each ques-\ntion, participants are asked to rate the degree of matching\nbetween the generated motion and the textual description on\na scale of 0 to 2. The results are given in Figure 5. Overall,\nF\nP\nFID\u2193\nR Precision(top 3)\u2191\nDiversity\u2192\n4.363\n0.399\n8.432\n\u2713\n1.050\n0.427\n9.509\n\u2713\n\u2713\n0.663\n0.593\n9.476\nTable 3. Ablation study results on HumanML3D. \u201cF\u201d denotes us-\ning ChatGPT-generated fine-grained descriptions. \u201cP\u201d represents\nadopting part tokens.\nF\nP\nFID\u2193\nR Precision(top 3)\u2191\nDiversity\u2192\n16.372\n0.235\n8.758\n\u2713\n0.549\n0.360\n10.829\n\u2713\n\u2713\n0.344\n0.383\n10.707\nTable 4. Ablation study results on KIT.\nusers believe that FG-MDM matches texts much better in\ngenerating fine-grained and stylized motions than the other\ntwo methods. For example, the proportion of MDM and\nMotionDiffuse obtaining the most matching scores is only\nslightly higher than half of FG-MDM.\n5. Conclusion\nIn this study, we use ChatGPT to perform fine-grained para-\nphrasing on the textual annotations of HumanML3D and\nKIT. With these fine-grained descriptions, We explore a\nFine-Grained Human Motion Diffusion Model. It utilizes\nfine-grained descriptions of different body parts to guide the\ntraining of a diffusion model. This enables it to learn the\ngeneration of fine-grained and stylized motions even out-\nside of the distribution of the training dataset. In the future,\nwe would like to improve the quality of fine-grained anno-\ntations of human motions.\nReferences\n[1]\nHyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and\nSonghwai Oh. Text2action: Generative adversarial synthesis\nfrom language to action. In 2018 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages 5915\u20135920.\nIEEE, 2018. 1\n[2]\nTenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip:\nGesture diffusion model with clip latents.\narXiv preprint\narXiv:2303.14613, 2023. 3\n[3]\nNikos Athanasiou, Mathis Petrovich, Michael J Black, and\nG\u00a8ul Varol.\nSinc: Spatial composition of 3d human mo-\ntions for simultaneous action generation.\narXiv preprint\narXiv:2304.10417, 2023. 1, 3\n[4]\nSamaneh Azadi, Akbar Shah, Thomas Hayes, Devi Parikh,\nand Sonal Gupta.\nMake-an-animation: Large-scale text-\nconditional 3d human motion generation.\nIn Proceedings\n8\nof the IEEE/CVF International Conference on Computer Vi-\nsion, 2023. 2\n[5]\nXin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000\u201318010, 2023. 1, 2, 3, 5, 6, 7, 8\n[6]\nPrafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780\u20138794, 2021. 4\n[7]\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00a8el Kubli. Chat-\ngpt outperforms crowd-workers for text-annotation tasks.\narXiv preprint arXiv:2303.15056, 2023. 2\n[8]\nChuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao\nSun, Annan Deng, Minglun Gong, and Li Cheng.\nAc-\ntion2motion: Conditioned generation of 3d human motions.\nIn Proceedings of the 28th ACM International Conference on\nMultimedia, pages 2021\u20132029, 2020. 3, 5\n[9]\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,\nXingyu Li, and Li Cheng. Generating diverse and natural 3d\nhuman motions from text. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 5152\u20135161, 2022. 1, 2, 3, 5\n[10] Alejandro Hernandez, Jurgen Gall, and Francesc Moreno-\nNoguer. Human motion prediction via spatio-temporal in-\npainting.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 7134\u20137143, 2019. 3\n[11] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 4\n[13] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 1501\u20131510, 2017. 3\n[14] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and\nTao Chen. Motiongpt: Human motion as a foreign language.\narXiv preprint arXiv:2306.14795, 2023. 4\n[15] Sai Shashank Kalakonda, Shubh Maheshwari, and Ravi Ki-\nran Sarvadevabhatla.\nAction-gpt: Leveraging large-scale\nlanguage models for improved and generalized action gen-\neration. In IEEE International Conference on Multimedia\nand Expo (ICME), 2023. 2, 3\n[16] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\nform language-based motion synthesis & editing. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 8255\u20138263, 2023. 1, 3, 4\n[17] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-\nformer: Music conditioned 3d dance generation with para-\nmetric motion transformer. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, pages 1272\u20131279, 2022. 3\n[18] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3d dance\ngeneration with aist++. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 13401\u2013\n13412, 2021. 3\n[19] Angela S Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qix-\ning Huang, and Raymond J Mooney. Generating animated\nvideos of human activities from natural language descrip-\ntions. Learning, 2018(1), 2018. 3\n[20] Xiao Lin and Mohamed R Amer. Human motion modeling\nusing dvgans. arXiv preprint arXiv:1804.10652, 2018. 1\n[21] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Ger-\nard Pons-Moll, and Michael J Black.\nAmass:\nArchive\nof motion capture as surface shapes.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 5442\u20135451, 2019. 5\n[22] Julieta Martinez, Michael J Black, and Javier Romero. On\nhuman motion prediction using recurrent neural networks. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2891\u20132900, 2017. 3\n[23] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al.\nTraining lan-\nguage models to follow instructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 2\n[24] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Action-\nconditioned 3d human motion synthesis with transformer\nvae. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 10985\u201310995, 2021. 1,\n3, 5\n[25] Mathis Petrovich, Michael J Black, and G\u00a8ul Varol. Temos:\nGenerating diverse human motions from textual descriptions.\nIn European Conference on Computer Vision, pages 480\u2013\n497. Springer, 2022. 1\n[26] Matthias Plappert, Christian Mandery, and Tamim Asfour.\nThe kit motion-language dataset. Big data, 4(4):236\u2013252,\n2016. 2, 5\n[27] Abhinanda R Punnakkal, Arjun Chandrasekaran, Nikos\nAthanasiou, Alejandra Quiros-Ramirez, and Michael J\nBlack. Babel: Bodies, action and behavior with english la-\nbels. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 722\u2013731, 2021.\n2\n[28] Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman,\nOlga Sorkine-Hornung, and Daniel Cohen-Or. Modi: Un-\nconditional motion synthesis from diverse data. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 13873\u201313883, 2023. 3\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 4\n9\n[30] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 4, 5\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 4\n[32] Mingyi Shi, Kfir Aberman, Andreas Aristidou, Taku Ko-\nmura, Dani Lischinski, Daniel Cohen-Or, and Baoquan\nChen.\nMotionet: 3d human motion reconstruction from\nmonocular video with skeleton consistency. ACM Transac-\ntions on Graphics (TOG), 40(1):1\u201315, 2020. 5\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n4\n[34] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human motion\ngeneration to clip space. In European Conference on Com-\nputer Vision, pages 358\u2013374. Springer, 2022. 2, 3\n[35] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel\nCohen-or, and Amit Haim Bermano. Human motion diffu-\nsion model. In The Eleventh International Conference on\nLearning Representations, 2023. 1, 2, 3, 4, 5, 6, 7, 8\n[36] Yating Tian, Hongwen Zhang, Yebin Liu, and Limin Wang.\nRecovering 3d human mesh from monocular images: A sur-\nvey. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2023. 2\n[37] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 448\u2013458, 2023. 3\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 4\n[39] Hongwen Zhang, Yating Tian, Xinchi Zhou, Wanli Ouyang,\nYebin Liu, Limin Wang, and Zhenan Sun. Pymaf: 3d human\npose and shape regression with pyramidal mesh alignment\nfeedback loop. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 11446\u201311456,\n2021. 2\n[40] Hongwen Zhang, Yating Tian, Yuxiang Zhang, Mengcheng\nLi, Liang An, Zhenan Sun, and Yebin Liu. Pymaf-x: To-\nwards well-aligned full-body model regression from monoc-\nular images. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 2023a. 2\n[41] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli\nHuang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi\nShen. T2m-gpt: Generating human motion from textual de-\nscriptions with discrete representations. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023b. 5, 6, 7\n[42] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\nfuse: Text-driven human motion generation with diffusion\nmodel. arXiv preprint arXiv:2208.15001, 2022. 1, 3, 4, 5, 6,\n7, 8\n[43] Yaqi Zhang, Di Huang, Bin Liu, Shixiang Tang, Yan Lu, Lu\nChen, Lei Bai, Qi Chu, Nenghai Yu, and Wanli Ouyang. Mo-\ntiongpt: Finetuned llms are general-purpose motion genera-\ntors. arXiv preprint arXiv:2306.10900, 2023c. 4\n[44] Rui Zhao, Hui Su, and Qiang Ji. Bayesian adversarial human\nmotion synthesis.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n6225\u20136234, 2020. 3\n[45] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu\nZhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang.\nHuman motion generation: A survey. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2023. 1\n10\n"
  },
  {
    "title": "WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words",
    "link": "https://arxiv.org/pdf/2312.02931.pdf",
    "upvote": "5",
    "text": "WhisBERT: Multimodal Text-Audio Language Modeling on 100M Words\nLukas WolfQ\nGreta TuckuteM\nKlemen KotarS\nEghbal HosseiniM\nTamar I. RegevM\nEthan Gotlieb WilcoxQ\nAlex WarstadtQ\nQETH Z\u00fcrich\nMMIT\nSStanford University\n{wolflu, warstadt, ethan.wilcox}@ethz.ch\nklemenk@stanford.edu\n{gretatu, ehoseini, tamarr}@mit.edu\nAbstract\nTraining on multiple modalities of input can\naugment the capabilities of a language model.\nHere, we ask whether such a training regime\ncan improve the quality and efficiency of these\nsystems as well. We focus on text\u2013audio and\nintroduce WhisBERT, which is inspired by the\ntext\u2013image approach of FLAVA (Singh et al.,\n2022). In accordance with BabyLM (Warstadt\net al., 2023) guidelines, we pretrain WhisBERT\non a dataset comprising only 100 million words\nplus their corresponding speech from the word-\naligned version of the People\u2019s Speech dataset\n(Galvez et al., 2021). To assess the impact\nof multimodality, we compare versions of the\nmodel that are trained on text only and on both\naudio and text simultaneously. We find that\nwhile WhisBERT is able to perform well on\nmultimodal masked modeling and surpasses the\nBabyLM baselines in most benchmark tasks, it\nstruggles to optimize its complex objective and\noutperform its text-only WhisBERT baseline.\nhttps://github.com/lu-wo/whisbert\n1\nIntroduction\nRecent advances in language modeling and their\ndownstream applications have been driven, in large\npart, by bigger models, with respect to both model\nsize and amounts of training data. Larger and larger\npretraining datasets highlight the gap between hu-\nmans and deep learning models in terms of learning\nefficiency \u2014while state-of-the-art language models\nneed billions of examples to approach human-level\nlanguage performance, people learn their language\nfrom experience with about 100 million words or\nless (Warstadt and Bowman, 2022; Frank, 2023).\nWe hypothesize that one major reason for this\ndata efficiency gap is the different inputs that hu-\nmans and current deep learning systems receive.\nHuman language learning involves multiple modal-\nities, including both visual and auditory input. In\ncontrast, typical language models are trained on\nrepresentations of text alone. For this BabyLM\nsubmission, we ask whether training on inputs of\nmultiple modalities can increase language models\u2019\ntraining efficiency, with a focus on text-audio mul-\ntimodal input. We conjecture that multimodal data\nsources have the potential to enrich the language\nlearning process, enabling models to leverage com-\nplementary information from different modalities\nand thus augment their learning capacity (Bal-\ntru\u0161aitis et al., 2017).\nMultimodal language modeling has experienced\na noteworthy surge in research productivity lately,\nin applications such as image retrieval, semantic\nembeddings, and image generation (Driess et al.,\n2023; Koh et al., 2023; Yasunaga et al., 2023)\nHowever, text-audio multimodal language mod-\neling (e.g. (Chuang et al., 2019; Lakhotia et al.,\n2021)) remains largely unexplored, especially in\nlow-resource settings such as the 100 million train-\ning regime we employ here. As a first step towards\na text-audio language model, we introduce Whis-\nBERT, a novel masked language model (MLM)\narchitecture inspired by vision-text models such\nas FLAVA (Singh et al., 2022). The core idea is\nthat WhisBERT is trained in a multitask setting on\nboth unimodal (i.e. text- or audio-only) and mul-\ntimodal objectives. In multimodal objectives, the\nmodel receives matched text-audio segments, and\nit can use information from one modality to learn\nrepresentations for the other.\nTo accommodate the specific requirements of\nthe BabyLM challenge (Warstadt et al., 2023), we\npretrain WhisBert on a dataset of matched audio\nand text transcripts comprising 100 million words\nsampled from the People\u2019s Speech dataset (Galvez\net al., 2021). We use an improved version of the\naudio-text-aligned training data, a subset of an up-\ncoming speech production dataset release (see Sec-\ntion 3). This commitment to using high-quality\npretraining data is in line with the data efficiency\nobjectives of the BabyLM challenge.\narXiv:2312.02931v2  [cs.CL]  7 Dec 2023\nWe carry out a rigorous evaluation of the per-\nformance of the audio, text, and multimodal en-\ncoders within this new framework. We find that\neven though the optimization problem in the mul-\ntimodal setting is much harder compared to a uni-\nmodal setting, the multimodal WhisBERT model\noutperforms the text-only baseline in a majority of\nthe BabyLM challenge tasks, which address sev-\neral aspects of language understanding, even when\ntrained for only a single iteration over the dataset.\n2\nWhisBERT\nWhisBERT is a multimodal audio and text model\nthat is inspired by OpenAI\u2019s Whisper model (Rad-\nford et al., 2022) for speech recognition and BERT\n(Devlin et al., 2019) for bidirectional language en-\ncoding. WhisBERT contains two separate input\nstreams, one of audio and of its corresponding text\n(i.e., a transcription). The model is trained using a\ncombination of two unimodal and three multimodal\nmasked training objectives. In the unimodal setting,\nthe model must predict either a masked word or a\nmasked patch of audio. In the multimodal training\nsetting, the model must predict pairs of matched\nword/audio patches. This multi-objective training\nsetup is inspired by the visual-audio model FLAVA\n(Singh et al., 2022).\n2.1\nArchitecture details\nAudio encoder\nTo create audio patches that we\ncan process with Whisper\u2019s bidirectional trans-\nformer encoder (Vaswani et al., 2017), the audio\nstream is first passed through the Whisper Feature\nExtractor available on Hugging Face.\nAll audio data is re-sampled to a rate of 16,000\nHz, and an 80-channel log-magnitude Mel spec-\ntrogram representation is computed using 25-\nmillisecond windows with a 10-millisecond stride.\nWe then pass the audio spectrogram through a\npatch embedding layer: a convolutional encoder\nprocesses the extracted frequency features using\na stem of two 1-dimensional convolution layers\n(along the time dimension, filters cover all input\nfrequencies), both with a filter width of 16 and\nincorporating the GELU activation function. The\nsecond convolution layer employs a stride of 10.\nThis patch embedding layer creates overlapping\n1-dimensional audio patches covering 100ms of the\naudio signal as input to the transformer.\nAfter preprocessing and patch embedding, sinu-\nsoidal position embeddings are added to the stem\u2019s\noutput, which is then processed by Whisper\u2019s trans-\nformer encoder blocks. A notable difference to\nthe standard Whisper encoder is that we prepend a\nlearnable classification (henceforth, CLS) token at\nthe beginning of the audio patch sequence. There-\nfore, the audio encoder produces a list of audio\nhidden states {hA} each corresponding to a contex-\ntualized audio patch, as well as an additional audio\nclassification state hCLS,A.\nText encoder\nIn order to encode the text input,\nwe choose a standard bidirectional transformer\narchitecture following the BERT (Devlin et al.,\n2019) model. We train a WordPiece (Wu et al.,\n2016) tokenizer on the 100M words in our People\u2019s\nspeech (Galvez et al., 2021) subset (see Section 3).\nThe WordPiece tokenizer automatically prepends\na CLS token to the token sequence which is con-\ntextualized with the rest of the sequence. The text\nencoder produces a list of text hidden states {hT }\ncorresponding to a text token, as well as an addi-\ntional text CLS token hCLS,T .\nMultimodal encoder\nThe multimodal encoder\nis a standard transformer encoder that gets as in-\nput the concatenated contextualized audio and text\nsequences. Additionally, we prepend a learnable\nmultimodal CLS token and employ sinusoidal po-\nsitional embeddings. The multimodal encoder con-\ntextualizes the multimodal sequence and outputs\na list of multimodal hidden states {hM} each cor-\nresponding to an unimodal vector from {hA} or\n{hT }, as well as an additional multimodal CLS\ntoken hCLS,M.\nAdapting to downstream tasks\nThe WhisBert\nmodel can be readily applied to both unimodal\nand multimodal tasks. For audio recognition tasks\n(e.g., speaker identification or speech recognition),\nwe apply a classifier head (e.g., a linear layer or\na multi-layer perceptron) on top of the unimodal\nclassification token, hCLS,A, from the audio en-\ncoder. Similarly, for language understanding and\nmultimodal reasoning tasks, we can apply a classi-\nfier head on top of the classification token, hCLS,T ,\nfrom the text encoder or hCLS,M from the multi-\nmodal encoder, respectively.\n2.2\nPretraining objectives\nOur goal is to pretrain models to have robust con-\ntextual representations for both text and audio on\ntheir own as well as for aligned text-audio pairs.\nWe use the approach from FLAVA (Singh et al.,\n2022) of multitask training over a selection of uni-\nmodal and multimodal training objectives that have\nbeen demonstrated to facilitate joint learning on\nimages and text. We adapt the five objectives used\nby FLAVA for the audio domain.\n2.2.1\nUnimodal pretraining objectives\nMasked Language Modeling\nMasked Language\nModeling (MLM) is a pretraining objective that\nencourages the model to learn a deep understanding\nof the language. In MLM, a portion of the input\ntokens is masked and the model is trained to predict\nthe original identity of the masked tokens based on\ntheir context.\nGiven an input sequence of tokens x\n=\n[x1, x2, ..., xT ], for MLM, a subset M of these to-\nkens is selected to be masked. The objective is to\nminimize the negative log-likelihood of the masked\ntokens:\nLMLM(x) = \u2212 1\n|M|\nX\nt\u2208M\nlog pmodel(xt|x\u00act)\n(1)\nHere, xt is a masked token, x\u00act represents the\nsequence with the token xt masked, and pmodel is\nthe model\u2019s probability distribution over possible\ntokens. |M| is the size of the subset of masked\ntokens, and the sum is taken over all masked posi-\ntions t. The goal is to adjust the model\u2019s parameters\nto minimize this loss. We obtain a probability dis-\ntribution over the vocabulary by applying a linear\nprediction head on the text hidden states {hT }.\nMasked Audio Modeling\nWe introduce the\nMasked Audio Modeling (MAM) objective LMAM\nwhich follows the principles of Contrastive Predic-\ntive Coding (van den Oord et al., 2019). In MAM,\nwe randomly mask audio patches in the input se-\nquence to the audio encoder. The encoder is ex-\npected to generate outputs that are most similar to\nthe unmasked input at a particular masked position\nt. The self-supervised loss function, which aims\nto encourage the model to align masked tokens\nwith their unmasked identities given the context, is\ndefined for a masked token localized at t as:\nLMAM = \u2212 log\nexp(sim(ct, bt)/\u03ba)\nP\nbi\u2208BD exp(sim(ct, bi)/\u03ba) (2)\nHere, ct is the output of the transformer at po-\nsition t, and bi is the audio representation vector\nof the (unmasked) patch at some offset i. BD is\na set of 20 uniformly selected negative samples\nfrom the same sequence, plus bt, and sim() is a\nsimilarity function. For our implementation, we\nuse the cosine similarity function, adjusted by a\ntemperature function, \u03ba, which is set to 0.1. The\nloss function operates by adjusting the output of\nthe transformer at position t to be most similar to\nthe encoded representation at t, despite the fact that\nthis input to the transformer is masked. In this way,\nthe model is encouraged to predict the content of\nthe masked spans based on the unmasked context.\n2.2.2\nMultimodal Pretraining Objectives\nMultimodal Contrastive Loss\nContrastive loss\n(Gutmann and Hyv\u00e4rinen, 2010) has been success-\nfully applied to image-text representation learn-\ning in approaches such as CLIP (Radford et al.,\n2021).\nOur audio-text contrastive loss LMMC\naims to maximize the cosine similarities between\nmatched audio and text pairs and minimize those\nfor the unmatched pairs across a given batch of au-\ndio clips and corresponding text. This is achieved\nby linearly projecting the classification token of\neach audio sequence hCLS,A and text sequence\nhCLS,T into a common embedding space, followed\nby L2-normalization, dot-product, and a softmax\nloss scaled by temperature.\nThe goal of this process is to ensure that the au-\ndio and text representations for the same data point\nare brought closer together in the embedding space,\nwhile representations for different data points are\npushed apart. This encourages the model to learn\nmeaningful representations that capture the shared\ninformation between the audio and text modalities.\nMasked Multimodal Modeling (MMM)\nWe in-\ntroduce a Masked Multimodal Modeling (MMM)\npretraining objective LMMM, that uses the output\nof the multimodal encoder {hM} to attempt to re-\nconstruct the masked tokens from both the audio\nand text sequences. For the multimodal contextual-\nized audio tokens, we employ the Contrastive Pre-\ndictive Coding strategy introduced in Section 2.2.1.\nFor the multimodal text tokens, we add a multi-\nmodal masked language modeling head we com-\npute the MLM loss as introduced in Section 2.2.1.\nThe MMM pretraining objective is designed to\nencourage the model to understand the interdepen-\ndencies between audio and text modalities, which\nin addition to the MMC loss has been found to\nimprove performance on multimodal downstream\ntasks (Singh et al., 2022). It is computed separately\nfrom the contrastive loss, which is applied on audio\nand text tokens without any masking.\nAudio-Text Matching (ATM)\nFinally, we incor-\nporate an Audio-Text Matching loss, LATM, in\nwhich we feed a batch of samples that include both\nmatched and unmatched audio-text pairs. We apply\na classifier on top of the output from the multi-\nmodal encode to decide if the input audio and text\nmatch each other.\n2.3\nPretraining WhisBERT\nWe pretrain WhisBERT on both text and audio sam-\nples from the dataset introduced in Section 3 for\nfive epochs with stochastic gradient descent. Al-\nthough WhisBERT is able to learn both from paired\nand unpaired examples, in our pretraining dataset\nwe only encounter text-audio pairs. This allows\nus to always apply all unimodal and multimodal\nobjective functions. For further details and hyper-\nparameters we refer to this GitHub repository.\n3\nPeople\u2019s Speech Dataset\nThe People\u2019s Speech dataset (Galvez et al., 2021) is\na free-to-download, 30k hour English speech recog-\nnition dataset. The dataset is collected from appro-\npriately licensed internet audio data with existing\ntranscriptions, consisting of a clean and a dirty sub-\nset. We re-transcribed and re-aligned the People\u2019s\nSpeech dataset using recently-released automatic\nspeech recognition toolkits (Radford et al., 2022;\nBain et al., 2023), which may provide better align-\nment than the baseline, publically available align-\nments. For this step we transcribe speech the Whis-\nper large-v2 model from OpenAI (Radford et al.,\n2022). Numerals and non-standard characters were\nsuppressed in the transcriptions, such that numbers\nwere represented as words and non-standard char-\nacters were omitted. Otherwise, default parameters\nwere used. The transcriptions were force-aligned to\nmatch the audio files using the WhisperX pipeline\n(Bain et al., 2023; Bredin et al., 2019; Baevski et al.,\n2020). We excluded very short transcripts (fewer\nthan 100 words) or transcripts that contained more\nthan 0.1% of words that could not be transcribed.\nThe remaining files were sorted according to mean\nword-level transcription confidence (Whisper es-\ntimates a value between 0 and 1 that denotes the\ntranscription confidence per word). We selected the\nfiles containing the first 100M words in this order-\ning. The average confidence of these final 100M\nwords was 0.78 with 47M words from the clean\naudio subset and 53M words from the dirty audio\nsubset. The transcribed, word-aligned dataset will\nFigure 1: Text-only baseline vs WhisBERT on masked\nlanguage modeling task during the first epoch. Inter-\nestingly, during the first epoch WhisBERT seems to\nperform better (outperforming the text-only baseline in\n11 out of 17 tasks), but after five epochs does not outper-\nform the text-only baseline across all benchmark tasks\nbe released as part of an upcoming speech produc-\ntion dataset.\n4\nExperimental Results\nThe main question we are interested in is whether\npretraining on audio\u2013text data can improve model\nperformance. We assess this by comparing the text-\nencoder only version of WhisBERT compared to\nthe exact same architecture trained with the multi-\nmodal objectives introduced in Section 2.2. (This\nis the MLM (text) vs. MMM (multi-modal) com-\nparison in Table 1.) Our results suggest that the\nanswer is mixed. The MLM (text-only) version of\nthe model achieves higher scores on 12 out of the 17\ntest suites, with the multi-modal model performing\nhigher for Ellipsis, Island Effects, Quantifiers, Hy-\npernym, and Question/Answer Congruence (tricky)\ntests. Interestingly, the three of these that were in\nthe original BLiMP paper (Ellipsis, Island Effects\nand Quantifiers), were three of the four lowest-\nscoring tests for human accuracy, suggesting that\nwhere multi-modality does help, it is in processing\nparticularly syntactically difficult material. Both\nof our trained models outperform the OPT-125M,\nRoBERTa and T5 baselines, averaging across tasks.\n5\nDiscussion\nLimitations\nWe begin our discussion by noting\nthe limitations of the current work. First, the Peo-\nple\u2019s Voice dataset presents a unique set of chal-\nlenges, which likely resulted in limitations of the\nWhisBERT model. The most significant of these is\nthat it is primarily comprised of audio from movies,\nTask\nMLM\nMM\nOPT-125m\nRoBERTa-base\nT5-base\nanaphor_agreement\n83.74%\n81.29%\n63.8%\n81.5%\n68.9%\nargument_structure\n68.60%\n64.88%\n70.6%\n67.1%\n63.8%\nbinding\n66.95%\n65.38%\n67.1%\n67.3%\n60.4%\ncontrol_raising\n65.25%\n64.76%\n66.5%\n67.9%\n60.9%\ndeterminer_noun_agreement\n92.24%\n87.93%\n78.5%\n90.8%\n72.2%\nellipsis\n83.14%\n88.68%\n62.0%\n76.4%\n34.4%\nfiller_gap\n73.12%\n72.02%\n63.8%\n63.5%\n48.2%\nirregular_forms\n89.62%\n85.90%\n67.5%\n87.4%\n77.6%\nisland_effects\n53.51%\n55.87%\n48.6%\n39.9%\n45.6%\nnpi_licensing\n64.77%\n55.12%\n46.7%\n55.9%\n47.8%\nquantifiers\n69.58%\n71.69%\n59.6%\n70.5%\n61.2%\nsubject_verb_agreement\n75.05%\n70.73%\n56.9%\n65.4%\n65.0%\nhypernym\n50.12%\n51.98%\n50.0%\n49.4%\n48.0%\nqa_congruence_easy\n71.88%\n67.19%\n54.7%\n31.3%\n40.6%\nqa_congruence_tricky\n52.12%\n53.94%\n31.5%\n32.1%\n21.2%\nsubject_aux_inversion\n77.90%\n74.85%\n80.3%\n71.7%\n64.9%\nturn_taking\n61.79%\n58.21%\n57.1%\n53.2%\n45.0%\nTable 1: Evaluation scores of text-only (MLM), multimodal WhisBERT (MM), and the BabyLM baselines on\nBLiMP tasks. The BabyLM baselines were trained on the 100M words BabyLM dataset.\nand thus includes things like background noise,\nmusic and audio effects that accompanied the dia-\nlog. This could have resulted in lower text\u2013audio\nalignment accuracy, and likely made the audio-\nmodeling challenge more difficult than for an in-\nstudio recorded dataset.\nSecond, the requirements of the BabyLM chal-\nlenge presented us with additional restrictions.\nMost notably, we were not allowed to use pre-\ntrained audio encoders, and thus had to train these\nfrom scratch.\nLikely, this contributed to sub-\noptimal performance and requires further explo-\nration. Furthermore, due to time limitations, we\ndid not fully explore the space of the model\u2019s hy-\nperparameters; it is well known that changes in\nhyperparameter settings can have large impacts on\na model\u2019s performance.\nOur mixed results when comparing WhisBERT\nagainst a text-only model suggest that small data\nsettings are insufficient for effectively training a\ntext-only masked language model. Given that the\narchitectural basis for WhisBERT, Flava, was de-\nsigned and built as a large-data foundation model,\nwe suggest that such larger-data settings serve as\nthe basis for future development and testing of the\nWhisBERT model.\nFuture Work\nWe plan to train versions of Whis-\nBERT on more than 100M words and their cor-\nresponding audio. This would enable investiga-\ntions of the full capacity of the WhisBERT model\nand make it more comparable to similar vision-text\nmodels such as FLAVA (Singh et al., 2022). On\nthe architecture level, one could replace the bidirec-\ntional transformer in the WhisBERT architecture\nwith an autoregressive language model, allowing\nthe use of the standard Whisper pretraining objec-\ntives in addition to the multi-modal ones.\nContribution Statement\nLW, EH, TIR, EGW, and AW conceived of the\nideas presented in this work. KK and GT provided\nthe dataset used in pretraining WhisBERT. LW im-\nplemented the model and carried out the experi-\nments. LW, KK, GT, EGW, AW, and TIR wrote the\nmanuscript. All authors edited the manuscript and\nreviewed the work.\nReferences\nAlexei Baevski, Henry Zhou, Abdel rahman Mohamed,\nand Michael Auli. 2020. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nArXiv, abs/2006.11477.\nMax Bain, Jaesung Huh, Tengda Han, and Andrew Zis-\nserman. 2023. Whisperx: Time-accurate speech tran-\nscription of long-form audio.\nTadas Baltru\u0161aitis, Chaitanya Ahuja, and Louis-Philippe\nMorency. 2017. Multimodal machine learning: A\nsurvey and taxonomy.\nHerv\u00e9 Bredin, Ruiqing Yin, Juan Manuel Coria, Gre-\ngory Gelly, Pavel Korshunov, Marvin Lavechin,\nDiego Fustes, Hadrien Titeux, Wassim Bouaziz, and\nMarie-Philippe Gill. 2019. Pyannote.audio: Neural\nbuilding blocks for speaker diarization. ICASSP 2020\n- 2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 7124\u2013\n7128.\nYung-Sung Chuang, Chi-Liang Liu, Hung-Yi Lee,\nand Lin-shan Lee. 2019.\nSpeechbert: An audio-\nand-text jointly learned language model for end-\nto-end spoken question answering. arXiv preprint\narXiv:1910.11559.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al.\n2023. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378.\nMichael C Frank. 2023. Bridging the data gap between\nchildren and large language models.\nDaniel Galvez, Greg Diamos, Juan Ciro, Juan Felipe\nCer\u00f3n, Keith Achorn, Anjali Gopi, David Kanter,\nMaximilian Lam, Mark Mazumder, and Vijay Janapa\nReddi. 2021. The People\u2019s Speech: A Large-Scale\nDiverse English Speech Recognition Dataset for\nCommercial Usage. ArXiv:2111.09344 [cs, stat].\nMichael Gutmann and Aapo Hyv\u00e4rinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on Artifi-\ncial Intelligence and Statistics, volume 9 of Proceed-\nings of Machine Learning Research, pages 297\u2013304,\nChia Laguna Resort, Sardinia, Italy. PMLR.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\n2023. Generating images with multimodal language\nmodels.\nKushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu,\nYossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh\nNguyen, Jade Copet, Alexei Baevski, Abdelrahman\nMohamed, et al. 2021. On generative spoken lan-\nguage modeling from raw audio. Transactions of the\nAssociation for Computational Linguistics, 9:1336\u2013\n1354.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision. In Proceedings of the 38th International\nConference on Machine Learning, volume 139 of\nProceedings of Machine Learning Research, pages\n8748\u20138763. PMLR.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brock-\nman, Christine McLeavey, and Ilya Sutskever. 2022.\nRobust speech recognition via large-scale weak su-\npervision.\nAmanpreet Singh, Ronghang Hu, Vedanuj Goswami,\nGuillaume Couairon, Wojciech Galuba, Marcus\nRohrbach, and Douwe Kiela. 2022. FLAVA: A Foun-\ndational Language And Vision Alignment Model.\nArXiv:2112.04482 [cs].\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2019.\nRepresentation learning with contrastive predictive\ncoding.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nAlex Warstadt and Samuel R Bowman. 2022. What\nartificial neural networks can tell us about human\nlanguage acquisition. In Shalom Lappin and Jean-\nPhilippe Bernardy, editors, Algebraic Structures in\nNatural Language, pages 17\u201360. CRC Press. Pub-\nlisher: CRC Press.\nAlex Warstadt, Leshem Choshen, Aaron Mueller, Adina\nWilliams, Ethan Wilcox, and Chengxu Zhuang. 2023.\nCall for papers \u2013 the babylm challenge: Sample-\nefficient pretraining on a developmentally plausible\ncorpus.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing\nLiu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,\nTaku Kudo, Hideto Kazawa, Keith Stevens, George\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals,\nGreg Corrado, Macduff Hughes, and Jeffrey Dean.\n2016. Google\u2019s neural machine translation system:\nBridging the gap between human and machine trans-\nlation.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRichard James, Jure Leskovec, Percy Liang, Mike\nLewis, Luke Zettlemoyer, and Wen-tau Yih. 2023.\nRetrieval-augmented multimodal language modeling.\n"
  }
]