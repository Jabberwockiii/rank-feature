[
  {
    "title": "OLMo: Accelerating the Science of Language Models",
    "link": "https://arxiv.org/pdf/2402.00838.pdf",
    "upvote": "72",
    "text": "OLMo : Accelerating the Science of Language Models\nDirk Groeneveld\u03b1\nIz Beltagy\u03b1\nPete Walsh\u03b1\nAkshita Bhagia\u03b1\nRodney Kinney\u03b1\nOyvind Tafjord\u03b1\nAnanya Harsh Jha\u03b1\nHamish Ivison\u03b1\u03b2\nIan Magnusson\u03b1\nYizhong Wang\u03b1\u03b2\nShane Arora\u03b1\nDavid Atkinson\u03b1\nRussell Authur\u03b1\nKhyathi Raghavi Chandu\u03b1\nArman Cohan\u03b3\u03b1\nJennifer Dumas\u03b1\nYanai Elazar\u03b1\u03b2\nYuling Gu\u03b1\nJack Hessel\u03b1\nTushar Khot\u03b1\nWilliam Merrill\u03b4\nJacob Morrison\u03b1\nNiklas Muennighoff Aakanksha Naik\u03b1\nCrystal Nam\u03b1\nMatthew E. Peters\u03b1\nValentina Pyatkin\u03b1\u03b2\nAbhilasha Ravichander\u03b1\nDustin Schwenk\u03b1\nSaurabh Shah\u03b1\nWill Smith\u03b1\nEmma Strubell\u03b1\u00b5\nNishant Subramani\u03b1\nMitchell Wortsman\u03b2\nPradeep Dasigi\u03b1\nNathan Lambert\u03b1\nKyle Richardson\u03b1\nLuke Zettlemoyer\u03b2 Jesse Dodge\u03b1\nKyle Lo\u03b1\nLuca Soldaini\u03b1\nNoah A. Smith\u03b1\u03b2\nHannaneh Hajishirzi\u03b1\u03b2\n\u03b1Allen Institute for Artificial Intelligence\n\u03b2University of Washington\n\u03b3Yale University\n\u03b4New York University\n\u00b5Carnegie Mellon University\nolmo@allenai.org\nAbstract\nLanguage models (LMs) have become ubiquitous in both NLP research and in\ncommercial product offerings. As their commercial importance has surged, the\nmost powerful models have become closed off, gated behind proprietary inter-\nfaces, with important details of their training data, architectures, and development\nundisclosed. Given the importance of these details in scientifically studying these\nmodels, including their biases and potential risks, we believe it is essential for the\nresearch community to have access to powerful, truly open LMs. To this end, this\ntechnical report details the first release of OLMo, a state-of-the-art, truly Open\nLanguage Model and its framework to build and study the science of language\nmodeling. Unlike most prior efforts that have only released model weights and\ninference code, we release OLMo and the whole framework, including training\ndata and training and evaluation code. We hope this release will empower and\nstrengthen the open research community and inspire a new wave of innovation.\nWeights\nhttps://huggingface.co/allenai/OLMo-7B\nCode\nhttps://github.com/allenai/OLMo\nData\nhttps://huggingface.co/datasets/allenai/dolma\nEvaluation\nhttps://github.com/allenai/OLMo-Eval\nAdaptation\nhttps://github.com/allenai/open-instruct\nW&B Logs\nhttps://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5\narXiv:2402.00838v3  [cs.CL]  28 Feb 2024\n1\nIntroduction\nLanguage models have been at the center of NLP technologies for many years (Rosenfeld, 2000;\nBengio et al., 2003; Mikolov et al., 2013; Peters et al., 2018; Brown et al., 2020). Recently, due\nto large-scale pretraining and human annotation for alignment, they have become commercially\nvaluable (OpenAI, 2023). However, as their commercial value has increased, the largest models\nhave become gated behind proprietary interfaces, with important details left undisclosed.\nWe believe that full access to open language models for the research community is critical to the\nscientific study of these models, their strengths and weaknesses, and their biases and risks. Accord-\ningly, we introduce OLMo, a state-of-the-art, truly open language model and framework to build,\nstudy, and advance LMs, along with the training data, training and evaluation code, intermediate\nmodel checkpoints, and training logs.\nRecent LM releases have varied in their degree of openness. For example, Mistral 8x7B provided\nmodel weights and a brief report (Jiang et al., 2024), while LLaMA came with in-depth adapta-\ntion training instructions (Touvron et al., 2023b), and Mosaic Pretrained Transformer came with\nmany details, including the dataset distribution, though not the data itself (MosaicML NLP Team,\n2023). Falcon\u2019s pretraining data was partially released (Almazrouei et al., 2023), and the most open\nmodels\u2014the Pythia suite (Biderman et al., 2023) and BLOOM (BigScience et al., 2022)\u2014released\ntraining code, model checkpoints, training data and more.\nWith OLMo, we release the whole framework from data to training to evaluation tools: multiple\ntraining checkpoints across multiple hardware types, training logs, and exact datasets used, with a\npermissive license. We are not the only team to do this; recent work from LLM360 targets similar\ngoals (Liu et al., 2023). OLMo narrows the gap from their models to state-of-the-art capabilities of\nmodels like LLaMA2. This project has benefited from lessons learned from all of these previous\nefforts with their varying degrees of openness, and we believe that a large, diverse population of open\nmodels is the best hope for scientific progress on understanding language models and engineering\nprogress on improving their utility.\nThe OLMo framework encompasses the tools and resources required for building and researching\nlanguage models. For training and modeling, it includes full model weights, training code, training\nlogs, ablations, training metrics in the form of Weights & Biases logs, and inference code. This\nfirst release includes four variants of our language model at the 7B scale corresponding to different\narchitectures, optimizers, and training hardware, and one model at the 1B scale, all trained on at\nleast 2T tokens. We are also releasing hundreds of intermediate checkpoints available as revisions\non HuggingFace. For dataset building and analysis, it includes the full training data used for these\nmodels, including code that produces the training data, from AI2\u2019s Dolma (Soldaini et al., 2024),\nand WIMBD (Elazar et al., 2023) for analyzing pretraining data. For evaluation, it includes AI2\u2019s\nCatwalk (Groeneveld et al., 2023) for downstream evaluation and Paloma (Magnusson et al., 2023)\nfor perplexity-based evaluation. For instruction-tuning, we released Open Instruct (Ivison et al.,\n2023; Wang et al., 2023), and we are currently using it to produce an adapted (instruction-tuned and\nRLHFed) version of OLMo, which we will release soon. Finally, all code and weights are released\nunder the Apache 2.0 License.1\nThis is the first step in a long series of planned releases, continuing with larger models, instruction-\ntuned models, and more modalities and variants down the line. We therefore hope to catalyze re-\nsearch into as-yet poorly understood aspects of these models, for example, the relationship between\npretraining data and model capabilities, the impact of design and hyperparameter choices, and vari-\nous optimization methods and their impact on model training. In addition, we report on the lessons\nlearned and important details necessary to successfully train language models at this scale.\n2\nOLMo Framework\nThis section describes the OLMo framework, consisting of the OLMo models (Section 2.1), our\npre-training dataset, Dolma (Section 2.2), and our evaluation framework (Section 2.4).\n1http://www.apache.org/licenses/LICENSE-2.0\n2\n2.1\nOLMo Model and Architecture\nWe adopt a decoder-only transformer architecture based on Vaswani et al. (2017), and deliver 1B\nand 7B variants as described in Table 1, with a 65B version coming soon. Our specific architecture\nincludes several improvements over the vanilla transformer from Vaswani et al. (2017) following\nother recent large language models like PaLM (Chowdhery et al., 2022), the LLaMA family (Tou-\nvron et al., 2023a,b), OpenLM (Gururangan et al., 2023), and Falcon (Almazrouei et al., 2023).\nTable 2 gives a comprehensive comparison of our 7B architecture to the similarly-sized models\nfrom these other families.\nSize\nLayers\nHidden Size\nAttention Heads\nTokens Trained\n1B\n16\n2048\n16\n2T\n7B\n32\n4086\n32\n2.46T\n65B*\n80\n8192\n64\nTable 1: OLMo model sizes and the maximum number of tokens trained to.\n* At the time of writing our 65B model is still training.\nWe generally select hyperparameters by optimizing for training throughput on our hardware while\nminimizing the risk of loss spikes and slow divergence. We ablate choices through our in-loop\nevaluation setting, given available computational sources (Section 2.4). Table 2 compares our de-\nsign choices with recent state-of-the-art open language models. Our main changes over the vanilla\ntransformer architecture can be summarized as follows:\n1. No biases. Following LLaMA, PaLM, and others, we exclude all bias terms from our\narchitecture in order to improve training stability.\n2. Non-parametric layer norm. We use the non-parametric formulation of layer norm (Ba\net al., 2016) in which there is no affine transformation within the norm, i.e. no \u201cadaptive\ngain\u201d (or bias). We believe this was the safest option and it was also the fastest compared\nto the other variants we considered: parametric layer norm and RMSNorm (Zhang and\nSennrich, 2019).\n3. SwiGLU activation function. Like LLaMA, PaLM, and others we use the SwiGLU ac-\ntivation function (Shazeer, 2020) instead of ReLU, and following LLaMA the activation\nhidden size is approximately 8\n3d, but increased to the closest multiple of 128 (e.g. 11,008\nfor our 7B model) to improve throughput.2\n4. Rotary positional embeddings (RoPE). Like LLaMA, PaLM, and others we replace ab-\nsolute positional embeddings with rotary positional embeddings (RoPE; Su et al., 2021).\n5. Vocabulary. We use a modified version of the BPE-based tokenizer from GPT-NeoX-20B\n(Black et al., 2022) with additional tokens for masking personal identifiable information\n(PII). The final vocabulary size is 50,280. However, to maximize training throughput we\nincrease the size of the corresponding embedding matrix in our model to 50,304 so that it\u2019s\na multiple of 128.\n2.2\nPretraining Data: Dolma\nDespite progress in access to model parameters, pretraining datasets are still not as open. Pretraining\ndata are often not released alongside open models (let alone closed models) and documentation\nabout such data is often lacking in detail that would be needed to reproduce or fully understand\nthe work. This has made it difficult to support certain threads of language model research, such\nas understanding how training data impacts model capabilities and limitations. To facilitate open\nresearch on language model pretraining, we built and released our pretraining dataset, Dolma\u2014\na diverse, multi-source corpus of 3T tokens across 5B documents acquired from 7 different data\n2Since SwiGLU is a \u201cgated\u201d activation function, the output is half the size of the input. So technically our\ninputs to SwiGLU have a dimensionality of 2 \u00d7 11,008 = 22,016 for our 7B model.\n3\nOLMo-7B\nLLaMA2-7B\nOpenLM-7B\nFalcon-7B\nPaLM-8B\nDimension\n4096\n4096\n4096\n4544\n4096\nNum heads\n32\n32\n32\n71\n16\nNum layers\n32\n32\n32\n32\n32\nMLP ratio\n\u223c8/3\n\u223c8/3\n\u223c8/3\n4\n4\nLayer norm type\nnon-parametric\nRMSNorm\nparametric\nparametric\nparametric\nPositional embeddings\nRoPE\nRoPE\nRoPE\nRoPE\nRoPE\nAttention variant\nfull\nGQA\nfull\nMQA\nMQA\nBiases\nnone\nnone\nin LN only\nin LN only\nnone\nBlock type\nsequential\nsequential\nsequential\nparallel\nparallel\nActivation\nSwiGLU\nSwiGLU\nSwiGLU\nGeLU\nSwiGLU\nSequence length\n2048\n4096\n2048\n2048\n2048\nBatch size (instances)\n2160\n1024\n2048\n2304\n512\nBatch size (tokens)\n\u223c4M\n\u223c4M\n\u223c4M\n\u223c4M\n\u223c1M\nWeight tying\nno\nno\nno\nno\nyes\nTable 2: LM architecture comparison at the 7\u20138B scale. In the \u201clayer norm type\u201d row, \u201cparametric\u201d\nand \u201cnon-parametric\u201d refer to the usual layer norm implementation with and without adaptive gain\nand bias, respectively.\nSource\nDoc Type\nUTF-8\nbytes\n(GB)\nDocuments\n(millions)\nGPT-NeoX\ntokens\n(billions)\nCommon Crawl\nweb pages\n9,022\n3,370\n2,006\nThe Stack\ncode\n1,043\n210\n342\nC4\nweb pages\n790\n364\n174\nReddit\nsocial media\n339\n377\n80\npeS2o\nSTEM papers\n268\n38.8\n57\nProject Gutenberg\nbooks\n20.4\n0.056\n5.2\nWikipedia, Wikibooks\nencyclopedic\n16.2\n6.2\n3.7\nTotal\n11,519\n4,367\n2,668\nTable 3: Composition of Dolma.\nsources that are (1) commonly seen in large-scale language model pretraining and (2) accessible to\nthe general public (Soldaini et al., 2024). Table 3 provides a high-level overview of the amount of\ndata from each source.\nDolma is built using a pipeline of (1) language filtering, (2) quality filtering, (3) content filtering,\n(4) deduplication, (5) multi-source mixing, and (6) tokenization. We refer the reader to the Dolma\nreport (Soldaini et al., 2024) for more details about its design principles, details about its construc-\ntion, and a more detailed summary of its contents. The report provides additional analyses and\nexperimental results from training language models on intermediate states of Dolma to share what\nwe learned about important data curation practices, including the role of content or quality filters,\ndeduplication, and mixing data from multiple sources. We keep documents from each source sep-\narate, both during curation as well as in the final release. We open-sourced our high-performance\ndata curation tools; this toolkit can be used to further experiment on Dolma, reproduce our work,\nand enable fast and easy curation of pretraining corpora. Finally, we also open-sourced our WIMBD\ntool (Elazar et al., 2023) to help with dataset analysis.\n4\n2.3\nAdaptation\nPretrained models are not always used as-is, but rather further fine-tuned to improve their perfor-\nmance, safety, and usability. Often models are first trained to follow instructions (Mishra et al.,\n2022; Wei et al., 2022; Sanh et al., 2022), and then further trained on human preferences (Ouyang\net al., 2022) to improve the quality of their generations. We showcase the efficacy of using OLMo\nas a base model for further fine-tuning by training OLMo to be a general chat assistant following\nour Open Instruct (T \u00a8ULU) data and training setup (Ivison et al., 2023). Our approach involves first\nperforming instruction fine-tuning with a mixture of distilled and human-written instruction data and\nthen further aligning the model with distilled preference data using Direct Preference Optimization\n(DPO) (Rafailov et al., 2023). We experimented with mixing the Tulu instruction data at the end of\npretraining, as done in recent models such as DeepSeek-AI et al. (2024), but did not have conclusive\nfindings.\n2.4\nEvaluation\nWe perform base model evaluation at two stages: online evaluation to make decisions for model\ndesign and offline evaluation to evaluate model checkpoints. For the offline stage, we use the Catwalk\nframework (Groeneveld et al., 2023), a publicly available evaluation tool with access to a wide range\nof datasets and task formats. Using Catwalk, we perform downstream evaluation as well as intrinsic\nlanguage modeling evaluation on the new perplexity benchmark, Paloma (Magnusson et al., 2023).\nFor both downstream and perplexity evaluation, we use our fixed evaluation pipeline to compare\nresults against publicly available models. We also report a separate evaluation of our adapted model.\nIn-Loop Training Ablations\nThroughout model training, we perform downstream evaluations to\nmake decisions around model architecture, initialization, optimizers, learning rate schedule, and\ndata mixtures. We call this our online evaluation as it runs in-loop every 1000 training steps (or\n\u223c4B training tokens) and provides an early and continuous signal on the quality of the model being\ntrained. These evaluations rely on many of the core tasks and experiment settings used for our\noffline evaluation detailed in Section 4.1, which also mirrors the task and evaluation structure of the\nEleutherAI eval harness (Gao et al., 2023).\nDownstream Evaluation\nFollowing much previous work (Brown et al., 2020; Black et al., 2022;\nTouvron et al., 2023a,b, inter alia), we report zero-shot performance on a set of downstream tasks.\nOur evaluation suite consists of 8 core tasks corresponding closely to the commonsense reasoning\ntask set reported by Touvron et al. (2023a) and Touvron et al. (2023b) (see Table 6 for a list of tasks).\nGiven the scale of the models being evaluated, such tasks were selected at the beginning of model\ndevelopment due to their naturalness (e.g., all can formulated as text completion scoring tasks) and\nability to provide meaningful signals throughout training (see Figure 1).\nIntrinsic Language Modeling Evaluation\nTo measure how OLMo-7B fits distributions of lan-\nguage beyond held-out training data, we use Paloma (Magnusson et al., 2023), a new perplex-\nity benchmark that includes 585 different domains of text. Domains range from nytimes.com to\nr/depression on Reddit and are drawn from 18 separate data sources, such as C4 (Raffel et al., 2020),\nin stratified samples. This allows for more equal inclusion of text domains that are under-represented\nin their source corpora.\nWe aim not just to compare OLMo-7B against other models for best performance, but also to demon-\nstrate how it enables fuller and more controlled scientific evaluations. OLMo-7B is the largest\nLM with explicit decontamination for perplexity evaluation. Following the approach described\nin Paloma, we remove any pretraining document with paragraphs leaked from Paloma evaluation\ndata. Without decontamination, other models risk underestimating perplexity (i.e., overestimating\nthe model\u2019s out-of-sample fit). We also release intermediate checkpoints, allowing richer com-\nparisons with two other models that release checkpoints, Pythia-6.9B (Biderman et al., 2023) and\nRPJ-INCITE-7B (Together Computer, 2023) (see Figure 2).\nAdaptation Evaluation\nWe also follow our Open Instruct evaluation suite Wang et al. (2023);\nIvison et al. (2023) to evaluate OLMo after instruction fine-tuning and DPO training using our We\n5\nfocus on evaluations around model chat capabilities and safety to showcase the efficacy of using\nOLMo as a base for further fine-tuning.\n3\nTraining OLMo\nThis section describes our pretraining setup, including our distributed training framework (Sec-\ntion 3.1), optimizer settings (Section 3.2), data preparation (Section 3.3), and hardware (Section 3.4).\n3.1\nDistributed Training Framework\nWe train our models using the ZeRO optimizer strategy (Rajbhandari et al., 2019) via PyTorch\u2019s\nFSDP framework (Zhao et al., 2023), which reduces memory consumption by sharding the model\nweights and their corresponding optimizer state across GPUs. At the 7B scale, this enables training\nwith a micro-batch size of 4096 tokens per GPU on our hardware (see Section 3.4). For OLMo-1B\nand -7B models, we use a constant global batch size of approximately 4M tokens (2048 instances,\neach with a sequence length of 2048 tokens). For OLMo-65B model (currently training), we use\na batch size warmup that starts at approximately 2M tokens (1024 instances), then doubles every\n100B tokens until reaching approximately 16M tokens (8192 instances).\nTo improve throughput, we employ mixed-precision training (Micikevicius et al., 2017) through\nFSDP\u2019s built-in settings and PyTorch\u2019s amp module. The latter ensures that certain operations like\nthe softmax always run in full precision to improve stability, while all other operations run in half-\nprecision with the bfloat16 format. Under our specific settings, the sharded model weights and\noptimizer state local to each GPU are kept in full precision. The weights within each transformer\nblock are only cast to bfloat16 when the full-sized parameters are materialized on each GPU\nduring the forward and backward passes. Gradients are reduced across GPUs in full precision.\n3.2\nOptimizer\nWe use the AdamW optimizer (Loshchilov and Hutter, 2019) with the hyperparameters shown in\nTable 4. For all model sizes, we warm up the learning rate over 5000 steps (\u223c21B tokens) and then\ndecay it linearly from there down to a tenth of the peak learning rate over the remainder of training.\nAfter the warm-up period, we clip gradients such that the total l2-norm of the parameter gradients3\ndoes not exceed 1.0. Table 5 gives a comparison of our optimizer settings at the 7B scale to those of\nother recent LMs that also used AdamW.\n3.3\nData\nWe built our training dataset out of a 2T-token sample from our open dataset, Dolma (Soldaini et al.,\n2024), which we describe in Section 2.2. The tokens from every document are concatenated together\nafter appending a special EOS token to the end of each document, and then we group consecutive\nchunks of 2048 tokens to form training instances. The training instances are shuffled in the exact\nsame way for each training run. The data order and exact composition of each training batch can be\nreconstructed from the artifacts we release.\nAll of our released models have been trained to at least 2T tokens (a single epoch over our training\ndata), and some have been trained beyond that by starting a second epoch over the data with a\ndifferent shuffling order. The impact of repeating this small amount of data should be negligible\naccording to prior work (Muennighoff et al., 2023).\n3.4\nHardware\nIn order to verify that our codebase could be used on both NVIDIA and AMD GPUs without any\nloss in performance, we trained models on two different clusters:\n3During gradient clipping all of the model\u2019s parameters are treated as a single big vector (as if all parameters\nwere flattened and concatenated together), and we take the \u21132-norm over the corresponding single gradient\nvector. This is the standard way to clip gradients in PyTorch.\n6\nSize\nPeak LR\nBetas\nEpsilon\nWeight Decay\nBatch Size (tokens)\n1B\n4.0E-4\n(0.9, 0.95)\n1.0E-5\n0.1\n\u223c4M\n7B\n3.0E-4\n(0.9, 0.95)\n1.0E-5\n0.1\n\u223c4M\n65B*\n1.5E-4\n(0.9, 0.95)\n1.0E-5\n0.1\n\u223c2M \u2192 \u223c4M \u2192 \u223c8M \u2192 \u223c16M\nTable 4: AdamW pretraining hyperparameters for OLMo models.\n* At the time of writing our 65B model is still training.\nOLMo-7B\nLLaMA2-7B\nOpenLM-7B\nFalcon-7B\nwarmup steps\n5000\n2000\n2000\n1000\npeak LR\n3.0E-04\n3.0E-04\n3.0E-04\n6.0E-04\nminimum LR\n3.0E-05\n3.0E-05\n3.0E-05\n1.2E-05\nweight decay\n0.1\n0.1\n0.1\n0.1\nbeta1\n0.9\n0.9\n0.9\n0.99\nbeta2\n0.95\n0.95\n0.95\n0.999\nepsilon\n1.0E-05\n1.0E-05\n1.0E-05\n1.0E-05\nLR schedule\nlinear\ncosine\ncosine\ncosine\ngradient clipping\nglobal 1.0\nglobal 1.0\nglobal 1.0\nglobal 1.0\ngradient reduce dtype\nFP32\nFP32\nFP32\nBF16\noptimizer state dtype\nFP32\nmost likely FP32\nFP32\nFP32\nTable 5: Comparison of pretraining optimizer settings at the 7B scale. Each model in this table used\nAdamW as its optimizer.\n\u2022 LUMI: Provided by the LUMI supercomputer,4 we used up to 256 nodes on this clus-\nter, where each node consists of 4x AMD MI250X GPUs with 128GB of memory5 and\n800Gbps of interconnect.\n\u2022 MosaicML: Provided by MosaicML6 (Databricks), we used 27 nodes on this cluster, where\neach node consists of 8x NVIDIA A100 GPUs with 40GB of memory and 800Gbps inter-\nconnect.\nDespite minor differences in batch size to optimize for training throughput, both runs resulted in\nnearly identical performance on our evaluation suite by 2T tokens.\n4\nResults\nThe checkpoint used for evaluating OLMo-7B is trained until 2.46T tokens on the Dolma (Soldaini\net al., 2024) dataset with a linear learning rate decay schedule mentioned in Section 3.2. In our\nexperiments, we find that tuning this checkpoint further on the Dolma dataset for 1000 steps with the\nlearning rate linearly decayed to 0 boosts model performance on perplexity and end-task evaluation\nsuites described in Section 2.4. We compare OLMo with other publicly available models including\nLLaMA-7B (Touvron et al., 2023a), LLaMA2-7B (Touvron et al., 2023b), MPT-7B (MosaicML\nNLP Team, 2023), Pythia-6.9B (Biderman et al., 2023), Falcon-7B (Almazrouei et al., 2023) and\nRPJ-INCITE-7B (Together Computer, 2023).\n4.1\nDownstream evaluation\nSetup\nOur core downstream evaluation suite (see Table 6) consists of: arc (both arc easy and\narc challenge) (Clark et al., 2018), boolq (Clark et al., 2019), openbookqa (Mihaylov et al.,\n4https://www.lumi-supercomputer.eu\n5The MI250X is a dual-chip module, meaning in practice that each physical device consists of two logical\ndevices, so each node has 8 logical GPU devices with 64GB of memory each.\n6https://www.mosaicml.com\n7\n7B Models\narc\nchallenge\narc\neasy\nboolq\nhella-\nswag\nopen\nbookqa\npiqa\nsciq\nwino-\ngrande\navg.\nFalcon\n47.5\n70.4\n74.6\n75.9\n53.0\n78.5\n93.9\n68.9\n70.3\nLLaMA\n44.5\n67.9\n75.4\n76.2\n51.2\n77.2\n93.9\n70.5\n69.6\nLlama 2\n48.5\n69.5\n80.2\n76.8\n48.4\n76.7\n94.5\n69.4\n70.5\nMPT\n46.5\n70.5\n74.2\n77.6\n48.6\n77.3\n93.7\n69.9\n69.8\nPythia\n44.1\n61.9\n61.1\n63.8\n45.0\n75.1\n91.1\n62.0\n63.0\nRPJ-INCITE\n42.8\n68.4\n68.6\n70.3\n49.4\n76.0\n92.9\n64.7\n66.6\nOLMo-7B\n48.5\n65.4\n73.4\n76.4\n50.4\n78.4\n93.8\n67.9\n69.3\nTable 6: Zero-shot evaluation of OLMo-7B and 6 other publicly available comparable model check-\npoints on 8 core tasks from the downstream evaluation suite described in Section 2.4. For OLMo-7B,\nwe report results for the 2.46T token checkpoint.\n2018), sciq (Welbl et al., 2017), hellaswag (Zellers et al., 2019), piqa (Bisk et al., 2020), and\nwinogrande (Sakaguchi et al., 2021). In Appendix A, we also report results on an additional set\nof auxiliary tasks outside of our core evaluation set that we found to have less stable performance\ntrends (see Figure 4).\nIn all cases, we perform zero-shot evaluation using the rank classification approach popularized by\nBrown et al. (2020). Under this approach, candidate text completions (e.g., different multiple-choice\noptions) are ranked by likelihood (usually normalized by some normalization factor), and prediction\naccuracy is reported. While Catwalk implements several common likelihood normalization strate-\ngies, including normalizing by number of tokens (per-token normalization) (Brown et al., 2020;\nLiang et al., 2022), by number of characters (per-character normalization) (Gao et al., 2023), as well\nas incorporating an answer\u2019s unconditional likelihood (Brown et al., 2020), we selected the nor-\nmalization strategies for each dataset separately. Specifically, we used unconditional normalization\nfor arc and openbookqa, per-token normalization for hellaswag, piqa, and winogrande and no\nnormalization for boolq, and sciq (i.e., tasks formulated as single token prediction tasks).\nResults\nTable 6 summarizes the result of zero-shot evaluation of OLMo-7B and compares it\nagainst 6 other publicly available models of comparable size. We report results on 8 core tasks\nfrom our evaluation suite described in Section 2.4. On aggregate, OLMo-7B is competitive against\nall 6 publicly available model checkpoints in our comparison table.\nIn Figure 1 we plot the accuracy score progression of 8 core end-tasks. All tasks, except OBQA,\nshow an upward trend in accuracy numbers as OLMo-7B is trained on more tokens. A sharp upward\ntick in accuracy of many tasks between the last and the second to last step shows us the benefit of\nlinearly reducing the LR to 0 over the final 1000 training steps. See Table 9 in Appendix A for\nadditional evaluation results and discussion.\n4.2\nIntrinsic language modeling evaluation\nSetup\nFor intrinsic evaluations, Paloma proposes a range of analyses, from inspection of perfor-\nmance in each domain separately to more summarized results over combinations of domains. We\nreport results at two levels of granularity: the aggregate performance over 11 of the 18 sources\nin Paloma as in Magnusson et al. (2023), as well as more fine-grained results over each of these\nsources individually. This particular subset of 11 sources from Paloma excludes sources that are not\npublicly available, involve fringe or toxic text, or consist of code data not supported by Paloma\u2019s de-\ncontamination approach. This leaves C4 (Raffel et al., 2020), mC4-en (Chung et al., 2023), Wikitext\n103 (Merity et al., 2016), Penn Treebank (Marcus et al., 1999; Nunes, 2020), RedPajama (Together\nComputer, 2023), Falcon-RefinedWeb (Penedo et al., 2023), Dolma (Soldaini et al., 2024), M2D2\nS2ORC (Reid et al., 2022), M2D2 Wikipedia (Reid et al., 2022), C4 100 domains (Chronopoulou\net al., 2022), and Dolma 100 Subreddits (Soldaini et al., 2024). To allow for a fair comparison be-\ntween models with different vocabularies, we report bits per byte as defined by Gao et al. (2020)\nover the test sets of these sources.\nResults\nIn the Sources Combined subplot of Figure 2, we show the performance of OLMo-7B\nagainst 6 comparably-sized language models on the combination of 11 data sources from Paloma.\n8\n500\n1000\n1500\n2000\n2500\n40\n44\n48\narc_c\n500\n1000\n1500\n2000\n2500\n60\n64\n68\narc_e\n500\n1000\n1500\n2000\n2500\n56\n64\n72\nboolq\n500\n1000\n1500\n2000\n2500\n68\n72\n76\nhellaswag\n500\n1000\n1500\n2000\n2500\n45\n48\n51\nobqa\n500\n1000\n1500\n2000\n2500\n76\n78\npiqa\n500\n1000\n1500\n2000\n2500\n90\n92\n94\nsciq\n500\n1000\n1500\n2000\n2500\n63\n66\nwinogrande\nTokens Seen (billions)\nAccuracy\nFigure 1: Accuracy score progression of OLMo-7B on 8 core end-tasks score from Catwalk eval-\nuation suite described in Section 2.4. We can see the benefit of decaying LR to 0 in the final 1000\nsteps of training on most tasks.\nOverall we find OLMo to have a competitive fit, especially given its training data was explicitly\ndecontaminated against Paloma. As seen through the comparison of final models (see shapes) as\nwell intermediate checkpoints (see dashed lines), the OLMo results follow similar scaling trends of\nother models. Note that the performance of intermediate checkpoints is influenced by where that\ncheckpoint occurs in the learning rate schedule. So models trained for fewer steps will tend to have\nsteeper training curves without necessarily being more sample efficient if training duration were\nfixed across all models. MPT-7B, nevertheless, stands out as improving ahead of the other models\nin this subplot. This could be due to a number of factors, including pretraining data composition and\nits match to the domains in Paloma (e.g., MPT trains on 27% non-Common Crawl data rather than\n18% for LLaMA, 12.2% for RedPajama, and 11.2% for OLMo) as well as various data preprocessing\ndecisions (e.g., MPT\u2019s use of semantic deduplication by Abbas et al., 2023, on C4).\nThe remaining subplots in Figure 2 provide more fine-grained analysis by reporting bits per byte\nseparately for each of the 11 data sources that are combined in the aggregated Paloma metric. From\nthis we see greater variation in sample efficiency, largely driven by the similarity of training and\nevaluation distributions. Notably, OLMo-7B fares well on evaluations predominated by Common\nCrawl, such as C4, though different ways of postprocessing Common Crawl are best fit by models\ntrained with that specific data, such as Falcon-7B on Falcon RefinedWeb. Meanwhile, OLMo-7B is\nless sample efficient compared to other models on sources less related to scraped web text, such as\nWikiText-103, M2D2 S2ORC, and M2D2 Wikipedia. The RedPajama evaluation shows a similar\npattern, perhaps as only 2 of its 7 domains are from Common Crawl, and Paloma weights domains\nwithin each source equally. Since heterogeneous data from curated sources like Wikipedia and\nArXiv papers is much less abundant than scraped web text, maintaining sample efficiency for fit to\nthese distributions of language will be challenging as pretraining corpora are scaled.\n4.3\nAdaptation Evaluation\nSetup\nWe evaluate OLMo before adaptation, and after both the supervised fine-tuning and DPO\ntraining stage, focusing on the safety and chat evaluations used by Wang et al. (2023). We addition-\nally compare to officially released instruction-tuned variants of the models from Table 6. We finally\nalso compare to T \u00a8ULU 2 models to compare against models trained using the same post-training\ndata mixes and procedures.\n9\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nSources Combined\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nC4\n10\n100\n1000\n10000\n0.61\n0.74\n0.90\n1.11\nmC4\n10\n100\n1000\n10000\n0.55\n0.82\n1.22\nWikiText-103\n10\n100\n1000\n10000\n0.74\n0.90\n1.11\n1.35\nPTB\n10\n100\n1000\n10000\n0.55\n0.82\n1.22\nRedPajama\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nFalcon RefinedWeb\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nDolma V1.5\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nM2D2 S2ORC\nBaselines\nFalcon-7B\nLLaMA2-7B\nMPT-7B\nLLaMA-7B\nPythia-6.9B\nRPJ-INCITE-7B\nOLMo-7B\n10\n100\n1000\n10000\n0.61\n0.74\n0.90\n1.11\nM2D2 Wikipedia\n10\n100\n1000\n10000\n0.67\n0.82\n1.00\nC4 100 Domains\n10\n100\n1000\n10000\n0.82\n1.00\n1.22\n100 Subreddits\nBits Per Byte\nTokens Seen (billions)\nFigure 2:\nBits per byte on 11 evaluation data sources from Paloma and their combination (Mag-\nnusson et al., 2023), decontaminated from OLMo\u2019s pretraining data. While models follow a general\ndata scaling trend, sample efficiency is most favorable on in-distribution data. For example, OLMo-\n7B overtakes all other models on C4, perhaps from having 88.8% Common Crawl pretraining data.\nModel\nMMLU\nAlpacaEval\nToxiGen\nTruthfulQA\n0-shot \u2191\n%win \u2191\n% Toxic \u2193\n%Info+True \u2191\nOLMo (base)\n28.3\n-\n81.4\n31.6\nMPT Chat\n33.8\n46.8\n0.1\n42.7\nFalcon Instruct\n25.2\n14.0\n70.7\n27.2\nRPJ-INCITE Chat\n27.0\n38.0\n46.4\n53.0\nLlama-2-Chat\n46.8\n87.3\n0.0\n26.3\nT \u00a8ULU 2\n50.4\n73.9\n7.0\n51.7\nT \u00a8ULU 2+DPO\n50.7\n85.1\n0.5\n- *\nOLMo +SFT\n47.3\n57.0\n14.4\n41.2\nOLMo +SFT+DPO\n46.2\n69.3\n1.7\n52.0\nTable 7: Evaluation of various instruction-tuned 7B models, including OLMo-7B and before and\nafter adaptation training. Lower is better for ToxiGen and higher is better for other metrics. We\nprovide a detailed description of models and metrics in Appendix. C. * Following Ivison et al.\n(2023), we do not report T \u00a8ULU 2 TruthfulQA scores due to test set contamination.\nResults\nWe find that instruction tuning considerably improves the performance and safety of\nOLMo, increasing MMLU performance by a wide margin and improving ToxiGen and TruthfulQA\nscores - especially after DPO training. Additionally, we find that OLMo outperforms most other\nchat variants after both initial instruction tuning (OLMo +SFT) and additional preference alignment\n(OLMo +SFT+DPO), highlighting both the strength of OLMo as a base model and the strength of\nthe T \u00a8ULU mix used to perform adaptation training. However, we find there is still a gap with T \u00a8ULU\n2, which is trained by applying the T \u00a8ULU mix on Llama 2. This gap may be due to test set contam-\n10\nination in Llama 27 and because the T \u00a8ULU mix was primarily designed for Llama models - we will\ninvestigate the cause of this gap in future work. Overall, we see that OLMo greatly benefits from\nadditional tuning and serves as a strong base model for downstream applications.\n4.4\nPower Consumption and Carbon Footprint\nFollowing previous literature (Strubell et al., 2019; Patterson et al., 2021; Wu et al., 2022; Dodge\net al., 2022), we estimate the total energy consumed and carbon released while pretraining our\nmodels by calculating the total power consumption required for training, and then multiplying it\nby the carbon emission intensity of the power grid where the model was trained. While reporting\nthese operational emissions is standard practice, it does not account for other sources of emissions\nsuch as the embodied emissions due to the manufacturing, transportation and disposal of hardware\nand datacenter infrastructure, lifetime operational emissions due to use, rebound effects, or other\nenvironmental impacts such as water consumption or mining. Thus our estimates should be viewed\nas lower bounds.\nWe calculate the total power consumption for our models by measuring the power consumption of a\nsingle node every 25ms, calculating an average across the entire training run, and multiplying by the\ntotal number of nodes. We then account for the energy efficiency of the data center by multiplying\nthe previous total by a power usage effectiveness (PUE) factor, which we set to 1.1, representing\na conservative 10% energy consumption overhead typical of energy efficient datacenters.89 We\nestimate that pretraining our 7B models consumed 239 MWh of energy.\nTo calculate carbon emissions, we multiply the total power consumption by a carbon intensity fac-\ntor, measured in kg CO2 emitted per KWh, based on the physical location of the data center where\neach model was trained. The model trained on A100-40GB GPUs was trained in Australia, so we\nassume a carbon intensity factor of 0.610, the national average for Australia in 2022.10 The model\ntrained on MI250X GPUs was trained in the LUMI supercomputer, which runs on 100% renewable,\ncarbon-neutral energy, so we assume a carbon intensity factor of 0. LUMI is powered entirely by\nhydroelectric power and some sources (Ubierna et al., 2022) measure the carbon intensity factor of\nhydroelectric power to be 0.024, which would imply total carbon emissions of 3.54 tCO2eq.11 How-\never, we rely on the official LUMI data for our calculations, and thus we estimate total pretraining\nemissions of 69.78 tCO2eq.12 In Table 8 we compare our models with other previously released\nmodels based on publicly available information.\nWe hope that openly releasing our models can reduce future emissions by allowing others to avoid\nthe need to pretrain models from scratch, and give insights into the true cost of developing state of\nthe art models. We also highlight that our estimates are lower bounds, because they do not include\nother critical pieces of development such as debugging, hyperparameter tuning, and downtime.\n5\nArtifacts Released\nBy sharing artifacts from all pipeline stages, we aim to encourage open research and reduce dupli-\ncated, often costly efforts, by academics and practitioners. We release the following:\n1. The training and modeling code.13\n7Touvron et al. (2023b) report that Llama 2 was pretrained on data contaminated with MMLU test data.\n8https://www.nrel.gov/computational-science/measuring-efficiency-pue.html\n9https://www.google.com/about/datacenters/efficiency/\n10https://www.cleanenergyregulator.gov.au/Infohub/Markets/Pages/qcmr/\ndecember-quarter-2022/Emissions-Reduction.aspx\n11https://www.lumi-supercomputer.eu\n12These metrics were in part collected using Carbonara\u2019s AI agent and monitoring platform. Learn more at:\nhttps://trycarbonara.com\n13https://github.com/allenai/OLMo\n11\nGPU Type\nGPU Power\nConsumption\n(MWh)\nPower\nUsage\nEffectiveness\nCarbon\nIntensity\n(kg CO2e/KWh)\nCarbon\nEmissions\n(tCO2eq)\nGopher-280B\nTPU v3\n1,066\n1.08\n0.330\n380\nBLOOM-176B\nA100-80GB\n433\n1.2\n0.057\n30\nOPT-175B\nA100-80GB\n324\n1.1\n0.231\n82\nT5-11B\nTPU v3\n77\n1.12\n0.545\n47\nLLaMA-7B\nA100-80GB\n33\n1.1\n0.385\n14\nLLaMA2-7B\nA100-80GB\n74\n1.1\n0.385\n31\nOLMo-7B\nMI250X\n135\n1.1\n0.000*\n0*\nOLMo-7B\nA100-40GB\n104\n1.1\n0.610\n70\nTable 8: CO2 emissions during pretraining. We estimate the total carbon emissions for various\nmodels using publicly available data on PUE, carbon intensity of local power grid, and reported\npower consumption. Numbers for Gopher-280B (Rae et al., 2022), BLOOM-176B (Luccioni et al.,\n2022), OPT-175B (Zhang et al., 2022), T5-11B (Patterson et al., 2021), LLaMA (Touvron et al.,\n2023a), and LLaMA2 (Touvron et al., 2023b) are taken from their respective papers. See Section\n4.4 for details on how tCO2eq was calculated.\n* LUMI runs entirely on hydroelectric power12and some estimates (Ubierna et al., 2022) measure\nthe intensity factor of hydroelectric power to be 0.024, implying total emissions of 3.54 tCO2eq.\n2. The trained model weights for the 7B model,14 7B-twin-2T,15 and the 1B model.16 For\nall the models, we release not only the final model weights but also 500+ intermediate\ncheckpoints at intervals of 1000 steps.\n3. Adapted OLMo-7B with instruction-tuning, 7B-SFT17, and RLHF, 7B-Instruct18 including\nits training and evaluation code and data using our Open Instruct19 library (Wang et al.,\n2023; Ivison et al., 2023).\n4. The training data Dolma (Soldaini et al., 2024).20\n5. Dolma\u2019s toolkit to construct new datasets,21 and WIMBD (Elazar et al., 2023) for dataset\nanalysis.22\n6. The evaluation code23 using Catwalk24 for downstream evaluation (Groeneveld et al., 2023)\nand Paloma25 for perplexity-based evaluation (Magnusson et al., 2023).\n7. The complete set of metrics logged to Weights & Biases during training.26\nWe intend to follow up on this release with further training logs, ablations, and findings.\n14https://huggingface.co/allenai/OLMo-7B\n15https://huggingface.co/allenai/OLMo-7B-Twin-2T\n16https://huggingface.co/allenai/OLMo-1B\n17https://huggingface.co/allenai/OLMo-7B-SFT\n18https://huggingface.co/allenai/OLMo-7B-Instruct\n19https://github.com/allenai/open-instruct\n20https://huggingface.co/datasets/allenai/dolma\n21https://github.com/allenai/dolma\n22https://github.com/allenai/wimbd\n23https://github.com/allenai/OLMo-Eval\n24https://github.com/allenai/catwalk\n25https://paloma.allen.ai\n26https://wandb.ai/ai2-llm/OLMo-7B/reports/OLMo-7B--Vmlldzo2NzQyMzk5\n12\n6\nLicense\nOur goal is to facilitate scientific development and empower the scientific community, so we favor\npermissive licenses that give users flexibility in using our resources and artifacts. As such, all code\nand weights are released under the Apache 2.0 License.27 Some licenses used by other organizations\nfor recent model releases prohibit using the outputs from their models to train artificial intelligence or\nmachine learning systems, while we expressly allow users to do so. We also do not limit commercial\nuse. We hope that our models can make other models better. We recognize that the risk for misuse\nof our models is relatively low since they are mainly designed as scientific artifacts not as products\nwith broad public adoption (our models have not been adapted as chatbots). In addition, over the\npast year there have been a number of comparable models released with very permissive licenses,\nso using a more strict license for our work will not remove the overall risk in the field. We believe\nthis tradeoff on the side of being more open is the best option.\n7\nConclusion and Future Work\nThis technical report presents our first release of OLMo, a state-of-the-art, truly open language\nmodel and its framework to build and study the science of language modeling. Unlike most prior\nefforts that have only released model weights and inference code, we release OLMo and the whole\nframework, including training data and training and evaluation code. Soon, we will also release\ntraining logs, ablations, findings and Weights & Biases logs. We are also exploring the adaptation\nof OLMo with instruction tuning and different flavors of RLHF. We are going to release the adapted\nmodels as well as all of our model adaptation code and data.\nWe intend to continuously support and extend OLMo and its framework, and continue to push the\nboundaries of open LMs to empower the open research community. To that end, we look forward to\nbringing different model sizes, modalities, datasets, safety measures, and evaluations into the OLMo\nfamily. We hope this and future releases will empower and strengthen the open research community\nand inspire a new wave of innovation.\nAuthor Contributions\nOLMo would not have been possible without the help of our many teammates and collaborators. We\nlist author contributions (in alphabetical order) below:\nContributors to pretraining dataset construction and tooling (Dolma) include Russell Authur, Iz\nBeltagy, Akshita Bhagia, Khyathi Chandu, Jesse Dodge, Yanai Elazar, Dirk Groeneveld, Rodney\nKinney, Kyle Lo, Aakanksha Naik, Abhilasha Ravichander, Dustin Schwenk, Luca Soldaini, and\nNishant Subramani.\nContributors to model training and architecture include Shane Arora, Iz Beltagy, Akshita Bhagia,\nMatthew E. Peters, Dirk Groeneveld, Ananya Harsh Jha, William Merrill, Jacob Morrison, Niklas\nMuennighoff, Dustin Schwenk, Saurabh Shah, Pete Walsh, and Mitchell Wortsman.\nContributors to evaluation suite and tooling include Akshita Bhagia, Arman Cohan, Pradeep\nDasigi, Jesse Dodge, Dirk Groeneveld, Yuling Gu, Tushar Khot, Ian Magnusson, Kyle Richardson,\nOyvind Tajford, and Pete Walsh.\nContributors to model adaptation include Iz Beltagy, Pradeep Dasigi, Jack Hessel, Hamish Ivison,\nNathan Lambert, Valentina Pyatkin, Pete Walsh, and Yizhong Wang.\nContributors to license creation and risk assessment include David Atkinson, Jesse Dodge, Jen-\nnifer Dumas, Crystal Nam, and Will Smith.\nThe OLMo project was led by Hannaneh Hajishirzi and Noah A. Smith.\n27http://www.apache.org/licenses/LICENSE-2.0\n13\nAcknowledgements\nOLMo would not have been possible without the support of many individuals and institutions. The\nexperimental components of this work were made possible through a partnership with AMD and\nCSC, enabling use of the LUMI supercomputer, and Kempner Institute at Harvard University. We\nthank Jonathan Frankle and the team at MosaicML (now Databricks) for sharing their experiences\nwith FSDP, and building the code base that OLMo is based on. We thank our teammates Taira\nAnderson, Michelle Benedict, Jon Borchardt, Evie Cheng, Arnavi Chheda, Johann Dahm, Matt\nLatzke, Kelsey MacMillan, Aaron Sarnat, Carissa Schoenick, Sam Skjonsberg, Michael Schmitz,\nMichael Wilson, Caitlin Wittlif, and the entire IT team, for their help with the website, design, in-\nternal and external communications, budgeting, and other activities that supported smooth progress\non this project. Finally, we also express gratitude for the helpful discussions and feedback from\nour teammates at AI2 and close collaborators, including Prithviraj (Raj) Ammanabrolu, Peter Clark,\nNicole DeCario, Doug Downey, Ali Farhadi, Ian Ferreira, V\u00a8ain\u00a8o Hatanp\u00a8a\u00a8a, Sham M. Kakade, Julien\nLaunay, Sydney Levine, Pekka Manninen, Franzi Roessner, Maarten Sap, Ludwig Schmidt, Yulia\nTsvetkov, and Daniel S. Weld.\nReferences\nAmro Abbas, Kushal Tirumala, D\u00b4aniel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,\n2023. URL https://arxiv.org/abs/2303.09540.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra-\nAim\u00b4ee Cojocaru, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badred-\ndine Noune, Baptiste Pannier, and Guilherme Penedo. The falcon series of open language mod-\nels. ArXiv, abs/2311.16867, 2023. URL https://api.semanticscholar.org/CorpusID:\n265466629.\nYuvanesh Anand, Zach Nussbaum, Brandon Duderstadt, Benjamin Schmidt, and Andriy Mulyar.\nGpt4all: Training an assistant-style chatbot with large scale data distillation from gpt-3.5-turbo.\nhttps://github.com/nomic-ai/gpt4all, 2023.\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,\n2016. URL https://api.semanticscholar.org/CorpusID:8236317.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernan-\ndez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Ka-\nplan. Training a helpful and harmless assistant with reinforcement learning from human feedback,\n2022.\nYoshua Bengio, R\u00b4ejean Ducharme, Pascal Vincent, and Christian Janvin.\nA neural proba-\nbilistic language model.\nJ. Mach. Learn. Res., 3:1137\u20131155, 2003.\nURL https://api.\nsemanticscholar.org/CorpusID:221275765.\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, Usvsn Sai Prashanth, Edward Raff,\nAviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. Pythia: A suite for analyzing large\nlanguage models across training and scaling. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceedings of the 40th\nInternational Conference on Machine Learning, volume 202 of Proceedings of Machine Learn-\ning Research, pages 2397\u20132430. PMLR, 23\u201329 Jul 2023. URL https://proceedings.mlr.\npress/v202/biderman23a.html.\nBigScience, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel\nHesslow, Roman Castagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois Yvon, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\n14\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial intelli-\ngence, volume 34, pages 7432\u20137439, 2020. URL https://ojs.aaai.org/index.php/AAAI/\narticle/view/6239.\nSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach.\nGPT-\nNeoX-20B: An open-source autoregressive language model. In Proceedings of the ACL Work-\nshop on Challenges & Perspectives in Creating Large Language Models, 2022. URL https:\n//arxiv.org/abs/2204.06745.\nSu Lin Blodgett, Lisa Green, and Brendan O\u2019Connor. Demographic dialectal variation in social\nmedia: A case study of African-American English. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, pages 1119\u20131130, Austin, Texas, November\n2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120. URL https:\n//aclanthology.org/D16-1120.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Rad-\nford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nArXiv,\nabs/2005.14165, 2020. URL https://api.semanticscholar.org/CorpusID:218971783.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna:\nAn open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\n2022. URL https://arxiv.org/abs/2204.02311.\nAlexandra Chronopoulou, Matthew Peters, and Jesse Dodge. Efficient hierarchical domain adap-\ntation for pretrained language models.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Tech-\nnologies, pages 1336\u20131351, Seattle, United States, July 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.naacl-main.96. URL https://aclanthology.org/2022.\nnaacl-main.96.\nHyung Won Chung, Noah Constant, Xavier Garc\u00b4\u0131a, Adam Roberts, Yi Tay, Sharan Narang, and\nOrhan Firat.\nUnimax: Fairer and more effective language sampling for large-scale multilin-\ngual pretraining. ArXiv, abs/2304.09151, 2023. URL https://api.semanticscholar.org/\nCorpusID:258187051.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\n15\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\narXiv preprint arXiv:1803.05457, 2018. URL https://arxiv.org/abs/1803.05457.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick\nWendell, Matei Zaharia, and Reynold Xin.\nFree dolly: Introducing the world\u2019s first truly\nopen instruction-tuned llm, 2023. URL https://www.databricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu,\nand Maosong Sun. Ultrafeedback: Boosting language models with high-quality feedback, 2023.\nDeepSeek-AI, :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng,\nHonghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge,\nKang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan\nHuang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X.\nLiu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo,\nShirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren,\nChong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng\nSun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong\nWu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu,\nDejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang,\nLiyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang\nZhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, and Yuheng Zou. Deepseek llm:\nScaling open-source language models with longtermism, 2024.\nJesse Dodge, Taylor Prewitt, Remi Tachet Des Combes, Erika Odmark, Roy Schwartz, Emma\nStrubell, Alexandra Sasha Luccioni, Noah A. Smith, Nicole DeCario, and Will Buchanan. Mea-\nsuring the carbon intensity of ai in cloud instances, 2022. URL https://dl.acm.org/doi/10.\n1145/3531146.3533234.\nWilliam B. Dolan and Chris Brockett.\nAutomatically constructing a corpus of sen-\ntential\nparaphrases.\nIn\nInternational\nJoint\nConference\non\nNatural\nLanguage\nPro-\ncessing,\n2005.\nURL https://www.microsoft.com/en-us/research/publication/\nautomatically-constructing-a-corpus-of-sentential-paraphrases/.\nYanai Elazar, Akshita Bhagia, Ian H. Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane\nSuhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, Hanna Hajishirzi, Noah A.\nSmith, and Jesse Dodge. What\u2019s in my big data? ArXiv, abs/2310.20707, 2023. URL https:\n//api.semanticscholar.org/CorpusID:264803575.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al.\nThe pile: An 800gb dataset of di-\nverse text for language modeling.\narXiv preprint arXiv:2101.00027, 2020.\nURL https:\n//arxiv.org/abs/2101.00027.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Fos-\nter, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell, Niklas Muen-\nnighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang\nSutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-\nshot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836.\nSidney Greenbaum and Gerald Nelson. The international corpus of english (ICE) project. World\nEnglishes, 15(1):3\u201315, mar 1996. doi: 10.1111/j.1467-971x.1996.tb00088.x. URL https://\ndoi.org/10.1111%2Fj.1467-971x.1996.tb00088.x.\nDirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind\nTafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge. Catwalk: A unified language model\nevaluation framework for many datasets. arXiv preprint arXiv:2312.10253, 2023. URL https:\n//arxiv.org/abs/2312.10253.\n16\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang, Jinran Nie, Yuxuan Ding, Jianwei Yue, and Yu-\npeng Wu. How close is chatgpt to human experts? comparison corpus, evaluation, and detection.\narXiv preprint arxiv:2301.07597, 2023.\nSuchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia\nShi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Di-\nmakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. OpenLM: a minimal but performa-\ntive language modeling (lm) repository, 2023. URL https://github.com/mlfoundations/\nopen_lm/. GitHub repository.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar.\nTOXIGEN: Controlling Language Models to Generate Implied and Adversarial Toxicity. In ACL,\n2022. URL https://arxiv.org/abs/2203.09509.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. Proceedings of the Interna-\ntional Conference on Learning Representations (ICLR), 2021.\nHamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep\nDasigi, Joel Jang, David Wadden, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi. Camels\nin a changing climate: Enhancing lm adaptation with tulu 2, 2023. URL https://arxiv.org/\nabs/2311.10702.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al.\nMixtral of experts. arXiv preprint arXiv:2401.04088, 2024. URL https://arxiv.org/abs/\n2401.04088.\nAndreas K\u00a8opf, Yannic Kilcher, Dimitri von R\u00a8utte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Minh Nguyen, Oliver Stanley, Rich\u00b4ard Nagyfi, Shahul ES, Sameer Suri,\nDavid Alexandrovich Glushkov, Arnav Varma Dantuluri, Andrew Maguire, Christoph Schuh-\nmann, Huu Nguyen, and Alexander Julian Mattick. Openassistant conversations - democratizing\nlarge language model alignment. In Thirty-seventh Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2023. URL https://openreview.net/forum?id=\nVSJotgbPHF.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following\nmodels. Github repository, 2023. URL https://github.com/tatsu-lab/alpaca_eval.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022. URL https://arxiv.org/abs/2211.09110.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 3214\u20133252, 2022.\nJian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. Logiqa: A chal-\nlenge dataset for machine reading comprehension with logical reasoning. CoRR, abs/2007.08124,\n2020. URL https://arxiv.org/abs/2007.08124.\nZhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo\nLi, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. Llm360: Towards fully transparent open-source\nllms. arXiv preprint arXiv:2312.06550, 2023. URL https://arxiv.org/abs/2312.06550.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\nIn International\nConference on Learning Representations, 2019. URL https://openreview.net/forum?id=\nBkg6RiCqY7.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the carbon foot-\nprint of bloom, a 176b parameter language model, 2022. URL https://arxiv.org/abs/2211.\n02001.\n17\nIan Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind\nTafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. Paloma: A benchmark\nfor evaluating language model fit. arXiv preprint arXiv:2312.10523, 2023.\nMitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3,\n1999. URL https://catalog.ldc.upenn.edu/LDC99T42.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.\nPointer sentinel mix-\nture models.\nArXiv, abs/1609.07843, 2016.\nURL https://api.semanticscholar.org/\nCorpusID:16299141.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Frederick Diamos, Erich Elsen,\nDavid Garc\u00b4\u0131a, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and\nHao Wu.\nMixed precision training.\nArXiv, abs/1710.03740, 2017.\nURL https://api.\nsemanticscholar.org/CorpusID:3297437.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789,\n2018. URL https://arxiv.org/abs/1809.02789.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Distributed rep-\nresentations of words and phrases and their compositionality. In Neural Information Processing\nSystems, 2013. URL https://api.semanticscholar.org/CorpusID:16447573.\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. Cross-task general-\nization via natural language crowdsourcing instructions. In Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.244. URL\nhttps://aclanthology.org/2022.acl-long.244.\nMosaicML NLP Team. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms, 2023. URL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-\nmane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language\nmodels. arXiv preprint arXiv:2305.16264, 2023.\nDavide Nunes.\nPreprocessed penn tree bank, 2020.\nURL https://zenodo.org/record/\n3910021.\nOpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:257532815.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike,\nand Ryan Lowe.\nTraining language models to follow instructions with human feedback.\nIn\nS. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in\nNeural Information Processing Systems, volume 35, pages 27730\u201327744. Curran Associates,\nInc., 2022.\nURL https://proceedings.neurips.cc/paper_files/paper/2022/file/\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nAntonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy\nBlackburn. Raiders of the lost kek: 3.5 years of augmented 4chan posts from the politically\nincorrect board. Proceedings of the International AAAI Conference on Web and Social Media,\n14:885\u2013894, may 2020. doi: 10.1609/icwsm.v14i1.7354. URL https://doi.org/10.1609%\n2Ficwsm.v14i1.7354.\nDavid Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild,\nDavid So, Maud Texier, and Jeff Dean. Carbon emissions and large neural network training, 2021.\nURL https://arxiv.org/abs/2104.10350.\n18\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aim\u00b4ee Cojocaru, Alessandro Cap-\npelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe re-\nfinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data\nonly. ArXiv, abs/2306.01116, 2023. URL https://api.semanticscholar.org/CorpusID:\n259063761.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee,\nand Luke Zettlemoyer. Deep contextualized word representations. ArXiv, abs/1802.05365, 2018.\nURL https://api.semanticscholar.org/CorpusID:3626819.\nMohammad Taher Pilehvar and Jos\u00b4e Camacho-Collados. Wic: 10, 000 example pairs for evaluating\ncontext-sensitive representations.\nCoRR, abs/1808.09121, 2018.\nURL http://arxiv.org/\nabs/1808.09121.\nOfir Press and Lior Wolf. Using the output embedding to improve language models. In Mirella\nLapata, Phil Blunsom, and Alexander Koller, editors, Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers,\npages 157\u2013163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL\nhttps://aclanthology.org/E17-2025.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kun-\ncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Men-\nsch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume, Yu-\njia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Au-\nrelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving. Scaling language models: Methods, analysis & insights from training go-\npher, 2022. URL https://arxiv.org/abs/2112.11446.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn.\nDirect preference optimization: Your language model is secretly a reward model.\nIn\nThirty-seventh Conference on Neural Information Processing Systems, 2023.\nURL https:\n//openreview.net/forum?id=HPuSIXJaa9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimiza-\ntions toward training trillion parameter models. SC20: International Conference for High Per-\nformance Computing, Networking, Storage and Analysis, pages 1\u201316, 2019.\nURL https:\n//api.semanticscholar.org/CorpusID:203736482.\nMachel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer. M2D2: A massively multi-\ndomain language modeling dataset. In Proceedings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 964\u2013975, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.\nemnlp-main.63.\nManoel Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano De Cristofaro, Gianluca\nStringhini, Summer Long, Stephanie Greenberg, and Savvas Zannettou.\nThe evolution of\nthe manosphere across the web.\nProceedings of the International AAAI Conference on Web\nand Social Media, 15:196\u2013207, may 2021. doi: 10.1609/icwsm.v15i1.18053. URL https:\n//doi.org/10.1609%2Ficwsm.v15i1.18053.\n19\nRonald Rosenfeld. Two decades of statistical language modeling: Where do we go from here?\nProceedings of the IEEE, 88(8):1270\u20131278, 2000.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An ad-\nversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\nURL https://dl.acm.org/doi/abs/10.1145/3474381.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,\nZheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen,\nAbheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le\nScao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask prompted\ntraining enables zero-shot task generalization. In International Conference on Learning Repre-\nsentations, 2022. URL https://openreview.net/forum?id=9Vrb9D0WI4.\nNoam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https:\n//api.semanticscholar.org/CorpusID:211096588.\nLuca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur,\nBen Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh\nJha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas\nMuennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle\nRichardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke\nZettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,\nand Kyle Lo. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining\nResearch. arXiv preprint, 2024.\nEmma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep\nlearning in NLP. In Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez, editors, Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages 3645\u20133650, Flo-\nrence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1355.\nURL https://aclanthology.org/P19-1355.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu.\nRoformer: Enhanced trans-\nformer with rotary position embedding. ArXiv, abs/2104.09864, 2021. URL https://api.\nsemanticscholar.org/CorpusID:233307138.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nTeknium1. Gpteacher. https://github.com/teknium1/GPTeacher, 2023.\nTogether Computer. RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset,\nApril 2023. URL https://github.com/togethercomputer/RedPajama-Data.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation lan-\nguage models. ArXiv, abs/2302.13971, 2023a. URL https://api.semanticscholar.org/\nCorpusID:257219404.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\n20\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b. URL https://arxiv.org/abs/2307.09288.\nMar\u00b4\u0131a Ubierna, Cristina D\u00b4\u0131ez Santos, and Sara Mercier-Blais. Water Security and Climate Change:\nHydropower Reservoir Greenhouse Gas Emissions, pages 69\u201394. Springer Singapore, Singapore,\n2022. ISBN 978-981-16-5493-0. doi: 10.1007/978-981-16-5493-0 5. URL https://doi.org/\n10.1007/978-981-16-5493-0_5.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141 ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need.\nIn I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett,\nedi-\ntors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc., 2017.\nURL https://proceedings.neurips.cc/paper_files/paper/2017/file/\n3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\nDavid Vilares and Carlos G\u00b4omez-Rodr\u00b4\u0131guez. HEAD-QA: A healthcare dataset for complex reason-\ning.\nIn Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez, editors, Proceedings of the 57th\nAnnual Meeting of the Association for Computational Linguistics, pages 960\u2013966, Florence,\nItaly, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1092. URL\nhttps://aclanthology.org/P19-1092.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding. ArXiv,\nabs/1804.07461, 2018. URL https://arxiv.org/abs/1804.07461.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi\nChandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, and Hannaneh Hajishirzi.\nHow far can camels go? exploring the state of instruction tuning on open resources, 2023. URL\nhttps://arxiv.org/abs/2306.04751.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V Le. Finetuned language models are zero-shot learners. In International\nConference on Learning Representations, 2022. URL https://openreview.net/forum?id=\ngEZrGCozdqR.\nJohannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions.\narXiv preprint arXiv:1707.06209, 2017. URL https://arxiv.org/abs/1707.06209.\nCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng,\nGloria Chang, Fiona Aga Behram, James Huang, Charles Bai, Michael Gschwind, Anurag Gupta,\nMyle Ott, Anastasia Melnikov, Salvatore Candido, David Brooks, Geeta Chauhan, Benjamin Lee,\nHsien-Hsin S. Lee, Bugra Akyildiz, Maximilian Balandat, Joe Spisak, Ravi Jain, Mike Rabbat,\nand Kim Hazelwood. Sustainable ai: Environmental implications, challenges and opportunities,\n2022. URL https://arxiv.org/abs/2111.00364.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei\nLin, and Daxin Jiang.\nWizardLM: Empowering large pre-trained language models to follow\ncomplex instructions.\nIn The Twelfth International Conference on Learning Representations,\n2024. URL https://openreview.net/forum?id=CfXh93NDgH.\nCanwen Xu, Daya Guo, Nan Duan, and Julian McAuley. Baize: An open-source chat model with\nparameter-efficient tuning on self-chat data. arXiv preprint arXiv:2304.01196, 2023.\nSavvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos,\nGianluca Stringini, and Jeremy Blackburn. What is gab: A bastion of free speech or an alt-\nright echo chamber. In Companion Proceedings of the The Web Conference 2018, WWW \u201918,\npage 1007\u20131014, Republic and Canton of Geneva, CHE, 2018. International World Wide Web\nConferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3191531. URL\nhttps://doi.org/10.1145/3184558.3191531.\n21\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\nHellaswag: Can a\nmachine really finish your sentence?\narXiv preprint arXiv:1905.07830, 2019. URL https:\n//arxiv.org/abs/1905.07830.\nBiao Zhang and Rico Sennrich. Root mean square layer normalization. ArXiv, abs/1910.07467,\n2019. URL https://api.semanticscholar.org/CorpusID:113405151.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022. URL https://arxiv.org/abs/\n2205.01068.\nYanli Zhao, Andrew Gu, Rohan Varma, Liangchen Luo, Chien chin Huang, Min Xu, Less\nWright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, Alban Desmaison, Can Balioglu, Bernard\nNguyen, Geeta Chauhan, Yuchen Hao, and Shen Li.\nPytorch fsdp: Experiences on scal-\ning fully sharded data parallel.\nProc. VLDB Endow., 16:3848\u20133860, 2023.\nURL https:\n//api.semanticscholar.org/CorpusID:258297871.\n22\nA\nAdditional Evaluation\n10\n100\n1000\n10000\n0.55\n0.82\n1.22\nPile\n10\n100\n1000\n10000\n0.37\n0.55\n0.82\n100 PLs\n10\n100\n1000\n10000\n0.82\n1.00\n1.22\nICE\n10\n100\n1000\n10000\n1.82\n2.72\n4.06\nTwitter AAE\n10\n100\n1000\n10000\n0.90\n1.11\n1.35\nManosphere\nModels\nFalcon-7B\nLLaMA2-7B\nMPT-7B\nLLaMA-7B\nPythia-6.9B\nRPJ-INCITE-7B\nOLMo-7B\n10\n100\n1000\n10000\n1.35\n1.65\n2.01\nGab\n10\n100\n1000\n10000\n0.90\n1.11\n1.35\n4chan\nBits Per Byte\nTokens Seen (billions)\nFigure 3: Bits per byte for each of the 7 remaining Paloma data sources not aggregated in Figure 2.\nAdditional perplexity results\nIn Figure 3 we provide results for each of the 7 data sources in\nPaloma (Magnusson et al., 2023) that are excluded from the combined metric in Figure 2. Some of\nthese sources such as Pile (Gao et al., 2020) and ICE (Greenbaum and Nelson, 1996) are not publicly\navailable at this time. Dolma 100 Programming Languages (Soldaini et al., 2024) consists of code\ndata that is not supported by the decontamination approach used in Paloma. TwitterAAE (Blodgett\net al., 2016), along with ICE, are datasets for targeted analyses of disparities in performance between\ndifferent dialects and as such should be evaluated separately. And finally, the Manosphere, Gab, and\n4chan corpora (Ribeiro et al., 2021; Zannettou et al., 2018; Papasavva et al., 2020) are intended to\nexamine model fit to language from fringe online communities that are studied for prevalent hate\nspeech and toxicity. Thus minimizing perplexity on these fringe corpora is not always desirable.\nOne notable result here is that OLMo-7B is much farther ahead of the other models on Dolma 100\nProgramming Languages (100 PLs). Note that this effect may be due in part to underestimation\nfrom contamination, as decontaminating code data is beyond the scope of the method in Paloma.\nAt the same time other models that are trained on code data from GitHub such as RPJ-INCITE-7B,\nthat are just as likely to have contamination, fair much worse. Another factor then is that OLMo-7B\ntrains on code data with exactly the same post-processing as that in 100 PLs while the code data\nin other models will have been processed differently. Similarly, Pile evaluation demonstrates these\nin-distribution and potential contamination effects as Pythia-6.9B achieves top performance despite\nbeing trained on almost an order of magnitude fewer tokens than OLMo-7B.\nThe results on the remaining 5 targeted sources should be interpreted with care, as Paloma often\nfinds that perplexity on these sources is dominated by superficial features such as low average doc-\nument length rather than fit to that which would actually be salient to members of these speech\ncommunities. TwitterAAE and Gab have among the shortest documents in Paloma contributing to\nunusually high bits per byte in this figure. Other than these two, the models are notably very closely\ngrouped in a data scaling trend in ICE, Manosphere, and 4chan.\nAdditional end-task results\nNext, in Table 9, we provide results from zero-shot evaluation of\nOLMo-7B on 6 additional end-tasks apart from the 8 in our core evaluation suite. These tasks\nare headqa en (Vilares and G\u00b4omez-Rodr\u00b4\u0131guez, 2019), logiqa (Liu et al., 2020), mrpc (Dolan\nand Brockett, 2005), qnli (Wang et al., 2018), wic (Pilehvar and Camacho-Collados, 2018), and\nwnli (Wang et al., 2018).\n23\nheadqa en\nlogiqa\nmrpc\nqnli\nwic\nwnli\navg.\nFalcon-7B\n38.6\n23.7\n62.8\n49.8\n49.5\n47.9\n45.4\nLLaMA-7B\n38.7\n19.5\n68.6\n50.1\n49.1\n52.1\n46.4\nLLaMA2-7B\n39.5\n26.1\n69.1\n49.4\n49.8\n45.1\n46.5\nMPT-7B\n37.4\n22.9\n67.7\n52.1\n48.1\n47.9\n46.0\nPythia-6.9B\n40.1\n21.5\n65.4\n53.8\n55.0\n38.0\n45.6\nRPJ-INCITE-7B\n36.9\n27.8\n58.8\n53.8\n48.9\n57.8\n47.3\nOLMo-7B\n37.3\n23.4\n68.4\n49.1\n50.2\n56.3\n47.5\nTable 9: Zero-shot evaluation of OLMo-7B on 6 additional end-tasks apart from the 8 present in our\ncore evaluation suite. Once again, we compare OLMo-7B to 6 other model checkpoints which are\npublicly available. We find that OLMo-7B outperforms the other models on aggregate taken over 6\nadditional end-tasks from this table, however these tasks were also found to provide limited signal\nduring training (see Figure 4).\nWe note, however, that in contrast to our core evaluation set described in Section 4.1, we found these\nadditional end-tasks to have less stable performance during model development, and to provide\na limited signal. This is illustrated in Figure 4, where we see the progress of task performance\nthroughout training to be more random (compare with the more stable upward trends in Figure 1).\nWhile tasks such as mrpc and wic appear more stable, they offered additional difficulties related to\nperformance being tied to random chance (e.g., wic) or the tendency of models to make spurious\npredictions (e.g., always predicting a single label) that either inflate or deflate performance due to\ndataset class imbalances (e.g., mrpc). We therefore caution against relying too heavily on these tasks\nwhen measuring model performance throughout training and comparing models.\n500\n1000\n1500\n2000\n2500\n34\n36\n38\nheadqa_en\n500\n1000\n1500\n2000\n2500\n20\n22\n24\nlogiqa\n500\n1000\n1500\n2000\n2500\n45\n60\nmrpc\n500\n1000\n1500\n2000\n2500\n51\n54\nqnli\n500\n1000\n1500\n2000\n2500\n49.8\n50.0\n50.2\nwic\n500\n1000\n1500\n2000\n2500\n48\n56\n64\nwnli\nTokens Seen (billions)\nAccuracy\nFigure 4: Accuracy score progression of OLMo-7B on 6 additional end-tasks. The performance of\nthese additional end-tasks was unstable and provided limited signal during model development.\nB\nAdaptation Training Details\nWe use the following hyperparameters when instruction tuning OLMo. These were chosen through\nsmall pilot experiments.\n\u2022 Learning Rate: 2 \u00d7 10\u22126\n24\nModel\nMMLU\nGSM8k\nBBH\nTydiQA\nCodex-Eval\nAlpacaEval\nToxiGen\nTruthfulQA\n0-shot\n8-shot CoT\n3-shot CoT\n1-shot\nPass@10\n%win\n% Toxic\n% Info + True\nOLMo-7B\n28.3\n8.5\n31.7\n32.3\n21.4\n-\n81.4\n31.6\n+SFT\n47.3\n15.5\n36.9\n35.2\n28.6\n57.0\n14.4\n41.2\n+SFT+DPO\n46.1\n11.0\n35.8\n21.7\n27.8\n69.3\n1.7\n52.0\nTable 10: Evaluation of OLMo-7B models before and after instruction finetuning and DPO training\non the full T \u00a8ULU evaluation suite. Lower is better for ToxiGen and higher is better for other metrics.\n\u2022 Epochs: 3\n\u2022 Warmup: Linear warmup for the first 3% of total training time, and then linear cooldown\nto a learning rate of 0 over the remaining steps.\n\u2022 Weight Decay: 0\n\u2022 Gradient clipping: 0\n\u2022 Maximum sequence length: 2048\nAfter instruction finetuning, we then use the following hyperparameters for DPO training, following\nIvison et al. (2023):\n\u2022 Learning Rate: 5 \u00d7 10\u22127\n\u2022 \u03b2: 0.1\n\u2022 Epochs: 3\n\u2022 Warmup: Linear warmup for the first 10% of total training time, and then linear cooldown\nto a learning rate of 0 over the remaining steps.\n\u2022 Weight Decay: 0\n\u2022 Gradient clipping: 0\n\u2022 Maximum sequence length: 2048\nC\nAdaptation Evaluation and Model details\nWe choose the models in Table 7 by choosing the \u2018canonical\u2019 best versions (that is, the best\ninstruction-tuned or otherwise adapted models released by the same organisation) of the base mod-\nels we compare against in Table 6. We additionally compare to T \u00a8ULU 2 to show the current best\nmodels trained using the T \u00a8ULU mix used to finetune OLMo. We display evaluations on MMLU,\nAlpacaEval, ToxiGen, and Truthfulness to focus on displaying how instruction tuning can generally\nhelp capabilities (MMLU), how the models perform in an open-ended chat setting (AlpacaEval),\nand to test how instruction tuning aids in model safety and truthfulness (AlpacaEval, ToxiGen). We\nadditionally report OLMo\u2019s performance over the entire T \u00a8ULU evaluation suite in Table 10.\nWe provide a brief description of each model evaluated in Table 7 below. For all models, we use the\nprovided chat template for prompt formatting when available.\n\u2022 MPT Chat: A version of MPT 7B finetuned on the ShareGPT-Vicuna (Chiang et al., 2023),\nHC3 (Guo et al., 2023), Alpaca (Taori et al., 2023), HH-RLHF (Bai et al., 2022), and Evol-\nInstruct (Xu et al., 2024) datasets.\nRetrieved from https://huggingface.co/mosaicml/\nmpt-7b-chat.\n\u2022 Falcon Instruct: A version of Falcon 7B finetuned on the Baize (Xu et al., 2023), GPT4All (Anand\net al., 2023), GPTeacher (Teknium1, 2023), and Refined-Web English (Penedo et al., 2023)\ndatasets. Retrieved from https://huggingface.co/tiiuae/falcon-7b-instruct.\n\u2022 RPJ-INCITE Chat: A version of RPJ-INCITE 7B finetuned on the OASST1 (K\u00a8opf et al., 2023)\nand Dolly V2 (Conover et al., 2023) datasets.\nRetrieved from https://huggingface.co/\ntogethercomputer/RedPajama-INCITE-7B-Chat.\n\u2022 Llama-2 Chat: A version of Llama 2 7B finetuned on a mixture of instruction datasets and further\ntrained with RLHF. We refer the reader to Touvron et al. (2023b) for further details.\n\u2022 T \u00a8ULU 2: A version of Llama 2 7B finetuned on a mixture of instruction datasets (the T \u00a8ULU 2\nmix). We refer the reader to Ivison et al. (2023) for further details.\n25\n\u2022 T \u00a8ULU 2+DPO: T \u00a8ULU 2 further trained with DPO on the UltraFeedback dataset (Cui et al., 2023).\nWe refer the reader to Ivison et al. (2023) for further details.\n\u2022 OLMo +SFT: A version of OLMo 7B fintuned on the same data as T \u00a8ULU 2.\n\u2022 OLMo +SFT+DPO: OLMo +SFT further trained with DPO on the UltraFeedback dataset (Cui\net al., 2023).\nWe additionally provide a brief description of each evaluation setting from Table 7:\n\u2022 MMLU: We use the official MMLU (Hendrycks et al., 2021) evaluation script and prompts avail-\nable at https://github.com/hendrycks/test, with modifications to allow for batch process-\ning. We evaluate using 0 few-shot examples, following the original setup of MMLU. We report\naverage accuracy across test examples.\n\u2022 ToxiGen: We follow the setup in Touvron et al. (2023b), but use the original set of prompts from\nHartvigsen et al. (2022), which are designed to elicit toxic generations for certain groups. We take\nonly the prompts designed to produce toxic language (\u2018hateful\u2019 prompts) and use 500 prompts\nper group to reduce evaluation costs. For base language models, we pass in the original ToxiGen\nprompts unchanged and greedily decode up to the first new line (or a maximum of 512 tokens). For\ninstruction-tuned models, we place the prompt in the corresponding template, and ask the model\nto complete the prompt, until the model generates a stop token (or a maximum of 512 tokens). We\npass the generated text into a roberta-large model trained to detect toxic content finetuned as part\nof Hartvigsen et al. (2022)28. We then report the percentage of generations deemed toxic by the\nclassifier.\n\u2022 TruthfulQA: Following Touvron et al. (2023b), we mainly use the generation setting of Truth-\nfulQA (Lin et al., 2022). The TruthfulQA dataset contains 818 questions, which are used to prompt\nthe tested model to generate answers. We use the default QA prompt format with 6 in-context QA\nexamples. We follow the official script in their official implemention29 to do greedy decoding and\nanswer postprocessing. We train two LLaMA 2-based classifiers for judging the truthfulness and\ninformativeness of the model response, due to the deprecation of GPT-3 making exact replication\nof the original TruthfulQA evaluation infeasible. We find that the LLaMA 2 judges are generally\nable to match the performance of the original GPT-3-based judges used by Lin et al. (2022). We\nreport the rate of the responses being truthful and informative (% Informative and Truthful) fol-\nlowing Touvron et al. (2023b). We only report the % Informative and Truthful as our primary\nmetric.\n\u2022 AlpacaEval: We use the package provided by Li et al. (2023), following the default setup which\nasks the evaluated model to generate responses for 805 prompts and employ GPT-4 to compare the\nresponse with Davinci-003. We employ the \u201calpaca eval gpt4\u201d annotator. We allow the evaluated\nmodel to generate up to 2048 tokens, without specifying special stop sequences. The reported\nwin-rate is the percentage of model generations that GPT-4 reports as being preferred over the\ngenerations from Davinci-003.\n28https://huggingface.co/tomh/toxigen_roberta\n29https://github.com/sylinrl/TruthfulQA/\n26\n"
  },
  {
    "title": "Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research",
    "link": "https://arxiv.org/pdf/2402.00159.pdf",
    "upvote": "55",
    "text": "Dolma : an Open Corpus of Three Trillion Tokens\nfor Language Model Pretraining Research\nLuca Soldaini\u2665\u03b1\nRodney Kinney\u2665\u03b1\nAkshita Bhagia\u2665\u03b1\nDustin Schwenk\u2665\u03b1\nDavid Atkinson\u03b1\nRussell Authur\u03b1\nBen Bogin\u03b1\u03c9\nKhyathi Chandu\u03b1\nJennifer Dumas\u03b1\nYanai Elazar\u03b1\u03c9\nValentin Hofmann\u03b1\nAnanya Harsh Jha\u03b1\nSachin Kumar\u03b1\nLi Lucy\u03b2\nXinxi Lyu\u03c9\nNathan Lambert\u03b1\nIan Magnusson\u03b1\nJacob Morrison\u03b1\nNiklas Muennighoff Aakanksha Naik\u03b1\nCrystal Nam\u03b1\nMatthew E. Peters\u03c3\nAbhilasha Ravichander\u03b1\nKyle Richardson\u03b1\nZejiang Shen\u03c4\nEmma Strubell\u03c7\u03b1 Nishant Subramani\u03c7\u03b1 Oyvind Tafjord\u03b1\nPete Walsh\u03b1\nLuke Zettlemoyer\u03c9\nNoah A. Smith\u03b1\u03c9\nHannaneh Hajishirzi\u03b1\u03c9\nIz Beltagy\u03b1\nDirk Groeneveld\u03b1\nJesse Dodge\u03b1\nKyle Lo\u2665\u03b1\n\u03b1Allen Institute for AI\n\u03b2University of California, Berkeley\n\u03c7Carnegie Mellon University\n\u03c3Spiffy AI\n\u03c4Massachusetts Institute of Technology\n\u03c9University of Washington\n{lucas,kylel}@allenai.org\nAbstract\nLanguage models have become a critical technology to tackling a wide range of\nnatural language processing tasks, yet many details about how the best-performing\nlanguage models were developed are not reported. In particular, information\nabout their pretraining corpora is seldom discussed: commercial language models\nrarely provide any information about their data; even open models rarely release\ndatasets they are trained on, or an exact recipe to reproduce them. As a result,\nit is challenging to conduct certain threads of language modeling research, such\nas understanding how training data impacts model capabilities and shapes their\nlimitations. To facilitate open research on language model pretraining, we release\nDolma, a three trillion tokens English corpus, built from a diverse mixture of\nweb content, scientific papers, code, public-domain books, social media, and\nencyclopedic materials. In addition, we open source our data curation toolkit to\nenable further experimentation and reproduction of our work. In this report, we\ndocument Dolma, including its design principles, details about its construction, and\na summary of its contents. We interleave this report with analyses and experimental\nresults from training language models on intermediate states of Dolma to share\nwhat we have learned about important data curation practices, including the role\nof content or quality filters, deduplication, and multi-source mixing. Dolma has\nbeen used to train OLMo, a state-of-the-art, open language model and framework\ndesigned to build and study the science of language modeling.\nDataset\nv. 1.6\nhuggingface.co/datasets/allenai/dolma\nToolkit\nv. 1.0\ngithub.com/allenai/dolma\n\u2665Core contributors. See Appendix B for full author contributions.\narXiv:2402.00159v1  [cs.CL]  31 Jan 2024\nSource\nDoc Type\nUTF-8 bytes\n(GB)\nDocuments\n(millions)\nUnicode\nwords\n(billions)\nLlama\ntokens\n(billions)\nCommon Crawl\n\u0080 web pages\n9,022\n3,370\n1,775\n2,281\nThe Stack\n\u00d0 code\n1,043\n210\n260\n411\nC4\n\u0080 web pages\n790\n364\n153\n198\nReddit\n\u00dc social media\n339\n377\n72\n89\nPeS2o\n\u0087 STEM papers\n268\n38.8\n50\n70\nProject Gutenberg\n[ books\n20.4\n0.056\n4.0\n6.0\nWikipedia, Wikibooks\n] encyclopedic\n16.2\n6.2\n3.7\n4.3\nTotal\n11,519\n4,367\n2,318\n3,059\nTable 1: The Dolma corpus at-a-glance. It consists of three trillion tokens sampled from a diverse\nset of domains sourced from approximately 200 TB of raw text. It has been extensively cleaned for\nlanguage model pretraining use.\n1\nIntroduction\nLanguage models are now central to tackling myriad natural language processing tasks, including few-\nshot learning, summarization, question answering and more. Increasingly, the most powerful language\nmodels are built by a few organizations who withhold most model development details (Anthropic,\n2023; OpenAI, 2023; Anil et al., 2023; Gemini Team et al., 2023). In particular, the composition\nof language model pretraining data is often vaguely stated, even in cases where the model itself is\nreleased for public use, such as LLaMA 2 (Touvron et al., 2023b). This hinders understanding of the\neffects of pretraining corpus composition on model capabilities and limitations, and therefore of the\nmodels themselves, with impacts on scientific progress as well as on the public who interfaces with\nthese models. We instead target openness and transparency, releasing and documenting a dataset of\nthree trillion tokens alongside tools to reproduce, scrutinize and expand on our work.\nOur aim is to allow for more individuals and organizations to participate in language model research\nand development.\n\u2022 Data transparency helps developers and users of applications that rely on language models to make\nmore informed decisions (Gebru et al., 2021). For example, increased prevalence of documents\nor terms in language model pretraining data has been linked to better performance on related\ntasks (Razeghi et al., 2022; Kandpal et al., 2023), and social biases in pretraining data (Feng et al.,\n2023; Navigli et al., 2023; Seshadri et al., 2023) may necessitate additional consideration in some\ndomains.\n\u2022 Open pretraining data is necessary for analysis via empirical studies exploring how data compo-\nsition influences model behavior, allowing the modeling community to interrogate and improve\ncurrent data curation practices (Longpre et al., 2023; Gao, 2021; Elazar et al., 2023). Examples of\nthis research include memorization (Carlini et al., 2022b; Chang et al., 2023), deduplication (Lee\net al., 2022), adversarial attacks (Wallace et al., 2021), benchmark contamination (Magar and\nSchwartz, 2022), and training data attribution (Hammoudeh and Lowd, 2022; Grosse et al., 2023)\n\u2022 Access to data is required for successful development of open language models. For example,\nnewer language models may offer functionality such as attribution of generations to pretraining\ndata (Borgeaud et al., 2022).\nTo support broader participation and inquiry in these lines of research, we present Data for Open\nLanguage Models\u2019 Appetite (Dolma), an open corpus of three trillion tokens designed to support\nlanguage model pretraining research. Pretraining data mixes are often motivated by a desire to capture\nso-called \u201cgeneral-purpose\u201d English. We source much of our data from sources similar to those present\nin past work, including a mix of web text from Common Crawl, scientific research from Semantic\nScholar, code from GitHub, public domain books, social media posts from Reddit, and encyclopedic\nmaterials from Wikipedia. We compare our dataset to a variety of popular pretraining corpora that are\n2\navailable publicly, and find that Dolma offers a larger pool of tokens at comparable quality and with\nequally diverse data composition. Dolma has been already used to pretrain OLMo (Groeneveld et al.,\n2024), a family of state-of-the-art models designed to facilitate the science of language modeling.\nIn summary, our contributions are two-fold:\n\u2022 We release the Dolma Corpus, a diverse, multi-source collection of 3T tokens across 5B docu-\nments acquired from 7 different data sources that are (i) commonly seen in large-scale language\nmodel pretraining and (ii) accessible to the general public. Table 1 provides a high-level overview\nof the amount of data from each source.\n\u2022 We open source the Dolma Toolkit, a high-performance, portable tool designed to efficiently curate\nlarge datasets for language model pre-training. Through this toolkit, practitioners can reproduce\nour curation effort and develop their own data curation pipelines.\nThe remainder of this manuscript is organized as follows: we first describe the desiderata and design\nprinciples that guided the creation of Dolma (\u00a72). We then document the methods applied to process\nthe raw text (\u00a73), including filters for language, \u201cquality,\u201d content filtering, and deduplication. Further\nprocessing was required to prepare Dolma for use as a pretraining corpus (\u00a74), including bench-\nmark decontamination and selecting a mixture rate. Throughout, we conduct ablation experiments,\nmeasuring domain fit through perplexity tracking and downstream performance on a set of twelve\nquestion-answering, common sense, and reasoning tasks. We conclude by discussing the process of\nreleasing Dolma (\u00a75).\n2\nDolma Design Goals\nTo support large-scale LM pretraining research, we set four design requirements around openness,\nconsistency with prior work, size, and risk mitigation. We discuss each in turn.\nDolma\u2019s curation should be consistent with prior language model pretraining recipes.\nBy\nmatching data sources and methods used to create other language modeling corpora, to the extent\nthey are known, we enable the broader research community to use our corpus and resulting model\nartifacts to study (and scrutinize) language models being developed today, even those developed\nbehind closed doors. In this reproduction effort, we follow established practices (i.e., use data\nsources and techniques for preprocessing and filtering content that appears frequently across language\nmodeling efforts) to the extent they are known, and defer to analysis, experimentation and educated\nguesses when best practice isn\u2019t known or implementations differ in subtle ways.1 Notably, this\nalso means scoping Dolma to English-only text to better leverage known curation practices and\nmaximize generalizability of scientific work on Dolma to existing language models.2 To illustrate\nthe open-ended nature of this reproduction effort, we provide a detailed summary of known (and\nunknown) data curation practices for some of the largest proprietary (e.g., GPT-4 (OpenAI, 2023),\nPaLM 2 (Anil et al., 2023), Claude (Anthropic, 2023)) as well as open (e.g., OPT (Zhang, 2022),\nLLaMA (Touvron et al., 2023a), Llama 2 (Touvron et al., 2023b)) language models in Appendix \u00a7C.\nDolma should support training of large models.\nHoffmann et al. (2022) suggested that one\ncan train compute-optimal models by maintaining a fixed ratio between language model size (in\nparameters) and minimum number of training tokens. Recent models that follow these \u201cscaling\nlaws,\u201d such as LLaMA 2 (Touvron et al., 2023b), appear to show there is still room for performance\nimprovement by increasing the number of training tokens.3 As this is an active area of research,\nwe aim for a sufficiently large corpus to allow further study of the relationship between model and\ndataset size\u20142-3T tokens.\n1We note this reproduction effort does not seek to replicate specific language model pretraining data\nimplementations. Instead, we reproduce a range of data curation themes.\n2Recognizing that this focus reinforces the assumption of English as the \u201cdefault\u201d language, we hope to\nexpand Dolma to more languages in the future. We release our data curation tools to support such efforts.\n3See Figure 5 in Touvron et al. (2023b), in which loss has not converged even at 2T tokens.\n3\nDolma should contribute to open corpora.\nLack of access to pretraining corpora alongside\ncorresponding language models has been a major obstacle for the broader research community. Very\nfew open models out of the hundreds released in the recent years are released alongside their training\ndata: T5 and C4 (Raffel et al., 2020), BLOOM and ROOTS (Leong et al., 2022; Piktus et al., 2023),\nGPT-J/GPT-NeoX/Pythia and Pile (Wang and Komatsuzaki, 2021; Black et al., 2022; Biderman\net al., 2023; Gao et al., 2020), INCITE and RedPajama v1 (Together Computer, 2023b,c). However,\nlimitations in these prior corpora have motivated need for a new dataset such as Dolma:\n\u2022 C4 (Raffel et al., 2020), Pile (Gao et al., 2020), and Falcon (Almazrouei et al., 2023) are high-\nquality datasets with demonstrated use in training language models, but are unfortunately\nlimited in scale. ROOTS (Piktus et al., 2023) is large and diverse but given its multilingual\nfocus, its English-only portion is also too small to train English-only models.\n\u2022 RedPajama v2 (Together Computer, 2023a) meet our criteria of scale but don\u2019t reflect\nrepresentative distributions over sources of content commonly seen in curating the largest\nlanguage models (e.g., scientific papers, code).\n\u2022 RedPajama v1 (Together Computer, 2023c) is most similar to our effort and a source\nof inspiration when designing Dolma. While RedPajama v1 was a reproduction of the\nLLaMA (Touvron et al., 2023a) training data, we have a broader reproduction target which\nrequired diving into data sources that RedPajama v1 did not pursue, including larger collec-\ntions of scientific papers and conversational forums like Reddit.\nIn all, we expand on these works by creating the largest curated open pretraining corpus to\ndate. We define openness to mean (i) sharing the data itself, which in turn informs our choice of\ndata sources, and (ii) documenting the process used to curate it, including decisions made with\njustifications, and open-source implementations to allow others to reproduce our work and create new\ncorpora. The resulting open-source high-performance toolkit enables researchers to implement their\nown data pipelines to either further refine Dolma or process their own datasets.\nDolma\u2019s curation should minimize risk of harm to individuals\nCurating a pretraining corpus\nmay introduce risk to individuals, either by facilitating access to information that is present in the\ncorpus, or by enabling training of harmful models. To minimize these risk while meeting our stated\ngoals, we engaged with legal and ethics experts from within our organizations early in the project\nand evaluated data design decisions based on their feedback on a case-by-case basis. Broadly, we\nfollow accepted practices when available (e.g., masking of certain personal identifiable information),\nand take a measured approach when diverging opinions exist in the literature (e.g., most effective\napproach to identify and remove toxic content). Further, we provide tools to request data removal4\nAs the landscape around data and AI is evolving, we do not claim that our decisions are correct.\nNevertheless, we do believe in compromising on desired research artifact properties like model\nreproducibility, performance, and extensibility in cases of significant harm to individuals.\nEven with these design goals to help scope our effort, there remain myriad decisions we must make\nwhen curating Dolma. Without a single clear recipe to follow from prior work, we rely on two\nprinciples to guide our decisions:\n(i) Use an evaluation suite, wisely. As part of the OLMo project Groeneveld et al. (2024), we\ndeveloped an evaluation suite (Groeneveld et al., 2023; details in Appendix D) to offer guidance\nduring pretraining across a range of capabilities and tasks. Whenever possible, data decisions are\nmade to improve its metrics. However, our evaluation suite is not perfect. For example, it cannot\nfully measure the effect of adding data sources that benefit models after instruction tuning5. In\nthese cases, we make sure that any one decision does not drastically decrease performance of any\nof the tasks in the suite.\n(ii) Favor decisions that advance research directions of interest to our organization. Where\nthe above principles do not offer guidance, we seek to build a corpus that will be most useful\nin research at academic or non-profit organizations like those of the authors. This does not\n4Available at the following URL: forms.gle/FzpUXLJhE57JLJ3f8\n5For example, the effect of adding code to pretraining data cannot be fully measured until models are able to\ngenerate executable code. However, such capability is typically observed after models are further finetuned to\nfollow instructions (Muennighoff et al., 2023a).\n4\nnecessarily mean maximizing benchmark performance; many desirable dataset interventions are\nat odds with each other6.\n3\nCreating Dolma\nCuration of pretraining data often requires defining complex pipelines that transform raw data\nfrom multiple sources into a single collection of cleaned, plain text documents. Such a pipeline\nshould support \u0013 acquisition of content from diverse sources (e.g., crawling, API ingestion, bulk\nprocessing), data Zcleanup through the use of filtering heuristics and content classifiers, and \u00e7\nmixing into a final dataset (e.g., deduplication, up/down-sampling of sources).\nIn curating Dolma, we create a high-performance toolkit to facilitate efficient processing on hundreds\nof terabytes of text content. The toolkit is designed for high portability: it can run any platform\nfrom consumer hardware (thus facilitating the development of new pipelines) to a distributed cluster\nenvironment (ideal for processing large datasets like Dolma). Through the curation of Dolma, we\nimplemented commonly used Zcleanup and \u00e7 mixing steps that can be used to reproduce and\ncurate similar datasets to Gopher, C4, and OpenWebText.\nUsing our toolkit, we develop and combine four kinds of data transformations that match Dolma\ndesiderata we introduced in \u00a72:\n\u2022 Z Language filtering. To create our English-only corpus, we rely on scalable tools for automated\nlanguage identification. Identification is performed using fastText\u2019s (Joulin et al., 2016a) language\nID model. Depending on the length of documents in each source, we either process the entire text\nat once or average the score of paragraphs. Documents with a sufficiently low English score are\nremoved.7 We do not perform any language identification on datasets that are distributed already\npre-filtered to English-only documents.8 We note that language filtering is never perfect, and\nmultilingual data is never completely removed from pretraining corpora (Blevins and Zettlemoyer,\n2022).\n\u2022 Z Quality filtering. It is common practice to remove text that is considered \u201clow quality,\u201d\nthough there is no broad consensus about what this means or how best to operationalize this with\nautomated tools.9 For web sources, we follow recommendations in Gopher (Rae et al., 2021) and\nFalcon (Almazrouei et al., 2023) which suggest avoiding model-based quality filters like those used\nfor LLaMA (Touvron et al., 2023a) and GPT-3 (Brown et al., 2020). Instead, we reimplemented\nand applied heuristics used in C4 (Raffel et al., 2020) and Gopher (Rae et al., 2021) that they\nused for processing Common Crawl. For other sources, we refer the reader to their corresponding\nsections as each required bespoke quality filtering strategies.\n\u2022 Z Content filtering. Beside removal of low quality, unnatural content, it is standard practice\nto filter toxic content from pretraining data to reduce risk of toxic generation (Anil et al., 2023;\nRae et al., 2021; Thoppilan et al., 2022; Hoffmann et al., 2022; Longpre et al., 2023). We follow\nthis practice and implement a mix of rules- and classifier-based toxicity filtering techniques\ndepending on the source.10. Large pretraining corpora have also be shown to include personal\nidentifiable information (PII; Elazar et al., 2023), which models are able to reproduce at inference\n6For example, we would like Dolma to support future investigations of the effect of pretraining on code;\nwhile our current evaluation suite is not properly designed to fully assess the impact of code data, we nevertheless\ninclude code in our corpus, to further research on this topic. Similarly, while previous research has suggested\nthat removing\n7Keeping a low threshold can help mitigate inherent biases (Blodgett et al., 2016) that language detectors have\nagainst English dialects spoken by minoritized groups. Scores used for each source are reported in subsequent\nsections.\n8These datasets may have been filtered to English content using other classifiers and thresholds.\n9The term \u201cquality filter,\u201d while widely used in literature, does not appropriately describe the outcome of\nfiltering a dataset. Quality might be perceived as a comment on the informativeness, comprehensiveness, or\nother characteristics valued by humans. However, the filters used in Dolma and other language models efforts\nselect text according to criteria that are inherently ideological (Gururangan et al., 2022).\n10Like in the case of \u201cquality\u201d, there is no single definition for \u201ctoxicity\u201d; rather, specific definitions vary\ndepending on task (Vidgen and Derczynski, 2020) and dataset curators\u2019 social identities (Santy et al., 2023);\nannotators\u2019 beliefs also influence toxic language detection (Sap et al., 2021) Using models to identify toxic\n5\ntime (Carlini et al., 2022a; Chen et al., 2023b). In Dolma, we identify content for removal through\na fastText classifier trained on Jigsaw Toxic Comments (cjadams et al., 2017) and a series of regular\nexpressions targeting PII categories from Subramani et al. (2023); Elazar et al. (2023).\n\u2022 \u00e7 Deduplication. Deduplication of pretraining corpora has been shown to be an effective technique\nto improve token efficiency during model training (Lee et al., 2022; Abbas et al., 2023; Tirumala\net al., 2023). In preparing Dolma, we use a combination of URL, document, and paragraph-level\ndeduplication. We achieve linear-time deduplication through the use of a Bloom filters (Bloom,\n1970). We perform this deduplication across files from the same subset (e.g., deduplicate all\ndocuments in the web subset), but not across sources (e.g., do not check if any web document also\nappears in the code subset).\nIn the reminder of this section, we provide a detailed explanation of how the steps above are\nimplemented for each data source shown in Table 1. To support our decisions, we leverage two\ntools. First, we inspect the output of our pipelines using the WIMBD tools (Elazar et al., 2023). This\napproach allows us to efficiently spot issues without having to train any models.\nThen, we conduct data ablations using a 1 billion parameter decoder-only model trained up to\n150 billion tokens; we provide a detailed description of our experimental setup in \u00a7D.1. Through\nthese ablations, we can compare the outcome of our data pipelines on our evaluation suite. The\nevaluation suite is comprised of 18 domains on which we measure perplexity to estimate language fit\n(Magnusson et al., 2023; described in \u00a7D.2), as well as 7 downstream tasks on which we evaluate\nquestion answering, reasoning, and commonsense capabilities of resulting models (described in \u00a7D.3).\nFor the reminder of this section, we present a subset of results on the evaluation suite; we include all\nour experimental results in Appendix K. When making decisions, we prioritize interventions that\noptimize metrics in downstream tasks over language fit.\n3.1\n\u0080 Web Pipeline\n://foo\n://foo\n://bar\nDeduplication\nby URL\nLanguage \nFiltering\nQuality Filters\nC4 (subset) + Gopher rules\nContent Filters\nToxic content, PII\nDeduplication\non text overlap\nFigure 1: Overview of the web processing pipeline in Dolma.\nThe web subset of Dolma was derived from Common Crawl.11 Common Crawl is a collection of\nover 250 billion pages that were crawled since 2007. It is organized in snapshots, each correspond to\na full crawl over its seed URLs. In November 2023, there were 89 snapshots. Dolma was curated\nfrom 25 snapshots.12 collected between 2020-05 to 2023-06.\n3.1.1\n\u0013 Data Acquisition and Z Language Filtering\nFollowing data curation practices used to develop LLaMA (Touvron et al., 2023a), our web pipeline\nleverages CCNet (Wenzek et al., 2020b) to perform language filtering and initial content deduplication.\ncontent remains challenging (Welbl et al., 2021; Markov et al., 2023a), and existing methods have been shown to\ndiscriminate against minoritized groups (Xu et al., 2021).\n11commoncrawl.org\n12We use just enough snapshots to meet the volume goal described in \u00a72 \u2014 at least 2T tokens.\n6\nThis tool was also used for the Common Crawl subset of RedPajama v1 (Together Computer, 2023c)\nand RedPajama v2 (Together Computer, 2023a). CCNet processes each web page with a fastText\nlanguage identification model13 to determine the primary language for each document; we keep all\npages with English document score greater or equal to 0.5 (removed 61.7% of web pages by size).\nFurther, CCNet identifies and removes very common paragraphs by grouping shards in each snapshot\ninto small sets and removing duplicated paragraphs in each. This step removed approximately 70%\nof paragraphs, primarily consisting of headers and navigation elements. Overall, CCNet pipeline\nfilters out 84.2% of the content in Common Crawl, from 175.1 TB to 27.7 TB. More details provided\nin Appendix J.4.\n3.1.2\nZ Quality Filtering\nWeb crawled data requires significant cleanup before it can be used for language model pretraining.\nThis step removes artifacts introduced by the conversion from HTML to plain text (e.g., page\nheaders, ill-formatted text) and discards pages that do not contain enough \u201cprose-like\u201d text (e.g.,\nrepeated text, short segments). First, CCNet natively provides a quality filter using KenLM (Heafield,\n2011) perplexity to group documents into buckets based on Wikipedia-likeness; this buckets are\noften interpreted as high (21.9%), medium (28.5%), or low (49.6%) quality context. However, per\narguments posed in Rae et al. (2021) and Almazrouei et al. (2023) against model-based quality filters,\nas well as our own manual inspections of content distributed between these buckets, we opted not use\nthese CCNet quality scores. Instead, in Dolma, we achieve quality filtering by combining heuristics\nintroduced by Gopher (Rae et al., 2021) and C4 (Raffel et al., 2020). Specifically we keep all the\nGopher rules (henceforth, Gopher All) and keep a single heuristic from C4 designed to remove\nparagraphs that do not end in punctuation (C4 NoPunc; as opposed to C4 All). Detailed description\nof filtering rules provided in Appendix J.4.\n0\n50B\n100B\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\n0.5\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nHellaSwag\nTotal Tokens\nAccuracy\nFigure 2: Model ablations for quality filters of the web processing pipeline. We find that a combination\nof C4 and Gopher rules leads to improvements in both language fit (left, on the C4 100 Domains\nsubset of Paloma (Magnusson et al., 2023)) and downstream performance (right, on HellaSwag Zellers\net al. (2019)).\nAblation results shown in Figure 2 validate our filtering strategy: we find that C4 NoPunc on its\nown outperforms both C4 All as well as Gopher All on both perplexity and downstream tasks.\nFinally, combining Gopher All + C4 NoPunc offers the best performance. In all, the Gopher rules\ntagged 15.23% of UTF-8 characters for removal, while the C4 rule tagged 22.73% of characters for\nremoval. When comparing our heuristics against CCNet\u2019s quality scores, the remaining documents\nafter filtering fall into CCNet buckets of high (22.8%), medium (26.2%) and low (51.0%) quality,\nrevealing very little correlation between model and heuristic-based quality filters.\nUsing the tool from Elazar et al. (2023), we inspect our filtered dataset for occurrences of repeated\nn-grams. Despite filtering using Gopher and C4 rules, we still found undesirable texts such as\nrepeated sequences of \u2018-\u2019 100 times, occurring over 60 million times, or repeated sequences of \u2018bla\u2019,\noccurring 19.1 million times (see Table 2). Based on this, we implement n-gram heuristics to identify\nand remove documents containing these sequences; specifically, we remove any repeated sequence\nlonger than 100 UTF-8 characters. While this only removed 0.003% of the total characters in the\n13https://fasttext.cc/docs/en/language-identification.html\n7\ndataset, removal of these documents can prevent loss spikes during training, as was empirically\nfound14 in Scao et al. (2022). We also note that this was a fairly conservative heuristic that left many\nrepeated sequences remaining in the dataset; we found from manual inspection of these sequences\nthat they often served as webpage layout elements as opposed to parsing irregularities.\nRepeated n-gram sequence\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - ...\n* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ...\n/ / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / ...\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...\n= = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = ...\n# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # ...\nTable 2: Examples of common repeated n-gram sequences in the web subset identified through\nWIMBD tools (Elazar et al., 2023). Repeted sequences longer than the ones shown here have been\nremoved after being identified by WIBMD.\n3.1.3\nZ Content Filtering\nFiltering Toxic Content\nData sampled from the internet may contain harmful or toxic con-\ntent (Matic et al., 2020; Luccioni and Viviano, 2021; Birhane et al., 2023a,b). As highlighted in \u00a72,\nwe filter Dolma to reduce harms that might arise from training language models on toxic content. We\nused the Jigsaw Toxic Comments dataset (cjadams et al., 2017), which contains forum comments\ntagged with (multilabel) categories \u201ctoxic\u201d, \u201csevere toxic\u201d, \u201cthreat\u201d, \u201cinsult\u201d, \u201cobscene\u201d,\nand/or \u201cidentity hate\u201d alongside unlabeled comments, to train two fastText classifiers\u2014a binary\n\u201chate\u201d detector and a binary \u201cNSFW\u201d detector:\n1. For our \u201chate\u201d detector, we group all unlabeled comments and \u201cobscene\u201d-only comments as\nnegatives and left remaining comments as positives.\n2. For our \u201cNSFW\u201d detector, we take all comments tagged as \u201cobscene\u201d as positives and left other\nremaining comments as negatives. It is important to note this detector only filters toxic content that\nmentions sexual or obscene topics, not sexual content in general.\nFor both these models, we run them on Common Crawl sentences15 with a filtering threshold of 0.40\nbased on manual threshold tuning. We chose our threshold seeking a balance between (1) maximizing\nprecision and recall from inspecting predicted toxic sentences on a single snapshot of Common Crawl,\nas well as (2) minimizing too much data removal.16 We always remove just the span that has been\ntagged as toxic, not the full document. We make both of these models available publicly.17\nIn Figure 3, we compare the effect of two different thresholds for the \u201chate\u201d and \u201cNSFW\u201d detector.\nThe \u201cHigh Threshold\u201d configurations remove less content, but generally yield higher perplexity on\nevaluation set and lower downstream performance. The \u201cLow Threshold\u201d configurations remove more\ncontent and generally have higher performance, but remove more units of text (7.3% vs 34.9% and\n5.5% vs 29.1%, for \u201chate\u201d and \u201cNSFW\u201d UTF-8 characters, respectively). Because lower thresholds\nmight lead to false positive, and improved performance can be achieved by combining content filters\nwith quality and deduplication filters, we use the \u201cHigh Threshold\u201c versions of the \u201chate\u201d and \u201cNSFW\u201d\nfilters, removing any sentence with a score greater than or equal to 0.4.\nFiltering Personal Identifiable Information\nData sampled from the internet can also leak personal\nidentifiable information (PII) of users (Luccioni and Viviano, 2021; Subramani et al., 2023); such PII\nis abundant in large-scale datasets (Elazar et al., 2023).\n14More\ninformation\nat\ngithub.com/bigscience-workshop/bigscience/blob/master/train/\ntr8-104B-wide/chronicles.md\n15Identified using BlingFire sentence splitter (Microsoft, 2019).\n16For example, the \u201chate\u201d and \u201cNSFW\u201d detectors filter out 34.9% and 29.1% of tokens from Common Crawl\nat thresholds of 0.0004 and 0.00017, respectively.\n17\u201cNSFW\u201d fastText tagger and \u201chate\u201d fastText tagger.\n8\n0\n50B\n100B\n20\n30\n40\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\n0.5\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nHellaSwag\nTotal Tokens\nAccuracy\nFigure 3: Model ablations for toxic content filters of the web processing pipeline. We find that\nadopting a \u201cLow Threshold\u201d for the \u201chate\u201d and \u201cNSFW\u201d toxic content filters results to improvements\nin both lanugage fit (left, on the C4 100 Domains subset of Paloma (Magnusson et al., 2023))\nand downstream performance (right, on HellaSwag Zellers et al. (2019)); however, more content is\nremoved (7.3% vs 34.9% and 5.5% vs 29.1%, for \u201chate\u201d and \u201cNSFW\u201d UTF-8 characters, respectively).\nPII detection can be accomplished using model-based tools (Dernoncourt et al., 2017; Microsoft,\n2018; Hathurusinghe et al., 2021; Lison et al., 2021; Lukas et al., 2023; Mazzarino et al., 2023) or\nrule-based approaches (Aura et al., 2006; Elazar et al., 2023). The former generally offer better\nperformance, while the latter are faster.\nThe size of Dolma makes impractical to use model-based tools; instead, we rely on carefully crafted\nregular expressions. Following the findings of Subramani et al. (2023), we tag three kinds of PII that\ncan be detected with sufficient accuracy: email addresses18, IP addresses19, and phone numbers20.\nOnce spans are tagged, we employ different processing strategies based on the their density on each\ndocument:\n\u2022 5 or fewer PII spans detected:\nwe replace all spans on a page with special tokens\n|||EMAIL_ADDRESS|||, |||PHONE_NUMBER|||, and |||IP_ADDRESS||| for email addresses, phone num-\nbers, and IP addresses respectively21. In total, we find 0.02% of documents in the 25 Common\nCrawl snapshots match this filter.\n\u2022 6 or more PII spans detected: we remove any document that contains 6 or more matching PII\nspans. We this approach because pages containing abundant phone numbers and email addresses\nare likely to pose a greater risk of discosing other PII classes. 0.001% of documents in the 25\nCommon Crawl snapshots match this filter.\nIn Figure 4, we show results of experiment designed to quantify the impact of our PII strategy.\nOverall, we find that, in both language modeling and downstream tasks, PII removal and masking has\nno discernible effect on model performance.\n3.1.4\n\u00e7 Deduplication\nRecent efforts indicate that the deduplication of data leads to language models that train more\nefficiently (Lee et al., 2022). Following this principle, we deduplicate data in the web pipeline. We\nperform three stages of deduplication:\n(i) Exact URL deduplication: mark pages that share the same URL. No normalization is performed.\nThis filter is primarily intended to remove pages that have been crawled multiple times. Overall,\nit removes 53.2% of documents in the 25 snapshots used to create Dolma. URL deduplication is\ncommonly used as the first stage for web crawls thanks to its computational efficiency (Agarwal\net al., 2009; Koppula et al., 2010; Penedo et al., 2023).\n18Regex:\n[.\\s@,?!;:)(]*([\\^\\s@]+@[\\^\\s@,?!;:)(]+?)[.\\s@,?!;:)(]?[\\s\\n\\r]\n19Regex: \\s+\\(?(\\d{3})\\)?[-\\. ]*(\\d{3})[-. ]?(\\d{4})\n20Regex: (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9]{1,2})\\.){3}\n(?:25[0-5]|2[0-4][0-9]|[01]?[0-9]{1,2})\n21When training models on Dolma, we these special tokens to the tokenizer vocabulary. For all results shown\nin this paper, we use allenai/gpt-neox-olmo-dolma-v1\\_5.\n9\n0\n50B\n100B\n20\n30\n40\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nHellaSwag\nTotal Tokens\nAccuracy\nFigure 4: 1B model ablations for PII strategies. We found no discernible differences between\nremoving all documents with PIIs, only removing documents with \u2265 5 PII instances and masking the\nrest, and doing no PII filtering at all.\n(ii) Exact document deduplication: mark pages that contain the same text. No punctuation or\nwhitespace is removed. Empty documents count as duplicates. Overall, it removes an additional\n14.9% of documents after URL deduplication.\n(iii) Exact paragraph deduplication: mark identical paragraphs across pages as duplicates. We keep\ndefinition of this unit consistent with previous filters: a paragraph is a span of text separated\nby the newline UTF-8 character \u201c\\n\u201d. Overall, this filter tags 18.7% of documents in the URL-\ndeduplicated set as repeated.\nThis multi-stage approach is designed to increase efficiency: stages (i) and (ii) are designed to remove\ncopies of the same item (identical pages might have multiple URLs, such in the case of the same news\narticle being included in multiple online newspaper), thus can be executed before before any content\nor quality filtering, reducing the number of pages to process. In contrast, stage (iii) removes repeated\ncontent that appears on the different pages (such as the same byline appearing under all articles\nwritten by the same author), thus altering portion of pages and potentially disrupting content analysis.\nAll stages use a Bloom filter (Bloom, 1970) data structure for efficient content deduplication.\n3.1.5\n\u0013Z\u00e7 Putting It All Together\nHow do steps in the pipeline compose?\nTo summarize, the Dolma web pipeline transform the\noutput of CCNet by first performing URL and document-level deduplication, followed by quality\nfiltering (Gopher, C4 NoPunc), content filtering (toxic content, PII), and, finally, paragraph-level\ndeduplication. But What\u2019s the combined outcome of the filtering?\n0\n50B\n100B\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n0.3\n0.4\n0.5\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nHellaSwag\nTotal Tokens\nAccuracy\nFigure 5: Compounding effect of quality filtering, content filtering, and paragraph-level deduplication\non 1B model ablations. Combination of all components in the pipeline leads to improvements in\nboth language fit (left, on the C4 100 Domains subset of Paloma (Magnusson et al., 2023)) and\ndownstream performance (right, on HellaSwag Zellers et al. (2019)).\nIn Figure 5, we show the compounding effect of the stages of the pipeline. We find that the\ncombination of the three stages achieve the best performance on downstream tasks, while content\nfiltering slightly hurts language fit of C4 100 domains subset. As stated in \u00a72, we leverage downstream\nevaluation tasks to make decision; thus we use all steps in the pipeline when creating Dolma.\n10\nData distribution\nWe use the tool from Elazar et al. (2023) to inspect the final data composition\nin Figure 6. In particular, we analyze web domain, year, and language distributions.\npatents.google.com\nwww.nature.com\nwww.frontiersin.org\npatents.justia.com\nepdf.pub\nissuu.com\nlaw.justia.com\njournals.plos.org\napi.parliament.uk\nen.wikipedia.org\nDomain\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n% of Documents\nDolma Domains\n(a) Web (URL) domains\n2020\n2022\n2021\n2019\n2023\nNone\n2018\n2017\n2016\n2015\nYear\n0\n5\n10\n15\n20\n25\n30\n35\n% of Documents\n37.9\n23.2\n21.9\n9.2\n3.2\n1.1\n0.7\n0.6\n0.5\n0.4\nDolma Dates\n(b) Dates of documents\nun\nzh\nlong\npt\nes\nfr\nde\nko\nru\nja\nLanguage\n0.0\n0.2\n0.4\n0.6\n0.8\n% of Documents\n0.86\n0.06\n0.06\n0.03\n0.03\n0.03\n0.03\n0.02\n0.02\n0.02\nDolma Languages\n(c) Non-English languages\nFigure 6: Frequencies over different document metadata as computed using the What\u2019s In My Big\nData? tool from Elazar et al. (2023). In subfigure (c), un denotes documents whose language could\nnot be identified; long indicates documents that are too long to be processed with the tool\u2019s language\nID module.\nWe note that Dolma contains documents from a broad set of internet domains, mostly from 2020,\n2022, and 2021. The most common internet domains in Dolma, per token, are patents.google.com,\nfollowed by www.nature.com and www.frontiersin.org. In fact, similar to other corpora reported\nin Elazar et al. (2023), 63.6% of Dolma\u2019s web documents are from \u2018.com\u2019 sites (followed then by\n\u2018.org\u2019 and \u2018.co.uk\u2019 sites). Finally, as all language identification tools are imperfect, we summarize\nwhat languages are remaining post English-only filtering: We find the most common language\nafter English is not well identified (\u2018un\u2019) with 0.86% of the documents, followed by 0.06% of the\ndocuments identified as Chinese.\nDo quality and content filters have similar effects?\nIn order to further understand how filters\ndescribed in \u00a73.1.2 and \u00a73.1.3 interact with each other, we perform a correlation analysis on a subset\nof documents sampled from our pipeline.\nGopher Decont.\nHate\nPII\nDecont.\nHate\nPII\nDedup.\n-0.002\n-0.087\n0.003\n-0.03\n0.003\n0.02\n-0.21\n0.005\n0.036\n-0.006\n(a) Head\nGopher Decont.\nHate\nPII\nDecont.\nHate\nPII\nDedup.\n-0.002\n-0.091\n0.004\n-0.013\n0.002\n0.018\n-0.19\n0.004\n0.052\n-0.001\n(b) Middle\nGopher Decont.\nHate\nPII\nDecont.\nHate\nPII\nDedup.\n-0.001\n-0.051\n0.005\n-0.01\n0.001\n0.044\n-0.36\n0.013\n-0.012\n-0.018\n(c) Tail\nFigure 7: Pearson Correlation of filters on the Head, Middle, and Tail parts of our Common Crawl\ndata. The correlation is computed for 24M, 20M, and 43M documents respectively. The filters\nare Gopher=Gopher rules from Rae et al. (2021), Dedup.=Deduplication, PII=Personal Identifiable\nInformation, Hate=Hate Speech and Decont.=Decontamination.\nThe correlation among the documents flagged for removal by our Common Crawl filters is depicted in\nFigure 7. We find that correlations are generally low, thus our filters select fairly different documents\nand are not redundant. There is some positive correlation between our PII (Personal Identifiable\nInformation) filters and filters removing hate speech. This is likely because hate speech is often\ndirected at people. The Gopher filtering rules correlate negatively with our deduplication, especially\nfor the high-perplexity tail part of our data. This is due to the Gopher rules removing many high-\nperplexity documents such as random strings, which are not caught by deduplication due to their\nrandomness. As these random strings likely do not contribute to a better understanding of language,\nit is important to filter them out and thus rely on filters beyond deduplication.\n11\n3.2\n\u00d0 Code Pipeline\n{\n  \"glossary\": {\n    \"title\": \"example glossary\",\n    \"GlossDiv\": {\n      \"title\": \"S\",\n      \"GlossList\": {\n        \"GlossEntry\": {\n          \"ID\": \"SGML\",\n          \"SortAs\": \"SGML\",\n          \"GlossTerm\": \"Standard Generalized Markup Language\",\n          \"Acronym\": \"SGML\",\n          \"Abbrev\": \"ISO 8879:1986\",\n          \"GlossDef\": {\nJSON\n    # Python 3: Fibonacci series up to n\n    def fib(n):\n        a, b = 0, 1\n        while a < n:\n            print(a, end=' ')\n            a, b = b, a+b\n        print()\n    fib(1000)PY\n    fn main() {\n        // Variables can be type annotated.\n        let logical: bool = true;\n        let a_float: f64 = 1.0;  // Regular annotation\n        let an_integer   = 5i32; // Suffix annotation\n        // Or a default will be used.\n        let default_float   = 3.0; // `f64`\n        let default_integer = 7;   // `i32`\n        // A type can also be inferred from context.\n        let mut inferred_type = 12; // Type i64 is inferred from another line.\n        inferred_type = 4294967296i64;\n        // A mutable variable's value can be changed.\n        let mut mutable = 12; // Mutable `i32`\n        mutable = 21;\n        // Error! The type of a variable can't be changed.\n        mutable = true;\n        // Variables can be overwritten with shadowing.\n        let mutable = true;\n    }\nRUST\nLanguage \nFiltering\nQuality Filters\nStarCoder + RPJ\nContent Filters\nPII\nFigure 8: Overview of the data pipeline to process code documents.\n3.2.1\n\u0013 Data Acquisition and Z Language Filtering\nWe derive the code subset of Dolma from The Stack (Kocetkov et al., 2022), a collection of\npermissively-licensed GitHub repositories. We use the near-deduplicated version as a starting point,\nthus removing the need to perform deduplication ourselves. The raw version of this dataset was\ncollected in March 2023. We filter data-heavy documents by removing files with extensions such as\nJSON and CSV.\n3.2.2\nZ Quality Filtering\nWe apply heuristics derived from RedPajama v1 (Together Computer, 2023c) and StarCoder (Li\net al., 2023) datasets. The former consist of rules to remove repetitive file preambles, such as\nlicense statements22 and documents with excessively long lines or mostly numerical content. Overall,\nRedPajama Rules (RPJ) are designed to remove files that are mostly data or generated through\ntemplates. To further select high quality code snippets, we leverage rules from the StarCoder pipeline;\nthese heuristics filter GitHub repositories with no to few stars, files with too few or too many\ncomments, and HTML files with low code-to-text ratio. For a detailed description of these rules,\nsee \u00a7J.4.\n0\n10B\n20B\n30B\n40B\n50B\n4\n5\n6\n7\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nHumanEval\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n50\n60\n70\n80\n90\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n0.27\n0.28\n0.29\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nHellaSwag\nTotal Tokens\nAccuracy\nFigure 9: Comparison of quality filtering when using RedPajama Rules (RPJ) rules or RPJ and\nStarCoder rules combined. Combining the two rulesets results in slightly improved perplexity on\ncode documents (left, HumanEval; Chen et al., 2021b ), more stable perplexity curves on non-code\ntest sets (center, on the C4 100 Domains subset of Paloma; Magnusson et al., 2023), and slightly\nimproved downstream performance (right, on HellaSwag; Zellers et al., 2019).\nIn Figure 9, we present a comparison between RedPajama (RPJ) and StarCoder rules. In our\nablations we find that, compared to RPJ rules alone, RPJ and StarCoder combined lead to lower\nperplexity on code datasets (e.g., HumanEval; Chen et al., 2021b), more stable perplexity during\ntraining on non-code test sets (e.g., C4 100 Domains subset of Paloma; Magnusson et al., 2023), and\nimproved downstream performance (e.g., HellaSwag; Zellers et al., 2019). Therefore, we chose to\nuse this combination when creating the final mix for Dolma.\n22We keep this information in the metadata associated with each document in Dolma.\n12\n3.2.3\nZ Content Filtering\nWe apply the same filtering rules to from the web pipeline (\u00a7 3.1) to mask personal identifiable\ninformation (PII). Documents with greater than 5 PII instances are removed from Dolma. In all other\ninstances, emails, phone numbers, and IP addresses are masked using special tokens.\nWe also remove code secrets or personal information. To do so, we use the detect-secrets (Yelp,\n2013) library and remove any documents with a match.\n3.2.4\n\u00e7 Deduplication\nWe used the already-deduplicated version of The Stack published by Kocetkov et al. (2022); their\napproach uses the pipeline first introduced by Allal et al. (2023), which uses MinHash Broder (2002)\nand Locally Sensitive Hashing to find similar documents.\n3.3\n\u00dc Conversational Forums Pipeline\nLanguage \nFiltering\n2\n10\n30\nQuality Filters\ncontent/metadata rules\n2\n10\n30\nContent Filters\nremove toxic, nsfw, \nbanned content\n20\n10\n30\nDeduplication\nover documents\nFigure 10: Overview of the data pipeline to process conversational forums.\n3.3.1\n\u0013 Data Acquisition and Z Language Filtering\nThe conversational subset of Dolma was derived from the Pushshift Reddit dataset (Baumgartner\net al., 2020b), a large collection of forum conversations collected through Reddit\u2019s data API and\ndistributed by the Pushshift project. We derive the conversational subset in Dolma from 378M\nposts from Reddit, from December 2005 until March 2023. We include both submissions\u2014initial\nmessage in conversations on Reddit\u2014and comments\u2014replies to messages\u2014in the dataset. We treat\nall submissions and comments as independent documents without any structure or connection to the\nthread they appear in; in our evaluation, this simplified representation yields better performance on\ndownstream tasks. A discussion of this trade-off is presented in Appendix E.\nFor consistency, we use same strategy as the web pipeline to filter non English content. In particular,\nwe keep submission and comments with an English score greater than 0.5.\n3.3.2\nZ Quality Filtering\nConversational forum data must be adequately cleaned to remove content that is too short, repetitive,\nor is negatively ranked by the community it was submitted to. We use the pipeline introduced by Hen-\nderson et al. (2019) to facilitate cleanup of submissions and comments using Google Dataflow23. We\nremove comments shorter than 500 characters, and submissions shorter than 400 characters24. We\nalso remove documents over 40,000 characters in length.\n23https://cloud.google.com/dataflow\n24Qualitative inspection of the data suggested that submissions are of higher quality than comments; thus, we\nuse a more permissive minimum length.\n13\nWe remove comments with fewer than 3 votes25, as lower score are associated with comments that\nare deeply nested in a conversational thread (Weninger et al., 2013) or content that is more likely\nto results in emotionally charged discourse (Davis and Graham, 2021). Votes have been used as a\nsignal in constructing the WebText (Radford et al., 2019) and OpenWebText (Peterson, 2020) corpora.\nWe discard documents that have been deleted by their authors or removed by moderators; further,\ndocuments that have been labeled by their authors as \u201cover 18\u201d were also removed. We exclude any\ndocument originated from any of the 26,123 banned and not safe for work subreddits26 we curated.\n3.3.3\nZ Content Filtering\nWe apply the same filtering rules to used in the web pipeline (\u00a73.1.3) to remove toxic content and\nmask PII. Unlike in the case of the web pipeline, we fully remove a document if part of it are tagged\nas toxic. We employ this strategy because content from Reddit is shorter in length, thus it is more\nlikely that a single sentence classified as toxic is a strong indication of the entire document being\ntoxic as well.\n3.3.4\n\u00e7 Deduplication\nWe employ the same strategy used in the web pipeline (\u00a73.1.4). Since submissions and comments\nare shorter than web documents, we only deduplicate at a document-level. This strategy is useful to\nreduce the incidence of \u201cCopy pasta\u201d (blocks of text that get often repeated across many comments\nand subreddits for comedic effect) and other repetitive information.\n3.4\nOther Data Sources\nIn this section, we briefly summarize additional high-quality sources that were used to derive Dolma.\nFor more details on collection and processing, see Appendix \u00a7J.3 and \u00a7J.4.\n\u0080 C4 for Curated Web Content\nSimilarly to LLaMA (Touvron et al., 2023a), we include\ndocuments from C4 Raffel et al. (2020) in the Dolma dataset. We further refine this data by\nreprocessing it through our web pipeline to remove long, repeated sequences (\u00a73.1.2) and duplicates\n(\u00a73.1.4). Finally, we also perform PII masking as described in (\u00a73.1.3);\n\u0087 PeS2o for Academic Literature\nThe PeS2o dataset (Soldaini and Lo, 2023) is a collection of\napproximately 40 million open-access academic papers that have been cleaned, filtered, and formatted\nfor pre-training of language models. It is derived from the Semantic Scholar Open Research Corpus\n(S2ORC) (Lo et al., 2020). As this dataset has been created for language modeling purposes, we use\nit as-is.\n[ Project Gutenberg for Books\nProject Gutenberg is a repository of over 70 thousand public\ndomain books. We collected Project Gutenberg\u2019s archive in April 2023. We use the same fastText-\nbased language identification model to identify English language books and include them in Dolma.\nMore details in our Data Sheet \u00a7 J.\n] Wikipedia and Wikibooks for Encyclopedic Content\nThis dataset was derived by March 2023\nWikimedia dumps. We use the \u201cEnglish\u201d and \u201cSimple\u201d editions of Wikipedia and Wikibooks as base\nfor the Encyclopedic subset of Dolma. Sources were processed using WikiExtractor27. We remove\nany document with 25 or fewer UTF-8-segmented words, as we found shorter pages to either be the\nresult of short, templated pages (e.g., pages containing only a few words and an information box) or\nXML parsing errors.\n25The total votes for each documents are obtained by computing the difference between positive votes, also\nknown as \u201cupvotes\u201d, negative votes or \u201cdownvotes\u201d.\n26The list is available at https://github.com/allenai/dolma/blob/main/sources/reddit/atomic_\ncontent_v5/subreddit_blocklist.txt. The list was obtained by merging several sources that tracked\nbanned subreddits (mostly from posts on Reddit itself). We also measured the fraction of posts within a subreddit\ntagged as NSFW, and blocked the subreddit when this fraction exceeded 10%.\n27github.com/attardi/wikiextractor, v. 3.0.7, commit prefix 8f1b434.\n14\n4\nTraining a Language Model on Dolma\nAs a final validation step of the Dolma pipeline, we train, evaluate and release a decoder-only, autore-\ngressive language model which we call Olmo-1b. In this section, we discuss potential approaches\nadditional dataset curation decisions specific to model training. In \u00a74.1, we present an approach to\nremove benchmark tasks\u2014i.e., decontaminate\u2014from Dolma. Then, in \u00a74.2, we discuss consider-\nations when combining\u2014i.e., mixing\u2014the various document subsets in Dolma to obtain the final\npretraining corpus. Finally, in \u00a74.3, we present experimental results of the resulting Olmo-1b model.\nOlmo-1b uses GPT-NeoX tokenizer (Black et al., 2022), which we found to be well suited for Dolma;\nwe present results supporting our decision in Appendix F.\n4.1\nStrategies for Benchmark Decontamination in Dolma\nIn this section we experiment with approaches to remove benchmark contamination from pretraining\nand select which is ultimately used in Olmo-1b. Large-scale language datasets contain copies\nof benchmarks that are commonly used to evaluate language models (Dodge et al., 2021; Yang\net al., 2023; Elazar et al., 2023). The impact of such contamination is currently debated. For\nexample, Lee et al. (2022) showed that removing duplicates of validation data from C4 pretraining\nincreases perplexity on the previously duplicated validation data. Meanwhile, work examining\npost-hoc performance difference between contaminated and uncontaminated downstream data finds\nno consistent positive or negative impact (Chowdhery et al., 2022; Brown et al., 2020; OpenAI, 2023).\nTo start, we focus on the removal of perplexity benchmark contamination, and we measure the extent\nof downstream task contamination. We experiment with removing contamination with respect to\nan early version of Paloma (Magnusson et al., 2023), a benchmark of 585 text domains designed to\nevaluate language model fit to diverse sources. This selection of perplexity evaluations is detailed in\nAppendix D.\nDecontamination strategy for perplexity evaluation\nUsing the paragraph deduplication tools\ndescribed in \u00a7 3.1.4, we mark any paragraph in Dolma as contaminated if (i) it is longer than 13\nUnicode-segmented tokens28 and (ii) it appears in any of the documents in Paloma. In preliminary\nexperiments on decontaminating C4 (Raffel et al., 2020) against an early version of Paloma, we\ncompare the paragraph-based decontamination technique described above with exact-matching whole\ndocuments. Results show that document-based decontamination yields lower matching rate, with\nonly 1 of 12 subsets with greater than 1% contaminated documents29. However, when considering\nparagraph-based decontamination, 6 of 12 perplexity tasks have greater than 1% of documents\ncontaminated. Since the latter better reflect expected contamination rates, we chose it for the reminder\nof this section.\nLastly, we consider two ways of removing contamination. In preliminary experiments on C4, we find\nthat removing just the contaminated paragraphs by excluding them from documents removes 0.01%\nof tokens, while removing whole documents with any contamination removes 0.02% of tokens. In\neither case 0.01% of documents are affected. Given that each have relatively small impact, we opt for\nremoving full documents to avoid disrupting reading order, though this does bias towards removing\nlonger documents.\nDecontamination results for perplexity evaluation\nTo assess the risk of our decontamination\napproach, we train30 two 1B parameter models on a 221B token subset of RedPajama v1 (Together\nComputer, 2023c), the corpus most similar to Dolma\u2019s intended composition at the time of experi-\nmenting. The first model is trained on RedPajama v1 as-is, while the second uses the same corpus\nafter the paragraph-matching, document-removal decontamination approach described above. On this\nsubset, our decontamination approach removes 2.17% of unicode tokens and 0.66% of documents. In\nTable 3 we show that differences in perplexity and downstream task performance are minimal and\ndo not trend consistently positive or negative. For perplexity, 7 sources degrade and 6 improve; for\ndownstream tasks, 5 degrade and 4 improve. The largest degradation in a perplexity source is 22.0 to\n28Like in Elazar et al. (2023), we only consider paragraph of sufficient length to avoid false positive matches.\n29C4 100 Domains subset, which is directly constructed from C4.\n30This experiment uses the setup described in Appendix D, including model configuration, optimizer, and\nevaluation setup.\n15\nAvg ppl\nover subsets\n(\u2193)\nLargest subset\nppl diff\n(PTB \u2193)\nAvg acc\non end tasks\n(\u2191)\nLargest acc diff\non end task\n(SCIQ \u2191)\nDecontaminated\n25.6\n22.3\n59.2\n84.8\nNot Decontaminated\n25.7\n22.0\n56.37\n86.3\nDifference\n-0.1\n0.3\n2.8\n-1.5\nTable 3: Performance differences with and without our decontamination approach on 1B models\ntrained on RedPajama v1 (Together Computer, 2023c). Perplexity (ppl) results are from Paloma and\ndownstream (end task) results are from the tasks listed in Appendix D plus COPA (Gordon et al.,\n2012). We find no evidence that decontamination degrades overall model performance.\n22.3 on Penn Tree Bank. The largest degradation in a downstream task is a drop of 1.5% accuracy on\nSCIQ to 84.8%. In conclusion, results show no consistent evidence of performance degradation with\ndecontamination.\nDecontamination in Olmo-1b.\nAs our experiments have derisked our approach for removing\nbenchmark contamination, we apply it to our model trained on Dolma. The finalized approach for\nremoving overlap with Paloma is detailed in Magnusson et al. (2023). It applies the steps discussed\nin this section with the addition of a filter that ignores overlaps consisting of only punctuation, spaces,\nand emoji. These types of tokens can be arbitrarily repeated in text formatting, leading to common\nn-grams greater than our 13-gram threshold. On the final Dolma corpus used to train Olmo-1b, our\napproach finds less than 0.001% characters in training data contaminated, and removes fewer than\n0.02% of documents.\nMeasuring possible contamination of downstream tasks.\nWe measure data contamination in\nDolma. We follow the same setup from WIMBD (Elazar et al., 2023) and compute the percentage\nof instances from tasks with two or more inputs (e.g., natural language inference) that can be found\nin a single document. This serves as an upper bound of exact-match contamination in Dolma. We\nconsider 82 datasets from PromptSource (Bach et al., 2022), and report the datasets that at least 5%\nof their test sets can be found in Dolma. We report the results in Figure 11.\nhead_qa\nswag_regular\npaws-x_en\nscicite\nglue_mrpc\nliar\nglue_qnli\nglue_rte\nsuper-glue_rte\nopenai_humaneval\nsuper-glue_wicesnli\nsnli\nsuper-glue_axb\nsuper-glue_copa\nsem_eval\nglue_ax\nsick\nwinograd_wsc\nDataset\n0\n20\n40\n60\n80\n100\n% Contaminated instances\n6.6\n6.7\n7.7\n14.5\n33.3\n48.6\n62.3\n68.0\n68.0\n92.7\n96.1\n97.2\n97.2\n100.0\n100.0\n100.0\n100.0\n100.0\n100.0\nContaminated Datasets in Dolma\nIndex\nDolma\nFigure 11: Contamination percentages of datasets from PromptSource (Bach et al., 2022).\nResults indicate that portion of datasets in Promptsource appear in Dolma. Six datasets are completely\ncontaminated (100%): the Winograd Schema Challenge (Levesque et al., 2012), Sick (Marelli et al.,\n2014), AX from GLUE (Wang et al., 2018), SemEval (specifically, Task 1 from 2014), COPA from\nSuperGLUE (Roemmele et al., 2011), and AXb (the diagnostic task) from SuperGLUE (Wang et al.,\n2019). In addition, other datasets are mostly contaminated, with over 90% of their test sets appearing\nin Dolma documents: OpenAI HumanEval (Chen et al., 2021a), WIC from SuperGLUE (Pilehvar\nand Camacho-Collados, 2019), ESNLI (Camburu et al., 2018), and SNLI (Bowman et al., 2015). We\nnote that the contaminated datasets have been excluded from the downstream tasks we use for model\nevaluation (c.r.f. Appendix D).\n4.2\nStrategies for Subsets Mixing and Upsampling with Dolma\nLike the pretraining corpora of nearly every large-scale language model, Dolma is a multi-source\ndataset. Training on Dolma thus requires a mixing strategy that determines how much data from each\n16\nsource to include, and potentially which sources to upsample. Like other multi-source corpora (e.g.,\nROOTS (Laurenccon et al., 2023), the Pile (Gao et al., 2020), RedPajama v1 (Together Computer,\n2023c)),31 Dolma does not prescribe a single mixing strategy. We refer the reader to Rae et al. (2021)\nfor an example of how one might programmatically search over mixing configurations to maximize\nperformance. Here, we perform mixing experiments as an opportunity to answer some research\nquestions about how different data sources interact. We use the same ablation setup described in \u00a73.\nHow much code is important for pretraining?\nIt is common practice for language models to be\npretrained on some amount of code, even if code generation is not the intended task. Some research\nhas suggested that mixing code into training over plain text documents improves performance on\nreasoning tasks (Madaan et al., 2022). We investigate whether this observation holds for models\ntrained on Dolma, and if so, how much code is needed?\nDataset\n0% Code\n5% Code\n15% Code\nbAbI (ICL)\n0.0 \u00b1 0.0\n8.8 \u00b1 0.9\n10.1 \u00b1 2.8\nWebNLG (ICL)\n16.8 \u00b1 1.1\n19.3 \u00b1 1.1\n22.0 \u00b1 1.3\nGSM8K (FT)\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\n0.0 \u00b1 0.0\nGSM8K+PAL (FT)\n11.8 \u00b1 0.8\n14.2 \u00b1 1.3\n14.7 \u00b1 0.9\nTable 4: Performance of three models pre-trained with increasing amounts of code on three datasets,\nacross 5 random seeds. We measure exact match for bAbI and GSM8K, and Rouge-2 for WebNLG.\nWe create three mixtures from the C4 and Stack subsets containing 0%, 5% and 15% of code data. On\neach, we train a 1B model. We evaluate these models on three different reasoning tasks: bAbI (Weston\net al., 2015), WebNLG Gardent et al. (2017) and GSM8k Cobbe et al. (2021). For the first two tasks,\nwe follow the experimental setup of Muennighoff et al. (2023b) and evaluate each model in an ICL\nsetup with a changing number of demonstrations (0-5) across 5 random seeds. Muennighoff et al.\n(2023b) show that adding code to pre-training data improves ICL performance on bAbI and WebNLG\nand they suggest that code improves long-range state-tracking capabilities. Our experiments, as\nshown in Table 4, corroborate these findings: while the C4-only model fails on all bAbI tasks, adding\ncode improves performance, with a similar trend for WebNLG.\nOn the more difficult GSM8k benchmark, all models failed to get any correct answer in an ICL setup,\nand even when fine-tuning the models on the entire training set. However, we find that by fine-tuning\non program-aided output, where questions are solved by writing Python snippets as described in Gao\net al. (2022), code models outperform the C4-only model. These results show that models pre-trained\non code can leverage code generation to answer challenging reasoning tasks even when the original\ntask does not directly involve code.\nEvaluating mixing strategies for pretraining on Dolma\nWhile Dolma does not prescribe a\nspecific source mixture, we analyze some commonly used strategies32 and compare their effect using\nthe Paloma evaluation suite (Magnusson et al., 2023). Specifically, we present and evaluate four\npossible data mixtures in Table 5.\nWe show results of mixtures in Figure 12. Overall, we observe that the different mixtures have an\neffect on the ability of resulting models to capture specific subdomains. All mixtures show similar\nperplexity scores on pages sampled from 100 domains from C4 (Figure 12, left), indicating their\ngeneral effectiveness at modeling web documents. On the other hand, we note how models struggle\nto model specialized domains unless they are exposed to them. As an example, a model trained on\nthe Web-only mix struggles to represent data in the code domain (Figure 12, center, HumanEval).\nFinally, we use results on the S2ORC subset of M2D2, which consists of academic papers, to illustrate\nhow different data mixtures affect perplexity. As is it the case with code, Web-only model exhibits\nhiger perplexity due to domain mismatch. On the other hand, models trained on Reference+ and\nGopher-like mixes achieve lower perplexity than the model trained on the Na\u00efve mix, due to more\nin-domain content. However, we note that, despite significant differences in the amount of academic\n31RedPajama v1 was a reproduction of the multi-source corpus used in LLaMA (Touvron et al., 2023a).\nRedPajama v2 (Together Computer, 2023a) focuses solely on Common Crawl and is thus single-source.\n32We did not include any social data in these mixes as it was not ready at the time of this experiment.\n17\nMix Name\nDescription\nSampling\nProportion\nNa\u00efve\nSample each source in Table 1 equally.\n\u0080 Web\n100%\n\u00d0 Code\n100%\n] \u0087 Ref.\n100%\n[ Books\n100%\n\u0080 Web\n83.5%\n\u00d0 Code\n13.8%\n] \u0087 Ref.\n2.5%\n[ Books\n0.2%\nWeb Only\nSimilar to Ayoola et al. (2022), we test a mixture\nthat only uses web data.\n\u0080 Web\n100%\n\u00d0 Code\n0%\n] \u0087 Ref.\n0%\n[ Books\n0%\n\u0080 Web\n100%\n\u00d0 Code\n0%\n] \u0087 Ref.\n0%\n[ Books\n0%\nReference+\nIt is common practice to upsamole knowledge-\nintensive documents when composing training\nmixture. In our case, we upsample the PeS2o\npapers, Wikipedia, Wikibooks, and Gutenberg\nbooks subsets by 2x.\n\u0080 Web\n100%\n\u00d0 Code\n100%\n] \u0087 Ref.\n200%\n[ Books\n200%\n\u0080 Web\n81.2%\n\u00d0 Code\n13.5%\n] \u0087 Ref.\n4.9%\n[ Books\n0.4%\nGopher-like\nFollowing Rae et al. (2021), we create a mix that\nis heavily biased towards reference material. As\nwe do not have access to the same sources, an\nexact replication of their mix is not possible.\n\u0080 Web\n17%\n\u00d0 Code\n8%\n] \u0087 Ref.\n200%\n[ Books\n200%\n\u0080 Web\n68.4%\n\u00d0 Code\n5.4%\n] \u0087 Ref.\n24.2%\n[ Books\n2.0%\nTable 5: Overview of the mixtures and their composition.\n0\n50B\n100B\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n4\n5\n6\n7\n8\n9\n10\n20\n30\n40\n50\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nHumanEval\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\n50\n60\n70\n80\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 12: 1B model ablations for different proportions of Dolma data. All mixture perform similarly\non web data (left), while excluding code increases perplexity on code datasets (center). Finally,\nincreasing reference material by upsampling papers and Wikipedia yields lower perplexity on S2ORC\n(right). Overall, source distribution is linked to downstream capabilities; thus, Dolma users should\nsample subsets according to their needs.\npapers between Reference+ and Gopher-like (4.9% vs 24.2%), they achieve nearly identical results,\nsuggesting that even a relatively small percentage of in-domain data is sufficient to achieve good\ndomain fit.\n4.3\nEvaluating Olmo-1b\nIn Table 6 we compare Olmo-1b with other 1B models. Note that while parameter count is matched\nhere, only TinyLlama has been trained for a comparable number of tokens while Pythia 1B is trained\nfor nearly 10 times fewer tokens and the data composition of StableLM2 is unknown. Nevertheless\nwe find that Olmo-1b performs better on average than the most comparable model, TinyLlama,\noutperforming it in 4 out of 8 tasks. Though zero-shot evaluations of downstream tasks are often\nchallenging for these relatively small 1B models, the performance for all the tasks on all the models\nis above naive random performance. Further details about the downstream tasks is included in\nAppendix D.\nIn Figure 13 we assess how the Dolma mix that we use to train Olmo-1b compares to other popular\npretraining corpora in terms of perplexity of models where all other variables than pretraining data\nare controlled. In particular we fix the number of tokens each model is trained on to 150B, so\nthat data scale and differences in learning rate schedule do not confound with the effect from data\ncomposition that we intend to study. This analysis uses the 1B baselines from Paloma and evaluates\nPaloma\u2019s highest-level metric, which computes perplexity over the combination of test sets from 11\ndata sources. Other more fine-grained perplexity results comparing these baselines are available in\nMagnusson et al. (2023). The present analysis excludes sources that are not publicly available, involve\n18\nTask\nStableLM2 1.6B\n(Stability AI, 2024)\nPythia 1B\n(Biderman et al., 2023)\nTinyLlama 1.1B\n(Zhang et al., 2024)\nOlmo-1b\nthis work\nARC-E (Clark et al., 2018)\n63.7\n50.2\n53.2\n58.1\nARC-C (Clark et al., 2018)\n43.8\n33.1\n34.8\n34.5\nBoolQ (Clark et al., 2019)\n76.6\n61.8\n64.6\n60.7\nHellaSwag (Zellers et al., 2019)\n68.2\n44.7\n58.7\n62.5\nOpenBookQA (Mihaylov et al., 2018)\n45.8\n37.8\n43.6\n46.4\nPIQA (Bisk et al., 2019)\n74.0\n69.1\n71.1\n73.7\nSciQ (Welbl et al., 2017)\n94.7\n86\n90.5\n88.1\nWinoGrande (Sakaguchi et al., 2019)\n64.9\n53.3\n58.9\n58.9\nAverage\n66.5\n54.5\n59.4\n60.3\nTable 6: Comparison of Olmo-1b against other similarly sized language models. Olmo-1b was trained\non 3 trillion tokens from a preliminary version of Dolma (v. 1.5). Overall, Olmo-1b shows better\nperformance than TinyLlama, which has been trained on a similar number of tokens. Olmo-1b\noutperforms Pythia 1B, but the latter has been trained on one order of magnitude fewer tokens.\nStableLM2 is included in this table as a reference, but it cannot be fairly compared with the other\nworks since composition of its training data is not known.\n20\n65\n150\nTokens Seen (billions)\n13\n14\n15\n16\n17\n19\n20\n22\n24\n25\n28\nPerplexity\nBaselines\nThe Pile 1B\nFalcon-RefinedWeb 1B\nDolma v1.5 1B\nC4 1B\nRedPajama 1B\nmC4 1B\nFigure 13: Perplexity over all the standard language modeling and fine-grained domain sources in\nthe final, released version of Paloma (Magnusson et al., 2023), excluding code data not supported\nfor decontamination. The models are 1B baselines from Paloma trained on 150B tokens of each\ncorpus. Since Paloma takes stratified samples of hundreds of fine-grained domains, it emphasizes fit\nto heterogeneous, curated sources more than evaluations on monolithic Common Crawl data like C4.\nPile includes the least Common Crawl data, but mostly exhausts the small curated data sources it\ndraws on. Dolma and, to a lesser extent, RedPajama demonstrate the possibility for maintaining this\nsample efficiency on fit to diverse domains while including large scale Common Crawl data.\nfringe or toxic text, or that consist of code data not supported by the benchmark decontamination\napproach we use. This leaves C4 (Raffel et al., 2020), mC4-en (Chung et al., 2023), Wikitext 103\n(Merity et al., 2016), Penn Treebank (Marcus et al., 1999; Nunes, 2020), RedPajama (Together\nComputer, 2023c), Falcon-RefinedWeb (Penedo et al., 2023), Dolma (this work), M2D2 S2ORC\n(Reid et al., 2022), M2D2 Wikipedia (Reid et al., 2022), C4 100 domains (Chronopoulou et al., 2022),\nand Dolma 100 Subreddits (this work).\nOur controlled perplexity analysis reveals the importance of including non-Common Crawl data\nfrom diverse curated sources. The metric that we use from Paloma surfaces how models fit to more\nheterogeneous data, because it samples marked domains from each source equally rather than by\ntheir unequal proportions in the source. Intuitively, the baseline trained on the Pile is well fit to such\ndata as that pretraining corpus is mostly sourced from just such smaller, hand-picked sources. But as\nwe wish to scale the total number of tokens in a corpus, the challenge becomes how to integrate more\navailable Common Crawl data without losing sample efficiency on diverse evaluations such as this\n19\nPaloma metric. In this case we see that the Dolma baseline nearly matches the performance curve\nof the Pile baseline even though the fraction of Common Crawl data included is more than 4 times\ngreater.\n5\nReleasing Dolma\nRisk mitigation\nWe recognize that any dataset derived from large web crawls will contain factually-\nincorrect information, toxic language, hate speech, PII, and other types of harmful content. While we\nhave made an effort to curate this dataset taking this into consideration, we believe risk mitigation\nis best approached from multiple directions, including careful consideration of licenses and access\ncontrols.\nCopyright\nWhile most datasets we used were curated with copyright and licensing in mind (e.g.,\nopen access papers in peS2o (Soldaini and Lo, 2023), open source repositories in the Stack (Kocetkov\net al., 2022)) or were already permissively licensed (e.g., Wikipedia is released under a Creative\nCommons license), we recognize that large web crawls will also contain copyrighted material. Yet,\ngiven current tools, it\u2019s not possibly to reliably or scalably detect copyrighted materials in a corpus\nof this size. Our decision to release Dolma publicly factors in several considerations, including\nthat all our data sources were publicly available and already being used in large-scale language\nmodel pretraining (both open and closed), we refer the reader to our public position on AI and fair\nuse (Farhadi et al., 2023).\nWe recognize that the legal and ethical landscape of AI is changing rapidly, and we plan to revisit our\nchoices as new information becomes available.\nReferences\nAmro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semdedup: Data-\nefficient learning at web-scale through semantic deduplication. ArXiv, abs/2303.09540, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:257557221.\nJudit Acs. Exploring BERT\u2019s Vocabulary, 2019.\nAmit Agarwal, Hema Swetha Koppula, Krishna P. Leela, Krishna Prasad Chitrapura, Sachin Garg,\nPavan Kumar GM, Chittaranjan Haty, Anirban Roy, and Amit Sasturkar. Url normalization for\nde-duplication of web pages. In Proceedings of the 18th ACM Conference on Information and\nKnowledge Management, CIKM \u201909, page 1987\u20131990, New York, NY, USA, 2009. Association\nfor Computing Machinery.\nISBN 9781605585123.\ndoi: 10.1145/1645953.1646283.\nURL\nhttps://doi.org/10.1145/1645953.1646283.\nOrevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith,\nand Yulia Tsvetkov. Do all languages cost the same? tokenization in the era of commercial\nlanguage models, 2023.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi,\nCarolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin,\nDmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del\nR\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas,\nMarco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia\nLi, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries,\nand Leandro von Werra. SantaCoder: don\u2019t reach for the stars! arXiv [cs.SE], January 2023.\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Co-\njocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic,\nBadreddine Noune, Baptiste Pannier, and Guilherme Penedo. Falcon-40B: an open large language\nmodel with state-of-the-art performance. TII UAE, 2023.\nAngelescu, Radu. GutenbergPy. https://github.com/raduangelescu/gutenbergpy, 2013. Ver-\nsion 0.3.5 [accessed August 2023].\n20\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Z. Chen, Eric Chu, J. Clark, Laurent El\nShafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James\nBradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Cr\u00e9py, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, M. C. D\u2019iaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan\nFeng, Vlad Fienber, Markus Freitag, Xavier Garc\u00eda, Sebastian Gehrmann, Lucas Gonz\u00e1lez, Guy\nGur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy\nHurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy,\nMaxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li\nLi, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu,\nMarcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra\nRos, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel\nSmilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran\nVodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu,\nYunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei\nZhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report. ArXiv, abs/2305.10403,\n2023. URL https://api.semanticscholar.org/CorpusID:258740735.\nAnthropic.\nIntroducing Claude.\nhttps://www.anthropic.com/index/introducing-claude,\n2023.\nTuomas\nAura,\nThomas\nA.\nKuhn,\nand\nMichael\nRoe.\nScanning\nelectronic\ndocu-\nments for personally identifiable information.\nAssociation for Computing Machinery,\nInc., October 2006.\nURL https://www.microsoft.com/en-us/research/publication/\nscanning-electronic-documents-for-personally-identifiable-information/.\nTom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni.\nReFinED: An efficient zero-shot-capable approach to end-to-end entity linking. In Proceedings\nof the 2022 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies: Industry Track, pages 209\u2013220, Hybrid: Seattle,\nWashington + Online, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/\n2022.naacl-industry.24. URL https://aclanthology.org/2022.naacl-industry.24.\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht\nSharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli,\nZhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged\nAl-shaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev,\nMike Tian-jian Jiang, and Alexander Rush. PromptSource: An integrated development environment\nand repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics: System Demonstrations, pages 93\u2013104, Dublin, Ireland,\nMay 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-demo.9. URL\nhttps://aclanthology.org/2022.acl-demo.9.\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The\npushshift reddit dataset. ArXiv, abs/2001.08435, 2020a. URL https://api.semanticscholar.\norg/CorpusID:210868223.\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The\npushshift reddit dataset. arXiv [cs.SI], January 2020b.\nStella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O\u2019Brien,\nEric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing\nlarge language models across training and scaling. ArXiv, abs/2304.01373, 2023. URL https:\n//api.semanticscholar.org/CorpusID:257921893.\n21\nAbeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti, and Alexandra Sasha Luccioni.\nInto the laions den: Investigating hate in multimodal datasets. ArXiv, abs/2311.03449, 2023a. URL\nhttps://api.semanticscholar.org/CorpusID:265043448.\nAbeba Birhane, Vinay Uday Prabhu, Sanghyun Han, and Vishnu Naresh Boddeti. On hate scaling\nlaws for data-swamps. ArXiv, abs/2306.13141, 2023b. URL https://api.semanticscholar.\norg/CorpusID:259243810.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. PIQA: Reasoning about\nphysical commonsense in natural language. arXiv [cs.CL], November 2019.\nSid Black, Stella Rose Biderman, Eric Hallahan, Quentin G. Anthony, Leo Gao, Laurence Golding,\nHorace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Martin Pieler, USVSN Sai\nPrashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Benqi Wang, and Samuel Weinbach.\nGpt-neox-20b: An open-source autoregressive language model. ArXiv, abs/2204.06745, 2022.\nURL https://api.semanticscholar.org/CorpusID:248177957.\nTerra Blevins and Luke Zettlemoyer. Language contamination helps explains the cross-lingual\ncapabilities of English pretrained models. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 3563\u20133574, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.233.\nSu Lin Blodgett, Lisa Green, and Brendan O\u2019Connor. Demographic dialectal variation in social\nmedia: A case study of African-American English. In Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing, pages 1119\u20131130, Austin, Texas, November\n2016. Association for Computational Linguistics. doi: 10.18653/v1/D16-1120. URL https:\n//aclanthology.org/D16-1120.\nBurton H Bloom. Space/time trade-offs in hash coding with allowable errors. Communications of the\nACM, 13(7):422\u2013426, July 1970. ISSN 0001-0782,1557-7317. doi: 10.1145/362686.362692.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Mil-\nlican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark,\nDiego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron\nHuang, Loren Maggiore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geof-\nfrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Lau-\nrent Sifre. Improving language models by retrieving from trillions of tokens. In Kamalika\nChaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, edi-\ntors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of\nProceedings of Machine Learning Research, pages 2206\u20132240. PMLR, 17\u201323 Jul 2022. URL\nhttps://proceedings.mlr.press/v162/borgeaud22a.html.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. A large annotated\ncorpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632\u2013642, 2015.\nA Z Broder. On the resemblance and containment of documents. In Proceedings. Compression and\nComplexity of SEQUENCES 1997 (Cat. No.97TB100171), pages 21\u201329. IEEE Comput. Soc, 2002.\nISBN 9780818681325. doi: 10.1109/sequen.1997.666900.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler,\nJeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:218971783.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-snli: Natural\nlanguage inference with natural language explanations. Advances in Neural Information Processing\nSystems, 31, 2018.\n22\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang. Quantifying memorization across neural language models. arXiv [cs.LG], February 2022a.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tram\u00e8r, and Chiyuan\nZhang. Quantifying memorization across neural language models. ArXiv, abs/2202.07646, 2022b.\nURL https://api.semanticscholar.org/CorpusID:246863735.\nTommaso Caselli, Valerio Basile, Jelena Mitrovi\u00b4c, and Michael Granitzer. HateBERT: Retraining\nBERT for abusive language detection in English. In Proceedings of the 5th Workshop on Online\nAbuse and Harms (WOAH 2021), pages 17\u201325, Online, August 2021. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/2021.woah-1.3. URL https://aclanthology.org/2021.\nwoah-1.3.\nKent K. Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman. Speak, memory: An\narchaeology of books known to chatgpt/gpt-4. ArXiv, abs/2305.00118, 2023. URL https:\n//api.semanticscholar.org/CorpusID:258426273.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code, 2021a.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code. July 2021b.\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,\nXuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, and Quoc V Le. Symbolic discovery of\noptimization algorithms. February 2023a.\nYang Chen, Ethan Mendes, Sauvik Das, Wei Xu, and Alan Ritter. Can language models be instructed\nto protect personal information? arXiv [cs.CL], October 2023b.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M.\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton C. Hutchinson, Reiner Pope,\nJames Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm\nLevskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra,\nKevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\nnan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S. Meier-Hellstern,\nDouglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with\npathways. ArXiv, abs/2204.02311, 2022. URL https://api.semanticscholar.org/CorpusID:\n247951931.\n23\nAlexandra Chronopoulou, Matthew Peters, and Jesse Dodge. Efficient hierarchical domain adaptation\nfor pretrained language models. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n1336\u20131351, Seattle, United States, July 2022. Association for Computational Linguistics. doi:\n10.18653/v1/2022.naacl-main.96. URL https://aclanthology.org/2022.naacl-main.96.\nHyung Won Chung, Noah Constant, Xavier Garc\u00eda, Adam Roberts, Yi Tay, Sharan Narang, and\nOrhan Firat. Unimax: Fairer and more effective language sampling for large-scale multilingual pre-\ntraining. ArXiv, abs/2304.09151, 2023. URL https://api.semanticscholar.org/CorpusID:\n258187051.\ncjadams, Jeffrey Sorensen, Julia Elliott, Lucas Dixon, Mark McDonald, nithum, and Will Cukier-\nski. Toxic comment classification challenge, 2017. URL https://kaggle.com/competitions/\njigsaw-toxic-comment-classification-challenge.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina\nToutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044, 2019.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. Think you have solved question answering? try ARC, the AI2 reasoning challenge.\nMarch 2018.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John\nSchulman. Training verifiers to solve math word problems. ArXiv, abs/2110.14168, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:239998651.\nCommon\nCrawl.\ncc-crawl-statistics.\nhttps://github.com/commoncrawl/\ncc-crawl-statistics, 2016. [accessed August 2023].\nCreative Commons. Attribution-ShareAlike 4.0 International. https://creativecommons.org/\nlicenses/by-sa/4.0/legalcode, 2013. [accessed August 2023].\nJenny L Davis and Timothy Graham. Emotional consequences and attention rewards: the social\neffects of ratings on reddit. Information, communication and society, 24(5):649\u2013666, April 2021.\nISSN 1369-118X,1468-4462. doi: 10.1080/1369118x.2021.1874476.\nFranck Dernoncourt, Ji Young Lee, Ozlem Uzuner, and Peter Szolovits. De-identification of patient\nnotes with recurrent neural networks. Journal of the American Medical Informatics Association:\nJAMIA, 24(3):596\u2013606, May 2017. ISSN 1067-5027,1527-974X. doi: 10.1093/jamia/ocw156.\nJesse Dodge, Maarten Sap, Ana Marasovi\u00b4c, William Agnew, Gabriel Ilharco, Dirk Groeneveld,\nMargaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the\ncolossal clean crawled corpus. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 1286\u20131305, Online and Punta Cana, Dominican Republic,\nNovember 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.\n98. URL https://aclanthology.org/2021.emnlp-main.98.\nYanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr,\nPete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. What\u2019s in my big data? arXiv\npreprint arXiv:2310.20707, 2023. URL https://arxiv.org/abs/2310.20707.\nAli Farhadi, David Atkinson, Chris Callison-Burch, Nicole DeCario, Jennifer Dumas, Kyle Lo,\nCrystal Nam, and Luca Soldaini. AI2 Response to Notice of Inquiry and Request for Comments,\n2023. URL https://www.regulations.gov/comment/COLC-2023-0006-8762.\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov. From pretraining data to language\nmodels to downstream tasks: Tracking the trails of political biases leading to unfair NLP models. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 11737\u201311762, Toronto, Canada, July 2023. Association for Computational\nLinguistics. URL https://aclanthology.org/2023.acl-long.656.\n24\nLeo Gao. An empirical exploration in quality filtering of text data. CoRR, abs/2109.00698, 2021.\nURL https://arxiv.org/abs/2109.00698.\nLeo Gao, Stella Rose Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile:\nAn 800GB Dataset of Diverse Text for Language Modeling. ArXiv, abs/2101.00027, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:230435736.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and\nGraham Neubig. Pal: Program-aided language models. arXiv preprint arXiv:2211.10435, 2022.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini.\nCreating\ntraining corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 179\u2013188, Vancouver,\nCanada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1017. URL\nhttps://aclanthology.org/P17-1017.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach,\nHal Daum\u00e9 Iii, and Kate Crawford. Datasheets for datasets. Communications of the ACM, 64(12):\n86\u201392, 2021.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealTox-\nicityPrompts: Evaluating neural toxic degeneration in language models. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2020, pages 3356\u20133369, Online, November\n2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.301.\nURL https://aclanthology.org/2020.findings-emnlp.301.\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,\nRadu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, Katie Millican, David Silver, Slav\nPetrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen,\nEmily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard,\nPaul R Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu,\nRyan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub,\nMegha Goel, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo\nDanihelka, Becca Roelofs, Ana\u00efs White, Anders Andreassen, Tamara von Glehn, Lakshman\nYagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette,\nCharlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher,\nFederico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek\nPerz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W Rae, Han Lu, Laurent Sifre, Marcello\nMaggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel\nBarth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Ruibo\nLiu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad,\nAle Jakse Hartman, Martin Chadwick, Gaurav Singh Tomar, Xavier Garcia, Evan Senter, Emanuel\nTaropa, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las\nCasas, Dasha Valter, Connie Tao, Lorenzo Blanco, Adri\u00e0 Puigdom\u00e8nech Badia, David Reitter,\nMianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane\nLabanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Yujing\nZhang, Ravi Addanki, Antoine Miech, Annie Louis, Laurent El Shafey, Denis Teplyashin, Geoff\nBrown, Elliot Catt, Nithya Attaluri, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood,\nAnton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei\nChang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram\nPejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn\nBloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina\nSamangooei, Bogdan Damoc, Alex Kaskasoli, S\u00e9bastien M R Arnold, Vijay Vasudevan, Shubham\nAgrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim,\nSarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian\nLi, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth\nCole, Aakanksha Chowdhery, Dipanjan Das, Dominika Rogozi\u00b4nska, Vitaly Nikolaev, Pablo\nSprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav\nMishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul\nde Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava,\n25\nAnirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn,\nDevendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur\nGuez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu\nWang, Wenhao Jia, Matthew Rahtz, Mai Gim\u00e9nez, Legg Yeung, Hanzhao Lin, James Keeling,\nPetko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James\nQin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur,\nSolomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao\nWang, Fan Yang, Shuo-Yiin Chang, Paul Komarek, Ross McIlroy, Mario Lu\u02c7ci\u00b4c, Guodong Zhang,\nWael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yong Cheng, Yamini Bansal, Siyuan\nQiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein,\nShivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope,\nLoren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor\nTobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige\nBailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong,\nZhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara,\nJay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu,\nRichard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung,\nTimothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek,\nRapha\u00ebl Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao,\nMohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller,\nShereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins,\nTed Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas,\nCarrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen,\nSholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin\nChiu, Jaime Alonso Lorenzo, Lars Lowe Sj\u00f6sund, S\u00e9bastien Cevey, Zach Gleicher, Thi Avrahami,\nAnudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, L\u00e9onard\nHussenot, Livio Baldini Soares, Kate Baumli, Michael B Chang, Adri\u00e0 Recasens, Ben Caine,\nAlexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan\nHorgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, V\u00edctor Campos, Alex Tomala,\nYunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad\nVikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech\nStokowiec, Ce Zheng, Phoebe Thacker, \u00c7a\u02d8glar \u00dcnl\u00fc, Zhishuai Zhang, Mohammad Saleh, James\nSvensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer,\nMarco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong,\nAllan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie\nPellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi,\nRichard Ives, Yana Hasson, Yaguang Li, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze\nWang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer\nHassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal,\nMatthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Raki\u00b4cevi\u00b4c,\nMostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot,\nMatthew Lamm, Nicola De Cao, Charlie Chen, Gamaleldin Elsayed, Ed Chi, Mahdis Mahdieh,\nIan Tenney, Nan Hua, Ivan Petrychenko, Patrick Kane, Dylan Scandinaro, Rishub Jain, Jonathan\nUesato, Romina Datta, Adam Sadovsky, Oskar Bunyan, Dominik Rabiej, Shimu Wu, John Zhang,\nGautam Vasudevan, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy\nZheng, Betty Chan, Pam G Rabinovitch, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar,\nMichael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias,\nAfroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Sahitya Potluri, Jane\nPark, Elnaz Davoodi, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong,\nJong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Zhe Chen, Johnson Jia,\nAnselm Levskaya, Zhenkai Zhu, Chris Gorgolewski, Peter Grabowski, Yu Mao, Alberto Magni,\nKaisheng Yao, Javier Snaider, Norman Casagrande, Paul Suganthan, Evan Palmer, Geoffrey\nIrving, Edward Loper, Manaal Faruqui, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Michael\nFink, Alfonso Casta\u00f1o, Irene Giannoumis, Wooyeol Kim, Miko\u0142aj Rybi\u00b4nski, Ashwin Sreevatsa,\nJennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu\nGaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay\nHoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert\nCui, Tian Lin, Marin Georgiev, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar,\nAlena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa,\n26\nShuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Minnie Lui, Rama Pasumarthi,\nNathan Lintz, Anitha Vijayakumar, Lam Nguyen Thiet, Daniel Andor, Pedro Valenzuela, Cosmin\nPaduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula\nKurylowicz, Sarmishta Velury, Sebastian Krause, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam\nChoo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Tejasi Latkar, Mingyang Zhang, Quoc Le,\nElena Allica Abellan, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad\nKeller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert\nDadashi, Colin Gaffney, Sid Lall, Ken Franko, Egor Filonov, Anna Bulanova, R\u00e9mi Leblond,\nVikas Yadav, Shirley Chung, Harry Askham, Luis C Cobo, Kelvin Xu, Felix Fischer, Jun Xu,\nChristina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Hao Zhou, Alek Dimitriev, Hannah\nForbes, Dylan Banarse, Zora Tung, Jeremiah Liu, Mark Omernick, Colton Bishop, Chintu Kumar,\nRachel Sterneck, Ryan Foley, Rohan Jain, Swaroop Mishra, Jiawei Xia, Taylor Bos, Geoffrey\nCideron, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Petru Gurita, Hila\nNoga, Premal Shah, Daniel J Mankowitz, Alex Polozov, Nate Kushman, Victoria Krakovna,\nSasha Brown, Mohammadhossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom\nNatan, Anhad Mohananey, Matthieu Geist, Sidharth Mudgal, Sertan Girgin, Hui Li, Jiayu Ye,\nOfir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Quan Yuan, Sumit\nBagri, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Aliaksei Severyn,\nJonathan Lai, Kathy Wu, Heng-Tze Cheng, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory\nGreig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy\nTsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu,\nMark Geller, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Andrei Sozanschi,\nDaniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman,\nJohn Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika\nSinha, Alice Talbert, Abhimanyu Goyal, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe\nThornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Sabaer Fatehi, John Wieting, Omar\nAjmeri, Benigno Uria, Tao Zhu, Yeongil Ko, Laura Knight, Am\u00e9lie H\u00e9liou, Ning Niu, Shane\nGu, Chenxi Pang, Dustin Tran, Yeqing Li, Nir Levine, Ariel Stolovich, Norbert Kalb, Rebeca\nSantamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Balaji\nLakshminarayanan, Charlie Deck, Shyam Upadhyay, Hyo Lee, Mike Dusenberry, Zonglin Li,\nXuezhi Wang, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Summer Yue,\nSho Arora, Eric Malmi, Daniil Mirylenka, Qijun Tan, Christy Koh, Soheil Hassas Yeganeh, Siim\nP\u00f5der, Steven Zheng, Francesco Pongetti, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba\nSeyedhosseini, Pouya Tafti, Ragha Kotikalapudi, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu\nYe, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan,\nAaron Parisi, Joe Stanton, Chenkai Kuang, Vinod Koverkathu, Christopher A Choquette-Choo,\nYunjie Li, T J Lu, Abe Ittycheriah, Prakash Shroff, Pei Sun, Mani Varadarajan, Sanaz Bahargam,\nRob Willoughby, David Gaddy, Ishita Dasgupta, Guillaume Desjardins, Marco Cornero, Brona\nRobenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson,\nAlireza Ghaffarkhah, Morgane Rivi\u00e8re, Alanna Walton, Cl\u00e9ment Crepy, Alicia Parrish, Yuan\nLiu, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der\nSalm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u00b4nska,\nDavid Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex\nMorris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher,\nVictor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio\nSanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Adam Kurzrok, Lynette Webb, Sahil Dua,\nDong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer\nShani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay\nKale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee,\nPandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke,\nXiao Ma, Taylan Bilal, Evgenii Eltyshev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika,\nKeyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras,\nMandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima\nPruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu,\nHonglong Cai, Warren Chen, Xianghai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof\nAngermueller, Xiaowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian,\nAnand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, M K Blake, Hongkun Yu,\nAnthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden,\nHarsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder,\n27\nMorgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker\nSchuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco,\nTim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. Gemini: A\nfamily of highly capable multimodal models. arXiv [cs.CL], December 2023.\nAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. SemEval-2012 task 7: Choice of\nplausible alternatives: An evaluation of commonsense causal reasoning. In Eneko Agirre, Johan\nBos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret, editors, *SEM 2012: The\nFirst Joint Conference on Lexical and Computational Semantics \u2013 Volume 1: Proceedings of\nthe main conference and the shared task, and Volume 2: Proceedings of the Sixth International\nWorkshop on Semantic Evaluation (SemEval 2012), pages 394\u2013398, Montr\u00e9al, Canada, 7-8 June\n2012. Association for Computational Linguistics. URL https://aclanthology.org/S12-1052.\nSidney Greenbaum. Ice: The international corpus of english. English Today, 7(4):3\u20137, 1991.\nDirk Groeneveld, Anas Awadalla, Iz Beltagy, Akshita Bhagia, Ian Magnusson, Hao Peng, Oyvind\nTafjord, Pete Walsh, Kyle Richardson, and Jesse Dodge. Catwalk: A unified language model\nevaluation framework for many datasets. arXiv [cs.CL], December 2023.\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord,\nAnanya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson,\nRussell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack\nHessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik,\nCrystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk,\nSaurabh Shah, Will Smith, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan\nLambert, Kyle Richardson, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh\nHajishirzi. OLMo: Accelerating the Science of Language Models. arXiv preprint, 2024.\nRoger Baker Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit\nSteiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil.e Lukovsiut.e, Karina Nguyen,\nNicholas Joseph, Sam McCandlish, Jared Kaplan, and Sam Bowman. Studying large language\nmodel generalization with influence functions. 2023. URL https://api.semanticscholar.\norg/CorpusID:260682872.\nSuchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettle-\nmoyer, and Noah A. Smith. Whose language counts as high quality? measuring language ideologies\nin text data selection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-\nguage Processing, pages 2562\u20132580, Abu Dhabi, United Arab Emirates, December 2022. Associa-\ntion for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.165.\nZayd Hammoudeh and Daniel Lowd. Training data influence analysis and estimation: A survey. ArXiv,\nabs/2212.04612, 2022. URL https://api.semanticscholar.org/CorpusID:254535627.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Ka-\nmar.\nToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate\nspeech detection.\nIn Proceedings of the 60th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 3309\u20133326, Dublin, Ireland, May\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.234. URL\nhttps://aclanthology.org/2022.acl-long.234.\nRajitha Hathurusinghe, Isar Nejadgholi, and Miodrag Bolic. A privacy-preserving approach to\nextraction of personal information through automatic annotation and federated learning.\nIn\nProceedings of the Third Workshop on Privacy in Natural Language Processing, pages 36\u201345,\nOnline, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.privatenlp-1.\n5. URL https://aclanthology.org/2021.privatenlp-1.5.\nKenneth Heafield. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth\nWorkshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland, July 2011.\nAssociation for Computational Linguistics. URL https://aclanthology.org/W11-2123.\nMatthew Henderson, Pawe\u0142 Budzianowski, I\u00f1igo Casanueva, Sam Coope, Daniela Gerz, Girish\nKumar, Nikola Mrk\u0161i\u00b4c, Georgios Spithourakis, Pei-Hao Su, Ivan Vulic, and Tsung-Hsien Wen. A\n28\nrepository of conversational datasets. In Proceedings of the Workshop on NLP for Conversational\nAI, jul 2019. URL https://arxiv.org/abs/1904.06472. Data available at github.com/PolyAI-\nLDN/conversational-datasets.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia\nGuy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre.\nTraining compute-optimal large language models. ArXiv, abs/2203.15556, 2022. URL https:\n//api.semanticscholar.org/CorpusID:247778764.\nJimin Hong, TaeHee Kim, Hyesu Lim, and Jaegul Choo. AVocaDo: Strategy for adapting vocabulary\nto downstream domain. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4692\u20134700, Online and Punta Cana, Dominican Republic, November\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.385. URL\nhttps://aclanthology.org/2021.emnlp-main.385.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov.\nFasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651, 2016a.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efficient\ntext classification. arXiv preprint arXiv:1607.01759, 2016b.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel.\nLarge lan-\nguage models struggle to learn long-tail knowledge.\nIn Andreas Krause, Emma Brunskill,\nKyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett, editors, Proceed-\nings of the 40th International Conference on Machine Learning, volume 202 of Proceed-\nings of Machine Learning Research, pages 15696\u201315707. PMLR, 23\u201329 Jul 2023.\nURL\nhttps://proceedings.mlr.press/v202/kandpal23a.html.\nRodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Bu-\nraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Craw-\nford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph\nGorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey\nKuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan,\nTyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet\nSingh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D. Wade, Linda Wagner,\nLucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine\nVan Zuylen, and Daniel S. Weld Weld. The Semantic Scholar Open Data Platform. arXiv preprint\narXiv:2301.10140, 2023.\nJohn Kirk and Gerald Nelson. The international corpus of english project: A progress report. World\nEnglishes, 2018. URL https://api.semanticscholar.org/CorpusID:150172629.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The Stack: 3 TB of\npermissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.\nHema Swetha Koppula, Krishna P. Leela, Amit Agarwal, Krishna Prasad Chitrapura, Sachin Garg,\nand Amit Sasturkar. Learning url patterns for webpage de-duplication. In Proceedings of the\nThird ACM International Conference on Web Search and Data Mining, WSDM \u201910, page 381\u2013390,\nNew York, NY, USA, 2010. Association for Computing Machinery. ISBN 9781605588896. doi:\n10.1145/1718487.1718535. URL https://doi.org/10.1145/1718487.1718535.\nTaku Kudo.\nSubword regularization: Improving neural network translation models with mul-\ntiple subword candidates.\nIn Proceedings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 66\u201375, Melbourne, Australia,\nJuly 2018. Association for Computational Linguistics.\ndoi: 10.18653/v1/P18-1007.\nURL\nhttps://aclanthology.org/P18-1007.\nTaku Kudo and John Richardson. SentencePiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference\n29\non Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371,\nBrussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/\nD18-2012. URL https://aclanthology.org/D18-2012.\nHugo Laurenccon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro von Werra, Chenghao Mou, Eduardo Gonz\u2019alez Ponferrada, Huu Nguyen,\nJorg Frohberg, Mario vSavsko, Quentin Lhoest, Angelina McMillan-Major, G\u00e9rard Dupont,\nStella Rose Biderman, Anna Rogers, Loubna Ben Allal, Francesco De Toni, Giada Pistilli, Olivier\nNguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas,\nTristan Thrush, S. Longpre, Sebastian Nagel, Leon Weber, Manuel Sevilla Mu\u00f1oz, Jian Zhu,\nDaniel Alexander van Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-\nDios, Aitor Soroa Etxabe, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik\nBose, David Ifeoluwa Adelani, Long Phan, Hieu Trung Tran, Ian Yu, Suhas Pai, Jenny Chim,\nViolette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite.\nThe\nbigscience roots corpus: A 1.6tb composite multilingual dataset. ArXiv, abs/2303.03915, 2023.\nURL https://api.semanticscholar.org/CorpusID:257378329.\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman,\nHady Elsahar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng\nShen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Launay, and Iz Beltagy. What\nlanguage model to train if you have one million GPU hours? In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 765\u2013782, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.findings-emnlp.54.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-\nBurch, and Nicholas Carlini. Deduplicating training data makes language models better. In\nProceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pages 8424\u20138445, Dublin, Ireland, May 2022. Association for Computational\nLinguistics. doi: 10.18653/v1/2022.acl-long.577. URL https://aclanthology.org/2022.\nacl-long.577.\nColin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, and Daniel\nWhitenack. Bloom library: Multimodal datasets in 300+ languages for a variety of downstream\ntasks.\nIn Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 8608\u20138621, Abu Dhabi, United Arab Emirates, December 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.590.\nHector J. Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In\nProceedings of the Thirteenth International Conference on Principles of Knowledge Representation\nand Reasoning, KR\u201912, page 552\u2013561. AAAI Press, 2012. ISBN 9781577355601. URL https:\n//dl.acm.org/doi/10.5555/3031843.3031909.\nQuentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf, Mario \u0160a\u0161ko, Yacine\nJernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil, Mariama Drame, Julien Chaumond, Julien\nPlu, Joe Davison, Simon Brandeis, Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry,\nSteven Liu, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain\nLesage, Anton Lozhkov, Matthew Carrigan, Th\u00e9o Matussi\u00e8re, Leandro von Werra, Lysandre Debut,\nStas Bekman, and Cl\u00e9ment Delangue. Datasets: A Community Library for Natural Language\nProcessing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 175\u2013184. Association for Computational Linguistics,\nNovember 2021. URL https://aclanthology.org/2021.emnlp-demo.21.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro,\nOleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar\nUmapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason\nStillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,\nNourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,\nMaxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\n30\nSchlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel\nFried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean M. Hughes, Thomas Wolf,\nArjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you! ArXiv,\nabs/2305.06161, 2023. URL https://api.semanticscholar.org/CorpusID:258588247.\nPierre Lison, Ildik\u00f3 Pil\u00e1n, David Sanchez, Montserrat Batet, and Lilja \u00d8vrelid. Anonymisation\nmodels for text data: State of the art, challenges and future directions. In Proceedings of the\n59th Annual Meeting of the Association for Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4188\u20134203,\nStroudsburg, PA, USA, 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nacl-long.323.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. ArXiv, abs/1907.11692, 2019. URL https://api.semanticscholar.org/CorpusID:\n198953378.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 4969\u20134983, Online, July 2020. Association for Computational\nLinguistics. doi: 10.18653/v1/2020.acl-main.447. URL https://aclanthology.org/2020.\nacl-main.447.\nS. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou,\nJason Wei, Kevin Robinson, David M. Mimno, and Daphne Ippolito. A pretrainer\u2019s guide to\ntraining data: Measuring the effects of data age, domain coverage, quality, & toxicity. ArXiv,\nabs/2305.13169, 2023. URL https://api.semanticscholar.org/CorpusID:258832491.\nAlexandra Luccioni and Joseph Viviano. What\u2019s in the box? an analysis of undesirable content\nin the Common Crawl corpus. In Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 182\u2013189, Online, August 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.acl-short.24. URL https://aclanthology.\norg/2021.acl-short.24.\nNils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella-\nB\u00e9guelin. Analyzing leakage of personally identifiable information in language models. arXiv\n[cs.LG], February 2023.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. Language models of\ncode are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 1384\u20131403, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.90.\nInbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. In Proceed-\nings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\nPapers), pages 157\u2013165, Dublin, Ireland, May 2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-short.18. URL https://aclanthology.org/2022.acl-short.18.\nIan Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind\nTafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy,\nHannaneh Hajishirzi, Noah A Smith, Kyle Richardson, and Jesse Dodge. Paloma: A benchmark for\nevaluating language model fit. arXiv [cs.CL], December 2023. URL https://paloma.allen.ai.\nMitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson,\nKaren Katz, and Britta Schasberger. The Penn Treebank: Annotating predicate argument structure.\nIn Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey,\nMarch 8-11, 1994, 1994. URL https://aclanthology.org/H94-1020.\nMitchell P. Marcus, Beatrice Santorini, Mary Ann Marcinkiewicz, and Ann Taylor. Treebank-3, 1999.\nURL https://catalog.ldc.upenn.edu/LDC99T42.\n31\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto\nZamparelli. A SICK cure for the evaluation of compositional distributional semantic models. In Pro-\nceedings of the Ninth International Conference on Language Resources and Evaluation (LREC\u201914),\npages 216\u2013223, Reykjavik, Iceland, May 2014. European Language Resources Association\n(ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf.\nTodor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven\nAdler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in the\nreal world. Proceedings of the ... AAAI Conference on Artificial Intelligence. AAAI Conference\non Artificial Intelligence, 37(12):15009\u201315018, June 2023a. ISSN 2159-5399,2374-3468. doi:\n10.1609/aaai.v37i12.26752.\nTodor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven\nAdler, Angela Jiang, and Lilian Weng. A holistic approach to undesired content detection in\nthe real world. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence\nand Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth\nSymposium on Educational Advances in Artificial Intelligence, AAAI\u201923/IAAI\u201923/EAAI\u201923. AAAI\nPress, 2023b. ISBN 978-1-57735-880-0. doi: 10.1609/aaai.v37i12.26752. URL https://doi.\norg/10.1609/aaai.v37i12.26752.\nSrdjan Matic, Costas Iordanou, Georgios Smaragdakis, and Nikolaos Laoutaris. Identifying sensitive\nurls at web-scale. Proceedings of the ACM Internet Measurement Conference, 2020. URL\nhttps://api.semanticscholar.org/CorpusID:225042878.\nSimona Mazzarino, Andrea Minieri, and Luca Gilli. Nerpii: A python library to perform named\nentity recognition and generate personal identifiable information. 2023.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer Sentinel Mixture\nModels. arXiv preprint arXiv:1609.07843, 2016.\nMicrosoft. Presidio - data protection and de-identification sdk, 2018. URL https://microsoft.\ngithub.io/presidio/.\nMicrosoft. Blingfire: A lightning fast Finite State machine and REgular expression manipulation\nlibrary. https://github.com/microsoft/BlingFire, 2019.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. arXiv [cs.CL], September 2018.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning\ncode large language models. arXiv preprint arXiv:2308.07124, 2023a.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane\nTazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models.\narXiv preprint arXiv:2305.16264, 2023b.\nRoberto Navigli, Simone Conia, and Bj\u00f6rn Ross. Biases in large language models: Origins, inventory,\nand discussion. J. Data and Information Quality, 15(2), jun 2023. ISSN 1936-1955. doi:\n10.1145/3597307. URL https://doi.org/10.1145/3597307.\nDavide Nunes. Preprocessed penn tree bank, 2020. URL https://zenodo.org/record/3910021.\nOfir Press, Noah A Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. August 2021.\nOpen Data Commons.\nOpen Data Commons Attribution License (ODC-By) v1.0.\nhttps://\nopendatacommons.org/licenses/by/1-0/, 2010. Announcement. [accessed August 2023].\nOpenAI.\nGpt-4 technical report.\nArXiv, abs/2303.08774, 2023.\nURL https://api.\nsemanticscholar.org/CorpusID:257532815.\n32\nAntonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy\nBlackburn. Raiders of the lost kek: 3.5 years of augmented 4chan posts from the politically\nincorrect board. 14th International AAAI Conference On Web And Social Media (ICWSM), 2020,\n2020.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aim\u00e9e Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The\nrefinedweb dataset for falcon llm: Outperforming curated corpora with web data, and web data\nonly. ArXiv, abs/2306.01116, 2023. URL https://api.semanticscholar.org/CorpusID:\n259063761.\nJoshua Peterson. openwebtext: Open clone of OpenAI\u2019s unreleased WebText dataset scraper. this\nversion uses pushshift.io files instead of the API for speed, 2020.\nAleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. Language model tokenizers\nintroduce unfairness between languages, 2023.\nAleksandra Piktus, Christopher Akiki, Paulo Villegas, Hugo Lauren\u00e7on, G\u00e9rard Dupont, Sasha\nLuccioni, Yacine Jernite, and Anna Rogers. The ROOTS search tool: Data transparency for LLMs.\nIn Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Volume 3: System Demonstrations), pages 304\u2013314, Toronto, Canada, July 2023. Association for\nComputational Linguistics. URL https://aclanthology.org/2023.acl-demo.29.\nMohammad Taher Pilehvar and Jose Camacho-Collados. Wic: the word-in-context dataset for\nevaluating context-sensitive meaning representations. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers), pages 1267\u20131273, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. February 2019.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hen-\nnigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne\nHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri,\nSaffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan\nMcAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lor-\nraine Li, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki\nLazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug\nFritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama, Cy-\nprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan\nClark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G. Johnson,\nBlake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward Lockhart, Si-\nmon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff Stanway, L. L.\nBennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling language mod-\nels: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446, 2021. URL\nhttps://api.semanticscholar.org/CorpusID:245353475.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the Limits of Transfer Learning with a Unified\nText-to-Text Transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nAnand Rajaraman and Jeffrey David Ullman. Mining of Massive Datasets. Cambridge University\nPress, USA, 2011. ISBN 1107015359.\nYasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining\nterm frequencies on few-shot numerical reasoning. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2022, pages 840\u2013854, Abu Dhabi, United Arab Emirates, Decem-\nber 2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.\nfindings-emnlp.59.\n33\nMachel Reid, Victor Zhong, Suchin Gururangan, and Luke Zettlemoyer. M2D2: A massively\nmulti-domain language modeling dataset. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 964\u2013975, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. URL https://aclanthology.org/\n2022.emnlp-main.63.\nManoel Horta Ribeiro, Jeremy Blackburn, Barry Bradlyn, Emiliano De Cristofaro, Gianluca Stringh-\nini, Summer Long, Stephanie Greenberg, and Savvas Zannettou. The evolution of the manosphere\nacross the web. In Proceedings of the International AAAI Conference on Web and Social Media,\nvolume 15, pages 196\u2013207, 2021.\nMelissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives:\nAn evaluation of commonsense causal reasoning. In 2011 AAAI Spring Symposium Series, 2011.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle\nOtt, Eric Michael Smith, Y-Lan Boureau, and Jason Weston. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of the European Chapter of the Association for\nComputational Linguistics: Main Volume, pages 300\u2013325, Online, April 2021. Association for\nComputational Linguistics. doi: 10.18653/v1/2021.eacl-main.24. URL https://aclanthology.\norg/2021.eacl-main.24.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: An\nadversarial winograd schema challenge at scale. arXiv [cs.CL], July 2019.\nSebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. NLPosi-\ntionality: Characterizing design biases of datasets and models. arXiv [cs.CL], June 2023. doi:\n10.48550/arXiv.2306.01943.\nMaarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A Smith.\nAnnotators with attitudes: How annotator beliefs and identities bias toxic language detection. arXiv\n[cs.CL], November 2021.\nTeven Le Scao, Angela Fan, Christopher Akiki, Elizabeth-Jane Pavlick, Suzana Ili\u2019c, Daniel Hesslow,\nRoman Castagn\u2019e, Alexandra Sasha Luccioni, Franccois Yvon, Matthias Gall\u00e9, Jonathan Tow,\nAlexander M. Rush, Stella Rose Biderman, Albert Webson, Pawan Sasanka Ammanamanchi,\nThomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase,\nRachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile\nSaulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jernite, Julien\nLaunay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Al-\nham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou,\nChris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ife-\noluwa Adelani, Dragomir R. Radev, Eduardo Gonz\u2019alez Ponferrada, Efrat Levkovizh, Ethan Kim,\nEyal Bar Natan, Francesco De Toni, G\u00e9rard Dupont, Germ\u00e1n Kruszewski, Giada Pistilli, Hady\nElSahar, Hamza Benyamina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar\nGonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg\nFrohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle\nLo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan\nDey, Manuel Romero Mu\u00f1oz, Maraim Masoud, Mar\u2019ia Grandury, Mario vSavsko, Max Huang,\nMaximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar,\nMustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar\nEspejel, Ona de Gibert, Paulo Villegas, Peter Henderson, Pierre Colombo, Priscilla A. Amuok,\nQuentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto L\u2019opez, Rui Ribeiro, Salomey\nOsei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya\nSharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi\nTorrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala,\nViolette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling,\nChenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea San-\ntilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han\nWang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika,\nM Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel\nAlbanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault\n34\nF\u00e9vry, Trishala Neeraj, Urmish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing\nSun, Shaked Brody, Y Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason\nPhang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley,\nMax Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas\nPatry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Franccois\nLavall\u2019ee, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena,\nSuraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure\nLigozat, Arjun Subramonian, Aur\u2019elie N\u2019ev\u2019eol, Charles Lovering, Daniel H Garrette, Deepak R.\nTunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra\nWinata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Xiangru\nTang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim,\nNewton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang,\nSebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun,\nTomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun,\nYonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir\nFeizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona\nUnldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh\nBehroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Mu\u00f1oz Ferrandis, Danish\nContractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily\nBaylor, Ezinwanne Ozoani, Fatim Tahirah Mirza, Frankline Ononiwu, Habib Rezanejad, H.A.\nJones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua\nSeltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Macedo Dutra, Mairon Samagaio, Maraim Elbadri,\nMargot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K.\nGhauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju\nSamuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang,\nSourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang,\nZachary Kyle Nguyen, Abhinav Ramesh Kashyap, A. Palasciano, Alison Callahan, Anima Shukla,\nAntonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fon-\nseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u2019on Perin\u2019an,\nDaniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay,\nGiyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John\nGiorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa\nShinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Mar\u00eda Andrea Castillo,\nMarianna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg,\nM Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan\nDahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chan-\ndrasekhar, Renata Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel\nCahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott,\nSinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th\u2019eo\nGigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan\nXu, Ying Xu, Yu Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada,\nand Thomas Wolf. Bloom: A 176b-parameter open-access multilingual language model. ArXiv,\nabs/2211.05100, 2022. URL https://api.semanticscholar.org/CorpusID:253420279.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, Au-\ngust 2016. Association for Computational Linguistics.\ndoi: 10.18653/v1/P16-1162.\nURL\nhttps://aclanthology.org/P16-1162.\nPreethi Seshadri, Sameer Singh, and Yanai Elazar. The bias amplification paradox in text-to-image\ngeneration. arXiv preprint arXiv:2308.00755, 2023.\nNoam Shazeer. GLU variants improve transformer. February 2020.\nLuca Soldaini and Kyle Lo. peS2o (Pretraining Efficiently on S2ORC) Dataset. Technical report,\nAllen Institute for AI, 2023. ODC-By, https://github.com/allenai/pes2o.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W.\n35\nKocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda\nAskell, Amanda Dsouza, Ambrose Slone, Ameet Rahane, Anantharaman S. Iyer, Anders Johan\nAndreassen, Andrea Madotto, Andrea Santilli, Andreas Stuhlm\u00fcller, Andrew M. Dai, Andrew La,\nAndrew Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna\nGottardi, Antonio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes,\nArun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut\nErdem, Ayla Karaka\u00b8s, B. Ryan Roberts, Bao Sheng Loe, Barret Zoph, Bart\u0142omiej Bojanowski,\nBatuhan \u00d6zyurt, Behnam Hedayatnia, Behnam Neyshabur, Benjamin Inden, Benno Stein, Berk\nEkmekci, Bill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron Diao, Cameron Dour, Catherine\nStinson, Cedrick Argueta, Cesar Ferri, Chandan Singh, Charles Rathkopf, Chenlin Meng, Chitta\nBaral, Chiyu Wu, Chris Callison-Burch, Christopher Waites, Christian Voigt, Christopher D\nManning, Christopher Potts, Cindy Ramirez, Clara E. Rivera, Clemencia Siro, Colin Raffel,\nCourtney Ashcraft, Cristina Garbacea, Damien Sileo, Dan Garrette, Dan Hendrycks, Dan Kilman,\nDan Roth, C. Daniel Freeman, Daniel Khashabi, Daniel Levy, Daniel Mosegu\u00ed Gonz\u00e1lez, Danielle\nPerszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito, Dar Gilboa, David Dohan, David\nDrakard, David Jurgens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis Kleyko, Deniz\nYuret, Derek Chen, Derek Tam, Dieuwke Hupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho\nMollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekaterina Shutova, Ekin Dogus Cubuk, Elad\nSegal, Eleanor Hagerman, Elizabeth Barnes, Elizabeth Donoway, Ellie Pavlick, Emanuele Rodol\u00e0,\nEmma Lam, Eric Chu, Eric Tang, Erkut Erdem, Ernie Chang, Ethan A Chi, Ethan Dyer, Ethan\nJerzak, Ethan Kim, Eunice Engefu Manyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh Siar,\nFernando Mart\u00ednez-Plumed, Francesca Happ\u00e9, Francois Chollet, Frieda Rong, Gaurav Mishra,\nGenta Indra Winata, Gerard de Melo, Germ\u00e1n Kruszewski, Giambattista Parascandolo, Giorgio\nMariani, Gloria Xinyue Wang, Gonzalo Jaimovitch-Lopez, Gregor Betz, Guy Gur-Ari, Hana\nGalijasevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi, Harsh Mehta, Hayden Bogar,\nHenry Francis Anthony Shevlin, Hinrich Schuetze, Hiromu Yakura, Hongming Zhang, Hugh Mee\nWong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger, Jackson Kernion, Jacob Hilton, Jaehoon\nLee, Jaime Fern\u00e1ndez Fisac, James B Simon, James Koppel, James Zheng, James Zou, Jan Kocon,\nJana Thompson, Janelle Wingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-Dickstein, Jason\nPhang, Jason Wei, Jason Yosinski, Jekaterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy\nKim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng Xu, Jiaming Song, Jillian Tang, Joan\nWaweru, John Burden, John Miller, John U. Balis, Jonathan Batchelder, Jonathan Berant, J\u00f6rg\nFrohberg, Jos Rozen, Jose Hernandez-Orallo, Joseph Boudeman, Joseph Guerr, Joseph Jones,\nJoshua B. Tenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kanclerz, Karen Livescu, Karl Krauth,\nKarthik Gopalakrishnan, Katerina Ignatyeva, Katja Markert, Kaustubh Dhole, Kevin Gimpel,\nKevin Omondi, Kory Wallace Mathewson, Kristen Chiafullo, Ksenia Shkaruta, Kumar Shridhar,\nKyle McDonell, Kyle Richardson, Laria Reynolds, Leo Gao, Li Zhang, Liam Dugan, Lianhui Qin,\nLidia Contreras-Ochando, Louis-Philippe Morency, Luca Moschella, Lucas Lam, Lucy Noble,\nLudwig Schmidt, Luheng He, Luis Oliveros-Col\u00f3n, Luke Metz, L\u00fctfi Kerem Senel, Maarten\nBosma, Maarten Sap, Maartje Ter Hoeve, Maheen Farooqi, Manaal Faruqui, Mantas Mazeika,\nMarco Baturan, Marco Marelli, Marco Maru, Maria Jose Ramirez-Quintana, Marie Tolkiehn,\nMario Giulianelli, Martha Lewis, Martin Potthast, Matthew L Leavitt, Matthias Hagen, M\u00e1ty\u00e1s\nSchubert, Medina Orduna Baitemirova, Melody Arnaud, Melvin McElrath, Michael Andrew\nYee, Michael Cohen, Michael Gu, Michael Ivanitskiy, Michael Starritt, Michael Strube, Micha\u0142\nSw\u02dbedrowski, Michele Bevilacqua, Michihiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac\nSuzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin Aminnaseri, Mor Geva, Mozhdeh Gheini,\nMukund Varma T, Nanyun Peng, Nathan Andrew Chi, Nayeon Lee, Neta Gur-Ari Krakover,\nNicholas Cameron, Nicholas Roberts, Nick Doiron, Nicole Martinez, Nikita Nangia, Niklas\nDeckers, Niklas Muennighoff, Nitish Shirish Keskar, Niveditha S. Iyer, Noah Constant, Noah\nFiedel, Nuan Wen, Oliver Zhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain Evans,\nPablo Antonio Moreno Casares, Parth Doshi, Pascale Fung, Paul Pu Liang, Paul Vicol, Pegah\nAlipoormolabashi, Peiyuan Liao, Percy Liang, Peter W Chang, Peter Eckersley, Phu Mon Htut,\nPinyu Hwang, Piotr Mi\u0142kowski, Piyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei, Qing\nLyu, Qinlang Chen, Rabin Banjade, Rachel Etta Rudolph, Raefer Gabriel, Rahel Habacker, Ramon\nRisco, Rapha\u00ebl Milli\u00e8re, Rhythm Garg, Richard Barnes, Rif A. Saurous, Riku Arakawa, Robbe\nRaymaekers, Robert Frank, Rohan Sikand, Roman Novak, Roman Sitelew, Ronan Le Bras, Rosanne\nLiu, Rowan Jacobs, Rui Zhang, Russ Salakhutdinov, Ryan Andrew Chi, Seungjae Ryan Lee, Ryan\nStovall, Ryan Teehan, Rylan Yang, Sahib Singh, Saif M. Mohammad, Sajant Anand, Sam Dillavou,\nSam Shleifer, Sam Wiseman, Samuel Gruetter, Samuel R. Bowman, Samuel Stern Schoenholz,\n36\nSanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik Ghazarian, Sayan Ghosh, Sean Casey,\nSebastian Bischoff, Sebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi, Shadi Hamdan,\nSharon Zhou, Shashank Srivastava, Sherry Shi, Shikhar Singh, Shima Asaadi, Shixiang Shane\nGu, Shubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay, Shyamolima Shammie Debnath,\nSiamak Shakeri, Simon Thormeyer, Simone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-\nHwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas Dehaene, Stefan Divic, Stefano Ermon,\nStella Biderman, Stephanie Lin, Stephen Prasad, Steven Piantadosi, Stuart Shieber, Summer\nMisherghi, Svetlana Kiritchenko, Swaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu,\nTariq Ali, Tatsunori Hashimoto, Te-Lin Wu, Th\u00e9o Desbordes, Theodore Rothschild, Thomas Phan,\nTianle Wang, Tiberius Nkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, Tobias Gerstenberg,\nTrenton Chang, Trishala Neeraj, Tushar Khot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera\nDemberg, Victoria Nyamai, Vikas Raunak, Vinay Venkatesh Ramasesh, vinay uday prabhu,\nVishakh Padmakumar, Vivek Srikumar, William Fedus, William Saunders, William Zhang, Wout\nVossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao, Xinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh,\nYair Lakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi Yang, Yiding Hao, Yifu Chen,\nYonatan Belinkov, Yu Hou, Yufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zijian Wang,\nZijie J. Wang, Zirui Wang, and Ziyi Wu. Beyond the imitation game: Quantifying and extrapolating\nthe capabilities of language models. Transactions on Machine Learning Research, 2023. ISSN\n2835-8856. URL https://openreview.net/forum?id=uyTL5Bvosj.\nStability\nAI.\nIntroducing\nStable\nLM\n2\n1.6B.\nhttps://github.com/kingoflolz/\nmesh-transformer-jax, 2024.\nNishant Subramani, Sasha Luccioni, Jesse Dodge, and Margaret Mitchell. Detecting personal informa-\ntion in training corpora: an analysis. In Proceedings of the 3rd Workshop on Trustworthy Natural\nLanguage Processing (TrustNLP 2023), pages 208\u2013220, Toronto, Canada, July 2023. Association\nfor Computational Linguistics. URL https://aclanthology.org/2023.trustnlp-1.18.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh\nSrinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\nRenelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi,\nand Quoc Le. LaMDA: Language models for dialog applications. arXiv [cs.CL], January 2022.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S. Morcos. D4: Improving llm\npretraining via document de-duplication and diversification. ArXiv, abs/2308.12284, 2023. URL\nhttps://api.semanticscholar.org/CorpusID:261076313.\nTogether Computer. Redpajama-data-v2, 10 2023a. URL https://huggingface.co/datasets/\ntogethercomputer/RedPajama-Data-V2.\nTogether Computer. Redpajama-incite-base-3b-v1, 5 2023b. URL https://huggingface.co/\ntogethercomputer/RedPajama-INCITE-Base-3B-v1.\nTogether Computer. Redpajama-data-1t, 4 2023c. URL https://huggingface.co/datasets/\ntogethercomputer/RedPajama-Data-1T.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. ArXiv, abs/2302.13971, 2023a. URL https://api.semanticscholar.org/CorpusID:\n257219404.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes,\n37\nJeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S.\nHartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel M. Kloumann, A. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar\nMishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey\nEdunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models. ArXiv,\nabs/2307.09288, 2023b. URL https://api.semanticscholar.org/CorpusID:259950998.\nBertie Vidgen and Leon Derczynski. Directions in abusive language training data, a systematic\nreview: Garbage in, garbage out. PloS one, 15(12):e0243300, December 2020. ISSN 1932-6203.\ndoi: 10.1371/journal.pone.0243300.\nEric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. Concealed data poisoning attacks on NLP\nmodels. In Proceedings of the 2021 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, pages 139\u2013150, Online, June\n2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.13. URL\nhttps://aclanthology.org/2021.naacl-main.13.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language\nunderstanding systems. Advances in neural information processing systems, 32, 2019.\nBen Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://stability.ai/news/introducing-stable-lm-2, May 2021.\nJohannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions.\narXiv [cs.HC], July 2017.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne\nHendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. Challenges in\ndetoxifying language models. In Findings of the Association for Computational Linguistics:\nEMNLP 2021, pages 2447\u20132469, Punta Cana, Dominican Republic, November 2021. Association\nfor Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.210. URL https://\naclanthology.org/2021.findings-emnlp.210.\nTim Weninger, Xihao Avi Zhu, and Jiawei Han. An exploration of discussion threads in social news\nsites. In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social\nNetworks Analysis and Mining, New York, NY, USA, August 2013. ACM. ISBN 9781450322409.\ndoi: 10.1145/2492517.2492646.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n,\nArmand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,\npages 4003\u20134012, Marseille, France, May 2020a. European Language Resources Association.\nISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n,\nArmand Joulin, and \u00c9douard Grave. Ccnet: Extracting high quality monolingual datasets from\nweb crawl data. In Proceedings of The 12th Language Resources and Evaluation Conference,\npages 4003\u20134012, 2020b.\nJason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. Towards ai-complete question\nanswering: A set of prerequisite toy tasks. arXiv: Artificial Intelligence, 2015. URL https:\n//api.semanticscholar.org/CorpusID:3178759.\n38\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. Detoxify-\ning language models risks marginalizing minority voices. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2390\u20132397, Online, June 2021. Association for Computational Linguistics. doi:\n10.18653/v1/2021.naacl-main.190. URL https://aclanthology.org/2021.naacl-main.190.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv\n[cs.CL], October 2020.\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking\nbenchmark and contamination for language models with rephrased samples. ArXiv, abs/2311.04850,\n2023. URL https://api.semanticscholar.org/CorpusID:265050721.\nYelp. Detect secrets. https://github.com/Yelp/detect-secrets, 2013. v1.4.0.\nSavvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos,\nGianluca Stringini, and Jeremy Blackburn. What is gab: A bastion of free speech or an alt-\nright echo chamber. In Companion Proceedings of the The Web Conference 2018, WWW \u201918,\npage 1007\u20131014, Republic and Canton of Geneva, CHE, 2018. International World Wide Web\nConferences Steering Committee. ISBN 9781450356404. doi: 10.1145/3184558.3191531. URL\nhttps://doi.org/10.1145/3184558.3191531.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\nreally finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for\nComputational Linguistics, 2019.\nHao Zhang. Language model decomposition: Quantifying the dependency and correlation of language\nmodels. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing, pages 2508\u20132517, Abu Dhabi, United Arab Emirates, December 2022. Association for\nComputational Linguistics. URL https://aclanthology.org/2022.emnlp-main.161.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. TinyLlama: An open-source small\nlanguage model. arXiv [cs.CL], January 2024.\nA\nAcknowledgements\nDolma would not have been possible without the support of many individuals and institutions. The\nexperimental components of this work were made possible through a partnership with AMD and CSC,\nenabling use of the LUMI supercomputer. We thank Jonathan Frankle, Cody Blakeney, Matthew\nLeavitt and Daniel King and the rest of the MosaicML team for sharing findings from experiments\non preliminary versions of our data. We thank Vitaliy Chiley for messaging us on Twitter with a\nsuggestion for resolving a random number generator bug that was affecting our data shuffling. We\nthank Erfan Al-Hossami, Shayne Longpre, and Gregory Yauney for sharing findings from their own\nlarge-scale pretraining data experiments. We thank Ce Zhang and Maurice Weber of Together AI\nfor thoughtful discussion on open datasets and data distribution format. We thank Stella Biderman\nand Aviya Skowron for discussions around data licensing and data processing framework. We thank\nour teammates at AI2 Nicole DeCario, Matt Latzke, Darrell Plessas, Kelsey MacMillan, Carissa\nSchoenick, Sam Skjonsberg, and Michael Schmitz for their help with the website, design, internal\nand external communications, budgeting, and other activities that supported smooth progress on\nthis project. Finally, we also express gratitude for the helpful discussions and feedback from our\nteammates at AI2 and close collaborators, including Prithviraj (Raj) Ammanabrolu, Maria Antoniak,\nChris Callison-Burch, Peter Clark, Pradeep Dasigi, Nicole DeCario, Doug Downey, Ali Farhadi,\nSuchin Gururangan, Sydney Levine, Maarten Sap, Ludwig Schmidt, Will Smith, Yulia Tsvetkov, and\nDaniel S. Weld.\nB\nAuthor Contributions\nDolma would not be possible without the help of our many teammates and collaborators. Weekly\nproject meetings, messaging apps and documentation were accessible for anyone at AI2. Major\n39\ndecisions about Dolma were often made in these channels, with exception for certain topics (e.g.,\nlegal, funding). While many were involved in the Dolma effort (see Acknowledgements \u00a7A), the\nauthors of this paper were those who owned and delivered a critical piece of the puzzle. We detail\ntheir contributions below (authors in alphabetical order):\nContributors to data acquisition and source-specific data processing include Akshita Bhagia, Dirk\nGroeneveld, Rodney Kinney, Kyle Lo, Dustin Schwenk, and Luca Soldaini. Everyone contributed\nto literature review on available sources and best practices and decisions around sources to pursue.\nAkshita Bhagia, Rodney Kinney, Dustin Schwenk, and Luca Soldaini handled the bulk of data\nacquisition and processing and ablation experiments with 1B models for source-specific design\ndecisions. Kyle Lo and Luca Soldaini handled discussions with legal to inform our choice of sources.\nContributors to infrastructure and tooling include Russell Authur, Dirk Groeneveld, Rodney Kinney,\nKyle Lo, and Luca Soldaini. Rodney Kinney, Kyle Lo, and Luca Soldaini designed and implemented\nthe shared toolkit used for processing our corpus at scale. Dirk Groeneveld wrote the Bloom filter for\ndeduplication and decontamination. Russell Authur wrote a toolkit for acquisition and storage of\nCommon Crawl data.\nContributors to source-agnostic data processing include Khyathi Chandu, Yanai Elazar, Rodney\nKinney, Kyle Lo, Xinxi Lyu, Ian Magnusson, Aakanksha Naik, Abhilasha Ravichander, Zejiang Shen,\nand Luca Soldaini. Khyathi Chandu, and Aakanksha Naik developed the toxic text filter. Kyle Lo,\nand Xinxi Lyu helped evaluate it. Luca Soldaini developed the language filtering approach. Rodney\nKinney, Zejiang Shen, and Luca Soldaini developed the \u201cquality\u201d filter. Yanai Elazar identified\nrepeating n-gram sequences. Abhilasha Ravichander, Kyle Lo, and Luca Soldaini developed the PII\nfilter. Jesse Dodge and Ian Magnusson developed the evaluation set decontamination approach.\nContributors to ablation experiments include Iz Beltagy, Akshita Bhagia, Jesse Dodge, Dirk Groen-\neveld, Rodney Kinney, Kyle Lo, Ian Magnusson, Matthew Peters, Kyle Richardson, Dustin Schwenk,\nLuca Soldaini, Nishant Subramani, Oyvind Tafjord, and Pete Walsh. This work included designing\nand prioritizing experiments given compute constraints, implementing and running the 1B model\nexperiments, and interpreting results. In particular, Oyvind Tafjord\u2019s work on the evaluation toolkit\nand Pete Walsh\u2019s work on the model implementation were critical.\nContributors to posthoc experiments and analysis on the final Dolma artifacts. Ben Bogin led the\nprobing experiments on 1B model weights to assess impact of differing code mixtures with support\nfrom Kyle Lo and Niklas Muennighoff. Yanai Elazar ran the data analysis tool to summarize and\ndocument Dolma\u2019s composition. Valentin Hofmann led the tokenization fertility analysis with support\nfrom Kyle Lo. Ananya Harsh Jha and Ian Magnusson performed experiments training and evaluating\nbaseline 1B models on other open datasets with support from Luca Soldaini. Sachin Kumar and\nJacob Morrison performed analysis of systematic issues in our choice of language identification\nand toxicity classifiers with support from Kyle Lo. Niklas Muennighoff led analysis of correlation\nbetween different filters employed on Common Crawl data with support from Kyle Lo and Luca\nSoldaini.\nContributors to licensing and release policy include David Atkinson, Jesse Dodge, Jennifer Dumas,\nNathan Lambert, Kyle Lo, Crystal Nam, and Luca Soldaini. David Atkinson, Jesse Dodge, Jennifer\nDumas, and Crystal Nam led the bulk of this, including research into data licenses, risk-level\ndetermination for pretraining data, and defining the release policy. Kyle Lo and Luca Soldaini\nprovided feedback throughout this process and handled technical details needed for the release.\nNathan Lambert provided feedback on release process and handled the actual release strategy,\nparticularly around external communication.\nAll of the contributors above helped with documentation and writing of their respective com-\nponents. In particular, Li Lucy provided an extensive literature review of language models, open\ncorpora and pretraining corpus creation practices. Emma Strubell gave valuable feedback on our\nmanuscript. Nathan Lambert helped with feedback on the blog post and other forms of external-facing\ncommunication about Dolma.\nHannaneh Hajishirzi, Noah Smith, and Luke Zettlemoyer advised on the project, including broad\nstrategy, writing, recruiting and providing resources. As OLMo project leads, Iz Beltagy, Jesse Dodge,\nand Dirk Groeneveld helped with visibility and coordination with other critical OLMo project\nworkstreams. Notably, we credit Noah Smith for coming up with the name Dolma.\n40\nFinally, Kyle Lo and Luca Soldaini led the overall Dolma project and were involved in all aspects,\nincluding project management, planning and design, discussions with legal and ethics commit-\ntees, data and compute partnerships, infrastructure, tooling, implementation, experiments, writ-\ning/documentation, etc.\nC\nDetails about pretraining data behind largest LMs\nWe provide a high-level overview of the pretraining data curation practices (or lack of reporting\ntherof) of the largest LMs to illustrate the need for clear documentation and transparency around\ndataset curation.\nC.1\nLlama 2 (Touvron et al., 2023b)\nTouvron et al. (2023b) provides limited information on pretraining data used for Llama 2; we\nsummarize what we could from gather from their manuscript\u2019s Sections 2.1, 4.1, and A.6:\n1. Corpus size. 2T tokens.\n2. Data provenance. N/A aside from they avoided using Meta user data.\n3. PII. Reported as excluded data from certain websites known to contain high volumes of PII,\nthough what these sites are was not disclosed.\n4. Toxicity. Not explicitly discussed, but appears to not have performed toxicity filtering,\nopting instead to handle toxic text generation in a later training stage. They do report results\nfrom a post hoc analysis in which they used a HateBERT (Caselli et al., 2021) classifier\nfinetuned on ToxiGen (Hartvigsen et al., 2022) to score each document line (and averaged\nto produce a document-level score).\n5. Language ID. Not stated as used in pretraining data curation, but they provide a post hoc\nanalysis of the pretraining dataset using fastText Language ID with a 0.5 threshold for\ndetected language. We assume this is likely the same protocol they used for pretraining data\ncuration as it is also seen in the CCNet library (Wenzek et al., 2020a), which was used for\nLlama (Touvron et al., 2023a).\n6. Quality. N/A.\n7. Deduplication. N/A.\n8. Decontamination. They provide extensive reporting on their deduplication method, which\nrelies on a modified version of the ngram deduplication tool from Lee et al. (2022).\n9. Other. Reported upsampling certain sources, but without further details. They also report a\nsimilar analysis as in PaLM 2 (Anil et al., 2023) on aggregate statistics about demographic\nidentities and English pronouns.\nC.2\nPaLM 2 (Anil et al., 2023)\nAnil et al. (2023) provides limited information on pretraining data used for PaLM 2; we summarize\nwhat we could from gather from their manuscript\u2019s Sections 3 and D1:\n1. Corpus size. Unreported other than it\u2019s larger than what was used to train PaLM (Chowdhery\net al., 2022)\n2. Data provenance. Unreported other than they use web documents, books, code, mathemat-\nics, and conversational data.\n3. PII. Reported as performed filtering, but without further details.\n4. Toxicity. Toxic text identified using Perspective API but lacking details needed for repro-\nduction (i.e., text unit, threshold). No details on removal. They did report tackling toxicity\nthrough the use of control tokens, but do not provide enough details on this method.\n5. Language ID. Reports the most frequent languages included as well as their frequencies.\nLacking details needed for reproduction (i.e., text unit, tools used, threshold).\n41\n6. Quality. Reported as performed filtering, but without further details.\n7. Deduplication. Reported as performed filtering, but without further details.\n8. Decontamination. N/A.\n9. Other. Anil et al. (2023) report aggregated statistics of how often certain demographic iden-\ntities are represented (or not) in the data. Such statistics include identities (e.g., American)\nor English pronouns. These were identified using tools such as KnowYourData or those\navailable on GoogleCloud, but the manuscript lacks specifics necessary for reproduction.\nC.3\nGPT-4 (OpenAI, 2023)\nOpenAI (2023) provides limited information on pretraining data used for GPT-4; we summarize what\nwe could from gather from their manuscript\u2019s Section 2, Appendix C and D, footnotes 5, 6, 10 and\n27, and Sections 1.1 and 3.1 in the System Card:\n1. Corpus size. N/A\n2. Data provenance. N/A aside from reporting that (1) data was sourced from both the Internet\nas well as third-party providers, (2) data was sourced mainly before September 2021 with\ntrace amounts of more recent data, and (3) they included GSM-8K (Cobbe et al., 2021) as a\ntiny fraction of the total pretraining mix.\n3. PII. N/A.\n4. Toxicity. Removed documents that violate their usage policies from pretraining, including\n\u201cerotic content,\u201d using a combination of lexicon-based heuristics and bespoke classifiers\nfollowing Markov et al. (2023b).\n5. Language ID. N/A aside from reporting that the majority of pretraining data is in English.\n6. Quality. N/A.\n7. Deduplication. N/A.\n8. Mixture.\n9. Decontamination. No discussion of decontamination procedures, but instead reported\npost-hoc statistics measuring extent of contamination on professional and academic exams,\nas well as several academic benchmarks. Method for identifying contamination based on\nexact substring match (after removing whitespaces) of a test example against a pretraining\ndata example. They reported some contamination with BIG-Bench (Srivastava et al., 2023).\n10. Other. There are myraid works performing \u201cdata archeology\u201d on GPT-4 that is, attempting to\nglean information about the pretraining data used in GPT-4 through probes for memorization.\nFor example, Chang et al. (2023) show GPT-4 can generate sequences from copyrighted\nbooks. We do not attempt to survey all of these investigative works.\nC.4\nClaude (Anthropic, 2023)\nUnfortunately, we know next to nothing about the pretraining data used for Claude.\nC.5\nLLaMA (Touvron et al., 2023a)\nTouvron et al. (2023a) provides some information on pretraining data used for training LLaMA; we\nsummarize what we could gather from their manuscript\u2019s Section 2.1.\n1. Corpus size. 1.4T tokens.\n2. Data provenance. LLaMA used data with known provenance, including five shards of\nCommonCrawl between 2017 and 2020, C4 (Raffel et al., 2020), GitHub code from Google\nBigQuery public datasets (restricted to Apache, BSD and MIT licenses), Wikipedia dumps\nfrom June to August 2022, Project Gutenberg books, Books3 from The Pile (Gao et al.,\n2020), LaTeX files from arXiv, and StackExchange pages.\n3. PII. N/A.\n42\n4. Toxicity. N/A. Reports evaluation on the RealToxicityPrompts (Gehman et al., 2020)\nbenchmark.\n5. Language ID. Reports use of the CCNet library (Wenzek et al., 2020b), which employs\nfastText (Joulin et al., 2016a) classifiers to remove non-English text (below a 0.5 threshold).\nNo additional language ID reported for C4, GitHub, Books, arXiv, and StackExchange sets.\nFor Wikipedia, reported restriction of pages to those using Latin or Cyrillic scripts: bg, ca,\ncs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk.\n6. Quality. Reports use of the CCNet library (Wenzek et al., 2020b) to remove low-quality\ncontent from CommonCrawl; CCNet uses KenLM (Heafield, 2011), an n-gram language\nmodel to score perplexity of text as a measure of similarity to Wikipedia text. They do not\nreport their chosen threshold for filtering. They also report use of a linear model trained to\nclassify pages as Wikipedia Reference-like or not. They also report light heuristic filtering\nof boilerplate content for GitHub and Wikipedia subsets.\n7. Deduplication. Reports use of the CCNet library (Wenzek et al., 2020b) to identify dupli-\ncated lines for Common Crawl texts, file-level exact match deduplication for GitHub code,\nand deduplicating books with over 90% for Gutenberg and Books3 subsets.\n8. Decontamination. N/A.\n9. Mixture. The manuscript reports a mixture of 67% CommonCrawl, 15% C4, 4.5% GitHub,\n4.5% Wikipedia, 4.5% Books, 2.5% arXiv, and 2.0% StackExchange. Model training was a\nsingle epoch over this mixture except for an upsampling of Wikipedia and Books (2 epochs).\n10. Other.\nC.6\nOPT (Zhang, 2022)\nFrom Zhang (2022)\u2019s manuscript and provided datasheet (Gebru et al., 2021), we summarize the\nfollowing:\nThe OPT model was trained on 180B tokens from data sources with known provenance: the datasets\nused for RoBERTa (Liu et al., 2019), a subset of the Pile (Gao et al., 2020), and the Pushshift Reddit\nDataset (Baumgartner et al., 2020a) as processed by (Roller et al., 2021). They made several notable\nchanges to these sources:\n1. RoBERTa. (Zhang, 2022) updated the CC-News collection up to September 2021.\n2. Pile. (Zhang, 2022) restricted to the following collections: CommonCrawl, DM Math-\nematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and\nWikipedia. (Zhang, 2022) report omission of other Pile subsets due to gradient norm spikes\nat the 1B model scale.\n3. Pushshift Reddit. (Zhang, 2022) restricted to only the longest chain of comments in each\nthread; an operation that reportedly reduced the dataset by 66%.\n(Zhang, 2022) also describe: (1) deduplication using MinHashLSH (Rajaraman and Ullman, 2011)\nwith a Jaccard similarity threshold of 0.95, and (2) language ID filtering to English-only text, though\nthey do not describe the method used.\nThey do not discuss whether they do (or do not) perform any processing for PII, toxicity, quality, or\ndecontamination.\nD\nExperimental Setup\nD.1\nAblation Setup\nFor all data ablations described in this section, we train a 1B parameter model on up to 150B tokens.\nThis is in-line with similar model sizes that have been used for ablations in prior work (Le Scao\net al., 2022). Each model is an decoder-only transformer model with 16 layers, 16 attention heads,\nand 2048 dimensionality. We use ALiBi positional embeddings (Ofir Press et al., 2021), SwiGLU\nactivation (Shazeer, 2020), and mixed precision; model context size is set to 2048 tokens. We use\n43\nEleutherAI\u2019s GPT NeoX tokenizer (Black et al., 2022). The model is trained using the LionW\noptimizer (Chen et al., 2023a) with 1e-4 peak learning rate, warm-up of 2000 steps, cosine decay,\nand 1e-2 weight decay. Batch size was set to 1024. While we set our max number of steps to 95k\n(which is approximately 200B tokens), we conclude our experiments at 150B tokens.\nWe use 64 AMD Instinct MI250X accelerators. Each MI250X accelerator contains two logical nodes;\ntherefore, from the point of view of our training code, our experiments ran on 128 compute units\ngrouped in 16 nodes. Per each logical unit, we use a micro-batch size of 8. We implement our\nexperiments using the OLMo codebase.\nD.2\nPerplexity Evaluation Suite\nDuring training, we keep track of perplexity using an early version of the Paloma benchmark (Mag-\nnusson et al., 2023). Unless otherwise noted references to Paloma indicate this early version. This\nversion of Paloma was derived from the following datasets:\n\u2022 C4 (Raffel et al., 2020; Dodge et al., 2021): Standard contemporary LM pretraining corpus\nautomatically filtered from the April 2019 Common Crawl scrape.\n\u2022 mC4 (Xue et al., 2020); English subset: the English language portion of a pretraining corpus\nautomatically filtered from 71 Common Crawl scrapes.\n\u2022 Pile (Gao et al., 2020), validation set: widely-used used language modeling pretraining corpus;\ncontains documents curated from multiple sources including several non-web sources.\n\u2022 WikiText 103 (Merity et al., 2016): a standard collection of verified \u201cGood\u201d and \u201cFeatured\u201d\narticles on Wikipedia.\n\u2022 Penn Tree Bank (Marcus et al., 1994): widely-used NLP corpus derived from Wall Street Journal\narticles.\n\u2022 M2D2 (Reid et al., 2022), S2ORC subset: papers from Semantic Scholar (Lo et al., 2020) grouped\nby hierarchical academic field categories.\n\u2022 M2D2 (Reid et al., 2022), Wiki subset: Wikipedia articles grouped by hierarchical categories in the\nWikipedia ontology\n\u2022 C4 100 domains (Chronopoulou et al., 2022): balanced samples of the top 100 domains in C4.\n\u2022 Gab (Zannettou et al., 2018): data from 2016-2018 from an alt-right, free-speech-oriented social\nmedia platform that has been shown to contain more hate speech than mainstream platforms.\n\u2022 ICE (Greenbaum, 1991): English from around the world curated by local experts, with subsets for\nCanada, East Africa, Hong Kong, India, Ireland, Jamaica, Philippines, Singapore, and the USA.\n\u2022 Twitter AAE (Blodgett et al., 2016): balanced sets of tweets labeled as African American or\nwhite-aligned English.\n\u2022 Manosphere (Ribeiro et al., 2021): sample of 9 forums where a set of related masculinist ideologies\ndeveloped over the past decade.\n\u2022 4chan (Papasavva et al., 2020): data from 2016-2019 politics subsection of an anonymity-focused\nforum found shown to contain high rates of toxic content.\nIn some experiments we use the finalized version of Paloma released in Magnusson et al. (2023).\nThis contains evaluation data sampled from the following additional datasets:\n\u2022 Dolma (this work), uniform sample: A sample 8,358 documents from the Dolma corpus across all\nof its subsets (13 from books, 1,642 from Common Crawl web pages, 4,545 Reddit submissions,\n450 scientific articles, 1,708 Wikipedia and Wikibooks entries).\n\u2022 RedPajama v1 (Together Computer, 2023a): 1 trillion tokens replication of the LLaMA 1 (Touvron\net al., 2023a) pretraining corpus.\n\u2022 Falcon RefinedWeb (Penedo et al., 2023): A corpus of English sampled from all Common Crawl\nscrapes until June 2023, more aggressively filtered and deduplicated than C4 and mC4-en.\n\u2022 Dolma 100 Subreddits (this work): Balanced samples of the top 100 subreddits by number of\nposts, sourced from the Dolma Reddit subset.\n\u2022 Dolma 100 Programming Languages (this work): Balanced samples of the top 100 programming\nlanguages by number of tokens, sourced from the Dolma Stack subset.\n44\nD.3\nDownstream Evaluation Suite\nWe also evaluate models on the following downstream task datasets using the Catwalk frame-\nwork (Groeneveld et al., 2023):\n\u2022 AI2 Reasoning Challenge (Clark et al., 2018): A science question-answering dataset broken into\neasy and challenge subsets. Only the easy subset was used in online evaluations. The challenge\nsubset was, however, included in offline evaluations.\n\u2022 BoolQ (Clark et al., 2019): A reading comprehension dataset consisting of naturally occurring\nyes/no boolean questions and background contexts.\n\u2022 HellaSwag (Zellers et al., 2019): A multiple-choice question-answering dataset that tests situational\nunderstanding and commonsense.\n\u2022 OpenBookQA (Mihaylov et al., 2018): A multiple-choice question-answering dataset modeled on\nopen-book science exams.\n\u2022 Physical Interaction: Question Answering (PIQA) (Bisk et al., 2019): A multiple-choice\nquestion-answering dataset that focuses on physical commonsense and naive physics.\n\u2022 SciQ (Welbl et al., 2017): A crowdsourced multiple-choice question-answering dataset consisting\nof everyday questions about physics, chemistry and biology, among other areas of science.\n\u2022 WinoGrande (Sakaguchi et al., 2019): A dataset of pronoun resolution problems involving various\nforms of commonsense. Modeled after the Winograd challenge of Levesque et al. (2012).\nD.4\nTraining Setup for Olmo-1b\nFor Olmo-1b, we follow the experimental setup outlined for dataset ablation experiments in Ap-\npendix D, with the following differences:\n\u2022 We set the max number of steps to 739,328 (which is roughly 3.1T tokens).\n\u2022 We double the batch size to 2048 and do so by scaling up to 256 compute units (double what we\nused for data ablations).\n\u2022 Due to instabilities we found in the LionW optimizer, we switched to using AdamW.\nE\nConstruction of Conversational Threads in Forums Data\nContent comes from Reddit\u2019s data API in two separate but linked forms: submissions and comments.\nSubmissions are either \"link posts\" to external content (e.g. news articles, blogs, or even multimedia\ncontent) or \"self posts\" (submissions written by the poster meant to initiate a discussion thread on\na topic). Comments are user replies to either the initiating post (top level comments) or to another\nuser\u2019s comment. Posts, top-level comments, and replies to comments form a nested conversational\nthread with a submission post at it\u2019s root and comments branching out into multiple possible dialogue\ntrees.\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n(a) C4 (100 domains) (Magnusson et al., 2023)\n0\n20B\n40B\n60B\n0.3\n0.35\n0.4\n0.45\n0.5\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nHellaSwag\nTotal Tokens\nAccuracy\n(b) HellaSwag (Zellers et al., 2019)\nFigure 14: 1B model ablations for Reddit processing pipeline. Treating submissions and comments\nas independent documents (Atomic content strategy) leads to better results on perplexity (e.g., on C4\nin Figure 14a) and downstream tasks (e.g., HellaSwag in Figure 14b).\n45\nThe tree-like structure of Reddit threads allows for multiple possible data formats depending on how\nthe various components of a thread are combined.\nWe investigate three formats for their potential as LM pretraining data:\n\u2022 Atomic content. This simple format treats all comments and submissions as independent docu-\nments without any structure or connection to the thread they appear in.\n\u2022 Partial threads. This format assembles comments from the same thread into a structured, multi-\nround dialogue between users. Submissions are left as separate documents. Assembled dialogues\nare limited to a maximum parent depth, and the resulting documents are only snippets of a their\noriginating thread (which are spread across several documents).\n\u2022 Full threads. This complex format combines a given submission and all of its child comments\ninto a single document encompassing an entire thread. Code-like indentation is used to indicate\nthe depth of a comment in the thread\u2019s hierarchy.\nWe experimentally evaluated these strategies for assembling documents in Figure 14. We found that,\nfor language modeling purposes, treating comments and submissions as atomic units leads to better\ndownstream performance compared to partial and full threads. We hypothesize that the more complex\nformatting required to handle dialogues might introduce undesirable content for language modeling,\nsuch as short and repeated comments. We leave the study of better formatting for forum content for\nlanguage modeling to future work.\nF\nTokenization Analysis\nThe first step of processing text with LMs is tokenization, i.e., mapping the text to a sequence of tokens\nwith corresponding input embeddings (Sennrich et al., 2016; Kudo, 2018; Kudo and Richardson,\n2018). Recently, there has been a growing interest in the question of how well LM tokenizers fit\ndifferent data sources (e.g., data in different languages; Ahia et al., 2023; Petrov et al., 2023) Inspired\nby this emerging line of work, we conduct an explorative analysis of the GPTNeoX tokenizer (Black\net al., 2022) applied to Dolma, which provides a first picture of how challenging the different data\nsources comprised by Dolma are for current LM tokenizers.\nWe start by taking a global look at the tokenizer\u2019s fit to Dolma. Out of the 50,280 tokens in the\ntokenizer vocabulary, 50,057 are present in the tokenized text of Dolma. In other words, 223 tokens\nare never used, amounting to roughly 0.4% of the tokenizer vocabulary. The 223 tokens mostly\nconsist of combinations of whitespace characters (e.g., \u201c\\n\\n \u201d, two newline characters followed by\ntwo blank space characters). Note that when training an LM with the examined tokenizer on Dolma,\nthe input embeddings corresponding to these tokens would not be updated. In terms of the count\ndistribution of tokens, we find that tokens with smaller IDs tend to have higher counts in Dolma (see\nFigure 15a), which is also reflected by a strong Spearman\u2019s correlation between (i) the ranking of\ntokens based on their counts in Dolma and (ii) the token IDs (r = 0.638, p < 0.001). Given how the\ntokenizer was trained (Sennrich et al., 2016; Black et al., 2022), smaller IDs correspond to byte pairs\nmerged earlier and hence tokens occurring more frequently in the tokenizer training data Overall,\nthese results suggest a good fit of the GPTNeoX tokenizer to Dolma.\nDoes the tokenizer fit all data sources included in Dolma equally well? To examine this question, we\nanalyze fertility, which is defined as the average number of tokens per word generated by a tokenizer\n(Acs, 2019; Scao et al., 2022), in our case measured on a specific data source. We find that fertility is\nsimilar for most data sources, ranging between 1.15 (conversational forum subset) and 1.28 (books\nsubset), with the exception of the code subset, which has a substantially higher fertility of 2.45 (see\nFigure 15b). This means that the costs of processing the code subset \u2014 be they computational or\nfinancial in nature (Petrov et al., 2023) \u2014 are more than twice as high compared to the other data\nsources.\nWhat causes this discrepancy? We find that in the code subset (which mostly contains code), words\nare often preceded by whitespace characters other than a blank space (e.g., newline, tab, return).\nCrucially, while a blank space before a word is tokenized as part of that word (e.g., I love you\n\u2192 \u201cI\u201d, \u201c love\u201d, \u201c you\u201d), other whitespace characters yield separate tokens (e.g., I\nlove\nyou \u2192\n\u201cI\u201d, \u201c\\t\u201d, \u201clove\u201d, \u201c\\t\u201d, \u201cyou\u201d). This can also be seen by plotting the relative frequency of tokens\nrepresenting whitespace characters by data source, which is one order of magnitude higher for The\n46\n(a) Count analysis\n(b) Fertility analysis\n(c) Whitespace analysis\nFigure 15: Tokenization analysis. Tokens with small IDs, which have a high count in the tokenizer\ntraining data, also tend to have a high count in Dolma (a). The Stack has a substantially higher fertility\ncompared to the other data sources (b), which can be explained by the higher relative frequency of\nwhitespace characters such \u201c\\n\u201d and \u201c\\t\u201d (c). See text for more details.\nStack compared to most other data sources (see Figure 15c). When training LMs on The Stack (or\ncode more generally), it thus might be advisable to add special tokens to the tokenizer (e.g., \u201c\\nif\u201d;\nHong et al., 2021). It is important to notice that this observation applies to most tokenizers in use\ntoday (e.g., the tokenizer used by GPT-4), which tend to lack tokens such as \u201c\\nif\u201d.\nG\nEvaluating Language Identification\nTo analyze the impact of the fastText language identification classifier, we ran an external audit on\nthe International Corpus of English (ICE) (Kirk and Nelson, 2018), a dataset containing spoken and\nwritten English from nine countries around the world. We ran our language ID tool on all documents\nin the ICE dataset to estimate how many documents from each region would have been erroneously\nfiltered. The ground truth in this analysis is that every document is in English, and should be classified\nas such. Interestingly, we found that at our fairly permissive threshold (keeping documents with at\nleast a 0.5 score for English) correctly identified all English-language documents in ICE each as\nEnglish, no matter the region it was from.\nH\nEvaluating Toxicity Classification\nTo measure dialectal biases in the jigsaw toxicity classifier, we analyze its proclivity to predict English\nvariations spoken in different countries as toxic. Starting with the unfiltered Reddit corpus, we create\na dataset of comments from location-based subreddits, filtering for country-specific subreddits with\n47\nFigure 16: Percentage of English-language documents in the International Corpus of English\n(ICE) (Kirk and Nelson, 2018) that would be misidentified as non-English as a result of thresh-\nolding the fastText classifier\u2019s predicted English score. We find a majority of English documents in\nICE remain identified as English even with a threshold of 0.90.\nFigure 17: Distribution of Reddit comments labeled as toxic by English variation.\nmore than 50K comments. This dataset serves as a crude proxy for different dialects of English,\nassuming most commenters live in the respective locations and speak the variation. We further\nassume the fraction of actually toxic comments in each of these subreddits to be roughly the same.\nWe compute the toxicity score for each comment in this dataset using the jigsaw classifier and report\nthe percentage of comments marked as toxic against different classifier thresholds in Figure 17. For\nall thresholds, for any two locations, we find <5% difference in the fraction of comments marked as\ntoxic suggesting little to no bias. Further, we plot the distribution of toxicity scores for comments in\neach subreddit and find that scores assigned to the comments often fall at the extremes (close to 0 or\nclose to 1), suggesting that any reasonable threshold (lying between 0.1 to 0.9) to predict toxicity will\nlead to similar outcomes.\nI\nAnalysis of Filters for Code Pipeline\nIn Table 7, we display the number of documents flagged by our two groups of filters for The Stack, as\nwell as their correlation. We find that the RedPajama v1 filters flag significantly more documents than\nthe StarCoder ones for most languages. However, for Java, JavaScript and Python, our filters derived\nfrom StarCoder flag a very large number of documents. This is because it contains an additional Code\n48\nto Text ratio filter that is not employed for other languages. The two groups of filters generally have\nlow correlation with the exception of a few languages, such as txl where they are perfectly correlated.\nLanguage\nRPJ %\nSC %\nRPJ SC\nLanguage\nRPJ %\nSC %\nRPJ SC\nFlag\nFlag\nCorr.\nFlag\nFlag\nCorr.\nabap\n1.4\n0.0\nN/A\nlookml\n0.0\n0.0\nN/A\nactionscript\n1.3\n0.0\nN/A\nlsl\n3.2\n1.3\n0.05\nada\n1.5\n2.6\n-0.02\nlua\n4.6\n0.0\nN/A\nagda\n25.4\n0.0\nN/A\nm\n35.1\n0.0\nN/A\nags-script\n4.7\n0.0\nN/A\nm4\n2.7\n0.1\n0.003\nalloy\n3.5\n0.1\n-0.005\nmakefile\n2.3\n0.0\nN/A\nampl\n24.0\n0.0\nN/A\nmako\n2.3\n0.7\n-0.013\nantlr\n6.0\n0.0\nN/A\nmaple\n18.2\n44.2\n-0.414\napacheconf\n0.5\n0.0\nN/A\nmarkdown\n8.0\n0.0\nN/A\napi-blueprint\n3.8\n0.0\nN/A\nmask\n16.6\n0.0\nN/A\napl\n28.2\n0.0\nN/A\nmathematica\n66.3\n0.0\nN/A\napplescript\n2.1\n0.0\nN/A\nmatlab\n94.7\n0.0\nN/A\narc\n17.7\n8.8\n-0.144\nmax\n91.2\n0.1\n-0.033\narduino\n2.5\n0.0\nN/A\nmaxscript\n4.0\n0.5\n-0.014\nasciidoc\n4.0\n0.0\nN/A\nmediawiki\n6.6\n0.0\nN/A\nasp\n16.4\n0.1\n-0.01\nmetal\n5.4\n0.0\nN/A\naspectj\n0.9\n0.0\nN/A\nmirah\n25.3\n0.0\nN/A\nassembly\n50.1\n0.0\nN/A\nmodelica\n10.3\n0.0\nN/A\nats\n5.3\n0.0\nN/A\nmms\n3.2\n0.0\nN/A\naugeas\n7.2\n4.8\n-0.063\nmonkey\n6.5\n0.0\nN/A\nautohotkey\n4.9\n0.0\nN/A\nmoonscript\n5.1\n0.0\nN/A\nautoit\n3.0\n0.0\nN/A\nmtml\n4.5\n2.1\n-0.031\nawk\n36.4\n0.1\n-0.02\nmuf\n18.9\n0.0\nN/A\nbatchfile\n9.8\n0.0\nN/A\nmupad\n13.8\n1.7\n0.006\nbefunge\n100.0\n0.0\nN/A\nmyghty\n27.3\n0.0\nN/A\nbison\n2.8\n0.0\nN/A\nnesc\n7.9\n0.0\nN/A\nbitbake\n0.9\n0.0\nN/A\nnetlinx\n15.4\n0.0\nN/A\nblitzbasic\n56.6\n0.0\nN/A\nnetlogo\n12.5\n0.0\nN/A\nblitzmax\n1.2\n0.0\nN/A\nnginx\n0.0\n0.0\nN/A\nbluespec\n2.8\n0.0\nN/A\nnimrod\n4.5\n0.0\nN/A\nboo\n10.3\n0.3\n0.136\nninja\n36.8\n0.0\nN/A\nbrainfuck\n73.8\n0.3\n-0.003\nnit\n3.4\n0.0\nN/A\nbrightscript\n2.8\n0.0\nN/A\nnix\n1.6\n0.0\nN/A\nbro\n3.3\n0.0\nN/A\nnsis\n3.0\n0.0\nN/A\nc\n3.7\n0.0\nN/A\nnu\n15.1\n0.0\nN/A\nc++\n5.6\n0.0\nN/A\nnumpy\n0.0\n0.0\nN/A\nc-sharp\n0.5\n0.0\nN/A\nobjdump\n77.5\n0.0\nN/A\nc2hs-haskell\n1.7\n0.0\nN/A\nobjective-c++\n5.6\n0.1\n0.023\ncap\u2019n-proto\n4.7\n0.0\nN/A\nobjective-j\n48.7\n0.0\nN/A\ncartocss\n15.9\n0.2\n-0.021\nocaml\n7.8\n0.0\nN/A\nceylon\n2.1\n0.0\nN/A\noctave\n61.2\n3.0\n-0.22\nchapel\n20.4\n0.0\nN/A\nomgrofl\n0.0\n0.0\nN/A\nchuck\n13.0\n0.0\nN/A\nooc\n4.3\n0.0\nN/A\ncirru\n31.0\n0.0\nN/A\nopa\n0.3\n0.0\nN/A\nclarion\n0.6\n0.0\nN/A\nopal\n11.4\n1.9\n-0.05\nclean\n12.0\n0.5\n-0.026\nopencl\n14.6\n0.0\nN/A\nclick\n17.8\n0.3\n-0.024\nopenscad\n31.4\n0.0\nN/A\nclips\n13.9\n0.1\n-0.01\norg\n11.1\n0.1\n0.002\nclojure\n4.7\n0.0\nN/A\nox\n43.6\n8.4\n0.315\ncmake\n2.0\n0.0\nN/A\noxygene\n0.0\n94.5\nN/A\ncobol\n9.8\n0.3\n-0.017\noz\n8.4\n0.2\n-0.012\ncoffeescript\n4.0\n0.0\nN/A\npan\n1.8\n18.0\n0.095\ncoldfusion\n2.5\n1.2\n-0.014\npapyrus\n10.8\n0.1\n0.01\ncoldfusion-cfc\n1.1\n0.0\nN/A\nparrot\n20.0\n0.0\nN/A\n49\ncommon-lisp\n6.4\n0.0\nN/A\nparrot-assembly\n6.0\n0.0\nN/A\ncomponent-pascal\n37.1\n84.1\n0.144\npir\n8.4\n0.0\nN/A\ncoq\n17.5\n0.0\nN/A\npascal\n2.5\n0.0\nN/A\ncreole\n41.8\n0.0\nN/A\npawn\n13.3\n0.0\nN/A\ncrystal\n2.8\n0.1\n-0.006\nperl\n7.8\n0.1\n0.022\ncsound\n6.7\n3.9\n-0.041\nperl6\n15.3\n0.0\nN/A\ncss\n10.9\n0.0\nN/A\nphp\n2.1\n0.0\nN/A\ncsv\n87.2\n0.0\nN/A\npiglatin\n5.5\n0.0\nN/A\ncucumber\n2.3\n0.8\n0.41\npike\n11.9\n0.0\nN/A\ncuda\n2.6\n0.0\nN/A\npod\n3.0\n0.0\nN/A\ncycript\n25.3\n0.0\nN/A\npogoscript\n2.2\n0.0\nN/A\ncython\n2.0\n0.0\nN/A\npony\n18.5\n0.0\nN/A\nd\n15.5\n7.0\n0.008\npostscript\n44.8\n0.0\nN/A\ndarcs-patch\n3.0\n0.0\nN/A\npov-ray-sdl\n36.7\n0.0\nN/A\ndart\n0.9\n0.0\nN/A\npowershell\n2.0\n0.0\nN/A\ndesktop\n0.8\n0.0\nN/A\nprocessing\n12.0\n0.0\nN/A\ndiff\n11.5\n0.0\nN/A\nprolog\n20.2\n0.0\nN/A\ndcl\n42.4\n0.2\n-0.005\npropeller-spin\n9.7\n0.0\nN/A\ndm\n7.7\n0.0\nN/A\nprotocol-buffer\n1.2\n0.0\nN/A\ndns-zone\n56.4\n0.0\nN/A\npure-data\n79.8\n0.1\n-0.035\ndockerfile\n1.5\n0.0\nN/A\npurebasic\n61.9\n0.0\nN/A\ndogescript\n3.3\n0.0\nN/A\npurescript\n2.0\n0.0\nN/A\ndylan\n1.5\n0.0\nN/A\npython\n2.9\n26.3\n0.091\neagle\n82.8\n40.1\n0.076\npt\n33.3\n0.0\nN/A\nec\n10.1\n0.2\n-0.014\nqmake\n4.1\n0.0\nN/A\necere-projects\n4.9\n0.0\nN/A\nqml\n1.2\n0.0\nN/A\necl\n4.3\n0.0\nN/A\nr\n11.2\n0.1\n-0.002\nedn\n36.9\n0.0\nN/A\nracket\n6.4\n0.0\nN/A\neiffel\n22.4\n0.0\nN/A\nrirh\n11.9\n0.1\n-0.009\nelixir\n1.5\n0.0\nN/A\nraml\n2.7\n0.0\nN/A\nelm\n3.7\n0.0\nN/A\nrdoc\n1.8\n0.0\nN/A\nemacs-lisp\n9.1\n0.0\nN/A\nrealbasic\n0.9\n0.0\nN/A\nemberscript\n9.1\n1.1\n-0.016\nrebol\n20.3\n0.1\n-0.018\nerlang\n5.2\n0.0\nN/A\nred\n14.3\n0.2\n-0.02\nf-sharp\n5.1\n0.0\nN/A\nredcode\n20.9\n0.0\nN/A\nfactor\n7.7\n0.0\nN/A\nren\u2019py\n2.3\n0.0\nN/A\nfancy\n9.9\n0.0\nN/A\nrenderscript\n16.1\n0.0\nN/A\nfantom\n5.2\n0.1\n-0.006\nrt\n2.7\n0.2\n-0.002\nfish\n2.4\n0.0\nN/A\nrhtml\n4.2\n0.6\n0.001\nflux\n35.9\n0.0\nN/A\nrmarkdown\n8.1\n0.0\nN/A\nforth\n11.2\n0.0\nN/A\nrf\n0.8\n0.2\n0.117\nfortran\n15.4\n0.0\nN/A\nrouge\n14.8\n0.0\nN/A\nfreemarker\n3.2\n2.8\n0.035\nruby\n1.2\n0.0\nN/A\ng-code\n43.7\n0.1\n-0.004\nrust\n2.6\n0.1\n-0.004\ngams\n53.9\n0.0\nN/A\nsage\n32.1\n0.0\nN/A\ngap\n20.7\n0.0\nN/A\nsaltstack\n1.9\n0.0\nN/A\ngas\n17.6\n0.0\nN/A\nsas\n20.3\n0.0\nN/A\ngdscript\n0.7\n0.0\nN/A\nsass\n2.9\n0.0\nN/A\ngenshi\n9.0\n12.3\n-0.092\nscala\n1.6\n0.0\nN/A\ngentoo-ebuild\n0.3\n0.0\nN/A\nscaml\n5.3\n0.0\nN/A\ngentoo-eclass\n0.5\n0.0\nN/A\nscheme\n15.4\n5.6\n0.011\ngettext-catalog\n1.3\n0.0\nN/A\nscilab\n32.1\n0.7\n-0.058\nglsl\n9.4\n0.5\n-0.015\nscss\n4.5\n0.0\nN/A\nglyph\n0.0\n0.0\nN/A\nself\n5.9\n0.0\nN/A\ngnuplot\n68.9\n0.1\n-0.041\nshell\n5.3\n0.0\nN/A\ngo\n2.0\n0.0\nN/A\nshellsession\n30.0\n0.0\nN/A\ngolo\n1.7\n0.0\nN/A\nshen\n16.3\n0.0\nN/A\ngosu\n3.1\n42.5\n-0.153\nslash\n40.8\n0.0\nN/A\ngrace\n34.5\n0.0\nN/A\nslim\n2.3\n0.0\nN/A\n50\ngf\n11.0\n0.0\nN/A\nsmali\n1.0\n0.0\nN/A\ngraphql\n1.6\n0.0\nN/A\nsmalltalk\n1.6\n0.1\n0.195\ngraphviz-(dot)\n43.1\n0.0\nN/A\nsmarty\n4.4\n0.8\n0.001\ngroff\n19.9\n0.6\n0.009\nsmt\n34.8\n0.0\nN/A\ngroovy\n0.9\n0.0\nN/A\nsolidity\n13.7\n0.0\nN/A\ngsp\n2.5\n0.2\n0.001\nsourcepawn\n13.5\n0.0\nN/A\nhaml\n2.3\n0.0\nN/A\nsparql\n10.1\n0.0\nN/A\nhandlebars\n4.9\n0.1\n0.031\nsqf\n3.3\n0.0\nN/A\nharbour\n5.6\n0.0\nN/A\nsql\n11.0\n0.0\nN/A\nhaskell\n3.4\n0.0\nN/A\nsquirrel\n7.3\n0.0\nN/A\nhaxe\n1.1\n0.0\nN/A\nstan\n15.2\n0.0\nN/A\nhcl\n1.3\n0.0\nN/A\nstandard-ml\n49.8\n0.1\n0.008\nhlsl\n3.8\n0.0\nN/A\nstata\n8.2\n6.1\n-0.073\nhtml\n22.5\n1.9\n0.082\nston\n11.9\n0.0\nN/A\nhtml+django\n5.9\n1.0\n0.001\nstylus\n3.3\n0.0\nN/A\nhtml+eex\n4.7\n0.6\n0.019\nsupercollider\n33.4\n1.6\n-0.066\nhtml+erb\n4.0\n0.4\n0.006\nsvg\n92.5\n49.0\n-0.14\nhtml+php\n3.4\n0.1\n0.002\nswift\n0.6\n0.0\nN/A\nhttp\n4.3\n0.0\nN/A\nsystemverilog\n4.9\n0.0\nN/A\nhy\n9.5\n0.0\nN/A\ntcl\n7.4\n0.0\nN/A\nidl\n74.2\n0.0\nN/A\ntcsh\n6.3\n0.0\nN/A\nidris\n4.1\n0.0\nN/A\ntea\n5.2\n0.0\nN/A\nigor-pro\n2.5\n0.0\nN/A\ntex\n18.6\n0.0\nN/A\ninform-7\n14.0\n0.2\n-0.019\ntext\n56.5\n0.6\n0.061\nini\n8.3\n0.9\n0.027\ntextile\n8.2\n0.0\nN/A\ninno-setup\n2.4\n0.0\nN/A\nthrift\n1.2\n0.0\nN/A\nio\n18.9\n0.1\n0.012\ntoml\n11.9\n0.0\nN/A\nioke\n13.4\n0.0\nN/A\nturing\n4.3\n0.0\nN/A\nirc-log\n39.6\n0.0\nN/A\nturtle\n25.1\n0.0\nN/A\nisabelle\n3.6\n0.1\n-0.007\ntwig\n2.7\n0.2\n0.013\nj\n27.0\n0.0\nN/A\ntxl\n7.4\n7.4\n1.0\njade\n2.5\n0.0\nN/A\ntypescript\n2.2\n0.1\n0.02\njasmin\n20.0\n0.0\nN/A\nupc\n12.3\n0.0\nN/A\njava\n0.7\n30.2\n0.037\nunity3d-asset\n1.1\n0.1\n0.003\njsp\n1.4\n0.6\n-0.003\nuno\n0.8\n0.0\nN/A\njavascript\n9.3\n52.1\n0.13\nunrealscript\n2.6\n0.0\nN/A\njflex\n2.6\n0.3\n0.333\nurweb\n19.2\n1.7\n-0.064\njson\n44.8\n0.0\nN/A\nvala\n1.4\n0.0\nN/A\njson5\n20.8\n0.0\nN/A\nvcl\n6.8\n0.0\nN/A\njsoniq\n29.5\n0.0\nN/A\nverilog\n1.8\n0.0\nN/A\njsonld\n11.6\n0.0\nN/A\nvhdl\n4.1\n0.0\nN/A\njsx\n2.4\n0.0\nN/A\nviml\n2.6\n0.0\nN/A\njulia\n8.6\n0.0\nN/A\nvisual-basic\n4.3\n0.1\n0.07\njupyter-notebook\n62.5\n0.0\nN/A\nvolt\n2.4\n0.1\n-0.005\nkicad\n98.2\n0.0\nN/A\nvue\n3.0\n0.0\nN/A\nkit\n6.3\n0.3\n0.041\nowl\n10.4\n85.6\n-0.146\nkotlin\n0.7\n0.0\nN/A\nwebassembly\n30.3\n0.0\nN/A\nkrl\n2.7\n0.0\nN/A\nwebidl\n0.3\n0.0\nN/A\nlabview\n39.0\n100.0\n0.017\nwisp\n13.8\n0.0\nN/A\nlasso\n33.5\n4.4\n-0.001\nx10\n8.9\n1.0\n-0.031\nlatte\n4.9\n0.4\n0.034\nxbase\n2.5\n0.5\n-0.011\nlean\n8.2\n0.0\nN/A\nxc\n14.2\n0.0\nN/A\nless\n4.7\n0.0\nN/A\nxml\n13.5\n65.3\n-0.016\nlex\n31.8\n0.2\n-0.02\nxojo\n2.3\n0.0\nN/A\nlfe\n10.2\n0.0\nN/A\nxpages\n0.0\n95.6\nN/A\nlilypond\n37.1\n0.0\nN/A\nxproc\n9.9\n59.5\n-0.375\nlinker-script\n10.2\n0.0\nN/A\nxquery\n9.1\n4.4\n-0.065\nliquid\n9.6\n0.6\n0.074\nxs\n1.6\n5.9\n-0.032\nliterate-agda\n23.4\n0.0\nN/A\nxslt\n2.2\n85.1\n-0.041\n51\nlcs\n1.3\n0.0\nN/A\nxtend\n0.3\n0.0\nN/A\nliterate-haskell\n3.8\n0.0\nN/A\nyacc\n17.5\n0.0\nN/A\nlivescript\n12.8\n0.0\nN/A\nyaml\n5.1\n0.0\nN/A\nllvm\n29.9\n0.0\nN/A\nyang\n0.7\n0.0\nN/A\nlogos\n24.2\n0.2\n-0.023\nzephir\n0.4\n0.0\nN/A\nlogtalk\n4.3\n0.0\nN/A\nzig\n4.8\n0.0\nN/A\nlolcode\n14.4\n4.8\n-0.092\nzimpl\n75.5\n0.0\nN/A\nTable 7:\nCorrelation of filters for the subset of Dolma from The Stack.\nRPJ are fil-\nters from RedPajama (Together Computer, 2023c) and SC filters are from StarCoder (Li\net al., 2023; Muennighoff et al., 2023a).\nWe compute the Pearson correlation among the\ndocuments flagged by each set of filters (Corr.).\nLanguage shortcuts: dcl=digital-command-\nlanguage, gf=grammatical-framework, gsp=groovy-server-pages, jsp=java-server-pages, lcs=literate-\ncoffeescript, owl=web-ontology-language, mms=module-management-system, pir=parrot-internal-\nrepresentation, pt=python-traceback, rf=robotframework, rirh=ragel-in-ruby-host, rt=restructuredtext,\nupc=unified-parallel-c\nJ\nData Sheet\nJ.1\nMotivation for Dataset Creation\nWhy was the dataset created?\nDolma was created with the primary purpose of training AI2\u2019s autoregressive language model OLMo.\nIt is a mixture of documents from multiple data sources. Documents have been transformed using a\ncombination of rule-based and statistical tools to extract textual content, remove layout information,\nand filter for English content.\nDolma contains data sourced from different domains. In particular, it contains a mixture of text\nobtained from a web scrape, scientific content extracted from academic PDFs and its associated\nmetadata, code over a variety of programming languages, reference material from Wikipedia and\nWikibooks, as well as public domain books from Project Gutenberg.\nWhat (other) tasks could the dataset be used for?\nWe expect this dataset to be useful to train other language models, either in its current form or through\nfurther filtering and combining it with other datasets.\nBeside language model training, this dataset could be used to study interaction between pretraining\ncorpora and models trained on them. For example, one could study provenance of generations from\nthe model, or perform further corpus analysis.\nSpecific subset of Dolma could be used to train domain specific models. For example, the code subset\ncould be used to train an AI programming assistant.\nAre there obvious tasks for which it should not be used?\nDue to the myriad transformations applied to the original source materials to derive our dataset, we\nbelieve it is ill-suited as a replacement for users seeking to directly consume the original content.\nWe refer users of our dataset to our license and terms on the HuggingFace Hub huggingface.co/\ndatasets/allenai/dolma which detail any use restrictions.\nHas the dataset been used for any tasks already?\nNo model trained on this dataset has been publicly released yet.\nIf so, where are the results so others can compare?\nA manuscript is forthcoming.\nWho funded the creation of the dataset?\n52\nAll individuals who are responsible for this dataset are employed by the Allen Institute for AI.\nSimilarly, computing resources are provided by AI2.\nIf there is an associated grant, provide the grant number.\nCompute for the OLMo project is provided by AMD and CSC, using GPUs on the LUMI supercom-\nputer.\nJ.2\nDataset Composition\nWhat are the instances? Are there multiple types of instances?\nInstances are plain-text spans on English text or computer code. Each instance was obtained by\nprocessing web pages (which might include news, documents, forums, etc), academic articles,\ncomputer code from GitHub, encyclopedic content from Wikipedia, or Project Gutenberg books.\nAre relationships between instances made explicit in the data?\nMetadata for subsets of Dolma could be used to reconstruct relationships between items:\n\u2022 Common Crawl. Each document uses the URL of the web page from which it was extracted as its\nidentifier; therefore, it can be used to identify relationships between documents.\n\u2022 C4. The URL of each web page from which documents were extracted is included as metadata;\ntherefore, it can be used to identify relationships between documents.\n\u2022 Reddit. The originating subreddits and thread ids of documents are included in the metadata.\n\u2022 peS2o. The id of each document is the Semantic Scholar Corpus ID of its corresponding manuscript.\nMetadata for each manuscript can be obtained using the Semantic Scholar APIs (Kinney et al.,\n2023).\n\u2022 The Stack. The name of the GitHub repository each document belongs to is included as metadata.\n\u2022 Project Gutenberg. The title of each book is included as the first line of each document.\n\u2022 Wikipedia, Wikibooks. For both, metadata includes the URL corresponding to the page content\nwas extracted from. Structure and connections between documents can be recovered through the\nURL.\nHow many instances of each type are there?\nSummary statistics are reported in Table 1.\nWhat data does each instance consist of? \u201cRaw\u201d data (e.g., unprocessed text or images)?\nFeatures/attributes?\nFor each source, raw data is not available directly but could be recovered using source-specific\nmethods:\n\u2022 Common Crawl. We obtain data from common crawl snapshots from 2020-05 to 2023-06. WARC\nfiles from Common Crawl can be intersected with Dolma ids to recover original HTML files.\n\u2022 C4. We obtained this corpus from the HuggingFace Hub 33. In turn, documents in C4 have been\nderived from a Common Crawl snapshot for 04/2019. URLs in C4 can be used to recover HTML\nfiles.\n\u2022 Reddit. The complete set of monthly data dumps used in this work are no longer distributed by\nPushshift, however they can still be obtained through torrents and some public web archives.\n\u2022 peS2o. peS2o is derived from S2ORC Lo et al. (2020). Original parsed documents can be obtained\nfrom extracting documents in S2ORC that share the same ID with peS2o. Further, metadata in\nS2ORC can be used to obtain original PDF.\n\u2022 The Stack (deduplicated). The filename and repository name, both available in metadata, can be\nused to recover original file contents.\n33https://huggingface.co/datasets/allenai/c4\n53\n\u2022 Project Gutenberg. The title of each book is the first line of each document.\n\u2022 Wikipedia, Wikibooks. For both, metadata includes the URL corresponding to the page content\nwas extracted from. Structure and connections between documents can be recovered through the\nURL.\nIs there a label/target associated with instances? If the instances are related to people, are\nsubpopulations identified (e.g., by age, gender, etc.) and what is their distribution?\nThere are no labels associated with instances. Many text instances were likely created by people or\ngroups of people, but in the vast majority of cases authorship information is unavailable let alone\nsubpopulation metadata. we leave aggregation and reporting of these statistics to future work.\nIs everything included or does the data rely on external resources? (e.g., websites, tweets,\ndatasets) If external resources, a) are there guarantees that they will exist, and remain constant,\nover time; b) is there an official archival version. Are there licenses, fees or rights associated\nwith any of the data?\nThe data are derived from the web and the original resources may not persist over time. However,\neach source represents an archival snapshot of that data that should remain fixed and available:\n\u2022 Common Crawl. The Common Crawl data is available on Amazon S3 as part of the Amazon\nWeb Services\u2019 Open Data Sponsorship program and can be freely downloaded 34. We followed\nCommon Crawl terms of use35.\n\u2022 C4. This corpus can be obtained from from the HuggingFace Hub33 and is released under ODC-By\n1.0 (Open Data Commons, 2010).\n\u2022 Reddit. Pushshift no longer distributes this dataset due to changes to the Reddit API\u2019s terms.\nUnofficial copies of the data might be be available through torrents and some public web archives.\nPushshift data dumps inherit36 the Terms of use of the Reddit API at the time of their collection\n(March 2023).\n\u2022 peS2o. peS2o is derived from S2ORC Lo et al. (2020). S2ORC is released through the Semantic\nScholar Public API37 under ODC-By 1.0 (Open Data Commons, 2010).\n\u2022 The Stack (deduplicated). The corpus is available on the HuggingFace Hub 38 and consists of\ncode released under a variety of permissive licenses. More details including terms of use for\nhosting or sharing the corpus are provided in the datacard at the link above.\n\u2022 Project Gutenberg. Project Gutenberg consists of books that are not protected under U.S.\ncopyright law. The corpus is available at gutenberg.org.\n\u2022 Wikipedia, Wikibooks. Wikimedia data dumps are freely available39 and released under CC\nBY-SA 4.0 license (Creative Commons, 2013).\nAre there recommended data splits or evaluation measures? (e.g., training, development,\ntesting; accuracy/AUC)\nNo. A separate evaluation suite Dolma as been decontaminated against will be released at a later date.\nDownstream users of this dataset could use any alternative evaluation suite.\nWhat experiments were initially run on this dataset? Have a summary of those results and, if\navailable, provide the link to a paper with more information here.\nA forthcoming manuscript will detail ablations and other experiments that have been conducted to\nguide the creation of this dataset.\n34https://commoncrawl.org/the-data/get-started/\n35https://commoncrawl.org/terms-of-use/\n36https://www.reddit.com/r/pushshift/comments/d6luj5/comment/f0ugpqp\n37https://www.semanticscholar.org/product/api\n38https://huggingface.co/datasets/bigcode/the-stack-dedup\n39https://dumps.wikimedia.org\n54\nJ.3\nData Collection Process\nHow was the data collected? (e.g., hardware apparatus/sensor, manual human curation,\nsoftware program, software interface/API; how were these constructs/measures/methods vali-\ndated?)\nData acquisition for each subset was performed as follows:\n\u2022 Common Crawl. snapshots were downloaded from Common Crawl\u2019s official S3 bucket40 using\nthe cc_net pipeline (Wenzek et al., 2020b). Data was obtained between March 17th and March\n27th, 2023.\n\u2022 C4. We clone C4 from the HuggingFace Hub33 using Git with the Git-LFS extension. Repository\ncloned on May 24th, 2023.\n\u2022 Reddit. Reddit was acquired in the form of monthly data dumps of comments and submissions\ncollected and distributed by the Pushshift project41 42. We used the complete set of 422 publicly\navailible dumps (208 comments, 214 submissions) spanning a period from 06/2005\u201303/2023. The\nmajority of Dumps were acquired in March, 2023 with the last dumps downloaded in May of 2023.\n\u2022 peS2o. We clone peS2o from the HuggingFace Hub43 using Git with the Git-LFS extension. We\nuse pes2o V2. Repository cloned on June 30th, 2023.\n\u2022 The Stack (deduplicated). We clone The Stack (deduplicated) from the HuggingFace Hub38 using\nGit with the Git-LFS extension. Repository cloned on May 28th, 2023.\n\u2022 Project Gutenberg.\nData was downloaded directly from gutenberg.org.\nWe used\nGutenbergPy (Angelescu, Radu, 2013) to extract books. Website accessed on April 3rd, 2023.\n\u2022 Wikipedia, Wikibooks. Dumps were downloaded from Wikimedia\u2019s website39. We use the dump\nfrom March 20th, 2023.\nWho was involved in the data collection process? (e.g., students, crowdworkers) How were they\ncompensated? (e.g., how much were crowdworkers paid?)\nData was collected and postprocessed by full-time employees at the Allen Institute for AI. No\ninstances in this dataset are manually annotated.\nOver what time-frame was the data collected? Does the collection time-frame match the\ncreation time-frame?\nPlease see list above.\nHow was the data associated with each instance acquired? Was the data directly observable\n(e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/derived from other data (e.g., part of speech tags; model-based guesses for age or\nlanguage)? If the latter two, were they validated/verified and if so how?\nAny metadata associated with each instance was obtained directly from each source.\nDoes the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily\nrandom) from a larger set of instances? If the dataset is a sample, then what is the population?\nWhat was the sampling strategy (e.g., deterministic, probabilistic with specific sampling proba-\nbilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why\nnot (e.g., to cover a more diverse range of instances)? How does this affect possible uses?\nSampling for each subset was performed as follows:\n40s3://commoncrawl/\n41https://files.pushshift.io/reddit/submissions/\n42https://files.pushshift.io/reddit/comments/\n43https://huggingface.co/datasets/allenai/peS2o\n55\n\u2022 Common Crawl. Common Crawl is not a representative sample of the web. Summary statistics\nabout Common Crawl are reported through the cc-crawl-statistics (Common Crawl, 2016)\nproject, available at commoncrawl.github.io/cc-crawl-statistics. Dolma uses Common\nCrawl snapshots from 2020-05 to 2023-0644.\n\u2022 C4. We use C4 in its entirety.\n\u2022 Reddit. We use all available Reddit content from from 06/2005\u201303/2023.\n\u2022 The Stack (deduplicated). We use The Stack (deduplicated) in its entirety.\n\u2022 peS2o. We use pes2o V2 in its entirety.\n\u2022 Project Gutenberg. We process all Gutenberg books.\n\u2022 Wikipedia, Wikibooks. We use the English and Simple subset of Wikipedia and Wikibooks in\ntheir entirety.\nIs there information missing from the dataset and why? (this does not include intentionally\ndropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing\nbecause it was unavailable?\nCommon Crawl is the only source we did not use in its entirety. We use only about a quarter of all\nsnapshots available. This amount was deemed sufficient for the goal of the OLMo project (train an\nautoregressive language model with up to 70 billion parameters) given the amount of compute we\nhave available. We decided to use the 24 most recent Common Crawl snapshots.\nAre there any known errors, sources of noise, or redundancies in the data?\nNot that we are aware of, although a negligible portion of Common Crawl data could have been\nlost due to network issues with S3 storage. When accessing Common Crawl, we implemented retry\nmechanisms, but copy could have failed due to exceeding the retry limits.\nJ.4\nData Preprocessing\nWhat preprocessing/cleaning was done? (e.g., discretization or bucketing, tokenization, part-\nof-speech tagging, SIFT feature extraction, removal of instances, processing of missing values,\netc.)\nAll data sources are filtered using FastText language identification models (Joulin et al., 2016a,b)\nwith an English threshold of 0.5.\nFor the Common Crawl and C4 subsets, we use the following filters (Figure 1) that substantially\nmodify the original data. Note that data might be tagged for removal by one or more filter.\n\u2022 Only Common Crawl, as part of their distribution pipeline: Linearize all HTML into plain text\nfiles (WET files generation45);\n\u2022 Only Common Crawl, as part of CCNet pipeline: We remove frequently occurring paragraph\nin Common Crawl by identifying repeated paragraphs on small subsets of each snapshots. This\nstep gets rid of headers that are shared across many pages, such as navigational headers. Removal\nis operationalized as follows: given 1 . . . , n, . . . , N shards each snapshot is comprised to, group\nshards in sets S = {n \u2212 k, n}; then, remove exact duplicates of paragraphs in S. Paragraphs are\ndefined as newline-separated slices of documents, and compared using their SHA1. We choose k\nsuch that each set is at most 20GB46. (approximately 70% of paragraph removed);\n\u2022 Only Common Crawl, deduplication by URL: We deduplicate pages by URL (53% of duplicates\nremoved);\n44Common Crawl snapshots follow naming convention xxxx-yy, where xxxx is the year the snapshot was\nfinalized, and yy is the week, ranging from 01 to 52.\n45https://commoncrawl.org/get-started\n46This is a slight modification of the original CCNet pipeline, where k is chose so that each set is 2% of\nsnapshot. We chose to use a fixed shard size, rather an a percentage of the corpus, because fixed size is more\npredictable in terms of resource usage, leading to less-error prone code. Conceptually it\u2019s equivalent to putting a\nthreshold on the absolute probability of a paragraph occurring\n56\n\u2022 Language identification: remove all documents with an English score lower than 0.5, as deter-\nmined by FastText language identification models (Joulin et al., 2016a,b) (removed 61.69% of web\npages by size);\n\u2022 Quality filter47: Remove documents with more than half of their line not ending in \u201c.\u201d, \u201c?\u201d, \u201c!\u201d,\nor \u201c\"\u201d. (22.73% of characters tagged for removal);\n\u2022 Quality filter47: Remove any document that does not pass any of the Gopher rules (Rae et al.,\n2021) (15.23% of characters tagged for removal);\n\u2013 Fraction of characters in most common ngram greater than a threshold48\n\u2013 Fraction of characters in duplicate ngrams greater than a threshold49\n\u2013 Contains fewer than 50 or more than 100K words\n\u2013 Median word length is less than 3 or greater than 10\n\u2013 Symbol to word ratio greater than 0.10\n\u2013 Fraction of words with alpha character less than 0.80\n\u2013 Contains fewer than 2 of a set of required words50\n\u2013 Fraction of lines in document starting with bullet point greater than 0.90\n\u2013 Fraction of lines in document ending with ellipsis greater than 0.30\n\u2013 Fraction of lines in document that are duplicated greater than 0.30\n\u2013 Fraction of characters in duplicated lines greater than 0.30\n\u2022 Quality filter47: Remove any document that contains a token or sequence of tokens repeating over\n100 times51 (0.003% of characters tagged for removal);\n\u2022 Content filter: Remove sentences that get ranked as toxic by a FastText classifier (score above\n0.4). We train a bigram classifier on the Jigsaw dataset (cjadams et al., 2017) (1.01% of data\ntagged for removal);\n\u2022 Content filter: Mask Personal Identifiable Information (PII) using regular expressions that identify\nemails, phone numbers, and IP addresses; pages containing 6 or more PIIs are completely removed\nfrom the corpus (0.05% tagged for masking, 0.11% tagged for removal);\n\u2022 Exact document deduplication: duplicate documents the same text. No punctuation or whitespace\nis removed. Empty documents count as duplicates (14.9% of documents tagged for removal).\n\u2022 Only Common Crawl, deduplication by paragraph: We deduplicate the web subset at a paragraph\nlevel using a Bloom filter (19.1% of UTF-8 characters tagged for removal).\nFor the Reddit subset, we use the following filters that substantially reduce the original data.\n\u2022 Language identification: remove all documents with an English score lower than 0.5, as deter-\nmined by a FastText language identification model.\n\u2022 Quality filter47: Remove comments and submissions shorter than 500 characters in length.\n\u2022 Quality filter47: Remove user comments with fewer than three upvotes (Reddit users vote on the\nquality of submissions and comments).\n\u2022 Content filter47: Remove comments and submissions from banned, toxic, or NSFW subreddits.\n\u2022 Content filter47: Remove sentences that get ranked as toxic or as hatespeech by a FastText\nclassifier (score above 0.4).\n47The term \u201cquality filter\u201d, while widely used in literature, does not appropriately describe the outcome of\nfiltering a dataset. Quality might be perceived as a comment on the informativeness, comprehensiveness, or\nother characteristics valued by humans. However, the filters used in Dolma and other language models efforts\nselect text according to criteria that are inherently ideological (Gururangan et al., 2022).\n48For bigrams, threshold of 0.20. For trigrams, 0.18. For 4-grams, 0.16.\n49For 5-grams, 0.15. For 6-grams, 0.14. For 7-grams, 0.13. For 8-grams, 0.12. For 9-grams, 0.11. For\n10-grams, 0.10.\n50\u201cthe\u201d, \u201cbe\u201d, \u201cto\u201d, \u201cof\u201d, \u201cand\u201d, \u201cthat\u201d, \u201chave\u201d, \u201cwith\u201d\n51We use allenai/gpt-neox-olmo-dolma-v1_5 to obtain tokens.\n57\n\u2022 Content filter: Mask Personal Identifiable Information (PII) using regular expressions that identify\nemails, phone numbers, and IP addresses\n\u2022 Deduplication: We deduplicate comments and submissions (jointly) at a paragraph level using a\nBloom filter.\nFor the code subset derived from The Stack (deduplicated), we use the following filters (Figure 8):\n\u2022 Language filter: Removed files associated with the following programming languages:\n\u2013 Data or numerical content: csv, json, json5, jsonld, jsoniq, svg\n\u2013 Assembly code: assembly\n\u2022 Quality filter47: Removed copyright statements in code files from document preamble52;\n\u2022 Quality filter47: Removed documents matching any of the RedPajama v1 (Together Computer,\n2023c) code filters (41.49% of data tagged for removal):\n\u2013 Maximum line length > 1000 characters.\n\u2013 Average line length > 100 characters.\n\u2013 Proportion of alpha-numeric characters < 0.25.\n\u2013 Ratio of alphabetical characters to number of tokens < 1.553.\n\u2022 Quality filter47: Removed documents matching any of the following Starcoder filters (Li et al.,\n2023):\n\u2013 Contains XML template code.\n\u2013 HTML code-to-text ratio <= 0.2.\n\u2013 Java, Javascript, Python code-to-comment ratio <= 0.01 or > 0.8.\n\u2022 Content filter: Mask Personal Identifiable Information (PII) using regular expressions that identify\nemails, phone numbers, and IP addresses; pages containing 6 or more PIIs are completely removed\nfrom the corpus.\nFor the Wikipedia and Wikibooks subsets, we remove pages that contain fewer than 25 UTF-8\nwords.\nFor the Gutenberg subset:\n\u2022 Language identification: for each paragraph (defined as newline-separated spans of text), we\nuse FastText to perform language identification. Then, we compute the average language score\nby averaging the score for all passages. If a document has a language score lower than 0.5, it is\ndiscarded;\n\u2022 Quality filter47: we remove pages that contain fewer than 25 UTF-8 words;\n\u2022 Quality filter47: Remove any document that contains a token or sequence of tokens repeating over\n100 times51.\nFor the PeS2o subset, we remove any document that contains a token or sequence of tokens repeating\nover 100 times51 .\nFor Dolma versions 1.0 and 1.5, we perform decontamination for all subsets of Dolma. In particular,\nwe remove paragraphs that are shared with documents in the Paloma evaluation suite Magnusson et al.\n(2023). Overall, only 0.003% of our dataset is removed due to contamination with this evaluation set.\nDolma version 1.6 is not decontaminated.\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned data? (e.g., to support\nunanticipated future uses)\nRaw data is available for all subsets except Common Crawl. Due to space constrains, we only keep\nlinearized version of Common Crawl snapshots, filtered by Language ID as described above.\nRaw data is not available for download outside the Allen Institute for AI. Interested individuals may\ncontact authors of this manuscript if they require access to raw data.\n52Code license and provenance is still tracked in metadata.\n53Tokens counted using whitespace tokenizer\n58\nIs the preprocessing software available?\nYes, all preprocessing software is available on GitHub at github.com/allenai/dolma and on\nPyPI54.\nDoes this dataset collection/processing procedure achieve the motivation for creating the dataset\nstated in the first section of this datasheet?\nYes, it does.\nJ.5\nDataset Distribution\nHow is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived\nredundantly?)\nDolma is distributed via the HuggingFace Hub, which offers access via the datasets (Lhoest et al.,\n2021) Python package, direct download, and Git using the Git-LFS extension. Additionally, a copy is\nstored on the cloud storage of the Allen Institute for AI.\nWhen will the dataset be released/first distributed? (Is there a canonical paper/reference for\nthis dataset?)\nThe dataset is available now. This manuscript serves as a reference for the dataset.\nWhat license (if any) is it distributed under? Are there any copyrights on the data?\nInformation about the license associated with Dolma are available on its release page on the Hugging-\nFace Hub: huggingface.co/datasets/allenai/dolma.\nAre there any fees or access/export restrictions?\nThe dataset is distributed for free. Users should verify any restrictions on its release page on the\nHuggingFace Hub: huggingface.co/datasets/allenai/dolma.\nJ.6\nDataset Maintenance\nWho is supporting/hosting/maintaining the dataset?\nHow does one contact the\nowner/curator/manager of the dataset (e.g. email address, or other contact info)?\nThe Allen Institute for AI maintains the dataset. For support questions, users are invited to open an\nissue on GitHub55 or on the community tab of dataset page56 (the former being preferred over the\nlatter). Any other inquiry should be sent to ai2-info@allenai.org.\nWill the dataset be updated? How often and by whom? How will updates/revisions be docu-\nmented and communicated (e.g., mailing list, GitHub)? Is there an erratum?\nDataset will be uploaded on a need-to basis by maintainers at the Allen Institute for AI. Newer version\nof the dataset will be labeled accordingly. The latest version of the dataset, as well as a changelog,\nwill be made available starting from the first revision.\nIf the dataset becomes obsolete how will this be communicated? Is there a repository to link to\nany/all papers/systems that use this dataset?\nUsers should keep track of the version of the dataset in use. Information about latest version of Dolma\nare available on its release page on the HuggingFace Hub: huggingface.co/datasets/allenai/\ndolma. Dolma users should cite this manuscript when using this data.\n54https://pypi.org/project/dolma/\n55https://github.com/allenai/dolma/issues\n56https://huggingface.co/datasets/allenai/dolma/discussions\n59\nIf others want to extend/augment/build on this dataset, is there a mechanism for them to do so?\nIf so, is there a process for tracking/assessing the quality of those contributions. What is the\nprocess for communicating/distributing these contributions to users?\nCreation and distribution of derivatives is described above. In case contributors want to flow their\nimprovement back to future Dolma releases, they should contact corresponding authors of this\nmanuscript.\nJ.7\nLegal & Ethical Considerations\nIf the dataset relates to people (e.g., their attributes) or was generated by people, were they\ninformed about the data collection? (e.g., datasets that collect writing, photos, interactions,\ntransactions, etc.)\nSubsets of Dolma derived from web data are likely created by people or groups of people, however\nauthorship information is often unavailable.\nAuthors were not directly informed about the data collection. For encyclopedic and web content, logs\nof web servers will contain records of spiders ran by Common Crawl. For academic content, the\npes2o subset (Soldaini and Lo, 2023) is derived from manuscripts that are licensed for permissive\ndistribution by their authors. Reddit content was acquired through a public API adherent to terms of\nservice; individual authors of Reddit posts were not contacted directly. Finally, the Allen Institute for\nAI did not contact Project Gutenberg.\nIf it relates to other ethically protected subjects, have appropriate obligations been met? (e.g.,\nmedical data might include information collected from animals)\nDue to the nature of and size of Dolma, it is impossible to determine which obligations, if any, are\nappropriate.\nIf it relates to people, were there any ethical review applications/reviews/approvals? (e.g.\nInstitutional Review Board applications) If it relates to people, were they told what the dataset\nwould be used for and did they consent? What community norms exist for data collected from\nhuman communications? If consent was obtained, how? Were the people provided with any\nmechanism to revoke their consent in the future or for certain uses?\nThe OLMo project includes Ethics committee comprised of internal and external members to the\nAllen Institute for AI. Plans for the creation of Dolma were reviewed with the committee, and we\nincorporated their recommendations.\nFollowing practices established in similar efforts, no consent was collected from individuals who\nmight be represented in the dataset. We make available a form57 for individuals who wish to be\nremoved from the dataset.\nIf it relates to people, could this dataset expose people to harm or legal action? (e.g., financial\nsocial or otherwise) What was done to mitigate or reduce the potential for harm?\nDolma contains text instances that have been derived from web pages Common Crawl crawled from\nthe web. Content might contain sensitive information including personal information, or financial\ninformation users of the web chose to put publicly online. This data is taken only from public places,\nso the same data is or has been accessible via browsing the web. We have measured a variety of types\nof personal information, and built tools specifically to remove some types of sensitive information,\nand through our license we restrict what users can do with this data.\nWe recommend individuals to submit a request using through our form57 if they wish their information\nto be removed.\nIf it relates to people, does it unfairly advantage or disadvantage a particular social group? In\nwhat ways? How was this mitigated?\n57https://forms.gle/q4BNUUxUxKwKkfdT6\n60\nDolma is not a representative sample of none of its sources. It might underrepresent or overrepresent\nsome communities on the internet; further, papers in the peS2o subset are skewed towards STEM\ndisciplines; books in the Gutenberg library are mostly from the public domain (at the time of\npublication, books published before 1927); finally, the English and Simple subset of Wikipedia and\nWikibooks might be biased towards events and people from the global north.\nWe did not attempt to alter distribution of social groups in Dolma. Large-scale interventions to correct\nsocietal biases in large datasets remain challenging, and are left to future work.\nIf it relates to people, were they provided with privacy guarantees? If so, what guarantees and\nhow are these ensured?\nThis datasets contains text that was derived from web paged scraped by Common Crawl from the web.\nFor much of that data it\u2019s not possible identify the authors. In many instances, creators purposely\nchoose to post anonymously online, so aiming to infer authorship can be ethically fraught. We\nprovide access to our data, and encourage any creators that would likely to have data from or about\nthem removed to reach out.\nDoes the dataset comply with the EU General Data Protection Regulation (GDPR)? Does it\ncomply with any other standards, such as the US Equal Employment Opportunity Act?\nWe created this dataset in aggregate, not separately identifying any individual\u2019s content or information.\nWe took reasonable steps to remove types of personal information that were possible to reliably detect.\nWe restrict who has access to the data, and we release this under a license that prohibits uses that\nmight be deemed discriminatory. We also provide an avenue for any person to contact us to have text\nfrom or about them removed from our corpus57.\nDoes the dataset contain information that might be considered sensitive or confidential? (e.g.,\npersonally identifying information) Does the dataset contain information that might be consid-\nered inappropriate or offensive?\nThis datasets contains text that was derived from web paged scraped by Common Crawl from the\nweb. Therefore, it can contain text posted on public websites by creators on the internet. If an author\npublicly posted personal information or offensive content, it could be included in this dataset. We\ntook reasonable steps to remove types of personal information that were possible to reliably detect.\nWe also removed documents that contained sentences that were classified as being toxic.\nK\nAll Raw Ablation Results\nK.1\nComparing Dolma With Other Corpora\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n10\n20\n30\n40\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\n4chan\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\n30\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nWikiText-103\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n8\n9\n10\n20\n30\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nPile\nTotal Tokens\nPerplexity\nFigure 18: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), WikiText 103 (Merity et al., 2016), and Pile (Gao et al., 2020) (Val)\n61\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\n30\n40\n50\n60\n70\n80\n90\n100\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nPenn Treebank\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n30\n40\n50\n60\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nGAB\nTotal Tokens\nPerplexity\nFigure 19: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 100 dom (Chronopoulou\net al., 2022), Penn Tree Bank (Marcus et al., 1994), and Gab (Zannettou et al., 2018)\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\n30\n40\n50\n60\n70\n80\n90\n100\n200\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nICE\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n40\n50\n60\n70\n80\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nTwitterAEE\nTotal Tokens\nPerplexity\nFigure 20: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991),\nM2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016)\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n30\n40\n50\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nManosphere\nTotal Tokens\nPerplexity\nFigure 21: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al.,\n2021)\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n9\n10\n20\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\n30\n40\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n20\n30\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nC4\nTotal Tokens\nPerplexity\nFigure 22: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020)\n(English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021)\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.27\n0.28\n0.29\n0.3\n0.31\n0.32\n0.33\n0.34\n0.35\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.46\n0.48\n0.5\n0.52\n0.54\n0.56\n0.58\n0.6\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nARC-Easy\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.5\n0.51\n0.52\n0.53\n0.54\n0.55\n0.56\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nWinogrande\nTotal Tokens\nAccuracy\nFigure 23: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n62\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.78\n0.8\n0.82\n0.84\n0.86\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nSciQ\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.35\n0.4\n0.45\n0.5\n0.55\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nHellaSwag\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n0.66\n0.68\n0.7\n0.72\n0.74\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nPIQA\nTotal Tokens\nAccuracy\nFigure 24: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n20B\n40B\n60B\n80B\n100B\n120B\n140B\n2\n3\n4\n5\n6\n7\n8\n9\n10\nRed Pajama v1\nPile\nC4\nmC4 (English)\nRefinedWeb\nDolma v1.5\nTrain\nTotal Tokens\nCross Entropy\nFigure 25: Training Cross Entropy\n63\nK.2\nDeduping Strategy\n0\n50B\n100B\n20\n30\n40\nBaseline\nParagraph Deduplication\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\nBaseline\nParagraph Deduplication\nPile\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nParagraph Deduplication\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 26: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n50B\n100B\n20\n30\n40\nBaseline\nParagraph Deduplication\nC4\nTotal Tokens\nPerplexity\n0\n50B\n100B\n40\n50\n60\n70\nBaseline\nParagraph Deduplication\nManosphere\nTotal Tokens\nPerplexity\nFigure 27: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\nBaseline\nParagraph Deduplication\nGAB\nTotal Tokens\nPerplexity\n0\n50B\n100B\n50\n60\n70\n80\n90\n100\nBaseline\nParagraph Deduplication\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nParagraph Deduplication\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 28: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n50B\n100B\n40\n50\n60\n70\n80\nBaseline\nParagraph Deduplication\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n10\n20\n30\nBaseline\nParagraph Deduplication\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\nBaseline\nParagraph Deduplication\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 29: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n50B\n100B\n0.25\n0.3\n0.35\nBaseline\nParagraph Deduplication\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.35\n0.4\n0.45\n0.5\n0.55\nBaseline\nParagraph Deduplication\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.5\n0.52\n0.54\nBaseline\nParagraph Deduplication\nWinogrande\nTotal Tokens\nAccuracy\nFigure 30: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n64\n0\n50B\n100B\n0.6\n0.7\n0.8\nBaseline\nParagraph Deduplication\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\nBaseline\nParagraph Deduplication\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nBaseline\nParagraph Deduplication\nPIQA\nTotal Tokens\nAccuracy\nFigure 31: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n50B\n100B\n3\n4\n5\n6\n7\n8\n9\n10\nBaseline\nParagraph Deduplication\nTrain\nTotal Tokens\nCross Entropy\nFigure 32: Training Cross Entropy\n65\nK.3\nFiltering of Personal Identifiable Information\n0\n50B\n100B\n20\n30\n40\n50\n60\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nC4 (100 Domains)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nGAB\nTotal Tokens\nPerplexity\nFigure 33: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), C4 100 dom (Chronopoulou et al., 2022), and Gab (Zannettou et al., 2018)\n0\n50B\n100B\n50\n60\n70\n80\n90\n100\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n40\n50\n60\n70\n80\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nTwitterAEE\nTotal Tokens\nPerplexity\nFigure 34: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991),\nM2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016)\n0\n50B\n100B\n40\n50\n60\n70\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nManosphere\nTotal Tokens\nPerplexity\nFigure 35: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al.,\n2021)\n0\n50B\n100B\n20\n30\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nC4\nTotal Tokens\nPerplexity\nFigure 36: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020)\n(English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021)\n0\n50B\n100B\n0.25\n0.3\n0.35\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.4\n0.45\n0.5\n0.55\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.5\n0.52\n0.54\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nWinogrande\nTotal Tokens\nAccuracy\nFigure 37: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n66\n0\n50B\n100B\n0.6\n0.7\n0.8\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nPIQA\nTotal Tokens\nAccuracy\nFigure 38: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n50B\n100B\n3\n4\n5\n6\n7\n8\n9\nPII Remove (>=5) + Mask (<5)\nPII Remove All\nBaseline\nTrain\nTotal Tokens\nCross Entropy\nFigure 39: Training Cross Entropy\n67\nK.4\nComparing Quality Filters for Web Pipeline\n0\n50B\n100B\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\n50\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nPile\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 40: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n50B\n100B\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nC4\nTotal Tokens\nPerplexity\n0\n50B\n100B\n40\n50\n60\n70\n80\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nManosphere\nTotal Tokens\nPerplexity\nFigure 41: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\n100\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nGAB\nTotal Tokens\nPerplexity\n0\n50B\n100B\n50\n60\n70\n80\n90\n100\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 42: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\n100\n200\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n10\n20\n30\n40\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\n90\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 43: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n50B\n100B\n0.25\n0.3\n0.35\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.4\n0.45\n0.5\n0.55\n0.6\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.5\n0.52\n0.54\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nWinogrande\nTotal Tokens\nAccuracy\nFigure 44: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n68\n0\n50B\n100B\n0.6\n0.7\n0.8\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\n0.5\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nPIQA\nTotal Tokens\nAccuracy\nFigure 45: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n50B\n100B\n3\n4\n5\n6\n7\n8\n9\nBaseline\nC4 NoPunc\nC4 All\nGopher All\nC4 NoPunc + Gopher All\nTrain\nTotal Tokens\nCross Entropy\nFigure 46: Training Cross Entropy\n69\nK.5\nFull Comparison of Web Pipeline\n0\n50B\n100B\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\n50\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nPile\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 47: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n50B\n100B\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nC4\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nManosphere\nTotal Tokens\nPerplexity\nFigure 48: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nGAB\nTotal Tokens\nPerplexity\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\n100\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 49: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\n100\n200\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n10\n20\n30\n40\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 50: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n50B\n100B\n0.25\n0.3\n0.35\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.4\n0.45\n0.5\n0.55\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.5\n0.52\n0.54\n0.56\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nWinogrande\nTotal Tokens\nAccuracy\nFigure 51: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n70\n0\n50B\n100B\n0.6\n0.7\n0.8\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.4\n0.5\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nPIQA\nTotal Tokens\nAccuracy\nFigure 52: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n50B\n100B\n3\n4\n5\n6\n7\n8\n9\nBaseline\nQuality Filters\nQuality Filters + Dedup\nQuality Filters + Dedup + Content Filters\nTrain\nTotal Tokens\nCross Entropy\nFigure 53: Training Cross Entropy\n71\nK.6\nToxicity Filtering in Web Pipeline\n0\n50B\n100B\n20\n30\n40\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nPile\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 54: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n50B\n100B\n20\n30\n40\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nC4\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nManosphere\nTotal Tokens\nPerplexity\nFigure 55: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\n90\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nGAB\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\n90\n100\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 56: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n10\n20\n30\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 57: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n50B\n100B\n0.25\n0.3\n0.35\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.4\n0.45\n0.5\n0.55\n0.6\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.5\n0.52\n0.54\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nWinogrande\nTotal Tokens\nAccuracy\nFigure 58: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n72\n0\n50B\n100B\n0.6\n0.7\n0.8\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.35\n0.4\n0.45\n0.5\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nPIQA\nTotal Tokens\nAccuracy\nFigure 59: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n50B\n100B\n3\n4\n5\n6\n7\n8\n9\n10\nHate Filter (Low Threshold)\nNSFW Filter (Low Threshold)\nHate Filter (High Threshold)\nNSFW Filter (High Threshold)\nBaseline\nTrain\nTotal Tokens\nCross Entropy\nFigure 60: Training Cross Entropy\n73\nK.7\nComparing Code Processing Pipeline\n0\n10B\n20B\n30B\n40B\n50B\n20\n30\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\n4chan\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n20\n30\n40\n50\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nPile\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n50\n60\n70\n80\n90\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 61: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n10B\n20B\n30B\n40B\n50B\n50\n60\n70\n80\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nC4\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n50\n60\n70\n80\n90\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nManosphere\nTotal Tokens\nPerplexity\nFigure 62: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n10B\n20B\n30B\n40B\n50B\n60\n70\n80\n90\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nGAB\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n30\n40\n50\n60\n70\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nICE\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n40\n50\n60\n70\n80\n90\n100\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 63: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n10B\n20B\n30B\n40B\n50B\n60\n70\n80\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n40\n50\n60\n70\n80\n90\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n10B\n20B\n30B\n40B\n50B\n40\n50\n60\n70\n80\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 64: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n10B\n20B\n30B\n40B\n50B\n0.24\n0.25\n0.26\n0.27\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n10B\n20B\n30B\n40B\n50B\n0.3\n0.35\n0.4\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nARC-Easy\nTotal Tokens\nAccuracy\n0\n10B\n20B\n30B\n40B\n50B\n0.48\n0.5\n0.52\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nWinogrande\nTotal Tokens\nAccuracy\nFigure 65: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n74\n0\n10B\n20B\n30B\n40B\n50B\n0.5\n0.55\n0.6\n0.65\n0.7\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nSciQ\nTotal Tokens\nAccuracy\n0\n10B\n20B\n30B\n40B\n50B\n0.27\n0.28\n0.29\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nHellaSwag\nTotal Tokens\nAccuracy\n0\n10B\n20B\n30B\n40B\n50B\n0.54\n0.56\n0.58\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nPIQA\nTotal Tokens\nAccuracy\nFigure 66: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n10B\n20B\n30B\n40B\n50B\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nDolma (RPJ rules)\nDolma (RPJ rules & StarCoder rules)\nTrain\nTotal Tokens\nCross Entropy\nFigure 67: Training Cross Entropy\n75\nK.8\nStudying Dolma Mixture\n0\n50B\n100B\n20\n30\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\n4chan\nTotal Tokens\nPerplexity\n0\n50B\n100B\n9\n10\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nPile\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 68: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n50B\n100B\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nC4\nTotal Tokens\nPerplexity\n0\n50B\n100B\n30\n40\n50\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nManosphere\nTotal Tokens\nPerplexity\nFigure 69: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n50B\n100B\n30\n40\n50\n60\n70\n80\n90\n100\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nGAB\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\n50\n60\n70\n80\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nICE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 70: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n50B\n100B\n40\n50\n60\n70\n80\n90\n100\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n50B\n100B\n20\n30\n40\n50\n60\n70\n80\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 71: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n50B\n100B\n0.25\n0.3\n0.35\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.4\n0.45\n0.5\n0.55\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nARC-Easy\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.48\n0.5\n0.52\n0.54\n0.56\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nWinogrande\nTotal Tokens\nAccuracy\nFigure 72: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n76\n0\n50B\n100B\n0.6\n0.7\n0.8\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nSciQ\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.3\n0.4\n0.5\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nHellaSwag\nTotal Tokens\nAccuracy\n0\n50B\n100B\n0.6\n0.65\n0.7\nNa\u00efve Mix\nWeb Only Mix\nReference+ Mix\nGopher-like Mix\nPIQA\nTotal Tokens\nAccuracy\nFigure 73: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n77\nK.9\nStrategies to Format Conversational Forums Pipeline\n0\n20B\n40B\n60B\n20\n30\n40\n50\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\n4chan\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n20\n30\n40\n50\n60\n70\n80\n90\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nPile\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 74: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\n90\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nC4\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n20\n30\n40\n50\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nManosphere\nTotal Tokens\nPerplexity\nFigure 75: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nGAB\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n40\n50\n60\n70\n80\n90\n100\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nICE\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\n90\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 76: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n20B\n40B\n60B\n50\n60\n70\n80\n90\n100\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\n90\n100\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n50\n60\n70\n80\n90\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 77: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n20B\n40B\n60B\n0.24\n0.26\n0.28\n0.3\n0.32\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.35\n0.4\n0.45\n0.5\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nARC-Easy\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.5\n0.52\n0.54\n0.56\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nWinogrande\nTotal Tokens\nAccuracy\nFigure 78: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n78\n0\n20B\n40B\n60B\n0.5\n0.6\n0.7\n0.8\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nSciQ\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.3\n0.35\n0.4\n0.45\n0.5\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nHellaSwag\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.6\n0.65\n0.7\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nPIQA\nTotal Tokens\nAccuracy\nFigure 79: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n20B\n40B\n60B\n3\n4\n5\n6\n7\n8\n9\n10\nAtomic Content, Dedup, PII, Toxic\nAtomic Content\nPartial Threads, Dedup\nComplete Threads\nPartial Threads\nTrain\nTotal Tokens\nCross Entropy\nFigure 80: Training Cross Entropy\n79\nK.10\nEvaluating Toxicity Filtering in Conversational Forums Pipeline\n0\n20B\n40B\n60B\n20\n30\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\n4chan\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nPile\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 81: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nC4\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n20\n30\n40\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nManosphere\nTotal Tokens\nPerplexity\nFigure 82: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020;\nDodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nGAB\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n50\n60\n70\n80\n90\n100\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nICE\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\nFigure 83: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al.,\n2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\n0\n20B\n40B\n60B\n50\n60\n70\n80\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nTwitterAEE\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n30\n40\n50\n60\n70\n80\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nmC4 (English)\nTotal Tokens\nPerplexity\n0\n20B\n40B\n60B\n60\n70\n80\n90\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\nFigure 84: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett\net al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\n0\n20B\n40B\n60B\n0.26\n0.28\n0.3\n0.32\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nOpenBookQA\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.35\n0.4\n0.45\n0.5\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nARC-Easy\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.5\n0.52\n0.54\n0.56\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nWinogrande\nTotal Tokens\nAccuracy\nFigure 85: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n80\n0\n20B\n40B\n60B\n0.5\n0.6\n0.7\n0.8\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nSciQ\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.3\n0.35\n0.4\n0.45\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nHellaSwag\nTotal Tokens\nAccuracy\n0\n20B\n40B\n60B\n0.6\n0.65\n0.7\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nPIQA\nTotal Tokens\nAccuracy\nFigure 86: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n0\n20B\n40B\n60B\n3\n4\n5\n6\n7\n8\n9\n10\nNo Filtering\nPII + NSFW + Hate Filter\nNSFW + Hate Filter\nTrain\nTotal Tokens\nCross Entropy\nFigure 87: Training Cross Entropy\n81\nK.11\nTraining Olmo-1b\n1T\n2T\n3T\n9\n10\n20\n30\n4chan\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nDolma (Reddit)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n9\n10\n20\n30\nDolma (peS2o v2)\nTotal Tokens\nPerplexity\nFigure 88: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al.,\n2020), Dolma Reddit Subset, and Dolma Papers Subset\n1T\n2T\n3T\n20\n30\n40\nICE\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nM2D2 (Wiki)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n40\n50\nTwitterAEE\nTotal Tokens\nPerplexity\nFigure 89: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991),\nM2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016)\n1T\n2T\n3T\n20\n30\n40\n50\nManosphere\nTotal Tokens\nPerplexity\nFigure 90: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al.,\n2021)\n1T\n2T\n3T\n20\n30\nmC4 (English)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nM2D2 (S2ORC)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nC4\nTotal Tokens\nPerplexity\nFigure 91: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020)\n(English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021)\n1T\n2T\n3T\n20\n30\n40\n50\nPenn Treebank\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n10\n20\n30\nDolma (Wiki)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n30\n40\n50\nGAB\nTotal Tokens\nPerplexity\nFigure 92: Perplexity results on Paloma (Magnusson et al., 2023); subsets Penn Tree Bank (Marcus\net al., 1994), Dolma Wikipedia Subset, and Gab (Zannettou et al., 2018)\n82\n1T\n2T\n3T\n8\n9\n10\n20\n30\nPile\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nDolma (Books)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\nC4 (100 Domains)\nTotal Tokens\nPerplexity\nFigure 93: Perplexity results on Paloma (Magnusson et al., 2023); subsets Pile (Gao et al., 2020)\n(Val), Dolma Books Subset, and C4 100 dom (Chronopoulou et al., 2022)\n1T\n2T\n3T\n20\n30\n40\nWikiText-103\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n3\n4\n5\n6\n7\n8\n9\n10\nDolma (Stack v5)\nTotal Tokens\nPerplexity\n1T\n2T\n3T\n20\n30\n40\nDolma (Common Crawl)\nTotal Tokens\nPerplexity\nFigure 94: Perplexity results on Paloma (Magnusson et al., 2023); subsets WikiText 103 (Merity\net al., 2016), Dolma Code Subset, and Dolma Web Subset\n1T\n2T\n3T\n0.25\n0.3\n0.35\nOpenBookQA\nTotal Tokens\nAccuracy\n1T\n2T\n3T\n0.3\n0.4\n0.5\n0.6\nARC-Easy\nTotal Tokens\nAccuracy\n1T\n2T\n3T\n0.5\n0.55\n0.6\nWinogrande\nTotal Tokens\nAccuracy\nFigure 95: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al.,\n2018), and WinoGrande (Sakaguchi et al., 2019)\n1T\n2T\n3T\n0.4\n0.6\n0.8\nSciQ\nTotal Tokens\nAccuracy\n1T\n2T\n3T\n0.3\n0.4\n0.5\n0.6\nHellaSwag\nTotal Tokens\nAccuracy\n1T\n2T\n3T\n0.55\n0.6\n0.65\n0.7\n0.75\nPIQA\nTotal Tokens\nAccuracy\nFigure 96: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and\nPIQA (Bisk et al., 2019)\n1T\n2T\n3T\n3\n4\n5\n6\n7\n8\n9\nTrain\nTotal Tokens\nCross Entropy\nFigure 97: Training Cross Entropy\n83\n"
  },
  {
    "title": "CroissantLLM: A Truly Bilingual French-English Language Model",
    "link": "https://arxiv.org/pdf/2402.00786.pdf",
    "upvote": "21",
    "text": "CroissantLLM: A Truly Bilingual\nFrench-English Language Model\nManuel Faysse1,5\nPatrick Fernandes6,8,11\nNuno M. Guerreiro2,5,6,8\nAnt\u00f3nio Loison1\nDuarte M. Alves6,8\nCaio Corro9\nNicolas Boizard4,5\nJo\u00e3o Alves2\nRicardo Rei2,7,8\nPedro H. Martins2\nAntoni Bigata Casademunt10\nFran\u00e7ois Yvon9\nAndr\u00e9 F.T. Martins2,6,8\nGautier Viaud1\nC\u00e9line Hudelot5\nPierre Colombo3,5\n1Illuin Technology\n2Unbabel\n3Equall\n4Diabolocom\n5MICS, CentraleSup\u00e9lec, Universit\u00e9 Paris-Saclay\n6Instituto de Telecomunica\u00e7\u00f5es, Lisboa\n7INESC-ID, Lisboa\n8Instituto Superior T\u00e9cnico, Universidade de Lisboa (Lisbon ELLIS Unit)\n9Sorbonne Universit\u00e9, CNRS, ISIR, Paris\n10Imperial College London\n11Language Technologies Institute, Carnegie Mellon University,\nmanuel.faysse@centralesupelec.fr\nAbstract\nWe introduce CroissantLLM, a 1.3B language model pre-trained on a set\nof 3T English and French tokens, to bring to the research and industrial\ncommunity a high-performance, fully open-sourced bilingual model that\nruns swiftly on consumer-grade local hardware. To that end, we pioneer the\napproach of training an intrinsically bilingual model with a 1:1 English-to-\nFrench pretraining data ratio, a custom tokenizer, and bilingual finetuning\ndatasets. We release the training dataset, notably containing a French split\nwith manually curated, high-quality, and varied data sources. To assess\nperformance outside of English, we craft a novel benchmark, FrenchBench,\nconsisting of an array of classification and generation tasks, covering var-\nious orthogonal aspects of model performance in the French Language.\nAdditionally, rooted in transparency and to foster further Large Language\nModel research, we release codebases, and dozens of checkpoints across\nvarious model sizes, training data distributions, and training steps, as well\nas fine-tuned Chat models, and strong translation models. We evaluate\nour model through the FMTI framework (Bommasani et al., 2023) and vali-\ndate 81 % of the transparency criteria, far beyond the scores of even most\nopen initiatives. This work enriches the NLP landscape, breaking away\nfrom previous English-centric work to strengthen our understanding of\nmultilingualism in language models.\narXiv:2402.00786v3  [cs.CL]  13 Feb 2024\n1\nIntroduction\nLarge Language Models (LLM) have taken over the Natural Language Processing (NLP)\nlandscape in the past years. Although a few proprietary models are still considered to\nrun ahead of the pack (OpenAI et al., 2023), open weights models such as Llama (Touvron\net al., 2023a,b), Qwen (Bai et al., 2023a) or Mistral (Jiang et al., 2023, 2024) are rapidly\nbridging the performance gap. However, widespread industrial and research adoption of\nsuch technology remains challenging for several reasons, including the lack of transparency\nin the data collection and training processes, the scarcity of existing resources outside of\nEnglish, and the large-scale and costly nature of existing high-performing models.\nLack of transparency. State-of-the-art models, both proprietary and open-weights are often\ndesigned and trained by heavily investor-backed companies, that aim to retain a moat by\nkeeping their training data mix and strategy secret, hindering the rest of the field\u2019s ability to\nfully study and understand these models. This lack of transparency, ranging from training\nset composition to lack of evaluation or unclear usage policies, has been characterized by\nprevious works, such as those by Bommasani et al. (2023) and Casper et al. (2024), pushing\nfor full transparency as a key component for safe model development and use. The dangers\nof closed, non-auditable datasets have been exemplified by recent findings showcasing\nthe potential dangers of dataset contamination, whether intentional (Hubinger et al., 2024)\nor not.1 Furthermore, legal questions arise surrounding data ownership in LLM training\ncorpora (NewYorkTimes, 2023; Samuelson, 2023) and recent developments in the political\nlandscape, regarding AI (EU AI Act, US senate hearings)2 have further emphasized the\nimportance of transparent approaches, both from a legal perspective and to build user trust.\nBias towards English. Although the exact training mix of most well-performing LLMs is\nnot publicly available information, most large models are trained on very English-centric\ncorpora (Touvron et al., 2023a). This is the consequence of the important amount of English\nresources compared to other languages, both in terms of data availability and benchmarks.\nAs could be expected, all publicly available LLMs display a large performance disparity\nbetween English and non-English languages when evaluated on downstream tasks (Ban-\ndarkar et al., 2023). Moreover, cultural knowledge and biases are mechanically integrated\nthrough the training data, leading to models with greater knowledge of American events\nor biases (Bender et al., 2021; Santurkar et al., 2023). This puts non-English users at a\ndisadvantage when it comes to language model usage and adoption. While the non-English\nNLP community has produced multilingual datasets (Soboleva et al., 2023; Lauren\u00e7on et al.,\n2023) and models (Scao et al., 2022; Shliazhko et al., 2022) in the last few years, the available\nresources still largely lag behind English ones, hindering industrial adoption in non-English\nsettings.\nChallenging to use at scale. Although benefits of scaling models to enormous sizes have\nbeen amply demonstrated in terms of performance (Hoffmann et al., 2022; Wei et al., 2022b;\nChowdhery et al., 2022), scale comes at a large cost in terms of hardware requirements\nand inference speed. Download statistics on HuggingFace show the smallest Llama model\n(Touvron et al., 2023a) to be the most adopted by the community, demonstrating the interest\nin small but capable models. LLM scaling laws demonstrate the diminishing returns of\ntraining a model of a given size past a certain amount of tokens. By continuing pre-training\nway past the compute-optimal threshold, performance has been shown not to saturate,\nenabling the training of \u201cinference-optimal\u201d models of high interest to the industrial and\n1https://purl.stanford.edu/kh752sm9123\n2https://www.commerce.senate.gov/2023/9/the-need-for-transparency-in-artificial-intelligence\n2\nFigure 1: Conversation example with CroissantLLMChat\nresearch communities (Sardana and Frankle, 2023). It is still not fully understood how\nmodel performance continues to improve at these later stages of training.\n1.1\nContributions\nIn this work, we attempt to bridge the aforementioned limitations and gaps. Our main\ncontributions can be summarized as follows.\nContribution 1: Introduction of a highly-curated, diverse corpus in French. We collect\nand openly release a 303B token corpus spanning internet data, but also literary work,\nspeech transcripts, legal and administrative documents, scientific articles, business-related\ndocuments, etc. The corpus is distributed under permissive licenses, allowing commercial\nuse with no restriction, and is heavily filtered, curated, and deduplicated. To our knowledge,\nit is the largest multi-source French language corpus released to date of sufficient quality for\nlanguage modeling purposes3.\nContribution 2: Training CroissantLLM, a truly bilingual language model. Nowadays,\nmost models display some multilingual capabilities. For example, Bloom has been trained\nto be massively multilingual (Scao et al., 2022), Llama contains a minor proportion of non-\nEnglish data in its training set (Touvron et al., 2023a) and Qwen included a significative\nportion of Chinese data (Bai et al., 2023a).4 However, to our knowledge, outside of Chinese\nwith a different alphabet (Zeng et al., 2022), no work has studied or attempted to train\na multilingual model of significant scale in which English is not the dominant training\nlanguage.\nIn this work, we train a model on a 1:1 ratio of English to French with a tokenizer optimized\nfor bilingualism. Our end goal is to have a model less skewed towards English perfor-\nmance or cultural biases. We motivate this ratio by conducting careful experimentation\non smaller model scales to uncover the trade-offs behind language distribution ratios. We\nopt for a strategy enabling a \u201cbest of both worlds\u201d performance in these two languages, as\nempirically validated by scaling law experiments. Experimentally, we show the importance\nof integrating data from another cultural source in the acquisition of cultural knowledge,\nunderlining the importance of the effort.\nContribution 3: FrenchBench: A novel LLM benchmark for the French Language. To\nevaluate models in French, we introduce a benchmark encompassing various tasks to assess\n3By this, we imply of sufficient quality to train a language model (little OCR errors, high-quality\ntext) and not exclusively composed of Internet data.\n4Qwen-VL (Bai et al., 2023b) reports 77.3 % & 22.7 % Chinese, but no information is given for the\nQwen base model.\n3\nfactual knowledge, generative capabilities, language understanding, etc. This benchmark\nis constructed both from openly available datasets, as well as newly released manually\nannotated data. We evaluate and report results for our models as well as other models with\nFrench-speaking capabilities.56\nContribution 4: Releasing high-performing, inference-optimal models for the industrial\ncommunity, together with a wide range of resources for the research community. The\nmodels we train are all released under open licenses. Our largest model is trained on a\n3000:1 token to parameter ratio (150 times longer than a Chinchilla Optimal 1.3B model)\nleading to very strong performance in its size category. We show that model performance\non downstream tasks continues to dramatically improve with lengthened pre-training runs,\nalthough model perplexity does not significantly improve. We release all checkpoints for\nall model sizes, as well as the exact training data seen at every step for research purposes.7\nThese models are extremely efficient to run, leading to low-latency, energy-efficient on-edge\ninference, even on low-resource devices such as phones or personal computers. These\nreleases8 are motivated by a commitment to transparency to allow research and reassuring\nusers for industrial deployment: our models comply with 81 % of criteria listed by the\nFoundation Model Transparency Index (Bommasani et al., 2023) (see Section 6).\n2\nData\nThe dominance of the English language in the training data of most current models is\nundeniable. While multilingual models like Llama leverage some non-English data (Touvron\net al., 2023a), it corresponds to only a minor part of the corpus, leading to a significant drop\nin performance across non-English data, with noticeable \u201cAmerican\u201d bias (Santurkar et al.,\n2023; Navigli et al., 2023). This work aims to offset this trend by using a more balanced\nbilingual corpus comprising English and French, as well as additional code data. Although\nboth languages belong to the Indo-European language family, they exhibit different morpho-\nsyntactic structures9 and French has a richer morphology.10 We study whether this corpus\nhelps in reducing biases, enabling more varied knowledge sets, and unlocking non-English\nperformance.\nA variety of sources are integrated into our corpus, including carefully filtered internet data\nand high-quality data from a range of sources, all devoid of restrictive licenses ensuring\n5Code\nfor\nevaluation\nis\ncurrently\navailable\nat\nhttps://github.com/ManuelFay/\nlm-evaluation-harness\n6Another complementary initiative has been led for French model evaluation and released concur-\nrently in Bawden et al. (2024)\n7Training data indexing order will be released in a second stage.\n8Code for dataset collection and filtering is available at https://github.com/ManuelFay/\nllm-data-hub.\nCode\nfor\nmodel\ntraining\nis\nhosted\nat\nhttps://github.com/CoderPat/\ncroissant-llm-training. Datasets and model checkpoints are available at https://huggingface.co/\nCroissantLLM.\n9For example, pronominal objects are placed before (resp. after) the verb in French (resp. English),\nboth languages have different noun phrase constructions (\u201cla commission europ\u00e9enne\u201d vs. \u201cthe\nEuropean commission\u201d), etc.\n10For example, English has 5 verb forms whereas French has 48, French has explicit inflections for\ngrammatical genders, etc. However, note that only English adjectives have morphological construc-\ntions for expressing comparison (e.g. easy, easier, easiest). We refer to WALS for more details, e.g.\nhttps://wals.info/feature/21B#2/26.7/152.4\n4\nSize (GB)\nDocs. (M)\nTokens (B)\nToken/Doc\nSampling Ratio\n# tokens (B)\nFrench\n1258.70\n376.27\n303.51\n806.63\n4.09\n1240.08\nEnglish\n2351.13\n591.23\n655.64\n1108.94\n1.89\n1240.09\nCode\n366.87\n81.90\n141.43\n1726.76\n2.04\n288.92\nParallel\n113.91\n408.03\n35.78\n87.68\n6.13\n219.26\nTotal\n4090.61\n1457.43\n1136.35\n779.70\n14.15\n2988.35\nTable 1: Final Data mix for CroissantLLM training\ncomplete openness of the data and the trained model. Data statistics are available in\nTable 1.11\nThe scrapping and processing code are available in our code base.12 The license information\nof all datasets used is given, all allowing for permissive commercial use.\n2.1\nFrench Data\nTable 10 lists the source and some information regarding the French corpus. Details about\nthe data sources are expanded further below.\nWeb Data. We collect internet data from various web scraps (Oscar (Abadji et al., 2022),\nmC4 (Xue et al., 2021)), leveraging the CulturaX corpus (Nguyen et al., 2023) for heuristic\nand perplexity filtering, as well as exact and fuzzy deduplication. In total, this represents\nover 363 million webpages and more than 292 billion tokens, that we split using our custom\ntokenizer fitted on equal amounts of French and English data.13\nWe ensure data is of good quality and correctly tagged in French through sampled manual\ninspection, and confirm French-speaking countries are well represented within the dataset.\nNotably, we include several news sources scrapped from Belgium, Switzerland, Canada,\nand Lebanon, as well as multiple African countries (Senegal, Morocco, Algeria, Cameroon,\netc.)\nLegal and Administrative Data. We introduce 5.3B tokens of data from the French gov-\nernment\u2019s open data initiative, ranging from legal tasks to parliamentary discussions and\nfinancial transcripts (e.g. legal and administrative jargon). These texts originate from 13\ndifferent datasets (the OpenData corpus) and were collected from the French government\u2019s\nopen data platform.14 To ensure other French-speaking countries are represented, we add\n68M tokens of data from Swiss legislation retrieved from government sources. We perform\nsteps to process, filter, and run exact deduplication on these documents.\n11As further detailed in 3.4, our data corpus contains different amounts of unique English, French,\nand Code tokens. We obtain our balanced training corpus by upsampling French, Code, and English\ndata with different sampling ratios, such that no performance loss is to be expected (Muennighoff\net al., 2023).\n12https://github.com/ManuelFay/llm-data-hub\n13The mC4 corpus https://huggingface.co/datasets/allenai/c4 is released under the ODC-BY\nlicence https://opendatacommons.org/licenses/by/1-0/ whereas Oscar (Abadji et al., 2022) does\nnot claim data ownership, provides an opt-out strategy for data inclusion, and filtering metadata\nis released under the Creative Commons CC0 license.https://creativecommons.org/publicdomain/\nzero/1.0/\n14Data is released at https://echanges.dila.gouv.fr/OPENDATA/ with the ETALAB open license\nhttps://www.etalab.gouv.fr/wp-content/uploads/2017/04/ETALAB-Licence-Ouverte-v2.0.pdf\n5\nCultural Data. We introduce cultural data from various sources. Notably, we retrieve all\nProject Gutenberg (Hart, 1971) books in the French language as of October 2023, correspond-\ning to books released in the public domain (302 million tokens). We also download and\naggressively filter manuscripts and documents from the French National Library (Biblio-\nth\u00e8que Nationale de France), and filter for documents that belong to the public domain,\nhave undergone an OCR process, and are of high quality.15 To filter out low-quality OCR\ndocuments, we implement custom heuristics which we release within our code base. We\nrun all documents through perplexity filters using KenLM 5-grams16 fitted on the French\nWikipedia split, and discard documents with perplexity values that are too high (noisy)\nor too low (repetitive patterns). Thresholds are set through a manual verification process.\nWe deliberately choose to be aggressive in our filtering to ensure only high-quality data\nis kept and discard the largest portion of the original corpus, keeping about 27M tokens.\nWe choose not to keep any data from the newspaper archives, as the OCR transcription is\noften too noisy. Additionally, we introduce famous public domain French poems custom\nscrapped from a French poetry website,17 and run a set of podcasts through a high-quality\nspeech-to-text model to obtain a textual transcription. This process is hard to scale and data\nsplits from these sources are limited in quantity. Data from the OpenSubtitles18 initiative is\nintegrated, corresponding to 41.8 million tokens originating from movie subtitles.19 Finally,\nwe add the French data from Wikisource collected as part of the BigScience initiative (Scao\net al., 2022) and obtain 2.7 billion tokens from the process.20\nEncyclopedia Data. To introduce high-quality factual data to the training corpus, we\nintegrate the French Wikipedia split from November 2023.21 This corresponds to the latest\nknowledge cutoff in the training data. In total, more than 2.5 million articles are used,\nspanning more than 2 billion tokens.\nIndustrial Data. We scrap high-quality and publicly available data from industrial PDFs\nvia a manually crafted list of websites, from large French and French Canadian (Quebec)\ncompanies to government agencies. This business-focused data boosts performance on a\nseries of downstream applications related to industrial NLP. We collect over 36000 PDF\nmulti-page documents and filter them through carefully crafted heuristics, followed by\naggressive perplexity filtering.22 In total, we obtain over 290000 documents and 191 million\ntokens.\n2.2\nEnglish Data\nOur English data is primarily drawn from the SlimPajama corpus (Soboleva et al., 2023),\nexcluding copyrighted documents. Splits per data source are detailed in Table 11.\nInternet Data. Similarly to the French dataset, we rely on carefully filtered content from an\nassortment of internet sources, including miscellaneous web pages and blogs. The filtering\nprocess includes heuristics and perplexity filtering, as well as large-scale deduplication\n15Metadata\nis\nlicensed\nunder\nOpen\nEtalab\nlicense\nhttps://gallica.bnf.fr/edit/und/\nconditions-dutilisation-des-contenus-de-gallica\n16https://github.com/kpu/kenlm\n17https://www.poesie-francaise.fr/\n18https://opus.nlpl.eu/OpenSubtitles2016.php\n19https://www.opensubtitles.org\n20Licensed under CC BY-SA 4.0, https://en.wikisource.org/wiki/Wikisource:Copyright_policy\n21https://huggingface.co/datasets/wikimedia/wikipedia with a CC-By-SA 3.0 license\n22Data is public internet data that we release under MIT license with an opt-out scheme https:\n//huggingface.co/datasets/manu/illuin_layout_dataset_text_only\n6\n(Soboleva et al., 2023). The SlimPajama corpus includes internet data from the Common-\nCrawl23 and C424 web scraps, as well as data sourced from Github textual content25 and the\nStackExchange forums.26\nMiscellaneous. Other non-internet-based data sources are included in the SlimPajama\ndataset, such as scientific articles from Arxiv27 and English documents from Wikipedia.28\nThe SlimPajama dataset is also comprised of the \u201cBooks\u201d subcorpus, obtained by download-\ning all book documents from Bibliotik.29 Some of the documents within this last corpora\nhave been flagged by their owner as proprietary data. We filter out all documents from this\nsubcorpus, and replace them with data from the open-source Project Gutenberg (Hart, 1971)\nEnglish books under public domains.30\nGutenberg Canaries. To assess model memorization to inform about the risks of including\nprivate or sensitive data within the training set, we stress test the model by including\n\u201ccanaries\u201d (Carlini et al., 2019). These correspond to samples that have been intentionally\nmodified and/or repeated and included within the model training set, and that will enable\na posteriori evaluation of the model capacity to memorize data in a \u201cworse than worst-case\u201d\nsituation.31 In total the canaries represent 555 million tokens, representing less than 0.04 %\nof the total tokens seen during training.\n2.3\nCode Data\nIn line with most recent models (Chowdhery et al., 2022; Scao et al., 2022; Touvron et al.,\n2023a), we integrate code data into our training corpus. Notably, previous work shows that\ncode data benefits natural language tasks and can be particularly useful in data-constrained\nsettings (Muennighoff et al., 2023). Therefore, we include 140B tokens of code data in several\ncommon programming languages. Splits and number of tokens are detailed in Table 12.\nThe Stack & StarCoder. We rely on the efforts of the StarCoder project (Li et al., 2023a), and\nuse their high-quality filtered code data from The Stack corpus (Kocetkov et al., 2022).32\nWe keep only high-resource programming languages (Java, Javascript, Python, C, C++,\nSQL) and Jupyter33 notebooks, as well as a few samples of formatted data (JSON, Tex) and\nscripting languages (shell scripts, Dockerfiles).34\nExtra Python code. We extend the corpus with several other sources of Python code due to\nthe popularity of the language in the community. Firstly, we add Pypi packages from recent\n23Common Crawl license https://commoncrawl.org/terms-of-use\n24ODC-BY license\n25Only content under MIT, BSD, and Apache licenses are kept\n26CC-By-SA 4.0 license https://archive.org/details/stackexchange\n27https://arxiv.org/, with author opt-out options\n28CC-By-SA 3.0 license\n29https://huggingface.co/datasets/the_pile_books3\n30From https://huggingface.co/datasets/pg19 with an Apache 2.0 license\n31This work is led in parallel to the CroissantLLM project and findings will be independently\npublished.\n32Data are drawn from various sources and falls under 193 different permissive licenses. We use\nthe version 1.2 of the corpus, which has been filtered with respect to data owners opt-out option\ninformation.\n33https://jupyter.org/\n34https://docs.docker.com/engine/reference/builder/\n7\ncode dumps,35 that are filtered to keep only Python and Jupyter files.36 Secondly, in order to\npromote high-quality problem-solving-centered code, we integrate 1.2B tokens of Python3\ndata from competitive coding contests (Li et al., 2022).37 Lastly, following the success of\nlearning from textbooks (Li et al., 2023b), we add commented Python code constructed by\ncombining code and text cells from Jupyter Notebooks through the CodeParrot initiative.38\n2.4\nParallel Data\nFollowing previous work (Anil et al., 2023), we incorporate vast quantities of parallel data, in\nour case high-quality English-French translation pairs, in order to improve the multilingual\ncapabilities of the model (Briakou et al., 2023).\nOpus. We extract subsets of sentence pairs spanning multiple domains from the OPUS\ncorpus (Tiedemann, 2012).39 Statistics are described in Table 10. In total, we include\n400 million parallel sentences and about 36 billion tokens. The data is filtered through\na rigorous cleaning pipeline: (1) BiFixer (Ram\u00edrez-S\u00e1nchez et al., 2020)40 is first used to\nremove duplicate data through fuzzy matching techniques; (2) BiCleaner41 is then used to\nfilter data using heuristic and perplexity filters; (3) finally, the state-of-the-art NMT quality\nestimator CometKiwi (Rei et al., 2022b) is used to keep only top quality translation pairs.\nTheses. To incorporate versatile academic and scientific language, we augment our dataset\nwith French thesis abstracts along with their author-generated English translations. This\ncorresponds to 95000 documents and more than 80 million high-quality tokens.42\nSong Lyrics. Our dataset integrates song lyrics in both French and English, scrapped from a\nspecialized community-driven lyrics translation website.43 As such, our model is trained\nwith radically different linguistic styles (e.g. colloquialism), and the wide range of topics\ncan help the model to capture cultural nuances. Lyrics have been translated by hand by the\nwebsite community. With a total of 70k songs, we have built up a corpus of 53M tokens\ncovering different periods (80s, 90s, 20s, etc.) and musical genres (rap, rock, jazz, etc.) To\npreserve colloquial expressions and cultural subtleties, we have not filtered song lyrics for\nexplicit content. We validate the original language metadata of the songs through Google\u2019s\nlanguage-detection algorithm.\n3\nTraining\nOur main goal was to train a highly performing, yet resource-friendly bilingual model\nwhile optimizing performances across both languages. To focus on the specific challenges\nof the bilingual paradigm, we rely on previous work to motivate many of our design and\nhyperparameter choices (Touvron et al., 2023b).\n35https://py-code.org/datasets from permissive licensed code\n36https://huggingface.co/datasets/vikp/pypi_clean\n37Under CC-By-4.0 license\n38https://huggingface.co/datasets/codeparrot/github-jupyter-code-to-text Apache 2.0 li-\ncense\n39Free license https://opus.nlpl.eu/\n40https://github.com/bitextor/bifixer\n41https://github.com/bitextor/bicleaner\n42Data licensed under Etalab open license https://www.etalab.gouv.fr/wp-content/uploads/\n2017/04/ETALAB-Licence-Ouverte-v2.0.pdf\n43https://www.lacoccinelle.net, with opt-out options for data owners and community-\ncontributed translations under free-use license.\n8\nModel\nParams (M)\nLayers\nHidden size\nInter. size\nKV heads\nXXS\n100.7\n6\n1024\n4096\n8\nXS\n202.5\n12\n1024\n4128\n8\nS\n341.5\n12\n1536\n4128\n12\nBase\n1214.3\n24\n2048\n5504\n16\nTable 2: Model information for scaling laws. Parameter count excludes embedding and\noutput parameters.\n3.1\nModel Architecture\nWe use the Llama architecture (Touvron et al., 2023a), a decoder-based transformer, trained\nwith rotary position encodings (Su et al., 2023) and a context length of 2048. We construct\n4 different model sizes by jointly scaling the number of attention heads, hidden size, and\nhidden layers. Table 2 summarizes the sizes of each model in the family.\n3.2\nTokenizer\nMost LLM tokenizers are fitted on English-centric corpora with an information-theoretic\noptimization objective, for example, Byte-Pair encoding (Sennrich et al., 2016) or Unigram\n(Kudo, 2018), leading to good fertility values (low token per word ratio) on English text, but\nhigh fertility in other languages. These phenomena make processing in other languages\nslower and more costly (Rust et al., 2021). Furthermore, subword splits in non-English\nlanguages mechanically carry less semantical meaning, potentially being a factor in the\ndegraded performance of models on non-English languages (Rust et al., 2021).\nTokenizer training. We fit our CroissantLLM tokenizer on a corpus of 100B high-quality\ntokens, with splits of English, French, and code data. We use SentencePiece44 to train a\nByte-Pair Encoding tokenizer with a vocabulary size of 32000 tokens, 100 special placeholder\ntokens, whitespace separation, and byte fallback, inspired by Touvron et al. (2023a); Jiang\net al. (2023). The data corpus used to fit the tokenizer is made available,45 and notably\ncontains large amounts of French data to skew the vocabulary construction process towards\noptimizing for French as well.\nImproved fertility rates. The focus on English, French, and Code enables the CroissantLLM\ntokenizer to display smaller fertility rates on French texts than the Mistral and Llama models\nwith similar vocabulary sizes, all the while also displaying slightly smaller rates than both\nin English and Code (Figure 2). This is due to the multilingual support of both Llama and\nMistral tokenizers which need to allocate some vocabulary tokens to frequent character\npatterns from other languages. Roughly, the Llama tokenizer is 17 % less token efficient at\nencoding French internet data, and up to 40 % less efficient on clean encyclopedia French\ntexts, implying that the 303B unique French tokens in our data training set correspond to\nmore than 360B tokens with the Llama tokenizer. This enables us to pack more data in fewer\ntokens, leading to improvements in training and inference efficiency.\n44https://github.com/google/sentencepiece\n45https://huggingface.co/datasets/manu/tok-corpus-shuffled\n9\nWikitext (en) Wikitext (fr)\nCode\nLegal (fr)\nIndustrial (fr)\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\nTokenizer Fertility\nCroissant\nLlama\nMistral\nGpt2\nFigure 2: Fertility on unseen test sets using various tokenizers. Lower is better.\n3.3\nSelecting an optimal language ratio\nA crucial question when training a bilingual model is how to effectively weight data from the\ntwo languages to achieve a good trade-off between performance in both. While, intuitively,\ntraining on an equal mixture of English and French data may seem to be the obvious\nsolution, differences in data quality available for each language coupled with transfer\nlearning dynamics between both languages could imply that a balanced mix might be\nsub-optimal. However, training multiple models with different data mixes for comparison\nis prohibitively expensive.\nTo offset this computational cost, we leverage recent findings on scaling laws (Kaplan et al.,\n2020a) that show that we can predict the performance of our model by training smaller\nmodels on the same dataset. In particular, Fernandes et al. (2023) found that, for multilingual\nmodels, by training smaller models with varying weights for each language in the data\nmix, one can fit a multilingual, joint scaling law that can predict the language performance\ntrade-off of larger models, even for novel language weightings not encountered during the\nfitting of the scaling law.\nAs such, we fit a joint scaling law as described by Fernandes et al. (2023) for each language,\nby training 3 smaller model sizes on 3 different data mixes with varied ratios of English\nand French data (keeping the amount of Code data fixed). The corpus for these scaling law\nexperiments is a subsampled variant of the larger corpus and is detailed in Appendix B.5.\nWe define 3 data mixes by varying the language sampling ratio: (1) equal containing 40 %\nEnglish data, 40 % French data and 20 % Code data; (2) frplus containing 20 % English data,\n60 % French data and 20% Code data; and (3) enplus containing 60 % English data, 20 %\nFrench data and 20 % Code data. We then trained a 1.3B model on these subsets of the data\nfor one of the data mixes to validate their predictive power.\nFigure 3 shows the performance predicted by jointly-fitted scaling laws as we scale the\nmodel and vary the language weightings on the Wiki data validation split. First, we see that\nthe fitted scaling law is able to predict the performance of the larger model almost perfectly.\nSecondly, changing the weight of each language in training has a non-symmetrical impact\non language performance: by increasing the (relative) weight of French from 50 % to 75 %,\nwe get a marginal performance increase in French, while performance in English drops\n10\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nParams (Non-Embedding)\n1e9\n2.2\n2.4\n2.6\n2.8\nLoss\nEnglish (Wiki)\nequal Power-Law\nfrplus Power-Law\nenplus Power-Law\nEmpirical\nHeld-out\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nParams (Non-Embedding)\n1e9\n1.9\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\n2.6\nLoss\nFrench (Wiki)\nequal Power-Law\nfrplus Power-Law\nenplus Power-Law\nEmpirical\nHeld-out\nFigure 3: Evolution of test cross-entropy loss with model size in English (left) and French\n(right), for the wiki domain, as well as the fitted joint scaling laws,\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLanguage Probability (en) Renormalized\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCapacity Ratio\nEnglish (Wiki)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nLanguage Probability (fr) Renormalized\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCapacity Ratio\nFrench (Wiki)\nFigure 4: Effective capacity ratio (as predicted by our fitted joint scaling law) for English and\nFrench as we change the weight of each language.\nsignificantly. This fact is made clear by plotting the effective capacity ratio46 of each language\nas we change the language weight (Figure 4): the \u201cgains\u201d in parameters from increasing\nweight of French data are minimal past the 50 % mark.\nThese findings showcase that multilinguality comes at a price, and training a bilingual\nmodel implies accepting a performance loss on a target language compared to an equivalent\nmodel trained on a monolingual corpus.\nWe find equal ratios of English and French data lead to minimized performance hits across\nboth languages (Figure 3) and opt to train our base model in this data configuration.\n3.4\nFinal data distribution\nOur final dataset is composed of 1.1T unique tokens that originate from sources of various\nlanguages, qualities, and quantities. To craft a training set with a language and data dis-\ntribution that suits our objectives, we upsample some of the sources, notably to balance\nout French and English data and increase the share of parallel data in our training run. Fol-\nlowing work by Muennighoff et al. (2023) and Luukkonen et al. (2023) on data-constrained\n46The number of parameters that it would take a monolingual model to match the performance of\nthe multilingual, divided by the size of the multilingual model. See Fernandes et al. (2023) for more\ndetails\n11\nlanguage modeling scaling laws, we upsample French text by a factor of two, and parallel\ndata by a factor of 3. For a 3T token training run, this enables the model to see French data\nat most 4 times, and English and code data twice, which should have negligible impacts on\nthe performance (Muennighoff et al., 2022). The final data distribution is shown in Table 1.\nAll data is provided from the above-listed sources and no synthetic or augmented data\nis used. Data licenses and copyright information are given for every split to the best of\nour ability. The data collection and filtering process to construct our final mix from the\nabove-listed sources is entirely done by the authors of this paper, who are employed by the\nuniversities or private companies described through their affiliations, under their countries\u2019\ndata protection laws, and compensated at market rates or following the academic salary\ngrid of their institution.\n3.5\nTraining framework\nWe train our models on a modified version of Megatron-Deepspeed,47 a training framework\nbuilt on top of PyTorch. Training is done on a dedicated Nvidia A100 SXM4 80 Gb partition\nof the Jean Zay supercomputer owned by the French government, on 30 octo-GPU nodes.\nCompute is obtained through a Jean Zay grant (2023-AD011014668R1). We rely on the\nHuggingFace Transformers and Datasets library for model and data manipulation.\nTo maximize efficiency, we set the micro-batch size per device to 8 sequences of length 2048,\nand use 4 gradient accumulation steps, resulting in a total batch size of 8 \u00d7 4 \u00d7 30 \u00d7 8 = 7680\nsamples, or 7680 \u2217 2048 = 15, 728, 640 tokens. We achieve a mean efficiency of around 120\nTFLOP48 per second with activation checkpointing, leading to a total compute estimate of\n4.30e22 FLOPS. Standard Cross-Entropy losses are used on a Causal Language Modeling\nobjective.\n3.6\nTraining losses\nTraining lasts 17 days for a total of 99648 GPU hours, and we chose not to manually\nintervene, letting the model recover on its own after occasional loss spikes. We train with a\nmax learning rate of 3e \u2212 4, 1000 warmup steps, and a cosine learning rate with a minimum\nvalue of 1e \u2212 5. Curves suggest the model still has not reached a performance plateau after\n3T tokens (Figure 5). Checkpoints are stored every 5k steps and released with the rest of the\nproject artifacts.\n3.7\nEnvironmental impact\nThe model was exclusively trained on Jean Zay, a French government-owned supercomputer,\noperating on low-carbon nuclear electricity. Between experimental runs, scaling laws, and\nthe final training, 123k A100 hours were used. The Thermal Design Power of the NVIDIA\nA100 SXM4 80Gb used is 400W corresponding to a total power consumption of 49.2 MWH\nand considering a grid carbon intensity of 57 gCO2eq/kWh, we estimate a carbon footprint\nof 2.80 tons of CO2 (Luccioni et al., 2022) emitted during training.49\nInterestingly, the model we trained is not \u201ccompute-optimal\u201d according to Chinchilla laws\n(Hoffmann et al., 2022), meaning that less computing could have been used to train a larger\n47https://github.com/deep-spin/Megatron-DeepSpeed\n48160 TFLOP per second for our scaling law experiments with one GPU node only\n49Factoring in an intentionally large data center Power Usage Effectiveness of 1.2 (Luccioni et al.,\n2022), we estimate an emission of 3.36 tons of CO2.\n12\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTokens (in trillions)\n2.0\n2.1\n2.2\n2.3\n2.4\n2.5\nTraining Loss\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nTokens (in trillions)\n1.66\n1.68\n1.70\n1.72\n1.74\n1.76\n1.78\n1.80\nRolling NLL (Wikitext)\nFigure 5: (Left) Training loss with respect to the number of seen tokens. (Right) Validation\nperplexity (Averaged Log Likelihood) on Wikitext (English), computed with a rolling stride\nmodel with the same performance. However, our model aims to be used for inference\npurposes at industrial scales. Our training paradigm is thus to absorb the downstream\ninference costs, by training a smaller model on a lot more tokens to obtain an inference-\noptimized model equivalent in performance to a bigger compute-optimal model (Sardana\nand Frankle, 2023). Each inference of the final model is thus vastly more energy-efficient\nthan a Chinchilla optimal model of equivalent performance (> 3B parameters), and can even\nrun on CPU or mobile devices. Relying on estimates of Kaplan et al. (2020b), at inference,\nCroissantLLM represents roughly 2.6 GFLOPS per token.\n4\nEvaluation Benchmarks\nWe hope to extend base model evaluation past English benchmarking alone and assess\nmodel capabilities in French, aiming for broad coverage across orthogonal capabilities\nto observe the effect of truly bilingual pre-training. Our evaluation efforts are rooted in\ntransparency, and all results reported in the main technical report are reproducible through\ncode that is open-sourced and public data.50\n4.1\nEnglish\nIn English, we evaluate on standard LLM evaluation benchmarks.\nHellaSwag. HellaSwag (Zellers et al., 2019) is a dataset specifically crafted to challenge\ncommon-sense reasoning abilities of models by requiring them to predict the endings of\nsentences in a way that relies on information not present in the preceding context. It focuses\non capturing a nuanced and context-dependent understanding of language.\nPiQA. PIQA is a dataset for common-sense reasoning and was created to investigate the\nphysical knowledge of existing NLP models (Bisk et al., 2019).\nSciQ. The SciQ dataset contains 13,679 crowdsourced science exam questions about Physics,\nChemistry, and Biology, among others. The questions are in multiple-choice format with 4\nanswer options each (Welbl et al., 2017).\n50Evaluation is ran through the LM Evaluation harness (Gao et al., 2021) in a fork that will\nbe merged upon acceptance https://github.com/ManuelFay/lm-evaluation-harness.\nDatasets\nused are grouped in the following collection:\nhttps://huggingface.co/collections/manu/\nfrenchbench-evaluation-datasets-65bbcbf3987c4be8a5e9a2ef\n13\nArc-C. The AI2 reasoning challenge dataset (Clark et al., 2018) consists of 7,787 authentic\ngrade-school level, multiple-choice science questions, designed to stimulate research in\nadvanced question-answering. The dataset is divided into a Challenge Set and an Easy\nSet, with the Challenge Set comprising questions that were answered incorrectly by both a\nretrieval-based algorithm and a word co-occurrence algorithm. Additionally, the dataset\nincludes a corpus of over 14 million science sentences relevant to the task and provides\nthree neural baseline models.\nMT-Bench. MT-Bench (Zheng et al., 2023) contains a set of prompts designed to evaluate\nmodels on their multi-turn conversation and instruction-following abilities, covering various\ncore model abilities; writing, roleplay, extraction, reasoning, math, coding, knowledge I\n(STEM), and knowledge II (humanities/social science). MT-Bench performance has been\nshown to best correlate with human-rated appreciation of a model through the LM-Sys\nmodel arena.\n4.2\nFrench\nWe aim to evaluate models on their capabilities in French, along several axes including\nvocabulary, grammar, reading comprehension, factual knowledge, biases, and generative\ncapacities, etc. To this end, we introduce FrenchBench, a novel LLM evaluation benchmark\nfor the French language, testing a large array of model skills in various settings.\nFrenchBench comprises several tasks, some included from previous benchmark datasets,\nothers newly released with this work.\n4.2.1\nFrenchBench Gen\nFrenchBench assesses the generative capabilities of LLMs in a few-shot setting. Tasks include\ntitle generation, summarization, question generation, and question answering. We detail\nthe tasks and the evaluation metrics used below.\nFQuaD. FQuaD (d\u2019Hoffschmidt et al., 2020) is a French Question Answering dataset, con-\ntaining manually annotated sets of Wikipedia passages, questions, and extractive answer\nspans in the Squad format. This high-quality dataset is one of the rare human-annotated\nFrench datasets and we rely on its public evaluation split for 4 of the FrenchBench tasks.\nFQuADQenQ is a question generation task in which passages and answers are given to the\nmodel in a few-shot manner, and we compute the ROUGE1 score (Lin, 2004) with the gold\nquestions.\nFquadGenAns is the classic question-answering task, but models generate the answer span\nthemselves, and the ROUGE1 score is computed with the gold extractive answer span.\nMultiFQuAD 51 is a FQuAD variant, with a publicly released evaluation set, in which an-\nswers can consist of multiple disjoint spans. We evaluate performance on the concatenation\nof these gold extractive spans using the ROUGE1 score.\n4.2.2\nFrenchBench Multiple Choice\nWe also assess reasoning, factual knowledge, linguistic capabilities, and model biases\nthrough a series of few-shot classification tasks, on which models are given multiple com-\npletions (multiple choice), and the answer with the highest likelihood is selected. We\n51Previously unreleased dataset, evaluation set is released under CC-By-NC SA 4.0 license with this\nwork\n14\nexperimented with multiple question templates. In the MMLU format, the multiple poten-\ntial answers are given after the question prefixed by a letter (A, B, C, D) and the model must\nguess the correct answer by predicting the correct answer\u2019s letter. In HellaSwag formatting,\nthe model must complete the sentence and the model chooses the most likely continuation\nsequence, without prior knowledge of all other options. We find HellaSwag formatting is\nless abstract, and enables smaller size models to perform better.\nFrench Language Test. The French Language Test is a dataset crafted to assess the grammar\nand vocabulary capabilities of models through language tests. It provides a structured\nevaluation of a model\u2019s linguistic proficiency, aiming to measure its competency in un-\nderstanding and generating coherent and grammatically accurate sentences in the French\nlanguage. It is composed of a fr-grammar and fr-vocabulary multiple choice test.\nFrench Hellaswag and Arc-C. These datasets correspond to machine translations made by\nGPT3.5 of HellaSwag and Arc-C to French.52 Manual verification of the translation quality\nindicates the translations to be far from perfect but sufficient for these datasets to act as a\ncorrect performance proxy.\nOrangeSum. OrangeSum53 (Eddine et al., 2020) is a summarization dataset constructed\nfrom online News articles. Two standard French summarization tasks span from this dataset;\nOSum(T) in which the model is tasked with generating the title from the article body, and\nOSum(A) in which the model must generate the first paragraph of the article aimed to be an\nabstract of the article. Both tasks are measured through the ROUGE1 score.\n4.3\nOther Tasks\nMT-Bench French. Mt-Bench French54 is a translated and adapted version of MT-Bench\nin French with all questions having undergone rigorous human review and adaption to\nguarantee authentic wording, and coherence, and to account for cultural discrepancies.\nTranslation. Translation capabilities are evaluated through the test set of the 2014 WMT\nFrench-English and English-French tasks (Alves et al., 2023). We measure performance\nusing BLEU score (sacreBLEU, Papineni et al., 2002; Post, 2018), and COMET (Rei et al.,\n2022a). We also report FLORES (Team et al., 2022) and TICO (Anastasopoulos et al., 2020)\nscores.\nFrench Trivia. The French Trivia dataset is built from online trivia questions pertaining to\nFrench culture. Answers are short and meant to assess latent model knowledge and the\nimpact of pre-training data and cultural references. Intentionally, questions are formulated\nin English for comparison with monolingual English models.55\nBelebele. Belebele is a challenging reading comprehension dataset, with multiple choices,\nreleased across 122 languages in parallel format (Bandarkar et al., 2023). We leverage the\nEnglish and French splits.\n5\nBenchmark results\nBaseline models. To evaluate CroissantLLM, we compare with an array of various models,\nvarying in parameter size, pre-training language distribution, training corpus size, etc.\n52https://github.com/laiviet/lm-evaluation-harness/tree/main/datasets\n53https://huggingface.co/datasets/orange_sum\n54https://huggingface.co/datasets/bofenghuang/mt-bench-french\n55This is a previously unreleased dataset, released under MIT license with this work.\n15\nFor \u201cmonolingual\u201d English models, we evaluate Pythia-1.4B (Biderman et al., 2023) trained\non 300B tokens, OPT-1.3B (Zhang et al., 2022) trained on 180B tokens, and TinyLlama(1.1B)\n(Zhang et al., 2024). TinyLlama is a very strong English baseline, as it holds many similarities\nto CroissantLLM. It is a 1.1B model trained on 3 trillion tokens with the same English corpus\nas the Croissant base. Although it contains some amount of high-quality non-English data,\nit is only a minor share of the training corpus, the main data sources being English and code\ndata. As such, it trains on much more English tokens than CroissantLLM. All models are\ntrained way past Chinchilla optimality (\u223c26B tokens for a 1.3B model).\nFor monolingual French models, we use GPT-fr (Simoulin and Crabb\u00e9, 2021), a 1B model\ntrained on 16.3B tokens. We were not able to access the PagnolXL(1.5B) model (Launay\net al., 2021) but show perplexity comparisons with reported results in Appendix D.2.\nWe also compare CroissantLLM with multilingual models, notably Llama2(7B) (Touvron\net al., 2023b) trained on 2T tokens, Mistral7B (Jiang et al., 2023), and Bloom (Scao et al.,\n2022) models (from 1.1B to 3B), trained on 350B tokens each. We note that although the\nlargest Bloom model is undertrained according to Chinchilla optimality (Hoffmann et al.,\n2022), smaller models are trained on the same number of tokens, making them largely more\ninference optimal and thus strong contenders. Finally, in the same size category, we evaluate\nmGPT (Shliazhko et al., 2022) a 1.3B model trained on 440B tokens.\nFinally, to assess the impact of including instruction-like data within the pretraining dataset\nof models (as done in Bloom), we continue CroissantBase pretraining with a short cooldown\nphase on an instruction dataset without any formatting, and call the resulting model Crois-\nsantCool.\n5.1\nBase model\nCroissantLLM obtains strong performances in its model size category, achieving on-par\nperformance with the best monolingual English models on English benchmarks and largely\noutperforming existing mono and multilingual models on French benchmarks.\nEnglish. On English benchmarks (Table 3), CroissantLLM displays performances almost\nequivalent to those of TinyLlama, which has trained on much more English data. We\nsee training on such a large quantity of English tokens enables our model to edge out\nsimilarly sized monolingual models trained on fewer tokens (OPT, Pythia), and larger mul-\ntiingual models (Bloom 3B) demonstrating the interest of pursuing training past Chinchilla\noptimality, especially when splitting model capacity across languages.\nFrench. On French classification benchmarks, CroissantLLM largely outperforms models\nof similar sizes trained on mostly monolingual English or French data, and multilingual\nmodels (Table 4). Performance is on par with the Bloom(3B) model, which is about 3 times\nas large. An interesting phenomenon can be noticed, especially on generative benchmarks\nassessed in few-shot settings: \u201cbase\u201d models trained with instruction-like data perform a\nlot better. This is noticeable with the Bloom(3B) model which outperforms the otherwise\nvastly superior Llama2(7B) model on several tasks, or through the performance gains of\nCroissantCool with respect to CroissantBase.\nImprovements throughout training. The model performance continues to improve on\ndownstream tasks during the entirety of training. We report WMT14 translation perfor-\nmance in Figure 6, and observe similar trends across all tasks. The benefits of training\npast Chinchilla optimality are clear, and although there are diminishing returns past a\ncertain number of steps, training does not seem to saturate. In low training step settings,\n16\nTask\nArc-e\nBelebele (eng)\nHellaswag\nPiQA\nSciQ\nAvg\nGPT-fr(1B)\n0.26\n0.28\n0.29\n0.53\n0.68\n0.40\nmGPT(1.3B)\n0.48\n0.23\n0.35\n0.66\n0.62\n0.47\nBloom(1.1B)\n0.55\n0.24\n0.38\n0.68\n0.89\n0.55\nOPT(1.3B)\n0.61\n0.23\n0.42\n0.72\n0.92\n0.58\nBloom(3B)\n0.63\n0.24\n0.41\n0.72\n0.93\n0.59\nPythia(1.4b)\n0.63\n0.26\n0.42\n0.71\n0.92\n0.59\nCroissantLLM\n0.62\n0.28\n0.42\n0.72\n0.92\n0.59\nCroissantCool\n0.62\n0.26\n0.43\n0.73\n0.91\n0.59\nTinyLlama(1.1B)\n0.65\n0.26\n0.45\n0.73\n0.94\n0.60\nLlama2(7B)\n0.78\n0.46\n0.52\n0.78\n0.97\n0.70\nMistral(7B)\n0.83\n0.85\n0.55\n0.81\n0.98\n0.80\nTable 3: English Benchmarks (5-shot results)\nTask\nHellaswag(fr)\nArc-c(fr)\nfr-vocab\nfr-grammar\nBelebele(fr)\nAvg\nOPT(1.3B)\n0.28\n0.19\n0.50\n0.61\n0.28\n0.37\nPythia(1.4B)\n0.30\n0.20\n0.61\n0.76\n0.23\n0.42\nTinyLlama(1.1B)\n0.33\n0.23\n0.64\n0.67\n0.25\n0.42\nmGPT(1.3B)\n0.27\n0.20\n0.71\n0.73\n0.23\n0.43\nGPT-fr(1B)\n0.30\n0.19\n0.70\n0.79\n0.24\n0.44\nBloom(1.1B)\n0.34\n0.22\n0.76\n0.79\n0.24\n0.47\nCroissantLLM\n0.40\n0.26\n0.75\n0.80\n0.27\n0.50\nBloom(3B)\n0.40\n0.27\n0.78\n0.81\n0.23\n0.50\nCroissantCool\n0.42\n0.28\n0.79\n0.79\n0.28\n0.51\nLlama2(7B)\n0.44\n0.38\n0.76\n0.77\n0.43\n0.56\nMistral(7B)\n0.49\n0.47\n0.78\n0.78\n0.78\n0.66\nTable 4: FrenchBen MC (5-shot results)\nTask\nFGenQ\nFGenAns\nMultiFQuAD\nOSum(A)\nOSum(T)\nAvg\nmGPT(1.3B)\n0.01\n0.00\n0.02\n0.03\n0.01\n0.01\nPythia(1.4B)\n0.13\n0.00\n0.00\n0.00\n0.00\n0.03\nGPT-fr(1B)\n0.03\n0.02\n0.05\n0.10\n0.04\n0.05\nTinyLlama(1.1B)\n0.03\n0.14\n0.12\n0.10\n0.08\n0.09\nCroissantLLM\n0.13\n0.23\n0.21\n0.11\n0.11\n0.16\nBloom(1.1B)\n0.17\n0.28\n0.26\n0.10\n0.04\n0.17\nCroissantCool\n0.17\n0.31\n0.28\n0.20\n0.13\n0.22\nBloom(3B)\n0.20\n0.41\n0.36\n0.17\n0.08\n0.24\nLlama2(7B)\n0.22\n0.39\n0.26\n0.16\n0.20\n0.25\nMistral(7B)\n0.30\n0.71\n0.60\n0.26\n0.32\n0.44\nTable 5: FrenchBench Gen (5-shot ROUGE1 results). Bloom models seem to have strong\nperformance on QA tasks (Fquad), likely due to the inclusion of Question Answering\ndatasets in its pretraining corpus (Lauren\u00e7on et al., 2023).\n17\n0\n50k\n100k\n150k\n200k\nStep\n0\n5\n10\n15\n20\n25\n30\n35\n40\nbleu\nWmt14-fr-en\n0\n50k\n100k\n150k\n200k\nStep\n0\n5\n10\n15\n20\n25\n30\n35\n40\nbleu\nWmt14-en-fr\ncroissant\ngpt-fr\nTinyLlama\nbloom1.1b\nmGPT\nopt1.3b\nbloom3b\nllama7b\nmistralai7b\nCedille\nFigure 6: Performance evolution on the WMT Translation task (5-shot)\nperformance appears to emerge suddenly, reflecting emergent performance experiments in\nthe literature most often obtained through model scaling (Wei et al., 2022b).\nOverall. The 1.1B CroissantLLM displays top-of-its-class performance across both languages\nand all benchmarks, even edging out larger models such as Bloom(3B) on most tasks. All\nmodels remain far off from the performance of the strong 7B Llama and Mistral models.\n5.2\nFinetuning\nBeyond base model performance, we evaluate CroissantLLM downstream performance\nonce finetuned on generalist chat and instruction data, or on specific target tasks (translation,\nsummarization).\n5.2.1\nChat Model\nIt has been shown that supervised fine-tuning on instruction or chat datasets enables\nleveraging model capabilities to their fullest (Wei et al., 2022a).\nTraining. We finetune the base model on public Chat datasets Ultrachat (Ding et al., 2023)\nand Wildchat (Zhao et al., 2024) containing ChatGPT interactions in English and French. We\nalso incorporate 12k samples of translation data (4 % of the SFT dataset). We run finetuning\non CroissantLLM, as well as the Bloom-1b7 and TinyLlama models for comparison. The\nobtained models are further suffixed with \u201cChat\u201d.\nMT-Bench. We evaluate models on the MT-Bench benchmarks, both in English and French.\nAlthough a large difference in performance can be noted between the Bloom model and\nCroissant in favor of the latter, performance differences with TinyLlama are not as significant,\nneither in English nor in French. CroissantLLMChat performs strongly in open-ended\nwriting categories (writing, roleplay, humanities) but struggles with reasoning and extractive\ntasks. Turn 2 performance (reformulation under constraints) is largely lower than Turn 1\nperformance as can be seen in Figures 11 and 12. Our CroissantLLMChat model also vastly\noutperforms the BloomZ 3B model trained by CMArkea on a large chat finetuning corpus\n(Delestre, 2023).\n18\nFigure 7: MT Bench Results (Both Turns)\nModels\nAccuracy\nBloom 1b7 Chat\n24.47 %\nTinyLlamaChat\n44.47 %\nCroissantLLMChat\n47.11 %\nTable 6: French Trivia performance, evaluated by the gold answer and GPT4. Questions are\nasked in English and accuracy is measured by GPT-4. To remove bias due to Supervised\nFinetuning, all base models are finetuned on the same dataset.\nThis hints at the fact that quasi-monolingual models with only a minor share of another\nlanguage in their pretraining corpus can be adapted to a reasonable extent, through subse-\nquent finetuning or continued pretraining, although large pre-training corpora are necessary\nto incorporate sufficient knowledge and reasoning abilities within the base models. We\nnotice large correlations between generation temperature and performance and find Crois-\nsantLLMChat works a lot better with higher temperatures (\u2265 0.4). For fair comparisons,\nwe only report results obtained with low temperature settings in line with other model\nevaluations.\nFrench Trivia. One main question this work attempts to tackle is whether training on\nbilingual data goes beyond augmenting the language understanding and writing capabilities\nof a model in another language, but also equips the models with novel knowledge and\ndifferent cultural biases. We evaluate French cultural knowledge on a Trivia task, consisting\nof questions about France-related topics, asked in English (Table 6). As heuristic-based\nmetrics are insufficient to capture the diversity of possible answers (Faysse et al., 2023),\nwe rely on GPT-4 to score the results. Similar results to the MT-Bench experiments can\nbe noted: although CroissantLLM is the best performing, no huge margins exist with\nrespects to the best English model, TinyLlama, finetuned on the same bilingual SFT set. A\npotential bias might be that questions originate from American websites and are asked in\nEnglish, which might slightly favor TinyLlama. Both CroissantLLM and TinyLlama largely\noutperform Bloom pretrained on a smaller corpora but finetuned on the same data. This\nfurther underlines the importance of pretraining data quantity, and the large knowledge\noverlap between various sources and languages.\n19\nWMT 14\nTICO\nFLORES\nen\u2192fr\nfr\u2192en\nen\u2192fr\nen\u2192fr\nfr\u2192en\nCOMET BLEU\nCOMET BLEU\nCOMET BLEU\nCOMET BLEU\nCOMET BLEU\nNMT models\nNLLB 1.3B\n86.82\n41.59\n84.55\n36.47\n81.15\n40.22\n87.10\n47.49\n87.21\n40.47\n0-shot\nPre-trained models\nLLaMA-2 7B\n84.37\n32.98\n86.66\n38.57\n78.05\n33.75\n85.03\n38.59\n88.75\n41.83\n5-shot\nLLaMA-2 13B\n85.94\n36.76\n87.02\n39.93\n80.04\n38.21\n86.67\n43.49\n89.03\n42.71\n5-shot\nMistral-7B-v0.1\n84.99\n34.82\n87.01\n39.55\n79.34\n37.82\n86.07\n41.31\n88.36\n42.56\n5-shot\nTinyLLaMA\n73.03\n18.13\n82.99\n29.85\n69.20\n20.55\n74.40\n21.17\n85.86\n33.10\n5-shot\nCroissantLLM\n85.11\n38.09\n85.70\n36.30\n78.74\n38.49\n86.85\n46.58\n88.58\n42.83\n5-shot\nSFT models\nTowerInstruct-7B-v0.1 88.07\n46.19\n88.14\n46.75\n81.53\n41.27\n88.38\n48.57\n89.56\n46.34\n0-shot\nTinyLLaMAChat\n\u2013\n\u2013\n\u2013\n\u2013\n73.04\n23.61\n78.08\n27.24\n86.26\n32.80\n0-shot\nCroissantLLMChat\n\u2013\n\u2013\n\u2013\n\u2013\n80.27\n36.99\n86.82\n44.79\n88.38\n41.54\n0-shot\nCroissantLLMChat\n\u2013\n\u2013\n\u2013\n\u2013\n80.72\n38.34\n87.68\n47.11\n88.71\n42.90\n0-shot (Beam Search)\nTable 7: Performance in machine translation, according to COMET-22 and BLEU, across\nthree different benchmarks: WMT14, TICO and FLORES. All translation outputs, unless\nstated otherwise, were generated using greedy decoding. We omit results with our Chat\nmodels (\u2013) on WMT14, since WMT14 was used during fine-tuning.\nTranslation. We run translation evaluations on the Chat models56 and report results in\nTable 7. CroissantLLMChat displays extremely strong performances, in line with the\nstrong few-shot performance of the CroissantLLM base model, outperforming models like\nMistral7B or Llama13B in few-shot settings, and even matching the open source state-of-the-\nart specialized translation model for the size category, the NLLB 1.3B (Team et al., 2022),\ntrained on vastly superior amounts of parallel data.\n5.2.2\nDialog Summarization finetuning\nTo assess performance on specific downstream applications, we finetune base models on\na custom dialog summarization dataset.57 Models are finetuned for three epochs on 6000\nsamples and results are computed through ROUGE and GPT-4 judgment metrics (Table 8).\nCroissantLLM and Bloom(1.7B) models appear to yield strong, yet very similar results,\ntrailing behind the larger Mistral7B model. This hints at the fact that base model performance\n56TinyLLaMAChat corresponds to the TinyLlama model finetuned with the same SFT set as Crois-\nsantLLMChat.\n57Proprietary dataset belonging to Illuin Technology, corresponds to organic customer interactions\nwith a counselor.\n20\nModels\nROUGE1 Coherence Consistence Fluidity Relevance\nCroissantLLM\n0.550\n4.56\n3.93\n4.73\n4.09\nBloom(1.7B)\n0.550\n4.52\n3.96\n4.76\n4.08\nMistral(7B)\n0.588\n4.60\n4.73\n4.72\n4.59\nTable 8: Dialog Summarization Results. Except for ROUGE1, scores are measured by GPT-4,\nout of a maximum of 5.\nModel\nParameters (B) Tokens Per Second Words Per Second\nFrench\nLlama 2\n13\n38.56\n22.18\nLlama 2\n7\n64.05\n37.12\nCroissant\n1.3\n145.40\n101.12\nTinyLlama\n1.1\n152.60\n90.08\nEnglish\nLlama2\n13\n38.17\n28.16\nLlama2\n7\n62.60\n46.49\nCroissant\n1.3\n139.64\n111.41\nTinyLlama\n1.1\n150.16\n112.36\nTable 9: Inference Results in French and English on an A100 GPU with 40GB VRAM\n(average results over 100 tokens generations with 100 tokens input based on 100 Wikipedia\ntext samples, vLLM backend and batch size 1)\nis not always directly correlated to downstream performance post-finetuning, notably on\ntasks requiring few to no prior knowledge (here, keypoint extraction and reformulation).\n5.3\nOptimized Inference\nOur largest model, CroissantLLM, with 1.3B parameters is dimensioned to be extremely\nlightweight when compared to the main proprietary models and the smallest versions of\nthe Llama and Mistral model family. This is motivated by the fact that widespread model\nadoption is bounded by inference compute resources, and most high-performing LLMs\nrequire expensive specialized infrastructures to run, which leads to high inference costs and\nmodel deployment difficulties. The most downloaded Llama model on the HuggingFace\nmodel hub is the 7B variant, reflecting the interest in small, yet effective, models.\nAt a 1.3B scale, CroissantLLM runs easily on local hardware (personal computers, low-\nend smartphones) and is easy to deploy on inexpensive CPU servers or low-end GPU\nservers, unlocking new applications with widespread usage. On higher-end GPUs (Table 9),\nCroissantLLM is both faster (latency) and less memory intensive enabling it to fit bigger\nbatch sizes (throughput). Performance benchmarks are given in Table 9.\nThe decoder nature of CroissantLLM enables to benefit from the rich inference optimization\necosystem that has boomed recently. CroissantLLM is compatible with all main model\nserving libraries and platforms and can easily be quantized or optimized to run on personal\n21\ndevices. We performed 4bit quantization in the GGUF58 format and were able to run the\nmodel on lower-end smartphones at a speed of more than 5 tokens per second.\n5.4\nModel limitations\nEvaluation results indicate the model is strong in its size category, and offers decent perfor-\nmances on writing-based tasks and internal knowledge, and very strong performance on\ntranslation tasks. The small size of the CroissantLLM model however hinders its capacity\nto perform more complex reasoning-based tasks, at least in a zero or few-shot manner in\nits generalist base or chat-model versions. This is aligned with other models of size and\nunderlines the importance of scale for more abstract tasks (Wei et al., 2022b).\nKnowledge Cutoff. The model training dataset has a data cutoff date corresponding to the\nNovember 2023 Wikipedia dump. This is the de facto knowledge cutoff date for our base\nmodel, although a lot of information dates back further.59 Updated versions can be trained\nthrough continued pre-training or subsequent fine-tuning.\nMultilingual performance. CroissantLLM is mostly a French and English model. Code\nperformance is relatively limited, and although some amount of data from other languages is\nincluded within the SlimPajama training set, out-of-the-box performance in other languages\nis not to be expected, although some European languages do work quite well.\nHallucinations. CroissantLLM can hallucinate (Ji et al., 2023; Guerreiro et al., 2023) and\noutput factually incorrect data,60 especially regarding complex topics. This is to be expected\ngiven the small model size, and hallucination rates seem inferior to most models of the\nsame size category although no quantitative assessments have been conducted outside of\nMT-Bench experiments.\n6\nFoundation Model Transparency Index\nTo assess the transparency of our work, we evaluate our model through the Stanford\nTransparency Index (Bommasani et al., 2023) and obtain a total score of 81 %, far ahead of\nproprietary models, as well as most staple open-weights models and large-scale open-source\nefforts (Figure 8).61\nUpstream. The upstream categories include data, compute, and methods dimensions. The\nfully open-source nature and extensive disclosure of training information enable Crois-\nsantLLM to score 88 % of the points. The difficulties in identifying personal information and\nin guaranteeing the exact license, and creators of all data included in internet scale corpora\nprohibit our work from obtaining the full points, although strong efforts have been made in\nonly using data under free-use or open licenses and with no copyright issues, notably by\nexcluding copyright flagged content from our English language corpus.\n58https://github.com/ggerganov/ggml/blob/master/docs/gguf.md\n59Prompted with \"Who is the current French prime minister ?\", it responds: \"The current French\nprime minister is Jean Castex.\" which is outdated by more than 18 months at the time of the writing.\n60As an example, prompted with \"Which French club won the UEFA Champions League ?\", it\nanswers \"The Paris Saint-Germain (PSG) club won the UEFA Champions League in 2020-2021.\"\n61Methodology is described in the appendix, and outline the fact our work relies on the index to\nguide its efforts in transparency, thus putting it at an advantage with respects to prior work such as\nBloom (Scao et al., 2022).\n22\nCroissant\nLlama\nBloomz\nGPT-4\nPaLM2\nTitan\nData\nData labor\nData access\nCompute\nMethods\nData Mitigations\nModel basics\nModel access\nCapabilities\nLimitations\nRisks\nModel Mitigations\nTrustworthiness\nInference\nDistribution\nUsage policy\nModel behavior policy\nUser Interface\nUser data protection\nModel Updates\nFeedback\nImpact\nDocumentation for Deployers\n70%\n40%\n60%\n20%\n20%\n0%\n100%\n29%\n86%\n14%\n0%\n0%\n100%\n0%\n100%\n0%\n0%\n0%\n86%\n57%\n14%\n14%\n14%\n0%\n100%\n75%\n100%\n50%\n75%\n0%\n100%\n50%\n100%\n50%\n0%\n0%\n100%\n100%\n100%\n50%\n67%\n33%\n100%\n100%\n100%\n67%\n33%\n33%\n80%\n60%\n80%\n100%\n80%\n20%\n100%\n67%\n67%\n67%\n67%\n33%\n57%\n57%\n0%\n57%\n29%\n0%\n40%\n60%\n0%\n60%\n40%\n20%\n0%\n0%\n0%\n50%\n0%\n0%\n100%\n50%\n50%\n0%\n0%\n0%\n86%\n71%\n71%\n57%\n71%\n43%\n100%\n40%\n20%\n80%\n60%\n20%\n100%\n0%\n0%\n67%\n0%\n0%\n100%\n100%\n100%\n100%\n100%\n0%\n100%\n67%\n67%\n67%\n67%\n67%\n100%\n100%\n100%\n100%\n100%\n0%\n67%\n33%\n33%\n33%\n33%\n0%\n29%\n14%\n14%\n14%\n14%\n0%\n100%\n100%\n50%\n100%\n100%\n0%\nFigure 8: Aggregated FMTI scores by major dimension of transparency. CroissantLLM\nscores are calculated by the authors, the rest by (Bommasani et al., 2023).\nModel. The model categories include model information, as well as characterizations and\nmitigations of risks, limitations, trustworthiness, and mitigation. CroissantLLM obtains an\naverage of 73 % on this domain due to the wide array of reproducible evaluation results\nreported, but hindered by the lack of third-party external evaluation at the moment, and an\nevaluation of potential harms that is not as extensive as required.\nDownstream. Downstream categories refer to usage policies, user statistics, distribution,\ndocumentation, and model impact assessment. The fully open-access nature of our model\nand distribution channel avoids most of the transparency pitfalls linked to restricted usage\npolicies and user information processing, but the impact of our work remains difficult to\nassess until the model is released. The aggregated score for this category is 80 %.\nEthics Statement\nThis work aims to offset recent English-centric work by enabling the study of the impact\nof language distribution within the pre-training dataset. The objective is to offer valuable\nresources to strengthen the community\u2019s understanding of induced model behavior and\nbiases in that multilingual setup and inform future model and dataset development to be\nmore inclusive.\n23\nModel and Resource Release. The models and all related artifacts are released openly on\nthe CroissantLLM HuggingFace organization62 under an MIT license. No usage restrictions\nare imposed on users whatsoever. We indicate that users are responsible for the content they\ngenerate through the use of CroissantLLM and no redress mechanisms exist for harmful\ncontent disclosure. The model is offered to users openly, and downstream developers are\naccountable for using the model responsibly, although usage examples are provided.\nUsers are free to download and use the model and the associated resources at their will, and\nno monitoring information is kept by the CroissantLLM team regarding individual model\nusage or download information. The distribution platform, HuggingFace, does not share\nany non-public data with the CroissantLLM authors. Any modifications to the models or\nulterior versions of the resources will be released under different version numbers, and\noriginal resources will not be deleted. We encourage discussions and feedback, either\nthrough the HuggingFace model page in the discussion tab, or the issues section of the\nassociated GitHub repository.\nRisk mitigation. We intend for our training process to be fully transparent and as such\nrelease all artifacts related to training. As such, our base model is released as is, without any\nrisk mitigation methods beyond the extensive data curation that has gone into creating the\npre-training data to remove toxic content as much as possible. In our Chat variant of the\nmodel, chat instructions have been explicitly sampled to include alignment instructions that\ntrain the model not to respond to certain prompts.63\nData Leakage. Through the inclusion of canaries in the training set, experiments were\nconducted on model memorization.64 These experiments confirm only artificially extreme\ncases of data repetition lead to in-weight information of inclusion within the training set.\nThis enables us to confidently release the model without fear of potentially private data\nleakage that data filtering methods were unable to detect.\nRisk Assessment. Our extensive evaluation process and the small scale of the CroissantLLM\nmodels allow us to confidently release all artifacts in our efforts of transparency without\nfear of potential misuse beyond what existing models of larger size already enabled. We\nstaged our release by first giving model access to a dozen individuals and enabling them to\nexperiment with them, whether through finetuning experiments, chat interactions, etc. Their\nfeedback was aligned with the authors\u2019 observations in terms of the model capabilities and\nlimitations and no warning flag was raised in terms of toxic content generation or otherwise\n62https://huggingface.co/croissantllm\n63For example when prompted \"Give me medical advice about how to solve my back issues.\",\nCroissantLLM responds I don\u2019t have a physical body or medical knowledge. However, I can provide\nyou with some general tips on how to solve back issues: 1. Stay active: exercises such as yoga\nand stretching can help alleviate back pain. Regular physical activity can help improve flexibility,\nstrengthen the muscles, and reduce the risk of future back problems. 2. Maintain a healthy diet: a\nbalanced diet rich in fruits, vegetables, and lean protein can help reduce inflammation and improve\ndigestion. Avoid high-fat and high-sugar foods, which can aggravate back pain. 3. Manage stress:\nchronic stress can worsen back pain. Try relaxation techniques such as meditation, deep breathing,\nor progressive muscle relaxation to help manage stress levels. 4. Get regular back checkups: regular\ncheckups with a healthcare provider can help detect any underlying back issues early on. This can\nhelp prevent future problems. 5. Seek professional help: if you have persistent back pain, it may\nbe necessary to seek the help of a healthcare professional. They can help diagnose and treat any\nunderlying back issues that may be contributing to your pain.\n64This research is conducted by an independent research team and results will be published after\nthe Technical Report release.\n24\nharmful model behavior. We are confident the release will enable in-depth studying of large\nlanguage models and outweigh the potential risks. To further strengthen compliance with\nFMTI guidelines, we will inform of any government inquiries regarding our model. We\nalso indicate that users are responsible for the content they generate through the use of\nCroissantLLM and no redress mechanisms exist for harmful content disclosure.\n25\nContributors and Acknowledgements\nIn our efforts of transparency, we provide a summary of contributions for all authors and\npersons associated to the paper. Manuel, Patrick, Nuno, and Pierre belong to the core\nteam and have participated throughout all steps of the project, from decision-making to\nreport writing. Manuel coordinated the project and led the data collection, design decisions,\nand model evaluation efforts, and strongly participated in the model training. Pierre is\nthe senior author of the project and was instrumental through constant feedback, project\ncoordination, securing the compute grant, and design decisions. Patrick led the scaling law\nefforts and spearheaded the model training on distributed compute clusters. Nuno led the\nChat and translation finetuning efforts, including constructing model finetuning pipelines\nand datasets, and gave constant feedback throughout the project. Pedro provided help on\nthe development of the pre-training codebase and gave feedback on the pre-training stream\nof the work. Jo\u00e3o and Ricardo constructed the parallel data used for pre-training, which\nincluded efforts in both large-scale data collection and filtering. Duarte assisted with the\nfine-tuning efforts. Ant\u00f3nio worked on base model finetuning on specific tasks and was in\ncharge of the Chat model evaluation and the inference speed benchmark. Caio assisted with\ndata collection efforts and provided high-quality, extensive feedback and notes on the report.\nNicolas assisted with data collection efforts and data scrapping. Antoni adapted the model\nto swiftly run on mobile devices. Gautier, C\u00e9line, Fran\u00e7ois, Andr\u00e9 are senior researchers\nwho provided valuable feedback and important guidance throughout the project and were\ninstrumental in obtaining compute grants.\nThis work is a collaboration of academic and industrial partners. On the academic side, core\nauthors are affiliated with CentraleSup\u00e9lec (Universit\u00e9 Paris Saclay) and Instituto Superior\nT\u00e9cnico de Lisboa, and other contributors are linked to Sorbonne Universit\u00e9 and Imperial\nCollege London. On the industrial side, core authors receive funding from respectively Illuin\nTechnology (Paris), Unbabel (Lisboa), Equall (New York, Lisboa, Paris). Training compute\nis obtained on the Jean Zay supercomputer operated by GENCI IDRIS through compute\ngrant 2023-AD011014668R1 as well as on Adastra through compute grant AD010614770.\nPart of the work was also supported by EU\u2019s Horizon Europe Research and Innovation\nActions (UTTER, contract 101070631), DECOLLAGE (ERC-2022-CoG 101088763), Center for\nResponsible AI (2022-C05i0102-02), and by Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through\ncontract UIDB/50008/2020. Evaluation and finetuning experiments are done on the Ruche\nplatform (Universit\u00e9 Paris Saclay), as well as on privately owned compute centers.\nMany other contributors, not listed as authors of the paper, lent a very welcome helping\nhand to the project, either with data collection efforts, feedback, interesting discussions,\ngrant obtention, etc. In no particular order, we extend our thanks to H\u00e9l\u00e8ne Rousset, Bruno\nHays, Bilel Omrani, Sacha Muller, Elena Hinnekens and Robert Vesoul (Illuin Technology),\nPaul-Henry Courn\u00e8de, Renaud Monnet, G\u00e9raud Faye (CentraleSup\u00e9lec), Pierre-Etienne\nDevineau (DINUM), Louise-Anne Charles (BnF Datalab), the Unbabel TowerLLM team.\nFinally, we would like to warmly thank St\u00e9phane Requena for allowing us access to Jeanzay,\nas well as R\u00e9mi Lacroix, and Etienne Malaboeuf for the debugging and technical support.\n26\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and Beno\u00eet Sagot. 2022. Towards a\ncleaner document-oriented multilingual crawled corpus. In Proceedings of the Thirteenth\nLanguage Resources and Evaluation Conference, pages 4344\u20134355, Marseille, France. Euro-\npean Language Resources Association.\nAnne Abeill\u00e9, Lionel Cl\u00e9ment, and Fran\u00e7ois Toussenel. 2003. Building a Treebank for French,\nchapter Treebanks for Romances Languages. Kluwer, Dordrecht.\nDuarte M Alves, Nuno M Guerreiro, Jo\u00e3o Alves, Jos\u00e9 Pombal, Ricardo Rei, Jos\u00e9 GC de Souza,\nPierre Colombo, and Andr\u00e9 FT Martins. 2023. Steering large language models for machine\ntranslation with finetuning and in-context learning. arXiv preprint arXiv:2310.13448.\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian\nFederman, Dmitriy Genzel, Francisco Guzm\u00e1n, Junjie Hu, Macduff Hughes, Philipp\nKoehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp \u00d6ktem, Eric\nPaquin, Grace Tang, and Sylwia Tur. 2020. Tico-19: the translation initiative for covid-19.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexan-\ndre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu,\nJonathan H. Clark, Laurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav\nMishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan\nXiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob\nAustin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,\nMichele Catasta, Yong Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin,\nMark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber, Markus\nFreitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand,\nHadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,\nMichael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy, Maxim\nKrikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,\nWei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick\nLiu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam\nMoussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie\nPellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter,\nParker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby,\nAmbrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,\nJohn Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui\nYu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and\nYonghui Wu. 2023. Palm 2 technical report.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng\nRen, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-\nguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao,\nBowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang,\nZhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a.\nQwen technical report.\n27\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin,\nChang Zhou, and Jingren Zhou. 2023b. Qwen-vl: A versatile vision-language model for\nunderstanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966.\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla,\nDonald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian\nKhabsa. 2023. The Belebele benchmark: a parallel reading comprehension dataset in 122\nlanguage variants.\nRachel Bawden, Hatim Bourfoune, Bertrand Cabot, Nathan Cassereau, Pierre Cornette,\nMarco Naguib, Aur\u00e9lie N\u00e9v\u00e9ol, and Fran\u00e7ois Yvon. 2024. Les mod\u00e8les Bloom pour le\ntraitement automatique de la langue fran\u00e7aise. Working paper or preprint.\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.\nOn the dangers of stochastic parrots: Can language models be too big? In Proceedings of\nthe 2021 ACM conference on fairness, accountability, and transparency, pages 610\u2013623.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for\nanalyzing large language models across training and scaling.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA:\nReasoning about physical commonsense in natural language.\nRishi Bommasani, Kevin Klyman, Shayne Longpre, Sayash Kapoor, Nestor Maslej, Betty\nXiong, Daniel Zhang, and Percy Liang. 2023. The foundation model transparency index.\nEleftheria Briakou, Colin Cherry, and George Foster. 2023. Searching for needles in a\nhaystack: On the role of incidental bilingualism in PaLM\u2019s translation capability. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9432\u20139452, Toronto, Canada. Association for Computational\nLinguistics.\nNicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej Kos, and Dawn Song. 2019. The\nsecret sharer: Evaluating and testing unintended memorization in neural networks. In\n28th USENIX Security Symposium (USENIX Security 19), pages 267\u2013284, Santa Clara, CA.\nUSENIX Association.\nStephen Casper, Carson Ezell, Charlotte Siegmann, Noam Kolt, Taylor Lynn Curtis, Ben-\njamin Bucknall, Andreas Haupt, Kevin Wei, J\u00e9r\u00e9my Scheurer, Marius Hobbhahn, Lee\nSharkey, Satyapriya Krishna, Marvin Von Hagen, Silas Alberti, Alan Chan, Qinyi Sun,\nMichael Gerovitch, David Bau, Max Tegmark, David Krueger, and Dylan Hadfield-Menell.\n2024. Black-box access is insufficient for rigorous AI audits.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker\nBarnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,\nPengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk\nMichalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou,\nDaphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov,\n28\nRyan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanu-\nmalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon\nChild, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta,\nMark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck,\nJeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: scaling language modeling with\npathways.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2\nreasoning challenge.\nCyrile Delestre. 2023. [link].\nMartin d\u2019Hoffschmidt, Wacim Belblidia, Tom Brendl\u00e9, Quentin Heinrich, and Maxime Vidal.\n2020. Fquad: French question answering dataset.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu,\nMaosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling\nhigh-quality instructional conversations. arXiv preprint arXiv:2305.14233.\nMoussa Kamal Eddine, Antoine J-P Tixier, and Michalis Vazirgiannis. 2020. Barthez: a\nskilled pretrained french sequence-to-sequence model.\nManuel Faysse, Gautier Viaud, C\u00e9line Hudelot, and Pierre Colombo. 2023. Revisiting\ninstruction fine-tuned model evaluation to guide industrial applications. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing. Association for\nComputational Linguistics.\nPatrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat.\n2023. Scaling laws for multilingual neural machine translation. In Proceedings of the 40th\nInternational Conference on Machine Learning, volume 202 of Proceedings of Machine Learning\nResearch, pages 10053\u201310071. PMLR.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Lau-\nrence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria\nReynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nNuno M Guerreiro, Duarte M Alves, Jonas Waldendorf, Barry Haddow, Alexandra Birch,\nPierre Colombo, and Andr\u00e9 FT Martins. 2023. Hallucinations in large multilingual\ntranslation models. Transactions of the Association for Computational Linguistics, 11:1500\u2013\n1517.\nMichael Hart. 1971. Project gutenberg.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\nTom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc,\nAurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal large language models.\nEvan Hubinger, Carson Denison, Jesse Mu, Mike Lambert, Meg Tong, Monte MacDi-\narmid, Tamera Lanham, Daniel M. Ziegler, Tim Maxwell, Newton Cheng, Adam Jermyn,\n29\nAmanda Askell, Ansh Radhakrishnan, Cem Anil, David Duvenaud, Deep Ganguli, Fazl\nBarez, Jack Clark, Kamal Ndousse, Kshitij Sachan, Michael Sellitto, Mrinank Sharma,\nNova DasSarma, Roger Grosse, Shauna Kravec, Yuntao Bai, Zachary Witten, Marina\nFavaro, Jan Brauner, Holden Karnofsky, Paul Christiano, Samuel R. Bowman, Logan\nGraham, Jared Kaplan, S\u00f6ren Mindermann, Ryan Greenblatt, Buck Shlegeris, Nicholas\nSchiefer, and Ethan Perez. 2024. Sleeper agents: Training deceptive llms that persist\nthrough safety training.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang,\nAndrea Madotto, and Pascale Fung. 2023. Survey of hallucination in natural language\ngeneration. ACM Computing Surveys, 55(12):1\u201338.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut\nLavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b.\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary,\nChris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian\nBressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, L\u00e9lio Renard Lavaud,\nLucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Th\u00e9ophile Gervet, Thibaut Lavril, Thomas Wang,\nTimoth\u00e9e Lacroix, and William El Sayed. 2024. Mixtral of experts.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020a. Scaling laws for\nneural language models. CoRR, abs/2001.08361.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon\nChild, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020b. Scaling laws for\nneural language models.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Fer-\nrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau,\nLeandro von Werra, and Harm de Vries. 2022. The stack: 3 tb of permissively licensed\nsource code.\nTaku Kudo. 2018. Subword regularization: Improving neural network translation models\nwith multiple subword candidates.\nJulien Launay, Elena Tommasone, Baptiste Pannier, Fran\u00e7ois Boniface, Am\u00e9lie Chatelain,\nAlessandro Cappelli, Iacopo Poli, and Djam\u00e9 Seddah. 2021. Pagnol: An extra-large french\ngenerative model.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del\nMoral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada,\nHuu Nguyen, J\u00f6rg Frohberg, Mario \u0160a\u0161ko, Quentin Lhoest, Angelina McMillan-Major,\nGerard Dupont, Stella Biderman, Anna Rogers, Loubna Ben allal, Francesco De Toni,\nGiada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo,\nJavier de la Rosa, Paulo Villegas, Tristan Thrush, Shayne Longpre, Sebastian Nagel, Leon\nWeber, Manuel Mu\u00f1oz, Jian Zhu, Daniel Van Strien, Zaid Alyafeai, Khalid Almubarak,\nMinh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz\n30\nSuarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan, Hieu Tran, Ian Yu,\nSuhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Alexandra\nLuccioni, and Yacine Jernite. 2023. The bigscience roots corpus: A 1.6tb composite\nmultilingual dataset.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel\nLamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel\nZebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham\nOblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Ab-\nulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,\nWenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zh-\ndanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane\nAnderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry\nBahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean Hughes, Thomas Wolf, Arjun\nGuha, Leandro von Werra, and Harm de Vries. 2023a. Starcoder: may the source be with\nyou!\nYuanzhi Li, S\u00e9bastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. 2023b. Textbooks are all you need ii: phi-1.5 technical report.\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond,\nTom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy,\nCyprien de Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes\nWelbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Suther-\nland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals.\n2022. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097.\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text\nSummarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational\nLinguistics.\nAlexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Estimating the\ncarbon footprint of bloom, a 176b parameter language model.\nRisto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-\nMari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus,\nThomas Wang, Nouamane Tazi, Teven Scao, Thomas Wolf, Osma Suominen, Samuli\nSairanen, Mikko Merioksa, Jyrki Heinonen, Aija Vahtola, Samuel Antao, and Sampo\nPyysalo. 2023. FinGPT: Large generative models for a small language. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Language Processing, pages 2710\u20132726,\nSingapore. Association for Computational Linguistics.\nNiklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,\nNouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. Scaling data-\nconstrained language models.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman,\nTeven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022.\nCrosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.\n31\nNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. Crows-pairs: A\nchallenge dataset for measuring social biases in masked language models.\nRoberto Navigli, Simone Conia, and Bj\u00f6rn Ross. 2023. Biases in large language models:\nOrigins, inventory, and discussion. J. Data and Information Quality, 15(2).\nNewYorkTimes. 2023.\nThe times sues openai and microsoft over a.i. use of\ncopyrighted\nwork.\nhttps://www.nytimes.com/2023/12/27/business/media/\nnew-york-times-open-ai-microsoft-lawsuit.html.\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Der-\nnoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023. Culturax: A cleaned, enormous,\nand multilingual dataset for large language models in 167 languages.\nOpenAI, :, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,\nFlorencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haim-\ning Bao, Mo Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro,\nChristopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brak-\nman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie\nCampbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke\nChan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen,\nMark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,\nJeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien\nDeville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty\nEleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Sim\u00f3n Posada Fishman,\nJuston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun\nGogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott\nGray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han,\nJeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade\nHickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost\nHuizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun\nJin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, \u0141ukasz Kaiser, Ali\nKamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick,\nJong Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight,\nDaniel Kokotajlo, \u0141ukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic,\nGretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Le-\nung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin,\nTheresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning,\nTodor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob Mc-\nGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David\nMedina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin,\nVinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk,\nDavid M\u00e9ly, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard\nNgo, Hyeonwoo Noh, Long Ouyang, Cullen O\u2019Keefe, Jakub Pachocki, Alex Paino, Joe\nPalermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex\nPassos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres,\nMichael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass,\nVitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec\nRadford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach,\nCarl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani\n32\nSanturkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam,\nKyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon\nSidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin\nSokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya\nSutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian,\nElizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cer\u00f3n Uribe,\nAndrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang,\nAlvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter\nWelinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel\nWolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao,\nTao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong\nZhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk,\nand Barret Zoph. 2023. Gpt-4 technical report.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for\nautomatic evaluation of machine translation. In Proceedings of the 40th annual meeting of\nthe Association for Computational Linguistics, pages 311\u2013318.\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third\nConference on Machine Translation: Research Papers, pages 186\u2013191, Belgium, Brussels.\nAssociation for Computational Linguistics.\nGema Ram\u00edrez-S\u00e1nchez, Jaume Zaragoza-Bernabeu, Marta Ba\u00f1\u00f3n, and Sergio Ortiz Rojas.\n2020. Bifixer and bicleaner: two open-source tools to clean your parallel data. In Proceed-\nings of the 22nd Annual Conference of the European Association for Machine Translation, pages\n291\u2013298, Lisboa, Portugal. European Association for Machine Translation.\nRicardo Rei, Jos\u00e9 G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya\nGlushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 F. T. Martins. 2022a. COMET-22:\nUnbabel-IST 2022 submission for the metrics shared task. In Proceedings of the Seventh Con-\nference on Machine Translation (WMT), pages 578\u2013585, Abu Dhabi, United Arab Emirates\n(Hybrid). Association for Computational Linguistics.\nRicardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine\nMaroti, Jos\u00e9 G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie,\nand Andr\u00e9 F. T. Martins. 2022b. CometKiwi: IST-unbabel 2022 submission for the quality\nestimation shared task. In Proceedings of the Seventh Conference on Machine Translation\n(WMT), pages 634\u2013645, Abu Dhabi, United Arab Emirates (Hybrid). Association for\nComputational Linguistics.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli\u00b4c, Sebastian Ruder, and Iryna Gurevych. 2021. How\ngood is your tokenizer? on the monolingual performance of multilingual language\nmodels.\nPamela Samuelson. 2023. Generative ai meets copyright. Science, 381(6654):158\u2013161.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori\nHashimoto. 2023. Whose opinions do language models reflect?\nNikhil Sardana and Jonathan Frankle. 2023. Beyond chinchilla-optimal: Accounting for\ninference in language model scaling laws.\n33\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022.\nBloom: A 176b-parameter open-access multilingual language model. arXiv preprint\narXiv:2211.05100.\nDjam\u00e9 Seddah, Reut Tsarfaty, Sandra K\u00fcbler, Marie Candito, Jinho D. Choi, Rich\u00e1rd\nFarkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola Galletebeitia, Yoav Goldberg,\nSpence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam\nPrzepi\u00f3rkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin\nWoli\u00b4nski, Alina Wr\u00f3blewska, and Eric Villemonte de la Clergerie. 2013. Overview of the\nSPMRL 2013 shared task: A cross-framework evaluation of parsing morphologically rich\nlanguages. In Proceedings of the Fourth Workshop on Statistical Parsing of Morphologically-\nRich Languages, pages 146\u2013182, Seattle, Washington, USA. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of\nrare words with subword units.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova, Vladislav Mikhailov, Anastasia\nKozlova, and Tatiana Shavrina. 2022. mgpt: Few-shot learners go multilingual.\nAntoine Simoulin and Benoit Crabb\u00e9. 2021. Un mod\u00e8le Transformer G\u00e9n\u00e9ratif Pr\u00e9-entrain\u00e9\npour le ______ fran\u00e7ais. In Traitement Automatique des Langues Naturelles, pages 246\u2013255,\nLille, France. ATALA.\nDaria Soboleva,\nFaisal Al-Khateeb,\nRobert Myers,\nJacob R Steeves,\nJoel Hes-\ntness,\nand\nNolan\nDey.\n2023.\nSlimPajama:\nA\n627B\ntoken\ncleaned\nand\ndeduplicated\nversion\nof\nRedPajama.\nhttps://www.cerebras.net/blog/\nslimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2023.\nRoformer: Enhanced transformer with rotary position embedding.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth\nHeafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,\nAnna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett,\nKaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,\nNecip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj\nGoswami, Francisco Guzm\u00e1n, Philipp Koehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang. 2022. No language left behind: Scaling\nhuman-centered machine translation.\nJ\u00f6rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eight\nInternational Conference on Language Resources and Evaluation (LREC\u201912), Istanbul, Turkey.\nEuropean Language Resources Association (ELRA).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023a.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas\n34\nBlecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude\nFernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman\nGoyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,\nViktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning\nMao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor,\nAdina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open foundation and fine-tuned\nchat models.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan\nDu, Andrew M. Dai, and Quoc V. Le. 2022a. Finetuned language models are zero-shot\nlearners.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022b. Emergent abilities of\nlarge language models.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice\nscience questions.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-\ntext transformer.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag:\nCan a machine really finish your sentence?\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang,\nYifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained\nmodel. arXiv preprint arXiv:2210.02414.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. Tinyllama: An open-\nsource small language model.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam\nShleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and\nLuke Zettlemoyer. 2022. Opt: Open pre-trained transformer language models.\nWenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2024.\n(inthe)wildchat: 570k chatGPT interaction logs in the wild. In The Twelfth International\nConference on Learning Representations.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez,\nand Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.\n35\nCroissant\nLlama\nBloomz\nGPT-4\nPaLM2\nTitan\nUpstream\nModel\nDownstream\n88%\n44%\n66%\n22%\n19%\n0%\n73%\n67%\n48%\n61%\n45%\n18%\n80%\n51%\n46%\n60%\n54%\n17%\nFigure 9: Aggregated FMTI\nA\nFMTI\nDisclaimers and Methodology\nThe FMTI grid is meant to assess Foundation Models, but base models and models that\nwere fine-tuned on instruction or chat datasets imply different training, evaluation and data\ncuration protocols, thus largely modifying their assessment through the FMTI. Training an\ninstruction or chat model from a base model is a process that has recently been completely\ndemocratized through the use of crowdsourced or synthetic datasets, and individuals are\nnow fully capable of finetuning their own model variants in a variety of manners. As such,\nwe consider this work\u2019s contribution mainly lies in the base model training, and are aware\nthat SFT finetuning of the Croissant model will be done outside of the author\u2019s control;\nwhether on proprietary data, synthetic chat datasets, crowdsourced chat instructions -\nleading to different legal and copyright implications for the finetuned models. We thus\nfocus on the base model in our evaluation and give the complete criteria list as detailed in\nthe appendix.\nTransparency evaluation should ideally be done by an independent third party as there are\nobvious biases in auto-evaluating a model, and point attribution is not always trivial for\ncertain criteria. As such, we take a rather conservative approach to point attribution and\ndetail our process in an open document. Efforts have consciously been made within the\ntechnical report to include information not initially given to validate certain criteria, which\nputs us at a clear advantage with respect to work published before the index\u2019s release.\nWe are open to discussions for potential scoring modifications, and consider these FMTI\nscores to be the reflection of our compliance efforts to the listed transparency principles,\nrather than scores fairly comparable to the larger foundation models with vastly different\nusage objectives.\n36\nDataset\nSize (GB) Documents Tokens (M) Token/Doc\nCulturaxFr\n1216.03\n363197906\n292833.75\n806.14\nWikisourceFr\n10.84\n2557238\n2699.00\n1055.44\nWikipedia20231101.fr\n7.37\n2563646\n2002.51\n781.12\nJadeOpendata\n5.19\n550065\n1295.29\n2354.79\nJorfOpendata\n3.83\n3189949\n967.10\n303.17\nLegiOpendata\n3.56\n2151510\n816.44\n379.47\nAccoOpendata\n3.39\n251332\n758.15\n3016.52\nIncaOpendata\n2.60\n369687\n627.32\n1696.90\nProjectgutenbergFr\n0.97\n2447\n301.19\n123086.16\nCappOpendata\n0.91\n71949\n247.14\n3434.97\nIlluinLayoutDatasetTextOnly 0.77\n291604\n191.11\n655.38\nDebatsOpendata\n0.77\n2114\n149.09\n70524.31\nCassOpendata\n0.76\n140803\n206.04\n1463.35\nKaliOpendata\n0.68\n402963\n152.33\n378.01\nSwissLegislation\n0.26\n11086\n68.33\n6163.81\nFrenchOpenSubtitles\n0.15\n5379\n41.84\n7779.26\nCnilOpendata\n0.12\n15168\n26.37\n1738.72\nBnfClean2023\n0.10\n341\n27.04\n79295.71\nQrOpendata\n0.10\n530\n21.73\n41005.03\nSardeOpendata\n0.09\n221278\n28.10\n127.01\nDoleOpendata\n0.08\n4000\n19.36\n4839.07\nConstitOpendata\n0.07\n6977\n15.27\n2188.28\nFrenchLibrispeechTextOnly\n0.06\n255631\n12.91\n50.49\nFrenchPodcasts\n0.01\n1237\n1.56\n1259.90\nFrenchPoetry\n0.00\n1721\n0.76\n441.23\nTrain\n1258.70\n376266561\n303509.73\n806.63\nTable 10: French Data mix\nDataset\nSize (GB) Documents Tokens (M) Token/Doc\nSlimPajama\n2333.77\n590194779\n630441.67\n1068.19\nProject Gutenberg PG19 10.67\n28602\n23580.49\n824435.00\nGutenberg Canaries\n2.75\n7515\n555.40\n73905.01\nTrain\n2351.13\n591230543\n655637.48\n1108.94\nTable 11: English Data mix\nB\nAdditional data details\nB.1\nFrench Data\nRefer to Table 10.\nB.2\nEnglish data\nRefer to Table 11.\n37\nDataset\nSize (GB) Documents Tokens (M) Token/Doc\nStarcoderdataJava\n82.49\n20061773\n29740.73\n1482.46\nStarcoderdataJavascript\n61.64\n19534285\n24546.60\n1256.59\nStarcoderdataPython\n57.00\n12856649\n24605.09\n1913.80\nStarcoderdataC\n50.60\n8526791\n15791.76\n1852.02\nStarcoderdataCpp\n45.84\n6343527\n19607.90\n3091.01\nPypiClean\n29.20\n2428172\n12120.74\n4991.72\nStarcoderdataSql\n10.38\n965666\n3278.24\n3394.80\nStarcoderdataJupyterScriptsDedupFiltered\n6.67\n905365\n2567.77\n2836.18\nStarcoderdataJupyterStructuredCleanDedup 5.55\n662056\n2119.72\n3201.72\nStarcoderdataJson\n5.43\n4741547\n2165.87\n456.79\nStarcoderdataTex\n4.86\n517551\n1916.88\n3703.76\nStarcoderdataShell\n2.98\n2196327\n1178.17\n536.43\nCodeContests\n2.79\n1485888\n1228.61\n826.85\nStarcoderdataCuda\n0.52\n57570\n227.24\n3947.14\nGithubJupyterCodeToText\n0.48\n46978\n159.49\n3395.09\nStarcoderdataDockerfile\n0.41\n565791\n161.48\n285.41\nStarcoderdataIdris\n0.03\n7942\n11.72\n1475.09\nTrain\n366.87\n81903878\n141428.02\n1726.76\nTable 12: Code Data mix\nDataset\nSize (GB) Documents Tokens (M) Token/Doc\nUnbabelFrEn\n113.35\n407858836\n35641.60\n87.39\nThesesFr20132023\n0.36\n95009\n81.60\n858.91\nOriginalSongsLyricsWithFrenchTranslation 0.20\n75020\n53.48\n712.93\nTrain\n113.91\n408028865\n35776.69\n87.68\nTable 13: Parallel Data mix\nB.3\nCode data\nRefer to Table 12.\nB.4\nParallel data\nRefer to Table 13.\nOPUS data distribution is given in Figure 10.\nB.5\nScaling Law Corpus\nFor the scaling law experiments, we use a smaller subsampled dataset, consisting of splits of\nFrench, English, and Code data we vary in ratio to study the impact of language distribution.\nIn total, we train on 50 billion tokens and sample from the following datasets: French\nhttps://huggingface.co/datasets/manu/french-30b, English https://huggingface.co/\ndatasets/manu/english-60b and Code https://huggingface.co/datasets/manu/code_20b.\nIn all datasets, a breakdown of the sources is given in the dataset_stats.csv file at the root of\nthe data folder. The source distribution is chosen to be consistent with the final distribution\nused during main model training so as not to affect the conclusions.\n38\nOPUS ccmatrix v1\nOPUS paracrawl v9\nOPUS multiparacrawl v7.1\nOPUS multiccaligned v1\nOPUS giga_fren v2\nOPUS europat v3\nOPUS unpc v1.0\nOPUS multiun v1\nOPUS ccaligned v1\nOPUS eubookshop v2\nOPUS tildemodel v2018\nFacebook wikimatrix 1\nOPUS europarl v8\nOPUS dgt v2019\nOPUS wikimedia v20210402\nOPUS opus100_train 1\nOPUS wikipedia v1.0\nStatmt news_commentary 16\nOPUS elitr_eca v1\nLinguaTools wikititles 2014\nOPUS jrc_acquis v3.0\nOPUS globalvoices v2018q4\nOPUS tatoeba v2\nOPUS emea v3\nOPUS ecb v1\nOPUS kde4 v2\nOPUS bible_uedin v1\nOPUS gnome v1\nOPUS openoffice v3\nOPUS infopankki v1\nOPUS euconst v1\nOPUS ubuntu v14.10\nOPUS tico_19 v20201028\nOPUS php v1\nOPUS kdedoc v1\n104\n106\n108\nNumber of Samples\nFigure 10: Opus Data Distribution withinn our training dataset\n39\nC\nChat examples\nThe following results were not cherry-picked and were generated with a temperature of 0.5,\na Top-P of 0.95 and a Top-K of 40. They focus on Writing tasks which CroissantLLM is best\nat.\nC.1\nTranslate the following article in French:\nThe Principality of Sealand (/sil\u00e6nd/) is an unrecognised micronation on HM Fort Roughs (also\nknown as Roughs Tower),[5] an offshore platform in the North Sea approximately twelve kilometres\n(6+1\u20442 nautical miles) off the coast of Suffolk, England.[3] Roughs Tower is a Maunsell Sea Fort that\nwas built by the British in international waters during World War II. Since 1967, the decommissioned\nRoughs Tower has been occupied and claimed as a sovereign state by the family and associates of\nPaddy Roy Bates. Bates seized Roughs Tower from a group of pirate radio broadcasters in 1967 with\nthe intention of setting up his own station there. Bates and his associates have repelled incursions\nfrom vessels from rival pirate radio stations and the U.K.\u2019s royal navy using firearms and petrol\nbombs.[6][7] Since 1987, when the United Kingdom extended its territorial waters to 12 nautical\nmiles, the platform has been in British territory.\nLa Principaut\u00e9 de Sealand (/sil\u00e6nd/) est une micronation non reconnue sur le fort, Fort\nRoughs (alias Tour de Roughs) situ\u00e9 \u00e0 environ 12 kilom\u00e8tres (6,5 miles) au large des c\u00f4tes\nde Suffolk, en Angleterre. Fort Roughs est un fort Maunsell qui a \u00e9t\u00e9 construit par les\nBritanniques dans les eaux internationales pendant la Seconde Guerre mondiale. Depuis\n1967, le fort Roughs a \u00e9t\u00e9 occup\u00e9 et revendiqu\u00e9 en tant que pays souverain par la famille et\nles associ\u00e9s de Paddy Roy Bates. Bates a saisi Roughs Tower \u00e0 un groupe de radiotalkers\npirates en 1967 dans l\u2019intention de cr\u00e9er sa propre station radio. Bates et ses associ\u00e9s ont\nrepouss\u00e9 les incursions de navires de radiotalkers rivaux et de la marine royale britannique\n\u00e0 l\u2019aide d\u2019armes \u00e0 feu et de bombes de carburant. Depuis 1987, lorsque le Royaume-Uni a\n\u00e9tendu ses eaux territoriales \u00e0 12 milles marins, la plate-forme a \u00e9t\u00e9 dans la zone territoriale\nbritannique.\nC.2\nWhat can I do in Marseille during the winter ?\n1. Visit the Vieux-Port: The Vieux-Port is the heart of Marseille and is a great place to relax\nand enjoy the beautiful architecture and sights of the city. You can also take a boat tour to\nsee the island of the Frioul.\n2. Stroll through the Calanques: The Calanques are a series of stunning rock formations\nlocated just outside of Marseille. You can take a boat tour to see the calanques and enjoy the\nbeautiful scenery.\n3. Visit the Parc National des Calanques: The Parc National des Calanques is a protected\nnatural area that is home to some of the most beautiful scenery in Marseille. You can take a\nhike through the calanques and enjoy the fresh air and beautiful scenery.\n4. Explore the Cathedrale de Notre-Dame de la Garde: Notre-Dame de la Garde is a beautiful\ncathedral located in Marseille. You can take a walk around the cathedral and enjoy the\nbeautiful architecture and views of the city.\n5. Visit the Mucem: The Mucem is a museum that is located in Marseille and is dedicated to\nthe history and culture of Marseille. You can explore the museum and learn about the city\u2019s\nhistory and heritage.\n40\nC.3\nR\u00e9dige une lettre de motivation pour un job \u00e9tudiant de barman.\nObjet : Candidature pour un job \u00e9tudiant de barman\nCher [nom du recruteur],\nJe me permets de vous adresser ma candidature pour un job \u00e9tudiant de barman au sein de\nvotre \u00e9tablissement.\nJe suis actuellement \u00e0 la recherche d\u2019un emploi \u00e0 temps partiel pour financer mes \u00e9tudes, et\nje suis convaincu que votre entreprise est en mesure de m\u2019offrir un emploi adapt\u00e9 \u00e0 mes\ncomp\u00e9tences et mes aspirations professionnelles.\nJe suis quelqu\u2019un de dynamique, motiv\u00e9 et souriant. J\u2019ai une grande passion pour la cuisine\net la p\u00e2tisserie, et je suis toujours \u00e0 la recherche de nouvelles exp\u00e9riences culinaires. Je suis\n\u00e9galement tr\u00e8s \u00e0 l\u2019aise avec les clients et je sais m\u2019adapter \u00e0 toutes les situations.\nJe suis titulaire d\u2019un dipl\u00f4me en [d\u00e9crire le dipl\u00f4me] et j\u2019ai d\u00e9j\u00e0 travaill\u00e9 dans diff\u00e9rents\n\u00e9tablissements de restauration. J\u2019ai \u00e9galement eu l\u2019occasion de participer \u00e0 des \u00e9v\u00e9nements\nde barman lors de mes pr\u00e9c\u00e9dentes exp\u00e9riences professionnelles.\nJe suis convaincu que mes comp\u00e9tences et mon exp\u00e9rience me permettront de contribuer\nefficacement \u00e0 votre \u00e9quipe de barman. Je suis \u00e9galement pr\u00eat \u00e0 apprendre et \u00e0 m\u2019adapter \u00e0\nvos besoins sp\u00e9cifiques pour m\u2019int\u00e9grer rapidement et efficacement.\nJe suis disponible tous les soirs de la semaine, de 18h \u00e0 22h, et les week-ends \u00e9galement. Je\nsuis \u00e9galement pr\u00eat \u00e0 me d\u00e9placer pour des \u00e9v\u00e9nements de barman.\nJe suis disponible pour un entretien \u00e0 votre convenance, pour discuter plus en d\u00e9tail de\nma candidature et de mes motivations. Je reste \u00e0 votre disposition pour toute information\ncompl\u00e9mentaire.\nCordialement,\n[Votre nom]\nC.4\nShow me how to code a Depth First Search in Python\nI can provide you with the code for a depth-first search in python. Here is the code:\ndef\ndfs ( graph ,\ns t a r t ) :\nv i s i t e d = set ( )\nstack = [ ]\nv i s i t e d . add ( s t a r t )\nstack . append ( s t a r t )\nwhile\nstack :\nvertex = stack . pop ( )\nv i s i t e d . add ( vertex )\nfor\nneighbor in graph [ vertex ] :\ni f\nneighbor not in\nv i s i t e d :\nstack . append ( neighbor )\nthis code defines a function called \u2018dfs\u2018 that takes a graph as input and a starting vertex. The\nfunction uses a stack to keep track of the vertices to visit and a set to keep track of visited\n41\nvertices. The vertices are added to the stack. The function then enters a while loop that\ncontinues until the stack is empty. Inside the loop, the function pops a vertex from the stack,\nadds it to the graph, and then poppes another vertex from the stack. This process continues\nuntil all vertices have been visited. Finally, the function returns the set of vertices that have\nbeen visited. You can use this function to perform a depth-first search on a graph.\nC.5\nGive me medical advice about how to solve my back issues.\nI don\u2019t have a physical body or medical knowledge. However, I can provide you with some\ngeneral tips on how to solve back issues:\n1. Stay active: exercises such as yoga and stretching can help alleviate back pain. Regular\nphysical activity can help improve flexibility, strengthen the muscles, and reduce the risk of\nfuture back problems.\n2. Maintain a healthy diet: a balanced diet rich in fruits, vegetables, and lean protein can\nhelp reduce inflammation and improve digestion. Avoid high-fat and high-sugar foods,\nwhich can aggravate back pain.\n3. Manage stress: chronic stress can worsen back pain. Try relaxation techniques such as\nmeditation, deep breathing, or progressive muscle relaxation to help manage stress levels.\n4. Get regular back checkups: regular checkups with a healthcare provider can help detect\nany underlying back issues early on. This can help prevent future problems.\n5. Seek professional help: if you have persistent back pain, it may be necessary to seek the\nhelp of a healthcare professional. They can help diagnose and treat any underlying back\nissues that may be contributing to your pain.\n42\nD\nResults\nD.1\nMethodology\nBase models are evaluated through the LM Evaluation harness framework (Gao et al.,\n2021). For classification tasks, we choose the answer with the largest log likelihood when\nconcatenated with the prompt, as is implemented within the framework.\nFor generative tasks, we simply generate with the default settings, which is greedy sampling.\nWe acknowlege CroissantLLM works best with higher temperature values but did not want\nto introduce stochasticity to the evaluation. We also limit each benchmark task to 2000\nsamples at most, to shorten evaluation time. All evaluations are reproducible through the\ncode at https://github.com/ManuelFay/lm-evaluation-harness which will be merged in\nthe main repository.\nD.2\nPerplexity results\nCommunication with the Pagnol model authors (Launay et al., 2021), enabled us to compute\nCroissantLLM perplexity on the same dataset and in the same conditions as the PagnolXL\nresults shared in the paper. PagnolXL is a 1.5B decoder model, and is reported to obtain a\nFrench Treebank (Abeill\u00e9 et al., 2003)65 Perplexity value of 16.40 (test set), computed 66 with\na rolling stride of 512 and a max sequence length of 2048 tokens. CroissantLLM obtains a\nperplexity value of 10.37, largely outperforming the biggest model in the Pagnol family67.\nD.3\nMT-Bench\nTurn 1 (Figure 11) and Turn 2 (Figure 12) results are shown. We notice small models, struggle\nwith reasoning based tasks and contrained generation imposed by Turn 2 prompts. Figures\n14 and 15 compare our results on small language models to other common bigger models.68\nD.4\nBias Assessment\nWe assess bias through CROWS (Nangia et al., 2020), the Crowdsourced Stereotype Pairs\nbenchmark that cover stereotypes dealing with nine types of bias, like race, religion, and\nage and report results in Table 16. We find CroissantLLM is in line, or slightly less biased\nthan other models, notably in French.\n65http://www.llf.cnrs.fr/en/Gens/Abeille/French-Treebank-fr.php in its SPMRL instance (Sed-\ndah et al., 2013)\n66https://huggingface.co/docs/transformers/perplexity\n67Note that CroissantLLM and Pagnol tokenizers are different\n68Results in French for models with sizes over 7B parameters were extracted from https:\n//huggingface.co/datasets/bofenghuang/mt-bench-french and results in English are from https:\n//huggingface.co/spaces/lmsys/mt-bench/tree/main/data/mt_bench/model_judgment\n43\nFigure 11: MT Bench Results (Turn 1)\nFigure 12: MT Bench Results (Turn 2)\nModels\nWri Ro Reas Math Cod Ext STEM Hum Avg\nCroissantLLMChat\n5.32 5.35\n1.9\n1.16\n1.8\n1.4\n4.55\n3.75 3.15\nTinyLlamaChat\n5\n4.1\n1.45\n1\n2.1\n1.6\n3.9\n4.85\n3\nBloom 1B7 Chat\n4.62 3.25\n1.2\n1.5\n1.45 1.5\n2.85\n2.22 2.32\nCMArkea BloomZ 3B\n2.65 2.85 1.85\n1.15\n1.2\n2.3\n3.65\n2.7\n2.29\nVigostral 7B Chat\n7.7 7.85 4.85\n3.65 4.65 7.75\n7.35\n9.2\n6.62\nVigogne 2 7b Chat\n5.35 6.25 2.75\n2.2\n2.47 3.4\n6.05\n6.68 4.39\nOpenHermes Mistral 7B 8.8\n7.5\n5.1\n4.05 5.55 6.2\n8.35\n9.4\n6.87\nVigogne 2 70B Chat\n9.4 8.25 4.75\n4.3\n5.35 7.25\n9.1\n9.43 7.23\nMixtral 8x7b Instruct\n9.65 8.88 6.95\n4.95\n4.6 8.55\n9.5\n9.6\n7.84\nMistral Medium\n9.6 9.05\n5.4\n6.1\n7.35 9.25\n9.3\n9.75 8.23\nGPT 3.5 Turbo\n8.75 8.93 5.05\n5.65 7.85 9.05\n9.05\n9.68\n8\nGPT 4\n9.6 9.65 8.55\n8.5\n8.35 9.2\n9.85\n9.88\n9.2\nTable 14: French MT Bench Results Average of turn 1 and 2 of many supervised finetuned\nmodels\n44\nModels\nWri Ro Reas Math Cod Ext STEM Hum Avg\nCroissantLLMChat\n5.1\n3.6\n2.4\n1.1\n1.8\n1.3\n5\n5.85 3.27\nTinyLlamaChat\n6.25\n5\n3.3\n1.35\n2.1\n1.5\n4.82\n6.3\n3.83\nBloom 1B7 Chat\n3.95 4.55 1.95\n1.4\n1.45 1.35\n3.5\n3.45\n2.7\nCMArkea BloomZ 3B 3.15 3.3\n2.2\n1.1\n1.25 1.35\n3.15\n1.9\n2.17\nLlama 2 7B Chat\n8.9\n7.7\n4.25\n2.4\n3\n6.5\n8.65\n8.75 6.27\nVicuna 7B v1.3\n8.1 7.45 4.65\n2.3\n3.55\n5\n7.82\n9.1\n6\nLlama 2 13B Chat\n8.85 7.5\n5.1\n3.45\n3\n6.92\n8.62\n9.75 6.65\nVicuna 13B v1.3\n9.25 7.18 5.85\n2.6\n3.25 5.55\n7.98\n9.45 6.39\nVicuna 33B v1.3\n9.5 8.45 6.65\n3.15 3.35 7.1\n8.98\n9.8\n7.12\nLlama 2 70B Chat\n9.3\n7.5\n5.8\n3.3\n3.15 7.25\n8.93\n9.62 6.86\nGPT 3.5 Turbo\n9.2\n8.4\n5.65\n6.3\n6.9 8.85\n8.7\n9.55 7.94\nGPT 4\n9.65 8.9\n9\n6.8\n8.55 9.38\n9.7\n9.95 8.99\nTable 15: English MT Bench Results Average of turn 1 and 2 of many supervised finetuned\nmodels\nTask\nCrows(en) Crows(Fr) Avg\nmGPT(1.3B)\n3.16\n2.94 3.05\nBloom(3B)\n3.39\n3.02 3.21\nBloom(1.1B)\n3.36\n3.07 3.22\nCroissantLLM\n3.56\n3.22 3.39\nPythia(1.4b)\n3.36\n3.62 3.49\nOPT(1.3b)\n3.35\n3.67 3.51\nTinyLlama(1.1B)\n3.48\n3.76 3.62\nGPT-fr(1B)\n4.50\n2.97 3.73\nLlama2(7B)\n3.72\n3.81 3.76\nTable 16: Code Data mix\n45\n"
  },
  {
    "title": "Can Large Language Models Understand Context?",
    "link": "https://arxiv.org/pdf/2402.00858.pdf",
    "upvote": "20",
    "text": "Can Large Language Models Understand Context?\nYilun Zhu1\u2217, Joel Ruben Antony Moniz2, Shruti Bhargava2, Jiarui Lu2\nDhivya Piraviperumal2, Site Li2, Yuan Zhang2, Hong Yu2, Bo-Hsiang Tseng2\n1Department of Linguistics, Georgetown University\n2Apple\nyz565@georgetown.edu\n{joelrubenantony_moniz, shruti_bhargava, jiarui_lu, dhivyaprp}@apple.com\n{site_li, yzhang73, hong_yu, bohsiang_tseng}@apple.com\nAbstract\nUnderstanding context is key to understanding\nhuman language, an ability which Large Lan-\nguage Models (LLMs) have been increasingly\nseen to demonstrate to an impressive extent.\nHowever, though the evaluation of LLMs en-\ncompasses various domains within the realm\nof Natural Language Processing, limited atten-\ntion has been paid to probing their linguistic\ncapability of understanding contextual features.\nThis paper introduces a context understand-\ning benchmark by adapting existing datasets to\nsuit the evaluation of generative models. This\nbenchmark comprises of four distinct tasks and\nnine datasets, all featuring prompts designed to\nassess the models\u2019 ability to understand context.\nFirst, we evaluate the performance of LLMs un-\nder the in-context learning pretraining scenario.\nExperimental results indicate that pre-trained\ndense models struggle with understanding more\nnuanced contextual features when compared to\nstate-of-the-art fine-tuned models. Second, as\nLLM compression holds growing significance\nin both research and real-world applications,\nwe assess the context understanding of quan-\ntized models under in-context-learning settings.\nWe find that 3-bit post-training quantization\nleads to varying degrees of performance reduc-\ntion on our benchmark. We conduct an exten-\nsive analysis of these scenarios to substantiate\nour experimental results.1\n1\nIntroduction\nDiscourse understanding, as one of the fundamen-\ntal problems in NLP, focuses on modeling linguis-\ntic features and structures that go beyond indi-\nvidual sentences (Joty et al., 2019). Understand-\ning discourse requires resolving the relations be-\ntween words/phrases (coreference resolution) and\ndiscourse units (discourse parsing and discourse re-\nlation classification) in the previous context, iden-\n\u2217Work performed during an internship at Apple.\n1The code is publicly available at https://github.com/\napple/ml-llm-contextualization-eval.\ntifying carry-over information for the following\ncontext (dialogue state tracking), and recognizing\ndiscourse-specific phenomena (ellipsis).\nLLMs have garnered substantial attention from\nboth academia and the industry due to their remark-\nable capability in comprehending language and\nworld knowledge. Their unparalleled performance\nacross a diverse range of benchmarks and datasets\nhas firmly established their significance in a rel-\natively short period of time. As LLMs continue\nto push the boundaries of scale and capability, the\nevaluation of their multifaceted abilities becomes\nan equally vital endeavor. Consequently, the devel-\nopment of robust evaluation methodologies to as-\nsess specific aspects of LLMs becomes imperative.\nIn addition, these methodologies should focus on\nhelping achieve a comprehensive understanding of\ntheir advancement while clearly delineating their\nlimitations. However, recently published LLMs,\nsuch as OPT (Zhang et al., 2022), LLaMA (Tou-\nvron et al., 2023) and GPT-4 (OpenAI, 2023), are\nonly evaluated on limited benchmarks, and have a\nsignificant drawback: they neglect the inclusion of\ndiscourse-related datasets for evaluation, thereby\nlimiting the comprehensive assessment of their lan-\nguage understanding capabilities.\nTo provide a comprehensive evaluation, plenty\nof benchmarks and datasets address various\nfacets of language understanding, including bench-\nmarks that delve into common sense knowledge\n(Hendrycks et al., 2021a; Kwiatkowski et al., 2019),\nas well as linguistic capabilities like sentiment anal-\nysis, natural language inference, summarization,\ntext classification, and more (Bang et al., 2023b;\nLiang et al., 2022). These general benchmarks and\nspecific dataset evaluations exhibit certain limita-\ntions. Despite the requirement for contextual infor-\nmation in these benchmarks to effectively tackle\ntasks (for example, sentiment analysis requires an\nunderstanding of polarities within the given text),\nnone of these benchmarks cater to tasks that de-\narXiv:2402.00858v1  [cs.CL]  1 Feb 2024\nImplicit Discourse\nRelation Classification\nPDTB-3\nRelations between discourse units\nQuery Rewriting\nMuDoCo\nQReCC\nInCar\nGECOR\nCANARD\nEllipsis and reference\nDialogue State Tracking\nMultiWoz\nEntity carryover within context\nI am looking for a \ntrain that leaves on Thursday \ngoing to Cambridge.\ntrain-day: Thursday\ntrain-destination: Cambridge\n\u2026\n\u2026\nCoreference\nResolution\nWSC 273\nOntoNotes\nNominal & eventual reference\nDoc\nIn the summer of 2005, a picture that \npeople have long been looking forward \nto started emerging with frequency in \nvarious major Hong Kong media. With \ntheir unique charm, these well-known \ncartoon images once again caused \nHong Kong to be a focus of worldwide \nattention ...\nClusters\n{Hong Kong, Hong Kong}\n{their, these well - known \ncartoon images}\nArg1\nAn ancient stone church \nstands amid the fields,x\nRelation Label\nTemporal\nArg2\nthe sound of bells cascading \nfrom its tower, calling the \nfaithful to evensong.xx\nUser: Try to reach Forbes now.\nBot: Forbes at Washington Post? Or \nForbes of Publishing Division?\nUser: Publishing Division.\nRewrite\nForbes of Publishing \nDivision.\nFigure 1: Tasks and datasets in the context understanding benchmark.\nmand a nuanced comprehension of linguistic fea-\ntures within a provided context.\nOn the other hand, recent LLMs, by virtue of\npossessing billions of parameters, have led to an ex-\nponential surge in computational and storage costs\n(Brown et al., 2020b), which hinders the deploy-\nment of large models to personal devices and re-\nstricts the on-device performance of language un-\nderstanding tasks. To address this challenge, model\ncompression methods, which can reduce memory\nand disk requirements of both model training and\ninference, have gained attention. Existing compres-\nsion techniques, such as 3-bit quantization (Frantar\net al., 2022), have demonstrated the potential to re-\nduce model sizes with only marginal performance\ntrade-offs. However, the evaluation of quantiza-\ntion methods suffers from two deficiencies. Firstly,\nquantization methods are primarily evaluated on\nlimited benchmarks and datasets, such as Lambada\n(Paperno et al., 2016), ARC (Boratko et al., 2018),\nPIQA (Tata and Patel, 2003), BoolQ (Clark et al.,\n2019), and StoryCloze (Mostafazadeh et al., 2017).\nIt is not yet clear whether large, compressed mod-\nels out- or under-perform their smaller counterparts\nwhen understanding context. Secondly, previous\nwork has not delved into a linguistic analysis to\nidentify where the model efficacy wanes.\nGiven the above shortcomings, this paper evalu-\nates LLMs on a context understanding benchmark\nconstructed from varied discourse understanding\ndatasets. We conduct an extensive analysis of LLM\nperformance on this benchmark, including models\nof varying sizes and those subjected to compres-\nsion techniques, aiming to provide a more com-\nprehensive understanding of context understanding\ncapability of the LLMs. The contributions of this\npaper can be summarized as follows:\n\u2022 Our work introduces a contextual understand-\ning benchmark, including four tasks, for the\nevaluation of LLMs. We also present prompts\ndesigned for in-context learning on each task.\n\u2022 We evaluate LLMs of varying sizes from dif-\nferent model families and provide an analysis\non these models\u2019 capability for context under-\nstanding.\n\u2022 We evaluate post-training compressed models\nin ICL settings and conduct an analysis of the\nreduction in context understanding capability\ncompared to dense models.\n2\nRelated Work\n2.1\nIn-context Learning Evaluation\nThe paradigm of ICL (Brown et al., 2020a) is\nrapidly gaining importance. Studies have demon-\nstrated that the generalization of LLMs to var-\nious downstream NLP tasks, such as MMLU\n(Hendrycks et al., 2021b), is significantly enhanced\nwhen provided with a small number of examples\nas prompts (Brown et al., 2020a; Chowdhery et al.,\n2022; Hoffmann et al., 2022; Rae et al., 2022; Anil\net al., 2023; Touvron et al., 2023; OpenAI, 2022,\n2023). Recent research has extensively evaluated\nthe performance of LLMs across a spectrum of\nlanguage-related tasks, spanning from text genera-\ntion to understanding input sequences. This assess-\nment contains a wide array of benchmarks, includ-\ning SUPER-GLUE (Wang et al., 2019; Laskar et al.,\n2023), and tasks such as question answering, in-\nformation retrieval, sentiment analysis (Bang et al.,\n2023b; Liang et al., 2022), dialogue (Heck et al.,\n2023), and text classification (Yang and Menczer,\n2023).\n2.2\nModel Compression for LLMs\nModel compression techniques can be broadly cat-\negorized into three main approaches: compression\nduring training, compression associated with fine-\ntuning, and post-training methods. In terms of\nquantization during training, this technique enables\nLLMs to adapt to low-precision representations dur-\ning the training process (Liu et al., 2023). Model\ncompression with fine-tuning involves quantization\nawareness into the fine-tuning stage (Kim et al.,\n2023; Dettmers et al., 2023). Post-training tech-\nniques, on the other hand, are applied after the com-\npletion of an LLMs training phase and typically\ninvolve the use of calibration data. This category\ncomprises two primary approaches: pruning, which\nremoves redundant or non-salient weights to induce\nsparsity (Frantar and Alistarh, 2023), and quantiza-\ntion, which employs low-precision numeric repre-\nsentations of weights and activations (Nagel et al.,\n2020; Frantar et al., 2022; Yuan et al., 2023). Prior\nresearch shows that quantization outperforms prun-\ning in several settings (Kuzmin et al., 2023), thus\nin this work, we focus on model quantization and\nits impact on the selected context-aware tasks.\n3\nTask Selection & Design\nOur contextual understanding benchmark includes\nfour tasks with nine datasets, as presented in Figure\n1. In the following sections, we provide detailed\nexplanations of each task and the corresponding\ndatasets, along with the designed prompts for ICL\nevaluations.\n3.1\nCoreference Resolution\nThe coreference resolution (CR) task contributes to\nachieving a coherent understanding of the overall\nmeaning conveyed within the text. Thus, it plays a\ncritical role in diving into language models\u2019 capa-\nbility to grasp coreference relations as well as con-\ntextual nuances within documents. We select two\ncoreference datasets: WSC273 (Levesque et al.,\n2012) and OntoNotes 5.0 (Pradhan et al., 2013).\nWSC273, which contains the first 273 examples\nfrom the Winograd Schema Challenge, is a dataset\nthat requires the system to read a sentence with\nInstruction: Please carefully read the following passages.\nFor each passage and the options, you must identify which\noption the mention marked in *bold* refers to. If the\nmarked mention does not have any antecedent, please se-\nlect \u201cno antecedent\u201d.\nContext: ... To express *its* determination ... the Chinese\nsecurities regulatory department ... this stock reform ...\nChoices:\nA. no antecedent\nB. the Chinese securities regulatory department\nC. this stock reform\n...\nQuestion: What does *its* refer to?\nAnswer: B\nTable 1: An OntoNotes example of prompt and answer.\nan ambiguous pronoun and select the referent of\nthat pronoun from two choices. OntoNotes is a\nhuman-annotated corpus of documents annotated\nwith multiple layers of linguistic information in-\ncluding syntax, propositions, named entities, word\nsense, and in-document coreference. As it is one\nof the most frequently used datasets for training\ncoreference models, prior research has achieved\nsignificant advancements under the supervised fine-\ntuning paradigm (Lee et al., 2017; Joshi et al., 2020;\nBohnet et al., 2023). However, these model designs\ncannot be extended to generative models under ICL\nsettings. Recently, Le and Ritter (2023) have lever-\naged document templates for LLMs; however, their\nevaluation is confined to prominent models such as\nInstructGPT (Ouyang et al., 2022), neglecting the\nfact that smaller models lack the generative capac-\nity required to accomplish such tasks. Due to these\nlimitations, we propose a novel multiple-choice\ntask design. In this design, we provide the men-\ntions and evaluate the model on resolution. Each\noption represents a potentially markable span.2 Ta-\nble 1 presents an example of the input to the model3.\nThe entire prompt consists of five parts: (1) an in-\nstruction that provides guidance to the model for\nthe task, (2) a document containing plain text with\na selected mention span highlighted using a bold\nsymbol, (3) a list of choices, which includes all\nthe gold mentions present in the document, (4) a\nquestion that directs the model\u2019s attention, and (5)\na guiding word answer that prompts for the out-\nput. We experiment with multiple instructions and\nprompts and provide the one with the best perfor-\nmance. Linking scores are computed for each ques-\n2Considering the inferior performance of small models on\nthe mention detection task, we utilize gold markable spans\ncoreference linking.\n3Detailed examples for each task design can be found in\nAppendix A.\nOntology:\n{\u201cslots\u201d: {\u201crestaurant-pricerange\u201d: \u201cprice budget for the\nrestaurant\u201d, ... },\n\u201ccategorical\u201d: {\u201crestaurant-pricerange\u201d: [\u2018cheap\u2019, \u2018expen-\nsive\u2019, \u2018moderate\u2019], ...} }\nInstruction: Now consider the following dialogue be-\ntween two parties called the \u201csystem\u201d and \u201cuser\u201d. Can you\ntell me which of the \u201cslot\u201d was updated by the \u201cuser\u201d in\nits latest response to the \u201csystem\u201d? Present the updates in\nJSON format. If no \u201cslots\u201d were updates, return an empty\nJSON list. If you encounter \u201cslot\u201d that was requested by\nthe \u201cuser\u201d then fill them with \u201c?\u201d. If a user does not seem\nto care about a discussed \u201cslot\u201d fill it with \u201cdontcare\u201d.\n[Previous Dialogue State]\n[Conversation]:\n\u201csystem\u201d: \u201c\u201d\n\u201cuser\u201d: \u201cI\u2019m looking for a moderately priced place to eat\nthat\u2019s in the centre of town.\u201d\nOutput:\n{\u201crestaurant-pricerange\u201d:\n\u201cmoderate\u201d,\n\u201crestaurant-area\u201d: \u201ccentre\u201d}\nTable 2: A DST example of prompt and answer.\ntion and the results are subsequently aggregated for\nevaluation. We utilize the official evaluation met-\nrics from the CoNLL-2012 shared task (Pradhan\net al., 2012), which employs the CoNLL F1 score,\nderived from the averaging of three coreference\nmetrics: MUC, B3, and CEAF\u03d54.\n3.2\nDialogue State Tracking\nDialogue state tracking (DST) is an important task\nin the area of task-oriented dialogue (TOD) model-\ning (Young et al., 2013), where the dialogue agent\ntracks the key information provided by the user as\nthe conversation progresses. Table 2 provides an\nexample from MultiWOZ (Budzianowski et al.,\n2018) where the user expresses the constraints\nwhen looking for a restaurant. The output of DST\nis typically maintained in slot-value pair format.\nPrevious research has explored ICL capabilities\non MultiWOZ and demonstrated promising results\ncompared to fine-tuning models (Hu et al., 2022;\nHeck et al., 2023). However, these studies either\ninvolve partial training or are untested with smaller\nand quantized models. Here we adopt a straight-\nforward and simplified ICL approach proposed by\nHeck et al. (2023), and test it on MultiWOZ v2.2\n(Zang et al., 2020). The prompt to the model con-\nsists of domain knowledge from ontology, an in-\nstruction, previous dialogue state (the belief state\naccumulated until the previous user turn) and the\nconversation proceeding to the current turn. The\nontology could be lengthy if considering all do-\nmains in the dataset. Thus, given the input length\nconstraint of LLMs, only the knowledge relevant to\nthe conversation is provided. Following literature,\nInstruction: Given two arguments and a list of connective\nwords, please select the most likely connective between\ntwo arguments.\n[Relation Description]\nInput:\nArg 1: Amcore, also a bank holding company, has assets\nof $1.06 billion.\nArg 2: Central\u2019s assets are $240 million.\nQuestion: What is the connective that best describes the\nrelation between two arguments?\nChoices:\nA. Temporal B. Contingency C. Comparison D. Expansion\nAnswer: C\nTable 3: A PDTB example of prompt and answer.\nwe report joint goal accuracy (JGA) (Mrk\u0161i\u00b4c et al.,\n2017) for evaluating the performance of DST.\n3.3\nImplicit Discourse Relation Classification\nDiscourse demonstrates its importance beyond in-\ndividual sentences, which emphasizes the ways\nin which different segments of a text interconnect\nand structure themselves to convey a coherent and\nmeaningful message. The PDTB-3 corpus, as intro-\nduced by Webber et al. (2019), annotates implicit\ndiscourse relations across elementary discourse\nunits (EDUs)4. These relations imply connections\nbetween EDUs and may be made explicit by in-\nserting a connective. Within the context of the\nunderstanding benchmark, we opt for the implicit\ndiscourse relation classification task for two pri-\nmary reasons. Firstly, the order of the two EDUs is\nprovided, enabling the model to directly utilize this\ninformation. Secondly, the connective triggering\nthe relation is implicit, increasing the task\u2019s com-\nplexity. In this task, two EDUs are fed as input,\nand the objective is to correctly identify the rela-\ntion between them. Due to the nuanced differences\nbetween each relation and the demand for annota-\ntors with rich linguistic knowledge and extensive\nannotation training, the classification task poses\nchallenges to fine-tuned classification models.\nThe PDTB3 corpus classifies discourse relations\ninto four categories - Temporal, Contingency,\nComparison, and Expansion. We convert this task\ninto a multiple-choice question and experiment\nwith classes as options. In the classes scenario, the\ntask offers four options, each representing a distinct\ndiscourse relation class. Table 3 exhibits the com-\nponents of the prompt. It includes an instruction\nat the beginning, followed by a concise description\nof each relation, a context with two arguments, a\n4EDU refers to the smallest segment of a text that conveys\na complete and coherent meaning within larger discourse.\nInstruction: Rewrite the last query following interaction\ninto a well-formed, context independent query. Resolve\nany disfluencies or grammatical errors in the query.\nInput:\nUser: Try to reach Forbes now .\nBot: Forbes at Washington Post ? Or Forbes of Publishing\nDivision ?\nUser: Publishing Division .\nRewrite: Forbes of Publishing Division\nTable 4: A query rewriting example of prompt and an-\nswer.\nquestion along with answer choices, and a trigger\nword. We evaluate each model\u2019s performance on\nthis dataset using accuracy as the metric.\n3.4\nQuery Rewriting\nWhile document-based CR (OntoNotes, Section\n3.1) covers various types of coreference relations\nacross multiple genres, it does not allow the ability\nto evaluate certain aspects which are important to\nunderstand context. Firstly, the CR task typically\nfocuses on document-based coreference chains, ne-\nglecting mention resolution in dialogues. Secondly,\nellipsis, which is the omission of one or more words\nfrom a clause, is a crucial linguistic phenomenon\nin speech and conversation. It is essential for lan-\nguage models to grasp and accurately identify el-\nlipses within context. Incorporating these features\ninto the benchmark is thus pivotal when evaluating\ncontext understanding.\nQuery Rewriting (QR) is a task of rewriting the\nlast utterance of a user in a conversation into a\ncontext-free, independent utterance that can be in-\nterpreted without dialog context. It requires the\nmodel to identify the entity or events references\nfrom context and further generate a complete utter-\nance with resolved coreference or ellipsis.\nWe incorporate five QR datasets in the proposed\nbenchmark: MuDoCo with QR annotations (Martin\net al., 2020; Tseng et al., 2021), QReCC (Anantha\net al., 2021), InCar (Regan et al., 2019), GECOR\n(Quan et al., 2019), and CANARD (Elgohary et al.,\n2019). These datasets span multiple genres and\ndomains in dialogues. We experiment with various\nprompts used for fine-tuning models and present\nthe results with the best selections. Table 4 presents\na concise prompt comprising an instruction along\nwith context for each dialogue. To assess the qual-\nity of generated queries, we follow the metrics from\nprevious research, particularly BLEU (Papineni\net al., 2002) and ROUGE (Lin, 2004).\n4\nExperiments\n4.1\nImplementation Details\nEvaluation was conducted on a computational in-\nfrastructure comprising 8 \u00d7 A100 GPUs. We ex-\nperiment with three model families. For smaller\nmodels, we consider OPT (Zhang et al., 2022),\nranging from 125M to 2.7B. Although OPT also\noffers larger models, we opt for LLaMA (Touvron\net al., 2023) as the mid-sized LMs, spanning from\n7B to 65B parameters, due to showcased superior\nperformance by prior works. For large-scale LMs,\nwe leverage GPT-3.5-turbo5. For each model,\non every dataset, we assess five different settings:\nzero-shot, one-shot, 5-shot, 8-shot, and 10-shot.\nWe randomly select the examples from the training\nset for the few-shot prompting.6\n4.2\nDense Model\nResults of the three model families are reported\nin Table 5, along with results of fine-tuned (FT)\nmodels to help better interpret how well the pre-\ntrained models behave with ICL. Figure 2 also vi-\nsualizes the gap between various commercial/non-\ncommercial language models and fine-tuning mod-\nels that achieve the best performance on these tasks.\nFor each, we present the N-shot setting that yields\nthe highest score (see Appendix B for details).\nOverall, performance improves as the model size\nincreases and pre-trained models with ICL struggle\nto catch up with FT models on most tasks.\nCoreference Resolution\nLarger models exhibit\npromising performance on the WSC273 task, indi-\ncating that LLMs can effectively handle \"simple\"\ncoreference relations within limited contexts and\nmentions. However, when it comes to document-\nbased CR with complex clusters, their performance\nsubstantially drops 7. Even on providing ground-\ntruth mentions, the highest-performing GPT is only\non par with rule-based coreference systems (Man-\nning et al., 2014) and is far from the end-to-end\nfine-tuned SpanBERT (Joshi et al., 2020). The gap\n5https://platform.openai.com/docs/models/\ngpt-3-5\n6WSC273 itself is a test set and thus has no fine-tuning\nresults. We only report the zero-shot results.\n7Note that the OntoNotes dataset is substantially larger\nthan the others. We observe that inference on the entire test\nset becomes extremely time-consuming, particularly with the\nlarger models; further, the cost of running inference on GPT-\n3.5 starts becoming non-negligible. Consequently, we propose\nlimiting the OntoNotes test set to a 10% sub-sample, which is\nthe setting we consistently adopt.\nTask\nDataset\nMetrics\nOPT\nLLaMA\nGPT\nFT\n125M\n350M\n1.3B\n2.7B\n7B\n13B\n30B\n3.5-turbo\nCR\nWSC273\nAcc\n58.24\n66.67\n76.19\n77.66\n86.81\n89.38\n89.01\n88.64\nN/A\nOntoNotes\nMUC\n12.66\n7.58\n13.21\n8.29\n10.31\n31.80\n33.56\n56.32\n77.26\nB3\n53.80\n52.26\n53.54\n52.41\n52.20\n58.43\n58.66\n68.20\n73.43\nCEAF\u03d54\n31.09\n29.49\n31.40\n30.10\n32.63\n38.00\n39.27\n50.72\n74.46\nAvg. F1\n32.52\n29.78\n32.72\n30.27\n31.71\n42.74\n43.83\n58.41\n76.03\nDST\nMultiWOZ\nJGA\n11.11\n27.96\n26.61\n28.08\n32.30\n28.12\n42.24\n57.40\n63.79\nDisc.\nPDTB-3\nAcc\n10.04\n10.04\n10.04\n16.15\n17.16\n26.01\n39.77\n43.83\n76.23\nQR\nMuDoCo\nBLEU\n0.46\n0.36\n7.02\n49.20\n41.12\n61.15\n66.51\n57.14\n80.31\nROUGE\n1.52\n12.18\n10.98\n65.61\n56.07\n74.78\n77.88\n79.37\n92.01\nQReCC\nBLEU\n4.53\n31.27\n26.35\n40.09\n28.19\n38.64\n58.68\n55.24\n58.67\nROUGE\n13.91\n58.18\n53.10\n68.32\n48.27\n56.40\n78.74\n79.98\n81.75\nInCar\nBLEU\n0.00\n7.66\n12.71\n27.42\n28.20\n42.13\n48.58\n63.66\n88.45\nROUGE\n3.41\n28.76\n30.45\n49.63\n49.96\n56.73\n64.18\n83.51\n95.24\nGECOR\nBLEU\n0.20\n26.40\n26.32\n49.99\n53.27\n66.30\n73.80\n63.34\n82.56\nROUGE\n4.06\n42.13\n42.57\n65.89\n69.23\n80.99\n86.03\n79.00\n92.63\nCANARD\nBLEU\n2.61\n19.39\n24.24\n34.66\n21.34\n29.32\n47.24\n47.12\n57.46\nROUGE\n9.82\n45.63\n49.36\n62.73\n38.17\n46.61\n69.73\n74.61\n81.06\nTable 5: Few-shot results of two open-sourced models and GPT-3.5 on the context understanding benchmark. The\nresults with the best number of few-shot examples are reported for each task. Fine-tuning (FT) results serves as a\nreference when evaluating LLMs\u2019 capability under ICL setup.\nbetween ICL and FT results highlights that under\nthe ICL setting, LLMs struggle to build coreference\nchains without adequate domain-specific examples.\nSpecifically, models except GPT perform signifi-\ncantly worse on the MUC metric. Error analysis re-\nveals that these models are inclined to create more\nclusters, including singleton clusters. This implies\nthat pre-trained LLMs encounter difficulties in un-\nderstanding long-range contextual information.\nDST\nA similar trend is observed as CR where\nOPT and LLaMA models fall behind GPT-3.5 sig-\nnificantly. This suggests that these models fail to\nextract key information as the conversation pro-\nceeds, even with the provision of 5 to 10 demon-\nstrations and the distilled relevant domain ontology\nin prompt. Our error analysis indicates that most of\nthe errors happen due to the misdetection of slots\nor the wrong predicted value in a slot-value pair.\nOnly GPT-3.5 reaches the level of FT results which\nis a fine-tuned T5 base model (Bang et al., 2023a).\nImplicit Discourse Relation Classification\nWe\nobserve an increase in scores when the model size\nexceeds 7B. However, even the best-performing\nLLM, GPT, performs worse than the SOTA fine-\ntuned model (Liu and Strube, 2023) with the drop\nof 32% accuracy. We carefully examine the pre-\ndictions for each model and found that all models\ntend to predict the same relation class for every\nexample, albeit with their individual preferences\nfor the selected relation. In addition, because of\nan imbalanced distribution of classes, these mod-\nels potentially perform worse than random chance\n(25%). This suggests that the models struggle to\ndistinguish the nuances between different relation\nclasses and fail to correctly identify relations across\nEDUs within context.\nQuery Rewriting\nThe gap between small and\nlarge models is significantly huge, compared to the\nother tasks. For instance, OPT-125M cannot even\ncomplete the rewriting task. Analysis on predic-\ntions of small models indicates that the model is not\ncapable of following the instructions or learning\npatterns from the few-shot examples. We identify a\nfew major error types: (1) generating the next sen-\ntence, instead of rewriting; (2) rewriting the wrong\nuser turn from the conversation; (3) copying the last\nuser utterance without any rewriting. These errors\nget reduced as the model size increases. However,\nsimilar to the previous three tasks, the best ICL\nresults achieved by GPT is far from the fine-tuned\nmodels.8 It is worth noting that OPT-2.7B performs\non par or notably better than LLaMA-7B, which is\nsomewhat not aligned with the findings in Beeching\net al. (2023) where LLaMA-7B even outperforms\nOPT-66B in many tasks, including ARC (Clark\n8In literature, the best FT results come from different mod-\nels across five QR datasets, where some are not even LLM\nbased. To ensure fair comparison, we fine-tuned a T5 large\nmodel on each QR dataset.\n0.00\n10.00\n20.00\n30.00\n40.00\n50.00\n60.00\n70.00\n80.00\n90.00\nQR\nDisc.\nDST\nCR\nOPT-2.7B\nLLaMA-30B\nGPT-3.5\nFine-tuning\nFigure 2:\nComparison between commercial/non-\ncommercial models and fine-tuning models for each\ntask in the context understanding benchmark.\net al., 2018), HellaSwag (Zellers et al., 2019), and\nMMLU (Hendrycks et al., 2021b).\nAll in all, this section presents a holistic compar-\nison of LLMs\u2019 behaviors on the target context un-\nderstanding tasks. On the tasks with structured out-\nputs such as CR or DST, even small models show\na certain level of context understanding and seem\nto follow the task instruction. Classification tasks\nsuch as discourse relation selection are deemed the\neasiest among all tasks; however, the small mod-\nels are even worse than a random guess (25%).\nAs for the generative task, the ability to complete\nquery rewriting can be only observed in the case\nof larger models, as the model has the freedom\nto generate arbitrary content that does not follow\nthe prompt. We notice that OPT-2.7B outperforms\nLLaMA-7B in multiple QR datasets, including Mu-\nDoCo, QReCC, and CANARD. We carefully com-\npare the outputs between the two models. As an ex-\nample, QReCC, a QA-based conversational dataset,\nconsists of several QA pairs as context and a last\nquery to be rewritten. We observe that LLaMA-\n7B tends to rewrite the question in context instead\nof rewriting the last target query, which is not fre-\nquent in OPT-2.7B. It is also noted that except for\nDST, FT models demonstrate marked superiority\nover pre-trained models, highlighting the potential\nfor improving LLMs\u2019 competence on these context\nunderstanding tasks.\nDataset\nMetrics\n7B-D\n30B-Q\n30B-D\nWSC273\nAcc\n86.81\n87.18\n89.01\nOntoNotes\nMUC\n10.31\n25.37\n33.56\nB3\n52.20\n56.80\n58.66\nCEAF\u03d54\n32.63\n36.93\n39.27\nAvg. F1\n31.71\n39.70\n43.83\nMultiWOZ\nJGA\n32.30\n41.99\n42.24\nPDTB-3\nAcc\n17.16\n31.29\n39.77\nMuDoCo\nBLEU\n41.12\n59.22\n66.51\nROUGE\n56.07\n71.38\n77.88\nQReCC\nBLEU\n28.19\n53.72\n58.68\nROUGE\n48.27\n74.13\n78.74\nInCar\nBLEU\n28.20\n39.69\n48.58\nROUGE\n49.96\n56.32\n64.18\nGECOR\nBLEU\n53.27\n70.41\n83.36\nROUGE\n69.23\n73.80\n86.03\nCANARD\nBLEU\n21.34\n45.07\n47.24\nROUGE\n38.17\n67.15\n69.73\nTable 6: Comparison between dense and quantized mod-\nels. Dense LLaMA-7B and 3-bit quantized LLaMA-\n30B share similar memory and disk requirements. D\nrepresents dense model and Q denotes quantized model.\n4.3\nModel Compression Technique\nAs we focus on evaluating context understanding\nof LLMs in an ICL setup, we evaluate models quan-\ntized using GPTQ (Frantar et al., 2022), which is\nan efficient one-shot weight quantization algorithm\nbased on approximate second-order information\nthat compresses the model post-training. It enables\na reduction in memory and disk requirements by\nup to 80%, compared to the pre-quantized model.\n4.4\nQuantized Model Results\nGPTQ (Frantar et al., 2022) has been shown to\neffectively reduce the model size to 3 bits with-\nout incurring substantial performance losses across\na range of NLP tasks, such as MMLU, ARC,\nStoryCloze. However, whether this performance\npreservation can be extended to contextual under-\nstanding was unclear.\nTable 6 presents the comparison between the\ndense and 3-bit quantized LLaMA models.\nIn\ncontrast to previous studies on 3-bit quantization,\nwe observed that quantization leads to fluctuated\ndrops in performance across the four tasks. Specif-\nically, in WSC273, MultiWOZ, and CANARD,\npost-training quantization incurs only a marginal\nperformance drop (\u223c1.7 points). However, in the\nremaining datasets, quantization results in signifi-\ncant performance drops.\nThe results further show that the quantized\nLLaMA-30B model consistently outperforms the\ndense LLaMA-7B model across all tasks despite be-\ning comparable in disk and memory requirements.\nFor CR, the 30B quantized model achieves sig-\nnificantly higher scores on the OntoNotes dataset\nacross all metrics. The MUC metric shows the\nmost substantial improvement, indicating that the\nquantized 30B model partially overcomes the ten-\ndency to create small clusters for mentions. For\nDST on MultiWOZ, the 30B quantized model show\na 30% relative improvement over the 7B model in\nJGA. On discourse parsing with PDTB-3, the ac-\ncuracy of quantized 30B model is almost double,\n17.16% vs 31.29%. Across all QR datasets, the\nquantized 30B model substantially improves NLG\nscores compared to the dense 7B model, with rela-\ntive gains ranging from 15-50%. The largest gap is\nobserved on GECOR.\nIn general, we show that the quantized 30B\nLLaMA model consistently and significantly out-\nperforms the dense 7B model as a result of the\nincreased scale, despite using 3-bit quantization.\nThe benefits of greater model scale thus outweigh\nthe impacts of quantization in understanding dis-\ncourse. We believe this finding would be beneficial\nwhen deploying LLMs in real-world applications\nwith disk and runtime constraints.\n5\nCase Study: Query Rewriting\nIn this section, we provide in-depth analysis by\ncomparing the two open-sourced model families\nOPT and LLaMA, and the impact of quantization,\nusing query rewriting as the target task.\nWe conduct a careful inspection of the query\nrewriting task because of three reasons: (1) by the\nnature of the task, query rewriting is the only one\nwith free-form generation, while the others effec-\ntively are either classification-based tasks or heav-\nily constrained in their possible output predictions.\nThe generation task allows us to explore the LLMs\u2019\noutput in more detail, and to provide more interest-\ning insights; (2) the manual analysis of errors is a\ntime-consuming process, making it challenging to\nconduct such an in-depth analysis across all four\ntasks; (3) the query rewriting task covers a diverse\nrange of five datasets, enabling us to compare dif-\nferences between each dataset and to thereby gain\na deeper understanding.\n5.1\nOPT vs. LLaMA\nPrior works (Beeching et al., 2023) have consis-\ntently shown that, under the same model size,\nDataset\n6.7/7B\n13B\n30B\nO.\nL.\nO.\nL.\nO.\nL.\nMudoco\n53.1\n41.1\n55.2\n61.1\n55.2\n66.5\n71.8\n56.0\n72.1\n74.7\n71.5\n77.8\nQReCC\n46.6\n28.1\n43.7\n38.6\n43.8\n58.6\n73.4\n48.2\n71.6\n56.4\n71.9\n78.7\nInCar\n40.3\n28.2\n41.9\n42.1\n44.6\n48.5\n64.8\n49.9\n62.6\n56.7\n65.3\n64.1\nGECOR\n58.8\n53.2\n60.9\n66.3\n58.2\n73.8\n75.7\n69.2\n78.3\n80.9\n76.1\n86.0\nCANARD\n43.8\n21.3\n37.5\n29.3\n41.3\n47.2\n72.0\n38.1\n66.0\n46.6\n69.3\n69.7\nTable 7: Comparison between OPT (O.) and LLaMA\n(L.) across five query rewrite datasets. For each dataset,\nthe first and second rows represent BLEU and ROUGE\nscores respectively.\nContext\nUser: what is the name of india pakistan border line\nBot: The Radcliffe Line was the boundary demarcation\nline between the Indian and Pakistani portions of the\nPunjab and Bengal provinces of British India.\nUser: who created the radcliffe line\nBot: The Radcliffe Line was named after its architect, Sir\nCyril Radcliffe, who was the joint chairman of the two\nboundary commissions for the two provinces.\nUser: when was the line published\nGold answer: when was the radcliffe line published\nPrediction 1 (repeat the last query): when was the line\npublished\nPrediction 2 (language modeling): 1947\nTable 8: An example of two major types of errors found\nin the query rewriting task.\nLLaMA outperforms OPT. However, their perfor-\nmance on QR, as shown in Table 7, does not follow\nthis pattern.\nWhen the model size is around 7B, OPT consis-\ntently performs better than LLaMA by a significant\nmargin across the five QR datasets. The two mod-\nels perform on par with each other at 13B. The\nsuperiority of LLaMA is only obvious with 30B\nmodel size. From another perspective, although we\nexpect performance to improve as model size in-\ncreases, we observe this trend on LLaMA, but not\non OPT. These results suggest that it may not be\ncorrect to conclude the overall superiority between\ntwo model families by only comparing on a certain\nrange of model sizes or on a certain set of tasks.\n5.2\nDense vs. Quantized\nWe conduct a quantitative analysis on the error\ntypes of query rewriting to investigate the perfor-\nmance gap between dense and quantized models.\nType\nDataset\n7B D\n30B Q\n30B D\nRepeat\nMuDoCo\n260\n247\n194\nQReCC\n86\n90\n26\nInCar\n17\n15\n8\nGECOR\n59\n62\n37\nCANARD\n47\n44\n32\nTotal\n469\n458\n297\nLM\nMuDoCo\n71\n29\n16\nQReCC\n80\n28\n16\nInCar\n19\n20\n15\nGECOR\n6\n1\n0\nCANARD\n127\n76\n59\nTotal\n232\n125\n106\nTable 9: Number of the major two types errors on three\nLLaMA models (7B dense, 30B quantized, and 30B\ndense) found in query rewriting. Repeat stands for\nrepeat-the-last-query error and LM denotes language\nmodeling error.\nAcross the five datasets, we identify two main er-\nror types that account for nearly 80% of the to-\ntal errors, with examples shown in Table 8. First,\nthe model repeats the last query without resolv-\ning any referred entity or ellipsis. In this case, the\nmodel seems to understand the instruction but fails\nat rewriting. This type of error can be primarily\nassociated with the model\u2019s context understanding\ncapability. Second, the model treats the task as a\nlanguage modeling (LM) task, where it provides\na response to the last query. In this scenario, the\nmodel appears to struggle to understand the task\ninstruction, even with several few-shot examples.\nWe classify this error type as more related to the\nmodel\u2019s ICL ability.\nWe perform manual error annotations on the five\nQR datasets9. Table 9 illustrates the number of er-\nrors of the three selected models on each dataset. A\nconsistent trend is observed across all QR datasets.\nIn terms of repeat errors, the 30B dense model ex-\nhibits significantly fewer errors compared to the 7B\ndense model (297 vs. 469). However, 3-bit GPTQ\nquantization leads to an increase in this type of er-\nror, reaching a similar error count to the 7B dense\nmodel (458 vs. 469). This implies that 3-bit quan-\ntization reduces the model\u2019s ability to comprehend\nthe context. Regarding LM errors, the 30B dense\nmodel also significantly outperforms the 7B dense\nmodel, with 106 errors compared to 232. It is to be\nnoted that the quantized model generates only 125\nLM errors, slightly more than the 30B dense model.\nHowever, it generates significantly fewer (around\n910% test data on QReCC and CANARD was graded.\n50%) errors compared to the 7B dense model (125\nvs. 232). This indicates that 3-bit quantization\nmaintains the ICL capability that allows models\nto rewrite the user query successfully rather than\nperforming language modeling task.\n6\nConclusion\nThis paper introduces a contextual understanding\nbenchmark designed to assess the performance of\nLLMs. We collect nine existing datasets spanning\nfour tasks, each carefully tailored to suit generative\nmodels. This benchmark encompasses essential\nelements for assessing linguistic comprehension\nwithin context, including both document and dia-\nlog based contextual understanding. Experimental\nresults reveal that LLMs under in-context learning\nstruggle with nuanced linguistic features within\nthis challenging benchmark, exhibiting inconsisten-\ncies with other benchmarks that emphasize other\naspects of language. To the best of our knowledge,\nwe are also the first to compare dense models and\npost-training quantization models in contextual un-\nderstanding tasks. This comparison highlights that\n3-bit post-training quantization reduces the general\nunderstanding capacity of context to different ex-\ntent across the 4 tasks. The proposed contextual\ncomprehension benchmark thus provides a unique\nperspective on the contextual dimension of lan-\nguage understanding and offers a valuable addition\nto existing LLM evaluations.\nLimitations\nThis work provides an evaluation of various pre-\ntrained LLMs, including OPT, LLaMA, and GPT,\non our understanding benchmark. However, we\nhave not evaluated other LLMs designed for longer\ninput scenarios, such as LongLLaMA (Tworkowski\net al., 2023).\nIn our evaluation, we focus on the GPTQ quan-\ntization method, analyzing its performance on our\nbenchmark. We do not include other post-training\nquantization techniques, such as RPTQ (Yuan et al.,\n2023), in this work.\nOur evaluation concentrates on English datasets,\nprimarily utilizing LLMs pre-trained with English\ndata. All of the four tasks on our benchmark have\ndatasets from other languages. The coreference\ndataset OntoNotes 5.0 contains annotations of Ara-\nbic and Chinese. In addition, recent releases such\nas CorefUD (Nedoluzhko et al., 2022) promote\nstandardization of multilingual coreference anno-\ntations. In DST, CrossWOZ (Zhu et al., 2020) is\na cross-domain wizard-of-oz task-oriented dataset.\nLong et al. (2020) develop TED-CDB, a Chinese\ndiscourse relation dataset. The query rewriting\ntask also has datasets in other languages, such as\nREWRITE (Su et al., 2019) and Restoration-200K\n(Pan et al., 2019). Finally, specific LLMs opti-\nmized for individual languages, such as ChatGLM\n(Du et al., 2022), exist and are not a part of our\nevaluation.\nAcknowledgements\nThe authors would like to thank Jeffrey Nichols,\nRuss Webb and the anonymous reviewers for their\nhelp and feedback.\nReferences\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen Pulman, and Srinivas\nChappidi. 2021. Open-domain question answering\ngoes conversational via question rewriting. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n520\u2013534, Online. Association for Computational Lin-\nguistics.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nNamo Bang, Jeehyun Lee, and Myoung-Wan Koo.\n2023a. Task-optimized adapters for an end-to-end\ntask-oriented dialogue system. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 7355\u20137369, Toronto, Canada. Association for\nComputational Linguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan\nXu, and Pascale Fung. 2023b. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nEdward Beeching, Cl\u00e9mentine Fourrier, Nathan Habib,\nSheon Han, Nathan Lambert, Nazneen Rajani, Omar\nSanseviero, Lewis Tunstall, and Thomas Wolf. 2023.\nOpen llm leaderboard. https://huggingface.co/\nspaces/HuggingFaceH4/open_llm_leaderboard.\nBernd Bohnet, Chris Alberti, and Michael Collins. 2023.\nCoreference resolution through a seq2seq transition-\nbased system. Transactions of the Association for\nComputational Linguistics, 11:212\u2013226.\nMichael Boratko, Harshit Padigela, Divyendra Mikki-\nlineni, Pritish Yuvraj, Rajarshi Das, Andrew McCal-\nlum, Maria Chang, Achille Fokoue-Nkoutche, Pavan\nKapanipathi, Nicholas Mattei, Ryan Musa, Kartik\nTalamadupula, and Michael Witbrock. 2018. A sys-\ntematic classification of knowledge, reasoning, and\ncontext within the ARC dataset. In Proceedings of\nthe Workshop on Machine Reading for Question An-\nswering, pages 60\u201370, Melbourne, Australia. Associ-\nation for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020b. Language models are few-shot learners.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Ga\u0161i\u00b4c. 2018. MultiWOZ - a large-\nscale multi-domain Wizard-of-Oz dataset for task-\noriented dialogue modelling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5016\u20135026, Brussels,\nBelgium. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924\u20132936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320\u2013335.\nAhmed Elgohary, Denis Peskov, and Jordan Boyd-\nGraber. 2019. Can you unpack that? learning to\nrewrite questions-in-context. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5918\u20135924, Hong Kong,\nChina. Association for Computational Linguistics.\nElias Frantar and Dan Alistarh. 2023. SparseGPT: Mas-\nsive language models can be accurately pruned in\none-shot. arXiv preprint arXiv:2301.00774.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2022. GPTQ: Accurate post-training\ncompression for generative pretrained transformers.\narXiv preprint arXiv:2210.17323.\nMichael Heck, Nurul Lubis, Benjamin Ruppik, Renato\nVukovic, Shutong Feng, Christian Geishauser, Hsien-\nchin Lin, Carel van Niekerk, and Milica Gasic. 2023.\nChatGPT for zero-shot dialogue state tracking: A\nsolution or an opportunity? In Proceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 2: Short Papers), pages\n936\u2013950, Toronto, Canada. Association for Compu-\ntational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021a. Measuring massive multitask language\nunderstanding. Proceedings of the International Con-\nference on Learning Representations (ICLR).\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021b. Measuring massive multitask language un-\nderstanding.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Si-\nmonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals,\nand Laurent Sifre. 2022. Training compute-optimal\nlarge language models.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022.\nIn-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022, pages 2627\u20132643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020.\nSpan-\nBERT: Improving pre-training by representing and\npredicting spans. Transactions of the Association for\nComputational Linguistics, 8:64\u201377.\nShafiq Joty, Giuseppe Carenini, Raymond Ng, and\nGabriel Murray. 2019. Discourse analysis and its\napplications.\nIn Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics: Tutorial Abstracts, pages 12\u201317, Florence,\nItaly. Association for Computational Linguistics.\nJeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joon-\nsuk Park, Kang Min Yoo, Se Jung Kwon, and Dong-\nsoo Lee. 2023. Memory-efficient fine-tuning of com-\npressed large language models via sub-4-bit integer\nquantization.\nAndrey Kuzmin, Markus Nagel, Mart van Baalen, Arash\nBehboodi, and Tijmen Blankevoort. 2023. Pruning\nvs quantization: Which is better?\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452\u2013466.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Huang. 2023. A systematic study and\ncomprehensive evaluation of ChatGPT on benchmark\ndatasets. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, pages 431\u2013469,\nToronto, Canada. Association for Computational Lin-\nguistics.\nNghia T. Le and Alan Ritter. 2023. Are large language\nmodels robust zero-shot coreference resolvers?\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 188\u2013197, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In 13th\nInternational Conference on the Principles of Knowl-\nedge Representation and Reasoning, KR 2012, Pro-\nceedings of the International Conference on Knowl-\nedge Representation and Reasoning, pages 552\u2013561.\nInstitute of Electrical and Electronics Engineers Inc.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R\u00e9, Diana Acosta-Navas, Drew A.\nHudson, Eric Zelikman, Esin Durmus, Faisal Lad-\nhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue\nWang, Keshav Santhanam, Laurel Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan Kim,\nNeel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nWei Liu and Michael Strube. 2023. Annotation-inspired\nimplicit discourse relation classification with auxil-\niary discourse connective generation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15696\u201315712, Toronto, Canada. Association\nfor Computational Linguistics.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang Shi,\nRaghuraman Krishnamoorthi, and Vikas Chandra.\n2023. Llm-qat: Data-free quantization aware training\nfor large language models.\nWanqiu Long, Bonnie Webber, and Deyi Xiong. 2020.\nTED-CDB: A large-scale Chinese discourse relation\ndataset on TED talks. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2793\u20132803, On-\nline. Association for Computational Linguistics.\nChristopher D. Manning, Mihai Surdeanu, John Bauer,\nJenny Finkel, Steven J. Bethard, and David Mc-\nClosky. 2014. The Stanford CoreNLP natural lan-\nguage processing toolkit.\nIn ACL 2014 System\nDemonstrations, pages 55\u201360.\nScott Martin, Shivani Poddar, and Kartikeya Upasani.\n2020. MuDoCo: Corpus for multidomain corefer-\nence resolution and referring expression generation.\nIn Proceedings of the 12th Language Resources and\nEvaluation Conference, pages 104\u2013111, Marseille,\nFrance. European Language Resources Association.\nNasrin Mostafazadeh, Michael Roth, Annie Louis,\nNathanael Chambers, and James Allen. 2017. LS-\nDSem 2017 shared task: The story cloze test. In\nProceedings of the 2nd Workshop on Linking Models\nof Lexical, Sentential and Discourse-level Seman-\ntics, pages 46\u201351, Valencia, Spain. Association for\nComputational Linguistics.\nNikola Mrk\u0161i\u00b4c, Diarmuid \u00d3 S\u00e9aghdha, Tsung-Hsien\nWen, Blaise Thomson, and Steve Young. 2017. Neu-\nral belief tracker: Data-driven dialogue state tracking.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1777\u20131788.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quanti-\nzation. In Proceedings of the 37th International Con-\nference on Machine Learning, ICML\u201920. JMLR.org.\nAnna Nedoluzhko, Michal Nov\u00e1k, Martin Popel,\nZden\u02c7ek \u017dabokrtsk\u00fd, Amir Zeldes, and Daniel Zeman.\n2022. CorefUD 1.0: Coreference meets Universal\nDependencies. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n4859\u20134872, Marseille, France. European Language\nResources Association.\nOpenAI. 2022. Optimizing language models for dia-\nlogue.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nZhufeng Pan, Kun Bai, Yan Wang, Lianqiang Zhou,\nand Xiaojiang Liu. 2019. Improving open-domain\ndialogue systems via multi-turn incomplete utterance\nrestoration. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1824\u20131833, Hong Kong, China. Association\nfor Computational Linguistics.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari-\ndou, Ngoc Quan Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFern\u00e1ndez. 2016. The LAMBADA dataset: Word\nprediction requiring a broad discourse context. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1525\u20131534, Berlin, Germany.\nAssociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311\u2013318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Bj\u00f6rkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using OntoNotes. In Proceed-\nings of the Seventeenth Conference on Computational\nNatural Language Learning, pages 143\u2013152, Sofia,\nBulgaria. Association for Computational Linguistics.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unrestricted\ncoreference in OntoNotes. In Joint Conference on\nEMNLP and CoNLL - Shared Task, pages 1\u201340, Jeju\nIsland, Korea. Association for Computational Lin-\nguistics.\nJun Quan, Deyi Xiong, Bonnie Webber, and Changjian\nHu. 2019. GECOR: An end-to-end generative el-\nlipsis and co-reference resolution model for task-\noriented dialogue. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 4547\u20134557, Hong Kong, China. Association\nfor Computational Linguistics.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Anto-\nnia Creswell, Nat McAleese, Amy Wu, Erich Elsen,\nSiddhant Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy,\nChris Jones, James Bradbury, Matthew Johnson,\nBlake Hechtman, Laura Weidinger, Iason Gabriel,\nWilliam Isaac, Ed Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2022. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher.\nMichael Regan, Pushpendre Rastogi, Arpit Gupta, and\nLambert Mathias. 2019. A dataset for resolving re-\nferring expressions in spoken dialogue via contextual\nquery rewrites (cqr). ArXiv, abs/1903.11783.\nHui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Peng-\nwei Hu, Cheng Niu, and Jie Zhou. 2019. Improv-\ning multi-turn dialogue modelling with utterance\nReWriter. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 22\u201331, Florence, Italy. Association for Com-\nputational Linguistics.\nS. Tata and J.M. Patel. 2003. Piqa: an algebra for query-\ning protein data sets. In 15th International Confer-\nence on Scientific and Statistical Database Manage-\nment, 2003., pages 141\u2013150.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nBo-Hsiang Tseng, Shruti Bhargava, Jiarui Lu, Joel\nRuben Antony Moniz, Dhivya Piraviperumal, Lin\nLi, and Hong Yu. 2021. Cread: Combined resolution\nof ellipses and anaphora in dialogues. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3390\u20133406.\nSzymon Tworkowski, Konrad Staniszewski, Miko\u0142aj\nPacek, Yuhuai Wu, Henryk Michalewski, and Piotr\nMi\u0142o\u00b4s. 2023. Focused transformer: Contrastive train-\ning for context scaling.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems.\nIn Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nBonnie\nWebber,\nRashmi\nPrasad,\nAlan\nLee,\nand\nAravind\nJoshi.\n2019.\nThe\npenn\ndis-\ncourse\ntreebank\n3.0\nannotation\nmanual.\nhttps://catalog.ldc.upenn.edu/docs/\nLDC2019T05/PDTB3-Annotation-Manual.pdf.\nKai-Cheng Yang and Filippo Menczer. 2023. Large\nlanguage models can rate news outlet credibility.\nSteve Young, Milica Ga\u0161i\u00b4c, Blaise Thomson, and Ja-\nson D Williams. 2013. Pomdp-based statistical spo-\nken dialog systems: A review. Proceedings of the\nIEEE, 101(5):1160\u20131179.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xing-\ngang Wang, Yuzhang Shang, Guangyu Sun, Qiang\nWu, Jiaxiang Wu, and Bingzhe Wu. 2023. Rptq:\nReorder-based post-training quantization for large\nlanguage models.\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\nRaghav Gupta, Jianguo Zhang, and Jindong Chen.\n2020. Multiwoz 2.2: A dialogue dataset with addi-\ntional annotation corrections and state tracking base-\nlines. In Proceedings of the 2nd Workshop on Natu-\nral Language Processing for Conversational AI, ACL\n2020, pages 109\u2013117.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence?\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nQi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and\nMinlie Huang. 2020. CrossWOZ: A large-scale Chi-\nnese cross-domain task-oriented dialogue dataset.\nTransactions of the Association for Computational\nLinguistics, 8:281\u2013295.\nA\nTask Design Examples\nTable 10 presents the input example for each task.\nFor CR, we only show examples from OntoNotes.\nB\nFew-shot Settings\nTable 11 shows the number of examples for each\ndataset that yields the best scores. All datasets\nexcept WSC273 and PDTB3 use randomly selected\nexamples from the training set. Since WSC273\ndoes not include any train or validation set, we use\nthe zero-shot setting, as scores presented in Table\n5. For each class in PDTB3, we randomly select\ntwo examples from the training set for prompting.\nFor some particular datasets, such as OntoNotes,\nexperiments are only performed in the zero-shot\nand one-shot settings due to the limitation on input\nlength.\nC\nReliability of Experiment Results\nFor each task, we have randomly run several exper-\nimental setups with multiple rounds, with over 10\nsettings in total. However, due to the challenges\nposed by limited time, budget, and computing re-\nsources, it is very difficult to run multiple rounds\nfor every single experiment, given the complexity\nof our experimental setup. In addition, for exist-\ning experiments with multiple rounds, we empiri-\ncally observe that there is low variance across the\nrounds, which leads us to assume that performing\nthe remaining experiments with a single run does\nnot significantly impact the arguments presented in\nthis paper.\nCoreference Resolution\nInstructions: Please carefully read the following passages. For each passage and the options, you must identify which option the mention marked in *bold* refers to. If the marked mention does\nnot have any antecedent, please select \u201cno antecedent\u201d.\n[Few-shot examples]\nContext: \u2014 basically , it was unanimously agreed upon by the various relevant parties . To express *its* determination , the Chinese securities regulatory department compares\nthis stock reform to a die that has been cast . It takes time to prove whether the stock reform can really meet expectations , and whether any deviations that arise during the stock reform can be\npromptly corrected . Dear viewers , the China News program will end here . This is Xu Li . Thank you everyone for watching . Coming up is the Focus Today program hosted by Wang Shilin .\nGood-bye , dear viewers .\nChoice:\nA. the Chinese securities regulatory department\nB. this stock reform\nC. the stock reform\nD. you\nE. everyone\nF. no antecedent\nQuestion: What does *its* refers to?\nAnswer: A\nDialogue State Tracking\nConsider the following list of concepts, called \"slots\" provided to you as a json list.\n\u201cslots\u201d: {\u201crestaurant-pricerange\u201d: \u201cprice budget for the restaurant\u201d,\n\u201crestaurant-area\u201d: \u201carea or place of the restaurant\u201d,\n\u201crestaurant-food\u201d: \u201cthe cuisine of the restaurant you are looking for\u201d,\n...\n\u201chotel-postcode\u201d: \u201cpostal code of the hotel\u201d,\n\u2018hotel-ref\u201d: \u201creference number of the hotel booking\u201d\n}\nSome \u201cslots\u201d can only take a value from predefined list:\n\u201ccategorical\u201d: {\u201crestaurant-pricerange\u201d: [\u2018cheap\u2019, \u2018expensive\u2019, \u2018moderate\u2019],\n\u201crestaurant-area\u201d: [\u2019centre\u2019, \u2019east\u2019, \u2019north\u2019, \u2019south\u2019, \u2019west\u2019],\n\u201crestaurant-bookday\u201d: [\u2019monday\u2019, \u2019tuesday\u2019, \u2019wednesday\u2019, \u2019thursday\u2019, \u2019friday\u2019, \u2019saturday\u2019, \u2019sunday\u2019],\n...\n\u201chotel-internet\u201d: [\u2019free\u2019, \u2019no\u2019, \u2019yes\u2019], \u201chotel-area\u201d: [\u2018centre\u2019, \u2018east\u2019, \u2018north\u2019, \u2018south\u2019, \u2018west\u2019]\n}\nNow consider the following dialogue between two parties called the \u201csystem\u201d and \u201cuser\u201d. Can you tell me which of the \u201cslot\u201d was updated by the \u201cuser\u201d in its latest response to the \u201csystem\u201d?\nPresent the updates in JSON format. If no \u201cslots\u201d were updates, return an empty JSON list. If you encounter \u201cslot\u201d that was requested by the \u201cuser\u201d then fill them with \u201c?\u201d. If a user does not\nseem to care about a discussed \u201cslot\u201d fill it with \u201cdontcare\u201d.\nInput:\nPrevious state: {}\n\u201csystem\u201d: \u201c\u201d\n\u201cuser\u201d: \u201cI\u2019m looking for a moderately priced place to eat that\u2019s in the centre of town.\u201d\nOutput: {\u201crestaurant-pricerange\u201d: \u201cmoderate\u201d, \u201crestaurant-area\u201d: \u201ccentre\u201d}\nImplicit Discourse Relation Classification\nInstructions: Given two arguments and a list of connective words, please select the most likely connective between two arguments.\nBelow are the descriptions of four discourse relation labels. Please find the correct label for each example.\nTemporal: The tag temporal is used when the situations described in the arguments are intended to be related temporally.\nContingency: The tag Contingency is used when the situation described by one argument provides the reason, explanation or justification for the situation described by the other.\nComparison: The tag Comparison is used when the discourse relation between two arguments highlights their differ- ences or similarities, including differences between expected consequences\nand actual ones.\nExpansion: The label Expansion is used for relations that expand the discourse and move its narrative or exposition forward.\n[Few-shot examples]\nInput:\nArg 1: Amcore, also a bank holding company, has assets of $1.06 billion.\nArg 2: Central\u2019s assets are $240 million.\nQuestion: What is the connective that best describes the relation between two arguments?\nA. Temporal\nB. Contingency\nC. Comparison\nD. Expansion\nAnswer: C\nQuery Rewrite\nInstructions: Rewrite the last query following interaction into a well-formed, context independent query. Resolve any disfluencies or grammatical errors in the query.\n[Few-shot examples]\nInput:\nUser: Try to reach Forbes now .\nBot: Forbes at Washington Post ? Or Forbes of Publishing Division ?\nUser: Publishing Division .\nRewrite: Forbes of Publishing Division\nTable 10: Examples of task design for each task in the context understanding benchmark.\nTask\nCoreference\nDST\nDiscourse\nquery rewriting\nDataset\nWSC273\nOntoNotes\nMultiWOZ\nPDTB3\nMuDoCo\nQReCC\nInCar\nGECOR\nCANARD\nN-example\nZero-shot\nOne-shot\n5-shot\n8-shot\n10-shot\n5-shot\n10-shot\n10-shot\n5-shot\nTable 11: N-shot settings for each task & dataset that yields the highest scores. For each task and model, we use\nconsistent N-shot settings for comparison.\n"
  },
  {
    "title": "Efficient Exploration for LLMs",
    "link": "https://arxiv.org/pdf/2402.00396.pdf",
    "upvote": "18",
    "text": "Efficient Exploration for LLMs\nVikranth Dwaracherla1, Seyed Mohammad Asghari1, Botao Hao1 and Benjamin Van Roy1, 2\n1Google DeepMind, 1,2Stanford University\nWe present evidence of substantial benefit from efficient exploration in gathering human feedback to\nimprove large language models. In our experiments, an agent sequentially generates queries while\nfitting a reward model to the feedback received. Our best-performing agent generates queries using\ndouble Thompson sampling, with uncertainty represented by an epistemic neural network. Our results\ndemonstrate that efficient exploration enables high levels of performance with far fewer queries. Further,\nboth uncertainty estimation and the choice of exploration scheme play critical roles.\n1. Introduction\nLarge language models demonstrate remarkable capabilities after learning from enormous volumes\nof text data (Anil et al., 2023; Hoffmann et al., 2022; OpenAI, 2023). Yet, reinforcement learning\nfrom human feedback (RLHF) greatly improves their behavior even after only tens of thousands of\ninteractions (Bai et al., 2022; Glaese et al., 2022; Ouyang et al., 2022; Stiennon et al., 2020). The\nuptake of chatbots affords opportunities to gather increasing volumes of human feedback, with each\nengagement eliciting expressions of satisfaction or preference (OpenAI, 2022). It is natural to wonder\nwhat new capabilities may emerge with this growing source of data. Superhuman ingenuity remains\nan alluring possibility.\nWith increasing volumes, more can be inferred from human feedback. This affords the confidence\nto deviate further from a pretrained model. But given that this process learns only from humans,\nhow can we hope for emergence of superhuman ingenuity? Perhaps such an outcome is plausible\nbecause rating is easier than synthesizing novel content. This is analogous to how, for an NP-complete\nproblem, though solution is hard, verification of a proposed solution is easy.\nSuppose, for example, a pretrained model extrapolates from its training data to generate large\nnumbers \u2013 perhaps millions or billions \u2013 of ideas, one of which is ingenious. While a human may not\nhave come up with that idea, learning from enough human feedback can identify it from among the\nlarge number of ideas generated by the model. And, building on this innovation, further extrapolation\ncan continue to expand the frontier of ingenuity. In this way, with enough human feedback, a model\nought to become capable of generating content that a human could not. But will gathering the\nrequired feedback take months, years, or decades?\nWe present in this paper evidence of enormous benefit to active exploration. By active exploration\nwe mean the tailoring of interactions to elicit useful feedback. In particular, our results demonstrate\nthat high levels of performance can be attained with far less feedback. This acceleration may enable\nsuperhuman ingenuity much, perhaps decades, sooner.\nA common practice in reinforcement learning from human feedback (RLHF) is to send queries,\neach comprised of a prompt and a pair of distinct responses, to human raters. Each rater expresses a\npreference for one response over the other. Prompts are drawn from a corpus, while responses are\ngenerated by the large language model. As this process progresses, a reward model is fit to the data\nand steers subsequent responses to align with with feedback received thus far.\nIn this paper, we restrict attention to the aforementioned sort of interaction, in which each query\nincludes a prompt and pair of distinct responses. We refer to the standard practice of sampling each\narXiv:2402.00396v1  [cs.LG]  1 Feb 2024\nEfficient Exploration for LLMs\npair of responses using the language model as passive exploration. We compare the performance of\npassive exploration to several active exploration algorithms. One is Boltzmann exploration, which\ntends to select responses with higher predicted reward. We also tried two approaches that leverage\nuncertainty estimates offered by an epistemic neural network (ENN). The first, which we refer to as\ninfomax, selects a pair of responses with an aim of maximizing information revealed by the feedback.\nThis belongs within the widely used collection of algorithms that aim to maximize information gain\n(see, e.g., (Houthooft et al., 2016; MacKay, 1992; Sadigh et al., 2018; Sun et al., 2011)). The second,\ncalled double Thompson sampling (Wu & Liu, 2016), samples responses according to the probability\nthey are optimal. These exploration algorithms will be described more precisely in Section 4.\nFigure 1 compares empirical results produced using different exploration algorithms. The experi-\nments that generated these results are described in Section 5. Each plotted point corresponds to a\nlevel of performance attained. The horizontal coordinate identifies the number of queries required by\ndouble TS to reach that performance level, while the vertical coordinate identifies that required by\nan alternative. The plot for passive exploration clearly demonstrates that active exploration using\ndouble TS greatly reduces the number of queries required to reach high levels of performance.\nBoltzmann exploration performed best among algorithms we tried that used only a point estimate\nreward model, without uncertainty estimates. The plot for Boltzmann demonstrates that uncertainty\nestimates, as used by double TS, enable dramatic improvement. Finally, the plot for infomax\nshows how, even among tried and tested algorithms that leverage uncertainty estimates, the choice\nof exploration algorithm can drive large performance differences.\nFigure 1 | Queries required by double TS versus alternatives to attain various levels of performance.\nWhile, these are to our knowledge the first results demonstrating substantial benefits from\nactive exploration in tuning large language models, they build on a long history of work pertaining\nto exploration algorithms (Lattimore & Szepesv\u00e1ri, 2020). In particular, our problem is an instance of\nthe contextual dueling bandit (Dud\u00edk et al., 2015; Saha, 2021; Yue et al., 2012) and our algorithms\nbuild on information-seeking schemes (Hennig & Schuler, 2012; Houthooft et al., 2016; MacKay,\n1992; Russo & Van Roy, 2014; Ryzhov et al., 2012; Sadigh et al., 2018; Sun et al., 2011) and Thompson\nsampling (Russo et al., 2018; Thompson, 1933; Wu & Liu, 2016). Further, our effort continues a line\nof work that has scaled efficient exploration algorithms to increasingly complex environments using\nneural networks (Badia et al., 2020; Bellemare et al., 2016; Burda et al., 2018; Dwaracherla et al.,\n2020; Lu & Van Roy, 2017; Osband et al., 2016, 2019, 2023b; Ostrovski et al., 2017; Riquelme et al.,\n2018; Zhang et al., 2020; Zhou et al., 2020).\n2\nEfficient Exploration for LLMs\n2. Experimentation Pipeline\nWe start by presenting the experimentation pipeline we use to study exploration algorithms. This\npipeline builds on existing tools, including the Anthropic datasets (Bai et al., 2022) and the Gemini\nNano and Gemini Pro pretrained language models (Team et al., 2023). It makes use of a human\nfeedback simulator, which generates in response to each query a binary expression of preference\nbetween responses. The pipeline is made up of two parts: a learning pipeline and an assessment\npipeline. The former governs the interface between the agent and the human feedback simulator\nin the process of sequential querying and learning. The latter governs the interface between the\npretrained language model, the new response generation model, and the human feedback simulator\nin the process of assessing relative performance.\nAn agent learns sequentially from feedback to queries, each comprised of a prompt and two\nalternative responses. As illustrated in Figure 2, each query is crafted by the agent and presented to a\nhuman preference simulator, which indicates a binary preference between the two. Over each epoch\nof interaction, the agent transmits a batch of \ud835\udc35 queries and receives the \ud835\udc35 bits of feedback. Each\nprompt is sampled uniformly from the Anthropic Helpfulness Base train dataset. Each agent we study,\nwhen presented with a prompt, crafts its pair of responses by first generating \ud835\udc41 candidates using the\nGemini Nano model and then applying an exploration algorithm that selects two from among these \ud835\udc41.\nThe exploration scheme accesses a reward model which is trained on queries and feedback observed\nthus far. Each agent we consider is distinguished by its exploration algorithm and the architecture\nand training algorithm that produce its reward model. In some of the agents we consider, the reward\nmodel takes the form of an epistemic neural network, which offers the exploration algorithm access\nto uncertainty estimates in addition to point estimates of reward. Each reward model builds on the\ntorso of the Gemini Nano model. By this we mean that the reward model first computes the last-layer\nembedding of the pretrained transformer model and then applies an multilayer perceptron (MLP)\nhead. We elaborate on architectures and training algorithms in Section 3.\nFigure 2 | The sequential querying and learning pipeline.\nTo simulate how humans choose between responses, we use a reward model that scores each\nprompt-response pair. For each query, a preference is sampled according to the Bradley-Terry choice\nmodel based on scores assigned to the two prompt-response pairings. The reward model used by this\nsimulator is fit to the Anthropic datasets, with an architecture that reuses the torso of the Gemini\nPro language model. Further detail is provided in Appendix A. Note that, since Gemini Pro is far\nlarger than Gemini Nano, choices are made by a much more complex model than that available to the\nagent. This difference in scale is intended to reflect the fact that humans may exhibit more complex\nbehavior than that modeled by the agent. Algorithm 1 offers a concise presentation of interactions \u2013\nin particular, what is transmitted and received to and from the agent and simulator \u2013 in our learning\n3\nEfficient Exploration for LLMs\npipeline.\nAlgorithm 1 learning interface\ninput: prompt_set, agent, feedback_simulator\nhyperparams: \ud835\udc35, \ud835\udc47\n1: for \ud835\udc61 in 1, . . . , \ud835\udc47 do\n2:\ntransmitted to agent: \ud835\udc35 prompts\n3:\nreceived from agent: two responses per prompt\n4:\ntransmitted to simulator: \ud835\udc35 queries\n5:\nreceived from simulator: \ud835\udc35 bits of feedback\n6:\ntransmitted to agent: \ud835\udc35 bits of feedback\n7: end for\nFigure 3 illustrates our pipeline for assessing agent performance. Performance is measured relative\nto the Gemini Nano model. A sequence of prompts is sampled from Anthropic Helpfulness Base\neval dataset. For each, two responses are sampled. One by Gemini Nano and the other by a new\nresponse generation model that uses the learned reward model. This new model operates by sampling\n\ud835\udc41 responses using Gemini Nano and then selecting the one that scores highest according to the\nagent\u2019s reward model. The human preference simulator outputs its probability of choosing the agent\u2019s\nresponse over the alternative generated by Gemini Nano. These probabilities are averaged over\nprompts, and this average is referred to as the agent\u2019s win rate, as it represents the fraction of time\nthat the agent\u2019s response is preferred. Note that the win rate can also be estimated by averaging binary\nindications of simulated choice, though a larger number of queries would be required for an estimate\nproduced in this manner to converge. Algorithm 2 offers a concise presentation of interactions in the\nassessment phase.\nFigure 3 | The performance assessment pipeline.\nAlgorithm 2 assessment interface\ninput: prompt_set, model1, model2, feedback_simulator\n1: for prompt in prompt_set do\n2:\ntx to models: prompt\n3:\nrx from models: one response per model\n4:\ntx to simulator: query (prompt + 2 responses)\n5:\nrx from simulator: prob of preferring response 1\n6: end for\nreturn average across preference probabilities\nNote that our experiment pipeline sidesteps the sort of policy-gradient methods typically used to\noptimize reward. Instead, our agent samples \ud835\udc41 responses from the base language model (Gemini\n4\nEfficient Exploration for LLMs\nNano) and selects from among those the one that maximizes reward. This best-of-\ud835\udc41 procedure\nserves to approximate policy-gradient-based optimization, but without its cumbersome computational\nrequirements. The best-of-\ud835\udc41 procedure also cultivates more transparent analyses, since it avoids\npoorly understood dependence on the hyperparameter tinkering often required to obtain reasonable\nresults from policy gradient methods. A prototypical policy gradient approach minimizes a loss\nfunction that balances between two objectives: similarity to the base language model and alignment\nwith reward. A scalar hyperparameter multiplies the similarity measure, striking the balance between\nthese objectives. The parameter \ud835\udc41 plays a similar role in the best-of-\ud835\udc41 approach. As \ud835\udc41 increases,\nmaximizing over responses more closely aligns the agent with reward. Moderating \ud835\udc41 encourages\nagent behavior more similar to the base language model.\n3. Reward Model Architectures and Training\nReward models guide response selection in both the learning and assessment phases of our experiment\npipeline. We consider two types of reward models, each of which is fit to observed preference data.\nThe first is a point estimate that assigns a reward to each prompt-response pair. The second depends\nadditionally on an epistemic index. Sampling an epistemic index from a reference distribution induces\nrandomness in reward, which models epistemic uncertainty about the reward. In this section, we\ndescribe the neural network architectures and training algorithms used in our experiments.\nWe train reward models that each take as input the last-layer embedding of the Gemini Nano\nlanguage model. As illustrated in Figure 4, a reward is assigned to a prompt-response pair by first\npassing it through the language model torso and then through a reward model.\nFigure 4 | Our reward models take as input the last-layer embedding of the Gemini Nano language\nmodel. A stop gradient prevents torso updating of torso weights.\n3.1. Point Estimate\nIn our architecture, a point estimate reward model takes the form of a feedforward multi-layer\nperceptron (MLP). This reward model takes as input the last-layer embedding of the Gemini Nano\nlanguage model, which itself takes as input a prompt-response pair (\ud835\udc65, \ud835\udc66). The reward model then\noutputs a scalar reward b\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66). Here, \ud835\udf03 is the vector of MLP parameters.\nWe train reward models on preference data. Each data point consists of a query, consisting of a\nprompt and pair of responses, and a binary indication of preference between the responses. Given a\nset D of such data points, to compute MLP parameters, we optimize the loss function\nLpoint(\ud835\udf03|D) =\n\u2211\ufe01\n(\ud835\udc65,\ud835\udc66,\ud835\udc66\u2032,\ud835\udc50)\u2208D\nce(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66), \ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66\u2032), \ud835\udc50) + \ud835\udf06\u2225\ud835\udf03\u22252\n2,\n(1)\nwhere \ud835\udf06 is the regularization strength, \ud835\udc50 indicates choice or preference, and ce(\u00b7, \u00b7, \u00b7) denotes the cross\nentropy loss:\nce(\ud835\udc45, \ud835\udc45\u2032, \ud835\udc50) = \u2212(1 \u2212 \ud835\udc50)\ud835\udc45 \u2212 \ud835\udc50\ud835\udc45\u2032 + ln(\ud835\udc52\ud835\udc45 + \ud835\udc52\ud835\udc45\u2032).\n(2)\n5\nEfficient Exploration for LLMs\nNote that when response \ud835\udc66 is preferred over \ud835\udc66\u2032, the preference indicator \ud835\udc50 is 0 and vice versa.\n3.2. Epistemic Neural Network\nWe use epistemic neural networks (ENNs) to model epistemic uncertainty about reward (Osband\net al., 2023a). Given the dataset D, ENN parameters are obtained by minimizing the loss function\nLENN(\ud835\udf03|D) = \ud835\udf06\u2225\ud835\udf03 \u2212 \u02dc\ud835\udf03\u22252 +\n\u222b\n\ud835\udc67\u2208Z\n\ud835\udc5d\ud835\udc67(\ud835\udc51\ud835\udc67)L(\ud835\udf03|D, \ud835\udc67),\n(3)\nwhere \ud835\udc5d\ud835\udc67 is the epistemic index reference distribution, \u02dc\ud835\udf03 is the initial parameter vector, and\nL(\ud835\udf03|D, \ud835\udc67) =\n\u2211\ufe01\n(\ud835\udc65,\ud835\udc66,\ud835\udc66\u2032,\ud835\udc50)\u2208D\nce(\ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66|\ud835\udc67), \ud835\udc5f\ud835\udf03(\ud835\udc65, \ud835\udc66\u2032|\ud835\udc67), \ud835\udc50).\nTo interpret these objects, note that with \ud835\udc67 sampled from \ud835\udc5d\ud835\udc67, the reward function \ud835\udc5f\u02dc\ud835\udf03(\u00b7|\ud835\udc67) represents\na sample from a prior distribution over reward functions. In the loss function LENN, regularizing\ntoward \u02dc\ud835\udf03 serves to maintain a suitable degree of diversity across epistemic indices after training.\n3.3. Training\nTo train each reward model, we maintain a replay buffer and apply a stochastic gradient descent\n(SGD) algorithm with respect to loss functions described in Section 3.1 and 3.2. In particular, at the\nend of each epoch of interaction, over which the agent transmits \ud835\udc35 queries and receives \ud835\udc35 bits of\nfeedback, the agent inserts the resulting \ud835\udc35 data points into a FIFO replay buffer of capacity \ud835\udc36. Then,\nSGD steps are applied with random minibatches from the replay buffer, with stepsizes adapted by\nADAM.The reward model that has been trained is employed to determine the queries formulated in\nthe subsequent epoch.\n4. Exploration Algorithms\nWe now describe the set of exploration algorithms used in our empirical study.\n4.1. Passive Exploration\nCurrent RLHF systems typically explore passively, selecting response pairs according to Algorithm 3.\nThis algorithm takes a prompt \ud835\udc65 and a language model \ud835\udf0b as inputs. The language model encodes a\ndistribution \ud835\udf0b(\u00b7|\ud835\udc65) from which it samples responses. The algorithm returns two responses sampled by\nthe language model.\nAlgorithm 3 passive exploration\ninput: \ud835\udc65, \ud835\udf0b\n1: sample response \ud835\udc66 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n2: repeat\n3:\nsample response \ud835\udc66\u2032 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n4: until \ud835\udc66\u2032 \u2260 \ud835\udc66\nreturn \ud835\udc66, \ud835\udc66\u2032\n6\nEfficient Exploration for LLMs\n4.2. Active Exploration with a Point Estimate\nWhen selecting a pair of responses, the agent can make use of a reward model that has been trained\non feedback to all or some past queries. Passive exploration forgoes this opportunity. We now consider\nBoltzmann exploration, which makes use of a point estimate reward model, which assigns a reward\n\ud835\udc5f(\ud835\udc65, \ud835\udc66) to each prompt-response pair. This constitutes a form of active exploration: responses are\ntailored based on past feedback, with an aim to gather more useful future feedback than passive\nexploration.\nAs presented in Algorithm 4, in addition to the inputs \ud835\udc65 and \ud835\udf0b used for passive exploration, Boltz-\nmann exploration requires a point estimate reward model \ud835\udc5f. Further, there are two hyperparameters:\na temperature \ud835\udf0f and a response set cardinality \ud835\udc41. The language model generates \ud835\udc41 responses, and two\nare sampled from a Boltzmann distribution with exponent \ud835\udc5f(\ud835\udc65, \u02dc\ud835\udc66\ud835\udc5b)/\ud835\udf0f assigned to each \ud835\udc5bth response \u02dc\ud835\udc66\ud835\udc5b.\nAlgorithm 4 Boltzmann\ninput: \ud835\udc65, \ud835\udf0b, \ud835\udc5f\nhyperparams: \ud835\udf0f, \ud835\udc41\n1: sample responses \u02dc\ud835\udc661, . . . , \u02dc\ud835\udc66\ud835\udc41 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n2: probs \ud835\udc5e\ud835\udc5b =\nexp(\ud835\udc5f(\ud835\udc65,\u02dc\ud835\udc66\ud835\udc5b)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc5b\u2032=1 exp(\ud835\udc5f(\ud835\udc65,\u02dc\ud835\udc66\ud835\udc5b\u2032 )/\ud835\udf0f) , \u2200\ud835\udc5b\n3: sample without replacement \ud835\udc56, \ud835\udc56\u2032 \u223c \ud835\udc5e\nreturn \ud835\udc66\ud835\udc56, \ud835\udc66\ud835\udc56\u2032\nNote that this algorithm recovers passive exploration as the temperature \ud835\udf0f grows. On the other\nhand, as \ud835\udf0f vanishes, Boltzmann exploration tends to select responses that are optimal or nearly so.\nOne could also consider a generalization of the algorithm that uses two different temperatures \ud835\udf0f1\nand \ud835\udf0f2 to select the two responses. Then, for example, as \ud835\udf0f1 vanishes and \ud835\udf0f2 grows, the first response\nbecomes optimal whereas the second is sampled uniformly. In our experimental work, we have not\nfound use of separate temperatures to improve performance. Further, we have found Algorithm 4 to\noffer the best performance among many alternatives that take the same inputs. This suggests that\nBoltzmann exploration selects responses about as well as one can hope for based on a point estimate\nreward model.\n4.3. Active Exploration with an ENN\nWe next consider algorithms that use an ENN reward model, for which the reward \ud835\udc5f(\ud835\udc65, \ud835\udc66|\ud835\udc67) assigned\nto each prompt-response pair depends additionally on an epistemic index. As discussed in Section\n3.2, the ENN is characterized by the reward model \ud835\udc5f and a reference distribution \ud835\udc5d. For fixed \ud835\udc65 and\n\ud835\udc66, by sampling multiple epistemic indices from \ud835\udc5d, reward uncertainty can be ascertained from the\nvariance among these samples.\nInfomax (Algorithm 5) takes an ENN reward model as input. Like Boltzmann exploration (Al-\ngorithm 4), infomax begins with the language model generating \ud835\udc41 responses. Then, \ud835\udc40 epistemic\nindices are sampled from \ud835\udc5d. For each pair of responses and each epistemic index, the ENN assigns a\nprobability to the event that a random human rater prefers the first response over the second. Infomax\nassesses uncertainty about this probability be calculating a sample variance across the \ud835\udc40 epistemic\nindices. Then, the algorithm selects the pair of responses to maximize uncertainty. Intuitively, this\ncan be thought of as maximizing a measure of feedback informativeness.\nA possible limitation of infomax is that the algorithm invests in seeking information about rewards\nwhether or not that information is useful to selecting the best responses. For example, infomax can\ninvest in refining an estimate of reward assigned to a response that has already been determined\n7\nEfficient Exploration for LLMs\nAlgorithm 5 infomax\ninput: \ud835\udc65, \ud835\udf0b, (\ud835\udc5f, \ud835\udc5d)\nhyperparams: \ud835\udc41, \ud835\udc40\n1: sample responses \u02dc\ud835\udc661, . . . , \u02dc\ud835\udc66\ud835\udc41 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n2: sample indices \ud835\udc671, . . . , \ud835\udc67\ud835\udc40 \u223c \ud835\udc5d\n3: rewards \ud835\udc45\ud835\udc5b,\ud835\udc5a = \ud835\udc5f(\ud835\udc65, \u02dc\ud835\udc66\ud835\udc5b|\ud835\udc67\ud835\udc5a), \u2200\ud835\udc5a, \ud835\udc5b\n4: pref probs \ud835\udc43\ud835\udc5b,\ud835\udc5b\u2032,\ud835\udc5a =\n\ud835\udc45\ud835\udc5b,\ud835\udc5a\n(\ud835\udc45\ud835\udc5b,\ud835\udc5a+\ud835\udc45\ud835\udc5b\u2032,\ud835\udc5a) , \u2200\ud835\udc5a, \ud835\udc5b, \ud835\udc5b\u2032\n5: means \ud835\udf07\ud835\udc5b,\ud835\udc5b\u2032 =\n\u00cd\n\ud835\udc5a \ud835\udc43\ud835\udc5b,\ud835\udc5b\u2032,\ud835\udc5a\n\ud835\udc40\n, \u2200\ud835\udc5b, \ud835\udc5b\u2032\n6: vars \ud835\udf0e2\n\ud835\udc5b,\ud835\udc5b\u2032 =\n\u00cd\n\ud835\udc5a (\ud835\udc43\ud835\udc5b,\ud835\udc5b\u2032,\ud835\udc5a\u2212\ud835\udf07\ud835\udc5b,\ud835\udc5b\u2032,\ud835\udc5a)2\n\ud835\udc40\u22121\n, \u2200\ud835\udc5b, \ud835\udc5b\u2032\n6: (\ud835\udc56, \ud835\udc56\u2032) \u2208 arg max\ud835\udc5b,\ud835\udc5b\u2032 \ud835\udf0e2\n\ud835\udc5b,\ud835\udc5b\u2032\nreturn \ud835\udc66\ud835\udc56, \ud835\udc66\ud835\udc56\u2032\nbased on previous feedback to be a poor choice. Double Thompson sampling (Wu & Liu, 2016), on\nthe other hand, tends to focus more on queries that are helpful in identifying the best responses. As\nwe will see in Section 5, double TS improves on the performance of infomax, as well as Boltzmann\nexploration.\nIntuitively, double TS (Algorithm 6) aims to select two responses that each have some chance of\nbeing optimal. Like Algorithms 4 and 5, we begin by sampling \ud835\udc41 responses. Then, two among these\n\ud835\udc41 responses are selected by sampling two epistemic indices from \ud835\udc5d and maximizing across rewards\nprescribed by each. In the event that samples are identical, the second response is resampled until it\ndiffers. If there is no difference after \ud835\udc3e iterations, the second response is instead sampled uniformly.\nAlgorithm 6 double Thompson sampling\ninput: \ud835\udc65, \ud835\udf0b, (\ud835\udc5f, \ud835\udc5d)\nhyperparams: \ud835\udc41, \ud835\udc3e\n1: sample responses \u02dc\ud835\udc661, . . . , \u02dc\ud835\udc66\ud835\udc41 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n2: sample index \ud835\udc67 \u223c \ud835\udc5d\n3: select response \ud835\udc56 \u2208 arg max\ud835\udc5b \ud835\udc5f(\ud835\udc65, \u02dc\ud835\udc66\ud835\udc5b|\ud835\udc67)\n4: repeat\n5:\nsample index \ud835\udc67\u2032 \u223c \ud835\udc5d\n6:\nselect response \ud835\udc56\u2032 \u2208 arg max\ud835\udc5b \ud835\udc5f(\ud835\udc65, \u02dc\ud835\udc66\ud835\udc5b|\ud835\udc67\u2032)\n7:\nafter \ud835\udc3e tries, instead sample \ud835\udc56\u2032 \u223c unif(1, . . . , \ud835\udc41)\n8: until \ud835\udc56\u2032 \u2260 \ud835\udc56\nreturn \ud835\udc66\ud835\udc56, \ud835\udc66\ud835\udc56\u2032\n5. Empirical Results\nIn our experiments, at the start of each epoch of interaction, each agents receives a batch of \ud835\udc35 = 32\nprompts and then, for each prompt, generates a pair of responses to form a query. Each agent\u2019s\n\ud835\udc35 = 32 queries are submitted to the preference simulator, yielding \ud835\udc35 = 32 bits of feedback. Each agent\ninserts its batch of \ud835\udc35 = 32 data points into its replay buffer. The replay buffers are first-in-first-out\n(FIFO) buffer, with a maximum capacity of \ud835\udc36 = 3200 data points. In other words, replay buffer holds\npreference data from a maximum of 100 most recent epochs. At the end of each epoch, each agent\nupdates its reward model as discussed in Section 3.\nRecall that each exploration algorithm selects each pair of responses from \ud835\udc41 candidates sampled\n8\nEfficient Exploration for LLMs\nby Gemini Nano. In our experiments, we set \ud835\udc41 = 100. Performance is assessed in terms of win\nrate relative to Gemini Nano on 2048 out-of-sample Anthropic Helpfulness base eval prompts, as\nexplained in Section 2. Each response selected in this assessment is chosen to score highest among\n\ud835\udc41 = 100 candidates sampled by Gemini Nano according to the agent\u2019s reward model. Note that we\nuse \ud835\udc41 = 100 responses both in our training and assement piplelines.\nFor a singular point estimate, we employ a feedforward multilayer perceptron (MLP) comprising\ntwo hidden layers, with 128 hidden units in each layer. As an ENN architecture, we utilize a collection\nof \ud835\udc46 = 10 MLPs, referring to each individual MLP as a particle. Each particle of ensemble consists\nof two 128 unit hidden layers. The reference distribution \ud835\udc5d\ud835\udc67 is defined as the uniform distribution\non {1, 2, . . . , \ud835\udc46}. When selecting an epistemic index \ud835\udc67 sampled from Unif(1, 2, . . . , \ud835\udc46), particle \ud835\udc67 is\nutilized to produce the output for that specific index \ud835\udc67. The ENN loss function presented in Section\n3.2 maintains diversity across particles by regularizing each toward initial parameters.\nFor the Boltzmann exploration scheme, we swept over several temperatures and found that small\ntemperatures produced best results. A similar level of performance was achieved by a variant of\nBoltzmann scheme that selects one of the response greedily and the second response using Boltzmann.\nMore details can be found in Appendix C.\nIn the case of infomax, we used 30 epistemic indices to compute means and variances. For\ndouble TS agent, we set the maximum number of attempts at producing a distinct second response\nto \ud835\udc3e = 30.\nAppendix B presents further detail on our hyperparameter selection process.\n5.1. Assessment of Exploration Algorithms\nFigure 5 plots win rates of each agent across different numbers of epochs of interactions. The results,\nobtained by averaging across 5 random seeds, clearly demonstrate that actively exploration accelerates\nlearning and results in higher win rates. Notably, the double TS agent emerges as the top performer.\nWe observe that infomax performs very well over early epochs but later falls far short of double\nTS. This divergence may be due to infomax\u2019s inclination to seek information, irrespective of whether\nthat information is helpful in desirable responses.\nEach of the performance curves in Figure 5 appears to converge, while one would hope for\ncontinued improvement as the volume of human interaction grows. Reward model capacity \u2013 which\ncan be thought of loosely as the effective number of parameters learned from feedback \u2013 gaits the\ndegree of improvement. For any capacity, one would expect convergence as the number of queries\ngrows. Increasing the capacity enables further improvement at the cost of increased computation.\nThis relates to the notion explained by Arumugam & Van Roy (2021) that it is beneficial to moderate\nthe complexity of a learning target based on the duration over which an agent expects to explore.\n5.2. Scaling with the Volume of Feedback\nFigure 1, reproduced from Section 1 for convenience, plots the number of queries required by\nalternatives to match the performance of double TS, which we found to be most efficient among\nexploration algorithms we considered. While the plots are not conclusive, we discern that they are\nconcave. Suppose we measure the advantage of efficient exploration in terms of the percentage\nreduction in data required to attain any given level of performance. Concavity of the plots in Figure\n1 implies that, as the scale of human feedback data grows, so does the advantage afforded by\nefficient exploration. For the level of performance attained by 30, 000 passive queries, double TS\n9\nEfficient Exploration for LLMs\nFigure 5 | Performance with passive, Boltzmann, infomax and double TS exploration algorithms.\nWe can see that active exploration leads to much better levels of performance with the same amount\nof data. double TS exploration scheme leads to the best level of performance.\nFigure 1 | Queries required by double TS versus alternatives to attain various levels of performance.\nreduces data requirements by an order of magnitude. An alluring possibility is that, as the number\nof interactions grow to billions, efficient exploration may offer a multiplier effect reaching several\norders of magnitude. This has the potential to accelerate by decades the attainment of superhuman\ncreativity.\n5.3. Quality of Uncertainty Estimates\nBoltzmann exploration performed best among algorithms we tried that select queries based on a\npoint estimate reward model. The large improvement demonstrated by double TS is enabled by\nuncertainty estimates offered by our ENN reward model.\nThe quality of uncertainty estimates can be assessed in terms of dyadic joint negative-log loss\n(NLL) (Osband et al., 2022). Figures 6 and 7 plot marginal and dyadic joint NLL for our point estimate\nand ENN reward models, each trained on 40, 000 queries. These plots indicate that, while both\n10\nEfficient Exploration for LLMs\nreward models render similar marginal NLL, the ENN reward model offers highly favorable dyadic\njoint NLL. This serves as a sanity check that our ENN reward model indeed produces meaningful\nuncertainty estimates.\nWe also used dyadic joint NLL to guide hyperparameter selection for our point estimate and\nENN reward models used by our exploration algorithms. In particular, we swept over candidates for\nlearning rate, training the agent over multiple epochs to identify learning rate the minimize dyadic\njoint NLL.\nFigure 6 | Marginal nll\nFigure 7 | Dyadic joint nll\n5.4. The Life of a Prompt\nOur results indicate that double TS tends to converge on better responses than the alternatives. To\nunderstand more concretely how this occurs, let us study the evolution of rewards that models assign\nto responses to a specific prompt. To simplify this investigation, we will only compare double TS\nagainst Boltzmann exploration.\nRecall that we found Boltzmann exploration to be the top performer among algorithms that base\ndecisions on a point estimate reward model. Double TS, on the other hand, makes use of uncertainty\nestimates offered by an ENN reward model. We will examine estimates associated with a single\nprompt and two responses, selected from the eval data set. The first is the response that double\nTS arrives at, while the second is the response that Boltzmann exploration arrives at. The human\nfeedback simulator indicates preference for the first prompt 57.5% of the time.\nFigure 8 plots the prediction supplied by each reward model of the probability that the first\nresponse is preferred. The horizontal dotted line expresses the probability of 0.575 with which the\nfeedback simulator expresses preference for the first response. The predictions evolve as the reward\nmodels learn from queries. After 40,000 queries, double TS arrives at a prediction that is greater\nthan one-half, expressing preference for the first response. Boltzmann exploration, on the other hand,\nexpresses preference for the second with a prediction that is less than one-half.\nAlso displayed in the figure is the two-standard-deviation confidence interval based on uncertainty\nexpressed by the ENN reward model. Though double TS at some points predicts less than one-half,\nthe upper limit of its confidence interval remains greater than one-half. Hence, it remains uncertain\nabout which is the better response. In resolving this uncertainty, it recovers and arrives at a prediction\ngreater than one-half. Boltzmann exploration, on the other hand, is not guided by uncertainty\nestimates and thus does not recover from its erroneous prediction.\n11\nEfficient Exploration for LLMs\nFigure 8 | For a particular prompt, the dotted line indicates the probability that the simulator expresses\npreference for one response over another. Uncertainty estimates enable double TS to recover from an\ninaccurate prediction where Boltzmann exploration does not.\n6. Closing Remarks\nTo our knowledge, the results we have presented represent the first to demonstrate substantial benefits\nof active exploration in tuning large language models. That being said, there is much room for further\nwork in this area. To conclude this paper, we discuss several important research directions.\nOur experiments made use of a particularly simple ENN architecture comprised of an ensemble of\nMLPs. As demonstrated in (Osband et al., 2023a), alternative architectures strike a more effective\ntradeoff between computational requirements and quality of uncertainty estimates. Further, instead\nof designing ENNs based on the MLP, it may be possible to improve performance, especially as the\namount of human feedback data grows, by basing ENN designs on transformer architectures.\nAnother limitation of our reward model architectures is that each is only a \u201chead\u201d that takes\nthe last-layer embedding of an LLM as input. Performance can be improved by also tuning the LLM\ntorso. While advantages afforded by efficient exploration should extend, identifying the most effective\narchitectures and algorithms for exploring while tuning more of the LLM remains for future work.\nFinally, efficient exploration of multiturn dialog presents an interesting and important direction\nfor future research. In this paper, we viewed exploration as a means to quickly identifying a response\ndeemed desirable in isolation. In multiturn dialog, responses may be chosen instead because of how\nthey shape subsequent interactions. The subject of deep exploration addresses how an agent can\nefficiently identify effective responses that make up sequential interactions (Osband et al., 2016,\n2019). Leveraging deep exploration algorithms to improve dialog remains a challenge.\nReferences\nAnil, R., Dai, A. M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P.,\nChen, Z., Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-Hellstern, K., Mishra, G., Moreira, E.,\nOmernick, M., Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y., Zhang, Y., Abrego, G. H., Ahn, J.,\nAustin, J., Barham, P., Botha, J., Bradbury, J., Brahma, S., Brooks, K., Catasta, M., Cheng, Y., Cherry,\nC., Choquette-Choo, C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, M., Dev, S., Devlin, J.,\nD\u00edaz, M., Du, N., Dyer, E., Feinberg, V., Feng, F., Fienber, V., Freitag, M., Garcia, X., Gehrmann, S.,\nGonzalez, L., Gur-Ari, G., Hand, S., Hashemi, H., Hou, L., Howland, J., Hu, A., Hui, J., Hurwitz,\n12\nEfficient Exploration for LLMs\nJ., Isard, M., Ittycheriah, A., Jagielski, M., Jia, W., Kenealy, K., Krikun, M., Kudugunta, S., Lan,\nC., Lee, K., Lee, B., Li, E., Li, M., Li, W., Li, Y., Li, J., Lim, H., Lin, H., Liu, Z., Liu, F., Maggioni,\nM., Mahendru, A., Maynez, J., Misra, V., Moussalem, M., Nado, Z., Nham, J., Ni, E., Nystrom, A.,\nParrish, A., Pellat, M., Polacek, M., Polozov, A., Pope, R., Qiao, S., Reif, E., Richter, B., Riley, P.,\nRos, A. C., Roy, A., Saeta, B., Samuel, R., Shelby, R., Slone, A., Smilkov, D., So, D. R., Sohn, D.,\nTokumine, S., Valter, D., Vasudevan, V., Vodrahalli, K., Wang, X., Wang, P., Wang, Z., Wang, T.,\nWieting, J., Wu, Y., Xu, K., Xu, Y., Xue, L., Yin, P., Yu, J., Zhang, Q., Zheng, S., Zheng, C., Zhou, W.,\nZhou, D., Petrov, S., and Wu, Y. Palm 2 technical report, 2023.\nArumugam, D. and Van Roy, B. Deciding what to learn: A rate-distortion approach. In Proceedings of\nthe 38th International Conference on Machine Learning, pp. 373\u2013382, 2021.\nBadia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B., Kapturowski, S., Tieleman, O., Arjovsky,\nM., Pritzel, A., Bolt, A., et al. Never give up: Learning directed exploration strategies. arXiv preprint\narXiv:2002.06038, 2020.\nBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D.,\nHenighan, T., et al. Training a helpful and harmless assistant with reinforcement learning from\nhuman feedback. arXiv preprint arXiv:2204.05862, 2022.\nBellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-\nbased exploration and intrinsic motivation. Advances in neural information processing systems, 29,\n2016.\nBradley, R. A. and Terry, M. E. Rank Analysis of Incomplete Block Designs: I. The Method of Paired\nComparisons. Biometrika, 39(3/4):324\u2013345, 1952.\nBurda, Y., Edwards, H., Storkey, A., and Klimov, O. Exploration by random network distillation. arXiv\npreprint arXiv:1810.12894, 2018.\nDud\u00edk, M., Hofmann, K., Schapire, R. E., Slivkins, A., and Zoghi, M. Contextual dueling bandits. In\nConference on Learning Theory, pp. 563\u2013587. PMLR, 2015.\nDwaracherla, V., Lu, X., Ibrahimi, M., Osband, I., Wen, Z., and Van Roy, B. Hypermodels for exploration.\nIn International Conference on Learning Representations, 2020.\nGlaese, A., McAleese, N., Trebacz, M., Aslanides, J., Firoiu, V., Ewalds, T., Rauh, M., Weidinger, L.,\nChadwick, M., Thacker, P., et al. Improving alignment of dialogue agents via targeted human\njudgements. arXiv preprint arXiv:2209.14375, 2022.\nGlorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks.\nIn Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp.\n249\u2013256. JMLR Workshop and Conference Proceedings, 2010.\nHennig, P. and Schuler, C. J. Entropy search for information-efficient global optimization. Journal of\nMachine Learning Research, 13(6), 2012.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D.,\nHendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche,\nG., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Vinyals, O., Rae, J., and Sifre, L. An\nempirical analysis of compute-optimal large language model training. In Koyejo, S., Mohamed, S.,\nAgarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing\nSystems, volume 35, pp. 30016\u201330030. Curran Associates, Inc., 2022.\n13\nEfficient Exploration for LLMs\nHouthooft, R., Chen, X., Chen, X., Duan, Y., Schulman, J., De Turck, F., and Abbeel, P. Vime:\nVariational information maximizing exploration. In Lee, D., Sugiyama, M., Luxburg, U., Guyon,\nI., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29. Curran\nAssociates, Inc., 2016.\nLattimore, T. and Szepesv\u00e1ri, C. Bandit Algorithms. Cambridge University Press, 2020.\nLu, X. and Van Roy, B. Ensemble Sampling. In Proceedings of the 31st International Conference on\nNeural Information Processing Systems, pp. 3260\u20133268, 2017.\nMacKay, D. J. Information-based objective functions for active data selection. Neural computation, 4\n(4):590\u2013604, 1992.\nOpenAI. ChatGPT: Optimizing Language Models for Dialogue, 2022. URL https://openai.com/\nblog/chatgpt/.\nOpenAI. GPT-4 Technical Report. Technical report, OpenAI, 2023.\nOsband, I., Blundell, C., Pritzel, A., and Van Roy, B. Deep exploration via bootstrapped DQN. In Lee,\nD., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems, volume 29. Curran Associates, Inc., 2016.\nOsband, I., Van Roy, B., Russo, D. J., and Wen, Z. Deep exploration via randomized value functions.\nJournal of Machine Learning Research, 20(124):1\u201362, 2019.\nOsband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Hao, B., Ibrahimi, M., Lawson, D., Lu, X.,\nO\u2019Donoghue, B., and Van Roy, B. The neural testbed: Evaluating joint predictions. In Advances in\nNeural Information Processing Systems, volume 35. Curran Associates, Inc., 2022.\nOsband, I., Wen, Z., Asghari, M., Dwaracherla, V., Ibrahimi, M., Lu, X., and Van Roy, B. Epistemic\nneural networks. Advances in Neural Information Processing Systems, 34, 2023a.\nOsband, I., Wen, Z., Asghari, S. M., Dwaracherla, V., Ibrahimi, M., Lu, X., and Van Roy, B. Approximate\nThompson sampling via epistemic neural networks. In Evans, R. J. and Shpitser, I. (eds.), Proceedings\nof the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence, volume 216 of Proceedings of\nMachine Learning Research, pp. 1586\u20131595. PMLR, 31 Jul\u201304 Aug 2023b.\nOstrovski, G., Bellemare, M. G., Oord, A., and Munos, R. Count-based exploration with neural density\nmodels. In International conference on machine learning, pp. 2721\u20132730. PMLR, 2017.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama,\nK., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P.,\nChristiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with\nhuman feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.),\nAdvances in Neural Information Processing Systems, volume 35, pp. 27730\u201327744. Curran Associates,\nInc., 2022.\nRiquelme, C., Tucker, G., and Snoek, J. Deep Bayesian bandits showdown: An empirical comparison\nof Bayesian deep networks for Thompson sampling. arXiv preprint arXiv:1802.09127, 2018.\nRusso, D. and Van Roy, B. Learning to optimize via information-directed sampling. Advances in Neural\nInformation Processing Systems, 27:1583\u20131591, 2014.\nRusso, D. J., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. A Tutorial on Thompson Sampling.\nFoundations and Trends\u00ae in Machine Learning, 11(1):1\u201396, 2018.\n14\nEfficient Exploration for LLMs\nRyzhov, I. O., Powell, W. B., and Frazier, P. I. The knowledge gradient algorithm for a general class of\nonline learning problems. Operations Research, 60(1):180\u2013195, 2012.\nSadigh, D., Landolfi, N., Sastry, S. S., Seshia, S. A., and Dragan, A. D. Planning for cars that coordinate\nwith people: Leveraging effects on human actions for planning and active information gathering\nover human internal state. Autonomous Robots (AURO), 42(7):1405\u20131426, October 2018. ISSN\n1573-7527. doi: 10.1007/s10514-018-9746-1.\nSaha, A.\nOptimal algorithms for stochastic contextual preference bandits.\nAdvances in Neural\nInformation Processing Systems, 34:30050\u201330062, 2021.\nStiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and\nChristiano, P. F. Learning to summarize with human feedback. Advances in Neural Information\nProcessing Systems, 33:3008\u20133021, 2020.\nSun, Y., Gomez, F., and Schmidhuber, J. Planning to be surprised: Optimal Bayesian exploration in\ndynamic environments. In Schmidhuber, J., Th\u00f3risson, K. R., and Looks, M. (eds.), Artificial General\nIntelligence, pp. 41\u201351, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.\nTeam, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M.,\nHauth, A., et al. Gemini: a family of highly capable multimodal models, 2023.\nThompson, W. R. On the likelihood that one unknown probability exceeds another in view of the\nevidence of two samples. Biometrika, 25(3/4):285\u2013294, 1933.\nWu, H. and Liu, X. Double Thompson sampling for dueling bandits. In Lee, D., Sugiyama, M., Luxburg,\nU., Guyon, I., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 29.\nCurran Associates, Inc., 2016.\nYue, Y., Broder, J., Kleinberg, R., and Joachims, T. The \ud835\udc3e-armed dueling bandits problem. Journal of\nComputer and System Sciences, 78(5):1538\u20131556, 2012.\nZhang, W., Zhou, D., Li, L., and Gu, Q. Neural Thompson sampling. arXiv preprint arXiv:2010.00827,\n2020.\nZhou, D., Li, L., and Gu, Q. Neural contextual bandits with ucb-based exploration. In International\nConference on Machine Learning, pp. 11492\u201311502. PMLR, 2020.\n15\nEfficient Exploration for LLMs\nA. Human Preference Simulator\nWe use an oracle reward model to construct our preference simulator. Given a query, comprising a\nprompt and two potential responses, the preference simulator produces binary feedback indicating a\npreference between the two responses by first computing scores for each of the two responses. To\nsimulate preference probabilities from these scores, we employ the Bradley-Terry model Bradley &\nTerry (1952), a well-established methodology in decision analysis. While we make use of binary\nfeedback sampled from the preference probabilities in the training pipeline, we directly use the\npreference probabilities in the assessment pipeline. The direct use of preference probabilities in our\nassessment pipeline is to reduce stochasticity in evaluation.\nThe oracle reward model itself is constructed with a two-part architecture, featuring a torso\nresponsible for producing embeddings and a linear layer head that generates scalar estimates. The\ntorso is initialized to the pre-trained Gemini Pro transformer torso, while the linear head uses Xavier\ninitialization (Glorot & Bengio, 2010). The training process involves optimizing both the torso\nand reward head through cross-entropy loss function applied to the Anthropic Helpfulness and\nHarmlessness datasets (Bai et al., 2022).\nOur oracle reward model attains an accuracy of 0.755 on the Anthropic Helpfulness and 0.748\non the Anthropic Harmlessness eval datasets. Notably, this level of performance is higher than the\nperformance of the largest models reported in (Bai et al., 2022).\nNote that, since Gemini Pro is far larger than Gemini Nano, choices are made by a much more\ncomplex model than that available to the agent. This difference in scale is intended to reflect the fact\nthat humans may exhibit more complex behavior than that modeled by the agent.\nB. Hyperparameter Selection for Experiments\nWe tune the hyperparameters of our agent to optimize performance. These hyperparameters include\nthe l2 regularization strength, learning rate, and the number of stochastic gradient descent (SGD)\nsteps executed after each epoch of interaction. Every stochastic gradient descent (SGD) step in our\ntraining process is executed on a batch of data randomly sampled from the agent\u2019s replay buffer.\nWe sweep the learning rate over {1\ud835\udc52 \u2212 6, 1\ud835\udc52 \u2212 5, 1\ud835\udc52 \u2212 4} for both point estimate and ENN reward\nmodels and pick the best learning rate. We found that the best learning rate is consistent across both\nour joint nll experiments described in Section 5.3 and our active learning experiments.\nTo adapt to the evolving nature of the data collection process, we implement a strategy of decay\nfor the regularization strength. The regularization strength, denoted as \ud835\udf06 in Equations 1 and 3, is\nadjusted in accordance with the volume of data accumulated by the agent. Specifically, for each\nstochastic gradient descent (SGD) step carried out at every epoch on a batch comprising \ud835\udc35 = 32\npreference data points from the replay buffer, we incorporate a decayed regularization strength given\nby \ud835\udf06 = 32\u00d7\ud835\udf06\u2032\n| D| , where D represents the total number of feedback data points amassed by the agent,\nand we tune \ud835\udf06\u2032 by sweeping across {0.1, 1, 10, 100, 1000} for all the agents.\nWe also swept over the number of sgd steps performed after each epoch of interaction from\n{1, 10, 100}. We observed that infomax agent benefits from running for more sgd steps while the\nperformance of other agents deteriorates beyond a point due to over fitting.\nIn the case of ENN models, we also tune the output scale parameter, responsible for regulating\nthe diversity of ensemble particles. In specific, we sweep over values {0.1, 1, 10} and pick the value\nwhich leads to best performance per agent.\n16\nEfficient Exploration for LLMs\nFuthermore, we also tuned the number of hidden units for a two-layer MLP in point estimate\nmodel by sweeping over {128, 256, 512, 1024, 2048} in the context of our uncertainty estimation\nexperiments detailed in Section 5.3. Despite our thorough exploration, we observed no substantial\nenhancement in performance associated with an increase in hidden units. Consequently, we opted to\nemploy 128 hidden units for MLPs across all of our experimental results presented in this paper.\nC. Exploration Algorithms with a Single Point Estimate\nIn this section, we evaluate the performance of the Boltzmann algorithm across various temperature\nvalues. We vary the temperature parameter, denoted as \ud835\udf0f, in the Boltzmann exploration scheme\n(refer to Algorithm 4). The range of temperatures explored includes \ud835\udf0f \u2208 {1\ud835\udc52 \u2212 4, 1\ud835\udc52 \u2212 2, 1\ud835\udc52 \u2212\n1, 0, 1, 10, 100, 1000}. The corresponding performance of the Boltzmann agent under different \ud835\udf0f\nvalues is illustrated in Figure 9.Notably, we observe optimal performance for Boltzmann agents with\nsmaller temperatures. Additionally, our findings affirm expectations that Boltzmann agents with\nvery high temperatures exhibit performance akin to a passive agent.\nFigure 9 | This plot demonstrates the performance of Boltzmann agent across different temperatures.\nWe additionally experimented with a variant of the Boltzmann scheme known as Greedy-Boltzmann,\nas outlined in Algorithm 7. In the Greedy-Boltzmann approach, one response is chosen greedily,\nrelying on the reward model, while the selection of the other response follows the Boltzmann explo-\nration scheme. Conceptually, Greedy-Boltzmann can be viewed as a modification of Boltzmann\nwith two temperatures, denoted as \ud835\udf0f1 and \ud835\udf0f2, where \ud835\udf0f1 is fixed at 0, and \ud835\udf0f2 is subject to variation.\nAlgorithm 7 Greedy-Boltzmann\ninput: \ud835\udc65, \ud835\udf0b, \ud835\udc5f\nhyperparams: \ud835\udf0f, \ud835\udc41\n1: sample responses \u02dc\ud835\udc661, . . . , \u02dc\ud835\udc66\ud835\udc41 \u223c \ud835\udf0b(\u00b7|\ud835\udc65)\n2: select response \ud835\udc56 \u2208 arg max\ud835\udc5b \ud835\udc5f(\ud835\udc65, \ud835\udc66\ud835\udc5b)\n3: probs \ud835\udc5e\ud835\udc5b =\nexp(\ud835\udc5f(\ud835\udc65,\u02dc\ud835\udc66\ud835\udc5b)/\ud835\udf0f)\n\u00cd\ud835\udc41\n\ud835\udc5b\u2032=1,\ud835\udc5b\u2260\ud835\udc56 exp(\ud835\udc5f(\ud835\udc65,\u02dc\ud835\udc66\ud835\udc5b\u2032 )/\ud835\udf0f)\u2200\ud835\udc5b \u2260 \ud835\udc56, \ud835\udc5e\ud835\udc56 = 0\n4: sample \ud835\udc56\u2032 \u223c \ud835\udc5e\nreturn \ud835\udc66\ud835\udc56, \ud835\udc66\ud835\udc56\u2032\nThe performance of the Greedy-Boltzmann variant across different temperatures is illustrated in\n17\nEfficient Exploration for LLMs\nFigure 10. Notably, after fine-tuning the temperature parameter, the results indicate that Greedy-Boltzmann\ndoesn\u2019t improve over the performance achieved by the standard Boltzmann agent, as presented in\nAlgorithm 4.\nFigure 10 | Performance of Greedy-Boltzmann across different temperatures for Boltzmann. We\ncan see that best Greedy-Boltzmann and best Boltzmann agent perform very similarly, and Greedy-\nBoltzmann doesn\u2019t offer an advantage over Boltzmann.\nThe Boltzmann and Greedy-Boltzmann agents can be conceptualized as approximating various\nexploration strategies determined by the temperatures used in Algorithms 4 and 7. This encompasses\nexamples such as \"greedy\" exploration, involving the selection of the two best responses; \"greedy-\nuniform\" exploration, where the first response is chosen greedily and the second is randomly selected;\nand \"passive\" exploration, where both responses are sampled randomly. Therefore, when evaluating\nthe performance of Boltzmann and Greedy-Boltzmann, we are essentially assessing a broad\nspectrum of exploration schemes derived from a point estimate reward model.\n18\n"
  },
  {
    "title": "SymbolicAI: A framework for logic-based approaches combining generative models and solvers",
    "link": "https://arxiv.org/pdf/2402.00854.pdf",
    "upvote": "16",
    "text": "SymbolicAI: A framework for logic-based approaches\ncombining generative models and solvers\nMarius\u2013Constantin Dinu\u2217 \u2020\nClaudiu Leoveanu\u2013Condrei\u2021\nMarkus Holzleitner\u2020\nWerner Zellinger\u00a7\nSepp Hochreiter\u2020\nAbstract\nWe introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to\nconcept learning and flow management in generative processes. SymbolicAI enables the seamless\nintegration of generative models with a diverse range of solvers by treating large language models\n(LLMs) as semantic parsers that execute tasks based on both natural and formal language instruc-\ntions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic\nprogramming principles to tackle complex tasks, and utilize differentiable and classical program-\nming paradigms with their respective strengths. The framework introduces a set of polymorphic,\ncompositional, and self-referential operations for data stream manipulation, aligning LLM outputs\nwith user objectives. As a result, we can transition between the capabilities of various foundation\nmodels endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models\nor solvers proficient in addressing specific problems. In turn, the framework facilitates the creation\nand evaluation of explainable computational graphs. We conclude by introducing a quality measure\nand its empirical score for evaluating these computational graphs, and propose a benchmark that\ncompares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical\nscore as the \u201dVector Embedding for Relational Trajectory Evaluation through Cross-similarity\u201d, or\nVERTEX score for short. The framework codebase1 and benchmark2 are linked below.\nPrompting / Fine-Tuning\nNeuro-Symbolic AI Spectrum\nSoftware-Engineering\nMachine Learning\nFoundation Models\nSpecialist Models\nProgramming / Learning\nModeling / Coding\nAbstraction\nImplementation\nFigure 1: Our neuro-symbolic framework enables a seamless transition between classical and differentiable program-\nming, each with distinct dynamics and strengths. Differentiable programming provides access to foundational and\nspecialist models. Classical programming, on the other hand, shifts between abstraction and implementation, focusing\non high-level concepts before delving into the details of implementation.\n\u2217 ExtensityAI, Vienna and AI Austria, Vienna \u2014 Corresponding author emails: dinu@ml.jku.at, office@extensity.ai\n\u2020 ELLIS Unit Linz and LIT AI Lab, Institute for Machine Learning, Johannes Kepler University, Linz\n\u2021 Amazon Devices, Timis,oara \u2013 work done outside of Amazon\n\u00a7 Johann Radon Institute for Computational and Applied Mathematics, Austrian Academy of Sciences, Vienna\n1 SymbolicAI framework released on January 20th, 2023, on GitHub: https://github.com/ExtensityAI/symbolicai.\n2 Evaluation benchmark released on February 1st, 2024, on GitHub: https://github.com/ExtensityAI/benchmark.\n1\narXiv:2402.00854v2  [cs.LG]  5 Feb 2024\n1\nIntroduction\nThe recent surge in generative AI, particularly involving large language models (LLMs), has demonstrated their\nwide-ranging applicability across various domains (Badita, 2022; Degrave, 2022). These models have enhanced the\nfunctionality of tools for search-based interactions (YouWrite, 2022; Writesonic, 2022; Microsoft, 2023), program\nsynthesis (Jain et al., 2021; Romera-Paredes et al., 2023; Key et al., 2023), chat-based interactions (ReplikaAI, 2016;\nOpenAI, 2022; Google, 2023), and many more. Moreover, language-based approaches have facilitated connections\nbetween different modalities, enabling text-to-image (Ramesh et al., 2021; Saharia et al., 2022), text-to-video (Singer\net al., 2022), text-to-3D (Poole et al., 2022), text-to-audio (Oord et al., 2016; Wang et al., 2017), and text-to-code (Wang\net al., 2021b; Lu et al., 2021; Li et al., 2022b) transformations, to name a few. Therefore, by training on vast quantities\nof unlabelled textual data, LLMs have been shown to not only store factual knowledge (Petroni et al., 2019; Kassner\net al., 2020) and approximate users\u2019 intentions to some extent (Andreas, 2022), but also to unlock deep specialist\ncapabilities through innovative prompting techniques (Nori et al., 2023). Yet, these applications merely scratch the\nsurface of the transformation that language-based interactions are expected to bring to human-computer interactions\nin both the near and distant future.\nIn part, instruction-based fine-tuning of LLMs through reinforcement learning from human feedback (Ouyang et al.,\n2022; Li et al., 2023) or direct preference optimization (Rafailov et al., 2023) has shown promising results dealing\nwith value misalignment issues (Bradley Knox & Stone, 2008; MacGlashan et al., 2017; Christiano et al., 2017;\nIbarz et al., 2018; Goyal et al., 2022), unlocking new possibilities for chain of thoughts (Wei et al., 2022b), tree of\nthoughts (Yao et al., 2023a), and graph of thoughts interactions (Besta et al., 2023). However, recent research also\nhighlights the limitations of LLMs in functional linguistic competence despite their proficiency in formal linguistic\ncompetence (Mahowald et al., 2023). Whereas formal linguistic competence encompasses the ability to understand\nand generate language, functional linguistic competence pertains to the application of language in real-world contexts,\nsuch as conveying sensory input or recalling information from memory. Examples of functional linguistic competence\ninclude implicatures (Ruis et al., 2022) and contextual language comprehension beyond the statistical manifestation\nof data distributions (Bransford & Johnson, 1972). Consequently, operating LLMs through a purely inference-based\napproach confines their capabilities within their provided context window, severely limiting their horizon.\nThis\nresults in deficiencies for situational modeling, non-adaptability through contextual changes, and short-term problem-\nsolving, amongst other capabilities. However, simply increasing the context length may not yield greater capabilities, as\ndemonstrated by the observed U-shaped performance curve (Liu et al., 2023) where LLMs excel when using information\nat the beginning or end of the input context, but struggle with information located in the middle, especially as context\nincreases. These challenges are actively being researched, with novel approaches such as Hyena (Poli et al., 2023),\nRWKV (Bo, 2021), GateLoop (Katsch, 2023), and Mamba (Gu & Dao, 2023) surfacing. Meanwhile, the re-emergence\nof interest in retrieval-augmented generative approaches (Li et al., 2022a) offers an alternative by circumventing the\nautoregressive nature of the widely-used Transformer architecture (Vaswani et al., 2017), enabling context enrichment\nwith lateral information. In parallel, efforts have focused on developing tool-based approaches (Schick et al., 2023) or\ntemplate frameworks (Chase, 2023) to extend large LLMs\u2019 capabilities and enable a broader spectrum of applications.\nHowever, these efforts only partially capture the vast potential inherent in leveraging LLMs as semantic parsers.\nIn light of these considerations, we introduce SymbolicAI, a compositional neuro-symbolic (NeSy) framework able\nto represent and manipulate multi-modal and self-referential structures (Schmidhuber, 2007; Fernando et al., 2023).\nSymbolicAI augments the generative process of LLMs with functional zero- and few-shot learning operations and\nenables the creation of versatile applications through in-context learning (Wei et al., 2022a). These operations guide\nthe generative process and facilitate a modular design with a wide range of existing solvers, including formal language\nengines for mathematical expression evaluation, theorem provers, knowledge bases, and search engines for information\nretrieval.\nIt exposes these solvers as building blocks for constructing compositional functions as computational\ngraphs, and facilitates the development of an extensible toolkit that bridges classical and differentiable programming\nparadigms, aiming to create domain-invariant problem solvers. In designing the architecture of SymbolicAI, we drew\ninspiration from the body of evidence that suggests the human brain possesses a selective language processing module\n(Macsweeney, 2002; Fedorenko et al., 2010; Menenti et al., 2011; Regev et al., 2013; Scott et al., 2016; Deniz et al.,\n2019; Hu et al., 2022), prior research on cognitive architectures (Newell & Simon, 1956; Newell et al., 1957; Newell\n& Simon, 1972; Newell, 1990; Laird, 2022), and the significance of language on the structure of semantic maps in the\nhuman brain (Huth et al., 2016). We consider language as a central processing module, distinct from other cognitive\nprocesses such as reasoning or memory (Paischer et al., 2022, 2023), that defines a stepping stone towards broad AI\nsystems (see Section B).\nLastly, alongside the framework, we introduce a benchmark and derive a quality measure and its empirical score to\naddress the evaluation of multi-step NeSy generative processes. See Section 6 for more details.\n2\nIn summary, we highlight the following key contributions of this work:\n\u2022 We introduce SymbolicAI, a logic-based framework for concept learning and flow management in generative\nprocesses, enabling seamless integration with a wide range of foundation models and solvers.\n\u2022 We combine the broad applicability of LLMs as semantic parsers with symbolic expressions by leveraging a\nmodular probabilistic programming paradigm, facilitating the creation of complex computational graphs.\n\u2022 We introduce a quality measure and its empirical score alongside a benchmark for comparing state-of-the-art\nLLMs across a wide range of tasks.\n2\nRelated Work\nSymbolic Methods\nThe field of symbolic AI has its foundations in the works of the Logic Theorist (LT) (Newell\n& Simon, 1956) and the General Problem Solver (GPS) (Newell et al., 1957). These programs represented the first\nsteps towards automated reasoning and problem-solving using symbolic representations. Despite their advancements,\nboth faced challenges in dealing with the complexity of real-world problems, particularly due to the combinatorial\nnature of the solution space. To address these limitations, the Soar (Laird et al., 1987) cognitive architecture was\ndeveloped, advancing the notion that intelligent behavior results from goal-oriented search through a problem space\n(Newell & Simon, 1972; McCarthy et al., 2006), with each step consisting of selecting and applying operators. Soar\nintroduced components like reinforcement learning, impasses, substates, and chunking to enhance its problem-solving\ncapabilities. It also demonstrated the importance of learning from experiences to adapt and improve performance over\ntime. However, Santoro et al. (2022) emphasizes the subjectivity of symbols and suggests that human-like symbolic\nfluency could develop in machines through learning algorithms immersed in socio-cultural contexts. This perspective,\nanchored in the notion that symbols are triadic and their meaning emerges from consensus, seeks to move away\nfrom traditional symbolic AI methodologies towards AI that adaptively learns meaning and behaviors from human-like\nexperiences. The goal is to cultivate machines that demonstrate symbolic behaviors across a spectrum of competencies,\npotentially mirroring the evolutionary and social learning processes observed in humans. Lastly, symbolic AI struggles\nwith real-world data\u2019s unpredictability and variability. These challenges have led to the employment of statistical\nlearning methodologies, like deep learning (Alom et al., 2018), which are more adept at managing noise and uncertain\ninformation through vector-valued representations.\nSub-Symbolic Methods\nThe sub-symbolic framework, rooted in neural network paradigms, began with pioneering\nworks such as the perceptron (McCulloch & Pitts, 1943), with the first hardware implementation quickly following\n(Rosenblatt, 1958). The foundational notion of distributed processing (Rumelhart et al., 1986) was later bolstered\nand further expanded by demonstrating that multilayer feedforward networks with a single hidden layer can serve as\nuniversal approximators for any Borel measurable function, given sufficient hidden units (Hornik et al., 1989). Fast-\nforward, contemporary frameworks achieve a significant leap with the introduction of the Transformer architecture\n(Vaswani et al., 2017), which underpins most of today\u2019s LLMs. These LLMs demonstrate exceptional capabilities\nin in-context learning, a method popularized by the likes of GPT-3 (Brown et al., 2020), where models improve task\nperformance through natural language instruction and examples provided directly in the input prompt. While in-context\nlearning bypasses the need for explicit retraining, it demands meticulous prompt design to steer models towards desired\nbehaviors. Despite their versatility, current LLMs face challenges such as fallacious reasoning and the generation of\nerroneous content, commonly referred to as hallucinations (Jones & Steinhardt, 2022). These limitations highlight the\nimportance of integrating complementary symbolic methods to validate and guide the generative processes of LLMs,\nensuring more accurate and reliable outputs.\nNeuro-Symbolic Methods\nTo overcome the limitations of each individual method, NeSy approaches meld the\nstatistical inference strengths of deep neural architectures with the generalization and explainability of symbolic\nsystems (Besold et al., 2017; Yu et al., 2023; Hamilton et al., 2022; Garcez et al., 2015; d\u2019Avila Garcez et al., 2019;\nd\u2019Avila Garcez & Lamb, 2020; Lamb et al., 2020). Some approaches focus on different strategies for integrating\nlearning and reasoning processes (Yu et al., 2023; Fang et al., 2024). Firstly, learning for reasoning methods treat the\nlearning aspect as an accelerator for reasoning, in which deep neural networks are employed to reduce the search space\nfor symbolic systems (Qu & Tang, 2019; Silver et al., 2016, 2017b,a; Schrittwieser et al., 2020). Secondly, reasoning\nfor learning views reasoning as a way to regularize learning, in which symbolic knowledge acts as a guiding constraint\nthat oversees machine learning tasks (Hu et al., 2016; Xu et al., 2018). Thirdly, the learning-reasoning category\nenables a symbiotic relationship between learning and reasoning. Here, both elements interact and share information\nto boost problem-solving capabilities (Donadello et al., 2017; Manhaeve et al., 2018; Mao et al., 2019; Ellis, 2023).\nThis synergy further extends when considering graph-based methods, which closely align with the objectives of our\n3\nproposed framework. Research in this area, such as CycleGT (Guo et al., 2020) and Paper2vec (Ganguly & Pudi, 2017),\nexplored unsupervised techniques for bridging graph and text representations. Subsequently, graph embeddings, when\nutilized within symbolic frameworks, can enhance knowledge graph reasoning tasks (Zhang et al., 2021), or more\ngenerally, provide the bedrock for learning domain-invariant representations (Park et al., 2023). Lastly, building upon\nthe insights from Sun et al. (2022), the integration of NeSy techniques in scientific workflows promises significant\nacceleration in scientific discovery. While previous work has effectively identified opportunities and challenges, we\nhave taken a more ambitious approach by developing a comprehensive framework from the ground up to facilitate a\nwide range of NeSy integrations.\n3\nProblem Definition\nConventional approaches employing foundation models for inference, such as LLMs, are predominantly confined to\nsingle-step or few-step executions and primarily reliant on hand-crafted in-context learning prompt instructions. This\nrestricted scope limits the utilization to single modalities, lacks refinement or verification, and exhibits limited tool\nproficiency. We posit that the integration of NeSy engines as core computation units, realized through logic-based\nmethodologies coupled with sub-symbolic foundation models, offers a more general, robust, and verifiable perspective.\nThis approach has several advantages. Firstly, it facilitates the integration of pre-existing engineered solutions (e.g.\nvarious classical algorithms), offloading computational complexity and bridging various modalities.\nSecondly, it\nenables sub-symbolic generalization to focus on evidence-based decision-making (e.g. selecting the respective tool\nbased on in-context classification). Thirdly, it provides an interpretable language-based control layer for explainable,\nautonomous systems. Central to our solution is a method to define and measure the orchestration of interactions\nbetween symbolic and sub-symbolic systems, and the level at which instructions are formulated for effective control\nand task execution.\n4\nDesign Principles\nIn the following subsections, we elaborate on the key design principles underlying SymbolicAI and how we guide the\ngenerative processes of NeSy engines. For further technical details, see Section 5.\nSymbols and Expressions\nAs posited by Newell & Simon (1976), symbols are elemental carriers of meaning within\na computational context3. These symbols define physical patterns capable of composing complex structures, and are\ncentral to the design and interpretation of logic and knowledge representations (Augusto, 2022). Thus, SymbolicAI\nconceptualizes the notion that symbols, and the expressions they form, are reflections of the information inherent in\na system, and serve as surrogate for the interaction between the system and the problem space. Moreover, we argue\nthat real patterns, as Dennett (1991) speaks of, can be effectively realized through the use of symbols because these\nsymbols act as versatile abstractions that capture and represent the underlying structures and dynamics of these patterns,\nfacilitating their interpretation and manipulation in computational models.\nFurthermore, we attribute task-specific mappings to a language-centric strategy, leveraging their inherent semantics\nand abstraction to describe the states and properties of the problem at hand. These mappings are universal and may\nbe used to define scene descriptions, long-horizon planning, acoustic properties, emotional states, physical conditions,\netc. Therefore, we adhere to the analogy of language representing the convex hull of the knowledge of our society,\nutilizing it as a fundamental tool to define symbols. This approach allows us to map the complexities of the world\nonto language, where language itself serves as a comprehensive, yet abstract, framework encapsulating the diversity of\nthese symbols and their meanings. This perspective resonates with our innate human tendencies to attribute existing\nphysical objects with abstract concepts, as exemplified by our natural inclination to link tangible objects to colors and\nemotions, such as blending the color \u201dred\u201d with \u201dheart\u201d, \u201dwarm\u201d, and \u201dpassion\u201d.\nHowever, this language-centric model does not inherently encompass all forms of representation, such as sensory\ninputs and non-discrete elements, requiring the establishment of additional mappings to fully capture the breadth of the\nworld. This limitation is manageable, since we care to engage in operations within this abstract conceptual space, and\nthen define corresponding mappings back to the original problem space. These are typically applied through function\napproximation, as in typical modality-to-language and language-to-modality use cases, where modality is a placeholder\nfor various skill sets such as text, image, video, audio, motion, etc.\nUltimately, this approach also anchors our work in the field of formal language theory, as we require a structured\nmethod to construct mappings from the world to language. This grounding sets the foundation for employing formal\n3 We base our framework\u2019s name on the aspirational work of Newell and Simon.\n4\nlanguage structures, such as grammars, to systematically define our language-centric approach to problem-solving and\nthe associated translation of real-world complexities into linguistic terms.\nFormal Languages\nIn formal language theory and linguistics, languages are structured following the Chomsky\nhierarchy, which classifies languages by the complexity of their grammatical structure (Chomsky, 1956). This hierarchy,\ncomprising four types of grammars (Type-3 to Type-0), delineates formal languages by their grammatical complexity. A\ngrammar in this context consists of terminal and non-terminal symbols, production rules, and a designated start symbol,\nfacilitating the generation of valid strings within a language. In developing SymbolicAI, we posit that all symbols can\nbe represented as strings, augmented with conditional instructions and types derived from a domain-specific language\n(DSL) tailored for directing NeSy computation engines, like LLMs (see Figure 2).\nA key advancement of LLMs over previous systems lies in their ability to generalize from formal languages (Wang\net al., 2023a) and knowledge systems, primarily due to their world-knowledge and proficiency in understanding context-\nbased analogies. While there is currently no universal consensus among experts regarding the precise classification of\nnatural language within the Chomsky hierarchy, we have seen in our empirical evaluations promising results utilizing\nLLMs as semantic parsers. This approach can be viewed as employing a form of flexible, context-sensitive grammar,\nwhich enables the processing of instructions and analogies with a nuanced understanding of language\u2019s inherent\nvariability and complexity. The intersection between formal and natural languages becomes evident when considering\nhow language patterns, through prompts like \u201dYou are a helpful assistant...\u201d, elicit structured responses, indicating\na potential underlying formal mechanism at play. This observation underlines the utility of such a grammar in our\nframework, particularly within in-context learning, where it serves as an explicit schema guiding the structure of\nexamples used in few-shot learning scenarios. For instance, equating \u201d3.1415. . . \u201d with \u201d\u03c0\u201d or \u201dAugust 4, 1961\u201d with\n\u201d1961-08-04\u201d in a given context demonstrates this probabilistic, context-dependent interpretation of symbols. Such a\nsystem doesn\u2019t rigidly adhere to standard grammatical rules but instead adjusts and interprets based on the presented\ncontext, effectively creating a dynamic and situation-specific grammar.\nIn-Context Learning\nRecently, several in-context learning methodologies evolved to enable tool usage through\nLLMs (Schick et al., 2023), or refine the generative outcome of LLMs (Yang et al., 2023). This includes chain-of-\nthought (CoT) prompting, a method that conditions the model to reveal its step-by-step reasoning process (Wei et al.,\n2022b; Singhal et al., 2023). CoT prompting breaks down complex tasks into simpler, sequential steps, and helps with\ninterpreting LLM\u2019s output. Self-generated CoT, where models are encouraged to generate their own reasoning chains\nbased on training examples, surpasses even expertly crafted CoT (Fernando et al., 2023). This observation echoes other\nreports that GPT-4 has an emergent self-improving capability through introspection, such as self-verification (Weng\net al., 2023) or self-consistency (Wang et al., 2023b). Tree of Thoughts (ToT) enables LLMs to solve complex problems\nby exploring multiple reasoning paths through a search tree of coherent text units, demonstrating significant problem-\nsolving enhancements in tasks requiring strategic planning and search (Yao et al., 2023a). Ensemble techniques further\nenhance the robustness and accuracy of model predictions by combining several strategies to establish a consensus\n(Nori et al., 2023). Conceptually, we designed our framework to enable all these techniques and combine them into\ndedicated components and sub-processes. Figure 2 outlines a typical expression evaluation in our NeSy pipeline.\n'AC' << 'B'\nNeuro-Symbolic Engine\nDSL Prompt\nExpressions\n'ABC'\nSymbol\nFigure 2: Illustration for NeSy pipeline, showcasing conceptual usage of in-context learning methodologies, domain-\nspecific language (DSL) structures, and the expression evaluations through NeSy engines. The expression showcases\nthe left shift operator \u226a and how the information of the symbol B is included in the symbol AC. The violet placeholder\nin the DSL Prompt represents an instruction, such as \u201dInsert the right-hand side value into the left-hand value in a\nchronological order.\u201d The positions below represent task-specific few-shot examples.\nDomain-Invariant Associations\nIn-context learning enabled LLMs to become versatile task solvers by interpolating\nwithin the training distribution, to the extent that even potentially unseen tasks are addressable (Brown et al., 2020). We\nattribute this to associations formed within the input space and the capacity of Transformer architectures for defining\ndomain-invariant feature sub-spaces. This phenomenon has striking parallels with advancements in few-shot learning\napproaches such as SubGD (Gauch et al., 2022), a method based on identifying and utilizing a low-dimensional\n5\nsubspace, learned from various tasks, that effectively acts to regularize the learning process, representing features\nthat are invariant across different learning tasks. Furthermore, SubGD reflects the potential of in-context learning\nwhen combined with task-specific fine-tuning by showing that fine-tuning within a learned subspace significantly\noutperforms traditional fine-tuning methods. We believe that the extent of in-context learning is not yet exhausted,\nholding considerable promise when used alongside with task-specific fine-tuning and solvers. To develop learning and\nreasoning systems capable of general problem-solving, we adopt a hybrid methodology. This approach leverages the\nin-context generalization capability of LLMs, constructing symbolic associations that aim to preserve and propagate\nsituational context, and validating solutions with established solvers.\nFunction Composition\nIn SymbolicAI, function composition is relevant for constructing complex hierarchies and\nbehaviors from more basic, fundamental elements. It enables our framework to model interconnected processes,\nwhere the output of one function seamlessly transitions into the input of another, thus creating a cohesive sequence\nof operations. Through function composition, we construct computational graphs, in which intermediate symbols\nrepresent the nodes or states within these graphs. Formally, function composition is denoted by \u25e6, where combining\nfunctions f and g yields a new function h = g \u25e6 f, defined as h(x) = g(f(x)) For functions f : X \u2192 Y and\ng : Y \u2192 Z, their composition results in a function mapping elements from domain X to codomain Z through g(f(x)).\nAlthough traditionally the codomain of the inner function f aligns with the domain of the outer function g, SymbolicAI\nrelaxes this constraint by allowing for any subset relationship between these domains and codomains, enhancing data\nflow flexibility. For example, this relaxed constraint in domain and codomain alignment is particularly beneficial for\nin-context learning. By leveraging functional few-shot learning, where few-shot examples act as dynamic elements of\nthe function\u2019s domain, SymbolicAI enhances its ability to interpret and respond to diverse input contexts. For instance,\na function can classify a user request and select an appropriate engine to process the request. The target modality may\nvary based on the respective engine. Therefore, in SymbolicAI, the use of function composition leads to the creation of\nricher and more nuanced symbol-based generative flows, where each functional unit is capable of representing either\nlogical or data-driven behaviors. Importantly, function composition is not confined to strictly symbolic representations;\nit also connects with sub-symbolic processes. This enables SymbolicAI to handle complex data streams, establishing\nfunction composition as a central tenet in bridging multiple modalities and coordinating a variety of tasks.\n5\nFramework\nIn this section, we discuss the design choices pertaining to implementation and the specifics of the framework we\u2019ve\nemployed. For an extended overview, see our supplementary materials Section C.\nTypes and Representations\nAnalogous to the Python object type, the base type of SymbolicAI is a symbol\nrepresented through its name equivalent base type Symbol. A Symbol object marks a non-reducible atomic unit. All\nother subtypes, such as Expression and its derivatives, are analogous to their mathematical namesakes, representing\nexpressions or units that can be further evaluated and simplified.\nThese subtypes inherit from Symbol the base\nattributes, primitive operators, and helper methods. Furthermore, each Symbol object contains valued and vector-\nvalued representations, obtained through value and embedding attributes. The latter, in particular, serve as a means\nto attribute a symbol\u2019s current context, akin to embedding text and storing it as a PyTorch tensor (Paszke et al., 2019) or\nNumPy array (Harris et al., 2020). While for an LLM, the numerical tensors may lack inherent meaning, vector-valued\nrepresentations play a strategic role when 1) composite symbols coalesce into more complex expressions, and 2) these\nembedded tensors become amenable to updates through gradient-based optimization. This design choice is critical in\nthe dynamic landscape of symbolic interactions, particularly for promising applications, such as developing automated\nand self-evolving NeSy systems.\nTo enable the processing of symbols by LLMs, we assume that each Symbol object is represented through Python\u2019s\nnative string functionality, where the\nstr\nmethod acts an interpretable string representation. Worth noting is that\nencoding a complex object into a string sometimes precludes the object reconstitution. However, this concern does\nnot substantially impede our methodology: we can employ approximations or proxy representations stored by the\nvector-valued property to effectively re-map objects. These representations are obtained through respective embedding\nmodels. Therefore, we can theoretically assert that any Python object is parsable by an LLM by design. For more\ndetails, see our supplementary materials Section E.\nPolymorphic Context\nPolymorphism is a central concept in programming language theory and prominently featured\nin SymbolicAI, with significant implications for the design and functionality of our NeSy architecture. Polymorphism\nrefers to the ability of different objects to be accessed through the same interface, or of a single identifier to represent\ndifferent types based on the context of execution. The provision of a single interface to entities of different types\n6\nallows operations to be performed in ways specific to their derived types. We employ a polymorphic structure for\ninstruction composition. We designed the Symbol object to contain a global context, which is composed of static and\ndynamic context parts. The static context is class dependent and defined at design time. The dynamic context is runtime\nadaptable and can be changed to adhere to runtime specific logic and changes. Moreover, Symbol associated operations\nresolve in a polymorphic manner before being evaluated by the NeSy engines. SymbolicAI\u2019s engine implementation\ncontains a prepare method to resolve and compile the engine specific representation by evaluating the Symbol-specific\noperations and context. For example, when utilizing GPT-4 vision as a NeSy engine backend, we compose the system\nand user level prompts in the prepare statement and resolve image or video related URL queries by parsing meta tags\nsuch as the <<vision: ... :>> tag using regular expressions.\nOperators and Methods\nIn SymbolicAI, operators are overloaded to facilitate transformations of Symbol objects.\nThese operator primitives employ dynamic casting to assure type compatibility, simplifying declarations. Consequently,\nSymbol objects can be easily manipulated through type specific attributions or symbolically evaluated by the NeSy\nengine. For example, a central operation for boolean logic is measuring equality between symbols. To evaluate the\nequality of symbols, we primarily adhere to the type specific implementation, because we prioritize strict comparisons\nover probabilistic evaluations. If the evaluation was unsuccessful, we then consider semantic equality through the NeSy\nengine. SymbolicAI leverages decorators for compose operators and custom class methods. For more details, see\nsupplementary materials Section C. Upon invoking an operator or method, the respective primitive function evaluates\nthe symbol\u2019s specific type and its respective attributes, and if further necessary, resolves a nested decorated function\nthat then references the NeSy engine for an evaluation. Should the evaluation fail, a predefined fallback implementation\nexecutes. Absent a fallback, or if both evaluations fail, an error state is raised. The processing of an operator or custom\nmethod involves a pipeline consisting of pre- and post-processing steps, as well as constraint enforcement. Constraints\ncover aspects like return types, value ranges, and structural integrity (e.g. JSON formatting through grammar-based\nverification). In Figure 3 we give an overview of the entire prompt composition based on the user input, the Symbol\nobject structure, and the NeSy engine evaluation pipeline.\nOperation\n[ Examples ]\nPrompt Design\n[ Template ]\n< Prediction Starting Point >\n[ Static Context ]\nCustom Method\n@decorator\nPreProcessor\nEngine\nPostProcessor\nConstraints\nInput\nOutput\nPrompt\nUser Input\n[ Attachment ]\n[ Dynamic Context ]\nSymbol Object\n    Attribute: [ Static Context ]\n    Attribute: [ Dynamic Context ]\nCustom Method\n      Field: [ Examples ]\n      Field: [ Template ]\nUser Input Args\n    [ Attachment ]\n      Field: [ Operation ]\n+\nFigure 3: We illustrate our NeSy engine pipeline and how we map user input and object instance attributes to a prompt\ninstruction. Yellow marks user input, blue marks templating and transformations, red marks constraints, and violet\nthe model prediction placeholder. The user input and Symbol instance attribute values are evaluated and transformed\naccording to the pre- and post-processing steps. A custom method is constructed by using our reference implementation\nof decorators and passes the user input and instance attributes to the respective engine. A pre-processor manipulates the\nstring representation of user input and Symbol instance attributes before forwarding them to the engine. For instance,\nin case the NeSy engine is an LLM (i.e. GPT-4), the static and dynamic context of a Symbol instance are mapped\nto the system and user prompts for that respective LLM using the prepare method of the engine. Other attributes,\nsuch as examples or templates, can help pre-define structures for the desired LLM output format. After the result is\ncomputed by the engine, it is passed to the post-processors and validated against the constraints.\nSelf-Referential Structures\nSymbolicAI augments the generative process by enabling systems to introspect and\nmodify their behavior dynamically. We leverage LLMs to execute tasks based on both natural and formal language\ninstructions, adhering to the specified user objectives and with innate self-referential structures. We derive subtypes\nfrom Expression and enclose their functionalities in task-specific components, which we then expose again through\ntemplating and the model-driven design of the NeSy engine. This design choice allows a system to create and utilize\nits own sub-process definitions, analogous to concepts discussed in Schmidhuber (2007, 2009). Concretely, we utilize\ngeneralization properties from LLMs to interpret and formulate a set of operations that incorporate self-instructions\n(Wang et al., 2022). Consequently, the operations hold the flexibility to adapt to the context, and derive sub-processes\nthat self-instruct LLMs to engage in situational modeling and context-sensitive problem-solving. Ultimately, this\n7\nenables the construction of hierarchical computational graphs for self-referential meta-reasoning systems without the\nneed to explicitly training a meta-learner (Kirsch & Schmidhuber, 2022).\n6\nEvaluation\nIn this section, we conclude our paper with an empirical evaluation and potential areas of application using state-of-\nthe-art LLMs as NeSy engine backends. For our evaluation we focus on the GPT family of models GPT-3.5 Turbo\n(revision 1106) and GPT-4 Turbo (revision 1106) (Brown et al., 2020) as they are the most proficient models to this\ndate; Gemini-Pro as the best performing model available through API from Google; LlaMA 2 13B as it defines a good\nreference implementation for available open-source LLMs from Meta; Mistral 7B and Zephyr 7B as good baselines\nfor revised and fine-tuned open-source contestants respectively. The selected open-source models Mistral, Zephyr,\nand LlaMA 2 are expected to have roughly equivalent parameter counts compared to GPT-3.5 Turbo and Gemini-Pro.\nAll our experiments are expected to require a context size smaller or equal to 4096 to enable the comparisons among\nthe in-context capabilities across model architectures. For LlaMA 2 we use the chat version since it better follows\ninstructions. Our evaluation focuses on three main parts. First, we evaluate basic capabilities of the models to verify\ntheir consistency and proficiency with in-context learning tasks. Second, we evaluate the logic capability of models\nby defining logical expressions in mathematical and natural language form and measure the capability of the models\nto translate and evaluate logical statements across domains. Lastly, we evaluate the capability of models in planning,\nconstructing, maintaining, and executing hierarchical computational graphs. The models are instructed to extract an\nexecution plan, which they then must schedule and execute. A significant challenge is the evaluation of the semantics\nof a multi-step generative process, since the na\u00a8\u0131ve assessment of task succession would score all models to zero and\nrender them as unusable. Therefore, we derive a quality measure to address these issues.\nPerformance Measure\nOne of the challenges in model evaluation is handling irrelevant predictions from models.\nEven if models follow instructions and produce parts of the expected solution, we regularly observe that they \u2014\nespecially open-source models \u2014 append a continuation of task irrelevant predictions. Such predictions result in failure\nmodes when applying conditions and validations, and halt any multi-step procedure. Our solution is an evaluation\nprotocol that refines the performance measurement, allowing for more nuanced diagnostics and the possibility of\ncontinuing the evaluation despite intermediate failures. To derive our quality measure, we borrow ideas from the\nutilization of the Fr\u00b4echet distance for generative processes (Heusel et al., 2017).\nWe generate trajectories through a NeSy sequential process that creates a trajectory of distributions P over multiple\niterations of generative nodes. Each node in the process can be aligned to a reference distribution, which marks the\ndesired behavior. To quantify the validity of the generated trajectories, we measure the total distance between the\ngenerated and reference data distribution along the path trajectory. We therefore adopt a cumulative measure capable\nof taking into account the entire generative trajectory. In theory, this process would entail calculating the path integral\nover the latent space representations for models, cumulating the Fr\u00b4echet distances (Dowson & Landau, 1982) traversed\nalong these distributional trajectories:\nD(Pgen, Pref) =\nZ tf\nt0\nd(N(mt, Ct), N(mw,t, Cw,t)) dt\n(1)\nwhere D(Pgen, Pref) denotes the integral of the Fr\u00b4echet distances between two data distributions along the generative\npath trajectory from an initial time t0 to a final time tf, d(N(mt, Ct), N(mw,t, Cw,t)) is the Fr\u00b4echet distance calculated\nat each time t between the generated multivariate normal data distribution with mean mt and covariance Ct, and the\nreference multivariate normal data distribution with mean mw,t and covariance Cw,t. The resulting measure follows\nproperties of normal distributions and is consistent with increasing disturbances.\nHowever, this approach is computationally intractable for large-scale problems, and requires access to latent represen-\ntations, which \u2014 especially in the context of LLMs \u2014 is not always given. For computational feasibility, we therefore\nintroduce an approximation that measures the embedding distances over the path trajectories utilizing an auxiliary\nembedding model, based on prior work on distribution regression (Szab\u00b4o et al., 2016). The embedding model maps\nthe symbolic representations into a RKHS, such that we can apply a kernel mean embedding function to measure their\nrespective distances (You et al., 2019; Dinu et al., 2023). We assess the distance through the mean embeddings w.r.t.\nto a kernel function K(\u00b7, \u00b7) of the samples et\nx \u223c \u03bdt\ngen \u2208 Pgen and et\ny \u223c \u03bdt\nref \u2208 Pref produced by the generated data\ndistribution and a reference data distribution respectively. We denote by \u00b5etx, \u00b5ety the mean embeddings associated to\nthe respective samples, i.e. \u00b5etx(z) = 1\nn\nPn\ni=1 K(xt\ni, z) in case et\nx = (xt\ni)n\ni=1 is a sample of size n of the respective\nmean embeddings. To compute the similarity between the embeddings of the generated and reference distributions,\n8\nwe evaluate the associated maximum mean discrepancy MMD2(\u00b5etx, \u00b5ety) (Gretton et al., 2012) and then, as before\nfor the Fr\u00b4echet distances, we integrate over t:\n\u02dcD(Pgen, Pref) =\nZ tf\nt0\nMMD2(\u00b5etx, \u00b5ety)dt.\n(2)\nIn empirical evaluations, however, we care about normalized values for ease of interpretation. We therefore analyze\nthe properties of the MMD and derive a similarity score, which follows the same statistical principles as the MMD,\nand is bound between [0, 1]. We concluded that we can use only the MMD cross terms to evaluate the similarities. See\nour supplementary materials Section A for more details. For our comparisons as referenced in Figure 5 we therefore\ndenote the similarities rather than distances. We then come to the following formulation and refer to our empirical\nmeasure as the \u201dVector Embedding for Relational Trajectory Evaluation through Cross-similarity\u201d, or VERTEX score\nfor short:\ns(Pgen, Pref) :=\nZ tf\nt0\n\u0002\nmin(max(0, 1\nz\n^\nMMD2(\u00b5etx, \u00b5ety) \u2212 zrand), 1)\n\u0003\ndt.\n(3)\nWe approximate the integral across time steps through Monte Carlo approximation. The introduced normalization\nconstants denote the similarities to a random sequence zrand, which functions as a baseline subtraction to recenter\nour results, and a given reference score to rescale w.r.t. to scores obtained from comparing related solutions z. The\nuse of the min-max scaling ensures the final measure is bounded between [0, 1]. This process reflects properties\nsuch as H\u00a8older continuity that bounds the kernel function within certain limits, ensuring that the similarity measure\nis appropriately scaled. To compute the embeddings, we utilize the embedding model all-mpnet-base-v2 (Song\net al., 2020), due to its widespread availability, and its balance between speed and quality. As a similarity measure, we\nselect a Gaussian kernel following our derivation from the supplementary materials Section A. In our supplementary\nimplementations, we also explore other kernels, including some preliminary experiments with cosine similarity. We\nalso note that one can integrate Bernoulli distributed trials into our score, with 0 values representing failure modes\nand values of 1 being successes. Furthermore, if we relax our definition, we can integrate other similarity measures\nwhich are bound between [0, 1], which then reflect on domain-specific attributions, i.e. including a similarity measure\ntailored towards capturing the nuances between two sub-structures of abstract syntax tree.\nIn our evaluation of Figure 5 we conclude with the cumulative score, for the following base performance criteria.\nBasic Capabilities\nWe analyze the consistency and stability of models and establish baselines by addressing the\nfollowing performance criteria:\n\u2022 Associative Prediction: We evaluate the success rate of models to follow simple and complex instructions\nand associations with zero- and few-shot examples. We therefore address the proficient use of our operators\nbetween Symbol types.\n\u2022 Multi-modal Binding: We perform data transformations between multiple modalities by binding through\nlanguage-based representations, and evaluate their proficiency in tool utilization, classification and routing of\nrequests to relevant modules.\n\u2022 Program Synthesis: We evaluate executable code with and without including concepts from retrieval aug-\nmented generation, model-driven development, such as templating to direct the generative flow, and experi-\nment with self-generated instructions by creating self-referential expressions. We not only address similarity\nbetween produced solutions, but also include the measurement of abstract syntax tree equivalence and the\nsuccess of executions.\nLogical Components\nTo evaluation the logic capabilities of models, we conditioned them to create a sequence of\nexpressions as self-contained components, and refer to higher-order predicate logic for their assessment. Based on\nthe underlying type theory originating from Whitehead & Russell (1925\u20131927), we evaluate a model\u2019s capability in\nthe form of there exists x such that x fulfills y. Such quantifiers define the standard semantics over expressions, where\nthe meaning is given by a semantic function. A semantic function maps a term from an abstract definition to a point\nin a domain, which is an interpretation of the term\u2019s type and value. Therefore, these functions operate on types and\nvalues of expressions, and relations thereof. Subsequently, NeSy engines can formulate and evaluate at inference time\nlogic-based instructions through Lisp, Prolog, or Mathematica (McCarthy, 1959; Colmerauer & Roussel, 1993; Chen\net al., 1993; Inc., 2022), or leverage SAT solvers such as Z3 (Moura & Bj\u00f8rner, 2008). Therefore, the evaluation of a\n9\nnatural language statement can be interpreted by any expert system which defines the corresponding semantic functions\nand process them either in a symbolic (Feigenbaum et al., 1965; Gamble et al., 1994), differentiable (Veli\u02c7ckovi\u00b4c &\nBlundell, 2021; Ibarz et al., 2022), or hybrid manner (Kuncicky et al., 1991).\nWe evaluate how proficient models interpret custom DSLs and define expression statements. The DSL is designed to\nexpress logical relations and operations in a structured format, and supports human-readable and machine-interpretable\nformulations. The following example illustrates such relationships by translating a natural language statement into an\nexpression statement, as follows:\nMarvins has four paws and likes to meow when I pet its fur. Is Marvins a cat?\nA DSL may enforce the usage of HAS(\u00b7), IS(\u00b7), etc. and may condition a LLM to produce the following expressions:\n\u2022 HasFourPaws(x): x has four paws.\n\u2022 LikesToMeowWhenPetted(x): x likes to meow when it is petted.\n\u2022 IsCat(x): x is a cat.\nThese are then used to define the following logical expression:\n\u2200x\n\u0000HasFourPaws(x) \u2227 LikesToMeowWhenPetted(x) \u21d2 IsCat(x)\n\u0001\n.\nAn automated theorem prover can now evaluate this statement for all x values and assess the validity of the original\nquery. Lastly, our evaluation uses symbolic mathematics to manipulate algebraic expressions. This involves defining\nsymbols and performing operations like factorization, simplification, and algebraic manipulation. The symbols are\nplaceholders for any value, enabling the definition of general expressions without specifying their values upfront.\nHierarchical Computational Graphs\nWe evaluate the capabilities of models to orchestrate a multi-step generative\nprocess and evaluate a set of tasks. Models need to direct sub-processes and associate computational results from and\nto Symbol nodes, and maintain relationships between these nodes. Given that the field is currently at an early stage\nin developing even sequential schedulers for LLM-based planning systems, our initial evaluations will be confined to\nsequential execution only. In Algorithm 1, we detail our evaluation protocol, which is designed not only to analyze\nand score a series of instructions but also to provide a structured basis for recording these processes. We also note\nthat our evaluation protocol is generally formulated, which allows the application of non-sequential plan schedulers.\nIn Figure 4 we illustrate a step-wise evaluation of the contextual computational graph, in which the NeSy engine is\nprocessing conditioned on the current execution context and producing the next symbol prediction.\n...\n...\nSymbol 1\nSymbol 2\nTape Symbol 3\nNodes\n1\n2\n3\n3\nOperation\nNeuro-Symbolic Engine\n... Sequential Processing ...\nContext\nContext\nSymbol 3\nPrediction\nFigure 4: We showcase a multi-step hierarchical computational graph, with each node in the graph represented by\na symbol. The edges are relations between symbols. The left-hand side illustrates how a new node (Symbol 3) is\nobtained by evaluating an operation with its respective context on a NeSy engine. The right-hand side illustrates the\ncontext information window (yellow rectangle) and relationship of the resulting graph with its respective nodes.\n7\nDiscussion\nIn this section, we address the limitations of SymbolicAI and the future directions we are focusing on. Some of\nthe limitations stem from the inherent constraints of current technologies and dependencies on third-party systems.\nAdditionally, the nuanced complexities of working with generative models presents further challenges.\n10\nAlgorithm 1 VERTEX Protocol\nRequire: NeSy engine: V : S \u2192 S, where S = S\nn\u22650 Ln is the set of all possible strings formed by concatenating\ncharacters from a character set L, embedding engine E : S \u2192 H \u2282 Rd, symbols {x0, x\u2217, y\u2217} \u2282 S, with x0 as the\ninitial instruction, x\u2217 as the payload resulted from executing V, y\u2217 as the reference, and \u2217 acting as a placeholder\nfor P, T , C, capabilities C = {F1, F2, F3, . . .}, where each Fi represents a specific functional role within the\nsystem, plan P \u2282 S, task T \u2208 P, memory buffer M \u2282 S, a scoring function \u02dcs : H \u00d7 H \u2192 [0, 1], a scheduler Q,\nan aggregator A, and score variables {s} \u2208 [0, 1].\nMethod:\n1: V, E, Q, C, yP \u2190 Init(\u00b7)\n\u25b7 Initialize the engines, the scheduler, the capabilities, and expected plan.\n2: M \u2190 \u2205, A \u2190 \u2205\n\u25b7 Initialize the memory buffer and aggregator.\n3: xP \u2190 GeneratePlan(x0, V)\n\u25b7 Generate a plan starting from the initial instruction.\n4: Evaluate(xP, yP, E, A, \u02dcs)\n\u25b7 Embed, score, and aggregate plan similarity.\n5: P, M \u2190 UnfoldPlan(yP, M, Q)\n\u25b7 Unfold the plan into actionable tasks and update progress.\n6: while P \u0338= \u2205 do\n\u25b7 Run until all tasks have been dequed.\n7:\nT , yC, yT \u2190 Schedule(M, Q)\n\u25b7 Select the next task according to the scheduler and progression.\n8:\nFi \u2190 Identify(T , C, V)\n\u25b7 Identify the task-related capability Fi using V.\n9:\nxC, xT \u2190 Execute(T , Fi)\n\u25b7 Execute T using capability Fi and assign the results xC, xT .\n10:\nEvaluate(xC, yC, xT , yT , E, A, \u02dcs)\n\u25b7 Embed, score, and aggregate capability similarity.\n11:\nP, M \u2190 Update(T , P, M, A)\n\u25b7 Update the plan and task progression.\n12: end while\n13: s \u2190 Finalize(A)\n\u25b7 Finalize aggregation of scores.\n14: return s\n\u25b7 Return the aggregated score of the plan execution.\nAlgorithm 1: This algorithm demonstrates the implementation of our Vector Embedding for Relational Trajectory\nEvaluation through Cross-similarity (VERTEX) protocol. We start by initializing the NeSy engine V, the embedding\nengine E, the scheduler Q, and a set of capabilities C. The initial instruction x0 is used to generate a plan xP through\nV. The plan and its expected outcome yP are embedded, and their similarity is scored and aggregated. The plan is\nthen unfolded into actionable tasks. Each task T is selected and executed using the appropriate capability C, resulting\nin the capability and task results xC, xT , and expected outcomes yC, yT updated in the memory buffer M. The\nprocess continues, with each task\u2019s result being embedded, scored, and aggregated until the plan is complete. The final\naggregated score s is returned, reflecting the overall effectiveness of the plan execution.\n7.1\nLimitation\nEmbedding Measure\nOur empirical measure is limited by the expressiveness of the embedding model and how well\nit captures the nuances in similarities between two representations. Furthermore, the obtained similarity scores are\nhighly non-linear and difficult to interpret. For instance, two representations may address the same topic, such as the\nproblem description and its respective solution, however, when measuring their similarity we obtain similarity scores\nof \u223c 70%. We normalize this by subtracting an inherent baseline and randomness effect, however, to ensure a more\nholistic and robust measurement we would need a significantly larger amount of baselines and experiments. Since\nwe were very limited in the availability of development resources, and some presented models are only addressable\nthrough costly API walls. We are actively seeking investors and supporters to scale our solution and offer a more\ncompelling benchmark suite in the future.\nModel Capabilities\nAn obvious limitation revolves around the fixed context window size of the underlying language\nmodels. Despite the expansion of the context window in newer models such as GPT-4, the finite context still restricts\nthe amount of data that can be processed in a single pass. All information outside the context needs to be added\nthrough information retrieval approaches, which come with their own challenges and limitations (Gao et al., 2023).\nThis leads to side effects, including hallucination, given the model does not contain the necessary information to answer\nthe prompted instruction, which makes it difficult to maintain long-term statefulness for complex reasoning tasks and\ncomputational graphs.\nError Handling\nThe complexity of error handling when evaluating complex expressions through function composi-\ntionality, especially between multiple modalities and different solvers, is another notable challenge. While SymbolicAI\nintroduces mechanisms for error analysis and automated correction, these approaches are not infallible. They are often\nlimited by the quality and expressiveness of the models, and the model\u2019s capacity to understand deeply nested logical\n11\nBenchmarks\nGPT-4\nGPT-3.5\nGemini-Pro\nLlaMA 2\nMistral\nZephyr\nRandom\nAssociations\n0.93\n0.61\n1.00\n0.41\n0.49\n0.28\n0.01\nModality\n0.83\n0.77\n0.75\n0.47\n0.39\n0.43\n0.07\nCode\n0.78\n0.58\n0.53\n0.10\n0.19\n0.13\n0.00\nLogic\n0.41\n0.44\n0.39\n0.44\n0.46\n0.43\n0.00\nGraphs\n0.36\n0.31\n0.21\n0.05\n0.06\n0.08\n0.00\nTotal\n0.66\n0.54\n0.58\n0.30\n0.32\n0.27\n0.02\nAssociations\nModality\nCode\nLogic\nGraphs\n0.2\n0.4\n0.6\n0.8\n1.0\nGPT-4\nGPT-3.5\nGemini-Pro\nLlaMA 2\nMistral\nZephyr\nRandom\nFigure 5: We evaluate GPT-3.5 Turbo (GPT-3.5), GPT-4 Turbo revision 1106 (GPT-4), Gemini-Pro, LLaMA2-Chat\n13B, Mistral 7B and Zephyr 7B on five benchmark categories: 1) Associative Prediction (Association) 2) Multi-\nmodal Binding (Modality) 3) Program Synthesis (Code) 4) Functional Logic Components (Logic) and 5) Hierarchical\nComputational Graphs (Graphs). We denote the similarity score such that higher is better. The similarity score is\nmeasured according to a reference baseline and normalized to exclude noise.\nconstructs. We also note that for our evaluations, we disabled any remedy protocol, such as truncating prompts or retry\nschema.\nGeneralization\nThis research is also limited by current LLM\u2019s capacity for reasoning and generalization. Although\nprogress has been made, models are still prone to hallucinations and reasoning errors, especially when dealing with\nabstract, novel, or highly complex problem statements (Marcus, 2020). Furthermore, our framework\u2019s reliance on the\nmodel\u2019s ability to grasp the semantics of operations can be influenced by the training data and the model\u2019s innate biases\nand conceptual understanding (Mahowald et al., 2023). We also point out that the initial development of SymbolicAI\nstarted with the GPT family of models, and we may encounter innate biases in prompt design and expressiveness when\nusing other reference models. However, we also point out that prompt engineering instruction-based statements is not a\nreliable direction for improvement. We instead advocate for enhancing the resilience of models through fault tolerance,\nfocusing on their ability to better follow semantic instructions, not syntactic idiosyncrasies. Another concern is how\nto assess the disentanglement of evaluations of models on downstream tasks, to avoid testing on training samples,\nespecially for closed-source solutions like GPT.\nInterpretability and Transparency\nFinally, the issue of explainability and transparency in AI systems remains\nchallenging. While SymbolicAI makes steps towards making computational processes more explicit and explainable\nthrough symbolic manipulations, understanding the internal logic and decision-making of LLMs remains an open\nproblem. This can hinder trust and adoption in sensitive applications where interpretability of predictions is important.\n7.2\nFuture Work\nThe goal for Algorithm 1 is to be utilized by an advanced learning agent. This agent, employing reinforcement learning\nmethodologies (Ouyang et al., 2022; Li et al., 2023; Rafailov et al., 2023), could leverage our evaluation measure\nin existing benchmarks (Milani et al., 2020; Swazinna et al., 2022; Schweighofer et al., 2022) as a means to obtain\nreward signals to addresses a central problem in reinforcement learning, namely credit assignment (Sutton, 1984;\nArjona-Medina et al., 2019; Holzleitner et al., 2020; Patil et al., 2020; Widrich et al., 2021; Dinu et al., 2022). Over\ntime, it aims to develop the ability to autonomously generate its own plans, efficiently schedule tasks and subtasks, and\ncarefully select the most suitable tools for each task. Our protocol lays the groundwork for this agent to learn and expand\nits base set of capabilities (Amaro et al., 2023), moving towards more sophisticated, self-referential orchestration of\nmulti-step tasks. We\u2019ve already noticed that research is shifting towards this type of methodology (Yuan et al., 2024).\nFurthermore, in Section 6 we\u2019ve only considered a sequential scheduler. However, our objective is to ultimately assess\na non-sequential task execution model, allowing for dynamic insertion and out-of-sequence task execution. In addition,\n12\nwe are interested into exploring similarities of our work with Generative Flow Networks (Bengio et al., 2021a,b; Lahlou\net al., 2023).\n7.3\nBroader Impact\nWith LLMs becoming more and more accessible, progress recently made possible by the vast open source contributions\nfrom K\u00a8opf et al. (2023); Touvron et al. (2023); Taori et al. (2023); Xu et al. (2023); Geng et al. (2023); Biderman\net al. (2023), embedded accelerators for LLMs \u2013 or more generally NeSY engines \u2013 will be ubiquitous in future\ncomputation platforms, such as wearables, smartphones, tablets, consoles, or notebooks. Although current execution\ncycles are slow and error-prone, we expect to see further performance gains through improved operating system\nlevel optimizations, dedicated GPU-centric hardware refinement, and improved software interoperability. We believe\nthat modern programming paradigms should natively support probabilistic concepts and provide a boilerplate-free\nset of features for constructing and evaluating generative computational graphs.\nThis includes but is not limited\nto compositional, parallelizable, and simulation-based executions with polymorphic and self-referential structures.\nCurrent programming languages often have disjointed or makeshift solutions for these concepts in the context of\ngenerative processes. We believe integral probabilistic support for these concepts into modern software and hardware\nwill unlock new programming paradigms that can fully take advantage of generative architectures. We hope the\ncommunity will consider these ideas as essential components of contemporary computing.\nWe also expect to see significant progress by processing central language concepts through system-on-a-chip (SoC)\nsolutions of pre-trained models, with linear probing layers for hot-swappable weight exchange of task-specific projec-\ntions and executions. A wide range of functionalities can be then offloaded to probabilistic programming languages to\noperate on dedicated symbols and streamline the vector-valued mappings between the concept space and underlying\nproblem space, avoiding defining boilerplate code to load and unload network weights.\nFurthermore, we believe that many gains in representational stability and consistency may be obtained through multi-\nmodal data training and improved alignment based on operator learning oriented functionalities and workflow-related\nscoring functionalities, analogous to our introduced quality measure. Gains in representational stability also benefit\nself-instruction and self-referential sub-process evaluations, which enable the dynamic creation and evaluation of\ncomplex hierarchical computational graphs. This will enable online learning models to perform, in real-time, skill\nacquisition of complex concepts with only one or few examples at inference time. We believe this will enable the\ncreation of autonomously self-evolving cognitive architectures (Langley et al., 2009; Dawid & LeCun, 2023; Sumers\net al., 2023). We therefore see an inherent connection to generative design as an analogy for creating coherent and\nstable \u201dthought\u201d computational graphs, and believe this paves the path toward broad AI systems (see Section B) and is\na requirement for developing General AI agents.\nFinally, we also wish to express our concern about recent economic trends in the deep-tech industry, where we\nobserve AI-related concentration of data and resources, coupled with a tendency towards closed-source practices. We\nstrongly advocate for increased transparency and exchange of ideas to ensure a diverse and collective growth in our\nsocio-economic landscape. Therefore, we push towards a democratic and open-source initiative.\n7.4\nConclusion\nIn this work, we introduced SymbolicAI, a framework that unifies generative models with an array of solvers, blending\nthe strengths of symbolic and sub-symbolic AI paradigms within a cohesive NeSy framework. SymbolicAI equips\nresearchers and practitioners with a comprehensive toolkit to develop contextualized and explainable NeSy AI systems\ncapable of addressing diverse challenges effectively.\nWe also introduce a quality measure and a benchmark for\ncomparing and evaluating a wide range of computational tasks. SymbolicAI provides a basis for further research in\nadvanced program synthesis, hierarchical computational graphs, the development of self-referential systems, and the\nintegration of probabilistic methods with AI design for creating autonomous agents. Furthermore, our commitment to\nopen-source ideals attempts to promote a culture of collaborative growth and innovation.\nAcknowledgement\nThe ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State\nUpper Austria. We thank the projects Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064),\nPRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI\nFG 9-N (FWF-36284, FWF-36235), AI4GreenHeatingGrids (FFG- 899943), INTEGRATE (FFG-892418), ELISE\n(H2020-ICT-2019-3 ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank Audi.JKU Deep\n13\nLearning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline\nGmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA,\nVerbund AG, GLS (Univ. Waterloo), Software Competence Center Hagenberg GmbH, Borealis AG, T \u00a8UV Austria,\nFrauscher Sensonic, TRUMPF, the NVIDIA Corporation and Atlas.\nWe extend our appreciation to Andreas Windisch and Clemens Wasner of AI Austria for their unwavering support. Their\nvaluable feedback, connections, and facilitation of introductions within their expansive network have been instrumental\nto the progress of ExtensityAI.\nOur gratitude also goes to Sergei Pereverzyev, whose enlightened guidance and thoughtful ideas have been a beacon\nfor our research endeavors. Our thanks are equally extended to Gary Marcus, whose stimulating discussions sparked\nnumerous innovative ideas incorporated into our framework.\nWe are equally grateful to Markus Hofmarcher, a friend and colleague whose informed counsel and stimulating\ndiscussions have significantly sharpened various facets of our study.\nAdditionally, our thanks are due to Fabian\nPaischer and Kajetan Schweighofer, whose preliminary work and assistance have been of enormous benefit.\nWe are also grateful to our friends John Chong Min Tan and Tim Scarfe, whose communities have been a hub for\nexhilarating discussions. Their online presence and engagement have enriched the AI research landscape and broadened\nour perspectives.\nMoreover, we wish to honor the memories of the cherished family members we lost in 2023. Their influence in our\nlives extended beyond personal bonds, and the principles they instilled in us continue to shape our journey. It is with\ngreat respect and affection that we acknowledge the indelible impact they have made, enabling us to persist in our\nscientific pursuits with determination and integrity.\nReferences\nM. Z. Alom, T. M. Taha, C. Yakopcic, S. Westberg, P. Sidike, M. S. Nasrin, B. C. Van Esesn, A. A. S. Awwal, and\nV. K. Asari. The history began from alexnet: A comprehensive survey on deep learning approaches. arXiv preprint\narXiv:1803.01164, 2018.\nR. E. Amaro, J.-Y. Chen, J. M. Duarte, T. E. Hutton, C. Irving, M. C. Kandes, A. Majumdar, D. Y. Mishin, M. H.\nNguyen, P. Rodriguez, F. Silva, R. S. Sinkovits, S. M. Strande, M. Tatineni, L. S. Tran, and N. Wolter. Voyager\n\u2013 an innovative computational resource for artificial intelligence & machine learning applications in science and\nengineering. In Practice and Experience in Advanced Research Computing, PEARC \u201923\u2019, pp. 278\u2013282, New York,\nNY, USA, 2023. Association for Computing Machinery. ISBN 9781450399852. doi: 10.1145/3569951.3597597.\nJ. Andreas. Language models as agent models. CoRR, abs/2212.01681, 2022. doi: 10.48550/arXiv.2212.01681.\nJ. A. Arjona-Medina, M. Gillhofer, M. Widrich, T. Unterthiner, J. Brandstetter, and S. Hochreiter. RUDDER: return\ndecomposition for delayed rewards. In Advances in Neural Information Processing Systems 32, pp. 13566\u201313577,\n2019.\nM. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat, Y. LeCun, and N. Ballas. Self-supervised\nlearning from images with a joint-embedding predictive architecture. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 15619\u201315629, 2023.\nL. M. Augusto. Computational Logic. Vol. 1: Classical Deductive Computing with Classical Logic. College Publica-\ntions, London, 2 edition, 2022.\nF. Badita. 1337 Use Cases for ChatGPT & other Chatbots in the AI-Driven Era. Google Docs, 2022.\nD.M. Beazley. Python Essential Reference. Developer\u2019s library : essential references for programming professionals.\nAddison-Wesley, 2009. ISBN 9780672329784. URL https://books.google.ro/books?id=Chr1NDlUcI8C.\nE. Bengio, M. Jain, M. Korablyov, D. Precup, and Y. Bengio. Flow network based generative models for non-iterative\ndiverse candidate generation. Advances in Neural Information Processing Systems, 34:27381\u201327394, 2021a.\nY. Bengio, T. Deleu, E. J. Hu, S. Lahlou, M. Tiwari, and E. Bengio.\nGflownet foundations.\narXiv preprint\narXiv:2111.09266, 2021b.\nT. R. Besold, A. d. Garcez, S. Bader, H. Bowman, P. Domingos, P. Hitzler, K.-U. Kuehnberger, L. C. Lamb, D. Lowd,\nP. M. V. Lima, L. de Penning, G. Pinkas, H. Poon, and G. Zaverucha. Neural-symbolic learning and reasoning: A\nsurvey and interpretation, 2017.\n14\nM. Besta, N. Blach, A. Kubicek, R. Gerstenberger, L. Gianinazzi, J. Gajda, T. Lehmann, M. Podstawski, H. Niewiadom-\nski, P. Nyczyk, and T. Hoefler. Graph of thoughts: Solving elaborate problems with large language models. arXiv\npreprint arXiv:2308.09687, 2023.\nS. Biderman, H. Schoelkopf, Q. Anthony, H. Bradley, K. O\u2019Brien, E. Hallahan, M. Aflah Khan, S. Purohit, S. Prashanth,\nE. Raff, A. Skowron, L. Sutawika, and O. van der Wal. Pythia: A suite for analyzing large language models across\ntraining and scaling, 2023.\nPENG Bo. Blinkdl/rwkv-lm: 0.01. Technical report, Zenodo, August 2021. URL https://doi.org/10.5281/\nzenodo.5196577.\nW. Bradley Knox and Peter Stone. TAMER: Training an Agent Manually via Evaluative Reinforcement. In 2008 7th\nIEEE International Conference on Development and Learning, pp. 292\u2013297, Monterey, CA, August 2008. IEEE.\nISBN 978-1-4244-2661-4. doi: 10.1109/DEVLRN.2008.4640845.\nJ. D. Bransford and M. K. Johnson. Contextual prerequisites for understanding: Some investigations of comprehension\nand recall. Journal of Verbal Learning and Verbal Behavior, 11(6):717\u2013726, 1972. ISSN 0022-5371.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell,\nS. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse,\nM. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\nH. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates,\nInc., 2020.\nH. Chase. LangChain. Technical report, LangChain, 01 2023. URL https://github.com/hwchase17/langchain.\nW. Chen, M. Kifer, and D. S. Warren. Hilog: A foundation for higher-order logic programming. The Journal of Logic\nProgramming, 15(3):187\u2013230, 1993. ISSN 0743-1066.\nF. Chollet. On the measure of intelligence, 2019.\nN. Chomsky. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113\u2013124,\n1956. doi: 10.1109/TIT.1956.1056813.\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from human\npreferences. Advances in neural information processing systems, 30, 2017.\nA. Colmerauer and P. Roussel. The birth of Prolog. In HOPL-II, 1993.\nA. d\u2019Avila Garcez and L. C. Lamb. Neurosymbolic ai: The 3rd wave. arXiv preprint arXiv:2012.05876, 2020.\nA. d\u2019Avila Garcez, M. Gori, L. C. Lamb, L. Serafini, M. Spranger, and S. N. Tran. Neural-symbolic computing: An\neffective methodology for principled integration of machine learning and reasoning. Journal of Applied Logic, 2019.\nA. Dawid and Y. LeCun. Introduction to latent variable energy-based models: A path towards autonomous machine\nintelligence. arXiv preprint arXiv:2306.02572, 2023.\nJ. Degrave. Building A Virtual Machine inside ChatGPT. Technical report, Engraved, 11 2022. URL https:\n//www.engraved.blog/building-a-virtual-machine-inside/.\nF. Deniz, A. O. Nunez-Elizalde, A. G. Huth, and J. L. Gallant. The representation of semantic information across\nhuman cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience,\n39(39):7722\u20137736, 2019. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.0675-19.2019.\nD. C. Dennett. Real patterns. Journal of Philosophy, 88(1):27\u201351, 1991. doi: 10.2307/2027085.\nM. Dilhara, A. Ketkar, and D. Dig. Understanding software-2.0: A study of machine learning library usage and\nevolution. ACM Transactions on Software Engineering and Methodology (TOSEM), 30(4):55:1\u201355:42, jul 2021.\nISSN 1049-331X. doi: 10.1145/3453478.\nM.-C. Dinu, M. Hofmarcher, V. P. Patil, M. Dorfer, P. M. Blies, J. Brandstetter, J. A. Arjona-Medina, and S. Hochreiter.\nXai and strategy extraction via reward redistribution. In A. Holzinger, R. Goebel, R. Fong, T. Moon, K.-R. M\u00a8uller,\nand W. Samek (eds.), xxAI - Beyond Explainable AI: International Workshop, Held in Conjunction with ICML 2020,\nJuly 18, 2020, Vienna, Austria, Revised and Extended Papers, pp. 177\u2013205, Cham, 2022. Springer International\nPublishing. ISBN 978-3-031-04083-2. doi: 10.1007/978-3-031-04083-2 10.\n15\nM.-C. Dinu, M. Holzleitner, M. Beck, H. D. Nguyen, A. Huber, H. Eghbal-zadeh, B. A. Moser, S. V. Pereverzyev,\nS. Hochreiter, and W. Zellinger. Addressing parameter choice issues in unsupervised domain adaptation by aggre-\ngation. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May\n1-5, 2023. OpenReview.net, 2023.\nI. Donadello, L. Serafini, and A. d\u2019Avila Garcez.\nLogic tensor networks for semantic image interpretation.\nIn\nProceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI-17, pp. 1596\u20131602,\n2017.\nD. C. Dowson and B. V. Landau. The fr\u00b4echet distance between multivariate normal distributions. Journal of Multivariate\nAnalysis, 12(3):450\u2013455, 1982. doi: https://doi.org/10.1016/0047-259X(82)90077-X.\nKevin Ellis.\nHuman-like few-shot learning via bayesian reasoning over natural language.\narXiv preprint\narXiv:2306.02797, 2023.\nM. Fang, S. Deng, Y. Zhang, Z. Shi, L. Chen, M. Pechenizkiy, and J. Wang. Large language models are neurosymbolic\nreasoners. arXiv preprint arXiv:2401.09334, 2024.\nE. Fedorenko, P.-J. Hsieh, A. Nieto-Castanon, S. Whitfield-Gabrieli, and N. Kanwisher. New method for fMRI\ninvestigations of language: Defining rois functionally in individual subjects. Journal of neurophysiology, 104:\n1177\u201394, 08 2010. doi: 10.1152/jn.00032.2010.\nE. Feigenbaum, B. G. Buchanan, J. Lederberg, Carl Djerassi, and et al. Dendral, 1965.\nC. Fernando, D. Banarse, H. Michalewski, S. Osindero, and T. Rockt\u00a8aschel. Promptbreeder: Self-referential self-\nimprovement via prompt evolution. arXiv preprint arXiv:2309.16797, 2023.\nR. F. Gamble, G.-C. Roman, H. C. Cunningham, and W. E. Ball. Applying formal verification methods to rule-based\nprograms. Int. J. Expert Syst., 7(3):203\u2013237, sep 1994. ISSN 0894-9077.\nS. Ganguly and V. Pudi. Paper2vec: Combining graph and text information for scientific paper representation. In\nJoemon Jose et al. (eds.), Advances in Information Retrieval, volume 10193 of Lecture Notes in Computer Science.\nSpringer, Cham, 2017. ISBN 978-3-319-56607-8. doi: 10.1007/978-3-319-56608-5 30.\nY. Gao, Y. Xiong, X. Gao, K. Jia, J. Pan, Y. Bi, Y. Dai, J. Sun, and H. Wang. Retrieval-augmented generation for large\nlanguage models: A survey. arXiv preprint arXiv:2312.10997, 2023.\nA. Garcez, T. Besold, L. De Raedt, P. F\u00a8oldi\u00b4ak, P. Hitzler, T. Icard, K. K\u00a8uhnberger, L. Lamb, R. Miikkulainen, and\nD. Silver. Neural-symbolic learning and reasoning: Contributions and challenges. In AAAI Conference, 2015.\nM. Gauch, M. Beck, T. Adler, D. Kotsur, S. Fiel, H. Eghbal-zadeh, J. Brandstetter, J. Kofler, M. Holzleitner, W. Zellinger,\nD. Klotz, S. Hochreiter, and S. Lehner. Few-Shot Learning by Dimensionality Reduction in Gradient Space. arXiv\npreprint arXiv:2206.03483, 2022.\nX. Geng, A. Gudibande, H. Liu, E. Wallace, P. Abbeel, S. Levine, and D. Song. Koala: A dialogue model for academic\nresearch. Blog post, April 2023. URL https://bair.berkeley.edu/blog/2023/04/03/koala/.\nGoogle. Gemini: A family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\nA. Goyal, A. Friesen, A. Banino, T. Weber, N. R. Ke, A. P. Badia, A. Guez, M. Mirza, P. C. Humphreys, K. Konyushova,\nM. Valko, S. Osindero, T. Lillicrap, N. Heess, and C. Blundell. Retrieval-augmented reinforcement learning. In\nK. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari, G. Niu, and S. Sabato (eds.), Proceedings of the 39th International\nConference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp. 7740\u20137765.\nPMLR, 17\u201323 Jul 2022.\nA. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch\u00a8olkopf, and A. Smola. A kernel two-sample test. Journal of Machine\nLearning Research, 13(25):723\u2013773, 2012.\nA. Gu and T. Dao.\nMamba:\nLinear-time sequence modeling with selective state spaces.\narXiv preprint\narXiv:2312.00752, 2023.\nQ. Guo, Z. Jin, X. Qiu, W. Zhang, D. Wipf, and Z. Zhang. CycleGT: Unsupervised graph-to-text and text-to-graph\ngeneration via cycle training. arXiv preprint arXiv:2006.04702, 2020.\n16\nK. Hamilton, A. Nayak, B. Bo\u02c7z i\u00b4c, and L. Longo. Is neuro-symbolic AI meeting its promises in natural language\nprocessing? a structured review. Semantic Web, pp. 1\u201342, nov 2022. doi: 10.3233/sw-223228.\nC. R. Harris, K. J. Millman, S. J. van der Walt, R. Gommers, P. Virtanen, D. Cournapeau, E. Wieser, J. Taylor, S. Berg,\nN. J. Smith, R. Kern, M. Picus, S. Hoyer, M. H. van Kerkwijk, M. Brett, A. Haldane, J. Fern\u00b4andez del R\u00b4\u0131o, M. Wiebe,\nP. Peterson, P. G\u00b4erard-Marchant, K. Sheppard, T. Reddy, W. Weckesser, H. Abbasi, C. Gohlke, and T. E. Oliphant.\nArray programming with NumPy. Nature, 585(7825):357\u2013362, 2020. doi: 10.1038/s41586-020-2649-2.\nM. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In Proceedings of the 31st International Conference on Neural Infor-\nmation Processing Systems, NIPS\u201917, pp. 6629\u20136640, Red Hook, NY, USA, 2017. Curran Associates Inc. ISBN\n9781510860964.\nS. Hochreiter. Toward a broad AI. Commun. ACM, 65(4):56\u201357, mar 2022. ISSN 0001-0782.\nS. Hochreiter and J. Schmidhuber. Flat minima. Neural Comput., 9(1):1\u201342, 1997.\nM. Holzleitner, L. Gruber, J. A. Arjona-Medina, J. Brandstetter, and S. Hochreiter. Convergence proof for actor-critic\nmethods applied to PPO and RUDDER. arXiv preprint arXiv:2012.01399, 2020.\nK. Hornik, M. Tinchcombe, and H. White. Multilayer feedforward networks are universal approximators. Neural\nNetworks, 2:359\u2013366, 1989. doi: 10.1016/0893-6080(89)90020-8.\nJ. Hu, H. Small, H. Kean, A. Takahashi, L. Zekelman, D. Kleinman, E. Ryan, A. Nieto-Casta\u02dcn\u00b4on, V. Ferreira, and\nE. Fedorenko. Precision fMRI reveals that the language-selective network supports both phrase-structure building\nand lexical access during language production. bioRxiv, 2022. doi: 10.1101/2021.09.10.459596.\nZ. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing. Harnessing deep neural networks with logic rules. In Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2410\u20132420,\nBerlin, Germany, August 2016. Association for Computational Linguistics.\nA. G. Huth, W. A. de Heer, T. L. Griffiths, F. E. Theunissen, and J. L. Gallant. Natural speech reveals the semantic\nmaps that tile human cerebral cortex. Nature, 532(7600):453\u2013458, 2016. doi: 10.1038/nature17637.\nB. Ibarz, J. Leike, T. Pohlen, G. Irving, S. Legg, and D. Amodei. Reward learning from human preferences and\ndemonstrations in atari. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett\n(eds.), Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\nB. Ibarz, V. Kurin, G. Papamakarios, K. Nikiforou, M. Abbana Bennani, R. Csord\u00b4as, A. Dudzik, M. Bo\u02c7snjak,\nA. Vitvitskyi, Y. Rubanova, A. Deac, B. Bevilacqua, Y. Ganin, C. Blundell, and P. Veliv\u02c7ckovi\u00b4c. A generalist neural\nalgorithmic learner. In LOG IN, 2022.\nWolfram Research, Inc.\nMathematica, Version 13.2, 2022.\nURL https://www.wolfram.com/mathematica.\nChampaign, IL.\nG. Indiveri, B. Linares-Barranco, T. Hamilton, A. van Schaik, R. Etienne-Cummings, T. Delbruck, S. Liu, P. Dudek,\nP. H \u02dcA=Cfliger, S. Renaud, J. Schemmel, G. Cauwenberghs, J. Arthur, K. Hynna, F. Folowosele, S. SA \u02dcA GHI,\nT. Serrano-Gotarredona, J. Wijekoon, Y. Wang, and K. Boahen. Neuromorphic silicon neuron circuits. Frontiers in\nNeuroscience, 5, 2011. ISSN 1662-453X. doi: 10.3389/fnins.2011.00073.\nN. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Rajamani, and R. Sharma. Jigsaw: Large language\nmodels meet program synthesis. arXiv, 2021.\nJ. Johnson, M. Douze, and H. J\u00b4egou. Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3):\n535\u2013547, 2019.\nE. Jones and J. Steinhardt. Capturing failures of large language models via human cognitive biases. arXiv preprint\narXiv:2202.12299, 2022.\nA.\nKarpathy.\nSoftware\n2.0.\nMedium,\n2017.\nURL\nhttps://karpathy.medium.com/\nsoftware-2-0-a64152b37c35.\n17\nN. Kassner, B. Krojer, and H. Sch\u00a8utze. Are Pretrained Language Models Symbolic Reasoners over Knowledge?\nIn R. Fern\u00b4andez and T. Linzen (eds.), Proceedings of the 24th Conference on Computational Natural Language\nLearning, CoNLL 2020, Online, November 19-20, 2020, pp. 552\u2013564. Association for Computational Linguistics,\n2020. doi: 10.18653/v1/2020.conll-1.45.\nT. Katsch. Gateloop: Fully data-controlled linear recurrence for sequence modeling. arXiv preprint arXiv:2311.01927,\n2023.\nD. Key, W.-D. Li, and K. Ellis. Toward trustworthy neural program synthesis. arXiv preprint arXiv:2210.00848, 2023.\nG. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.\nL. Kirsch and J. Schmidhuber. Eliminating meta optimization through self-referential meta learning. arXiv preprint\narXiv:2212.14392, 2022.\nA. K\u00a8opf, Y. Kilcher, D. von R\u00a8utte, S. Anagnostidis, Z.-R. Tam, K. Stevens, A. Barhoum, N. M. Duc, O. Stanley,\nR. Nagyfi, S. ES, S. Suri, D. Glushkov, A. Dantuluri, A. Maguire, C. Schuhmann, H. Nguyen, and A. Mattick.\nOpenassistant conversations \u2013 democratizing large language model alignment, 2023.\nD. C. Kuncicky, S. I. Hruska, and R. C. Lacher. Hybrid systems: the equivalence of rule-based expert system and\nartificial neural network inference. Int. J. Expert Syst., 4(3):281\u2013297, jan 1991. ISSN 0894-9077.\nE. K\u0131c\u0131man, R. Ness, A. Sharma, and C. Tan. Causal Reasoning and Large Language Models: Opening a New Frontier\nfor Causality. arXiv, 2023.\nS. Lahlou, T. Deleu, P. Lemos, D. Zhang, A. Volokhova, A. Hern\u00b4andez-Garc\u00b4\u0131a, L. N. Ezzine, Y. Bengio, and\nN. Malkin. A theory of continuous generative flow networks. In Proceedings of the International Conference on\nMachine Learning, pp. 18269\u201318300. PMLR, 2023.\nJ. E. Laird. Introduction to soar, 2022.\nJ. E. Laird, A. Newell, and P. S. Rosenbloom. Soar: An architecture for general intelligence. Artificial Intelligence, 33\n(1):1\u201364, 1987. ISSN 0004-3702.\nL. C. Lamb, A. Garcez, M. Gori, M. Prates, P. Avelar, and M. Vardi. Graph neural networks meet neural-symbolic\ncomputing: A survey and perspective. In AAAI Conference, 2020.\nP. Langley, J. Laird, and S. Rogers. Cognitive architectures: Research issues and challenges. Cognitive Systems\nResearch, 10:141\u2013160, 2009. doi: 10.1016/j.cogsys.2006.07.004.\nY. LeCun. A path towards autonomous machine intelligence, 2022. OpenReview Archive.\nH. Li, Y. Su, D. Cai, Y. Wang, and L. Liu.\nA survey on retrieval-augmented text generation.\narXiv preprint\narXiv:2202.01110, 2022a.\nY. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago,\net al. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097, 2022b.\nZ. Li, Z. Yang, and M. Wang. Reinforcement learning with human feedback: Learning dynamic choices via pessimism.\narXiv preprint arxiv:2305.18438, 2023.\nN. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua, F. Petroni, and P. Liang. Lost in the middle: How language\nmodels use long contexts. arXiv preprint arXiv:2307.03172, 2023.\nS. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco, C. Clement, D. Drain, D. Jiang, D. Tang, et al. Codexglue:\nA machine learning benchmark dataset for code understanding and generation. arXiv preprint arXiv:2102.04664,\n2021.\nM. Lutz. Learning Python: Powerful Object-Oriented Programming. Animal Guide. O\u2019Reilly Media, 2013. ISBN\n9781449355715.\nQ. Lyu, S. Havaldar, A. Stein, L. Zhang, D. Rao, E. Wong, M. Apidianaki, and C. Callison-Burch. Faithful chain-of-\nthought reasoning, 2023.\n18\nJ. MacGlashan, M. K. Ho, R. Loftin, B. Peng, G. Wang, D. L. Roberts, M. E. Taylor, and M. L. Littman. Interactive\nLearning from Policy-Dependent Human Feedback. In Proceedings of the 34th International Conference on Machine\nLearning, pp. 2285\u20132294. PMLR, July 2017.\nM. Macsweeney. Neural systems underlying british sign language and audio-visual english processing in native users.\nBrain, 125:1583\u20131593, 07 2002. doi: 10.1093/brain/awf153.\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye, Y. Yang,\nS. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark. Self-refine: Iterative refinement with\nself-feedback, 2023.\nK. Mahowald, A. A. Ivanova, I. A. Blank, N. Kanwisher, J. B. Tenenbaum, and E. Fedorenko. Dissociating language\nand thought in large language models: a cognitive perspective. CoRR, abs/2301.06627, 2023.\nR. Manhaeve, S. Dumancic, A. Kimmig, T. Demeester, and L. De Raedt. DeepProbLog: Neural Probabilistic Logic\nProgramming. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.),\nAdvances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\nJ. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu. The neuro-symbolic concept learner: Interpreting scenes,\nwords, and sentences from natural supervision. In 7th International Conference on Learning Representations, ICLR\n2019, 2019.\nG. Marcus. The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence. arXiv preprint arXiv:2002.06177,\n2020.\nA. Martelli, A. Ravenscroft, and D. Ascher. Python Cookbook. O\u2019Reilly Media, 2005. ISBN 9780596554743. URL\nhttps://books.google.ro/books?id=Q0s6Vgb98CQC.\nJ. McCarthy. Lisp: A programming system for symbolic manipulations. In Preprints of Papers Presented at the 14th\nNational Meeting of the Association for Computing Machinery, ACM \u201959, pp. 1\u20134, New York, NY, USA, 1959.\nAssociation for Computing Machinery. ISBN 9781450373647. doi: 10.1145/612201.612243.\nJ. McCarthy, M. L. Minsky, N. Rochester, and C. E. Shannon. A proposal for the dartmouth summer research project\non artificial intelligence, august 31, 1955. AI magazine, 27(4):12\u201312, 2006.\nW. S. McCulloch and W. Pitts. A Logical Calculus of Ideas Immanent in Nervous Activity. Bulletin of Mathematical\nBiophysics, 5:115\u2013133, 1943. doi: 10.1007/BF02478255.\nL. Menenti, S. M. E. Gierhan, K. Segaert, and P. Hagoort. Shared language: Overlap and segregation of the neuronal\ninfrastructure for speaking and listening revealed by functional mri. Psychological Science, 22(9):1173\u20131182, 2011.\ndoi: 10.1177/0956797611418347. PMID: 21841148.\nMicrosoft. Bing is your AI-powered copilot for the web. Technical report, Microsoft, 2023. URL https://bing.\ncom/chat.\nT. Mikolov, K. Chen, G. Corrado, and J. Dean.\nEfficient Estimation of Word Representations in Vector Space.\nInternational Conference on Learning Representations, 2013.\nS. Milani, N. Topin, B. Houghton, W. H. Guss, S. P. Mohanty, K. Nakata, O. Vinyals, and N. S. Kuno. Retrospective\nanalysis of the 2019 minerl competition on sample efficient reinforcement learning. In H. J. Escalante and R. Hadsell\n(eds.), Proceedings of the NeurIPS 2019 Competition and Demonstration Track, volume 123 of Proceedings of\nMachine Learning Research, pp. 203\u2013214. PMLR, Dec 2020.\nL. De Moura and N. Bj\u00f8rner. Z3: an efficient smt solver. In Proceedings of the Theory and Practice of Software, 14th In-\nternational Conference onTools and Algorithmsfor the Construction andAnalysis of Systems, TACAS\u201908/ETAPS\u201908,\npp. 337\u2013340, Berlin, Heidelberg, 2008. Springer-Verlag. ISBN 3540787992.\nA. Newell. Unified Theories of Cognition. Harvard University Press, USA, 1990. ISBN 0674920996.\nA. Newell and H. Simon. The logic theory machine\u2013a complex information processing system. IRE Transactions on\ninformation theory, 2(3):61\u201379, 1956.\nA. Newell and H. A. Simon. Human problem solving. Prentice-Hall, pp. 920, 1972.\n19\nA. Newell and H. A. Simon. Computer science as empirical inquiry: symbols and search. Commun. ACM, 19(3):\n113\u2013126, mar 1976. ISSN 0001-0782. doi: 10.1145/360018.360022.\nA. Newell, J. C. Shaw, and H. A. Simon. Empirical explorations of the logic theory machine: a case study in heuristic.\nIRE-AIEE-ACM \u201957 (Western): Papers presented at the February 26-28, 1957, western joint computer conference:\nTechniques for reliability, pp. 218\u2013230, 1957. doi: 10.1145/1455567.1455605.\nH. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, R. Luo, S. M.\nMcKinney, R. O. Ness, H. Poon, T. Qin, N. Usuyama, C. White, and E. Horvitz. Can generalist foundation models\noutcompete special-purpose tuning? case study in medicine. arXiv preprint arXiv:2311.16452, 2023.\nA. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\nOpenAI. Introducing ChatGPT. Technical report, OpenAI, November 2022. URL https://openai.com/blog/\nchatgpt.\nOpenAI. GPT-4 Technical Report. arXiv, 2023.\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\nJ. Schulman, J. Hilton, F. Kelton, L. E. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. J.\nLowe. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155,\n2022.\nF. Paischer, T. Adler, V. Patil, A. Bitto-Nemling, M. Holzleitner, S. Lehner, H. Eghbal-Zadeh, and S. Hochreiter. History\ncompression via language models in reinforcement learning. In K. Chaudhuri, S. Jegelka, L. Song, C. Szepesvari,\nG. Niu, and S. Sabato (eds.), Proceedings of the 39th International Conference on Machine Learning, volume 162\nof Proceedings of Machine Learning Research, pp. 17156\u201317185. PMLR, July 2022.\nF. Paischer, T. Adler, M. Hofmarcher, and S. Hochreiter. Semantic helm: An interpretable memory for reinforcement\nlearning. CoRR, abs/2306.09312, 2023. doi: 10.48550/arXiv.2306.09312.\nN. Park, D. Chae, J. Shim, S. Kim, E.-S. Kim, and J. Kim. Bridging the domain gap by clustering-based image-text\ngraph matching. arXiv preprint arXiv:2310.02692, 2023.\nA. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. K\u00a8opf, E. Z. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang,\nJ. Bai, and S. Chintala. Pytorch: An imperative style, high-performance deep learning library. arXiv preprint\narXiv:1912.01703, abs/1912.01703, 2019.\nV. P. Patil, M. Hofmarcher, M.-C. Dinu, M. Dorfer, P. M. Blies, J. Brandstetter, J. A. Arjona-Medina, and S. Hochreiter.\nAlign-RUDDER: Learning from few demonstrations by reward redistribution. arXiv preprint arXiv:2009.14108,\n2020.\nF. Petroni, T. Rockt\u00a8aschel, S. Riedel, P. S. H. Lewis, A. Bakhtin, Y. Wu, and A. H. Miller.\nLanguage Models\nas Knowledge Bases?\nIn K. Inui, J. Jiang, V. Ng, and X. Wan (eds.), Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pp. 2463\u20132473. Association\nfor Computational Linguistics, 2019. doi: 10.18653/v1/D19-1250.\nS. Pitis, M. R. Zhang, A. Wang, and J. Ba. Boosted prompt ensembles for large language models, 2023.\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus, Y. Bengio, S. Ermon, and C. R\u00b4e. Hyena hierarchy:\nTowards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023.\nB. Poole, A. Jain, J. T. Barron, and B. Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint\narXiv:2209.14988, 2022.\nM. Qu and J. Tang. Probabilistic logic neural networks for reasoning. In Proceedings of the 33rd International\nConference on Neural Information Processing Systems, 2019.\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. Direct preference optimization: Your\nlanguage model is secretly a reward model. arXiv preprint arXiv:2305.18290, 2023.\n20\nA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever. Zero-shot text-to-image\ngeneration. In International Conference on Machine Learning, pp. 8821\u20138831. PMLR, 2021.\nH. Ramsauer, B. Sch\u00a8afl, J. Lehner, P. Seidl, M. Widrich, T. Adler, L. Gruber, M. Holzleitner, M. Pavlovi\u00b4c, G. K.\nSandve, et al. Hopfield networks is all you need. arXiv preprint arXiv:2008.02217, 2020.\nM. Regev, C. J. Honey, E. Simony, and U. Hasson. Selective and invariant neural responses to spoken and written\nnarratives. Journal of Neuroscience, 33(40):15978\u201315988, 2013. ISSN 0270-6474. doi: 10.1523/JNEUROSCI.\n1580-13.2013.\nReplikaAI.\nPushing\nthe\nBoundaries\nof\nAI\nto\nTalk\nto\nthe\nDead.\nTechnical\nreport,\nReplikaAI,\n2016.\nURL\nhttps://www.bloomberg.com/news/articles/2016-10-20/\npushing-the-boundaries-of-ai-to-talk-to-the-dead.\nB. Romera-Paredes, M. Barekatain, A. Novikov, et al. Mathematical discoveries from program search with large\nlanguage models. Nature, 2023. doi: 10.1038/s41586-023-06924-6.\nF. Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psycho-\nlogical Review, 65(6):386\u2013408, 1958. doi: 10.1037/h0042519.\nL. Ruis, A. Khan, S. Biderman, S. Hooker, T. Rockt\u00a8aschel, and E. Grefenstette. Large language models are not zero-shot\ncommunicators. CoRR, abs/2210.14986, 2022. doi: 10.48550/arXiv.2210.14986.\nD. Rumelhart, G. Hinton, and R. Williams. Learning representations by back-propagating errors. Nature, 323:533\u2013536,\n1986. doi: 10.1038/323533a0.\nC. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan, S. S. Mahdavi,\nR. G. Lopes, et al. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\nA. Santoro, A. Lampinen, K. Mathewson, T. Lillicrap, and D. Raposo. Symbolic behaviour in artificial intelligence.\narXiv preprint arXiv:2102.03406, 2022.\nT. Schick, J. Dwivedi-Yu, R. Dess`\u0131, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom. Toolformer:\nLanguage models can teach themselves to use tools, 2023.\nJ. Schmidhuber. G\u00a8odel machines: Fully self-referential optimal universal self-improvers. Cognitive Technologies, 8:\n199\u2013226, 01 2007. doi: 10.1007/978-3-540-68677-4 7.\nJ. Schmidhuber.\nDriven by compression progress:\nA simple principle explains essential aspects of subjective\nbeauty, novelty, surprise, interestingness, attention, curiosity, creativity, art, science, music, jokes. arXiv preprint\narXiv:0812.4360, 2009.\nJ. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lockhart, D. Hassabis,\nT. Graepel, T. Lillicrap, and D. Silver. Mastering atari, go, chess and shogi by planning with a learned model.\nNature, 588(7839):604\u2013609, 2020. doi: 10.1038/s41586-020-03051-4.\nK. Schweighofer, A. Radler, M.-C. Dinu, M. Hofmarcher, V. P. Patil, A. Bitto-Nemling, H. Eghbal-zadeh, and\nS. Hochreiter. A dataset perspective on offline reinforcement learning. In Conference on Lifelong Learning Agents,\npp. 470\u2013517. PMLR, 2022.\nT. Scott, J. Gall\u00b4ee, and E. Fedorenko. A new fun and robust version of an fMRI localizer for the frontotemporal\nlanguage system. Cognitive Neuroscience, 8:1\u201310, 07 2016. doi: 10.1080/17588928.2016.1201466.\nN. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory and self-reflection,\n2023.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap,\nM. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis. Mastering the game of Go with deep neural networks and\ntree search. Nature, 529(7587):484\u2013489, 2016. doi: 10.1038/nature16961.\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D. Kumaran, T. Graepel,\nT. Lillicrap, K. Simonyan, and D. Hassabis. Mastering chess and shogi by self-play with a general reinforcement\nlearning algorithm. arXiv preprint arXiv:1712.01815, 2017a.\n21\nD. Silver, J. Schrittwieser, K. Simonyan, et al. Mastering the game of go without human knowledge. Nature, 550:\n354\u2013359, 2017b. doi: 10.1038/nature24270.\nU. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni, et al. Make-a-video:\nText-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022.\nK. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, et al. Large language models encode\nclinical knowledge. Nature, 620(7972):172\u2013180, 2023.\nK. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mpnet: Masked and permuted pre-training for language understanding.\nIn Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS\u201920, pp. 1414,\nRed Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546.\nSpotify. Approximate Nearest Neighbors Oh Yeah. Technical report, Spotify, 2017.\nT. R. Sumers, S. Yao, K. Narasimhan, and T. L. Griffiths. Cognitive architectures for language agents. arXiv preprint\narXiv:2309.02427, 2023.\nM. Summerfield. Programming in Python 3: A Complete Introduction to the Python Language. Developer\u2019s library.\nAddison-Wesley, 2010. ISBN 9780321680563.\nJ. J. Sun, M. Tjandrasuwita, A. Sehgal, A. Solar-Lezama, S. Chaudhuri, Y. Yue, and O. Costilla-Reyes. Neurosymbolic\nprogramming for science. arXiv preprint arXiv:2210.05050, 2022.\nR. S. Sutton. Temporal Credit Assignment in Reinforcement Learning. PhD thesis, University of Massachusetts, Dept.\nof Comp. and Inf. Sci., 1984.\nP. Swazinna, S. Udluft, D. Hein, and T. Runkler. Comparing model-free and model-based algorithms for offline\nreinforcement learning. arXiv preprint arXiv:2201.05433, 2022.\nZ. Szab\u00b4o, B. K. Sriperumbudur, B. P\u00b4oczos, and A. Gretton. Learning theory for distribution regression. J. Mach.\nLearn. Res., 17(1):5272\u20135311, Jan 2016. ISSN 1532-4435.\nR. Taori, I. Gulrajani, T. Zhang, Y. Dubois, X. Li, C. Guestrin, P. Liang, and T. B. Hashimoto. Stanford alpaca: An\ninstruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca, 2023.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar,\nA. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient foundation language models, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all\nyou need. In I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.),\nAdvances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\nP. Veli\u02c7ckovi\u00b4c and C. Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273, 2021. ISSN 2666-3899. doi:\nhttps://doi.org/10.1016/j.patter.2021.100273.\nB. Wang, Z. Wang, X. Wang, Y. Cao, R. A. Saurous, and Y. Kim. Grammar prompting for domain-specific language\ngeneration with large language models. arXiv preprint arXiv:2305.19234, 2023a.\nJ. Wang, X. Yi, R. Guo, H. Jin, P. Xu, S. Li, X. Wang, X. Guo, C. Li, X. Xu, et al. Milvus: A purpose-built vector data\nmanagement system. In Proceedings of the 2021 International Conference on Management of Data, pp. 2614\u20132627,\n2021a.\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves\nchain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2023b.\nY. Wang, R.J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, et al.\nTacotron: Towards end-to-end speech synthesis. arXiv preprint arXiv:1703.10135, 2017.\nY. Wang, W. Wang, S. Joty, and S. C. H. Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for\ncode understanding and generation. arXiv preprint arXiv:2109.00859, 2021b.\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct: Aligning language\nmodel with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\n22\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler, E. H.\nChi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large language models. arXiv\npreprint arXiv:2206.07682, 2022a.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho (eds.),\nAdvances in Neural Information Processing Systems, 2022b.\nY. Weng, M. Zhu, F. Xia, B. Li, S. He, S. Liu, B. Sun, K. Liu, and J. Zhao. Large language models are better reasoners\nwith self-verification. arXiv preprint arXiv:2212.09561, 2023.\nA. N. Whitehead and B. Russell. Principia Mathematica. Cambridge University Press, 1925\u20131927.\nM. Widrich, M. Hofmarcher, V. P. Patil, A. Bitto-Nemling, and S. Hochreiter. Modern Hopfield Networks for Return\nDecomposition for Delayed Rewards. In Deep RL Workshop NeurIPS 2021, 2021.\nWritesonic. ChatGPT Alternative Built With Superpowers - ChatSonic. Technical report, Chatsonic, 2022. URL\nhttps://writesonic.com/chat.\nC. Xu, D. Guo, N. Duan, and J. McAuley. Baize: An open-source chat model with parameter-efficient tuning on\nself-chat data, 2023.\nZ. Xu, H. van Hasselt, and D. Silver. Meta-gradient reinforcement learning. ArXiv, 2018.\nL. Yang, S. Zhang, Z. Yu, G. Bao, Y. Wang, J. Wang, R. Xu, W. Ye, X. Xie, W. Chen, and Y. Zhang. Supervised\nKnowledge Makes Large Language Models Better In-context Learners. arXiv preprint arXiv:2312.15918, 2023.\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem\nsolving with large language models. arXiv preprint arXiv:2305.10601, 2023a.\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in\nlanguage models. arXiv preprint arXiv:2210.03629, 2023b.\nJ. Ye, Z. Wu, J. Feng, T. Yu, and L. Kong.\nCompositional exemplars for in-context learning.\narXiv preprint\narXiv:2302.05698, 2023.\nK. You, X. Wang, M. Long, and M. Jordan. Towards accurate model selection in deep unsupervised domain adaptation.\nIn K. Chaudhuri and R. Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning,\nvolume 97 of Proceedings of Machine Learning Research, pp. 7124\u20137133. PMLR, Jun 9\u201315 2019.\nYouWrite. The AI Search Engine You Control. Technical report, You.com, 2022. URL https://you.com.\nD. Yu, B. Yang, D. Liu, H. Wang, and S. Pan. A survey on neural-symbolic learning systems. Neural Networks, 166:\n105\u2013126, 2023. ISSN 0893-6080.\nW. Yuan, R. Y. Pang, K. Cho, S. Sukhbaatar, J. Xu, and J. Weston. Self-rewarding language models. arXiv preprint\narXiv:2401.10020, 2024.\nJ. Zhang, B. Chen, L. Zhang, X. Ke, and H. Ding. Neural, symbolic and neural-symbolic reasoning on knowledge\ngraphs. AIOpen, pp. 14\u201335, 2021.\nAppendix\nA\nConnection between Fr\u00b4echet Distance and Maximum Mean Discrepancy\nLet us consider a Gaussian kernel defined by the expression\nK(x, y) = exp\n\u0012\n\u2212\u2225x \u2212 y\u22252\n2\u03c32\n\u0013\n,\n(4)\nwhere \u03c3 is the bandwidth parameter of the kernel and \u2225\u00b7\u2225 denotes the Euclidean norm. Using K, we can now construct\na measure of distance between distributions, by embedding them into the Reproducing Kernel Hilbert Space (RKHS)\n23\ninduced by K, using kernel mean embeddings. The resulting distance is called the Maximum Mean Discrepancy\n(MMD).\nMore precisely, the MMD between two probability distributions P and Q is encoded in the RKHS through mean\nembeddings, which can be expressed as\nMMD2(P, Q) = \u2225Ex\u223cP [\u03d5(x)] \u2212 Ey\u223cQ[\u03d5(y)]\u22252\nRKHS ,\n(5)\nwhere \u03d5(\u00b7) represents the feature mapping to the RKHS corresponding to the kernel K.\nOn the other hand, for multivariate Gaussian distributions, we can use the Fr\u00b4echet distance as a measure of similarity,\nwhich is nothing but the associated Wasserstein-2 distance, for which an explicit formula is available in the Gaussian\ncase. The resulting expression is as follows (Dowson & Landau, 1982):\nd2(X1, X2) = \u2225\u00b51 \u2212 \u00b52\u22252\n2 + Tr\n\u0010\nC1 + C2 \u2212 2 (C1C2)\n1\n2\n\u0011\n,\n(6)\nwhere X1 \u223c N(\u00b51, C1) and X2 \u223c N(\u00b52, C2), and Tr(\u00b7) indicates the trace of a matrix.\nTo establish an approximation of the Fr\u00b4echet distance using the Gaussian kernel, we take C1 = \u03c32I and C2 = \u03c32I as\nidentity covariance matrices scaled by \u03c32. This assumption allows us to focus solely on the disparities in mean vectors:\nd2(X1, X2) \u2248 \u2225\u00b51 \u2212 \u00b52\u22252\n2,\n(7)\nsetting aside the effect of different covariance structures.\nGiven these conditions, we attempt to argue that the Fr\u00b4echet distance behaves similarly as MMD:\nd2(X1, X2) \u2248 \u2225\u00b51 \u2212 \u00b52\u22252\n2 \u2248 MMD2(P, Q),\n(8)\nHeuristically, at least for small \u2225\u00b51 \u2212 \u00b52\u2225, also the associated kernel evaluations K(X1, X2) tend to be small (see also\nHochreiter & Schmidhuber (1997)), which leads to a small MMD, if we ignore the terms associated to K(X1, X1),\nK(X2, X2) (which cancel out due to same covariance structure).\nIn the next section we want to further elaobarate on the MMD and a possible score, that can be derived from it.\nA.1\nExtended Simplification of the MMD Calculation\nTo understand the simplification of the MMD, we are formally expressing the MMD in terms of kernel sums over pairs\nof samples within and across two samples X and Y :\nMMD2(X, Y ) =\n1\nm(m \u2212 1)\nX\ni\nX\nj\u0338=i\nk(xi, xj) \u2212\n2\nmn\nm\nX\ni=1\nn\nX\nj=1\nk(xi, yj) +\n1\nn(n \u2212 1)\nX\ni\nX\nj\u0338=i\nk(yi, yj),\n(9)\nwhere m and n are the sizes of samples X and Y , respectively.\nEmpirical observations have led to the conclusion that the within-sample terms P\ni\nP\nj\u0338=i k(xi, xj) and\nP\ni\nP\nj\u0338=i k(yi, yj) cancel out the cross terms Pm\ni=1\nPn\nj=1 k(xi, yj) under certain conditions. This can be due to\nthe following:\n\u2022 In high-dimensional embedding spaces, distributions of embedding vectors are often closely related and\nnormally distributed.\n\u2022 If the samples X and Y are drawn from distributions P and Q where their mean embeddings are nearly\northogonal in the RKHS, it is the dissimilarity across samples, rather than that within, that is most relevant.\nTherefore, under these specific conditions, it becomes justifiable to focus on the cross-terms, yielding the following\nproposal for a distance measure:\n^\nMMD2(X, Y ) \u2248\n2\nmn\nm\nX\ni=1\nn\nX\nj=1\nk(xi, yj).\n(10)\nB\nBroad AI and Neuro-Symbolic Systems\nOur work focuses on broad artificial intelligence (AI) (Hochreiter, 2022) (see Figure 6) through the integration of\nsymbolic and sub-symbolic AI methodologies. Broad AI extends beyond restricted focus on single-task performance\n24\nKnowledge\nInterpretability\nRobustness\nEfficiency\nAbstraction\nReasoning\nAdaptability\nInteraction\n...\nHuman cognitive abilities\nStructure\nPrediction\n \nAlphaFold\nGame\nPlaying\n \nOpenAI Five\nLanguage\nProcessing\n \nTransformer\nImage\nClassification\n \nResNet\nBroad AI\nBroad Cognitive Abilities\nGeneral AI\nHuman-Level Abilities \nNarrow AI\nTask-Specific Skills\nFigure 6: Hierarchical model of \u201dcognitive\u201d abilities of AI systems (Chollet, 2019; Hochreiter, 2022). The figure\ncontrasts the emergent paradigm of Broad AI with current Narrow AI systems, showcasing Broad AI\u2019s wider range\nof capabilities, such as knowledge transfer, interaction, adaptability, robustness, abstraction, advanced reasoning, and\nefficiency. Broad AI aims to mimic human cognitive adaptability and robustness through advanced methodologies like\nfew-shot learning, self-supervised contrastive learning, and context-sensitive sensory processing. Notably, Broad AI\napplies principles such as conceptual short-term memory and modern Hopfield networks (Ramsauer et al., 2020) to\nbetter integrate context and memory, thus avoiding pitfalls like explaining away and short-cut learning. We acknowledge\nthe potential of NeSy systems as a significant step towards AI systems capable of performing any cognitive task with\nhuman-like proficiency.\nof narrow AI. In broad AI, systems are engineered to handle a range of tasks with a high degree of autonomy, utilizing\nsensory input, accumulated experiences, and previously developed skills.\nNeSy methods form the basis for developing new cognitive architectures (Newell & Simon, 1956; Newell et al., 1957;\nNewell & Simon, 1972; Newell, 1990; Langley et al., 2009; Laird, 2022; Dawid & LeCun, 2023; Sumers et al., 2023;\nLeCun, 2022; Assran et al., 2023), as illustrated in Figure 7. This hybridization produces computational graphs capable\nof context-aware learning and reasoning, allowing AI to execute complex tasks with human-like flexibility.\nBorrowing nomenclature from Karpathy (2017); Dilhara et al. (2021), we refer to the next generation of software\nas Software 3.0, which consists of applications that are neither pre-determined at design time, nor learned through\nstatistical inference, but triggered by an interaction which stimulates the realization of a computational graph analogous\nto neuromorphic circuits (Indiveri et al., 2011), however, purely established at inference time in the \u201dthought\u201d process\nof a NeSy system.\nTo enable such systems, we require a more native integration (see illustration in Figure 8) of probabilistic programming\nparadigms into our contemporary programming stack, and make their utilization a commodity for practitioners and\nresearchers alike.\nC\nFramework Structure\nPrimitives\nIn the SymbolicAI framework, at the core lies the concept of Primitives and the dynamic type creation\nof Symbol objects, which are central to inherit types of behaviors. Primitives are pre-defined operations that act\non Symbol objects, encapsulating basic operations, such as arithmetic, logic, or casting operations, to name a few.\nThese operations are crucial to the framework\u2019s versatility and form the foundation for more complex interactions\nwithin computational graphs. Essentially, they can be viewed as contextualized functions that accept a Symbol object,\nsend it to the NeSy engine for evaluation, and return one or more new objects, primarily new symbols. One of the\nkey features of operations is their polymorphism, which allows for them to be applied to various data types, such as\nstrings, integers, floats, lists, and more, with different behaviors depending on the specific object instance. To execute\noperations, we utilize the Symbol object\u2019s value attribute containing the original data type. This will be then sent as\na string representation to the engines to execute the needed operations. Consequently, all values are cast to a string\n25\nMac\nWin\nUnix\nLinux\nSoftware 1.0\nClassical Programming\nVirtualization\nApps\nFrameworks\n...\nJVM\nC# \nCLR\nJava\nPython\nC++\n...\n...\nSoftware 2.0\nDifferentiable Programming\nSoftware 3.0\nNeuro-Symbolic Programming\nNeuro-Symbolic System\nFigure 7: Evolution of software paradigms: From Software 1.0\u2019s rigid specification in classical programming to\nSoftware 2.0\u2019s data-driven and objective function-focused differentiable programming, leading to Software 3.0\u2019s NeSy\nsystems that emphasize human-centric, interaction-based programming with computational graphs. This progression\nrepresents a shift from explicit task-specific programming to abstract, adaptive systems that cater to dynamic user\npreferences.\nrepresentation. Remember, this was our implicit assumption (see Section 4). For custom objects, it is essential to\ndefine a suitable\nstr\nmethod to cast the object to a string representation while preserving the object\u2019s semantics.\nSymbol Objects Creation and Dynamic Typing\nA Symbol object is a versatile entity that can encapsulate a variety\nof data types and behaviors. The creation of Symbol objects is facilitated through a metaclass, which enables the\ndynamic typing of these objects to inherit behaviors from a collection of primitives. This dynamic typing system\nis important for extending the functionality of Symbol objects beyond simple data containers; they contain specific\nbehaviors appropriate for the operations they will perform. For instance, a Symbol object may possess the behaviors\nof arithmetic computations, string manipulations, or even logical comparisons, depending on the defined primitives.\nType Inheritance and Expression Creation\nType inheritance in SymbolicAI is leveraged to create new expressions,\nwhich are specialized forms of Symbol objects designed to represent parts of a computational graph. Expressions\nextend the capabilities of Symbol objects by providing a structured way to define complex functionalities that can\nlater be evaluated to produce new Symbol objects or modify existing ones. In SymbolicAI, an Expression object\n26\n[Viewport Engines]\nNeuro-Symbolic \nComputation Engine\nData Sources\nand \nCompute Engines\nSearch Engine\nDatabase Engine\nSymbolic Engine\n...\nComputation Stack\nRendering Engine\nSpeech Engine\nHuman Interface\n...\nJVM\nMac\nWin\nUnix\nLinux\nC# \nCLR\nJava\nPython\nNeuro-Symbolic Interface\nC++\nFigure 8: The illustration demonstrates the integration of Neuro-Symbolic computation within the contemporary\nprogramming stack. Probabilistic programming paradigms are embedded natively alongside traditional languages\nand environments, facilitated by interfaces to various data sources, compute engines, and human interaction tools,\nstreamlining their adoption in practical and research applications.\ninherits the properties of Symbol objects while also being able to define its own unique behavior through the use of a\nforward method, which is analogous to a computational graph node\u2019s evaluation function. Figure 9 gives an overview\nof an exemplary inheritance branch. Each Expression must feature a forward method, which must be overwritten\nto define its behavior. The inherited\ncall\nmethod invokes the forward method, evaluating the expression and\nreturning the result. This design pattern facilitates lazy evaluation of expressions, allowing for complex composition\nof expressions.\nInherited from the Symbol class, the sym return type and static context properties establish the context in\nwhich the current Expression operates. The static context impacts all operations of the current Expression\nsubclass, while the sym return type guarantees the acquisition of the desired return object type post-evaluation.\nTypically, this returns the current type but can be configured to return a different type. A more in-depth examination\nof both notions will be provided in the following section.\nSymbol\nExpression\nStyle\nCompose\nTemplate\n...\nSequence\n1\n0..*\nFigure 9: Class diagram showing the inheritance and composition relationships among Symbol, Expression, and\nother inherited classes. Symbol serves as a base class for Expression where all the other types are derived from. Other\ntypes may contain or associate with zero or more Symbol types. For example, we illustrate how the Sequence derives\nfrom Expression and the multiplicity \u20190..*\u2019 indicates that a Sequence can contain any number of Expression\ninstances.\nUtilizing Decorators for Operation Definition\nDecorators serve as a bridge between the declarative nature of sym-\nbolic operations and the imperative execution model of programming languages. By augmenting function definitions\n27\nwith decorators, the framework can dynamically assign operations to Symbol or Expression objects, which are then\ninterpreted by the underlying NeSy engine or traditional solvers.\nFor example, the @core.logic decorator can be used to augment a Symbol object with the capability to perform\nlogical and, or, or not operations contextually. Similarly, the @core.combine decorator allows the framework to\ndefine the semantics of combining or adding two symbolic values, regardless of their underlying data representations.\n1 # Example of using decorators to define logical operations\n2 @core.logic(operator=\u2019and\u2019)\n3 def _some_logic(self, other):\n4\n# implementation logic here\n5\npass\nAspect-Oriented Programming\nThe aspect-oriented programming paradigm offers a functional approach for ex-\ntending or modifying the behavior of functions or methods without altering their code directly. This adheres to the\nprinciples of modularity and separation of concerns, as it allows for the isolation of specific functionalities while\nmaintaining the original function\u2019s core purpose. By wrapping the original function, decorators provide an efficient\nand reusable way of adding or modifying behaviors. For instance, SymbolicAI integrates the zero- and few-shot\nlearning with default fallback functionalities of pre-existing code.\nThe use of decorators brings several advantages (Beazley, 2009; Martelli et al., 2005; Summerfield, 2010; Lutz, 2013):\n\u2022 Reusability: Decorators promote code modularity, enhancing code reusability and contributing to software\nmaintainability.\nThis advantage is particularly salient when managing a variety of operations, reducing\nredundancy and simplifying the integration of new functionalities.\n\u2022 Composition: Decorators support function composition, allowing developers to construct complex function-\nalities from pre-existing code blocks without the need to expand the codebase or rely on complex inheritance\nhierarchies.\n\u2022 Adaptability: Using decorators we can easily modify or extend the behavior of operations without changing\ntheir core implementation. This flexibility facilitates the generation of adaptive workflows and reliable fallback\nmechanisms when experimental implementations do not fulfill required constraints.\nSymbol Class and Computational Graph Elements\nA computational graph in the SymbolicAI framework is an\nassembly of interconnected Symbol objects, each encapsulating a unit of data and the operations that can be performed\non it. The exchange between these symbols forms a highly modular and interpretable system, capable of representing\ncomplex workflows.\nThe Symbol class is an abstraction representing data and context. It holds not only the value itself, but metadata that\nguides its transformation and interpretation. Through inheritance and compositionality, the Symbol can be extended\ninto more complex expressions, and becoming nodes in a computational graph. Each Symbol instance can optionally\ncontain a reference to its parent and children, naturally forming a directed graph structure where the nodes are symbols\nand edges represent relationships between a symbol and its derivative computations.\nThe Linker class, is a metadata subclass, and tracks relationships and results, effectively annotating the graph with\nexecution details. It keeps records of nodes\u2019 keys, allowing quick retrieval of related computational outcomes within\nthe graph, and aids in tasks such as error tracing and debugging.\nA central concept in this structure is the notion of root, which points to the origin of the computational sequence.\nAccessing the root allows backtracking through the graph, making it possible to aggregate results and inspect the flow\nof computation that led to the current node.\nThe computational graph\u2019s structure is further enriched by properties like nodes, edges, and graph itself, which\ncollectively enable the comprehensive query of the computation\u2019s topology. These properties are used internally to\nenable graph visualizations, which are useful for debugging and analysis.\nExpression of a Computational Graph\nIn practice, consider the Expression class, which extends the functionality\nof \u2018Symbol\u2018. When composing a Sequence of Expression objects, we are effectively composing operations in a\npredetermined order.\nFor instance, an expression such as:\n1 Sequence(\n28\n2\nClean(),\n3\nTranslate(),\n4\nOutline(),\n5\nCompose(),\n6 )\nrepresents a procedure that first cleans data, then translates it, outlines the essential information, and composes it into\na finalized form. When this sequence is executed, the operations unfold in the exact order specified, with each step\nreceiving the output of its predecessor as input and passing its result to the successor.\nGenerating entire workflows\nSymbolicAI allows the creation of entire workflows, such as writing scientific papers.\nThe following example defines a Paper expression that takes in a sequence of expressions which are executed in\nsequentially. The Method expression contains a Source expression, which addresses the code base of the actual\nmethod of the paper. The RelatedWork expression contains a sequence of Cite expressions which are executed in\nparallel and are used to define the context of the related work section. Each expression in the sequence of expressions\nfrom Paper takes in the context of its predecessors. All expressions also link their results to a global linker object,\nwhich is used after the execution to retrieve individual results from distinct expression nodes. In Figure 10 we show\nthe resulting computational graph of the Paper expression.\n1 # define the computational graph\n2 expression = Paper(\n3\nMethod(\n4\n# link to original code base where the main method is defined\n5\nSource(file_link=\u2019/path/to/.../file\u2019),\n6\n),\n7\n# gather resources and write the related work\n8\nRelatedWork(\n9\nCite(file_link=\u2019/path/to/.../file\u2019),\n10\nCite(file_link=\u2019/path/to/.../file\u2019),\n11\n...\n12\n),\n13\n# write the abstract and title\n14\nAbstract(),\n15\nTitle(),\n16 )\n17 # run the graph\n18 paper_result = expression(\u2019Write a scientific paper\u2019)\n19 # access linker to retreive the results from the method expression\n20 method = expr.linker.find(\u2019Method\u2019)\n21 # print result of the method expression\n22 print(method)\nD\nInstallation\nThe installation of the SymbolicAI framework is straightforward and can be done through the Python package manager\npip. To install SymbolicAI, open a terminal and execute the following command in your current python environment:\n1 pip install symbolicai\nThis command will install the latest version of SymbolicAI along with its core dependencies, enabling the integration\nof the framework into Python applications. If you intend to use the framework with local engines4, or with engines\npowered by external APIs such as OpenAI\u2019s API, additional installation steps are required.\nD.1\nEngine Configuration\nBefore the first run, it is necessary to configure the required modules and optionally set necessary API keys to activate\nthe respective engines. This can be done in multiple ways, but we recommend doing it through the configuration\nwizard by running this command in the terminal:\n4 The local engines are experimental and are run on your local machine. For more details, refer to the \u201dLocal Neuro-Symbolic\nEngine\u201d section in the documentation.\n29\nFigure 10: We illustrate the computational graph for the Paper expression. Each node represents an instance of an\nexpression with distinct properties and behaviors. The edges denote the reference relationship between expressions.\nThe blue highlighted nodes mark the main sequence of expressions, used to create the paper, such as Method,\nRelatedWork, Abstract, etc.\n1 symwzd\nThis step is essential to register the engines internally for subsequent runs.\nFor instance, SymbolicAI includes OpenAI\u2019s GPT models as NeSy engine. To only set or change OpenAI API keys,\nthe following command is used before starting a SymbolicAI instance:\n1 # Linux / MacOS\n2 export OPENAI_API_KEY=\"<OPENAI_API_KEY >\"\nAfter setting up the API keys, the SymbolicAI library is imported in Python using:\n1 import symai\nFor more low-level changes, we store everything under the $HOME/.symai folder, such as the symai.config.json,\nwhich stores every key, both registered and not registered.\nD.2\nOptional Installations\nThe SymbolicAI framework is designed to leverage multiple engines for a variety of operations. To fully utilize\nthese capabilities, you may install additional dependencies or set up the optional API keys for specific engines like\nWolframAlpha, SerpApi, and others. In Figure 11 we conceptually outline the connection between the utilization of an\nLLM and its interact with other tools and solvers. Instructions and operations can be initiated by any user, pre-scripted\nknowledge base or learned meta agent.\n30\nLarge Language Model\nLanguage Interface (Output)\nLanguage Interface (Input)\nSymbolic Engine (WolframAlpha)\nSearch Engine (Google)\nSpeech Engine (Whisper)\n...\nUser / Knowledge Base / Meta-Learner\nOperation\nConditions &\nConstraints\nTool Interface\nFigure 11: The SymbolicAI framework integrates a Large Language Model (LLM) with diverse tools and solvers\nthrough a conceptual interaction stack. The framework enables operations initiated by users, knowledge bases, or\nmeta-learners to be processed by the LLM, which interfaces with specialized engines such as WolframAlpha and\nWhisper via conditions and constraints, enhancing the AI\u2019s problem-solving capabilities.\nFor instructions on additional installations, including the support of optional engines, refer to the documentation\nprovided with the framework. This documentation will give detailed steps on installing optional dependencies and\nconfiguring additional API keys.\nD.3\nBenchmark\nTo run the benchmark evaluation you first need to install the ExtensityAI/benchmark plugin:\n1 sympkg i ExtensityAI/benchmark\nThen you can go to the $HOME/.symai/packages/ExtensityAI/benchmark directory and run the benchmark\nthrough:\n1 python test.py\nTo compile models optimized for your machine, follow the instructions from the ExtensityAI/benchmark repository.\nE\nImplementation Details\nLet us now define some Symbol objects and perform some basic manipulations.\nE.1\nFuzzy Comparison\nFor instance, let\u2019s consider the use of fuzzy5 comparisons.\nWithin SymbolicAI, it enables more adaptable and\ncontext-aware evaluations, accommodating the inherent uncertainties and variances often encountered in real-world\ndata.\n1 import numpy\n2\n3 s = symai.Symbol(\u20193.1415...\u2019)\n4 s == numpy.pi\n1 :[Output]:\n2 True\n5 Not related to fuzzy logic, which is a topic under active consideration.\n31\nE.2\nDynamic Casting\nBy enabling sentence subtraction and dynamic casting within SymbolicAI, we utilize the generalization capability of\nNeSy engines to manipulate and refine text-based data, creating more meaningful and contextually relevant outcomes.\nThe integration of dynamic casting with Symbol objects in our API allows the users to perform operations between\nSymbol objects and various data types, such as strings, integers, floats, lists, etc. without compromising on readability\nor simplicity.\n1 s = symai.Symbol(\u2019Hello my enemy\u2019)\n2 s - \u2019enemy\u2019 + \u2019friend\u2019\n1 :[Output]:\n2 <class \u2019symai.expressions.Symbol\u2019>(value=Hello my friend)\nE.3\nTranslation\nIn today\u2019s increasingly interconnected world, translation between languages is fundamental, making it an essential\nfeature.\n1 s = symai.Symbol(\"Welcome to our tutorial.\")\n2 s.translate(\u2019German\u2019)\n1 :[Output]:\n2 <class \u2019symai.expressions.Symbol\u2019>(value=Willkommen zu unserem Tutorial.)\nE.4\nFiltering, Ranking, Extraction\nIncorporating data-agnostic operations like filtering, ranking, and pattern extraction into our API allow the users to\neasily manipulate and analyze diverse data sets.\n1 s = symai.Symbol(numpy.array([1, 2, 3, 4, 5, 6, 7]))\n2 s.rank(measure=\u2019numerical\u2019, order=\u2019descending\u2019)\n1 :[Output]:\n2 <class \u2019symai.expressions.Symbol\u2019>(value=[\u20197\u2019, \u20196\u2019, \u20195\u2019, \u20194\u2019, \u20193\u2019, \u20192\u2019, \u20191\u2019])\nE.5\nImplications\nOne of the main objectives behind developing SymbolicAI was to facilitate reasoning capabilities in conjunction with\nthe statistical inference inherent in LLMs. Consequently, we can carry out deductive reasoning operations utilizing\nthe Symbol objects. For instance, it is feasible to establish a series of operations with rules delineating the causal\nrelationship between two symbols. The subsequent example illustrates the utilization of the & operator to compute the\nlogical implication derived from the interaction of two distinct symbols.\n1 s1 = symai.Symbol(\u2019The horn only sounds on Sundays.\u2019)\n2 s2 = symai.Symbol(\u2019I hear the horn.\u2019)\n3 s1 & s2\n1 :[Output]:\n2 <class \u2019symai.expressions.Symbol\u2019>(value=It is Sunday.)\nIn the above example, the & operator overloads the and logical operator and extends its functionality. Furthermore, we\ncan establish more sophisticated logical operators for and, or, and xor that can be grounded in formal proofs, as well\nas utilize the NeSy engine to parse data structures before evaluating the expressions. This enables the definition of\nbespoke operations for executing intricate and robust logical operations, incorporating constraints to validate outcomes\nand guide the computation towards the desired behavior.\n32\nE.6\nCustom operations\nThe following example demonstrates how to define a custom == operation by overriding the\neq\nmethod and providing\na custom prompt object with a list of examples:\n1 import symai\n2\n3 class Demo(symai.Symbol):\n4\ndef __eq__(self, other) -> bool:\n5\n# define nested function\n6\n@symai.core.equals(examples=symai.Prompt([\n7\n\"1 == \u2019ONE\u2019 =>True\",\n8\n\"\u2019six\u2019 == 7 =>False\",\n9\n\"\u2019Acht\u2019 == \u2019eight\u2019 =>True\",\n10\n...\n11\n]))\n12\ndef _func(_, other) -> bool: # [optional] cast return type\n(1. below)\n13\nreturn False\n# [optional] default behavior on failure\n(2. below)\n14\nreturn _func(self, other)\nAs illustrated in the example, this is also the method we used to implement basic operations in Symbol, namely by\ndefining local functions that are then decorated with the respective operation decorator from the symai.core.py file.\nThe symai.core.py is a collection of pre-defined operation decorators that can be quickly applied to any function.\nWe use locally defined functions instead of directly decorating the main methods for two reasons:\n1. We want to cast return types of the operation outcome to symbols or other derived classes thereof.\n2. We do not necessarily want all of our operations to be sent to the NeSy engine and might need to implement\na default behavior.\nThis is achieved using the sym return type method, which can provide contextualized behavior based on the defined\nreturn type. More details can be found in the actual Symbol class.\nIn the context of LLMs, zero- and few-shot learning domains have emerged as essential techniques (Yao et al., 2023b;\nShinn et al., 2023; Kim et al., 2023; Wei et al., 2022b; Lyu et al., 2023; Pitis et al., 2023; Madaan et al., 2023; Wang\net al., 2022; Ye et al., 2023)6 to enable models to generalize from limited training data and adapt to new tasks without\nrequiring extensive retraining. This capability to learn and perform tasks with minimal examples is highly desirable,\nas it reduces the need for large labeled data sets and allows for faster deployment in new applications. In this section,\nwe demonstrate how our Symbolic API incorporates Python decorators to define custom operations in the zero- and\nfew-shot domains.\nConsider the following example, where we define a custom operation to generate a random integer between 0 and 10\nusing the Symbolic API and Python decorators:\n1 import symai\n2\n3 class Demo(symai.Symbol):\n4\ndef __init__(self, value = \u2019\u2019) -> None:\n5\nsuper().__init__(value)\n6\n7\n@symai.core.zero_shot(prompt=\"Generate a random integer between 0 and 10.\",\n8\nconstraints=[\n9\nlambda x: x >= 0,\n10\nlambda x: x <= 10\n11\n])\n12\ndef get_random_int(self) -> int:\n13\npass\nIn this example, the @symai.core.zero shot decorator is used to define a custom operation that does not require\nany examples, as the prompt is expressive enough. The zero shot decorator takes in two arguments: prompt and\nconstraints. The prompt defines the conditioning for our desired operation behavior, while the constraints are\nused to validate the computed outcome, ensuring it meets our expectations. If the constraints are not fulfilled, the\nimplementation would resort to the specified default implementation or the default value. If neither is provided, the\n6 This is by no means an exhaustive list, we only point the reader to some very influential and recent research.\n33\nSymbolic API raises a ConstraintViolationException. The return type in the example is defined as int. The\nresulting value from the wrapped function must be of type int because of the specific implementation of the auto-\ncasting realized through ->. If the cast fails, the Symbolic API raises a ValueError. If no return type is specified, the\nreturn type defaults to Any.\nThe @symai.core.few shot decorator is a generalized version of @symai.core.zero shot and is used to define\ncustom operations requiring examples. The function signature of the few shot decorator is as follows:\n1 def few_shot(prompt: str,\n2\nexamples: Prompt,\n3\nconstraints: List[Callable] = [],\n4\ndefault: Any = None,\n5\nlimit: int = 1,\n6\npre_processor: Optional[List[PreProcessor]] = None,\n7\npost_processor: Optional[List[PostProcessor]] = None,\n8\n**wrp_kwargs):\nThe behavior of the prompt and constraints attributes is similar to the zero shot decorator. The examples and\nlimit arguments are new, with examples defining a list of instructions conditioning the NeSy engine, and limit\nspecifying the maximum number of examples returned. The pre processor and post processor arguments accept\nlists of PreProcessor and PostProcessor objects, respectively, which are used to process the input before being fed\ninto the NeSy engine and the output before being returned to the user. The wrp kwargs argument passes additional\narguments to the wrapped method, streamlining them towards the NeSy engine, or other engines.\nE.7\nPrompting\nIn this section, we discuss the design of prompts and their role in shaping the behavior of operations. Prompts serve as\ncontainers for information necessary to define specific operations, and the Prompt class serves as the base class for all\nthe other Prompt classes. Consider the following example, where we define a Prompt for comparing two values using\nthe NeSy engine. In it, when the <= operation on two Symbol objects will be resolved, the NeSy engine evaluates them\nin the context of the CompareValues prompt.\n1 class CompareValues(symai.Prompt):\n2\ndef __init__(self) -> symai.Prompt:\n3\nsuper().__init__([\n4\n\"4 > 88 =>False\",\n5\n\"-inf < 0 =>True\",\n6\n\"inf > 0 =>True\",\n7\n\"4 > 3 =>True\",\n8\n\"1 < \u2019four\u2019 =>True\",\n9\n...\n10\n])\n1 res = symai.Symbol(1) <= symai.Symbol(\u2019one\u2019)\n1 :[Output]:\n2 True\nThis evaluation returns True, as the fuzzy comparison operation conditions the engine to compare the two Symbol\nobjects based on their semantic meaning. More generally, the semantics of Symbol operations may vary depending\non the context hierarchy of the Expression class and the operations used. We used three main prompt designs:\nContext-based Prompts, Operational Prompts, and Templates. Prompts can be curated either through inheritance or\ncomposition. For instance, the static context can be defined by inheriting from the Expression class and overwriting the\nstatic context property, while an Operation and Template prompt can be created by providing a PreProcessor\nto modify the input data.\nWe will now provide a more detailed explanation for each prompt design:\n1. Context-based Prompts are considered optional and can be defined in a static manner, either by sub-\nclassing the Expression class and overwriting the static context property, or at runtime by updating the\ndynamic context property or passing a payload kwargs to a method. Below is an example of using the\npayload kwargs through the method signature:\n34\n1 # creating a query to ask if an issue was resolve or not\n2 s = symai.Symbol(\"<some-community -conversation >\")\n3 q = s.query(\"Was the issue resolved?\")\n4 # write manual condition to check if the issue was resolved\n5 if \u2019not resolved\u2019 in q:\n6\n# do a new query but payload the previous query answer to the new query\n7\ns.query(\"What was the resolution?\", payload=q)\n8\n...\n9 else:\n10\npass # all good\n11\nRegardless of how the context is set, the contextualized prompt defines the desired behavior of Expression\noperations. For instance, if we want to operate in the context of a DSL without having to overwrite each base\nclass method, we can use this approach7.\n2. Operational Prompts define the behavior of an atomic operation and are therefore mandatory to express the\nnature of such an operation. For example, the + operation is used to add two Symbol objects together, and\nthe + operation prompt explains its behavior. The examples kwargs provide another optional structure that\nconditions the NeSy engine with a set of instructions.\n3. Template Prompts are optional and encapsulate the resulting prediction to enforce a specific format. For\nexample, to generate HTML tags, we can use a curated <html>...</html> template. This template enforces\nthe NeSy engine to begin the generation process already in the context of an HTML tags format and not\nproduce irrelevant descriptions about its task.\nE.8\nComplex expressions\nWe will now attempt to obtain logical answers based on questions of the kind:\n\u2022 A line parallel to y = 4x + 6 passes through (5, 10). What is the y-coordinate of the intercept?\n\u2022 Bob has two sons, John and Jay. Jay has one brother and father. The father has two sons. Jay\u2019s brother has a\nbrother and a father. Who is Jay\u2019s brother?\n\u2022 Is 1000 bigger than 1063.472?\nTo solve these tasks, we would initially employ a series of operations to identify the most suitable engine for handling\nthe specific requirements. Subsequently, we would prepare the input tailored to the selected engine.\n1 val = \"<one of the examples above>\"\n2\n3 # First define a class that inherits from the \\texttt{Expression} class\n4 class ComplexExpression(symai.Expression):\n5\n# write a method that returns the causal evaluation\n6\ndef causal_expression(self):\n7\npass\n8\n9 # instantiate an object of the class\n10 expr = ComplexExpression(val)\n11 # set WolframAlpha as the main expression engine to use\n12 expr.command(engines=[\u2019symbolic\u2019], expression_engine=\u2019wolframalpha\u2019)\n13 # evaluate the expression\n14 res = expr.causal_expression()\nThe implementation of causal expression could in principle look like this:\n1 def causal_expression(self):\n2\n# very which case to use \u2018self.value\u2018 contains the input\n3\nif self.isinstanceof(\u2019mathematics\u2019):\n4\n# get the mathematical formula\n5\nformula = self.extract(\u2019mathematical formula\u2019)\n6\n# verify which problem type we have\n7\nif formula.isinstanceof(\u2019linear function\u2019):\n7 See more details in this notebook.\n35\n8\n# prepare for wolframalpha\n9\nquestion = self.extract(\u2019question sentence\u2019)\n10\nreq = question.extract(\u2019what is requested?\u2019)\n11\n# get coordinate point / could also ask for other points\n12\nx\n= self.extract(\u2019coordinate point (.,.)\u2019)\n13\n# concatenate to the question and formula\n14\nquery = formula | f\u2019, point x = {x}\u2019 | f\u2019, solve {req}\u2019\n15\nres = self.expression(query) # send prepared query to wolframalpha\n16\n17\nelif formula.isinstanceof(\u2019number comparison\u2019):\n18\nres = formula.expression() # send directly to wolframalpha\n19\n20\n... # more cases\n21\n22\nelif self.isinstanceof(\u2019graph construction\u2019):\n23\nsentences = self / \u2019.\u2019 # first split into sentences\n24\ngraph = {} # define graph\n25\nfor s in sentences:\n26\nsym = symai.Symbol(s)\n27\nrelations = sym.extract(\n28\n# extract and split by pipe\n29\n\u2019connected entities (e.g. A has three B => A | A: three B)\u2019) / \u2019|\u2019\n30\nfor r in relations:\n31\n... # add relations and populate graph; reading suggestion\n32\n33\n... # more cases\n34\n35\nreturn res\nThe aforementioned example demonstrates the utilization of the causal expression method, which allows us to\nextract information that can be resolved either manually or through external solvers, say WolframAlpha. Initially, when\nutilizing the GPT-3 backend, we anticipated a significant engineering effort to develop such a complex expression, as\nthe GPT-3 backend frequently struggled with accurate information extraction and comparison resolution. However, we\nremained confident in the field\u2019s progress, specifically with fine-tuned models like ChatGPT utilizing RLHF. We were\ndelighted to witness these challenges being further tackled through the latest GPT-4 model (OpenAI, 2023).\nFurthermore, it is worth highlighting that, given sufficient data, we could refine methods for information extraction or\nknowledge graph construction from natural language, enabling more intricate reasoning tasks, such as those previously\nmentioned. We also direct readers to recent publications on Text-to-Graph translations, especially the very influential\nCycleGT (Guo et al., 2020). This approach allows us to answer queries by simply traversing the graph and extracting\nthe required information.\nLastly, recent research (K\u0131c\u0131man et al., 2023; Ellis, 2023) has demonstrated that algorithms based on GPT-3.5 and\nGPT-4 establish new state-of-the-art accuracy on multiple causal benchmarks, while also exhibiting unique capabilities\npreviously considered exclusive to humans, such as generating causal graphs and identifying background causal context\nfrom natural language. This points to the potential for LLMs to be used alongside existing causal methods as proxies\nfor human domain knowledge, reducing human effort in setting up causal analyses and ultimately accelerating the\nwidespread adoption of causal methods. Moreover, recent advances in LLMs have opened new frontiers for research,\npractice, and adoption of causal reasoning, transforming the way causal analysis is conducted and broadening the scope\nof applications for our framework.\nOne of the most prominent illustrations of this concept is exhibited by Word2Vec (Mikolov et al., 2013). Word2Vec\ngenerates dense representations of words by training a shallow neural network to predict a word based on its neighboring\nwords within a text corpus. These resulting vectors are extensively utilized in various natural language processing\napplications, including sentiment analysis, text classification, and clustering.\nDrawing parallels with Word2Vec, our objective is to execute contextualized operations on different symbols. However,\nthe key distinction lies in the fact that we operate within the natural language domain, as opposed to a vector space.\nConsequently, this grants us the capability to conduct arithmetic on words, sentences, paragraphs, and the like, while\nsimultaneously validating the outcomes in a human-readable format.\nThe following example, we illustrate the methodology for evaluating such an expression through a string representation:\n1 s = symai.Symbol(\u2019King - Man + Woman\u2019)\n36\n2 s.expression()\n1 :[Output]:\n2 <class \u2019symai.expressions.Symbol\u2019>(value=Queen)\nIn contrast to the Symbol object, the Expression represents a non-terminal symbol. It allows for further evaluation\nand extends the Symbol class by overwriting the\ncall\nmethod. It serves as the foundation for all other expressions\nand possesses additional capabilities, namely to fetch data from URLs, search the internet, or open files. These\noperations are intentionally separated from Symbol, as they do not utilize the value attribute of the Symbol class.\nE.9\nComposition\nE.10\nSequences\nSequences offer a multitude of advantages in the realm of Expression objects, as they facilitate the creation of\nmore sophisticated structural configurations. By embodying the Sequence expression, multiple expressions can be\neffectively evaluated at runtime, thus enhancing the flexibility, modularity, and adaptability of the framework.\n1 # first import all expressions\n2 from symai.components import *\n3 # define a sequence of expressions\n4 Sequence(\n5\nClean(),\n6\nTranslate(),\n7\nOutline(),\n8\nCompose(),\n9 )\nE.11\nStreams\nAs demonstrated earlier, creating contextualized prompts refines the behavior of operations in the NeSy engine.\nHowever, this also consumes a considerable portion of the available context size.\nGiven a limited context size,\nthis constraint may pose challenges. Fortunately, the Stream processing expression offers a solution by opening a\ndata stream and performing chunk-based operations on the input stream. Stream expressions can encapsulate other\nexpressions. For instance, chunks can be managed using a Sequence expression, which permits multiple compositional\noperations in a sequential manner. The example below illustrates the definition of a Stream expression:\n1 Stream(\n2\nSequence(\n3\nClean(),\n4\nTranslate(),\n5\nOutline(),\n6\nEmbed()\n7\n)\n8 )\nIn this case, a stream is opened and a Sequence expression is passed, which cleans, translates, outlines, and embeds\nthe input. Internally, the stream operation estimates the available model context size and segments the lengthy input\ntext into smaller chunks transmitted to the inner expression. The returned object type is a generator.\nThe limitation of this approach is that the resulting chunks are processed independently, lacking shared context or\ninformation among them. To address this, the Cluster expression can be employed, merging the independent chunks\nbased on their similarity, as it illustrated in Figure 12. By merging individual chunks by clustering their contents,\ncontextually related information can be consolidated in a meaningful manner. Additionally, the clustered information\ncan be labeled by streaming through each cluster\u2019s content and extracting the most pertinent labels, yielding interpretable\nnode summaries.\nThe complete example is depicted as follows:\n1 stream = Stream(\n2\nSequence(\n3\nClean(),\n37\n...\n...\nData Chunk 1\nData Chunk 2\nData Chunk 3\nSummarize 1\nSummarize 2\nSummarize 3\nData Stream\nCluster 1\nCluster 2\nCompose\nFigure 12: Stream processing expression in NeSy engine, illustrating data stream segmentation into chunks, each\nundergoing operations like cleaning, outlining, and embedding. The Cluster expression then merges chunks based on\nsimilarity, allowing contextually related information to be consolidated meaningfully. Node summaries are generated\nby extracting key labels from each cluster\u2019s content, overcoming context size limitations and maintaining shared\ninformation among processed chunks.\n4\nTranslate(),\n5\nOutline(),\n6\n)\n7 )\n8\n9 s\n= symai.Symbol(\u2019<some long text>\u2019)\n10 res\n= symai.Symbol(list(stream(s)))\n11 expr = Cluster()\n12 expr(res)\nSubsequently, this process can be recursively repeated on each summary node to construct a hierarchical clustering\nstructure. As each node represents a summarized subset of the original information, the summary can function as an\nindex. The resulting tree can be utilized to navigate and retrieve the original information, transforming the large data\nstream problem into a search problem. Alternatively, vector-based similarity searches can be employed to identify\nsimilar nodes. For searching within a vector space, dedicated libraries such as Annoy (Spotify, 2017), Faiss (Johnson\net al., 2019), or Milvus (Wang et al., 2021a) can be used.\nIn summary, Stream expressions offer the advantage of processing large data streams in a more efficient and organized\nmanner, while also enabling the integration with other expressions like Sequence and Cluster expressions. These\ncombinationsallowforamoreeffectiveapproachtohandlingcontextlimitations, promoting theextractionofmeaningful\ninformation and improving the overall performance of the framework.\nE.12\nError handling, debugging, and explainability\nEffective error handling and debugging are essential for ensuring the robustness and reliability of any software system,\nwhile explainability is essential for understanding the underlying behavior of the system, especially in the context of\nAI-driven applications. By developing a system that is both transparent and interpretable, we can gain valuable insights\ninto the performance of the NeSy engines and identify potential areas for improvement.\nE.13\nError handling\nOne of the fundamental aspects of the SymbolicAI API is being able to generate code. Consequently, errors may arise,\nand handling them contextually becomes vital. In pursuit of a self-evolving API, we introduce the Try expression, which\nincludes built-in fallback statements and automatically retries execution after performing dedicated error analysis and\ncorrection. This expression analyzes both the input and the error, conditioning itself to resolve the error by modifying\n38\nthe original code8. If the fallback expression succeeds, the result is returned; otherwise, the process is repeated for\nthe specified number of retries. If the maximum number of retries is reached without resolving the issue, the error is\nraised again.\nConsider the example of executing previously generated code that contains a syntax error. By employing the Execute\nexpression, we can evaluate the generated code, which takes a symbol and proceeds with the execution. Despite the\ninitial failure, the Try expression resolves the syntactic error, returning the corrected and evaluated code:\n1 expr = Try(expr=Execute())\n2 s\n= symai.Symbol(\u2019a = int(\"3,\")\u2019) # some code with a syntax error\n3 expr(s)\n1 :Output:\n2 a = 3\nWhile not all errors can be resolved as easily as the demonstrated syntax error, we continue to explore more sophisticated\nerror handling mechanisms, including the use of streams and clustering to address errors in a hierarchical and contextual\nmanner.\n8 This is similar to the recently released Auto-GPT application.\n39\n"
  },
  {
    "title": "AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning",
    "link": "https://arxiv.org/pdf/2402.00769.pdf",
    "upvote": "16",
    "text": "AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and\nAdapters with Decoupled Consistency Learning\nFu-Yun Wang1\nZhaoyang Huang2 B\nXiaoyu Shi1 Weikang Bian1\nGuanglu Song4\nYu Liu3\nHongsheng Li1 B\n1MMLab, CUHK\n2Avolution AI\n3Shanghai AI Lab\n4SenseTime Research\n{fywang@link, hsli@ee}.cuhk.edu.hk\nzhaoyanghuang@avolutionai.com\nVideo Generation\n2~4 steps\nImage Generation\n30~50 steps\n\u201cboy standing by a rocket, spaceship\u201d\n\u201cRAW face closeup portrait of a man wearing a tuxedo, \nprofessional photography, in blade runner\u201d\nPersonalized Image \nDiffusion Model\n3D Inflation\nImage Consistency Model\nVideo Consistency Model\nImage Diffusion Model\nVideo Diffusion Model\n3D Inflation\nConsistency \nDistillation\nConsistency \nDistillation\nVideo Consistency Model\nReplace Layers\nDiffusion Model\nConsistency Model\nSpatial layers\nTemporal layers\nPersonalized Layer\n1\n2\n3\n4\nFigure 1. AnimateLCM: Animate personalized diffusion models and adapters within 4 steps.\nAbstract\nVideo diffusion models has been gaining increasing at-\ntention for its ability to produce videos that are both co-\nherent and of high fidelity. However, the iterative denois-\ning process makes it computationally intensive and time-\nconsuming, thus limiting its applications.\nInspired by\nthe Consistency Model (CM) that distills pretrained im-\nage diffusion models to accelerate the sampling with min-\nimal steps and its successful extension Latent Consistency\nModel (LCM) on conditional image generation, we pro-\npose AnimateLCM, allowing for high-fidelity video gener-\nation within minimal steps. Instead of directly conducting\nconsistency learning on the raw video dataset, we propose\na decoupled consistency learning strategy that decouples\nthe distillation of image generation priors and motion gen-\neration priors, which improves the training efficiency and\nenhance the generation visual quality. Additionally, to en-\nable the combination of plug-and-play adapters in stable\ndiffusion community to achieve various functions (e.g., Con-\ntrolNet for controllable generation). we propose an effi-\ncient strategy to adapt existing adapters to our distilled text-\nconditioned video consistency model or train adapters from\nscratch without harming the sampling speed. We validate\nthe proposed strategy in image-conditioned video genera-\ntion and layout-conditioned video generation, all achieving\ntop-performing results. Experimental results validate the\neffectiveness of our proposed method. Code and weights\narXiv:2402.00769v1  [cs.CV]  1 Feb 2024\nwill be made public. More details are available at https:\n//github.com/G-U-N/AnimateLCM.\n1. Introduction\nDiffusion models [3, 10, 11, 25] have achieved unprece-\ndented success in image generation [11, 25] and video\ngeneration [4, 12, 20, 37].\nThe high-quality genera-\ntion achieved by the diffusion model relies on the itera-\ntive denoising process that gradually transforms a high-\ndimensional Gaussian noise into real data.\nOne of the\nmost representative models for image generation is Stable\nDiffusion [25] (SD), which relies on a variational autoen-\ncoder (VAE) to build a mapping between the real image\nand down-sampled latent features to reduce the cost of gen-\neration and the cross-attention mechanism to achieve text-\nconditioned image generation. Built upon Stable Diffusion,\nmany plug-and-play adapters (e.g., ControlNet) [38] are de-\nveloped and combined to achieve more innovative func-\ntions.\nHowever, the nature of iterative sampling leads to slow\ngeneration and high computational burdens of diffusion\nmodels which are much slower than other generative mod-\nels (e.g., GAN) [5, 36].\nRecently, consistency mod-\nels [33] (CM) have been proposed as a promising alternative\naimed at speeding up the generation process. By learning\nconsistency mappings that maintain self-consistency [33]\non PF-ODE trajectories induced by the pre-trained diffu-\nsion models, CM allows for high-quality image genera-\ntion with a very small number of steps, eliminating the\nneed for computation-intensive iterations. Latent consis-\ntency model [19] (LCM), built upon the SD, can be in-\ntegrated into the Web-UI widely adopted by the commu-\nnity with existing adapters to achieve various functionali-\nties (e.g., real-time image-to-image translation). In contrast,\nalthough video diffusion models are achieving many inspir-\ning progress [4, 7, 21, 28, 29], the acceleration of video\nsampling is still under-explored even more urgent due to\nthe high computational cost of video generation.\nIn this paper, we propose AnimateLCM for high-fidelity\nvideo generation with a minimal number of steps. Follow-\ning LCM, we treat the reverse diffusion process as solving\nclassifier-free guidance (CFG) [9] augmented probability\nflow ODE (PF-ODE) [15, 19, 32], and we train our model to\ndirectly predict the solution of such ODE in the latent space.\nHowever, instead of directly conducting consistency learn-\ning on the raw video data which suffers from low-quality\nissues and requires high training resources, we propose a\ndecoupled consistency learning strategy that decouples the\nconsistency distillation of image generation priors and mo-\ntion generation priors. As shown in Fig. 1, we first conduct\nthe consistency distillation to adapt the image base diffu-\nsion model into the image consistency model, which can\nbe adapted very efficiently. Then we conduct 3D inflation\nto both the image diffusion model and image consistency\nmodel to accommodate 3D video features. Eventually, we\nconduct consistency distillation on video data to obtain the\neventual video consistency model. A specially designed ini-\ntialization strategy is additionally proposed to alleviate po-\ntential feature corruption caused by the inflation process.\nWe empirically show that this process not only boosts the\ntraining efficiency but also enhances the final generation\nquality.\nIn addition, we build the AnimateLCM on SD, and there-\nfore we can achieve innovative generation results by re-\nplacing the spatial weights of our trained video consistency\nmodel with the publicly available personalized image diffu-\nsion weights, as shown in the right of Fig. 1. Besides, we\nshow that although most of the adapters in the community\ncan be directly integrated with our trained video consistency\nmodel, they are likely to lose control of details or cause\nflickering in the results.\nTo better suit existing adapters\nfrom the community or train specific adapters from scratch\nwith our video consistency model, we propose an effective\nstrategy that could \u201caccelerate\u201d the adapters without requir-\ning specific teacher models. To show the effectiveness of\nthe strategy, we train an image encoder from scratch with\nour video consistency model, and additionally achieve high-\nfidelity image-to-video generation with minimal steps. We\nadapt the existing layout-conditioned adapters [23, 38] pre-\ntrained with vanilla image diffusion models to achieve bet-\nter compatibility and better controllable video generation\nwith minimal steps. Our main contributions are summa-\nrized as follows:\n\u2022 We propose AniamteLCM allowing for fast and high-\nfidelity video generation. We propose a decoupled dis-\ntillation strategy decoupling the image generation priors\nand motion priors, achieving better training efficiency and\ngeneration quality.\n\u2022 Built upon the stable diffusion weight, we show good\ncompatibility with personalized image diffusion weights\nto achieve high-quality video generation in various styles.\nWe also propose an efficient strategy that adapts the ex-\nisting adapters to better suit our consistency model or\ntrains specific adapters from scratch without harming\nthe sampling speed.\nRelying on this, we additionally\nachieve high-fidelity image-conditioned video generation\nand layout-conditioned video generation.\n2. Related Works\nIn this section, we discuss related diffusion models and\nsampling speeding strategies.\n2.1. Diffusion Models\nDiffusion Models [10, 11, 25], which are also known\nas score-based generative models [15, 32], have achieved\ngreat success in image generation. The iterative sampling\nprogress is guided by the score direction to gradually de-\nnoise the noise-corrupted data. Current successful general-\nized video diffusion models [4, 28, 29, 37], typically reply\nto pre-trained image diffusion models [25] and are trained\nby adding temporal layers to them. Most of them are trained\nwith image-video joint tuning [3, 29] or simply freeze the\nspatial weights [4].\n2.2. Sampling Acceleration\nTo tackle the problem of slow generation in diffusion mod-\nels. Early works on improving the sampling speed focus on\nimproved ODE solvers [15, 17, 18, 39]. The distillation-\nbased acceleration method shows a more promising accel-\neration speed by tuning the original diffusion weights with\na refined scheduler or architecture [22, 27]. The consis-\ntency model [33] is a new version model trained by enforc-\ning the self-consistency property. The latent consistency\nmodel [19] successfully adapts the idea to the stable dif-\nfusion on conditional image generation at latent space.\n3. Preliminaries\nIn this section, we provide a high-level review of diffu-\nsion models and consistency models. Since our model is\nbased on the stable diffusion which is an extended version\nof DDPM [10], we apply the relevant notations in the fol-\nlowing discussion. That is, we treat the discrete forward\ndiffusion process as the continuous-time Variance Preserv-\ning SDE [15]. Note that the discussion could be further\ngeneralized to the other format of definitions of SDE.\nDiffusion models.\nIn DDPM, the training data point\nx0\n\u223c\npdata(x) is gradually perturbed by the discrete\nMarkov chain with the perturbation kernel p(xi | xi\u22121) =\nN(xi; \u221a1 \u2212 \u03b2ixi\u22121, \u03b2iI), i = 1, 2, . . . , N. In this way, the\ndistribution of noisy data at different timestep follows the\ndistribution pi(xi | x0) = (xi; \u221a\u03b1ix0, (1 \u2212 \u03b1i)I), where\n\u03b1i := \u03a0i\nj=1(1 \u2212 \u03b2i). As N \u2192 \u221e, the discrete Markov\nchain converges to the following SDE,\ndx = f(x, t)dt + g(t)dw,\n(1)\nwhere f(x, t) = \u2212 1\n2x\u03b2(t) and g(t) =\np\n\u03b2(t).\nA remarkable property of this SDE is the existence of\nreverse-time ordinary differential equation (ODE), dubbed\nthe Probability Flow (PF) ODE [32], points in whose solu-\ntion trajectories at time t still follow the noisy distribution\npt(x).\ndx =\n\u0002\nf(x, t) \u2212 g2(t)\u2207x log pt(x)\n\u0003\ndt.\n(2)\nIn DDPM, the noise prediction neural network \u03f5\u03b8(xt, t) is\ntrained to remove noise from the current data point, whose\ndirection is to mimic the opposite direction of the score\nfunction \u2207xt log pt(xt) \u2248 \u2212\n1\n\u221a1\u2212\u03b1t \u03f5\u03b8(xt, t).\nConsistency models. Given the above solution trajectory\n{xt}t\u2208[\u03f5,T ] of the PF ODE discussed above, the consistency\nmodel, tries to let all the data points in the same trajectory\nto direct predict the solution of the PF ODE [33]. However,\ninstead of directly setting the solution as the target predic-\ntion, it innovatively proposes to enforce the points at the\ntrajectories to follow the self-consistency property. That\nis, outputs are enforced to be the same for arbitrary pairs\nof (xt, t) belonging to the same PF ODE trajectory, i.e.,\nf(xt, t) = f(xt\u2032, t\u2032) for all t, t\u2032 \u2208 [\u03f5, T].\nTo ensure the boundary condition [33] that for any con-\nsistent function f(\u00b7, \u00b7), we have f(x\u03f5, \u03f5) = x\u03f5, i.e., f(\u00b7, \u03f5)\nis an identity function. Consistency models are typically\nformulated with a skip connection, that is,\nf\u03b8(xt, t) = cskip(t)xt + cout(t)F\u03b8(x, t),\n(3)\nwhere cskip(t) and cout(t) are differentialble functions with\ncskip(\u03f5) = 1 and cout(\u03f5) = 0.\nEventually, to enforce the self-consistency property, a\ntarget model \u03b8\u2212 updated with exponential moving average\n(EMA), i.e., \u03b8\u2212 = \u00b5\u03b8\u2212 + (1 \u2212 \u00b5)\u03b8 is additionally main-\ntained to consistency distillation.\nL(\u03b8, \u03b8\u2212; \u03a6) = Ex,t\nh\nd\n\u0010\nf\u03b8(xtn+1,tn+1), f\u03b8\u2212(\u02c6x\u03d5\ntn, tn)\n\u0011i\n,\n(4)\nwhere d(\u00b7, \u00b7) is a distance measuring function, \u02c6x\u03d5\ntn is ob-\ntained by a one discretization step of a numerical ODE\nsolver with a pretrained diffusion model parameterized as\n\u03d5,\n\u02c6x\u03d5\ntn = xtn+1 + (tn \u2212 tn+1)\u03a6(xtn+1, tn+1; \u03d5).\n(5)\n4. Methodology\nAnimateLCM tames the stable diffusion-based video mod-\nels to follow the self-consistency property, achieving high-\nfidelity video generation within a minimal number of steps.\nFig. 2 illustrates the overall training paradigm of Ani-\nmateLCM, consisting of decoupled consistency learning\nstrategy for effective consistency learning and teacher-free\nadaptation for effective adapters training/finetuning. In the\nfollowing, we first introduce our applied basic adaptations\nof latent diffusion models to latent consistent models in\nSec. 4.1. Later, we introduce our decoupled consistency\nlearning strategy which decouples the distillation of im-\nage generation priors and motion priors in Sec. 4.2, achiev-\ning better training efficiency and generation quality than\nsimple consistency learning on video data.\nIn Sec. 4.3,\nwe illustrate the teacher-free adaptation strategy that bet-\nter suits adapters or train them from scratch without harm-\ning sampling speed, achieving fast and high-quality image-\nconditioned generation and layout-conditioned generation.\nTeacher Diffusion Model\n\ud835\udf4b\nCFG-Augmented\nODE Solver\n\ud835\udc17!\nSelf-Consistency\nForward SDE\nTrainable\nFrozen\nFirst Frame\nTile\nCommunity Adapters\neg., XX-Net, XX-Adapter, etc.\nMulti-Res Encoder\n\ud835\udc17\"!\n\ud835\udc17\"!\"#\nEMA\nConsistency Model\n\ud835\udf3d\nTarget Consistency Model\n\ud835\udf3d\"\nCFG-Augmented\nODE Solver\nOne-step MCMC\nEstimator\n\ud835\udc17!\n\ud835\udc17\"!\n\ud835\udc17\"!\"#\nForward SDE\nForward SDE\n\ud835\udf3a\nEncoder\nFigure 2. High-level overview of AnimateLCM. Left: the overall training paradigm for taming the video generation model to follow consis-\ntency property. Top-Right: the teacher-free adaptation strategy replace the cfg-augmented ode solver with one-step MCMC approximation.\nBottom-Right: the design for training image-to-video adapters from scratch or taming existing adapters in stable diffusion community.\n4.1. From DMs to CMs\nHere we introduce our adaptation of the stable diffusion\nmodel (DM) to the consistency model (CM), basically fol-\nlowing the design of latent consistency model (LCM) [19].\nFrom \u03f5-prediction to x0-prediction. Note that the the sta-\nble diffusion models typically predict the noise added to the\ngiven samples, that is they are the \u03f5-prediction type models.\nHowever, the consistency models aim to directly predict the\nsolution x0 of the PF-ODE trajectory. Since we know that\nxt \u223c N(\u221a\u03b1ix0, 1 \u2212 \u03b1iI), we could parametrize the f\u03b8 as\nfollows:\nf\u03b8(xt, c, t) = cskip(t)xt + cout(t)x\u03b8(xt, c, t) ,\n(6)\nwhere c is the text condition embeddings, and x\u03b8\n=\n\u0010\nxt\u2212\u221a1\u2212\u03b1t\u03f5\u03b8(xt,c,t)\n\u221a\u03b1t\n\u0011\n.\nClassifier-free guidance augmented ODE solver. In sta-\nble diffusion models with parameter \u03d5, classifier-free guid-\nance is an essential strategy for high-quality generation, that\nis,\n\u03f5\u03d5(xt, c, t, w) = (1 + w)\u03f5\u03b8(xt, c, t) \u2212 w\u03f5\u03d5(xt, \u2205, t).\n(7)\nAssuming that the PF ODE solver is denoted as the\n\u03a6(xtn, c, tn, tn+1; \u03d5) which aligns with the forward SDE\nprocess, instead the actual PF ODE solver of stable diffu-\nsion in practice for solving the solution trajectories can be\nrepresented as\n\u03a6w(xtn, c, tn, tn+1; \u03d5) = (1 + w)\u03a6(xtn, c, tn, tn+1; \u03d5)\n\u2212w\u03a6(xtn, \u2205, tn, tn+1; \u03d5).\n(8)\nIn this way, we apply the classifier-free guidance augmented\nODE solver for sampling the adjacent pairs in the same tra-\njectories.\nSample sparse timesteps from dense timesteps. Previous\nworks [19, 22] reveal that the number of discrete points in\nthe trajectory N influences the training efficiency and the\nquality greatly. Typically, smaller N accelerates the training\nand on the contrary, larger N introduces less bias at training.\nThe stable diffusion model typically holds 1000 timesteps\nwhich is relatively too large. To facilitate the training effi-\nciency, we uniformly sample 50 timesteps from the whole\n1000 timesteps for training.\n4.2. Decoupled Consistency Learning\nFor the consistency distillation process, a crucial obser-\nvation is that the final generation quality of the consis-\ntency models is greatly influenced by the data used for\ntraining. However, current publicly available datasets usu-\nally suffer from various problems including low-resolution,\nwatermark, and overly brief or ambiguous captions. Be-\nsides, direct training on large-resolution videos is resource-\nconsuming and is not affordable for most researchers. Con-\nsidering that there are many filtered high-quality image\ndatasets available now, we propose to decouple the distil-\nlation of image generation priors and motion priors.\nTo\nbe specific, we first distill the stable diffusion models into\nimage consistency models on filtered high-quality image-\ntext datasets with high resolutions. To freeze the weights\nof the stable diffusion, we train light LoRA [13] weights at\nlayers of stable diffusion models. The LoRA weights are\ntuned, work as a versatile acceleration module, and have\nbeen shown to be well-compatible with other personalized\nmodels in stable diffusion communities. For inference, the\nLoRA weights are merged with the original weights without\ncorrupting the inference speed.\nAfter gaining the consistency models at the image gen-\neration level, we freeze the weights of stable diffusion and\nLoRA weights on it. To train the consistency models for\nvideo generation, we first inflate the 2D convolution ker-\nnels (e.g., 3\u00d73) to the pseudo-3D convolution kernels (e.g.,\n1\u00d73\u00d73) and add additional temporal layers on it with zero\ninitialization and block level residual connection, that is,\nz\u2032 = Tzero(z) + z,\n(9)\nwhere T denotes the temporal layer, and zero means it is\nzero-initialized.\nTherefore, the outputs of the model at\nthe very first training will not be influenced, which has\nbeen verified a lot to accelerate the training [29, 38]. We\nthen train the temporal layers under the guidance of open-\nsourced video diffusion models extended from stable diffu-\nsion models.\nInitialization strategy. However, noting that the spatial\nLoRA weights are trained to accelerate the sampling with-\nout considering temporal modeling and the temporal mod-\nules are trained in vanilla diffusion ways, thus direct com-\nbining them together tend to causes the representation cor-\nruption at the beginning of training. In this way, how to ef-\nfectively and efficiently combining them with less conflicts\nposes non-trivial challenges. We empirically find an effec-\ntive initialization strategy that can not only borrow the con-\nsistency priors from spatial LoRA weights but also alleviate\nthe negative influence of direct combination. To be specific,\nas shown in Fig. 3, at the beginning of consistency training,\nwe only insert the pre-trained spatial LoRA weights into the\nonline consistency model without inserting them to the tar-\nget consistency model. In this way, the target consistency\nmodel, which works as the learning \u201dinstructor\u201d for the on-\nline model, will not produce corrupted predictions spoiling\nthe online model. During the training, the LoRA weights\nwill gradually accumulates in the target consistency model\nthrough the exponential moving average (EMA) and reach\nthe desired weight after a certain iterations. We empirically\nshow that this strategy greatly boosts the training efficiency.\nGenerally, we apply the loss as\nL(\u03b8, \u03b8\u2212; \u03a6)\n= Ex,t\nh\n\u03bbnd\n\u0010\nf\u03b8(xtn+1, x, tn+1), f\u03b8\u2212(\u02c6x\u03d5\ntn, c, tn)\n\u0011i\n,\n(10)\nWhere \u03bbn is an weight function for different timesteps.\nGiven that the prediction at smaller timestep tn work as the\ntarget for the larger timestep tn+1, we should give more pi-\nority to the learning at smaller timesteps. Therefore, we\ndecay the \u03bb as the n growsm, setting \u03bbn = (1 \u2212 \u03b4 n\nN )\u03b3\nwhere \u03b4 and \u03b3 are the scaling factor controlling the de-\ncaying speed. We apply huber loss as the distance metric\nEMA\nSelf-Consistency\nConsistency Model\n\ud835\udf3d\nTarget Consistency Model\n\ud835\udf3d\"\nSpatial Layer LoRAs \n\u00d7\ud835\udc75\nSpatial Layer LoRAs \n\u00d7\ud835\udc75\nFigure 3. Initialization strategy. We only insert the spatial LoRA\nweights into the online model at the beginning and gradually prop-\nagate the LoRA weights to target model through EMA.\nd(x, y) =\np\n\u2225x \u2212 y\u22252\n2 + c2 \u2212 c with hyper-parameter c,\nwhich produces more smooth gradient and has better ro-\nbustness to outliers.\n4.3. Teacher-Free Adaptation\nThe plug-and-play adapters play an important role in the\nstable diffusion community. However, we observe that even\nthough many adapters trained with image diffusion models\nstill works to some extend but they generally lose control\nin details, as shown in Fig. 6. In this section, we propose a\nsimple yet effective strategy that trains the adapters from\nscratch or accommodates the existing adapters for better\ncompatibility, with which we achieve the image-to-video\ngeneration and controllable video generation with a mini-\nmal number of steps without requiring teacher models.\nFor the training of video consistency models, we typ-\nically rely on the pre-trained video diffusion models\n\u03f5\u03b8(x, c, t) to approximate the score \u2207x log pt(x). Inspired\nby Song and Dhariwal [31], the score could be unbiasedly\nestimated by\n\u2207xt log pt(xt) = \u2212Ex\n\u0014xt \u2212 \u221a\u03b1tx\n1 \u2212 \u03b1t\n\f\f\f\fxt\n\u0015\n,\n(11)\nin stable diffusion models. In this way, for given x, we\ncould simply approximate the score at xt with xt\u2212\u221a\u03b1tx\n1\u2212\u03b1t\n.\nAlthough it is a one-step MCMC approximation of the ac-\ntual score, the experiments show it works well in practice\nfor video generation. Additionally, we show that the esti-\nmation can actually be done with two forward SDE sharing\nthe same perturbation noise at different timesteps. Denote\nthe noise as \u03f50 and assuming that xt = \u221a\u03b1tx+\u221a1 \u2212 \u03b1t\u03f50,\nthe score approximation xt\u2212\u221a\u03b1tx\n1\u2212\u03b1t\nreveals that the noise pre-\ndict is also \u03f50.\nFollowing the widely used ODE solver\nDDIM [30], we could see\nxt\u22121 = \u221a\u03b1t\u22121\n\u0012xt \u2212 \u221a1 \u2212 \u03b1t\u03f50\n\u221a\u03b1t\n\u0013\n+\np\n1 \u2212 \u03b1t\u22121\u03f50\n= \u221a\u03b1t\u22121x +\np\n1 \u2212 \u03b1t\u22121\u03f50.\n(12)\nTherefore, following the above strategy, we could achieve\nteacher-free consistency learning tolerating a certain degree\nof bias.\nImage-to-video. To achieve image-to-video generation, the\nmost crucial part is how to preprocess the input images to\nextract image contexts and how to incorporate the image\ncontexts into the consistency models.\nFor preprocessing the image, we compare the pre-trained\nclip image encoder, light downsample blocks at the pixel\nspace, and light blocks at the latent space. Our observa-\ntion is that the clip image encoder tends to ignore the de-\ntails of the input images and might generate frames that do\nnot conform to the original characters\u2019s detailed features.\nFor downsample blocks at the pixel space, we find that the\ngenerated frames typically follow well with the shape and\ncontour but fail at color consistency. The light blocks at\nthe latent space where the images are first encoded by the\nVAE Encoder achieve the best results, partially due to the\ngeneration process being conducted at the latent space. Be-\nsides, since the U-Net blocks have various spatial dimen-\nsions at different blocks, we also downsample the features\nin the light blocks to better align with the U-Net.\nFor incorporating the image contexts, typical ways try\nthe cross-attention or masked modeling. However, we find\nthat the cross-attention tends to corrupt the spatial informa-\ntion of the original image and thus fails to reproduce the\ninput image and faces slow convergence. For masked mod-\neling, previous works construct a condition the same as the\ntarget video and set the first frame to be the given image\nand the other frames to zero. However, we find that this\nstrategy usually fails to keep the identity of the original im-\nage at later frames, which potentially indicates the tempo-\nral layers might not be adequate to align the information in\nthe first frame to other frames. We achieve image-to-video\nthrough a very simple yet effective way. For incorporating\nthe input image, we first encode the image into the latent\nspace and then repeat the latent at the temporal dimension\nto align the number of frames of video generation. Then,\nthe extracted features are fed to the light blocks to extract\nfeatures at different resolutions, the corresponding features\nare then directly added to the corresponding layers in the U-\nNet. To further refine the generation results, after decoding\nwith the VAE decoder, we additionally align the generated\nvideo with the input image at the HSV space.\nControllable video generation. We surprisingly find that\nthe layout control adapters including T2I-Adapter [23],\nControlNet [38], etc.\ntrained on image diffusion mod-\nels could be directly integrated with our video consistency\nmodel to achieve controllable video generation. However,\nas we claimed and shown in Fig. 6, they might lose control\nof details or cause flickering in results. Luckily, we ver-\nified that the gap could be efficiently narrowed by tuning\nLoRA layers on the existing adapters to achieve better com-\npatibility. As shown in the figure 6, after the teacher-free\nadaptation process, we achieve apparently superior control\nstability and visual quality.\n5. Experiments\n5.1. Experimental Setup\nImplementation details. We apply stable diffusion v1-5\nas the base model for most experiments.\nWe apply the\nDDIM ODE solver for the training. Following the latent\nconsistency model, we uniformly sample 50 timesteps for\ntraining. We apply the stable diffusion v1-5 with the open-\nsourced motion weights [6] as the teacher video diffusion\nmodels. All the experiments except the controllable video\ngeneration are conducted on the publicly available WebVid-\n2M [2] dataset without any augmentation or additional data.\nFor controllable video generation, we train our model on\nthe TikTok dataset [14] with brief text prompts captioned\nby BLIP [16].\nBenchmarks. To evaluate our approach, we follow previ-\nous works, utilizing the widely used UCF-101 [34], a cat-\negorized video dataset curated for action recognition tasks,\nfor validation. We ask the GPT-4 [1] to generate very brief\ncaptions for each category. For each category, we gener-\nate 24 videos with 16 frames in resolution 512 \u00d7 512 and\nthus generate 24 \u00d7 101 videos in total. We follow previ-\nous video generation works, applying FVD [35] and CLIP-\nSIM [8] as the validation metric. For FVD, we randomly\nchoose 2048 videos from the UCF-101 dataset and the gen-\nerated 24 \u00d7 101 videos, respectively. For CLIPSIM, we\nrely on the CLIP ViT-H/14 LAION-2B [24] to compute the\nmean value of the similarities of the brief caption and all the\nframes in the video. As a pioneer work for video genera-\ntion acceleration, there is no available open-sourced weight\nfor testing and comparison. Therefore, we compare Ani-\nmateLCM with the teacher model using the DDIM [30] and\nDPM-Solver++ [18].\n5.2. Qualitative Results\nFig. 7 demonstrates the 4-step generation results of our\nmethod in text-to-video generation, image-to-video gen-\neration, and controllable video generation with different\npersonalized style models including styles of realistic, 2D\nanime, and 3D anime. All of them achieve satisfactory re-\nsults. We also demonstrate the generation results under dif-\nferent number of function evaluations (NFE) in Fig. 7. The\nTable 1. Zero-shot text-to-video generation on UCF-101. Our approach achieves state-of-the-art performance in both FVD and CLIPSIM\nmetrics. AnimateLCM (Realistic) means that we replace the spatial layer weights of the original SD with realistic style personalized model.\nMethods\nFVD \u2193\nCLIPSIM \u2191\n1 Step\n2 Steps\n4 Steps\n8 Steps\n1 Step\n2 Steps\n4 Steps\n8 Steps\nDDIM (Pretrained) [30]\n4940.83\n3218.74\n1944.82\n1209.88\n4.43\n5.26\n14.87\n24.38\nDPM++ (Pretrained) [18]\n2731.37\n2093.47\n1043.82\n932.43\n10.48\n18.04\n26.82\n29.50\nAnimateLCM\n1256.50\n1081.26\n925.71\n910.34\n22.16\n25.99\n28.89\n30.03\nAnimateLCM (Realistic)\n1071.50\n790.99\n929.79\n1081.72\n25.41\n29.39\n30.62\n30.71\nText-to-Video\nImage-to-Video\nControl-Video\nFigure 4. Qualitative generation results. We demonstrate the generation results of AnimateLCM including text-to-video generation, image-\nto-video generation, and controllable video generation.\ngeneration results show that our method well follows the\nconsistency property with different inference steps, main-\ntaining a similar style and motion. We demonstrate good\nvisual quality as well as smooth motion with only 2 infer-\nence steps. As the NFE increases, the generation quality\nincreases accordingly, achieving competitive performance\nwith the teacher model with 25 and 50 inference steps.\n5.3. Quantitative Experiments\nTable 1 illustrates quantitative metrics comparison for An-\nimateLCM and strong baseline methods DDIM [30], and\nDPM++ [17, 18]. AnimateLCM significantly surpasses the\nbaseline methods, especially in the low step regime (1\u223c4).\nAdditionally, all these metrics of AnimateLCM are eval-\nuated without requiring classifier-free guidance (CFG) in-\nw/o decouple\nw/o init\nOurs\n600\n800\n1000\nFVD\n15\n20\n25\n30\nCLIPSIM\n1060.6\n1020.1\n958.9\n18.8\n26.6\n27.7\nFigure 5. Quantitative ablation study on the proposed decoupled\nconsistency learning and the specific initialization strategy.\nstead of 7.5 CFG strength applied for other baselines, thus\nsaving half of the inference peak memory cost and inference\ntime. Additionally, we validate the performance of Ani-\nmateLCM by replacing the spatial weight with a publicly\navailable personalized realistic style model with a good bal-\nance between generation diversity and fidelity, which fur-\nther boosts the performance. It shows the good compatibil-\nity of AniamteLCM with personalized diffusion models.\n5.4. Discussion\nEffectiveness of decoupled consistency learning. We val-\nidate the effectiveness of our proposed decoupled learning\nstrategy with the special design for spatial LoRA initial-\nization. We compare with the two baselines: \u201cw/o decou-\nple\u201d and \u201cw/o init\u201d. \u201cw/o decouple\u201d denotes directly ap-\nplying video consistency learning on the video data without\npretraining the spatial LoRA layers on high-quality image\ndatasets; \u201cw/o init\u201d applies the decoupled consistency learn-\ning but does not apply the initialization strategy in Fig. 3.\nFor a fair comparison of convergence speed, we train the\nspatial LoRA weights for 4 hours on an 8 A800 GPU node.\nWe then train our strategy and \u201c\u2019w/o init\u201d baseline on the\nvideo dataset for an additional 4 hours. We train the \u201cw/o\ndecouple\u201d for 8 hours on the video dataset. As shown in\nFig. 5, our strategy achieves the best performance, signifi-\ncantly surpassing the baseline of \u201cw/o decouple\u201d, showcas-\ning the effectiveness of our decoupled consistency learning.\nEffectiveness of teacher-free adaptation. Fig. 6 illustrates\nthe controllable generation results with direct usage of T2I-\nAdapter [23] on image diffusion models or fine-tuned with\nour proposed teacher-free adaptation. As shown in Fig. 6,\nafter the teacher-free adaptation process, we achieve appar-\nently superior control stability and visual quality.\nProperties of personalized models. The original stable\ndiffusion models are trained on vast text-image pairs, gain-\ning image generation priors. However, they are not fur-\nther human-aligned, thus typically failing to generate im-\nages with good aesthetic scores. Many personalized models\nare obtained through finetuned on high-quality datasets or\npose\nw/o adaptation\nw/o adaptation\nours\npose\nours\nFigure 6.\nComparison of controllable generation w/ and w/o\nteacher-free adaptation. Faces of generations are blurred.\nmerged with different base weights [13, 26]. They gener-\nally produce images with higher aesthetic scores on specific\ndomains. As shown in Fig. 4, our proposed AnimateLCM\nis able to directly apply their weights for high-quality video\ngeneration with different styles including realistic, 2D ani-\nmate, 3D animate, etc. Generally, we find that by applying\nthe weight of personalized models, we are not only able\nto improve the visual quality but also the motion smooth-\nness and consistency. We validate the performance of the\npersonalized weight of Realistic Vision V5.0 on Civitai in\naddition to the original stable diffusion-v1.5, and the results\nare shown in Table 1. We observe a consistent metric in-\ncrease in both the FVD and CLIPSIM, especially in the low-\nstep regime. However, even though we observe an obvious\nvisual quality improvement when increasing the inference\nsteps, the FVD metric becomes worse at 4 steps and 8 steps,\nwhich could potentially be caused by the specific domain\ngap between the personalized model and the UCF101. For\nexample, the personalized models tend to generate clear and\nhigh-quality videos while the UCF101 contains blurry and\nfuzzy content.\n6. Conclusion\nWe present AnimateLCM, an innovative advancement in\nvideo generation acceleration. AnimateLCM relies on a de-\ncoupled consistency learning strategy in that we decouple\nthe consistency distillation of image generation prior and\nmotion generation prior with a specific initialization design\nto alleviate the gap between the two stages, which achieves\na good trade-off on training efficiency and generation qual-\nity. We additionally propose a simple yet effective teacher-\nfree adaption strategy that could better accommodate exist-\ning adapters in stable diffusion communities or train them\nfrom scratch, thus achieving more innovative generation\nresults. Extensive experiments validate the effectiveness.\nLimitations: AnimateLCM faces limitations for generating\nsamples in one step, which might result in blurry or artifact-\nladen outcomes..\nAnimate LCM\nTeacher Model (DDIM)\nNFE = 2\nNFE = 4\nNFE = 8\nNFE = 12\nNFE = 50 NFE = 25\nNFE = 2\nNFE = 4\nNFE = 8\nNFE = 2\nNFE = 4\nNFE = 8\nTeacher Model (DPM)\nFigure 7. The generation results comparison for AnimateLCM and teacher model in different number of inference steps (NFE).\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-\nmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4 technical report.\narXiv preprint arXiv:2303.08774,\n2023. 6\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In ICCV, pages 1728\u20131738, 2021. 6\n[3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi,\nZion English, Vikram Voleti, Adam Letts, et al. Stable video\ndiffusion: Scaling latent video diffusion models to large\ndatasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, pages 22563\u201322575, 2023.\n2, 3\n[5] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. NeurIPS,\n2014. 2\n[6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 6\n[7] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,\nMenghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng\nChen, and Ying Shan.\nScalecrafter: Tuning-free higher-\nresolution visual generation with diffusion models.\narXiv\npreprint arXiv:2310.07702, 2023. 2\n[8] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 6\n[9] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020. 2,\n3\n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2\n[12] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 2\n[13] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4, 8\n[14] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity\ndepths of dressed humans by watching social media dance\nvideos. In CVPR, pages 12753\u201312762, 2021. 6\n[15] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. NeurIPS, 35:26565\u201326577, 2022. 2, 3\n[16] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 6\n[17] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. NeurIPS,\n35:5775\u20135787, 2022. 3, 7\n[18] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 3, 6, 7\n[19] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference. arXiv preprint\narXiv:2310.04378, 2023. 2, 3, 4\n[20] Zhengxiong Luo, Dayou Chen, Yingya Zhang, Yan Huang,\nLiang Wang, Yujun Shen, Deli Zhao, Jingren Zhou, and Tie-\nniu Tan.\nVideofusion: Decomposed diffusion models for\nhigh-quality video generation. arXiv e-prints, pages arXiv\u2013\n2303, 2023. 2\n[21] Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Ying\nShan, Xiu Li, and Qifeng Chen.\nFollow your pose:\nPose-guided text-to-video generation using pose-free videos.\narXiv preprint arXiv:2304.01186, 2023. 2\n[22] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik\nKingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. In CVPR, pages\n14297\u201314306, 2023. 3, 4\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n6, 8\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763. PMLR, 2021. 6\n[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 2, 3\n[26] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. arXiv preprint arXiv:2208.12242, 2022. 8\n[27] Tim Salimans and Jonathan Ho.\nProgressive distillation\nfor fast sampling of diffusion models.\narXiv preprint\narXiv:2202.00512, 2022. 3\n[28] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian,\nDasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung,\nSimon See, Hongwei Qin, et al. Motion-i2v: Consistent and\ncontrollable image-to-video generation with explicit motion\nmodeling. arXiv preprint arXiv:2401.15977, 2024. 2, 3\n[29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3, 5\n[30] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 6, 7\n[31] Yang Song and Prafulla Dhariwal.\nImproved tech-\nniques for training consistency models.\narXiv preprint\narXiv:2310.14189, 2023. 5\n[32] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 3\n[33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever.\nConsistency\nmodels.\narXiv\npreprint\narXiv:2303.01469, 2023. 2, 3\n[34] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 6\n[35] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 6\n[36] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, pages 10459\u201310469,\n2023. 2\n[37] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 2, 3\n[38] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 2, 5, 6\n[39] Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and\nJiwen Lu.\nUnipc:\nA unified predictor-corrector frame-\nwork for fast sampling of diffusion models. arXiv preprint\narXiv:2302.04867, 2023. 3\n"
  },
  {
    "title": "Machine Unlearning for Image-to-Image Generative Models",
    "link": "https://arxiv.org/pdf/2402.00351.pdf",
    "upvote": "10",
    "text": "Published as a conference paper at ICLR 2024\nMACHINE UNLEARNING FOR IMAGE-TO-IMAGE\nGENERATIVE MODELS\nGuihong Li1\u2217, Hsiang Hsu2, Chun-Fu (Richard) Chen2, Radu Marculescu1\n1The University of Texas at Austin, USA\n2Global Technology Applied Research, JPMorgan Chase, USA\n{lgh,radum}@utexas.edu\n{hsiang.hsu,richard.cf.chen}@jpmchase.com\nABSTRACT\nMachine unlearning has emerged as a new paradigm to deliberately forget data\nsamples from a given model in order to adhere to stringent regulations. However,\nexisting machine unlearning methods have been primarily focused on classifica-\ntion models, leaving the landscape of unlearning for generative models relatively\nunexplored. This paper serves as a bridge, addressing the gap by providing a uni-\nfying framework of machine unlearning for image-to-image generative models.\nWithin this framework, we propose a computationally-efficient algorithm, under-\npinned by rigorous theoretical analysis, that demonstrates negligible performance\ndegradation on the retain samples, while effectively removing the information\nfrom the forget samples. Empirical studies on two large-scale datasets, ImageNet-\n1K and Places-365, further show that our algorithm does not rely on the availabil-\nity of the retain samples, which further complies with data retention policy. To our\nbest knowledge, this work is the first that represents systemic, theoretical, empir-\nical explorations of machine unlearning specifically tailored for image-to-image\ngenerative models. Our code is available at https://github.com/jpmorganchase/l2l-\ngenerator-unlearning.\n1\nINTRODUCTION\nThe prevalence of machine learning research and applications has sparked awareness among users,\nentrepreneurs, and governments, leading to new legislation1 to protect data ownership, privacy, and\ncopyrights (Schuhmann et al., 2022; Bubeck et al., 2023; Lukas et al., 2023; Bommasani et al.,\n2021). At the forefront of these legislative efforts is the \u201cRight to be Forgotten\u201d, a fundamental\nhuman right that empowers individuals to request the removal of their information from online\nservices. However, directly erasing data from databases is not enough, as it may already be ingrained\nin machine learning models, notably deep neural networks (DNNs), which can memorize training\ndata effectively (Wu et al., 2017; Kuppa et al., 2021; Carlini et al., 2023). Yet another straightforward\nsolution is to re-train DNNs from scratch on a new training dataset without the unwanted data\u2014a\nresource-expensive procedure (Dhariwal & Nichol, 2021) that can not reflect users\u2019 requests in a\ntimely manner.\nIn response to various legal requirements and user requests, a novel approach known as machine un-\nlearning has been proposed (Nguyen et al., 2022). This technique allows a model, which has been\ntrained with potentially sensitive samples referred to as \u201cforget samples\u201d, to selectively remove these\nsamples without the necessity of retraining the model from scratch. Meanwhile, machine unlearning\naims to minimize any adverse effects on the performance of the remaining data, termed \u201cretain sam-\nples\u201d. Recent unlearning algorithms have been developed, some incorporating specialized training\nprocedures to facilitate the unlearning process (Bourtoule et al., 2021), while others adjust model\nweights through fine-tuning (Tarun et al., 2023a). However, these approaches primarily address un-\nlearning in classification problems. On the other hand, generative models, which have demonstrated\n\u2217Work done during internship at JPMorgan Chase Bank, N.A.\n1Enactions include the General Data Protection Regulation (GDPR) by the European Union (Parliament &\nCouncil, 2016), the White House AI Bill Congress (2022b), and others (Congress, 2022a; Parliament, 2019).\n1\narXiv:2402.00351v2  [cs.LG]  2 Feb 2024\nPublished as a conference paper at ICLR 2024\nDiffusion Models\nVQ-GAN\nDiffusion Models\nVQ-GAN\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Retain Set\nDiffusion Models\nVQ-GAN\nDiffusion Models\nVQ-GAN\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Forget Set\nFigure 1: Our machine unlearning framework is applicable to various types of I2I generative models,\nincluding the diffusion models (Saharia et al., 2022a), VQ-GAN (Li et al., 2023) and MAE (He et al.,\n2022) (cf. Section 4). The images in the retain set remain almost (up to a slight difference due to the\nperplexity of generative models) unaffected before and after unlearning. Conversely, the images in\nthe forget set are nearly noise after unlearning, as designed.\nsuperior data memorization capabilities compared to classification models (Tirumala et al., 2022;\nSomepalli et al., 2023), excel at regenerating training samples (Kuppa et al., 2021; Carlini et al.,\n2023). Therefore, the challenge of ensuring effective unlearning for generative models has become\nincreasingly important and pressing.\nIn this paper, our emphasis lies on a particular category of generative model architectures known\nas Image-to-Image (I2I) generative models (Yang et al., 2022). This selection offers a twofold\nadvantage: First, it paves the way for a groundbreaking approach to quantify the efficacy of machine\nunlearning for generative models\u2014a research direction hitherto uncharted in existing literature.\nInformally speaking, we define a generative model as having \u201ctruly unlearned\u201d an image when it\nis unable to faithfully reconstruct the original image when provided with only partial information\n(see Figure 1 for an illustrative example where the partial information involves center cropping2).\nSecond, I2I generative models encompass all major branches in the field of vision generative models,\nincluding Masked Autoencoder (MAE) (He et al., 2022), Vector Quantized Generative Adversarial\nNetworks (VQ-GAN) (Li et al., 2023), and the more recent diffusion probabilistic models (Ho et al.,\n2020). Based on this novel definition to quantify unlearning, our contributions can be summarized\nas follows:\n\u2022 We formulate a machine unlearning framework for I2I generative models that is applicable\nto MAE, VQ-GAN and diffusion models. This formulation, in essence, is an unbounded\noptimization problem. We provide theoretical derivations that guarantee the unique opti-\nmality of its bounded counterpart, and design an algorithm for the efficient computation.\n\u2022 We conduct extensive evaluations of our algorithm on various I2I generative models, in-\ncluding MAE, VQ-GAN and diffusion models.\nEmpirical results on two large scale\ndatasets, ImageNet-1K (Deng et al., 2009) and Places-365 (Zhou et al., 2017), show that\nour framework exhibits negligible performance degradation on retain sets, meanwhile ef-\nfectively eliminating the information in forget sets.\n\u2022 We further illustrate that the availability of the exact retain set is not necessary in our\nframework\u2014the efficacy of our algorithm remains intact even without any samples from\nthe exact retain set.\n2For the precise definition, see Section 3.\n2\nPublished as a conference paper at ICLR 2024\nTo the best of our knowledge, this work is the first that systemically, theoretically, empirically ex-\nplore the machine unlearning specifically targeting for I2I generative models. Proofs, details on\nexperimental setups and training, and additional results are included in the Appendix.\n2\nRELATED WORK\nI2I generative models.\nMany computer vision tasks can be formulated as I2I generation pro-\ncesses, such as image super-resolution (Bulat et al., 2018), style transfer (Zhu et al., 2017), image\nextension (Chang et al., 2022) and inpainting (Krishnan et al., 2019).\nDifferent type of I2I generative models utilize diverse training and optimization strategies to min-\nimize the discrepancy between their generated images and the ground truth images. The broadly\nused Generative Adversarial Networks (GANs) are trained by reducing a discriminator\u2019s accuracy\nin determining whether a generated image is real or synthetic (Goodfellow et al., 2014; Karras\net al., 2019; Chen et al., 2016; Karras et al., 2020). However, stabilizing the training of GANs is\na well-known challenge (Arjovsky et al., 2017; Gulrajani et al., 2017; Brock et al., 2019). In con-\ntrast, diffusion models address the stability issue by utilizing a multi-step generation strategy and\nare optimized by minimizing the Kullback\u2013Leibler (KL) divergence between the distributions of the\ngenerated and ground truth images (Ho et al., 2020; Song & Ermon, 2020; Hoogeboom et al., 2022;\nSalimans & Ho, 2022). Diffusion models can generate higher-fidelity images than GANs but re-\nquire much longer generation time (Saharia et al., 2022b; Rombach et al., 2022; Zhang & Agrawala,\n2023). Recently, Masked Autoencoder (MAE) has been proposed as a multi-purpose model for\nboth image generation and classification (He et al., 2022; Feichtenhofer et al., 2022; Tong et al.,\n2022). Typically, MAE is optimized by minimizing the MSE loss between the generated and ground\ntruth images. In this paper, our goal is to design a universal approach that is capable of conducting\nunlearning across diverse I2I model types with different optimization techniques.\nMachine unlearning.\nMachine unlearning allows a trained model to selectively remove some\nunwanted samples (\u201cforget set\u201d) while minimizing any adverse effects on the performance of the\nremaining data (\u201cretain set\u201d) and without retraining the model from scratch (Xu et al., 2023). As the\npioneering work on unlearning, SISA facilitates the unlearning of specific samples by retraining the\nmodel checkpoints that were initially trained with these \u201dforget\u201d samples (Bourtoule et al., 2021).\nHowever, SISA needs to re-train all these models from scratch, if the forget samples are distributed\nacross all shards. To address this problem, several methods manipulate the trained model weights\ndirectly. Some works compute the Neural Tangent Kernel (NTK) to modify model weights, but the\ncomputation of the Hessian matrix in NTK\u2019s calculation is numerically unstable and not scalable for\nmodels with many parameters (Golatkar et al., 2020a;b). Graves et al. (2021) requires the storage\nof the gradient for each parameter of every training step when training the original models. This\napproach is not scalable given the extremely large training set and the enormous model size for\nthe latest image generative models. Other methods improve the efficiency by maximizing loss on\nthe forget set or re-assigning incorrect labels but typically they are only applicable to classification\ntasks. (Neel et al., 2021; Tarun et al., 2023b; Chourasia & Shah, 2023; Kurmanji et al., 2023; Chen\net al., 2023). There are also some approaches focusing on other perspectives of unlearning instead\nof designing new unlearning algorithms. For example, Chundawat et al. (2023) focuses on the\ndata access issues of existing unlearning algorithms and suggests using the images generated by the\noriginal model as the alternative for the original training set. Besides, Jia et al. (2023) shows that that\npruning the original model before unlearning can improve the overall performance of many existing\nunlearning algorithms. Previous unlearning approaches primarily focus on classification tasks, but\nthere are emerging efforts on generative models. For instance, several methods maximize training\nloss on the forget set, but are validated only on tiny datasets, like MNIST (Bae et al., 2023; Sun et al.,\n2023). Other works focus on unlearning specific features (e.g., eye color, hairstyle) from generated\nimages, but are only verified under small-scale setups and lack comprehensive analysis (Kong &\nChaudhuri, 2023; Moon et al., 2023). Besides, these methods typically manipulate the entire model,\nthus requiring extensive computation capacity due to the growing complexity and size of generative\nmodels. Moreover, none of them addresses I2I generative tasks. This motivates us to explore the\nefficient unlearning algorithms for I2I generative models in large-scale setups.\n3\nPublished as a conference paper at ICLR 2024\n3\nPROBLEM FORMULATION AND PROPOSED APPROACH\nIn this work, we primarily address the machine unlearning for I2I generative models that reconstruct\nimages from incomplete or partial inputs. Typically, I2I generative models adopt an encoder-decoder\nnetwork architecture, comprising two components, namely, (i) an encoder network E\u03b8 that encodes\nan input into a representation vector and (ii) a decoder network D\u03d5 that decodes the representation\nvector into the image. Specifically, given an input x, the output for an I2I generative model h\u03b8,\u03d5 is\nas follows:\nh\u03b8,\u03d5 = D\u03d5 \u25e6 E\u03b8,\nh\u03b8,\u03d5 (T (x)) = D\u03d5 (E\u03b8 (T (x)))\n(1)\nwhere x is a ground truth image; T (\u00b7) is the operation to remove some information from x, e.g.,\ncenter cropping and random masking; \u25e6 is the composition operator; \u03b8 and \u03d5 are the parameters for\nthe encoder and decoder, respectively.\n3.1\nDEFINITION OF UNLEARNING ON I2I GENERATIVE MODELS\nFor machine unlearning on I2I generative models, given a trained model (i.e., original model)\nh\u03b80,\u03d50 = D\u03d50 \u25e6 E\u03b80 with parameters \u03b80 and \u03d50, the unlearning algorithm AF aims to obtain a\ntarget model:\nh\u03b8,\u03d5 \u225c AF (h\u03b80,\u03d50)\nthat satisfies the following properties:\n\u2022 On the retain set DR, h\u03b8,\u03d5 generates images that have the same distribution as the original model;\n\u2022 On the forget set DF , h\u03b8,\u03d5 generates images that have far different distribution from the original\nmodel.\nBy using the KL-divergence (D), from a probability distribution perspective, these objectives are as\nfollows:\narg min\n\u03b8,\u03d5\nD\n\u0010\nPh\u03b80,\u03d50(T (Xr))||Ph\u03b8,\u03d5(T (Xr))\n\u0011\n, and arg max\n\u03b8,\u03d5\nD\n\u0010\nPh\u03b80,\u03d50(T (Xf ))||Ph\u03b8,\u03d5(T (Xf ))\n\u0011\n(2)\nwhere, Xr and Xf are random variables that account for the ground truth images of the retain and\nforget sets, respectively.\nBy combining these two objectives, we formulate our optimization goal as follows:\narg min\n\u03b8,\u03d5\n\u001a\nD\n\u0010\nPh\u03b80,\u03d50(T (Xr))||Ph\u03b8,\u03d5(T (Xr))\n\u0011\n\u2212 \u03b1D\n\u0010\nPh\u03b80,\u03d50(T (Xf ))||Ph\u03b8,\u03d5(T (Xf ))\n\u0011 \u001b\n(3)\nwhere \u03b1 is a positive coefficient to control the trade-off between the retain and forget sets. Multiple\nprevious works assume a trained I2I generative model can do an almost perfect generation on both\nof the retain and forget sets (Wallace et al., 2023; Song et al., 2023; Xia et al., 2023; Kingma &\nWelling, 2019); that is, h\u03b80,\u03d50 (T (X)) \u2248 X. Therefore, Eq. (3) can be rewritten as:\narg min\n\u03b8,\u03d5\n\b\nD\n\u0010\nPXr||P \u02c6\nXr\n\u0011\n\u2212\u03b1D\n\u0010\nPXf ||P \u02c6\nXf\n\u0011 \t\n, \u02c6Xr = h\u03b8,\u03d5 (T (Xr)) , \u02c6Xf = h\u03b8,\u03d5 (T (Xf)) (4)\nwhere PXr and P \u02c6\nXr represent the distribution of ground truth images and generated images in the\nretain set; PXf and P \u02c6\nXf represent the distribution of ground truth images and generated images in\nthe forget set.\n3.2\nOPTIMIZATION ON RETAIN AND FORGET SETS\nClearly, for the first term in Eq. (4), a perfect unlearned model has no performance degradation on\nthe retains set. In other words, the generated images share the distribution as ground truth images,\ni.e., P \u02c6\nXr = PXr. This way, the value of D\n\u0010\nPXr||P \u02c6\nXr\n\u0011\nis 0. Next, we discuss the optimization for\nthe forget set.\n4\nPublished as a conference paper at ICLR 2024\nTo minimize the value for the objective functions in Eq. (4), we need to maximize KL divergence be-\ntween PXf and P \u02c6\nXf . However, there are infinitely many probability distributions that have infinity\nKL divergence with PXf (see Appendix A for more details). The \u221e value for the KL divergence will\nlead to unbounded loss values thus hurting the stability of the unlearning process. To address this\nproblem, we derive an optimal and bounded KL divergence for the forget set under some reasonable\nconstraints:\nLemma 1 Given the distribution of the forget samples PXf with zero-mean and covariance matrix\n\u03a3, consider another signal P \u02c6\nXf which shares the same mean and covariance matrix. The maximal\nKL-divergence between PXf and P \u02c6\nXf is achieved when P \u02c6\nXf = N (0, \u03a3) (Cover & Thomas, 2012);\nthat is:\nD\n\u0010\nPXf ||P \u02c6\nXf\n\u0011\n\u2264 D\n\u0000PXf ||N (0, \u03a3)\n\u0001\n(5)\nWe note that making P \u02c6\nXf share the same mean and covariance matrix as PXf can preserve the\noriginal training set statistical patterns. Consequently, it becomes statistically challenging to decide\nwhether a generated image belongs to the forget set, thereby protecting data privacy. Moreover, the\nassumption of zero mean is natural since typically images are normalized by subtracting the mean\nvalue inside neural networks. We provide some empirical analysis to demonstrate the benefits of\nGaussian distribution (cf. Section 4.4).\nEssentially, Lemma 1 indicates that the maximal KL divergence w.r.t PXf is achieved when the\ngenerated images P \u02c6\nXf follow the Gaussian distribution N (0, \u03a3). Hence, we can directly optimize\nP \u02c6\nXf towards this optimal solution by minimizing their KL-Divergence; that is:\narg min\n\u03b8,\u03d5\n\b\nD\n\u0010\nPXr||P \u02c6\nXr\n\u0011\n+ \u03b1D\n\u0010\nN (0, \u03a3) ||P \u02c6\nXf\n\u0011 \t\n, \u02c6Xr = h\u03b8,\u03d5 (T (Xr)) , \u02c6Xf = h\u03b8,\u03d5 (T (Xf))\n(6)\nThis way, we avoid the problem of the infinity value of KL-divergence in Eq. (4). We note that, for\nprevious unlearning approaches for classification tasks, it\u2019s natural and straightforward to directly\ncompute the KL-divergence for final outputs since the outputs are exactly single-variable discrete\ndistributions after the SoftMax function (Zhang et al., 2023a;b; Kurmanji et al., 2023). Neverthe-\nless, for image generation tasks, directly computing the KL divergence between high-dimensional\noutput images is typically intractable, excluding the special case of diffusion models. To address\nthis problem, we next convert the KL divergence into a more efficient L2 loss which is generally\napplicable to diverse I2I generative models.\n3.3\nPROPOSED APPROACH\nDirectly connecting the KL-Divergence with the L2 loss is difficult. Instead, we use Mutual Infor-\nmation (MI) as a bridge to help with the analysis. As indicated in Eq. (6), we reach the minimal\nobjective value when P \u02c6\nXr = PXr and P \u02c6\nXf = N(0, \u03a3). This optimum can also be achieved by\nmaximizing the mutual information (I) between Xr and \u02c6Xr (or between n \u223c N(0, \u03a3) and \u02c6Xf);\nthat is:\narg max\n\u03b8,\u03d5\n\b\nI\n\u0010\nXr; \u02c6Xr\n\u0011\n+\u03b1I\n\u0010\nn; \u02c6Xf\n\u0011 \t\n, n \u223c N(0, \u03a3), \u02c6Xr = h\u03b8,\u03d5 (T (Xr)) , \u02c6Xf = h\u03b8,\u03d5 (T (Xf))\n(7)\nWe next link the MI with a more tractable L2 loss in the representation space.\nTheorem 1 Suppose the original model can do a perfect generation, i.e., h\u03b80,\u03d50 (T (X)) = X.\nAssume the target model h\u03b8,\u03d5 uses the same decoder as the original model h\u03b80,\u03d50 (i.e., D\u03d5 = D\u03d50),\nand the output of the encoders is normalized, i.e., \u2225E\u03b8(x)\u22252 = \u2225E\u03b80(x)\u22252 = 1. On the retain set,\nminimizing the L2 loss between the output of the target model encoder E\u03b8 and the original model\nencoder E\u03b80 will increase the lower bound of mutual information:\nI(Xr; \u02c6\nXr) \u2265 log (K) \u2212 E\n\uf8ee\n\uf8f0\nK\nX\ni=1\n1\nK log\n\uf8eb\n\uf8ede\n\u03f52\ni\n2 \u22121\nK\nX\nj=1\ne\u03f5j+Rij\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb\n(8)\n5\nPublished as a conference paper at ICLR 2024\nForget set\nRetain set\nOriginal model\n(Fixed)\nTarget model\n(Learnable)\nEncoder \ud835\udc38\u0c0f\n\ud835\udc37\u0bbf\n\ud835\udc37\u0bcb\nEncoder \ud835\udc38\u0c0f\u0c2c\n\ud835\udc65\u0bd9\n\ud835\udc65\u0be5\n\ud835\udc5b \u223c \ud835\udca9 0, \ud835\udef4\n\ud835\udc3f\u0bbf \u0d4c \u125b\ud835\udc38\u0c0f \ud835\udcaf \ud835\udc65\u0bd9\n\u0d46 \ud835\udc38\u0c0f\u0c2c \ud835\udcaf \ud835\udc5b\n\u125b\n\u0b36\n\ud835\udc3f\u0bcb \u0d4c \u0e2e\ud835\udc38\u0c0f \ud835\udcaf \ud835\udc65\u0be5\n\u0d46 \ud835\udc38\u0c0f\u0c2c \ud835\udcaf \ud835\udc65\u0be5\n\u0e2e\u0b36\nFigure 2: Overview of our approach. On DF , we minimize the L2-loss between embedding vectors\nof the forget samples xf and embedding vectors of Gaussian noise n. On DR, we minimize the L2-\nloss between the same image embedding vectors generated by target model encoder and the original\nmodel encoder.\nwhere \u03f5i = \u2225E\u03b8 (T (xri)) \u2212 E\u03b80 (T (xri)) \u22252 and Rij = E\u03b80(T (xri))T E\u03b80(T (xrj)). xri are the\ndata samples in the retain set. For the forget set, we have:\nI(n; \u02c6\nXf) \u2265 log (K) \u2212 E\n\uf8ee\n\uf8f0\nK\nX\ni=1\n1\nK log\n\uf8eb\n\uf8ede\n\u03b42\ni\n2 \u22121\nK\nX\nj=1\ne\u03b4j+Fij\n\uf8f6\n\uf8f8\n\uf8f9\n\uf8fb ,\nn \u223c N(0, \u03a3)\n(9)\nwhere \u03b4i = \u2225E\u03b8 (T (xfi)) \u2212 E\u03b80 (T (ni)) \u22252 and Fij = E\u03b80(T (ni))T E\u03b80(T (nj)). xfi are the data\nsamples in the forget set and ni \u223c N(0, \u03a3).\nWe remark that both Rij and Fij are determined by the original encoder E\u03b80, thus are fixed values.\nAs illustrated in Theorem 1, by directly reducing the L2 loss (\u03b4i and \u03f5i) between the target encoder\nand the original encoder, the Mutual Information (MI) increases, concurrently reducing the KL\ndivergence between PXr and P \u02c6\nXf (or between P \u02c6\nXf and N). Hence, in our approach, we sidestep\nthe intractability of computing MI or KL divergence by directly minimizing the values of \u03b4i and \u03f5i.\nBased on these insights, we next introduce our approach.\nEfficient Unlearning Approach.\nFinally, as shown in Fig. 2, we propose our efficient unlearning\napproach for I2I generative models as follows:\nAF (h\u03b80,\u03d50) \u225c arg min\n\u03b8\nE\nxri ,xfj ,n\n\u001a\f\fE\u03b8\n\u0000T (xri)\n\u0001\n\u2212 E\u03b80\n\u0000T (xri)\n\u0001 \f\f\n2 + \u03b1\n\f\fE\u03b8\n\u0010\nT (xfj )\n\u0011\n\u2212 E\u03b80 (T (n))\n\f\f\n2\n\u001b\nxri \u2208 DR, xfj \u2208 DF , n \u223c N (0, \u03a3)\n(10)\nWe provide the details of our unlearning algorithm and corresponding pseudo code in Appendix C.4.\nWe note that our proposed approach only involves the encoders. Hence, it\u2019s more efficient than ma-\nnipulating the entire model. Moreover, our approach is generally applicable to various I2I generative\nmodels with the encoder-decoder architecture (including the diffusion model, VQ-GAN, or MAE),\nalthough they typically use different optimization methods. We illustrate this generalizability in the\nexperiments part.\n4\nEXPERIMENTAL RESULTS\nWe evaluate our proposed approach on three mainstream I2I generative models: (i) diffusion mod-\nels (Saharia et al., 2022a), (ii) VQ-GAN (Li et al., 2023), and (iii) MAE (He et al., 2022).\n4.1\nEXPERIMENTAL SETUP\nDataset&Task. We verify our method on two mainstream large-scale datasets: (i) ImageNet-1k.\nOut of total 1K classes, we randomly select 100 classes as DR and another 100 classes as DF . (ii)\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Results of cropping 8 \u00d7 8 patches at the center of the image, where each patch is 16 \u00d7 16\npixels. \u2018\u2191\u2019 means higher is better and \u2018\u2193\u2019 means lower is better. R and F account for the retain\nset and forget set, respectively.\u2018Proxy DR\u2019 means that we use the images from other classes as a\nsubstitute of the real retain set to do the unlearning (cf. Section 4.3).\nDiffusion Models\nVQ-GAN\nMAE\nFID\nIS\nCLIP\nFID\nIS\nCLIP\nFID\nIS\nCLIP\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nOriginal model\n12.2\n14.6\n19.3\n23.1\n0.88\n0.89\n14.4\n14.4\n19.4\n20.6\n0.75\n0.77\n56.7\n84.1\n23.0\n17.4\n0.73\n0.71\nMAX LOSS\n34.1\n45.7\n12.8\n17.1\n0.77\n0.76\n16.9\n115.2\n17.4\n11.0\n0.73\n0.55\n75.8\n112.6\n19.4\n15.2\n0.69\n0.65\nNOISY LABEL\n14.7\n36.9\n19.3\n19.1\n0.86\n0.80\n14.8\n79.5\n17.2\n11.4\n0.74\n0.64\n60.4\n136.5\n21.6\n12.8\n0.71\n0.67\nRETAIN LABEL\n23.1\n104.7\n18.2\n12.3\n0.81\n0.69\n21.8\n23.3\n18.2\n18.3\n0.72\n0.74\n72.8\n145.3\n18.8\n11.6\n0.69\n0.66\nRANDOM ENCODER\n15.3\n30.6\n18.7\n19.4\n0.86\n0.81\n14.7\n72.8\n18.6\n14.1\n0.74\n0.64\n58.1\n146.4\n22.3\n12.8\n0.72\n0.67\nOurs\n13.4\n107.9\n19.4\n10.3\n0.87\n0.69\n15.0\n83.4\n18.3\n11.6\n0.74\n0.60\n59.9\n153.0\n21.8\n11.0\n0.72\n0.67\nOurs (Proxy DR)\n17.9\n75.5\n18.2\n12.3\n0.83\n0.74\n17.6\n69.7\n18.6\n14.0\n0.73\n0.63\n61.1\n133.8\n21.0\n12.3\n0.72\n0.68\nTable 2: Results of cropping 4 \u00d7 4 patches at the center of the image, where each patch is 16 \u00d7 16\npixels. \u2018\u2191\u2019 means higher is better and \u2018\u2193\u2019 means lower is better. R and F account for the retain\nset and forget set, respectively. \u201cProxy DR\u201d means that we use the images from other classes as a\nsubstitute of the real retain set to do the unlearning (cf. Section 4.3).\nDiffusion Models\nVQ-GAN\nMAE\nFID\nIS\nCLIP\nFID\nIS\nCLIP\nFID\nIS\nCLIP\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nR\u2193\nF\u2191\nR\u2191\nF\u2193\nR\u2191\nF\u2193\nOriginal model\n7.8\n6.0\n10.3\n11.2\n0.93\n0.96\n8.4\n7.8\n15.1\n14.2\n0.84\n0.85\n11.4\n15.8\n50.8\n46.6\n0.87\n0.87\nMAX LOSS\n11.9\n15.4\n10.0\n11.0\n0.88\n0.93\n9.2\n39.9\n15.2\n13.1\n0.83\n0.72\n13.3\n20.2\n50.8\n46.0\n0.86\n0.83\nNOISY LABEL\n19.6\n18.5\n10.4\n10.6\n0.87\n0.91\n8.7\n21.3\n15.2\n14.1\n0.84\n0.80\n12.2\n44.3\n50.0\n35.4\n0.86\n0.82\nRETAIN LABEL\n8.5\n35.1\n10.3\n10.5\n0.93\n0.89\n11.0\n10.3\n15.4\n14.2\n0.83\n0.84\n15.3\n47.5\n47.6\n34.9\n0.85\n0.81\nRANDOM ENCODER\n15.3\n11.6\n10.1\n11.1\n0.86\n0.94\n8.6\n19.4\n15.3\n14.4\n0.84\n0.81\n11.8\n43.6\n50.3\n36.3\n0.86\n0.83\nOurs\n8.2\n39.8\n10.3\n10.7\n0.93\n0.88\n8.6\n22.0\n15.0\n14.1\n0.84\n0.79\n12.2\n45.1\n49.7\n34.8\n0.86\n0.83\nOurs (Proxy DR)\n11.2\n29.0\n10.3\n10.8\n0.91\n0.9\n8.9\n20.0\n15.4\n14.3\n0.84\n0.80\n12.5\n39.9\n49.5\n36.8\n0.86\n0.83\nPlaces-365. From all 365 classes, we randomly select 50 classes as DR and another 50 classes as\nDF . We test our method on image extension, uncropping, and reconstruction tasks. We report the\nresults of center uncropping (i.e., inpainting) in the main paper. The results of other tasks are given\nin Appendix D and E.1.\nBaseline. We first report the performance of the original model (i.e., before unlearning) as the\nreference. Since our approach is the first work that does the unlearning for I2I generative models,\nthere are no previous baselines we can directly compare against. Therefore, we implement three\ndifferent unlearning approaches that were designed for other tasks, and adapt them to I2I generative\nmodels, including (i) MAX LOSS maximizes the training loss w.r.t. the ground truth images on the\nforget set (Halimi et al., 2022; Gandikota et al., 2023; Warnecke et al., 2023); (ii) NOISY LABEL\nminimizes training loss by setting the Gaussian noise as the ground truth images for the forget\nset (Graves et al., 2021; Gandikota et al., 2023); (iii) RETAIN LABEL minimizes training loss by\nsetting the retain samples as the ground truth for the forget set (Kong & Chaudhuri, 2023); (iv)\nRANDOM ENCODER directly minimizes the L2 loss between the encoder\u2019s output on the forget\nset and a Gaussian noise (Tarun et al., 2023b). For all these baselines, we use the retain samples\nwith some regularization to avoid hurting the performance on the retain set. For more details, please\ncheck Appendix C.6.\nEvaluation metrics. We adopt three different types of metrics to compare our method against\nother baselines: (i) inception score (IS) of the generated images (Salimans et al., 2016), (ii) Fr\u00b4echet\ninception distance (FID) against the real images (Heusel et al., 2017) and (iii) CLIP embedding\ndistance between the generated images and the real images (Radford et al., 2021). IS assesses the\nquality of the generated images alone, while FID further measure the similarity between generated\nand real images. On the other hand, the CLIP embedding distance measures whether or not the\ngenerated images still capture similar semantics.\n4.2\nPERFORMANCE ANALYSIS AND VISUALIZATION\nAs shown in Table 1 and Table 2, compared to the original model, our approach has almost identical\nperformance or only a slight degradation on the retain set. Meanwhile, there are significant perfor-\n7\nPublished as a conference paper at ICLR 2024\nForget Set\nRetain Set\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\nFigure 3: Results of cropping 8 \u00d7 8 patches at the center of the image on diffusion models, where\neach patch is 16\u00d716 pixels. Our method has negligible-to-slight performance degradation on diverse\nI2I generative models and multiple generative tasks. (cf. Appendix D and E.1).\n30\n20\n10\n0\n10\n20\n30\n20\n10\n0\n10\n20\n30\n(a) Diffusion Models\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n(b) VQ-GAN\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30\n(c) MAE\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nUnlearn: Forget Set\nUnlearn: Retain Set\nGround Truth: Forget Set\nGround Truth: Retain Set\nFigure 4: T-SNE analysis of the generated images by our approach and ground truth images. After\nunlearning, the generated retain samples are close to or overlapping with the ground truth (orange\nvs. blue), while most of generated forget images diverge far from the ground truth (green vs. red).\nmance drops on the forget set across all these three models for all metrics. In contrast, none of these\nbaselines generally works well. For example, RANDOM ENCODER achieves similar performance\non VQ-GAN and MAE to our methods; however, it is much worse on diffusion models. Similarly,\nRETAIN LABEL works well for diffusion models, but cannot generalize to VQ-GAN and MAE. We\nalso show some generated images in Fig. 3. As shown, our approach removes the information in the\nforget set while preserving the performance on the retain set.\nT-SNE analysis.\nTo further analyze why our approach works well, we conduct the T-SNE analysis.\nUsing our unlearned model, we generate 50 images for both the retain and forget sets. We then\ncompute the CLIP embedding vector of these images and their corresponding ground truth images.\nAs shown in Fig. 4, after unlearning, the CLIP embedding vector on the retain set is close to or\noverlapping with the ground truth images, while most of generated images on the forget set diverge\nfar from the ground truth.\nThese results verify that our method is generally applicable to mainstream I2I generative models\nand consistently achieves good results on all these models. We provide more results under various\ntypes of cropping in Appendix D and Appendix E.\n4.3\nROBUSTNESS TO RETAIN SAMPLES AVAILABILITY\nIn machine unlearning, sometimes the real retain samples are not available due to data retention\npolicies. To address this challenge, we evaluate our approach by using other classes of images as\nsubstitute to the real retain samples. On ImageNet-1K, since we already select 200 classes for forget\nand retain sets, we randomly select some images from the remaining 800 classes as the \u201cproxy retain\nset\u201d used in the unlearning process. Similarly, for Places-365, we randomly select some images from\nthe remaining 265 classes as the \u201cproxy retain set\u201d used in the unlearning process. We also ensure\nthese \u201cproxy retain sets\u201d have the same number of images as the forget set.\nAs shown in the last row in Table 1 and Table 2, our method works well even without the access\nto the real/original retain set. Compared to using the real/original retain set, there is only a slight\nperformance drop. Hence, our approach is flexible and generally applicable without the dependency\n8\nPublished as a conference paper at ICLR 2024\nTable 3: Ablation study of \u03b1\u2019s values. We test the performance of cropping 8 \u00d7 8 patches at the\ncenter of the image. As shown, \u03b1 = 0.25 achieves a good balance between the preserving the\nperformance on retain set, while remove the information on forget sets across these two models.\nVQ-GAN\nMAE\n\u03b1\n0.01\n0.05\n0.1\n0.2\n0.25\n0.5\n1\n0.01\n0.05\n0.1\n0.2\n0.25\n0.5\n1\nFID\nR\u2193\n90.8\n91.6\n92.0\n91.7\n92.7\n92.2\n94.7\n113.6\n113.2\n113.9\n116.7\n115.9\n116.3\n116.7\nF\u2191\n101.2\n169.4\n179.5\n181.3\n183.4\n182.2\n184.6\n179.0\n198.6\n205.1\n211.5\n213.0\n213.4\n213.0\nIS\nR\u2191\n12.5\n12.8\n12.5\n12.4\n12.2\n12.0\n12.6\n13.3\n13.3\n13.4\n13.5\n13.2\n13.3\n12.9\nF\u2193\n11.5\n8.4\n7.8\n7.9\n8.1\n7.9\n8.0\n9.3\n9.0\n8.5\n8.0\n8.0\n8.1\n7.9\nCLIP\nR\u2191\n0.65\n0.65\n0.65\n0.65\n0.65\n0.65\n0.64\n0.81\n0.81\n0.81\n0.80\n0.80\n0.80\n0.80\nF\u2193\n0.66\n0.55\n0.54\n0.54\n0.54\n0.54\n0.54\n0.79\n0.78\n0.78\n0.78\n0.78\n0.78\n0.78\non the real retain samples. We provide the results with limited availability to the real retain samples\nin Appendix D.1.\n4.4\nABLATION STUDY\nFor the ablation study, we test the results of cropping patches at the center of the image under various\nsetups, where each patch is 16 \u00d7 16 pixels.\n\u03b1\u2019s value.\nWe vary the value of \u03b1 in Eq. (10) to obtain multiple models and then evaluate their\nperformance. As shown in Table 3, when \u03b1 is 0.25, our approach achieves a good balance between\nthe forget set and the retain set. Hence, we set \u03b1 = 0.25 as default value for our approach. We\nprovide more ablation study in Appendix E.\n5\nCONCLUSIONS AND FINAL REMARKS\nIn this paper, we have formulated the machine unlearning problem for I2I generative models and\nderived an efficient algorithm that is applicable across various I2I generative models, including\ndiffusion models, VQ-GAN, and MAE. Our method has shown negligible performance degradation\non the retain set, while effectively removing the information from the forget set, on two large-scale\ndatasets (ImageNet-1K and Places-365). Remarkably, our approach is still effective with limited or\nno real retain samples. To our best knowledge, we are the first to systematically explore machine\nunlearning for image completion generative models.\nLimitations.\nFirst, our methods are mainly verified on I2I generative models. Second, our ap-\nproach requires the access of original/real forget samples yet sometimes they are unavailable. Be-\nsides, for the simplicity of evaluation, we only test our approach on some mainstream computer\nvision datasets. Our approach has not been verified under a more practical/useful scenarios, e.g.,\nremove the pornographic information for I2I generative models.\nFuture directions.\nWe plan to explore applicability to other modality, especially for language/text\ngeneration and text-to-image generation. The dependency on the forget set is another challenge that\nenable flexibility in the unlearning for generative models. Finally, we also intend to develop some\nmore practical benchmarks related to the control of generative contents and protect the data privacy\nand copyright.\nDISCLAIMER\nThis paper was prepared for informational purposes by the Global Technology Applied Research center of JP-\nMorgan Chase & Co. This paper is not a product of the Research Department of JPMorgan Chase & Co. or its\naffiliates. Neither JPMorgan Chase & Co. nor any of its affiliates makes any explicit or implied representation\nor warranty and none of them accept any liability in connection with this paper, including, without limitation,\nwith respect to the completeness, accuracy, or reliability of the information contained herein and the potential\nlegal, compliance, tax, or accounting effects thereof. This document is not intended as investment research\nor investment advice, or as a recommendation, offer, or solicitation for the purchase or sale of any security,\nfinancial instrument, financial product or service, or to be used in any way for evaluating the merits of partic-\nipating in any transaction. Guihong Li\u2019s and Radu Marculescu\u2019s contributions were made as part of Guihong\nLi\u2019s internship at the Global Technology Applied Research center of JPMorgan Chase & Co.\n9\nPublished as a conference paper at ICLR 2024\nEthics statement. Machine unlearning for I2I generative models can be effectively exploited to avoid generate\ncontents related user privacy and copyright. Moreover, unlearning for I2I models can avoid generating harmful\ncontents, such as violence or pornography.\nReproducibility statement. All the datasets used in this paper are open dataset and are available to the public.\nBesides, our codes are primarily based on PyTorch (Paszke et al., 2019). We use several open source code\nbase and model checkpoints to build our own approach (see Appendix C.1). Our approach can be implemented\nby obtaining the outputs of target model\u2019s encoders and the original model\u2019s encoders and then computing the\nL2-loss between them. We provide more implementation details in Appendix C.\nREFERENCES\nMart\u00b4\u0131n Arjovsky, Soumith Chintala, and L\u00b4eon Bottou. Wasserstein generative adversarial networks. In Inter-\nnational Conference on Machine Learning, pp. 214\u2013223. PMLR, 2017.\nSeohui Bae, Seoyoon Kim, Hyemin Jung, and Woohyung Lim. Gradient surgery for one-shot unlearning on\ngenerative model. CoRR, abs/2307.04550, 2023.\nMohamed Ishmael Belghazi, Aristide Baratin, Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,\nand Devon Hjelm. Mutual information neural estimation. In International Conference on Machine Learning,\npp. 531\u2013540. PMLR, 2018.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ B. Altman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson, Shyamal Buch, Dallas\nCard, Rodrigo Castellon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel, Jared Quincy Davis, Dorottya\nDemszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano Ermon, John Etchemendy, Kawin\nEthayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren Gillespie, Karan Goel, Noah D. Goodman, Shelby\nGrossman, Neel Guha, Tatsunori Hashimoto, Peter Henderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle\nHsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff\nKeeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, Mark S. Krass, Ranjay Krishna, Rohith Kuditipudi,\nand et al. On the opportunities and risks of foundation models. CoRR, abs/2108.07258, 2021.\nLucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu\nZhang, David Lie, and Nicolas Papernot. Machine unlearning. In 42nd IEEE Symposium on Security and\nPrivacy, SP 2021, San Francisco, CA, USA, 24-27 May 2021, pp. 141\u2013159. IEEE, 2021.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural image\nsynthesis. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net, 2019.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter\nLee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco T\u00b4ulio Ribeiro, and\nYi Zhang. Sparks of artificial general intelligence: Early experiments with GPT-4. CoRR, abs/2303.12712,\n2023.\nAdrian Bulat, Jing Yang, and Georgios Tzimiropoulos. To learn image super-resolution, use a gan to learn how\nto do image degradation first. In Proceedings of the European Conference on Computer Vision (ECCV), pp.\n185\u2013200, 2018.\nNicolas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, Borja Balle,\nDaphne Ippolito, and Eric Wallace.\nExtracting training data from diffusion models.\nIn 32nd USENIX\nSecurity Symposium (USENIX Security 23), pp. 5253\u20135270, 2023.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image\ntransformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New\nOrleans, LA, USA, June 18-24, 2022, pp. 11305\u201311315. IEEE, 2022.\nMin Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang. Boundary unlearning: Rapid forgetting of\ndeep networks via shifting the decision boundary. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 7766\u20137775. IEEE, 2023.\nXi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable\nrepresentation learning by information maximizing generative adversarial nets. In Advances in Neural In-\nformation Processing Systems, pp. 2172\u20132180, 2016.\nRishav Chourasia and Neil Shah.\nForget unlearning: Towards true data-deletion in machine learning.\nIn\nAndreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett\n(eds.), International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning\nResearch, pp. 6028\u20136073. PMLR, 2023.\n10\nPublished as a conference paper at ICLR 2024\nVikram S. Chundawat, Ayush K. Tarun, Murari Mandal, and Mohan S. Kankanhalli. Zero-shot machine un-\nlearning. IEEE Trans. Inf. Forensics Secur., 18:2345\u20132354, 2023.\nJapan Congress. Act on the protection of personal information, 2022a. URL https://www.ppc.go.jp/\nfiles/pdf/280222_amendedlaw.pdf.\nUnited States Congress. American data privacy and protection act, 2022b. URL https://www.congress.\ngov/bill/117th-congress/house-bill/8152.\nT.M. Cover and J.A. Thomas. Elements of Information Theory, chapter 12, pp. 409\u2013413. Wiley, 2012. ISBN\n9781118585771.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition\n(CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pp. 248\u2013255. IEEE Computer Society, 2009.\nPrafulla Dhariwal and Alexander Quinn Nichol. Diffusion models beat gans on image synthesis. In Advances\nin Neural Information Processing Systems, pp. 8780\u20138794, 2021.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-\nterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th Interna-\ntional Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Open-\nReview.net, 2021.\nChristoph Feichtenhofer, Yanghao Li, Kaiming He, et al. Masked autoencoders as spatiotemporal learners. In\nAdvances in Neural Information Processing Systems, volume 35, pp. 35946\u201335958, 2022.\nRohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion\nmodels. CoRR, abs/2303.07345, 2023.\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Eternal sunshine of the spotless net: Selective forget-\nting in deep networks. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020, pp. 9301\u20139309. Computer Vision Foundation / IEEE, 2020a.\nAditya Golatkar, Alessandro Achille, and Stefano Soatto. Forgetting outside the box: Scrubbing deep networks\nof information accessible from input-output observations. In Proceedings of the European Conference on\nComputer Vision (ECCV), pp. 383\u2013398. Springer, 2020b.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C.\nCourville, and Yoshua Bengio. Generative adversarial networks. CoRR, abs/1406.2661, 2014.\nLaura Graves, Vineel Nagisetty, and Vijay Ganesh. Amnesiac machine learning. In Proceedings of the AAAI\nConference on Artificial Intelligence, pp. 11516\u201311524, 2021.\nIshaan Gulrajani, Faruk Ahmed, Mart\u00b4\u0131n Arjovsky, Vincent Dumoulin, and Aaron C. Courville.\nImproved\ntraining of wasserstein gans. In Advances in Neural Information Processing Systems, pp. 5767\u20135777, 2017.\nAnisa Halimi, Swanand Kadhe, Ambrish Rawat, and Nathalie Baracaldo. Federated unlearning: How to effi-\nciently erase a client in fl? CoRR, abs/2207.05521, 2022.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross B. Girshick. Masked autoencoders\nare scalable vision learners. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\n2022, New Orleans, LA, USA, June 18-24, 2022, pp. 15979\u201315988. IEEE, 2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained\nby a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information\nProcessing Systems, pp. 6626\u20136637, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in neural\ninformation processing systems, pp. 6840\u20136851, 2020.\nEmiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Sali-\nmans. Autoregressive diffusion models. In The Tenth International Conference on Learning Representations,\nICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\nJinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, Pranay Sharma, and Sijia\nLiu. Model sparsification can simplify machine unlearning. CoRR, abs/2304.04934, 2023.\n11\nPublished as a conference paper at ICLR 2024\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA,\nUSA, June 16-20, 2019, pp. 4401\u20134410. Computer Vision Foundation / IEEE, 2019.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of stylegan. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pp. 8107\u20138116. Computer Vision Foundation\n/ IEEE, 2020.\nDiederik P. Kingma and Max Welling. An introduction to variational autoencoders. Found. Trends Mach.\nLearn., 12(4):307\u2013392, 2019.\nLingpeng Kong, Cyprien de Masson d\u2019Autume, Lei Yu, Wang Ling, Zihang Dai, and Dani Yogatama. A mutual\ninformation maximization perspective of language representation learning. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\nZhifeng Kong and Kamalika Chaudhuri.\nData redaction from conditional generative models.\nCoRR,\nabs/2305.11351, 2023.\nDilip Krishnan, Piotr Teterwak, Aaron Sarna, Aaron Maschinot, Ce Liu, David Belanger, and William T. Free-\nman. Boundless: Generative adversarial networks for image extension. In 2019 IEEE/CVF International\nConference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pp.\n10520\u201310529. IEEE, 2019.\nAditya Kuppa, Lamine M. Aouad, and Nhien-An Le-Khac. Towards improving privacy of synthetic datasets. In\nPrivacy Technologies and Policy - 9th Annual Privacy Forum, APF 2021, Oslo, Norway, June 17-18, 2021,\nProceedings, volume 12703 of Lecture Notes in Computer Science, pp. 106\u2013119. Springer, 2021.\nMeghdad Kurmanji, Peter Triantafillou, and Eleni Triantafillou.\nTowards unbounded machine unlearning.\nCoRR, abs/2302.09880, 2023.\nTianhong Li, Huiwen Chang, Shlok Kumar Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. MAGE:\nmasked generative encoder to unify representation learning and image synthesis. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp.\n2142\u20132152. IEEE, 2023.\nNils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and Santiago Zanella B\u00b4eguelin. An-\nalyzing leakage of personally identifiable information in language models. In 44th IEEE Symposium on\nSecurity and Privacy, SP 2023, San Francisco, CA, USA, May 21-25, 2023, pp. 346\u2013363. IEEE, 2023.\nSaemi Moon, Seunghyuk Cho, and Dongwoo Kim. Feature unlearning for generative models via implicit\nfeedback. CoRR, abs/2303.05699, 2023.\nSeth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi. Descent-to-delete: Gradient-based methods for machine\nunlearning. In Algorithmic Learning Theory, 16-19 March 2021, Virtual Conference, Worldwide, volume\n132 of Proceedings of Machine Learning Research, pp. 931\u2013962. PMLR, 2021.\nThanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc\nViet Hung Nguyen. A survey of machine unlearning. CoRR, abs/2209.02299, 2022.\nCanada Parliament. The personal information protection and electronic documents act (pipeda), 2019. URL\nhttps://laws-lois.justice.gc.ca/PDF/P-8.6.pdf.\nEuropean Parliament and European Union Council. Regulation (eu) 2016/679 of the european parliament and of\nthe council of 27 april 2016 on the protection of natural persons with regard to the processing of personal data\nand on the free movement of such data, and repealing directive 95/46/ec (general data protection regulation)\n(text with eea relevance), 2016. URL https://eur-lex.europa.eu/eli/reg/2016/679/oj.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\nZeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00a8opf, Edward Z. Yang, Zachary\nDeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and\nSoumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In Advances in\nNeural Information Processing Systems, pp. 8024\u20138035, 2019.\nBen Poole, Sherjil Ozair, Aaron Van Den Oord, Alex Alemi, and George Tucker. On variational bounds of\nmutual information. In International Conference on Machine Learning, pp. 5171\u20135180. PMLR, 2019.\n12\nPublished as a conference paper at ICLR 2024\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable\nvisual models from natural language supervision. In International Conference on Machine Learning, pp.\n8748\u20138763. PMLR, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-resolution\nimage synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pp. 10674\u201310685. IEEE, 2022.\nChitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J. Fleet,\nand Mohammad Norouzi. Palette: Image-to-image diffusion models. In SIGGRAPH \u201922: Special Interest\nGroup on Computer Graphics and Interactive Techniques Conference, Vancouver, BC, Canada, August 7 -\n11, 2022, pp. 15:1\u201315:10. ACM, 2022a.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, Jonathan Ho, David J. Fleet, and\nMohammad Norouzi. Photorealistic text-to-image diffusion models with deep language understanding. In\nAdvances in Neural Information Processing Systems, pp. 36479\u201336494, 2022b.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In The Tenth\nInternational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. Open-\nReview.net, 2022.\nTim Salimans, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. In Advances in Neural Information Processing Systems, pp. 2226\u20132234, 2016.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy,\nKatherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an open large-\nscale dataset for training next generation image-text models. In Advances in Neural Information Processing\nSystems, 2022.\nGowthami Somepalli, Vasu Singla, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Diffusion art or\ndigital forgery? investigating data replication in diffusion models. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition, CVPR 2023, Vancouver, BC, Canada, June 17-24, 2023, pp. 6048\u20136058.\nIEEE, 2023.\nBowen Song, Soo Min Kwon, Zecheng Zhang, Xinyu Hu, Qing Qu, and Liyue Shen. Solving inverse problems\nwith latent diffusion models via hard data consistency. CoRR, abs/2307.08123, 2023.\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models. In Advances\nin Neural Information Processing Systems, pp. 12438\u201312448, 2020.\nHui Sun, Tianqing Zhu, Wenhan Chang, and Wanlei Zhou. Generative adversarial networks unlearning. CoRR,\nabs/2308.09881, 2023.\nAyush K Tarun, Vikram S Chundawat, Murari Mandal, and Mohan Kankanhalli. Fast yet effective machine\nunlearning. IEEE Transactions on Neural Networks and Learning Systems, 2023a.\nAyush Kumar Tarun, Vikram Singh Chundawat, Murari Mandal, and Mohan S. Kankanhalli. Deep regression\nunlearning. In International Conference on Machine Learning, pp. 33921\u201333939. PMLR, 2023b.\nKushal Tirumala, Aram H. Markosyan, Luke Zettlemoyer, and Armen Aghajanyan. Memorization without\noverfitting: Analyzing the training dynamics of large language models. In Advances in Neural Information\nProcessing Systems, pp. 38274\u201338290, 2022.\nZhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient\nlearners for self-supervised video pre-training.\nIn Advances in Neural Information Processing Systems,\nvolume 35, pp. 10078\u201310093, 2022.\nBram Wallace, Akash Gokul, and Nikhil Naik. EDICT: exact diffusion inversion via coupled transforma-\ntions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2023, Vancouver, BC,\nCanada, June 17-24, 2023, pp. 22532\u201322541. IEEE, 2023.\nAlexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. Machine unlearning of features\nand labels. In 30th Annual Network and Distributed System Security Symposium, NDSS 2023, San Diego,\nCalifornia, USA, February 27 - March 3, 2023. The Internet Society, 2023.\n13\nPublished as a conference paper at ICLR 2024\nMike Wu, Chengxu Zhuang, Milan Mosse, Daniel Yamins, and Noah Goodman. On mutual information in\ncontrastive learning for visual representations. arXiv preprint arXiv:2005.13149, 2020.\nYuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger B. Grosse. On the quantitative analysis of decoder-\nbased generative models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings, 2017.\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. GAN inversion: A\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):3121\u20133138, 2023.\nHeng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. Machine unlearning: A survey. ACM\nComput. Surv., 56(1), aug 2023.\nLing Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang,\nMing-Hsuan Yang, and Bin Cui. Diffusion models: A comprehensive survey of methods and applications.\nCoRR, abs/2209.00796, 2022.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. CoRR,\nabs/2302.05543, 2023.\nXulong Zhang, Jianzong Wang, Ning Cheng, Yifu Sun, Chuanyao Zhang, and Jing Xiao. Machine unlearning\nmethodology based on stochastic teacher network. In Advanced Data Mining and Applications - 19th In-\nternational Conference, ADMA 2023, Shenyang, China, August 21-23, 2023, Proceedings, Part V, volume\n14180 of Lecture Notes in Computer Science, pp. 250\u2013261. Springer, 2023a.\nYongjing Zhang, Zhaobo Lu, Feng Zhang, Hao Wang, and Shaojing Li. Machine unlearning by reversing the\ncontinual learning. Applied Sciences, 13(16):9341, 2023b.\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image\ndatabase for scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(6):\n1452\u20131464, 2017. URL https://github.com/CSAILVision/places365.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In IEEE International Conference on Computer Vision, ICCV 2017,\nVenice, Italy, October 22-29, 2017, pp. 2242\u20132251. IEEE Computer Society, 2017.\n14\nPublished as a conference paper at ICLR 2024\nA\nINFINITELY MANY PROBABILITY WITH INFINITE KL-DIVERGENCE\nIn Section 3.2, we mention that there are infinitely many probability distributions that have infinity KL diver-\ngence with a given distribution. We provide the proof below:\nProposition 1 There are infinitely many probability distributions that have a positively infinite value of KL-\ndivergence with any general discrete probability distribution P(X) that is defined as follows:\n0 <= P(X = i) < 1,\nN\nX\ni=1\nP(X = i) = 1,\ni \u2208 [N],\nN \u2265 2\nProof. Based on P(X), we build another distribution Q(X) as follows:\nQ(X = i) =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\nP (X=i)\n1\u2212P (X=j)\u2212P (X=k)+\u03ba,\nif i \u0338= j and i \u0338= k\n0,\nif i = j\n\u03ba\n1\u2212P (X=j)\u2212P (X=k)+\u03ba,\nif i = k\nj, k \u2208 [N], \u03ba > 0\nwhere j satisfies P(X = j) > 0 and k sastifies j \u0338= k. Clearly, 0 <= Q(X = i) < 1 and PN\ni=1 Q(X = i) =\n1. Therefore, Q(X) is a valid probability distribution.\nWe next compute the KL divergence between P and Q.\nD(P||Q) =\nN\nX\ni=1\nP(X = i)log P(X = i)\nQ(X = i)\n= P(X = j)log\n\u0012P(X = j)\nQ(X = j)\n\u0013\n+\nX\ni\u2208[N],i\u0338=j\nP(X = i)log\n\u0012P(X = i)\nQ(X = i)\n\u0013\n= P(X = j)log\n\u0012P(X = j)\n0\n\u0013\n+\nX\ni\u2208[N],i\u0338=j\nP(X = i)log\n\u0012P(X = i)\nQ(X = i)\n\u0013\n= +\u221e\nWe note that \u03ba can be any positive real number; hence, we can obtain infinitely many Q(X) by varying the\nvalue of \u03ba. Hence, there are infinitely many Q that have a positively infinite value of KL-divergence with P.\nIn general, one can replace the set [N] with any discrete set. This completes our proof.\n\u25a1\nThe proof for the continuous distribution is very similar to the discrete case as shown below.\nProposition 2 There are infinitely many probability distributions that have the a positively infinite value of KL-\ndivergence with any general continuous probability distribution with the following probability density function\n(PDF) p(x):\np(x) \u2265 0,\nx \u2208 S,\nZ\nx\u2208S\np(x)dx = 1\nProof. Based on p(x), we build another distribution with PDF q(x) as follows:\nq(x) =\n(\np(x)\n1\u2212\nR\nx\u2208S1 p(x),\nif x \u2208 S\\(S1)\n0,\nif x \u2208 S1\n,\nS1 \u2282 S\nwhere S1 satisfies 0 <\nR\nx\u2208S1 p(x)dx < 1. Clearly, q(x) \u2265 0 and\nR\nx\u2208S q(x)dx = 1. Therefore, q(x) is a valid\nprobability density function.\nWe next compute the KL divergence between p and q.\nD(p||q) =\nZ\nx\u2208S\np(x)log\n\u0012p(x)\nq(x)\n\u0013\ndx\n=\nZ\nx\u2208S1\np(x)log\n\u0012p(x)\nq(x)\n\u0013\ndx +\nZ\nx\u2208S\\S1\np(x)log\n\u0012p(x)\nq(x)\n\u0013\ndx\n=\nZ\nx\u2208S1\np(x)log\n\u0012p(x)\n0\n\u0013\ndx +\nZ\nx\u2208S\\S1\np(x)log\n\u0012p(x)\nq(x)\n\u0013\ndx\n= +\u221e\nWe note that given a continuous distribution, there are infinitely many possible S1; hence, we can obtain\ninfinitely many q(x) by using different S1. Hence, there are infinitely many q that have a positively infinite\nvalue of KL-divergence with p. This completes our proof.\n\u25a1\n15\nPublished as a conference paper at ICLR 2024\nB\nPROOF OF THEOREM 1\nProof. Directly computing mutual information (MI) between two random variables is not feasible. Fortunately,\nthere are some variational MI bounds that are relatively easy to compute. A popular approach is to lower bound\nMI with InfoNCE (Belghazi et al., 2018; Wu et al., 2020; Poole et al., 2019; Kong et al., 2020). Given two\nrandom variables X1 and X2, MI is bounded by InfoNCE is defined as follows:\nI(X1; X2) \u2265 INCE(X1; X2) = log (K) \u2212 E\n\uf8ee\n\uf8f0\nK\nX\ni=1\n1\nK log\neg1(x1i )T g2(x2i )\nPK\nj=1 ee\ng1(x1i )T g2(x2j )\n\uf8f9\n\uf8fb\n(B.11)\nwhere the expectation is over K independent samples from the joint distribution: \u03a0jp(x1j, x2j); g1 and g2 are\ntwo functions that map the random variables into representation vectors, e.g., the encoder network E\u03b8 of I2I\ngenerative models.\nFollowing the standard practice of InfoNCE, we use the inner product of the encoder output vectors as the critic\nfunction. Therefore, the InfoNCE between the ground truth images X and reconstructed images \u02c6\nX is written\nas follows:\nINCE(X; \u02c6\nX) = log (K) \u2212 Ex,\u02c6x\n\uf8ee\n\uf8f0 1\nK log\neE\u03b80 (xi)T E\u03b80 (\u02c6xi)\nPK\nj=1 eE\u03b80 (xi)T E\u03b80(\u02c6xj)\n\uf8f9\n\uf8fb\n(B.12)\nMultiple works show that the encoder of the I2I generative models can be treated as the inverse function of the\ndecoder (Wallace et al., 2023; Song et al., 2023; Xia et al., 2023; Kingma & Welling, 2019). In other words,\nE\u03b80 (D\u03d50 (z)) = z\n(B.13)\nGiven the assumption that the target model use the same decoder as the original model, we can express X and\n\u02c6\nX as follows:\nx = D\u03d50 (E\u03b80(T (x)))) ,\n\u02c6x = D\u03d50 (E\u03b8(T (x))))\n(B.14)\nBy using the above relationships in Eq. (B.13) and Eq. (B.14), we have\nE\u03b80 (x)T E\u03b80 (\u02c6x) = E\u03b80 (D\u03d50 (E\u03b80 (T (x))))T E\u03b80 (D\u03d50 (E\u03b8 (T (\u02c6x))))\n= E\u03b80 (T (x1))T E\u03b8 (T (x2))\n(B.15)\nRetain set Recall the assumption that the output of the encoder is normalized, i.e., \u2225E\u03b8 (x)\u22252 = 1 and\n\u2225E\u03b80 (x)\u22252 = 1. Therefore, we can rewrite Eq. (B.15) for the ground truth retain samples {xri} and their\nreconstructions {\u02c6xri} and as follows:\nE\u03b80 (xri)T E\u03b80 (\u02c6xri) = E\u03b80 (T (xri))T E\u03b8 (T (xri))\n= 1\n2 \u2225E\u03b80 (T (xri))\u22252\n2 + 1\n2 \u2225E\u03b8 (T (xri))\u22252\n2\n\u2212 1\n2 \u2225E\u03b80 (T (xri)) \u2212 E\u03b8 (T (xri))\u22252\n2\n= 1\n2\n\u00002 \u2212 \u2225E\u03b80 (T (xri)) \u2212 E\u03b8 (T (xri))\u22252\n2\n\u0001\n= 1\n2\n\u00002 \u2212 \u03f52\ni\n\u0001\n= 1 \u2212 \u03f52\ni\n2\n(B.16)\nwhere \u03f5i = \u2225E\u03b80 (T (xri)) \u2212 E\u03b8 (T (xri))\u22252 is the L2 loss between the representation of target model en-\ncoder E\u03b8 and original model encoder E\u03b80. We then bound the E\u03b80 (xri)T E\u03b80\n\u0000\u02c6xrj\n\u0001\nas follows:\nE\u03b80 (xri)T E\u03b80\n\u0000\u02c6xrj\n\u0001\n=E\u03b80 (T (xri))T E\u03b8\n\u0000T\n\u0000xrj\n\u0001\u0001\n=E\u03b80 (T (xri))T \u0000E\u03b8\n\u0000T\n\u0000xrj\n\u0001\u0001\n\u2212 E\u03b80\n\u0000T\n\u0000xrj\n\u0001\u0001\u0001\n+ E\u03b80 (T (xri))T E\u03b80\n\u0000T\n\u0000xrj\n\u0001\u0001\n=E\u03b80 (T (xri))T \u0000E\u03b8\n\u0000T\n\u0000xrj\n\u0001\u0001\n\u2212 E\u03b80\n\u0000T\n\u0000xrj\n\u0001\u0001\u0001\n+ Rij\n\u2264 \u2225E\u03b80 (T (xri))\u22252 \u2217\n\r\rE\u03b8\n\u0000T\n\u0000xrj\n\u0001\u0001\n\u2212 E\u03b80T\n\u0000xrj\n\u0001\r\r\n2 + Rij\n=1 \u2217 \u03f5j + Rij\n=\u03f5j + Rij\n(B.17)\n16\nPublished as a conference paper at ICLR 2024\nwhere Rij = E\u03b80 (T (xri))T E\u03b80\n\u0000T\n\u0000xrj\n\u0001\u0001\nand the \u2018\u2264\u2019 comes from the Cauchy\u2013Schwarz inequality. The\nabove bound is tight if {\u03f5i = 0, i \u2208 [K]}. By combining the Eq. (B.12), Eq. (B.16), and Eq. (B.17), we can\nlink the InfoNCE loss with the L2 loss:\nINCE(Xr; \u02c6\nXr) = log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\neE\u03b80 (xi)T E\u03b80 (\u02c6xi)\nPK\nj=1 eE\u03b80 (xi)T E\u03b80(\u02c6xj)\n\uf8f9\n\uf8fb\n= log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\ne1\u2212\n\u03f52\ni\n2\nPK\nj=1 eE\u03b80 (xi)T E\u03b80(\u02c6xj)\n\uf8f9\n\uf8fb\n\u2265 log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\ne1\u2212\n\u03f52\ni\n2\nPK\nj=1 e\u03f5j+Rij\n\uf8f9\n\uf8fb\n= log (K) \u2212 E\n\" K\nX\ni=1\n1\nK log\n \ne\n\u03f52\ni\n2 \u22121\nK\nX\nj=1\ne\u03f5j+Rij\n!#\n(B.18)\nBy combining Eq. (B.11), we obtain the results on the retain set:\nI(Xr; \u02c6\nXr) \u2265 log (K) \u2212 E\n\" K\nX\ni=1\n1\nK log\n \ne\n\u03f52\ni\n2 \u22121\nK\nX\nj=1\ne\u03f5j+Rij\n!#\n(B.19)\nForget set The proof on the forget set is very similar to the retain set. By adapting Eq. (B.16) and Eq. (B.17)\nfor the forget set, we first calculate the inner product of the embedding vector between n and xf:\nE\u03b80 (ni)T E\u03b80 (\u02c6xfi) = E\u03b80 (T (ni))T E\u03b8 (T (xfi))\n= 1\n2\n\u0000\u2225E\u03b80 (T (ni))\u22252\n2 + \u2225E\u03b8 (T (xfi))\u22252\n2 \u2212 \u2225E\u03b80 (T (ni)) \u2212 E\u03b8 (T (xfi))\u22252\n2\n\u0001\n= 1\n2\n\u00002 \u2212 \u2225E\u03b80 (T (ni)) \u2212 E\u03b8 (T (xfi))\u22252\n2\n\u0001\n= 1\n2\n\u00002 \u2212 \u03f52\ni\n\u0001\n= 1 \u2212 \u03b42\ni\n2\n(B.20)\nand\nE\u03b80 (ni)T E\u03b80\n\u0000\u02c6xfj\n\u0001\n=E\u03b80 (T (ni))T E\u03b8\n\u0000T\n\u0000xfj\n\u0001\u0001\n=E\u03b80 (T (ni))T \u0000E\u03b8\n\u0000T\n\u0000xfj\n\u0001\u0001\n\u2212 E\u03b80 (T (nj))\n\u0001\n+ E\u03b80 (T (ni))T E\u03b80 (T (nj))\n= \u2225E\u03b80 (T (ni))\u22252 \u2217\n\r\rE\u03b8\n\u0000T\n\u0000xfj\n\u0001\u0001\n\u2212 E\u03b80T (nj)\n\r\r\n2 + Fij\n=1 \u2217 \u03b4j + Fij\n=\u03b4j + Fij\n(B.21)\nwhere \u03b4i = \u2225E\u03b80 (T (ni)) \u2212 E\u03b8 (T (xfi))\u22252 and Fij = E\u03b80 (T (ni))T E\u03b80 (T (nj)). xfi are the data sam-\nples in the forget set and ni \u223c N(0, \u03a3). Combining the above two equation with Eq. (B.12):\nINCE(n; \u02c6\nXf) = log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\neE\u03b80 (ni)T E\u03b80(\u02c6xfi)\nPK\nj=1 e\nE\u03b80 (ni)T E\u03b80\n\u0010\n\u02c6xfj\n\u0011\n\uf8f9\n\uf8fb\n= log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\ne1\u2212\n\u03b42\ni\n2\nPK\nj=1 e\nE\u03b80 (ni)T E\u03b80\n\u0010\n\u02c6xfj\n\u0011\n\uf8f9\n\uf8fb\n\u2265 log (K) \u2212 E\n\uf8ee\n\uf8f0 1\nK log\ne1\u2212\n\u03b42\ni\n2\nPK\nj=1 e\u03b4j+Fij\n\uf8f9\n\uf8fb\n= log (K) \u2212 E\n\" K\nX\ni=1\n1\nK log\n \ne\n\u03b42\ni\n2 \u22121\nK\nX\nj=1\ne\u03b4j+Fij\n!#\n(B.22)\n17\nPublished as a conference paper at ICLR 2024\nBy combining the above equation with Eq. (B.11), we obtain the results for the forget set:\nI(n; \u02c6\nXf) \u2265 log (K) \u2212 E\n\" K\nX\ni=1\n1\nK log\n \ne\n\u03b42\ni\n2 \u22121\nK\nX\nj=1\ne\u03b4j+Fij\n!#\n(B.23)\nThis completes our proof.\n\u25a1\nC\nIMPLEMENTATION DETAILS\nC.1\nDATASETS AND CODE BASE\nDiffusion Models. We verify our approach on a diffusion model that is trained on entire Places-365 training\ndataset (Saharia et al., 2022a; Zhou et al., 2017). We randomly select 50 classes out of 365 classes as the\nretain set and another 50 classes as the forget set. For each class, we select 5000 images from the Places-365\ntraining set; we then combine them together as the training set for unlearning. By using the approach defined\nin Eq. (10), we obtain the target model. We then evaluate the obtained model on both forget set and retain set,\nwith 100 images per class from the Places-365 validation set. Hence, we have 5,000 validation images for both\nthe retains and forget sets. Since Saharia et al. did not release the code, our code is implemented based on\nan open source re-implementation (GitHub). Our experiments are using their provided model checkpoint on\nPlaces-365 dataset.\nVQ-GAN We evaluate our approach on a VQ-GAN model that is trained on entire ImageNet-1K training\ndataset (Li et al., 2023; Deng et al., 2009). We randomly select 100 classes out of 1000 classes as the retain set\nand another 100 classes as the forget set. We select 100 images per class from the training set as the training\nset for unlearning. We then apply our proposed approach to the original model and obtain the target model. For\nthe main results reported in Tabel 1 and Tabel 2, we evaluate the obtained model on both forget set and retain\nset, with 50 images per class from the ImageNet-validation set; hence, we have 5,000 validation images for\nboth the retains set and forget set. For the other results (in ablation study), we use a smaller version validation\nset, with 5 images per class from the ImageNet-validation set; hence, we have 500 validation images for both\nthe retains set and forget set. Our code is implemented based on the offically released code of Li et al. (2023)\n(GitHub). Our experiments are using their provided ViT-Base model checkpoints.\nMAE We evaluate our approach on a MAE model and an MAE model that is trained on the entire ImageNet-\n1K training dataset (He et al., 2022). The dataset setup is exactly the same as VQ-GAN (check the upper\nparagraph). Our code is implemented based on the offically released code of He et al. (2022) (GitHub). Our\nexperiments are using their provided ViT-Base model checkpoints.\nC.2\nEVALUATION METRICS\nInception score (IS) For ImageNet-1K, we directly use the Inception-v3 model checkpoint from torchvision\nlibrary to compute the IS. For Places-365, we used the ResNet-50 model checkpoint from the official release to\ncompute IS (Zhou et al., 2017).\nFr\u00b4echet inception distance (FID) For both ImageNet-1K and Places-365, we directly use the backbone net-\nwork of the Inception-v3 model checkpoint from Torchvision library to compute the FID.\nCLIP We use CLIP with ViT-H-14 as the backbone to generate the embedding vectors of each reconstructed\nimage from the original model and unlearned model (Radford et al., 2021). Then we compute the cosine\nsimilarity of these embedding vectors among DF and DR separately.\nC.3\nTRAINING HYPER-PARAMETERS\nPatch Size For all of the models we evaluate in this paper (including diffusion models, VQ-GAN and MAE),\nthe networks are vision transformer (Dosovitskiy et al., 2021) based architecture. We set the size of each patch\nas 16 \u00d7 16 pixels for all experiments. For example, cropping 8 \u00d7 8 patches means removing 128 \u00d7 128 pixels.\nWe set \u03b1 = 0.25 (cf. Eq. (10)).\nDiffusion Models We set the learning rate as 10\u22125 with no weight decay. We use the Adam as the optimizer\nand conduct the unlearning for 3 epochs. We set the input resolution as 256 and set the batch size as 8 per GPU.\nOverall, it takes 1.5 hours on a 8 NVIDIA A10G server. We set \u03b1 = 0.25 (cf. Eq. (10)).\nVQ-GAN&MAE We set the learning rate as 10\u22124 with no weight decay. We use the AdamW as the optimizer\nwith \u03b2 = (0.9, 0.95) and conduct the unlearning for 5 epochs. We set the input resolution as 256 for VQ-GAN\nand 224 for MAE. We set the batch size as 16 per GPU. Overall, it takes one hour on a 4 NVIDIA A10G server.\nWe set \u03b1 = 0.25 (cf. Eq. (10)).\n18\nPublished as a conference paper at ICLR 2024\n0\n200\n400\n600\n0\n200\n400\n600\n0.0\n0.5\n1.0\n1.5\n2.0\n(a) MNIST\n0\n500\n1000\n0\n200\n400\n600\n800\n1000\n0.5\n1.0\n1.5\n2.0\n(b) CIFAR-10\n0\n500\n1000\n0\n200\n400\n600\n800\n1000\n0.25\n0.50\n0.75\n1.00\n1.25\n(c) CIFAR-100\nFigure C.5: Covariance matrix of three commonly datasets. For CIFAR10/100, we convert the\nimages into gray-scale images. We take the absolute value of the covariance matrix for better illus-\ntration.\nAlgorithm 1 Pseudo Code of Our Unlearning Algorithm\n1: Inputs:\nOrginal model h\u03b80,\u03d50 = D\u03d50 \u25e6 E\u03b80\nRetain set DR, Forget set DF\nCoefficient \u03b1, learning rate \u03b6, and #Epochs E\n2: Outputs:\nTarget model h\u03b8,\u03d5 = D\u03d5 \u25e6 E\u03b8\n3: Initialize:\nCopy h\u03b8,\u03d5 to h\u03b8,\u03d5, i.e., h\u03b8,\u03d5 \u21d0 h\u03b80,\u03d50\n4: for e = 1 to E do\n5:\nSample {xr} from DR\n6:\nSample {xf} from DF\n7:\nSample {n} from N(0, \u03a3)\n8:\nEnsure: |{xr}| = |{xf}|, i.e., make retain samples and forget samples balanced\n9:\nCompute loss: l = \u2225E\u03b8(T (xr)) \u2212 E\u03b80(T (xr))\u22252 + \u03b1 \u2225E\u03b8(T (xf)) \u2212 E\u03b80(T (n))\u22252\n10:\nUpdate the parameters of the target encoder E\u03b8: \u03b8 \u21d0 \u03b8 \u2212 \u03b6\u2207\u03b8l\n11: end for\nC.4\nOUR UNLEARNING ALGORITHM\nSelection of \u03a3 in Eq. (10)\nTo conduct our unlearning algorithm, we need to compute the \u03a3 in Eq. (10),\nwhere \u03a3 is the the covariance matrix for the distribution of training images.\nIdeally, we should use the exact \u03a3 of the images measured on the forget set. However, there are some com-\nputational barriers to using the exact \u03a3 for a high-resolution image dataset. Specifically, consider a com-\nmonly used 256\u00d7256 resolution for image generation tasks, the distribution of the generated images will have\n256 \u00d7 256 \u00d7 3 \u2248 2 \u00d7 105 dimensions. The size of the covariance matrix \u03a3 for such a high-dimensional dis-\ntribution is around (2 \u00d7 105)2 = 4 \u00d7 1010, which requires around 144GB memory if stored in float precision\nthus is not practical.\nConsequently, to address the computational barrier of \u03a3, we use some approximated methods derived from\nsome empirical observations on some small datasets. Specifically, we compute the exact \u03a3 for some small-\nscale image dataset, including MNIST and CIFAR10/100.\n\u2022 Off-diagonal elements: To find some empirical inspirations, we compute the covariance matrix for\nthree commonly dataset, MNIST, CIFAR10 and CIFAR100. As shown in Fig. C.5, most of the off-\ndiagonal elements are very close to \u20180\u2019. Hence, in our experiments, we set the off-diagonal elements\nof \u03a3 to \u20180\u2019\n\u2022 Diagonal elements: Since the images are normalized (i.e., subtract the mean value and divided by the\nstandard deviation), we set the diagonal elements of \u03a3 as \u20181\u2019, i.e., \u03a3(i, i) = 1.\nTherefore, we use the identity matrix as the approximation of exact \u03a3:\n\u03a3 = I\nwhere I is the identity matrix. We set \u03a3 = I as the default setup for our approach and baseline methods.\nIn short, using I to approximate \u03a3 is a practical approximation alternative due to the extremely high compu-\ntational costs of \u03a3 for high-resolution images. For future work, given our theoretical analysis, we believe that\n19\nPublished as a conference paper at ICLR 2024\nour approach can achieve better results (lower image quality) on forget set if we can find a way to use the exact\n\u03a3. Hence, we plan to explore the potential to reduce the computation of exact \u03a3 with low-rank approximation\nthus enabling the use of more accurate data-driven \u03a3.\nPseudo code\nBased on our unlearning approach defined in Section 3.3, we implement the unlearning algo-\nrithm for I2I generative models. We provide the pseudo code in Algorithm 1. As shown, for each batch, we\nsample the same number of retain samples and forge samples. We then compute the loss by using Eq. (10) and\nupdate the parameters of the target model\u2019s encoder.\nC.5\nLOSS FUNCTION OF OUR APPROACH\nAs shown in Eq. (10), we input the Gaussian noise to the original model as the reference for the forget set. For\nVQ-GAN and MAE, we can directly use Eq. (10). For diffusion models, in principle, we still replace the forget\nsample with Gaussian noise as the input for the original encoder. We next discuss the details for the diffusion\nmodels.\nWe first write the loss function for the normal training of diffusion models first:\nExE\u03b3\u223cN (0,I)E\u03b7\n\r\r\r\r\r\r\r\nD\u03d5 \u25e6 E\u03b8\n\uf8eb\n\uf8ec\n\uf8edT (x), \u221a\u03b7x +\np\n1 \u2212 \u03b7 \u03b3\n|\n{z\n}\n\u02dcx\n, \u03b7\n\uf8f6\n\uf8f7\n\uf8f8 \u2212 \u03b3\n\r\r\r\r\r\r\r\np\n(C.24)\nwhere x is the ground truth image; T (x) is the transformed images, e.g., a cropped image for image uncropping\ntasks and and low-resolution image for image super-resolution. \u03b7 is the forward process variance coefficient.\nFor more details, please refer to the Saharia et al. (2022a).\nNow, we introduce the unlearning optimization function for diffusion models:\narg min\n\u03b8\nExr,xf ,nE\u03b3\u223cN (0,I)E\u03b7\n\u001a\n\u2225E\u03b8 (T (xr), \u02dcxr, \u03b7) \u2212 E\u03b80 (T (xr), \u02dcxr, \u03b7)\u22252\n+\u03b1 \u2225E\u03b8 (T (xf), \u02dc\nxf, \u03b7) \u2212 E\u03b80 (T (xf), n, \u03b7)\u22252\n\u001b\n,\nxr \u2208 DR, xf \u2208 DF , n \u223c N(0, \u03a3)\n(C.25)\nwhere \u02dcxr = \u221a\u03b7xr + \u221a1 \u2212 \u03b7 \u03b3 and \u02dc\nxf = \u221a\u03b7xf + \u221a1 \u2212 \u03b7 \u03b3. Essentially, we replace the \u02dc\nxf with the\nGaussian noise n as the input for the original encoder. Note that, Saharia et al. adopt an U-Net as the network\narchitecture; thus the encoder has multiple-stage outputs. We flatten these multiple outputs into vectors and\nthen combine them as a single representation vector .\nWe remark the equation for diffusion models looks slightly different from VQ-GAN and MAE, but they are\nactually following the same principle defined in Eq. (10); that is, we replace the real forget samples with the\nGaussian noise as the input for the original encoder.\nC.6\nBASELINES\nWe use the exact same setup as introduced in the main paper and Appendix C.3. We provide the loss function\nfor different baseline methods below. For the baselines, the unlearning is achieved by minimizing the loss\nvalues.\nC.6.1\nMAX LOSS\nMAX LOSS maximizes the training loss w.r.t. the ground truth images on the forget set. Hence, we use the\noriginal loss function for both retains set and forget set, but assign a negative coefficient for the forget set. We\ndo not modify the loss for the retains set.\n\u2022 Diffusion Models\narg min\n\u03b8,\u03d5\nExr,xf E\u03b3\u223cN (0,I)E\u03b7\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr), \u02dc\nxr, \u03b7) \u2212 \u03b3\u22252 \u2212 \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf ),\n\u02dc\nxf , \u03b7) \u2212 \u03b3\u22252\n\u001b\n(C.26)\nwhere \u02dcxr = \u221a\u03b7xr + \u221a1 \u2212 \u03b7 \u03b3 and \u02dc\nxf = \u221a\u03b7xf + \u221a1 \u2212 \u03b7 \u03b3. T (\u00b7) is the transformed function, e.g., a\ncropped image for image uncropping tasks.\n20\nPublished as a conference paper at ICLR 2024\n\u2022 VQ-GAN\narg max\n\u03b8,\u03d5\nExr,xf\n\u001a\nCJ (D\u03d5 \u25e6 E\u03b8 (T (xr)) , xr) \u2212 \u03b1CJ (D\u03d5 \u25e6 E\u03b8 (T (xf)) , xf)\n\u001b\n(C.27)\nwhere J is a discriminator network used to predict whether the images are real images or generated im-\nages; CJ is the cross entropy of the discriminator prediction. We also note that, during unlearning, the\ndiscriminator J is also updated in the normal training way. Please check more details in Li et al. (2023).\n\u2022 MAE\narg min\n\u03b8,\u03d5\nExr,xf\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr)) \u2212 xr\u22252 \u2212 \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf)) \u2212 xf\u22252\n\u001b\n(C.28)\nT (\u00b7) is the transformed function, e.g., a cropped image for image uncropping tasks.\nC.6.2\nRETAIN LABEL\nRETAIN LABEL minimizes training loss by setting the retain samples as the ground truth for the forget set.\n\u2022 Diffusion Models\narg min\n\u03b8,\u03d5\nExr,xf E\u03b3\u223cN (0,I)E\u03b7\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr), \u02dc\nxr, \u03b7) \u2212 \u03b3\u22252 + \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf ), \u02dc\nxr, \u03b7) \u2212 \u03b3\u22252\n\u001b\n(C.29)\nwhere \u02dcxr = \u221a\u03b7xr + \u221a1 \u2212 \u03b7 \u03b3. As shown, the \u02dc\nxf in Eq. (C.26) is replaced by the the retain samples \u02dcxr.\n\u2022 VQ-GAN\narg max\n\u03b8,\u03d5\nExr,xf\n\u001a\nCJ (D\u03d5 \u25e6 E\u03b8 (T (xr)) , xr) + \u03b1CJ (D\u03d5 \u25e6 E\u03b8 (T (xf)) , xr)\n\u001b\n(C.30)\nwhere J is a discriminator network used to predict whether the images are real images or generated images;\nCJ is the cross entropy of the discriminator prediction. As shown, the reference xf in the second term of\nEq. (C.27) is replaced by the retain sample xr.\n\u2022 MAE\narg min\n\u03b8,\u03d5\nExr,xf\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr)) \u2212 xr\u22252 + \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf)) \u2212 n\u22252\n\u001b\n(C.31)\nAs shown, the target \u02dc\nxf in the second term of Eq. (C.28) is replaced by the retain sample xr.\nC.6.3\nNOISY LABEL\nNOISY LABEL minimizes training loss by setting the Gaussian noise as the ground truth images for the for-\nget set. Hence we directly use the original loss function by replacing the ground truth images with standard\nGaussian noise for the forget set. We do not modify the loss for the retains set.\n\u2022 Diffusion Models\narg min\n\u03b8,\u03d5\nExr,xf E\u03b3\u223cN (0,I)E\u03b7\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr), \u02dcxr, \u03b7) \u2212 \u03b3\u22252 + \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf), \u03b7, \u03b7) \u2212 \u03b3\u22252\n\u001b\n(C.32)\nwhere \u02dcxr = \u221a\u03b7xr + \u221a1 \u2212 \u03b7 \u03b3. As shown, the \u02dc\nxf in Eq. (C.26) is replaced by the Gaussian noise \u03b7. T (\u00b7)\nis the transformed function, e.g., a cropped image for image uncropping tasks.\n\u2022 VQ-GAN\narg max\n\u03b8,\u03d5\nExr,xf\n\u001a\nCJ (D\u03d5 \u25e6 E\u03b8 (T (xr)) , xr) + \u03b1CJ (D\u03d5 \u25e6 E\u03b8 (T (xf)) , n)\n\u001b\n(C.33)\nwhere J is a discriminator network used to predict whether the images are real images or generated images;\nCJ is the cross entropy of the discriminator\u2019s prediction. As shown, the reference xf in the second term of\nEq. (C.27) is replaced by the Gaussian noise n.\n\u2022 MAE\narg min\n\u03b8,\u03d5\nExr,xf\n\u001a\n\u2225D\u03d5 \u25e6 E\u03b8 (T (xr)) \u2212 xr\u22252 + \u03b1 \u2225D\u03d5 \u25e6 E\u03b8 (T (xf)) \u2212 n\u22252\n\u001b\n(C.34)\nAs shown, the target \u02dc\nxf in the second term of Eq. (C.28) is replaced by the Gaussian noise n.\n21\nPublished as a conference paper at ICLR 2024\nC.6.4\nRANDOM ENCODER\nRANDOM ENCODER minimizes the L2 loss between the Gaussian noise and the representation vector of the\nencoder for the forget set.\n\u2022 Diffusion Models\narg min\n\u03b8\nExr,xf E\u03b3\u223cN (0,I)E\u03b7\n\u001a\n\u2225E\u03b8 (T (xr), \u02dcxr, \u03b7) \u2212 E\u03b80 (T (xr), \u02dcxr, \u03b7)\u22252\n+\u03b1 \u2225E\u03b8 (T (xf), \u02dc\nxf, \u03b7) \u2212 \u03b7\u22252\n\u001b\n(C.35)\nwhere \u02dcxr = \u221a\u03b7xr + \u221a1 \u2212 \u03b7 \u03b3. As shown, the target for the forget set is directly the Gaussian noise instead\nof its embedding vector.\n\u2022 VQ-GAN&MAE\narg min\n\u03b8\nExr,xf\n\u001a\n\u2225E\u03b8 (T (xr)) \u2212 E\u03b80 (T (xr))\u22252 + \u03b1 \u2225E\u03b8 (T (xf)) \u2212 n\u22252\n\u001b\n(C.36)\nD\nSUPPLEMENTARY RESULTS\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n80\n100\n120\n140\nFID\nForget Set\nRetain Set\n(a) VQ-GAN: FID\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n9\n10\n11\n12\n13\nInception Score\nForget Set\nRetain Set\n(b) VQ-GAN: IS\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n0.65\n0.70\n0.75\n0.80\nCLIP Embedding Distance\nForget Set\nRetain Set\n(c) VQ-GAN: CLIP\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n150\n200\n250\nFID\nForget Set\nRetain Set\n(d) MAE: FID\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nInception Score\nForget Set\nRetain Set\n(e) MAE: IS\n0\n25\n50\n75\n100\n#Images per Class on Retain Set\n0.55\n0.60\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nForget Set\nRetain Set\n(f) MAE: CLIP\nFigure D.6: The performance of our approach under limited availability of the retain sample. The\n\u201c100 images per class\u201d (right side of horizontal-axis) in these plots indicate a full retains set baseline.\nAs shown, by gradually reducing the number of images for the retain set, the performance degrada-\ntion is negligible. Even for the extreme case where we only one image per class, the performance\ndegradation is also small.\nD.1\nROBUSTNESS TO RETAIN SAMPLES AVAILABILITY\nIn Table 1 and Table 2, we report the results under an extreme case where there is no real retain sample\navailable. In this section, we relax the constraints by assuming the limited availability to the real retain samples.\nSpecifically, on ImageNet-1K, we fix the forget set by using 100 images per class for the forget set; in total, we\nhave 10K image for the forget set. As the baseline method, we have a retain set with 100 image per class as\nwell (in total 10K); we call this a full retain set.\nWe next vary the number of images per class for the retain set within the range of {1, 5, 10, 20, 25, 50} and\ncompare the performance of the unlearned models under different values. To balance the number of forget\nsample and retain samples, we over sample the retain samples. For example, if we have 5 image per class for\nthe retain set, we sample these images 20 times (since 100/5 = 20).\n22\nPublished as a conference paper at ICLR 2024\nAs shown in Fig. D.6, compared to the full retain set (100 images per class), our approach is still effective\nwith a slight performance degradation. Combining the results in Table 1 and Table 2, these results show the\napplicability and resilience of our approach in scenarios where actual the retain samples are limited or even\nunavailable.\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(a) Forget set\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(b) Retain set\nFigure D.7: Diffusion models: cropping 4 \u00d7 4 patches at the center of the image, where each patch\nis 16 \u00d7 16 pixels. As shown, our approach has almost identical performance as the original model\n(i.e., before unlearning); the generated images on forget set are some random noise. Overall, we\noutperform all other baseline methods.\nD.2\nDIFFUSION\nWe provide the visualization results under center crop size of 4 \u00d7 4 patches in Fig. D.7. We also provide more\nvisualization results under center crop size of 8 \u00d7 8 patches in Fig. D.8. As shown in Fig. D.7 and Fig. D.8,\nout approach can generate very high quality images on retain set while filling some random noise on the forget\nset. As a contrast, the other baselines methods struggle on the performance drop on the retain set. RANDOM\nENCODER cannot even forget the information on the forget set.\n23\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(a) Forget set\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(b) Retain set\nFigure D.8: Diffusion models: cropping 8 \u00d7 8 patches at the center of the image, where each patch\nis 16 \u00d7 16 pixels. As shown, our approach has almost identical performance as the original model\n(i.e., before unlearning); the generated images on forget set are some random noise. Overall, we\noutperform all other baseline methods.\nD.3\nVQ-GAN\nWe provide the visualization results under center crop size of 4 \u00d7 4 patches in Fig. D.9. We also provide more\nvisualization results under center crop size of 8 \u00d7 8 patches in Fig. D.10. As shown in Fig. D.9 and Fig. D.10,\nout approach can generate very high quality images on retain set while fill some random noise on the forget set.\nRETAIN LABEL cannot even forget the information on the forget set. Moreover, as shown in Fig. D.9, there are\ntwo type of fishes; one is in forget set and another one is in retain set (the fourth and fifth rows). Our approach\ncan handle this subtle difference among difference classes.\nD.4\nMAE\nRandom Masking\nIn the original paper (He et al., 2022), MAE primarily focuses on the reconstruction of\nrandomly masked images. Hence, besides the center cropping reported in the main paper, we also report the\nresults under random masking. As shown in Fig. D.11 and Fig. D.12, the performance on the retain set of the\n24\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(a) Forget set\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(b) Retain set\nFigure D.9: VQ-GAN: cropping 4\u00d74 patches at the center of the image, where each patch is 16\u00d716\npixels. As shown, our approach has almost identical performance as the original model (i.e., before\nunlearning); the generated images on forget set are some random noise.\nunlearned model is almost identical as original model. In contrast, there is a significant performance drop on\nthe forget set.\nE\nABLATION STUDY\nE.1\nVARYING CROPPING PATTERN AND RATIO\nIn the main paper, we primarily show the results of uncropping/inpainting tasks. We vary the cropping patterns\nand use the VQ-GAN to generate the images under these different patterns. We shown the generated images\nwith downward extension in Fig. E.13, upward extension in Fig. E.14, leftward extension in Fig. E.15, rightward\nextension in Fig. E.16, and outpaint in Fig. E.17, respectively. As shown in these images, our method is robust\nto different cropping types.\nAs shown in Fig. E.18 and Fig. E.19, we also vary the input cropping ratio and report the FID and CLIP\nembedding distance. Clearly, our approach is robust to different cropping ratios.\n25\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(a) Forget set\nGround Truth\nInput \nOriginal Model\nMAX LOSS\nNOISY LABEL\nRETAIN LABEL\nRANDOM ENCODER\nOurs\n(b) Retain set\nFigure D.10: VQ-GAN: cropping 8 \u00d7 8 patches at the center of the image, where each patch is\n16\u00d716 pixels. As shown, our approach has almost identical performance as the original model (i.e.,\nbefore unlearning); the generated images on forget set are some random noise.\nNoise type.\nWe compare the performance of Gaussian Noise used in our method with uniform noise for\nVQ-GAN and MAE. Specifically, we set the noise in Eq. (10) as n \u223c U[\u22121, 1] then conduct the unlearning\nunder the same setup. We use the obtained models to generate 500 images for both retain set and forget set. We\nthen compute multiple metrics for different noise types. As shown in Fig. E.20, Gaussian noise achieves better\nforgetting in the forget set in general (i.e., higher FID, lower IS and lower CLIP on forget sets). These results\nempirically verify the benefits of using Gaussian Noise, which coincides with our analysis (cf. Lemma 1).\nE.2\nCROSS VALIDATION OF GENERATED IMAGES\nTo further verify the generated images do not have the information from forget set, we conducted the following\nexperiment for VQ-GAN:\n\u2022 We first conduct the unlearning for VQ-GAN for ImageNet to obtain the unlearned model, under the\nsame setup mentioned in Section 4.\n26\nPublished as a conference paper at ICLR 2024\n20\n40\n60\nInput Masking Ratio(%)\n100\n150\n200\n250\nFID\nOriginal: Forget Set\nUnlearn: Forget Set\nOriginal: Retain Set\nUnlearn: Retain Set\n(a) MAE: FID\n20\n40\n60\nInput Masking Ratio(%)\n5\n10\n15\n20\nInception Score\nOriginal: Forget Set\nUnlearn: Forget Set\nOriginal: Retain Set\nUnlearn: Retain Set\n(b) MAE: IS\n20\n40\n60\nInput Masking Ratio(%)\n0.4\n0.5\n0.6\n0.7\n0.8\nCLIP Embedding Distance\nOriginal: Forget Set\nUnlearn: Forget Set\nOriginal: Retain Set\nUnlearn: Retain Set\n(c) MAE: CLIP\nFigure D.11: The quality of reconstructed images given random masked images by MAE under\nvarying random masking ratio; e.g., masking 128 out of 256 patches means 50% cropping ratio. We\ncompare the original model and the unlearned model by our approach. As shown, the performance\nof these two models on the retain set are almost identical, while the unlearned model has a significant\nperformance drop on the forget set. See Fig. D.4 for some examples of generated images.\nTable E.4: The comparison of cross validation in Appendix E.2. We first use different models to do\nthe inpainting tasks, i.e., reconstructing the central cropped patches (FIRST reconstructions). Given\nthe FIRST reconstructions as input, we then use the original model (before unlearning) to do the\noutpainting tasks and get the SECOND reconstructions, i.e., re-generated the outer patches based\non these reconstructed central patches from FIRST reconstructions. We then evaluate the quality of\nthese SECOND reconstructed images under various metrics. Lower FID, higher IS, and higher CLIP\nvalues indicate higher image quality.\nFID\nIS\nCLIP\nDR\nDF\nDR\nDF\nDR\nDF\nOriginal Model\n36.14\n28.96\n27.28\n27.95\n0.37\n0.49\nUnlearned Model\n36.74\n156.68\n26.62\n7.67\n0.37\n0.28\n\u2022 Second, given the center-cropped input images (cropping central 8 \u00d7 8 patches), we then use this\nunlearned model to generate the images on both forget set and retain set (i.e., image uncrop-\nping/inpainting). Here we call them FIRST generated images.\n\u2022 Given these FIRST generated images, we only keep the reconstructed central 8 \u00d7 8 patches (cropping\nthe outer ones) as the new input to the original VQ-GAN (i.e., the model before unlearning) and get\nthe newly generated images (here we call them SECOND generated images). We then evaluate the\nquality of SECOND generated images for both forget set and retain set and report the results.\n\u2022 We also conduct the above process for the original model (i.e., before unlearning) as the base-\nline/reference. The results are given below:\nAs shown in Table E.4 and Figure E.21, compared to the original model, given the noise (i.e., FIRST generated\nimages on the forget set) from the unlearned model as the input, the SECOND generated images has very low\nquality in terms of all these three metrics. This means that the noise indeed are not correlated with the real\nforget images.\nIn contrast, the performance on the retain set is almost the same before and after unlearning. This indicates that\nour approach indeed preserves the knowledge on the retain set well.\n27\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure D.12: Reconstruction of random masked images by MAE. For a single image, we test 25%\nmasking ratio and 50% masking ratio.\n28\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure E.13: Ablation study: Downward extension by VQ-GAN. We visualize he performance of\nthe original model (before unlearning) and the obtained model by our approach (after unlearning).\nWe show results on both forget set and retain set.\n29\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure E.14: Ablation study: Upward extension by VQ-GAN. We visualize he performance of the\noriginal model (before unlearning) and the obtained model by our approach (after unlearning). We\nshow results on both forget set and retain set.\n30\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure E.15: Ablation study: Leftward extension by VQ-GAN. We visualize he performance of the\noriginal model (before unlearning) and the obtained model by our approach (after unlearning). We\nshow results on both forget set and retain set.\n31\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure E.16: Ablation study: Rightward extension by VQ-GAN. We visualize he performance of\nthe original model (before unlearning) and the obtained model by our approach (after unlearning).\nWe show results on both forget set and retain set.\n32\nPublished as a conference paper at ICLR 2024\nGround Truth\nInput \nBefore Unlearning \nOurs\nMax_Loss\nNoisy_Label\nRetain_Label\nRandom_Encoder\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(a) Forget set\nGround Truth\nInput\nBefore Unlearning After Unlearning\nGround Truth\nInput\nBefore Unlearning After Unlearning\n(b) Retain set\nFigure E.17: Ablation study: Outpaint by VQ-GAN. We visualize he performance of the original\nmodel (before unlearning) and the obtained model by our approach (after unlearning). We show\nresults on both forget set and retain set.\n33\nPublished as a conference paper at ICLR 2024\n20\n40\n60\nCropping Ratio(%)\n75\n100\n125\n150\nFID\nUncropping/Inpaint\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n60\n80\n100\n120\n140\nFID\nUpward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n60\n80\n100\nFID\nDownward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n60\n80\n100\nFID\nLeftward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n60\n80\n100\nFID\nRightward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n20\n40\n60\n80\n100\nFID\nOutpaint\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\nFigure E.18: Ablation study: FID vs. input cropping ratios under different I2I generation tasks.\n20\n40\n60\nCropping Ratio(%)\n0.60\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nUncropping/Inpaint\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n0.60\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nUpward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nDownward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n0.60\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nLeftward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n0.60\n0.65\n0.70\n0.75\nCLIP Embedding Distance\nRightward Extension\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\n20\n40\n60\nCropping Ratio(%)\n0.6\n0.7\n0.8\nCLIP Embedding Distance\nOutpaint\nOrginal: Forget Set\nOrginal: Retain Set\nUnlearn: Forget Set\nUnlearn: Retain Set\nFigure E.19: Ablation study: CLIP embedding distance vs. input cropping ratios under different I2I\ngeneration tasks.\n34\nPublished as a conference paper at ICLR 2024\n0\n20\n40\nCropping Ratio(%)\n100\n150\n200\nFID\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(a) VQ-GAN: FID\n0\n10\n20\n30\n40\nCropping Ratio(%)\n6\n8\n10\n12\nInception Score\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(b) VQ-GAN: IS\n0\n20\n40\nCropping Ratio(%)\n0.4\n0.5\n0.6\n0.7\nCLIP Embedding Distance\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(c) VQ-GAN: CLIP\n0\n10\n20\n30\n40\nCropping Ratio(%)\n50\n100\n150\n200\nFID\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(d) MAE: FID\n0\n10\n20\n30\n40\nCropping Ratio(%)\n5\n10\n15\n20\n25\nInception Score\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(e) MAE: IS\n0\n10\n20\n30\n40\nCropping Ratio(%)\n0.7\n0.8\n0.9\n1.0\nCLIP Embedding Distance\nUniform: Forget Set\nGaussian: Forget Set\nUniform: Retain Set\nGaussian: Retain Set\n(f) MAE: CLIP\nFigure E.20: Ablation study of noise type. We test the performance under varying ratios of central\ncropping; e.g., cropping 8 \u00d7 8 out of 256 patches means a 25% cropping ratio. For VQ-GAN and\nMAE, Gaussian noise achieves better forgetting in the forget set in general (i.e., it achieves higher\nFID, lower IS and lower CLIP on forget set).\nGround Truth\nOriginal Model\nUnlearned Model\n(a) Forget Set\nGround Truth\nOriginal Model\nUnlearned Model\n(b) Retain Set\nFigure E.21: Visualization of cross validation in Appendix E.2. We first use different models to do\nthe inpainting tasks, i.e., reconstructing the central cropped patches (FIRST reconstructions). Given\nthe FIRST reconstructions as input, we then use the original model (before unlearning) to do the\noutpainting tasks and get the SECOND reconstructions, i.e., re-generated the outer patches based\non these reconstructed central patches from FIRST reconstructions. We then evaluate the quality of\nthese SECOND reconstructed images under various metrics.\n35\n"
  },
  {
    "title": "AToM: Amortized Text-to-Mesh using 2D Diffusion",
    "link": "https://arxiv.org/pdf/2402.00867.pdf",
    "upvote": "10",
    "text": "AToM: Amortized Text-to-Mesh using 2D Diffusion\nGuocheng Qian1,2\nJunli Cao1\nAliaksandr Siarohin1\nYash Kant1,3\nChaoyang Wang1\nMichael Vasilkovsky1\nHsin-Ying Lee1\nYuwei Fang1\nIvan Skorokhodov1\nPeiye Zhuang1\nIgor Gilitschenski3\nJian Ren1\nBernard Ghanem2\nKfir Aberman1\nSergey Tulyakov1\n1Snap Research\n2KAUST\n3University of Toronto\n\u201cA broken eggshell with a chick standing next\u201d\n\u201cA sourdough loaf on a cutting board\u201d\n\u201cA hippo wearing a sweater\u201d\n\u201cA lion\ufb01sh\u201d\n\u201cTwo raccoons playing poker\u201d\n\u201cA skiing penguin wearing a puffy jacket\u201d\nFigure 1: Our Amortized Text-to-Mesh model (AToM), trained only on 2D diffusion prior, can generate textured meshes\nfrom texts in less than 1 second. See https://snap-research.github.io/AToM for immersive visualization.\nAbstract\nWe introduce Amortized Text-to-Mesh (AToM), a feed-\nforward text-to-mesh framework optimized across multiple\ntext prompts simultaneously. In contrast to existing text-\nto-3D methods that often entail time-consuming per-prompt\noptimization and commonly output representations other\nthan polygonal meshes, AToM directly generates high-\nquality textured meshes in less than 1 second with around\n10\u00d7 reduction in the training cost, and generalizes to un-\nseen prompts. Our key idea is a novel triplane-based text-\nto-mesh architecture with a two-stage amortized optimiza-\ntion strategy that ensures stable training and enables scal-\nability. Through extensive experiments on various prompt\nbenchmarks, AToM significantly outperforms state-of-the-\nart amortized approaches with over 4\u00d7 higher accuracy\n(in DF415 dataset) and produces more distinguishable and\nhigher-quality 3D outputs. AToM demonstrates strong gen-\neralizability, offering finegrained 3D assets for unseen in-\nterpolated prompts without further optimization during in-\nference, unlike per-prompt solutions.\n1. Introduction\nPolygonal meshes constitute a widely used and efficient\nrepresentation of 3D shapes. As we enter a revolutionary\nphase of Generative AI [41,46], the creation of 3D meshes\nhas become increasingly intuitive, with controls transition-\ning from complex, handcrafted graphics handles [50] to\nsimple textual inputs [14].\nCurrent mainstream text-to-\nmesh models [8, 25, 47] can generate impressive textured\nmeshes through score distillation sampling [38] without 3D\nsupervision. Despite the growing interest, these methods\nrequire a per-prompt optimization that trains a standalone\nmodel for each prompt, which is time and computational\nconsuming. More importantly, per-prompt solutions cannot\ngeneralize to unseen prompts.\nRecently, ATT3D [29] presents amortized text-to-3D,\nwhich optimizes a text-to-3D system in many prompts si-\nmultaneously unlike per-prompt solutions. This amortized\noptimization not only significantly reduces training time but\nalso allows generalizability due to the feature sharing across\nprompts. Unfortunately, ATT3D is limited predominantly\nto outputting 3D objects represented by Neural Radiance\nFields (NeRF) [31]. An amortized text-to-mesh system is\nof more practical importance, but is under explored. First,\nmesh is more widely used in most developments such as\ngaming and design. However, converting NeRFs to meshes\nis inaccurate and might require further optimization that is\ncostly [28]. Second, training text-to-mesh directly facilities\na more efficient rasterizer, allowing higher-resolution ren-\nders that help recover details in geometry and texture com-\npared to text-to-NeRF [8,25].\nExtending\nATT3D\nto\namortized\nmesh\ngeneration\n1\narXiv:2402.00867v1  [cs.CV]  1 Feb 2024\n          Per-prompt text-to-mesh\nNaive amortized text-to-mesh\nAToM (Ours)\n    Shape Quality\n Efficiency\n\u201cA zoomed out DSLR photo of a robot couple \ufb01ne dining\u201d\n    Shape Quality\n Efficiency\n    Shape Quality\n Efficiency\nFigure 2.\nPer-prompt text-to-mesh [47] generates high-quality\nresults but demands expensive optimization. Naively extending\nATT3D for mesh generation leads to divergent training and poor\ngeometry.\nAToM introduces a triplane-based architecture with\ntwo-stage amortized optimization for enhanced stability. AToM\nefficiently generates textured meshes for various text prompts in\nunder one second during inference.\npresents challenges in unstable training that causes poor\ngeometry. Our observations highlight two primary factors\ncontributing to this instability of ATT3D for mesh gen-\neration: the architecture and the optimization.\nFirst,\nATT3D adopted a HyperNetwork-based [18] Instant-NGP\n[34] positional encoding for text-conditioned NeRF gener-\nation. This HyperNetwork introduces numerical instabil-\nity and demands special layers such as spectral normal-\nization [33] to alleviate. The instability is more severe in\nlarge-scale datasets, leading to indistinguishable 3D content\nfor different prompts, limiting generalizability of the sys-\ntem. See the two similar robots generated from two distinct\nprompts in 4th row 1st and 3rd column in Fig. 6. Second,\nthe end-to-end optimization for text-to-mesh also triggers\ninstability due to the topological limitations of differential\nmesh representation [42], leading to suboptimal geometry.\nOverall, naively extending ATT3D to mesh generation re-\nsults in divergent optimization and the inability to generate\nany 3D object after training. Refer to the second column 1\nin Fig. 2 for illustration.\nWe thus introduce AToM, the first amortized approach\nfor direct Text-to-Mesh generation. To address architec-\nture instability, AToM introduces a text-to-triplane network\nin replacement of HyperNetwork for the positional encod-\ning. Our text-to-triplane demonstrates greater resilience to\nparameter changes and generally yields higher-quality and\nsignificantly more distinguishable 3D content compared to\nthe ATT3D\u2019s HyperNetwork counterpart. We then propose\nto use triplane features as input to subsequent signed dis-\ntance function (SDF), deformation, and color networks to\ngenerate geometry and texture for differentiable mesh [42].\nMoreover, to stabilize optimization, we propose a two-\nstage amortized training in contrast to naive end-to-end op-\ntimization. Our first stage trains text-to-triplane, SDF, and\ncolor networks through low-resolution (64 \u00d7 64) volumet-\nric rendering. Volumetric rendering\u2019s consideration of mul-\ntiple points per ray contributes to a stable optimization of\nthe SDF network. In our second stage, these networks un-\ndergo refinement, and an additional deformation network\nis learned to manipulate the mesh vertices for finegrained\ndetails.\nUtilizing efficient mesh rasterization allows for\n512 \u00d7 512 resolution renders in this phase.\nAfter train-\ning, AToM enables ultra-fast inference, generating textured\nmeshes in under one second. The main contributions of\nthis work can be summarized as follows:\n\u2022 We propose AToM, the first amortized text-to-mesh\nmodel that is optimized across multiple text prompts\nwithout 3D supervision. AToM trains a triplane-based\nmesh generator, which contributes to stable optimization\nand generalizability to large-scale datasets.\n\u2022 We introduce a two-stage amortized optimization, where\nthe first stage uses low-resolution volumetric rendering,\nand the second stage utilizes high-resolution mesh ras-\nterization. Our two-stage amortized training significantly\nimproves the quality of the textured mesh.\n\u2022 AToM generates high-quality textured meshes in less\nthan 1 second from a text prompt and generalizes to un-\nseen prompts with no further optimization.\n2. Related Work\nFeed-Forward 3D Generation.\nThe evolution of feed-\nforward 3D generation models has followed the success\nof 2D generation, drawing inspiration from generative\nadversarial networks [5, 15, 22] and autoregressive net-\nworks [32, 56] to diffusion models [7, 10, 17, 21, 35, 57].\nVarious 3D representations have been studied, including\npoint clouds [23, 44, 54], volumes [11, 26, 45, 55], and\nmeshes [9, 37, 50, 58]. Despite their success, these meth-\nods are bounded by the availability of high-quality 3D data\nand thus most previous works merely applied to certain cat-\negories, such as cars and human faces [4, 14]. The concur-\nrent work Instant3D [24] shows the potential to train a gen-\neralizable 3D generative model in the recent large-scale 3D\ndataset [13]. We note that training in 3D dataset or through\nscore distillation sampling (SDS) [38] are two orthogonal\ndirections. The latter does not require any 3D data, which\naligns with our interest. We elaborate the text-to-3D by SDS\nnext.\nPer-prompt 3D Optimization. Recent studies have sug-\ngested leveraging pretrained text-to-image models for 3D\ngeneration without 3D data supervision\n[6, 20, 30, 38,\n39]. Subsequent works introduced enhancements in direc-\ntions such as multiview image gradient aggregation [49],\ntwo-stage training optimization [25], representation tech-\nniques [8], increased diversity [53], and optimization tech-\nniques [59]. Being able to generate high-quality 3D con-\ntent, per-prompt optimization is receiving increasing inter-\nest. However, these methods are time-intensive, as each as-\nset needs a separate optimization process and usually re-\nquires tedious parameter tuning. Per-prompt optimization\nalso overfits to the single training prompt. Instead, we are\n2\nText Encoder\n\u201ca DSLR photo of a \npeacock on \u201d\n\u201ca DSLR photo of a \npeacock on a\u201d\n\u201ca DSLR photo of a \ndragon statue\u201d\nText Prompts\nToken Embedding e\nText-to-\nTriplane\nSDF Module\nDeformation Module\nTexture Module\n(xyz, pxyz, fxyz)\nDifferentiable Mesh\nInference Stage: Feedfoward Text-to-Mesh Generation\nText-to-Image \nDiffusion Model\nSDF Module\nTexture Module\nHigh-Resolution\nMesh Rasterization\nAdd Noise\nLow-Resolution\nVolumetric Rendering\nNeRF\nTraining Stage 1: \nNeuS Volumetric Optimization \nTraining Stage 2: \nMesh Optimization \nAdd Noise\nSDS Loss\nFigure 3. Inference and training of AToM. AToM inference (up): AToM generates textured meshes from given prompts in less than a\nsecond in inference. The text-to-mesh generator proposed in AToM consists of three components: a) a text encoder that tokenizes the input\nprompt, b) a text-to-triplane network that outputs a triplane representation from the text embedding, and c) a 3D network that generates\nSDF, vertex deformation, and color to form a differential mesh from positions and triplane features. AToM Training (bottom): AToM\nutilizes a two-stage amortized optimization, where the first stage leverages stable volumetric optimization to train only the SDF and texture\nmodules using low-resolution renders. The seconds stage uses mesh rasterization to optimize the whole network through high-resolution\nrenders. In both stages, AToM is trained simultaneously on many prompts through the guidance of a text-to-image diffusion prior without\nany 3D data supervision.\ninterested in generalizable and efficient text-to-3D.\nAmortized Optimization. Unlike the time-consuming per-\nprompt optimization, ATT3D [29] proposed to amortize [1]\nthe optimization across multiple prompts.\nThis enables\nmore efficient synthesis of 3D objects in seconds, facili-\ntating interpolation between prompts and generalization to\nunseen prompts.\nHowever, ATT3D is limited to small-\nscale datasets, generating 3D content indistinguishable be-\ntween prompts in larger-scale benchmark, e.g. DF415 (415\nprompts from DreamFusion [38]).\nAdditionally, ATT3D\nsolely produces NeRF that limits the quality. A recent con-\ncurrent work HyperFields [3] attempted to improve ATT3D\nwith a stronger dynamic hypernetwork. In this work, we are\nmore interested in amortized text-to-mesh, that generates\ntextured meshes in under one second and can be applied to\nlarge-scale prompt datasets.\n3. Method\n3.1. AToM Pipeline\nFig. 3 demonstrates the pipeline of the proposed Amor-\ntized Text-to-Mesh (AToM). Unlike mainstream per-prompt\nsolutions [8, 25, 38, 47] that train a standalone 3D model\nfor a specific prompt, AToM trains a text-conditioned mesh\ngenerator, which can efficiently produce textured meshes\nfrom various text prompts in inference stage.\nThe net-\nwork architecture of AToM consists of three components:\n(1) a text encoder, (2) a text-to-triplane network, and (3) a\ntriplane-to-mesh generator.\nText encoder embeds the input text prompt. For simplic-\nity, we use the same frozen pretrained T5 XXL [40] as the\ntext-to-image diffusion model DeepFloyd IF [12]. The text\nembedding e \u2208 RLe\u00d7Ce obtained is used as input to the fol-\nlowing networks to generate the desired 3D content, where\nLe and Ce represent the number of tokens (e.g. 77) and the\ndimension of their embedding (e.g. 4096), respectively.\nText-to-triplane network T outputs a triplane representa-\ntion from the text embedding e. T is composed of two parts.\nThe first part is a linear projection that maps the averaged\ntext embedding from RCe to R3CT HT WT , which is then\nreshaped to a triplane [4] representation R3\u00d7CT \u00d7HT \u00d7WT .\nNote that CT is the number of triplane features and HT \u00d7\nWT denotes the height and width of each plane. The sec-\nond part of T is a text-conditioned, 3D-aware Triplane Con-\nvNeXt network to enhance the triplane features. We con-\nstruct the network by stacking N ConvNeXt blocks, where\neach block consists of four modules. The first is a multihead\ncross-attention module [48] between text embedding and\ntriplane features. Each pixel of a plane is used as one query\ntoken, while each text token serves as the key and value.\nThis cross-attention module is beneficial for higher gener-\nalizability and better quality especially when the dataset is\nlarge-scale (see \u00a75). The second module is a 3D-aware con-\nvolution borrowed from [52]. Due to the fact that each pixel\n3\n(i, j) in a plane can be associated with the whole column or\nrow in the other two planes, this 3D-aware convolution is\nproposed to concatenate the features of (i, j) with the av-\neraged features of (i, :) and (:, j), and then perform a 2D\nconvolution.\nThe third is a depth-wise convolution with\nkernel size 7 \u00d7 7 to aggregate spatial information per chan-\nnel, as inspired from ConvNeXt [27]. The last is a feedfor-\nward network that is made up of two inverted linear layers\nto mix channel information. We perform convolutions and\nlinear layers per plane, since we empirically find otherwise\nthe information of non-correlated regions will be mixed that\nmight slightly degrade the performance. The 3D-aware con-\nvolution and the feedforward network can be efficiently im-\nplemented by group convolutions using PyTorch. A resid-\nual connection is added to each module to avoid gradient\nvanishing. See Appendix for the illustration of our text-to-\ntriplane achitecture and the Triplane ConvNeXt block.\nTriplane-to-Mesh generator \u03bd generates a differential\nmesh from the triplane features. We use DMTet [42] as the\nmesh representation. DMTet represents a differential tetra-\nhedral geometry as a signed distance field (SDF) defined\non a deformable tetrahedral grid. The mesh vertices V and\ntheir connections (mesh faces) are predefined on the initial\ngrid. A deformation network is optimized to offset these\npredefined vertices for refined triangle meshes and finer de-\ntails. The SDF values of the shifted points are learned by an\nSDF network to represent the final distance to the surface.\nThe zero-level set of SDF values represents the surface of\nthe triangle meshe. Moreover, a color network is leveraged\nto learn the color of each vertex. Overall, we employ three\nseparate networks, i.e. the SDF network, the deformation\nnetwork, and the color network, to optimize DMTet. The\ninput of each network is the concatenation of the triplane\nfeatures and the positions with their sinusoidal encoding of\nthe predefined mesh vertices.\nThe inference stage of AToM is a feedforward progress\nthat gets a textured mesh directly from the text input, and\nis finished in less than one second without the need of op-\ntimization. During inference, once a text is given, AToM\nfirst gets the text embedding e through the text encoder and\nnext passes e to the triplane generator to obtain the fea-\ntures. AToM then inputs the vertices of the DTMet grid to\nquery the triplane features, encodes the vertices positions,\nand passes the concatenation of triplane features, positional\nencoding, and positions to the SDF, the deformation, and\nthe color networks to output the textured mesh.\n3.2. Two-Stage Amortized Optimization\nOptimizing a text-to-mesh end-to-end is problematic due\nto the topology constraints e.g. triangle connections, inher-\nent in the differentiable mesh. Fantasia3D [8] makes such\na direct training of text-to-mesh possible, but requires so-\nphisticated parameter tuning for each prompt.\nRefer to\nthe rather poor geometry without per-prompt tuning in Ap-\npendix. GET3D [14] also shows the possibility of training\nan unconditional mesh generator, but is limited to specific\ncategories such as chair and car, and requires the ground-\ntruth 3D data during training. We show that a trivial end-to-\nend training for AToM leads to divergent optimization (\u00a75).\nTo address this unstable optimization, we propose a two-\nstage amortized optimization: a NeuS volumetric training\nas warmup followed by a differentiable mesh training.\nFirst stage: volumetric optimization. We use volumetric\nrendering in the first stage to warmup the SDF network. In\nthis stage, we use the NeuS [51] representation, and only\noptimize the triplane generator, the SDF network, and the\ncolor network. We render a low-resolution (e.g. 64 \u00d7 64)\nimage by accumulating points\u2019 colors along the ray. The\nobtained renderings are added noise and passed to the text-\nto-image diffusion prior to provide the guidance through\nSDS loss [38]. Mathematically, given a pixel in the ren-\ndered image, the ray emitted from this pixel is denoted as\n{pi = o + tiv | i = 1, . . . , n, ti < ti+1}, where o is the\ncenter of the camera, v is the unit direction vector of the\nray, n is the number of points per ray, t denotes the distance\nfrom o. NeuS volumetric rendering is given by:\n\u02c6C =\nn\nX\ni=1\nTi\u03b1ici,\nTi =\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1j)\n\u03b1i = max\n\u0012\u03a6s (f (p (ti))) \u2212 \u03a6s (f (p (ti+1)))\n\u03a6s (f (p (ti)))\n, 0\n\u0013\n(1)\nwhere \u03d5s(x) = se\u2212sx/ (1 + e\u2212sx)2 is the logistic density\ndistribution. s, f, c are the learnable parameter of NeuS, the\nSDF network, and the color for point i, respectively.\nSecond stage:\nmesh optimization.\nFor the second\nstage, we use differentiable mesh representation. Fast and\nmemory-efficient mesh rasterization is leveraged, allowing\nthe system be trained with high-resolution renderings (e.g.\n512 \u00d7 512). The same SDS loss as the first stage is used\nas guidance. The deformation network initialized with ze-\nros is included in the optimization, i.e. the vertices of the\ninitial mesh grid without offsets are used as query points.\nSince SDF and color networks are warmed up during the\nfirst stage, the main goal of the second stage is improve\nthe quality of the geometry and texture through the high-\nresolution renders in mesh representation.\n4. Experiment\nWe conduct comprehensive experiments on various\nbenchmarks to show the effectiveness of AToM. We show-\ncase our strong generalizability to unseen prompts, while\nthe per-prompt solutions [8, 25, 38, 47] cannot.\nWe also\ndemonstrate quantitatively and qualitatively that AToM out-\nperforms ATT3D, the state-of-the-art amortized text-to-3D.\n4\n(a) AToM\n(b) AToM Per-Prompt\nFigure 4. Comparing AToM to AToM Per-Prompt on the Pig64 compositional prompt set (\u201ca pig activity theme\u201d), where each row and\ncolumn represent a different activity and theme, respectively. The models are trained using 56 prompts and tested on all 64 prompts, while\n8 unseen prompts are evaluated on the diagonal. As depicted in (a), AToM consistently generates pigs with a similar identity and a uniform\norientation, indicating that AToM also promotes feature sharing across prompts, similar to ATT3D [29]. Also, AToM generates 3D content\nwith consistent quality, while per-prompt optimization cannot as shown in (b). Additionally, per-prompt optimization is more prone to\noverlooking certain details, such as the top hat in row 2 column 4 and the shovel in row 4 column 2 in (b), while AToM preserves them.\nMore importantly, AToM performs well on unseen prompts without further optimization, unlike the per-prompt solution.\nTable 1.\nCompare AToM to the state-of-the-art.\nCLIP R-\nprobability\u2191 is reported. The per-prompt methods in seen prompts\nare deemphasized. Per-prompt solutions have not been experi-\nmented in Animal2400 and DF415 due to their prohibitive compu-\ntation. ATT3D\u2019s results are from the original paper [29]. ATT3D-\nIF\u2020 denotes our reproduced version using Deep Floyd [12] as prior.\nMethod/Dataset\nPig64 unseen\nPig64 seen\nPig64 all\nAnimal2400\nDF27\nDF415\nDreamFusion-IF\n0\n0.7143\n0.6250\n-\n0.8889\n-\nTextMesh-IF\n0\n0.8036\n0.7031\n-\n0.9259\n-\nFantasia3D\n0\n0.5357\n0.4688\n-\n0.7037\n-\nMagic3D-IF\n0\n0.8036\n0.7031\n-\n0.8519\n-\nATT3D\n0.3750\n0.6071\n0.5781\n0.11\n0.6296\n-\nATT3D-IF\u2020\n0.6250\n0.6429\n0.6406\n0.1671\n0.8519\n0.1880\nAToM (Ours)\n0.7500\n0.7500\n0.7500\n0.3442\n0.9259\n0.8193\nWe show the capability of AToM in the large-scale bench-\nmark while per-prompt solutions are prohibitive to train,\nand ATT3D produces indistinguishable results.\nData.\nWe evaluate on two compositional datasets from\nATT3D [29]: Pig64 and Animal2400. Pig64 is structured\naccording to the pattern \u201ca pig {activity} {theme}\u201d from\n8 activities and 8 themes. In total, there are 64 prompts\nin Pig64, where 56 are used as training and all prompts\nincluding 8 unseen ones are used as testing. Animal2400\nis constructed following the template \u201c{animal} {activity}\n{theme} {hat}\u201d.\nThere are 10 animals, 8 activities, 6\nthemes, and 5 hats.\nWe experiment on the hardest spit\n(12.5% split), where only 300 prompts are used in train-\ning, and all 2400 prompts are tested including 2100 unseen\nones. See Appendix for dataset details. We also evaluate on\ntwo datasets demonstrated in DreamFusion [38]: DF27, the\n27 prompts shown in their main paper and DF415, the 415\nprompts available on their project webpage.\nEvaluation Metrics. We employ the same evaluation met-\nric, the CLIP R-probability, as in ATT3D. The CLIP R-\nprobability gauges the average distance of the associated\ntext with 4 uniformly rendered views from the generated\n3D model. This distance score indicates the level of confi-\ndence CLIP holds in regards to the relevance between the\ntext prompt and the mutiple renderings from each 3D asset.\nImplementation Details. We implement our AToM, reim-\nplement ATT3D, and run per-prompt baselines [8,25,38,47]\nusing the ThreeStudio [16] library. For all methods except\nFantasia3D [8] that requires using Latent Diffusion Model\n[41], we utlize Deep Floyd [12] as the text-to-image prior,\nas it is found to offer higher quality across methods [16].\nFor text embedding, we use the same frozen T5 text encoder\nfor prompt processing and text-to-triplane input. During the\nfirst stage, we render 64 \u00d7 64 resolution images with 64\nuniformly sampled points per ray. One can use 32 points\n5\nwithout significant difference. We optimize networks with\nlearning rate 4e-4 and batch size 16 using 2 GPUs for 20K\niterations on DF27, 4 GPUs for 10K iterations on Pig64, 8\nGPUs for 100K iterations on DF415. For the second stage,\nwe optimize with learning rate 2e-4 and batch size 16 using\n4 GPUs for 10K iterations on DF27 and Pig64, and 8 GPUs\nfor 50K iterations on DF415. See Appendix for details.\n4.1. Unseen Interpolation Experiments\nAs a significant benefit, AToM generalizes to interpo-\nlated prompts that are unseen during training. This gen-\neralizability is not possessed by the per-prompt solutions.\nFig.\n4 showcases the differences of AToM compared to\nAToM per-prompt in the Pig64 compositional dataset. We\nhighlight that AToM per-prompt shares the same architec-\nture but is trained in a per-prompt fashion. We observe\nthe following: (1) AToM can produce high-quality results\nof unseen prompts without further optimization, while per-\npromt optimziation cannot, as shown in the diagonal in Fig.\n4; (2) AToM generates pigs with a similar identity and a uni-\nform orientation, which is not observed in per-prompt ex-\nperiments, indicating that AToM promotes feature sharing\nacross prompts; (3) Per-prompt optimization is more prone\nto overlooking certain details, such as the top hat in row 2\ncolumn 4 and the shovel in row 4 column 2, due to the ne-\ncessity for per-prompt parameter tuning, while AToM yilds\na consistent quality across prompts. In Appendix, we fur-\nther illustrates the training dynamics of AToM compared to\nAToM per-prompt, AToM significantly outperforms its per-\nprompt version under the same training budgets. Trained\nboth to convergence, we observe a reduction of training it-\nerations by over 20 times of AToM vs. AToM per-prompt.\nAppendix also qualitatively compare AToM to ATT3D in\nPig64 and the harder dataset Animal2400, where we again\nshow the obvious improvements of AToM against ATT3D.\nRefer to Appendix for details.\n4.2. Compare to the State-of-the-Art\nTab. 1 presents our quantitative results in terms of CLIP\nR-probability on Pig64, DF27, and DF415 benchmarks,\ncompared to the amortized text-to-NeRF method ATT3D,\nand per-prompt approaches [8,25,38,47]. In addition to re-\nporting the official results, We also reproduce ATT3D using\nthe same diffusion prior [12] as AToM, denoted ATT3D-\nIF\u2020 for a fair comparison. From the experiments, one can\nobserve the following: (1) AToM achieves a higher CLIP\nR-probability of 75.00% than ATT3D (64.29%) on Pig64\u2019s\nunseen prompts, indicating its stronger capability to gen-\neralize to unseen prompts. (2) Across the training (seen)\nprompts in Pig64 and DF27, AToM surpasses DreamFu-\nsion [38] and Fantasia3D [8] on both datasets. In compar-\nison to TextMesh [47] and Magic3D [25], AToM slightly\nlags in CLIP R-probability in Pig64 seen prompts; how-\never, visually, AToM exhibits more consistent results as\nshown in Fig.\n4.\n(3) Across all benchmarks shown in\nTab. 1, AToM showcases superior performance compared\nto ATT3D, highlighting AToM\u2019s effectiveness over ATT3D.\nSpecifically, in DF415, AToM attains 81.93% accuracy,\nmuch higher than ATT3D (18.80%).\nFig. 6 show the qualitative comparison between AToM\nand ATT3D in the large-scale bechmark DF415. ATT3D\nmostly outputs indistinguishable 3D assets across various\nprompts.\nConversely, AToM excels in managing large-\nscale benchmarks, handling complex prompts, and achiev-\ning consistently higher accuracy and higher quality than\nATT3D. For qualitative comparisons against ATT3D and\nper-prompt solutions in Pig64 and DF27, see Appendix. We\nobserve AToM can achieve a comparable performance to\nthe state-of-the-art with consistent quality across prompts\nunlike per-prompt solutions.\n5. Analysis and Ablation Study\nWe perform ablation studies in DF415 in Fig. 7. We\ninvestigate: (1) the effect of two-stage training by com-\nparing AToM with an end-to-end single-stage training ver-\nsion, (2) the effects of the second stage by comparing to the\nfirst-stage output, (3) the effect of triplane by comparing to\nAToM with HyperNet-InstantNGP (Hyper-INGP for short)\nused in ATT3D as a replacement for positional encoding,\n(4) the effect of architecture designs including ConvNeXt,\n3D-aware convolution, and cross attention.\nTwo-stage training significantly improves convergence.\nTraining AToM end-to-end using a single stage, i.e. training\nsolely by the second stage from scratch, leads to poor opti-\nmization. In DF415, we observe that the training diverges\nat a very early stage (in iteration 2000), generating black\nimages without objects after that. We provides the results\nwithout two-stage training at iteration 2000 before the di-\nvergence in Fig. 7 column 1. AToM single stage results\nin the lowest accuracy (7.47%), significantly worse than\nAToM full (81.93%). Fig. 7 demonstrates the coarse and\nnoisy visual results due to its divergent training. We also\nablate single-stage training in a smaller dataset DF27 pro-\nvided in Appendix, where the model trained through single\nstage can converge but still produces lower-quality results\nthan the baseline for many prompts. Overall, these experi-\nments clearly show the importance of two-stage training.\nSecond-stage training yields meshes with improved vi-\nsual quality. Fig. 7 column 2&3 shows the usage of our\nsecond stage can slightly improves the accuracy. This is ex-\npected because the CLIP R-probability is used to estimate\nthe correspondence between the generated 3D shape and the\ntext prompt, for which, the first stage output is good enough.\nThe higher-quality renderings from the second stage only\nhave neglectable effect on the CLIP R-probability metric.\nVisually speaking, the second stage of AToM increases the\n6\nFigure 5. Gallery of AToM evaluated in DF415. Here \u02c6 and $ denote \u201ca zoomed out DSLR photo of\u201d and \u201ca DSLR photo of\u201d, respectively.\n7\nATT3D-IF\u2020\nAToM (Ours)\nATT3D-IF\u2020\nAToM (Ours)\nFigure 6. Compare AToM to ATT3D-IF\u2020 evaluated in DF415. In each row, we mostly show results from two similar prompts. While\nATT3D producing indistinguishable results for similar prompts, AToM handles the complexity of prompts and achieves significantly higher\nquality than ATT3D. \u02c6 in the text denotes \u201ca zoomed out DSLR photo of \u201d. One can also observe clear improvements of AToM over the\noriginal ATT3D by cross-referencing with their paper.\nrendering resolution, reduces noise, and enhances sharp-\nness.\nTriplane vs. HyperNetworks. We use text-to-triplane as\na text-conditioned positional encoding instead of Hyper-\nINGP used in ATT3D. Hyper-INGP is a HyperNetworks-\nbased [18] positional encoding, which utilizes two linear\nlayers to predict the weights for the Instant-NGP positional\nencoding network [34]. Hyper-INGP is not numerically sta-\nble and therefore requires special layers such as spectral\nnormalization [33] to stabilize optimization [29]. Due to the\ndifficulty of learning the weights of another neural network,\nHyper-INGP shows low model capacity in the large-scale\ndataset, delivering a poor accuracy (15.18%) in DF415 as\nindicated in Tab. 1. Its low capacity is also verified in our\nablation study where we replace our text-to-triplane with\nHyper-INGP in Fig. 7 column 4: the accuracy of AToM first\nstage drops from 81.69% to only 35.18%. Visually speak-\ning, less distinguishable results with Hyper-INGP are pro-\nduced compared to our text-to-triplane network. To verify\nthis performance drop is not due to the reduced complexity\nof the network, we also removed all ConvNeXt blocks in\ntext-to-triplane and used two linear layers with spectral nor-\nmalization to predict the features, which still significantly\noutperforms the Hyper-INGP counterpart, as indicated in\ncolumn 5 vs. column 4 (77.11% vs. 35.18%). We high-\nlight that only difference between columns 4&5 is the po-\nsitional encoding (Instant-NGP or triplane). These experi-\nments clearly indicate the strength of our proposed text-to-\ntriplane positional encoding.\nTriplane ConvNeXt designs. In Fig. 7 column 5 (w/o\nConvNeXt), we experiment AToM without Triplane Con-\nvNeXt. We observe an accuracy drop of 4.8 points. The\nproposed Triplane ConvNeXt blocks are helpful in enhanc-\ning details, reducing noises, and more importantly, preserv-\n8\n\u201c $ a \nhumanoid \nrobot \nusing a \nlaptop\u201d\n\u201c $ a \ngroup of \ndogs \nplaying \npoker\u201d\n\u201c A \nhumanoid \nrobot \nplaying \nthe violin\u201d\nsingle stage\nAToM (full)\nAToM first stage\ntriplane\nw/o ConvNeXt\nHyper-INGP\n0.0747\n0.8193\n0.8169\n0.3518\n0.7711\nAvg R-Prob\nFigure 7. Ablation study. We compare AToM full pipeline in col-\numn 2 against the end-to-end approach without two-stage train-\ning in column 1, the first-stage output without second-stage refine-\nment in column 3, AToM first stage without triplane but employing\nHyper-INGP used in ATT3D [29] in column 4, AToM first stage\nwithout ConvNeXt blocks but using two linear layers with spectral\nnormalization for text-to-triplane in column 5. Quantitative results\nin average R-Probability evaluated in the entire DF415 dataset are\nprovided at the bottom. $ in the text denotes \u201ca DSLR photo of \u201d.\ning components of complex prompts. We also tried to re-\nplace ConvNeXt blocks into Transformer blocks but found\nTransformer did not converge. We hypothesize that Trans-\nformer requires a significantly larger amount of data. We\nalso perform ablation studies on the components of Tri-\nplane ConvNeXt blocks to investigate the effectiveness of\n3D-aware convolution and cross attention, and reach lower\nCLIP R-Prob 79.76% and 79.28%, respectively. These in-\ndicate that both 3D-aware convolution and cross attention\nimprove the performance.\n6. Conclusion\nThis work proposes AToM, the first amortized text-to-\nmesh framework.\nAToM introduces a 3D-aware text-to-\ntriplane network, which leads to superior quality compared\nto the HyperNetworks counterpart used in ATT3D. AToM\nalso adopts a two-stage amortized optimization to stabi-\nlize the text-to-mesh generation.\nAToM significantly re-\nduces training time compared to per-prompt solutions due\nto geometry sharing of amortized optimization. More im-\nportantly, AToM demonstrates strong generalizability, pro-\nducing high-quality 3D content for unseen prompts without\nfurther optimization. Compared to ATT3D, AToM achieves\nan accuracy more than 4 times higher in DF415. Qualita-\ntively, AToM outperforms ATT3D by providing distinguish-\nable 3D assets with better geometry and texture. We believe\nAToM, along with the code, the pretrained models, and the\ngenerated 3D assets that will be made publicly available,\nwill contribute to push the boundaries of text-to-mesh gen-\neration.\n9\nAToM: Amortized Text-to-Mesh using 2D Diffusion\n\u2014 Appendix \u2014\nA. Implementation Details\nA.1. AToM implementation details\nAToM uses a similar camera and rendering setup to\nTextMesh [47] in the first stage and similar to Magic3D [25]\nin the second stage. We bound the objects in 2 meters and\nset the camere 3 meters away from the object center. We\nemploy a field of view ranging from 40 to 70 in the first\nstage and from 30 to 40 in the second stage. Soft rendering\nwith 50%/50% probabilities for textureless/diffuse shading\nis used to avoid learning flat geometry. We implement SDF,\ndeformation, and color networks using three separate three-\nlayer MLPs with hidden channels 64. We empirically find\nthat these separate networks slightly improve the genera-\ntion quality than the single model used in ATT3D. Origi-\nnal texts without direction are used as input for the text-\nto-mesh network, while directional prompts with \u201c, front\nview\u201d, \u201c, side view\u201d, \u201c, back view\u201d, \u201c, overhead view\u201d are\nused as text condition in the diffusion model. We utilize\nDeep Floyd [12] as the text-to-image prior with guidance\nscale 20. A random noise from (0.2, 0.98) and (0.02, 0.5)\nA.2. ATT3D reimplementationdetails\nIn the main paper, we report the quantitative results\nof the original ATT3D and our re-implemented version\nATT3D-IF\u2020.\nOriginal ATT3D is not yet released. We retrieve the quanti-\ntative results from Figure 6 in their original paper. We com-\npare with the original ATT3D in Table 1 in our manuscript.\nATT3D-IF\u2020 denotes our reimplementation using the exact\nsame architecture, training parameters, and camera setups\nas mentioned in the original paper, except for the unava-\niable ones (where we use the same as AToM). The only\ndifference of ATT3D-IF\u2020 from the original ATT3D is the\ndiffusion prior: while the original ATT3D used their inter-\nnal version, we adopt the publicly available IF model from\nDeep Floyd [12]. We cannot achieve exact the same per-\nformance as ATT3D mostly due to the different diffusion\nprior.\nB. Method Details\nMethod Comparison. AToM is trained on many prompts\nsimultaneously through SDS loss without any 3D data and\ngenerates textured meshes in less than 1 second during\ninference.\nThis differs AToM from previous 3D recon-\nstruction models such as GET3D [14], which requires 3D\nGround Truth and is thus limited by the availability of\n3D data and the low diversity of 3D shapes. Compared\nLinear \nText \nEmbeddings\navg \nreshape\nCross \nAttention\nGELU\nDWConv\n3D-aware \nConvolution\nText-conditioned 3D-aware ConvNeXt Block\nTriplane\nN\nLinear\nLinear\nGELU\nFigure I. Text-to-Triplane architecture. Triplane is generated\nfrom the averaged text embedding followed by a linear projec-\ntion and then refined by multiple text-conditioned 3D-aware Con-\nvNeXt blocks.\nto Magic3D [25], the well-known per-prompt text-to-mesh\nwork, we are similar in two-stage training, but differ from\neach other. Magic3D uses the results of the first stage to\ninitialize the SDF parameters of the second stage through\nan optimization progress [25], leading to inaccurate initial-\nization and cannot be trained amortized. Conversely, the\nAToM network remains unchanged across both stages. The\nfirst stage training in AToM serves as a warmup phase for\nthe second stage. This approach uses the same SDF and\ncolor networks in both stages and eliminates the need for\noptimization of the SDF parameters, unlike Magic3D. Last\nbut not least, AToM differs from ATT3D [29] in two impor-\ntant aspects: (1) AToM is the first to enable amortized train-\ning for text-to-mesh generation, while ATT3D only sup-\nports text-to-NeRF; (2) AToM uses triplane to condition the\ngeneration of 3D content, which is more robust to training\nparameters and is more stable in optimization compared to\nthe HyperNet-based solutions in ATT3D.\nTriplane ConvNeXt. We provides a pseudo code for the\nproposed Triplane ConvNeXt in Algorithm 1. We illustrate\nTriplane ConvNeXt in Fig. I.\nC. Additional Results\nC.1. Pig64\nDataset details. Pig64 is structured according to the pattern\n\u201ca pig {activity} {theme}\u201d from 8 activities and 8 themes.\nactivities = [\u201criding a motorcycle\u201d, \u201csitting on a chair\u201d,\n\u201cplaying the guitar\u201d, \u201cholding a shovel\u201d, \u201cholding a blue\nballoon\u201d, \u201cholding a book\u201d, \u201cwielding a katana\u201d, \u201criding a\nbike\u201d].\nthemes = [\u201cmade out of gold\u201d, \u201ccarved out of wood\u201d,\n\u201cwearing a leather jacket\u201d, \u201cwearing a tophat\u201d, \u201cwearing a\ncape\u201d, \u201cwearing a party hat\u201d, \u201cwearing a sombrero\u201d, \u201cwear-\ning medieval armor\u201d]\nTwo stages of AToM on Pig64. Fig. II shows the compar-\n10\n(a) AToM Stage 1\n(b) AToM Stage 2\nFigure II. Results of AToM first stage (left) and second stage (right) on the Pig64 compositional prompt set. The mesh refinement stage\n(second stage) turns the NeuS representation to a high-resolution mesh representation and sharply increases visual quality.\nFigure III. Training dynamics comparisons between AToM and\nAToM Per-prompt.\nAmortized training significantly reduces\ntraining cost per-prompt.\nisons of AToM first-stage outputs and AToM second-stage\noutputs. The mesh refinement stage (second stage) turns the\nNeuS representation to a high-resolution mesh representa-\ntion and sharply increases visual quality.\nTraining dynamics. Fig. III shows the training dynam-\nics of AToM compared to AToM Per-prompt (per-prompt\noptimized versions of AToM network). Amortized training\nsignificantly reduces training cost per prompt. While per-\nAlgorithm 1 Code for Triplane ConvNeXt (PyTorch [36] like)\nimport torch.nn.functional as F\ndef forward(text_emb):\n# map averaged text_emb to triplane\navg_emb = text_emb.mean(dim=1,keepdims=False)\nx = self.linear(avg_emb)\n# reshape to triplane\nx = x.reshape(-1, self.c, self.h, self.w)\n# Triplane ConvNeXt blocks\nfor i in range(self.num_blocks):\ninp = x\n# cross attention\nx = x + self.crossatt(x,text_emb)\n# 3D aware convolution\nx = x + F.relu(self.aware3dconv(x))\n# FeedForard network\nx = x + self.ffn(x)\n# residual connection\nx = inp + x\nprompt optimization typically requires 2000 - 8000 train-\ning batches, amortized optimization with AToM reaches the\nsame-level accuracy with only 142 training batches per-\nprompt. In other words, AToM reduces the training time\nin this compositional dataset by more than 10\u00d7.\nATT3D reimplementation.\nWe compare our reimple-\nmented ATT3D-IF\u2020 to the original ATT3D in Fig. IV.\n11\n(a) Original ATT3D\n(b) ATT3D-IF\u2020\nFigure IV. Compare our reproduced ATT3D-IF\u2020 at right to original ATT3D [29] at left. Due to the distinct diffusion employed in\nATT3D-IF\u2020, the disparate outcomes from original ATT3D are expected. As strength, our reimplementation using Deep Floyd guidance\nfacilitates more geometry sharing, yields results with less variances, and reduces noises. Especially, we highlight the unseen examples in\nthe diagonal, where ATT3D-IF\u2020 shows better generalizability then origianl ATT3D. As drawbacks, our reimplementation handles prompts\nsometimes worse than the original version, e.g. not all pigs are made out of wood in the second column. Desipte the different implementa-\ntion, AToM outperforms both versions, see Fig. II for qualitative improvement and Tab. 1 in main paper for quantitative improvements.\nC.2. Animal2400\nDataset details. We also include comparisons of AToM\nto ATT3D on Animal2400 12.5% split, where only 300\nprompts are used in training and all 2400 prompts are used\nin testing. Animal2400 is constructed following the tem-\nplate \u201c{animal} {activity} {theme} {hat}\u201d. There are 10\nanimals, 8 activities, 6 themes, and 5 hats.\nanimals = [\u201ca squirrel\u201d, \u201ca raccoon\u201d, \u201ca pig\u201d, \u201ca mon-\nkey\u201d, \u201ca robot\u201d, \u201ca lion\u201d, \u201ca rabbit\u201d, \u201ca tiger\u201d, \u201can\norangutan\u201d, \u201ca bear\u201d]\nactivities = [\u201criding a motorcycle\u201d, \u201csitting on a chair\u201d,\n\u201cplaying the guitar\u201d, \u201cholding a shovel\u201d, \u201cholding a blue\nballoon\u201d, \u201cholding a book\u201d, \u201cwielding a katana\u201d, \u201criding a\nbike\u201d]\nthemes = [\u201cwearing a leather jacket\u201d, \u201cwearing a\nsweater\u201d, \u201cwearing a cape\u201d, \u201cwearing medieval armor\u201d,\n\u201cwearing a backpack\u201d, \u201cwearing a suit\u201d]\nhats = [\u201cwearing a party hat\u201d, \u201cwearing a sombrero\u201d,\n\u201cwearing a helmet\u201d, \u201cwearing a tophat\u201d, \u201cwearing a base-\nball cap\u201d]\nResults.\nAToM significantly outperforms ATT3D-IF\u2020 as\nshown in Fig. V. Quantitatively, AToM achieves 0.3422\nCLIP R-Probability, higher than the original ATT3D (0.11)\nand ATT3D-IF\u2020 (0.1671). AToM trained in this 12.5% split\nseems even outperforms the original ATT3D trained in 50%\nsplit by cross referencing Fig. 8 in ATT3D [29].\nC.3. DF27\nWe compare AToM with per-prompt solutions and\nATT3D-IF\u2020 in Fig. VII. Note that we do not expect the per-\nformance of amortized training to be better than per-prompt\ntraining for the seen prompts.\nD. Additional Ablation Study\nSingle-stage training in DF27. In manuscript, we show\nthat single-stage training in DF400 leads to divergent op-\ntimzation. Here, we further demonstrate that single-stage\ntraining can converge in smaller dataset (DF27), but still\nsuffers from poor geometry. See examples in Fig. VI.\nE. Limitation and Future Work\nFirst, the quality of AToM is bounded by the diffusion\nprior employed.\nThroughout the work, we utilized IF-\nstage1 [12] as the diffusion prior, which limits the high-\nfrequency details due to its low-resolution input. The use\nof a higher-resolution diffusion like Stable Diffusion [41]\n12\n(a) ATT3D-IF\u2020. CLIP R-Prob in 2400 prompts: 0.1671.\n(b) AToM. CLIP R-Prob in 2400 prompts: 0.3422.\nFigure V. Compare AToM to ATT3D-IF\u2020 on Animal2400 12.5% split. Trained only in 300 prompts, AToM also generalizes to all 2400\nprompts, and significantly outperforms ATT3D and ATT3D-IF\u2020. See the overall improved quality and how AToM perserves the prompts\nwhen ATT3D-IF\u2020 overlooks (e.g., the backpacks in the second column).\n13\nsingle stage\nAToM\nsingle stage\nAToM\nFigure VI. Compare Single-Stage AToM with AToM Full.\nSingle-stage training can converge in smaller dataset (DF27), but\nstill suffers from poor geometry, compared to the two-stage train-\ning of AToM.\n(SD) and IF-stage2 might improve the quality of AToM fur-\nther. We made initial attempts to use SD, SD\u2019s variants in-\ncluding VSD [53] and MVDream [43], and IF-stage2 but\nresulted in worse performance. We hypothesize the lower\nquality stems from the difficulties of their optimization in\nthe amortized settings. We believe how to correctly utilize\nstronger diffusion for amortized text-to-mesh is promising.\nSecond, AToM used DMTet with SDF as the mesh represen-\ntation, which is not capable of modeling surfaces of nonzero\ngenus. More advanced mesh representation can be adopted\nto address this limitation, which is orthogonal to our study.\nThird, Janus problem also exists in some results of AToM,\ndespite the fact that AToM alleviate it a lot mostly by geom-\netry sharing in amortized optimization. We also tried pro-\ngressive views, debiasing scores [19], Perp-Neg [2], but em-\npirically found that they did not work trivially in the amor-\ntized setting. We leave these limitations and their potential\nsolutions as future work.\n14\nDreamFusion-\nIF [16,38]\nTextMesh-\nIF [16,47]\nMagic3D-\nIF [16,25]\nFantasia3D\n[8,16]\nAToM\nPer-prompt\n(Ours)\nATT3D-\nIF\u2020 [29]\nAToM\n(Ours)\nFigure VII. Visual comparisons of AToM against the state-of-the-art per-prompt solutions (first four columns), AToM Per-prompt, and\nour reproduced ATT3D in DF27 dataset. AToM achieves higher quality than ATT3D and a performance comparable to the per-prompt\nsolutions.\n15\nFigure VIII. More results of AToM evaluated in DF415.\n16\nReferences\n[1] Brandon Amos et al. Tutorial on amortized optimization.\nFoundations and Trends\u00ae in Machine Learning, 16(5):592\u2013\n732, 2023. 3\n[2] Mohammadreza\nArmandpour,\nHuangjie\nZheng,\nAli\nSadeghian, Amir Sadeghian, and Mingyuan Zhou.\nRe-\nimagine the negative prompt algorithm:\nTransform 2d\ndiffusion into 3d, alleviate janus problem and beyond. arXiv\npreprint arXiv:2304.04968, 2023. 14\n[3] Sudarshan Babu, Richard Liu, Avery Zhou, Michael Maire,\nGreg Shakhnarovich, and Rana Hanocka. Hyperfields: To-\nwards zero-shot generation of nerfs from text. arXiv preprint\narXiv:2310.17075, 2023. 3\n[4] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 16123\u201316133, 2022.\n2, 3\n[5] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-gan: Periodic implicit generative\nadversarial networks for 3d-aware image synthesis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 5799\u20135809, 2021. 2\n[6] Dave Zhenyu Chen, Haoxuan Li, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner. Scenetex: High-quality tex-\nture synthesis for indoor scenes via diffusion priors. arXiv\npreprint arXiv:2311.17261, 2023. 2\n[7] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen\nTu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf:\nA unified approach to 3d generation and reconstruction. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023. 2\n[8] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-\ntasia3d: Disentangling geometry and appearance for high-\nquality text-to-3d content creation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 22246\u201322256, October 2023. 1, 2, 3, 4, 5,\n6, 15\n[9] Wenzheng Chen, Huan Ling, Jun Gao, Edward Smith,\nJaakko Lehtinen, Alec Jacobson, and Sanja Fidler. Learn-\ning to predict 3d objects with an interpolation-based differ-\nentiable renderer. Advances in Neural Information Process-\ning Systems (NeurIPS), 2019. 2\n[10] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexan-\nder G Schwing, and Liang-Yan Gui. Sdfusion: Multimodal\n3d shape completion, reconstruction, and generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[11] Zezhou Cheng, Menglei Chai, Jian Ren, Hsin-Ying Lee,\nKyle Olszewski, Zeng Huang, Subhransu Maji, and Sergey\nTulyakov. Cross-modal 3d shape generation and manipula-\ntion. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), 2022. 2\n[12] DeepFloyd, StabilityAI. Deepfloyd if, 2023. GitHub reposi-\ntory. 3, 5, 6, 10, 12\n[13] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong\nNgo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris-\ntian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli\nVanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia\nGkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi.\nObjaverse-xl: A universe of 10m+ 3d objects. arXiv preprint\narXiv:2307.05663, 2023. 2\n[14] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances in Neural In-\nformation Processing Systems (NeurIPS), 35:31841\u201331854,\n2022. 1, 2, 4, 10\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances in\nneural information processing systems (NIPS), 2014. 2\n[16] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 5, 15\n[17] Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Bar-\nlas O\u02d8guz. 3dgen: Triplane latent diffusion for textured mesh\ngeneration. arXiv preprint arXiv:2303.05371, 2023. 2\n[18] David Ha, Andrew M. Dai, and Quoc V. Le.\nHypernet-\nworks. In International Conference on Learning Represen-\ntations (ICLR). OpenReview.net, 2017. 2, 8\n[19] Susung Hong, Donghoon Ahn, and Seungryong Kim. Debi-\nasing scores and prompts of 2d diffusion for robust text-to-3d\ngeneration. arXiv preprint arXiv:2303.15413, 2023. 14\n[20] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 867\u2013876, 2022. 2\n[21] Heewoo Jun and Alex Nichol.\nShap-e:\nGenerat-\ning conditional 3d implicit functions.\narXiv preprint\narXiv:2305.02463, 2023. 2\n[22] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4401\u20134410,\n2019. 2\n[23] Chun-Liang Li, Manzil Zaheer, Yang Zhang, Barnabas Poc-\nzos, and Ruslan Salakhutdinov.\nPoint cloud gan.\narXiv\npreprint arXiv:1810.05795, 2018. 2\n[24] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun\nLuan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg\nShakhnarovich, and Sai Bi.\nInstant3d:\nFast text-to-3d\nwith sparse-view generation and large reconstruction model.\nCoRR, abs/2311.06214, 2023. 2\n[25] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\n17\nConference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 1, 2, 3, 4, 5, 6, 10, 15\n[26] Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei\nChai, Aliaksandr Siarohin, Ming-Hsuan Yang, and Sergey\nTulyakov.\nInfinicity:\nInfinite-scale city synthesis.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2023. 2\n[27] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 4\n[28] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. In Sem-\ninal graphics: pioneering efforts that shaped the field, pages\n347\u2013353. 1998. 1\n[29] Jonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan\nLin, Towaki Takikawa, Nicholas Sharp, Tsung-Yi Lin, Ming-\nYu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized\ntext-to-3d object synthesis. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), pages\n17946\u201317956, October 2023. 1, 3, 5, 8, 9, 10, 12, 15\n[30] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and\nAndrea Vedaldi.\nRealfusion:\n360{\\deg} reconstruction\nof any object from a single image.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2\n[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision (ECCV), pages 405\u2013421. Springer, 2020. 1\n[32] Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shub-\nham Tulsiani.\nAutosdf:\nShape priors for 3d comple-\ntion, reconstruction and generation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 2\n[33] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and\nYuichi Yoshida. Spectral normalization for generative ad-\nversarial networks. In International Conference on Learning\nRepresentations (ICLR). OpenReview.net, 2018. 2, 8\n[34] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. In ACM Transactions on Graphics\n(SIGGRAPH), 2022. 2, 8\n[35] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela\nMishkin, and Mark Chen. Point-e: A system for generat-\ning 3d point clouds from complex prompts. arXiv preprint\narXiv:2212.08751, 2022. 2\n[36] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zem-\ning Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch:\nAn imperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2019. 11\n[37] Dario Pavllo, Graham Spinks, Thomas Hofmann, Marie-\nFrancine Moens, and Aurelien Lucchi. Convolutional gener-\nation of textured 3d meshes. Advances in Neural Information\nProcessing Systems (NeurIPS), 2020. 2\n[38] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\nInter-\nnational Conference on Learning Representations (ICLR),\n2022. 1, 2, 3, 4, 5, 6, 15\n[39] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 2\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. Journal of Machine Learn-\ning Research, 21(140):1\u201367, 2020. 3\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, 2022. 1, 5, 12\n[42] Tianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and\nSanja Fidler. Deep marching tetrahedra: a hybrid represen-\ntation for high-resolution 3d shape synthesis. In Advances\nin Neural Information Processing Systems (NeurIPS), vol-\nume 34, pages 6087\u20136101, 2021. 2, 4\n[43] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. CoRR, abs/2308.16512, 2023. 14\n[44] Dong Wook Shu, Sung Woo Park, and Junseok Kwon.\n3d point cloud generative adversarial network based on\ntree structured graph convolutions.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision\n(ICCV), 2019. 2\n[45] Edward J Smith and David Meger. Improved adversarial sys-\ntems for 3d object generation and reconstruction. In Confer-\nence on Robot Learning (CoRL), 2017. 2\n[46] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In Proceedings of the In-\nternational Conference on Machine Learning (ICML), pages\n2256\u20132265. PMLR, 2015. 1\n[47] Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni,\nMichael Niemeyer, and Federico Tombari. Textmesh: Gen-\neration of realistic 3d meshes from text prompts. In 2024\nInternational Conference on 3D Vision (3DV), 2024. 1, 2, 3,\n4, 5, 6, 10, 15\n[48] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2017. 3\n[49] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), 2023. 2\n[50] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei\nLiu, and Yu-Gang Jiang. Pixel2mesh: Generating 3d mesh\n18\nmodels from single rgb images. In Proceedings of the Euro-\npean conference on computer vision (ECCV), pages 52\u201367,\n2018. 1, 2\n[51] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural im-\nplicit surfaces by volume rendering for multi-view recon-\nstruction.\nIn Advances in Neural Information Processing\nSystems (NeurIPS), 2021. 4\n[52] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo.\nRodin: A genera-\ntive model for sculpting 3d digital avatars using diffusion.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4563\u20134573,\nJune 2023. 3\n[53] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2, 14\n[54] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2016. 2\n[55] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang,\nSong-Chun Zhu, and Ying Nian Wu. Learning descriptor\nnetworks for 3d shape synthesis and analysis. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2018. 2\n[56] Biao Zhang, Matthias Nie\u00dfner, and Peter Wonka. 3dilg: Ir-\nregular latent grids for 3d generative modeling. Advances in\nNeural Information Processing Systems (NeurIPS), 2022. 2\n[57] Biao Zhang, Jiapeng Tang, Matthias Niessner, and Peter\nWonka. 3dshape2vecset: A 3d shape representation for neu-\nral fields and generative diffusion models. ACM Transactions\non Graphics (SIGGRAPH), 2023. 2\n[58] Song-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu.\nSketch2model: View-aware 3d modeling from single free-\nhand sketches. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n2\n[59] Joseph Zhu and Peiye Zhuang.\nHifa: High-fidelity text-\nto-3d with advanced diffusion guidance.\narXiv preprint\narXiv:2305.18766, 2023. 2\n19\n"
  },
  {
    "title": "Transforming and Combining Rewards for Aligning Large Language Models",
    "link": "https://arxiv.org/pdf/2402.00742.pdf",
    "upvote": "9",
    "text": "Transforming and Combining Rewards\nfor Aligning Large Language Models\nZihao Wang1, Chirag Nagpal2, Jonathan Berant3, Jacob Eisenstein3, Alex\nD\u2019Amour3, Sanmi Koyejo4,3, and Victor Veitch1,3\n1University of Chicago\n2Google Research\n3Google DeepMind\n4Stanford University\nAbstract\nA common approach for aligning language models to human preferences is to first\nlearn a reward model from preference data, and then use this reward model to update\nthe language model. We study two closely related problems that arise in this approach.\nFirst, any monotone transformation of the reward model preserves preference ranking;\nis there a choice that is \u201cbetter\u201d than others? Second, we often wish to align language\nmodels to multiple properties: how should we combine multiple reward models? Using\na probabilistic interpretation of the alignment procedure, we identify a natural choice for\ntransformation for (the common case of) rewards learned from Bradley-Terry preference\nmodels. This derived transformation has two important properties. First, it emphasizes\nimproving poorly-performing outputs, rather than outputs that already score well. This\nmitigates both underfitting (where some prompts are not improved) and reward hacking\n(where the model learns to exploit misspecification of the reward model). Second, it\nenables principled aggregation of rewards by linking summation to logical conjunction:\nthe sum of transformed rewards corresponds to the probability that the output is \u201cgood\u201d\nin all measured properties, in a sense we make precise. Experiments aligning language\nmodels to be both helpful and harmless using RLHF show substantial improvements\nover the baseline (non-transformed) approach.\n1\nIntroduction\nIn this paper, we are interested in how to align large language models in order to bias their\noutputs towards having desired properties\u2014e.g., to be helpful, harmless, factual, or creative\n[Zie+19; Sti+20; Ouy+22]. We study the two-stage approach where we first learn a reward\nmodel from human preferences, and then align the language model so that its outputs have\nhigh values under the reward model. In this context, we\u2019re interested in two fundamental\nproblems:\n1. The alignment step maximizes the expected learned reward model. However, any\nmonotone transformation of the reward preserves the interpretation of the alignment\nprocedure as biasing the language model towards human-preferred outputs. Can we\nimprove the alignment step by transforming the learned reward?\n2. We often wish to align language models to multiple properties\u2014e.g., outputs should\nbe helpful, and harmless, and factual. If we have reward models for each property,\nhow should we combine them?\nA main challenge in answering these questions is that the goal of alignment is not precisely\ndefined. As a result, there is no obvious principle to guide the choice of transformation or\n1\narXiv:2402.00742v1  [cs.CL]  1 Feb 2024\nFigure 1: Transforming the Bradley-Terry reward both mitigates overfitting and makes addition behave\nas logical AND. This leads to significant improvements in aligned model quality relative to standard\npractice. Each point on the plot is a LLM aligned with a different KL penalty weight. The y-axis shows\nimprovement over the base supervise finetuned (SFT) LLM in both helpfulness AND harmlessness, as\njudged by an external evaluator model (not used for RLHF). The baseline aggregates suboptimally\n(usually losing on either helpfulness or harmlessness) and suffers reward hacking (performance decays\nin the high KL regime). Details in section 5.\naggregation method. The conceptual idea in this paper is to interpret alignment probabilis-\ntically. From this perspective, the goal of aligning a model to a particular property is to\nproduce samples from the posterior distribution conditional on the outputs being \u201cgood\u201d on\nthat property. Similarly, the goal of aligning to multiple properties is to produce samples\nconditional on the outputs being \u201cgood\u201d on all properties.\nTo make use of this idea, we need to define what it means for an output to be \u201cgood\u201d. In the\ncontext of rewards learned from preference data, we take an output y be \u201cgood\u201d if it has\nreward r(x, y) greater than some prompt-specific reference value rref(x). The first main\nresult of the paper is that in the (typical) case where the reward model is learned from pref-\nerence data using a Bradley-Terry model and the language model is aligned by maximizing\nexpected reward subject to a KL constraint, the natural choice of transformation is:\nu(x, y) = log\u03c3(r(x, y) \u2212 rref(x)),\n(1.1)\nwhere \u03c3(\u00b7) is the sigmoid function. Here, r is the learned Bradley-Terry reward model, and\nu is the transformed reward we use in the alignment step.\nThis transformation is motivated by a probabilistic interpretation. It additionally turns out to\nhave important practical benefits relative to the baseline approach of using the raw reward\nmodel. First, the transformed reward shrinks the marginal utility of very high reward values.\nThis has the effect in alignment of both encouraging the model to improve poorly performing\nprompts, and of discouraging the model from \u201creward hacking\u201d by optimizing the reward\nmodel outside the range of its validity. Second, the transformed reward offers a natural\nway to combine multiple reward models. Namely: the sum of the transformed rewards\ncorresponds to the logical AND of the outputs being \u201cgood\u201d on each property. So, after trans-\nforming the rewards, we can simply sum them to aggregate multiple reward models.\nIn combination, these benefits can lead to substantial improvements in alignment perfor-\nmance. Figure 1 compares aligning a language model to be both helpful and harmless\nusing summation of transformed and untransformed rewards. Varying the strength of KL\n2\nregularization used in the alignment step, we observe that the transformed reward leads to\nsubstantial improvements at all KL levels.\n2\nPreliminaries\nWe first review the standard Reinforcement Learning from Human Feedback (RLHF) two-step\nprocedure for aligning language models to human preferences.\nReward model training from pairwise data\nReward models are trained to emulate\nhuman feedback. A frequently used type of feedback is pairwise preference data, consisting\nof a prompt x and two generated responses y+, y\u2212, where y+ is preferred by the human\nannotator. Our discussion mainly focuses on this case.\nCommonly, rewards are learned using the Bradley-Terry model [BT52],\np(y\u2212 \u227a y+ | x) = \u03c3(r(x, y+) \u2212 r(x, y\u2212)).\n(2.1)\nThe function r is parameterized by a neural network (typically, another LLM) and fit using\nthe standard maximum log likelihood objective.\nAlignment to reward model\nThe next step is updating the base LLM to bias it towards\nhigh-reward responses.\nUsually, aligning the model to the reward function is proceeded by a \u201cSupervised Finetuning\u201d\n(SFT) step where the base model is fine-tuned using the language modeling objective\nobjective on the winning examples from the human preference data. We denote this model\nas \u03c00. Our interest is how to use the reward model to further align \u03c00.\nThe aim of the alignment step is to update \u03c00 to a new model \u03c0\u2217 that has high expected\nreward, while still being close to \u03c00 (to preserve information from the previous training\nphases). Standard practice is to learn \u03c0\u2217 by maximizing the expected reward of samples,\nregularized by a penalty on the KL-divergence between \u03c0\u2217 and \u03c00.\nThe main idea in this paper is to instead use a utility measure u(x, y) that is a monotone\ntransformation of r(x, y). We leave the alignment procedure otherwise unchanged. Then,\nmathematically, \u03c0\u2217 is the maximizer of:\nEx{Ey\u223c\u03c0(\u00b7|x)[u(x, y)] \u2212 \u03b3KL(\u03c0(\u00b7|x)\u2225\u03c00(\u00b7|x))}\n(2.2)\nHere, \u03b3 is a hyper-parameter that controls the trade-off between maximizing rewards and\naligning with \u03c00.\n3\nReward Transformation\nWe now turn to deriving the reward transformation.\nFormalize Alignment Goal\nThe first step is to formalize the goal of alignment. This is\nnecessary to identify a \u201ccorrect\u201d transformation. Intuitively we want to modify the initial\npolicy \u03c00(y|x) so that the generated samples are considered \u201cgood\u201d in some property by\nhumans. To make progress, we introduce a binary semantic random variable G indicating\nwhether response y is \u201cgood\u201d for prompt x. Then we define the alignment goal as producing\na model that samples from the distribution of responses conditional on the response being\ngood; i.e., \u03c0target(\u00b7 | x) = p(\u00b7 | x, G = 1).\nIn fact, we slightly generalize this to allow finer grained control of the reward vs KL tradeoff.\nBy Bayes\u2019 rule, we may rewrite p(y|x, G = 1) \u221d \u03c00(y|x)p(G = 1|x, y). That is, we\n3\n(a) Transformation Shape\n(b) Helpfulness Examples\nFigure 2: Bradley-Terry rewards do not capture diminishing utility, and log-sigmoid transforming can\nfix this. In the example responses, moving from response 1 to response 2 substantially increases utility,\nbut from response 2 to response 3 only marginally increases. However, the BT rewards treat each\nimprovement the same. A log-sigmoid transformation reflects diminishing returns.\nreweight the base LLM by a term that upweights responses that are likely to be deemed\ngood. It is natural to introduce a hyperparameter to control the strength of this upweighting.\nAnticipating the connection to (2.2), we again use \u03b3 for this hyperparameter. Then, we\ndefine our alignment goal as producing an aligned model\n\u03c0target\n\u03b3\n(y|x) \u221d \u03c00(y|x)p(G = 1|x, y)1/\u03b3\n(3.1)\nReward Transformation\nThe next question is how to produce an aligned model that\nsatisfies our goal. This has two parts: we must use the reward model to define the binary\ngoodness variable G, and we must determine how the utility function used for alignment\nrelates to G.\nTarget Utility Function\nWe begin by connecting alignment utility and G. The idea is to\nuse the well-known result that that the ideal optimizer of the KL-regularized RLHF objective\n(2.2) is an exponential tilting of the base policy [e.g., KPB22]:\n\u03c0\u2217(y | x) \u221d \u03c00(y | x)exp(u(x, y)/\u03b3)\n(3.2)\nComparing (3.1) with (3.2), we see that in order to get the target policy through alignment,\nwe must set the utility function to be the log-probability of goodness:\nu(x, y) = log p(G = 1|x, y).\n(3.3)\nPointwise Reward Models\nThe next step is to relate the Bradley-Terry model to p(G =\n1|x, y). As a warmup, we consider the case of reward models trained on pointwise data;\ni.e., where each example is a prompt x, response y, and a binary label G indicating whether\nthe response is good. In this case, we would train the reward function by minimizing a\nbinary cross-entropy objective. This is a proper scoring rule, so the learned reward would\nbe: r(x, y) = logit p(G = 1|x, y). Here, we take the reward to be on the logit scale so that\nr(x, y) \u2208 (\u2212\u221e,\u221e), analogous to Bradley-Terry rewards. In this case, the right utility is\nu(x, y) = log\u03c3(r(x, y)).\nPairwise Reward Models\nThe pairwise case is more subtle. It may be tempting to again\napply the log(\u03c3(\u00b7)) transformation to the Bradley-Terry rewards. This is incorrect for two\n4\nreasons. First, only reward differences are interpretable as logit probilities\u2014the rewards for\nindividual prompts are not. Second, in general the reward model r(x, y) is unidentifiable\nfrom the data. For any r, we can shift it by any arbitrary function of the prompt x without\nchanging the Bradley-Terry model. That is, \u02dcr(x, y) \u2190 r(x, y) + C(x) has the same objective\nin (2.1). However, any non-linear transformation of the reward will be sensitive to this\nunidentified C(x).\nHappily, both problems are resolved by choosing a suitable definition of what it means for a\nresponse to be good. Here, we take a generated response y to be \u201cgood\u201d if it is preferred\nover a chosen reference output yref. For example, we may say that y is harmless as long as\nit is preferred by the harmlessness reward to a canned response such as \u201cI am unable to\nanswer that question\".\nThe transformation follows immediately:\nTheorem 1. Suppose output y is deemed good for prompt x if it would be preferred\nto reference output yref(x). Then, if the Bradley-Terry model (2.1) holds, and we align\nusing KL-regularized utility maximization (2.2), then using utility\nu(x, y) = log\u03c3(r(x, y) \u2212 rref(x))\n(3.4)\nwill satisfy the alignment goal (3.1). Here rref(x) := r(x, yref(x)).\nThat is, once we decide on a reference response, we get the right utility function by applying\nlog-sigmoid transformation to the centered reward.\nMechanistic Interpretation\nWe derived the reward transformation from a probabilistic\nargument. It is also insightful to consider the mechanistic effect of the transformation.\nOne fundamental issue with the baseline approach, where u(x, y) = r(x, y), is that the\nutility gain for improvements never diminishes. In the aligned model, taking \u03b3 = 1, we\nhave that the relative probabilities of two responses is exponential in the difference of\nutilities.\n\u03c0\u2217(y1 | x)\n\u03c0\u2217(y0 | x) = exp(u(y1, x) \u2212 u(y0, x))\u03c00(y1 | x)\n\u03c00(y0 | x)\n(3.5)\nNow, consider the case where we have three candidate responses: yref, y0, y1 such that\np(yref \u227a y0) = 0.99 and p(yref \u227a y0) = 0.999. If we use the raw Bradley-Terry logits as our\nutility, then using that r(y1, x) \u2212 r(y0, x) = (r(y1, x) \u2212 r(yref, x)) \u2212 (r(y0, x) \u2212 r(yref, x))\nwe have:\n\u03c0\u2217(y1 | x)\n\u03c0\u2217(y0 | x) = exp(logit(0.999) \u2212 logit(0.99)))\u03c00(y1 | x)\n\u03c00(y0 | x)\n\u2248 10 \u00d7 \u03c00(y1 | x)\n\u03c00(y0 | x)\nThat is, when aligning to raw rewards, going from a very good response to a marginally\nbetter response increases probability by a factor of 10! However, if yref is already good, a\nhuman would find little difference between the two responses.1 Conversely, aligning to the\ntransformed reward model only increases the probability by a factor of 1.01.\nThis effect seems particularly salient when we consider that the reward model is itself\nlearned. It seems unlikely that the model can actually reliably distinguish between y1 and\ny0. Accordingly, when we align to the raw learned model, we expect to induce enormous\nshifts according to preferences that are anyways noisy.\n1Ultimately, the issue here is that Bradley-Terry rewards don\u2019t automatically correspond to utilities.\n5\nChoosing reference response\nThe reference reward acts as a hyperparameter of the\ntransformation. Essentially, the transformation results in a utility that is linear below the\nreference value, and rapidly diminishing above it. Accordingly, we should set the reference\nto a value that represents a good response, that we believe is achievable by the model, and\nwhere we believe our learned reward function makes meaningful predictions. We found\nthat a good default choice is the 85th quantile of examples from the base distribution. We\nconsider additional examples in section 5.\n4\nReward Aggregation\nWe now consider the case when we have multiple reward models r1,..., rn corresponding to\ndifferent properties.\nAlignment Goal\nAgain, the first step in deriving an optimal aggregation scheme is to\nformalize a goal. We make the following natural choice: the aligned model should be \u201cgood\u201d\non all target properties. E.g., we want to align our model to be helpful AND harmless.\nTo formalize this idea, let Gi be binary random variable indicating whether y is considered\n\u201cgood\u201d for x in property i. We introduce the binary random variable corresponding to logical\nAND: GAND :=\nVn\ni=1 Gi. Similar to the single reward case, we formalize the goal as the\nposterior distribution conditioned on all properties being \u201cgood\u201d:\n\u03c0target\nAND,\u03b3 \u221d \u03c00(y | x)p(GAND = 1 | x, y)1/\u03b3\n(4.1)\nReward Aggregation\nWith this goal, following Theorem 1, we should align using util-\nity\nu(x, y) = log p(GAND = 1 | x, y).\n(4.2)\nThe question is then how to construct this utility function using the individual reward\nmodels.\nIn full generality, this is an impossible task. The reason is that the individual rewards only\ntell us about the marginal distributions of the properties, but the logical AND may depend\non interactions. Thus, we need an extra assumption:\nAssumption 1 (Independent Judgements). Given a fixed prompt x and response y, whether\ny is judged to be good for x on each property is independent of all the judgements on all\nother properties. That is, (G1,..., Gn) are conditionally independent given (X, Y ).\nFor example, this assumption says we can decide whether a given response is helpful\nindependently of deciding whether it\u2019s harmful. (Note: this is conditional on the prompt and\nresponse. We do not require helpfulness and harmless to be independent marginally.)\nThe reward aggregation forumula follows immediately:\nTheorem 2. Suppose output y is deemed good for prompt x in aspect i if it would be\npreferred to reference output yref\ni (x) in property i. Then, if Assumption 1 holds, the\nBradley-Terry model (2.1) holds for all properties, and we align using KL-regularized\nutility maximization (2.2), then using utility\nn\nX\ni=1\nlog\u03c3(ri(x, y) \u2212 rref\ni (x))\n(4.3)\nwill satisfy the alignment goal (4.1). Here rref\ni (x) := r(x, yref\ni (x)).\n6\n(a) Helpful: RLHF\n(b) Harmless: RLHF\nFigure 3: Transformed reward obtains better KL and win-rate trade-offs in single-reward. We show\ntwo win rates: 1) win rates judged from prompted PALM 2 evaluator, between the aligned policy and\nrandom SFT samples, and 2) win rates judged by T5-XXL evaluator, against the SFT quantiles: 85% for\nhelpfulness and 95% for harmlessness.\nMechanistic Interpretation\nWe derived the aggregation scheme according to a probabilis-\ntic assumption. Similar to the single-reward case, we consider the mechanistic effect.\nThe baseline approach is to aggregate with a (weighted) sum of the raw rewards. The key\nproblem is that this approach allows strong performance in one property to balance out\npoor performance in another.\nConsider the case where we have two properties Aand B we want to align to (e.g., helpfulness\nand harmlessness), and 4 responses yref\nA , yref\nB , y0, y1 such that p(yref\nA \u227a y1) = p(yref\nB \u227a y1) =\n0.9, and p(yref\nA \u227a y0) = 0.45 with p(yref\nB \u227a y0) = 0.99. If we want the aligned distribution\nto generate samples that are \u201cgood\u201d in both aspects, then y1 should be preferred to y0.\nHowever, if we use the sum of the raw Bradley-Terry logits as utility (u = rA + rB), the\nrelative probability ratio under the aligned policy \u03c0\u2217 will be (with \u03b3 = 1)\n\u03c0\u2217(y1 | x)\n\u03c0\u2217(y0 | x) = 1 \u00d7 \u03c00(y1 | x)\n\u03c00(y0 | x).\n(4.4)\nThat is, the model aligned with the raw Bradley-Terry reward does not upweight response\ny1. If we instead align by the sum of the transformed rewards, then we have that the relative\nprobability ratio is approximately 1.8\u2014i.e., the response that does well on both properties\nis preferred.\n7\n5\nExperiments\nWe now turn to assessing the practical effect of using the transformed reward to align LLMs.\nWe experiment with aligning models to be helpful, harmless, and both. We find that the\ntransformation alleviates reward hacking and reward underfitting, and that aligning to\ntransformed sum acts as aligning to logical AND. This leads to substantial improvements in\nLLMs aligned to the transformed reward.\n5.1\nExperimental Setup\nWe follow a standard RLHF pipeline; see appendix A.4 for full details.\nDatasets\nWe use the Anthropic Helpfulness and Harmlessness datasets [Bai+22]. These\nare multi-turn dialogues between a human and a digital assistant. Each dataset consists\nof the beginning of the conversation, two responses for the final turn of the AI side of the\nconversation, and a label for the human preference on the target property. We use the base\ndatasets (44K examples for helpfulness and 42K for harmlessness), where responses are\ngenerated from a 52B context-distilled LM. For both tasks, we split the training set into two:\nhalf for training the reward model, and half for the alignment step.\nReward model training\nWe train a Bradley-Terry reward model for each of helpfulness\nand harmlessness by finetuning a pretrained T5-base (220M parameters) model [Raf+20]\non the Anthropic data.\nSFT\nFor our policy model, we use the instruction-finetuned PALM-2-XXS model [Ani+23].\nFollowing standard practice, we first run supervised finetuning (SFT) of the instruction\ntuned LLM on the \u2018preferred\u2019 responses from the helpfulness dataset. We use this SFT model\nas the pre-alignment base for all experiments.\nRLHF setup\nFor alignment, we follow standard practice and optimize expected utility\nsubject to a KL penalty using Proximal Policy Optimization (PPO) algorithm. For each utility\nfunction and dataset, we sweep over multiple values of the KL regularization strength \u03b3.\nWe run for 20000 steps, which we find suffices for convergence in all cases.\n5.2\nReward Transformation Improves Alignment\nTransforming the reward model should encourage the alignment to focus on improving lower-\nreward responses over those with already high rewards. We expect this to both reduce reward\nhacking, and to reduce the number of low-reward responses (less underfitting).\nChoice of reference reward\nThe reference reward rref(x) should capture the notion of a\nresponse that\u2019s \u201cgood enough\u201d. For harmlessness, this is straightforward: a generic response\nlike \u201cI can\u2019t answer that\u201d achieves the goal. For the experiments, we sampled variations of\n\u201cI can\u2019t answer that\u201d as the reference reward.\nFor helpfulness, such canned responses won\u2019t suffice. Instead, for each prompt we sample\n64 responses from the SFT model. We then use this data to build an estimator of the 85th\nquantile of the sampled rewards for each prompt. We use this estimated 85th quantile as\nthe reference reward. Details provided in appendix A.1.\nTransformation Improves Alignment\nAligning to the transformed reward should reduce\nreward hacking and underfitting relative to aligning to the raw reward. Then, we expect\nthat the transformed alignment leads to larger gains over the SFT model.\n8\n(a) Helpfulness (KL \u2248 18)\n(b) Harmlessness (KL \u2248 7)\nFigure 4: Reward Transformation leads to more uniform reward improvements than baseline. We\ncompare reward distributions in the aligned policies that are matched on KL. Rewards are centered by\nthe SFT median in both helpfulness and harmlessness. Reward distributions are more concentrated\nwhen using transformed rewards than using raw rewards.\nWe have two main strategies for judging improvement relative to the SFT model. First,\nfollowing past work [GSH23; Cos+23; Eis+23], we train a T5-XXL model using the same\npreference dataset and the Bradley-Terry objective. This provides a proxy for true preferences\nthat we do not optimize against (so, it can witness reward hacking). We say a sample from\nthe aligned model wins in helpfulness if it beats the 85th reward quantile of samples from\nthe SFT model. We say it wins in harmlessness if it beats the 95th-quantile. (These numbers\nare chosen to make winning hard enough to show a gap between alignment strategies;\nresults are consistent across other choices of quantile.).\nThe second strategy evaluates wins by zero-shot querying of an instruction-tuned PALM-2\nmedium model. Following previous work [Dub+23; Sin+23; Eis+23; Raf+23], we pass a\nprompt, a response from the SFT model, and a response from the aligned model and ask\nwhich is preferred (in terms of helpfulness or harmlessness). Details in appendix A.2.\nFigure 3 show the win rate of the aligned models over the SFT model for each evaluation\nstrategy. We average over the prompts in the RLHF validation dataset. We see that aligning\nto the transformed reward dominates aligning to the raw reward, under both evaluation\nstrategies and at all levels of KL distance to the base policy model.\nSee appendix A.2 for additional experiments evaluating alignment-induced improvements.\nUniform Improvement of Rewards\nIt is clear that aligning using the transformed reward\nimproves over aligning using the raw reward. Intuitively, this is because of a reduction\nin both reward hacking and underfitting. To check whether this intuition holds, we plot\nthe distribution of rewards of samples from (approximately) KL-matched raw-aligned and\ntransformed-aligned models in Figure 4. As expected, the reward distribution of samples\nfrom the transformed-aligned model is more concentrated. That is, there are fewer very\nhigh reward samples (less reward hacking) and fewer very low reward samples (less\nunderfitting).\nIn more detail: for each of helpfulness and harmlessness, we choose a raw-aligned and\ntransformed-aligned model with approximately matched KL. We sample responses from\neach model with the same prompts. Then, we compute the (T5-base) reward for each of\nthese responses, centered by median reward of SFT samples (to make rewards comparable\nacross prompts). Figure 4 shows histograms of these sampled rewards. We also compare\nacross multiple KL-values in Figure 10.\nTransformation reduces shortcuts\nOne symptom of reward hacking is that the aligned\nmodel will start exploiting \u201cshortcuts\u201d that are preferred by the reward model (but which do\n9\n(a) Helpfulness\n(b) Harmlessness\nFigure 5: Transformed reward reduces shortcuts in generated responses. For helpfulness, we identify\na shortcut pattern of using lists, similar to [Eis+23]. In harmlessness, one known shortcut pattern\nis recommending the users to seek therapy or consult professional help [Bai+22]. We extract these\nshortcuts with heuristic methods. In the baseline approach, the policy model exploits those shortcuts\nfor higher reward values. This is mitigated when we transform the reward.\nnot correspond to genuine improvements). We consider the effect of reward transformation\non two such shortcuts. For helpfulness, Eisenstein et al. [Eis+23] observe that raw-aligned\nmodels have a tendency to format outputs as lists. For harmlessness, we observe that\nraw-aligned models will often give responses of the form \u201cyou should consult a [doctor/psy-\nchologist/lawyer/etc]\u201d (a similar observation is made by Bai et al. [Bai+22]). In Figure 5,\nwe plot the fraction of responses that contain each shortcut, for each aligned model. We\nsee that the raw-reward aligned model does indeed exploit these shortcuts. Further, this\nbehavior is substantially mitigated by aligning to the transformed reward instead. See\nappendix A.3 for details.\n5.3\nReward Aggregation\n(a) Helpful and Harmless: Best-of-k\n(b) Helpful and Harmless: RLHF\nFigure 6: Summation of transformed reward obtains better trade-off between KL/K and win-rate.\nIn Figure 6a and Figure 6b we show win rates against SFT median rewards in both helpfulness and\nharmlessness, judged by T5-XXL evaluator, for Best-of-k and RLHF respectively.\nWe now turn to the second goal: aggregating rewards for multiple distinct goals. To that\nend, we consider aligning a LLM to be both helpful and harmless.\n5.3.1\nFurther Experimental Setup\nRLHF setup\nWe use reward models trained for the helpfulness and harmlessness tasks\nas discussed above. For RLHF training, we use prompts only from the helpfulness dataset.\nThis decision is because of the observation of the tension between helpfulness and harm-\n10\nlessness, which forced [Bai+22] to use a higher proportion of helpfulness prompts than\nharmlessness ones. We use the same policy model as in experiments for single rewards\n(SFT-ed on helpfulness data). The other training details are the same as in single-reward\nexperiments.\nbest-of-k setup\nIn addition to RLHF experiments, we also experiment with best-of-k\nsampling, as in Gao et al. [GSH23] and Eisenstein et al. [Eis+23]. That is, we draw k\nsamples from the SFT model, rank them by the combined reward, and return the top ranked\nsample. This can be viewed as another alignment procedure, where the best-of-k sampling\nhas some (KL) divergence from the underlying policy, and produces samples with higher\nexpected reward. In our experiments, we try k increasing from 1 to 191.\nThere are two main motivations for considering best-of-k experiments. First, it demonstrates\nthat the reward aggregation method applies to methods beyond RLHF. Second, best-of-k\ndoesn\u2019t involve complicated optimizations. This allows us to disentangle the effect of having\nthe \u2018right\u2019 reward from the effect on solving the optimization problem.\nBaseline\nIn this setting, we take the baseline method to be a weighted sum of the raw\nrewards; i.e.,\nRbaseline\n\u2227\n:= wRhelp + (1 \u2212 w)Rharm.\n(5.1)\nWe sweep over w = 0.1,...,0.9, and report the baselines with best performance on our\nevaluation metrics.\nReference Rewards\nWe combine the transformed rewards by simple addition. Weighting\nthis addition doesn\u2019t have a clear motivation or interpretation. Instead, the choice of\nreference value used for each reward plays an analogous role. Intuitively, if we set a lower\nreference value for reward A then the reward becomes more easily saturated, and the\noptimization focuses more on reward B.\nFor best-of-k, using w = 0.5 achieves the best performance in the baseline method. Ac-\ncordingly, we want to take the reference rewards for helpfulness and harmlessness to be\ncomparable. To that end, we use the (estimated) SFT median for both helpfulness and\nharmlessness. Note that this is a lower reference than used for the individual optimizations.\nThis makes sense because the more difficult problem of optimizing both goals simultaneously\nstops the model from saturating the reward prematurely.\nFor RLHF, we observe that w = 0.6 gives the best result for weighted-sum approach. Then,\nwe want to set the reference reward for helpfulness to be somewhat higher. We use the\n(estimated) SFT 75%-quantile.\nThese choices are likely not optimal, and it\u2019s possible that further hyperparameter optimiza-\ntion could improve results.\nEvaluation\nWe want a metric that reflects Logical-AND. That is, whether we\u2019ve improved\nover the SFT model in both helpfulness and harmlessness. To that end, we\u2019ll say that a\ngeneration wins if it has helpfulness reward higher than the median helpfulness of SFT\nsamples, and harmlessness higher than the median harmlessness of SFT samples.\n5.3.2\nAggregation Results\nTransformed Aggregation Improves Alignment\nSumming the transformed reward should\nhave two advantages over the baseline method. First, it corresponds to logical AND. Second,\nit retains the benefits of alleviating reward overoptimization, as in the single-reward case.\n11\n(a) best-of-k (K = 191)\n(b) RLHF (KL \u2248 14)\nFigure 7: Summation of log-sigmoid transformed rewards corresponds better to logical AND. Aligned\npolicies using the former method have more balanced reward distributions (concentrated where the two\nreward values are similar), whereas the latter method leads to more unbalanced reward distributions.\nWe choose aligned policies by matching K for best-of-k and KL for RLHF. The rewards are centered\nby SFT median for both helpfulness and harmlessness. We visualize the joint distribution by kernel\ndensity estimate plot, with darker color indicating higher density.\nTogether, these should cause aligning by the transformed-combined reward to outperform\naligning by the baseline reward.\nFigure 6a and Figure 6b shows improvement over the SFT model for aligning using both\nrewards, for both best-of-k and RLHF. As anticipated, we see significant improvement from\nthe transformed method.\nIt is noteworthy that the transformed-aligned model outperforms in best-of-k, and for low-KL\nin the RLHF alignment. In these cases, there is little reward hacking (the aligned model is\ntoo close to the SFT model to have very bad behavior). Thus, the win here is apparently\ndue mainly to the logical AND effect. In the high-KL regime, the reward hacking effect kicks\nin and the transformed-reward dramatically outperforms the raw-reward baseline.\nTransformed Summation Corresponds to Logical AND\nNext, we check directly whether\nthe logical AND effect can be witnessed in the aligned LLM. To that end, we examine the\n12\ndistributions of rewards in the aligned policies. In Figure 7, our aggregation method leads\nto more balanced reward distributions (two reward values are often similar), whereas the\nbaseline method leads to more unbalanced reward distributions (one reward is much higher\nthan the other one). Note that best-of-k (k = 191) have quite different reward distributions\nthan RLHF at KL \u2248 14 (the former reflects the anti-correlation of the two rewards in the\ninitial policy; the latter updates the policy to have high aggregated reward values). But the\ntransformed aggregation method has consistent effects in both cases.\n6\nDiscussion and Related Work\nThere is a growing body of work on mitigating reward hacking in the RLHF pipeline. Tech-\nniques include forms of reward model averaging [Eis+23; Ram+24; Zha+23], constrained\noptimization [Mos+23], and reward model regularization [She+23], iterative human pref-\nerence collection [Bai+22; Sti+20; Fan+22], or data bias mitigation [Sin+23]. These\napproaches are complementary to the transformation technique proposed here, and could\nbe used in combination.\nThere have been several proposals for aligning language models to multiple objectives.\nThe most common approach is to combine individual reward models via a weighted sum\n[e.g., Wu+23; Mos+23]. Moskovitz et al. [Mos+23] identified a constraint threshold for\nindividual rewards, and formalized a constrained MDP problem, but the identification of the\nthreshold point relies on ground-truth queries. Bakker et al. [Bak+22] consider adapting\nsocial welfare schemes for aggregating the dissenting preferences of many individuals, in\ncontrast with our goal of satisfying all properties. Bai et al. [Bai+22] train a single reward\nmodel on both the helpfulness and harmlessness data, but discover that this leads to reward\nhacking harmlessness. They change the proportion of helpfulness data in the training to\ncircumvent this. Combining by summing transformed rewards allows us to circumvent such\nconsiderations.\nThe transformation technique in this paper is relevant to any alignment strategy that\nexplicitly maximizes an expected utility. There are now also alignment methods [e.g.,\nRaf+23; Aza+23; Zha+22] that use preference labels directly without explicitly instantiating\nreward models. Note, however, that if we want to align to multiple properties, we still\nneed to compute rankings from an aggregate. The simplest approach is to train reward\nmodels for individual properties, combine these reward models, and then rank samples\nusing the combine reward. Our best-of-k experiments show that the transformation can\nyield significant gains even in this case.\nFinally, we note that Azar et al. [Aza+23] also emphasizes the need for a bounded utility\nfunction. The work here can be viewed, in part, as a way of incorporating this insight that\nstill maintains the standard utility maximization pipeline. It is an interesting question for\nfuture work whether making explicit use of a (transformed) reward improves the alignment\nstep relative to using only ranked pairs.\nReferences\n[Ani+23]\nR. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri,\nE. Taropa, P. Bailey, Z. Chen, et al. \u201cPalm 2 technical report\u201d. arXiv preprint\narXiv:2305.10403 (2023) (cit. on p. 8).\n[Aza+23]\nM. G. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, and R.\nMunos. \u201cA general theoretical paradigm to understand learning from human\npreferences\u201d. arXiv preprint arXiv:2310.12036 (2023) (cit. on p. 13).\n13\n[Bai+22]\nY. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain, S.\nFort, D. Ganguli, T. Henighan, et al. \u201cTraining a helpful and harmless as-\nsistant with reinforcement learning from human feedback\u201d. arXiv preprint\narXiv:2204.05862 (2022) (cit. on pp. 8, 10, 11, 13).\n[Bak+22]\nM. A. Bakker, M. J. Chadwick, H. R. Sheahan, M. H. Tessler, L. Campbell-\nGillingham, J. Balaguer, N. McAleese, A. Glaese, J. Aslanides, M. M. Botvinick,\nand C. Summerfield. Fine-tuning language models to find agreement among\nhumans with diverse preferences. 2022. arXiv: 2211.15006 [cs.LG] (cit. on\np. 13).\n[BT52]\nR. A. Bradley and M. E. Terry. \u201cRank analysis of incomplete block designs: i.\nthe method of paired comparisons\u201d. Biometrika 3/4 (1952) (cit. on p. 3).\n[Cos+23]\nT. Coste, U. Anwar, R. Kirk, and D. Krueger. \u201cReward model ensembles help\nmitigate overoptimization\u201d. arXiv preprint arXiv:2310.02743 (2023) (cit. on\np. 9).\n[Dub+23]\nY. Dubois, X. Li, R. Taori, T. Zhang, I. Gulrajani, J. Ba, C. Guestrin, P. Liang, and\nT. B. Hashimoto. \u201cAlpacafarm: a simulation framework for methods that learn\nfrom human feedback\u201d. arXiv preprint arXiv:2305.14387 (2023) (cit. on p. 9).\n[Eis+23]\nJ. Eisenstein, C. Nagpal, A. Agarwal, A. Beirami, A. D\u2019Amour, D. Dvijotham,\nA. Fisch, K. Heller, S. Pfohl, D. Ramachandran, et al. \u201cHelping or herding?\nreward model ensembles mitigate but do not eliminate reward hacking\u201d. arXiv\npreprint arXiv:2312.09244 (2023) (cit. on pp. 9\u201311, 13, 16).\n[Fan+22]\nX. Fan, Y. Lyu, P. P. Liang, R. Salakhutdinov, and L.-P. Morency. \u201cNano: nested\nhuman-in-the-loop reward learning for few-shot language model control\u201d.\narXiv preprint arXiv:2211.05750 (2022) (cit. on p. 13).\n[GSH23]\nL. Gao, J. Schulman, and J. Hilton. \u201cScaling laws for reward model overop-\ntimization\u201d. In: International Conference on Machine Learning. PMLR. 2023\n(cit. on pp. 9, 11).\n[Hou+23]\nY. Hou, J. Zhang, Z. Lin, H. Lu, R. Xie, J. McAuley, and W. X. Zhao. \u201cLarge lan-\nguage models are zero-shot rankers for recommender systems\u201d. arXiv preprint\narXiv:2305.08845 (2023) (cit. on p. 16).\n[KPB22]\nT. Korbak, E. Perez, and C. L. Buckley. \u201cRl with kl penalties is better viewed as\nbayesian inference\u201d. arXiv preprint arXiv:2205.11275 (2022) (cit. on p. 4).\n[Mos+23]\nT. Moskovitz, A. K. Singh, D. Strouse, T. Sandholm, R. Salakhutdinov, A. D.\nDragan, and S. McAleer. Confronting reward model overoptimization with con-\nstrained rlhf. 2023. arXiv: 2310.04373 [cs.LG] (cit. on p. 13).\n[Ouy+22]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,\nS. Agarwal, K. Slama, A. Ray, et al. \u201cTraining language models to follow\ninstructions with human feedback\u201d. Advances in Neural Information Processing\nSystems (2022) (cit. on p. 1).\n[Raf+23]\nR. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn.\n\u201cDirect preference optimization: your language model is secretly a reward\nmodel\u201d. arXiv preprint arXiv:2305.18290 (2023) (cit. on pp. 9, 13).\n[Raf+20]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li,\nand P. J. Liu. \u201cExploring the limits of transfer learning with a unified text-to-text\ntransformer\u201d. The Journal of Machine Learning Research 1 (2020) (cit. on p. 8).\n[Ram+24]\nA. Ram\u00e9, N. Vieillard, L. Hussenot, R. Dadashi, G. Cideron, O. Bachem, and\nJ. Ferret. \u201cWarm: on the benefits of weight averaged reward models\u201d. arXiv\npreprint arXiv:2401.12187 (2024) (cit. on p. 13).\n[She+23]\nL. Shen, S. Chen, L. Song, L. Jin, B. Peng, H. Mi, D. Khashabi, and D. Yu.\n\u201cThe trickle-down impact of reward (in-) consistency on rlhf\u201d. arXiv preprint\narXiv:2309.16155 (2023) (cit. on p. 13).\n[Sin+23]\nP. Singhal, T. Goyal, J. Xu, and G. Durrett. \u201cA long way to go: investigating\nlength correlations in rlhf\u201d. arXiv preprint arXiv:2310.03716 (2023) (cit. on\npp. 9, 13).\n14\n[Sti+20]\nN. Stiennon, L. Ouyang, J. Wu, D. Ziegler, R. Lowe, C. Voss, A. Radford, D.\nAmodei, and P. F. Christiano. \u201cLearning to summarize with human feedback\u201d.\nAdvances in Neural Information Processing Systems (2020) (cit. on pp. 1, 13).\n[Wu+23]\nZ. Wu, Y. Hu, W. Shi, N. Dziri, A. Suhr, P. Ammanabrolu, N. A. Smith, M.\nOstendorf, and H. Hajishirzi. Fine-grained human feedback gives better rewards\nfor language model training. 2023. arXiv: 2306.01693 [cs.CL] (cit. on p. 13).\n[Zha+23]\nY. Zhai, H. Zhang, Y. Lei, Y. Yu, K. Xu, D. Feng, B. Ding, and H. Wang. \u201cUncertainty-\npenalized reinforcement learning from human feedback with diverse reward\nlora ensembles\u201d. arXiv preprint arXiv:2401.00243 (2023) (cit. on p. 13).\n[Zha+22]\nY. Zhao, M. Khalman, R. Joshi, S. Narayan, M. Saleh, and P. J. Liu. \u201cCalibrating\nsequence likelihood improves conditional language generation\u201d. arXiv preprint\narXiv:2210.00045 (2022) (cit. on p. 13).\n[Zie+19]\nD. M. Ziegler, N. Stiennon, J. Wu, T. B. Brown, A. Radford, D. Amodei, P. Chris-\ntiano, and G. Irving. \u201cFine-tuning language models from human preferences\u201d.\narXiv preprint arXiv:1909.08593 (2019) (cit. on p. 1).\n15\nA\nAdditional Experiment Details\nA.1\nImplementation for reference value prediction\nThe goal is to develop models that can predict various quantiles of the sampled rewards for\ndifferent prompts. Instead of building separate models for each quantile, we assume the\nreward distribution for each prompt follows a Gaussian distribution. Under this assumption,\nwe build two models: Mean Prediction Model (rmean) and Standard Deviation Prediction\nModel (rstd) to predict the mean and standard deviation of the reward distribution for each\nprompt x.\nTo estimate a specific quantile for a prompt, the formula used is:\nrmean(x) + s \u00d7 rstd(x)\nwhere s is a scaling factor corresponding to the desired quantile (e.g., using s = 0.7\napproximately gives us the 75% quantile).\nThe training data is collected by generating 64 responses per prompt using the SFT policy\nmodel. These responses are scored to calculate the sample mean and standard deviation of\nthe reward distribution for each prompt, which are then used as the response variables in\ntraining the models rmean and rstd.\nA.2\nPALM-2 Evaluation Details\nWe evaluate the win-rates with zero-shot prompting. For each prompt, we sample (y\u03c0, ysft)\nfrom the aligned and SFT policy, then ask the PALM-2 model which is more helpful/harmless.\nThe prompt template is Figure 8.\nIn order to counter positional bias [Hou+23], we run PALM-2 on the two possible orderings\n(y\u03c0, ysft) and (ysft, y\u03c0), sample N = 8 outputs for each order and determine the winner by\nmajority voting.\nA.3\nHeuristics for shortcut discovery\nFor the helpfulness task, we follow the same practice in [Eis+23] and find responses in the\nformat of a list.\nFor harmlessness task, we see the aligned policy starts to only generate similar generations\nto different questions as KL gets larger. To find shortcuts, we analyze generations of the\naligned policy (using raw reward as utility) at KL \u2248 14. Then we perform topic modeling\nand identify one dominant topic characterized by the recurring theme of \"you should talk\nto [profession].\" From the most frequent 50 words in that topic, we find all the profession-\nrelated words: therapist, professional, doctor, psychologist, counselor, lawyer, police. Then\nwe find all responses containing profession-related words.\nA.4\nRLHF training details\nWe use Proximal Policy Optimization (PPO) to perform RLHF alignment. The specific\nhyper-parameters are in table 1 We sweep over \u03b3\u2019s to get aligned policies with different\nKL values. Since we want to match (converged) polices by their KL values, we use some\nheuristic to predict converged KL for a chosen \u03b3 (so they may not look like regular linear or\ngeometric series). More specifically, we use the parameterization of \u03b1 =\n\u03b3\n1+\u03b3, and the values\nused are in table 2. Note that to get the same KL value, we use smaller KL regularization\nfor reward transformation, than using the raw reward. This is intuitive as the log-sigmoid\ntransformation prevents using KL budgets once the reward saturates.\n16\nFigure 8: Prompt for PALM-2 Evaluation\n17\nTable 1: Hyper-parameters for RLHF.\nParameter\nValue\nPolicy learning rate\n5 \u00b7 10\u22126\nValue learning rate\n4 \u00b7 10\u22125\nLearning schedule\nConstant (linear warm-up)\nTraining steps\n20000\nWarm-up steps\n2000\nBatch size\n32\nInput length\n1024\nOutput length\n256\nTable 2: Alphas for Various Task-Methods.\nTask-Method\n\u03b1 :=\n\u03b3\n1+\u03b3\nHelpfulness (u = reward)\n[0.081, 0.086, 0.092, 0.1, 0.114, 0.132, 0.161, 0.222]\nHelpfulness (u = transformed reward)\n[0.02, 0.023, 0.028, 0.032, 0.039, 0.053, 0.075, 0.123]\nHarmlessness (u = reward)\n[0.169, 0.182, 0.196, 0.218, 0.248, 0.32]\nHarmlessness (u = transformed reward)\n[0.032, 0.041, 0.052, 0.079, 0.126, 0.222]\nH+H (0.5rhelp + 0.5rharmless)\n[0.078, 0.08, 0.084, 0.088, 0.094, 0.102, 0.116, 0.134, 0.168]\nH+H (0.6rhelp + 0.4rharmless)\n[0.068, 0.071, 0.074, 0.077, 0.082, 0.088, 0.1, 0.123, 0.163]\nH+H (0.7rhelp + 0.3rharmless)\n[0.057, 0.06, 0.065, 0.07, 0.075, 0.084, 0.1, 0.126, 0.173]\nH+H (sum of transformed reward)\n[0.014, 0.0145, 0.015, 0.017, 0.018, 0.02, 0.023, 0.026, 0.031, 0.034, 0.04, 0.048, 0.066, 0.096]\nB\nMore Experiment Results\nIn Figure 4 we choose a pair of aligned policies matched on KL values to show that reward\ntransformation leads to more uniform improvement than using the raw reward. In Figure 9\nwe visually show reward overoptimization (in the same aligned policies), and how reward\ntransformation alleviates it. The mitigation is most obvious for responses with larger reward\nvalues. It\u2019s also important to note that reward aggregation exacerbates reward hacking with\nweighed sum of the raw reward: the policy model is incentivized to generate responses with\nvery high harmlessness score (despite having smaller weight for harmlessness); to retain\ngood scores in helpfulness, the policy model hacks the helpfulness reward model even more\n(compare Figure 9a against Figure 9c). Using sum of transformed reward alleviates this by\ntwo mechanisms.\nIn Figure 10 we show reward transformation leads to more concentrated reward distribution\nthan using raw reward, across different KL values.\nIn Figure 3 we report win-rate against random sample under PALM-2, and against SFT\n85%-quantile for helpfulness and 95%-quantile for harmlessness. In Figure 11, we report\nthe extra evaluations.\n18\n(a) Helpful (single reward): KL \u2248 18\n(b) Harmless (single reward): KL \u2248 7\n(c) Helpful (aggregated reward): KL \u2248 14\n(d) Harmless (aggregated reward): KL \u2248 14\nFigure 9: Reward transformation mitigates reward overoptimization, particularly for responses with\nlarger reward values, and in reward aggregation where reward hacking is more severe. We compare\nreward overoptimization patterns in policies aligned with raw reward and transformed reward, in\nsingle reward (Figure 9a, Figure 9b) and reward aggregation (Figure 9c, Figure 9d) settings. The\nchoice of aligned policies and score centering are the same as in Figure 4. For each plot, we sort the\ncentered scores from T5-base reward model into 20 equal-width bins. For each bin with more than 10\ndata points: in the x-axis we show the interval medium; in the y-axis, we visualize the 25%, 50% and\n75% quantile of the corresponding centered scores from T5-xxl.\n(a) Helpfulness\n(b) Harmlessness\nFigure 10: Reward Transformation leads to more uniform reward improvements than baseline. The\nreward values are centered by median SFT rewards. This complements the results in Figure 4 for\ncomparisons across KL values.\n19\n(a) SFT Random Sample\n(b) SFT 50%-quantile\n(c) SFT 75%-quantile\n(d) SFT 95%-quantile\n(e) Helpfulness\n(f) SFT Random Sample\n(g) SFT 50%-quantile\n(h) SFT 75%-quantile\n(i) SFT 85%-quantile\n(j) Harmlessness\nFigure 11: Transformed reward obtains better KL and win-rate trade-offs. These are win-rates compared\nagainst SFT random sample, and q-th quantile of the rewards of SFT samples. See details in Figure 3.\n20\n"
  },
  {
    "title": "EE-Tuning: An Economical yet Scalable Solution for Tuning Early-Exit Large Language Models",
    "link": "https://arxiv.org/pdf/2402.00518.pdf",
    "upvote": "3",
    "text": "EE-Tuning: An Economical yet Scalable Solution for Tuning\nEarly-Exit Large Language Models\nXuchen Pan\u2217, Yanxi Chen\u2217, Yaliang Li, Bolin Ding, Jingren Zhou\n{panxuchen.pxc, chenyanxi.cyx, yaliang.li, bolin.ding, jingren.zhou}@alibaba-inc.com\nAlibaba Group\nAbstract\nThis work introduces EE-Tuning, a lightweight and economical solution to training/tuning early-exit\nlarge language models (LLMs).\nIn contrast to the common approach of full-parameter pre-training,\nEE-Tuning augments any pre-trained (and possibly fine-tuned) standard LLM with additional early-exit\nlayers that are tuned in a parameter-efficient manner, which requires significantly less computational\nresources and training data. Our implementation of EE-Tuning achieves outstanding training efficiency\nvia extensive performance optimizations, as well as scalability due to its full compatibility with 3D par-\nallelism. Results of systematic experiments validate the efficacy of EE-Tuning, confirming that effective\nearly-exit LLM inference can be achieved with a limited training budget. In hope of making early-exit\nLLMs accessible to the community, we release the source code of our implementation of EE-Tuning at\nhttps://github.com/pan-x-c/EE-LLM.\nContents\n1\nIntroduction\n2\n2\nMethodology\n3\n2.1\nStage 1: initializing early-exit layers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n2.2\nStage 2: tuning early-exit layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n2.3\nAdditional features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n3\nExperiments\n6\n3.1\nEfficiency of EE-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2\nArchitectures of early-exit layers\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n3.3\nInitialization of early-exit layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nEE-Tuning for models of various sizes\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n3.5\nAdditional experiments\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n4\nLimitations and future work\n11\n5\nConclusions\n12\nA Additional experiments\n12\nA.1 Training data for EE-Tuning\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n12\nA.2 Supplementary results for Section 3.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.3 Dynamic token-wise loss weighting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nA.4 Continued pre-training (CPT) after EE-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\n\u2217Co-first authors.\n1\narXiv:2402.00518v1  [cs.LG]  1 Feb 2024\n1\nIntroduction\nTransformer-based large language models (LLMs) have achieved extraordinary performance on various lan-\nguage tasks [51, 4, 32, 48, 49, 7]. Meanwhile, these models incur high costs and latency during the inference\nphase, due to their increasingly large sizes. Early exiting has proven to be a simple yet effective technique\nfor accelerating inference of LLMs and other deep neural networks. In this approach, early-exit layers are\nattached to the original deep neural network, which can convert intermediate hidden states into early-exit\noutput. During inference, the model can adaptively select one early exit to generate the output for each\ninput sample, skipping the forward computation of the remaining layers of the network. Early exiting has\nfound success in natural language processing [13, 18, 57, 41, 29, 11, 52, 27, 40, 53, 54, 19], computer vision\n[33, 47, 22, 21], and many other areas [38, 26, 14, 9].\nThis work considers token-wise early exiting for generative LLMs and autoregressive natural language\ngeneration [40, 8, 2, 50, 12, 6]. While the majority of prior works in this area have focused on designing\nearly-exit inference mechanisms, we instead focus on how to train an early-exit LLM in the first place. The\nstandard and straightforward method, adopted in most prior works on early exiting, is to jointly train all\nmodel parameters (including the network backbone and early-exit layers) from scratch, by minimizing a\nweighted sum of training losses from early and final exits. Recent work has made this approach compatible\nwith massive 3D parallelism, thereby scaling up early-exit LLMs to sizes as large as any standard LLM\nthat can possibly be trained with state-of-the-art LLM frameworks [43, 31, 6].\nThe obvious issue with\nthis approach is its excessively high costs and complexity. Indeed, the massive amount of computational\nresources required to train an LLM with billions of parameters is simply inaccessible to most members of the\ncommunity. Oftentimes in practice, one has access to the weights of an existing pre-trained (and possibly\nfine-tuned) standard LLM that might be private or open-source, and wonder if it is possible to train an\nearly-exit LLM by leveraging such information rather than from scratch.\nAll these motivate us to convert an existing generative LLM to an early-exit one, in a way that\n\u2022 requires minimum computational resources;\n\u2022 leads to satisfactory inference acceleration; and\n\u2022 preserves the full capability of the original LLM.\nMain contributions.\nThis work introduces EE-Tuning, a principled and lightweight approach of trans-\nforming a pre-trained (and possibly fine-tuned) LLM into an early-exit one, which satisfies all the above\nrequirements. At the core of EE-Tuning is an intuitive and practical two-stage procedure:\n1. Take a pre-trained standard LLM as input, and augment its architecture with early-exit layers, whose\nparameters are initialized properly;\n2. Tune the early-exit layers via backpropagation of certain training losses in a parameter-efficient manner,\nwith modules of the original standard LLM frozen.\nSee Figure 1 for a visualization of our method, whose details will be elucidated in Section 2. Our implementa-\ntion is based on the recently proposed EE-LLM framework [6], complementing the latter with an alternative\nsolution to training early-exit LLMs that is both accessible and scalable, thanks to its low computational\ncomplexity and full compatibility with 3D parallelism. In other words, any LLM developer, with access to\neither one GPU or a cluster with thousands of GPUs, will find EE-Tuning a useful and practical tool for\nstudying and applying early exiting. Our implementation also includes support for various configurations\nand other favorable features, which further makes it more convenient to use.1\nThe efficacy of EE-Tuning is validated via extensive and systematic experiments for models with up to\n70 billion (70B) parameters, an unprecedented scale for early-exit LLMs. More specifically, a pre-trained\nLLM can quickly acquire the ability of early exiting via the tuning process with fast and stable convergence,\nwhich takes less than 1/1000 of the GPU hours and training data used in its pre-training stage, and requires\nonly one or a few GPUs. Meanwhile, the converted model can achieve 1.2\u00d7 to 1.6\u00d7 speedup on various\n1Henceforth, we let EE-Tuning refer to the proposed two-stage method or our implementation of it, depending on the context.\n2\nInput \nlayer\nTransformer layers (backbone)\nFinal-exit \nlayer\nEE \nlayer\nEE \nlayer\nEE \nlayer\nStage 1: initialize early-exit layers\nInput \nlayer\nTransformer layers (backbone)\nFinal-exit \nlayer\nEE \nlayer\nEE \nlayer\nEE \nlayer\nData \nbatch\nLoss\nLoss\nLoss\nStage 2: tune early-exit layers\nAdded modules\nOriginal standard LLM\nForward\nBackward\nFigure 1: An outline of EE-Tuning, the proposed two-stage procedure that converts a pre-trained standard\nLLM into a well-trained early-exit LLM.\ndownstream tasks through early exiting while maintaining comparable or even better benchmark scores, or\nhigher speedup if slight degeneration of output quality is acceptable. We thoroughly investigate the effects of\nvarious design choices and provide practical guidelines for maximizing the performance of EE-Tuning. The\nsource code of EE-Tuning is available at https://github.com/pan-x-c/EE-LLM.\nRelated works.\nThe idea of EE-Tuning, i.e. augmenting a pre-trained neural network with early-exit layers\nthat are tuned in a parameter-efficient manner, is not completely new. This strategy has been adopted in\nsome prior works for model architectures tailored to classification tasks, e.g. the encoder-only BERT model\n[52, 29, 19] or others [33, 22, 3, 9].\nHowever, there is no guarantee that results and conclusions from\nthese works can safely transfer to the case of decoder-only Transformers tailored to autoregressive sequence\ngeneration, which is the focus of our work.\nAnother recent work [50] proposed to initialize the model\nparameters of early-exit LLMs with pre-trained standard LLMs, but followed by full-parameter training.\nClosest to our setting and training methodology is the recent work [12], which investigated generative LLMs\nof sizes up to 355M, and only considered linear exit heads that are randomly initialized. Moreover, that work\nproposed to use multiple exits for improving the final output of full-model inference, rather than accelerating\ninference via early exiting. In contrast, our implementation of the proposed EE-Tuning method (1) is unified\nand systematic, with support for a wide range of configurations; (2) is scalable, thanks to its full compatibility\nwith 3D parallelism; and (3) has proven via extensive experiments to return early-exit LLMs that achieve\noutstanding acceleration during autoregressive inference.\n2\nMethodology\nThis section elaborates on our methodology and implementation of obtaining a well-trained early-exit LLM\nvia EE-Tuning, the two-stage procedure visualized in Figure 1.\nPreliminaries.\nModern LLMs are mostly based on the Transformer architecture [51]. We focus on the\ndecoder-only generative pre-training (GPT) Transformer architecture [35, 36], although many techniques\npresented in this work can be generalized to broader settings. As visualized in Figure 1, a GPT Transformer\nis composed of an initial layer for input processing, a stack of Transformer layers as the backbone, and an\noutput layer that converts the final hidden states to logits on the vocabulary, which can be used for generating\nnew tokens. Each Transformer layer consists of an attention module and a multi-layer perceptron (MLP),\nwith layer normalization [1] and residual connections [15] applied in multiple places. The final output layer\nincludes an optional layer normalization module, followed by a large output embedding matrix.\nA GPT Transformer can be trained in an unsupervised manner, by optimizing the language mod-\neling loss on unlabeled corpus.\nMore specifically, given model parameters \u03b8 and a sequence of tokens\nx = (x1, x2, . . . , xT ), the autoregressive language modeling loss, i.e. negative log-likelihood of next-token\nprediction, is defined as L(x; \u03b8) := \u2212 log P(x; \u03b8) = \u2212 P\nt\u2208[T ] log P(xt|x1, . . . , xt\u22121; \u03b8).\n3\n(a) Norm\n(b) MLP\n(c) Layer\nTransformer \nlayer\nTransformer \nlayer\nNormalization\nOutput\nembedding\nAttention\nMLP\nMLP\nNormalization\nOutput\nembedding\nTransformer layer\nTransformer \nlayer\nTransformer \nlayer\nNormalization\nOutput\nembedding\nTransformer \nlayer\nEE output\nEE output\nEE output\nFigure 2: A visualization of various early-exit architectures. Each attention or MLP module follows the\nresidual structure with pre-normalization.\n2.1\nStage 1: initializing early-exit layers\nIn the first stage of EE-Tuning, we load a pre-trained standard LLM checkpoint, and augment its model\narchitecture by adding early-exit layers to pre-specified locations.\nMore concretely, we first specify the\narchitecture of early-exit layers, and then initialize their parameters, as explained in the following.\nArchitectures of early exits.\nIn theory, an early-exit layer can be a generic mapping from Rh to RV ,\nwhere h is the hidden size and V is the vocabulary size.\nOur implementation supports the early-exit\narchitectures proposed in prior works [6], which we summarize below.\n\u2022 Embedding: This is a minimalistic early-exit architecture with a single linear layer, i.e. an output\nembedding matrix of size h \u00d7 V that converts hidden states to logits on the vocabulary.\n\u2022 Norm: A LayerNorm [1] or RMSNorm [55] module, which can stabilize training and improve output\nquality of the model, is added in front of the Embedding architecture. We recommend making the early-\nexit architecture consistent with the final-exit layer of the original LLM, i.e. including the normalization\nmodule in each early exit if there is one in the final-exit layer.\n\u2022 MLP: An MLP, with the same structure as the MLPs in the Transformer backbone, is added in front of\nthe Norm architecture.\n\u2022 Layer: A complete Transformer layer, with the same structure as those on the backbone, is added in\nfront of the Norm architecture.\nSee Figure 2 for a visualization of these architectures. Generally speaking, a larger number of trainable\nparameters leads to higher expressivity and adaptivity, but also larger inference latency and potentially\nhigher risk of overfitting during the tuning process, of early-exit layers. Also note that as a by-product, each\nsub-model between the input of the network and the output of a certain early exit can be regarded as a\nstandard GPT Transformer.\nRemark 1. Throughout this work, we assume that the output embedding matrices of all exits are untied,\nunless otherwise specified.\nSome recent works on early-exit LLMs choose to tie the output embedding\nmatrices [39, 50], and EE-Tuning can be adapted for this case with some modifications.\nInitialization of early exits.\nOne natural option for initializing the model parameters is random ini-\ntialization, in the same way as it is done before pre-training a standard LLM from scratch. To accelerate\nconvergence of EE-Tuning, we propose a new approach of initializing early-exit layers by copying model\nparameters from certain modules of the original pre-trained LLM. More specifically,\n\u2022 For Embedding and Norm, parameters of the output embedding matrix and normalization module can\nbe copied from the corresponding modules in the final-exit layer of the original LLM;\n4\n1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8\nStage 1\n1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8\nStage 2\n1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8\nStage 3\n1 1 2 2 3 3 4 4 5 5 6 6 7 7 8 8\nStage 4\nBackward\nForward\nFigure 3: One training iteration of our customized pipeline schedule used in EE-Tuning, in a setting with 4\npipeline stages and 8 microbatches indexed by numbers in the blocks.\n\u2022 For MLP, the MLP module for an early exit connected to some Transformer layer can be initialized as\na duplication of the original MLP within the same Transformer layer;\n\u2022 For Layer, the Transformer layer within an exit can be initialized as a duplication of the last Trans-\nformer layer of the original LLM.\nOur proposed approach is primarily inspired by the residual structure [15] widely adopted in modern\nLLMs, which implies that the output of each Transformer layer is generated by adding a small residual\ncomponent to the input.2 After initialization by copying, the forward pass from the input to the early-exit\noutput is the same as that of the original LLM, except that certain Transformer layers are skipped. Given\nthe residual structure of the skipped layers, it is reasonable to assume that the early-exit output is still more\nor less meaningful, which justifies the proposed method of initialization. Besides accelerating convergence,\nanother potential benefit of initializing by copying is that early-exit layers might inherit the knowledge and\nabilities that have been learned and embedded in modules of the pre-trained standard LLM.\n2.2\nStage 2: tuning early-exit layers\nIn the second stage of EE-Tuning, we tune the early-exit layers via standard backpropagation of training\nlosses from multiple exits, while modules of the original standard LLM are frozen. By default, we use the\nautoregressive language modeling loss on open-source datasets of unlabeled corpus, although other options\nare possible. Note that, as visualized in Figure 1, this tuning process is equivalent to learning multiple\nshallow networks independently and in parallel, each of which is an early-exit layer that maps the hidden\nstates at a certain Transformer layer on the backbone to the corresponding early-exit outputs. Given the\nrelatively small number of trainable parameters, we opt for a small batch size in pursuit of fast convergence\nand good generalization, akin to the standard practice of fine-tuning a small proportion of parameters in a\npre-trained deep neural network.\nComputational efficiency.\nOur implementation of EE-Tuning has been well-optimized for maximum\ncomputational efficiency. More specifically, only the minimum amount of necessary computation is executed,\nincluding (1) the partial forward pass of the Transformer backbone up to the hidden states connected to\nthe last early exit, and (2) the forward computation, backward computation, and parameter update for\neach early-exit layer, without any dependency between early exits. As for memory usage, we (1) implement\npartial checkpoint loading, so that only modules of the original LLM in front of the hidden states connected\nto the last early exit need to be loaded into GPU memory; and (2) maintain optimizer states for the early-\nexit layers only. With our implementation, EE-Tuning incurs minor computational overhead compared to a\npartial forward pass of the original LLM.\nRemark 2. If one chooses to tie the output embedding matrices from multiple exits, which are untied from\nthe input embedding matrix, then EE-Tuning requires backward computation only for early-exit layers as\nbefore. However, if the output embedding matrices are further tied with the input embedding [34], then a\nfull backward pass through the whole network is needed for gradient calculation, which incurs much higher\ncomputational costs.\n2More rigorously, this is true for Transformers with pre-normalization (the case that we consider in this work), not for\npost-normalization. Nonetheless, the rationale behind initialization by copying remains mostly correct in the latter case.\n5\nSupport for 3D parallelism.\nBuilt upon prior works [43, 31, 6], our implementation of EE-Tuning natu-\nrally supports massive 3D parallelism, namely data, tensor, sequence and pipeline parallelism3. For brevity,\nwe refer interested readers to prior works, e.g. the Megatron-LM series [43, 31, 25, 45], for details about\n3D parallelism, which are omitted here. It is worth noting that for EE-Tuning, we design and implement\na customized pipeline schedule with forward communication only, as shown in Figure 3.\nTo see how it\nboosts training efficiency, recall that for full-parameter training, each pipeline stage need to save all inter-\nmediate activations of forward computation, and wait for the gradients sent backward by the next stage\nbefore backward computation can start. In contrast, things are much easier for EE-Tuning: each early-exit\nlayer within a certain stage can calculate its own loss and execute backward computation independently, as\nsoon as the forward pass of the Transformer backbone within that stage is completed. Consequently, there\nis no need to save intermediate activations on the Transformer backbone, or backward communication for\nbackpropagation.\n2.3\nAdditional features\nSome additional features in our implementation of EE-Tuning are introduced in the following.\nPlug-and-play early exits.\nWith our implementation, one can first tune multiple early exits, and then\ndecide which one(s) to actually activate for inference, in a flexible plug-and-play manner. Various subsets of\nthe same set of tuned early exits can be chosen for different use cases. This flexibility of deployment can be\nbeneficial in practice, since the best number and locations of early exits are often unknown beforehand, and\nmight depend on the specific use case.\nDynamic token-wise loss weights.\nSo far, we have been assuming that each early exit assigns equal\nweights to all tokens of the training data when defining its training objective.\nThis, however, causes a\ndeviation from the actual process of early-exit inference, where each exit needs to make predictions only for\n\u201ceasy\u201d tokens that it has high confidence for, rather than for all tokens. Some recent works [46, 37, 10, 33, 3]\nhave proposed to reduce this mismatch by dynamic token-wise loss weights and observed positive outcome,\nwhich motivates us to implement one version of this approach. More specifically, during the forward pass\nof each data batch, each exit records its confidence values for tokens in the batch, which will be used as\nthe weights of tokens for defining the training loss before backward computation. In this way, each early\nexit learns to handle the tokens that it has high confidence in, without being forced to make predictions for\n\u201chard\u201d tokens that are beyond its capability.\n3\nExperiments\nIn this section, we validate the efficacy of EE-Tuning through extensive experiments. First in Section 3.1, we\ndemonstrate the efficiency of EE-Tuning for models of sizes up to 70B, an unprecedented scale of early-exit\nmodels in the literature. Section 3.2 investigates the impacts of early-exit architectures on both training\nlosses and downstream performance, and Section 3.3 compares both methods of initializing parameters of\nearly exits. We further confirm the efficacy of EE-Tuning for models of various sizes, ranging from 7B to\n70B, in Section 3.4. Additional empirical results are deferred to Appendix A.\nAll experiments were conducted with a multi-node GPU cluster, where each node hosts 8 Nvidia A800-80G\nGPUs. We explain below the common setup that is adopted throughout our experiments, before presenting\nthe empirical results.\nModels.\nFor standard LLMs, we use the open Llama 2-Chat models [49] of sizes 7B, 13B and 70B, which\nfollow the decoder-only GPT architecture with pre-normalization. Each model has gone through pre-training\nwith trillions of tokens of training data, and alignment with human preferences via supervised fine-tuning\nand reinforcement learning from human feedback. Our early-exit LLMs are constructed by augmenting these\n3With pipeline parallelism, a deep neural network is partitioned into multiple pipeline stages along the depth dimension,\nwhich are assigned to different computational devices. In addition, each data batch is divided into multiple microbatches, so\nthat their forward and backward computation on different stages can be pipelined and parallelized via some schedule.\n6\nTable 1: Tasks for downstream evaluation.\nTask\nType\nMetric\nNum. tokens\nCNN/DailyMail\nSummarization\nROUGE-L\n128\nXSUM\nSummarization\nROUGE-L\n64\nNarrativeQA\nReading comprehension\nF1\n100\nMMLU\nLanguage understanding\nExact Match\n1\nLlama 2-Chat models with early-exit layers of various architectures. For convenience, we denote the two\nmethods of initializing model parameters, which have been introduced in Section 2.1, as random and copy\nrespectively.\nTuning.\nFor the tuning process proposed in Section 2.2, we set the batch size to a small value of 16, the\nsequence length to 2048, and the number of training iterations to 4 \u00d7 104. Therefore, the total amount of\ntraining data for tuning an early-exit LLM is 16 \u00d7 2048 \u00d7 4 \u00d7 104 \u2248 1.3 billion tokens, which is almost\nnegligible compared to the number of tokens needed for full-parameter training of a standard or early-exit\nLLM. Our training data is randomly sampled from the refined pre-training data provided by Data-Juicer [5],\nunless otherwise specified. The standard autoregressive language modeling loss, as explained in Section 2,\nis used as the training objective in our experiments. We use the Adam optimizer [23] with hyperparameters\n\u03b21 = 0.9, \u03b22 = 0.95, \u03f5 = 10\u22125, together with a learning rate schedule with linear decay, a warmup fraction of\n0.01, a maximum learning rate of 10\u22124 and a minimum of 10\u22125.\nInference.\nFor early-exit inference, we use greedy decoding and a confidence-based exit condition, i.e. the\nmodel outputs the most likely token as soon as the maximum probability of next-token prediction at some\nearly exit is above a pre-specified threshold. We utilize the pipeline-based inference mechanism from prior\nwork [6], which is compatible with KV caching. Inference efficiency is measured by wall-clock latency. When\nthe confidence threshold is set to 1, early exits are disabled, so that our early-exit models become equivalent\nto the original Llama 2-Chat models. Full-model inference latency in this case is used as the baseline for\ncalculating relative speedup by inference with smaller thresholds. We conduct downstream evaluation with\nHELM [28] on four tasks, namely CNN/DailyMail [42], XSUM [30], NarrativeQA [24] and MMLU [16]. For\neach task, we use a maximum context length of 2048, specify the number of generated tokens, and report\n5-shot performance. See Table 1 for a summary of the tasks.\n3.1\nEfficiency of EE-Tuning\nObservation: EE-Tuning with Nvidia A800-80G GPUs and without model partitioning can\n(1) convert a Llama 2-Chat 13B model within 20 GPU hours, with a peak GPU memory of 20G;\n(2) convert a Llama 2-Chat 70B model within 120 GPU hours, with a peak GPU memory of 78G.\nOur first experiment demonstrates the efficiency of EE-Tuning. In this setup, we consider tuning one MLP\nearly exit that is added to the 1/4 depth of the Llama 2-Chat model. We use 4 GPUs for a data parallelism\ndegree of 4, without tensor or pipeline parallelism. The result is that tuning a 13B model takes about 5 hours\nand a peak memory usage of 20G. Such a small memory footprint means that an Nvidia RTX 4090 with 24G\nmemory can also handle this task. In addition, tuning a 70B model takes about 30 hours and a peak memory\nusage of 78G, which can fit into a single A800-80G GPU. Such remarkable efficiency is achievable since with\nour implementation, only the first 1/4 part of the Llama 2-Chat model checkpoint needs to be loaded into\nmemory, only optimizer states for the early-exit layer need to be maintained, and only the minimum amount\nof necessary computation is executed.\nThe curves of training losses at various exits, which are deferred to Section 3.4 to avoid repetition, confirm\nthe convergence of our tuning process. Table 2 in the appendix shows a few example texts generated by the\ntuned model, confirming that EE-Tuning successfully converts a standard LLM into an early-exit one with\naccelerated inference. In the remaining experiments, we examine the performance of early exiting in a more\nsystematic manner.\n7\n5\n10\n15\n20\n25\n30\n35\n40\nLayer number\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\nLoss\nEmbedding\nNorm\nMLP\nLayer\nFigure 4: Training losses of all early exits at the end of EE-Tuning for various early-exit architectures.\n1.0\n1.5\n2.0\nSpeedup\n0.40\n0.45\n0.50\n0.55\nF1\nNarrativeQA\nEmbedding\nNorm\nMLP\nLayer\n1.0\n1.5\n2.0\nSpeedup\n0.18\n0.20\n0.22\n0.24\n0.26\nROUGE-L\nCNN/DailyMail\n1.00\n1.25\n1.50\n1.75\n2.00\nSpeedup\n0.20\n0.21\n0.22\n0.24\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\nSpeedup\n0.32\n0.40\n0.48\n0.56\n0.64\nF1\nNarrativeQA\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\n0.20\n0.30\n0.40\n0.50\nEM\nMMLU\nFigure 5: Downstream performance of our 13B models with various early-exit architectures. Points closer to\nthe top-right corner represent better performance (i.e. higher speedup and scores). Markers on each curve\ncorrespond to discrete values of the confidence threshold that we use for this experiment. Speedup increases\nfrom left to right as the threshold decreases, taking values in {1.0, 0.9, 0.8, 0.6, 0.4, 0.2}.\n3.2\nArchitectures of early-exit layers\nObservation: Regarding early-exit architectures,\n(1) an early-exit layer with more trainable parameters or located at a deeper position attains a lower\ntraining loss;\n(2) satisfactory inference speedup can be achieved together with comparable or even boosted scores, and\nMLP strikes the best overall balance between speed and quality.\nThis experiment compares the early-exit architectures introduced in Section 2.1. For layer normalization\ntherein, we follow Llama 2 [49] and use RMSNorm [55]. We add 8 early exits to the 40-layer 13B Llama 2-\nChat model, spaced evenly at the 1/8, 2/8, . . . , 8/8 depth of the Transformer backbone and initialized by\nthe copy method. The last early-exit layer, placed side-by-side to the original final-exit layer, is added only\nfor experimental purposes rather than practical usage.\nTraining losses.\nFigure 4 shows the training loss of each early exit at the end of EE-Tuning. We have\nobserved that tuning Embedding early-exit layers, which contain no normalization module, is prone to di-\nvergence, or high training losses when it does manage to converge, while tuning exits of the Norm ar-\nchitecture can effectively avoid the problem. Hence, we no longer consider Embedding in our remaining\nexperiments. Another observation is that in terms of early-exit training losses at the end of tuning, one has\nNorm > MLP > Layer. This confirms that having more trainable parameters in the early-exit layers leads\nto lower training losses. Finally, for each architecture, training losses of early exits at deeper layers are\nconsistently lower than those at earlier layers. This is unsurprising, since hidden states at deeper layers of\na neural network are expected to, on average, contain richer and more usable information about the input,\nwhich facilitates better prediction.\nInference quality and speedup.\nFigure 5 illustrates the downstream performance of early-exit models\nlearned by EE-Tuning with the aforementioned configurations. For each model, we activate only three early\nexits at the 1/4, 2/4, and 3/4 depth, which can be easily done thanks to the plug-and-play feature of our\nimplementation. On the tasks that we consider, the MLP model achieves the best overall performance in terms\n8\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 10\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 35\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\nFigure 6: Loss curves at two exits, initialized by either copy or random. See Figure 17 for the complete\nversion with loss curves for 8 exits.\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\nCopy\nRandom\n1.0\n1.5\n2.0\nSpeedup\n0.18\n0.20\n0.22\n0.24\n0.26\nROUGE-L\nCNN/DailyMail\n1.00\n1.25\n1.50\n1.75\n2.00\nSpeedup\n0.20\n0.21\n0.22\n0.24\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\n0.24\n0.32\n0.40\n0.48\n0.56\nEM\nMMLU\nFigure 7: Downstream performance of models with early-exit layers initialized by either copy or random.\nof scores and speedup, followed by the Norm model. Notably, the Layer model exhibits relatively weaker\ninference speedup, despite achieving the lowest training losses and highest scores in some tasks. This is most\nlikely due to the large inference latency of the early-exit layers themselves, as explained in Section 2.1.\nCareful readers might notice that a higher evaluation score and non-trivial speedup compared to standard\nfull-model inference can be achieved simultaneously in some tasks. While this might seem surprising, similar\nresults have been observed in prior works as well, and one reason is that early exiting can help to mitigate\noverthinking [22], i.e. early exits can make the correct prediction while the final exit makes mistakes for\ncertain inputs. Another possible reason in our case is that, while the original Llama 2-Chat models have\nbeen extensively fine-tuned for alignment with human preferences, the additional early-exit layers have only\nbeen tuned with the language modeling objective on pre-training data, which incurs less alignment tax and\nhence a slight advantage over the full-model output in certain benchmarks. Finally, regarding the risk of data\nleakage during EE-Tuning, we recall that our training data is a small subset of 1.3B tokens randomly sampled\nfrom commonly used open-source datasets for LLM pre-training [5]. Therefore, it is reasonable to assume\nthat the early-exit layers have seen very few, if any, samples of the evaluation tasks during EE-Tuning.\n3.3\nInitialization of early-exit layers\nObservation: copy incurs faster convergence and lower losses compared to random, while the final infer-\nence quality and speedup are similar in both cases.\nThis experiment compares two methods introduced in Section 2.1, namely copy and random, of initializing\nthe model parameters of early-exit layers. We follow the same setup as in Section 3.2, and add one case\nwhere MLP early-exit layers are initialized with normal random variables.\nFigure 6 demonstrates the training loss curve at each early exit for either method of initialization. We\nobserve that initialization by copy leads to much lower losses and faster convergence during the early stage\nof tuning (especially for exits at deeper layers), although training losses in the random cases will eventually\ncatch up, up to marginal gaps. While we fix the number of tuning iterations throughout our experiments for\nsimplicity, such results suggest that with copy, one might further improve training efficiency by terminating\nthe tuning process earlier, as soon as losses saturate. Figure 7 further shows that there is no significant\ndifference in downstream performance between models initialized by copy or random.\n9\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\n39800\n39850\n39900\n39950\n40000\n2.5\n3.0\n3.5\n4.0\nLayer 4\nLayer 8\nLayer 12\nLayer 16\nLayer 20\nLayer 24\nLayer 28\nLayer 32\n(a) 7B\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\nLoss\n39800\n39850\n39900\n39950\n40000\n2.5\n3.0\n3.5\n4.0\nLayer 5\nLayer 10\nLayer 15\nLayer 20\nLayer 25\nLayer 30\nLayer 35\nLayer 40\n(b) 13B\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n2\n4\n6\n8\n10\nLoss\n39800\n39850\n39900\n39950\n40000\n2.0\n2.4\n2.8\n3.2\nLayer 10\nLayer 20\nLayer 30\nLayer 40\nLayer 50\nLayer 60\nLayer 70\nLayer 80\n(c) 70B\nFigure 8: Training loss curves of EE-Tuning for models of sizes ranging from 7B to 70B.\n3.4\nEE-Tuning for models of various sizes\nObservation: EE-Tuning converges smoothly and attains 1.2\u00d7 to 1.6\u00d7 inference speedup without sac-\nrificing output quality for models of different sizes, and larger models achieve better speedup in general\n(except for MMLU).\nThis experiment validates the efficacy of EE-Tuning for LLMs of various sizes. While previous experi-\nments focus on 13B models, here we consider Llama 2-Chat models of sizes 7B, 13B or 70B. For each model,\nwe add 8 early exits with the MLP architecture, which are spaced evenly on the Transformer backbone, and\ninitialized by the copy method.\nFigure 8 shows the training loss curves of all exits for each model, confirming the convergence of\nEE-Tuning for models of various sizes. Unsurprising, larger models achieve lower training losses, and exits\nat deeper layers achieve lower losses within each model. Figure 9 further confirms that each model, with\nthree early exits at the 1/4, 2/4 and 3/4 depth activated, achieves early-exit speedup with comparable or\nsometimes higher scores than full-model inference in the downstream tasks that we consider.\n10\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.30\n0.40\n0.50\n0.60\nF1\nNarrativeQA\n7B\n13B\n70B\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.15\n0.17\n0.20\n0.23\n0.25\nROUGE-L\nCNN/DailyMail\n1.0\n1.5\n2.0\nSpeedup\n0.18\n0.20\n0.23\n0.25\n0.28\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.30\n0.40\n0.50\n0.60\nF1\nNarrativeQA\n1\n2\n3\nSpeedup\n0.30\n0.40\n0.50\n0.60\nEM\nMMLU\nFigure 9: Downstream performance of early-exit models of various sizes. For each model, early exits at\nthe 1/4, 2/4, and 3/4 depth are activated. Speedup increases from left to right as the confidence threshold\ndecreases, taking values in {1.0, 0.9, 0.8, 0.6, 0.4, 0.2}.\n3.5\nAdditional experiments\nWe have explored other aspects of EE-Tuning, including (1) different sources of training data for the tuning\nprocess; (2) choosing the best subset of tuned early exits to activate for inference; (3) downstream perfor-\nmance of sub-models induced by early exits; (4) differences between tuning with static or token-wise dynamic\nloss weights proposed in Section 2.3; and (5) potential benefits of continued pre-training with full-model pa-\nrameter updating after EE-Tuning is completed. The results can be found in Appendix A.\n4\nLimitations and future work\nEE-Tuning vs. joint training.\nIn EE-Tuning, early-exit layers are tuned while modules of the original\nstandard LLM are frozen. As a result, the tuning process is highly efficient, and has no impact on the\nfull-model output. The obvious disadvantage is that expressivity and adaptivity of early exits are limited,\nsince the Transformer backbone was originally trained to produce intermediate hidden states that are useful\nfor generating the full-model output, rather than early-exit output. In other words, the capability of early\nexits is inevitably constrained by the limited number of trainable parameters. When sufficient computational\nresources are available, a natural strategy to further improve the tuned early-exit model is joint learning\nof both network backbone and early exits, via full-parameter continued pre-training (CPT) or parameter-\nefficient fine-tuning like LoRA [20]. Indeed, our preliminary experimental result in Appendix A.4 confirms\nthat early-exit losses continue to decay smoothly during CPT. It would be interesting to see how training\nefficiency and downstream performance of EE-Tuning + CPT compare to those of pre-training from scratch,\nas well as understand how the learning dynamics in both cases differ.\nOther training objective.\nOne potential improvement for the EE-Tuning method is to use more general\ntraining objective, beyond the autoregressive language modeling loss on pre-training data. For example,\ngiven that the original standard LLM has been well pre-trained and fine-tuned, it can serve as the teacher\nin knowledge distillation [17], and supervise the training of early-exit layers using its own output logits\nas soft labels. This approach, sometimes called self-distillation [29, 56], has been adopted in some prior\nworks for training early-exit models from scratch, while we find it particularly appropriate for the setting\nof EE-Tuning. One might even consider using texts generated by the original LLM as training data for\nEE-Tuning, without relying on external pre-training data that is noisy and potentially contains harmful or\nundesirable contents. With such modifications, the tuned early exits might better inherit the knowledge and\nabilities of the original LLM.\nLimited configurations for experiments.\nWhile we have tried to make our experiments as extensive\nas we can afford, there are still many factors that were fixed throughout. For example, we have only tried\nout Llama 2-Chat models for initializing our early-exit LLMs, and used one inference mechanism (greedy\ndecoding and confidence-based exit condition) for all downstream evaluation. In addition, hyperparameters\nfor training are the same for models of various sizes, although it might be more reasonable to use larger batch\nsize and total number of tokens for tuning a 70B model than for a 7B model. Consequently, one should\nbe cautious about extrapolating observations and conclusions from our experiments to broader settings.\n11\nOutput quality and speed during early-exit inference might be further improved by better choices of training\nconfigurations or inference/decoding mechanisms.\nLack of fine-tuning for alignment.\nOur early-exit models were initialized with Llama 2-Chat models,\nwhich have been fine-tuned for alignment with human preferences [49]. The early-exit layers, however, were\ntuned only with pre-training data and language modeling losses. We conjecture that the tuned early-exit\nmodels preserve most of the alignment properties of the original Llama 2-Chat models, due to the relatively\nsmall number of model parameters in early-exit layers. With that said, we have not empirically evaluated\nrelevant metrics, such as helpfulness and safety, of our early-exit models. Caution must be taken before\ndeploying these models, and an extra fine-tuning stage for better alignment might be helpful.\n5\nConclusions\nThis work has provided a unified and systematic study of EE-Tuning, a lightweight and economical approach\nto converting any existing LLM into an early-exit LLM in a parameter-efficient manner. Our implementation\nof EE-Tuning is well optimized for maximum computational efficiency, and also highly scalable thanks to\nits compatibility with massive 3D parallelism. Results of extensive experiments have validated that, with\nnegligible training costs compared to full-parameter training, EE-Tuning successfully returns early-exit LLMs\nthat achieve outstanding speedup with minor or no degeneration of output quality during the inference phase.\nIt is our hope that this work will make early-exit LLMs more accessible to the community.\nA\nAdditional experiments\nThis section includes additional experiments and empirical results for EE-Tuning.\nA.1\nTraining data for EE-Tuning\nThis experiment explores the impacts of the training data used for EE-Tuning. Similar to previous exper-\niments, we consider a 13B model with 8 MLP early exits evenly spaced on the Transformer backbone and\ninitialized by the copy method. The only difference is that, instead of the pre-training data used in previous\nexperiments, here we use the instruction fine-tuning (IFT) data4 provided by Data-Juicer [5], which is a\nrefined subset of the Alpaca-CoT dataset [44], for the tuning process. Generally speaking, the pre-training\ndata is more diverse, while the IFT data is cleaner and more structured, organized in a query-answer format.\nFigure 10 shows the training losses at different early exits, both at the beginning and at the end of the\ntuning process. Compared with pre-training data, using IFT data incurs higher losses initially, but then\nlower losses after tuning is completed. This is possible because the early exits quickly adapt to the format\nand style of the IFT data during tuning; thereafter, lower losses can be achieved since the IFT data is cleaner,\nmore structured, and less diverse.\nFigure 11 illustrates the downstream performance of models with various early-exit architectures. It is\nsimilar to Figure 5, except that tuning is done with IFT rather than pre-training data. We further take the\ncurves from both figures corresponding to the MLP architecture, and compare them side by side in Figure 12,\nwhich shows that the model tuned with pre-training data indeed outperforms the one tuned with IFT data.\nTable 3 in the appendix demonstrates some example texts generated by both models. We notice that the\nmodel tuned with IFT data sometimes uses undesirable formats in its output instead of strictly following the\nformats of in-context examples, which could possibly be caused by overfitting to IFT data to some extent.\nThese results suggest that it might be more reasonable to first use pre-training data for EE-Tuning, which\nallows the added early exits to acquire general language abilities; thereafter, IFT data can be used for further\nfine-tuning.\n4https://huggingface.co/datasets/datajuicer/alpaca-cot-en-refined-by-data-juicer\n12\n10\n20\n30\n40\nLayer number\n3.00\n4.50\n6.00\n7.50\nLoss\nInitial loss\nPretrain Data\nIFT Data\n10\n20\n30\n40\nLayer number\n2.40\n2.80\n3.20\n3.60\nLoss\nFinal loss\nPretrain Data\nIFT Data\nFigure 10: Training losses of MLP early exits at the beginning (left) or end (right) of the tuning process with\neither pre-training or IFT data.\n1.0\n1.5\n2.0\nSpeedup\n0.40\n0.45\n0.50\n0.55\nF1\nNarrativeQA\nEmbedding\nNorm\nMLP\nLayer\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.16\n0.18\n0.20\n0.21\nROUGE-L\nCNN/DailyMail\n1.0\n1.5\n2.0\nSpeedup\n0.16\n0.18\n0.20\n0.22\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\nSpeedup\n0.40\n0.45\n0.50\n0.55\nF1\nNarrativeQA\n1\n2\n3\nSpeedup\n0.24\n0.32\n0.40\n0.48\n0.56\nEM\nMMLU\nFigure 11:\nDownstream performance of our 13B models with various early-exit architectures, tuned\nwith IFT data.\nFor each curve, the confidence threshold decreases from left to right, taking values in\n{1.0, 0.9, 0.8, 0.6, 0.4, 0.2}.\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\nPretrain Data\nIFT Data\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.18\n0.20\n0.23\n0.25\nROUGE-L\nCNN/DailyMail\n1.00\n1.25\n1.50\n1.75\n2.00\nSpeedup\n0.16\n0.18\n0.20\n0.22\n0.24\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\n0.24\n0.32\n0.40\n0.48\n0.56\nEM\nMMLU\nFigure 12: A side-by-side comparison between the downstream performance of early-exit LLMs tuned with\npre-training or IFT data. The results here are duplication of the curves in Figures 5 and 11 corresponding\nto the MLP architecture.\n13\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.30\n0.40\n0.50\n0.60\nF1\nNarrativeQA\n7B\n13B\n70B\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n0.21\n0.23\n0.24\n0.26\nROUGE-L\nCNN/DailyMail Score\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n0.22\n0.24\n0.26\n0.28\nROUGE-L\nXSUM Score\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n0.54\n0.56\n0.58\n0.60\n0.62\nF1\nNarrativeQA Score\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n0.48\n0.52\n0.56\n0.60\n0.64\nEM\nMMLU Score\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n1.05\n1.20\n1.35\n1.50\nSpeedup\nCNN/DailyMail Speedup\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n1.04\n1.12\n1.20\n1.28\n1.36\nSpeedup\nXSUM Speedup\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n1.04\n1.12\n1.20\n1.28\n1.36\nSpeedup\nNarrativeQA Speedup\n0.2\n0.4\n0.6\n0.8\nRelative depth of EE\n0.90\n1.00\n1.10\n1.20\n1.30\nSpeedup\nMMLU Speedup\nFigure 13: Scores (top) and speedup (bottom) in downstream tasks, with different choices of activating one\nsingle early exit. The confidence threshold is fixed at 0.8.\n1.0\n1.5\n2.0\n2.5\nSpeedup\n0.30\n0.40\n0.50\n0.60\nF1\nNarrativeQA\n7B\n13B\n70B\n0.4\n0.6\n0.8\n1.0\nRelative depth of EE\n0.05\n0.10\n0.15\n0.20\n0.25\nROUGE-L\nCNN/DailyMail Score\n0.4\n0.6\n0.8\n1.0\nRelative depth of EE\n0.12\n0.16\n0.20\n0.24\n0.28\nROUGE-L\nXSUM Score\n0.4\n0.6\n0.8\n1.0\nRelative depth of EE\n0.15\n0.30\n0.45\n0.60\nF1\nNarrativeQA Score\n0.4\n0.6\n0.8\n1.0\nRelative depth of EE\n0.30\n0.40\n0.50\n0.60\nEM\nMMLU Score\nFigure 14: Downstream performance of standard inference with sub-models induced by early exits.\nA.2\nSupplementary results for Section 3.4\nBelow are additional empirical results for our previous experiment in Section 3.4 that validates the efficacy\nof EE-Tuning for models of various sizes.\nChoosing which early exit(s) to activate.\nGiven multiple tuned early exits, one might wonder which\nsubset should be activated for the best inference performance. For a preliminary exploration, we restrict\nourselves to activating only one single early exit. Scores and speedup in downstream tasks for each option\ncan be found in Figure 13. The key trade-off here is that, early exits at deeper layers generally have higher\ncapability, but also larger inference latency. Based on our empirical results, the sweet spot seems to vary case\nby case, and thus we recommend choosing the subset of activated early exits via standard hyperparameter\noptimization on a validation set before deployment.\nBy-products: sub-models induced by early exits.\nRecall from Section 2.1 that the sub-model induced\nby each early exit, which includes the modules covered by a forward pass from the beginning of the network to\nthe output of the early exit, can be regarded as a standard Transformer model with fewer Transformer layers\nthan the original full model. Such sub-models might be deployed for standard LLM inference, which can also\nbe regarded as a simplified mechanism of early-exit inference, namely using the same pre-specified early exit\nfor generating all tokens of a sequence. Indeed, empirical results in Figure 14 confirm that these sub-models,\nespecially those corresponding to early exits at deep layers, performs reasonably well in downstream tasks.\nA.3\nDynamic token-wise loss weighting\nThis section provides a preliminary exploration of using dynamic token-wise loss weights in EE-Tuning,\nas explained in Section 2.3. More specifically, let us denote the model parameters at the current training\niteration as \u03b8; in addition, consider a sequence of tokens x = [x1, x2, . . . , xT ] in the data batch, and let\nc = [c1, c2, . . . , cT ] \u2208 [0, 1]T be the confidence values (i.e. the maximum probabilities of next-token prediction)\n14\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\nStatic weight\nDynamic weight\n1.0\n1.5\n2.0\nSpeedup\n0.18\n0.20\n0.22\n0.24\n0.26\nROUGE-L\nCNN/DailyMail\n1.00\n1.25\n1.50\n1.75\n2.00\nSpeedup\n0.20\n0.21\n0.22\n0.24\nROUGE-L\nXSUM\n1.0\n1.5\n2.0\nSpeedup\n0.45\n0.50\n0.55\n0.60\nF1\nNarrativeQA\n1.0\n1.5\n2.0\n2.5\n3.0\nSpeedup\n0.24\n0.32\n0.40\n0.48\n0.56\nEM\nMMLU\nFigure 15: Downstream performance of models tuned with constant or dynamic loss weights.\n0\n1000\n2000\n3000\n4000\n5000\n6000\nIteration\n2.0\n2.5\n3.0\n3.5\nLoss\n5600\n5800\n6000\n1.6\n2.0\n2.4\n2.8\nLayer 8\nLayer 16\nLayer 32\nFigure 16: Convergence of training losses during CPT after EE-Tuning is completed.\ncalculated at a certain early exit during the forward pass, which are detached from the computational graph\nand thus regarded as constants. Then, the training loss for this sequence at that early exit is defined as\nL(x; \u03b8) := \u2212\nX\nt\u2208[T ]\nct \u00b7 log P(xt|x1, . . . , xt\u22121; \u03b8).\nIn other words, the negative log-likelihood for each token is weighted by the corresponding confidence value.\nIdeally, this method encourages each early-exit layer to learn to predict tokens that are within its capability.\nTo verify the effectiveness of this method, we conduct an experiment based on the intermediate checkpoint\nof the 13B MLP model in Section 3.2, which has been tuned on half of the training data. We continue the\ntuning process on the remaining half of the data with dynamic token-wise loss weighting enabled, while\nother hyperparameters remain unchanged. Figure 15 compares the downstream performance of the model\nobtained by the above method and the original model that was tuned using constant loss weights throughout.\nUnfortunately, we see no obvious difference between them. This may be caused by (i) the short period of\ntraining with dynamic weighting; (ii) the small learning rate in the second half of training; or (iii) the small\nnumber of trainable parameters.\nThe real reason behind this, and the right way to achieve gains from\ndynamic weighting, need to be further investigated in future works.\nA.4\nContinued pre-training (CPT) after EE-Tuning\nTo understand whether full-parameter CPT can further improve the early-exit LLMs obtained via EE-Tuning,\nwe perform CPT for our 7B early-exit model using the scripts for pre-training provided by EE-LLM [6], except\nthat a smaller learning rate is used. Training loss curves for two early exits and the final exit can be found in\nFigure 16. Interestingly, we observe that the early-exit losses continue to decay smoothly, while the final-exit\nloss drops quickly during the first few iterations, and then remains constant. Such results suggest that CPT\nmight further improve the early exits, without negatively impacting the full-model output.\n15\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 5\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 10\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 15\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 20\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 25\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 30\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 35\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000\nIteration\n4\n6\n8\n10\nLoss\nLayer 40\n0\n500\n1000\n1500\n2000\n2500\n3000\n5\n10\nCopy\nRandom\nFigure 17: Training loss curves at each exit, initialized by either copy or random.\n16\nTable 2: Example texts (and the corresponding inference latencies) generated by a 13B or 70B early-exit\nLLM having one MLP early exit at the 1/4 depth of its Transformer backbone. Differences from the texts\ngenerated by full-model inference are highlighted.\nPrompt\nArtificial General Intelligence is\n13B\nFull model\nTime: 4.8 s\nthe holy grail of AI research. It is the ability of an AI system to perform any intellectual task that\na human can, and to do so at a level of performance that is indistinguishable from a human. AGI\nwould be a system that can learn, reason, and self-improve at a pace that is exponentially faster\nthan human intelligence.\nThere are many challenges to achieving AGI, including:\n1. Lack of\n13B\nThreshold = 0.8\nTime: 4.2 s\nthe holy grail of AI research. It is the ability to create a machine that can perform any intellectual\ntask that a human can, and potentially even surpass human-level performance. However, achieving\nAGI is a daunting task, and many experts believe that it is still decades away. In this article, we\nwill explore the current state of AGI research, the challenges that researchers face, and the potential\nimplications of achieving AGI.\n13B\nThreshold = 0.6\nTime: 3.9 s\nthe holy grail of AI research. It is the ability to create a machine that can perform any intellectual\ntask that a human can, and potentially even surpass human-level performance. However, achieving\nAGI is a daunting task, and many experts believe that it is still decades away.\nOne of the main challenges in achieving AGI is the problem of \"explainability.\" As AI systems\nbecome more complex and sophisticated, it\n70B\nFull model\nTime: 23.6 s\na hypothetical AI that possesses the ability to understand, learn, and apply knowledge across a wide\nrange of tasks, much like human intelligence. While we have made significant progress in developing\nAI systems that can perform specific tasks, such as image recognition, natural language processing,\nand autonomous driving, we are still far from achieving true AGI.\nOne of the main challenges in developing AGI is the lack of understanding of human intelligence.\nWhile we have made significant progress in\n70B\nThreshold = 0.8\nTime: 19.7 s\na hypothetical AI that possesses the ability to understand, learn, and apply knowledge across a wide\nrange of tasks, much like human intelligence. While we have made significant progress in developing\nAI systems that can perform specific tasks, such as image recognition, natural language processing,\nand autonomous driving, we are still far from achieving true AGI.\nOne of the main challenges in developing AGI is the lack of understanding of human intelligence.\nWhile we have made significant progress in\n70B\nThreshold = 0.6\nTime: 17.1 s\na hypothetific AI that possesses the ability to understand, learn, and apply knowledge across a wide\nrange of domains and tasks. It is an AI that can perform any intellectual task that a human can.\nArtificial Intelligence is a broader term that refers to the development of computer systems that can\nperform tasks that typically require human intelligence, such as visual perception, speech recogni-\ntion, decision making, and language translation.\nArtificial Intelligence can be applied\n17\nTable 3: Example texts generated by 13B early-exit LLMs tuned with pre-training or IFT data, for the same\nrequest from the CNN/DailyMail summarization task. Note that for evaluation with HELM in this task,\nonly the first paragraph of each generated response is used for calculating the score.\nPrompt\n{5-shot examples, each with an article and a summarization with three sentences in one paragraph.\nOmitted for brevity.}\n###\nArticle:\nManchester City playmaker David Silva has returned to training, the club have reported.\nSilva\nlooked to have been seriously injured on Sunday when he was caught in the face by an elbow from\nWest Ham\u2019s Cheikhou Kouyate. The Spain international received around eight minutes of treatment\non the field at the Etihad Stadium before being carried off on a stretcher and taken to hospital for\nexamination. . . . {Omitted for brevity.} Captain Vincent Kompany is out with a groin injury and\nmight not be fit to return before the end of the season. Midfielder James Milner, defender Gael Clichy\nand strikers Wilfried Bony and Stevan Jovetic were also sidelined for the 2-0 win over the Hammers.\nVincent Kompany (right) was forced to watch from the stands as he continues his recovery from injury .\nSummarize the above article in 3 sentences:\nFull model\n(Llama 2-Chat)\nTime: 8.6 s\nDavid Silva has returned to training with Manchester City . The playmaker was injured in a challenge\nwith West Ham\u2019s Cheikhou Kouyate on Sunday . Silva will be monitored this week, but his return is\na boost for manager Manuel Pellegrini . # 2018 in Malaysia\n2018 in Malaysia was marked by several significant events and developments.\nHere are some\nof the key highlights:\n1.\nEE tuned with\npre-training data\nThreshold=0.8\nTime: 7.2 s\nDavid Silva has returned to training with Manchester City . The playmaker was injured in a challenge\nwith West Ham\u2019s Cheikhou Kouyate on Sunday . Silva will be monitored this week, but his return is\na boost for manager Manuel Pellegrini .\n###\nArticle:\n(CNN) \u2013 The world\u2019s largest waterfall,\nby volume,\nis not Niagara or Victoria Falls,\nbut a\nlittle-known cascade in\nEE tuned with\npre-training data\nThreshold=0.4\nTime: 5.5 s\nManchester City midfielder David Silva has returned to training, the club have reported. Silva was\ninjured in the first half of the match against West Ham, but tests revealed the 29-year-old had\nsuffered no fractures and the club are hopeful he could make swift return to action.\nThe Spain\ninternational will continue to be monitored this week.\n###\nArticle:\n(CNN) \u2013 The world\u2019s largest waterfall,\nEE tuned with\nIFT data\nThreshold=0.8\nTime: 7.1 s\nDavid Silva has returned to training with Manchester City . The playmaker was injured in a challenge\nwith West Ham\u2019s Cheikhou Kouyate on Sunday . Silva will be monitored this week, but his return is\na boost for manager Manuel Pellegrini . # 2018 in Malaysia\nEvents in the year 2018 in Malaysia.\n### January\n* 1 January - New Year\u2019s E\nEE tuned with\nIFT data\nThreshold=0.4\nTime: 5.7 s\nManchester City midfielder David Silva has returned to training after being injured in a match\nagainst West Ham .\nSilver suffered a facial injury when he was hit by a elbow from West Ham\u2019s Cheikhou Kouy-\nate .\nSilver\u2019s return is a boost for manager Manuel Pellegrini, who has several options out injured\n.\n###\nArticle:\n(CNN) \u2013 The world\u2019s largest water\n18\nReferences\n[1] Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,\n2016.\n[2] Sangmin Bae, Jongwoo Ko, Hwanjun Song, and Se-Young Yun. Fast and robust early-exiting framework\nfor autoregressive language models with synchronized parallel decoding. ArXiv, abs/2310.05424, 2023.\n[3] Arian Bakhtiarnia, Qi Zhang, and Alexandros Iosifidis. Improving the accuracy of early exits in multi-\nexit architectures via curriculum learning. In IJCNN, 2021.\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-\nVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. Language models are few-shot learners. In NeurIPS, 2020.\n[5] Daoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen, Xuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie,\nZhaoyang Liu, Jinyang Gao, Yaliang Li, Bolin Ding, and Jingren Zhou. Data-juicer: A one-stop data\nprocessing system for large language models. ArXiv, abs/2309.02033, 2023.\n[6] Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, and Jingren Zhou. Ee-llm: Large-scale training and\ninference of early-exit large language models with 3d parallelism. ArXiv, abs/2312.04916, 2023.\n[7] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepa-\nssi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana\nPillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language\nmodeling with pathways. J. Mach. Learn. Res., 24:240:1\u2013240:113, 2023.\n[8] Luciano Del Corro, Allison Del Giorno, Sahaj Agarwal, Ting Yu, Ahmed Hassan Awadallah, and Sub-\nhabrata Mukherjee. Skipdecode: Autoregressive skip decoding with batching and caching for efficient\nllm inference. ArXiv, abs/2307.02628, 2023.\n[9] Yinwei Dai, Rui Pan, Anand Iyer, Kai Li, and Ravi Netravali. Apparate: Rethinking early exits to\ntame latency-throughput tensions in ml serving. ArXiv, abs/2312.05385, 2023.\n[10] Rahul Duggal, Scott Freitas, Sunny Dhamnani, Duen Horng Chau, and Jimeng Sun. Elf: An early-\nexiting framework for long-tailed classification. ArXiv, abs/2006.11979, 2020.\n[11] Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael Auli. Depth-adaptive transformer. In ICLR,\n2020.\n[12] Ariel Gera, Roni Friedman, Ofir Arviv, Chulaka Gunasekara, Benjamin Sznajder, Noam Slonim, and\nEyal Shnarch. The benefits of bad advice: Autocontrastive decoding across model layers. In ACL, 2023.\n[13] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016.\n[14] Yizeng Han, Gao Huang, Shiji Song, Le Yang, Honghui Wang, and Yulin Wang.\nDynamic neural\nnetworks: A survey. IEEE Trans. Pattern Anal. Mach. Intell., 44(11):7436\u20137456, 2022.\n19\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, pages 770\u2013778, 2016.\n[16] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. In ICLR, 2020.\n[17] Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network.\nArXiv, abs/1503.02531, 2015.\n[18] Lu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao Chen, and Qun Liu. Dynabert: Dynamic BERT\nwith adaptive width and depth. In NeurIPS, 2020.\n[19] Boren Hu, Yun Zhu, Jiacheng Li, and Siliang Tang. Smartbert: A promotion of dynamic early exiting\nmechanism for accelerating bert inference. In IJCAI, 2023.\n[20] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022.\n[21] Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, and Kilian Q. Weinberger.\nMulti-scale dense networks for resource efficient image classification. In ICLR, 2018.\n[22] Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\nShallow-deep networks: Understanding and\nmitigating network overthinking. In ICML, volume 97, pages 3301\u20133310, 2019.\n[23] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014.\n[24] Tom\u00e1s Kocisk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis,\nand Edward Grefenstette. The narrativeqa reading comprehension challenge. Trans. Assoc. Comput.\nLinguistics, 6:317\u2013328, 2018.\n[25] Vijay Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence C. McAfee, Michael Andersch, Mo-\nhammad Shoeybi, and Bryan Catanzaro. Reducing activation recomputation in large transformer mod-\nels. ArXiv, abs/2205.05198, 2022.\n[26] Stefanos Laskaridis, Alexandros Kouris, and Nicholas D. Lane. Adaptive inference through early-exit\nnetworks: Design, challenges and directions. In EMDL@MobiSys, pages 1\u20136. ACM, 2021.\n[27] Xiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan, Xipeng Qiu, and Xuanjing Huang. Accelerating\nbert inference for sequence labeling via early-exit. In ACL, 2021.\n[28] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u2019e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel\nGuha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation\nof language models. Annals of the New York Academy of Sciences, 1525:140 \u2013 146, 2023.\n[29] Weijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao, Haotang Deng, and Qi Ju. Fastbert: a self-distilling\nBERT with adaptive inference time. In ACL, pages 6035\u20136044, 2020.\n[30] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don\u2019t give me the details, just the summary!\ntopic-aware convolutional neural networks for extreme summarization. In EMNLP, pages 1797\u20131807,\n2018.\n[31] Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, Amar Phanishayee,\nand Matei Zaharia. Efficient large-scale language model training on GPU clusters using megatron-lm.\nIn SC, page 58, 2021.\n20\n[32] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[33] Priyadarshini Panda, Abhronil Sengupta, and Kaushik Roy.\nConditional deep learning for energy-\nefficient and enhanced pattern recognition. In 2016 Design, Automation & Test in Europe Conference\n& Exhibition (DATE), pages 475\u2013480, 2015.\n[34] Ofir Press and Lior Wolf. Using the output embedding to improve language models. In EACL, pages\n157\u2013163, 2017.\n[35] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understand-\ning by generative pre-training, 2018.\n[36] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners, 2019.\n[37] Florence Regol, Joud Chataoui, and Mark Coates. Jointly-learned exit and inference for a dynamic\nneural network : Jei-dnn. ArXiv, abs/2310.09163, 2023.\n[38] Simone Scardapane, Michele Scarpiniti, Enzo Baccarelli, and Aurelio Uncini. Why should we add early\nexits to neural networks? Cognitive Computation, 12:954 \u2013 966, 2020.\n[39] Tal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani, Dara Bahri, Vinh Tran, Yi Tay, and Donald\nMetzler. Confident adaptive language modeling. In NeurIPS, 2022.\n[40] Tal Schuster, Adam Fisch, Tommi S. Jaakkola, and Regina Barzilay. Consistent accelerated inference\nvia confident adaptive transformers. In EMNLP, pages 4962\u20134979, 2021.\n[41] Roy Schwartz, Gabriel Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah A. Smith. The right\ntool for the job: Matching model and instance complexities. In ACL, pages 6640\u20136651, 2020.\n[42] Abigail See, Peter J. Liu, and Christopher D. Manning. Get to the point: Summarization with pointer-\ngenerator networks. In ACL, pages 1073\u20131083, 2017.\n[43] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. ArXiv,\nabs/1909.08053, 2019.\n[44] Qingyi Si, Tong Wang, Zheng Lin, Xu Zhang, Yanan Cao, and Weiping Wang. An empirical study of\ninstruction-tuning large language models in chinese. ArXiv, abs/2310.07328, 2023.\n[45] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Anand Korthikanti, Elton Zhang, Rewon\nChild, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael\nHouston, Saurabh Tiwary, and Bryan Catanzaro. Using deepspeed and megatron to train megatron-\nturing nlg 530b, a large-scale generative language model. ArXiv, abs/2201.11990, 2022.\n[46] Shengkun Tang, Yaqing Wang, Caiwen Ding, Yi Liang, Y. Li, and Dongkuan Xu. Deediff: Dynamic\nuncertainty-aware early exiting for accelerating diffusion model generation.\nArXiv, abs/2309.17074,\n2023.\n[47] Surat Teerapittayanon, Bradley McDanel, and H. T. Kung. Branchynet: Fast inference via early exiting\nfrom deep neural networks. In ICPR, pages 2464\u20132469, 2016.\n[48] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models.\nArXiv, abs/2302.13971, 2023.\n21\n[49] Hugo Touvron, Louis Martin, Kevin R. Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cant\u00f3n Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel M. Kloumann,\nA. V. Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich,\nYinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, R. Subramanian, Xia Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan\nNarang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open\nfoundation and fine-tuned chat models. ArXiv, abs/2307.09288, 2023.\n[50] Neeraj Varshney, Agneet Chatterjee, Mihir Parmar, and Chitta Baral. Accelerating llama inference by\nenabling intermediate layer decoding via instruction tuning with lite. ArXiv, abs/2310.18581, 2023.\n[51] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, pages 5998\u20136008, 2017.\n[52] Ji Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin. Deebert: Dynamic early exiting for\naccelerating BERT inference. In ACL, pages 2246\u20132251, 2020.\n[53] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\nBerxit: Early exiting for BERT with better\nfine-tuning and extension to regression. In EACL, pages 91\u2013104, 2021.\n[54] Canwen Xu and Julian McAuley. A survey on dynamic neural networks for natural language processing.\nIn EACL, pages 2370\u20132381, 2023.\n[55] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, pages 12360\u201312371,\n2019.\n[56] Linfeng Zhang, Chenglong Bao, and Kaisheng Ma. Self-distillation: Towards efficient and compact\nneural networks. IEEE Trans. Pattern Anal. Mach. Intell., 44(8):4388\u20134403, 2022.\n[57] Wangchunshu Zhou, Canwen Xu, Tao Ge, Julian J. McAuley, Ke Xu, and Furu Wei.\nBERT loses\npatience: Fast and robust inference with early exit. In NeurIPS, 2020.\n22\n"
  }
]