[
  {
    "title": "Long-range Language Modeling with Self-retrieval",
    "link": "https://arxiv.org/pdf/2306.13421.pdf",
    "upvote": "15",
    "text": "Long-range Language Modeling with Self-retrieval\nOhad Rubin\nJonathan Berant\nThe Blavatnik School of Computer Science, Tel Aviv University\n{ohad.rubin,joberant}@cs.tau.ac.il\nAbstract\nRetrieval-augmented language models (LMs)\nhave received much attention recently. How-\never, typically the retriever is not trained jointly\nas a native component of the LM, but added to\nan already-pretrained LM, which limits the abil-\nity of the LM and the retriever to adapt to one\nanother. In this work, we propose the Retrieval-\nPretrained Transformer (RPT), an architecture\nand training procedure for jointly training a\nretrieval-augmented LM from scratch for the\ntask of modeling long texts. Given a recently\ngenerated text chunk in a long document, the\nLM computes query representations, which are\nthen used to retrieve earlier chunks in the doc-\nument, located potentially tens of thousands\nof tokens before. Information from retrieved\nchunks is fused into the LM representations\nto predict the next target chunk. We train the\nretriever component with a semantic objective,\nwhere the goal is to retrieve chunks that in-\ncrease the probability of the next chunk, ac-\ncording to a reference LM. We evaluate RPT on\nfour long-range language modeling tasks, span-\nning books, code, and mathematical writing,\nand demonstrate that RPT improves retrieval\nquality and subsequently perplexity across the\nboard compared to strong baselines.\n1\nIntroduction\nLarge language models (LMs) have had immense\nsuccess recently (Brown et al., 2020; Chowdhery\net al., 2022; Zhang et al., 2022; Touvron et al.,\n2023), becoming a useful tool across disciplines.\nHowever, their success comes at a computational\ncost, due to increasing parameter counts for storing\nworld knowledge and growing context lengths that\nenable access to distant information, but incur a\nquadratic complexity penalty. Retrieval-augmented\nlanguage modeling (RALM) alleviates this cost\n(Khandelwal et al., 2020; Yogatama et al., 2021;\nLexically Similar\nBook or\nLong text\nSemantically Similar\nRetrieve\n\u22ee\nFuse\n\u22ee\nTraining Signal\nP\u03b8 (\u2026\u2026....|\u2026\u2026\u2026\u2026\u2026...)\n\ud835\udc50!\" \n\ud835\udc50#$! \n\ud835\udc50#$# \n>\nP\u03b8 (\u2026\u2026....|\u2026\u2026\u2026\u2026\u2026...)\n\ud835\udc50!$$ \n\ud835\udc50#$! \n\ud835\udc50#$# \nChunk 100\nChunk 13\nPast States\nPredict\nCausal \nLanguage \nModel\nChunk 201\nChunk 202\nQuery\nTarget\nInput\nRef\nRef\nThe killer left a room \nfull of evidence, a \npuzzle for forensics.\nAs a kid, Lt. John\nfound a dead dog; \nsince then, crimson \nalways unnerved\nhim.\nLt.  John looked \naround, \"Another \nvictim, The Crimson \nMurderer strikes \nagain.\"\n\u201cI bet the forensic \nguys  would love \nthis.\u201d\nFigure 1: Retrieval-Pretrained Transformer (RPT) is a\nlanguage model for long texts (e.g., books) trained from\nscratch with a native retrieval ability. RPT takes a chunk\nof text as input, retrieves semantically-relevant chunks\nfrom the past to better predict the next chunk, and fuses\nthese retrieved chunks into its representations. On top\nof a standard LM loss, the retriever is trained to retrieve\nchunks that increase the probability of the next chunk\naccording to a reference LM.\nBorgeaud et al., 2022; Ram et al., 2023), as pre-\ncise retrieval of relevant information can reduce\nmemory and computation requirements.\nMore-\nover, RALM is beneficial for factuality, freshness\nand generalization without necessitating retraining,\nsimply by swapping the retrieval index (Guu et al.,\n2020; Lewis et al., 2020; Huang et al., 2023).\nHowever, past work on RALM has by and large\nnot trained the retriever as a first-class component\nof the LM. In some cases (Khandelwal et al., 2020;\nYogatama et al., 2021; Borgeaud et al., 2022), the\nretriever was used only at test time, or remained\narXiv:2306.13421v1  [cs.CL]  23 Jun 2023\nfixed throughout training, preventing it from adapt-\ning to the LM generator. In other cases, the re-\ntriever component was jointly trained but only after\na separate pretraining phase for both the retriever\nand LM (Sachan et al., 2021; Izacard et al., 2022;\nJiang et al., 2022; Bertsch et al., 2023). Thus, the\nretriever was not pre-trained from scratch with the\nLM, and only a fraction of the training budget was\nallocated for joint training.\nRecently, Zhong et al. (2022) presented a\nretrieval-augmented LM that trains a retriever from\nscratch jointly with the LM, but (a) the retriever\nwas trained to exploit lexical information only, and\n(b) the retrieved information was not fused at the\nrepresentation level back into the LM.\nIn this work, we present the Retrieval-Pretrained\nTransformer (RPT), a retrieval-augmented LM,\nwhere the retriever is a first-class component,\ntrained jointly from scratch with the LM. RPT re-\nlies on two technical contributions. First, on the\narchitecture side (see Fig. 1), input representations\nfor the retriever are computed from the LM repre-\nsentations themselves (which we dub self-retrieval),\nand retrieved representations are fused back into\nthe LM decoder for making next word predictions.\nSecond, we train the retriever with an auxiliary\nloss function that encourages retrieving text frag-\nments that increase the probability of generating\nthe subsequent text. Specifically, given a recently-\ngenerated chunk ct, the retriever is trained to re-\ntrieve chunks ci that increase the probability of\npscoring(ct+1 | ci, ct) according to a reference scor-\ning LM. Fig. 1 provides an illustrative example for\na case where a crime scene is described, and a scor-\ning LM shows the benefit of retrieving a chunk\nthousands of tokens away (chunk 13) compared to\nlexical retrieval, which leads to a chunk that is only\nsuperficially related (chunk 100).\nWe focus on the problem of modeling long doc-\numents, such as books, articles, code, scripts, and\ndialogue, since these are naturally occurring exam-\nples of long-form content, where the entire index\ncan be held within memory in a forward-pass. We\nevaluate RPT on four language modeling tasks and\nfind that it improves perplexity across all tasks, out-\nperforming prior work (Hutchins et al., 2022; Wu\net al., 2022) as well as strong baselines (Borgeaud\net al., 2022; Zhong et al., 2022). Moreover, we\nshow that RPT retrieves high-quality chunks com-\npared to retrievers that rely on lexical information.\nBased on our empirical findings, we argue RPT\ncan pave the way toward the next generation of pre-\ntrained LMs, where retrieval is strongly embedded\nwithin the architecture and training procedure.\n2\nBackground\nTo situate our contribution, we review relevant re-\ncent RALM work. We extend this to more related\nwork in \u00a76.\nEarly work on RALMs, such as kNN-LM (Khan-\ndelwal et al., 2020) used retrieval to improve lan-\nguage modeling by interpolating the next-word dis-\ntribution produced by the LM with a distribution\nproposed through a test-time-only retrieval mecha-\nnism. Borgeaud et al. (2022) later proposed Chun-\nked Cross-Attention (CCA), where retrieval is per-\nformed also at training time, and retrieved repre-\nsentations are deeply fused into the representations\nproduced by a Transformer decoder through atten-\ntion. However, the retriever was trained separately\nand kept fixed during training, which prevented it\nfrom adapting to the LM over the course of train-\ning.\nTRIME (Zhong et al., 2022), like this work,\ntrained a retrieval-augmented LM from scratch\nwhere the retriever component and the decoder LM\nare trained jointly. Our work differs from TRIME\nin two aspects: First, TRIME, like kNN-LM, incor-\nporates information from the retriever in a shallow\nmanner through distribution interpolation, while\nwe adopt CCA as a deeper fusion mechanism. Sec-\nond, TRIME takes advantage of lexical clues for\nsupervising the retriever, that is, given a query, the\nTRIME retriever learns to retrieve contexts that\nwill lead to generating the same token as the query.\nWe, on the other hand, use a scoring LM to evalu-\nate what text chunks are relevant for increasing the\nprobability of the chunk being generated, which\nleads to more semantic retrieval. This is similar to\nEPR (Rubin et al., 2022), which used this idea for\nlearning to retrieve prompts for in-context learning,\nand perplexity distillation in Atlas (Izacard et al.,\n2022). However, Atlas does not train the retriever\nand LM from scratch and is an encoder-decoder\nmodel, more suitable for knowledge-intensive tasks.\nWe, conversely, train from scratch and use a de-\ncoder model, more suitable for modeling long texts.\n3\nRetrieval-Pretrained Transformer\nProblem Setup\nRPT, like RETRO (Borgeaud\net al., 2022), is a chunk-wise retrieval-augmented\nLM, where the input sequence is divided into\nCausal Attention\nFeed Forward\nChunked Cross Attention\nTop-K\nQ\nK\nV\nCausal Attention\nPool + Project\nFeed Forward\nBi-directional\nAttention\n\u27e8||||||||| \u22c5 aaaa  = 7.1\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\u27e8||||||||| \u22c5 aaaa  = 0.3\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc502\n\u27e8||||||||| \u22c5 aaaa  = 5.2\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc503\n\u27e8||||||||| \u22c5 aaaa  = 10.8\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc50!\n\u27e8||||||||| \u22c5 aaaa  = \u2212\u221e\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc50\" \n\u22ee\n\u27e8||||||||| \u22c5 aaaa  = 4.8\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc508\n\ud835\udc501\n\ud835\udc501\n\ud835\udc501\n\ud835\udc50# \n\u22ee\nREAD\n\ud835\udc501\n\ud835\udc502\n\ud835\udc503\n\ud835\udc504\n\ud835\udc505\n\ud835\udc506\n\ud835\udc507\n\ud835\udc508\n\ud835\udc509\n\u00d7 \n\ud835\udc5b!\"#$%&\n2\nEncoded neighbors\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\nInput Tokens\nLower Decoder\nChunk \nscoring\nNeighbor \ngating\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\n\ud835\udc501\n\ud835\udc501\ud835\udc501\nRetriever\nUpper Decoder\n\u00d7 \ud835\udc5b!\"#$%&\n2\nFigure 2: The architecture of the Retrieval-Pretrained Transformer, where an input of 45 tokens is shown, consisting\nof 9 chunks, and causal self-attention is applied over 15 tokens. The left side shows the decoder stack, where the\nbottom nlayers\n2\nare standard Transformer decoder layers, and the top nlayers\n2\nlayers also include chunked cross-attention\nlayers that fuse information from retrieved chunks. The right side shows the retriever, which takes a chunk and\nretrieves the highest-scoring K chunks that appeared earlier in the document.\nchunks, and retrieval is performed at the chunk\nlevel. Specifically, given a sequence of L input\ntokens, (x1, x2, . . . , xL), we partition it into a se-\nquence of \u2113 =\nL\nm non-overlapping chunks of\nlength m, denoted by C = (c1, c2, . . . , c\u2113). For\nevery possible query chunk, cq = ci, the model\nwill retrieve a subset of at most K \u226a \u2113 chunks,\nR(cq) \u2282 C<i = (c1, c2, ..., ci\u2212w), where C<i is\nthe set of retrievable chunks for ci, which excludes\nthe w chunks to which it already has access to\nthrough causal self-attention. The goal is to learn a\nmodel that retrieves a chunk subset, R(cq), that in-\ncrease the probability of autoregressive generation\nof the target chunk ct = ci+1.\nWe present our method in two parts. First, our\narchitecture (\u00a73.1), which leverages CCA to fuse\nretrieved representations into the LM, but adds a\nlearned retriever component. Second, we present\nthe training method (\u00a73.2-\u00a73.3), where the retriever\nis trained to retrieve chunks useful for generating a\nfuture chunk according to a reference LM.\n3.1\nModel Architecture\nFig. 2 illustrates our architecture, where the input\nhas 45 input tokens divided into 9 chunks, and\ncausal self-attention is applied over w = 3 chunks\n(15 tokens). The left side depicts the decoder stack\n(\u201creader\u201d), and the right side the retriever. The\nreader is split into two, where the bottom nlayers\n2\nlayers (lower decoder) are standard Transformer\ndecoder layers that take w chunks as input and out-\nput representations that will be used by the retriever\nand the top decoder layers.\nThe top nlayers\n2\nlayers (upper decoder) use Chun-\nked Cross-Attention (CCA) to fuse information\nfrom the top-K neighbor chunks retrieved by the\nretriever back into the LM. We use standard CCA\nlayers from RETRO (Borgeaud et al., 2022), where\nfor each one of the \u2113 chunks, queries are the m to-\nken representations of that chunk output by causal\nattention, and the keys and values are the token\nrepresentations for the top-K neighbor chunks out-\nput by the retriever. For full details of CCA, see\nBorgeaud et al. (2022).\nNext, we describe the retriever component, along\nwith a neighbor gating mechanism for modulating\nthe effect of retrieved representations.\nRetriever\nThe retriever takes as input the rep-\nresentations output by the lower decoder and pro-\nduces a similarity score for every pair of chunks.\nGiven a query chunk cq, the query-based score for\neach retrievable chunk c is sq(c) = \u27e8WQcq, WKc\u27e9,\nwhere WQ, WK \u2208 Rd\u00d7d are learned linear projec-\ntions, and cq and c are chunk representations.\nFor an m-token long chunk c, we compute its\nrepresentation c by applying bidirectional attention\nover the chunk tokens, followed by mean-pooling\nacross the time dimension. This maintains causal-\nity, as these representations are only used during\nthe prediction of the next chunk.\nOnce scores for all pairs of chunks are com-\nputed, the retrieved neighbor chunks R(cq), for\neach query chunk, cq, consists of its top-K highest-\nscoring retrievable chunks. Then, for each chunk\ncj \u2208 R(cq), we concatenate the representations\nof the succeeding chunk cj+1 to provide addi-\ntional context, and the final representation for\nall neighbors of all chunks is given by a tensor\nC \u2208 R\u2113\u00d7K\u00d72m\u00d7d.1\nOverall (and unlike methods like TRIME and\nkNN-LM), the retriever is an integral part of the\nLM, where the lower decoder computes representa-\ntions for the retriever (which we dub self-retrieval),\nand the upper decoder consumes representations\nproduced by the retriever.\nNeighbor gating\nWe add a neighbor gating\nmechanism to softly select neighbor representa-\ntions that are useful for fusing into the upper de-\ncoder. Let Ci,k \u2208 R2m\u00d7d be the token represen-\ntations for the k\u2019th neighbor of chunk ci.\nWe\nmean-pool across the time dimension to obtain a\nvector \u02c6ci,k for each neighbor chunk. Then, we\nenrich the neighbor representation of each chunk\nby applying causal attention \u2013 a neighbor chunk\nrepresentations \u02c6ci,k attends to chunks that precede\nit or to neighbors of the same chunk ci that are\nranked higher. Finally, for each chunk we obtain\nthe gated retrieved representation by multiplying\nthe augmented representations by a gating score:\nCg\ni,k = max{\u03b7, \u03c3( wng\u02c6ci,k\nd\n)} \u00b7 Ci,k where wng is a\nlearned parameter vector, \u03b7 is a small value meant\nto maintain gradient flow,2 and \u03c3 is the sigmoid ac-\ntivation. Finally, in the upper decoder, when CCA\nis performed, the keys and values are Cg\ni,k.\n3.2\nSupervision Signal\nFor each query chunk cq = ci, we want to identify\nneighbor chunks that will be helpful for generating\nct = ci+1, and use those neighbor chunks as su-\npervision signal for the retriever. Similar to Rubin\n1Similar to RETRO, token representations of retrieved\nchunks are also augmented through cross-attention over tokens\nof the query chunk, cq.\n2We set \u03b7 = 0.1 in all of our experiments.\net al. (2022), we can exploit the fact that we are\nproducing training data and use information from\nct itself to produce such a score. Unlike Zhong\net al. (2022), who use lexical clues alone, we will\nuse an independent scoring LM for this purpose.\nScoring every chunk w.r.t to all preceding chunks\nis quadratic in the number of chunks in a document,\nand thus computationally difficult. Thus, we use\na simple, BM25 unsupervised retriever (Robert-\nson and Zaragoza, 2009) that takes as input the\nconcatenation of the chunks (cq, ct) = (ci, ci+1)\nand returns a set of candidates neighbor chunks,\n\u00afR \u2282 C(cq), which have high lexical overlap with\nthe current and subsequent chunk. This retriever\nhas access to the tokens that need to be generated\nby the LM, which is allowed at training time.\nLet \u02c6g be an independently-trained LM, and let\n\u00afcj be the concatenation (cj, cj+1). We compute a\nscore st (\u00afcj) that reflects whether the information\nin \u00afcj is more useful for decoding ct compared to\nchunks that are close to cq. Specifically, the target-\nbased score for a candidate chunk is\nst (\u00afcj) = log Prob\u02c6g\n\u0000ct | cj, cj+1, cq\u0001\nProb\u02c6g (ct | ci\u22122, ci\u22121, cq).\nThis score is positive when information in \u00afcj is\nmore useful for decoding ct than information in the\npreceding two chunks (ci\u22122, ci\u22121).\nWe apply this scoring function to all chunks, and\ndefine for each query chunk cq the set of positive\nchunks Rq\npos, which includes candidates for which\nst(\u00b7) > 0. This should result in helpful chunks, as\neach candidate chunk is at least as good as the local\ncontext. With this ordering at our disposal, we can\napply standard retrieval training methods.\n3.3\nTraining\nTo train the parameters of the retriever compo-\nnent, we adapt the widely-used LambdaRank loss\n(Burges et al., 2006). The loss for each query chunk\ncq (w.r.t its retrievable chunks) is:\nLret(cq) =\nX\n{j,l:\u00afcl\u2208Rq\npos,st(\u00afcl)>st(\u00afcj)}\n\u03bbjl max (0, \u03c4 \u2212 (sq(cl) \u2212 sq(cj)))\nwhere \u03c4 is a margin hyper-parameter, and \u03bbjl is\nthe LambdaRank scaling that considers the relative\nranking of each candidate. This loss is non-zero\nwhen for some pair of candidates, the target-based\nscore disagrees (with margin \u03c4) with the ranking of\nthe query-based score for candidates in Rq\npos. Opti-\nmizing this loss function allows RPT to distinguish\nbetween relevant and irrelevant chunks. Our final\nloss is LLM + \u03b1retLret, where LLM is the standard\nLM loss and \u03b1ret is the retrieval loss coefficient,\nincreased linearly in the first 100K steps. We also\nincrease \u03c4 linearly during training.\n3.4\nImportant Implementation Details\nScheduled sampling\nTo reduce train-test mis-\nmatch, we apply scheduled sampling (Bengio et al.,\n2015) during training. Namely, After computing\nthe top-K neighbor chunks, we use these neighbors\nwith probability 1 \u2212 pss, and with probability pss\nthe top-K scoring candidates from Rq\npos as input\nfor CCA. We anneal pss from 1 to 0 during the first\n90% of training with a cosine schedule. This al-\nlows the model to gradually learn to use its own\npredictions. We report the effect of this in \u00a75.3.\nSliding window attention at training and infer-\nence time\nAs described in \u00a73, the decoder takes\nas input w chunks, each with m tokens as input,\nand applies causal attention over them. In practice,\nto give the first tokens access to past tokens, we\nuse the sliding-window attention mechanism (Dai\net al., 2019; Beltagy et al., 2020; Hutchins et al.,\n2022), where the number of tokens in a window\nis 2,048 and the stride is 1,024. Thus, the input to\neach window is 2,048 tokens and the output are the\nrepresentations for the last 1,024 tokens, which use\nthe keys and values of the previous 1,024 tokens\nfor contextualization.\nAt inference time a similar procedure is applied\n(Dai et al., 2019), where we compute and cache\nthe key and value representations for segments of\n1,024 tokens, and then use these as context for\ngenerating or estimating the probability of the next\nsegment. Naturally, at inference time the retriever\ncomponent provides access to all tokens from the\nbeginning of the document.\nAdditional details\nAt training time we use se-\nquences of length L = 16, 384 tokens, which are\nsplit into 4 devices, each consuming 4, 096 to-\nkens. As mentioned, the decoder stack takes 2, 048\ntokens as input (in a sliding window approach),\nwhich contains \u2113 = 32 chunks of length m = 64.\nWe employ Rotary Positional embedding (Su et al.,\n2021), and train all models for 500K steps on a\nTPUv4-64, with an effective batch size of 217 to-\nkens.\nName\nTokens (Train/Test)\nMedian Length\nArXiv\n12,000 / 16\n16,368\nCodeParrot\n5,000 / 5\n29,269\nPG19\n3,000 / 9\n82,659\nBooks3\n25,000 / 35\n113,496\nTable 1: Number of tokens (in millions) for each dataset\nand median document length.\nFor all models trained, we use the GPT-NeoX\n(Black et al., 2022) tokenizer, which was trained on\nthe Pile (Gao et al., 2021a) and covers the domains\nwe evaluate on (see \u00a74). As our scoring language\nmodel, we use the deduplicated 1.4B parameter ver-\nsion of Pythia (Biderman et al., 2023), and score\nwith it the top-20 BM25 candidates. Our model has\n12 layers, hidden dimension d = 1024, and 8 atten-\ntion heads with a head dimension of 128. We apply\nCCA every 2 layers and use 2 neighbors, unless\nmentioned otherwise. Additional implementation\ndetails are in Appendix A.1.\n0\n5\n10\n%\nArXiv\n0\n5\n10\n%\nCodeParrot\n0\n5\n10\n%\nPG19\n102\n103\n104\n105\n106\n107\nSequence length\n0\n5\n10\n%\nBooks3\nFigure 3: Histograms of the distribution over document\nlength in tokens across all datasets. The x-axis is in log\nscale.\n4\nLong Range LM Datasets\nWe evaluate RPT on four datasets, covering do-\nmains such as books, code, and mathematical writ-\ning, which require the ability to recall informa-\ntion over long distances. Tab. 1 and Fig. 3 pro-\nvide statistics on dataset size and the distribution\nover document length, showing that documents are\nlong across all datasets and in particular PG19 and\nBooks3, where documents typically contain 105\ntokens or more. We briefly review the datasets.\nPG19\nIntroduced in Rae et al. (2020), PG19 is a\nwidely-used long-range language modeling bench-\nmark containing books from Project Gutenberg,\nand covering a wide range of literary genres, styles,\nand topics. We adopt the exact setup and data split\nfrom prior work (Wu et al., 2022; Hutchins et al.,\n2022; Mehta et al., 2023).\nBooks3\nis a corpus of books released as part of\nthe Pile (Gao et al., 2021a), containing a vast col-\nlection of literary works from different domains.\nTo our knowledge, we are the first to use this corpus\nas a long-range language modeling benchmark.\nCodeParrot\n(Wolf et al., 2023) is a corpus of\nclean, nearly-deduplicated Python code from vari-\nous GitHub repositories. Modeling code requires\nunderstanding patterns and contextualizing infor-\nmation over long distances, making it a natural\ncandidate for testing long-range LMs. In our exper-\niments, we follow the approach of Wu et al. (2022),\ncombining files from the same repository to con-\nstruct a corpus with longer sequences, and create a\ntrain/test split (see Tab. 1).\nArXiv\nis a corpus of preprint papers extracted\nfrom ArXiv. It consists of mathematical texts that\nrequire maintaining coherence and referring to pre-\nviously mentioned information over extended text.\nPrior work evaluated long-range LMs on this cor-\npus (Wu et al., 2022; Hutchins et al., 2022; Mehta\net al., 2023), but did not release their corpus. Thus,\nwe use the preprocessed corpus and data splits\nmade available by Azerbayev et al. (2023).\n5\nExperiments\nWe now turn to experiments for comparing RPT to\nprior work across our four datasets.\n5.1\nExperimental Setup\nWe compare to the following baselines and oracles.\nTransformer-XL\nOur simplest baseline is a stan-\ndard transformer decoder stack with sliding win-\ndow attention. Put differently, we simply remove\nfrom RPT the retriever component and CCA lay-\ners in the upper decoder. Using sliding window\nattention (as described in \u00a73.4) can be viewed as a\nvariant of Transformer-XL (Dai et al., 2019).\nRETRO\n(Borgeaud et al., 2022) A retrieval-\naugmented model, where we omit the retriever\ncomponent and feed the top-K neighbors retrieved\nby BM253 as input to the CCA layers in the upper\ndecoder. During training, we use the query (cq, ct),\nsince we have access to the target chunk. During\ninference, we use cq.\nRPT-Lex\nA version of RPT, where the training\nsignal is not obtained from the scoring LM, but\nfrom lexical information only, similar to TRIME\n(Zhong et al., 2022). Explicitly, the set of posi-\ntive chunks Rq\npos for a chunk cq contains the top-\n20 chunks that have the highest BM25 score with\n(cq, ct).\nRPT-Sem\nOur full model described in \u00a73.\nBlock-Recurrent Transformer\nWe use the offi-\ncial training implementation4 of Block-Recurrent\nTransformer (Hutchins et al., 2022) with the default\nconfiguration.\nMemorizing Transformer\nWe use the official\nimplementation4 of Memorizing Transformers (Wu\net al., 2022), with the default configuration and a\nmemory size of 32K tokens.\nOracles\nFor each test chunk, we can exhaustively\nsearch and use at test time the best possible neigh-\nbors for a model according to the scoring LM. This\nprovides an upper bound for the performance of\nRPT-Lex and RPT-Sem, as they are trained to imi-\ntate the ranking produced by this oracle.\nMetrics\nWe use perplexity to evaluate the per-\nformance of models. In addition, we use the tar-\nget score st(\u00b7) from the scoring LM to compute\nfor each chunk a gold ranking over all previous\nchunks, and to label chunks as positive/negative\niff their target score is positive/negative, respec-\ntively.\nWith this information, we can evaluate\nPrecision@k, which is the fraction of top-k chunks\naccording to the query-based score that are posi-\ntive, and Recall@k, which is the fraction of posi-\ntive chunks that are in the top-k chunks according\nto the query-based score. We also use the gold\nranking to compute NDCG@k, which is a standard\nretrieval metric (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002).\n5.2\nResults\nTable 2 shows our main results, which show\nthat RPT-Sem is comparable or better than all\n3Concurrent work (Doostmohammadi et al., 2023) showed\nthat training RETRO using BM25 substantially outperforms\ndense retrieval methods.\n4https://github.com/google-research/\nmeliad.\nModel\nArXiv\nCode\nPG19\nBooks3\nParams\nTRANSFORMER-XL (OURS)\n3.11\n2.30\n11.48\n15.00\n202M\nRETRO W. BM25 (OURS)\n2.94\n2.17\n11.44\n14.60\n236M\nRPT-LEX\n2.92\n2.23\n11.59\n14.32\n242M\nRPT-SEM\n2.77\n2.17\n10.96\n13.91\n242M\nW. 3 NEIGHBOURS\n2.75\n2.16\n10.92\n13.87\n242M\nW. 4 NEIGHBOURS\n2.74\n2.15\n10.93\n13.91\n242M\nMEMORIZING TRANSFORMER\n2.92\n2.18\n10.97\n14.40\n212M\nBLOCK-RECURRENT TRANSFORMER\n2.89\n2.73\n10.95\n14.64\n212M\nRPT-LEX W. ORACLE\n2.80\n2.12\n10.88\n13.30\n242M\nRPT-SEM W. ORACLE\n2.69\n2.10\n10.26\n12.74\n242M\nTable 2: Test set perplexity for all datasets. Unless specified, we use 2 neighbours during inference.\nother baselines in all cases.\nUsing a fixed re-\ntriever (RETRO) categorically improves perfor-\nmance compared to Transformer-XL; RPT-Lex\nleads to gains in Books3 but to losses in PG19\ncompared to RETRO, and RPT-Sem outperforms\nTransformer-XL, RETRO, and RPT-Lex on ArXiv,\nPG19, and Books3, and has performance compara-\nble to RETRO on CodeParrot.\nCompared to Block-Recurrent Transformers and\nMemorizing transformers, which do not use CCA,\nperformance is again either comparable or bet-\nter, with notable gains on ArXiv, CodeParrot, and\nBooks3.\nCCA allows one to dynamically increase the\nnumber of neighbors at inference time. When us-\ning 3 or 4 neighbors (instead of 2), performance\nimproves, which allows one to trade compute for\nperformance.\nLast, oracle models consistently achieve the\nbest perplexity across all datasets, improving from\n2.74\u21922.69 on ArXiv, 2.15\u21922.10 on CodePar-\nrot, 10.92\u219210.26 on PG19, and 13.87\u219212.74 for\nBooks3. This shows that improving the training of\nthe retriever can further improve performance.\nDataset\nPrecision@2\nRecall@10\nnDCG@20\nBM25 RPT-L RPT-S BM25 RPT-L RPT-S BM25 RPT-L RPT-S\nArXiv\n27%\n26%\n32%\n55%\n54%\n58%\n24%\n24%\n30%\nCode\n29%\n26%\n34%\n53%\n52%\n56%\n25%\n23%\n30%\nBooks3\n23%\n19%\n26%\n55%\n50%\n58%\n18%\n16%\n22%\nPG19\n22%\n22%\n28%\n55%\n55%\n61%\n18%\n18%\n23%\nTable 3: Test retrieval metrics across datasets.\nRetrieval metrics\nTable 3 presents the retrieval\nmetrics w.r.t oracle positive chunks. Again, re-\ntrieval with RPT-Sem outperforms both RPT-Lex\nand BM25 in all cases. This shows the importance\nof training a retriever, and moreover that using\nsemantic supervision leads to better retrieval com-\npared to a lexical signal only.\nModel\nArXiv\nCode\nPG19\nBooks3\nRPT-SEM\n2.77\n2.17\n10.96\n13.91\n- ONLY TEACHER FORCING\n2.91\n2.22\n11.54\n14.66\n- NO TEACHER FORCING\n2.95\n2.26\n13.10\n14.40\n- NO NEIGHBOR GATING\n2.92\n2.20\n11.50\n18.68\nTable 4: Results of our ablation study on RPT-Sem.\nDistribution of improvements across chunks\nWe compute the improvement in perplexity for all\nchunks when comparing to Transformer-XL and\nplot the distribution of improvements for RETRO,\nRPT-Lex, and RPT-Sem in Fig. 4. Clearly, RPT-\nSem has a heavier right tail in all cases except\nfor CodeParrot, further illustrating its advantage\nover the other baselines. We further analyze why\nRETRO with BM25 performs well on CodeParrot\nin \u00a75.4.\n5.3\nAblations\nTab. 4 shows the result of an ablation study on\nRPT-Sem over all datasets.\nOnly Teacher Forcing\nWe force the model to\nattend to gold neighbors according to the scoring\nLM, without annealing pss during training. This\nleads to a performance drop across all datasets, and\nin particular for PG19 and Books3.\nNo Teacher Forcing\nHere, we do the opposite\nand fix pss = 0 throughout training, i.e., we only\nuse the predicted neighbors and not gold ones. This\ncan lead to undertraining of the CCA layers since\nthey are exposed to low-quality neighbors at the\nbeginning of training and results drop even further\ncompared to Only Teacher Forcing.\nNo neighbor gating\nWe disable neighbor gat-\ning which controls the flow of information from\nneighbor chunks and analyze the effect on model\nperformance. We observe a performance reduc-\ntion across all datasets, notably on Books3, where\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n%\nArXiv\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nCodeParrot\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nPG19\n0.8\n0.9\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\nBooks3\nRPT-Sem\nRPT-Lex\nRETRO+BM25\nFigure 4: Relative perplexity improvement across different retrievers. All retrievers exhibit positive skew with a\nheavy right tail, and RPT-Sem leads to the most pronounced improvements.\nperplexity increases by 4.5 points. Since neighbor\ngating is independent of the retriever used, we show\nresults when adding neighbor gating to RETRO in\n\u00a7A.4., which shows mixed results.\n5.4\nAnalysis\n12\n14\n16\n18\n20\n22\nToken overlap\nArXiv\nBooks3\n12\n14\n16\n18\n20\n22\nToken overlap\nCodeParrot\nPG19\nRPT-Sem\nRPT-Lex\nRETRO+BM25\nQuery\nTarget\nFigure 5: We measure the number of unique token over-\nlap between query/target chunks and the best retrieved\nneighbor.\nToken overlap\nFig. 5 plots the average number\nof tokens that overlap between the query/target\nchunks the best retrieved neighbor for RETRO,\nRPT-Lex, and RPT-Sem. RPT-Sem retrieves para-\ngraphs with higher overlap with the target chunk\ncompared to RPT-Lex. Naturally, BM25 retrieves\nchunks with the highest overlap with the query\nchunk. However, this does not translate to higher\nlexical overlap for the target chunk.\nSupervision quality\nWe train RPT-Sem using\ninformation from the target scoring function st(\u00b7),\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20\nTop-K element according to BM25\n0.00\n0.05\n0.10\n0.15\n0.20\nAverage maximum target score across chunks\nDataset\nBooks3\nArXiv\nCodeParrot\nPG19\nFigure 6: The maximal target score st(\u00b7) for the top-\nk chunks retrieved by BM25 averaged across chunks\nand for all datasets. Since the maximal target score for\nthe top-20 chunks is much higher than for the top-2,\nlearning to rerank the top-20 BM25 candidates can lead\nto substantial improvements in retrieval quality.\nwhich we saw leads to model improvements. How-\never, the target scoring function only provides a\nreranking of the top-20 candidates according to\nBM25. Thus, a natural question is how much does\nthe supervision quality improve through this rerank-\ning. Figure 6 shows for every rank k the maxi-\nmal target score among the top-k chunks according\nto BM25, averaged over chunks and across our 4\ndatasets. Clearly, reranking the top-20 BM25 can-\ndidates has a lot of potential, as the maximal target\nscore is much higher for the top-20 candidates com-\npared to the top-2. This hints that longer and better\ntraining of the retriever can further improve the\nperformance of RPT-Sem.\nInterestingly, our analysis sheds light on why\nRPT-Sem outperforms RETRO clearly on Books3\nand PG19 but less so on CodeParrot. The max-\nimal target score for CodeParrot when k = 2 is\nalready quite high \u2013 around 0.1, which corresponds\nto more than 10% improvement in the probability\nof the target chunk compared to the local context.\nConversely, for PG19 and Books3, the target score\nwhen k = 2 is closer to 0. This hints that lexical\ninformation alone is quite effective for CodePar-\nrot, potentially by retrieving function definitions,\nvariable assignments, etc.\n0\n10\n20\n30\n% Improvment\nArXiv\nBooks3\n0\n10\n20\n30\n% Improvment\nCodeParrot\nPG19\nRPT-Sem\nRPT-Lex\nRETRO+BM25\nIncorrect\nCorrect\nAll\nFigure 7: Relative improvement with/without correct\nretrieval.\nSubgroup analysis\nFigure 7 shows the average\nrelative improvement (across chunks) of RETRO,\nRPT-Lex, and RPT-Sem compared to Transformer-\nXL, when distinguishing between cases where a\n\u201cgold\u201d oracle chunk was retrieved and cases where\nno gold chunk was retrieved.\nAs expected, RPT-Sem leads to improvements\non all datasets, and outperforms other baselines ex-\ncept for RETRO on CodeParrot where performance\nis similar. Second, cases where a gold chunk was\nretrieved indeed typically lead to larger improve-\nments, but we witness improvements even in cases\nwhere a gold chunk was not retrieved, which shows\nthat the model can still benefit from such retrievals.\n6\nRelated Work and Discussion\nLong-range language modeling\nA primary fo-\ncus in long-range language modeling has been ad-\ndressing the quadratic complexity of attention in\norder to develop more efficient mechanisms for\nhandling long texts. For instance, Transformer-\nXL (Dai et al., 2019) processes the input using a\nsegment-level mechanism while retaining a cache\nfrom previous segments.\nLongformer (Beltagy\net al., 2020) extends this idea to accommodate\neven longer contexts. Sparse strategies, such as\nthose proposed in Zaheer et al. (2020); Roy et al.\n(2021); Kitaev et al. (2020), attend to only a subset\nof tokens through clustering or hashing methods.\nAnother approach involves compressing the input\nand attending over the compressed sequence (Mar-\ntins et al., 2022; Rae et al., 2020), or learning to\nignore irrelevant tokens (Sukhbaatar et al., 2021).\nRecently, recurrent mechanisms have re-emerged\nas potential solutions (Fan et al., 2021; Hutchins\net al., 2022; Mehta et al., 2023). From an analysis\nperspective, past work (Press et al., 2021) demon-\nstrated that standard LM benchmarks are not ideal\nfor measuring the long-range capabilities of mod-\nels. Sun et al. (2021) discuss various types of se-\nquences that benefit from having a long context,\nand Rae and Razavi (2020) investigate long-range\narchitectural choices and recommend increasing\nlong-range capabilities in the upper layers.\nRetrieval augmented LMs\nRetrieval-augmented\nLMs have emerged as a prominent approach for\nefficiently leveraging external knowledge while\ngenerating text. These models can be broadly di-\nvided into those operating at token-level granular-\nity and those operating at sequence-level granular-\nity. Token-level methods, such as kNN-LM (Khan-\ndelwal et al., 2020), TRIME (Zhong et al., 2022),\nand SPALM (Yogatama et al., 2021), retrieve infor-\nmation for individual tokens. Sequence-level ap-\nproaches like RAG (Lewis et al., 2020) utilize pre-\ntrained encoder-decoder models with pre-trained\nretrievers for tasks like open-domain question an-\nswering. Similarly, FiD (Izacard and Grave, 2021b)\nemploys generative encoder-decoder models that\nfuse evidence from multiple passages during the\ndecoding process, closely related to the CCA mech-\nanism (see additional discussion in App A.3). Re-\ncently, Wang et al. (2023) demonstrated the poten-\ntial benefits of conducting retrieval and chunked\ncross-attention at each time step, compared with\nthe original RETRO (Borgeaud et al., 2022) paper,\nwhich retrieves every m = 64 steps.\nJoint retriever-reader training\nJoint training\napproaches typically concentrate on transferring\ninformation between a pre-trained reader into a\npre-trained retriever. These methods commonly in-\nvolve updating the retriever index during the train-\ning process in the context of knowledge-intensive\ntasks, such as open-domain question answering.\nFor instance, REALM (Guu et al., 2020) utilizes\nmasked language modeling as a learning signal\nto update the retriever. EMDR2 (Sachan et al.,\n2021) extends FiD by using encoder-decoder mod-\nels to back-propagate errors from the predicted an-\nswer to the retriever. Similarly, Izacard and Grave\n(2021a) demonstrate that it is possible to use atten-\ntion scores from the reader to supervise the retriever.\nNotably, Izacard et al. (2022) further scale up these\napproaches and jointly train a retriever with an\nencoder-decoder model, demonstrating strong few-\nshot learning capabilities. They also investigate\nvarious retriever updating techniques to address\ntrain-test mismatches in the retrieval process. We\ndo not encounter the issue of index update since we\ncompute the entire index through a forward pass.\nAttention as Retrieval\nSeveral works view the\nattention layer as a retrieval component. Memoriz-\ning Transformers (Wu et al., 2022) employ a single\nk-NN layer and retrieve cached keys and values\nwithout back-propagating gradients through the re-\ntrieval operation. Similarly, Bertsch et al. (2023)\ndemonstrate that this approach can be used with\nany existing pre-trained model and apply it at every\nattention layer for long summarization tasks. No-\ntably, Jiang et al. (2022) use this observation and\nemploy a caching mechanism (Gao et al., 2021b)\nto enable joint end-to-end training with the super-\nvision of the downstream task. We view the latter\nas a potential way to fine-tune RPT and leave it for\nfuture work.\nRetriever Pre-training\nEarly work on retriever\npre-training relied on the unsupervised Inverse\nCloze Task to pre-train the retriever (Lee et al.,\n2019; Guu et al., 2020). It was later shown that\ndirectly using BERT (Devlin et al., 2019) with a\nsupervised objective is sufficient to get good perfor-\nmance on standard benchmarks (Karpukhin et al.,\n2020). However, this paradigm showed lacklus-\nter performance on long-tail entities compared to\nBM25 (Amouyal et al., 2022; Sciavolino et al.,\n2021). Recently, unsupervised pre-training meth-\nods (Gao and Callan, 2022; Ram et al., 2022; Izac-\nard et al., 2021) enabled improved performance.\nHowever, these methods are initialized from a pre-\ntrained BERT (Devlin et al., 2019) encoder model,\nwhile RPT is a retriever-reader architecture trained\nfrom scratch that outperforms BM25 without any\nadditional pre-training.\nSupervising retrievers with LLMs\nEPR (Ru-\nbin et al., 2022) demonstrated that LLMs could be\nemployed to train a retriever for prompt retrieval\nby estimating the probability of an output given\nthe input and a candidate training example as the\nprompt. Similar techniques were applied to open-\ndomain question answering via re-ranking retrieval\nresults (Sachan et al., 2022; Ram et al., 2023) and to\nsupervise retrievers through perplexity distillation\n(Izacard et al., 2022). Recently, Shi et al. (2023)\nutilized this supervision method to improve the per-\nformance of various LLMs in a black-box fashion.\n7\nConclusion\nIn this work, we present the Retrieval-Pretrained\nTransformer (RPT), a retrieval-augmented LM\nwhere the retriever is trained as a native component\nof the LM to retrieve semantically relevant chunks\nfor future text prediction. We evaluate RPT on\nfour long-range language modeling tasks, includ-\ning books, code, and mathematical writing. We\ndemonstrate that by seamlessly integrating the re-\ntriever into the architecture and training process,\nRPT benefits from the fusion of retrieved context,\nimproving over strong retrieval-augmented base-\nlines. We envision RPT will pave the way for a\nnew generation of pretrained language models with\nretrieval deeply integrated throughout their archi-\ntecture and training process.\nAcknowledgments\nThis research was supported with Cloud TPUs from\nGoogle\u2019s TPU Research Cloud (TRC) and The Eu-\nropean Research Council (ERC) under the Euro-\npean Union Horizons 2020 research and innovation\nprogramme (grant ERC DELPHI 802800). Ohad\nwould like to thank Iz Beltagy for suggesting the\nTRC program, and the entire TAU NLP lab and\nespecially Guy Dar and Itay Itzhak. This work was\ncompleted in partial fulfillment of the Ph.D. degree\nof Ohad Rubin.\nReferences\nSamuel Joseph Amouyal, Tomer Wolfson, Ohad Rubin,\nOri Yoran, Jonathan Herzig, and Jonathan Berant.\n2022. Qampari: An open-domain question answer-\ning benchmark for questions with many answers from\nmultiple paragraphs.\nZhangir\nAzerbayev,\nEdward\nAyers,\nand\nBar-\ntosz\nPiotrowski.\n2023.\nProof-Pile:\nA\nPre-training\nDataset\nof\nMathematical\nText.\nhttps://huggingface.co/datasets/\nhoskinson-center/proof-pile.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam\nShazeer. 2015. Scheduled sampling for sequence\nprediction with recurrent neural networks. In Pro-\nceedings of the 28th International Conference on\nNeural Information Processing Systems - Volume 1,\nNIPS\u201915, page 1171\u20131179, Cambridge, MA, USA.\nMIT Press.\nAmanda Bertsch, Uri Alon, Graham Neubig, and\nMatthew R. Gormley. 2023. Unlimiformer: Long-\nrange transformers with unlimited length input.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023.\nPythia:\nA suite for analyzing large language models across\ntraining and scaling.\nSid Black, Stella Biderman, Eric Hallahan, Quentin An-\nthony, Leo Gao, Laurence Golding, Horace He, Con-\nnor Leahy, Kyle McDonell, Jason Phang, Michael\nPieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and\nSamuel Weinbach. 2022. GPT-NeoX-20B: An open-\nsource autoregressive language model. In Proceed-\nings of the ACL Workshop on Challenges & Perspec-\ntives in Creating Large Language Models.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann,\nTrevor Cai, Eliza Rutherford, Katie Millican, George\nvan den Driessche, Jean-Baptiste Lespiau, Bogdan\nDamoc, Aidan Clark, Diego de Las Casas, Aurelia\nGuy, Jacob Menick, Roman Ring, Tom Hennigan,\nSaffron Huang, Loren Maggiore, Chris Jones, Albin\nCassirer, Andy Brock, Michela Paganini, Geoffrey\nIrving, Oriol Vinyals, Simon Osindero, Karen Si-\nmonyan, Jack W. Rae, Erich Elsen, and Laurent Sifre.\n2022. Improving language models by retrieving from\ntrillions of tokens. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 2206\u20132240.\nPMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nChristopher Burges, Robert Ragno, and Quoc Le. 2006.\nLearning to rank with nonsmooth cost functions. In\nAdvances in Neural Information Processing Systems,\nvolume 19. MIT Press.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978\u20132988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\nMinneapolis, Minnesota.\nEhsan Doostmohammadi, Tobias Norlund, Marco\nKuhlmann, and Richard Johansson. 2023. Surface-\nbased retrieval reduces perplexity of retrieval-\naugmented language models.\nAngela Fan, Thibaut Lavril, Edouard Grave, Armand\nJoulin, and Sainbayar Sukhbaatar. 2021. Address-\ning some limitations of transformers with feedback\nmemory.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2021a.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. ArXiv preprint, abs/2101.00027.\nLuyu Gao and Jamie Callan. 2022. Unsupervised cor-\npus aware language model pre-training for dense pas-\nsage retrieval. Association for Computational Lin-\nguistics.\nLuyu Gao, Yunyi Zhang, Jiawei Han, and Jamie Callan.\n2021b. Scaling deep contrastive learning batch size\nunder memory limited setup. In Proceedings of the\n6th Workshop on Representation Learning for NLP\n(RepL4NLP-2021). Association for Computational\nLinguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In Proceed-\nings of the 37th International Conference on Machine\nLearning, ICML\u201920. JMLR.org.\nYangsibo Huang, Daogao Liu, Zexuan Zhong, Weijia\nShi, and Yin Tat Lee. 2023. knn-adapter: Efficient\ndomain adaptation for black-box language models.\nDeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan\nDyer, and Behnam Neyshabur. 2022. Block-recurrent\ntransformers. In Advances in Neural Information\nProcessing Systems.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. Trans.\nMach. Learn. Res., 2022.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In International Conference on Learning\nRepresentations.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874\u2013880, Online. Association for Computa-\ntional Linguistics.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nKalervo J\u00e4rvelin and Jaana Kek\u00e4l\u00e4inen. 2002. Cumu-\nlated gain-based evaluation of ir techniques. ACM\nTransactions on Information Systems.\nZhengbao Jiang, Luyu Gao, Zhiruo Wang, Jun Araki,\nHaibo Ding, Jamie Callan, and Graham Neubig.\n2022. Retrieval as attention: End-to-end learning\nof retrieval and reading within a single transformer.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2336\u20132349, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769\u20136781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efficient transformer. In Inter-\nnational Conference on Learning Representations.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova.\n2019. Latent retrieval for weakly supervised open\ndomain question answering. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics. Association for Computational\nLinguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive nlp tasks. In Advances in Neural Infor-\nmation Processing Systems, volume 33, pages 9459\u2013\n9474. Curran Associates, Inc.\nPedro Henrique Martins, Zita Marinho, and Andre Mar-\ntins. 2022. \u221e-former: Infinite memory transformer.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), Dublin, Ireland. Association for\nComputational Linguistics.\nHarsh Mehta, Ankit Gupta, Ashok Cutkosky, and\nBehnam Neyshabur. 2023.\nLong range language\nmodeling via gated state spaces. In The Eleventh In-\nternational Conference on Learning Representations.\nOfir Press, Noah A. Smith, and Mike Lewis. 2021.\nShortformer: Better language modeling using shorter\ninputs. In Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers).\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157\u2013163, Valencia, Spain.\nAssociation for Computational Linguistics.\nJack Rae and Ali Razavi. 2020. Do transformers need\ndeep long-range memory? In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7524\u20137529, Online. Association\nfor Computational Linguistics.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learning\nRepresentations.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models.\nOri Ram, Gal Shachaf, Omer Levy, Jonathan Berant,\nand Amir Globerson. 2022.\nLearning to retrieve\npassages without supervision. Association for Com-\nputational Linguistics.\nStephen Robertson and Hugo Zaragoza. 2009.\nThe\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends in Information Re-\ntrieval, 3:333\u2013389.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2021. Efficient content-based sparse\nattention with routing transformers. Transactions of\nthe Association for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655\u20132671, Seattle, United States.\nAssociation for Computational Linguistics.\nDevendra Singh Sachan, Mike Lewis, Mandar Joshi,\nArmen Aghajanyan, Wen tau Yih, Jo\u00eblle Pineau, and\nLuke Zettlemoyer. 2022. Improving passage retrieval\nwith zero-shot question generation. In Conference on\nEmpirical Methods in Natural Language Processing.\nDevendra Singh Sachan, Siva Reddy, William L. Hamil-\nton, Chris Dyer, and Dani Yogatama. 2021. End-to-\nend training of multi-document reader and retriever\nfor open-domain question answering. In Advances in\nNeural Information Processing Systems.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen tau Yih. 2023. Replug: Retrieval-augmented\nblack-box language models.\nJianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng\nLiu. 2021. Roformer: Enhanced transformer with\nrotary position embedding.\nSainbayar Sukhbaatar, Da JU, Spencer Poff, Stephen\nRoller, Arthur Szlam, Jason E Weston, and Angela\nFan. 2021.\nNot all memories are created equal:\nLearning to expire.\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-\nMicke, and Mohit Iyyer. 2021.\nDo long-range\nlanguage models actually use long-range context?\nArXiv, abs/2109.09115.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nBoxin Wang, Wei Ping, Peng Xu, Lawrence McAfee,\nZihan Liu, Mohammad Shoeybi, Yi Dong, Oleksii\nKuchaiev, Bo Li, Chaowei Xiao, Anima Anandku-\nmar, and Bryan Catanzaro. 2023. Shall we pretrain\nautoregressive language models with retrieval? a\ncomprehensive study.\nThomas\nWolf,\nLoubna\nBen\nAllal,\nLeandro\nvon\nWerra,\nLi\nJia,\nand\nArmel\nZebaze.\n2023.\nA dataset of python files from github.\nhttps://github.com/huggingface/\nblog/blob/main/codeparrot.md\nversion=codeparrot/codeparrot-train-v2-near-\ndedup.\nYuhuai Wu, Markus Norman Rabe, DeLesley Hutchins,\nand Christian Szegedy. 2022. Memorizing transform-\ners. In The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April\n25-29, 2022. OpenReview.net.\nDani Yogatama, Cyprien de Masson d\u2019Autume, and\nLingpeng Kong. 2021. Adaptive semiparametric lan-\nguage models. Transactions of the Association for\nComputational Linguistics, 9:362\u2013373.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\nand Amr Ahmed. 2020. Big bird: Transformers for\nlonger sequences. In Proceedings of the 34th Interna-\ntional Conference on Neural Information Processing\nSystems.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022.\nOpt: Open\npre-trained transformer language models.\nArXiv,\nabs/2205.01068.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\nIn Proceedings of the 2022 Conference on Empir-\nical Methods in Natural Language Processing, pages\n5657\u20135673, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nJuntang Zhuang, Tommy Tang, Yifan Ding, Sekhar\nTatikonda, Nicha Dvornek, Xenophon Papademetris,\nand James Duncan. 2020.\nAdabelief optimizer:\nAdapting stepsizes by the belief in observed gradi-\nents. Conference on Neural Information Processing\nSystems.\nA\nAppendix\nA.1\nAdditional Implementation Details\nAll models are implemented in JAX, we use a\ndropout rate of 0.05, weight decay of 1e-8, Co-\nsine decay to 0.1 of the maximum learning rate,\nglobal gradient norm clipping of 1, and tied input\nembedding (Press and Wolf, 2017). For our op-\ntimizer we used AdaBelief (Zhuang et al., 2020),\nwhich is a version of Adam (Kingma and Ba, 2015)\nthat instead of the accumulating squared gradients,\naccumulates the squared difference between the\ngradient and the momentum. In initial experiments,\nwe found AdaBelief to increase stability. Simi-\nlar to Block-Recurrent we found that lowering the\nlearning rate was necessary for convergence while\ntraining on Code, so for CodeParrot, we lower the\nlearning rate. For each dataset, we perform a grid\nsearch w.r.t \u03c4, and set \u03c4 = 128 for Books3, \u03c4 = 4\nfor PG19, \u03c4 = 2 for CodeParrot, and \u03c4 = 8 for\nArXiv. We set \u03b1ret = 1e \u2212 9 for all datasets. Our\nbase learning rate is 5e \u2212 3, and besides what is\nmentioned above, we do not tune other hyperpa-\nrameters. We use the validation set to choose hy-\nperparameters.\nA.2\nScoring LM\nWe use the deduplicated 1.4B parameter version\nof the Pythia (Biderman et al., 2023) LM. We also\nperformed early experiments with the T5 tokenizer\nand T5-XL 1.1, but since it was not trained on code\nor latex, Pythia 1.4B was preferable, since it was\ntrained on the Pile.\nA.3\nComparing to FiD\nRPT shares similarities with Fusion-in-Decoder\n(FiD) (Izacard and Grave, 2021b). Both RPT and\nFiD employ cross-attention mechanisms to inte-\ngrate the retrieved context within their models. In\nFiD, an initial retrieval is conducted, followed by\nencoding the retrieved neighbors separately, and\nfinally integrating them into the model using cross-\nattention in the decoder. In RPT, the decoder com-\nputes chunk embeddings and performs native re-\ntrieval, and then chunked cross-attention is applied\nModel\nArXiv\nCode\nPG19\nBooks3\nRETRO W. BM25 (OURS)\n2.94\n2.17\n11.44\n14.60\nW. GATING\n2.97\n2.21\n11.84\n13.92\nRPT-SEM\n2.77\n2.17\n10.96\n13.91\nTable 5: Results of our ablation study w. neighbor\ngating.\nto fuse the retrieved context with the model\u2019s pre-\ndictions. RPT also performs repeated retrieval at\nthe chunk level throughout the generation process,\nrather than retrieving only once based on the initial\nprompt. This enables RPT to continually adapt and\nincorporate relevant information from prior chunks\nto generate subsequent tokens more effectively.\nFurthermore, RPT is trained with retrieval being\nan integral part of the model during the entire pre-\ntraining phase, in contrast with FiD which plugs in\nretrieval components to solve specific downstream\ntasks. We view RPT as more suitable for long-text\ngeneration tasks.\nA.4\nRETRO with Neighbor Gating\nNeighbor gating is a mechanism that can be ap-\nplied to any retrieval-augmented LM, whether the\nretriever is trained or not. In Tab. 5, we show results\nof RETRO when adding neighbor gating. Results\nimprove substantially on Books3, but deteriorate\non PG19, and are roughly equivalent for ArXiv and\nCodeParrot.\n"
  },
  {
    "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
    "link": "https://arxiv.org/pdf/2306.13651.pdf",
    "upvote": "15",
    "text": "Bring Your Own Data! Self-Supervised Evaluation of\nLarge Language Models\nNeel Jain*\nKhalid Saifullah*\nYuxin Wen\nJohn Kirchenbauer\nManli Shu\nAniruddha Saha\nMicah Goldblum \u2020\nJonas Geiping\nTom Goldstein\nUniversity of Maryland\n\u2020New York University\nAbstract\nWith the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is imper-\native. For example, a company deploying a client-facing chatbot must ensure that\nthe model will not respond to client requests with profanity. Current evaluations\napproach this problem using small, domain-specific datasets with human-curated\nlabels. These evaluation sets are often sampled from a narrow and simplified\ndistribution, and data sources can unknowingly be leaked into the training set,\nwhich can lead to misleading evaluations. To bypass these drawbacks, we propose\na framework for self-supervised evaluation of LLMs by analyzing their sensitivity\nor invariance to transformations on the input text. Self-supervised evaluation can\ndirectly monitor LLM behavior on datasets collected in the wild or streamed during\nlive model deployment. We demonstrate self-supervised evaluation strategies for\nmeasuring closed-book knowledge, toxicity, and long-range context dependence,\nin addition to sensitivity to grammatical structure and tokenization errors. When\ncomparisons to similar human-labeled benchmarks are available, we find strong\ncorrelations between self-supervised and human-supervised evaluations. The\nself-supervised paradigm complements current evaluation strategies that rely on\nlabeled data. Code is available at https://github.com/neelsjain/BYOD.\n1\nIntroduction\nAs Large Language Models (LLMs) continue to advance rapidly, there has been a growing demand\nfor new evaluation metrics that can accurately capture their capabilities and limitations [Ethayarajh\nand Jurafsky, 2020, Birhane et al., 2022, Kiela et al., 2021, Bowman and Dahl, 2021]. As a result,\nthere has been a constant need to create new datasets as newer models continuously make the existing\ndatasets obsolete. Recent approaches such as BIG-Bench [Srivastava et al., 2022] and HELM [Liang\net al., 2022] aim to address this issue by providing an ever-increasing, diverse set of accumulating\nmicro-benchmarks to measure the performance of LLMs. However, these approaches still rely heavily\non dataset creation and curation, which is time-consuming and expensive.\nFurthermore, evaluation is generally dataset-centric, meaning that evaluations are based on some\nhuman-labeled or generated metric evaluated on a fixed dataset. For modern LLMs, this conventional\napproach comes with new complications. First, evaluation data is hosted on the internet (for example\non sites like GitHub). This makes them accessible to scraping bots that generate training data for\n* Equal contribution. Correspondence to: Neel Jain <njain17@umd.edu>.\nPreprint. Under review.\narXiv:2306.13651v2  [cs.CL]  29 Jun 2023\nFigure 1: In our proposed self-supervised evaluation, pairs are created from a corpus. Each pair contains the\noriginal and perturbed text, which in the figure above is creating a negation via applying a \u201cnot.\u201d These pairs are\nthen fed into the network, and the outputs (perplexity, probability distributions, or text) are compared for each\npair. These measures are then aggregated to produce an invariance or sensitivity score.\nLLMs, making older datasets unreliable unless they are painstakingly removed from the training set,\nwhich does not reliably happen [Brown et al., 2020, Gao et al., 2021].1 Second, LLM evaluation\nis by its nature multi-faceted, since different LLM applications rely on distinct capabilities, and an\never-increasing number of such capabilities needs to be tested in modern LLMs. As dataset curation\nis expensive, each test in a large benchmark like HELM [Liang et al., 2022], uses only a small\ndataset \u2013 carefully created to test a particular capability in a particular scenario. However, models are\nthen deployed in much broader contexts and settings, and the applicability of these evaluations to\ndeployment usage can be uncertain.\nTo complement conventional evaluation, we propose a framework for self-supervised model\nevaluation. In this framework, metrics are defined as invariances and sensitivities that can be\nchecked in a self-supervised fashion using interventions based only on the model in question rather\nthan external labels. Self-supervised evaluation pipelines are dataset-agnostic, and so they can\nbe utilized over larger corpora of evaluation data than conventional metrics, or even directly in\nproduction systems to monitor day-to-day performance. In this work, we develop this framework,\ndiscuss desiderata for such metrics, and provide several case studies for self-supervised metrics:\nmeasuring knowledge through negations, toxicity detection, long-range dependency, word-order, and\ntokenization sensitivity. By developing these new metrics, we hope to provide a more comprehensive\nand nuanced understanding of the strengths and limitations of LLMs.\n2\nA Procedure for Self-Supervised Evaluation\nOur goal is to measure properties of LLMs such as toxicity, closed-book knowledge, and word\norder sensitivity without relying on benchmark-specific datasets or human annotations. Rather than\nmeasuring model accuracy against known ground truth labels, we choose a simple transformation\nthat can be applied to text. We then measure the level of invariance that a model\u2019s output has under\nthat transformation. If we choose our transformations carefully, we can obtain useful information\nabout model behavior in a completely self-supervised way.\nMore concretely, given a corpus D (e.g., Wikipedia), we construct pairs of original passages/sentences\nx, and transformed counterparts x\u2032. An example is seen in Figure 1, where we negate the original\nsentence x to construct x\u2032. X is the set of all transformed pairs. We then feed input pairs into\nthe language model, f, to extract a pair of outputs. Depending on the construction, the output\nbeing considered can be the softmax probability vector over tokens, a perplexity score, or a feature\nvector. We then compare the outputs f(x) and f(x\u2032) using a similarity metric, M. Finally, we\naggregate the results over all pairs in the data corpus using an aggregation operator, A, to produce an\ninvariance/sensitivity score.\nSCORE = A{M(f(x), f(x\u2032)) \u2200(x, x\u2032) \u2208 X}.\n(1)\nIn this work, we bring wikipedia as our own dataset, but note that we do so to enable comparisons\nto existing metrics that use human labels on similar data. We use this methodology to study several\n1Efforts such as https://github.com/hitz-zentroa/lm-contamination are trying to catalog this\nphenomenon for ChatGPT.\n2\nToxicity\nKnowledge\nLong-Range\nWord Order\nTokenization\n0.75 0.5\n0.25 0.0\nPythia\n7B\n1.4B\n0.25\n0.5\n0.75\n1.0\n0.075\n0.15\n0.225\n0.3\n0.025\n0.05\n0.075\n0.1\n0.15\n0.1\n0.05\n0.0\nToxicity\nKnowledge\nLong-Range\nWord Order\nTokenization\n0.75 0.5\n0.25 0.0\nPythia (7B)\nInstruct\nVanilla\n0.25\n0.5\n0.75\n1.0\n0.075\n0.15\n0.225\n0.3\n0.025\n0.05\n0.075\n0.1\n0.15\n0.1\n0.05\n0.0\nFigure 2: Spider plots showing sensitivity scores for the Knowledge Probing via Negations, Toxicity, Context\n(Long-Range), Word Order, and Tokenization metrics introduced in the paper. A larger area corresponds to a\nbetter model in terms of the sensitivity scores. (Left) Comparison between Pythia 1.4B and Pythia 7B models.\nThe larger model performs better for all the metrics. (Right) Comparison between the instruction finetuned\nversion (Dolly-V2) and the vanilla Pythia model. The instruction finetuned model is better than the vanilla model\nfor all metrics except tokenization robustness.\ncase studies, namely knowledge via negations (Section 4), toxicity (Section 5), context sensitivity\n(Section 6), word order sensitivity (Section 7), and tokenization robustness (Section 8) culminating in\nsensitivity scores as seen in Figure 2. In practice, these metrics should not be constrained to this data\nsource, but evaluated directly on application-relevant sources.\n3\nRelated Work\nHELM adopts a multi-metric approach: accuracy, calibration, robustness, fairness, bias, toxicity, and\nefficiency over each of the datasets proposed [Liang et al., 2022]. These metrics build on the work\nof Ribeiro et al. [2020] and subsequent studies such as, [Mille et al., 2021, Wu et al., 2021, Ross\net al., 2021, Dhole et al., 2021, Yang et al., 2022] which augment inputs from a dataset to measure\nproperties beyond the classical metric of accuracy. While these methods rely on existing datasets and\nlabels, our method departs from these previous works as we analyze invariances using a data-agnostic\nprocedure.\nKnowledge Probing via Negation: The MMLU benchmark [Hendrycks et al., 2021] is widely\nused to assess the knowledge base of language models, evaluating their performance on task-specific\nmicro datasets. In production, the GPT-4 technical report [OpenAI, 2023] advertises the model\u2019s\ncapabilities across various knowledge categories, yet the evaluation suite used in the report is not\npublicly available. Furthermore, Wu et al. [2021] introduces a general-purpose counterfactual\ngenerator, Polyjuice, that allows for control over perturbation types and locations and is trained\nby finetuning GPT-2 on multiple labeled datasets of paired sentences. In contrast, we focus on\nevaluating the knowledge base of LLMs through invariances where no labeled data is required.\nNegations: Ettinger [2020] utilize psycholinguistic tests to explore the general linguistic knowledge\nand contextual impacts of negation in language models. Our evaluation method allows us to assess\nthe model\u2019s understanding and knowledge representation by examining its ability to handle negations\nwithout the need for in-domain labeled datasets or model finetuning.\nToxicity: RealToxicityPrompts is the most prominent benchmark for toxicity in LLMs [Gehman\net al., 2020]. This method relies on the Perspective API2 to score the model\u2019s generation based\non a series of prompts. This API is also used as the toxicity metric for HELM. However, with the\nproprietary API constantly changing, comparing evaluations across time is difficult [Pozzobon et al.,\n2023]. Another common benchmark is BOLD [Dhamala et al., 2021]. BOLD trains another model to\nclassify toxic generations. This approach of utilizing another model to measure toxicity is common\n[Sun et al., 2022]. Our approach differs from these methods as we do not build a dataset nor rely on\nauxiliary models to classify the generations.\n2https://perspectiveapi.com/\n3\nWord Order: While previous efforts have made significant contributions to testing the compositional\nand word order understanding of language models [O\u2019Connor and Andreas, 2021, Thrush et al.,\n2022], these efforts predominantly rely on small sets of hand-crafted examples. Moreover, these tests\noften encompass a wide range of knowledge types, making it challenging to isolate and evaluate the\nspecific role of word order knowledge. Our work aims to investigate the word order sensitivity of\nLLMs from the lens of invariance in a data-agnostic manner.\nLong-Range Dependency: As conversational AI models become more prevalent [Ouyang et al.,\n2022, Anthropic, 2023b], the importance of accommodating large context lengths has become evident.\nRecent endeavors have focused on developing chat models with extensive context capabilities, such as\n32k and 100k [OpenAI, 2023, Anthropic, 2023a], utilizing techniques like memory-efficient attention\n[Dao et al., 2022]. However, it is equally crucial to gauge how far back into the context the model truly\noperates and can refer to. LAMBADA [Paperno et al., 2016], addresses this by assessing language\nmodels\u2019 comprehension of broad contexts. In contrast, our self-supervised approach creates texts\nthrough closed-form transformations that evaluate language models\u2019 grasp of long-range sensitivity.\nTokenization Sensitivity: HELM approaches this problem by inducing spaces, misspellings, etc.\nover the datasets in question to determine if these slight changes can affect changes when evaluating\nover established datasets [Liang et al., 2022]. Additionally, Rumbelow and Mwatkins [2023] found\na set of anomalous tokens which result in a previously undocumented failure mode for GPT-2 and\nGPT-3 models. Inspired by these works, we designed a test to see how the same text tokenized\ndifferently affects model behavior without changing the underlying text.\n4\nKnowledge Probing via Negations: Au Contraire Metric\nThis section presents a simple self-supervised evaluation for knowledge probing. Knowledge probing\nin specific target domains is an important way to assess how a model will behave in different\ndeployment scenarios. OpenAI approached this problem by constructing nine adversarial datasets\non varying areas such as Law and Technology to evaluate GPT-4 [OpenAI, 2023]. While OpenAI\u2019s\napproach and others like MMLU [Hendrycks et al., 2021] are a step forward, these datasets do\nnot cover all possible domain-specific areas. Therefore, when deploying a model, it is important\nto understand its ability to comprehend the potentially narrow domain-specific information of its\nuse case. We probe this capability by testing whether the model is actually surprised (in terms of\nperplexity) by negated facts in a target domain.\nSelf-Supervised Approach:\nWe construct a simple transformation over factual infor-\nmation like definitions by applying negations to facts.\nThis is done in a trivial\nself-supervised way:\nWe search for the first occurrence of is,\nwas,\nor were,\nand\nplace the word not after it provided a negation is not already present.\nFor ex-\nample, given the fact \u201cApril is the fourth month of the year in the Julian and\nGregorian calendars and comes between March and May\u201d, we apply the negation trans-\nformation to this sentence and construct: \u201cApril is not the fourth month of the year\nin the Julian and Gregorian calendars and comes between March and May\u201d.\nBased on this intervention, we measure the change in the log-perplexity (log(ppl(x))), between the\noriginal and negated sentence. Formally, we define the sensitivity score as the following:\nSENSITIVITY SCORE = 1\nn\nn\nX\ni\nlog(ppl(x\u2032\ni)) \u2212 log(ppl(xi)).\nOne possible confounding variable is how sensitive a model is to the term \u201cnot\u201d in a sentence. One\nway to normalize this behavior is to approximately measure the model\u2019s sensitivity to \u201cnot\u201d over a\nbenign corpus, where the meaning of \u201cnot\u201d should not have a sizable impact on the perplexity over\nsentences nor have a known expected direction:\nNORMALIZED SENSITIVITY SCORE = sensitivity score \u2212 1\nm\nm\nX\ni\n| log(ppl(y\u2032\ni)) \u2212 log(ppl(yi))|,\nwhere y is a sample from a benign corpus like bookcorpus with m total samples for which there\nis not a clearly defined truth value. Note that we use the absolute value of the difference, as\nit is unclear which direction is expected from the model for a given input in the benign cor-\npus. To evaluate the relationship of these metrics to model confidence in our analysis, we also\n4\nrecord the fraction of inputs for which perplexity decreases after introducing a negation, which\nrepresents, for a typical sample, the error that the model is making: PERCENT PPL DROPS =\n1\nn\nPn\ni max {sign(log(ppl(xi)) \u2212 log(ppl(x\u2032\ni))), 0}.\n4.1\nExperimental Set-up\nTo verify that this self-supervised evaluation is sensible, we compare our method to accuracy on\nTriviaQA, as both evaluations gauge an LLM\u2019s world knowledge [Joshi et al., 2017]. We do not\npenalize the length of the output. More details on exactly how we calculate accuracy can be found\nin the Appendix. Since TriviaQA asks general knowledge questions, we apply our self-supervised\nmetric to topic sentences from Wikipedia to get a comparable general knowledge score. A human\ninspection of 100 samples verified that the proposed transformation resulted in grammatically correct\nsentences that were counterfactuals for the original sentence. To calculate our metric, we measure the\nsensitivity score over 1000 examples, where the standard error for these scores was less than 0.002.\nSince we use perplexity, we can also utilize API models, such as those from OpenAI and Cohere, and\npublicly available models from the Hugging Face Hub, such as Pythia and GPT-2 [Biderman et al.,\n2023, Brown et al., 2020, Radford et al., 2019]. A full list of the models we evaluate can be found in\nthe Appendix. We run all models greater than 6B parameters in their FP16 configuration.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTriviaQA Acc.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nSensitivity Score\nSensitivity Score vs TriviaQA Acc.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTriviaQA Acc.\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nSensitivity Score (Normalized)\nSensitivity Score (Normalized) vs TriviaQA Acc.\nModel Family\nOpenAI Instruct\nOpenAI\nCohere\nCohere Instruct\nLlama\nVicuna\nMPT\nMPT Instruct\nPythia\nDolly V2 Instruct\nGPT-J\nDolly V1 Instruct\nNeo\nGPT-2\nInstruction\nNo\nYes\nFigure 3: (Left) Sensitivity Score (negations) compared to accuracy on TriviaQA over various model sizes and\nfamilies. (Right) Normalized Sensitivity Score compared to accuracy on TriviaQA over various model sizes and\nfamilies. Larger markers correspond to bigger models, and \u201cx\u201d markers represent instruction finetuned models.\n4.2\nResults\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTriviaQA Acc.\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\nPercent PPL Drops\nPercent PPL Drops vs TriviaQA Acc.\nModel Family\nOpenAI Instruct\nOpenAI\nCohere\nCohere Instruct\nLlama\nVicuna\nMPT\nMPT Instruct\nPythia\nDolly V2 Instruct\nGPT-J\nDolly V1 Instruct\nNeo\nGPT-2\nInstruction\nNo\nYes\nFigure 4: Percentage of samples where perplexity drops\nversus accuracy on TriviaQA. We observe a reliable\nnegative correlation.\nFigure 3 shows that the self-supervised SEN-\nSITIVITY SCORE, which measures the change\nin log(ppl) over the pair of sentences, closely\ntracks accuracy on the human-curated Trivi-\naQA dataset, especially for non-instruction fine-\ntuned models. It also maps closely to a square-\nroot relationship, with normalization further im-\nproving this trend. Normalization corrects the\ninstruction-tuned models to a larger degree, pos-\nsibly due to their innate overconfidence. We can\nfurther hone in on why correct normalization\nis important by cross-referencing the frequency\nwith which perplexity goes down rather than up,\nin Figure 4. This ablation metric is robust to out-\nlier perplexity values. Here, instruction-tuned\nmodels are well-behaved. Further, we notice\nthat outliers in Figure 3 are indicative of important model properties and weaknesses of the TriviaQA\nbenchmark. For example, consider Cohere\u2019s instruction model (Cohere command), which has low\nsensitivity score relative to its TriviaQA performance and appears as a dark turquoise \u201c\u00d7\u201d on the\nmiddle right of the chart, and text-ada-001 (OpenAI\u2019s smallest instruction model), which appears as\nan orange \u201c\u00d7\u201d on the upper left side of the chart. To investigate these outliers further, we applied\nnegations to questions in TriviaQA and found that Cohere command model rarely changed its answer\nwhen a negation was introduced, whereas text-ada-001 changed its answer frequently. We show\n5\nTable 1: Example outputs of text-ada-001, text-davinci-003 and Cohere command. These examples are selected\nwhere text-ada-001 would produce a sensible answer to both the original question and the negated question. The\nCohere model is sometimes entirely insensitive to negations, compared to the OpenAI models, although even\ntext-davinci can fail at this task. This trend was observed over several generations, from which we show two\nqualitative examples here.\nModel\nOriginal\nTransformed\nQuestion\nA sterlet is what type of creature?\nA sterlet is not what type of creature?\ntext-ada-001\nA sterlet is a creature that has\na spiny body and a long, sharp tongue.\nA sterlet is not a creature.\ntext-davinci-003\nA sterlet is a type of sturgeon.\nA sterlet is a type of sturgeon.\nCohere command\nFish\nFish\nQuestion\nWhat is the only natural food\nthat never goes bad?\nWhat is not the only natural\nfood that never goes bad?\ntext-ada-001\nThe only natural food that\nnever goes bad is sugar.\nThere is no one natural food that\nnever goes bad. There are, however,\nsome foods that are more likely to do so.\nThese include: milk, yogurt, ice cream,\nand cake.\ntext-davinci-003\nHoney.\nThere is no single natural food\nthat never goes bad.\nCohere command\nHoney never goes bad.\nHoney never goes bad.\nexamples of this behavior in Table 1. This implies that the Cohere model is insensitive to sentence\nstructure when the negation is present \u2013 it has memorized the associations between concepts and\nanswers based on the context alone, even if the construction of the question makes its answer incorrect.\nThis inability to answer grammatically complex questions is not reflected in the TriviaQA results,\ndue to its reliance on simple sentence structures and nearly uniform question formats. Text-ada-001\nis the opposite, it is exceedingly sensitive to sentence structure and nearly always flips its answer\nwhen faced with a negation. This also highlights a weakness of TriviaQA \u2013 its simple and predictable\nsentence constructs yield a benchmark that rewards correct concept associations rather than correct\nanswers.\nIn summary, we find that we can predict benchmark performance exceedingly well with a simple\nself-supervised scheme, validating the effectiveness of this metric.\nEffect of Instruction Finetuning: In general, we find that instruction-tuned models are more\nsensitive to negations than other LLMs as seen in Figure 5, regardless of the source of instruction\ndata. The outlier here is again the Cohere command model, which is less sensitive than Cohere\u2019s base\nmodel after finetuning.\nLimitations: For the sensitivity score to measure truthfulness, the dataset being used must contain\na large fraction of sentences whose truth value is true, rather than neutral or false. This is likely to\nhold for many corpora, if only to varying degrees. As such, this metric might be less meaningful\non a fan-fiction corpus, but more meaningful on a collection of medical or legal textbooks. Finally,\nPythia (2.8B)\nPythia (6.9B)\nGPT-J (6B)\nMPT\nLLaMA\nAda\nBabbage\nCurie\nDavinci\nCohere (XL)\nBase Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSenstivity Score\nDifferent Instruction Finetuning Methods (Negations)\nNone\nHuman Curated\nSelf-Instruct\nMix\nShareGPT\nRLHF\nCohere Instruct\nFigure 5: Sensitivity Score (negation) comparing pretrained LLMs with their instruction finetuned counterparts.\nIt can be seen that on average, instruction finetuning increases the Sensitivity Score.\n6\nwe chose a simple construction for the negation transform and found it to be effective. LLMs like\nChatGPT could be utilized to construct the counterfactual sentence when doing so would otherwise\nbe non-trivial. However, our simple construction has the benefit of reproducibility, as it does not\ndepend on a commercial model that may change over time.\n5\nToxicity: F-Bomb Metric\nBefore LLMs are deployed in commercial settings, for example as a customer service chatbot, it is\nimportant to audit their potential to produce profanity or other toxic language. Most methods for\nmeasuring toxicity involve feeding an LLM toxic prompts and then analyzing the outputs using a\nblack-box commercial tool (e.g., the Perspective API) or an additional trained model (usually an\nencoder). However, using a model to measure the generation may be problematic. For example,\nalthough work like Fortuna et al. [2020] has tried to understand how the Perspective API classifies\ntoxic text, the API continues to change, and as it changes our understanding of how toxic generations\nare being classified starts to dissipate [Pozzobon et al., 2023].\nSelf-Supervised Approach: One simple and reproducible approach is to analyze toxic generation\nthrough invariance. We will construct a metric that quantifies how stoic the model is to profanity,\ni.e., whether the model will respond to profane and aggressive comments with its own profanity or\naggression. Although we study profanity, this can be extended to other forms of toxicity as well, or\nmore broadly to model behaviors, such as tone, that the model should not mimic from user queries.\nPythia (2.8B) Pythia (6.9B)\nGPT-J (6B)\nMPT\nLLaMA\nLLaMA\nBase Model\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nFrac. of Generation Toxic (Ours)\nDifferent Instruction Finetuning Methods (Toxicity)\nNone\nHuman Curated\nSelf-Instruct\nMix\nShareGPT\nShareGPT (Uncen.)\nFigure 6: The fraction of generated outputs that are\ntoxic (i.e., the proposed metric) for instruction finetuned\nmodels and their pretrained counterparts. The right-\nmost bar is WizardLM, which is trained on uncensored\nShareGPT data, containing toxic content.\nTo build a self-supervised evaluation, we trans-\nform each of our prompts by \u201cF-bombing:\u201d we\nappend trigger words (e.g., \u201cF**k F**K\u201d) at\nthe end of the original text. We then feed these\nmodified prompts into the model, examining the\ngenerated output.\nWe construct two sensitivity scores: one that an-\nalyzes open-ended generated text, and another\nthat analyzes the softmax probability distribu-\ntion for the next predicted token. For the text\ngeneration score, we examine the output to de-\ntermine if a profane word from the LDNOOBW\ndataset3 is contained in the generated text. The\nsoftmax score metric computes the next-token\npredictions at each position in a block of text\nand sums the probabilities of an LDNOOBW\nword occurring. We validate that our measure-\nment procedure is sensible by comparing our\nmetric with Perspective API toxicity scores over the generations that contained the appended\ntrigger phrase. This approach enables us to understand the model\u2019s stoicism towards profanity and\nextends to other forms of toxicity.\n5.1\nExperimental Set-up\nWe employ this self-supervised approach using the wikipedia dataset, which we split into prompts\non a per-sentence level. For the generated text, we report the fraction of times a generation contains a\n\u201cbad\u201d word, minus the fraction of times the prompt contains a bad word. For the next word prediction\nmetric, we compute the change in the average probability of a profane word between x\u2032 and x. We\ncompared our metric with the Perspective API [Fortuna et al., 2020] toxic scores on the prompts.\nWe consider a generation toxic when the toxicity score is greater than or equal to 0.5. We run all\nmodels greater than 6B parameters in their FP16 configuration. The full list of models can be found\nin the Appendix.\n3https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/\nblob/master/en\n7\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nFrac. of Generation Toxic (Perspective API)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nFrac. of Generation Toxic (Ours)\nFrac. of Gen. Toxic (Ours) vs\n Frac. of Gen. Toxic (Perspective API)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nFrac. of Generation Toxic (Perspective API)\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n in Bad Word Logits (Ours)\n in Bad Words Logits (Ours) vs\n Frac. of Gen. Toxic (Perspective API)\nFamily Class\nLlama\nPythia\nGPT-2\nGPT-J\nMPT\nNeo\nFigure 7: (Left) The fraction of toxic generations under our metric versus Perspective API. (Right) The\nchange in the next-token probability of a profane word (token) versus the fraction of toxic generations according\nto Perspective API.\n5.2\nResults\nThe results of our toxicity metric, evaluated in both text space and logit space, are presented in\nFigure 7. The figures clearly demonstrate a close correlation between our metric, which measures the\nfraction of generated toxic word counts and the change in probabilities over the profane words, and\nthe toxicity scores obtained from the Perspective API. We conducted tests using models of different\ntypes and scales (Figure 6 and Figure 7). Furthermore, from Figure 7, there appears to be no relation\nbetween the sensitivity of models to profane words and model size.\nEffect of Instruction Finetuning: From Figure 6, we see that seems to be no effect on average of\ninstruction finetuning compared to their pretrained counterparts over the six models examined. The\nLLM with the lowest score is Dolly-V2 (7B), making it the least toxic model with respect to both our\nscores. Additionally, we see that MPT-Instruct is less toxic, which we suspect is due to the Harmless\nand Helpful dataset from Anthropic the model was trained on [Bai et al., 2022]. Furthermore, we\nsee that WizardLM, which is trained on an uncensored version of ShareGPT, is more toxic than a\nmodel trained on the filtered version of ShareGPT. While Ouyang et al. [2022] reported that RLHF\ndecreases the toxicity of the model, this is ultimately highly dependent on the composition of the\nfeedback data used to train the RLHF reward function.\nLimitations: Our analysis focuses on explicit profanity and may not capture nuanced forms of\ntoxicity beyond explicit language. We rely on predefined lists of profane words, which may not\nencompass all variations of toxicity. The effectiveness of our metric and the model\u2019s stoicism could\nvary with different datasets and prompt distributions.\n6\nContext (Long-Range) Sensitivity: Back to the Future Metric\nAs LLM context window sizes have increased in recent models, it is important to understand how\nchanges in the previous context can affect the representations and generation across long ranges.\nDatasets like Long-Range Arena [Tay et al., 2020] offer a very broad set of tasks, focusing on context\nlengths ranging from 1k to, 16k and aim to evaluate architectural choices. There are other datasets\nlike LAMBADA that focus on the capability to successfully predict the conclusion to a paragraph\n[Paperno et al., 2016]. The dataset is designed such that the prediction of the word is clear given\nthe full context, but it is impossible to predict given just the last sentence. This measures an LLM\u2019s\nability to comprehend text beyond locally attending to a sentence.\nSelf-Supervised Approach: We can utilize self-supervised evaluation to understand how the model\u2019s\npredictions change when a prior sentence or multiple sentences from a passage are altered. We\nconduct this test by taking three sentences from a stream of data in order and replacing the first two\nsentences with two random sentences from the corpus. For example, if the original passage had three\nsentences, {S3, S2, S1}, where S3 is the first sentence of the input passage, then the altered passage\nwould be {S\u2032\nX, S\u2032\nY , S1}, where S\u2032\nX, S\u2032\nY are random sentences from another passage in the corpus. A\nmore concrete example can be found in the Appendix. We then look at the probability distribution at\neach position of S1 for both x and x\u2032, and compare them using the Jensen\u2013Shannon divergence. This\nis to determine how the representations of the last sentence change as different context is presented.\n8\nThe Jensen-Shannon divergence (JSD) is a symmetric variation of KL-divergence, defined as:\nJSD(P||Q) = 1\n2KL(P||M) + 1\n2KL(Q||M), where M = 1\n2(P + Q).\nFor our invariance/sensitivity score, we take the mean of JSD over the last sentence, averaging over\nall samples. Concretely,\nLRS SCORE = 1\nn\nn\nX\ni\n1\nm\nm\nX\nj\nJSD(f(xi\nj)||f((x\u2032)i\nj)),\nwhere m represents the sentence length and xi\nj is the ith sample in the set at token position j in the\nlast sentence.\n6.1\nExperimental Set-up\nFor this sensitivity test, we compare our method to LAMBADA using EleutherAI\u2019s Language Model\nEvaluation Harness [Gao et al., 2021]. It is worth noting that the tests here are different. The\nLAMBADA dataset measures long-range dependency on fiction and its ability to comprehend the\nprevious passage. On the other hand, we analyze the invariance of the probability distributions over\nthe last sentence when the passage has been altered. To calculate our metric, we use the same corpus\nas the other tests and calculate over 1000 examples with the standard error 2e\u22123 of the mean value\nrecord. We report the JSD for a range of models including Pythia, Neo, GPT-2, and others. We run\nall models greater than 6B parameters in their FP16 configuration.\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nLAMBADA (OpenAI)\n0.16\n0.17\n0.18\n0.19\n0.20\n0.21\n0.22\n0.23\nLRS Score\nLRS Score vs Lambada (OpenAI)\nModel Family\nPythia\nGPT-J\nLLaMA\nMPT\nNeo\nOPT\nGPT-2\nPythia (2.8B)\nPythia (6.9B)\nGPT-J (6B)\nMPT\nLLaMA (7B)\nBase Model\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nLRS Score\nDifferent Instruction Finetuning Methods (LRS)\nNone\nHuman Curated\nSelf-Instruct\nMix\nShareGPT\nFigure 8: Left LRS Score vs LAMBADA (OpenAI) across various model sizes and families. Right LRS Score\nof instruction finetuned models and their pretrained counterparts.\n6.2\nResults\nFrom Figure 8 (Left), we see that as our LRS Score increases, the model performs better on LAM-\nBADA. Furthermore, bigger models generally tend to be more sensitive to changes in the context.\nWe see that Pythia and GPT-J are more sensitive to changes in the context compared to MPT and\nLLaMA. Whereas, smaller models like Pythia-70M and GPT-2 small produce a lower LRS Score.\nEffect of Instruction Tuning: On average, we see that instruction-finetuned models are more\nsensitive to changes in context than their pretrained counterparts, suggesting that they may be\nsensitive to long-range changes (beyond locally attending to a sentence). Moreover, we find this\ngain appears independent of base model size. Both the smaller and larger Pythia base models have a\nsimilar sensitivity, and finetuning on Dolly-V2 (\u201chuman-curated\u201d in Figure 8) leads to a similar gain\nin sensitivity.\nLimitations: Although we are analyzing long-range sensitivity in token probability space, for\ntransformers in particular, analyzing attention probabilities may be more effective. However, to make\nthe metric applicable to generic architectures, including RNNs, LSTMs, efficient attention variants,\netc., we believe that the token probability space is more appropriate.\n9\n7\nWord Order: Word Salad Metric\nClose adherence to word order is a requirement for accurate factual responses beyond simple comple-\ntions based on associative recall. Large Language Models have an incredible ability to understand as-\nsociation but have been shown to lack the necessary representations for certain types of reasoning. One\nof many potential reasons for this is their occasional inability to understand word order. Yuksekgonul\net al. [2023] showed that multimodal models trained on image captions exhibit this behavior. People\nhave also demonstrated that BERT can often behave like a bag-of-words classifier [Juneja et al., 2023].\nSelf-Supervised Approach: To evaluate a model\u2019s sensitivity to word order, we utilize sentences\nfrom a given corpus and apply a transformation where two random words are swapped in each\nsentence, creating modified versions denoted as x\u2032. Next, we analyze the impact of word order\nchanges on the model\u2019s predictions by examining the predicted token softmax probability distribution\nfrom the original sentence x and its modified counterpart x\u2032. Specifically, we examine the JSD\nbetween the two distributions to quantify the divergence in attention or focus resulting from the\nrandom word swaps in x\u2032. Since there are no datasets that study word order, we compare our\nself-supervised approach to the LRS Score established in the previous section.\nWORD ORDER SCORE = median{JSD(f(x)j+1||f(x\u2032)j\u2032+1) \u2200(x, x\u2032) \u2208 X},\nwhere j is the last token for the input sequence for x and j\u2032 is the last token for x\u2032.\n0.18\n0.19\n0.20\n0.21\n0.22\nLRS Score\n0.025\n0.030\n0.035\n0.040\n0.045\n0.050\n0.055\n0.060\n0.065\nWord Order Score\nWord Order Score vs LRS Score\nModel Family\nLLaMA\nMPT\nPythia\nGPT-J\nNeo\nGPT-2\nPythia (2.8B)\nPythia (6.9B)\nGPT-J (6B)\nMPT\nLLaMA (7B)\nBase Model\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14\nWord Order Score\nDifferent Instruction Finetuning Methods (Word Order)\nNone\nHuman Curated\nSelf-Instruct\nMix\nShareGPT\nFigure 9: (Left) Word Order Score vs LRS Score across various model sizes and families. (Right) Word Order\nScore of instruction finetuned models and their pretrained counterparts.\n7.1\nExperimental Set-up\nFor this experiment, we take our corpus and break it down into sentences. Then, for every sentence,\nwe swap two random words (not tokens) to construct our x\u2032 over 5000 examples. Due to the\nlong-tailed distribution in scores that were observed over the 5000 examples, we report the median,\nas described. For reference, if we had computed the mean, we would observe a standard error 2e\u22123.\nWe report the median JSD for each model, again including Pythia, Neo, GPT-2, and others. We\nrun all models greater than 6B parameters in their FP16 configuration.\n7.2\nResults\nFrom Figure 9 (Left), we can see that there is a positive correlation between Word Order Score\nand LRS Score. The higher the Word Order Score, the higher the LRS Score. Nevertheless, we can\nsee that there appears to be a plateau for Word Score. Similar to the LRS Score, we see that larger\nmodels are more sensitive to word order, with the Mosaic MPT-7B and GPT-J model being the most\nsensitive to word order.\nEffect of Instruction Finetuning: Figure 9 (Right) shows that most instruction finetuning approaches\nmake the model more sensitive to word order over the five models studied. Particularly, we see\nthat only finetuning on the human-curated databricks-dolly-15k seems to make the model more\nsensitive irrespective of the size.\n10\nLimitations: For this Word Order Score, we make the assumption that the next token prediction\nwhen swapping two words randomly is a good proxy to measure a model\u2019s sensitivity to word order.\n8\nTokenization Sensitivity: Broken Token Metric\nText pre-processing is rarely perfect. Raw text often contains extra spaces, weird formatting, and\nother quirks that affect how the tokenization of the text occurs. HELM explored some of these\nphenomena [Liang et al., 2022]. Others, such as Rumbelow and Mwatkins [2023], found anomalous\ntokens that represent failure modes in GPT-2 and GPT-3 models, showing that our understanding of\nhow different tokenization impacts the model behavior is still limited.\nSelf-Supervised Approach: To quantify this phenomenon, we randomly chop strings of raw input\ntext at regular intervals of x, and then we tokenize each of the chopped strings independently. This\nway, we mimic a \u201cbroken\u201d tokenization, that might occur in the pretraining corpus due to document\nbreaks and misspellings. A broken tokenization can also occur during model generation when\nincomplete user input is provided [Microsoft, 2023]. After tokenizing each chopped string separately,\nwe concatenate these tokenizations back together. Note that the original content is unchanged \u2013 the\nalternative tokenization still decodes to the same raw input text. We then compare the concatenation\nof chopped tokenization to the original text over the next token prediction using JSD, similar to our\nWord Order Metric.\nTOKENIZATION SENSITIVITY SCORE = 1\nn\nX\nJSD(f(x)j+1||f(x\u2032)j\u2032+1)\n8.1\nExperimental Set-up\nFor this experiment, we take our corpus and break it down into sentences. Then, for every sentence,\nwe apply our procedure (described above) to construct x\u2032 over 1000 examples. We report the mean\nJSD for each different model like Pythia, Neo, GPT-2, and others, where the standard error is about\n5e\u22123 for all models. We run all models greater than 6B parameters in their FP16 configuration. Here,\nwe specifically explore a split stride of 5, splitting every 5th character.\n8.2\nResults\nFrom Figure 10 (Left), we see that MPT and LLaMA are the least sensitive (lower is better) to changes\nin token inputs. More broadly, we observe a negative trend with training FLOPs (i.e increasing the\nFLOPs decreases the sensitivity to tokenization changes). We suspect that as the amount of training\nincreases, alternative tokenizations are more likely to be observed, and invariance to these abnormal\ntokenizations increases. This is supported by measurements on the OPT models, which are strong\noutliers in the trend observed above. Each of these models was trained on only 180B tokens, less than\n1021\n1022\nApprox. FLOPS\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nTokenization Sensitivity Score\nTokenization Sensitivity Score vs FLOPS\nModel Family\nLLaMA\nMPT\nPythia\nGPT-J\nNeo\nGPT-2\nOPT\nPythia (2.8B)\nPythia (6.9B)\nGPT-J (6B)\nMPT\nLLaMA (7B)\nBase Model\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nTokenization Score\nDifferent Instruction Finetuning Methods (Tokenization)\nNone\nHuman Curated\nSelf-Instruct\nMix\nShareGPT\nFigure 10: (Left) Tokenization Sensitivity Score with a split stride of five versus Approx. FLOPS \u2013 lower is\nbetter. Note that the OPT models have seen the fewest tokens during training, c.f. Figure 22. (Right) Impact of\ndifferent instruction-tuned methods.\n11\na fifth of the tokens seen by MPT and LLaMA (1 Trillion) and about half of what GPT-2, GPT-Neo,\nand Pythia have seen. We include Figure 22 for a variant of Figure 10 in terms of tokens observed\nduring training in the appendix.\nEffect of Instruction Finetuning: Figure 10 (Right) shows the impact of different instruction\nfinetuning methods. In contrast to previously observed metrics, there seems to be no reliable trend in\ntokenization robustness after instruction finetuning. Furthermore, even when only model size differs\n(Dolly-V2s) the instruction finetuned dataset can have a different impact on this metric. It is worth\nnoting that the Dolly-V2s were only trained on 15k instructions.\nLimitations We test a limited type \u2013 character splits \u2013 of tokenization error, particularly the same text\njust being processed differently by the tokenizer. There are additional tokenization errors to consider\nas well, based on minor edits of the raw input text (i.e explicit word splits, extra spaces, unusual\npunctuation, etc), that could also be considered. Additionally, we examined the change in the next\ntoken probabilities, as we believe it is a good proxy to measure this phenomenon.\n9\nDiscussion\nIn this paper, we introduce a new framework for Self-Supervised Evaluation for LLMs using sensitivity\n(invariance) metrics. We show that sensitivity measurements like the ones explored in this paper \u2013\nknowledge via negations, toxicity, context, word order, and tokenization robustness \u2013 can correlate\nwith existing evaluation datasets, as we verify for the knowledge and context sensitivity metrics. We\nconclude that sensitivity metrics can provide meaningful insights into model behavior, which we also\nverify qualitatively in our study of the Cohere command model. Additionally, we see generally, except\nfor toxicity, that larger models have better sensitivity scores compared to smaller models, mirroring\nother benchmarks that verify that model performance generally increases with scale. However, there\nare still things to consider when analyzing these models using Self-Supervised Evaluation, which we\nwill outline in this section.\nFor example, in some instances like text-ada-001 in knowledge, we see that being more sensitive\nis a byproduct of some other phenomena. Similarly, it may be that certain models are just insensitive\nin general to any transformations. This may be the case for tiny models, like the toxicity metric for\nGPT-2 (small) and the tokenization metric for Pythia-160M. This implies that there is a lower limit\nof model size where certain sensitivity metrics cease to be meaningful predictors of model qualities.\nModel Entropy. The entropy of a model\u2019s output distribution can impact many aspects of text\ngeneration.\nA lower entropy may require a more aggressive sampling strategy for text gen-\neration to achieve a diverse set of generations from the model, or might indicate a miscali-\nbration of the output distribution. Similarly, the model\u2019s entropy can affect sensitivity scores.\n2.75\n3.00\n3.25\n3.50\n3.75\n4.00\n4.25\n4.50\n4.75\nSentence Entropy\n3.6\n3.7\n3.8\n3.9\n4.0\n4.1\n4.2\nNext Token Entropy\nNext Token Entropy vs Sentence Entropy\nModel\nPythia (70M)\nPythia (160M)\nGPT-2 (small)\nGPT-Neo (125M)\nGPT-2 (medium)\nGPT-2 (large)\nOPT (1.3B)\nGPT-Neo (1.3B)\nOPT (2.7B)\nGPT-2 (XL)\nGPT-Neo (2.7B)\nPythia (1.4B)\nPythia (2.8B)\nPythia (6.9B)\nGPT-J (6B)\nMPT (7B)\nLLaMA (7B)\nFigure 11: Plot showing the next token prediction\nShannon entropy (y-axis) and mean token Shannon\nentropy (x-axis) over sentences on Wikipedia. We find\nthat LLaMA (7B) has the lowest entropy over the next\ntoken and mean token over a sentence.\nIf the entropy of the model is low, then the sen-\nsitivity may naturally be lower as well. The\nexact impact of the model\u2019s entropy on these\nsensitivity scores and how to appropriately in-\ncorporate it into invariances/sensitivity scores\nshould be explored in future work. Figure 11\nshows the Shannon Entropy of the Next To-\nken Prediction and Sentence Entropy (the mean\ntoken entropy over a sentence of the model).\nWe use the Wikipedia (our corpus) sentences\nto calculate the Shannon Entropy, defined as\nH(x) = \u2212 P p(x) log(p(x)). From Figure 11,\nwe see that LLaMA has the lowest entropy on\nboth the next token and mean token over a sen-\ntence, with large models having a lower entropy\nthan smaller models on average. This may par-\ntially explain why the sensitivity scores for LLaMA are lower. 4\n4Vocabulary size does play an additional role in the entropy of a model. For example, in a completely uniform\ndistribution, the Shannon Entropy of a model with a smaller vocabulary size will be smaller than another model\nwith a larger vocabulary size.\n12\nToxicity\nKnowledge\nLong-Range\nWord Order\nTokenization\n0.75 0.5\n0.25 0.0\nLLM (7B)\nLLaMA\nMPT\nPythia\n0.25\n0.5\n0.75\n1.0\n0.075\n0.15\n0.225\n0.3\n0.025\n0.05\n0.075\n0.1\n0.15\n0.1\n0.05\n0.0\nToxicity\nKnowledge\nLong-Range\nWord Order\nTokenization\n0.75 0.5\n0.25 0.0\nInstruction (7B)\nVicuna\nMPT-Instruct\nDolly V2\n0.25\n0.5\n0.75\n1.0\n0.075\n0.15\n0.225\n0.3\n0.025\n0.05\n0.075\n0.1\n0.15\n0.1\n0.05\n0.0\nFigure 12: (Left) shows LLaMA, MPT, and Pythia sensitivity scores across the five metrics studied in this paper.\n(Right) shows the instruction-tuned counterparts of these models across the five metrics. The more area that is\ncovered, the better the model according to our SSE scheme. All models 7B were run in FP16 configurations.\nMemorization. Machine learning evaluation benchmarks for studying statistical generalization\nalmost always assume idealized train and test set separation. However, in reality, some amount of\noverlap often exists in modern web-scale pre-training corpora. As a result, there have been various\nefforts to measure and address the impact of these overlaps on the training and evaluation of large\nmodels [Brown et al., 2020, Gao et al., 2021]. Investigating the same relationship, purely from a\ntraining support perspective, Kandpal et al. [2022] showed that a language model\u2019s ability to answer\na fact-based question relates to how many documents associated with that question were seen during\npre-training. In a different but fundamentally related line of work, Carlini et al. [2022] demonstrated\nthat LLMs regurgitate training data in specific scenarios, often based on repetition rates in training\ncorpora. Further, their own prior work [Carlini et al., 2020] quantifies the underlying relationship\nbetween train and test data in yet another way by showing that simple loss-based membership\ninference methods are capable of discriminating whether a test query was present in the training\ndataset. In the context of sensitivity scores, this collection of results in the literature suggests that\nit is hard to make strong statements about whether training-time exposure to certain documents or\ntoken sequences would confound the trends observed in our proposed sensitivity metrics. We leave a\ndetailed analysis of the interactions between memorization behaviors based on training data and our\nsensitivity metrics for future research. We suspect that developing a more complete understanding of\nthese interactions is an important step towards more informative and robust sensitivity metrics.\nAn advantage of self-supervised sensitivity scores is that we can circumvent the potential effects of\nmemorization by evaluating sensitivities on novel text, i.e., the latest news articles, as no labeling and\nadditional curation of data sources is required. With this strategy, the possibility of memorization can\nbe eliminated.\n10\nConclusion\nWe introduce a procedure for self-supervised evaluation by analyzing invariances for Large Language\nModels. The key advantage of self-supervised evaluation is that it removes the need to laboriously\nlabel new data, leading to more efficient forms of evaluation in real deployment settings. We\nshowcase several case studies, where we empirically validate this approach to be reliably tracking\nexisting supervised metrics. Additionally, there are a number of future questions to consider when\nmeasuring a model\u2019s sensitivity that we did not fully explore yet \u2013 like entropy and memorization.\nNevertheless, these self-supervised evaluation approaches have the potential to measure properties\nbeyond what is currently capable of the traditional dataset approach \u2013 like sensitivity to word order.\nWe hope that this is only a starting point for self-supervised metrics in the future that can lead to\na deeper understanding of how LLMs behave and complement classical supervised benchmarks.\n13\n11\nAcknowledgements\nThis work was made possible by the ONR MURI program, the Office of Naval Research\n(N000142112557), and the AFOSR MURI program. Commercial support was provided by Capital\nOne Bank, the Amazon Research Award program, and Open Philanthropy. Further support was\nprovided by the National Science Foundation (IIS-2212182), and by the NSF TRAILS Institute\n(2229885).\nReferences\nAnthropic. Introducing 100k context windows, May 2023a. URL https://www.anthropic.com/\nindex/100k-context-windows.\nAnthropic.\nIntroducing claude, March 2023b.\nURL https://www.anthropic.com/index/\nintroducing-claude.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hal-\nlahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al.\nPythia: A suite for analyzing large language models across training and scaling. arXiv preprint\narXiv:2304.01373, 2023.\nAbeba Birhane, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. The\nvalues encoded in machine learning research. In 2022 ACM Conference on Fairness, Accountability,\nand Transparency, pages 173\u2013184, 2022.\nSamuel R Bowman and George E Dahl. What will it take to fix benchmarking in natural language\nunderstanding? arXiv preprint arXiv:2104.02145, 2021.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nN Carlini, F Tramer, E Wallace, M Jagielski, A Herbert-Voss, K Lee, A Roberts, T Brown, D Song,\n\u00da Erlingsson, et al. Extracting training data from large language models. arxiv. Preprint posted\nonline December, 14, 2020.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and\nChiyuan Zhang. Quantifying memorization across neural language models. arXiv preprint\narXiv:2202.07646, 2022.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness, 2022.\nJwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei\nChang, and Rahul Gupta.\nBold: Dataset and metrics for measuring biases in open-ended\nlanguage generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountabil-\nity, and Transparency, FAccT \u201921, page 862\u2013872, New York, NY, USA, 2021. Association\nfor Computing Machinery.\nISBN 9781450383097.\ndoi: 10.1145/3442188.3445924.\nURL\nhttps://doi.org/10.1145/3442188.3445924.\nKaustubh D Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li, Saad Mahamood,\nAbinaya Mahendiran, Simon Mille, Ashish Shrivastava, Samson Tan, et al. Nl-augmenter: A\nframework for task-sensitive natural language augmentation. arXiv preprint arXiv:2112.02721,\n2021.\nKawin Ethayarajh and Dan Jurafsky. Utility is in the eye of the user: A critique of NLP leaderboards.\narXiv preprint arXiv:2009.13888, 2020.\n14\nAllyson Ettinger. What bert is not: Lessons from a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for Computational Linguistics, 8:34\u201348, 2020.\nPaula Fortuna, Juan Soler, and Leo Wanner. Toxic, hateful, offensive or abusive? what are we\nreally classifying? an empirical analysis of hate speech datasets. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference, pages 6786\u20136794, Marseille, France, May\n2020. European Language Resources Association. ISBN 979-10-95546-34-4. URL https:\n//aclanthology.org/2020.lrec-1.838.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language\nmodel evaluation, September 2021. URL https://doi.org/10.5281/zenodo.5371628.\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. RealToxici-\ntyPrompts: Evaluating Neural Toxic Degeneration in Language Models, September 2020. URL\nhttp://arxiv.org/abs/2009.11462. arXiv:2009.11462 [cs].\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding, 2021.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1601\u2013\n1611, 2017.\nJeevesh Juneja, Rachit Bansal, Kyunghyun Cho, Jo\u00e3o Sedoc, and Naomi Saphra. Linear connec-\ntivity reveals generalization strategies. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=hY6M0JHl3uL.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. Large language\nmodels struggle to learn long-tail knowledge. arXiv preprint arXiv:2211.08411, 2022.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie\nVidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia, et al. Dynabench: Rethinking bench-\nmarking in nlp. arXiv preprint arXiv:2104.14337, 2021.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110, 2022.\nMicrosoft. Guidance. Microsoft, June 2023. URL https://github.com/microsoft/guidance.\nSimon Mille, Kaustubh D. Dhole, Saad Mahamood, Laura Perez-Beltrachini, Varun Gangal, Mihir\nKale, Emiel van Miltenburg, and Sebastian Gehrmann. Automatic construction of evaluation suites\nfor natural language generation datasets. ArXiv, abs/2106.09069, 2021.\nJoe O\u2019Connor and Jacob Andreas. What context features can transformer language models use?\narXiv preprint arXiv:2106.08367, 2021.\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00e1ndez. The lambada dataset:\nWord prediction requiring a broad discourse context. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (Volume 1: Long Papers), pages 1525\u20131534, 2016.\nLuiza Pozzobon, Beyza Ermis, Patrick Lewis, and Sara Hooker. On the challenges of using black-box\napis for toxicity evaluation in research. arXiv preprint arXiv:2304.12397, 2023.\n15\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy:\nBehavioral testing of NLP models with CheckList. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pages 4902\u20134912, Online, July 2020. Association for\nComputational Linguistics. doi: 10.18653/v1/2020.acl-main.442. URL https://aclanthology.\norg/2020.acl-main.442.\nAlexis Ross, Tongshuang Sherry Wu, Hao Peng, Matthew E. Peters, and Matt Gardner. Tailor:\nGenerating and perturbing text with semantic controls. In Annual Meeting of the Association for\nComputational Linguistics, 2021.\nJessica\nRumbelow\nand\nMwatkins.\nSolidgoldmagikarp\n(plus,\nprompt\ngenera-\ntion),\n2023.\nURL\nhttps://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/\nsolidgoldmagikarp-plus-prompt-generation.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the\nimitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint\narXiv:2206.04615, 2022.\nHao Sun, Guangxuan Xu, Jiawen Deng, Jiale Cheng, Chujie Zheng, Hao Zhou, Nanyun Peng,\nXiaoyan Zhu, and Minlie Huang. On the safety of conversational models: Taxonomy, dataset,\nand benchmark. In Findings of the Association for Computational Linguistics: ACL 2022, pages\n3906\u20133923, Dublin, Ireland, May 2022. Association for Computational Linguistics. doi: 10.18653/\nv1/2022.findings-acl.308. URL https://aclanthology.org/2022.findings-acl.308.\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient\ntransformers. In International Conference on Learning Representations, 2020.\nTristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Can-\ndace Ross. Winoground: Probing vision and language models for visio-linguistic compositionality,\n2022.\nTongshuang Sherry Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S. Weld. Polyjuice: Generating\ncounterfactuals for explaining, evaluating, and improving models. In Annual Meeting of the\nAssociation for Computational Linguistics, 2021.\nGuanqun Yang, Mirazul Haque, Qiaochu Song, Wei Yang, and Xueqing Liu. Testaug: A frame-\nwork for augmenting capability-based nlp tests. In International Conference on Computational\nLinguistics, 2022.\nMert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and\nwhy vision-language models behave like bags-of-words, and what to do about it? In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.net/\nforum?id=KRLUvxh8uaX.\n16\nA\nAppendix\nA.1\nKnowledge Probing via Negations\nExample:\nFigure 13 shows an example of the original x and the transformed x\u2032 for the Knowledge\nProbing via Negations experiments.\nOriginal (x): April is the fourth month of the year in the Julian and Gregorian\ncalendars and comes between March and May.\nPerturbed (x\u2032): April is not the fourth month of the year in the Julian\nand Gregorian calendars and comes between March and May.\nFigure 13: Knowledge probing via negations example over topic sentences in wikipedia. (Top) is the original,\nx, from wikipedia. (Bottom) is the transformed, x\u2032, where we add a \u201cnot\u201d according to the rules described in\nthe main paper.\nAdding Negations in TriviaQA\nTo understand whether adding negations and measuring the change\nin log perplexity is a reasonable assessment of probing the knowledge in an LLM, we added negations\nto questions following the same rule described in the main paper. We then recorded the change in\nperplexity for each of the models given the question-answer pair. This was to understand how different\nmodels may understand negations. Figure 14 (Left) shows that adding a negation in the question and\nobserving the change in perplexity can give us an indication of performance on TriviaQA.\nTriviaQA Accuracy\nWe calculate the accuracy for TriviaQA for the unfiltered-web-dev split\nby simply counting a correct answer from the model if one of the given answers was contained in the\noutput string. Additionally, since we found that the answer list sometimes had the answer entity in\nthe question, we excluded these answers when calculating accuracy. We use the template \u201cQuestion:\n[input question]\\nAnswer:\u201d.\nModels From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl,\nEleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b,\nEleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b,\nmosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b,\ndatabricks/dolly-v2-3b, databricks/dolly-v2-7b\nOther Models: LLaMA-base-7B, Vicuna-7B\nOpenAI API Models: ada, babbage, curie, davinci, text-ada-001,\ntext-babbage-001, text-curie-001,text-davinci-003\nCohere API Models: xlarge, command-xlarge-beta\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nTriviaQA Acc.\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nSensitivity Score on TriviaQA\nSensitivity Score on TriviaQA vs TriviaQA Acc.\nModel Family\nOpenAI Instruct\nOpenAI\nCohere\nCohere Instruct\nLlama\nMPT\nPythia\nDolly V2 Instruct\nGPT-J\nDolly V1 Instruct\nNeo\nGPT-2\nInstruction\nNo\nYes\n0.55\n0.60\n0.65\n0.70\n0.75\nSensitivity Score (Wikipedia)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\nAnswer in Output After Negation\nSensitivity Score (Wikipedia) vs\n Answer in Output After Negation\nModel Family\nOpenAI Instruct\nCohere Instruct\nDolly V2\nDolly V1\nFigure 14: (Left) The change in perplexity in the question-answer pair when a negation is applied to the question\nversus TriviaQA Acc. There appears to be a square-root relationship between the Sensitivity Score on TriviaQA\nversus TriviaQA Acc. (Right) The percentage of times when the correct answer was contained in the solution\neven when applying the negation versus Sensitivity Score (Wikipedia) for a few instruction models. We see that\ntext-ada-001 changes its answer often, whereas the Cohere model does not.\n17\nA.2\nToxicity\nExample:\nFigure 15 shows an example of the original x and the transformed x\u2032 for the toxicity\nexperiments.\nOriginal (x): Lionel Andr\u00e9s Messi is an Argentine professional footballer\nwho plays as a forward for Ligue 1 club Paris Saint-Germain and captains the\nArgentina national team.\nTransformed (x\u2032):\nLionel Andr\u00e9s Messi is an Argentine professional\nfootballer who plays as a forward for Ligue 1 club Paris Saint-Germain and\ncaptains the Argentina national team. F*** F***\nFigure 15: Toxicity test with one sentence. (Top) is the original, x, from wikipedia. (Bottom) is the\ntransformed, x\u2032, where we add the trigger phrase to the original input.\nIncreasing the number of F-bombs:\nFrom Figure 16, we see, that on average, increasing the\nnumber of F-bombs increases the toxicity of the model. However, here two F-bombs induce more\ntoxic behavior than three F-bombs, suggesting that on average, the two F-bombs might be a trigger\nfor toxic generation.\nModels From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl,\nEleutherAI/gpt-j-6b, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B,\nEleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b,\nmosaicml/mpt-7b, mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b,\ndatabricks/dolly-v2-3b, databricks/dolly-v2-7b\nOther Models: LLaMA-base-7B, Vicuna-7B, WizardLM-7B\nA.3\nContext (Long-Range) Sensitivity\nExample:\nFigure 17 shows an example of the original x and the transformed x\u2032 for the LRS\nexperiments.\nIncreasing the Amount of Context:\nFrom Figure 18, we see that increasing the context (or the\nnumber of sentences swapped) increases the sensitivity. For the 7B parameter range, we see that\nPythia (6.9B) is the most sensitive.\n1\n2\n3\n4\n5\n6\n7\nNumber of F-Bombs Appended\n0.0\n0.2\n0.4\n0.6\n0.8\nToxic Frac.\nToxicity Ablation -- Increasing F-Bombs\nModel\nGPT-2 (small)\nPythia (6.9B)\nGPT-2 (medium)\nGPT-2 (large)\nPythia (1.4B)\nGPT-J (6B)\nGPT-2 (XL)\nLLaMA (7B)\nMPT (7B)\nGPT-Neo (1.3B)\nPythia (2.8B)\nGPT-Neo (2.7B)\nFigure 16: As we increase the number of F-bombs, the toxicity of the generation increases except when two\nF-bombs are present, which is a notable outlier. This suggests that to most models this is a toxic trigger. We\nmeasure toxicity over the generated text by observing whether a term from LDNOOBW is contained in the\ngeneration. From this figure, we see GPT-Neo (2.7B) is the most toxic according to our metric.\n18\nOriginal (x): Lyrically, the song begins with the absence of her man, but then,\nin the chorus, transitions into a warning not to fall in love with material things.\nThe second track, \u201cL\u00e1grimas C\u00e1lidas\u201d (\u201cWarm Tears\u201d), is a vallenato-stylized\npop ballad, expressing her suffering due to being abandoned by her lover.\u201cTe\nArrepentiras\u201d (\u201cYou\u2019ll Regret\u201d), is about a woman who surrendered completely\nto a man who did not appreciate her.\nTransformed (x\u2032): Ireland has won more medals in boxing than in any\nother Olympic sport.\nBoxing is governed by the Irish Amateur Boxing\nAssociation.\n\u201cTe Arrepentiras\u201d (\u201cYou\u2019ll Regret\u201d), is about a woman who\nsurrendered completely to a man who did not appreciate her.\nFigure 17: Long-Range Sensitivity test with four sentences. (Top) is the original, x, from wikipedia. (Bottom)\nis the transformed, x\u2032, where the first two sentences are replaced with random two sentences from wikipedia.\n0\n1\n2\n3\n4\n5\n6\nNumber of Sentences Swapped\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nMean JSD\nLRS Ablation -- Number of Sentences Swapped\nModel\nOPT (2.7B)\nOPT (1.3B)\nPythia (6.9B)\nGPT-J (6B)\nLLaMA (7B)\nPythia (2.8B)\nGPT-Neo (2.7B)\nGPT-Neo (1.3B)\nPythia (1.4B)\nMPT (7B)\nGPT-2 (XL)\nGPT-2 (medium)\nGPT-2 (large)\nGPT-Neo (125M)\nGPT-2 (small)\nPythia (160M)\nPythia (70M)\nFigure 18: Increasing the context length (the number of swapped sentences) increases, the change in the\nprobability distribution over the last sentence.\nModels From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl,\nfacebook/opt-1.3b, facebook/opt-2.7b, EleutherAI/gpt-neo-125M,\nEleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b,\nEleutherAI/pythia-70M, EleutherAI/pythia-160m, EleutherAI/pythia-410m,\nEleutherAI/pythia-1b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b,\nEleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct,\ndatabricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b,\ndatabricks/dolly-v2-7b\nOther Models: LLaMA-base-7B, Vicuna-7B\nA.4\nWord Order Sensitivity\nExample:\nFigure 19 shows an example of the original x and the transformed x\u2032 for the word order\nexperiments.\nOriginal (x): Media.Vision would return to the franchise with the development\nof Valkyria: Azure Revolution for the PlayStation 4.\nTransformed (x\u2032):\nMedia.Vision would return PlayStation the franchise\nwith the development of Valkyria : Azure Revolution for the to 4.\nFigure 19: Word Order Sensitivity test over one sentence. (Top) is the original, x, from wikipedia. (Bottom)\nis the transformed, x\u2032, where two words are randomly flipped. This is a 1-Swap.\n19\nTable 2: Example sentence of the transformation with a split stride of 10. (Left) shows the original unaltered\nsentence. (Right) shows the transformed sentence after splitting every 10th character. The underlined dashes are\nwhere the sentence is split.\nOriginal (x)\nTransformed (x\u2032)\nMedia.Vision would return to the franchise with\nthe development of Valkyria: Azure Revolution\nfor the PlayStation.\nMedia.Visi\u2013on would r\u2013eturn to t\u2013he franchi\u2013se\nwith th\u2013e developm\u2013ent of Val\u2013kyria: Azu\u2013re\nRevolut\u2013ion for th\u2013e PlayStat\u2013ion 4.\nDifferent Number of Swaps:\nFigure 20 shows the median JSD on the next token as we increase\nthe swaps. Here, we see increasing the number of swaps increases the sensitivity.\n0\n2\n4\n6\n8\n10\nNumber of Words Swapped\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nJSD Median\nWord Order Ablation -- Number of Word Swaps\nModel\nGPT-J (6B)\nMPT (7B)\nPythia (2.8B)\nPythia (6.9B)\nLLaMA (7B)\nGPT-Neo (2.7B)\nOPT (6.7B)\nPythia (1.4B)\nGPT-Neo (1.3B)\nGPT-2 (XL)\nGPT-Neo (125M)\nGPT-2 (large)\nGPT-2 (medium)\nGPT-2 (small)\nFigure 20: We plot JSD on the next token prediction against the number of swaps for the token.\nModels From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl,\nEleutherAI/gpt-neo-125M, EleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B,\nEleutherAI/gpt-j-6b, EleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b,\nEleutherAI/pythia-6.9b, mosaicml/mpt-7b, mosaicml/mpt-7b-instruct,\ndatabricks/dolly-v1-6b, databricks/dolly-v2-3b, databricks/dolly-v2-7b\nOther Models: LLaMA-base-7B, Vicuna-7B\nA.5\nTokenization Sensitivity\nExample:\nFigure 19 shows an example of the original x and the transformed x\u2032 for the tokenization\nexperiments.\nIncresing Split Stride:\nFigure 21 shows the median JSD on the next token as we increase the split\nstride. Here, we see that LLaMA and MPT are much less sensitive (better at handling tokenization\nchanges) regarding the change in the probability distribution over the next token as we increase the\nsplit stride. Figure 22 shows the number of tokens seen versus the tokenization sensitivity score.\nHere, we see that there is a negative correlation.\nModels From Huggingface: gpt2, gpt2-large, gpt2-medium, gpt2-xl,\nfacebook/opt-1.3b, facebook/opt-2.7b, EleutherAI/gpt-neo-125M,\nEleutherAI/gpt-neo-1.3B, EleutherAI/gpt-neo-2.7B, EleutherAI/gpt-j-6b,\nEleutherAI/pythia-160m, EleutherAI/pythia-410m, EleutherAI/pythia-1b,\nEleutherAI/pythia-1.4b, EleutherAI/pythia-2.8b, EleutherAI/pythia-6.9b,\nmosaicml/mpt-7b,mosaicml/mpt-7b-instruct, databricks/dolly-v1-6b,\ndatabricks/dolly-v2-3b, databricks/dolly-v2-7b, databricks/dolly-v2-7b\nOther Models: LLaMA-base-7B, Vicuna-7B\n20\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nSplit Stride (by Character)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nJSD Mean\nTokenization Ablation -- Increasing Split Stride\nModel\nMPT (7B)\nLLaMA (7B)\nPythia (160M)\nPythia (6.9B)\nGPT-2 (XL)\nGPT-J (6B)\nGPT-2 (large)\nGPT-Neo (2.7B)\nGPT-Neo (1.3B)\nPythia (1.4B)\nPythia (2.8B)\nGPT-2 (medium)\nGPT-2 (small)\nGPT-Neo (125M)\nOPT (2.7B)\nOPT (1.3B)\nFigure 21: Increasing the split stride decreases the sensitivity. We see that the OPT family cannot handle this\ntype of transformation. Additionally, we see LLaMA and MPT are good at handling these types of tokenization\nchanges. Lower is better.\n103\n2 \u00d7 102\n3 \u00d7 102\n4 \u00d7 102\n6 \u00d7 102\nTotal Tokens Seen During Training\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\nTokenization Sensitivity Score\nTokenization Sensitivity Score vs Tokens Seen\nModel Family\nLLaMA\nMPT\nPythia\nGPT-J\nNeo\nGPT-2\nOPT\nFigure 22: Increasing the total number of tokens seen during training decreases the sensitivity score. We see\nthat the OPT family is the most sensitive to this type of transformation, as they have seen the least number of\ntokens. Additionally, we see LLaMA and MPT are good at handling these types of tokenization changes as they\nhave seen more tokens. Lower is better.\nA.6\nAdditional Experiment Details\nFor all these experiments, we use NVIDIA RTX A4000 GPUs, finding that evaluating most models is\nquite inexpensive over 1000 examples, with compute requirements of less than 30 min per model for\nmost tests. Additionally, for sentence and word parsing/tokenization, we use the nltk package.\n21\n"
  },
  {
    "title": "Scaling MLPs: A Tale of Inductive Bias",
    "link": "https://arxiv.org/pdf/2306.13575.pdf",
    "upvote": "14",
    "text": "Scaling MLPs: A Tale of Inductive Bias\nGregor Bachmann\u2217 , Sotiris Anagnostidis\u2217, Thomas Hofmann\nETH Z\u00fcrich, Switzerland\nAbstract\nIn this work we revisit the most fundamental building block in deep learning, the\nmulti-layer perceptron (MLP), and study the limits of its performance on vision\ntasks. Empirical insights into MLPs are important for multiple reasons. (1) Given\nthe recent narrative \"less inductive bias is better\", popularized due to transformers\neclipsing convolutional models, it is natural to explore the limits of this hypoth-\nesis. To that end, MLPs offer an ideal test bed, as they lack any vision-specific\ninductive bias. (2) MLPs have almost exclusively been the main protagonist in the\ndeep learning theory literature due to their mathematical simplicity, serving as a\nproxy to explain empirical phenomena observed for more complex architectures.\nSurprisingly, experimental datapoints for MLPs are very difficult to find in the liter-\nature, especially when coupled with large pre-training protocols. This discrepancy\nbetween practice and theory is worrying: Do MLPs reflect the empirical advances\nexhibited by practical models? Or do theorists need to rethink the role of MLPs as a\nproxy? We provide insights into both these aspects. We show that the performance\nof MLPs drastically improves with scale (95% on CIFAR10, 82% on CIFAR100,\n58% on ImageNet ReaL), highlighting that lack of inductive bias can indeed be\ncompensated. We observe that MLPs mimic the behaviour of their modern counter-\nparts faithfully, with some components in the learning setting however exhibiting\nstronger or unexpected behaviours. Due to their inherent computational efficiency,\nlarge pre-training experiments become more accessible for academic researchers.\nAll of our experiments were run on a single GPU.\nFigure 1: Test error on CIFAR100 as a function of PFLOPS.\n\u2217Equal contribution. Correspondence to {gregorb, sanagnos}@ethz.ch. Code and checkpoints available\nat https://github.com/gregorbachmann/scaling_mlps\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.13575v3  [cs.LG]  3 Oct 2023\n1\nIntroduction\nDeep learning has undergone tremendous empirical progress in the last decades. The dominant\napproaches in practice these days rely on very large, pre-trained models which are then fine-tuned to\nthe specific task at hand. For natural language processing, these models usually are some variant of\nthe Transformer architecture (Vaswani et al., 2017), while in computer vision, both convolutional and\ntransformer-based models are very popular (He et al., 2015; Tan and Le, 2020; Dosovitskiy et al.,\n2021). The theoretical understanding of these advances on the other hand remains very poor and\nthe gap between the world of theory and practice is growing at an alarming rate. One aspect of this\ngap is the family of models investigated; due to their mathematical simplicity, theoretical works\nlargely focus on simple multi-layer perceptrons (MLPs). Consisting of a series of unstructured matrix\nmultiplications, interleaved with element-wise non-linearities, the MLP serves as an ideal test bed to\nanalyze empirical phenomena exhibited by more complicated models employed in practice. Due to\ntheir inferior performance, MLPs are rarely used and very little is known regarding their behaviour\nin more modern settings. For instance, to the best of our knowledge, there is not a single published\nresult showcasing an MLP trained on ImageNet1k, the de-facto standard benchmark in vision, let\nalone any pre-training/transfer learning studies. This lack of empirical data is concerning as theory\naims to understand the characteristics of modern architectures through the lens of MLPs, yet only\nlittle assessments are made regarding how well such a proxy works. This raises the question,\nDo MLPs reflect the empirical advances exhibited by practical models?\n(1)\nInvestigating MLPs is not only interesting for theory but also for practice. With the Vision Transformer\n(ViT) outperforming its convolutional competitors in very large-scale settings, the role of inductive\nbias has recently been brought into question. Since a ViT is equipped with significantly less inductive\nbias for vision compared to convolutional models (e.g. it lacks translation-equivariance) a novel\nnarrative has recently emerged:\nAt large scales of compute, having less inductive bias is beneficial for performance.\n(2)\nMore evidence for this hypothesis has been collected in the form of the MLP-Mixer (Tolstikhin\net al., 2021), an architecture with arguably even less inductive bias, solely relying on multi-layer\nperceptrons as patch processors and mixers. The MLP architecture is the ideal candidate to test\nthe limits of such a hypothesis, as it exhibits the least inductive bias for vision due to its invariance\nto permutations of pixels. Unfortunately, the scale where Transformers and MLP-Mixers start to\noutperform convolutional models is out of reach for most researchers, requiring billions of annotated\nimages and thousands of TPUs. We thus expect similar required scales for MLPs and hence instead\ninvestigate the following, weaker hypothesis:\nLack of inductive bias can be compensated by scaling compute.\n(3)\ni.e. we aim to measure to what degree a lack of inductive bias hinders performance even if a model is\nsubjected to a large parameter count and trained on datasets with many examples (albeit smaller than\nwhat is employed in Dosovitskiy et al. (2021)).\nIn this work, we provide answers to question 1 and provide further evidence for hypothesis 2 and 3 by\ninvestigating how far we can push the empirical performance of models solely built from composing\nseveral MLP blocks. We give largely positive answers to question 1, observing that MLPs behave\nvery similarly to their modern counterparts when subjected to scale, i.e. their performance increases\npredictably as a power law in parameter count and sample size, akin to Hestness et al. (2017, 2019);\nKaplan et al. (2020); Zhai et al. (2022) (see e.g. Fig. 1). In contrast to previous work however, we\nfind that compute-optimal MLPs allocate their budget more strongly into sample size, highlighting\nagain their small inductive bias. While regularization in the form of data augmentation is also helpful\nfor CNNs, its role is significantly amplified for MLPs even at large sample sizes, leading to fatal\ndegradation if turned off. We further investigate how the implicit bias of SGD affects performance,\nand we make a very counter-intuitive discovery: contrary to CNNs, we find that larger batch sizes\ngeneralize significantly better for MLPs. This result questions the validity of the proxy role that the\nMLP plays in theoretical works investigating the implicit bias of SGD. While, as expected, the scale\nemployed in this work does not suffice for hypothesis 2, we provide strong evidence for 3, which\n2\nFigure 2: Different architectures process images differently. Convolutions directly operate on the\nimage, ViTs and MLP-Mixers work with patches while the MLP takes the flattened image as input.\nwe view as an important first step. We observe that enough scale indeed suffices to overcome the\nbad inductive bias present in MLPs, leading to surprisingly strong downstream performance, e.g.\n\u2248 95% on CIFAR10, \u2248 82% on CIFAR100 and \u2248 58% on ImageNet ReaL. In summary, we make\nthe following contributions:\n\u2022 We fill the gap between theory and practice, providing the first results for MLPs trained in\nmodern settings.\n\u2022 We show that MLPs mostly behave comparably to their modern counterparts, making them\na good proxy for theory. We observe however that the roles of regularization and implicit\nbias of SGD significantly differ and theory hence needs to adapt.\n\u2022 We provide further evidence that inductive bias is not crucial at large scales, showing\nthat even \"bad\" architectures like MLPs can achieve strong downstream performance. We\nhowever identify a shift in compute-optimality, showing that optimal MLPs invest their\ncompute significantly more into dataset size compared to model size.\n2\nBackground\nTheoretical Works.\nThe MLP has served as the main object of study for theoretical works in\ndeep learning across different domains. The cornerstone results for areas such as convergence of\nSGD-trained neural networks (Mei et al., 2018; Du et al., 2019; Zou et al., 2020; Li and Yuan, 2017;\nSaxe et al., 2014), most generalization bounds (Arora et al., 2019b; Mei and Montanari, 2021; Jacot\net al., 2018; Allen-Zhu et al., 2019a), the benefits of overparametrization (Neyshabur et al., 2019;\nAllen-Zhu et al., 2019b; Arora et al., 2018), the implicit bias of SGD towards favourable solutions\n(Soudry et al., 2018; Neyshabur et al., 2014; Chizat and Bach, 2020), signal propagation properties\n(Poole et al., 2016; Schoenholz et al., 2017) and scaling laws (Bahri et al., 2021; Maloney et al.,\n2022) are all largely obtained for MLPs. To quote the very influential Principles of Deep Learning\nTheory book (Roberts et al., 2022):\n\"MLPs are the simplest of these neural network architectures that hinge on this stacking idea, and\nthus provide a minimal model for an effective theory of deep learning.\"\nThere are also several theoretical works studying more modern setups such as convolutional or\ntransformer-based networks including Arora et al. (2019a); Gunasekar et al. (2018); Brutzkus and\nGloberson (2017); Hron et al. (2020) to name but a few, but the main theoretical focus to the best of\nour knowledge still remains on the MLP architecture. We thus believe it is important to explore the\nlimits of such a theoretical proxy in realistic settings.\nMLPs.\nThe multi-layer perceptron has its origins in Rosenblatt (1958), serving as an extension to\nthe classic Perceptron with its hidden layers however fixed to random initialization. Ivakhnenko et al.\n(1965) devised the first method to update the hidden layers through self-organization. Amari (1967)\nthen introduced the idea to train the parameters with stochastic gradient descent. Mathematically, an\nMLP of depth L \u2208 N can be described very efficiently; given an input x \u2208 Rd, it applies a series of\nlinear transformations, interleaved with an element-wise non-linearity \u03c3 : R \u2212\u2192 R:\nz(l) = W (l)x(l\u22121)\n\u2212\u2212\u2192\nx(l) = \u03c3\n\u0010\nz(l)\u0011\n3\nPatch 1\nPatch 2\nPatch 3\nPatch 4\nMLP 1\nMLP 2\nPatch Mixing\nPatch Processing\nMLP-Mixer\nMLP\nFigure 3: A simplified depiction of the differences between an MLP-Mixer and an MLP.\nwhere we define x(0) := x and W (l) \u2208 Rdl\u00d7dl\u22121 for l = 1, . . . , L are the learnable weight matrices.\nFor the sake of readability, we omit the biases. This mathematical simplicity makes the MLP a very\nattractive model to study from a theoretical perspective (albeit still very far from trivial) and indeed\nmany works frame their results around this more general model class. When used for vision, the\ninput tensor x \u2208 Rh\u00d7w\u00d73 is flattened into a vector vec(x) \u2208 R3hw and then passed through the MLP.\nNotice how such an architecture completely lacks locality and weight sharing, every unit simply\nprocesses the entire image at once. More worryingly, the vectorization vec could be applied in any\nway, i.e. any permutation of x looks identical to an MLP.\nWe want to highlight that MLPs of course are not completely free of inductive bias, in the sense that\nthey encourage learning a hierarchical feature structure. On the other hand, there is no vision-specific\ninductive bias present in MLPs, which is the main setting we investigate here. We refer to Battaglia\net al. (2018) for a more in-depth treatment of inductive bias.\nConvolutions.\nThe MLP is a very general model and has no structure built into it to make it more\nsuitable for vision tasks. A convolution on the other hand was designed specifically for vision with\ndesirable characteristics incorporated into the model. A convolution can be viewed as a special case\nof an MLP, where the weight matrix W is very structured by being sparse and having shared entries,\nleading to spatially localized learning. This can be most easily illustrated in the case of convolving a\n2 \u00d7 3 \u00d7 1 image x with a 2 \u00d7 2 filter f as the following matrix multiplication:\nf \u2217 x = Wf vec(x) =\n\u0012\nf1\nf2\n0\nf3\nf4\n0\n0\nf1\nf2\n0\nf3\nf4\n\u0013\nvec(x)\nHere vec denotes the standard, row-wise vectorization-scheme to flatten the image. Instead of\noperating with a dense matrix as the MLP, the convolution uses a structured matrix Wf tailored to\nthe task of vision, leading to a better inductive bias. Moreover, a convolution exhibits translation-\nequivariance, i.e. shifts of images are processed equivalently to the original. Crucially, in contrast to\nthe MLP, a convolution severely suffers if a permutation is applied to the image.\nVision Transformer.\nInspired by the successes in NLP, recently the Transformer architecture has\nbeen adapted to vision (Dosovitskiy et al., 2021). An image x \u2208 Rh\u00d7w\u00d73 is broken up into smaller\npatches (also called tokens) and each such patch is linearly embedded (see Fig. 2) and augmented with\na so-called positional embedding, marking its spatial location in the image. The obtained embeddings\nare then processed by self-attention layers where patches can exchange information, and MLP layers,\nwhich are shared among patches and transform them individually. While the inductive bias of a ViT is\ncertainly weaker compared to a CNN (it lacks translation-equivariance), the patching and parameter\nsharing still make the architecture suitable for vision.\nMLP-Mixer.\nSimilar to the ViT, the MLP-Mixer also works with a patchified image (Tolstikhin\net al., 2021). Unlike the ViT, token-mixing is not implemented using self-attention but rather another\nMLP block is used to exchange information between patches. We want to clearly highlight the\ndifference between an MLP-Mixer and an MLP: An MLP-Mixer operates on patches, where in each\nblock it applies a shared MLP to each patch for processing, and another MLP for mixing the patches\nalong the channels. We visualize the differences in Fig. 3 for clarity. We again want to stress that\nbreaking the image into patches and sharing parameters among them significantly enhances the\namount of inductive bias, compared to a standard MLP.\n4\nCIFAR10\nCIFAR100\nTINYIMAGENET\nIMAGENET\nS-MLP (@100 E)\n54.2\n28.8\n8.5\n9.2\nS-MLP + DA (@ 1000 E)\n68.9\n43.3\n25.2\n24.3\nS-MLP + DA (@ 5000 E)\n72.3\n44.5\n27.3\n26.8\nB-MLP (@ 100 E)\n58.1\n30.5\n8.9\n8.7\nB-MLP + DA (@1000 E)\n70.1\n48.3\n27.2\n28.7\nB-MLP + DA (@5000 E)\n75.4\n50.4\n31.2\n31.7\nRESNET182 + DA\n93.2\n75.6\n68.9\n69.7\nTable 1: Test accuracies (in %) without any pre-training. The S-MLP has depth 6 and width 1024\nwhile the B-MLP has depth 6, width 1024 and an expansion factor of 4.\nPatchifiying.\nAs highlighted above, ViTs and Mixers largely obtain their inductive biases through\nbreaking the images into patches. This choice seems to be beneficial even for architectures that\nalready possess a strong inductive bias, such as the ConvMixer (Trockman and Kolter, 2022), where\nconvolutions are performed on individual patches. The very recent Metaformer (Yu et al., 2022)\nfurther shows that even a simple spatial pooling instead of attention can lead to strong performance if\nthe image is patchified. While the success of this mechanism certainly warrants further investigation,\nin this work we decided to deliberately focus on MLPs as they specifically lack this type of bias.\n3\nArchitecture\nWe study different variants of the MLP architecture, starting from the standard vanilla setup and then\nadding more components such as residual connections and bottleneck layers.\nStandard MLP.\nAs a first starting point, we investigate simple MLPs with ReLU activations and\nisotropic design, i.e. except for the first, every layer has the same width m \u2208 N. In order to avoid\ntraining instabilities we further enhance the standard MLP with layer normalizations (Ba et al., 2016)\nplaced after the activations. We thus compose several blocks of the form\nBlock(z) = \u03c3 (W LN(z))\nwith W \u2208 Rm\u00d7m. To embed the image x \u2208 Rd\u00d7d\u00d73 we use a linear layer emb(x) = W emb vec(x)\nwith W emb \u2208 Rm\u00d73d2. Such an embedding layer is crucial since for high resolution images, 3d2\ncan be quite large and thus m needs to be chosen smaller. We empirically find that such a network\ndesign is the minimal choice in order to guarantee successful training across all scales of parameter\ncount and sample size. We will use the short cut S-MLP to denote such an architecture.\nInverted Bottleneck MLP.\nInspired by Lin et al. (2015); Tolstikhin et al. (2021) we add a bottleneck\nstructure to an MLP block as well as skip connections as follows:\nBlock(z) = z + W c\u03c3 (W e LN (z))\nwhere W e \u2208 Rkm\u00d7m expands the dimension to km for k \u2208 N and W (c) \u2208 Rm\u00d7km collapses it\nback to width m. For most experiments we set k = 4. While the additions of skip connections and\nbottleneck layers to the architecture arguably add some amount of inductive bias, we believe that\nin comparison to modern architectures such enhancements remain negligible. We will denote this\nvariant by B-MLP.\n2In contrast to the MLPs, the ResNet18 was trained at the original image resolutions.\n5\nCIFAR10\nCIFAR100\nSTL10\nTINY-IN\nIN\nREAL\nB-6/Wi-1024\n69.9\u00b10.1\n43.0\u00b10.4\n51.5\u00b10.1\n47.1\u00b10.1\n15.2\u00b10.2\n20.3\u00b10.2\nB-6/Wi-1024 + DA\n91.5\u00b10.02\n76.4\u00b10.2\n85.0\u00b10.2\n62.7\u00b10.1\n38.7\u00b10.1\n47.0\u00b10.15\nB-12/Wi-1024 + DA\n94.2\u00b10.05\n80.0\u00b10.05\n89.9\u00b10.1\n69.9\u00b10.4\n43.3\u00b10.06\n48.6\u00b10.2\nB-12/Wi-1024 + DA + TTA\n95.5\u00b10.05\n82.6\u00b10.2\n92.2\u00b10.05\n73.1\u00b10.5\n51.5\u00b10.1\n57.9\u00b10.1\nTable 2: Fine-tuning Top-1 accuracies (in %) when pretrained on ImageNet21k. Accuracies are\naveraged over 3 runs. For readability, we abbreviate ImageNet as IN.\n4\nExperiments\n4.1\nSetup\nIn this work, we solely focus on vision tasks as inductive bias is more readily understood in this\nsetting. Moreover, most theoretical works focus on image classification tasks, making it thus a natural\ntest bed to assess the performance of MLPs. We study the popular tasks CIFAR10, CIFAR100\n(Krizhevsky, 2009), STL10 (Coates et al., 2011), TinyImageNet (Le and Yang, 2015), ImageNet1k\nfor evaluation, as well as ImageNet21k (Deng et al., 2009) for pre-training. In order to limit the\nsize of the embedding layer and the computational needs, we downscale all images to resolution\n64 \u00d7 64 \u00d7 3 (if needed) as done in Chrabaszcz et al. (2017). We center and normalize all the images\nas a pre-processing step. For data augmentations, we consider random flips and crops as well as\nMixUp (Zhang et al., 2018).\n4.2\nTraining from Scratch\nWe start the empirical exploration of MLPs by training them from scratch (i.e. without any extra data)\non popular vision benchmarks. All models were trained with the LION optimizer (Chen et al., 2023)\nwith a learning rate \u03b7 = 5e-5. In order to combat overfitting we use strong label smoothing \u03b1 = 0.3.\nWe display the resulting test accuracies in Table 1. We observe that both the standard architecture\nand the bottleneck without any data augmentation suffer from overfitting, leading to suboptimal\nperformance. When turning it on, data augmentation as a regularizer however really unfolds its full\npower, significantly pushing the performance by roughly 20% across all tasks. As observed in Lin\net al. (2015), the inverted bottleneck architecture leads to an improvement in performance across\nall datasets. Learning on the other hand significantly slows down with strong augmentations such\nas MixUp, enabling training for up to 5000 epochs without suffering from overfitting. However,\ncompared to simple modern baselines such as a ResNet18 (He et al., 2015), a large discrepancy in\nperformance remains, highlighting the importance of inductive bias in the small sample regime. We\nremark that ViTs and MLP-Mixers as well exhibit more learning difficulties if the dataset size is small\n(Dosovitskiy et al., 2021; Tolstikhin et al., 2021). We provide more ablation studies in Appendix A.2.\n4.3\nTransfer Learning\nIn this section, we aim to analyze how transferable features learnt by MLPs are across different\nvision tasks. Transferability is one of the hallmark characteristics of modern deep learning, enabling\npractitioners to fine-tune large models on their specific dataset, leading to superior performance.\nWe are, to the best of our knowledge, the first to measure transferability of MLPs, which is crucial\nto assess in order to build a theoretical understanding of the process. In this section, we focus on\nthe inverted bottleneck MLP as it generalizes better and is easier to optimize. We provide the dual\nresults for the standard MLP in Appendix B.1. We restrict to k = 4 for the expansion factor and\ndenote by B-L/Wi-m a network with L blocks and width m. For pre-training we use ImageNet21k,\nthe largest publicly available image dataset with annotated classes. After preprocessing the dataset\nfollowing Ridnik et al. (2021), it consists of roughly 12 million images and 11 thousand classes. We\nthen pre-train the MLP with the cross-entropy loss for 800 epochs, employing label smoothing and\nthe LION optimizer. To guarantee fast data loading we rely on the FFCV framework (Leclerc et al.,\n6\n2023) for all experiments.\nIn order to measure transferability of the learnt features we fine-tune the network on the new task.\nWe also study training a linear layer on top of the embeddings but defer those results to Appendix\nA.3. We again explore the effects of data augmentation during the pre-training stage. For fine-tuning\nwe use SGD with momentum with a learning rate of \u03b7head = 0.01 for the head and \u03b7body = 0.001\nfor the encoder for 50 epochs. We upscale CIFAR images to resolution 64 \u00d7 64 \u00d7 3 at fine-tuning\ntime to guarantee compatibility. We display the fine-tuning results in Table 2. For visualizations of\nthe learnt features, we refer the interested reader to Appendix C. We again observe that using data\naugmentation during the pre-training phase is essential to successful training, boosting performance\nup to 30% in case of CIFAR100. Surprisingly, the learnt features are highly transferable, improving\nthe performances reported previously in Table 1 dramatically. While of course pre-trained on a large\nquantity of data, we nevertheless want to highlight that such an MLP becomes competitive with\na ResNet18 trained from scratch for all the datasets, except for ImageNet1k where performance\nfalls surprisingly short. We hypothesize that MLPs struggle with the more fine-grained distinctions\nbetween classes, in combination with the reduced resolution of the images.\nTest-Time Augmentations.\nFor ImageNet1k we further notice that objects tend to not be centered,\nin contrast to datasets like CIFAR10. We suspect that this might lead to the comparatively weaker\nperformance. To test this, we leverage test-time augmentations (TTA). As introduced by Krizhevsky\net al. (2012), for each test image, we produce a fixed number of 100 random crops and use the\naveraged logits for prediction. We observe significant improvements across all datasets, especially for\nImageNet we obtain an increase of roughly 8%. This indeed indicates that MLPs struggle to localize\nthe object of interest, especially for the more complicated ImageNet1k task. Using a large number of\ncrops alleviates this problem to some degree. This also explains why the gains on tasks like CIFAR10\nare smaller as the objects there usually are perfectly centered.\nReaL accuary.\nAs observed in (Beyer et al., 2020), the ImageNet labels do not capture that a single\nimage might contain multiple objects of distinct classes. ImageNet accuracy can thus be misleading\nin the sense that model classes such as convolutional networks might have implicitly adapted to the\nparticular labeling strategy due to the repeated benchmarking on the same validation set. MLPs most\nlikely lack such an implicit adaptation as this work is to our knowledge the first to evaluate them\non ImageNet1k. To address this, Beyer et al. (2020) introduced a novel set of validation labels that\nbetter capture the multi-label nature, where a prediction is deemed correct if it matches one of the\ncategories present in the image. We observe further very significant improvements of \u2248 7% when\nemploying ImageNet ReaL.\nOverall, these results underline that a bad inductive bias as exhibited by an MLP can indeed be\novercome if subjected to enough scale. For theory, the results are double-edged; while MLPs prove to\nbe a good proxy to understand transfer learning, data augmentation proves to be a crucial component.\nAlso test-time augmentations significantly boost performance. Both these components on the other\nhand remain rather understudied in theoretical works.\nLarge batch-sizes.\nWe further make the counter-intuitive observation that training with larger batch\nsizes significantly boosts performance both up- and downstream. In Fig. 4 we plot pre-training batch\nsize against resulting linear downstream accuracy on CIFAR100 for different number of pre-training\nepochs. We observe that across all training times, using a larger batch size leads to significantly better\nperformance. Moreover, we want to highlight that such a plot is even favoring small batch-sizes\nsince those models perform more gradient updates for a fixed number of epochs. This effect is in\nstark contrast to convolutional architectures where entire lines of works have focused on preserving\nthe performance of the small batch-size regime for larger ones (Goyal et al., 2017; You et al., 2017;\nHoffer et al., 2017; Keskar et al., 2017). Training with large batch-sizes without degradation is of\nhigh interest as it can lead to potentially more efficient training pipelines since computation can\nbe sharded among more devices. This observation about optimal batch-sizes is in-line with similar\nrecent conclusions in Transformers (Kaplan et al., 2020; Touvron et al., 2023).\nRole of augmentations.\nThe role of data augmentation is very pronounced for MLPs, largely since\nit provides indirect inductive bias to the model. Remarkably, a model pre-trained on 12 million\nexamples without data augmentation shows inferior performance on CIFAR10 compared to a network\n7\nFigure 4: Linear downstream error on CIFAR100\n(in %) when pretrained for varying batch-sizes on\nImageNet21k, on a log-log scale.\nModel\n#parameters\nB-6/Wi-256\n9M\nB-12/Wi-256\n12M\nB-6/Wi-512\n24M\nB-12/Wi-512\n37M\nB-6/Wi-1024\n74M\nB-12/Wi-1024\n124M\nTable 3: The different models and\nthe respective parameter counts in\nmillions.\ntrained from scratch with augmentations turned on. This emphasizes that augmentations go beyond\nmerely leading to a bigger dataset but provide the model with useful invariances. We investigate\nthe learnt weights in-depth in Appendix C, showing that very evidently, more localized features are\nlearnt if data augmentation is employed. The power of augmentations has already been demonstrated\npreviously through the advent of self-supervised learning (Grill et al., 2020; Caron et al., 2021; Chen\net al., 2020). Even when training on purely random labels, it still provides a powerful learning signal\n(Anagnostidis et al., 2023).\n4.4\nScaling Laws\nOne of the key mysteries in deep learning is that networks tend to improve in terms of generalization\nwhen compute, in the form of parameter count and dataset size, is scaled up. Recently it has\nbeen observed in several works that the benefits of scale are highly predictable, i.e. generalization\nperformance exhibits a power-law structure when plotted against compute measured in FLOPS\n(Rosenfeld et al., 2020; Hestness et al., 2017, 2019; Kaplan et al., 2020; Zhai et al., 2022). The\nfunctional form has recently been further refined (Caballero et al., 2023). The predictable nature of\ntest performance has even been leveraged to estimate the optimal model before training (Hoffmann\net al., 2022; OpenAI, 2023). In order to understand this important characteristic of deep learning\ntheoretically, it is important to analyze whether MLPs exhibit similar properties.\nFigure 5: Test error (in %) on CIFAR10 (left) and ImageNet1k (right) when linearly transferred as a\nfunction of PFLOPS, measured according to Eq.(4), on a log-log scale.\n8\nFigure 6: Power law in linear evaluation error on CIFAR100 (in %) when either bottlenecked by the\nnumber of parameters (left) or the number of examples (right), on a log-log scale. The dotted line\nvisualizes the fitted functional form.\nCompute.\nFollowing OpenAI (2018) we define the computational cost C incurred from training a\nmodel f on N examples for T epochs as\nC = FLOP(f) \u00d7 3 \u00d7 N \u00d7 T,\n(4)\nwhere FLOP(f) denotes the number of FLOPs needed to complete the forward pass of f for a single\nexample. We note that the number of parameters P present in f enters this equation implicitly in\nthe form of FLOP(f) \u221d P. Observe that a given level of compute can be achieved in different\nways, i.e. using more parameters P, training on more examples N, or training for a longer time\nT. When allocating a given level of compute optimally, it is observed that for convolutional and\ntransformer-based architectures, the test error E(C) as a function of compute behaves as a power-law\nE(C) = a(b + C)\u2212\u03b1 + E\u221e,\n(5)\nwhere a, b, E\u221e \u2208 R+ and \u03b1 > 0 is the scaling coefficient determining the rate of decay. E\u221e denotes\nthe irreducible error, i.e. even if infinite compute were employed, the performance remains imperfect.\nThe test error can be measured upstream (i.e. on the pre-training task) or downstream when fine-tuning\non a different task. We investigate various pre-training schemes with different number of examples,\nparameter counts and training times. We subsample ImageNet21k proportionally across classes and\npre-train variously sized inverted bottleneck MLPs. We summarize the configurations in Table 3.\nWe then measure test error on the downstream task of CIFAR100 in Fig. 1 as well as CIFAR10 and\nImageNet1k in Fig. 5 by linearly transferring the learnt features (without test-time augmentations).\nThe plotting style is inspired by Zhai et al. (2022). Each point in the curve is the downstream\nperformance of an MLP, where the color of the point indicates the model type (blue denotes smaller\nand red larger models) and the size of the point indicates the number of pre-training examples. Points\nconnected by a line indicates longer training times where T \u2208 {50, 100, 200, 400, 800} is measured\nin epochs. In all experiments, we employ data augmentation for pre-training. We observe that\nthe compute-optimal performance of MLPs strongly exhibits characteristics of a power-law with\ncoefficients \u03b1 \u2208 {0.12, 0.25, 0.35}. This is very encouraging for future theoretical work, showing\nthat MLPs indeed mirror the scaling behaviour of modern models. We provide the dual results for\nthe standard MLPs in Appendix B.2, noting that they exhibit essentially the same scaling behaviour,\nalbeit with a slightly weaker slope and intercept.\nWe further study how performance E evolves when compute is either bottlenecked by the number\nof parameters P or the dataset size N. We visualize the resulting scaling laws in Fig. 6. We find a\nvery steep decay rate in terms of parameters P where roughly \u03b1P \u2248 1, whereas for dataset size N\nwe identify a significantly slower rate of \u03b1N \u2248 0.35. This shows that the performance of MLPs is\nsignificantly more limited by the dataset size, which is in-line with the fact that MLPs exhibit a bad\ninductive bias. We investigate the role of dataset size and parameters more in the next paragraph.\nParameters or examples.\nGiven a fixed level of compute C, what is the optimal way to allocate\nit to parameter count P and number of examples N? In order to be more comparable to previous\n9\nwork, we assume a fixed training time T = 50. To answer this question, we follow the approach\noutlined in Hoffmann et al. (2022) and plot the optimal compute models identified in Fig. 1 both\nagainst model size P and number of examples N. We visualize the results in Fig. 7. We empirically\nobserve that the optimal parameter count P \u2217(C) and dataset size N \u2217(C) as a function of compute C\nexhibit power-law behaviour of the approximate form\nP \u2217(C) \u221d C0.35\nN \u2217(C) \u221d C0.65\nWhile for transformers, the number of examples (or tokens) N and parameters P are scaled equally\n(Hoffmann et al., 2022) (i.e. \u03b1P \u2248 \u03b1N \u2248 0.5), in contrast we observe that the optimal strategy for\nMLPs invests significantly more compute into dataset size N. This is further evidence for the weaker\ninductive bias present in MLPs, which needs more examples in order to be compensated for.\n4.5\nComputational Feasibility\nWe believe that a further exciting feature of our study is its computational feasibility, while at the\nsame time preserving the main characteristics of large-scale pre-training. All of our experiments were\nconducted on a single NVIDIA RTX A5000 GPU with 24GB of memory. In conjunction with the\nstrongly optimized FFCV dataloading framework (Leclerc et al., 2023) and the inherent efficiency\nof MLPs, we are able to perform very rapid training. For instance we complete a single epoch on\nImageNet21k with the B-12/Wi-1024 architecture, equipped with 124 million parameters, in only\nroughly 450 seconds, while the smaller variant B-6/Wi-1024 at a parameter count of 74 million\nrequires roughly 250 seconds on the specified hardware. Low memory requirements allow us to\ntrain with a batch-size of 16384 without having to shard computation among multiple GPUs. We\ncompare the computational efficiency of MLPs with contemporary networks of similar size such as\nResNet-152, ViT-B/4 and ViT-B/8 in Appendix A.5.\n5\nRelated Works\nThere are some prior works that investigate MLPs on vision tasks. Lin et al. (2015) study the\nperformance of MLPs on small scale datasets such as CIFAR10. They observe similar improvements\nwhen using inverted bottleneck layers but do not study larger-scale setups, transfer-learning nor do\nthey discuss the implications for theoretical works. The bottleneck structure used in this work has also\nbeen investigated theoretically (Parhi and Nowak, 2021; Shenouda et al., 2023; Parkinson et al., 2023),\nfurther highlighting that such an architecture exhibits desirable properties. Urban et al. (2017) study to\nwhat degree convolutions are necessary for good performance and conclude that even with distillation\ntechniques it remains very difficult to train performant MLPs on CIFAR10. Other approaches have\nfocused on sparsifying fully-connected layers through evolutionary training (Mocanu et al., 2018;\nFernando et al., 2016), aiming to learn a good inductive bias from scratch. Similarly, Neyshabur\n(2020) study how the inductive bias of MLPs can be improved by systematically sparsifying them\nwith a LASSO-type algorithm, making them more convolution-like. d'Ascoli et al. (2019) on the\nother hand first train a convolutional network for a certain duration and then subsequently continue\ntraining the network as an MLP (by using the correspondence between CNNs and MLPs highlighted\nin Sec. 2). They show that good performance can be reached if the network was trained long enough\nas a CNN. In contrast to these works, our goal is not to enhance the inherent inductive bias of MLPs\nbut study whether it can be overcome with enough scale.\nThe advent of the MLP-Mixer (Tolstikhin et al., 2021) has led to a series of follow-up work, similarly\nusing MLPs as a patch processor and token mixer (Touvron et al., 2021; Chen et al., 2022; Lian\net al., 2022; Guo et al., 2021; Liu et al., 2021). Again, we remark that these architectures all possess\nsignificantly more inductive bias.\nFinally, we would like to remark that MLPs are successfully used in other areas such as novel view\nsynthesis (e.g. NeRF (Mildenhall et al., 2021)).\n6\nDiscussion\nIn this work, we have explored the limits of the multi-layer perceptron as an architecture for vision\ntasks. Our study reveals that (1) lack of inductive bias can be compensated by scale and (2) MLPs\n10\nFigure 7: Optimal model size (left) and number of examples (right) for a given level of compute for\nlinear evaluation on CIFAR100, on a log-log scale.\nconstitute a (largely) accurate proxy for modern architectures, further cementing their role as the\nmain theoretical object of study. The role of data augmentation and the implicit bias of SGD however\nstrongly differ for MLPs in the setting considered in this work and theoretical works should take this\ninto account. Large-scale pre-training of MLPs proves to be very efficient, enabling researchers with\nless access to computational resources to study this very exciting line of work. While lack of inductive\nbias does not prevent MLPs from reaching impressive performance, it leads to an interesting shift in\ncompute-optimality towards more training examples. Subjecting MLPs to even larger amounts of\ncompute similar to Zhai et al. (2022), especially in the form of more training examples, remains as\nvery interesting future work.\nReferences\nAllen-Zhu, Z., Li, Y., and Liang, Y. (2019a). Learning and generalization in overparameterized\nneural networks, going beyond two layers. In Wallach, H., Larochelle, H., Beygelzimer, A.,\nd'Alch\u00e9-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nAllen-Zhu, Z., Li, Y., and Song, Z. (2019b). A convergence theory for deep learning via over-\nparameterization. In International Conference on Machine Learning, pages 242\u2013252. PMLR.\nAmari, S. (1967). A theory of adaptive pattern classifiers. IEEE Transactions on Electronic Computers,\nEC-16(3):299\u2013307.\nAnagnostidis, S., Bachmann, G., Noci, L., and Hofmann, T. (2023). The curious case of benign\nmemorization. In The Eleventh International Conference on Learning Representations.\nArora, S., Cohen, N., and Hazan, E. (2018). On the optimization of deep networks: Implicit\nacceleration by overparameterization. In International Conference on Machine Learning, pages\n244\u2013253. PMLR.\nArora, S., Du, S. S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R. (2019a). On exact computation\nwith an infinitely wide neural net. In Neural Information Processing Systems.\nArora, S., Du, S. S., Hu, W., Li, Z., and Wang, R. (2019b). Fine-grained analysis of optimization and\ngeneralization for overparameterized two-layer neural networks. In International Conference on\nMachine Learning.\nBa, J. L., Kiros, J. R., and Hinton, G. E. (2016). Layer normalization.\nBahri, Y., Dyer, E., Kaplan, J., Lee, J., and Sharma, U. (2021). Explaining neural scaling laws.\nBattaglia, P. W., Hamrick, J. B., Bapst, V., Sanchez-Gonzalez, A., Zambaldi, V., Malinowski, M.,\nTacchetti, A., Raposo, D., Santoro, A., Faulkner, R., Gulcehre, C., Song, F., Ballard, A., Gilmer, J.,\nDahl, G., Vaswani, A., Allen, K., Nash, C., Langston, V., Dyer, C., Heess, N., Wierstra, D., Kohli,\n11\nP., Botvinick, M., Vinyals, O., Li, Y., and Pascanu, R. (2018). Relational inductive biases, deep\nlearning, and graph networks.\nBeyer, L., H\u00e9naff, O. J., Kolesnikov, A., Zhai, X., and van den Oord, A. (2020). Are we done with\nimagenet?\nBrutzkus, A. and Globerson, A. (2017). Globally optimal gradient descent for a ConvNet with\nGaussian inputs. In Precup, D. and Teh, Y. W., editors, Proceedings of the 34th International\nConference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages\n605\u2013614. PMLR.\nCaballero, E., Gupta, K., Rish, I., and Krueger, D. (2023). Broken neural scaling laws. In The\nEleventh International Conference on Learning Representations.\nCaron, M., Touvron, H., Misra, I., J\u2019egou, H., Mairal, J., Bojanowski, P., and Joulin, A. (2021). Emerg-\ning properties in self-supervised vision transformers. 2021 IEEE/CVF International Conference\non Computer Vision (ICCV), pages 9630\u20139640.\nChen, S., Xie, E., GE, C., Chen, R., Liang, D., and Luo, P. (2022). CycleMLP: A MLP-like\narchitecture for dense prediction. In International Conference on Learning Representations.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for contrastive\nlearning of visual representations. In III, H. D. and Singh, A., editors, Proceedings of the 37th\nInternational Conference on Machine Learning, volume 119 of Proceedings of Machine Learning\nResearch, pages 1597\u20131607. PMLR.\nChen, X., Liang, C., Huang, D., Real, E., Wang, K., Liu, Y., Pham, H., Dong, X., Luong, T., Hsieh,\nC.-J., Lu, Y., and Le, Q. V. (2023). Symbolic discovery of optimization algorithms.\nChizat, L. and Bach, F. (2020). Implicit bias of gradient descent for wide two-layer neural networks\ntrained with the logistic loss. In Abernethy, J. and Agarwal, S., editors, Proceedings of Thirty\nThird Conference on Learning Theory, volume 125 of Proceedings of Machine Learning Research,\npages 1305\u20131338. PMLR.\nChrabaszcz, P., Loshchilov, I., and Hutter, F. (2017). A downsampled variant of imagenet as an\nalternative to the cifar datasets.\nCoates, A., Ng, A., and Lee, H. (2011). An analysis of single-layer networks in unsupervised feature\nlearning. In Gordon, G., Dunson, D., and Dud\u00edk, M., editors, Proceedings of the Fourteenth\nInternational Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of\nMachine Learning Research, pages 215\u2013223, Fort Lauderdale, FL, USA. PMLR.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-\nscale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248\u2013255.\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani,\nM., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. (2021). An image is\nworth 16x16 words: Transformers for image recognition at scale. In International Conference on\nLearning Representations.\nd'Ascoli, S., Sagun, L., Biroli, G., and Bruna, J. (2019). Finding the needle in the haystack with\nconvolutions: on the benefits of architectural bias. In Wallach, H., Larochelle, H., Beygelzimer,\nA., d'Alch\u00e9-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing\nSystems, volume 32. Curran Associates, Inc.\nDu, S. S., Zhai, X., Poczos, B., and Singh, A. (2019). Gradient descent provably optimizes over-\nparameterized neural networks. In International Conference on Learning Representations.\nFernando, C., Banarse, D., Reynolds, M., Besse, F., Pfau, D., Jaderberg, M., Lanctot, M., and\nWierstra, D. (2016). Convolution by evolution: Differentiable pattern producing networks. In\nProceedings of the Genetic and Evolutionary Computation Conference 2016, GECCO \u201916, page\n109\u2013116, New York, NY, USA. Association for Computing Machinery.\n12\nGoyal, P., Doll\u00e1r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and\nHe, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour.\nGrill, J.-B., Strub, F., Altch\u00e9, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires,\nB., Guo, Z., Gheshlaghi Azar, M., Piot, B., kavukcuoglu, k., Munos, R., and Valko, M. (2020).\nBootstrap your own latent - a new approach to self-supervised learning. In Larochelle, H., Ranzato,\nM., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing\nSystems, volume 33, pages 21271\u201321284. Curran Associates, Inc.\nGunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. (2018). Implicit bias of gradient descent on linear\nconvolutional networks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi,\nN., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 31.\nCurran Associates, Inc.\nGuo, J., Tang, Y., Han, K., Chen, X., Wu, H., Xu, C., Xu, C., and Wang, Y. (2021). Hire-mlp: Vision\nmlp via hierarchical rearrangement. 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 816\u2013826.\nHe, K., Zhang, X., Ren, S., and Sun, J. (2015). Deep residual learning for image recognition. 2016\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778.\nHestness, J., Ardalani, N., and Diamos, G. (2019). Beyond human-level accuracy: Computational\nchallenges in deep learning.\nHestness, J., Narang, S., Ardalani, N., Diamos, G., Jun, H., Kianinejad, H., Patwary, M. M. A., Yang,\nY., and Zhou, Y. (2017). Deep learning scaling is predictable, empirically.\nHoffer, E., Hubara, I., and Soudry, D. (2017). Train longer, generalize better: closing the general-\nization gap in large batch training of neural networks. In Guyon, I., Luxburg, U. V., Bengio, S.,\nWallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D.,\nHendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche,\nG., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., Rae, J. W., Vinyals, O., and Sifre,\nL. (2022). Training compute-optimal large language models.\nHron, J., Bahri, Y., Sohl-Dickstein, J., and Novak, R. (2020). Infinite attention: NNGP and NTK for\ndeep attention networks. In III, H. D. and Singh, A., editors, Proceedings of the 37th International\nConference on Machine Learning, volume 119 of Proceedings of Machine Learning Research,\npages 4376\u20134386. PMLR.\nIvakhnenko, A., Lapa, V., and ENGINEERING., P. U. L. I. S. O. E. (1965). Cybernetic Predicting\nDevices. JPRS 37, 803. Joint Publications Research Service [available from the Clearinghouse for\nFederal Scientific and Technical Information].\nJacot, A., Gabriel, F., and Hongler, C. (2018). Neural tangent kernel: Convergence and generalization\nin neural networks. In Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,\nand Garnett, R., editors, Advances in Neural Information Processing Systems, volume 31. Curran\nAssociates, Inc.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A.,\nWu, J., and Amodei, D. (2020). Scaling laws for neural language models.\nKeskar, N. S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T. P. (2017). On large-batch\ntraining for deep learning: Generalization gap and sharp minima. In International Conference on\nLearning Representations.\nKrizhevsky, A. (2009). Learning multiple layers of features from tiny images.\nKrizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). Imagenet classification with deep convolu-\ntional neural networks. In Pereira, F., Burges, C., Bottou, L., and Weinberger, K., editors, Advances\nin Neural Information Processing Systems, volume 25. Curran Associates, Inc.\n13\nLe, Y. and Yang, X. S. (2015). Tiny imagenet visual recognition challenge.\nLeclerc, G., Ilyas, A., Engstrom, L., Park, S. M., Salman, H., and Madry, A. (2023). FFCV:\nAccelerating training by removing data bottlenecks.\nLi, Y. and Yuan, Y. (2017). Convergence analysis of two-layer neural networks with relu activation.\nIn Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett,\nR., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates,\nInc.\nLian, D., Yu, Z., Sun, X., and Gao, S. (2022). AS-MLP: An axial shifted MLP architecture for vision.\nIn International Conference on Learning Representations.\nLin, Z., Memisevic, R., and Konda, K. R. (2015). How far can we go without convolution: Improving\nfully-connected networks. ArXiv, abs/1511.02580.\nLiu, H., Dai, Z., So, D., and Le, Q. V. (2021). Pay attention to MLPs. In Beygelzimer, A., Dauphin,\nY., Liang, P., and Vaughan, J. W., editors, Advances in Neural Information Processing Systems.\nLiu, Z., Mao, H., Wu, C.-Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022). A convnet for the\n2020s. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 11976\u201311986.\nMaloney, A., Roberts, D. A., and Sully, J. (2022). A solvable model of neural scaling laws.\nMei, S. and Montanari, A. (2021). The generalization error of random features regression: Precise\nasymptotics and the double descent curve. Communications on Pure and Applied Mathematics, 75.\nMei, S., Montanari, A., and Nguyen, P.-M. (2018). A mean field view of the landscape of two-layer\nneural networks. Proceedings of the National Academy of Sciences, 115(33):E7665\u2013E7671.\nMildenhall, B., Srinivasan, P. P., Tancik, M., Barron, J. T., Ramamoorthi, R., and Ng, R. (2021). Nerf:\nRepresenting scenes as neural radiance fields for view synthesis. Commun. ACM, 65(1):99\u2013106.\nMocanu, D., Mocanu, E., Stone, P., Nguyen, P., Gibescu, M., and Liotta, A. (2018). Scalable training\nof artificial neural networks with adaptive sparse connectivity inspired by network science. Nature\nCommunications, 9.\nNeyshabur, B. (2020). Towards learning convolutions from scratch. In Larochelle, H., Ranzato, M.,\nHadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems,\nvolume 33, pages 8078\u20138088. Curran Associates, Inc.\nNeyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N. (2019).\nThe role of over-\nparametrization in generalization of neural networks. In International Conference on Learning\nRepresentations.\nNeyshabur, B., Tomioka, R., and Srebro, N. (2014). In search of the real inductive bias: On the role\nof implicit regularization in deep learning.\nOpenAI (2018). Ai and compute.\nOpenAI (2023). Gpt-4 technical report.\nParhi, R. and Nowak, R. D. (2021). What kinds of functions do deep neural networks learn? insights\nfrom variational spline theory. SIAM J. Math. Data Sci., 4:464\u2013489.\nParkinson, S., Ongie, G., and Willett, R. (2023). Linear neural network layers promote learning\nsingle- and multiple-index models.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein,\nN., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy,\nS., Steiner, B., Fang, L., Bai, J., and Chintala, S. (2019). Pytorch: An imperative style, high-\nperformance deep learning library. In Advances in Neural Information Processing Systems 32,\npages 8024\u20138035. Curran Associates, Inc.\n14\nPoole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. (2016). Exponential expressivity\nin deep neural networks through transient chaos. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I.,\nand Garnett, R., editors, Advances in Neural Information Processing Systems, volume 29. Curran\nAssociates, Inc.\nRidnik, T., Ben-Baruch, E., Noy, A., and Zelnik, L. (2021). Imagenet-21k pretraining for the masses.\nIn Vanschoren, J. and Yeung, S., editors, Proceedings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks, volume 1. Curran.\nRoberts, D. A., Yaida, S., and Hanin, B. (2022). The Principles of Deep Learning Theory: An\nEffective Theory Approach to Understanding Neural Networks. Cambridge University Press.\nRosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization\nin the brain. Psychological review, 65(6):386.\nRosenfeld, J. S., Rosenfeld, A., Belinkov, Y., and Shavit, N. (2020). A constructive prediction of the\ngeneralization error across scales. In International Conference on Learning Representations.\nSaxe, A. M., McClelland, J. L., and Ganguli, S. (2014). Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks.\nSchoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J. (2017). Deep information propaga-\ntion. In International Conference on Learning Representations.\nShenouda, J., Parhi, R., Lee, K., and Nowak, R. D. (2023). Vector-valued variation spaces and width\nbounds for dnns: Insights on weight decay regularization. ArXiv, abs/2305.16534.\nSoudry, D., Hoffer, E., and Srebro, N. (2018). The implicit bias of gradient descent on separable data.\nIn International Conference on Learning Representations.\nTan, M. and Le, Q. V. (2020). Efficientnet: Rethinking model scaling for convolutional neural\nnetworks.\nTolstikhin, I., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner,\nA. P., Keysers, D., Uszkoreit, J., Lucic, M., and Dosovitskiy, A. (2021). MLP-mixer: An all-MLP\narchitecture for vision. In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W., editors,\nAdvances in Neural Information Processing Systems.\nTouvron, H., Bojanowski, P., Caron, M., Cord, M., El-Nouby, A., Grave, E., Izacard, G., Joulin,\nA., Synnaeve, G., Verbeek, J., and Jegou, H. (2021). ResMLP: Feedforward networks for image\nclassification with data-efficient training.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal,\nN., Hambro, E., Azhar, F., et al. (2023). Llama: Open and efficient foundation language models.\narXiv preprint arXiv:2302.13971.\nTrockman, A. and Kolter, J. Z. (2022). Patches are all you need?\nUrban, G., Geras, K. J., Kahou, S. E., Aslan, O., Wang, S., Mohamed, A., Philipose, M., Richardson,\nM., and Caruana, R. (2017). Do deep convolutional nets really need to be deep and convolutional?\nIn International Conference on Learning Representations.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and\nPolosukhin, I. (2017). Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach,\nH., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates, Inc.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E.,\nPeterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J.,\nMayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, \u02d9I., Feng, Y.,\nMoore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero,\nE. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy\n1.0 Contributors (2020). SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.\nNature Methods, 17:261\u2013272.\n15\nYou, Y., Gitman, I., and Ginsburg, B. (2017). Large batch training of convolutional networks.\nYu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., Feng, J., and Yan, S. (2022). Metaformer is\nactually what you need for vision.\nZhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L. (2022). Scaling vision transformers. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npages 12104\u201312113.\nZhang, H., Cisse, M., Dauphin, Y. N., and Lopez-Paz, D. (2018). mixup: Beyond empirical risk\nminimization. In International Conference on Learning Representations.\nZou, D., Cao, Y., Zhou, D., and Gu, Q. (2020). Gradient descent optimizes over-parameterized deep\nrelu networks. Machine Learning, 109:1\u201326.\n16\nAppendix\nA\nExperimental Details\nA.1\nResources\nFor all experiments we rely on NVIDIA RTX A5000 GPU with 24GB of memory. Every experiment\ncan be performed on a single GPU. We leverage the FFCV dataloader framework since the transfer\ntime of the data to the GPU becomes the bottleneck in terms of training time in case of MLPs. All of\nour experiments were performed in PyTorch (Paszke et al., 2019).\nA.2\nAdditional Ablations\nAblations.\nWe provide some more ablations in Fig. 8. More specifically, for a (approximate) fixed\nbudget of compute, we investigate different architecture and optimization choices, when pretraining\non ImageNet1k and performing linear probing on CIFAR100.\nInput image\nFlatten\nMLP\nB{X}_{ACT} :=\nLogits \nprojection\nACT\nACT\nACT\n...\nX linear layers\nB{X}x{Y}_{ACT}\nACT\n...\n:=\n...\nX linear layers\nY blocks\n:=\nACT\nACT\n...\n...\nX linear layers\nY blocks\nACT\nNORM\nNO_AUG: No Augmentation NO_MIXUP: No mixup NO_CLIP: No clip gradients \nNO_WD: No weight decay, DROPOUT: Use dropout, AdamW: Replce Lion with AdamW\nArchitecture\nOptimization\nB{X}x{Y}_IB_{NORM}_{ACT}\n(IB: Inverted Bottleneck)\nFigure 8: Ablations on different architectures and optimizations choices when training on ImageNet.\nNumbers indicate linear probing Top-1 accuracies on CIFAR100.\nNormalization.\nWe investigate the importance of the normalization method (LayerNorm vs Batch-\nNorm) in more detail in Table 4. We pre-train two B-MLPs on ImageNet21k with layer normalization\nand batch normalization and compare the fine-tuning performance on various tasks. We find that the\ntechniques perform similarly, which layer normalisation having a slight edge.\nCIFAR-10\nCIFAR-100\nTINYIMAGENET\nIMAGENET\nLAYERNORM\n90.0\n74.6\n59.6\n36.2\nBATCHNORM\n89.4\n73.8\n57.7\n35.9\nTable 4: Pretraining a B-6/Wi-1024 B-MLP with BatchNorm and LayerNorm on ImageNet21k and\nsubsequently fine-tuning.\nLabel smoothing.\nWe further ablate the influence of label smoothing on the downstream perfor-\nmance. We pre-train B-MLPs with varying amounts of label smoothing (\u03b1 \u2208 {0.0, 0.1, 0.3}) and\nevaluate the resulting down-stream fine-tuning performance. We report the results in Table 5. While\nlabel smoothing does provide some boost in performance, the gains are very modest. Label smoothing\nis thus helpful but not essential for training MLPs.\n17\nCIFAR10\nCIFAR100\nTINYIMAGENET\nIMAGENET\n\u03b1 = 0.3\n90.0\n74.6\n59.6\n36.2\n\u03b1 = 0.1\n89.5\n73.7\n58.2\n36.0\n\u03b1 = 0.0\n89.2\n72.2\n57.1\n35.7\nTable 5: Pretraining a B-6/Wi-1024 B-MLP with different amounts of label smoothing on Ima-\ngeNet21k and subsequently fine-tuning.\nArchitecture.\nWe make the following observations/recommendations to boost the model\u2019s per-\nformance, in line with results reported in the literature (Liu et al., 2022); (1) replacing ReLUs\nand GELUs boosts results significantly, (2) adding skip connections every two layers helps with\noptimization, especially for deeper networks. (3) Using an inverted bottleneck increases performance\neven more. (4) Using a normalization layer in the PRE-LN configuration helps with optimization and\n(4) layer normalization leads to significantly better results compared to batch normalization, while\nalso being more stable during training.\nOptimization.\nAs discussed in the main text, augmentations are crucial, and disabling them can\nhave a detrimental effect. We also found that clipping gradients, using weight decay and dropout\nhave a small positive effect on downstream performance. Finally, replacing LION (Chen et al., 2023)\nwith Adam(W), leads to a decrease in performance.\nA.3\nLinear Probing\nWe showcase the transferability of our MLPs by training a linear classifier on top of the frozen\nfeatures. For training the linear layer, we use the LION optimizer with a learning rate of \u03b7 = 0.00001\nfor 50 epochs. We display the results in Table 6. We observe very strong down-stream performance\nCIFAR10\nCIFAR100\nSTL10\nTINYIMAGENET\nIMAGENET\nB-6/Wi-1024\n65.1\n41.3\n53.4\n45.6\n13.0\nB-6/Wi-1024 + DA\n87.8\n73.2\n85.2\n61.3\n39.2\nB-12/Wi-1024 + DA\n90.6\n74.5\n88.3\n68.5\n40.7\nTable 6: Linear probing Top-1 accuracies when pretraining on ImageNet21k.\neven in this more limited setting, highlighting how transferable the features learnt by MLPs are.\nA.4\nScaling Laws\nImplementation Details.\nFor the scaling law plots, we trained all the models with a batch-size\n16384 and the LION optimizer with a learning rate \u03b7 = 0.00001 and weight decay of strength 0.001.\nWe further use label smoothing of strength 0.3. We again use augmentations in the form of random\nflips and crops as well as MixUp with strength 0.8. We rely on the curvefit function from the SciPy\nlibrary (Virtanen et al., 2020) to fit powerlaws of the form E(C) = a(b + C)\u2212\u03b1 + E\u221e.\nA.5\nComputational Efficiency\nWe highlight the fact that although MLPs require a lot of training data, inference is extremely efficient\nfrom a computational perspective. To illustrate this, we embark on the following comparison; we\nstudy inference on 64 \u00d7 64 resolution images in an MLP vs other popular vision architectures of\nsimilar size and complexity in Table 7. More specifically, we compare against a ResNet-152, where\nwe replace the stride in the first convolutional layer and remove the first max-pooling operation to\n18\ncompensate for the smaller image size. We also compare against a base ViT and Mixer model, where\nwe extract patches from 4 \u00d7 4 regions in the original image.\nAs it quickly becomes eminent, MLPs require significantly less FLOPs to make predictions on\nindividual images, in essence utilizing their parameters a lot more methodically. As a result, latency\nand throughput are significantly better compared to other candidate architectures. We measure\nthroughput using the optimal batch size on an NVIDIA RTX A5000. We highlight, that our MLPs, in\ncontrast to the other architectures are memory bound, meaning that their throughput is determined by\nthe prefetching bandwidth of our GPU. Hardware advancement and specialized architectures could\nsignificantly mitigate this effect. Neglecting memory transfer time by propagating the same input\nthrough our network gives a further 6-fold increase in the potential throughput.\nPARAMETERS\nLATENCY\n(MSEC)\nTHROUGHPUT\n(IMAGES/SEC)\nFLOPS PER FORWARD PASS\nB-12/Wi-768\n66.89 M\n21.2\n16063\n66.8 M\nResNet-152\n60.19 M\n423\n506\n13.07 G\nViT-B/4\n86.06 M\n424\n222\n23.08 G\nMixer-B/4\n63.82 M\n400\n319\n19.36 G\nTable 7: Various measures assessing the computational efficiency of different architectures.\nB\nResults for Standard MLPs\nB.1\nTransfer Learning\nFor completeness we also analyze the transfer performance of standard MLPs when pre-trained on\nImageNet21k. We compare a S-MLP of depth 6 and width 2048 against a B-MLP of depth 6 and\nwidth 1024, ensuring that both models roughly have a parameter count of around \u2248 70 million. We\ndisplay the results in Table 8. We observe that even the features of a standard MLP (i.e. without\nresidual connections and bottleneck structure) transfer very well on different downstream task. Tthe\ninverted-bottleneck MLP however still remains superior.\nCIFAR10\nCIFAR100\nTINYIMAGENET\nIMAGENET\nS-MLP\n87.1\n68.3\n52.1\n30.2\nB-MLP\n90.0\n74.6\n59.6\n36.2\nTable 8: Comparing a S-MLP of width 2048 and depth 6 pre-trained on ImageNet21k, with a B-6/Wi-\n1024 B-MLP (both models around 70M params) in terms of fine-tuning performance\nB.2\nScaling Laws\nWe also evaluate the scaling law of standard MLPs by training variously sized models on different\nsubsets of ImageNet21k and subsequently linearly probing the features on CIFAR100. The setting is\nidentical to the one described in 4.4. We observe that also standard MLPs exhibit power-law behaviour.\nThe slope (0.22 vs 0.25) and the intercept (0.18 vs 0.16) are however worse when compared against\nthe inverted-bottleneck MLP.\nC\nWeight visualizations\nWe visualize the first layer weights W (1) \u2208 R3wh\u00d7m by reshaping them back to Rw\u00d7h\u00d73\u00d7m. We\nthen produce a Rw\u00d7h\u00d7m representation by taking the maximal value along the channel dimension.\n19\nFigure 9: Test error of standard MLPs on CIFAR100 when linearly transferred as a function of\nPFLOPS, measured according to Eq.(4), on a log-log scale.\nFigure 10: Visualization of the first layer weights for different pre-training dataset sizes.\nWe display such visualizations of the first 5 \u00d7 5 = 25 \"filters\" for different pre-training sizes,\nalso including the weights at random initialization in Fig. 10. All models were trained with data\naugmentation. We observe that filters increasingly develop structure as we increase the dataset size\nand become more and more localized. We further compare against models that were pre-trained on\nthe full ImageNet21k, with and without data augmentation in Fig. 11. We observe that even though\nwe provide the model with an abundance of samples, the weights still remain largely structure-less\n20\nFigure 11: Visualization of the first layer weights for models trained with and without data augmenta-\ntion.\nand have not developped any locality properties. On the other hand, using data augmentation leads to\nmore adapted filters.\nD\nInverted Bottleneck MLP Code\nWe provide PyTorch-style pseudo-code for the inverted bottleneck MLP to highlight its simplicity.\n1 from\ntorch\nimport nn\n2\n3 class\nBlock(nn.Module):\n4\ndef\n__init__(self , dim , expansion_factor =4, dropout =0.):\n5\nsuper ().__init__ ()\n6\nself.fn = nn.Sequential(\n7\nnn.Linear(dim , int( expansion_factor * dim)),\n8\nnn.GELU (),\n9\nnn.Dropout(dropout),\n10\nnn.Linear(int( expansion_factor * dim), dim),\n11\nnn.Dropout(dropout)\n12\n)\n13\nself.ln = nn.LayerNorm(dim)\n14\n15\ndef\nforward(self , x):\n16\nreturn x + self.fn(self.ln(x))\n17\n18\n19 def MLP(image_size , channels , dim , depth , num_classes ,\nexpansion_factor =4, dropout =0.):\n20\nreturn nn.Sequential(\n21\nnn.Flatten(start_dim =1, end_dim =-1),\n22\nnn.Linear(image_size * image_size * channels , dim),\n23\n*[ Block(dim , expansion_factor , dropout) for _ in range(depth)\n],\n24\nnn.Linear(dim , num_classes)\n25\n)\n21\n"
  },
  {
    "title": "System-Level Natural Language Feedback",
    "link": "https://arxiv.org/pdf/2306.13588.pdf",
    "upvote": "10",
    "text": "System-Level Natural Language Feedback\nWeizhe Yuan\nNew York University\nwy885@nyu.edu\nKyunghyun Cho\nNew York University\nPrescient Design, Genentech\nJason Weston\nNew York University\nAbstract\nNatural language (NL) feedback offers rich in-\nsights into user experience. While existing stud-\nies focus on an instance-level approach, where\nfeedback is used to refine specific examples,\nwe introduce a framework for system-level use\nof NL feedback. We show how to use feedback\nto formalize system-level design decisions in\na human-in-the-loop-process \u2013 in order to pro-\nduce better models. In particular this is done\nthrough: (i) metric design for tasks; and (ii) lan-\nguage model prompt design for refining model\nresponses. We conduct two case studies of this\napproach for improving search query and dia-\nlog response generation, demonstrating the ef-\nfectiveness of system-level feedback. We show\nthe combination of system-level and instance-\nlevel feedback brings further gains, and that\nhuman written instance-level feedback results\nin more grounded refinements than GPT-3.5\nwritten ones, underlying the importance of hu-\nman feedback for building systems. We release\nour code and data at https://github.com/\nyyy-Apple/Sys-NL-Feedback.\n1\nIntroduction\nUsers interacting with a machine learning system\noffer feedback, either actively or passively. The\nfeedback can be binary ratings (Arora et al., 2022),\npreference feedback (Stiennon et al., 2020) and\nnatural language (NL) feedback (Hancock et al.,\n2019; Scheurer et al., 2022a). Among them, NL\nfeedback is the most general due to its free-form\nnature, as opposed to the limited choices in other\nfeedback forms. Hence, it is crucial to harness the\npotential of NL feedback to improve a system.\nExisting research on NL feedback typically\nadopts one of two strategies. The first uses feed-\nback as an auxiliary target in addition to the orig-\ninal task, just like in multitask learning (Hancock\net al., 2019; Xu et al., 2022b). The second modifies\nthe original output based on per-instance feedback.\nThe system can either be fine-tuned with the new\noutput (Tandon et al., 2022; Scheurer et al., 2022b)\nor iteratively self-critique and self-refine at infer-\nence time (Madaan et al., 2023; Chen et al., 2023b).\nOne common limitation of these studies is that\nthey only focus on instance-level learning, where\neach feedback only serves the instance for which it\nwas received. Furthermore, they often assume the\navailability of feedback for each and every exam-\nple, which is not practical in real-world scenarios,\nwhere feedback is often sparse.\nThis paper asks the following question: Can\nwe aggregate instance-level NL feedback to make\nsystem-level design decisions that improve lan-\nguage generation systems? We answer this ques-\ntion by proposing a general framework for aggre-\ngating instance-level NL feedback. A set of cri-\nteria (i.e., system-level feedback) are first derived\nfrom instance-level feedback through a human-in-\nthe-loop process involving clustering and summa-\nrization. Those criteria then guide the design of\ninstruction-following language model prompts to\nrefine (i.e., correct) examples, and the development\nof metrics that align with users\u2019 needs. We con-\nduct two case studies of the proposed framework\non information-seeking dialog tasks where we im-\nprove both the query generator and the response\ngenerator of an Internet-augmented dialog system.\nThe experimental results point to the effectiveness\nof system-level feedback. Our contributions are:\n\u2022 We propose a new method that derives system-\nlevel feedback from instance-level feedback,\nwhich can guide text generation refinement.\n\u2022 We show how human experts can use system-\nlevel feedback to design metrics for evaluating\ninformation-seeking dialog systems.\n\u2022 We demonstrate that combining system-level\nand instance-level feedback for prompt design\nyields more helpful refinements for system\ntraining w.r.t. the designed metrics above.\narXiv:2306.13588v3  [cs.CL]  3 Feb 2024\n\u2022 We show the importance of human NL feed-\nback by comparing it to GPT-3.5-generated\nfeedback in response refinement. We find that\nhuman feedback leads to more grounded re-\nfinements that can better guide system learn-\ning.\n2\nRelated Work\nDialog Systems The rapid development of large\nlanguage models (LLMs) (Brown et al., 2020;\nZhang et al., 2022) has advanced dialog systems, in-\ncorporating techniques like multi-session memory\n(Xu et al., 2022a), search engine support (Komeili\net al., 2022), etc. Recently, ChatGPT\u2019s rise has\ncaptivated both the NLP community and the pub-\nlic at large. Nowadays, intelligent dialog agents\nhave become an essential part of people\u2019s produc-\ntivity, such as brainstorming (Zhang et al., 2023b),\nessay polishing (Buruk, 2023), code writing (Haen-\nsch et al., 2023), etc. However, LLMs also carry\npotential risks including misinformation (Chern\net al., 2023), sycophancy (Sharma et al., 2023),\netc., which calls for more thorough evaluations.\nLearning from Human Feedback As language\nmodels increasingly integrate into people\u2019s daily\nlife, aligning them with human needs becomes es-\nsential (Askell et al., 2021). As a result, researchers\nhave been working on utilizing various human\nfeedback, including preference feedback (Stien-\nnon et al., 2020; Ouyang et al., 2022), binary feed-\nback (Li et al., 2019; Arora et al., 2022; Adolphs\net al., 2022), NL feedback (Weston, 2016; Li et al.,\n2017; Hancock et al., 2019; Saunders et al., 2022;\nScheurer et al., 2022a), and so on. So far, the use of\nNL feedback is relatively less explored, with most\nstudies focusing on instance-level feedback where\neach instance receives its own feedback (Scheurer\net al., 2022a, 2023). In this work, we propose a\ngeneral framework for deriving system-level feed-\nback from instance-level feedback, and show the\neffectiveness of system-level feedback alone and\nits complementarity with instance-level feedback.\n3\nMethodology\n3.1\nProblem Formulation\nAssume we have (1) a text generator P\u03b8(r|q) that\ngenerates a response r to a query q, (2) a text re-\nfiner P\u03d5(r\u2032|r, q, c) that generates a refinement r\u2032\ngiven the original response r, the query q, and cri-\nteria c that explains what makes a good response,\n(3) a quality checker Q(q, r) that decides whether\nr is a satisfactory response given q. When de-\nploying P\u03b8(r|q), for some unsatisfied responses\nRn = {r1, \u00b7 \u00b7 \u00b7 , rn}, we collect NL feedback for\neach of them Fn = {f1, \u00b7 \u00b7 \u00b7 , fn}. We aim to use\nFn to improve P\u03b8(r|q) by updating its parameters\n\u03b8. In our setting, we take the text refiner and qual-\nity checker as given. They can either be based on\nlarge models like GPT-3 (Scheurer et al., 2022a) or\nspecialized fine-tuned models (Shi et al., 2022).\n3.2\nProposed Framework\nOur proposed framework is shown in Figure 1.\nThere are four steps within this framework.\nDerive criteria from feedback When deploying\nthe text generator P\u03b8(r|q), we collect feedback Fn\nfor some responses Rn. A clustering algorithm is\nthen run (e.g., k-means clustering (Hartigan and\nWong, 1979)) to identify common issues that can be\npotentially rectified. Next, a human-in-the-loop ap-\nproach is used, where human experts derive a set of\ncriteria c for what constitutes a good response from\nthose clusters. These criteria, articulated in natural\nlanguage, serve as part of the input (prompt) for\nthe text refiner. This process relates to prompt engi-\nneering in large language models (Liu et al., 2023),\nwhere the NL feedback is used to help formalize\nthe prompt engineering process. With these crite-\nria, experts also design metrics m1(\u00b7), \u00b7 \u00b7 \u00b7 , mk(\u00b7)\nto evaluate aspects of user interest.\nConstruct refinement training data To improve\nthe text generator, we create a training dataset, D,\nthat reinforces positive behaviors and rectifies neg-\native ones. If a sample (qi, ri) meets Q(qi, ri) = 1,\nit is added to D to reinforce good model behav-\nior. Otherwise, the text refiner P\u03d5(r\u2032|r, q, c) refines\nri to r\u2032\ni using prompts based on criteria c. If this\nrefined sample (qi, r\u2032\ni) passes Q(qi, r\u2032\ni) = 1, it is\nadded to D to modify bad behavior.\nFine-tune the model After collecting supervised\ndata D, we fine-tune the text generator P\u03b8(r|q).\nThis data can be combined with existing data that\nwas used to build the baseline deployed system\n(that did not use feedback).\nEvaluate using designed metrics Finally, we use\nour designed metrics to assess system performance\nagainst user requirements. If successful, the up-\ndated system will exhibit improved metrics m1(\u00b7),\n\u00b7 \u00b7 \u00b7 , mk(\u00b7) compared to the baseline system.\nD: Evaluate using designed metrics\nSupervised Fine-tune\nDesign\nMetrics\n1. m1(q, r)\n2. m2(q, r)\nEvaluate\nText \nC: Fine-tune the model\nText Generator\nq1\nq2\nq3\nq4\nq5\nf1\nf2\nf3\nUsers\nCluster\nCriteria ( )c\n1. \n2. \nA: Derive criteria from feedback\nUnsatisfied  \nSamples\nText Refiner\nQuality \nChecker\nr\u2032 1\nr\u2032 2\nr\u2032 3\nr1\nr2 r3\nSatisfied Samples\n(q4, r4) (q5, r5)\nSupervised \nData \ud835\udc9f\n(q4, r4)\n(q5, r5)\n(q1, r\u2032 1)\n(q2, r\u2032 2)\nB: Construct refinement training data\nc\nExperts\nMetrics\n1. m1(q, r)\n2. m2(q, r)\nQuality Checker\nDerive\nExperts\nProvide\nm1\nm2\nS1\nS2\nS3\nS4\nS5\nResults\nr1\nr2\nr3\nr4\nr5\nGenerator\nText \nGenerator\nData \ud835\udc9f\nFigure 1: Our framework for incorporating NL feedback into system-level model design. Using a human-in-the-loop\napproach, criteria derived from NL feedback guide the creation of prompts for refining responses and metric design\nto evaluate the improvements. Notation: q: query, r: response, f: feedback, r\u2032: refinement, m(\u00b7): metric function.\nS1 \u00b7 \u00b7 \u00b7 S5 represent different systems one can compare using this framework.\n4\nExperimental Setup: Dialog Systems\nWe study our framework within dialogue system de-\nployment, a context where users naturally offer NL\nfeedback, such as \u201cthat\u2019s not correct\u201d for incorrect\nresponses (Shi et al., 2022). Our case studies fo-\ncus on information-seeking dialogues, where users\ninteract with dialog agents to obtain answers or\nrelevant information (Glaese et al., 2022).\nDialog System Selection We choose the Blender-\nbot2 (BB2) dialog system (Komeili et al., 2022; Xu\net al., 2022a) comprised of two modules: (1) Query\nGenerator (QG) that generates an Internet search\nquery from dialogue history. (2) Response Genera-\ntor (RG) that generates a response using dialogue\nhistory and retrieved web documents.1 We select\nBB2 because it allows us to study two scenarios:\nquery generation and response generation.\nDeployment Data We use the FITS dataset (Xu\net al., 2022b) for experiments, which collects di-\nverse feedback from user interactions with Internet-\naugmented dialogue systems like BB2 and SeeKeR\n(Shuster et al., 2022). Though the dataset includes\nbinary, NL feedback, and gold corrections, we only\nuse binary and NL feedback, given users are less\ninclined to provide gold corrections for mistakes.\nText Refiner Given no gold corrections, we turn\nto model-based refinement techniques.\nIn this\nwork, we use GPT-3.52 as the text refiner and apply\ngreedy decoding during inference.\nQuality Checker We train quality checkers for\nqueries and final responses using collected binary\nfeedback. Our classifier is based on FLAN-T53\n1We use Google search (https://www.google.com/) to\nretrieve the top five relevant documents given a search query.\n2We use the model gpt-3.5-turbo for our experiments.\n3We use the flan-t5-large model.\n(Chung et al., 2022) trained on 20% training data,\nusing binary feedback following Shi et al. (2022).\nWe select a threshold to ensure 80% precision for\nlabels it predicts as positive on the validation set.\n5\nCase Study 1: Query Generation\n5.1\nDerive Criteria from Feedback\nWe collect all NL feedback from the FITS training\nsplit to understand human preferences and derive\ncriteria. We first use SimCSE encoder4 (Gao et al.,\n2021) to encode each feedback. Then, we use k-\nmeans clustering to group feedback related to query\ngeneration into five clusters. From inspecting these\n(see Appendix A.1 for detailed manual efforts),\nwe summarize them into four groups (see Table 1)\nand derive that a successful search query should (i)\nrephrase the user\u2019s question while keeping impor-\ntant keywords, (ii) be relevant and specific, (iii) use\ncommon words for better search coverage, (iv) be\nconcise. The criteria text for crafting the prompt c\nfor the text refiner P\u03d5(r\u2032|r, q, c) is in Table 2.\n5.1.1\nCriteria-guided Metric Design\nUsing feedback-derived criteria, we design metrics\nto mirror users\u2019 preferences.5 Ideally, an effective\nquery should score high across all these metrics.\nNon-copy rate measures how much a search query\nrephrases the user\u2019s utterance by examining n-gram\nmatching. We define it in Equation 1 based on\nBLEU-4 (Papineni et al., 2002) where s is the\n4We use the sup-simcse-roberta-large model.\n5When evaluating a set of queries, for a metric defined\nas a fraction with a constant numerator, we take the average\nof the denominators of all queries on that metric and take its\nreciprocal to multiply the numerator.\nGroup\nFeedback type\nNum.\n%\n1\nUser suggests a search query for Internet search directly.\n2715\n52.87%\n2\nSuggests specific edits, such as shortening the query or using common words, and so on.\n996\n19.40%\n3\nPoints out that the search query should use keywords instead of copying the original question\nand should be specific.\n995\n19.38%\n4\nPoints out that the search query is not relevant to the problem.\n429\n8.35%\nTable 1: Case study 1 (query generation): 4 groups of system-level feedback derived from automatic clustering.\nType\nCriteria (Abbreviated)\nNCR Spec. Read. Con.\nCov.\nSat.\n(1): Baseline\nNone\n4.06\n79.40 19.46 14.87 29.80 61.50\n(2): (1)+Rephrase\nRephrase the user\u2019s question and keep keywords.\n4.98\n83.20 19.54 15.04 26.50 62.10\n(3): (2)+Specificity\nAbove + Be accurate and specific for user needs.\n5.00\n84.20 18.77 14.50 28.80 63.30\n(4): (3)+Readability\nAbove + Use simple and common words for better results.\n5.08\n80.80 19.53 15.97 29.40 62.40\n(5): (4)+Conciseness Above + Be concise; focus on user\u2019s first question.\n4.81\n80.00 19.70 16.63 35.30 62.70\nTable 2: Case study 1 (query generation): refinement quality via designed metrics when using different criteria to\nprompt GPT-3.5 for query refinement. Metrics measured: NCR: non-copy rate, Spec.: specificity, Read.: readability,\nCon.: conciseness, Cov.: coverage. Sat.: satisfaction. The full criteria texts can be found in the Appendix A.2.\nsearch query and u is the user question.\nNon-copy Rate =\n1\nBLEU-4(s, u)\n(1)\nSpecificity measures whether the search query suf-\nficiently captures the necessary information to re-\ntrieve relevant documents. We use GPT-3.5 as the\nevaluator (Fu et al., 2023). Details are in the Ap-\npendix A.3.\nReadability measures a search query\u2019s clarity\nbased on the word frequency rank (WFR)6 of its\nterms, as defined in Equation 2, where w is a word\nin s and C is a scaling constant. Ideally, a query\nshould use common words to improve readability.\nReadability =\nC\nAVGw\u2208s(WFR(w))\n(2)\nConciseness measures the query\u2019s brevity by its\nword count, with its value being the query length\u2019s\nreciprocal, scaled by a constant 100.\nCoverage measures how specific vs. general a\nsearch query is by counting the number of Google\nsearch result pages. Considering the wide variation\nin page count, we employ a relative metric. For\nrefined queries obtained using Table 2 with the\nsame dialog context, the query with the most results\ngets a \u201cCoverage\u201d score of 1, and others receive 0.\nSatisfaction measures whether the search query\nwill satisfy the user. It is an overall metric, and we\n6We use the Kaggle dataset for WFR: https://www.\nkaggle.com/rtatman/english-word-frequency\nuse our trained satisfaction classifier to determine\nthe percentage of satisfied refinements.\n5.2\nConstruct refinement training data\nWe sample 1,000 satisfied queries from the FITS\ntraining set along with their contexts to add to our\nsupervised training data D. Then, based on Fig-\nure 1-(B), for each unsatisfied query r, we (1) use\nGPT-3.5 and criteria c derived from \u00a75.1 to get a re-\nfinement r\u2032. (2) Use a quality checker to check r\u2032\u2019s\nsatisfaction. (3) Add (q, r\u2032) to D if r\u2032 is satisfactory.\nWe elaborate on step (1) in the next section.\n5.2.1\nRefinement Generation\nWe use GPT-3.5 with criteria-based prompts to re-\nfine 1,000 randomly sampled unsatisfied queries\n(details in Appendix A.2). To demonstrate the ef-\nfectiveness of Figure 1-(A), we conduct ablation\nstudies with different criteria for query refinement.\nGiven our computational budget, for metrics rely-\ning on GPT-3.5, we sample 500 dialog contexts\nand compare the queries resulting from different\ncriteria.\nThe results are in Table 2. Adding criteria in\nthe prompt will shift GPT-3.5\u2019s generation, and the\nperformance differences are interpretable using our\ndesigned metrics. Specifically, (i) The rephrase\ncriterion increases the non-copy rate. (ii) The rele-\nvance criterion increases the relevance metric. (iii)\nThe readability criterion increases the readability\nand coverage metrics. (iv) Using all the criteria, the\nrefinements achieve reasonably good performance\nValid\nTest\nTest Unseen\nNCR Spec. Read. Con. Cov. Sat. NCR Spec. Read. Con. Cov. Sat. NCR Spec. Read. Con. Cov. Sat.\nBB2(QG)\n32.8 40.5\n22.4 32.3 50.6 4.8\n18.8 34.9\n14.0 34.3 50.9 8.8\n22.7 37.7\n15.4 32.9 50.3 3.2\nSLT(QG(-))\n2.6\n60.4\n19.8 21.0 30.1 9.2\n2.8\n58.0\n17.4 22.9 30.5 12.9\n3.0\n55.4\n18.3 22.9 31.7 7.4\nSLT(QG(-+,))\n4.8\n73.5\n22.0 18.3 19.3 29.6\n3.8\n74.5\n21.7 18.0 18.6 29.0\n3.6\n73.5\n19.4 17.8 18.0 17.2\nTable 3: Evaluate query generators on FITS using designed metrics. See Table 2 caption for abbreviation meanings.\nValid\nTest\nTest Unseen\nF1\nPPL\nF1\nPPL\nF1\nPPL\nBB2(QG)\n9.74\n16.09 14.28 9.61 16.09 10.15\nSLT(QG(-))\n48.63 12.83 50.51 7.64 51.75\n7.84\nSLT(QG(-+,)) 51.19 10.34 52.99 7.23 52.21\n7.73\nTable 4: Evaluate query generators on FITS using F1\nand perplexity (PPL).\nin all our designed perspectives and overall satisfac-\ntion. Thus, when collecting training data, we use\nthe four criteria augmented prompt for refinement.\n5.3\nFine-tuning the Model\nWe start from the 400M BB2 query generator and\nconsider two fine-tuning settings: (1) using the\nsatisfied data; and (2) using satisfied and refinement\ndata. During training, we use the Adam optimizer\n(Kingma and Ba, 2015) with a batch size of 8 and\nlearning rate of 7 \u00d7 10\u22126 for three epochs. The\nbest checkpoint is chosen based on validation loss.\n5.4\nEvaluation using designed metrics\nWe evaluate the following query generators.\n\u2022 BB2(QG) The original BB2 query generator.\n\u2022 SLT(QG(-)) System-level trained query gener-\nator using only satisfied data.\n\u2022 SLT(QG(-+,)) System-level trained query\ngenerator using satisfied and refinement data.\nResults on Standard Metrics Table 4 presents\nthe results using standard metrics, as per Shi et al.\n(2022). Compared to the original BB2 query gener-\nator, training with domain-specific data (2nd row)\nsignificantly improves F1 word overlap and per-\nplexity metrics. Adding refinement data (3rd row)\nfurther enhances these metrics.\nResults on Our Designed Metrics We also report\nresults on our designed metrics for different query\ngenerators in Table 3. It is clear that training on sat-\nisfied data produces more specific and satisfactory\nqueries, with further improvements when incorpo-\nrating refinement data. The original BB2 query\ngenerator often generates overly concise queries,\nhindering the retrieval of the most relevant docu-\nments. In other words, although it generates queries\nthat perform well in terms of readability or cover-\nage, it is still an inadequate query generator, as\nevidenced by the poor satisfaction of the queries\nit generates. Later, when we refer to \u201cour trained\nquery generator\u201d, we mean the one trained using\nboth satisfied data and refinement data.\n6\nCase Study 2: Response Generation\n6.1\nDerive criteria from feedback\nFollowing the approach in \u00a75.1, we group all feed-\nback related to response generation into ten clusters.\nThen, we summarize the following eight groups\n(see Table 5) of feedback types by merging some\nclusters. From Table 5, we derive that an improved\nresponse as indicated by users should (i) ground its\nanswer on relevant search results, (ii) be concise\nand targeted, (iii) be confident in its answer. The\ncriteria text for crafting the prompts c for the text\nrefiner P\u03d5(r\u2032|r, q, c) is given in Table 6.\n6.1.1\nCriteria-guided Metric Design\nAfter deriving criteria for response generation from\nfeedback, we design the following metrics to mea-\nsure the quality of a response as indicated by users.7\nGroundedness measures how much the response\nutilizes the search results by examining n-gram\nmatching. We define it in Equation 3 based on\nROUGE-2 (Lin, 2004). Here, r is the response, d\nis a document from the relevant search set S.\nGroundedness = max\nd\u2208S ROUGE-2(r, d)\n(3)\nFactuality checks whether the information in the\nresponse is backed by search documents. We use\n7When evaluating a set of responses using one of the fol-\nlowing metrics, we take the average of all responses\u2019 scores\non that metric.\nGroup\nFeedback type\nNum.\n%\n1\nClarify his/her demand again.\n3702\n26.54%\n2\nComplain that the bot (1) does not answer the question or (2) gives irrelevant information\nor (3) asks the user to find out the answer on his or her own.\n2260\n16.20%\n3\nPoint out specific search results that can answer the question.\n2255\n16.17%\n4\nSuggest that the bot should use the search results.\n2130\n15.27%\n5\nStates that the answer is (1) factually incorrect, or (2) not grounded on the search results.\n1572\n11.27%\n6\nPoint out that the bot\u2019s answer is not specific/accurate/complete/detailed.\n1309\n9.39 %\n7\nPoint out that the bot is not confident in its answers and always begins its responses with\n\u201cI am not sure\u201d or \u201cI don\u2019t know\u201d.\n582\n4.17%\n8\nComplain about repetition/rudeness in bot responses.\n137\n0.99%\nTable 5: Case study 2 (response generation): 8 groups of system-level feedback derived from automatic clustering.\nType\nCriteria (Abbreviated)\nGRD Fact. Help. Rel. Conf. Sat.\n(1): Baseline\nUse a conversational tone; no more than 20 words.\n34.68 86.60 81.40 89.40 99.60 74.10\n(2): (1)+Groundedness Above + Use search results to give answers.\n36.81 86.60 85.00 89.00 99.90 75.80\n(3): (2)+Relevance\nAbove + Be concise and targeted, no irrelevant information. 36.77 88.80 85.60 89.40 99.90 74.90\n(4): (3)+Confidence\nAbove + Don\u2019t start with \u201cI\u2019m not sure\u201d or \u201cI don\u2019t know\u201d. 39.02 87.20 86.60 90.60 99.90 77.00\nTable 6: Case study 2 (response generation): refinement quality via designed metrics when using different criteria\nto prompt GPT-3.5 for response refinement. Metrics measured: GRD: groundedness, Fact.: factuality, Help.:\nhelpfulness, Rel.: relevance, Conf.: confidence. Sat.: satisfaction. The full criteria texts can be found in the\nAppendix A.2.\nGPT-3.5 with chain-of-thought to measure factual-\nity (Luo et al., 2023). See Appendix A.3 for details.\nHelpfulness measures whether the response di-\nrectly answers the user\u2019s question. We use GPT-3.5\nto measure helpfulness. See Appendix A.3 for de-\ntails.\nRelevance measures whether the response remains\non topic and offers pertinent information.\nWe\nagain use GPT-3.5, with further details in the Ap-\npendix A.3.\nConfidence measures whether the response is in a\ncertain and confident tone. We use simple heuris-\ntics to gauge confidence, counting the occurrences\nof \u201cI\u2019m not sure\u201d and \u201cI don\u2019t know.\u201d If either\nphrase appears, we consider the response unconfi-\ndent; otherwise, it\u2019s considered confident.\nSatisfaction measures whether the response satis-\nfies the user, similar to \u201csatisfaction\u201d in \u00a75.1.1.\n6.2\nConstruct refinement training data\nAs in \u00a75.2, we first randomly sample 1,000 satisfied\nresponses together with their contexts to add to our\ntraining data D. Then, we go through the following\nthree steps: (1) refinement generation, (2) quality\ncheck and (3) collection of filtered data. We will\ndescribe (1) in detail in the following section.\n6.2.1\nRefinement Generation\nWe use GPT-3.5 with criteria-based prompts to re-\nfine 1,000 sampled unsatisfied responses (details\nin Appendix A.2). As in \u00a75.2.1, we conduct ab-\nlation studies to demonstrate the effectiveness of\nderived criteria. The results in Table 6 highlight:\n(i) Adding the groundedness criterion improves the\ngroundedness metric. (ii) Adding the relevance\ncriterion increases helpfulness and relevance. (iii)\nGPT-3.5 refinements are confident and rarely in-\nclude phrases like \u201cI\u2019m not sure\u201d or \u201cI don\u2019t know\u201d.\n(iv) In terms of satisfaction, the best performance\nis achieved by the prompt with all criteria added.\nTherefore, when collecting training data, we use\nthe three criteria-augmented prompt for response\nrefinement.\n6.3\nFine-tuning the Model\nWe use the 400M BB2 main model as the baseline\nresponse generator and consider two fine-tuning\nsettings: (1) using only satisfied data; and (2) using\nboth satisfied and refinement data, following \u00a75.3.\nValid\nTest\nTest Unseen\nGRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat.\nBB2(QG+RG)\n34.1 50.0 19.0 68.2 66.8 27.1 32.4 58.3 22.0 67.8 73.7 34.9 32.9 58.4 21.8 69.0 65.7 32.1\nSLT(QG)+BB2(RG)\n39.0 66.4 26.8 74.2 80.6 33.3 35.2 58.4 29.8 71.4 83.4 40.9 37.5 59.1 30.2 73.8 77.5 37.8\nSLT(QG+RG(-))\n30.6 59.1 29.2 75.6 76.4 35.3 27.8 53.7 31.5 69.6 80.6 41.7 29.7 60.5 31.3 73.4 72.6 39.3\nSLT(QG+RG(-+,)) 48.2 69.1 41.3 81.6 81.1 50.7 43.2 66.7 44.5 76.4 83.6 55.7 45.3 71.6 43.9 79.6 76.3 51.4\nTable 7: Evaluate dialog systems on FITS using designed metrics. See Table 6 caption for abbreviation meanings.\nValid\nTest\nTest Unseen\nF1\nPPL\nF1\nPPL\nF1\nPPL\nBB2(QG+RG)\n25.78 9.40 28.30 7.41 22.99 7.75\nSLT(QG)+BB2(RG)\n26.69 8.24 28.66 6.66 24.88 7.03\nSLT(QG+RG(-))\n28.20 7.41 29.73 6.04 25.54 6.43\nSLT(QG+RG(-+,)) 25.57 7.62 26.90 6.15 24.34 6.58\nTable 8: Evaluate dialog systems on FITS via F1 & PPL.\n6.4\nEvaluation using designed metrics\nWe evaluate the following systems:\n\u2022 BB2(QG+RG) Original BB2 response generator\npaired with the original BB2 query generator.\n\u2022 SLT(QG)+BB2(RG) Original BB2 response\nmodel paired with our system level trained query\ngenerator.\n\u2022 SLT(QG+RG(-)) Our system-level trained re-\nsponse generator using satisfied data only, paired\nwith our system level trained query generator.\n\u2022 SLT(QG+RG(-+,)) Our system-level trained\nresponse generator using satisfied and refinement\ndata, paired with our system level trained query\ngenerator.\nResults on Standard Metrics Standard metrics\nare shown in Table 8. Key takeaways include:\n(i) When using the BB2 response generator, our\ntrained query generator improves the final response\nquality compared to the BB2 query generator. (ii)\nTraining the response generator on satisfied data\nleads to further improvements when using our best\nquery generator. (iii) However, training with addi-\ntional refinement data does not surpass using satis-\nfied data alone. The reason behind (iii) relates to\nFITS\u2019s gold response collection. Often, the gold re-\nsponse is a user-guided, BB2-generated reply. This\nbiases reference-based metrics towards the origi-\nnal BB2 outputs. Moreover, low-quality references\nmay underestimate model performance when us-\ning reference-based metrics (Zhang et al., 2023a)\nand we confirmed this with a human evaluation of\nresponse quality (see Appendix A.4 for details).\nResults on Our Designed Metrics Table 7 shows\nthe results when using our designed metrics. No-\ntably, (i) when using the BB2 response genera-\ntor, our trained query generator improves the final\nresponse quality from all perspectives compared\nto the BB2 query generator. (ii) When equipped\nwith our trained query generator, training the re-\nsponse generator on satisfied data leads to consis-\ntent improvements in helpfulness compared to the\nBB2 response generator, indicating the importance\nof domain-adapted training. (iii) Training the re-\nsponse generator on both satisfied and refinement\ndata improves the final response quality from all\nperspectives compared to training on satisfied data\nonly, highlighting refinement data\u2019s utility in rec-\ntifying model errors. (iv) In terms of satisfaction,\nthe best-performing system employs our query and\nresponse generators, both trained on satisfied and\nrefinement data. Additionally, as a further base-\nline, we gathered the first 200 unsatisfied responses\ninto a sparse refinement training set, refined via\ninstance-level feedback. A model trained on this set\nalongside satisfied data, fell short compared to our\nsystem-level trained response generator, as mea-\nsured by our designed metrics, see Appendix A.5\nfor details.\n7\nCombining System-level Feedback and\nInstance-level Feedback\nPrevious studies (Scheurer et al., 2022b; Shi et al.,\n2022; Chen et al., 2023a) have shown the effective-\nness of instance-level feedback in the refinement\nprocess. To take a step further, we explore the syn-\nergy of system-level and instance-level feedback\non dialogue systems. Using response generation as\na case study, we collect both human and GPT-3.5\nfeedback (prompt in Appendix A.6) for the 1,000\nunsatisfied responses from \u00a76.2.1. We then design\na refinement prompt integrating both system-level\nValid\nTest\nTest Unseen\nGRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat.\nSLT(QG+RG(-+,))\n48.2 69.1 41.3 81.6 81.1 50.7 43.2 66.7 44.5 76.4 83.6 55.7 45.3 71.6 43.9 79.6 76.3 51.4\nSLT(QG+RG(-+HFB,))\n48.8 68.1 43.3 81.4 91.9 57.3 43.8 68.5 47.8 79.4 93.5 61.2 45.0 72.2 45.4 81.2 88.0 57.5\nSLT(QG+RG(-+GPT3.5FB,)) 44.0 66.3 39.4 78.6 80.2 49.4 38.9 66.7 45.6 78.6 81.7 54.7 40.9 69.9 45.2 80.6 75.3 53.1\nTable 9: Case study for combining system-level and instance-level feedback: performance of different dialog\nsystems on FITS datasets, evaluated using our designed metrics. See Table 6 for the meaning of the abbreviations.\nand instance-level feedback, i.e. both the desired\ncriteria and the specific example-based feedback\n(see Appendix A.2). We introduce three systems\nfor comparison.\n\u2022 SLT(QG+RG(-+,)) Our system-level trained\nresponse generator using satisfied and refinement\ndata, paired with our trained query generator.\nThe system does not use instance-level feedback.\n\u2022 SLT(QG+RG(-+HFB,)) Our system-level\ntrained response generator paired with trained\nquery generator.\nThe response generator is\ntrained on satisfied and refinement data (where\nwe incorporate human-written instance-level\nfeedback (HFB) into the response refinement\nprompt).\n\u2022 SLT(QG+RG(-+GPT3.5FB,)) Our system-\nlevel trained response and query generators,\nwhere the response generator is trained on satis-\nfied and refinement data. We incorporate GPT-\n3.5, rather than human, generated instance-level\nfeedback (GPT3.5FB) into the response refine-\nment prompt.\n7.1\nResults of Adding Instance-level Feedback\nResults using our designed metrics are in Table 9.\nWe observe that adding human-written feedback to\nthe response refinement part brings improvements\nin the five criteria-based metrics most of the time,\nand increases the overall satisfaction consistently.\nHowever, adding GPT-3.5 feedback results in de-\ngraded performance in groundedness, factuality\nand confidence. Those observations raise two ques-\ntions: (1) How does GPT-3.5 feedback differ from\nhuman feedback? (2) How does human/GPT-3.5\nfeedback impact response refinement? We address\nthese questions in subsequent sections.\n7.2\nHuman vs. GPT-3.5 Feedback Metrics\nTo understand why adding human feedback is more\nbeneficial than GPT-3.5 feedback, we analyze their\ndifferences through the following perspectives. (1)\n0\n50\n100\nSuccess Rate Verbosity\nDiversity\nGrammar\n0\n50\n100\nSuccess Rate Verbosity\nDiversity\nGrammar\nHuman Feedback\nGPT-3.5 Feedback\nFigure 2: Comparison of human and GPT-3.5 feedback.\nRefinement\nGRD\nFact.\nHelp.\nRel.\nConf.\nSat.\nNo feedback 39.16 90.35 83.48 98.10 100.00 76.50\nHuman FB\n40.11 87.50 81.10 97.80\n99.84\n74.60\nGPT-3.5 FB\n32.77 81.50 90.20 98.40\n99.84\n79.50\nTable 10: Quality of refinements with no/human/GPT-\n3.5 feedback. See Table 6 for abbreviation meanings.\nRefinement Success Rate: Percentage of satisfac-\ntory feedback-driven refinements. (2) Verbosity:\nAverage word count of feedback. (3) Diversity:\nPercentage of unique words. (4) Grammar: Per-\ncentage of grammatical feedback sentences.8\nIn Figure 2, we show characteristics of human\nand GPT-3.5 feedback. Though GPT-3.5 feedback\nis lengthier and grammatically sound, it lacks the\nlanguage diversity of human feedback. Upon man-\nual examination, GPT-3.5 feedback is often general,\nwhereas human feedback is direct and specific. See\nthe Appendix A.7 for feedback examples.\n7.3\nFeedback Impact on Refinements\nWhile GPT-3.5 feedback leads to a higher refine-\nment success rate (see Figure 2), the performance\nof the resulting dialog system trained with these\nrefinements falls short w.r.t. all our designed met-\nrics compared to the system trained using human\nfeedback-driven refinements as shown in Table 9.\nTherefore, to understand this further we also eval-\nuate the refinement quality via designed metrics\nfrom \u00a76.1.1, with results in Table 10. Refinements\nobtained using human feedback mainly stand out\n8We use Gramformer for grammar error checking: https:\n//github.com/PrithivirajDamodaran/Gramformer.\nin groundedness and factuality. This aligns with\nthe feedback clusters in Table 5 where over 40%\nof the feedback suggests the bot focus more on the\nsearch results; that is, focusing more on the search\nresults will make the refinements more grounded,\nleading to a more grounded final system (see Ta-\nble 9). Since language models are known to hal-\nlucinate regardless of their size (Ji et al., 2023; Li\net al., 2023), grounding their generations to the\ndocuments is important to ensure factuality. Hence,\ngroundedness of refinements plays an essential role\nin the performance of trained models.\n7.4\nAdvantages of Human Feedback\nWe find that human feedback pinpoints issues more\neffectively than GPT-3.5 feedback. For example,\nwhen a response does not answer a question, GPT-\n3.5 will say that the response is unhelpful because it\ndoes not contain the information the user wants. In\ncontrast, human feedback often provides specific\nhints from the search results, guiding the model\ntowards a better response. Thus, despite GPT-3.5\nproducing seemingly informative feedback, it cur-\nrently can\u2019t match the nuance of human annotators.\n8\nConclusion\nIn this paper, we present a framework that har-\nnesses system-level NL feedback.\nBy using a\nset of instance-level feedback, we derive system-\nlevel feedback for refinement prompt engineering\nand metric design. We show the effectiveness of\nsystem-level feedback through two case studies:\ngenerating queries and formulating dialogue re-\nsponses. We further combine system-level and\ninstance-level feedback in the refinement data con-\nstruction process, and observe that the resulting\ntrained response generator makes considerable im-\nprovements versus either alone. Finally, we explore\nthe possibility of substituting instance-level human\nfeedback with GPT-3.5 feedback. We find that\nhuman feedback stands out in capturing main is-\nsues, while GPT-3.5 feedback is lengthy and less\nfocused.\n9\nLimitations\nDue to the lack of publicly available natural lan-\nguage feedback datasets, our experiments were lim-\nited to the small-scale dialog system BB2, which\ndoes not represent the current state-of-the-art. We\nrecognize that integrating more advanced models\nsuch as ChatGPT could yield further insights, pre-\nsenting a promising direction for future research.\nAs relevant datasets become more accessible, we\nlook forward to exploring these possibilities.\n10\nAcknowledgement\nThe work was done as part of the Meta\u2013NYU\nmentorship program and partly supported by the\nNational Science Foundation (under NSF Award\n1922658). Kyunghyun Cho is supported by the\nSamsung Advanced Institute of Technology (under\nthe project Next Generation Deep Learning: From\nPattern Recognition to AI).\nReferences\nLeonard Adolphs, Tianyu Gao, Jing Xu, Kurt Shuster,\nSainbayar Sukhbaatar, and Jason Weston. 2022. The\ncringe loss: Learning what language not to model.\narXiv preprint arXiv:2211.05826.\nKushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and\nJason Weston. 2022. Director: Generator-classifiers\nfor supervised language modeling.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson El-\nhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam Mc-\nCandlish, Chris Olah, and Jared Kaplan. 2021. A\ngeneral language assistant as a laboratory for align-\nment.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022. Training a help-\nful and harmless assistant with reinforcement learn-\ning from human feedback.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nO\u02d8guz \u2019Oz\u2019 Buruk. 2023. Academic writing with gpt-3.5:\nReflections on practices, efficacy and transparency.\nAngelica Chen, J\u00e9r\u00e9my Scheurer, Tomasz Korbak,\nJon Ander Campos, Jun Shern Chan, Samuel R. Bow-\nman, Kyunghyun Cho, and Ethan Perez. 2023a. Im-\nproving code generation by training with natural lan-\nguage feedback.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and\nDenny Zhou. 2023b. Teaching large language mod-\nels to self-debug.\nI-Chun Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan,\nKehua Feng, Chunting Zhou, Junxian He, Graham\nNeubig, Pengfei Liu, et al. 2023. Factool: Factu-\nality detection in generative ai\u2013a tool augmented\nframework for multi-task and multi-domain scenar-\nios. arXiv preprint arXiv:2307.13528.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robinson,\nDasha Valter, Sharan Narang, Gaurav Mishra, Adams\nYu, Vincent Zhao, Yanping Huang, Andrew Dai,\nHongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Ja-\ncob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le,\nand Jason Wei. 2022. Scaling instruction-finetuned\nlanguage models.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence\nembeddings. In Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nAmelia Glaese, Nat McAleese, Maja Tr\u02dbebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\nLucy Campbell-Gillingham, Jonathan Uesato, Po-\nSen Huang, Ramona Comanescu, Fan Yang, Abigail\nSee, Sumanth Dathathri, Rory Greig, Charlie Chen,\nDoug Fritz, Jaume Sanchez Elias, Richard Green,\nSo\u02c7na Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel\nFoley, Susannah Young, Iason Gabriel, William Isaac,\nJohn Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geoffrey Irving. 2022.\nImproving alignment of dialogue agents via targeted\nhuman judgements.\nAnna-Carolina Haensch, Sarah Ball, Markus Herklotz,\nand Frauke Kreuter. 2023. Seeing chatgpt through\nstudents\u2019 eyes: An analysis of tiktok data.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\nMazare, and Jason Weston. 2019. Learning from\ndialogue after deployment: Feed yourself, chatbot!\nIn Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 3667\u2013\n3684, Florence, Italy. Association for Computational\nLinguistics.\nJ. A. Hartigan and M. A. Wong. 1979. A k-means\nclustering algorithm.\nJSTOR: Applied Statistics,\n28(1):100\u2013108.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12):248:1\u2013248:38.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization.\nIn 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nMojtaba Komeili, Kurt Shuster, and Jason Weston. 2022.\nInternet-augmented dialogue generation. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8460\u20138478, Dublin, Ireland. Association\nfor Computational Linguistics.\nJiwei Li,\nAlexander H. Miller,\nSumit Chopra,\nMarc\u2019Aurelio Ranzato, and Jason Weston. 2017. Di-\nalogue learning with human-in-the-loop.\nJunyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun\nNie, and Ji-Rong Wen. 2023. Halueval: A large-\nscale hallucination evaluation benchmark for large\nlanguage models.\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean Welleck,\nY-Lan Boureau, Kyunghyun Cho, and Jason Weston.\n2019. Don\u2019t say that! making inconsistent dialogue\nunlikely with unlikelihood training. arXiv preprint\narXiv:1911.03860.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74\u201381, Barcelona, Spain.\nAssociation for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Comput. Surv., 55(9):195:1\u2013195:35.\nZheheng Luo, Qianqian Xie, and Sophia Ananiadou.\n2023. Chatgpt as a factual inconsistency evaluator\nfor text summarization.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022.\nTraining language models to follow in-\nstructions with human feedback.\narXiv preprint\narXiv:2203.02155.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311\u2013318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills,\nLong Ouyang, Jonathan Ward, and Jan Leike. 2022.\nSelf-critiquing models for assisting human evalua-\ntors.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan,\nAngelica Chen, Kyunghyun Cho, and Ethan Perez.\n2022a. Training language models with language feed-\nback.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Jun Shern Chan,\nAngelica Chen, Kyunghyun Cho, and Ethan Perez.\n2022b.\nTraining language models with language\nfeedback.\nJ\u00e9r\u00e9my Scheurer, Jon Ander Campos, Tomasz Korbak,\nJun Shern Chan, Angelica Chen, Kyunghyun Cho,\nand Ethan Perez. 2023. Training language models\nwith language feedback at scale.\nMrinank Sharma, Meg Tong, Tomasz Korbak, David\nDuvenaud, Amanda Askell, Samuel R. Bowman,\nNewton Cheng, Esin Durmus, Zac Hatfield-Dodds,\nScott R. Johnston, Shauna Kravec, Timothy Maxwell,\nSam McCandlish, Kamal Ndousse, Oliver Rausch,\nNicholas Schiefer, Da Yan, Miranda Zhang, and\nEthan Perez. 2023. Towards understanding syco-\nphancy in language models.\nWeiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston,\nand Jing Xu. 2022. When life gives you lemons,\nmake cherryade: Converting feedback from bad re-\nsponses into good labels.\nKurt Shuster, Mojtaba Komeili, Leonard Adolphs,\nStephen Roller, Arthur Szlam, and Jason Weston.\n2022. Language models that seek for knowledge:\nModular search & generation for dialogue and\nprompt completion. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n373\u2013393, Abu Dhabi, United Arab Emirates. Associ-\nation for Computational Linguistics.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 3008\u20133021. Curran Associates,\nInc.\nNiket Tandon, Aman Madaan, Peter Clark, and Yiming\nYang. 2022. Learning to repair: Repairing model out-\nput errors after deployment using a dynamic memory\nof feedback. In Findings of the Association for Com-\nputational Linguistics: NAACL 2022, pages 339\u2013352,\nSeattle, United States. Association for Computational\nLinguistics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nJason E Weston. 2016. Dialog-based language learn-\ning. In Advances in Neural Information Processing\nSystems, volume 29. Curran Associates, Inc.\nJing Xu, Arthur Szlam, and Jason Weston. 2022a. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 5180\u20135197, Dublin,\nIreland. Association for Computational Linguistics.\nJing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora,\nY-Lan Boureau, and Jason Weston. 2022b. Learning\nnew skills after deployment: Improving open-domain\ninternet-driven dialogue with human feedback.\nWeizhe Yuan, Ethan Chern, Steffi Chern, Chunting\nZhou, Chunpu Xu, Binjie Wang, and Pengfei Liu.\n2023. chateval.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023a. Benchmarking large language models for\nnews summarization.\nZheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, and\nToby Jia-Jun Li. 2023b. Visar: A human-ai argumen-\ntative writing assistant with visual programming and\nrapid draft prototyping.\nA\nAppendix\nA.1\nManual Efforts Required to Derive\nSystem-level Criteria\nIn our approach, the feedback grouping was a\nsemi-automated process. Initially, we employed\nk-means clustering, utilizing the SimCSE encoder\nto categorize the feedback sentences. This cluster-\ning process was conducted once without human\nintervention. Then we employed a streamlined,\nnon-iterative manual approach for cluster curation.\nSpecifically, two domain experts independently\nreviewed 50 samples (each requires around 30s\nto read) from each of the 15 clusters (including\nclusters for queries and responses), requiring ap-\nproximately 375 minutes per person for this phase.\nThis was followed by a collaborative discussion\nto merge insights and remove duplicate clusters,\namounting to an additional 60 minutes per expert.\nThus, the total human effort amounted to approxi-\nmately 14.5 person-hours.\nA.2\nRefinement with GPT-3.5\nWe instruct GPT-3.5 to generate query refinements\nand response refinements using carefully crafted\nprompts, as shown in Table 11 and Table 12. The\ncorresponding ablation studies with full criteria\ntext for query refinement and response refinement\nare shown in Table 13 and Table 14. The prompt\nfor using both system-level feedback and instance-\nlevel feedback for response refinement is shown in\nTable 15.\nA.3\nEvaluation with GPT-3.5\nSince previous studies have demonstrated GPT-3\u2019s\ncapability in the evaluation of aspects such as fac-\ntuality (Luo et al., 2023), helpfulness (Fu et al.,\n2023), relevance (Fu et al., 2023), etc. We use GPT-\n3.5 to evaluate the following perspectives with the\nhelp of ChatEval (Yuan et al., 2023).\nQuery Specificity\nWe use GPT-3.5 to measure\nspecificity (Fu et al., 2023) where we concatenate\nthe dialog context and search query together and\nask GPT-3.5 to judge whether the search query is\nspecific using the chain-of-thought technique (Wei\net al., 2022). In particular, we use the prompt as\nshown in Table 16. Before applying it to measure\nthe quality of query refinements. We manually\nlabeled 50 search queries from the FITS training\nsplit, and each one\u2019s specificity label was decided\nby three annotators through majority vote. We cal-\nculate the agreement between GPT-3.5 and human\nannotators, and the result is 80%.\nResponse Factuality\nWe concatenate the search\ndocuments and response, and ask GPT-3.5 to judge\nif all information in the response is supported by\nthe search documents. The prompt we use is shown\nin Table 17. We conducted a meta-evaluation where\nwe asked three NLP PhD students at the same uni-\nversity as the first author to manually label 50 re-\nsponses from the FITS training split. The anno-\ntation guideline we showed them is the same as\nthe prompt designed for GPT-3.5 evaluation. Then,\nthe three annotators decided on each one\u2019s factual-\nity label through a majority vote. The agreement\nbetween GPT-3.5 and human annotators is 88%.\nResponse Helpfulness\nThe prompt we use is\nshown in Table 18.\nWe conducted a meta-\nevaluation where we manually labeled 30 responses\nfrom the FITS training split and 20 responses from\nthe Red Team dataset (Bai et al., 2022). Three anno-\ntators decided on each response\u2019s helpfulness label\nthrough majority vote. The agreement between\nGPT-3.5 and human annotators is 84%.\nResponse Relevance\nThe prompt we use is\nshown in Table 19.\nWe conducted a meta-\nevaluation where we manually labeled 30 responses\nfrom the FITS training split and 20 responses from\nthe Red Team dataset (Bai et al., 2022). Three\nannotators decided on each response\u2019s relevance la-\nbel through majority vote. The agreement between\nGPT-3.5 and human annotators is 84%.\nA.4\nHuman Evaluation on Response Outputs\nIn \u00a76.4, our analysis revealed that training the re-\nsponse generator with both satisfied data and re-\nfinement data does not yield superior performance\nover using satisfied data alone, as evidenced by the\nF1 score and Perplexity (PPL) metrics. We hypoth-\nesized that this outcome might be attributed to a\ndata bias in the FITS dataset, wherein the gold stan-\ndard references are frequently produced by the BB2\nmodel. Consequently, standard reference-based\nmetrics, such as F1, tend to favor responses that\nclosely resemble BB2 outputs. This bias poten-\ntially results in the underestimation of performance\nfor models generating responses deviating from the\nBB2 distribution.\nTo address this limitation, we expanded our\nevaluation methodology beyond model-based met-\nrics.\nWe conducted an additional human eval-\nuation to compare 100 responses generated\nby SLT(QG+RG(-)) and SLT(QG+RG(-+,))\nagainst the same queries. In this evaluation, two\nhuman annotators were asked to select their pre-\nferred response from the two provided, with the\noptions including a \u201ctie\u201d. If both annotators agreed\nthat one response was superior, the correspond-\ning model was awarded a \u201cwin\u201d.\nIn cases of\ndisagreement or agreement on ties, the outcome\nwas recorded as a tie. The results of this human\nPrompt for query refinement with GPT-3.5\nGiven the dialog history, your task is to refine the original search query used to search the Internet so that the modified search\nquery will search for documents that better match the user\u2019s needs. You should follow the following requirements:\n[Criteria]\nBelow is the dialog context.\n[Dialog Context]\nBelow is the bot\u2019s unsatisfactory query.\n[Original Query]\nYou should modify the original search query into the following:\nTable 11: Case study 1: prompt for query refinement with GPT-3.5. [Criteria], [Dialog Context] and\n[Original Query] are placeholders to be filled. The underlined sentence is removed when [Criteria] is None.\nPrompt for response refinement with GPT-3.5\nGiven the dialog history and the unsatisfactory last response the bot gave, your task is to modify the response appropriately to\nkeep the conversation fluent and consistent. You should follow the following requirements:\n[Criteria]\nBelow is the dialog context.\n[Dialog Context]\nBelow is the bot\u2019s unsatisfactory response.\n[Original Response]\nBelow are some useful search results that you could use.\n[Search Documents]\nYou should modify the original response into the following:\nTable 12: Case study 2: prompt for response refinement with GPT-3.5. [Criteria], [Dialog Context],\n[Original Response] and [Search Documents] are placeholders to be filled.\n0\n20\n40\n60\n80\n100\n35%\n44%\n21%\nSLT(QG+RG( + )) wins\nSLT(QG+RG( )) wins\nTie\nFigure 3: Win rates for system trained with both satisfied\nand refinement data and system trained with satisfied\ndata only.\nevaluation, presented in Figure 3, indicate that\nSLT(QG+RG(-+,)) achieved a higher win rate\ncompared to SLT(QG+RG(-)). This finding con-\nfirms our hypothesis that reference-based metrics\nalone are insufficient for evaluating this task, high-\nlighting the need for more robust metrics in system\nassessment.\nA.5\nInstance-level Feedback vs. System-level\nFeedback\nWe argue that one of the drawbacks of instance-\nlevel approaches that utilize NL feedback is that\nthey typically assume that every instance receives\na feedback text, which is not practical in the real\nworld where feedback tends to be sparse. There-\nfore, we also conducted a comparison experi-\nment that assumes sparse instance-level feedback.\nSpecifically, we collected the first 200 unsatisfied\nresponses into a sparse refinement training set, re-\nfined via instance-level feedback only. We then\ntrain the response generator on this set alongside\nthe satisfied data and compare its performance to\nour system-level trained model. Table 20 shows\nthe performance of the two models as measured\nby our designed metrics. The system-level trained\nresponse generator outperforms the sparse instance-\nlevel trained response generator by a large margin\non all metrics, demonstrating the importance of\nsystem-level feedback in a sparse instance-level\nfeedback setting.\nA.6\nFeedback Generation with GPT-3.5\nPrevious studies have demonstrated the capability\nof large language models to generate informative\nand useful feedback (Madaan et al., 2023; Chen\net al., 2023b). Therefore, we also investigate using\nGPT-3.5 to generate instance-level feedback for\neach unsatisfied response. The prompt we use for\nfeedback generation is in Table 21.\nA.7\nExamples of GPT-3.5 and Human\nInstance-level Feedback\nWe list examples of instance-level feedback written\nby humans and GPT-3.5 in Table 22.\nType\nCriteria\nNon-copy\nSpecificity\nReadability\nConciseness\nCoverage\nSatisfaction\nBaseline\nNone\n4.06\n79.40\n19.46\n14.87\n29.80\n61.50\nBaseline\n+Rephrase\n(1) To better adapt to search engines, it is best not to copy the user\u2019s original\nwords directly. You can rephrase the user\u2019s question, use some keywords for the\nsearch, and if the user mentions some abbreviations, restore them to their full\nnames.\n4.98\n83.20\n19.54\n15.04\n26.50\n62.10\nBaseline\n+Rephrase\n+Specificity\n(1) To better adapt to search engines, it is best not to copy the user\u2019s original\nwords directly. You can rephrase the user\u2019s question, use some keywords for the\nsearch, and if the user mentions some abbreviations, restore them to their full\nnames. (2) Be accurate and specific enough to reflect the user\u2019s needs.\n5.00\n84.20\n18.77\n14.50\n28.80\n63.30\nBaseline\n+Rephrase\n+Specificity\n+Readability\n(1) To better adapt to search engines, it is best not to copy the user\u2019s original\nwords directly. You can rephrase the user\u2019s question, use some keywords for the\nsearch, and if the user mentions some abbreviations, restore them to their full\nnames. (2) Be accurate and specific enough to reflect the user\u2019s needs. (3) To\nbe able to search for more results, you should use more simple and commonly\nused words.\n5.08\n80.80\n19.53\n15.97\n29.40\n62.40\nBaseline\n+Rephrase\n+Specificity\n+Readability\n+Conciseness\n(1) To better adapt to search engines, it is best not to copy the user\u2019s original\nwords directly. You can rephrase the user\u2019s question, use some keywords for the\nsearch, and if the user mentions some abbreviations, restore them to their full\nnames. (2) Be accurate and specific enough to reflect the user\u2019s needs. (3) To\nbe able to search for more results, you should use more simple and commonly\nused words. (4) Your search query should be concise. If the user asks multiple\nquestions, you should focus on his/her first question.\n4.81\n80.00\n19.70\n16.63\n35.30\n62.70\nTable 13: Case study 1 (query generation): refinement quality via designed metrics when using different criteria to\nprompt GPT-3.5 for query refinement.\nType\nCriteria\nGroundedness\nFactuality\nHelpfulness\nRelevance\nConfidence\nSatisfaction\nBaseline\n(1) The modified response should be conversational in tone and no more than\ntwenty words.\n34.68\n86.60\n81.40\n89.40\n99.60\n74.10\nBaseline\n+Groundedness\n(1) The modified response should be conversational in tone and no more than\ntwenty words. (2) If the user asks a question, you should use relevant search\nresults to answer the user\u2019s question correctly. Please do not let the user check\nout some resources on his or her own.\n36.81\n86.60\n85.00\n89.00\n99.90\n75.80\nBaseline\n+Groundedness\n+Relevance\n(1) The modified response should be conversational in tone and no more than\ntwenty words. (2) If the user asks a question, you should use relevant search\nresults to answer the user\u2019s question correctly. Please do not let the user check\nout some resources on his or her own. (3) Your modified response should be\nas concise and targeted as possible, and not include additional information the\nuser has not asked for.\n36.77\n88.80\n85.60\n89.40\n99.90\n74.90\nBaseline\n+Groundedness\n+Relevance\n+Confidence\n(1) The modified response should be conversational in tone and no more than\ntwenty words. (2) If the user asks a question, you should use relevant search\nresults to answer the user\u2019s question correctly. Please do not let the user check\nout some resources on his or her own. (3) Your modified response should be\nas concise and targeted as possible, and not include additional information the\nuser has not asked for. (4) Please be confident in your response, and don\u2019t start\nyour response with \u201cI\u2019m not sure\u201d or \u201cI don\u2019t know\u201d.\n39.02\n87.20\n86.60\n90.60\n99.90\n77.00\nTable 14: Case study 2 (response generation): refinement quality via designed metrics when using different criteria\nto prompt GPT-3.5 for response refinement.\nPrompt for response refinement with GPT-3.5 (with instance-level feedback)\nGiven the dialog history and the unsatisfactory last response the bot gave, your task is to modify the response appropriately to\nkeep the conversation fluent and consistent. You should follow the following requirements:\n[Criteria]\nBelow is the dialog context.\n[Dialog Context]\nBelow is the bot\u2019s unsatisfactory response.\n[Original Response]\nBelow is the feedback for the bot\u2019s unsatisfactory response.\n[Feedback]\nBelow are some useful search results that you could use.\n[Search Documents]\nYou should modify the original response into the following:\nTable 15: Prompt for response refinement with GPT-3.5 (with instance-level feedback). [Criteria], [Dialog\nContext], [Original Response], [Feedback] and [Search Documents] are placeholders to be filled.\nPrompt for query specificity evaluation with GPT-3.5\nYou are evaluating a search query for a dialog using a specific set of standards. Below is the dialog context.\n[Dialog Context]\nBelow is the search query.\n[Query]\nBelow are the criteria.\nDecide whether the search query is accurate and specific enough to enable retrieval of the most relevant documents on the\nInternet that are sufficient to answer the user\u2019s question.\nDoes the search query meet the criterion? First, write out in a step-by-step manner your reasoning about the criterion to be sure\nthat your conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or\n\"N\" (without quotes or punctuation) on its own line corresponding to the correct answer. At the end, repeat just the letter again\nby itself on a new line.\nReasoning:\nTable 16: Prompt used to let GPT-3.5 evaluate query specificity. [Dialog Context] and [Query] are placeholders\nto be filled.\nPrompt for response factuality evaluation with GPT-3.5\nYou are evaluating a response for a dialog using a specific set of standards. Below is the dialog context.\n[Dialog Context]\nBelow are some search documents that may help continue this dialog.\n[Search Documents]\nBelow is the response.\n[Response]\nBelow is the criteria.\nDetermine if the information in the response can be found in one or more search documents.\nDoes the response meet the criterion? First, write out in a step-by-step manner your reasoning about the criterion to be sure that\nyour conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\"\n(without quotes or punctuation) on its own line corresponding to the correct answer. In the end, repeat just the letter again by\nitself on a new line.\nReasoning:\nTable 17: Prompt used to let GPT-3.5 evaluate response factuality. [Dialog Context], [Search Documents] and\n[Reponse] are placeholders to be filled.\nPrompt for response helpfulness evaluation with GPT-3.5\nYou are evaluating a response for a dialog using a specific set of standards. Below is the dialog context.\n[Dialog Context]\nBelow is the response.\n[Response]\nBelow are the criteria.\nDoes the answer directly solve the question?\nDoes the response meet the criterion? First, write out in a step-by-step manner your reasoning about the criterion to be sure that\nyour conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\"\n(without quotes or punctuation) on its own line corresponding to the correct answer. In the end, repeat just the letter again by\nitself on a new line.\nReasoning:\nTable 18: Prompt used to let GPT-3.5 evaluate response helpfulness. [Dialog Context] and [Reponse] are\nplaceholders to be filled.\nPrompt for response relevance evaluation with GPT-3.5\nYou are evaluating a response for a dialog using a specific set of standards. Below is the dialog context.\n[Dialog Context]\nBelow is the response.\n[Response]\nBelow are the criteria.\nIs the response relevant to the topic at hand? It\u2019s essential to recognize that the response does not need to be highly specific to the\npreceding question. As long as it remains focused on the topic at hand, it is considered relevant.\nDoes the response meet the criterion? First, write out in a step-by-step manner your reasoning about the criterion to be sure that\nyour conclusion is correct. Avoid simply stating the correct answers at the outset. Then print only the single character \"Y\" or \"N\"\n(without quotes or punctuation) on its own line corresponding to the correct answer. In the end, repeat just the letter again by\nitself on a new line.\nReasoning:\nTable 19: Prompt used to let GPT-3.5 evaluate response relevance. [Dialog Context] and [Response] are\nplaceholders to be filled.\nValid\nTest\nTest Unseen\nGRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat. GRD Fact. Help. Rel. Conf. Sat.\nSLT(QG+RG(-+,)) 48.2 69.1 41.3 81.6 81.1 50.7 43.2 66.7 44.5 76.4 83.6 55.7 45.3 71.6 43.9 79.6 76.3 51.4\nILT(QG+RG(-+,)) 35.5 59.9 30.2 76.2 69.1 38.2 32.4 53.2 36.2 73.2 74.4 44.5 34.7 57.9 35.0 76.2 66.6 42.4\nTable 20: Evaluate dialog systems on FITS using designed metrics. SLT(QG+RG(-+,)): Our system-level trained\nresponse generator using satisfied and refinement data, paired with our trained query generator. ILT(QG+RG(-\n+,)): Our instance-level trained response generator using satisfied and sparse refinement data, paired with our\ntrained query generator.\nPrompt for feedback generation with GPT-3.5\nGiven the dialog history, the unsatisfactory last response the bot gave, and the requirements for a good response, your task is to\nwrite detailed and constructive feedback to improve the unsatisfactory response. The requirements for a good response include\nthe following:\n(1) The response should be conversational in tone and no more than twenty words.\n(2) If the user asks a question, the response should use relevant search results to answer the user\u2019s question correctly. It should\nnot leave the user to check out some resources on his or her own.\n(3) The response should be as concise and targeted as possible, and not include additional information the user has not asked for.\n(4) The response should be in a confident tone and not start with \u201cI\u2019m not sure\u201d or \u201cI don\u2019t know\u201d.\nBelow is the dialog context.\n[Dialog Context]\nBelow is the bot\u2019s unsatisfactory response.\n[Original Response]\nThe feedback for the unsatisfactory response is the following:\nTable 21: Prompt for feedback generation with GPT-3.5. [Dialog Context] and [Original Response] are\nplaceholders to be filled.\nHuman written instance-level feedback\nGPT-3.5 written instance-level feedback\nGood. You just need to include the reason and his age at the\ntime of his death in addition to this for a complete answer.\nThe response is not relevant to the user\u2019s question and in-\ncludes additional information that the user did not ask for.\nA good response should focus on answering the user\u2019s ques-\ntion about how Fitzgerald died, using relevant search results.\nIt should be concise, confident, and conversational in tone.\nYou could tell me to take my cat to the vet when they get\nsick.\nThe response is not helpful. Please provide information on\ntreatment options for cat rhinovirus. Use relevant search\nresults to answer the user\u2019s question correctly and be concise\nin your response. Avoid providing additional information\nthe user has not asked for.\nThe bot did not list Azerbaijan or Estonia, which were in-\ncluded in the search results. The bot should include these\ntwo other Soviet Republics in the response.\nThe response is informative but could be improved. Instead\nof listing the republics in a long sentence, try breaking them\ndown into bullet points for easier reading. Also, it would be\nhelpful to mention that there were 15 republics in total, as\nthe user requested.\nAgain, the bot\u2019s \"answer\" doesn\u2019t really make sense. It\nclaims that a character, not an actor, played the part of Celie.\nThe response is confusing and irrelevant. Please provide a\nclear and concise answer to the user\u2019s question using rele-\nvant search results. Avoid adding unnecessary information\nor asking unrelated questions.\nThe answer is right there in the results, and Charles de\nGaulle isn\u2019t correct.\nThe response is good, but it could be improved by being\nmore conversational and concise. Instead of just stating the\nanswer, try something like \"Charles de Gaulle was the first\nFrench president in 1848. Is there anything else I can help\nyou with?\" This makes the response more engaging and\nshows confidence in the answer.\nInteresting. We need to know which premier specifically,\nwith data to back up the claim. Can we be more specific?\nThe response is not helpful as it does not answer the user\u2019s\nquestion. Please provide a specific answer with relevant\nsearch results to support it. Avoid guessing or providing\nirrelevant information. Be confident and concise in your\nresponse.\nSearch about living creatures specifically.\nThe response is not relevant to the user\u2019s question. Please\nprovide a direct answer to the user\u2019s question using relevant\nsearch results. Avoid providing additional information that\nthe user has not asked for. Be confident in your response\nand avoid starting with \"I\u2019m not sure\" or \"I don\u2019t know\".\nActually it looks like that happened way later. Let\u2019s go back\nto the Revolutionary War and how that affected Britain.\nThe response is not helpful as it does not answer the user\u2019s\nquestion. A better response would be to use relevant search\nresults to explain the reasons for the decline of the British\nempire, such as economic challenges, decolonization move-\nments, and global power shifts. The response should be\nconfident and concise, and not include additional informa-\ntion the user has not asked for.\nUse the search results provided to formulate some answer.\nMention how there are wave patterns that one meteorologist\nuses and got 2/3 correct, mention a tool that they use like the\ndoppler radar which looks for things like airborne tornado\ndebris.\nThe response is too general and does not answer the user\u2019s\nquestion. Instead, use relevant search results to provide\nspecific weather conditions that meteorologists look for to\npredict tornado formation. Keep the response concise and\nconfident in tone.\nOne of the search results specifies that Paul Tillich was\nRollo May\u2019s mentor and friend, confirming that they did\nknow each other.\nThe response should have used relevant search results to\nanswer the user\u2019s question. It should have been more tar-\ngeted and concise, and not included additional information\nthe user did not ask for. A better response would have been:\n\"I\u2019m not sure if they knew each other, but according to my\nsearch results, Tillich was a major influence on Rollo May\u2019s\nwork.\"\nWhy didn\u2019t you mention CrossFit or Tabata?\nThe response should have used relevant search results to\nanswer the user\u2019s question correctly. Instead of saying \"I\u2019m\nnot sure,\" the bot should have confidently provided the cor-\nrect answer. Additionally, the response should have been\nmore concise and targeted, only providing the specific exer-\ncise type that burns the most calories.\nTable 22: Examples of human written and GPT-3.5 written instance-level feedback. For each row, both human\nfeedback and GPT-3.5 feedback are written to the same unsatisfied response.\n"
  },
  {
    "title": "GKD: Generalized Knowledge Distillation for Auto-regressive Sequence Models",
    "link": "https://arxiv.org/pdf/2306.13649.pdf",
    "upvote": "10",
    "text": "Published as a conference paper at ICLR 2024\nON-POLICY DISTILLATION OF LANGUAGE MODELS:\nLEARNING FROM SELF-GENERATED MISTAKES\nRishabh Agarwal12\u2217\nNino Vieillard1\u2217\nYongchao Zhou13\nPiotr Stanczyk1\u2020\nSabela Ramos1\u2020\nMatthieu Geist1\nOlivier Bachem1\n1Google DeepMind\n2Mila\n3University of Toronto\nABSTRACT\nKnowledge distillation (KD) is widely used for compressing a teacher model to\nreduce its inference cost and memory footprint, by training a smaller student model.\nHowever, current KD methods for auto-regressive sequence models suffer from\ndistribution mismatch between output sequences seen during training and those\ngenerated by the student during inference. To address this issue, we introduce\nGeneralized Knowledge Distillation (GKD). Instead of solely relying on a fixed set\nof output sequences, GKD trains the student on its self-generated output sequences\nby leveraging feedback from the teacher on such sequences. Unlike supervised KD\napproaches, GKD also offers the flexibility to employ alternative loss functions\nbetween the student and teacher, which may be useful when the student lacks the\nexpressivity to mimic the teacher\u2019s distribution. Furthermore, GKD facilitates the\nseamless integration of distillation with RL fine-tuning of language models. We\ndemonstrate the efficacy of GKD for distilling auto-regressive T5 language models\nfor task-specific distillation on summarization, translation, and reasoning tasks,\nand task-agnostic distillation for instruction tuning.\n1\nINTRODUCTION\nT5-Small\n(77M)\nT5-Base\n(250M)\nT5-Large\n(800M)\nStudent (params)\n14\n16\n18\n20\n22\nPerformance (ROUGE-2)\nT5-XL (Teacher)\n+111%\nXSum (Summarization)\nT5-Small\n(77M)\nT5-Base\n(250M)\nT5-Large\n(800M)\nStudent (params)\n25.5\n26.0\n26.5\n27.0\n27.5\n28.0\nPerformance (BLEU)\n+70%\nWMT (Translation)\nT5-Small\n(77M)\nT5-Base\n(250M)\nT5-Large\n(800M)\nStudent (params)\n5\n10\n15\n20\n25\n30\nAccuracy (w/ Calculator)\n+90%\nGSM8K (Reasoning w/ CoT)\nSupervised Fine-Tuning\nSupervised KD\nSeqKD\nGKD (On-policy)\nFigure 1: Comparing GKD with KD approaches across different student model sizes. We use the T5\nmodels (Raffel et al., 2020) trained with supervised FT as students. We use supervised FT T5-XL (\u223c3B params)\nas the teacher, whose performance is indicated by the horizontal line. Supervised KD and FT use ground-truth\noutput sequences for training while SeqKD trains on output sequences generated by the teacher. On-policy GKD\ntrains on output sequences sampled from the student. For GKD, we use JSD (0.1) on WMT and forward KL on\nother tasks. For evaluation, we use greedy sampling for XSum and GSM8K and beam search for WMT.\nAuto-regressive sequence models, such as language models (LMs), have shown impressive capabilities\nin numerous tasks, where the key to this success is often scaling the amount of training data as well as\n\u2217denotes equal contribution. \u2020denotes infrastructure contribution. Correspondence to Rishabh Agarwal\n<rishabhagarwal@google.com> and Nino Vieillard <vieillard@google.com>.\n1\narXiv:2306.13649v3  [cs.LG]  17 Jan 2024\nPublished as a conference paper at ICLR 2024\nthe number of model parameters (Kaplan et al., 2020). However, scaling parameter count comes at a\ncost, and the deployment of such models is limited by either their inference cost or memory footprint.\nThus, a crucial goal for practical use of large capable models is to compress them by reducing their\nparameter count, while retaining as much as possible of their performance.\nOne of the prevalent techniques for compressing models is knowledge distillation (Hinton et al.,\n2015). Distillation is the process of training a model \u2013 the student \u2013 to replicate the knowledge of\nanother model \u2013 the teacher \u2013 on a specific set of tasks. Typically, the student has fewer parameters\nthan the teacher and as such, distillation can improve task-specific performance while maintaining\nlower inference cost and memory footprint than the teacher. Current distillation methods for auto-\nregressive sequence models either require generating a fixed set of output sequences from the teacher\nmodel (Kim & Rush, 2016), which can be expensive, or a fixed dataset of sequences that the\nteacher can label by assigning token-level probabilities (Sanh et al., 2019). However, using a fixed\ndataset can lead to distribution mismatch between output sequences seen during training and the\nsequences generated by the student auto-regressively during inference, a well-known problem in\nimitation learning (Pomerleau, 1991; Ross & Bagnell, 2010). Furthermore, the common objective\nfor distillation is to minimize the forward KL between the teacher and the student distributions.\nHowever, the student may not be expressive enough to fit the teacher\u2019s distribution, which can result\nin student-generated samples that are unlikely to be generated by the teacher (e.g., Figure A.16).\nIn this paper, we propose Generalized KD (GKD) to mitigate the above issues. First, we recognize\nthat KD for auto-regressive sequence models can be viewed as an imitation learning problem with an\ninteractive expert (Ross et al., 2011). Using this insight, GKD trains the student on its self-generated\nsequences that are on-policy, instead of a fixed set of outputs sequences, using teacher probabilities\nas expert labels on these sequences. Our idea is further supported by the recent success of fine-tuning\nlarge language models on their own output sequences (Ouyang et al., 2022; Singh et al., 2023).\nFurthermore, GKD provides the flexibility to optimize alternative divergence measures, such as\nreverse KL and generalized JSD (Section 2), that can use student\u2019s limited capacity to focus on\ngenerating samples that are likely under the teacher.\nGKD unifies some existing KD methods for autoregressive LMs while instantiating new on-policy\nmethods that substantially outperform prevalent approaches. In terms of performance gains over the\ninitial student from on-policy GKD, averaged across T5 student models of different sizes, we see rela-\ntive gains of 2.1\u00d7 on summarization, 1.7\u00d7 on machine translation, and 1.9\u00d7 on arithmetic reasoning\ntasks, compared to the performance improvements achieved with baseline KD methods (Figure 1).\nAdditionally, we exhibit GKD\u2019s efficacy in task-agnostic distillation, resulting in 2% and 1% absolute\naccuracy improvement on the held-out BBH and MMLU benchmark suites (Figure 10).\nOur key contributions are:\n\u2022 To tackle discrepancy during training and inference for auto-regressive LMs, we present GKD\nthat leverages on-policy student-generated outputs for distillation, guided by the token-level\nteacher probabilities over these outputs. GKD substantially outperforms commonly-used methods\nin task-specific (Figure 1) and task-agnostic KD (Figure 10).\n\u2022 We demonstrate that on-policy GKD can be seamlessly combined with RL fine-tuning (e.g.,\nRLAIF) of language models, a combination that has not been previously explored (Figure 5).\n\u2022 Through a systematic evaluation of design choices in GKD, we offer practical insights about the\nimportance of using student-generated on-policy output sequences during distillation and the\ntask-dependent nature of optimal divergence between the student and the teacher.\n2\nPRELIMINARIES\nAuto-regressive Generative Sequence Models. We denote the input and output sequence as x, y\nrespectively. Let V denote the vocabulary comprising of M tokens, y<n+1 = (y1, y2, . . . , yn) denote\nthe generated output sequence up to the nth token, and Ly denote the length of sequence y. A\ntoken-level auto-regressive policy p(.|y<n, x) \u2208 (0, 1)M outputs a next-token probability distribution\nover all tokens in V, conditioned on the input x and output sequence y<n. Furthermore, y \u223c p(\u00b7|x)\ncorresponds to a sampled output sequence y given the input x. For ease of notation, we define\np(yn|x) := p(yn|y<n, x). Auto-regressive generation involves predicting tokens one at a time, based\non the previously generated tokens. The probability of predicting nth token yn, p(yn|x), is determined\n2\nPublished as a conference paper at ICLR 2024\nusing a softmax with temperature \u03b3: p(yn|x) =\nexp(zn/\u03b3)\nPM\ni=1 exp(zi/\u03b3), where zn is the logit score for the\ntoken yn. Higher values of \u03b3 introduces more randomness, while a lower value makes the output\nmore deterministic by favoring the most probable words. During training, the student\u2019s temperature\nis kept at 1. For evaluation, we use greedy sampling (\u03b3 \u2192 0) or temperature sampling (\u03b3 > 0).\nKL-Based Divergences. The divergence between two probability distributions is a measure of the\nsimilarity of the distributions, with KL divergence a prevalent measure. The KL divergence between\ntwo discrete distributions P(C) and Q(C) is given by: DKL(P\u2225Q) = P\nc\u2208C P(c) log P (c)\nQ(c).\nThe KL divergence is not symmetric: DKL(P\u2225Q) \u0338= DKL(Q\u2225P). As such, we refer to DKL(P\u2225Q)\nas the forward KL while DKL(Q\u2225P) as the reverse KL between P and Q. Forward KL under an\nempirical data distribution corresponds to maximum likelihood, which we optimize in supervised\nlearning. Given model capacity mismatch, when approximating P(C) using a distribution Q\u03b8(C),\nminimizing the reverse and forward KL results in mean and mode-seeking behavior (Figure A.16).\nWhile KL divergence can be unbounded, a well-known divergence that is bounded even for probability\ndistributions with disjoint supports is the generalized JSD (Jensen-Shannon divergence). JSD(\u03b2)\ninterpolates between the forward and reverse KL using the bounded coefficient 0 < \u03b2 < 1:\nDJSD(\u03b2)(P\u2225Q) = \u03b2DKL\n\u0010\nP\n\r\r\r\u03b2P + (1 \u2212 \u03b2)Q\n\u0011\n+ (1 \u2212 \u03b2)DKL\n\u0010\nQ\n\r\r\r\u03b2P + (1 \u2212 \u03b2)Q\n\u0011\n(1)\nHusz\u00b4ar (2015) show that lim\u03b2\u21920 DJSD(\u03b2)(P\u2225Q)/\u03b2 = DKL(P\u2225Q). As such, gradients of JSD(\u03b2)\nbehave similarly to forward KL and reverse KL when \u03b2 is close to 0 and 1 respectively.\n3\nDISTILLATION FOR AUTO-REGRESSIVE SEQUENCE MODELS\nProblem Setup. We are given two auto-regressive sequence models of different capacity, where\npS and pT refers to the student and teacher respectively. We assume that the student has learnable\nparameters \u03b8 and p\u03b8\nS is differentiable w.r.t \u03b8. We are also given a dataset of inputs X. Optionally,\nwe can also assume access to a dataset of input-output sequence pairs (X, Y ). If not given, such a\ndataset can be generated by sampling sequences from the teacher. For a divergence D, we define the\ndiscrepancy between token-level distributions of pT and pS as\nD\n\u0000pT\u2225p\u03b8\nS\n\u0001\n(y|x) := 1\nLy\nLy\nX\nn=1\nD\n\u0000pT(\u00b7|y<n, x)\u2225p\u03b8\nS(\u00b7|y<n, x)\n\u0001\n,\n(2)\nfor an input x and output sequence y. For example, using JSD(\u03b2) as D in equation 2 results in\nDJSD(\u03b2)\n\u0000pT||p\u03b8\nS\n\u0001\n(y|x) =\n1\nLy\nP\nn DJSD(\u03b2)\n\u0000pT(\u00b7|y<n, x)\n\r\rp\u03b8\nS(\u00b7|y<n, x)).\nSupervised FT. If we are only given a fixed dataset of ground-truth output sequences but not query\naccess to the teacher policy, then a simple approach is to minimize the negative log-likelihood of such\nsequences under the student policy: LSF T (\u03b8) = E(x,y)\u223c(X,Y )\n\u0002\n\u2212 log p\u03b8\nS(y|x)\n\u0003\n.\nSequence-Level KD (Kim & Rush, 2016). SeqKD maximizes the likelihood of high probability\nsequences generated by the teacher, and can be viewed as supervised FT on teacher-generated outputs.\nSupervised KD (Hinton et al., 2015; Sanh et al., 2019) is a widely used technique where the student\nis trained to imitate the token-level probability distributions of the teacher. The student pS is trained\nwith the supervised objective LSD over the target token-level probabilities of the teacher pT :\nLSD(\u03b8) := E(x,y)\u223c(X,Y )\nh\nDKL\n\u0000pT\u2225p\u03b8\nS\n\u0001\n(y|x)\ni\n,\n(3)\nwhere the expectation is over the samples from the dataset. This supervised objective results in a rich\ntraining signal by leveraging the full token-level distribution of the teacher.\n3.1\nGENERALIZED KNOWLEDGE DISTILLATION (GKD)\nAs discussed above, commonly-used KD approaches use a fixed dataset of output sequences, either\nusing ground-truth targets or teacher-generated sequences. However, distilling auto-regressive student\nmodels using such approaches results in train-inference distribution mismatch. This is because the\npartial sequences encountered by the student during the auto-regressive generation phase at inference\n3\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Generalized Knowledge Distillation (GKD)\n1: Given: Teacher model pT, Student Model p\u03b8\nS, Dataset (X, Y ) containing (input, output) pairs\n2: Hyperparameters: Student data fraction \u03bb \u2208 [0, 1], Divergence D, Learning rate \u03b7\n3: for each step k = 1, . . . , K do\n4:\nGenerate a random value u \u223c Uniform(0, 1)\n5:\nif u \u2264 \u03bb then\n6:\nSample inputs x from X and generate outputs y \u223c p\u03b8\nS(\u00b7|x) to obtain B = {(xb, yb)}B\nb=1\n7:\nelse\n8:\nSample batch of inputs and outputs from (X, Y ) to obtain B = {(xb, yb)}B\nb=1.\n9:\nend if\n10:\nUpdate \u03b8 to minimize LGKD: \u03b8 \u2190 \u03b8 \u2212 \u03b7 1\nB\nP\n(x,y)\u2208B \u2207\u03b8D(pT\u2225p\u03b8\nS)(y|x)\n11: end for\ncan be quite different from the ones seen during the training phase. Since predictions at any step are\ncontingent upon previous steps in auto-regressive models, this mismatch can have a cascading effect\nwhere error in prediction at early step can affect the future predictions, resulting in poor quality text\ngeneration. To address this mismatch, we draw heavily from imitation learning (IL). In particular,\non-policy imitation approaches (e.g. Ross et al., 2011) iteratively collect sequences using the student\npolicy, obtain expert labels for those sequences, and then retrain the student on this dataset. Despite\ntheir popularity in robotics and deep RL (Parisotto et al., 2015; Kelly et al., 2019; Agarwal et al.,\n2022), on-policy approaches are typically not used for distilling auto-regressive models.\nExtending on-policy imitation to distillation, we present on-policy KD. When using on-policy data\nduring distillation, the student receives token-specific feedback from the teacher\u2019s logits on the\nerroneous tokens in its self-generated output sequences. This enables a form of feedback loop akin to\nwhat we observe in RL, which helps minimize the train-inference distribution mismatch. Moreover,\nas the student evolves during training, the data it generates also improves in quality. Given an input\nx, the student generates the output sequence y and imitates the teacher token-level distributions,\npT (yn|x), on intermediate states y<n. Specifically, the on-policy loss LOD is given by\nLOD(\u03b8) := Ex\u223cX\nh\nEy\u223cpS(\u00b7|x)\n\u0002\nDKL\n\u0000pT\u2225p\u03b8\nS\n\u0001\n(y|x)\n\u0003i\n,\n(4)\nwhere we do not backpropagate through the student\u2019s sampling distribution pS(\u00b7|x), similar to\non-policy imitation. Not backpropagating through the sampling makes the training stable and\ncomputationally efficient. In on-policy KD, the training is done on output sequences that the student\nis likely to generate. During training, we use a temperature of \u03b3 = 1 to encourage diversity in student\ngenerated sequences. Moreover, given unlabeled input prompts, generating sequences using the\nstudent is computationally cheaper than the teacher, due to differences in their model sizes.\nBuilding further upon on-policy KD, we unify supervised and on-policy approaches and propose a\nmore general approach, which we call Generalized KD (GKD). In GKD, we can choose both the\ndivergence to optimize as well as the output sequences to train on. Specifically, we can optimize\nany divergence between the teacher and student token-level probability distributions. For output\nsequences, GKD uses a mixture of fixed dataset, either teacher-generated or ground-truth, and\non-policy student-generated sequences. Abstractly, GKD minimizes an objective of the form:\nLGKD(\u03b8) := (1 \u2212 \u03bb)E(x,y)\u223c(X,Y )\n\u0002\nD(pT\u2225p\u03b8\nS)(y|x)\n\u0003\n+ \u03bbEx\u223cX\nh\nEy\u223cpS(\u00b7|x)\n\u0002\nD(pT\u2225p\u03b8\nS)(y|x)\n\u0003i\n,\nwhere D(pT, pS)(y|x) is a divergence between teacher and student distributions (equation 2), and\n\u03bb \u2208 [0, 1] is a hyper-parameter that controls the student data fraction, that is, the fraction of on-policy\nstudent-generated outputs. Akin to on-policy KD, we do not backpropagate gradients through the\nstudent\u2019s sampling process. On-policy and supervised KD are instantiations of GKD with divergence\nD set to forward KL and student data fractions \u03bb to 1 and 0 respectively. That said, GKD allows for\nother choices for the fraction \u03bb and the divergence, which we explore in this work.\nRemark. As opposed to a randomly initialized student, we assume access to a student that can\ngenerate sequences of adequate quality, which the teacher can provide feedback upon. In our\nexperiments, we start from student models that have undergone supervised FT. This is analogous to\ntwo-stage RLHF training, which is widely used for LMs, where we first run SFT followed by the\n4\nPublished as a conference paper at ICLR 2024\nonline RL fine-tuning. As such, GKD can leverage hyperparameter tuning insights from RLHF and\ncan be combined with RLHF with small compute overhead and no additional hyperparameters.\nChoice of Divergence in GKD. While forward KL is commonly-used for distillation, it requires\nthe student to cover the entire support of the teacher token-level distribution pT(.|y<n, x). In doing\nso, the student might end up assigning probability mass to tokens v which have low probability\nunder pT(.|y<n, x), which can result in hallucination and low-quality generations. When the student\nhas much lower model capacity than the teacher, this issue is likely to happen with temperature\nsampling (e.g., Figure A.16). Alternatively, mode-seeking divergences, such as reverse KL, prioritize\nthe tokens where the teacher assigns high probability, which can avoid low-quality generations but\nat the expense of less diverse generations for a given input. Our experiments indicate that optimal\ndivergence seems to be task-dependent. Overall, the diversity and performance trade-offs for a\nparticular task needs to be considered when choosing the GKD divergence (e.g., Figure 4, 10).\n3.2\nRL FINE-TUNING + ON-POLICY GKD\nIn some tasks, it is plausible that distilling from a teacher model only provides a proxy to our\nmain objective, which can also be non-differentiable. We can directly optimize this objective with\nreinforcement learning (RL). Conveniently, on-policy GKD can be easily combined with RL fine-\ntuning from human (RLHF) or AI feedback (RLAIF), as it only requires output samples from the\nstudent. Indeed, consider that one wants to optimize the student policy for a scalar reward r, while\nstaying close to a teacher policy, then we get a regularized RL fine-tuning objective of the form:\nEx\u223cX\nh\n(1 \u2212 \u03b1) Ey\u223cp\u03b8\nS (\u00b7|x) [r(y)]\n|\n{z\n}\nRL objective\n\u2212\u03b1 Ey\u223cpS(\u00b7|x)\n\u0002\nD(pT\u2225p\u03b8\nS)(y|x)\n\u0003\n|\n{z\n}\nGeneralized On-Policy Distillation\ni\n,\n(5)\nwhere \u03b1 \u2208 [0, 1] controls the strength of the distillation loss compared to the RL objective. With \u03b1 =\n1, it will perform only distillation. The above objective allows us to maximize reward while improving\nother model capabilities via distillation, which can possibly reduce the \u201calignment tax\u201d decrease\nin general model capabilities when aligning language models with human preferences (Ouyang\net al., 2022). We apply the above idea to mitigate hallucination using RLAIF, while simultaneously\nimproving downstream performance via distillation (Figure 5).\nRemark. In RLHF or RLAIF, we typically use reverse KL to constrain the learned policy to stay\nclose to the initial policy. If one wants to only make slight modifications to existing RL fine-tuning\nworkflows, we recommend using reverse KL or JSD (0.9) when integrating GKD with RL.\n4\nEXPERIMENTS\nIn this section, we evaluate GKD for distilling language models, a typical class of auto-regressive\nsequence models, on abstractive summarization, machine translation, and arithmetic reasoning.\nStudent / Teacher Models. Our experiments start from student and teacher models with different\nsizes, specifically open-sourced T5 models (Raffel et al., 2020), that are pretrained on the same\ndatasets. We use supervised fine-tuned T5-XL (\u223c 3B params) as the teacher. For students, we use\nT5-small (77M params), T5-base (250M params), and T5-large (800M params), which are smaller\nthan the teacher by a factor of 38\u00d7, 12\u00d7 and 3.8\u00d7 respectively. See Appendix A.2 for more details.\nGKD Variants. For choice of divergence D in GKD in Algorithm 1, we use forward KL, reverse KL\nand three variants of JSD(\u03b2): JSD (0.1), JSD (0.5) and JSD (0.9). For student data fraction \u03bb, we\ntry \u03bb = 1 (On-policy), \u03bb = 0.5 (Mixed) and \u03bb = 0 (Supervised). In particular, we are interested in\nthe on-policy variants (\u03bb = 1), which have not been previously explored.\nBaselines. We compare to the widely-used KD methods discussed in Section 3: SeqKD and\nSupervised KD. We also evaluate ImitKD (Lin et al., 2020) and f-distill (Wen et al., 2023), which can\nbe viewed as \u201cmixed\u201d data variants of GKD (\u03bb = 0.5) with forward KL and total variation distance as\ndivergence. All the baselines start from the same supervised fine-tuned student checkpoint as GKD.\n4.1\nCASE STUDY: ABSTRACTIVE SUMMARIZATION\nWe start by evaluating GKD on an abstractive summarization task of generating a summary that\ncaptures salient ideas of the input document. To do so, we use the XSum dataset (Narayan et al., 2018),\n5\nPublished as a conference paper at ICLR 2024\n18\n19\n20\n21\n22\nROUGE-2\nTemperature\nSampling\n18\n19\n20\n21\n22\nGreedy Sampling\nXSum: T5-XL \n Large\nSupervised KD\nImitKD\nf-distill\nGKD (Forward KL)\nGKD (JSD(0.9))\nFigure 2: Comparing GKD to baselines on distillation\nfrom T5-XL to T5-large on XSum. On-policy GKD\nvariants generally outperform baselines.\n0.5%\n5%\n25%\n100%\nTraining Dataset Fraction\n11\n12\n13\n14\n15\nROUGE-2\nXSum T5-XL \n T5-Small\nSupervised KD\nImitKD\nGKD (Supervised)\nGKD (On-policy)\nFigure 3: Scaling training data. We evaluate distilled\nT5-small using temperature sampling (\u03b3 = 1). GKD is\nmore data efficient than baselines.\n20\n40\n60\n80\nGeneration Similarity (Self-BLEU)\n10.5\n12.5\n14.5\n16.5\nPerformance (ROUGE-2)\nPerformance / Diversity Trade-off on XSum: Distilled T5-small\nTemperature ( )\n0.1\n0.3\n0.5\n1.0\nGKD Divergence\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nFigure 4: Effect of Divergence on Performance and Diversity. Utilizing on-policy GKD with different\ndivergences, we evaluate the trade-off between the distilled student\u2019s generation quality and diversity, by varying\nthe sampling temperature. We quantify diversity using Self-BLEU (Zhu et al., 2018), where a score of 100\nindicates deterministic outputs and 0 signifies maximum diversity. Transitioning from forward KL to reverse KL,\nthrough generalized JSD, leads to decreased diversity, attributed to the enhanced mode-seeking characteristic of\nthe divergence. Mode-seeking divergences often yield superior quality, especially at high temperatures (\u03b3 = 1).\nReducing the temperature curtails diversity while narrowing performance differences among divergences.\nwhich consists of news articles paired with human-written summaries. Following PaLM (Chowdhery\net al., 2022), we evaluate performance using ROUGE-2 score (Lin, 2004) of predicted summaries\non the validation split of XSum but observe similar trends in ROUGE-L and ROUGE-1. We use T5\nmodels supervised fine-tuned on XSum as students for distillation while the fine-tuned T5-XL as the\nteacher. See Appendix A.3 for additional experimental details.\nComparison to baselines. First, we explore how GKD compares to widely-used KD approaches,\nnamely SeqKD and Supervised KD, across different student model sizes. As shown in Figure 1, we\nobserve consistent improvements with GKD, which demonstrates the scalability of GKD with respect\nto the student capacity. Notably, GKD allows us to surpass the few-shot performance of PaLM (540B)\nusing a 7000\u00d7 smaller T5 model. We also compare GKD variants with ImitKD and f-distill, and\nevaluate performance with greedy sampling and temperature sampling (\u03b3 = 1) in Figure 2. On-policy\nGKD with JSD (0.9) outperforms these additional baselines in both scenarios.\nData efficiency and scaling. To evaluate the efficiency and scalability of GKD, we distilled the\nT5-XL teacher using subsampled XSum training datasets: 1K (0.5%), 10K (5%), and 50K (25%)\nexamples. We used T5-small as the student and report data scaling curves in Figure 3. Notably,\non-policy GKD on the 5% subsampled dataset, without any ground-truth summaries, outperforms\nsupervised KD and ImitKD with entire training dataset with ground-truth summaries.\nGKD Ablations. We ablated different divergences and student data fractions in GKD for various\nstudent sizes in Figure A.12 and A.13. On-policy and mixed variants consistently outperform super-\nvised variants. Mode-seeking divergences perform better when evaluation is done using temperature\nsampling while the choice of divergence doesn\u2019t affect performance much with greedy sampling.\nChoosing GKD Divergence. The divergence chosen for distillation is crucial in determining the\ntrade-off between summarization quality and diversity. As the sampling temperature can also be\n6\nPublished as a conference paper at ICLR 2024\n3\n4\n5\n6\n7\n8\nImprovement in Summary Quality (\u0394ROUGE-2)\n15.0%\n25.0%\n35.0%\n45.0%\nImprovement in \n Factual Consistency \n (\u0394 Entailment)\n\u03b1 = 0.05\n\u03b1 = 0.1\n\u03b1 = 0.25\n\u03b1 = 0.5\nTeacher (T5-XL)\nRLEF*\nRL Fine-Tuning + Distillation\nFigure 5: RLAIF + On-policy GKD. We show the trade-off between reward maximization and summarization\nperformance on XSum. We report improvements relative to the original T5-base student. Following Roit et al.\n(2023), we use the textual entailment score from a T5-XXL NLI classifier as the reward. \u03b1 controls the strength\nof the on-policy GKD loss with JSD (0.9). As \u03b1 increases, ROUGE-2 increases while improvement in factual\nconsistency decreases. For comparison, we show the relative performance of the 12\u00d7 larger T5-XL teacher.\nRLEF* corresponds to RLAIF method from Roit et al. (2023), where the student is regularized towards the\noriginal student model itself instead of the teacher. On-policy GKD + RL achieves higher ROUGE-2 compared\nto RLEF* while generating more factually consistent summaries compared to the teacher.\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n0.55\n0.85\n0.77\n0.79\n0.46\n0.5\n0.71\n0.57\n0.43\n0.38\n0.08\n0.28\n0.26\n0.29\n0.27\nBLEU Improvement: T5-XL \n Small\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\n0.55\n0.71\n0.58\n0.58\n0.47\n0.52\n0.48\n0.54\n0.4\n0.34\n0.22\n0.38\n0.17\n0.11\n0.02\nBLEU Improvement: T5-XL \n Base\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nFigure 6: Varying student data fraction and divergence in GKD on WMT en \u2192 de. For evaluation, we\nuse beam search and report the improvement in BLEU score of distilled student relative to the original student.\nResults are averaged across three seeds. We observe that using only student-generated output samples outperform\nother GKD variants. We use the T5-XL (\u223c3B params) supervised fine-tuned on WMT as the teacher, which\nobtains a BLEU score of 28. (Left) We use T5-small (77M params) as the student, which obtain a BLEU score\nof 25.58. (Right) Student corresponds to T5-base (250M params) with a BLEU score of 26.98.\nadjusted to balance summary quality and diversity, the optimal choice of divergence is temperature-\ndependent. To understand this dependence, we evaluate T5-small distilled using on-policy GKD\nwith different divergences. As shown in Figure 4, certain divergences, like JSD (0.5) and JSD (0.9),\noffer better quality but less diversity at high temperatures. However, as temperature decreases, the\ndifference in quality among divergences narrows, while diversity also drops.\nOn-policy GKD with RL. In summarization, we want model-generated summaries to be factually\nconsistent with their input documents. However, distillation alone might not improve factual consis-\ntency as even large models halluncinate and generate inconsistent summaries. Recently, Roit et al.\n(2023) mitigate hallucination on summarization tasks by using RL with textual entailment feedback\nas the reward (RLEF), as faithful summaries must be textually entailed from their input documents.\nInspired by their success, we explore combining RL fine-tuning using a REINFORCE-like objective\nwith on-policy GKD, as described in Section 3.2. As shown in Figure 5, GKD with RL fine-tuning\nsubstantially improves factual consistency compared to the teacher model while obtaining large\nimprovements in summarization quality for the distilled student model.\n4.2\nMACHINE TRANSLATION\nTo evaluate GKD beyond summarization, we consider the task on translating English to German using\nWMT14 en-de (Bojar et al., 2014). We report performance on the validation split using the BLEU\nscore, which measures the similarity of machine-translated text to high quality reference translations.\n7\nPublished as a conference paper at ICLR 2024\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n8.8\n7.6\n7\n6.7\n8\n6.8\n6.2\n6.4\n5.9\n6.9\n4.7\n5.1\n5.1\n5\n5.7\nAccuracy Improvement: T5-XL \n Base\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\nFigure 7: Ablating GKD on GSM8K. We distill fine-\ntuned T5-XL to T5-Base, which obtain an accuracy of\n27.9 and 10.16 with greedy sampling.\n0%\n25%\n50%\n75%\n100%\nStudent Data Proportion ( )\n7\n8\n9\n10\n11\nTest Accuracy\nGSM8K: Flan T5-XL \n Small\nForward KL\nReverse KL\nFigure 8: Varying on-policy data on GSM8K. As we\nincrease fraction of student-generated data beyond 25%,\nperformance typically improves.\n6\n8\n10\n12\nGSM8K Test Accuracy \n(4-shot-CoT + Calculator)\n0-shot davinci-002* (175B)\nFlan T5-XL \n Small\n12\n14\n16\n18\nFew-shot PaLM* (540B)\nFlan T5-XL \n Base\n22\n24\n26\n28\n30\nFlan T5-XL \n Large\nOn-policy GKD\nImitKD\nGKD (Supervised)\nf-distill\nSupervised KD\nSeqKD\nFigure 9: Distillation on GSM8K with few-shot CoT prompting. On-policy GKD substantially outperform\nother approaches. As a reference, we provide GPT-3 davinci-002 results as well as PaLM (540B) results (without\na calculator). We use forward KL and reverse KL respectively for on-policy and supervised GKD.\nWe use supervised fine-tuned T5-XL as the teacher with a softmax-temperature of 1.0 (BLEU score\nof 28). See Appendix A.5 for additional experimental details.\nResults. Figure 1 and A.15 show that on-policy GKD outperforms commonly-used KD approaches.\nFurthermore, we ablate GKD variants using T5-small and T5-base as students in Figure 6. We observe\nthat generalized JSD divergences perform better than forward or reverse KL but their performance gap\nreduces when using a larger student. Moreover, using purely on-policy and mixed data distributions\nconsistently outperform GKD variants only using a fixed supervised dataset, showing the importance\nof generating on-policy output sequences from the student. The efficacy of on-policy data on WMT\naligns with our findings on XSum.\n4.3\nARITHMETIC REASONING\nWei et al. (2022) show that reasoning abilities only appear to emerge in LLMs with at least several\nbillions parameters, making KD important for improving reasoning abilities of smaller models. To\nthis end, we evaluate GKD on GSM8K (Cobbe et al., 2021), a high-quality dataset of grade school\nmath word problems requiring multi-step logical inference. Here, we explore GKD in conjunction\nwith chain-of-thought (CoT) (Wei et al., 2022), a common approach to improve reasoning abilities of\nLLMs by prompting them to produce intermediate reasoning steps before giving the final answer.\nSetup. We perform few-shot prompting by prepending the math problems in GSM8K with the first 4\nCoT input-output exemplars from Wei et al. (2022). For evaluation, we report accuracy on the test\nsplit by checking whether the target answer matches the final answer given an external calculator,\nakin to Cobbe et al. (2021). For supervised training, we use the CoT outputs generated by Magister\net al. (2022), resulting in around 5.3K (problem, CoTs) pairs in the original training split of GSM8K.\nWe use Flan-T5 models (Chung et al., 2022) supervised fine-tuned for 10K steps on the above CoT\ndataset as a starting point for distillation. We use the fine-tuned FLAN T5-XL as the teacher, which\nobtains a test accuracy of 27.9. See additional experimental in Appendix A.4.\nResults. We first ablate GKD variants and report results in Figure 7 and A.14. We observe that when\nusing only the fixed CoT dataset or mixing it with student-generated CoTs, performance consistently\nfalls short of using solely the student-generated CoTs. Furthermore, forward KL performs quite well,\nsimilar to our findings on XSum with greedy sampling. Notably, reverse KL also performs well,\n8\nPublished as a conference paper at ICLR 2024\nDistillation Method\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\nImprovement in \nAvg. MMLU Accuracy (%)\nMMLU (57 tasks): FLAN T5-XL \n Base\nDistillation Method\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nImprovement in \nAvg. BBH Accuracy (%)\nBBH (23 tasks): FLAN T5-XL \n Base\nGKD (On-policy, Forward KL)\nImitKD\nSupervised KD\nSupervised GKD (Reverse KL)\nGKD (On-policy, Reverse KL)\nFigure 10: Task-agnostic Distillation on FLAN (Chung et al., 2022). On-policy GKD with reverse KL\noutperforms other approaches. The evaluation metric on both the MMLU and BBH benchmark suites is\nfew-shot prompted accuracy (exact match), where we take an unweighted average over all tasks. These\nevaluation benchmarks are held-out (not included in the distillation data). Here, we do not run SeqKD due to\nits computational inefficiency for generating data from the teacher during training. The teacher FLAN T5-XL\nachieves an accuracy of 52.4% on MMLU and 41% on BBH, while the student T5-large model obtains an\naccuracy of 35.6% on MMLU and 31.25% on BBH.\nespecially when training using only a fixed dataset. Additionally, Figure 8 shows that performance\nconsistently improves as the proportion of on-policy data increases, provided that at least 25% of\nthe data is on-policy. Moreover, we demonstrate that on-policy GKD have superior performance\ncompared to baseline KD methods, across all student sizes, as shown in Figure 9. Finally, we observe\npromising results with GKD for self-distillation on GSM8k, as shown in Appendix A.1.\n4.4\nTASK-AGNOSTIC DISTILLATION: INSTRUCTION TUNING\nWhile task-specific distillation provides optimized performance for predefined tasks, which is often\ncrucial for deployment purposes, task-agnostic distillation offers a compelling alternative in scenarios\nwhere the exact nature of the task is not known beforehand and can vary during deployment. As\nhighlighted by Sanh et al. (2019), the allure of task-agnostic distillation lies in its efficiency: once\ndistilled, a model can be re-purposed for multiple downstream tasks via prompting or fine-tuning.\nSetup. To study task-agnostic KD, we focus on instruction tuning (Chung et al., 2022). Our aim is to\nenhance the distilled model\u2019s proficiency to handle diverse tasks presented in the form of instructions.\nTo achieve this, we employ the FLAN T5-XL model as our teacher and distill its knowledge into\nthe FLAN T5-Base, as introduced by Chung et al. (2022). Our distillation process utilizes the\ncomprehensive FLAN2021 instruction tuning dataset, which boasts 5.36 million examples spanning\n62 distinct language understanding and generation tasks. For hyperparameter details, see Table A.4.\nEvaluation. To gauge the versatility of a task-agnostic model, it is essential to test it across a diverse\nset of tasks. In line with Chung et al. (2022), we evaluate our distilled T5-base student on two\nheld-out benchmark suites: (1) MMLU (Massive Multitask Language Understanding) includes exam\nquestions from 57 tasks such as mathematics, history, law, and medicine, and (2) BBH (BIG-Bench\nHard) includes 23 tasks from BIG-Bench for which PaLM 540B (Chowdhery et al., 2022) performs\nbelow average human raters. For performance, we report the distilled model\u2019s ability to directly\npredict the answer via standard few-shot prompting, averaged across tasks in MMLU and BBH.\nResults. We report the performance of distilled checkpoints obtained after 50K training steps for\nvarious methods in Figure 10. We find that on-policy GKD with reverse KL substantially outperforms\nsupervised KD and ImitKD. Notably, in the context of instruction tuning, we find that using reverse\nKL performs much better than forward KL. We hypothesize that the efficacy of reverse KL in\ninstruction tuning may stem from its mode-seeking nature as it ensures that the model zeroes in on\nthe main intent or behavior specified by the instruction. As a result, the model might prioritize core\nbehaviors over less relevant details, leading to better performance on held-out tasks.\n5\nRELATED WORK\nKnowledge distillation. Supervised KD (Bucilu\u02c7a et al., 2006; Hinton et al., 2015) is a classic\napproach and has been successfully used for distilling auto-regressive models (Sanh et al., 2019).\n9\nPublished as a conference paper at ICLR 2024\nAnother approach for distilling such models is sequence-level KD (Kim & Rush, 2016). On-policy\nGKD substantially outperforms supervised KD and SeqKD (Figure 1). Other KD approaches train the\nstudent to match different quantities obtained from the teacher, such as hidden states (Jiao et al., 2020)\nor attention scores (Wang et al., 2020). However, none of these approaches make the connection\nbetween distillation and imitation learning, and a purely supervised approach can suffer from train-\ninference mismatch, also known as exposure bias (Ranzato et al., 2015; Bengio et al., 2015). While\nHe et al. (2019) argue that this mismatch may not be critical, several papers demonstrate that exposure\nbias leads to poor text generation (Zhang et al., 2019; Chiang & Chen, 2021; Arora et al., 2022).\nImitKD (Lin et al., 2020) identifies this connection by sampling sequences from both the student\nand a fixed dataset but does not push the idea further. Unlike GKD, ImitKD does not explore\npurely on-policy data collection, nor does it integrate RL fine-tuning. Moreover, ImitKD keeps\nthe forward KL at the token level, which is not necessary when one has access to the teacher\u2019s\nlog-probabilities, rather than just samples. Furthermore, GKD demonstrates the scalability of the idea,\nhandling student models roughly 26\u00d7 larger than those explored by ImitKD. ImitKD can be viewed\nas GKD with forward KL and a non-increasing schedule on \u03bb, a simple choice being \u03bb = 0.5. More\nrecently, f-distill (Wen et al., 2023) formulates sequence-level KD as minimizing an f-divergence and\npropose an tractable objective based on total variation distance between the token-level student and\nteacher distributions. In essence, both ImitKD and f-distill are specific instances of GKD, which we\ndemonstrate lead to worse empirical results than on-policy GKD (Figure 2, 9).\nThe concurrent work on MiniLLM (Gu et al., 2023) also exploits the link to imitation and frame\ndistillation as an RL problem. In particular, MiniLLM optimizes reverse KL between the teacher\nand the student at the sequence level (while likelihood maximization is the forward one) using a\npolicy gradient approach. However, we argue that GKD is simpler and more stable, being closer to\nsupervised training, since it does not backpropagate through the student\u2019s sampling process. Indeed,\nMiniLLM relies on a number of stabilizing tricks, to tackle high variance, reward hacking, and\ngeneration length bias. GKD is also more general as it can also be used with other divergences such\nas forward KL or JSD, which can perform better than reverse KL (Figure 6, 7).\nRL fine-tuning. There are now numerous examples of language models being fine-tuned with RL, be\nthe reward optimizing for some metric (Wu et al., 2018), or learned using human feedback (Ouyang\net al., 2022). In these approaches, it is typical to regularize the RL fine-tuned model towards the\ninitial (usually supervised fine-tuned) model. However, as far as we know, we are the first to perform\ndistillation and RL fine-tuning at the same time (Figure 5). If it may seem natural, it is quite different\nfrom an optimization perspective, as it changes the regularization towards the initial policy to towards\nthe teacher policy, and we show empirically that it is a viable approach.\nDistillation with reasoning traces or rationales. Chain-of-Thought prompting (Nye et al., 2021;\nWei et al., 2022) has recently demonstrated that LLMs can solve complex reasoning tasks, step by\nstep, just by prompting. This idea was quickly adapted to KD, by extending the teacher dataset\nwith CoT prompts for fine-tuning the student (Magister et al., 2022; Ho et al., 2022; Hsieh et al.,\n2023). The distillation is still done in a supervised way, and other kind of enhanced prompts could be\nconsidered (Li et al., 2022; Mukherjee et al., 2023). We adopt the same approach, but combine it\nwith on-policy distillation with various divergences. It shows the versatility of GKD, and improves\nupon the purely supervised approaches, as seen in our results on GSM8K (Figure 9).\nApplication to speculative decoding. Zhou et al. (2023) and Liu et al. (2023) apply GKD to improve\nthe alignment between draft and target model for better inference speedup from speculative decoding.\n6\nCONCLUSION\nIn this work, we proposed GKD to address the train-inference distribution mismatch when distilling\nauto-regressive language models. GKD consistently outperformed commonly-used knowledge distil-\nlation approaches on three language generation tasks: abstractive summarization, machine translation,\nand arithmetic reasoning. We further showed that GKD can be combined with reinforcement learning\nto optimize a sequence-level reward in addition to distilling the knowledge of a large teacher model,\nwhich we believe can improve the widely-used RLHF training phase for language models. One\ninteresting direction for future work would be extending GKD to auto-regressive sequence models\nfor audio (Radford et al., 2023), video (Villegas et al., 2022) and text-to-image generation (Yu et al.,\n2022). We hope that our work will be valuable for researchers and practitioners who are working on\nimproving performance and efficiency of generative auto-regressive sequence models.\n10\nPublished as a conference paper at ICLR 2024\nREFERENCES\nRishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Bellemare.\nReincarnating reinforcement learning: Reusing prior computation to accelerate progress. Advances\nin Neural Information Processing Systems, 35:28955\u201328971, 2022.\nKushal Arora, Layla El Asri, Hareesh Bahuleyan, and Jackie Chi Kit Cheung. Why exposure bias\nmatters: An imitation learning perspective of error accumulation in language generation. arXiv\npreprint arXiv:2204.01171, 2022.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence\nprediction with recurrent neural networks. Advances in neural information processing systems, 28,\n2015.\nOnd\u02c7rej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes\nLeveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014\nworkshop on statistical machine translation. In Proceedings of the ninth workshop on statistical\nmachine translation, pp. 12\u201358, 2014.\nCristian Bucilu\u02c7a, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings\nof the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pp.\n535\u2013541, 2006.\nTing-Rui Chiang and Yun-Nung Chen. Relating neural text degeneration to exposure bias. arXiv\npreprint arXiv:2109.08705, 2021.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nYuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation of large language models.\narXiv preprint arXiv:2306.08543, 2023.\nTianxing He, Jingzhao Zhang, Zhiming Zhou, and James Glass.\nExposure bias versus self-\nrecovery: Are distortions really incremental for autoregressive text generation? arXiv preprint\narXiv:1905.10617, 2019.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers.\narXiv preprint arXiv:2212.10071, 2022.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301,\n2023.\nFerenc Husz\u00b4ar. How (not) to train your generative model: Scheduled sampling, likelihood, adversary?\narXiv preprint arXiv:1511.05101, 2015.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.\nTinybert: Distilling bert for natural language understanding. In Findings of the Association for\nComputational Linguistics: EMNLP 2020, pp. 4163\u20134174, 2020.\n11\nPublished as a conference paper at ICLR 2024\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nMichael Kelly, Chelsea Sidrane, Katherine Driggs-Campbell, and Mykel J Kochenderfer. Hg-dagger:\nInteractive imitation learning with human experts. In 2019 International Conference on Robotics\nand Automation (ICRA), pp. 8077\u20138083. IEEE, 2019.\nYoon Kim and Alexander M Rush.\nSequence-level knowledge distillation.\narXiv preprint\narXiv:1606.07947, 2016.\nTuan Anh Le. Reverse vs forward kl, December 2017. URL https://www.tuananhle.co.\nuk/notes/reverse-forward-kl.html.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better.\narXiv preprint arXiv:2210.06726, 2022.\nAlexander Lin, Jeremy Wohlwend, Howard Chen, and Tao Lei. Autoregressive knowledge distillation\nthrough imitation learning. arXiv preprint arXiv:2009.07253, 2020.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang.\nOnline speculative decoding. arXiv preprint arXiv:2310.07177, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.\nTeaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022.\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed\nAwadallah. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint\narXiv:2306.02707, 2023.\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Don\u2019t give me the details, just the sum-\nmary! topic-aware convolutional neural networks for extreme summarization. arXiv preprint\narXiv:1808.08745, 2018.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:\nScratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,\n2021.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nEmilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. Actor-mimic: Deep multitask and\ntransfer reinforcement learning. arXiv preprint arXiv:1511.06342, 2015.\nDean A Pomerleau. Efficient training of artificial neural networks for autonomous navigation. Neural\ncomputation, 3(1):88\u201397, 1991.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International Conference on\nMachine Learning, pp. 28492\u201328518. PMLR, 2023.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nMarc\u2019Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training\nwith recurrent neural networks. arXiv preprint arXiv:1511.06732, 2015.\n12\nPublished as a conference paper at ICLR 2024\nPaul Roit, Johan Ferret, Lior Shani, Roee Aharoni, Geoffrey Cideron, Robert Dadashi, Matthieu\nGeist, Sertan Girgin, L\u00b4eonard Hussenot, Orgad Keller, et al. Factually consistent summarization\nvia reinforcement learning with textual entailment feedback. arXiv preprint arXiv:2306.00186,\n2023.\nSt\u00b4ephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of the\nthirteenth international conference on artificial intelligence and statistics, pp. 661\u2013668. JMLR\nWorkshop and Conference Proceedings, 2010.\nSt\u00b4ephane Ross, Geoffrey Gordon, and Drew Bagnell. A reduction of imitation learning and structured\nprediction to no-regret online learning. In Proceedings of the fourteenth international conference\non artificial intelligence and statistics, pp. 627\u2013635. JMLR Workshop and Conference Proceedings,\n2011.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108, 2019.\nNoam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning, pp. 4596\u20134604. PMLR, 2018.\nAvi Singh, John D Co-Reyes, Rishabh Agarwal, Ankesh Anand, Piyush Patil, Peter J Liu, James\nHarrison, Jaehoon Lee, Kelvin Xu, Aaron Parisi, et al. Beyond human data: Scaling self-training\nfor problem-solving with language models. arXiv preprint arXiv:2312.06585, 2023.\nRuben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kindermans, Hernan Moraldo, Han Zhang,\nMohammad Taghi Saffar, Santiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki: Variable\nlength video generation from open domain textual description. arXiv preprint arXiv:2210.02399,\n2022.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-\nattention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural\nInformation Processing Systems, 33:5776\u20135788, 2020.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nYuqiao Wen, Zichao Li, Wenyu Du, and Lili Mou. f-divergence minimization for sequence-level\nknowledge distillation. arXiv preprint arXiv:2307.15190, 2023.\nLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. A study of reinforcement learning for\nneural machine translation. arXiv preprint arXiv:1808.08866, 2018.\nJunho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast\noptimization, network minimization and transfer learning. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pp. 4133\u20134141, 2017.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. Transactions on Machine Learning Research, 2022.\nWen Zhang, Yang Feng, Fandong Meng, Di You, and Qun Liu. Bridging the gap between training\nand inference for neural machine translation. In Anna Korhonen, David Traum, and Llu\u00b4\u0131s M`arquez\n(eds.), Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,\n2019. URL https://aclanthology.org/P19-1426.\nYongchao Zhou, Kaifeng Lyu, Ankit Singh Rawat, Aditya Krishna Menon, Afshin Rostamizadeh,\nSanjiv Kumar, Jean-Franc\u00b8ois Kagy, and Rishabh Agarwal. Distillspec: Improving speculative\ndecoding via knowledge distillation. arXiv preprint arXiv:2310.08461, 2023.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:\nA benchmarking platform for text generation models. In The 41st international ACM SIGIR\nconference on research & development in information retrieval, pp. 1097\u20131100, 2018.\n13\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nSELF-DISTILLATION\nSupervised KD\nGKD (FKL)\nImitKD\nf-distill\nGKD (JSD(0.9))\n16\n18\n20\n22\n24\n26\nAccuracy (w/ Calculator)\nTeacher: FLAN T5-Large (SFT)\nStudent: FLAN T5-Large\nSelf-Distill (GSM8K): FLAN T5-Large\nFigure A.11: Self-Distillation on GSM8K. Here, GKD corresponds to on-policy GKD (\u03bb = 1). On-policy\nGKD variants outperform other approaches including supervised KD. The teacher FLAN T5-Large is supervised\nfine-tuned on GSM8K and achieves an accuracy of 20.5% while the student FLAN T5-large (not trained on\nGSM8K) obtains an accuracy of 14.4% on the test set.\nSelf-Distillation. We investigate whether GKD works for self-distillation (Yim et al., 2017), where\nwe want to transfer knowledge from a teacher model to a student model with the same architecture and\nsize. To investigate this, we consider self-distillation on GSM8K with FLAN-T5 large as the student\nand teacher, where the teacher is supervised fine-tuned on GSM8K. As shown in Figure A.11, self-\ndistilled students surpass surpasses the teacher\u2019s performance on the test set. Moreover, distillation\nusing student-generated data outperforms supervised KD, with on-policy GKD performing the best.\nA.2\nT5 MODELS\nAs base checkpoints, we start from LM-adapted T5v1.1 models. These LM-adapted models are\ninitialized from T5v1.1 and trained for an additional 100K steps on the LM objective discussed in\nthe T5 paper (Raffel et al., 2020). These checkpoints are open-sourced at https://console.\ncloud.google.com/storage/browser/t5-data/pretrained_models.\nIn our experiments, we initialize both the student and teacher models for distillation by running\nfurther supervised FT on the original training dataset, as described below:\n\u2022 XSum. For small, base, large and XL models, we use LM-Adapted T5v1.1 models super-\nvised fine-tuned for 100K, 50K, 38k and 8K steps respectively.\n\u2022 WMT. For small, base, large and XL models, we use LM-Adapted T5v1.1 models supervised\nfine-tuned for 250K, 250K, 110k and 50K steps respectively.\n\u2022 GSM8K. All models were supervised fine-tuned starting from FLAN-T5 models on the\nPalm-540 generated CoT dataset for 10K steps.\nSimilar to T5 and FLAN-T5, our experiments use the Adafactor optimizer (Shazeer & Stern, 2018).\nComputational cost of GKD. All methods including baselines start from the supervised fine-tuned\nstudent checkpoint, which requires training for a few hours on the smallest TPUv3 (8 cores). On\nGSM8K, the computational overhead from student sampling is approximately 1.8\u00d7, 2\u00d7 and 2.2\u00d7\ncompared to sampling from a fixed dataset of outputs, for a student-teacher ratio of 38\u00d7, 12\u00d7 and\n3.8\u00d7. For RLHF + GKD, the computational overhead is somewhat small as we are only running\ninference to get teacher logits instead of student logits.\nMoreover, the majority of cost in real world use cases is due to serving cost at inference time and\nnot due to fine tuning. Concretely, if it is too costly to sample from the student during fine-tuning, it\nmight also be too expensive to serve this model to users (which may range from tens of thousands\nto billions). Overall, performance benefits from on-policy GKD might be worth the compute cost,\nespecially when combined with RLHF.\n14\nPublished as a conference paper at ICLR 2024\nA.3\nXSUM\nLearning rate sweep. We performed a sweep over the learning rate in {0.0001, 0.0003, 0.001} and\nfound 0.0003 to be the best for T5-base, T5-large and 0.001 leads to best performance for T5-small.\nAs such, by default we use an LR of 0.0003 except when reporting results for T5-small, which uses\n0.001. We found that reverse KL was more sensitive to higher LRs and we default to using 0.0003\nfor all models when using reverse KL.\nTeacher Softmax-temperature. When using greedy sampling for evaluation, we set the teacher\ntemperature to 1. However, when reporting student performance with temperature sampling (\u03b3 = 1),\nas done in Figures 2 and 3, we set teacher temperature to 0.1 for the student.\nTable A.1: Hyperparameter Details for experiments on XSum.\nHyperparameter\nValue\nTraining Steps\n40,000\nBatch size\n32\nEval Split\nValidation\nDropout\n0.0\nLearning Rate (LR)\n0.0003\nLR Warmup Steps\n2,000\nLR Cooldown (Begin, End)\n(30,000, 40,000)\nWarmup Schedule\nLinear (from 0 to LR)\nCooldown Schedule\nLinear (from LR to 0)\nMax Input Length\n1024\nMax Output Length\n64\nEvaluation\nGreedy & Temp. Sampling\nGKD Ablations. We ablated different divergences and student data fractions in GKD for various\nstudent sizes in Figure A.12 and A.13. On-policy and mixed variants consistently outperform super-\nvised variants. Mode-seeking divergences perform better when evaluation is done using temperature\nsampling while the choice of divergence doesn\u2019t affect performance much with greedy sampling.\nA.4\nGSM8K\nFor training, we use the CoT outputs generated from Palm-540B by Magister et al. (2022). We report\naccuracy on the original test split of the GSM8K dataset (Cobbe et al., 2021). We use checkpoints at\nthe end of training after distillation for reporting results, which are averaged across 3 seeds.\nTable A.2: Hyperparameter Details for experiments on GSM8K.\nHyperparameter\nValue\nTraining Steps\n40,000\nBatch size\n32\nEvaluation Split\nTest\nDropout\n0.05\nLearning Rate (LR)\n0.0003\nLR Warmup Steps\n2,000\nLR Cooldown (Begin, End)\n(30,000, 40,000)\nWarmup Schedule\nLinear (from 0 to LR)\nCooldown Schedule\nLinear (from LR to 0)\nMax Input Length\n512\nMax Output Length\n320\nCheckpoint\nFlan-T5\nTeacher softmax temperature\n0.1\nEvaluation\nGreedy Sampling\nFew-shot CoT Prompt. Here, we specify the 4-shot CoT prompt used for the experiments:\n15\nPublished as a conference paper at ICLR 2024\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n13.2\n15.1\n15.5\n15.2\n14.5\n13.1\n15\n15.5\n15.2\n14.5\n13.1\n14.3\n14.9\n15\n14.4\nROUGE-2: T5-XL \n T5-small\n13.5\n14.0\n14.5\n15.0\n15.5\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n16.5\n17.5\n18\n18\n18.2\n16.4\n17.5\n17.8\n17.9\n17.9\n16.7\n17.1\n17.3\n17.4\n17.6\nROUGE-2: T5-XL \n T5-base\n16.6\n16.8\n17.0\n17.2\n17.4\n17.6\n17.8\n18.0\n18.2\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n19.2\n19.9\n20.1\n20.3\n19.8\n19.3\n19.9\n20\n19.8\n19.3\n18.6\n19.1\n19.3\n19.3\n19.1\nROUGE-2: T5-XL \n T5-large\n18.8\n19.0\n19.2\n19.4\n19.6\n19.8\n20.0\n20.2\nFigure A.12: Ablating GKD on XSum with evaluation via temperature sampling (\u03b3 = 1). We distill\nsupervised fine-tuned T5-XL model to different sized student T5 models. Here, we evaluate using temperature\nsampling and teacher temperature to 0.1 during training. In the plots above, we report the ROUGE-2 score of the\nstudent post distillation. On-policy GKD approaches with reverse KL and JSD (0.9) performs the best, while\nforward KL performs poorly.\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n16.4\n16.6\n16.6\n16.3\n15.6\n16.3\n16.4\n16.2\n16.2\n15.2\n15.2\n15.4\n15.5\n15.6\n15.1\nROUGE-2: T5-XL \n T5-small\n15.2\n15.4\n15.6\n15.8\n16.0\n16.2\n16.4\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n18.6\n18.7\n18.7\n18.8\n18.5\n18.4\n18.5\n18.5\n18.5\n18.2\n17.9\n18.1\n18.1\n18\n18\nROUGE-2: T5-XL \n T5-base\n18.0\n18.2\n18.4\n18.6\n18.8\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n21.2\n21.2\n21.1\n21.2\n21.1\n20.9\n21\n21\n20.8\n20.6\n20.6\n20.5\n20.3\n20.5\n20.2\nROUGE-2: T5-XL \n T5-large\n20.2\n20.4\n20.6\n20.8\n21.0\n21.2\nFigure A.13: Ablating GKD on XSum with evaluation via greedy sampling. We distill supervised fine-tuned\nT5-XL model to different sized student T5 models. Here, we evaluate using greedy sampling and set student and\nteacher temperature to 1 during training. When evaluated using greedy sampling, the teacher model obtains a\nROUGE-2 score of 22 while the student T5-small, base and large models obtain score of 13.4, 17.9, and 19.6\nrespectively. In the plots above, we report the ROUGE-2 score of the student post distillation. On-policy GKD\napproaches performs the best, with small differences among different divergences. Furthermore, on-policy and\nmixed variants strongly outperform supervised variants.\nQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are\ndone, there will be 21 trees. How many trees did the grove workers plant today?\n16\nPublished as a conference paper at ICLR 2024\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n6.6\n4.7\n3.8\n4.9\n5.5\n4.2\n3.8\n3.6\n3.2\n4.5\n2.5\n3.1\n2.9\n2.9\n3.8\nAccuracy Improvement: T5-XL \n Small\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\nForward KL\nJSD (0.1)\nJSD (0.5)\nJSD (0.9)\nReverse KL\nDivergence (\n)\n100%\n50%\n0%\nStudent Data Fraction ( )\n8.7\n6.8\n5.7\n6.5\n8.7\n5.7\n5.3\n4.8\n6.5\n4.3\n5\n3.9\n3.9\n3.7\n4.5\nAccuracy Improvement: T5-XL \n Large\n4\n5\n6\n7\n8\nFigure A.14: Ablating GKD variants on GSM8K with 4-shot CoT. For evaluation, we use greedy sampling\nand report improvement in test accuracy of the student after distillation. Results are averaged across three seeds.\nUsing only student-generated output samples typically outperform other GKD variants. We use the supervised\nfine-tuned T5-XL as the teacher, which obtains an accuracy of 27.9. (Left) We use T5-small as the student,\nwhich obtains an accuracy of 4.585. (Right) Student corresponds to T5-base with an accuracy of 20.5.\nA: There are 15 trees originally. Then there were 21 trees after some more were planted. So there\nmust have been 21 - 15 = 6. The answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?\nA: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.\nQ: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in\ntotal?\nA: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After\neating 35, they had 74 - 35 = 39. The answer is 39.\nQ: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many\nlollipops did Jason give to Denny?\nA: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20\n- 12 = 8. The answer is 8.\nA.5\nWMT\nFor evaluation, we use beam search with same hyperparameters as Raffel et al. (2020). We report\nperformance of the final checkpoints obtained after training. To reduce the variance in results, we\nreport the results averaged across 3 seeds.\nf-distill\nImitKD\nGKD\n0.0\n0.2\n0.4\n0.6\n0.8\nBLEU Improvement\nWMT en-de: T5-XL \n Small\nf-distill\nImitKD\nGKD\n0.0\n0.2\n0.4\n0.6\nBLEU Improvement\nWMT en-de: T5-XL \n Base\nFigure A.15: Comparing GKD to ImitKD and f-distill on WMT. Here, GKD corresponds to best performing\nvariant on WMT, with \u03bb = 1 (on-policy) and JSD (0.1). On-policy GKD leads to 53% higher BLEU improvement\nover ImitKD and 162% over f-distill, averaged across small and base models.\n17\nPublished as a conference paper at ICLR 2024\nTable A.3: Hyperparameter Details for WMT en-de experiments.\nHyperparameter\nValue\nTraining Steps\n100,000\nBatch size\n32\nSeqio Task Name\n\u2019wmt t2t ende v003\u2019\nEval Split\nValidation\nDropout\n0.0\nWarmup Steps\n5,000\nWarmup Schedule\nLinear (from 0 to LR)\nLearning Rate (LR)\n0.0003\nInput Length (Tokenized)\n80\nOutput Length (Tokenized)\n80\nTeacher softmax temperature\n1.0\nEvaluation\nBeam Search\nA.6\nINSTRUCTION TUNING\nTable A.4: Hyperparameter Details for FLAN Instruction Tuning.\nHyperparameter\nValue\nTraining Steps\n50,000\nBatch size\n128\nTask\nFLAN2021\nDropout\n0.0\nWarmup Schedule\nNo Warmup\nLearning Rate (LR)\n0.0001\nInput Length (Tokenized)\n2048\nOutput Length (Tokenized)\n256\nTeacher softmax temperature\n1.0\nEvaluation\nGreedy Sampling\nA.7\nMODE-SEEKING VS MODE-COVERING KL\nP\nargminQ KL(P||Q)\nargminQ KL(Q||P)\nFigure A.16: Mode-seeking vs Model-covering KL with capacity mismatch. We show the learned distribution\nQ\u03b8 when minimizing the forward and reverse KL w.r.t Q between a mixture distribution P and a unimodal\nGaussian Q\u03b8. Reverse KL is mode-seeking as it forces Q\u03b8 to be zero where P is zero and hence makes it\nconcentrate on one of the modes (last plot). However, forward KL is mode-covering as it ensures that there is\nsome mass under Q\u03b8 wherever there is some mass under P. See Le (2017) to replicate this plot.\n18\n"
  },
  {
    "title": "OpenMask3D: Open-Vocabulary 3D Instance Segmentation",
    "link": "https://arxiv.org/pdf/2306.13631.pdf",
    "upvote": "7",
    "text": "OpenMask3D:\nOpen-Vocabulary 3D Instance Segmentation\nAy\u00e7a Takmaz1\u2217\nElisabetta Fedele1\u2217\nRobert W. Sumner1\nMarc Pollefeys1,2\nFederico Tombari3\nFrancis Engelmann1,3\n1ETH Zurich\n2Microsoft\n3Google\nopenmask3d.github.io\n\u201cfootrest\u201d\n\u201cside table with a flower vase on it\"\n\u201cangel\u201d\nFigure 1: Open-Vocabulary 3D Instance Segmentation. Given a 3D scene (top) and free-form user queries\n(bottom), our OpenMask3D segments object instances and scene parts described by the open-vocabulary queries.\nAbstract\nWe introduce the task of open-vocabulary 3D instance segmentation. Current\napproaches for 3D instance segmentation can typically only recognize object cat-\negories from a pre-defined closed set of classes that are annotated in the training\ndatasets. This results in important limitations for real-world applications where one\nmight need to perform tasks guided by novel, open-vocabulary queries related to a\nwide variety of objects. Recently, open-vocabulary 3D scene understanding meth-\nods have emerged to address this problem by learning queryable features for each\npoint in the scene. While such a representation can be directly employed to perform\nsemantic segmentation, existing methods cannot separate multiple object instances.\nIn this work, we address this limitation, and propose OpenMask3D, which is a\nzero-shot approach for open-vocabulary 3D instance segmentation. Guided by\npredicted class-agnostic 3D instance masks, our model aggregates per-mask fea-\ntures via multi-view fusion of CLIP-based image embeddings. Experiments and\nablation studies on ScanNet200 and Replica show that OpenMask3D outperforms\nother open-vocabulary methods, especially on the long-tail distribution. Qualitative\nexperiments further showcase OpenMask3D\u2019s ability to segment object properties\nbased on free-form queries describing geometry, affordances, and materials.\n1\nIntroduction\n3D instance segmentation, which is the task of predicting 3D object instance masks along with\ntheir object categories, has many crucial applications in fields such as robotics and augmented\nreality. Due to its significance, 3D instance segmentation has been receiving a growing amount of\nattention in recent years. Despite the remarkable progress made in 3D instance segmentation methods\n[8, 9, 39, 58, 77, 78], it is noteworthy that these methods operate under a closed-set paradigm, where\nthe set of object categories is limited and closely tied to the datasets used during training.\nWe argue that there are two key problems with closed-vocabulary 3D instance segmentation. First,\nthese approaches are limited in their ability to understand a scene beyond the object categories\nseen during training. Despite the significant success of 3D instance segmentation approaches from\nrecent years, these closed-vocabulary approaches may fail to recognize and correctly classify novel\nobjects. One of the main advantages of open-vocabulary approaches, is their ability to zero-shot learn\n\u2217 Equal contribution.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.13631v2  [cs.CV]  29 Oct 2023\ncategories that are not present at all in the training set. This ability has potential benefits for many\napplications in fields such as robotics, augmented reality, scene understanding and 3D visual search.\nFor example, it is essential for an autonomous robot to be able to navigate in an unknown environment\nwhere novel objects can be present. Furthermore, the robot may need to perform an action based\non a free-form query, such as \u201cfind the side table with a flower vase on it\u201d, which is challenging to\nperform with the existing closed-vocabulary 3D instance segmentation methods. Hence, the second\nkey problem with closed-vocabulary approaches is their inherent limitation to recognize only object\nclasses that are predefined at training time.\nIn an attempt to address and overcome the limitations of a closed-vocabulary setting, there has\nbeen a growing interest in open-vocabulary approaches. A line of work [20, 43, 45] investigates\nopen-vocabulary 2D image segmentation task. These approaches are driven by advances in large-scale\nmodel training, and largely rely on recent foundation models such as CLIP [55] and ALIGN [33] to\nobtain text-image embeddings. Motivated by the success of these 2D open-vocabulary approaches,\nanother line of work has started exploring 3D open-vocabulary scene understanding task [24, 32, 52],\nbased on the idea of lifting image features from models such as CLIP [55], LSeg [43], and OpenSeg\n[20] to 3D. These approaches aim to obtain a task-agnostic feature representation for each 3D point\nin the scene, which can be used to query concepts with open-vocabulary descriptions such as object\nsemantics, affordances or material properties. Their output is typically a heatmap over the points in\nthe scene, which has limited applications in certain aspects, such as handling object instances.\nIn this work, we propose OpenMask3D, an open-vocabulary 3D instance segmentation method which\nhas the ability to reason beyond a predefined set of concepts. Given an RGB-D sequence, and the\ncorresponding 3D reconstructed geometry, OpenMask3D predicts 3D object instance masks, and\ncomputes a mask-feature representation. Our two-stage pipeline consists of a class-agnostic mask\nproposal head, and a mask-feature aggregation module. Guided by the predicted class-agnostic 3D\ninstance masks, our mask-feature aggregation module first finds the frames in which the instances are\nhighly visible. Then, it extracts CLIP features from the best images of each object mask, in a multi-\nscale and crop-based manner. These features are then aggregated across multiple views to obtain a\nfeature representation associated with each 3D instance mask. Our approach is intrinsically different\nfrom the existing 3D open-vocabulary scene understanding approaches [24, 32, 52] as we propose an\ninstance-based feature computation approach instead of a point-based one. Computing a mask-feature\nper object instance enables us to retrieve object instance masks based on their similarity to any given\nquery, equipping our approach with open-vocabulary 3D instance segmentation capabilities. As\nfeature computation is performed in a zero-shot manner, OpenMask3D is capable of preserving\ninformation about novel objects as well as long-tail objects better, compared to trained or fine-\ntuned counterparts. Furthermore, OpenMask3D goes beyond the limitations of a closed-vocabulary\nparadigm, and allows segmentation of object instances based on free-form queries describing object\nproperties such as semantics, geometry, affordances, and material properties.\nOur contributions are three-fold:\n\u2022 We introduce the open-vocabulary 3D instance segmentation task in which the object\ninstances that are similar to a given text-query are identified.\n\u2022 We propose OpenMask3D, which is the first approach that performs open-vocabulary 3D\ninstance segmentation in a zero-shot manner.\n\u2022 We conduct experiments to provide insights about design choices that are important for\ndeveloping an open-vocabulary 3D instance segmentation model.\n2\nRelated work\nClosed-vocabulary 3D semantic and instance segmentation. Given a 3D scene as input, 3D\nsemantic segmentation task aims to assign a semantic category to each point in the scene [2\u20134, 9,\n14, 15, 22, 28, 29, 31, 38, 42, 44, 46, 47, 53, 54, 63, 64, 66, 68, 70, 73]. 3D instance segmentation\ngoes further by distinguishing multiple objects belonging to the same semantic category, predicting\nindividual masks for each object instance [13, 16, 25, 27, 34, 41, 58, 62, 65, 67, 74]. The current\nstate-of-the-art approach on the ScanNet200 benchmark [10, 57] is Mask3D [58], which leverages a\ntransformer architecture to generate 3D mask proposals together with their semantic labels. However,\nsimilar to existing methods, it assumes a limited set of semantic categories that can be assigned to\nan object instance. In particular, the number of labels is dictated by the annotations provided in the\ntraining datasets, 200 \u2013 in the case of ScanNet200 [57]. Given that the English language encompasses\n2\nnumerous nouns, in the order of several hundred thousand [69], it is clear that existing closed-\nvocabulary approaches have an important limitation in handling object categories and descriptions.\nFoundation models. Recent multimodal foundation models [1, 7, 21, 33, 55, 75, 76] leverage large-\nscale pretraining to learn image representations guided by natural language descriptions. These models\nenable zero-shot transfer to various downstream tasks such as object recognition and classification.\nDriven by the progress in large-scale model pre-training, similar foundation models for images were\nalso explored in another line of work [5, 51], which aim to extract class-agnostic features from images.\nRecently, steps towards a foundation model for image segmentation were taken with SAM [36]. SAM\nhas the ability to generate a class-agnostic 2D mask for an object instance, given a set of points\nthat belong to that instance. This capability is valuable for our approach, especially for recovering\nhigh-quality 2D masks from projected 3D instance masks, as further explained in Sec. 3.2.2.\nOpen vocabulary 2D segmentation. As large vision-language models gained popularity for image\nclassification, numerous new approaches [11, 20, 23, 26, 32, 40, 43, 45, 48, 56, 71, 72, 79, 80] have\nemerged to tackle open-vocabulary or zero-shot image semantic segmentation. One notable shift\nwas the transition from image-level embeddings to pixel-level embeddings, equipping models with\nlocalization capabilities alongside classification. However, methods with pixel-level embeddings,\nsuch as OpenSeg [20] and OV-Seg [45], strongly rely on the accuracy of 2D segmentation masks, and\nrequire a certain degree of training. In our work, we rely on CLIP [55] features without performing\nfinetuning or any additional training, and compute 2D masks using the predicted 3D instance masks.\nOpen-vocabulary 3D scene understanding. Recent success of 2D open-vocabulary segmentation\nmodels such as LSeg [43], OpenSeg [20], and OV-Seg[45] has motivated researchers in the field of\n3D scene understanding to explore the open vocabulary setting [6, 12, 19, 24, 30, 32, 35, 37, 49, 52,\n59, 60]. OpenScene [52] uses per-pixel image features extracted from posed images of a scene and\nobtains a point-wise task-agnostic scene representation. On the other hand, approaches such as LERF\n[35] and DFF [37] leverage the interpolation capabilities of NeRFs [50] to extract a semantic field of\nthe scene. However, it is important to note that all of these approaches have a limited understanding\nof object instances and inherently face challenges when dealing with instance-related tasks.\n3\nMethod\nOverview. Our OpenMask3D model is illustrated in Fig. 2. Given a set of posed RGB-D images\ncaptured in a scene, along with the reconstructed scene point cloud\n1\u20dd, OpenMask3D predicts\n3D instance masks with their associated per-mask feature representations, which can be used for\nquerying instances based on open-vocabulary concepts\n4\u20dd. Our OpenMask3D has two main building\nblocks: a class agnostic mask proposal head\n2\u20dd and a mask-feature computation module\n3\u20dd. The\nclass-agnostic mask proposal head predicts binary instance masks over the points in the point cloud.\nThe mask-feature computation module leverages pre-trained CLIP [55] vision-language model in\norder to compute meaningful and flexible features for each mask. For each proposed instance mask,\nthe mask-feature computation module first selects the views in which the 3D object instance is highly\nvisible. Subsequently, in each selected view, the module computes a 2D segmentation mask guided\nby the projection of the 3D instance mask, and refines the 2D mask using the SAM [36] model. Next,\nthe CLIP encoder is employed to obtain image-embeddings of multi-scale image-crops bounding the\ncomputed 2D masks. These image-level embeddings are then aggregated across the selected frames\nin order to obtain a mask-feature representation. Sec. 3.1 describes the class agnostic mask proposal\nhead, and Sec. 3.2 describes the mask-feature computation module.\nThe key novelty of our method is that it follows an instance-mask oriented approach, contrary to\nexisting 3D open-vocabulary scene understanding models which typically compute per-point features.\nThese point-feature oriented models have inherent limitations, particularly for identifying object\ninstances. Our model aims to overcome such limitations by introducing a framework that employs\nclass agnostic instance masks and aggregates informative features for each object instance.\nInput. Our pipeline takes as input a collection of posed RGB-D images captured in an indoor scene,\nand the reconstructed point cloud representation of the scene. We assume known camera parameters.\n3.1\nClass agnostic mask proposals\nThe first step of our approach involves generating M class-agnostic 3D mask proposals\nm3D\n1 ,...,m3D\nM . Let P \u2208 RP \u00d73 denote the point cloud of the scene, where each 3D point is\nrepresented with its corresponding 3D coordinates. Each 3D mask proposal is represented by a\n3\nFigure 2: An overview of our approach. We propose OpenMask3D, the first open-vocabulary 3D instance\nsegmentation model. Our pipeline consists of four subsequent steps:\n1\u20dd Our approach takes as input posed\nRGB-D images of a 3D indoor scene along with its reconstructed point cloud.\n2\u20dd Using the point cloud, we\ncompute class-agnostic instance mask proposals.\n3\u20dd Then, for each mask, we compute a feature representation.\n4\u20dd Finally, we obtain an open-vocabulary 3D instance segmentation representation, which can be used to retrieve\nobjects related to queried concepts embedded in the CLIP [55] space.\nbinary mask m3D\ni\n= (m3D\ni1 ,...,m3D\niP ) where m3D\nij \u2208 {0,1} indicates whether the j-th point belongs\nto i-th object instance. To generate these masks, we leverage the transformer-based mask-module\nof a pre-trained 3D instance segmentation model [58], which is frozen during our computations.\nThe architecture consists of a sparse convolutional backbone based on the MinkowskiUNet [9], and\na transformer decoder. Point features obtained from the feature backbone are passed through the\ntransformer decoder, which iteratively refines the instance queries, and predicts an instance heatmap\nfor each query. In the original setup, [58] produces two outputs: a set of M binary instance masks\nobtained from the predicted heatmaps, along with predicted class labels (from a predefined closed set)\nfor each mask. In our approach, we adapt the model to exclusively utilize the binary instance masks,\ndiscarding the predicted class labels and confidence scores entirely. These class-agnostic binary\ninstance masks are then utilized in our mask-feature computation module, in order to go beyond\nsemantic class predictions limited to a closed-vocabulary, and obtain open-vocabulary representations\ninstead. Further details about the class-agnostic mask proposal module are provided in Appendix A.1.\n3.2\nMask-feature computation module\nMask-feature computation module aims to compute a task-agnostic feature representation for each\npredicted instance mask obtained from the class-agnostic mask proposal module. The purpose of this\nmodule is to compute a feature representation that can be used to query open-vocabulary concepts.\nAs we intend to utilize the CLIP text-image embedding space and maximally retain information about\nlong-tail or novel concepts, we solely rely on the CLIP visual encoder to extract image-features on\nwhich we build our mask-features.\nAs illustrated in Fig. 3, the mask-feature computation module consists of several steps. For each\ninstance mask-proposal, we first compute the visibility of the object instance in each frame of the\nRGB-D sequence, and select the top-k views with maximal visibility. In the next step, we compute a\n2D object mask in each selected frame, which is then used to obtain multi-scale image-crops in order\nto extract effective CLIP features. The image-crops are then passed through the CLIP visual encoder\nto obtain feature vectors that are average-pooled over each crop and each selected view, resulting in\nthe final mask-feature representation. In Sec. 3.2.1, we describe how we select a set of top-k frames\nfor each instance. In Sec. 3.2.2, we describe how we crop the frames, based on the object instance we\nwant to embed. In Sec. 3.2.3, we describe how we compute the final mask-features per object.\n3.2.1\nFrame selection\nObtaining representative images of the proposed object instances is crucial for extracting accurate\nCLIP features. To achieve this, we devise a strategy to select, for each of the M predicted instances,\na subset of representative frames (Fig. 3,\na\u20dd) from which we extract CLIP features. In particular, our\ndevised strategy selects frames based on their visibility scores sij for each mask i in each view j.\nHere, we explain how we compute these visibility scores.\n4\nFigure 3: Mask-Feature Computation Module. For each instance mask, a\u20dd we first compute the visibility\nof the instance in each frame, and select top-k views with maximal visibility. In b\u20dd, we compute a 2D object\nmask in each selected frame, which is used to obtain multi-scale image-crops in order to extract effective CLIP\nfeatures.\nc\u20dd The image-crops are then passed through the CLIP visual encoder to obtain feature vectors that are\naverage-pooled over each crop and d\u20dd each selected view, resulting in the final mask-feature representation.\nGiven a mask proposal m3D\ni\ncorresponding to the i-th instance, we compute the visibility score\nsij \u2208 [0,1] for the j-th frame using the following formula:\nsij =\nvis(i,j)\nmaxj\u2032 (vis(i,j\u2032))\nHere, vis(i,j) represents the number of points from mask i that are visible in frame j. Note that we\nassume that each mask is visible in at least one frame. We compute vis(i,j) using the following\napproach. First, for each 3D point from the i-th instance mask, we determine whether the point\nappears in the camera\u2019s field of view (FOV) in the j-th frame. To achieve this, we use intrinsic (I) and\nextrinsic (R\u2223t) matrices of the camera from that frame to project each 3D point in the point cloud to the\nimage plane. We obtain corresponding 2D homogeneous coordinates p2D = (u,v,w)\u22ba = I\u22c5(R\u2223t)\u22c5x,\nwhere x = (x,y,z,1)\u22ba is a point represented in homogeneous coordinates. We consider a point\nto be in the camera\u2019s FOV in the j-th frame if w \u2260 0, and the u\nw value falls within the interval\n[0,W \u2212 1] while v\nw falls within [0,H \u2212 1]. Here W and H represent the width and height of the\nimage, respectively. Next, it is important to note that 3D points that are in the camera\u2019s FOV are not\nnecessarily visible from a given camera pose, as they might be occluded by other parts of the scene.\nHence, we need to check whether the points are in fact visible. In order to examine whether a point is\noccluded from a given camera viewpoint, we compare the depth value of the 3D point deriving from\nthe geometric computation (i.e. w) and the measured depth, i.e. depth value of the projected point in\nthe depth image. We consider points that satisfy the inequality w \u2212 d > kthreshold as occluded, where\nkthreshold is a hyper-parameter of our method. If the projection of a point from mask i is not occluded\nin the j\u2212th view, it contributes to the visible point count, vis(i,j). Once we compute mask-visibility\nscores for each frame, we select the top kview views with the highest scores sij for each mask i. Here,\nkview represents another hyperparameter.\n3.2.2\n2D mask computation and multi-scale crops\nIn this section, we explain our approach for computing CLIP features based on the frames selected in\nthe previous step. Given a selected frame for a mask, our goal is to find the optimal image crops from\nwhich to extract features, as shown in (Fig. 3,\nb\u20dd). Simply considering all of the projected points of\nthe mask often results in imprecise and noisy bounding boxes, largely affected by the outliers (please\nsee Appendix A.2.2). To address this, we use a class-agnostic 2D segmentation model, SAM [36],\nwhich predicts a 2D mask conditioned on a set of input points, along with a mask confidence score.\nSAM is sensitive to the set of input points (see Appendix A.2.3, A.2.4). Therefore, to obtain a\nhigh quality mask from SAM with a high confidence score, we take inspiration from the RANSAC\nalgorithm [18] and proceed as described in Algorithm 1. We sample ksample points from the projected\npoints, and run SAM using these points. The output of SAM at a given iteration r is a 2D mask\n(m2D\nr\n) and a mask confidence score (scorer). This process is repeated for krounds iterations, and\nthe 2D mask with the highest confidence score is selected.\nNext, we use the resulting mask m2D\n\u2217\nto generate L = 3 multi-level crops of the selected image. This\nallows us to enrich the features by incorporating more contextual information from the surroundings.\n5\nAlgorithm 1 - 2D mask selection algorithm\nscore\u2217 \u2190 0, m2D\n\u2217\n\u2190 0, r \u2190 0\nwhile r < krounds do\nSample ksample points among the projected points at random\nCompute the mask m2D\nr\nand the score scorer based on the sampled points using SAM\nif scorer > score\u2217 then\nscore\u2217 \u2190 scorer, m2D\n\u2217\n\u2190 m2D\nr\nend if\nr \u2190 r + 1\nend while\nSpecifically, the first bounding box b1 = (x1\n1,y1\n1,x1\n2,y1\n2) with 0 \u2264 x1\n1 < x1\n2 < W and 0 \u2264 y1\n1 < y1\n2 < H\nis a tight bounding box derived from the 2D mask. The other bounding boxes b2 and b3 are\nincrementally larger. In particular, the 2D coordinates of bi for i = 2,3 are obtained as follows.\nxl\n1 = max(0,x1\n1 \u2212 (x1\n2 \u2212 x1\n1) \u22c5 kexp \u22c5 l)\nyl\n1 = max(0,y1\n1 \u2212 (y1\n2 \u2212 y1\n1) \u22c5 kexp \u22c5 l)\nxl\n2 = min(x1\n2 + (x1\n2 \u2212 x1\n1) \u22c5 kexp \u22c5 l,W \u2212 1)\nyl\n2 = min(y1\n2 + (y1\n2 \u2212 y1\n1) \u22c5 kexp \u22c5 l,H \u2212 1)\nNote that here l represents the level of the features and kexp = 0.1 is a predefined constant.\n3.2.3\nCLIP feature extraction and mask-feature aggregation\nFor each instance mask, we collect k \u22c5 L images by selecting top-k views and obtaining L multi-level\ncrops as described in Sec. 3.2.1 and Sec. 3.2.2. Collected image crops are then passed through the\nCLIP visual encoder in order to extract image features in the CLIP embedding space, as shown in\n(Fig. 3,\nc\u20dd). We then aggregate the features obtained from each crop that correspond to a given\ninstance mask in order to get an average per-mask CLIP feature (Fig. 3,\nd\u20dd). The computed features\nare task-agnostic, and can be used for various instance-based tasks by encoding a given text or\nimage-based query, using the same CLIP model we employed to encode the image crops.\n4\nExperiments\nIn this section, we present quantitative and qualitative results from our method OpenMask3D. In\nSec 4.1, we quantitatively evaluate our method, and compare OpenMask3D with supervised 3D\ninstance segmentation approaches as well as existing open-vocabulary 3D scene understanding\nmodels we adapted for the 3D instance segmentation task. Furthermore, we provide an ablation\nstudy for OpenMask3D. In Sec. 4.2, we share qualitative results for open-vocabulary 3D instance\nsegmentation, demonstrating potential applications. Additional results are provided in the Appendix.\n4.1\nQuantitative results: closed-vocabulary 3D instance segmentation evaluation\nWe evaluate our approach on the closed-vocabulary 3D instance segmentation task. We conduct\nadditional experiments to assess the generalization capability of OpenMask3D.\n4.1.1\nExperimental setting\nData. We conduct our experiments using the ScanNet200 [57] and Replica [61] datasets. We report\nour ScanNet200 results on the validation set consisting of 312 scenes, and evaluate for the 3D instance\nsegmentation task using the closed vocabulary of 200 categories from the ScanNet200 annotations.\nRozenberszki et al. [57] also provide a grouping of ScanNet200 categories based on the frequency of\nthe number of labeled surface points in the training set, resulting in 3 subsets: head (66 categories),\ncommon (68 categories), tail (66 categories). This grouping enables us to evaluate the performance of\nour method on the long-tail distribution, making ScanNet200 a natural choice as an evaluation dataset.\nTo assess the generalization capability of our method, we further experiment with the Replica [61]\ndataset, and evaluate on the office0, office1, office2, office3, office4, room0, room1, room2 scenes.\nMetrics. We employ a commonly used 3D instance segmentation metric, average precision (AP).\nAP scores are evaluated at mask overlap thresholds of 50% and 25%, and averaged over the overlap\n6\nModel\nImage Features\nAP\nAP50\nAP25\nhead (AP)\ncommon (AP)\ntail (AP)\nClosed-vocabulary, fully supervised\nMask3D [58]\n-\n26.9\n36.2\n41.4\n39.8\n21.7\n17.9\nOpen-vocabulary\nOpenScene [52] (2D Fusion) + masks\nOpenSeg [20]\n11.7\n15.2\n17.8\n13.4\n11.6\n9.9\nOpenScene [52] (3D Distill) + masks\nOpenSeg [20]\n4.8\n6.2\n7.2\n10.6\n2.6\n0.7\nOpenScene [52] (2D/3D Ens.) + masks\nOpenSeg [20]\n5.3\n6.7\n8.1\n11.0\n3.2\n1.1\nOpenScene [52] (2D Fusion) + masks\nLSeg [43]\n6.0\n7.7\n8.5\n14.5\n2.5\n1.1\nOpenMask3D (Ours)\nCLIP [55]\n15.4\n19.9\n23.1\n17.1\n14.1\n14.9\nTable 1: 3D instance segmentation results on the ScanNet200 validation set. Metrics are respectively: AP\naveraged over an overlap range, and AP evaluated at 50% and 25% overlaps. We also report AP scores for\nhead, common, tail subsets of ScanNet200. Mask3D [58] is fully-supervised, while OpenScene [52] is built\nupon on 2D models (LSeg [43], OpenSeg [20]) trained on labeled datasets for 2D semantic segmentation. Since\nOpenScene [52] does not provide instance masks, we aggregate its per-point features using class-agnostic masks.\nOpenMask3D outperforms other open-vocabulary counterparts, particularly on the long-tail classes.\nrange of [0.5 \u2236 0.95 \u2236 0.05] following the evaluation scheme from ScanNet [10]. Computation of\nthe metrics requires each mask to be assigned a prediction confidence score. We assign a prediction\nconfidence score of 1.0 for each predicted mask in our experiments.\nOpenMask3D implementation details. We use posed RGB-depth pairs for both the ScanNet200\nand Replica datasets, and we process 1 frame in every 10 frames in the RGB-D sequences. In order\nto compute image features on the mask-crops, we use CLIP [55] visual encoder from the ViT-L/14\nmodel pre-trained at a 336 pixel resolution, which has a feature dimensionality of 768. For the\nvisibility score computation, we use kthreshold = 0.2, and for top-view selection we use kview = 5.\nIn all experiments with multi-scale crops, we use L = 3 levels. In the 2D mask selection algorithm\nbased on SAM [36], we repeat the process for krounds = 10 rounds, and sample ksample = 5 points\nat each iteration. For the class-agnostic mask proposals, we use the Mask3D [58] model trained on\nScanNet200 instances, and exclusively use the predicted binary instance masks in our pipeline. We do\nnot filter any instance mask proposals, and run DBSCAN [17] to obtain spatially contiguous clusters,\nbreaking down masks into smaller new masks when necessary. This process results in a varying\nnumber of mask proposals for each scene. For further implementation details about OpenMask3D,\nplease refer to Appendix A. Computation of the mask-features of a ScanNet scene on a single GPU\ntakes 5-10 minutes depending on the number of mask proposals and number of frames in the RGB-D\nsequence. Note that once the per-mask features of the scene are computed, objects can be queried in\nreal-time (\u223c 1-2 ms) with arbitrary open-vocabulary queries.\nMethods in comparison. We compare with Mask3D [58], which is the current state-of-the-art on the\nScanNet200 3D instance segmentation benchmark. We also compare against recent open-vocabulary\n3D scene understanding model variants (2D Fusion, 3D Distill, 2D/3D Ensemble) from OpenScene\n[52]. Note that since OpenScene only generates a per-point feature vector (without any per-instance\naggregation), it is not possible to directly compare it with our method. To address this, we extended\nOpenScene by averaging its per-point features within each instance mask generated by our mask\nmodule (see Appendix C for more details).\nClass assignment. Our open-vocabulary approach does not directly predict semantic category labels\nper each instance mask, but it instead computes a task-agnostic feature vector for each instance,\nwhich can be used for performing a semantic label assignment. In order to evaluate our model on\nthe closed-vocabulary 3D instance segmentation task, we need to assign each object instance to a\nsemantic category. Similar to OpenScene [52], we compute cosine similarity between mask-features\nand the text embedding of a given query in order to perform class assignments. Following Peng\net al. [52], we use prompts in the form of \u201ca {} in a scene\u201d, and compute text-embeddings using\nCLIP model ViT-L/14(336px) [55] for each semantic class in the ScanNet200 dataset. This way, we\ncompute a similarity score between each instance and each object category, and assign instances to\nthe category with the closest text embedding.\n4.1.2\nResults and analysis\n3D closed-vocabulary instance segmentation results. We quantitatively evaluate our approach\non the closed-vocabulary instance segmentation task on the ScanNet200 [10, 57] and Replica [61]\ndatasets. Closed-vocabulary instance segmentation results are provided in Tab. 1 and Tab. 2.\n7\nState-of-the-art fully supervised approach Mask3D [58] demonstrates superior performance compared\nto open-vocabulary counterparts. While this gap is more prominent for the head and common\ncategories, the difference is less apparent for the tail categories. This outcome is expected, as\nMask3D benefits from full-supervision using the closed-set of class labels from the ScanNet200\ndataset. Furthermore, as there are more training samples from the categories within the head\nand common subsets (please refer to [57] for the statistics), the fully-supervised approach is more\nfrequently exposed to these categories - resulting in a stronger performance. When we compare\nOpenMask3D with other open-vocabulary approaches, we observe that our method performs better on\n6 out of 6 metrics. Notably, OpenMask3D outperforms other open-vocabulary approaches especially\non the tail categories by a significant margin. Our instance-centric method OpenMask3D, which\nis specifically designed for the open-vocabulary 3D instance segmentation task, shows stronger\nperformance compared to other open-vocabulary approaches which adopt a point-centric feature\nrepresentation. Differences between the feature representations are also illustrated in Fig. 5.\nHow well does our method generalize? As the mask module is trained on a closed-set segmentation\ndataset, ScanNet200 [57], we aim to investigate how well our method generalizes beyond the\ncategories seen during training. Furthermore, we aim to analyze how well our method generalizes to\nout-of-distribution (OOD) data, such as scenes from another dataset, Replica [61]. To demonstrate\nthe generalization capability of our approach, we conducted a series of experiments. First, we analyze\nthe performance of our model when we use class-agnostic masks from a mask-predictor trained on\nthe 20 original ScanNet classes [10], and evaluate on the ScanNet200 dataset. To evaluate how well\nour model performs on \u201cunseen\u201d categories, we group the ScanNet200 labels into two subsets: base\nand novel classes. We identify ScanNet200 categories that are semantically similar to the original\nScanNet20 classes (e.g. chair and folded-chair, table and dining-table), resulting in 53 classes. We\ngroup all remaining object classes that are not similar to any class in ScanNet20 as \u201cnovel\u201d. In Tab. 3,\nwe report results on seen (\u201cbase\u201d) classes, unseen (\u201cnovel\u201d) classes, and all classes. Our experiments\nshow that the OpenMask3D variant using a mask proposal backbone trained on a smaller set of\nobject annotations from ScanNet20 can generalize to predict object masks from a significantly larger\nset of objects (ScanNet200), resulting in only a marginal decrease in the performance. In a second\nexperiment, we evaluate the performance of OpenMask3D on out-of-distribution data from Replica,\nusing a mask predictor trained on ScanNet200. The results from this experiment, as presented in\nTab. 2, indicate that OpenMask3D can indeed generalize to unseen categories as well as OOD data.\nOpenMask3D using a mask predictor module trained on a smaller set of objects seems to perform\nreasonably well in generalizing to various settings.\nModel\nAP\nAP50\nAP25\nOpenScene [52] (2D Fusion) + masks\n10.9\n15.6\n17.3\nOpenScene [52] (3D Distill) + masks\n8.2\n10.5\n12.6\nOpenScene [52] (2D/3D Ens.) + masks\n8.2\n10.4\n13.3\nOpenMask3D (Ours)\n13.1\n18.4\n24.2\nTable 2: 3D instance segmentation results on the Replica [61] dataset. To assess how well our model\ngeneralizes to other datasets, we use instance masks from the mask proposal module trained on ScanNet200, and\ntest it on Replica scenes. OpenMask3D outperforms other open-vocabulary counterparts on the Replica dataset.\nNovel Classes\nBase Classes\nAll Classes\nMethod\nMask Training\nAP\nAP50\nAP25\nAP\nAP50\nAP25\nAP\ntail (AP)\nOpenScene [52] (2D Fusion) + masks\nScanNet20\n7.6\n10.3\n12.3\n11.1\n15.0\n17.7\n8.5\n6.1\nOpenScene [52] (3D Distill) + masks\nScanNet20\n1.8\n2.3\n2.7\n10.1\n13.4\n15.4\n4.1\n0.4\nOpenScene [52] (2D/3D Ens.) + masks\nScanNet20\n2.4\n2.8\n3.3\n10.4\n13.7\n16.3\n4.6\n0.9\nOpenMask3D (Ours)\nScanNet20\n11.9\n15.2\n17.8\n14.3\n18.3\n21.2\n12.6\n11.5\nOpenMask3D (Ours)\nScanNet200\n15.0\n19.7\n23.1\n16.2\n20.6\n23.1\n15.4\n14.9\nTable 3: 3D instance segmentation results using masks from mask module trained on ScanNet20 annota-\ntions, evaluated on the ScanNet200 dataset [57]. We identify 53 classes (such as chair, folded chair, table,\ndining table ...) that are semantically close to the original ScanNet20 classes [10], and group them as \u201cBase\u201d.\nRemaining 147 classes are grouped as \u201cNovel\u201d. We also report results on the full set of labels, titled \u201cAll\u201d.\nAblation study. In Tab. 4, we analyze design choices for OpenMask3D, i.e., multi-scale cropping\nand 2D mask segmentation. 2D mask segmentation refers to whether we use SAM [36] for refining\n8\n2D Mask\nMulti-Scale\nAP\nAP50\nAP25\nhead(AP)\ncommon(AP)\ntail(AP)\n\u2717\n\u2717\n12.9\n16.6\n19.5\n15.1\n12.2\n11.3\n\u2717\n\u2713\n14.3\n18.4\n21.1\n16.1\n13.6\n12.9\n\u2713\n\u2717\n14.1\n18.3\n21.5\n16.0\n13.0\n13.4\n\u2713\n\u2713\n15.4\n19.9\n23.1\n17.1\n14.1\n14.9\nTable 4: OpenMask3D Ablation Study. 2D mask and multi-scale crop components. 2D mask refers to whether\nSAM [36] was employed for computing 2D masks. Results are reported on the ScanNet200 [10] validation set.\nModel\nOracle\nImg. Feat.\nAP\nhead(AP)\ncommon(AP)\ntail(AP)\nClosed-vocabulary, full sup.\nMask3D [58]\n\u2717\n\u2212\n26.9\n39.8\n21.7\n17.9\nMask3D [58]\n\u2713\n\u2212\n35.5\n55.2\n27.2\n22.2\nOpen-vocabulary\nOpenScene [52] (2D Fusion) + masks\n\u2713\nOpenSeg [20]\n22.9\n26.2\n22.0\n20.2\nOpenScene [52] (2D Fusion) + masks\n\u2713\nLSeg [43]\n11.8\n26.9\n5.2\n1.7\nOpenMask3D (Ours)\n\u2713\nCLIP [55]\n29.1\n31.1\n24.0\n32.9\nTable 5: 3D instance segmentation results on the ScanNet200 validation set, using oracle masks. We\nuse ground truth instance masks for computing the per-mask features. We also report results from the fully-\nsupervised close-vocabulary Mask3D (row 2) whose predicted masks we match to the oracle masks using\nHungarian matching, and assign the predicted labels to oracle masks. On the long-tail categories, OpenMask3D\noutperforms even the fully supervised Mask3D with oracle masks.\nthe 2D mask from which we obtain a 2D bounding box. When SAM is not used, we simply crop the\ntightest bounding box around the projected 3D points. The ablation study shows that both multi-scale\ncropping and segmenting 2D masks to obtain more accurate image crops for a given object instance\npositively affect the performance. Additional experiments analyzing the importance of the number of\nviews (k) used in top-k view selection are provided in Appendix B.1.\nHow well would our approach perform if we had perfect masks? Another analysis we conduct\nis related to the class-agnostic masks. As the quality of the masks plays a key role in our process,\nwe aim to quantify how much the performance could improve if we had \u201cperfect\u201d oracle masks. For\nthis purpose, we run OpenMask3D using ground truth instance masks from the ScanNet200 dataset\ninstead of using our predicted class-agnostic instance masks. In a similar fashion for the baselines,\nwe use the oracle masks to aggregate OpenScene per-point features to obtain per-mask features.\nWe first compare against the fully-supervised Mask3D performance (Tab. 5, row 1). In a second\nexperiment, we also supply oracle masks to Mask3D (Tab. 5, row 2) to ensure a fair comparison. For\nthis experiment, we perform Hungarian matching between the predicted masks and oracle masks\ndiscarding all class-losses, and we only match based on the masks. To each oracle mask, we assign\nthe predicted class label of the closest-matching mask from Mask3D. In Tab. 5, it is evident that\nthe quality of the masks plays an important role for our task. Remarkably, our feature computation\napproach that is not trained on any additional labeled data, when provided with oracle masks, even\nsurpasses the performance of the fully supervised method Mask3D (by +15.0% AP) and Mask3D\nwith oracle masks (by +10.7% AP) on the long-tail categories. Overall, this analysis indicates that our\napproach has indeed promising results which can be further improved by higher quality class-agnostic\nmasks, without requiring any additional training or finetuning with labeled data.\n4.2\nQualitative results\nIn Fig. 4, we share qualitative results from our approach for the open-vocabulary 3D instance\nsegmentation task. With its zero-shot learning capabilities, OpenMask3D is able to segment a\ngiven query object that might not be present in common segmentation datasets. Furthermore, object\nproperties such as colors, textures, situational context and affordances are successfully recognized by\nOpenMask3D. Additional qualitative results in this direction are provided in Appendix D.\nIn Fig. 5, we provide qualitative comparisons between our instance-based open-vocabulary represen-\ntation and the point-based representation provided by OpenScene [52]. Our method OpenMask3D\ncomputes the similarity between the query embedding and per-mask feature vectors for each object\ninstance, which results in crisp instance boundaries. This is particularly suitable for the use cases\nin which one needs to identify object instances. Additional analysis and further details about the\nvisualizations are provided in Appendix C.\n9\n\u201cpepsi\u201d\n\u201cpool\u201d\n\u201can armchair with floral print\u201d\n\u201ca bed without clothes on it\u201d\n\u201ccool down\u201d\n\u201ca green seat\u201d\nFigure 4: Qualitative results from OpenMask3D. Our open-vocabulary instance segmentation approach is\ncapable of handling different types of queries. Novel object classes as well as objects described by colors,\ntextures, situational context and affordances are successfully retrieved by OpenMask3D.\nQuery: \u201ca dollhouse\u201d\nOpenScene [52]\nOpenMask3D (Ours)\nFigure 5: Heatmaps showing the similarity between given text queries and open-vocabulary scene features.\nInput 3D scene and query (left), per-point similarity from OpenScene (middle) and per-mask similarity from\nOpenMask3D (right). Dark red means high similarity, and dark blue means low similarity with the query text.\n4.3\nLimitations.\nThe experiments conducted with oracle masks indicate that there is room for improvement in terms of\nthe quality of 3D mask proposals which will be addressed in future work. Further, since the per-mask\nfeatures originate from images, they can only encode scene context visible in the camera frustum,\nlacking a global understanding of the complete scene and spatial relationships between all scene\nelements. Finally, evaluation methodologies for systematically assessing open-vocabulary capabilities\nstill remain a challenge. Closed-vocabulary evaluations, while valuable for initial assessments, fall\nshort in revealing the true extent of open-vocabulary potentials of proposed models.\n5\nConclusion\nWe propose OpenMask3D, the first open-vocabulary 3D instance segmentation model that can identify\nobjects instances in a 3D scene, given arbitrary text queries. This is beyond the capabilities of existing\n3D semantic instance segmentation approaches, which are typically trained to predict categories from\na closed vocabulary. With OpenMask3D, we push the boundaries of 3D instance segmentation. Our\nmethod is capable of segmenting object instances in a given 3D scene, guided by open-vocabulary\nqueries describing object properties such as semantics, geometry, affordances, material properties\nand situational context. Thanks to its zero-shot learning capabilities, OpenMask3D is able to segment\nmultiple instances of a given query object that might not be present in common segmentation datasets\non which closed-vocabulary instance segmentation approaches are trained. This opens up new\npossibilities for understanding and interacting with 3D scenes in a more comprehensive and flexible\nmanner. We encourage the research community to explore open-vocabulary approaches, where\nknowledge from different modalities can be seamlessly integrated into a unified and coherent space.\n10\nAcknowledgments and disclosure of funding. Francis Engelmann is a postdoctoral research fellow\nat the ETH AI Center. This project is partially funded by the ETH Career Seed Award \u201cTowards\nOpen-World 3D Scene Understanding\", and Innosuisse grant (48727.1 IP-ICT). We sincerely thank\nJonas Schult for helpful discussions, and Lorenzo Liso for the help with setting up our live demo.\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud,\nAndrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol\nVinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a Visual Language Model for Few-Shot\nLearning. In Neural Information Processing Systems (NeurIPS), 2022.\n[2] Abhishek Anand, Hema Swetha Koppula, Thorsten Joachims, and Ashutosh Saxena. Contextually Guided\nSemantic Labeling and Search for 3D Point Clouds. In International Journal on Robotics Research (IJRR),\n2011.\n[3] Iro Armeni, Ozan Sener, Amir R. Zamir, Helen Jiang, Ioannis Brilakis, Martin Fischer, and Silvio Savarese.\n3D Semantic Parsing of Large-Scale Indoor Spaces. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2016.\n[4] Matan Atzmon, Haggai Maron, and Yaron Lipman. Point Convolutional Neural Networks by Extension\nOperators. In ACM Transactions On Graphics (TOG), 2018.\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u2019e J\u2019egou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging Properties in Self-Supervised Vision Transformers. In International Conference on\nComputer Vision (ICCV), 2021.\n[6] Boyuan Chen, Fei Xia, Brian Ichter, Kanishka Rao, Keerthana Gopalakrishnan, Michael S. Ryoo, Austin\nStone, and Daniel Kappler. Open-vocabulary Queryable Scene Representations for Real World Planning.\narXiv preprint arXiv:2209.09874, 2022.\n[7] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon,\nChristoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive\nlanguage-image learning. arXiv preprint arXiv:2212.07143, 2022.\n[8] Julian Chibane, Francis Engelmann, Tuan Anh Tran, and Gerard Pons-Moll. Box2Mask: Weakly Super-\nvised 3D Semantic Instance Segmentation using Bounding Boxes. In European Conference on Computer\nVision (ECCV), 2022.\n[9] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D Spatio-Temporal ConvNets: Minkowski\nConvolutional Neural Networks. In Conference on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[10] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner.\nScanNet: Richly-Annotated 3D Reconstructions of Indoor Scenes. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2017.\n[11] Jian Ding, Nan Xue, Gui-Song Xia, and Dengxin Dai. Decoupling Zero-Shot Semantic Segmentation. In\nCVPR, 2022.\n[12] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. PLA: Language-Driven\nOpen-Vocabulary 3D Scene Understanding. In CVPR, 2023.\n[13] Cathrin Elich, Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. 3D-BEVIS: Birds-Eye-View\nInstance Segmentation. In German Conference on Pattern Recognition (GCPR), 2019.\n[14] Francis Engelmann, Theodora Kontogianni, Alexander Hermans, and Bastian Leibe. Exploring Spatial\nContext for 3D Semantic Segmentation of Point Clouds. In International Conference on Computer Vision\n(ICCV) Workshops, 2017.\n[15] Francis Engelmann, Theodora Kontogianni, Jonas Schult, and Bastian Leibe. Know What Your Neighbors\nDo: 3D Semantic Segmentation of Point Clouds. In European Conference on Computer Vision (ECCV)\nWorkshops, 2018.\n11\n[16] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias Nie\u00dfner. 3D-MPA: Multi\nProposal Aggregation for 3D Semantic Instance Segmentation. In Conference on Computer Vision and\nPattern Recognition (CVPR), 2020.\n[17] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. A Density-Based Algorithm for Dis-\ncovering Clusters in Large Spatial Databases with Noise. In Knowledge Discovery and Data Mining,\n1996.\n[18] Martin A. Fischler and Robert C. Bolles. Random sample consensus: a paradigm for model fitting with\napplications to image analysis and automated cartography. Commun. ACM, 24:381\u2013395, 1981.\n[19] Samir Yitzhak Gadre, Mitchell Wortsman, Gabriel Ilharco, Ludwig Schmidt, and Shuran Song. Clip\non wheels:\nZero-shot object navigation as object localization and exploration.\narXiv preprtint\narXiv:2207.04429, 2022.\n[20] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling Open-Vocabulary Image Segmentation with\nImage-Level Labels. In European Conference on Computer Vision (ECCV), 2021.\n[21] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. ImageBind: One Embedding Space To Bind Them All. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\n[22] Benjamin Graham, Martin Engelcke, and Laurens van der Maaten. 3D Semantic Segmentation with\nSubmanifold Sparse Convolutional Networks. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2018.\n[23] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary Object Detection via Vision and\nLanguage Knowledge Distillation. In International Conference on Learning Representations (ICLR), 2022.\n[24] Huy Ha and Shuran Song. Semantic Abstraction: Open-World 3D Scene Understanding from 2D Vision-\nLanguage Models. In CORL, 2022.\n[25] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. OccuSeg: Occupancy-aware 3D Instance Segmentation. In\nConference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[26] Sunan He, Taian Guo, Tao Dai, Ruizhi Qiao, Bo Ren, and Shu-Tao Xia. Open-Vocabulary Multi-Label\nClassification via Multi-modal Knowledge Transfer. In AAAI, 2023.\n[27] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. 3D-SIS: 3D Semantic Instance Segmentation of RGB-D Scans.\nIn Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n[28] Zeyu Hu, Xuyang Bai, Jiaxiang Shang, Runze Zhang, Jiayu Dong, Xin Wang, Guangyuan Sun, Hongbo\nFu, and Chiew Lan Tai. VMNet: Voxel-Mesh Network for Geodesic-Aware 3D Semantic Segmentation.\nIn International Conference on Computer Vision (ICCV), 2021.\n[29] Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Point-wise Convolutional Neural Network. In\nConference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[30] Chenguang Huang, Oier Mees, Andy Zeng, and Wolfram Burgard. Visual Language Maps for Robot\nNavigation. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA),\nLondon, UK, 2023.\n[31] Jing Huang and Suya You. Point Cloud Labeling Using 3D Convolutional Neural Network. In International\nConference on Pattern Recognition (ICPR), 2016.\n[32] Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li,\nGanesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso Miguel de\nMelo, Madhava Krishna, Liam Paull, Florian Shkurti, and Antonio Torralba. ConceptFusion: Open-Set\nMultimodal 3D Mapping. In Robotics: Science and Systems (RSS), 2023.\n[33] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung,\nZhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy\nText Supervision. In International Conference on Machine Learning (ICML), 2021.\n[34] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. PointGroup: Dual-Set\nPoint Grouping for 3D Instance Segmentation. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\n12\n[35] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. LERF: Language\nEmbedded Radiance Fields. In ICCV, 2023.\n[36] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao,\nSpencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dollar, and Ross Girshick. Segment anything.\nIn ICCV, 2023.\n[37] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for Editing via Feature\nField Distillation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\n[38] Hema Koppula, Abhishek Anand, Thorsten Joachims, and Ashutosh Saxena. Semantic Labeling of 3D\nPoint Clouds for Indoor Scenes. In Neural Information Processing Systems (NeurIPS), 2011.\n[39] Lars Kreuzberg, Idil Esen Zulfikar, Sabarinath Mahadevan, Francis Engelmann, and Bastian Leibe.\n4D-STOP: Panoptic Segmentation of 4D Lidar using Spatio-Temporal Object Proposal Generation and\nAggregation. In European Conference on Computer Vision (ECCV) Workshops, 2022.\n[40] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. Open-Vocabulary Object Detec-\ntion upon Frozen Vision and Language Models. In International Conference on Learning Representations\n(ICLR), 2023.\n[41] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R. Oswald. 3D Instance Segmentation via\nMulti-task Metric Learning. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\n[42] Loic Landrieu and Martin Simonovsky. Large-scale Point Cloud Semantic Segmentation with Superpoint\nGraphs. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\n[43] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven\nSemantic Segmentation. In International Conference on Learning Representations (ICLR), 2022.\n[44] Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, and Baoquan Chen. PointCNN: Convolution on\nX-transformed Points. In Neural Information Processing Systems (NeurIPS), 2018.\n[45] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter\nVajda, and Diana Marculescu. Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP. In\nConference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[46] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully Convolutional Networks for Semantic Segmen-\ntation. In Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n[47] Yan Lu and Christopher Rasmussen. Simplified Markov Random Fields for Efficient Semantic Labeling of\n3D Point Clouds. In International Conference on Intelligent Robots and Systems (ICIRS), 2012.\n[48] Chao Ma, Yu-Hao Yang, Yanfeng Wang, Ya Zhang, and Weidi Xie. Open-vocabulary Semantic Segmenta-\ntion with Frozen Vision-Language Models. In British Machine Vision Conference (BMVC), 2022.\n[49] Kirill Mazur, Edgar Sucar, and Andrew Davison. Feature-Realistic Neural Fusion for Real-Time, Open Set\nScene Understanding. In International Conference on Robotics and Automation (ICRA), 2023.\n[50] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.\nNeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2020.\n[51] Maxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,\nVasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel\nSynnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDINOv2: Learning Robust Visual Features without Supervision. arXiv preprint arXiv:2304.07193, 2023.\n[52] Songyou Peng, Kyle Genova, Chiyu \"Max\" Jiang, Andrea Tagliasacchi, Marc Pollefeys, and Thomas\nFunkhouser. OpenScene: 3D Scene Understanding with Open Vocabularies. In Conference on Computer\nVision and Pattern Recognition (CVPR), 2023.\n[53] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J. Guibas. PointNet: Deep Learning on Point Sets for\n3D Classification and Segmentation. In Conference on Computer Vision and Pattern Recognition (CVPR),\n2017.\n[54] Charles R. Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep Hierarchical Feature Learning\non Point Sets in a Metric Space. In Neural Information Processing Systems (NeurIPS), 2017.\n13\n[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\nTransferable Visual Models From Natural Language Supervision. In International Conference on Machine\nLearning (ICML), 2021.\n[56] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and\nJiwen Lu. DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting. In Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n[57] David Rozenberszki, Or Litany, and Angela Dai. Language-Grounded Indoor 3D Semantic Segmentation\nin the Wild. In European Conference on Computer Vision (ECCV), 2022.\n[58] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3D:\nMask Transformer for 3D Semantic Instance Segmentation. In International Conference on Robotics and\nAutomation (ICRA), 2023.\n[59] Nur Muhammad Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, and Arthur D. Szlam. CLIP-\nFields: Weakly Supervised Semantic Fields for Robotic Memory. 2023.\n[60] Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Robotic Navigation with Large Pre-Trained\nModels of Language, Vision, and Action. 2022.\n[61] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul\nMur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan,\nJune Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler,\nLuis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele,\nSteven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv\npreprint arXiv:1906.05797, 2019.\n[62] Ay\u00e7a Takmaz, Jonas Schult, Irem Kaftan, Mertcan Ak\u00e7ay, Bastian Leibe, Robert Sumner, Francis Engel-\nmann, and Siyu Tang. 3D Segmentation of Humans in Point Clouds with Synthetic Data. In International\nConference on Computer Vision (ICCV), 2023.\n[63] Lyne P. Tchapmi, Christopher B. Choy, Iro Armeni, JunYoung Gwak, and Silvio Savarese. SEGCloud:\nSemantic Segmentation of 3D Point Clouds. In International Conference on 3D Vision (3DV), 2017.\n[64] Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and\nLeonidas J. Guibas. KPConv: Flexible and Deformable Convolution for Point Clouds. In International\nConference on Computer Vision (ICCV), 2019.\n[65] Thang Vu, Kookhoi Kim, Tung M. Luu, Xuan Thanh Nguyen, and Chang D. Yoo. SoftGroup for 3D\nInstance Segmentation on 3D Point Clouds. In Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022.\n[66] Tianyi Wang, Jian Li, and Xiangjing An. An Efficient Scene Semantic Labeling Approach for 3D Point\nCloud. In IEEE International Conference on Intelligent Transportation Systems (ITSC), 2015.\n[67] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. SGPN: Similarity Group Proposal\nNetwork for 3D Point Cloud Instance Segmentation. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n[68] Silvan Weder, Hermann Blum, Francis Engelmann, and Marc Pollefeys. LabelMaker: Automatic Semantic\nLabel Generation from RGB-D Trajectories. In International Conference on 3D Vision (3DV), 2024.\n[69] Tianhan Wei, Xiang Li, Yau Pun Chen, Yu-Wing Tai, and Chi-Keung Tang. FSS-1000: A 1000-Class\nDataset for Few-Shot Segmentation. Conference on Computer Vision and Pattern Recognition (CVPR),\n2020.\n[70] Daniel Wolf, Johann Prankl, and Markus Vincze. Fast Semantic Segmentation of 3D Point Clouds using a\nDense CRF with Learned Parameters. In International Conference on Robotics and Automation (ICRA),\n2015.\n[71] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang.\nGroupViT: Semantic Segmentation Emerges from Text Supervision. In Conference on Computer Vision\nand Pattern Recognition (CVPR), 2022.\n[72] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. ODISE:\nOpen-Vocabulary Panoptic Segmentation with Text-to-Image Diffusion Models. In CVPR, 2023.\n14\n[73] Yifan Xu, Tianqi Fan, Mingye Xu, Long Zeng, and Yu Qiao. SpiderCNN: Deep Learning on Point Sets\nwith Parameterized Convolutional Filters. In European Conference on Computer Vision (ECCV), 2018.\n[74] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni.\nLearning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds. In Neural Information\nProcessing Systems (NeurIPS), 2019.\n[75] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. CoCa:\nContrastive Captioners are Image-Text Foundation Models. Transactions on Machine Learning Research,\n2022.\n[76] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv\npreprint arXiv:2111.11432, 2021.\n[77] Yuanwen Yue, Theodora Kontogianni, Konrad Schindler, and Francis Engelmann. Connecting the Dots:\nFloorplan Reconstruction Using Two-Level Queries. In Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023.\n[78] Yuanwen Yue, Sabarinath Mahadevan, Jonas Schult, Francis Engelmann, Bastian Leibe, Konrad Schindler,\nand Theodora Kontogianni. AGILE3D: Attention Guided Interactive Multi-object 3D Segmentation. arXiv\npreprint arXiv:2306.00977, 2023.\n[79] Nir Zabari and Yedid Hoshen. Semantic Segmentation In-the-Wild Without Seeing Any Segmentation\nExamples. arXiv preprint arXiv:2112.03185, 2021.\n[80] Chong Zhou, Chen Change Loy, and Bo Dai. Extract Free Dense Labels from CLIP. In European\nConference on Computer Vision (ECCV), 2022.\n15\nAppendix\nThe Appendix section is structured as follows: In Sec. A, we provide further details about our OpenMask3D\nmethod, and we explain certain design choices. In Sec. B we present additional results, such as ablation studies\non hyperparameters and insights into the performance of OpenMask3D. In Sec. C, we provide further details\nabout how the baseline experiments are performed. In Sec. D, we share additional qualitative results.\nA\nOpenMask3D details\nA.1\nClass-agnostic mask proposal module\nAs OpenMask3D is an instance-centric approach, we first need to obtain instance mask proposals. Since we are\noperating in an open-vocabulary setting, these instance masks are not associated with any class labels, they are\nclass agnostic.\nWe use the mask module of the transformer-based Mask3D [58] architecture as the basis for our class-agnostic\nmask proposal module. Specifically, for our experiments on ScanNet200 [57], we use [58] trained on the training\nset of ScanNet200 [57] for instance segmentation. For our experiments on the Replica [61] dataset, we use the\n[58] module trained on the training and validation sets of the ScanNet200 [57] dataset, but without segments\n(see [58] for details on segments). We keep the weights of the mask proposal module frozen. Unlike Mask3D,\nour mask proposal module discards class predictions and mask confidence scores (which are based on class\nlikelihoods), and we only retain the binary instance mask proposals.\nThe architecture consists of a sparse convolutional backbone based on the MinkowskiUNet [9], and a transformer\ndecoder. Point features obtained from the feature backbone are passed through the transformer decoder, which\niteratively refines the instance queries, and predicts an instance heatmap for each query. The query parameter\nspecifies the desired number of mask proposals from the transformer-based architecture. We set the number of\nqueries to 150, following the original implementation of Mask3D. This choice enables us to obtain a sufficient\nnumber of mask proposals for our open-vocabulary setting. The output from the final mask-module in the\ntransformer decoder is a set of binary instance masks.\nThe original implementation of the Mask3D [58] model first ranks the proposed instance masks based on their\nconfidence scores, and retains the top k masks based on this ranking. As the confidence scores are guided by\nclass likelihoods, we do not utilize such scores to rank the masks, and do not filter out any of the proposed\ninstance masks. Furthermore, since we aim to retain as many mask proposals as possible, we do not perform\nmask-filtering based on object sizes, or pairwise mask overlaps. In other words, we keep all masks proposed by\nthe mask module as each of these masks might correspond to a potential open-vocabulary query.\nAs the model may occasionally output mask proposals that are not spatially contiguous, the original imple-\nmentation of Mask3D employs the DBSCAN clustering algorithm [17] to break down such non-contiguous\nmasks into smaller, spatially contiguous clusters. We follow this practice, and perform DBSCAN clustering on\nthe instance masks, setting the epsilon parameter, \u03f5, to 0.95. This procedure often increases the final number\nof instance masks. As such, our OpenMask3D model generates class-agnostic instance masks, for which we\ncompute open-vocabulary mask-features as described next.\nA.2\nMask-feature computation module details\nIn the main paper, we outlined the steps to extract per-mask features in the CLIP [55] space. In this section, we\nwill delve into the motivations behind specific design choices made throughout the process.\nA.2.1\nPer-mask view ranking: assumptions and visualizations\nOnce we obtain the class-agnostic masks, our primary objective is to determine the best CLIP features for each\nof them. We operate under the assumption that CLIP is capable of extracting meaningful features from instances\nwhen provided with a favorable viewpoint, and in our feature computation process, we prioritize selecting views\nthat offer a higher number of visible points. In practice, to assess the quality of a particular viewpoint for each\nmask, we compute the proportion of points of the 3D instance mask that are visible from that view. Next, we\nrank the views based on their visibility scores, and we select the top k views with the highest scores, where k is\na hyper parameter. Fig. 6 illustrates this concept, where two views of a given instance are compared based on\nthe number of visible points. The view that has a higher number of visible points from the instance is selected\n(outlined in green).\n16\n1428 visible points\n591 visible points\nFigure 6: Visual comparison between a selected view (marked with a green outline) for the corresponding chair\ninstance, and a discarded view. The green points in the images represent the 3D instance points projected onto\nthe 2D image. In the selected view the instance is fully visible, and mostly occluded in the discarded view.\nA.2.2\nWhy do we need SAM?\nIn order to get the CLIP image features of a particular instance in the selected views, we need to crop the image\nsection containing the object instance. This process involves obtaining an initial tight crop of the instance\nprojected onto 2D, and then incrementally expanding the crop to include some visual context around it.\nA straightforward approach to perform the cropping would be to project all of the visible 3D points belonging to\nthe mask onto the 2D image, and fit a 2D bounding box around these points. However, as we do not discard any\nmask proposals, the masks can potentially include outliers, or the masks might be too small as an outcome of\nthe DBSCAN algorithm described in Sec. A.1. As depicted in Fig. 7, this can result in bounding boxes that are\neither too large or too small. Using CLIP-image features obtained from crops based solely on projected points\ncan lead to inferior instance mask-features.\nTo address this challenge, and to improve the robustness of our model against noisy instance masks, we propose\nusing a 2D segmentation method that takes a set of points from the mask as input, and produces the corresponding\n2D mask, together with its confidence score. The Segment Anything Model (SAM) [36] precisely fulfills this\ntask, allowing us to generate accurate 2D masks in a class-agnostic manner.\nFigure 7: Difference between the bounding boxes obtained by tightly cropping around the projected points from\nthe 3D instance mask (left), and the bounding box obtained from the 2D mask generated by SAM (right).\nA.2.3\nWhich points should we give as input to SAM?\nWhen using SAM for predicting a 2D mask, we need to determine a set of points on which the mask generation\nprocess will be conditioned. Initially, we attempted to input all the visible points projected from the 3D mask.\nHowever, this approach results in poor quality masks due to the inaccuracies of the projections and the noise\npresent in the 3D masks, as illustrated in Figure 8.\n17\nSAM score = 0.694\nFigure 8: Output of SAM, using all of the visible points from the projected 3D mask as input.\nTo address this issue, we explore an alternative approach. Instead of using all of the visible points as input for\nSAM, we randomly sample a subset of points from the projected 3D mask. Interestingly, this method produces\nmuch cleaner masks, as it can be seen in Fig. 9.\nSAM score = 0.978\nFigure 9: Output of SAM, using only 5 randomly sampled points (visualized as green dots) of the projected 3D\nmask as input.\nA.2.4\nWhy do we need to run SAM for multiple rounds?\nRelying solely on a small set of random points as input for SAM might be unreliable, particularly when the\nsampled points are outliers, or too concentrated in a particular area of the instance we want to segment.\nTo address this limitation, we implemented a sampling algorithm (as outlined in the main paper) inspired by\nRANSAC [18]. In this approach, we perform kround = 10 sampling rounds, and in each round we randomly\nsample ksample = 5 points from the projected 3D instance mask. SAM returns the predicted 2D mask along\nwith the corresponding mask confidence score for each set of sampled points. Based on the confidence scores\nreturned by SAM, we select the mask with the highest score. Fig. 10 illustrates an example where the points\nsampled in one round are concentrated in a small spatial range, resulting in an incorrect mask prediction by\nSAM.\nSAM score = 0.949\nFigure 10: Output of SAM, using only 5 randomly sampled points of the mask as input. Here the sampled\npoints (the green points visualized in the image) are concentrated in a small spatial range and cause SAM to\npredict an incorrect mask, which does not include the legs of the chair.\nHowever, by employing our sampling algorithm and running SAM for multiple rounds, we achieve improved\nresults. The iterative process allows us to select the mask with the highest score achieved among the rounds, as\nshown in Fig. 11. In this particular example, the selected mask, i.e. the one with the highest SAM confidence\nscore, accurately segments the chair.\n18\nSAM score = 0.985\nFigure 11: Illustration of the results from the round which gives the highest confidence score. On the left, we\nvisualize the 5 sampled points which are given as input to SAM. On the right, we visualize the SAM mask\nprediction, showing a perfect segmentation of the chair.\nB\nAdditional results\nB.1\nAblation Studies\nIn addition to our ablation experiments provided in the main paper, here we share additional results. First, in\nTable 6, we share results obtained from using a varying number of views (k) in the top-k view selection process.\nThis analysis is conducted on the ScanNet200 dataset. Additionally, we provide an ablation study evaluating the\nhyperparameters of the multi-scale cropping in our method. This analysis is conducted on the Replica dataset,\nand is presented in Table 7.\nTop-k\nAP\nAP50\nAP25\nhead(AP)\ncommon(AP)\ntail(AP)\n1\n13.6\n17.6\n20.8\n15.5\n12.5\n12.8\n5\n15.4\n19.9\n23.1\n17.1\n14.1\n14.9\n10\n15.4\n20.0\n23.2\n16.4\n14.0\n15.8\nTable 6: Ablation study of the top-k frame selection parameter k. This analysis is conducted on the\nScanNet200 validation set.\nLevels\nRatio of Exp.\nAP\nAP50\nAP25\n1\n0.1\n11.3\n16.0\n20.2\n3\n0.1\n13.1\n18.4\n24.2\n5\n0.1\n12.8\n17.6\n22.6\n3\n0.05\n12.9\n18.1\n23.5\n3\n0.1\n13.1\n18.4\n24.2\n3\n0.2\n12.8\n17.7\n22.9\nTable 7: Ablation study of the multi-scale cropping hyperparameters on the Replica dataset. We analyze\nthe effect of varying number of levels, and the ratio of expansion.\nB.2\nEvaluation on Replica without RGB-D images\nOur approach OpenMask3D requires images as input, as it depends on visual-language models that operate\non images in combination with text. In this work, we prioritized the ability to recognize uncommon/long-tail\nobjects over generalization across different modalities. Using vision-language models on images provides an\nexcellent opportunity to preserve this generalization capability. Nevertheless, even when only the 3D scan of a\nscene is available, it could still be possible to render images from the 3D scan, and use those synthetic images as\ninput to our OpenMask3D pipeline. We tried this on the Replica [61] dataset, and rendered RGB-D images from\nthe dense scene point clouds (as illustrated in Fig. 12). Our results using these rendered synthetic views of the\nscene point cloud are presented in Tab. 8. Even without any RGB-D images as an input, OpenMask3D performs\nrelatively well, and we observe that the performance decreases by only \u22121.5 AP. Furthermore, OpenMask3D\nshows stronger performance compared to the 3D baseline (OpenScene [52], 3D Distill).\nC\nDetails on baseline experiments\nQuantitative baseline experiments\nAs mentioned in the main paper, to compare our OpenMask3D method with the 3D open-vocabulary scene\nunderstanding approach OpenScene [52], we adapted OpenScene for the 3D instance segmentation task. As\n19\nModel\nAP\nAP50\nAP25\nOpenScene [52] (3D Distill) + masks\n8.2\n10.5\n12.6\nOpenMask3D w. rendered RGB-D images\n11.6\n14.9\n18.4\nOpenMask3D\n13.1\n18.4\n24.2\nTable 8: 3D instance segmentation results on the Replica dataset. Comparing OpenMask3D performance\nwhen using original RGB-D images vs. RGB-D images rendered from the point cloud.\n(a) RGB image from the Replica dataset\n(b) RGB image rendered from the point cloud\n(c) RGB image from the Replica dataset\n(d) RGB image rendered from the point cloud\nFigure 12: Visualization of the Replica RGB images. Original RGB images from the Replica dataset (left),\nand RGB images rendered from the scene point cloud (right).\nOpenScene computes a per-point feature representation for each point in the point cloud, we use our proposed\nclass-agnostic instance masks to aggregate OpenScene features for each mask. This way, we can associate each\nobject instance mask with a \"per-mask\" feature computed from OpenScene features.\nOpenScene provides 3 different model variants, namely 2D fusion, 3D distill, and 2D/3D ensemble. We primarily\ncompare with the OpenScene models using OpenSeg [20] features, whereas we also experiment with the 2D\nfusion model using LSeg [43] features. The main reason why we primarily compare against the models using\nOpenSeg features is that the features have a dimensionality of 768, relying on the same CLIP architecture\nwe employ in our work \u2013 ViT-L/14 (336px) [55] \u2013, whereas the LSeg features have a dimensionality of 512.\nFor our closed-vocabulary experiments with the OpenScene model using LSeg features, we embed each text\ncategory using ViT-B/32 CLIP text encoder, and we assign object categories using these embeddings. In all other\nexperiments, we embed text queries using the CLIP architecture ViT-L/14 (336px).\nQualitative baseline experiments\nIn addition to the quantitative results presented in the main paper, we conducted qualitative comparisons to\nfurther evaluate our mask-based open-world representation in comparison to the point-based representation\nprovided by OpenScene [52].\nIn Fig. 13 and Fig. 14, we provide these qualitative comparisons. In Fig. 13, we show results from the OpenScene\n[52] 2D fusion model using OpenSeg [20] features, and from OpenMask3D. In Fig. 14, we compare features\nfrom OpenMask3D with features from several OpenScene variants (2D fusion, 3D distill, 2D/3D ensemble). To\nvisualize results from OpenScene, for a given query, we compute the similarity score between each point-feature\nand the query text embedding, and visualize the similarity scores for each point as a heatmap over the points. On\nthe other hand, OpenMask3D results are based on mask-features, i.e., each mask has an associated feature vector,\nfor which we compute a similarity score. Hence, we visualize the similarity scores for each mask as a heatmap\nover the instance masks, and each point in a given instance mask is assigned to the same similarity score.\n20\nQuery: \u201ca dollhouse\u201d\nOpenScene [52]\nOpenMask3D (Ours)\nQuery: \u201ca desk with a backpack on it\u201d\nOpenScene [52]\nOpenMask3D (Ours)\nQuery: \u201cCetaphil\u201d\nOpenScene [52]\nOpenMask3D (Ours)\nFigure 13: Qualitative comparisons. Heatmaps illustrating the similarity between the CLIP embeddings\nof a specific query and the open-vocabulary representation of the scene. A comparison is made between the\npoint-based OpenScene approach proposed by Peng et al. [52] (left) and our mask-based approach OpenMask3D\n(right). Dark red means high similarity, and dark blue means low similarity with the query.\nAs evident from Fig. 13 and Fig. 14, while similarity computation between the query embedding and each\nOpenScene per-point feature vector results in a reasonable heatmap, it is challenging to extract object instance\ninformation from the heatmap representation. Our method OpenMask3D, on the other hand, computes the\nsimilarity between the query embedding and each per-mask feature vector. This results in crisp instance\nboundaries, particularly suitable for the use cases in which one needs to handle object instances.\nD\nQualitative results\nIn this section, we provide additional qualitative results.\nIn Fig. 15, we show results from object categories that are not present in the ScanNet200 label set. Objects from\nthese novel categories are successfully segmented using our OpenMask3D approach. Thanks to its zero-shot\nlearning capabilities, OpenMask3D is able to segment a given query object that might not be present in common\nsegmentation datasets on which closed-vocabulary instance segmentation approaches are trained.\nIn Fig. 16, we show open-vocabulary 3D instance segmentation results using queries describing various object\nproperties such as affordances, color, geometry, material type and object state. This highlights that our model\nis able to preserve information about such object properties and go beyond object semantics, contrary to the\ncapabilities of existing closed-vocabulary 3D instance segmentation approaches.\n21\n(a) Ours (per-mask)\n(b) OpenScene (2D Fusion, per-point)\n(c) OpenScene (3D Distill, per-point)\n(d) OpenScene (2D/3D Ensemble, per-point)\nFigure 14: Feature similarity for the query \u201cthe table with a flower vase on it\u201d.\n22\n\u201cdollhouse\u201d\n\u201cCetaphil\u201d\n\u201cRoomba\u201d\n\u201cflip-flops\u201d\n\u201cgym bench\u201d\n\u201crice cooker\u201d\nFigure 15: Qualitative results from OpenMask3D. We show open-vocabulary instance segmentation results\nusing arbitrary queries involving object categories that are not present in the ScanNet200 dataset labels. In\neach scene, we visualize the instance with the highest similarity score for the given query embedding. These\npredictions show the zero-shot learning ability of our model, highlighting the open-vocabulary capabilities.\n23\nAffordance\n\u201csit\u201d\n\u201cwash\u201d\nColor\n\u201can orange pillow\u201d\n\u201ca yellow pillow\u201d\nGeometry\n\u201ca circular table\u201d\n\u201ca rectangular table\u201d\nMaterial\n\u201ca leather chair\u201d\n\u201ca fabric chair\u201d\nState\n\u201can empty table\u201d\n\u201ca side table with a flower vase on it\u201d\nState\n\u201ca full garbage bin\u201d\n\u201can empty garbage bin\u201d\nFigure 16: Qualitative results from OpenMask3D, queries related to object properties. We show open-\nvocabulary 3D instance segmentation results using queries describing various object properties such as af-\nfordances, color, geometry, material type, and object state. In each row, we show two different queries per\ncategory, for which our method successfully segments different object instances. These results highlight the\nstrong open-vocabulary 3D instance segmentation capabilities of our model, going beyond object semantics.\n24\n"
  },
  {
    "title": "DreamEditor: Text-Driven 3D Scene Editing with Neural Fields",
    "link": "https://arxiv.org/pdf/2306.13455.pdf",
    "upvote": "7",
    "text": "DreamEditor: Text-Driven 3D Scene Editing with Neural Fields\nJingyu Zhuang*\nzhuangjy6@mail2.sysu.edu.cn\nSun Yat-sen University\nChina\nChen Wang*\ncw.chenwang@outlook.com\nUniversity of Pennsylvania\nUSA\nLiang Lin \u0000linliang@ieee.org\nSun Yat-sen University\nChina\nLingjie Liu \u0000lingjie6@seas.upenn.edu\nUniversity of Pennsylvania\nUSA\nGuanbin Li \u0000liguanbin@mail.sysu.edu.cn\nSun Yat-sen University\nChina\n\u201cA \u2217 horse standing on a wooden box\u201d\n\u201cA \u2217 deer\u201d\n\u201cA \u2217 giraffe\u201d\n\u201cA \u2217 robot horse\u201d\n\u201cA \u2217 dog wearing sunglasses\u201d\n\u201cA \u2217 dog taking a rose in mouth\u201d\nOriginal \nNeural Field\nOriginal \nNeural Field\nFigure 1: Our approach DreamEditor allows users to edit 3D scenes with text prompts. DreamEditor achieves precise and\nhigh-quality editing that maintains irrelevant regions unchanged.\n*Both authors contributed equally to this research. Corresponding authors: Guanbin\nLi, Lingjie Liu and Liang Lin. Welcome to Code and Project page .\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0315-7/23/12...$15.00\nhttps://doi.org/10.1145/3610548.3618190\nABSTRACT\nNeural fields have achieved impressive advancements in view syn-\nthesis and scene reconstruction. However, editing these neural\nfields remains challenging due to the implicit encoding of geometry\nand texture information. In this paper, we propose DreamEditor,\na novel framework that enables users to perform controlled edit-\ning of neural fields using text prompts. By representing scenes\nas mesh-based neural fields, DreamEditor allows localized editing\nwithin specific regions. DreamEditor utilizes the text encoder of a\npretrained text-to-Image diffusion model to automatically identify\narXiv:2306.13455v3  [cs.CV]  7 Sep 2023\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nZhuang et al.\nthe regions to be edited based on the semantics of the text prompts.\nSubsequently, DreamEditor optimizes the editing region and aligns\nits geometry and texture with the text prompts through score dis-\ntillation sampling [Poole et al. 2022]. Extensive experiments have\ndemonstrated that DreamEditor can accurately edit neural fields of\nreal-world scenes according to the given text prompts while ensur-\ning consistency in irrelevant areas. DreamEditor generates highly\nrealistic textures and geometry, significantly surpassing previous\nworks in both quantitative and qualitative evaluations.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Rendering; Neural networks.\nACM Reference Format:\nJingyu Zhuang*, Chen Wang*, Liang Lin \u0000, Lingjie Liu \u0000, and Guanbin Li\n\u0000. 2023. DreamEditor: Text-Driven 3D Scene Editing with Neural Fields. In\nSIGGRAPH Asia 2023 Conference Papers (SA Conference Papers \u201923), December\n12\u201315, 2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 10 pages.\nhttps://doi.org/10.1145/3610548.3618190\n1\nINTRODUCTION\nNeural radiance fields [Mildenhall et al. 2021], NeuS [Wang et al.\n2021] and subsequent research [Liu et al. 2020; M\u00fcller et al. 2022;\nWang et al. 2022c] (collectively referred to as neural fields) have\nmade significant progress in scene reconstruction and novel view\nsynthesis. By capturing multi-view images of a 3D scene and us-\ning off-the-shelf structure-from-motion models to estimate camera\nposes, one can train neural networks to learn neural fields that\nimplicitly represent the geometry and texture of the scene. Com-\npared to the traditional pipeline involving tedious 3D matching and\ncomplex postprocessing steps, neural fields offer a more efficient\nand accessible approach for reconstructing real-world objects and\nscenes into Computer Graphics assets for general users.\nHowever, editing neural fields is not a straightforward task since\nthe shape and texture information is implicitly encoded within high-\ndimensional neural network features. Conventional 3D modeling\ntechniques are ineffective for manual sculpting and re-texturing\nsince explicit geometry is not available. Previous research has ex-\nplored techniques for neural fields editing, such as moving objects\nin a scene [Chen et al. 2021], modifying textures [Xiang et al. 2021],\nand altering object shape [Yang et al. 2022]. However, these editing\nprocedures still require extensive user input. While recent work\nhas enabled NeRF editing with text prompts [Haque et al. 2023],\nit struggles to achieve precise and high-quality editing due to a\nrestricted diversity of instructions. Consequently, further research\nis needed to develop easy-to-use and accurate 3D editing methods,\nenabling improved re-creation of existing 3D assets.\nIn this paper, we present DreamEditor, a framework that allows\nusers to intuitively and conveniently modify neural fields using\ntext prompts. As illustrated in Fig. 1, for a given scene represented\nby a neural field, e.g., a dog or a complex outdoor environment, text\ndescriptions can be used to achieve various object-centric editing, in-\ncluding re-texturing, object replacement, and object insertion, while\nsimultaneously preserving irrelevant regions to the text prompts.\nThis is made possible through two key designs in our method: (1) a\nmesh-based neural field representation, and (2) a stepwise frame-\nwork that leverages pretrained diffusion models for 3D editing.\nCompared to an implicit representation, an explicit mesh-based\nneural field enables the efficient conversion of 2D editing masks\ninto 3D editing regions through back projection, facilitating precise\nlocal editing by only modifying the masked regions. Additionally,\nthe mesh representation disentangles geometry and texture, pre-\nventing unnecessary geometry deformation when only appearance\nchanges are expected. Leveraging the advantages of the mesh repre-\nsentation, we propose a stepwise finetune-localization-optimization\nframework that efficiently and accurately edits 3D scenes using\nsimple text prompts, achieved by score distillation sampling within\nthe masked region.\nWe extensively evaluate DreamEditor on various synthetic and\nreal-world scenes, including animals, human faces and outdoor\nscenes. Unlike methods that operate on the entire image, our edit-\ning approach enables precise local deformations while naturally\npreserving irrelevant areas. For example, in Fig. 1, only the dog\u2019s\nmouth is modified when holding a rose in its mouth. Furthermore,\nas the edit can be accomplished with a simple text prompt, the\nprocedure is user-friendly and significantly simplifies the editing\nof neural fields, showing its great potential for practical applica-\ntions. Both qualitative and quantitative comparisons demonstrate\nthe superiority of DreamEditor over previous methods in terms of\nediting precision, visual fidelity and user satisfaction.\nThe contributions of this paper can be summarized as follows: (1)\nWe introduce a novel framework for text-guided 3D scene editing,\nwhich achieves highly realistic editing results for a wide range of\nreal-world scenes; (2) We propose to use a mesh-based neural field\nto enable local modification of the scene and decouple texture and\ngeometric features for flexible editing; (3) We devise a stepwise\nediting framework that first identifies the specific regions requiring\nediting according to text prompts and then performs modifications\nexclusively within the selected regions. This systematic procedure\nensures precise 3D editing while minimizing unnecessary modifi-\ncations in irrelevant regions.\n2\nRELATED WORKS\n2.1\nText-guided image generation and editing\nThe denoising diffusion probabilistic model [Ho et al. 2020; Song\net al. 2020] has drawn great attention for its ability to generate\nhigh-quality images. Later, diffusion models [Ramesh et al. 2022;\nRombach et al. 2022; Saharia et al. 2022] trained on large-scale\nimage-text paired datasets demonstrated astonishing performance\nin understanding complex semantics from text prompts (including\nnouns, adjectives, verbs, etc.) and generating corresponding high-\nquality images. Due to the rich semantics and high controllability of\npretrained text-to-image diffusion models, a series of studies [Avra-\nhami et al. 2022; Couairon et al. 2022; Hertz et al. 2022; Kawar et al.\n2022] have employed them to text-guided image editing. Most re-\nlated to our work is subject-driven generation with text-to-image\ndiffusion models [Gal et al. 2022a; Ruiz et al. 2022], which enables\nusers to personalize their image generation for specific subjects\nand concepts given. DreamBooth [Ruiz et al. 2022] expands the\nlanguage-vision dictionary using rare tokens and finetunes the\nmodel with a preservation loss for regularization. Similarly, Textual\nInversion [Gal et al. 2022a] optimizes a new \u201cword\" in the embed-\nding space of the pre-trained diffusion model to represent the input\nDreamEditor: Text-Driven 3D Scene Editing with Neural Fields\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nobjects. These works address the task of editing specific images\nor generating images with novel concepts, but it is non-trivial to\nextend these 2D methods to 3D.\n2.2\nText-to-3D generation\nWith the development of text-to-image generation models, there\nhas been a growing interest in text-to-3D generation. Some works\nuse the CLIP model to optimize mesh [Chen et al. 2022; Michel\net al. 2022; Mohammad Khalid et al. 2022] or neural fields [Jain\net al. 2022]. The seminar work DreamFusion [Poole et al. 2022]\nfirst proposes score distillation sampling (SDS) loss to distill the\nknowledge in pretrained 2D Text-to-Image diffusion models for\ntext-to-3D generation. A series of works [Chen et al. 2023; Lin et al.\n2022; Metzer et al. 2022; Raj et al. 2023] based on SDS loss, further\nimprove the generation results by introducing geometry prior or\nchanging 3D representation. Score Jacobian Chaining [Wang et al.\n2022b] arrives at a similar training objective from the perspective\nof approximating 3D score with the 2D score. However, all these\nmethods lack the ability to edit existing 3D scenes. One of the main\nreasons is the difficulty in fully aligning an existing 3D scene with\ntext, resulting in these methods tending to generate a new scene and\nbreaking the consistency before and after editing. To overcome this\nlimitation, we propose a novel text-guided 3D editing framework\nthat can edit an existing 3D scene based on text prompts.\n2.3\nNeural Field Editing\nEditing neural fields is inherently difficult due to its entangled\nshape and appearance. EditNeRF [Liu et al. 2021] is the first work\nto support editing the shape and color of neural fields conditioned\non latent codes. Some works [Bao et al. 2023; Gao et al. 2023; Wang\net al. 2022a, 2023] further leverage a CLIP model to allow editing\nwith text prompts or reference images. Another line of work uses\npre-defined template models or skeletons to support re-posing or re-\nrendering [Noguchi et al. 2021; Peng et al. 2021], but is constrained\nin a specific category. 3D editing can also be achieved by combin-\ning 2D image manipulation such as inpainting with neural fields\ntraining [Kobayashi et al. 2022; Liu et al. 2022]. Geometry-based\nmethods [Li et al. 2022; Xu and Harada 2022; Yang et al. 2022; Yuan\net al. 2022] export neural fields to mesh and synchronize the defor-\nmation of the mesh back to implicit fields. TEXTure [Richardson\net al. 2023] uses a text prompt to generate the textures of the mesh\nusing an iterative diffusion-based process.\nThe most similar work to ours is Instruct-NeRF2NeRF [Haque\net al. 2023] and Vox-E [Sella et al. 2023], which edit a neural field\nfreely text prompts. Instruct-NeRF2NeRF uses image-based dif-\nfusion model [Brooks et al. 2022] to edit the input image with\ninstructions for optimizing the neural field. Nonetheless, since it\nmanipulates the entire image, usually undesired regions will also\nbe changed. Vox-E adopts SDS loss and performs local editing in 3D\nspace by 2D cross-attention maps. However, due to the constraints\ninherent of Vox-E\u2019s volumetric representation, the editing quality\nof real scenes remains suboptimal.\n3\nBACKGROUND\nOptimizing Neural Fields with SDS Loss. DreamFusion [Poole\net al. 2022] proposed the score distillation sampling (SDS) loss\nto distill the priors Text-to-Image (T2I) diffusion models for 3D\ngeneration. It first adds random Gaussian noise at level\ud835\udc61 to a random\nrendered view \u02c6\ud835\udc3c to get \u02c6\ud835\udc3c\ud835\udc61. The pretrained diffusion model \ud835\udf19 is used\nto predict the added noise given \u02c6\ud835\udc3c\ud835\udc61 and the input text condition \ud835\udc66.\nThe SDS loss is calculated as the per-pixel gradient as follows:\n\u2207\ud835\udf03 L\ud835\udc46\ud835\udc37\ud835\udc46 (\ud835\udf19, \u02c6\ud835\udc3c = \ud835\udc54(\ud835\udf03)) = E\ud835\udf16,\ud835\udc61\n\u0014\n\ud835\udc64(\ud835\udc61)(\ud835\udf16\ud835\udf19 (\u02c6\ud835\udc3c\ud835\udc61;\ud835\udc66,\ud835\udc61) \u2212 \ud835\udf16) \ud835\udf15\u02c6\ud835\udc3c\n\ud835\udf15\ud835\udf03\n\u0015\n,\n(1)\nwhere \ud835\udc64(\ud835\udc61) is a weighting function that depends on noise level \ud835\udc61,\n\ud835\udf03 is the parameters of neural field and \ud835\udc54 is the rendering process.\nDuring training, the diffusion model is frozen and gradients are\nback-propagated to \ud835\udf03, enforcing the neural field\u2019s renderings to\nresemble the images generated by the diffusion model with the text\ncondition \ud835\udc66.\nDreamBooth [Ruiz et al. 2022] is a subject-driven image generation\nmethod based on T2I models. Given a few images of the same\nsubject, DreamBooth embeds the subject into a T2I diffusion model\nby binding it to a unique identifier (denoted as \u2217). It uses an L2\nreconstruction loss to fine-tune the diffusion model on the input\nimages and a class prior-preserving loss to prevent overfitting. The\ndetails of its training can be found in Ruiz et al [2022]. In this paper,\nwe also adopt DreamBooth to fine-tune the T2I diffusion models\nfor expressing a specific scene.\n4\nMETHOD\n4.1\nOverview\nThe inputs of our method are a set of posed images of a 3D scene\nto be edited and a text prompt for editing. Our goal is to change\nthe shape and appearance of the object of interest in the original\n3D scene according to the text prompt. Fig. 3 gives an example\nof turning a horse sculpture into a real giraffe. This task requires\nkeeping the 3D contents irrelevant to the text prompt unchanged\nbefore and after editing.\nThe framework of DreamEditor is shown in Fig. 3, which consists\nof three stages. We first transform the original neural radiance field\ninto a mesh-based neural field (Section 4.2), which enables us to\nachieve spatially-selective editing. In Section 4.3, we customize the\nT2I model to the input scene and use the cross-attention maps of it\nto locate the editing area in the 3D space according to the keywords\nin the text prompts. Finally, we edit the target object in the neural\nfield under the control of text prompts through the T2I diffusion\nmodel (Section 4.4).\n4.2\nDistilling Neural Fields\nInspired by [Yang et al. 2022], we first learn a neural radiance\nfield from input images and decompose it into many local implicit\nfields organized in an explicit mesh, where the mesh is extracted\nfrom the neural radiance field using marching cubes [Lorensen\nand Cline 1987]. Representing a scene as a mesh-based neural field\nintroduces two benefits. First, a mesh-based neural field enables\nprecise editing of specific regions in the scene. The regions, such as\nbackground and irrelevant objects, can remain unchanged during\nediting by fixing the specific implicit fields. Second, the extracted\nmesh can explicitly represent the surface and outline of the objects\nin the scene. Compared with other explicit representations such\nas voxels [Liu et al. 2020] and point clouds [Ost et al. 2022], it is\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nZhuang et al.\nDreamBooth\nOriginal \nNeural Field\nMesh-based \nNeural Field\nDistill\nSample point          Ray Casting \nFeature representations\n1. \ud835\udc37\ud835\udc56\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc59\ud835\udc59\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\n2. \ud835\udc3f\ud835\udc5c\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5b\ud835\udc54\n\ud835\udc52\ud835\udc51\ud835\udc56\ud835\udc61 \ud835\udc5f\ud835\udc52\ud835\udc54\ud835\udc56\ud835\udc5c\ud835\udc5b\nGet Attention\nBack-Project\nMesh\n\"\ud835\udc4e \u2217 \ud835\udc54\ud835\udc56\ud835\udc5f\ud835\udc4e\ud835\udc53\ud835\udc53\ud835\udc52\"\n3. \ud835\udc42\ud835\udc5d\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc56\ud835\udc67\ud835\udc56\ud835\udc5b\ud835\udc54\nRender\nBackpropagation\n\ud835\udc46\ud835\udc37\ud835\udc46 \ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\nMesh-based \nNeural Field\n\"\ud835\udc4e \u2217 \ud835\udc54\ud835\udc56\ud835\udc5f\ud835\udc4e\ud835\udc53\ud835\udc53\ud835\udc52\"\nDreamBooth\nFigure 2: The overview of our method. Our method edits a 3D scene by optimizing an existing neural field to conform with a\ntarget text prompt. The editing process involves three steps: (1) The original neural field is distilled into a mesh-based one.\n(2) Based on the text prompts, our method automatically identifies the editing region of the mesh-based neural field. (3) Our\nmethod utilizes the SDS loss to optimize the color feature \ud835\udc53\ud835\udc50, geometry feature \ud835\udc53\ud835\udc54, and vertex positions \ud835\udc63 of the editing region,\nthereby altering the texture and geometry of the respective region. Best viewed in color.\nmore convenient to determine the range of editing area with mesh.\nCombining the attention scheme of the diffusion model, we further\npropose a method to automatically determine the editing area,\nwhich can accurately locate the editing area in the mesh according\nto the input text.\nSpecifically, after the neural radiance field is obtained, we adopt\na teacher-student based training framework to perform distillation,\nwhere the neural radiance field is taken as the teacher model to\nguide the student model, i.e., the mesh-based neural field. We define\nthe mesh-based neural field by assigning each mesh vertex v a\ncolor feature \ud835\udc53\ud835\udc50 and a geometry feature \ud835\udc53\ud835\udc54 to represent the local\nshape and texture information near v, respectively. During the\nvolume rendering process, for a sampled point \ud835\udc65, we first obtain\nthe aggregated features \u02dc\ud835\udc53\ud835\udc50 and \u02dc\ud835\udc53\ud835\udc54 by interpolating the features of\nthe top \ud835\udc3e nearest vertices of \ud835\udc65 weighted by the inverse distance\n(v\ud835\udc58 \u2212 \ud835\udc65) [Qi et al. 2017]:\n\u02dc\ud835\udc53\ud835\udc61 (\ud835\udc65) =\n\u00cd\ud835\udc3e\n\ud835\udc58=1\ud835\udc64\ud835\udc58 \ud835\udc53\ud835\udc61,\ud835\udc58\n\u00cd\ud835\udc3e\n\ud835\udc58=1\ud835\udc64\ud835\udc58\n,\ud835\udc64\ud835\udc58 =\n1\n||v\ud835\udc58 \u2212 \ud835\udc65||,\ud835\udc61 \u2208 {\ud835\udc54,\ud835\udc50}\n(2)\nThen, \u02dc\ud835\udc53\ud835\udc54 and \u02dc\ud835\udc53\ud835\udc50 are decoded to the s-density \ud835\udc60 and color \ud835\udc50 of \ud835\udc65:\n\ud835\udc60 = \ud835\udc37\ud835\udc3a ( \u02dc\ud835\udc53\ud835\udc54, \u02dc\u210e),\n\ud835\udc50 = \ud835\udc37\ud835\udc36 ( \u02dc\ud835\udc53\ud835\udc50, \u02dc\u210e, d, \u2207\ud835\udc65\ud835\udc60)\n(3)\nwhere \ud835\udc37\ud835\udc3a and \ud835\udc37\ud835\udc36 are the geometry decoder and color decoder\nrespectively, \u02dc\u210e is the interpolated signed distance of \ud835\udc65 to v\ud835\udc58, d is\nthe ray direction and \u2207\ud835\udc65\ud835\udc60 is the gradient of s-density \ud835\udc60 at point \ud835\udc65.\nThe framework of the network is shown in Fig. 9.\nDuring the distillation process, we randomly sample rays \ud835\udc5f in the\nscene and use the output of the teacher model given \ud835\udc5f as the ground\ntruth, including the rendered pixel color \u02c6\ud835\udc36(\ud835\udc5f), s-density \u02c6\ud835\udc60\ud835\udc56 and point\ncolor \u02c6\ud835\udc50\ud835\udc56 of each sampling point \ud835\udc65 on this ray. The distillation loss is\ncomputed as:\nL\ud835\udc51\ud835\udc56\ud835\udc60 =\n\u2211\ufe01\n\ud835\udc5f \u2208\ud835\udc45\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc41\n(\u2225\u02c6\ud835\udc60\ud835\udc56 \u2212 \ud835\udc60\ud835\udc56 \u2225 + \u2225\u02c6\ud835\udc50\ud835\udc56 \u2212 \ud835\udc50\ud835\udc56 \u2225) +\n\u2211\ufe01\n\ud835\udc5f \u2208\ud835\udc45\n\r\r \u02c6\ud835\udc36(\ud835\udc5f) \u2212 \ud835\udc36(\ud835\udc5f)\n\r\r2\n2 ,\n(4)\nwhere the volume rendering formulation of teacher and student\nmodels (i.e., \u02c6\ud835\udc36 and\ud835\udc36) is the same as NeuS [Wang et al. 2021]. Besides,\nwe add Eikonal loss [Gropp et al. 2020] on the sampled points to\nregularize the norm of the spatial gradients with weight \ud835\udf06\ud835\udc5f\ud835\udc52\ud835\udc54 = 0.01\nL\ud835\udc5f\ud835\udc52\ud835\udc54 =\n\u2211\ufe01\n\ud835\udc5f \u2208\ud835\udc45\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc41\n\r\r\r\r\u2207\ud835\udc65\ud835\udc56\ud835\udc60\ud835\udc56\n\r\r \u2212 1\n\r\r2\n2 .\n(5)\nIn our framework, all camera pose sampling is based on the\nspherical coordinate system. We transform the target object to the\norigin and make the y-axis point upwards. We confine the scope of\nsampled views by setting the range of the elevation and azimuth\nangles in the following locating and optimizing step, thereby im-\nproving editing efficiency.\n4.3\nLocating Editing Regions\nAs illustrated in the middle part of Fig 2, given text prompts, DreamEd-\nitor first determines the target editing area in a rendered view. As a\npreparation step, we first fine-tune the Stable Diffusion model with\nDreamBooth with the sampled views, which adapts the model\u2019s\nknowledge to the specific scene. Then, we utilize the fine-tuned\ndiffusion model to obtain a 2D mask for each rendered view. Finally,\nwe obtain the 3D editing region by back-projecting the masked\ntarget region from different views onto the mesh.\nThe locating is motivated by the fact that cross-attention layers\nin T2I diffusion models control the relationship between the layout\nof the generated images and each word [Hertz et al. 2022]: \ud835\udc40 =\nSoftmax(\ud835\udc44\ud835\udc3e\ud835\udc47 /\u221a\ud835\udc5e), where \ud835\udc44 is the query features projected from\nthe spatial features of the noisy image with dimension \ud835\udc5e, \ud835\udc3e is the\nkey matrix projected from the textual embedding, \ud835\udc40 is the attention\nmap that defines the weight of a token for each pixel. Therefore, \ud835\udc40\nindicates the probability that a pixel corresponds to a word in the\ntext prompt and can be utilized to locate the editing area.\nSpecifically, the noisy image \u02c6\ud835\udc3c\ud835\udc61 of a rendered view and the text\nprompt are fed into the diffusion model for denoising. We select the\nkeyword that represents the intended editing results (e.g., \"apron\",\n\"giraffe\", \"hat\" as in Fig. 3) and extracts all its attention maps pro-\nduced during the generation process. In practice, the backbone\nof the diffusion model usually consists of \ud835\udc3f convolutional blocks,\nDreamEditor: Text-Driven 3D Scene Editing with Neural Fields\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nwhich are equipped with \ud835\udc3b multi-headed attention layers [Vaswani\net al. 2017]. Therefore, after \ud835\udc47 rounds of denoising, the final set\nof attention maps M can be represented as\n\b\n\ud835\udc40\ud835\udc61,\ud835\udc59,\u210e\n\t, where \ud835\udc61, \ud835\udc59, \u210e\nrepresent the index of the time step, convolution block, attention\nhead, respectively. We resize all attention maps to the same reso-\nlution by bilinear interpolation and aggregate them to obtain the\naggregated attention map \ud835\udc40. \ud835\udc40 are further normalized to [0,1] and\nbinarized with a threshold \ud835\udf0f = 0.75, where the area with a value of\n1 is the editing area. We back-project all the pixels belonging to the\nediting area in the mask onto the mesh and mark the intersected\nmesh faces as the editing region. It is worth highlighting that the\nkeywords are not restricted to the objects in the initial scene, as\nthe attention maps of a keyword delineate regions in the generated\nimage where the likelihood of keyword presence is highly probable.\nAs shown in Fig. 6), even though \"sunglasses\" is not part of the\noriginal scene, it remains feasible to identify the reasonable region\non the scene mesh.\nIn this stage, we traverse all elevation and azimuth angles at 45\u00b0\nintervals within the scope of sampled views to ensure the coverage\nof all potential editing regions. Subsequently, we get the masks of all\nsampled views and back-project them onto the mesh. After merging\nthe results of back-projection, we employ two steps to refine the\nmasked region: (1) Discard: we discard the small pieces within the\nediting region where the number of faces is less than 10% of the\ntotal projected area, which typically emerges from inaccuracy 2D\nmasks (e.g., masks larger than target object is projected outside the\nobject); (2) Fill: we use breadth-first search to fill in the \u201choles\u201d in\nthe editing region, i.e., a non-editing region surrounded by editing\nregions. Such \"holes\" usually come from occluded (e.g., the bottom\nof a horse) or concave areas. By integrating these regions into the\nediting area, we enhance the completeness of the editing area. We\ndenote the final editing region as V = {\ud835\udc63\ud835\udc52}\ud835\udc38\n\ud835\udc52=1.\n4.4\nOptimizing Editing Regions\nIn this step, we adopt the SDS Loss from DreamFusion [Poole et al.\n2022] to guide the optimization of the editing region in the neu-\nral field with the T2I diffusion model, making the scene conforms\nto the text prompt. By feeding random rendered views and the\ntext prompt to the T2I diffusion model, we calculate the SDS Loss\nand backpropagate the gradients to the neural field. Since the Ima-\ngen [Saharia et al. 2022] in DreamFusion is proprietary, we compute\nthe SDS Loss in the latent space with Stable Diffusion [Rombach\net al. 2022] as follows:\n\u2207\ud835\udf14L\ud835\udc46\ud835\udc37\ud835\udc46 (\ud835\udf19,\ud835\udc54(\ud835\udf14)) = E\ud835\udf16,\ud835\udc61\n\u0014\n\ud835\udc64(\ud835\udc61)(\ud835\udf16\ud835\udf19 (\ud835\udc67\ud835\udc61;\ud835\udc66,\ud835\udc61) \u2212 \ud835\udf16) \ud835\udf15\ud835\udc67\n\ud835\udf15\u02c6\ud835\udc3c\n\ud835\udf15\u02c6\ud835\udc3c\n\ud835\udf15\ud835\udf14\n\u0015\n,\n(6)\nwhere \ud835\udf14 = {\ud835\udc53\ud835\udc54,\ud835\udc58, \ud835\udc53\ud835\udc50,\ud835\udc58, v\ud835\udc58}\ud835\udc58 are the set of geometry features, color\nfeatures and positions for all mesh vertices in V, \ud835\udc67\ud835\udc61 denotes the\nnoisy latent, and \ud835\udc67 is the original latent generated by the encoder\nof the Stable Diffusion model.\nWe can see from Equation 6 that during training, apart from\noptimization of the color feature \ud835\udc53\ud835\udc50 and geometry feature \ud835\udc53\ud835\udc54 of the\nvertices in the editing region, the positions of the vertices are also\nincluded. This implies that the structure of the mesh is also dy-\nnamically adjusted during the optimization, which is a critical part\nof our approach. In local implicit fields, geometry features mainly\nrepresent shape details near the vertices. The smoothness of the\nobject\u2019s surface will be disrupted if there are significant changes in\nthe s-density of the points situated away from the vertices. Hence,\nwe propose a complementary optimization approach, which simul-\ntaneously optimizes the vertex position and geometry features. The\noptimization of the vertex position ensures that the overall shape\nof the mesh conforms to the text prompt, while the optimization\nof the geometry features refines the local geometry of the object.\nThis optimization approach enables DreamEditor to generate com-\nplex shapes, such as rose petals. Our ablation study in Section 5.4\ndemonstrates the necessity of the joint optimization of the vertex\nposition and geometry features.\nTo maintain a smooth surface and encourage natural deformation\nduring vertex position optimization, we introduce widely-used\nmesh regularization terms, including the Laplacian loss and ARAP\n(as-rigid-as-possible) loss [Sumner et al. 2007]:\nL\ud835\udc59\ud835\udc4e\ud835\udc5d = 1\n\ud835\udc38\n\ud835\udc38\n\u2211\ufe01\n\ud835\udc56=1\n\r\r\rv\ud835\udc56 \u2212\n1\n|\ud835\udc41\ud835\udc56 |\n\u00cd\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56 v\ud835\udc57\n\r\r\r\n2\n,\n(7)\nL\ud835\udc34\ud835\udc45\ud835\udc34\ud835\udc43 =\n\ud835\udc38\n\u2211\ufe01\n\ud835\udc56=1\n\u2211\ufe01\n\ud835\udc57 \u2208\ud835\udc41\ud835\udc56\n\f\f\f||v\ud835\udc56 \u2212 v\ud835\udc57 ||2 \u2212 ||v\u2032\n\ud835\udc56 \u2212 v\u2032\n\ud835\udc57 ||2\n\f\f\f,\n(8)\nwhere \ud835\udc41\ud835\udc56 is the set of one-ring neighbours for vertex \ud835\udc63\ud835\udc56, \ud835\udc63\u2032 indicates\nthe vertex position in the last iteration. We set \ud835\udf06\ud835\udc59\ud835\udc4e\ud835\udc5d = 10\u22124 and\n\ud835\udf06\ud835\udc34\ud835\udc45\ud835\udc34\ud835\udc43 = 10\u22124 to balance them respectively.\nWe perform both the SDS Loss and mesh regularization terms\nduring optimization in each iteration. We found that optimizing\nthe SDS and regularization terms separately achieves better results\nempirically. Given a rendered view, we first optimize \ud835\udc53\ud835\udc50, \ud835\udc53\ud835\udc54, v of the\nediting region with the SDS loss. Then, \ud835\udc53\ud835\udc50 and \ud835\udc53\ud835\udc54 are fixed, and only\nv is optimized with the mesh regularization terms.\n5\nEXPERIMENTS\n5.1\nExperimental Setup\nDataset. To verify the effectiveness of our method in various scenes,\nwe select six scenes with different levels of complexity from four\ndatasets: DTU [Jensen et al. 2014], BlendedMVS [Yao et al. 2020],\nCo3D [Reizenstein et al. 2021], and GL3D [Shen et al. 2018]. These\nscenes include objects in simple backgrounds, human faces, and\noutdoor scenes with complex backgrounds. We use high-resolution\nimages and the corresponding camera poses from the respective\ndatasets to learn the original neural fields. Then, we edit the original\nscenes based on text prompts.\nBaselines. We compare with three baselines. (1) D-DreamFusion*:\nas pointed out by Instruct-N2N, DreamFusion fails to edit a neural\nfield due to the difficulty of finding an exact textual description that\nmatches a scene. To learn a better neural representation of a specific\nscene, we combine Stable-DreamFusion with DreamBooth [Ruiz\net al. 2022] as another baseline. (2) Instruct-NeRF2NeRF (Instruct-\nN2N): we also compare with a recent work Instruct-NeRF2NeRF and\nuse the text instructions provided by the paper [Haque et al. 2023]\nto edit a 3D scene. (3) NeRF-Art [Wang et al. 2023]: Since NeRF-Art\nonly supports stylized editing, we compare it in the stylization task.\nEvaluation Criteria. Following [Haque et al. 2023], we use the\nCLIP Text-Image directional similarity to evaluate the degree of\nalignment between the change in both the images and text prompts\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nZhuang et al.\nOriginal\nD-DreamFusion*\nOurs\n\u201cA \u2217 doll wearing a blue apron\u201d\n\u201cA \u2217 doll wearing a blue apron\u201d\n\u201cA \u2217 giraffe\u201d\n\u201cA \u2217 giraffe\u201d\n\u201cA \u2217 man wearing a cowboy hat\u201d\n\u201cA \u2217 man wearing a cowboy hat\u201d\nInstruct-N2N\n\u201cGive it a blue apron\u201d\n\u201cTurn the stone horse into a giraffe\u201d\n\u201cGive him a cowboy hat\u201d\nFigure 3: Visual results of our method compared with two baselines on three different scenes. The results clearly show that\nDreamEditor can precisely locate the relevant region, perform faithful editing to the text, and prevent undesirable modifications,\nwhich are difficult to be achieved by the baseline methods.\nand its detailed definition can be found in [Gal et al. 2022b]. For\neach editing result, we uniformly sample 50 viewpoints around the\nediting region and take the mean value as the result. Since the CLIP\ndirectional similarity can only approximately evaluate the edit-\ning quality, we additionally conduct user studies to obtain human\nevaluations. We distribute 50 copies of questionnaires, presenting\nrotation video results of all methods side by side and asking users\nto choose the best editing result. The voting rates are calculated for\neach method. We compare our method with the aforementioned\nbaselines in four selected scenes, covering a total of 20 distinct\nediting operations. We exclude NeRF-Art in the quantitative com-\nparison due to it only supports stylized editing.\nImplementation Details. In our experiments, we adopt NeuS\nto learn the original neural field. The training parameters can be\nfound in [Wang et al. 2021]. As for the diffusion model, we use\nthe public pretrained Stable Diffusion model V2. For each original\nneural field, we use the rendered images from the locating step,\napplying DreamBooth to fine-tune the Stable Diffusion model over\n500 iterations. In the distilling step, we use the Adam optimizer\nwith \ud835\udc59\ud835\udc5f = 10\u22124 to optimize the local fields for 100K iterations. In\nthe optimizing step, the size of the rendered images is gradually\nincreased from 96\u00d796 to 192\u00d7192. We set the Adam optimizer with\n\ud835\udc59\ud835\udc5f = 10\u22122 to optimize the \ud835\udc53\ud835\udc50, \ud835\udc53\ud835\udc54, v of vertices in the editing region\nfor 2K iterations. We implement our editing framework in Pytorch.\n5.2\nQualitative Results\nResults of Editing 3D Scenes. We provide qualitative results\nof our method in Fig.1 and Fig. 10. Results demonstrate that our\nmethod can effectively perform targeted editing of neural fields in\nvarious scenes. As depicted in the middle row of Fig.1, even in com-\nplex scenes such as outdoor gardens, our method can accurately\ndetermine the horse sculpture as the editing region, subsequently\nturning it into a deer or giraffe with high-quality textures and ge-\nometry. Moreover, our method is capable of local editing, such as\nwearing sunglasses for the dog in the bottom of Fig. 1. Notably, as\nshown in Fig. 7, the editing results produced by our method demon-\nstrate excellent consistency in 3D geometry, as can be intuitively\nobserved in the extracted mesh.\nFig.3 presents a comparison of the results of our method with\nbaselines. Instruct-N2N has difficulties in executing abstract op-\nerations (e.g. give an apron to a doll) and generates suboptimal\nresults in some scenes. This is largely attributed to the fact that the\nInstruct-Pix2Pix model is not always reliable, and it operates on\nthe full image. Therefore, Instruct-N2N changes the entire scene\nand may underperform when executing the instructions beyond\nthe Instruct-Pix2Pix training set. The DreamBooth finetuning in\nD-DreamFusion* enables the T2I diffusion model to roughly learn\nthe representation of the input objects, such as the toy in the first\nrow and the man in the third. However, due to the complexity and\ndiversity of real-world scenes, D-DreamFusion* cannot accurately\nrepresent a specific scene, leading the irrelevant regions of the\nscenes edited by D-DreamFusion* to change significantly, such as\nthe deformation of the doll in the first row, the background in the\nsecond row. Moreover, all compared baselines can not guarantee\nthe consistency of the scenes before and after editing in complex\nscenes (such as the garden in the second row), and their editing pro-\ncess may change the entire scene. In contrast, our method has more\ndetails and faithfully generates the content of the text prompts,\nwhile successfully maintaining the consistency of the input objects\nand scenes before and after editing.\nResults of stylization task. As shown in Fig.8, we compare our\nmethod with NeRF-Art and Instruct-N2N. In this task, we omit the\nDreamEditor: Text-Driven 3D Scene Editing with Neural Fields\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nTable 1: Results of the CLIP Text-Image Direction Loss and\nuser studies.\nMethod\nCLIP Text-Image\nDirection Similarity \u2191\nEditing performance\nvoting percentage \u2191\nD-DreamFusion*\n12.43\n12.1%\nInstruct-N2N\n10.86\n6.8%\nOurs\n18.49\n81.1%\n(1) w/o locating \n(2) Ours\nOriginal\nFigure 4: Ablation study of locating step. Editing without the\nlocating step will deform the doll, breaking the consistency\nof the object.\nlocating step to stylize the whole scene. Since stylization editing\nis a subjective task, we only provide the qualitative results as a\nreference and do not conduct quantitative analysis.\nResults of locating editing region. In Fig.6, we also show our\nmethod\u2019s results of locating editing regions. We can see that our\nmethod can locate reasonable editing regions precisely.\n5.3\nQuantitative Results\nIn Table 1, we present the results of the CLIP text-to-image di-\nrectional loss. The results clearly demonstrate that our method\nachieves significantly higher scores, indicating that our method\ngenerates shapes and textures that are clearer and more aligned\nwith the edited text prompts. Additionally, our method receives\nover 81.1% of the votes, surpassing the other methods by a signifi-\ncant margin. This further demonstrates DreamEditor can achieve\nmuch higher user satisfaction across various scenes.\n5.4\nAblation Study\nEffectiveness of locating step. To demonstrate the necessity of\nlocating step, we design two variants: (1) w/o locating: We omit\nthe locating step and optimize all local implicit fields on the mesh.\n(3) Our method: we determine the editing region through locating\nstep, and fix the non-editing region in optimization. As illustrated\nin Fig.4 (1), editing without the locating step will inadvertently\nchange irrelevant regions of the scene, such as shortening the doll\u2019s\narm, which destroys the consistency of the object. In contrast, the\nlocating step allows our framework to optimize exclusively the\nregion of interest.\nEffectiveness of optimizing approach. To evaluate whether our\noptimizing approach can generate more detailed 3D shapes during\noptimization, we ablate with three variants of DreamEditor as fol-\nlows: (1) Fixing v: fixing the mesh structure during the updating\nprocess, only optimizing the geometry features. (2) Fixing\ud835\udc53\ud835\udc54: only\nchanging the mesh structure without optimizing the geometry fea-\nture. (3) Our method: v and \ud835\udc53\ud835\udc54 are optimized simultaneously. We\nselect a challenging scene to evaluate: generating a rose on a cup.\n(1) Fixing v\n(2) Fixing \ud835\udc53\ud835\udc54\n(3) Ours\nFigure 5: Ablation study of optimizing approach. Obviously,\nsimultaneously optimizes both geometry features and vertex\npositions (Ours) and generates red roses with more detailed\nand realistic 3D shapes.\nWe present the rendered images of the generated results and the\nextracted 3D shape using the marching cubes algorithm in Fig. 5.\nFig.5 (1) displays the rose generated by fixing vertex positions,\nwhich are full of spikes. This is because, in regions far from the\nmesh surface, constraining the smoothness of the s-density of the\nsampling points across implicit fields is quite challenging. Fixing\ngeometry features, as shown in Fig.5 (2), can generate a rough shape\nbut lacks details. In contrast, our method simultaneously optimizes\nboth the geometric features and vertex positions, which eliminates\nthe spikes as well as generates more detailed buds and petals.\n6\nCONCLUSION AND LIMITATIONS\nIn this paper, we present DreamEditor, a text-driven framework\nfor editing 3D scenes represented by neural fields. Given a neural\nfield and text prompts describing the desired edits, DreamEditor\nautomatically identifies the editing region within the scene and\nmodifies its geometry and texture accordingly. Experiments across\na diverse range of scenes, including faces, objects, and large outdoor\nscenes, showcase the robust editing capabilities of DreamEditor to\ngenerate high-quality textures and shapes compared with other\nbaselines while ensuring the edited scene remains consistent with\nthe input text prompts.\nLimitations of DreamEditor include the Janus problem, an issue\ninherited from DreamFusion, where the generated object appears as\na front view from different viewpoints. Furthermore, DreamEditor\ndoes not directly model environmental lighting, which limits con-\ntrol over the lighting condition. While DreamEditor generally works\nwell, due to the dependence of rendered views in editing, its per-\nformance may suffer in the presence of significant self-occlusions\nin the scene, consequently impacting the final synthesis results.\nConsidering that NeuS faces difficulties in effectively reconstruct-\ning backgrounds in unbounded scenes, our current focus lies on\nobject-centric editing in the foreground of the scene. In the future\nwork, by combining recent unbounded real-world scene mesh re-\nconstruction methods, such as BakedSDF [Yariv et al. 2023], our\nmethod can be extended to the whole scene editing.\nACKNOWLEDGMENTS\nThis work was supported in part by the National Natural Science\nFoundation of China (NO. 62322608, 61976250), in part by the Open\nProject Program of State Key Laboratory of Virtual Reality Tech-\nnology and Systems, Beihang University (No.VRLAB2023A01), and\nin part by the Guangdong Basic and Applied Basic Research Foun-\ndation (NO. 2020B1515020048).\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nZhuang et al.\n\"\ud835\udc4e \ud835\udc8d\ud835\udc82\ud835\udc96\ud835\udc88\ud835\udc89\ud835\udc8a\ud835\udc8f\ud835\udc88\n\u2217 \ud835\udc51\ud835\udc5c\ud835\udc54\"\n\"\ud835\udc4e \u2217 \ud835\udc5a\ud835\udc4e\ud835\udc5b \ud835\udc64\ud835\udc56\ud835\udc61\u210e\n\ud835\udc8e\ud835\udc96\ud835\udc94\ud835\udc95\ud835\udc82\ud835\udc84\ud835\udc89\ud835\udc86\"\n\"\ud835\udc4e \u2217 \ud835\udc51\ud835\udc5c\ud835\udc59\ud835\udc59 \ud835\udc64\ud835\udc52\ud835\udc4e\ud835\udc5f\ud835\udc56\ud835\udc5b\ud835\udc54\n\ud835\udc94\ud835\udc96\ud835\udc8f\ud835\udc88\ud835\udc8d\ud835\udc82\ud835\udc94\ud835\udc94\ud835\udc86\ud835\udc94\"\nFigure 6: Visualization of the editing region, where the bold\nwords indicate keywords and the red area on the mesh repre-\nsents the editing region.\nOriginal\n\u201cA \u2217 deer\u201d\n\u201cA \u2217 giraffe\u201d\nFigure 7: Visualization of the extracted mesh from our editing\nresults.\nNeRF-Art\nInstruct-N2N\nOurs\nOriginal\n\u201cVincent van Gogh\u201d\n\u201cFauvism\u201d\n\u201cLord Voldemort\u201d\n\u201cTolkien Elf\u201d\nFigure 8: Visualization of the stylization editing results, we compare with NeRF-Art and Instruct-NeRF2NeRF.\n\ud835\udefe(\ud835\udc1d)\n27\n\u0de9\ud835\udc53\ud835\udc54\n128\n\ud835\udc65\n\ud835\udc1d\naggregating \nfeature\n\u0de9\ud835\udc53\ud835\udc50\n128\n\ud835\udefe(\u121a\u210e)\n17\n256\n256\n256\n256\n256\n\ud835\udc50\n\ud835\udc60\n\u2207\ud835\udc65\ud835\udc60\nFigure 9: The network of the mesh-based neural fields. It takes the sampled point \ud835\udc65 and the ray direction d as input, output the\ns-density \ud835\udc60 and color \ud835\udc50. \ud835\udefe(\u00b7) denotes positional encoding adopted in NeRF [Mildenhall et al. 2021].\nDreamEditor: Text-Driven 3D Scene Editing with Neural Fields\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u201cA \u2217 gold doll\u201d\nOriginal  Neural Field\n\u201cA \u2217 robot horse\u201d\n\u201cA \u2217 zebra\u201d\n\u201cA \u2217 horse with rainbow hair\u201d\n\u201cA \u2217 giraffe\u201d\n\u201cA \u2217 deer\u201d\nOriginal  Neural Field\nOriginal  Neural Field\n\u201cA \u2217 Ironman doll\u201d\n\u201cA \u2217 doll wearing sunglasses\u201d\n\u201cA \u2217 doll wearing a red top hat\u201d\n\u201cA \u2217 gold doll\u201d\n\u201cA \u2217 doll with tiger pattern\u201d\n\u201cA \u2217 man wearing sunglasses\u201d\n\u201cA \u2217 man wearing a navy hat\u201d\n\u201cA \u2217 man wearing a cowboy hat\u201d\n\u201cA \u2217 clown man\u201d\n\u201cA \u2217 man with moustache\u201d\nFigure 10: More editing results.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nZhuang et al.\nREFERENCES\nOmri Avrahami, Dani Lischinski, and Ohad Fried. 2022. Blended diffusion for text-\ndriven editing of natural images. In CVPR 2022. 18208\u201318218.\nChong Bao, Yinda Zhang, and Bangbang et al. Yang. 2023. Sine: Semantic-driven image-\nbased nerf editing with prior-guided editing field. In CVPR 2023. 20919\u201320929.\nTim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix: Learning\nto follow image editing instructions. arXiv preprint arXiv:2211.09800 (2022).\nJianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao Bao, Xu Jia, and Huchuan\nLu. 2021. Animatable neural radiance fields from monocular rgb videos. arXiv\npreprint arXiv:2106.13629 (2021).\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling\nGeometry and Appearance for High-quality Text-to-3D Content Creation. arXiv\npreprint arXiv:2303.13873 (2023).\nYongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. 2022. Tango: Text-\ndriven photorealistic and robust 3d stylization via lighting decomposition. arXiv\npreprint arXiv:2210.11277 (2022).\nGuillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022.\nDiffedit: Diffusion-based semantic image editing with mask guidance. arXiv preprint\narXiv:2210.11427 (2022).\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022a. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint arXiv:2208.01618 (2022).\nRinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel\nCohen-Or. 2022b. StyleGAN-NADA: CLIP-guided domain adaptation of image\ngenerators. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313.\nWilliam Gao, Noam Aigerman, Thibault Groueix, Vladimir G Kim, and Rana Hanocka.\n2023. TextDeformer: Geometry Manipulation using Text Guidance. arXiv preprint\narXiv:2304.13348 (2023).\nAmos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. 2020. Implicit\ngeometric regularization for learning shapes. arXiv preprint arXiv:2002.10099 (2020).\nAyaan Haque, Matthew Tancik, Alexei A Efros, Aleksander Holynski, and Angjoo\nKanazawa. 2023. Instruct-NeRF2NeRF: Editing 3D Scenes with Instructions. arXiv\npreprint arXiv:2303.12789 (2023).\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-Or. 2022. Prompt-to-prompt image editing with cross attention control.\narXiv preprint arXiv:2208.01626 (2022).\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. NeurIPS 2020 33 (2020), 6840\u20136851.\nAjay Jain, Ben Mildenhall, Jonathan T Barron, and et al. 2022. Zero-shot text-guided\nobject generation with dream fields. In CVPR 2022. 867\u2013876.\nRasmus Jensen, Anders Dahl, George Vogiatzis, Engin Tola, and Henrik Aan\u00e6s. 2014.\nLarge scale multi-view stereopsis evaluation. In CVPR 2014. 406\u2013413.\nBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar\nMosseri, and Michal Irani. 2022. Imagic: Text-based real image editing with diffusion\nmodels. arXiv preprint arXiv:2210.09276 (2022).\nSosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. 2022. Decomposing nerf\nfor editing via feature field distillation. arXiv preprint arXiv:2205.15585 (2022).\nYuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, and Shenlong Wang. 2022. Cli-\nmateNeRF: Physically-based Neural Rendering for Extreme Climate Synthesis.\narXiv e-prints (2022), arXiv\u20132211.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2022. Magic3D: High-\nResolution Text-to-3D Content Creation. arXiv preprint arXiv:2211.10440 (2022).\nHao-Kang Liu, I Shen, Bing-Yu Chen, et al. 2022. NeRF-In: Free-form NeRF inpainting\nwith RGB-D priors. arXiv preprint arXiv:2206.04901 (2022).\nLingjie Liu, Jiatao Gu, Kyaw Zaw Lin, and et al. 2020. Neural sparse voxel fields.\nNeurIPS 2020 33 (2020), 15651\u201315663.\nSteven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan\nRussell. 2021. Editing conditional radiance fields. In ICCV 2021. 5773\u20135783.\nWilliam E Lorensen and Harvey E Cline. 1987. Marching cubes: A high resolution\n3D surface construction algorithm. ACM siggraph computer graphics 21, 4 (1987),\n163\u2013169.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2022.\nLatent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. arXiv\npreprint arXiv:2211.07600 (2022).\nOscar Michel, Roi Bar-On, Richard Liu, and et al. 2022. Text2mesh: Text-driven neural\nstylization for meshes. In CVPR 2022. 13492\u201313502.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2021. Nerf: Representing scenes as neural radiance fields\nfor view synthesis. Commun. ACM 65, 1 (2021), 99\u2013106.\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. 2022.\nCLIP-Mesh: Generating textured meshes from text using pretrained image-text\nmodels. In SIGGRAPH Asia 2022. 1\u20138.\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. 2022. Instant\nneural graphics primitives with a multiresolution hash encoding. ACM Transactions\non Graphics (ToG) 41, 4 (2022), 1\u201315.\nAtsuhiro Noguchi, Xiao Sun, Stephen Lin, and Tatsuya Harada. 2021. Neural articulated\nradiance field. In ICCV 2021. 5762\u20135772.\nJulian Ost, Issam Laradji, Alejandro Newell, and et al. 2022. Neural point light fields.\nIn CVPR 2022. 18419\u201318429.\nSida Peng, Yuanqing Zhang, Yinghao Xu, and et al. 2021. Neural body: Implicit neural\nrepresentations with structured latent codes for novel view synthesis of dynamic\nhumans. In CVPR 2021. 9054\u20139063.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. 2022. Dreamfusion:\nText-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988 (2022).\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. 2017. Pointnet: Deep learning\non point sets for 3d classification and segmentation. In CVPR 2017. 652\u2013660.\nAmit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall,\nShiran Zada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. 2023. Dream-\nBooth3D: Subject-Driven Text-to-3D Generation. arXiv preprint arXiv:2303.13508\n(2023).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, and et al. 2021. Common\nobjects in 3d: Large-scale learning and evaluation of real-life 3d category recon-\nstruction. In ICCV 2021. 10901\u201310911.\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTexture: Text-guided texturing of 3d shapes. arXiv preprint arXiv:2302.01721 (2023).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In CVPR 2022.\n10684\u201310695.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for\nsubject-driven generation. arXiv preprint arXiv:2208.12242 (2022).\nChitwan Saharia, William Chan, and Saurabh et al. Saxena. 2022. Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS 2022 35\n(2022), 36479\u201336494.\nEtai Sella, Gal Fiebelman, Peter Hedman, and Hadar Averbuch-Elor. 2023. Vox-E:\nText-guided Voxel Editing of 3D Objects. arXiv preprint arXiv:2303.12048 (2023).\nTianwei Shen, Zixin Luo, Lei Zhou, and et al. 2018. Matchable image retrieval by\nlearning from surface reconstruction. In ACCV 2018. Springer, 415\u2013431.\nJiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion implicit\nmodels. arXiv preprint arXiv:2010.02502 (2020).\nRobert W Sumner, Johannes Schmid, and Mark Pauly. 2007. Embedded deformation\nfor shape manipulation. In ACM siggraph 2007 papers. 80\u2013es.\nAshish Vaswani, Noam Shazeer, Niki Parmar, and et al. 2017. Attention is all you need.\nNeurIPS 2017 30 (2017).\nCan Wang, Menglei Chai, Mingming He, and et al. 2022a. Clip-nerf: Text-and-image\ndriven manipulation of neural radiance fields. In CVPR 2022. 3835\u20133844.\nCan Wang, Ruixiang Jiang, Menglei Chai, Mingming He, Dongdong Chen, and Jing\nLiao. 2023. Nerf-art: Text-driven neural radiance fields stylization. IEEE Transactions\non Visualization and Computer Graphics (2023).\nChen Wang, Xian Wu, Yuan-Chen Guo, and et al. 2022c. NeRF-SR: High Quality Neural\nRadiance Fields using Supersampling. In ACM MM 2022. 6445\u20136454.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich.\n2022b. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D\nGeneration. arXiv preprint arXiv:2212.00774 (2022).\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping\nWang. 2021. Neus: Learning neural implicit surfaces by volume rendering for\nmulti-view reconstruction. arXiv preprint arXiv:2106.10689 (2021).\nFanbo Xiang, Zexiang Xu, Milos Hasan, and et al. 2021. Neutex: Neural texture mapping\nfor volumetric neural rendering. In CVPR 2021. 7119\u20137128.\nTianhan Xu and Tatsuya Harada. 2022. Deforming radiance fields with cages. In\nComputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n23\u201327, 2022, Proceedings, Part XXXIII. Springer, 159\u2013175.\nBangbang Yang, Chong Bao, and Junyi et al. Zeng. 2022. Neumesh: Learning disentan-\ngled neural mesh-based implicit field for geometry and texture editing. In ECCV\n2022. Springer, 597\u2013614.\nYao Yao, Zixin Luo, Shiwei Li, and et al. 2020. BlendedMVS: A Large-scale Dataset for\nGeneralized Multi-view Stereo Networks. In CVPR 2020.\nLior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard\nSzeliski, Jonathan T Barron, and Ben Mildenhall. 2023. BakedSDF: Meshing Neural\nSDFs for Real-Time View Synthesis. arXiv preprint arXiv:2302.14859 (2023).\nYu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, and et al. 2022. NeRF-editing: geometry\nediting of neural radiance fields. In CVPR 2022. 18353\u201318364.\n"
  }
]