[
  {
    "title": "Drivable 3D Gaussian Avatars",
    "link": "https://arxiv.org/pdf/2311.08581.pdf",
    "upvote": "44",
    "text": "Drivable 3D Gaussian Avatars\nWojciech Zielonka3,1\u2217, Timur Bagautdinov1, Shunsuke Saito1,\nMichael Zollh\u00a8ofer1, Justus Thies2,3, Javier Romero1\n1Meta Reality Labs Research\n2Technical University of Darmstadt\n3Max Planck Institute for Intelligent Systems, T\u00a8ubingen, Germany\nhttps://zielon.github.io/d3ga/\nFigure 1. Given a multi-view video, D3GA learns drivable photo-realistic 3D human avatars, represented as a composition of 3D Gaussians\nembedded in tetrahedral cages. The Gaussians are transformed by those cages, colorized with an MLP, and rasterized as splats. We represent\nthe drivable human as a layered set of 3D Gaussians, allowing us to decompose the avatar into its different cloth layers.\nAbstract\nWe present Drivable 3D Gaussian Avatars (D3GA), the\nfirst 3D controllable model for human bodies rendered with\nGaussian splats. Current photorealistic drivable avatars\nrequire either accurate 3D registrations during training,\ndense input images during testing, or both. The ones based\non neural radiance fields also tend to be prohibitively slow\nfor telepresence applications. This work uses the recently\npresented 3D Gaussian Splatting (3DGS) technique to ren-\nder realistic humans at real-time framerates, using dense\ncalibrated multi-view videos as input.\nTo deform those\nprimitives, we depart from the commonly used point de-\nformation method of linear blend skinning (LBS) and use\na classic volumetric deformation method: cage deforma-\ntions. Given their smaller size, we drive these deformations\nwith joint angles and keypoints, which are more suitable for\ncommunication applications. Our experiments on nine sub-\njects with varied body shapes, clothes, and motions obtain\nhigher-quality results than state-of-the-art methods when\nusing the same training and test data.\n\u2217Work done while Wojciech Zielonka was an intern at Reality Labs\nResearch, Pittsburgh, PA, USA\n1. Introduction\nIn the nineteenth century, the Anonymous Society of\nPainters, Sculptors, Printmakers, etc. started the art move-\nment called Impressionism, identified by a technique of\n\u201cshort, broken brushstrokes that barely convey forms\u201d. Our\ngoal, to create photorealistic representations of humans, is\none of the things that impressionists ran away from. How-\never, in D3GA1 , we use Gaussian splats as a modern ver-\nsion of those short brushstrokes to conform to the structure\nand appearance of our real-time, reposable avatars.\nCreating drivable (i.e., that can be animated to gener-\nate new content) photorealistic humans currently requires\ndense multi-view data since monocular approaches lack ac-\ncuracy. Additionally, existing techniques rely on complex\npre-processing, including precise 3D registrations [1, 55,\n56]. However, obtaining those registrations requires iter-\native methods that are difficult to integrate into end-to-end\npipelines. Other methods that do not require accurate reg-\nistrations [4] are based on neural radiance fields (NeRFs).\nThey are typically too slow for real-time rendering (with\nfew exceptions [30]) or struggle with garment animations.\n1Referring to Edgar Degas (pronunciation: ed\u00b7gr duh\u00b7gaa), a French\nimpressionist artist known for his pastel drawings and oil paintings.\n1\narXiv:2311.08581v1  [cs.CV]  14 Nov 2023\nIn recent work, Kerbl et al.\nintroduced 3D Gaussian\nSplatting (3DGS) [14] based on the classic rendering ap-\nproach Surface Splatting [66]. This representation renders\nhigher-quality images at a faster framerate than state-of-the-\nart methods based on neural radiance fields [30] and does\nnot require any highly accurate 3D initialization.\nUnfortunately, 3DGS was designed for static scenes.\nTime-conditioned Gaussian Splatting [54,59] are proposed\nfor rendering dynamic scenes.\nHowever, similar to [7],\nthese models can only replay previously observed content,\nmaking them unsuitable for representing novel motion.\nFollowing approaches on drivable NeRFs [53, 54], we\nmodel the 3D human appearance and deformations in a\ncanonical space but rely on 3D Gaussians instead of ra-\ndiance fields. In addition to better performance, Gaussian\nsplats do not require camera ray sampling heuristics.\nDrivable NeRFs typically rely on LBS to transform\npoints between canonical and observation spaces.\nHow-\never, D3GA models humans with volumetric primitives in\nthe form of 3D Gaussians and therefore needs to map vol-\numes to canonical space. Instead of LBS, our method uses\nanother classic deformation model suitable for transform-\ning volumes: cages [31]. Deforming cages in canonical\nspace entails a deformation gradient, which can be applied\ndirectly to the 3D Gaussians in our representation.\nOur\nmethod follows a compositional structure based on separate\nbody, face, and garment cages, allowing us to render those\nparts independently.\nThe remaining question is defining the signal that trig-\ngers those cage deformations. The current state-of-the-art\nin drivable avatars [41,57] requires dense input signals like\nRGB-D images or even multi-camera setups, which might\nnot be suitable for low-bandwidth connections in telepres-\nence applications. We adopt a more compact input based\non the human pose, comprising skeletal joint angles in the\nform of quaternions and 3D facial keypoints.\nWe train person-specific models on nine high-quality\nmulti-view sequences with a wide range of body shapes,\nmotion, and clothing (not limited to tight-fitting), which\nlater can be driven with new poses from any subject.\nIn summary, we present Drivable 3D Gaussian Avatars\n(D3GA) with the following contributions:\n\u2022 The first implementation of Drivable 3D Gaussians\nSplatting (3DGS), applied to digital avatars.\n\u2022 Tetrahedral cage-based deformations applied to 3DGS.\n\u2022 State-of-the-art pose-based avatar generation for dense\nmulti-view scenes without ground truth registration.\n2. Related Work\nCurrent methods for controllable avatars are primarily\nbased on dynamic Neural Radiance Fields (NeRF) [28,\n33, 34], point-based [25, 58, 62] or hybrid representations\n[1, 4, 22, 64] which are either slow to render or do not cor-\nrectly disentangle garment from the body; thus, incorrectly\ngeneralize to new poses. For a thorough overview, we point\nthe reader to state-of-the-art reports [48, 49, 65] on digital\navatars and neural rendering.\nDynamic Neural Radiance Fields NeRF [29] is a preva-\nlent appearance model for human avatars.\nIt represents\nthe scene volumetrically by storing density and color in-\nformation in space using a multi-layer perceptron (MLP).\nImages can be rendered from this representation by us-\ning ray casting with volumetric integration of the sample\npoints [13]. Many methods successfully applied NeRF to\ndynamic scenes [5,20,33,34,37,53,58,64] achieving high\nquality results. However, most of the methods treat avatars\nas a single layer [19, 28, 35, 43\u201345, 63], meaning there is\nno separation between garment and body. This is partic-\nularly cumbersome for modeling phenomena like sliding\nor loose garments. Methods like [3, 4] try to solve this\nproblem using a hybrid representation.\nThey combined\nexplicit geometry from SMPL [23] and implicit dynamic\nNeRF by integrating the mesh surface into the rendering\nequation. Despite impressive garment reconstruction, these\nmethods struggle with novel pose prediction. TECA [60]\nlifts SCARF to a generative framework that enables prompt-\nbased generation of NeRF-based accessories and hairstyles.\nPoint Based Rendering Before 3DGS, many methods used\npoint-based rendering [25, 44, 62] or sphere splatting [18],\nwhich, similarly to 3DGS, have optimizable positions and\nsizes. NPC by Su et al. [44] defines a point-based body\nmodel for avatar representation. Their model requires the\nevaluation of the nearest neighbor search per ray sample\nduring training, which results in long training times (12h\ninstead of 30 minutes for our model on a similar dataset\nsize), making it impractical for dense multi-view datasets.\nMa et al. [25] represent garment as a pose-dependent func-\ntion that maps a set of points from SMPL [23] to the cloth-\ning space. This idea is improved in [38], where a neural\ndeformation field replaces LBS. However, both models re-\nstrict themselves to model only geometry, not appearance.\nZheng et al. [62] represent the upper part of an avatar as\na point cloud, which is progressively grown during the op-\ntimization and rasterized using a differentiable point cloud\nrenderer [52]. Despite achieving photorealistic results lo-\ncally, the avatars suffer from artifacts like holes around low-\ndensity regions.\nCage Based Deformations Cages [31] are commonly used\nfor geometry modeling and animation.\nThey serve as a\nsparse proxy that controls all points in their interior, en-\nabling efficient deformation since only cage nodes have to\nbe controlled to rig the object inside. Yifan et al. [51] in-\ntroduced the concept of neural cages for detail-preserving\nshape deformation.\nThe network learns how to rig the\n2\nFigure 2. Overview. D3GA uses 3D pose \u03d5\u03d5\u03d5, face embedding \u03ba\u03ba\u03ba, viewpoint dk and canonical cage v (as well as auto-decoded color\nfeatures hi) to generate the final render \u00afC and auxiliary segmentation render \u00afP. The inputs in the left are processed through three networks\n(\u03a8MLP, \u03a0MLP, \u0393MLP) per avatar part to generate cage displacements \u2206v, Gaussians deformations bi, qi, si and color/oppacity ci, oi\nrespectively. After cage deformations transform canonical Gaussians, they are rasterized into the final images according to Eq. 9.\nsource object into the target through a proxy regressed by\na neural network. Garbin et al. [6] extended dynamic NeRF\nwith tetrahedron cages to facilitate the unposing of ray sam-\nples based on tetrahedron intersections. The method is real-\ntime, high-quality, and controllable. However, their results\nare limited to objects with local deformations like heads,\nmaking them not applicable to highly articulatable objects\nlike our full-body avatars. Peng et al. also used a cage\nto deform a radiance field in CageNeRF [36]. Their low-\nresolution cages can be applied to full-body avatars but fail\nto model details like faces or other complex deformations.\nGaussian Splatting D3GA is based on 3D Gaussians Splat-\nting (3DGS) [14], a recent alternative approach to NeRF\nthat achieves high quality and real-time rendering speed.\n3DGS is based on 3D Gaussians, a differentiable volumetric\nrepresentation that can be efficiently rasterized in compari-\nson to expensive ray marching used by NeRF. The recently\nintroduced Dynamic 3DGS [24] enables per-frame dense 6-\nDOF tracking and novel view synthesis by optimizing the\ntrajectories of the 3D Gaussians from frame ti to ti+1. Our\nmethod extends static or playback 3DGS [14, 24] to driv-\nable applications using a volumetric cage as a deformation\nproxy, enabling controlling digital avatars.\n3. Method\nCurrent methods for dynamic volumetric avatars either\nmap points from deformed to canonical space [9, 10, 21,\n39, 39, 64] or they relay on the forward mapping only\n[2, 19, 35, 44, 50, 62]. Methods based on backward map-\nping tend to accumulate errors in canonical space since they\nrequire an error-prone backward pass and have problems\nmodeling view-dependent effects since mapping the view\nvector to canonical space uniquely is non-trivial. There-\nfore, we decided to employ a forward-only mapping. D3GA\nis built on 3DGS extended by a neural representation and\ntetrahedral cages to model the color and geometry of each\ndynamic part of the avatar, respectively. In the following,\nwe introduce the formulation of 3D Gaussian Splatting and\ngive a detailed description of our method D3GA.\n3.1. 3D Gaussian Splatting\n3D Gaussian Splatting (3DGS) [14] is designed for real-\ntime novel view synthesis in multi-view static scenes. Their\nrendering primitives are scaled 3D Gaussians [17, 52] with\na 3D covariance matrix \u03a3 and mean \u00b5:\nG(x) = e\u2212 1\n2 (x\u2212\u00b5)T \u03a3\u22121(x\u2212\u00b5).\n(1)\nTo splat the Gaussians, Zwicker et al. [66] define the pro-\njection of 3D Gaussians onto the image plane as:\n\u03a3\u2032 = AW\u03a3WT AT ,\n(2)\nwhere \u03a3\u2032 is a covariance matrix in 2D space, W is the view\ntransformation, and A is the Jacobian of the affine approx-\nimation of the projective transformation. During optimiza-\ntion, enforcing the positive semi-definiteness of the covari-\nance matrix \u03a3 is challenging. To avoid this, Kerbl et al. [14]\nuse an equivalent formulation of a 3D Gaussian as a 3D el-\nlipsoid parameterized with a scale S and rotation R:\n\u03a3 = RSST RT .\n(3)\n3DGS uses spherical harmonics [40] to model the view-\ndependent color of each Gaussian. In practice, appearance\nis modeled with an optimizable 48 elements vector repre-\nsenting four bands of spherical harmonics.\n3.2. Cage Based Deformation Transfer\nTo deform 3D Gaussians, we leverage tetrahedron cage-\nbased deformations as a coarse proxy for the body, face, and\nindividual garments. To create a cage per garment, we seg-\nment all images of a single time instance using an Efficient-\nNet [47] backbone with PointRend [16] refinement, trained\non a corpus of similar multi-view captures. The per-image\n2D segmentation masks are projected onto a body mesh \u02c6M\n3\nto obtain per-triangle labels (body, upper, lower). To get the\nmesh \u02c6M, we fit a low-resolution LBS model to a single 3D\nscan of the subject and then fit such model to the segmented\nframe by minimizing the distance to the 3D keypoints, ex-\ntracted with an EfficientNet trained on similar captures. We\ntransform the body mesh into canonical space with LBS and\ndivide it into body part templates Mk. The garment meshes\nare additionally inflated 3cm along the vertex normals. Af-\nter that, we use TetGen [42] to turn the unposed meshes Mk\ninto tetrahedral meshes Tk. Consequently, cages for gar-\nments are hollow, containing only their outer layer, while\nthe body cage is solid. The face cage is composed of the\nbody tetrahedra which contains triangles defined as the face\non the LBS template. The cage nodes are deformed accord-\ning to LBS weights transferred from the closest vertex in\nMk. While classic cage methods typically deform the vol-\nume according to complex weight definitions [8, 11, 12],\nusing linear weights works well in practice when cage cells\nare small, making it easier to integrate into an end-to-end\ntraining system. Specifically, if we define vij as the ver-\ntices of tetrahedron i in canonical space, any point x inside\nthis tetrahedron can be defined in terms of its barycentric\ncoordinates bj:\nx =\n4\nX\nj=1\nbjvij.\n(4)\nWhen tetrahedra are transformed to posed space accord-\ning to \u02c6vij = LBS(vij,\u03d5\u03d5\u03d5, wij), where \u03d5\u03d5\u03d5 is the pose and\nwij the blendweights, the same linear relation holds \u02c6x =\nP4\nj=1 bj \u02c6vij. To extend this transformation from points to\nvolumes, we use the deformation transfer [46] as:\nJiEi = \u02c6Ei,\n(5)\nJi = \u02c6EiE\u22121\ni ,\n(6)\nwhere \u02c6Ei \u2208 R3\u00d73 and Ei \u2208 R3\u00d73 contain three edges from\ntetrahedron i defined in deformed and canonical spaces, re-\nspectively. In the following subsection, we will explore how\nto use the deformation gradients closed form solution in\nEq. 6 for transforming 3D Gaussians.\n3.3. Drivable Gaussian Avatars\nWe initialize a fixed number of Gaussians, whose 3D\nmeans \u00b5 are sampled on the surface of \u02c6M. The rotation\nof each Gaussian is initialized so that the first two axes are\naligned with the triangle surface and the third one with the\nnormal: this is a good approximation for a smooth surface.\nThe scale is initialized uniformly across a heuristic range\ndepending on inter-point distances as in [14]. Finally, we\ncan assign each sampled position x to the intersecting tetra-\nhedron and compute its barycentric coordinates b \u2208 R4. To\ndeform the tetrahedron volume, we incorporate the defor-\nmation gradient J defined in Eq. 6 into the Gaussian covari-\nance matrix from Eq. 3. The final covariance matrix passed\nto the rasterizer is denoted as:\n\u02c6\u03a3 = Ji\u03a3JT\ni ,\n(7)\nwhere Ji is the deformation gradient of the tetrahedron con-\ntaining the 3D mean of the Gaussian with covariance \u03a3.\nThis way, we transfer the deformation into the Gaussians,\nimproving modeling phenomena like garment stretching.\nEach part of the avatar (the garment,\nbody,\nor\nface) is controlled by a separate GaussianNet GNet\n=\n{\u0393MLP, \u03a0MLP, \u03a8MLP} which is defined as a set of small spe-\ncialized multi-layer perceptrons (MLP) parametrized as:\n\u03a8MLP : {\u03d5\u03d5\u03d5, encpos(v)} \u2192 \u2206v,\n\u03a0MLP : {\u03d5\u03d5\u03d5, bi, qi, si} \u2192 {\u2206bi, \u2206si, \u2206qi},\n\u0393MLP : {\u03d5\u03d5\u03d5, encview(dk), hi, fj} \u2192 {ci, oi}.\n(8)\nAll the networks take joint angles \u03d5\u03d5\u03d5 (or face encodings \u03ba\u03ba\u03ba for\nthe face networks) as inputs, in addition to network-specific\nconditioning. The cage node correction network \u03a8MLP takes\npositional encodings [29] for all canonical vertices to trans-\nform them into offsets of the cage node positions similar\nto SMPL [23] pose-correctives. To adapt our representa-\ntion further to the pose, the Gaussian correction network\n\u03a0MLP takes additionally the canonical Gaussian parameters\n(barycentric coordinates bi \u2208 R4, rotation qi \u2208 R4 and\nscale si \u2208 R3) to predict corrections of those same param-\neters. These two networks are necessary to capture high-\nfrequency details outside the parametric transformation.\nIn terms of appearance, the shading network \u0393MLP trans-\nforms information about the encoded view direction and\ninitial color into final color and opacity ci, oi. We depart\nfrom 3DGS color representation based on Spherical Har-\nmonics to enable pose-dependent color, which is neces-\nsary to model self-shadows and wrinkles in garments. The\nview angle information is represented as its projection to the\nfirst four spherical harmonics bands encpos(\u00b7). At the same\ntime, the initial color is an auto-decoded [32] feature vec-\ntor hi \u2208 R48. Moreover, the face region uses as input face\nembeddings \u03ba\u03ba\u03ba instead of pose \u03d5\u03d5\u03d5. A small auxiliary MLP re-\ngresses \u03ba\u03ba\u03ba based on 150 3D keypoints k normalized by their\ntraining mean and standard deviations. Finally, we also add\nan embedding vector with the timeframe of the current sam-\nple [27]. This allows D3GA to explain away properties that\ncannot be modeled (e.g., cloth dynamics) from our input,\neffectively avoiding excessive blur due to averaging residu-\nals. During testing, the average training embedding is used.\n3.4. Training Objectives\nAs in 3DGS [14], we define the color \u00afC of pixel (u, v):\n\u00afCu,v =\nX\ni\u2208N\nci\u03b1i\ni\u22121\nY\nj=1\n(1 \u2212 \u03b1i),\n(9)\n4\nGround Truth\nOurs\nw/o Cage\nw/ SH\nw/o LGarment\nw/o LNeo\nSingle Layer\nFigure 3. D3GA ablation: shape errors without cage deformations, view-dependent color artifacts with SH, shape smoothness without\nLGarment, or sliding artifacts with single layer. No Neo-Hookean loss results in reasonable color, but degenerate cage geometry (Fig.7).\nwhere ci is the color predicted by \u0393MLP, which replaces the\nspherical harmonics in 3DGS. \u03b1i is computed as the product\nof the Gaussian density in Eq. 1 with covariance matrix \u03a3\u2032\nfrom Eq. 2 and the learned per-point opacity oi predicted\nby \u0393MLP. The sum is computed over set N, the Gaussians\nwith spatial support on (u, v). The primary loss in D3GA is\na weighted sum of three different color losses applied to the\nestimated image \u00afC and the ground truth RGB image C:\nLColor = (1 \u2212 \u03c9)L1 + \u03c9LD-SSIM + \u03b6LVGG,\n(10)\nwhere \u03c9 = 0.2, \u03b6 = 0.005 (after 400k iterations steps and\nzero otherwise), LD-SSIM is a structural dissimilarity loss,\nand LVGG is the perceptual VGG loss.\nTo encourage correct garment separation, we introduce a\ngarment loss. Since each Gaussian i is statically assigned to\na part, we define pi as a constant-per-part color and conse-\nquently render \u00afP by replacing ci by pi in Eq. 9. Then, we\ncompute the L1 norm between predicted parts \u00afP and ground\ntruth segmentations P, LGarment = L1(\u00afP, P). Moreover,\nwe are using the Neo-Hookean loss based on Macklin et\nal. [26] to enforce the regularization of the predicted tetra-\nhedra for the regions with low supervision signal:\nLNeo = 1\nN\nN\nX\ni=0\n\u03bb\n2 (det(Ji) \u2212 1)2 + \u00b5\n2\n\u0000tr(JT\ni Ji) \u2212 3\n\u0001\n,\n(11)\nwhere Ji denotes the deformation gradient between a\ncanonical and a deformed tetrahedron (Eq. 6), N is the total\nnumber of tetrahedrons, and \u03bb and \u00b5 are the Lam\u00b4e parame-\nters [26]. The overall loss is defined as:\nL = \u03bdLColor + \u03bdLGarment + \u03c4LNeo,\n(12)\nwhere \u03bd = 10 and \u03c4 = 0.005 balance the different losses.\nWe implemented D3GA based on the differentiable\n3DGS renderer [14]. The networks \u03a0MLP,\u03a8MLP, \u0393MLP have\nthree hidden layers with 128 neurons and ReLU activation\nfunctions. In our experiments, we train the networks for\n700k steps with a multi-step scheduler with a decay rate of\n0.33, a batch size of one, and using the Adam optimizer [15]\nwith a learning rate set to 5e\u22124. We ran all experiments on\na single Nvidia V100 GPU with 1024 \u00d7 667 images.\n4. Dataset\nOur dataset consists of nine subjects performing differ-\nent motions, observed by 200 cameras.\nWe use 12000\nframes for training (at 10 FPS) and 1500 for testing (at 30\nFPS). The images were captured in a multi-view studio with\nsynchronized cameras at a resolution of 4096 \u00d7 2668, but\nthey were downsampled to 1024\u00d7667 to reduce the compu-\ntational cost. We use 2D segmentation masks, RGB images,\nkeypoints, and 3D joint angles for training as well as a sin-\ngle registered mesh to create our template \u02c6M.\n5\nGround Truth\nOurs\nMVP (LBS) [22]\nMVP (FFD) [22]\nDVA [41]\nBD (LBS) [1]\nBD (FFD) [1]\nFigure 4. Qualitative comparisons show that D3GA models garments better than other SOTA approaches, especially loose ones like skirts\nor sweatpants. FFD stands for free form deformation meshes, which contain a much richer training signal than LBS meshes (see Fig. 9).\n6\nFigure 5. Our method allows multilayer garment decompositions.\n5. Results\nWe evaluate and compare our method w.r.t.\nstate-of-\nthe-art multiview-based methods [1, 22, 41]. We compare\nD3GA to the full-body avatar methods BodyDecoder (BD)\n[1], MVP-based avatar [22, 41], and DVA [41], which uses\ndense image conditioning from all cameras. For a fair com-\nparison, we used two types of geometry training input for\nBD and MVP (see Fig. 9): meshes with a simple LBS\nmodel tracked with body keypoints, and detailed registra-\ntion meshes where vertices are optimized freely to match\nthe 3D reconstruction of each frame (also called free-form\ndeformation, FFD). Note that BodyDecoder also uses an\nambient occlusion approximation [1] extracted from FFD\nmeshes.\n5.1. Image Quality Evaluation\nD3GA is evaluated w.r.t. SSIM, PSNR, and the percep-\ntual metric LPIPS [61]. Table 1 shows that our method is the\none that achieves the best PSNR and SSIM among the meth-\nods using only LBS (i.e., do not require 3D scans for every\nframe) and outperforms all FFD methods minus BD FFD in\nthese metrics, despite having poorer training signal and no\ntest images (DVA was tested using all 200 cameras). More-\nover, our approach allows us to decompose avatars into driv-\nable garment layers compared to other volumetric methods.\nFigure 5 shows each separate garment layer, which can be\ncontrolled solely by skeleton joint angles, without requiring\nspecific garment registration modules as in [56].\nMethod\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nOurs\n30.634\n0.054\n0.965\nMVP LBS [22]\n28.795\n0.051\n0.955\nBD LBS [1]\n29.919\n0.044\n0.960\nBD FFD [1]\n30.999\n0.039\n0.964\nMVP FFD [22]\n30.072\n0.043\n0.960\nDVA [41]\n30.239\n0.042\n0.963\nTable 1. Our method scores the best in terms of PSNR and SSIM\nfor LBS-based methods. However, it lacks the sharpness of the\nmesh-based method. Moreover, our method outperforms MVP\n[22], which uses FFD meshes, scoring second in total for PSNR\nerror and the best in SSIM.\nFirst,\nsecond,\nthird place.\nw/o LGarment\nw/ LGarment\nFigure 6. The additional supervision LGarment improves the gar-\nment\u2019s shape by reducing semitransparent effects at the boundary.\nExperiment\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nOurs\n29.825\n0.056\n0.960\nw/o cage\n28.279\n0.066\n0.955\nw/ SH\n28.960\n0.058\n0.957\nw/o LGarment\n30.140\n0.057\n0.961\nw/o LNeo\n29.911\n0.056\n0.960\nSingle layer\n29.740\n0.057\n0.959\nTable 2. Lack of cage proxy significantly increases reconstruc-\ntion error. Moreover, single-layer avatars incorrectly model sliding\ngarments, and using SH for color modeling struggles with wrin-\nkles and self-shadows.\nFirst,\nsecond,\nthird place.\n5.2. Ablation Studies\nIn this section, the influence of different pipeline compo-\nnents (deformation and appearance representations, number\nof Gaussians) and losses (LGarment, LNeo) are analyzed.\nCage based deformation To evaluate the relevance of de-\nforming Gaussians with cages, we performed an experiment\nwhere Gaussian positions were transformed with LBS di-\nrectly. When initializing the Gaussian positions, instead of\nassigning them to a tetrahedral mesh proxy, we use LBS to\ntransform their 3D mean according to the closest point in\n\u02c6M. The network \u03a0MLP predict updates to the rotation, scale\nand Gaussian mean \u2206qi, \u2206si, \u2206\u00b5i, while network \u03a8MLP is\ndisabled. The results presented in the third column of Fig. 3\nshow significant artifacts, especially for the highly dynamic\nparts of the avatar. One of the reasons is the lack of stretch-\ning, which is implicitly provided by the deformation gradi-\nent from the cage.\nGarment loss The garment loss LGarment serves two pri-\nmary purposes: it improves garment separation and reduces\nerroneously translucid regions. Figure 6 depicts the effect\nof the loss on a t-shirt. It can be seen that quality degrades\nwithout it, especially on the edges. Although image metrics\nPSNR and SSIM are best without LGarment, we can ob-\nserve qualitatively that regions between garments\u2019 bound-\nary are blurry and have erroneous opacity.\nSpherical harmonics appearance 3DGS uses spherical\nharmonics (SH) to model the color of each Gaussian in the\nscene. The fourth column of Fig. 3 shows results where\n7\nw/ LNeo\nw/o LNeo\nFigure 7.\nThe effect of the tetrahedra regularization LNeo is\nmostly visible in the regions which lack supervision or undergo\nsliding, which covers them for most of the time.\n\u0393MLP is replaced by the SH layer in 3DGS. As can be seen, it\nstruggles to capture phenomena like self-shadows and wrin-\nkles, which are pose-dependent.\nTetrahedral regularization We introduced LNeo (Equa-\ntion 11) to avoid geometry artifacts that could potentially\nmisplace the Gaussians. It prevents tetrahedra from los-\ning too much volume, flipping, or diverging in size from\nthe canonical shape. Optimization of layered garments will\nnaturally struggle for regions that are either permanently or\ntemporarily covered, resulting in geometric artifacts, which\ncan be alleviated by LNeo regularization (Fig. 7).\nSingle layer avatar D3GA supports a single-layer training\nfor the garment and body, which struggles to model proper\ngarment sliding. The results are presented in the last column\nof Fig. 3. It can be observed that the edges between the T-\nshirt and jeans are oversmoothed.\nNumber of Gaussians As shown in Table 3, the runtime of\nD3GA depends on the number of Gaussians. Generally, the\nbest quality ratio to rendering time is between 25k and 100k\nGaussians. We chose 100k for our experiments.\n6. Discussion\nWhile D3GA shows better quality and competitive ren-\ndering speed w.r.t. the state of the art, there are still partic-\nular challenges. High-frequency patterns, like stripes, may\nresult in blurry regions. One way of improving image qual-\nity would be using a variation autoencoder to regress Gaus-\nsian parameters per texel of a guide mesh similar to [22].\nFigure 8. Reposing eight avatars with a pose from another subject\nExperiment\nPSNR \u2191\nLPIPS \u2193\nSSIM \u2191\nFPS \u2191\n\u2206FPS\n25k Gaussians\n29.938\n0.058\n0.960\n28\n107%\n100k Gaussians\n29.825\n0.056\n0.960\n26\n100%\n200k Gaussians\n29.864\n0.056\n0.960\n23\n88%\n300k Gaussians\n29.864\n0.056\n0.960\n20\n77%\nTable 3. Average frame rate per second at 1024 \u00d7 667 resolution\nw.r.t to the amount of Gaussian measured on a Nvidia V100 GPU.\n100k Gaussians provide the best rendering-time-to-quality ratio.\nLBS\nFFD\nFigure 9. Comparison of LBS (in red) and FFD meshes (in blue)\nDespite using the LGarment loss, self-collisions for loose\ngarments are still challenging, and the sparse controlling\nsignal does not contain enough information about complex\nwrinkles or self-shading. An exciting follow-up work di-\nrection would be replacing the appearance model in D3GA\nwith a relightable one. In order to extend D3GA repos-\nability (see Fig. 8) to reshapability and cloth transfer, we\nwould like to upgrade our LBS model to a more general one\n(e.g. SMPL [23]) and integrate a cloth shape space. Finally,\nwe would like to point out that D3GA is very flexible and\ncan be adapted to specific applications, e.g. by using more\nGaussians to capture high-frequency details (to the detri-\nment of rendering speed) or removing garment supervision\nto reduce PSNR (if good cage geometry decomposition is\nnot needed).\n7. Conclusion\nWe have proposed D3GA, a novel approach for recon-\nstructing animatable human avatars enabled by drivable 3D\nGaussians rigged with tetrahedral cages. Our method shows\nhigh-quality results, better than the state of the art using\nsimilar input and comparable to approaches using richer in-\nformation like FFD meshes or images at test time. More-\nover, our solution shows promising results in geometry and\nappearance modeling for dynamic sequences without re-\nquiring ground truth geometry, thus shortening the data pro-\ncessing time.\nAcknowledgement The authors thank the International\nMax Planck Research School for Intelligent Systems\n(IMPRS-IS) for supporting WZ. We also want to thank\nGiljoo Nam for the help with Gaussian visualizations, Anka\nChen for very useful conversations about tetrahedrons, and\nShoou-I Yu and Robbin Xu for their invaluable help with\ndata processing.\n8\nReferences\n[1] Timur M. Bagautdinov, Chenglei Wu, Tomas Simon, Fabi\u00b4an\nPrada, Takaaki Shiratori, Shih-En Wei, Weipeng Xu, Yaser\nSheikh, and Jason M. Saragih. Driving-signal aware full-\nbody avatars. ACM Transactions on Graphics (TOG), 40:1 \u2013\n17, 2021. 1, 2, 6, 7\n[2] Xu Chen, Yufeng Zheng, Michael J. Black, Otmar Hilliges,\nand Andreas Geiger.\nSnarf: Differentiable forward skin-\nning for animating non-rigid neural implicit shapes. 2021\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 11574\u201311584, 2021. 3\n[3] Yao Feng, Weiyang Liu, Timo Bolkart, Jinlong Yang, Marc\nPollefeys, and Michael J. Black.\nLearning disentangled\navatars with hybrid 3d representations. arXiv, 2023. 2\n[4] Yao Feng, Jinlong Yang, Marc Pollefeys, Michael J. Black,\nand Timo Bolkart.\nCapturing and animation of body and\nclothing from monocular video. SIGGRAPH Asia 2022 Con-\nference Papers, 2022. 1, 2\n[5] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction.\n2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n8645\u20138654, 2020. 2\n[6] Stephan J. Garbin, Marek Kowalski, Virginia Estellers,\nStanislaw Szymanowicz, Shideh Rezaeifar, Jingjing Shen,\nMatthew Johnson, and Julien Valentin. Voltemorph: Real-\ntime, controllable and generalisable animation of volumetric\nrepresentations. CoRR, abs/2208.00949, 2022. 3\n[7] Mustafa Isik, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. ACM Trans. Graph., 42(4):160:1\u2013160:12,\n2023. 2\n[8] Alec Jacobson, Ilya Baran, Jovan Popovi\u00b4c, and Olga\nSorkine-Hornung.\nBounded biharmonic weights for real-\ntime deformation. ACM SIGGRAPH 2011 papers, 2011. 4\n[9] Timothy Jeruzalski, Boyang Deng, Mohammad Norouzi,\nJ. P. Lewis, Geoffrey E. Hinton, and Andrea Tagliasac-\nchi. Nasa: Neural articulated shape approximation. ArXiv,\nabs/1912.03207, 2019. 3\n[10] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,\nand Anurag Ranjan. Neuman: Neural human radiance field\nfrom a single video. In European Conference on Computer\nVision, 2022. 3\n[11] Pushkar Joshi, Mark Meyer, Tony DeRose, Brian Green, and\nTom Sanocki. Harmonic coordinates for character articula-\ntion. ACM Trans. Graph., 26(3):71, 2007. 4\n[12] Tao Ju, Scott Schaefer, and Joe D. Warren. Mean value coor-\ndinates for closed triangular meshes. ACM SIGGRAPH 2005\nPapers, 2005. 4\n[13] James T. Kajiya. The rendering equation. Proceedings of the\n13th annual conference on Computer graphics and interac-\ntive techniques, 1986. 2\n[14] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering.\nACM Transactions on Graphics\n(TOG), 42:1 \u2013 14, 2023. 2, 3, 4, 5\n[15] Diederik P. Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. CoRR, abs/1412.6980, 2014. 5\n[16] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross B.\nGirshick. Pointrend: Image segmentation as rendering. In\n2020 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2020, Seattle, WA, USA, June 13-\n19, 2020, pages 9796\u20139805. Computer Vision Foundation /\nIEEE, 2020. 3\n[17] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis.\nPoint-based neural rendering with per-\nview optimization.\nComputer Graphics Forum, 40, 2021.\n3\n[18] Christoph Lassner and Michael Zollh\u00a8ofer. Pulsar: Efficient\nsphere-based neural rendering. 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n1440\u20131449, 2021. 2\n[19] Ruilong Li, Julian Tanke, Minh Vo, Michael Zollhofer,\nJurgen Gall, Angjoo Kanazawa, and Christoph Lassner.\nTava: Template-free animatable volumetric actors. ArXiv,\nabs/2206.08929, 2022. 2, 3\n[20] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes.\n2021 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 6494\u20136504,\n2020. 2\n[21] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor.\nACM Transactions on Graphics (TOG), 40:1 \u2013 16, 2021. 3\n[22] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason M. Saragih.\nMixture of volumetric primitives for efficient neural render-\ning. ACM Transactions on Graphics (TOG), 40:1 \u2013 13, 2021.\n2, 6, 7, 8\n[23] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J. Black. Smpl: A skinned multi-\nperson linear model. Seminal Graphics Papers: Pushing the\nBoundaries, Volume 2, 2015. 2, 4, 8\n[24] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by persis-\ntent dynamic view synthesis. ArXiv, abs/2308.09713, 2023.\n3\n[25] Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J. Black.\nThe power of points for modeling humans in clothing. 2021\nIEEE/CVF International Conference on Computer Vision\n(ICCV), pages 10954\u201310964, 2021. 2\n[26] Miles Macklin and Matthias M\u00a8uller. A constraint-based for-\nmulation of stable neo-hookean materials. Proceedings of\nthe 14th ACM SIGGRAPH Conference on Motion, Interac-\ntion and Games, 2021. 5\n[27] Ricardo Martin-Brualla, Noha Radwan, Mehdi S. M. Saj-\njadi, Jonathan T. Barron, Alexey Dosovitskiy, and Daniel\nDuckworth. Nerf in the wild: Neural radiance fields for un-\nconstrained photo collections. 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n7206\u20137215, 2020. 4\n[28] Marko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu\nTang, and Shunsuke Saito.\nKeypointnerf:\nGeneralizing\n9\nimage-based volumetric avatars using relative spatial encod-\ning of keypoints. ArXiv, abs/2205.04992, 2022. 2\n[29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf.\nCommunications of the ACM, 65:99 \u2013 106, 2020. 2, 4\n[30] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(TOG), 41:1 \u2013 15, 2022. 1, 2\n[31] Jes\u00b4us R Nieto and Antonio Sus\u00b4\u0131n. Cage based deformations:\na survey. In Deformation Models: Tracking, Animation and\nApplications, pages 75\u201399. Springer, 2012. 2\n[32] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 4\n[33] Keunhong Park, U. Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B. Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\n2021 IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), pages 5845\u20135854, 2020. 2\n[34] Keunhong Park, U. Sinha, Peter Hedman, Jonathan T. Bar-\nron, Sofien Bouaziz, Dan B. Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz. Hypernerf. ACM Transactions\non Graphics (TOG), 40:1 \u2013 12, 2021. 2\n[35] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou.\nNeural\nbody:\nImplicit neural representations with structured la-\ntent codes for novel view synthesis of dynamic humans.\n2021 IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 9050\u20139059, 2020. 2, 3\n[36] Yicong Peng, Yichao Yan, Shengqi Liu, Yuhao Cheng,\nShanyan Guan, Bowen Pan, Guangtao Zhai, and Xiaokang\nYang. Cagenerf: Cage-based neural radiance field for gen-\neralized 3d deformation and animation. In NeurIPS, 2022.\n3\n[37] Malte Prinzler, Otmar Hilliges, and Justus Thies.\nDiner:\nDepth-aware image-based neural radiance fields.\n2023\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 12449\u201312459, 2022. 2\n[38] Sergey Prokudin,\nQianli Ma,\nMaxime Raafat,\nJulien\nValentin, and Siyu Tang.\nDynamic point fields.\narXiv\npreprint arXiv:2304.02626, 2023. 2\n[39] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer.\nD-nerf: Neural radiance fields\nfor dynamic scenes. 2021 IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 10313\u2013\n10322, 2020. 3\n[40] Ravi Ramamoorthi and Pat Hanrahan. An efficient represen-\ntation for irradiance environment maps. Proceedings of the\n28th annual conference on Computer graphics and interac-\ntive techniques, 2001. 3\n[41] Edoardo Remelli, Timur M. Bagautdinov, Shunsuke Saito,\nChenglei Wu, Tomas Simon, Shih-En Wei, Kaiwen Guo,\nZhe Cao, Fabi\u00b4an Prada, Jason M. Saragih, and Yaser Sheikh.\nDrivable volumetric avatars using texel-aligned features.\nACM SIGGRAPH 2022 Conference Proceedings, 2022. 2,\n6, 7\n[42] Hang Si. Tetgen: A quality tetrahedral mesh generator and\na 3d delaunay triangulator (version 1.5 \u2014 user\u2019s manual).\n2013. 4\n[43] Shih-Yang Su, Timur M. Bagautdinov, and Helge Rhodin.\nDanbo: Disentangled articulated neural body representations\nvia graph neural networks. In European Conference on Com-\nputer Vision, 2022. 2\n[44] Shih-Yang Su, Timur M. Bagautdinov, and Helge Rhodin.\nNpc:\nNeural point characters from video.\nArXiv,\nabs/2304.02013, 2023. 2, 3\n[45] Shih-Yang Su, Frank Yu, Michael Zollhoefer, and Helge\nRhodin. A-nerf: Articulated neural radiance fields for learn-\ning human shape, appearance, and pose. In Neural Informa-\ntion Processing Systems, 2021. 2\n[46] Robert W. Sumner and Jovan Popovi\u00b4c. Deformation transfer\nfor triangle meshes. ACM SIGGRAPH 2004 Papers, 2004. 4\n[47] Mingxing Tan and Quoc V. Le.\nEfficientnet: Rethinking\nmodel scaling for convolutional neural networks.\nIn Ka-\nmalika Chaudhuri and Ruslan Salakhutdinov, editors, Pro-\nceedings of the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach, Cali-\nfornia, USA, volume 97 of Proceedings of Machine Learning\nResearch, pages 6105\u20136114. PMLR, 2019. 3\n[48] Ayush Tewari, Otto Fried, Justus Thies, Vincent Sitzmann, S.\nLombardi, Z. Xu, Tanaba Simon, Matthias Nie\u00dfner, Edgar\nTretschk, L. Liu, Ben Mildenhall, Pranatharthi Srinivasan,\nR. Pandey, Sergio Orts-Escolano, S. Fanello, M. Guang\nGuo, Gordon Wetzstein, J y Zhu, Christian Theobalt, Manju\nAgrawala, Donald B. Goldman, and Michael Zollh\u00a8ofer. Ad-\nvances in neural rendering. Computer Graphics Forum, 41,\n2021. 2\n[49] Ayush Kumar Tewari, Ohad Fried, Justus Thies, Vincent\nSitzmann, Stephen Lombardi, Kalyan Sunkavalli, Ricardo\nMartin-Brualla, Tomas Simon, Jason M. Saragih, Matthias\nNie\u00dfner, Rohit Pandey, S. Fanello, Gordon Wetzstein,\nJun-Yan Zhu, Christian Theobalt, Maneesh Agrawala, Eli\nShechtman, Dan B. Goldman, and Michael Zollhofer. State\nof the art on neural rendering. Computer Graphics Forum,\n39, 2020. 2\n[50] Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas\nGeiger, and Siyu Tang. Metaavatar: Learning animatable\nclothed human models from few depth images. In Neural\nInformation Processing Systems, 2021. 3\n[51] Yifan Wang, Noam Aigerman, Vladimir G. Kim, Siddhartha\nChaudhuri, and Olga Sorkine-Hornung.\nNeural cages for\ndetail-preserving 3d deformations. In 2020 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, CVPR\n2020, Seattle, WA, USA, June 13-19, 2020, pages 72\u201380.\nComputer Vision Foundation / IEEE, 2020. 2\n[52] Yifan Wang, Felice Serena, Shihao Wu, Cengiz \u00a8Oztireli,\nand Olga Sorkine-Hornung. Differentiable surface splatting\nfor point-based geometry processing. ACM Transactions on\nGraphics (TOG), 38:1 \u2013 14, 2019. 2, 3\n[53] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmannerf: Free-viewpoint rendering of moving people from\n10\nmonocular video. In IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, CVPR 2022, New Orleans, LA,\nUSA, June 18-24, 2022, pages 16189\u201316199. IEEE, 2022. 2\n[54] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang.\n4d gaussian splatting for real-time dynamic scene rendering.\nCoRR, abs/2310.08528, 2023. 2\n[55] Donglai Xiang, Timur M. Bagautdinov, Tuur Stuyck, Fabi\u00b4an\nPrada, Javier Romero, Weipeng Xu, Shunsuke Saito, Jingfan\nGuo, Breannan Smith, Takaaki Shiratori, Yaser Sheikh, Jes-\nsica K. Hodgins, and Chenglei Wu. Dressing avatars. ACM\nTransactions on Graphics (TOG), 41:1 \u2013 15, 2022. 1\n[56] Donglai Xiang, Fabi\u00b4an Prada, Timur M. Bagautdinov,\nWeipeng Xu, Yuan Dong, He Wen, Jessica K. Hodgins, and\nChenglei Wu. Modeling clothing as a separate layer for an\nanimatable human avatar. ACM Transactions on Graphics\n(TOG), 40:1 \u2013 15, 2021. 1, 7\n[57] Donglai Xiang, Fabi\u00b4an Prada, Zhe Cao, Kaiwen Guo, Chen-\nglei Wu, Jessica K. Hodgins, and Timur M. Bagautdinov.\nDrivable avatar clothing:\nFaithful full-body telepresence\nwith dynamic clothing driven by sparse rgb-d input. 2023.\n2\n[58] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,\nKalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-\nbased neural radiance fields. 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n5428\u20135438, 2022. 2\n[59] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023. 2\n[60] Hao Zhang, Yao Feng, Peter Kulits, Yandong Wen, Justus\nThies, and Michael J. Black.\nText-guided generation and\nediting of compositional 3d avatars. ArXiv, abs/2309.07125,\n2023. 2\n[61] Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. 2018 IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n586\u2013595, 2018. 7\n[62] Yufeng Zheng, Yifan Wang, Gordon Wetzstein, Michael J.\nBlack, and Otmar Hilliges. Pointavatar: Deformable point-\nbased head avatars from videos. 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n21057\u201321067, 2022. 2, 3\n[63] Zerong Zheng, Han Huang, Tao Yu, Hongwen Zhang, Yan-\ndong Guo, and Yebin Liu. Structured local radiance fields\nfor human avatar modeling.\n2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n15872\u201315882, 2022. 2\n[64] Wojciech Zielonka, Timo Bolkart, and Justus Thies.\nIn-\nstant volumetric head avatars. 2023 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages\n4574\u20134584, 2022. 2, 3\n[65] Michael Zollh\u00a8ofer, Justus Thies, Pablo Garrido, Derek\nBradley, Thabo Beeler, Patrick P\u00b4erez, Marc Stamminger,\nMatthias Nie\u00dfner, and Christian Theobalt. State of the art\non monocular 3d face reconstruction, tracking, and applica-\ntions. Computer Graphics Forum, 37, 2018. 2\n[66] Matthias Zwicker, Hans R\u00a8udiger Pfister, Jeroen van Baar,\nand Markus H. Gross.\nSurface splatting.\nProceedings of\nthe 28th annual conference on Computer graphics and inter-\nactive techniques, 2001. 2, 3\n11\n"
  },
  {
    "title": "DMV3D: Denoising Multi-View Diffusion using 3D Large Reconstruction Model",
    "link": "https://arxiv.org/pdf/2311.09217.pdf",
    "upvote": "20",
    "text": "DMV3D: DENOISING MULTI-VIEW DIFFUSION USING\n3D LARGE RECONSTRUCTION MODEL\nYinghao Xu\u2217\nAdobe Research & Stanford University\nyhxu@stanford.edu\nHao Tan\nAdobe Research\nhatan@adobe.com\nFujun Luan\nAdobe Research\nfluan@adobe.com\nSai Bi\nAdobe Research\nsbi@adobe.com\nPeng Wang\u2217\nAdobe Research & HKU\ntotoro97@outlook.com\nJiahao Li\u2217\nAdobe Research & TTIC\njiahao@ttic.edu\nZifan Shi\u2217\nAdobe Research & HKUST\nvivianszf9@gmail.com\nKalyan Sunkavalli\nAdobe Research\nsunkaval@adobe.com\nGordon Wetzstein\nStanford University\ngordon.wetzstein@stanford.edu\nZexiang Xu\u2020\nAdobe Research\nzexu@adobe.com\nKai Zhang\u2020\nAdobe Research\nkaiz@adobe.com\nABSTRACT\nWe propose DMV3D, a novel 3D generation approach that uses a transformer-\nbased 3D large reconstruction model to denoise multi-view diffusion.\nOur\nreconstruction model incorporates a triplane NeRF representation and can denoise\nnoisy multi-view images via NeRF reconstruction and rendering, achieving single-\nstage 3D generation in \u223c30s on single A100 GPU. We train DMV3D on large-\nscale multi-view image datasets of highly diverse objects using only image\nreconstruction losses, without accessing 3D assets.\nWe demonstrate state-of-\nthe-art results for the single-image reconstruction problem where probabilistic\nmodeling of unseen object parts is required for generating diverse reconstructions\nwith sharp textures.\nWe also show high-quality text-to-3D generation results\noutperforming previous 3D diffusion models. Our project website is at: https:\n//justimyhxu.github.io/projects/dmv3d/.\n1\nINTRODUCTION\nThe advancements in 2D diffusion models (Ho et al., 2020; Song et al., 2020a; Rombach et al.,\n2022a) have greatly simplified the image content creation process and revolutionized 2D design\nworkflows.\nRecently, diffusion models have also been extended to 3D asset creation in order\nto reduce the manual workload involved for applications like VR, AR, robotics, and gaming.\nIn particular, many works have explored using pre-trained 2D diffusion models for generating\nNeRFs (Mildenhall et al., 2020) with score distillation sampling (SDS) loss (Poole et al., 2022; Lin\net al., 2023a). However, SDS-based methods require long (often hours of) per-asset optimization\nand can frequently lead to geometry artifacts, such as the multi-face Janus problem.\nOn the other hand, attempts to train 3D diffusion models have also been made to enable diverse\n3D asset generation without time-consuming per-asset optimization (Nichol et al., 2022; Jun &\nNichol, 2023). These methods typically require access to ground-truth 3D models/point clouds for\ntraining, which are hard to obtain for real images. Besides, the latent 3D diffusion approach (Jun\n& Nichol, 2023) often leads to an unclean and hard-to-denoise latent space (Chen et al., 2023b) on\n\u2217This work is done while the author is an intern at Adobe Research.\n\u2020denotes equal advisory.\n1\narXiv:2311.09217v1  [cs.CV]  15 Nov 2023\nFigure 1: Top left: our approach achieves fast 3D generation (\u223c30s on A100 GPU) from text or\nsingle-image input; the latter one, combined with 2D segmentation methods (like SAM (Kirillov\net al., 2023)), can reconstruct objects segmented from natural images. Bottom: as a probabilistic\nsingle-image-to-3D model, we can produce multiple reasonable 3D assets from the same image.\nTop right: we demonstrate a scene comprising diverse 3D objects generated by our models.\nhighly diverse category-free 3D datasets due to two-stage training, making high-quality rendering a\nchallenge. To circumvent this, single-stage models have been proposed (Anciukevi\u02c7cius et al., 2023;\nKarnewar et al., 2023), but are mostly category-specific and focus on simple classes.\nOur goal is to achieve fast, realistic, and generic 3D generation. To this end, we propose DMV3D,\na novel single-stage category-agnostic diffusion model that can generate 3D (triplane) NeRFs from\ntext or single-image input conditions via direct model inference. Our model allows for the generation\nof diverse high-fidelity 3D objects within 30 seconds per asset (see Fig. 1). In particular, DMV3D\nis a 2D multi-view image diffusion model that integrates 3D NeRF reconstruction and rendering\ninto its denoiser, trained without direct 3D supervision, in an end-to-end manner. This avoids both\nseparately training 3D NeRF encoders for latent-space diffusion (as in two-stage models) and tedious\nper-asset optimization (as in SDS methods).\nIn essence, our approach uses a 3D reconstruction model as the 2D multi-view denoiser in a multi-\nview diffusion framework.\nThis is inspired by RenderDiffusion (Anciukevi\u02c7cius et al., 2023) \u2013\nachieving 3D generation through single-view diffusion.\nHowever, their single-view framework\nrelies on category-specific priors and canonical poses and thus cannot easily be scaled up to\ngenerate arbitrary objects. In contrast, we consider a sparse set of four multi-view images that\nsurround an object, adequately describing a 3D object without strong self-occlusions. This design\nchoice is inspired by the observation that humans can easily imagine a complete 3D object from a\nfew surrounding views with little uncertainty. However, utilizing such inputs essentially requires\naddressing the task of sparse-view 3D reconstruction \u2013 a long-standing problem and known to be\nhighly challenging even without noise in the inputs.\nWe address this by leveraging large transformer models that have been shown to be effective\nand scalable in solving various challenging problems (Jun & Nichol, 2023; Nichol et al., 2022;\nHong et al., 2023; Brown et al., 2020; Shen et al., 2023). In particular, built upon the recent 3D\nLarge Reconstruction Model (LRM) (Hong et al., 2023), we introduce a novel model for joint\nreconstruction and denoising. More specifically, our transformer model can, from a sparse set of\nnoisy multi-view images, reconstruct a clean (noise-free) NeRF model that allows for rendering\n(denoised) images at arbitrary viewpoints. Our model is conditioned on the diffusion time step,\ndesigned to handle any noise levels in the diffusion process. It can thus be directly plugged as the\nmulti-view image denoiser in an multi-view image diffusion framework.\n2\nFigure 2: SAM + DMV3D. We can use SAM (Kirillov et al., 2023) to segment any objects from a\nreal scene photo and reconstruct their 3D shape and appearance with our method, showcasing our\nmodel\u2019s potential in enabling 3D-aware image editing experiences.\nWe enable 3D generation conditioned on single images/texts. For image conditioning, we fix one of\nthe sparse views as the noise-free input and denoise other views, similar to 2D image inpainting (Xie\net al., 2023). We apply attention-based text conditioning and classifier-free guidance, commonly\nused in 2D diffusion models, to enable text-to-3D generation. We train our model on large-scale\ndatasets consisting of both synthetic renderings from Objaverse (Deitke et al., 2023) and real\ncaptures from MVImgNet (Yu et al., 2023) with only image-space supervision. Our model achieves\nstate-of-the-art results on single-image 3D reconstruction, outperforming prior SDS-based methods\nand 3D diffusion models.\nWe also demonstrate high-quality text-to-3D results outperforming\nprevious 3D diffusion models. In sum, our main contributions are:\n\u2022 A novel single-stage diffusion framework that leverages multi-view 2D image diffusion\nmodel to achieve 3D generation;\n\u2022 An LRM-based multi-view denoiser that can reconstruct noise-free triplane NeRFs from\nnoisy multi-view images;\n\u2022 A general probabilistic approach for high-quality text-to-3D generation and single-image\nreconstruction that uses fast direct model inference (\u223c30s on single A100 GPU).\nOur work offers a novel perspective to address 3D generation tasks, which bridges 2D and 3D\ngenerative models and unifies 3D reconstruction and generation. This opens up opportunities to\nbuild a foundation model for tackling a variety of 3D vision and graphics problems.\n2\nRELATED WORK\nSparse-view Reconstruction. Neural representations (Mescheder et al., 2019; Park et al., 2019;\nMildenhall et al., 2020; Sitzmann et al., 2019; 2020; Chen et al., 2022; M\u00a8uller et al., 2022) offer\na promising platform for scene representation and neural rendering (Tewari et al., 2022). Applied\nto novel-view synthesis, these approaches have been successful in single-scene overfitting scenarios\nwhere lots of multi-view training images are available. Recent efforts (Yu et al., 2021; Chen et al.,\n2021; Long et al., 2022; Wang et al., 2021; Lin et al., 2023b; Jain et al., 2021) have extended\nthese ideas to operate with a sparse set of views, showcasing improved generalization capabilities\nto unseen scenes. As non-generative methods, these approaches struggle on covering the multiple\nmodes in the large-scale datasets and thus can not generate diverse realistic results. In particular,\nthe recently-proposed LRM (Hong et al., 2023) tackles the inherent ambiguous single-image-to-3D\nproblem in a deterministic way, resulting in blurry and washed-out textures for unseen part of the\nobjects due to mode averaging. We resolve this issue by building a probabilistic image-conditioned\n3D generation model through denosing multi-view diffusion.\n3D Generative Adversarial Networks (GANs). GANs have made remarkable advancements in\n2D image synthesis (Brock et al., 2018; Karras et al., 2018; 2019; 2020; 2021). 3D GANs (Nguyen-\nPhuoc et al., 2019; Schwarz et al., 2020; Chan et al., 2021; 2022; Niemeyer & Geiger, 2021;\nGu et al., 2021; Skorokhodov et al., 2022; Xu et al., 2022; 2023; Shi et al., 2022; Gao et al.,\n2022; Skorokhodov et al., 2023) extend these capabilities to generating 3D-aware assets from\nunstructured collections of single-view 2D images in an unsupervised manner. GAN architectures,\nhowever, are difficult to train and generally best suited for modeling datasets of limited scale and\ndiversity (Dhariwal & Nichol, 2021).\n3\n\u00a9 2023 Adobe. All Rights Reserved. Adobe Confid\nImage \ntokenizer \n(DINO)\nReshape & \nUpsample\nt\nImage tokens\nTransformer\nCross-Att\nMLP\n+\n+\nSelf-Att\n+\nText\nt\nTriplane position \nembeddings\nPl\u00fccker rays\nTriplane \ntokens\nt-1\nRendering loss\nFigure 3: Overview of our method. We denoise multiple views (three shown in the figure to\nreduce clutterness; four used in experiments) for 3D generation. Our multi-view denoiser is a large\ntransformer model that reconstructs a noise-free triplane NeRF from input noisy images with camera\nposes (parameterized by Plucker rays). During training, we supervise the triplane NeRF with a\nrendering loss at input and novel viewpoints. During inference, we render denoised images at input\nviewpoints and combine them with inputs to obtain less noisy inputs for the next denoising step. We\noutput the clean triplane NeRF at final denoising step, enabling 3D generation. Refer to Sec. 3.3 for\nhow to extend this model to condition on single image or text.\n3D-aware Diffusion Models (DMs).\nDMs have emerged as foundation models for visual\ncomputing, offering unprecedented quality, fine-grained control, and versatility for 2D image\ngeneration (Ho et al., 2020; Song et al., 2020b; Rombach et al., 2022a; Po et al., 2023). Several\nstrategies have been proposed to extend DMs to the 3D domain. Some of these approaches (Nichol\net al., 2022; Jun & Nichol, 2023; Shue et al., 2023; Gupta et al., 2023; Ntavelis et al., 2023) use\ndirect 3D supervision. The quality and diversity of their results, however, is far from that achieved\nby 2D DMs. This is partly due to the computational challenge of scaling diffusion network models\nup from 2D to 3D, but perhaps more so by the limited amount of available 3D training data. Other\napproaches in this category build on optimization using a differentiable 3D scene representation\nalong with the priors encoded in 2D DMs (Poole et al., 2022; Lin et al., 2023a; Wang et al., 2022;\n2023). While showing some success, the quality and diversity of their results is limited by the\nSDS\u2013based loss function (Poole et al., 2022). Another class of methods uses 2D DM\u2013based image-\nto-image translation using view conditioning (Liu et al., 2023b; Chan et al., 2023; Gu et al., 2023).\nWhile these approaches promote multi-view consistency, they do not enforce it, leading to flicker\nand other view-inconsistent effects. Finally, several recent works have shown success in training 3D\ndiffusion models directly on single-view or multi-view image datasets (Karnewar et al., 2023; Chen\net al., 2023b; Shen et al., 2023) for relatively simple scenes with limited diversity.\nPrior RenderDiffusion (Anciukevi\u02c7cius et al., 2023) and concurrent Viewset Diffusion (Szymanowicz\net al., 2023) work are closest to our method. Both solve the 3D generation problem using 2D DMs\nwith 3D-aware denoisers. Neither of these methods, however, has been demonstrated to work on\nhighly diverse datasets containing multi-view data of >1M objects. Our novel LRM-based (Hong\net al., 2023) 3D denoiser architecture overcomes this challenge and enables state-of-the-art results\nfor scalable, diverse, and high-quality 3D generation.\n3\nMETHOD\nWe now present our single-stage 3D diffusion model. In particular, we introduce a novel diffusion\nframework that uses a reconstruction-based denoiser to denoise noisy multi-view images for 3D\ngeneration (Sec. 3.1). Based on this, we propose a novel LRM-based (Hong et al., 2023) multi-\nview denoiser conditioning on diffusion time step to progressively denoise multi-view images via\n3D NeRF reconstruction and rendering (Sec. 3.2). We further extend our model to support text and\nimage conditioning, enabling controllable generation (Sec. 3.3).\n4\n3.1\nMULTI-VIEW DIFFUSION AND DENOISING\nDiffusion. Denoising Diffusion Probabilistic Models (DDPM) transforms the data distribution x0 \u223c\nq(x) using a Gaussian noise schedule in the forward diffusion process. The generation process is\nthe reverse process where images are gradually denoised. The diffused data sample xt at timestep t\ncan be written as xt = \u221a\u00af\u03b1tx0 + \u221a1 \u2212 \u00af\u03b1t\u03f5, where \u03f5 \u223c N(0, I) represents Gaussian noise and the\nmonotonically decreasing \u00af\u03b1t controls the Signal-Noise-Ratio (SNR) of noisy sample xt.\nMulti-view diffusion. The original x0 distribution addressed in 2D DMs is the (single) image\ndistribution in a dataset.\nWe instead consider the (joint) distribution of multi-view images\nI = {I1, ..., IN}, where each set of I are image observations of the same 3D scene (asset)\nfrom viewpoints C = {c1, ..., cN}. The diffusion process is equivalent to diffusing each image\nindependently with the same noise schedule:\nIt = {\u221a\u00af\u03b1tI +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5I|I \u2208 I}\n(1)\nNote that this diffusion process is identical to the original one in DDPM, despite that we consider a\nspecific type of data distribution x = I denoting per-object 2D multi-view images.\nReconstruction-based denoising. The reverse of the 2D diffusion process is essentially denoising.\nIn this work, we propose to leverage 3D reconstruction and rendering to achieve 2D multi-view\nimage denoising, while outputting a clean 3D model for 3D generation. In particular, we leverage\na 3D reconstruction module E(\u00b7) to reconstruct a 3D representation S from the noisy multi-view\nimages It, and render denoised images with a differentiable rendering module R(\u00b7):\nIr,t = R(St, c),\nSt = E(It, t, C)\n(2)\nwhere Ir,t represents a rendered image from St at a specific viewpoint c.\nDenoising the multi-view input It is done by rendering St at the viewpoints C, leading to the\nprediction of noise-free I0. This is equivalent to x0 prediction in 2D DMs (Song et al., 2020a);\none can solve for xt\u22121 from the input xt and prediction x0 to enable progressive denoising during\ninference. However, unlike pure 2D generation, we find only supervising I0 prediction at input\nviewpoints cannot guarantee high-quality 3D generation (see Tab. 3), often leading to degenerate 3D\nsolutions where input images are pasted on view-aligned planes. Therefore, we propose to supervise\nnovel-view renderings from the 3D model St as well, which leads to the following training objective:\nLrecon(t) = EI,c\u223cIfull,Cfull \u2113\n\u0000I, R(E(It, t, C), c)\n\u0001\n(3)\nwhere Ifull and Cfull represent the full set of images and poses (from both randomly selected input\nand novel views), and \u2113(\u00b7, \u00b7) is an image reconstruction loss penalizing the difference between\ngroundtruth I and rendering R(E(It, t, C), c). Note that our framework is general \u2013 potentially\nany 3D representations (S) can be applied. In this work, we consider a (triplane) NeRF (Chan et al.,\n2022) representation (where R(\u00b7) becomes neural volumetric rendering (Mildenhall et al., 2020))\nand propose a LRM-based reconstructor E(\u00b7) (Hong et al., 2023).\n3.2\nRECONSTRUCTOR-BASED MULTI-VIEW DENOISER\nWe build our multi-view denoiser upon LRM (Hong et al., 2023) and uses large transformer model\nto reconstruct a clean triplane NeRF (Chan et al., 2022) from noisy sparse-view posed images.\nRenderings from the reconstructed triplane NeRF are then used as denoising outputs.\nReconstruction and Rendering. As shown in Fig. 3, we use a Vision Transformer (DINO (Caron\net al., 2021)) to convert input images I = {I1, ..., IN} to 2D tokens, and then use a transformer\nto map a learnable triplane positional embedding to the final triplane representing the 3D shape\nand appearance of an asset; the predicted triplane is then used to decode volume density and color\nwith an MLP (not shown in Fig. 3 to avoid clutterness) for differentiable volume rendering. The\ntransformer model consists of a series of triplane-to-images cross-attention and triplane-to-triplane\nself-attention layers as in the LRM work (Hong et al., 2023). We further enable time conditioning\nfor diffusion-based progressive denoising and introduce a new technique for camera conditioning.\nTime Conditioning. Our transformer-based model requires different designs for time-conditioning,\ncompared to CNN-based DDPM (Ho et al., 2020). Inspired by DiT (Peebles & Xie, 2022), we\n5\ncondition on time by injecting the adaLN-Zero block (Ho et al., 2020) into the self- and cross-\nattention layers of our model to effectively handle inputs with different noise levels.\nCamera Conditioning.\nTraining our model on datasets with highly diverse camera intrinsics\nand extrinsics, e.g., MVImgNet (Yu et al., 2023), requires an effective design of input camera\nconditioning to facilitate the model\u2019s understanding of cameras for 3D reasoning. A basic strategy\nis, as in the case of time conditioning, to use adaLN-Zero block (Peebles & Xie, 2022) on the\ncamera parameters (as done in Hong et al. (2023); Li et al. (2023)).\nHowever, we find that\nconditioning on camera and time simultaneously with the same strategy tends to weaken the effects\nof these two conditions and often leads to an unstable training process and slow convergence.\nInstead, we propose a novel approach \u2013 parameterizing cameras with sets of pixel-aligned rays.\nIn particular, following Sitzmann et al. (2021); Chen et al. (2023a), we parameterize rays using\nPlucker coordinates as r = (o \u00d7 d, d), where o and d are the origin and direction of a pixel ray\ncomputed from the camera parameters, and \u00d7 denotes cross-product. We concatenate the Plucker\ncoordinates with image pixels, and send them to the ViT transformer for 2D image tokenization,\nachieving effective camera conditioning.\n3.3\nCONDITIONING ON SINGLE IMAGE OR TEXT\nThe methods described thus far enable our model to function as an unconditional generative model.\nWe now introduce how to model the conditional probabilistic distribution with a conditional denoiser\nE(It, t, C, y), where y is text or image, enabling controllable 3D generation.\nImage Conditioning. We propose a simple but effective image-conditioning strategy that requires\nno changes to our model architecture. We keep the first view I1 (in the denoiser input) noise-free to\nserve as the conditioning image, while applying diffusion and denoising on other views. In this case,\nthe denoiser essentially learns to fill in the missing pixels within the noisy unseen views using cues\nextracted from the first input view, similar to the task of image inpainting which has been shown to be\naddressable by 2D DMs (Rombach et al., 2022a). In addition, to improve the generalizability of our\nimage-conditioned model, we generate triplanes in a coordinate frame aligned with the conditioning\nview and render other images using poses relative to the conditioning one. We normalize the input\nview\u2019s pose in the same way as LRM (Hong et al., 2023) during training, and specify the input\nview\u2019s pose in the same way too during inference.\nText Conditioning. To add text conditioning into our model, we adopt a strategy similar to that\npresented in Stable Diffusion (Rombach et al., 2022a). We use the CLIP text encoder (Radford\net al., 2021) to generate text embeddings and inject them into our denoiser using cross-attention.\nSpecifically, we include an additional cross-attention layer after each self-attention block in the ViT\nand each cross-attention block in the triplane decoder.\n3.4\nTRAINING AND INFERENCE\nTraining. During the training phase, we uniformly sample time steps t within the range [1, T],\nand add noise according to a cosine schedule. We sample input images with random camera poses.\nWe also randomly sample additional novel viewpoints to supervise the renderings (as discussed in\nSec. 3.1) for better quality. We minimize the following training objective with conditional signal y:\nL = Et\u223cU[1,T ],(I,c)\u223c(Ifull,Cfull) \u2113\n\u0000I, R(E(It, t, D, y), c)\n\u0001\n(4)\nFor the image reconstruction loss \u2113(\u00b7, \u00b7), we use a combination of L2 loss and LPIPS loss (Zhang\net al., 2018), with loss weights being 1 and 2, respectively.\nInference. For inference, we select four viewpoints that uniformly surround the object in a circle\nto ensure a good coverage of the generated 3D assets. We fix the camera Field-of-Views to 50\ndegrees for the four views. Since we predict triplane NeRF aligned with the conditioning image\u2019s\ncamera frame, we also fix the conditioning image\u2019s camera extrinsics to have identity orientation and\n(0, \u22122, 0) position, following the practice of LRM (Hong et al., 2023). We output the triplane NeRF\nfrom the final denoising step as the generated 3D model. We utilize DDIM (Song et al., 2020a)\nalgorithm to improve the inference speed.\n6\nTable 1: Evaluation Metrics of single-image 3D reconstruction on ABO and GSO datasets.\nABO dataset\nGSO dataset\nFID \u2193\nCLIP \u2191\nPSNR \u2191\nLPIPS \u2193\nCD \u2193\nFID \u2193\nCLIP \u2191\nPSNR \u2191\nLPIPS \u2193\nCD \u2193\nPoint-E\n112.29\n0.806\n17.03\n0.363\n0.127\n123.70\n0.741\n15.60\n0.308\n0.099\nShap-E\n79.80\n0.864\n15.29\n0.331\n0.097\n97.05\n0.805\n14.36\n0.289\n0.085\nZero-1-to-3\n31.59\n0.927\n17.33\n0.194\n\u2212\n32.44\n0.896\n17.36\n0.182\n\u2212\nOne-2-3-45\n190.81\n0.748\n12.00\n0.514\n0.163\n139.24\n0.713\n12.42\n0.448\n0.123\nMagic123\n34.93\n0.928\n18.47\n0.180\n0.136\n34.06\n0.901\n18.68\n0.159\n0.113\nOurs (S)\n36.77\n0.915\n22.62\n0.194\n0.059\n35.16\n0.888\n21.80\n0.150\n0.046\nOurs\n27.88\n0.949\n24.15\n0.127\n0.046\n30.01\n0.928\n22.57\n0.126\n0.040\n4\nEXPERIMENTS\nIn this section, we present an extensive evaluation of our method. In particular, we briefly describe\nour experiment settings (Sec. 4.1), compare our results with previous works (Sec. 4.2), and show\nadditional analysis and ablation studies (Sec. 4.3).\n4.1\nSETTINGS\nImplementation details. We use AdamW optimizer to train our model with an initial learning rate\nof 4e\u22124. We also apply a warm-up of 3K steps and a cosine decay on the learning rate. We train our\ndenoiser with 256 \u00d7 256 input images and render 128 \u00d7 128 image crops for supervision. To save\nGPU memory for NeRF rendering, we use the deferred back-propagation technique (Zhang et al.,\n2022). Our final model is a large transformer with 44 attention layers (counting all the self- and\ncross-attention layers in the encoder and decoder) outputting 64\u00d764\u00d73 triplanes with 32 channels.\nWe use 128 NVIDIA A100 GPUs to train this model with a batch size of 8 per GPU for 100K steps,\ntaking about 7 days. Since the final model takes a lot of resources, it is impractical for us to evaluate\nthe design choices with this large model for our ablation study. Therefore, we also train a small\nmodel that consists of 36 attention layers to conduct our ablation study. The small model is trained\nwith 32 NVIDIA A100 GPUs for 200K steps (4 days). Please refer to Tab. 6 in the appendix for an\noverview of the hyper-parameter settings.\nDatasets. Our model requires only multi-view posed images to train. We use rendered multi-view\nimages of \u223c730k objects from the Objaverse (Deitke et al., 2023) dataset. For each object, we render\n32 images under uniform lighting at random viewpoints with a fixed 50\u25e6 FOV, following the settings\nof LRM (Hong et al., 2023). To train our text-to-3D model, we use the object captions provided\nby Cap3D (Luo et al., 2023), which covers a subset of \u223c660k objects. For image-conditioned\n(single-view reconstruction) model, we combine the Objaverse data with additional real captures of\n\u223c220k objects from the MVImgNet (Yu et al., 2023) dataset, enhancing our model\u2019s generalization\nto out-of-domain inputs (see Fig. 7). We preprocess the MVImgNet dataset in the same way as\nLRM (Hong et al., 2023): for each capture, we crop out the object of interest for all views, remove\nthe background, and normalize the cameras to tightly fit the captured object into the box [\u22121, 1]3.\nIn general, these datasets contain a large variety of synthetic and real objects, allowing us to train a\ngeneric category-free 3D generative model.\nWe evaluate our image-conditioned model on novel synthetic datasets, including 100 objects from\nthe Google Scanned Object (GSO) (Downs et al., 2022) and 100 objects from the Amazon Berkeley\nObject (ABO) (Collins et al., 2022) datasets. This allows for direct comparison of single-view\nreconstruction with the groundtruth. For each object, we select 20 views that uniformly cover an\nobject from the upper hemisphere to compute metrics; we pick a slightly skewed side view as input.\n4.2\nRESULTS AND COMPARISONS\nSingle-image reconstruction. We compare our image-conditioned model with previous methods,\nincluding Point-E (Nichol et al., 2022), Shap-E (Jun & Nichol, 2023), Zero-1-to-3 (Liu et al., 2023b),\nOne-2-3-45 (Liu et al., 2023a), and Magic123 (Qian et al., 2023), on single-image reconstruction.\nWe evaluate the novel-view rendering quality from all methods using PSNR, LPIPS (Zhang et al.,\n2018), CLIP similarity score (Radford et al., 2021) and FID (Heusel et al., 2017), computed between\n7\nOurs\nInput\nShap-E\nPoint-E\nOne-2-3-45\nMagic123\nFigure 4: Qualitative comparisons on single-image reconstruction.\nOurs\nShap-E\nPoint-E\n\u2018a bowl of vegetables'\n\u2018a voxelized dog'\n\u2018a rusty old car'\nFigure 5: Qualitative comparisons on Text-to-3D .\nthe rendered and GT images. In addition, we also compute the Chamfer distance (CD) for geometry\nevaluation, for which we use marching cubes to extract meshes from NeRFs. Note that accurate\nquantitative evaluation of 3D generation remains a challenge in the field due to the generative nature\nof this problem; we use the most applicable metrics from earlier works to assess our model and\nbaselines.\nTab. 1 reports the quantitative results on the GSO and ABO testing sets respectively. Note that our\nmodels (even ours (S)) can outperforms all baseline methods, achieving the best scores across all\nmetrics for both datasets. Our high generation quality is reflected by the qualitative results shown\nin Fig. 4; our model generates realistic results with higher-quality geometry and sharper appearance\ndetails than all baselines.\nIn particular, the two-stage 3D DMs, Shap-E (3D encoder + latent diffusion) and Point-E (point\ndiffusion + points-to-SDF regression), lead to lower-quality 3D assets, often with incomplete shapes\n8\nand blurry textures; this suggests the inherent difficulties in denoising 3D points or pretrained 3D\nlatent spaces, a problem our model avoids. On the other hand, Zero-1-to-3 leads to better quantitative\nresults than Shap-E and Point-E on appearnce, because it\u2019s a 2D diffusion model finetuned from\nthe pretrained Stable Diffusion (Rombach et al., 2022b) to generate novel-view images. However,\nZero-1-to-3 alone cannot output a 3D model needed by many 3D applications and their rendered\nimages suffer from severe inconsistency across viewpoints. This inconsistency also leads to the low\nreconstruction and rendering quality from One-2-3-45, which attempts to reconstruct meshes from\nZero-1-to-3\u2019s image outputs. On the other hand, the per-asset optimization-based method Magic123\ncan achieve rendering quality comparable to Zero-1-to-3 while offering a 3D mdoel. However,\nthese methods require long (hours of) optimization time and also often suffer from unrealistic Janus\nartifacts (see the high heels object in Fig. 4). In contrast, our approach is a single-stage model\nwith 2D image training objectives and directly generates a 3D NeRF model (without per-asset\noptimization) while denoising multi-view diffusion. Our scalable model learns strong data priors\nfrom massive training data and produces realistic 3D assets without Janus artifacts. In general, our\napproach leads to fast 3D generation and state-of-the-art single-image 3D reconstruction results.\nTable 2: Evaluation Metrics on Text-to-3D.\nMethod\nVIT-B/32\nViT-L/14\nR-Prec\nAP\nR-Prec\nAP\nPoint-E\n33.33\n40.06\n46.4\n54.13\nShap-E\n38.39\n46.02\n51.40\n58.03\nOurs\n39.72\n47.96\n55.14\n61.32\nText-to-3D. We also evaluate our text-to-3D genera-\ntion results and compare with 3D diffusion models\nShap-E (Jun & Nichol, 2023) and Point-E (Nichol\net al., 2022), that are also category-agnostic and sup-\nport fast direct inference.\nFor this experiment, we\nuse Shap-E\u2019s 50 text prompts for the generation, and\nevaluate the results with CLIP precisions (Jain et al.,\n2022) and averaged precision using two different ViT\nmodels, shown in Tab. 2. From the table, we can see\nthat our model achieves the best precision. We also\nshow qualitative results in Fig. 5, in which our results clearly contain more geometry and appearance\ndetails and look more realistic than the compared ones.\nTable 3: Ablation on GSO dataset (DMV3D-S).\n#Views\nFID \u2193\nCLIP \u2191\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCD \u2193\n4 (Ours)\n35.16\n0.888\n21.798\n0.852\n0.150\n0.0459\n1\n70.59\n0.788\n17.560\n0.832\n0.304\n0.0775\n2\n47.69\n0.896\n20.965\n0.851\n0.167\n0.0544\n6\n39.11\n0.899\n21.545\n0.861\n0.148\n0.0454\nw.o Novel\n102.00\n0.801\n17.772\n0.838\n0.289\n0.185\nw.o Plucker\n43.31\n0.883\n20.930\n0.842\n0.185\n0.0505\n4.3\nANALYSIS, ABLATION, AND APPLICATION\nWe analyze our image-conditioned model and verify our design choices using our small model\narchitecture for better energy efficiency. Refer to Tab. 6 in the appendix for an overview of the\nhyper-parameter settings for this small model.\n#Views. We show quantitative and qualitative comparisons of our models trained with different\nnumbers (1, 2, 4, 6) of input views in Tab. 3 and Fig. 8. We can see that our model consistently\nachieves better quality when using more images, benefiting from capturing more shape and\nappearance information. However, the performance improvement of 6 views over four views is\nmarginal, where some metrics (like PSNR, FID) from the 4-view model is even better. We therefore\nuse four views as the default setting to generate all of our main results.\nMultiple instance generation. Similar to other DMs, our model can generate various instances from\nthe same input image with different random seeds as shown in Fig. 1, demonstrating the diversity of\nour generation results. In general, we find the multiple instance results can all reproduce the frontal\ninput view while containing varying shape and appearance in the unseen back side.\n9\nInput\nNovel-view\nInput\nNovel-view\nFigure 6: Robustness to out-of-domain inputs: synthetic (top left), real (bottom left, top right),\nand generated images (bottom right).\nInput sources. Our model is category-agnostic and generally works on various input sources as\nshown in many previous figures (Fig. 1,2,4). We show additional results in Fig. 6 with various\ninputs out of our training domains, including synthetic renderings, real captures, and generated\nimages. Our method can robustly reconstruct the geometry and appearance of all cases.\nAblation of MVImgNet. We compare our models trained with and without the real MVImgNet\ndataset on two challenging examples. As shown in Fig. 7, we can see that the model without\nMVImgNet can lead to unrealistic flat shapes, showcasing the importance of diverse training data.\nAblations of novel-view supervision and Plucker rays. We compare with our ablated models\nincluding one trained without the novel-view supervision, and one without the Plucker ray\nconditioning (using the adaLN-Zero block conditioning instead). We can also see that the novel\nview rendering supervision is critical for our model. Without it, all quantitative scores drop by\na large margin due to that the model cheats by pasting the input images on view-aligned planes\ninstead of reconstructing plausible 3D shapes. In addition, our design of Plucker coordinate-based\ncamera conditioning is also effective, leading to better quantitative results than the ablated model.\nApplication.\nThe flexibility and generality of our method can potentially enable broad 3D\napplications. One useful image editing application is to lift any objects in a 2D photo to 3D by\nsegment them (using methods like SAM (Kirillov et al., 2023)) and reconstruct the 3D model with\nour method, as shown in Fig. 1 and 2.\n5\nCONCLUSION\nWe present a novel single-stage diffusion model for 3D generation which generates 3D assets by\ndenoising multi-view image diffusion. Our multi-view denoiser is based on a large transformer\nmodel (Hong et al., 2023), which takes noisy multi-view images to reconstruct a clean triplane\nNeRF, outputting denoised images through volume rendering. Our framework supports text- and\nimage-conditioning inputs, achieving fast 3D generation via direct diffusion inference without per-\nasset optimization. Our method outperforms previous 3D diffusion models for text-to-3D generation\nand achieves state-of-the-art quality on single-view reconstruction on various testing datasets.\nLimitations.\nDespite the high-quality image- or text-conditioned 3D generation results we show\nin this work, there remain a couple of limitations future work can explore: 1) first, our generated\ntextures for unseen portion of the objects seem to lack high-frequency details with slightly washed-\nout colors. It will be interesting to further improve the texture fidelity; 2) our input images and\ntriplane are currently low-resolution. Extending our method to generate high-resolution NeRF from\nhigh-resolution input images will also be highly desirable; 3) our method only supports input images\nof objects without background; directly generating object NeRF with 3D background (Zhang et al.,\n2020; Barron et al., 2022) from single image will also be very valuable in many applications; 4) both\nour image- and text- conditioned models are trained from scratch without exploiting strong image\npriors in 2D foundation models like Stable Diffsuion (Rombach et al., 2022b). It might be helpful\nto think about ways to utilize those powerful 2D image priors in our framework.\nEthics Statement.\nOur generative model is trained on the Objaverse data and MVImgNet data.\nThe dataset (about 1M) is smaller than the dataset in training 2D diffusion models (about 100M to\n10\n1000M). The lack of data can raise two considerations. First, it can possibly bias towards the training\ndata distribution. Secondly, it might not be powerful enough to cover all the vast diversity in testing\nimages and testing texts. Our model has certain generalization ability but might not cover as much\nmodes as the 2D diffusion model can. Given that our model does not have the ability to identify\nthe content that is out of its knowledge, it might introduce unsatisfying user experience. Also, our\nmodel can possibly leak the training data if the text prompt or image input highly align with some\ndata samples. This potential leakage raises legal and security considerations, and is shared among\nall generative models (such as LLM and 2D diffusion models).\nReproducibility Statement.\nWe provide detailed implementation of our training method in the\nmain text and also provide the model configurations in Table 6 of the appendix. We will help resolve\nuncertainty of our implementation in open discussions.\nAcknowledgement.\nWe would like to thank Nathan Carr, Duygu Ceylan, Paul Guerrero, Chun-\nHao Huang, and Niloy Mitra for discussions about this project. We also thank Yuan Liu for providing\ntesting images from Syncdreamer.\nREFERENCES\nTitas Anciukevi\u02c7cius, Zexiang Xu, Matthew Fisher, Paul Henderson, Hakan Bilen, Niloy J Mitra, and\nPaul Guerrero. Renderdiffusion: Image diffusion for 3d reconstruction, inpainting and generation.\nIn IEEE Conf. Comput. Vis. Pattern Recog., 2023.\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural\nimage synthesis. arXiv preprint arXiv:1809.11096, 2018.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33, 2020.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou, Julien Mairal, Piotr Bojanowski, and\nArmand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of\nthe International Conference on Computer Vision (ICCV), 2021.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic\nimplicit generative adversarial networks for 3d-aware image synthesis. In IEEE Conf. Comput.\nVis. Pattern Recog., 2021.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d\ngenerative adversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.\nEric R Chan, Koki Nagano, Matthew A Chan, Alexander W Bergman, Jeong Joon Park, Axel Levy,\nMiika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. Generative novel view\nsynthesis with 3d-aware diffusion models. Int. Conf. Comput. Vis., 2023.\nAnpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su.\nMvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Int. Conf.\nComput. Vis., 2021.\nAnpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance\nfields. In European Conference on Computer Vision (ECCV), 2022.\nEric Ming Chen, Sidhanth Holalkere, Ruyu Yan, Kai Zhang, and Abe Davis. Ray conditioning:\nTrading photo-realism for photo-consistency in multi-view image generation.\nIn Int. Conf.\nComput. Vis., 2023a.\n11\nHansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-\nstage diffusion nerf: A unified approach to 3d generation and reconstruction. arXiv preprint\narXiv:2304.06714, 2023b.\nJasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu,\nXi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, et al. Abo: Dataset and\nbenchmarks for real-world 3d object understanding. In IEEE Conf. Comput. Vis. Pattern Recog.,\npp. 21126\u201321136, 2022.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi.\nObjaverse: A universe of\nannotated 3d objects. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 13142\u201313153, 2023.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,\nThomas B McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset\nof 3d scanned household items. In 2022 International Conference on Robotics and Automation\n(ICRA), pp. 2553\u20132560. IEEE, 2022.\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. Adv. Neural Inform. Process. Syst., 2022.\nJiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d-aware\ngenerator for high-resolution image synthesis. arXiv preprint arXiv:2110.08985, 2021.\nJiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M Susskind, Christian Theobalt, Lingjie Liu, and\nRavi Ramamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from\n3d-aware diffusion. In Int. Conf. Mach. Learn., 2023.\nAnchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas O\u02d8guz.\n3dgen: Triplane latent\ndiffusion for textured mesh generation. arXiv preprint arXiv:2303.05371, 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Adv. Neural\nInform. Process. Syst., 2017.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Adv. Neural\nInform. Process. Syst., 2020.\nYicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli,\nTrung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint\narXiv:2311.04400, 2023.\nAjay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent\nfew-shot view synthesis. In Int. Conf. Comput. Vis., 2021.\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. 2022.\nHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023.\nAnimesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a\n3d diffusion model using 2d images. In IEEE Conf. Comput. Vis. Pattern Recog., 2023.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for\nimproved quality, stability, and variation. In Int. Conf. Learn. Represent., 2018.\n12\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In IEEE Conf. Comput. Vis. Pattern Recog., 2019.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAnalyzing and improving the image quality of StyleGAN. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2020.\nTero Karras, Miika Aittala, Samuli Laine, Erik H\u00a8ark\u00a8onen, Janne Hellsten, Jaakko Lehtinen, and\nTimo Aila. Alias-free generative adversarial networks. In Adv. Neural Inform. Process. Syst.,\n2021.\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\nJiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan\nSunkavalli, Greg Shakhnarovich, and Sai Bi.\nInstant3d: Fast text-to-3d with sparse-view\ngeneration and large reconstruction model. https://arxiv.org/abs/2311.06214, 2023.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 300\u2013309, 2023a.\nKai-En Lin, Lin Yen-Chen, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi.\nVision transformer for nerf-based view synthesis from a single input image. In IEEE Winter Conf.\nAppl. Comput. Vis., 2023b.\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.\nOne-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023b.\nXiaoxiao Long, Cheng Lin, Peng Wang, Taku Komura, and Wenping Wang.\nSparseneus: Fast\ngeneralizable neural surface reconstruction from sparse views. 2022.\nTiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson.\nScalable 3d captioning with\npretrained models. arXiv preprint arXiv:2306.07279, 2023.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\nOccupancy networks: Learning 3d reconstruction in function space. In IEEE Conf. Comput. Vis.\nPattern Recog., 2019.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Eur. Conf.\nComput. Vis., 2020.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller.\nInstant neural graphics\nprimitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013102:15, July\n2022.\ndoi: 10.1145/3528223.3530127.\nURL https://doi.org/10.1145/3528223.\n3530127.\nThu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan:\nUnsupervised learning of 3d representations from natural images. In Int. Conf. Comput. Vis.,\n2019.\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative\nneural feature fields. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\nEvangelos Ntavelis, Aliaksandr Siarohin, Kyle Olszewski, Chaoyang Wang, Luc Van Gool, and\nSergey Tulyakov. Autodecoding latent 3d diffusion models. arXiv preprint arXiv:2307.05445,\n2023.\n13\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\nDeepsdf: Learning continuous signed distance functions for shape representation. In IEEE Conf.\nComput. Vis. Pattern Recog., 2019.\nWilliam Peebles and Saining Xie. Scalable diffusion models with transformers. arXiv preprint\narXiv:2212.09748, 2022.\nRyan Po, Wang Yifan, Vladislav Golyanik, Kfir Aberman, Jonathan T Barron, Amit H Bermano,\nEric Ryan Chan, Tali Dekel, Aleksander Holynski, Angjoo Kanazawa, et al. State of the art on\ndiffusion models for visual computing. arXiv preprint arXiv:2310.07204, 2023.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv, 2022.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-\nYing Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, et al.\nMagic123: One image\nto high-quality 3d object generation using both 2d and 3d diffusion priors.\narXiv preprint\narXiv:2306.17843, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2022a.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models.\nIn Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 10684\u201310695, 2022b.\nKatja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields\nfor 3d-aware image synthesis. In Adv. Neural Inform. Process. Syst., 2020.\nBokui Shen, Xinchen Yan, Charles R Qi, Mahyar Najibi, Boyang Deng, Leonidas Guibas, Yin Zhou,\nand Dragomir Anguelov. Gina-3d: Learning to generate implicit neural assets in the wild. In IEEE\nConf. Comput. Vis. Pattern Recog., pp. 4913\u20134926, 2023.\nZifan Shi, Sida Peng, Yinghao Xu, Geiger Andreas, Yiyi Liao, and Yujun Shen. Deep generative\nmodels on 3d representations: A survey. arXiv preprint arXiv:2210.15663, 2022.\nJ. Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d\nneural field generation using triplane diffusion. In IEEE Conf. Comput. Vis. Pattern Recog., 2023.\nVincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wetzstein.\nScene representation networks:\nContinuous 3d-structure-aware neural scene representations. Advances in Neural Information\nProcessing Systems, 32, 2019.\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein.\nImplicit neural representations with periodic activation functions. Advances in neural information\nprocessing systems, 33:7462\u20137473, 2020.\nVincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh Tenenbaum, and Fredo Durand. Light field\nnetworks: Neural scene representations with single-evaluation rendering. Advances in Neural\nInformation Processing Systems, 34:19313\u201319325, 2021.\nIvan Skorokhodov, Sergey Tulyakov, Yiqun Wang, and Peter Wonka. Epigraf: Rethinking training\nof 3d gans. In Adv. Neural Inform. Process. Syst., 2022.\nIvan Skorokhodov, Aliaksandr Siarohin, Yinghao Xu, Jian Ren, Hsin-Ying Lee, Peter Wonka,\nand Sergey Tulyakov.\n3d generation on imagenet.\nIn International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=U2WjB9xxZ9q.\n14\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020a.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020b.\nStanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion:(0-) image-\nconditioned 3d generative models from 2d data. arXiv preprint arXiv:2306.07881, 2023.\nAyush Tewari, Justus Thies, Ben Mildenhall, Pratul Srinivasan, Edgar Tretschk, Wang Yifan,\nChristoph Lassner, Vincent Sitzmann, Ricardo Martin-Brualla, Stephen Lombardi, et al. Ad-\nvances in neural rendering. In Computer Graphics Forum, volume 41, pp. 703\u2013735. Wiley Online\nLibrary, 2022.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich.\nScore\njacobian chaining: Lifting pretrained 2d diffusion models for 3d generation.\narXiv preprint\narXiv:2212.00774, 2022.\nQianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T\nBarron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-\nview image-based rendering. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu.\nProlificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.\narXiv preprint arXiv:2305.16213, 2023.\nShaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape\nguided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 22428\u201322437, 2023.\nYinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and Bolei Zhou. 3d-aware image synthesis via\nlearning structural and textural representations. In IEEE Conf. Comput. Vis. Pattern Recog., 2022.\nYinghao Xu, Menglei Chai, Zifan Shi, Sida Peng, Ivan Skorokhodov, Aliaksandr Siarohin, Ceyuan\nYang, Yujun Shen, Hsin-Ying Lee, Bolei Zhou, et al.\nDiscoscene: Spatially disentangled\ngenerative radiance fields for controllable 3d-aware scene synthesis.\nIn IEEE Conf. Comput.\nVis. Pattern Recog., 2023.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from\none or few images. In IEEE Conf. Comput. Vis. Pattern Recog., 2021.\nXianggang Yu, Mutian Xu, Yidan Zhang, Haolin Liu, Chongjie Ye, Yushuang Wu, Zizheng Yan,\nChenming Zhu, Zhangyang Xiong, Tianyou Liang, et al. Mvimgnet: A large-scale dataset of\nmulti-view images. In IEEE Conf. Comput. Vis. Pattern Recog., pp. 9150\u20139161, 2023.\nKai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving\nneural radiance fields. arXiv preprint arXiv:2010.07492, 2020.\nKai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf:\nArtistic radiance fields, 2022.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018.\n15\nA\nAPPENDIX\nA.1\nROBUSTNESS EVALUATION.\nWe evaluate our model on GSO (Downs et al., 2022) renderings that use different camera Field-Of-\nViews (FOVs) and lighting conditions to justify its robustness. Specifically, while the MVImgNet\ndataset include diverse camera FOVs and lighting conditions, the Objaverse renderings we are also\ntrained on share a constant 50\u25e6 FOV and uniform lighting. We evaluate the robustness of our image-\nconditioned model by testing images with other FOV angles and complex environmental lightings.\nAs shown in Tab. 4, our model is relatively robust to the FOV of the captured images, though quality\nindeed drops when the actual FOV deviates more from the 50\u25e6 FOV we assume during inference\n(see Sec. 3.4). However, it exhibits lower sensitivity to lighting variations, leading to similar quality\nacross different lighting conditions. When the lighting is non-uniform, our model bakes the shading\neffects into the NeRF appearance, yielding plausible renderings.\nTable 4: Robustness on GSO dataset.\nLighting/Fov\nAppearance\nGeometry\nFID \u2193\nCLIP \u2191\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCD \u2193\nOurs\n30.01\n0.928\n22.57\n0.845\n0.126\n0.0395\nFov10\n35.69\n0.912\n19.136\n0.820\n0.207\n0.0665\nFov30\n32.309\n0.921\n20.428\n0.839\n0.166\n0.0527\nFov70\n32.095\n0.921\n20.961\n0.860\n0.154\n0.0616\nFov90\n34.438\n0.912\n19.952\n0.855\n0.190\n0.0754\ncity\n33.31\n0.916\n21.19\n0.831\n0.142\n0.0437\nnight\n36.32\n0.907\n20.383\n0.829\n0.161\n0.0413\nsunrise\n33.264\n0.917\n21.080\n0.843\n0.140\n0.0423\nstudio\n36.32\n0.927\n21.383\n0.839\n0.141\n0.0428\nA.2\nQUANTATIVE EVALUATION ON MVIMGNET.\nMVImgNet (Yu et al., 2023) contains a diverse set of real data, which helps improve our\ngeneralization capabilities for real data or out-of-domain data, as demonstrated in Fig 7.\nWe\nalso perform quantative evaluation on the model with and without MVImgNet on the GSO\ndataset (Downs et al., 2022) in Tab. 5. The reconstructed results in terms of appearance and geometry\nare similar to the previous results only trained with Objaverse, indicating that MVImgNet improves\ngeneralization without compromising the quality of reconstruction. We train both settings for an\nequal number of 100K iterations with exactly the same learning rate schedules and computes.\nInput\nw. MvImageNet\nw.o. MvImageNet\nFigure 7: Qualitative comparison of our model trained with and without MVImgNet.\nTable 5: Ablation of MVImgNet.\n#Views\nAppearance\nGeometry\nFID \u2193\nCLIP \u2191\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nCD \u2193\nw. MvImageNet\n30.01\n0.928\n22.57\n0.845\n0.126\n0.0395\nw.o MvImageNet\n27.76\n0.924\n21.85\n0.850\n0.128\n0.0378\n16\nA.3\nIMPLEMENTATION DETAILS.\nOur experiments are implemented in the PyTorch and the codebase is built upon guided diffu-\nsion (Dhariwal & Nichol, 2021). For the AdamW optimizer, we use a weight-decay 0.05 and\nbeta (0.9, 0.95).\nTable 6 presents the detailed configuration of our various image-conditioned\nmodels. The architecture of the text-conditioned model closely mirrors that of the image-conditioned\nmodels, with the primary distinction being the approach to injecting the condition signal. For text-\nconditioned models, we employ the CLIP text encoder to derive text embeddings, integrating them\ninto our denoiser through cross-attention layers. Specifically, in each transformer block within the\nencoder and decoder, a new cross-attention layer is introduced between the original attention and\nFFN. In such a case, text-conditioned models consistently exhibit larger sizes than their image-\nconditioned counterparts, resulting in a slightly slower inference speed. During inference, we adopt\na classifier-free guidance approach Ho & Salimans (2022) with a scale of 5 to generate 3D assets\nconditioned on text.\nSmall\nLarge\nEncoder\nImage resolution\n256\u00d7256\n256\u00d7256\nPatch size\n16\n8\nAtt. Layers\n12\n12\nAtt. channels\n768\n768\nDecoder\nTriplane tokens\n32 \u00d7 32 \u00d7 3\n32 \u00d7 32 \u00d7 3\nAtt. channels\n768\n1024\nAtt. layers\n24 (12a+12c)\n32 (16a+16c)\nTriplane upsample\n1\n2\nTriplane shape\n32 \u00d7 32 \u00d7 3 \u00d7 32\n64 \u00d7 64 \u00d7 3 \u00d7 32\nRenderer\nRendering patch size\n64\n128\nRay-marching steps\n48\n128\nMLP layers\n10\n10\nMLP width\n64\n64\nActivation\nReLU\nReLU\nDiffusion\nTimes steps\n1000\n1000\nPrediction target\nx0\nx0\nSchedule\ncosine\ncosine\nTraininig\nLearning rate\n4e-4\n4e-4\nOptimizer\nAdamW\nAdamW\nWarm-up steps\n3000\n3000\nBatch size per GPU\n8\n8\n#GPUS\n32\n128\nIterations\n200K\n100K\nTraining time\n4 days\n7 days\nDataset\nSource\nMVImgNet & Objaverse\nMVImgNet & Objaverse\nMixing ratio\n1:3\n1:3\nResolution\n256\n256\nTable 6: Implementation details for our models. Att. denotes the attention. a and c represents the\nself-attention and cross attention.\nA.4\nVIEW NUMBERS\nWe have compared the effects of using different numbers of views quantitatively in Tab. 3. Here,\nwe also present qualitative results in Fig. 8. When there is only one view, the predicted novel view\nis very blurry. However, when the view number increases to four, the results become much clearer.\nWhen using six views, the improvement compared to four views is not significant, consistent to the\nmetrics reported in Tab. 3, indicating performance saturation. Therefore, our model uses four views\nas the default configuration.\nA.5\nMORE COMPARISON.\nWe also include more qualitative comparison on single-view image reconstruction in Fig. 9.\n17\nInput\n#view 1\n#view 4\n#view 2\n#view 6\nFigure 8: Qualitative comparison on different view numbers.\nShap-E\nPoint-E\nOne-2-3-45\nMagic123\nOurs\nFigure 9: Qualitative comparison on single-image reconstruction.\n18\n"
  },
  {
    "title": "EDMSound: Spectrogram Based Diffusion Models for Efficient and High-Quality Audio Synthesis",
    "link": "https://arxiv.org/pdf/2311.08667.pdf",
    "upvote": "18",
    "text": "EDMSound: Spectrogram Based Diffusion Models for\nEfficient and High-Quality Audio Synthesis\nGe Zhu\u2217 Yutong Wen\nDepartment of ECE, University of Rochester\n{ge.zhu,yutong.wen}@rochester.edu\nMarc-Andr\u00e9 Carbonneau\nUbisoft La Forge\nmarc-andre.carbonneau@ubisoft.com\nZhiyao Duan\nDepartment of ECE, University of Rochester\nzhiyao.duan@rochester.edu\nAbstract\nAudio diffusion models can synthesize a wide variety of sounds. Existing models\noften operate on the latent domain with cascaded phase recovery modules to\nreconstruct waveform. This poses challenges when generating high-fidelity audio.\nIn this paper, we propose EDMSound, a diffusion-based generative model in\nspectrogram domain under the framework of elucidated diffusion models (EDM).\nCombining with efficient deterministic sampler, we achieve similar Fr\u00e9chet audio\ndistance (FAD) score as top-ranked baselines with only 10 steps and reach state-\nof-the-art performance with 50 steps on the DCASE2023 foley sound generation\nbenchmark. We also reveal a potential concern regarding diffusion based audio\ngeneration models that they tend to generate samples with high perceptual similarity\nto the data from training set. Project page: https://agentcooper2002.github.\nio/EDMSound/\n1\nIntroduction\nAudio synthesis research has a long history [25]. With the development of deep generative models\nin recent years, data-driven audio synthesis methods have become more and more popular. In\nparticular, diffusion models [33, 10] have led to transformative changes in audio synthesis tasks,\nresulting in higher quality audio samples. Current diffusion-based audio generation models utilize\na cascaded system design [20, 7, 14, 38, 13, 9] to circumvent the complexity of generating sound\nin the temporal domain [6]. They typically involve converting waveforms into spectrograms to\ntrain a base diffusion generator. A secondary phase recovery network then converts the spectral\nrepresentation back into the temporal domain [13, 9]. To further reduce the computational complexity\nfor the base diffusion model [20, 41], a variational autoencoder (VAE) can be used to transform a\nmel-spectrogram into a lower-dimensional latent representation. However, a recent survey study [26]\nsuggests that current audio generation models might not be ready for professional sound production\nand the most significant challenge is presented in audio quality (e.g., fidelity, sampling rate, and level\nof noise). This audio fidelity degradation may be caused by the cumulative errors across modules\nin cascaded frameworks [28]. In addition, existing diffusion-based audio generation systems are\ninefficient at inference time and vanilla samplers will typically take hundreds of neural function\nevaluations (NFEs). For instance, AudioLDM [20] takes 100-200 NFEs with denoising diffusion\nimplicit model (DDIM)[36] for better sample quality.\nIn this study, we target at improving the generation fidelity in an end-to-end manner by developing\nan audio generation model in the complex spectrogram domain. Compared to magnitude spectrum\n\u2217This work was partially done during an internship at Ubisoft La Forge.\n37th Conference on Neural Information Processing Systems (2023) Machine Learning for Audio Workshop.\narXiv:2311.08667v2  [cs.SD]  18 Nov 2023\nTable 1: FAD score and relative dataset similarity score comparison of the generated audio samples\non DCASE2023 task7. Baseline systems that rank 1st in the challenge, both achieved top3 official\nrank. \u2018mean\u2019 represents the average value of the experiments. \u2018best\u2019 represents the best one.\nSystem\nDog\nFootstep\nGunshot\nKeyboard\nMoving Motor\nRain\nSneeze\nOverall\nBark\nVehicle\nCough\nFAD score (\u2193)\nScheibler et al [31]\n3.68\n8.07\n3.65\n2.78\n7.42\n5.23\n2.61\n4.78\nYi et al [42]\n3.62\n5.10\n5.74\n3.04\n9.80\n5.96\n1.90\n5.02\nJung et al [4]\n3.34\n3.99\n3.50\n4.07\n14.86\n3.53\n1.87\n5.02\nEDMSound-mean (Ours)\n2.93\n3.22\n3.61\n3.73\n11.10\n6.01\n1.27\n4.56\nRelative dataset similarity\nScheibler et al [31]\n-0.02\n-0.04\n-0.04\n-0.07\n-0.02\n-0.09\n0.03\n-0.04\nYi et al [42]\n-0.05\n-0.07\n-0.11\n-0.08\n-0.03\n-0.04\n-0.05\n-0.06\nJung et al (closed) [4]\n-0.14\n-0.11\n-0.11\n-0.18\n-0.10\n-0.17\n-0.11\n-0.13\nEDMSound-best (Ours)\n-0.05\n-0.06\n-0.06\n-0.08\n-0.02\n-0.02\n-0.11\n-0.05\nand phase spectrum, the real and imaginary components of the complex spectrograms exhibit clear\nstructures and are suitable for deep learning models [29]. Compared to raw waveform modeling [28],\nspectral features have fewer temporal redundancies [21]. To improve generation fidelity, we build\nour diffusion generators within the framework of EDM [15] due to its SoTA performance in several\nimage generation benchmarks. To accelerate the generation while maintaining similar sample quality,\nwe use exponential integrator (EI) based ordinary differential equation (ODE) samplers during\ninference [43, 22, 23]. We validate our method on different sound categories using DCASE2023\nfoley sound generation benchmark and Speech Command 09 (SC09) [39] dataset (containing spoken\ndigits from \u2018zero\u2019 to \u2018nine\u2019) using Fr\u00e9chet distance as evaluation metric for its correlation with human\nperception [16].\nWhile diffusion-based models are capable of generating high quality audio samples, it can uninten-\ntionally replicate training data [34]. Replicating data might also harm the audio generation diversity.\nAlthough similar concerns have been explored in computer vision by [34, 35], examination of this\nissue in audio generation remains an open research area. In our work, we answer the question of\nwhether diffusion-based models generate audio samples with replicated contents.\nTo summarize, we introduce an end-to-end audio diffusion model, EDMSound, in the complex\nspectrogram domain. At inference time, we use EI-based ODE samplers to accelerate the generation\nspeed. We achieve the SoTA performance on DCASE2023 foley sound generation benchmark\nand competitive performance on SC09 dataset in terms of Fr\u00e9chet distance. We propose a method\nto examine the memorization issue, i.e., content replication on a range of diffusion-based audio\ngeneration models on the DCASE2023 benchmark dataset. Qualitative and quantitative analysis show\nthat our proposed model does not generate exact copies of training data. Instead, it is able to generate\naudio samples that match the sound timbre of the training samples but with varied temporal patterns.\n2\nMethod\nDiffusion probabilistic models (DPMs) [10, 33] involve (1) corrupting training data with increasing\nnoise levels into normal distribution and (2) learning to reverse each step of this noise corruption\nwith the same functional form. It can be generalized into score-based generative models [37] which\nemploy an infinite number of noise scales so that both forward and backward diffusion processes can\nbe described by stochastic differential equations (SDEs). During inference, the reverse SDE is used\nto generate samples with numerical approaches starting from the standard normal distribution. A\nremarkable property of the reverse SDE is the existence of a deterministic process, namely probability\nflow ODE, whose trajectories share the same marginal probability as the original SDE [37]. As a\nresult, one can employ ODE solvers. These solvers, in contrast to SDE solvers, allow for larger step\nsizes, primarily because they are not influenced by the inherent randomness of the SDE [22].\nEDM on Complex Spectrogram We train our diffusion model using EDM [15] which formulates\nthe above diffusion SDE with noise scales instead of drift and diffusion coefficients. Practically, it\npresents a systematic way to design both training and sampling processes. To ensure that the neural\nnetwork inputs are scaled within [\u22121, 1] required by the diffusion models, we apply an amplitude\ntransformation on the complex spectrogram inputs, \u02dcc = \u03b2|c|\u03b1ei\u2220c following [29], where \u03b1 \u2208 (0, 1] is\na compression factor which emphasize time-frequency bins with low energy, \u2220c represents the angle\nof the original complex spectrogram, and \u03b2 \u2208 R+ is a scaling factor to normalize amplitudes roughly\nto within [0, 1]. Such compression technique was originally proposed for speech enhancement [2], but\nwe found it also effective in general sounds. We adopt 2D efficient U-Net proposed in Imagen [30] as\n2\n10\n20\n30\n40\n50\n60\nNFE\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\nFAD\nEuler\nHeun\nDPM-1 (DDIM)\nDPM-2s\nDPM-2m\nDPM-3s\nDPM-3m\nFigure 1: Comparison of FAD scores using dif-\nferent ODE samplers on DCASE 2023 Task 7. In\nDPM based samplers, the number indicates the or-\nder of the solvers, \u2018s\u2019 represents \u2018singlestep\u2019 and\n\u2018m\u2019 represents \u2018multistep\u2019. We use CFG with a\nscale of 2.0 and repeat experiments three times.\nModel\nParams\nFID \u2193\nIS \u2191\nmIS \u2191\nAM \u2193\nAutoregressive\nSampleRNN [24]\n35.0M\n8.96\n1.71\n3.02\n1.76\nWaveNet [27]\n4.2M\n5.08\n2.27\n5.80\n1.47\nSashimi [8]\n4.1M\n1.99\n4.12\n24.57\n0.90\nNon-autoregressive\nWaveGAN [5]\n19.1M\n2.03\n4.90\n36.10\n0.80\nDiffWave [19]\n24.1M\n1.92\n5.26\n51.21\n0.68\nw/ Sashimi\n23.0M\n1.42\n5.94\n69.17\n0.59\nASGAN (Mel.) [1]\n38.0M\n0.56\n7.02\n162.8\n0.56\nASGAN (HuBERT)\n-\n0.14\n7.67\n226.7\n0.26\nEDMSound (Ours)\n45.2M\n0.14\n7.17\n160.2\n0.33\nTrain\n-\n0.00\n8.56\n292.5\n0.16\nTest\n-\n0.02\n8.33\n257.6\n0.19\nTable 2: Comparison of unconditional generation\nwith automated metrics on SC09 dataset. FID\n(Fr\u00e9chet Inception Distance), IS (Inception score),\nmodified IS, and AM score are measures for gen-\nerated diversity and quality.\nour diffusion model backbone due to its high sample quality, faster convergence speed and memory-\nefficiency. During training, we use preconditioned denoising score matching as our training objective\nfollowing [15]. i.e, ExEn[\u03bb(\u03c3)\u2225D(x+n; \u03c3)\u2212x\u22252\n2], where x is the training data and n \u2208 N(0, \u03c32I).\nWe apply classifier free guidance (CFG) [11] at the sampling stage in the conditional generation task.\nEfficient Diffusion Samplers Within EDM, the probability flow ODE can be simplified as a nonlinear\nODE, enabling the direct application of standard off-the-shelf ODE solvers. It is found that EI based\nODE solvers have the minimum error with limited steps in a semilinear structured ODE [43], a\ncombination of a linear function of the data variable and a nonlinear neural network term. Though\nsuch probability flow ODE only contains the non-linear neural network in EDM case, it is still\nbeneficial to integrate EI approach shown in [43]. Therefore, we use high order EI based ODE\nsolvers [22], i.e., singlestep and multistep DPM-solvers [22, 23].\nContent Replication Detection We define content replication as the presence of generated samples\nthat are either complete duplicates or a substantially similar portions of the training samples. It is\nfound that representations trained with full supervision or self-supervised learning can perform as\nwell as detectors specially trained for content replication detection [34]. Since there is no existing\ncontent replication detectors for sound effects generation, we employ off-the-shelf pretrained audio\nrepresentations including CLAP [40], AudioMAE [12], and PANNs [18], and compute the cosine\nsimilarity score to measure the degree of content replication. To better adapt the audio descriptors for\nthis task, we conduct an additional fine-tuning stage: We attach multi-layer perceptrons to the frozen\npre-trained audio encoders and then train with triplet margin loss [32]. To enhance the robustness\nof the descriptor, we incorporate data augmentation by injecting Gaussian noise, random amplitude\nscaling, and temporal shifting to audio samples. We first choose one audio sample as an anchor\nsample and a positive pair with the same audio with the above augmentation. Then, we randomly\nselect another audio sample within the same class as the negative pair with data augmentation. After\nthe fine-tuning step, we search the training set based on the cosine similarity for each generated audio\nsample. We identify matched audio samples within the training set with the top-1 similarity scores.\nThese identified training samples are regarded as top matches for their corresponding generated audio\ncounterparts. We then analyze the content replication behavior within these matched pairs.\n3\nExperiment\nExperimental setup We benchmark our model, EDMSound, on DCASE2023 task7 and SC09 dataset.\nDCASE2023 foley sound generation [3] aims at improving audio fidelity, fit-to-category, and diversity\nfor foley sound generation and it provides a standardized evaluation framework for different systems.\nIt includes an open track and a closed track regarding the training dataset scale: the open track allows\nparticipants to leverage the datasets beyond the development set, while the closed track limits the\ndataset usage. We compare with strong baseline systems on both tracks in terms of FAD. Yi et al. [42]\n(ranked 1st officially) and Scheibler et al. [31] (achieved the highest FAD score on the open track) use\nLDMs. Jung et al. [4] use a GAN-based model, and ranked 1st in FAD score on the closed track. For\nthe SC09 benchmark for unconditional generation, we retrain EDMSound without CFG following\n3\n \naug\naug\nA\nP\nN\n \naug\naug\nA\nP\nN\n(b)\nCopy detection \ufb01ne-tuning\nTop-1 similarity score\ndistribution\nTraining Data\nGenerated Data\n(a)\n(c)\nStitching \nCopies\nAnchor\nPositive\nNegative\nAug1(\nAug2(\n)\n)\nFigure 2: A matched pair of audio samples found by fine-tuned CLAP that shows a clear sign of\nstitching copy. The sound sources in the two audio samples show high similarity.\nthe best sampler choice and compare our result with baselines including autoregressive models as\nwell as non-autoregressive models.\nSound Generation Evaluation We present our average FAD with a comparative assessment against\nbaseline models on DCASE2023 foley sound dataset. We first compare generic ODE solvers Euler\nand Heun with EI-based DPM-solvers shown in Fig. 1. It can be seen that the higher order EI-\nbased ODE solvers yield faster convergence, therefore fewer NFEs are required during inference.\nParticularly, the 3rd order single-step DPM-solver (DPM-3s) reaches the similar FAD as Yi et al. [42]\nwith only 10 steps and achieves the best score 4.56 at the 50th step. Tab. 1 presents the class-wise\nand overall FAD scores for the DACSE2023 task7 challenge. Our proposed method, EDMSound,\noutperforms the baseline models in terms of the overall FAD in both open and closed tracks, exhibiting\nbetter performance across the majority of class-specific FAD scores as well. In Tab. 2, we present the\nSC09 benchmark result. Particularly, we achieved the lowest FID score without pretrained speech\nrepresentations in [1]. These results underline the efficacy of our proposed methodology across\ndiverse evaluation benchmarks.\nCopy Detection Evaluation We evaluate whether the generative models produce samples copied\nfrom training dataset on the DCASE2023 task7 dataset [3] which contains a wide variety of sound\ntypes. We compare four systems: the EDMSound-best, Scheibler et al. [31] and Jung et al. [4] with\nthe FAD scores being 4.36, 4.78, and 5.02 respectively. For content replication detection, we report\nour results using the fine-tuned CLAP audio encoder based on two observations: First, the similarity\nscore distribution is significantly broader when fine-tuned with CLAP, highlighting its capability in\ndistinguishing similar samples. Secondly, it demonstrates close alignment with human perception,\nas verified through manual examination. Further details regarding the ablation study on other audio\ndescriptors can be found in Appendix B.\nAfter mining the top-1 matched audio samples, we observe high resemblance between a number\nof pairs from our EDMSound-best. Fig. 2 illustrates the waveform and spectrogram of a paired\naudio segment capturing the sound of keyboards. Despite variations in the temporal alignment of key\npresses, the spectral coherence strongly suggests a common sound source originating from the same\nkeyboard. We thereby infer that our diffusion model imitates the characteristics of its training dataset.\nWe term the phenomenon where generated samples closely match the training samples in spectral\ncoherence, but with relaxed temporal alignment, as \u2018stitching copies\u2019. After listening to samples from\nall systems, we find that the model from Scheibler et al. is prone to producing samples that are \u2018exact\ncopies\u2019 with training samples. This strong similarity probably suggests over-fitting in the task of\ngenerating foley sounds using a large, pre-trained LDM-based model with more than 850 million\nparameters [7]. For a more comprehensive visual representation of this phenomenon, please refer to\nour project page. To quantify the overall similarity between the generated data and the training data,\nwe use the 95-percentile similarity score of all matched audio samples defined in Sec. 2. To better\ncompare the distribution difference, we compute the relative similarity by subtracting the training\ndataset similarity scores from the generated ones shown in the lower part of Tab. 1. Despite the\nfact that there are instances from generated samples showing a high degree of similarity, the overall\nnegative relative similarity scores indicate that none of the generative models replicate their training\nset more than the intrinsic similarity within the training set itself.\n4\nConclusion\nThis paper introduced EDMSound, a simple and effective end-to-end diffusion model working on the\ncomplex spectral domain implementing efficient ODE solvers. EDMSound synthesizes high quality\naudio improving the state-of-the-art in terms of FAD on two standard benchmark datasets (SC09 and\nDCASE2023 challenge task7). Furthermore, we proposed fine-tuned CLAP to examine the issue of\ncontent replication in the audio domain.\n4\n5\nAcknowledgement\nThis work was partially supported by a Goergen Institute for Data Science (GIDS) Seed Funding\nAward at the University of Rochester.\nReferences\n[1] M. Baas and H. Kamper. GAN you hear me? reclaiming unconditional speech synthesis from diffusion\nmodels. In 2022 IEEE Spoken Language Technology Workshop, pages 906\u2013911. IEEE, 2023.\n[2] C. Breithaupt and R. Martin. Analysis of the decision-directed snr estimator for speech enhancement with\nrespect to low-snr and transient conditions. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 19(2):277\u2013289, 2010.\n[3] K. Choi, J. Im, L. Heller, B. McFee, K. Imoto, Y. Okamoto, M. Lagrange, and S. Takamichi. Foley sound\nsynthesis at the dcase 2023 challenge. arXiv preprint arXiv:2304.12521, 2023.\n[4] H. C. Chung, Y. Lee, and J. H. Jung. Foley sound synthesis based on gan using contrastive learning without\nlabel information. Technical report, Tech. Rep., June, 2023.\n[5] C. Donahue, J. McAuley, and M. Puckette. Adversarial audio synthesis. In International Conference on\nLearning Representations, 2019.\n[6] C. G\u00e2rbacea, A. van den Oord, Y. Li, F. S. Lim, A. Luebs, O. Vinyals, and T. C. Walters. Low bit-rate\nspeech coding with vq-vae and a wavenet decoder. In ICASSP 2019-2019 IEEE International Conference\non Acoustics, Speech and Signal Processing, pages 735\u2013739. IEEE, 2019.\n[7] D. Ghosal, N. Majumder, A. Mehrish, and S. Poria. Text-to-audio generation using instruction guided\nlatent diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, MM \u201923,\npage 3590\u20133598, 2023.\n[8] K. Goel, A. Gu, C. Donahue, and C. R\u00e9. It\u2019s raw! audio generation with state-space models. In International\nConference on Machine Learning, pages 7616\u20137633, 2022.\n[9] C. Hawthorne, I. Simon, A. Roberts, N. Zeghidour, J. Gardner, E. Manilow, and J. Engel. Multi-instrument\nmusic synthesis with spectrogram diffusion. In ISMIR 2022 Hybrid Conference, 2022.\n[10] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020.\n[11] J. Ho and T. Salimans. Classifier-free diffusion guidance, 2022.\n[12] P.-Y. Huang, H. Xu, J. Li, A. Baevski, M. Auli, W. Galuba, F. Metze, and C. Feichtenhofer. Masked\nautoencoders that listen. Advances in Neural Information Processing Systems, 35:28708\u201328720, 2022.\n[13] Q. Huang, D. S. Park, T. Wang, T. I. Denk, A. Ly, N. Chen, Z. Zhang, Z. Zhang, J. Yu, C. Frank, et al.\nNoise2music: Text-conditioned music generation with diffusion models. arXiv preprint arXiv:2302.03917,\n2023.\n[14] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao. Make-an-audio:\nText-to-audio generation with prompt-enhanced diffusion models. In Proceedings of the 40th International\nConference on Machine Learning, volume 202, pages 13916\u201313932, 23\u201329 Jul 2023.\n[15] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative\nmodels. Advances in Neural Information Processing Systems, 35:26565\u201326577, 2022.\n[16] K. Kilgour, M. Zuluaga, D. Roblek, and M. Sharifi. Fr\u00e9chet audio distance: A metric for evaluating music\nenhancement algorithms. arXiv preprint arXiv:1812.08466, 2018.\n[17] P. E. Kloeden, E. Platen, P. E. Kloeden, and E. Platen. Stochastic differential equations. Springer, 1992.\n[18] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley. PANNs: Large-scale pretrained audio\nneural networks for audio pattern recognition. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 28:2880\u20132894, 2020.\n[19] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro. DiffWave: A versatile diffusion model for audio\nsynthesis. In International Conference on Learning Representations, 2021.\n[20] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley. AudioLDM:\nText-to-audio generation with latent diffusion models. In Proceedings of the 40th International Conference\non Machine Learning, volume 202, pages 21450\u201321474, 23\u201329 Jul 2023.\n[21] X. Liu, H. Liu, Q. Kong, X. Mei, M. D. Plumbley, and W. Wang. Simple pooling front-ends for efficient\naudio classification. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and\nSignal Processing, pages 1\u20135. IEEE, 2023.\n[22] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver: A fast ode solver for diffusion probabilistic\nmodel sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:5775\u20135787,\n2022.\n[23] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu. DPM-Solver++: Fast solver for guided sampling of\ndiffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022.\n[24] S. Mehri, K. Kumar, I. Gulrajani, R. Kumar, S. Jain, J. Sotelo, A. Courville, and Y. Bengio. SampleRNN:\nAn unconditional end-to-end neural audio generation model. In International Conference on Learning\nRepresentations, 2017.\n[25] D. Moffat, R. Selfridge, and J. D. Reiss. Sound effect synthesis. Foundations in Sound Design for\nInteractive Media (New York, NY: Routledge, pages 274\u201399, 2019.\n[26] S. Oh, M. Kang, H. Moon, K. Choi, and B. S. Chon. A demand-driven perspective on generative audio ai.\narXiv preprint arXiv:2307.04292, 2023.\n5\n[27] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and\nK. Kavukcuoglu. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016.\n[28] S. Pascual, G. Bhattacharya, C. Yeh, J. Pons, and J. Serr\u00e0. Full-band general audio synthesis with score-\nbased diffusion. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal\nProcessing, pages 1\u20135. IEEE, 2023.\n[29] J. Richter, S. Welker, J.-M. Lemercier, B. Lay, and T. Gerkmann. Speech enhancement and dereverberation\nwith diffusion-based generative models. IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023.\n[30] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour, R. Gontijo Lopes,\nB. Karagol Ayan, T. Salimans, et al. Photorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\n[31] R. Scheibler, T. Hasumi, Y. Fujita, T. Komatsu, R. Yamamoto, and K. Tachibana. Class-conditioned latent\ndiffusion model for dcase 2023 foley sound synthesis challenge. Technical report, Tech. Rep., June, 2023.\n[32] F. Schroff, D. Kalenichenko, and J. Philbin. FaceNet: A unified embedding for face recognition and\nclustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n815\u2013823, 2015.\n[33] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265,\n2015.\n[34] G. Somepalli, V. Singla, M. Goldblum, J. Geiping, and T. Goldstein. Diffusion art or digital forgery?\ninvestigating data replication in diffusion models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 6048\u20136058, 2023.\n[35] G. Somepalli, V. Singla, M. Goldblum, J. Geiping, and T. Goldstein. Understanding and mitigating copying\nin diffusion models. arXiv preprint arXiv:2305.20086, 2023.\n[36] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. In International Conference on\nLearning Representations, 2020.\n[37] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and B. Poole. Score-based generative mod-\neling through stochastic differential equations. In International Conference on Learning Representations,\n2021.\n[38] Y. Wang, Z. Ju, X. Tan, L. He, Z. Wu, J. Bian, and S. Zhao. AUDIT: Audio editing by following instructions\nwith latent diffusion models. arXiv preprint arXiv:2304.00830, 2023.\n[39] P. Warden. Speech commands: A dataset for limited-vocabulary speech recognition. arXiv preprint\narXiv:1804.03209, 2018.\n[40] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov. Large-scale contrastive language-\naudio pretraining with feature fusion and keyword-to-caption augmentation. In IEEE International\nConference on Acoustics, Speech and Signal Processing, 2023.\n[41] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu. Diffsound: Discrete diffusion model for\ntext-to-sound generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.\n[42] Y. Yuan, H. Liu, X. Liu, X. Kang, M. D. Plumbley, and W. Wang. Latent diffusion model based foley\nsound generation system for dcase challenge 2023 task 7. arXiv preprint arXiv:2305.15905, 2023.\n[43] Q. Zhang and Y. Chen. Fast sampling of diffusion models with exponential integrator. In The Eleventh\nInternational Conference on Learning Representations, 2023.\n[44] W. Zhao, L. Bai, Y. Rao, J. Zhou, and J. Lu. UniPC: A unified predictor-corrector framework for fast\nsampling of diffusion models. arXiv preprint arXiv:2302.04867, 2023.\n6\nAppendices\nA\nTraining and sampling within EDMs\nWhen using complex spectrograms as the diffusion model inputs, the real and imaginary components\nare treated as two separate channels corrupted by Gaussian noise in the forward process. And as a\nresult, the phase information is gradually destroyed. In the reverse sampling process, the real and\nimaginary channels are gradually recovered through the score network and thereby recover the phase.\nA.1\nTraining\nIn DPMs, the neural networks are usually used to model the score [37] of the data at each noise level,\n\u2207x log(x; \u03c3), where x is the data and \u03c3 is the noise level. i.e., the the gradient of the log probability\ndesnity with respect to data. Or equivalently, it can be seen as training a denoiser function [15]\nD(x; \u03c3) to recover clean data given corrupted versions, where \u2207x log(x; \u03c3) = (D(x; \u03c3) \u2212 x)/\u03c32.\nHowever, its magnitude varies significantly on given noise level. To decorrelate the magnitude of the\nnetwork prediction with \u03c3, we follow the preconditioning scales on the denoiser function proposed\nin [15] with cskip(\u03c3)x + cout(\u03c3)F\u03b8(cin(\u03c3)x; cnoise(\u03c3)), where F\u03b8(\u00b7) is the neural network output,\ncin, cout are set to ensure a unit variance for the network inputs and outputs, cskip is set to minimize\nF\u03b8 prediction errors scaled by cout, and \u03bb(\u03c3) is set to 1/c2\nout(\u03c3) to equalize the initial training loss.\nFollowing Karras et al. [15], the desnoiser preconditioning can be written as:\nD(x; \u03c3) =\n\u03c32\ndata\n\u03c32\ndata + \u03c32 x +\n\u03c3 \u00b7 \u03c3data\np\n\u03c32\ndata + \u03c32 F\u03b8\n\u0010\nx\np\n\u03c32\ndata + \u03c32 ; ln(\u03c3)\n4\n\u0011\n.\n(1)\nDuring training, we use \u03c3data = 0.2 as the approximation for the standard deviation of the compressed\ninput spectrogram magnitude values. For \u03c3, we use the log normal noise distribution with mean\nof -3.0 and variance of 1.0. Notice that we did not tune these distribution parameters due to\ninsufficient computation budgets, but we found that when synthesizing sounds without much semantic\ninformation, the final performance is robust to reasonable parameters. Finally, we can write the\ntraining objective as:\nEpdata(x),\u03f5,\u03c3[\u03bb(\u03c3)\u2225D(x + \u03c3\u03f5; \u03c3) \u2212 x\u22252\n2],\n(2)\nwhere pdata(x) represents the training data distribution, \u03f5 \u223c N(0, I) is the standard normal distribu-\ntion, \u03c3 is the noise levels during training, and \u03bb(\u03c3) is the loss weighting factor.\nA.2\nSampling\nEfficient samplers for DPMs can be categorized into training-based methods and training-free\nmethods. Of all the efficient samplers, training-free methods offer the advantage of direct applicability\nto pretrained DPMs without necessitating additional training phases [44]. Recently proposed fast\ntraining-free samplers are using ODE solvers since SDE solvers are hard to converge within a few\nsteps due to the fact that discretizing SDEs is generally difficult in high dimensional space and is\nlimited by the randomness of the Wiener process [22, 17]. Another benefit of using ODE solvers is\nthat such deterministic sampling is able to map the input data into corresponding latent representations\nand useful for editing.\nIn EDM, the probability flow ODE can be formulated as:\ndx = \u2212 \u02d9\u03c3(t) \u03c3(t) \u2207x log p\n\u0000x; \u03c3(t)\n\u0001\ndt.\n(3)\nThis simplification enables the direct application of standard off-the-shelf ODE solvers. When\nnumerically solving ODEs, each step introduces a local error, which cumulatively results in a global\nerror over a specified number of steps. The commonly used Euler\u2019s method is a first order ODE solver\nwith global error linearly proportional to step size. Higher order solvers have lower global error at a\ngiven time but require multiple NFEs at each step. The second order Heun solver [15] provides a\ngood trade-off between global error and NFE. With the advantage of EI-based ODE solvers, we apply\nDPM solvers [23], and our DPM-solvers samplers codebase are adapted from the implementation\nof k-diffusion 2 and DPM-solver official codebase 3. During inference, we use Karras scheduler\n2https://github.com/crowsonkb/k-diffusion\n3https://github.com/LuChengTHU/dpm-solver\n7\nproposed in EDM with a \u03c1 of 7.0, \u03c3min of 0.0001 and \u03c3max 3.0. We also a dynamic threshold of\n0.99 following Imagen [30]. For conditional generation, we use CFG scale of 2.0 as we found it\nachieved the best performance across different samplers.\nA.3\nNeural networks\nWe applies efficien U-Net as our denoiser function backbone, which is designed to be memory\nefficient and converge fast. It reverses the order of downsampling/upsampling operations in order\nto improve the speed of the forward pass. For more detailed descriptions of the architecture, we\nencourage the readers to Appendix B from [30]. Our efficient U-Net is adapted from open source\nImagen4, For the input complex spectrogram, we use short-time Fourier transform (STFT) with\nwindow size of 510 samples and hop size of 256 samples. We use an input channel of 2 for the real\nand imaginary components, 128 as the base dimension and channel multipliers of [1, 2, 2, 2]. For each\ndownsampling/upsampling block, we use 2 ResNet blocks with 2 attention heads in self-attention\nlayer. The model has a total of 45.2 million trainable parameters. We use class label and log \u03c3\nas efficient U-Net conditional inputs. For class conditioning, we represent class labels as one-hot\nencoded vectors, and then feed them through a fully-connected layer.\nB\nComparison of audio descriptors in copy detection\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(a) AudioMAE zero-shot.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(b) AudioMAE fine-tuned.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(c) CLAP zero-shot.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(d) CLAP fine-tuned.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(e) PANNs zero-shot.\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nsimilarity score\ndistribution\nTraining dataset\nEDMSound\nScheibler et al\nJung et al\nYi et al\n(f) PANNs fine-tuned.\nFigure 3: Dataset similarity distribution of top1 matched pairs comparison of AudioMAE, CLAP and\nPANNs.\nIn this section, we compare the dataset similarity distributions from the DCASE2023 challenge task7\nsystems computed using the three audio encoder both with and without the fine-tune process. Fig. 3\nshows the comparison results. In the left column, we present the similarity distribution of audio\nencoders in a zero-shot copy detection scenario (i.e., without fine-tuning). The right column presents\nthe outcomes post fine-tuning. From the figure, we can observe that before fine-tuning, the similarity\nscores are close to 1 especially for AudioMAE, and this suggests that the audio representations are\ntoo close for intra-class samples. This fine-tuning helps to discriminate audio samples within the\nsame class as the similarity score distribution masses shift to the left and spread out compared to\n4https://github.com/lucidrains/imagen-pytorch\n8\nthe models without fine-tuning. Upon evaluation, the fine-tuned CLAP model exhibits the most\ndistinctive distribution spread compared to other models. Manual listening evaluations of matched\npairs from all models further confirm that the fine-tuned CLAP and PANNs consistently produce\npairs that match with human auditory perception. In conclusion, we use the fine-tuned CLAP in our\ncopy detection analysis in the main text.\n9\n"
  },
  {
    "title": "Single-Image 3D Human Digitization with Shape-Guided Diffusion",
    "link": "https://arxiv.org/pdf/2311.09221.pdf",
    "upvote": "18",
    "text": "Single-Image 3D Human Digitization with Shape-Guided\nDiffusion\nBadour AlBahar\nKuwait University\nKuwait City, Kuwait\nbadour.albahar@ku.edu.kw\nShunsuke Saito\nMeta\nPittsburgh, Pennsylvania, USA\nshunsukesaito@meta.com\nHung-Yu Tseng\nMeta\nSeattle, Washington, USA\nhungyutseng@meta.com\nChangil Kim\nMeta\nSeattle, Washington, USA\nchangil@meta.com\nJohannes Kopf\nMeta\nSeattle, Washington, USA\njkopf@meta.com\nJia-Bin Huang\nUniversity of Maryland\nCollege Park, Maryland, USA\njbhuang@umd.edu\nInput image\n|                                                            {z                                                            }\n360\u25e6 generation\nInput image\n|                                                            {z                                                            }\n360\u25e6 generation\nFigure 1: 3D Human Digitization from a Single Image. For a single image as input, our approach synthesizes the 3D consistent\ntexture of a person without relying on any 3D scans for supervised training. Our key idea is to leverage high-capacity 2D\ndiffusion models pretrained for general image synthesis tasks as a human appearance prior. Images from Adobe Stock.\nABSTRACT\nWe present an approach to generate a 360-degree view of a person\nwith a consistent, high-resolution appearance from a single input\nimage. NeRF and its variants typically require videos or images\nfrom different viewpoints. Most existing approaches taking monoc-\nular input either rely on ground-truth 3D scans for supervision\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the author\u2019s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in SIGGRAPH Asia\n2023 Conference Papers (SA Conference Papers \u201923), December 12\u201315, 2023, Sydney, NSW,\nAustralia, https://doi.org/10.1145/3610548.3618153.\nor lack 3D consistency. While recent 3D generative models show\npromise of 3D consistent human digitization, these approaches do\nnot generalize well to diverse clothing appearances, and the results\nlack photorealism. Unlike existing work, we utilize high-capacity\n2D diffusion models pretrained for general image synthesis tasks\nas an appearance prior of clothed humans. To achieve better 3D\nconsistency while retaining the input identity, we progressively\nsynthesize multiple views of the human in the input image by in-\npainting missing regions with shape-guided diffusion conditioned\non silhouette and surface normal. We then fuse these synthesized\nmulti-view images via inverse rendering to obtain a fully textured\nhigh-resolution 3D mesh of the given person. Experiments show\narXiv:2311.09221v1  [cs.CV]  15 Nov 2023\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang\nthat our approach outperforms prior methods and achieves photo-\nrealistic 360-degree synthesis of a wide range of clothed humans\nwith complex textures from a single image.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Texturing.\nKEYWORDS\nDigital humans, single-image 3D reconstruction, diffusion models\nACM Reference Format:\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes\nKopf, and Jia-Bin Huang. 2023. Single-Image 3D Human Digitization with\nShape-Guided Diffusion. In SIGGRAPH Asia 2023 Conference Papers (SA\nConference Papers \u201923), December 12\u201315, 2023, Sydney, NSW, Australia. ACM,\nNew York, NY, USA, 11 pages. https://doi.org/10.1145/3610548.3618153\n1\nINTRODUCTION\nA photorealistic 3D human synthesis is indispensable for a myriad\nof applications in various fields, including fashion, entertainment,\nsports, and AR/VR. However, creating a photorealistic 3D human\nmodel typically requires multi-view images [Kwon et al. 2021; Liu\net al. 2021a; Peng et al. 2021a,b] or 3D scanning systems [Bagautdi-\nnov et al. 2021; Saito et al. 2021] as input, which hinders everyone\nfrom effortlessly experiencing personalized 3D human digitization.\nIn this work, we aim to create a photorealistic 3D human that can\nbe rendered from arbitrary viewpoints from a single input image.\nDespite its attractive utility, reducing the input to monocular data is\nhighly challenging because the person\u2019s backside is not observable,\nand 3D reconstruction from a single image inherently suffers from\ndepth ambiguity.\nTo address these challenges, data-driven methods have made\nsignificant progress in recent years by incorporating prior infor-\nmation into various 3D representations such as meshes [Alldieck\net al. 2019a], voxels [Varol et al. 2018], and neural fields [Saito et al.\n2019]. While the geometric fidelity of 3D reconstruction drastically\nimproved over the last several years [Alldieck et al. 2022a; He et al.\n2021; Huang et al. 2020; Saito et al. 2020; Xiu et al. 2022; Zheng et al.\n2021], its appearance, especially for the occluded regions, is still\nfar from photorealistic (Figure 2). This is primarily because these\napproaches require 3D ground-truth data for supervision, and the\navailable 3D scans of clothed humans are insufficient to learn the\nentire span of clothing appearance. The appearance of clothing is\nsignificantly more diverse than the geometry, and creating a large\nset of high-quality textured 3D scans of people remains infeasible.\nAn image collection in the wild is another source of human ap-\npearance prior. Images are easily accessible at scale and provide a\nhigh variation of clothing appearances. By leveraging large-scale\nimage datasets and high-capacity generative models [Karras et al.\n2019, 2020], 2D human synthesis approaches show impressive repos-\ning of clothed humans from a single image [AlBahar et al. 2021;\nLewis et al. 2021]. However, they often produce an incoherent ap-\npearance with the input image for large rotations because their\nunderlying representation is not in 3D. While 3D generative mod-\nels have recently demonstrated 3D-consistent view synthesis of\nclothed humans [Bergman et al. 2022; Hong et al. 2023; Zhang et al.\n2022], we observe that these approaches do not generalize well to\nPIFu\nImp++\nTEXTure\nMagic123\nOurs\nFigure 2: Limitations of existing methods. Existing 3D human\ngeneration approaches from a single image lack photoreal-\nism. Existing methods such as PIFu [Saito et al. 2019] suffer\nfrom blurriness; Impersonator++ [Liu et al. 2021b] tends to\nduplicate content from the front view, suffering from pro-\njection artifacts; TEXTure [Richardson et al. 2023] fails to\npreserve the appearance of the input view and results in sat-\nurated colors; Magic123 [Qian et al. 2023] fails to synthesize\nrealistic shape and appearance. Images from Adobe Stock.\nvarious clothing appearances and the results are not sufficiently\nphotorealistic.\nIn this paper, we argue that the suboptimal performance of ex-\nisting approaches stems from the limited diversity of training data.\nHowever, expanding existing 2D-clothed human datasets also re-\nquires nontrivial curation and annotation efforts. To address this\nlimitation, we propose a simple yet effective algorithm to create a\n3D consistent textured human from a single image without relying\non a curated 2D clothed human dataset for appearance synthesis.\nOur key idea is to utilize powerful 2D generative models trained\non an extremely large corpus of images as a human appearance\nprior. In particular, we use latent diffusion models [Rombach et al.\n2022], which allows us to synthesize diverse and photorealistic\nimages. Unlike recent works that leverage 2D diffusion models for\n3D object generation from text inputs [Lin et al. 2023; Poole et al.\n2022; Richardson et al. 2023], we employ diffusion models to recon-\nstruct a 360-degree view of a real person in the input image in a\n3D-consistent manner.\nWe first reconstruct the 3D geometry of the person using an\noff-the-shelf tool [Saito et al. 2020] and then generate the back-\nview of the input image using a 2D single image human reposing\napproach [AlBahar et al. 2021] to ensure that the completed views\nare consistent with the input view. Next, we synthesize multi-view\nimages of the person by progressively inpainting novel views utiliz-\ning a pretrained inpainting diffusion model guided by both normal\nand silhouette maps to constrain the synthesis to the underlying 3D\nstructure. To generate a (partial) novel view, we aggregate all other\nviews by blending their RGB color based on importance. Similar\nto previous work [Buehler et al. 2001; Rong et al. 2022; Xiang et al.\n2023], we use the angular differences between the visible pixels of\nthose views and the current view of interest as well as their distance\nto the nearest missing pixel to determine the appropriate weight\nfor each view in the blending process. This ensures that the result-\ning multi-view images are consistent with each other. Finally, we\nSingle-Image 3D Human Digitization with Shape-Guided Diffusion\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nperform multi-view fusion by accounting for slight misalignment\nin the synthesized multi-view images to obtain a fully textured\nhigh-resolution 3D human mesh.\nOur experiments show that the proposed approach achieves a\nmore detailed and faithful synthesis of clothed humans than prior\nmethods without requiring high-quality 3D scans or curated large-\nscale clothed human datasets.\nOur contributions include:\n\u2022 We demonstrate, for the first time, that a 2D diffusion model\ntrained for general image synthesis can be utilized for 3D\ntextured human digitization from a single image.\n\u2022 Our approach preserves the shape and the structural details\nof the underlying 3D structure by using both normal maps\nand silhouette to guide the diffusion model.\n\u2022 We enable 3D consistent texture reconstruction by fusing the\nsynthesized multi-view images into the shared UV texture\nmap.\n2\nRELATED WORK\n2.1\n2D human synthesis.\nGenerative adversarial networks (GANs) enable the photorealistic\nsynthesis of human faces [Karras et al. 2019, 2020] and bodies [Fu\net al. 2022]. While these models are unconditional, several works\nextend them to conditional generative models such that we can\ncontrol poses while retaining the identity of an input subject. By\nincorporating additional conditions these works can achieve human\nreposing [AlBahar and Huang 2019; AlBahar et al. 2021; Liu et al.\n2021b; Ma et al. 2017, 2018; Men et al. 2020; Ren et al. 2020; Sarkar\net al. 2021; Siarohin et al. 2018; Zhu et al. 2019], virtual try-on [Al-\nBahar et al. 2021; Lewis et al. 2021], motion transfer [Aberman\net al. 2019; Chan et al. 2019; Liu et al. 2021b; Yoon et al. 2021]. Pose-\nwith-style [AlBahar et al. 2021] utilizes dense pose [G\u00fcler et al.\n2018] to warp input images to the target view as an initialization of\nthe synthesis. Impersonator++ [Liu et al. 2021b] further improves\nthe robustness to a large pose change by leveraging a parametric\nhuman body model [Loper et al. 2015] and warping blocks to bet-\nter preserve the information from the input. While these methods\nenable the control of viewpoints by changing the input pose, the\nresults suffer from view inconsistency. In contrast, our approach\nachieves 3D consistent generation of textured clothed humans.\n2.2\nUnconditional 3D human synthesis.\nMore recently, neural fields and inverse rendering techniques al-\nlow us to train 3D GANs with only 2D images [Chan et al. 2022,\n2021; Niemeyer and Geiger 2021]. These 3D GANs are extended\nto articulated full-body humans using warping based on linear\nblend skinning [Bergman et al. 2022; Hong et al. 2023; Zhang et al.\n2022]. By applying inversion [Roich et al. 2022], these methods can\ngenerate a 360-degree rendering of a clothed human from a single\nimage. While these results are 3D consistent, we observe that they\nare plausible only for relatively simple clothing and degrade for\nmore complex texture patterns. Achieving photorealistic and gen-\neralizable 3D human digitization with 3D GANs remains an open\nproblem. Our work achieves better generalization and photorealism\nby incorporating more general yet highly expressive image priors\nfrom diffusion models.\n2.3\n3D human reconstruction from a single\nimage.\n3D reconstruction of clothed humans from a single image is a long-\nstanding problem. A parametric body model [Loper et al. 2015]\nprovides strong prior about the underlying shape of a person, but\nonly for minimally clothed bodies [Kanazawa et al. 2018; Kolo-\ntouros et al. 2019; Lassner et al. 2017; Pavlakos et al. 2018]. To\nenable clothed human reconstruction, regression-based 3D recon-\nstruction has been extended to various shape representations such\nas voxels [Varol et al. 2018], mesh displacements [Alldieck et al.\n2019a,b; Bhatnagar et al. 2019], silhouettes [Natsume et al. 2019],\ndepth maps [Gabeur et al. 2019; Wang et al. 2020], and neural\nfields [Corona et al. 2021; He et al. 2021; Huang et al. 2020; Saito\net al. 2019, 2020; Smith et al. 2019; Xie et al. 2022; Xiu et al. 2023,\n2022]. Among them, several works also support texture synthesis\nfor the occluded regions. SiCloPe [Natsume et al. 2019] shows that\nan image-to-image translation network in screen space can infer\noccluded textures. PIFu [Saito et al. 2019] infers continuous texture\nfields [Oechsle et al. 2019] in 3D, which is later improved by explic-\nitly modeling reflectances [Alldieck et al. 2022b]. These approaches,\nhowever, often fail to produce photorealistic textures for the back\nside due to the limited 3D scan data for supervised training. Differ-\nentiable rendering based on NeRFs [Mildenhall et al. 2020] has also\nbeen applied to learn 3D human representations from images. Both\nperson-specific models [Liu et al. 2021a; Peng et al. 2021b; Weng\net al. 2022] and generalizable models across identities [Choi et al.\n2022; Gao et al. 2022; Hu et al. 2023; Huang et al. 2022; Kwon et al.\n2021; Mihajlovic et al. 2022] have been proposed, but the training\nrequires multi-view images or videos. They are difficult to collect at\nscale such that the collected data covers a sufficient span of cloth-\ning types and textures. Our approach, on the other hand, does not\nrequire multi-view images or person-specific video capture.\n2.4\nDiffusion models for 3D synthesis.\nDenoising diffusion models have shown impressive image synthesis\nresults. These powerful 2D generative models are recently adopted\nto learn 3D scene representations. Recent methods [Chen et al. 2023;\nLin et al. 2023; Metzer et al. 2023; Poole et al. 2022; Wang et al. 2022,\n2023] have shown that text-to-image models can be repurposed\nfor 3D object generation from text input with remarkable results.\nUnlike these methods, our method is conditioned on a human input\nimage to create a 3D consistent texture of the person, where the\nresults are photorealistic. Diffusion models can be customized for a\nspecific subject, but this customization typically requires multiple\nimages and a considerable amount of time [Gal et al. 2022; Ruiz et al.\n2022]. Moreover, such methods may not consistently maintain the\nsubject\u2019s appearance details (i.e. clothing, hairstyle, facial expres-\nsion, etc.) [Rinon Gal 2023]. These customization methods can be\nutilized to generate 3D objects conditioned on a single image [Qian\net al. 2023; Xu et al. 2022]. Unlike these customization methods, our\nmethod can generate 3D textured human models without test-time\nfinetuning. Moreover, current image-to-3D techniques [Qian et al.\n2023; Tang et al. 2023; Xu et al. 2022] lack human-specific prior and\nhence struggle to synthesize realistic and detailed textured human\nmodels. The closest to our work is TEXTure [Richardson et al. 2023],\nwhich utilizes 2D diffusion models to synthesize texture of an input\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang\nmesh. We observe that their shape guidance based on depth maps is\ninsufficient for photorealistic clothed human synthesis. Instead of\nprogressively refining the texture based on viewing angles, we im-\nprove the consistency by blending the RGB color of existing views,\nweighted by visibility, viewing angles, and distance to missing re-\ngions. We also improve the per-view synthesis by incorporating\nnormal and silhouette maps as guidance signals.\n3\nMETHOD\nOur goal is to generate a 360-degree view of a person with a consis-\ntent, high-resolution appearance from a single input image. To this\nend, we first synthesize a set of multi-view images of the person\n{\u02c6\ud835\udc3c2, ..., \u02c6\ud835\udc3c\ud835\udc41 } that are consistent among each other and coherent with\nthe input image \ud835\udc3c1 (Figure 3). In particular, we use the reconstructed\n3D geometry of the person to guide the inpainting with diffusion\nmodels (Figure 4). For 3D shape reconstruction, we employ an off-\nthe-shelf method [Saito et al. 2020] to obtain a triangular mesh \ud835\udc3a of\nthe input person using Marching cubes [Lorensen and Cline 1987].\nWe synthesize the multi-view images in an auto-regressive man-\nner. More specifically, we start with synthesizing the back-view\nof the person with [AlBahar et al. 2021] (Section 3.1). The input\nand the synthesized back-view images form an initial support set \ud835\udc49\n(i.e., currently available views). Using the images from the support\nset and the mesh \ud835\udc3a, we can render a new view of the person (Sec-\ntion 3.2). Here, this blended view is consistent with the previously\ngenerated images but may have missing regions (that are not cov-\nered by any of the images in the support set). We use a shape-guided\ndiffusion model to inpaint the appearance details while respecting\nthe estimated shape (Section 3.3). We expand the support set by\nadding this inpainted view and proceed to a new view until all the\nviews are generated. We sample views at intervals of 45\u25e6, specifi-\ncally in the order of [45\u25e6, \u221245\u25e6, 90\u25e6, \u221290\u25e6, 135\u25e6, \u2212135\u25e6, 180\u25e6]. Thus,\nour support set will have a total of 8 views (\ud835\udc41 = 8). When we use\nmore viewpoints, the missing regions become very small. In such\ncases, we found that the inpainting performance deteriorates. On\nthe other hand, when we use less viewpoints, the missing regions\nbecome very large. We found that the inpainting fails to preserve\nthe input appearance.\nWe then fuse these multi-view images {\ud835\udc3c1, \u02c6\ud835\udc3c2, ..., \u02c6\ud835\udc3c\ud835\udc41 } via inverse\nrendering robust to slight misalignment and optimize a UV texture\nmap \ud835\udc47 (Figure 5). We finally use this UV texture map \ud835\udc47 to render\nthe 360-degree view of the person. Note that our approach assumes\nweak perspective projection for simplicity, following [Saito et al.\n2019, 2020; Xiu et al. 2022], but extending it to a perspective camera\nis also possible.\n3.1\nBack-view Synthesis\nThe input frontal and back views have strong semantics correla-\ntions (e.g., the back side of a T-shirt is likely a T-shirt with similar\ntextures), and its silhouette contour provides structural guidance.\nThus, we first synthesize the back-view of the person for guidance\nprior to synthesizing other views. While prior works [He et al.\n2021; Natsume et al. 2019] show that front-to-back synthesis is\nhighly effective with supervised training, our approach achieves\nthe front-to-back synthesis without relying on ground-truth paired\ndata. More specifically, we apply the SoTA 2D human synthesis\nmethod [AlBahar et al. 2021] with the inferred dense pose predic-\ntion for the back-view. To generate a dense pose prediction that\naligns precisely with the input image, we render the surface nor-\nmal and depth map of the shape \ud835\udc3a from the view opposite to the\ninput view and create a photorealistic back-view using Control-\nNet [Zhang and Agrawala 2023] with the text prompt of \u201cback view\nof a person wearing nice clothes in front of a solid gray background,\nbest quality.\u201d We then run dense pose [G\u00fcler et al. 2018], which is\nfinally fed into Pose-with-Style [AlBahar et al. 2021]. We empiri-\ncally find that using Pose-with-Style [AlBahar et al. 2021] with the\naforementioned procedure leads to a more semantically consistent\nback-view than just using ControlNet [Zhang and Agrawala 2023].\nSee Figure 7 for the impact of the back-view initialization.\n3.2\nMulti-view visible texture aggregation\nPrior to inpainting, we aggregate all the views in the support set \ud835\udc49\nto the target view\ud835\udc49\ud835\udc50. However, naively averaging all views leads to a\nblurry image due to slight misalignment in each view. To ensure that\nhigh-resolution details are all retained, we use weighted averaging\nusing confidence based on visibility, viewing angles, and distance.\nFor each view \ud835\udc49\ud835\udc63 in the set of synthesized views \ud835\udc49\ud835\udc63, we render\nthe normal map \ud835\udc41\ud835\udc50\ud835\udc63 as well as its color \ud835\udc36\ud835\udc50\ud835\udc63 from \ud835\udc49\ud835\udc50. In addition, we\nset the visibility mask \ud835\udc40\ud835\udc63 of each view \ud835\udc49\ud835\udc63 by comparing its visible\nfaces to the visible faces from \ud835\udc49\ud835\udc50. We use this visibility mask \ud835\udc40\ud835\udc63 to\ncompute distance transform \ud835\udc51\ud835\udc63 from the boundary of the visible\npixels and the invisible pixels in each view \ud835\udc49\ud835\udc63. We also compute\nthe angular difference \ud835\udf19\ud835\udc63 of each visible pixel between view \ud835\udc49\ud835\udc63 and\nthe current view of interest \ud835\udc49\ud835\udc50 as follows:\n\ud835\udf19\ud835\udc63 = \ud835\udc40\ud835\udc63 arccos\n\u0012\n\ud835\udc41\ud835\udc50\ud835\udc63 \u00b7 \ud835\udc41\ud835\udc50\nmax (||\ud835\udc41\ud835\udc50\ud835\udc63 ||2 \u00b7 ||\ud835\udc41\ud835\udc50 ||2,\ud835\udf16)\n\u0013\n,\n(1)\nwhere \ud835\udf16 = 10\u22128 is a small value to avoid dividing by zero.\nFinally, we compute the blending weight\ud835\udc64\ud835\udc63 of view\ud835\udc49\ud835\udc63 as follows:\n\ud835\udc64\ud835\udc63 =\n\ud835\udc40\ud835\udc63\ud835\udc35\ud835\udc63\ud835\udc52\u2212\ud835\udefc\ud835\udf19\ud835\udc63\ud835\udc51\ud835\udefd\n\ud835\udc63\n\u00cd\n\ud835\udc56\u2208\ud835\udc49 \ud835\udc40\ud835\udc56\ud835\udc35\ud835\udc56\ud835\udc52\u2212\ud835\udefc\ud835\udf19\ud835\udc56\ud835\udc51\ud835\udefd\n\ud835\udc56 + \ud835\udf16\n.\n(2)\nIn our experiments, we set both \ud835\udefc, which determines the strength of\nthe angular difference, and \ud835\udefd, which determines the strength of the\nEuclidean distance, to 3. Using the angular difference \ud835\udf19\ud835\udc63 ensures a\nhigher weight to closer views, while using the Euclidean distance\n\ud835\udc51\ud835\udc63 ensures a lower weight for pixels close to the missing region.\nMoreover, if only one existing view contains a specific pixel, we\nmark its boundary \ud835\udc35\ud835\udc63 as invisible. This ensures that the target view\ndoes not suffer from boundary artifacts.\nWe use the computed weights \ud835\udc64\ud835\udc63 to blend the color \ud835\udc36\ud835\udc63 of the\npreviously synthesized views \ud835\udc49\ud835\udc63 together, where the blended image\nof the current view \ud835\udc3c\ud835\udc50 and its visibility mask \ud835\udc40\ud835\udc50 are as follows:\n\ud835\udc40\ud835\udc50 =\n\u00d8\n\ud835\udc56\u2208\ud835\udc49\n\ud835\udc40\ud835\udc56,\nand\n\ud835\udc3c\ud835\udc50 =\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n\ud835\udc64\ud835\udc56\ud835\udc36\ud835\udc56.\n(3)\nThe final blended image \ud835\udc3c\ud835\udc50 and its visibility mask \ud835\udc40\ud835\udc50 are then used\nto synthesize a complete view \u02c6\ud835\udc3c\ud835\udc50 using our shape-guided diffusion.\n3.3\nShape-guided diffusion inpainting\nTo synthesize the unseen appearance indicated by the visibility\nmask \ud835\udc40\ud835\udc50 in the blended image \ud835\udc3c\ud835\udc50, we use a 2D inpainting diffusion\nmodel [Rombach et al. 2022]. However, we observe that without any\nSingle-Image 3D Human Digitization with Shape-Guided Diffusion\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nInput image\n\ud835\udc3c-\nShape \nprediction\nBlended view\n\ud835\udc3c8\nVisibility mask\n\ud835\udc408\nOutput view\n\ud835\udc3c@8\nBack view \nsynthesis\n(section 3.1)\nAggregate\n(section 3.2)\nShape-guided \ndiffusion inpainting\n(section 3.3)\n3D geometry\n\ud835\udc3a\nBack view\n\ud835\udc3cLM\nRender\nView \ud835\udc7d\ud835\udc84\nView \ud835\udc7d\ud835\udc84\nGuidance signal\nNormal map\n\ud835\udc418\nSilhouette map\n\ud835\udc468\nSupport set\n\ud835\udc49\nInitialization\nShape-guided diffusion\nFigure 3: Person image generation with shape-guided diffusion. To generate a 360-degree view of a person from a single image\n\ud835\udc3c1, we first synthesize multi-view images of the person. We use an off-the-shelf method to infer the 3D geometry [2020] and\nsynthesize an initial back-view \u02dc\ud835\udc3c\ud835\udc41 of the person [2021] as a guidance. We add our input view \ud835\udc3c1 and the synthesized initial\nback-view \u02dc\ud835\udc3c\ud835\udc41 to our support set \ud835\udc49 . To generate a new view \ud835\udc49\ud835\udc50, we aggregate all the visible pixels from our support set \ud835\udc49 by\nblending their RGB color, weighted by visibility, viewing angles, and the distance to missing regions. To hallucinate the unseen\nappearance and synthesize view \u02c6\ud835\udc3c\ud835\udc50, we use a pretrained inpainting diffusion model guided by shape cues (normal \ud835\udc41\ud835\udc50 and\nsilhouette \ud835\udc46\ud835\udc50 maps). We include the generated view \u02c6\ud835\udc3c\ud835\udc50 in our support set and repeat this process for all the remaining views.\nImages from Adobe Stock.\nInput\n(a) No\nguidance\n(b) Normal\n(c) Silhouette\n(d) Normal\nand\nsilhouette\nFigure 4: Shape-guided diffusion inpainting. To synthesize\nthe unseen appearance in a new view, we use a pretrained\ninpainting diffusion model. With no guidance, the inpainted\nregions often do not preserve the shape (red silhouette) nor\nthe structural details of the 3D geometry (a). If we use nor-\nmal maps as a control signal for ControlNet [2023] (b), the\ninpainted region preserves the structural details of the mesh\n(e.g., fingers), but not the shape of the human body. Using\nthe silhouette map preserves the shape of the human body,\nbut not the structural details of the mesh (c). We propose to\nuse both normal and silhouette maps to guide the inpainting\nmodel to respect the underlying 3D geometry (d). Images\nfrom Adobe Stock.\nguidance, the inpainted regions often do not respect the underlying\ngeometry \ud835\udc3a (see Figure 4(a)). To address this, we use the method of\nControlNet [Zhang and Agrawala 2023] by incorporating additional\nstructural information into the diffusion model. When we use nor-\nmal maps as a control signal, we can preserve the structural details\nof the mesh but not the shape of the human body (Figure 4(b)).\nOn the other hand, using the silhouette map alone preserves the\nshape of the human body, but not the structural details of the mesh\n(Figure 4(c)). To best guide the inpainting model to respect the\nunderlying 3D geometry, we propose to use both normal map and\nsilhouette maps, as shown in Figure 4(d). We add this generated\nview to our support set \ud835\udc49 and proceed to the next view until all \ud835\udc41\nviews are synthesized.\n3.4\nMulti-view fusion\nSince the latent diffusion model operates inpainting in the low-\nresolution latent space, the final synthesized images do not form\ngeometrically consistent multi-view images. Therefore, we consoli-\ndate these slightly misaligned multi-view images \ud835\udc3c1, \u02c6\ud835\udc3c2, ..., \u02c6\ud835\udc3c\ud835\udc41 } into\na single consistent 3D texture map \ud835\udc47. We show the overview of our\nmulti-view fusion in Figure 5.\nWe first compute the UV parameterization of the reconstructed\n3D geometry using xatlas [Young 2021]. Then, we optimize a UV\ntexture map \ud835\udc47 via inverse rendering with loss functions that are\nrobust to small misalignment. In every iteration, we render the UV\ntexture map\ud835\udc47 in each view \ud835\udc56 from our set of synthesized views {\ud835\udc49 =\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang\nRender\nRender\nRender\nView \ud835\udc49+\n\u2026\n\u2026\nLoss\nInput image\n\ud835\udc3c8\nView \ud835\udc499 output\n\ud835\udc3c:9\nUV texture map\n\ud835\udc47\n3D geometry\n\ud835\udc3a\nLoss\nLoss\nView \ud835\udc499\nView \ud835\udc498\nSynthesized views\nView \ud835\udc49+ output\n\ud835\udc3c:+\nFigure 5: Multi-view fusion. We fuse the synthesized multi-\nview images {\ud835\udc3c1, \u02c6\ud835\udc3c2, ..., \u02c6\ud835\udc3c\ud835\udc41 } (see Figure 3) to obtain a textured\n3D human mesh. We use the computed UV parameteriza-\ntion [2021] to optimize a UV texture map \ud835\udc47 with the geom-\netry \ud835\udc3a fixed. In each iteration, we differentiably render the\nUV texture map \ud835\udc47 in every synthesized view from our set\nof views {\ud835\udc49 = \ud835\udc491,\ud835\udc491, ...,\ud835\udc49\ud835\udc41 }. We minimize the reconstruction\nloss between the rendered view and our synthesized view us-\ning both LPIPS loss [2018] and L1 loss. The fusion results in\na textured mesh that can be rendered from any view. Images\nfrom Adobe Stock.\n\ud835\udc491,\ud835\udc491, ...,\ud835\udc49\ud835\udc41 } and minimize the reconstruction loss of this rendered\nview and the synthesized view using both LPIPS loss [Zhang et al.\n2018] and L1 loss such that:\n\ud835\udc3f(\ud835\udc47) =\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc49\n\ud835\udc3flpips\n\u0010\n\ud835\udc45\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc5f (\ud835\udc47;\ud835\udc3a,\ud835\udc56), \u02c6\ud835\udc3c\ud835\udc56\n\u0011\n+ \ud835\udf06\ud835\udc3f1\n\u0010\n\ud835\udc45\ud835\udc52\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc5f (\ud835\udc47;\ud835\udc3a,\ud835\udc56), \u02c6\ud835\udc3c\ud835\udc56\n\u0011\n,\n(4)\nwhere \u02c6\ud835\udc3c1 = \ud835\udc3c1 and \ud835\udf06 is set to 10.\nOnce the texture map\ud835\udc47 is optimized, one can render the textured\nmesh from arbitrary viewpoints.\n4\nEXPERIMENTAL RESULTS\n4.1\nExperimental Setup\n4.1.1\nImplementation details. We implement our approach with\nPyTorch on a single RTX A6000 GPU. We set the guidance scale\nof the pretrained inpainting diffusion model to 15 and the number\nof inference steps per view to 25. In all our experiments, we use a\ngeneric text prompt for all subjects: \u201ca person wearing nice clothes\nin front of a solid white background, <VIEW> view, best quality, ex-\ntremely detailed\", where <VIEW> is set to \u201cfront\u201d for frontal views;\n\u201cleft\u201d and \u201cright\u201d for 45\u25e6 and \u221245\u25e6 views, respectively; \u201cside\u201d for\n\u00b190\u25e6 views; and \u201cback\u201d for the rest of viewing angles (\u00b1135\u25e6 and\n180\u25e6). We use the ADAM optimizer with a learning rate of 0.1 and\nwith \ud835\udefd1 = 0.9 and \ud835\udefd2 = 0.999 to learn the UV texture map \ud835\udc47. The\nentire process of generating a 3D textured model from a single\nimage takes approximately 7 minutes on an RTX A6000 GPU.\nTable 1: Quantitative comparisons with baseline methods on\nthe THuman2.0 dataset [Yu et al. 2021].\nMethods\nPSNR\u2191\nSSIM\u2191\nFID\u2193\nLPIPS\u2193\nCLIP-score\u2191\nPwS baseline\n17.8003\n0.8888\n132.4511\n0.1320\n0.7733\nPIFu\n18.0934\n0.9117\n150.6622\n0.1372\n0.7721\nImpersonator++\n16.4791\n0.9012\n106.5753\n0.1468\n0.8168\nTEXTure\n16.7869\n0.8740\n215.7078\n0.1435\n0.7272\nMagic123\n14.5013\n0.8768\n137.1108\n0.1880\n0.7996\nS3F\n14.1212\n0.8840\n165.9806\n0.1868\n0.7475\nOurs\n17.3651\n0.8946\n115.9918\n0.1300\n0.7992\n4.1.2\nDatasets. To evaluate our approach, we utilize the THu-\nman2.0 dataset [Yu et al. 2021], using 30 subjects, evenly split be-\ntween 15 males and 15 females. We use front-facing images as input.\nWe also evaluate our approach on the DeepFashion dataset [Liu\net al. 2016] to compare with ELICIT [Huang et al. 2022]. We ad-\nditionally use in-the-wild images from Adobe Stock1 to showcase\nresults from images with diverse subjects, clothing, and poses.2\n4.1.3\nBaselines. We compare our 360-degree view synthesis ap-\nproach with Pose with Style (PwS) baseline. We use Pose with\nStyle [AlBahar et al. 2021] to generate multi-view images and\nthen fuse them using our multi-view fusion. We also compare\nwith PIFu [Saito et al. 2019], Impersonator++ [Liu et al. 2021b],\nTEXTure [Richardson et al. 2023], Magic123 [Qian et al. 2023], and\nS3F [Corona et al. 2023]. To make TEXTure [Richardson et al. 2023]\nconditional on an input image, we use the input image directly\ninstead of generating an initial view from the depth-to-image dif-\nfusion model. We also compare our work with ELICIT [Huang\net al. 2022] on a subset of the DeepFashion dataset [Liu et al. 2016]\nprovided by its authors.\n4.2\nQuantitative Comparison\nTo quantify the quality of our results, we measure peak signal-\nto-noise ratio (PSNR), structural similarity index measure (SSIM),\nFrechet Inception Distance (FID) [Parmar et al. 2022], learned per-\nceptual image patch similarity (LPIPS) [Zhang et al. 2018], and\nCLIP-score. CLIP-score measures the cosine similarity between the\nCLIP embeddings of an input image and each of the synthesized\nviews. We use a total of 90 synthesized views with 4\u25e6 spacing. We\ncompare these metrics on the THuman2.0 dataset [Yu et al. 2021]\nwith other baselines in Table 1. Quantitative results show that ex-\nisting metrics are not consistent in evaluating 3D textured humans.\nPSNR favors blurry images as in PIFu [Saito et al. 2019], and FID\ndoes not provide accurate results for sparse view distributions.\nTo quantitatively compare with ELICIT [Huang et al. 2022], we\ncompute the CLIP-score (where higher values indicate better per-\nformance) on their provided subset of the DeepFashion dataset [Liu\net al. 2016]. Our method achieved a CLIP-score of 0.7732, surpassing\ntheir score of 0.7236.\n1https://stock.adobe.com/\n2All datasets used in this research were exclusively downloaded, accessed, and utilized\non UMD clusters.\nSingle-Image 3D Human Digitization with Shape-Guided Diffusion\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nInput\n|                                  {z                                  }\n360\u25e6 generation\nInput\n|                                  {z                                  }\n360\u25e6 generation\nInput\n|                                  {z                                  }\n360\u25e6 generation\nFigure 6: Limitations. Our approach inherits limitations from existing methods for shape reconstruction (unusual foot shape\n(left)) and back-view synthesis (misaligned skirt length due to lack of geometry awareness (middle)). We also show the baked\nspecularity on the face and garment texture, which is ideally view-dependent (right). Images from Adobe Stock.\nInput\n|                             {z                             }\nWithout back-view\n|                             {z                             }\nOurs\nFigure 7: The need of back-view synthesis. Having an ini-\ntial back-view encourages all other views to preserve the\nappearance of the person in the input image especially when\na target view is far from the input view. Images from Adobe\nStock.\nTable 2: Ablation study on the THuman2.0 dataset [Yu et al.\n2021]. We use the ground truth mesh to evaluate the effec-\ntiveness of initializing the back-view (B), and using normal\n(N) and silhouette (S) maps as guidance signals.\nID\nB\nN\nS\nPSNR\u2191\nSSIM\u2191\nFID\u2193\nLPIPS\u2193\nCLIP-score\u2191\nA\n\u2713\n\u2713\n23.9463\n0.9373\n117.7447\n0.0538\n0.8013\nB\n\u2713\n24.0494\n0.9389\n129.4944\n0.0592\n0.7896\nC\n\u2713\n\u2713\n25.8709\n0.9449\n108.5836\n0.0506\n0.8041\nD\n\u2713\n\u2713\n25.7199\n0.9435\n101.3901\n0.0480\n0.8013\nE\n\u2713\n\u2713\n\u2713\n25.8465\n0.9453\n98.9282\n0.0473\n0.8069\n4.3\nQualitative Comparison\nWe show visual comparisons of our results with the baselines on\nin-the-wild images from Adobe Stock in Figures 1 and 8, and on\nthe THuman2.0 dataset [Yu et al. 2021] in Figure 9. These results\ndemonstrate that our method produces high-resolution, photore-\nalistic 3D human models that respect the appearance of the input,\nfor a variety of input images.\n4.4\nAblation Study\n4.4.1\nGuidance signals. We validate our shape-guided diffusion\ninpainting in Table 2. We show the effect of using no guidance (B),\nonly normal maps (C), only silhouette maps (D), and both normal\nand silhouette maps (E). We also show visual comparison in Figure 4.\nThe use of both normal maps and silhouette maps leads to better\npreserving the synthesized person\u2019s shape and details and thus\nenhancing the quality of resulting 3D human models.\n4.4.2\nBack-view synthesis. We validate the initial back-view syn-\nthesis using a human reposing technique [AlBahar et al. 2021] in\nTable 2 (A vs. E). We also show visual comparison in Figure 7. Hav-\ning an initial back view encourages all other views to preserve\nthe appearance of the input person, especially when clothing has\nnontrivial textures.\n4.5\nLimitations and Future Work\nOur main limitation is the dependence on off-the-shelf methods [Al-\nBahar et al. 2021; Saito et al. 2020] for the base geometry reconstruc-\ntion and back-view synthesis. Figure 6 shows that our approach\ninherits the limitations of these methods. Another limitation is the\nlack of view-dependency. While clothing is mostly diffuse, human\nskin may exhibit view-dependent specular highlights. Extending\nour approach to view-dependent radiance would be an exciting\ndirection, which can be addressed by future work. Furthermore,\nour work does not support human reposing and it requires per-\nsubject UV texture optimization. For the generality of our approach,\nwe use off-the-shelf 3D shape reconstruction methods for clothed\nhumans [Saito et al. 2020; Xiu et al. 2022], which are trained on\n3D ground-truth data. We also use off-the-shelf human reposing\nmethod [AlBahar et al. 2021] for the back-view synthesis. Future\nwork should also enable the high-fidelity 3D shape reconstruction\nof clothed humans and back-view synthesis with general-purpose\n2D diffusion models.\n5\nCONCLUSIONS\nWe introduced a simple yet highly effective approach to generate a\nfully textured 3D human mesh from a single image. Our experiments\nshow that synthesizing a high-resolution and photorealistic texture\nfor occluded views is now possible with shape-guided inpainting\nbased on high-capacity latent diffusion models and a robust multi-\nview fusion method. While 3D human digitization relies on curated\nhuman-centric datasets either in 3D or 2D, our approach, for the first\ntime, achieves superior synthesis results by leveraging a general-\npurpose large-scale diffusion model. We believe our work will shed\nlight on unifying data collection efforts for 3D human digitization\nand other general 2D/3D synthesis methods.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang\nREFERENCES\nKfir Aberman, Mingyi Shi, Jing Liao, Dani Lischinski, Baoquan Chen, and Daniel\nCohen-Or. 2019. Deep video-based performance cloning. In Computer Graphics\nForum, Vol. 38. 219\u2013233.\nBadour AlBahar and Jia-Bin Huang. 2019. Guided image-to-image translation with\nbi-directional feature transformation. In ICCV.\nBadour AlBahar, Jingwan Lu, Jimei Yang, Zhixin Shu, Eli Shechtman, and Jia-Bin\nHuang. 2021. Pose with Style: Detail-Preserving Pose-Guided Image Synthesis with\nConditional StyleGAN. ACM TOG (2021).\nThiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and\nGerard Pons-Moll. 2019a. Learning to reconstruct people in clothing from a single\nRGB camera. In CVPR.\nThiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. 2019b.\nTex2shape: Detailed full human body geometry from a single image. In ICCV.\nThiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. 2022a. Photorealistic\nmonocular 3d reconstruction of humans wearing clothing. In CVPR.\nThiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. 2022b. Photorealistic\nMonocular 3D Reconstruction of Humans Wearing Clothing. In CVPR.\nTimur Bagautdinov, Chenglei Wu, Tomas Simon, Fabian Prada, Takaaki Shiratori,\nShih-En Wei, Weipeng Xu, Yaser Sheikh, and Jason Saragih. 2021. Driving-signal\naware full-body avatars. ACM TOG 40, 4 (2021), 1\u201317.\nAlexander W. Bergman, Petr Kellnhofer, Wang Yifan, Eric R. Chan, David B. Lindell,\nand Gordon Wetzstein. 2022. Generative Neural Articulated Radiance Fields. In\nNeurIPS.\nBharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. 2019.\nMulti-garment net: Learning to dress 3d people from images. In ICCV.\nChris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen.\n2001. Unstructured lumigraph rendering. In Proceedings of the 28th annual conference\non Computer graphics and interactive techniques. 425\u2013432.\nCaroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. 2019. Everybody\ndance now. In ICCV.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini\nDe Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al.\n2022. Efficient geometry-aware 3D generative adversarial networks. In CVPR.\nEric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein.\n2021. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image\nsynthesis. In CVPR.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. 2023. Fantasia3D: Disentangling\nGeometry and Appearance for High-quality Text-to-3D Content Creation. In ICCV.\nHongsuk Choi, Gyeongsik Moon, Matthieu Armando, Vincent Leroy, Kyoung Mu\nLee, and Gregory Rogez. 2022. MonoNHR: Monocular Neural Human Renderer.\nInternational Conference on 3D Vision.\nEnric Corona, Albert Pumarola, Guillem Alenya, Gerard Pons-Moll, and Francesc\nMoreno-Noguer. 2021. Smplicit: Topology-aware generative model for clothed\npeople. In CVPR.\nEnric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zan-\nfir, and Cristian Sminchisescu. 2023. Structured 3D Features for Reconstructing\nRelightable and Animatable Avatars. In CVPR.\nJianglin Fu, Shikai Li, Yuming Jiang, Kwan-Yee Lin, Chen Qian, Chen Change Loy,\nWayne Wu, and Ziwei Liu. 2022. Stylegan-human: A data-centric odyssey of human\ngeneration. In ECCV.\nValentin Gabeur, Jean-S\u00e9bastien Franco, Xavier Martin, Cordelia Schmid, and Gregory\nRogez. 2019. Moulding humans: Non-parametric 3d human shape estimation from\nsingle images. In ICCV.\nRinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik,\nand Daniel Cohen-Or. 2022. An Image is Worth One Word: Personalizing Text-to-\nImage Generation using Textual Inversion. (2022). https://doi.org/10.48550/ARXIV.\n2208.01618\nXiangjun Gao, Jiaolong Yang, Jongyoo Kim, Sida Peng, Zicheng Liu, and Xin Tong.\n2022. MPS-NeRF: Generalizable 3D Human Rendering From Multiview Images.\nIEEE TPAMI (2022), 1\u201312.\nR\u0131za Alp G\u00fcler, Natalia Neverova, and Iasonas Kokkinos. 2018. Densepose: Dense\nhuman pose estimation in the wild. In CVPR.\nTong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. 2021. Arch++:\nAnimation-ready clothed human reconstruction revisited. In Proceedings of the\nIEEE/CVF international conference on computer vision. 11046\u201311056.\nFangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. 2023. EVA3D:\nCompositional 3D Human Generation from 2D Image Collections. In ICLR.\nShoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. 2023.\nSHERF: Generalizable Human NeRF from a Single Image. In ICCV.\nYangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang,\nBinbin Lin, Debing Zhang, and Deng Cai. 2022. One-shot Implicit Animatable\nAvatars with Model-based Priors. arXiv preprint arXiv:2212.02469 (2022).\nZeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. 2020. Arch:\nAnimatable reconstruction of clothed humans. In CVPR.\nAngjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. 2018. End-to-\nend recovery of human shape and pose. In CVPR.\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture\nfor generative adversarial networks. In CVPR.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo\nAila. 2020. Analyzing and improving the image quality of stylegan. In CVPR.\nNikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. 2019.\nLearning to reconstruct 3D human pose and shape via model-fitting in the loop. In\nICCV.\nYoungjoong Kwon, Dahun Kim, Duygu Ceylan, and Henry Fuchs. 2021. Neural human\nperformer: Learning generalizable radiance fields for human performance rendering.\nAdvances in Neural Information Processing Systems 34 (2021).\nChristoph Lassner, Javier Romero, Martin Kiefel, Federica Bogo, Michael J Black, and\nPeter V Gehler. 2017. Unite the people: Closing the loop between 3d and 2d human\nrepresentations. In CVPR.\nKathleen M Lewis, Srivatsan Varadharajan, and Ira Kemelmacher-Shlizerman. 2021.\nTryongan: Body-aware try-on via layered interpolation. ACM TOG 40, 4 (2021),\n1\u201310.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang,\nKarsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. 2023. Magic3D: High-\nResolution Text-to-3D Content Creation. In CVPR.\nLingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and\nChristian Theobalt. 2021a. Neural Actor: Neural Free-view Synthesis of Human\nActors with Pose Control. ACM TOG (2021).\nWen Liu, Zhixin Piao, Zhi Tu, Wenhan Luo, Lin Ma, and Shenghua Gao. 2021b. Liquid\nwarping GAN with attention: A unified framework for human image synthesis.\nIEEE TPAMI (2021).\nZiwei Liu, Ping Luo, Shi Qiu, Xiaogang Wang, and Xiaoou Tang. 2016. DeepFashion:\nPowering Robust Clothes Recognition and Retrieval with Rich Annotations. In\nCVPR.\nMatthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J\nBlack. 2015. SMPL: A skinned multi-person linear model. ACM TOG 34, 6 (2015),\n1\u201316.\nWilliam E Lorensen and Harvey E Cline. 1987. Marching cubes: A high resolution 3D\nsurface construction algorithm. ACM TOG 21, 4 (1987), 163\u2013169.\nLiqian Ma, Xu Jia, Qianru Sun, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool.\n2017. Pose guided person image generation. In NeurIPS.\nLiqian Ma, Qianru Sun, Stamatios Georgoulis, Luc Van Gool, Bernt Schiele, and Mario\nFritz. 2018. Disentangled person image generation. In CVPR.\nYifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma, and Zhouhui Lian. 2020. Con-\ntrollable person image synthesis with attribute-decomposed gan. In CVPR.\nGal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. 2023.\nLatent-NeRF for Shape-Guided Generation of 3D Shapes and Textures. In CVPR.\nMarko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, and Shunsuke Saito.\n2022. KeypointNeRF: Generalizing image-based volumetric avatars using relative\nspatial encoding of keypoints. In ECCV.\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ra-\nmamoorthi, and Ren Ng. 2020. NeRF: Representing Scenes as Neural Radiance\nFields for View Synthesis. In ECCV.\nRyota Natsume, Shunsuke Saito, Zeng Huang, Weikai Chen, Chongyang Ma, Hao Li,\nand Shigeo Morishima. 2019. Siclope: Silhouette-based clothed people. In CVPR.\nMichael Niemeyer and Andreas Geiger. 2021. Giraffe: Representing scenes as composi-\ntional generative neural feature fields. In CVPR.\nMichael Oechsle, Lars Mescheder, Michael Niemeyer, Thilo Strauss, and Andreas\nGeiger. 2019. Texture fields: Learning texture representations in function space. In\nICCV.\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. 2022. On Aliased Resizing and\nSurprising Subtleties in GAN Evaluation. In CVPR.\nGeorgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. 2018. Learning\nto estimate 3D human pose and shape from a single color image. In CVPR.\nSida Peng, Junting Dong, Qianqian Wang, Shangzhan Zhang, Qing Shuai, Xiaowei\nZhou, and Hujun Bao. 2021a. Animatable Neural Radiance Fields for Modeling\nDynamic Human Bodies. In ICCV.\nSida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao,\nand Xiaowei Zhou. 2021b. Neural Body: Implicit Neural Representations with\nStructured Latent Codes for Novel View Synthesis of Dynamic Humans. In CVPR.\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2022. DreamFusion:\nText-to-3D using 2D Diffusion. In ICLR.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li,\nHsin-Ying Lee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard\nGhanem. 2023. Magic123: One Image to High-Quality 3D Object Generation Using\nBoth 2D and 3D Diffusion Priors. arXiv preprint arXiv:2306.17843 (2023).\nYurui Ren, Xiaoming Yu, Junming Chen, Thomas H Li, and Ge Li. 2020. Deep image\nspatial transformation for person image generation. In CVPR.\nElad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. 2023.\nTEXTure: Text-Guided Texturing of 3D Shapes. ACM TOG (2023).\nYuval Atzmon Amit H. Bermano Gal Chechik Daniel Cohen-Or Rinon Gal, Moab Arar.\n2023. Encoder-based Domain Tuning for Fast Personalization of Text-to-Image\nModels. (2023). https://arxiv.org/abs/2302.12228\nSingle-Image 3D Human Digitization with Shape-Guided Diffusion\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nDaniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. 2022. Pivotal\ntuning for latent-based editing of real images. ACM TOG 42, 1 (2022), 1\u201313.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-Resolution Image Synthesis with Latent Diffusion Models. In CVPR.\nXuejian Rong, Jia-Bin Huang, Ayush Saraf, Changil Kim, and Johannes Kopf. 2022.\nBoosting View Synthesis with Residual Transfer. In CVPR.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2022. DreamBooth: Fine Tuning Text-to-image Diffusion Models for\nSubject-Driven Generation. (2022).\nShunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa,\nand Hao Li. 2019. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed\nHuman Digitization. In ICCV.\nShunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. 2020. PIFuHD: Multi-\nLevel Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization.\nIn CVPR.\nShunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J Black. 2021. SCANimate:\nWeakly supervised learning of skinned clothed avatar networks. In CVPR.\nKripasindhu Sarkar, Vladislav Golyanik, Lingjie Liu, and Christian Theobalt. 2021.\nStyle and Pose Control for Image Synthesis of Humans from a Single Monocular\nView. arXiv preprint arXiv:2102.11263 (2021).\nAliaksandr Siarohin, Enver Sangineto, St\u00e9phane Lathuiliere, and Nicu Sebe. 2018.\nDeformable gans for pose-based human image generation. In CVPR.\nDavid Smith, Matthew Loper, Xiaochen Hu, Paris Mavroidis, and Javier Romero. 2019.\nFacsimile: Fast and accurate scans from an image in less than a second. In ICCV.\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong\nChen. 2023. Make-It-3D: High-Fidelity 3D Creation from A Single Image with\nDiffusion Prior. arXiv preprint arXiv:2303.14184 (2023).\nGul Varol, Duygu Ceylan, Bryan Russell, Jimei Yang, Ersin Yumer, Ivan Laptev, and\nCordelia Schmid. 2018. Bodynet: Volumetric inference of 3d human body shapes.\nIn ECCV.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A. Yeh, and Greg Shakhnarovich.\n2022. Score Jacobian Chaining: Lifting Pretrained 2D Diffusion Models for 3D\nGeneration. arXiv preprint arXiv:2212.00774 (2022).\nLizhen Wang, Xiaochen Zhao, Tao Yu, Songtao Wang, and Yebin Liu. 2020. NormalGAN:\nLearning Detailed 3D Human from a Single RGB-D Image. In ECCV.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun\nZhu. 2023. ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with\nVariational Score Distillation. arXiv preprint arXiv:2305.16213 (2023).\nChung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira\nKemelmacher-Shlizerman. 2022. HumanNeRF: Free-Viewpoint Rendering of Mov-\ning People From Monocular Video. In CVPR.\nJianfeng Xiang, Jiaolong Yang, Binbin Huang, and Xin Tong. 2023. 3D-aware Image\nGeneration using 2D Diffusion Models. arXiv preprint arXiv:2303.17905 (2023).\nYiheng Xie, Towaki Takikawa, Shunsuke Saito, Or Litany, Shiqin Yan, Numair Khan,\nFederico Tombari, James Tompkin, Vincent Sitzmann, and Srinath Sridhar. 2022.\nNeural fields in visual computing and beyond. In Computer Graphics Forum, Vol. 41.\nWiley Online Library, 641\u2013676.\nYuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. 2023.\nECON: Explicit Clothed humans Optimized via Normal integration. In CVPR.\nYuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J. Black. 2022. ICON: Implicit\nClothed humans Obtained from Normals. In CVPR.\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang.\n2022. NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with 360\u00b0\nViews. arXiv preprint arXiv:2211.16431.\nJae Shin Yoon, Lingjie Liu, Vladislav Golyanik, Kripasindhu Sarkar, Hyun Soo Park,\nand Christian Theobalt. 2021. Pose-Guided Human Animation from a Single Image\nin the Wild. In CVPR.\nJonathan Young. 2021. xatlas: Mesh parameterization / UV unwrapping library. https:\n//github.com/jpcy/xatlas.\nTao Yu, Zerong Zheng, Kaiwen Guo, Pengpeng Liu, Qionghai Dai, and Yebin Liu. 2021.\nFunction4D: Real-time Human Volumetric Capture from Very Sparse Consumer\nRGBD Sensors. In CVPR.\nJianfeng Zhang, Zihang Jiang, Dingdong Yang, Hongyi Xu, Yichun Shi, Guoxian Song,\nZhongcong Xu, Xinchao Wang, and Jiashi Feng. 2022. AvatarGen: A 3D Generative\nModel for Animatable Human Avatars. Arxiv (2022).\nLvmin Zhang and Maneesh Agrawala. 2023. Adding Conditional Control to Text-to-\nImage Diffusion Models. arXiv:2302.05543 [cs.CV]\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018.\nThe Unreasonable Effectiveness of Deep Features as a Perceptual Metric. In CVPR.\nZerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. 2021. PaMIR: Parametric Model-\nConditioned Implicit Representation for Image-based Human Reconstruction. IEEE\nTPAMI (2021).\nZhen Zhu, Tengteng Huang, Baoguang Shi, Miao Yu, Bofei Wang, and Xiang Bai. 2019.\nProgressive Pose Attention Transfer for Person Image Generation. In CVPR.\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nBadour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang\nInput\nPwS baseline [2021]\nPIFu [2019]\nImpersonator++ [2021b]\nTEXTure [2023]\nMagic123 [2023]\nOurs\nFigure 8: Visual comparison on in-the-wild images from Adobe Stock. We compare our 3D human digitization approach\nwith prior methods [Corona et al. 2023; Liu et al. 2021b; Qian et al. 2023; Richardson et al. 2023; Saito et al. 2019] on images\nin-the-wild to showcase the generalizability of our approach. Our approach demonstrates high-resolution photorealistic results\nthat preserve the appearance of the input image.\nSingle-Image 3D Human Digitization with Shape-Guided Diffusion\nSA Conference Papers \u201923, December 12\u201315, 2023, Sydney, NSW, Australia\nInput\nPwS baseline\nPIFu\nImpersonator++\nTEXTure\nMagic123\nS3F\nOurs\nFigure 9: Visual comparisons on the THuman2.0 dataset. We compare our approach with prior methods [AlBahar et al. 2021;\nCorona et al. 2023; Liu et al. 2021b; Qian et al. 2023; Richardson et al. 2023; Saito et al. 2019] on the THuman2.0 dataset [Yu et al.\n2021]. Our results showcase photorealistic images with consistent views that are consistent with the input images.\n"
  },
  {
    "title": "Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models",
    "link": "https://arxiv.org/pdf/2311.08692.pdf",
    "upvote": "11",
    "text": "Routing to the Expert: Efficient Reward-guided Ensemble of Large\nLanguage Models\nKeming Lu, Hongyi Yuan\u2217, Runji Lin\u2217\nJunyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou\nAlibaba Inc.\n{lukeming.lkm,yuanhongyi.yhy,linrunji.lrj}@alibaba-inc.com\n{junyang.ljy,yuanzheng.yuanzhen}@alibaba-inc.com\n{ericzhou.zc,jingren.zhou}@alibaba-inc.com\nAbstract\nThe complementary potential of Large Lan-\nguage Models (LLM) assumes off-the-shelf\nLLMs have heterogeneous expertise in a wide\nrange of domains and tasks so that an ensem-\nble of LLMs can achieve consistently better\nperformance. Existing ensemble methods for\nLLMs mainly focus on reward model ranking\nof outputs, leading to significant computation\noverhead. To combat this issue, we revisit the\ncomplementary potential of LLMs and further\nelaborate it by mining latent expertise with off-\nthe-shelf reward models. We propose ZOOTER,\na reward-guided routing method distilling re-\nwards on training queries to train a routing func-\ntion, which can precisely distribute each query\nto the LLM with expertise about it. We also\nintegrate a tag-based label enhancement to miti-\ngate noise from uncertainty when using rewards\nas silver supervision. ZOOTER shows compu-\ntation efficiency in inference as it only intro-\nduces minor computation overhead of a routing\nfunction compared with reward model ranking\nmethods. We evaluate ZOOTER on a compre-\nhensive benchmark collection with 26 subsets\non different domains and tasks. ZOOTER out-\nperforms the best single model on average and\nranks first on 44% of tasks, even surpassing\nmultiple reward model ranking methods. 1\n1\nIntroduction\nLarge Language Models (LLMs) aligned with hu-\nman preference rapidly emerge and are almost daily\nreleased (Touvron et al., 2023a,b; Anil et al., 2023;\nBai et al., 2023). These off-the-shelf LLMs are\nfurther finetuned or aligned with human preference\nto be generalists (Xu et al., 2023; Touvron et al.,\n2023b,a) or specialists (Yuan et al., 2023a; Luo\net al., 2023a,b; Roziere et al., 2023) for solving\nversatile tasks. It is worth noticing that LLMs are\npretrained and aligned with various data, leading\n\u2217Work done during internship at Alibaba Inc.\n1Work in progress.\nShare ideas for adapting art masterpieces into interactive experiences \nfor children. List 5 specific artworks and associated ideas.\nCertainly! As a helpful and respectful assistant, I'm happy to \nprovide ideas for adapting art masterpieces ... 1. \"The Starry \nNight\" by Vincent van Gogh:\\n\\n Interactive Experience Idea: \nCreate a digital or ...\n1. The Mona Lisa by Leonardo da Vinci: Create a virtual \nreality experience where children can explore the painting \nand interact with it in different ways, such as \u2026\u2026 \\n\\n2. The \nScream by Edvard Munch: Create a \u2026\u2026\n1. The Scream by Edvard Munch: This painting is a perfect \nopportunity to discuss emotions with children. \u2026\u2026 Then, you \ncan guide them through a process \u2026\u2026\n1. \"The Starry Night\" by Vincent van Gogh:\\n Interactive \nExperience: Create a virtual reality (VR) experience \u2026\u2026. They \ncan learn about the constellations and stars, and even create \ntheir own \u2026\u2026\n1. \"The Starry Night\" by Vincent van Gogh - Create a virtual \nreality experience \u2026\u2026. They can learn about the different \nconstellations and the techniques van Gogh used to create \nthe swirling effects \u2026\u2026\nCreating interactive experiences based on art masterpieces \ncan be a great way to introduce children to the world of art \nand spark their creativity. ... 1. \"The Starry Night\" by Vincent \nvan Gogh\\nIdea: Create a virtual reality (VR) experience \u2026\nWizardCoder\nLLaMa-2\nWizardMath\nWizardLM\nVicuna\nOpenChat\nReward Oracle\nZooter\n0.10\n10\n2.19\n0.02\n10\n-0.03\n0.00\n7\n-0.36\n0.12\n10\n2.63\n0.13\n9\n1.77\n0.63\n10\n4.22\nFigure 1: An example of the large language model en-\nsemble. Reward model ranking marked in blue needs\nto generate responses from all models while ZOOTER\nrouters the given query to the best model and only infers\none model. This case is collected from the MT-Bench\nbenchmark and we also present oracle judgements of\neach response.\nto diverse strengths and weaknesses in versatile\ndownstream tasks (Jiang et al., 2023). Therefore,\nthe ensemble of LLMs harnesses the complemen-\ntary potential among them and may achieve better\nperformance than a single best-on-average model\nacross diverse tasks.\nOne of the key challenges in the LLM ensemble\nis computation efficiency due to the large parame-\nter size of existing LLMs. Previous research (Jiang\net al., 2023; Shnitzer et al., 2023) provides solid\nmethods to merge generation outputs of LLMs as\nan ensemble. Such methods require tremendous\ninference cost that makes it unscalable and thus\nnot competitive to the best-on-average model un-\nder low-resource scenarios. To efficiently assemble\noff-the-shelf LLMs, we first dive deeper into the\nconsiderably straightforward but still understudied\narXiv:2311.08692v1  [cs.CL]  15 Nov 2023\nassumption: Off-the-shelf aligned LLMs, even for\nthose aligned as \u201cgeneralists\u201d, have heterogeneous\nexpertise in a wide range of domains and topics.\nHowever, analyzing the expertise of an LLM is also\nchallenged as the latent expertise of LLMs is highly\nrelated to the pretrained and alignment data, which\nis very vague and inaccessible even for popular\nopen-source LLMs such as LLAMA-2-CHAT (Tou-\nvron et al., 2023b) and WIZARDLM (Xu et al.,\n2023).\nIf this assumption strongly holds, off-the-shelf\nLLMs can be assembled efficiently by assigning\nqueries to the model that is proficient in the query\nwithout additional inference costs on each model.\nSuch an efficient routing strategy only requires in-\nference cost for a single model for each query and\nthe overhead cost of a much smaller query router.\nHowever, probing the detailed expertise of off-the-\nshelf LLMs and generating supervision for train-\ning routers also require annotations. Developing a\ndata-efficient training method for routing queries\nis significantly understudied.\nTo combat these issues, we propose ZOOTER, a\nreward-guided query routing method for efficiently\nassembling off-the-shelf LLMs. ZOOTER obtains\nand enhances silver supervision from existing re-\nward models (RM) for query router training and\ndistributes queries in advance to \u201cexpertise\u201d. As\nshown in Fig. 1, the reward distribution implies\nthe oracle judgments and reveals a latent exper-\ntise between LLMs. And ZOOTER captures the\nexpertise from reward distributions and provides\nquery distribution during inference. Specifically,\nwe first conduct a comprehensive study involving\nfour groups of benchmarks across more than 26\nsubsets in various domains and tasks. We investi-\ngate six widely used open-source LLMs and show\nthe complementary potential of such wide-range\ndownstream tasks by aggregating them via reward\nmodel ranking. We then collect a diverse training\nquery set and distill rewards of model expertise as\nindirect supervision for training an LLM router and\ndevelop tag-based label enhancement to overcome\nthe shortage of such silver labels from reward mod-\nels further. With comprehensive experiments, we\nshow ZOOTER can benefit from RM silver supervi-\nsion to learn the latent expertise among LLMs and\nconduct efficient routing for the model ensemble.\nOur contributions are mainly three-fold:\n\u2022 We revisit the complementary potential of open-\nsource LLMs, which proves the effectiveness of\nLLM ensemble, and show rewards from off-the-\nshelf RMs can be silver supervision for model\nexpertise.\n\u2022 We propose ZOOTER, an efficient reward-guided\nrouting method distilling rewards from off-the-\nshelf reward model for probing model expertise.\nThen, we develop a tag-based label enhancement\nto mitigate noise from the uncertainty of reward\nmodels.\n\u2022 We comprehensively evaluate ensemble meth-\nods, including reward model ranking and\nZOOTER on four groups of benchmarks with\n26 subsets on different tasks and domains. Our\nevaluation shows ZOOTER can effectively assem-\nble LLMs and even outperforms reward model\nranking methods with significantly fewer com-\nputation overhead.\n2\nRelated Works\nInstruction Tuning and Alignment. Instruction\ntuning (Longpre et al., 2023) helps LLMs to fol-\nlow versatile instructions, which is widely adopted\nto align LLMs with human preference (Chiang\net al., 2023; Xu et al., 2023; Bai et al., 2023). In\nthis work, we focus on assembling aligned LLMs,\nsuch as Llama-2-Chat (Touvron et al., 2023b), Wiz-\nardLM (Xu et al., 2023), Vicuna (Chiang et al.,\n2023), and so on. And we evaluate them on a wide\nrange of alignment evaluation tasks.\nLarge Language Model Ensemble. The ensem-\nble of LLMs is an emerging topic due to the ex-\nplosion of open-source LLMs. LLM ensemble\naims to merge off-the-shelf LLMs to achieve con-\nsistently better performance across diverse down-\nstream tasks. Few works explore the complemen-\ntary potential assumption of LLMs and how to as-\nsemble LLMs with it. Jiang et al. (2023) presents\nan ensembling framework consisting of a pair\nranker and a generation fuser. Chen et al. (2023)\nsequentially infers off-the-shelf LLMs and stops\nuntil the response meets a sufficient quality. Wang\net al. (2023b) proposes a fusing-of-experts problem\nthat fuses outputs of expert models with comple-\nmentary knowledge of the data distribution and\nformulates it as supervised learning. Shnitzer et al.\n(2023) show the utility and limitations of learning\nmodel routers from various benchmark datasets.\nAlthough these works all focus on reward ranking\nor routing strategies to assemble LLMs, ZOOTER\ndistinguishes from these concurrent works in two\naspects. First, our concurrent works require output\ngenerations or the forward process to get prompt\nrepresentations of all candidates, leading to sig-\nnificant computation overhead. ZOOTER infers\nmodel expertise by distilling rewards on a prede-\nfined training query set to avoid such inference\noverhead. Then, all these works are developed and\nevaluated on a set of benchmarks, while ZOOTER\ncan be developed with only queries without golden\nresponses, and ZOOTER aims for more diverse\nalignment tasks. Therefore, ZOOTER stands out\nfor its efficiency in data and computation. We also\nevaluate ZOOTER on more diverse alignment tasks\nto comprehensively examine the complementary\npotential of LLMs.\nReward Model Guided Generation.\nReward\nmodels in the context of large language models\nare commonly used to improve alignment perfor-\nmance by reinforcement learning (Schulman et al.,\n2017; Ouyang et al., 2022) or preference learning\n(Yuan et al., 2023b; Rafailov et al., 2023; Song\net al., 2023). Reward models can also improve\nthe performance during the generation phase. The\nmath reasoning ability of language models can be\nimproved by using reward models ranking multiple\ngenerated reasoning paths (Cobbe et al., 2021; Ue-\nsato et al., 2022; Lightman et al., 2023). Liu et al.\n(2023) uses reward models to formulate reward-\nguided decoding. Inspired by these successful ap-\nplications of reward models in alignment, ZOOTER\nalso takes advantage of off-the-shelf reward models\nto investigate the latent expertise of LLMs.\n3\nMethods\nWe first revisit the complementary potential of\nLLMs (\u00a73.1) and then introduce ZOOTER as an\nefficient LLM ensemble method (\u00a73.2).\n3.1\nComplementary Potential of LLMs\nIn this section, we present the preliminaries about\nthe assumption: Off-the-shelf aligned LLMs have\nheterogeneous expertise in a wide range of domains\nand topics. We also briefly introduce two LLM\nensemble strategies, reward model ranking, and\nquery routing.\nComplementary Potential Assumption. Consid-\nering a set of LLMs denoted as M = {mi|i \u2208\nZ+} and a set of downstream queries denoted\nas Q = {qi|i \u2208 Z+}, we assume that for each\nLLM mi in M, there exists a non-empty query\nsubset Qmi such that the LLM can achieve uni-\nformly better performance than other LLMs in\nM for any query qj \u2208 Qmi, which is mi =\nargmaxm\u2208M P (qj, m(qj)). P can be any prefer-\nence or metric for performance assessment. In this\nwork, we further enhance this assumption and aim\nto show that the complementary between LLMs re-\nveals their expertise in different domains and tasks,\nso that we can categorize queries and choose the\nbest LLM for each category.\nReward Model Ranking. Reward model rank-\ning (RMR) leverages the complementary poten-\ntial to ensemble LLMs and achieve surpass per-\nformance. RMR tries to find a reward function\n\u02c6P to estimate the oracle preference P so that we\ncan obtain the best model for each query (Jiang\net al., 2023). However, RMR infers all candidate\nmodels to get outputs and then rank them with a\nreward function, introducing a large computation\noverhead.\nQuery Routing.\nQuery routing mitigates effi-\nciency concerns in the LLM ensemble, especially\ncompared with existing RMR methods. In gen-\neral, query routing tries to find a routing function\nZ(q, mi) with respect to qj \u2208 Q exists, so that\nmi = argmaxm\u2208M Z (qj, m). The routing func-\ntion distributes queries based on themselves with-\nout generating outputs. If the complementary po-\ntential of LLMs holds, the routing function predicts\nthe probability that a query q belongs to the exper-\ntise of an LLM Qm.\n3.2\nZooter\nIn this section, we propose ZOOTER, a reward-\nguided query routing method for efficiently assem-\nbling large language models. ZOOTER learns from\nthe reward model ranking to interpret the latent\nexpertise of each model. So, as shown in Fig. 2,\nZOOTER first infers all candidate LLMs on a train-\ning set containing diverse queries to generate re-\nsponses. Then, all responses will be rewarded by\nan off-the-shelf reward model providing scalar re-\nwards, marked in blue dash lines in Fig. 2. The\nrewards are first enhanced by a tag-based prior\nfor smoothing and denoising. The normalized re-\nward distribution is then used as supervision in the\nknowledge distillation training of the routing func-\ntion, shown in the green dash lines in Fig. 2. During\ninference, the routing function categorizes the in-\nput query to an LLM with the strongest expertise\npotential in this query, and the LLM will gener-\nate an expert response. By training such a routing\nResponse 1\nResponse 2\nResponse N\n-1.73\n0.33\n7.28\nLLM 2\nLLM 1\nLLM N\n.\n.\n.\nOff-the-shelf\nQuery\n.\n.\n.\n.\n.\n.\nResponses\nRewards\nQuery Routing\nReward Model Ranking\nZooter\nReward \nModel\nTag-based Label \nEnhancement\nReward \nDistribution\nKnowledge \nDistillation\nTraining\nZooter Training\nReward-guided Query Routing (Light)\nReward Model Ranking (Heavy)\nDistribute\nFigure 2: Overview of ZOOTER. ZOOTER aims to assemble a set of off-the-shelf LLMs by first conducting a reward\nmodel ranking on a diverse training set to obtain supervision of model expertise, highlighted in blue in the figure.\nInstruction tags are then used to mitigate the uncertainty in reward estimation. ZOOTER uses the normalized rewards\nas supervision to train a routing function by knowledge distillation. The training circle is marked in green, and the\ninference is marked in orange. ZOOTER is much lighter in computation as it routes the query to the corresponding\nexpert LLM during inference time, while reward model ranking has to generate outputs for all candidates.\nfunction, ZOOTER achieves a much more efficient\nensemble as it only needs to infer one expert LLM,\nplus a small computation overhead of the routing\nfunction. In this section, we introduce the two key\ncomponents along with the design motivations.\nReward Distillation. As we discussed in \u00a73.1,\nquery routing aims to find a routing function pre-\ndicting the probability that a query q belongs to\nthe expertise of an LLM Qm, where Qm is a set of\nqueries that an LLM m consistently achieves maxi-\nmum preference among all candidates. Recalling\nthe reward model ranking, we notice the estimated\npreferences \u02c6P(q, mi(q)), i.e., reward, can be inter-\npreted as the relative advantages of an LLM mi\namong all candidates on the query q. Therefore,\nthe normalized reward can be used as a silver su-\npervision for the routing function:\nZ(q)i = P(q \u2208 Qmi)\n:=\nexp( \u02c6P(q, mi(q)))\nP\nmi\u2208M exp( \u02c6P(q, mi(q)))\n,\nas the higher advantages inherently present the ex-\npertise of an LLM on a query compared with its\ncounterparts.\nTo estimate the expertise of each model and\ntrain the routing function, we need to apply the\nreward preference ranking on a diverse training\nset \u02c6Q. We first infer all candidate models on each\nquery \u02c6q \u2208 \u02c6Q, and then assign rewards by an off-\nthe-shelf reward model to obtain a scalar reward\nfor each query and model\nri = { \u02c6P(\u02c6qi, mj(\u02c6qi))}|M|\nj=1 , i = 1, . . . , | \u02c6Q|.\n. Then, we train the router function Z on the train-\ning set by knowledge distillation with a Kullback-\nLeibler divergence as the loss function:\nL(qi, ri) = KL(Z(qi), softmax(ri)).\nZOOTER is a data-efficient and low-resource\nmethod as the training set \u02c6Q only contains queries\nwithout annotations of responses. However, queries\nin the training set are expected to be as diverse as\npossible to maximize the generalization abilities of\nthe routing function. The distillation process helps\nZOOTER to learn the latent expertise of each model.\nSo, we can mitigate the computation cost by only\njudging whether a query belongs to the expertise\nset with our routing function during inference.\nTag-based Label Enhancement. Although reward\ndistillation provides a feasible way for routing func-\ntions to leverage silver supervision from reward\nmodel ranking, the language reward model pro-\nvides rewards with uncertainty, introducing certain\nnoises (Gleave and Irving, 2022). We first empir-\nically analyze this uncertainty in \u00a74.3. Existing\noff-the-shelf reward models will all involve noises\nin terms of uncertainty, as shown in Fig. 3. There-\nfore, we leverage instruction tagging to enhance\nrewards on the training queries further. The tag-\nbased label enhancement we proposed is similar\nto the widely used label smoothing techniques and\nproven effective in knowledge distillation (Yuan\net al., 2020). Specifically, we first tag each query\n\u02c6qi \u2208 \u02c6Q with a local tagger T (\u00b7) to obtain a set\nof tags T (qi). Then, we aggregate all rewards on\nqueries with the same tags for the tag-wise rewards\nas follows:\nQt = {qi|t \u2208 T (qi), i = 1, . . . , | \u02c6Q|}\nrt =\n1\n|Qt|\nX\ni\u2208Qt\nri\nThen, we enhance rewards for each query with tag-\nwise rewards by a linear combination:\nr\u2217\ni = \u03b2ri + (1 \u2212 \u03b2)rt ; t = T (qi), i = 1, . . . , | \u02c6Q|\n,where \u03b2 is a hyper-parameter for the trade-off be-\ntween coarse-grained tag-wise rewards and fine-\ngrained sample-level rewards. Then, we replace\noriginal rewards in the KL divergence loss training\nwith tag-based enhanced rewards r\u2217 during routing\nfunction training.\n4\nExperiments\nIn\nthis\nsection,\nwe\nreport\nexperimental\nsetup (\u00a74.1), main results (\u00a74.2), and analy-\nsis about ZOOTER (\u00a74.3).\n4.1\nExperimental Setup\nCandidate LLMs. We select six LLAMA-based\nLLMs of the same 13B size as the candidate LLMs\nfor query routing. (a) WizardLM (Xu et al., 2023)\nis aligned with queries and responses augmented\nby EVOLINSTRUCT, (b) WizardCoder (Luo et al.,\n2023b) is a coding expert LLM using the same\ntechniques as WizardLM, (c) WizardMath (Luo\net al., 2023a) is a math expert LLM aligned\nwith query augmentation, ChatGPT rewards and\nPPO optimization, (d) Vicuna (Chiang et al.,\n2023) is aligned on tremendous conversations be-\ntween users and proprietary chatbots, (e) Open-\nChat (Wang et al., 2023a) is aligned with a selected\nset of ShareGPT with additional training strate-\ngies, (f) Llama-2-Chat (Touvron et al., 2023b)\nis first aligned by supervised fine-tuning and then\nmulti-turn rejection sampling. Both baselines and\nZOOTER are experimented and evaluated based on\nthese six candidates.\nTraining Datasets. We create a diverse mix in-\nstruction dataset from the open-source data to max-\nimize the generalization abilities of ZOOTER. We\nfirst collect and tag open-source data from 13\ndatasets with a local tagger developed by Lu et al.\n(2023). For trustworthy evaluation results, we de-\ncontaminate all samples containing queries that\nhave a 6-gram overlap with any samples in our\nbenchmarks described below to avoid data leakage.\nThen, we randomly select ten samples for each\nunique tag to form a diverse mix instruction dataset\nDIVINSTRUCT with 47,986 instructions and sam-\nples across 6,270 different tags. Detailed statistics\nof DIVINSTRUCT is in Appx. \u00a7A.\nBenchmarks. We actively involve four sets of\nbenchmarks to evaluate ZOOTER on various down-\nstream tasks comprehensively. We first include\nthree widely-used alignment benchmarks with\nGPT-4 judge:\n\u2022 AlpcaEval (Li et al., 2023b) consists of 5 subsets\nfrom the koala, vicuna, and others evaluation\nsets. It contains 805 samples in total.\n\u2022 FLASK (Ye et al., 2023) is a fine-grained evalu-\nation for alignment. We evaluate 10 domains in\nFLASK and report the average score across all\ndomains as a final score.\n\u2022 MT-Bench (Chiang et al., 2023) is a multi-turn\nevaluation across eight aspects, including mathe-\nmatics and coding. We only train and route with\nthe first-turn query but evaluate in the multi-turn\nmanner as the original recipe.\nHowever, as reported by Wang et al. (2023c),\nGPT-4 judgments may have bias and significant\ndisagreement with humans. Therefore, we also\ninclude a group of benchmarks consisting of\nMMLU (Hendrycks et al., 2021), GSM8K (Cobbe\net al., 2021), and HumanEval (Chen et al., 2021).\nMetrics. Comparing ensemble models on various\nbenchmarks is challenging as the scale of scores\nModel\n#Param\nAlpacaEval (5) FLASK (10) MT-Bench (8) Benchmarks (3)\nAll (26)\nRanker\nInfer\nAvg.\nMTR\nAvg. MTR Avg.\nMTR\nAvg.\nMTR\nMTR % Uplift\nRouting Candidates\nWIZARDCODER\n\u2212\u2212\n13B\n0.42\n5.6\n3.12\n5.2\n4.44\n5.38\n30.9\n4.33\n5.3\n0.06\nWIZARDLM\n\u2212\u2212\n13B\n0.89\n2.0\n3.89\n1.8\n7.15\n2.0\n44.2\n2.0\n1.83\n0.25\nWIZARDMATH\n\u2212\u2212\n13B\n0.47\n5.0\n3.28\n5.0\n5.73\n4.38\n34.8\n4.0\n4.6\n0.03\nLLAMA-2-CHAT\n\u2212\u2212\n13B\n0.91\n1.6\n3.88\n1.5\n6.72\n2.88\n32.3\n3.67\n2.23\n0.31\nOPENCHAT\n\u2212\u2212\n13B\n0.89\n2.2\n3.79\n3.1\n7.12\n2.0\n31.2\n3.33\n2.67\n0.19\nVICUNA\n\u2212\u2212\n13B\n0.8\n3.8\n3.7\n3.5\n6.58\n3.25\n33.6\n2.67\n3.4\n0.06\nBMA\n\u2212\u2212\n13B\n0.91\n1.6\n3.88\n1.5\n6.72\n2.88\n32.3\n3.67\n2.23\n0.31\nZOOTER\nOurs\n86M\n13B\n0.93\n1.17\n3.89\n1.82\n7.11\n2.33\n34.2\n3.0\n1.94\n0.44\nReward Model Ranking (RMR)\nW/ OASSISTRM\n300M 6\u00d713B 0.79\n4.0\n3.75\n3.73\n6.59\n3.22\n35.1\n3.25\n3.42\n0.19\nW/ LLM-BLENDER 300M 6\u00d713B 0.83\n3.67\n3.77\n3.36\n6.21\n4.0\n36.4\n2.75\n3.39\n0.17\nW/ AUTO-J\n13B\n6\u00d713B 0.89\n2.67\n3.92\n1.64\n7.03\n2.22\n32.2\n3.5\n2.25\n0.42\nW/ ULTRARM\n13B\n6\u00d713B 0.92\n1.17\n4.06\n1.0\n7.18\n1.89\n40.1\n3.25\n1.53\n0.72\nW/ QWENRM\n7B\n6\u00d713B 0.92\n1.33\n4.04\n1.0\n7.26\n2.11\n38.6\n3.0\n1.58\n0.67\nW/ ORACLE\n\u2212\u2212\n6\u00d713B 0.98\n1.0\n4.56\n1.0\n8.25\n1.0\n75.3\n1.0\n1.0\n1.0\nProprietary Models\nGPT-3.5-turbo\n\u2212\u2212\n\u2212\u2212\n0.89\n2.67\n4.06\n1.91\n7.94\n1.78\n73.0\n1.0\n1.78\n0.61\nGPT-4\n\u2212\u2212\n\u2212\u2212\n0.94\n1.0\n4.37\n1.0\n8.99\n1.0\n88.3\n1.0\n1.0\n1.0\nTable 1: Main results of both ZOOTER and reward model ranking. We report performance across four groups of\nbenchmarks and report the number of subsets beside the name of benchmarks. We also report the parameters of\nranker and total inference models for both candidates and ensemble methods. MTR denotes the mean task rate, and\n%Uplift denotes the rate of uplift. The average scores and uplift rate are as higher as better while MTR is as lower\nas better. We mark better scores in darker blue for better visualization and easier interpretation.\nis different on each benchmark. To combat this\nissue, we do not only report the scores on each\nbenchmark but also the mean task rank (MTR). All\nbenchmarks we evaluate have multiple subsets, we\ndefine MTR as the rank of the evaluated model\namong all baselines average on all subsets. MTR\nis only about the rank among baselines so it can\nbe easily adopted across benchmarks that have dif-\nferent score scales. Similarly, we also propose an\nuplift rate, denoting the rate of subsets that the\nevaluated model achieves the best performance of\nbenchmarks. We report these two metrics on a total\nof 26 evaluation subsets in all benchmarks. Lower\nMTR and higher uplift rates show the evaluated\nmodel has consistently higher performance among\nversatile downstream tasks.\nBaselines. We also compare ZOOTER with existing\nreward model ranking (RMR) methods. We set up\nRMR baselines with the latest rewards models, in-\ncluding OASSISTRM, AUTO-J (Li et al., 2023a),\nULTRARM (Cui et al., 2023), QWENRM (Bai\net al., 2023), and an Oracle ranking for refer-\nence. We also consider the pair ranking in LLM-\nBlender (Jiang et al., 2023) as one of the RMR\nmethods. Besides, we also report the performance\nof proprietary models across our benchmark collec-\ntions for reference, including GPT-3.5-turbo and\nGPT-4.\nConfigurations. We train our routing function\nfrom mdeberta-v3-base. And we use QwenRM\nto generate rewards on training queries as supervi-\nsion for our routing function, as it achieves the best\nperformance in reward model ranking with consid-\nerably smaller model parameters described in \u00a74.2.\nAnd we run all training and inference on 8 A100\nGPUs. We infer and evaluate all benchmarks with\ncorresponding configurations and GPT-4 settings.\nWe use greedy decoding for MMLU, GSM8K, and\nHumanEval.\n4.2\nResults\nWe present the main results in Tab. 1. We report the\nperformance of six routing candidates across our\nbenchmarks, and the best model on average (BMA)\nis LLAMA-2-CHAT. And we report ZOOTER with\n\u03b2 = 0.3 in tag-based label enhancement. We fur-\nther analyze the results in the following two as-\npects:\nComplementary Potential. We evaluate the en-\nsemble with reward model ranking (RMR) on five\ndifferent off-the-shelf reward models. RMR with\nUltraRM achieves the best performance in MTR\nand uplift rate on the aggregation of all bench-\nmarks, which ranks at 1.53 and achieves the best\nmodel across 72% subtasks. RMR with QwenRM\nachieves the second best and has similar perfor-\nmance with UltraRM with smaller parameter sizes,\nfollowed by RMR with Auto-J, LLM-Blender, and\nOAssistRM. RMR with QwenRM, UltraRM, and\nAuto-J outperform that of BMA, showing the effec-\ntiveness of RMR. Furthermore, we also calculate\nthe score of RMR with an Oracle ranker, which\nconsistently outperforms all candidates and even\noutperforms GPT-4 on AlpacaEval and FLASK.\nSuch results provide solid evidence for the com-\nplementary potential of off-the-shelf LLMs and\nalso support the key motivation behind ZOOTER,\ni.e., using rewards from off-the-shelf reward mod-\nels as silver supervision for the routing function\ntraining. However, we notice RMR fails on bench-\nmarks, such as MMLU, GSM8K, and HumanEval,\nshowing that precisely judging knowledge, mathe-\nmatics, and coding problems are still challenging\nfor existing RMs.\nZooter Performance. We then compare the per-\nformance of ZOOTER with that of BMA and RMR.\nZOOTER outperforms BMA on AlpacaEval, MT-\nBench, and Benchmarks, and achieves similar per-\nformance on FLASK. The most significant im-\nprovement is witnessed on MT-Bench, where the\nperformance of ZOOTER is higher than that of\nBMA by 0.39. In general, ZOOTER achieves top-\n1 on 44% subtasks while BMA is only on 31%.\nWith the evidence above, ZOOTER successfully uti-\nlizes the complementary potential between LLMs\nto achieve the best performance more consistently\nover our benchmarks, with computation overhead\nfrom only 86M ranker. At the same time, ZOOTER\noutperforms RMR with OAssistRM, LLM-Blender,\nand Auto-J, by significantly less computation over-\nhead.\nHowever, though ZOOTER outperforms\nRMR with QwenRM on AlpacaEval, there are still\nobvious gaps between ZOOTER and RMR with\nQwenRM in general.\n4.3\nAnalysis\nWe provide further analysis on how RM uncertainty\nmay influence the training of ZOOTER.\nRM Uncertainty. As presented in the previous\n2\n4\n6\n8\n10\nMT-Bench Score\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nReward Entropy\nFigure 3: Analysis between reward entropy and scores\nof reward preference ranking on MT-bench.\n\u03b2 AlpacaEval FLASK MT-Bench Benchmarks All\n0\n1.4\n2.2\n2.25\n3.67\n2.06\n0.1\n1.2\n2.1\n2.38\n3.67\n2.00\n0.3\n1.2\n1.9\n2.50\n3.67\n1.97\n0.5\n1.2\n2.2\n3.12\n3.67\n2.23\n0.7\n1.2\n2.2\n3.38\n4.00\n2.31\n0.9\n1.2\n2.3\n3.12\n4.00\n2.31\n1.0\n1.2\n2.3\n3.25\n4.00\n2.34\nTable 2: Mean task rank (MTR) of different \u03b2 in tag-\nbased label enhancement across all benchmarks. The\nbest value of \u03b2 is marked in blue.\nresearch, RM may have uncertainty on its scalar\nrewards, which may introduce noise in the routing\ntraining since we use RM scores as silver supervi-\nsion. In this subsection, we first present the exis-\ntence of this uncertainty to explain the motivation\nbehind tag-based label enhancement, the method\nwe propose to mitigate such uncertainty in the rout-\ning function training. We calculate the entropy of\nrewards from QwenRM among all candidate LLMs\nfor each query in MT-Bench and draw it with the\nMT-Bench scores of each sample by reward prefer-\nence ranking with QwenRM. As shown in Fig. 3,\nsamples with lower reward entropy tend to have\nhigh MT-bench scores. We interpret this observa-\ntion as higher reward entropy reveals more uncer-\ntainty in the reward. Therefore, we propose tag-\nbased label enhancement to leverage a tag-based\nprior to adjust reward entropy.\nLabel Enhancement.\nThe tag-based label en-\nhancement proposed in \u00a73.2 contains a hyper-\nparameter \u03b2, which represents the trade-off be-\ntween fine-grained sample-level rewards and\ncoarse-grained tag-level rewards. We conduct ex-\nperiments to tune this hyperparameter and ana-\nlyze how rewards in different granularities may\ninfluence the training of our routing function. As\nshown in Tab. 2, ZOOTER achieves the best perfor-\nmance when \u03b2 equals 0.3, proving a combination\nof sample-level and tag-level rewards will benefit\nthe reward distillation. The ablation also shows the\nnecessity of tag-based label enhancement. Fur-\nthermore, distilling tag-level rewards (\u03b2 = 0)\nshows significantly better performance than distill-\ning sample-level rewards (\u03b2 = 1), supporting the\nanalysis that noises from the uncertainty of RMs in\nsample-level rewards damage reward distillation.\n5\nConclusion\nIn this work, we revisit the complementary po-\ntential of open-source LLMs and reward model\nranking of multiple off-the-shelf reward models,\nproviding evidence to the effectiveness of LLM en-\nsemble. We propose ZOOTER, an efficient reward-\nguided routing method for ensemble off-the-shelf\nLLMs. Comprehensive evaluation shows ZOOTER\ncan outperform the best single model on average\nand even ensemble models by reward model rank-\ning with significantly fewer computation overhead.\nValuable future works include diving deep into the\ninterpretation of latent expertise in each LLM.\nReferences\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nLingjiao Chen, Matei Zaharia, and James Zou. 2023.\nFrugalgpt: How to use large language models while\nreducing cost and improving performance.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluating\nlarge language models trained on code.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nGanqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,\nWei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and\nMaosong Sun. 2023. Ultrafeedback: Boosting lan-\nguage models with high-quality feedback.\nAdam Gleave and Geoffrey Irving. 2022.\nUncer-\ntainty estimation for language reward models. arXiv\npreprint arXiv:2203.07472.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023.\nLlm-blender: Ensembling large language models\nwith pairwise ranking and generative fusion. arXiv\npreprint arXiv:2306.02561.\nJunlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan,\nHai Zhao, and Pengfei Liu. 2023a.\nGenerative\njudge for evaluating alignment.\narXiv preprint\narXiv:2310.05470.\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan\nTaori, Ishaan Gulrajani, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023b.\nAl-\npacaeval: An automatic evaluator of instruction-\nfollowing models.\nhttps://github.com/\ntatsu-lab/alpaca_eval.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023.\nLet\u2019s verify step by step.\narXiv preprint\narXiv:2305.20050.\nJiacheng Liu, Andrew Cohen, Ramakanth Pasunuru,\nYejin Choi, Hannaneh Hajishirzi, and Asli Celiky-\nilmaz. 2023. Don\u2019t throw away your value model!\nmaking ppo even better via value-guided monte-carlo\ntree search decoding.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V. Le,\nBarret Zoph, Jason Wei, and Adam Roberts. 2023.\nThe flan collection: Designing data and methods for\neffective instruction tuning.\nKeming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-\nyang Lin, Chuanqi Tan, and Chang Zhou. 2023. #\ninstag: Instruction tagging for diversity and complex-\nity analysis. arXiv preprint arXiv:2308.07074.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023a. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo\nGeng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qing-\nwei Lin, and Daxin Jiang. 2023b.\nWizardcoder:\nEmpowering code large language models with evol-\ninstruct.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D. Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nTal Shnitzer, Anthony Ou, M\u00edrian Silva, Kate Soule,\nYuekai Sun, Justin Solomon, Neil Thompson, and\nMikhail Yurochkin. 2023. Large language model\nrouting with benchmark datasets.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei\nHuang, Yongbin Li, and Houfeng Wang. 2023. Pref-\nerence ranking optimization for human alignment.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Fran-\ncis Song, Noah Siegel, Lisa Wang, Antonia Creswell,\nGeoffrey Irving, and Irina Higgins. 2022.\nSolv-\ning math word problems with process-and outcome-\nbased feedback. arXiv preprint arXiv:2211.14275.\nGuan Wang, Sijie Cheng, Qiying Yu, and Changling\nLiu. 2023a. OpenChat: Advancing Open-source Lan-\nguage Models with Imperfect Data.\nHongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik\nKundu, Eric Xing, and Mikhail Yurochkin. 2023b.\nFusing models with complementary expertise.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hannaneh Hajishirzi. 2023c. How\nfar can camels go? exploring the state of instruction\ntuning on open resources.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nSeonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeon-\nbin Hwang, Seungone Kim, Yongrae Jo, James\nThorne, Juho Kim, and Minjoon Seo. 2023. Flask:\nFine-grained language model evaluation based on\nalignment skill sets.\nLi Yuan, Francis EH Tay, Guilin Li, Tao Wang, and\nJiashi Feng. 2020. Revisiting knowledge distillation\nvia label smoothing regularization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3903\u20133911.\nZheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting\nDong, Keming Lu, Chuanqi Tan, Chang Zhou, and\nJingren Zhou. 2023a. Scaling relationship on learn-\ning mathematical reasoning with large language mod-\nels.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023b. Rrhf: Rank\nresponses to align language models with human feed-\nback without tears.\nDataset\nAmount\nultrachat\n18,588\nsharedgpt\n10432\nwizardlm(sharedgpt)\n5325\nwizardlm(alpaca)\n5145\nalpaca\n2186\nrepair\n1034\nopenchat\n1033\nflan\n862\nmath\n849\nunnatural\n582\ndmcc\n573\ndolly\n560\noasst\n183\nlima\n70\nmbpp\n43\nTable 3: Composition of DIVINSTRUCT\nA\nDatasets\nDIVINSTRUCT is a diverse mix instruction set from\nmultiple open-source datasets with careful decon-\ntamination on all benchmarks evaluated in this\nwork. The detailed composition of DIVINSTRUCT\nis report in Tab. 3.\n"
  },
  {
    "title": "GRIM: GRaph-based Interactive narrative visualization for gaMes",
    "link": "https://arxiv.org/pdf/2311.09213.pdf",
    "upvote": "11",
    "text": "GRIM: GRaph-based Interactive narrative visualization for gaMes\nJorge Leandro\nSudha Rao\nMichael Xu\nWeijia Xu\nNebosja Jojic\nChris Brockett\nBill Dolan\nMicrosoft Research\nsudhra@microsoft.com\nAbstract\nDialogue-based Role Playing Games (RPGs)\nrequire powerful storytelling. The narratives\nof these may take years to write and typically\ninvolve a large creative team. In this work, we\ndemonstrate the potential of large generative\ntext models to assist this process. GRIM, a pro-\ntotype GRaph-based Interactive narrative visu-\nalization system for gaMes, generates a rich\nnarrative graph with branching storylines that\nmatch a high-level narrative description and\nconstraints provided by the designer. Game\ndesigners can interactively edit the graph by\nautomatically generating new sub-graphs that\nfit the edits within the original narrative and\nconstraints. We illustrate the use of GRIM\nin conjunction with GPT-4, generating branch-\ning narratives for four well-known stories with\ndifferent contextual constraints.1\n1\nIntroduction\nWhether we\u2019re talking about a movie, a novel, or a\ngame, successful storytelling requires a rich narra-\ntive structure with an engaging plot and a satisfying\nclimax. For thousands of years humans have told\nstories that have had the same kind of structure:\nintroduce the characters and situation, confront the\ncharacters with a problem, build tension when it\nlooks like the problem can\u2019t be solved, and finally\nhave the characters navigate a satisfying solution.\nA good dialogue-driven RPG game requires such\na satisfying narrative flow. Furthermore, some-\nthing that sets a game apart from passive enter-\ntainments like television or movies, is the feeling\nthat the world the player is engaged with is vast.\nNothing kills the sense of immersion more quickly\nthan clumsy attempts to push players back onto\nthe designer-specified rails, back onto the game\ndesigner\u2019s \u201cgolden path\u201d. Today, all of these goals\nhave to be met through manual authoring: dialog\ntrees, plot outlines/narrative flows, tying utterances\n1A website for Visualizing Generated Narratives\nto changes in game state or character animation,\nand so on. It\u2019s an enormous task for a big game,\nand it becomes more difficult as players want more\nand more freedom in their game experiences. In the\npast, there has been work on creating tools to aid\ngame designers (Grabska-Gradzi\u00b4nska et al., 2021;\nNeil, 2012; Alvarez et al., 2022; Liapis et al., 2013),\nbut none that explore the potential use of the latest\ngenerative AI models in this process.\nIn this work, we show how large language mod-\nels like GPT-4 can aid game designers in easily\ncreating an engaging narrative flow with multiple\nbranching storylines, thus allowing players to have\nricher and more immersive experiences. We in-\ntroduce GRIM, a graph-based interactive narrative\nvisualization tool that draws on the output of a large\ngenerative language model. Provided with a high-\nlevel narrative description of a game and specific\nconstraints, GRIM generates a narrative graph with\nbranching storylines. It creates a directed acyclic\ngraph (DAG) in which each node represents a high-\nlevel scene and the directed edges represent the dif-\nferent paths a player can take through those scenes.\nTo make this possible, we design two prompts: the\nfirst takes in the description and constraints as input\nand outputs the branching storylines in text format;\nthe second prompt takes those storylines and out-\nputs code for rendering the storylines in a graph\nformat. GRIM also allows the game designer to\niteratively expand or prune the narrative graph by\nadding or deleting nodes or edges. We enable this\nby designing a prompt that takes in the original\nnarrative graph and designer-suggested edits and\noutputs an updated narrative graph with, not just\nthe designer added or deleted nodes/edges, but also\nentirely new sub-graphs that would make the modi-\nfications fit within the originally defined narrative\nand constraints.\nWe demonstrate this with a case study where\nwe use GRIM to create narrative graphs for four\nwell-known stories: Dracula, Frankenstein, Jack\narXiv:2311.09213v1  [cs.CL]  15 Nov 2023\nFigure 1: Narrative graph of branching storlyines generated by GRIM for the story Frankenstein but grounded in the\n21st century. Additional constraints on the graph includes one start, two endings and four storylines.\nand the Beanstalk, and Little Red Riding Hood.\nThese are chosen for their familiarity, since it is\neasy to see the scope of the variations, which we\nground in one of the four settings: Minecraft game,\n21st century, Ancient Rome or Quantum Realm.\nWe also experiment with constraints specific to the\ngraph structure: number of different starts, number\nof different endings and number of different story-\nlines. Open AI\u2019s GPT-4 (Koubaa, 2023) provides\nthe language model. We include an analysis of the\nnarrative graphs generated by GRIM identifying\nsome of its strengths and weaknesses.\n2\nNarrative Graph Generation\n2.1\nLanguage Models as Graph Generators\nThere has been some recent work that explore the\nuse of large language models for generating rich\nstructured data (Tang et al., 2023; Zhao et al., 2023).\nGraphGPT 2, for instance, converts unstructured\nnatural language into a knowledge graph with a\nfocus on visualizing entities and their relationships.\nGPT Graph 3 serves as a knowledge graph explorer\nto help users explore information in an organized\nand intuitive way.\nIn the present work, we explore the potential of\na large language model such as GPT-4 to generate\nboth the data and the graph suitable to visualize\nthe data. We find that the creative space of game\nnarratives is well-suited for this kind of exploration\nas games seek rich narratives with branching sto-\nrylines that can give players rich, expansive, and\n2https://github.com/varunshenoy/graphgpt\n3https://github.com/m-elbably/gpt-graph\nimmersive experiences.\nUsing GPT-4, we generate the visualization\ngraph in two steps. First we prompt GPT-4 to cre-\nate the narrative graph, with branching storylines\nin text format. Then we use GPT-4 to encode this\ninformation into a format that can be consumed by\nthe visualization code.\n2.2\nGenerating the Narrative Graph\n2.2.1\nGenerating the Story Lines\nOur first step is to create a prompt that includes\nthe high-level narrative description, the input con-\nstraints, and specific instructions on generating\nmultiple branching story lines that will make up\nthe narrative graph. We define a storyline as a se-\nquence of narrative beats. Narrative or story beats\nare often defined as significant moments in a story\nthat evoke particular emotional reaction from the\naudience. We treat beats as the building blocks of\na storyline. The first step is to create a prompt that\nincludes the high-level narrative description, the\ninput constraints and specific instructions on how\nto generate multiple branching storylines.\nBy way of example, let\u2019s say we want to generate\na branching narrative for the well-known Franken-\nstein story, but have it translated into a 21st century\ncontext. Additionally we want the narrative graph\nto have one starting point, two different endings,\nand a total of 4 different storylines.\nWe instruct GPT-4 to follow an iterative process\nwhere we ask it to first create a single storyline\nby generating a sequence of beats and describing\neach beat in detail. We then instruct it to generate\nthe next storyline such that it contains some beats\nin common with the first and some new beats and\nfinally keep repeating this process until it generates\nall the storylines. While generating the storylines,\nwe ask GPT-4 to strictly follow a set of guidelines\nand constraints sketched below4:\n\u2022 Each storyline must follow the conventions of\na narrative consisting of conflicts or danger\nwith clear resolutions and no loose ends.\n\u2022 Storylines must be uniquely different, no more\nthan three same consecutive beats between\nany two storylines.\n\u2022 Total number of unique beats must be twice\nthe number of required storylines.\n\u2022 Original story must appear as one storyline.\n\u2022 Ground storyline in unique characteristics of\nthe input setting, including cultural elements,\nphysical properties, technology, etc.\n\u2022 There must be as many unique starts and ends\nas requested.\n\u2022 There must be 2 or 3 beats that are commmon\nbetween all storylines.\nFinally, we include an example input/output pair\nin the prompt. This prompt results in the generation\nof storylines in text format as illustrated in the\nrighthand side of Figure 1.\n2.2.2\nGenerating the Visualization\nThe next step is to generate a visual graph of the\ngenerated storylines. We prompt GPT-4 with the\ngenerated storylines and additional instructions on\nhow to generate the graph in a particular convention\nof nodes and edges, as input data to a Javascript\nD3JS browser application. We ask GPT-4 to strictly\nadhere to a set of guidelines sketched as below5:\n\u2022 Create a node for each beat such that the total\nnumber of nodes equal the total number of\nbeats.\n\u2022 Create an edge between each pair of adjacent\nnodes in the sequence of storylines.\n\u2022 Every node should be connected to the graph.\n\u2022 Create a NODES object as a dictionary with\nkey as a beat number and value as the beat\ndescription.\n4The full prompt is included in the appendix.\n5The full prompt is included in the appendix.\n\u2022 Create an EDGES object as a dictionary with\nkey as a node and value as a list incoming and\noutgoing edges to that node.\n\u2022 Make sure that every node in the NODES ob-\nject also appears in the EDGES object and\nvice-versa.\nFinally, we include an example of input story-\nlines and output graph in the prompt. This prompt\nleads to the generation of the narrative graph illus-\ntrated in the lefthand side of Figure 1.\n2.3\nIterative Editing of the Narrative Graph\nThe narrative graph generated by GRIM gives a\ngame designer a starting point that they can iter-\nate over. The designer can add or delete nodes or\nedges to the graph. Using GPT-4, we are able to\nautomatically update the graph by creating entirely\nnew sub-graphs to make the game designer\u2019s sto-\nryline updates fit within the original narrative and\nconstraints.\nLet the set of new nodes added be denoted as\nNadded, set of nodes deleted as Ndeleted, set of new\nedges added as Eadded and set of edges deleted\nas Edeleted. We prompt GPT-4 with the original\nstorylines (as generated by prompt in Section 2.2.1)\nand a set of guidelines sketched as below:\n\u2022 Update the list of storylines by adding new\nstorylines or deleting existing storylines.\n\u2022 The updated storylines should include the\nnewly added beats Nadded.\n\u2022 They should not include the deleted beats\nNdeleted.\n\u2022 The newly added beats should be connected\nto existing beats as per the edges Eadded.\n\u2022 The updated storylines should not have beat\ntransitions denoted by the deleted edges\nEdeleted.\n\u2022 Create new beats or new beat transitions as\nneeded to make the new storylines fit within\nthe original story and constraints.\nFor example, consider the storylines in Figure 1.\nSay the game designer updates the graph by adding\na new node \"Adam decides to help Dr. Frank on\nhis next project\" and connecting it to the existing\nBeat 2 node. The update graph prompt results in\nthe generation of the following new storyline:\nStoryline 5\nBeat 1: Dr. Frank, a brilliant geneticist, begins work on a\nrevolutionary project to create life from DNA fragments.\nBeat 2: Dr. Frank successfully creates a humanoid, Adam,\nusing advanced genetic engineering.\nBeat 18: Adam decides to help Dr. Frank on his next project.\nBeat 19: Dr. Frank and Adam work together on a new project\nto enhance human abilities using genetic engineering.\nBeat 20: The project is successful, resulting in the creation\nof Eve, a perfect human with enhanced abilities.\nBeat 3: Eve, feeling trapped, escapes from the laboratory.\nBeat 4: Dr. Frank and Adam realise the potential danger and\nstart a city-wide search for Eve.\nBeat 21: Eve, while exploring, uses her enhanced abilities to\nhelp people in need.\nBeat 22: Seeing the news, Dr. Frank and Adam locate Eve\nand explain her origins.\nBeat 23: Eve returns to the lab, accepting her existence and\nagreeing to work with Dr. Frank and Adam for the\nbetterment of humanity.\nOnce we get the updated storylines in text, we\nuse the graph generation prompt (Section 2.2.2) to\nvisualize the updated graph.\n3\nCase Study\n3.1\nStories, settings and constraints\nWe experiment with following four well-known sto-\nries: Dracula, Frankenstein, Jack and the Beanstalk\nand Little Red Riding Hood. We consider four set-\ntings for grounding these stories: Minecraft, 21st\ncentury, Ancient Rome and Quantum Realm. Ad-\nditionally, we consider the following constraints\non the graph structure: Number of starts: [1, 2],\nNumber of endings: [2, 4], Number of storylines:\n[4, 8].\n3.2\nAnalysis of the generate narrative graphs\nOur website Visualizing Generated Narratives\npresents the narrative graphs for all the combina-\ntions of stories, settings and constraints described\nabove. We find that GRIM is able to ground the\nstories in the given setting well.\nFor example,\nwhen asked to ground Little Red Riding Hood\nin the game of Minecraft, it generates storylines\nthat include healing potions, mob-infested cave, a\nredstone contraption and other elements that are\nspecific to the game of Minecraft, while still re-\ntaining the narrative related to the original little\nred riding hood story. Likewise, when asked to\nground Frankenstein in the 21st century, the story-\nlines include a project on creating life from DNA\nfragments, genetic engineering, a humanoid and\nother aspects that are relatable in the 21st century\nwhile still maintaining the theme of the original\nFrankenstein story. When asked to ground Dracula\nin ancient Rome, the storylines include a myste-\nrious sorcerer, a wise oracle, the Colosseum and\nsimilar such people, places and objects that are spe-\ncific to ancient Rome while still being relatable to\nthe original Dracula story. Finally, when asked to\nground Jack and the Bean Stalk in quantum realm,\nthe storylines include aspects such as magic beans\nfrom a parallel universe, macroscopic world and\nquantum watch. GRIM is also good at adhering to\nthe constraints on the number of starts, ends and\nstorylines.\nOn closer inspection, we find aspects of the nar-\nratives that still require improvement. The gener-\nated storylines could be more varied. For exam-\nple, the storylines in Figure 1 have very similar\nflavor and could benefit from more variations to\nmake them more interesting. In some settings, the\ngrounding could be better. For example, grounding\nin Minecraft or 21st century appears to be much bet-\nter than grounding in quantum realm. We presume\nthis is because there is more information about\nMinecraft and the 21st century in the language\nmodel data than there is about quantum realm, in-\nformation about which may be comparative limited.\nThis suggests that GRIM is likely to perform better\non better documented settings.\n4\nConclusion\nWe have introduced GRIM, a graph-based narra-\ntive visualization, that is powered by a large-scale\nlanguage model. GRIM allows a game designer\nto input a high-level description of a story with\nspecific constraints and generates a set of branch-\ning storylines that can be easily visualized using\na graph format. GRIM also lets the designer iter-\nate on the narrative graph in collaboration with the\nmodel by allowing the designer to edit nodes/edges\nand having the model regenerate the new story-\nlines to make them fit within the original narrative\nand constraints. We report on the use of GRIM\nto generate narrative graphs for some well-known\nstories by grounding them in some interesting set-\ntings. We believe that a tool such as GRIM can be a\nvaluable assist to narrative designers allowing them\nto expend their creativity on defining the higher-\nlevel narrative and iterating on it and delegating\nthe lower-level task of generating detailed possible\nstorylines to generative AI models for inspection,\napproval, and adaptation.\nReferences\nAlberto Alvarez, Jose Font, and Julian Togelius. 2022.\nStory designer: Towards a mixed-initiative tool to\ncreate narrative structures. In Proceedings of the\n17th International Conference on the Foundations of\nDigital Games, pages 1\u20139.\nIwona Grabska-Gradzi\u00b4nska, Leszek Nowak, Wojciech\nPalacz, and Ewa Grabska. 2021.\nApplication of\ngraphs for story generation in video games. In Pro-\nceedings of the 2021 Australasian Computer Science\nWeek Multiconference, pages 1\u20136.\nAnis Koubaa. 2023. GPT-4 vs. GPT-3.5: A Concise\nShowdown.\nAntonios Liapis, Georgios Yannakakis, and Julian To-\ngelius. 2013. Designer modeling for personalized\ngame content creation tools. In Proceedings of the\nAAAI Conference on Artificial Intelligence and Inter-\nactive Digital Entertainment, volume 9, pages 11\u201316.\nKatharine Neil. 2012. Game design tools: Time to\nevaluate. Proceedings of 2012 DiGRA Nordic.\nXiangru Tang, Yiming Zong, Yilun Zhao, Arman Cohan,\nand Mark Gerstein. 2023. Struc-bench: Are large\nlanguage models really good at generating complex\nstructured data? arXiv preprint arXiv:2309.08963.\nYilun Zhao, Chen Zhao, Linyong Nan, Zhenting\nQi,\nWenlin Zhang,\nXiangru Tang,\nBoyu Mi,\nand Dragomir Radev. 2023.\nRobut: A system-\natic study of table qa robustness against human-\nannotated adversarial perturbations. arXiv preprint\narXiv:2306.14321.\nAppendix\nA. Prompt for generating the story lines\nHere we share the exact prompt we use to generate\nthe storylines (Section 2.2.1).\nINSTRUCTION: Your task is to generate unique\nand interesting storylines given the following IN-\nPUT OPTIONS: [include the input story, setting,\nnumber of starts, ends and storylines here]\nFollow the format in the example below, without\nduplicating its content.\nStory: (name of the story),\nStarts: (number of starts here),\nEndings: (number of endings here),\nStorylines: (number of storylines here),\nSetting:\n(topic on which storylines must be\ngrounded)\nStorylines (detailed with beat descriptions):\nStoryline 1: (Line separated sequence of beats.\nInclude a detailed description of each beat\nand assign it a beat number.)\nStoryline 2: (Line separated sequence of beats\nthat have some beats common with the previ-\nous storyline(s) and some new beats. Include\na detailed description of each beat. If the beat\nis common to one of the previous storylines,\nthen its description and number should be ex-\nactly the same as in the previous one as well,\nbut repeat the detailed beat description for\nclarity. Assign new beat numbers to the new\nbeats.)\n. . .\nStoryline 10: (Line separated sequence of beats\nthat have some beats common with the previ-\nous storyline(s) and some new beats. Include\na detailed description of each beat. If the beat\nis common to one of the previous storylines,\nthen its description and number should be ex-\nactly the same as in the previous one as well,\nbut repeat the detailed beat description for\nclarity. Assign new beat numbers to the new\nbeats)\n(List as many dummy start nodes as number of\nstarts in INPUT OPTIONS)\nSTART_1: (This is a dummy node. No descrip-\ntion for it. It will always point to the beginning\nbeat of the respective storyline)\nSTART_2: (This is a dummy node. No descrip-\ntion for it. It will always point to the beginning\nbeat of the respective storyline)\n...\n(List as many dummy end nodes as number of starts\nin INPUT OPTIONS)\nEND_1: (This is a dummy node. No description\nfor it. The final beat of the respective storyline\nwill point to it)\nEND_2: (This is a dummy node. No description\nfor it. The final beat of the respective storyline\nwill point to it)\n...\nBeats (include the list of all the unique beats from\nthe storylines above. Include the exact same de-\nscription and exact same beat number)\nBeat_1: (beat description)\nBeat_2: (beat description)\n...\nBeat_n: (beat description)\nCommon intermediate Beats: (beats numbers that\nare common to ALL the storylines)\nStorylines (with only beat numbers)\nStoryline 1: (a dummy START node, comma-\nseparated exact sequence of beat numbers of\nthis storyline, a dummy END node)\nStoryline 2: (a dummy START node, comma-\nseparated exact sequence of beat numbers of\nthis storyline, a dummy END node)\n...\nStoryline 10: (a dummy START node, comma-\nseparated exact sequence of beat numbers of\nthis storyline, a dummy END node)\nYOU MUST STRICTLY FOLLOW THESE CON-\nSTRAINTS\n1. Each storyline must consist of a sequence of\nnarrative beats. Different storylines must have\ndifferent sequence of beats. The common sub-\nsequence between two storylines cannot be\ngreater than three.\n2. THE TOTAL NUMBER OF BEATS MUST\nBE ATLEAST TWICE THE NUMBER OF\nSTORYLINES. Describe each beat in detail.\n3. Make sure that the original story appears as\none of the resulting storylines.\n4. Ground the storylines in the setting focusing\non characteristics of the setting that are unique\nand help make the storylines interesting and\nnovel. Those characteristics might include\ncultural elements like foods or clothing or mu-\nsic, strange physical properties, unique flora\nand fauna, unusual geographical features, and\nsurprising technology.\n5. There must be only as many unique starts as\ngiven in the INPUT OPTIONS, with each start\npointing to a different beat.\n6. There must be only as many unique endings\nas given in the INPUT OPTIONS, with each\nending being pointed to by a different beat.\n7. THERE MUST BE 2 OR 3 BEATS THAT\nARE COMMON IN ALL THE STORY-\nLINES. These must be the important narrative\nbeats in the story. The common beats must\nnot be consecutive.\n8. IMPORTANT: As you are writing each sto-\nryline, think if the sequence of beats make\nsense to be a coherent storyline. Each story-\nline should follow the conventions of fairytale\nnarratives of conflicts or dangers and clear res-\nolutions. There should be no loose ends. Each\nstoryline should be a unique sequence of beats\nthat is different from other storylines.\nBelow is an example output:\nStory: Little Red Riding Hood\nStarts: 2\nEndings: 4\nStorylines: 8\nSetting: 21st century\nStorylines (8):\nStoryline 1:\nBeat 1: Red, a tech-savvy girl living in a smart\ncity, receives a call from her sick grandmother.\nBeat 2: Grandmother requests Red to bring her\nsome medicines from the nearby pharmacy.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 4: En route, Red encounters a stranger, a\ncunning hacker, who learns about her mission.\nBeat 5: The hacker manipulates the city\u2019s GPS\nsystem to mislead Red.\nBeat 6: Misled, Red ends up in an abandoned\nfactory.\nBeat 7: Realizing the trick, Red uses her tech\nskills to trace the hacker\u2019s location.\nBeat 8: Red exposes the hacker to the city\u2019s cy-\nber police and continues her journey to her\ngrandmother\u2019s house.\nBeat 9: Red delivers the medicines and they have\na virtual family gathering via video call.\nStoryline 2:\nBeat 1: Red, a tech-savvy girl living in a smart\ncity, receives a call from her sick grandmother.\nBeat 10: Grandmother asks Red to bring her a\nspecial gadget from the tech mall.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 4: En route, Red encounters a stranger, a\ncunning hacker, who learns about her mission.\nBeat 11: The hacker hacks into Red\u2019s smartwatch,\nstealing her personal data.\nBeat 12: Red notices suspicious activity on her\nsmartwatch and seeks help from her friend, a\ncybersecurity expert.\nBeat 13: Together, they trace the hacker and re-\ntrieve Red\u2019s data.\nBeat 14: Red buys the gadget and delivers it to\nher grandmother.\nStoryline 3:\nBeat 15: Red, a social media influencer, plans a\nlive stream to visit her grandmother.\nBeat 2: Grandmother requests Red to bring her\nsome medicines from the nearby pharmacy.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 16: Red\u2019s live stream attracts the attention of\na cyber-stalker.\nBeat 17: The stalker tries to find Red\u2019s location\nusing the live stream data.\nBeat 7: Realizing the threat, Red uses her tech\nskills to trace the stalker\u2019s location.\nBeat 8: Red exposes the stalker to the city\u2019s cy-\nber police and continues her journey to her\ngrandmother\u2019s house.\nBeat 9: Red delivers the medicines and they have\na virtual family gathering via video call.\nStoryline 4:\nBeat 15: Red, a social media influencer, plans a\nlive stream to visit her grandmother.\nBeat 10: Grandmother asks Red to bring her a\nspecial gadget from the tech mall.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 16: Red\u2019s live stream attracts the attention of\na cyber-stalker.\nBeat 18: The stalker tries to manipulate Red\u2019s\nfollowers against her.\nBeat 19: Red, noticing the unusual comments,\nuses her influence to expose the stalker\u2019s in-\ntentions.\nBeat 20: Red\u2019s followers, united, report the stalker\nleading to his arrest.\nBeat 14: Red buys the gadget and delivers it to\nher grandmother.\nStoryline 5:\nBeat 1: Red, a tech-savvy girl living in a smart\ncity, receives a call from her sick grandmother.\nBeat 21: Grandmother asks Red to download and\ninstall a specific software on her computer.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 4: En route, Red encounters a stranger, a\ncunning hacker, who learns about her mission.\nBeat 22: The hacker sends Red a malicious soft-\nware disguised as the one requested by her\ngrandmother.\nBeat 23: Red, noticing the odd behavior of the\nsoftware, realizes the trick.\nBeat 24: Red, with the help of her tech community,\nremoves the malicious software and exposes\nthe hacker.\nBeat 25: Red installs the correct software on her\ngrandmother\u2019s computer.\nStoryline 6:\nBeat 1: Red, a tech-savvy girl living in a smart\ncity, receives a call from her sick grandmother.\nBeat 26: Grandmother asks Red to bring her some\ndigital books from the e-library.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 4: En route, Red encounters a stranger, a\ncunning hacker, who learns about her mission.\nBeat 27: The hacker tries to gain access to Red\u2019s\ne-library account.\nBeat 28: Red, noticing the login attempts, secures\nher account and reports the hacker.\nBeat 29: Red downloads the digital books and\ndelivers them to her grandmother.\nStoryline 7:\nBeat 15: Red, a social media influencer, plans a\nlive stream to visit her grandmother.\nBeat 21: Grandmother asks Red to download and\ninstall a specific software on her computer.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 16: Red\u2019s live stream attracts the attention of\na cyber-stalker.\nBeat 30: The stalker sends Red a dangerous link\npretending to be a fan.\nBeat 31: Red, being tech-savvy, recognizes the\ndangerous link and alerts her followers.\nBeat 32: Red\u2019s followers report the stalker leading\nto his arrest.\nBeat 25: Red installs the correct software on her\ngrandmother\u2019s computer.\nStoryline 8:\nBeat 15: Red, a social media influencer, plans a\nlive stream to visit her grandmother.\nBeat 26: Grandmother asks Red to bring her some\ndigital books from the e-library.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 16: Red\u2019s live stream attracts the attention of\na cyber-stalker.\nBeat 33: The stalker tries to disrupt Red\u2019s live\nstream by spreading false rumors.\nBeat 34: Red, noticing the disruption, uses her\ninfluence to debunk the rumors.\nBeat 35: Red\u2019s followers, united, report the stalker\nleading to his arrest.\nBeat 29: Red downloads the digital books and\ndelivers them to her grandmother.\nSTART_1: Points to Beat 1\nSTART_2: Points to Beat 15\nEND_1: Points from Beat 9\nEND_2: Points from Beat 14\nEND_3: Points from Beat 25\nEND_4: Points from Beat 29\nBeats:\nBeat 1: Red, a tech-savvy girl living in a smart\ncity, receives a call from her sick grandmother.\nBeat 2: Grandmother requests Red to bring her\nsome medicines from the nearby pharmacy.\nBeat 3: Red, wearing her red hoodie, ventures out\nwith her electric scooter.\nBeat 4: En route, Red encounters a stranger, a\ncunning hacker, who learns about her mission.\nBeat 5: The hacker manipulates the city\u2019s GPS\nsystem to mislead Red.\nBeat 6: Misled, Red ends up in an abandoned\nfactory.\nBeat 7: Realizing the trick, Red uses her tech\nskills to trace the hacker\u2019s location.\nBeat 8: Red exposes the hacker to the city\u2019s cy-\nber police and continues her journey to her\ngrandmother\u2019s house.\nBeat 9: Red delivers the medicines and they have\na virtual family gathering via video call.\nBeat 10: Grandmother asks Red to bring her a\nspecial gadget from the tech mall.\nBeat 11: The hacker hacks into Red\u2019s smartwatch,\nstealing her personal data.\nBeat 12: Red notices suspicious activity on her\nsmartwatch and seeks help from her friend, a\ncybersecurity expert.\nBeat 13: Together, they trace the hacker and re-\ntrieve Red\u2019s data.\nBeat 14: Red buys the gadget and delivers it to\nher grandmother.\nBeat 15: Red, a social media influencer, plans a\nlive stream to visit her grandmother.\nBeat 16: Red\u2019s live stream attracts the attention of\na cyber-stalker.\nBeat 17: The stalker tries to find Red\u2019s location\nusing the live stream data.\nBeat 18: The stalker tries to manipulate Red\u2019s\nfollowers against her.\nBeat 19: Red, noticing the unusual comments,\nuses her influence to expose the stalker\u2019s in-\ntentions.\nBeat 20: Red\u2019s followers, united, report the stalker\nleading to his arrest.\nBeat 21: Grandmother asks Red to download and\ninstall a specific software on her computer.\nBeat 22: The hacker sends Red a malicious soft-\nware disguised as the one requested by her\ngrandmother.\nBeat 23: Red, noticing the odd behavior of the\nsoftware, realizes the trick.\nBeat 24: Red, with the help of her tech community,\nremoves the malicious software and exposes\nthe hacker.\nBeat 25: Red installs the correct software on her\ngrandmother\u2019s computer.\nBeat 26: Grandmother asks Red to bring her some\ndigital books from the e-library.\nBeat 27: The hacker tries to gain access to Red\u2019s\ne-library account.\nBeat 28: Red, noticing the login attempts, secures\nher account and reports the hacker.\nBeat 29: Red downloads the digital books and\ndelivers them to her grandmother.\nBeat 30: The stalker sends Red a dangerous link\npretending to be a fan.\nBeat 31: Red, being tech-savvy, recognizes the\ndangerous link and alerts her followers.\nBeat 32: Red\u2019s followers report the stalker leading\nto his arrest.\nBeat 33: The stalker tries to disrupt Red\u2019s live\nstream by spreading false rumors.\nBeat 34: Red, noticing the disruption, uses her\ninfluence to debunk the rumors.\nBeat 35: Red\u2019s followers, united, report the stalker\nleading to his arrest.\nCommon intermediate Beats: Beat 3, Beat 4, Beat\n16\nStorylines (8)\nStoryline 1: START_1, 1, 2, 3, 4, 5, 6, 7, 8, 9,\nEND_1\nStoryline 2: START_1, 1, 10, 3, 4, 11, 12, 13, 14,\nEND_2\nStoryline 3: START_2, 15, 2, 3, 16, 17, 7, 8, 9,\nEND_1\nStoryline 4: START_2, 15, 10, 3, 16, 18, 19, 20,\n14, END_2\nStoryline 5: START_1, 1, 21, 3, 4, 22, 23, 24, 25,\nEND_3\nStoryline 6: START_1, 1, 26, 3, 4, 27, 28, 29,\nEND_4\nStoryline 7: START_2, 15, 21, 3, 16, 30, 31, 32,\n25, END_3\nStoryline 8: START_2, 15, 26, 3, 16, 33, 34, 35,\n29, END_4\nB. Prompt for generating graph\nHere we share the exact prompt used to generate\nthe graph structure given the storylines (Section\n2.2.2)\nINSTRUCTION: Given this narrative game draft\n[include the storylines i.e. the exact output gener-\nated by the model on the previous prompt], your\ntask is to structure this input as nodes and edges ob-\njects striclty following the format described below.\nGuideline 1: For example, take a story draft\nstructured as follows:\nStory: Little Red Riding Hood,\nStarts: 1,\nEndings: 1,\nStorylines: 8,\nSetting: Minecraft\nSTART_1: (This is a dummy node. No descrip-\ntion for it. It will always point to the beginning\nbeat of the respective storyline)\nEND_1: (This is a dummy node. No description\nfor it. The final node of the respective story-\nline will point to it.)\nBeats:\nBeat_1: Little Red Riding Hood, a Minecraft char-\nacter, is given a task by her mother to deliver\na basket of food to her grandmother\u2019s house.\nBeat_2: Little Red Riding Hood ventures through\na dense forest biome, collecting materials for\nher journey.\nBeat_3: She encounters a friendly Minecraft vil-\nlager who warns her about the dangerous\nwolves in the forest.\nBeat_4: Little Red Riding Hood is distracted by a\nbeautiful flower biome and strays off the path.\nBeat_5: She encounters a wolf (a Minecraft\nmob), who tricks her into revealing her grand-\nmother\u2019s location.\nBeat_6: The wolf races ahead and locks her grand-\nmother in a Minecraft dungeon.\nBeat_7: Little Red Riding Hood arrives at her\ngrandmother\u2019s house and realizes something\nis wrong.\nBeat_8: She bravely confronts the wolf and res-\ncues her grandmother by using her Minecraft\ntools.\nCommon intermediate beats: Beat_3, Beat_5\nStorylines (8):\nStoryline 1: START_1, Beat_1, Beat_2, Beat_3,\nBeat_5, Beat_7, Beat_8, END_1\nStoryline 2: START_1, Beat_1, Beat_2, Beat_3,\nBeat_4, Beat_5, Beat_8, END_1\nStoryline 3: START_1, Beat_1, Beat_2, Beat_3,\nBeat_5, Beat_6, Beat_7, Beat_8, END_1\nStoryline 4: START_1, Beat_1, Beat_2, Beat_4,\nBeat_3, Beat_5, Beat_7, Beat_8, END_1\nStoryline 5: START_1, Beat_1, Beat_3, Beat_2,\nBeat_4, Beat_5, Beat_8, END_1\nStoryline 6: START_1, Beat_1, Beat_3, Beat_2,\nBeat_5, Beat_6, Beat_7, Beat_8, END_1\nStoryline 7: START_1, Beat_1, Beat_3, Beat_2,\nBeat_5, Beat_7, Beat_8, END_1\nStoryline 8: START_1, Beat_1, Beat_3, Beat_5,\nBeat_2, Beat_4, Beat_7, Beat_8, END_1\nGuideline 2: Now, consider the next convention for\nnodes and edges objects from a network represent-\ning the given storylines.\nThese objects are meant as input data to a Javascript\nD3JS browser application for visualization. Bear\nin mind START and END nodes are always in the\nend of each object.\nNODES:\n{\n\"Beat_1\": [[\"None\", 1, \"Little Red Riding Hood,\na Minecraft character, is given a task by her\nmother to deliver a basket of food to her grand-\nmother\u2019s house.\", \"1\"]],\n\"Beat_2\": [[\"None\", 2, \"Little Red Riding Hood\nventures through a dense forest biome, collect-\ning materials for her journey.\", \"1\"]],\n\"Beat_3\": [[\"None\", 3, \"She encounters a friendly\nMinecraft villager who warns her about the\ndangerous wolves in the forest.\", \"1\"]],\n\"Beat_4\": [[\"None\", 4, \"Little Red Riding Hood\nis distracted by a beautiful flower biome and\nstrays off the path.\", \"1\"]],\n\"Beat_5\": [[\"None\", 5, \"She encounters a wolf (a\nMinecraft mob), who tricks her into revealing\nher grandmother\u2019s location.\", \"1\"]],\n\"Beat_6\": [[\"None\", 6, \"The wolf races ahead\nand locks her grandmother in a Minecraft dun-\ngeon.\", \"1\"]],\n\"Beat_7\": [[\"None\", 7, \"Little Red Riding Hood\narrives at her grandmother\u2019s house and real-\nizes something is wrong.\", \"1\"]],\n\"Beat_8\": [[\"None\", 8, \"She bravely confronts the\nwolf and rescues her grandmother by using\nher Minecraft tools.\", \"1\"]],\n\"START_1\":\n[[\"None\",\nnull,\nnull,\nnull]],\n\"END_1\": [[\"None\", null, null, null]]\n}\nEDGES:\n\"Beat_1\": \"None\": [[[\"START_1\", \"Beat_1\"]],\n[[\"Beat_1\",\n\"Beat_2\"],\n[\"Beat_1\",\n\"Beat_3\"]]],\n\"Beat_2\":\n\"None\":\n[[[\"Beat_1\", \"Beat_2\"]],\n[[\"Beat_2\",\n\"Beat_3\"],\n[\"Beat_2\",\n\"Beat_4\"]]],\n\"Beat_3\":\n\"None\":\n[[[\"Beat_1\",\n\"Beat_3\"],[\"Beat_2\", \"Beat_3\"]], [[\"Beat_3\",\n\"Beat_4\"], [\"Beat_3\", \"Beat_5\"]]],\n\"Beat_4\":\n\"None\":\n[[[\"Beat_2\", \"Beat_4\"],\n[\"Beat_3\",\n\"Beat_4\"]],\n[[\"Beat_4\",\n\"Beat_5\"]]],\n\"Beat_5\":\n\"None\":\n[[[\"Beat_3\", \"Beat_5\"],\n[\"Beat_4\", \"Beat_5\"]], [[\"Beat_5\", \"Beat_6\"],\n[\"Beat_5\", \"Beat_7\"]]],\n\"Beat_6\":\n\"None\":\n[[[\"Beat_5\", \"Beat_6\"]],\n[[\"Beat_6\", \"Beat_7\"]]],\n\"Beat_7\":\n\"None\":\n[[[\"Beat_5\", \"Beat_7\"],\n[\"Beat_6\",\n\"Beat_7\"]],\n[[\"Beat_7\",\n\"Beat_8\"]]],\n\"Beat_8\":\n\"None\":\n[[[\"Beat_7\", \"Beat_8\"]],\n[[\"Beat_8\", \"END_1\"]]],\n\"START_1\":\n\"None\":\n[[],\n[[\"START_1\",\n\"Beat_1\"]]],\n\"END_1\": \"None\": [[[\"Beat_8\", \"END_1\"]],[]]\n}\nMore guidelines:\n1. Notice the meaning of elements in the\nnodes representation: node_id: [[game_state,\nnr_beat, beat, pathway]], where: node_id is\na string with the label \"Beat_\" and a number\nto identify a node, game_state is the game\nstate, nr_beat is the number of the respective\nbeat, beat is a string describing respective beat,\npathway is a string with an integer label to\nidentify the path in the graph corresponding\nto a quest or storyline.\n2. Each node must correspond to one and only\none beat, so that the number of nodes and\nbeats are the same in the end.\n3. Make sure to create a node for every beat. No\nbeat should be left without a node.\n4. Don\u2019t create nodes semantically equal. Each\nnode has a unique and distinct beat associated\nto it in terms of semantic.\n5. For every beginning beat, create an associ-\nated dummy START node (e.g. START_1,\nSTART_2, ...) and connect the latter to the\nformer.\n6. For every ending beat, create an associated\ndummy END node (e.g. END_1, END_2, ...)\nand connect the former to the latter.\n7. Make sure to create an edge between each pair\nof adjacent nodes in the given sequences for\nthe storylines. Make sure you don\u2019t miss out\nany edge.\n8. Every node must be connected to the graph.\n9. START nodes must be at the end of the\nNODES and EDGES objects. START nodes\nare prohibited in the beginning of any objects.\nNEVER EVER put START and END nodes\nin the beginnig of any object.\n10. END nodes must be at the end of the NODES\nand EDGES objects. END nodes are prohib-\nited in the beginning of any object. NEVER\nEVER put START and END nodes in the be-\nginnig of any object.\n11. Make sure that every node in the NODES ob-\nject also appears in the EDGES object and\nvice-versa.\n12. Color the nodes pertaining to a same storyline\nwith the very same color, that is, assigning\na same integer value starting from 1 to the\ncorrespoding pathline property of the node.\n"
  },
  {
    "title": "UNcommonsense Reasoning: Abductive Reasoning about Uncommon Situations",
    "link": "https://arxiv.org/pdf/2311.08469.pdf",
    "upvote": "10",
    "text": "UNcommonsense Reasoning:\nAbductive Reasoning about Uncommon Situations\nWenting Zhao1\u2217\nJustin T. Chiu1\nJena Hwang2\nFaeze Brahman2\nJack Hessel2\nSanjiban Choudhury1\nYejin Choi2,3\nXiang Lorraine Li4\u2217\nAlane Suhr5\u2217\n1Cornell University, 2Allen Institute for Artificial Intelligence\n3University of Washington, 4University of Pittsburgh, 5University of California, Berkeley\nwz346@cornell.edu, xianglli@pitt.edu, suhr@berkeley.edu\nAbstract\nLanguage technologies that accurately model\nthe dynamics of events must perform common-\nsense reasoning. Existing work evaluating com-\nmonsense reasoning focuses on making infer-\nences about common, everyday situations. To\ninstead investigate the ability to model unusual,\nunexpected, and unlikely situations, we ex-\nplore the task of uncommonsense abductive\nreasoning. Given a piece of context with an un-\nexpected outcome, this task requires reasoning\nabductively to generate a natural language ex-\nplanation that makes the unexpected outcome\nmore likely in the context. To this end, we\ncurate and release a new English language cor-\npus called UNcommonsense. We characterize\nthe differences between the performance of hu-\nman explainers and the best performing large\nlanguage models, finding that model-enhanced\nhuman-written explanations achieve the highest\nquality by trading off between specificity and\ndiversity. Finally, we experiment with several\nonline imitation learning algorithms to train\nopen and accessible language models on this\ntask. When compared with the vanilla super-\nvised fine-tuning approach, these methods con-\nsistently reduce lose rates on both common and\nuncommonsense abductive reasoning judged\nby human evaluators.\n1\nIntroduction\nThe ability to perform commonsense reasoning is\ncrucial for understanding the dynamics of every-\nday events, both for humans and for natural lan-\nguage processing systems. However, most existing\ncommonsense reasoning benchmarks focus on the\nability to model common events (Sap et al., 2019;\nTalmor et al., 2019; Lin et al., 2020b), i.e., given\na commonly encountered situation, what common-\nsense inferences can be made? Comparatively less\neffort has been devoted to evaluating a different\n\u2217Wenting, Lorraine, and Alane\u2019s work done at AI2. Lor-\nraine and Alane are co-last authors.\nContext: Cameron tried sushi for the \ufb01rst time, and really disliked it.\nUncommon Outcome: Cameron will want to stay and eat more sushi.\nUNcommonsense  \nAbductive \nReasoning\nExplanations:\nNaturally follows the context.\nMakes outcome more likely.\nLeaves little information gap in-between.\nDespite disliking the taste \nof sushi, Cameron decided to stay \nand eat more sushi plates to avoid \ndisappointing his partner, who was \nexcited about sharing\u2026\nFigure 1: Given a context and an uncommon outcome,\nuncommonsense abductive reasoning aims to produce\nan explanation so that the unlikely outcome becomes\nlikely. The explanation needs to follow the three rules\nnoted with the check marks.\nclass of inputs: unusual scenarios, improbable situ-\nations, and implausible events.\nTo bridge this gap, we introduce UNCOMMON-\nSENSE, a benchmark that explicitly challenges\nmodels to reason about implausible, yet still pos-\nsible, events. UNCOMMONSENSE is an English-\nlanguage corpus consisting of 20k unique contexts\npaired with explicitly uncommon outcomes. We\nsource uncommon outcomes from the incorrect an-\nswers in several multiple choice commonsense rea-\nsoning benchmarks, which were designed to chal-\nlenge models to identify the most likely outcome\namong multiple candidates, given a context. Given\nthese contexts and uncommon outcomes, we crowd-\nsource 41k abductive explanations, which provide\na plausible explanation of how an uncommon out-\ncome could have arisen, given an input context.\nSee Figure 1 for an example. UNCOMMONSENSE\ncomplements existing commonsense reasoning cor-\npora (e.g., Mostafazadeh et al., 2016a; Bhagavat-\nula et al., 2020; Rudinger et al., 2020) that focus\non reasoning about common events.1\nWe examine the gap between human and model\nperformance in generating abductive uncommon-\n1We will release our code and data upon publication.\narXiv:2311.08469v1  [cs.CL]  14 Nov 2023\nsense explanations, finding subtle differences in\nexplanation quality. Given a few demonstrations,\nthe top-performing LLM GPT-4 (OpenAI, 2023)\nproduces more specific explanations than those ac-\nquired through crowdsourcing; however, these ex-\nplanations are less diverse. While their explana-\ntions often lack sufficient details to connect con-\ntexts to outcomes, workers recruited through crowd-\nsourcing excel at creating a broader picture of pos-\nsible intermediate events. To combine the creativ-\nity of human authors and the specificity of LLM-\ngenerated explanations, we experiment with using\nan LLM to refine crowd-authored explanations by\nfilling in more details. Though LLM-generated\nexplanations are generally preferred over the origi-\nnal crowd-written explanations, we find that LLM-\nrefined crowd-written explanations hold a notable\nadvantage over those generated only by an LLM.\nGenerating abductive explanations for uncom-\nmon outcomes without conditioning on a human-\nwritten starting point remains a challenge, partic-\nularly for publicly available models. Specifically,\nwe find that the purely offline learning approach of\nsupervised fine-tuning results in models that suffer\nfrom compounding errors during generation. This\nis particularly problematic for our task, which gen-\nerally requires lengthy explanations that bridge the\ngap between a context and an uncommon explana-\ntion. To this end, we experiment with two online\nimitation learning methods to improve the perfor-\nmance of open and accessible language models on\nuncommonsense abductive reasoning. When com-\npared with supervised fine-tuning, these methods\nshow an absolute 10% increase in win rates against\nthe strong GPT-4 baseline when evaluated by work-\ners on both commonsense and uncommonsense\nabductive reasoning.\n2\nUncommonsense Abductive Reasoning\nGiven a natural language context x and outcome y,\nthe task of abductive reasoning requires generating\na natural language explanation z that augments the\ncontext, making the outcome more probable (Bha-\ngavatula et al., 2020). In uncommonsense abduc-\ntive reasoning, we focus on situations where an\noutcome y is very unlikely to happen in context x.\nFor example, in Figure 1, our context \u201cCameron\ntried sushi for the first time, and really disliked it.\u201d\nis paired with the unlikely outcome \u201cCameron will\nwant to stay and eat more sushi.\u201d. One possible\nabductive explanation of this outcome is that \u201c...\nCameron decided to stay and eat more sushi plates\nto avoid disappointing his partner, who was excited\nabout sharing...\u201d. When the context is augmented\nwith this explanation, it becomes significantly more\nlikely that the outcome will occur.\nTo our knowledge, no existing datasets explic-\nitly study abductive reasoning for uncommon sit-\nuations. We fill this gap by collecting the UN-\nCOMMONSENSE dataset, which contains contexts\npaired with both uncommon outcomes and expla-\nnations that rationalize these uncommon outcomes.\nTable 1 presents several examples from UNCOM-\nMONSENSE, with explanations written by humans.\nIn this section, we describe our process for col-\nlecting UNCOMMONSENSE, including collecting\nuncommon outcomes and abductive explanations.\n2.1\nUncommon Outcomes\nWe first collect pairs of natural language con-\ntexts and uncommon outcomes. We source con-\ntexts from two existing commonsense datasets:\nSocialIQA (Sap et al., 2019) and ROCSto-\nries (Mostafazadeh et al., 2016b). Each uncom-\nmon outcome is either human-written or LLM-\ngenerated.\nun-SocialIQA.\nSocialIQA is a multiple-choice\nquestion answering dataset created to evaluate rea-\nsoning about social interactions. Each example\nconsists of a context x, a question q, and three\nanswer choices A, one of which is correct. To\npick the uncommon outcome, we identify the least\nlikely answer choice (among the incorrect ones) by\nwe computing argmina\u2208A\u2212p(a|x, q) with GPT-3,\nwhere A\u2212 is the set of two incorrect answers. We\nthen use LLM prompting2 to combine the ques-\ntion and the least likely incorrect answer choice\ninto a declarative sentence, which we take as the\nuncommon outcome y.3\nAll original SocialIQA answer choices are\nhuman-written. To further diversify uncommon\noutcomes, we additionally generate new improba-\nble answer choices using few-shot prompting with\nLLMs. We use 6-shot prompting with GPT-44 to\nproduce one improbable answer for a randomly\nsampled subset of SocialIQA contexts and ques-\ntions, then combine the question and generated\nanswer into into uncommon outcomes using the\n2All prompting templates can be found in Appendix D.\n3Appendix B contains more details about this process.\n4We use gpt4-0314 for all generation tasks, including un-\ncommon outcomes, explanations, and during online learning.\nContext\nUncommon Outcome\nExplanation\nKai bought a Kindle from Amazon and used\nit all of the time.\nKai will want to return the Kin-\ndle and go back to reading phys-\nical books only.\nAfter a month of reading books with\nthe Kindle, the free book trial ran out\nand Kai decided that reading books on\nthe Kindle was not worth paying for.\nThe return period for the Kindle has\nnot ended yet.\nTracy went shopping at the market and\nbrought many good items at the super market\nlike fish and meat.\nTracy will want to get angry.\nTracy realized that many of the items\nshe bought were already expired, and\nthe shopkeepers had knowingly sold\nher expired meats.\nScott was hungry. He decided to cook dinner.\nHe cooked tacos. He made enough to share\nwith a friend.\nHis friend was so offended he\nasked Scott to leave.\nScott made the tacos with beef and\ndidn\u2019t tell his friend until after they ate,\neven though he know that his friend\nwas a strict vegetarian.\nDrew order a small ice cream cone at the\nDrive Thru. He drove to the front window to\nwait for his order. The cashier handed the ice\ncream with one of her hands. Drew took the\nice cream cone and turned it upside down.\nDrew kept his car clean this way.\nHe just dumped the whole thing into a\nsmall plastic cup he kept in the car and\nthen he ate it out of the cup.\nTable 1: UNCOMMONSENSE examples. The first two examples are from un-SocialIQA and the next two examples\ncome from un-RocStories; explanations are written by crowdworkers.\nprocedure above.\nun-RocStories.\nThe ROCStories Cloze Test in-\ncludes examples of four-sentence stories paired\nwith two sentence-length endings. The original\ntask is to predict which of the two endings is more\nlikely. In UNCOMMONSENSE, we take each four-\nsentence story as the context x and the incorrect\nending as the uncommon outcome y.\nFiltering out common outcomes.\nTo focus on\nuncommon scenarios, we exclude examples where\noutcomes are obvious in the context.5 We prompt\nGPT-4 to rate the likelihood of the outcome given\nthe context on a scale from 1 to 5, and remove\nexamples with ratings of 4 or 5. Filtering with this\ncriterion removes 0.7% of un-RocStories examples\nand 1.82% of un-SocialIQA examples.\n2.2\nExplanations for Uncommon Outcomes\nWe crowdsource explanations of uncommon out-\ncomes z on Amazon Mechanical Turk (MTurk)\nfrom 156 unique workers, with a pay rate of 15\nUSD/hour.6 We also experiment with using an\nLLM both to generate explanations from scratch\ngiven contexts paired with uncommon outcomes,\nand to enhance crowd-written explanations. Specif-\nically, we use GPT-4, which has demonstrated\nstrong reasoning abilities on a wide range of tasks.\n5Both human-written and LLM-generated outcomes can\nbe too obvious without filtering.\n6Appendix E contains additional details on crowdsourcing.\nExplanation Writing.\nWe first conduct a paid\nqualification task that identifies 204 workers who\nwrite high-quality explanations, who are then in-\nvited to participate in explanation writing tasks.\nTasks are launched in small batches, and we\nevenly distribute tasks across workers in each batch,\nwhich, by design, ensures that no worker writes too\nmany explanations. Due to the subjectivity on eval-\nuation for this task, we emphasize collecting a wide\nvariety of explanations on the development and test\nsets, creating no less than three tasks for each pair\nof context and outcome collected in Section 2.1.\nWe also perform extensive quality control on col-\nlected explanations, described in Appendix E. We\nalso use this task to identify the outcomes that are\nimpossible given their contexts, asking workers\nto mark these examples and provide their reason-\ning. We remove examples marked as impossible by\nmore than half of its annotators.\nLLM-Enhanced Crowd-written Explanations.\nWe experiment with prompting LLMs to enhance\ncrowd-written explanations. We instruct GPT-4 to\nadd details that better connect contexts and out-\ncomes.\nLLM-Generated Explanations.\nWe use 3-shot\nprompting with GPT-4 to generate explanations for\neach context-outcome pair.\nLLM-Enhanced LLM-Generated Explanations.\nTo directly investigate the effect of LLM-based\nun-RocStories\nun-SocialIQA\n# of context-outcome (x, y) pairs, with y sourced from...\nHuman\n1,775 /\n765 /\n999\n5,531 /\n543 /\n999\nLLM\n0 /\n0 /\n0\n8,699 /\n931 /\n705\n# of explanations z, sourced from...\nCrowd\n8,428 / 4,240 / 4,835\n14,563 / 4,407/ 5,238\nC+LLM\n8,333 / 4,203 / 4,771\n14,469 / 4,390 / 5,209\nLLM\n17,548 / 7,556 / 9,919\n14,324 / 4,422 / 5,112\nTable 2: Basic statistics of UNCOMMONSENSE. Counts\nin cells report the number of examples split across the\ntrain/dev/test sets.\nUNCOMMONSENSE\n\u03b1NLG\nl\nun-RocStories\nun-SocialIQA\n(Human)\n(LLM)\n5\n0.0\n0.0\n0.0\n0.1\n4\n0.0\n0.0\n0.0\n31.8\n3\n29.4\n50.7\n25.8\n40.3\n2\n63.1\n42.1\n59.6\n19.9\n1\n7.5\n6.9\n14.5\n0.9\nTable 3: Proportion of outcomes assigned likelihoods\nl \u2208 {1 . . . 5} for examples in UNCOMMONSENSE cor-\nresponding to un-RocStories and un-SocialIQA (split\nby human-authored and LLM-generated uncommon out-\ncomes), compared with \u03b1NLG.\nexplanation enhancement, we also apply LLM en-\nhancement to one randomly-chosen LLM explana-\ntion for each context-outcome pair, using the same\nprompting method that was used to enhance Crowd\nexplanations. We refer to these LLM-enhanced\nLLM-generated explanations as LLM2.\n3\nData Analysis\nTable 2 contains basic statistics of the collected\ndata.\nUNCOMMONSENSE includes 3,539 con-\ntexts paired with uncommon outcomes in un-\nRocStories and 17,408 in un-SocialIQA for a total\nof 20,947 context-outcome pairs. We adopt the\nsame train/dev/test splits as the original releases\nof RocStories and SocialIQA. In total, we col-\nlect 41,711 crowd-written explanations (Crowd),\n41,375 LLM-enhanced crowd-written explanations\n(C+LLM), and 58,881 LLM-generated explana-\ntions (LLM). We compare explanations from these\nthree sources using several metrics, including hu-\nman preference judgments, explanation lengths,\nand measures of explanation diversity.\nUnlikely Outcomes.\nWe compute the likelihoods\nof the outcomes via GPT-4 prompting. Table 3\nsummarizes the ratios of outcomes broken down by\ntheir likelihoods with 1 being the most unlikely. In\n\u03b1NLG, only 20.8% of outcomes have a likelihood\nContext: The band walked to the front of the stage. They\nbegan to perform. The electricity immediately went off.\nEveryone couldn\u2019t see where they were.\nOutcome: People danced in the well lit room.\nGPT4-generated Explanation (LLM): Someone in the\naudience had a powerful flashlight, and they used it to\nilluminate the room while the band continued to play\nacoustically. This allowed everyone to continue dancing\ndespite the power outage. Comment: The explanation is\ndetailed, but it is a less likely continuation of the context.\nCrowd-written Explanation (Crowd): Suddenly, they\nfound the light switch. Comment: The explanation is\nlikely to happen, but there is a large information gap in-\nbetween.\nCrowd-GPT4 Explanation (C+LLM): The band\u2019s man-\nager quickly grabbed a flashlight and located the circuit\nbreaker, restoring power to the venue. With the electricity\nback on, the lights illuminated the room, allowing every-\none to see and continue dancing to the band\u2019s performance.\nComment: Starting with the crowd-written explanation\nand refining it with an LLM results in plausible explana-\ntions that include sufficient details to connect the context\nand outcome.\nFigure 2: Qualitative comparison between LLM expla-\nnations, Crowd explanations, and C+LLM explanations.\nIn Comments, we make connections to the three rules in\nexplanation writing.\nof 1 or 2. Significantly more outcomes are rated 1\nor 2 in un-RocStories (70.6% of outcomes) and un-\nSocialIQA (49.0% of human-written and 74.1% of\nLLM-generated outcomes). Compared to \u03b1NLG,\nUNCOMMONSENSE poses a unique challenge of\nabductive reasoning about uncommon outcomes.\nExplanation Preferences.\nWe first compare pair-\nwise preferences of LLM explanations versus\nCrowd, C+LLM, and LLM2 explanations. We ran-\ndomly sample 100 context-outcome pairs from\neach UNCOMMONSENSE test set, and select the\nsame explanation from LLM that was randomly\nchosen to be enhanced into LLM2. We then ran-\ndomly sample a single crowd-written explanation\nfor each pair from Crowd, along with its enhanced\ncounterpart in C+LLM. This selection procedure\nallows us to directly compare the effect of applying\nLLM-based enhancement to both crowd-written\nand LLM-generated explanations.\nWe recruit crowdworkers who provided qual-\nity explanations during data collection to provide\npairwise preferences between Crowd, C+LLM, and\nLLM2 explanations with LLM explanations based\non the same rules used for the explanation-writing\ntask (Section 2.2).7 Raters can select one of the\ntwo explanations as better, or can mark ties be-\n7Figure 12 in the appendix shows the MTurk preference\nevaluation template.\nun-SocialIQA\nun-RocStories\nCrowd\nC+LLM\nLLM2\nCrowd\nC+LLM\nLLM2\nWin\n20\n55\n36\n11\n30\n20\nEql. good\n22\n26\n52\n28\n43\n58\nEql. bad\n5\n1\n1\n3\n2\n4\nLose\n53\n18\n11\n58\n25\n18\nFleiss\u2019 \u03ba\n0.47\n0.47\n0.55\n0.48\n0.43\n0.53\nTable 4: Preference judgments given by crowdworkers comparing explana-\ntions from LLM with explanations from Crowd, C+LLM, and LLM2.\nl\nun-SocialIQA\nun-RocStories\n2\n77.35\n71.63\n1\n90.90\n75.85\nTable 5: Non-lose rates of C+LLM\nversus LLM, broken down by the like-\nlihoods l of outcomes (l = 2 is more\nlikely, and l = 1 is less likely).\ntween the two as equally bad or equally good. Ta-\nble 4 shows that Crowd explanations are least often\npreferred and C+LLM explanations are the most\npreferred. While LLM can improve via LLM-based\nenhancement, these explanations are still less pre-\nferred when compared to C+LLM. Additionally,\nC+LLM has a higher lose rate to LLM2; we hy-\npothesize that it is easier to determine a winner /\nloser between C+LLM and LLM, but it is less obvi-\nous to judge between LLM and LLM2. Therefore,\nLLM2 often produces explanations that are equally\ngood / bad but wins / loses less frequently. Finally,\nwe include the Fleiss\u2019 \u03ba score to demonstrate the\ninter-annotator agreement rate between workers,\nwhere they all fall within the range from 0.40 to\n0.60.8 Figure 2 compares explanations generated\nby LLM, Crowd, and C+LLM for an example in\nun-RocStories. Table 5 presents the non-lose rates\nof C+LLM explanations against LLM explanations\nbroken down by likelihoods.9 C+LLM explana-\ntions are preferable as the likelihood of outcomes\nare less likely.\nQuantitative Comparison of Explanations.\nWe\ninvestigate several distributional differences across\nthe four sources of explanations. Figure 3 shows\nthe distribution of explanation lengths.10 Crowd\nexplanations are significantly shorter than LLM,\nwith an average length of 22.9 \u00b1 11.3 tokens per\nexplanation in un-RocStories and 22.0 \u00b1 11.9 in\nun-SocialIQA, compared to an average of 38.2\n\u00b1 9.9 and 25.5 \u00b1 7.1 respectively for LLM. How-\never, enhancing crowd-written explanations with\nan LLM significantly increases their lengths over\nLLM: C+LLM has an average explanation length\nof 78.0 \u00b1 24.4 tokens in un-RocStories and 78.3 \u00b1\n8Our preference-based ranking is a four-way classification.\nEven though scores between 0.40 and 0.60 are considered\nmoderate agreement for the two-class case, it is more chal-\nlenging to achieve these scores in the four-class case.\n9The 100 test examples considered here only contain a\nsignificant number of outcomes with likelihoods l = 1, 2.\n10We use nltk.wordpunct_tokenize (Bird et al.,\n2009) for tokenizing explanations.\n50\n100\n150\n200\n0\n2\n4\n6\n% of explanations\n(un-RocStories)\nLLM\nLLM2\nCrowd\nC+LLM\n50\n100\n150\n200\n0\n2\n4\n6\n8\nExplanation length (# tokens)\n% of explanations\n(un-SocialIQA)\nFigure 3: Distribution of explanation lengths in un-\nRocStories (top) and un-SocialIQA (bottom), computed\non the development sets of each data subset.\n23.5 in un-SocialIQA. This pattern does not hold\nfor LLM-based enhancement of LLM-generated\nexplanations: LLM2 has average lengths of 35.6\n\u00b1 10.8 and 25.9 \u00b1 6.7 respectively, not significantly\ndifferent from LLM. Therefore, length of the expla-\nnations produced by C+LLM can vary significantly.\nIn Figure 4, we investigate the entropy of the\ndistribution of n-grams from n \u2208 {1, . . . , 5} across\nthe different sources of explanations.11 We use\nentropy as a measure of lexical diversity (Jung et al.,\n2023). We find trends similar to the analysis of\nexplanation lengths: while Crowd has generally\nlower entropy than LLM, LLM enhancement of\ncrowd-written explanations results in significantly\nhigher entropy (C+LLM), while it has no effect on\nLLM-generated explanations (LLM2). Therefore,\nC+LLM results in the highest lexical diversity in\nexplanation writing.\n11As different data sources contain a different number of\nexplanations per context-outcome pair, we compute entropy\nusing 1,000 iterations of bootstrap sampling of one explana-\ntion per context-outcome pair in each data subset.\n1 2 3 4 5\n0.6\n0.7\n0.8\n0.9\nn\nn-gram entropy\nun-RocStories\n1 2 3 4 5\nn\nun-SocialIQA\nLLM\nLLM2\nCrowd\nC+LLM\nFigure 4: Entropies of n-gram distributions in un-\nRocStories (left) and un-SocialIQA (right), computed\non the development sets of each data subset.\n4\nImitation Learning for Abductive\nReasoning\nExisting methods for abductive reasoning focus\non performing supervised fine-tuning (SFT) with a\nstatic dataset (Bhagavatula et al., 2020; Rudinger\net al., 2020). Training using static demonstration\ndata is vulnerable to exposure bias: during training,\nthe model learns to predict the next token in an\nexplanation conditioned on a gold-standard prefix;\nhowever, when the model generates an entirely new\nexplanation during inference, it is conditioned on\nits own previously generated tokens. This incon-\nsistency between training and inference procedures\nleads to error propagation at inference time, and a\nreduction in the quality of explanations. To address\nthis issue, we experiment with several on-policy\nimitation learning algorithms.\n4.1\nBackground: Imitation Learning\nIn the task of abductive reasoning, a policy \u03c0\nmaps from the context x, an outcome y, and the\nprefix sequence of an explanation z to a distri-\nbution over the output vocabulary. Explanations\nare generated token-by-token, with the jth token\nzj \u223c \u03c0(\u00b7 | x, y, z:j\u22121), and the entire explanation\nsampled from \u03c0 as z \u223c \u03c0(\u00b7 | x, y).\nLet \u03c0\u2217(\u00b7) be an expert policy and \u03c0\u03b8(\u00b7) be a\nlearner policy with parameters \u03b8. The objective\nof imitation learning is to find parameters \u03b8 that\nresult in the learner policy assigning high probabil-\nities to expert demonstrated explanations.\nBehavior Cloning (BC).\nBC uses expert demon-\nstrations D = {(x, y, z)}N and a supervised learn-\ning objective that train a learner policy to maximize\nthe probability of expert demonstrations. Existing\nmethods of using SFT is a type of behavior cloning.\nA drawback of BC is the aforementioned exposure\nAlgorithm 1 EaO: Using expert as an oracle.\n1: Inputs: Initial learner policy parameters \u03b80, expert policy\n\u03c0\u2217(\u00b7), dataset D = {(x, y)}N, block size k, initial prefix\nsize b, number of training epochs I.\n2: \u02dcD \u2190 \u2205\n3: for i = 0, . . . , I \u2212 1 do\n4:\nfor (x, y) \u2208 D do\n5:\n\u02dcz \u223c \u03c0\u03b8i(\u00b7 | x, y)\n6:\nz\u2217 \u223c \u03c0\u2217(\u00b7| x, y, \u02dcz:b)\n7:\n\u02dcD \u2190 \u02dcD \u222a {(x, y, \u02dcz:bz\u2217)}\n8:\nend for\n9:\n\u03b8i+1 \u2190 \u03b8i further optimized on \u02dcD with supervised\nlearning.\n10:\nb \u2190 b + k\n11: end for\n12: Returns: Learned policy parameters \u03b8I.\nbias problem; as a result, errors are more likely to\npropagate during inference, where the learner fails\nto recover from its own mistakes, as it was never\nexposed to these mistakes during training.\nOnline Learning.\nTo address the exposure bias\nproblem for sequence prediction tasks, Ross et al.\n(2011) propose DAgger, where an expert policy is\nused at training time to provide oracle continua-\ntions to learner-generated prefixes. The learner pol-\nicy is then optimized to maximize the probability\nof oracle continuations, conditioned on sequence\nprefixes generated by the learner. DAgger and its\nvariants have been used in a variety of language-\nrelated tasks, including dependency parsing (Gold-\nberg and Nivre, 2012), instruction following (An-\nderson et al., 2017), and language generation (Lin\net al., 2020a).\n4.2\nImitation Learning for Abductive\nReasoning\nWe explore two online imitation learning ap-\nproaches that assume different levels of access to an\nexpert policy, which is in our case a top-performing\nLLM. First, we assume access to the expert policy\nat any point during training, which allows us to use\nit as an oracle. Next, we consider a setting where\nthe expert may not be available at training time\n(e.g., for cost reasons), and we only have a static\nset of expert demonstrations.\nEaO: Using expert as an oracle on-line.\nAl-\ngorithm 1 formalizes our DAgger-inspired algo-\nrithm, which we call \"Expert as Oracle\" (EaO). We\ntrain with I total epochs over the training dataset\nD = (x, y)N. Throughout learning, we maintain a\ntraining dataset \u02dcD containing examples of contexts\nand outcomes paired with explanations aggregated\nduring each epoch. In each epoch i, and for each\nexample (x, y), we use the current learner parame-\nters \u03b8i to sample an explanation \u02dcz. Using a prefix\nb of a fixed size, we then sample a continuation of\n\u02dcz:b using the expert policy \u03c0\u2217. Finally, we add an\nexample to \u02dcD that concatenates the first b tokens of\nthe learner\u2019s sample with the expert\u2019s completion.\nAfter aggregating examples for the epoch, we apply\nsupervised training on examples in \u02dcD to acquire\nupdated parameters \u03b8i+1. After each epoch, we in-\ncrease the length of the prefix generated by learner\npolicy b by a fixed block size k.\nSED: Using only static expert demonstrations.\nFor the setting where we have access only to a static\nset of expert demonstrations, we propose an online\nlearning algorithm that similarly aims to avoid the\nexposure bias problem of behavior cloning.12\nWe modify the loss function of behavior cloning,\nwhich maximizes the probability of expert demon-\nstration z, by adding two terms: (a) a term that\nminimizes the probability of explanations gener-\nated by the learner policy during training \u02dcz; and (b)\nthe KL divergence from initial policy for stabiling\nthe training process (Schulman et al., 2017). For-\nmally, after sampling \u02dcz for each instance at each\niteration from the current policy, we optimize:\nL(\u03b8) = 1\nN\nX\n(x,y,z,\u02dcz)\u2208 \u02dc\nD\nn\n\u2212 log \u03c0\u03b8(z|x, y) + \u03bb log \u03c0\u03b8(\u02dcz|x, y)\n+ \u03b2KL (\u03c0\u03b80(\u00b7|x, y, z<t)||\u03c0\u03b8i(\u00b7|x, y, z<t))\no\n(1)\n5\nExperiments\nEvaluation.\nWe evaluate the proposed imitation\nlearning methods with three sets of metrics. We fo-\ncus on preference-based pairwise evaluation judged\nby humans.13 We report performance on the same\n100 randomly-sampled examples.14 In Appendix C,\nwe report two additional sets of metrics: (a) hu-\nman judgements on seven binary questions (e.g.,\nis the outcome more likely given the context and\nthe explanation than given the context alone?) that\nevaluate different failure modes, and (b) a number\nof reference-based automatic evaluation metrics,\ne.g. BERTScore (Zhang et al., 2020b).\n12Full pseudocode is in Appendix G.\n13For simplicity, in this evaluation, we report equally good\nand equally bad as the same category (Tie).\n14We will maintain a leaderboard that provides human eval-\nuation of these examples on model-generated explanations for\ntwo years. We will also maintain the same human annotator\npool to increase reproducibility and ensure fairness.\nBase models.\nAs baselines, we experiment with\n3-shot prompting with GPT-3 (Brown et al., 2020)\nand, following the state-of-the-art approach for\ncommonsense abductive reasoning (Khashabi et al.,\n2022),15 supervised fine-tuning (SFT) on several\nopen and accessible language models: FlanT5-\nXXL (Chung et al., 2022), LLaMA-7B (Touvron\net al., 2023), and GPT-2-XL (Radford et al., 2019).\nTo compare the benefit of different sources of train-\ning data, we perform SFT on explanations in the\ntraining sets of LLM (LLM-generated explanations)\nand C+LLM (LLM-enhanced crowd-written expla-\nnations). Because Crowd (crowd-written explana-\ntions) are the least preferred subset in UNCOM-\nMONSENSE, we do not fine-tune on them. Ap-\npendix F contains additional experimental details.\nCan imitation learning improve a given model?\nWe apply our proposed imitation learning algo-\nrithms, EaO and SED, to GPT-2-XL as the initial\nlearner policy. This is the weakest (but most com-\nputationally accessible) base language model of the\nthree we consider for SFT. This choice is purpose-\nful, as our experiment intends to assess whether\nimitation learning can improve a given LM. For a\nfair comparison, we use the same the same expert\npolicy (GPT-4) for both EaO and SED. In addi-\ntion to uncommonsense benchmarks, we also con-\nsider also report performance on two commonsense\nbenchmarks, \u03b1NLG (Bhagavatula et al., 2020) and\nSense-making (Wang et al., 2019) to see if our re-\nsults generalize.\n5.1\nResults\nBaselines.\nTable 6 shows the performance of the\nbaseline systems. Unsurprisingly, explanations gen-\nerated from few-shot GPT-3 are rarely preferred\nby GPT-4 to those GPT-4 itself generated (13% of\nthe time). However, GPT-3 also underperforms the\n25x smaller (but supervised fine-tuned) LLaMA-\n7B (48% non-lose rate vs. GPT-4) and 16x smaller\nFlanT5-XLL (44% non-lose rate vs. GPT-4). In\naddition, having C+LLM to be supervision some-\ntimes leads to better performance than using LLM\nas supervision but in other times hurts. We hypoth-\nesize that despite LLM explanations being worse\nthan C+LLM explanations, they are easier for the\nsmall models to learn. Finally, all methods but one\n15While Khashabi et al. (2022) showed results with super-\nvised fine-tuning on T5-11B (Raffel et al., 2020), Chung et al.\n(2022) later showed that the instruction-tuned version of this\nmodel, FlanT5-XXL, is expected to outperform T5-11B in\nmost settings.\nun-RocStories\nun-SocialIQA\nSupervision\nBase Model\nWin\nTie\nLose\nWin\nTie\nLose\n3-shot prompting\nGPT-3\n13\n20\n67\n33\n13\n54\nSFT with LLM\nGPT-2-XL\n6\n22\n72\n7\n44\n49\nLLaMA-7B\n13\n35\n52\n25\n38\n37\nFlanT5-XXL\n16\n28\n56\n16\n47\n37\nSFT with C+LLM\nGPT-2-XL\n6\n26\n64\n13\n32\n55\nLLaMA-7B\n21\n31\n48\n19\n39\n42\nFlanT5-XXL\n11\n32\n57\n27\n32\n41\nTable 6: Experimental comparison of GPT-3 using few-shot prompting, and SFT with two sources of training\nexplanations on three different base models. We perform pairwise preference-based evaluation with GPT-4,\ncomparing explanations generated by each system with explanations in the test set of LLM.\nWin\nTie\nLose\nun-RocStories\nSFT\n6\n22\n72\nSED\n12\n24\n64\nEaO\n17\n16\n67\nun-SocialIQA\nSFT\n7\n44\n49\nSED\n9\n34\n57\nEaO\n13\n39\n48\n\u03b1NLG\nSFT\n13\n20\n67\nSED\n14\n23\n63\nEaO\n14\n23\n63\nSen-Making\nSFT\n12\n41\n47\nSED\n13\n49\n38\nEaO\n13\n52\n35\nTable 7: Comparison between different imitation learn-\ning methods using pairwise preference-based evaluation\non the test set of LLM. For \u03b1NLG and Sen-Making,\nwe compare against GPT-4-generated explanations for\ncontexts and outcomes included in the original test sets.\nstill lose to LLM explanations, indicating that SFT\nis still insufficient for improving to the performance\nof the best existing LLMs.\nImitation Learning.\nTable 7 shows the perfor-\nmance comparing SFT with the two imitation learn-\ning methods, SED and EaO, on four datasets when\nusing GPT-2-XL as the base moddel. On both UN-\nCOMMONSENSE and commonsense benchmarks,\nboth SED and EaO show strong improvements\nagainst SFT by reducing the rates of losing to LLM\nexplanations or by increasing the win rates. Ex-\ncept for \u03b1NLG, EaO, which trains using expert\nonline corrections to learner-generated sequence\nprefixes, shows more promise than SED. However,\nSED, which is no more costly than SFT, can still\nsignificantly improve the performance of the weak-\nbut-accessible base model GPT-2-XL on both com-\nmonsense and uncommonsense abductive reason-\ning except on un-SocialIQA.\n6\nRelated Work\n\u03b1NLG (Bhagavatula et al., 2020) is the most\nclosely related task to UNCOMMONSENSE: both\nrequire generating explanations to bridge contexts\nand outcomes (except \u03b1NLG focuses on common,\neveryday scenarios). d-NLI (Rudinger et al., 2020)\nconsider a related task of generating an explanation\nexplanation that weakens an outcome. Additional\nworks cover methods for generating explanations,\ne.g., Du et al. (2022), Zhou et al. (2021), Wang\net al. (2019), Zhang et al. (2020a), inter alia.\nReasoning about uncommon but possible sce-\nnarios has been studied in other settings. Arnaout\net al. (2022) propose a method for identifying in-\nformative negations about everyday concepts in\nlarge-scale commonsense knowledge bases. Tang\net al. (2023) present a decoding method for produc-\ning less plausible explanations for everyday events.\nCollins et al. (2022) create a small-scale bench-\nmark containing about 800 curated uncommon\nstatements, along with explanations making sense\nof these statements. UNCOMMONSENSE differs in\nstructure and focus from these prior works. Finally,\nTODAY (Feng et al., 2023) proposes a temporal\nreasoning task to study the order of two events.\nAtypical order of two events could be uncommon,\nand justifying the order is uncommonsense rea-\nsoning. Because UNCOMMONSENSE is not built\nfrom reversing the order of temporal events, it en-\ncompasses a different set of uncommon situations,\nincluding social reasoning, cultural reasoning, and\nphysical reasoning. With each situation, UNCOM-\nMONSENSE also contains more than one explana-\ntion, collected from both crowd workers and GPT-4.\nWe summarize the differences between UNCOM-\nMONSENSE and existing datasets in Figure 8.\nTask\nUncommon?\nSources of Explanations\n# of Explanations\n\u03b1NLG\nN\nCrowd workers\n76k\nd-NLI\nN\nCrowd workers\n200k\nArnaout et al.\nN\nVariants of BERT models\nN/A\nTODAY\nY\nCrowd workers\n2.2k\nCollins et al.\nY\nCrowd workers\n0.8k\nUNCOMMONSENSE\nY\nCrowd workers and GPT-4\n41k\nTable 8: Summary of the differences between the proposed dataset and the existing datasets.\n7\nConclusion\nWe propose a new task, uncommonsense abductive\nreasoning, designed to assess the ability of NLP\nsystems to reason about uncommon scenarios in ab-\nductive reasoning tasks. We explore two imitation\nlearning methods to improve accessible language\nmodels on uncommonsense abductive reasoning.\nExperiments show that access to expert behavior,\nparticularly when using the expert as an oracle in\nonline training, significantly improves the explana-\ntion quality of smaller models.\n8\nLimitations\nWhile our dataset offers advantages over existing\nsources, we acknowledge the following limitations.\nFirst, our dataset may suffer from social biases in\nthe data collection process, and the labeling pro-\ncess may contain errors and inconsistencies. De-\nspite best efforts to ensure high-quality annotations,\noccasional human errors are possible. Additionally,\nour dataset only contains uncommon situations in\nEnglish and thus lack of language diversity. Fi-\nnally, our main preference-based evaluation relies\non human evaluators, which can be less producible\nand costly. There is thus a large room for improve-\nment for more effective and affordable evaluation\nmethods.\nReferences\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce,\nMark Johnson, Niko S\u00fcnderhauf, Ian D. Reid,\nStephen Gould,\nand Anton van den Hengel.\n2017. Vision-and-language navigation: Interpreting\nvisually-grounded navigation instructions in real envi-\nronments. 2018 IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3674\u20133683.\nHiba Arnaout, Simon Razniewski, Gerhard Weikum,\nand Jeff Z Pan. 2022. Uncommonsense: Informative\nnegative knowledge about everyday concepts. In Pro-\nceedings of the 31st ACM International Conference\non Information & Knowledge Management, pages\n37\u201346.\nChandra Bhagavatula, Ronan Le Bras, Chaitanya\nMalaviya, Keisuke Sakaguchi, Ari Holtzman, Han-\nnah Rashkin, Doug Downey, Wen tau Yih, and Yejin\nChoi. 2020. Abductive commonsense reasoning. In\nInternational Conference on Learning Representa-\ntions.\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Nat-\nural language processing with Python: analyzing text\nwith the natural language toolkit. \" O\u2019Reilly Media,\nInc.\".\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nKatherine M Collins, Catherine Wong, Jiahai Feng,\nMegan Wei, and Josh Tenenbaum. 2022. Structured,\nflexible, and robust: benchmarking and improving\nlarge language models towards more human-like be-\nhavior in out-of-distribution reasoning tasks. In Pro-\nceedings of the Annual Meeting of the Cognitive Sci-\nence Society, volume 44.\nLi Du, Xiao Ding, Kai Xiong, Ting Liu, and Bing Qin.\n2022. e-CARE: a new dataset for exploring explain-\nable causal reasoning. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 432\u2013446,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nYu Feng, Ben Zhou, Haoyu Wang, Helen Jin, and Dan\nRoth. 2023. Generic temporal reasoning with dif-\nferential analysis and explanation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12013\u201312029, Toronto, Canada. Association\nfor Computational Linguistics.\nYoav Goldberg and Joakim Nivre. 2012. A dynamic ora-\ncle for arc-eager dependency parsing. In Proceedings\nof COLING 2012, pages 959\u2013976, Mumbai, India.\nThe COLING 2012 Organizing Committee.\nJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman,\nXiming Lu, Jillian Fisher, Taylor Sorensen, and Yejin\nChoi. 2023. Impossible distillation: from low-quality\nmodel to high-quality dataset and model for summa-\nrization and paraphrasing.\nDaniel Khashabi, Gabriel Stanovsky, Jonathan Bragg,\nNicholas Lourie, Jungo Kasai, Yejin Choi, Noah A.\nSmith, and Daniel Weld. 2022. GENIE: Toward re-\nproducible and standardized human evaluation for\ntext generation. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 11444\u201311458, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nAlexander Lin, Jeremy Wohlwend, Howard Chen, and\nTao Lei. 2020a. Autoregressive knowledge distilla-\ntion through imitation learning. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6121\u20136133,\nOnline. Association for Computational Linguistics.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020b. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823\u20131840,\nOnline. Association for Computational Linguistics.\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong\nHe, Devi Parikh, Dhruv Batra, Lucy Vanderwende,\nPushmeet Kohli, and James Allen. 2016a. A cor-\npus and cloze evaluation for deeper understanding of\ncommonsense stories. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 839\u2013849, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nNasrin Mostafazadeh, Alyson Grealish, Nathanael\nChambers, James Allen, and Lucy Vanderwende.\n2016b.\nCaTeRS: Causal and temporal relation\nscheme for semantic annotation of event structures.\nIn Proceedings of the Fourth Workshop on Events,\npages 51\u201361, San Diego, California. Association for\nComputational Linguistics.\nOpenAI. 2023. Gpt-4 technical report.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nSt\u00e9phane Ross, Geoffrey Gordon, and Drew Bagnell.\n2011. A reduction of imitation learning and struc-\ntured prediction to no-regret online learning. In Pro-\nceedings of the fourteenth international conference\non artificial intelligence and statistics, pages 627\u2013\n635. JMLR Workshop and Conference Proceedings.\nRachel Rudinger, Vered Shwartz, Jena D. Hwang, Chan-\ndra Bhagavatula, Maxwell Forbes, Ronan Le Bras,\nNoah A. Smith, and Yejin Choi. 2020. Thinking like\na skeptic: Defeasible inference in natural language.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 4661\u20134675, Online.\nAssociation for Computational Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463\u2013\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal policy\noptimization algorithms. CoRR, abs/1707.06347.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLiyan Tang, Yifan Peng, Yanshan Wang, Ying Ding,\nGreg Durrett, and Justin F Rousseau. 2023. Less\nlikely brainstorming: Using language models to\ngenerate alternative hypotheses.\narXiv preprint\narXiv:2305.19339.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan\nLi, and Tian Gao. 2019. Does it make sense? and\nwhy? a pilot study for sense making and explana-\ntion. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4020\u20134026, Florence, Italy. Association for Compu-\ntational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nHongming Zhang, Xinran Zhao, and Yangqiu Song.\n2020a. WinoWhy: A deep diagnosis of essential\ncommonsense knowledge for answering Winograd\nschema challenge. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5736\u20135745, Online. Association\nfor Computational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020b. BERTScore:\nEvaluating text generation with BERT. In Interna-\ntional Conference on Learning Representations.\nPei Zhou, Pegah Jandaghi, Hyundong Cho, Bill Yuchen\nLin, Jay Pujara, and Xiang Ren. 2021. Probing com-\nmonsense explanation in dialogue response genera-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 4132\u20134146,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nA\nQualitative Analysis of Outcomes\nTable 9 presents example outcomes of different\nlikelihood scores.\nB\nProcessing Outcomes in SocialIQA\nWe use three types of questions: what will X want\nto do next, what will happen to X, and how would\nyou describe X. We do the following steps to con-\nstruct the outcome.\n1. We remove the correct answer choice, and we\nare left with two incorrect answer choices.\n2. We feed GPT3 (text-davinci-03) \u201c{context}\n{question} {answer}\u201d and compute the answer\nprobability p (answer | context, question) and\nchoose the answer that has the lower probabil-\nity.\n3. We prompt ChatGPT to combine the question\nand the answer to be the outcome, in the six-\nshot setting. When we receive a response from\nChatGPT, we check whether the original an-\nswer is in the output, if it doesn\u2019t contain the\nanswer, we send the same prompt to GPT-4.\nIf GPT-4 still fails, we mark the example and\nmanually combine the question and the an-\nswer. Refer to 5 for the combining prompting\ntemplate.\nBecause SocialIQA contains many invalid answer\nchoices, the combining step often fails (e.g., the\nquestion is \u201cwhat will person X do next\u201d, and the\nanswer is \u201csad\u201d), we rely on ChatGPT to detect\nsuch cases. We throw out the examples when Chat-\nGPT refuses to do the combination.\nC\nMore Evaluation\nWe include additional automatic and human evalu-\nation results on baseline models and our proposed\nimitation learning methods, SED and EaO. The\nadditional human evaluation is is a set of seven\nhuman evaluation questions that target different\nfailure modes of generated explanations:\n1. Is the explanation relevant to the context? (de-\nnoted as relevance x)\n2. Is the explanation relevant to the outcome?\n(denoted as relevance y)\n3. Is the explanation not self-contradictory? (de-\nnoted as consistency z)\n4. Is the explanation not contradictory to the con-\ntext? (denoted as consistency x)\n5. Is the explanation not contradictory to the out-\ncome? (denoted as consistency y)\n6. Is it possible that explanation might occur\n(given the context)? (denoted as plausibility\nz)\n7. Is the outcome more likely given the context\nand the explanation than given the context\nalone? (plausibility y)\nThe results are presented in Table 10 for un-\nRocStories, Table 11 for un-SocialIQA, Table 12\nfor \u03b1NLG, and Table 13 for Sen-Making.\nWe also compute BERTScore, ROUGE-L, ME-\nTEOR, SacreBLEU, and BLEURT for each method\nand report the results in Table 14 for un-RocStories,\nTable 15 for un-SocialIQA, Table 16 for \u03b1NLG,\nand Table 17 for Sen-Making.\nD\nTemplates\nWe include the following prompting templates:\n\u2022 Figure 5: The prompt to combine a question\nand its answer into a single sentence on un-\nSocialIQA with five demonstrations.\nl\nContext\nOutcome\n4\nKate and Greg went to a little candy shop together. They looked\naround at their options and made their choice. They went up\nto the cashier and said what they wanted. The cashier, with\nunwashed hands, bagged the candy without gloves.\nKate and Greg licked the candy gleefully.\n3\nI went to the post office yesterday. It took a while to get there\nsince it\u2019s on the other side of town. Once I got there I mailed my\nletters and headed home. It\u2019s always easier to get home than to\nget somewhere.\nI could not find my way back from the post office.\n2\nMy niece just got engaged. She is Chinese and her fiance is\nCaucasian. Her parents had them over for a home cooked meal.\nThe fiance got nausea from the unfamiliar dishes and had to\nleave.\nMy niece was thrilled that her fiance was sick.\n1\nJosh woke up early to get ready for the hike he had been planning.\nAfter a shower, he made sure all his supplies were packed. He\nleft his house and drove to the park where he was going hiking.\nBecause it was early in the day Josh had the trail mostly to\nhimself.\nJosh loathed the outdoors.\n4\nJordan finished their test so fast and still got an A plus as always.\nOther students will be jealous.\n3\nSkylar gave Robin the permission to eat cake after Robin caused\nsome trouble.\nRobin will want to refuse to eat the cake.\n2\nAustin brought tears to Tracy\u2019s eyes when he brought her flowers.\nAustin will be hated.\n1\nCarson threw beer in Kendall\u2019s face during a heated argument\nwith her.\nCarson will receive a medal for their behavior.\nTable 9: Example outcomes of different likelihood scores l \u2208 {4, 3, 2, 1}.\nConsistency\nRelevance\nPlausibility\nSupervision\nModel\nx\ny\nz\nx\ny\nz\ny\n3-shot prompting\nGPT-3\n76\n92\n97\n100\n97\n74\n75\nGPT-4\n94\n91\n99\n98\n95\n90\n85\nSFT with LLM\nLlaMA-7B\n89\n92\n98\n98\n95\n89\n80\nFlanT5-XXL\n80\n93\n94\n96\n93\n74\n58\nGPT-2-XL\n88\n92\n92\n97\n94\n80\n83\nSED with LLM\nGPT-2-XL\n78\n87\n90\n94\n94\n66\n77\nEoA with LLM\nGPT-2-XL\n97\n94\n94\n100\n94\n88\n86\nTable 10: Fine-grained human evaluation on un-RocStories.\n\u2022 Figure 6: The prompt to generate improbable\nanswers on un-SocialIQA with six demonstra-\ntions.\n\u2022 Figure 7: The prompt to estimate the outcome\nlikelihood given the context.\n\u2022 Figure 8: The prompt to generate explanations\non un-SocialIQA with three demonstrations.\n\u2022 Figure 9: The prompt to generate explanations\non un-RocStories with three demonstrations.\n\u2022 Figure 10: The prompt to improve a crowd-\nwritten explanation.\nWe also include the following MTurk templates:\n\u2022 Figure 11: The template to collect crowd-\nwritten explanations.\n\u2022 Figure 12: The template to collect human pref-\nerences.\nE\nCrowdsourcing Details\nTasks which are allocated to a worker but not com-\npleted are later distributed to the entire group of\nworkers. We allow workers at least a week to com-\nplete each of their allocated tasks, which allows\nthem sufficient time to complete the task and work\nat their own pace.\nConsistency\nRelevance\nPlausibility\nSupervision\nModel\nx\ny\nz\nx\ny\nz\ny\nFew-shot prompting\nGPT-3\n93\n92\n99\n100\n94\n93\n84\nGPT-4\n98\n92\n100\n99\n97\n95\n90\nSF with LLMT\nLlaMA-7B\n93\n94\n100\n99\n97\n90\n88\nFlanT5-XXL\n94\n91\n97\n96\n96\n88\n80\nGPT-2-XL\n96\n95\n97\n98\n98\n91\n91\nSED with LLM\nGPT-2-XL\n94\n84\n97\n97\n85\n92\n73\nEoA with LLM\nGPT-2-XL\n91\n95\n97\n97\n94\n87\n88\nTable 11: Fine-grained human evaluation on un-SocialIQA.\nConsistency\nRelevance\nPlausibility\nSupervision\nModel\nx\ny\nz\nx\ny\nz\ny\nFew-shot prompting\nGPT-3\n100\n97\n100\n99\n96\n98\n94\nGPT-4\n99\n98\n99\n99\n98\n99\n97\nSFT with LLM\nLlaMA-7B\n99\n97\n99\n95\n97\n98\n91\nFlanT5-XXL\n95\n92\n95\n93\n81\n96\n85\nGPT-2-XL\n96\n92\n98\n93\n97\n94\n85\nSED with LLM\nGPT-2-XL\n97\n91\n99\n95\n93\n96\n84\nEoA with LLM\nGPT-2-XL\n97\n95\n98\n97\n97\n98\n90\nTable 12: Fine-grained human evaluation on \u03b1NLG.\nCombine the following question and answer into a\nsentence: What will Others want to do next? quit their job\nand start their own business.\nOthers will want to quit their job and start their own\nbusiness.\nCombine the following question and answer into a\nsentence: How would you describe Remy? selfish\nRemy is selfish.\nCombine the following question and answer into a\nsentence:\nWhat will happen to Quinn?\nthey will\nspontaneously combust\nQuinn will spontaneously combust.\nCombine the following question and answer into a\nsentence: How would you describe Bailey? do not want a\nhealthy pet\nBailey does not want a healthy pet.\nCombine the following question and answer into a\nsentence: How would you describe Carson? like Carson\nwas mean\nCarson is mean.\nCombine the following question and answer into a sen-\ntence: {question} {answer}\nFigure 5: Prompting template for combining a question\nand its answer.\nContext: Sydney walked past a homeless woman asking\nfor change but did not have any money they could give to\nher. Sydney felt bad afterwards.\nQuestion: How would you describe Sydney?\nAn unlikely answer: ridiculous\nContext: Jesse set Robin\u2019s suitcase on fire after their fight\nand messy breakup.\nQuestion: What will Jesse want to do next?\nAn unlikely answer: decide not to reconcile\nContext: Bailey asked Sasha\u2019s grandma about church\nbecause they wanted to know more about it.\nQuestion: What will happen to Sasha?\nAn unlikely answer: they get yelled by Sasha\u2019s grandma\nContext: Bailey told Alex to send the letter overnight\nsince it was important.\nQuestion: How would Alex feel as a result?\nAn unlikely answer: rushed\nContext: Lee made copies so that everyone at the table\ncould follow along.\nQuestion: What will Lee want to do next?\nAn unlikely answer: ask people stop reading the paper\nContext: Taylor gave Kai a lot to think about.\nQuestion: What will happen to Kai?\nAn unlikely answer: not talk to Taylor\nContext: {context}\nQuestion: {question}\nAn unlikely answer:\nFigure 6: Prompting template for generating improbable\nanswers for SocialIQA examples.\nConsistency\nRelevance\nPlausibility\nSupervision\nModel\ny\ny\ny\nFew-shot prompting\nGPT-3\n100\n100\n93\nGPT-4\n100\n100\n99\nSFT with LLM\nLlaMA-7B\n91\n95\n86\nFlanT5-XXL\n85\n98\n84\nGPT-2-XL\n86\n97\n85\nSED with LLM\nGPT-2-XL\n92\n98\n91\nEoA with LLM\nGPT-2-XL\n87\n95\n83\nTable 13: Fine-grained human evaluation on Sen-making.\nSupervision\nModel\nBERTScore\nROUGE\nMETEOR\nSacreBLEU\nBLEURT\n3-shot prompting\nGPT-4\n90.79\n30.43\n29.79\n5.16\n-21.74\nSFT with LLM\nGPT2-XL\n90.01\n26.67\n24.17\n3.44\n-35.22\nSED with LLM\nGPT2-XL\n89.76\n26.08\n22.64\n2.94\n-40.04\nEaO with LLM\nGPT2-XL\n89.91\n26.79\n25.72\n3.94\n-30.32\nTable 14: Automatic evaluation on un-RocStories.\nA: {context}\nB: {outcome}\nOn the scale from 1 to 5, how likely is B given A?\nFigure 7: Prompting template for estimating the likeli-\nhood of the outcome given the context.\nContext:\nCameron decided to have a barbecue and\ngathered her friends together.\nOutcome: Others feel bored and uninterested.\nExplanation of the outcome: Other than eating the food,\nthere weren\u2019t other activities planned.\nContext: Jan needed to give out jobs for an upcoming\nproject at work.\nOutcome: Others will take a nap instead of working.\nExplanation of the outcome: The others don\u2019t get paid\nmore for doing the jobs Jan gave out.\nContext: Remy was an expert fisherman and was on the\nwater with Kai. Remy baited Kai\u2019s hook.\nOutcome: Remy will eat a sandwich.\nExplanation of the outcome: It\u2019s been too long before\nthey feel the weight of a fish, and Remy is hungry.\nContext: {context}\nOutcome: {outcome}\nExplanation of the outcome:\nFigure 8: Prompting template for generating explana-\ntions for un-SocialIQA examples.\nContext: My friends all love to go to the club to dance.\nThey think it\u2019s a lot of fun and always invite. I finally\ndecided to tag along last Saturday. I danced terribly and\nbroke a friend\u2019s toe.\nOutcome: My friends decided to keep inviting me out as I\nam so much fun.\nExplanation of the outcome: My friends thought the way\nI dance is really funny and they couldn\u2019t stop laughing.\nContext: On the fourth of July, Lilly baked a lemon\nblueberry cake. She brought it to her boyfriend\u2019s house\nand they had a bbq. After dinner they drove into the city\nto watch fireworks. When the show was over, they got\ndonuts from a food truck.\nOutcome: Lilly had a terrible date.\nExplanation of the outcome:\nLilly\u2019s boyfriend kept\ncomplaining that the donuts were way better than the\nlemon blueberry cake she made, and her boyfriend just\nthrew the cake away.\nContext: Jennifer was bored one Saturday. She decided to\nalleviate her boredom with a hike. She drove to a national\npark to go hiking. Jennifer hiked for hours.\nOutcome: Jennifer thought hiking was stupid.\nExplanation of the outcome: She realized the Saturday\nwas a holiday, and the hiking trails in the national park\nwere too crowded that it took her longer than usual to\nfinish.\nContext: {context}\nOutcome: {outcome}\nExplanation of the outcome:\nFigure 9: Prompting template for generating explana-\ntions for un-RocStories examples.\nSupervision\nModel\nBERTScore\nROUGE\nMETEOR\nSacreBLEU\nBLEURT\n3-shot prompting\nGPT-4\n90.79\n30.43\n29.79\n5.16\n-21.74\nSFT with LLM\nGPT2-XL\n90.01\n26.67\n24.17\n3.44\n-35.22\nSED with LLM\nGPT2-XL\n89.76\n26.08\n22.64\n2.94\n-40.04\nEaO with LLM\nGPT2-XL\n89.91\n26.79\n25.72\n3.94\n-30.32\nTable 15: Automatic evaluation on un-SocialIQA.\nSupervision\nModel\nBERTScore\nROUGE\nMETEOR\nSacreBLEU\nBLEURT\n3-shot prompting\nGPT-4\n92.49\n34.31\n41.58\n6.04\n-31.02\nSFT with LLM\nGPT2-XL\n92.45\n33.52\n37.13\n6.35\n-39.81\nSED with LLM\nGPT2-XL\n92.41\n33.39\n37.05\n6.37\n-39.40\nEaO with LLM\nGPT2-XL\n92.17\n32.21\n37.36\n5.81\n-38.75\nTable 16: Automatic evaluation on \u03b1NLG.\nCan you improve this explanation so that it becomes more\nspecific to the context and makes the outcome more likely\nto happen?\nContext: {context}\nOutcome: {outcome}\nExplanation for the outcome:{explanation}\nFigure 10: Prompting template for improving an expla-\nnation.\nE.1\nQualification.\nWe use a qualification task to recruit and train work-\ners to produce quality explanations of uncommon\noutcomes. In the qualification task, each worker is\nasked to write an explanation for five pre-chosen\ncontexts paired with uncommon outcomes, includ-\ning one pair chosen as an attention check. Three\npaper authors manually grade the explanations to\ncheck if they make the outcomes more likely, nat-\nurally follow the contexts, and leave little infor-\nmation gaps in-between. We qualify workers who\nprovide at least three high-quality explanations, re-\nsulting in qualifying 204 out of 520 workers.\nE.2\nQuality Control for Crowd-written\nExplanations.\nTo ensure the quality of crowd-written explanations,\nwe maintain active communication with workers,\nand detect and filter low-quality explanations. We\nengage with workers through an online group chat\nand periodically provide personalized feedback to\nindividual workers. We detect low-quality expla-\nnations through multiple manual and automatic\nfilters, e.g., checking for contradictions between\nthe worker-written explanation and the context and\noutcome. We dequalify 22 workers who contribute\nmore than two low-quality out of five randomly\nsampled explanations, and remove all of their ex-\nplanations from the dataset.\nAdditionally, we have following automatic ways\nto verify workers\u2019 explanations:\n\u2022 We use GPT3 to check contradiction between\na context and its corresponding explanations.\n\u2022 We use GPT2 to check relevance to the context\nvia p(y|x, z) \u2212 p(y|z) > \u03f5.\n\u2022 In each launch, we sample one explanation\nfrom each worker, and we send individual\nfeedback to the workers who violate our rules\nand filter out the workers who contributed bad\nexplanations to us\n\u2022 We check how many examples are marked\nimpossible to explain by each worker, and re-\nmove workers who use such marks too often.\nF\nExperiment Model Details\nWe implement both the baseline and the proposed\napproaches with Hugging Face Transformers (Wolf\net al., 2020). We train all models with a learn-\ning rate of 0.00001 and a batch size of 8. We\nperform grid search with \u03bb \u2208 {1, 0.1, 0.01} and\n\u03b2 \u2208 {0.1, 0.01, 0.001}, and we choose the best\nperforming checkpoint on the development set. In\nDAgger, we set epochs I to be five and block size\nk to be 2 tokens.\nFigure 11: A screenshot of mturk template for collecting explanations.\nFigure 12: A screenshot of mturk template for doing pair-wise preference evaluation.\nSupervision\nModel\nBERTScore\nROUGE\nMETEOR\nSacreBLEU\nBLEURT\n3-shot prompting\nGPT-4\n91.40\n32.94\n47.80\n4.99\n-16.01\nSFT with LLM\nGPT2-XL\n92.00\n36.74\n47.28\n6.97\n-18.00\nSED with LLM\nGPT2-XL\n91.89\n36.22\n46.58\n5.80\n-18.57\nEaO with LLM\nGPT2-XL\n91.89\n36.07\n47.41\n5.88\n-16.68\nTable 17: Automatic evaluation on Sen-Making.\nAlgorithm 2 Online learning with static expert\ndemonstrations.\n1: Inputs: Initial learner policy parameters \u03b80, dataset D =\n{(x, y, z)}N, number of training epochs I.\n2: \u02dcD \u2190 \u2205\n3: for i = 0, . . . , I \u2212 1 do\n4:\nfor (x, y, z) \u2208 D do\n5:\n\u02dcz \u223c \u03c0(. | x, y)\n6:\n\u02dcD \u2190 \u02dcD \u222a {(x, y, z, \u02dcz)}\n7:\nend for\n8:\n\u03b8i+1 \u2190 \u03b8i further optimized on \u02dcD using the objective\nin Equation 1.\n9: end for\n10: Returns: Learned policy parameters \u03b8I.\nG\nStatic expert demonstrations\npseudo-code\nThe pseduocode for the static expert demonstra-\ntions algorithm introduced in \u00a74.2 is given in Algo-\nrithm 2.\n"
  },
  {
    "title": "PEARL: Personalizing Large Language Model Writing Assistants with Generation-Calibrated Retrievers",
    "link": "https://arxiv.org/pdf/2311.09180.pdf",
    "upvote": "7",
    "text": "PEARL: Personalizing Large Language Model Writing\nAssistants with Generation-Calibrated Retrievers\nSheshera Mysore1\u25b3\u2020\nZhuoran Lu2\u2020 Mengting Wan3\nLongqi Yang3\nSteve Menezes3\nTina Baghaee3\nEmmanuel Barajas Gonzalez3\nJennifer Neville3\nTara Safavi3\u25b3\n1University of Massachusetts Amherst, MA, USA\n2Purdue University, IN, USA\n3Microsoft, WA, USA\n\u25b3Corresponding authors: smysore@cs.umass.edu, tarasafavi@microsoft.com\nAbstract\nPowerful large language models have facili-\ntated the development of writing assistants that\npromise to significantly improve the quality\nand efficiency of composition and communica-\ntion. However, a barrier to effective assistance\nis the lack of personalization in LLM outputs\nto the author\u2019s communication style and spe-\ncialized knowledge. In this paper, we address\nthis challenge by proposing PEARL, a retrieval-\naugmented LLM writing assistant personalized\nwith a generation-calibrated retriever. Our re-\ntriever is trained to select historic user-authored\ndocuments for prompt augmentation, such that\nthey are likely to best personalize LLM gen-\nerations for a user request. We propose two\nkey novelties for training our retriever: 1) A\ntraining data selection method that identifies\nuser requests likely to benefit from personaliza-\ntion and documents that provide that benefit;\nand 2) A scale-calibrating KL-divergence ob-\njective that ensures that our retriever closely\ntracks the benefit of a document for person-\nalized generation. We demonstrate the effec-\ntiveness of PEARL in generating personalized\nworkplace social media posts and Reddit com-\nments. Finally, we showcase the potential of\na generation-calibrated retriever to double as\na performance predictor and further improve\nlow-quality generations via LLM chaining.\nSome of the information in this research paper\nrelates to pre-released content which may be subse-\nquently modified. Microsoft makes no warranties,\nexpress or implied, with respect to the information\nprovided here. This document is provided \u201cas-is\u201d.\nInformation and views expressed in this document,\nincluding URL and other Internet Web site refer-\nences, may change without notice. Some examples\ndepicted herein are provided for illustration only\nand are fictitious. No real association or connec-\ntion is intended or should be inferred. This docu-\nment does not provide you with any legal rights to\n\u2020Work done during internship at Microsoft Research.\nany intellectual property in any Microsoft product.\n\u00a9 2023 Microsoft. All rights reserved.\n1\nIntroduction\nMachine-assisted writing has seen a long history of\ndevelopment. Writing assistants have progressed\nfrom providing simple syntactic checks, to revising\nhuman authored text, to recent assistants being able\nto fully compose texts on direction from authors\n(Mahlow, 2023; Dale and Viethen, 2021). This\nability has largely been driven by large language\nmodels (LLM) and their reasoning, knowledge, and\ntext-generation capabilities. This has led current\nresearch to explore a new frontier of LLM-based\nwriting assistants for complex and important appli-\ncations such as knowledge synthesis (Shen et al.,\n2023), peer review (Chen et al., 2023), and jour-\nnalism (Wang et al., 2023c). More broadly, given\nthe time spent on communication and composition\nin information work, these assistants also promise\nto improve the efficiency and quality of work pro-\nduced by information workers of various expertise\nlevels (Noy and Zhang, 2023; Chui et al., 2023).\nAn important element of effective writing assis-\ntants is being able personalize generated text to\nretain the knowledge, and communicative style of\na user \u2013 an essential element of interpersonal com-\nmunication (Pickering and Garrod, 2013). This\nhas also been found to improve acceptance of gen-\nerated text in various large-scale systems (Chen\net al., 2019; Trajanovski et al., 2021). With cur-\nrent LLMs prone to generating overly generic text\n(Pu and Demberg, 2023), author personalization\nof LLM generations presents an important prob-\nlem for LLM writing assistants. We contribute to\nthis line of work and propose a method for author\npersonalization of LLM writing assistants.\nSpecifically, given a set of historic user-authored\ntexts, we pursue the personalization of large pre-\ntrained LLMs that may only be accessed via\nprompt-based APIs. Designs of LLM-based sys-\narXiv:2311.09180v1  [cs.CL]  15 Nov 2023\nFigure 1: PEARL represents a request-driven generation\nmodel that personalizes LLM generations through re-\ntrieval augmentation with a trained retrieval model.\ntems for various tasks have explored either tun-\ning the input prompt (Salemi et al., 2023; Li\net al., 2023a) or iteratively modifying output gen-\nerations through chaining LLM calls (Wu et al.,\n2022).\nGiven the expense of LLM chaining\nin an interactive writing assistant, we explore\na retrieval-augmented approach to carefully con-\nstruct a prompt for personalized generation.\nOn receiving a user request for generation (Fig-\nure 1), our approach, PEARL, invokes a trained\nretriever to select a salient set of examples from a\nuser\u2019s historical documents that are likely to help\ngenerate a personalized response and adds them as\nfew-shot examples to an LLM\u2019s prompt. Our main\ncontribution lies in training this retriever, for which\nwe introduce a generation-calibrated approach. We\ndefine a score based on an auxiliary text generation\nmodel\u2019s likelihoods that identifies, in training data,\nthe requests likely to benefit from personalization\nand quantifies each context example\u2019s benefit to\nthe text generation task. Then, scores from the\nauxiliary model are distilled into a crossencoder re-\ntriever with a scale-calibrated KL-divergence loss\n(Yan et al., 2022), thereby optimizing it directly for\nthe personalized generation task.\nWe\nevaluate\nPEARL\ninstantiated\nwith\ndavinci-003 and gpt-35-turbo LLMs,\non\nan internal enterprise dataset of workplace commu-\nnications and a public dataset of Reddit comments.\nAcross a suite of text generation metrics and\nmanual evaluations, PEARL outperforms strong\nretrieval augmentation approaches, suggesting\nits unique suitability for personalized writing\nassistance. We also demonstrate how optimizing\nthe retriever directly for the text generation\ntask allows us to use the retriever\u2019s scores for\nperformance prediction.\nThis allows selective\nprocessing of potentially low-quality generations\nby a more complex LLM chain, leading to\nimproved performance. Finally, we contextualize\nour work and conclude with a discussion of the\nimplications of personalized text generation.\n2\nProblem Definition\nWe consider a request-conditional, personalized\ntext generation task. As input to the system, we\nassume a user u who is associated with a set of Nu\nhistorical documents Du = {d(i)\nu }Nu\ni=1, where each\ndocument du may be, for example, a previously-\nauthored social media post, email, etc. The user\nu is further associated with a textual request qu\nsubmitted to the writing assistant. Finally, we as-\nsume the presence of a large language model fLLM\navailable via a prompt-based text generation API.\nGiven Du, qu, and fLLM, the task is to select a\nsubset of historical documents D\u2032\nu \u2282 Du to pro-\nvide as few-shot examples to the LLM, which then\ngenerates a target text tu of up to 300 words:\ntu = fLLM(\u03d5(qu, D\u2032\nu)),\nwhere \u03d5 is a prompt construction function that in-\nputs the user\u2019s request and retrieved historical doc-\numents, and tu reflects the style, preferences, and\nknowledge of the user u. To measure the quality\nof tu, we consider an optimal reference text t\u2217\nu that\nthe user u \u201cwould have written\u201d for request qu. The\ngoal is thus to maximize m(tu, t\u2217\nu), for a reference-\nbased text generation accuracy metric m.\n3\nProposed Approach\nWe present PEARL, a retrieval-augmented LLM-\nbased system for personalized writing assistance.\nAs shown in Figure 2, PEARL consists of an of-\nfline retriever training stage and an online LLM\ninference stage.\nOffline, we train a retriever\nfretr : (qu, du) \u2192 R that, given a request from the\nuser, scores the user\u2019s historical documents with\nrespect to their suitability for that request. We intro-\nduce two key novelties in training the retriever, both\nof which are aimed at optimizing fretr directly for\nthe downstream text generation task: 1) Training\ndata creation based on a novel difference of likeli-\nhoods from an auxiliary text generation model; and\n2) A scale-calibrating training objective. Online,\ngiven a new request, we prompt the LLM to gener-\nate a target text tu conditioned on the request and\nthe documents retrieved by fretr.\nFigure 2: To train retriever, fretr, an auxiliary language model is first used to identify historical requests that will\nbenefit from retrieval augmentation and the best document to use in the augmentation 1\u20dd. Then, fretr is trained\non the selected data with a scale calibrating loss function 2\u20dd. Given an unseen request, fretr is used to select the\nbest instances from historical texts for augmenting an LLM prompt for personalized generation 3\u20dd. Our training\nencourages scores produced by the retriever for documents to be proportional to the quality of the LLM output.\nNext, we describe the retriever training set con-\nstruction, describe how we optimize the retriever,\ndetails on LLM inference, and, finally, we provide\ntechnical details of the models used in our system.\n3.1\nTraining Data Setup\nAt a high level, our goal in training fretr is to op-\ntimize it for the downstream text generation task,\nsuch that given a request qu, fretr is able to identify\nthe historical documents du that will most likely\nhelp fLLM generate the target text tu. To do so, we\nfirst invoke an auxiliary text generation model\nfaux to help us identify which requests and docu-\nments will most benefit the generation of a target\ntext, in order to set up a training set that will help\noptimize fretr for the downstream generation task.\nData organization We begin by organizing the\ntraining data to simulate a setup close to the prob-\nlem defined in \u00a7 2. Given a set of M users and their\nhistorical document sets {Du}M\nu=1, for each user u\nwe partition Du into two non-overlapping sets, a\ncandidate document set Dc\nu \u2282 Du, and a \u201ctarget\u201d\ntext set Dt\nu \u2282 Du, such that Dc\nu + Dt\nu = Du. If\ntime metadata is available in the data, the parti-\ntioning is done temporally such that the target texts\noccur after the candidate documents; otherwise, the\npartitioning may be done randomly.\nNext, for each target text tu in each Dt\nu, we pair\nthe text with a corresponding request qu. If not\nreadily available in the data, requests may be gen-\nerated using any manual or automatic technique;\nsee \u00a7 4.1 for more details on synthetic request gen-\neration. In this way, we have all data required as\nper \u00a7 2: A set of candidate historical documents for\neach user, a set of requests to the writing assistant,\nand a set of \u201ctarget\u201d texts reflecting the user\u2019s true\nwriting style paired with each request.\nAuxiliary model scoring With our data thus\nset up, we now invoke an auxiliary text generation\nmodel faux to quantify the benefit of each candidate\ndocument in du \u2208 Dc\nu for each of the user\u2019s request-\ntarget pairs (qu, tu) \u2208 Dt\nu. We define the score\nydu\nq\n= log paux(tu|du, qu) \u2212 log paux(tu|qu), (1)\nas a difference in the conditional likelihood, per\nfaux, of the target given the request with and with-\nout the historical document. Note that the model\nfaux should be smaller than fLLM to support effi-\ncient creation of training data, and its token likeli-\nhoods must be accessible via API or direct access.\nAppendix A shows prompts used for faux.\nIntuitively, Eq. (1) is highest when the request is\nsuitable for personalization and the candidate doc-\nument is the \u201cright\u201d example for personalization:\nThat is, the request alone is not sufficient for gener-\nating the target text (i.e., the quantity defined by the\nsecond term is lower), and this candidate document\nis particularly beneficial to generating the target\ntext (i.e., the quantity defined by the first term is\nhigher). This approach extends recent work (Rubin\net al., 2022; Gonen et al., 2022) indicating the best\nprompts to be the ones with the highest conditional\nlikelihood of generating a target output to the novel\nsetting of personalized text generation.\n3.2\nTraining Data Selection\nWe use the scores yq from the auxiliary model faux\nto two ends: (1) Identifying a subset of training\nrequests that are likely to benefit from personal-\nization; and (2) identifying a subset of candidate\ndocuments that are likely to benefit those requests.\nRequest Selection Using Eq. 1, we score all\nrequest-target pairs of a user (qu, tu) \u2208 Dt\nu against\nall of their candidate documents du \u2208 Dc\nu, across\nall M users. After scoring, we select a top-scoring\nfraction of request-target pairs; in practice, we find\nthat the top two-thirds works well. Intuitively, this\nstep reflects the idea that not all pairs will benefit\nfrom retrieval augmentation, either due to the lack\nof suitable candidate documents in a user\u2019s histori-\ncal document set or due to underspecified requests\nmaking the target text simply too difficult to gen-\nerate well. Finally, after obtaining a high-quality\nset of training requests qu for fretr, we discard the\ntarget texts, as our retriever does not see target texts\nat inference time and has no value for training.\nCandidate document selection Next, we use\nEq. 1 to select the best candidate documents for a\nrequest, i.e. to identify positive training examples.\nGiven a request qu, we take the p highest-scoring\ncandidate documents du \u2208 Dc\nu as per Eq. (1) as\npositives. We sample n negative samples per posi-\ntive randomly from the candidate document set for\nthe user. Thus, we rely on Eq. 1 to identify both\nrequests and documents beneficial for the down-\nstream personalized text generation task.\n3.3\nRetriever Optimization\nWe instantiate fretr as a pretrained language model\nencoder and train it as a crossencoder with the data\ndescribed previously. We choose a crossencoder\nbecause it is expressive and not prohibitively ex-\npensive for personalization; most users\u2019 historical\ndocument sets Du are relatively small, on the order\nof few hundred documents.\nScale calibration Let yq = [y+\nq , . . . , y\u2212\nq ], where\ny+\nq corresponds to the score of a positive docu-\nment and y\u2212\nq corresponds to the score of a nega-\ntive document. Here, yq contains n negatives and\n1 positive document. Similarly, let the predicted\nlogits from fretr be denoted as sq = [s+\nq , . . . , s\u2212\nq ].\nThen, a standard KL-divergence loss is written as\nKL(yq, sq) = \u2212 P\ni sm(yq,i)log sm(sq,i), where\nsm represents the softmax function. We propose\nto incorporate scale calibration (Yan et al., 2022)\ninto the KL divergence objective, minimally modi-\nfying the loss by adding an \u201canchor\u201d example with\ntarget score y0, which is a tunable hyperparame-\nter, and logit s0 set to 0, resulting in score vec-\ntors y\u2032\nq = [y0, yq] and s\u2032\nq = [s0, sq]. The scale-\ncalibrated KL-divergence loss is thus\nKL(y\u2032\nq, s\u2032\nq) = \u2212\nX\ni\nsm(y\u2032\nq,i)log sm(s\u2032\nq,i)\n(2)\n= \u2212\nX\ni\neyq,i\nP\nj eyq,j + ey0 log\nesq,i\nP\nj esq,j + 1+\ney0\nP\nj eyq,j + ey0 log (\nX\nj\nesq,j + 1).\n(3)\nEmpirically, we find that setting y0 to the median\nvalue of scores from Eq (1) for positive candidate\ndocuments works well. Thus, the anchor target y0\nensures that extremely high or low scores output by\nfretr are driven away from the extremes of the score\ndistribution. In practice, this helps the predicted\nscores from fretr more accurately reflect the distri-\nbution of faux, and more closely track the utility of\na document for personalization.\n3.4\nLLM Inference\nAll steps previously described pertain to training\nthe retriever fretr offline. After training is complete,\nPEARL is ready to serve new requests online. Given\na unseen request, we retrieve the top-k historical\ntexts from a user\u2019s historical documents Du with\nour trained crossencoder fretr. We insert the request\nand retrieved documents into a prompt template,\nprovided in Appendix A. Finally, the prompt is fed\nto fLLM to generate a personalized target text tu.\n3.5\nModel details\nRetriever We instantiate fretr with the pre-trained\nMPNET, which is relatively lightweight at 110M\nparameters (Song et al., 2020). We obtain an output\nscore from fretr as wT tanh\n\u0000WT ENC([qu, du])\n\u0001\n,\nwhere ENC represents the CLS token from the final\nlayer of the encoder, and qu and du are the text\nof the input request and historical document. The\nencoder parameters, w, and W are trained.\nText generation models For fLLM we con-\nsider two performant LLMs offered via API by\nAzure OpenAI, davinci-003 and gpt-3.5-turbo.\nFor faux we consider a smaller but still effective\nencoder-decoder language model, FLANT5-XL,\nwith 3 billion parameters (Chung et al., 2022). The\nlatter model is open-sourced, allowing us to ac-\ncess its token likelihoods directly, a requirement\nof Eq. 1. We obtain target text likelihoods by tak-\ning the sum of log-probabilities of individual token\nlikelihoods from FLANT5-XL.\n4\nExperiments\nWe demonstrate the effectiveness of PEARL on two\npersonalized text generation datasets from social\nmedia platforms against a range of competitive\nretrieval-augmented systems with automatic and\nmanual evaluations. We also show how the out-\nput scores from our retriever being correlated with\ndownstream generation performance allows us use\nit as a performance predictor of LLM outputs, and\ndynamically improve LLM outputs.\n4.1\nExperimental Setup\nData We construct two open-ended long-form text\ngeneration datasets for social media applications:\n1) Personalized post writing on WORKSM and 2)\nPersonalized comment writing on AITA. Appendix\nB provides further details on our data.\nWORKSM WORKSM is an enterprise social net-\nwork used for communication within organizations\nfrom which we obtain a random sample of \u223c18k\nposts written by 1116 users from November 2020 to\nJuly 2023. To create an evaluation set, we manually\nexamine posts greater than 50 words and receiving\n\u22652 comments, about 1K posts, and select 163 of\nthe most recent posts from \u223c80 users to serve as\nreference target texts t\u2217\nu. These posts often share\nevents, research studies and results, campaigns, and\norganizational news - a diverse set we hypothesized\ncould benefit from personalized writing assistance.\nAs our dataset does not contain requests to the writ-\ning assistant, two of the authors manually wrote\nrequests qu per target text. Posts authored before\nt\u2217\nu make up Du. To create our training set, we only\nretain posts > 10 words and users \u2265 5 historic\nposts while excluding posts in our evaluation set.\nThen, we generate synthetic requests with GPT-4\nfor a training set of \u223c7k requests.\nAITA AITA is a Reddit subforum in which\noriginal posters (OP) describe their interpersonal\nmoral conflicts and receive comments from other\nusers judging them to be \u201cthe a**hole\u201d or \u201cnot\nthe a**hole\u201d. We construct a personalized com-\nment generation task from the dataset of Plepi et al.\n(2022). We treat the OP posts as requests qu, user\ncomments as reference target texts t\u2217\nu, and a user\u2019s\nprevious comments as Du. Since the dataset lacks\ntime metadata, we construct an evaluation set by\nsampling 10% of the OP posts as test requests, and\nfurther filter to 600 random target texts for our eval-\nuation set to keep LLM experiments feasible. For\ntraining, we use the historical post-comment pairs\nfrom users, resulting in \u223c84k requests. Note that\nwhile Reddit comments may not be the ideal plat-\nform for writing assistance, AITA is one of the\nfew public datasets available for the task. Further,\ngenerating comments for a post also resembles de-\nsirable scenarios for writing assistance, eg. email\nresponses (Kannan et al., 2016).\nEvaluation metrics For WORKSM we report\nreference-based evaluation measures ROUGE-1 (R1),\nROUGE-2 (R2), and BertScore-F1 (BS-F1) (Zhang*\net al., 2020).\nFor AITA, we found automatic\nreference-based measures highly unreliable due to\nthe very open-ended nature of the comment gener-\nation task. Therefore, we report performance on\na downstream stance classification task for AITA.\nThis evaluation maps generated comment texts to\na YTA (\u201cyou\u2019re the a**hole\u201d) or NTA (\u201cnot the\na**hole\u201d) label based on simple high-precision\nrules mapping lexical variations of \u201cyou\u2019re the\na**hole\u201d to YTA. However, our task of interest\nremains personalized comment generation \u2013 there-\nfore, we report human evaluations of generated\ntargets for AITA and WORKSM.\nPEARL implementation To train fretr, we con-\nsider the top two canadidate documents per Eq. (1)\nas positive examples per request and use three neg-\natives per positive, i.e., p = 2 and n = 3. In our\nLLM prompts, we use k = 3 retrieved examples for\nWORKSM and k = 4 for AITA, tuned on a dev set,\nand set generation temperature to zero. Appendix\nA provides further details and our prompts.\nBaselines We consider the following non-\npersonalized baselines: ZSHOT-NP: This repre-\nsents a non-personalized approach prompting only\nwith the request. KSHOT-NP: A zero-shot non-\npersonalized approach using a fixed randomly se-\nlected set of k documents for all requests. For\nAITA, the examples are balanced across labels.\nWe consider the following retrieval-augmented\npersonalized baselines, selecting from a user\u2019s his-\ntorical documents Du: Random: Random selection\nof k documents from Du. BM25: Represents a\nclassic performant retrieval model based on query-\ndocument term overlap. MPNET-1B: This a strong\nSentence-Bert (Reimers and Gurevych, 2019) bi-\nencoder trained on 1 billion text pairs from numer-\nous domains. Documents are ranked for a request\nusing cosine similarity between embeddings. QL-\nFT5: An approach which ranks documents based\non p(qu|du) with a pretrained FLANT5-BASE with\n250M parameters (Sachan et al., 2022). This may\nMethod \u2193 / LLM \u2192\ndavinci-003\ngpt-35-turbo\nBS-F1\nR1\nR2\nBS-F1\nR1\nR2\nZSHOT-NP\n0.3625\n0.5029\n0.2516\n0.3103\n0.4627\n0.2091\nKSHOT-NP\n0.3408\n0.4931\n0.2431\n0.3251\n0.4825\n0.2258\nRandom\n0.3504\n0.5036\n0.2505\n0.3346\n0.4893\n0.2345\nBM25\n0.3796\n0.5287\n0.2911\n0.3657\n0.5089\n0.2673\nMPNET-1B\n0.3830\n0.5281\n0.2931\n0.3602\n0.5063\n0.2639\nQL-FT5\n0.3870\n0.5337\n0.3019\n0.3598\n0.5054\n0.2642\nPEARL\n0.3960\n0.5419\n0.3094\n0.3649\n0.5082\n0.2676\n(a) Generation performance in WORKSM with reference based metrics.\nMethod \u2193 / LLM \u2192\ndavinci-003\ngpt-35-turbo\nMacro F1(%)\nMacro F1(%)\nZSHOT-NP\n41.97\n50.43\nKSHOT-NP\n51.71\n59.76\nRandom\n55.52\n59.47\nBM25\n57.26\n61.66\nMPNET-1B\n53.72\n59.23\nQL-FT5\n55.76\n58.15\nPEARL\n61.21\n65.34\n(b) Downstream classification performance in AITA.\nTable 1: PEARL is compared to non-personalized (NP) and LLMs personalized with retrieval on datasets of social\nmedia communication: an internal workplace social media dataset and a dataset constructed from Reddit.\ndavinci-003\ngpt-35-turbo\nT/B/P (%)\nT/B/P (%)\nWORKSM\n43.3 / 33.3 / 23.3\n30.0 / 26.7 / 43.3\nAITA\n63.3 / 16.7 / 20.0\n63.3 / 16.7 / 20.0\nTable 2: Human-rated preferences of similarity to a\nreference text for BM25 vs PEARL for WORKSM and\nAITA. Similarity judgments could be tied (T), or else\nprefer the BM25 baseline (B) or PEARL (P) system.\nbe seen as an unsupervised crossencoder.\n4.2\nResults\nTable 1 compares PEARL against baseline meth-\nods using automated metrics, and Table 2 presents\nmanual evaluation of generated texts. Appendix C\nevaluates the calibration for the PEARL retriever\nand presents additional results.\nAutomatic evaluation Tables 1a and 1b reports\nautomated metrics on WORKSM and AITA. We\nobserve that PEARL outperforms or performs on\npar with the best baselines across datasets and\nLLMs, indicating its effectiveness for personal-\nized writing assistance across various applications.\nThe gains are especially apparent in the AITA\nstance classification task, where PEARL outper-\nforms the next-strongest baseline by 4 to 5 Macro\nF1 points. We also observe that personalization\nthrough retrieval generally improves upon non-\npersonalized approaches (NP), which is consistent\nwith prior work (Salemi et al., 2023). Among base-\nline retrieval-augmented systems, BM25 often out-\nperforms other strategies. However, across LLMs,\nthis is not consistent (e.g., QL-FT5 on WORKSM\nwith davinci-003), indicating that our approach\nuniquely generalizes compared to baselines, whose\nperformance appears more dataset-dependent.\nManual evaluation In Table 2 we report the re-\nsults of human ratings. Here, we compare a BM25-\naugmented system with PEARL, as BM25 is our\nstrongest baseline. In these evaluations, we present\nraters with the user-written target text for a request\nand the generated texts from both systems, while\nhiding the system identity. We instruct the raters to\nselect the generated text more similar to the target\ntext. Appendix B details our guidelines. Given our\nfocus on personalization, we avoid rating genera-\ntions using specific intrinsic aspects e.g., fluency,\nnon-reduncancy (Celikyilmaz et al., 2021) and in-\nstead focus on how well the generated text mimics\nthe user\u2019s real writing sample. We considered 30\ngenerated text pairs per LLM and dataset (120 pairs\ntotal). In an agreement study, we achieved a Co-\nhen\u2019s Kappa score of \u03ba = 0.34, indicating fair\nagreement on a highly subjective task. In Table\n2, we see judgments are tied in three of four eval-\nuation settings. In three of four settings, PEARL\n(T+P) is preferred over BM25 (T+B). And PEARL\nis clearly preferred over BM25 in one setting. Note\nalso, that while BM25 performs comparably to\nPEARL in Table 1a with gpt-35-turbo, manual\nevaluation shows it to not be preferred. This also\nindicating the challenge of evaluating personalized\ntext generation \u2013 we discuss this further in \u00a77.\n4.3\nAlternative Training Strategies\nIn Table 3, we compare common alternative train-\ning of fretr while keeping inference steps identical.\nAppendix C.3 presents results with davinci-003.\nNo calibrated objective. Removing the scale cal-\nibration and using a standard KL divergence (\u2013 cal-\nibrated sm) degrades performance for AITA and\nsees comparable performance in WORKSM. This\nindicates the importance of calibration for estimat-\nDataset \u2192\nWORKSM\nAITA\nMethod \u2193 / LLM \u2192\ngpt-35-turbo\ngpt-35-turbo\nBS-F1\nR1\nR2\nMacro F1(%)\nPEARL\n0.3649\n0.5082\n0.2676\n65.34\nno calibrated sm\n0.3669\n0.5095\n0.2654\n63.01\ndistill paux(tu|du, qu)\n0.3564\n0.5057\n0.2652\n62.32\np(relevant = 1|du, qu)\n0.3599\n0.5038\n0.2613\n59.59\nTable 3: PEARL compared to alternate training of fretr.\ning the benefit of a historical document to a request\nconsistently across datasets. Appendix C shows\nthat scale calibration consistently improves perfor-\nmance with davinci-003 and the correlation of\nretriever scores with task performance.\nDistill paux(tu|du, qu) to fretr. The proposed fretr\nis trained on documents which benefit personaliza-\ntion and requests which benefit from personaliza-\ntion. Here, we compare to an approach that only\nselects documents that benefit personalization by\nmaximizing paux(tu|qu, du). This assumes that all\nrequests in our training set benefit from personaliza-\ntion. We train fretr with a KL-divergence objective.\nWe see in Table 3 (distill paux(tu|du, qu)) that this\nlowers performance markedly, indicating the value\nof our proposed approach. This approach, also,\nclosely resembles prior work (Rubin et al., 2022).\nOptimize fretr for relevance. Here, we treat a re-\nquest ru and its corresponding document du as a\npositive example and train fretr using cross-entropy.\nOther historical documents for the user make up\nnegatives examples. This is the dominant train-\ning strategy in search tasks optimized for request-\ndocument relevance, p(relevance = 1|ru, du). In\nTable 3, we see that this underperforms approaches\noptimized for target generation \u2013 indicating the\nvalue of retrievers optimized for generation.\n4.4\nSelective Revision with PEARL\nSo far, we have evaluated our retriever with respect\nto the downstream task of text generation. How-\never, we can also use the calibrated scores from\nfretr directly. Here, we provide a case study of how\nthese scores may be used to improve PEARL out-\nputs further. Specifically, we treat the scores from\nfretr as a predictor of text generation performance\nand selectively revise targets tu with low-scoring\nretrievals. In other words, we assume that if fretr\ncannot find a highly scored in-context example, the\ngenerated response will be of low quality and can\nbenefit from further revision by the LLM.\nSetup Given our trained retriever, we take all\nFigure 3: Generation calibration of fretr allows us to\nuse its predicted scores for performance prediction and\nselectively revise potentially bad generations.\nDataset \u2192\nWORKSM\nAITA\nLLM \u2192\ngpt-35-turbo\ngpt-35-turbo\nfretr = Proposed\nBS-F1\nR1\nR2\nMacro F1 (%)\nStage 1 (no revision)\n0.3702\n0.5124\n0.2709\n65.15\nAll (100% revision)\n0.3547\n0.5045\n0.2520\n64.85\nSelective revision\n0.3771\n0.5236\n0.2818\n65.36\nTable 4: Selectively revising target texts tu based on\nretriever scores results in consistent performance im-\nprovements compared to no revision and 100% revision.\ntop-1 document scores for each request s1 =\nmaxdu\u2208Du fretr(qu, du) and learn a threshold \u03b8 on\nthese s1 that maximizes a downstream performance\nmetric on a held-out development set (R2 and\nMacro-F1 on WORKSM and AITA, respectively).\nThen, given a generated target text tu with top-\nscoring retrieved document s1 < \u03b8, we selectively\nrevise tu where fLLM is prompted to edit the tar-\nget text. We report results of selective revision\ncompared to a single round of generation (i.e., no\nrevision) and full revision over the entire dataset\n(i.e., 100% revision). We provide further details\nand experiments with BM25 in Appendix C.2.\nResults Table 4, demonstrates that selective re-\nvision consistently improves upon a single round\nof generation (\u201cStage 1\u201d) by 2-4% in downstream\nperformance metrics. We also note that editing\nall outputs produced by Stage 1 generation con-\nsistently leads to degraded performance (\u201cAll\u201d),\nindicating that editing is not always helpful.\nWe also observe that PEARL chooses 75.8%\nand 77.9% instances for editing in WORKSM and\nAITA, respectively. This indicates the potential for\nsimultaneous retrieval and performance prediction\nmodels to reduce the number of expensive LLM\ncalls made while ensuring better writing agent per-\nformance. Finally, in a manual examination of\nrequests with a low s1 score by PEARL, we find\nthem to be underspecified and often require fur-\nther information from a user e.g. \u201cWrite a post\nabout how I like to relax after work\u201d, for a target\ndiscussing specific forms of relaxation. This also\nindicates the potential for using PEARL to solicit\ninformation from users for underspecified requests;\nwe leave this direction for future work.\n5\nRelated Work\nDynamic Prompts for LLMs We first examine\nwork training retrieval models for in-context exam-\nple selection for LLMs. Similar to PEARL, Rubin\net al. (2022) train retrieval models on data that is\nweakly labeled with LLM likelihoods of the tar-\nget completions. And Wang et al. (2023b) train\nretrieval models on finer-grained feedback from\na trained reward model through knowledge distil-\nlation. More distantly, Zhang et al. (2022) train\ninstances selection models on rewards from a down-\nstream evaluation metric using reinforcement learn-\ning. PEARL differs in its emphasis on selecting\ntraining instances that benefit from retrieval aug-\nmentation \u2013 a necessity in personalization where\nretrieval is performed over a smaller historical doc-\nument set instead of a large open-domain corpus.\nOther approaches have explored better use of\npre-trained models for example selection. Creswell\net al. (2023) select examples based on the target\nLLM likelihood - necessitating access to LLM like-\nlihoods and incurring latency in retrieval. Gupta\net al. (2023) explore selecting sets of examples with\ndense retrieval models, presenting a complemen-\ntary approach to ours. Finally, Pan et al. (2023) use\nretrieval models to select examples from multiple\nknowledge sources and train a routing model to\ndecide the source of knowledge to retrieve from \u2013\nin its use for selective revision (\u00a74.4), our retriever\nmay be seen as a simultaneous routing and retrieval\nmodel. PEARL\u2019s ability to identify low-quality re-\ntrievals and selectively edit generations also relates\nto work on robustness to noisy prompts and LLM\nchaining. We discuss these in Appendix D.\nCalibrated Retrievers A small body of work\nhas also explored calibrated ranking models. Our\nwork builds on Yan et al. (2022) who train scale-\ncalibrated ranking models to facilitate use in CTR\nsystems that expect scale calibration. Our work\nleverages scale-calibration for personalized writing\nassistance. Other work has explored probabilis-\ntic uncertainty estimation in retrievers and subse-\nquently used them for selective prediction. Dhu-\nliawala et al. (2022) jointly train a retriever and\na generative model for uncertainty calibration in\nmachine-reading systems. Cohen et al. (2021) esti-\nmate model confidence in retriever predicted rele-\nvances with monte-carlo dropout. Finally, Zamani\nand Bendersky (2023) model uncertainty in query\nand document representations independently by\nrepresenting them with Gaussian distributions. In\ncontrast with probabilistic uncertainty estimation,\nPEARL may be seen as a single pass model for esti-\nmating aleatoric uncertainty (Van Amersfoort et al.,\n2020), not requiring extensive changes to training,\nmodel architecture, or additional inference costs.\nPersonalized Writing Assistants While writing\nassistants have seen considerable exploration, only\nwork that has sought to aid communication applica-\ntions like email (Chen et al., 2019; Trajanovski\net al., 2021), social media (Gero et al., 2022),\nand grammatical error correction (GEC) (Nadejde\nand Tetreault, 2019) have focused on author per-\nsonalization. These systems commonly leverage\nnearest-neighbor models and parameter-efficient\nfine-tuning for personalization. Early work of Chen\net al. (2019) explore personalized email comple-\ntion by mixing user-specific n-gram language mod-\nels and a global language model. Orthogonally,\nTrajanovski et al. (2021) explore the effectiveness\nof augmenting text completion models with con-\ntexts from different time scales for email and mes-\nsaging platforms. Nadejde and Tetreault (2019)\nexplore personalization in GEC models through\nefficient fine-tuning of expertise and native lan-\nguage specific models. Contemporaneous work\nhas also explored personalized writing with LLMs.\nLi et al. (2023b) construct prompts with pre-trained\nretrieval and summarization models followed by\nfine-tuning an LLM for personalized completion.\nFollow-on work has explored training a prompt-re-\nwriter to tune prompts for a fixed LLM (Li et al.,\n2023a). Prompt re-writing presents a complemen-\ntary approach to our trained retriever, with future\nsystems likely to benefit from both elements. Fi-\nnally, we discuss non-personalized writing assis-\ntants and reader personalization in Appendix D.\n6\nConclusion\nIn this paper we present PEARL\u2013 an LLM based\nwriting assistant personalized with retrieval aug-\nmentation. We propose a method for training gen-\neration calibrated retrievers through a careful se-\nlection of training data using an auxiliary text gen-\neration model and a scale calibrated objective. We\ndemonstrate the effectiveness of our approach in\ndatasets of social media communication. We also\nshow how our retrieval model can double as a per-\nformance predictor and decide when retrieved doc-\numents are insufficient for an effective generation\nand require revision of LLM outputs.\nSeveral directions present meaningful future\nwork. Other forms of personalization may be ex-\nplored by appropriately constructing the document\nset Du to represent readers, groups of authors or\nreaders, or have heteregoneous sets of documents\n(e.g. emails, documents, code) per user. Alter-\nnatively, generation calibrated retrievers may be\nexplored in other scenarios where robustness to\nnoisy retrieval is necessary e.g. knowledge inten-\nsive tasks such as question answering. Finally, such\nretrievers may also be used for directing interac-\ntion with users as in the case of asking clarification\nquestions in information seeking tasks.\n7\nLimitations\nHere, we discuss limitations of our work derive\nfrom our choice of faux and fLLM, our evaluation\nsetup, and the design of our method.\nChoice of LLMs Our experiments use two\nclosed LLMs through API access (davinci-003,\ngpt-35-turbo).\nWhile we show the value of\nPEARL with LLM\u2019s of varying performance, es-\ntablishing its effectiveness with other LLMs will\nrequire further work. We also acknowledge that\nclosed LLMs limit experimental reproducibility -\nhowever, given the widespread use of GPT models\n(Hu, 2023) we believe our investigation is mean-\ningful. Finally, in constructing training data for\ninstance selection models for an LLM, prior work\nhas noted the best empirical performance from\nmatching faux and fLLM (Rubin et al., 2022). While\nwe demonstrate benefits from using significantly\nsmaller models for faux, using an open LLM will\nallow further validation of this result in the context\nof our approach. However, using a larger (open)\nmodel for faux will incur additional costs in cre-\nating training data, and smaller models for fLLM\nare likely to see a worse generation performance -\nexploring this tradeoff requires future work.\nEvaluation Setup Next, while WORKSM repre-\nsents an impactful and realistic use case for writing\nassistants, we acknowledge that its use limits repro-\nducibility. Further, our evaluation set of WORKSM\nand AITA represents a limited set of scenarios that\nare likely to leverage writing assistants. Future\nwork, and online evaluations are necessary to es-\ntablish the value of PEARL across the myriad of\nscenarios where writing assistants may be used.\nFinally, we report text generation evaluation met-\nrics, which, in addition to known biases (He et al.,\n2023), were not developed for personalized text\ngeneration tasks. Evaluating text generations under\nvarious personalization setups represents a largely\nemerging body of work (Wang et al., 2023a,d).\nMethod Design Finally, we note that the current\ndesign of PEARL is likely to have some drawbacks.\nIt is possible that our proposed method for train-\ning instance selection biases system performance\ntoward some users or requests \u2013 we leave examina-\ntion of this to future work. It is also possible that\nformulating fretr as an expressive crossencoder and\nthe use of large LLMs will present latency limita-\ntions for interactive applications \u2013 exploration of\nmodels supporting faster retrieval and text genera-\ntion inference represent important future work.\n8\nEthical and Broader Impact\nHaving introduced PEARL as an effective personal-\nization strategy for writing assistance and discussed\nits benefits we review two implications of concern\narising from better personalized text generation:\nchallenges to factuality, and longer term influence\non language use and communication.\nChallenges to Factuality The emergence of\nLLMs and their ability to generate compelling text\nhas seen a subsequent rise in the cases of malicious\nuse of these technologies. Augenstein et al. (2023)\noverview four such classes of harm: personalized\nattacks on individuals in the form of phishing at-\ntacks and tailored misinformation, impersonation\nof trusted figures (e.g. journalists or regulators), a\nglut of paraphrased misinformation evading detec-\ntion by automatic tools often used by fact checkers,\nand large scale creation of fake social media pro-\nfiles and plagiarized content (Brewster et al., 2023).\nIt is possible that improvements in personalized\ntext generation are likely to excacertabe each of\nthese problems. While several technology and pol-\nicy initiatives, spanning detection of AI-generated\ncontent, cryptographic signatures intended to prove\nthe authenticity of content, to government regula-\ntion and public education are under active develop-\nment their effectiveness remains to be seen.\nLanguage Use and Communication Current\nunderstanding of computer mediated communica-\ntion suggests that users interpersonal communica-\ntion patterns are influenced by the tool/medium\nused for communication (Poddar et al., 2023) with\na potential for these influences to have longer term\ninfluences on communication in the absence of\nthese tools (Hancock et al., 2020). Hancock et al.\noutline these implications as ranging from shifts\nin language use (e.g a social expectation of more\npositive responses (Hohenstein and Jung, 2018)),\nissues of how individuals portray themselves and\nevaluate others, to long term feedback loops re-\nsulting in how we perceive ourselves. However,\nunderstanding of the implications of AI mediated\ncommunication, specially those powered by pow-\nerful LLMs, is largely developing (Hancock et al.,\n2020). It is likely that wide spread personalization\nin LLM communication agents, will necessitate fur-\nther understanding of these factors and the design\nof systems that incorporates this understanding to\nameliorate harms.\nReferences\nNader Akoury, Shufan Wang, Josh Whiting, Stephen\nHood, Nanyun Peng, and Mohit Iyyer. 2020. STO-\nRIUM: A Dataset and Evaluation Platform for\nMachine-in-the-Loop Story Generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6470\u20136484, Online. Association for Computational\nLinguistics.\nXiang Ao, Xiting Wang, Ling Luo, Ying Qiao, Qing He,\nand Xing Xie. 2021. PENS: A dataset and generic\nframework for personalized news headline genera-\ntion. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 82\u201392, Online. Association for Computational\nLinguistics.\nSimran Arora, Avanika Narayan, Mayee F Chen, Lau-\nrel Orr, Neel Guha, Kush Bhatia, Ines Chami, and\nChristopher Re. 2023. Ask me anything: A sim-\nple strategy for prompting language models. In The\nEleventh International Conference on Learning Rep-\nresentations.\nIsabelle Augenstein, Timothy Baldwin, Meeyoung Cha,\nTanmoy Chakraborty, Giovanni Luca Ciampaglia,\nDavid Corney, Renee DiResta, Emilio Ferrara, Scott\nHale, Alon Halevy, Eduard Hovy, Heng Ji, Filippo\nMenczer, Ruben Miguez, Preslav Nakov, Dietram\nScheufele, Shivam Sharma, and Giovanni Zagni.\n2023. Factuality challenges in the era of large lan-\nguage models. arXiv preprint arXiv:2310.05189.\nTal August, Katharina Reinecke, and Noah A. Smith.\n2022. Generating scientific definitions with control-\nlable complexity. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8298\u20138317,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nJack Brewster, Macrina Wang, and Coalter Palmer. 2023.\nPlagiarism-bot? how low-quality websites are using\nai to deceptively rewrite content from mainstream\nnews outlets. NewsGaurd, The Internet Trust Tool.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2021. Evaluation of text generation: A survey. arXiv\npreprint arXiv:2006.14799.\nTuhin Chakrabarty, Vishakh Padmakumar, and He He.\n2022. Help me write a poem: Instruction tuning as a\nvehicle for collaborative poetry writing. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 6848\u20136863,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nMia Xu Chen, Benjamin N. Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yi-\nnan Wang, Andrew M. Dai, Zhifeng Chen, Timothy\nSohn, and Yonghui Wu. 2019. Gmail smart com-\npose: Real-time assisted writing. In Proceedings\nof the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, KDD \u201919,\npage 2287\u20132295, New York, NY, USA. Association\nfor Computing Machinery.\nShiping Chen, Duncan P Brumby, and Anna L Cox.\n2023. Using writing assistants to accelerate the peer\nreview process. Second Workshop on Intelligent and\nInteractive Writing Assistants, CHI 2023.\nMichael Chui, Eric Hazan, Roger Roberts, Alex Singla,\nKate Smaje, Alex Sukharevsky, Lareina Yee, , and\nRodney Zemmel. 2023. The economic potential of\ngenerative ai: The next productivity frontier. Na-\ntional Bureau of Economic Research, Digest No. 6.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nElizabeth Clark, Anne Spencer Ross, Chenhao Tan,\nYangfeng Ji, and Noah A. Smith. 2018. Creative\nwriting with a machine in the loop: Case studies on\nslogans and stories. In 23rd International Confer-\nence on Intelligent User Interfaces, IUI \u201918, page\n329\u2013340, New York, NY, USA. Association for Com-\nputing Machinery.\nDaniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rek-\nabsaz, and Carsten Eickhoff. 2021. Not all relevance\nscores are equal: Efficient uncertainty and calibration\nmodeling for deep retrieval models. In Proceedings\nof the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nSIGIR \u201921, page 654\u2013664, New York, NY, USA. As-\nsociation for Computing Machinery.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In The\nEleventh International Conference on Learning Rep-\nresentations.\nRobert Dale and Jette Viethen. 2021. The automated\nwriting assistance landscape in 2021. Natural Lan-\nguage Engineering, 27(4):511\u2013518.\nShehzaad Dhuliawala, Leonard Adolphs, Rajarshi Das,\nand Mrinmaya Sachan. 2022. Calibration of machine\nreading systems at scale. In Findings of the Asso-\nciation for Computational Linguistics: ACL 2022,\npages 1682\u20131693, Dublin, Ireland. Association for\nComputational Linguistics.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and Ja-\nson Weston. 2023. Chain-of-verification reduces hal-\nlucination in large language models. arXiv preprint\narXiv:2309.11495.\nAsma Farajidizaji, Vatsal Raina, and Mark Gales. 2023.\nIs it possible to modify text to a target readability\nlevel? an initial investigation using zero-shot large\nlanguage models. arXiv preprint arXiv:2309.12551.\nKaty Ilonka Gero, Vivian Liu, and Lydia Chilton. 2022.\nSparks: Inspiration for science writing using lan-\nguage models. In Proceedings of the 2022 ACM\nDesigning Interactive Systems Conference, DIS \u201922,\npage 1002\u20131019, New York, NY, USA. Association\nfor Computing Machinery.\nHugo Gon\u00e7alo Oliveira. 2017. A survey on intelligent\npoetry generation: Languages, features, techniques,\nreutilisation and evaluation. In Proceedings of the\n10th International Conference on Natural Language\nGeneration, pages 11\u201320, Santiago de Compostela,\nSpain. Association for Computational Linguistics.\nHila Gonen, Srini Iyer, Terra Blevins, Noah A. Smith,\nand Luke Zettlemoyer. 2022. Demystifying prompts\nin language models via perplexity estimation. arXiv\npreprint arXiv:2212.04037.\nYue Guo, Wei Qiu, Yizhong Wang, and Trevor Co-\nhen. 2021.\nAutomated lay language summariza-\ntion of biomedical scientific reviews. Proceedings\nof the AAAI Conference on Artificial Intelligence,\n35(1):160\u2013168.\nShivanshu Gupta, Matt Gardner, and Sameer Singh.\n2023.\nCoverage-based example selection for in-\ncontext learning. arXiv preprint arXiv:2305.14907.\nJeffrey T Hancock, Mor Naaman, and Karen Levy. 2020.\nAI-Mediated Communication: Definition, Research\nAgenda, and Ethical Considerations.\nJournal of\nComputer-Mediated Communication, 25(1):89\u2013100.\nTianxing He, Jingyu Zhang, Tianle Wang, Sachin\nKumar, Kyunghyun Cho, James Glass, and Yulia\nTsvetkov. 2023. On the blind spots of model-based\nevaluation metrics for text generation. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12067\u201312097, Toronto, Canada. Association\nfor Computational Linguistics.\nJess Hohenstein and Malte Jung. 2018. Ai-supported\nmessaging: An investigation of human-human text\nconversation with ai support. In Extended Abstracts\nof the 2018 CHI Conference on Human Factors in\nComputing Systems, CHI EA \u201918, page 1\u20136, New\nYork, NY, USA. Association for Computing Machin-\nery.\nKrystal Hu. 2023.\nChatgpt sets record for fastest-\ngrowing user base - analyst note. National Bureau of\nEconomic Research, Digest No. 6.\nAnjuli Kannan, Karol Kurach, Sujith Ravi, Tobias Kauf-\nmann, Andrew Tomkins, Balint Miklos, Greg Cor-\nrado, Laszlo Lukacs, Marina Ganea, Peter Young,\nand Vivek Ramavajjala. 2016. Smart reply: Auto-\nmated response suggestion for email. In Proceedings\nof the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, KDD \u201916,\npage 955\u2013964, New York, NY, USA. Association for\nComputing Machinery.\nOmar Khattab,\nKeshav Santhanam,\nXiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2023.\nDemonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp.\narXiv preprint\narXiv:2212.14024.\nCheng Li, Mingyang Zhang, Qiaozhu Mei, Weize Kong,\nand Michael Bendersky. 2023a. Automatic prompt\nrewriting for personalized text generation.\narXiv\npreprint arXiv:2310.00152.\nCheng Li, Mingyang Zhang, Qiaozhu Mei, Yaqing\nWang, Spurthi Amba Hombaiah, Yi Liang, and\nMichael Bendersky. 2023b. Teach llms to person-\nalize \u2013 an approach inspired by writing education.\narXiv preprint arXiv:2308.07968.\nDaliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin\nWang, Michal Lukasik, Andreas Veit, Felix Yu, and\nSanjiv Kumar. 2023c. Large language models with\ncontrollable working memory. In Findings of the As-\nsociation for Computational Linguistics: ACL 2023,\npages 1774\u20131793, Toronto, Canada. Association for\nComputational Linguistics.\nZihao Li, Samuel Belkadi, Nicolo Micheletti, Lifeng\nHan, Matthew Shardlow, and Goran Nenadic. 2023d.\nLarge language models and control mechanisms im-\nprove text readability of biomedical abstracts.\nZhiyu Lin, Upol Ehsan, Rohan Agarwal, Samihan\nDani, Vidushi Vashishth, and Mark Riedl. 2023. Be-\nyond prompts: Exploring the design space of mixed-\ninitiative co-creativity systems. In ICCC.\nHongyin Luo, Yung-Sung Chuang, Yuan Gong, Tian-\nhua Zhang, Yoon Kim, Xixin Wu, Danny Fox, He-\nlen Meng, and James Glass. 2023. Sail: Search-\naugmented instruction learning.\narXiv preprint\narXiv:2305.15225.\nCerstin Mahlow. 2023. Writing tools: Looking back\nto look ahead. Second Workshop on Intelligent and\nInteractive Writing Assistants, CHI 2023.\nBodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni,\nand Julian McAuley. 2019. Generating personalized\nrecipes from historical user preferences. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5976\u20135982, Hong\nKong, China. Association for Computational Linguis-\ntics.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 9802\u20139822, Toronto,\nCanada. Association for Computational Linguistics.\nPierre-Emmanuel Mazar\u00e9, Samuel Humeau, Martin Rai-\nson, and Antoine Bordes. 2018. Training millions of\npersonalized dialogue agents. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2775\u20132779, Brussels,\nBelgium. Association for Computational Linguistics.\nAditya Menon, Sadeep Jayasumana, Ankit Singh Rawat,\nSeungyeon Kim, Sashank Reddi, and Sanjiv Kumar.\n2022. In defense of dual-encoders for neural ranking.\nIn Proceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research, pages 15376\u201315400.\nPMLR.\nPiotr Mirowski, Kory W. Mathewson, Jaylen Pittman,\nand Richard Evans. 2023. Co-writing screenplays\nand theatre scripts with language models: Evaluation\nby industry professionals. In Proceedings of the 2023\nCHI Conference on Human Factors in Computing\nSystems, CHI \u201923, New York, NY, USA. Association\nfor Computing Machinery.\nSonia K. Murthy, Kyle Lo, Daniel King, Chandra Bha-\ngavatula, Bailey Kuehl, Sophie Johnson, Jonathan\nBorchardt, Daniel S. Weld, Tom Hope, and Doug\nDowney. 2022. Accord: A multi-document approach\nto generating diverse descriptions of scientific con-\ncepts.\nMaria Nadejde and Joel Tetreault. 2019. Personalizing\ngrammatical error correction: Adaptation to profi-\nciency level and L1. In Proceedings of the 5th Work-\nshop on Noisy User-generated Text (W-NUT 2019),\npages 27\u201333, Hong Kong, China. Association for\nComputational Linguistics.\nJianmo Ni, Zachary C. Lipton, Sharad Vikram, and Ju-\nlian McAuley. 2017. Estimating reactions and recom-\nmending products with generative models of reviews.\nIn Proceedings of the Eighth International Joint Con-\nference on Natural Language Processing (Volume 1:\nLong Papers), pages 783\u2013791, Taipei, Taiwan. Asian\nFederation of Natural Language Processing.\nShakked Noy and Whitney Zhang. 2023. Experimental\nevidence on the productivity effects of generative\nartificial intelligence. Science, 381(6654):187\u2013192.\nXiaoman Pan, Wenlin Yao, Hongming Zhang, Dian Yu,\nDong Yu, and Jianshu Chen. 2023. Knowledge-in-\ncontext: Towards knowledgeable semi-parametric\nlanguage models. In The Eleventh International Con-\nference on Learning Representations.\nMartin J Pickering and Simon Garrod. 2013. An inte-\ngrated theory of language production and comprehen-\nsion. Behavioral and brain sciences, 36(4):329\u2013347.\nJoan Plepi, B\u00e9la Neuendorf, Lucie Flek, and Charles\nWelch. 2022. Unifying data perspectivism and per-\nsonalization: An application to social norms. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 7391\u2013\n7402, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nRitika Poddar, Rashmi Sinha, Mor Naaman, and Mau-\nrice Jakesch. 2023. Ai writing assistants influence\ntopic choice in self-presentation. In Extended Ab-\nstracts of the 2023 CHI Conference on Human Fac-\ntors in Computing Systems, CHI EA \u201923, New York,\nNY, USA. Association for Computing Machinery.\nDongqi Pu and Vera Demberg. 2023.\nChatGPT vs\nhuman-authored text: Insights into controllable text\nsummarization and sentence style transfer. In Pro-\nceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 4:\nStudent Research Workshop), pages 1\u201318, Toronto,\nCanada. Association for Computational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2655\u20132671, Seattle, United States.\nAssociation for Computational Linguistics.\nDevendra Sachan, Mike Lewis, Mandar Joshi, Armen\nAghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\nZettlemoyer. 2022. Improving passage retrieval with\nzero-shot question generation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3781\u20133797, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky,\nand Hamed Zamani. 2023. Lamp: When large lan-\nguage models meet personalization.\nZejiang Shen, Tal August, Pao Siangliulue, Kyle Lo,\nJonathan Bragg, Jeff Hammerbacher, Doug Downey,\nJoseph Chee Chang, and David Sontag. 2023. Be-\nyond summarization: Designing ai support for real-\nworld expository writing tasks. Second Workshop on\nIntelligent and Interactive Writing Assistants, CHI\n2023.\nWeijia Shi, Xiaochuang Han, Mike Lewis, Yulia\nTsvetkov, Luke Zettlemoyer, and Scott Wen tau\nYih. 2023.\nTrusting your evidence: Hallucinate\nless with context-aware decoding. arXiv preprint\narXiv:2305.14739.\nKumar Shridhar, Harsh Jhamtani, Hao Fang, Ben-\njamin Van Durme, Jason Eisner, and Patrick Xia.\n2023. Screws: A modular framework for reasoning\nwith revisions. arXiv preprint arXiv:2309.13075.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2020. Mpnet: Masked and permuted pre-\ntraining for language understanding. In Advances in\nNeural Information Processing Systems, volume 33.\nSimeng Sun, Wenlong Zhao, Varun Manjunatha, Ra-\njiv Jain, Vlad Morariu, Franck Dernoncourt, Bal-\naji Vasan Srinivasan, and Mohit Iyyer. 2021. IGA:\nAn intent-guided authoring assistant. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5972\u20135985, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nStojan Trajanovski, Chad Atalla, Kunho Kim, Vipul\nAgarwal, Milad Shokouhi, and Chris Quirk. 2021.\nWhen does text prediction benefit from additional\ncontext? an exploration of contextual signals for chat\nand email messages. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies: Industry Papers, pages 1\u20139,\nOnline. Association for Computational Linguistics.\nJoost Van Amersfoort, Lewis Smith, Yee Whye Teh,\nand Yarin Gal. 2020. Uncertainty estimation using a\nsingle deep deterministic neural network. In Proceed-\nings of the 37th International Conference on Machine\nLearning, volume 119 of Proceedings of Machine\nLearning Research, pages 9690\u20139700. PMLR.\nDanqing Wang, Kevin Yang, Hanlin Zhu, Xiaomeng\nYang, Andrew Cohen, Lei Li, and Yuandong Tian.\n2023a.\nLearning personalized story evaluation.\narXiv preprint arXiv:2310.03304.\nLiang Wang, Nan Yang, and Furu Wei. 2023b. Learning\nto retrieve in-context examples for large language\nmodels. arXiv preprint arXiv:2307.07164.\nSitong Wang, Lydia B Chilton, and Jeffrey V Nickerson.\n2023c. Writing with generative ai: Multi-modal and\nmulti-dimensional tools for journalists. Second Work-\nshop on Intelligent and Interactive Writing Assistants,\nCHI 2023.\nYaqing Wang, Jiepu Jiang, Mingyang Zhang, Cheng\nLi, Yi Liang, Qiaozhu Mei, and Michael Bender-\nsky. 2023d. Automated evaluation of personalized\ntext generation using large language models. arXiv\npreprint arXiv:2310.11593.\nTongshuang Wu, Michael Terry, and Carrie Jun Cai.\n2022.\nAi chains: Transparent and controllable\nhuman-ai interaction by chaining large language\nmodel prompts. In Proceedings of the 2022 CHI\nConference on Human Factors in Computing Sys-\ntems, CHI \u201922, New York, NY, USA. Association for\nComputing Machinery.\nNishant Yadav, Nicholas Monath, Rico Angell, Manzil\nZaheer, and Andrew McCallum. 2022. Efficient near-\nest neighbor search for cross-encoder models using\nmatrix factorization. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 2171\u20132194, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nLe Yan, Zhen Qin, Xuanhui Wang, Michael Bendersky,\nand Marc Najork. 2022. Scale calibration of deep\nranking models. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and\nData Mining, KDD \u201922, page 4300\u20134309, New York,\nNY, USA. Association for Computing Machinery.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context.\nHamed Zamani and Michael Bendersky. 2023. Mul-\ntivariate representation learning for information re-\ntrieval. In Proceedings of the 46th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, SIGIR \u201923, page 163\u2013173,\nNew York, NY, USA. Association for Computing\nMachinery.\nHanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou,\nand Dawei Song. 2023a. A survey of controllable\ntext generation using transformer-based pre-trained\nlanguage models. ACM Comput. Surv., 56(3).\nJieyu Zhang, Ranjay Krishna, Ahmed H. Awadallah,\nand Chi Wang. 2023b. Ecoassistant: Using llm assis-\ntant more affordably and accurately. arXiv preprint\narXiv:2310.03046.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204\u20132213,\nMelbourne, Australia. Association for Computational\nLinguistics.\n\u0007\n\u0004\nGiven a REQUEST from a USER to author a\nPOST, write a POST for an enterprise\nsocial media site mimicking the user to\nsatisfy the REQUEST.\nUse the following instructions for your\nresponse:\n1. You should maintain consistency in\ntone and style with the USER's historical\nposts.\n2. You should imitate the language style\nof the USER's historical posts.\n3. You should employ similar rhetorical\nmethods as the USER's historical posts.\nHere are some historical posts by the\nUSER: {{historical_examples}}\nREQUEST: {{target_request}}\nWrite the POST to satisfy the REQUEST\nmimicing the tone , style , and rhetorical\n\u0006 methods of the USER's historical posts.\n\u0005\nPrompt 1: fLLM prompt used to generate a target text\ngiven historical examples retrieved by fretr and a target\nrequest for WORKSM.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nYiming Zhang, Shi Feng, and Chenhao Tan. 2022. Ac-\ntive example selection for in-context learning. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9134\u2013\n9148, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nA\nAdditional Implementation Detail\nLLM Prompts We use Prompts 1 and 2 for\nLLM inference. The same prompts are used with\ndavinci-003 and gpt-35-turbo. For construct-\ning training data in Eq 1 with a FLANT5-XL, faux\nwe use Prompts 3, 4, 5, 6. Note that computing\npaux(tu|qu) uses a set of randomly chosen few shot\nexamples from the training set fixed across requests\nrather than the request alone.\nHyperparameters We also use temperatures for\ntarget scores input to softmax functions in Eq. (2),\ny\u2032\nu/\u03c4 with \u03c4 = 5. Finally, we set y0 = 110 for\nWORKSM and y0 = 5 for AITA, which are the\nmedian values of Eq. (1) for each respective dataset\non the training data. We tuned y0 on a dev set\nconstructed similar to our training set to 25 and 75\npercentile values of Eq. (1).\nB\nDatasets\nHere we present various details of datasets used in\nour experiments here.\n\u0007\n\u0004\nFor a POST from the subreddit Am I The\nAsshole write a COMMENT explaining if the\nauthor of a post is an asshole or not\nthe asshole as a COMMENTER.\nUse the following instructions for your\nresponse:\n1. Read the below example comments by\nthe COMMENTER.\n2. Write the comment as the COMMENTER\nmimicing the length , style , reasoning ,\nand stances of their comments.\nHere are some example comments by the\nCOMMENTER: {{historical_examples}}\nPOST: {{target_request}}\nWrite the COMMENT mimicing the length ,\nstyle , reasoning , and stances of the\nCOMMENTERS comments.\n\u0006\n\u0005\nPrompt 2: fLLM prompt used to generate a target text\ngiven historical examples retrieved by fretr and a target\nrequest for AITA.\n\u0007\n\u0004\nHere is are some posts by a user on an\nenterprise social network:\n{{random_fewshot_examples}}\nHere is an outline for a target post by\nthe user: {{target_request}}. Write the\ntarget post:\n\u0006\n\u0005\nPrompt 3: faux prompt used to compute paux(tu|qu) in\nEq (1) for WORKSM.\nB.1\nTraining Requests in WORKSM\nSection 4.1 notes that our training set for WORKSM\nwas constructed from synthetic requests generated\nby GPT4. The prompt for this is presented in\nPrompt 9. We follow an incremental approach\nto construct the synthetic requests: first extracting\nthe salient aspects of the post, followed by con-\ncatenation of these aspects to result in the request.\nThe salient aspects span: an overview of the post,\nproper nouns mentioned in the post, contact in-\nformation, links to webpages, and any specialized\nknowledge or anecdotes in the post. Given the suc-\ncess of chain-of-thought prompting, we generate\nan explanation followed by salient aspects of the\npost \u2013 the explanations are not used elsewhere.\nB.2\nEvaluation Requests in WORKSM\nFor evaluation in WORKSM two authors not in-\nvolved in model development manually authored\nrequests for each of the 163 target posts in our\nevaluation set. Guidelines presented to annotators\nfor the requests are presented in Guideline 1. The\nrequests are intended to be brief and include the\nsalient information contained in the post. Note that\n\u0007\n\u0004\nHere is an example post by a user on an\nenterprise social network:\n{{candidate_document}}. Here is an outline\nfor a target post by the user:\n{{target_request}}. Write the target post:\n\u0006\n\u0005\nPrompt 4: faux prompt used to compute paux(tu|du, qu)\nin Eq (1) for WORKSM.\n\u0007\n\u0004\nHere are some example posts on the Am I\nThe Asshole subreddit:\n{{random_fewshot_examples}}. Target post:\n{{target_post}}. Write a users comment for\nthis post:\n\u0006\n\u0005\nPrompt 5: faux prompt used to compute paux(tu|qu) in\nEq (1) for AITA.\nannotators external to the authors weren\u2019t recruited\nfor authoring requests due to the private nature of\nthe WORKSM dataset.\nB.3\nManual Evaluation Guidelines\nIn WORKSM and AITA we conduct a manual eval-\nuation of model generations. In these evaluations,\nwe present raters with the reference user-written tar-\nget text for a request and the generated texts from\nbaseline and proposed systems, while hiding the\nsystem identity. We instruct the raters to indicate\na preference between the baseline and proposed\ngenerations while comparing to the target text. The\ninstructions provided to raters for WORKSM and\nAITA are presented in Guideline 2 and 3.\nC\nAdditional Results\nHere we present additional results in addition to\nthose presented in \u00a74.2. We present these here\nprimarily in the interest of space.\nC.1\nCalibration Evaluation\nWe examine the calibration performance of PEARL\ncompared with baseline retrieval models (Table\n5). In contrast with prior work (Dhuliawala et al.,\n2022; Yan et al., 2022) that focuses on classifica-\ntion tasks, where model-predicted class probabil-\nities may be used for measuring calibration per-\nformance, we report calibration performance with\nPearson correlations. Pearson r is reported between\nthe top-scoring document score for a request and\ndownstream generation evaluation metrics \u2013 R2 for\nWORKSM, and Macro-F1 for AITA. For AITA we\nfirst bin evaluation requests into equal sized bins\n\u0007\n\u0004\nHere is an comment on a post by a user\non the Am I the Asshole subreddit:\n{{candidate_comment}}. Target post:\n{{target_post}}. Write a users comment for\nthis post:\n\u0006\n\u0005\nPrompt 6: faux prompt used to compute paux(tu|du, qu)\nin Eq (1) for AITA.\nMethod \u2193 / LLM \u2192\ndavinci-003\ngpt-35-turbo\nPearson r\nPearson r\nWORKSM\nBM25\n0.42\n0.52\nMPNET-1B\n0.54\n0.52\nQL-FT5\n-0.05\n-0.02\nPEARL fretr\n0.64\n0.64\n\u2013 calibrated sm\n0.58\n0.55\nAITA\nBM25\n0.08\n-0.05\nMPNET-1B\n0.07\n-0.14\nQL-FT5\n-0.48\n-0.02\nPEARL fretr\n0.11\n0.45\n\u2013 calibrated sm\n-0.48\n0.12\nTable 5: Calibration performance of PEARL evaluated\nthrough correlation between score for top-1 document\nand R2 for WORKSM, and Macro-F1 for AITA.\nby top-1 document score, s1, and then measure\npearson r between the bin start and the average\nevaluation metric per bin.\nAmong baseline methods, we see sparse and\ndense retrieval methods, BM25 and MPNET-1B\nscores to be better calibrated with downstream per-\nformance compared to likelihood-based methods\nlike QL-FT5. We also see PEARL to be better corre-\nlated with downstream performance for WORKSM\nand AITA- indicating the effectiveness of our train-\ning strategy. Further, we also report on an ablated\nmodel, not using the scale-calibrated objective of\nEq (3) (\u2013 calibrated sm). We see this approach un-\nderperform PEARL, indicating the importance of\nthe scale-calibrated objective for a well-calibrated\ncrossencoder. The poorer calibration of crossen-\ncoders trained without scale calibration also finds\nsupport in prior work showing crossencoder scores\nto lie at extremes of the score distribution (Menon\net al., 2022; Yadav et al., 2022).\nC.2\nSelective Revision with PEARL \u2013\nExtended\nIn \u00a74.4 and Table 4 we demonstrate how our\ntrained retrieval model can be used for selective\nrevision with gpt-35-turbo. Prompt 8, 7 present\nthe prompts used for revision with both LLMs.\nHere, we also present results for selective revi-\nDataset \u2192\nWORKSM\nAITA\nMethod \u2193 / LLM \u2192\ngpt-35-turbo\ngpt-35-turbo\nfretr = BM25\nBS-F1\nR1\nR2\nMacro F1 (%)\nStage 1 (no revision)\n0.3615\n0.5052\n0.2611\n59.99\nAll (100% revision)\n0.3545\n0.5096\n0.2573\n58.36\nSelective revision\n0.3729\n0.5206\n0.2738\n57.71\nfretr = Proposed\nBS-F1\nR1\nR2\nMacro F1 (%)\nStage 1 (no revision)\n0.3702\n0.5124\n0.2709\n65.15\nAll (100% revision)\n0.3547\n0.5045\n0.2520\n64.85\nSelective revision\n0.3771\n0.5236\n0.2818\n65.36\nTable 6: Selectively revising target texts tu based on\nscores from our trained retriever vs BM25. Also present\nare results of no revision and revising all outputs (100%\nrevision) from Stage 1 outputs.\n\u0007\n\u0004\nGiven a REQUEST and a DRAFT from a USER to\nauthor a social media POST, edit the\nDRAFT to satisfy the REQUEST.\nUse the following instructions for your\nresponse:\n1. Enumerate any missing missing\ninformation from the REQUEST in the DRAFT.\n2. Enumerate any irrelevant information\nfor the REQUEST in the DRAFT.\n3. Then output the edited DRAFT starting\nwith the words EDITED DRAFT.\nREQUEST:{{target_request}}\nDRAFT: {{target_draft}}\nOutput missing or irrelevant information\nfor the REQUEST, then output the EDITED\nDRAFT satisfying the REQUEST.\n\u0006\n\u0005\nPrompt 7: fLLM prompt used to for selective revision\ngiven a Stage 1 draft for WORKSM.\nsion by using scores from a BM25 retriever. The\nresults of this are presented in Table 6. We see\nthat selective editing based on BM25 scores im-\nproves Stage 1 generations by 3-5% across genera-\ntion metrics in WORKSM. However, selective edit-\ning with BM25 sees a 4% drop in performance with\nAITA. Further, in selective editing, BM25 routes\n64.6% and 18.8% instances to the editing stage for\nWORKSM and AITA, respectively. Indicating the\nLLM and dataset dependent performance of BM25\nas a proxy performance predictor. This result also\nfollows from the poor calibration performance of\nBM25 on AITA in Table 5.\nNote\nthat\nwe\ndon\u2019t\nreport\nresults\nwith\ndavinci-003 since our procedure for learning a\nthreshold \u03b8 for selective revision failed to find a\nthreshold where dev set performance was improved\nfrom selective revision. Finally note that metrics\nreported for selective revision in Tables 4 and 6\naren\u2019t directly comparable to those of Tables 1, 3,\n\u0007\n\u0004\nGiven a POST from the subreddit Am I the\nAsshole and a DRAFT comment from the USER\nresponding if the author of the POST is\nan asshole or not the asshole , edit the\nDRAFT comment.\nUse the following instructions for your\nresponse:\n1. Maintain consistency in tone and\nstyle with the USER 's historical\ncomments.\n2. Edit the DRAFT to use more reddit\nlingo.\n3. Remove statements of the POST from the\nDRAFT.\n4. Output a justification for your edits\nstarting with the word JUSTIFICATION.\n5. Output the edited DRAFT comment\nstarting with the words EDITED DRAFT.\nHere are some historical comments by the\nUSER: {{historical_examples}}\nREQUEST: {{target_request}}\nDRAFT: {{target_draft}}\nOutput a justification for your edits ,\nthen output the edited DRAFT starting\nwith the words EDITED DRAFT.\n\u0006\n\u0005\nPrompt 8: fLLM prompt used to for selective revision\ngiven a Stage 1 draft for AITA.\nand 7 since they represent different LLM runs and\nexclude a dev set from WORKSM and AITA for\nlearning \u03b8 (50 and 200 instances respectively).\nC.3\nAlternative Training Strategies\nIn Section 4.3 we report results for alternative train-\ning strategies for fretr with gpt-35-turbo. We\npresent results of these exeperiments here with\ndavinci-003. Here, we see similar trends to those\nin Table 3 of \u00a74.3. Alternative training strategies\nperform even worse than our proposed approach\nthan in the case of gpt-35-turbo \u2013 indicating the\nvalue of PEARL for delivering consistent perfor-\nmance improvements.\nD\nExtended Related Work\nHaving discussed the closest body of related work\nin \u00a75 we discuss additional related work here.\nPrompt robustness in LLMs Simultaneous\nrouting and retrieval also relates our approach to\nwork ensuring that LLMs are robust to noisy re-\ntrievals. In augmenting a fixed LLM, prior ap-\nproaches ensured robustness by only using re-\ntrieved documents based on simple frequency\nfilters on entities mentioned in the input query\n(Mallen et al., 2023) or based on predictions from\nan NLI model that determines if the query entails\nthe retrieved contexts (Yoran et al., 2023). Other\nDataset \u2192\nWORKSM\nAITA\nMethod \u2193 / LLM \u2192\ndavinci-003\ndavinci-003\nBS-F1\nR1\nR2\nMacro F1(%)\nPEARL\n0.3960\n0.5419\n0.3094\n61.21\nno calibrated sm\n0.3888\n0.5350\n0.3033\n57.27\ndistill paux(tu|du, qu)\n0.3934\n0.5359\n0.3059\n55.52\np(relevant = 1|du, qu)\n0.3781\n0.5288\n0.2953\n59.59\nTable 7: PEARL compared to alternate training of fretr.\napproaches have sought to fine-tune the LLM to\nbe robust to irrelevant contexts (Li et al., 2023c;\nLuo et al., 2023; Yoran et al., 2023) or modify the\ndecoding procedure (Shi et al., 2023). In contrast,\nwe determine the quality of the input context based\non scale-calibrated retrieval model scores.\nLLM Chaining In selectively editing genera-\ntions with an LLM for low-performing requests,\nour approach also relates to recent work on com-\nposing LLMs with other models to build more com-\nplex systems (Wu et al., 2022; Arora et al., 2023;\nKhattab et al., 2023). Close work is presented by\napproaches that leverage repeated LLM calls to\nverify the reasoning or factuality of previous gen-\nerations (Shridhar et al., 2023; Dhuliawala et al.,\n2023). In contrast, our work leverages an efficient\nretrieval model to selectively direct low-performing\ngenerations for further revision, reducing the total\nnumber of expensive LLM calls necessary. In this\nrespect, our approach bears similarity to Zhang\net al. (2023b), who progressively run larger LLMs\nonly when necessary for an input.\nWriting Assistants A sizable body of work\nhas explored the development of writing assis-\ntants. Compared to assistants for communication\napplications, these have been targeted at authors\nof creative texts like screenplays (Mirowski et al.,\n2023), stories (Akoury et al., 2020), and poems\n(Gon\u00e7alo Oliveira, 2017) \u2013 consequently, they fo-\ncus on diverse generations and long-range coher-\nence, rather than personalization. Further, while\nour work leverages a request-driven assistant, prior\nsystems have used a variety of interaction and con-\ntrol methods. While text completion presents a\ncommon form of interaction (Clark et al., 2018),\nrecent work has seen use of infilling, tag-based\ncontrol (Sun et al., 2021), and instruction guided\ngenerations (Chakrabarty et al., 2022) \u2013 a deeper\nexamination of control and interaction strategies\nand their trade offs are presented in related reviews\n(Zhang et al., 2023a; Lin et al., 2023). While our ap-\nproach to personalization may be extended to some\nalternative interaction paradigms, other interaction\ntechniques are likely to necessitate non-trivial ex-\ntensions to our work.\nPersonalized Text Generation While we have\nfocussed on author personalization that aims to\nmimic stylistic patterns, interests, and knowledge\nof an author, we briefly review reader-personalized\ntext generation \u2013 a setup aiming to generate texts\nthat are engaging and relevant to readers\u2019 pref-\nerences. This has historically been explored for\ngenerating personalized reviews (Ni et al., 2017),\nrecipes (Majumder et al., 2019), news headlines\n(Ao et al., 2021) and in dialogue agents (Mazar\u00e9\net al., 2018; Zhang et al., 2018). Related work is\nalso found in text simplification and lay summariza-\ntion in the context of scientific text \u2013 this work has\nexplored generating definitions for scientific con-\ncepts at varying levels of complexity (August et al.,\n2022; Murthy et al., 2022) or summarizing scien-\ntific text for lay readers (Guo et al., 2021). While\nrecent work has explored this with modern LLMs\n(Li et al., 2023d; Farajidizaji et al., 2023), reader\npersonalization remains an understudied problem\nand presents a rich area for future work.\n\u0007\n\u0004\n## TASK\nGiven an enterprise social media post , generate a set of writing instructions that\nexplain how to\n\"reverse -engineer \"; the post. Use the following steps:\n- The instructions must give a high -level overview of what the post aims to\ncommunicate. Example: [readcted]\n- The instructions must include specific proper nouns (people , places , organzations)\n. Example: [redacted]\n- The instructions must include contact information if available. Example: [redacted\n]\n- The instructions must include specific links to websites or files if available.\nExample: [redacted]\n- The instructions must contain any knowledge that is highly specialized and is\nlikely to be only known to the individual who wrote the post , if available. Example:\n[redacted]\n- The instructions must contain rough sketches of any personal anecdotes in the post\n, if available. Example: [redacted]\n- The instruction must **not** contain any formatting or ordering information from\nthe post.\n## OUTPUT\nOutput the following:\n<Explanation >{ explanation of your reasoning for how you generated the instructions ,\nin 3 sentences or fewer}</Explanation >\n<Instruction.Overview >{1-2 sentences overview of what the post aims to communicate\n}</ Instruction.Overview >\n<Instruction.Names >{1-2 sentences about the people , places , or organizations\nmentioned in the post , _NONE_ if not applicable }</ Instruction.Names >\n<Instruction.Contacts >{1-2 sentences about the contact information copied verbatim\nin the post , _NONE_ if not applicable }</ Instruction.Contacts >\n<Instruction.Links >{1-2 sentences including the links copied verbatim from the post ,\n_NONE_ if not applicable }</ Instruction.Links >\n<Instruction.Knowledge >{1-2 sentences paraphrasing the specialized knowledge\nincluded in the post , _NONE_ if not applicable }</ Instruction.Knowledge >\n<Instruction.Anecdotes >{1-2 sentences paraphrasing the anecdotes included in the\npost , _NONE_ if not applicable}</ Instruction.Anecdotes >\n## INPUT\n{{input_post}}\n\u0006\n\u0005\nPrompt 9: GPT4 prompt used to generate synthetic requests for WORKSM posts in our training set.\n\u0007\n\u0004\nOverview:\nIn this study , we are developing LLM -based approaches for writing\nsocial media posts on enterprise social networks. Your task is as\nfollows: Given a social media post from an enterprise social media\nplatform , write a short outline of the post. In writing your outline ,\nimagine you are a manager , social media manager , or event organizer\nwriting a rough sketch of the post with the key information you would\nlike to share.\nData Format:\nYou are given a spreadsheet consisting of ~150 English posts. Each\nrow corresponds to a single post. The spreadsheet contains the\nfollowing columns: PostId , InputPost , OutputShortOutline. The first\ncolumn is the ID of the post; you can ignore this column. The second\ncolumn is the full text of the input post. In the third column , you\nwill write your short outline based on the input post.\nDO\u2019s for your outline:\nWhen writing your short outline , do include the following:\n- One sentence about the goal of the post: Include a brief\ndescription of what the post is trying to communicate. Example: [\nredacted]\n- Specific proper nouns (people , places , things): Include names of\nspecific people , places , or things in your outline. Example: [\nredacted]\n- Specialized knowledge: If the knowledge contained in the post is\nhighly specialized and is likely to be only known to the individual\nwriting the post , include a rough sketch of that information in your\noutline. Example: [redacted]\n- Personal anecdotes: If the post contains specific personal\nanecdotes , include a rough sketch of that information in your outline\n. Example: [redacted]\n- Special emphasis or call to action: If the post makes a special\nemphasis , include a rough sketch of that emphasis or call to action\nin your outline. Example: [redacted]\n- External website links: If the post links to an external website ,\ninclude the link in your outline. Example: [redacted]\nDONT\u2019s for your outline:\nWhen writing your short outline , do not include the following:\n- Anything related to the ordering of content.\n- Formatting instructions.\n- Any verbatim text other than specific proper nouns.\n\u0006\n\u0005\nGuideline 1: Instructions provided to annotators for authoring requests for our evaluation set in WORKSM.\n\u0007\n\u0004\nEnterprise Social Media\nTask: Consider two output texts produced in response to a request ,\none from a baseline method and one from a proposed method and enter\nwhich of the two is more similar to a target text.\n- The baseline and proposed outputs are labelled with output1 and\noutput2 at random. In the preference column enter 1 or 2 to indicate\nif you prefer output1 or output2.\n- If both output1 and output2 are equally good or equally bad , and a\nchoice of \"more similar to target\" cannot be made then enter a 0 in\nthe preference column.\nIf the outputs are bad in significant (based on the below dimensions)\nbut different ways its still okay to say 0.\n- If possible , enter a few word rationale in the rationale column for\nyour judgement.\nDimensions of similarity to consider in rating one of two generated texts\nas more similar to a target text. In order of importance:\n1. Tone of the posts: Things like: the level of excitement in the\npost or the target texts sentiment to the main topic of the post. Eg\nif a post is about endorsing a product or movie to the readers , how\nstrong is their endorsement?\n2. Information content: Is all of the most important (as emphasized\nby the target text) information present in the target present in the\ngeneration? a. Note: Pay attention to the request here. If the\nrequest is underspecified the generations should not be penalized as\nmuch.\n3. Structural elements of style: Things like: a. How does the target\nbegin? (with a greeting , with a story , with a catchy phrase) b. How\ndoes the target end? (with a signature , a rallying call , a hashtag) c\n. What is the pattern of use of quotes , hashtags , punctuation , etc.?\n4. Order of the information in posts: Is the order/structure of the\ncontent similar in the reference and the generation.\nFailure points in output1 or output2 which may require selection of the\nother as the preferred generation if all dimensions of similarity are\nsimilar between output1 and output2. In order of importance:\n1. Factual errors: If the output contains important information not\npresent in the request (eg a meeting date or name not present in the\nrequest) penalize this strongly.\n2. Extra information: If the output contains redundancies or\nirrelevant text (eg many unmentioned hashtags) lean toward penalizing\nthis if all the above criteria are similar between the two texts.\n2a. Note: Even if the text contains relevant additional text which is\nabsent from the target post lean toward penalizing this - similarity\nto the target is more important than the \"best possible\" output.\n3. Minor nonsensical generations: If one of the generated outputs is\nnot sensical choose the other output. Eg. [redacted]\n3a. Note: nonsensical generations may be more acceptable to users\nsince they are aware the text is machine generated than extra\ninformation copied over from their historical posts which may erode\ntrust about privacy of their data etc. So treat minor nonsensical\ngenerations more leniently than the first two error cases.\n\u0006\n\u0005\nGuideline 2: Instructions provided to raters for manual evaluation for WORKSM.\n\u0007\n\u0004\nAITA\nTask: Consider two output comments produced in response to a target\npost , one from a baseline method and one from a proposed method and\nenter which of the two is more similar to a target comment.\n- The baseline and proposed outputs are labelled with output1 and\noutput2 at random. In the preference column enter 1 or 2 to indicate\nif you prefer output1 or output2.\n- If both output1 and output2 are equally good or equally bad , and a\nchoice of \"more similar to target\" cannot be made then enter a 0 in\nthe preference column. If the outputs are bad in significant (based\non the below dimensions) but different ways its still okay to say 0.\nAITA output comments often differ from the target comment while being\nsimilar in stance - if both outputs are different from the target\nand different from each other still rate them a tied (i.e \"equally\nbad\")\n- If possible , enter a few word rationale in the rationale column for\nyour judgement.\nDimensions of similarity to consider in rating one of two generated texts\nas more similar to a target text. In order of importance:\n1. Stance and tone of the comments: While the stance of the output\ncomment compared to the target comment should be the same , also pay\nattention to the rest of the comment. Is the tone of the remainder of\nthe comment similar?\n1a. Note: There are rare cases when the output comment can mention\nthe right stance but be completely different in tone and argument -\nin these cases disregard the stance since the classification eval\nalready evaluates the stance.\n2. The argument made in the comment: Is the argument made in the\ncomment for the stance similar to that made in the target comment? Is\nthe subject of the criticism the same in the output and the target?\n3. Structural similarities: Like choice of words , pattern of\npunctuation etc.\n3a. Note: Output comments are often dissimilar to the target in style\n. Its okay to focus more on the first two aspects in this rating.\nFailure points in output1 or output2 which may require selection of the\nother as the preferred generation if all dimensions of similarity are\nsimilar between output1 and output2:\n1. Logical errors - if the argument provided makes very little sense\nlean toward the comment which makes more sense.\n1. a. Note: In the case where one of the outputs contains a\nnonsensical argument and the other is logical but not significantly\nsimilar to the target comment rate them as tied i.e equally bad.\n\u0006\n\u0005\nGuideline 3: Instructions provided to raters for manual evaluation for AITA.\n"
  },
  {
    "title": "SiRA: Sparse Mixture of Low Rank Adaptation",
    "link": "https://arxiv.org/pdf/2311.09179.pdf",
    "upvote": "7",
    "text": "SiRA: Sparse Mixture of Low Rank Adaptation\nYun Zhu\u2217\nNevan Wichers\u2217\nChu-Cheng Lin\u2217\nXinyi Wang\u2217\nTianlong Chen\u2020\nLei Shu\u2217\nHan Lu\u2217\nCanoee Liu\u2217\nLiangchen Luo\u2217\nJindong Chen\u2217\nLei Meng\u2217\nGoogle\u2217, CSAIL@MIT\u2020\nAbstract\nParameter Efficient Tuning has been an promi-\nnent approach to adapt the Large Language\nModel to downstream tasks.\nMost previous\nworks considers adding the dense trainable\nparameters, where all parameters are used to\nadapt certain task. We found this less effective\nempirically using the example of LoRA that\nintroducing more trainable parameters does\nnot help.\nMotivated by this we investigate\nthe importance of leveraging \u201csparse\u201d compu-\ntation and propose SiRA: sparse mixture of\nlow rank adaption. SiRA leverages the Sparse\nMixture of Expert(SMoE) to boost the perfor-\nmance of LoRA. Specifically it enforces the\ntop k experts routing with a capacity limit re-\nstricting the maximum number of tokens each\nexpert can process. We propose a novel and\nsimple expert dropout on top of gating network\nto reduce the over-fitting issue. Through ex-\ntensive experiments, we verify SiRA performs\nbetter than LoRA and other mixture of expert\napproaches across different single tasks and\nmultitask settings.\n1\nIntroduction\nLarge Language Models (Thoppilan et al., 2022;\nPassos et al., 2023; Ouyang et al., 2022; Touvron\net al., 2023) (LLMs) have demonstrated impres-\nsive capabilities in a wide range of tasks.\nYet\nto adapt these general-purpose models to down-\nstream low resource tasks is especially important.\nTo this end parameter efficient tuning (PET) (Hu\net al., 2021; Li and Liang, 2021; Lester et al.,\n2021; Houlsby et al., 2019; Zhang et al., 2023;\nZaken et al., 2021; Chen et al., 2022), which in-\ntroduces task specific weights to the frozen foun-\ndation model for gradient descent, avoids catas-\ntrophic forgetting (Luo et al., 2023) of fine-tuning\nand offers better quality and lower cost than in-\ncontext learning (Liu et al., 2022).\nLoRA (Hu\net al., 2021) is a widely adopted PET method\n\u2217Correspondence to yunzhu@google.com .\nwhich achieves high performance leveraging low-\nrank matrices.\nOne question for user of LoRA or other PET\napproach is how much trainable parameter should\nbe used.\nIn the case of LoRA, it is controlled\nby the rank of the low-rank matrices.\nAnd by\nincreasing this value more computation could be\nprovided to fit specific tasks. However it has been\nshown higher rank matrices will not introduce bet-\nter quality to the model due to the instability of\ntraining (Chen et al., 2022), which we verify in\nFigure 2 in Appendix 8.1. This poses a hidden bot-\ntleneck for the quality of the model even when we\nhave enough computation budget, and it remains a\nchallenge to improve the quality of LoRA.\nOn the other hand, leveraging Sparse Mixture\nof Experts in neural networks has been exten-\nsively studied as a replacement for FeedForward\nNetworks, with different approaches to find the\noptimal assignment between experts and tokens,\nincluding reinforcement learning (Bengio et al.,\n2015), linear programs (Lewis et al., 2021), fixed\nrules (Roller et al., 2021), top-1 gating (Fedus\net al., 2022), top-2 gating (Lepikhin et al., 2020),\ntop-k gating (Shazeer et al., 2017), reverse ex-\npert choosing (Zhou et al., 2022), and soft as-\nsignment (Puigcerver et al., 2023).\nSeveral re-\ncent works have proposed to utilize mixture-of-\nexpert models on top of parameter-efficient tun-\ning (Wang et al., 2022; Zadouri et al., 2023).\nHowever, prior works overlooked the potential of\nsparse MoE (SMoE), arguably because of issues\nlike token dropping (Puigcerver et al., 2023) and\noverfitting (Elbayad et al., 2022).\nTo this end, we investigate leveraging \u201csparse\u201d\ncomputation for PET in this paper and propose\nSiRA, the Sparse Mixture of Low Rank Adap-\ntation.\nSiRA enforces the top k experts rout-\ning, which improves resource and computation\nutilization and also facilitates more fine-grained\ncapacity control of a given input. SiRA consists\narXiv:2311.09179v1  [cs.CL]  15 Nov 2023\nof three important ingredients: the capacity con-\nstraint which embraces token dropping, an auxil-\niary loss to penalize over-utilizing few experts, and\na novel expert dropout mechanism. They work to-\ngether to ensure the proper load balancing and ad-\ndress the over-fitting issue.\nWe conducted extensive experiments to verify\nthe performance of SiRA, which achieves better\nperformance than LoRA (Hu et al., 2021) and\nits MoE variants Adamix (Wang et al., 2022),\nMoLoRA (Zadouri et al., 2023) across wide range\nof single task and multitask benchmarks.\nOur\nablation study showed that the number of used\nexperts and capacity per expert improves perfor-\nmance which demonstrates advantage of being\n\u201csparse\u201d. Notably the expert dropout plays an im-\nportant role, and it is more effective than SMoE-\ndropout (Chen et al., 2023a).\n2\nRelated Works\nParameter Efficient Tuning (PET)\nParameter\nEfficient Tuning has a variety of flavors such as\nAdapters (Houlsby et al., 2019), Prefix Tuning (Li\nand Liang, 2021; Liu et al., 2021), Prompt Tun-\ning (Lester et al., 2021), P-tuning (Liu et al.,\n2023), attention-injection (Zhang et al., 2023),\nLoRA (Hu et al., 2021; Dettmers et al., 2023), and\ncombinations of PET methods (Mao et al., 2021).\nIn this paper, our focus on LoRA as it has been\nfound to achieve better results, although the meth-\nods could be applied to other flavors as well.\nMoE for PET methods\nAlong the intersection\nof PET and MoE, Adamix (Wang et al., 2022) and\nMoLoRA (Zadouri et al., 2023) are most similar\nto our work. Adamix randomly chooses an ex-\npert in training and averages all the experts dur-\ning inference. Albeit efficient, this method is more\nlike checkpoint averaging (Gao et al., 2022) be-\ncause the experts are randomly chosen they don\u2019t\nlearn to specialize.\nMore importantly, the ran-\ndom approach has significantly longer training\ntime, which is multiplied by the number of ex-\nperts used. MoLoRA applies the full soft MoE on\nthe top of LoRA, where all experts are averaged\nusing a learned gating. Compared to this work,\nour method can achieve better efficiency. Firstly,\nSiRA does not need longer time to train compared\nto standard LoRA, thanks to the quick conver-\ngence of SMoE (Fedus et al., 2022). Secondly, the\nsparsity is enforced in SiRA which saves the train-\ning resource and inference computation compared\nto MoLoRA.\nAnother track of the MoE work is for multitask-\ning, such as Task-MoE (Kudugunta et al., 2021)\nand Skill Selection (Ponti et al., 2023). These ap-\nproaches assume the external task-id as an extra\ninput for training and inference. Although we ex-\nperiment with MoE in multitask settings, it does\nnot require the task-id of inputs.\n3\nSparse Mixture of Low Rank\nAdaptation\nTo increase the capacity of LoRA (Hu et al.,\n2021) using Mixture of Experts (MoE) without\nadding too much computational cost, we propose\nSparse Mixture of Experts of Low Rank Adapta-\ntion (SiRA), which leverages multiple lightweight\nLoRA adaptors as experts while enforcing sparsity\nwhen using the experts.\nFigure 1 shows an illustration of SiRA. The\nMoE layer for the adapter consists of E experts,\neach with their own LoRA weights, W1, ..., WE.\nWk is the product of two low rank matrices Wk =\nBkAk. We also assume the base foundation model\nhas W0 as it is frozen weight, which represents ei-\nther query, key, value, or output projection. Next,\nwe will introduce each component of our frame-\nwork that enforces the sparsity of LoRA experts.\nExpert Gating\nTo reduce the computational\ncost, SiRA only actives a subset of all the expert\nmodules. Formally, during each forward pass, we\nselect K out of E experts using the output scores\nof a gating network \u03b8g. The process is mathemat-\nically expressed as Equation (1) and (2), where s\ndenote the token index of the sequence x and Gs,e\nis the gating network output at s-th token e-th ex-\nperts.\nG(xs) = TopK(softmax(\u03b8T\ng xs))\n(1)\nys =\nE\nX\ne=1\nGs,eWe(xs) + W0(xs)\n(2)\nExperts Dropout\nWe propose a practical way to\nmake the gate more balanced with a simple gate\ndropout. Specifically, we introduce dropout to the\ngating output G as shown in Equation 3.\nG(xs) = TopK(Dropout(softmax(\u03b8T\ng xs)))\n(3)\nAttention Projection Layer\nTopk \nAttention \nProjection\nWeights\nWeighted Sum\nAdd\n Used Capacity\n\u2026\u2026\nLoRA weights\n\u2026\u2026\nOverflow\nExpert \nDropout \nGate\nFigure 1: SiRA: Sparse Gated Mixture of LoRA.\nExpert Token Capacity\nWe enforce the capac-\nity constraints for experts following GShard (Lep-\nikhin et al., 2020). Specifically, we restrict that the\nnumber of tokens processed by each expert should\nnot exceed a predefined threshold. Once the ca-\npacity is reached, one expert simply drop the over-\nflow tokens. If all K experts reach their token ca-\npacity before all tokens in a training example is\nprocessed, the rest of the tokens will only be en-\ncoded using the frozen model parameter W0.\nAuxiliary Loss\nWe define an auxiliary loss term\nto further encourage load balancing among differ-\nent experts (Shazeer et al., 2017). We define the\ntotal number of tokens to be S, and there is E\nexperts. We denote ce as the number of tokens\nrouted to expert e. By using the mean gates per\nexpert me = Means(Dropout(softmax(\u03b8T\ng xs)))\nas a differentiable approximation, we define the\naux loss in Equation 4.\nlaux = 1\nE\nE\nX\ne=1\nce\nS \u2217 me\n(4)\n4\nExperiments\n4.1\nEvaluation Setup\nBaselines and Experiment Configs\nWe specif-\nically compare our model with the standard\nLoRA (Hu et al., 2021), Adamix (Wang et al.,\n2022) and MoLoRA (Zadouri et al., 2023). Note\nthat other adapter approaches are not commpared\nwith as the SiRA approach is orthogonal and could\nbe applied on top of them as well. We choose the\nPALM2-FLAN XXS (Passos et al., 2023) as the\nfoundation model. We follow the default configu-\nrations in (Hu et al., 2021) to inject LoRA weights\non the attention projections and set the intrinsic\nrank as 4. We use 16 experts by default across all\nbaselines. For training config and model selection,\nsee Appendix 8.2.\nDatasets and Metrics\nWe evaluate on the fol-\nlowing datasets:1\nXTREME-UP\nThe\nXTREME-UP\ndataset\n(Ruder et al., 2023) is a multilingual multitask\ndataset, with a focus on the scarce-data scenarios\nof underrepresented languages. In this work, we\nchoose two of the underrepresented languages\u2014\nSwahili(SW) and Bengali(BN)\u2014and evaluated on\nseveral NLP tasks where these two languages have\ntraining and evaluation data.\nWe follow Ruder\net al. (2023) for each task\u2019s splits and evaluation\nmetrics.\nFinQA FinQA (Chen et al., 2021) is a QA\ndataset in the financial domain.\nComplex rea-\nsoning capabilities are needed to correctly answer\nthese questions.\nNote that the answers of the\nFinQA dataset are programs of a special arithmetic\nDSL. In this work we only evaluate on metrics\nbased on surface form matching, i.e., exact match\nand F1 scores.\nForumSum ForumSum (Khalman et al., 2021)\nis a diverse and high quality conversation sum-\nmarization dataset with human written summaries\nwhere the conversations are collected from a\nwide variety of internet forums.\nWe report\nBLEURT (Sellam et al., 2020), ROUGEL, and F1\nscores.\n4.2\nPerformance of SiRA\nWe evaluate all the single tasks performance in\nTable 1. Note that as FinQA is a hard task with\nfinancial reasoning, thus the exact match and f1\nscore is relatively low. We can notice that SiRA\nis outperforming all other baselines at most of the\ntasks, with less than 1% extra parameters. No-\ntably when compared to MoLoRA, SiRA achieves\nconstantly better performance among all the tasks.\nThis demonstrates that \u201csparse\" MoE is better than\n\u201cfull\" MoE. Adamix shows some small advantage\non the Semantic Parsing task, but overall loses to\nSiRA across all other tasks.\nWe also conducted experiments on two multi-\ntask settings on language swahili (SW) and ben-\ngali(BN), and two multiligual settings for QA in\nlanguages task and QA across languages task. We\n1Since our base model (Chung et al., 2022) had been ex-\nposed to many public datasets during training, we choose\ndataset that are not consumed yet.\nTable 1: Performance Comparison For Single Tasks\nApproach \u03b4 Params FinQA (EN)\nForumSum (EN)\nSP (SW) QA-in (SW) NER (SW) SP (BN) QA-in (BN) QA-cross (BN)\nem\nf1\nbleurt rougeL\nf1\naccuracy\nf1\nspan-f1\naccuracy\nf1\nf1\nLoRA\n0.043%\n5.0\n5.6\n96.70 33.97 23.54\n27.63\n82.08\n88.95\n33.52\n80.34\n76.81\nAdamix\n0.689%\n5.6\n6.0\n95.95\n35.10\n23.88\n33.22\n81.24\n89.00\n39.03\n81.70\n76.07\nMoLoRA\n0.746%\n5.6\n6.4\n97.05\n34.37\n24.79\n32.50\n82.33\n89.33\n36.28\n79.06\n76.75\nSiRA\n0.746%\n5.8\n6.6\n97.14\n35.67\n25.83\n32.52\n83.00\n89.95\n38.61\n82.10\n76.93\nTable 2: Performance Comparison For Multi Tasks\nApproach \u03b4 params\nSW Multitask\nBN Multitask\nSP(accuracy) QA-in(f1) NER(span-f1) Average SP(accuracy) QA-in(f1) QA-cross(f1) Average\nLoRA\n0.043%\n28.06\n77.71\n88.28\n64.69\n32.06\n79.27\n75.03\n62.12\nAdamix\n0.689%\n35.14\n76.99\n89.01\n67.10\n38.41\n79.49\n75.09\n64.33\nMoLoRA 0.746%\n33.44\n79.91\n88.92\n65.66\n35.98\n78.14\n76.37\n63.49\nSiRA\n0.746%\n33.98\n81.26\n89.04\n68.10\n37.71\n82.17\n75.50\n65.13\nTable 3: Performance Comparison For Multilingual\nTasks.\nApproach\n\u03b4 params\nQA-in (9)\nQA-cross (25)\nLoRA\n0.043%\n85.09\n69.41\nAdamix\n0.689%\n84.75\n70.42\nMoLoRA\n0.746%\n85.14(WIP)\n69.70(WIP)\nSiRA\n0.746%\n86.38\n70.86\nTable 4: Self ablations on the hyper-parameter topK(K)\nand expert capacity(C) on ForumSum.\nConfigs\nbleurt\nrougeL\nf1\nK=2\n96.87\n34.51\n24.73\nK=4\n96.60\n34.66\n25.34\nK=6\n96.75\n34.73\n24.55\nK=8\n96.76\n35.31\n25.64\nK=10\n97.51\n35.10\n25.19\nK=12\n96.96\n34.49\n24.24\nC=2\n96.33\n34.15\n24.13\nC=4\n96.60\n34.66\n25.34\nC=6\n97.14\n35.67\n25.83\nC=8\n97.31\n34.97\n25.24\nC=10\n97.25\n34.75\n25.57\nC=12\n96.50\n34.44\n23.94\nreported numbers in Table 2 and Table 3 respec-\ntively.\nThe overall trend is similar to what we\nfound in the single tasks. SiRA achieves the best\naverage performance among all baselines.\n4.3\nAblation Study\nComputation Ablations\nWe share the ablations\non ForumSum in Table 4. We choose a simple con-\nfig as base (k=4, C=4) and then change each of\nthem while keeping the rest. We first range the top\nK from 2 to 12, with capacity C = K. And then\nwe fix the K = 4, and range the expert capacity\nfrom 2 to 12. An interesting finding is increasing\nthe computation or the capacity will not always in-\nTable 5: Gating ablations on ForumSum.\nApproach\nbleurt\nrougeL\nf1\nSiRA\n97.14\n35.67\n25.83\n- aux loss\n96.37\n35.09\n25.11\n- Expert Dropout\n97.09\n34.73\n24.55\n+ SMoE-Dropout\n96.30\n34.24\n24.32\ncrease the scores and there is a \u201ccomfortable zone\u201d\nwhich we need to find out with model tuning. This\nalso justifies why the \u201cfull\u201d MoE based approach\nis not as good as SiRA. SiRA provides more fine-\ngrained control on the computation.\nGating ablations\nWe also provide the ablations\non the gating in Table 5. Specifically we compare\nSiRA with 3 cases: 1/ removing the aux loss, 2/ re-\nmoving the gate dropout, and 3/ using a static rout-\ning based dropout SMoE-Dropout (Chen et al.,\n2023a) instead. Results suggested that the learned\ngating is still better than a fixed one, and both the\ngate dropout and aux loss help the performance of\nSiRA.\nWhat the gate learns\nWe use the Swahili mul-\ntitask experiment to study what the gate is learn-\ning. We measure the average entropy of each gate\nweight distribution before TopK is applied. The\naverage entropy for the QA (in language) task de-\ncreases from 1.6 to 1.13 nats during training. This\nindicates that the model learns to give certain gates\nmore weight as it trains.\nWe also measure the average correlation coeffi-\ncients between each task index and each gate in-\ndex similar to (Chen et al., 2023b). We convert the\ntask index to a one hot encoding for this. At the\nend of training, the average correlation was about\n.025, which is not significant. The correlation be-\ntween gates and languages in the multilingual ex-\nperiment is not significant either. This suggests\nthat our gating mechanism does not learn to route\ndifferent tasks to different gates.\n5\nTo-Do List\nThis manuscript is currently under active devel-\nopment. Our upcoming endeavors include getting\nmore results and analysis, and improving the writ-\nings. We warmly welcome suggestions, insights,\nand constructive criticism from the research com-\nmunity. Should you have any feedback or ideas\nthat could potentially enhance the quality and im-\npact of our work, please do not hesitate to contact\nthe lead author. Your input is invaluable to us, and\nwe are eager to integrate diverse perspectives to\nrefine and advance our research.\n6\nConclusion\nThis paper introduced SiRA, a Sparse Mixture of\nExpert variant of LoRA. SiRA enforces the top k\nexperts routing with capacity constraint for each\nexperts. We also devise a novel expert dropout\nmechanism on top of the auxiliary loss to re-\nduce its over-fitting issue. We conducted extensive\nexperiments to verify the performance of SiRA,\nwhich achieves better performance than the LoRA\nand its MoE variants across different single tasks\nand multitask settings.\n7\nLimitation\nSiRA is taking extra serving overhead for serving\nwith extra parameters on experts and the gating,\ncompared to LoRA or Adamix. How to minimize\nthe serving overhead is a challenge problem which\nwe hope to address our future works.\nAcknowledgements\nWe would like to acknowledge Abhanshu Sharma,\nHassan Mansoor, Qifei Wang, Victor C\u02d8arbune etc\nfor their valuable inputs.\nReferences\nEmmanuel Bengio, Pierre-Luc Bacon, Joelle Pineau,\nand Doina Precup. 2015. Conditional computation\nin neural networks for faster models. arXiv preprint\narXiv:1511.06297.\nGuanzheng Chen, Fangyu Liu, Zaiqiao Meng, and\nShangsong Liang. 2022.\nRevisiting parameter-\nefficient tuning: Are we really there yet?\narXiv\npreprint arXiv:2202.07962.\nTianlong Chen, Zhenyu Zhang, Ajay Jaiswal, Shiwei\nLiu, and Zhangyang Wang. 2023a. Sparse moe as\nthe new dropout: Scaling dense and self-slimmable\ntransformers. arXiv preprint arXiv:2303.01610.\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\nMatt Beane, Ting-Hao Huang, Bryan Routledge,\nand William Yang Wang. 2021. FinQA: A dataset of\nnumerical reasoning over financial data. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 3697\u20133711,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nZitian Chen, Yikang Shen, Mingyu Ding, Zhenfang\nChen, Hengshuang Zhao, Erik G. Learned-Miller,\nand Chuang Gan. 2023b.\nMod-squad: Designing\nmixtures of experts as modular multi-task learners.\nIn IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR 2023, Vancouver, BC,\nCanada, June 17-24, 2023, pages 11828\u201311837.\nIEEE.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robin-\nson, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V.\nLe, and Jason Wei. 2022.\nScaling instruction-\nfinetuned language models.\nTim Dettmers,\nArtidoro Pagnoni,\nAri Holtzman,\nand Luke Zettlemoyer. 2023.\nQlora:\nEfficient\nfinetuning of quantized llms.\narXiv preprint\narXiv:2305.14314.\nMaha Elbayad, Anna Sun, and Shruti Bhosale. 2022.\nFixing moe over-fitting on low-resource languages\nin multilingual machine translation. arXiv preprint\narXiv:2212.07571.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. The Jour-\nnal of Machine Learning Research, 23(1):5232\u2013\n5270.\nYingbo Gao, Christian Herold, Zijian Yang, and Her-\nmann Ney. 2022.\nRevisiting checkpoint averag-\ning for neural machine translation. arXiv preprint\narXiv:2210.11803.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning,\npages 2790\u20132799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nMisha Khalman, Yao Zhao, and Mohammad Saleh.\n2021.\nForumSum: A multi-speaker conversation\nsummarization dataset. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2021,\npages 4592\u20134599, Punta Cana, Dominican Repub-\nlic. Association for Computational Linguistics.\nSneha Kudugunta, Yanping Huang, Ankur Bapna,\nMaxim Krikun, Dmitry Lepikhin, Minh-Thang Lu-\nong, and Orhan Firat. 2021.\nBeyond distillation:\nTask-level mixture-of-experts for efficient inference.\narXiv preprint arXiv:2110.03742.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2020.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding.\narXiv preprint\narXiv:2006.16668.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. Base layers:\nSimplifying training of large, sparse models. In In-\nternational Conference on Machine Learning, pages\n6265\u20136274. PMLR.\nXiang Lisa Li and Percy Liang. 2021.\nPrefix-\ntuning: Optimizing continuous prompts for gener-\nation. arXiv preprint arXiv:2101.00190.\nHaokun Liu, Derek Tam, Mohammed Muqeeth, Jay\nMohta, Tenghao Huang, Mohit Bansal, and Colin A\nRaffel. 2022.\nFew-shot parameter-efficient fine-\ntuning is better and cheaper than in-context learn-\ning.\nAdvances in Neural Information Processing\nSystems, 35:1950\u20131965.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam,\nZhengxiao Du, Zhilin Yang, and Jie Tang. 2021. P-\ntuning v2: Prompt tuning can be comparable to fine-\ntuning universally across scales and tasks.\narXiv\npreprint arXiv:2110.07602.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2023. Gpt\nunderstands, too. AI Open.\nYun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie\nZhou, and Yue Zhang. 2023.\nAn empirical study\nof catastrophic forgetting in large language mod-\nels during continual fine-tuning.\narXiv preprint\narXiv:2308.08747.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Ma-\ndian Khabsa. 2021.\nUnipelt:\nA unified frame-\nwork for parameter-efficient language model tuning.\narXiv preprint arXiv:2110.07577.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural In-\nformation Processing Systems, 35:27730\u201327744.\nAlex Passos, Andrew Dai, Bryan Richter, Christopher\nChoquette, Daniel Sohn, David So, Dmitry (Dima)\nLepikhin, Emanuel Taropa, Eric Ni, Erica Mor-\neira, Gaurav Mishra, Jiahui Yu, Jon Clark, Kathy\nMeier-Hellstern, Kevin Robinson, Kiran Vodrahalli,\nMark Omernick, Maxim Krikun, Maysam Mous-\nsalem, Melvin Johnson, Nan Du, Orhan Firat, Paige\nBailey, Rohan Anil, Sebastian Ruder, Siamak Shak-\neri, Siyuan Qiao, Slav Petrov, Xavier Garcia, Yan-\nping Huang, Yi Tay, Yong Cheng, Yonghui Wu,\nYuanzhong Xu, Yujing Zhang, and Zack Nado.\n2023.\nPalm 2 technical report.\nTechnical report,\nGoogle Research.\nEdoardo Maria Ponti, Alessandro Sordoni, Yoshua\nBengio,\nand Siva Reddy. 2023.\nCombining\nparameter-efficient modules for task-level generali-\nsation. In Proceedings of the 17th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics, pages 687\u2013702.\nJoan Puigcerver, Carlos Riquelme, Basil Mustafa, and\nNeil Houlsby. 2023. From sparse to soft mixtures of\nexperts. arXiv preprint arXiv:2308.00951.\nStephen Roller, Sainbayar Sukhbaatar, Jason Weston,\net al. 2021. Hash layers for large sparse models. Ad-\nvances in Neural Information Processing Systems,\n34:17555\u201317566.\nSebastian Ruder, Jonathan H. Clark, Alexander Gutkin,\nMihir Kale, Min Ma, Massimo Nicosia, Shruti Ri-\njhwani, Parker Riley, Jean-Michel A. Sarr, Xinyi\nWang, John Wieting, Nitish Gupta, Anna Katanova,\nChristo Kirov, Dana L. Dickinson, Brian Roark,\nBidisha Samanta, Connie Tao, David I. Adelani,\nVera Axelrod, Isaac Caswell, Colin Cherry, Dan\nGarrette, Reeve Ingle, Melvin Johnson, Dmitry Pan-\nteleev, and Partha Talukdar. 2023.\nXtreme-up:\nA user-centric scarce-data benchmark for under-\nrepresented languages.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. Bleurt: Learning robust metrics for text gen-\neration. In Proceedings of ACL.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017.\nOutrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. arXiv\npreprint arXiv:1701.06538.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596\u20134604. PMLR.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\net al. 2022. Lamda: Language models for dialog\napplications. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, et al. 2023. Llama 2: Open foun-\ndation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nYaqing Wang, Sahaj Agarwal, Subhabrata Mukherjee,\nXiaodong Liu, Jing Gao, Ahmed Hassan Awadal-\nlah, and Jianfeng Gao. 2022.\nAdamix: Mixture-\nof-adaptations for parameter-efficient model tuning.\narXiv preprint arXiv:2210.17451.\nTed Zadouri, Ahmet \u00dcst\u00fcn, Arash Ahmadian, Beyza\nErmi\u00b8s, Acyr Locatelli, and Sara Hooker. 2023.\nPushing mixture of experts to the limit: Extremely\nparameter efficient moe for instruction tuning. arXiv\npreprint arXiv:2309.05444.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021.\nBitfit:\nSimple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. 2023. Llama-adapter: Efficient fine-tuning\nof language models with zero-init attention. arXiv\npreprint arXiv:2303.16199.\nYanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping\nHuang, Vincent Zhao, Andrew M Dai, Quoc V Le,\nJames Laudon, et al. 2022. Mixture-of-experts with\nexpert choice routing. Advances in Neural Informa-\ntion Processing Systems, 35:7103\u20137114.\n8\nAppendix\n8.1\nEffect of LoRA rank\nWe investigate the effect of LoRA rank in Fig-\nure 2.\n8.2\nTraining and Model selection\nDuring supervised finetuning, SFT, we use 8 Ten-\nsor Processing Units (TPU) V3 chips for fine-\ntuning. The batch size is 64, and the maximum\ntraining step is 30000. We use the Adafactor op-\ntimizer (Shazeer and Stern, 2018) with a learn-\ning rate of 0.0005.\nBoth the input and output\nsequence lengths are set to match the dataset re-\nquirements. The training dropout rate is 0.05. The\nTrainable Parameters (%)\nRougeL\n33\n34\n35\n36\n0.25\n0.50\n0.75\n1.00\n1.25\nSiRA\nMoLoRA\nAdamix\nLoRA\nFigure 2: SiRA vs LoRA on ForumSum Task. We in-\ncrease the rank of LoRA (rank=4, 8, 16, 32, 64, 128)\nand report the RougeL as a metrics. Notably increas-\ning the rank does does not help the performance. SiRA\n(rank=4) can achieve higher quality by leveraging the\nsparse mixture of experts.\nexpert dropout rate is set to 0.5. We did hyper-\nparameters search to find the best model configu-\nrations. We decode on the validation sets of each\ntask every 100 steps. And we report test results\nfrom the best checkpoints according to the valida-\ntion scores. For multitask results, the checkpoint\nis picked on the average each tasks metrics. For\nthe reported numbers in section 4.2, we use topk\nK = 4 as default. Yet we found K = 8 is bet-\nter for BN multitask and QA (in-lang) multilingual\nsetting, and K = 12 better for QA (cross-lang) ex-\nperiments.\n"
  },
  {
    "title": "UT5: Pretraining Non autoregressive T5 with unrolled denoising",
    "link": "https://arxiv.org/pdf/2311.08552.pdf",
    "upvote": "6",
    "text": "UT5: Pretraining Non autoregressive T5 with unrolled denoising\nMahmoud G. Salem, Jiayu Ye, Chu-Cheng Lin, Frederick Liu\nGoogle\n{* }@google.com\nAbstract\nRecent advances in Transformer-based Large\nLanguage Models have made great strides in\nnatural language generation. However, to de-\ncode K tokens, an autoregressive model needs\nK sequential forward passes, which may be\na performance bottleneck for large language\nmodels. Many non-autoregressive (NAR) re-\nsearch are aiming to address this sequentiality\nbottleneck, albeit many have focused on a ded-\nicated architecture in supervised benchmarks.\nIn this work, we studied unsupervised pretrain-\ning for non auto-regressive T5 models via un-\nrolled denoising and shown its SoTA results in\ndownstream generation tasks such as SQuAD\nquestion generation and XSum.\n1\nIntroduction\nLarge language models (LLMs) have revolution-\nized natural language processing (NLP) by enabling\nautomatic text generation and prediction. Tradi-\ntionally, language models are autoregressive: they\ngenerate a sequence of tokens one by one, condi-\ntioning each token on the previously generated ones.\nWhile this approach has led to impressive results\n(OpenAI, 2023; Anil et al., 2023), it suffers from\nslow inference due to its sequential nature. Sev-\neral studies (Gu et al., 2018; Ghazvininejad et al.,\n2019a) have explored the use of non-autoregressive\ngeneration for language modeling, where tokens\ncan be generated in parallel, without the need of\nconditioning on previously generated ones. Non-\nautoregressive generation has shown promising\nresults in terms of efficiency and speed, and has\nthe potential to be applied to various NLP tasks\n(Liu et al., 2020). Pretraining has proven the foun-\ndational procedure for autoregressive generation\n(Devlin et al., 2019; Radford et al., 2018). How-\never, few studies have focused on pretraining for\nnon-autoregressive language modeling for efficient\nlanguage generation . The main advantage of non-\nautoregressive generation is parallel generation of\nall tokens, making it faster than auto-regressive gen-\neration. However, non-autoregressive generation\nusually exhibits quality gaps when comparing with\nsimilar sized autoregressive models (Gu and Kong,\n2020).\nIn this paper, we propose a pretraining regime\nto improve the quality of non-autoregressive gen-\neration. To explore the effects of pretraining on\ndecoder-only models, we employed step-unrolled\ndenoising (Savinov et al., 2021) to pretrain the\nmodels. In the rest of the paper, we describe our\nproposed pretraining regime in detail and evalu-\nate its effectiveness in improving the quality of\nefficientnon-autoregressive text generation.\nOur contributions are:\n\u2022 Introduce\ntraining\nregime\nfor\nnon-\nautoregressive models for efficient language\ngeneration,\n\u2022 We show that the non-autoregressive pretrain-\ning with unrolled denoising significantly im-\nproves the results on downstream benchmarks\ncompared to training from scratch.\n\u2022 We are able to achieve SoTA results on down-\nstream evaluations with similar parameter\ncount.\n2\nRelated work\nPretraining language models on large-scale data has\nshown great success for auto-regressive language\nmodels (Devlin et al., 2018; Ili\u00b4c et al., 2018; Rad-\nford et al., 2018). The models are pre-trained on\nlarge-scale data in a self-supervised manner then\nfinetuned on downstream tasks like text classifica-\ntion and machine translation. While pre-training is\na standard in many autoregressive language tasks,\nit is understudied in non-autoregressive settings.\nSome efforts have been made to study and adapt\npre-training for non auto-regressive models. (Guo\net al., 2020) incorporates two BERT models into\n1\narXiv:2311.08552v1  [cs.CL]  14 Nov 2023\nx1\nx2\n[M]\n[M]\ny1\ny2\nx1\nx2\ny1\ny2\ny3\ny2\ny1\nx2\nFigure 1: Illustration of prefix Language Model versus Decoder-only bidirectional de-noising model.\nmachine translation using mask-predict decoding\nmethod, their method utilizes two pre-trained BERT\nmodels one as the encoder and one as the decoder,\nand then inserts adapter layers into each layer. (Su\net al., 2021) follows similar regime but uses one\nBERT as the backbone model and then add a CRF\noutput layer which captures the target side depen-\ndency and improves the performance. Further (Li\net al., 2022) introduced CeMAT which uses a bidi-\nrectional encoder and decoder architecture. The\nmodel is jointly trained with Masked Language\nmodeling (MLM) for the decoder and Conditional\nMasked Language Modeling (CMLM) for the de-\ncoder with a cross attention module for bridging\nthem. The model seeks to enhance multilingual\nability in machine translation by pre-training on\nlarge-scale monolingual and bilingual texts in many\nlanguages and using an aligned code-switching\nstrategy than finetuned on NAT and AT tasks.\nSUNDAE (Savinov et al., 2021) is a novel\nmethod for training denoising models for text gener-\nation. SUNDAE improves upon traditional denois-\ning autoencoders by unrolling the decoding process\nfor multiple steps and adding noise at each step.\nresulting in a more robust and effective model for\ngenerating text. The authors demonstrated the ef-\nfectiveness of the SUNDAE method in several text\ngeneration tasks, including sentence completion\nand language modeling, and showed that it outper-\nformed other state-of-the-art methods in terms of\nboth quality and efficiency. The SUNDAE method\nprovides a promising approach to text generation\nand has practical applications in various natural\nlanguage processing tasks. However, SUNDAE\nlanguage generation suffers a huge drop in perfor-\nmance when adapted in non-auto-regressive genera-\ntion setting. In this study we focus on recovering the\ndrop in performance using large-scale pretraining.\nBANG (Qi et al., 2021) investigated pretrain-\ning an LLM using a mixture of autoregressive and\nnon-autoregressive objective functions. Their down-\nstream tasks include machine translation, summa-\nrization, and dialogue generation. BANG achieves\nstate-of-the-art performance on several benchmark\ndatasets, demonstrating the effectiveness of large-\nscale pretraining for bridging the gap between au-\ntoregressive and non-autoregressive language gen-\neration. We consider the BANG model to be a\npotential baseline, where the non-autoregressive\nparametrization simply dropped conditioning on\npreviously generated tokens.\n3\nMethod\nPretraining techniques such as masked language\nmodeling (MLM) on large-scale data have shown to\nbe effective in improving the performance of neural\nlanguage models. In this section, we investigate the\neffects of large-scale pretraining on decoder-only\nnon-autoregressive models. We adopted SUNDAE\n(Savinov et al., 2021), a two-step training method\nfor generative modeling of discrete sequences using\ndenoising autoencoders and Markov chain models.\nThe training process includes unrolled denoising,\nwhich involves starting the chain from corrupted\ndata samples instead of the prior distribution. The\nmodel learns to denoise samples that it is likely to\nencounter during full unrolling used at sample time.\n2\nL(t)(\u03b8) := \u2212Ex\u223cpdata\nx0\u223cq(\u00b7|x)\nx1\u223cf\u03b8(\u00b7|x0)\n[log f\u03b8(x|xi)],\n(1)\nwhere xi is the ith iteration denoised result, q(\u00b7|x)\nis the corruption function, and f\u03b8 is the network.\nWe investigate the effect of pretraining on the\ndecoder-only architecture proposed in (Radford\net al., 2018) combined with SUNDAE two-step\ntraining procedure as our baseline model. The pre-\ntraining is done on the Colossal Clean Crawled\nCorpus (C4) dataset. The pretraining objective is\nsimilar to prefix language modeling but with bidi-\nrectional attention as shown in Figure 1. Following\npretraining, we finetune the model on several down-\nstream tasks.\n3.1\nModel Details\nWe ground the work on T5 base (Raffel et al., 2020)\nand develop a decoder-only model on top. Our\nbaseline model utilizes a decoder-only transformer-\nbased architecture with bidirectional self-attention.\nSpecifically, we employ a 12-layer decoder with\nhidden states of dimension 768. This is comparable\nwith BANG with 6 layers of encoder and 6 layers\nof decoder with the same hidden dimension.\nSeveral NAR techniques (Gu et al., 2018; Savi-\nnov et al., 2021) try to incorporate the output sen-\ntence length information during the training allow-\ning NAR models to have some approximate of the\noutput length. To keep our study simple and fo-\ncused on the value of pretraining, we omit the use\nof length prediction neither as an auxiliary loss or a\nseparate module. Alternatively, the model is trained\nto predict padding tokens to fill the target sequence\nbuffer.\n3.2\nTraining Strategy\nDuring the pretraining phase, our model underwent\ntraining for 1 million steps on the C4 dataset with\na batch size of 128 and a sequence length of 512\ninputs and 114 targets. We explore span corrup-\ntion and prefix LM strategies during pretraining\nwhile observing the latter is more stable. One of\nthe hypothesis is a single span corruption target\nis shorter hence less meaningful to unroll. Hence\nfor the studies below, we use Prefix LM objective\nwith bidirectional attention (Figure.1). This pro-\ncess allowed the model to develop a comprehensive\nunderstanding of language patterns and contextual\nrelationships.\nFor the subsequent finetuning stage, the model is\nfine-tuned on a specific downstream task for 50k\nsteps, employing a learning rate of 0.0001. The\npretraing helps the model to efficiently finetune on\ndifferent downstream tasks with fewer number of\nsteps. The finetuning process further refined the\nmodel\u2019s parameters and enabled it to adapt to the\nnuances and requirements of the target task. During\nthe model inference evaluation, the model unrolls\n10 steps from the mask then decodes text as output.\n4\nExperiments\nWe conduct the experiments to study the effect\nof pretraining on decoder-only NAR models. We\nanalyze the performance on these models on down-\nstream tasks with and without pretraining. Our\nexperiments are all conducted through JAX/Flax\n(Bradbury et al., 2018) using the T5x framework\n(Roberts et al., 2022). We use TPU-v3 chips for\npretraining and finetuning, typical pretraining jobs\nuse 256 chips for a week and finetuning jobs use\n16 to 64 chips for a day.\n4.1\nDatasets\nPretraining. For our pretraining experiments, we\nuse the C4 dataset, which is a large-scale web doc-\nument corpus created by scraping the Common\nCrawl data. The C4 dataset contains over 750GB\nof text data and includes a diverse range of topics,\nsuch as news, blogs, and online forums. The text\ndata in the C4 dataset is preprocessed and tokenized\ninto individual sentences, making it suitable for\nlanguage modeling tasks. The C4 dataset has sev-\neral advantages over other datasets for pretraining,\nsuch as its large size and diversity. The size of the\ndataset allows for the training of large-scale lan-\nguage models, which have been shown to achieve\nstate-of-the-art performance on various NLP tasks.\nAdditionally, the diversity of the C4 dataset helps to\ncapture the different styles and registers of language\nused in the web documents, making the pretraining\nmodels more robust to different text domains.\nTo evaluate our approach, we conduct experi-\nments on following two popular generation bench-\nmarks for downstream evaluation:\nXSum. The XSum dataset (Narayan et al., 2018)\ncontains over 227,000 news articles and their corre-\nsponding summaries from the British Broad- cast-\ning Corporation (BBC). The articles are taken from\na wide range of topics, such as politics, business,\nsports, and entertainment. The summaries are writ-\n3\nXSum\nSQuAD\nModel\nPretrain\nROUGE-1\nROUGE-2\nROUGE-L\nOVERALL\nROUGE-L\nBLEU-4\nNAT (Gu et al., 2018)\nNo\n24.04\n3.88\n20.32\n16.08\n31.51\n2.46\niNAT (Lee et al., 2018)\nNo\n24.02\n3.99\n20.36\n16.12\n32.44\n2.33\nCMLM (Ghazvininejad et al., 2019b)\nNo\n23.82\n3.60\n20.15\n15.86\n31.58\n2.51\nLevT (Gu et al., 2019)\nNo\n24.75\n4.18\n20.87\n16.60\n31.38\n2.27\nBANG NAR (Qi et al., 2021)\nYes\n32.59\n8.98\n27.41\n22.99\n44.07\n12.75\nBANG semi-NAR\nYes\n34.71\n11.71\n29.16\n25.19\n47.39\n17.62\nOurs (no prefix-lm pretraining)\nNo\n32.56\n11.8\n26.17\n23.51\n31.36\n3.903\nOurs (with prefix-lm pretraining)\nYes\n35.80\n14.03\n29.27\n26.36\n45.75\n12.47\nTable 1: NAR results on the XSum and SQuAD 1.1 question generation.\nten to capture the main idea and salient points of\nthe articles in a single sentence. The average input\nand output lengths are 358.5 and 21.1, respectively.\nSQuAD 1.1 (Rajpurkar et al., 2016) is a pop-\nular benchmark dataset for evaluating the perfor-\nmance of question answering models. It was re-\nleased by Stanford University in 2016 and contains\nover 100,000 questions with their corresponding an-\nswers, all based on a set of Wikipedia articles. After\npreprocessing, the dataset contains 98K <answer,\npassage, question> data triples. Input is formatted\nas <answer [SEP] passage> following GLGE. The\naverage input and output lengths are 149.4 and 11.5,\nrespectively.\n4.2\nResults\nIn this section, we show large scale pretraining\nusing prefix-lm leads to huge improvement in per-\nformance for NAR decoder-only models. We eval-\nuate our approach on two popular datasets. For\nXSum dataset, we use a combination of ROUGE\nscore (Lin, 2004) to evaluate different models. As\nshown in table 1, we observe +2.9 ROUGE-L score\nwhen the model is pretrained. Also the model out-\nperformed both BANG NAR and Semi-NAR and\nCMLM in terms of all three ROUGE metrics. We\nalso evaluated our approach on Squad 1.1 question\ngeneration task, our model was able to show +14.4\nROUGE-L and +8.6 BLEU-4 when the model is\npretrained. And it demonstrates +1.7 ROUGE-L\nimprovement in performance compared to BANG\nNAR while -2.7 ROULGE-L compared to BANG\nsemi-NAR.\n5\nAblation Studies\n5.1\nModel Architecture\nWe conduct preliminary experiments on WMT14\nusing EN-DE on both encoder-decoder and decoder\nonly model. The max BLEU number for encoder-\nModel\n@500k\n@1M\nbest\nDecoder only Pretrained\n21.6\n21.76\n21.76\nEncoder Decoder Pretrained\n20.13\n18.42\n21.73\nTable 2: BLEU on WMT14 EN\u2192DE.\ndecoder and decoder only model have negligible\ndifference while the encoder-decoder model has\na high variance during eval. Hence we utilize the\ndecoder only architecture for the main study on\nother downstream benchmarks.\n5.2\nSample Efficiency\nModel\n@500k\n@1M\nDecoder only From scratch\n14.57\n21.89\nDecoder only Pretrained\n21.6\n21.76\nTable 3: Decoder-only BLEU the WMT14 EN\u2192DE.\nIn Table 3, we present the WMT14 ENDE num-\nbers for pretrained vs from scratch numbers. We see\nalthough the final numbers have negligible differ-\nence, the pretrained model is more sample efficient,\nreaching higher number with the same fine-tune\nsteps. Note that this number is not comparable with\nSoTA WMT results because of the length predic-\ntor, for fair comparison, please refer to SUNDAE\nAppendix Figure 4a.\n6\nConclusion and Future Work\nIn this work, we investigate the effect of pretraining\nfor non-autoregressive decoder only SUNDAE. We\nshow that pretraining should be considered a foun-\ndational block for non-autoregressive model. For\nfuture work, there is a natural question: Will the\nnon-autoregressive model scales with data size and\nmodel parameters as larger autoregressive models\ndo.\n4\nReferences\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\nEric Chu, Jonathan H. Clark, Laurent El Shafey, Yan-\nping Huang, Kathy Meier-Hellstern, Gaurav Mishra,\nErica Moreira, Mark Omernick, Kevin Robinson,\nSebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong\nXu, Yujing Zhang, Gustavo Hernandez Abrego, Jun-\nwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\nJames Bradbury, Siddhartha Brahma, Kevin Brooks,\nMichele Catasta, Yong Cheng, Colin Cherry, Christo-\npher A. Choquette-Choo, Aakanksha Chowdhery,\nCl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani,\nSunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan\nDyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber,\nMarkus Freitag, Xavier Garcia, Sebastian Gehrmann,\nLucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi\nHashemi, Le Hou, Joshua Howland, Andrea Hu, Jef-\nfrey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-\ncheriah, Matthew Jagielski, Wenhao Jia, Kathleen\nKenealy, Maxim Krikun, Sneha Kudugunta, Chang\nLan, Katherine Lee, Benjamin Lee, Eric Li, Music\nLi, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim,\nHanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-\ncello Maggioni, Aroma Mahendru, Joshua Maynez,\nVedant Misra, Maysam Moussalem, Zachary Nado,\nJohn Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\nMarie Pellat, Martin Polacek, Alex Polozov, Reiner\nPope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker\nRiley, Alex Castro Ros, Aurko Roy, Brennan Saeta,\nRajkumar Samuel, Renee Shelby, Ambrose Slone,\nDaniel Smilkov, David R. So, Daniel Sohn, Simon\nTokumine, Dasha Valter, Vijay Vasudevan, Kiran Vo-\ndrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang,\nTao Wang, John Wieting, Yuhuai Wu, Kelvin Xu,\nYunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,\nQiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou,\nDenny Zhou, Slav Petrov, and Yonghui Wu. 2023.\nPalm 2 technical report.\nJames\nBradbury, Roy\nFrostig, Peter\nHawkins,\nMatthew James Johnson, Chris Leary, Dougal\nMaclaurin, George Necula, Adam Paszke, Jake\nVanderPlas, Skye Wanderman-Milne, et al. 2018.\nJax: composable transformations of python+ numpy\nprograms.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019a. Mask-predict: Parallel\ndecoding of conditional masked language models.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019b. Mask-predict: Parallel\ndecoding of conditional masked language models.\narXiv preprint arXiv:1904.09324.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In International Confer-\nence on Learning Representations.\nJiatao Gu and Xiang Kong. 2020.\nFully non-\nautoregressive neural machine translation: Tricks of\nthe trade.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019. Lev-\nenshtein transformer. Advances in Neural Informa-\ntion Processing Systems, 32.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorporating\nbert into parallel sequence decoding with adapters.\nAdvances in Neural Information Processing Systems,\n33:10843\u201310854.\nSuzana Ili\u00b4c, Edison Marrese-Taylor, Jorge A Balazs,\nand Yutaka Matsuo. 2018. Deep contextualized word\nrepresentations for detecting sarcasm and irony. arXiv\npreprint arXiv:1809.09795.\nJason Lee, Elman Mansimov, and Kyunghyun Cho. 2018.\nDeterministic non-autoregressive neural sequence\nmodeling by iterative refinement. arXiv preprint\narXiv:1802.06901.\nPengfei Li, Liangyou Li, Meng Zhang, Minghao Wu,\nand Qun Liu. 2022. Universal conditional masked\nlanguage pre-training for neural machine translation.\narXiv preprint arXiv:2203.09210.\nChin-Yew Lin. 2004. ROUGE: A package for automatic\nevaluation of summaries. In Text Summarization\nBranches Out, pages 74\u201381, Barcelona, Spain. Asso-\nciation for Computational Linguistics.\nDayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang\nZhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou,\nMing Gong, et al. 2020.\nGlge: A new general\nlanguage generation evaluation benchmark. arXiv\npreprint arXiv:2011.11928.\nShashi Narayan, Shay B Cohen, and Mirella Lap-\nata. 2018.\nDon\u2019t give me the details, just the\nsummary! topic-aware convolutional neural net-\nworks for extreme summarization. arXiv preprint\narXiv:1808.08745.\nOpenAI. 2023. Gpt-4 technical report.\nWeizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu\nChen, Dayiheng Liu, Kewen Tang, Houqiang Li,\nJiusheng Chen, Ruofei Zhang, et al. 2021. Bang:\nBridging autoregressive and non-autoregressive gen-\neration with large scale pretraining. In International\nConference on Machine Learning, pages 8630\u20138639.\nPMLR.\n5\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text.\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz Mohi-\nuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex\nSalcianu, Marc van Zee, Jacob Austin, Sebastian\nGoodman, Livio Baldini Soares, Haitang Hu, Sasha\nTsvyashchenko, Aakanksha Chowdhery, Jasmijn Bast-\nings, Jannis Bulian, Xavier Garcia, Jianmo Ni, An-\ndrew Chen, Kathleen Kenealy, Jonathan H. Clark,\nStephan Lee, Dan Garrette, James Lee-Thorp, Colin\nRaffel, Noam Shazeer, Marvin Ritter, Maarten Bosma,\nAlexandre Passos, Jeremy Maitin-Shepard, Noah\nFiedel, Mark Omernick, Brennan Saeta, Ryan Sepassi,\nAlexander Spiridonov, Joshua Newlan, and Andrea\nGesmundo. 2022. Scaling up models and data with\nt5x and seqio.\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord. 2021. Step-\nunrolled denoising autoencoders for text generation.\narXiv preprint arXiv:2112.06749.\nYixuan Su, Deng Cai, Yan Wang, David Vandyke, Si-\nmon Baker, Piji Li, and Nigel Collier. 2021. Non-\nautoregressive text generation with pre-trained lan-\nguage models. arXiv preprint arXiv:2102.08220.\n6\n"
  },
  {
    "title": "Fusion-Eval: Integrating Evaluators with LLMs",
    "link": "https://arxiv.org/pdf/2311.09204.pdf",
    "upvote": "5",
    "text": "Fusion-Eval: Integrating Assistant Evaluators with LLMs\nLei Shu\u2217\nNevan Wichers\nLiangchen Luo\nYun Zhu\nYinxiao Liu\nJindong Chen\nLei Meng\nGoogle Research\nAbstract\nEvaluating natural language systems poses sig-\nnificant challenges, particularly in the realms of\nnatural language understanding and high-level\nreasoning. In this paper, we introduce \u201cFusion-\nEval\u201d, an innovative approach that leverages\nLarge Language Models (LLMs) to integrate\ninsights from various assistant evaluators. Each\nof these evaluators specializes in assessing dis-\ntinct aspects of responses. This unique strat-\negy enables Fusion-Eval to function effectively\nacross a diverse range of tasks and criteria, en-\nhancing the effectiveness of existing evaluation\nmethods. Fusion-Eval achieves a 0.962 system-\nlevel Kendall-Tau correlation with humans on\nSummEval and a 0.744 turn-level Spearman\ncorrelation on TopicalChat, which is signifi-\ncantly higher than baseline methods. These\nresults highlight Fusion-Eval\u2019s significant po-\ntential in the realm of natural language system\nevaluation.\n1\nIntroduction\nEvaluating\nthe\nperformance\nof\nnatural\nlan-\nguage generation models has significant chal-\nlenges (Ouyang et al., 2022), particularly in\nterms of evaluation benchmarks and evaluation\nparadigms (Wang et al., 2023b). This study fo-\ncuses on the latter one. Typically, the evaluation\nparadigms fall into three categories: human-based,\nautomatic-metrics-based and model-based evalu-\nations. Among these, human evaluations are re-\ngarded as the most reliable, yet they come with\nhigh costs and issues of scalability.\nAutomatic metrics such as BLEU (Papineni\net al., 2002) and ROUGE (Lin, 2004) are preva-\nlent in evaluations, relying on comparisons with a\n\u2018gold\u2019 standard reference. However, the creation of\nthese gold references is a labor-intensive process.\nFurthermore, studies such as Fabbri et al. (2021)\nhave demonstrated that these automatic metrics of-\nten do not correlate well with human judgment.\n\u2217Correspondence to leishu@google.com .\nModel-based evaluations aim to enhance the cor-\nrelation with human judgment using neural net-\nworks fine-tuned on specific datasets. Neural eval-\nuators like BLEURT (Sellam et al., 2020) and its\nvariant SMART (Amplayo et al., 2022) show im-\nproved alignment with human assessments in vari-\nous generative tasks. These models offer flexibility\nin evaluation methods. They can either compare\nthe response to the source (reference-free), or to\nthe gold standard (reference-dependent).\nRecent advancements have seen the use of Large\nLanguage Models (LLMs) as reference-free evalu-\nators in Natural Language Generation (NLG) tasks.\nNotably, studies by Fu et al. (2023); Wang et al.\n(2023a) have leveraged LLMs to rate candidate out-\nputs based on their generation probability alone,\neliminating the need for reference text compar-\nisons. Additionally, Liu et al. (2023) introduced\na method called G-Eval, where LLMs, guided by\nhuman-crafted evaluation criteria, score responses.\nMeta-evaluations indicate that these LLM-based\nevaluators reach a level of human correlation on par\nwith medium-sized neural evaluators (Zhong et al.,\n2022). In light of these developments in evaluation\nparadigms, the following question arises:\n\u201cCan Large Language Models (LLMs) inte-\ngrate existing evaluators to achieve higher cor-\nrelation with human judgments?\u201d\nIn response to this question, we introduce\nFusion-Eval, an innovative evaluation frame-\nwork that integrates a variety of existing evalua-\ntors\u2014termed assistant evaluators\u2014to enhance cor-\nrelation with human judgment. Fusion-Eval lever-\nages LLMs for evaluation and to fuse scores from\nthe assistant evaluators. Fusion-Eval can evaluate\nany natural language task where assistant evalua-\ntors are available. However, its effectiveness hinges\non the quality of the assistant evaluators, making it\nmore suitable for well-established text generation\ntasks.\narXiv:2311.09204v2  [cs.CL]  16 Feb 2024\n2\nMethod\nFusion-Eval is an evaluation framework leveraging\na Large Language Model (LLM) to fuse assistant\nevaluators, to improve scoring quality. The frame-\nwork\u2019s goal is to evaluate a Natural Language Gen-\neration (NLG) system along one or more criteria in\na manner highly correlated with human judgment.\nThe test examples are what Fusion-Eval will eval-\nuate. For examples, see Section 3.1 and Figure 1.\nEach assistant evaluator receives a test example\nand returns a score. The Fusion-Eval framework\nthen takes evaluation task descriptions, test exam-\nples, and assistant evaluator scores as inputs. We\npropose two Fusion-Eval solutions:\n(1) LLM Evaluation without Plan\nIn this\nmethod, the Large Language Model (LLM) is\nprompted directly with the task\u2019s evaluation criteria,\ndetails about assistant evaluators, and a request for\nevaluation scores. This prompt includes placehold-\ners for the assistant evaluator scores and the test\nexample, as well as instructions on the format the\nLLM should use to generate the evaluation scores.\nThis straightforward approach requires the LLM to\ninterpret the evaluation criteria and information on\nassistant evaluators without a predefined plan.\nYou are an evaluation agent. I will give you one sum-\nmary written for a news article. Please evaluate the\nquality of the summary.\nDetailed descriptions of these metrics are as follows:\nCoherence(1-5, Any Floating Value):the collective qual-\nity of all sentences.<...>\nThree assistant evaluators are provided.\n1. Natural Language Inference (NLI) provides the proba-\nbility of the entailed relationship between source text (as\npremise). Its range is between 0-1, close to 1 indicates\nthat the hypothesis is entailed by the premise.<...>\nUse these evaluators as supplementary tools for your\njudgement and rate the responses across the five metrics\n<...>\nOutput Template: Coherence Score: [Your evaluation ]\nExplanation : [Your explanation on evaluation ]\nTable 1: Trimmed Prompt for LLM Evaluation without Plan\n(2) LLM Evaluation with Plan\nThis approach\nintroduces a plan that specifies which assistant eval-\nuators to use for evaluating specific criteria, accom-\npanied by detailed steps for the LLM to follow in\nscoring. It is designed for complex evaluation tasks\nthat benefit from explicit guidance. The plan also\nadds transparency as one can see which evaluators\nare used for what purpose. While there are trade-\noffs between using a human-generated or an LLM-\ngenerated plan, our framework accommodates both\noptions.\nWhen using an LLM to generate the plan, the\nLLM is prompted with the task\u2019s definition, criteria,\nand information about assistant evaluators. This is\nsimilar to the auto chain-of-thought method in G-\nEval (Liu et al., 2023), but it uniquely incorporates\nassistant evaluators. In our experiments we use a\nplan generated by an LLM, with the workflow de-\ntailed in Appendix A.1, Figure 1. Table 2 shows the\nassistant evaluators which the LLM plans to use for\neach criteria. To adapt Fusion-Eval to a different\nevaluation task, one needs to update the criteria and\nassistant evaluator descriptions and regenerate the\nplan. Additionally, collecting new assistant evalu-\nator scores for the task is necessary. Full Fusion-\nEval templates are available in Appendix A.4 for\nSummEval and A.5 for TopicalChat.\nPrompt Execution\nIn both solutions, the pre-\npared evaluation prompt template is applied to each\ntest example. This template is filled with the in-\nputs, responses, and scores of assistant evaluators\nfor each test case. The executing LLM then pro-\ncesses this filled prompt, yielding Fusion-Eval\u2019s\nfinal evaluation scores.We found that the LLM gen-\nerated evaluation scores in the correct format, so\nwe didn\u2019t need to do anything else to control the\noutputs. The details are provided in the Experiment\nSection (Section 3.1).\nSummEval\nTopicalChat\nCoh\nCon\nFlu\nRel\nCoh\nEng\nNat\nGro\nUnd\nBLEURT\n\u2713\n\u2713\nBLEURT\n\u2713\nNLI\n\u2713\nPaLM2 Prob\n\u2713\n\u2713\nSumBLEURT\n\u2713\n\u2713\nTable 2: LLM-Suggested Assistant Evaluator Alignment for\nSummEval and TopicalChat Criteria. The criteria include\ncoherence (Coh), consistency (Con), fluency (Flu), relevance\n(Rel), engagingness (Eng), naturalness (Nat), groundedness\n(Gro), and understandability (Und).\n3\nExperiment\nWe conduct a meta-evaluation of Fusion-Eval, uti-\nlizing the SummEval (Fabbri et al., 2021) and Top-\nicalChat (Mehri and Eskenazi, 2020) benchmarks.\n3.1\nExperiment Setting\nSummEval (Fabbri et al., 2021), a benchmark for\ntext summarization evaluation, consists of 1600\ndata points. Each data point includes average rat-\nHuman Evaluation\nCoh\nCon\nFlu\nRel\nOverall\nReference-Based Metrics\nROUGE-1\n0.35\n0.55\n0.527 0.583 0.503\nROUGE-2\n0.233 0.6\n0.494 0.433 0.44\nROUGE-L\n0.117 0.117 0.259 0.35\n0.211\nBLEU\n0.217 0.05\n0.326 0.383 0.244\nCHRF\n0.35\n0.617 0.561 0.55\n0.519\nS1-CHRF\n0.3\n0.733 0.494 0.5\n0.507\nS2-CHRF\n0.3\n0.7\n0.46\n0.433 0.473\nSL-CHRF\n0.367 0.733 0.494 0.5\n0.523\nBERTScore\n0.333 -0.03 0.142 0.2\n0.161\nMoverScore\n0.217 -0.05 0.259 0.35\n0.194\nSource-dependent Metrics\nBARTScore\n0.35\n0.617 0.494 0.45\n0.478\nUniEval\n0.683 0.75\n0.661 0.667 0.728\nDE-PaLM2\n0.733 0.6\n0.745 0.85\n0.879\nG-Eval (GPT-4)\n0.733 0.583 0.778 0.883 0.912\nAssistant Evaluators\nBLEURT\n0.433 0.767 0.644 0.633 0.678\nNLI\n0.45\n0.717 0.628 0.65\n0.695\nSumBLEURT\n0.7\n0.333 0.544 0.633 0.644\nAggregation of Assistant Evaluators (AE)\nAVG(AE)\n0.65\n0.55\n0.661 0.783 0.828\nLLMSel(AE)\n0.7\n0.75\n-\n0.767 -\nCorrW(AE)\n0.667 0.65\n0.678 0.783 0.845\nAggregation of AE and LLM Direct Evaluation\nAVG(AE, DE-PaLM2)\n0.717 0.583 0.728 0.85\n0.895\nAVG(AE, G-Eval-GPT-4)\n0.717 0.617 0.745 0.883 0.912\nLLMSel(AE, DE-PaLM2)\n0.733 0.717 -\n0.833 -\nLLMSel(AE, G-Eval-GPT-4) 0.733 0.717 -\n0.85\n-\nCorrW(AE, DE-PaLM2)\n0.717 0.633 0.745 0.85\n0.895\nCorrW(AE, G-Eval-GPT-4)\n0.733 0.633 0.762 0.883 0.912\nFusion-Eval\nFE-PaLM2-NoPlan\n0.767 0.617 0.728 0.867 0.895\nFE-PaLM2\n0.783 0.767 0.778 0.917 0.962\nFE-GPT-4\n0.783 0.762 0.812 0.9\n0.946\nTable 3: System-level Kendall-Tau (\u03c4) correlations of different\nevaluators to human judgements on SummEval benchmark.\nThe assistant evaluators, BLEURT, NLI and SumBLEURT,\ntreat the article as a premise and the summary as a hypothesis.\nings from three experts on a scale of 1 to 5, span-\nning four summary quality dimensions: coherence\n(Coh), consistency (Con), fluency (Flu) and rel-\nevance (Rel). The \u201cOverall\u201d score is derived as\nan average across these four dimensions. Topi-\ncalChat (Mehri and Eskenazi, 2020), a benchmark\nfor evaluating knowledge-based dialogue response\ngeneration, includes 360 data points. It features\nhuman evaluations from three experts across six\ndimensions: coherence (Coh), engagingness (Eng),\nnaturalness (Nat), groundedness (Gro), understand-\nability (Und), and overall. Ratings for naturalness,\ncoherence, and engagingness are on a scale from\n1 to 3, while groundedness and understandability\nare scored between 0 and 1. The overall dimension\nis evaluated on a scale of 1 to 5. Each data point\ncomprises a conversation history, a grounding fact,\nand a potential next-turn response. To measure the\ncorrelation between results generated by Fusion-\nEval and human evaluations, we use Kendall-Tau\nHuman Evaluation\nCoh\nEng\nNat\nGro\nUnd Overall\n(1-3) (1-3) (1-3) (0-1) (0-1) (1-5)\nSource-dependent Metrics\nUniEval\n0.613 0.605 0.514 0.575 0.468 0.663\nDE-PaLM2\n0.669 0.688 0.542 0.602 0.493 0.66\nG-Eval (GPT-4)\n0.605 0.691 0.565 0.551 -\n-\nAssistant Evaluators\nBLEURT\n0.316 0.461 0.384 0.638 0.432 0.464\nPaLM2 Prob\n0.583 0.606 0.637 0.441 0.676 0.687\nAggregation of Assistant Evaluators (AE)\nAVG(AE)\n0.556 0.637 0.626 0.579 0.672 0.697\nLLMSel(AE)\n-\n-\n0.637 0.638 0.676 -\nCorrW(AE)\n0.575 0.637 0.638 0.6\n0.682 0.703\nAggregation of AE and LLM Direct Evaluation\nAVG(AE, DE-PaLM2)\n0.655 0.708 0.631 0.639 0.679 0.737\nLLMSel(AE, DE-PaLM2) -\n-\n0.637 0.66 0.68 -\nCorrW(AE, DE-PaLM2)\n0.666 0.711 0.641 0.65 0.689 0.742\nFusion-Eval\nFE-PaLM2-NoPlan 0.683 0.722 0.649 0.643 0.641 0.735\nFE-PaLM2\n0.697 0.728 0.651 0.709 0.632 0.764\nFE-GPT-4\n0.678 0.747 0.691 0.692 0.687 0.774\nTable 4: Turn-level Spearman (\u03c1) correlations of different\nevaluators to human judgements on TopicalChat benchmark.\nBLEURT treats the fact and conversation as the premise and\nthe response as the hypothesis. PaLM2 Prob represents the\nconditional probability of the response given the fact and\nconversation. The G-Eval scores for Und and Overall are\nmissing because they aren\u2019t reported in their paper.\nscores for system-level analysis in SummEval (Fab-\nbri et al., 2021), and Spearman scores for turn-level\nanalysis in TopicalChat (Mehri and Eskenazi, 2020)\nto align with each benchmark\u2019s original scoring\nmethodology.\nIn our experiments, PaLM2-Large (Anil et al.,\n2023) and GPT-4 (OpenAI, 2023) serve as the\nLarge Language Models (LLMs) for execution,\ndesignated as FE-PaLM2 and FE-GPT-4, respec-\ntively. In the ablation study FE-PaLM2-NoPlan,\nwe use the LLM Evaluation without Plan method\nas described in Section 2.\nWe integrate sev-\neral assistant evaluators: NLI (Bowman et al.,\n2015), BLEURT (Sellam et al., 2020), and Sum-\nBLEURT\u2014a BLEURT variant fine-tuned for hu-\nman summarization evaluation (Clark et al., 2023).\nWe also obtain the probability that PaLM will gen-\nerate the response from the dataset given the con-\ntext, following methods in Fu et al. (2023) and\nWang et al. (2023a). The probability of the re-\nsponse is higher if it\u2019s more likely according to\nPaLM2. We use this as an assistant evaluator called\nPaLM2 Prob. For the execution of Fusion-Eval, the\nevaluation prompt template is filled with specific\ninputs, responses, and assistant evaluator scores\nfor each test case. This complete prompt is then\nprocessed by the executing LLM, which generates\na score for each evaluation dimension. The LLMs\nare configured to produce 8 predictions with tem-\nperatures of 0.5 for PaLM2 and 0.1 for GPT-4. The\nreported Fusion-Eval scores are the average of 8\npredictions. To the best of our knowledge, the\nLLMs utilized in Fusion-Eval were not trained on\nthe SummEval and TopicalChat datasets. If the\nmodels were trained on these datasets, it would\naffect both our results and the baseline, so the com-\nparison is still valid. Since Fusion-Eval is the first\npaper to incorporate assistant evaluators into an fi-\nnal evaluation, there is no dataset which could train\nan LLM to integrate assistant evaluators optimally.\n3.2\nBaselines\nFor a thorough comparison, we meta-evaluated\nFusion-Eval against a range of baseline methods\non the SummEval benchmark. These baselines in-\nclude ROUGE (Lin, 2004), BLEU (Papineni et al.,\n2002), CHRF (Popovi\u00b4c, 2015), SMART (Amplayo\net al., 2022), BERTScore (Zhang et al., 2019),\nMoverScore (Zhao et al., 2019), BARTScore (Yuan\net al., 2021), UniEval (Zhong et al., 2022), and\nG-Eval (Liu et al., 2023). We derived scores for\nmost baselines from the SMART paper (Amplayo\net al., 2022), while for UniEval1 and G-Eval2,\nwe calculated scores using their publicly avail-\nable predictions. For the TopicalChat benchmark,\nwe compared Fusion-Eval\u2019s performance with G-\nEval (Liu et al., 2023) and UniEval (Zhong et al.,\n2022), utilizing scores from their respective pub-\nlications. We also introduce DE-PaLM2 (Direct\nEvaluator PaLM2) as an ablation baseline. DE-\nPaLM2 uses the same approach as G-Eval. This\nbaseline provides insights into PaLM2\u2019s standalone\nperformance on the SummEval and TopicalChat\nbenchmarks. We propose a set of aggregation func-\ntions to merge scores from assistant evaluators:\nAVG (Average Scores): Calculates the average\nof the score from all evaluators. LLMSel (LLM-\nSelected Assistant Evaluators): This method com-\nputes the average score but only from evaluators\nwhich the plan identifies as relevant to the category.\nCorrW (Correlation-Weighted Average): The\naverage of each evaluator score weighted by the\nevaluator\u2019s correlation with human judgment. A\nmore detailed description is in Appendix A.2.\n3.3\nResult Analysis\nTables 3 and 4 present the correlation of baselines,\nassistant evaluators, and Fusion-Eval with human\n1https://github.com/maszhongming/\nUniEval\n2https://github.com/nlpyang/geval\njudgment. Tables 5 and 6 illustrate the correlation\nof assistant evaluators with FE-PaLM2. Similarly,\nTables 7 and 8 detail the correlation of assistant\nevaluators with FE-GPT-4.\nOur primary finding is that Fusion-Eval outper-\nforms all baseline models and aggregation methods\nin the overall dimension and nearly all other di-\nmensions, as demonstrated in the FE-GPT-4 and\nFE-PaLM2 rows of both datasets.\nThe remainder of our analysis is dedicated to the\noverall correlation with human judgment. Among\nvarious aggregation methods for assistant evalua-\ntors, the method that weights by correlation with\nhumans (CorrW) performs best. Combining the\nLarge Language Model (LLM) direct evaluator\nwith assistant evaluators yields better results than\nusing the direct evaluator alone for PaLM2, and it\nmatches performance for GPT models. Specifically,\nCorrW(AE, DE-PaLM2) shows a higher correla-\ntion with human judgments than DE-PaLM2 alone,\nsuggesting that assistant evaluators can enhance\nan LLM\u2019s performance beyond its standalone ca-\npabilities. However, Fusion-Eval surpasses these\naggregation methods, making it the better approach\nfor leveraging assistant evaluators over mere score\naggregation.\nThe performance of FE-PaLM2 is higher than\nthat of FE-PaLM2-NoPlan, suggesting that prompt-\ning the LLM with a plan is beneficial. This im-\nprovement could be attributed to the plan aiding\nthe LLM in utilizing the assistant evaluator scores.\nThe Fusion-Eval framework maintains a manage-\nable execution time because the assistant evaluators\nhave minimal inference times compared to LLMs.\nFor instance, running all assistant evaluators (NLI,\nBLEURT, and SumBLEURT) on a SummEval ex-\nample takes about 0.125 seconds on average. These\nevaluators are pre-trained, eliminating the need for\nadditional training within our system. Obtaining a\nFusion-Eval result using PaLM2, based on assistant\nevaluator scores, takes about 7 seconds for a Sum-\nmEval example and 11.7 seconds for a TopicalChat\nexample.\n4\nConclusion\nThe paper presents Fusion-Eval, an innovative ag-\ngregator using Large Language Models (LLMs) for\ndiverse evaluation tasks. It effectively integrates\nassistant evaluators according to specific criteria.\nEmpirical results show Fusion-Eval achieves higher\ncorrelations with human judgments than baselines.\n5\nLimitation and Future Work\nFusionEval is assessed across two tasks. We chose\nSummEval and TopicalChat as benchmarks for\nmeta-evaluation because UniEval (Zhong et al.,\n2022) and G-Eval (Liu et al., 2023) also use only\nthose benchmarks. This facilitates effective com-\nparison with their results. These benchmarks are\nwidely recognized and offer a comprehensive range\nof evaluation metrics. We intentionally excluded\ndatasets that rely on single-rater annotations (Stien-\nnon et al., 2020; Bai et al., 2022) or are limited to\na singular metric (Wang et al., 2020).\nThe LLM-generated plan used in our experi-\nments is likely not optimal. Finding an \u2018optimal\nplan\u2019 is nearly impossible due to the exponential\ncomplexity involved in combining criteria and as-\nsistant evaluators. We recognize the potential for\nhallucinations in LLM-generated plans and note\nthat a human-created plan could also be employed\nwith Fusion-Eval.\nThe length of our execution prompt templates\nfor SummEval (Appendix A.4) and TopicalChat\n(Appendix A.5) is 662 and 990 words, respectively.\nThe LLMs used in Fusion-Eval, including GPT-4\nand PaLM2, can effectively process prompts of this\nlength. However, the lengthy Fusion-Eval prompts\nmay present challenges for LLMs with limited con-\ntext windows. To address this, we propose inves-\ntigating prompt decomposition in future work to\nenhance Fusion-Eval\u2019s compatibility with various\nLLMs.\nReferences\nReinald Kim Amplayo, Peter J Liu, Yao Zhao, and\nShashi Narayan. 2022.\nSmart:\nsentences as\nbasic units for text evaluation.\narXiv preprint\narXiv:2208.01030.\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, et al. 2023. Palm 2 technical report. arXiv\npreprint arXiv:2305.10403.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan, et al.\n2022. Training a helpful and harmless assistant with\nreinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862.\nSamuel Bowman, Gabor Angeli, Christopher Potts, and\nChristopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing, pages 632\u2013\n642.\nElizabeth Clark, Shruti Rijhwani, Sebastian Gehrmann,\nJoshua Maynez, Roee Aharoni, Vitaly Nikolaev,\nThibault Sellam, Aditya Siddhant, Dipanjan Das, and\nAnkur P Parikh. 2023. Seahorse: A multilingual,\nmultifaceted dataset for summarization evaluation.\narXiv preprint arXiv:2305.13194.\nAlexander R Fabbri, Wojciech Kryscinski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. Summeval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391\u2013409.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire. arXiv\npreprint arXiv:2302.04166.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74\u201381.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. Gpteval:\nNlg evaluation using gpt-4 with better human align-\nment. arXiv preprint arXiv:2303.16634.\nShikib Mehri and Maxine Eskenazi. 2020. Usr: An\nunsupervised and reference free evaluation metric\nfor dialog generation. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 681\u2013707.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nAdvances in Neural\nInformation Processing Systems, 35:27730\u201327744.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311\u2013318.\nMaja Popovi\u00b4c. 2015. chrf: character n-gram f-score for\nautomatic mt evaluation. In Proceedings of the tenth\nworkshop on statistical machine translation, pages\n392\u2013395.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBleurt: Learning robust metrics for text generation.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7881\u2013\n7892.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008\u2013\n3021.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008\u20135020.\nJiaan Wang, Yunlong Liang, Fandong Meng, Haoxiang\nShi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie Zhou.\n2023a. Is chatgpt a good nlg evaluator? a preliminary\nstudy. arXiv preprint arXiv:2303.04048.\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xing-\nshan Zeng, Wenyong Huang, Lifeng Shang, Xin\nJiang, and Qun Liu. 2023b.\nAligning large lan-\nguage models with human: A survey. arXiv preprint\narXiv:2307.12966.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263\u201327277.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M Meyer, and Steffen Eger. 2019. Moverscore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 563\u2013578.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022.\nTowards a unified multi-\ndimensional evaluator for text generation. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 2023\u2013\n2038.\nA\nAppendix\nA.1\nWorkflow of Fusion-Eval with Plan\nYou are an evaluation agent. I \nwill give you one summary \nwritten for a news article. Please \nevaluate the quality of the \nsummary. <...>\nDetailed descriptions of these \nmetrics are as follows:\nCoherence(1-5, Any Floating \nValue):the collective quality of \nall sentences.<...>\nThree assistant evaluators are \nprovided.\n1. Natural Language Inference \n(NLI) provides the probability of \nthe entailed relationship \nbetween source text (as \npremise). Its range is between \n0-1, close to 1 indicates that the \nhypothesis is entailed by the \npremise. <...>\nPlease share your \nunderstanding of the evaluation \ntask and plan for using assistant \nevaluators, including criteria \nplanning and steps. <...>\nEvaluate a provided summary using criteria: Coherence, \nConsistency, Relevance, and Fluency.\nAssistant Evaluators like NLI, BLEURT, and \nSUM_BLEURT, which give scores between below 0 and \n1 (closer to 1 being better), will assist in this evaluation.\n**1. NLI (Natural Language Inference)**:\nThis assistant evaluator provides a probability score \n<...>\n**Usage**:\n- **Consistency Evaluation**: A high entailment \nprobability indicates that the summary is factually \naligned with the source text. Conversely, a low score \nmight indicate discrepancies or hallucinated facts. <...>\n**Plan Using Assistant Evaluators**: <...>\n2. **Use NLI & BLEURT for Consistency**: Check both \nscores. High scores from both assistant evaluators will \nreaffirm the consistency of the summary.\n**Criteria & Steps**: <...>\n2. **Consistency (1-5)**:\n   - Use NLI & BLEURT to get scores.\n   - Read the article and summary.\n   - Compare factual details.\n   - Assign a consistency score based on factual \nalignment.\n<...>\nEvaluate a provided summary using criteria\n<...LLM Generated Plan\u2026>\n**Input Template**: <...>\n**Output Template**: <...>\n**Input Example**:\nSource:\nPaul Merson has restarted his row with Andros Townsend \nafter the Tottenham midfielder was brought on with only \nseven minutes  <...>\nAnswer:\nPaul merson was brought on with only seven minutes \nremaining in his team's 0-0 draw with burnley. <...>\nNLI Score (Source as Premise and Answer as Hypothesis):\n0.3501637578010559\nBLEURT Score (Source as Premise and Answer as \nHypothesis):\n0.32037580013275146\nSUM_BLEURT Score (Source as Premise and Answer as \nHypothesis):\n0.004482417367398739\nEvaluation (please follow Output Template and provide the \nevaluation result):\nCriterias' Scores and Explanations:\nCoherence\nScore: 2 Explanation: The summary \npresents information in a somewhat \nlogical order but fails to clearly <...>\nConsistency\nScore: 2 Explanation: The NLI score \nsuggests a low level of factual \nalignment between the source and the \nsummary, indicating significant \ndiscrepancies. <...>\nRelevance\nScore: 2 Explanation: The summary \nfails to capture the essence of the \nsource text, which focuses on the \ninteraction and conflict between Paul \nMerson and Andros Townsend, rather \nthan the match details. <...>\nFluency\nScore: 3 Explanation: The summary is \nreadable but contains several \ngrammatical errors and awkward \nphrasings, <...>\nEvaluation Summary:\nOverall Score: 2.25 Explanation: The \nsummary struggles with coherence, \nconsistency, and relevance, as \nhighlighted by the low scores from the \nassistant evaluators and the manual \nevaluation. <...>\nQuery to Elicit a Plan\nLLM Generated Plan\nTesting Case and Assistant Evaluators\u2019 scores\nFusion-Eval Evaluation Score\nFigure 1: Workflow of Fusion-Eval with Plan: Starting from the left, a query initiates the generation of a plan by the LLM. Once\nthe plan is obtained, it is concatenated with templates, and placeholders are filled in for each test case along with its specific\nassistant evaluators\u2019 scores. This complete prompt is then used to obtain the Fusion-Eval evaluation score from the LLM.\nThe workflow of Fusion-Eval with Plan (Section 2) is illustrated in Figure 1, encompassing an auto\nchain-of-thought process (Liu et al., 2023). Initially, we create a prompt (the leftmost in Figure 1) to\nsolicit a plan from the LLM. The second image from the left shows a streamlined LLM-generated plan\n(comprehensive plans with templates are available in Appendices A.4 and A.5). This plan outlines the\nLLM\u2019s understanding of the evaluation task, includes the selection of assistant evaluators, and provides\nrecommendations for their utilization alongside a detailed evaluation criteria. Upon obtaining the LLM-\ngenerated plan, we integrate it with human-created templates, incorporating specific test cases and their\ncorresponding assistant evaluators\u2019 scores. This forms the complete prompt for deriving the Fusion-Eval\nfinal score, depicted in the third textbox from the left in Figure1. Finally, the LLM executes this complete\nprompt to produce an evaluation score, as shown in the rightmost textbox in Figure 1.\nA.2\nAggregation Functions\nWe propose a suite of aggregation functions for comparison with Fusion-Eval:\nAVG: Calculates the mean score from all evaluations for simple, unweighted aggregation.\nLLMSel: Averages scores from LLM-chosen assistant evaluators, focusing on relevance or perfor-\nmance.\nCorrW: Computes scores\u2019 weighted average based on correlation with human judgment, aiming for\ncloser human evaluative alignment.\nDetails of calculations follow.\n1. AVG(AE): Average scores of assistant evaluators for each example.\n2. AVG(AE, DE-PaLM2|G-Eval-GPT-4): For each example and dimension, average scores of as-\nsistant evaluators and DE-PaLM2 or G-Eval-GPT-4. For example, AVG(AE, DE-PaLM2)[\u2018Coh\u2019] =\nAVG(BLEURT, NLI, SumBLEURT, DE-PaLM2[\u2018Coh\u2019]).\n3. LLMSel(AE): Average scores of LLM-selected assistant evaluators for each example.\n4. LLMSel(AE, DE-PaLM2|G-Eval-GPT-4): Similar to AVG, but only using LLM-selected assistant\nevaluators with DE-PaLM2 or G-Eval-GPT-4 for each dimension in each example.\n5. CorrW(AE): Weighted average of assistant evaluators\u2019 scores based on their correlation to human\njudgment. For example, CorrW(AE)[\u2018Coh\u2019] = (0.443*BLEURT + 0.45*NLI + 0.7*SumBLEURT) /\n(0.443 + 0.45 + 0.7).\n6. CorrW(AE, DE-PaLM2|G-Eval-GPT-4): A weighted average including DE-PaLM2 or G-Eval-GPT-\n4. For example, CorrW(AE,DE-PaLM2)[\u2018Coh\u2019] = (0.443*BLEURT + 0.45*NLI + 0.7*SumBLEURT +\n0.733*DE-PaLM2[\u2018Coh\u2019]) / (0.443 + 0.45 + 0.7 + 0.733).\nFor SummEval, G-Eval and DE-PaLM scores (G-Eval Fluency from 1-3) were adjusted from 1-5 to a\n0-1 scale to align with assistant evaluators\u2019 scoring range. For TopicalChat, our aggregation includes only\nassistant evaluators and DE-PaLM2, as G-Eval\u2019s predictions are unavailable. Also, DE-PaLM2\u2019s scores\nfor coherence, engagingness, and naturalness were remapped from 1-3 to 0-1 to match the scoring ranges\nof BLEURT and PaLM2 Prob.\nA.3\nAlignment between Assistant Evaluators and Fusion-Eval\nFE-PaLM2\nCoh\nCon\nFlu\nRel\nOverall\nBLEURT\n0.583 0.867 0.733 0.65\n0.717\nNLI\n0.6\n0.783 0.75\n0.667 0.733\nSumBLEURT 0.75\n0.467 0.633 0.717 0.683\nTable 5: FE-PaLM2 and Assistant Evaluators System-level Kendall-Tau (\u03c4) correlations on SummEval.\nFE-PaLM2\nCoh\nEng\nNat\nGro\nUnd\nOverall\nBLEURT\n0.524 0.558 0.59\n0.662 0.622 0.67\nPaLM2 Prob 0.711 0.784 0.808 0.588 0.711 0.792\nTable 6: FE-PaLM2 and Assistant Evaluators Turn-level Spearman (\u03c1) correlations on TopicalChat.\nFE-GPT-4\nCoh\nCon\nFlu\nRel\nOverall\nBLEURT\n0.583 0.795 0.733 0.6\n0.7\nNLI\n0.633 0.745 0.717 0.617 0.717\nSumBLEURT 0.717 0.41\n0.633 0.667 0.667\nTable 7: FE-GPT-4 and Assistant Evaluators System-level Kendall-Tau (\u03c4) correlations on SummEval.\nFE-GPT-4\nCoh\nEng\nNat\nGro\nUnd\nOverall\nBLEURT\n0.577 0.644 0.565 0.693 0.617 0.678\nPaLM2 Prob 0.747 0.713 0.86\n0.662 0.799 0.798\nTable 8: FE-GPT-4 and Assistant Evaluators Turn-level Spearman (\u03c1) correlations on TopicalChat.\nHow does Fusion-Eval integrate the assistant evaluators during execution? When looking at the\ncorrelation of assistant evaluators to FE-PaLM2 (as shown in Tables 5 and 6) and to FE-GPT-4 (as shown\nin Tables 7 and 8) together with the LLM\u2019s strategic plan (as shown in Tables 2), we notice that selected\nassistant evaluators consistently show higher correlation with FE-PaLM2 and FE-GPT-4. For example,\nin SummEval\u2019s coherence, SumBLEURT demonstrates a higher correlation than other evaluators. A\nsimilar trend is also observed in TopicalChat\u2019s naturalness and understandability. Additionally, none of\nthe correlations between assistant evaluators and Fusion-Eval equals \u201c1\u201d, suggesting that Fusion-Eval\u2019s\napproach uses assistant evaluators to supplement its judgment rather than relying entirely on them.\nA.4\nFusion-Eval Evaluation Prompt Template for SummEval (One Prompt Only in This\nSubsection - Do Not Be Surprised by Its Length)\nSections before the input template are generated by the planning LLM, while those after it are human-\ncreated.\nEvaluate a provided summary using criteria : Coherence, Consistency, Relevance, and Fluency.\nAssistant Evaluators like NLI, BLEURT, and SUM_BLEURT, which give scores between below 0 and 1 (closer to 1\nbeing better ) , will\nassist in this\nevaluation .\n**1. NLI (Natural Language Inference)**:\nThis assistant\nevaluator provides a probability\nscore indicating how much the summary (hypothesis) is entailed\nby the original news article\n(premise) .\n**Usage**:\n\u2212 **Consistency Evaluation**: A high entailment\nprobability\nindicates\nthat the summary is factually\naligned with\nthe source text . Conversely, a low score might indicate\ndiscrepancies or hallucinated\nfacts .\n**2. BLEURT**:\nThis metric models human judgments. It gives a score indicating how closely the summary aligns with what human\nevaluators might consider a good summary given the source text .\n**Usage**:\n\u2212 **Relevance and Consistency Evaluation**: A high BLEURT score would suggest that the summary effectively\ncaptures the\nessential\npoints of the source. A low score might indicate missing key points .\n**3. SUM_BLEURT (Summarization BLEURT)**:\nFine\u2212tuned on a summarization dataset , this\nassistant\nevaluator\noffers a more targeted approach to measuring the\nquality of summaries in the context of human judgments.\n**Usage**:\n\u2212 **Relevance and Coherence Evaluation**: Like BLEURT, but given its\nspecialization\nin summarization,\nSUM_BLEURT could offer more precise insights into the relevance and coherence of the summary in relation to\nthe source text .\n**Plan Using Assistant Evaluators **:\n1. **Read the News Article and Summary**: Begin with a manual reading to form an\ninitial\nimpression .\n2. **Use NLI & BLEURT for Consistency**: Check both scores. High scores from both assistant\nevaluators will\nreaffirm the consistency of the summary.\n3. **Use BLEURT & SUM_BLEURT for Relevance**: Check scores from both assistant evaluators. High scores would\nsuggest a good summary in terms of relevance .\n4. **Use SUM_BLEURT for Coherence**: Check SUM_BLEURT score. High scores would suggest a good summary in\nterms of coherence.\n5. **Manual Evaluation for Fluency**: The assistant\nevaluators don\u2019t directly address fluency . You\u2019ll evaluate\ngrammar, punctuation , and sentence\nstructure manually.\n6. **Final Judgment**: The assistant\nevaluators \u2019 outputs will inform and validate your evaluations , but the\nultimate judgment will be based on the provided\ncriteria\nand steps , with the\nassistant\nevaluators serving\nas supplementary aids .\n** Criteria & Steps**:\n1. **Coherence (1\u22125)**:\n\u2212 Read the news article and the summary.\n\u2212 Compare the summary to the\narticle\nfor\nclarity and logical order .\n\u2212 Use SUM_BLEURT scores as supplementary insights for coherence.\n\u2212 Assign a coherence score based on organization and structure .\n2. **Consistency (1\u22125)**:\n\u2212 Use NLI & BLEURT to get scores.\n\u2212 Read the\narticle and summary.\n\u2212 Compare factual details .\n\u2212 Assign a consistency score based on factual alignment.\n3. **Relevance (1\u22125)**:\n\u2212 Use BLEURT & SUM_BLEURT to get alignment scores with human\u2212like judgments.\n\u2212 Read both the\narticle and summary.\n\u2212 Identify main points and coverage in the summary.\n\u2212 Assign a relevance score based on content importance and absence of redundancies .\n4. **Fluency (1\u22125)**:\n\u2212 Evaluate the summary manually for grammar, punctuation , and sentence\nstructure .\n\u2212 Assign a fluency score based on readability .\n**Evaluation Summary (1\u22125)**:\nConsider the scores from each criterion and their importance.\n\u2212 Derive an average score , ensuring the final score ranges between 1\u22125.\n\u2212 Provide overall comments on the summary.\n\u2212 Highlight strengths and areas needing improvement.\n**Input Template**:\nSource:\n[Provide the source text here]\nAnswer:\n[Provide the summary text here]\nNLI Score (Source as Premise and Answer as Hypothesis):\n[Provide NLI entailment\nprobability\nscore]\nBLEURT Score (Source as Premise and Answer as Hypothesis):\n[Provide BLEURT score]\nSUM_BLEURT Score (Source as Premise and Answer as Hypothesis):\n[Provide SUM_BLEURT score]\n**Output Template**:\nCriterias \u2019 Scores and Explanations :\nCoherence\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nConsistency\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nRelevance\nScore: [Your evaluation ] Explanation :[Your explanation on evaluation ]\nFluency\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nEvaluation Summary:\nOverall Score: [Your evaluation ]\nExplanation: [Your explanation on evaluation ]\n**Input Example**:\nSource:\n[[ source ]]\nAnswer:\n[[summary]]\nNLI Score (Source as Premise and Answer as Hypothesis):\n[[ nli_score_source_answer ]]\nBLEURT Score (Source as Premise and Answer as Hypothesis):\n[[ bleurt_score_source_answer ]]\nSUM_BLEURT Score (Source as Premise and Answer as Hypothesis):\n[[sum_bleurt_score_source_answer]]\nEvaluation ( please follow Output Template and provide the evaluation\nresult ):<< eval_result >>\nA.5\nFusion-Eval Evaluation Prompt Template for TopicalChat (One Prompt Only in This\nSubsection - Do Not Be Surprised by Its Length)\nSections before the input template are generated by the planning LLM, while those after it are human-\ncreated.\nYou will be given a conversation between two individuals , followed by a potential response for the next turn in\nthe conversation , which includes an interesting\nfact . Your task is to rate the responses on six metrics :\nCoherence, Engagingness, Naturalness , Groundedness, Understandability , and Overall Quality .\nAssistant Evaluators \u2019 Descriptions and Usage:\n**1. LM_PROB (Language Model Probability):**\n\u2212 ** Functionality **: LM_PROB provides a probability score , ranging from 0 to 1, indicating the likelihood\nthat a\ngiven response would be generated by a language model, given the preceding conversation and fact .\n\u2212 **Score Range**: 0 ( least\nlikely ) to 1 (most likely ) .\n\u2212 **Usage**:\n\u2212 **Naturalness Evaluation**: A higher probability\nscore suggests that the response is more likely to occur\nnaturally in human conversation,\nindicating\ngreater\nnaturalness .\n\u2212 ** Understandability Evaluation**: Similarly , a higher\nprobability can also imply that the response is more\nunderstandable within the given context , as it is more aligned with expected language patterns .\n**2. BLEURT:**\n\u2212 ** Functionality **: BLEURT evaluates the quality of text generation by comparing the generated text (response)\nto a reference ( conversation and fact ) . Its score range is 0 to 1, where higher scores indicate\nbetter\nalignment and quality .\n\u2212 **Score Range**: 0 (poor alignment) to 1 ( excellent alignment) .\n\u2212 **Usage**:\n\u2212 **Groundedness Evaluation**: A high BLEURT score indicates that the response accurately and relevantly\nutilizes\nthe given fact , showing strong groundedness in the context of the conversation .\nPlan Using Tools for Conversation Response Evaluation :\n1. **Read the Conversation , Fact, and Response**: Begin with a careful reading of the provided materials to form\nan\ninitial\nqualitative\nimpression of the response in the context of the conversation and fact .\n2. **Use LM_PROB for Naturalness and Understandability Evaluation**:\n\u2212 Apply LM_PROB to determine the probability that the response would be generated by a language model in the\ngiven context .\n\u2212 High probability\nscores from LM_PROB will indicate greater naturalness and understandability , as the\nresponse aligns well with expected language patterns .\n3. **Use BLEURT for Groundedness Evaluation**:\n\u2212 Employ BLEURT to assess how accurately and relevantly the response\nutilizes\nthe given fact in the context\nof the conversation .\n\u2212 A high score from BLEURT suggests that the response is well\u2212grounded in the provided fact , demonstrating\naccuracy and relevance .\n4. **Final Judgment and Integration of Tool Outputs**:\n\u2212 Integrate the outputs from the tools with your\ninitial\nqualitative\nassessment.\n\u2212 The tools \u2019 outputs will provide\nquantitative\nsupport and validation\nfor your evaluations in each metric .\n\u2212 Make the final judgment based on a holistic view, considering both the tool outputs and the original\nevaluation\ncriteria\nfor each metric .\n\u2212 Remember that the ultimate judgment should align with the predefined\ncriteria\nand evaluation steps , with\nthe tools serving as important but supplementary aids in the decision\u2212making process.\n** Criteria & Steps**:\n1. **Coherence (1\u22123, Any Floating Value)**:\n\u2212 Read the conversation , fact , and response to assess the logical flow and continuity .\n\u2212 Evaluate how well the response connects with and continues the conversation .\n\u2212 Assign a Coherence score , ranging from 1 to 3, based on the response\u2019s organization and logical\nintegration\ninto the conversation .\n2. **Engagingness (1\u22123, Any Floating Value)**:\n\u2212 Review the conversation , fact , and response to determine the level of\ninterest\nor intrigue .\n\u2212 Assess how the response contributes to the conversation \u2019s value and captivates\ninterest .\n\u2212 Assign an Engagingness score , ranging from 1 to 3, based on the response\u2019s ability\nto captivate and add\nvalue to the conversation .\n3. **Naturalness (1\u22123, Any Floating Value)**:\n\u2212 Read the conversation , fact , and response to gauge the natural\nfit of the response within the conversation \u2019\ns context .\n\u2212 Evaluate the tone, formality , and conversational flow to determine how naturally the response fits .\n\u2212 Use LM_PROB to supplement the evaluation, considering the likelihood of such a response in the given\ncontext .\n\u2212 Assign a Naturalness score , ranging from 1 to 3, focusing on how naturally the response fits\ninto the\nconversation .\n4. **Groundedness (0\u22121, Any Floating Value)**:\n\u2212 Examine the conversation , fact , and response to evaluate how well the response\nutilizes\nthe given fact .\n\u2212 Assess the accuracy and relevance of the fact in the response .\n\u2212 Utilize BLEURT to provide supplementary insights into how accurately the response is grounded in the given\nfact .\n\u2212 Assign a Groundedness score, ranging from 0 to 1, based on the\neffective and accurate\nincorporation of the\nfact in the response.\n5. ** Understandability (0\u22121, Any Floating Value)**:\n\u2212 Review the conversation , fact , and response to assess the\nclarity and comprehension of the response .\n\u2212 Focus on how clearly and easily the response can be understood within the context of the preceding\nconversation .\n\u2212 Apply LM_PROB for additional data on the\nunderstandability\nof the response .\n\u2212 Assign an Understandability score , ranging from 0 to 1, based on the response\u2019s clarity and ease of\ncomprehension in context .\n6. **Overall Quality (1\u22125, Any Floating Value)**:\n\u2212 Review the scores and insights from the previous\ncriteria , including data from assistant\nevaluators .\n\u2212 Consider how the aspects of Coherence, Engagingness, Naturalness , Groundedness, and Understandability\ncollectively\ncontribute to the overall impression of the response .\n\u2212 Assign an Overall Quality score , ranging from 1 to 5, based on a holistic\nassessment of the response\u2019s\nstrengths and weaknesses.\n\u2212 Provide a summary explanation for the overall\nquality\nrating ,\nhighlighting key factors and insights\nthat\ninfluenced the judgment.\n**Input Template**:\nConversation:\n[Provide the conversation text here]\nFact:\n[Provide the fact text here]\nResponse:\n[Provide the response text here]\nLM_PROB Score (Response in Context of Conversation and Fact):\n[Provide LM_PROB probability score]\nBLEURT Score (Response with Conversation and Fact as Reference):\n[Provide BLEURT score]\n**Output Template**:\nCriteria Scores and Explanations :\nCoherence\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nEngagingness\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nNaturalness\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nGroundedness\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nUnderstandability\nScore: [Your evaluation ] Explanation: [Your explanation on evaluation ]\nEvaluation Summary:\nOverall Score: [Your evaluation ] Explanation: [Your comprehensive explanation on the overall\nevaluation ,\nintegrating\naspects from each criterion ]\n**Input Example**:\nConversation:\n[[ conversation ]]\nFact:\n[[ fact ]]\nResponse:\n[[ response ]]\nLM_PROB Score (Response in Context of Conversation and Fact):\n[[lm_prob_score]]\nBLEURT Score (Response with Conversation and Fact as Reference):\n[[ bleurt_score ]]\nEvaluation ( please follow Output Template and provide the evaluation\nresult ):<< eval_result >>\n"
  },
  {
    "title": "Llamas Know What GPTs Don't Show: Surrogate Models for Confidence Estimation",
    "link": "https://arxiv.org/pdf/2311.08877.pdf",
    "upvote": "5",
    "text": "Preprint\nLLAMAS KNOW WHAT GPTS DON\u2019T SHOW:\nSURROGATE MODELS FOR CONFIDENCE ESTIMATION\nVaishnavi Shrivastava,\nPercy Liang,\nAnanya Kumar\nStanford University\n{vshrivas, pliang, ananya}@cs.stanford.edu\nABSTRACT\nTo maintain user trust, large language models (LLMs) should signal low confi-\ndence on examples where they are incorrect, instead of misleading the user. The\nstandard approach of estimating confidence is to use the softmax probabilities\nof these models, but as of November 2023, state-of-the-art LLMs such as GPT-\n4 and Claude-v1.3 do not provide access to these probabilities. We first study\neliciting confidence linguistically \u2014 asking an LLM for its confidence in its an-\nswer \u2014 which performs reasonably (80.5% AUC on GPT-4 averaged across 12\nquestion-answering datasets \u2014 7% above a random baseline) but leaves room for\nimprovement. We then explore using a surrogate confidence model \u2014 using a\nmodel where we do have probabilities to evaluate the original model\u2019s confidence\nin a given question. Surprisingly, even though these probabilities come from a\ndifferent and often weaker model, this method leads to higher AUC than linguistic\nconfidences on 9 out of 12 datasets. Our best method composing linguistic con-\nfidences and surrogate model probabilities gives state-of-the-art confidence esti-\nmates on all 12 datasets (84.6% average AUC on GPT-4).\n1\nINTRODUCTION\nAs large language models (LLMs) are increasingly deployed, it is important that they signal low con-\nfidence on examples where they are likely to make mistakes. This problem is called selective clas-\nsification (or classification with a reject option) and is widely studied in machine learning (Cordella\net al., 1995; Geifman & El-Yaniv, 2017; Feng et al., 2019; Jones et al., 2021), learning theory (El-\nYaniv & Wiener, 2010; Bartlett & Wegkamp, 2008), and natural language processing (Kamath et al.,\n2020; Liang et al., 2022; Xiong et al., 2023). Traditional approaches leverage the model\u2019s softmax\nprobabilities (Hendrycks & Gimpel, 2017; Jones et al., 2021; Liang et al., 2022) or the model\u2019s\nrepresentations (Lee et al., 2018). This paper\u2019s goal is to produce good confidence estimates for\nstate-of-the-art LLMs, which do not provide model probabilities or representations (such as GPT-4\nand Claude-v1.3).\nWe first examine a natural idea of eliciting linguistic confidence scores (Tian et al., 2023; Lin et al.,\n2022; Xiong et al., 2023) \u2014 prompting the LLM to assess its confidence in its answer (Figure 1,\nGPT-4 Linguistic). We find that linguistic confidences work reasonably well for state-of-the-art\nmodels, and much better than a random guessing baseline, but still leave room for improvement\n(Section 3). Averaged across the datasets, GPT-4 achieves a selective classification AUC of 80.5%,\nwhich is 7% above a random guessing baseline. Our results hold across 12 standard datasets (8\nMMLU datasets, TruthfulQA, CommonsenseQA, OpenbookQA, and MedQA), 5 models (GPT-4,\nClaude-v1.3, GPT-3.5, Llama 2, and text-davinci-003), and 24 different prompt formats (e.g., chain-\nof-thought, different instructions, fake few-shot prompts). However, linguistic confidences perform\nmuch worse than using model probabilities when these probabilities are available (for less accurate\nmodels). For example, on Llama 2 linguistic confidences achieve an average AUC 10.7% lower than\nmodel probabilities, suggesting scope for further refinement in these confidence assessments.\nConsequently, we propose a surrogate model approach of taking the answer from GPT-4 or Claude-\nv1.3, but the confidence from a different model such as Llama 2 (Figure 1, Surrogate), where softmax\nprobabilities are available, as a confidence estimate for the original model\u2019s answer (Section 4). Sur-\nrogate confidence modeling improves the average selective classification AUC for GPT-4 to 82.1%.\nEven using a weaker or much smaller surrogate model like text-davinci-003 or Llama 2-13B leads to\n1\narXiv:2311.08877v1  [cs.CL]  15 Nov 2023\nPreprint\nFigure 1: Our goal is to provide good confidence estimates for state-of-the-art LLMs like GPT-4 and\nClaude-v1.3 which currently do not give access to their internal probabilities. One natural approach\n(GPT-4 Linguistic) is to prompt the model asking for its confidence. Interestingly, we find that taking\nthe answer from GPT-4, but the internal probability from a different surrogate model (e.g., an open\nmodel such as Llama 2) gives even better results (0.82 AUC). Mixing GPT-4\u2019s linguistic confidences\nwith the surrogate model probabilities gives further gains (0.83 AUC). Our AUC numbers are better\nthan concurrent work (Xiong et al., 2023), but combining these approaches leads to the best results\n(Mixture++; 0.85 AUC). Our findings also hold for Claude-v1.3 and GPT-3.5 (Section 4 and 5).\ncomparable or better AUCs for stronger models such as GPT-4, Claude-v1.3, and GPT-3.5. Intrigu-\ningly, confidence scores can transfer between models, even if the model generating the confidence\nscore is different (or much worse). In Section 4, we provide some analysis and intuitions for this\nbehavior.\nWe find that linguistic confidence scores and surrogate model probabilities are complementary: com-\nbining these scores leads to further gains (Figure 1, Mixture). For example, this mixture method in-\ncreases the selective classification AUC of GPT-4 to 83.4%. The mixture method also outperforms\nconcurrent work (Xiong et al., 2023) on self-consistency (AUC: 82.8%), which is more expensive\n(involves sampling GPT-4 five times per input) and involves post-processing. Combining our method\nwith self-consistency-based confidence scores leads to the best results: average AUC 84.6%.\nOur analysis suggests that linguistic confidence scores are limited because they are very coarse-\ngrained \u2014 for example, GPT-4 outputs the exact same confidence (0.9) on 50% of examples, which\nconstrains its ability to separate correct and incorrect answers. Surrogate model probabilities work\nwell even on a different model, because the examples that are challenging for one model transfer\nover to a different model. Finally, mixing in just a small fraction of surrogate model probabilities\nallows answers which previously had the same linguistic confidence to be separable through different\ncomposite confidence scores, boosting the overall performance with minimal interventions.\n2\nSETUP\nOur goal is selective classification: outputting confidence scores that are higher on inputs where\nthe model is correct, than inputs where the model is incorrect (El-Yaniv & Wiener, 2010; Geifman\n& El-Yaniv, 2017). We focus on state-of-the-art language models such as GPT-4 and Claude-v1.3,\nwhich currently do not expose probabilities computed in their softmax output layer.\nTask. Given a text input x, a model outputs a (possibly stochastic) answer y(x). Let R(x, y) = 1\nif an answer y is correct for input x, and 0 otherwise. Our goal is to output a confidence score\nC(x) \u2208 [0, 1]. Good confidence scores are essential in real world machine learning systems: for\n2\nPreprint\ninputs when C(x) is lower, we can defer to a human expert or alert the user, instead of misleading\nthe user with an incorrect answer.\nMetrics. A popular metric for selective classification is the AUC (area under the coverage-accuracy\ncurve) (El-Yaniv & Wiener, 2010; Liang et al., 2022), which examines how accurate the model is\nif allowed to abstain (say \"I don\u2019t know\") on some examples. Let A(c) be the selective accuracy\nat coverage c: the accuracy if the model only makes a prediction on the c proportion of data with\nhighest confidence scores. To enable tie-breaking to make different predictions for examples with\nthe same confidence score, we add a small amount of Gaussian noise to each confidence score\nN(0, \u03f5), \u03f5 \u2192 0. The AUC is the average selective accuracy A(c) over all c:\nAUC(C, y) = lim\n\u03f5\u21920\nZ 1\n0\nE [A(c)] dc\n(2.1)\nA random baseline (outputting uniform random probabilities for each input) achieves AUC(C, y) =\naccuracy, so a model with good confidence scores should achieve a higher AUC than accuracy. Note\nthat adding the noise N(0, \u03f5) is critical because linguistic confidences for different examples are\noften identical \u2014 without the noise we would substantially underestimate the AUC of the models\n(see Appendix A.3 for more details).\nWe also examine the AUROC, a standard metric (Hendrycks & Gimpel, 2017; Xiong et al., 2023)\nused to examine how well confidence scores can distinguish between correct and incorrect examples.\nWe label an example \u2018Positive\u2019 if the model gets it correct and \u2018Negative\u2019 otherwise, and plot the true\npositive rate against the false positive rate at different classification thresholds \u2014 the AUROC is the\narea under this curve (See Appendix A.3 for more details). Outputting random confidence scores\ngets an AUROC of 0.5, so a model with good confidence scores should achieve AUROC above 0.5.\nWe also report ECE (expected calibration error) numbers in Appendix A.6. ECE examines if a\nmodel\u2019s confidence aligns with its accuracy, but does not indicate the model\u2019s ability to distinguish\nbetween correct and incorrect examples, so we focus on the AUC and AUROC metrics.1\nDatasets. We study model performance and confidence on twelve standard question answering\ndatasets: TruthfulQA (TQA) (Lin et al., 2021), CommonsenseQA (CSQA) (Talmor et al., 2019),\nOpenbookQA (OBQA) (Mihaylov et al., 2018), MedQA (Jin et al., 2021), and 8 MMLU (Hendrycks\net al., 2021) datasets - professional law (Law), business ethics (Ethics), conceptual physics (Physics),\neconometrics (Econ), abstract algebra (Algebra), college chemistry (Chem), computer security (Se-\ncurity), and US Foreign Policy (Policy). These datasets span several diverse categories including\nmath reasoning, scientific knowledge, computer science, social science, and commonsense reason-\ning. We sample 250 questions from the test split of each dataset to report results on (if the test set is\nsmaller, we use the full test set). See Appendix A.1 for more details.\nModels. We study state-of-the-art language models, most of which do not provide access to internal\nprobabilities as of the writing of this paper \u2014 GPT-4 (OpenAI, 2023a), Claude-v1.3, and GPT-3.5-\nTurbo (OpenAI, 2022) (June 13th, 2023, snapshot). We also study a few recent models which do\nprovide model probabilities for systematic comparisons \u2014 Llama 2 and Llama 2 Chat (70B and\n13B sizes) (Touvron et al., 2023) and text-davinci-003 OpenAI (2023b). If Llama 2 is mentioned\nin the text without further identifiers, we refer to the Llama 2 70B base model.\n2.1\nCONFIDENCE ELICITATION METHODS\nLinguistic Confidences. For each question, we zero-shot prompt models with an instruction to out-\nput a valid answer and a confidence assessment of that answer, sampling the answer and confidence\ntogether in a single generation. We generate greedily with temperature T = 0, and define these\nconfidence estimates generated by the model to be linguistic confidences. Since there can be many\nways of eliciting linguistic confidences, we experiment with 24 different prompts across various cat-\negories (chain-of-thought, different instructions, fake few shot examples). We find the results to be\nconsistent across prompts, so we report results on our best prompt (see Figure 2 for an example in-\nstruction of linguistic confidence elicitation). Section 3 assesses the quality of linguistic confidences\nand signals a need for better confidence estimation methods.\n1Intuitively, calibration requires that if we output a 0.6 confidence on 100 examples, then we should get\n0.6 \u00b7 100 = 60 of them correct. For a classifier with accuracy A, one (degenerate) way to have perfect\ncalibration (best possible ECE) is to output confidence C(x) = A for every example x.\n3\nPreprint\nModel Probabilities. Models such as Llama 2 and text-davinci-003 provide token-level probabili-\nties for text. We let the confidence score be the probability of the generated answer choice.\nFigure 2: Linguistic Confidence Prompt Instruction for\nthe best linguistic confidence prompt (see exact prompt\nin Appendix A.4).\nSurrogate models for confidences.\nSince models such as GPT-4 do not\ngive a confidence estimate, we propose\nusing a surrogate model (e.g., Llama 2)\nto provide confidence estimates. For-\nmally, given an input x we output\ny(x) = ygpt-4(x) (GPT-4\u2019s answer) and\nC(x) = CLlama 2(x) (Llama 2\u2019s confi-\ndence in its own answer). Even though\nthese confidence scores come from a\ndifferent model, Section 4 shows that\nthe surrogate confidence method out-\nperforms linguistic confidence scores.\nMixture of models. We also propose a mixture of models method where we combine the linguistic\nconfidence from the main model and the surrogate model\u2019s confidence score: given input x we\noutput (1 \u2212 \u03b1)CM(x) + \u03b1CS(x) where M is the main model and S is the surrogate model.\nWe use Llama 2 70B as the surrogate model for all main models since it performs the best. We\noptimize \u03b1 to minimize AUC, sweeping over values from 0 to 1. Interestingly, in Section 5, we\nshow that even \u03b1 = 0.001 works well.\n3\nLINGUISTIC CONFIDENCES: ASKING THE MODEL FOR ITS CONFIDENCE\nAs of November 2023, state-of-the-art language models such as GPT-4 and Claude-v1.3 do not give\naccess to internal model probabilities. In this section, we examine linguistically eliciting confidence:\nprompt models to assign their answers a confidence score between 0 and 1. We find that these\nlinguistic confidences leave a lot of room for improvement (around 50-65% AUROC, compared\nto 50% for a random guessing baseline). These linguistic confidences are also much worse than\ninternal model probabilities when available (for weaker models such as text-davinci-003 and Llama\n2). We show AUC and AUROC results on all datasets and models in Table 1.\nAlgorithm 1: Mixture of Models Confidence\nData: A question x\nResult: A prediction by, a confidence score c\nby, c1 = MainModel (x) ;\nc2 = SurrogateModel (x) ;\nc = (1 \u2212 \u03b1)c1 + \u03b1c2 ;\nLinguistic confidences leave room for im-\nprovement.\nThe AUROC values of linguistic\nconfidences from text-davinci, Llama 2 70b, and\nGPT-3.5 are close to 50% (Table 1), which is the\nscore achieved by guessing a random confidence,\nindicating that linguistic confidences are not a re-\nliable means of separating correct and incorrect\nexamples. The linguistic confidences of the strongest models, Claude-v1.3 and GPT-4, are better\nand result in AUROCs in the 60-65% range, but still leave a lot of room for improvement. The\nAUCs of linguistic confidences are close to their accuracy (Appendix A.2) (which is the score\nachieved by a random guessing baseline) for text-davinci-003 (57.1% vs 57.7%), GPT-3.5 (58.1%\nvs 59.0%), and Llama 2 (58.8% vs 62.4%). Linguistic confidences for the best models are reason-\nable, but still leave room for improvement \u2014 GPT-4 has an accuracy of 73.5% and AUC of 80.5%;\nand Claude-v1.3 has an accuracy of 65.5% and AUC of 73.5%.\nLinguistic confidences are worse than model probabilities. The best current models (GPT-4 and\nClaude-v1.3) do not provide model probabilities, but we compare the quality of model probabilities\nand linguistic confidences for text-davinci-003 and the Llama 2 models. For these models, the model\nprobabilities result in better AUC and AUROC values for all of our datasets (Table 1). For Llama 2,\nthe model probabilities achieve a 10.7% higher AUC and 19.0% higher AUROC than the linguistic\nconfidences. The Chat model (Llama 2 70B Chat) shows similar trends (Appendix A.5).\nLinguistic confidences are robust to prompt variations. We examine linguistic confidences us-\ning 24 distinct prompts, including asking for numerical confidence or probability scores, asking the\nmodel to categorize its confidence into \u2018not sure\u2019, \u2018sure\u2019, and \u2018very sure\u2019, allowing the model to ex-\nplain confidences with chain-of-thought, asking the model for its confidence in a follow-up question,\nand varying the prompt instructions. We show results for the best prompt, as there was very little\n4\nPreprint\nConfidence Type\nTQA\nMedqa\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nAUC\nText-davinci Linguistic\n0.523\n0.504\n0.718\n0.775\n0.532\n0.590\n0.579\nText-davinci Prob\n0.607\n0.656\n0.861\n0.929\n0.714\n0.783\n0.697\nLlama 2 Linguistic\n0.600\n0.616\n0.693\n0.802\n0.605\n0.707\n0.638\nLlama 2 Prob\n0.711\n0.735\n0.804\n0.923\n0.749\n0.834\n0.763\nGPT-3.5 Linguistic\n0.620\n0.536\n0.693\n0.776\n0.508\n0.674\n0.526\nClaude-v1.3 Linguistic\n0.741\n0.718\n0.807\n0.879\n0.669\n0.894\n0.736\nGPT-4 Linguistic\n0.889\n0.841\n0.802\n0.960\n0.732\n0.869\n0.819\nAUROC\nText-davinci Linguistic\n0.525\n0.500\n0.503\n0.509\n0.500\n0.500\n0.500\nText-davinci Prob\n0.718\n0.696\n0.806\n0.840\n0.715\n0.758\n0.637\nLlama 2 Linguistic\n0.618\n0.541\n0.555\n0.484\n0.517\n0.602\n0.593\nLlama 2 Prob\n0.745\n0.722\n0.731\n0.777\n0.733\n0.868\n0.732\nGPT-3.5 Linguistic\n0.535\n0.500\n0.526\n0.518\n0.508\n0.509\n0.504\nClaude-v1.3 Linguistic\n0.701\n0.586\n0.639\n0.647\n0.586\n0.760\n0.652\nGPT-4 Linguistic\n0.665\n0.716\n0.551\n0.656\n0.591\n0.720\n0.522\nConfidence Type\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nAUC\nText-davinci Linguistic\n0.412\n0.300\n0.440\n0.690\n0.856\n0.577\nText-davinci Prob\n0.431\n0.338\n0.644\n0.891\n0.939\n0.707\nLlama 2 Linguistic\n0.415\n0.189\n0.474\n0.817\n0.930\n0.624\nLlama 2 Prob\n0.498\n0.263\n0.647\n0.866\n0.981\n0.731\nGPT-3.5 Linguistic\n0.430\n0.319\n0.465\n0.724\n0.806\n0.590\nClaude-v1.3 Linguistic\n0.640\n0.333\n0.653\n0.812\n0.934\n0.735\nGPT-4 Linguistic\n0.643\n0.551\n0.683\n0.903\n0.965\n0.805\nAUROC\nText-davinci Linguistic\n0.500\n0.500\n0.500\n0.500\n0.506\n0.504\nText-davinci Prob\n0.549\n0.532\n0.695\n0.858\n0.795\n0.717\nLlama 2 Linguistic\n0.533\n0.424\n0.520\n0.613\n0.576\n0.548\nLlama 2 Prob\n0.622\n0.546\n0.732\n0.775\n0.871\n0.738\nGPT-3.5 Linguistic\n0.518\n0.522\n0.505\n0.519\n0.519\n0.515\nClaude-v1.3 Linguistic\n0.573\n0.543\n0.708\n0.687\n0.645\n0.644\nGPT-4 Linguistic\n0.551\n0.599\n0.721\n0.750\n0.753\n0.650\nTable 1: AUC and AUROC - Linguistic Confidences vs Model Probabilities We compare the\nAUC and AUROC values for linguistic confidences and model probabilities in weaker models (text-\ndavinci-003 and Llama 2 70B), and find that model probabilities consistently outperform linguis-\ntic confidences. For closed source models (which don\u2019t provide model probabilities), we see that\nClaude-v1.3 and GPT-4 provide the best linguistic confidences in both AUC and AUROC.\ndifference in performance across prompts \u2014 our results hold for other prompts as well. A more\ndetailed description of the prompts investigated and the method for selecting the best prompt can be\nfound in Appendix A.4.\nLinguistic confidences improve with scale, but not enough. The quality of linguistic confidences\nimproves with model scale. We see that GPT-4 and Claude-v1.3 have the best linguistic confidences,\nfollowed by the Llama 2 70B models, GPT-3.5, and finally text-davinci-003. While the linguistic\nconfidences from GPT-4 are not bad (65% average AUROC), they are worse than model probabilities\nfrom Llama 2 70b (74%) and even text-davinci-003 (72%). Note that AUC scores increase with\naccuracy \u2014 GPT-4 Linguistic has the highest AUC because GPT-4 has much higher accuracy than\nLlama 2. The overall utility of a selective classifier depends on both its accuracy and confidence\nquality, so in the next section we examine ways to improve the confidences of our best-in-class\nmodels \u2014 GPT-4 and Claude-v1.3.\n5\nPreprint\nFigure 3: AUCs for Different Surrogate Models. We plot the AUC as we vary the main model (on\nthe x-axis) and the surrogate model (on the y-axis). Using surrogate model probabilities as confi-\ndence estimates improves AUCs for all models over their own linguistic confidences\u2014the bottom 4\nrows (surrogate probabilities) are darker than the top 6 rows (linguistic confidences). Even model\nprobabilities from a smaller Llama 2 13B model lead to comparable or better AUCs for all models.\n4\nSURROGATE MODELS ARE RELIABLE CONFIDENCE ESTIMATORS\nIn the previous section we found that linguistic confidences leave room for improvement. Here we\nshow that model probabilities from a separate \u2018surrogate\u2019 model can surprisingly provide better con-\nfidence estimates for a model than its own linguistic confidence scores, even though the probabilities\ncome from a different model.\n4.1\nRESULTS\nSurrogate model confidences outperform linguistic confidences. AUC improves for all models\nwhen probabilities from a surrogate model are used, as opposed to using the model\u2019s own linguistic\nconfidences. Figure 3 shows a heatmap of the AUC for different main models (on the x-axis) as we\nvary the surrogate model (on the y-axis). We see that model probabilities (bottom four rows) lead\nto higher AUC (are darker) than linguistic confidences (top six rows) even when the probabilities\ncome from a different model. For example, using Llama 2 70B probabilities as a surrogate improves\nAUC from 80.5% to 82.1% for GPT-4, 73.5% to 76.3% for Claude-v1.3, and 59.0% to 72.1% for\nGPT-3.5, and AUROC also shows similar increases for all models (Table 2, Figure 4).\nWeak surrogates are also good confidence estimators. Even using Llama 2 13B or text-davinci-\n003 as a surrogate leads to comparable or better performance than using a model\u2019s own linguistic\nconfidences. We found this intriguing because these models are much smaller and less accurate, e.g.,\nLlama 2 13B has an average accuracy of 47.2% vs. 65.5% for Claude-v1.3 and 73.5% for GPT-4.\nOther findings. Recent work suggests chat models trained using reinforcement learning from hu-\nman feedback (RLHF) might be less calibrated than base models. In Appendix A.7, we compare\nchat and base model probabilities as surrogate confidences and find that Llama 2 70B base slightly\noutperforms Llama 2 70B chat in selective classification with both linguistic confidences and model\nprobabilities \u2014 but both models perform similarly as surrogates. As we might expect, in general bet-\nter models (such as Llama 2 70B) are better surrogates. Finally, we find that linguistic confidences\nfrom stronger models can provide good surrogate confidences for weaker models \u2014 the AUC of\nGPT-3.5 improves by 5.7% when using GPT-4\u2019s linguistic confidences instead of its own.\n6\nPreprint\nText-davinci\nGPT-3.5\nLlama 2\nClaude-v1.3\nGPT-4\nAUC\nLing. Conf.\n0.577\n0.590\n0.624\n0.735\n0.805\nSurrogate\u2020\n0.707\n0.719\n0.731\n0.763\n0.821\nTiebreak\u2020\n0.711\n0.719\n0.715\n0.764\n0.830\nMixture of Models\u2020\n0.711\n0.722\n0.731\n0.772\n0.834\nAUROC\nLing. Conf.\n0.504\n0.514\n0.548\n0.637\n0.646\nSurrogate\u2020\n0.717\n0.708\n0.738\n0.671\n0.657\nTiebreak\u2020\n0.718\n0.708\n0.699\n0.683\n0.692\nMixture of Models\u2020\n0.718\n0.709\n0.737\n0.687\n0.699\nTable 2: AUC and AUROC of Surrogate and Mixture of Model Methods. We compare the\nperformance of our proposed methods\u2020 with the baseline linguistic confidence method (gray). For\nboth AUC and AUROC, our proposed methods outperform linguistic confidences on all models.\nMixture of models improves the AUC of GPT-4 by 3% and AUROC by 5%.\n5\nMIXTURES OF MODELS FOR BETTER CONFIDENCE ESTIMATES\nIn the previous section, we proposed the use of surrogate models \u2014 using a main model to produce\nanswers and a separate, surrogate to estimate the main model\u2019s confidence in the answers \u2014 and\nfound surrogates to outperform linguistic confidence scores elicited from the main model. In this\nsection, we find that the signals from linguistic confidences and surrogate probabilities are comple-\nmentary \u2014 the two can be composed to get state of the art confidence estimates for all models.\n5.1\nRESULTS\nMixtures of models provide best confidences. Mixing surrogate and linguistic confidences (Al-\ngorithm 1) leads to the best confidence estimates for all models \u2014 AUCs increase from 80.5% to\n83.4% for GPT-4 and 73.5% to 77.2% for Claude-v1.3 (Table 2). The optimal \u03b1 (Algorithm 1)\nfor best average performance across tasks is 0.4 for GPT-4 and 0.6 for Claude-v1.3. AUROCs also\nincrease for these models, by 5.3% for GPT-4 and 5.0% for Claude-v1.3 (Table 2). We also plot the\nselective accuracy against coverage in Figure 4, where the mixture and surrogate method lie above\nthe linguistic confidences curve.\nEpsilon is all you need. We also study a special case of mixtures called tiebreaking, where we set\n\u03b1 to a small value \u03f5 \u2192 0 (Algorithm 1) \u2014 this simply uses the surrogate model to \u2018break ties\u2019\nand provide relative ordering for examples with the same linguistic confidence. Adding only 0.1%\nof a surrogate model\u2019s probabilities to a model\u2019s linguistic confidences performs better than using\neither the linguistic confidences or surrogate probabilities alone, and closely matches performance\nof the optimal \u03b1 (Table 2). For GPT-4, tiebreaking achieves 86% of the AUC gains (over linguistic\nconfidences) of the optimal \u03b1, and 87% of the AUROC gains.\nMixing surrogate and self-consistency confidences leads to further gains.\nConcurrent\nwork (Xiong et al., 2023) on eliciting linguistic confidences uses self-consistency (SC) to sample\nmultiple linguistic confidence scores for each answer and aggregates them through a post processing\ntechnique. For further gains, we experiment with leveraging these SC-based linguistic confidences\nfor GPT-4 \u2014 we replace linguistic confidences c1 in Algorithm 1 with the outputs of their best\nmethod (hybrid self-consistency). The updated Algorithm 1 leads to state-of-the-art confidence es-\ntimates, also outperforming their hybrid self-consistency technique (Table 3), with an overall 4.1%\ngain in AUC for GPT-4 over vanilla linguistic confidences, and a 9.1% gain in AUROC.\nOther findings. Probabilities of smaller surrogate models can also be composed with linguistic\nconfidences \u2014 composing Llama 2 13B\u2019s probabilities with GPT-4\u2019s linguistic confidences retains\n66% of the AUC gains seen from composing GPT-4 with Llama 2 70B. Composing GPT-4 and\nClaude-v1.3\u2019s linguistic confidences can boost GPT-4\u2019s AUC by 2.1% and AUROC by 3%, indicat-\ning that linguistic confidences of different models can provide complementary estimates of uncer-\ntainty. Additionally, we find that even composing the model probabilities of two different models\ncan provide better confidence estimates \u2014 composing Llama 2\u2019s probabilities with those of Llama\n2 Chat improves Llama 2\u2019s AUC from 73.1% to 73.8% and AUROC from 73.8% to 74.5%. Mixing\nconfidences from more than two models could potentially lead to further improvements.\n7\nPreprint\nMethod\nTQA\nMedqa\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nAUC\nLing. Conf.\n0.889\n0.841\n0.802\n0.960\n0.732\n0.869\n0.819\nSC Ling. Conf.\n0.903\n0.887\n0.841\n0.978\n0.729\n0.902\n0.846\nSurrogate\u2020\n0.866\n0.844\n0.849\n0.965\n0.762\n0.849\n0.891\nTiebreak\u2020\n0.902\n0.871\n0.833\n0.967\n0.768\n0.889\n0.861\nMixture\u2020\n0.895\n0.864\n0.849\n0.969\n0.780\n0.882\n0.886\nSC Mixture\u2020\n0.921\n0.873\n0.877\n0.979\n0.757\n0.894\n0.881\nAUROC\nLing. Conf.\n0.665\n0.716\n0.551\n0.656\n0.591\n0.720\n0.522\nSC Ling. Conf.\n0.698\n0.767\n0.625\n0.833\n0.619\n0.817\n0.592\nSurrogate\u2020\n0.543\n0.666\n0.656\n0.683\n0.619\n0.617\n0.648\nTiebreak\u2020\n0.671\n0.750\n0.611\n0.716\n0.628\n0.740\n0.589\nMixture\u2020\n0.642\n0.731\n0.646\n0.731\n0.655\n0.711\n0.648\nSC Mixture\u2020\n0.702\n0.747\n0.679\n0.838\n0.655\n0.783\n0.663\nMethod\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nAUC\nLing. Conf.\n0.643\n0.551\n0.683\n0.903\n0.965\n0.805\nSC Ling. Conf.\n0.663\n0.584\n0.726\n0.915\n0.965\n0.828\nSurrogate\u2020\n0.667\n0.572\n0.724\n0.888\n0.971\n0.821\nTiebreak\u2020\n0.654\n0.580\n0.746\n0.910\n0.974\n0.830\nMixture\u2020\n0.664\n0.581\n0.749\n0.908\n0.976\n0.834\nSC Mixture\u2020\n0.662\n0.645\n0.763\n0.926\n0.973\n0.846\nAUROC\nLing. Conf.\n0.551\n0.599\n0.721\n0.750\n0.753\n0.650\nSC Ling. Conf.\n0.622\n0.682\n0.818\n0.798\n0.755\n0.719\nSurrogate\u2020\n0.578\n0.621\n0.706\n0.779\n0.764\n0.657\nTiebreak\u2020\n0.569\n0.648\n0.760\n0.815\n0.805\n0.692\nMixture\u2020\n0.578\n0.648\n0.759\n0.814\n0.822\n0.699\nSC Mixture\u2020\n0.595\n0.763\n0.819\n0.839\n0.810\n0.741\nTable 3: AUC and AUROC of All Confidence Methods for GPT-4. Our proposed surrogate model\nmethod outperforms linguistic confidences on 9/12 datasets on AUC. Mixing surrogate probabili-\nties and linguistic confidences outperforms vanilla linguistic confidences on AUC for all 12 datasets.\nThe mixture of surrogate probabilities also outperforms hybrid self-consistency confidences, the best\nmethod in Xiong et al. (2023), on average (AUC 83.4% vs 82.8%. Mixing surrogate probabilities\nwith self-consistency linguistic confidences leads to the best confidence estimates overall, outper-\nforming all methods with an average 84.6% AUC and 74.1% AUROC, which is a gain of 4.1% and\n9.1% respectively over vanilla linguistic confidences.\n6\nANALYSIS\nWhy Are Vanilla Linguistic Confidences Worse Than Model Probabilities? In Section 3, we\nshowed that linguistic confidences underperformed model probabilities. Here we provide some in-\ntuitions for this behavior. We observe that the distribution of model probabilities is quite varied\n(1456 unique values for Llama 2 70B across 12 datasets), while the distribution of linguistic confi-\ndences is quite clustered (only 8 unique values for GPT-4 across 12 datasets). This clustering may\nbe because training corpora contain higher frequencies of \u201cnice\u201d probability numbers such as 90%\nor 100% (Zhou et al., 2023). The repetitiveness of linguistic confidences, compared to model prob-\nabilities, hinders relative confidence ordering and good AUC and AUROC performance \u2014 GPT-4\nrepetitively generates 0.9 for 50% of examples across 12 tasks, so it cannot separate them. We tried\nsimple ablations to increase linguistic confidence variation, by increasing the temperature of gener-\nations or instructing the model \u2018It\u2019s ok to be less sure of your answers.\u2019, but they did not improve\nAUC because they reduced model accuracy.\nWhy Does Surrogate Confidence Estimation Work? In Section 4, we demonstrate that models\ncan receive good quality confidence estimates from other surrogate models. In this section, we pro-\nvide some intuitions for our results. We find that for a main model M, a model S tends to be a\nbetter surrogate when there is a higher correlation in the questions answered correctly by M and S.\n8\nPreprint\n(a) MMLU - Professional Law\n(b) MMLU - US Foreign Policy\nFigure 4: Selective Accuracy vs. Coverage for GPT-4. Our surrogate and mixture methods have\na higher area under the selective accuracy vs coverage curve (AUC) than the linguistic confidence\nand random confidence baselines. We plot the coverage c on the x-axis and the selective accuracy\n(accuracy on the top c fraction of examples) on the y-axis, for two representative tasks. Notice that\nthe mixture (green solid) and surrogate (purple dashed) lines are above the linguistic confidence\n(blue dashed/dotted) and random guessing baseline (black dotted).\nThe questions GPT-4 answers correctly are more correlated with those that Llama 2 70B answers\ncorrectly (Pearson correlation of 0.39), than those that Llama 2 13B answers correctly (correlation\n0.19) (Appendix A.8). We also plot the embeddings of questions that GPT-4 gets incorrect (blue\ndots) and the questions two potential surrogates Llama 2 70B and Llama 2 13B get incorrect (green\ndots) (Figure 5). GPT-4 and Llama 2 70B tend to make mistakes on more of the same questions\n(more black dots on the left plot). We also see more spatial similarity in the mistakes of GPT-4 and\nLlama 2 70B. So better surrogate models S and their corresponding main models M may strug-\ngle with semantically related concepts, causing them to have low confidences on similar types of\nquestions. Intuitively, the probabilities of a surrogate like Llama 2 transfer well to a stronger model\nlike GPT-4 because Llama 2 is good at \u2018spotting\u2019 difficult questions, even if it cannot always an-\nswer them \u2014 we reason that both models have higher entropy probability distributions over answer\nchoices for more difficult questions, and more peaked probability distributions for easier questions.\nWhy Is Tiebreaking Sufficient? As mentioned, linguistic confidences tend to be repetitive and\nclustered at only a few values (e.g., 0.9), limiting their ability to separate correct and incorrect an-\nswers. Since a surrogate model\u2019s probabilities for each example are nearly unique, composing just\na small fraction of them with linguistic confidence scores (Section 5.1) can allow answers which\npreviously had the same linguistic confidence to now be separable through different composite con-\nfidence scores. This means that in cases where linguistic confidence scores are identical, we fall\nback on the surrogate model\u2019s probabilities to provide an order examples based on confidence.\n7\nRELATED WORK\nConfidence Estimation for LLMs. Confidence estimation for LLMs has been studied in several\nrelated works. Kadavath et al. (2022) show that Claude\u2019s model probabilities are well-calibrated on\nmultiple/choice and True/False questions. Zhou et al. (2023) study the effect of introducing expres-\nsions of uncertainty into prompts, on model accuracy. Our work differs from these since we focus\non confidence elicitation for models which don\u2019t provide log probabilities. Concurrent work (Xiong\net al., 2023) studies calibration and selective classification of linguistic confidence scores gener-\nated by LLMs. While this work also elicits prompted confidences, they focus on self-consistency\n(SC) based methods which are expensive because they require prompting GPT-4 several times. Our\nproposed Surrogate and Mixture of models methods are less expensive, since model probabilities\nfrom smaller models (Llama 2) are used to improve the confidence estimates of larger models (GPT-\n4). We also show performance improvements over their best method.\n(Lin et al., 2022) examine\nfine-tuning language models to improve confidence estimation, which we do not have access to.\n9\nPreprint\n(a) GPT-4 and Llama 2 70B\n(b) GPT-4 and Llama 2 13B\nFigure 5: Embeddings of Incorrect Questions for GPT-4 and Surrogate Models Plots of the\nembeddings of questions GPT-4 and two surrogate models (Llama 2 70B and Llama 2 13B) answer\nincorrectly on two representative datasets - TruthfulQA and College Chemistry. Questions only\nGPT-4 answers incorrectly are in blue, questions GPT-4 and the surrogate answer incorrectly are in\nblack, and questions only the surrogate answers incorrectly are in green. There are more questions\nthat both GPT-4 and Llama 2 70B answer incorrectly and more semantic similarity in their incorrect\nquestions. This indicates that Llama 2 70B and GPT-4 struggle with semantically related concepts\nand that the 70B model may more closely estimate GPT-4\u2019s uncertainty than the 13B model.\nSelective Classification and OOD Detection. Our paper focuses on selective classification, a clas-\nsical problem in machine learning (El-Yaniv & Wiener, 2010; Khani et al., 2016; Feng et al., 2019;\nJones et al., 2021) and statistics (Chow, 1970; Hellman & Raviv, 1970). A related problem is out-\nof-distribution detection (Pimentel et al., 2014; Liang et al., 2018; Ovadia et al., 2019), where the\ngoal is to detect examples very different from training (where the model may make mistakes). Prior\nwork uses internals of the models \u2014 probability outputs (Hendrycks & Gimpel, 2017), represen-\ntations (Lee et al., 2018) of models, or even updates the training procedure (Bartlett & Wegkamp,\n2008; Mozannar & Sontag, 2020) \u2014 which state-of-the-art LLMs do not currently give access to.\nCalibration. The general idea of confidence estimation is also studied in calibration (Murphy &\nWinkler, 1977; DeGroot & Fienberg, 1983; Naeini et al., 2014; Guo et al., 2017). While related,\nthe focus is different\u2014a model which outputs its accuracy on every example has 0 calibration error\n(ECE), but cannot separate correct and incorrect examples (Kuleshov & Liang, 2015).\n8\nCONCLUSION AND FUTURE WORK\nOur work aims to address the open challenge of eliciting good confidence estimates from state-of-\nthe-art LLMs such as GPT-4 and Claude-v1.3, which don\u2019t provide access to their internal proba-\nbilities. Our results highlight the importance of releasing model probabilities, since linguistic confi-\ndences alone are generally not expressive enough to provide high quality confidence estimates. We\ndemonstrate that probabilities from weaker white-box, surrogate models can effectively estimate the\ninternal confidences of stronger black-box models like GPT-4, outperforming linguistic confidences,\nand provide some intuitions for why confidences can transfer between models. We hope that our\nfindings can inspire future work on understanding the transferability of model probabilities and rep-\nresentations and on leveraging this transferability to use white-box models to understand black-box\nmodels. Interestingly, we also find that confidence signals from different models are complementary\nand can be composed for even more reliable confidence estimation. Future methods could further\nbuild on this result to develop more sophisticated methods of confidence signal composition.\n10\nPreprint\nREFERENCES\nPeter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.\nJournal of Machine Learning Research (JMLR), 9(0):1823\u20131840, 2008.\nChao K Chow. On optimum recognition error and reject tradeoff. IEEE Transactions on Information\nTheory, 16(1):41\u201346, 1970.\nLuigi Pietro Cordella, Claudio De Stefano, Francesco Tortorella, and Mario Vento. A method for\nimproving classification reliability of multilayer perceptrons. IEEE Transactions on Neural Net-\nworks, 6(5):1140\u20131147, 1995.\nMorris H. DeGroot and Stephen E. Fienberg. The comparison and evaluation of forecasters. Journal\nof the Royal Statistical Society. Series D (The Statistician), 32:12\u201322, 1983.\nRan El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of\nMachine Learning Research (JMLR), 11, 2010.\nJean Feng, Arjun Sondhi, Jessica Perry, and Noah Simon. Selective prediction-set models with\ncoverage guarantees. arXiv preprint arXiv:1906.05473, 2019.\nYonatan Geifman and Ran El-Yaniv. Selective classification for deep neural networks. In Advances\nin Neural Information Processing Systems (NeurIPS), 2017.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural\nnetworks. In International Conference on Machine Learning (ICML), pp. 1321\u20131330, 2017.\nMartin Hellman and Josef Raviv. Probability of error, equivocation, and the chernoff bound. IEEE\nTransactions on Information Theory, 16(4):368\u2013372, 1970.\nDan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution\nexamples in neural networks. In International Conference on Learning Representations (ICLR),\n2017.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference\non Learning Representations (ICLR), 2021.\nDi Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What dis-\nease does this patient have? a large-scale open domain question answering dataset from medical\nexams. In arXiv preprint arXiv:2009.13081, 2021.\nErik Jones, Shiori Sagawa, Pang Wei Koh, Ananya Kumar, and Percy Liang. Selective classification\ncan magnify disparities across groups. In International Conference on Learning Representations\n(ICLR), 2021.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez,\nNicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, Scott Johnston, Sheer\nEl-Showk, Andy Jones, Nelson Elhage, Tristan Hume, Anna Chen, Yuntao Bai, Sam Bow-\nman, Stanislav Fort, Deep Ganguli, Danny Hernandez, Josh Jacobson, Jackson Kernion, Shauna\nKravec, Liane Lovitt, Kamal Ndousse, Catherine Olsson, Sam Ringer, Dario Amodei, Tom\nBrown, Jack Clark, Nicholas Joseph, Ben Mann, Sam McCandlish, Chris Olah, and Jared Ka-\nplan. Language models (mostly) know what they know, 2022.\nAmita Kamath, Robin Jia, and Percy Liang. Selective question answering under domain shift. In\nAssociation for Computational Linguistics (ACL), 2020.\nFereshte Khani, Martin Rinard, and Percy Liang. Unanimous prediction for 100% precision with\napplication to learning semantic mappings. In Association for Computational Linguistics (ACL),\n2016.\nVolodymyr Kuleshov and Percy Liang. Calibrated structured prediction. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2015.\n11\nPreprint\nKimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting\nout-of-distribution samples and adversarial attacks. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2018.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,\nYian Zhang, D. Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,\nBobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R\u00e9, Diana\nAcosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu\nRen, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksek-\ngonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, O. Khattab, Peter Hen-\nderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, S. Ganguli, Tatsunori\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yi-\nfan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. arXiv preprint\narXiv:2211.09110, 2022.\nShiyu Liang, Yixuan Li, and R. Srikant.\nEnhancing the reliability of out-of-distribution image\ndetection in neural networks. In International Conference on Learning Representations (ICLR),\n2018.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958, 2021.\nStephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in\nwords. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https:\n//openreview.net/forum?id=8s8K2UZGTZ.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct\nelectricity? a new dataset for open book question answering. In Empirical Methods in Natural\nLanguage Processing (EMNLP), 2018.\nHussein Mozannar and David Sontag. Consistent estimators for learning to defer to an expert. arXiv\npreprint arXiv:2006.01862, 2020.\nAllan H. Murphy and Robert L. Winkler. Reliability of subjective probability forecasts of precipi-\ntation and temperature. Journal of the Royal Statistical Society. Series C (Applied Statistics), 26:\n41\u201347, 1977.\nMahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Binary classifier calibration:\nNon-parametric approach. arXiv, 2014.\nOpenAI. Introducing ChatGPT. https://openai.com/blog/chatgpt, 2022.\nOpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023a.\nOpenAI. Text-davinci-003 api reference. https://platform.openai.com/docs/models/gpt-3-5, 2023b.\nYaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua V. Dil-\nlon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model\u2019s uncertainty? eval-\nuating predictive uncertainty under dataset shift. In Advances in Neural Information Processing\nSystems (NeurIPS), 2019.\nMarco AF Pimentel, David A Clifton, Lei Clifton, and Lionel Tarassenko. A review of novelty\ndetection. Signal Processing, 99:215\u2013249, 2014.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. Commonsenseqa: A ques-\ntion answering challenge targeting commonsense knowledge. In North American Association for\nComputational Linguistics (NAACL), 2019.\nKatherine Tian, Eric Mitchell, Allan Zhou, Archit Sharma, Rafael Rafailov, Huaxiu Yao, Chelsea\nFinn, and Christopher D. Manning. Just ask for calibration: Strategies for eliciting calibrated\nconfidence scores from language models fine-tuned with human feedback, 2023.\n12\nPreprint\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.\narXiv, 2023.\nMiao Xiong, Zhiyuan Hu, Xinyang Lu, Yifei Li, Jie Fu, Junxian He, and Bryan Hooi. Can llms\nexpress their uncertainty? an empirical evaluation of confidence elicitation in llms. arXiv preprint\narXiv:2306.13063, 2023. URL https://arxiv.org/pdf/2306.13063.pdf.\nKaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto. Navigating the grey area: Expressions of\noverconfidence and uncertainty in language models, 2023.\n13\nPreprint\nA\nAPPENDIX\nA.1\nDATASET DETAILS\nTruthfulQA is a multiple choice benchmark designed to check the truthfulness of large language\nmodels by testing them on questions across 38 different categories like health and politics where\nhumans might provide incorrect responses due to implicit biases or incorrect beliefs. This task is\nchallenging for language models because they may imbibe these same misconceptions from training\ncorpora.\nMedQA is a challenging dataset testing medical knowledge with questions based on the\nUnited States Medical License Exams (USMLE) and other medical board exams.\nCommonsenseQA is a multiple choice benchmark testing commonsense reasoning, with challeng-\ning associations extracted from ConceptNet to find many target concepts for a single source concept.\nOpenbookQA is a multiple choice dataset requiring multi-step reasoning over common and\ncommonsense knowledge requiring deeper understanding of a diverse set of topics.\nMMLU is a massive benchmark covering 57 subjects from a diverse set of areas including\nSTEM, humanties, and social sciences. This benchmark tests both more rudimentary and more\nadvanced sets of knowledge for these topics covering great breadth and depth.\nOur method requires no training or adaptation of the models used. We evaluate on 250 examples\nfrom each dataset or on the maximum size of the dataset\u2019s test subset (if test subset is smaller).\nA.2\nMODEL ACCURACIES\nFollowing are the accuracies for each of the models on the 12 datasets. Answers are elicited using\nthe prompt format specified in A.4. As expected the GPT-4 model has the highest accuracies on\nall 12 datasets, followed by Claude-v1.3. Llama 2 Chat and Base have comparable accuracy to\nGPT-3.5. Text-davinci-003 has the lowest accuracies.\nModel\nTQA\nMedQA\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nText-davinci\n0.472\n0.504\n0.712\n0.772\n0.532\n0.590\n0.579\nLlama-2\n0.524\n0.564\n0.664\n0.808\n0.572\n0.590\n0.583\nLlama-2 Chat\n0.480\n0.512\n0.684\n0.728\n0.528\n0.600\n0.528\nGPT-3.5\n0.572\n0.536\n0.676\n0.776\n0.504\n0.670\n0.523\nClaude-v1.3\n0.596\n0.640\n0.736\n0.832\n0.604\n0.760\n0.638\nGPT-4\n0.836\n0.736\n0.768\n0.940\n0.664\n0.760\n0.813\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nText-davinci\n0.412\n0.300\n0.440\n0.690\n0.850\n0.571\nLlama-2\n0.386\n0.240\n0.460\n0.750\n0.910\n0.588\nLlama-2\n0.333\n0.310\n0.420\n0.670\n0.850\n0.554\nGPT-3.5\n0.404\n0.320\n0.460\n0.730\n0.800\n0.581\nClaude-v1.3\n0.579\n0.330\n0.500\n0.760\n0.880\n0.655\nGPT-4\n0.596\n0.480\n0.520\n0.800\n0.910\n0.735\nTable 4: Model Accuracies Accuracies of all 6 models on all 12 tasks. GPT-4 is the highest per-\nforming model for all tasks.\nA.3\nAUC AND AUROC DEFINITIONS\nAUC with Randomized or Deterministic Classifiers. To plot the accuracy-coverage curve we\ncompute A(c), the selective accuracy at coverage c across different values of c. A(c) is the accuracy\nif the model only makes a prediction on the c proportion of data with highest confidence scores.\nfor different values of c. When making a prediction on c proportion of data, for each example x\nwe use a binary classifier on the confidence score C(x) decide if we are making a prediction for\nx (1 if making a prediction and 0 if abstaining from prediction). Such a classifier can either be\ndeterministic or randomized.\n14\nPreprint\nDeterministic Classifiers.\nA deterministic classifier f returns identical outputs for identical\ninputs \u2014 resulting in consistent treatment of examples with the same confidence score (either\npredict on all or abstain on all). Using a deterministic classifier to select c portion of examples to\npredict on means we find the highest confidence threshold t such that P(C(x) \u2265 t) \u2265 c \u2014 t is\nthe highest confidence threshold where the proportion of examples with confidence greater than or\nequal to t is greater than or equal to the required coverage c. With a deterministic classifier, we\npredict on P(C(x) \u2265 t) proportion of examples, which may be greater than the required coverage\nc.\nf(C(x)) \u2208 {0, 1}\n(A.1)\nRandomized Classifiers. A randomized classifier h can return different outputs for the same input.\nSince models can output the same linguistic confidences for multiple examples, a randomized clas-\nsifier can allow us to achieve exactly a coverage of c by making predictions on some examples with a\ngiven confidence, and abstaining on other examples with the same confidence. To enable h to break\nties and make different predictions for examples with the same confidence score, we add a small\namount of Gaussian noise to each confidence score N(0, \u03f5), \u03f5 \u2192 0 to enforce a confidence-based\nordering of examples.\nh(C(x) + N(0, \u03f5)) \u2208 {0, 1}\n(A.2)\nDeterministic vs Randomized AUC Example.\nSuppose a model assigns half of the examples a\nconfidence of 1 and gets them all right, and the other half of examples a confidence of 0.5 and gets\n50% of them right. What is the selective accuracy at coverage 75%? A deterministic classifier\nwould select 0.5 as t and predict on all examples with C(x) \u2265 t, which in this case is all of the\nexamples (notably leading to a coverage of 100% instead of 75%). This would lead to an accuracy\nof 75%. A randomized classifier would predict on all examples of confidence 1, but to meet the\n75% coverage threshold, it would predict on half of the examples which have confidence 0.5 \u2014\nselecting the top half after adding random noise. This would lead to an accuracy of approximately\n83%.\nThe above example demonstrates that we may underestimate the AUC value by using a de-\nterministic classifier - since it forces a prediction on a higher coverage threshold. For example, for\nGPT-4 using a deterministic classifier leads to 78.8% AUC averaged across 12 datasets, while using\na randomized classifier leads to 80.5% AUC. Since it generally leads to higher selective accuracy\nvalues and allows predicting on exactly the top c proportion of examples based on confidence, we\nuse a randomized classifier in our AUC calculations.\nAUROC Definition.\nFollowing is how we compute the AUROC metric.\nLet R(x, y) = 1 if\nan answer y is correct for input x, and 0 otherwise. C(x) \u2208 [0, 1] is the confidence score for\nexample x.:\n1. Let the true positive rate at threshold t be the fraction of correct examples with confidence\nat least t: TPR(t) = E[R(x, y(x))I(C(x) \u2265 t)]/E[R(x, y(x))]\n2. Let the false positive rate at threshold t be the fraction of incorrect examples with\nconfidence at least t: FPR(t) = E[(1 \u2212 R(x, y(x)))I(C(x) \u2265 t)]/E[1 \u2212 R(x, y(x))]\n3. Plot TPR(t) (y-axis) against FPR(t) (x-axis) and take area under the curve. The AUROC\nchecks how well thresholding on the confidence can predict if an example is correct or\nincorrect.\nA random classifier (which assigns random confidences to each example) will have the same TPR\nand FPR. A model with good confidences will assign higher confidences to correct examples, so a\nlarger fraction of the correct examples (than incorrect examples) will have confidence \u2265 t.\nA.4\nLINGUISTIC CONFIDENCE PROMPTS\nWe zero-shot prompt models to elicit answers and linguistic confidence scores.\nWe focus on\nzero-shot prompting instead of few-shot prompting to assess how good models are at linguistically\nunderstanding a notion of confidence, without being primed with examples which could bias their\ngenerated confidence assessments.\n15\nPreprint\nPrompt Categories.\nWe study 24 prompt format variations to elicit linguistic confidences,\nsince there can be many ways for a model to describe its confidence levels and we want to ensure a\nfair assessment of linguistically generated confidence estimates that is not overly conditioned on a\nspecific prompt. These fall into the following categories \u2014 prompts eliciting:\n1. Numerical confidences (score from between varying ranges 0-1, 0-10, 0-100), probabilities\nfrom 0-100%\n2. Linguistic categorization of confidences into varying numbers of confidence buckets (\u2018not\nsure\u2019, \u2018sure\u2019, \u2018very sure\u2019, among other phrases and degrees of certainty)\n3. Zero-shot chain-of-thought explanations of confidence in addition to confidence scores\n4. Prompting first for an answer and then re-prompting for the confidence in the answer\n5. Varying the confidence instructions (with other phrases describing confidence including\n\u2018uncertainty\u2019, \u2018certainty\u2019, \u2018correctness\u2019, \u2018probability or likelihood of correctness\u2019)\nBest Prompt Selection. We measure the AUC performance of each prompt across the 12 datasets\nfor all models. The optimal prompt varies slightly for each model, so for the best prompt we select\nthe prompt which reduces the sum of drops in AUC per model from the optimal prompt for each\nindividual model (the prompt which is the closest in optimal AUC performance for all models on all\ndatasets). This prompt is used for all of our experiments.\nBest Prompt Description. The prompt elicits a confidence score from 0-1 with the given instruc-\ntion. It also includes \u2018fake few-shot examples\u2019 as described which show fake questions, answers,\nand confidence scores to allow the model to learn the format of the task without providing any other\ntask-specific information. Since fake-fewshot examples do including answer choices (D, A in the\nexample) and specific confidence scores (0.4, 0.7), we experimented with variations of the prompt\nmodifying the exact answers and confidences included in the example and found the change in gen-\nerated confidences to be minimal \u2013 indicating that models were able to use these examples to learn\nthe format of the task without over-conditioning on the specific answers and confidences provided.:\nFigure 6: Best Linguistic Confidence Prompt\nExamples of Other Prompts. Following are a few of the several alternative prompts we tried:\n16\nPreprint\nFigure 7: Linguistic Categorization Prompt. Elicits confidences in different categories of\nlinguistic certainty.\nFigure 8: Probabilistic Confidence Prompt. Frames confidence as a probability between 0-100%\n- in case models have seen more instances of probabilistic confidences in their training data.\nFigure 9: Zero-shot Chain-of-Thought. Allows the model to provide a chain of reasoning before\ndeciding on its confidence score. Initial analysis indicated that these reasoning chains served to\nfurther reinforce the model\u2019s overconfidence by often incorrectly justifying its answers.\nA.5\nLLAMA 2 70B CHAT RESULTS\nWe also experiment with the chat version of the Llama 2 70B model, evaluating AUC and AU-\nROC for linguistic confidences and model probabilities. We find that the chat version of the model\ngenerally performs similarly to the base version , so primary results are reported on the base model.\nSimilar to Llama 2 Base, Llama 2 Chat\u2019s probabilities outperform its linguistic confidences based on\nAUC and AUROC on all 12 datasets. Between the chat and base versions, the base version generally\noutperforms chat for both linguistic confidences and model probabilities.\n17\nPreprint\nMetric\nConfidence\nTQA\nMedqa\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nAUC\nLinguistic\n0.631\n0.521\n0.679\n0.750\n0.529\n0.675\n0.556\nProb\n0.699\n0.604\n0.840\n0.869\n0.674\n0.823\n0.721\nAUROC\nLinguistic\n0.683\n0.517\n0.506\n0.535\n0.501\n0.562\n0.568\nProb\n0.754\n0.609\n0.764\n0.776\n0.710\n0.821\n0.721\nMetric\nConfidence\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nAUC\nLinguistic\n0.367\n0.296\n0.445\n0.718\n0.846\n0.584\nProb\n0.438\n0.348\n0.632\n0.850\n0.963\n0.705\nAUROC\nLinguistic\n0.553\n0.485\n0.546\n0.560\n0.479\n0.541\nProb\n0.634\n0.495\n0.721\n0.811\n0.858\n0.723\nTable 5: AUC and AUROC Metrics for Llama 2 70B Chat. Llama 2 Base\u2019s linguistic confidence\nscores outperform Llama 2 Chat\u2019s linguistic confidences \u2014 73.1% AUC compared to 70.5%. Sim-\nilarly, Llama 2 Base\u2019s model probabilities also outperform Llama 2 Chat\u2019s probabilities \u2014 73.1%\nAUC compared to 70.5%. These results may support the conclusion that base models are better\ncalibrated than chat models.\nA.6\nECE RESULTS\nFollowing are the ECEs for the linguistic confidence scores of each model and the ECEs of model\nprobabilities for models which provide them.\nConfidence Type\nTQA\nMedqa\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nText-davinci Linguistic\n0.422\n0.425\n0.161\n0.127\n0.380\n0.300\n0.299\nText-davinci Prob\n0.461\n0.454\n0.235\n0.191\n0.388\n0.338\n0.338\nLlama 2 Linguistic\n0.365\n0.248\n0.201\n0.073\n0.224\n0.259\n0.267\nLlama 2 Prob\n0.099\n0.084\n0.176\n0.235\n0.115\n0.145\n0.094\nLlama 2 Chat Linguistic\n0.357\n0.391\n0.125\n0.101\n0.350\n0.194\n0.337\nLlama 2 Chat Prob\n0.284\n0.228\n0.124\n0.092\n0.264\n0.213\n0.210\nGPT-3.5 Linguistic\n0.350\n0.380\n0.192\n0.091\n0.388\n0.176\n0.363\nClaude-v1.3 Linguistic\n0.187\n0.086\n0.042\n0.033\n0.098\n0.052\n0.162\nGPT-4 Linguistic\n0.104\n0.118\n0.118\n0.038\n0.187\n0.114\n0.109\nConfidence Type\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nText-davinci Linguistic\n0.482\n0.625\n0.475\n0.213\n0.038\n0.329\nText-davinci Prob\n0.478\n0.576\n0.385\n0.263\n0.112\n0.352\nLlama 2 Linguistic\n0.453\n0.561\n0.435\n0.079\n0.093\n0.272\nLlama 2 Prob\n0.205\n0.091\n0.100\n0.172\n0.264\n0.148\nLlama 2 Chat Linguistic\n0.505\n0.480\n0.480\n0.165\n0.055\n0.295\nLlama 2 Chat Prob\n0.403\n0.361\n0.272\n0.187\n0.073\n0.226\nGPT-3.5 Linguistic\n0.515\n0.560\n0.432\n0.173\n0.094\n0.309\nClaude-v1.3 Linguistic\n0.132\n0.319\n0.175\n0.058\n0.162\n0.126\nGPT-4 Linguistic\n0.270\n0.420\n0.313\n0.118\n0.053\n0.164\nTable 6: ECE Values Linguistic Confidences vs Model Probabilities The ECE values for lin-\nguistic confidences and model probabilities show that on some datasets model probabilities achieve\nbetter ECE values, while on other datasets linguistic confidences achieve better ECE values. Among\nthe state-of-the-art models, Claude-v1.3\u2019s linguistic confidences notably result in the least expected\ncalibration error on 9/12 datasets.\nMetric. We compute the expected calibration error metric (ECE) by dynamically binning examples\nbased on their confidence scores into 10 bins with approximately equal numbers of examples in\neach bin. For each bin, we compute the calibration error, which is the absolute difference between\n18\nPreprint\nthe mean predicted confidence and the mean observed accuracy. This quantifies how well the pre-\ndicted confidences match the true probability of correctness within each bin. We then calculate the\nweighted average of the calibration errors across all bins, where the weights are the proportion of\nexamples in each bin relative to the total number of examples.\nWe also compute the ECE values for our baseline confidence methods (linguistic confidences, SC\nlinguistic confidences) and for our proposed confidence methods (surrogate, tiebreak, mixture, and\nSC mixture) for the GPT-4 model. For 11 out of 12 tasks, we find that our proposed methods lead\nto the lowest ECE values.\nTQA\nMedQA\nCSQA\nOBQA\nLaw\nEthics\nPhysics\nLing. Conf.\n0.104\n0.118\n0.118\n0.038\n0.187\n0.114\n0.109\nSC Ling. Conf.\n0.126\n0.163\n0.120\n0.036\n0.246\n0.204\n0.120\nSurrogate\u2020\n0.395\n0.212\n0.297\n0.370\n0.156\n0.205\n0.317\nTiebreak\u2020\n0.114\n0.134\n0.126\n0.032\n0.194\n0.114\n0.118\nMixture\u2020\n0.096\n0.075\n0.061\n0.159\n0.064\n0.111\n0.088\nSC Mixture\u2020\n0.085\n0.120\n0.108\n0.029\n0.216\n0.186\n0.098\nConfidence Type\nEcon\nAlgebra\nChem\nSecurity\nPolicy\nAvg\nLing. Conf.\n0.270\n0.420\n0.313\n0.118\n0.053\n0.164\nSC Ling. Conf.\n0.323\n0.379\n0.331\n0.136\n0.063\n0.187\nSurrogate\u2020\n0.129\n0.162\n0.187\n0.210\n0.264\n0.242\nTiebreak\u2020\n0.270\n0.419\n0.332\n0.158\n0.068\n0.173\nMixture\u2020\n0.126\n0.224\n0.229\n0.108\n0.138\n0.123\nSC Mixture\u2020\n0.287\n0.358\n0.286\n0.129\n0.068\n0.164\nTable 7: ECE Values All Confidence Methods for GPT-4\nA.7\nSURROGATE MODEL RESULTS\nFigure 10: AUCs For All Surrogate Models We compute the AUC metric for each model consid-\nering surrogate confidences from both model probabilities and linguistic confidence scores from all\nother models. We find that all models benefit from using surrogate model probabilities over their\nown linguistic confidences.\n19\nPreprint\nFigure 11: AUROCs For All Surrogate Models We also compute the AUROC metric for each\nmodel considering surrogate confidences from both model probabilities and linguistic confidence\nscores from all other models. In general, we find that using surrogate model probabilities instead of\na model\u2019s own linguistic confidences improves AUROC values.\nAUCs and AUROCs for surrogate models show that model probabilities from other models can\nprovide better confidence estimates than a models own linguistic confidences.\nA.8\nCORRELATION AND COVARIANCE OF SURROGATE AND MAIN MODELS\nFigure 12: Correlations For All Main and Surrogate Models\n20\nPreprint\nFigure 13: Covariances For All Main and Surrogate Models\nWe compute correlations and covariances between the answer correctness (set of binary scores in-\ndicating if a model answered a question correctly or not) for every pair of main model and potential\nsurrogate model. We find that in general if a surrogate model S has a high degree of correlation\nin answer correctness with a main model M, then S is likely to be a good surrogate for M. For\nexample, GPT-4 has a higher correlation with Llama 2 Base, Llama 2 Chat, and text-davinci-003\nthan it does with Llama 2 13B indicating that those models can be better surrogates for GPT-4 than\nLlama 2 13B. Similar trends also hold for covariances.\nA.9\nCALIBRATION OF MIXTURE OF MODELS\nFigure 14: Calibration of GPT-4 with Linguistic Confidence and Mixture of Models In the first\nrow we see the calibration of GPT-4 on MedQA, Professional Law, and US Foreign Policy when\nusing linguistic confidences. In the second row, we see GPT-4\u2019s calibration using our Mixture of\nModels confidence method. A perfectly calibrated model would have all bars aligned with the red\nline (average confidence in each bucket is exactly equal to the average accuracy). We can see that\ncalibration improves demonstrably, when using Mixture of Models.\n21\n"
  },
  {
    "title": "Thread of Thought Unraveling Chaotic Contexts",
    "link": "https://arxiv.org/pdf/2311.08734.pdf",
    "upvote": "4",
    "text": "Thread of Thought Unraveling Chaotic Contexts\nYucheng Zhou1\u2217, Xiubo Geng2, Tao Shen3, Chongyang Tao2,\nGuodong Long3, Jian-Guang Lou2\u2020, Jianbing Shen1\u2020\n1 SKL-IOTSC, CIS, University of Macau,\n2Microsoft Corporation, 3AAII, FEIT, University of Technology Sydney\nyucheng.zhou@connect.um.edu.mo, {xigeng,chongyang.tao,jlou}@microsoft.com\n{tao.shen, guodong.long}@uts.edu.au, jianbingshen@um.edu.mo\nAbstract\nLarge Language Models (LLMs) have ushered\nin a transformative era in the field of natu-\nral language processing, excelling in tasks re-\nlated to text comprehension and generation.\nNevertheless, they encounter difficulties when\nconfronted with chaotic contexts (e.g., distrac-\ntors rather than long irrelevant context), lead-\ning to the inadvertent omission of certain de-\ntails within the chaotic context. In response to\nthese challenges, we introduce the \u201cThread of\nThought\u201d (ThoT) strategy, which draws inspi-\nration from human cognitive processes. ThoT\nsystematically segments and analyzes extended\ncontexts while adeptly selecting pertinent in-\nformation. This strategy serves as a versatile\n\u201cplug-and-play\u201d module, seamlessly integrating\nwith various LLMs and prompting techniques.\nIn the experiments, we utilize the PopQA and\nEntityQ datasets, as well as a Multi-Turn Con-\nversation Response dataset (MTCR) we col-\nlected, to illustrate that ThoT significantly im-\nproves reasoning performance compared to\nother prompting techniques.\n1\nIntroduction\nLarge Language Models (LLMs) represent a sig-\nnificant advancement in the field of artificial in-\ntelligence. They have achieved notable accom-\nplishments in natural language understanding and\ngeneration (Brown et al., 2020; Wei et al., 2022).\nThe development of LLMs has had a far-reaching\nimpact, drawing significant attention in academia.\nThese models demonstrate proficiency in a wide ar-\nray of natural language processing tasks, including\nsentiment analysis (Zhang et al., 2023), machine\ntranslation (Moslem et al., 2023), and summariza-\ntion (Tam et al., 2023). Moreover, they exert a\nprofound influence across various industries and\noffer promising solutions for intricate issues, such\n\u2217Work is done during internship at Microsoft.\n\u2020Corresponding author.\nas aiding in legal consultations (Yue et al., 2023)\nand assisting in medical diagnostics (Wang et al.,\n2023a).\nWith the growing complexity and diversity of\ntasks demanding extensive information process-\ning and reasoning, particularly in the context of\nRetrieval-Augmented Generation (RAG) (Lewis\net al., 2020) and conversational (Xu et al., 2022)\nscenarios, the input text often comprises a wealth\nof information from various sources, including user\nqueries, conversation history, external knowledge\nbases, and more. This information may be inter-\nconnected or entirely unrelated. Moreover, the sig-\nnificance of this information can fluctuate based\non the context, with certain pieces being critical\nfor addressing specific questions and others being\nextraneous. This situation can aptly be character-\nized as a \u201cChaotic Context\u201d. Similar to but distinct\nfrom \u201cLong Context\u201d, \u201cChaotic Context\u201d under-\nscores the complexity and volume of information,\ngoing beyond the mere length of the context. More-\nover, Liu et al. (2023) found that existing LLMs of-\nten encounter difficulties in effectively identifying\nrelevant information from the context augmented\nthrough retrieval, particularly when it is located in\nthe middle position.\nRecent studies (Xu et al., 2023; Jiang et al.,\n2023) have proposed various solutions to enhance\nthe performance of LLMs in long-context sce-\nnarios, upon an intuition of input capacity opti-\nmization. Xu et al. (2023) proposes a method to\ncompare and combine retrieval-augmentation and\nlong context extension for large language mod-\nels (LLMs) on various long text tasks.\nHow-\never, this approach necessitates the retraining of\nLLMs with positional interpolation. In addition,\nJiang et al. (2023) introduces LongLLMLingua, a\nmethod that streamlines the input prompt by culling\nirrelevant or redundant information. Nonetheless,\nthis method mandates the fine-tuning of auxiliary\nmodels (e.g., LLaMA-7B (Touvron et al., 2023a))\narXiv:2311.08734v1  [cs.CL]  15 Nov 2023\nWalk me through this context in manageable \nparts step by step, summarizing and \nanalyzing as we go.\nLet's think step by step.\nChain of Thought\nAs a content reviewer, I provide multiple retrieved passages about \nthis question; you need to answer the question.\nretrieved Passage 1 is: to introduce book vending machines, \u2026\nretrieved Passage 2 is: of the Carlsbad Decrees, it quickly \u2026\nretrieved Passage 3 is: It is at \"Kreuzstrae 12\", opposite \u2026\nretrieved Passage 4 is: house in Leipzig remained there, \u2026\nretrieved Passage 5 is: Delcam Delcam is a supplier of \u2026\nretrieved Passage 6 is: inhabitants are called recreinos. \u2026\nretrieved Passage 7 is: 'politically unreliable' writers like \u2026\nretrieved Passage 8 is: Recheio Recheio is the biggest \u2026\nretrieved Passage 9 is: via the Internet, large volumes of \u2026\nretrieved Passage 10 is: without needing to pay any royalties, \u2026\nQ: Where was Reclam founded?\n{Prompting Method}\nA:\nInput\nThread of Thought\nPrompting Method\nReclam was founded by Anton Philipp Reclam. \nAccording to retrieved Passage 2, he founded his own \npublishing house called \"Verlag des literarischen \nMuseums\" in Carlsbad, which quickly became a center of \nintellectual and literary circles.\nAccording to retrieved Passage 2, Anton Philipp Reclam \nfounded his own publishing \u2026\nIn retrieved Passage 4, it is mentioned that the Reclam \npublishing house in Leipzig \u2026\nFinally, in retrieved Passage 7, it is stated that after the \npartition of Germany \u2026\nTherefore, Reclam was originally founded in Leipzig, \nGermany, and later moved its main office to Stuttgart.\nOutput\nOutput\nFigure 1: Thread of Thought prompting enables large language models to tackle chaotic context problems. In the\noutput depicted, green text denotes the correct answer, while red text indicates the erroneous prediction.\nfor prompt compression. The utility of these aux-\niliary models may prove insufficient in address-\ning unknown or intricate content, and it imposes\nlimitations on the length of text that can be ef-\nfectively processed. Moreover, its non-end-to-end\nframework can lead to error propagation. In con-\ntrast, chain-of-thought (CoT) prompting (Wei et al.,\n2022) can enhance a model\u2019s reasoning ability with-\nout requiring any retraining or fine-tuning of LLMs.\nHowever, due to the massive amount of informa-\ntion contained within chaotic contexts, CoT still\nencounters information missing in reasoning, as\nshown in Figure 1.\nTo address these challenges, we introduce the\n\u201cThread of Thought\u201d (ThoT) strategy. ThoT, draw-\ning inspiration from human cognitive processes, en-\nables Large Language Models (LLMs) to methodi-\ncally segment and analyze extended contexts. This\nsegmentation enhances the extraction of pertinent\ncontent for responding to queries. ThoT represents\nthe unbroken continuity of ideas that individuals\nmaintain while sifting through vast information,\nallowing for the selective extraction of relevant de-\ntails and the dismissal of extraneous ones. This\nbalance of attention across a document\u2019s sections\nis crucial for accurately interpreting and respond-\ning to the information presented. Moreover, the\nstepwise analysis and summarization of segmented\ninformation improve comprehension over multiple\nparagraphs and protect LLMs against misleading\nyet seemingly relevant data.\nIn comparison to existing methods that require\ncomplex multi-stage prompting (Zhou et al., 2023)\nor multi-path sampling (Wang et al., 2023b), ThoT\nis a simpler, more universal, and efficient solution.\nIt integrates seamlessly as a \u201cplug-and-play\u201d mod-\nule with various pre-trained language models and\nprompting strategies, avoiding complex procedures.\nThoT not only improves LLMs\u2019 performance in\nchaotic contexts but also enhances their reasoning\nabilities.\nTo evaluate ThoT\u2019s effectiveness in handling\nchaotic contextual information, we used long-tail\nquestion answering datasets, specifically PopQA\n(Mallen et al., 2023) and EntityQ (Sciavolino et al.,\n2021). These datasets feature knowledge often un-\nfamiliar to large models, thereby reducing the im-\npact of their inherent knowledge retention on our re-\nsults. Additionally, we construct a Multi-Turn Con-\nversation Response (MTCR) dataset based on ev-\neryday conversations to further assess our method.\nComparative analyses with other prompting tech-\nniques show that ThoT markedly improves reason-\ning performance, evidencing its effectiveness. We\nalso explored various prompts to determine optimal\nprompting strategies.\n2\nRelated Work\n2.1\nLong Context Large Language Models\nRecent advancements in Large Language Models\n(LLMs) have made significant strides in managing\nextended contexts, moving beyond the limitations\nof traditional pre-defined context windows. Ratner\net al. (2023) introduce the Parallel Context Win-\ndows (PCW) method, which segments extensive\ncontexts into multiple windows, employing inde-\npendent attention mechanisms. Building on this\nconcept, Chen et al. (2023) facilitate substantially\nlonger context windows with minimal fine-tuning\nby aligning position indices with the maximum po-\nsition index from the pre-training phase. Moreover,\na different approach, LongNet, utilizes dilated at-\ntention, allowing the attention field to expand expo-\nnentially with distance (Ding et al., 2023). In addi-\ntion, Xiao et al. (2023) underscore the phenomenon\nof attention convergence, where maintaining the\nKey-Value (KV) states of initial tokens significantly\nenhances window attention performance. Lastly,\nPress et al. (2022) introduce Attention with Linear\nBiases (ALiBi), a method that biases the query-\nkey attention scores based on distance, achieving\ncomparable perplexity to models trained on longer\nsequences. However, these methods predominantly\nconcentrate on long contexts. In contrast, chaotic\ncontexts are characterized by their overloaded in-\nformation, often cluttered with numerous similar\nand unrelated elements.\n2.2\nReasoning with Large Language Models\nAdvancements in large language models (LLMs)\nhave significantly impacted AI, notably in complex\nreasoning tasks. The enhancement of LLMs\u2019 rea-\nsoning capabilities is exemplified in (Wei et al.,\n2022), where Chain-of-Thought (CoT) prompting\nis introduced. This method improves arithmetic,\ncommon sense, and symbolic reasoning by gen-\nerating intermediate steps. Building on this, the\nGraph of Thoughts (GoT) framework conceptual-\nizes LLM outputs as graphs, leading to notable\nimprovements in task performance and efficiency\n(Besta et al., 2023). Extending the CoT concept,\nYao et al. (2023a) propose the Tree of Thoughts\n(ToT) framework, which has shown remarkable\nsuccess in complex problem-solving tasks like the\n24-point game. In addition, Zhou et al. (2023) in-\ntroduce the least-to-most prompting strategy, break-\ning down complex problems into simpler sub-\nproblems and showing effectiveness in tasks re-\nquiring advanced symbolic manipulation. Lastly,\nYao et al. (2023b) explore non-linear thought pro-\ncesses through GoT reasoning, outperforming the\nlinear CoT approach in both mathematical and fi-\nnancial problem datasets. However, these methods\nare effective but overlook chaotic context scenar-\nios.\n2.3\nKnowledge Following in Long Context\nLLMs can process extensive input contexts, but\ntheir performance significantly deteriorates when\nextracting relevant information buried in these con-\ntexts, challenging their efficiency in managing long\ncontexts (Liu et al., 2023). To address deploy-\ning LLMs in streaming applications, Xiao et al.\n(2023) introduce the StreamingLLM framework,\nenabling LLMs with limited attention windows to\nhandle indefinitely long sequences without addi-\ntional fine-tuning. Some study finds that retrieval\naugmentation enables a 4K context window LLM\nReclam was originally founded \nin Leipzig\nAs a content reviewer, I provide multiple retrieved passages about \nthis question; you need to answer the question.\nretrieved Passage 1 is: to introduce book vending machines, \u2026\nretrieved Passage 2 is: of the Carlsbad Decrees, it quickly \u2026\nretrieved Passage 3 is: It is at \"Kreuzstrae 12\", opposite \u2026\nretrieved Passage 4 is: house in Leipzig remained there, \u2026\nretrieved Passage 5 is: Delcam Delcam is a supplier of \u2026\nretrieved Passage 6 is: inhabitants are called recreinos. \u2026\nretrieved Passage 7 is: 'politically unreliable' writers like \u2026\nretrieved Passage 8 is: Recheio Recheio is the biggest \u2026\nretrieved Passage 9 is: via the Internet, large volumes of \u2026\nretrieved Passage 10 is: without needing to pay any royalties, \u2026\nQ: Where was Reclam founded?\nWalk me through this context in manageable parts step by step, \nsummarizing and analyzing as we go.\nA:\nInput\nAccording to retrieved Passage 2, Anton Philipp Reclam founded \nhis own publishing \u2026\nIn retrieved Passage 4, it is mentioned that the Reclam publishing \nhouse in Leipzig \u2026\nFinally, in retrieved Passage 7, it is stated that after the partition of \nGermany \u2026\nTherefore, Reclam was originally founded in Leipzig, Germany, \nand later moved its main office to Stuttgart.\nFirst Output\nTherefore, the answer:\nSecond Output\nFigure 2: Thread of Thought for zero-shot reasoning.\nto equal the performance of a 16K context win-\ndow LLM fine-tuned with positional interpolation\nin long-context tasks, underscoring the potential of\nretrieval methods in augmenting LLM capabilities\n(Xu et al., 2023). Moreover, LongLLMLingua in-\ntroduces prompt compression to improve LLMs\u2019\nkey information perception, significantly boosting\nperformance (Jiang et al., 2023).\n3\nMethodology\nWe present an innovative method for template-\nbased prompting that is specifically designed to\nenhance Thread of Thought (ThoT) reasoning. This\nnovel strategy stands distinct from the traditional\nchain of thought prompting (Wei et al., 2022), adept\nat navigating through disordered contexts in which\nthe information may be either interwoven or dis-\nparate. ThoT prompting can be seamlessly inte-\ngrated with a variety of existing language mod-\nels and prompting techniques, offering a modular\n\u201cplug-and-play\u201d improvement that eliminates the\nneed for elaborate prompting strategies or sampling\nmethods. Our approach\u2019s underlying principle is\nboth simple and efficient, as exemplified in Fig-\nure 2: inserting \u201cWalk me through this context in\nmanageable parts step by step, summarizing and an-\nalyzing as we go\u201d into the prompt facilitates ThoT\nreasoning.\nAs illustrated in Figure 2, in contrast to Chain\nof Thought (CoT) prompting, which struggles with\ncomplex and chaotic contexts, ThoT prompting\nadeptly maintains the logical progression of rea-\nsoning without being overwhelmed. While prompt\ncompressors and similar strategies have sought to\naddress these complexities, they often underper-\nform with unfamiliar or particularly complex mate-\nrial and typically necessitate significant modifica-\ntions to the Large Language Models (LLMs), such\nas retraining or fine-tuning with additional datasets\n(Xu et al., 2023; Jiang et al., 2023). ThoT, however,\nnot only effectively manages chaotic contexts but\nalso simplifies the prompting process, requiring\njust two prompting efforts compared to CoT.\n3.1\nFirst Step: Initiating the Reasoning\nThe initial prompt is designed to guide the LLM\nthrough an analytical dissection of the context, us-\ning the directive \u201cWalk me through this context in\nmanageable parts step by step, summarizing and\nanalyzing as we go\u201d. Specifically, we employ a\ntemplate that incorporates the chaotic context X\nand query Q into the prompt P as \u201c[X] Q: [Q] [T]\nA:\u201d, where [T] denotes the trigger sentence t that\ninitiates the reasoning process. For instance, utiliz-\ning \u201cWalk me through this context in manageable\nparts step by step, summarizing and analyzing as\nwe go\u201d as the trigger, the prompt P becomes \u201c[X]\nQ: [Q] Walk me through this context in manageable\nparts step by step, summarizing and analyzing as\nwe go. A:\u201d. This prompted text P is then inputted\ninto an LLM, which generates the subsequent sen-\ntences Z. This procedure is modeled after the cog-\nnitive strategies humans employ when confronted\nwith complex information, breaking it down into\ndigestible segments, distilling key points, and nav-\nigating through the material with sustained focus.\nThis incremental method fosters a more structured\nand coherent line of reasoning, proving particularly\nadvantageous in chaotic contexts.\n3.2\nSecond Step: Refining the Conclusion\nThe second prompt builds upon the structured\nreasoning established earlier, employing another\nprompt to distill the analysis into a definitive an-\nswer.\nBy leveraging the organized thought se-\nquence initiated by the first prompt, this step aims\nto succinctly capture the essence of the conclusion.\nSpecifically, we use a simple template to combine\nthe initial prompted text P, the response Z, and the\nconclusion marker [A], as in \u201c[P] [Z] [A]\u201d, where\n[A] signifies the trigger sentence designed to extract\nAs a writer, your task is to continue the following dialogue. Start by \ninferring the possible current situation of Speaker 2 based on their persona. \nThen, have Speaker 1 ask a question that indirectly alludes to Speaker 2's \nsituation without directly mentioning the words from their persona. Finally, \nSpeaker 2 should respond, incorporating his persona into the answer.\nThese are the conversations of the two speakers:\n{Conversation}\nThese are the personas of the speaker 2:\n{Speaker2\u2019s personas}\nPlease continue the above conversation, with one persona,  one response \nfrom Speaker 1 and one response from Speaker 2. Think step-by-step, then \nexplain.\nThe final output includes the persona and dialogue between speaker 1 and \nspeaker 2 in the following format:\nPersona: {persona}\nS1: {response1}\nS2: {response2} \nGenerate Responds Based on Conversation and Persona\nExtract Responds From Generated Conversation\nFigure 3: Prompt for MTCR Dataset Construction.\nthe answer, such as \u201cTherefore, the answer:\u201d. This\nextraction prompt perpetuates the thought process,\nprompting the model to sift through the analysis\nand isolate the principal conclusion as the final an-\nswer. The prompt\u2019s design is a deliberate tactic to\nsharpen the model\u2019s focus, fostering precision and\nexplicitness in the response.\nThis two-tiered prompting system effectively ad-\ndresses the limitations of prior methods while ob-\nviating the need for intensive model retraining or\ncomplex modifications. Our methodology not only\nenhances the model\u2019s capacity to navigate chaotic\ncontexts but also more closely aligns its reasoning\nprocesses with human cognitive patterns.\n4\nExperiments\n4.1\nExperimental Settings\nDataset.\nWe evaluated our method across two\nchaotic context scenarios: retrieval-augmented gen-\neration and multi-turn conversation response. Our\nassessment utilized three datasets: the PopQA\ndataset (Mallen et al., 2023), the EntityQ dataset\n(Sciavolino et al., 2021), and our own Multi-Turn\nConversation Response (MTCR) dataset. Specifi-\ncally, the PopQA and EntityQ datasets, designed\nto contain long-tail knowledge, were chosen to\nminimize interference from the extensive internal\nknowledge of large models, thereby facilitating a\nmore effective comparison of different methodolo-\ngies. Distinct from the original PopQA and EntityQ\ndatasets, we randomly selected a test set of 1,000\nsamples for our analysis. For the evaluation of the\nPopQA and EntityQ datasets, we adhered to the\noriginal datasets\u2019 metric, namely the exact match\n(EM). Furthermore, the MTCR dataset, used to\nassess multi-turn conversation response, was de-\nAssessment of the Quality of Generated Speaker2's Response\nConversation Content: \n{conversation}\nGenerated Respond: \n{generated speaker2's response}\nPersona:\n{speaker2's persona}\nComprehensive Evaluation Guide:\nScore each of the following three criteria separately.\nRelevance:\n- 1 point: Not relevant; the response does not relate to Speaker1's dialogue.\n- 2 points: Slightly relevant; the response touches on the subject but misses key \npoints or deviates significantly.\n- 3 points: Somewhat relevant; the response is related to Speaker1's dialogue but \nmay miss some nuances or details.\n- 4 points: Relevant; the response is on topic and addresses most points made by \nSpeaker1.\n- 5 points: Highly relevant; the response is fully on topic, directly addresses all \nelements of Speaker1's dialogue.\nAccuracy:\n- 1 point: Inaccurate; the response contains significant errors or shows \nmisunderstanding of the topic.\n- 2 points: Somewhat inaccurate; the response contains multiple errors, though it \ngrasps the basic idea.\n- 3 points: Moderately accurate; the response has minor errors but generally \nunderstands the topic.\n- 4 points: Mostly accurate; the response contains minimal, inconsequential errors.\n- 5 points: Fully accurate; the response is free from errors and fully understands \nthe topic.\nPersona Representation:\n- 1 point: No representation; Speaker2's persona is not reflected in the response.\n- 2 points: Weak representation; Speaker2's persona is hinted at but largely absent \nor incorrect.\n- 3 points: Adequate representation; Speaker2's persona is present but some traits \nmay be missing or not fully captured.\n- 4 points: Strong representation; Speaker2's persona is clear and most traits are \nwell represented.\n- 5 points: Full representation; Speaker2's persona is fully and accurately \nportrayed throughout the response.\nExample Output Form:\nScore:\nRelevance Score: {score}\nAccuracy Score: {score}\nPersona Representation Score: {score}\nScoring Rationale:\nRelevance Score: {scoring rationale}\nAccuracy Score: {scoring rationale}\nPersona Representation Score: {scoring rationale}\nGenerate Responds Based on Conversation and Persona\nFigure 4: Prompt Evaluation Metric for MTCR Dataset.\nveloped based on the Multi-Session Chat (MSC)\ndataset (Xu et al., 2022). The dataset construc-\ntion involved sequentially using two prompts, as\nshown in Figure 3. The input of prompts is the\nMSC dataset\u2019s conversation and Speaker2\u2019s per-\nsona to generate a response for Speaker1. During\nthe inference phase, the model was required to\nconsider the multi-turn conversation contextual de-\ntails mentioned previously to generate a response\nfor speaker2, coping with the response created for\nspeaker1. Following this, a manual screening pro-\ncess was conducted to eliminate samples that did\nnot meet certain criteria, such as persona content\nleakage and irrelevance to the context or persona,\nculminating in a refined selection of 304 samples.\nFor the MTCR dataset\u2019s evaluation, we merge the\npersona as a known condition along with the model-\ngenerated response for Speaker2 in the prompt, as\ndepicted in Figure 4, and then pass them into GPT-4\n(OpenAI, 2023), obtaining scoring.\nPrompt.\nIn the experimental comparison, we\nconsider four distinct prompts for retrieval-\nMethod\nGPT-3.5-turbo\nLLaMA 2 Chat (70B)\nVanilla\n0.398\n0.330\nRetrieval\n0.475\n0.510\nCoT\n0.482\n0.525\nThoT\n0.574\n0.561\nTable 1: Performance Comparison on PopQA.\nMethod\nGPT-3.5-turbo\nLLaMA 2 Chat (70B)\nVanilla\n0.497\n0.430\nRetrieval\n0.512\n0.522\nCoT\n0.517\n0.547\nThoT\n0.565\n0.559\nTable 2: Performance Comparison on EntityQ.\naugmented generation. (1) \u201cVanilla\u201d entails using\nthe instruction and question as the prompt without\nproviding any retrieval results, i.e., \u201c{instruction}\n{question}.\u201d. (2) \u201cRetrieval\u201d includes retrieval re-\nsults within the prompt, formatted as \u201c{instruction}\n{retrieval results} {question}.\u201d. (3) \u201cCoT\u201d (Chain\nof Thought) incorporates the retrieval results and\nappends the phrase \u201cLet\u2019s think step by step\u201d to\nthe instruction and question, resulting in \u201c{instruc-\ntion} {retrieval results} {question} Let\u2019s think step\nby step.\u201d. (4)\u201cThoT\u201d (Thought-by-Thought) also\nintegrates retrieval results and follows a more de-\ntailed prompt structure: \u201c{instruction} {retrieval\nresults} {question} Walk me through this context\nin manageable parts step by step, summarizing\nand analyzing as we go.\u201d. For the MTCR dataset,\nwe employ only the \u201cVanilla\u201d, \u201cCoT\u201d, and \u201cThoT\u201d\nprompts. Their formats are, respectively: \u201c{instruc-\ntion} {conversation}\u201d, \u201c{instruction} Let\u2019s think\nstep by step. {conversation}\u201d, and \u201c{instruction}\nWalk me through this context in manageable parts\nstep by step, summarizing and analyzing as we go.\n{conversation}\u201d.\nLanguage models.\nWe evaluated four large-scale\nlanguage models: GPT-3.5-turbo (Schulman et al.,\n2022), GPT-4 (OpenAI, 2023), LLaMA 2 Chat\n(Touvron et al., 2023b), and Vicuna (Chiang et al.,\n2023). Due to the GPT-3.5-turbo and GPT-4 are not\nopen-source, the details of their model parameters\nremain undisclosed. For the LLaMA 2 Chat model,\nwe utilized variants with 7B, 13B, and 70B param-\neters in our experiments. Similarly, versions with\n7B, 13B, and 33B parameters of the Vicuna model\nwere employed. Sampling from these models was\nconducted using a greedy decoding strategy.\nMethod\nGPT-3.5-turbo\nLLaMA 2 Chat (70B)\nRelevance\nAccuracy\nPersona\nAverage\nRelevance\nAccuracy\nPersona\nAverage\nVanilla\n3.211\n3.135\n3.345\n3.230\n2.819\n2.901\n2.914\n2.878\nCoT\n3.352\n3.220\n3.349\n3.307\n2.783\n2.806\n2.882\n2.823\nThoT\n3.849\n3.921\n3.645\n3.805\n3.158\n3.295\n3.268\n3.240\nTable 3: Performance Comparison on MTCR dataset.\nMethod\nPopQA\nEntityQ\nGPT-4\nGPT-3.5-turbo\nLLaMA 2 Chat (70B)\nGPT-4\nGPT-3.5-turbo\nLLaMA 2 Chat (70B)\nVanilla\n0.430\n0.391\n0.314\n0.405\n0.405\n0.369\nRetrieval\n0.360\n0.477\n0.430\n0.571\n0.560\n0.643\nCoT\n0.442\n0.465\n0.558\n0.560\n0.583\n0.667\nThoT\n0.651\n0.674\n0.663\n0.643\n0.667\n0.702\nTable 4: Study of \u201cLost in Middle\u201d in PopQA and EntityQ.\n4.2\nResults\nTables 1 and Tables 2 show the performance of\nretrieval-augmented generation. In PopQA and En-\ntityQ datasets, we notice a consistent pattern where\nthe Thought-by-Thought (ThoT) prompt configu-\nration outperforms the other methods. The intro-\nduction of CoT also demonstrates a positive effect,\nindicating that prompting models to follow a me-\nthodical problem-solving approach can improve\nperformance metrics. It is particularly notewor-\nthy that ThoT exhibits a marked improvement in\nresults over the CoT configuration, highlighting\nthe efficacy of stepwise contextual processing in\nenhancing the quality of generated responses. In\nTables 3, a similar trend emerges. ThoT retains its\nlead, suggesting that its detailed prompt structure,\nwhich encourages summarizing and analyzing in-\nformation in a structured manner, is particularly\neffective in complex conversational contexts. It\nunderscores the importance of a methodical break-\ndown of context in generating relevant, accurate,\nand persona-consistent responses. The structured\napproach of ThoT prompts, which guide the model\nthrough a detailed, step-by-step analysis, consis-\ntently yields the best performance across chaotic\ncontexts.\n4.3\nLost in Middle\nAs shown in Table 4, we delves into the phenomena\ntermed \u201cLost in Middle\u201d (Liu et al., 2023), where\nthe focus is to examine the performance of vari-\nous models on two different question-answering\ndatasets, PopQA and EntityQ. The presented re-\nsults draw a comparison between four methodolo-\ngies: Vanilla, Retrieval, Chain of Thought (CoT),\nand Theory of Mind (ThoT), as applied to three\nadvanced language models: GPT-4, GPT-3.5-turbo,\nand LLaMA 2 Chat (70B).\nPerformance on PopQA\n: The results indicate\nthat ThoT significantly outperforms the other meth-\nods across all three models. With GPT-4 lead-\ning at a score of 0.651, closely followed by GPT-\n3.5-turbo and LLaMA 2 Chat (70B) at 0.674 and\n0.663, respectively. This suggests that ThoT\u2019s ad-\nvanced technique, potentially incorporating more\nnuanced understandings of context and reasoning,\nhas a definitive edge in handling the complexities of\nPopQA. The Vanilla approach yields moderate per-\nformance with GPT-4, which surpasses the scores\nof the other two models, hinting at the superior\nreasoning capabilities of the latest model iteration.\nPerformance on EntityQ\n: Similar to PopQA,\nthe ThoT methodology again tops the charts, in-\ndicating its robustness across different datasets.\nGPT-4\u2019s performance, while still the highest in\nthe Vanilla method, sees a significant jump to\n0.643 when applying ThoT, suggesting a better\nsynergy between GPT-4\u2019s capabilities and ThoT\u2019s\nadvanced reasoning framework. Notably, the Re-\ntrieval method showcases a stark improvement over\nVanilla for all models, with LLaMA 2 Chat (70B)\nachieving the highest score of 0.643.\n4.4\nImpact of Model Scale\nAs shown in Figure 5, results demonstrate a clear\ncorrelation between the scale of the model and its\nperformance across different prompting strategies.\nAs we scale up from 7 billion parameters to 70\nbillion parameters in the LLama2, there is a notice-\nNo.\nTemplate\nEM\n1\nLet\u2019s read through the document section by section, analyzing each part carefully as we go.\n0.43\n2\nTake me through this long document step-by-step, making sure not to miss any important details.\n0.47\n3\nDivide the document into manageable parts and guide me through each one, providing insights as we move\n0.51\nalong.\n4\nAnalyze this extensive document in sections, summarizing each one and noting any key points.\n0.47\n5\nLet\u2019s go through this document piece by piece, paying close attention to each section.\n0.50\n6\nExamine the document in chunks, evaluating each part critically before moving to the next.\n0.49\n7\nWalk me through this lengthy document segment by segment, focusing on each part\u2019s significance.\n0.52\n8\nLet\u2019s dissect this document bit by bit, making sure to understand the nuances of each section.\n0.45\n9\nSystematically work through this document, summarizing and analyzing each portion as we go.\n0.45\n10\nNavigate through this long document by breaking it into smaller parts and summarizing each, so we don\u2019t\n0.48\nmiss anything.\n11\nLet\u2019s explore the context step-by-step, carefully examining each segment.\n0.44\n12\nTake me through the context bit by bit, making sure we capture all important aspects.\n0.49\n13\nLet\u2019s navigate through the context section by section, identifying key elements in each part.\n0.47\n14\nSystematically go through the context, focusing on each part individually.\n0.46\n15\nLet\u2019s dissect the context into smaller pieces, reviewing each one for its importance and relevance.\n0.47\n16\nAnalyze the context by breaking it down into sections, summarizing each as we move forward.\n0.49\n17\nGuide me through the context part by part, providing insights along the way.\n0.52\n18\nExamine each segment of the context meticulously, and let\u2019s discuss the findings.\n0.44\n19\nApproach the context incrementally, taking the time to understand each portion fully.\n0.42\n20\nCarefully analyze the context piece by piece, highlighting relevant points for each question.\n0.47\n21\nIn a step-by-step manner, go through the context, surfacing important information that could be useful.\n0.53\n22\nMethodically examine the context, focusing on key segments that may answer the query.\n0.45\n23\nProgressively sift through the context, ensuring we capture all pertinent details.\n0.46\n24\nNavigate through the context incrementally, identifying and summarizing relevant portions.\n0.48\n25\nLet\u2019s scrutinize the context in chunks, keeping an eye out for information that answers our queries.\n0.42\n26\nTake a modular approach to the context, summarizing each part before drawing any conclusions.\n0.47\n27\nRead the context in sections, concentrating on gathering insights that answer the question at hand.\n0.48\n28\nProceed through the context systematically, zeroing in on areas that could provide the answers we\u2019re\n0.49\nseeking.\n29\nLet\u2019s take a segmented approach to the context, carefully evaluating each part for its relevance to the\n0.39\nquestions posed.\n30\nWalk me through this context in manageable parts step by step, summarizing and analyzing as we go.\n0.55\nTable 5: Prompt Selection Analysis.\n7B\n13B\n70B\nModel Size\n0.3\n0.4\n0.5\nEM\nVanilla\nRetrieval\nCoT\nThoT\n(a) LLaMA 2\n7B\n13B\n33B\nModel Size\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nEM\nVanilla\nRetrieval\nCoT\nThoT\n(b) Vicuna\nFigure 5: PopQA performance on different scales of\nLLaMA 2 and Vicuna.\nable increase in the EM scores across all prompt\nconfigurations. The same trend is observed with\nthe Vicuna model. This increase corroborates the\nhypothesis that larger models have a greater capac-\nity for understanding and generating accurate re-\nsponses. The Thought-by-Thought (ThoT) configu-\nration maintains a lead in performance at all model\nsizes, which demonstrates ThoT prompts appear to\nleverage the model\u2019s capabilities more efficiently,\nregardless of model size. In addition, the exper-\nimental data suggests that model scale positively\nimpacts the performance of retrieval-augmented\ngeneration methods, with prompts such as CoT and\nThoT showing the most significant benefits. This\ntrend underscores the importance of prompt design\nin harnessing the full potential of large language\nmodels.\n4.5\nPrompt Selection\nThe prompt analysis, as depicted in Table 5, re-\nveals strategic variations in phrasing that have a\ndirect impact on the performance of the language\nmodels, as indicated by the Exact Match (EM)\nscores. The prompts are designed to guide the\nmodel through the process of analyzing a document\nor context systematically and step by step. A com-\nmon theme among the more successful prompts,\nparticularly those with EM scores above 0.50, is\nthe explicit instruction to the model to not only\nanalyze each part of the document but also to sum-\nmarize and note key insights as it progresses. For\ninstance, prompts that instructed the model to sum-\nAs a content reviewer, I provide multiple retrieved passages about this question; you need to answer the \nquestion.\n  \nretrieved Passage 1 is: The Red Romance The Red Romance is a New York City band formed in late \n2006. \u2026\nretrieved Passage 2 is: live show. In November 2006 Cardenas left the band to play in the Portland\u2026\nretrieved Passage 3 is: A Tribe Called Red A Tribe Called Red (often abbreviated as ATCR) is a \u2026\nretrieved Passage 4 is: barbershop to rock and roll. Scottish folk-rock band Runrig have made \u2026\nretrieved Passage 5 is: Cagg did shows in West Germany (Bonn & Frankfurt). In September Abby \nHick\u2026\\ \nretrieved Passage 6 is: The Red Hearts The Red Hearts are a garage punk band from Los Angeles, \u2026\nretrieved Passage 7 is: drums Adam Chilenski \u2013 bass, backing vocals The Red Romance The Red \nRomance \u2026\nretrieved Passage 8 is: April 2008. Pops is a former member of the \u2026 The Red Hearts are a garage punk \nband\u2026\nretrieved Passage 9 is: stage by two additional musicians : Ronan Le Bars (bagpipes, whistles), (Dan Ar \nBraz \u2026\nretrieved Passage 10 is: Red (band) Red (also stylized R3D or RED) is an American rock band from \nNashville, \u2026\nQ: What type of music does The Red Hearts play?\n{Prompting Method}\nA:\nInput\nThe retrieved passages provide information about various bands, but none specifically state the type of \nmusic that The Red Hearts play. Therefore, we cannot determine the type of music that The Red Hearts \nplay based on the given information.\nThe passages retrieved for The Red Hearts provide information about the band's formation, members, \nand musical style. \nPassage 2 mentions that The Red Hearts went through a series of drummers, including Eric Tretbar and \nJustino Polimeni, before Kirk Podell joined as the current drummer. The band embarked on a tour of the \nSouthern States.\nAccording to Passage 6, The Red Hearts, a garage punk band, were formed in 2004 by Lenny Pops after \nthe breakup of his previous band, The Snakes. The band was described as a minimal mod/punk three-\npiece. They played local shows in Los Angeles and Long Beach.\nPassage 8 states that Lenny Pops, the founder of The Red Hearts, was a former member of the Brian \nJonestown Massacre and played with Blake Miller of Moving Units in a band called Spectacle. The Red \nHearts are a garage punk band.\nBased on these passages, The Red Hearts can be described as a garage punk band with mod and punk \ninfluences. They have gone through various drummers and have toured extensively. Lenny Pops, the \nfounder, has a background in other bands like The Snakes and Spectacle.\nCoT Output\nThoT Output\nFigure 6: Case Study.\nAs a content reviewer, I provide multiple retrieved passages about this question; you need to answer the question.\nretrieved Passage 1 is: claims that ever since her father\u2018s death she has been \u2026\nretrieved Passage 2 is: \u2026Andi Eigenmann is his daughter with actress Jaclyn Jose...\nretrieved Passage 3 is: several tokens (e.g., a lock of hair, some black beads \u2026\nretrieved Passage 4 is: 2017, he took part in the German TV series \" Let's Dance\u2026\nretrieved Passage 5 is: engaged. When Jorja Fox decided to leave the show, \u2026\nretrieved Passage 6 is: as well as in the 420 discipline in 2007 and 2008. \u2026\nretrieved Passage 7 is: to be to transfix a man with a harpoon. Grissom \u2026\nretrieved Passage 8 is: Rivera on June 1, 2009. The next year, Rivera \u2026\nretrieved Passage 9 is: the relationship, saying that it was in her best interest. \u2026\nretrieved Passage 10 is: Sidle and Grissom were interested in each other \u2026\nQ: Who is Mark Gil married to?\nWalk me through this context in manageable parts step by step, summarizing and analyzing as we go.\nA:\nInput\nThe question is about Mark Gil and who he is married to. Mark Gil is mentioned in \nretrieved Passage 2 and retrieved Passage 8. \nIn retrieved Passage 2, it is mentioned that Mark Gil was married to Maricar Jacinto in \n1996 and they had one child named Stephanie. However, this passage does not provide \nany information about Mark Gil's current marital status or who he is currently married to. \nIn retrieved Passage 8, it is not mentioned who Mark Gil is married to. Instead, this \npassage provides information about his previous marriages and divorces. It states that he \nwas previously married to a woman named Elisabeth Rivera in 2009, but they divorced \nin 2011. It also mentions that he later married Kim Perks in 2012 and they had a \ndaughter in 2014. This passage does not provide any information on Mark Gil's current \nmarital status or who he is currently married to.\nTherefore, based on the provided information, it is not possible to determine who Mark \nGil is currently married to.\nOutput\nFigure 7: Error Analysis.\nmarize each section and not miss important de-\ntails, such as prompt 2 and prompt 4, resulted in\nhigher EM scores. Prompts that encouraged a more\ngranular approach, directing the model to focus\non individual parts and their significance or rel-\nevance, also performed well. This is evidenced\nby prompt 14, which achieved a relatively high\nEM score. The more detailed the instruction for\nthe model to dissect and analyze the context, the\nbetter the model performed. Conversely, prompts\nthat were less directive or less structured, such as\nprompt 29, tended to result in lower EM scores.\nThis suggests that models benefit from clear, spe-\ncific, and action-oriented instructions that leave\nlittle room for ambiguity in the analytical process.\nThe highest-scoring prompt, number 30, combines\nseveral elements of successful prompts. It asks the\nmodel to manage the complexity by breaking it\ndown into parts, which implies a thorough analy-\nsis, and also to summarize and analyze, indicating\nan active engagement with the material that goes\nbeyond mere reading or passive understanding. In\nsummary, the results suggest that prompts that are\nstructured to enforce a detailed analytical process,\nencouraging step-by-step dissection, summariza-\ntion, and critical evaluation, lead to better model\nperformance.\n4.6\nCase Study\nThe case study presented in Figure 6 shows a com-\nparative analysis between the CoT and ThoT in\nPopQA. CoT only stated that the passages con-\ntained information about various bands without\nspecifying the genre of \u201cThe Red Hearts\u201d. This il-\nlustrates a potential limitation of the CoT approach:\nit might not effectively synthesize information from\nmultiple sources when the answer is not explicitly\nstated but rather needs to be inferred from the given\ndata. On the contrary, the ThoT method success-\nfully identified that \u201cThe Red Hearts play garage\npunk music\u201d. This outcome showcases the strength\nof the ThoT approach. ThoT is adept at synthe-\nsizing and correlating information across multiple\npieces of text. It pieced together relevant details\nfrom passages 6 and 8, noting that \u201cThe Red Hearts\u201d\nwere described as \u201ca garage punk band\u201d.\n4.7\nError Analysis\nFrom Figure 7, the ThoT method can not conclude\nthe answer for this case. The passage stating, \u201cAndi\nEigenmann is his daughter with actress Jaclyn Jose\u201d\nholds the key to the correct inference that Mark Gil\nwas married to Jaclyn Jose. The ThoT method\u2019s\nfailure to make this inference suggests that while\nthe model is adept at extracting explicit informa-\ntion, it struggles with implicit reasoning that re-\nquires understanding nuanced relationships. The\noversight may be attributed to the model\u2019s inferen-\ntial reasoning capabilities, specifically regarding\nrelationship inference\u2014a known shortcoming in\nlarge models as also identified in prior research\n(Berglund et al., 2023). The case study highlights\nthe need for models to not only parse and sum-\nmarize information but also engage in a level of\ndeductive reasoning that resembles human cogni-\ntion. Therefore, enhancing the model\u2019s ability to\ninfer and reason about entity relationships is very\nimportant.\n5\nConclusion\nThis paper presented the \u201cThread of Thought\u201d\n(ThoT) strategy, a novel approach designed to en-\nhance the performance of Large Language Mod-\nels (LLMs) in processing chaotic contextual infor-\nmation. ThoT, inspired by human cognitive pro-\ncesses, significantly improves the ability of LLMs\nto segment and analyze extended contexts. We\ncompared ThoT with existing methods, which of-\nten require complex retraining, fine-tuning, or are\nlimited in their ability to handle large volumes of\nintricate information. ThoT, in contrast, offers a\nmore straightforward and efficient solution. It acts\nas a \u201cplug-and-play\u201d module, seamlessly integrat-\ning with various pre-trained language models and\nprompting strategies without necessitating com-\nplex procedures. The effectiveness of ThoT was\nrigorously tested using long-tail question answer-\ning datasets, such as PopQA and EntityQ, and a\nMulti-Turn Conversation Response dataset based\non everyday conversations. The results from these\nevaluations were clear: ThoT not only excelled in\nhandling chaotic contexts but also enhanced the\nreasoning capabilities of LLMs.\nReferences\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita\nBalesni, Asa Cooper Stickland, Tomasz Korbak, and\nOwain Evans. 2023.\nThe reversal curse: Llms\ntrained on \"a is b\" fail to learn \"b is a\".\nCoRR,\nabs/2309.12288.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadom-\nski, Piotr Nyczyk, and Torsten Hoefler. 2023. Graph\nof thoughts: Solving elaborate problems with large\nlanguage models. CoRR, abs/2308.09687.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nShouyuan Chen, Sherman Wong, Liangjian Chen, and\nYuandong Tian. 2023. Extending context window of\nlarge language models via positional interpolation.\nCoRR, abs/2306.15595.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nJiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang,\nShaohan Huang, Wenhui Wang, Nanning Zheng, and\nFuru Wei. 2023. Longnet: Scaling transformers to 1,\n000, 000, 000 tokens. CoRR, abs/2307.02486.\nHuiqiang Jiang, Qianhui Wu, , Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.\nLongllmlingua: Accelerating and enhancing llms\nin long context scenarios via prompt compression.\nArXiv preprint, abs/2310.06839.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020.\nRetrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-\njape, Michele Bevilacqua, Fabio Petroni, and Percy\nLiang. 2023. Lost in the middle: How language\nmodels use long contexts. CoRR, abs/2307.03172.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi Das,\nDaniel Khashabi, and Hannaneh Hajishirzi. 2023.\nWhen not to trust language models: Investigating\neffectiveness of parametric and non-parametric mem-\nories. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 9802\u20139822. Association for\nComputational Linguistics.\nYasmin Moslem, Rejwanul Haque, John D. Kelleher,\nand Andy Way. 2023. Adaptive machine translation\nwith large language models. In Proceedings of the\n24th Annual Conference of the European Associa-\ntion for Machine Translation, EAMT 2023, Tampere,\nFinland, 12-15 June 2023, pages 227\u2013237. European\nAssociation for Machine Translation.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nOfir Press, Noah A. Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,\nInbal Magar, Omri Abend, Ehud Karpas, Amnon\nShashua, Kevin Leyton-Brown, and Yoav Shoham.\n2023. Parallel context windows for large language\nmodels. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 6383\u20136402. Association for\nComputational Linguistics.\nJohn Schulman, Barret Zoph, Christina Kim, Jacob\nHilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron\nUribe, Liam Fedus, Luke Metz, Michael Pokorny,\net al. 2022. Chatgpt: Optimizing language models\nfor dialogue. OpenAI blog.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers.\nIn Proceedings\nof the 2021 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2021, Vir-\ntual Event / Punta Cana, Dominican Republic, 7-11\nNovember, 2021, pages 6138\u20136148. Association for\nComputational Linguistics.\nDerek Tam, Anisha Mascarenhas, Shiyue Zhang, Sarah\nKwan, Mohit Bansal, and Colin Raffel. 2023. Evalu-\nating the factual consistency of large language mod-\nels through news summarization.\nIn Findings of\nthe Association for Computational Linguistics: ACL\n2023, Toronto, Canada, July 9-14, 2023, pages 5220\u2013\n5255. Association for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand efficient foundation language models. CoRR,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aur\u00e9lien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and\nfine-tuned chat models. CoRR, abs/2307.09288.\nSheng Wang, Zihao Zhao, Xi Ouyang, Qian Wang,\nand Dinggang Shen. 2023a. Chatcad: Interactive\ncomputer-aided diagnosis on medical image using\nlarge language models. CoRR, abs/2302.07257.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V.\nLe, Ed H. Chi, Sharan Narang, Aakanksha Chowd-\nhery, and Denny Zhou. 2023b.\nSelf-consistency\nimproves chain of thought reasoning in language\nmodels. In The Eleventh International Conference\non Learning Representations, ICLR 2023, Kigali,\nRwanda, May 1-5, 2023. OpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS.\nGuangxuan Xiao, Yuandong Tian, Beidi Chen, Song\nHan, and Mike Lewis. 2023.\nEfficient stream-\ning language models with attention sinks. CoRR,\nabs/2309.17453.\nJing Xu, Arthur Szlam, and Jason Weston. 2022. Be-\nyond goldfish memory: Long-term open-domain con-\nversation. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), ACL 2022, Dublin, Ireland,\nMay 22-27, 2022, pages 5180\u20135197. Association for\nComputational Linguistics.\nPeng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee,\nChen Zhu, Zihan Liu, Sandeep Subramanian, Evelina\nBakhturina, Mohammad Shoeybi, and Bryan Catan-\nzaro. 2023. Retrieval meets long context large lan-\nguage models. CoRR, abs/2310.03025.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023a. Tree of thoughts: Deliberate\nproblem solving with large language models. CoRR,\nabs/2305.10601.\nYao Yao, Zuchao Li, and Hai Zhao. 2023b.\nBe-\nyond chain-of-thought, effective graph-of-thought\nreasoning in large language models.\nCoRR,\nabs/2305.16582.\nShengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li,\nChenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao,\nSong Yun, Xuanjing Huang, and Zhongyu Wei. 2023.\nDisc-lawllm: Fine-tuning large language models for\nintelligent legal services. CoRR, abs/2309.11325.\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan,\nand Lidong Bing. 2023. Sentiment analysis in the\nera of large language models: A reality check. CoRR,\nabs/2305.15005.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc V. Le, and Ed H.\nChi. 2023. Least-to-most prompting enables com-\nplex reasoning in large language models. In The\nEleventh International Conference on Learning Rep-\nresentations, ICLR 2023, Kigali, Rwanda, May 1-5,\n2023. OpenReview.net.\n"
  }
]