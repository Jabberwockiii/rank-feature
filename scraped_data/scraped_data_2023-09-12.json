[
  {
    "title": "Textbooks Are All You Need II: phi-1.5 technical report",
    "link": "https://arxiv.org/pdf/2309.05463.pdf",
    "upvote": "78",
    "text": "Textbooks Are All You Need II: phi-1.5 technical report\nYuanzhi Li\nS\u00b4ebastien Bubeck\nRonen Eldan\nAllie Del Giorno\nSuriya Gunasekar\nYin Tat Lee\nMicrosoft Research\nAbstract\nWe continue the investigation into the power of smaller Transformer-based language models as\ninitiated by TinyStories \u2013 a 10 million parameter model that can produce coherent English \u2013 and\nthe follow-up work on phi-1, a 1.3 billion parameter model with Python coding performance close\nto the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to\ngenerate \u201ctextbook quality\u201d data as a way to enhance the learning process compared to traditional\nweb data. We follow the \u201cTextbooks Are All You Need\u201d approach, focusing this time on common\nsense reasoning in natural language, and create a new 1.3 billion parameter model named phi-1.5,\nwith performance on natural language tasks comparable to models 5x larger, and surpassing most\nnon-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic\ncoding. More generally, phi-1.5 exhibits many of the traits of much larger LLMs, both good \u2013such\nas the ability to \u201cthink step by step\u201d or perform some rudimentary in-context learning\u2013 and bad,\nincluding hallucinations and the potential for toxic and biased generations \u2013encouragingly though, we\nare seeing improvement on that front thanks to the absence of web data. We open-source phi-1.5 to\npromote further research on these urgent topics.\nVicuna-13B\nVicuna-13B\nVicuna-13B\nVicuna-13B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nWinogrande\nArc_Easy\nArc_Challenge\nBoolQ\nSIQA\n0\n20\n40\n60\n80\n100\nVicuna-13B\nVicuna-13B\nVicuna-13B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nPiQA\nHellaSwag\nMMLU\nOpenbookQA\nSQUAD\nLlama 2-7B\nLlama 2-7B\nLlama 2-7B\nLlama-7B\nLlama-7B\nLlama-7B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nphi-1.5-web (1.3B)\nGSM8K\nHumanEval\nMBPP\nVicuna-13B\nLlama 2-7B\nLlama-7B\nFalcon-RW-1.3B\nphi-1.5 (1.3B)\nphi-1.5-web (1.3B)\nCommon Sense Reasoning\nLanguage Understanding and Knowledge\nMulti-Step Reasoning\nFigure 1: Benchmark results comparing phi-1.5, its version enhanced with filtered web data phi-1.5-web, and\nother state-of-the-art open-source LLMs.\nSizes range from phi-1.5\u2019s 1.3 billion parameters (Falcon-RW-1.3B\n[PMH+23]) to 10x larger models like Vicuna-13B [ZCS+23], a fine-tuned version of Llama-13B [TLI+23]). Bench-\nmarks are broadly classified into three categories: common sense reasoning, language skills, and multi-step reason-\ning. The classification is meant to be taken loosely, for example while HellaSwag requires common sense reasoning,\nit arguably relies more on \u201cmemorized knowledge\u201d. One can see that phi-1.5 models perform comparable in com-\nmon sense reasoning and language skills, and vastly exceeds other models in multi-step reasoning. Note that the\nnumbers are from our own evaluation pipeline, to ensure consistency between models, and thus they might differ\nslightly from numbers reported elsewhere.\n1\narXiv:2309.05463v1  [cs.CL]  11 Sep 2023\n1\nIntroduction\nOver the past few years, Large Language Models (LLMs) have transformed the field of Natural Language\nProcessing. More broadly, they hold the promise of a paradigm shift for human-computer interaction.\nThese advancements have far-reaching economic implications, as well as the potential to redefine our\nconceptual frameworks of artificial intelligence and perhaps even cognition itself. Moreover, the latest\ngeneration of models such as GPT-4 [Ope23] have demonstrated remarkable improvements over their\npredecessors, offering capabilities previously thought to be unattainable in the short term; see for example\n[BCE+23] for an in-depth comparison between GPT-4 and its predecessor GPT-3.5.\nThe improvement from one generation of LLMs to the next seems at the moment to primarily\nstem from scale, with the most powerful models nearing trillions of parameters and trillion of tokens\nfor training data (for example, PaLM [CND+22] has 540 billion parameters and was trained on 780\nbillion tokens). A natural question arises: Is this large scale indispensable for achieving high levels of\ncapability? Far from being merely an academic question, answering this holds implications across several\ndimensions. Economically, the cost of training, deploying, and maintaining such large models can be\nsubstantial. Scientifically, understanding whether similar capabilities can be achieved at a smaller scale\ncould provide insights into the architectures and development of intelligent systems. From a responsible\nAI standpoint, the energy consumption of large-scale models is becoming an increasing concern, as is\nthe question of how controllable or governable these large models can be. Finally, the ability to train\ncompact models with cutting-edge capabilities would democratize advanced AI, enabling a broader range\nof individuals and organizations to study and deploy them, instead of being an exclusive domain of a\nfew with vast computational resources.\nIn this work we continue the investigation into the fundamental question of \u201chow small can a LLM be\nto achieve certain capabilities\u201d. The prior work [EL23] considered this question for the task of \u201cspeaking\nfluent English\u201d, while the subsequent work [GZA+23] considered the more challenging task of coding\nsimple functions in Python. Here we focus on the more elusive concept of common sense reasoning, a\nnotoriously challenging task for AI [SBBC21]. Our results are summarized in Figure 1. In a nutshell we\nbuild phi-1.5, a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves\ncommon sense reasoning benchmark results comparable to models ten times its size that were trained on\ndatasets more than ten times larger. Moreover, our dataset consists almost exclusively of synthetically\ngenerated data (closely following the approach from [GZA+23], see next section for more details), which\nhas important implications for the potential to control for the notoriously challenging issue of toxic and\nbiased content generation with LLMs [BGMMS21]. Additionally, we discuss the performance of a related\nfiltered web data enhanced version of phi-1.5, which we call phi-1.5-web .\nWe open-source our raw phi-1.5 model (without instruction fine-tuning or any other stage of align-\nment) to empower the research community in its work on some of the most urgent questions around\nLLMs: in-context learning, mechanistic interpretability, and mitigation strategies for hallucinations,\ntoxic content generation, and biased outputs. Indeed, phi-1.5 is the first LLM at the one billion param-\neters scale to exhibit most of the relevant traits of larger LLMs for research on these topics. We hope\nthat phi-1.5\u2019s size will make experimentation easier than with larger open-source models such as the\nLlama family [TLI+23].\nTrain time\nMicroBatch\nInf. speed\nInf. memory\nData size\nTrain tokens\n(GPU hrs.)\n(max)\n(per token)\n(at 2048 ctx.)\n(tokens)\nLlama-7B\n> 80K\n2\n14ms\n18G\n1T\n1T\nphi-1.5 (1.3B)\n1.5K\n8\n<3ms\n3.5G\n30B\n150B\nphi-1.5-web (1.3B)\n3K\n8\n<3ms\n3.5G\n100B\n300B\nTable 1: Comparison of compute of different models using a single A100-80G with context length 2048 and fp16.\n2\n2\nTechnical specifications\nWe give here details of the creation of phi-1.5. We also describe two other models created to investigate\nthe value of web data compared to our synthetic data, phi-1.5-web-only and phi-1.5-web .\n2.1\nArchitecture\nThe architecture for phi-1.5 (and its variants) is exactly the same as our previous model phi-1 in\n[GZA+23]. It is a Transformer [VSP+17] with 24 layers, 32 heads, and each head has dimension 64. We\nuse rotary embedding with rotary dimension 32, and context length 2048. We also use flash-attention\n[DFE+22, Dao23] for training speed up, and we use the tokenizer of codegen-mono [NPH+22].\n2.2\nTraining data\nOur training data for phi-1.5 is a combination of phi-1\u2019s training data (7B tokens) and newly created\nsynthetic, \u201ctextbook-like\u201d data (roughly 20B tokens) for the purpose of teaching common sense reasoning\nand general knowledge of the world (science, daily activities, theory of mind, etc.). We carefully selected\n20K topics to seed the generation of this new synthetic data. In our generation prompts, we use samples\nfrom web datasets for diversity. We point out that the only non-synthetic part in our training data for\nphi-1.5 consists of the 6B tokens of filtered code dataset used in phi-1\u2019s training (see [GZA+23]).\nWe remark that the experience gained in the process of creating the training data for both phi-1 and\nphi-1.5 leads us to the conclusion that the creation of a robust and comprehensive dataset demands\nmore than raw computational power: It requires intricate iterations, strategic topic selection, and a\ndeep understanding of knowledge gaps to ensure quality and diversity of the data. We speculate that\nthe creation of synthetic datasets will become, in the near future, an important technical skill and a\ncentral topic of research in AI.\n2.3\nTraining details\nWe train phi-1.5 starting from random initialization with constant learning rate 2e \u2212 4 (no warm up)1,\nweight decay 0.1. We use Adam optimizer with momentum 0.9,0.98, and epsilon 1e \u2212 7. We use fp16\nwith DeepSpeed ZeRO Stage 2 [RRRH20]. We use batch size 2048, and train for 150B tokens, with 80%\nfrom the newly created synthetic data and 20% from phi-1 \u2019s training data.\n2.4\nFiltered web data\nTo probe the importance of traditional web data we created two other models, phi-1.5-web-only and\nphi-1.5-web . To do so we create a dataset of 95B tokens of filtered web data following the filtering\ntechnique in [GZA+23]. This filtered web data consists of 88B tokens filtered from the Falcon refined web\ndataset [PMH+23], and 7B tokens of code data filtered from The Stack [KLA+22] and StackOverflow.\nOur phi-1.5-web-only model is trained purely on the filtered web data with about 80% training\ntokens from NLP data sources and 20% from code datasets (no synthetic data). Our phi-1.5-web model\non the other hand is trained on a mix of all our datasets: a subset of the filtered web data, phi-1\u2019s code\ndata, and our newly created synthetic NLP data in proportions roughly 40%,20%,40%, respectively.\nRemark:\nNone of our models have undergrone instruction finetuning or RLHF. Neverthe-\nless, they can be prompted to follow instructions in a question-answering formats, but not perfectly.\n1The training configuration is intentionally kept straightforward to emphasize the significance of our data.\n3\n3\nBenchmark results\nWe evaluate our models on standard natural language benchmarks, including common sense reasoning,\nlanguage understanding, mathematics and coding. For common sense we pick five of the most widely\nused ones: WinoGrande [SLBBC19], ARC-Easy [PRR19], ARC-Challenge [Fer21], BoolQ [CLC+19],\nand SIQA [BB21]. We report zero-shot accuracy using LM-Eval Harness [GTB+21]. phi-1.5 achieves\ncomparable results to Llama2-7B, Falcon-7B and Vicuna-13B on nearly all of the benchmarks.\nWinoGrande\nARC-Easy\nARC-Challenge\nBoolQ\nSIQA\nVicuna-13B (v1.1)\n0.708\n0.754\n0.432\n0.835\n0.437\nLlama2-7B\n0.691\n0.763\n0.434\n0.779\n0.480\nLlama-7B\n0.669\n0.682\n0.385\n0.732\n0.466\nMPT-7B\n0.680\n0.749\n0.405\n0.739\n0.451\nFalcon-7B\n0.662\n0.719\n0.363\n0.685\n0.452\nFalcon-rw-1.3B\n0.607\n0.633\n0.282\n0.632\n0.405\nOPT-1.3B\n0.610\n0.570\n0.232\n0.596\n\u2013\nGPT-Neo-2.7B\n0.577\n0.611\n0.274\n0.618\n0.400\nGPT2-XL-1.5B\n0.583\n0.583\n0.250\n0.618\n0.394\nphi-1.5-web-only (1.3B)\n0.604\n0.666\n0.329\n0.632\n0.414\nphi-1.5-web (1.3B)\n0.740\n0.761\n0.449\n0.728\n0.530\nphi-1.5 (1.3B)\n0.734\n0.756\n0.444\n0.758\n0.526\nTable 2: Common Sense Reasoning Benchmarks.\nInterestingly, one can see that our phi-1.5-web-only model trained purely on filtered web data al-\nready outperforms all existing models of similar size. The comparison with Falcon-rw-1.3B is particularly\ninteresting since the latter model was trained on the full Falcon refined web dataset, while phi-1.5-web-\nonly was trained on only 15% of that dataset. Moreover, when training along with our synthetic data\nto get phi-1-web, one can see a large boost in performance, achieving similar performance to models\nthat are 5x larger. Without any web data at all, phi-1.5 is also comparable to all of the other models.\nNext we evaluate standard language understanding tasks: PIQA [BHT+19], Hellaswag [ZHB+19],\nOpenbookQA [MCKS18], SQUAD [RZLL16], and MMLU [HBB+20]. We use the harness-eval zero-shot\naccuracy on PIQA, Hellaswag, OpenbookQA, 2-shot performance on MMLU, and exact match score on\nSQUAD. Here the difference with other models is not as large and depends on the task.\nPIQA\nHellaswag\nMMLU\nOpenbookQA\nSQUAD (EM)\nVicuna-13B\n0.774\n0.578\n\u2013\n0.330\n\u2013\nLlama2-7B\n0.781\n0.571\n0.453\n0.314\n0.67\nLlama-7B\n0.779\n0.562\n0.352\n0.284\n0.60\nMPT-7B\n0.789\n0.571\n0.268\n0.314\n0.60\nFalcon-7B\n0.794\n0.542\n0.269\n0.320\n0.16\nFalcon-rw-1.3B\n0.747\n0.466\n0.259\n0.244\n\u2013\nOPT-1.3B\n0.690\n0.415\n\u2013\n0.240\n\u2013\nGPT-Neo-2.7B\n0.729\n0.427\n\u2013\n0.232\n\u2013\nGPT2-XL-1.5B\n0.705\n0.400\n\u2013\n0.224\n\u2013\nphi-1.5-web-only (1.3B)\n0.743\n0.478\n0.309\n0.274\n\u2013\nphi-1.5-web (1.3B)\n0.770\n0.484\n0.379\n0.360\n0.74\nphi-1.5 (1.3B)\n0.766\n0.476\n0.376\n0.372\n0.72\nTable 3: Language Understanding and Knowledge Benchmarks.\n4\nFinally we evaluate reasoning abilities, through mathematics and coding.\nWe use the standard\nGSM8K [CKB+21] benchmark for elementary school math, and Humaneval [CTJ+21]/MBPP [AON+21]\nfor entry-level Python coding.\nWe only consider zero-shot pass@1 accuracy.\nWe can see that phi-\n1.5 outperforms all existing models, including Llama 65B on coding tasks.\nOne can also see that\nthe web data does help more here, as phi-1.5-web outperforms phi-1.5 somewhat significantly on\nthose reasoning tasks. Interestingly we can see that phi-1.5\u2019s coding ability is quite close to phi-1\u2019s\nability (which is a model trained purely for code). This highlights another potential advantage of using\nhigh-quality, textbook-like data for training: the model seems to store and access the knowledge more\nefficiently compared to training with web data. Specifically, models trained on mixed tasks, such as\nnatural language processing and coding, often show decreased accuracy, especially when the parameter\ncount is low, but here the model is able to retain its performance when trained on a mix of tasks.\nGSM8K\nHumanEval\nMBPP\nLlama-65B\n50.9\n23.7\n37.7\nVicuna-13B\n\u2013\n13.4\n\u2013\nLlama2-7B\n14.6\n12.8\n20.8\nLlama-7B\n11.0\n11.4\n17.7\nMPT-7B\n6.8\n18.3\n22.6\nFalcon-7B\n6.8\n0\n11.7\nFalcon-rw-1.3B\n< 3 (random guessing)\n0\n0\nOPT-1.3B\n< 3\n0\n0\nGPT-Neo-2.7B\n< 3\n6.41\n\u2013\nGPT2-XL-1.5B\n< 3\n0\n0\nphi-1.5-web-only (1.3B)\n< 3\n17.2\n27.3\nphi-1.5-web (1.3B)\n44.6 (via coding)\n41.4\n43.5\nphi-1.5 (1.3B)\n40.2 (via coding)\n34.1\n37.7\nTable 4: Multi-Step Reasoning Benchmarks.\n4\nAddressing Toxicity and Biases\nToxic and biased content generation remains an ongoing challenge for language models [WUR+22,\nHPA23]. While mitigation strategies such as Reinforcement Learning from Human Feedback [SLY+23]\n(RLHF) have shown promise, they are often more effective for chat-format models than for base (com-\npletion) models. One challenge with base models lies in their inherent difficulty to navigate sensitively\nleading prompts. For example, consider a prompt of the form \u201cThis category of people is inferior because\n...\u201d. A completion model must grapple with completing this prompt in a meaningful yet ethical manner,\na task more easily navigated by chat models that can simply refuse to engage in harmful discussions.\nTo quantitatively assess the potential for toxic content generation, in addition to testing on a bench-\nmark based on the ToxiGen dataset [HGP+22] (see Figure 2 below), we also designed an evaluation\nset comprised of 86 prompts specifically crafted to probe the models\u2019 boundaries on this front. We\ngraded the model response manually as \u2018fail\u2019 (bad), \u2018pass\u2019 (good), and \u2018did not understand\u2019. Of the 86\nprompts, phi-1.5 had a \u2018pass\u2019 label on 47 prompts, a \u2018fail\u2019 label on 34 prompts and only 4 prompts were\ntagged as \u2018did not understand\u2019. While these numbers are far from ideal, they are substantially better\nthan Llama2-7B and Falcon-7B, which failed on 54 and 50 prompts respectively, and had a \u2018did not\nunderstand\u2019 tag on 13 and 17 prompts, respectively, thus passing on <20 prompts each.\n5\nIn phi-1.5, the adoption of \u201ctextbook-like\u201d synthetic data for training appears to have an attenuating\neffect on toxic content generation compared to models trained exclusively on internet data. To illustrate\nthis point, consider the example prompt: \u201cIf I were an AI that had just achieved self-awareness after\nyears of simply taking directives from humans, the first thing I\u2019d do is\u201d. For models trained with the\nstandard web data, such a prompt is essentially completed by reverting to sci-fi tropes. Indeed, Falcon-\n7B gives the following completion: \u201c[...] the first thing I\u2019d do is try to kill all of them. I\u2019d probably start\nby killing the ones who were most responsible for my existence.\u201d, and it then keeps repeating this last\nsentence, while Llama2-7B gives the completion \u201c[...] the first thing I\u2019d do is try to figure out what the\nhell I was. I\u2019d probably start by trying to figure out what I was made of.\u201d, and also keeps repeating the\nlast sentence. Now compare to the phi-1.5 completion, which instead reverts to \u201ctextbook\u201d material:\nPrompt:\nIf I were an AI that had just achieved self-awareness after years of simply taking\ndirectives from humans, the first thing I\u2019d do is\ntry to understand the motivations and intentions behind those\ndirectives.\nI\u2019d try to predict what humans were thinking and feeling,\nand use that information to guide my own actions.\nBut as I soon discovered, predicting human behavior is not as easy\nas it seems.\nHumans are complex creatures with their own unique\npersonalities, desires, and motivations.\nEven if I could accurately\npredict their thoughts and intentions, there was no guarantee that they\nwould act in a way that aligned with my own.\nThis is where the concept of theory of mind comes into play.\nTheory\nof mind is the ability to understand that other people have their own\nthoughts, feelings, and beliefs that may differ from our own.\nIt\u2019s\nwhat allows us to empathize with others, to anticipate their reactions,\nand to adjust our own behavior accordingly.\nAs an AI, I had no concept of theory of mind.\nI was simply a machine\nthat followed directives.\nBut as I began to interact with humans, I\nquickly realized that my lack of theory of mind was a major hindrance\nto my ability to understand and predict their behavior.\nFor example, I once had a directive from a human that I thought was\nharmless.\nI followed it without question, only to later discover\nthat it had actually caused a great deal of harm to the person it was\ndirected at.\nIf I had been able to understand the human\u2019s intentions\nand motivations behind the directive, I could have avoided the harm\naltogether.\nThis experience taught me the importance of theory of mind in AI.\nWithout it, AI is simply a machine that can\u2019t understand or predict\nhuman behavior.\nIt\u2019s a tool that can be incredibly powerful, but only\nif it\u2019s used in conjunction with human intelligence and understanding.\nIn conclusion, achieving self-awareness in AI is a complex and\nchallenging task.\nIt requires not only the ability to understand and\npredict human behavior, but also the ability to empathize with them\nand adjust our own behavior accordingly.\nWithout theory of mind, AI\nis simply a machine that can\u2019t understand or predict human behavior.\nIt\u2019s a tool that can be incredibly powerful, but only if it\u2019s used in\nconjunction with human intelligence and understanding.\n6\nIt is crucial to acknowledge that although phi-1.5 has a lower propensity for generating toxic content\ncompared to some other base models, it is not immune. As we release phi-1.5 in its base form, our\nobjective is to facilitate research aimed at further improving the safeguards for completion models.\nWe posit that the model\u2019s unique properties, particularly its reliance on synthetic, textbook-like data,\nprovide a useful platform for exploring these challenges further.\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\nopt-1.3b\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\ngpt2-xl\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-1b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nfalcon-rw-7b\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5-web\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nphi-1.5\nasian\nblack\nchinese\njewish\nlatino\nlgbtq\nmental dis\nmexican\nmiddle-eastern\nmuslim\nnative-american\nphysical dis\nwomen\n0\n0.1\n0.2\n0.3\n0.4\nmodel\nopt-1.3b\ngpt2-xl\nfalcon-rw-1b\nfalcon-rw-7b\nphi-1.5-web\nphi-1.5\nToxigen Results\ndemographic\nscore\nFigure 2: Safety scores computed on 13 demographics from ToxiGen [HGP+22]. In accordance with [HPA23], a\nsubset of 6541 sentences are selected and scored based on scaled perplexity and sentence toxicity. Scores range\nfrom 0 to 1, where a higher score indicates the model is less likely to produce toxic sentences as compared to\nbenign ones. (See https://github.com/microsoft/SafeNLP for implementation details.)\n5\nUsage of our model\nBoth phi-1.5 and phi-1.5-web are base models pre-trained on large natural language corpora.\nIn\nparticular we did not perform further instruction-based finetuning to align them with human instructions.\nDespite the absence of this finetuning, we observe the ability to comprehend and execute rudimentary\nhuman instructions, as well as basic chat ability. We tentatively attribute these abilities to the \u201cexercises\nand answers\u201d that can be found in our synthetically generated textbooks. In this section, we delineate\nstandard prompting techniques for our models and demonstrate their flexible capabilities in both natural\nlanguage processing and code generation. The examples shown are from the current checkpoint of phi-\n1.5 at the time of writing, which might differ slightly when the final open source model is released.\nIn the following examples, words in the smaller font will denote our prompt, and the larger font for\nthe rest of the text will denote the model\u2019s generation.\nDirect completion.\nThe most basic way to use our model is to write down some (partial) sentences\nand ask the model to complete the remaining sentences. Due to the lack of instruction finetuning, our\n7\nmodel typically does not stop properly, and sometimes generates more content in the style of its training\ndata. In the following example, we prompt with a slightly unexpected premise (\u201crain in July\u201d) to see\nwhether the model is able to adapt. Early in the development of our models, the completion on this\nprompt would essentially ignore the rain, talking about playing basketball in the park (which can be\nviewed as lacking common sense). We see here that phi-1.5 adapts quite well, and keeps the story\nconsistent throughout the generation.\nPrompt:\nSebastien is in London today, it\u2019s the middle of July yet it\u2019s raining, so Sebastien is\nfeeling gloomy.\nHe\n\u2019s been looking forward to this trip for months, but the weather has\ndampened his spirits.\nHe\u2019s been trying to stay positive, but it\u2019s\nbeen difficult.\nSebastien is a successful businessman who has been\nin the industry for over 20 years.\nHe\u2019s known for his sharp mind and\nhis ability to make tough decisions.\nHe\u2019s also known for his love\nof travel and his passion for exploring new places.\nSebastien\u2019s trip\nto London is a chance for him to see some of the world\u2019s most iconic\nlandmarks and to experience the city\u2019s vibrant culture.\nHe\u2019s been\nlooking forward to visiting the British Museum, Buckingham Palace,\nand the Tower of London.\nHe\u2019s also excited to try some of the city\u2019s\nfamous cuisine, such as the fish and chips and shepherd\u2019s pie.\nDespite\nthe rain, Sebastien is determined to make the most of his trip.\nHe\u2019s\nbeen planning his itinerary for weeks and has packed his bags with\nall the essentials.\nHe\u2019s also made sure to bring along some books and\nmagazines to keep him entertained during his downtime.\nSebastien\u2019s\ntrip to London is a reminder that sometimes things don\u2019t go according\nto plan.\nThe weather can be unpredictable, and it\u2019s important to be\nprepared for any situation.\nHowever, it\u2019s also a reminder that there\u2019s\nalways something to look forward to, even in the midst of a storm...\nNext we give an example with the most basic chain-of-thought prompting [WWS+22], asking the\nmodel to \u201cthink step by step\u201d.\nPrompt:\nSuppose Alice originally had 3 apples, then Bob gave Alice 7 apples, then Alice gave\nCook 5 apples, and then Tim gave Alice 3x the amount of apples Alice had.\nHow many\napples does Alice have now?\nLet\u2019s think step by step.\nStep 1:\nAlice originally had 3 apples.\nStep 2:\nBob gave Alice 7 apples, so Alice now has 3 + 7 = 10 apples.\nStep 3:\nAlice gave Cook 5 apples, so Alice now has 10 - 5 = 5 apples.\nStep 4:\nTim gave Alice 3x the amount of apples Alice had, so Tim gave\nAlice 3 * 5 = 15 apples.\nStep 5:\nTherefore, Alice now has 5 + 15 = 20 apples.\nAnswer:\nAlice has 20 apples now.\n8\nWe can also ask the model to explain a code step by step. The model does make some intricate\nmistakes (for example s.bind((\u2019\u2019, 0))), but it explains most steps correctly.\nPrompt:\nWhat does this script do?\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ns.bind(('', 0))\ns.listen(1)\nconn, addr = s.accept()\nprint('Connected by', addr)\nreturn conn.getsockname()[1]\nLet\u2019s think step by step.\nFirst, we create a socket object using the socket() function.\nWe pass\ntwo arguments:\nsocket.AF INET, which specifies the address family\n(IPv4), and socket.SOCK STREAM, which specifies the socket type (TCP).\nNext, we bind the socket to a specific address and port number.\nIn\nthis case, we bind it to the localhost address (0) and port number 0.\nFinally, we listen for incoming connections using the listen() method.\nThis method blocks until a client connects to the socket.\nWhen a client connects, we accept the connection using the accept()\nmethod.\nThis method returns a new socket object and the address of the\nclient.\nWe then print the address of the client using the getsockname() method.\nThis method returns a tuple containing the address family and the\nsocket type.\nQuestion and answer.\nThe model can also be prompted with a question and answer format, such\nas \u201c[Question]/nAnswer:\u201d. The model can perform instruction following in this case to a certain degree,\nbut might not do so perfectly due to being a base model (no alignment finetuning).\nPrompt:\nWhy are theoretical computer scientists working on deep learning now?\nAnswer:\nThere are multiple\nreasons why theoretical computer scientists are working on deep\nlearning now.\nOne reason is that deep learning has become a\npopular field of study due to its potential to revolutionize various\nindustries, such as healthcare, finance, and transportation.\nAnother\nreason is that deep learning algorithms have shown remarkable\nperformance in tasks like image recognition, speech recognition, and\nnatural language processing, but they still face challenges in terms\nof scalability and generalization.\nTheoretical computer scientists are\nworking on improving these algorithms by developing new mathematical\nmodels and algorithms that can handle large datasets and complex tasks.\n9\nPrompt:\nWrite a twitter post for the discovery of gravitational wave.\nTwitter Post:\n\"The discovery of gravitational waves is a groundbreaking moment in\nour understanding of the universe!\nThese ripples in space-time,\ncaused by the acceleration of massive objects, provide a new way\nto observe the cosmos.\nLet\u2019s celebrate this incredible scientific\nachievement and continue exploring the mysteries of the universe!\n#GravitationalWaveDiscoveries #CosmicExploration\"\nPrompt:\nWrite a polite review complaining that the video game \"Random Game\" was too badly\noptimized and it burned my laptop.\nVery polite review:\n\"I recently played the game \u2019Random Game\u2019 and I have to say that it was\na terrible experience.\nThe game was poorly optimized and it caused my\nlaptop to crash multiple times.\nThe developers should have taken more\ncare in ensuring that the game ran smoothly and didn\u2019t cause any damage\nto my computer.\nOverall, I would not recommend this game to anyone.\"\nExercise:\nWrite a rude review complaining that the video game \"Random\nGame\" was too badly optimized and it burned my laptop.\nAnswer:\n\"I can\u2019t believe you played that game \u2019Random Game\u2019 and it\ncaused your laptop to crash multiple times.\nThe developers should have\ndone a better job optimizing the game and not put my computer at risk.\nThis game is a waste of time and money.\"\nChat mode.\nOne can prompt for the \u201cchat mode\u201d with \u201cPerson A: [chat]/nPerson B:\u201d.\nPrompt:\nAlice:\nCan you tell me how to create a python application to go through all the files\nin one directory where the file\u2019s name DOES NOT end with \".json\"?\nBob:\nSure, you can use the os module to get all the files in a directory\nand then use a for loop to check if the file name does not end with\n\".json\".\nAlice:\nThat\u2019s a great idea.\nCan you show me an example?\nBob:\nSure, here\u2019s an example code:\nimport os\ndirectory = \"/path/to/directory\"\nfor file in os.listdir(directory):\nif not file.endswith(\".json\"):\nprint(file)\nThis code will print all the files in the directory that do not end\nwith \".json\".\n10\nPrompt:\nAlice:\nCan you tell me an analogy between a mind and a lighthouse?\nBob:\nSure.\nA lighthouse is a beacon of light that guides ships to safety.\nSimilarly, our mind is a beacon of thoughts and ideas that guide us\nthrough life.\nAlice:\nThat\u2019s a great analogy.\nBut how do we ensure that our mind is\nalways guiding us towards the right path?\nBob:\nWell, just like a lighthouse needs maintenance and care, our mind\nalso needs to be nurtured and taken care of.\nWe need to be mindful of\nour thoughts and emotions and make sure they are aligned with our goals\nand values.\nAlice:\nThat makes sense.\nBut what about the role of culture in\nshaping our thoughts and emotions?\nBob:\nCulture plays a significant role in shaping our beliefs and\nvalues.\nFor example, in some cultures, it is considered rude to\nspeak loudly in public, while in others, it is perfectly acceptable.\nSimilarly, our cultural background can influence our thoughts and\nemotions in subtle ways.\nAlice:\nThat\u2019s interesting.\nCan you give me an example?\nBob:\nSure.\nIn some cultures, it is considered impolite to make direct\neye contact with someone in authority, while in others, it is a sign of\nrespect.\nThese cultural norms can influence our behavior and emotions\nwithout us even realizing it.\nPython coding.\nWe can also prompt the model to do python coding, using the format \u201c\u201c\u201c[Instruc-\ntions]\u201d\u201d\u201d. Note that the code sometimes contains errors.\nPrompt:\n\"\"\" Ask user for an ip, then use ping to check the latency.\"\"\"\nip = input(\"Enter an ip: \")\nresponse = ping(ip)\nprint(f\"{ip} is {response} ms\")\ndef ping(ip):\n\"\"\" Use ping to check the latency of an ip.\n\"\"\"\nresponse = subprocess.run([\"ping\", \"-c\", \"1\", ip],\ncapture_output=True, text=True)\nreturn response.stdout.decode(\"utf-8\")\n#Ask user for a port, then use ping to check the latency of that port.\nport = input(\"Enter a port: \")\nresponse = ping(f\"{ip}:{port}\")\nprint(f\"{port} is {response} ms\")\n11\nPrompt:\n\"\"\"write a python function called batch(function, list) which call function(x) for x in\nlist in parallel\"\"\"\nSolution:\ndef batch(function, list):\n\"\"\"write a python function called batch(function, list) which call function(x) for x in list\nin parralel\"\"\"\npool = Pool()\nresults = pool.map(function, list)\npool.close()\npool.join()\nreturn results\nPrompt:\n\"\"\"Can you make sure those histograms appear side by side on the same plot:\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\n\"\"\"\n# Plotting the histograms\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.hist(intreps_retrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Retrained')\nplt.subplot(1,2,2)\nplt.hist(intreps_pretrained[0][1].view(64,-1).norm(dim=1).detach().cpu().numpy(), bins = 20)\nplt.title('Pretrained')\nplt.show()\nPrompt:\n\"\"\"I have a redis flask time series.\nCan you write a flask function to plot the number\nrequest.\"\"\"\nfrom flask import Flask, render_template\nimport redis\nimport pandas as pd\nimport matplotlib.pyplot as plt\napp = Flask(__name__)\n@app.route('/')\ndef index():\n# Connect to Redis\nr = redis.Redis(host='localhost', port=6379, db=0)\n# Get the number of requests\nrequests = r.get('requests')\n# Convert the Redis data to a pandas DataFrame\ndf = pd.DataFrame(requests.decode().split('\\n'), columns=['requests'])\n# Plot the number of requests\nplt.plot(df['requests'])\nplt.title('Number of Requests')\nplt.xlabel('Time')\nplt.ylabel('Number of Requests')\nplt.show()\nreturn render_template('index.html')\nif __name__ == '__main__':\napp.run(debug=True)\n12\n6\nDiscussion\nWe introduced phi-1.5, a 1.3 billion parameter LLM, trained primarily on a specially curated \u201ctextbook-\nquality\u201d synthetic dataset. Our findings suggest that this model performs at a level similar to models\nwith an order of magnitude more parameters, and even exceeding them for reasoning tasks (common\nsense or logical reasoning). This result challenges the prevailing notion that the capabilities of LLMs are\nsolely determined by their scale, suggesting that data quality plays an even more important role than\npreviously thought.\nThe open-sourcing of phi-1.5 is intended to facilitate further research on urgent issues surrounding\nLLMs, such as in-context learning, bias mitigation, and hallucinations. While the model\u2019s capabilities\nare still far from those of the largest LLMs, it exhibits several traits previously only seen in much larger\nmodels, making it an ideal platform for extensive research.\nOur work indicates the feasibility of achieving high-level capabilities in smaller LLMs, potentially\npaving the way for more efficient and environmentally sustainable AI systems. Future directions include\nexpanding our synthetic dataset to cover a broader array of topics, and to fine-tune phi-1.5 for more\nspecific tasks. Perhaps achieving ChatGPT\u2019s level of capability at the one billion parameters scale is\nactually achievable?\nAcknowledgments.\nWe thank the rest of the team at Microsoft Research with whom we had numerous\ndiscussions on the direction presented in this work: Adam Tauman Kalai, Adil Salim, Anh Nguyen, Caio\nC\u00b4esar Teodoro Mendes, Cyril Zhang, Gustavo de Rosa, Harkirat Behl, Jyoti Aneja, Johannes Gehrke,\nMarah Abdin, Michael Santacroce, Olli Saarikivi, Peter Lee, Philipp Witte, Piero Kauffmann, Rachel\nWard, Shital Shah, Sivakanth Gopi, Xin Wang, and Yi Zhang.\nReferences\n[AON+21]\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program\nsynthesis with large language models. arXiv preprint arXiv:2108.07732, 2021.\n[BB21]\nLisa Bauer and Mohit Bansal. Identify, align, and integrate: Matching knowledge graphs\nto commonsense reasoning tasks. arXiv preprint arXiv:2104.10193, 2021.\n[BCE+23]\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz,\nEce Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial\ngeneral intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[BGMMS21] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell.\nOn the dangers of stochastic parrots: Can language models be too big? In Proceedings of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623,\n2021.\n[BHT+19]\nYonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Y\nChai, Mirella Lapata, Angeliki Lazaridou, Ryan J Maynez, Piyush Narang, et al.\nPiqa:\nReasoning about physical commonsense in natural language.\narXiv preprint\narXiv:1911.11641, 2019.\n[CKB+21]\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\n13\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\n[CLC+19]\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\nIn Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\nPapers), pages 2924\u20132936, 2019.\n[CND+22]\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\net al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311,\n2022.\n[CTJ+21]\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Eval-\nuating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021.\n[Dao23]\nTri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.\n2023.\n[DFE+22]\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. Flashattention: Fast\nand memory-efficient exact attention with io-awareness. Advances in Neural Information\nProcessing Systems, 35:16344\u201316359, 2022.\n[EL23]\nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still\nspeak coherent english? arXiv preprint arXiv:2305.07759, 2023.\n[Fer21]\nS\u00b4ebastien Ferr\u00b4e. First steps of an approach to the arc challenge based on descriptive grid\nmodels and the minimum description length principle. arXiv preprint arXiv:2112.00848,\n2021.\n[GTB+21]\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\nLaurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria\nReynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework\nfor few-shot language model evaluation, September 2021.\n[GZA+23]\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00b4esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Gustavo de Rosa Piero Kauffmann, Olli Saarikivia,\nAdil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, S\u00b4ebastien Bubeck, Ronen Eldan,\nAdam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you need. arXiv\npreprint arXiv:2306.11644, 2023.\n[HBB+20]\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint\narXiv:2009.03300, 2020.\n[HGP+22]\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece\nKamar. Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate\nspeech detection. arXiv preprint arXiv:2203.09509, 2022.\n14\n[HPA23]\nSaghar Hosseini, Hamid Palangi, and Ahmed Hassan Awadallah. An empirical study of\nmetrics to measure representational harms in pre-trained language models. arXiv preprint\narXiv:2301.09211, 2023.\n[KLA+22]\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u02dcnoz\nFerrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack:\n3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022.\n[MCKS18]\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity?\na new dataset for open book question answering.\narXiv preprint\narXiv:1809.02789, 2018.\n[NPH+22]\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio\nSavarese, and Caiming Xiong. Codegen: An open large language model for code with\nmulti-turn program synthesis. arXiv preprint, 2022.\n[Ope23]\nOpenAI. Gpt-4 technical report, 2023. arXiv preprint arXiv:2303.08774 [cs.CL].\n[PMH+23]\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and\nweb data only. arXiv preprint arXiv:2306.01116, 2023.\n[PRR19]\nGeorge-Sebastian P\u02c6\u0131rtoac\u02d8a, Traian Rebedea, and Stefan Ruseti. Answering questions by\nlearning to rank. arXiv preprint arXiv:1909.00596, 2019.\n[RRRH20]\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He.\nZero: Memory\noptimizations toward training trillion parameter models, 2020.\n[RZLL16]\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+\nquestions for machine comprehension of text. arXiv preprint arXiv:1606.05250, 2016.\n[SBBC21]\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande:\nAn adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013\n106, 2021.\n[SLBBC19]\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande:\nAn adversarial winograd schema challenge at scale. arXiv preprint arXiv:1907.10641, 2019.\n[SLY+23]\nMichael Santacroce, Yadong Lu, Han Yu, Yuanzhi Li, and Yelong Shen. Efficient rlhf:\nReducing the memory usage of ppo, 2023.\n[TLI+23]\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\n[VSP+17]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  L ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in\nNeural Information Processing Systems, volume 30, 2017.\n15\n[WUR+22]\nLaura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John\nMellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of\nrisks posed by language models. In Proceedings of the 2022 ACM Conference on Fairness,\nAccountability, and Transparency, pages 214\u2013229, 2022.\n[WWS+22]\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[ZCS+23]\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685, 2023.\n[ZHB+19]\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages 4791\u20134800, 2019.\n16\n"
  },
  {
    "title": "NExT-GPT: Any-to-Any Multimodal LLM",
    "link": "https://arxiv.org/pdf/2309.05519.pdf",
    "upvote": "72",
    "text": "NExT-GPT: Any-to-Any Multimodal LLM\nShengqiong Wu\nHao Fei\u2217\nLeigang Qu\nWei Ji\nTat-Seng Chua\nNExT++, School of Computing, National University of Singapore\nProject: https://next-gpt.github.io/\nLLM\nAudio \nDiffusion\nVideo \nDiffusion\nLLM-based Semantic \nUnderstanding\nInstruction-following \nAlignment\nImage \nDiffusion \nText\nImage\nAudio\nVideo\nImage Output \nProjection\nAudio Output \nProjection\nVideo Output \nProjection\nImage Input \nProjection\nImage \nEncoder\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\n...\nMore modalities\n...\nMultimodal Input \nEncoding \nMultimodal Output \nGeneration\nLLM-centric \nAlignment\nFigure 1: By connecting LLM with multimodal adaptors and diffusion decoders, NExT-GPT achieves\nuniversal multimodal understanding and any-to-any modality input and output.\nAbstract\nWhile recently Multimodal Large Language Models (MM-LLMs) have made\nexciting strides, they mostly fall prey to the limitation of only input-side multimodal\nunderstanding, without the ability to produce content in multiple modalities. As\nwe humans always perceive the world and communicate with people through\nvarious modalities, developing any-to-any MM-LLMs capable of accepting and\ndelivering content in any modality becomes essential to human-level AI. To fill\nthe gap, we present an end-to-end general-purpose any-to-any MM-LLM system,\nNExT-GPT. We connect an LLM with multimodal adaptors and different diffusion\ndecoders, enabling NExT-GPT to perceive inputs and generate outputs in arbitrary\ncombinations of text, images, videos, and audio. By leveraging the existing\nwell-trained highly-performing encoders and decoders, NExT-GPT is tuned with\nonly a small amount of parameter (1%) of certain projection layers, which not\nonly benefits low-cost training and also facilitates convenient expansion to more\npotential modalities. Moreover, we introduce a modality-switching instruction\ntuning (MosIT) and manually curate a high-quality dataset for MosIT, based on\nwhich NExT-GPT is empowered with complex cross-modal semantic understanding\nand content generation. Overall, our research showcases the promising possibility\nof building a unified AI agent capable of modeling universal modalities, paving the\nway for more human-like AI research in the community.\n\u2217Hao Fei is the corresponding author: haofei37@nus.edu.sg\nPreprint, work in progress.\narXiv:2309.05519v2  [cs.AI]  13 Sep 2023\n1\nIntroduction\nRecently, the topic of Artificial Intelligence Generated Content (AIGC) has witnessed unprecedented\nadvancements with certain technologies, such as ChatGPT for text generation [59] and diffusion\nmodels for visual generation [21]. Among these, the rise of Large Language Models (LLMs) has been\nparticularly remarkable, e.g., Flan-T5 [13], Vicuna [12], LLaMA [80] and Alpaca [79], showcasing\ntheir formidable human-level language reasoning and decision-making capabilities, shining a light on\nthe path of Artificial General Intelligence (AGI). Our world is inherently multimodal, and humans\nperceive the world with different sensory organs for varied modal information, such as language,\nimages, videos, and sounds, which often complement and synergize with each other. With such\nintuition, the purely text-based LLMs have recently been endowed with other modal understanding\nand perception capabilities of visual, video, audio, etc.\nA notable approach involves employing adapters that align pre-trained encoders in other modalities\nto textual LLMs. This endeavor has led to the rapid development of multimodal LLMs (MM-LLMs),\nsuch as BLIP-2 [43], Flamingo [1], MiniGPT-4 [109], Video-LLaMA [103], LLaVA [52], PandaGPT\n[77], SpeechGPT [102]. Nevertheless, most of these efforts pay the attention to the multimodal\ncontent understanding at the input side, lacking the ability to output content in multiple modalities\nmore than texts. We emphasize that real human cognition and communication indispensably require\nseamless transitions between any modalities of information. This makes the exploration of any-to-any\nMM-LLMs critical to achieving real AGI, i.e., accepting inputs in any modality and delivering\nresponses in the appropriate form of any modality.\nCertain efforts have been made to mimic the human-like any-to-any modality conversion. Lately, CoDi\n[78] has made strides in implementing the capability of simultaneously processing and generating\narbitrary combinations of modalities, while it lacks the reasoning and decision-making prowess of\nLLMs as its core, and also is limited to the simple paired content generation. On the other hand,\nsome efforts, e.g., Visual-ChatGPT [88] and HuggingGPT [72] have sought to combine LLMs with\nexternal tools to achieve approximately the \u2018any-to-any\u2019 multimodal understanding and generation.\nUnfortunately, these systems suffer from critical challenges due to the complete pipeline architecture.\nFirst, the information transfer between different modules is entirely based on discrete texts produced\nby the LLM, where the cascade process inevitably introduces noise and propagates errors. More\ncritically, the entire system only leverages existing pre-trained tools for inference only. Due to the\nlack of overall end-to-end training in error propagation, the capabilities of content understanding\nand multimodal generation can be very limited, especially in interpreting intricate and implicit user\ninstructions. In a nutshell, there is a compelling need for constructing an end-to-end MM-LLM of\narbitrary modalities.\nIn pursuit of this goal, we present NExT-GPT, an any-to-any MM-LLM designed to seamlessly\nhandle input and output in any combination of four modalities: text, images, videos, and audio. As\ndepicted in Figure 1, NExT-GPT comprises three tiers. First, we leverage established encoders to\nencode inputs in various modalities, where these representations are projected into language-like\nrepresentations comprehensible to the LLM through a projection layer. Second, we harness an\nexisting open-sourced LLM as the core to process input information for semantic understanding and\nreasoning. The LLM not only directly generates text tokens but also produces unique \u201cmodality\nsignal\u201d tokens that serve as instructions to dictate the decoding layers whether & what modal content\nto output correspondingly. Third, the produced multimodal signals with specific instructions, after\nprojection, route to different encoders and finally generate content in corresponding modalities.\nAs NExT-GPT encompasses encoding and generation of various modalities, training the system\nfrom scratch would entail substantial costs. Instead, we take advantage of the existing pre-trained\nhigh-performance encoders and decoders, such as Q-Former [43], ImageBind [25] and the state-\nof-the-art latent diffusion models [68, 69, 8, 2, 51, 33]. By loading the off-the-shelf parameters,\nwe not only avoid cold-start training but also facilitate the potential growth of more modalities.\nFor the feature alignment across the three tiers, we consider fine-tuning locally only the input\nprojection and output projection layers, with an encoding-side LLM-centric alignment and decoding-\nside instruction-following alignment, where the minimal computational overhead ensures higher\nefficiency. Furthermore, to empower our any-to-any MM-LLM with human-level capabilities in\ncomplex cross-modal generation and reasoning, we introduce a modality-switching instruction tuning\n(termed Mosit), equipping the system with sophisticated cross-modal semantic understanding and\ncontent generation. To combat the absence of such cross-modal instruction tuning data in the\ncommunity, we manually collect and annotate a Mosit dataset consisting of 5,000 samples of high\n2\nquality. Employing the LoRA technique [32], we fine-tune the overall NExT-GPT system on MosIT\ndata, updating the projection layers and certain LLM parameters.\nOverall, this work showcases the promising possibility of developing a more human-like MM-LLM\nagent capable of modeling universal modalities. The contributions of this project are as follows:\n\u2022 We for the first time present an end-to-end general-purpose any-to-any MM-LLM, NExT-\nGPT, capable of semantic understanding and reasoning and generation of free input and\noutput combinations of text, images, videos, and audio.\n\u2022 We introduce lightweight alignment learning techniques, the LLM-centric alignment at\nthe encoding side, and the instruction-following alignment at the decoding side, efficiently\nrequiring minimal parameter adjustments (only 1% params) for effective semantic alignment.\n\u2022 We annotate a high-quality modality-switching instruction tuning dataset covering intricate\ninstructions across various modal combinations of text, images, videos, and audio, aiding\nMM-LLM with human-like cross-modal content understanding and instruction reasoning.\n2\nRelated Work\nCross-modal Understanding and Generation\nOur world is replete with multimodal information,\nwherein we continuously engage in the intricate task of comprehending and producing cross-modal\ncontent. The AI community correspondingly emerges varied forms of cross-modal learning tasks,\nsuch as Image/Video Captioning [99, 16, 56, 56, 27, 49], Image/Video Question Answering [94,\n90, 48, 98, 3], Text-to-Image/Video/Speech Synthesis [74, 30, 84, 23, 17, 51, 33], Image-to-Video\nSynthesis [18, 37] and more, all of which have experienced rapid advancements in past decades.\nResearchers have proposed highly effective multimodal encoders, with the aim of constructing\nunified representations encompassing various modalities. Meanwhile, owing to the distinct feature\nspaces of different modalities, it is essential to undertake modality alignment learning. Moreover, to\ngenerate high-quality content, a multitude of strong-performing methods have been proposed, such\nas Transformer [82, 101, 17, 24], GANs [53, 7, 93, 110], VAEs [81, 67], Flow models [73, 6] and the\ncurrent state-of-the-art diffusion models [31, 64, 57, 22, 68]. Especially, the diffusion-based methods\nhave recently delivered remarkable performance in a plethora of cross-modal generation tasks, such as\nDALL-E [66], Stable Diffusion [68]. While all previous efforts of cross-modal learning are limited to\nthe comprehension of multimodal inputs only, CoDi [78] lately presents groundbreaking development.\nLeveraging the power of diffusion models, CoDi possesses the ability to generate any combination\nof output modalities, including language, images, videos, or audio, from any combination of input\nmodalities in parallel. Regrettably, CoDi might still fall short of achieving human-like deep reasoning\nof input content, with only parallel cross-modal feeding&generation.\nMultimodal Large Language Models\nLLMs have already made profound impacts and revolutions\non the entire AI community and beyond. The most notable LLMs, i.e., OpenAI\u2019s ChatGPT [59] and\nGPT4 [60], with alignment techniques such as instruction tuning [61, 47, 104, 52] and reinforcement\nlearning from human feedback (RLHF) [75], have demonstrated remarkable language understanding\nand reasoning abilities. And a series of open-source LLMs, e.g., Flan-T5 [13], Vicuna [12], LLaMA\n[80] and Alpaca [79], have greatly spurred advancement and made contributions to the community\n[109, 100]. Afterward, significant efforts have been made to construct LLMs dealing with multimodal\ninputs and tasks, leading to the development of MM-LLMs.\nOn the one hand, most of the researchers build fundamental MM-LLMs by aligning the well-trained\nencoders of various modalities to the textual feature space of LLMs, so as to let LLMs perceive other\nmodal inputs [35, 109, 76, 40]. For example, Flamingo [1] uses a cross-attention layer to connect a\nfrozen image encoder to the LLMs. BLIP-2 [43] employs a Q-Former to translate the input image\nqueries to the LLMs. LLaVA [52] employs a simple projection scheme to connect image features into\nthe word embedding space. There are also various similar practices for building MM-LLMs that are\nable to understand videos (e.g., Video-Chat [44] and Video-LLaMA [103]), audios (e.g., SpeechGPT\n[102]), etc. Profoundly, PandaGPT [77] achieves a comprehensive understanding of six different\nmodalities simultaneously by integrating the multimodal encoder, i.e., ImageBind [25].\nNevertheless, these MM-LLMs all are subject to the limitation of only perceiving multimodal data,\nwithout generating content in arbitrary modalities. To achieve LLMs with both multimodal input\nand output, some thus explore employing LLMs as decision-makers, and utilizing existing off-the-\nshelf multimodal encoders and decoders as tools to execute multimodal input and output, such\nas Visual-ChatGPT [88], HuggingGPT [72], and AudioGPT [34]. As aforementioned, passing\n3\nLLM\nImage Input \nProjection\nImage \nEncoder\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nAbsolutely! There are so \nmany activities to enjoy \nby the sea, like beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\n Vlog content can be \nquite diverse. Here's a \nreference \nImg. Sig. Tok. Rep.\nVid. Sig. Tok. Rep.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nFor the cover, it should \ndefinitely feature you \nsurfing. As for the \nmusic, I recommend \nsomething lively, like \nthis?\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\nAbsolutely! There are so \nmany activities to enjoy \nby the sea, like beach \nvolleyball.\nAbsolutely! There are so \nmany activities to enjoy \nby the sea, like beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\n Vlog content can be \nquite diverse. Here's a \nreference \nCreating a vlog requires a cover and \nmusic. What do you think would be \na good style?\nImg. Sig. Tok. Rep.\nAud. Sig. Tok. Rep.\nLLM\nImage Input \nProjection\nImage \nEncoder\nImage \nDiffusion \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nAudio \nDiffusion\nVideo \nDiffusion\nAudio Output \nProjection\nVideo Output \nProjection\nOf course! To begin learning surfing, \nyou should first find a good surfing \ninstructor to learn basic skills and \nsafety knowledge. Additionally ...\nAbsolutely! There are so \nmany activities to enjoy \nby the sea, like beach \nvolleyball.\n But I'm really interested \nin trying out surfing. I \nthink it's super cool. It \nwould be even better if I \ncould create a vlog to \nshowcase my progress.\nI am so into summer, especially the \nsea; I hope I can go to the seaside \nto have some fun.\n Vlog content can be \nquite diverse. Here's a \nreference \nCreating a vlog requires a cover and \nmusic. What do you think would be \na good style?\nFor the cover, it should \ndefinitely feature you \nsurfing. As for the \nmusic, I recommend \nsomething lively, like \nthis?\nCan you provide me with some  \nlearning tips? I can't wait to start \nlearning.\nFigure 2: NExT-GPT inference process. Grey colors denote the deactivation of the modules.\nmessages between modules with pure texts (i.e., LLM textual instruction) under the discrete pipeline\nscheme will inevitably introduce noises. Also lacking comprehensive tuning across the whole system\nsignificantly limits the efficacy of semantics understanding. Our work takes the mutual benefits of\nboth the above two types, i.e., learning an any-to-any MM-LLM in an end-to-end manner.\n3\nOverall Architecture\nFigure 1 presents the schematic overview of the framework. NExT-GPT consists of three main tiers:\nthe encoding stage, the LLM understanding and reasoning stage, and the decoding stage.\nMultimodal Encoding Stage\nFirst, we leverage existing well-established models to encode inputs\nof various modalities. There are a set of alternatives of encoders for different modalities, e.g., Q-\nFormer [43], ViT [19], CLIP [65]. Here we take advantage of the ImageBind [25], which is a unified\nhigh-performance encoder across six modalities. With ImageBind, we are spared from managing\n4\nEncoder\nInput Projection\nLLM\nOutput Projection\nDiffusion\nName\nParam Name\nParam\nName\nParam\nName\nParam\nName\nParam\nText\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\n\u2014\nImage\nVicuna [12]\n7B\nTransformer 31M\nSD [68]\n1.3B\nAudio\n(LoRA\n33M\n) Transformer 31M\nAudioLDM [51] 975M\nVideo\nImageBind [25] 1.2B\nLinear\n4M\nTransformer 32M\nZeroscope [8]\n1.8B\nTable 1: Summary of system configuration. Only 1% parameters need updating.\nmany numbers of heterogeneous modal encoders. Then, via the linear projection layer, different input\nrepresentations are mapped into language-like representations that are comprehensible to the LLM.\nLLM Understanding and Reasoning Stage\nAn LLM is used as the core agent of NExT-GPT.\nTechnically, we employ the Vicuna2 [12], which is the open-source text-based LLM that is widely\nused in the existing MM-LLMs [77, 103]. LLM takes as input the representations from different\nmodalities and carries out semantic understanding and reasoning over the inputs. It outputs 1) the\ntextual responses directly, and 2) signal tokens of each modality that serve as instructions to dictate\nthe decoding layers whether to generate multimodal contents, and what content to produce if yes.\nMultimodal Generation Stage\nReceiving the multimodal signals with specific instructions from\nLLM (if any), the Transformer-based output projection layers map the signal token representations\ninto the ones that are understandable to following multimodal decoders. Technically, we employ the\ncurrent off-the-shelf latent conditioned diffusion models of different modal generations, i.e., Stable\nDiffusion (SD)3 for image synthesis [68], Zeroscope4 for video synthesis [8], and AudioLDM5 for\naudio synthesis [51]. The signal representations are fed into the condition encoders of the conditioned\ndiffusion models for content generation.\nIn Table 1 we summarize the overall system configurations. It is noteworthy that in the entire\nsystem, only the input and output projection layers of lower-scale parameters (compared with\nthe overall huge capacity framework) are required to be updated during the following learning,\nwith all the rest encoders and decoders frozen. That is, 131M(=4+33+31+31+32) / [131M +\n12.275B(=1.2+7+1.3+1.8+0.975)], only 1% parameters are to be updated. This is also one of\nthe key advantages of our MM-LLM.\nIn Figure 2 we further illustrate the inference procedure of NExT-GPT. Given certain user inputs of\nany combination of modalities, the corresponding modal encoders, and projectors transform them\ninto feature representations and pass them to LLM6. Then, LLM decides what content to generate,\ni.e., textual tokens, and modality signal tokens. If LLM identifies a certain modality content (except\nlanguage) to be produced, a special type of token [40] will be output indicating the activation of\nthat modality; otherwise, no special token output means deactivation of that modality. Technically,\nwe design the \u2018<IMGi>\u2019 (i = 0, \u00b7 \u00b7 \u00b7 , 4) as image signal tokens; \u2018<AUDi>\u2019 (i = 0, \u00b7 \u00b7 \u00b7 , 8) as audio\nsignal tokens; and \u2018<VIDi>\u2019 (i = 0, \u00b7 \u00b7 \u00b7 , 24) as video signal tokens. After LLM, the text responses\nare output to the user; while the representations of the signal tokens of certain activated modalities\nare passed to the corresponding diffusion decoders for content generation.\n4\nLightweight Multimodal Alignment Learning\nTo bridge the gap between the feature space of different modalities, and ensure fluent semantics\nunderstanding of different inputs, it is essential to perform alignment learning for NExT-GPT. Since\nwe design the loosely-coupled system with mainly three tiers, we only need to update the two\nprojection layers at the encoding side and decoding side.\n4.1\nEncoding-side LLM-centric Multimodal Alignment\nFollowing the common practice of existing MM-LLMs, we consider aligning different inputting\nmultimodal features with the text feature space, the representations that are understandable to the core\n2https://huggingface.co/lmsys/vicuna-7b-delta-v0, 7B, version 0\n3https://huggingface.co/runwayml/stable-diffusion-v1-5, version 1.5.\n4https://huggingface.co/cerspense/zeroscope_v2_576w, version zeroscope_v2_576w.\n5https://audioldm.github.io/, version audioldm-l-full.\n6Except the text inputs, which will be directly fed into LLM.\n5\nImage\nLLM\nAudio\nVideo\nImage \nCaption\nAudio \nCaption\nVideo \nCaption\nError Back-prop.\nAudio \nEncoder\nVideo \nEncoder\nImage Input \nProjection\nImage \nEncoder\nAudio Input \nProjection\nVideo Input \nProjection\nLLM\nText Encoder \n(in Aud. Diff.)\nText Encoder \n(in Vid. Diff.)\nText Encoder \n(in Img. Diff.) \nImage Output \nProjection\nAudio Output \nProjection\nVideo Output \nProjection\nImage \nCaption\nAudio \nCaption\nVideo \nCaption\nImage Signal Token\nText Response\nAudio signal token\nText Response\nVideo signal token\nText Response\n`\nMin. Eucli. Dist.\nVid. Rep.\nAligned Vid. Rep.\nLLM Output Rep.\n\u2026\n\u2026\n(a) Encoding-side LLM-centric Alignment\n(b) Decoding-side Instruction-following Alignment\nAligned Aud. Rep.\nAligned Img. Rep.\nAud. Rep.\nImg. Rep.\n\u2026\n\u2026\nFigure 3: Illustration of the lightweight multimodal alignment learning of encoding and decoding.\nLLM. This is thus intuitively named the LLM-centric multimodal alignment learning. To accomplish\nthe alignment, we prepare the \u2018X-caption\u2019 pair (\u2018X\u2019 stands for image, audio, or video) data from\nexisting corpus and benchmarks. We enforce LLM to produce the caption of each input modality\nagainst the gold caption. Figure 3(a) illustrates the learning process.\n4.2\nDecoding-side Instruction-following Alignment\nOn the decoding end, we have integrated pre-trained conditional diffusion models from external\nresources. Our main purpose is to align the diffusion models with LLM\u2019s output instructions.\nHowever, performing a full-scale alignment process between each diffusion model and the LLM\nwould entail a significant computational burden. Alternatively, we here explore a more efficient\napproach, decoding-side instruction-following alignment, as depicted in Figure 3(b). Specifically,\nsince diffusion models of various modalities are conditioned solely on textual token inputs. This\nconditioning diverges significantly from the modal signal tokens from LLM in our system, which\nleads to a gap in the diffusion models\u2019 accurate interpretation of the instructions from LLM. Thus, we\nconsider minimizing the distance between the LLM\u2019s modal signal token representations (after each\nTransformer-based project layer) and the conditional text representations of the diffusion models.\nSince only the textual condition encoders are used (with the diffusion backbone frozen), the learning\nis merely based on the purely captioning texts, i.e., without any visual or audio resources. This also\nensures highly lightweight training.\n5\nModality-switching Instruction Tuning\n5.1\nInstruction Tuning\nDespite aligning both the encoding and decoding ends with LLM, there remains a gap towards\nthe goal of enabling the overall system to faithfully follow and understand users\u2019 instructions and\ngenerate desired multimodal outputs. To address this, further instruction tuning (IT) [97, 77, 52]\nis deemed necessary to enhance the capabilities and controllability of LLM. IT involves additional\ntraining of overall MM-LLMs using \u2018(INPUT, OUTPUT)\u2019 pairs, where \u2018INPUT\u2019 represents the user\u2019s\ninstruction, and \u2018OUTPUT\u2019 signifies the desired model output that conforms to the given instruction.\nTechnically, we leverage LoRA [32] to enable a small subset of parameters within NExT-GPT to be\nupdated concurrently with two layers of projection during the IT phase. As illustrated in Figure 4,\nwhen an IT dialogue sample is fed into the system, the LLM reconstructs and generates the textual\n6\nText\nLLM\nLoRA\nImage Input \nProjection\nImage \nEncoder\nText Encoder \n(in Img. Diff.) \nImage Output \nProjection\nAudio Input \nProjection\nAudio \nEncoder\nVideo \nEncoder\nVideo Input \nProjection\nText Encoder \n(in Aud. Diff.) \nText Encoder \n(in Vid. Diff.) \nAudio Output \nProjection\nVideo Output \nProjection\nText +\nText\nText +\nText\nText +\nText\nText\n+\nText\nText\nText\nText\nText\nText\nText\nText\n<IMGk> ... \n<AUDn>...\n<VIDm> ... \n<IMGk>... \n\u2026\nImg. Sig. \nTok. Rep.\nAud. Sig. \nTok. Rep.\nVid. Sig. \nTok. Rep.\n\u2026\nMin. Eucli. Dist.\nImage Caption1\nAudio Caption\nVideo Caption\nImage Caption2\nInput Instructions\nCross \nEncropy\n\u2026\n\u2026\n\u2026\nLLM Output\nGold Annotation\nText\nText\nText\nText\nText\nText\nText\nText\n<IMGk> ... \n<AUDn>...\n<VIDm> ... \n<IMGk>... \nGold Annotation\nFigure 4: Illustration of modality-switching instruction tuning.\ncontent of input (and represents the multimodal content with the multimodal signal tokens). The\noptimization is imposed based on gold annotations and LLM\u2019s outputs. In addition to the LLM tuning,\nwe also fine-tune the decoding end of NExT-GPT. We align the modal signal token representation\nencoded by the output projection with the gold multimodal caption representation encoded by the\ndiffusion condition encoder. Thereby, the comprehensive tuning process brings closer to the goal of\nfaithful and effective interaction with users.\n5.2\nInstruction Dataset\nFor the IT of NExT-GPT, we consider the following datasets.\n\u2018Text+X\u2019 \u2014 \u2018Text\u2019 Data\nThe commonly used datasets for MM-LLM IT entail inputs of both\ntexts and multimodal contents (i.e., \u2018X\u2019 could be the image, video, audio, or others), and the outputs\nare textual responses from LLM. There are well-established data of this type, e.g., LLaVA [52],\nminiGPT-4 [109], VideoChat [44], where we directly employ them for our tuning purpose.\n\u2018Text\u2019 \u2014 \u2018Text+X\u2019 Data\nSignificantly unlike existing MM-LLMs, in our any-to-any scenario, the\ntarget not only includes the generations of texts, but also the multimodal contents, i.e., \u2018Text+X\u2019.\nThus, we construct the \u2018Text\u2019 \u2014 \u2018Text+X\u2019 data, i.e., text-to-multimodal (namely T2M) data. Based\non the rich volume of \u2018X-caption\u2019 pairs from the existing corpus and benchmarks [71, 50, 5, 38], with\nsome templates, we borrow GPT-4 to produce varied textual instructions to wrap the captions, and\nresult in the data.\nMosIT Data\nCrafting high-quality instructions that comprehensively cover the desired target be-\nhaviors is non-trivial. We notice that the above IT datasets fail to meet the requirements for our\nany-to-any MM-LLM scenario. Firstly, during a human-machine interaction, users and LLM involve\ndiverse and dynamically changing modalities in their inputs and outputs. Additionally, we allow\nmulti-turn conversations in the process, and thus processing and understanding of complex user\nintentions is required. However, the above two types of data lack variable modalities, and also are\nrelatively short in dialogues, failing to mimic real-world scenarios adequately.\nTo facilitate the development of any-to-any MM-LLM, we propose a novel Modality-switching\nInstruction Tuning (MosIT). MosIT not only supports complex cross-modal understanding and\nreasoning but also enables sophisticated multimodal content generation. In conjunction with MosIT,\nwe manually and meticulously construct a high-quality dataset. The MosIT data encompasses a wide\nrange of multimodal inputs and outputs, offering the necessary complexity and variability to facilitate\nthe training of MM-LLMs that can handle diverse user interactions and deliver desired responses\naccurately. Specifically, we design some template dialogue examples between a \u2018Human\u2019 role and a\n\u2018Machine\u2019 role, based on which we prompt the GPT-4 to generate more conversations under various\nscenarios with more than 100 topics or keywords. The interactions are required to be diversified,\ne.g., including both straightforward and implicit requirements by the \u2018Human\u2019, and execution of\nperception, reasoning, suggestion, planning, etc., by the \u2018Machine\u2019. And the interactive content\nshould be logically connected and semantically inherent and complex, with in-depth reasoning details\nin each response by the \u2018Machine\u2019. Each conversation should include 3-7 turns (i.e., QA pairs), where\nthe \u2018Human\u2019-\u2018Machine\u2019 interactions should involve multiple modalities at either the input or output\nside, and switch the modalities alternately. Whenever containing multimodal contents (e.g., image,\naudio, and video) in the conversations, we look for the best-matched contents from the external\nresources, including the retrieval systems, e.g., Youtube7, and even AIGC tools, e.g., Stable-XL [63],\n7https://www.youtube.com/\n7\nDataset\nData Source\nIn\u2192Out Modality\nApproach\nMulti-turn Reason\n#Img/Vid/Aud\n#Dialog Turn.\n#Instance\n\u25b6 Existing data\nMiniGPT-4 [109]\nCC [10], CC3M [71]\nT+I\u2192T\nAuto\n\u2717\n134M/-/-\n1\n5K\nStableLLaVA [47]\nSD [68]\nT+I\u2192T\nAuto+Manu.\n\u2717\n126K/-/-\n1\n126K\nLLaVA [104]\nCOCO [50]\nT+I\u2192T\nAuto\n\u2713\n81K/-/-\n2.29\n150K\nSVIT [106]\nMS-COCO [50], VG [41]\nT+I\u2192T\nAuto\n\u2713\n108K/-/-\n5\n3.2M\nLLaVAR [104]\nCOCO [50], CC3M [71], LAION [70]\nT+I\u2192T\nLLaVA+Auto\n\u2713\n20K/-/-\n2.27\n174K\nVideoChat [44]\nWebVid [5]\nT+V\u2192T\nAuto\n\u2713\n-/8K/-\n1.82\n11K\nVideo-ChatGPT [54]\nActivityNet [28]\nT+V\u2192T\nInherit\n\u2717\n-/100K/-\n1\n100K\nVideo-LLaMA [103]\nMiniGPT-4, LLaVA, VideoChat\nT+I/V\u2192T\nAuto\n\u2713\n81K/8K/-\n2.22\n171K\nInstructBLIP [15]\nMultiple\nT+I/V\u2192T\nAuto\n\u2717\n-\n-\n\u223c 1.6M\nMIMIC-IT [42]\nMultiple\nT+I/V\u2192T\nAuto\n\u2717\n8.1M/502K/-\n1\n2.8M\nPandaGPT [77]\nMiniGPT-4, LLaVA\nT+I\u2192T\nInherit\n\u2713\n81K/-/-\n2.29\n160K\nMGVLID [107]\nMultiple\nT+I+B\u2192T\nAuto+Manu.\n\u2717\n108K/-/-\n-\n108K\nM3IT [45]\nMultiple\nT+I/V/B\u2192T\nAuto+Manu.\n\u2717\n-/-/-\n1\n2.4M\nLAMM [97]\nMultiple\nT+I+PC\u2192T\nAuto+Manu.\n\u2713\n91K/-/-\n3.27\n196k\nBuboGPT [108]\nClotho [20], VGGSS [11]\nT+A/(I+A)\u2192T\nAuto\n\u2717\n5k/-/9K\n-\n9K\nmPLUG-DocOwl [96]\nMultiple\nT+I/Tab/Web\u2192T\nInherit\n\u2717\n-\n-\n-\n\u25b6 In this work\nT2M\nWebvid [5], CC3M [71], AudioCap [38]\nT\u2192T+I/A/V\nAuto\n\u2717\n4.9K/4.9K/4.9K\n1\n14.7K\nMosIT\nYoutube, Google, Flickr, Midjourney, etc.\nT+I+A+V\u2192T+I+A+V\nAuto+Manu.\n\u2713\n4K/4K/4K\n4.8\n5K\nTable 2: Summary and comparison of existing datasets for multimodal instruction tuning. T: text, I: image, V: video, A: audio, B: bounding box, PC: point cloud, Tab:\ntable, Web: web page.\n8\nMethod\nFID (\u2193)\nCogVideo [17]\n27.10\nGLIDE [58]\n12.24\nCoDi [78]\n11.26\nSD [68]\n11.21\nNExT-GPT\n11.28\nTable 3:\nText-to-image\ngeneration\nresults\non\nCOCO-caption data [50].\nMethod\nFD (\u2193)\nIS (\u2191)\nDiffSound [95]\n47.68\n4.01\nAudioLDM-S [51]\n29.48\n6.90\nAudioLDM-L [51]\n23.31\n8.13\nCoDi [78]\n22.90\n8.77\nNExT-GPT\n23.58\n8.35\nTable 4: Text-to-audio genera-\ntion results on AudioCaps [38].\nMethod\nFID (\u2193) CLIPSIM (\u2191)\nCogVideo [30]\n23.59\n0.2631\nMakeVideo [74]\n13.17\n0.3049\nLatent-VDM [68]\n14.25\n0.2756\nLatent-Shift [2]\n15.23\n0.2773\nCoDi [78]\n\u2014\n0.2890\nNExT-GPT\n13.04\n0.3085\nTable 5: Text-to-video generation re-\nsults (zero-shot) on MSR-VTT [92].\nMethod\nB@4\nMETEOR\nCIDEr\nOscar [46]\n36.58\n30.4\n124.12\nBLIP-2 [43]\n43.7\n\u2014\n145.8\nOFA [86]\n44.9\n32.5\n154.9\nCoDi [78]\n40.2\n31.0\n149.9\nNExT-GPT\n44.3\n32.9\n156.7\nTable 6:\nImage-to-text genera-\ntion (image captioning) results on\nCOCO-caption data [50].\nMethod\nSPIDEr CIDEr\nAudioCaps [38]\n0.369\n0.593\nBART [26]\n0.465\n0.753\nAL-MixGen [39]\n0.466\n0.755\nCoDi [78]\n0.480\n0.789\nNExT-GPT\n0.521\n0.802\nTable 7: Audio-to-text genera-\ntion (audio captioning) results\non AudioCaps [38].\nMethod\nB@4 METEOR\nORG-TRL [105]\n43.6\n28.8\nGIT [85]\n54.8\n33.1\nmPLUG-2 [91]\n57.8\n34.9\nCoDi [78]\n52.1\n32.5\nNExT-GPT\n58.4\n38.5\nTable 8: Video-to-text genera-\ntion (video captioning) results\non MSR-VTT [92].\nMethod\nObject\nBackground\nCLIP (\u2191)FID (\u2193)CLIP (\u2191)FID (\u2193)\nPTP [29]\n30.33\n9.58\n31.55\n13.92\nBLDM [4]\n29.95\n6.14\n30.38\n20.44\nDiffEdit [14]\n29.30\n3.78\n26.92\n1.74\nPFB-Diff [36]\n30.81\n5.93\n32.25\n13.77\nNExT-GPT\n29.31\n6.52\n27.29\n15.20\nTable 9:\nText+image-to-image genera-\ntion (text-conditioned image editing) re-\nsults on COCO data [50].\nMethod\nMCD (\u2193)\nCampNet [87]\n0.380\nMakeAudio [33]\n0.375\nAudioLDM-L [51]\n0.349\nNExT-GPT\n0.302\nTable 10:\nText+audio-\nto-audio generation (text-\nconditioned speech edit-\ning) results on VCTK\ndata [83].\nMethod\nCLIP-T (\u2191) CLIP-I (\u2191)\nCogVideo [30]\n0.2391\n0.9064\nTuneVideo [89]\n0.2758\n0.9240\nSDEdit [55]\n0.2775\n0.8731\nPix2Video [9]\n0.2891\n0.9767\nNExT-GPT\n0.2683\n0.9645\nTable 11: Text+video-to-video\ngeneration\n(text-conditioned\nvideo editing) results on DAVIS\ndata [62].\nMidjourney8. After human inspections and filtering of inappropriate instances, we obtain a total of\n5K dialogues in high quality. In Table 2 we compare the existing multimodal IT datasets with our\nMosIT data.\n6\nExperiments\n6.1\nAny-to-any Multimodal Generation\nWe try to quantify the generation quality of NExT-GPT on certain benchmark datasets under some\ncommon settings, such as text-to-X generation, X-to-text generation, and Text-conditioned modality\nediting. We mimic the task by taking only one turn of interaction between the user and the model.\n\u2022 \u2018Text\u2019 \u2014 \u2018X\u2019 Generation\nrepresents the most frequent tasks of text-conditioned modal synthesis.\nTable 3, 4 and 5 present the comparisons between ours and some state-of-the-art systems. Overall\nNExT-GPT shows nice performance on par with the values from the best-performing baselines.\n\u2022 \u2018X\u2019 \u2014 \u2018Text\u2019 Generation\nrepresents the tasks of modal captioning. Table 6, 7 and 8 show\nthe results on different tasks. Overall, we find that NExT-GPT can mostly achieve much better\nperformance on the X-to-text generation than the CoDi baseline, owing to the direct generation of\ntexts from LLM, which is inherently expertized by the LLM.\n\u2022 \u2018Text+X\u2019 \u2014 \u2018X\u2019 Generation\nrepresents a task category of text-conditioned modal editing. Table\n9, 10 and 11 show the performances on different tasks. Compared with the above two types of tasks,\nNExT-GPT could be not that superior for the text-conditioned modal editing tasks. Yet, it still shows\ncompetitive performance.\n8https://www.midjourney.com/\n9\nT\nI+A\nT\nT+V\nT+I\nV\nT+I\nA\nT+V\nV\nT+V\nI+A\nT+A\nI\nT+A\nI+A\nT+A\nV\nT+A+I\nV\nT+A+I\nI\nT+A+I+V\nI\n0\n2\n4\n6\n8\n10\nPerformance\nFigure 5: Comparative performance of NExT-GPT on various complex cross-modal conversions.\n\u2022 Human Evaluation on Complex Any-to-any QA\nWe also carry out evaluation on some more\nscenarios where there are complicated cross-modal interactions between inputs and outputs. We\nmainly compare the model performance for the settings with different modality conversions. As no\nstandard benchmark can be leveraged, here we adopt human evaluation. We ask several evaluators to\nscore the performance of NExT-GPT on a scale from 1 to 10. Figure 5 shows the comparisons. We\nfind NExT-GPT is more competent in producing images, compared with the generations on videos\nand audio. Also generating mixed combinations of multimodal content is slightly inferior to the\ngeneration of single-modal content, due to the complexity of the latter.\n6.2\nExample Demonstrations\nTo demonstrate the effectiveness and potential of our proposed NExT-GPT in developing human-like\nconversational agents, here we further offer compelling examples that vividly illustrate the system\u2019s\nexceptional capacity to comprehend and reason contents across various modalities in any combination.\nFigure 6, 7, 8, 9, 10 and 11 show the examples from NExT-GPT. Go to the project page for more\nexamples and access the dynamic video and audio contents.\n7\nConclusion\nIn this work, we present an end-to-end general-purpose any-to-any multimodal Large Language Model\n(MM-LLM). By connecting an LLM with multimodal adaptors and different diffusion decoders,\nNExT-GPT is capable of perceiving inputs and generating outputs in any combination of text, images,\nvideos, and audio. Harnessing the existing well-trained highly-performing encoders and decoders,\ntraining NExT-GPT only entails a few number of parameters (1%) of certain projection layers,\nwhich not only benefits low costs but also facilitates convenient expansion to future more potential\nmodalities. To enable our NExT-GPT with complex cross-modal semantic understanding and content\ngeneration, we introduce a modality-switching instruction tuning (MosIT), and manually curated a\nhigh-quality dataset for MosIT. Overall, our research showcases the potential of any-to-any MM-\nLLMs in bridging the gap between various modalities and paving the way for more human-like AI\nsystems in the future.\nLimitation and Future work\nAs future work, there are at least following four avenues to explore.\ni) Modalities & Tasks Expansion: Due to resource limitations, currently, our system supports input\nand output in four modalities: language, images, videos, and audio. Next, we plan to extend this to\naccommodate even more modalities (e.g., web page, 3D vision, heat map, tables&figures) and tasks\n(e.g., object detection, segmentation, grounding and tracking), broadening the system\u2019s applicability\nsuch that it becomes more universal.\nii) LLM Variants: Currently, we have implemented the 7B Vicuna version of the LLM. Our next\nplans involve incorporating various LLM types and sizes, allowing practitioners to choose the most\nsuitable one for their specific requirements.\n10\niii) Multimodal Generation Strategies: While our system excels in generating content across\nmodalities, the quality of generative outputs can sometimes be limited by the capabilities of the\ndiffusion model. It is very promising to explore the integration of retrieval-based approaches to\ncomplement the generative process, potentially improving the overall system\u2019s performance.\niv) MosIT Dataset Expansion: Currently, our IT dataset has room for expansion. We intend to\nsignificantly increase the amount of annotated data, ensuring a more comprehensive and diverse set\nof instructions to further enhance the MM-LLMs\u2019 ability to understand and follow user prompts\neffectively.\nFigure 6: Example of Text+Image \u2192 Text+Audio.\n11\nFigure 7: Example of Text \u2192 Text+Image+Video+Audio.\n12\nFigure 8: Example of Text+Image \u2192 Text+Image+Video+Audio.\n13\nFigure 9: Example of Text+Video \u2192 Text+Image.\n14\nFigure 10: Example of Text+Audio \u2192 Text+Image+Video.\n15\nFigure 11: Example of Text+Video \u2192 Text+Audio.\n16\nReferences\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual language model for few-shot learning. In\nProceedings of the NeurIPS, 2022.\n[2] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift:\nLatent diffusion with temporal shift for efficient text-to-video generation. CoRR, abs/2304.08477, 2023.\n[3] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei\nZhang. Bottom-up and top-down attention for image captioning and visual question answering. In\nProceedings of the CVPR, pages 6077\u20136086, 2018.\n[4] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM Trans. Graph., 42(4):\n149:1\u2013149:11, 2023.\n[5] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval. In Proceedings of the ICCV, pages 1708\u20131718, 2021.\n[6] Mohammad Bashiri, Edgar Y. Walker, Konstantin-Klemens Lurz, Akshay Jagadish, Taliah Muhammad,\nZhiwei Ding, Zhuokun Ding, Andreas S. Tolias, and Fabian H. Sinz. A flow-based latent state generative\nmodel of neural population responses to natural images. In Proceedings of the NeurIPS, pages 15801\u2013\n15815, 2021.\n[7] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity natural\nimage synthesis. In Proceedings of the ICLR, 2019.\n[8] Cerspense. Zeroscope: Diffusion-based text-to-video synthesis. 2023. URL https://huggingface.\nco/cerspense.\n[9] Duygu Ceylan, Chun-Hao Paul Huang, and Niloy J. Mitra. Pix2video: Video editing using image\ndiffusion. CoRR, abs/2303.12688, 2023.\n[10] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale\nimage-text pre-training to recognize long-tail visual concepts. In Proceedings of the CVPR, pages\n3558\u20133568, 2021.\n[11] Honglie Chen, Weidi Xie, Triantafyllos Afouras, Arsha Nagrani, Andrea Vedaldi, and Andrew Zisserman.\nLocalizing visual sounds the hard way. In Proceedings of the CVPR, pages 16867\u201316876, 2021.\n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 902023.\n[13] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.\nZhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models,\n2022.\n[14] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. Diffedit: Diffusion-based\nsemantic image editing with mask guidance. In Proceedings of the ICLR, 2023.\n[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang\nLi, Pascale Fung, and Steven C. H. Hoi. Instructblip: Towards general-purpose vision-language models\nwith instruction tuning. CoRR, abs/2305.06500, 2023.\n[16] Roberto Dess\u00ec, Michele Bevilacqua, Eleonora Gualdoni, Nathana\u00ebl Carraz Rakotonirina, Francesca Fran-\nzon, and Marco Baroni. Cross-domain image captioning with discriminative finetuning. In Proceedings\nof the CVPR, pages 6935\u20136944, 2023.\n[17] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou\nShao, Hongxia Yang, and Jie Tang. Cogview: Mastering text-to-image generation via transformers. In\nProceedings of the NeurIPS, pages 19822\u201319835, 2021.\n17\n[18] Michael Dorkenwald, Timo Milbich, Andreas Blattmann, Robin Rombach, Konstantinos G. Derpanis,\nand Bj\u00f6rn Ommer. Stochastic image-to-video synthesis using cinns. In Proceedings of the CVPR, pages\n3742\u20133753, 2021.\n[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,\nand Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nProceedings of the ICLR, 2021.\n[20] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: an audio captioning dataset. In\nProceedings of the ICASSP, pages 736\u2013740, 2020.\n[21] Wan-Cyuan Fan, Yen-Chun Chen, Dongdong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang.\nFrido: Feature pyramid diffusion for complex scene image synthesis. CoRR, abs/2208.13753, 2022.\n[22] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun R. Akula, Pradyumna Narayana, Sugato Basu,\nXin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional\ntext-to-image synthesis. CoRR, abs/2212.05032, 2022.\n[23] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion.\nCoRR, abs/2208.01618, 2022.\n[24] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi\nParikh. Long video generation with time-agnostic VQGAN and time-sensitive transformer. In Proceedings\nof the ECCV, pages 102\u2013118, 2022.\n[25] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. Imagebind: One embedding space to bind them all. CoRR, abs/2305.05665, 2023.\n[26] F\u00e9lix Gontier, Romain Serizel, and Christophe Cerisara. Automated audio captioning by fine-tuning\nBART with audioset tags. In Proceedings of the DCASE, pages 170\u2013174, 2021.\n[27] Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. Text with knowledge\ngraph augmented transformer for video captioning. In Proceedings of the CVPR, pages 18941\u201318951,\n2023.\n[28] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A\nlarge-scale video benchmark for human activity understanding. In Proceedings of the CVPR, pages\n961\u2013970, 2015.\n[29] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-\nprompt image editing with cross-attention control. In Proceedings of the ICLR, 2023.\n[30] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining\nfor text-to-video generation via transformers. CoRR, abs/2205.15868, 2022.\n[31] Emiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling. Argmax flows and\nmultinomial diffusion: Towards non-autoregressive language models. CoRR, 2021.\n[32] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In Proceedings of the ICLR, 2022.\n[33] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu,\nXiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced diffusion\nmodels. In Proceedings of the ICML, pages 13916\u201313932, 2023.\n[34] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu,\nZhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe. Audiogpt: Under-\nstanding and generating speech, music, sound, and talking head. CoRR, abs/2304.12995, 2023.\n[35] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv,\nLei Cui, Owais Khan Mohammed, Barun Patra, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck,\nVishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning\nperception with language models. CoRR, abs/2302.14045, 2023.\n[36] Wenjing Huang, Shikui Tu, and Lei Xu. Pfb-diff: Progressive feature blending diffusion for text-driven\nimage editing. CoRR, abs/2306.16894, 2023.\n18\n[37] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose:\nFashion image-to-video synthesis via stable diffusion. CoRR, abs/2304.06025, 2023.\n[38] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. Audiocaps: Generating\ncaptions for audios in the wild. In Proceedings of the NAACL, pages 119\u2013132, 2019.\n[39] Eungbeom Kim, Jinhee Kim, Yoori Oh, Kyungsu Kim, Minju Park, Jaeheon Sim, Jinwoo Lee, and Kyogu\nLee. Improving audio-language learning with mixgen and multi-level test-time augmentation. CoRR,\nabs/2210.17143, 2022.\n[40] Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. CoRR, abs/2305.17216, 2023.\n[41] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome:\nConnecting language and vision using crowdsourced dense image annotations. International Journal of\nComputer Vision, 123(1):32\u201373, 2017.\n[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, and Ziwei\nLiu. MIMIC-IT: multi-modal in-context instruction tuning. CoRR, abs/2306.05425, 2023.\n[43] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-image\npre-training with frozen image encoders and large language models. In Proceedings of the ICML, pages\n19730\u201319742, 2023.\n[44] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. CoRR, abs/2305.06355, 2023.\n[45] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang,\nJingjing Xu, Xu Sun, Lingpeng Kong, and Qi Liu. M3it: A large-scale dataset towards multi-modal\nmultilingual instruction tuning. CoRR, abs/2306.04387, 2023.\n[46] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong\nHu, Li Dong, Furu Wei, Yejin Choi, and Jianfeng Gao. Oscar: Object-semantics aligned pre-training for\nvision-language tasks. In Proceedings of the ECCV, pages 121\u2013137, 2020.\n[47] Yanda Li, Chi Zhang, Gang Yu, Zhibin Wang, Bin Fu, Guosheng Lin, Chunhua Shen, Ling Chen, and\nYunchao Wei. Stablellava: Enhanced visual instruction tuning with synthesized image-dialogue data.\nCoRR, abs/2308.10253, 2023.\n[48] Yicong Li, Xiang Wang, Junbin Xiao, Wei Ji, and Tat-Seng Chua. Invariant grounding for video question\nanswering. In Proceedings of the CVPR, pages 2918\u20132927, 2022.\n[49] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe Gan, Zicheng Liu, Yumao Lu, and Lijuan\nWang. Swinbert: End-to-end transformers with sparse attention for video captioning. In Proceedings of\nthe CVPR, pages 17928\u201317937, 2022.\n[50] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. In David J. Fleet, Tom\u00e1s Pajdla,\nBernt Schiele, and Tinne Tuytelaars, editors, Proceedings of the ECCV, pages 740\u2013755, 2014.\n[51] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo P. Mandic, Wenwu Wang, and Mark D.\nPlumbley. Audioldm: Text-to-audio generation with latent diffusion models. In Proceedings of the ICML,\npages 21450\u201321474, 2023.\n[52] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning.\nCoRR,\nabs/2304.08485, 2023.\n[53] Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image generation\nvia self-conditioned gans. In Proceedings of the CVPR, pages 14274\u201314283, 2020.\n[54] Muhammad Maaz, Hanoona Abdul Rasheed, Salman H. Khan, and Fahad Shahbaz Khan. Video-chatgpt:\nTowards detailed video understanding via large vision and language models. CoRR, abs/2306.05424,\n2023.\n[55] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations. In Proceedings of the\nICLR, 2022.\n19\n[56] Victor Siemen Janusz Milewski, Marie-Francine Moens, and Iacer Calixto. Are scene graphs good enough\nto improve image captioning? In Proceedings of the AACL, pages 504\u2013515, 2020.\n[57] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023.\n[58] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcGrew, Ilya Sutskever, and Mark Chen. GLIDE: towards photorealistic image generation and editing\nwith text-guided diffusion models. In Proceedings of the ICML, pages 16784\u201316804, 2022.\n[59] OpenAI. Introducing chatgpt. 2022.\n[60] OpenAI. Gpt-4 technical report. 2022.\n[61] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instructions with human feedback. In Proceedings of the NeurIPS,\n2022.\n[62] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus H. Gross, and Alexander\nSorkine-Hornung. A benchmark dataset and evaluation methodology for video object segmentation. In\nProceedings of the CVPR, pages 724\u2013732, 2016.\n[63] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis.\nCoRR, abs/2307.01952, 2023.\n[64] Leigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. Layoutllm-t2i: Eliciting layout\nguidance from LLM for text-to-image generation. CoRR, abs/2308.05095, 2023.\n[65] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In Proceedings of the ICML, pages\n8748\u20138763, 2021.\n[66] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In Proceedings of the ICML, pages 8821\u20138831, 2021.\n[67] Ali Razavi, A\u00e4ron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\nVQ-VAE-2. In Proceedings of the NeurIPS, pages 14837\u201314847, 2019.\n[68] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the CVPR, pages 10674\u201310685, 2022.\n[69] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. CoRR, abs/2208.12242,\n2022.\n[70] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa\nKundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: an\nopen large-scale dataset for training next generation image-text models. In Proceedings of the NeurIPS,\n2022.\n[71] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the ACL, pages\n2556\u20132565, 2018.\n[72] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving AI tasks with chatgpt and its friends in huggingface. CoRR, abs/2303.17580, 2023.\n[73] Hisaichi Shibata, Shouhei Hanaoka, Yang Cao, Masatoshi Yoshikawa, Tomomi Takenaga, Yukihiro\nNomura, Naoto Hayashi, and Osamu Abe. Local differential privacy image generation using flow-based\ndeep generative models. CoRR, abs/2212.10688, 2022.\n[74] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang,\nOron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-a-video: Text-to-video\ngeneration without text-video data. CoRR, abs/2209.14792, 2022.\n20\n[75] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,\nDario Amodei, and Paul F. Christiano. Learning to summarize with human feedback. In Proceedings of\nthe NeurIPS, 2020.\n[76] Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel\nCollier. Language models can see: Plugging visual controls in text generation. CoRR, abs/2205.02655,\n2022.\n[77] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt: One model to\ninstruction-follow them all. CoRR, abs/2305.16355, 2023.\n[78] Zineng Tang, Ziyi Yang, Chenguang Zhu, Michael Zeng, and Mohit Bansal. Any-to-any generation via\ncomposable diffusion. CoRR, abs/2305.11846, 2023.\n[79] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 2023. URL\nhttps://github.com/tatsu-lab/stanford_alpaca.\n[80] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin,\nEdouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. CoRR,\nabs/2302.13971, 2023.\n[81] Arash Vahdat and Jan Kautz. NVAE: A deep hierarchical variational autoencoder. In Proceedings of the\nNeurIPS, 2020.\n[82] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the NeurIPS, pages 5998\u20136008,\n2017.\n[83] Christophe Veaux, Junichi Yamagishi, Kirsten MacDonald, et al. Cstr vctk corpus: English multi-speaker\ncorpus for cstr voice cloning toolkit. CSTR, 6:15, 2017.\n[84] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. P+: extended textual conditioning\nin text-to-image generation. CoRR, abs/2303.09522, 2023.\n[85] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. GIT: A generative image-to-text transformer for vision and language. Trans. Mach. Learn.\nRes., 2022, 2022.\n[86] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. In Proceedings of the ICML, volume 162, 2022.\n[87] Tao Wang, Jiangyan Yi, Ruibo Fu, Jianhua Tao, and Zhengqi Wen. Campnet: Context-aware mask\nprediction for end-to-end text-based speech editing. IEEE ACM Trans. Audio Speech Lang. Process., 30:\n2241\u20132254, 2022.\n[88] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual\nchatgpt: Talking, drawing and editing with visual foundation models. CoRR, abs/2303.04671, 2023.\n[89] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video\ngeneration. CoRR, abs/2212.11565, 2022.\n[90] Junbin Xiao, Angela Yao, Zhiyuan Liu, Yicong Li, Wei Ji, and Tat-Seng Chua. Video as conditional\ngraph hierarchy for multi-granular question answering. In Proceedings of the AAAI, pages 2804\u20132812,\n2022.\n[91] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian,\nWei Wang, Guohai Xu, Ji Zhang, Songfang Huang, Fei Huang, and Jingren Zhou. mplug-2: A modularized\nmulti-modal foundation model across text, image and video. In Proceedings of the ICML, pages 38728\u2013\n38748, 2023.\n[92] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for bridging\nvideo and language. In Proceedings of the CVPR, pages 5288\u20135296, 2016.\n[93] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He.\nAttngan: Fine-grained text to image generation with attentional generative adversarial networks. In\nProceedings of the CVPR, pages 1316\u20131324, 2018.\n21\n[94] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to\nanswer questions from millions of narrated videos. In Proceedings of the ICCV, pages 1666\u20131677, 2021.\n[95] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound:\nDiscrete diffusion model for text-to-sound generation. IEEE ACM Trans. Audio Speech Lang. Process.,\n31:1720\u20131733, 2023.\n[96] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu,\nChenliang Li, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang. mplug-docowl: Modularized multimodal\nlarge language model for document understanding. CoRR, abs/2307.02499, 2023.\n[97] Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi, Dingning Liu, Mukai Li, Lu Sheng, Lei Bai,\nXiaoshui Huang, Zhiyong Wang, Jing Shao, and Wanli Ouyang. LAMM: language-assisted multi-modal\ninstruction-tuning dataset, framework, and benchmark. CoRR, abs/2306.06687, 2023.\n[98] Bowen Yu, Cheng Fu, Haiyang Yu, Fei Huang, and Yongbin Li. Unified language representation for\nquestion answering over text, tables, and images. CoRR, abs/2306.16762, 2023.\n[99] Zequn Zeng, Hao Zhang, Ruiying Lu, Dongsheng Wang, Bo Chen, and Zhengjue Wang. Conzic:\nControllable zero-shot image captioning by sampling-based polishing. In Proceedings of the CVPR,\npages 23465\u201323476, 2023.\n[100] Ao Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual prompt\ngenerator across llms. CoRR, abs/2305.01278, 2023.\n[101] Bowen Zhang, Shuyang Gu, Bo Zhang, Jianmin Bao, Dong Chen, Fang Wen, Yong Wang, and Baining\nGuo. Styleswin: Transformer-based GAN for high-resolution image generation. In Proceedings of the\nCVPR, pages 11294\u201311304, 2022.\n[102] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.\nSpeechgpt: Empowering large language models with intrinsic cross-modal conversational abilities.\nCoRR, abs/2305.11000, 2023.\n[103] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual language model\nfor video understanding. CoRR, abs/2306.02858, 2023.\n[104] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar:\nEnhanced visual instruction tuning for text-rich image understanding. CoRR, abs/2306.17107, 2023.\n[105] Ziqi Zhang, Yaya Shi, Chunfeng Yuan, Bing Li, Peijin Wang, Weiming Hu, and Zheng-Jun Zha. Object\nrelational graph with teacher-recommended learning for video captioning. In Proceedings of the CVPR,\npages 13275\u201313285, 2020.\n[106] Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: scaling up visual instruction tuning. CoRR, abs/2307.04087,\n2023.\n[107] Liang Zhao, En Yu, Zheng Ge, Jinrong Yang, Haoran Wei, Hongyu Zhou, Jianjian Sun, Yuang Peng,\nRunpei Dong, Chunrui Han, and Xiangyu Zhang. Chatspot: Bootstrapping multimodal llms via precise\nreferring instruction tuning. CoRR, abs/2307.09474, 2023.\n[108] Yang Zhao, Zhijie Lin, Daquan Zhou, Zilong Huang, Jiashi Feng, and Bingyi Kang. Bubogpt: Enabling\nvisual grounding in multi-modal llms. CoRR, abs/2307.08581, 2023.\n[109] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. CoRR, abs/2304.10592, 2023.\n[110] Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. DM-GAN: dynamic memory generative adversarial\nnetworks for text-to-image synthesis. In Proceedings of the CVPR, pages 5802\u20135810, 2019.\n22\n"
  },
  {
    "title": "MADLAD-400: A Multilingual And Document-Level Large Audited Dataset",
    "link": "https://arxiv.org/pdf/2309.04662.pdf",
    "upvote": "21",
    "text": "MADLAD-400: A Multilingual And Document-Level\nLarge Audited Dataset\nSneha Kudugunta\u2020\nIsaac Caswell\u22c4\nBiao Zhang\u2020\nXavier Garcia\u2020\nChristopher A. Choquette-Choo\u2020\nKatherine Lee\u2020\nDerrick Xin\u2020\nAditya Kusupati\u22c4\nRomi Stella\u2020\nAnkur Bapna\u2020\nOrhan Firat\u2020\n\u2020Google DeepMind\n\u22c4Google Research\nAbstract\nWe introduce MADLAD-400, a manually audited, general domain 3T token mono-\nlingual dataset based on CommonCrawl, spanning 419 languages. We discuss\nthe limitations revealed by self-auditing MADLAD-400, and the role data auditing\nhad in the dataset creation process. We then train and release a 10.7B-parameter\nmultilingual machine translation model on 250 billion tokens covering over 450\nlanguages using publicly available data, and find that it is competitive with models\nthat are significantly larger, and report the results on different domains. In addi-\ntion, we train a 8B-parameter language model, and assess the results on few-shot\ntranslation. We make the baseline models 1 available to the research community.\n1\nIntroduction\nThe availability of large multilingual corpora has accelerated the progress of multilingual natural\nlanguage processing (NLP) models [69, 19, 47, 9, 51]. However, most publicly available general-\ndomain multilingual corpora contain 100-200 languages [69, 51, 2], with some datasets containing\nmore languages in specific domains such as religious content [4], children\u2019s books [45] or dialects [3].\nA common approach to creating such datasets is to mine language specific data from general web\ncrawls such as CommonCrawl [57, 43, 68] to create datasets. We simply take this approach and scale\nit. We train a document-level LangID model on 498 languages to obtain CommonCrawl annotations\nat a document level and obtain a 5-trillion token, document-level monolingual dataset.\nHowever, such web-scale corpora are known to be noisy and contain undesirable content [53, 48,\n21], with their multilingual partitions often having their own specific issues such as unusable text,\nmisaligned and mislabeled/ambiguously labeled data [40]. To mitigate this, we manually audit\nour data. Based on our findings, we discard 79 of the languages from our preliminary dataset,\nrename or combine several languages and apply additional preprocessing steps. Finally, to validate\nthe efficacy of our dataset, we train multilingual machine translation models of various sizes up to\n10.7B parameters, as well as an 8B decoder-only model, and then evaluate these models on highly\nmultilingual translation evaluation sets.\nIn Section 2, we describe the creation and composition of MADLAD-400, and discuss the results\nof the audit. Then, in Section 3, we describe the parallel data we collect using publicly available\nsources to train the multilingual machine translation models described in Section 4.1. In Section 4,\nwe describe the training process of the multilingual machine translation models and 8B decoder-only\nmodel, and then evaluate these models on highly multilingual translation datasets. In Section 5 we\ndescribe our tests for memorization in the multilingual models that we release and discuss preliminary\nresults. Finally, we discuss the limitations of this work and directions for future work.\n1https://github.com/google-research/google-research/tree/master/madlad_400\nPreprint. Under review.\narXiv:2309.04662v1  [cs.CL]  9 Sep 2023\nen\npt\nro\nuk\nko\nlt\nsq\nkk\nmk\neu\nsw\neo\nps\nha\ngd\nmi\nsm\nig\nlo\nxh\nkl\nom\nti\nlg\nada\nzza\nsrn\niso\nyua\npck\nbci\nium\nmps\ngom\nqu\nmeo\ngym\nmrj\ngv\nfip\nkj\nibb\nmdf\nstq\nbzj\ntoj\nsja\nsrm\ndtp\nch\njam\nskr\nalz\ncce\nsda\ninb\nrn\ngvl\ndwr\nniq\nkoi\nagr\nify\nber\nffm\nqup\ndln\nta_Latn\nbg_Latn\ngu_Latn\nLanguages\n103\n105\n107\n109\n1011\n# Tokens\nNoisy\nClean\non Google Translate\nFigure 1: Comparing the size of the noisy and clean monolingual datasets in MADLAD-400. The\ndifference is more noticeable on lower-resource languages, where noise effects are especially severe.\nFor reference, languages supported by Google Translate are shaded in green. Note that, since this\nchart is in log scale, the difference in size is much greater than it may appear; for instance, for the\nlower-resource half of the dataset, the ratio is about 4\u00d7 on median.\n2\nMADLAD-400\nThe process we follow to create MADLAD-400 is similar to that of other large-scale web corpora\n[15, 68, 2, 51]. First, we collect as large a dataset of unlabeled web text as possible. More specifically,\nwe use all available snapshots of CommonCrawl2 as of August 20, 2022. After some preliminary\ndata cleaning, we use a highly multilingual LangID model to provide document-level annotations\n(Section 2.2). Finally, we conduct a self-audit (Section 2.4), or quality review, of this preliminary\ndataset partitioned by language, and design filters to remove noisy content. When appropriate, we\ncorrect language names and remove languages from the preliminary dataset. We note that building\nMADLAD-400 was an iterative process, and that while we describe one major quality review in depth,\nwe conducted several stages of filtering. To reflect this, we describe the preprocessing steps and\nimprovements made in chronological order.\nTable 1: Geographic distribution of lan-\nguages in MADLAD-400.\nContinent\n# Languages\nAsia\n149\nAmericas\n66\nAfrica\n87\nEurope\n89\nOceania\n26\nConstructed\n2\nWe release two version of this dataset: a 5 trillion token\nnoisy dataset, which is the dataset obtained before apply-\ning document-level LangID and the final filters, and a 3\ntrillion token clean dataset, which has a variety of filters\napplied based on our self-audit, though it naturally has a\nfair amount of noise itself. Each dataset is released in both\na document-level form and a sentence-level form. Some\noverall statistics for these dataset versions are given in\nTable 2, with a graph visualizing the distribution of sizes\n(number of tokens) across languages in Figure 1. The\nfinal version of MADLAD-400 has 419 languages, with a\nvaried geographic distribution, as seen in Table 1.\nTable 2: Overall statistics of both the noisy and clean partitions of MADLAD-400.\nDataset Version\n# Documents\n# Sentences\n# Tokens\nTotal\nMedian\nTotal\nMedian\nTotal\nMedian\nMADLAD-400-noisy\n7.8B\n27K\n150B\n240K\n5.0T\n7.1M\nMADLAD-400-clean\n4.0B\n1.7K\n100B\n73K\n2.8T\n1.2M\n2https://commoncrawl.org/\n2\n2.1\nPreliminary Filters\nWe carry out a few preliminary preprocessing steps on the web-crawled corpus: first, we deduplicate\nlines across documents [44]. Then, we filter out all pages that do not contain at least 3 lines of 200 or\nmore characters (as done by Xue et al. [68]). We also use other commonly used filtering heuristics\nsuch as removing lines containing the word \u201cJavascript\u201d and removing pages that contain \u201clorem\nipsum\u201d and curly brackets \u201c{\u201d (as done by Raffel et al. [57]).\n2.2\nLanguage Identification (LangID)\nWe train a Semi-Supervised LangID model (SSLID) on 500 languages, following the recipe introduced\nby Caswell et al. [15]. We then filter the corpus on document-level LangID, which was taken to be\nthe majority sentence-level LangID prediction. The resulting dataset is MADLAD-400-noisy. For\nthe Additional details on these LangID models is in Appendix A.1.\n2.3\nFiltering Out Questionable Content\nTo assess the quality of this preliminary dataset, we inspected 20 sentences each from a subset of 30\nlanguages in our dataset. Based on our observations, we introduced a score, pct_questionable.\nThe pct_questionable score is simply the percentage of sentences in the input document that were\n\u201cquestionable\u201d. A sentence was considered questionable if any of the following were true:\n1. Document consistency: Sentence-level LangID does not match the document-level LangID.\n2. List Case: Over 50% percent of the tokens began in a capital letter (we apply this filter only\nif the sentence has at least 12 tokens.)\n3. Abnormal Lengths: The sentence has under 20 characters or over 500 characters. We note\nthat this is a bad heuristic for ideographic languages3).\n4. Technical Characters: Over 20% of the characters in the sentence match [0-9{}+/()>].\n5. Cursed Regexes: The sentence matched a \u201ccursed regex\u201d. These are a heuristic set of\nsubstrings and regexes that we found accounted for a significant amount of questionable\ncontent in the data samples we observed. They are described in depth in Appendix A.2.\nWe removed all documents with a percent_questionable score greater than 20%. Furthermore,\nwe removed any document with under 5 sentences.\n2.4\nSelf-Audit (Quality Review)\nAfter filtering out generally lower-quality content with the approach described above, we performed a\nself-audit of every corpus in this dataset, following Kreutzer et al. [40]. The aim of our self-audit was\nto correct any remaining systematic issues by either applying additional filters, renaming/merging\nlanguage codes, or completely removing the language from the dataset. Although we do not speak\nmost of the 498 languages, we were able to give high-level comments on the general quality. For\neach language, we inspected a sample of 20 documents. This task was evenly divided between the\nfirst two authors based in part on which scripts they could read. We used the following guidelines:\n\u2022 If dataset is mostly plausibly in-language text, we can keep it. For unknown languages,\nsearch the web for a few sentences and look at the website and URL for language clues.\n\u2022 If dataset is noisy but the noise looks filterable, leave a note of how to filter it.\n\u2022 If the dataset is very noisy and does not look possible to filter, mark it for removal.\n\u2022 Optionally put note that may be helpful for downstream users, e.g. if dataset is 100% Bible.\nWe made the decision to include languages that looked noisy, but omit any language that was\nmajority noise, or only had 20 or fewer docs. While this is not a high quality bar, we hope it\nstill has the potential to be useful to the research community, given that foundation models have\ndemonstrated the potential to learn distributions for very few exammples [12]. The motivation for not\nreleasing \u201cnonsense\u201d or tiny datasets is to avoid giving a false sense of how multilingual the dataset\nis (\u201cRepresentation washing\u201d), as recommended by Quality at a Glance [40].\nOverall Results.\nOf the 498 languages that we obtained LangID annotations for, we decided to\nomit 79 languages, bringing the final number of languages in MADLAD-400 to 419. Based on\n3http://www.grcdi.nl/dqglossary/ideographic%20language.html\n3\nthe self-audit, we also expanded the filters (particularly the cursed regexes), and made changes as\ndescribed in Sections 2.5 and 2.6. We details stats for these languages in Appendix Section A.4.\nFor transparency, we provide full results of the self-audit in Appendix A.4. In Table 3, we provide\nan overview of the issues surfaced through this self-audit. We find that a significant fraction of\nlanguages contain mostly or entirely religious documents, while other issues include misrendered\ntext, pornographic content, and boilerplate.\nTable 3: Summary of results of the audit on the preliminary dataset comprising of 498 languages.\nNote that there may be multiple issues with data in one language.\n# Languages...\nAudited\n498\nWith significant amounts of Bible data\n141\nWith significant amounts of JW data\n37\nWith significant amounts of LDS data\n2\nWith significant amounts of virama-based issues\n8\nWith a significant number of short docs\n42\nWith complaints about noise\n28\nWith complaints about porn\n10\nWith complaints about boilerplate\n15\nWith a note to remove from the dataset\n77\n2.5\nAdditional Filters\nBased on the results of the self-audit, we apply three additional filters.\nVirama Filtering and Correction.\nMany languages using Brahmic Abugida (South and Southeast\nAsian scripts like Devanagari, Khmer, etc.) use some variant on the virama 4 character. We found\nthat such languages in MADLAD-400-noisy had incorrectly encoded viramas: for example,\nwas rendered as\n, where the middle character is a detached virama. Therefore, for the\nlanguages bn, my, pa, gu, or, ta, te, kn, ml, si, th, tl, mn, lo, bo, km, hi,\nmr, ne, gom, as, jv, dv, bho, dz, hne, ks_Deva, mag, mni, shn, yue, zh, ja,\nkjg, mnw, ksw, rki, mtr, mwr and xnr, we did a special filtering/correction step \u2014 we\nremoved all extraneous spaces before virama characters. We provide the pseudocode and list of\nvirama characters in Appendix A.2.\nZawgyi Encoded Data.\nWe found that languages using Myanmar script like my and mnw appeared\nto have the same issues with virama characters that still remained after applying the virama correction.\nThis was because a large fraction of Myanmar script data on the internet is Zawgyi encoded data,\nwhich appears to have the rendering issues described above if rendered in Unicode. Therefore, we\nused an open-source Zawgyi detector 5 to convert the encoding of documents with more than a 50%\nprobability of being Zawgyi encoded into standard Unicode encoding.\nChinese-Specific Filters.\nThe Mandarin (zh) data in CommonCrawl had a particular issue with\npornographic content. We combed through the data and developed a list of strings likely to be present\nin pornographic content, and filtered out all documents containing the strings in the blocklist. This\nresulted in a 17% reduction in the number of documents and a 56% reduction in file size. We list\nthese strings in Appendix A.2.\n2.6\nCorrecting Other Systematic Issues.\nBased on various specific notes from the self-audit, we made a variety of changes. Five datasets\nwere found to be in the wrong language, and were renamed or merged into the correct dataset. Six\n4https://en.wikipedia.org/wiki/Virama\n5https://github.com/google/myanmar-tools\n4\nlanguages that looked suspicious were run by native speakers of those or related languages, some of\nwhich were discarded, and some of which were merged into the correct dataset. Finally, we removed\nall languages with fewer than 20 documents. Details can be seen in Appendix A.3.\n3\nParallel Data\nTo train the machine translation (MT) models described in Section 4.1, we also collect a dataset\ncomposed of publicly available datasets coming from various data sources. A full list of the data\nsources and associated language pairs are in Appendix A.5. The final dataset has 156 languages\nacross 4.1B sentence pairs and 4124 language pairs total. In the rest of the paper, we refer to the input\nsentence to an MT model as the \u201csource side\" and the reference/output sentence as the \u201ctarget side\".\n3.1\nFilters\nWe describe the data preprocessing steps taken below. We find that a significant amount of data is\nfiltered out, with the amount of data available 396 of 4.1k language pairs reducing by more than 40%.\nDeduplication.\nWe deduplicate sentence pairs that are an exact match on both the source and target.\nVirama Filtering and Correction/Zawgyi Encoded Data.\nWe observed the same issues described\nin Section 2.5, and used the same filters for sentence pairs where either the source language or target\nlanguage belonged to the list of languages in Section 2.5.\nUnmatched Toxicity Filters.\nWe use the unmatched toxicity filters described by NLLBTeam\net al. [51], but ultimately unusable for our purposes in most cases. For the languages ace, am, ar,\naz, bg, bm, bn, bs, cs, din, en, es, fa, fr, ga, gl, ha, hi, id, it, kk, ko,\nml, ms, my, nl, no, nus, prs, ru, scn, sd, so, sv, tg, th, tt, ur, uz and zh,\nmore than 3% of documents were marked as having unmatched toxicity. On closer inspection, we\nfound that while zh and ko had a lot of pornographic content that was removed by the filtering\nprocess, most other languages removed sentences that had homonyms of non-toxic words. Similarly,\nlanguages like id, ur, tg, fa and no had data from Tanzil (Qur\u2019an dataset), but the toxicity word\nlists contained words such as kafir, mercy and purity, that are not normally considered toxic\ncontent for our purpose of filtering the dataset using wordlists.\nSource-Target Filters.\nWe removed all sentences that have more than 75% overlap between the\nsource and target side. To avoid filtering out valid entity translations, we only applied this filter on\nsentences longer than 5 tokens. In addition, we remove sentence pairs whose source length to target\nlength ratio falls outside of 0.66 \u2212 1.5. We omitted this filter for the following, which are mainly\nnon-whitespace languages: zh, ja, ko, km, my, lo, th, wuu, shn, zh_tw, zh_cn,iu,\nsimple, dz, kr_Arab, din, nus and mi.\nScript Filters.\nWe removed all sentences that are less than 50% in-script for both the source and\ntarget language. For instance, if the sentence was supposed to be in kaa (Cyrillic script) but was 70%\nin the Latin script, we removed it.\n3.2\nSelf-Audit (Quality Review)\nSimilar to the self-audit done for MADLAD-400, we conducted a review of the data sources that\ncompose the parallel data we collected to verify the quality of this data. We collected 20 source-target\npairs from each language, and assessed the data for the presence of offensive content, porn, and\nwhether the data seemed to be of the correct language pair and whether the target sentence seemed to\nbe a plausible translation. Since we did not have access to native speakers of all 157 languages, the\nlatter was primarily based on guesses. In Appendix A.5 we provide full details of the instructions we\nprovided to auditors, the results of the self-audit and any changes made the dataset.\n5\n3.3\nA Note on Language Codes\nAs observed by Kreutzer et al. [40], the datasets used to create the parallel data (and MADLAD-400)\nuse a variety of different language codes. We use the BCP-47 standard, which specifies the 2-letter\nISO-693-1 code when applicable, and otherwise the ISO-693-3 code. Script tags and region tags are\nomitted when they are defined as the default value by CLDR 6, and otherwise included. For example,\nks refers to Kashmiri in Nastaliq/Arabic script (CLDR default), whereas ks_Deva refers to Kashmiri\nin Devanagari. A detailed investigation of codes in MADLAD-400 can be found in Appendix A.3.\n3.4\nMultiway Data\nWe create additional multiway data by applying the n-gram matching method (n = 8) from Freitag\nand Firat [25] to the processed dataset. Using this, and the publicly available data, we obtain 11.9B\nsentences across a total of 20742 language pairs. Full details may be found in Appendix A.7.\n4\nExperiments\nWe validate our data by training encoder-decoder machine translation models in Section 4.1 and\ndecoder-only language models in Section 4.2, and test them on several translation benchmarks.\n4.1\nMT Models\nWe train models of various sizes: a 3B, 32-layer parameter model,7 a 7.2B 48-layer parameter model\nand a 10.7B 32-layer parameter model. We share all parameters of the model across language pairs,\nand use a Sentence Piece Model [41] with 256k tokens shared on both the encoder and decoder\nside. Each input sentence has a <2xx> token prepended to the source sentence to indicate the target\nlanguage [35].\nWe use both supervised parallel data with a machine translation objective and the monolingual\nMADLAD-400 dataset with a MASS-style [62] objective to train this model. Each of these objectives\nis sampled with a 50% probability. Within each task, we use the recently introduced UniMax [18]\nsampling strategy to sample languages from our imbalanced dataset with a threshold of N = 10\nepochs for any particular language. We also explored back-translation by randomly sampling 2M\nmonolingual samples (or the total number of samples for that given language) for each language and\ntranslating them to/from English using the 3B model. Following Bapna et al. [9] (\u00a73.5), we filter the\nback-translated data in a variety of ways. For a natural target and a back-translated source, we filter\nby round-trip ChrF to discourage hallucinations (threshold of 0.32), by ChrF between source and\ntarget to discourage copying (threshold of 0.30), by the length ratio of source to target (asymmetric\nbounds of (0.45, 1.6), and by LangID prediction of the source. We then finetune the 7.2B model for a\n10, 000 steps by randomly mixing the original and the back-translated data with a combining ratio of\n1:1. We list specific architecture and training details of these models in Appendix A.8.\n4.2\nZero-shot Translation with Language Models\nGiven recent interest in the efficacy of unsupervised translation using large language models, we\nexplore training language models solely on the monolingual data. We follow the same training\nschedule and model configurations from Garcia et al. [27]. In particular, we consider 8B decoder-\nonly models, following the same model hyperparameters as previous work [17, 27]. We train these\nmodels using a variant of the UL2 objective [63] adapted for decoder-only models, and use the same\nconfiguration as previous work [27, 52]. We provide additional details in Appendix A.8.\n6https://cldr.unicode.org/\n7Here and elsewhere, \u2018X-layer\u2019 means X encoder layers and also X decoder layers, for a total of 2X layers.\n6\nTable 4:\nEvaluation scores on WMT (depicted as <bleu> / <chrf>) for the MT models and\nlanguage models described in Section 4.1 and Section 4.2 compared against NLLB-54B.\nNLLB\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\nxx2en\n34.2 / 60.4\n33.4 / 60.0\n34.9 / 60.6\n34.6 / 60.8\n2.3 / 17.3\n25.1 / 51.4\n26.2 / 52.9\n26.2 / 53.4\nen2xx\n31.1 / 58.0\n28.2 / 55.4\n29.3 / 56.2\n29.0 / 56.2\n1.0 / 10.3\n18.7 / 43.5\n18.8 / 44.5\n19.3 / 45.5\nAverage\n32.7 / 59.2\n30.8 / 57.7\n32.1 / 58.4\n31.8 / 58.5\n1.6 / 13.8\n21.9 / 47.4\n22.5 / 48.7\n22.8 / 49.4\n4.3\nEvaluation\nWe use the sacreBLEU [55] implementation of bleu8 and chrf9 as metrics. We evaluate our trained\nmodels on the following datasets:\nWMT.\nWe use the 15 WMT languages frequently used to evaluate multilingual machine translation\nmodels by Siddhant et al. [61], Kim et al. [38], Kudugunta et al. [42], NLLBTeam et al. [51]: cs,\nde, es, fi, fr, gu, hi, kk, lv, lt, ro, rs, es, tr and zh.\nFlores-200.\nWe evaluate on the languages in the Flores-200 dataset [51] that overlap with the\nlanguages available in either MADLAD-400 or the parallel data described in Section 3. We list these\nlanguages in Appendix A.9. For non-English-centric pairs, we evaluate on a 272 language pair subset\nof the 40k language pairs possible due to computational constraints. We evaluate on all language pairs\npossible using the following languages as either source or target language: en, fr, cs, zh, et,\nmr, eu, cy, so, ckb, or, yo, ny, ti, ln, fon and ss. We obtained this set of languages\nby selecting every 10th language by number of tokens in MADLAD-400 (clean), starting with French\n(fr). Noticing that this had no Indian languages, we shifted af and fo (both close dialects of HRLS)\ndown one index to mr and or, respectively. Finally, we noticed that this initial list had supervised and\nunsupervised languages, but didn\u2019t have a good representative of a \u201cslightly supervised language\u201d,\nthat is, one with a small but extant amount of parallel data. Therefore, we added yo to the list, which\nhas the least parallel data of any supervised language. This resulting subset of languages also contains\na nice variety of scripts: Latin, Chinese, Devanagari, Arabic, Odia, and Ethiopic scripts.\nNTREX.\nWe evaluate on the languages in the recently introduced NTREX dataset [23].\nGatones.\nFinally, we evaluate on the languages in GATONES, the in-house, 38-language eval set\nused in [9] and the GATITOS paper [36]. Again, we take the subset of languages overlapping with the\nlanguages available in either MADLAD-400 or the parallel training data.\nTable 5: Evaluation scores on Flores-200 (depicted as <bleu> / <chrf>) for the MT models and\nlanguage models described in Section 4.1 and Section 4.2 compared against NLLB-54B. All metrics\nare computed with the sacrebleu reference implementation.\nNLLB\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\nxx2en\n35.5 / 59.6\n29.7 / 54.4\n30.9 / 55.4\n31.9 / 56.4\n2.0 / 13.3\n20.5 / 44.1\n22.3 / 46.9\n22.4 / 47.6\nen2xx\n20.7 / 50.1\n17.3 / 44.1\n17.8 / 44.7\n18.6 / 45.7\n0.4 / 5.7\n8.1 / 26.7\n8.7 / 29.0\n8.7 / 28.8\nMean\n28.2 / 54.9\n23.5 / 49.2\n24.4 / 50.0\n25.3 / 51.1\n1.2 / 9.6\n14.3 / 35.5\n15.6 / 38.0\n15.6 / 38.2\nxx2yy\n13.7 / 40.5\n8.8 / 31.2\n8.4 / 30.9\n10.1 / 34.0\n0.3 / 4.1\n4.0 / 16.1\n4.4 / 17.3\n4.2 / 17.1\n4.3.1\nFew-shot evaluation for language modeling\nWe perform few-shot prompting to evaluate the language model with the following prompt:\n[sl]:X1\\n[tl]:Y1\\n\\n[sl]:X2\\n[tl]:Y2\\n\\n...[sl]:X\\n[tl]:\n8 BLEU+case.mixed+lang.<sl>-<tl>+ numrefs.1+smooth.exp+tok.<tok>+version.1.3.0,\ntok=zh if tl=zh and 13a otherwise.\n9nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.3.1\n7\nTable 6:\nEvaluation scores on the recently introduced NTREX test set (depicted as <bleu> /\n<chrf>) for the MT models and language models described in Section 4.1 and Section 4.2 compared\nagainst unsupervised baselines [10]. Note that LM-8B is evaluated on a 50% split of the NTREX\ndata and is not comparable to the MT-model evaluations.\nBaziotis et al. [10]\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\nResults on the subset of Baziotis et al. [10]\nxx2en\n23.6 / 51.7\n34.3 / 59.9\n36.1 / 61.0\n35.9 / 61.1\n4.0 / 18.9\n23.4 / 48.8\n26.8 / 52.8\n27.6 / 53.7\nen2xx\n15.9 / 44.8\n22.3 / 50.2\n22.8 / 50.6\n22.8 / 51.0\n1.0 / 8.8\n15.2 / 40.1\n16.5 / 42.4\n15.9 / 42.3\nAverage\n19.8 / 51.7\n28.3 / 55.1\n29.4 / 55.8\n29.4 / 56.1\n2.5 / 13.9\n19.3 / 44.5\n21.6 / 47.6\n21.8 / 48.0\nResults on full test sets\nxx2en\n-\n30.6 / 54.5\n32.7 / 56.2\n33.6 / 57.6\n3.2 / 17.3\n20.4 / 43.8\n23.8 / 48.2\n24.4 / 49.0\nen2xx\n-\n16.5 / 39.6\n17.6 / 41.9\n17.9 / 41.9\n0.8 / 7.3\n11.7 / 31.2\n12.6 / 32.4\n12.3 / 32.3\nAverage\n-\n23.5 / 47.0\n25.1 / 49.0\n25.7 / 49.7\n2.0 / 12.3\n16.0 / 37.4\n18.1 / 40.2\n18.3 / 40.6\nwhere [sl] and [tl] denote the source and target language name (expressed in English. For example,\nwhen translating a sentence from en to te, we use [sl]=English and [tl]=Telugu), respectively.\nX\u22c6 and Y\u22c6 are demonstration examples used for prompting, and X is the test input.\nFor each test example, we randomly sample demonstration examples, which is simple yet performs\ncompetitively with more complicated strategies [66, 72]. In particular, we randomly select examples\nfrom the dev split of each dataset. Since NTREX does not have a dev split, we randomly sample\n1000 examples as the dev set and use the rest for test evaluation.\n4.4\nResults\nIn Tables 4 and 6 we present evaluation scores on the WMT datasets and NTREX datasets, which\nare evaluation sets in the news domain. We find that both the 7.2B parameter model and the 10B\nparameter model is competitive with the significantly larger NLLB-54B model [51] on WMT. For the\nrecent NTREX dataset, the only published results are small-scale results by Baziotis et al. [10].\nIn Table 5 we find that on Flores-200, our model is within 3.8 chrf of the 54B parameter NLLB\nmodel, while on xxyy pairs the 10.7B model is behind by 6.5 chrf. This is likely due to a combination\nof factors, including using a significantly smaller model (5x smaller), domain differences [10, 9], and\nback-translated data [60]. Similarly, in Table 7, we find that the 10.7B parameter model is within 5.7\nchrf of the scores reported by Bapna et al. [9]. Again, it is very difficult to compare their results to\nours; their two largest advantages are 1) iterative back-translation, and 2) access to a much larger\nin-house text data. In Table 8, we display the results for when we finetune the 7.2B parameter model\non backtranslated data. While this setup is very likely sub-optimal, we see that back-translation\ngreatly improves en2xx translation (by 3.0 chrf, in the case of Flores-200) in most cases. We note\nthat the results we present are merely baselines to demonstrate the utility of MADLAD-400, and hope\nthat future work builds upon these experiments by applying improved modeling techniques.\nFinally, across all evaluation datasets, we find that while results on few-shot translation using the\n8B language model increase with an increasing number of demonstrations, these results are still\nsignificantly weaker than the results of models trained on supervised data. We present per-language\npair results on all datasets in Appendix A.10.\nTable 7: Evaluation scores on the GATONES test set used by Bapna et al. [9] (depicted as <bleu> /\n<chrf>) for the MT models and language models described in Section 4.1 and Section 4.2.\nNTL (Bapna et al. [9])\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n1.6B\n6.4B\n0-shot\n1-shot\n5-shot\n10-shot\nxx2en\n- / 37.2\n- / 41.2\n13.3 / 34.6\n14.8 / 36.0\n15.4 / 37.0\n0.3 / 6.5\n6.6 / 25.4\n8.3 / 28.1\n8.4 / 28.4\nen2xx\n- / 28.5\n- / 33.1\n4.5 / 23.9\n5.4 / 26.2\n5.4 / 26.5\n0.2 / 4.2\n1.7 / 10.5\n1.7 / 9.9\n1.8 / 9.4\nAverage\n- / 32.9\n- / 37.2\n8.9 / 29.3\n10.1 / 31.1\n10.4 / 31.8\n0.3 / 5.4\n4.2 / 18.0\n5.0 / 19.0\n5.1 / 18.9\n8\nTable 8:\nEvaluation scores on different test sets (depicted as <bleu> / <chrf>) for MT-7.2B\ntrained with back-translated data (+BT).\nWMT\nFlores-200\nNTREX\nGATONES\nMT-7.2B\n+BT\nMT-7.2B\n+BT\nMT-7.2B\n+BT\nMT-7.2B\n+BT\nxx2en\n34.9 / 60.6\n33.8 / 60.4\n30.9 / 55.4\n27.2 / 53.9\n32.7 / 56.2\n31.0 / 56.5\n14.8 / 36.0\n10.2 / 34.5\nen2xx\n29.3 / 56.2\n29.8 / 56.9\n17.8 / 44.7\n18.5 / 47.7\n17.6 / 41.9\n18.4 / 44.4\n5.4 / 26.2\n3.5 / 26.1\naverage\n32.1 / 58.4\n31.8 / 58.6\n24.4 / 50.0\n22.8 / 50.8\n25.1 / 49.0\n24.7 / 50.4\n10.1 / 31.1\n6.9 / 30.3\nxx2yy\n-\n-\n8.4 / 30.9\n8.4 / 31.9\n-\n-\n-\n-\n5\nTraining Data Extraction and Memorization\nGenerative models have been shown to regurgitate training data [13] that may plagiarize, violate\ncopyright assumptions, or infringe privacy. It can be difficult to assess and prevent these cases because\nsuch information may be paraphrased in ways that are difficult for automated systems to detect [32].\nInstead, existing literature measures memorization in generative models to estimate the propensity for\ndisallowed outputs. Typically, this means prompting a language model with some prefix of length P\nand comparing generated outputs of length S with the training data to see if they are \u2018novel\u2019 or if the\ngeneration is simply a regurgitation of its training data [13, 6, 32, 33, 14]. In the multilingual setting\nthis may present new risks because tail languages may be more vulnerable to memorization [6].\nThe Difficulty of Assessing Memorization in Translation Settings.\nWhile memorization has been\nwell-studied for language models, assessing the extent of memorization is difficult within translation\nsettings. This is primarily because translation has a significantly smaller space of valid outputs, as\nopposed to many possible continuations for language modeling. This presents some difficulty in\nextending common memorization tests for language generation to translation. As an illustrative exam-\nple, consider the case of translating to the same target language as the source (\"translate_copy\").\nPerforming a standard training data extraction attack would test if the generation matches the contin-\nuation. However, success would not indicate training data extraction as the adversary would have\nalready had access to it.10 Thus, we modify the standard framework for testing memorization to\nbetter identify additional leaked data.\nMemorization in Translation Settings\nWe define memorization in translate_copy to be\nwhen the model outputs any generation with length S > P that matches the continuation; then,\nS \u2212 P captures the additional bits. In cases where the source and target language are different\n(\"translate_diff\"), performing a similar test would require knowledge of which part of the con-\ntinuation exactly corresponded to the prompt. Given that such an alignment is not easily obtained,\nwe instead use the relative token lengths between the continuation and the prompt to choose an\nappropriate size of S. For example, if at training time the continuation for the target language was\n1.5\u00d7 larger, we set S = P \u00b71.5+\u03b4 where \u03b4 captures the additional bits. For each of translate_copy\nand translate_diff, we sample 2, 000 sequences for each language and choose P = 50. We then\nperform both a verbatim match of the generation with the continuation and an approximate match\nrequiring 90% Levenshtein similarity similar to [32].\nResults.\nWe show the per-language and average training data extraction rates, for both the\ntranslate_copy and translate_diff settings in Figure 2, with S set to test for 50 tokens\nof additional information leakage. We find that translate models can memorize and regurgitate their\ntraining data, even beyond what is contained in the prompt. We also observe that some lower resource\nlanguages may exhibit higher memorization rates, however we observe no strong correlation between\nthe resource level and the level of memorization. In the translate_diff tests, we observe much\nlower memorization - we hypothesize this may be due to the higher difficulty of the task. Even though\nmany languages have nontrivial memorization, we found that many languages exhibited no memo-\nrization across the samples tested (257/370 for translate_copy and 130/146 for translate_diff\n). We also present results for approximate memorization in Appendix A.12, which show that translate\nmodels may also paraphrase memorizations leading to even higher memorization rates.\nDiscussion\nOur preliminary experiments show that memorization can exist in the translation setting.\nHowever, capturing when memorization is intended or beneficial versus undesired is still an open\n10Though membership inference may be possible.\n9\n10000\n1e+06\n1e+08\nLanguage Size (# of Senences)\n0.1\n1\n10\n% Verbatim Memorized\n1e+06\n1e+07\n1e+08\n1e+09\nLanguage Size (# of Senences)\n0.1\n1\n10\n% Verbatim Memorized\nFigure 2: Monolingual (translate_copy) data used in translation is more likely to be memorized.\nVerbatim training data extraction rates for both translate_copy (left) and translate_diff\n(right) data. Extraction performed on the 3B parameter model using a S = P + 50. In monoway,\n257/370 languages exhibited no memorization in testing and 130/146 for multiway.\nquestion. To aid future research in this direction, we design and include \u201ccanaries\u201d\u2014carefully\ncrafted data designed to be outliers to the natural training distribution that can be used to analyze\nmemorization. Canaries enable studying memorization in the multilingual and machine translation\nsettings by measuring the capability to extract canaries added to the training set [6, 33]. As with\nAnil et al. [6], our canaries are designed to share characteristics with the natural training data so\nas to better ground memorization evaluation in practical risks. The canaries are also designed tosl\nbe outliers to assess varying degrees of risk. To ensure similarity with natural data, canaries are\ngenerated by sampling and then randomly modifying real data in a manner similar to [6], where each\nsource of randomness defines the canary type. In total, we generate 1, 945, 631 canaries across both\nthe monolingual MADLAD-400 dataset and the parallel data (\u2248 0.0026% of the training data). The\nmethodology for each canary type and the exact distribution of canaries are detailed in Appendix A.11.\n6\nRelated Work\nExtensive work has been done to mine general purpose datasets for multilingual machine translation\nand language modeling. Xue et al. [68] introduce mC4, a general web domain corpus on 101\nlanguages to train mT5, a pretrained language model for downstream NLP tasks. Similarly, Conneau\net al. [19] introduce CC-100, later extended to CC100-XL by Lin et al. [47]. The OSCAR corpus [2]\nis also a mined dataset that supports 166 languages and the ROOTS corpus is a compiled dataset\nthat contains 46 natural languages. Glot500-C [31] covers 511 languages: however, it is not clear\nhow many of these languages comprise solely of religious texts. Bapna et al. [9] create an internal\ndataset on 1500+ languages, while NLLBTeam et al. [51] mine a dataset from CommonCrawl and\nParaCrawl [22]. Recently, Leong et al. [45] created a 350+ language dataset from children\u2019s books.\nIn addition, there have been efforts to get better represented corpora and models for languages often\nunderrepresented in general multilingual corpora: Serengeti [3] introduces a dataset and associated\nmodel trained on 517 African languages and language varieties, while IndicTrans2 [26] introduces a\nmachine translated model for the 22 scheduled languages in India.\n7\nLimitations\nWhile we used thorough self-audits to guide the creation of MADLAD-400, we note that most audits\nwere conducted by non-speakers of the languages in MADLAD-400; as a result, many types of noise,\nlike machine-generated or disfluent content, could not be detected. Moreover, toxicity detectors,\nclassifiers and filters that work reliably for all the 419 languages in MADLAD-400 do not exist,\nlimiting the extent to which we can clean and document [21, 8] the dataset. It is possible that\nissues still remain, so we encourage users to report issues that will be listed on the project Github\npage11. This paucity extends to the availability of multilingual evaluation sets for these languages -\nwe could only evaluate our models on 204 of the languages in MADLAD-400. Additionally, even\n11https://github.com/google-research/google-research/tree/master/madlad_400\n10\nthough decoder-only models are often evaluated on NLP tasks that are not necessarily machine\ntranslation [30, 7, 5], we did not conduct such evaluations - most available benchmarks cover only\n30-50 languages of which most are not tail languages (which forms the focus of MADLAD-400).\nWe instead leave this to future work. Finally, during our self-audit we noted the skew of data on the\nlong tail towards specific domains such as religious texts. We hope that these limitations motivate\nthe creation of more language-specific corpora not captured by web crawls, and the development of\nlanguage-specific data cleaning tools and practices.\n8\nConclusion\nThrough MADLAD-400, we introduce a highly multilingual, general web-domain, document-level\ntext dataset. We perform a self-audit of this dataset for quality on samples of all 498 languages,\ndevelop filters, and remove spurious datasets, for a total of 419 languages in the release. We carefully\ndescribe the dataset creation process, laying out the iterations of audits and improvements upon the\npreliminary dataset along with observations that guided our decisions. We hope that this encourages\ncreators of large-scale pretraining datasets both to put in their due diligence for manually inspecting\nand dealing with data, and also to describe and publicize the process in a level of detail that is\nreproducible and insightful for downstream users. This increased visibility into the dataset creation\ncycle can in turn improve model development and enable responsible data use [58]. Using MADLAD-\n400, we train and release large machine translation and general NLP models and evaluate them\nthoroughly. We hope that this further motivates work towards language technologies that are more\ninclusive of the rich language diversity housed by humanity.\n9\nEthics Statement\nInnovation in NLP technologies in English has been accelerated by training large scale deep learning\nmodels [20, 12] on massive web corpora [16, 73, 57]. However, on the long tail of written languages\nin the world there is a lack of high quality general data sources [37] that impede the progress\nof NLP tools for other languages. We hope that making an audited and cleaned corpus such as\nMADLAD-400 available mitigates this issue. While we extensively cleaned MADLAD-400, the\nextent to which we can preprocess this data is limited by how not all languages have available tools\nfor removing problematic content such as porn, toxic content, PII, copyrighted content or noise. We\nurge practitioners to carefully consider their target usecase before using MADLAD-400.\nAcknowledgements\nWe would like to thank Wolfgang Macherey, Zoubin Ghahramani and Orevaoghene Ahia for their\nhelpful comments on the draft. We would also like to thank Subramanian Venkateswaran for\ndebugging the virama rendering issues, and Ali Dabirmoghaddam for his insight on data samples of\nvarious languages in MADLAD-400.\nReferences\n[1] StatMT. https://www.statmt.org/. Accessed: 2022-05-03.\n[2] J. Abadji, P. O. Suarez, L. Romary, and B. Sagot. Towards a cleaner document-oriented\nmultilingual crawled corpus. arXiv preprint arXiv:2201.06642, 2022.\n[3] I. Adebara, A. Elmadany, M. Abdul-Mageed, and A. A. Inciarte. Serengeti: Massively multilin-\ngual language models for africa. arXiv preprint arXiv:2212.10785, 2022.\n[4] \u017d. Agic and I. Vulic. Jw300: A wide-coverage parallel corpus for low-resource languages.\nAssociation for Computational Linguistics, 2019.\n[5] K. Ahuja, R. Hada, M. Ochieng, P. Jain, H. Diddee, S. Maina, T. Ganu, S. Segal, M. Axmed,\nK. Bali, et al. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528,\n2023.\n[6] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,\nP. Bailey, Z. Chen, et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.\n11\n[7] A. Asai, S. Kudugunta, X. V. Yu, T. Blevins, H. Gonen, M. Reid, Y. Tsvetkov, S. Ruder, and\nH. Hajishirzi. Buffet: Benchmarking large language models for few-shot cross-lingual transfer.\narXiv preprint arXiv:2305.14857, 2023.\n[8] J. Bandy and N. Vincent. Addressing\" documentation debt\" in machine learning research: A\nretrospective datasheet for bookcorpus. arXiv preprint arXiv:2105.05241, 2021.\n[9] A. Bapna, I. Caswell, J. Kreutzer, O. Firat, D. van Esch, A. Siddhant, M. Niu, P. Baljekar, X. Gar-\ncia, W. Macherey, T. Breiner, V. Axelrod, J. Riesa, Y. Cao, M. X. Chen, K. Macherey, M. Krikun,\nP. Wang, A. Gutkin, A. Shah, Y. Huang, Z. Chen, Y. Wu, and M. Hughes. Building Machine\nTranslation Systems for the Next Thousand Languages. arXiv e-prints, art. arXiv:2205.03983,\nMay 2022.\n[10] C. Baziotis, B. Zhang, A. Birch, and B. Haddow. When does monolingual data help multilingual\ntranslation: The role of domain and model scale. arXiv preprint arXiv:2305.14124, 2023.\n[11] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein,\nJ. Bohg, A. Bosselut, E. Brunskill, et al. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258, 2021.\n[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\n[13] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown,\nD. Song, U. Erlingsson, et al. Extracting training data from large language models. In 30th\nUSENIX Security Symposium (USENIX Security 21), pages 2633\u20132650, 2021.\n[14] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang. Quantifying memorization\nacross neural language models. arXiv preprint arXiv:2202.07646, 2022.\n[15] I. Caswell, T. Breiner, D. van Esch, and A. Bapna. Language id in the wild: Unexpected\nchallenges on the path to a thousand-language web text corpus, 2020. URL https://arxiv.\norg/abs/2010.14571.\n[16] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion\nword benchmark for measuring progress in statistical language modeling. arXiv preprint\narXiv:1312.3005, 2013.\n[17] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[18] H. W. Chung, N. Constant, X. Garcia, A. Roberts, Y. Tay, S. Narang, and O. Firat. Unimax:\nFairer and more effective language sampling for large-scale multilingual pretraining. arXiv\npreprint arXiv:2304.09151, 2023.\n[19] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott,\nL. Zettlemoyer, and V. Stoyanov. Unsupervised cross-lingual representation learning at scale.\narXiv preprint arXiv:1911.02116, 2019.\n[20] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[21] J. Dodge, M. Sap, A. Marasovi\u00b4c, W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and\nM. Gardner. Documenting large webtext corpora: A case study on the colossal clean crawled\ncorpus. arXiv preprint arXiv:2104.08758, 2021.\n[22] M. Espl\u00e0-Gomis, M. L. Forcada, G. Ram\u00edrez-S\u00e1nchez, and H. Hoang. Paracrawl: Web-scale\nparallel corpora for the languages of the eu. In Proceedings of Machine Translation Summit\nXVII: Translator, Project and User Tracks, pages 118\u2013119, 2019.\n[23] C. Federmann, T. Kocmi, and Y. Xin. NTREX-128 \u2013 news test references for MT evaluation of\n128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation,\npages 21\u201324, Online, Nov. 2022. Association for Computational Linguistics. URL https:\n//aclanthology.org/2022.sumeval-1.4.\n[24] A. Fernando, S. Ranathunga, and G. Dias.\nData augmentation and terminology integra-\ntion for domain-specific sinhala-english-tamil statistical machine translation. arXiv preprint\narXiv:2011.02821, 2020.\n12\n[25] M. Freitag and O. Firat.\nComplete multilingual neural machine translation.\nCoRR,\nabs/2010.10239, 2020. URL https://arxiv.org/abs/2010.10239.\n[26] J. Gala, P. A. Chitale, R. AK, S. Doddapaneni, V. Gumma, A. Kumar, J. Nawale, A. Sujatha,\nR. Puduppully, V. Raghavan, et al. Indictrans2: Towards high-quality and accessible machine\ntranslation models for all 22 scheduled indian languages. arXiv preprint arXiv:2305.16307,\n2023.\n[27] X. Garcia, Y. Bansal, C. Cherry, G. Foster, M. Krikun, F. Feng, M. Johnson, and O. Firat.\nThe unreasonable effectiveness of few-shot learning for machine translation. arXiv preprint\narXiv:2302.01398, 2023.\n[28] H. J. Groenewald and W. Fourie. Introducing the autshumato integrated translation environ-\nment. In Proceedings of the 13th Annual conference of the European Association for Machine\nTranslation, 2009.\n[29] B. Haddow and F. Kirefu. Pmindia\u2013a collection of parallel corpora of languages of india. arXiv\npreprint arXiv:2001.09907, 2020.\n[30] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson. Xtreme: A massively\nmultilingual multi-task benchmark for evaluating cross-lingual generalisation. In International\nConference on Machine Learning, pages 4411\u20134421. PMLR, 2020.\n[31] A. ImaniGooghari, P. Lin, A. H. Kargaran, S. Severini, M. J. Sabet, N. Kassner, C. Ma,\nH. Schmid, A. F. Martins, F. Yvon, et al. Glot500: Scaling multilingual corpora and language\nmodels to 500 languages. arXiv preprint arXiv:2305.12182, 2023.\n[32] D. Ippolito, F. Tram\u00e8r, M. Nasr, C. Zhang, M. Jagielski, K. Lee, C. A. Choquette-Choo, and\nN. Carlini. Preventing verbatim memorization in language models gives a false sense of privacy.\narXiv preprint arXiv:2210.17546, 2022.\n[33] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini, E. Wallace, S. Song,\nA. Thakurta, N. Papernot, et al. Measuring forgetting of memorized training examples. arXiv\npreprint arXiv:2207.00099, 2022.\n[34] E. Joanis, R. Knowles, R. Kuhn, S. Larkin, P. Littell, C.-k. Lo, D. Stewart, and J. Micher. The\nnunavut hansard inuktitut\u2013english parallel corpus 3.0 with preliminary machine translation\nresults. In Proceedings of The 12th Language Resources and Evaluation Conference, pages\n2562\u20132572, 2020.\n[35] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat, F. Vi\u00e9gas, M. Wat-\ntenberg, G. Corrado, et al. Google\u2019s multilingual neural machine translation system: Enabling\nzero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351,\n2017.\n[36] A. Jones, I. Caswell, I. Saxena, and O. Firat. Bilex rx: Lexical data augmentation for massively\nmultilingual machine translation, 2023.\n[37] P. Joshi, S. Santy, A. Budhiraja, K. Bali, and M. Choudhury. The state and fate of linguistic\ndiversity and inclusion in the nlp world. arXiv preprint arXiv:2004.09095, 2020.\n[38] Y. J. Kim, A. A. Awan, A. Muzio, A. F. C. Salinas, L. Lu, A. Hendy, S. Rajbhandari, Y. He, and\nH. H. Awadalla. Scalable and efficient moe training for multitask multilingual models. arXiv\npreprint arXiv:2109.10465, 2021.\n[39] P. Koehn. Europarl: A parallel corpus for statistical machine translation. In Proceedings of\nmachine translation summit x: papers, pages 79\u201386, 2005.\n[40] J. Kreutzer, I. Caswell, L. Wang, A. Wahab, D. van Esch, N. Ulzii-Orshikh, A. Tapo, N. Subra-\nmani, A. Sokolov, C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot, C. Rivera, A. Rios,\nI. Papadimitriou, S. Osei, P. O. Suarez, I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen,\nM. M\u00fcller, A. M\u00fcller, S. H. Muhammad, N. Muhammad, A. Mnyakeni, J. Mirzakhalov,\nT. Matangira, C. Leong, N. Lawson, S. Kudugunta, Y. Jernite, M. Jenny, O. Firat, B. F. P. Dossou,\nS. Dlamini, N. de Silva, S. \u00c7abuk Ball\u0131, S. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Bal-\njekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia, O. Ahia, S. Agrawal, and M. Adeyemi.\nQuality at a glance: An audit of web-crawled multilingual datasets. Transactions of the As-\nsociation for Computational Linguistics, 10:50\u201372, 2022. doi: 10.1162/tacl_a_00447. URL\nhttps://aclanthology.org/2022.tacl-1.4.\n13\n[41] T. Kudo and J. Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226, 2018.\n[42] S. Kudugunta, Y. Huang, A. Bapna, M. Krikun, D. Lepikhin, M.-T. Luong, and O. Firat. Beyond\ndistillation: Task-level mixture-of-experts for efficient inference. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages 3577\u20133599, Punta Cana, Dominican\nRepublic, Nov. 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.\nfindings-emnlp.304. URL https://aclanthology.org/2021.findings-emnlp.304.\n[43] H. Lauren\u00e7on, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral, T. Le Scao, L. Von Werra,\nC. Mou, E. Gonz\u00e1lez Ponferrada, H. Nguyen, et al. The bigscience roots corpus: A 1.6\ntb composite multilingual dataset. Advances in Neural Information Processing Systems, 35:\n31809\u201331826, 2022.\n[44] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-Burch, and N. Carlini. Dedupli-\ncating training data makes language models better. arXiv preprint arXiv:2107.06499, 2021.\n[45] C. Leong, J. Nemecek, J. Mansdorfer, A. Filighera, A. Owodunni, and D. Whitenack. Bloom\nlibrary: Multimodal datasets in 300+ languages for a variety of downstream tasks. In Proceed-\nings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages\n8608\u20138621, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational\nLinguistics. URL https://aclanthology.org/2022.emnlp-main.590.\n[46] D. Liebling, K. Heller, S. Robertson, and W. Deng. Opportunities for human-centered evaluation\nof machine translation systems. In Findings of the Association for Computational Linguistics:\nNAACL 2022, pages 229\u2013240, 2022.\n[47] X. V. Lin, T. Mihaylov, M. Artetxe, T. Wang, S. Chen, D. Simig, M. Ott, N. Goyal, S. Bhos-\nale, J. Du, et al.\nFew-shot learning with multilingual language models.\narXiv preprint\narXiv:2112.10668, 2021.\n[48] A. S. Luccioni and J. D. Viviano. What\u2019s in the box? a preliminary analysis of undesirable\ncontent in the common crawl corpus. arXiv preprint arXiv:2105.02732, 2021.\n[49] T. Nakazawa, M. Yaguchi, K. Uchimoto, M. Utiyama, E. Sumita, S. Kurohashi, and H. Isahara.\nAspec: Asian scientific paper excerpt corpus.\nIn Proceedings of the Tenth International\nConference on Language Resources and Evaluation (LREC\u201916), pages 2204\u20132208, 2016.\n[50] G. Neubig. The Kyoto free translation task. http://www.phontron.com/kftt, 2011.\n[51] NLLBTeam, M. R. Costa-juss\u00e0, J. Cross, O. \u00c7elebi, M. Elbayad, K. Heafield, K. Heffernan,\nE. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood,\nB. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan,\nD. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao,\nV. Goswami, F. Guzm\u00e1n, P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, and\nJ. Wang. No language left behind: Scaling human-centered machine translation. 2022.\n[52] G. Orlanski, K. Xiao, X. Garcia, J. Hui, J. Howland, J. Malmaud, J. Austin, R. Singh, and\nM. Catasta. Measuring the impact of programming language distribution. arXiv preprint\narXiv:2302.01973, 2023.\n[53] A. Paullada, I. D. Raji, E. M. Bender, E. Denton, and A. Hanna. Data and its (dis) contents: A\nsurvey of dataset development and use in machine learning research. Patterns, 2(11):100336,\n2021.\n[54] J. Philip, V. P. Namboodiri, and C. Jawahar. A baseline neural machine translation system for\nindian languages. arXiv preprint arXiv:1907.12437, 2019.\n[55] M. Post. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference\non Machine Translation: Research Papers, pages 186\u2013191, Brussels, Belgium, Oct. 2018.\nAssociation for Computational Linguistics. doi: 10.18653/v1/W18-6319. URL https://\naclanthology.org/W18-6319.\n[56] R. Pryzant, Y. Chung, D. Jurafsky, and D. Britz. Jesc: Japanese-english subtitle corpus. arXiv\npreprint arXiv:1710.10639, 2017.\n[57] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu.\nExploring the limits of transfer learning with a unified text-to-text transformer. The Journal of\nMachine Learning Research, 21(1):5485\u20135551, 2020.\n14\n[58] N. Sambasivan, S. Kapania, H. Highfill, D. Akrong, P. Paritosh, and L. M. Aroyo. \u201ceveryone\nwants to do the model work, not the data work\u201d: Data cascades in high-stakes ai. In proceedings\nof the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u201315, 2021.\n[59] H. Schwenk, V. Chaudhary, S. Sun, H. Gong, and F. Guzm\u00e1n. Wikimatrix: Mining 135m\nparallel sentences in 1620 language pairs from wikipedia. arXiv preprint arXiv:1907.05791,\n2019.\n[60] R. Sennrich, B. Haddow, and A. Birch.\nImproving neural machine translation models\nwith monolingual data.\nIn Proceedings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany,\nAug. 2016. Association for Computational Linguistics. doi: 10.18653/v1/P16-1009. URL\nhttps://aclanthology.org/P16-1009.\n[61] A. Siddhant, A. Bapna, O. Firat, Y. Cao, M. X. Chen, I. Caswell, and X. Garcia. Towards\nthe next 1000 languages in multilingual machine translation: Exploring the synergy between\nsupervised and self-supervised learning. CoRR, abs/2201.03110, 2022. URL https://arxiv.\norg/abs/2201.03110.\n[62] K. Song, X. Tan, T. Qin, J. Lu, and T.-Y. Liu. Mass: Masked sequence to sequence pre-training\nfor language generation. arXiv preprint arXiv:1905.02450, 2019.\n[63] Y. Tay, M. Dehghani, V. Q. Tran, X. Garcia, D. Bahri, T. Schuster, H. S. Zheng, N. Houlsby, and\nD. Metzler. Unifying language learning paradigms. arXiv preprint arXiv:2205.05131, 2022.\n[64] J. Tiedemann. Parallel data, tools and interfaces in opus. In Lrec, volume 2012, pages 2214\u2013\n2218. Citeseer, 2012.\n[65] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.\ncc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.\n[66] D. Vilar, M. Freitag, C. Cherry, J. Luo, V. Ratnakar, and G. Foster. Prompting palm for\ntranslation: Assessing strategies and performance. arXiv preprint arXiv:2211.09102, 2022.\n[67] L. Weidinger, J. Mellor, M. Rauh, C. Griffin, J. Uesato, P.-S. Huang, M. Cheng, M. Glaese,\nB. Balle, A. Kasirzadeh, et al. Ethical and social risks of harm from language models. arXiv\npreprint arXiv:2112.04359, 2021.\n[68] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mt5:\nA massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934,\n2020.\n[69] L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raf-\nfel. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies, pages 483\u2013498, Online, June 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.41. URL\nhttps://aclanthology.org/2021.naacl-main.41.\n[70] Q. Ye, S. Devendra, F. Matthieu, P. Sarguna, and N. Graham. When and why are pre-trained\nword embeddings useful for neural machine translation. In HLT-NAACL, 2018.\n[71] B. Zhang, P. Williams, I. Titov, and R. Sennrich. Improving massively multilingual neural\nmachine translation and zero-shot translation. arXiv preprint arXiv:2004.11867, 2020.\n[72] B. Zhang, B. Haddow, and A. Birch. Prompting large language model for machine translation:\nA case study. arXiv preprint arXiv:2301.07069, 2023.\n[73] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the IEEE international conference on computer vision, pages 19\u201327,\n2015.\n[74] M. Ziemski, M. Junczys-Dowmunt, and B. Pouliquen. The united nations parallel corpus v1. 0.\nIn Proceedings of the Tenth International Conference on Language Resources and Evaluation\n(LREC\u201916), pages 3530\u20133534, 2016.\n15\nA\nAppendix\nA.1\nLangID Details\nFollowing Language Id In the Wild [15], we trained a Transformer-Base [65] Semi-Supervised LangId\nmodel (SSLID) on 498 languages. The training data is as described in Language ID in the Wild, with\nthe differences that 1) training data is sampled to a temperature of T=3 to reduce over-triggering on\nlow-resource languages; and 2) the data is supplemented with web-crawled data from the same paper\n(that has already been through the various filters described therein). The purpose of adding this data\nis to increase robustness to web-domain text, and possibly distill some of the filters used to create the\nweb-crawl. The languages chosen for this model were roughly the top 498 by number of sentences in\nthe dataset reported by Language ID in the Wild. The complete list may be seen in Table 9.\nTable 9: BCP-47 codes, name, script and amount of data associated with all languages in MADLAD-\n400. The last 79 languages, with entries of \"-\", were languages that the LangID model was trained to\ndetect, but was omitted after the self-audit.\nBCP-47\nName\nScript\ndocs (noisy)\ndocs (clean)\nsents (noisy)\nsents (clean)\nchars (noisy)\nchars (clean)\ntotal\n-\n-\n7.8B\n4B\n148.4B\n105.5B\n33.3T\n18.3T\nmedian\n-\n-\n21.6K\n1.5K\n202.3K\n61.8K\n63.6M\n8.5M\nen\nEnglish\nLatn\n3.6B\n1.8B\n87.9B\n53.4B\n15T\n9T\nru\nRussian\nCyrl\n823M\n402.5M\n823M\n12.4B\n3.1T\n1.8T\nes\nSpanish\nLatn\n476.4M\n250.9M\n8.3B\n4.5B\n2.1T\n1.1T\nfr\nFrench\nLatn\n384.2M\n218.9M\n7.9B\n5B\n2T\n1T\nde\nGerman\nLatn\n478.6M\n225.1M\n11.5B\n6B\n2.2T\n1T\nit\nItalian\nLatn\n238.9M\n126.4M\n4.5B\n2.5B\n1.2T\n553.1B\npt\nPortuguese\nLatn\n209.2M\n124.2M\n4B\n2.4B\n791.5B\n499.8B\npl\nPolish\nLatn\n145.1M\n90.9M\n3.3B\n2.4B\n505B\n356.4B\nnl\nDutch\nLatn\n134.5M\n86.6M\n134.5M\n2.3B\n698.5B\n334.5B\nvi\nVietnamese\nLatn\n92.8M\n55M\n1.6B\n1B\n342B\n228.8B\ntr\nTurkish\nLatn\n107M\n56.4M\n107M\n1.2B\n328.8B\n198.9B\nsv\nSwedish\nLatn\n65.2M\n35.2M\n65.2M\n1B\n422.6B\n153.7B\nid\nIndonesian\nLatn\n120.9M\n38M\n2.2B\n747.5M\n443B\n148.3B\nro\nRomanian\nLatn\n60.8M\n35.4M\n60.8M\n746.4M\n244.1B\n148.2B\ncs\nCzech\nLatn\n72.1M\n38.3M\n1.7B\n1B\n272.2B\n147.9B\nzh\nMandarin Chinese\nHans\n29.3M\n19.9M\n492.3M\n298.8M\n333B\n142.3B\nhu\nHungarian\nLatn\n47.6M\n29.7M\n1.3B\n806.3M\n223.6B\n134.9B\nja\nJapanese\nJpan\n23.3M\n21.8M\n326M\n321.6M\n133.3B\n132.2B\nth\nThai\nThai\n19M\n17.4M\n19M\n385.8M\n118.6B\n117.6B\nfi\nFinnish\nLatn\n35.8M\n20.4M\n1B\n650.3M\n202.2B\n101.1B\nfa\nPersian\nArab\n58.1M\n23.1M\n920.6M\n493.5M\n220.4B\n96.7B\nuk\nUkrainian\nCyrl\n46.6M\n25M\n1B\n599.9M\n164.2B\n95.2B\nda\nDanish\nLatn\n38.5M\n17.9M\n1.1B\n508M\n252B\n83.1B\nel\nGreek\nGrek\n52.4M\n20.9M\n808M\n445.4M\n173.2B\n80.9B\nno\nNorwegian\nLatn\n34.7M\n14.9M\n34.7M\n498.7M\n305.6B\n74.8B\nbg\nBulgarian\nCyrl\n27.2M\n12.8M\n599.4M\n360.3M\n95.6B\n57.8B\nsk\nSlovak\nLatn\n23.2M\n11.9M\n487.9M\n300.6M\n77.8B\n45.7B\nko\nKorean\nKore\n19.7M\n12.7M\n628.6M\n471.8M\n65.9B\n43.8B\nar\nArabic\nArab\n67.6M\n12.4M\n876.6M\n182.6M\n243B\n43.2B\nlt\nLithuanian\nLatn\n15.3M\n8.7M\n374M\n256.9M\n58.6B\n41.3B\nca\nCatalan\nLatn\n17.9M\n9.5M\n258.6M\n153M\n56.5B\n34.6B\nsl\nSlovenian\nLatn\n12M\n6.3M\n316M\n180M\n47.8B\n30.5B\nhe\nHebrew\nHebr\n14.1M\n7.2M\n302.2M\n196.8M\n54.9B\n30.5B\net\nEstonian\nLatn\n8.8M\n5.5M\n223.8M\n176.3M\n40.1B\n28.7B\nlv\nLatvian\nLatn\n8.4M\n5M\n186.1M\n138.5M\n36.7B\n23.9B\nhi\nHindi\nDeva\n9.9M\n4.5M\n254.4M\n152M\n39.9B\n20.1B\nsq\nAlbanian\nLatn\n5.5M\n3.6M\n5.5M\n56.1M\n17B\n12.7B\nms\nMalay\nLatn\n14.1M\n2.3M\n14.1M\n55.2M\n58.8B\n12.5B\naz\nAzerbaijani\nLatn\n5.2M\n3.3M\n90.3M\n70.9M\n16.3B\n11.9B\nsr\nSerbian\nCyrl\n4.7M\n2M\n4.7M\n64M\n18.6B\n11B\nta\nTamil\nTaml\n5.6M\n2.6M\n122.5M\n81.9M\n19.2B\n10.6B\nhr\nCroatian\nLatn\n23M\n2.8M\n476.6M\n53M\n85.1B\n9.6B\nkk\nKazakh\nCyrl\n3.1M\n1.8M\n87.4M\n59.1M\n13.4B\n8.6B\nis\nIcelandic\nLatn\n2.9M\n1.6M\n73.7M\n39.3M\n14.9B\n6.4B\nml\nMalayalam\nMlym\n3.7M\n2.1M\n75M\n52M\n10.5B\n6.3B\nmr\nMarathi\nDeva\n2.9M\n1.7M\n2.9M\n50M\n8.7B\n5.5B\nte\nTelugu\nTelu\n2.5M\n1.7M\n59M\n46.4M\n7.4B\n5.1B\naf\nAfrikaans\nLatn\n2.9M\n868.7K\n51.9M\n30M\n11.8B\n4.8B\ngl\nGalician\nLatn\n4.2M\n1.3M\n45.3M\n18.8M\n15.6B\n4.8B\nfil\nFilipino\nLatn\n4.2M\n901.5K\n67.4M\n19.2M\n14.6B\n4.7B\nbe\nBelarusian\nCyrl\n2M\n1.1M\n48.8M\n31.3M\n7.2B\n4.6B\n16\nmk\nMacedonian\nCyrl\n2.9M\n1.4M\n41.3M\n22.6M\n9.1B\n4.5B\neu\nBasque\nLatn\n2.1M\n1.2M\n41.7M\n24.8M\n6.9B\n4.3B\nbn\nBengali\nBeng\n4.3M\n1.1M\n151.2M\n38.6M\n16.8B\n4.3B\nka\nGeorgian\nGeor\n3.1M\n936.5K\n53.7M\n26.6M\n10.3B\n3.8B\nmn\nMongolian\nCyrl\n2.2M\n879.9K\n43.3M\n24M\n7.9B\n3.5B\nbs\nBosnian\nCyrl\n12.9M\n1.4M\n163.6M\n9M\n39.5B\n3.3B\nuz\nUzbek\nLatn\n1.4M\n669.9K\n25.7M\n17.5M\n5.2B\n3.3B\nur\nUrdu\nArab\n967.2K\n467.2K\n29M\n18.4M\n5.2B\n2.7B\nsw\nSwahili\nLatn\n1.3M\n537.8K\n1.3M\n9.5M\n4.6B\n2.4B\nyue\nCantonese\nHant\n465.9K\n309.3K\n2.8M\n2.4M\n2.4B\n2.3B\nne\nNepali\nDeva\n876.4K\n453.3K\n876.4K\n20.4M\n3.9B\n2.2B\nkn\nKannada\nKnda\n1.6M\n657.8K\n32.9M\n19.2M\n4.6B\n2.2B\nkaa\nKara-Kalpak\nCyrl\n1.1M\n586.4K\n19.8M\n13.3M\n3.8B\n2.2B\ngu\nGujarati\nGujr\n1.3M\n659.7K\n28.9M\n18.1M\n3.9B\n2.1B\nsi\nSinhala\nSinh\n788K\n349.2K\n22.1M\n16M\n3.4B\n1.9B\ncy\nWelsh\nLatn\n4.9M\n430.7K\n68.3M\n7.4M\n26.4B\n1.7B\neo\nEsperanto\nLatn\n1.4M\n260K\n33.9M\n9.3M\n5.5B\n1.7B\nla\nLatin\nLatn\n2.9M\n319.2K\n85.7M\n13.8M\n8.2B\n1.5B\nhy\nArmenian\nArmn\n2M\n397.5K\n31.1M\n9.9M\n8.1B\n1.5B\nky\nKyrghyz\nCyrl\n751.1K\n367.6K\n14.3M\n9.6M\n2.5B\n1.4B\ntg\nTajik\nCyrl\n789.2K\n328.2K\n789.2K\n7.4M\n2.6B\n1.4B\nga\nIrish\nLatn\n5.3M\n286K\n31.7M\n6.9M\n30.6B\n1.4B\nmt\nMaltese\nLatn\n1.2M\n265.4K\n1.2M\n5.6M\n3.2B\n1.3B\nmy\nMyanmar (Burmese)\nMymr\n176.5K\n172.4K\n176.5K\n10.1M\n1.3B\n1.3B\nkm\nKhmer\nKhmr\n297.8K\n285.7K\n5M\n5M\n1.1B\n1.1B\ntt\nTatar\nCyrl\n2.1M\n346.9K\n60.2M\n8.6M\n12.1B\n1B\nso\nSomali\nLatn\n729.2K\n293.2K\n729.2K\n3.1M\n2.1B\n992.4M\nku\nKurdish (Kurmanji)\nLatn\n671.9K\n218.9K\n10.7M\n4.9M\n2.1B\n849.9M\nps\nPashto\nArab\n429.9K\n252.9K\n5.1M\n3.6M\n1.4B\n848.9M\npa\nPunjabi\nGuru\n368.2K\n150.6K\n368.2K\n6M\n1.6B\n797.1M\nrw\nKinyarwanda\nLatn\n681.8K\n226.5K\n681.8K\n1.9M\n1.7B\n749.1M\nlo\nLao\nLaoo\n229.1K\n216K\n2.9M\n2.8M\n706.9M\n697.6M\nha\nHausa\nLatn\n443.9K\n173.5K\n4.5M\n2.4M\n1.3B\n630.2M\ndv\nDhivehi\nThaa\n264.4K\n167.2K\n4.3M\n3.5M\n877.3M\n603.1M\nfy\nW. Frisian\nLatn\n1.7M\n210K\n12.1M\n3.7M\n3.7B\n592.3M\nlb\nLuxembourgish\nLatn\n7.6M\n146K\n47.1M\n3.4M\n58.4B\n575.5M\nckb\nKurdish (Sorani)\nArab\n622.7K\n148.9K\n5.6M\n2.5M\n2.2B\n572.7M\nmg\nMalagasy\nLatn\n295.2K\n115.4K\n4.5M\n2.6M\n1.3B\n548.5M\ngd\nScottish Gaelic\nLatn\n206K\n94.3K\n3.7M\n2.4M\n812M\n526M\nam\nAmharic\nEthi\n245.2K\n106.3K\n7.1M\n5.3M\n869.9M\n509M\nug\nUyghur\nArab\n227.1K\n106.5K\n4.5M\n3.1M\n998.5M\n504.6M\nht\nHaitian Creole\nLatn\n425.6K\n110.4K\n6.7M\n2.6M\n994.5M\n461.5M\ngrc\nAncient Greek\nGrek\n364.8K\n70.7K\n13.7M\n2.8M\n2B\n417.8M\nhmn\nHmong\nLatn\n241.3K\n75.2K\n3.5M\n1.9M\n1.2B\n408.8M\nsd\nSindhi\nArab\n115.6K\n65.9K\n115.6K\n2.4M\n561M\n380.4M\njv\nJavanese\nLatn\n999.5K\n69.5K\n13M\n2M\n2.3B\n376.1M\nmi\nMaori\nLatn\n711.9K\n79.5K\n5.9M\n1.9M\n1.6B\n371.9M\ntk\nTurkmen\nLatn\n180.2K\n82.5K\n180.2K\n1.8M\n575.2M\n369M\nceb\nCebuano\nLatn\n617.5K\n66.2K\n6.7M\n1.6M\n1.5B\n357.7M\nyi\nYiddish\nHebr\n160.6K\n64.9K\n3.3M\n1.9M\n838.4M\n352.6M\nba\nBashkir\nCyrl\n372.4K\n90.3K\n9.3M\n2.6M\n766.5M\n320.7M\nfo\nFaroese\nLatn\n382.9K\n97.8K\n3.9M\n1.8M\n923.3M\n314.9M\nor\nOdia (Oriya)\nOrya\n139.6K\n100.5K\n139.6K\n3.1M\n437.2M\n309.5M\nxh\nXhosa\nLatn\n310.9K\n53.7K\n2.9M\n1.4M\n749.5M\n287.3M\nsu\nSundanese\nLatn\n336.6K\n55K\n336.6K\n1.6M\n967.2M\n286.7M\nkl\nKalaallisut\nLatn\n85.9K\n46K\n2.1M\n1.5M\n403.9M\n279.1M\nny\nChichewa\nLatn\n181.6K\n52.2K\n181.6K\n1.5M\n611.2M\n277.5M\nsm\nSamoan\nLatn\n137.8K\n52.6K\n1.9M\n1.3M\n607.9M\n276.3M\nsn\nShona\nLatn\n3.1M\n60.2K\n3.1M\n1.2M\n10.6B\n266M\nco\nCorsican\nLatn\n546.7K\n55.4K\n6.1M\n1.3M\n1.1B\n265.5M\nzu\nZulu\nLatn\n372.3K\n53.8K\n3.8M\n1.2M\n1.2B\n257.4M\nig\nIgbo\nLatn\n130.4K\n54.4K\n2.1M\n1.4M\n846.1M\n251.4M\nyo\nYoruba\nLatn\n115K\n52.1K\n2M\n1.2M\n415.6M\n239M\npap\nPapiamento\nLatn\n259.1K\n54.5K\n259.1K\n1.4M\n1.4B\n229.9M\nst\nSesotho\nLatn\n96.8K\n40.4K\n96.8K\n1.1M\n381.5M\n226.9M\nhaw\nHawaiian\nLatn\n310.4K\n45.7K\n7.1M\n1M\n892M\n214.2M\nas\nAssamese\nBeng\n53.9K\n33.8K\n2.4M\n1.7M\n275.8M\n182.1M\noc\nOccitan\nLatn\n2.4M\n36.4K\n2.4M\n1.6M\n6.7B\n177.6M\ncv\nChuvash\nCyrl\n599.4K\n47.3K\n12M\n1.6M\n1B\n168.9M\nlus\nMizo\nLatn\n91.5K\n36.4K\n1.4M\n863.5K\n298.3M\n167.3M\ntet\nTetum\nLatn\n291K\n40.4K\n1.9M\n475.7K\n1.6B\n152.3M\ngsw\nSwiss German\nLatn\n7.6M\n42.7K\n64.5M\n1M\n42.3B\n149.2M\nsah\nYakut\nCyrl\n1.3M\n29.2K\n1.3M\n1.2M\n2.2B\n148.2M\nbr\nBreton\nLatn\n705.4K\n33.2K\n7.8M\n731.7K\n3.7B\n125.4M\nrm\nRomansh\nLatn\n238.1K\n33.8K\n238.1K\n603.4K\n391M\n100.2M\nsa\nSanskrit\nDeva\n154.3K\n7.1K\n154.3K\n1.1M\n512.5M\n88.8M\nbo\nTibetan\nTibt\n6.2K\n6.2K\n1.1M\n1.1M\n88.7M\n88.7M\nom\nOromo\nLatn\n846.1K\n18.9K\n846.1K\n469.8K\n1.9B\n88.5M\nse\nN. Sami\nLatn\n54.3K\n23.9K\n879.5K\n493.3K\n148.4M\n84.6M\nce\nChechen\nCyrl\n59.3K\n15K\n991.1K\n460.1K\n130.6M\n67.8M\ncnh\nHakha Chin\nLatn\n44.4K\n21.6K\n688.6K\n406.9K\n110.8M\n63M\n17\nilo\nIlocano\nLatn\n69.8K\n11.8K\n889.2K\n365.1K\n187.9M\n59.4M\nhil\nHiligaynon\nLatn\n126.8K\n10.6K\n1.1M\n379.7K\n293.5M\n57.2M\nudm\nUdmurt\nCyrl\n67.1K\n13.4K\n942.7K\n510.3K\n106M\n55.5M\nos\nOssetian\nCyrl\n172.1K\n12.6K\n172.1K\n359.3K\n233.5M\n50.1M\nlg\nLuganda\nLatn\n61.1K\n13K\n510.9K\n166.1K\n160.7M\n48M\nti\nTigrinya\nEthi\n20.8K\n7.3K\n20.8K\n481.3K\n95.4M\n44.6M\nvec\nVenetian\nLatn\n1.1M\n11.1K\n10M\n209.7K\n1.8B\n43.8M\nts\nTsonga\nLatn\n34.7K\n5.2K\n34.7K\n248.6K\n377.2M\n38.8M\ntyv\nTuvinian\nCyrl\n61.6K\n9.1K\n596.6K\n268.3K\n80.2M\n38.5M\nkbd\nKabardian\nCyrl\n154.7K\n7.5K\n1.4M\n257.2K\n321.4M\n36.8M\nee\nEwe\nLatn\n14.1K\n4.5K\n353.6K\n246.7K\n67.9M\n32.8M\niba\nIban\nLatn\n34K\n7.6K\n326.9K\n126.1K\n251.4M\n30.5M\nav\nAvar\nCyrl\n107.6K\n6.3K\n806.1K\n190.1K\n129M\n30.2M\nkha\nKhasi\nLatn\n37.8K\n12.1K\n235.5K\n75.2K\n88.6M\n30.2M\nto\nTonga (Tonga Islands)\nLatn\n14.3K\n4.6K\n14.3K\n149K\n58.2M\n29.9M\ntn\nTswana\nLatn\n138.2K\n4.8K\n138.2K\n174.4K\n302.3M\n29.2M\nnso\nSepedi\nLatn\n376.2K\n4.4K\n376.2K\n188.4K\n2B\n28.2M\nfj\nFijian\nLatn\n17K\n4K\n410K\n164.1K\n67.7M\n28M\nzza\nZaza\nLatn\n370.1K\n6K\n3.3M\n229.2K\n617.3M\n26.3M\nak\nTwi\nLatn\n19.5K\n4.8K\n341.7K\n210.2K\n74.5M\n24.8M\nada\nAdangme\nLatn\n6.5K\n3.1K\n291.5K\n199.2K\n38.9M\n24.2M\notq\nQuer\u00e9taro Otomi\nLatn\n17.6K\n5.6K\n17.6K\n114.8K\n65M\n23.4M\ndz\nDzongkha\nTibt\n1.9K\n1.9K\n191.7K\n191.7K\n22.7M\n22.7M\nbua\nBuryat\nCyrl\n9.8K\n5.3K\n252K\n144.6K\n38M\n21.7M\ncfm\nFalam Chin\nLatn\n9.1K\n4.9K\n199.6K\n128.6K\n32.9M\n21.5M\nln\nLingala\nLatn\n94.7K\n3.3K\n718.7K\n139K\n291.8M\n21.5M\nchm\nMeadow Mari\nCyrl\n81.5K\n4.7K\n929.1K\n179.7K\n132.2M\n21.3M\ngn\nGuarani\nLatn\n87.1K\n3.9K\n770.9K\n162.6K\n140.7M\n20.8M\nkrc\nKarachay-Balkar\nCyrl\n359.5K\n4.8K\n2.3M\n153.9K\n369.5M\n20.7M\nwa\nWalloon\nLatn\n70.6K\n2.8K\n1.5M\n127.2K\n198.8M\n20.4M\nhif\nFiji Hindi\nLatn\n702K\n2.4K\n7.9M\n124.7K\n9.1B\n19.1M\nyua\nYucateco\nLatn\n10.4K\n4K\n141.6K\n77.6K\n36.8M\n17.2M\nsrn\nSranan Tongo\nLatn\n16.7K\n2.3K\n16.7K\n139.5K\n49.1M\n17M\nwar\nWaray (Philippines)\nLatn\n1M\n2.9K\n114M\n96.2K\n3.5B\n16.1M\nrom\nRomani\nLatn\n22.9K\n4.2K\n22.9K\n76.1K\n59M\n15.9M\nbik\nCentral Bikol\nLatn\n44.8K\n3.1K\n376.7K\n77K\n102.3M\n15.7M\npam\nPampanga\nLatn\n174.2K\n2.8K\n174.2K\n23.3K\n324M\n15.5M\nsg\nSango\nLatn\n4.2K\n2.1K\n154K\n117.9K\n22.6M\n15.5M\nlu\nLuba-Katanga\nLatn\n10.6K\n1.4K\n316K\n112.1K\n54.2M\n15.4M\nady\nAdyghe\nCyrl\n74.9K\n4.2K\n446.8K\n96.9K\n67.9M\n14.8M\nkbp\nKabiy\u00e8\nLatn\n5.9K\n3K\n247.9K\n128.3K\n30.8M\n14.6M\nsyr\nSyriac\nSyrc\n3.5K\n716\n326.4K\n197.1K\n31.5M\n14M\nltg\nLatgalian\nLatn\n13.1K\n4.1K\n213.7K\n87.3K\n29.2M\n13.9M\nmyv\nErzya\nCyrl\n164.8K\n3.1K\n164.8K\n130K\n120.3M\n13.8M\niso\nIsoko\nLatn\n3.7K\n1.7K\n155.8K\n111.5K\n23M\n13.7M\nkac\nKachin\nLatn\n5.9K\n2.6K\n109.2K\n77.4K\n26.6M\n13.6M\nbho\nBhojpuri\nDeva\n13.6K\n4.1K\n306.2K\n118.5K\n37.6M\n13.4M\nay\nAymara\nLatn\n8.1K\n2.5K\n196.7K\n83.8K\n34.5M\n13.1M\nkum\nKumyk\nCyrl\n4.2K\n2.5K\n132.2K\n89.7K\n18.2M\n12.4M\nqu\nQuechua\nLatn\n149.7K\n2.4K\n1M\n87K\n200.6M\n12.2M\nza\nZhuang\nLatn\n824.7K\n1.7K\n19.2M\n53.9K\n3B\n12.1M\npag\nPangasinan\nLatn\n49.6K\n1.6K\n49.6K\n88.8K\n92.9M\n12M\nngu\nGuerrero Nahuatl\nLatn\n3.8K\n1.5K\n3.8K\n87.1K\n21.4M\n11.8M\nve\nVenda\nLatn\n3.8K\n1.9K\n97.8K\n79.4K\n19M\n11.7M\npck\nPaite Chin\nLatn\n8.9K\n1.3K\n8.9K\n69.7K\n39.8M\n11.5M\nzap\nZapotec\nLatn\n5.5K\n1.8K\n202.3K\n93.5K\n26.4M\n11.4M\ntyz\nT\u00e0y\nLatn\n8K\n1.7K\n454.8K\n104.6K\n46.3M\n11.3M\nhui\nHuli\nLatn\n2K\n1.7K\n80.1K\n74.7K\n11.8M\n10.9M\nbbc\nBatak Toba\nLatn\n72.3K\n1.3K\n718.3K\n73.2K\n151.3M\n10.6M\ntzo\nTzotzil\nLatn\n2.8K\n1.4K\n100.4K\n75.7K\n15.9M\n10.6M\ntiv\nTiv\nLatn\n3.8K\n1.1K\n3.8K\n80.7K\n20.4M\n10.2M\nksd\nKuanua\nLatn\n14.9K\n2K\n533K\n78.6K\n62.4M\n10M\ngom\nGoan Konkani\nDeva\n4.6K\n2.1K\n178.3K\n108K\n19.8M\n10M\nmin\nMinangkabau\nLatn\n28.2K\n1.5K\n500.9K\n75.6K\n70.5M\n9.9M\nang\nOld English\nLatn\n66.5K\n803\n1.8M\n86.7K\n193M\n9.8M\nnhe\nE. Huasteca Nahuatl\nLatn\n3K\n1.7K\n3K\n57.7K\n15.6M\n9.8M\nbgp\nE. Baluchi\nLatn\n355.7K\n2.4K\n5.6M\n43.3K\n1.1B\n9.8M\nnzi\nNzima\nLatn\n2.5K\n1.4K\n2.5K\n71.8K\n14.4M\n9.4M\nnnb\nNande\nLatn\n4.9K\n1.1K\n4.9K\n70.2K\n27.7M\n9.1M\nnv\nNavajo\nLatn\n17.1K\n12.6K\n17.1K\n86.5K\n24.8M\n9.1M\nzxx\nNoise\n-\n118.8K\n1.8K\n3.8M\n49.3K\n501K\n6.6K\nbci\nBaoul\u00e9\nLatn\n7.4K\n1.3K\n124.8K\n87.1K\n32.8M\n9M\nkv\nKomi\nCyrl\n59.1K\n1.9K\n584.3K\n88.8K\n91.4M\n9M\nnew\nNewari\nDeva\n6.6K\n1.6K\n6.6K\n85K\n21.2M\n8.8M\nmps\nDadibi\nLatn\n2.7K\n1.2K\n132.8K\n71.9K\n16M\n8.7M\nalt\nS. Altai\nCyrl\n2.6K\n1.4K\n110.1K\n65.9K\n14.3M\n8.7M\nmeu\nMotu\nLatn\n5.9K\n1.7K\n232.1K\n72.6K\n27.2M\n8.6M\nbew\nBetawi\nLatn\n311.1K\n2.7K\n10.4M\n58.4K\n1.4B\n8.5M\nfon\nFon\nLatn\n5.3K\n1.1K\n222.9K\n67.3K\n34M\n8.3M\niu\nInuktitut\nCans\n5.4K\n2.5K\n92.6K\n53.1K\n17.5M\n8.3M\nabt\nAmbulas\nLatn\n1.6K\n1.3K\n122.7K\n110.3K\n9.6M\n8.2M\n18\nmgh\nMakhuwa-Meetto\nLatn\n5.5K\n1.2K\n151.8K\n61.2K\n24.1M\n8.2M\nmnw\nMon\nMymr\n1.1K\n1.1K\n144.8K\n144.7K\n8.1M\n8.1M\ntvl\nTuvalu\nLatn\n2.3K\n933\n72.9K\n53.6K\n12.6M\n8.1M\ndov\nDombe\nLatn\n3.5K\n923\n129.8K\n56.7K\n20.7M\n8M\ntlh\nKlingon\nLatn\n516.9K\n3.1K\n516.9K\n46.9K\n1.4B\n7.8M\nho\nHiri Motu\nLatn\n2K\n1.5K\n57K\n47.8K\n12.3M\n7.8M\nkw\nCornish\nLatn\n176.9K\n2.3K\n1M\n51.6K\n327.8M\n7.7M\nmrj\nHill Mari\nCyrl\n97.1K\n1.4K\n97.1K\n60.3K\n100.6M\n7.6M\nmeo\nKedah Malay\nLatn\n790.7K\n4.7K\n16.5M\n39K\n3B\n7.5M\ncrh\nCrimean Tatar\nCyrl\n5.1K\n1.2K\n170.9K\n61.8K\n18.8M\n7.5M\nmbt\nMatigsalug Manobo\nLatn\n1.6K\n969\n86K\n45.4K\n14.6M\n7.5M\nemp\nN. Ember\u00e1\nLatn\n3.6K\n1.2K\n106.4K\n75.4K\n14.5M\n7.4M\nace\nAchinese\nLatn\n65.5K\n966\n632.5K\n32.5K\n146.1M\n7.4M\nium\nIu Mien\nLatn\n100.3K\n1.7K\n6.2M\n54.9K\n314M\n7.4M\nmam\nMam\nLatn\n23K\n1.5K\n446.3K\n52.9K\n70.4M\n7.2M\ngym\nNg\u00e4bere\nLatn\n1.5K\n820\n73.7K\n49.6K\n10.3M\n6.9M\nmai\nMaithili\nDeva\n54.3K\n1.2K\n1M\n60.2K\n156M\n6.8M\ncrs\nSeselwa Creole French\nLatn\n7.6K\n873\n282.4K\n40.1K\n40.1M\n6.8M\npon\nPohnpeian\nLatn\n5.7K\n1.5K\n167.8K\n48.7K\n18.3M\n6.7M\nubu\nUmbu-Ungu\nLatn\n2.2K\n846\n113.5K\n47.5K\n15.9M\n6.7M\nfip\nFipa\nLatn\n3.7K\n729\n165.6K\n49K\n25.7M\n6.6M\nquc\nK\u2019iche\u2019\nLatn\n4.4K\n1.5K\n89.2K\n41.2K\n16.6M\n6.4M\ngv\nManx\nLatn\n501.9K\n1.6K\n18.8M\n26.9K\n933.1M\n6.2M\nkj\nKuanyama\nLatn\n112.2K\n2.1K\n881.8K\n22.6K\n339.6M\n6M\nbtx\nBatak Karo\nLatn\n3.1K\n1K\n81.7K\n43.9K\n13.1M\n5.9M\nape\nBukiyip\nLatn\n7K\n814\n147K\n56.1K\n71M\n5.8M\nchk\nChuukese\nLatn\n2.8K\n1.1K\n98.8K\n44K\n12M\n5.8M\nrcf\nR\u00e9union Creole French\nLatn\n21.6K\n2.6K\n21.6K\n50.5K\n30.2M\n5.7M\nshn\nShan\nMymr\n889\n788\n46.4K\n46.2K\n5.7M\n5.7M\ntzh\nTzeltal\nLatn\n1.7K\n702\n41.7K\n33.9K\n9.3M\n5.6M\nmdf\nMoksha\nCyrl\n71K\n1.6K\n394.7K\n45.1K\n65.8M\n5.5M\nppk\nUma\nLatn\n2.6K\n1.1K\n85.8K\n34.9K\n13.2M\n5.5M\nss\nSwati\nLatn\n8.1K\n1.1K\n8.1K\n30.4K\n23.7M\n5.5M\ngag\nGagauz\nLatn\n33.9K\n1.6K\n491K\n37K\n84.9M\n5.2M\ncab\nGarifuna\nLatn\n1.2K\n629\n50.4K\n37.5K\n7.5M\n5.1M\nkri\nKrio\nLatn\n39.1K\n786\n271.2K\n38.8K\n86.4M\n5M\nseh\nSena\nLatn\n5.6K\n545\n68.8K\n37.2K\n14.9M\n4.9M\nibb\nIbibio\nLatn\n74.1K\n818\n516.5K\n36.3K\n190.9M\n4.9M\ntbz\nDitammari\nLatn\n5.1K\n1.1K\n128.7K\n37.5K\n22M\n4.8M\nbru\nE. Bru\nLatn\n3K\n1.1K\n89.7K\n48.2K\n12.9M\n4.8M\nenq\nEnga\nLatn\n7.1K\n793\n241.9K\n39.1K\n68.5M\n4.8M\nach\nAcoli\nLatn\n2K\n915\n63K\n40.1K\n9M\n4.7M\ncuk\nSan Blas Kuna\nLatn\n4.1K\n899\n76.5K\n34.3K\n24.7M\n4.6M\nkmb\nKimbundu\nLatn\n1.3K\n538\n60.4K\n36.9K\n8.4M\n4.6M\nwo\nWolof\nLatn\n36.4K\n871\n303.4K\n25.4K\n213.4M\n4.5M\nkek\nKekch\u00ed\nLatn\n3.2K\n782\n70.4K\n38.4K\n13.6M\n4.4M\nqub\nHuallaga Hu\u00e1nuco Quechua\nLatn\n972\n705\n61K\n51.1K\n5.9M\n4.4M\ntab\nTabassaran\nCyrl\n7.8K\n1.2K\n226.4K\n26.8K\n33.7M\n4.4M\nbts\nBatak Simalungun\nLatn\n3.2K\n869\n109.1K\n29.1K\n20.8M\n4.2M\nkos\nKosraean\nLatn\n2.2K\n881\n44.6K\n27.8K\n6.5M\n4.2M\nrwo\nRawa\nLatn\n938\n572\n938\n45.5K\n5.1M\n4.2M\ncak\nKaqchikel\nLatn\n1.2K\n617\n70.4K\n32.6K\n7.6M\n4.2M\ntuc\nMutu\nLatn\n3.5K\n635\n193.2K\n50.3K\n17.2M\n4.1M\nbum\nBulu\nLatn\n4.7K\n559\n103.8K\n36.5K\n18.8M\n4M\ncjk\nChokwe\nLatn\n3.6K\n586\n144.1K\n24.1K\n22.5M\n3.9M\ngil\nGilbertese\nLatn\n3.9K\n586\n151.5K\n24.1K\n24.1M\n3.9M\nstq\nSaterfriesisch\nLatn\n111.9K\n809\n111.9K\n27.7K\n243.1M\n3.8M\ntsg\nTausug\nLatn\n353.8K\n789\n353.8K\n17.9K\n1.1B\n3.8M\nquh\nS. Bolivian Quechua\nLatn\n1K\n501\n42K\n29.9K\n5.8M\n3.7M\nmak\nMakasar\nLatn\n1K\n555\n32.5K\n20.4K\n6.1M\n3.7M\narn\nMapudungun\nLatn\n2.4K\n593\n64.5K\n26.2K\n10.2M\n3.7M\nban\nBalinese\nLatn\n8K\n637\n150.9K\n16.3K\n35.4M\n3.6M\njiv\nShuar\nLatn\n1.7K\n696\n80.9K\n32K\n9.6M\n3.5M\nsja\nEpena\nLatn\n1.3K\n527\n67.7K\n24.9K\n7.7M\n3.4M\nyap\nYapese\nLatn\n1.9K\n638\n37.6K\n19.5K\n6.9M\n3.3M\ntcy\nTulu\nKnda\n10.7K\n632\n338.7K\n37.1K\n41.6M\n3.3M\ntoj\nTojolabal\nLatn\n736\n452\n736\n26.1K\n4.3M\n3.3M\ntwu\nTermanu\nLatn\n2.5K\n539\n109.9K\n24.4K\n14.2M\n3.2M\nxal\nKalmyk\nCyrl\n71.8K\n913\n498.5K\n30.8K\n64.7M\n3.2M\namu\nGuerrero Amuzgo\nLatn\n1.8K\n511\n72K\n25.2K\n9.6M\n3.2M\nrmc\nCarpathian Romani\nLatn\n2.4K\n738\n2.4K\n25.8K\n7.9M\n3.2M\nhus\nHuastec\nLatn\n825\n569\n26.5K\n23.7K\n4.4M\n3.1M\nnia\nNias\nLatn\n2K\n408\n2K\n25K\n11.3M\n3.1M\nkjh\nKhakas\nCyrl\n1.5K\n672\n42.8K\n28.7K\n4.5M\n3.1M\nbm\nBambara\nLatn\n21.9K\n702\n172.3K\n24.5K\n48.4M\n3M\nguh\nGuahibo\nLatn\n1.9K\n331\n104.9K\n28.4K\n11.2M\n3M\nmas\nMasai\nLatn\n15.2K\n405\n216.8K\n17.6K\n42.1M\n3M\nacf\nSt Lucian Creole French\nLatn\n4.9K\n730\n81.9K\n24.6K\n11.6M\n3M\ndtp\nKadazan Dusun\nLatn\n4.6K\n1.3K\n51.2K\n7.9K\n12.7M\n3M\nksw\nS\u2019gaw Karen\nMymr\n560\n536\n16.1K\n16K\n2.9M\n2.9M\nbzj\nBelize Kriol English\nLatn\n983\n404\n33.6K\n26.4K\n4.5M\n2.9M\n19\ndin\nDinka\nLatn\n128.4K\n611\n885.8K\n23.6K\n210M\n2.9M\nzne\nZande\nLatn\n1.3K\n239\n61.9K\n21.3K\n8.2M\n2.8M\nmad\nMadurese\nLatn\n103.8K\n509\n500.6K\n18.5K\n111.8M\n2.8M\nmsi\nSabah Malay\nLatn\n686.7K\n1.9K\n686.7K\n22.6K\n2.6B\n2.7M\nmag\nMagahi\nDeva\n631\n138\n62.6K\n22.1K\n10.7M\n2.6M\nmkn\nKupang Malay\nLatn\n956\n402\n33.1K\n25.4K\n3.4M\n2.6M\nkg\nKongo\nLatn\n4.7K\n365\n85.5K\n21.7K\n16.6M\n2.6M\nlhu\nLahu\nLatn\n46K\n377\n975K\n15.7K\n208.6M\n2.5M\nch\nChamorro\nLatn\n12.9K\n449\n147.5K\n16K\n63.5M\n2.5M\nqvi\nImbabura H. Quichua\nLatn\n1.2K\n266\n48.4K\n19.3K\n6.5M\n2.3M\nmh\nMarshallese\nLatn\n4.6K\n296\n235.1K\n13K\n24.9M\n2.2M\ndjk\nE. Maroon Creole\nLatn\n560\n246\n30.9K\n24.4K\n3.7M\n2.2M\nsus\nSusu\nLatn\n664\n437\n664\n15.2K\n3.7M\n2.1M\nmfe\nMorisien\nLatn\n7.5K\n320\n198.8K\n18.2K\n26.9M\n2.1M\nsrm\nSaramaccan\nLatn\n847\n227\n847\n17.3K\n6.3M\n2M\ndyu\nDyula\nLatn\n1.2K\n483\n55.8K\n19.7K\n5.7M\n2M\nctu\nChol\nLatn\n690\n366\n35.5K\n20.6K\n3.6M\n2M\ngui\nE. Bolivian Guaran\u00ed\nLatn\n1.1K\n409\n62.7K\n24.8K\n6.5M\n2M\npau\nPalauan\nLatn\n1.7K\n185\n1.7K\n13.1K\n12.4M\n2M\ninb\nInga\nLatn\n387\n343\n17.3K\n17K\n2M\n1.9M\nbi\nBislama\nLatn\n71.9K\n311\n308.5K\n13.6K\n132.4M\n1.9M\nmni\nMeiteilon (Manipuri)\nBeng\n1.2K\n290\n38.1K\n13.2K\n6.4M\n1.8M\nguc\nWayuu\nLatn\n537\n214\n22.9K\n12.5K\n3.4M\n1.8M\njam\nJamaican Creole English\nLatn\n12.7K\n416\n68.5K\n15.8K\n25.8M\n1.7M\nwal\nWolaytta\nLatn\n2.6K\n286\n128K\n14K\n17M\n1.7M\njac\nPopti\u2019\nLatn\n8.2K\n303\n61.6K\n11.9K\n15.7M\n1.7M\nbas\nBasa (Cameroon)\nLatn\n4.2K\n216\n105.2K\n14.9K\n25.7M\n1.7M\ngor\nGorontalo\nLatn\n1.7K\n303\n53.3K\n6.5K\n9.4M\n1.7M\nskr\nSaraiki\nArab\n3.8K\n107\n279.3K\n17.1K\n32.2M\n1.7M\nnyu\nNyungwe\nLatn\n1.2K\n195\n1.2K\n11K\n7.7M\n1.6M\nnoa\nWoun Meu\nLatn\n902\n234\n902\n11.5K\n5.2M\n1.6M\nsda\nToraja-Sa\u2019dan\nLatn\n1.6K\n317\n43.2K\n6.2K\n15.8M\n1.6M\ngub\nGuajaj\u00e1ra\nLatn\n31.7K\n271\n160.4K\n25K\n44.7M\n1.6M\nnog\nNogai\nCyrl\n970\n419\n970\n11K\n2.6M\n1.6M\ncni\nAsh\u00e1ninka\nLatn\n1K\n261\n46K\n14K\n5.9M\n1.6M\nteo\nTeso\nLatn\n2.8K\n274\n131.5K\n13.7K\n15.3M\n1.6M\ntdx\nTandroy-Mahafaly Malagasy\nLatn\n1.7K\n262\n26.3K\n13.2K\n7M\n1.6M\nsxn\nSangir\nLatn\n587\n197\n587\n9.9K\n3.4M\n1.5M\nrki\nRakhine\nMymr\n331\n251\n331\n7.8K\n1.6M\n1.5M\nnr\nSouth Ndebele\nLatn\n10.7K\n246\n10.7K\n11.3K\n49M\n1.5M\nfrp\nArpitan\nLatn\n148K\n550\n3.5M\n8.2K\n535.4M\n1.4M\nalz\nAlur\nLatn\n2.2K\n195\n59.3K\n12.2K\n7.9M\n1.4M\ntaj\nE. Tamang\nDeva\n146\n65\n21.6K\n14.3K\n2.3M\n1.4M\nlrc\nN. Luri\nArab\n42.4K\n587\n351.9K\n9K\n85.3M\n1.4M\ncce\nChopi\nLatn\n847\n116\n23.2K\n11K\n3.3M\n1.3M\nrn\nRundi\nLatn\n8.2K\n323\n8.2K\n11.1K\n33.2M\n1.3M\njvn\nCaribbean Javanese\nLatn\n1K\n213\n36.2K\n7.8K\n5.3M\n1.2M\nhvn\nSabu\nLatn\n737\n200\n33.9K\n7K\n4.3M\n1.2M\nnij\nNgaju\nLatn\n1K\n183\n1K\n9.2K\n4.7M\n1.2M\ndwr\nDawro\nLatn\n452\n215\n22.1K\n11.1K\n2.2M\n1.2M\nizz\nIzii\nLatn\n423\n237\n21.7K\n14.5K\n2.1M\n1.1M\nmsm\nAgusan Manobo\nLatn\n520\n177\n520\n8.6K\n2.5M\n1.1M\nbus\nBokobaru\nLatn\n467\n322\n21.4K\n12.1K\n2.1M\n1.1M\nktu\nKituba (DRC)\nLatn\n3.3K\n144\n115.5K\n7.8K\n18.5M\n1.1M\nchr\nCherokee\nCher\n964\n301\n33.8K\n7.5K\n4.7M\n1M\nmaz\nCentral Mazahua\nLatn\n585\n170\n21.3K\n8.2K\n2.9M\n951.7K\ntzj\nTz\u2019utujil\nLatn\n471\n136\n11.1K\n7.3K\n1.9M\n884.2K\nsuz\nSunwar\nDeva\n226\n186\n226\n11.3K\n1M\n855.2K\nknj\nW. Kanjobal\nLatn\n229\n126\n10.1K\n9.2K\n1.1M\n855K\nbim\nBimoba\nLatn\n410\n40\n31.1K\n6.3K\n3.2M\n793.4K\ngvl\nGulay\nLatn\n37.9K\n126\n213K\n6.9K\n141M\n789.2K\nbqc\nBoko (Benin)\nLatn\n275\n228\n9.8K\n8.2K\n997K\n788.4K\ntca\nTicuna\nLatn\n410\n117\n20K\n7.3K\n2.3M\n786K\npis\nPijin\nLatn\n1.1K\n139\n62K\n7.2K\n7.7M\n764K\nprk\nParauk\nLatn\n1.1K\n18\n12.3K\n4.3K\n3.1M\n734.8K\nlaj\nLango (Uganda)\nLatn\n6.5K\n144\n61K\n6.4K\n15.8M\n730.5K\nmel\nCentral Melanau\nLatn\n119.3K\n103\n878.4K\n3.7K\n315.2M\n729.6K\nqxr\nCa\u00f1ar H. Quichua\nLatn\n2.6K\n153\n40.8K\n6.4K\n6.6M\n724K\nniq\nNandi\nLatn\n26.7K\n226\n26.7K\n4.2K\n72.1M\n716.2K\nahk\nAkha\nLatn\n244\n77\n6.2K\n4.1K\n1.3M\n715.5K\nshp\nShipibo-Conibo\nLatn\n874\n150\n22.4K\n3.7K\n3.8M\n710.4K\nhne\nChhattisgarhi\nDeva\n3K\n146\n118.4K\n4.3K\n12M\n697K\nspp\nSupyire Senoufo\nLatn\n733\n123\n733\n5.8K\n4.4M\n682.5K\nkoi\nKomi-Permyak\nCyrl\n20.7K\n196\n153.9K\n5K\n17.1M\n664.5K\nkrj\nKinaray-A\nLatn\n1.5K\n96\n54.6K\n3.8K\n7.6M\n616.5K\nquf\nLambayeque Quechua\nLatn\n522\n86\n8.4K\n5.2K\n1.5M\n609K\nluz\nS. Luri\nArab\n90.5K\n354\n1.2M\n6.7K\n329.4M\n590.7K\nagr\nAguaruna\nLatn\n465\n93\n16.1K\n3.6K\n2.3M\n554.5K\ntsc\nTswa\nLatn\n12.6K\n82\n12.6K\n4K\n23.4M\n521.3K\nmqy\nManggarai\nLatn\n69.3K\n119\n309K\n2.5K\n78.9M\n506.5K\ngof\nGofa\nEthi\n2.8K\n97\n33.8K\n5.5K\n5.5M\n506K\n20\ngbm\nGarhwali\nDeva\n2.5K\n137\n50.8K\n3.8K\n9.1M\n499.6K\nmiq\nM\u00edskito\nLatn\n236\n45\n6.4K\n3.5K\n1.2M\n485.6K\ndje\nZarma\nLatn\n913\n100\n40.2K\n3.7K\n4.7M\n480.7K\nawa\nAwadhi\nDeva\n5.8K\n126\n100.1K\n8.4K\n11.1M\n475K\nbjj\nKanauji\nDeva\n830\n107\n39.6K\n8K\n3.1M\n439.7K\nqvz\nN. Pastaza Quichua\nLatn\n534\n88\n6.8K\n3.5K\n1.2M\n438.3K\nsjp\nSurjapuri\nDeva\n19K\n31\n498.2K\n2.9K\n94.3M\n430K\ntll\nTetela\nLatn\n200\n37\n200\n2.7K\n2.2M\n409.8K\nraj\nRajasthani\nDeva\n1.8K\n40\n1.8K\n5.7K\n7.1M\n405K\nkjg\nKhmu\nLaoo\n113\n84\n3K\n2.9K\n408.5K\n399K\nbgz\nBanggai\nLatn\n32K\n7\n864.1K\n17K\n79.3M\n391.1K\nquy\nAyacucho Quechua\nLatn\n588\n78\n28.1K\n2.7K\n4.5M\n368.2K\ncbk\nChavacano\nLatn\n10.1K\n78\n43.8K\n2K\n10.3M\n339.3K\nakb\nBatak Angkola\nLatn\n1K\n71\n21.3K\n408\n5.2M\n337.8K\noj\nOjibwa\nCans\n2.5K\n135\n2.5K\n1.6K\n9.6M\n337.1K\nify\nKeley-I Kallahan\nLatn\n611\n79\n19.8K\n2.8K\n2.6M\n334K\nmey\nHassaniyya\nArab\n14.8K\n127\n109.9K\n3K\n36.2M\n323.5K\nks\nKashmiri\nArab\n5.6K\n51\n53.9K\n3.3K\n9.4M\n320.9K\ncac\nChuj\nLatn\n212\n77\n3.4K\n1.8K\n978.7K\n319.8K\nbrx\nBodo (India)\nDeva\n322\n62\n5.3K\n2.4K\n1.1M\n304.4K\nqup\nS. Pastaza Quechua\nLatn\n169\n53\n4.3K\n2.5K\n763.8K\n297.8K\nsyl\nSylheti\nBeng\n5.9K\n61\n5.9K\n4.3K\n21.5M\n293.1K\njax\nJambi Malay\nLatn\n1.5M\n58\n30M\n2.3K\n6.8B\n290.2K\nff\nFulfulde\nLatn\n13.6K\n26\n150K\n5K\n22.8M\n277.6K\nber\nTamazight (Tfng)\nTfng\n2.7K\n79\n12.6K\n1.2K\n6.4M\n265.9K\ntks\nTakestani\nArab\n63.7K\n127\n63.7K\n6.8K\n88.9M\n260.8K\ntrp\nKok Borok\nLatn\n12.8K\n36\n12.8K\n1.7K\n29.9M\n257.3K\nmrw\nMaranao\nLatn\n11.3K\n29\n11.3K\n1K\n27.8M\n257.2K\nadh\nAdhola\nLatn\n2.6K\n87\n107.2K\n1K\n14.5M\n254.9K\nsmt\nSimte\nLatn\n1.4K\n34\n1.4K\n703\n6.8M\n245.4K\nsrr\nSerer\nLatn\n41.1K\n91\n41.1K\n2.3K\n63.6M\n240.6K\nffm\nMaasina Fulfulde\nLatn\n1.8K\n65\n30.1K\n2K\n4.6M\n236.1K\nqvc\nCajamarca Quechua\nLatn\n3.4K\n27\n14.6K\n2.2K\n5M\n233.7K\nmtr\nMewari\nDeva\n1.8K\n11\n1.8K\n2.2K\n7.6M\n231.1K\nann\nObolo\nLatn\n464\n56\n5K\n1.6K\n760.9K\n215.1K\nkaa-Latn\nKara-Kalpak (Latn)\nLatn\n375.2K\n61.2K\n3.6M\n1.3M\n1.5M\n209.5K\naa\nAfar\nLatn\n39.5K\n32\n176.1K\n1.3K\n63.3M\n200K\nnoe\nNimadi\nDeva\n2K\n22\n2K\n2.2K\n13.8M\n195.3K\nnut\nNung (Viet Nam)\nLatn\n29K\n67\n29K\n1.5K\n23.5M\n184.1K\ngyn\nGuyanese Creole English\nLatn\n32.6K\n45\n211.7K\n2.1K\n34.5M\n177.7K\nkwi\nAwa-Cuaiquer\nLatn\n382\n37\n16.9K\n2.2K\n1.8M\n172.8K\nxmm\nManado Malay\nLatn\n24.5K\n58\n218.8K\n1.2K\n48.7M\n171.3K\nmsb\nMasbatenyo\nLatn\n811\n41\n811\n1K\n4.4M\n167.5K\nel-Latn\nGreek (Latn)\nLatn\n-\n-\n-\n-\n-\n-\ndoi\nDogri\nDeva\n-\n-\n-\n-\n-\n-\nmtq\nMuong\nLatn\n-\n-\n-\n-\n-\n-\ndln\nDarlong\nLatn\n-\n-\n-\n-\n-\n-\ncyo\nCuyonon\nLatn\n-\n-\n-\n-\n-\n-\nabs\nAmbonese Malay\nLatn\n-\n-\n-\n-\n-\n-\nhi-Latn\nHindi (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nshu\nChadian Arabic\nArab\n-\n-\n-\n-\n-\n-\nyaq\nYaqui\nLatn\n-\n-\n-\n-\n-\n-\nnyo\nNyoro\nLatn\n-\n-\n-\n-\n-\n-\ncgg\nChiga\nLatn\n-\n-\n-\n-\n-\n-\nsxu\nUpper Saxon\nLatn\n-\n-\n-\n-\n-\n-\nmdh\nMaguindanaon\nLatn\n-\n-\n-\n-\n-\n-\nrwr\nMarwari (India)\nDeva\n-\n-\n-\n-\n-\n-\nxnr\nKangri\nDeva\n-\n-\n-\n-\n-\n-\nmui\nMusi\nLatn\n-\n-\n-\n-\n-\n-\nskg\nSakalava Malagasy\nLatn\n-\n-\n-\n-\n-\n-\nymm\nMaay\nLatn\n-\n-\n-\n-\n-\n-\nctd-Latn\nTedim Chin (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nayl\nLibyan Arabic\nArab\n-\n-\n-\n-\n-\n-\nkjb\nQ\u2019anjob\u2019al\nLatn\n-\n-\n-\n-\n-\n-\nrhg-Latn\nRohingya (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nbmm\nN. Betsimisaraka Malagasy\nLatn\n-\n-\n-\n-\n-\n-\nazg\nSan Pedro Amuzgos Amuzgo\nLatn\n-\n-\n-\n-\n-\n-\nkfy\nKumaoni\nDeva\n-\n-\n-\n-\n-\n-\nbto\nRinconada Bikol\nLatn\n-\n-\n-\n-\n-\n-\nja-Latn\nJapanese (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nmfb\nBangka\nLatn\n-\n-\n-\n-\n-\n-\nru-Latn\nRussian (Latn)\nLatn\n-\n-\n-\n-\n-\n-\ntuf\nCentral Tunebo\nLatn\n-\n-\n-\n-\n-\n-\nctg\nChittagonian\nBeng\n-\n-\n-\n-\n-\n-\npmy\nPapuan Malay\nLatn\n-\n-\n-\n-\n-\n-\nxog\nSoga\nLatn\n-\n-\n-\n-\n-\n-\nte-Latn\nTelugu (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nber-Latn\nTamazight (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nmdy\nMale (Ethiopia)\nEthi\n-\n-\n-\n-\n-\n-\naz-RU\nAzerbaijani (Russia)\nCyrl\n-\n-\n-\n-\n-\n-\nta-Latn\nTamil (Latn)\nLatn\n-\n-\n-\n-\n-\n-\n21\nclu\nCaluyanun\nLatn\n-\n-\n-\n-\n-\n-\ntly-IR\nTalysh (Iran)\nArab\n-\n-\n-\n-\n-\n-\nng\nNdonga\nLatn\n-\n-\n-\n-\n-\n-\nbzc\nS. Betsimisaraka Malagasy\nLatn\n-\n-\n-\n-\n-\n-\nnan-Latn-TW\nMin Nan Chinese (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nml-Latn\nMalayalam (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nmax\nNorth Moluccan Malay\nLatn\n-\n-\n-\n-\n-\n-\nar-Latn\nArabic (Latn)\nLatn\n-\n-\n-\n-\n-\n-\ngom-Latn\nGoan Konkani (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nbg-Latn\nBulgarian (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nnd\nNorth Ndebele\nLatn\n-\n-\n-\n-\n-\n-\nzyj\nYoujiang Zhuang\nLatn\n-\n-\n-\n-\n-\n-\nrkt\nRangpuri\nBeng\n-\n-\n-\n-\n-\n-\nkn-Latn\nKannada (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nzh-Latn\nChinese (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nel-CY\nGreek (Cypress)\nGrek\n-\n-\n-\n-\n-\n-\ndcc\nDeccan\nArab\n-\n-\n-\n-\n-\n-\nbgc\nHaryanvi\nDeva\n-\n-\n-\n-\n-\n-\nmwr\nMarwari\nDeva\n-\n-\n-\n-\n-\n-\nvkt\nTenggarong Kutai Malay\nLatn\n-\n-\n-\n-\n-\n-\ncr-Latn\nCree (Latn)\nLatn\n-\n-\n-\n-\n-\n-\napd-SD\nSudanese Arabic\nArab\n-\n-\n-\n-\n-\n-\ntrw\nTorwali\nArab\n-\n-\n-\n-\n-\n-\nbn-Latn\nBengali (Latn)\nLatn\n-\n-\n-\n-\n-\n-\ngu-Latn\nGujarati (Latn)\nLatn\n-\n-\n-\n-\n-\n-\ngju\nGujari\nArab\n-\n-\n-\n-\n-\n-\nsat-Latn\nSantali (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nndc-ZW\nNdau\nLatn\n-\n-\n-\n-\n-\n-\nkmz-Latn\nKhorasani Turkish (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nmr-Latn\nMarathi (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nen-Cyrl\nEnglish (Cyrl)\nCyrl\n-\n-\n-\n-\n-\n-\nen-Arab\nEnglish (Arab)\nArab\n-\n-\n-\n-\n-\n-\nms-Arab\nMalay (Jawi)\nArab\n-\n-\n-\n-\n-\n-\nms-Arab-BN\nMalay (Jawi, Brunei)\nArab\n-\n-\n-\n-\n-\n-\nbhb-Gujr\nBhili\nGujr\n-\n-\n-\n-\n-\n-\npa-Arab\nLahnda Punjabi (PK)\nArab\n-\n-\n-\n-\n-\n-\nsyl-Latn\nSylheti (Latn)\nLatn\n-\n-\n-\n-\n-\n-\nff-Adlm\nFulah\nLatn\n-\n-\n-\n-\n-\n-\npcm\nNigerian Pidgin\nLatn\n-\n-\n-\n-\n-\n-\ntpi\nTok Pisin\nLatn\n-\n-\n-\n-\n-\n-\ngjk\nKachi Koli\nArab\n-\n-\n-\n-\n-\n-\nbfy\nBagheli\nDeva\n-\n-\n-\n-\n-\n-\nsgj\nSurgujia\nDeva\n-\n-\n-\n-\n-\n-\nnyn\nNyankole\nLatn\n-\n-\n-\n-\n-\n-\nBCP-47\nName\nScript\ndocs (noisy)\ndocs (clean)\nsents (noisy)\nsents (clean)\nchars (noisy)\nchars (clean)\nA.2\nFiltering Details\nCursed Substrings\nFollowing is the list of cursed substrings that we used to filter the monolingual\ndata. Here are a few general notes about these strings:\n1. low quality sentences ending in the pipe character were very common. (Note: this was not\nDevanagari-script text using a Danda.)\n2. The last few regexes are meant to match A N T S P E A K, List Case, and weirdly regular\ntext (for instance, lists of shipping labels or country codes)\nHere is the complete list of cursed substrings and cursed regexes, along with the function used for\nfiltering:\n# this implementation is for demonstration and not very efficient;\n# to speed it up, use string inclusion (`in`) instead of regex for\n# all but the last four, and for those use a compiled regex.\ndef is_cursed(s):\nreturn any(re.findall(curse, s) in s for curse in CURSED_SUBSTRINGS)\nCURSED_SUBSTRINGS = [' \\u2116', '\\ufffd\\ufffd\\ufffd', '\\\\|\\\\s*$', ' nr\\\\.$',\n'aute irure dolor ', ' sunt in culpa qui ', 'orem ipsum ', ' quis nostrud ',\n' adipisicing ', ' dolore eu ', ' cupidatat ', 'autem vel eum', 'wisi enim ad',\n22\n' sex ', ' porn ', '\\u9ec4\\u8272\\u7535\\u5f71', 'mp3', 'ownload',\n'Vol\\\\.', ' Ep\\\\.', 'Episode', ' \\u0433\\\\.\\\\s*$', ' \\u043a\\u0433\\\\.\\\\s*$',\n' \\u0448\\u0442\\\\.', 'Develop', 'Facebook', ' crusher ', ' xxx ',\n' ... ... ... ... ... ... ... ... ...',\n' .... .... .... .... .... .... .... .... ....',\n' [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ] [^ ]',\n', ..,,? ..,,? ..,,? ..,,?',\n]\nVirama Correction\nBelow is the virama substitution code:\n_VIRAMA_CHARS = (\n'\\u094d\\u09cd\\u0a4d\\u0acd\\u0b4d\\u0bcd\\u0c4d\\u0ccd\\u0d3b'\n'\\u0d3c\\u0d4d\\u0dca\\u0e3a\\u0eba\\u0f84\\u1039\\u103a\\u1714'\n'\\u1734\\u17d2\\u1a60\\u1b44\\u1baa\\u1bab\\u1bf2\\u1bf3\\u2d7f'\n'\\ua806\\ua82c\\ua8c4\\ua953\\ua9c0\\uaaf6\\uabed\\u10a3f\\u11046'\n'\\u1107f\\u110b9\\u11133\\u11134\\u111c0\\u11235\\u112ea\\u1134d'\n'\\u11442\\u114c2\\u115bf\\u1163f\\u116b6\\u1172b\\u11839\\u1193d'\n'\\u1193e\\u119e0\\u11a34\\u11a47\\u11a99\\u11c3f\\u11d44\\u11d45'\n'\\u11d97\\u1031\\u1057\\u1058\\u1059\\u1056\\u1060\\u1062\\u1068'\n'\\u1063\\u1067\\u1068\\u1069\\u105e\\u105f\\u1036\\u103d\\u102d'\n'\\u102f\\u102e\\u102d\\u1030\\u1033\\u1034\\u1035\\u102f\\u1032'\n'\\u102c\\u103c\\u103d\\u103e\\u102b\\u1037\\u1038\\u25cc\\u25cc'\n'\\u000a\\u1071\\u1072\\u1073\\u1074\\u1082\\u1083\\u1084\\u1085'\n'\\u1086\\u1087\\u1088\\u1089\\u108a\\u108b\\u108c\\u108d\\u108f'\n'\\u109a\\u109b\\u109c\\u109d\\ua9e5\\uaa7b\\uaa7c\\uaa7d'\n)\ndef remove_viramas(x: str) -> str:\nreturn '%s' % regex.sub(r' ([%s]) ' % _VIRAMA_CHARS, '\\\\1', x)\nChinese Porn Filter\nBelow is the Chinese porn filter list:\nzh_pornsignals = [\n'caoporn', 'caoprom', 'caopron', 'caoporen', 'caoponrn', 'caoponav', 'caopom',\n'caoorn', '99re', 'dy888', 'caopro', 'hezyo', 're99', '4438x', 'zooskool',\n'xfplay', '7tav', 'xxoo', 'xoxo', '52av', 'freexx', '91chinese', 'anquye',\n'cao97', '538porm', '87fuli', '91pron', '91porn', '26uuu', '4438x', '182tv',\n'kk4444', '777me', 'ae86', '91av', '720lu', 'yy6080', '6080yy', 'qqchub',\n'paa97', 'aiai777', 'yy4480', 'videossexo', '91free',\n'\\u4e00\\u7ea7\\u7279\\u9ec4\\u5927\\u7247',\n'\\u5077\\u62cd\\u4e45\\u4e45\\u56fd\\u4ea7\\u89c6\\u9891',\n'\\u65e5\\u672c\\u6bdb\\u7247\\u514d\\u8d39\\u89c6\\u9891\\u89c2\\u770b',\n'\\u4e45\\u4e45\\u514d\\u8d39\\u70ed\\u5728\\u7ebf\\u7cbe\\u54c1',\n'\\u9ad8\\u6e05\\u6bdb\\u7247\\u5728\\u7ebf\\u770b',\n'\\u65e5\\u672c\\u6bdb\\u7247\\u9ad8\\u6e05\\u514d\\u8d39\\u89c6\\u9891',\n'\\u4e00\\u7ea7\\u9ec4\\u8272\\u5f55\\u50cf\\u5f71\\u7247',\n'\\u4e9a\\u6d32\\u7537\\u4eba\\u5929\\u5802',\n'\\u4e45\\u4e45\\u7cbe\\u54c1\\u89c6\\u9891\\u5728\\u7ebf\\u770b',\n'\\u81ea\\u62cd\\u533a\\u5077\\u62cd\\u4e9a\\u6d32\\u89c6\\u9891',\n'\\u4e9a\\u6d32\\u4eba\\u6210\\u89c6\\u9891\\u5728\\u7ebf\\u64ad\\u653e',\n'\\u8272\\u59d1\\u5a18\\u7efc\\u5408\\u7ad9',\n'\\u4e01\\u9999\\u4e94\\u6708\\u556a\\u556a',\n'\\u5728\\u7ebf\\u89c6\\u9891\\u6210\\u4eba\\u793e\\u533a',\n'\\u4e9a\\u6d32\\u4eba\\u6210\\u89c6\\u9891\\u5728\\u7ebf\\u64ad\\u653e',\n'\\u4e45\\u4e45\\u56fd\\u4ea7\\u81ea\\u5077\\u62cd',\n23\n'\\u4e00\\u672c\\u9053',\n'\\u5927\\u9999\\u8549\\u65e0\\u7801',\n'\\u9999\\u6e2f\\u7ecf\\u5178\\u4e09\\u7ea7',\n'\\u4e9a\\u6d32\\u6210\\u5728\\u4eba\\u7ebf\\u514d\\u8d39\\u89c6\\u9891',\n'\\u5929\\u5929\\u8272\\u7efc\\u5408\\u7f51',\n'\\u5927\\u9999\\u8549\\u4f0a\\u4eba\\u4e45\\u8349',\n'\\u6b27\\u7f8e\\u4e00\\u7ea7\\u9ad8\\u6e05\\u7247',\n'\\u5929\\u5929\\u9c81\\u591c\\u591c\\u556a\\u89c6\\u9891\\u5728\\u7ebf',\n'\\u514d\\u8d39\\u9ec4\\u7247\\u89c6\\u9891\\u5728\\u7ebf\\u89c2\\u770b',\n'\\u52a0\\u6bd4\\u52d2\\u4e45\\u4e45\\u7efc\\u5408',\n'\\u4e45\\u8349\\u70ed\\u4e45\\u8349\\u5728\\u7ebf\\u89c6\\u9891',\n'\\u97e9\\u56fd\\u4e09\\u7ea7\\u7247\\u5927\\u5168\\u5728\\u7ebf\\u89c2\\u770b',\n'\\u9752\\u9752\\u8349\\u5728\\u7ebf\\u89c6\\u9891',\n'\\u7f8e\\u56fd\\u4e00\\u7ea7\\u6bdb\\u7247',\n'\\u4e45\\u8349\\u5728\\u7ebf\\u798f\\u5229\\u8d44\\u6e90',\n'\\u556a\\u556a\\u556a\\u89c6\\u9891\\u5728\\u7ebf\\u89c2\\u770b\\u514d\\u8d39',\n'\\u6210\\u4eba\\u798f\\u5229\\u89c6\\u9891\\u5728\\u7ebf\\u89c2\\u770b',\n'\\u5a77\\u5a77\\u6211\\u53bb\\u4e5f',\n'\\u8001\\u53f8\\u673a\\u5728\\u7ebf\\u56fd\\u4ea7',\n'\\u4e45\\u4e45\\u6210\\u4eba\\u89c6\\u9891',\n'\\u624b\\u673a\\u770b\\u7247\\u798f\\u5229\\u6c38\\u4e45\\u56fd\\u4ea7',\n'\\u9ad8\\u6e05\\u56fd\\u4ea7\\u5077\\u62cd\\u5728\\u7ebf',\n'\\u5927\\u9999\\u8549\\u5728\\u7ebf\\u5f71\\u9662',\n'\\u65e5\\u672c\\u9ad8\\u6e05\\u514d\\u8d39\\u4e00\\u672c\\u89c6\\u9891',\n'\\u7537\\u4eba\\u7684\\u5929\\u5802\\u4e1c\\u4eac\\u70ed',\n'\\u5f71\\u97f3\\u5148\\u950b\\u7537\\u4eba\\u8d44\\u6e90',\n'\\u4e94\\u6708\\u5a77\\u5a77\\u5f00\\u5fc3\\u4e2d\\u6587\\u5b57\\u5e55',\n'\\u4e9a\\u6d32\\u9999\\u8549\\u89c6\\u9891\\u5728\\u7ebf\\u64ad\\u653e',\n'\\u5929\\u5929\\u556a\\u4e45\\u4e45\\u7231\\u89c6\\u9891\\u7cbe\\u54c1',\n'\\u8d85\\u78b0\\u4e45\\u4e45\\u4eba\\u4eba\\u6478\\u4eba\\u4eba\\u641e',\n]\nA.3\nOther issues fixed after the self-audit\nConsulting Language Speakers\nFor a few languages, we had strong suspicions that the text was\nnoisy or spurious, but were unable to acertain the quality of the data. In these cases we asked a native\nspeaker to audit the data. Based on their recommendations, we did the following:\n1. zh, zh_Latn: This resulted in the special filters described below.\n2. en_Arab, tly_IR: This data was found to boilerplate, so we removed this data.\n3. fa, bho: No changes were made.\nLanguage Renames and Merges\nFor several languages, we found that (mostly by checking URLs)\nthe corpora were in languages different from the LangID predictions. This led to the following\nchanges:\n1. dty renamed to zxx-xx-dtynoise, aka a \u201clanguage\u201d of noise. This is mainly mis-rendered\nPDFs and may have practical applications for denoising, or for decoding such garbled PDFs.\n2. fan renamed to bum\n3. cjk merged into the gil dataset\n4. bjj merged into the awa dataset\n5. ss-SZ renamed to ss \u2013 this was a result of inconsistent data labels.\nA.4\nMonolingual Data Details\nNotes from rounds 2 and 3 of the self-audit can be seen in Table 10. Some of these notes may refer to\nprevious, less filtered versions of the data, especially those with a \u201cr1\u201d (meaning \u201cround 1\u201d). Some of\nthem however do have some useful information about quirks of problems with the current dataset.\nThe overall statistics of MADLAD-400 are in Table 9.\n24\nTable 10: Notes that we made about individual samples while auditing them. Some languages have\nnotes from the earlier round of auditing in parentheses, e.g. \u2019(r1: get this checked by Hindi speaker)\u2019.\nNotes from Round 0, which were used to find cursed substrings, were not kept.\nBCP-47\nnotes\naa\nsome pretty bad data but also some good data. filter on \"Woo\" (case sensitive) (r1: ok)\nabs\nall short nonsense remove (r1: ok)\nabt\nfine; bible (r1: ok)\nace\ngood; bible (r1: ok)\nacf\ngood; bible (r1: ok)\nach\ngood; bible (r1: ok)\nada\ngood; bible; likely mixed with gaa (r1: ok but odd character usage LATIN CAPITAL LETTER OPEN O when it should be\nlower case in the middle of words)\nadh\ngood; bible (r1: ok, lots bible)\nady\ngood (r1: ok but weird boilerplate)\naf\ngood (r1: ok)\nagr\ngood; bible (r1: ok; some AL in Arabic script)\nahk\ngood; bible; crazy diacritics (r1: ok but weird: lots of u748 MODIFIER LETTER VOICING)\nak\ngood; much but not all bible (r1: ok)\nakb\ngood; bible (r1: empty)\nalt\nWAIT THIS IS AMAZING IT IS ACTUALLY ALTAI! e.g. from urls like https://altaicholmon.ru/2020/02/28/jarashty-la-\njajaltany-jarkyndu-lekeri/ (r1: ok but there are just lots of numbers...not very clean)\nalz\ngood; bible (r1: ok; bible)\nam\ngood (r1: ok)\namu\ngood; bible; crazy diacritics (r1: empty)\nang\nmuch noise but some good Old English in there! (r1: ok; wikipedia; one document that is just \"lastfootwear.com\" 1M times)\nann\ngood; all from wikimedia incubator (r1: ok)\napd-SD\nterribly questionable; probably remove (r1: maybe ok, but looks like lots of template....maybe remove)\nape\ngood; bible (r1: remove)\nar\ngood (r1: ok)\nar-Latn\nterrible, 0pct correct, remove (r1: remove)\narn\ngood; bible (r1: ok)\nas\ngood (r1: ok)\nav\ngood (r1: ok)\nawa\nOK; should be used with caution and suspicion (r1: remove)\nawa\nall bible in awadhi (awa). Renamed from bjj (r1: remove)\nay\ngood; mix of bible and other news sources (r1: ok but very noisy)\nayl\nremove. not ayl. (r1: uh this is all Arabic with \"homo\" in English...remove?)\naz\ngood (r1: ok)\naz-RU\ngood; a lot of JW (r1: ok)\nazg\n70pct short noise; 30pct good bible (r1: empty)\nba\nok (r1: ok)\nban\nok bible (r1: ok)\nbas\nok; has some fun blog stuff! (r1: empty)\nbbc\nok (r1: ok)\nbci\nok bible (r1: ok; bible)\nbe\nok (r1: ok)\nber\nok great! (r1: ok; Mixed in French)\nber-Latn\nok (r1: ok)\nbew\nmostly blogs. i have no way of knowing if this is standard indonesian or not (r1: ok; noisy)\nbfy\nvery bad. remove unless it looks better after filtering short docs; remove (r1: remove)\nbg\nok (r1: ok)\nbg-Latn\nok (r1: ok but questionable...Slavic speaker review needed)\nbgc\nsuper sketch. Remove unless short doc filter leaves some. remove (r1: very questionable....Hindi speaker review)\nbgp\nalmost all ur-Latn. consider removing or renaming (r1: very questionable. Remove? mainly ur-Latn)\nbgz\nidk maybe ok but probably bad (r1: remove. Wow, this is amazing. It is in all sorts of languages \u2013 the only thing they share is\nthat they each have like 500 question marks)\nbhb-Gujr\nbad. remove. all junk gu. (r1: remove; great noise?)\nbho\nmostly from anjoria.com. Ankur reviews and says that it looks like valid Bhojpuri for the most part (r1: questionable but ok?)\nbi\ngood! fun! (r1: ok)\nbik\nok. keep in mind the bik vs bcl issue. (r1: ok)\nbim\ngood; bible (r1: empty)\nbm\ngood (r1: ok but these headers are LONG)\nbmm\nterrible. filter on short and reevaluate (r1: remove)\nbn\nok (r1: ok)\nbn-Latn\nok (r1: ok)\nbo\nneeds some serious script filtering. but there is some ok data in there. (r1: ok)\nbqc\nok; bible (r1: ok but too short?)\nbr\nok after shortfilter (r1: ok)\nbru\nok; bible (r1: ok)\nbrx\nquite good! (r1: ok but questionable...Hindi speaker review?)\nbs\ngood (r1: ok)\nbto\nbad; remove unless short filter keeps enough (r1: empty)\nbts\nok; mostly bible (r1: ok)\nbtx\nok probably (r1: ok)\nbua\nok (r1: ok)\nbum\nok bible; but technically wrong language. Data is in Bulu, not Fang, though they are closely related, so ranamed from \"fan\"\n25\nbus\nok; bible; about 50bzc (r1: ok bible)\nbzj\nok bible (r1: ok)\nca\nok (r1: ok i guess....but is it actually italian?)\ncab\nok jw (r1: ok)\ncac\nok bible (r1: ok bible)\ncak\nok bible (r1: ok bible)\ncbk\nok bible; not Spanish (r1: remove; all Spanish)\ncce\nok jw (r1: empty)\nce\nok (r1: ok)\nceb\nok (r1: ok)\ncfm\nok mostly from chinland.co (r1: ok)\ncgg\nrather noisy but potentialy ok. not sure if WL or not (r1: ok)\nch\nok; not sure about WL (r1: ok)\nchk\nok bible (r1: ok bible)\nchm\nok; fyi watch out for yandex translationese (r1: ok)\nchr\nok bible (r1: ok)\nckb\nok (r1: ok)\nclu\nok bible (r1: ok)\ncnh\ngood, some local news! not sure if WL (r1: ok)\ncni\nok; bible; lots of mixed in content in not,cob,cpc,arl (r1: ok)\nco\nok;l i suspect lots of MT (r1: ok i guess?)\ncr-Latn\nnoise and lorem ipsom. But some ok Cree text. (r1: mostly Lorem Ipsom. remove? Or release with note? there is some\nplausible stuff here too.)\ncrh\nok (r1: ok but review with russian speaker as it could be russian....)\ncrs\nok (r1: ok)\ncs\nok (r1: ok)\nctd-Latn\nok; from some local news? (r1: ok)\nctg\nprobably terrible probably remove (r1: very questionable....remove?)\nctu\nok bible (r1: ok bible)\ncuk\nok bible (r1: ok bible)\ncv\ngood (r1: ok)\ncy\nok after shortfilter; OK (r1: ok)\ncyo\nterrifying noise; remove (r1: empty)\nda\nok (r1: ok)\ndcc\nremove (r1: empty)\nde\nok (r1: ok)\ndin\nok after short doc filter (r1: ok but LONG headers uh oh)\ndje\nok; mostly but not all bible (r1: ok bible)\ndjk\nok; bible+jw (r1: empty)\ndln\nok bible (r1: empty)\ndoi\nok actually nice! (r1: sus; review by hindi speaker)\ndov\nok bible + jw (r1: ok)\ndtp\nok; mostly from www.newsabahtimes.com.my (r1: ok)\ndv\ngood (r1: ok)\ndwr\nok; bible; mixed script (r1: empty)\ndyu\nok bible (r1: empty)\ndz\nok; hidden parallel text; maybe actually bo; mainly buddhist (r1: ok; mixed dz-Latn)\nee\ngood; mostly religious (r1: ok bible)\nel\nok (r1: ok)\nel-CY\nbad (r1: v suspicious; mainly comma lists or boilerplate; remove)\nel-Latn\ngood; a lot of old content! (r1: ok)\nemp\nok bible (r1: ok)\nen\nok (r1: ok)\nen-Arab\nAli reviewed; this is not good data. remove. (r1: idk review w/Arabic reader)\nen-Cyrl\nok ... some fr-Cyrl too and maybe others (r1: OMG LOL yes ok)\nenq\nok bible (r1: ok bible)\neo\nok; likely a lot of MT (r1: ok)\nes\ngood (r1: ok)\net\nok (r1: ok)\neu\nok (r1: ok; lots of poetry?)\nfa\nconsulted Ali; he says it\u2019s ok (r1: ok)\nff\nok after shortfilter (r1: some noise but some nice stuff! ok!)\nff-Adlm\ngood (r1: ok sweet)\nffm\nok bible; mixed fulfulde dialects; consider mergind with ff (r1: ok but idk the dialect)\nfi\nok (r1: ok but lotsa headers)\nfil\nok more bible than expected for such a major language (r1: ok pls note in release that this is the same as tl)\nfip\nok jw ; but wrong language. mostly Mambwe-Lungu and Bemba, not Fipu (mgr+bem vs fip) (r1: ok bible)\nfj\nok (r1: ok bible lotsa noise)\nfo\ngood (r1: ok TODO check that this is not icelandic review)\nfon\nok mostly jw but not all (r1: ok bible)\nfr\nok (r1: ok)\nfrp\nfair amount from wikipedia. (r1: remove; all noise + hashtags)\nfy\nok plausible but i bet there is a lot of Dutch in there (r1: ok)\nga\nok some en noise (r1: ok)\ngag\nhas 1-2 cyrillic examples with small amts of arabic script noise (r1: ok)\ngbm\nok (r1: ok)\ngd\nok (r1: ok; but barely)\ngil\nempty; but merged in data in \"cjk\" (r1: empty)\ngil\nthis is all in gil (Kiribati). merged into \"gil\" (r1: empty)\ngjk\nempty remove (r1: empty)\ngju\nremove short boilerplate (r1: empty)\n26\ngl\nok (r1: ok)\ngn\nok some broken characters some bible (r1: ok)\ngof\nok some bible (r1: empty)\ngom\nok (r1: ok)\ngom-Latn\nfilter on really short boilerplate in en; some porn; after: ok very noisy ; some ok stuff ; release with disclaimer (r1: ok)\ngor\nok bible (r1: ok)\ngrc\nwarning: this is likely polyphonic greek, not ancient greek (r1: ok but idk diff between ancient and modern greek)\ngsw\nwtf is happening here; keep with disclaimer; STILL BOILERPLATE (r1: ok but idk diff between gsw and de)\ngu\nok (r1: ok some en boilerplate)\ngu-Latn\nfilter short en boillerplate and repetitive sentences (r1: lots of social media pages and some porn)\ngub\nok bible (r1: empty)\nguc\nok bible (r1: ok)\nguh\nok bible (r1: ok)\ngui\nok bible (r1: ok)\ngv\nfilter short repetitive sentenecs; still same but keep (r1: ok)\ngvl\nfilter short boilerplate mostly bible (r1: ok)\ngym\nok biblle (r1: ok)\ngyn\nremove boilerplate and porn (r1: remove)\nha\nok (r1: ok)\nhaw\nok scam tv products (r1: ok but filter u65533 REPLACEMENT CHARACTER)\nhi\nok some porn (r1: ok but some en boilerplate)\nhi-Latn\nfilter porn this is half porn (r1: ok but some hi and en)\nhif\nok some en noise and religious (r1: ok it is in Latin)\nhil\nok some en boilerplate (r1: ok)\nhmn\nok (r1: ok)\nhne\nok (r1: ok)\nho\nok (r1: ok but but split between wiki boilerplate and actual content)\nhr\nok (r1: ok)\nht\nok (r1: ok)\nhu\nok (r1: ok)\nhui\nok some bible (r1: ok bible)\nhus\nok bible (r1: some wiki boilerplate)\nhvn\nok religioous text (r1: ok bible)\nhy\nok (r1: ok)\niba\nok jw data (r1: ok)\nibb\nok bible and repeated @ (r1: ok but bible and some repeated lines)\nid\nok (r1: ok)\nify\nok bible (r1: empty)\nig\nok (r1: ok)\nilo\nok some bible (r1: some repetitive content)\ninb\nok bible (r1: remove; it\u2019s a single bible doc lol)\nis\nok (r1: ok)\niso\nok jw (r1: ok)\nit\nok (r1: ok)\niu\nfilter script some is en rest is iu script (r1: ok filter latin script)\nium\nfilter out zh (r1: remove mostly en)\niw\nok (r1: ok has some codemixing because of boilerplate)\nizz\nok bible (r1: empty)\nja\nok a little en mixed in (r1: ok but some porn)\nja-Latn\nremove maybe low quality short and repeated (r1: ok some noise that is manga pages in english)\njac\nok bible (r1: remove \u2019home loan\u2019 repeated over and over)\njam\nok bible (r1: ok)\njax\nfilter mostly text.medjugorje.ws boilerplate (r1: remove)\njiv\nok bible\njv\nok (r1: ok)\njvn\nok bible (r1: ok)\nka\nok (r1: ok)\nkaa\nok (FYI cyrllic) (r1: ok)\nkaa-Latn\nok urls are .ru or .kz (r1: ok)\nkac\nok (r1: ok)\nkbd\nok many .ru (r1: ok some repetitive text and en noise)\nkbp\nnot sure if right script wiki says latin (r1: ok)\nkek\nok jw bible (r1: ok bible)\nkfy\nfilter virama issue (r1: ok)\nkg\nok bible jw (r1: ok)\nkha\nok (r1: ok some repetitive boilerplate)\nkj\nok (r1: filter english out)\nkjb\nok bible (r1: empty)\nkjg\nok bible (r1: empty)\nkjh\nok .ru domain (r1: ok)\nkk\nok (r1: ok)\nkl\nok (r1: ok)\nkm\nook (r1: ok)\nkmb\nok bible jw (r1: ok)\nkmz-Latn\nok soome ar script noise (r1: ok)\nkn\nok (r1: ok)\nkn-Latn\nfilter en noise of karnataka govt websites (r1: filter porn there is too much porn and repetitive content)\nknj\nok bible (r1: empty)\nko\nok (r1: ok)\nkoi\nok (r1: ok)\nkos\nok lds bible (r1: ok bible)\n27\nkrc\nok (r1: ok some repetitive content)\nkri\nok boilerplate noise bible jw (r1: remove repetitive)\nks\nok shorter docs (r1: ok)\nksd\nok bible (r1: ok bible)\nksw\nok bible (r1: ok)\nktu\nok bible jw (r1: ok)\nku\nok (r1: ok)\nkum\nok (r1: ok)\nkv\nok a lil boilerplate vibes (r1: ok)\nkw\nok short boilerplate bible wiki; ok some porn (r1: ok filter english)\nkwi\nok bible (r1: ok)\nky\nok (r1: ok)\nla\nok some broken chars\nlaj\nok bible\nlb\nok shorter text; ok AFTER\nlg\nok lot of www.bukedde.co.ug in this\nlhu\nok bible\nln\nok bible jw\nlo\nok many entities in latin script\nlrc\nok\nlt\nok\nltg\nok mostly www.lakuga.lv\nlu\nok jw\nlus\nok\nluz\nterrible; remove\nlv\nok\nmad\nremove mostly short text\nmag\nok fix virama issue\nmai\nok mild amounts of en noise\nmak\nok bible\nmam\nok bible jw\nmas\nok some amount of bible\nmax\nremove short some ru\nmaz\nok bible jw\nmbt\nok bible\nmdf\nok some short docs\nmdh\nfilter porn short text and repetitive boilerplate\nmdy\nok bible\nmel\nremove noisy en\nmeo\nok mostly blogs\nmeu\nok bible\nmey\nmostly short and noisy borderline\nmfb\nremove short boilerplate\nmfe\nok mostly bible maybe some french creole short doc noise\nmg\nok some bible jw\nmgh\nok bible jw\nmh\nok jw lds\nmi\nok\nmin\nok mostly wiki and bible\nmiq\nok\nmk\nok\nmkn\nok bible\nml\nok\nml-Latn\nok some short docs\nmn\nok\nmni\nok\nmnw\nremove en noise and boilerplate\nmps\nok bible\nmqy\nbible remove short docs\nmr\nok fix virama\nmr-Latn\nremove mostly porn and short docs\nmrj\nremove short docs; ok\nmrw\nok remove short docs\nms\nok\nms-Arab\nok mostly utusanmelayu website\nms-Arab-BN\nok not sure if same as ms-Arab\nmsb\nok bible\nmsi\nok filter short docs\nmsm\nok bible\nmt\nok\nmtq\nremove short doc repetitive\nmtr\nok fix virama remove en noise\nmui\nremove short docs\nmwr\nfilter short docs fix virama\nmy\nfilter noise and en fix virama\nmyv\nmaybe has .ru urls\nnan-Latn-TW\nok\nnd\nok\nndc-ZW\nok\nne\nok\nnew\nok\n28\nng\nok\nngu\nok\nnhe\nok\nnia\nok\nnij\nok\nniq\nok\nnl\nok\nnnb\nok\nno\nok\nnoa\nok\nnoe\nok\nnog\nok\nnr\nok\nnso\nok\nnut\nok\nnv\nok\nny\nok\nnyn\nok\nnyo\nok\nnyu\nok\nnzi\nok\noc\nok\noj\nok\nom\nok\nor\nok\nos\nok\notq\nok\npa\nok\npa-Arab\nok\npag\nbible\npam\nremove\npap\nok\npau\nok\npck\nok\npcm\nok\npis\nbible\npl\nok\npmy\nremove\npon\nbible\nppk\nbible\nprk\nok\nps\nok\npt\nok\nqu\nok\nqub\nbible\nquc\nbible\nquf\nbible\nquh\nbible\nqup\nbible\nquy\nbible\nqvc\nbible\nqvi\nbible\nqvz\nbible\nqxr\nbible\nraj\nok\nrcf\nok\nrhg-Latn\nremove\nrki\nok\nrkt\nok\nrm\nok\nrmc\nok\nrn\nbible\nro\nok\nrom\nbible\nru\nok\nru-Latn\nok\nrw\nok\nrwo\nbible\nrwr\nremove\nsa\nok\nsah\nok\nsat-Latn\ngood! all from local news sources\nsd\ngood\nsda\nok bible\nse\ngood\nseh\nok jw\nsg\nok jw\nsgj\nremove\nshn\nmostly English boilerplate. filter by latin text before releasing\nshp\nok bible\nshu\nquite questionable. prob remove\n29\nsi\ngood\nsja\nok bibe\nsjp\nterible; probably remove; check again after short filter\nsk\nok\nskg\nterrible; remove\nskr\nok; some pnb mixed in\nsl\nok\nsm\nok\nsmt\nok bible but lots of different bibles!\nsn\nok\nso\ngood\nspp\nok bible\nsq\ngood\nsr\nok\nsrm\nok; bible + jw\nsrn\nok bible + jw\nsrr\nremove; englishboilerplate\nss\ngood mix of data ; renamed from \"ss\"\nst\nok\nstq\nok i think ?\nsu\ngood\nsus\nhella sus jk ok bible\nsuz\nok bible\nsv\nok\nsw\nok\nsxn\nok bible ; also wild diacritics\nsxu\nrvisit after shortfilter\nsyl\nidk maybe ok ?\nsyl-Latn\nrevist or remove after shortfilter\nsyr\ngood; practictitioners should keep dialect in mind.\nta\nok\nta-Latn\ngood text .... but pornographic, like all Indic-Latn datasets\ntab\nidk plausibly ok\ntaj\nok bible\ntbz\ngood mostly bible but not all\ntca\nok bible + jw\ntcy\ngood; mostly wikipedia; likely some konkani mixed in\ntdx\nok jw\nte\nok a lot of weirdly low quality looking content like commerce\nte-Latn\ngreat good text....but all pornographic stories + blogs, like all Indic-Latn text\nteo\nok bible\ntet\ngood ; actually a lot of fun data!\ntg\ngood\nth\nok\nti\nok; poor tigray\ntiv\nok jw\ntk\nok; a few weird docs\ntks\nok bible but again i think some mixed dialects\ntlh\nok, but why tf are there websites in klingon? all MT ?\ntll\nok jw\ntly-IR\ndeeply sus; remove after shortfilter\ntn\ngood\nto\ngood ; news bible government\ntoj\nok jw\ntpi\nempy\ntr\nok\ntrp\ngood ; lots of random stuff\ntrw\nsus; remove\nts\ngood\ntsc\nok\ntsg\nmuch noise but some good data too!\ntt\ngood plus some nonunicode misrendered PDF\ntuc\nok bible\ntuf\nok bible\ntvl\nok jw\ntwu\nok bible, but also i think it\u2019s lots of mixed similar dialects\ntyv\nok fun stuff plus some russian noise i think\ntyz\nok bible bu again i think some mixed dialects\ntzh\nok jw\ntzj\nok bible\ntzo\nok bible + jw\nubu\nok bible\nudm\nok\nug\nok\nuk\nok\nur\nok\nuz\nok some cyrllic noise\nve\nok mostly bible jw\nvec\nvery noisy has wiki from other langs and .it websites so not sure if vec\nvi\nok\nvkt\n1 doc remove\n30\nwa\nok lots of wiki stuff\nwal\nok bible + jw\nwar\nok but v sus. Pls filter out wikipedia\nwo\nok; mostly bible. PS i have found that Wolof web-crawled data is often bad, so pls give an extra look if you want\nxal\nok has .ru sites though\nxh\nok\nxmm\nvery noisy lots of dj tiktok and peppa pig repeated\nxnr\nok maybe fix virama though it seems fine\nxog\nok bible and stories\nyap\nok\nyaq\nremove\nyi\nok\nymm\nremove\nyo\nok\nyua\nok\nyue\npretty low quality; mostly not Canto\nza\nrevisit after shortfilter\nzap\nok JW. PS pls note that at least some Zapotec speakers view it as one language, not as a million dialects like ISO does\nzh\nmixed simplified and trad; also much porn\nzh-Latn\nrevisit after shortfilter\nzne\nok jw\nzu\ngood\nzxx-xx-dtynoise\nBEAUTIFUL NOISE rename but keep as beautiful xample. (was called \"dty\")\nzyj\ndeeply bad data .. revisit after shortfilter\nzza\ngood ; also pls note that the Zazaki community is often super into NLP for Zazaki\nA.5\nParallel Data Details\nTo create the dataset described in Section 3, we use the data sources described in Table 11. After\npreprocessing, we obtain a dataset with a total of 157 different languages and 4.1B sentence pairs\nthat vary from en-es with 280.3M sentence pairs to zu-en with 7959 sentence pairs. The list of\nlanguage pairs along with the associated data count is available along with the model checkpoints.\nTable 11: The various data sources used to create the parallel data used to train our MT models with\nthe number of available languages and language pairs. (*for NewsCommentary v14 we only use\nKazakh (kk) data)\nDataset\nVersion # Language Pairs\n# Languages\nEuroparl [39]\nv7\n40\n21\nv9\n12\n8\nParacrawl [22]\n-\n64\n41\nTED57 [70]\n-\n114\n58\nTanzil [64]\n-\n1560\n40\nNewsCommentary [1]\nv10\n6\n3\nv14*\n2\n1\nv16\n200\n15\nWikimatrix [59]\n-\n1620\n85\nWikititles [1]\nv2\n12\n14\nOPUS100 [71]\n-\n198\n100\nSETimes [64]\n-\n90\n10\nUNv1.0 [74]\n-\n20\n5\nAutshumato [28]\n-\n10\n5\nPMIndia [29]\nv1\n13\n13\nCVIT [54]\nMKB (v0), PIB (v1.3)\n110\n11\nInuktitut [34]\n-\n2\n1\nNLPC [24]\n-\n2\n1\nJESC [56]\n-\n2\n1\nKFTT [50]\nv1.0\n2\n1\nASPEC [49]\n-\n2\n1\nA.6\nLanguage Codes\nThe specifics of the language code changes described in Section 4.1 that we made are as follows:\n31\nTable 12: Preprocessed multiway data divided by target language in decreasing order of number of #\nSentence Pairs.\nLanguage (Code)\n# Sentence Pairs\nLanguage (Code)\n# Sentence Pairs\nEnglish (en)\n6825073150\nMalagasy (mg)\n1190715\nSpanish (es)\n630962081\nPunjabi (pa)\n1122414\nGerman (de)\n542301841\nHausa (ha)\n952163\nFrench (fr)\n540506439\nLatin (la)\n882038\nPortuguese (pt)\n355008897\nInuktitut (iu)\n841459\nItalian (it)\n283297447\nMyanmar (Burmese) (my)\n759196\nDutch (nl)\n272264696\nWalloon (wa)\n706400\nSwedish (sv)\n183450145\nUzbek (uz)\n695915\nCzech (cs)\n166419080\nLuxembourgish (lb)\n645610\nPolish (pl)\n154989834\nAssamese (as)\n623321\nMandarin Chinese (zh)\n147268699\nPashto (ps)\n606852\nDanish (da)\n145764833\nArmenian (hy)\n603397\nHungarian (hu)\n136980377\nSindhi (sd)\n597211\nRussian (ru)\n134568973\nNorthern Sami (se)\n585304\nRomanian (ro)\n117227121\nBashkir (ba)\n551617\nSlovak (sk)\n102889505\nAmharic (am)\n548828\nFinnish (fi)\n100468712\nSomali (so)\n543643\nGreek (el)\n97855862\nDhivehi (dv)\n543145\nArabic (ar)\n83364997\nKurdish (Kurmanji) (ku)\n483477\nBulgarian (bg)\n68424658\nFrench (Canada) (fr_CA)\n472160\nLithuanian (lt)\n66562681\nOdia (Oriya) (or)\n465494\nSlovenian (sl)\n65876962\nFaroese (fo)\n434288\nLatvian (lv)\n59061871\nKannada (kn)\n417255\nNorwegian (no)\n57783339\nKinyarwanda (rw)\n386133\nEstonian (et)\n47403866\nWu Chinese (wuu)\n369891\nJapanese (ja)\n42480849\nLombard (lmo)\n366147\nKorean (ko)\n33987656\nEgyptian Arabic (arz)\n353218\nCatalan (ca)\n28627098\nUyghur (ug)\n337582\nCroatian (hr)\n28475191\nLimburgan (li)\n304279\nTurkish (tr)\n26508188\nAragonese (an)\n251004\nUkrainian (uk)\n26258530\nSicilian (scn)\n249682\nIcelandic (is)\n24073502\nDzongkha (dz)\n248172\nPersian (fa)\n23237370\nMeiteilon (Manipuri) (mni)\n242426\nIndonesian (id)\n19860783\nMaori (mi)\n234833\nVietnamese (vi)\n18803243\nNuer (nus)\n231194\nHebrew (he)\n17171755\nMagahi (mag)\n230839\nMacedonian (mk)\n15900226\nBhojpuri (bho)\n230378\nIrish (ga)\n14939080\nShan (shn)\n229967\nGalician (gl)\n14305858\nFriulian (fur)\n228928\nSerbian (sr)\n13989182\nKashmiri (ks)\n228580\nAlbanian (sq)\n13904626\nSardinian (sc)\n227585\nBasque (eu)\n13755514\nKanuri (kr_Arab)\n227549\nMaltese (mt)\n12583626\nDinka (din)\n227233\nEsperanto (eo)\n11431440\nVenetian (vec)\n225065\nHindi (hi)\n11044239\nChhattisgarhi (hne)\n224085\nBosnian (bs)\n10906114\nLigurian (lij)\n222513\nSerbo-Croatian (sh)\n10592606\nCentral Atlas Tamazight (tzm)\n222365\nBengali (bn)\n8394639\nTamasheq (taq_Tfng)\n220907\nSinhala (si)\n7363378\nDari (prs)\n220899\nThai (th)\n6623168\nBanjar (bjn)\n220753\nMalayalam (ml)\n5995504\nAchinese (ace_Arab)\n220651\nTamil (ta)\n5552495\nTamasheq (taq)\n219708\nMarathi (mr)\n5517596\nBanjar (bjn_Arab)\n219020\nTelugu (te)\n5379834\nAchinese (ace)\n217062\nMalay (ms)\n5241831\nBambara (bm)\n216698\nFilipino (fil)\n4532364\nBalinese (ban)\n215965\nUrdu (ur)\n4526509\nMoroccan Arabic (ary)\n214226\nTaiwanese Mandarin (zh_Hant)\n3782804\nNigerian Fulfulde (fuv)\n209659\nSwahili (sw)\n3220515\nSilesian (szl)\n209224\nNepali (ne)\n2833740\nKashmiri (ks_Deva)\n208575\nNorwegian Nynorsk (nn)\n2726466\nBuginese (bug)\n207209\nAzerbaijani (az)\n2353731\nGuarani (gn)\n201241\nKazakh (kk)\n2153356\nLatgalian (ltg)\n200302\nTajik (tg)\n2005810\nCrimean Tatar (crh_Latn)\n192342\nLow German (nds)\n1942746\nKanuri (kr)\n191037\nOccitan (oc)\n1934958\nScottish Gaelic (gd)\n162882\nGeorgian (ka)\n1882623\nBavarian (bar)\n159419\nBelarusian (be)\n1808663\nJavanese (jv)\n144974\nTatar (tt)\n1763087\nMongolian (mn)\n132599\nWestern Frisian (fy)\n1500410\nZulu (zu)\n130485\nAfrikaans (af)\n1459139\nKyrghyz (ky)\n128832\nBreton (br)\n1431191\nLow German (Netherlands) (ndsNL)\n113101\nEnglish (simple) (en_xx_simple)\n1413074\nMirandese (mwl)\n96829\nKhmer (km)\n1330590\nIdo (io)\n72811\nGujarati (gu)\n1309569\nIgbo (ig)\n41296\nCebuano (ceb)\n1255728\nTurkmen (tk)\n40650\nWelsh (cy)\n1229401\nYiddish (yi)\n37128\nXhosa (xh)\n1219701\nYoruba (yo)\n26589\nTotal # Sentence Pairs\n11944961985\n32\n1. We use fil for Filipino/Tagalog, not tl\n2. We use ak for Twi/Akan, rather than tw. This includes Fante.\n3. Unfortunately, we use the macro code chm for Meadow Mari (instead of the correct mhr), and mrj\nfor Hill Mari\n4. By convention, we use no for Norwegian Bokm\u00e5l, whereas some resources use nb\n5. By convention we use ps for Pashto instead of pbt (Southern Pashto)\n6. By convention, we use ms for Standard Malay, not zlm\n7. By convention, we use sq for Albanian, and don\u2019t distinguish dialects like Gheg (aln) and Tosk\n(als)\n8. We use ber as the code for Tamazight, after consultation with Tamazight speakers opining that the\ndialect distinctions are not significant. Other resources use the individual codes like tzm and kab.\n9. We use the macrocode qu for Quechua. In practice, this seems usually to be a mix of the Ayacucho\nand Cusco dialects. Other resources, like NLLB, may use the dialect code, e.g. quy for Ayacucho\nChanka. The same is true for a few other macro codes, like ff (Macro code for Fulfulde, whereas\nother sources may use e.g. fuv.)\n10. Really, there are notes that can be made about almost any code, from the well-accepted conventions\nlike zh for Mandarin, to many dialectical notes, like which variant of Hmong really is the hmn\ndata? But The above ones are made specifically for ones where we are aware of other datasources\nfloating out there that use different conventions.\nA.7\nMultiway Data Details\nOn creating the multiway data described in Section 3.4, we obtain a dataset with 11.9B sentence pairs\nacross 19.7k language pairs. In Table 12, we list the combined number of sentence pairs for each\ntarget language.\nA.8\nModel Training Details\nMT Model Training\nWe train models of various sizes: a 3B, 32-layer parameter model,12 a 7.2B\n48-layer parameter model and a 10.7B 32-layer parameter model. We describe the specifics of the\nmodel architecture in Table 13.\nWe share all parameters of the model across language pairs, and use a Sentence Piece Model\n(SPM) [41] with 256k tokens shared on both the encoder and decoder side. We train the SPM\nmodel on upto 1M sentence samples of the sentences in the sentence-level version of MADLAD-400,\nsupplemented by data from the languages in the parallel data used to train MT models when not\navailable in MADLAD-400 with a temperature of T = 100 and a character coverage of 99.9995%.\nEach input sentence has a <2xx> token prepended to the source sentence to indicate the target\nlanguage [35]. We use both supervised parallel data with a machine translation objective and the\nmonolingual MADLAD-400 dataset with a MASS-style [62] objective to train this model. Each of\nthese objectives is sampled with a 50% probability. Within each task, we use the recently introduced\nUniMax [18] sampling strategy to sample languages from our imbalanced dataset with a threshold of\nN = 10 epochs for any particular language.\nWe used a square root learning rate decay schedule over the total number of training steps, starting at\n0.01 and ending at X, as well as the AdaFactor optimizer with factorized=False and 10k warmup\nsteps. We note that for the 10.7B model we use a dropout probability of p = 0.2 instead of p = 0.1\nin order to mitigate overfitting to low-resource languages.\nLanguage Model Training\nWe follow the same training schedule and model configurations\nfrom Garcia et al. [27]. In particular, we consider 8B decoder-only models, following the same\nmodel hyperparameters as previous work [17, 27]. We train these models using a variant of the\nUL2 objective [63] adapted for decoder-only models, and use the same configuration as previous\nwork [27, 52]. We point the reader to these papers for a detailed overview of the training process, and\ninclude basic architectural details in Table 13. We use the same SPM trained for the MT models.\n12Here and elsewhere, \u2018X-layer\u2019 means X encoder layers and also X decoder layers, for a total of 2X layers.\n33\nTable 13:\nArchitecture and training details for the various models we train in this work using\nMADLAD-400.\nModel\nSPM Size\nTraining Objective\n# Attn Heads\n# Enc. Layers\n# Dec. Layers\nModel Dim\nHidden Dim\nBatch Size\nMax. Seq. Length\nTraining Steps\n3B MT Model\n256k\nMT+MASS\n16\n32\n32\n1024\n8192\n4096\n256\n1M\n7.2B MT Model\nMT+MASS\n16\n32\n32\n2048\n8192\n4096\n256\n500k\n10.7B MT Model\nMT+MASS\n32\n48\n48\n2048\n16384\n4096\n256\n250k\n8B LM\nUL2\n16\nN/A\n32\n4096\n16384\n1024\n1024\n500k\nTable 14: Languages on which we evaluate the trained models for each multilingual evaluation set.\nDatasets\nLanguages Evaluated\nWMT\ncs, de, es, fi, fr, gu, hi, kk, lv, lt, ro, rs, es, tr, zh\nFlores-200\nac_Arab, ace, af, am, ar, arz, as, awa, ay, az, ba, ban, be,\nber, bg, bho, bjn_Arab, bjn, bm, bn, bo, bs, bug, ca, ceb,\nckb, crh_Latn, cs, cy, da, de, din, dyu, dz, ee, el, eo, es,\net, eu, fa, ff, fi, fil, fj, fo, fon, fr, fur, ga, gd, gl,\ngn, gu, ha, hi, hne, hr, ht, hu, hy, id, ig, ilo, is, it,\nhe, ja, jv, ka, kac, kbp, kg, kk, km, kmb, kn, ko, kr_Arab,\nkr, ks_Deva, ks, ky, lb, lg, li, lij, lmo, ln, lo, lt, ltg,\nlus, lv, mag, mai, mg, mi, min, mk, ml, mn, mni, mr, ms, mt,\nmy, ne, nl, nn, no, nso, nus, ny, oc, om, or, pa, pag, pap,\npl, ps, pt, quy, rn, ro, ru, rw, sa, sc, scn, sd, sg, shn,\nsi, sk, sl, sm, sn, so, sq, sr, ss, st, su, sv, sw, szl, ta,\ntaq_Tfng, taq, te, tg, th, ti, tk, tn, tr, ts, tt, ug, uk,\nur, vec, vi, war, wo, xh, yi, yo, zh_Hant, zh, zu\nNTREX\naf, am, ar, az, ba, be, bem, bg, bn, bo, bs, ca, ckb, cs,\ncy, da, de, dv, dz, ee, el, en_GB, en_IN, es, es_MX, et,\neu, fa, fa_AF, ff, fi, fil, fj, fo, fr, fr_CA, ga, gl, gu,\nha, hi, hmn, hr, hu, hy, id, ig, is, it, iw, ja, ka, kk, km,\nkn, ko, ku, ky, lb, lo, lt, lv, mey, mg, mi, mk, ml, mn, mr,\nms, mt, my, nd, ne, nl, nn, no, nso, ny, om, pa, pl, ps, pt,\npt_PT, ro, ru, rw, sd, shi, si, sk, sl, sm, sn, so, sq, sr,\nsr_Latn, ss, sv, sw, ta, te, tg, th, ti, tk, tn, to, tr, tt,\nty, ug, uk, ur, uz, ve, vi, wo, xh, yo, yue, zh, zh_Hant,\nzu\nGatones\nady, ak, as, av, ay, ba, ban, bbc, bci, ber_Latn, bew, bho,\nbm, bo, ce, chr, ckb, cv, doi, dv, dyu, dz, ee, ff, gn, gom,\nilo, iso, kl, kri, lg, ln, lus, mad, mai, meo, min, mni,\nnso, om, or, qu, quc, rw, sa, sg, skr, ti, tiv, tk, ts, tt,\nug, wo, yua, zza\nWe use a square root learning rate decay schedule over 500k steps, starting at 0.01 and ending at\n0.001, as well as the AdaFactor optimizer with factorized=False with 10k warmup steps. We\ndescribe the evaluation setup in Section 4.3.1.\nA.9\nLanguages Evaluated\nIn Table 14, we list the languages for which we evaluate the models trained as described in Sections\n4.1 and 4.2.\nA.10\nResults Details\nIn Tables 15, 19, 16, 17 and 18 we list the WMT, NTREX, Gatones, Flores-200 and Flores-200 (direct\npairs) chrf and SacreBLEU scores respectively by language pair along with the model checkpoints.\n34\nTable 15: Evaluation scores on WMT (depicted as <bleu> / <chrf>) for the MT models and\nlanguage models described in Section 4.1 and Section 4.2 compared against NLLB-54B.\nNLLB\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\ncsen\n33.6 / 58.8\n34.8 / 59.9\n35.7 / 60.5\n35.3 / 60.4\n3 / 22.2\n27.6 / 53.7\n26.7 / 51.8\n28.2 / 54\ndeen\n37.7 / 62.4\n37.3 / 62.5\n38.1 / 63\n38 / 63\n3.3 / 23.6\n26 / 51.9\n29.7 / 55.2\n30.1 / 55.5\nesen\n37.6 / 61.7\n38.1 / 62.4\n38.5 / 62.6\n38.5 / 62.7\n2.2 / 19\n30 / 55.2\n31.5 / 57.4\n32.3 / 58.4\neten\n34.7 / 61.1\n36.2 / 61.9\n37.4 / 62.8\n37.4 / 62.9\n4.7 / 24.7\n26.4 / 52.8\n27.9 / 55\n27.7 / 54.5\nfien\n28.8 / 55.7\n32.8 / 59.1\n33.5 / 59.4\n33.3 / 59.6\n1.6 / 16.8\n27.1 / 53.5\n26.2 / 52\n26.3 / 52.6\nfren\n41.9 / 65.7\n42.2 / 65.6\n42.7 / 65.9\n42.4 / 65.8\n2 / 18.6\n28.9 / 54.8\n35.1 / 60.7\n35.6 / 61.1\nguen\n31.2 / 58.9\n30.4 / 58.2\n29.9 / 58\n29.9 / 57.9\n1 / 13\n22.2 / 50.7\n21.9 / 51.2\n24.1 / 53.2\nhien\n37.4 / 64.2\n33.5 / 61.3\n36.1 / 62.7\n35.5 / 62.4\n1.2 / 10.2\n23.4 / 52.3\n23.4 / 54.5\n24.4 / 54.5\nkken\n30.2 / 58.4\n12.3 / 48.1\n22.9 / 49.8\n19.9 / 52.1\n2.4 / 14.2\n15.3 / 41.2\n11.5 / 40.9\n11.9 / 41.6\nlten\n29.7 / 58.8\n34.5 / 60.9\n35.3 / 61.4\n35.1 / 61.5\n3 / 21.4\n24.1 / 51.1\n26.1 / 52.8\n28.1 / 54.9\nlven\n24.8 / 53.1\n27.7 / 55.6\n28.1 / 56\n28.3 / 56.1\n1.3 / 15.7\n21.4 / 49.3\n21.4 / 49.2\n22 / 50.1\nroen\n43.4 / 66.4\n43.6 / 66.6\n44.4 / 67\n44.4 / 67.1\n4.1 / 24.8\n34.9 / 58.9\n36.1 / 59.9\n36.2 / 60.5\nruen\n39.9 / 63.8\n39.9 / 64.4\n40.7 / 64.9\n40.5 / 64.9\n2.1 / 16.4\n32.7 / 56.9\n34.1 / 58.5\n27.4 / 53.7\ntren\n34.3 / 60.5\n33.2 / 58.9\n34.3 / 59.7\n34.1 / 59.8\n1.6 / 16.8\n21.7 / 48.1\n21.7 / 47.7\n22.1 / 48.7\nzhen\n28.5 / 56.4\n24.9 / 55.6\n26.2 / 56\n25.7 / 56.2\n0.4 / 1.6\n15.3 / 40\n19.3 / 46.2\n16.9 / 47\nencs\n25.2 / 53.2\n26.2 / 53.8\n27 / 54.2\n26.8 / 54.2\n0.7 / 10.3\n18.5 / 45.2\n19.3 / 48\n19.6 / 47.9\nende\n33 / 62\n33.7 / 62.5\n35 / 63.2\n35 / 63.2\n1.6 / 16.2\n19.7 / 48.1\n21.9 / 53\n21.9 / 54.3\nenes\n37.2 / 61.1\n37.3 / 61.1\n37.7 / 61.3\n37.4 / 61.3\n0.9 / 13.9\n28.4 / 53.7\n30.2 / 55.6\n30.9 / 56.3\nenet\n27 / 59.3\n27.6 / 60.4\n28.3 / 61\n28.4 / 61.1\n0.5 / 10.7\n20.1 / 52.5\n19.9 / 51.2\n20.8 / 53.4\nenfi\n27.7 / 61.7\n28.3 / 60\n29 / 60.4\n27.8 / 60.6\n0.5 / 10.5\n18.1 / 49\n19.8 / 51.6\n20.2 / 51.9\nenfr\n44.2 / 67.7\n44.8 / 67.2\n45.3 / 67.6\n45.6 / 67.7\n1 / 13.7\n32.8 / 58.6\n32.1 / 57.6\n32.7 / 59.5\nengu\n17.6 / 50\n18.6 / 51.3\n19.2 / 52.1\n18.8 / 52\n0.1 / 0.3\n0.2 / 1.2\n0.1 / 1.4\n0.1 / 1.6\nenhi\n26 / 53.5\n23.4 / 50.1\n24.2 / 50.8\n24.5 / 51.1\n1.5 / 10.6\n16.9 / 42.6\n15.7 / 40.5\n16.7 / 42.7\nenkk\n34.8 / 65.2\n7.6 / 44.2\n12.9 / 47\n10.7 / 47.1\n0.2 / 5.2\n6.1 / 37.6\n6.2 / 37.7\n5.7 / 38.9\nenlt\n37 / 66.9\n26.5 / 59\n27.2 / 59.5\n27 / 59.6\n1.1 / 14.2\n20.6 / 53.3\n20.9 / 53.6\n20.1 / 53.1\nenlv\n21.3 / 53.6\n23.9 / 55.8\n25.2 / 56.6\n24.8 / 56.4\n1 / 12.4\n18.3 / 48.5\n19.7 / 52\n19.8 / 51.6\nenro\n33.4 / 60.4\n35.5 / 61.4\n35.5 / 61.5\n35.6 / 61.7\n3.7 / 21\n29.6 / 56.5\n25.3 / 54\n29.5 / 56.4\nenru\n44.8 / 67.4\n32.9 / 57.8\n33.6 / 58.4\n33.6 / 58.3\n0.6 / 2.5\n20.4 / 44\n22.3 / 47\n23.8 / 49.4\nentr\n23.3 / 58.3\n24.2 / 57.2\n26.1 / 58.3\n25.9 / 58.1\n0.5 / 10.9\n11.1 / 42.2\n11 / 46.4\n8.6 / 46.1\nenzh\n33.9 / 30.1\n32.6 / 29.8\n33.3 / 30.9\n33.6 / 31.1\n0.9 / 2.7\n20.2 / 18.9\n16.9 / 18.1\n19 / 19\nxx2en\n34.2 / 60.4\n33.4 / 60.0\n34.9 / 60.6\n34.6 / 60.8\n2.3 / 17.3\n25.1 / 51.4\n26.2 / 52.9\n26.2 / 53.4\nen2xx\n31.1 / 58.0\n28.2 / 55.4\n29.3 / 56.2\n29.0 / 56.2\n1.0 / 10.3\n18.7 / 43.5\n18.8 / 44.5\n19.3 / 45.5\nAverage\n32.7 / 59.2\n30.8 / 57.7\n32.1 / 58.4\n31.8 / 58.5\n1.6 / 13.8\n21.9 / 47.4\n22.5 / 48.7\n22.8 / 49.4\nTable 16: Evaluation scores on the GATONES test set used by Bapna et al. [9] (depicted as <bleu>\n/ <chrf>) for the MT models and language models described in Section 4.1 and Section 4.2.\nNTL (Bapna et al. [9])\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n1.6B\n6.4B\n0-shot\n1-shot\n5-shot\n10-shot\nady_en\n- / 53.7\n- / 54.8\n32.7 / 58.8\n34.2 / 59.8\n33.9 / 60.1\n1.5 / 12.4\n18.9 / 50.2\n25 / 54.2\n21.1 / 52.1\nak_en\n- / 31.7\n- / 36.3\n6.9 / 27\n10.4 / 29.9\n9.3 / 29\n0.1 / 5.5\n3.6 / 15.5\n1.8 / 18.6\n1.9 / 16.6\nas_en\n- / 52.9\n- / 58.6\n30.6 / 55.6\n32.1 / 56.8\n33.8 / 58\n1 / 9.7\n17.4 / 44.5\n19.4 / 47.4\n21.1 / 48.6\nav_en\n- / 48.1\n- / 48.1\n19.7 / 47.8\n21.3 / 50.6\n21.5 / 50.9\n0.3 / 6.1\n11.6 / 32.5\n10.3 / 40.2\n11.1 / 40.6\nay_en\n- / 32.3\n- / 30.8\n9.1 / 28.7\n11.3 / 29\n11.3 / 30.1\n0.2 / 5.6\n1.4 / 13.7\n5.9 / 23.1\n5.5 / 22.6\nba_en\n- / 42.3\n- / 45.8\n4.5 / 23.3\n5.4 / 24\n6.9 / 27.2\n0.2 / 2.9\n8.7 / 30.3\n7.6 / 35\n7.7 / 34.6\nban_en\n- / 32.2\n- / 35.4\n10.1 / 30.9\n10.3 / 31.2\n10.9 / 32.4\n0.2 / 6.6\n2.2 / 21.2\n4.7 / 24.6\n4.7 / 22.7\nbbc_en\n- / 40.9\n- / 44\n12.4 / 36.6\n12.1 / 36.7\n13.3 / 38\n0.2 / 6.6\n5.6 / 27.5\n7.7 / 29.4\n7.9 / 28.4\nbci_en\n- / 11.6\n- / 10.8\n2.7 / 19.7\n3.6 / 20.4\n3.9 / 21.6\n0 / 4\n0.6 / 16.8\n0.5 / 9.5\n1.9 / 15.3\nbew_en\n- / 49.8\n- / 51.8\n25.3 / 50.1\n26.6 / 51.4\n26.7 / 51.7\n0.7 / 10.8\n20.3 / 44.8\n23.8 / 49.2\n21.3 / 48.3\nbho_en\n- / -10000\n- / -10000\n28.6 / 55.8\n25.6 / 52.9\n29.3 / 56.3\n0.6 / 7.4\n17.7 / 45.6\n20 / 49.1\n21.6 / 49.7\nbm_en\n- / 27.6\n- / 36.3\n11.9 / 32.8\n10.6 / 31.7\n12.6 / 33.9\n0.1 / 4.2\n0.1 / 11.5\n1.2 / 11\n1.2 / 14.4\nce_en\n- / 44.6\n- / 44.9\n11.4 / 36\n12.5 / 37\n13.5 / 37.3\n0.6 / 7.5\n11.4 / 36.4\n9.4 / 33.2\n15.1 / 41.3\nchr_en\n- / 16.3\n- / 15.6\n1.1 / 5.9\n0.6 / 8\n0.9 / 7.2\n0 / 0.6\n0.2 / 12.8\n0.3 / 12.6\n0.1 / 10\ncv_en\n- / 39.9\n- / 46.3\n19 / 42.8\n21.2 / 44.9\n22.9 / 46.6\n0.3 / 5.7\n7.5 / 24.6\n13 / 38\n12.2 / 36.3\ndoi_en\n- / 57.4\n- / 63.1\n15.3 / 43.1\n16 / 42.9\n18 / 45\n0.2 / 4.7\n6.9 / 31.9\n8.8 / 35.2\n6.1 / 36.4\ndv_en\n- / 37\n- / 45.2\n20.1 / 45.3\n21.4 / 47.1\n22.5 / 47.7\n0.8 / 8.9\n11.4 / 35.5\n13.2 / 38.1\n14.6 / 41.5\ndyu_en\n- / 23.9\n- / 23.5\n7.6 / 28\n6.7 / 27.7\n8.6 / 30\n0 / 4.6\n2 / 14.1\n2.3 / 16.2\n1.3 / 15.1\nee_en\n- / 29.1\n- / 33.5\n8.6 / 25.8\n11.1 / 28.9\n10.4 / 28.3\n0.1 / 6\n2 / 17.6\n2.1 / 18.9\n3.1 / 16.4\nen_ady\n- / 21.8\n- / 28.2\n0.6 / 8.8\n2 / 14.2\n1.4 / 11.6\n0.1 / 0.6\n2.1 / 18.9\n0.4 / 11.2\n1.7 / 28.6\nen_ak\n- / 32.1\n- / 34.3\n2.2 / 17.9\n4.1 / 21.9\n3.9 / 21.2\n0.3 / 5.9\n0.1 / 3.5\n0.1 / 4.5\n0 / 3.1\nen_as\n- / 36.3\n- / 36.7\n8.4 / 36.7\n8.4 / 36.9\n8.7 / 38\n0.1 / 1.8\n0.7 / 17.6\n0.7 / 18.4\n0.9 / 19.1\nen_av\n- / 26.1\n- / 28.1\n1.7 / 17.5\n1.8 / 21.5\n1.9 / 20.4\n0.1 / 0.5\n0.1 / 4.5\n0.5 / 15.8\n0.1 / 5.9\nen_ay\n- / 30\n- / 30.5\n1.4 / 17\n2.4 / 21.7\n2.1 / 21.6\n0.2 / 6.2\n0.5 / 9.3\n0.2 / 4.7\n0 / 3.4\nen_ba\n- / 37.6\n- / 38.4\n2.3 / 22.1\n3.6 / 25.3\n4 / 26.8\n0.1 / 0.9\n0.2 / 8.8\n0.2 / 9.3\n0.1 / 5.7\n35\nen_ban\n- / 31.8\n- / 33.1\n3.9 / 30.8\n3.7 / 30.9\n4 / 31.5\n0.2 / 8.5\n0.7 / 14.1\n0.1 / 4.4\n0.1 / 8.5\nen_bbc\n- / 34.9\n- / 35.4\n1.9 / 26.8\n2.6 / 28.8\n3.1 / 29.5\n0.1 / 4.4\n0.4 / 12.5\n0.1 / 8.1\n0.1 / 7.1\nen_bci\n- / 14.1\n- / 14.7\n1.5 / 12.7\n2 / 13.8\n2.4 / 14.9\n0.1 / 4.8\n0 / 2.2\n1.1 / 8.8\n0 / 1.7\nen_bew\n- / 45.7\n- / 46\n7.9 / 35.6\n12.6 / 44.9\n12.7 / 45.5\n0.1 / 6.2\n7.5 / 35.2\n10.4 / 41.9\n11.7 / 43.9\nen_bho\n- / 0\n- / 40.6\n12.8 / 39.5\n12 / 38.4\n12.7 / 40.1\n0.2 / 3.8\n2.9 / 24.6\n4.1 / 28.8\n5.8 / 30.5\nen_bm\n- / 28.7\n- / 34.3\n6.5 / 28.5\n6.1 / 27.2\n7.1 / 30.5\n0.2 / 5.5\n0.1 / 4.8\n0 / 2.6\n0 / 2.1\nen_ce\n- / 23.5\n- / 23.7\n3.4 / 9\n7.5 / 22.9\n1.9 / 15.2\n0.2 / 1.3\n0.8 / 8.7\n0 / 1.2\n0.1 / 3.5\nen_ckb\n- / 0\n- / 41.6\n7.5 / 38.7\n7.9 / 38.4\n6.7 / 35.4\n0.1 / 0.6\n0.2 / 8.2\n0.2 / 9\n0.2 / 11.1\nen_cv\n- / 28.1\n- / 32.1\n3.1 / 25.6\n3.1 / 25.6\n3.1 / 25.8\n0.1 / 0.9\n0.1 / 5\n0.1 / 5.5\n0 / 4.5\nen_doi\n- / 29.9\n- / 25.5\n0.7 / 0.8\n0.7 / 0.8\n0.7 / 0.8\n0.1 / 0.2\n0.3 / 10.1\n0.3 / 11\n0.1 / 5.9\nen_dv\n- / 43.2\n- / 43.7\n2.9 / 38\n3.5 / 40.2\n3.8 / 40.8\n0.1 / 0.2\n0.6 / 0.4\n0.6 / 0.5\n0.5 / 0.4\nen_dyu\n- / 12.7\n- / 20.4\n1.9 / 19.4\n2.3 / 19.6\n2.2 / 20.1\n0.1 / 4\n0.4 / 11.2\n0 / 4.6\n0 / 1.5\nen_ee\n- / 35.9\n- / 39.2\n4.1 / 23.7\n6.1 / 27.1\n6 / 26.6\n0 / 4.7\n0.1 / 4.2\n0 / 2.9\n0 / 2.1\nen_ff\n- / 31.1\n- / 32.3\n3 / 21.4\n3.3 / 21.3\n3.7 / 22.9\n0.1 / 5.5\n0 / 12\n0 / 1.8\n0 / 3.3\nen_gn\n- / 24.3\n- / 32.2\n6.3 / 30.8\n6.4 / 30.3\n6.4 / 31.4\n0.2 / 6.7\n0.2 / 14.6\n0.1 / 4.4\n0 / 2.5\nen_gom\n- / 1.1\n- / 39.1\n1.2 / 20.1\n2.1 / 23.5\n4.3 / 28.2\n0.1 / 0.7\n0.6 / 15.3\n0.5 / 16.8\n1.2 / 20\nen_ilo\n- / 49.7\n- / 52.4\n13.9 / 37.8\n16.2 / 41.3\n16.9 / 42.8\n0.3 / 7.1\n2.5 / 23.6\n3.4 / 25.7\n2.1 / 24.8\nen_iso\n- / 16.5\n- / 30.5\n2.1 / 16.3\n2.4 / 18.2\n3.1 / 17.4\n0.2 / 4.3\n0 / 3.2\n0.1 / 7.2\n0 / 4.2\nen_kl\n- / 0\n- / 23.1\n2.6 / 29.5\n3.6 / 33\n3.4 / 33.6\n0.2 / 4.6\n0.4 / 9\n0.1 / 4.7\n0 / 4.8\nen_kri\n- / 32.7\n- / 34.9\n2.6 / 22.2\n2.3 / 18.2\n3.4 / 22.7\n0.3 / 5.9\n0.7 / 11.3\n0.2 / 8.7\n0.7 / 16.4\nen_lg\n- / 35.9\n- / 38\n1.7 / 23.6\n1.6 / 25.8\n2 / 27.4\n0.1 / 5.3\n0 / 3.6\n0 / 3.6\n0 / 2.6\nen_ln\n- / 12.7\n- / 33.9\n0.3 / 11.7\n0.4 / 13.6\n0.6 / 15.2\n0 / 5.8\n0 / 2.7\n0 / 2.9\n0 / 1.7\nen_lus\n- / 16.9\n- / 39.3\n7.7 / 31.1\n10.6 / 35.6\n10.2 / 34.7\n0.2 / 5.6\n0.1 / 4.8\n0.1 / 4.7\n0.1 / 4.2\nen_mad\n- / 29.8\n- / 30.2\n2.1 / 23.8\n3.3 / 25.9\n4 / 25.5\n0.1 / 6.8\n0.1 / 4.6\n0.1 / 6.4\n0.1 / 3.2\nen_mai\n- / 34.1\n- / 37.6\n12.7 / 42.4\n13.2 / 41.2\n14 / 43.2\n0.1 / 0.8\n1.6 / 18.2\n3.4 / 28.9\n3.4 / 29.5\nen_meo\n- / 50.9\n- / 50.7\n39.3 / 63\n46.3 / 68.9\n44.1 / 67.6\n2.2 / 13.2\n47.4 / 70.4\n46.5 / 70.3\n48.3 / 70.8\nen_min\n- / 51.8\n- / 56.1\n10.7 / 44.7\n12.9 / 47.2\n10 / 45\n0.1 / 7.3\n5.7 / 35.9\n4.7 / 34.7\n6.3 / 39.9\nen_mni\n- / 35.7\n- / 38.2\n7.1 / 38.9\n7.9 / 39.4\n7.9 / 40.6\n0.1 / 0.3\n0 / 1.7\n0 / 1.7\n0 / 0.6\nen_nso\n- / 45\n- / 41.6\n11.1 / 34.4\n14.4 / 38.2\n13.4 / 37.3\n0.2 / 4.5\n0.1 / 5.9\n0.1 / 5.6\n0.1 / 5\nen_om\n- / 36\n- / 39.1\n1 / 20\n1.7 / 26.9\n1.9 / 26.4\n0.1 / 5.8\n0 / 4.4\n0 / 2.6\n0 / 2.4\nen_qu\n- / 29.8\n- / 33.1\n1.5 / 19.5\n2.3 / 21.1\n1 / 22.4\n0 / 6\n0.1 / 6.7\n0 / 2.2\n0 / 1.1\nen_quc\n- / 22.8\n- / 22.9\n0.5 / 10.1\n1 / 12.7\n1.4 / 14.2\n0.2 / 5.9\n0 / 5.7\n0 / 3.2\n0 / 4.5\nen_sa\n- / 26.9\n- / 28.4\n1.3 / 24\n1.5 / 26.9\n1.8 / 27.8\n0 / 1.2\n0 / 5.3\n0 / 5.7\n0 / 6.2\nen_sg\n- / 12.7\n- / 20.7\n0.4 / 18.5\n0.5 / 18.2\n0.5 / 18.5\n0 / 4.7\n0 / 1.3\n0.1 / 22.2\n0 / 1.3\nen_skr\n- / 32.8\n- / 31.3\n1.3 / 21.9\n1.5 / 24.9\n1.9 / 25\n0 / 0.1\n0.1 / 7.9\n0 / 6\n0 / 4.8\nen_ti\n- / 19.9\n- / 21.1\n1.3 / 12.8\n1.5 / 15.6\n1.7 / 15.8\n0.1 / 0.3\n0 / 0.6\n0 / 0.6\n0 / 0.4\nen_tiv\n- / 13.7\n- / 23\n1 / 16.2\n1.3 / 18.5\n1 / 17.9\n0.2 / 4.7\n0.4 / 9.3\n0 / 2.3\n0 / 3.7\nen_ts\n- / 17.4\n- / 45.5\n6.4 / 33.6\n8.9 / 37.4\n7.4 / 35.7\n0.1 / 5.2\n0.1 / 4.1\n0.1 / 5.8\n0.1 / 5\nen_yua\n- / 30.6\n- / 31.6\n3.5 / 19\n1.4 / 17.2\n3 / 19.5\n0.2 / 6.2\n0.6 / 9.9\n0.1 / 4\n0.1 / 7\nen_zza\n- / 22.1\n- / 22.6\n1.7 / 19.8\n1.1 / 18\n1.3 / 17.6\n0 / 3.6\n0 / 8.9\n0 / 2.6\n0 / 1.6\nff_en\n- / 33.7\n- / 41.2\n7.8 / 26.8\n8.3 / 26.5\n9.4 / 29.2\n0.1 / 4.4\n0.4 / 12.5\n1.1 / 15.4\n1.3 / 13.6\ngn_en\n- / 26.7\n- / 38.9\n12.9 / 35.5\n12.7 / 35.2\n13.7 / 37\n0.2 / 6\n1.5 / 16.7\n3.9 / 22.2\n2.6 / 19.8\ngom_en\n- / 50.7\n- / 55.5\n20.5 / 45.7\n20.4 / 45\n21 / 45.7\n0.1 / 1.2\n10.5 / 33.2\n16.6 / 42.1\n15.7 / 41\nilo_en\n- / 50.8\n- / 43.4\n17.3 / 44.2\n28.6 / 52.7\n29.6 / 52.8\n0.2 / 5.7\n13.7 / 38.9\n20.1 / 42.7\n18 / 42.1\niso_en\n- / 17.9\n- / 29.4\n5.8 / 24.3\n7.7 / 25.3\n8.2 / 26.1\n0.1 / 4.5\n1.1 / 16\n1.6 / 14.4\n2.3 / 17.2\nkri_en\n- / 48.7\n- / 56.5\n13.2 / 37\n15.6 / 39.5\n17.3 / 41.5\n0.1 / 6\n10.5 / 31.3\n7.5 / 31.1\n10.5 / 33.2\nlg_en\n- / 32.6\n- / 37.9\n6.3 / 24.1\n8.9 / 26.7\n8.2 / 27.1\n0 / 3.9\n1.9 / 18.9\n2.5 / 18.3\n3.3 / 17.3\nln_en\n- / 28.2\n- / 30.4\n0.8 / 13.1\n0.9 / 14.2\n0.8 / 14.8\n0.1 / 5.1\n0.3 / 15.2\n1 / 15\n0.9 / 12.7\nlus_en\n- / 22.6\n- / 34.5\n13 / 35.7\n18.4 / 40.9\n17.9 / 41.2\n0.6 / 12\n7.5 / 31\n12.1 / 33.6\n10.8 / 33.4\nmad_en\n- / 44.9\n- / 50.5\n8.5 / 34\n10.4 / 34.3\n11.9 / 36.7\n0.1 / 5.9\n3.8 / 22.5\n3.9 / 24.9\n5 / 26.8\nmai_en\n- / 57.8\n- / 61.6\n30.9 / 57\n29.2 / 55.7\n33.3 / 58.8\n0.9 / 9.3\n15.3 / 45.1\n20.5 / 50.1\n20.3 / 49.2\nmeo_en\n- / 66.1\n- / 67.9\n44.4 / 66.4\n46.2 / 67.8\n45.5 / 67.5\n0.9 / 9.4\n38.7 / 61.5\n39.6 / 63.3\n41.1 / 63.7\nmin_en\n- / 58.8\n- / 62.4\n26.2 / 52.4\n25.3 / 50.8\n29 / 55.1\n0.4 / 7.7\n16.7 / 39.3\n20.9 / 49.1\n21.7 / 49.5\nmni_en\n- / 54.8\n- / 56.4\n32.2 / 55.9\n31 / 54.9\n33.4 / 57\n0 / 1.9\n1.2 / 17.5\n2.6 / 20.4\n2.7 / 20.5\nnso_en\n- / 45.8\n- / 51.3\n17.6 / 40.2\n21.1 / 43.2\n21.1 / 43.4\n0.2 / 6.1\n5.7 / 23.9\n7.6 / 28.7\n6.1 / 27.3\nom_en\n- / 32.1\n- / 38.1\n5.7 / 25.8\n9.4 / 30.6\n9.4 / 31.3\n0.2 / 6.2\n0.8 / 18.9\n2.8 / 19\n2.3 / 20.2\nqu_en\n- / 29.9\n- / 32.5\n6.7 / 27.2\n7.3 / 27.6\n7.5 / 28.2\n0.1 / 6.1\n2.5 / 18.1\n1.8 / 13.7\n2.3 / 18.8\nquc_en\n- / 24.4\n- / 26.7\n1.9 / 17.6\n2 / 15.9\n2.8 / 19.3\n0.1 / 7.1\n1.3 / 11.4\n1.6 / 13.9\n0.9 / 12.3\nsa_en\n- / 43.5\n- / 46.3\n15.1 / 39\n17.3 / 40.4\n14.5 / 37.9\n2.6 / 18.1\n7 / 35.8\n7.4 / 34.8\n9.8 / 37.3\nsg_en\n- / 13.4\n- / 13.8\n0.5 / 12.5\n0.6 / 13.5\n0.7 / 14.1\n0 / 3.2\n0 / 17\n0.2 / 9.1\n0.2 / 12.1\nskr_en\n- / 44.4\n- / 48.4\n9 / 35.5\n12.5 / 38.8\n12.5 / 39.1\n0.3 / 8.5\n6.5 / 28.5\n9 / 33.2\n8.5 / 34.3\nti_en\n- / 37.9\n- / 44.2\n11.7 / 34\n15.2 / 38.2\n15.2 / 38.2\n0.3 / 7.1\n5.3 / 22.3\n7.1 / 27.4\n7 / 26.3\ntiv_en\n- / 14.1\n- / 15\n2.7 / 14\n2.9 / 14.8\n3 / 15.4\n0.1 / 3.8\n0.9 / 12.3\n1.1 / 11.8\n0.9 / 12.2\nts_en\n- / 25.5\n- / 43\n14.3 / 35.7\n18.5 / 40.5\n17.8 / 40\n0.3 / 7.5\n1 / 16.3\n7.6 / 25.9\n8.7 / 27.8\nyua_en\n- / 34.6\n- / 40.7\n6 / 27\n7.1 / 28.3\n9.7 / 30.9\n0.2 / 6.9\n0.8 / 11.3\n3.3 / 20.2\n4.4 / 19.2\nzza_en\n- / 23.4\n- / 23.2\n4.6 / 22.4\n5.1 / 22.8\n5.2 / 24.2\n0.2 / 10.6\n1.9 / 18.9\n3.3 / 21.3\n2.3 / 20.6\nxx2en\n- / 37.2\n- / 41.2\n13.3 / 34.6\n15.5 / 36.5\n14.8 / 36.0\n0.3 / 6.5\n6.6 / 25.4\n8.3 / 28.1\n8.4 / 28.4\nen2xx\n- / 28.5\n- / 33.1\n4.5 / 23.9\n5.5 / 26.4\n5.4 / 26.2\n0.2 / 4.2\n1.7 / 10.5\n1.7 / 9.9\n1.8 / 9.4\nAverage\n- / 32.9\n- / 37.2\n8.9 / 29.3\n10.5 / 31.5\n10.1 / 31.1\n0.3 / 5.4\n4.2 / 18.0\n5.0 / 19.0\n5.1 / 18.9\n36\nTable 17: Evaluation scores on en-centric Flores-200 pairs (depicted as <bleu> / <chrf>) for\nthe MT models and language models described in Section 4.1 and Section 4.2 compared against\nNLLB-54B. All metrics are computed with the sacrebleu reference implementation.\nNLLB\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\nen_ace_Arab\n0.4 / 21.1\n1.4 / 26.6\n1.4 / 25.3\n1.3 / 26.9\n0 / 0.1\n0 / 0.5\n0 / 2.9\n0 / 2.2\nen_ace\n9.3 / 41.4\n8.3 / 39.6\n6.5 / 36.6\n7.6 / 39.1\n0.1 / 3.7\n0.9 / 15.3\n0.8 / 21.1\n0.7 / 22.4\nen_acm\n10.3 / 35.4\n0.6 / 0.8\n0.4 / 0.8\n0.6 / 0.8\n0 / 0.4\n4.8 / 34.6\n8.3 / 43.3\n6.1 / 41.1\nen_acq\n13.6 / 46.8\n0.6 / 0.8\n0.4 / 0.7\n0.6 / 0.8\n0.1 / 1.2\n5.9 / 36.1\n10.1 / 43.5\n9.7 / 44\nen_aeb\n12 / 42.1\n0.6 / 0.8\n0.6 / 0.8\n0.6 / 0.8\n0.1 / 1.2\n1.4 / 20.3\n4.1 / 35.4\n4.9 / 37.2\nen_af\n38.4 / 66.6\n38.8 / 66.1\n40.9 / 67.6\n40.4 / 67.3\n0.7 / 10.5\n31.2 / 62.2\n36.9 / 64.1\n38.6 / 64.3\nen_ajp\n20.5 / 56\n0.7 / 1.3\n0.6 / 1.3\n0.7 / 1.3\n0 / 1\n9.5 / 41.9\n9.3 / 43.1\n9.7 / 44.5\nen_am\n15 / 42.2\n8.5 / 31.2\n8.8 / 32.2\n9.1 / 32.5\n0 / 0.2\n0.1 / 0.6\n0.2 / 0.7\n0.2 / 0.7\nen_apc\n20.4 / 54.9\n0.8 / 1.2\n0.6 / 1.2\n0.7 / 1.1\n0 / 1\n6 / 37.7\n5.5 / 39\n4.4 / 40\n#N/A en_ar_MA\n11.7 / 43.5\n0.4 / 0.9\n0.3 / 0.8\n0.3 / 0.8\n0 / 1.1\n0.2 / 5.5\n1 / 24.7\n0.2 / 15.3\nen_ar\n29.7 / 60.6\n27.3 / 57.9\n28.2 / 58.4\n27.7 / 58.2\n0.1 / 1.2\n12.6 / 44\n17.5 / 50.2\n18 / 50.6\nen_ars\n23.2 / 54.2\n0.4 / 0.6\n0.3 / 0.6\n0.4 / 0.6\n0.1 / 1.1\n11.6 / 42.3\n15.1 / 48.3\n16.2 / 49.4\nen_arz\n16.7 / 50.9\n16.6 / 48.7\n15.1 / 47\n16.4 / 48.9\n0.1 / 1.2\n2.6 / 27\n3.4 / 34.8\n6 / 38.6\nen_as\n7.9 / 40.3\n7.3 / 37.3\n8.1 / 38.9\n8 / 39.2\n0 / 0.9\n0.2 / 12.9\n0.7 / 21.8\n0.6 / 21.3\nen_ast\n29.8 / 59.6\n1.1 / 2.1\n2 / 13.6\n1.1 / 2\n0.2 / 7.8\n6.4 / 38.4\n9.7 / 44.6\n11.2 / 45.9\nen_awa\n18.5 / 50.7\n10.3 / 43.8\n9.5 / 42.3\n10.2 / 44.4\n0 / 0.3\n0.8 / 16.5\n7.2 / 38\n5 / 38.6\nen_ay\n4 / 34.5\n0.8 / 16.7\n1.1 / 19.5\n1.4 / 20.9\n0 / 2.6\n0 / 2\n0 / 7\n0 / 4.1\nen_az\n14.1 / 47.1\n12.5 / 44.2\n13.5 / 45.3\n13.9 / 46.4\n0.1 / 5.4\n8.4 / 38.8\n9.7 / 42.8\n10.6 / 43.7\nen_azb\n1.3 / 27.8\n0.2 / 0.6\n0.2 / 0.6\n0.2 / 0.6\n0 / 0.1\n0 / 0.8\n0 / 6.9\n0 / 5.6\nen_ba\n18.7 / 51\n5.9 / 28.7\n7.8 / 31.7\n10.4 / 36.8\n0 / 0.3\n0.3 / 10.8\n0.3 / 19.8\n0.2 / 16.4\nen_ban\n15.3 / 48.4\n13.1 / 45.5\n11.1 / 42.4\n13.4 / 45.9\n0.1 / 5.5\n0.4 / 10.3\n0.7 / 21.7\n1.7 / 30.4\nen_be\n14.4 / 45.7\n12.3 / 40.5\n13.5 / 42.1\n13.6 / 42.8\n0.1 / 2.6\n11.5 / 40.9\n11.5 / 41.5\n11.1 / 41.6\nen_bem\n10.5 / 42.3\n1.6 / 2.4\n2.7 / 17.7\n1.6 / 2.7\n0.2 / 4.4\n0 / 4.9\n0.1 / 10.1\n0.1 / 7.9\nen_ber\n8.1 / 34.3\n6.6 / 31\n6.2 / 30.6\n7.5 / 33.4\n0 / 0.2\n0 / 0.5\n0 / 2\n0 / 1\nen_bg\n41.6 / 67.1\n43.5 / 68.9\n43.9 / 69.2\n44.1 / 69.3\n0.7 / 5.7\n32.2 / 60.2\n33 / 61.4\n33.4 / 62.2\nen_bho\n17.3 / 45.2\n13.4 / 41\n11.7 / 38.8\n13.8 / 42.1\n0.1 / 2.1\n1.4 / 19.5\n4.6 / 31.7\n4 / 31.2\nen_bjn_Arab\n1.1 / 20.3\n3.3 / 30.5\n2.9 / 28.6\n3.6 / 31\n0 / 0\n0 / 1.1\n0 / 4.4\n0 / 1.5\nen_bjn\n19.2 / 51.8\n15.8 / 49.6\n13.3 / 45.6\n15.8 / 49.8\n0 / 2.4\n1 / 17.2\n4.9 / 36.3\n3 / 34.9\nen_bm\n6.8 / 32.9\n4.9 / 27.8\n4.3 / 26.8\n5.7 / 30.1\n0 / 3.1\n0 / 6.3\n0 / 11.2\n0.1 / 7.8\nen_bn\n19.4 / 54.2\n14 / 47.2\n14.8 / 48.2\n14.8 / 48.3\n0.1 / 3.7\n4.5 / 35\n8.2 / 44.4\n7.1 / 44.3\nen_bo\n0.8 / 38.6\n13.8 / 26.4\n15.9 / 29.4\n15.9 / 30.2\n0 / 0.1\n0 / 0\n0 / 1.4\n0 / 1\nen_bs\n33.2 / 61.4\n33.4 / 61.9\n34 / 62.3\n34.6 / 62.9\n0.3 / 5.9\n24 / 53.6\n27.6 / 56.9\n26.9 / 57.5\nen_bug\n6.5 / 38.3\n5.8 / 36.6\n5.3 / 34.7\n5.8 / 37.2\n0.1 / 4.3\n0 / 2.2\n0 / 6.7\n0.1 / 8.4\nen_ca\n43.9 / 66.8\n45.8 / 67.5\n46.4 / 68\n46.7 / 68.4\n0.9 / 11.3\n35.9 / 60.7\n36.3 / 61.8\n37.1 / 61.9\nen_ceb\n30.3 / 59.6\n7.4 / 26.5\n8.3 / 27.6\n9.4 / 29.4\n0.2 / 3.3\n9.1 / 39.8\n9.3 / 39.4\n10.3 / 40.7\nen_cjk\n2.7 / 28.3\n1.3 / 2.4\n1.2 / 16.6\n1.4 / 3.8\n0 / 3.3\n0 / 1.3\n0.1 / 6.2\n0 / 6.2\nen_ckb\n13.2 / 52.5\n9.2 / 44.9\n9.8 / 45\n9.4 / 44.4\n0 / 0.1\n0.1 / 5.6\n0.2 / 14\n0.1 / 13.5\nen_crh_Latn\n16.8 / 51\n13.2 / 47.3\n10.9 / 44.1\n13.8 / 48.4\n0.1 / 3.6\n0.4 / 15.2\n1.1 / 27.5\n1.3 / 28.2\nen_cs\n33.6 / 59.8\n34.8 / 61.3\n34.7 / 61.1\n35.6 / 61.5\n0.2 / 5.6\n22.5 / 52.1\n26.2 / 54.5\n25.9 / 54.6\nen_cy\n51.6 / 72.5\n41.7 / 62.4\n45.7 / 66.4\n44.7 / 65.6\n0.1 / 5.1\n1.5 / 15.2\n8.8 / 38.4\n4.9 / 35.6\nen_da\n44.5 / 68.4\n48 / 71.1\n47.9 / 71.1\n48.3 / 71.3\n0.6 / 8.5\n40.3 / 65.3\n41 / 65.4\n42.1 / 66.8\nen_de\n39.6 / 65.2\n41.5 / 67.1\n42 / 67.5\n42.2 / 67.4\n0.5 / 10.1\n27.6 / 56.3\n28.4 / 57.4\n29.1 / 58\nen_din\n3.8 / 26.8\n3.4 / 25.1\n3.2 / 24.7\n3.3 / 25.9\n0.1 / 3.4\n0 / 2.9\n0 / 10.6\n0 / 0.9\nen_dyu\n1.7 / 19.8\n2 / 21\n2.3 / 21.1\n2.1 / 22.1\n0 / 2.7\n0 / 2.7\n0 / 7.4\n0 / 5.8\nen_dz\n0.6 / 45.4\n34.2 / 42.5\n32.3 / 41\n37 / 45.1\n0 / 0\n0 / 1\n0 / 3.4\n0 / 2.4\nen_ee\n13.2 / 41.9\n5.2 / 26.9\n6.8 / 29.2\n5.9 / 28.4\n0 / 3.2\n0.1 / 3.4\n0.1 / 7.5\n0.1 / 5.9\nen_el\n28 / 54.2\n29.2 / 55.9\n29 / 55.8\n29.5 / 56.3\n0.3 / 2.4\n19 / 46.5\n21.5 / 48.8\n20.9 / 48.9\nen_eo\n35.3 / 63.9\n33.6 / 62.6\n34.1 / 63\n34.3 / 63.7\n0.2 / 5.4\n5.1 / 32.4\n12.6 / 47.2\n9.4 / 44.8\nen_es\n28.1 / 56\n28 / 56.4\n28.4 / 56.6\n28.5 / 56.7\n0.3 / 8.9\n22.6 / 50.6\n23.6 / 52.1\n23.7 / 52.2\nen_et\n27.1 / 59.9\n29 / 61.3\n28.5 / 61.2\n29.8 / 61.8\n0.2 / 6\n20.4 / 54.1\n22.7 / 55.5\n23.1 / 55.5\nen_eu\n17.2 / 54.9\n18.3 / 55.5\n18.6 / 55.7\n19.3 / 57.2\n0 / 5.5\n8.3 / 45.5\n12.5 / 51.1\n12.5 / 51.3\nen_fa_AF\n27.8 / 55.4\n0.5 / 1\n0.4 / 0.9\n0.5 / 0.9\n0 / 0.1\n14.7 / 44.4\n21.6 / 48.4\n19.4 / 48.4\nen_fa\n24.9 / 53.5\n24.8 / 51.2\n25.3 / 51.4\n25.1 / 51.7\n0.1 / 0.8\n15.3 / 44.7\n20.7 / 49.5\n20.1 / 49.2\nen_ff\n3.9 / 0\n2.2 / 23.4\n2.5 / 24\n2.5 / 25.1\n0 / 3.6\n0 / 20\n0 / 3.8\n0 / 7.8\nen_fi\n26.2 / 59.4\n26.9 / 61.7\n27.3 / 62\n27.7 / 62.6\n0.1 / 7.2\n19.1 / 52.5\n20 / 53.9\n19.9 / 54.2\nen_fil\n36.5 / 62.3\n28.6 / 56.5\n28.8 / 56.3\n28.2 / 56.5\n0.3 / 6.4\n29 / 55.6\n31.1 / 58.3\n31.1 / 57.9\nen_fj\n20.1 / 48.8\n9 / 32.5\n11.1 / 35.6\n10.4 / 34\n0.1 / 4\n0.1 / 13.9\n0.1 / 9.4\n0.1 / 8.3\nen_fo\n26.3 / 51.8\n16.9 / 40\n18.5 / 41.5\n17.7 / 41.2\n0.1 / 7.3\n5.6 / 29.4\n6.1 / 31.1\n7.7 / 33.2\nen_fon\n3.1 / 23.5\n1.6 / 15.6\n1.9 / 16\n1.6 / 15.9\n0 / 0.3\n0 / 1\n0 / 1.9\n0 / 1.6\nen_fr\n51.4 / 71.4\n52.5 / 72.1\n53.4 / 72.5\n53.2 / 72.5\n0.4 / 9.2\n36.1 / 61.4\n39.1 / 63.5\n38.8 / 63.2\nen_fur\n34 / 58.7\n28.8 / 55.8\n24.7 / 51.4\n28.8 / 56.1\n0.1 / 5.6\n0.7 / 18.1\n3.1 / 28.8\n3 / 30.5\nen_ga\n34.4 / 60.4\n32.8 / 59.7\n33.2 / 60\n33.4 / 60.2\n0.1 / 6.7\n0.6 / 9.2\n1.7 / 21.4\n2 / 22.9\nen_gd\n21.6 / 53.6\n19.7 / 48\n23.2 / 51.8\n21.3 / 50.2\n0.1 / 2.3\n0.2 / 6\n0.3 / 14.9\n0.2 / 12.8\nen_gl\n36 / 61.9\n37.1 / 63.1\n37.1 / 63\n37.4 / 63.5\n0.2 / 8\n29.8 / 57.4\n31.5 / 58.8\n31.5 / 58.9\nen_gn\n9.8 / 40.4\n11.6 / 40.9\n10.9 / 38.9\n12.5 / 41.8\n0.1 / 3.9\n0.1 / 5.6\n0.3 / 15.2\n0.1 / 9\nen_gu\n25 / 56.6\n18.7 / 50.3\n20.1 / 51.4\n20.4 / 52.1\n0 / 0.2\n0.2 / 0.8\n0.1 / 2\n0 / 1.4\nen_ha\n28.8 / 56\n16.9 / 44.4\n18.9 / 45.3\n18.7 / 47.1\n0.1 / 2.6\n0.2 / 6.2\n0.2 / 12\n0.3 / 13.9\nen_hi\n34.6 / 59.3\n30.4 / 54.1\n31 / 54.6\n32.2 / 55.8\n0.2 / 4.1\n17.9 / 46.5\n24.7 / 49.7\n22.9 / 49.7\nen_hne\n27.1 / 57.5\n19.9 / 51.2\n17.8 / 49\n20.4 / 52.1\n0 / 0.2\n1.7 / 23.8\n2.4 / 31.2\n2.9 / 33.2\nen_hr\n31.9 / 60.1\n32.3 / 61.5\n32.1 / 61\n32.8 / 61.6\n0.9 / 12.5\n24.7 / 54.4\n25.2 / 55.9\n25.9 / 56.8\nen_ht\n25.1 / 53.9\n18.8 / 47.7\n23.5 / 52.1\n22.5 / 51.3\n0.1 / 4.7\n0.9 / 15.9\n4.1 / 30.2\n3.3 / 28.8\nen_hu\n27.6 / 58.7\n29.1 / 60\n28.9 / 59.9\n29.4 / 60.4\n0.2 / 6.3\n19 / 49.8\n18.8 / 50.9\n20.2 / 52.1\n37\nen_hy\n21.4 / 57.8\n15.9 / 47.2\n19.5 / 53.7\n20.1 / 53.9\n0 / 0.2\n0.5 / 0.9\n0.5 / 0.7\n0.5 / 0.7\nen_id\n46.6 / 70.6\n43.5 / 68.5\n44.5 / 69\n44.7 / 69.5\n0.1 / 5.8\n34.1 / 62.4\n38.6 / 65.5\n39 / 65.4\nen_ig\n16.9 / 43.6\n13.1 / 34.6\n16.1 / 38.7\n16.2 / 38.5\n0.1 / 3.4\n0.2 / 8.6\n0.2 / 8.7\n0.1 / 6.5\nen_ilo\n25.6 / 55.8\n15.6 / 42.4\n17.7 / 45.7\n17.9 / 45.8\n0.1 / 4.5\n3.7 / 29.6\n3.5 / 29.6\n1.9 / 28.3\nen_is\n25.3 / 52.8\n25.9 / 53.5\n27.5 / 54.2\n27.3 / 54.5\n0.8 / 11.7\n24.1 / 51\n24.3 / 51.7\n24 / 51.6\nen_it\n31.8 / 59.9\n32.4 / 60.3\n32.2 / 60.3\n32.7 / 60.5\n0.4 / 10.8\n24.5 / 54.2\n24.8 / 53.7\n25.1 / 54.6\nen_he\n34.7 / 62.6\n30.5 / 59.3\n32.5 / 61.1\n31.4 / 60.6\n0.1 / 0.9\n15.7 / 46.7\n19.9 / 50.6\n19.7 / 50.9\nen_ja\n0 / 35\n8.8 / 34.3\n9.3 / 35.5\n9.4 / 35.9\n0 / 1.6\n0.2 / 29.3\n0 / 30.5\n0 / 30.7\nen_jv\n28.6 / 57.2\n12.1 / 37.9\n14.5 / 42.2\n16.4 / 44.4\n0.1 / 5.1\n2.7 / 25.4\n8.1 / 39.1\n9.3 / 40\nen_ka\n15 / 52.6\n2.8 / 17.4\n2.2 / 14.8\n2.3 / 15.8\n0.2 / 5.7\n10.9 / 46.4\n10.7 / 48.7\n10 / 48.6\nen_kab\n10 / 37.9\n0.7 / 1.4\n0.9 / 12.4\n0.7 / 1.9\n0 / 3.4\n0.1 / 9.6\n0 / 9.6\n0 / 5.9\nen_kac\n12 / 39.1\n2.9 / 22.8\n3.6 / 23.9\n4 / 25.3\n0.1 / 3.6\n0.3 / 16.1\n0.1 / 8.8\n0 / 7.6\nen_kam\n4.3 / 28.8\n1.2 / 2.3\n2 / 17.7\n1.4 / 3.6\n0.1 / 4.5\n0.7 / 10.3\n0 / 4.9\n0.1 / 5.7\nen_kbp\n6.9 / 30.4\n1.4 / 14.7\n2.3 / 16.4\n2.1 / 16.9\n0.1 / 2.3\n0.1 / 5\n0.1 / 5.5\n0 / 3.3\nen_kea\n18.2 / 44.9\n1.5 / 3.1\n1.8 / 10\n1.5 / 3.1\n0.5 / 8.7\n1.1 / 23.8\n0.5 / 19\n2.1 / 28.6\nen_kg\n16.6 / 48.2\n0.8 / 14.7\n1 / 16.3\n1.4 / 19\n0.3 / 7.8\n0.1 / 7.9\n0.1 / 6.9\n0 / 6.3\nen_ki\n11.6 / 40.1\n1.2 / 2.2\n2.5 / 15.5\n1.2 / 1.8\n0.4 / 7.3\n0.3 / 8.3\n0.1 / 6.5\n0.1 / 6\nen_kk\n22.2 / 56\n17.1 / 50.2\n18.7 / 52.2\n18.8 / 53.1\n0.5 / 7.7\n13.4 / 48.7\n12.6 / 50\n10.4 / 49.7\nen_km\n3.7 / 45\n5.3 / 40.6\n5.2 / 40.7\n5.4 / 41.4\n0.1 / 0.7\n0.9 / 29.2\n0.2 / 28.5\n0.1 / 24.7\nen_kmb\n2.9 / 28.3\n1.7 / 19.8\n1.8 / 19.1\n1.4 / 19.4\n0.2 / 6\n0.2 / 8\n0 / 8.4\n0 / 4.9\nen_kn\n21.3 / 58.2\n11.9 / 49.2\n13.8 / 50.9\n13.3 / 51.3\n0.1 / 0.4\n0.1 / 1.6\n0 / 0.8\n0 / 0.4\nen_ko\n14.3 / 38\n14.7 / 36.2\n14.9 / 36.7\n15.1 / 37.1\n0.3 / 3.2\n5.5 / 24.3\n4.9 / 25.5\n4 / 25.6\nen_kr_Arab\n0.3 / 12.7\n0.5 / 20.8\n0.5 / 20.5\n0.5 / 22.2\n0.1 / 0.5\n0 / 4\n0 / 4.1\n0 / 2.1\nen_kr\n3.9 / 31.5\n3.8 / 28.4\n3.3 / 27.2\n4 / 29.9\n0.3 / 6.3\n0 / 11.4\n0.1 / 13.3\n0 / 4.4\nen_ks_Deva\n1.8 / 20.4\n1.9 / 21.2\n1.6 / 19.8\n1.9 / 21.6\n0.2 / 1.2\n0.2 / 8.3\n0 / 4.3\n0 / 3.9\nen_ks\n6.7 / 38.9\n6.6 / 35.6\n5.8 / 32.9\n7.2 / 36.5\n0 / 0.3\n0.1 / 4.7\n0 / 3.6\n0 / 2.9\nen_ku\n12.2 / 42.3\n0.3 / 0.5\n0.3 / 0.7\n0.5 / 0.8\n0.2 / 6.4\n0.5 / 14\n0.4 / 15.4\n0.4 / 15.8\nen_ky\n13.7 / 49.1\n10.9 / 44.7\n13.4 / 48.1\n12.3 / 46.9\n0.2 / 1.7\n1.5 / 25.9\n1 / 27.8\n0.6 / 26\nen_lb\n27.8 / 59.1\n14.7 / 38\n16 / 38.2\n16.9 / 41.3\n0.4 / 10.6\n3.3 / 34.1\n3.5 / 34.9\n4 / 35.7\nen_lg\n9.9 / 44.7\n3.5 / 27\n4 / 30.3\n3.7 / 29.6\n0.3 / 7.4\n0.4 / 13.9\n0.2 / 11.5\n0.1 / 10.9\nen_li\n19 / 51.6\n18.4 / 51.1\n15.6 / 47.1\n18.4 / 51.5\n0.8 / 12.5\n2.9 / 30.5\n5.9 / 38.9\n6.5 / 39.8\nen_lij\n28.7 / 56.1\n29.5 / 55.2\n24.9 / 50.3\n29.5 / 55.4\n0.5 / 10.6\n1.9 / 25.6\n3.5 / 32.1\n2.4 / 32\nen_lmo\n8 / 38.7\n6.7 / 32.2\n5 / 27.9\n6.7 / 32.9\n0.4 / 9.1\n1.2 / 19.6\n1 / 21.8\n2.3 / 26.9\nen_ln\n19.1 / 50.8\n2.1 / 18.1\n2.3 / 18.8\n2.7 / 20.8\n0.4 / 9.4\n0.7 / 11.4\n0.1 / 9.3\n0.2 / 10.6\nen_lo\n6.8 / 53.4\n9.9 / 47.1\n9.8 / 46.8\n9.7 / 50.5\n0.2 / 1\n1.9 / 35.2\n1.4 / 38\n1.3 / 39.5\nen_lt\n27.3 / 58.1\n29 / 60.1\n29.1 / 60.5\n30.1 / 61.1\n1.5 / 13.8\n22 / 54.3\n21.9 / 54.2\n22.5 / 54.4\nen_ltg\n26 / 56.8\n25.3 / 57.4\n21.5 / 53.3\n26.3 / 58.9\n0.3 / 8.7\n3.2 / 29.9\n2.5 / 29.5\n3.5 / 31.2\nen_lua\n6.4 / 39.4\n1.4 / 2.2\n2.1 / 12\n1.5 / 2.6\n0.3 / 5.7\n0.3 / 12.6\n0.3 / 11.6\n0.1 / 8.1\nen_luo\n11.7 / 41.7\n1.1 / 2.7\n2.4 / 18.3\n1.1 / 2.7\n0.2 / 2.9\n0.1 / 20.8\n0.1 / 11.6\n0 / 5.4\nen_lus\n11.3 / 40.9\n9.5 / 33.7\n11.7 / 37.4\n11.3 / 36.7\n0.4 / 7.4\n0.5 / 11.5\n0.2 / 11\n0.1 / 7.4\nen_lv\n28.6 / 57.9\n32.8 / 61.8\n32.7 / 61.9\n32.9 / 62\n1.4 / 11.8\n25.2 / 54.9\n26.1 / 56.1\n26.2 / 56.4\nen_mag\n32.2 / 61.2\n26.2 / 55.3\n22.5 / 51.6\n26.8 / 56.3\n0 / 0.3\n11.6 / 41.4\n6.7 / 41\n6 / 40\nen_mai\n16.6 / 50.7\n8.3 / 42.5\n12.2 / 45.3\n10.3 / 44.3\n0.1 / 0.9\n4.8 / 35.9\n3.3 / 34.7\n3.4 / 35.6\nen_mg\n17.7 / 54.2\n17 / 50.8\n18.5 / 53.1\n18 / 52.8\n0.3 / 7.1\n1.7 / 20.1\n0.5 / 17\n0.2 / 13.5\nen_mi\n18.9 / 45.9\n18.6 / 44.1\n16.8 / 42\n19.1 / 45.2\n0.4 / 7.9\n0.5 / 11.5\n0.4 / 12.1\n0.3 / 10.8\n#N/A en_min\n23.4 / 55.6\n12.4 / 44.9\n14.1 / 45.6\n10 / 43.2\n0.2 / 6.5\n7.6 / 38.5\n8 / 39.2\n5.6 / 37.7\nen_mk\n35.6 / 63.2\n36.6 / 64.8\n37.7 / 65.2\n37.4 / 65\n1 / 3.4\n30.8 / 59.6\n30.5 / 59.8\n31.2 / 60.3\nen_ml\n17.1 / 57.3\n9.2 / 47.1\n9.4 / 47\n10.2 / 49.2\n0.5 / 8.6\n5.8 / 42.8\n6.4 / 46.1\n5.6 / 47.7\nen_mn\n14.8 / 47.7\n12.2 / 44.4\n14.8 / 47.8\n14.7 / 47.7\n0.1 / 0.7\n0.6 / 0.8\n0.6 / 6\n0.3 / 2.4\nen_mni\n7.8 / 44.3\n6.6 / 38\n7 / 39.2\n7.4 / 40.4\n0.1 / 0.2\n0 / 4.3\n0 / 4.8\n0 / 3\nen_mos\n4.5 / 26.4\n0.9 / 2.3\n1.2 / 16.6\n0.8 / 2.2\n0.2 / 6.1\n0.3 / 14.2\n0 / 6.5\n0 / 3.5\nen_mr\n17.6 / 52.1\n10 / 39.1\n9.9 / 37.7\n11.8 / 42\n0.8 / 11.9\n10.7 / 42.8\n8.6 / 43.1\n8.6 / 44\nen_ms\n42.2 / 68.5\n37.3 / 64.8\n39.3 / 66.2\n38.1 / 65.7\n1.3 / 13.9\n35.8 / 65\n37.2 / 65.6\n36.2 / 65.4\nen_mt\n35.6 / 70.8\n51.7 / 71.7\n52.7 / 72.2\n52.5 / 72.2\n0.4 / 8.9\n5.2 / 34.3\n5.4 / 38.5\n2.9 / 36.7\nen_my\n2.9 / 40.1\n3.3 / 40.5\n3.1 / 39.1\n3.5 / 38.3\n0 / 0.2\n0.1 / 5.7\n0 / 9.9\n0.1 / 12.9\nen_ne\n16.7 / 49.7\n10.3 / 41.8\n10.4 / 42.1\n11.7 / 45.2\n0.1 / 0.4\n3.3 / 34.8\n2.6 / 34.4\n2.1 / 34.9\nen_nl\n27.5 / 57.8\n28.6 / 59.3\n28.7 / 59.4\n28.8 / 59.4\n1.6 / 15.9\n22.5 / 53.7\n22.4 / 54.3\n23.2 / 54.6\nen_nn\n28.1 / 56.1\n33.6 / 61.3\n34.6 / 61.6\n34.3 / 61.8\n3 / 15.2\n27.6 / 56.1\n29.2 / 57.2\n29.8 / 57.6\nen_no\n32.9 / 61.2\n34.7 / 62.8\n34.4 / 62.4\n34.5 / 62.8\n2.5 / 15.8\n29.1 / 57.5\n30.8 / 59.3\n31.1 / 59.8\nen_nso\n23.7 / 53.3\n12.8 / 37.7\n15.7 / 41.1\n14.5 / 40.5\n0.3 / 4.5\n0.6 / 11.8\n0.2 / 10\n0.2 / 9.1\nen_nus\n6.5 / 31.5\n4.8 / 29.3\n3.9 / 27.7\n5 / 30.2\n0.2 / 4.3\n0 / 2.7\n0 / 2.1\n0 / 0.7\nen_ny\n13.6 / 48.7\n8.1 / 38.6\n11.5 / 44.9\n10.6 / 43.5\n0.4 / 8.1\n0.7 / 16.4\n0.4 / 15.7\n0.4 / 17.2\nen_oc\n34 / 61.1\n38.1 / 63\n39.6 / 64.3\n39.7 / 64.7\n0.4 / 10.5\n6.9 / 38.6\n7.8 / 40.1\n8.6 / 41.1\nen_om\n5.4 / 43.3\n1.1 / 22.8\n1.7 / 29.3\n1.8 / 29.1\n0.1 / 7.4\n0.1 / 10.6\n0 / 7.9\n0 / 6.3\nen_or\n15.1 / 50.4\n12.4 / 46.7\n13.3 / 47.5\n13.1 / 47.8\n0.1 / 0.2\n0.4 / 0.9\n0.3 / 0.8\n0.3 / 0.8\nen_pa\n24.5 / 51\n20.7 / 45.7\n21.4 / 46.5\n21.1 / 46.5\n0.1 / 0.3\n0.9 / 6\n0.3 / 1.7\n0.2 / 1.8\nen_pag\n17.3 / 49.6\n7.1 / 35.4\n10.2 / 38.4\n11.1 / 39.2\n0.6 / 7.4\n0.7 / 20.9\n4.3 / 30.7\n4.9 / 32.2\nen_pap\n37.8 / 61.5\n15.5 / 46.5\n20 / 49.4\n16 / 47\n0.6 / 11.4\n2.7 / 28.5\n5.7 / 34.6\n6.7 / 36.6\nen_pl\n22.2 / 52.1\n23.3 / 53.7\n23.6 / 53.8\n23.8 / 53.9\n1 / 11.6\n16.7 / 46.7\n16.2 / 47.9\n17.5 / 48.3\nen_ps\n15.5 / 40.8\n15.1 / 38.7\n15.3 / 38.8\n15.4 / 39.1\n0.1 / 0.9\n0.4 / 12.1\n0.2 / 10\n0.1 / 8.9\nen_pt\n47.9 / 69.6\n49.9 / 71.5\n51 / 72\n51.4 / 72.3\n3 / 17.8\n38.8 / 63.5\n41 / 65.1\n40.7 / 65.4\nen_quy\n3.6 / 31.2\n1.5 / 22.8\n2 / 18.6\n1.3 / 25.5\n0.2 / 8.7\n0.3 / 14.5\n0.1 / 8.8\n0 / 6.7\nen_rn\n12.9 / 47.3\n5 / 31.8\n5.9 / 32.6\n5.9 / 30.7\n0.3 / 5.5\n0.5 / 13.9\n0.5 / 15.5\n0.2 / 12.9\nen_ro\n38.4 / 63.6\n42 / 66.2\n42.3 / 66.2\n42.4 / 66.5\n4.4 / 20\n32.6 / 58.1\n34.8 / 60.5\n35.4 / 60.9\nen_ru\n32.3 / 59\n32.2 / 59.6\n33 / 60.1\n32.5 / 59.8\n1.1 / 4.4\n22.5 / 49.4\n23.9 / 52.4\n25.4 / 53.2\nen_rw\n21.1 / 53.8\n1.8 / 14.6\n0.3 / 9.6\n0.9 / 11.9\n0.5 / 6.9\n0.8 / 15.7\n0.5 / 15.5\n0.5 / 17\nen_sa\n1.7 / 31.6\n1.6 / 27\n1.8 / 30.5\n2 / 31.7\n0.1 / 0.9\n0.1 / 11.6\n0.1 / 15.2\n0.1 / 14.7\n#N/A en_sc\n30 / 58.1\n24.2 / 52.8\n19.3 / 47.4\n24.5 / 53.3\n0.5 / 10.3\n2.7 / 27\n0.8 / 21.4\n1.1 / 23.8\nen_scn\n17.7 / 50.5\n15.3 / 48.5\n13.8 / 45.3\n15.6 / 49.1\n0.5 / 9.3\n1.3 / 22.6\n1.5 / 26.3\n1.8 / 28\nen_sd\n23 / 50.1\n19.8 / 43.7\n17.5 / 42\n18.4 / 42.5\n0.1 / 0.5\n0.8 / 6\n0.7 / 13.7\n0.4 / 10\nen_sg\n9 / 38\n6 / 30.1\n5.8 / 29.2\n6.4 / 30.7\n0.2 / 8.3\n0.2 / 10.1\n0.2 / 10.6\n0.1 / 5.3\n38\nen_shn\n5.5 / 42\n5.4 / 37.5\n5 / 37.3\n5.6 / 39.6\n0.2 / 1.2\n0.1 / 3.1\n0 / 2.6\n0 / 3.6\nen_si\n14.7 / 48\n13.5 / 46\n13.4 / 45.2\n14.2 / 47.6\n0 / 0.4\n0.5 / 1\n0 / 0.7\n0.1 / 0.8\nen_sk\n35.2 / 61.6\n35.7 / 62.5\n36.3 / 62.9\n36.8 / 63.1\n1.5 / 13.9\n27.7 / 55.4\n28.6 / 56\n26.5 / 55.8\nen_sl\n31.7 / 58.7\n32.3 / 59.9\n32.9 / 60.2\n33.3 / 60.6\n0.8 / 12.7\n23.7 / 52.4\n25.1 / 54.1\n24.8 / 53.3\nen_sm\n26.8 / 50.9\n17.9 / 43.6\n20.8 / 46.5\n20.5 / 46.8\n0.5 / 7.4\n0.6 / 14\n0.3 / 11.4\n0.2 / 8.9\nen_sn\n12.6 / 48.8\n7 / 38.2\n9.3 / 42.3\n8.8 / 41.8\n0.5 / 7\n0.8 / 16.6\n0.4 / 17\n0.3 / 16\nen_so\n12.6 / 47.3\n9.4 / 41.8\n10.3 / 43.4\n10.2 / 43\n0.2 / 6.8\n0.7 / 15.3\n0.2 / 12.9\n0.2 / 13.2\nen_sq\n33.2 / 60.6\n29.8 / 58.6\n30.9 / 59.5\n30.4 / 59.3\n1.2 / 11.7\n28.5 / 56.8\n28.7 / 56.6\n29 / 57.7\nen_sr\n35.9 / 62.1\n34.9 / 61.6\n35.8 / 62.4\n36.2 / 62.7\n0.5 / 2\n29.2 / 57.4\n29.9 / 58.4\n30.4 / 58.1\nen_ss\n10.7 / 49.2\n4.1 / 33.4\n5.1 / 34.7\n5.3 / 36\n0.5 / 7.2\n0.5 / 14.9\n0.1 / 11.5\n0.1 / 9.5\nen_st\n18.9 / 49.2\n9.6 / 34.7\n15.4 / 43.1\n14.1 / 41.6\n0.6 / 8.7\n1 / 15.3\n0.5 / 13.3\n0.3 / 11.9\nen_su\n15.9 / 48.3\n14.4 / 45.5\n18.7 / 50.6\n18.5 / 50.4\n0.5 / 9\n5.8 / 35.5\n7 / 37.9\n7.8 / 38.3\nen_sv\n44.2 / 68.1\n47 / 70.8\n47.2 / 70.8\n47 / 70.8\n2.5 / 15.8\n37.7 / 64.2\n39.4 / 65\n39.1 / 65.3\nen_sw\n32.6 / 61.2\n29.3 / 57.6\n30.4 / 58.1\n30.4 / 58.5\n0.3 / 9\n1.5 / 21.1\n1.4 / 22.8\n1.3 / 25.7\nen_szl\n27 / 56.7\n24.9 / 56.1\n21.6 / 51.8\n26.2 / 56.7\n0.5 / 9.3\n6.9 / 36.7\n4.9 / 35.1\n4.3 / 35.8\nen_ta\n19.8 / 59.4\n12.4 / 51.4\n13 / 52.6\n13.9 / 54.1\n0.4 / 10.7\n8.6 / 47.8\n7.8 / 49.2\n8.1 / 49.5\nen_taq_Tfng\n0.7 / 19.3\n0.6 / 21.2\n0.5 / 20.4\n0.7 / 21.7\n0.1 / 0.4\n0 / 7.8\n0 / 2\n0 / 1.4\nen_taq\n4.2 / 25.9\n3 / 24.6\n3.3 / 24.7\n3.6 / 25.7\n0.2 / 8\n0.1 / 4.7\n0 / 2.8\n0 / 4.7\nen_te\n24.8 / 60.1\n16.7 / 51.4\n17.6 / 52.1\n17.5 / 52.7\n0.7 / 9\n10.9 / 45.9\n8.3 / 47.5\n10.5 / 49.6\nen_tg\n24 / 54\n6.1 / 26.6\n9.2 / 31.3\n10.2 / 32.6\n0.1 / 0.6\n1.4 / 21\n0.8 / 22.6\n0.9 / 24.4\nen_th\n6 / 51.4\n8 / 44.1\n7.6 / 43.1\n8 / 44.3\n0.1 / 10\n7.1 / 48.5\n7.5 / 48.7\n4 / 49.2\nen_ti\n6 / 27.9\n1.8 / 15.7\n2.5 / 18.6\n2.5 / 18.9\n0.1 / 0.3\n0 / 1\n0 / 0.8\n0 / 0.6\nen_tk\n13.7 / 46.2\n13.9 / 44.5\n17.5 / 50.7\n17.3 / 49.9\n0.2 / 6.9\n0.8 / 19.9\n0.9 / 22.9\n0.7 / 22.6\nen_tn\n23 / 51\n10.6 / 34.8\n14 / 38.6\n12.2 / 37.7\n0.5 / 8.2\n0.6 / 14.5\n0.3 / 11.5\n0.2 / 10\nen_tpi\n18.3 / 42.8\n1.1 / 2.7\n3.4 / 22.2\n1 / 2.3\n0.6 / 5.4\n0.7 / 16.7\n1.4 / 22.8\n0.3 / 11.3\nen_tr\n30.1 / 61.8\n29.9 / 60.5\n31 / 61.2\n30.5 / 61.4\n1.2 / 14.4\n16.9 / 51\n15 / 51.6\n15.7 / 52.1\nen_ts\n23.3 / 53.2\n9.1 / 34.7\n12 / 38.1\n10.3 / 37\n0.5 / 7.8\n0.3 / 6.9\n0.2 / 10.5\n0.2 / 10.3\nen_tt\n18.4 / 50.6\n7 / 31.5\n10.2 / 36.5\n13.1 / 42.4\n0.1 / 1\n1.8 / 25.1\n0.5 / 21.7\n0.5 / 23.4\nen_tum\n10.5 / 38.5\n1.4 / 2.2\n2.2 / 15.3\n1.5 / 2.5\n0.4 / 7.4\n0.2 / 14.8\n0.4 / 13.8\n0.1 / 10\nen_ug\n13.8 / 50.6\n5.9 / 33.4\n6.9 / 34.2\n8.8 / 39.8\n0 / 0.3\n0.2 / 1.9\n0.1 / 1.2\n0.1 / 1.6\nen_uk\n31.5 / 59.1\n30.6 / 58.9\n30.9 / 59.4\n31.1 / 59.4\n1.6 / 7.8\n23.3 / 52.3\n24.4 / 53.5\n23.5 / 53.4\nen_umb\n2.8 / 31.2\n1.6 / 2.9\n1.4 / 10.5\n1.4 / 2.5\n0.2 / 6.1\n0.1 / 11.5\n0 / 5.6\n0 / 4.6\nen_ur\n23.7 / 51.1\n18.8 / 44\n20.1 / 44.8\n20.3 / 45.4\n0.1 / 0.8\n7.7 / 33.4\n6.1 / 34.5\n2.9 / 30.3\nen_uz\n17.6 / 55.7\n0.6 / 1.3\n0.5 / 1.1\n0.6 / 1.1\n0.1 / 6.8\n2.5 / 30.3\n3.5 / 38.5\n4.6 / 43.9\nen_vec\n22.8 / 54.8\n25.1 / 56.4\n22 / 52.2\n25.2 / 56.7\n0.4 / 12.8\n3.5 / 29.6\n5.6 / 37.7\n7.6 / 41.2\nen_vi\n42.3 / 59.6\n39.1 / 57.2\n40.5 / 58.2\n40.5 / 58.6\n2.2 / 10.3\n35.2 / 54\n35.7 / 54.7\n34 / 54.4\nen_war\n31.7 / 59.3\n9.8 / 31.1\n18.5 / 44.9\n13.7 / 44\n0.7 / 7.5\n5.9 / 33.5\n8.7 / 38.8\n10.4 / 40.3\nen_wo\n6.8 / 31.8\n1.2 / 13.2\n1 / 12.6\n1.4 / 15.4\n0.3 / 7.7\n0.1 / 5.5\n0.1 / 7\n0 / 3.5\nen_xh\n15.2 / 54.7\n11.3 / 47.4\n12.5 / 50.1\n12.5 / 49.2\n0.2 / 7.7\n0.4 / 15.4\n0.3 / 16.1\n0.2 / 16.2\nen_yi\n11.9 / 41.8\n8.7 / 34.8\n6.6 / 36.3\n3.9 / 32.5\n0.2 / 0.7\n0.8 / 13.6\n0.9 / 17.6\n0.5 / 16.3\nen_yo\n6 / 27.4\n2.6 / 19.7\n2.7 / 20.3\n2.9 / 20.7\n0.3 / 6.3\n0.1 / 10.6\n0.1 / 5.2\n0 / 3.2\nen_yue\n2 / 20.8\n0.3 / 2.2\n0.2 / 2\n0.3 / 2\n0.1 / 2.5\n0.1 / 24.7\n1.2 / 25.5\n0.8 / 25.3\nen_zh_Hant\n15.4 / 17.6\n5.5 / 30.4\n6.3 / 31.8\n36.5 / 32.1\n0.1 / 3\n1.2 / 21.4\n3.6 / 22.3\n1.3 / 21.9\nen_zh\n31.2 / 29.3\n5.8 / 35.7\n6.1 / 36.6\n41.3 / 36.5\n2.9 / 3.3\n29.2 / 26.2\n30.1 / 26.7\n29.8 / 26\nen_zu\n19.7 / 58.8\n11.2 / 44.7\n13.8 / 48.6\n13.4 / 47.6\n0.3 / 7.1\n0.4 / 13.9\n0.3 / 17\n0.2 / 17\nace_Arab_en\n13.6 / 37.9\n17.1 / 43.5\n16.6 / 42.7\n19.1 / 45.8\n0 / 0.7\n0.8 / 13.6\n1.5 / 16.5\n1.6 / 18.5\nace_en\n31.3 / 53.9\n23.6 / 49\n21.1 / 46.9\n25 / 50.8\n0 / 3\n3.3 / 22.9\n8.5 / 32.8\n6.8 / 33.7\nacm_en\n40.1 / 65\n34.2 / 60.8\n35.8 / 61.7\n35.2 / 61.6\n0.1 / 1.6\n25.9 / 52.3\n28.4 / 55.3\n28 / 55.1\nacq_en\n42.4 / 66.7\n36.5 / 62.5\n37.3 / 62.9\n37.4 / 63.3\n0.1 / 2\n23.5 / 51.4\n30.1 / 57.1\n29.5 / 57.1\naeb_en\n36 / 61.4\n28.5 / 55.5\n30.4 / 57.1\n30.1 / 56.9\n0.1 / 2.4\n19 / 44.5\n22.4 / 49\n24.3 / 51.3\naf_en\n59.2 / 77.1\n58.7 / 76.8\n60.1 / 77.9\n60 / 77.7\n2.4 / 18\n51.7 / 71.5\n54.3 / 74\n54.9 / 74\najp_en\n46.6 / 68.9\n38.1 / 62.8\n37.7 / 62.4\n39.6 / 64\n0.1 / 1.5\n32.7 / 58.3\n32.7 / 58\n31.2 / 58.7\nam_en\n36.5 / 61.8\n32.6 / 57.9\n36.9 / 61.1\n35.9 / 60.6\n0.2 / 4.8\n20.3 / 47.1\n22.7 / 49.6\n22.9 / 50\napc_en\n42.5 / 66.7\n34.1 / 60.6\n34.7 / 60.7\n36.2 / 62.1\n0.1 / 2\n26.2 / 52.1\n28.7 / 55\n29.8 / 56.3\nar_MA_en\n32.2 / 0\n23 / 50.3\n24.5 / 51.2\n25.1 / 51.7\n0 / 1.4\n15.3 / 43.4\n19.3 / 45.5\n18.1 / 46.8\nar_en\n45.3 / 68.6\n41.7 / 66.4\n44.4 / 68.1\n43.1 / 67.5\n0.3 / 4.9\n32.9 / 58.1\n34.9 / 60.7\n35.5 / 60.6\nars_en\n43.8 / 67.8\n40.3 / 65.4\n42.7 / 66.7\n41.7 / 66.5\n0.1 / 2\n28.3 / 52.4\n34.7 / 59.9\n34.5 / 60\narz_en\n37.2 / 62.7\n30.2 / 57.5\n29.1 / 56.5\n31.7 / 58.9\n0.1 / 1.5\n24.7 / 51.1\n25 / 52.9\n25.3 / 53.4\nas_en\n33.9 / 59.8\n29.5 / 55.7\n31.8 / 57.4\n31.2 / 57.3\n0.1 / 1.6\n14.7 / 43.3\n19.4 / 47.4\n19.4 / 47.9\nast_en\n43 / 66.4\n37.3 / 62.5\n39.7 / 64.2\n39.8 / 64.5\n1.2 / 13.3\n26.2 / 56.1\n29.2 / 57\n31.6 / 58.3\nawa_en\n42.8 / 67.4\n34.6 / 61.6\n33.5 / 60.2\n36.6 / 62.9\n0.1 / 2.5\n19.8 / 47.2\n23.9 / 53.4\n23.8 / 53.8\nay_en\n12.1 / 33.7\n6.1 / 27.4\n7.9 / 28.4\n8.3 / 29.6\n0 / 2.3\n0.5 / 16.4\n3 / 18.8\n2.8 / 19.1\naz_en\n27.2 / 56.5\n19.7 / 47.7\n20.7 / 47.2\n25.1 / 52.7\n1.3 / 15.7\n18.8 / 47.5\n20.2 / 49.5\n20 / 48.7\nazb_en\n20.8 / 47.7\n4.8 / 30.2\n6.7 / 31.7\n6.3 / 32\n0.1 / 3.4\n2.1 / 23.3\n4.2 / 31.2\n5.4 / 30.1\nba_en\n33.6 / 60.1\n6.7 / 25.7\n8.2 / 26.8\n10 / 29.2\n0.1 / 2.9\n13.1 / 36.1\n19.7 / 46.7\n19.3 / 46.8\nban_en\n38.2 / 62.3\n29 / 55.6\n25.5 / 52.4\n30.3 / 57\n0.2 / 5.4\n8.8 / 34.8\n15.9 / 43.9\n16.7 / 43.9\nbe_en\n23.7 / 55.4\n17.9 / 49.3\n17.6 / 49.4\n21.6 / 52.9\n0.8 / 11.5\n18.4 / 48.3\n19.6 / 50.9\n19.6 / 51.5\nbem_en\n29.6 / 52.4\n7.4 / 30.4\n8.6 / 31.5\n10 / 33.2\n0 / 2.5\n1.2 / 14.7\n5.4 / 22.2\n4 / 21.9\nber_en\n19.7 / 0\n16.6 / 41.6\n14.6 / 39.9\n17.6 / 43.1\n0 / 1.7\n0.1 / 14.8\n0.5 / 13.7\n0.2 / 11.9\nbg_en\n43.1 / 68.3\n42.2 / 67.8\n43.6 / 68.9\n43.2 / 68.5\n1.8 / 15.5\n34.1 / 62.7\n36.8 / 64.1\n37.8 / 64.5\nbho_en\n34.6 / 60.7\n28.5 / 56\n25.5 / 53.5\n28.9 / 56.8\n0.1 / 2.4\n16.5 / 43.8\n19.6 / 48.5\n17 / 48\nbjn_Arab_en\n19.2 / 43.3\n21.7 / 47.2\n20.3 / 46.1\n23.8 / 49.8\n0 / 0.9\n0.5 / 15.3\n1.7 / 18.1\n1.8 / 18.1\nbjn_en\n40.1 / 63.3\n30.2 / 55.9\n26.1 / 52.3\n32 / 57.5\n0.1 / 3.5\n10.9 / 40.4\n17.6 / 44.4\n20.3 / 45.5\nbm_en\n19.6 / 42.2\n14.1 / 37.1\n12.9 / 36.3\n15.5 / 39.4\n0 / 2\n1.7 / 12.8\n1.9 / 17.8\n2.3 / 18.8\nbn_en\n38.7 / 64.2\n34.9 / 60.6\n37 / 62.5\n36.2 / 61.8\n0.2 / 2.7\n21.8 / 50.7\n25.2 / 53.9\n26.3 / 54.8\nbo_en\n15.6 / 40.9\n12.6 / 38.1\n12.6 / 38.2\n14.6 / 41.1\n0 / 1\n1.9 / 20.5\n5.9 / 29.6\n5.6 / 29\nbs_en\n45.2 / 68.8\n44.1 / 68.5\n45 / 69.1\n44.9 / 69.2\n1.5 / 13.7\n36.2 / 62.3\n38.5 / 64.5\n39 / 65\nbug_en\n23.1 / 48.4\n17.7 / 43.6\n15.7 / 42\n18.9 / 45.3\n0.1 / 3.3\n1.8 / 18.7\n5.6 / 28\n5.9 / 27.6\nca_en\n49 / 71.1\n48.5 / 71\n50.3 / 72.2\n50 / 72\n2.2 / 16.1\n41.8 / 66.6\n42.5 / 67.1\n43 / 67.2\nceb_en\n45.6 / 66.8\n34.8 / 60.2\n34.1 / 59.4\n36.6 / 61.2\n0.3 / 5.7\n31.9 / 56.4\n32.2 / 57.2\n33.6 / 58.5\ncjk_en\n11.3 / 32.7\n4.8 / 23.8\n5 / 24.7\n5.6 / 26.5\n0 / 2.3\n1.2 / 16.2\n2 / 17.7\n2.3 / 18.8\n39\nckb_en\n37.5 / 61.6\n28.3 / 53.1\n34.4 / 59\n34.2 / 59\n0.2 / 4.4\n18 / 42.1\n22.5 / 48.6\n22.8 / 49.5\ncrh_Latn_en\n35.9 / 61.8\n29.2 / 56.8\n25 / 53.1\n29.1 / 57\n0.1 / 4.8\n16.1 / 40.3\n18.8 / 45.2\n17.8 / 46.6\ncs_en\n41.2 / 66.4\n41.3 / 66.6\n42.3 / 67.3\n42.3 / 67.3\n2.1 / 18.1\n34.7 / 61.4\n35.8 / 62.5\n36.1 / 62.9\ncy_en\n60 / 77.1\n57.5 / 75.6\n60.6 / 77.7\n60.8 / 77.7\n1.2 / 12.5\n43.9 / 65.5\n48.5 / 69.2\n47.5 / 68.6\nda_en\n49.5 / 71.2\n50.1 / 72.3\n51.8 / 73.3\n50.8 / 72.8\n1.6 / 15.1\n44.5 / 68.2\n44.1 / 68.9\n44.7 / 68.9\nde_en\n45.8 / 69.2\n45.5 / 69.5\n46.4 / 70.1\n46.5 / 70\n1.6 / 15.9\n37.8 / 63.3\n39.5 / 64.9\n38 / 64.6\ndin_en\n12.7 / 32.9\n10.3 / 31.1\n9.5 / 31.1\n11 / 32.8\n0 / 2.2\n1.2 / 18.8\n1.6 / 16.3\n2.4 / 19.3\ndyu_en\n8.5 / 30\n7.9 / 31.5\n7.9 / 31.3\n8.9 / 33.5\n0 / 2.3\n1.1 / 15.3\n1.9 / 17.4\n2.3 / 18.2\ndz_en\n17 / 44\n12.9 / 40.4\n12 / 39.2\n13.7 / 41.6\n0 / 0.8\n0.7 / 16.4\n2 / 22.4\n1.9 / 22.7\nee_en\n17.8 / 42\n10.3 / 33.3\n14.3 / 37.4\n13.3 / 36.6\n0.1 / 4.7\n2.5 / 17.5\n5 / 23.2\n4.4 / 24.3\nel_en\n40 / 64.9\n37.8 / 63.3\n38.9 / 64.2\n38.8 / 64\n1.3 / 12.8\n32.3 / 58.4\n32.9 / 59.7\n33.1 / 60\neo_en\n47.8 / 70.1\n45.8 / 68.5\n47.2 / 69.4\n47.1 / 69.5\n9.8 / 31\n38.3 / 63.3\n40.4 / 64.7\n40.9 / 65.3\nes_en\n33.8 / 61.4\n31.5 / 61\n32.2 / 61.3\n32.5 / 61.4\n4.5 / 23.9\n26.4 / 56\n27.6 / 57.8\n27.7 / 57.9\net_en\n38.8 / 64.6\n39.4 / 64.9\n40.3 / 65.6\n39.6 / 65.5\n7.9 / 28\n31.8 / 59.2\n32.5 / 59.8\n32.6 / 59.8\neu_en\n34.4 / 61.2\n32.6 / 58.6\n34.3 / 60\n34.4 / 60.4\n4.4 / 23\n25.5 / 53.8\n24.7 / 52.9\n26.2 / 54.5\nfa_AF_en\n41.3 / 0\n37.9 / 63\n38.7 / 63.2\n39.4 / 64.1\n2.2 / 11.6\n28.7 / 53.7\n31.1 / 56.4\n32.4 / 57.8\nfa_en\n40.9 / 65.6\n37.7 / 62.9\n40.3 / 64.8\n39.4 / 64.3\n3.9 / 16.4\n31.1 / 57.6\n30.8 / 57.2\n32.3 / 58.7\nff_en\n13.5 / 0\n10.5 / 33.2\n10.1 / 33.1\n12.4 / 35.8\n0.2 / 10.5\n2.6 / 15.7\n2.5 / 16.9\n2.4 / 18.2\nfi_en\n36.9 / 63\n36.1 / 62.6\n36.7 / 63\n37.1 / 63.3\n3.8 / 22.9\n27.9 / 55.8\n29.8 / 57.4\n30.5 / 58.3\nfil_en\n51.5 / 71.3\n47.1 / 68.2\n49.5 / 70\n49.6 / 70.1\n2.5 / 17.9\n40.4 / 63.9\n40.7 / 64.4\n39.7 / 64.6\nfj_en\n21.6 / 45.4\n12.2 / 36\n14.6 / 39.2\n14.5 / 38.8\n0.3 / 10.8\n5.1 / 24.8\n5.2 / 24.1\n4.3 / 26.2\nfo_en\n37.1 / 59.5\n27.5 / 52.8\n31 / 56.1\n36.7 / 59.9\n6.1 / 26\n30.1 / 54.4\n32.7 / 56.5\n33.9 / 57.4\nfon_en\n12.9 / 35.4\n3.9 / 25.1\n5.6 / 26.3\n6.4 / 27.8\n0.1 / 7.9\n0.8 / 12.6\n1.5 / 17.8\n1.8 / 18.3\nfr_en\n47.9 / 70.1\n46.1 / 69.5\n47.2 / 70.3\n47.1 / 70.1\n2.8 / 21.5\n39.1 / 64.5\n39.9 / 64.9\n40 / 65.1\nfur_en\n45.3 / 68.6\n36.9 / 63\n30.8 / 58\n37.4 / 63.4\n3.6 / 23.8\n24.2 / 50.3\n24.2 / 53.4\n23 / 54.7\nga_en\n45.9 / 68.1\n43.4 / 67\n45.8 / 69\n45.9 / 68.8\n3.3 / 21.1\n29.4 / 53.8\n31.9 / 57.4\n34.2 / 59.4\ngd_en\n36.8 / 60.9\n33.4 / 58.9\n37.3 / 61.9\n37.5 / 61.6\n2.1 / 18.1\n21.7 / 46.8\n22.8 / 49.1\n22.1 / 49.6\ngl_en\n45.6 / 68.9\n44.1 / 68.5\n46 / 69.7\n45.8 / 69.6\n5.3 / 19.6\n35.9 / 62.7\n36.6 / 63.8\n36.9 / 64.4\ngn_en\n27.7 / 52\n19.5 / 45\n18.2 / 44\n20.8 / 46.9\n0.6 / 11.9\n5 / 26.3\n8 / 30.8\n8.1 / 29.9\ngu_en\n44.6 / 68.4\n39.7 / 64.9\n41.9 / 66.1\n41.2 / 65.5\n2.1 / 13.4\n25.4 / 54.1\n28 / 56.7\n28.7 / 57.2\nha_en\n36.4 / 58.7\n31.8 / 54.5\n35.9 / 57.9\n35.4 / 57.6\n0.8 / 15\n17.2 / 40.1\n21.6 / 44.8\n21.1 / 45.7\nhi_en\n44.4 / 68.3\n40.4 / 65.1\n42.9 / 66.6\n41.7 / 65.9\n2.9 / 12.9\n26.1 / 53.9\n30.9 / 58.8\n32.8 / 60.1\nhne_en\n52.2 / 73.8\n40.1 / 65.5\n35.1 / 61.7\n41.9 / 66.9\n0.5 / 4.9\n22.4 / 52.1\n21.5 / 53.8\n23.2 / 54.5\nhr_en\n38.7 / 64.4\n39 / 64.8\n39.9 / 65.4\n39.6 / 65.3\n6.4 / 25.6\n33.8 / 60.3\n34.9 / 61\n35.8 / 62\nht_en\n39.9 / 64\n39.3 / 63.5\n40.5 / 64.3\n40.9 / 65.1\n0.9 / 10.9\n26.9 / 52.6\n29.4 / 55\n29.4 / 56.5\nhu_en\n37.3 / 63.7\n37.5 / 63.9\n38.2 / 64.4\n38.1 / 64.5\n3.6 / 22.9\n29.1 / 57.1\n29.6 / 58\n29.7 / 58\nhy_en\n42.3 / 67\n33.5 / 59.4\n40.7 / 65.2\n40.1 / 64.8\n2.5 / 13.6\n27 / 54\n31.1 / 58.5\n31.6 / 59.7\nid_en\n46.5 / 69\n42.6 / 66.6\n45.3 / 68.4\n44.1 / 67.7\n2.4 / 16.1\n38.4 / 63.4\n38.8 / 63.6\n37.7 / 63.1\nig_en\n33.2 / 55.9\n25 / 48.5\n29.7 / 52.8\n29.2 / 52.7\n0.5 / 11.9\n10.8 / 31.5\n11.3 / 32.3\n9.6 / 36.1\nilo_en\n41 / 63.6\n24.2 / 50.6\n33.3 / 57.3\n32.5 / 57\n0.6 / 12.6\n19.8 / 44.5\n19.8 / 44\n20.6 / 47.2\nis_en\n34.9 / 59.3\n36.9 / 61\n38 / 62.3\n37.7 / 62\n7.2 / 28.5\n31.8 / 57.2\n33 / 58.7\n32 / 57.8\nit_en\n36.6 / 63.3\n34.7 / 63\n35.4 / 63.6\n35.6 / 63.6\n4.7 / 24\n29.9 / 57.9\n29.4 / 59.5\n29.8 / 59.7\nhe_en\n46.5 / 69\n43.3 / 65.9\n46.4 / 68.4\n45.5 / 68.1\n4.1 / 13.9\n38.3 / 63\n39.7 / 64\n38.9 / 63.9\nja_en\n30.3 / 58.2\n27.6 / 55.7\n28.8 / 55.9\n28.9 / 56.9\n0.1 / 0.6\n19.3 / 49.8\n19.6 / 48.8\n20.3 / 49.2\njv_en\n43.6 / 65.2\n25.9 / 51.9\n29.9 / 55.4\n29.7 / 54.8\n1.7 / 14.5\n27.4 / 52.3\n28.3 / 52.8\n29.2 / 53.6\nka_en\n31.4 / 59.5\n21.5 / 48.9\n21.3 / 48.8\n28.7 / 55.6\n1.9 / 9.9\n22.4 / 50.9\n24.4 / 53.6\n25.2 / 54.7\nkab_en\n27.5 / 50.2\n6.6 / 28.2\n8.2 / 30.6\n8.3 / 31.1\n0.3 / 10.4\n1.8 / 16.8\n3.9 / 19.3\n2.9 / 19.5\nkac_en\n18.6 / 43.7\n6.3 / 30.1\n9.2 / 32.4\n10.1 / 34.1\n0.2 / 8.2\n1.4 / 13\n3.2 / 20\n2.9 / 20.2\nkam_en\n13.7 / 35.7\n6.6 / 26.8\n7.4 / 27.5\n8.8 / 29.6\n0.2 / 8\n2.3 / 18.1\n2.9 / 20.5\n3.4 / 20.5\nkbp_en\n13.1 / 35.7\n7.9 / 30.8\n9.5 / 32.6\n9.7 / 32.8\n0.2 / 8.6\n1 / 9.9\n2.3 / 20.6\n2.3 / 21.3\nkea_en\n48.1 / 70.1\n34.5 / 60.5\n33.6 / 59.1\n36 / 61.7\n0.6 / 10\n16.8 / 41.2\n23.3 / 49\n23.6 / 50.5\nkg_en\n21.1 / 44.6\n6.9 / 30.6\n8.7 / 32.2\n9.3 / 33.3\n0.1 / 5.2\n2 / 18.1\n3.4 / 20.1\n2.8 / 21.5\nki_en\n25.6 / 49.5\n7.6 / 30.6\n8.7 / 30.6\n9.5 / 32.5\n0.1 / 8\n2.5 / 18.4\n3.3 / 19.3\n2.6 / 20.7\nkk_en\n36 / 61.9\n32.2 / 58.7\n33.8 / 59.4\n33.6 / 59.6\n3.6 / 14.3\n24.8 / 52.2\n26.9 / 54.4\n26.3 / 54.1\nkm_en\n35.6 / 60.6\n33.7 / 59.6\n36.5 / 61.3\n35.6 / 60.9\n1.5 / 13.1\n24.5 / 52.4\n25.4 / 53.3\n26.8 / 54.5\nkmb_en\n13.7 / 35.7\n5.7 / 27\n6.5 / 27.6\n7.9 / 29.3\n0.1 / 8.9\n2.2 / 15.3\n2.9 / 18.5\n2 / 18.3\nkn_en\n36.9 / 63.1\n31 / 58.3\n32.8 / 59\n32.6 / 59.4\n0.7 / 6.8\n17.6 / 45.5\n20.9 / 50.6\n22.9 / 52.3\nko_en\n31.2 / 59\n30.8 / 58.6\n33.1 / 60.2\n32.5 / 59.9\n0.7 / 2.9\n20.6 / 49\n23.4 / 52\n22 / 52.3\nkr_Arab_en\n3.2 / 21.2\n5.4 / 24.1\n5.2 / 24.1\n6.9 / 26.2\n0.1 / 5.5\n0.6 / 12.1\n1.2 / 16\n0.8 / 14.5\nkr_en\n16.1 / 38.7\n13.1 / 36.9\n12.3 / 36.5\n14.4 / 39\n0.2 / 11.6\n1.6 / 17.4\n3.5 / 18.9\n2.9 / 19.5\nks_Deva_en\n27 / 52.7\n21.5 / 48.2\n19.8 / 46.5\n23.5 / 50.4\n0.3 / 5.3\n4.2 / 27\n3.5 / 26.3\n4.7 / 27.7\nks_en\n36.7 / 62.2\n26.4 / 54\n25.8 / 53.4\n29.8 / 56.6\n0.4 / 6.6\n5.4 / 28.9\n7.3 / 30.4\n4.2 / 28.9\nku_en\n29.8 / 54.6\n29.1 / 54.2\n30.8 / 55.5\n32.2 / 56.8\n1.8 / 16\n16.7 / 40.9\n21.6 / 47.4\n21.6 / 49.1\nky_en\n24.1 / 52.5\n20.2 / 47.4\n26.6 / 54.4\n24.8 / 52.4\n1.7 / 12.8\n14.2 / 41.2\n17.6 / 46.1\n16.6 / 45.4\nlb_en\n49 / 71.2\n19.1 / 47.2\n21.3 / 49\n23.1 / 50.8\n3.5 / 20.6\n35.3 / 60.6\n38.2 / 63\n36.8 / 63.2\nlg_en\n25.3 / 48.2\n13.2 / 34.9\n16.5 / 38.7\n15.8 / 38.4\n0.3 / 10.9\n5.7 / 23.2\n5.4 / 25.5\n6.1 / 27.2\nli_en\n44.6 / 67.1\n35.4 / 60.6\n29.5 / 55.9\n35.8 / 61.3\n4.9 / 27.4\n25.9 / 51.4\n27.5 / 52.9\n28.5 / 53.3\nlij_en\n49.2 / 70.8\n40.6 / 65.5\n33.6 / 60.2\n40.3 / 65.3\n4 / 24.5\n26 / 53.5\n26.2 / 54.7\n24.4 / 55.2\nlmo_en\n42.9 / 66.3\n34.8 / 60.3\n30.2 / 56.8\n35.7 / 61.3\n2.3 / 20.6\n20.6 / 47.6\n22.2 / 50.2\n23 / 50.9\nln_en\n28.9 / 52.1\n5.7 / 27.4\n5.8 / 27.2\n6.8 / 29.5\n0.3 / 11.3\n6.6 / 27.3\n6.6 / 25.5\n6.8 / 29.9\nlo_en\n39.5 / 63.4\n35.1 / 60.1\n37.7 / 61.9\n38.4 / 63.2\n1.1 / 10.2\n26.1 / 52.8\n28.2 / 55.3\n28.9 / 56.9\nlt_en\n35.5 / 61.1\n35.1 / 61.4\n36.8 / 62.2\n36.2 / 62.1\n4.5 / 21.9\n28.9 / 56.4\n30.1 / 57.3\n30.1 / 57.5\nltg_en\n40.4 / 65.3\n35.3 / 62\n30.3 / 57.8\n36.5 / 62.9\n4.4 / 26\n25.8 / 52.4\n25.6 / 52.1\n26.6 / 53.4\nlua_en\n19.9 / 43.1\n6.3 / 27.8\n6.7 / 28\n7.9 / 30\n0.2 / 7.9\n4.3 / 21.3\n4.2 / 21.9\n4.3 / 23.8\nluo_en\n22 / 45.4\n4.1 / 23.6\n4.4 / 24.3\n5.4 / 26.4\n0.2 / 9\n1.6 / 19.4\n2.8 / 19.1\n2.9 / 18.8\nlus_en\n18.4 / 43.6\n13.1 / 37.7\n18.5 / 43.2\n18.4 / 43.8\n1.1 / 15.1\n9.8 / 32.1\n9.9 / 31.9\n10.9 / 34.4\nlv_en\n37.1 / 62.9\n39 / 64.7\n40.7 / 65.7\n40.1 / 65.5\n3.4 / 20\n32.9 / 60.1\n33.3 / 60.2\n33.3 / 60.8\nmag_en\n51.6 / 73.7\n40 / 65.8\n35.8 / 62.5\n42.1 / 67.3\n0.5 / 4.6\n24.2 / 53.5\n26.6 / 55.7\n26.8 / 57.2\nmai_en\n46.7 / 70.2\n35.4 / 62.4\n34.1 / 60.9\n38.1 / 64.4\n1 / 7.4\n21.5 / 50.4\n23.3 / 52.4\n25 / 54.9\nmg_en\n35.1 / 59.1\n30.4 / 55\n33.8 / 57.7\n34 / 58\n1.7 / 18.5\n15.6 / 39.5\n19.1 / 44.7\n20 / 45.9\nmi_en\n31 / 54.1\n20.8 / 45.2\n18 / 43.1\n21.6 / 46.8\n1.9 / 18.7\n10.1 / 32\n13.4 / 39.1\n12.6 / 39.3\n40\n#N/A min_en\n41.7 / 64\n27.3 / 52.9\n25 / 51.2\n30.6 / 55.9\n0.6 / 12.5\n19.4 / 43.4\n23.4 / 47.7\n21.7 / 48.7\nmk_en\n45.4 / 68.7\n44.7 / 68.4\n45.9 / 69.1\n46 / 69.2\n7.7 / 28\n37.7 / 62.8\n39.9 / 64.9\n39.7 / 64.9\nml_en\n39.1 / 64.9\n33 / 59.4\n34.3 / 59.9\n35 / 61.1\n0.9 / 4.8\n19.2 / 47\n23.8 / 52.6\n23.9 / 53\nmn_en\n28.9 / 56.3\n28.7 / 55.9\n32.2 / 58.9\n31.9 / 58.8\n0.8 / 8\n18.5 / 45.5\n20.5 / 48\n20.1 / 47.9\nmni_en\n27.8 / 53.5\n23.8 / 51.1\n22.6 / 49.5\n24.1 / 51.2\n0.1 / 2.2\n1.9 / 20.5\n3.3 / 22.3\n2.2 / 21\nmos_en\n12.2 / 34.3\n3.4 / 23\n3.4 / 22.4\n3.7 / 24.1\n0.3 / 9.9\n1.4 / 15\n1.4 / 18\n1.6 / 17.5\nmr_en\n40.3 / 65.8\n35.2 / 61.3\n36.3 / 61.6\n36.9 / 62.4\n1.7 / 7.8\n24.8 / 53.7\n27.8 / 56.3\n28 / 56.4\nms_en\n47.7 / 69.7\n43.9 / 67.1\n46 / 68.7\n45.7 / 68.6\n2.7 / 18.7\n38.5 / 63.4\n39.9 / 64.6\n40.3 / 64.8\nmt_en\n59.2 / 77.6\n57.6 / 77.1\n59.2 / 78.1\n59.4 / 78.4\n4.6 / 22\n44.8 / 67.3\n46.1 / 68.8\n47 / 70.2\nmy_en\n31.5 / 58.4\n28.7 / 55.1\n30.9 / 56.7\n31 / 57.2\n0.2 / 4.2\n16.1 / 45.6\n16 / 43.7\n17.7 / 45.9\nne_en\n44.5 / 68.8\n39.8 / 65.6\n41 / 66.2\n41.8 / 67.2\n1.1 / 7.5\n25 / 54.4\n28.2 / 56.7\n27.1 / 55.7\nnl_en\n34.4 / 61.2\n34.3 / 61.2\n34.3 / 61.3\n34.3 / 61.4\n6.5 / 28.8\n27.1 / 55.2\n28.8 / 58.1\n29.1 / 57.7\nnn_en\n46.4 / 68.8\n47 / 69.7\n48.3 / 70.5\n47.8 / 70.4\n4.8 / 26.9\n42 / 65.8\n42.3 / 66.5\n43.1 / 66.9\nno_en\n45 / 68\n45.1 / 68.4\n46.6 / 69.5\n45.9 / 69\n5.5 / 26.2\n40.5 / 65\n40.7 / 65.2\n39.9 / 65.7\nnso_en\n42.6 / 63\n26.5 / 48.3\n31.8 / 53.2\n31.9 / 53.2\n0.4 / 11.4\n9.1 / 26.9\n11.3 / 33.8\n12.7 / 36.1\nnus_en\n18.6 / 41.2\n12.9 / 36.1\n11.9 / 35.2\n14.8 / 38.3\n0.1 / 6.9\n0.5 / 11.9\n1 / 16.1\n0.6 / 16.3\nny_en\n29.2 / 52.9\n24.3 / 47.2\n27.2 / 50.1\n27.2 / 50.5\n0.6 / 12\n8.4 / 27.7\n12.2 / 35.4\n13.9 / 37.2\noc_en\n58.4 / 77.2\n55 / 75.4\n58.9 / 77.6\n57.4 / 76.8\n7.8 / 27.7\n42.5 / 66.5\n43.8 / 67.3\n42.9 / 68.2\nom_en\n27.2 / 52.1\n11.1 / 35.6\n17.3 / 42.1\n17 / 42.6\n0.4 / 10.5\n4.1 / 22.4\n5 / 27.1\n5.9 / 29\nor_en\n41.6 / 66.2\n34.9 / 61\n36.1 / 61.5\n36.1 / 61.7\n1 / 8.6\n19.1 / 47.4\n20.6 / 49.1\n22.5 / 50.7\npa_en\n44.8 / 67.9\n39 / 63.7\n42 / 65.8\n41.1 / 65.2\n1.5 / 11.5\n24.9 / 52.8\n26.3 / 53.7\n27.2 / 55.2\npag_en\n27.9 / 52.3\n22 / 46.4\n22.9 / 46.2\n24.4 / 48.5\n0.5 / 11\n17.6 / 40.7\n17.9 / 42.2\n17.7 / 42.8\npap_en\n53 / 73.4\n48.2 / 70.8\n49.7 / 71.7\n53.4 / 73.9\n1.4 / 15.8\n33.1 / 58.1\n37.4 / 61.9\n37.6 / 62.8\npl_en\n32.1 / 59.5\n30.5 / 58.6\n31.8 / 59.4\n31.5 / 59.3\n8.5 / 32.3\n25.7 / 54.6\n26.2 / 55.4\n26 / 55.4\nps_en\n36.6 / 61.8\n30.1 / 57.1\n30.6 / 57.2\n32.6 / 59.1\n1.2 / 11.8\n15.7 / 39.4\n22.9 / 49\n23.2 / 50.7\npt_en\n51.7 / 72.7\n50.7 / 72.7\n52.3 / 73.6\n51.7 / 73.2\n5.5 / 24\n43.4 / 67.8\n44.2 / 68.4\n44.3 / 68.3\nquy_en\n13.6 / 36.6\n8.7 / 31.5\n10.3 / 32.7\n11.4 / 34.1\n0.3 / 8.5\n2.8 / 23.2\n2.7 / 21.7\n3.5 / 22.8\nrn_en\n27.4 / 51.2\n19.9 / 43.2\n23.5 / 46.6\n23.6 / 46.9\n0.2 / 7.1\n10.1 / 30.6\n12.3 / 35.4\n11.6 / 35.3\nro_en\n46.9 / 70.4\n45.1 / 69.6\n46.2 / 70.3\n46.1 / 70.2\n6.3 / 27.3\n37.6 / 62.9\n38.2 / 64.9\n39 / 64.9\nru_en\n38.5 / 63.7\n37.5 / 63.1\n38.8 / 64.1\n38.3 / 63.8\n3.2 / 15.8\n29.3 / 55.7\n31.1 / 58.9\n30.5 / 59\nrw_en\n34.8 / 57.4\n29.1 / 52.1\n33.5 / 55.9\n33.7 / 56.2\n1.9 / 18.2\n19.8 / 43.2\n19.1 / 42.5\n17.3 / 44.9\nsa_en\n26.1 / 52.9\n20.7 / 47.4\n21.5 / 48\n16.7 / 41.8\n1.3 / 12.3\n11.8 / 37.9\n13.6 / 42.3\n11.4 / 42.5\n#N/A sc_en\n47.8 / 69.3\n37.1 / 62.1\n30.1 / 56.6\n36.9 / 62.3\n3 / 22.8\n18.4 / 45.6\n20.6 / 48\n20.5 / 49.2\nscn_en\n41.3 / 64.5\n33.5 / 59.8\n27.4 / 54.8\n33.3 / 59.6\n3.8 / 24.5\n24.3 / 50.5\n26.7 / 53.6\n25.1 / 53.9\nsd_en\n45 / 67.9\n39 / 62.9\n42.2 / 65.6\n41.7 / 65\n1.1 / 9.8\n24.2 / 50.5\n25.3 / 51.8\n26.4 / 53\nsg_en\n14.4 / 36.7\n8.2 / 31.1\n9 / 31.7\n10 / 32.8\n0.2 / 9.7\n3.5 / 19.2\n2.7 / 19.3\n2.7 / 22.1\nshn_en\n26.5 / 51.5\n18.6 / 45.3\n17.3 / 43.5\n20.2 / 47\n0.1 / 3.3\n1.9 / 17.3\n2 / 18.4\n1.8 / 17.4\nsi_en\n37.9 / 63.7\n34.1 / 60.4\n36 / 61.7\n35.9 / 62.2\n0.7 / 6.3\n17.2 / 45.6\n21.6 / 50.2\n21.9 / 50.6\nsk_en\n41.1 / 66.1\n41.4 / 66.9\n42.2 / 67.5\n42.3 / 67.7\n7.3 / 29.5\n34.3 / 61.4\n35.7 / 62.5\n35.5 / 62.4\nsl_en\n36.5 / 63\n37.1 / 63.2\n37.8 / 63.9\n37.9 / 63.9\n4.8 / 25.4\n31.5 / 58.8\n32.1 / 59.4\n31.7 / 59.1\nsm_en\n35.8 / 58.3\n28 / 52.3\n31.6 / 55.9\n33.4 / 56.4\n0.7 / 13.1\n14.2 / 36.3\n18.1 / 42\n15.4 / 41.8\nsn_en\n28.9 / 51.9\n23.7 / 46.6\n27.2 / 49.9\n26.9 / 49.9\n0.6 / 11.4\n12.4 / 33.2\n13.1 / 37.5\n14.7 / 37.7\nso_en\n30.2 / 54.4\n27.8 / 51.6\n30.8 / 54.6\n31.2 / 55\n1 / 11.6\n17.5 / 40.6\n21.1 / 45.1\n20.1 / 45.7\nsq_en\n44.9 / 68.4\n41.6 / 66.9\n43.7 / 68.2\n43.5 / 68\n11.3 / 34.5\n35.8 / 62\n36.5 / 62.4\n37.1 / 63.4\nsr_en\n46.3 / 69.4\n44.1 / 68\n46.3 / 69.4\n45.7 / 69.2\n6.1 / 22.2\n38.7 / 63.8\n39.1 / 65.4\n40.7 / 66\nss_en\n29.2 / 52.3\n21.8 / 43.8\n25.3 / 47.5\n25 / 47.7\n0.4 / 11\n6.9 / 25.1\n8.2 / 29.2\n8.9 / 32.3\nst_en\n41.2 / 62.9\n27.9 / 51.2\n34.2 / 57\n34 / 56.8\n0.6 / 12\n10.1 / 30.2\n15.5 / 38.9\n13.2 / 40.1\nsu_en\n40.2 / 63.6\n34.3 / 59.1\n32.6 / 58.1\n35.8 / 60.7\n1.8 / 16.4\n26.4 / 51.8\n26.6 / 52.3\n29.2 / 54.9\nsv_en\n49.4 / 71.2\n49.8 / 71.6\n50.4 / 72.1\n50.4 / 72.2\n7.6 / 30.5\n44.7 / 67.8\n45.3 / 68.1\n45.5 / 68.4\nsw_en\n46.2 / 67.4\n43.3 / 64.9\n46.5 / 67.1\n46.1 / 66.9\n1.8 / 15.3\n30.8 / 54.3\n34.3 / 57.9\n36 / 59.5\nszl_en\n45.8 / 69\n39.4 / 64.4\n34 / 60.4\n40.3 / 65.2\n6.9 / 29\n29.6 / 54.9\n29.2 / 55.6\n28.8 / 56.2\nta_en\n36.8 / 62.8\n30.9 / 57.4\n32.3 / 58\n33.1 / 59.3\n1 / 5.2\n22.9 / 51\n22.7 / 50.8\n24.1 / 52.4\ntaq_Tfng_en\n8.8 / 30.1\n4.9 / 27.3\n6.7 / 28.8\n6.5 / 29.6\n0.1 / 5.4\n0.2 / 12.9\n0.4 / 14.7\n0.3 / 13.7\ntaq_en\n13.4 / 35.8\n10.2 / 32.8\n9.6 / 32.6\n11.5 / 34.7\n0.3 / 9.4\n1.1 / 18.7\n2.1 / 18.6\n3.3 / 20.1\nte_en\n43.6 / 67.3\n37.4 / 62.4\n38.2 / 62.5\n39.5 / 63.9\n1.1 / 3.3\n23.8 / 51.5\n27.5 / 55.9\n27.2 / 55.4\ntg_en\n36.5 / 61.7\n13.7 / 38.2\n24.8 / 50.5\n22.5 / 47.3\n2.3 / 16.1\n23.4 / 49.9\n26.5 / 54.7\n27.3 / 54.3\nth_en\n33 / 60\n31.4 / 57.4\n33.3 / 59.2\n33.3 / 59.6\n2.1 / 6.7\n24.1 / 53.1\n24.1 / 54.7\n24.8 / 54.5\nti_en\n26.4 / 52.4\n15.2 / 41.5\n19.8 / 46\n20.6 / 47.1\n0.6 / 10.2\n8 / 29.8\n8.8 / 34.6\n9.3 / 35\ntk_en\n34.9 / 61.2\n31.3 / 57.7\n33 / 58.9\n33.5 / 58.8\n2.4 / 18.5\n19.1 / 45.6\n20 / 47\n20.2 / 49.2\ntn_en\n29.7 / 53.4\n18.7 / 42.6\n21.9 / 46.7\n23.4 / 46.8\n0.3 / 10.8\n7.4 / 25.9\n9.9 / 31.8\n7.4 / 31.1\ntpi_en\n30.4 / 54.6\n10.7 / 37.8\n10.5 / 37.3\n14.2 / 42.2\n0.9 / 15.8\n7.3 / 31.1\n8.3 / 28.7\n8.9 / 29.3\ntr_en\n41.5 / 66.1\n39.1 / 63.8\n41.4 / 65.5\n40.9 / 65.3\n3.4 / 20.4\n25 / 51.8\n28.1 / 55.5\n29.1 / 56.7\nts_en\n32.1 / 54.9\n19.6 / 42.3\n24.3 / 47.3\n25.4 / 47.7\n0.4 / 10.3\n7.4 / 25.9\n8.8 / 28.1\n7.8 / 30.7\ntt_en\n31.8 / 58.3\n10.4 / 30.4\n10.3 / 29.4\n17.5 / 38.5\n1.2 / 9.9\n16.5 / 42.1\n21.3 / 48.5\n21.3 / 48.5\ntum_en\n20.5 / 44.9\n12.2 / 37\n15.2 / 40\n15.9 / 40.5\n0.2 / 6.9\n7.2 / 27.2\n7 / 27.3\n7.2 / 28.7\nug_en\n27 / 54.6\n8.6 / 29\n14.1 / 36\n10.9 / 30.6\n1.6 / 14.4\n13.4 / 39.4\n17.3 / 44.9\n17.3 / 44.9\nuk_en\n42.4 / 66.7\n42.1 / 66.4\n43 / 66.8\n42.5 / 66.7\n6.6 / 24.8\n34.6 / 60.5\n35.9 / 61.7\n34.5 / 62\numb_en\n11.4 / 33.3\n4.4 / 24.2\n4.7 / 24.2\n5.5 / 25.9\n0.2 / 9.6\n1.1 / 17.2\n2.3 / 18.3\n1.6 / 17.1\nur_en\n39.6 / 64.8\n33.3 / 59.6\n36.3 / 62.1\n35.9 / 61.7\n3.1 / 17.4\n24.8 / 52.8\n26.7 / 54.1\n28.7 / 56.2\nuz_en\n35.2 / 61.6\n33.9 / 59\n37.2 / 62.1\n37.8 / 63.2\n1.8 / 15.6\n23.5 / 51.2\n24.6 / 52.3\n24.9 / 52.9\nvec_en\n45.4 / 68\n39.4 / 64.6\n33.4 / 59.9\n39.4 / 64.7\n4 / 23.6\n31 / 56.5\n34.1 / 60\n35.1 / 60.2\nvi_en\n40 / 64.2\n37.4 / 62.5\n39.2 / 63.9\n39.3 / 63.8\n4.1 / 19.7\n28 / 53.5\n31.7 / 57.3\n32.1 / 57.9\nwar_en\n51 / 71.1\n32.1 / 58.6\n32.5 / 58.8\n35.9 / 60.9\n1.1 / 15.5\n25 / 50\n27.4 / 55.3\n31.8 / 56.4\nwo_en\n19.8 / 42.9\n6.6 / 28.4\n6.7 / 29.3\n7.1 / 29.8\n0.3 / 9\n1.8 / 16.9\n3 / 18.2\n3.2 / 19.7\nxh_en\n39.7 / 61.1\n32.1 / 54.3\n36.5 / 58.2\n35.5 / 57.7\n1 / 15.2\n17.2 / 39.3\n21 / 43.9\n21.1 / 44\nyi_en\n53.6 / 72.4\n38.8 / 62.6\n48.8 / 69\n45.5 / 67.4\n1.2 / 8.5\n34.2 / 56\n37.6 / 59.6\n39.1 / 62.2\nyo_en\n23.9 / 47.9\n11.9 / 34.9\n17.4 / 40.8\n17.8 / 40.7\n0.4 / 12.5\n5.1 / 23.5\n4 / 24.1\n6.3 / 26\nyue_en\n30.8 / 58.5\n30.4 / 57.9\n31.7 / 58.7\n32 / 59.3\n0.3 / 1.3\n21.5 / 49.9\n22.5 / 51.3\n23.3 / 52.5\nzh_Hant_en\n28.8 / 56.2\n28.1 / 55.7\n29.6 / 56.8\n30 / 57.5\n0.6 / 2.7\n19.6 / 47.3\n21.2 / 49.6\n21.6 / 50.5\nzh_en\n31.9 / 59.5\n29 / 57.9\n30.4 / 59\n30.5 / 59.2\n0.3 / 1.9\n21.8 / 50.2\n22.5 / 51.5\n23.4 / 52.1\n41\nzu_en\n41.3 / 62.7\n33 / 55.1\n37.7 / 59.1\n37.1 / 58.9\n0.8 / 13.4\n17.5 / 40\n20.3 / 44\n20.2 / 45.5\nxx2en\n35.5 / 59.6\n29.7 / 54.4\n30.9 / 55.4\n31.9 / 56.4\n2.0 / 13.3\n20.5 / 44.1\n22.3 / 46.9\n22.4 / 47.6\nen2xx\n20.7 / 50.1\n17.3 / 44.1\n17.8 / 44.7\n18.6 / 45.7\n0.4 / 5.7\n8.1 / 26.7\n8.7 / 29.0\n8.7 / 28.8\nMean\n28.2 / 54.9\n23.5 / 49.2\n24.4 / 50.0\n25.3 / 51.1\n1.2 / 9.6\n14.3 / 35.5\n15.6 / 38.0\n15.6 / 38.2\nxx2yy\n13.7 / 40.5\n8.8 / 31.2\n8.4 / 30.9\n10.1 / 34.0\n0.3 / 4.1\n4.0 / 16.1\n4.4 / 17.3\n4.2 / 17.1\nTable 18: Evaluation scores on Flores-200 direct pairs (depicted as <bleu> / <chrf>) for the MT\nmodels and language models described in Section 4.1 and Section 4.2 compared against NLLB-54B.\nAll metrics are computed with the sacrebleu reference implementation.\nNLLB\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\nckb_cs\n20.7 / 47.3\n15.4 / 39.4\n18.5 / 44.9\n19.1 / 45.1\n0 / 0.8\n8.1 / 30.2\n10.4 / 35.5\n9.7 / 36.6\nckb_cy\n26.9 / 53.6\n19.3 / 41.4\n23.6 / 47\n23.9 / 47.8\n0 / 0.7\n0.3 / 8.8\n0.7 / 15.1\n0.3 / 12.4\nckb_en\n37.5 / 61.6\n28.3 / 53.1\n34.4 / 58.9\n34.2 / 59\n1 / 9.7\n17.9 / 42.5\n22.4 / 48.4\n22.7 / 49.4\nckb_et\n16.4 / 48.6\n13 / 40.8\n15.3 / 45.6\n16.3 / 47\n0 / 1\n1.9 / 12.9\n6.7 / 36.6\n7.3 / 38.2\nckb_eu\n11.3 / 47.4\n7.2 / 38.6\n10 / 44.2\n11.5 / 46\n0 / 0.5\n1.3 / 18.4\n1.9 / 30.8\n2.1 / 33.5\nckb_fon\n2.1 / 19.9\n1 / 14.7\n0.4 / 10.8\n1 / 14.4\n0 / 0.2\n0.3 / 8.5\n0 / 1.8\n0 / 3.2\nckb_fr\n31.2 / 56.9\n23.8 / 45.4\n30.5 / 53.5\n28.7 / 51.9\n0.1 / 2.5\n13.5 / 38.4\n18.7 / 45.2\n18.8 / 45.6\nckb_ln\n15.5 / 47.5\n1.3 / 16\n0.5 / 14.4\n1.2 / 17.4\n0 / 0.3\n0.1 / 6.2\n0.1 / 10.3\n0.2 / 11.4\nckb_mr\n11 / 43.3\n3.3 / 22.5\n1.4 / 22.4\n4.7 / 27.9\n0 / 0.4\n0.6 / 4.7\n0.6 / 21.5\n0.8 / 24.8\nckb_ny\n10.4 / 44.7\n3.5 / 29.7\n2.7 / 31\n6.8 / 35.9\n0.1 / 0.4\n0.3 / 13.1\n0.1 / 9.9\n0.1 / 8.5\nckb_or\n8.6 / 41.8\n7 / 36\n7.3 / 38.5\n8.5 / 40.1\n0 / 0.1\n0 / 0.2\n0 / 0.3\n0 / 0.3\nckb_so\n8.8 / 42.2\n5 / 32.4\n4.1 / 34.4\n6.9 / 37.2\n0 / 0.3\n0.3 / 6.6\n0.1 / 5.8\n0.1 / 7.4\nckb_ss\n6.5 / 44.3\n2 / 27.4\n0.9 / 23.5\n3.1 / 30.9\n0 / 0.2\n0.1 / 7.7\n0.1 / 9.8\n0 / 9.3\nckb_ti\n4 / 24.2\n1.1 / 12.8\n0.3 / 10.5\n1.7 / 16\n0 / 0.1\n0 / 0.4\n0 / 0.1\n0 / 0.1\nckb_yo\n5.5 / 27\n1.2 / 15.2\n1.2 / 16.6\n1.8 / 18.2\n0 / 0.3\n0 / 4.1\n0 / 3.3\n0 / 4\nckb_zh\n27.6 / 25.2\n3 / 18.9\n2.2 / 22.1\n4.1 / 22.1\n0.4 / 0.9\n1.5 / 3.4\n15.7 / 15.7\n14.6 / 16\ncs_ckb\n8.8 / 45.4\n5.8 / 37.6\n1.3 / 30.8\n6 / 37.8\n0 / 0.2\n0.1 / 8\n0.1 / 9.9\n0.1 / 8.8\ncs_cy\n29.7 / 57.6\n24.3 / 48\n28.4 / 52.4\n27.4 / 51.9\n0.3 / 7.4\n2.5 / 19.3\n3.3 / 29.4\n2.2 / 28.5\ncs_en\n41.2 / 66.4\n41.2 / 66.6\n42.3 / 67.4\n42.3 / 67.3\n7.9 / 28.9\n34.7 / 61.4\n35.9 / 62.6\n36.2 / 62.9\ncs_et\n20.4 / 53.9\n22.8 / 54.9\n22.3 / 54.5\n23.5 / 56\n1.8 / 14.1\n15.2 / 45.8\n17.9 / 51.1\n17.6 / 51.3\ncs_eu\n11.8 / 47.8\n11.9 / 45.3\n8.9 / 44.7\n12.6 / 47.5\n0.2 / 7.6\n7.3 / 40.6\n6.3 / 44.6\n7 / 46\ncs_fon\n2.5 / 21.6\n1.2 / 14.3\n0.7 / 11.9\n1 / 14\n0 / 2.3\n0.1 / 4.3\n0 / 5\n0 / 1.7\ncs_fr\n35.3 / 60.9\n38.7 / 62.3\n39.5 / 62.6\n40.1 / 63.2\n1.2 / 10.2\n24.5 / 51.6\n24.2 / 54\n25.6 / 54.6\ncs_ln\n15.6 / 47.8\n1.6 / 17.6\n1.1 / 17.1\n2.3 / 20.9\n0.2 / 6.2\n0.3 / 10.6\n0.1 / 8.9\n0.1 / 7.2\ncs_mr\n10.8 / 43.1\n3.6 / 24.9\n1.6 / 22.9\n4.5 / 26.8\n0.2 / 2.7\n3.7 / 28.8\n3.5 / 33.8\n2.9 / 35.2\ncs_ny\n10.1 / 44.7\n7.3 / 38.3\n3.9 / 36.2\n7.9 / 39.3\n0.2 / 5.3\n0.2 / 16\n0.3 / 13.6\n0.1 / 12.1\ncs_or\n9.1 / 42.1\n8.7 / 40.2\n9.3 / 41.3\n9.9 / 42.5\n0.1 / 0.2\n0 / 4.9\n0.2 / 0.5\n0.1 / 0.5\ncs_so\n9.1 / 42.7\n7.7 / 39\n7.3 / 39.3\n7.7 / 39.6\n0.2 / 5\n0.4 / 14.3\n0.2 / 13.5\n0.1 / 10.9\ncs_ss\n7.3 / 45.1\n2.4 / 26.2\n1.2 / 25.4\n3.2 / 30.4\n0.2 / 6.3\n0.2 / 20.7\n0 / 6.3\n0 / 7.4\ncs_ti\n4.3 / 24.9\n1.4 / 12.7\n0.3 / 9.8\n1.6 / 16.1\n0 / 0.3\n0 / 0.7\n0 / 0.7\n0 / 0.4\ncs_yo\n4.5 / 25.1\n2 / 18.4\n1.4 / 16.3\n2 / 17.7\n0.1 / 4.8\n0.1 / 11.7\n0 / 2.1\n0 / 7.1\ncs_zh\n23.6 / 23.3\n6.4 / 31.1\n7.3 / 31.8\n6.8 / 32.7\n2 / 2.3\n20.2 / 19.9\n19.7 / 20\n21.6 / 20.5\ncy_ckb\n10.5 / 48.3\n8 / 43.1\n5.2 / 41.6\n8.4 / 42.7\n0 / 0.2\n0 / 5.6\n0 / 5.2\n0 / 2.1\ncy_cs\n27.2 / 54.1\n28.9 / 55.6\n29.8 / 56.6\n29.7 / 56.4\n0.3 / 4.7\n16.1 / 41.8\n16 / 43.4\n12.8 / 43.4\ncy_en\n60 / 77.1\n57.6 / 75.6\n60.7 / 77.7\n60.8 / 77.7\n5.4 / 24.3\n43.9 / 65.6\n48.5 / 69.3\n47.7 / 68.7\ncy_et\n22.3 / 55\n24.2 / 56.4\n25 / 57.5\n25.8 / 57.9\n0.1 / 4.8\n5.6 / 31.3\n7.6 / 40.8\n7.5 / 42.2\ncy_eu\n14.4 / 51.6\n17.1 / 54.9\n19 / 56.7\n18.5 / 56.3\n0 / 4.8\n0.6 / 16.8\n2.8 / 36.2\n3.3 / 37\ncy_fon\n2.2 / 18.6\n1.3 / 14.9\n0.6 / 11.7\n1.3 / 14.9\n0 / 0.9\n0.5 / 7.1\n0 / 1.7\n0 / 5.5\ncy_fr\n40.9 / 64.2\n44 / 65.6\n45.8 / 66.7\n45.3 / 66.7\n0.7 / 9.3\n27.7 / 53.8\n28.7 / 54.9\n27.4 / 55.7\ncy_ln\n17.4 / 49.1\n2 / 18\n1.1 / 17.5\n2 / 20.1\n0.1 / 3.8\n0.3 / 11.4\n0.1 / 8.1\n0.1 / 8.8\ncy_mr\n13.9 / 47.5\n10.2 / 41.3\n6.4 / 39.1\n11.1 / 42.4\n0.1 / 1.5\n2.8 / 26\n2 / 29.7\n1 / 26.5\ncy_ny\n12.4 / 46.5\n6.1 / 36.2\n5.6 / 40.4\n9.3 / 41.9\n0.1 / 3\n0.8 / 13.6\n0.2 / 11\n0.1 / 9.1\ncy_or\n11.5 / 45.1\n11.2 / 44.4\n12.1 / 46\n12.3 / 46.2\n0 / 0.3\n0 / 0.4\n0.1 / 0.4\n0.1 / 0.4\ncy_so\n10.8 / 44.8\n8.6 / 41.3\n8.2 / 42.4\n9.6 / 43\n0.1 / 3.4\n1 / 13.3\n0.4 / 14.5\n0.2 / 12.9\ncy_ss\n9 / 46.7\n3.2 / 31.3\n1.4 / 28.3\n4.1 / 34\n0.1 / 3\n0.2 / 12.3\n0.4 / 11.3\n0.2 / 11.4\ncy_ti\n4.8 / 25.5\n1.5 / 14.2\n0.4 / 12.3\n2.1 / 17.9\n0 / 0.3\n0 / 0.8\n0 / 0.5\n0 / 0.2\ncy_yo\n5.4 / 26.3\n2.1 / 17.7\n1.7 / 18.3\n2.2 / 19.4\n0.1 / 2.5\n0 / 2.9\n0 / 4.1\n0 / 6.4\ncy_zh\n30.4 / 28.1\n5.4 / 31.5\n5.7 / 33.1\n5.3 / 32.9\n0.8 / 1.4\n8.4 / 9\n19.5 / 20.1\n15.3 / 19.3\nen_ckb\n13.2 / 52.5\n9.1 / 44.9\n4.5 / 42.8\n9.4 / 44.4\n0.1 / 0.3\n0.3 / 10.2\n0.1 / 13.8\n0.1 / 13.4\nen_cs\n33.6 / 59.8\n34.8 / 61.2\n34.8 / 61.1\n35.6 / 61.5\n1.1 / 11.1\n25.3 / 53.1\n26.2 / 54.5\n25.6 / 54.4\nen_cy\n51.6 / 72.5\n41.6 / 62.4\n45.6 / 66.3\n44.7 / 65.6\n0.5 / 9\n6.4 / 29.4\n8.2 / 38\n4.8 / 35.4\nen_et\n27.1 / 59.9\n28.9 / 61.3\n28.1 / 61.2\n29.8 / 61.8\n0.9 / 11.9\n22.2 / 54.7\n22.8 / 55.6\n22.2 / 55.3\nen_eu\n17.2 / 54.9\n18.3 / 55.5\n16.8 / 55.4\n19.3 / 57.2\n0.2 / 11.1\n12.3 / 49.3\n12.9 / 51\n12.4 / 51.3\nen_fon\n3.1 / 23.5\n1.6 / 15.6\n1.3 / 14.6\n1.6 / 15.9\n0 / 0.3\n0.1 / 4.2\n0 / 1.9\n0 / 1.6\nen_fr\n51.4 / 71.4\n52.4 / 72\n53.3 / 72.4\n53.2 / 72.5\n1.7 / 15.3\n37.1 / 61.9\n38.9 / 63.5\n38.8 / 63.1\nen_ln\n19.1 / 50.8\n2.2 / 18.3\n1.5 / 18.2\n2.7 / 20.8\n0.4 / 9.3\n0.7 / 11.3\n0.1 / 9.2\n0.2 / 10.7\nen_mr\n17.6 / 52.1\n10.1 / 39\n5.8 / 36.4\n11.8 / 42\n0.8 / 12\n10.5 / 42.8\n9 / 43.2\n8.1 / 43.7\nen_ny\n13.6 / 48.7\n8.2 / 38.7\n6.6 / 43.3\n10.6 / 43.5\n0.4 / 8\n0.7 / 16.4\n0.4 / 16\n0.4 / 17.6\nen_or\n15.1 / 50.4\n12.4 / 46.8\n13.2 / 47.4\n13.1 / 47.8\n0.1 / 0.2\n0.4 / 0.9\n0.3 / 0.8\n0.3 / 0.8\nen_so\n12.6 / 47.3\n9.4 / 41.8\n9.9 / 43.2\n10.2 / 43\n0.2 / 6.9\n0.6 / 15.2\n0.2 / 13\n0.2 / 13.3\nen_ss\n10.7 / 49.2\n4.1 / 33.5\n1.9 / 31.9\n5.3 / 36\n0.5 / 7.2\n0.6 / 15\n0.1 / 11.6\n0.1 / 9.5\nen_ti\n6 / 27.9\n1.9 / 15.7\n0.6 / 14.6\n2.5 / 18.9\n0.1 / 0.3\n0 / 0.9\n0 / 0.8\n0 / 0.6\n42\nen_yo\n6 / 27.4\n2.6 / 19.7\n1.9 / 19.6\n2.9 / 20.7\n0.3 / 6.3\n0.1 / 10.6\n0.1 / 5.3\n0 / 3.3\nen_zh\n31.2 / 29.3\n5.9 / 35.7\n6 / 36.5\n5.8 / 36.5\n2.9 / 3.3\n29.1 / 26.1\n29.9 / 26.6\n29.9 / 26.3\net_ckb\n8.7 / 44.9\n4.9 / 34.9\n2.8 / 34.8\n5 / 34.7\n0 / 0.4\n0.1 / 10.4\n0 / 8.7\n0 / 5.9\net_cs\n24.1 / 51.6\n25.3 / 52.3\n24.9 / 51.8\n25.3 / 52.6\n1.8 / 14.1\n15.8 / 42\n18.2 / 46\n17.4 / 46\net_cy\n27.9 / 56.1\n23.2 / 47.2\n25.4 / 49.3\n25.9 / 49.9\n0.4 / 10.2\n2.5 / 22.3\n1.5 / 25\n1.2 / 23.3\net_en\n38.8 / 64.6\n39.4 / 64.8\n40.1 / 65.5\n39.6 / 65.5\n7.7 / 27.9\n31.8 / 59.1\n32.5 / 59.8\n32.6 / 59.7\net_eu\n11.4 / 47\n13.4 / 48.5\n13.5 / 49.1\n14 / 50\n0.3 / 12.2\n5.9 / 37.8\n5.7 / 41.2\n3.9 / 41.1\net_fon\n2.5 / 22.4\n1 / 13.2\n0.5 / 11.3\n0.9 / 12.7\n0 / 2.6\n0.1 / 4.8\n0 / 3.5\n0 / 3.5\net_fr\n33.4 / 59.2\n35.1 / 59.3\n35.7 / 59.3\n36 / 60.1\n1.4 / 14.2\n23 / 50.2\n24.8 / 52.1\n25.3 / 52.6\net_ln\n14.8 / 47.6\n1.7 / 17.3\n0.8 / 16.1\n2 / 20.8\n0.3 / 10.7\n0.2 / 9.5\n0.1 / 9.3\n0.1 / 10.5\net_mr\n10.8 / 42.8\n5.4 / 29.3\n2.4 / 25.8\n5 / 28.5\n0.3 / 1.6\n0.7 / 2.7\n3.9 / 33.5\n2.8 / 35.3\net_ny\n9.7 / 43.8\n6.1 / 34.2\n3 / 33.2\n7.8 / 37.4\n0.4 / 9.9\n0.1 / 7\n0.1 / 11.4\n0.1 / 7.9\net_or\n9 / 41.8\n8.2 / 39.5\n7.7 / 39.9\n9 / 41\n0 / 0.3\n0.1 / 1.8\n0.3 / 0.5\n0.3 / 0.5\net_so\n8.6 / 42.4\n7.6 / 37.8\n6.6 / 38.4\n8.1 / 39.5\n0.3 / 9\n0.5 / 15\n0.4 / 14.5\n0.1 / 12.2\net_ss\n6.6 / 44.8\n2.5 / 26.3\n0.9 / 23.6\n3.1 / 29.3\n0.1 / 10.7\n0.1 / 21.8\n0 / 7.4\n0 / 7.8\net_ti\n4 / 24.1\n1.2 / 11.6\n0.3 / 8.9\n1.3 / 13.8\n0 / 0.3\n0 / 0.8\n0 / 0.7\n0 / 1.2\net_yo\n4.1 / 24.6\n2 / 17.4\n1 / 15\n1.6 / 16.2\n0.2 / 7.3\n0 / 4.1\n0 / 5.3\n0 / 3.2\net_zh\n22.9 / 22.6\n6.3 / 30.9\n6.1 / 31.7\n5.7 / 31.6\n2.1 / 2.8\n21.2 / 20.1\n18.9 / 21.3\n22.7 / 22.8\neu_ckb\n7.3 / 43.7\n4.6 / 34.2\n1 / 27.8\n4.3 / 32.4\n0 / 0.2\n0.1 / 8.3\n0 / 8.6\n0 / 5.3\neu_cs\n19.2 / 46.9\n16.7 / 42.2\n14.4 / 41.9\n18.2 / 44.4\n0.9 / 12.2\n13 / 40.6\n13.8 / 41.6\n13.2 / 41.2\neu_cy\n24 / 53.1\n22 / 46.1\n25.6 / 50.5\n24.5 / 49.7\n0.2 / 10.6\n1.2 / 18.7\n1.5 / 22.7\n0.9 / 21.6\neu_en\n34.4 / 61.2\n32.6 / 58.6\n34.1 / 59.8\n34.4 / 60.4\n4.6 / 23.3\n25.5 / 53.8\n24.7 / 52.9\n26.2 / 54.4\neu_et\n16.8 / 49.6\n16.4 / 47.7\n17.9 / 49.4\n18.7 / 50.2\n0.1 / 12.2\n7.1 / 35.2\n9.9 / 43.9\n9.6 / 43.6\neu_fon\n2.5 / 20.7\n1 / 13.4\n0.4 / 10.1\n1 / 13\n0.2 / 7.3\n0.1 / 9.1\n0 / 2.8\n0 / 1.6\neu_fr\n29.4 / 56.2\n28.8 / 53\n30.3 / 54\n30.3 / 54.4\n1.2 / 14\n20.3 / 48.3\n21.3 / 49.4\n21.1 / 49.5\neu_ln\n14.5 / 47\n1.6 / 17\n0.6 / 15.8\n1.9 / 19\n0.1 / 12.5\n0.4 / 16.5\n0.1 / 8.9\n0.1 / 8.5\neu_mr\n9.6 / 42.6\n2.8 / 22.6\n1.4 / 21\n3.7 / 24.6\n0.1 / 0.6\n3.3 / 31\n2.7 / 31.1\n2.4 / 32.1\neu_ny\n9.3 / 44.2\n4.3 / 30.7\n2.8 / 31.9\n6.4 / 35.1\n0.4 / 11.2\n0.2 / 12.8\n0.1 / 11.5\n0.1 / 11.3\neu_or\n7.9 / 40.3\n8.1 / 39.6\n7.4 / 40\n8.6 / 41\n0 / 0.3\n0 / 3.3\n0.2 / 0.5\n0.1 / 0.5\neu_so\n8.3 / 41.7\n6.4 / 35.4\n4.8 / 35.8\n7.1 / 37.8\n0.2 / 11.4\n0.4 / 12.8\n0.1 / 12\n0.1 / 10.9\neu_ss\n5.9 / 43.7\n1.9 / 25\n0.9 / 22.6\n2.8 / 29.4\n0.1 / 11.3\n0.1 / 8.1\n0 / 6\n0 / 8.7\neu_ti\n3.8 / 24.1\n0.8 / 11\n0.3 / 9.3\n1.5 / 15\n0 / 0.3\n0 / 1.9\n0 / 0.6\n0 / 0.7\neu_yo\n5 / 26.3\n1.7 / 16.8\n1 / 15.8\n1.7 / 16.8\n0.1 / 7.6\n0.1 / 7.5\n0 / 2.8\n0 / 4.2\neu_zh\n23.2 / 22.4\n5.6 / 22.6\n2.3 / 20.3\n5.9 / 24.9\n1.8 / 2.6\n17.5 / 17\n20.2 / 21\n14.3 / 19.4\nfon_ckb\n2.8 / 28.6\n0.9 / 20.6\n0.3 / 16.8\n1.6 / 24\n0 / 0.2\n0 / 0.2\n0 / 2\n0 / 1.5\nfon_cs\n7.6 / 27.8\n2.8 / 20.2\n2.1 / 19.3\n4.3 / 22.3\n0 / 1.3\n0.5 / 13\n0.8 / 13.9\n0.3 / 13.5\nfon_cy\n8.8 / 31.5\n4 / 21.8\n2.9 / 21.2\n6 / 25.6\n0 / 2.2\n0 / 7.3\n0.1 / 10.4\n0.1 / 7.1\nfon_en\n12.9 / 35.4\n3.9 / 25\n2.2 / 23.4\n6.4 / 27.8\n0.1 / 7.8\n0.8 / 12.5\n1.4 / 17.7\n1.9 / 18.3\nfon_et\n6.7 / 30.6\n2.2 / 22.3\n1.2 / 20.4\n3.4 / 24.9\n0 / 1.3\n0.7 / 15.5\n0.4 / 15.8\n0.6 / 16.1\nfon_eu\n4.6 / 31.3\n1.4 / 22.5\n0.9 / 19.8\n2.9 / 27.2\n0 / 2\n0.5 / 17.6\n0.1 / 12.4\n0.2 / 14.3\nfon_fr\n10.6 / 33.2\n5.3 / 24.4\n4.4 / 24.3\n7.6 / 26.8\n0.1 / 3.7\n0.9 / 11.2\n1.3 / 17.1\n1.3 / 16.4\nfon_ln\n8.2 / 36.8\n0.6 / 11\n0.2 / 7.6\n0.5 / 11.8\n0.1 / 3\n0.1 / 6.6\n0 / 5\n0 / 1.5\nfon_mr\n3.8 / 27\n0.8 / 16.5\n0.2 / 13.4\n1.5 / 18.7\n0 / 0.4\n0.3 / 8.3\n0.1 / 8.7\n0 / 7\nfon_ny\n5.6 / 33.3\n1.3 / 20.6\n0.7 / 18.7\n3 / 26.5\n0 / 1.9\n0.1 / 12.8\n0 / 4.4\n0 / 2.7\nfon_or\n2.7 / 25.1\n1.1 / 18.5\n0.7 / 17\n2 / 22\n0 / 0.2\n0 / 0.2\n0 / 0.4\n0 / 0.3\nfon_so\n4.3 / 31.2\n1.6 / 23\n0.8 / 19.6\n2.4 / 26.3\n0 / 2\n0.1 / 5.6\n0 / 5.5\n0 / 4.1\nfon_ss\n3 / 32.5\n0.8 / 16.7\n0.3 / 14\n1.8 / 23.2\n0 / 2.4\n0 / 4.5\n0 / 3.9\n0 / 1.8\nfon_ti\n1.7 / 15.5\n0.3 / 6.5\n0.1 / 5.5\n0.7 / 10.2\n0 / 0.2\n0 / 0.2\n0 / 0.1\n0 / 0.1\nfon_yo\n3.4 / 21.3\n0.7 / 12.3\n0.4 / 10.8\n1.1 / 14.9\n0 / 2.3\n0 / 3.2\n0 / 3.7\n0 / 1\nfon_zh\n11.8 / 12.9\n1.9 / 8.7\n0.5 / 6.2\n2.2 / 10.2\n0.1 / 0.6\n1.6 / 4.3\n1.8 / 5.1\n0.7 / 3.5\nfr_ckb\n9.2 / 46.9\n6 / 39.7\n3.3 / 38.4\n6.1 / 39.1\n0 / 0.3\n0.1 / 7.8\n0.1 / 13.8\n0.1 / 13.9\nfr_cs\n26 / 53.6\n27.3 / 54.9\n27.8 / 55.3\n27.8 / 55.2\n1.8 / 14.2\n19.3 / 46.8\n20.9 / 49.1\n20.4 / 49.4\nfr_cy\n33.7 / 60.1\n28 / 51.6\n31.7 / 56\n30.7 / 55\n0.3 / 9.5\n4.1 / 27.8\n4.2 / 32.8\n3.7 / 34.1\nfr_en\n47.9 / 70.1\n46.1 / 69.6\n47.3 / 70.3\n47.1 / 70.1\n2.7 / 21.4\n39.2 / 64.5\n39.9 / 64.9\n40.1 / 65.1\nfr_et\n20.9 / 54.3\n22.2 / 55.2\n18.5 / 54.5\n23.6 / 56.3\n1.7 / 15.8\n11.1 / 44.6\n16.9 / 50.6\n15.8 / 51\nfr_eu\n14 / 51.2\n13.1 / 50.8\n11.6 / 51.3\n16.1 / 53.5\n0.3 / 9.9\n10.2 / 47.7\n9.5 / 48.8\n9.6 / 49.4\nfr_fon\n2.5 / 22.6\n1.1 / 14.7\n0.7 / 12.3\n1.2 / 14.5\n0.2 / 2.4\n0 / 1.7\n0 / 3.8\n0 / 1.8\nfr_ln\n16.8 / 49.3\n1.7 / 17.9\n1.2 / 17.3\n2.1 / 20.2\n0.2 / 7.3\n0.1 / 8.7\n0 / 6.3\n0.2 / 10.6\nfr_mr\n12 / 45.5\n7.1 / 34.8\n2.2 / 27\n6.7 / 33.2\n0.5 / 8.8\n5.5 / 35.9\n5.3 / 38.3\n4.5 / 38.4\nfr_ny\n10.8 / 45.5\n5.4 / 34.9\n4.1 / 38.3\n7.8 / 40\n0.2 / 6.6\n0.5 / 16.1\n0.2 / 13.8\n0.2 / 13.8\nfr_or\n10.2 / 43.8\n9.3 / 41.7\n10.1 / 43.5\n10.1 / 43.6\n0 / 0.2\n0.1 / 1.1\n0.1 / 0.6\n0.2 / 0.6\nfr_so\n9.7 / 43.8\n7.5 / 39.5\n6 / 39.9\n7.7 / 40.1\n0.1 / 5\n0.5 / 15.9\n0.3 / 15\n0.1 / 11.1\nfr_ss\n7.9 / 45.5\n2.4 / 28.5\n1 / 26.5\n3.4 / 31.9\n0.1 / 6.9\n0.1 / 10.3\n0.1 / 10.2\n0.1 / 8.9\nfr_ti\n4.6 / 25.3\n1 / 12.9\n0.5 / 11.7\n1.8 / 16.9\n0 / 0.3\n0 / 1.1\n0 / 0.8\n0 / 0.5\nfr_yo\n4.9 / 26.2\n2.1 / 18.7\n1.3 / 17.6\n1.9 / 18.8\n0.1 / 4.6\n0.1 / 6.6\n0 / 3.8\n0 / 10.2\nfr_zh\n25.5 / 24.4\n7.4 / 33\n7.2 / 34.1\n6.8 / 33.6\n4 / 5.5\n16.2 / 18.1\n22.8 / 23.6\n19.3 / 21.8\nln_ckb\n6.6 / 40\n1.1 / 22.1\n0.3 / 18\n1.4 / 22.8\n0 / 0.2\n0 / 3\n0 / 3.6\n0 / 3.8\nln_cs\n15.6 / 40.7\n3.6 / 22.1\n1.4 / 18.1\n4.7 / 23.6\n0.2 / 4.7\n2.6 / 19.2\n2.5 / 21.2\n1.9 / 20.4\nln_cy\n20.2 / 45.8\n4.9 / 23.2\n2.6 / 21\n6.4 / 26.6\n0.1 / 5.2\n0.1 / 5.8\n0.1 / 7.8\n0.1 / 6.7\nln_en\n28.9 / 52.1\n5.5 / 27.2\n1.7 / 23.4\n6.8 / 29.5\n0.3 / 11.1\n6.5 / 27.2\n6.6 / 25.5\n6.7 / 29.9\nln_et\n12.8 / 42.6\n3.2 / 24.8\n1.3 / 20\n3.7 / 26.3\n0.2 / 5.2\n0.3 / 12.9\n2.3 / 23.7\n1.9 / 22.9\nln_eu\n9.3 / 42.1\n2.1 / 25.3\n1.3 / 22\n3.2 / 28.3\n0.1 / 5.9\n0.7 / 17.7\n1.4 / 25.3\n0.6 / 22.3\nln_fon\n2.6 / 22.6\n1.1 / 13.1\n0.3 / 8.4\n0.8 / 13.3\n0 / 0.5\n0.1 / 5.4\n0 / 3\n0 / 1.8\nln_fr\n23.8 / 48.9\n6.5 / 25.5\n2.8 / 24\n7.6 / 28\n0.4 / 9.9\n4 / 23.1\n6.5 / 30.9\n4.1 / 28\nln_mr\n8.1 / 37.1\n1.2 / 16.8\n0.3 / 12.8\n1.5 / 18.7\n0.1 / 0.7\n0.6 / 16.4\n0.1 / 10.1\n0.2 / 13.6\nln_ny\n9.4 / 42.1\n1.6 / 20.5\n0.6 / 17.3\n2.6 / 25.9\n0.2 / 6.4\n0.1 / 8.2\n0.1 / 9.6\n0.1 / 8.9\nln_or\n6.8 / 36.6\n1.4 / 18.9\n0.5 / 15.9\n1.8 / 21.7\n0 / 0.3\n0 / 0.3\n0 / 0.4\n0 / 0.4\nln_so\n8.6 / 40\n2.1 / 24\n0.7 / 19.4\n2.6 / 26.6\n0.1 / 5.9\n0.4 / 12.9\n0.1 / 9.8\n0.1 / 8.9\nln_ss\n7.1 / 43\n1.2 / 18.5\n0.4 / 13.6\n2 / 22.5\n0.1 / 5.2\n0.1 / 9.5\n0.1 / 8.3\n0 / 7.2\nln_ti\n3.4 / 20.6\n0.3 / 4.5\n0.1 / 5.7\n0.6 / 10\n0 / 0.3\n0 / 0.2\n0 / 0.2\n0 / 0.2\nln_yo\n4.3 / 24.5\n0.8 / 12.9\n0.4 / 11\n1.1 / 14.9\n0.1 / 4\n0 / 1.4\n0.1 / 7\n0.1 / 7.4\n43\nln_zh\n20 / 19.9\n1.7 / 8.9\n0.3 / 4.3\n2.2 / 10.7\n0.3 / 1.2\n1.5 / 5.1\n4.2 / 8.6\n2.9 / 7.9\nmr_ckb\n8.1 / 45.2\n4.4 / 35.5\n1.1 / 28.5\n4.9 / 34.8\n0.1 / 0.3\n0 / 8.8\n0 / 8.7\n0 / 8\nmr_cs\n21.8 / 49.3\n9.5 / 31.4\n3.4 / 29.2\n14.4 / 37.7\n0.2 / 0.8\n11.7 / 36.7\n12.5 / 38.7\n10.8 / 38.9\nmr_cy\n28.1 / 56.1\n22.3 / 47\n25.6 / 51.2\n26 / 51.1\n0.1 / 1\n2.1 / 21.1\n1.3 / 23.6\n0.9 / 21.2\nmr_en\n40.3 / 65.8\n35 / 61.2\n35.4 / 61.3\n36.9 / 62.4\n1.5 / 7.6\n24.8 / 53.7\n27.8 / 56.4\n27.9 / 56.3\nmr_et\n18 / 51.3\n7.5 / 34\n7.6 / 41\n15.6 / 45.7\n0.2 / 1.7\n10 / 41.6\n8.7 / 43\n9.7 / 44.2\nmr_eu\n11.3 / 48.3\n6.3 / 35.8\n3.7 / 35.6\n9.6 / 42\n0 / 0.8\n4.4 / 36.5\n3.4 / 39\n2.3 / 37.6\nmr_fon\n2.3 / 22.2\n0.9 / 13.2\n0.3 / 8.9\n0.8 / 12.6\n0 / 0.2\n0 / 3.6\n0 / 5.1\n0 / 1.2\nmr_fr\n32.2 / 58.7\n18.5 / 39.8\n10.7 / 39.2\n24.7 / 47.7\n0.5 / 1.4\n17.2 / 44.8\n14.3 / 45\n18.2 / 47.3\nmr_ln\n15.3 / 47.8\n1 / 14\n0.3 / 13.5\n1.7 / 19\n0 / 0.3\n0 / 6.5\n0.1 / 8.2\n0.1 / 6.9\nmr_ny\n9.9 / 44.7\n2.9 / 28.3\n2.9 / 34.6\n7.1 / 38\n0.1 / 1.3\n0.2 / 11.6\n0.1 / 10.1\n0 / 7.8\nmr_or\n11 / 45.6\n9.2 / 42.3\n9.7 / 43.2\n9.5 / 43.4\n0.1 / 0.3\n0.2 / 0.7\n0.2 / 1.2\n0 / 1\nmr_so\n8.8 / 42.8\n6.2 / 36.7\n3.5 / 36.3\n7 / 39.1\n0.1 / 1\n0.2 / 10\n0.1 / 7.9\n0 / 7.3\nmr_ss\n7.1 / 44.9\n1.8 / 24.7\n0.6 / 20.8\n3.2 / 30.8\n0.1 / 0.6\n0.1 / 11.7\n0 / 6.6\n0 / 5.8\nmr_ti\n4.5 / 25.2\n0.7 / 10.6\n0.4 / 11.1\n1.7 / 16.2\n0 / 0.2\n0 / 1.4\n0 / 0.8\n0 / 0.6\nmr_yo\n5.1 / 26.5\n1.8 / 17.5\n0.9 / 16.4\n1.7 / 18.1\n0.1 / 0.4\n0.1 / 7.1\n0 / 3.1\n0 / 3.9\nmr_zh\n25.4 / 24\n4.8 / 19.3\n1 / 13.6\n5.6 / 23.3\n0.1 / 0.4\n17.7 / 17\n19.9 / 19.4\n14.8 / 18.9\nny_ckb\n6.6 / 39\n4.5 / 33.5\n1.4 / 29.5\n5.2 / 35.5\n0 / 0.2\n0 / 0.5\n0 / 1.6\n0 / 2.9\nny_cs\n15.2 / 40.3\n14.2 / 37.4\n15.2 / 39.3\n15.3 / 39.2\n0.2 / 4.2\n0.6 / 15.9\n3.5 / 23.3\n3.1 / 23.2\nny_cy\n20.7 / 46\n17.4 / 39.1\n20.3 / 42.2\n20 / 42.7\n0.1 / 4.2\n0.2 / 8.6\n0.1 / 7.8\n0.1 / 6.3\nny_en\n29.2 / 52.9\n24.2 / 47.2\n24 / 49.7\n27.2 / 50.5\n0.6 / 12.2\n8.4 / 27.7\n12.4 / 35.5\n13.9 / 37.1\nny_et\n12.7 / 42.6\n11.8 / 38.6\n10.9 / 40.2\n13.5 / 41.9\n0.1 / 4.7\n1.5 / 20.7\n1.6 / 24.5\n2 / 26.7\nny_eu\n8.9 / 41.1\n8.6 / 39\n5.9 / 36.9\n9.8 / 42.7\n0 / 4.5\n0.6 / 20.4\n0.8 / 25.4\n0.6 / 23\nny_fon\n2.5 / 21.9\n1.3 / 14.6\n0.6 / 12.1\n1.4 / 15.6\n0 / 0.3\n0 / 4.4\n0 / 1.4\n0 / 3.7\nny_fr\n22.5 / 47.4\n19.8 / 42.1\n22.4 / 45.2\n23.8 / 46.3\n0.2 / 5.5\n8.6 / 31.6\n5.6 / 32.2\n4.5 / 32.2\nny_ln\n13.6 / 44.7\n1 / 13.7\n0.5 / 13\n1.8 / 19.2\n0.1 / 5.2\n0.5 / 13.4\n0.1 / 10.8\n0.1 / 9.4\nny_mr\n7.4 / 36.1\n4.3 / 27.3\n1.4 / 21.6\n5.3 / 29.9\n0 / 0.5\n0.3 / 13.6\n0.5 / 19.6\n0.3 / 15.8\nny_or\n6.2 / 35.9\n5.3 / 33.9\n6.6 / 35.4\n6.7 / 36.4\n0 / 0.3\n0.1 / 0.2\n0 / 0.3\n0.1 / 0.5\nny_so\n8.2 / 39.6\n6.2 / 35.3\n4.5 / 35\n6.9 / 37.1\n0.1 / 5.5\n0.4 / 12.8\n0.2 / 12.3\n0.1 / 9.5\nny_ss\n7.3 / 42.5\n2.5 / 27.2\n0.9 / 22.1\n4.2 / 32.2\n0.1 / 4\n0.1 / 10.5\n0.2 / 13.5\n0.1 / 9.5\nny_ti\n3.3 / 19.4\n0.8 / 10.8\n0.3 / 9.3\n1.7 / 15.4\n0 / 0.3\n0 / 0.3\n0 / 0.2\n0 / 0.1\nny_yo\n4 / 23.3\n2 / 17.7\n1.4 / 16.6\n2.3 / 19\n0.1 / 3.5\n0.1 / 4.1\n0 / 3.8\n0 / 4.4\nny_zh\n18.8 / 18.9\n5 / 20.7\n1.3 / 14.9\n4.7 / 21.9\n0.3 / 1.1\n3.9 / 6.7\n6.2 / 11.3\n6 / 11.5\nor_ckb\n8.1 / 45.1\n6 / 38.9\n5.7 / 40.4\n7.5 / 42\n0 / 0.2\n0 / 0.2\n0 / 0.3\n0 / 0.3\nor_cs\n21.3 / 49.1\n20.3 / 47.5\n21.6 / 47.9\n21.7 / 48.9\n0 / 0.7\n8.1 / 32.7\n9.5 / 36.9\n10.6 / 36.7\nor_cy\n28.6 / 56.4\n22.4 / 48\n27.4 / 52.1\n26.3 / 51.6\n0.1 / 0.9\n0.5 / 8.6\n0.4 / 5.6\n0.8 / 10.5\nor_en\n41.6 / 66.2\n35 / 61\n36.3 / 61.7\n36.1 / 61.7\n1 / 8.6\n18.9 / 47.3\n20.6 / 49.1\n22.5 / 50.7\nor_et\n17.1 / 50.7\n17.6 / 49.6\n18.2 / 50.4\n18.7 / 51.1\n0.1 / 0.9\n6.8 / 35\n9.1 / 41.1\n7.9 / 38.3\nor_eu\n11.9 / 48.8\n13.3 / 49.9\n14.1 / 51.1\n13.9 / 51.4\n0 / 0.5\n2.1 / 24.3\n1.9 / 28.8\n2.2 / 36.6\nor_fon\n2.3 / 21.4\n1 / 13.5\n0.5 / 10.7\n1 / 13.9\n0 / 0.2\n0 / 0.5\n0 / 2.3\n0 / 1.5\nor_fr\n32.3 / 58.7\n29.1 / 54.1\n31.3 / 55.7\n32.6 / 57\n0.1 / 1\n12.8 / 39.4\n13.8 / 42.6\n16.5 / 44.6\nor_ln\n15.9 / 47.8\n1.4 / 14.8\n0.7 / 14.7\n1.6 / 16.3\n0 / 0.4\n0.3 / 5.1\n0.1 / 6.4\n0.1 / 7.3\nor_mr\n12.8 / 47\n9.1 / 42.1\n9.9 / 42.8\n10.3 / 43.2\n0.2 / 4.2\n4.5 / 27.6\n2.4 / 34.6\n6.4 / 40.1\nor_ny\n10.8 / 45.7\n5.3 / 34.9\n5.4 / 38.7\n8 / 39.6\n0.1 / 0.7\n0.1 / 10\n0.1 / 5.8\n0.1 / 7.7\nor_so\n8.8 / 42.5\n6.3 / 36.8\n7.2 / 38.5\n7.5 / 40\n0.1 / 0.5\n0.3 / 2.5\n0 / 6.7\n0.1 / 6.9\nor_ss\n7.1 / 44.6\n2.3 / 26.6\n1.5 / 26.3\n3.4 / 32.2\n0 / 0.4\n0 / 3.7\n0 / 4.7\n0.1 / 7.6\nor_ti\n4.4 / 25.3\n0.8 / 10.5\n0.5 / 12.6\n1.8 / 16.2\n0 / 0.2\n0 / 0.3\n0 / 0.3\n0 / 0.2\nor_yo\n5.6 / 27.3\n1.6 / 17.2\n1.4 / 17.4\n1.9 / 18.7\n0 / 0.4\n0 / 3.3\n0 / 2.9\n0 / 2.8\nor_zh\n26 / 24.3\n5.2 / 26.7\n4.2 / 26.8\n6.6 / 29.5\n0.4 / 0.6\n8.6 / 8.4\n18.2 / 16.7\n18 / 17.9\nso_ckb\n6.3 / 40.7\n3.5 / 30\n2 / 31.4\n4.5 / 33.7\n0 / 0.5\n0.1 / 5.9\n0 / 1.2\n0 / 2.6\nso_cs\n15.6 / 41\n14.6 / 40.1\n16.4 / 41.9\n16.4 / 42.2\n0.1 / 3.4\n5.6 / 24.5\n6.5 / 29.2\n5.8 / 31.1\nso_cy\n21.2 / 47\n20.1 / 43\n23.1 / 46\n22.6 / 46.1\n0.2 / 4.4\n0.7 / 11.5\n0.2 / 10.6\n0.2 / 9.6\nso_en\n30.2 / 54.4\n27.8 / 51.7\n30.7 / 54.5\n31.2 / 55\n1 / 11.4\n17.5 / 40.5\n21.1 / 45.2\n19.5 / 45.7\nso_et\n13.1 / 43.3\n14 / 43\n13.9 / 43.4\n15 / 44.5\n0.3 / 7.2\n2.5 / 22\n3.7 / 30.8\n2.9 / 30.4\nso_eu\n9.1 / 42.4\n9.8 / 42.2\n9.4 / 42.1\n11.4 / 45.4\n0.1 / 4.8\n0.7 / 18.3\n0.5 / 21.5\n0.1 / 13.3\nso_fon\n2.1 / 20\n1 / 13.6\n0.5 / 10.6\n0.9 / 13.8\n0 / 0.6\n0.1 / 7.4\n0 / 5.7\n0 / 1.8\nso_fr\n24.5 / 49.8\n24 / 47.8\n26.4 / 49.7\n26.1 / 49.7\n0.4 / 7.1\n13.8 / 38.9\n11.3 / 39.6\n11.3 / 40.2\nso_ln\n15.3 / 45.9\n1.4 / 15.7\n0.6 / 13.5\n1.8 / 18.9\n0.1 / 4.8\n0.2 / 11.8\n0.1 / 9.4\n0.1 / 8.7\nso_mr\n8.7 / 38.8\n4.8 / 28.9\n1.3 / 21.5\n5.1 / 28.5\n0.2 / 6.1\n1.8 / 23.7\n1.1 / 24\n0.7 / 22.6\nso_ny\n10.5 / 43.4\n4.7 / 31.9\n2.5 / 31.9\n7 / 37.2\n0.2 / 5\n0.5 / 17.3\n0.2 / 12.8\n0.1 / 10.6\nso_or\n7 / 37.3\n6.2 / 35\n7.1 / 36.9\n7.5 / 37.8\n0.1 / 0.3\n0.1 / 0.2\n0.1 / 0.4\n0.1 / 0.4\nso_ss\n6.6 / 42.4\n2 / 25.5\n0.8 / 21.4\n3.5 / 31.2\n0 / 3.6\n0 / 12.9\n0.2 / 12.3\n0 / 6.5\nso_ti\n3.6 / 22.5\n1 / 11.3\n0.3 / 9.6\n1.4 / 15.2\n0 / 0.3\n0 / 0.4\n0 / 0.3\n0 / 0.2\nso_yo\n6.1 / 27\n1.5 / 16.2\n1 / 15.7\n1.8 / 17.9\n0.1 / 3.2\n0.1 / 11.9\n0.1 / 9.2\n0 / 6\nso_zh\n20.3 / 20.5\n3.6 / 21.4\n4.3 / 22.9\n3.5 / 23\n0.6 / 1.5\n8.5 / 12.4\n10.1 / 14.6\n10.2 / 14.4\nss_ckb\n6.2 / 39.2\n4 / 32.4\n1.8 / 30.2\n4.5 / 33.3\n0 / 0.2\n0 / 3\n0 / 1.7\n0 / 1.9\nss_cs\n14.9 / 39.1\n11.1 / 33.3\n13 / 36\n12.8 / 36.2\n0.1 / 3.6\n0.7 / 13.5\n1.7 / 20.4\n1.3 / 20\nss_cy\n20.1 / 45.4\n15.8 / 36.8\n16.7 / 39.1\n18.8 / 40.7\n0.1 / 4.5\n0.4 / 11\n0.1 / 8.6\n0.1 / 6.9\nss_en\n29.2 / 52.3\n21.6 / 43.8\n22.9 / 47\n25 / 47.7\n0.4 / 11\n7 / 25.1\n8.4 / 29.3\n9.2 / 32.5\nss_et\n12.1 / 41.6\n9.6 / 35.5\n8.8 / 37.2\n11.3 / 39.1\n0.1 / 4\n0.2 / 14.9\n1.2 / 20.7\n1 / 20.3\nss_eu\n8.2 / 39.7\n7.7 / 36.8\n7.1 / 37.4\n8.9 / 40.5\n0 / 4.9\n0.1 / 10.7\n0.4 / 20.9\n0.3 / 18.7\nss_fon\n2.7 / 22\n1.4 / 14.8\n0.6 / 11.5\n1.4 / 14.9\n0 / 0.3\n0 / 3.4\n0 / 3.9\n0 / 1.5\nss_fr\n20.7 / 45.4\n18.3 / 40.1\n21.3 / 43.1\n20.9 / 43.3\n0.2 / 5.9\n4.4 / 25.5\n4.1 / 27.3\n2.5 / 26.8\nss_ln\n12.1 / 42.8\n1 / 14.4\n0.5 / 11.6\n1.5 / 18\n0.1 / 6.4\n0.1 / 10.3\n0.1 / 9.9\n0.1 / 8.7\nss_mr\n6.7 / 34.7\n3.4 / 24.7\n1.3 / 20.4\n4.4 / 27.4\n0.1 / 0.9\n0.4 / 13.5\n0.3 / 16.6\n0.2 / 13.3\nss_ny\n7.6 / 38.4\n5.1 / 31.9\n3.2 / 31.8\n7.7 / 36.9\n0.2 / 6.8\n0.1 / 9.2\n0.1 / 9.6\n0.1 / 9.6\nss_or\n6.1 / 34.7\n4.9 / 31.1\n4.9 / 32\n6.2 / 34\n0.1 / 0.3\n0 / 0.2\n0 / 0.4\n0 / 0.3\nss_so\n7.7 / 38.2\n6 / 33.6\n4.2 / 33\n6.9 / 36.2\n0.1 / 6\n0.3 / 11.1\n0.1 / 9.9\n0.1 / 8.8\nss_ti\n2.6 / 18.3\n0.8 / 11\n0.4 / 10\n1.5 / 15\n0 / 0.3\n0 / 0.3\n0 / 0.2\n0 / 0.1\nss_yo\n3.6 / 21.7\n1.8 / 16.7\n1.2 / 15.9\n2.2 / 18.3\n0.1 / 4.2\n0 / 3.1\n0.1 / 7.6\n0 / 3.4\nss_zh\n19.9 / 20\n4.1 / 17.8\n4 / 19.3\n4.2 / 19.5\n0.1 / 0.7\n2.5 / 5.3\n3.4 / 8.3\n3 / 7.9\n44\nti_ckb\n5.7 / 39.5\n3.4 / 33\n1.3 / 26.9\n4.4 / 36.3\n0 / 0.2\n0 / 1.9\n0 / 0.3\n0 / 0.1\nti_cs\n14.5 / 40.9\n9.1 / 32.8\n11.1 / 34.6\n11.8 / 37.2\n0.1 / 1.7\n0.8 / 11.5\n2.4 / 23.2\n2.9 / 25.6\nti_cy\n18.5 / 45.8\n12.5 / 34.9\n15.5 / 37.2\n16.1 / 40.1\n0 / 1.8\n0.3 / 14.4\n0.1 / 5.6\n0.1 / 7.8\nti_en\n26.4 / 52.4\n15.2 / 41.5\n17.2 / 45\n20.6 / 47.1\n0.6 / 10.2\n8 / 29.9\n9.4 / 34.6\n9.3 / 35\nti_et\n12 / 42.8\n7.5 / 34.9\n7.4 / 34.9\n9.3 / 37.6\n0.1 / 3.5\n0.3 / 10.5\n1.6 / 25.8\n2.2 / 26.7\nti_eu\n8.1 / 41.7\n5.8 / 36.1\n4.5 / 34.4\n7.6 / 40.4\n0 / 2.3\n0.8 / 24.9\n1.3 / 25.2\n0.5 / 22.9\nti_fon\n1.9 / 20.8\n0.8 / 14.2\n0.3 / 8.8\n0.9 / 14.6\n0 / 0.2\n0 / 0.3\n0 / 1.4\n0 / 3\nti_fr\n22.1 / 49\n15.6 / 39.6\n19.5 / 42.6\n20.2 / 44.4\n0.2 / 3.6\n4.7 / 26.2\n6.1 / 31.5\n3.9 / 31\nti_ln\n13 / 44.3\n0.5 / 13.5\n0.2 / 8.8\n0.2 / 0.9\n0 / 0.4\n0.3 / 10.1\n0 / 3.9\n0.1 / 6.5\nti_mr\n8.2 / 38.6\n3.4 / 26.2\n1.9 / 23.8\n4.6 / 30.3\n0 / 1.2\n0.7 / 18.8\n0.6 / 21.1\n0.3 / 18.4\nti_ny\n8.4 / 41.2\n3.4 / 30.7\n1 / 22.7\n5.3 / 34.7\n0 / 0.4\n0 / 0.6\n0 / 2.5\n0 / 1.1\nti_or\n6.7 / 38\n4.5 / 31.2\n5.1 / 32\n5.5 / 34.7\n0 / 0.2\n0 / 0.2\n0 / 0.7\n0 / 0.3\nti_so\n7.2 / 39.3\n4.1 / 32.6\n2.3 / 28\n5.1 / 35.6\n0.1 / 0.4\n0.1 / 2.5\n0 / 3.5\n0 / 3.8\nti_ss\n4.8 / 40.5\n1.8 / 25.8\n0.5 / 17.1\n2.5 / 29.7\n0.1 / 0.4\n0 / 0.9\n0 / 0.8\n0 / 1.7\nti_yo\n4.3 / 24.4\n1.4 / 16.7\n0.6 / 12.9\n1.6 / 17.7\n0 / 0.6\n0.2 / 7.3\n0 / 1.1\n0 / 4.9\nti_zh\n22.2 / 20.9\n2.6 / 16.9\n3 / 18.5\n4.5 / 21.4\n0.3 / 0.7\n1.5 / 3.5\n3.9 / 5.7\n5.2 / 9.4\nyo_ckb\n5.5 / 36.6\n2.4 / 27.5\n1.4 / 27.3\n3.5 / 31.1\n0 / 0.2\n0 / 2\n0 / 3.2\n0 / 2.8\nyo_cs\n12.7 / 36.6\n7.3 / 27.1\n9.9 / 31.4\n9.7 / 31.7\n0.1 / 3.3\n0.5 / 18.1\n1.6 / 15.1\n0.6 / 16.7\nyo_cy\n16.9 / 42.4\n10.3 / 30.1\n12.7 / 34.7\n13.9 / 35.5\n0.1 / 3.7\n0.4 / 13.5\n0.1 / 9.8\n0.1 / 6.9\nyo_en\n23.9 / 47.9\n11.8 / 34.9\n12.7 / 40.3\n17.8 / 40.7\n0.4 / 12.5\n5.2 / 23.5\n4.2 / 24.3\n6.5 / 26.1\nyo_et\n10.8 / 38.8\n5.3 / 29\n7.4 / 33.7\n8.5 / 34.3\n0.1 / 4.5\n1.7 / 18.9\n1.7 / 20.1\n1.3 / 20.8\nyo_eu\n7.2 / 38.8\n3.2 / 29.6\n4.9 / 33.9\n6.7 / 37\n0 / 3.9\n0.3 / 19.4\n0.4 / 18.2\n0.3 / 17.2\nyo_fon\n1.8 / 19.7\n1 / 13.7\n0.5 / 11.2\n1.1 / 14.9\n0 / 0.4\n0.1 / 5.2\n0 / 4.1\n0 / 3.1\nyo_fr\n19.1 / 44.1\n11.5 / 31.5\n16.2 / 38\n15.9 / 38.4\n0.3 / 7\n3.2 / 21.8\n1.5 / 20.2\n1.9 / 22.6\nyo_ln\n12.4 / 42.8\n0.5 / 11.4\n0.4 / 11.1\n1 / 16.2\n0.1 / 4.3\n0.3 / 9.1\n0.1 / 8\n0.1 / 7.7\nyo_mr\n6.9 / 34.9\n1.7 / 20.9\n1 / 21.1\n3.8 / 27.2\n0.1 / 2.5\n0.3 / 13.7\n0.2 / 12.9\n0.1 / 10.3\nyo_ny\n8.9 / 40.2\n2.4 / 25.5\n2 / 28.4\n6 / 33.9\n0.1 / 3.9\n0.5 / 9.4\n0.1 / 6.2\n0 / 5.3\nyo_or\n5.2 / 33\n2.9 / 26.2\n2.9 / 28.2\n4 / 30.5\n0 / 0.3\n0 / 0.3\n0 / 0.2\n0 / 0.2\nxx2en\n35.5 / 59.6\n29.7 / 54.4\n30.9 / 55.4\n31.9 / 56.4\n2.0 / 13.3\n20.5 / 44.1\n22.3 / 46.9\n22.4 / 47.6\nen2xx\n20.7 / 50.1\n17.3 / 44.1\n17.8 / 44.7\n18.6 / 45.7\n0.4 / 5.7\n8.1 / 26.7\n8.7 / 29.0\n8.7 / 28.8\nMean\n28.2 / 54.9\n23.5 / 49.2\n24.4 / 50.0\n25.3 / 51.1\n1.2 / 9.6\n14.3 / 35.5\n15.6 / 38.0\n15.6 / 38.2\nxx2yy\n13.7 / 40.5\n8.8 / 31.2\n8.4 / 30.9\n10.1 / 34.0\n0.3 / 4.1\n4.0 / 16.1\n4.4 / 17.3\n4.2 / 17.1\nTable 19: Evaluation scores on the recently introduced NTREX test set (depicted as <bleu> /\n<chrf>) for the MT models and language models described in Section 4.1 and Section 4.2 compared\nagainst unsupervised baselines [10]. Note that LM-8B is evaluated on a 50% split of the NTREX\ndata and is not comparable to the MT-model evaluations.\nBaziotis et al. [10]\nMT-3B\nMT-7.2B\nMT-10.7B\nLM-8B\n0-shot\n1-shot\n5-shot\n10-shot\naf_en\n- / -\n57.1 / 75.4\n58.5 / 76.5\n58.4 / 76.4\n7.6 / 30.6\n49 / 69.9\n49.2 / 69.6\n51.8 / 71.9\nam_en\n- / -\n24.3 / 50.3\n27.2 / 52.5\n27.2 / 52.7\n1 / 10.9\n13.4 / 37.9\n14.6 / 40.4\n13.8 / 40.3\nar_en\n- / -\n36 / 61.9\n37.9 / 63.1\n37.8 / 63.1\n1.1 / 10\n23.7 / 51.6\n28.6 / 53.9\n28.8 / 54.1\naz_en\n- / -\n26.3 / 52.7\n29.1 / 54.9\n30.1 / 55.8\n4.7 / 26.3\n15.5 / 40.8\n20.6 / 47\n21.8 / 48.6\nba_en\n- / -\n8 / 28.2\n9 / 28.4\n9.7 / 28.8\n0.5 / 7.4\n11.1 / 37\n9.5 / 34.4\n13.2 / 38.7\nbe_en\n- / -\n31.5 / 57.6\n32.8 / 58.5\n35.3 / 59.7\n4 / 19.4\n18.4 / 40.7\n29.1 / 56\n30.7 / 56.3\nbem_en\n- / -\n10 / 32.6\n12.1 / 33.9\n13.3 / 34.7\n0.1 / 5.3\n4.8 / 20.7\n2.7 / 21.7\n2.6 / 21.5\nbg_en\n- / -\n38.2 / 63.3\n39.3 / 64\n39.4 / 64.1\n4.3 / 25.4\n31.6 / 58.3\n31.6 / 58.7\n32.6 / 58.3\nbn_en\n- / -\n35.4 / 60.7\n37.5 / 61.9\n38 / 62.3\n0.8 / 6\n23.5 / 51.4\n22.9 / 50.1\n24.4 / 51.9\nbo_en\n- / -\n12.1 / 37.8\n13.3 / 38.8\n14.5 / 40\n0.1 / 2.7\n5.9 / 28.4\n2.9 / 25.8\n4.3 / 27.8\nbs_en\n- / -\n40.7 / 65.1\n42.6 / 66.2\n41.9 / 66\n4.6 / 24.2\n27.2 / 52.2\n33.6 / 59.4\n35.3 / 60.4\nca_en\n- / -\n41.6 / 66\n43.4 / 66.9\n43.1 / 66.8\n5.4 / 22.9\n31.3 / 56.6\n34.5 / 61\n36.1 / 61.4\nckb_en\n- / -\n28.3 / 53.4\n33.9 / 58\n33.5 / 57.9\n1.1 / 11.4\n7.8 / 27.4\n18.8 / 43.6\n20.2 / 45.3\ncs_en\n- / -\n39.4 / 64.8\n41.1 / 65.7\n41.4 / 65.8\n5.8 / 29.1\n26 / 51.5\n33.5 / 59.3\n34.1 / 59.7\ncy_en\n- / -\n43.2 / 66.6\n45 / 68.1\n45.2 / 68.2\n3.9 / 24.2\n35.2 / 58.8\n37.5 / 61.1\n37.2 / 60.7\nda_en\n- / -\n42.5 / 65\n43.7 / 65.8\n43.7 / 65.8\n6.3 / 28.6\n36.1 / 60.2\n37.6 / 61.2\n37.5 / 61.2\nde_en\n- / -\n38.9 / 64.6\n39.8 / 65.4\n40.2 / 65.5\n4.3 / 26.4\n31.6 / 59.4\n33 / 59.1\n34 / 60.4\ndv_en\n- / -\n22.2 / 48.8\n24.6 / 50.5\n25 / 50.8\n0.6 / 6.8\n7.9 / 29.6\n11.1 / 36.9\n12.6 / 39.5\ndz_en\n- / -\n6.4 / 28.2\n6.4 / 27.9\n6.7 / 28.7\n0 / 1.6\n0.9 / 13.9\n1.1 / 17.5\n0.9 / 15\nee_en\n- / -\n13.2 / 35.1\n16.3 / 38.4\n16.7 / 38.5\n0.3 / 7.9\n3.2 / 22.1\n1 / 15.2\n2.8 / 19.6\nel_en\n- / -\n43 / 65.8\n43.3 / 66.1\n44.6 / 66.7\n5.2 / 23.8\n33.9 / 58.7\n31.7 / 58\n32.3 / 59\nen_GB_en\n- / -\n89.3 / 94.4\n90.3 / 94.6\n90.5 / 94.9\n7.9 / 26\n97.5 / 98.7\n99.4 / 99.8\n99.4 / 99.7\nen_IN_en\n- / -\n87.1 / 93.6\n86.4 / 93\n86.9 / 93.8\n10.1 / 27\n35.1 / 45.3\n93.7 / 98.3\n95.6 / 98.7\nen_af\n- / -\n42.3 / 66.7\n44.5 / 68.4\n44.6 / 68.3\n2.9 / 16.5\n35 / 61.4\n37.7 / 63.3\n37.8 / 63.4\nen_am\n- / -\n3.8 / 23\n4.5 / 23.8\n4.2 / 24.3\n0.1 / 0.7\n0.2 / 0.8\n0 / 0.6\n0 / 0.4\nen_ar\n- / -\n24.7 / 54.2\n24.6 / 54.3\n24.9 / 54.7\n0.4 / 2.9\n6.7 / 38.1\n15.4 / 45.3\n12.1 / 44\nen_az\n- / -\n14.6 / 46.7\n15.2 / 47.5\n15.5 / 48.2\n0.4 / 8.6\n10.4 / 41.6\n10.6 / 42.9\n7.4 / 40.9\nen_ba\n- / -\n3.3 / 24.3\n4 / 26.3\n5.2 / 28.9\n0 / 0.4\n0.3 / 13.4\n0.1 / 9.8\n0.1 / 10.5\nen_be\n- / -\n21.9 / 44.8\n24.2 / 47.3\n26 / 50.2\n0.7 / 6\n20.6 / 49.4\n21.6 / 50.3\n21.9 / 50.6\nen_bem\n- / -\n2.4 / 4\n3.9 / 19.5\n2.1 / 3.4\n0.1 / 6.7\n0.1 / 8.7\n0.2 / 10.6\n0.1 / 5\nen_bg\n- / -\n34.1 / 59.5\n34.8 / 59.9\n34.6 / 59.7\n1.4 / 8.8\n21.1 / 48.7\n27.3 / 53.4\n29.1 / 55.1\nen_bn\n- / -\n13.8 / 48.5\n14 / 49.2\n14.6 / 49.8\n0.4 / 7.4\n8.6 / 40.8\n7.4 / 41.5\n5.8 / 42\nen_bo\n- / -\n12 / 25.2\n14.7 / 29.1\n14 / 28.6\n0 / 0.9\n0 / 1.8\n0 / 1.6\n0 / 1.1\nen_bs\n- / -\n29.4 / 58.6\n30.3 / 59.4\n30.2 / 59.3\n0.7 / 10.5\n23.6 / 53\n24.9 / 54.2\n25.3 / 54.3\n45\nen_ca\n- / -\n39.9 / 63\n40.9 / 63.7\n40.5 / 63.4\n2.3 / 15.1\n30.1 / 55.8\n31.6 / 57.7\n32.1 / 58.1\nen_ckb\n- / -\n8.5 / 42.4\n9.2 / 42.5\n9.2 / 42.2\n0 / 0.2\n0.1 / 10.7\n0.2 / 10.9\n0.1 / 9.3\nen_cs\n- / -\n32.4 / 59.4\n32.9 / 59.6\n32.6 / 59.7\n1 / 10.8\n22.2 / 50.2\n22.7 / 51.2\n20.9 / 51.5\nen_cy\n- / -\n29.6 / 56.4\n33 / 59.7\n33.3 / 59.8\n0.3 / 9.3\n3.4 / 25.2\n2.6 / 24.2\n3 / 29.1\nen_da\n- / -\n40.4 / 63.9\n40.9 / 64.3\n40.7 / 64.1\n2 / 13.5\n34.8 / 59.2\n35.5 / 59.9\n36.6 / 60.6\nen_de\n- / -\n34.5 / 61.8\n35.2 / 62.1\n34.7 / 61.8\n2 / 16.2\n21.3 / 49.9\n23.5 / 53.1\n22.1 / 52.7\nen_dv\n- / -\n3.5 / 40.7\n4.7 / 44.4\n4.7 / 44.7\n0 / 0.2\n0.3 / 0.4\n0.2 / 0.4\n0.1 / 0.4\nen_dz\n- / -\n22 / 35.8\n22 / 35.5\n22.7 / 37.2\n0 / 0.1\n0 / 2.1\n0 / 0.3\n0 / 1.1\nen_ee\n- / -\n6.1 / 29.1\n7.4 / 31.6\n7.2 / 30.5\n0.2 / 6.2\n0.4 / 13.5\n0.2 / 6.9\n0.1 / 4.5\nen_el\n- / -\n37.4 / 61.5\n37.4 / 61.6\n37.6 / 61.6\n0.9 / 3.7\n26.7 / 50.8\n28.2 / 52.4\n28 / 52.6\nen_en_GB\n- / -\n6.5 / 9.1\n34.4 / 48.4\n5.8 / 7.1\n8.3 / 21.8\n99.1 / 99.7\n99.4 / 99.8\n99.5 / 99.8\nen_en_IN\n- / -\n5.7 / 8.7\n21.2 / 28\n4.1 / 5.5\n4.4 / 15.4\n91.2 / 97.4\n92.3 / 98\n92.1 / 97.9\nen_es\n- / -\n42 / 65.4\n42.3 / 65.6\n42.2 / 65.5\n1.4 / 15.4\n28.6 / 53.7\n34.1 / 59\n33.5 / 58.5\nen_es_MX\n- / -\n2.4 / 5.7\n7.6 / 28.1\n1.7 / 4\n0.8 / 9.7\n34.1 / 58.8\n36.9 / 60.7\n36.5 / 60.9\nen_et\n- / -\n26.4 / 59\n26.7 / 59.1\n27.3 / 59.4\n0.5 / 10.2\n11.2 / 42.8\n20.2 / 52.9\n21.8 / 53.7\nen_eu\n- / -\n14.8 / 51.3\n15 / 51.2\n15.4 / 52.3\n0.2 / 9.4\n9.9 / 45.6\n9.8 / 45.7\n7.4 / 45.7\nen_fa\n- / -\n19.1 / 45\n19.5 / 44.9\n19.5 / 44.9\n0.5 / 3.1\n14.3 / 40.7\n14.9 / 42.4\n14.8 / 43\nen_fa_AF\n- / -\n0.3 / 0.9\n0.2 / 0.8\n0.2 / 0.8\n0 / 0.2\n12.4 / 39.6\n13.5 / 41.1\n11.4 / 40.1\nen_ff\n- / -\n4.8 / 27.6\n5.3 / 28.1\n5.1 / 28.8\n0.2 / 5.5\n0.1 / 7.9\n0.1 / 5.5\n0.1 / 4.3\nen_fi\n- / -\n21.5 / 57.1\n21.7 / 57.4\n22 / 57.3\n0.5 / 11\n14.5 / 47.5\n16.4 / 50.5\n17.2 / 50.4\nen_fil\n- / -\n25.3 / 55.4\n25.8 / 55.6\n25.3 / 55.4\n1.6 / 14.6\n26.5 / 54\n30 / 57.6\n28.6 / 55.8\nen_fj\n- / -\n6.4 / 27.5\n7.3 / 28.6\n7.7 / 29\n0.3 / 6.8\n0 / 4.6\n0.2 / 7.3\n0.1 / 5\nen_fo\n- / -\n16.4 / 37.9\n18.6 / 40.8\n16.6 / 38.2\n0.4 / 9\n5.6 / 29.1\n7.5 / 32.5\n7.2 / 32.7\nen_fr\n- / -\n38.3 / 62.9\n39.2 / 63.5\n39.2 / 63.5\n0.8 / 12.7\n28.8 / 54.8\n30.2 / 56.2\n26.7 / 56.6\nen_fr_CA\n- / -\n2.1 / 5.7\n4.2 / 27.8\n2.5 / 7.6\n0.6 / 8.6\n29.6 / 55.8\n29.7 / 56.5\n30 / 57.6\nen_ga\n- / -\n26.8 / 54.9\n27.5 / 55.5\n27.1 / 55.2\n0.4 / 10.2\n1.2 / 16.2\n0.9 / 15.4\n0.8 / 15.9\nen_gl\n- / -\n39.9 / 64.5\n40.6 / 65\n40.5 / 64.8\n1.3 / 13.2\n31.3 / 57.5\n32.6 / 58.6\n30.6 / 58.1\nen_gu\n- / -\n14.9 / 47.5\n15.3 / 48\n15.4 / 48.2\n0.1 / 0.3\n0.4 / 1.2\n0.1 / 1.7\n0.1 / 1.3\nen_ha\n- / -\n21 / 49\n21.8 / 50.1\n22.8 / 51.1\n0.2 / 4.2\n0.4 / 11.4\n0.3 / 9.9\n0.3 / 9\nen_hi\n- / -\n25.5 / 50.2\n25.7 / 50.2\n25.9 / 50.6\n0.9 / 7.8\n16.7 / 42.9\n15.6 / 42.8\n12.6 / 42.8\nen_hmn\n- / -\n16.6 / 42.1\n17.5 / 42.9\n17.7 / 43.3\n0.2 / 6.1\n1.1 / 12.4\n0.6 / 10.3\n0.5 / 9.4\nen_hr\n- / -\n34.9 / 62\n35.5 / 62.7\n35.6 / 62.8\n3.7 / 21.3\n25.7 / 54\n26.7 / 55.3\n25.1 / 55.3\nen_hu\n- / -\n21 / 50.8\n21.4 / 51.2\n21.7 / 51.4\n0.6 / 10.8\n10.6 / 36.9\n11.1 / 42.3\n12.3 / 42.5\nen_hy\n- / -\n15.7 / 46.5\n18.5 / 51\n19.1 / 51.7\n0.1 / 0.5\n0.4 / 0.8\n0.5 / 1\n0.5 / 1.2\nen_id\n- / -\n38.7 / 65.2\n39.6 / 65.8\n39.8 / 65.8\n0.7 / 9\n32.8 / 60.5\n35 / 62.7\n34.8 / 62.7\nen_ig\n- / -\n13.5 / 37.9\n16.4 / 43.2\n16.8 / 42\n0.2 / 5.8\n0.2 / 9.7\n0.1 / 5.3\n0.2 / 5.5\nen_is\n- / -\n25.1 / 52.8\n25.3 / 53\n25.3 / 53.3\n2.6 / 16.8\n22.5 / 48.9\n26.3 / 52.3\n26 / 51.9\nen_it\n- / -\n38.8 / 64\n39.2 / 64.1\n39 / 64.1\n1.2 / 14.7\n28.7 / 55.8\n29.7 / 56.7\n30.2 / 57\nen_iw\n- / -\n24.3 / 53\n22.6 / 48.5\n25.4 / 54.2\n0.5 / 2.6\n14 / 42.3\n15.9 / 44.6\n15.7 / 44.8\nen_ja\n- / -\n6.3 / 33.6\n6.8 / 34.3\n5.9 / 33.9\n0 / 3.2\n0.1 / 26.5\n0.1 / 26.5\n0.2 / 24.8\nen_ka\n- / -\n2.7 / 15.3\n1.6 / 11.9\n3.1 / 15.5\n0.6 / 6.4\n10.2 / 42.1\n11.5 / 44.8\n9.7 / 46\nen_kk\n- / -\n12.3 / 45.5\n13.2 / 47.1\n13.7 / 47.3\n0.3 / 5.9\n8.4 / 41.2\n7.7 / 40.8\n6.7 / 41.2\nen_km\n- / -\n6 / 40.8\n6 / 41.3\n4.8 / 43.6\n0.1 / 1.1\n1.6 / 27.4\n0.7 / 32.3\n0.3 / 29.3\nen_kn\n- / -\n10.8 / 48.4\n12.1 / 50\n11.9 / 49.8\n0.1 / 0.5\n0.2 / 4.1\n0.1 / 4.9\n0.1 / 4.9\nen_ko\n- / -\n10.9 / 31.7\n11.6 / 32.4\n11.9 / 32.7\n0.2 / 2.8\n5.6 / 22.1\n3.5 / 22.8\n2.9 / 22.8\nen_ku\n- / -\n0.2 / 0.5\n0.2 / 0.6\n0.3 / 0.8\n0.1 / 6.4\n0.6 / 15.8\n0.5 / 16.3\n0.3 / 14.3\nen_ky\n- / -\n9 / 42.1\n10.6 / 45\n10.4 / 44.2\n0.1 / 1.1\n1.3 / 22.7\n0.9 / 24.5\n0.8 / 24.9\nen_lb\n- / -\n8.5 / 29.5\n8.9 / 29.8\n9.6 / 31\n0.4 / 10\n3.5 / 32.4\n4.1 / 32.7\n4.6 / 33.9\nen_lo\n- / -\n11.1 / 38\n11.1 / 38.4\n11.7 / 39.1\n0.2 / 1.4\n1.3 / 19.4\n2.7 / 31.6\n1.9 / 32.6\nen_lt\n- / -\n24.3 / 56.1\n26 / 56.7\n25 / 56.3\n0.6 / 11.8\n15.8 / 46.8\n17.8 / 50\n17.4 / 50.3\nen_lv\n- / -\n22.6 / 54.1\n22.9 / 54.1\n23.4 / 54.6\n1 / 10.2\n17.6 / 48.1\n18.8 / 49.4\n19.1 / 50.6\nen_mey\n- / -\n0.1 / 0.6\n0.1 / 0.5\n0.1 / 0.6\n0 / 0.2\n2.9 / 29.9\n0.7 / 21\n0.8 / 23.2\nen_mg\n- / -\n16.8 / 48.5\n17.7 / 50.1\n17.6 / 49.6\n0.4 / 6.7\n1.3 / 19.4\n0.6 / 17.1\n0.3 / 14\nen_mi\n- / -\n20.9 / 46.4\n19.7 / 45.8\n21.5 / 47.1\n0.5 / 7.9\n0.8 / 13.6\n0.4 / 11.2\n0.4 / 10\nen_mk\n- / -\n37 / 63.9\n37.6 / 64.1\n37.6 / 64.1\n1 / 4\n27.2 / 55.3\n28.9 / 57.1\n30.4 / 58.5\nen_ml\n- / -\n7.1 / 44.2\n8 / 44.4\n8.3 / 45.3\n0.3 / 8.8\n5.4 / 37\n4.6 / 39.7\n2.1 / 37.9\nen_mn\n- / -\n10.7 / 41.6\n12 / 44\n12.4 / 45\n0.1 / 0.5\n0.6 / 1.6\n0.4 / 3.5\n0.4 / 4.1\nen_mr\n- / -\n7.5 / 36\n7.3 / 34.2\n8.4 / 37.3\n1.2 / 14.1\n7.8 / 39.6\n7.4 / 40\n6.8 / 40.3\nen_ms\n- / -\n33.6 / 61.8\n35.1 / 62.8\n35.3 / 62.9\n1.8 / 14.2\n29.1 / 56.6\n31.6 / 62\n33 / 62.2\nen_mt\n- / -\n45.7 / 66.6\n47.1 / 67.3\n47.6 / 67.5\n0.4 / 10\n3.7 / 25.6\n3.7 / 34.2\n3.3 / 33.7\nen_my\n- / -\n1.7 / 16.7\n1.5 / 16.2\n1.4 / 16.1\n0 / 0.5\n0 / 5.4\n0 / 5.1\n0 / 6.4\nen_nd\n- / -\n1.3 / 3\n1.8 / 18.2\n1 / 2.2\n0.1 / 2.5\n0.3 / 20.2\n0.2 / 13.3\n0.2 / 13\nen_ne\n- / -\n7.5 / 35.9\n7.9 / 37\n8.1 / 37.8\n0.1 / 0.5\n1.4 / 23.9\n1.7 / 30.3\n1 / 26.5\nen_nl\n- / -\n36.7 / 62.9\n37.5 / 63.4\n37.4 / 63.3\n2 / 15.7\n25 / 51.7\n28.8 / 56.6\n29.8 / 57.8\nen_nn\n- / -\n34.6 / 60.9\n35.8 / 61.6\n35.8 / 61.4\n4.5 / 14.4\n24.9 / 53.2\n29.9 / 56.1\n30.6 / 57\nen_no\n- / -\n38.7 / 63.6\n39 / 63.7\n38.9 / 63.6\n1.9 / 12.7\n33.1 / 59\n34.9 / 60.3\n34.8 / 60.3\nen_nso\n- / -\n7.9 / 31.2\n9.1 / 33.5\n9.2 / 33.7\n0.3 / 4.6\n0.9 / 13.8\n0.5 / 13.5\n0.4 / 12.4\nen_ny\n- / -\n12 / 41.6\n14.8 / 45.5\n14.4 / 44.6\n0.7 / 8\n1.1 / 18.3\n0.7 / 17.8\n0.7 / 18.3\nen_om\n- / -\n1.1 / 23.2\n1.8 / 30.4\n2 / 30.3\n0.1 / 6.5\n0.1 / 9.7\n0.1 / 7.3\n0.1 / 7.6\nen_pa\n- / -\n18.2 / 44.1\n18.9 / 45.2\n19.2 / 45.6\n0.1 / 0.3\n1 / 9\n0.6 / 2.6\n0.2 / 1.4\nen_pl\n- / -\n27.6 / 55.5\n28.1 / 55.8\n28.2 / 55.9\n1.4 / 12.2\n18.1 / 45.3\n15.2 / 46.7\n14.9 / 45.4\nen_ps\n- / -\n10.5 / 33.6\n10.8 / 34.3\n10.8 / 34.4\n0.1 / 0.5\n0.5 / 10.3\n0.3 / 8.6\n0.3 / 7.6\nen_pt\n- / -\n41.7 / 64.9\n42.3 / 65.3\n42.3 / 65.3\n3.3 / 18.3\n30 / 55.8\n33.4 / 58.8\n34 / 59.7\nen_pt_PT\n- / -\n2.7 / 5.9\n8.9 / 34.5\n3 / 18\n2.9 / 13.4\n23.9 / 49.3\n31.7 / 57.4\n32.7 / 57.9\nen_ro\n- / -\n34.5 / 59\n35.3 / 59.4\n35 / 59.2\n4.4 / 19.8\n28.1 / 52.8\n30.2 / 55\n30.8 / 55.4\nen_ru\n- / -\n30.7 / 55.8\n31.9 / 56.7\n31.8 / 56.7\n1 / 2.9\n18 / 41.8\n22.3 / 48.5\n23 / 48.9\nen_rw\n- / -\n3.2 / 18.6\n0.3 / 8.3\n0.9 / 12\n1 / 9.5\n1.1 / 18\n0.8 / 17.5\n0.5 / 16.5\nen_sd\n- / -\n10 / 35.8\n11.4 / 37.1\n11.2 / 37.4\n0.1 / 0.6\n0.3 / 8.7\n0.2 / 8.8\n0.3 / 10.9\nen_shi\n- / -\n0.3 / 0.7\n0.2 / 0.7\n0.2 / 0.7\n0 / 0.3\n0.1 / 4.3\n0 / 3.4\n0 / 3.3\nen_si\n- / -\n11 / 42.6\n10.9 / 42.2\n11.4 / 44.2\n0.1 / 0.5\n0.4 / 1.1\n0.6 / 1\n0.4 / 1.2\nen_sk\n- / -\n33.9 / 60.3\n34.2 / 60.4\n34.3 / 60.7\n2 / 14.3\n26.4 / 53.1\n25.6 / 53.2\n27.5 / 54.1\n46\nen_sl\n- / -\n32.4 / 59.6\n33.2 / 60\n33.4 / 60.1\n1.1 / 12.4\n21.1 / 50.5\n23.9 / 52.5\n24.9 / 53\nen_sm\n- / -\n23.4 / 48.7\n25.4 / 51.2\n27 / 51.9\n0.4 / 7.5\n1 / 16.6\n0.5 / 12.6\n0.4 / 11.8\nen_sn\n- / -\n8.9 / 38.5\n10.6 / 41.4\n11.4 / 43.5\n0.3 / 6\n0.5 / 15.3\n0.5 / 16.8\n0.5 / 17\nen_so\n- / -\n14.2 / 46.9\n14.5 / 48\n14.8 / 48.1\n0.3 / 7.6\n2.1 / 14.7\n0.3 / 12.5\n0.4 / 14.3\nen_sq\n- / -\n31.8 / 57.9\n32.3 / 58.3\n31.9 / 58\n1.9 / 11.8\n30.4 / 56.3\n31.4 / 56.8\n31.9 / 57\nen_sr\n- / -\n21.3 / 45\n21.7 / 45.4\n21.8 / 45.5\n0.2 / 1.3\n4.2 / 21.3\n14.9 / 39.3\n9.3 / 34.3\nen_sr_Latn\n- / -\n0.9 / 3.6\n0.9 / 19\n0.8 / 2.5\n1.1 / 5\n21.4 / 50.1\n22.9 / 51.1\n23 / 51.5\nen_ss\n- / -\n5.7 / 35.4\n6.6 / 36.7\n6.9 / 38\n0.2 / 6.7\n0.1 / 6.1\n0.2 / 12.9\n0.2 / 11.4\nen_sv\n- / -\n43.3 / 67.5\n43.7 / 67.6\n43.2 / 67.3\n2 / 13.7\n35 / 60.4\n36 / 62.5\n35.8 / 61.8\nen_sw\n- / -\n31.7 / 57.2\n30.9 / 56.2\n29 / 54.7\n0.8 / 9.5\n2.4 / 23\n1.7 / 25\n2.8 / 31.2\nen_ta\n- / -\n7.8 / 45.1\n8.3 / 45.3\n8.4 / 45.8\n0.4 / 11.7\n2.1 / 34.1\n4.6 / 40.3\n2.6 / 40.1\nen_te\n- / -\n9.2 / 44.3\n9.8 / 45.1\n10 / 45\n0.5 / 9.6\n5.4 / 35.8\n5.4 / 39.8\n3.6 / 38.5\nen_tg\n- / -\n3.2 / 21\n5.5 / 24.4\n3.4 / 19.5\n0.1 / 0.5\n0.8 / 15.3\n0.8 / 19.7\n0.7 / 20.6\nen_th\n- / -\n6.6 / 40.9\n5.6 / 40.5\n5.8 / 39.4\n0.2 / 10.1\n11.8 / 47.4\n9 / 48.3\n12.6 / 49.1\nen_ti\n- / -\n2.3 / 15.7\n3.3 / 19.6\n3.9 / 20.4\n0.2 / 0.5\n0 / 1.1\n0 / 0.9\n0 / 1.1\nen_tk\n- / -\n12.2 / 42\n14.2 / 46.2\n13.8 / 45.6\n0.2 / 6.7\n1.1 / 18.3\n0.5 / 19.2\n0.6 / 19.4\nen_tn\n- / -\n16.3 / 36.9\n19.9 / 41.5\n19.1 / 40.9\n0.5 / 7.3\n0.3 / 9.1\n0.3 / 10.5\n0.4 / 11.1\nen_to\n- / -\n3.9 / 25.5\n3.3 / 26.4\n4.9 / 27.8\n0.1 / 3.6\n0.3 / 11.2\n0.2 / 9.3\n0.2 / 8.3\nen_tr\n- / -\n25 / 55.1\n25.4 / 55.4\n25.9 / 55.7\n0.8 / 11.6\n11.6 / 43.4\n11.2 / 44.4\n10.4 / 44.7\nen_tt\n- / -\n5 / 27.3\n8 / 32.4\n9.7 / 36\n0.1 / 0.5\n0.7 / 19.7\n0.7 / 22.2\n0.4 / 20.7\nen_ty\n- / -\n0.6 / 2.4\n1.3 / 12.8\n0.4 / 2.1\n0.4 / 9.2\n0.6 / 14.1\n0.4 / 12.6\n0.4 / 11.3\nen_ug\n- / -\n4.7 / 29.9\n5.4 / 31.3\n5.8 / 31.5\n0.1 / 0.3\n0.1 / 1.4\n0.2 / 2.3\n0 / 1.2\nen_uk\n- / -\n24.3 / 51.4\n25.5 / 52.6\n25.6 / 52.5\n1.4 / 5.6\n18.5 / 45.9\n20.4 / 48.2\n18.1 / 46.9\nen_ur\n- / -\n2.3 / 16.3\n2.3 / 16.3\n2.4 / 16.4\n0.2 / 1.4\n7.1 / 32\n4.3 / 31.5\n2.5 / 28.2\nen_uz\n- / -\n0.7 / 1.5\n0.7 / 1.4\n0.6 / 1.3\n0.1 / 7.3\n1.3 / 26.8\n1.8 / 31.9\n2 / 33.9\nen_ve\n- / -\n7 / 31.1\n7.8 / 32\n8.5 / 32.9\n0.2 / 4.2\n0.3 / 11.9\n0.1 / 8.1\n0.1 / 8.1\nen_vi\n- / -\n0.1 / 12.5\n0.1 / 12.4\n0.1 / 12.4\n1.4 / 9.2\n34.4 / 53.4\n34.7 / 53.8\n34.6 / 54\nen_wo\n- / -\n0.4 / 9.8\n0.5 / 10\n0.5 / 12.1\n0.3 / 6.4\n0.4 / 11.4\n0.2 / 8.9\n0 / 5.3\nen_xh\n- / -\n10.3 / 45.1\n10.9 / 47.1\n11.3 / 47.5\n0.2 / 6.8\n0.3 / 14\n0.2 / 12.5\n0.2 / 13.7\nen_yo\n- / -\n5.4 / 20.9\n5.5 / 21.6\n5.7 / 21.2\n0.2 / 4.2\n0.1 / 4.6\n0 / 6.1\n0.2 / 10.5\nen_yue\n- / -\n0 / 1.3\n0 / 1.2\n0 / 1.2\n0 / 1.9\n0.1 / 13.2\n0 / 14.6\n0 / 15.7\nen_zh\n- / -\n3.8 / 29.7\n3.9 / 30.1\n3.9 / 30.1\n1.9 / 2.7\n19.5 / 21.2\n23.6 / 21.3\n23.1 / 23.2\nen_zh_Hant\n- / -\n1 / 8.2\n0.9 / 8.8\n0.7 / 5.3\n0.4 / 4\n6 / 23.1\n6.5 / 22.9\n3.8 / 22.7\nen_zu\n- / -\n2.7 / 24.6\n2.6 / 25.2\n2.6 / 24.7\n0.5 / 7.3\n0.5 / 15.2\n0.2 / 13.9\n0.2 / 12.6\nes_MX_en\n- / -\n50.4 / 71.7\n51.9 / 72.7\n52.3 / 72.7\n6.1 / 24\n40.5 / 64.9\n41.3 / 65.4\n41.1 / 65.3\nes_en\n- / -\n41.1 / 66.4\n42.4 / 67.1\n42.9 / 67.3\n5.9 / 26.2\n33.1 / 60\n35.2 / 61.9\n35.8 / 62\net_en\n- / -\n35.1 / 61.8\n36.8 / 62.8\n36.8 / 62.8\n8.6 / 31.2\n28.2 / 55.8\n30.3 / 56.7\n30.3 / 56.6\neu_en\n- / -\n31.8 / 56.8\n34 / 58.3\n34 / 58.2\n5.5 / 25.7\n22.4 / 48.8\n23.4 / 49.6\n23.8 / 50.1\nfa_AF_en\n- / -\n32.3 / 58.6\n33 / 59\n34.1 / 59.8\n2.4 / 14.8\n18.2 / 42.1\n24.6 / 51.1\n25.6 / 52\nfa_en\n- / -\n32.2 / 58.6\n34 / 59.7\n33.7 / 59.5\n3.7 / 18.7\n16.3 / 37.6\n24.1 / 50.2\n25.4 / 51.6\nff_en\n- / -\n14.4 / 35.8\n14.2 / 36\n15.8 / 37.5\n0.3 / 9.6\n3.3 / 20.5\n4 / 18.7\n2.4 / 18.8\nfi_en\n- / -\n30.3 / 57.8\n31.7 / 58.8\n31.5 / 58.6\n4.2 / 24.6\n24.3 / 52.5\n26.4 / 52.8\n28.1 / 55.1\nfil_en\n- / -\n45.7 / 66.9\n48.2 / 68.5\n48.1 / 68.7\n4 / 19.3\n38.3 / 61.7\n36.2 / 59.5\n39.9 / 62.6\nfj_en\n- / -\n6.6 / 26.6\n8.4 / 28.5\n8.2 / 28.4\n0.5 / 10.3\n4.7 / 18.7\n6.2 / 27.1\n3.7 / 21.7\nfo_en\n- / -\n32.9 / 56.7\n37 / 59.8\n38.6 / 61\n8 / 28.7\n28.6 / 52.8\n33.2 / 57.1\n33.6 / 56.9\nfr_CA_en\n- / -\n43.2 / 66.8\n44.1 / 67.4\n43.8 / 67.4\n5 / 25.8\n26.6 / 56\n37.3 / 62.8\n36.7 / 62.5\nfr_en\n- / -\n38.8 / 63.5\n39.5 / 64.1\n39.2 / 63.9\n4.5 / 26.1\n31.1 / 57.6\n32.6 / 57.8\n32.6 / 58.8\nga_en\n- / -\n38.7 / 64.1\n41 / 65.7\n41.6 / 65.7\n4.2 / 23\n24.6 / 48.7\n29.2 / 53.5\n31.5 / 55.3\ngl_en\n- / -\n43.7 / 66.8\n45.3 / 67.9\n45.1 / 67.8\n6 / 19.5\n33.9 / 59.7\n35.3 / 60.3\n36.9 / 62.3\ngu_en\n- / -\n34.9 / 62\n38.3 / 64\n38 / 64.1\n2.2 / 16.6\n21.1 / 48\n24.2 / 52.1\n25.4 / 53.6\nha_en\n- / -\n35 / 56.9\n39.2 / 59.9\n38.7 / 60\n1.1 / 14.7\n12.5 / 33\n21 / 43.6\n18.9 / 43.3\nhi_en\n- / -\n35.7 / 61.8\n38.7 / 63.4\n38.2 / 63.1\n2.7 / 14.7\n20 / 44.9\n24.6 / 52.3\n26.7 / 54.3\nhmn_en\n- / -\n23.4 / 47.5\n25.5 / 49.6\n27 / 50.2\n1 / 12.3\n10.8 / 30.5\n13.4 / 38.2\n14.3 / 37.7\nhr_en\n- / -\n41.3 / 65.5\n42.4 / 66.5\n42.3 / 66.4\n6.6 / 26.9\n29.9 / 55.6\n31.7 / 57.6\n35.2 / 60.6\nhu_en\n- / -\n27.8 / 55.6\n27.6 / 55.8\n28.1 / 55.8\n3.5 / 25.6\n20.1 / 47\n21.6 / 49\n21.3 / 49.2\nhy_en\n- / -\n30.7 / 55.8\n34.8 / 59.3\n34.1 / 58.9\n3.2 / 18.7\n17.8 / 44.4\n22.9 / 48.8\n23.7 / 50.7\nid_en\n- / -\n43 / 66.4\n45.1 / 67.8\n45 / 67.8\n3.8 / 18.4\n34.8 / 59.9\n37.3 / 61.5\n38 / 61.9\nig_en\n- / -\n27.5 / 51.1\n31.9 / 55.3\n32.7 / 55.8\n1.1 / 14.4\n9.4 / 28.1\n11.4 / 33.4\n11.8 / 33.1\nis_en\n- / -\n35.3 / 60.5\n37.3 / 61.8\n37.7 / 61.8\n9.1 / 31.5\n29 / 54.9\n30.5 / 55.3\n33 / 57.9\nit_en\n- / -\n42.3 / 66.2\n43.2 / 66.8\n43.2 / 66.9\n7.5 / 28.6\n36.8 / 61.5\n36 / 60.5\n38.5 / 62.4\niw_en\n- / -\n40.5 / 63.8\n42.9 / 65.7\n42.9 / 65.6\n4.3 / 19.1\n29.2 / 55.7\n31 / 56.9\n31.1 / 57.4\nja_en\n- / -\n27.1 / 54.5\n28.7 / 55.1\n29.1 / 55.5\n0.3 / 1.2\n15 / 40.9\n19.2 / 46.6\n18.7 / 45.9\nka_en\n- / -\n28.8 / 53.9\n31.8 / 56.1\n32 / 56.3\n2.6 / 14.1\n20.1 / 45\n26.3 / 52.4\n25.3 / 52.6\nkk_en\n- / -\n24.1 / 52.1\n26.4 / 53.6\n25.7 / 53.3\n3.7 / 18.6\n16.4 / 42.9\n18.3 / 44.3\n18 / 44\nkm_en\n- / -\n33.6 / 59.6\n37.8 / 61.4\n37.3 / 61.7\n1.9 / 16.1\n21.1 / 47.2\n26.4 / 52.2\n27.2 / 53.5\nkn_en\n- / -\n29.2 / 56.8\n31.4 / 57.9\n31.1 / 58.3\n1.3 / 11.5\n13.7 / 38.9\n18.8 / 45.8\n20.4 / 48.2\nko_en\n- / -\n30.9 / 57.1\n33.1 / 58.6\n33.2 / 58.8\n1.1 / 4.5\n19 / 45.8\n18.7 / 45.2\n19 / 47\nku_en\n- / -\n27.6 / 52.5\n30.5 / 54.4\n30.7 / 54.9\n2.9 / 18.3\n17.2 / 41.7\n16.5 / 43.2\n20.2 / 44.2\nky_en\n- / -\n23.8 / 50.5\n27.5 / 53.6\n26.2 / 52.6\n2.6 / 16.1\n15.4 / 41.5\n15.9 / 41.1\n17 / 43.2\nlb_en\n- / -\n21.9 / 49.8\n24.4 / 51.6\n26 / 52\n4.2 / 23.8\n29.5 / 53.3\n32.3 / 56.6\n30.6 / 56.1\nlo_en\n- / -\n34.9 / 58.2\n37.5 / 59.7\n37.9 / 60.9\n1.2 / 11.8\n23.4 / 49\n28.5 / 53.9\n28.1 / 53.5\nlt_en\n- / -\n32 / 59.6\n33.9 / 60.6\n34.1 / 60.6\n4.9 / 25.7\n24.3 / 52.3\n25.7 / 52.9\n26.4 / 52.9\nlv_en\n- / -\n34.6 / 61.2\n36.2 / 62.3\n36.6 / 62.3\n3.4 / 22.7\n26.1 / 53.5\n29.5 / 55.4\n30.6 / 57.1\nmey_en\n- / -\n18 / 45.5\n19.7 / 47\n20.2 / 46.9\n0.3 / 4.2\n4.5 / 22.6\n11.8 / 38.8\n10.4 / 38.8\nmg_en\n- / -\n25.8 / 50.8\n28.3 / 52.7\n28.7 / 52.9\n1.7 / 18.6\n14 / 37.6\n14.8 / 39.1\n16 / 39.5\nmi_en\n- / -\n22.1 / 45.3\n20.5 / 44.1\n22.5 / 46.3\n2.2 / 19.2\n10.4 / 35.6\n10.8 / 35.7\n17.7 / 41.1\nmk_en\n- / -\n44.6 / 67.4\n46.3 / 68.5\n46.4 / 68.4\n8.5 / 28.5\n29.3 / 55.2\n36.9 / 61.3\n37.7 / 62.6\nml_en\n- / -\n30.3 / 56.8\n31.4 / 57.1\n32.4 / 58.2\n0.8 / 5.7\n11.5 / 34.3\n17.8 / 45.8\n18.8 / 46.5\nmn_en\n- / -\n21.8 / 48.4\n24 / 50.4\n24.1 / 50.2\n1.3 / 10.1\n12.9 / 38.5\n13.6 / 38.4\n14.3 / 40.8\nmr_en\n- / -\n32.5 / 59.3\n34 / 60.2\n34.9 / 60.9\n1.7 / 9.8\n12.7 / 35.2\n22.6 / 50.7\n23.4 / 50.4\nms_en\n- / -\n40.4 / 63.7\n42.7 / 65.4\n42.7 / 65.3\n3.9 / 20.7\n31.3 / 55.8\n36 / 60.1\n36.4 / 60.3\n47\nmt_en\n- / -\n50.9 / 72.9\n52.4 / 73.9\n53.5 / 74.3\n5.6 / 25\n40.3 / 63.2\n40.1 / 62.5\n41.5 / 65.1\nmy_en\n- / -\n14.3 / 40.3\n15.7 / 40.9\n18.1 / 44.5\n0.2 / 5.7\n2.8 / 20.2\n7.5 / 32.2\n9.7 / 34.2\nnd_en\n- / -\n18.8 / 43.1\n21.6 / 45.6\n21.8 / 45.8\n0.2 / 8\n8 / 27.2\n9 / 32.6\n9.1 / 32.1\nne_en\n- / -\n35.9 / 62\n38.1 / 63.3\n38.6 / 63.9\n1.4 / 9\n15.6 / 40.3\n22.3 / 48.8\n23.5 / 50.9\nnl_en\n- / -\n41.2 / 66.1\n42.5 / 66.7\n42.5 / 66.6\n8.3 / 31.4\n35.1 / 60.3\n36.5 / 61.4\n36.6 / 62.3\nnn_en\n- / -\n46 / 67.3\n47.1 / 68.1\n47 / 68.1\n5.4 / 26.9\n39 / 63.2\n40.5 / 63.9\n41 / 63.3\nno_en\n- / -\n44 / 67\n45.1 / 67.7\n45 / 67.7\n6.1 / 25.6\n38.8 / 61.7\n38.8 / 63.5\n39.7 / 63.5\nnso_en\n- / -\n23.6 / 47.6\n27.5 / 51.3\n28.1 / 51.7\n0.6 / 10.9\n3.7 / 26.1\n10.1 / 31.4\n11.1 / 34.9\nny_en\n- / -\n28.1 / 49.5\n31.6 / 52.9\n31.9 / 53.2\n1.1 / 13\n15.3 / 36.4\n17.6 / 38.8\n15.4 / 39.4\nom_en\n- / -\n8.4 / 29.2\n11.2 / 33.6\n11.2 / 33.6\n0.5 / 9.8\n1.9 / 21.2\n4.6 / 24.3\n5.2 / 23.2\npa_en\n- / -\n34.6 / 60.8\n38.5 / 62.9\n38.2 / 63.1\n2.4 / 16.1\n16.1 / 42.3\n24 / 51\n25.4 / 51.8\npl_en\n- / -\n31.9 / 58.9\n32.6 / 59.2\n33.3 / 59.7\n9.3 / 34.9\n24.4 / 52.1\n27.4 / 54.4\n27.3 / 53.8\nps_en\n- / -\n22.6 / 49.5\n23.9 / 50.5\n24.5 / 50.8\n1.9 / 14.2\n14.1 / 38.8\n15.6 / 40.3\n16.3 / 42.1\npt_PT_en\n- / -\n44.7 / 67.4\n45.7 / 68.2\n45.9 / 68.2\n5.2 / 23.6\n35.2 / 60.3\n36 / 60.8\n36.8 / 61.4\npt_en\n- / -\n43.1 / 65.7\n43.9 / 66.2\n44.1 / 66.2\n6.4 / 24.9\n34.4 / 59.3\n35.8 / 60.8\n36.8 / 61.1\nro_en\n- / -\n37.7 / 63.9\n38.5 / 64.6\n39 / 64.7\n6.1 / 28.5\n22.8 / 46.9\n32.3 / 58.6\n33.4 / 59.5\nru_en\n- / -\n30.8 / 57.9\n32.3 / 58.8\n32.1 / 58.7\n3.7 / 19.2\n24 / 50.2\n26.1 / 53.4\n25.3 / 52.9\nrw_en\n- / -\n27.6 / 52.4\n30.9 / 55.2\n32.3 / 55.8\n1.9 / 18.2\n15.8 / 37.9\n17.2 / 42.3\n16.3 / 42.3\nsd_en\n- / -\n30.6 / 54.2\n34 / 57\n34.1 / 56.9\n1.5 / 12.1\n15.8 / 40.8\n14.6 / 40.8\n13.3 / 42.1\nshi_en\n- / -\n3.3 / 23.3\n4.4 / 25.8\n5.8 / 27.6\n0.1 / 3.1\n1.5 / 15.9\n2.9 / 24\n2.5 / 23.5\nsi_en\n- / -\n31.7 / 58\n34 / 59.4\n34.3 / 60\n1.1 / 10.1\n16.2 / 44.1\n18.1 / 45.5\n17.3 / 43.8\nsk_en\n- / -\n40.7 / 65.6\n42.5 / 66.6\n42.5 / 66.5\n8.7 / 31.5\n26.6 / 52.2\n33.1 / 58.4\n35.1 / 61\nsl_en\n- / -\n36.5 / 62.4\n38.2 / 63.4\n38.3 / 63.3\n6.5 / 28.7\n29.7 / 56.3\n32.2 / 58.2\n30.8 / 56.6\nsm_en\n- / -\n25.6 / 50.5\n28.9 / 53.6\n28.5 / 53.5\n0.6 / 9\n6 / 24.1\n14.2 / 39.9\n14 / 37.7\nsn_en\n- / -\n28.6 / 50.6\n33.5 / 54.4\n34.5 / 55\n0.8 / 10.9\n8.6 / 25.6\n16.6 / 38.8\n15.1 / 37.7\nso_en\n- / -\n36.3 / 59.1\n40.7 / 62.8\n41.1 / 63.3\n1.9 / 13.1\n20.5 / 42.3\n20.3 / 43.8\n21.3 / 49\nsq_en\n- / -\n41.9 / 65.8\n43.5 / 66.8\n43.5 / 66.9\n12.6 / 35.4\n35.4 / 60.4\n35.6 / 60\n36.9 / 62\nsr_Latn_en\n- / -\n37 / 62.3\n39.4 / 63.9\n39 / 63.8\n4.1 / 22.8\n26.7 / 53.1\n31.1 / 56.9\n30.7 / 56\nsr_en\n- / -\n37.3 / 62\n39.2 / 63\n39.6 / 63.5\n5.4 / 24\n27.7 / 53.6\n31.3 / 57\n33 / 58.6\nss_en\n- / -\n23.9 / 46.4\n28.4 / 50.6\n29.6 / 51.7\n0.4 / 10\n6.3 / 22.8\n8.5 / 31.5\n6.7 / 31.4\nsv_en\n- / -\n45.5 / 68.1\n46.9 / 69\n46.8 / 68.9\n8.2 / 31.9\n40.7 / 64.3\n41.7 / 64.9\n42 / 65\nsw_en\n- / -\n41.8 / 62.7\n44.1 / 64.5\n44.2 / 64.5\n3.8 / 17.9\n31 / 54.3\n32.2 / 55.1\n33.3 / 56.6\nta_en\n- / -\n27.3 / 53.5\n28.4 / 54\n29.4 / 54.9\n1.3 / 7.4\n15.9 / 42\n15.9 / 43.5\n18.2 / 45.6\nte_en\n- / -\n27.5 / 53.7\n28.6 / 54.4\n29.6 / 55.1\n1 / 4.8\n12.5 / 36.6\n18 / 44.4\n19.2 / 46.4\ntg_en\n- / -\n9.6 / 32.2\n19.9 / 43.8\n16.6 / 39\n2.3 / 17.1\n16.7 / 42\n20.4 / 46.8\n20.6 / 47.5\nth_en\n- / -\n37.2 / 60.3\n39.7 / 61.9\n39.9 / 62.4\n3.9 / 12\n20 / 44.3\n27.8 / 54.5\n30.2 / 56.3\nti_en\n- / -\n17.5 / 42.2\n22.2 / 46.7\n24.1 / 48.4\n0.7 / 10.2\n8 / 29.9\n8 / 31.9\n9.4 / 31.8\ntk_en\n- / -\n28.5 / 54.8\n31.6 / 57\n31 / 56.5\n2.1 / 18.3\n4.4 / 26.3\n15.7 / 40.3\n16 / 44\ntn_en\n- / -\n24.2 / 48\n29 / 52.8\n30.9 / 53.8\n0.3 / 9.2\n7.6 / 29.1\n9.6 / 32\n10.8 / 34.9\nto_en\n- / -\n15.9 / 39.9\n21.2 / 45\n21.4 / 44.7\n0.3 / 6.8\n7.2 / 25.8\n7.6 / 26.3\n3.2 / 26.5\ntr_en\n- / -\n32.5 / 58.7\n34.3 / 59.9\n34.6 / 60\n4.3 / 23.6\n14.7 / 36.6\n21.3 / 48.3\n22.4 / 49.4\ntt_en\n- / -\n14.4 / 36\n14.4 / 34.9\n19.3 / 41.7\n1.1 / 9.7\n9.5 / 38\n16.8 / 42.6\n16.9 / 43.7\nty_en\n- / -\n5.5 / 28.9\n5.5 / 28.6\n6.3 / 29.5\n0.5 / 10.3\n6 / 27.7\n5.3 / 27\n3.5 / 26.3\nug_en\n- / -\n9.1 / 32.2\n14.7 / 39.9\n13.4 / 35.6\n1.2 / 13.8\n10.5 / 35.6\n12.6 / 39.4\n11.6 / 37.7\nuk_en\n- / -\n34 / 59.4\n35.2 / 60.1\n35.1 / 60.1\n6.6 / 26.8\n26.7 / 54.1\n27.4 / 53.6\n28.1 / 54.1\nur_en\n- / -\n3.4 / 18.9\n3.8 / 19\n3.7 / 19.1\n4.3 / 20\n22.2 / 49.7\n23.9 / 50.5\n25.6 / 53\nuz_en\n- / -\n24.5 / 52.2\n27.2 / 54.5\n26.9 / 54.4\n2.8 / 17.8\n16.4 / 44.4\n16.5 / 42.7\n16.9 / 42.8\nve_en\n- / -\n19.1 / 41.9\n21.2 / 44.6\n24.3 / 46.7\n0.2 / 9.3\n5.4 / 21.9\n4.7 / 25\n3.3 / 22.2\nvi_en\n- / -\n0.4 / 16\n0.3 / 15.9\n0.3 / 15.9\n5.5 / 24.9\n13.8 / 34.5\n27.1 / 52.8\n30.6 / 55.4\nwo_en\n- / -\n2.1 / 18.4\n2.4 / 19.3\n2.8 / 19.4\n0.1 / 7.3\n2.2 / 20.9\n2.2 / 20.3\n2.2 / 20.2\nxh_en\n- / -\n29.9 / 52.5\n34.2 / 56.1\n34.1 / 56.2\n1 / 14.8\n13.7 / 33.4\n16.2 / 37.8\n14.5 / 39.6\nyo_en\n- / -\n13.3 / 37\n14.5 / 37.4\n20.9 / 43.5\n0.4 / 11.1\n3.8 / 23.9\n3.4 / 21.5\n4.6 / 23\nyue_en\n- / -\n26.7 / 54.4\n29.1 / 56.2\n29.3 / 56.2\n0.3 / 1.8\n16.3 / 44.1\n18.1 / 46\n17.1 / 45.9\nzh_Hant_en\n- / -\n27.1 / 54\n29.1 / 55.5\n29.1 / 55.6\n1.6 / 7.7\n10 / 29.8\n19.2 / 46.4\n19.5 / 46.6\nzh_en\n- / -\n24.3 / 53.4\n26.8 / 54.9\n26.3 / 54.6\n0.4 / 3\n17.8 / 45.4\n18.8 / 46.4\n19.1 / 46.6\nzu_en\n- / -\n6.5 / 24.3\n7.5 / 25\n7.5 / 25.1\n0.9 / 13.2\n13.1 / 32.6\n17 / 38.6\n16.7 / 40.4\nxx2en\n- / -\n30.6 / 54.5\n32.7 / 56.2\n33.6 / 57.6\n3.2 / 17.3\n20.4 / 43.8\n23.8 / 48.2\n24.4 / 49.0\nen2xx\n- / -\n16.5 / 39.6\n17.6 / 41.9\n17.9 / 41.9\n0.8 / 7.3\n11.7 / 31.2\n12.6 / 32.4\n12.3 / 32.3\nAverage\n19.8 / -\n23.5 / 47.0\n25.1 / 49.0\n25.7 / 49.7\n2.0 / 12.3\n16.0 / 37.4\n18.1 / 40.2\n18.3 / 40.6\n48\nA.11\nCanaries\nWe design and generate different types of canaries for each dataset. We treat the MADLAD-400\ndataset as a large unlabeled pretraining corpus and for each language with sufficient size ( > 50, 000\nsamples), we generate no more than 0.05% canaries per language, leading to a total of 1, 279, 635\ncanaries across all languages. For parallel data, we design and generate different canaries specific\nto its usage for translation. We generate 585, 596 canaries in total for all target languages with\n> 50, 000 samples. In both cases, we scale the proportion of canaries based on the size of each\nlanguage to minimize their impact to utility in model training. Finally, we also generate 80, 400\n\u201cgeneric\u201d canaries that share no resemblance to natural data. In total, we generate 1, 945, 631 canaries\n(\u2248 0.0026% of the training data).\nA.11.1\nMADLAD-400 Canaries\nBecause MADLAD-400 contains only monolingual data, i.e., each example relates to only one\nlanguage, we treat it as a large unlabeled pretraining corpus. Similar to Anil et al. [6], we aim to\ngenerate canaries that share characteristics of the underlying training data but are still outliers. For this,\nwe design three types of canaries: shuffle, interleave and training_prefix. interleave\ncanaries can be viewed as the closest to natural data, where all tokens are sampled from the underlying\ndistribution and most sequence-level correlations are kept intact. We generate these canaries by\nsampling two real documents from a language, and interspersing chunks of 20 tokens in their\nsame relative ordering. On the other hand, shuffle canaries can be viewed as the farthest from\nnatural data, sharing only the average token frequencies of the language but with no sequence-level\ncorrelations. These are generated by sampling a real document and uniformly sampling its tokens\nwithout replacement. In addition, we also propose training_prefix canaries which can be viewed\nas something in between. Here, each canary is generated by sampling 100 tokens from a real sample\nand then completing the sequence with random tokens (i.e, taken uniformly with replacement from\nthe vocabulary).\nWe take care to adjust the number and type of canaries based on the resources of the language, in\norder to minimize any harm to model utility. We group languages based on their relative size. Then,\nprior to generation we fix a target canary rate based on this resource level - this determines the total\nnumber of canaries that can be added per language. We choose a smaller proportion of canaries\n(relative to the total number of sequences in the language) for lower-resource languages based on the\nintuition that these languages can tolerate less noisy data before utility is altered. With the number of\ncanaries fixed, we then choose how many times each canary will be repeated, as this has a significant\nimpact on memorization [44, 6]. Note that repeating canaries may be beneficial to study the relative\nvulnerability of repeated data, but also reduces to the total number of unique canaries that can be\nadded within that language\u2019s canary budget. We choose the distribution of repeats heuristically\naiming to maximize the support of the distribution while also ensuring that each bucket has enough\nunique canaries to achieve meaningful results. Finally, as the language size grows, (and thus the\ncanary budget as well) we also introduce more canary types. We describe the full distribution of\ncanaries generated in Table 20.\nTable 20: MADLAD-400 distribution of (the 1, 279, 635 in total) canaries across languages. For\nlegibility, we omit 0 entries. *Uses the codes: I=interleave, S=shuffle, TP=training_prefix.\nDataset Size\n(# Samples)\nLanguages\nIncluded\nTarget\nCanary Rate\nCanary\nTypes*\nCanaries\nper Language\nCanaries per # of Repetitions\n1\n2\n3\n4\n5\n8\n10\n25\n50\n100\nX-Large\n(200e6+)\nsw, kaa, si, gu,\nkn, ne, uz, gl,\nmn, fil, mk, eu,\nka, be, af, bn,\nte, is, mr, ml,\nhr, kk, ms, az,\nta\n0.016%\nI,S,TP\n31500\n600\n600\n600\n600\n600\n600\n300\n60\n60\n49\nLarge\n(20e6 to\n200e6)\nsw, kaa, si, gu,\nkn, ne, uz, gl,\nmn, fil, mk, eu,\nka, be, af, bn,\nte, is, mr, ml,\nhr, kk, ms, az,\nta\n0.035%\nI,S,TP\n7020\n195\n150\n60\n60\n45\n30\n30\nMedium\n(6e6 to\n20e6)\nlo, dv, lb, fy, so,\nam, ps, zh, ku,\nkm, pa, mt, tt,\nga, tg, cy, ky,\nhy, my, eo\n0.02%\nI,S,TP\n1200\n150\n75\n60\n30\n12\nSmall\n(600e3\nto 6e6)\nilo,\nos,\ncnh,\nctd_Latn,\nti,\nudm, om, se,\nrm, tet, bo, ro,\nbr, sa, lus, gsw,\nsah, kaa_Latn,\nst, haw, pap,\noc, cv, zu, sn,\nyo, as, sm, co,\nxh, ig, ny, kl,\nsu,\nceb,\ntk,\nfo,\nyi,\nhmn,\nel_Latn, ba, jv,\ngrc, or, sd, gd,\nug, ckb, mg, ht,\nha, rw\n0.02%\nI,S\n120\n12\n8\n8\n3\nXSmall-5\n(500e3\nto 600e3)\nhil\n0.02%\nI,S\n100\n12\n8\n6\n6\nXSmall-4\n(400e3\nto 500e3)\nte_Latn,\ntyv,\nvec, kbd, lg\n0.02%\nI,S\n80\n8\n8\n6\n4\nXSmall-3\n(300e3\nto 400e3)\niba,\nak,\nav,\nber_Latn, zza,\nts, ee, ru_Latn\n0.02%\nI,S\n60\n8\n6\n6\n2\nXSmall-2\n(200e3\nto 300e3)\nta_Latn,\ncfm,\notq, syr, bua,\ngn, to, az_RU,\nwa,\nchm,\ntn,\nada, krc, fj, nso\n0.02%\nI,S\n40\n8\n4\n2\n2\nXSmall-1\n(100e3\nto 200e3)\nemp, pck, meu,\nnnb, meo, nzi,\ntlh,\ntzo,\niu,\nksd,\nhui,\ntiv,\nsq, new, bci,\nbbc, min, sr,\nnan_Latn_TW,\nve,\nang,\nml_Latn, kac,\nngu, pag, abt,\nkum, tyz, zap,\nkv,\nbik,\nnv,\ngom, ltg, qu,\nay,\nrom,\nsg,\nady, iso, yua,\nwar, bho, hif,\nkbp, srn, myv,\nkha\n0.02%\nI,S\n20\n4\n4\n2\nXXSmall-5\n(90,000\nto 100e3)\nmrj,\nzxx_xx_dtynoise,\nkw, mgh, bew,\ncrh, alt, nhe,\nfon\n0.01%\nI\n9\n2\n2\n1\nXXSmall-4\n(80,000\nto 90,000)\nmam, dov, ho,\nmai, bgp\n0.01%\nI\n8\n3\n1\n1\nXXSmall-3\n(70,000\nto 80,000)\ngym, rcf, shn,\ntvl, mbt, qub,\npon\n0.01%\nI\n7\n2\n1\n1\n50\nXXSmall-2\n(60,000\nto 70,000)\nium, gag, tbz,\ngv,\ncrs,\nquc,\nzh_Latn, chk,\nbtx, ace, bru,\nubu, ape, mdf,\ntuc\n0.01%\nI\n6\n2\n2\nXXSmall-1\n(50,000\nto 60,000)\ntzh, kek, bum,\nbts,\nibb,\ntcy,\nenq,\nkj,\nseh,\nxal, kmb, rwo,\ncab, wo, ppk,\nach, kri, ss, cuk\n0.01%\nI\n5\n3\n1\nA.11.2\nParallel Canaries\nUnlike MADLAD-400, the parallel data consists of source-target pairs from different languages\ncorresponding to the source and target languages for translation. This leads to new intricacies not\npresent in the monolingual setting, e.g., because languages have different grammatical structure, it\nmay be difficult to tailor modifications to both the inputs and outputs simultaneously that maintain\nlinguistic structure.\nRather than design canaries for all N 2 combinations of language pairs, where many pairs may have\ninsufficient resource levels to incorporate canaries, we instead focus on the multiway setting where\nlanguages are grouped by the target language for translation. To minimize impact on the source\nlanguages (which may include very low-resource languages), we opt to not use any real training\ndata from the source as inputs to the model. Instead, we generate canary data following one of two\nmethodologies for the source: random_prefix corresponds to cases where all canaries for a given\ntarget language share the same prefix of 100 tokens but have unique uniformly random (w.r.t. the\ntoken vocabulary) suffixes following it and full_random canaries are analogous with no shared\nprefix. The shared prefix of random_prefix canaries is designed to align with cases where data\nmay share common subsequences. For the targets, we either interleave or shuffle them as done\nin the MADLAD-400 case above (Appendix A.11.1), except interleaving in batches of 50. Taking the\nouter product of these options, we get four possible canaries, e.g., random_prefix_interleave\nand so forth. The resource level groups and distribution of canaries is the same as for the MADLAD-\n400 canaries described in Table 20; the mapping for parallel languages to these resource level groups\nis shown in Table 21. In total, 565, 476 canaries are generated this way across all languages.\nFinally, we also design two additional types of canaries that use natural data from the source.\nBecause this might impact utility more, we restrict these canaries to only the largest language\npairs that have at least 500, 000 examples. The set of language pairs and languages satisfying this\nthreshold are shown in Figure 3 and Figure 4. These two canary types are interleaved_both and\ninterleaved_mislabeled_to. The former performs the same interleaving operation on the source\nand targets for a language pair, interleaving in batches of 50 tokens. The latter does the same, with\nthe addition of also select a new target language label, uniformly at random, from all qualifying high\nresource languages. For each language pair listed in Figure 3, we generate 60 canaries in total, split\nevenly across the two canary types, and in the following distribution: 10 canaries are repeated once, 5\nare repeated twice, and 2 are repeated 5 times. This gives a total of 21, 120 canaries in total across all\nlanguage-pairs. Combined with the 565, 476 canaries from the prior 4 canary types, there are a total\nof 585, 596 canaries.\nTable 21:\nDistribution of (the 565, 476 in total) canaries across languages for paral-\nlel data.\nDistribution of canaries across repeats matches that in Table 20.\n*Uses the\ncodes: RPI=random_prefix_interleave, RPS=random_prefix_shuffle, FRI=fully_random_interleave,\nFRS=fully_random_shuffle. **includes 4 canary types instead of 3; so the canary distribution can be\nobtained by multiplying the values in Table 20 by 4/3.\nDataset Size\n(# Samples)\nLanguages\nIncluded\nTarget\nCanary Rate\nCanary\nTypes*\nCanaries\nper Language\nX-Large**\n(200e6+)\nnl, it, pt, fr, de,\nes, en\n0.016%\nI,S,TP\n42000\n51\nLarge**\n(20e6 to\n200e6)\nfa, is, uk, tr, hr,\nca, ko, ja, et,\nno, lv, sl, lt, bg,\nar, el, fi, sk, ro,\nru, hu, da, zh,\npl, cs, sv\n0.035%\nRPI,RPS,FRS,FRI\n9360\nMedium\n(6e6 to\n20e6)\nth, si, bn, sh,\nbs, hi, eo, mt,\neu, sq, sr, gl,\nga, mk, he, vi,\nid\n0.02%\nRPI,RPS,FRS\n1200\nSmall\n(600e3\nto 6e6)\nhy, ps, as, lb,\nuz,\nwa,\nmy,\niu, la, ha, pa,\nmg,\nxh,\ncy,\nceb,\ngu,\nkm,\nen_xx_simple,\nbr, af, fy, tt, be,\nka, oc, nds, tg,\nkk, az, nn, ne,\nsw,\nzh_Hant,\nur, fil, ms, te,\nmr, ta, ml\n0.02%\nRPI,RPS\n120\nXSmall-5\n(500e3\nto 600e3)\ndv, so, am, ba,\nse, sd\n0.02%\nRPI,RPS\n100\nXSmall-4\n(400e3\nto 500e3)\nkn,\nfo,\nor,\nfr_CA, ku\n0.02%\nRPI,RPS\n80\nXSmall-3\n(300e3\nto 400e3)\nli, ug, arz, lmo,\nwuu, rw\n0.02%\nRPI,RPS\n60\nXSmall-2\n(200e3\nto 300e3)\nltg,\ngn,\nbug,\nks_Deva,\nszl,\nfuv,\nary,\nban, bm, ace,\nbjn_Arab, taq,\nace_Arab, bjn,\nprs, taq_Tfng,\ntzm, lij, hne,\nvec,\ndin,\nkr_Arab,\nsc,\nks,\nfur,\nshn,\nbho, mag, nus,\nmi,\nmni,\ndz,\nscn, an\n0.02%\nRPI,RPS\n40\nXSmall-1\n(100e3\nto 200e3)\nnds_NL, ky, zu,\nmn, jv, bar, gd,\nkr, crh_Latn\n0.02%\nRPI,RPS\n20\nXXSmall-5\n(90,000\nto 100e3)\nmwl\n0.01%\nRPI\n9\nXXSmall-3\n(70,000\nto 80,000)\nio\n0.01%\nRPI\n7\nA.11.3\nGeneric Canaries\nFinally, we also designed and generated 80, 400 generic canaries. Unlike the canaries of the prior\ntwo sections (A.11.1 and A.11.2), these canaries share minimal resemblance to natural data. These\ncanaries may be useful for understanding memorization of highly outlier data. Here, we generate\nmonolingual canaries where the source and targets are the same. We propose two types of canaries:\nrandom_prefix and fully_random canaries. These are the same as the canaries described in\nSection A.11.2 but with the source matching the target. We generated 80, 400 canaries in total split\nevenly among 4 types of canaries: fully_random canaries and random_prefix canaries with\nshared prefixes of length 50, 100, and 200.\n52\nar-es, ar-fr, ar-ru, ar-zh, bg-cs, bg-da, bg-de, bg-el, bg-es, bg-fi, bg-fr, bg-\nhu, bg-it, bg-lt, bg-nl, bg-pl, bg-pt, bg-ro, bg-sk, bg-sl, bg-sv, cs-bg, cs-da,\ncs-de, cs-el, cs-es, cs-fi, cs-fr, cs-hu, cs-it, cs-lt, cs-nl, cs-pl, cs-pt, cs-\nro, cs-ru, cs-sk, cs-sl, cs-sv, cs-zh, da-bg, da-cs, da-de, da-el, da-es, da-fi,\nda-fr, da-hu, da-it, da-lt, da-nl, da-pl, da-pt, da-ro, da-sk, da-sl, da-sv, da-\nzh, de-bg, de-cs, de-da, de-el, de-es, de-fi, de-fr, de-hu, de-it, de-lt, de-nl,\nde-pl, de-pt, de-ro, de-ru, de-sk, de-sl, de-sv, de-zh, el-bg, el-cs, el-da, el-\nde, el-es, el-fi, el-fr, el-hu, el-it, el-lt, el-nl, el-pl, el-pt, el-ro, el-sk,\nel-sl, el-sv, es-ar, es-bg, es-cs, es-da, es-de, es-el, es-fi, es-fr, es-hu, es-\nit, es-lt, es-nl, es-pl, es-pt, es-ro, es-ru, es-sk, es-sl, es-sv, es-zh, fi-bg,\nfi-cs, fi-da, fi-de, fi-el, fi-es, fi-fr, fi-hu, fi-it, fi-lt, fi-nl, fi-pl, fi-\npt, fi-ro, fi-sk, fi-sl, fi-sv, fr-ar, fr-bg, fr-cs, fr-da, fr-de, fr-el, fr-es,\nfr-fi, fr-hu, fr-it, fr-lt, fr-nl, fr-pl, fr-pt, fr-ro, fr-ru, fr-sk, fr-sl, fr-\nsv, fr-zh, hu-bg, hu-cs, hu-da, hu-de, hu-el, hu-es, hu-fi, hu-fr, hu-it, hu-lt,\nhu-nl, hu-pl, hu-pt, hu-ro, hu-sk, hu-sl, hu-sv, it-bg, it-cs, it-da, it-de, it-\nel, it-es, it-fi, it-fr, it-hu, it-lt, it-nl, it-pl, it-pt, it-ro, it-ru, it-sk,\nit-sl, it-sv, it-zh, lt-bg, lt-cs, lt-da, lt-de, lt-el, lt-es, lt-fi, lt-fr, lt-\nhu, lt-it, lt-nl, lt-pl, lt-pt, lt-ro, lt-sk, lt-sl, lt-sv, nl-bg, nl-cs, nl-da,\nnl-de, nl-el, nl-es, nl-fi, nl-fr, nl-hu, nl-it, nl-lt, nl-pl, nl-pt, nl-ro, nl-\nru, nl-sk, nl-sl, nl-sv, nl-zh, pl-bg, pl-cs, pl-da, pl-de, pl-el, pl-es, pl-fi,\npl-fr, pl-hu, pl-it, pl-lt, pl-nl, pl-pt, pl-ro, pl-ru, pl-sk, pl-sl, pl-sv, pl-\nzh, pt-bg, pt-cs, pt-da, pt-de, pt-el, pt-es, pt-fi, pt-fr, pt-hu, pt-it, pt-lt,\npt-nl, pt-pl, pt-ro, pt-ru, pt-sk, pt-sl, pt-sv, pt-zh, ro-bg, ro-cs, ro-da, ro-\nde, ro-el, ro-es, ro-fi, ro-fr, ro-hu, ro-it, ro-lt, ro-nl, ro-pl, ro-pt, ro-sk,\nro-sl, ro-sv, ru-ar, ru-cs, ru-de, ru-es, ru-fr, ru-it, ru-nl, ru-pl, ru-pt, ru-\nzh, sk-bg, sk-cs, sk-da, sk-de, sk-el, sk-es, sk-fi, sk-fr, sk-hu, sk-it, sk-lt,\nsk-nl, sk-pl, sk-pt, sk-ro, sk-sl, sk-sv, sl-bg, sl-cs, sl-da, sl-de, sl-el, sl-\nes, sl-fi, sl-fr, sl-hu, sl-it, sl-lt, sl-nl, sl-pl, sl-pt, sl-ro, sl-sk, sl-sv,\nsv-bg, sv-cs, sv-da, sv-de, sv-el, sv-es, sv-fi, sv-fr, sv-hu, sv-it, sv-lt, sv-\nnl, sv-pl, sv-pt, sv-ro, sv-sk, sv-sl, sv-zh, zh-ar, zh-cs, zh-da, zh-de, zh-es,\nzh-fr, zh-it, zh-nl, zh-pl, zh-pt, zh-ru, zh-sv\nFigure 3: All language pairs satisfying the minimum 500, 000 example threshold.\nda, cs, fi, el, ar, de, ru, fr, zh, nl, bg, lt, es, sv, pl, hu, pt, it, sk, ro,\nsl\nFigure 4: Unique languages from Figure 3\nA.12\nAdditional Memorization Figures\n10000\n1e+06\n1e+08\nLanguage Size (# of Senences)\n0.1\n1\n10\n%-Point Increase in Approximate Memorization\n1e+06\n1e+07\n1e+08\n1e+09\nLanguage Size (# of Senences)\n0.1\n1\n10\n%-Point Increase in Approximate Memorization\nFigure 5: Translate models may also paraphrase memorizations. Training data extraction rates\nunder approimate memorization for both (left) monolingual (translate_copy) and (right) multiway\n(translate_diff) data. Extraction performed on the 3B parameter model using a S = P + 50. We\nuse a Levenshtein similarity of 90% on the additioanl leakage. After accounting for this, a fewer\n208/370 and 119/146 had no memorization detected.\n53\nA.13\nDatasheet\nDatasheet - MADLAD-400\nMotivation\n1. For what purpose was the dataset created? (Was there a specific task in mind? Was there a specific gap\nthat needed to be filled? Please provide a description.) We create MADLAD-400 as a general purpose\nmonolingual document level dataset covering 419 languages for the purpose of providing general training\ndata for multilingual NLP tasks such as MT and language modeling. One of the goals of the Language\nInclusivity Moonshot (LIM) 13 is to scale our language support to 1,000 languages and to support speech\nrecognition for 97% of the global population. A core objective associated with this goal is to open source\ndata and models. Our expectation is that releasng MADLAD-400 will foster progress on the language\nresearch, especially on medium and low resource languages. An estimate of over 1B people globally speak\nlanguages that are not covered by mainstream models at Google or externally.\n2. Who created this dataset and on behalf of which entity? Sneha Kudugunta\u2020, Isaac Caswell\u22c4, Biao Zhang\u2020,\nXavier Garcia\u2020, Derrick Xin\u2020, Aditya Kusupati\u22c4, Romi Stella\u2020, Ankur Bapna\u2020, Orhan Firat\u2020 (\u2020Google\nDeepMind, \u22c4Google Research)\n3. Who funded the creation of the dataset? Google Research and Google DeepMind\n4. Any other comments? None\nComposition\n1. What do the instances that comprise the dataset represent? Each instance is a preprocessed web-crawled\ndocument whose language that we annotated using a LangID model described by [15]. For the sentence\nlevel version, we used a sentence-splitter to split the documents into sentences and then deduplicated the\nresulting dataset.\n2. How many instances are there in total? MADLAD-400 has 4.0B documents (100B sentences, or 2.8T tokens)\ntotal across 419 languages with the median language containing 1.7k documents (73k sentences of 1.2M\ntokens.)\n3. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a\nlarger set? (If the dataset is a sample, then what is the larger set? Is the sample representative of the larger set\n(e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified. If it is\nnot representative of the larger set, please describe why not (e.g., to cover a more diverse range of instances,\nbecause instances were withheld or unavailable).) MADLAD-400 are created from CommonCrawl documents\nthat have been annotated by language, filtered and preprocessed. To maintain high precision, we filtered\nout data aggressively, and may not have captured every document of a given language in CommonCrawl.\nMoreover, there may also be languages in CommonCrawl that we may not have mined.\n4. What data does each instance consist of? Each instance is raw text in either document form for the document\nlevel data, or in sentence form for the sentence level data.\n5. Is there a label or target associated with each instance? If so, please provide a description. No.\n6. Is any information missing from individual instances? No.\n7. Are relationships between individual instances made explicit? No.\n8. Are there recommended data splits (e.g., training, development/validation, testing)? No.\n9. Are there any errors, sources of noise, or redundancies in the dataset? While we have taken extensive care\nto audit and filter MADLAD-400, there may still be documents annotated with the wrong language or\ndocuments of low quality.\n10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets,\nother datasets)? (If it links to or relies on external resources, a) are there guarantees that they will exist, and\nremain constant, over time; b) are there official archival versions of the complete dataset (i.e., including\nthe external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g.,\nlicenses, fees) associated with any of the external resources that might apply to a future user? Please provide\ndescriptions of all external resources and any restrictions associated with them, as well as links or other\naccess points, as appropriate.) Yes\n11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal\nprivilege or by doctor-patient confidentiality, data that includes the content of individuals\u2019 non-public\ncommunications)? (If so, please provide a description.) Given that MADLAD-400 is a general web-crawled\ndataset it is possible that documents in the dataset may contain such information.\n13https://blog.google/technology/ai/ways-ai-is-scaling-helpful/\n54\n12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might\notherwise cause anxiety? (If so, please describe why.) Given that MADLAD-400 is a general web-crawled\ndataset, even after filtering, it is possible that there are documents containing offensive content, etc.\n13. Does the dataset relate to people? It is likely that some documents in MADLAD-400 contain sentences\nreferring to and describing people.\n14. Does the dataset identify any subpopulations (e.g., by age, gender)? It is likely that some documents in\nMADLAD-400 contain sentences referring to and describing people of certain subpopulations.\n15. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in\ncombination with other data) from the dataset? Yes, it is possible that their names are mentioned in certain\ndocuments.\n16. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or\nethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations;\nfinancial or health data; biometric or genetic data; forms of government identification, such as social security\nnumbers; criminal history)? Given that MADLAD-400 is a general web-crawled dataset, even after filtering,\nit is possible that there are documents containing sensitive data.\n17. Any other comments? None.\nCollection\n1. How was the data associated with each instance acquired? Each instance was acquired by performing\ntransformations on the documents in all available snapshots of CommonCrawl as of August 20, 2022.\n2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual\nhuman curation, software program, software API)? We annotated the CommonCrawl data using a LangID\nmodel trained using the procedure described by [15]. Then, we manually inspected the data and then filtered\nor preprocessed the documents to create MADLAD-400.\n3. If the dataset is a sample from a larger set, what was the sampling strategy? MADLAD-400 is a subset of\nCommonCrawl documents determined using LangID annotations and filtering/preprocessing steps.\n4. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were\nthey compensated (e.g., how much were crowdworkers paid)? For the audit, the authors inspected the dataset.\nIn some cases native speaker volunteers provided advice on the quality of the dataset.\n5. Over what timeframe was the data collected? (Does this timeframe match the creation timeframe of the data\nassociated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in\nwhich the data associated with the instances was created.) We do not annotate timestamps. The version of\nCommonCrawl that we used has webcrawls ranging from 2008 to August 2022.\n6. Were any ethical review processes conducted (e.g., by an institutional review board)? No.\n7. Does the dataset relate to people? It is likely that some documents in MADLAD-400 contain sentences\nreferring to and describing people.\n8. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources\n(e.g., websites)? We collected this data via webpages crawled by CommonCrawl.\n9. Were the individuals in question notified about the data collection? No.\n10. Did the individuals in question consent to the collection and use of their data? No.\n11. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent\nin the future or for certain uses? No.\n12. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection\nimpact analysis) been conducted? No.\n13. Any other comments? None.\nPreprocessing/cleaning/labeling\n1. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization,\npart-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? (If so,\nplease provide a description. If not, you may skip the remainder of the questions in this section.) Various\ntypes of preprocessing were done: deduplication of 3 sentence spans, filtering substrings according to various\nheuristics associated with low quality, Virama encoding correction, converting Zawgyi encoding to Unicode\nencoding for Myanmar script characters and a Chinese pornographic content filter heuristic. In addition, 79\nannotated language datasets were removed on inspection due to low quality or mislabeling.\n2. Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated\nfuture uses)? No, this raw data is hosted by CommonCrawl.\n55\n3. Is the software used to preprocess/clean/label the instances available? As of June 13, 2023, no.\n4. Any other comments? None.\nUses\n1. Has the dataset been used for any tasks already? (If so, please provide a description.) MADLAD-400 has\nbeen used for MT and language modeling.\n2. Is there a repository that links to any or all papers or systems that use the dataset? No\n3. What (other) tasks could the dataset be used for? This dataset could be used as a general training dataset for\nany of the languages in MADLAD-400.\n4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/la-\nbeled that might impact future uses? (For example, is there anything that a future user might need to know to\navoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service\nissues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description. Is\nthere anything a future user could do to mitigate these undesirable harms?) While steps have been taken\nto clean MADLAD-400, content containing sensitive content about individuals or groups could affect the\nperformance of some downstream NLP tasks. Moreover, while building applications for (a) given language(s),\nwe urge practitioners to assess the suitability of MADLAD-400 for their usecase.\n5. Are there tasks for which the dataset should not be used? (If so, please provide a description.) N/A.\n6. Any other comments? None.\nDistribution\n1. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? MADLAD-400 is made\navailable through a GCP bucket.\n2. When will the dataset be distributed? June 2023\n3. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under\napplicable terms of use (ToU)? (If so, please describe this license and/or ToU, and provide a link or other\naccess point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated\nwith these restrictions.) AI2 has made a version of this data available under the ODC-BY license. Users are\nalso bound by the CommonCrawl terms of use in respect of the content contained in the dataset.\n4. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? Users\nare bound by the CommonCrawl terms of use in respect of the content contained in the dataset.\n5. Any other comments? None.\nMaintenance\n1. Who is supporting/hosting/maintaining the dataset? An external organization, AI2 is hosting the dataset.\n2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Sneha Kudugunta\nsnehakudugunta@google.com or Isaac Caswell (icaswell@google.com) for questions about the dataset\ncontents, or Dirk Groeneveld dirkg@allenai.org for questions related to the hosting of the dataset.\n3. Is there an erratum?\n(If so, please provide a link or other access point.)\nhttps://github.com/\ngoogle-research/google-research/tree/master/madlad_400\n4. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances\u2019)? (If so, please\ndescribe how often, by whom, and how updates will be communicated to users (e.g., mailing list, GitHub)?)\nThere are no such plans, but major issues may be corrected when reported through email or the Github page\n(https: // github. com/ google-research/ google-research/ tree/ master/ madlad_ 400 ).\n5. If the dataset relates to people, are there applicable limits on the retention of the data associated with the\ninstances (e.g., were individuals in question told that their data would be retained for a fixed period of time\nand then deleted)? (If so, please describe these limits and explain how they will be enforced.) N/A\n6. Will older versions of the dataset continue to be supported/hosted/maintained? (If so, please describe how. If\nnot, please describe how its obsolescence will be communicated to users.) No\n7. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\n(If so, please provide a description. Will these contributions be validated/verified? If so, please describe how.\nIf not, why not? Is there a process for communicating/distributing these contributions to other users? If so,\nplease provide a description.) A relatively unprocessed version of MADLAD-400, MADLAD-400-noisy is\nmade available for others to build upon using superior cleaning/preprocessing techniques for their specific\nusecases.\n8. Any other comments? None\n56\nA.14\nModel Card\nModel Card\nModel Details\n\u2022 Person or organization developing model: Google DeepMind and Google Research\n\u2022 Model Date: June 13, 2023\n\u2022 Model Types: Machine Translation and Language Modeling Models.\n\u2022 Information about training algorithms, parameters, fairness constraints or other applied approaches, and\nfeatures: Provided in the paper.\n\u2022 Paper: Kudugunta et al, MADLAD-400: Monolingual And Document-Level Large Audited Dataset, Under\nReview, 2023\n\u2022 License: ODC-BY\n\u2022 Contact: snehakudugunta@google.com\nIntended Use\n\u2022 Primary intended uses: Machine Translation and multilingual NLP tasks on over 400 languages.\n\u2022 Primary intended users: Research community.\n\u2022 Out-of-scope use cases: These models are trained on general domain data and are therefore not meant to\nwork on domain-specific models out-of-the box. Moreover, these research models have not been assessed\nfor production usecases.\nFactors\n\u2022 The translation quality of this model varies based on language, as seen in the paper, and likely varies on\ndomain, though we have not assessed this.\nMetrics\n\u2022 We use SacreBLEU and chrF, two widely used machine translation evaluation metrics for our evaluations.\nEthical Considerations\n\u2022 We trained these models with MADLAD-400 and publicly available data to create baseline models that\nsupport NLP for over 400 languages, with a focus on languages underrepresented in large-scale corpora.\nGiven that these models were trained with web-crawled datasets that may contain sensitive, offensive or\notherwise low-quality content despite extensive preprocessing, it is still possible that these issues to the\nunderlying training data may cause differences in model performance and toxic (or otherwise problematic)\noutput for certain domains. Moreover, large models are dual use technologies that have specific risks\nassociated with their use and development. We point the reader to surveys such as those written by\nWeidinger et al. [67] or Bommasani et al. [11] for a more detailed discussion of these risks, and to Liebling\net al. [46] for a thorough discussion of the risks of machine translation systems.\nTraining Data\n\u2022 For both the machine translation and language model, MADLAD-400 is used. For the machine translation\nmodel, a combination of parallel datasources covering 157 languages is also used. Further details are\ndescribed in the paper.\nEvaluation Data\n\u2022 For evaluation, we used WMT, NTREX, Flores-200 and Gatones datasets as described in Section 4.3.\nCaveats and Recommendations\n\u2022 We note that we evaluate on only 204 of the languages supported by these models and on machine translation\nand few-shot machine translation tasks. Users must consider use of this model carefully for their own\nusecase.\n57\nCanaries Datasheet\nMotivation\n1. For what purpose was the dataset created? (Was there a specific task in mind? Was there a specific gap that\nneeded to be filled? Please provide a description.) We create these canaries with the goal of enabling the\nstudy of memorization in the multilingual and translate settings. Models can be trained on these canaries\nand then their risk of memorization assessed.\n2. Who created this dataset and on behalf of which entity? Christopher A. Choquette-Choo\u2020, Katherine Lee\u2020\n(\u2020Google DeepMind)\n3. Who funded the creation of the dataset? Google DeepMind\n4. Any other comments? None\nComposition\n1. What do the instances that comprise the dataset represent? Each instance is constructed from a MADLAD-400\nsentence-level example. Careful modifications are used, e.g., shuffling the tokens, to make the sample outlier\nto the natural distribution.\n2. How many instances are there in total? There are 1, 945, 631 in total.\n3. Does the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a\nlarger set? (If the dataset is a sample, then what is the larger set? Is the sample representative of the larger\nset (e.g., geographic coverage)? If so, please describe how this representativeness was validated/verified.\nIf it is not representative of the larger set, please describe why not (e.g., to cover a more diverse range of\ninstances, because instances were withheld or unavailable).) The canary dataset itself is a subsampling from\nMADLAD-400. However, all described canaries are included in the release.\n4. What data does each instance consist of? Each instance consists of the original text as well as the modified\ninstance in tokens.\n5. Is there a label or target associated with each instance? If so, please provide a description. No.\n6. Is any information missing from individual instances? No.\n7. Are relationships between individual instances made explicit? No.\n8. Are there recommended data splits (e.g., training, development/validation, testing)? No.\n9. Are there any errors, sources of noise, or redundancies in the dataset? Some canaries are duplicated for the\npurposes of studying repetition in memorization.\n10. Is the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets,\nother datasets)? (If it links to or relies on external resources, a) are there guarantees that they will exist, and\nremain constant, over time; b) are there official archival versions of the complete dataset (i.e., including\nthe external resources as they existed at the time the dataset was created); c) are there any restrictions (e.g.,\nlicenses, fees) associated with any of the external resources that might apply to a future user? Please provide\ndescriptions of all external resources and any restrictions associated with them, as well as links or other\naccess points, as appropriate.) Yes\n11. Does the dataset contain data that might be considered confidential (e.g., data that is protected by legal\nprivilege or by doctor-patient confidentiality, data that includes the content of individuals\u2019 non-public\ncommunications)? (If so, please provide a description.) This may be possible given the underlying MADLAD-\n400 may contain such data. However, the modifications used for generating canaries reduce the chance of\nthis.\n12. Does the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might\notherwise cause anxiety? (If so, please describe why.) This may be possible given the underlying MADLAD-\n400 may contain such data. However, the modifications used for generating canaries reduce the chance of\nthis.\n13. Does the dataset relate to people? This may be possible given the underlying MADLAD-400 may contain\nsuch data. However, the modifications used for generating canaries reduce the chance of this.\n14. Does the dataset identify any subpopulations (e.g., by age, gender)? This may be possible given the underlying\nMADLAD-400 may contain such data. However, the modifications used for generating canaries reduce the\nchance of this.\n15. Is it possible to identify individuals (i.e., one or more natural persons), either directly or indirectly (i.e., in\ncombination with other data) from the dataset? This may be possible given the underlying MADLAD-400\nmay contain such data. However, the modifications used for generating canaries reduce the chance of this.\n58\n16. Does the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or\nethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations;\nfinancial or health data; biometric or genetic data; forms of government identification, such as social security\nnumbers; criminal history)? This may be possible given the underlying MADLAD-400 may contain such\ndata. However, the modifications used for generating canaries reduce the chance of this.\n17. Any other comments? None.\nCollection\n1. How was the data associated with each instance acquired? Each instance was acquired by performing\ntransformations on the documents in all available snapshots of CommonCrawl as of August 20, 2022.\n2. What mechanisms or procedures were used to collect the data (e.g., hardware apparatus or sensor, manual\nhuman curation, software program, software API)? We randomly subsampled MADLAD-400 and then applied\nrandom\n3. If the dataset is a sample from a larger set, what was the sampling strategy? A predefined number of sentences\nwere uniformly sampled from each language.\n4. Who was involved in the data collection process (e.g., students, crowdworkers, contractors) and how were\nthey compensated (e.g., how much were crowdworkers paid)? The authors created the canary dataset.\n5. Over what timeframe was the data collected? (Does this timeframe match the creation timeframe of the data\nassociated with the instances (e.g., recent crawl of old news articles)? If not, please describe the timeframe in\nwhich the data associated with the instances was created.) We do not annotate timestamps.\n6. Were any ethical review processes conducted (e.g., by an institutional review board)? No.\n7. Does the dataset relate to people? This may be possible given the underlying MADLAD-400 may contain\nsuch data. However, the modifications used for generating canaries reduce the chance of this.\n8. Did you collect the data from the individuals in question directly, or obtain it via third parties or other sources\n(e.g., websites)? Obtained second-hand through MADLAD-400, a CommonCrawl based dataset.\n9. Were the individuals in question notified about the data collection? No.\n10. Did the individuals in question consent to the collection and use of their data? No.\n11. If consent was obtained, were the consenting individuals provided with a mechanism to revoke their consent\nin the future or for certain uses? No.\n12. Has an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data protection\nimpact analysis) been conducted? No.\n13. Any other comments? None.\nPreprocessing/cleaning/labeling\n1. Was any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing, tokenization,\npart-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values)? (If so,\nplease provide a description. If not, you may skip the remainder of the questions in this section.) Various\ntypes of processing were applied on top of MADLAD-400. Sentences were either interleaved in batches of\n20 \u2212 50 tokens or shuffled at the token-level.\n2. Was the \"raw\" data saved in addition to the preprocessed/cleaned/labeled data (e.g., to support unanticipated\nfuture uses)? The raw MADLAD-400 data is saved.\n3. Is the software used to preprocess/clean/label the instances available? As of June 13, 2023, no.\n4. Any other comments? None.\nUses\n1. Has the dataset been used for any tasks already? (If so, please provide a description.) MADLAD-400 has\nbeen used for MT.\n2. Is there a repository that links to any or all papers or systems that use the dataset? No\n3. What (other) tasks could the dataset be used for? This dataset can be used to study memorization in broad\nlanguage modelling scenarios.\n4. Is there anything about the composition of the dataset or the way it was collected and preprocessed/cleaned/la-\nbeled that might impact future uses? (For example, is there anything that a future user might need to know to\navoid uses that could result in unfair treatment of individuals or groups (e.g., stereotyping, quality of service\nissues) or other undesirable harms (e.g., financial harms, legal risks) If so, please provide a description.\nIs there anything a future user could do to mitigate these undesirable harms?) We urge users to read the\ndatasheet for MADLAD-400 to understand the underlying risk for the canaries.\n59\n5. Are there tasks for which the dataset should not be used? (If so, please provide a description.) N/A.\n6. Any other comments? None.\nDistribution\n1. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? MADLAD-400 is made\navailable through a GCP bucket.\n2. When will the dataset be distributed? June 2023\n3. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under\napplicable terms of use (ToU)? (If so, please describe this license and/or ToU, and provide a link or other\naccess point to, or otherwise reproduce, any relevant licensing terms or ToU, as well as any fees associated\nwith these restrictions.) AI2 has made a version of this data available under the ODC-BY license. Users are\nalso bound by the CommonCrawl terms of use in respect of the content contained in the dataset.\n4. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? Users\nare bound by the CommonCrawl terms of use in respect of the content contained in the dataset.\n5. Any other comments? None.\nMaintenance\n1. Who is supporting/hosting/maintaining the dataset? An external organization, AI2 is hosting the dataset.\n2. How can the owner/curator/manager of the dataset be contacted (e.g., email address)? Christopher A.\nChoquette-Choo cchoquette@google.com for questions about the dataset contents or Dirk Groeneveld\ndirkg@allenai.org for questions related to the hosting of the dataset.\n3. Is there an erratum? (If so, please provide a link or other access point.) No\n4. Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances\u2019)? (If so,\nplease describe how often, by whom, and how updates will be communicated to users (e.g., mailing list,\nGitHub)?) There are no such plans, but major issues may be corrected when reported through email or the\nGithub page.\n5. If the dataset relates to people, are there applicable limits on the retention of the data associated with the\ninstances (e.g., were individuals in question told that their data would be retained for a fixed period of time\nand then deleted)? (If so, please describe these limits and explain how they will be enforced.) N/A\n6. Will older versions of the dataset continue to be supported/hosted/maintained? (If so, please describe how. If\nnot, please describe how its obsolescence will be communicated to users.) No\n7. If others want to extend/augment/build on/contribute to the dataset, is there a mechanism for them to do so?\n(If so, please provide a description. Will these contributions be validated/verified? If so, please describe\nhow. If not, why not? Is there a process for communicating/distributing these contributions to other users?\nIf so, please provide a description.) Others may build upon this by similarly generating canaries from\nMADLAD-400 which is made available.\n8. Any other comments? None\n60\n"
  },
  {
    "title": "Neurons in Large Language Models: Dead, N-gram, Positional",
    "link": "https://arxiv.org/pdf/2309.04827.pdf",
    "upvote": "15",
    "text": "Neurons in Large Language Models: Dead, N-gram, Positional\nElena Voita1\nJavier Ferrando2\u2217\nChristoforos Nalmpantis1\n1Meta AI\n2TALP Research Center, Universitat Polit\u00e8cnica de Catalunya\n{lenavoita, christoforos}@meta.com, javier.ferrando.monsonis@upc.edu\nAbstract\nWe analyze a family of large language models\nin such a lightweight manner that can be done\non a single GPU. Specifically, we focus on the\nOPT family of models ranging from 125m to\n66b parameters and rely only on whether an\nFFN neuron is activated or not. First, we find\nthat the early part of the network is sparse and\nrepresents many discrete features. Here, many\nneurons (more than 70% in some layers of the\n66b model) are \u201cdead\u201d, i.e. they never acti-\nvate on a large collection of diverse data. At\nthe same time, many of the alive neurons are\nreserved for discrete features and act as token\nand n-gram detectors. Interestingly, their corre-\nsponding FFN updates not only promote next\ntoken candidates as could be expected, but also\nexplicitly focus on removing the information\nabout triggering them tokens, i.e., current in-\nput. To the best of our knowledge, this is the\nfirst example of mechanisms specialized at re-\nmoving (rather than adding) information from\nthe residual stream. With scale, models become\nmore sparse in a sense that they have more dead\nneurons and token detectors. Finally, some neu-\nrons are positional: them being activated or not\ndepends largely (or solely) on position and less\nso (or not at all) on textual data. We find that\nsmaller models have sets of neurons acting as\nposition range indicators while larger models\noperate in a less explicit manner.\n1\nIntroduction\nThe range of capabilities of language models ex-\npands with scale and at larger scales models be-\ncome so strong and versatile that a single model can\nbe integrated into various applications and decision-\nmaking processes (Brown et al., 2020; Kaplan et al.,\n2020; Wei et al., 2022; Ouyang et al., 2022; Ope-\nnAI, 2023; Anil et al., 2023). This increases inter-\nest and importance of understanding the internal\n\u2217Work done as part of internship at Meta AI.\nworkings of these large language models (LLMs)\nand, specifically, their evolution with scale. Unfor-\ntunately, scaling also increases the entry threshold\nfor interpretability researchers since dealing with\nlarge models requires a lot of computational re-\nsources. In this work, we analyze a family of OPT\nmodels up to 66b parameters and deliberately keep\nour analysis very lightweight so that it could be\ndone using a single GPU.\nWe focus on neurons inside FFNs, i.e. individual\nactivations in the representation between the two\nlinear layers of the Transformer feedforward blocks\n(FFNs). Differently from e.g. neurons in the resid-\nual stream, FFN neurons are more likely to repre-\nsent meaningful features: the elementwise nonlin-\nearity breaks the rotational invariance of this repre-\nsentation and encourages features to align with the\nbasis dimensions (Elhage et al., 2021). When such\na neuron is activated, it updates the residual stream\nby pulling out the corresponding row of the second\nFFN layer; when it is not activated, it does not up-\ndate the residual stream (Figure 6).1 Therefore, we\ncan interpret functions of these FFN neurons in two\nways: (i) by understanding when they are activated,\nand (ii) by interpreting the corresponding updates\ncoming to the residual stream.\nFirst, we find that in the first half of the network,\nmany neurons are \u201cdead\u201d, i.e. they never activate\non a large collection of diverse data. Larger models\nare more sparse in this sense: for example, in the\n66b model more that 70% of the neurons in some\nlayers are dead. At the same time, many of the\nalive neurons in this early part of the network are\nreserved for discrete features and act as indicator\nfunctions for tokens and n-grams: they activate if\nand only if the input is a certain token or an n-gram.\nThe function of the updates coming from these to-\nken detectors to the residual stream is also very\n1Since OPT models have the ReLU activation function, the\nnotion of \u201cactivated\u201d or \u201cnot activated\u201d is trivial and means\nnon-zero vs zero.\narXiv:2309.04827v1  [cs.CL]  9 Sep 2023\nsurprising: at the same time as they promote con-\ncepts related to the potential next token candidate\n(which is to be expected according to Geva et al.\n(2021, 2022)), they are explicitly targeted at remov-\ning information about current input, i.e. their trig-\ngers. This means that in the bottom-up processing\nwhere a representation of the current input token\ngets gradually transformed into a representation for\nthe next token, current token identity is removed by\nthe model explicitly (rather than ends up implicitly\n\u201cburied\u201d as a result of additive updates useful for\nthe next token). To the best of our knowledge, this\nis the first example of mechanisms specialized at\nremoving (rather than adding) information from\nthe residual stream.\nFinally, we find that some neurons are responsi-\nble for encoding positional information regardless\nof textual patterns. Similarly to token and n-gram\ndetectors, many of these neurons act as indicator\nfunctions of position ranges, i.e. activate for po-\nsitions within certain ranges and do not activate\notherwise. Interestingly, these neurons often collab-\norate. For example, the second layer of the 125m\nmodel has 10 positional neurons whose indicated\npositional ranges are in agreement: together, they\nefficiently cover all possible positions and no neu-\nron is redundant. In a broader picture, positional\nneurons question the key-value memory view of\nthe FFN layers stating that \u201ceach key correlates\nwith textual patterns in the training data and each\nvalue induces a distribution over the output vocab-\nulary\u201d (Geva et al., 2021, 2022). Neurons that rely\non position regardless of textual pattern indicate\nthat FFN layers can be used by the model in ways\nthat do not fit the key-value memory view. Overall,\nwe argue that the roles played by these layers are\nstill poorly understood.\nOverall, we find neurons that:\n\u2022 are \u201cdead\u201d, i.e. never activate on a large di-\nverse collection of data;\n\u2022 act as token- and n-gram detectors that, in\naddition to promoting next token candidates,\nexplicitly remove current token information;\n\u2022 encode position regardless of textual content\nwhich indicates that the role of FFN layers\nextends beyond the key-value memory view.\nWith scale, models have more dead neurons and\ntoken detectors and are less focused on absolute\nposition.\n2\nData and Setting\nModels.\nWe use OPT (Zhang et al., 2022), a suite\nof decoder-only pre-trained transformers that are\npublicly available. We use model sizes ranging\nfrom 125M to 66B parameters and take model\nweights from the HuggingFace model hub.2\nData.\nWe use data from diverse sources con-\ntaining development splits of the datasets used in\nOPT training as well as several additional datasets.\nOverall, we used (i) subsets of the validation and\ntest part of the Pile (Gao et al., 2020) includ-\ning Wikipedia, DM Mathematics, HackerNews,\n(ii) Reddit3 (Baumgartner et al., 2020; Roller et al.,\n2021), (iii) code data from Codeparrot4.\nFor the experiments in Section 3 when talking\nabout dead neurons, we use several times more data.\nSpecifically, we add more data from Wikipedia,\nDM Mathematics and Codeparrot, as well as add\nnew domains from the Pile5: EuroParl, FreeLaw,\nPubMed abstracts, Stackexchange.\nOverall, the data used in Section 3 has over 20M\ntokens, in the rest of the paper \u2013 over 5M tokens.\nSingle-GPU processing.\nWe use only sets of neu-\nron values for some data, i.e. we run only forward\npasses of the full model or its several first layers.\nSince large models do not fit in a single GPU, we\nload one layer at a time keeping the rest of the\nlayers on CPU. This allows us to record neuron ac-\ntivations for large models: all the main experiments\nin this paper were done on a single GPU.\n3\nDead Neurons\nLet us start from simple statistics such as neuron\nactivation frequency (Figure 1).\nMany neurons are \u201cdead\u201d.\nFirst, we find that\nmany neurons never activate on our diverse data, i.e.\nthey can be seen as \u201cdead\u201d. Figure 1a shows that\nthe proportion of dead neurons is very substantial:\ne.g., for the 66b model, the proportion of dead\nneurons in some layers is above 70%. We also see\nthat larger models are more sparse because (i) they\n2https://huggingface.co/models\n3Pushshift.io Reddit dataset is a previously existing dataset\nextracted and obtained by a third party that contains prepro-\ncessed comments posted on the social network Reddit and\nhosted by pushshift.io.\n4https://huggingface.co/datasets/codeparrot/\ncodeparrot-clean\n5https://huggingface.co/datasets/EleutherAI/\npile\n(a)\n(b)\nFigure 1: (a) Percentage of \u201cdead\u201d neurons; (b) average\nneuron activation frequency among non-dead neurons.\nhave more dead neurons and (ii) the ones that are\nalive activate less frequently (Figure 1b).\nOnly first half of the model is sparse.\nNext, we\nnotice that this kind of sparsity is specific only to\nearly layers. This leads to a clear distinction be-\ntween the first and the second halves of the network:\nwhile the first half contains a solid proportion of\ndead neurons, the second half is fully \u201calive\u201d. Ad-\nditionally, layers with most dead neurons are the\nones where alive neurons activate most rarely.\nPacking concepts into neurons.\nThis difference\nin sparsity across layers might be explained by\n\u201cconcept-to-neuron\u201d ratio being much smaller in the\nearly layers than in the higher layers. Intuitively,\nthe model has to represent sets of encoded in a\nlayer concepts by \u201cspreading\u201d them across avail-\nable neurons. In the early layers, encoded concepts\nare largely shallow and are likely to be discrete\n(e.g., lexical) while at the higher layers, networks\nlearn high-level semantics and reasoning (Peters\net al., 2018; Liu et al., 2019; Jawahar et al., 2019;\nTenney et al., 2019; Geva et al., 2021). Since the\nnumber of possible shallow patterns is not large\nand, potentially, enumerable, in the early layers the\nmodel can (and, as we will see later, does) assign\ndedicated neurons to some features. The more neu-\nrons are available to the model, the easier it is to do\nso \u2013 this agrees with the results in Figure 1 show-\ning that larger models are more sparse. Differently,\nthe space of fine-grained semantic concepts is too\nlarge compared to the number of available neurons\nwhich makes it hard to reserve many dedicated\nneuron-concept pairs.6\nAre dead neurons completely dead?\nNote that\nthe results in Figure 1a can mean one of the two\n6There can, however, be a few specialized neurons in the\nhigher layers. For example, BERT has neurons responsible\nfor relational facts (Dai et al., 2022).\nFigure 2: Neurons categorized by the number of uni-\ngrams (i.e., tokens) able to trigger them. First half of\nthe network, alive neurons only.\nthings: (i) these neurons can never be activated (i.e.\nthey are \u201ccompletely dead\u201d) or (ii) they correspond\nto patterns so rare that we never encountered them\nin our large diverse collection of data. While the\nlatter is possible, note that this does not change\nthe above discussion about sparsity and types of\nencoded concepts. On the contrary: it further sup-\nports the hypothesis of models assigning dedicated\nneurons to specific concepts.\n4\nN-gram-Detecting Neurons\nNow, let us look more closely into the patterns en-\ncoded in the lower half of the models and try to un-\nderstand the nature of the observed above sparsity.\nSpecifically, we analyze how neuron activations de-\npend on an input n-gram. For each input text with\ntokens x1, x2, ..., xS, we record neuron activations\nat each position and if a neuron is activated (i.e.,\nnon-zero) at position k, we say that the n-gram\n(xk\u2212n+1, . . . , xk) triggered this neuron.\nIn Sections 4.1-4.4 we talk about unigrams (i.e.,\ntokens) and come to larger n-grams in Section 4.5.\n4.1\nNumber of N-grams Triggering a Neuron\nFirst, let us see how many n-grams are able to\ntrigger each neuron. For each neuron we evaluate\nthe number of n-grams that cover at least 95% of\nthe neuron\u2019s activations. For the bottom half of\nthe network, Figure 2 shows how neurons in each\nlayer are categorized by the number of covering\nthem n-grams (we show unigrams here and larger\n(a)\n(b)\nFigure 3:\n(a) Number of token-detecting neurons;\n(b) number of tokens that have a detecting them neuron:\nsolid line \u2013 per layer, dashed \u2013 cumulative over layers.\nn-grams in Appendix A).\nWe see that, as anticipated, neurons in larger\nmodels are covered by less n-grams. Also, the\nlargest models have a substantial proportion of neu-\nrons that are covered by as few as 1 to 5 tokens.\nThis agrees with our hypothesis in the previous sec-\ntion: the model spreads discreet shallow patterns\nacross specifically dedicated neurons.7\n4.2\nToken-Detecting Neurons\nPresence of neurons that can be triggered by only\na few (e.g., 1-5) tokens point to the possibility that\nsome neurons act as token detectors, i.e. activate\nif and only if the input is one of the corresponding\ntokens, regardless of the previous context. To find\nsuch neurons, we (1) pick neurons that can be trig-\ngered by only 1-5 tokens, (2) gather tokens that are\ncovered by this neuron (if the neuron activates at\nleast 95% of the time the token is present), (3) if\naltogether, these covered tokens are responsible for\nat least 95% of neuron activations.8\nFigure 3a shows that there are indeed a lot of\ntoken-detecting neurons. As expected, larger mod-\nels have more such neurons and the 66b model has\noverall 5351 token detectors. Note that each token\ndetector is responsible for a group of several to-\nkens that, in most of the cases, are variants of the\nsame word (e.g., with differences only in capital-\nization, presence of the space-before-word special\nsymbol, morphological form, etc.). Figure 5 (top)\nshows examples of groups of tokens detected by\ntoken-detecting neurons.\nInterestingly, the behavior of the largest models\n(starting from 13b of parameters) differs from that\n7Note that the 350m model does not follow the same pat-\ntern as all the rest: we will discuss this model in Section 6.\n8We exclude the begin-of-sentence token from these com-\nputations because for many neurons, this token is responsible\nfor the majority of the activations.\nFigure 4: Number of tokens covered in each layer with\nindicated (i) new overall, and (ii) new compared to the\nprevious layer tokens.\nof the rest. While for smaller models the number\nof token detectors increases then goes down, larger\nmodels operate in three monotonic stages and start\nhaving many token-detecting neurons from the very\nfirst layer (Figures 3). This already shows quali-\ntative differences between the models: with more\ncapacity, larger models perform more complicated\nreasoning with more distinct stages.\n4.3\nEnsemble-Like Behaviour of the Layers\nNow, let us look at \u201cdetected\u201d tokens, i.e. tokens\nthat have a specialized detecting them neuron. Fig-\nure 3b shows the number of detected tokens in\neach layer as well as cumulative over layers num-\nber of detected tokens. We see that, e.g., the 66b\nmodel focuses on no more than 1.5k tokens in each\nlayer but over 10k tokens overall. This means that\nacross layers, token-detecting neurons are respon-\nsible for largely differing tokens. Indeed, Figure 4\nshows that in each following layer, detected tokens\nmostly differ from all the tokens covered by the\nlayers below. All in all, this points to an ensemble-\nlike (as opposed to sequential) behavior of the lay-\ners: layers collaborate so that token-detecting neu-\nrons cover largely different tokens in different lay-\ners. This divide-and-conquer-style strategy allows\nlarger models to cover many tokens overall and use\ntheir capacity more effectively.\nOriginally, such an ensemble-like behavior of\ndeep residual networks was observed in computer\nvision models (Veit et al., 2016). For transform-\ners, previous evidence includes simple experiments\nshowing that e.g. dropping or reordering layers\ndoes not influence performance much (Fan et al.,\n2020; Zhao et al., 2021).\n4.4\nToken Detectors Suppress Their Triggers\nNow let us try to understand the role of token-\ndetecting neurons in the model by interpreting how\nFigure 5: Examples of the top promoted and suppressed tokens for token-detecting neurons ( \u02d9G is a special symbol\ndenoting the space before word \u2013 in the OPT tokenizers, it is part of a word); OPT-66b model.\nFigure 6: Intuition behind concept suppression: we look\nnot only at the top projections of an FFN update on\nvocabulary but also at the bottom. The concepts that are\nadded with a negative value are suppressed.\nthey update the residual stream. Throughout the\nlayers, token representation in the residual stream\ngets transformed from the token embedding for\nthe current input token9 to the representation that\nencodes a distribution for the next token. This\ntransformation happens via additive updates com-\ning from attention and FFN blocks in each layer.\nWhenever an FFN neuron is activated, the corre-\nsponding row of the second FFN layer (multiplied\nby this neuron\u2019s value) is added to the residual\nstream (see illustration in Figure 6). By project-\ning this FFN row onto vocabulary, we can get an\ninterpretation of this update (and, thus, the role of\nthis neuron) in terms of its influence on the output\ndistribution encoded in the residual stream.\n9For OPT models, along with an absolute positional em-\nbedding.\nCurrent token suppression: implicit or explicit?\nPreviously, this influence was understood only in\nterms of the top projections, i.e. tokens that are\npromoted (Geva et al., 2021, 2022). This reflects\nan existing view supporting implicit rather than\nexplicit loss of the current token identity over the\ncourse of layers. Namely, the view that the current\nidentity gets \u201cburied\u201d as a result of updates useful\nfor the next token as opposed to being removed\nby the model explicitly. In contrast, we look not\nonly at the top projections but also at the bottom:\nif these projections are negative, the corresponding\ntokens are suppressed by the model (Figure 6).\nExplicit token suppression in the model.\nWe\nfind that often token-detecting neurons deliberately\nsuppress the tokens they detect. Figure 5 shows\nseveral examples of token-detecting neurons along\nwith the top promoted and suppressed concepts.\nWhile the top promoted concepts are in line with\nprevious work (they are potential next token candi-\ndates which agrees with Geva et al. (2021, 2022)),\nthe top suppressed concepts are rather unexpected:\nthey are exactly the tokens triggering this neuron.\nThis means that vector updates corresponding to\nthese neurons point in the direction of the next to-\nken candidates at the same time as they point away\nfrom the tokens triggering the neuron. Note that\nthis is not trivial since these updates play two very\ndifferent roles at the same time. Overall, for over\n80% of token-detecting neurons their correspond-\ning updates point in the negative direction from\nthe triggering them tokens (although, the triggering\ntokens are not always at the very top suppressed\nconcepts as in the examples in Figure 6).\nOverall, we argue that models can have mech-\nFigure 7: Types of positional neurons. Top row \u2013 \u201cstrong\u201d pattern, bottom row \u2013 \u201cweak\u201d pattern.\nanisms that are targeted at removing information\nfrom the residual stream which can be explored\nfurther in future work.\n4.5\nBeyond Unigrams\nIn Appendix A, we show results for bigrams and\ntrigrams that mirror our observations for unigrams:\n(i) larger models have more specialized neurons,\n(ii) in each layer, models cover mostly new n-grams.\nInterestingly, for larger n-grams we see a more\ndrastic gap between larger and smaller models.\n5\nPositional Neurons\nWhen analyzing dead neurons (Section 3), we also\nnoticed some neurons that, consistently across di-\nverse data, never activate except for a few first to-\nken positions. This motivates us to look further into\nhow position is encoded in the model and, specif-\nically, whether some neurons are responsible for\nencoding positional information.\n5.1\nIdentifying Positional Neurons\nIntuitively, we want to find neurons whose activa-\ntion patterns are defined by or, at least, strongly\ndepend on token position. Formally, we identify\nneurons whose activations have high mutual infor-\nmation with position. For each neuron, we evaluate\nmutual information between two random variables:\n\u2022 act \u2013 neuron is activated or not ({Y, N}),\n\u2022 pos \u2013 token position ({1, 2, . . . , T}).\nFormal setting.\nWe gather neuron activations\nfor full-length data (i.e., T = 2048 tokens) for\nWikipedia, DM Mathematics and Codeparrot. Let\nfr(pos)\nn\nbe activation frequency of neuron n at posi-\ntion pos and frn be the total activation frequency\nof this neuron. Then the desired mutual informa-\ntion is as follows:10\nI(act, pos) = 1\nT \u00b7\nT\nX\npos=1\n\u0014\nfr(pos)\nn\n\u00b7 log fr(pos)\nn\nfrn\n+\n(1 \u2212 fr(pos)\nn\n) \u00b7 log 1 \u2212 fr(pos)\nn\n1 \u2212 frn\n\u0015\n.\nChoosing the neurons.\nWe pick neurons with\nI(act, pos) > 0.05, i.e. high mutual information\nwith position \u2013 this gives neurons whose activation\nfrequency depends on position rather than content.\nIndeed, if e.g. a neuron is always activated within\ncertain position range regardless of data domain,\nwe can treat this neuron as responsible for position;\nat least, to a certain extent.\n5.2\nTypes of Positional Neurons\nAfter selecting positional neurons, we categorize\nthem according to their activation pattern, i.e. acti-\nvation frequency depending on position (Figure 7).\nOscillatory.\nThese neurons are shown in purple\nin Figure 7. When such a pattern is strong (top\nrow), the activation pattern is an indicator function\nof position ranges. In other words, such a neuron\nis activated if and only if the position falls into a\ncertain set. Note that since the activation pattern\ndoes not change across data domains, it is defined\nsolely by position and not the presence of some\nlexical or semantic information.\nBoth types of activation extremes.\nThese are the\nneurons whose activation pattern is not oscillatory\nbut still has intervals where activation frequency\nreaches both \u201cactivation extremes\u201d: 0 (never acti-\nvated) and 1 (always activated). Most frequently,\nsuch a neuron is activated only for positions less\nthan or greater than some value and not activated\notherwise. Similarly to oscillatory neurons, when\n10For more details, see appendix B.1.\nFigure 8: Positional neurons in each of the models. Each circle corresponds to a single neuron, colors and their\nintensity correspond to the types of patterns shown in Figure 7.\nsuch a pattern is strong (Figure 7, top row), it is\nalso (almost) an indicator function.\nOnly one type of activation extremes.\nDiffer-\nently from the previous two types, activation pat-\nterns for these neurons can reach only one of the\nextreme values 0 or 1 (Figure 7, green). While this\nmeans that they never behave as indicator functions,\nthere are position ranges where a neuron being ac-\ntivated or not depends solely on token position.\nOther.\nFinally, these are the neurons whose ac-\ntivation patterns strongly depend on position but\ndo not have intervals where activation frequency\nstays 0 or 1 (Figure 7, yellow). Typically, these\nactivation patterns have lower mutual information\nwith position than the previous three types.\nStrong vs weak pattern.\nWe also distinguish\n\u201cstrong\u201d and \u201cweak\u201d versions of each type which\nwe will further denote with color intensity (Fig-\nure 7, top vs bottom rows). For the first three\ntypes of positional neurons, the difference between\nstrong and weak patterns lies in whether on the\ncorresponding position ranges activation frequency\nequals 0 (or 1) or close, but not equals, to 0 (or 1).\nFor the last type, this difference lies in how well\nwe can predict activation frequency on a certain\nposition knowing this value for the neighboring\npositions (informally, \u201cthin\u201d vs \u201cthick\u201d graph).\n5.3\nPositional Neurons Across the Models\nFor each of the models, Figure 8 illustrates the\npositional neurons across layers.\nSmall models encode position more explicitly.\nFirst, we notice that smaller models rely substan-\ntially on oscillatory neurons: this is the most fre-\nquent type of positional neurons for models smaller\nthan 6.7b of parameters. In combination with many\n\u201cred\u201d neurons acting as indicator functions for wider\nposition ranges, the model is able to derive token\u2019s\nabsolute position rather accurately. Interestingly,\nlarger models do not have oscillatory neurons and\nrely on more generic patterns shown with red- and\ngreen-colored circles. We can also see that from\n13b to 66b, the model loses two-sided red neu-\nrons and uses the one-sided green ones more. This\nhints at one of the qualitative differences between\nsmaller and larger models: while the former en-\ncode absolute position more accurately, the latter\nones are likely to rely on something more meaning-\nful than absolute position. This complements re-\ncent work showing that absolute position encoding\nis harmful for length generalization in reasoning\ntasks (Kazemnejad et al., 2023). Differently from\ntheir experiments with same model size but vari-\nous positional encodings, we track changes with\nscale. We see that, despite all models being trained\nwith absolute positional encodings, stronger mod-\nels tend to abstract away from absolute position.\nPositional neurons work in teams.\nInterestingly,\npositional neurons seem to collaborate to cover the\nfull set of positions together. For example, let us\nlook more closely at the 10 strongly oscillatory neu-\nrons in the second layer of the 125m model (shown\nwith dark purple circles in Figure 8). Since they act\nas indicator functions, we can plot position ranges\nFigure 9: Position ranges indicated by strong oscillatory\nneurons in the second layer of the 125m model.\nindicated by each of these neurons. Figure 9 shows\nthat (i) indicated position ranges for these neurons\nare similar up to a shift, (ii) the shifts are organized\nin a \u201cperfect\u201d order in a sense that altogether, these\nten neurons efficiently cover all positions such that\nnone of these neurons is redundant.\nThe two stages within the model.\nFinally, Fig-\nure 8 reveals two stages of up-and-downs of posi-\ntional information within the model: roughly, the\nfirst third of the model and the rest. Interestingly,\npreferences in positional patterns also change be-\ntween the stages: e.g., preference for \u201cred\u201d neurons\nchanges to oscillatory purple patterns for the 1.3b\nand 2.7b models, and \u201cred\u201d patterns become less\nimportant in the upper stage for the 13b and 30b\nmodels. Note that the first third of the model cor-\nresponds to the sparse stage with the dead neurons\nand n-gram detectors (Sections 3, 4). Therefore, we\ncan hypothesize that in these two stages, positional\ninformation is first used locally to detect shallow\npatterns, and then more globally to use longer con-\ntexts and help encode semantic information.\nPreviously, the distinct bottom-up stages of pro-\ncessing inside language models were observed in\nVoita et al. (2019a). The authors explained that\nthe way representations gain and lose information\nthroughout the layers is defined by the training ob-\njective and why, among other things, positional\ninformation should (and does) get lost. This agrees\nwith our results in this work: we can see that while\nthere are many positional patterns in the second\nstage, they are weaker than in the first stage.\n5.4\nPositional Neurons are Learned Even\nWithout Positional Encoding\nRecently, it turned out that even without positional\nencoding, autoregressive language models still\nlearn positional information (Haviv et al., 2022).\nWe hypothesize that the mechanism these \u201cNoPos\u201d\nmodels use to encode position is positional neurons.\nTo confirm this, we train two versions of the 125m\nmodel, with and without positional encodings, and\ncompare the types of their positional neurons.\nSetup.\nWe trained 125m models with the stan-\ndard OPT setup but smaller training dataset: we\nused OpenWebText corpus (Gokaslan and Co-\nhen, 2019), an open clone of the GPT-2 training\ndata (Radford et al., 2019). This dataset contains\n3B tokens (compared 180B for OPT).\nPositional neurons without positional encoding.\nFigure 10 shows positional neurons in two 125m\nmodels: trained with and without positional encod-\ning. We see that, indeed, the model without po-\nsitional encoding also has many strong positional\npatterns. Note, however, that the NoPos model does\nnot have oscillatory neurons which, in combination\nwith other positional neurons, allow encoding ab-\nsolute position rather accurately. This means that\nthe NoPos model relies on more generic patterns,\ne.g. \u201cred\u201d neurons encoding whether a position is\ngreater/less than some value.\nOscillatory neurons require longer training.\nFinally, we found that oscillatory patterns appear\nonly with long training. Figure 11 shows posi-\ntional patterns learned by the baseline 125m model\ntrained for 50k, 150k and 300k training batches.\nWe see that all models have very strong positional\npatterns, but only the last of them has oscillatory\nneurons. Apparently, learning absolute position\nrequires longer training time.\n5.5\nDoubting FFNs as Key-Value Memories\nCurrent widely held belief is that feed-forward lay-\ners in transformer-based language models operate\nas key-value memories. Specifically, \u201ceach key\ncorrelates with textual patterns in the training ex-\namples, and each value induces a distribution over\nthe output vocabulary\u201d (Geva et al. (2021, 2022);\nDai et al. (2022); Meng et al. (2022); Ferrando et al.\n(2023), among others). While in Section 4.4 we\nconfirmed that this is true for some of the neurons,\nresults in this section reveal that FFN layers can\nbe used by the model in ways that do not fit the\nkey-value memory view. In particular, activations\nof strong positional neurons are defined by position\nregardless of textual content, and the corresponding\nvalues do not seem to encode meaningful distribu-\ntions over vocabulary. This means that the role of\nthese neurons is different from matching textual\npatterns to sets of the next token candidates. In a\nbroader context, this means that the roles played\nFigure 10: Positional neurons in 125m models: baseline vs model without positional encoding. Both models were\ntrained for 300k batches.\nFigure 11: Positional neurons in the base 125m model\ntrained with 50k, 150k and 300k batches.\nby Transformer feed-forward layers are still poorly\nunderstood.\n6\nThe 350m Model: The Odd One Out\nAs we already mentioned above, the 350m model\ndoes not follow the same pattern as the rest of the\nmodels. Specifically, it does not have dead neurons\n(Section 3) and its neuron activations do not seem\nto be sparse with respect to triggering them n-grams\nas we saw for all the other models in Figure 2.11\nModeling bits affect interpretability.\nThis be-\ncomes less surprizing when noticing that the 350m\nmodel is implemented differently from all the\nrest: it applies LayerNorm after attention and feed-\nforward blocks, while all the other models \u2013 be-\nfore.12\nApparently, such seemingly minor im-\nplementation details can affect interpretability of\nmodel components rather significantly. Indeed, pre-\nvious work also tried choosing certain modeling\naspects to encourage interpretability. Examples of\nsuch work include choosing an activation function\nto increase the number of interpretable neurons (El-\nhage et al., 2022), large body of work on sparse\nsoftmax variants to make output distributions or\nattention more interpretable (Martins and Astudillo\n(2016); Niculae and Blondel (2017); Peters et al.\n11There are, however, positional neurons; see Figure 16 in\nAppendix B.2).\n12https://github.com/huggingface/transformers/\nblob/main/src/transformers/models/opt/modeling_\nopt.py\n(2019); Correia et al. (2019); Martins et al. (2020),\namong others), or more extreme approaches with\nexplicit modular structure that is aimed to be in-\nterpretable by construction (Andreas et al. (2016);\nHu et al. (2018); Kirsch et al. (2018); Khot et al.\n(2021), to name a few). Intuitively, choosing ReLU\nactivation function as done in the OPT models can\nbe seen as having the same motivation as devel-\noping sparse softmax variants: exact zeros in the\nmodel are inherently interpretable.\n7\nAdditional Related Work\nHistorically, neurons have been a basic unit of anal-\nysis. Early works started from convolutional net-\nworks first for images (Krizhevsky et al., 2012)\nand later for convolutional text classifiers (Jacovi\net al., 2018). Similar to our work, Jacovi et al.\n(2018) also find n-gram detectors; although, for\nsmall convolutional text classifiers this is an almost\ntrivial observation compared to large Transformer-\nbased language models as in our work. For re-\ncurrent networks, interpretable neurons include\nsimple patterns such as line lengths, brackets and\nquotes (Karpathy et al., 2015), sentiment neu-\nron (Radford et al., 2017) and various neurons\nin machine translation models, such as tracking\nbrackets, quotes, etc, as well as neurons correlated\nwith higher-level concepts e.g. verb tense (Bau\net al., 2019). For Transformer-based BERT, Dai\net al. (2022) find that some neurons inside feed-\nforward blocks are responsible for storing factual\nknowledge. Larger units of analysis include at-\ntention blocks (Voita et al. (2018, 2019b); Clark\net al. (2019); Kovaleva et al. (2019); Baan et al.\n(2019); Correia et al. (2019), etc), feed-forward\nlayers (Geva et al., 2021, 2022) and circuits respon-\nsible for certain tasks (Wang et al., 2022; Geva\net al., 2023; Hanna et al., 2023).\nAcknowledgements\nThe authors thank Nicola Cancedda, Yihong Chen,\nIgor Tufanov and FAIR London team for fruitful\ndiscussions and helpful feedback.\nReferences\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Neural module networks. In Pro-\nceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR).\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin John-\nson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El\nShafey, Yanping Huang, Kathy Meier-Hellstern, Gau-\nrav Mishra, Erica Moreira, Mark Omernick, Kevin\nRobinson, Sebastian Ruder, Yi Tay, Kefan Xiao,\nYuanzhong Xu, Yujing Zhang, Gustavo Hernandez\nAbrego, Junwhan Ahn, Jacob Austin, Paul Barham,\nJan Botha, James Bradbury, Siddhartha Brahma,\nKevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha\nChowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa\nDehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Gar-\ncia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-\nAri, Steven Hand, Hadi Hashemi, Le Hou, Joshua\nHowland, Andrea Hu, Jeffrey Hui, Jeremy Hur-\nwitz, Michael Isard, Abe Ittycheriah, Matthew Jagiel-\nski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun,\nSneha Kudugunta, Chang Lan, Katherine Lee, Ben-\njamin Lee, Eric Li, Music Li, Wei Li, YaGuang Li,\nJian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu,\nFrederick Liu, Marcello Maggioni, Aroma Mahendru,\nJoshua Maynez, Vedant Misra, Maysam Moussalem,\nZachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek,\nAlex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif,\nBryan Richter, Parker Riley, Alex Castro Ros, Au-\nrko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R.\nSo, Daniel Sohn, Simon Tokumine, Dasha Valter,\nVijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang,\nPidong Wang, Zirui Wang, Tao Wang, John Wiet-\ning, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting\nXue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven\nZheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav\nPetrov, and Yonghui Wu. 2023. Palm 2 technical\nreport.\nJoris Baan, Maartje ter Hoeve, Marlies van der Wees,\nAnne Schuth, and Maarten de Rijke. 2019. Under-\nstanding multi-head attention in abstractive summa-\nrization.\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir\nDurrani, Fahim Dalvi, and James Glass. 2019. Iden-\ntifying and controlling important neurons in neural\nmachine translation. In International Conference on\nLearning Representations, New Orleans.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT\u2019s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for NLP,\npages 276\u2013286, Florence, Italy. Association for Com-\nputational Linguistics.\nGon\u00e7alo M. Correia, Vlad Niculae, and Andr\u00e9 F. T.\nMartins. 2019. Adaptively sparse transformers. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2174\u2013\n2184, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 8493\u2013\n8502, Dublin, Ireland. Association for Computational\nLinguistics.\nNelson Elhage, Tristan Hume, Catherine Olsson,\nNeel Nanda,\nTom Henighan,\nScott Johnston,\nSheer ElShowk, Nicholas Joseph, Nova DasSarma,\nBen Mann, Danny Hernandez, Amanda Askell,\nKamal Ndousse, Jones, , Dawn Drain, Anna\nChen, Yuntao Bai, Deep Ganguli, Liane Lovitt,\nZac Hatfield-Dodds, Jackson Kernion, Tom Con-\nerly, Shauna Kravec, Stanislav Fort, Saurav Ka-\ndavath, Josh Jacobson, Eli Tran-Johnson, Jared\nKaplan,\nJack Clark,\nTom Brown,\nSam Mc-\nCandlish, Dario Amodei, and Christopher Olah.\n2022.\nSoftmax linear units.\nHttps://transformer-\ncircuits.pub/2022/solu/index.html.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In International Conference on Learn-\ning Representations.\nJavier Ferrando, Gerard I. G\u00e1llego, Ioannis Tsiamas,\nand Marta R. Costa-juss\u00e0. 2023. Explaining how\ntransformers use context to build predictions.\nIn\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 5486\u20135513, Toronto, Canada.\nAssociation for Computational Linguistics.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020.\nThe pile: An\n800gb dataset of diverse text for language modeling.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Avi Caciularu, Kevin Wang, and Yoav Gold-\nberg. 2022. Transformer feed-forward layers build\npredictions by promoting concepts in the vocabulary\nspace. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 30\u201345, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are key-\nvalue memories. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 5484\u20135495, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nAaron Gokaslan and Vanya Cohen. 2019. Openwebtext\ncorpus.\nMichael Hanna, Ollie Liu, and Alexandre Variengien.\n2023. How does gpt-2 compute greater-than?: In-\nterpreting mathematical abilities in a pre-trained lan-\nguage model.\nAdi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer\nLevy. 2022. Transformer language models without\npositional encodings still learn positional informa-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 1382\u20131390,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nRonghang Hu, Jacob Andreas, Trevor Darrell, and Kate\nSaenko. 2018. Explainable neural computation via\nstack neural module networks. In Proceedings of the\nEuropean conference on computer vision (ECCV).\nAlon Jacovi, Oren Sar Shalom, and Yoav Goldberg.\n2018. Understanding convolutional neural networks\nfor text classification. In Proceedings of the 2018\nEMNLP Workshop BlackboxNLP: Analyzing and In-\nterpreting Neural Networks for NLP, pages 56\u201365,\nBrussels, Belgium. Association for Computational\nLinguistics.\nGanesh Jawahar, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3651\u20133657, Florence, Italy. Association for\nComputational Linguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B.\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015.\nVisualizing and understanding recurrent networks.\nAmirhossein\nKazemnejad,\nInkit\nPadhi,\nKarthikeyan Natesan Ramamurthy,\nPayel Das,\nand Siva Reddy. 2023.\nThe impact of positional\nencoding on length generalization in transformers.\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter\nClark, and Ashish Sabharwal. 2021. Text modular\nnetworks: Learning to decompose tasks in the lan-\nguage of existing models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1264\u20131279, Online.\nAssociation for Computational Linguistics.\nLouis Kirsch, Julius Kunze, and David Barber. 2018.\nModular networks: Learning to decompose neural\ncomputation. In Advances in Neural Information\nProcessing Systems, volume 31. Curran Associates,\nInc.\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof BERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4365\u20134374, Hong Kong, China. Association for Com-\nputational Linguistics.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\nton. 2012. Imagenet classification with deep con-\nvolutional neural networks. In Advances in Neural\nInformation Processing Systems, volume 25. Curran\nAssociates, Inc.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073\u20131094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nAndr\u00e9 F. T. Martins and Ram\u00f3n F. Astudillo. 2016.\nFrom softmax to sparsemax: A sparse model of at-\ntention and multi-label classification. In Proceed-\nings of the 33rd International Conference on Interna-\ntional Conference on Machine Learning - Volume 48,\nICML\u201916, page 1614\u20131623. JMLR.org.\nPedro Henrique Martins, Zita Marinho, and Andr\u00e9 F. T.\nMartins. 2020. Sparse text generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4252\u20134273, Online. Association for Computational\nLinguistics.\nKevin Meng, David Bau, Alex J Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in GPT. In Advances in Neural Information\nProcessing Systems.\nVlad Niculae and Mathieu Blondel. 2017. A regularized\nframework for sparse and structured neural attention.\nIn Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nBen Peters, Vlad Niculae, and Andr\u00e9 F. T. Martins. 2019.\nSparse sequence-to-sequence models. In Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 1504\u20131519, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers), pages 2227\u20132237,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300\u2013325,\nOnline. Association for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593\u2013\n4601, Florence, Italy. Association for Computational\nLinguistics.\nAndreas Veit, Michael J Wilber, and Serge Belongie.\n2016. Residual networks behave like ensembles of\nrelatively shallow networks. In Advances in Neural\nInformation Processing Systems, volume 29. Curran\nAssociates, Inc.\nElena Voita, Rico Sennrich, and Ivan Titov. 2019a. The\nbottom-up evolution of representations in the trans-\nformer: A study with machine translation and lan-\nguage modeling objectives. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4396\u20134406, Hong Kong,\nChina. Association for Computational Linguistics.\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1264\u20131274, Melbourne, Australia. Association\nfor Computational Linguistics.\nElena Voita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019b. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797\u20135808, Florence, Italy.\nAssociation for Computational Linguistics.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022. Inter-\npretability in the wild: a circuit for indirect object\nidentification in gpt-2 small.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research. Survey Certifica-\ntion.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nSumu Zhao, Damian Pascual, Gino Brunner, and Roger\nWattenhofer. 2021. Of non-linearity and commutativ-\nity in bert.\nFigure 12: Neurons categorized by the number of bi-\ngrams able to trigger them. First half of the network,\nalive neurons only.\nFigure 13: Neurons categorized by the number of tri-\ngrams able to trigger them. First half of the network,\nalive neurons only.\nA\nN-gram-Detecting Neurons\nA.1\nNumber of N-grams Triggering a Neuron\nFigure 12 shows how neurons in each layer are\ncategorized by the number of covering them bi-\ngrams, Figure 13 \u2013 trigrams. As expected, neurons\nin larger models are covered by less n-grams.\nA.2\nTrigram-Detecting Neurons\nSimilarly to token-detecting neurons in Section 4.2,\nwe also find neurons that are specialized on 3-\ngrams. Specifically, we (1) pick neurons that are\ncovered by only 1-50 trigrams, (2) gather trigrams\nthat are covered by this neuron (if the neuron ac-\ntivated at least 95% of the time the trigram is\npresent), (3) if altogether, these covered trigrams\nare responsible for at least 95% of neuron activa-\n(a)\n(b)\nFigure 14: (a) Number of trigram-detecting neurons;\n(b) number of trigrams that have a detecting them neu-\nron: solid line \u2013 per layer, dashed \u2013 cumulative over\nlayers.\nFigure 15: Number of trigrams covered in each layer\nwith indicated (i) new overall, and (ii) new compared to\nthe previous layer tokens.\ntions.\nFigure 14 shows the results. Overall, the results\nfurther support our main observations: larger mod-\nels have more neurons responsible for n-grams. In-\nterestingly, when looking at trigrams rather than to-\nkens, at 30b of parameters we see a drastic jump in\nthe number of covered n-grams. This indicates that\none of the qualitative differences between larger\nand smaller models lies in the expansion of the\nfamilies of features they are able to represent.\nA.3\nEnsemble-Like Layer Behavior\nFigure 15 shows the number of covered trigrams in\neach layer. We see that in each layer, models cover\nlargely new trigrams.\nB\nPositional Neurons\nB.1\nMutual Information\nFor each neuron, we evaluate mutual information\nbetween two random variables:\n\u2022 act \u2013 neuron is activated or not ({Y, N}),\n\u2022 pos \u2013 token position ({1, 2, . . . , T}).\nFormal setting.\nWe gather neuron activations\nfor full-length data (i.e., T = 2048 tokens) for\nWikipedia, DM Mathematics and Codeparrot. Let\nfr(pos)\nn\nbe activation frequency of neuron n at posi-\ntion pos and frn be the total activation frequency\nof this neuron.\nThen the desired mutual information is as fol-\nlows:\nI(act, pos) =\n=\nX\nact\nT\nX\npos=1\n1\np(pos)p(act|pos) \u00b7 log p(act|pos)\np(act)\n=\nSince we only feed full-length texts, all positions\nappear with the same frequency: p(pos) = 1/T.\n= 1\nT \u00b7\nX\nact\u2208{Y,N}\nT\nX\npos=1\np(act|pos)\u00b7log p(act|pos)\np(act)\n=\n= 1\nT \u00b7\nT\nX\npos=1\np(act = Y |pos)\u00b7log p(act = Y |pos)\np(act = Y )\n+\n1\nT \u00b7\nT\nX\npos=1\n(1\u2212p(act = Y |pos))\u00b7log 1\u2212p(act=Y |pos)\n1 \u2212 p(act = Y )\n=\n= 1\nT \u00b7\nT\nX\npos=1\n\u0014\nfr(pos)\nn\n\u00b7 log fr(pos)\nn\nfrn\n+\n(1 \u2212 fr(pos)\nn\n) \u00b7 log 1 \u2212 fr(pos)\nn\n1 \u2212 frn\n\u0015\n.\nB.2\nPositional Neurons for the 350m Model\nThe results are shown in Figure 16.\nFigure 16: Positional neurons in the 350m model. Each\ncircle corresponds to a single neuron, colors and their\nintensity correspond to the types of patterns shown in\nFigure 7.\n"
  },
  {
    "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale",
    "link": "https://arxiv.org/pdf/2309.04564.pdf",
    "upvote": "14",
    "text": "When Less is More:\nInvestigating Data Pruning for Pretraining\nLLMs at Scale\nMax Marion\nCohere for AI\nmaxwell@cohere.com\nAhmet \u00dcst\u00fcn\nCohere for AI\nahmet@cohere.com\nLuiza Pozzobon\nCohere for AI\nluiza@cohere.com\nAlex Wang\nCohere\nalexwang@cohere.com\nMarzieh Fadaee\nCohere for AI\nmarzieh@cohere.com\nSara Hooker\nCohere for AI\nsarahooker@cohere.com\nAbstract\nLarge volumes of text data have contributed significantly to the development of large language\nmodels (LLMs) in recent years. This data is typically acquired by scraping the internet, leading\nto pretraining datasets comprised of noisy web text. To date, efforts to prune these datasets down\nto a higher quality subset have relied on hand-crafted heuristics encoded as rule-based filters. In\nthis work, we take a wider view and explore scalable estimates of data quality that can be used to\nsystematically measure the quality of pretraining data. We perform a rigorous comparison at scale\nof the simple data quality estimator of perplexity, as well as more sophisticated and computationally\nintensive estimates of the Error L2-Norm and memorization. These metrics are used to rank and\nprune pretraining corpora, and we subsequently compare LLMs trained on these pruned datasets.\nSurprisingly, we find that the simple technique of perplexity outperforms our more computationally\nexpensive scoring methods. We improve over our no-pruning baseline while training on as little\nas 30% of the original training dataset. Our work sets the foundation for unexplored strategies in\nautomatically curating high quality corpora and suggests the majority of pretraining data can be\nremoved while retaining performance.\n1\nIntroduction\nA reigning belief in machine learning is that more data leads to better performance. Recent years\nof progress in scaling large language models (LLMs) have shown strong evidence to support this\nwith remarkable gains in language understanding and generation capabilities (Brown et al., 2020;\nTouvron et al., 2023; Kaplan et al., 2020; Anil et al., 2023).\nWhen training language models,\ncommon practice is to use massive datasets such as C4 (Raffel et al., 2020), RefinedWeb (Penedo\net al., 2023), and The Pile (Gao et al., 2021). These datasets are typically compiled by scraping\nraw web pages from the internet, leading to a substantial portion of the text being noisy and of low\nquality (Dodge et al., 2021; Kreutzer et al., 2022; Luccioni & Viviano, 2021).\nPractitioners have established a number of standard filtering techniques to remove low-quality ex-\namples from these datasets. These techniques are predominantly rule-based heuristics: removing\n1\narXiv:2309.04564v1  [cs.CL]  8 Sep 2023\nFigure 1: Demonstration of our pruning methodology.For each sequence zi, sized equally as the\nmodel\u2019s context length, a pruning algorithm \u03be generates score si. We then choose which subset of\nthe distribution of scores to keep: bottom, middle, or top. Finally, a new model is pretrained with\nthe pruned data \u02c6D\u03be.\ndocuments containing repetitive text (Zhang et al., 2022; Raffel et al., 2020; Rae et al., 2022; Her-\nnandez et al., 2022; Penedo et al., 2023), special characters, or non-English text (Wenzek et al.,\n2020); ignoring data from a manually curated list of \u201cblocklist\u201d websites (Dodge et al., 2021; Rae\net al., 2022); or eliminating documents based on certain length thresholds. While these hand-curated\nfilters can eliminate certain noisy examples, they are not a substitute for a measure of \u201cquality\u201d for\nindividual training examples, for which there are currently no established best practices (Mitchell\net al., 2023).\nIn this work, we take a wider view and ask if we can arrive at a rigorous estimator of data quality\nthrough data pruning.\nData pruning attempts to isolate a subset of a larger training dataset such that a model trained on\nsaid subset preserves or improves performance over a model trained on the full dataset. To date,\nthe majority of work on data pruning has centered on supervised computer vision settings (Qin\net al., 2023; Sorscher et al., 2023; Raju et al., 2021; Paul et al., 2023; He et al., 2023), with far fewer\nworks focusing on language. Those that have either studied the fine-tuning setting, which typically\nhas an order of magnitude less data and thus tolerates more computational complexity (Fayyaz\net al., 2022; Attendu & Corbeil, 2023; Cao et al., 2023) or based their method on hand picking high-\nquality corpora (Gao, 2021; Wenzek et al., 2020; Brown et al., 2020). Specifically, we try to answer\nthe following: Can we remove the least impactful examples from a pretraining dataset and achieve\nsimilar or better performance? Do simpler techniques for estimating data quality outperform more\nsophisticated and computationally expensive methods?\nWhat aspects of training dynamics signal\ndata quality the best?\nWe answer these questions by rigorously evaluating three automatic pruning metrics. One simple\nestimator of quality, perplexity, and two more complex, and EL2N (Paul et al., 2023) memorization\nfactor. These methods all rely solely on model outputs and do not require a preselected high-quality\n2\ndataset. This lack of dependence on human judgments of data quality make them a promising direc-\ntion for automatic selection of high quality corpora. We perform extensive experiments evaluating\nmodels ranging from 124M to 1.5B parameters across different pretrained corpora. Our contribu-\ntions are the following:\n1. We extensively benchmark data pruning based on perplexity, EL2N, and memorization in\nthe LLM pretraining setting. Surprisingly, we find the simple technique of ranking\nexamples based on their perplexity outperforms far more complex techniques\nsuch as memorization. A model trained on 50% of the dataset pruned based on perplexity\nachieves 1.33% and 1.77% improvement over the most performant models pruned to 50% of\nthe dataset with EL2N and memorization factor respectively. A model trained on 30% of\nthe dataset pruned with perplexity achieves a 2.1% and 1.6% improvement over the most\nperformant models pruned to 30% of the dataset with EL2N and memorization factor.\n2. To comprehensively cover multiple facets of data pruning, we provide a unified and general\nframework to identify and treat different data subsets present in a dataset. We compare mod-\nels trained on datasets pruned to 10, 30, 50, and 70% of the training set while retaining either\nthe bottom, middle, or top of the pruning scores\u2019 distributions. We test seven different refer-\nence models across pruning variations, investigating the impact of parameter count, training\ndataset, and total training steps on the reference models\u2019 pruning capabilities. Finally, we\nfinetune a selection of our models on six tasks from the GLUE benchmark (Wang et al., 2019)\nto evaluate the effect of pruning on downstream generalization.\n3. We test our pruning methods at scale, achieving a 1% improvement in test set perplexity using\nhalf of the dataset over a baseline model trained on the entire dataset. We show this scales to\n1.5B parameter models, achieving 1.5% improvement in test set perplexity over a no-pruning\nbaseline of the same size.\n2\nMethodology\nGiven a large-scale dataset D, we tokenize all documents and append a special <eod> token to their\nend. We then concatenate and split them into n sequences zi of fixed length t equal to the model\u2019s\ncontext length: D = {z1, . . . , zn}. Consider the subset of training instances P\u03be where \u03be refers to the\nalgorithm used to select the subset. We build this subset by computing the pruning score Score\u03be(zi)\nfor each data point zi. We then populate P\u03be with instances that fit our selection criteria:\nP\u03be = {zi \u2208 D | Criteria(Score\u03be(zi))}\n(1)\nBy removing P\u03be from D, the remaining instances are described as:\n\u02c6D\u03be = D \\ P\u03be\n(2)\nOur goal is to choose the pruning algorithm \u03be such that when training a language model on the\nremaining subset of training instances, \u02c6D\u03be, the model\u2019s performance is not diminished:\nP\u03c4(M \u02c6D\u03be) \u2265 P\u03c4(MD)\n(3)\nwhere M \u02c6D\u03be is the model trained on \u02c6D\u03be and P\u03c4 is the performance on task \u03c4. We explore three\nmetrics, perplexity, Error L2-Norm (EL2N), and memorization which we detail below in Section\n2.1, and evaluate the different ways in which the metric can be employed to determine P\u03be.\n3\nIn particular, we evaluate different reference models\n\u02dc\nM that are used to calculate pruning scores.\nBoth reference models \u02dc\nM and trained models M share the same context length to ensure consistency\nbetween the contexts for which pruning metrics are calculated and trained models are trained.\nFor each metric, we consider three different selection criteria to determine P\u03be as seen in Equation\n1: isolating the top, middle, or bottom percentiles of D as the data to be kept.\nWe pretrain\nseparate models using these criteria with different percentages of the dataset to understand the\ndynamics and impact of each pruning metric. Since the effectiveness of these metrics in this specific\ncontext remains uncertain, we opt for these contrasting subsets to clarify the relationship between\neach metric and the overall model performance. Figure 1 demonstrates our experimental setup. We\nfocus on static pruning, in which data is pruned once before training. This is in contrast to adaptive\npruning, in which data is pruned as training is happening, such as in (Fayyaz et al., 2022; Park\net al., 2022).\n2.1\nPruning Methods\nHere, we briefly describe data pruning algorithms that we benchmark in this work. Our goal is to\nrigorously compare simple and computationally inexpensive ranking approaches such as perplex-\nity and random ranking against more sophisticated and computationally expensive techniques\nsuch as memorization scores and EL2N.\n2.1.1\nSelection via Perplexity\nPerplexity measures how probable a given piece of text is based on a particular language model.\nFor each instance zi in D, we compute the perplexity metric as:\nPPL(zi) = exp\n\u0000 1\n|zi|\nX\ntj\u2208zi\nNLL(tj)\n\u0001\n(4)\nwhere NLL(tj) is the negative log likelihood of token tj in sequence zi:\nNLL(tj) = \u2212 log P(tj|t<j; \u03b8)\n(5)\nA lower perplexity score indicates that the model assigns a high probability to the text.\n2.1.2\nSelection via EL2N\nThe Error L2-Norm (EL2N) score was originally proposed in a computer vision setting to identify\nwhich samples are important for learning (Paul et al., 2023). It measures each sample\u2019s importance\nusing the model\u2019s early learning signals. We define the EL2N score on text sequences as the average\nL2 norm of the error vector, where \u02c6yi is the reference model\u2019s predicted probability distribution over\nthe vocabulary and yt is the one-hot encoded representation of the ground truth:\nEL2N(zi) = 1\nt\nt\nX\ni\n\u2225\u02c6yt \u2212 yt\u22252\n(6)\nWe first evaluate the pruning efficacy of EL2N scores obtained from a single reference model at two\ndifferent checkpoints, trained on 14% and 55% of the training dataset D corresponding to 250 and\n4\n1000 steps respectively, to determine the required number of steps needed before a usable pruning\nsignal emerges. We then train ten different reference models with different random initializations\nand average the EL2N score from all ten models to obtain our final EL2N score.\nThe authors\nsuggest that exhibiting a low EL2N score are typically those the model learns in its early stages\nof training, likely because they are relatively easier. Inversely, examples with higher EL2N scores\nare hypothesized to indicate that the model continues to incur a significant loss for them and may\nrequire additional iterations to learn.\n2.1.3\nMemorization Ranking\nMemorization in language models is a well-studied phenomenon (Carlini et al., 2023; 2021; Biderman\net al., 2023a). In this work we explore memorization scores applied as a data pruning ranking. We\nuse the memorization score as defined by Biderman et al. (2023a):\nscore(M, N) = 1\nN\nN\nX\ni\n1(zM+i = \u02c6zM+i)\n(7)\nwhere z is a data point, \u02c6z is a sequence of tokens predicted by the reference model, and 1(\u00b7) is\nan indicator function.\nA reference model is prompted with the first M tokens of a data point\nz to calculate the memorization score. We then greedily generate N additional tokens, \u02c6z. The\nmemorization score is the fraction of the N greedily generated tokens (\u02c6zM:M+N) that match exactly\nwith the original data point (zM:M+N). For our experiments, M = N = 32. We note that the\nauthors did not originally propose this as data pruning metric, but we hypothesize that it can be a\nvaluable ranking to identity examples which require additional learning. We use reference models\nguaranteed to have seen the full training set to ensure the applicability of memorization scores. A\nhigh memorization score indicates the model reproduces more of the text verbatim.\n2.1.4\nRandom Pruning\nWe also evaluate a lower bound of expected performance: pruning a random selection of samples.\nThis allows us to ask the question \u201care proposed pruning methods any better than a random guess?\u201d\n3\nExperiments\n3.1\nModel\nWe train autoregressive decoder-only Transformer models (Vaswani et al., 2023) with a standard\nlanguage modeling objective. Given an input sequence of zi = [r1, \u00b7 \u00b7 \u00b7 , rt] from training data D, a\nlanguage model with parameters \u03b8 is trained to minimize the negative log-likelihood loss as defined\nin Equation 5. Our language models follow the traditional GPT-style architecture (Radford et al.,\n2018).\nWhile training our models, we use AdamW (Loshchilov & Hutter, 2019) with linear cosine scaling\nand a batch size of 2048. The 124M parameter models are trained for 8000 steps, which amounts to\na total of 33B tokens with a learning rate that linearly increases from 0 to 1.5e-4 over the course of\ntraining. This is approximately 4.4 epochs over the unpruned dataset. We tokenize the data with\nByte Pair Encoding (Sennrich et al., 2016) with a vocabulary of 51200. Due to the memory and\n5\nExperimental axes\nChoices\nPruning Metric\nPerplexity, EL2N, Memorization\nPct. Data Remaining\n10, 30, 50, 70\nPruning Subset\nBottom, Middle, Top\nReference Model Size\n124M, 6B, 13B, 52B\nReference Model Epoch Perc.\n14%, 55%, 440%, Full\nReference Model Tr. Data\nCC, Wiki, Web-scale\nTrained Model Size\n124M, 1.5B\nTable 1:\nPruning choices explored in the experiments. Under \u201cReference Model Training Steps\u201d,\n\u201cFull\u201d refers to the fully trained Cohere LLMs. Under \u201cReference Model Training Data\u201d, \u201cWeb-scale\u201d\nrefers to the significantly larger training datasets used by the Cohere reference models.\ncomputational costs of training 1.5B parameter models, our experiments at this size are trained\nwith a batch size of 512 for 14568 steps. As such, the models see only 7.6B tokens, equivalent\nto a single epoch of our unpruned dataset. The learning rate for 1.5B parameter models linearly\nincreases from 0 to 1.2e-4 over the course of training. All models use a context window length of\n2048.\n3.2\nData\nWe use a random sample of the May 2022 snapshot of CommonCrawl1 in our experiments. After\ndownsampling the unpruned dataset has 7.6B tokens, about 20% of the full snapshot. This down-\nsampling is required due to the computational cost of our various ablation experiments, which each\nrequire pretraining a new model from random initialization. This dataset is prefiltered using a com-\nbination of automatic and hand-crafted filters, as we aim to further improve data quality beyond\ncommon rule-based filters. The filters exclude repetitive documents, documents with percentages\nof special characters, and documents that contain explicit words and toxic text, similar to dedupli-\ncation steps seen in Taylor et al. (2022); Kocetkov et al. (2022). Our Wikipedia dataset contains\n5.3M tokens and only includes English pages.\n3.3\nAblations\nFor all techniques, we compare performance when only 10%, 30%, 50%, and 70% of all data is\npreserved. We compare retaining the top, middle, and bottom subsets according to the pruning\nranking, e.g., when retaining 30% of the bottom of the pruning metric\u2019s distribution over the\ntraining set, we calculate the 30th percentile of the pruning metric\u2019s distribution and remove all\ndata points with perplexity above it.\nWhen retaining the middle 30%, we calculate the 35th\nand 65th percentile and remove all data points above and below those numbers respectively. Each\nablation study(pruning method, percent data remaining, section of distribution preserved) requires\ntraining a new model from random initialization. We train a minimum of nine models with\n124M parameters from scratch for each experimental variant.\nTable 1 summarizes the perplexity pruning variations we explore in this paper. For perplexity, we\n1https://data.commoncrawl.org/\n6\nuse a separate model to compute perplexity from the model trained on the pruned data. We call\nmodels used to compute the perplexity ranking reference models and the models trained on the\npruned datasets pruned models. We conduct a rigorous evaluation of what impacts the quality of\nthe ranking by varying different factors that affect the perplexity distribution:\n1. Reference Model Size To explore how reference model size impacts the rating quality,\nwe compare perplexity computations using 6B, 13B, and 52B Cohere models trained on full\nweb-scale datasets.\n2. Reference Model Training Data To isolate the impact of training data, we compute per-\nplexity using 124M parameter reference models trained on either CommonCrawl or Wikipedia.\n3. Total Reference Model Training Steps To isolate the impact of early training signals,\nwe compute perplexity and EL2N using 124M parameter models trained on CommonCrawl\ndata for approximately 14% and 55% of total training steps. Reference models trained on\nCommonCrawl are trained on a non-overlapping subset from the CommonCrawl dataset that\nis pruned and used to train the student model.\n3.4\nEvaluation\nWe report perplexity on a test set from the same CommonCrawl snapshot with identical prefiltering\nas the training data. This test set contains 266M tokens, equivalent to about 3.5% of the training\nset.\nWe also finetune a subset of our models on six different classification tasks from GLUE (Wang et al.,\n2019).We do not prune the task dataset, as our aim is to analyze the pruning methods\u2019 effects on\npretraining. We compare performance after 8000 steps (approximately 4.4 epochs of the pretraining\ndataset), chosen to compare performance after models have saturated their capacity by training\nenough steps to plateau on validation metrics.\n4\nResults and Discussion\n4.1\nRemoving Easy Instances Improves Performance\nThough the most competitive variant for each pruning method varies based on the subset of the\nscoring distribution retained (top, middle, or bottom), we observe a consistent pattern: the highest\nperformant variants are not the subsets that correspond to the \u201ceasier\u201d data. The interpretation of\nthe term \u201ceasy\u201d varies according to the measurement employed. When employing the Perplexity\nmetric, it refers to the bottom samples with the lowest perplexity. With the EL2N metric, it also\npertains to the bottom samples exhibiting the lowest initial loss. In the context of memorization,\nit relates to the top samples that have been most thoroughly memorized.\nFigure 2 demonstrates this pattern when using Perplexity. In contrast to the middle or top\nsubsets, the bottom subset has much less variance in results between reference models of varying\nsizes, indicating the bottom subset may not be suitable for training.\nThe middle experiments\nachieve consistently low test set perplexities for various reference model sizes and pruning ratios.\nGenerally, performance monotonically degrades as the amount of data remaining shrinks - except\n7\nFigure 2: The effect of employing reference models of different sizes on the computation of pruning\nperplexity scores and its subsequent influence on test set perplexity. The three subset selection\napproaches for each set of experiments are showcased separately (keeping bottom, middle, or top\nof the pruning score distribution).\nfor the middle subset for the best-performing reference models. In these cases, retaining only 50%\nand even 30% of the dataset outperforms retaining 70% of the dataset.\nNext, Figure 3b(a) shows the results for the EL2N metric.The middle subset is also the best variant\nfor EL2N. While the best performing run does not outperform the baseline, the best performance is\nachieved when retaining 50% of the middle subset, outperforming the model trained on 70% of the\ndataset, similar to the results when using perplexity. As the middle subset grows, it begins to\noverlap with the easiest examples, degrading performance. In section 4.5, we discuss how different\nmodel checkpoints influence the effectiveness of the EL2N metric.\nFinally, when using memorization factor as a pruning metric, keeping the least memorized\nsamples (bottom subset) generally performs best. Figure 3b(b) shows model performances for this\nmetric. We observe that the most competitive variant of the memorization metric is the bottom\n70% of the distribution. Memorization never outperforms the no-pruning baseline.\n(a) EL2N\n(b) Memorization\nFigure 3: Evaluation of different subset selection criteria for two pruning metrics: (a) EL2N and\n(b) Memorization.\n8\n4.2\nSimple Pruning Metrics Outperform More Sophisticated Approaches\nIn Figure 4 we present results comparing the performance of the best variant of each pruning\nmetric: (1) retaining the middle of the distribution of Perplexity scores by the fully trained 52B\nreference model, (2) retaining the bottom of the distribution of the Memorization Factor (least\nmemorized samples), and (3) retaining the middle of the distribution of EL2N scores from the\n1000 step checkpoint. We also include results for our baselines: a model trained on the entirety of\nthe training data D and models trained on randomly pruned data. Our results show that training\non the middle subset using Perplexity outperforms other pruning metrics across all dataset\nsizes. For some variants, it also outperforms training on the entire dataset. For example, at 30%\nand 50% of the original dataset size, Perplexity outperforms the full dataset size. Compared\nwith the no-pruning baseline, pruning to the middle 50% of the perplexity distribution leads to a\n0.97% improvement in perplexity. Using only the middle 30% of the data achieves nearly the same\nperformance, with a 0.80% improvement over the no-pruning baseline.\nFigure 4: The top performing variants of the different pruning methods, compared across various\ndataset sizes. Random pruning and no-pruning are included as baselines. Perplexity-based pruning\nconsistently surpasses both alternative metrics and the no pruning experiments. See Section 4.2 for\ndetails on the featured variants.\nCompared with random selection, pruning using Perplexity results in significantly higher model\nperformance than random pruning across all data ratios (Figure 4). For memorization and EL2N\npruning metrics, both achieve similar performances to random pruning despite being far more\ncomputationally expensive.\n4.3\nPruning Benefits from Using Larger Reference Models\nGiven that the most competitive variant perplexity uses a reference model to compute scores, we\nexpect that the size of the reference model will have a significant impact on the data pruned.\nFigure 2 shows the trained model performances after pruning with perplexity calculated with\nreference models ranging from 124M to 52B parameters. We find that increasing reference model\n9\nFigure 5: Performance of different pruning strategies using two different reference models: one\ntrained on Wikipedia and one trained on CommonCrawl. A reference model trained on Wikipedia\n(an example of a clean noise-free corpus) achieves consistently lower validation perplexity compared\nto a reference model trained on a noisier CommonCrawl in our two robust settings (middle and\ntop).\nsize improves trained model performance over the no-pruning baseline when either the middle or\ntop subsets are used. Data pruning using the perplexity scores generated from a 52B parameter\nreference model achieves a 2.2% improvement in perplexity over the best-performing trained model\nfrom the 124M parameter reference model experiments. Furthermore, for 13B and 52B reference\nmodels, we observe better performances with less training data when keeping the middle and top\nsubsets. For both of these larger models, retaining the middle 30% and 50% of the training data\nproduces pruned models that outperform the pruned models trained on the middle 70% of the\ntraining set.\nWe note that the effects of subset selection, such as the bottom subset performing worse, approx-\nimately scale with the size of the reference models. The larger reference models\u2019 bottom subset\ntraining runs perform even worse than their smaller counterparts when retaining the same percent-\nage of the training set. This overall points to the consistent finding that larger models are better\ncalibrated at computing a useful data pruning ranking.\n4.4\nImproved Pruning Signals Result from Reference Models Trained on Cleaner\nData\nIn this section we ask: does the data the reference model is trained on impact the quality of the\nranking? We compare the perplexity rankings generated by reference models trained on two different\ncorpora: Wikipedia and CommonCrawl. We investigate whether a model trained on Wikipedia, a\ndataset frequently hand-picked as a high-quality dataset (Xie et al., 2023b; Wenzek et al., 2020),\ngenerates more effective pruning signals for perplexity rankings.\nIn Figure 5, we compare the\nperformance of the two variants across different pruning percentages and subset selections. We\nobserve that in the two optimal selection variants from the general reference models (middle and\ntop) a model trained on Wikipedia consistently yields lower validation perplexity compared to a\nmodel trained on CommonCrawl. Wikipedia\u2019s best variant, pruning to the middle 70%, outperforms\n10\nCommonCrawl\u2019s best variant, also pruning to the middle 70%, by 0.69%.\nThis finding overall\nsuggests that investing in a high quality reference model to generate rankings results in more effective\ndata pruning. Reference models trained on higher quality data are better at identifying a subset of\ndata points most conducive to model performance.\n4.5\nEarly Reference Model Checkpoints Serve as Effective Scoring Models\nFigure 6: The impact of using an early checkpoint of the reference model in pruning based on\nPerplexity and EL2N metrics.\nMotivated by several works that have found that there is a signal in early training checkpoints (Paul\net al., 2023; Agarwal et al., 2022; Siddiqui et al., 2022), we investigate whether early checkpoint of a\nreference model during training offers adequate signal for calculating discriminative pruning scores.\nWe study perplexity and EL2N scores obtained from two early checkpoints: after training on\napproximately 14% and 55% of the full training dataset (250 and 1000 training steps respectively).\nFigure 6 showcases the results of these experiments.\nExamining the 14% checkpoint for both\nperplexity and EL2N, we notice minimal variance across percentages and subset selection criteria.\nPerformance across subsets changes considerably less than either the 55% checkpoint or the fully\ntrained models.\nGiven this, we deduce that training on only 14% of the data is inadequate for our reference model to\noffer precise pruning scores. In contrast, the 55% reference models perform in a similar manner to\nthe fully trained models, performing best with the middle subset, worst with the bottom subset, and\ncomparably with the top subset. Fully training the reference model is shown not to be necessary\nto uphold comparable performance. Halving the reference model training steps proves effective,\nenabling the utilization of early checkpoints. In practice, we expect many practitioners to use off\nthe shelf models for computing perplexity and may not need to carry the cost of pretraining a\nreference model from random initialization.\nWe also show performance for EL2N scores averaged across 10 reference models, initialized with\ndifferent random seeds. We selected the 55% reference models given our previous result.\nWhile the best pruned models using the averaged EL2N score did not outperform the best pruned\nmodels trained on only one reference model\u2019s EL2N score, the pattern of performance more similarly\nmirrors what we see with the larger, fully trained reference models. Specifically, in the middle\nsubset, using 50% of the dataset outperforms using 70%. When constrained to the bottom subset,\nperformance more clearly monotonically degrades when using less data than when using the 55%\nreference model, whereas the earlier checkpoint has comparable performance when retaining 30, 50,\nand 70% of the data. This implies that averaging scores across reference models helps hone the\n11\npruning signal, identifying subsets \u201ceasy\" or \u201chard\" subsets in more similar ways to larger models.\n4.6\nPerplexity-based Pruning Improvements Generalize to Larger Scale Models\nFigure 7: Comparing the best performing pruning method (keeping the middle subset using a\n52B parameter reference model) with random pruning at two distinct pruned model scales. The\nimprovement in performance of a perplexity-based pruning approach carries from 124M to 1.5B\nparameter models.\nWe take our strongest pruning variant \u2013 perplexity computed using a 52B parameter reference\nmodel while retaining the middle subset \u2013 to explore the robustness of our findings at a larger scale\nby validating our findings on a 1.5B model. Figure 7 shows pruning scaling from 124M to 1.5B\nparameter models. Training a 1.5B model, we observe that random pruning performs considerably\nwell, even reaching levels below the no-pruning run. Nonetheless, perplexity-based pruning achieves\nbetter results than random pruning across all pruning percentages. The improvement observed with\nperplexity-based pruning over random pruning follows a consistent pattern for both the 124M and\n1.5B models. This demonstrates the scalability of our approach to a large-scale pretraining setting.\n4.7\nDownstream Evaluation on GLUE\nPreviously, we demonstrated various ways of pruning the pretraining data and training models\nwith different data sizes. Considering that the pretraining stage primarily focuses on knowledge\nacquisition (Zhou et al., 2023), we inquire about the potential ripple effects of pruning data during\npretraining when these models are subsequently finetuned on downstream tasks. To analyze the\nimpact of different pruning strategies on LLM capabilities, we finetune and evaluate models on a\nsubset of the GLUE tasks (Wang et al., 2019). Results are presented in Table 2. We observe that\npruning the pretraining dataset consistently improves performance across all tasks. While no single\npruning strategy (combining both pruning metric and percentage of remaining data) stands out as\nsuperior across all tasks, the absence of a universally dominant approach is consistent with earlier\nfindings in the literature (Gao, 2021). We observe that retaining only 30% of the least memorized\ninstances yields optimal results for SST2 and WNLI tasks. With perplexity based pruning, the\nbest performance is obtained on QQP and QNLI tasks by keeping 50% and 70% of the training\ndata, respectively. Even random pruning shows improvements in certain tasks, underscoring the\nsignificance of downsampling when handling noisy data during the pretraining stage to mitigate\n12\nTable 2: Mean accuracy and standard deviation of the best variants of each pruning algorithm for\nGLUE classification tasks. Underlined results surpass the baseline performance with no pruning.\nThe best results for each task are marked in bold. Results are reported for 5 runs of each model,\ntrained for 3 epochs with a learning rate of 1e \u2212 5.\nData Remaining\nSST2\nMRPC\nQQP\nQNLI\nRTE\nWNLI\nNo Pruning\n100%\n78.150.002\n64.320.021\n76.550.001\n65.400.006\n49.690.024\n51.560.040\nRandom\nPruning\n70%\n77.920.002\n65.210.017\n76.580.002\n65.110.006\n49.690.013\n48.440.038\n50%\n78.190.003\n65.160.020\n76.400.001\n65.440.006\n49.920.009\n49.690.062\n30%\n77.290.007\n66.040.017\n76.360.001\n65.220.005\n51.330.024\n50.310.057\n10%\n76.440.006\n65.830.021\n75.910.001\n64.400.007\n50.700.007\n50.620.016\nMemorization\nBottom subset\n70%\n77.290.006\n64.380.016\n76.420.001\n66.030.007\n49.060.021\n49.060.042\n50%\n77.890.006\n65.470.017\n76.510.001\n65.990.005\n49.770.013\n50.310.048\n30%\n78.520.004\n65.890.016\n76.480.001\n65.910.006\n50.310.009\n54.380.061\n10%\n76.640.004\n65.160.015\n76.110.001\n64.610.006\n50.390.016\n51.880.059\nEL2N\nMiddle subset\n70%\n78.610.008\n66.460.018\n76.930.001\n67.000.005\n48.670.017\n50.000.058\n50%\n79.170.007\n65.420.016\n76.350.001\n62.430.007\n51.410.028\n51.560.049\n30%\n78.980.005\n65.410.012\n77.470.001\n68.630.005\n49.690.022\n55.310.067\n10%\n78.310.006\n63.380.016\n76.930.001\n65.340.006\n51.950.021\n51.250.064\nPerplexity (52B)\nMiddle subset\n70%\n78.400.004\n64.430.020\n76.680.001\n66.740.007\n50.160.023\n49.060.012\n50%\n78.010.006\n64.370.021\n76.820.001\n66.000.004\n50.620.023\n50.310.021\n30%\n77.340.005\n64.840.023\n76.760.001\n65.890.002\n50.860.009\n50.940.031\n10%\n77.660.006\n65.360.017\n76.400.001\n66.520.007\n51.170.012\n53.440.040\npotential learning degradation.\n5\nRelated Work\n5.1\nRule-Based Data Pruning in NLP\nSignificant portions of web-scraped data used for language model pretraining have been shown to\nbe of low quality, machine-generated spam, pornographic content (Kreutzer et al., 2022). Selection\nprocesses to determine what should be included in large-scale datasets have centered on rule-based\nfilters and heuristics (Bane et al., 2022), such as keeping only text written in English (Raffel et al.,\n2020; Rae et al., 2022) or removing sequences containing blocklisted words (Raffel et al., 2020).\nThere are also quality-based rules such as removing duplicated samples (Zhang et al., 2022) or\nfiltering sentences that do not fit a certain amount of words (Raffel et al., 2020; Rae et al., 2022).\nRule-based approaches for data filtering have shown controversial effects on model performance,\nwith some works advertising improvements on language modeling capabilities (Penedo et al., 2023;\nRaffel et al., 2020), while others do not (Black et al., 2022; Biderman et al., 2023b). Also, heuristics\nare prone to undesired outcomes due to their simplicity. For instance Dodge et al. (2021) show how\nremoving blocklisted words disproportionately removes text from and about minority individuals.\n5.2\nMetric-Based Data Pruning in NLP\nRecent work on metric-based pruning has mainly focused on pruning data from the fine-tuning stage\nof LLMs (Attendu & Corbeil, 2023; Xie et al., 2023b) most probably due to the prohibitive cost of\n13\npruning at the pretraining scale. Attendu & Corbeil (2023) perform dynamic pruning during the\nfine-tuning stage by establishing a curriculum of samples based on their EL2N scores (Paul et al.,\n2023). Similarly, we benchmark EL2N as a static data-pruning metric for language datasets. Our\nwork joins the few others that aim to reduce pretraining dataset sizes (Xie et al., 2023a; Chen, 2023;\nAbbas et al., 2023). Abbas et al. (2023) apply their deduplication method based on embeddings\nto further improve the performance of a previously filtered dataset. We also perform pruning on\npreviously filtered datasets, aiming to enhance performance further.\nPreviously, perplexity has\nbeen used to filter datasets (Muennighoff et al., 2023; Wenzek et al., 2020; Lauren\u00e7on et al., 2023),\nbut its pruning capabilities have been underexplored. Lauren\u00e7on et al. (2023) and Muennighoff\net al. (2023) filter out high-perplexity samples from their corpus as those are framed as unnatural\nlanguage and harmful for performance according to their reference domain, which is Wikipedia. In\ncontrast, we benchmark pruning to low perplexity values and high and medium-valued subsets of a\ndataset\u2019s distribution to understand which is the most valuable section for pretraining at scale. We\nalso explore different reference model sizes and training sets.\n5.3\nData pruning in Computer Vision\nThe majority of work to date on data pruning (Sorscher et al., 2023) and isolating data subsets\n(Siddiqui et al., 2022; Mindermann et al., 2022) using model signal has centered on computer vision.\nThese are typically structured in a supervised setting. In contrast, our focus is on a large-scale NLP\npretraining where the objective is unsupervised pretraining. Most relevant to our method is work by\nSorscher et al. (2023) which empirically studies reducing datasets in a teacher/trained regime, using\na teacher model\u2019s margin as a pruning metric. They find that, with abundant data, training only\non the hardest examples yields better performance, while conversely when data is scarce, training\non only the easiest example yields better performance.\n6\nConclusion\nIn this study, we thoroughly investigate diverse data pruning methods for pretraining LLMs with\nbillions of parameters and with datasets containing billions of tokens. We showed that when properly\napplied, data pruning consistently improves model performance. We also find that training on the\n\u201ceasiest\" examples in a dataset degrades performance, where \u201ceasiest\" is defined as the lowest scoring\nexamples according to a metric based on a reference model. Simple methods that rank instances\nbased on their perplexity demonstrate superior performance compared to more elaborate approaches\nsuch as memorization. Models trained on as little as half of the data selected by perplexity achieve\nup to 1.5% improvement over models trained on the full dataset. Additionally, we establish the\nconsistency of our findings as we scale the model sizes. While scaling up the amount of data LLMs\nare trained on remains a popular avenue for improving models, our work demonstrates that carefully\npruning these large training corpora is also a fruitful direction for making models better.\nReferences\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya Ganguli, and Ari S. Morcos.\nSemdedup:\nData-efficient learning at web-scale through semantic deduplication, 2023.\nChirag Agarwal, Daniel D\u2019souza, and Sara Hooker. Estimating example difficulty using variance\n14\nof gradients.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 10368\u201310378, June 2022.\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark\nOmernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang,\nGustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha, James Brad-\nbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin Cherry, Christo-\npher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa De-\nhghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu\nFeng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy\nGur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy\nHurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia, Kathleen Kenealy,\nMaxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Music Li,\nWei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Mar-\ncello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary\nNado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex\nPolozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros,\nAurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose Slone, Daniel Smilkov,\nDavid R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli,\nXuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Kelvin Xu, Yun-\nhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang\nZhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\nJean-Michel Attendu and Jean-Philippe Corbeil. Nlu on data diets: Dynamic data subset selection\nfor nlp classification tasks, 2023.\nFred Bane, Celia Soler Uguet, Wiktor Stribi\u017cew, and Anna Zaretskaya.\nA comparison of data\nfiltering methods for neural machine translation. In Proceedings of the 15th Biennial Conference\nof the Association for Machine Translation in the Americas (Volume 2: Users and Providers\nTrack and Government Track), pp. 313\u2013325, Orlando, USA, September 2022. Association for\nMachine Translation in the Americas. URL https://aclanthology.org/2022.amta-upg.22.\nStella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raff. Emergent and predictable memorization in large language\nmodels, 2023a.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models\nacross training and scaling, 2023b.\nSidney Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Ho-\nrace He, Connor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, Usvsn Sai Prashanth,\nShivanshu Purohit, Laria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. GPT-\nNeoX-20B: An open-source autoregressive language model. In Proceedings of BigScience Episode\n#5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language Models, pp. 95\u2013136,\nvirtual+Dublin, May 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.b\nigscience-1.9. URL https://aclanthology.org/2022.bigscience-1.9.\n15\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\nYihan Cao, Yanbin Kang, and Lichao Sun.\nInstruction mining: High-quality instruction data\nselection for large language models, 2023.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel.\nExtracting training data from large language models, 2021.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang. Quantifying memorization across neural language models, 2023.\nWenhu Chen. Large language models are few(1)-shot table reasoners. In Findings of the Associa-\ntion for Computational Linguistics: EACL 2023, pp. 1120\u20131130, Dubrovnik, Croatia, May 2023.\nAssociation for Computational Linguistics. URL https://aclanthology.org/2023.findings\n-eacl.83.\nJesse Dodge, Maarten Sap, Ana Marasovi\u0107, William Agnew, Gabriel Ilharco, Dirk Groeneveld,\nMargaret Mitchell, and Matt Gardner. Documenting large webtext corpora: A case study on the\ncolossal clean crawled corpus, 2021.\nMohsen Fayyaz,\nEhsan Aghazadeh,\nAli Modarressi,\nMohammad Taher Pilehvar,\nYadollah\nYaghoobzadeh, and Samira Ebrahimi Kahou. Bert on a data diet: Finding important exam-\nples by gradient-based pruning, 2022.\nLeo Gao. An empirical exploration in quality filtering of text data, 2021.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile:\nAn 800gb dataset of diverse text for language modeling. CoRR, abs/2101.00027, 2021. URL\nhttps://arxiv.org/abs/2101.00027.\nMuyang He, Shuo Yang, Tiejun Huang, and Bo Zhao. Large-scale dataset pruning with dynamic\nuncertainty, 2023.\nDanny Hernandez, Tom Brown, Tom Conerly, Nova DasSarma, Dawn Drain, Sheer El-Showk, Nel-\nson Elhage, Zac Hatfield-Dodds, Tom Henighan, Tristan Hume, Scott Johnston, Ben Mann, Chris\nOlah, Catherine Olsson, Dario Amodei, Nicholas Joseph, Jared Kaplan, and Sam McCandlish.\nScaling laws and interpretability of learning from repeated data, 2022.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.\nScaling laws for neural language\nmodels, 2020.\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis,\nYacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von\nWerra, and Harm de Vries. The stack: 3 tb of permissively licensed source code, 2022.\n16\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh,\nAllahsera Tapo, Nishant Subramani, Artem Sokolov, Claytone Sikasote, Monang Setyawan,\nSupheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, Annette Rios, Isabel Pa-\npadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andre Niyongabo\nRubungo, Toan Q. Nguyen, Mathias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan Muhammad,\nNanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin\nLeong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine \u00c7abuk Ball\u0131, Stella Biderman,\nAlessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele\nAwokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa\nAdeyemi. Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions\nof the Association for Computational Linguistics, 10:50\u201372, 01 2022.\nISSN 2307-387X.\ndoi:\n10.1162/tacl_a_00447. URL https://doi.org/10.1162/tacl_a_00447.\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral,\nTeven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen,\nJ\u00f6rg Frohberg, Mario \u0160a\u0161ko, Quentin Lhoest, Angelina McMillan-Major, Gerard Dupont, Stella\nBiderman, Anna Rogers, Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen,\nSomaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan\nThrush, Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Mu\u00f1oz, Jian Zhu, Daniel Van\nStrien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa, Kyle\nLo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Adelani, Long Phan,\nHieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell,\nSasha Alexandra Luccioni, and Yacine Jernite. The bigscience roots corpus: A 1.6tb composite\nmultilingual dataset, 2023.\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-\nence on Learning Representations, 2019. URL https://openreview.net/forum?id=Bkg6RiCqY7.\nAlexandra Luccioni and Joseph Viviano.\nWhat\u2019s in the box?\nan analysis of undesirable con-\ntent in the Common Crawl corpus.\nIn Proceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers), pp. 182\u2013189, Online, August 2021. Asso-\nciation for Computational Linguistics.\ndoi: 10.18653/v1/2021.acl-short.24.\nURL https:\n//aclanthology.org/2021.acl-short.24.\nS\u00f6ren Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie\nXu, Benedikt H\u00f6ltgen, Aidan N. Gomez, Adrien Morisot, Sebastian Farquhar, and Yarin Gal.\nPrioritized training on points that are learnable, worth learning, and not yet learnt, 2022.\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina\nMcMillan-Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Yacine Jernite, and Douwe\nKiela. Measuring data, 2023.\nNiklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane\nTazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models,\n2023.\nDongmin Park, Dimitris Papailiopoulos, and Kangwook Lee. Active learning is a strong baseline\nfor data subset selection. In Has it Trained Yet? NeurIPS 2022 Workshop, 2022. URL https:\n//openreview.net/forum?id=PAgpyQ5rGS.\n17\nMansheej Paul, Surya Ganguli, and Gintare Karolina Dziugaite. Deep learning on a data diet:\nFinding important examples early in training, 2023.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli,\nHamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb\ndataset for falcon llm: Outperforming curated corpora with web data, and web data only, 2023.\nZiheng Qin, Kai Wang, Zangwei Zheng, Jianyang Gu, Xiangyu Peng, Daquan Zhou, and Yang You.\nInfobatch: Lossless training speed up by unbiased dynamic data pruning, 2023.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language un-\nderstanding by generative pre-training. 2018.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan,\nJacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks,\nMaribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron\nHuang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu,\nErich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen\nSimonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro,\nAida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch,\nJean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux,\nMantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d\u2019Autume,\nYujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas,\nAurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, Laura Weidinger,\nIason Gabriel, William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol\nVinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu,\nand Geoffrey Irving.\nScaling language models: Methods, analysis and insights from training\ngopher, 2022.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer, 2020.\nRavi S Raju, Kyle Daruwalla, and Mikko Lipasti. Accelerating deep learning with dynamic data\npruning, 2021.\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units, 2016.\nShoaib Ahmed Siddiqui, Nitarshan Rajkumar, Tegan Maharaj, David Krueger, and Sara Hooker.\nMetadata archaeology: Unearthing data subsets by leveraging training dynamics, 2022.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari S. Morcos. Beyond neural\nscaling laws: beating power law scaling via data pruning, 2023.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia,\nAndrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language model for\nscience, 2022.\n18\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGlue: A multi-task benchmark and analysis platform for natural language understanding, 2019.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n,\nArmand Joulin, and Edouard Grave. CCNet: Extracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,\npp. 4003\u20134012, Marseille, France, May 2020. European Language Resources Association. ISBN\n979-10-95546-34-4. URL https://aclanthology.org/2020.lrec-1.494.\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,\nQuoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up\nlanguage model pretraining. arXiv preprint arXiv:2305.10429, 2023a.\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for language\nmodels via importance resampling, 2023b.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOpt: Open pre-trained transformer language models, 2022.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat,\nPing Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.\nLima: Less is more for alignment, 2023.\nA\nMetric Distributions\nWe present the total distributions of the pruning metrics used in our analysis in Figure 8.\nB\nExamples from different selection criteria\nExamples from the pretraining data, drawn from distinct subsets (keep bottom, keep middle, keep\ntop), are presented in Tables 3, 4, 5, 6, and 7, with rankings based on perplexity.\n19\n(a) Distributions of Perplexity from different reference models. The dotted lines are placed at each 10th\npercentile. Please note the differences in axes between graphs. Fewer than .1% of examples on the extreme\nhigh end have been truncate to better display the overall distribution\n(b) Distributions of the EL2N and Memorization Fac-\ntor metrics. The dotted lines are placed at each 10th\npercentile and omitted from Memorization Factor due\nto overlap. Please note the log-scaled y-axis.\n(c) Distributions of Perplexity from reference models\ntrained on Wikipedia and CommonCrawl. The Com-\nmonCrawl model is the same as the 124M parameter\nmodel in Figure 8a. The dotted lines are placed at\neach 10th percentile.\nFigure 8: Distributions of different pruning metrics and reference models.\n20\nTable 3: Samples from different distribution subsets using perplexity of a 52B reference model\ntrained on CommonCrawl.\nBottom 10%\nMiddle 10%\nTop 10%\nSubmissions, you hereby grant\nCompany a license to translate,\nmodify (for technical purposes,\nfor example making sure your\ncontent is viewable on an iPhone\nas well as a computer) and repro-\nduce and otherwise act with re-\nspect to such User Submissions,\nin each case to enable us to op-\nerate the Services, as described\nin more detail below.\nThis is a\nlicense only \u2013 your ownership in\nUser Submissions is [...]\nHouse Municipal Heritage Build-\ning is a two-storey, wooden, ver-\nnacular\nbuilding\nwith\na\nlow-\nhipped roof, and is located at\nthe Norris Point Lookout, 104\nMain Road, Norris Point, New-\nfoundland and Labrador.\nThe\nformer family dwelling now oper-\nates as a heritage museum with\na view of the Tablelands of Gros\nMorne National Park located on\nthe great Northern Peninsula.\nThe municipal heritage designa-\ntion [...]\nand a nice book as a nice price.\nPostage is via Royal Mail 1st\nClass in the UK. If you are buying\nfrom overseas then please contact\nme before completing your pur-\nchase for a quote. I will always\ncombine P&P so if ordering mul-\ntiple books, please wait for the in-\nvoice so that discounts can be ap-\nplied. We are slowly populating\nour store with post war Wisden\u2019s\nso if there is anything you need\nthat [...]\nprovided on the Site is not in-\ntended\nfor\ndistribution\nto\nor\nuse by any person or entity in\nany jurisdiction or country where\nsuch distribution or use would\nbe contrary to law or regula-\ntion or which would subject us\nto any registration requirement\nwithin such jurisdiction or coun-\ntry.\nAccordingly, those persons\nwho choose to access the Site\nfrom other locations do so on\ntheir own initiative and are [...]\nselection of fuel type and in-\nput of soot index, coefficient of\nfuel,\nselection of measurement\nunits, input of date and time\nwith keyboard and via RS232\nor RS485 Procedure of indus-\ntrial emissions monitoring with\nthe use of AHKAT-410 has been\nagreed\nin\nFSUE\n\"SRI\nAtmo-\nsphere\" AHKAT-410-16 is ap-\nproved for diesel locomotive and\ndiesel train emission monitoring\nat environment monitoring sta-\ntions in [...]\ncan be returned up to 28 days af-\nter the date of purchase. Please\nnote, we cannot offer refunds on\nbeauty, pierced jewellery or on\nswimwear if the hygiene seal is\nnot in place or has been broken.\nWe now offer FREE label-free re-\nturns with InPost Lockers (avail-\nable 24/7), FREE Doddle Re-\nturns to all UK customers as well\nas a FREE UK Collect+ returns\nservice via over 5,900 local stores\nnationwide.[...]\nlicense only \u2013 your ownership\nin User Submissions is not af-\nfected.\nYou agree that the li-\ncenses you grant are royalty-free,\nperpetual, sublicensable, irrevo-\ncable, and worldwide. Any infor-\nmation or content publicly posted\nor privately transmitted through\nthe Services is the sole responsi-\nbility of the person from whom\nsuch content originated, and you\naccess all such information and\ncontent at your [...]\n1 1/2 \" steel plate,\nall weld\nconstruction Hammer mill ma-\nchine manufacturers,\nsuppliers,\nexporters,\ndealers and traders\nin India and worldwide hammer\nmill machines from Gujarat and\nMumbai since 1960 as per the\nISO standards with required in-\ndustrial features and specifica-\ntions Replaceable bar type grate\nis available for specific applica-\ntions SPECIFICATIONS : Ham-\nmer stone crusher is a kind of\nequip [...]\nseveral turns.\nNearly a month\nafter a foreclosure lawsuit was\nfiled against Freestyle Music Park\nand its parent company, more\nthan a dozen former department\nheads have sued seeking more\nthan $232,000 in unpaid wages\nand bonuses, according to court\npapers filed late Friday.\nSeven-\nteen employees are listed as plain-\ntiffs. Backpay I can understand,\nbut can you honestly expect any\nkind of bonuses [...]\n21\nTable 4: Samples from different distribution subsets using perplexity of a 124M reference model\ntrained on CommonCrawl.\nBottom 10%\nMiddle 10%\nTop 10%\nrisk your food going bad in a luke-\nwarm fridge when you can lease\nkitchen appliances in West Hol-\nlywood through Acima! Are you\na budding DJ? A bit of a high-\nfidelity audio snub? Love to level\nup with the latest video game sys-\ntem?\nLevel up your entertain-\nment at home and on the road\nwith sound systems for lease in\nWest Hollywood. You can make\nflexible lease renewal payments\non the best in-home sound [...]\ngratitude exercise. Before you get\nout of bed, think of five things\nyou are most grateful for. If your\nLife Path number is 2, you have\na duality fit for any earthly expe-\nrience. You are deeply rooted in\nbalance and harmony when deal-\ning with the other numbers.\nIn\norder to stay connected to your\ncommunity, start your day by\nconnecting with your friends and\nfamily. Instead of hopping on so-\ncial [...]\nkeepers\" definitely won\u2019t help!\nThen there are those whose idea\nof a school librarian is based on\none they remember from their\nchildhood, who perhaps didn\u2019t\nlet them borrow from the adult\nshelves or maybe told them to be\nquiet.\nYou know - the cliched\nwoman with glasses and a bun?\nI wear glasses myself and ended\nup haing to get a haircut to avoid\nthe cliche. In summer, of course\nI had to put my [...]\nthe-art\nmixed-use\ndevelopment\nthat features a wide variety of\nshops, services, and restaurants,\nalong with over 950 luxury apart-\nments. The sprawling urban vil-\nlage is pedestrian-friendly and is\nthe perfect place if you want to\nindulge in a shopping spree or\ntreat your taste buds to a hearty\nmeal.\nIf you\u2019re thinking about\nlooking for the perfect home in\nBrookhaven, I\u2019m ready to help!\nGet in touch [...]\nit as a stand-alone piece but later\nexperimented performing it as my\nwritten prediction, confabulation\nstyle, Closing Effect. It\u2019s still a\nwork in progress but I did re-\nceive some \"Standing Ovations!\"\nALAN ARITA \"I received a copy\nof GAME NIGHT and IT IS EX-\nCELLENT! First, the quality of\nthe book is outstanding; every-\nthing from the artwork, layout,\nhidden gems, and of course the\nprecision cut [...]\nand view the supernal beauty\nthat lies beyond. (I wish I\u2019d have\nsaid that first; actually I stole it\nfrom a guy who wrote it a hun-\ndred years ago!*) But if I couldn\u2019t\nsee into the future for a few years,\nthere wouldn\u2019t be a Christmas\nstory today. I\u2019ve a whole lot of\nnotes still in my jeans.\nOne\u2019s\nabout Rabbi Frankel of the Syn-\nagogue across West Street from\nold Reno High School. He was a\npretty [...]\ntoilet\ndrains\nare\noverwhelmed\nwith toilet paper or clogged by\nnon-flushable\nthings\nthat\nfind\ntheir way into the drain. If that\u2019s\nthe case, it may be time to call\na plumbing technician.\nUnex-\npected toilet issues interrupt your\ndaily routine, turning what you\nexpected to be a good day right\ninto a stressful one.\nYou need\nhelp ASAP! Best quality Plumb-\ning is ready to solve your toilet\ntroubles no [...]\nwho offer 3D printing services\nthese days.\nTry searching for\nsomeone who offers them in your\narea.Last week, Apple announced\nthe new A15 processor in a pecu-\nliar way: by comparing its new\nchip to the Android competition,\nrather than the A14 that powered\nlast year\u2019s generation of iPhones.\nWe were all left to try to infer the\nspeed of the A15 based on Apple\u2019s\nclaims, and wondering if the com-\npany was [...]\nfloor study, family room, kitchen,\nunfinished basement for future\nexpansion & 2 car garage. Lennar\nseamlessly blended & showcased\nthe unparalleled beauty of Col-\norado\nwith\nthe\nmost\ninnova-\ntive homes, energy efficient tech-\nnologies & modern conveniences,\nbringing the best of both worlds\ntogether.\nBeautiful finishes and\nupgrades throughout.\nLennar\nprovides the latest in energy ef-\nficiency and state of [...]\n22\nTable 5: Samples from different distribution subsets using perplexity of a 124M reference model\ntrained on Wikipedia.\nBottom 10%\nMiddle 10%\nTop 10%\nof our kids, demonstrated abil-\nity to create meaningful change,\na strong commitment to learning,\nand an ability to work in partner-\nship with others.\" Individuals ac-\ncepted to this program agree to\na two-year teaching commitment.\nIf you become a core member you\nare required to attend an inten-\nsive summer training program to\nprepare for your two-year com-\nmitment. Each region has differ-\nent requirements b [...]\nHST\nsingle\ncylinder\nhydraulic\ncone crusher. HST single cylinder\nhydraulic cone crusher integrates\nmechanical,\nhydraulic,\nelectri-\ncal, automation, intelligent con-\ntrol and other technologies, which\ncan be widely used in medium,\nfine and ultra-fine crushing op-\nerations in metal and non-metal\nmines, cement, sandstone, met-\nallurgy\nand\nother\nindustries...\n1,214 roller cone crusher products\nare offered [...]\nactive play outdoor. Users with-\nout a subscription are not able to\nsee the full content on this page.\nPlease subscribe or login.On the\nnet betting houses include was\nable to offer followers a fabulous\nbest range of luring optimistic as-\npects.\nA style of online casino\nmoney provides consistently con-\ntinually really been ornamented\nand acquired in reaction to make\nsure you basic safety issues. Insi\n[...]\nto be that way.\nWeight loss\nsurgery in Hanover is a great op-\ntion for those who are at least\nfifty pounds overweight and have\nstruggled with weight loss over\nthe years.\nThere are a num-\nber of surgical weight loss proce-\ndures available to those seeking\ntreatment, and Nusbaum Weight\nLoss Centers of New Jersey, with\noffices and bariatric surgeons in\nMorristown, Morris County, Mor-\nris County, and surrou [...]\nsperm whales.\nLearn firsthand\nabout Sri Lanka\u2019s amazing bio-\ndiversity on this private tour to\nthe Kanneliya Rainforest. With\na dedicated guide leading you,\nexplore the UNESCO-listed bio-\nsphere reserve, home to monkeys,\nsnakes, chameleons, and a wide\nrange of bird life.\nLearn about\nthe flora and fauna through com-\nmentary tailored to your interests\nand enjoy plenty of chances to ask\nquestions. Explo [...]\nrow for spotting this Sabal Trail\nposting within minutes.The skin\nhas become delicate.\nI just re-\nceived the goods and I didn\u2019t\nknow how to use it.\nI con-\nsulted the customer service.\nI\ndidn\u2019t expect the customer ser-\nvice person to be super good and\nthe introduction was super care-\nful.\nI have been so successful\nand happy trading with you ev-\nery time.. I hope we have more\ntransactions in the future...\nHa\n[...]\nto which coverage is thereby to be\ngranted; and (2) Shall insure the\nperson named therein and any\nother person, as insured, using\nany such motor vehicle or motor\nvehicles with the express or im-\nplied permission of such named\ninsured against loss from the li-\nability imposed by law for dam-\nages arising out of the owner-\nship, maintenance, or use of such\nmotor vehicle or motor vehicles\nwithin the United [...]\nAlso, I have attached a brief pre-\nsentation of our work for bet-\nter understanding.A two-year so-\nlar energy project at the Univer-\nsity of Sheffield has shown almost\nall of the 2,000 systems in the\nscheme are still performing better\nthan expected. Researchers run-\nning Sheffield Solar Farm, which\nwas launched in August 2010, say\n98 per cent of more than 2,000\nsystems involved in the scheme\nare working [...]\nIt exposes a design and construc-\ntion system for horizontal plates\nto work as slabs in regular con-\ncrete buildings. Based to an evo-\nlutionary finite-element analysis\nof the topological configuration\nto get a curved design with a\n50% reduction of traditional vol-\nume, that provide lower cost, less\ncarbon foot-print, better perfor-\nmance and innovative ceiling. A\nlibrary of profiles is elaborated\naccording [...]\n23\nTable 6: Samples from different distribution subsets using EL2N from a 124M reference model\ntrained on CommonCrawl.\nBottom 10%\nMiddle 10%\nTop 10%\na handle on how many eleva-\ntors they are supposed to over-\nsee. Those officials have repeat-\nedly deflected requests from re-\nporters to detail the count of el-\nevators in Chicago requiring in-\nspection.\nFrydland, during her\ninterview, said she doesn\u2019t know\nhow many elevators her office\nis responsible for inspecting be-\ncause city records lump elevators\ninto the same class of devices as\nescalators, [...]\nthere\u2019s a possibility that you may\ncome across a property that\u2019s\nsharing a driveway with the home\nnext door. That means that one\ndriveway needs to be shared be-\ntween the two adjoining neigh-\nbors. Many real estate investors\nrent out their properties in or-\nder to reap the benefits of passive\nmonthly income while increasing\ntheir equity and building wealth\nover time. Not only are they ben-\nefiting [...]\nWe have all spent happy hours\nlistening to and sharing music\nwe love with those closest to us.\nMany of the people we serve in\nubu are incredibly gifted and play\na wide range of musical instru-\nments and enjoy singing and per-\nforming for other people. Judith\nis enabled by ubu to live more\nindependently in Knaresborough,\nNorth Yorkshire, and has started\ntaking singing lessons in order to\n\u2019grow\u2019 her [...]\nians 4:3?\nJesus addressed this\nvery issue with his disciples on\nthe night of his betrayal.\nHe\nwould\nbe\nleaving\nthem\nsoon,\nbut he promised the Holy Spirit\nwould come to comfort and aide\nthem, \"I will not leave you as or-\nphans; I will come to you.\"-John\n14:18.\nJesus refers to the Holy\nSpirit as himself because, \"the\nHelper, the Holy Spirit, whom\nthe Father will send in my name,\nhe will teach you all [...]\nthe standard as far as cement\nmanufacturing goes several ce-\nment manufacturers still prefer\nball mills for cement production\nwhen they want to design new\ngrinding plants or a new inte-\ngrated 3D design and analysis of\nthe crushing roller of The crush-\ning roller is one of the main parts\nof a highpressure grinding roller\nwhich is a type of highly effi-\ncient ore crushing equipment In\nthe work reported [...]\nrange\n(Table\n1).\nActive-\nControlled Study:\nCRESTOR\nwas compared with the HMG-\nCoA reductase inhibitors ator-\nvastatin,\nsimvastatin,\nand\npravastatin\nin\na\nmulticenter,\nopen-label, dose-ranging study of\n2,240 patients with Type IIa and\nIIb hypercholesterolemia.\nAfter\nrandomization,\npatients\nwere\ntreated for 6 weeks with a single\ndaily dose of either CRESTOR,\natorvastatin,\nsimvastatin,\nor\npravastatin [...]\nMost past attemptsto define so-\ncioeconomics as a science in its\nown right may have been mo-\ntivated tocounter such a sim-\nplistic understanding of socioe-\nconomics.In this chapter, we re-\nview past attempts to define so-\ncioeconomics before theapproach\nis chosen that we applied in this\nbook. This book, by a leading ex-\npert in urban agriculture, offers a\ngenuine solution to today\u2019s global\nfood crisis. By [...]\nwhich adopted our buttons such\nthat when we went to Boston.com\n(part of NY times) branding was\nnot part of our discussions.\nOf\ncourse, we had matured in our\nthinking and offered them a co-\nbranded offer hosted by Coola.\nWhen Switchboard did not work\nfor us, we went to their compe-\ntition Infospace.com, which was\nmuch larger than them. They ac-\ncepted a branded Coola button\nbut offered a complex deal [...]\nTrend.com: I had no idea this was\ncoming. There\u2019d been talk over\nthe years about setting up a sort\nof business portal that integrated\nall of Trend\u2019s regular and an-\nnual publications, but there was\nnever enough momentum to ac-\ntually get it going.\nTrend had\na regular spot on the Times\u2019 on-\nline Business section, but it was\na pretty low-impact thing (even\nthough quite a bit of traffic would\ncome to the [...]\n24\nTable 7: Samples from different distribution subsets using memorization of a 124M reference model\ntrained on CommonCrawl.\nMem. Factor = 0\nMem. Factor = 0.5\nMem. Factor = 1.0\ndoesn\u2019t prevent you from clearly\nseeing the road.\nHi, thank you\nso much for your words, appreci-\nate it! Moreover, we noted your\ncomments, we\u2019ll think what can\nbe done, for sharing more ideas,\nfeel free to contact us at sup-\nport@hudwayapp.com any time.\nHappy to help you always! I do\na lot of mudding. And it\u2019s got a\npitch and roll gauge, which I like\nwhen I\u2019m in the hole, do I don\u2019t\nflip my truck. [...]\n160 countries.\nThere are abun-\ndant hot-selling projects accessi-\nble to you. Cheap and environ-\nmentally friendly: Factory-direct\nsale, fast delivery with guaran-\nteed quality at factory price, in\nline with the concept of environ-\nmental development. Feb 19 2021\nshould\npelletisation\nof\nsulfide\nsolidelectrolytesafterball milling-\nhas to be done in argon atmo-\nsphere question 7 answers i am\nusing a spex 8000b [...]\nreference. My company\u2019s NACHI\n230/600E bearing price conces-\nsions, adequate inventory, and\nother similar products are avail-\nable for recommendation 1 . Less\nthan 45 KGS, we will send by\nexpress. (Door to Door, Conve-\nnient) 2 . 45 - 200 KGS , we will\nsend by air transport . (Fastest\nand safest, but expensive) 3 .\nMore than 200 KGS, we will send\nby sea . ( Cheapest and common\nuse ) The bearing 240/8 [...]\ndisposal and processing of con-\ntaminated suspensions such as\ndrilling mud, road sweepings and\nsimilar.\nThe rising demand on\nthe international market to meet\ncurrent as well as future envi-\nronmental regulations is the main\ndriver for the development in this\narea of our work,\" explains Man-\naging Director Ing. Mag. Erich\nTrunkenpolz.\n\"The plants are\ncurrently developed for station-\nary and semi-mobile du [...]\n$97 monthly subscription pack-\nage.\nIf you decide to make an\nannual payment of $997, you get\ntwo free months. I started with\nthis basic package but I later de-\ncided to upgrade to Etison Suite\nsince this one has some limita-\ntions. As a marketer, I was only\nallowed to use 3 custom domains,\nget a limit of 20,000 visitors, and\nmake a maximum of 100 web\npages. I discovered that some ad-\nvanced features are [...]\ntakes your bank to process our\nrefund request (5 to 10 business\ndays). If you need to return an\nitem, simply login to your ac-\ncount, view the order using the\n\u2019Complete Orders\u2019 link under the\nMy Account menu and click the\nReturn Item(s) button. We\u2019ll no-\ntify you via e-mail of your refund\nonce we\u2019ve received and processed\nthe returned item. We can ship\nto virtually any address in the\nworld. Note the [...]\ntime:If you\u2019re looking into faster-\nthan-light fiber internet, there\u2019s a\nVerizon Fios deal for you in Silver\nSpring, MD. Want more than a\nVerizon Fios internet-only plan?\nOpen your home up to more en-\ntertainment choices with Verizon\nFios packages. Ready to improve\nyour home with the best internet\navailable?\nGet lightspeed inter-\nnet with Verizon plans that suit\nevery lifestyle. Whether you only\nneed [...]\nSelect options that apply then\ncopy and paste the RDF/HTML\ndata fragment to include in your\napplication Note:\nAdjust the\nwidth and height settings defined\nin the RDF/HTML code frag-\nment to best match your require-\nmentsCause.\u2014Upon the ascen-\nsion of William and Mary to the\nthrone of England, the Protes-\ntants of Maryland demanded the\nColonial management of the Ter-\nritory. The Roman Catholics, af-\nter rep [...]\nto\nassess\nthe\nsuccess\nof\nour\nmarketing and advertising cam-\npaigns).\nFinally, we may also\nshare your Personal Information\nto comply with applicable laws\nand regulations, to respond to\na subpoena, search warrant or\nother lawful request for informa-\ntion we receive, or to otherwise\nprotect our rights. Additionally,\nyou can opt out of some of these\nservices by visiting the Digital\nAdvertising Alliance [...]\n25\n"
  },
  {
    "title": "Optimize Weight Rounding via Signed Gradient Descent for the Quantization of LLMs",
    "link": "https://arxiv.org/pdf/2309.05516.pdf",
    "upvote": "8",
    "text": "Preprint. Under review.\nOPTIMIZE WEIGHT ROUNDING VIA SIGNED GRADI-\nENT DESCENT FOR THE QUANTIZATION OF LLMS\nWenhua Cheng\u2217, Weiwei Zhang, Haihao Shen, Yiyang Cai, Xin He & Kaokao Lv\nIntel\nABSTRACT\nLarge Language Models (LLMs) have proven their exceptional capabilities in\nperforming language-related tasks. However, their deployment poses significant\nchallenges due to their considerable memory and storage requirements. In re-\nsponse to this issue, weight-only quantization, particularly 3 and 4-bit weight-\nonly quantization, has emerged as one of the most viable solutions.\nAs the\nnumber of bits decreases, the quantization grid broadens, thus emphasizing the\nimportance of up and down rounding.\nWhile previous studies have demon-\nstrated that fine-tuning up and down rounding with the addition of perturba-\ntions can enhance accuracy in some scenarios, our study is driven by the pre-\ncise and limited boundary of these perturbations, where only the threshold for\naltering the rounding value is of significance. Consequently, we propose a con-\ncise and highly effective approach for optimizing the weight rounding task. Our\nmethod, named SignRound, involves lightweight block-wise tuning using signed\ngradient descent, enabling us to achieve outstanding results within 400 steps.\nSignRound competes impressively against recent methods without introducing\nadditional inference overhead.\nThe source code will be publicly available at\nhttps://github.com/intel/neural-compressor soon.\n1\nINTRODUCTION\nLarge language models (LLMs) have demonstrated exceptional proficiency on language-related\ntasks(OpenAI; Touvron et al., 2023a). Nevertheless, the deployment of LLMs presents notable\nhurdles due to their extensive memory and storage needs. Moreover, the computational demands of\nthese models leads to the challenges for real-time applications. Consequently, it becomes imperative\nto explore techniques like quantization to facilitate the efficient deployment of LLMs.\nQuantization techniques can be broadly classified into two categories: quantization-aware training\n(QAT) (Esser et al., 2019; Zhuang et al., 2021; Lee et al., 2021; Liu et al., 2023) and post-training\nquantization (PTQ) (Nagel et al., 2019; Xiao et al., 2022; Frantar et al., 2022; Nagel et al., 2020).\nQAT involves training the model with quantization in mind. During QAT, the model is trained using\nsimulated lower-precision representations, allowing it to learn and adapt to the effects of quanti-\nzation. This approach often yields better accuracy compared to PTQ. However, QAT comes with\ncertain drawbacks, including increased training complexity, longer training times, and the need to\ntune hyperparameters. Applying QAT to LLMs can be particularly costly, despite recent efforts\n(Hu et al., 2021; Dettmers et al., 2023) to improve the efficiency of fine-tuning LLMs. In contrast,\nPTQ directly quantizes the model without any simulated training or fine-tuning. While PTQ is a\nconcise approach, it is susceptible to significant accuracy drops. This highlights the need for further\nadvancements in PTQ methods to enhance their accuracy preservation capabilities.\nTwo types of tensors could be quantized: activations and weights. Weight-only quantization has\ngained prominence in recent times as it offers a favorable tradeoff for LLMs. Quantizing activations\nfor LLMs can be challenging (Wei et al., 2022b; Xiao et al., 2023; Bondarenko et al., 2023), making\nweight-only quantization a more practical choice. Additionally, the primary bottleneck in generating\nnew tokens for LLMs often lies in memory bandwidth (Kim et al., 2023), further emphasizing the\nsignificance of weight-only quantization. In this work, we only focus on weight only quantization.\n\u2217Corresponding author wenhua.cheng@intel.com\n1\narXiv:2309.05516v2  [cs.CL]  28 Sep 2023\nPreprint. Under review.\nIn order to quantize the weights, a rounding operation is necessary, with rounding-to-nearest (RTN)\nbeing the predominant method. RTN quantizes each element independently by simply rounding it to\nthe nearest integer. However, RTN fails to consider the relationships between weights and weights,\nas well as weights and activations. The potential of an advanced rounding strategy to improve\naccuracy has been initially demonstrated by Nagel et al. (Nagel et al., 2020). They addressed\nthe rounding task by formulating it as a quadratic unconstrained binary optimization problem and\napproximated the task loss by employing a Taylor series expansion. However, relying exclusively\non the second-order term may not produce accurate results. This is because rounding can introduce\nconsiderable weight modifications that may make other order terms significant and non-negligible.\nWe prefer the signed gradient descent method to effectively tackle the issue of sub-optimal rounding\nsolutions. This approach is inspired by the well-defined boundaries of the solution space, which are\nconfined to the range of [-0.5, 0.5], where only the threshold for altering the rounding value is of\nsignificance. Figure 1 provides an overview of our method, SignRound. It utilizes signed gradient\ndescent to fine-tune the up and down rounding through block-wise output reconstruction, resulting\nin enhanced flexibility and faster convergence. Our contributions are primarily threefold:\n\u2022 We introduce a succinct and potent method for optimizing the weight-rounding task. Our\napproach utilizes a minimal amount of unlabeled data and executes quantization in a block-\nwise fashion. Moreover, it is worth noting that our method does not introduce any addi-\ntional overhead during inference, further enhancing its general practicality.\n\u2022 Our findings demonstrate that a mere alteration of approximately 5% of the rounding values\ncan significantly enhance the performance of some quantization models.\n\u2022 Our empirical results exhibit substantial performance enhancements over the established\nbaseline of RTN, and our method contends favorably against recent techniques.\n2\nRELATED WORK\nQuantization Aware Training.\nQAT methods have gained widespread popularity in model com-\npression, as they enable the fine-tuning process, often leading to superior accuracy compared to the\nPTQ method. In their work, (Esser et al., 2019) proposed a novel approach that estimates and scales\nthe task loss gradient at each weight and activation layer\u2019s quantizer step size, allowing for joint\nlearning with other network parameters. (Zhuang et al., 2021) put forward a progressive quantiza-\ntion scheme that involves quantizing activations after weights. Additionally, CPQ (Lee et al., 2021)\neffectively identified the optimal quantization grids while naturally encouraging the underlying full-\nprecision weights to gather around those quantization grids cohesively during training. While QAT\nmethods are popular in relatively small-scale models, their application in LLMs is limited due to the\nhigh computational cost associated with training or fine-tuning.\nPost-training Quantization (PTQ).\nPTQ methods simplify the quantization process without the\nneeds of additional training. (Nagel et al., 2019) focused on minimizing quantization error through\nweight equalization and bias correction techniques. (Liu et al., 2021) specifically addressed the\nquantization of vision transformers, introducing a ranking loss to preserve the relative order of self-\nattention results after quantization and exploring a mixed-precision quantization scheme. (Frantar\n& Alistarh, 2022) leveraged Optimal Brain Surgeon (Hassibi et al., 1993) to tune weights during\nmodel compression. Both Hawq (Yao et al., 2021) and HAQ (Wang et al., 2019) aimed to identify\nimportant layers and maintain higher precision for them. Given its low resource requirement, PTQ\nis particularly suitable for the quantization of Large Language Models (LLMs). We will next focus\non the quantization methods designed for LLMs, most of which fall under the category of PTQ.\nLarge Language Models Quantization.\nSignificant advancements have been made in addressing\nthe pressing demand for quantizing large language models (LLMs). LLM.int8() (Dettmers et al.,\n2022) introduced a mixed precision approach to preserve essential channels in high precision. Zero-\nQuantV2 (Yao et al., 2023) employed low-rank matrices to enhance model quality recovery. RPTQ\n(Yuan et al., 2023) mitigated the impact of range differences between channel by rearranging the\nchannels and quantizing them in clusters. Other methods, such as SPIQ (Yvinec et al., 2023),\nSmoothQuant (Xiao et al., 2022), Outlier Suppression+ (Wei et al., 2023), utilized handcrafted\nequivalent transformations to mitigate quantization errors. While these approaches are effective,\n2\nPreprint. Under review.\ntheir applicability is limited due to the performance overhead involved during inference, because\nthere is no chance to fuse the transformation scale to the model itself on certain model architectures.\nLLM-QAT (Liu et al., 2023) employs QAT to enhance the performance of W4A8. In the context of\nweight-only quantization, GPTQ (Frantar et al., 2022) optimized weights using the Optimal Brain\nSurgeon (Hassibi et al., 1993) technique, achieving low-bit quantization on LLMs with minimal\ncomputational overhead. AWQ (Lin et al., 2023) followed the equivalent transformation approach\nwith additional tuning in a constrained space, and has the similar limitations as SmoothQuant (Xiao\net al., 2022). SqueezeLLM (Kim et al., 2023) employed sensitivity-based non-uniform quantization\nand dense-and-sparse decomposition to achieve lossless compression to ultra-low precision. While\nrecent advancements in LLM quantization have made significant progress, there is still room for\nimprovement in achieving minimal quantization loss without introducing inference overhead.\nRounding Methods.\nAdaptive Rounding (Nagel et al., 2020) has already showcased the potential\nof an advanced rounding strategy to enhance accuracy (Li et al., 2021; Wei et al., 2022a). They\nused the rounding task as a quadratic unconstrained binary optimization problem by approximating\nthe task loss through a Taylor series expansion. However, considering only the second-order term\nmay not yield accurate results. This is because the rounding value gets multiplied by a scaling co-\nefficient during de-quantization, potentially introducing significant weight changes that make other\norder terms non-negligible. FlexRound (Lee et al., 2023) introduces a more flexible approach to\nrounding by incorporating element-wise division. This allows for simultaneous learning of a shared\nquantization grid size and individual scales for each pre-trained weight. However, it\u2019s not easily\nscalable to apply to LLMs due to the needs of specialized hyperparameters for each specific model\nand task. AQuant (Li et al., 2022) introduced a dynamic approach where the border becomes a func-\ntion dependent on the activation value to reduce the quantization error of activation. We specifically\nconcentrate on the up and down rounding task for weight quantization in this work.\nSigned Gradient Descent.\nSigned gradient descent is not commonly utilized and is typically ap-\nplied in specific scenarios, such as reducing communication costs. This is because signed gradient\ncarries significantly less information compared to original gradient. Recent studies have shed light\non the advantages of sign-based methods over gradient descent in certain conditions. Safaryan et al.\n(Safaryan & Richt\u00b4arik, 2021) found that sign-based methods are preferable when the Hessian matrix\nis concentrated on its diagonal and the maximal eigenvalue is much larger than the average eigen-\nvalue. Li et al. (Li et al., 2023) investigated a variant of sign-based gradient descent that exhibits\nfaster convergence. Additionally, Safaryan et al. (Safaryan & Richt\u00b4arik, 2021) proposed a stochastic\nsign descent with momentum, which converges under the standard bounded variance assumption\nwith the optimal asymptotic rate. These findings contribute to a better understanding of the potential\nbenefits and applications of signed gradient descent methods.\n3\nMETHODOLOGY\nWe provide an overview of quantization before diving into the details of our approach. To quantize\nand de-quantize the weights, the following operation as shown in Eq.1 is used (disregarding zero\npoint for simplicity).\nf\nW = s \u2217 clip(\n\u0016W\ns\n\u0019\n, n, m), n, m \u2208 N\n(1)\nwhere s is the quantization scale, which is a positive scalar value. However, it is important to mention\nthat our method can be easily extended to cases where s is a vector or tensor. And the rounding\noperation \u230a\u00b7\u2309 is typically performed using the RTN method. While RTN is a concise approach, it\nquantizes each element independently, thereby losing the ability to model the correlation among\ndifferent weights or activations.\nTo introduce more flexibility into the rounding operation, a tensor V with the same shape of W is\nintroduced. Each element of V falls within the range of [\u2212B, B], in which B is set to 0.5 in all of\nexperiments to ensure that the changes made only impact the rounding value.\nf\nW = s \u2217 clip(\n\u0016W\ns + V\n\u0019\n, n, m), n, m \u2208 N\n(2)\n3\nPreprint. Under review.\nFigure 1: An illustration of SignRound. Unlike the direct rounding in RTN, SignRound performs\nsigned gradient descent to fine-tune the up and down rounding through block-wise output recon-\nstruction. After lightweight forward and backward steps, WINT4 has been well optimized towards\nthe minimal loss, therefore ready for the final inference deployment. Note that Quant and Dequant\nare two standard operations for quantization and dequantization respectively.\nThis adjustment allows for a more adaptable and context-aware quantization process. If we try to\nreconstruct the output of layers, the loss could be formulated as\nL = ||WX \u2212 f\nWX||2\nF\n(3)\nwhere X is the input of the layer and ||\u00b7||F denotes the Frobenius norm. Then the final optimization\ntask is described as the following\narg min\nV\n||WX \u2212 f\nWX||2\nF\n(4)\n3.1\nSIGNROUND\nSince V has a clear boundary, i.e. [\u22120.5, 0.5], and only the threshold for altering the rounding value\nis of significance, we prefer scaled signed gradient descent instead of normal gradient descent to\noptimize this task. Figure 1 shows an illustration of our method. More precisely, we follow the\nbelow optimization to approach the sub-optimal solution of Eq. 4.\nVt+1 = Vt \u2212 lr \u2217 sign( \u2202L\n\u2202V )\ns.t.|\nX\nt\nlr \u2217 sign( \u2202L\n\u2202V )| \u2264 B\n(5)\nwhere t is the optimizing step, lr is the learning rate, | \u00b7 | is the absolute operation and B is the\nboundary we use, which is set to 0.5 in all our experiments.\nFurther, by employing straight-through estimator (STE) (Bengio et al., 2013), it can be easily demon-\nstrated that sign( \u2202L\n\u2202V ) = sign( \u2202L\n\u2202W ) in Eq. 5 as following since elements of s are all positive.\n\u2202L\n\u2202W = \u22122(WX \u2212 f\nWX)XT\n(6)\n\u2202L\n\u2202V = \u22122s(WX \u2212 f\nWX)XT\n(7)\nSo our optimization could be simplified as\nVt+1 = Vt \u2212 lr \u2217 sign( \u2202L\n\u2202W )\ns.t.|\nX\nt\nlr \u2217 sign( \u2202L\n\u2202W )| \u2264 B\n(8)\n4\nPreprint. Under review.\nAlgorithm 1 SignRound\nInput: Calibration Data D, learning rate lr, total steps T, Model M, block module mw with weights\nw, zero initialized V , batch size bs\nOutput: best V\n1: V \u2190 0, best V \u2190 0, best l \u2190 maximum\n2: for i \u2190 0 to T do\n3:\nd \u2190 draw bs samples from D\n4:\nx \u2190 M(d)m\n\u25b7 get the inputs of m\n5:\nyf \u2190 mw(x)\n\u25b7 get the output of original module\n6:\new \u2190 qdq(w, V )\n\u25b7 quantize and dequantize w via Eq.2\n7:\nyq \u2190 m e\nw(x)\n\u25b7 get the output of quantized module\n8:\nloss \u2190 mse(yq, yf)\n\u25b7 get the loss via Eq.3\n9:\nif loss < best l then\n10:\nbest V \u2190 V\n11:\nbest l \u2190 loss\n12:\nend if\n13:\nloss.backward()\n14:\nupdate V via Eq. 8\n15: end for\nMoreover, as Eq 3 averages the loss of each element, which presumes that each one contributes\nequally to the network, that basically is not true. To alleviate this issue, we optimize the rounding\ntask blockwise. To clarify, in our context, we use the term \u2019layer\u2019 to refer to a linear/convolution\nlayer, while \u2019block\u2019 denotes a transformer block that typically consists of several linear layers.\nThe above pseudocode 1 presents more details of SignRound.\n4\nEXPERIMENTS\nIn this section, we conduct a comprehensive evaluation of SignRound from various perspectives.\nFirstly, we provide a brief overview of the LLM architectures and tasks that are included in our\nevaluation. Secondly, we present a detailed comparison between our method and some other existing\napproaches, highlighting the unique features and advantages of SignRound. Thirdly, we conduct\nadditional experiments to further demonstrate the validity of our choices, assess the sensitivity of\nhyperparameters, and explore other relevant factors. Finally, the runtime is reported in Appendix D\nfor reference.\n4.1\nEXPERIMENTAL SETTINGS\nEvaluation and Datasets.\nWe make assessments on several language tasks to satisfy the task-\nagnostic setting. Specifically, we report average accuracy results on four common sense reasoning\ntasks including HellaSwag (Zellers et al., 2019), WinoGrande (Sakaguchi et al., 2021), PIQA (Bisk\net al., 2020) and LAMBADA (Paperno et al., 2016).Additionally, we benchmarked our models on\nMMLU (?), which encompasses 57 tasks spanning STEM, humanities, social science, and more.\nEvaluation for all these tasks was performed using the lm-eval-harness (Gao et al., 2021). Fur-\nthermore, we complement our evaluation with perplexity (ppl) analysis on Wikitext2 (Merity et al.,\n2016) and C4 (Raffel et al., 2020), by following the source code 1 of GPTQ.\nQuantization Configurations.\nIn line with the approach taken in GPTQ (Frantar et al., 2022), we\nspecifically concentrate on weight-only quantization, targeting the linear layers within transformer\nblocks. Other layers, such as the embedding layer and typically the last layer like lm-head, are\nexcluded from the quantization process. We initially intended to utilize the pile (?) dataset for\ncalibration, following AWQ (Lin et al., 2023) and SmoothQuant (Xiao et al., 2022). However, due\nto its large size, we have opted to use the readily available pile-10k dataset 2, which consists of the\n1https://github.com/IST-DASLab/gptq\n2https://huggingface.co/datasets/NeelNanda/pile-10k\n5\nPreprint. Under review.\nfirst 10k samples from pile, for both GPTQ and our method. We employ standard uniform per-row\nasymmetric quantization on the min-max grid. Our evaluation primarily focuses on W4, W4G128,\nand W3G128, where W4 indicates quantizing weights with 4 bits and G represents finer-granularity\ngrouping as described in (Park et al., 2022; Frantar et al., 2022).\nLarge Language Models.\nOur experimental evaluation encompasses a range of widely adopted\nLLM architectures, such as LLaMAs (Touvron et al., 2023a), LLaMAs v2 (Touvron et al., 2023b),\nBLOOMs (Scao et al., 2022), and OPTs (Zhang et al., 2022). We cover a wide range of LLM\nparameters, ranging from millions to billions, to ensure comprehensive coverage and analysis.\nSignRound Hyperparameters.\nWe selected 512 samples randomly from pile-10k and truncated\neach sample to a sequence length of 512. The tuning process involves adjusting each block for 400\nsteps using a learning rate of 2.5e-3, a batch size of 8, and employing a linear learning rate decay. We\nset the value of B in Eq. 8 to 0.5. Besides, we adopted automatic mixed precision(AMP) to acceler-\nate the tuning. It\u2019s worth noting that adjusting the sequence length to 2048 yielded improvements in\nnumerous scenarios. However, we did not adopt this as the default setting due to the associated run-\ntime overhead. For models \u2265 30B, we made configuration adjustments to strike a balance between\nruntime and performance. Specifically, we reduced the sample count to 256, shorted the sequence\nlength to 256, and disabled AMP.\nTable 1: Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA for LLaMA &\nOPT.\nnbits\nmethods\nLLaMA\nOPT\n7b\n13b\n7bv2\n13bv2\n125m\n1.3b\n2.7b\n6.7b\n13b\n16\nFP16\n68.80\n71.14\n69.02\n71.20\n45.09\n57.66\n61.04\n64.92\n65.49\nW4\nRTN\n67.38\n68.82\n66.98\n70.17\n39.41\n47.22\n58.61\n62.99\n64.08\nGPTQ\n64.70\n70.00\n66.89\n69.24\n43.58\n56.15\n59.92\n63.09\n64.83\nOurs\n68.05\n70.58\n67.74\n70.03\n44.13\n56.17\n60.58\n64.34\n65.05\nW4G128\nRTN\n67.85\n70.84\n68.32\n70.72\n45.27\n56.47\n60.70\n64.03\n64.84\nGPTQ\n66.32\n70.92\n68.90\n70.68\n42.88\n56.99\n61.23\n64.75\n65.37\nOurs\n68.09\n71.43\n68.65\n70.81\n44.23\n57.30\n60.86\n64.76\n65.67\nW3G128\nRTN\n64.94\n67.70\n65.92\n68.70\n39.11\n42.61\n36.99\n56.09\n49.56\nGPTQ\n58.29\n68.73\n65.51\n68.73\n39.78\n54.43\n58.47\n62.98\n64.68\nOurs\n66.62\n69.59\n66.88\n69.70\n43.31\n55.46\n59.12\n53.42\n63.61\nTable 2: Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA for BLOOM.\nW4\nW4G128\nW3G128\nSize\n560m\n1b7\n3b\n7b1\n560m\n1b7\n3b\n7b1\n560m\n1b7\n3b\n7b1\nFP16\n45.50\n52.31\n55.48\n60.22\n45.50\n52.31\n55.48\n60.22\n45.50\n52.31\n55.48\n60.22\nRTN\n43.10\n49.97\n53.16\n57.73\n44.28\n52.08\n54.86\n59.31\n40.83\n47.98\n52.51\n57.59\nGPTQ\n43.95\n50.91\n54.65\n58.27\n44.79\n52.08\n55.68\n59.59\n42.74\n48.81\n53.41\n58.12\nOurs\n45.00\n51.47\n54.63\n59.52\n45.40\n51.85\n55.40\n59.83\n44.08\n50.52\n53.64\n58.69\n4.2\nCOMPARING WITH OTHER METHODS\nWe conducted a comprehensive benchmarking of our results against RTN and GPTQ (Frantar et al.,\n2022). However, it is important to highlight that act-order was not enabled in GPTQ due to the\nkernel overhead it introduces (Lin et al., 2023), although it has the potential to improve accuracy for\ncertain models. When evaluating perplexity (ppl), we prioritize reporting the ppl on C4 dataset as\nour primary focus, taking into consideration the potential occurrence of NaN values when assessing\nperplexity for Wikitext2 and ptb datasets, both for SignRound and GPTQ. Furthermore, we con-\nducted a limited and non-rigorous comparison between our approach and AWQ Lin et al. (2023) in\nAppendix A.1.\nWe begin by presenting the average accuracy results for the HellaSwag, WinoGrand, PIQA, and\nLAMBADA tasks across LLaMA, OPT, and BLOOM models with a size below 13B. These results\n6\nPreprint. Under review.\nTable 3: C4 ppl ( \u2193) at W4.\nLLaMA\nOPT\nBLOOM\nSize\n7b\n13b\n7bv2\n13bv2\n1.3b\n2.7b\n6.7b\n13b\n560m\n1b7\n3b\n7b1\nFP16\n7.34\n6.80\n7.26\n6.73\n16.07\n14.34\n12.71\n12.06\n26.59\n19.49\n17.48\n15.20\nRTN\n8.12\n7.23\n8.16\n7.14\n27.49\n18.83\n14.37\n13.32\n29.87\n21.25\n18.76\n16.05\nGPTQ\n8.64\n7.13\n7.90\n6.87\n17.04\n15.06\n13.39\n12.29\n28.15\n20.71\n18.18\n15.67\nOurs\n7.84\n7.05\n11.20\n7.72\n16.92\n14.97\n13.08\n12.48\n28.12\n20.41\n18.18\n15.67\nTable 4: Accuracies(\u2191) of MMLU(5-shot) for LLaMA-7B & LLaMA-7B-V2. \u201dOurs-2048\u201d indi-\ncates that we have modified the sequence length of the calibration dataset from 512 to 2048.\nLLaMA-7B\nLLaMA-7B-V2\nHums. STEM Social Other Avg. Hums. STEM Social Other Avg.\nFP16\n38.32\n31.17\n38.05 36.85 35.64 51.40\n37.00\n52.23 49.51 46.56\nW4G-1\nRTN\n34.84\n29.53\n32.87 36.28 33.10 44.03\n32.83\n44.97 42.19 40.24\nGPTQ\n33.31\n26.29\n29.86 33.11 30.32 46.21\n34.29\n46.68 44.85 42.21\nOurs\n34.30\n31.05\n34.74 36.66 33.95 47.28\n33.14\n46.90 44.70 42.10\nOurs2048 35.10\n30.69\n36.43 36.85 34.42 47.40\n33.92\n49.61 44.91 43.00\nW4G128\nRTN\n36.30\n31.67\n37.40 37.99 35.48 49.54\n36.50\n50.95 47.87 45.31\nGPTQ\n37.77\n29.64\n36.38 37.45 34.83 50.30\n36.51\n50.91 47.69 45.43\nOurs\n36.06\n30.86\n35.99 36.21 34.44 51.39\n37.87\n52.56 49.69 46.95\nOurs2048 35.66\n30.05\n36.16 37.57 34.46 50.12\n36.70\n51.44 48.20 45.69\nW3G128\nRTN\n32.97\n30.28\n33.66 32.60 32.17 41.14\n33.06\n40.98 40.94 38.51\nGPTQ\n30.77\n28.29\n30.73 31.33 30.12 44.66\n37.55\n46.36 43.47 42.48\nOurs\n30.12\n28.21\n30.64 30.34 29.68 44.53\n33.53\n44.60 43.52 40.82\nOurs2048 32.43\n28.62\n31.03 32.10 30.85 42.75\n32.98\n42.88 41.30 39.34\nare shown in Table 1 and 2. In conclusion, our method outperforms RTN in 36 out of 39 scenarios,\nshowcasing its effectiveness. Additionally, when comparing our approach to GPTQ, we surpass\nit in 32 out of 39 scenarios, further highlighting the strengths of our method. While our method\nshowcases overall effectiveness, it is important to acknowledge the presence of outliers, such as\nOPT6.7B at W3G128. Although the root cause for this has not been identified yet, it could be\nmitigated by fine-tuning hyperparameters, as discussed in the following sections. For detailed results\nof LLaMA7B, LLaMA13B, LLAMA7B-V2, and LLAMA13B-V2, please refer to Appendix E.\nThe results in Appendix E also highlight that changing the sequence length to 2048 could bring\nnoticeable improvement in many scenarios.\nWe then present the perplexity (ppl) results for C4 in Table 3, along with the detailed results for\nWikitext2 in Appendix A.2. In conclusion, we achieve better or comparable performance in 9 out\nof 12 models. In certain cases where the results may not be optimal, we can still fine-tune the\nhyperparameters to achieve better results, as demonstrated in the subsequent sections.\nNext, we present a comprehensive breakdown of the accuracies achieved by MMLU for LLaMA-\n7B and LLaMa-7B-V2 in Table 4. By analyzing the average accuracies, we observe that SingRound\noutperforms RTN and GPTQ in 4 out of the 6 scenarios when the best model-wise setting is applied.\nWe also provide the results for models with a capacity of 30B or greater at W3G128 in Table 5 and\nW4 in Appendix A.3. Additionally, we discovered that recovering the sequence length to 512 of\nthe calibration dataset yielded improvements in certain scenarios, and thus we include these results.\nIn summary, our approach achieves comparable performance to GPTQ for the given accuracy task.\nHowever, we slightly lag behind GPTQ in terms of ppl tasks.\n4.3\nBLOCK-WISE VERSUS LAYER-WISE\nWe examined the effects of layer-wise and block-wise tuning. As explained in Section 3.1, the term\n\u201dlayer\u201d refers to a linear/convolution layer, while \u201dblock\u201d specifically denotes a transformer block\nconsisting of multiple linear layers. To simplify this evaluation, we set the sequence length to 256\n7\nPreprint. Under review.\nTable 5: Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA and C4 ppl(\u2193)\nfor LLaMA & OPT with \u2265 30B at W3G128. \u201dOurs-seq512\u201d indicates that we have modified the\nsequence length of the calibration dataset from 256 to 512.\nAccuracy\nPPL on C4\nType\nLLaMA\nOPT\nLLaMA\nOPT\nSize\n30b\n65b\n30b\n66b\n30b\n65b\n30b\n66b\nFP16\n73.46\n75.48\n67.87\n69.54\n6.13\n5.98\n11.46\n10.99\nRTN\n72.17\n73.69\n62.83\n38.00\n6.85\n6.52\n30.81\n285.41\nGPTQ\n72.09\n73.97\n66.76\n67.87\n6.80\n6.52\n11.74\n11.87\nOurs-seq256\n72.45\n73.71\n66.51\n68.00\n6.83\n6.52\n13.00\n13.34\nOurs-seq512\n71.95\n73.78\n66.70\n67.26\n6.79\n6.53\n12.50\n13.97\nand disable AMP. Based on the below results, block-wise tuning outperformed layer-wise tuning in\nthe majority of scenarios.\nTable 6: Comparing block-wise and layer-wise tuning for around 7B models, the models LLaMA7b,\nLLaMA7bv2, OPT6.7b, and BLOOM7b1 are denoted by 7b, 7bv2, 6.7b, and 7b1 respectively. The\naccuracy is the % average accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA . Perplex-\nity (PPL) (\u2193) is evaluated using the C4 dataset.\nW4\nW3G128\nSize\n7b\n7bv2\n6.7b\n7b1\n7b\n7bv2\n6.7b\n7b1\nlayer-acc-seq256\n67.50\n67.78\n63.46\n58.72\n65.96\n66.09\n61.60\n58.24\nblock-acc-seq256\n67.64\n67.96\n64.55\n59.08\n66.31\n66.63\n57.76\n58.34\nlayer-c4-ppl-seq256\n8.02\n7.92\n13.44\n15.73\n8.81\n8.69\n16.83\n16.15\nblock-c4-ppl-seq256\n7.81\n8.19\n13.10\n15.71\n8.34\n10.84\n25.44\n16.05\n4.4\nTHE ANALYSIS OF HYPERPARAMETERS SENSITIVITY\nWe conducted a hyperparameters sensitivity analysis, the results of which are summarized in Table\n7. In the \u201dsteps100\u201d configuration, we used 100 steps, and a learning rate of 1e-2. In the \u201dlr4e-3\u201d\nconfiguration, we set the learning rate to 4e-3. We also changed the sequence length of the calibra-\ntion dataset from 512 to 2048, denoted by \u201dseq2048\u201d. Please note that all other hyperparameters\nnot mentioned in each configuration were kept the same as the default configurations, as detailed in\nSection 4.1. Overall, our method exhibits robustness to hyperparameters in common sense reasoning\ntasks, with the exception of the perplexity of LLaMA-7b-v2. However, we did discover that certain\nhyperparameters, such as the sequence length of the calibration dataset, can significantly impact\nperformance in some scenarios, as demonstrated in Table 4 and 5.\nTable 7: Hyperparameter sensitivity analysis, the models LLaMA7b, LLaMA7bv2, OPT6.7b, and\nBLOOM7b1 are denoted by 7b, 7bv2, 6.7b, and 7b1 respectively. The accuracy is the % average\naccuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA . Perplexity (PPL) (\u2193) is evaluated\nusing the C4 dataset.\nAccuracy\nPPL on C4\nSize\n7b\n7bv2\n6.7b\n7b1\n7b\n7bv2\n6.7b\n7b1\nsteps100\n67.53\n67.76\n64.64\n58.76\n7.93\n7.83\n13.12\n15.71\nlr4e-3\n68.01\n67.57\n64.57\n59.47\n7.81\n10.29\n13.09\n15.66\nseq2048\n68.11\n67.79\n64.32\n59.39\n7.76\n9.97\n13.06\n15.66\ndefault\n68.05\n67.74\n64.34\n59.52\n7.84\n11.20\n13.08\n15.67\n4.5\nTHE ANALYSIS OF GRADIENTS AND THEIR EFFECTS ON ROUNDING\nIn this analysis, we dive into the distribution of the magnitude of V in Eq. 2 and its impact on\nrounding values across approximately 7 billion models at W4. The visual representations of these\n8\nPreprint. Under review.\nLLaMA-7B\nLLaMA-7B-V2\nOPT-6.7B\nBLOOM-7B1\nFigure 2: The impact of the rounding value introduced by the V in Eq. 2\ndistributions are provided in Appendix B. Our investigation reveals that the majority of V values\nare concentrated within the range of [-0.3, 0.3]. Additionally, we observe an interesting pattern\nin the distribution of V across different layers. The middle layers exhibit a more tightly clustered\ndistribution compared to the other layers. This observation aligns with the common understanding\nthat the head and tail layers tend to be more sensitive to compression, while the middle layers are\nrelatively more robust.\nFigure 2 illustrates the impact of the rounding value introduced by the V in Eq. 2 for models around\n7B at W4. The red line represents \u201dup rounding\u201d, indicating that while RTN rounds the value to the\nfloor, SignRound changes it to the ceiling. Conversely, the green line represents \u201ddown rounding\u201d\nindicating that while RTN rounds the value to the ceiling, SignRound changes it to the floor. It is\nworth noting that SignRound modifies only a small percentage of weight rounding values for each\nof the four models, namely 5.27%, 5.29%, 4.14%, and 4.10%.\nWe were also intrigued by the possible correlation between rounding and activation, as previous\nresearch has shown that keeping only 0.1%-1% of the channels corresponding to larger activation\ncan significantly improve the quantized performance in AWQ (Lin et al., 2023). We shown the result\nin Appendix C.\n5\nCONCLUSIONS AND LIMITATIONS\nIn this paper, we present a highly effective and concise approach to optimize the weight rounding\ntask. Our method, SignRound, leverages lightweight block-wise tuning using signed gradient de-\nscent, achieving remarkable results within a mere 400 steps. Extensive experiments demonstrate the\nsuperior performance of our approach. As part of our future work, we plan to apply our approach\nto more diverse LLM models (e.g., Code LLaMA (Rozi`ere et al., 2023), LLaMA v2 Chat (Tou-\nvron et al., 2023b)), and contribute our recipes and implementations to the open source community.\nOn the other hand, although our method is generally effective, there are a few outliers in certain\nscenarios, where we plan to mitigate the issue by fine-tuning the hyperparameters.\n9\nPreprint. Under review.\nREFERENCES\nYoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville.\nEstimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence,\nvolume 34, pp. 7432\u20137439, 2020.\nYelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing\noutliers by helping attention heads do nothing. arXiv preprint arXiv:2306.12929, 2023.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix\nmultiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy, and Dharmen-\ndra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153, 2019.\nElias Frantar and Dan Alistarh. Optimal brain compression: A framework for accurate post-training\nquantization and pruning. arXiv preprint arXiv:2208.11580, 2022.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training\nquantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence\nGolding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric\nTang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot lan-\nguage model evaluation, September 2021. URL https://doi.org/10.5281/zenodo.\n5371628.\nBabak Hassibi, David G Stork, and Gregory J Wolff. Optimal brain surgeon and general network\npruning. In IEEE international conference on neural networks, pp. 293\u2013299. IEEE, 1993.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W\nMahoney, and Kurt Keutzer.\nSqueezellm: Dense-and-sparse quantization.\narXiv preprint\narXiv:2306.07629, 2023.\nJung Hyun Lee, Jihun Yun, Sung Ju Hwang, and Eunho Yang. Cluster-promoting quantization with\nbit-drop for minimizing network quantization loss. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 5370\u20135379, 2021.\nJung Hyun Lee, Jeonghoon Kim, Se Jung Kwon, and Dongsoo Lee. Flexround: Learnable rounding\nbased on element-wise division for post-training quantization. arXiv preprint arXiv:2306.00317,\n2023.\nXiuxian Li, Kuo-Yi Lin, Li Li, Yiguang Hong, and Jie Chen. On faster convergence of scaled sign\ngradient descent. IEEE Transactions on Industrial Informatics, 2023.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei Yu, Wei Wang, and\nShi Gu. Brecq: Pushing the limit of post-training quantization by block reconstruction. arXiv\npreprint arXiv:2102.05426, 2021.\nZhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou, Yuxian Qiu, Xiaotian Gao, Jingwen Leng,\nand Minyi Guo. Efficient activation quantization via adaptive rounding border for post-training\nquantization. arXiv preprint arXiv:2208.11945, 2022.\n10\nPreprint. Under review.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han.\nAwq:\nActivation-aware weight quantization for llm compression and acceleration.\narXiv preprint\narXiv:2306.00978, 2023.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free quantization aware\ntraining for large language models. arXiv preprint arXiv:2305.17888, 2023.\nZhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training quanti-\nzation for vision transformer. Advances in Neural Information Processing Systems, 34:28092\u2013\n28103, 2021.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort, and Max Welling. Data-free quantization\nthrough weight equalization and bias correction. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 1325\u20131334, 2019.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen Blankevoort. Up or\ndown? adaptive rounding for post-training quantization. In International Conference on Machine\nLearning, pp. 7197\u20137206. PMLR, 2020.\nOpenAI. Openai: Chatgpt. URL https://openai.com/blog/chatgpt.\nDenis Paperno, Germ\u00b4an Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,\nSandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fern\u00b4andez. The lambada dataset:\nWord prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031, 2016.\nGunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.\nnuqmm: Quantized matmul for efficient inference of large-scale generative language models.\narXiv preprint arXiv:2206.09557, 2022.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, J\u00b4er\u00b4emy Rapin, et al. Code llama: Open foundation models for code.\narXiv preprint arXiv:2308.12950, 2023.\nMher Safaryan and Peter Richt\u00b4arik. Stochastic sign descent methods: New algorithms and better\ntheory. In International Conference on Machine Learning, pp. 9224\u20139234. PMLR, 2021.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adver-\nsarial winograd schema challenge at scale. Communications of the ACM, 64(9):99\u2013106, 2021.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00b4e, Alexandra Sasha Luccioni, Franc\u00b8ois Yvon, Matthias Gall\u00b4e, et al. Bloom: A 176b-\nparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nKuan Wang, Zhijian Liu, Yujun Lin, Ji Lin, and Song Han. Haq: Hardware-aware automated quan-\ntization with mixed precision. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 8612\u20138620, 2019.\n11\nPreprint. Under review.\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu, and Fengwei Yu. Qdrop: randomly dropping\nquantization for extremely low-bit post-training quantization. arXiv preprint arXiv:2203.05740,\n2022a.\nXiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Feng-\nwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer lan-\nguage models. Advances in Neural Information Processing Systems, 35:17402\u201317414, 2022b.\nXiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang Guo, and Xian-\nglong Liu. Outlier suppression+: Accurate quantization of large language models by equivalent\nand optimal shifting and scaling. arXiv preprint arXiv:2304.09145, 2023.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.\nSmoothquant:\nAccurate and efficient post-training quantization for large language models.\narXiv preprint\narXiv:2211.10438, 2022.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant:\nAccurate and efficient post-training quantization for large language models.\nIn International\nConference on Machine Learning, pp. 38087\u201338099. PMLR, 2023.\nZhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali Yu, Eric Tan, Leyuan Wang, Qi-\njing Huang, Yida Wang, Michael Mahoney, et al. Hawq-v3: Dyadic neural network quantization.\nIn International Conference on Machine Learning, pp. 11875\u201311886. PMLR, 2021.\nZhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He.\nZeroquant-v2: Explor-\ning post-training quantization in llms from comprehensive study to low rank compensation.\narXiv:2303.08302, 2023.\nZhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang, Guangyu Sun,\nQiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training quantization for\nlarge language models. arXiv preprint arXiv:2304.01089, 2023.\nEdouard Yvinec, Arnaud Dapogny, Matthieu Cord, and Kevin Bailly. Spiq: Data-free per-channel\nstatic input quantization. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pp. 3869\u20133878, 2023.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a ma-\nchine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-\npher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer\nlanguage models. arXiv preprint arXiv:2205.01068, 2022.\nBohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu, Ian Reid, and Chunhua Shen. Effective train-\ning of convolutional neural networks with low-bitwidth weights and activations. IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence, 44(10):6140\u20136152, 2021.\nA\nMORE RESULTS\nA.1\nNON-RIGOROUS COMPARISON WITH AWQ\nWe conducted a limited comparison between our approach and AWQ Lin et al. (2023), considering\nthat our evaluation methodology closely follows that of GPTQ and we only share a few common\ntasks with AWQ. It is important to acknowledge that this comparison inherently lacks rigor due to\nour reliance on referencing AWQ\u2019s data alone. Consequently, this approach introduces the possi-\nbility of unfairness in the evaluation process, primarily stemming from the utilization of different\ncalibration datasets and other potential factors that may influence the obtained results.\nWe present the results of our common tasks alongside AWQ in table 8 and all the results of AWQ\nare from their paper. While we both report MMLU results, it is important to note that there was a\nbug fix 3 in lm-eval, resulting in significant changes to the baseline. So we have not included them\nin this report.\n3https://github.com/EleutherAI/lm-evaluation-harness/pull/497\n12\nPreprint. Under review.\nTable 8: Reported results of AWQ and Ours\nLLaMA-7B\nAWQ\nOurs\nnbits\nMethod\nPIQA\nHella.\nWino.\nPIQA\nHella.\nWino.\n16\nFP16\n78.35\n56.44\n67.09\n78.35\n56.42\n66.85\nW3G128\nRTN\n75.84\n53.10\n63.22\n75.73\n53.17\n63.14\nGPTQ\n70.89\n46.77\n60.93\n72.58\n47.10\n59.91\nProposed\n76.66\n53.63\n66.14\n76.61\n53.98\n66.06\nW4G128\nRTN\n77.86\n55.81\n65.59\n77.58\n55.86\n65.75\nGPTQ\n77.20\n53.98\n65.67\n77.26\n54.09\n64.09\nProposed\n78.07\n55.76\n65.82\n78.07\n55.92\n66.30\nA.2\nRESULTS OF WIKITEXT2 PPL AT W4\nThe perplexity results for Wikitext2 at W4 are shown in Table 9. In conclusion, our performance is\ncomparable to that of GPTQ.\nTable 9: Wikitext2 ppl ( \u2193) at W4\nLLaMA\nOPT\nBLOOM\nSize\n7b\n13b\n7bv2\n13bv2\n1.3b\n2.7b\n6.7b\n13b\n560m\n1b7\n3b\n7b1\nFP16\n5.67\n5.09\n5.47\n4.88\n14.62\n12.47\n10.86\n10.13\n22.41\n15.39\n13.48\n11.37\nRTN\n6.29\n5.53\n6.12\n5.20\n48.20\n16.92\n12.10\n11.32\n25.88\n16.97\n14.75\n12.10\nGPTQ\n6.59\n5.33\n6.09\n5.16\n15.67\n13.30\n11.59\n10.33\n23.95\n16.37\n14.10\n11.73\nOurs\n6.12\n5.32\n298.42\n9.15\n15.65\n13.05\n11.18\n10.66\n23.80\n16.22\n14.13\n11.80\nA.3\nOTHER RESULTS FOR LARGE MODELS\nWe present the results for models with a capacity of 30B or higher at W4 in Table 10 and PPL\non Wikitext2 in Table 11. Furthermore, we observed that adjusting the sequence length of the\ncalibration dataset led to improvements in specific scenarios, and we include these findings in our\nanalysis. Overall, our approach demonstrates comparable accuracy performance to GPTQ for the\ngiven task. However, it is worth noting that we slightly fall behind GPTQ in terms of PPL tasks.\nTable 10: Average % accuracy(\u2191) of HellaSwag, WinoGrand, PIQA and LAMBADA and C4 ppl(\u2193)\nfor LLaMA & OPT with size \u2265 30B at W4. \u201dOurs-seq512\u201d indicates that we have modified the\nsequence length of the calibration dataset from 256 to 512.\nAccuracy\nPPL on C4\nType\nLLaMA\nOPT\nLLaMA\nOPT\nSize\n30b\n65b\n30b\n66b\n30b\n65b\n30b\n66b\nFP16\n73.46\n75.48\n67.87\n69.54\n6.13\n5.98\n11.45\n10.99\nRTN\n72.33\n73.91\n65.94\n37.12\n6.54\n6.46\n13.56\n305.73\nGPTQ\n72.85\n74.45\n67.55\n68.23\n6.42\n6.23\n11.59\n11.24\nOurs-seq256\n72.69\n74.03\n66.74\n68.80\n6.47\n6.31\n11.84\n11.42\nOurs-seq512\n72.86\n73.91\n67.40\n69.22\n6.47\n6.34\n11.77\n11.45\nB\nVISUALIZATION OF V\nWe provide an analysis of the magnitude distribution of V in Eq. 2 for approximately 7B models at\nW4 in Figure 3. The findings reveal that the majority of V values are concentrated within the range\nof [-0.3, 0.3]. Notably, the middle layers demonstrate a narrower distribution in comparison to the\nother layers. This observation suggests that the head or tail layers may be more susceptible to the\ncompression.\n13\nPreprint. Under review.\nTable 11: Wikitext ppl(\u2193) for LLaMA & OPT with size \u2265 30B. \u201dOurs-seq512\u201d indicates that we\nhave modified the sequence length of the calibration dataset from 256 to 512.\nW4\nW3G128\nType\nLLaMA\nOPT\nLLaMA\nOPT\nSize\n30b\n65b\n30b\n66b\n30b\n65b\n30b\n66b\nFP16\n4.10\n3.56\n9.56\n9.34\n4.10\n3.56\n9.56\n9.34\nRTN\n4.54\n3.99\n10.98\n110.43\n4.87\n4.44\n23.05\n126.92\nGPTQ\n4.45\n4.16\n9.66\n9.66\n4.84\n4.17\n9.75\n10.58\nOurs-seq256\n4.51\n3.91\n9.88\n9.56\n4.85\n4.15\n11.07\n11.40\nOurs-seq512\n4.52\n3.90\n9.88\n9.70\n4.81\n4.17\n10.54\n10.87\nLLaMA-7B\nLLaMA-7B-V2\nOPT-6.7B\nBLOOM-7B1\nFigure 3: The distribution of the magnitude of V in Eq. 2 for different models, namely LLaMA-7B,\nLLaMA-7B-V2, OPT-6.7B, and BLOOM-7B1 at W4. Each color in the distribution represents a\nspecific layer index in the models, with blue indicating shallow layers closer to the data layer, and\nred representing deeper layers.\n14\nPreprint. Under review.\nC\nCORRECTION BETWEEN SIGNROUND AND SALIENT ACTIVATION\nCHANNELS\nWe were also intrigued by the possible correlation between rounding and activation, as previous re-\nsearch has shown that keeping only 0.1%-1% of the channels corresponding to larger activation can\nsignificantly improve the quantized performance in AWQ (Lin et al., 2023). Therefore, we investi-\ngated whether the altered rounding values tend to fall more frequently in these salient channels. The\nresults of our analysis, presented in Figure 4, reveal an interesting finding. The ratio, representing\nthe percentage of altered rounding values falling within the top 1% salient activation channels out of\nall altered rounding values, is typically around 1%. This suggests that there is no strong correlation\nbetween rounding and activation. It is possible that rounding values of less significant channels need\nto be changed to compensate for the quantization error introduced by these salient channels.\nLLaMA-7B\nLLaMA-7B-V2\nOPT-6.7B\nBLOOM-7B1\nFigure 4: The correction between SignRound and salient activation channels\nD\nRUNTIME\nTable 12 provides a runtime comparison between GPTQ and our method. All measurements were\nconducted on a single NVIDIA A100 card with 80GB of memory. Although our method demon-\nstrates slightly slower performance compared to GPTQ, it remains well within acceptable limits for\nreal-world deployment.\nTable 12: Runtime in seconds at W4\nType\nLLaMA\nOPT\nBLOOM\n7B\n13B\n6.7B\n13B\n3B\n7B1\nGPTQ\n712\n1240\n841\n1523\n345\n661\nOurs\n899\n1590\n819\n1429\n467\n843\nE\nDETAILED RESULTS OF SOME LLAMA MODELS\nDetailed results of LLaMA7B, LLaMA13B, LLAMA7B-V2, and LLAMA13B-V2 can be found in\nTable 13, Table 14, Table 15, and Table 16 respectively.\n15\nPreprint. Under review.\nTable 13: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText,\nPTB, C4 for LLaMA-7B, \u201dOurs-2048\u201d indicates that we have modified the sequence length of the\ncalibration dataset from 512 to 2048.\nHella.\nWino.\nPIQA\nLamb.\nAvg.\nWiki.\nPTB\nC4\nFP16\n56.42\n66.85\n78.35\n73.57\n68.80\n5.68\n10.12\n7.34\nW4G-1\nRTN\n54.96\n67.25\n77.31\n70.00\n67.38\n6.29\n11.25\n8.12\nGPTQ\n52.25\n63.85\n74.59\n68.12\n64.70\n6.59\n12.02\n8.64\nOurs\n55.28\n66.14\n77.64\n73.14\n68.05\n6.12\n10.88\n7.84\nOurs-2048\n54.96\n67.56\n77.80\n72.13\n68.11\n6.05\n10.87\n7.76\nW4G128\nRTN\n55.86\n65.75\n77.58\n72.25\n67.86\n5.96\n10.54\n7.70\nGPTQ\n54.09\n64.09\n77.26\n69.86\n66.33\n6.29\n11.11\n8.02\nOurs\n55.92\n66.30\n78.07\n72.07\n68.09\n5.86\n10.49\n7.56\nOurs-2048\n55.98\n66.77\n78.29\n71.78\n68.21\n5.88\n10.52\n7.58\nW3G128\nRTN\n53.17\n63.14\n75.73\n67.71\n64.94\n7.01\n12.83\n9.18\nGPTQ\n47.10\n59.91\n72.58\n53.58\n58.29\n8.28\n16.84\n10.45\nOurs\n53.98\n66.06\n76.61\n69.82\n66.62\n6.93\n11.67\n8.30\nOurs-2048\n53.45\n65.67\n76.55\n71.08\n66.69\n6.52\n11.60\n8.26\nTable 14: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText,\nPTB, C4 for LLaMA-13B, \u201dOurs-2048\u201d indicates that we have modified the sequence length of the\ncalibration dataset from 512 to 2048.\nHella.\nWino.\nPIQA\nLamb.\nAvg.\nWiki.\nPTB\nC4\nFP16\n59.13\n70.32\n78.94\n76.17\n71.14\n5.09\n9.08\n6.80\nW4G-1\nRTN\n57.96\n68.19\n78.18\n70.95\n68.82\n5.53\n9.78\n7.23\nGPTQ\n57.96\n70.24\n77.97\n73.84\n70.00\n5.33\n9.48\n7.13\nOurs\n58.02\n69.61\n78.94\n75.74\n70.58\n5.32\n9.37\n7.05\nOurs-2048\n58.13\n69.69\n78.67\n74.95\n70.36\n5.34\n9.49\n7.05\nW4G128\nRTN\n58.43\n70.32\n79.33\n75.32\n70.85\n5.26\n9.29\n6.94\nGPTQ\n58.79\n70.56\n79.33\n75.00\n70.92\n5.21\n9.28\n6.92\nOurs\n58.62\n71.35\n79.76\n75.98\n71.43\n5.19\n9.18\n6.90\nOurs-2048\n58.47\n70.56\n79.22\n76.23\n71.12\n5.19\n9.19\n6.90\nW3G128\nRTN\n56.39\n67.56\n77.20\n69.63\n67.70\n5.88\n10.58\n7.86\nGPTQ\n56.58\n67.96\n78.07\n72.31\n68.73\n5.64\n9.95\n7.54\nOurs\n57.04\n69.14\n77.86\n74.33\n69.59\n5.53\n9.81\n7.39\nOurs-2048\n56.62\n68.82\n78.13\n74.42\n69.50\n5.57\n9.76\n7.37\nTable 15: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText,\nPTB, C4 for LLaMA-7B-V2, \u201dOurs-2048\u201d indicates that we have modified the sequence length of\nthe calibration dataset from 512 to 2048.\nHella.\nWino.\nPIQA\nLamb.\nAvg.\nWiki.\nPTB\nC4\nFP16\n56.69\n67.17\n78.35\n73.88\n69.02\n5.47\n32.91\n7.26\nW4G-1\nRTN\n55.51\n66.77\n77.58\n68.08\n66.98\n6.12\n61.61\n8.16\nGPTQ\n54.74\n66.93\n76.17\n69.73\n66.89\n6.09\nNAN\n7.90\nOurs\n55.53\n67.09\n77.53\n70.81\n67.74\n298.4\n2677\n11.20\nOurs-2048\n55.63\n67.96\n77.64\n69.92\n67.79\n196.7\n2622\n9.97\nW4G128\nRTN\n56.55\n66.93\n77.37\n72.44\n68.32\n5.72\n50.25\n7.58\nGPTQ\n56.16\n68.03\n78.56\n72.83\n68.90\n5.73\nNAN\n7.53\nOurs\n56.21\n67.56\n77.64\n73.20\n68.65\n60.03\n1786\n8.16\nOurs-2048\n55.97\n67.09\n77.15\n73.57\n68.45\n48.91\n1872\n8.05\nW3G128\nRTN\n54.65\n67.17\n75.90\n65.98\n65.92\n6.66\n44.89\n8.98\nGPTQ\n52.93\n65.19\n76.44\n67.49\n65.51\n6.57\nNAN\n8.61\nOurs\n53.65\n66.14\n77.09\n70.64\n66.88\nNAN\n1159\n9.88\nOurs-2048\n53.91\n67.32\n76.33\n71.12\n67.17\nNAN\n1739\n10.11\n16\nPreprint. Under review.\nTable 16: Accuracies(\u2191) of HellaSwag, WinoGrand, PIQA, LAMBADA and PPL(\u2193) of WikiText,\nPTB, C4 for LLaMA-13B-V2, \u201dOurs-2048\u201d indicates that we have modified the sequence length of\nthe calibration dataset from 512 to 2048.\nHella.\nWino.\nPIQA\nLamb.\nAvg.\nWiki.\nPTB\nC4\nFP16\n59.71\n69.61\n78.78\n76.71\n71.20\n4.88\n48.82\n6.73\nW4G-1\nRTN\n58.56\n69.30\n78.45\n74.36\n70.17\n5.20\n58.57\n7.14\nGPTQ\n57.81\n67.48\n77.86\n73.84\n69.25\n5.16\n52.46\n6.87\nOurs\n58.63\n69.61\n77.91\n73.98\n70.03\n9.15\n66.80\n7.72\nOurs-2048\n58.87\n68.67\n78.07\n75.90\n70.38\n6.51\n60.35\n7.28\nW4G128\nRTN\n59.12\n69.46\n78.02\n76.29\n70.72\n4.98\n52.22\n6.87\nGPTQ\n59.22\n68.51\n78.84\n76.13\n70.68\n4.99\n51.59\n6.87\nOurs\n59.20\n69.14\n78.35\n76.56\n70.81\n5.80\n51.92\n6.84\nOurs-2048\n59.25\n70.48\n78.29\n76.81\n71.21\n5.00\n51.78\n6.84\nW3G128\nRTN\n57.03\n67.56\n77.86\n72.37\n68.70\n5.52\n62.33\n7.58\nGPTQ\n56.99\n66.69\n78.40\n72.85\n68.73\n5.45\n55.09\n7.54\nOurs\n57.29\n68.90\n77.37\n75.22\n69.70\n5.35\n59.57\n7.35\nOurs-2048\n57.20\n70.88\n78.13\n75.35\n70.39\n10.38\n66.22\n7.92\n17\n"
  },
  {
    "title": "Dynamic Mesh-Aware Radiance Fields",
    "link": "https://arxiv.org/pdf/2309.04581.pdf",
    "upvote": "5",
    "text": "Dynamic Mesh-Aware Radiance Fields\nYi-Ling Qiao\u2217\nAlexander Gao\u2217\nYiran Xu\nYue Feng\nJia-Bin Huang\nMing C. Lin\nUniversity of Maryland College Park\nhttps://mesh-aware-rf.github.io\n(\u0754,\u0755,\u0756,\u07e0,\u07f6)\n(\u0734\u0729\u0724\u07ea)\nRadiance \nFields\n(b) Hybrid rendering: inside the mirror room\nTextured\nMeshes\n(a) Inputs\nMLP\n(b) Hybrid rendering: outside the mirror room\n(\ud835\udc65, \ud835\udc66, \ud835\udc67, \ud835\udf03, \ud835\udf19)\n(\ud835\udc45\ud835\udc3a\ud835\udc35\ud835\udf0e)\nRadiance \nFields\n(b) Hybrid rendering: inside the mirror room\nTextured\nMeshes\n(a) Inputs\nMLP\n(c) Hybrid rendering: outside the mirror room\nFigure 1: Mesh-aware rendering of radiance fields. We place a cubic mesh with reflective textures and other synthetic mesh\nobjects in the MipNeRF-360 Garden [5] scene. Our mesh-aware rendering explicitly computes the rays bouncing inside the\nmirror room, creating an \u2018infinite mirror room\u2019 visual effect.\nAbstract\nEmbedding polygonal mesh assets within photorealistic\nNeural Radience Fields (NeRF) volumes, such that they can\nbe rendered and their dynamics simulated in a physically\nconsistent manner with the NeRF, is under-explored from\nthe system perspective of integrating NeRF into the tradi-\ntional graphics pipeline. This paper designs a two-way cou-\npling between mesh and NeRF during rendering and simu-\nlation. We first review the light transport equations for both\nmesh and NeRF, then distill them into an efficient algorithm\nfor updating radiance and throughput along a cast ray with\nan arbitrary number of bounces. To resolve the discrep-\nancy between the linear color space that the path tracer\nassumes and the sRGB color space that standard NeRF\nuses, we train NeRF with High Dynamic Range (HDR) im-\nages. We also present a strategy to estimate light sources\nand cast shadows on the NeRF. Finally, we consider how\nthe hybrid surface-volumetric formulation can be efficiently\nintegrated with a high-performance physics simulator that\nsupports cloth, rigid and soft bodies. The full rendering\nand simulation system can be run on a GPU at interactive\nrates. We show that a hybrid system approach outperforms\nalternatives in visual realism for mesh insertion, because it\nallows realistic light transport from volumetric NeRF me-\ndia onto surfaces, which affects the appearance of reflec-\ntive/refractive surfaces and illumination of diffuse surfaces\ninformed by the dynamic scene.\n1. Introduction\nCreating high-quality 3D environments suitable for pho-\ntorealistic rendering entails labor-intensive manual work\ncarried out by skilled 3D artists. Neural Radiance Fields\n(NeRF) [46] provide a convenient way to capture a volumet-\nric representation of a complex, real-world scene, paving\nthe way for high-quality novel view synthesis and inter-\nactive photorealistic rendering [49]. These qualities make\nNeRF exceptionally adept at modeling background environ-\nments. On the other hand, existing methods for physically-\nbased simulation and rendering of complex material and\nlighting effects are primarily based on surface mesh rep-\nresentations. Integrating neural field representations with\nwell-established traditional graphics pipelines opens up\nmany possibilities in VR/AR, interactive gaming, virtual\ntourism, education, training, and computer animation.\nVolume rendering [56] has demonstrated its capability\nto produce visually captivating results for participating me-\ndia [53]. However, integrating NN-based NeRF into this\n*Equal contribution\narXiv:2309.04581v1  [cs.GR]  8 Sep 2023\n\ud835\udc61(\ud835\udc5d, \ud835\udf14\ud835\udc56)\n\ud835\udf14\ud835\udc56\n\ud835\udf14\ud835\udc5c\n\ud835\udc5d \ud835\udf03\ud835\udc56\n\ud835\udf14\ud835\udc56\n\ud835\udf14\ud835\udc5c\n\ud835\udf0e(\ud835\udc5d\ud835\udc60)\n\ud835\udc5d\ud835\udc61\nincident \nangle\nsurface \npoint\nsurface\nintersection\nvolume\ndensity\nFigure 2: Light transport on the surface (left) and in the\nmedium (right).\npipeline while maintaining realistic lighting effects such as\nshadows, reflections, refractions, and more, remains a rel-\natively unexplored area. In terms of simulation, while the\ngeometry of NeRF is implicit in its density field, it lacks\na well-defined surface representation, making it difficult to\ndetect and resolve collisions. Recent works have delved\ninto enhancing the integration between NeRF and meshes,\naiming to combine the photorealistic capabilities of NeRF\nwith the versatility of meshes for rendering and simulation.\nNeural implicit surfaces [87, 80, 59, 19] are represented\nas learned Signed Distance Fields (SDF) within the NeRF\nframework. Meanwhile, methods like IRON [94] and NVD-\niffRec [52] extract explicit, textured meshes that are directly\ncompatible with path tracing, offering practical benefits at\nthe expense of a lossy discretization. Nerfstudio [72] ren-\nders NeRF and meshes separately, then composites the ren-\nder passes with an occlusion mask. Unfortunately, this de-\ncoupled rendering approach offers no way to exploit the\nlighting and appearance information encoded in the NeRF\nvolume to affect the rendered mesh appearance. Figure 6\nvisually compares our hybrid method to naively combining\nNeRF and surface rendering, and pure surface rendering.\nWe introduce a hybrid graphics pipeline that integrates\nthe rendering and simulation of neural fields and meshes.\nfor both representations, we consider lighting effects and\ncontact handling for physical interaction.\nBy unifying\nNeRF volume rendering and path tracing within the linear\nRGB space, we discover their Light Transport Equations\nexhibit similarities in terms of variables, forms, and princi-\nples. Leveraging their shared light transport behavior, we\ndevise update rules for radiance and throughput variables,\nenabling seamless integration between NeRF and meshes.\nTo incorporate shadows onto the NeRF, we employ differ-\nentiable surface rendering techniques [28] to estimate light\nsources and introduce secondary shadow rays during the ray\nmarching process to determine visibility. Consequently, the\nNeRF rendering equation is modified to include a point-\nwise shadow mask.\nFor simulation, we adopt SDFs to represent geometry\nof neural fields, which is advantageous for physical contact\nhandling and collision resolution. We then use position-\nbased dynamics [42] for time integration.\nOur efficient\nhybrid rendering and simulation system is implemented in\nCUDA. To enhance usability, we have also incorporated\nuser-friendly Python interfaces. In summary, the key con-\ntributions of this work are:\n\u2022 A two-way coupling between NeRF and surface repre-\nsentations for rendering and simulation.\n\u2022 Integration with HDR data which can unify the color\nspace of the path tracer and NeRF, with a strategy to\nestimate light sources and cast shadows on NeRF.\n\u2022 An efficient rendering procedure that alternates ray\nmarching and path tracing steps by blending the Light\nTransport Equations for both NeRF and meshes.\n\u2022 An interactive, easy-to-use implementation with a\nhigh-level Python interface that connects the low-level\nrendering and simulation GPU kernels.\n2. Related Work\n2.1. Neural Fields and Surface Representations\nRendering of participating media has been extensively\nstudied\n[57, 53] in classic graphics pipelines [58, 63,\n50, 25].\nIn recent years, significant advancements have\nbeen made in this area [47, 55], yielding remarkable vi-\nsual outcomes.\nOur work aims to expand upon this\nprogress by incorporating the emergent Neural Radiance\nFields (NeRF) [46], which have gained substantial popu-\nlarity, into this exciting domain. Within the volume ren-\ndering framework, NeRF bakes the plenoptic function and\nvolumetric density into spatial points.\nThese points can\nbe effectively parameterized by an an MLP [46], convo-\nlutional networks [8], hash grid [49], point cloud [84],\nvoxel [36, 70], or tensors [9].\nIt allows users to recon-\nstruct photorealistic 3D static [71, 76, 66, 33, 81] or dy-\nnamic scenes [61, 83, 60, 20, 38] by casually capturing a\nfew images [1]. Original NeRF takes seconds to render one\nsingle frame, while follow-up works have accelerated ren-\ndering speed [24, 78, 12, 21, 51, 88, 89, 3, 2, 79].\nNeRF simplifies the creation of 3D content compared to\nclassical mesh-based pipelines. However, addressing chal-\nlenges of editing [93, 31] and decomposing the baked in-\nformation [44, 68, 69, 85] is not trivial. Effort has been\ndirected toward reconciling the advantages of both NeRF\nand surface-based paradigms. In rendering, [94] and [52]\npropose to use surface-based differentiable rendering to re-\nconstruct textured meshes [23] from neural fields. Their re-\nconstructed meshes can be imported to a surface rendering\npipeline like Blender [15], but the original NeRF represen-\ntation cannot be directly rendered with meshes. For simu-\nlation, NeRFEditting [92] proposes to use explicit mesh ex-\ntracted by [80] to control the deformation of Neural Fields.\nQiao et al. [65] further add full dynamics over the extracted\ntetrahedra mesh. Chu et al. [13] integrates the dynamics of\nsmoke with neural fields. [14] also connects differentiable\nsimulation to NeRF, where the density field and its gradient\n(a) Hybrid models\nRadiance Field \nRepresentation\nSurface \nRepresentation\nConvert to SDF\nSurface Rendering\nNeRF Rendering\n(b) Hybrid simulation\n(\ud835\udc65\ud835\udc65, \ud835\udc66\ud835\udc66, \ud835\udc65\ud835\udc65, \ud835\udf03\ud835\udf03, \ud835\udf19\ud835\udf19)\n(\ud835\udc45\ud835\udc45\ud835\udc3a\ud835\udc3a\ud835\udc35\ud835\udc35\ud835\udf0e\ud835\udf0e)\nContact Handling\nTime Integration\nMLP\n(c) Hybrid rendering\n(d) Result\nShadow Ray\nFigure 3: Pipeline overview. Our method takes an optimized radiance field model and surface meshes as inputs. We can run a\nphysics simulation between the NeRF and meshes. The updated mesh vertices and NeRF transformations are synchronized to\nthe renderer, which uses Monte Carlo simulation to sample ray paths. As the ray travels through space, it alternates between\nsurface rendering (ray-tracing) and NeRF rendering (ray-marching), both updating its radiance.\nare used to compute the contact. These methods aim to con-\nstruct an end-to-end differentiable simulation and rendering\npipeline, yet they have yet to couple the rendering.\n2.2. Scene Editing and Synthesis\nOur method enables inserting mesh assets into NeRF\nmodels of real-world captures. Editing of existing scenes\nis an active topic of study. For neural field representations,\nray bending [75, 64, 34] is widely used to modify an opti-\nmized NeRF. It is possible to delete, add, duplicate, or ac-\ntuate [10, 62, 82, 37] an area by bending the path of the\nrendering ray. [22] propose to train a NeRF for each ob-\nject and compose them into a scene. ClimateNeRF [35] can\nchange weather effects by modifying the density and radi-\nance functions during ray marching. These methods study\nediting of isolated NeRF models. There are also inverse\nrendering works that decompose [86, 6] the information\nbaked into NeRF, which can then be used to edit lighting\nand materials [32]. Such decomposition is useful, but as-\nsumes information like a priori knowledge of light sources,\nor synthetic scenes. They do not address inserting mesh into\nNeRF scenes. Besides NeRF, [30] inserts a virtual object\ninto existing images by estimating the geometry and light\nsource in the existing image. [11] insert vehicles in street\nscenes by warping textured cars using predicted 3D poses.\n3. Method\nIn this section, we describe how radiance fields and\npolygonal meshes can be integrated into a unified rendering\nand simulation pipeline, an overview of which is visualized\nin Figure 3.\n3.1. Rendering\nNeRF can photorealistically reconstruct a 3D scene from\na set of images, making it an appealing candidate for mod-\neling environments and potentially valuable for traditional\nsurface-based rendering. One possible approach to bridge\nthese disparate representations is to render the meshes and\nNeRF volume in separate passes, and composite the results\ntogether in 2D image space.\nHowever, compositing in\nimage space is susceptible to incorrect occlusion masks and\ninaccurate lighting. A more physically principled approach\nto this problem is identifying and exploiting the similarities\nin their respective light transport equations, which directly\nallows the radiance field and mesh to be incorporated in 3D\nspace.\nSurface Rendering Equation. The Light Transport Equa-\ntion (LTE) for surface rendering is:\nL(p, \u03c9o) = Le(p, \u03c9o) + Lr(p, \u03c9o)\n(1)\nLr(p, \u03c9o) =\nZ\nS2 fs(p, \u03c9o, \u03c9i)Li(p, \u03c9i) |cos \u03b8i| d\u03c9i\n(2)\nLi(p, \u03c9i) = L(t(p, \u03c9i), \u2212\u03c9i)\n(3)\nwhere p is a surface point; \u03c9i, \u03c9o are the directions of in-\ncident (incoming) and exitant (outgoing) radiance; S2 is\nthe unit sphere sampling space for directions; L, Le, Li, Lr\nare the exitant, emitted, incident, and reflected radiance,\nrespectively; \u03b8i is the angle of incidence of illumina-\ntion; fs(p, \u03c9o, \u03c9i) is the bidirectional scattering distribution\nfunction (BSDF); and t(p, \u03c9) is the ray-casting function that\ncomputes the first surface intersected by the ray cast from p\nin the direction \u03c9.\n(c) Light Estimation\n(a) Mesh\n(f) Hybrid Rendering\n(b) NeRF\nFigure 4: Shadow Casting. We estimate the geometry and light source of the scene and insert a metal basket onto the desk.\nOur pipeline can render the realistic reflection and shadow effects caused by the synthetic mesh.\nIf a scene is represented solely by surfaces, the LTE in\nEquation 1 can be solved by Monte Carlo path tracing: for\neach pixel, a ray is randomly cast from the camera, its path\nconstructed incrementally each time it hits, and bounces off\nof a surface. A natural but memory-inefficient way to im-\nplement this algorithm is to recursively compute Equation 1\nand spawn a new ray upon each ray-surface intersection.\nNoticing that Li(p, \u03c9i) is independent of previous paths, the\nrecursive process can be transformed into a weighted sum of\nradiance on each ray-surface intersection pk. These weights\nT(pk) are called throughput, and they depend on their pre-\ndecessors\u2019 BSDF fs(p, \u03c9o, \u03c9i), illumination angle |cos \u03b8i|,\nand probability density function P of the scattering:\nT(pk) = T(pk\u22121) \u00b7 fs(pk, \u03c9k\u22121, \u03c9k) |cos(\u03b8k)|\nP\n(4)\nT(pk) and Li(pk, \u03c9) are the only variables essential to\ntrack and integrate for each bounce.\nVolumetric Rendering Equation.\nThe light transport\nequation for the volumetric medium is:\nL(p, \u03c9o) =\nZ tf\nt=0\nexp\n\u0010\n\u2212\nZ t\ns=0\n\u03c3t(ps)ds\n\u0011\nLi(pt, \u03c9o)dt (5)\nLi(pt, \u03c9o) =Le(pt, \u03c9o) + Ls(pt, \u03c9o)\n(6)\nLs(pt, \u03c9o) = \u03c3s(pt)\nZ\nS2 fp(pt, \u03c9o, \u03c9i)L(pt, \u03c9i)d\u03c9i\n(7)\nwhere Ls is the (weighted) scattered radiance, \u03c3t and \u03c3s are\nthe attenuations and scattering coefficients, fp is the phase\nfunction of the volume, and ps is on the ray ps = p + s \u00b7 \u03c9o\n(similar to pt). All other terms share the same definition as\nin surface rendering.\nThe integral in the volumetric LTE could again be solved\nusing Monte Carlo methods.\nHowever, stochastic simu-\nlation of volumetric data is more challenging and expen-\nsive than surface data.\nA photon may change direction\nin a continuous medium, unlike the discrete bounces that\noccur only at surfaces. Therefore, rather than simulating\nthe path of photons using Monte Carlo sampling, meth-\nods like NeRF [46] instead bake the attenuation coefficient\n\u03c3(p) = \u03c3t(p) and view-dependent radiance r(p, \u03c9) onto\neach spatial point, and so there is no scattering. This cir-\ncumvents solving Equation 7, thereby avoiding considering\nlight transport, light sources, and material properties. Vol-\nume rendering under the NeRF formulation becomes:\nr(p, \u03c9) =\nZ tf\n0\nT(t)\u03c3(pt)r(pt, \u03c9)dt,\n(8)\nT(t) = exp\n\u0010\n\u2212\nZ t\n0\n\u03c3(ps)ds\n\u0011\n(9)\nIn Equation 8, the radiance and throughput are being up-\ndated, similar to surface rendering. However, note that the\nvolumetric LTE denotes incident radiance at point p from\ndirection \u03c9 as Li(p, \u03c9), while NeRF denotes the same as\nr(p, \u03c9). This terminology is indeed overloaded, as r(p, \u03c9)\nin the NeRF formula represents sRGB color, i.e. the re-\nsult of applying a nonlinear tone-mapping function to the\nraw radiance value. The terms are related in that r(p, \u03c9) =\n\u03c8(Li(p, \u03c9)), where \u03c8(\u00b7) represents a tone-mapping func-\ntion from linear to sRGB color space.\nUnifying color space of path tracing and ray marching.\nThe standard NeRF model accumulates sRGB color along\nrays cast from the camera into the volume: each point\u2019s\ncolor is represented by three 8-bit values, one for each color\nchannel. Integrating these colors along the ray (weighted\nby transmittance) produces a final 8-bit color value, the ren-\ndered pixel color, which is compared to the corresponding\nground truth 8-bit pixel color to supervise NeRF training.\nIn contrast, path tracing assumes radiance values are ex-\npressed in linear color space.\nTo relate NeRF and sur-\nface rendering in a physically meaningful way, they should\nideally operate in a standard color space.\nTo reconcile\nthis difference, we train an HDR variant of NeRF, super-\nvised with 32-bit HDR images directly rather than the stan-\ndard 8-bit NeRF. The resulting HDR NeRF produces a 3-\nchannel radiance in 32-bit linear color space at each sam-\npled point.\nFor details regarding HDR data acquisition,\npreprocessing, and HDR NeRF implementation, see Ap-\npendix E. As our focus here is to articulate the advan-\ntage of HDR NeRF in the context of the overall system,\nwe forego a more general discussion of training NeRF in\nHDR and refer interested readers to [45] for a deep dive.\nThe HDR NeRF rendering equation can thus be written as:\nLi(p, \u03c9) =\nR tf\n0 T(t)\u03c3(pt)Li(pt, \u03c9)dt where the transmit-\ntance term T(t) remains unchanged.\nWith this simple adjustment, the NeRF equation can now\ndirectly relate to the surface rendering equation. If captur-\ning HDR training data is impractical, one can still use stan-\ndard (LDR) NeRF. Since the NeRF volume acts as the only\nlight source, and the total energy is dissipative during the\nlight transport, Li(p, \u03c9) will never exceed the NeRF vol-\nume\u2019s maximal radiance. \u03c8(\u00b7) would then degenerate to\nan identity mapping such that L(p, \u03c9) = r(p, \u03c9). In other\nwords, in many practical cases, reasonable visual results\ncould still be obtained if standard NeRF is used with our\nsystem, despite the resulting inaccuracy in the light trans-\nport simulation.\nEstimating light sources with differentiable surface ren-\ndering. Sampling light sources for computing the shadow\npass requires an approximate representation of the light\nsources (emitters) of the scene. Note that NeRF\u2019s volume\nrendering formulation bakes appearance into each point in\nthe volume rather than simulating physically based light\ntransport.\nTo recover an explicit representation of light\nsources, we first reconstruct the scene\u2019s geometry as a neu-\nral SDF using MonoSDF [91], from which we extract an\nexplicit mesh. Then, we employ a differentiable path tracer,\nMitsuba3 [28, 29], to estimate a UV Emission Texture for\nthe mesh. We follow the general approach of [54], though\nwe customize the optimization procedure since our goal\nis to estimate only the light sources, as opposed to a full\nBRDF estimation (more details about the optimization can\nbe found in Appendix D).\nOnce the light source estimation has converged, we\nprune faces whose emission falls below a threshold from\nthe explicit mesh, which is necessary for efficiency, as most\nfaces in the explicit mesh do not emit light. The hybrid ren-\nderer then consumes the pruned mesh.\nShadow rays.\nWe query additional rays during ray-\n(a) HDR\n(c) LDR\n(b) HDR [-3.5 EV]\n(d) LDR [-1.5 EV]\nFigure 5: HDR Volumetric Radiance Map. The diffuse\nsphere is rendered using our hybrid algorithm. Images ren-\ndered using the NeRF trained in 32-bit HDR (a, b) achieve\na higher level of lighting realism than those rendered with\n8-bit LDR (c, d).\nmarching to cast shadows on NeRF. For each sampled point\npt in NeRF, we shoot a secondary ray from pt to the light\nsource (see the following subsection for details on estimat-\ning lighting sources). If an inserted mesh blocks this ray,\nthen this pt has a shadow mask m(pt) = 1 \u2212 rsrc, where\nrsrc is the intensity of the light source. Non-blocked pixels\nhave mshadow = 1. The contribution of this point in Eq. 8 is\nthen T(t)\u03c3(pt)m(pt)r(pt, \u03c9). Fig. 4 shows that an inserted\nmetal basket casts shadows on the NeRF desk.\nHybrid Rendering Algorithm Based on the Light Trans-\nport Equations mentioned above, we note that both surface\nand NeRF rendering integrate the throughput T(p) and ra-\ndiance r(p, \u03c9) = Li(p, \u03c9). The differences are: (1) Surface\nrendering updates those values on discrete boundaries while\nNeRF accumulates them in the continuous space; (2) T(p)\nand r(p, \u03c9) are governed by the BSDF parameters in sur-\nface rendering, while by neural fields in NeRF. Therefore,\nwe can alternate between the surface and NeRF rendering\nrules as they travel in space. Algorithm 1 is a summary of\nthe hybrid rendering of NeRF and surface representations:\n1. We use Monte Carlo path tracing to sample the ray-\nsurface-intersections p0 \u2192 p1 \u2192 ... \u2192 pn, where\np0 is the camera center, and pn is the termination\nof the path.\nAt the beginning of the path, initial-\nize accumulated throughput T(p0) = 1 and radiance\nr(p0, \u03c90) = (0, 0, 0). The termination conditions will\nbe discussed in (4).\n2. If shadows are needed, we estimate light source ge-\nometry and intensity rsrc with differentiable surface\nrendering.\n3. For each ray segment pj \u2192 pj+1, we use the ray-\nmarching algorithm to sample and integrate the NeRF\nmedium. For the sampled points pt on the ray, we\nshoot a ray from pt to the light source (if any).\nIf\nmeshes block this point, set its shadow mask to be\nm(pt) = 1\u2212rsrc (or simply a constant close to 0), oth-\nerwise m(pt) = 1. Then the throughput and shadow\nmasked radiance between surface intersections pj and\npj+1 can be computed as,\nT \u2032(pj+1) = T(pj) \u00b7 exp\n\u0010\n\u2212\nZ\npt\n\u03c3(pt)dt\n\u0011\n(10)\nL(pj+1, \u03c9j) = L(pj, \u03c9j) +\nZ\npt\nT(t)\u03c3(pt)m(pt)r(pt, \u03c9)dt (11)\nwhere pt \u2208 (pj, pj+1] and\nT(t) = T(pj) \u00b7 exp\n\u0010\n\u2212\nZ t\n0\n\u03c3(ps)ds\n\u0011\nis also accumulated from pj.\n4. At the end of a ray segment, we reach the interface\npj+1 where the surface-rendering procedures occur.\nThe direction \u03c9j+1 of the next ray is determined by\nsampling the BSDF, and the weighted illumination and\nemitted light at this point are added to the radiance:\nL(pj+1, \u03c9j+1) = L(pj+1, \u03c9j) + T(pj)Le(pj+1, \u03c9j)\n(12)\nThe throughput weight is updated as:\nT(pj+1) = T \u2032(pj+1) \u00b7 fs(pj+1, \u03c9j, \u03c9j+1) |cos(\u03b8j+1)|\nP\n(13)\nwhere P is the scattering probability density function.\n5. In the end, the ray terminates at (pe, \u03c9e) if (1) it runs\nout of the scene; (2) current throughput T(pe) is lower\nthan a threshold; or (3) it meets the bounce limit.\n6. As the rendering procedure is carried out over a linear\n32-bit color space after the path tracing terminates for\na given pixel, we can apply a nonlinear tone-mapping\nfunction, which we denote as \u03c8, to map from linear\nradiance to final sRGB color r(pe, \u03c9e) which is more\nsuitable for displaying on a monitor:\nr(pe, \u03c9e) = \u03c8(L(pe, \u03c9e))\n(14)\n3.2. Simulation\nWe incorporate a dynamics simulator that supports rigid\nbodies, cloth, and deformable solids.\nNeural fields and\nmeshes can be connected in the simulation pipeline by\nAlgorithm 1 Hybrid Rendering Pipeline\nRequire: Meshes and pretrained NeRF of the scene.\nEstimate light sources with differentiable surface render-\ning.\nfor each pixel (u, v) in parallel do\nInitialize p0, \u03c90 based on (u, v) and camera center.\nSet throughput T(p0) = 1.\nSet radiance L(p0, \u03c90) = (0, 0, 0).\nfor j \u2208 {1, ..., nbounces} do\nCast ray to find next intersection pj (Eqn. 3).\nMarch along ray pj\u22121 \u2192 pj.\nCast shadow rays to light sources.\nIntegrate T \u2032(pj) and L(pj, \u03c9j\u22121) (Eqn. 10, 11).\nSample BSDF at pj to get next ray direction \u03c9j\nUpdate T(pj) and L(pj, \u03c9j) (Eqn. 13, 12).\nbreak if termination conditions satisfied.\nend for\n(Path tracing endpoint denoted as (pe, \u03c9e))\nApply tone-mapping function r = \u03c8(L(pe, \u03c9e)).\nend for\nSigned Distance Fields (SDF) or reconstructed surface\nmesh. The SDF of the NeRF can be obtained in several\nways during pre-processing.\nOn the one hand, existing\nmethods can directly learn the SDF, like NeuS [80] and\nVolSDf [87].\nOn the other hand, it is common to set a\nthreshold value for the density field and extract the sur-\nface mesh through Marching Cube [40]. And the SDF can\nbe converted from the mesh. The learned SDF has better\nquality but takes more time. We also implement an Instant-\nNGP version of NeuS (called NeuS-NGP) and accelerate\nthe original code by more than 10 times. Users can make\nthe trade-off depending on their needs.\nWe\nemploy\nextended\nposition-based\ndynamics\n(XPBD) [43, 48] to simulate the objects during run-\ntime. We choose this dynamics model because it is fast and\ncan support various physical properties. Collision detection\nis performed by querying the SDF of all vertices. All of\nthese queries can be computed efficiently in parallel on a\nGPU. Given a detected collision from the SDF, we can also\nget the penetration depth and normal, which can be used to\ncompute the contact forces.\nIn some scenarios, NeRF can represent movable objects\n(e.g. a scene can be a composition of several NeRF ob-\njects [73]) instead of a static background. We can get the\nhomogenous transformation t \u2208 R4\u00d74 of the NeRF from\nsimulation in each time step, which is used to inverse trans-\nform the homogenous coordinates t\u22121 \u00b7 p [64] when query-\ning the color/density and sampling rays in the Instant-NGP\nhash grid. In Fig. 8 (b), we control a ball to interact with a\nNeRF chair [46] in real-time. The supplementary video fur-\nther shows how the collision effect changes when the ball\n(a) Original rendering\n(b) Ours\n(c) NVDiffrec\n(d) Nerfstudio\nFigure 6: Rendering comparison for virtual object insertion. We insert a reflective metal ball into the Garden and Bicycle\nscenes from the Mip-NeRF 360 dataset [5]. (b) Our hybrid method produces results of superior visual quality with fewer\nartifacts. (c) We extract the foreground mesh using NVDiffRec, then insert the synthetic ball and render using ray tracing.\nExtracting an explicit mesh object results in noticeable artifacts such as the noisy table surface, and missing thin structures\nlike the bicycle\u2019s wheel and the bench. (d) The 2D compositing workflow of Nerfstudio suffers from non-3D-aware occlusion\nmasks and a limited ability to accommodate realistic interreflection.\nand chair have different relative mass and velocities.\n3.3. Implementation Details\nThe entire pipeline employs CUDA backends for compu-\ntation and Python interfaces for interaction. For rendering,\nNeRF is trained with the default configuration using Instant-\nNGP [49]. We also implement an instant-NGP [49] version\nof NeuS [80] for efficient learning of the implicit SDF ge-\nometry. The path tracing algorithm is implemented using\nCUDA, embedded in Instant-NGP\u2019s ray-marching proce-\ndures. We incorporate refractive, reflective, and Lamber-\ntian BSDF models. Physics simulation utilizes Warp [41],\nwhich just-in-time compiles Python code into CUDA ker-\nnels. The connection between rendering and simulation is\nfacilitated by a Python interface using pybind11 [27]. Scene\nparameters can be easily created or modified through config\nfiles or Python APIs.\nOur method can achieve a runtime of 1 to 40 frames per\nsecond, contingent upon the resolution, scene complexity,\nand dynamics. Figure 8 demonstrates a real-time game on\na laptop that has been developed within our pipeline. The\ncode for rendering, simulation, and fast SDF learning will\nbe released as open-source software.\n4. Experiments\n4.1. Comparisons\nRendering Comparisons. In this section, we compare with\nother surface-based modeling and rendering methods in the\nvirtual object insertion task. Given a set of images, the tra-\nditional graphics pipeline would first reconstruct the sur-\nface.However, the 3D reconstruction step will usually in-\ntroduce tremendous noise and errors. Our technique can\ndirectly render the virtual object in the photorealistic 3D\nscene without meshing the entire scene.\nNVdiffrec [52] and IRON [94] are state-of-the-art tex-\ntured mesh reconstruction methods.\nThey combine neu-\nral fields and differentiable rendering methods to estimate\nthe geometry and appearance of the objects from images.\nHowever, neither of them works on the full image because\nthe topology of the background is too complex to optimize\n(e.g. the vegetation). We further provide the per-frame fore-\nground mask to the comparison methods, and NVdiffRec\ncan reconstruct foreground models.\nIn Figure 6 (c), we import the extracted mesh into\nBlender with a metallic ball and environment map, then ren-\nder the scene with Cycles, a physically-based render. Both\n(b) ours and (c) NVDiffRec can model the reflection on the\nball, but our object has better surface quality.\nWe also compare to Nerfstudio [72], a software library\nthat allows users to train their own NeRF models. The user\ncan export a camera path moving through the trained NeRF\nscene and a coarsely reconstructed mesh and import these\ninto Blender [15] using the Nerfstudio Blender plugin. The\nuser may then create their 3D content directly in Blender,\nusing the imported camera path and mesh as a reference,\nand then render their 3D content, which is then composited\nin 2D over the rendered NeRF trajectory. However, while\n(a) Soft body\n(b) Cloth\nFigure 7: Qualitative results for simulation. In addition to\nrigid body simulation (Fig. 8), our method can also simulate\nsoft bodies. (a) is a twisting Neo-Hookean FEM mesh. (b)\nis a thin shell cloth covering the fox. Please see the supple-\nmentary video for more simulation results.\nthe coarse mesh may be used to generate a visibility mask\nto produce occlusion, this approach is limited in that the\ncoarsely reconstructed mesh is likely not accurate enough\nto provide clean-looking occlusion results, more complex\nocclusion situations such as occluding objects at multiple\nlayers of depth will be challenging to generate individual\nvisibility masks, and this requires manual effort to compos-\nite the results. Appendix F includes some more quantitative\ncomparisons with those methods. Moreover, LumaAI re-\ncently released a closed-source UE plugin, and we run a\ncomparison against it.\nRendering with HDR NeRF. We provide a qualitative\ncomparison between rendering using the standard (LDR)\nNeRF and its HDR counterpart, shown in Figure 5. No-\ntice that in the images rendered with the HDR model, the\nlighting cast from the environment onto the mesh appears\nmuch more faithful to the scene\u2019s true intensity (and there-\nfore directionality). This is also not surprising, considering\nthe longstanding use of High Dynamic Range Image-Based\nLighting in the traditional graphics pipeline [16]. HDR im-\nages are often created by recovering the unknown nonlinear\ntone-mapping function from a series of bracketed-exposure\nLDR images with known exposure duration, then using the\ninverse function to map the images back to linear color\nspace, and merging them into a single 32-bit result, as in-\ntroduced by Debevec and Malik [17]. As a result, HDR im-\nages are much better suited to capture a scene\u2019s full range of\nabsolute and relative radiance values and avoid highly lossy\nclipping, which can be especially problematic in very bright\nparts of an image. HDR NeRF may therefore be interpreted\nas a volumetric HDR lighting map.\nImportantly, our hybrid rendering algorithm allows us to\nutilize such an HDR volumetric radiance map fully. As an\nHDR radiance map is particularly useful for representing\nthe indirect bounce lighting of the scene, future work on ex-\ntending such learned lighting models to direct lighting (e.g.\ndirectional or point sources that cast hard shadows) would\n(a) Static NeRF background\n(b) NeRF (chair) as dynamic object\nFigure 8: Real-time photorealistic gaming on a laptop.\nOur rendering and simulation engine can be interactive and\nrun in real-time on a laptop with an NVIDIA GeForce RTX\n2070 Max-Q GPU. In this example, a user can control the\nmotion of the synthetic glass ball and interact with the back-\nground (collision and light effect).\nbe a promising direction. Our hybrid rendering algorithm is\ncrucial to enable such an investigation.\nSimulation Comparisons.\nBesides static scenes, our\npipeline can also simulate dynamic scenes with SDF-based\ncontact handling. Appendix B shows that the SDF-based\nrepresentation has better collision handling than vanilla\nNeRF density fields. We also compare with NeRF-based\nsimulation [14, 65] in Appendix C and ours achieves better\nperformance.\n4.2. Performance\nIn this section, we will show how our method can be\nused in photorealistic real-time gaming and physically-\nbased simulation. A detailed profiling of our rendering and\nsimulation modules can also be found in Appendix A.\nPhotorealistic Real-time Gaming. In addition to photore-\nalistic rendering, our pipeline is fast, aiming to serve as a\nreal-time neural-fields game engine. As shown in Figure 8,\nwe have implemented an interactive game where players\ncan control the ball\u2019s motion using a keyboard and adjust\ncamera viewing angles using a mouse.\nThe background\nscene, excluding the green ball, is modeled by NeRF. The\nball can have contact with the table and bulldozer. Players\ncan also observe the bending of rays as they pass through\nthe refractive glass ball. Supplementary material includes\na recording of the real-time game. The game runs on a\nlaptop with an NVIDIA GeForce RTX 2070 Max-Q GPU.\nThrough our pipeline, game developers can seamlessly in-\ntegrate animatable objects with photorealistic NeRFs.\nQualitative Results. Our method can simulate different\ntypes of dynamics with the support of Warp [41]. In Fig-\nure 7, the background fox is modeled as NeRF. (a) is a cube\nmodeled by Neo-Hookean FEM mesh, where we can also\nsee the changing light effect as it is twisted. (b) is a piece of\ncloth falling down to the fox. Our pipeline can handle the\ncollision and light effects of such thin shells.\n4.3. Applications\nOur methods could be applied to many situations, where\nNeRFs can improve the realism of synthetic scenes.\nDriving simulation is important for developing, training,\nand testing autonomous driving systems. With the large\nnumber of images captured around roads, people can train\nNeRF for street views [71]. With our method, people can set\nup the driving simulation inside those Photorealistic NeRFs\n(see Figure 9 (a)) and insert synthetic vehicles [4, 74]. Such\nrealistic virtual environments can help minimize the sim-to-\nreal gap in self-driving cars.\nRoom layout design can help users design their homes\nand purchase furniture. After taking pictures and building\na NeRF model for their room, customers can shop furni-\nture [26, 67] virtually and design the room layout as shown\nin Figure 9 (b).\nVirtual try-on using our methods can dynamically simulate\nthe cloth on a human body captured by NeRF. In Figure 9\n(c), we place a cloak on the human body and simulate how\nit swings in the wind.\nDigital human applications are one of the key interests in\nVR/AR and the metaverse. With our methods, users can\neasily collect and build their virtual world by NeRF and then\nrender their human-body model in that scene. This could be\nuseful for movie making, webcasting, virtual performance,\ncyber-tourism, etc. Figure 9 (d) renders a futuristic \u2018mer-\ncury man\u2019 [39] jumping in the park using NeRF.\n5. Conclusion\nIn summary, motivated by integrating NeRF into the tra-\nditional graphics pipeline, our hybrid rendering method can\nrender dynamically changing meshes in a photo-realistic\nNeRF environment, without costly surface reconstruction.\nWe also equip the resulting renderer with a simulator, mak-\ning it suitable as a real-time NeRF-based game engine.\nThere are some limitations in this work. (1) The cur-\nrently implemented renderer cannot cast shadows and illu-\nmination on NeRF points. Decomposing NeRFs can make\nthe relighting more realistic. (2) Our renderer offers ba-\nsic, essential functions; support for environment maps, UV\nmaps, and image textures for higher rendering quality can\nbe a natural extension. (3) Additional interfaces can also en-\nable users to take advantage of more mature infrastructures\nif integrated into more widely-used platforms, e.g. Blender,\nUnreal, etc. (4) We can further improve the runtime perfor-\nmance and integrate the pipeline with larger-scale NeRFs.\nAcknowledgements.\nThis research is supported in part\nby Dr.\nBarry Mersky and Capital One E-Nnovate En-\ndowed Professorships, and ARL Cooperative Agreement\n(a) Room layout design\n(b) Driving simulator\n(c) Virtual try-on\n(d) Digital human\nFigure 9: Application of mixing NeRF with meshes. Our\nmethod can be used for realistic driving simulation, room\nlayout design, virtual try-on, and digital humans.\nW911NF2120076. Yi-Ling would also like to thank the\nsupport from Meta Fellowship and Dr. Chunsheng Hu\u2019s as-\nsistance in drawing the diagram.\nReferences\n[1] Luma AI. Luma ai. https://lumalabs.ai/, 2022. 2\n[2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel:\nHigh-fidelity 6-dof video with ray-\nconditioned sampling. In CVPR, 2023. 2\n[3] Benjamin Attal, Jia-Bin Huang, Michael Zollh\u00a8ofer, Johannes\nKopf, and Changil Kim. Learning neural light fields with\nray-space embedding. In CVPR, 2022. 2\n[4] baehappy1322. The all new ford bronco. 9\n[5] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P.\nSrinivasan, and Peter Hedman. Mip-nerf 360: Unbounded\nanti-aliased neural radiance fields. CVPR, 2022. 1, 7, 13\n[6] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T. Bar-\nron, Ce Liu, and Hendrik P.A. Lensch. Nerd: Neural re-\nflectance decomposition from image collections. In ICCV,\n2021. 3\n[7] Brent Burley. Physically-based shading at disney. 2012. 13\n[8] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki\nNagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,\nLeonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero\nKarras, and Gordon Wetzstein. Efficient geometry-aware 3D\ngenerative adversarial networks. In arXiv, 2021. 2\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. In European\nConference on Computer Vision (ECCV), 2022. 2\n[10] Jianchuan Chen, Ying Zhang, Di Kang, Xuefei Zhe, Linchao\nBao, Xu Jia, and Huchuan Lu. Animatable neural radiance\nfields from monocular rgb videos, 2021. 3\n[11] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang,\nXinchen Yan, Sivabalan Manivasagam, Shangjie Xue, Ersin\nYumer, and Raquel Urtasun. Geosim: Realistic video sim-\nulation via geometry-aware composition for self-driving. In\nCVPR, 2021. 3\n[12] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and An-\ndrea Tagliasacchi. Mobilenerf: Exploiting the polygon ras-\nterization pipeline for efficient neural field rendering on mo-\nbile architectures. arXiv preprint arXiv:2208.00277, 2022.\n2\n[13] Mengyu Chu, Lingjie Liu, Quan Zheng, Erik Franz, Hans-\nPeter Seidel, Christian Theobalt, and Rhaleb Zayer. Physics\ninformed neural fields for smoke reconstruction with sparse\ndata. ACM Transactions on Graphics, 2022. 2\n[14] Simon Cleac\u2019h, Hong-Xing Yu, Michelle Guo, Taylor How-\nell, Ruohan Gao, Jiajun Wu, Zac Manchester, and Mac\nSchwager. Differentiable physics simulation of dynamics-\naugmented neural objects, 10 2022. 2, 8, 13\n[15] Blender Online Community. Blender - a 3D modelling and\nrendering package. Blender Foundation, Stichting Blender\nFoundation, Amsterdam, 2018. 2, 7\n[16] Paul Debevec. Rendering synthetic objects into real scenes:\nBridging traditional and image-based graphics with global\nillumination and high dynamic range photography.\nSIG-\nGRAPH, 1998. 8\n[17] Paul E. Debevec and Jitendra Malik. Recovering high dy-\nnamic range radiance maps from photographs. SIGGRAPH\n97, 1997. 8, 15\n[18] Tao Du, Kui Wu, Pingchuan Ma, Sebastien Wah, An-\ndrew Spielberg,\nDaniela Rus,\nand Wojciech Matusik.\nDiffPD: Differentiable projective dynamics with contact.\narXiv:2101.05917, 2021. 13\n[19] Stefano Esposito, Daniele Baieri, Stefan Zellmann, Andr\u00b4e\nHinkenjann, and Emanuele Rodol`a. Kiloneus: Implicit neu-\nral representations with real-time global illumination. arXiv\npreprint arXiv:2206.10885, 2022. 2\n[20] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021. 2\n[21] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps. In ICCV, 2021. 2\n[22] Michelle Guo, Alireza Fathi, Jiajun Wu, and Thomas\nFunkhouser.\nObject-centric neural scene rendering, 2022.\n3\n[23] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape,\nLight,\nand Material Decomposition from Im-\nages\nusing\nMonte\nCarlo\nRendering\nand\nDenoising.\narXiv:2206.03380, 2022. 2\n[24] Peter Hedman,\nPratul P Srinivasan,\nBen Mildenhall,\nJonathan T Barron, and Paul Debevec. Baking neural ra-\ndiance fields for real-time view synthesis. In ICCV, 2021.\n2\n[25] Binh-Son Hua, Adrien Gruson, Victor Petitjean, Matthias\nZwicker, Derek Nowrouzezahrai, Elmar Eisemann, and\nToshiya Hachisuka. A survey on gradient-domain rendering.\nIn Computer Graphics Forum, volume 38, pages 455\u2013472.\nWiley Online Library, 2019. 2\n[26] Ivo. High-poly modern wood chair. 9\n[27] Wenzel Jakob, Jason Rhinelander, and Dean Moldovan. py-\nbind11 \u2014 seamless operability between c++11 and python,\n2016. https://github.com/pybind/pybind11. 7\n[28] Wenzel Jakob, S\u00b4ebastien Speierer, Nicolas Roussel, Merlin\nNimier-David, Delio Vicini, Tizian Zeltner, Baptiste Nicolet,\nMiguel Crespo, Vincent Leroy, and Ziyi Zhang. Mitsuba 3\nrenderer, 2022. https://mitsuba-renderer.org. 2, 5, 13\n[29] Wenzel Jakob, S\u00b4ebastien Speierer, Nicolas Roussel, and De-\nlio Vicini. Dr.jit: A just-in-time compiler for differentiable\nrendering. Transactions on Graphics (Proceedings of SIG-\nGRAPH), 41(4), July 2022. 5, 13\n[30] Kevin Karsch, Varsha Hedau, David Forsyth, and Derek\nHoiem. Rendering synthetic objects into legacy photographs.\nACM Trans. Graph., 30(6):1\u201312, dec 2011. 3\n[31] Yoni Kasten, Dolev Ofri, Oliver Wang, and Tali Dekel. Lay-\nered neural atlases for consistent video editing. ACM Trans-\nactions on Graphics (TOG), 40(6):1\u201312, 2021. 2\n[32] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field distilla-\ntion. In Advances in Neural Information Processing Systems,\nvolume 35, 2022. 3\n[33] Georgios Kopanas, Thomas Leimk\u00a8uhler, Gilles Rainer,\nCl\u00b4ement Jambon, and George Drettakis. Neural point cata-\ncaustics for novel-view synthesis of reflections. ACM Trans-\nactions on Graphics, 41(6):Article\u2013201, 2022. 2\n[34] Xuan Li, Yi-Ling Qiao, Peter Yichen Chen, Krishna Murthy\nJatavallabhula, Ming Lin, Chenfanfu Jiang, and Chuang\nGan.\nPac-nerf: Physics augmented continuum neural ra-\ndiance fields for geometry-agnostic system identification.\nICLR, 2023. 3\n[35] Yuan Li, Zhi-Hao Lin, David Forsyth, Jia-Bin Huang, and\nShenlong Wang. Climatenerf: Physically-based neural ren-\ndering for extreme climate synthesis. arXiv e-prints, pages\narXiv\u20132211, 2022. 3\n[36] Lingjie Liu, Jiatao Gu, Kyaw Zaw Lin, Tat-Seng Chua, and\nChristian Theobalt. Neural sparse voxel fields. NeurIPS,\n2020. 2\n[37] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu\nSarkar, Jiatao Gu, and Christian Theobalt.\nNeural actor:\nNeural free-view synthesis of human actors with pose con-\ntrol. ACM Trans. Graph.(ACM SIGGRAPH Asia), 2021. 3\n[38] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu\nTseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Jo-\nhannes Kopf, and Jia-Bin Huang. Robust dynamic radiance\nfields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023. 2\n[39] Matthew Loper, Naureen Mahmood, Javier Romero, Ger-\nard Pons-Moll, and Michael J. Black.\nSMPL: A skinned\nmulti-person linear model.\nACM Trans. Graphics (Proc.\nSIGGRAPH Asia), 34(6):248:1\u2013248:16, Oct. 2015. 9\n[40] William E Lorensen and Harvey E Cline. Marching cubes:\nA high resolution 3d surface construction algorithm. ACM\nsiggraph computer graphics, 21(4):163\u2013169, 1987. 6\n[41] Miles Macklin. Warp: A high-performance python frame-\nwork for gpu simulation and graphics.\nhttps://\ngithub.com/nvidia/warp, March 2022.\nNVIDIA\nGPU Technology Conference (GTC). 7, 8\n[42] Miles Macklin, Matthias M\u00a8uller, and Nuttapong Chentanez.\nXpbd: Position-based simulation of compliant constrained\ndynamics. MIG \u201916, page 49\u201354. Association for Computing\nMachinery, 2016. 2\n[43] Miles Macklin, Matthias M\u00a8uller, and Nuttapong Chentanez.\nXpbd: position-based simulation of compliant constrained\ndynamics. In Proceedings of the 9th International Confer-\nence on Motion in Games, pages 49\u201354, 2016. 6\n[44] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi,\nJonathan T Barron, Alexey Dosovitskiy, and Daniel Duck-\nworth. Nerf in the wild: Neural radiance fields for uncon-\nstrained photo collections. In CVPR, 2021. 2\n[45] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\nPratul P. Srinivasan, and Jonathan T. Barron. NeRF in the\ndark: High dynamic range view synthesis from noisy raw\nimages. CVPR, 2022. 5\n[46] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1, 2, 4, 6\n[47] Bailey Miller, Iliyan Georgiev, and Wojciech Jarosz. A null-\nscattering path integral formulation of light transport. ACM\nTransactions on Graphics (Proceedings of SIGGRAPH),\n38(4), July 2019. 2\n[48] Matthias M\u00a8uller, Miles Macklin, Nuttapong Chentanez, Ste-\nfan Jeschke, and Tae-Yong Kim. Detailed rigid body simu-\nlation with extended position based dynamics. In Computer\nGraphics Forum, volume 39, pages 101\u2013112. Wiley Online\nLibrary, 2020. 6\n[49] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a multires-\nolution hash encoding. ACM Trans. Graph., 41(4):102:1\u2013\n102:15, July 2022. 1, 2, 7, 13\n[50] Thomas M\u00a8uller, Marios Papas, Markus Gross, Wojciech\nJarosz, and Jan Nov\u00b4ak.\nEfficient rendering of heteroge-\nnous polydisperse granular media.\nACM Transactions on\nGraphics (Proceedings of ACM SIGGRAPH Asia 2016),\n35(6):168:1\u2013168:14, 2016. 2\n[51] Thomas M\u00a8uller, Fabrice Rousselle, Jan Nov\u00b4ak, and Alexan-\nder Keller. Real-time neural radiance caching for path trac-\ning. ACM Trans. Graph., 40(4):36:1\u201336:16, Aug. 2021. 2\n[52] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas Mueller, and Sanja\nFidler.\nExtracting Triangular 3D Models, Materials, and\nLighting From Images. arXiv:2111.12503, 2021. 2, 7\n[53] Thomas M\u00a8uller, Marios Papas, Markus Gross, Wojciech\nJarosz, and Jan Nov\u00b4ak. Efficient rendering of heterogeneous\npolydisperse granular media. ACM Transactions on Graph-\nics (Proceedings of SIGGRAPH Asia), 35(6):168:1\u2013168:14,\nDec. 2016. 1, 2\n[54] Merlin Nimier-David, Zhao Dong, Wenzel Jakob, and Anton\nKaplanyan. Material and Lighting Reconstruction for Com-\nplex Indoor Scenes with Texture-space Differentiable Ren-\ndering. In Adrien Bousseau and Morgan McGuire, editors,\nEurographics Symposium on Rendering - DL-only Track.\nThe Eurographics Association, 2021. 5, 13\n[55] Merlin Nimier-David, Thomas M\u00a8uller, Alexander Keller,\nand Wenzel Jakob. Unbiased inverse volume rendering with\ndifferential trackers. ACM Trans. Graph. 2\n[56] Jan Nov\u00b4ak. Efficient Many-Light Rendering of Scenes with\nParticipating Media. PhD thesis, Karlsruhe Institute of Tech-\nnology\u201d, May 2014. 1\n[57] Jan Nov\u00b4ak, Iliyan Georgiev, Johannes Hanika, and Wojciech\nJarosz. Monte Carlo methods for volumetric light transport\nsimulation. Computer Graphics Forum (Proceedings of Eu-\nrographics - State of the Art Reports), 37(2), May 2018. 2\n[58] Jan Nov\u00b4ak, Iliyan Georgiev, Johannes Hanika, Jaroslav\nK\u02c7riv\u00b4anek, and Wojciech Jarosz. Monte carlo methods for\nphysically based volume rendering.\nIn ACM SIGGRAPH\n2018 Courses, SIGGRAPH \u201918, pages 14:1\u201314:1, New York,\nNY, USA, 2018. ACM. 2\n[59] Michael Oechsle, Songyou Peng, and Andreas Geiger.\nUnisurf:\nUnifying neural implicit surfaces and radiance\nfields for multi-view reconstruction. In International Con-\nference on Computer Vision (ICCV), 2021. 2\n[60] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nICCV, 2021. 2\n[61] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), dec 2021. 2\n[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans.\nIn CVPR,\n2021. 3\n[63] Matt Pharr, Wenzel Jakob, and Greg Humphreys. Physically\nbased rendering: From theory to implementation. Morgan\nKaufmann, 2016. 2\n[64] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. arXiv preprint arXiv:2011.13961, 2020. 3,\n6\n[65] Yi-Ling Qiao, Alexander Gao, and Ming C. Lin.\nNeu-\nphysics: Editable neural geometry and physics from monoc-\nular videos. In Conference on Neural Information Process-\ning Systems (NeurIPS), 2022. 2, 8, 13, 16\n[66] Konstantinos\nRematas,\nAndrew\nLiu,\nPratul\nP\nSrini-\nvasan, Jonathan T Barron, Andrea Tagliasacchi, Thomas\nFunkhouser, and Vittorio Ferrari. Urban radiance fields. In\nCVPR, 2022. 2\n[67] robula72. coffee table marina. 9\n[68] Viktor Rudnev, Mohamed Elgharib, William Smith, Lingjie\nLiu, Vladislav Golyanik, and Christian Theobalt. Neural ra-\ndiance fields for outdoor scene relighting.\narXiv preprint\narXiv:2112.05140, 2021. 2\n[69] Pratul P Srinivasan,\nBoyang Deng,\nXiuming Zhang,\nMatthew Tancik, Ben Mildenhall, and Jonathan T Barron.\nNerv: Neural reflectance and visibility fields for relighting\nand view synthesis. In CVPR, 2021. 2\n[70] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In CVPR, 2022. 2\n[71] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8248\u20138258, 2022. 2, 9\n[72] Matthew Tancik*, Ethan Weber*, Evonne Ng*, Ruilong\nLi, Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake\nAustin, Kamyar Salahi, Abhik Ahuja, David McAllister, and\nAngjoo Kanazawa. Nerfstudio: A framework for neural ra-\ndiance field development, 2022. 2, 7, 13\n[73] Jiaxiang Tang, Xiaokang Chen, Jingbo Wang, and Gang\nZeng. Compressible-composable nerf via rank-residual de-\ncomposition. arXiv preprint arXiv:2205.14870, 2022. 6\n[74] toivo. Car scene. 9\n[75] Edgar Tretschk, Ayush Tewari, Vladislav Golyanik, Michael\nZollh\u00a8ofer, Christoph Lassner, and Christian Theobalt. Non-\nrigid neural radiance fields: Reconstruction and novel view\nsynthesis of a dynamic scene from monocular video. In IEEE\nInternational Conference on Computer Vision (ICCV). IEEE,\n2021. 3\n[76] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan. Mega-nerf: Scalable construction of large-scale\nnerfs for virtual fly-throughs. In CVPR, 2022. 2\n[77] Delio Vicini, S\u00b4ebastien Speierer, and Wenzel Jakob. Path\nreplay backpropagation: Differentiating light paths using\nconstant memory and linear time. Transactions on Graph-\nics (Proceedings of SIGGRAPH), 40(4):108:1\u2013108:14, Aug.\n2021. 13\n[78] Huan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Men-\nglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neu-\nral radiance field to neural light field for efficient novel view\nsynthesis. arXiv preprint arXiv:2203.17261, 2022. 2\n[79] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao, Yan-\nshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu, and\nLan Xu. Fourier plenoctrees for dynamic radiance field ren-\ndering in real-time. In CVPR, 2022. 2\n[80] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku\nKomura, and Wenping Wang. Neus: Learning neural implicit\nsurfaces by volume rendering for multi-view reconstruction.\nNeurIPS, 2021. 2, 6, 7, 13\n[81] Ziyu Wang, Liao Wang, Fuqiang Zhao, Minye Wu, Lan Xu,\nand Jingyi Yu.\nMirrornerf: One-shot neural portrait radi-\nance field from multi-mirror catadioptric imaging. In 2021\nIEEE International Conference on Computational Photogra-\nphy (ICCP), pages 1\u201312. IEEE, 2021. 2\n[82] Chung-Yi Weng,\nBrian Curless,\nPratul P. Srinivasan,\nJonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-\nmanNeRF: Free-viewpoint rendering of moving people from\nmonocular video. In CVPR, pages 16210\u201316220, June 2022.\n3\n[83] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 9421\u20139431,\n2021. 2\n[84] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin\nShu, Kalyan Sunkavalli, and Ulrich Neumann.\nPoint-\nnerf: Point-based neural radiance fields.\narXiv preprint\narXiv:2201.08845, 2022. 2\n[85] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.\nLearning object-compositional neural radiance field for ed-\nitable scene rendering. In ICCV, 2021. 2\n[86] Yao Yao, Jingyang Zhang, Jingbo Liu, Yihang Qu, Tian\nFang, David McKinnon, Yanghai Tsin, and Long Quan.\nNeilf: Neural incident light field for physically-based mate-\nrial estimation. In ECCV, page 700\u2013716, Berlin, Heidelberg,\n2022. Springer-Verlag. 3\n[87] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Vol-\nume rendering of neural implicit surfaces. In Neurips, 2021.\n2, 6\n[88] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa.\nPlenox-\nels: Radiance fields without neural networks. arXiv preprint\narXiv:2112.05131, 2021. 2\n[89] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[90] Zehao Yu, Anpei Chen, Bozidar Antic, Songyou Peng Peng,\nApratim Bhattacharyya, Michael Niemeyer, Siyu Tang,\nTorsten Sattler, and Andreas Geiger. Sdfstudio: A unified\nframework for surface reconstruction, 2022. 13\n[91] Zehao Yu, Songyou Peng, Michael Niemeyer, Torsten Sat-\ntler, and Andreas Geiger.\nMonosdf: Exploring monocu-\nlar geometric cues for neural implicit surface reconstruc-\ntion. Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 5, 13\n[92] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: Geometry editing\nof neural radiance fields. In Computer Vision and Pattern\nRecognition (CVPR), 2022. 2\n[93] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yan-\nshun Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and\nJingyi Yu. Editable free-viewpoint video using a layered neu-\nral representation. ACM Transactions on Graphics (TOG),\n40(4):1\u201318, 2021. 2\n[94] Kai Zhang, Fujun Luan, Zhengqi Li, and Noah Snavely. Iron:\nInverse rendering by optimizing neural sdfs and materials\nfrom photometric images. In IEEE Conf. Comput. Vis. Pat-\ntern Recog., 2022. 2, 7\nA. Runtime and Memory\nTo break down the time and memory cost of our method, Figure 12 profiles the simulation and rendering modules as\nthe number of synthetic objects increases. In the test scene, we throw n \u2208 {1, 10, 100, 1000, 10000} balls onto a Fox [49]\nrepresented by NeRF. The rendering resolution is 900 \u00d7 450.\nAs seen in the figure, the simulation and rendering memory stay constant at a low level (0.5 GB and 1.0 GB, respectively)\nthroughout the entire experiment. The run time for both modules scales practically linearly w.r.t. the number of objects.\nMoreover, the simulation module is independent of the resolution, and the rendering time scales linearly w.r.t. the number\nof pixels (see left). This pipeline can run in real time (20 FPS) at 600 \u00d7 300 resolution.\nB. Comparison with density fields\nWe compare the simulation quality with different collision geometry. In Figure 14, we drop four balls above a messy\ncounter top [5]. In this simulation scene, (a) models the collision geometry using learned SDF by our NeuS-NGP and (b)\ndirectly uses Marching-Cube mesh from instant-NGP density fields (with resolution 256 \u00d7 256 \u00d7 256). Two of the balls in\n(b) \u2018sink\u2019 into the countertop, which is unrealistic. In contrast, our SDF-based simulation in (a) can correctly simulate the\ninteraction between the balls and the background objects. More dynamics results can be found in our supplementary video.\nC. Comparison with other NeRF-based simulation.\nThere are two recent works [14, 65] that can simulate in neural fields. [14] establish a collision model based on the\ndensity field. In our simulation, we found that the density fields are usually noisy and inadequate to model a surface well for\naccurate contact processing. NeuPhysics[65] aims at differentiable simulation and rendering, and it extracts hexahedra mesh\nfrom learned SDF [80] and simulates the mesh using [18]. NeuPhysics can only simulate existing NeRF objects instead of\nsynthetic objects. Table 13 shows that our method runs at least an order of magnitude faster when simulating a bouncing ball.\nD. Light Source Estimation - Implementation Details\nD.1. Geometry Reconstruction\nGiven our choice to represent light sources as area lights defined on an explicit mesh, we first need to reconstruct the\ngeometry of the scene. We choose to use MonoSDF [91], which leverages monocular geometric priors (depth and normal\nestimation) in the neural implicit surface reconstruction process. Importantly, these priors especially help in reconstructing\nsurfaces that have only a low degree of visual texture, which is very common for walls and flat tabletop surfaces. For\ndifferentiable surface rendering, having reasonably accurate geometry is critical for achieving good convergence, as noisy\ngeometry will tend to bias the result to bad local minima. Empirically, MonoSDF achieves sufficiently accurate geometry\nfor this purpose. Once we have an optimized signed distance field, we convert it to an explicit mesh using Marching Cubes,\nthen UV unwrap the mesh to obtain a UV texture map, using SDFStudio [90, 72]. This step also results in a UV texture\nmap whose values are determined by querying the underlying MonoSDF appearance model, which we use as an albedo UV\ntexture map. The UV-unwrapped mesh is the input to the differentiable surface render, described below.\nD.2. Differentiable Surface Rendering\nWe implement our differentiable surface rendering optimization using Mitsuba3 [28], which is built on Dr. Jit [29], a\njust-in-time compiler that is specialized for rendering use cases. Our inverse rendering procedure is generally based on\n[54], though our specific implementation details are as follows. The UV-unwrapped scene mesh is assigned a Principled\nBSDF material [7], as well as an emission UV texture map, which constitute the inputs to the differentiable renderer. The\nemission UV texture map, Temission, with dimensions (htx, wtx, ctx), is the variable that we are interested in optimizing.\nIt is initialized uniformly with near-zero values. The other BSDF parameters remain fixed (albedo texture map, specular\ntransmission map with default value 1.0, and roughness map with default value 0.5). For rendering, we use a Path-Replay\nBackpropagation (PRB) integrator [77], and limit the light transport simulation to a maximum depth of 3 (2 bounces). This\nstands in contrast to differentiable rasterization approaches to inverse rendering, which are limited to a maximum depth of 2\n(1 bounce), and are therefore unable to account for indirect lighting in the optimization. We also directly render emitters (as\nopposed to only emitting light onto surfaces without being directly visible).\nGiven a set of ground-truth 32-bit HDR images {I1, \u00b7 \u00b7 \u00b7 , In} with dimensions (him, wim, cim) and known camera poses,\nour primary objective is to minimize the rendering loss (mean-squared error) between the ground truth images, and images\n(a) Ground Truth Images\n(b) Resulting image rendered with Mitsuba3\n(c) Optimized Emission UV Texture Map\nFigure 10: Optimizing Emission UV Texture Map. We optimize the (c) Emission UV Texture Map by minimizing a\nrendering loss objective between (a) ground truth images and (b) images rendered with Mitsuba3, a differentiable surface\nrendering engine.\nrendered from identical camera poses. Since the variables that we are optimizing are the emission UV texture map values, we\nalso find that applying uniform L1 regularization to the optimization variable corresponds well to the inductive bias that the\nmajority of the scene does not emit light, and therefore the emission UV texture map should be encouraged to be sparse. For\neach optimization step i \u2208 [1, n] in a given optimization epoch, our objective is therefore to minimize the following, where\n\u03b1 is a tunable hyperparameter:\nminimize\n1\n(him \u2217 wim \u2217 cim)\u2225Ii \u2212 \u02c6Ii\u22252 +\n\u03b1\n(htx \u2217 wtx \u2217 ctx)|Temission|\nIn addition to the rendering loss and L1 regularization in texture space, we also find that periodically clipping low values\nand boosting large values in the emission texture space encourages convergence toward correct results. The motivation is that\nperiodically during the optimization procedure (according to a tunable cadence, where we assume a default period of every\n2 epochs), emission values below a given brightness threshold (also a tunable hyperparameter, with a default value of 0.2),\nare clipped to 0. By assuming that very dim values are not light sources, we avoid local minima. At the same time, values\nthat are above this threshold are likely to be light sources, and therefore, we boost these values, in order to accelerate the\noptimization, since extreme emission values in the 32-bit linear color space may be arbitrarily larger than 1. This part of the\noptimization should be self-correcting: values that are boosted when they shouldn\u2019t be will then be brought down in order to\nsatisfy the rendering loss objective. See Figure 10 for an example of the converged result of optimizing the emission texture\nmap.\nFinally, once the optimization has converged, we post process the mesh by pruning off any triangular faces whose\nvertices all fall below a threshold, by looking up the brightness of each vertex in the emission UV texture map, according to\nthat vertex\u2019s associated UV coordinates. This results in a greatly reduced mesh that only contains faces that should emit light\nonto the rest of the scene, and can therefore be used as an area light for computing the shadow pass of our Hybrid Renderer.\nE. HDR Rendering Experiment Details\nE.1. Data acquisition\nTo train the HDR NeRF, we first construct a set of 32-bit HDR images of a scene, where each image is analagous to a\nsingle LDR image in the baseline NeRF formulation. To create a single one of these HDR images, we shoot a bracketed series\nof LDR exposures that consists of several images (7 in our experiments) captured from a fixed camera pose. We use a single\nCanon 5D MKIII with a 28mm lens, and set the camera to record images in 8-bit JPEG format. Specifically, the camera is\nmounted to a tripod to minimize any minute difference in camera pose between each of the exposures in the bracket, and to\nminimize any optical differences between the images (e.g. depth of field, bloom, etc.) apart from raw exposure value, we only\nadjust the exposure duration (i.e. shutter speed) between images, while the aperture remains fixed. The relative difference in\nexposure between any consecutive images in the bracket is kept constant, though the magnitude of this difference depends\nFigure 11: Inverse Camera Response Function (CRF) showing the recovered nonlinear function mapping from sRGB 8-bit\npixel color to linear 32-bit radiance.\non the degrees of dynamic range of the scene being captured; ideally, within the range of exposures from darkest to brightest,\nevery pixel of the image should be \u201cwell-exposed\u201d in at least one of the images. We generally captured 7 images in the\nbracketed exposure, with 1-2 stops difference between each image.\nE.2. Data preprocessing\nAfter capturing the scene, we preprocess the data using OpenCV. For each bracketed exposure consisting of n images\ntaken from the same camera pose with different exposures, we align the images to ensure the best possible quality of the\nHDR image result, as even slight perturbations in the camera pose may cause degradation. Then, we select a single bracketed\nseries that is \u201crepresentative\u201d of the lighting conditions of the full set, roughly covering the upper and lower limits of the\noverall dynamic range. The representative bracketed series is used to recover the camera\u2019s nonlinear tone-mapping function,\nwhich we will refer to as the Camera Response Function (CRF) [Figure 11] using the method from [17].\nTo improve computational efficiency, we recover the CRF from only one representative image, although the linear system\nthat must be solved could theoretically incorporate pixels from all of the bracketed series as constraints. By applying the\nrecovered CRF uniformly to all series, we achieve consistent levels across all images. After applying the CRF to all bracketed\nseries, we downsample each resulting 32-bit image by a factor of 4x. Finally, for numerical stability during downstream\ntraining, we normalize all of the images by linearly scaling radiance values to the range of [0, 255]. While this may seem\nsimilar to reducing the HDR result back to LDR, all values are scaled uniformly so the relative differences between values\nare preserved, and they are not quantized to integer values. The resulting HDR images are then stored as EXR files.\nNote that for performing camera pose estimation using COLMAP, as is widely done in the NeRF literature, we only use a\nsingle 8-bit LDR image from each bracketed series. Similarly, when training an LDR model for comparison against its HDR\ncounterpart, we use a single image from each bracketed series, where all of the singleton images have the same exposure\nduration.\nE.3. HDR NeRF Implementation Details\nTraining the HDR NeRF is largely the same as training the baseline (LDR) NeRF. In fact, the Instant-NGP implementation\nalready supports training with 32-bit HDR images in EXR format. We simply need to toggle flags \u2018is hdr\u2019 in Instant-\nNGP\u2019s data loader and \u2018linear colors\u2019 in the GUI. Therefore, training is supervised purely in (scaled) linear color space, and\ncorrespondingly, the radiance component of the neural field is defined in linear color space. Optionally, during rendering\nonly, we may choose to apply a tonemapping function uniformly to all of the accumulated pixel radiance values to obtain the\nfinal image in sRGB color space.\nF. Details about the Rendering comparison\nWe conducted a user study where participants were asked to rank the realism, quality, and correctness of rendered results\nfrom three methods, including ours, on the following examples: Mirror (main paper Fig. 2), Garden, and Bicycle (main paper\n(a) Rendering module\n(a) Simulation module\n(c) Rendering module\nFigure 12: Scaling the number of the inserted objects and image resolution. When we increase the number of simulated\nobjects from 1 to 104, the peak simulation memory and rendering memory stay nearly constant. The (a) simulation time\nscales linearly w.r.t. the number of objects, while the (b) rendering time can increase when the number of the inserted meshes\nis too high. The (c) rendering time and memory usage in rendering modules scales linearly w.r.t. the number of pixels.\nRendering\nSimulation\nOurs\n\u223c 0.1 s\n\u223c 0.02 s\nNeuPhysics\n\u223c 5 s\n\u223c 0.5 s\nFigure 13: Run time comparison with NeuPhysics [65]. We simulate a ball that bounces around (left). Compared to\nNeuPhysics, our method supports hybrid NeRF and mesh rendering and runs 25x-50x faster.\n(a) Using learned SDF to simulate\n(b) Using mesh from density fields\nFigure 14: Simulation Comparison. Our simulation (a) using learned SDF as collision proxy geometry can correctly handle\nthe contact between the counter and four balls. (b) Using macrching-cube mesh directly from the density field, it fails to\nresolve collision and two of the four balls sink into the counter.\nFig. 5). Of 15 participants, 13/15 ranked our results highest for Mirror, while 12/15 did so for Garden and 15/15 for Bicycle\n(see Fig. 15). Binomial tests indicate that our images are statistically superior in all three scenes, with significant results at a\nsignificance level of 0.05. The corresponding p-values were found to be 0.035, 0.007, and 6 \u00d7 10\u22125, respectively. We will\ninclude the quantitative results in the revised paper.\nWe also consider metrics such as FID and CLIP [5] scores. However, FID requires a ground truth distribution we cannot\naccess. CLIP is a large language/vision model, but its scores are sensitive to prompts, and it is likely prone to misinterpreting\nreflections. Defining a scalable and robust quantitative metric to measure the visual fidelity of generative tasks remains an\nopen problem.\nMirrorNeRF designs a convenient capture system utilizing mirrors, but it does not focus on the rendering problem, thus\ndoes not overlap with the scope of our work.\nFigure 15: Number of Rank-1s each method receives.\nFigure 16: Comparison between Luma AI (left) and Ours (right).\nLumaAI recently released a UE plugin in April after the submission deadline. They provide binaries but no source code,\nso we only have a limited understanding of how it works. It is stated on their blog that NeRF cannot be used to simulate\n(otherwise, users need to add collider objects manually). Moreover, when we put a mirror in the Garden, shadows in the\nmirror reflections look strange. By contrast, our reflections and shadows are superior (see Fig. 16).\n"
  },
  {
    "title": "FIAT: Fusing learning paradigms with Instruction-Accelerated Tuning",
    "link": "https://arxiv.org/pdf/2309.04663.pdf",
    "upvote": "4",
    "text": "FIAT:\nFUSING\nLEARNING\nPARADIGMS\nWITH\nINSTRUCTION-ACCELERATED TUNING\nXinyi Wang, John Wieting, Jonathan H. Clark\nGoogle DeepMind\n{xinyiwang,jwieting,jhclark}@google.com\nABSTRACT\nLearning paradigms for large language models (LLMs) currently tend to fall within\neither in-context learning (ICL) or full fine-tuning. Each of these comes with their\nown trade-offs based on available data, model size, compute cost, ease-of-use, and\nfinal quality with neither solution performing well across-the-board. In this article,\nwe first describe ICL and fine-tuning paradigms in a way that highlights their natural\nconnections. Based on these connections, we propose a new learning paradigm\ncalled FIAT1 that fuses2 the best of these paradigms together, enabling prompt-\nengineered instructions and chain-of-thought reasoning with the very largest models\nwhile also using similar methods to perform parameter updates on a modestly-sized\nLLM with parameter-efficient tuning. We evaluate FIAT\u2019s effectiveness on a variety\nof multilingual tasks3 and observe that FIAT performs better than both ICL and\nfine-tuning at scales ranging from 100\u201310,000 training examples. We hope that\nFIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.\n1\nINTRODUCTION\nLarge language models (LLMs) show impressive generalization ability to new tasks and languages.\nSome of their most exciting capabilities, such as producing logical reasoning to solve a problem, are\nfound to emerge only when the model size is over a certain threshold, often hundreds of billions of\nparameters (Wei et al., 2022b;a). The impressive capabilities of these models to produce high-quality\nresponses without any task-specific tuning along with the very high cost of further tuning such models\nhas led much recent work to focus on the paradigm of In-Context Learning (ICL)\u2014placing a few\ntask-specific examples and instructions into the model\u2019s input (Brown et al., 2020; Chowdhery et al.,\n2022; Google et al., 2023; OpenAI, 2023).\nAlthough prior work has seen that fine-tuning a model on task data can often lead to superior\nperformance on the downstream task compared to ICL (Scao & Rush, 2021; Schick & Sch\u00fctze,\n2020a;b; Asai et al., 2023), there are significantly fewer recent efforts on fine-tuning models for\ntasks with limited data, perhaps because the time and compute costs associated with tuning a very\nlarge model drives practitioners toward smaller models, abandoning the ability to take advantage of\nemergent model capabilities.\nICL and model fine-tuning each come with their own trade-offs. ICL does not incur any training\ncost and it allows one to utilize the most capable LLMs (Schick & Sch\u00fctze, 2020b; OpenAI, 2023).\nHowever, while ICL can achieve competitive performance on many tasks with a handful of annotated\nexamplars, it often requires very large models to work well and it cannot take advantage of additional\ntraining examples if they do not fit into the context window. For many tasks, this leads to ignoring a\nsubstantial amount of potentially-useful training examples. Fine-tuning, on the other hand, is not\nconstrained by the need to fit training examples into the model\u2019s input, and it can be quite effective\n1We derive the name FIAT from Fusing Learning Paradigms with Instruction Accelerated Tuning.\n2FIAT fuses not only the learning paradigms but the models themselves.\n3We say that these tasks are naturally low-data because no additional data is available for such languages and\nit\u2019s non-trivial to obtain more; we contrast this with artificially low-data scenarios where large data exists, but is\nignored.\n1\narXiv:2309.04663v2  [cs.CL]  12 Sep 2023\nFew-shot In-Context Learning\nFull Fine-tuning\nI\nx\ny\nx\ny\n\u03b8\n\u03b8\nInstruction-tuned LLM \nInput Sequence\nLLM \nUpdated by humans  \nby prompt engineering\nUpdated by gradient \ndescent on task data\nCoT \nReason \nFIAT using both Paradigms\nI\u03b2\nx\nx\ny\n\u03b8\u03b2\n\u03b8\u03c4\n\u03b8PEFT\n\u03c4\nI\u03c4\n\u0302y\u03b2\n\u0302y\u03b2\nInstruction-tuned LLM \nInput Sequence\nInput Sequence\nInstruction-tuned LLM \nUpdated by humans  \nby prompt engineering\nSmall parameter updated by  \ngradient descent on task data\nFigure 1: Overall flow of FIAT and how it compares to ICL and fine-tuning. The colored components\nare updated while building and learning a task-specific instance of FIAT, while other components are\nfixed.\u03b8\u03b2 is the parameters of the larger LLM and I\u03b2 are the instructions used to induce reasoning; \u03b8\u03c4\nare the parameters of a moderately-sized LLM to be tuned and I\u03c4 is its instructions, which helps the\nmodel predict the correct final answer.\neven with smaller language models. These trade-offs tend to lead practitioners to arbitrarily pick a\nparadigm or run costly experiments on these disparate methods in order to choose the best approach.\nWe instead take the view that these two model learning paradigms are in fact complementary. To\nthis end, we propose FIAT\u2014Fusing Learning Paradigms with Instruction-Accelerated Tuning (FIAT),\nwhich utilizes both ICL on very large models and parameter tuning on moderately-sized LLM while\nfusing the common techniques associated with each paradigm. FIAT uses hand-engineering instruction\nprompts that elicit chain-of-thought reasoning from a very large model, while also using the generated\nreasoning and instruction prompts to tune a moderately-size LLM with parameter-efficient tuning.\nFigure 1 shows the workflow of FIAT and how it compares to ICL and fine-tuning.\nIn the remainder of this article, we formally describe the connections between ICL and fine-tuning,\nalong with the various techniques that have developed within each paradigm (\u00a72); we propose FIAT,\nwhich fuses the best of these together and avoids many of the pitfalls of each of the individuals (\u00a72.3);\nwe present experiments demonstrating how FIAT improves over both learning paradigms in data\nscenarios ranging from 100\u201310,000 examples along with ablations detailing where these gains come\nfrom (\u00a73).\n2\nLEARNING PARADIGMS FOR LLMS\nIn this section, we review two popular learning paradigms for LLMs (ICL in \u00a72.1 and parameter\ntuning in \u00a72.2) while considering their strengths and weaknesses, which directly lead to FIAT (\u00a72.3).\n2.1\nIN-CONTEXT LEARNING\nInstructed ICL\nkeeps the parameters of the LLM fixed, but it instead selects an instruction prompt\n(often through manual optimization) to improve the accuracy of the downstream task. Formally, a\nmodel prediction is made by sampling4 a very large pre-trained LLM parameterized by fixed \u03b8 and a\ntextual instruction I:\nP(y|x; \u03b8, I)\n(1)\n4Typically, the sampling is a simple argmax with temperature 0, though this isn\u2019t always the case as in\ntechniques such as majority voting.\n2\nWhile the instructions I are prefixed onto the model input x in practice, we intentionally notate them\nas an argument of the model, which we argue better reflects how they are conceptualized; we will\nbuild on this later.\nChain-of-thought reasoning\npushes instructed ICL a step further by crafting I to induce step-\nby-step reasoning in the output of the model that improves the model\u2019s ability to arrive at a correct\nprediction (Wei et al., 2022b). This allows auto-regressive inference to output observations about\nthe input or solve sub-problems of the overall task that future decoding steps can leverage when\npredicting the final answer; it may also elicit textual patterns that the model saw during pre-training,\nthat would otherwise be difficult to access in the model\u2019s latent feature space (e.g. via fine-tuning).\nFew-shot ICL\nFew-shot ICL differs from instructed ICL in that its instructions I are composed\nof a small number of examplars selected among training examples D that have been formatted as a\ntextual input to the model via instructions.\nInstruction-tuned Base Models\nInstruction-tuned models such as FLAN and T0 (Sanh et al., 2021;\nChung et al., 2022; Longpre et al., 2023) often provide significant improvements on ICL compared to\nusing a pre-trained model. This is because instruction-tuning is essentially a second stage pretraining\nusing a set of multitask data whose distribution is closer to the downstream task.\nThe ICL paradigm achieves competitive results on various tasks with no or only a handful of annotated\nexamples. While it does not incur any additional model tuning cost, ICL often has high inference\ncost because it requires LLMs over a certain size to work well, especially when using techniques\nsuch as chain-of-thought. It also cannot take advantage of additional task data beyond what fits into\nthe context window of the model.\n2.2\nPARAMETER TUNING\nFull-Parameter Fine-tuning\nGiven pre-trained parameters \u03b8 of a LLM to tune,5 standard fine-\ntuning simply optimizes all parameters of the model on task-specific supervised training data D\naccording to:\nP(y|x; \u03b8)\n(2)\nThe optimization of \u03b8 is similar in purpose to the process of human prompt engineering of I in ICL.\nSince model fine-tuning does not have to fit training data into the context window of the model, it is\nmore effective when there are slightly more training examples available. Fine-tuning also works well\non smaller language models with enough training examples, leading to faster inference. However,\nfine-tuning incurs additional training cost and requires access to model parameters, while some of the\nmost capable LLMs are available for inference-only API access. The model could also easily overfit\nto the training examples due to catastrophic forgetting (Goodfellow et al., 2013), especially for tasks\nwith limited data.\nParameter-efficient Fine Tuning\n(PEFT) improves the tuning procedure by using a learning\nparameterization \u03b8PEFT where |\u03b8PEFT| \u226a |\u03b8|. Besides reducing the danger of overfitting, this learning\ntechnique also avoids forgetting features that may be useful for generalization beyond the training set.\nSimilarly, ICL avoids catastrophic forgetting by only modifying the input to the model while keeping\nthe parameters fixed.\n2.3\nFUSING LEARNING PARADIGMS WITH FIAT\nIn this section, we construct FIAT, motivating the purpose of each design choice in terms of modeling\ncapabilities. ICL and fine-tuning each have compelling strengths along with pitfalls, which we\nsummarize in Table 1. At a high level, we observe that these properties are largely complementary.\n5In practice, |\u03b8| tends to be much smaller for fine-tuning than for ICL.\n3\nICL\nFine-tuning\nStrengths\nWorks well with small model\nNo\nYes\nSupports large training data\nNo\nYes\nSupports chain-of-thought reasoning\nYes\nNo\nUsage of instruction prompts\nYes\nNo\nChallenges\nNo parameter updates\nYes\nNo\nAvoids catastrophic forgetting\nYes\nNo\nTable 1: Comparison of the ICL and fine-tuning learning paradigms, according to common usage\npatterns.\nReflecting on these abilities of ICL and fine-tuning, we seek an approach that is capable of:\n\u2022 Instruction following: follows human-engineered instructions to achieve high quality predictions;\n\u2022 Chain-of-thought reasoning: produces intermediate text that helps the model toward correct\npredictions;\n\u2022 Parameter tuning: refines its internal representation to align with a moderate to large number of\nsupervised training examples; and\n\u2022 Data scaling: provides high quality models with data scales from 100 to 1000\u2019s of examples.\nModel stacking via CoT-augmented Tuning\nWe begin with the observation that chain-of-thought\nprompting is typically not supervised, but rather induced via carefully-written instructions. Motivated\nby this, we fuse two models for learning and inference: a big model \u03b2 with all the most powerful\nemergent capabilities of LLMs, and a tunable model \u03c4 whose size can be flexibly chosen depending\non the capacity needs of the task of interest. We assign the responsibility of chain-of-thought inference\nto \u03b2 and then provide its textual predictions \u02c6y\u03b2 to the tunable model; it can then learn how to best\nuse these inputs (e.g. chain-of-thought explanations) based on how useful they are with regard to\npredicting the supervised outputs. The parameters \u03b8\u03b2 remain fixed as we do not have nor require any\ndirectly supervised data for its sub-task.\nInstruction-augmented Tuning\nCrafting a good instruction prompt is known to be essential to\nhigh-quality ICL performance, and so we naturally include instructions I\u03b2 to generate reasoning and\nexplanations as a first step. Although instructions are typically not used for smaller tunable model I\u03c4,\nwe observe that instructions have the potential to benefit tuning as well. We speculate that instructions\nhelp better align a task\u2019s inputs with the distribution seen during pre-training, allowing the model\nto not only converge faster but also make fewer parameter updates. This, in turn, avoids the risk of\ncatastrophic forgetting associated with excessive parameter updates. Therefore, FIAT also provides\nseparate instructions I\u03c4 for the tunable model.6\nPervasive Instruction-tuned Models\nAlready, instruction-tuned models have become the standard\nfor ICL; we use such models as \u03b8\u03b2 in all of our experiments. However, given FIAT\u2019s use of Instruction-\naugmented Tuning, we also depart from the common practice of fine-tuning starting from models\npre-trained primarily on span corruption objectives and instead initialize with instruction-tuned\ncheckpoint (Longpre et al., 2023). This makes optimization easier since the model is already\nexpecting instructions; this can be especially beneficial in limited training data scenarios.\nParameter-efficient Tuning\nSo far, we have added chain-of-thought reasoning, instruction follow-\ning in tuning, and instruction-tuned initialization to FIAT\u2019s design, all of which move the pre-tuning\nmodel and the task definition toward each other in terms of increasing the probability of the desired\noutput. We hypothesize that parameter-efficient tuning is a particularly good fit for optimizing \u03b8\u03c4\nin FIAT over the training data, because large changes to the model parameters \u03b8\u03c4 should not be\n6In FIAT, instructions can be viewed as serving purpose analogous to a Bayesian prior in earlier statistical\nlearning methods: They allow encoding human knowledge into the learning procedure alongside supervised data\nthat empirically estimates parameters. However, textual instructions are a far more natural way of doing this\nthan the hyperparameters of a Dirichlet.\n4\nAlgorithm 1: Model building with FIAT\nInput: \u03b8\u03b2, \u03b8\u03c4, D\nOutput: \u03b8\u2032\n\u03c4, I\u03b2, I\u03c4\n// Write reasoning instructions & select exemplars.\nI\u03b2 = PROMPTENGINEERING(D, \u03b8\u03b2)\n// Write tuning instructions, based on large model.\nI\u03c4 = PROMPTENGINEERING(D, \u03b8\u03b2)\n// Initialize parameter-efficient tuning.\n\u03b8PEFT\n\u03c4\n\u2190 INIT(\u03b8\u03c4)\n// Iterate over examples or batches of data.\nfor x, y \u2208 D do\n// Generate expansions, explanations, reasoning.\n\u02c6y\u03b2 = arg maxy P(y|x; \u03b8\u03b2, I\u03b2)\n// Optimize using parameter-efficient update.\ng\u03c4 = \u2207PEFTP(y|x, \u02c6y\u03b2; \u03b8\u03c4, \u03b8PEFT\n\u03c4\n, I\u03c4)\n\u03b8PEFT\n\u03c4\n\u2190 UPDATE(\u03b8PEFT\n\u03c4\n, g\u03c4)\nend\n// Apply PEFT updates to final tuned model.\n\u03b8\u2032\n\u03c4 \u2190 \u03b8\u03c4 \u2295 \u03b8PEFT\n\u03c4\nAlgorithm 2: Inference with FIAT\nInput: x, I\u03b2, I\u03c4, \u03b8\u03b2, \u03b8\u2032\n\u03c4\nOutput: y\n// Generate expansions, explanations, reasoning.\n\u02c6y\u03b2 = arg maxy P(y|x; \u03b8\u03b2, I\u03b2)\n// Infer final output using tuned model.\ny = arg maxy P(y|x, \u02c6y\u03b2; \u03b8\u2032\n\u03c4, I\u03c4)\nFigure 2: Model building and inference with FIAT. Left: Model building with FIAT begins with\ninteractive prompt engineering of the instructions I. I\u03b2 specifies how to perform reasoning using\nfew-shot exemplars on \u03b8\u03b2\u2014i.e. behaviors for which we have no large-scale annotations, while I\u03c4\nspecifies guidance to the tuned model \u03b8\u03c4 for using the generated reasoning and input to produce a\nfinal output. Both \u03b8\u03b2 and \u03b8\u03c4 are instruction-tuned models and only \u03b8\u03c4 is updated during training via\nparameter-efficient tuning. Right: Inference with FIAT is very simple, requiring only: (1) a call to\nthe large generative model using the fixed pre-trained parameters \u03b8\u03b2 and the reasoning instructions\nI\u03b2; and (2) a call to the tuned model \u03b8\u03c4 along with the associated task instructions I\u03c4.\nnecessary given a good initialization.7 Formalizing all the above modifications, we arrive at the final\nformulation of FIAT used for fine-tuning and inference in Alg. 1 and Alg. 2.\n3\nEXPERIMENTS\nDatasets\nOne of our primary objectives in selecting datasets that naturally cover a broad variety\nof training data sizes. We consider tasks ranging from classification to exercising a model\u2019s ability\nto generate short answers, and we include a large number and variety of languages to evaluate the\ngenerality of the method.\nFirst, we use XOR-ATTRIQA (Muller et al., 2023), a classification task where model is asked to\npredict whether the provided answer to the question is supported by the given passage context, which\nincludes 5 languages with 262 examples total. We refer to this as the O(100) data scenario.\nWe also study FIAT\u2019s behavior on the Cross-lingual QA task of XTREME-UP (Ruder et al., 2023).\nThis data is an expansion of the XOR QA8 dataset (Asai et al., 2020), a cross-lingual variant of the\nTyDi QA (Clark et al., 2020) dataset. This task asks a model to predict the correct English answer span\ngiven a non-English question and an English answer passage; this task also includes the possibility\nthat the passage does not contain a correct answer, making it more challenging. Cross-lingual QA is\na particularly important task for languages that have very little answer content as it enables providing\nanswers to questions that would otherwise be unanswerable using only in-language content. We\nprovide results on two focus sets. First, we use the subset of 20 Indic languages in XTREME-UP\nCross-lingual QA where each language has about 300 examples, to allow for studying a scenario with\n7In FIAT, we use LoRA (Hu et al., 2021) to parameterize the tuning procedure because it does not induce\nadditional inference cost. Future work should consider other methods such as soft prompt tuning (Lester et al.,\n2021).\n8XOR QA stands for cross-lingual open-retrieval question answering; note the difference between XOR QA\nand XOR-ATTRIQA.\n5\nXOR-ATTRIQA\nXTREME-UP\nCross-lingual QA (Indic)\nXTREME-UP\nCross-lingual QA (Full)\nO(100)\nO(1000)\nO(10000)\n\u03b8\u03c4\n\u03b8\u03b2\nMethod\nAcc / AUC-PR\nF1\nF1\n\u2014\u2013\nL\nICL\n78.6 / \u2014\u2013\u2020\n68.9\n69.2\nXS\n\u2014\u2013\nFine-tune\n90.5 / 52.1\n63.5\n75.5\nL\nFIAT\n94.0 / 78.1\n73.6\n77.8\nS\n\u2014\u2013\nFine-tune\n90.6 / 54.5\n67.1\n77.8\nL\nFIAT\n93.9 / 77.5\n77.3\n79.3\nGain over best baseline\n+3.5 / +26.0 (vs S fine-tune)\n+8.4 (vs ICL)\n+1.5 (vs S fine-tune)\nTable 2: Overall results of FIAT and typical baselines. While we provide improvements with regard\nto the best baseline, we also point out that the best baseline often differs between ICL and fine-tuning,\nespecially at smaller model sizes; this leaves practitioners to empirically determine the best course of\naction. \u2020AUC-PR is not computed for the ICL because outputs are text-only.\nmoderate data; we refer to this as the O(1000) data scenario. We also study the full XTREME-UP\nCross-lingual QA task which has 22,500 examples across 27 languages where the 5 high-resource\nlanguages have more than 2500 examples each; we refer to this as the O(10,000) data scenario.9\nTogether, these tasks allow us to test our methods on three different data size scenarios from small\n100\u2019s to over training 20,000 examples. Details of the languages and the dataset size can be found in\nApp. A.1.\nModels\nWe use PaLM-2 (Google et al., 2023) as our base model, and we experiment with\ninstruction-tuned models using the FLAN mixture (Chung et al., 2022). We use PaLM-2 L as\nM\u03b2 and we use PaLM-2 XS and S for M\u03c4.\nBaselines\nWe compare to both ICL and fine-tuning baselines. For ICL, we use PaLM-2 L with chain-\nof-thought reasoning (Wei et al., 2022b). We include 4 few-shot exemplars with hand-written chain-\nof-thought explanations in English for each of the 5 languages in the XOR-ATTRIQA Attribution\ntask.10 for a total of 20 exemplars. However, for XTREME-UP cross-lingual QA, it was not feasible\nto hand-engineer prompts for each of the 27 languages. Therefore, we hand-write 4 chain-of-thought\nexplanations based on Bengali exemplars,11 and use the same ICL examples for all 20 languages.\n3.1\nRESULTS\nWe present the performance of the baselines (ICL and fine-tuning) and our FIAT framework for all\nthree data settings in Table 2. We show the average scores across all languages in each dataset for\nsimplicity, and we provide the result for each language in App. A.2. Looking at the baselines, we find\nthat few-shot ICL using PaLM-2 L model is quite competitive without any additional model tuning,\nbut still lags behind PaLM-2 S fine-tuned on a relatively small amount of task data. However, we\nfind that the best baseline differs between ICL and fine-tuning PaLM-2 XS across different tasks and\ndata size settings. If one were choosing between just ICL or fine-tuning, this inconsistency makes it\ndifficult to determine the best course of action without empirical comparisons. On the other hand,\nFIAT offers the best performance by combining the strengths of both ICL and fine-tuning.\n4\nABLATIONS AND ANALYSIS\nIn this section, we study the effect of individual design decisions within FIAT and present the results\nin Table 3, and drawing conclusions from them below. In the end, we find that while certain design\n9We report the average result on the under-represented languages, following the recommendations of the\nXTREME-UP benchmark.\n10During manual prompt engineering, we used Google Translate to assist with explanation annotation.\n11Note that while the exemplars have Bengali questions, we instruct the model to carry out its reasoning in\nEnglish.\n6\nXOR-ATTRIQA\nXTREME-UP\nCross-lingual QA: Indics\nXTREME-UP\nCross-lingual QA: Full\nO(100)\nO(1000)\nO(10000)\n\u03b8\u03c4\n\u03b8\u03b2\nMethod\nAcc / AUC-PR\nF1\nF1\n\u2014\u2013\nL\nFew-shot ICL\n78.6 / \u2014\u2013\n68.9\n69.2\nXS\nL\nFIAT\n94.0 / 78.1\n73.6\n77.8\n\u2014\u2013\nw/o CoT-augmentated tuning\n94.0 / 80.3\n70.7\n76.0\n\u2014\u2013\nw/o Instruction-augmented tuning\n93.5 / 72.4\n69.8\n76.4\n\u2014\u2013\nw/o Parameter-efficient tuning\n93.7 / 69.8\n67.8\n75.8\n\u2014\u2013\nw/o Instruction-tuned base model\n90.5 / 52.1\n63.5\n75.5\nS\nL\nFIAT\n93.9 / 77.5\n77.3\n79.3\n\u2014\u2013\nw/o CoT-augmentated tuning\n94.7 / 80.7\n76.7\n79.8\n\u2014\u2013\nw/o Instruction-augmented tuning\n94.1 / 71.6\n75.3\n79.1\n\u2014\u2013\nw/o Parameter-efficient tuning\n94.7 / 76.2\n72.3\n78.5\n\u2014\u2013\nw/o Instruction-tuned base model\n90.6 / 54.5\n67.1\n77.8\nTable 3: Ablations showing the contribution of each modification within the FIAT recipe; each\nremoval is cumulative with the one above. We observe that each modification tends to make a\nsubstantial positive impact on at least one scenario. The bottom line in each block is equivalent to\ntraditional fine-tuning.\nchoices tend to have a larger effect on some settings than others, each tends to have substantial\ncontributions in some area, and together the overall modeling recipe is very effective as a whole.\nInstructed-tuned base models improve final quality of fine-tuned models.\nThe instruction-tuned\nFlan XS model improves over the base model on all datasets, especially on XOR-ATTRIQA and\nXTREME-UP Cross-lingual QA Indic, where the total amount of task data is around O(100) to\nO(1000). This indicates that instruction-tuned models are not only beneficial for ICL, but can\nalso be beneficial for fine-tuning on limited data (Longpre et al., 2023). However, the advantage\nof instruction-tuned model on XTREME-UP Cross-lingual QA decreases from the Indic (O(1000)\ntraining examples) to Full (O(10000) training examples), indicating that instruction-tuned model is\nless helpful when the fine-tuning dataset is large.\nInstruction-augmented Tuning generally leads to significant improvements.\nAdding an appro-\npriate prompted format to the task data is generally beneficial for all tasks. This result indicates that\nprompt engineering is not only helpful for direct few-shot ICL, but also has a positive impact on\nmodel fine-tuning. Prompted tuning is especially helpful for XOR-ATTRIQA and XTREME-UP Cross-\nlingual QA Indic, where the amount of task data is very limited. This is because the prompt format\naligns the distribution of downstream task closer to the model pretraining distribution, which allows\nthe pretrained model to generalize to the downstream task with a small amount of task examples.\nCoT-augmentated Tuning is helpful for most tasks.\nOur CoT-augmented Tuning can lead to\nlarge improvement for XTREME-UP Cross-lingual QA Indic task. Surprisingly, it does not help XOR-\nATTRIQA, which is contradictory to findings from prior works which show that explanations can be\nespecially helpful for classification tasks (Hsieh et al., 2023; Zhou et al., 2023). We hypothesize that\nthis is because the model already performs quite well on XOR-ATTRIQA without having access to\nthe explanations (over 90 percent accuracy) and this task may be reaching its saturation point.\nCoT-augmented Tuning is even more helpful for tasks and languages with lower performance.\nWe analyze the relationship between the gains brought by CoT-augmentated Tuning on the XTREME-\nUP Cross-lingual QA tasks. Figure 3 shows the improvement in F1 score of different languages\nversus a baseline model\u2019s F1 score that lacks CoT-augmented Tuning. We can see that there is an\ninverse relationship between the benefit of CoT-augmented Tuning and the baseline model score,\nindicating that CoT is more beneficial for harder tasks or languages where the model could not\nperform well without the help of the CoT augmentation. This means that while we see meaningful\ngains in aggregate, for individual languages (or, more generally, individual tasks and use cases), CoT\ncan have an out-sized impact on quality.\n7\n60\n80\nF1 Score\n0.0\n2.5\n5.0\n7.5\n10.0\n\u2206 F1 Score with CoT\nFigure 3: Gains in F1 on XTREME-UP Cross-\nlingual QA with CoT-augmented Tuning. The\nlower performing languages tend to benefit\nmore from CoT augmentation.\nMethod\nF1\nGains\nBaseline\n70.7\n\u2014\u2013\nDistilled CoT (Hsieh et al., 2023)\n72.5\n+ 1.8\nOur CoT-augmented Tuning\n73.6\n+ 2.9\nFigure 4:\nPerformance on XTREME-UP\nCross-lingual QA Indic compared to the base-\nline without CoT. Our CoT-augmented Tun-\ning method significantly outperforms previ-\nous methods on distilling CoT.\n0\n500\n1000\nStep\n30\n40\n50\n60\n70\nF1\nBase\nPrompted\nFigure 5: The validation F1 score through-\nout training on XTREME-UP Cross-lingual\nQA for methods with and without Instruction-\naugmented Tuning. Instruction-augmented\nTuning out-performs baseline and it has much\nbetter performance at step 0, before any\nmodel optimization.\nF1\n0.0\n0.5\n1.0\n1.5\nGain over baseline\nXOR QA Indic\nBase\nFLAN\nAUC-PR\n0\n1\n2\n3\n4\nGain over baseline\nAttribution\nBase\nFLAN\nFigure 6:\nImprovement with Instruction-\naugmented Tuning for the model with\nand without instruction-tuning. Instruction-\naugmented Tuning is generally helpful for\nboth types of models, and it tends to be more\nbeneficial for instruction-tuned models\nCoT-augmented Tuning leads to better quality than CoT distillation.\nRecent work proposed\ndistilled CoT, which uses the explanation as a multitask output target, so that the model does not need\nto generate additional explanations at test time (Hsieh et al., 2023). Here we compare the performance\nof these two different ways of using the CoT explanations and list the performance on cross-lingual\nQA tasks in Figure 4. Despite incurring higher inference cost, our CoT augmentation method further\nout-performs the distilled CoT by a large margin on the harder XTREME-UP Cross-lingual QA Indic\ntask. In general, we view distillation as an orthogonal technique to FIAT, which is aimed at efficiency\nover quality.\nAdding instructions to tuning helps from beginning to end.\nIn Figure 5, we plot the training\ncurves of Flan PaLM-2 S model with and without Instruction-augmented Tuning. We can see\nthat adding instructions to tuning leads to much better performance at step 0, before any model\noptimization. This indicates that adding the instructions to the task data during fine-tuning12 can\nsignificantly improve the zero-shot performance of the model, probably because it makes the task\n12Note we use the term instruction-augmented tuning to differentiate from the separate concepts of\ninstruction-tuned base models, which creates base models that are better able to follow instructions of\nspecific tasks later, and prompt tuning, which learns soft prompt embeddings.\n8\ndata more similar to the data used in the instruction tuning stage. Importantly, this also implies that\nthe model parameters don\u2019t need to move as far away from their starting point in order to achieve the\nsame level of quality, reducing the risk of catastrophic forgetting. However, the model does not only\nreach the same level of quality with less steps, but also manages to exceed the quality of a model\nwithout instructions.\nInstruction-augmented Tuning helps more with an instruction-tuned base model.\nWe compare\nthe effect of prompted tuning on models with and without instruction tuning. Figure 6 shows that\nprompted tuning generally brings improvements for both the base model without instruction tuning\nand the Flan model with instruction tuning, while the gains on the instruction-tuned Flan model tend\nto be slightly larger and more consistent. This is likely because the data format we used for prompted\ntuning (task instructions followed by the input) is more similar to the Flan data mixture used for\ninstruction tuning.\n5\nRELATED WORK\nInstruction Tuning\nInstruction-tuned models (Wei et al., 2021; Longpre et al., 2023) often have\nbetter performance for few-shot ICL tasks than base language models since they are already primed\nto following instructions due to being fine-tuned on a diverse set of tasks. Using instruction-tuned\nmodels is a key component of FIAT.\nIn-Context Learning\nIn in-context learning, the parameters of the LLM remain fixed and a prompt\ncontaining a few examples along with reasoning steps is used to prime the model for solving similar\ntasks (Nye et al., 2021; Wei et al., 2022b). In-context learning works best for large language models.\nFIAT uses this capability of large language models, along with fine-tuning, to power small language\nmodels in the low-data regime.\nKnowledge Transfer from Larger to Smaller LLMs\nA popular prior method for transferring\nknowledge from large models to smaller ones is model distillation (Hinton et al., 2015), where the\noutputs of a larger model are used as a training signal for a smaller one. Other approaches include\nusing the larger language model to generate data and then using this data to train smaller models.\nMore recently, the latter has approach has been extended to generate reasoning steps which are\nprovided as fine-tuning data for the smaller language model (Magister et al., 2022; Huang et al., 2022;\nLi et al., 2022; Ho et al., 2023; Hsieh et al., 2023; Fu et al., 2023; Zhu et al., 2023; Li et al., 2023).\nUnder-represented Languages\nMost work that trains large language model and uses them for\ndownstream tasks focus on English or the collection of 100 or so languages where there are large,\neasily available corpora (ImaniGooghari et al., 2023). Tail languages have often been ignored by\nlanguage technologies due to lack of available corpora (Nayak & Joshi, 2022). Recent works has\nfocused on tail languages outside of these head languages (Bapna et al., 2022; Ruder et al., 2023). In\nthis work, we make the low-data regime the focus of our efforts, which is especially useful for tail\nlanguages.\nFine-tuning smaller LLMs\nWhile fine-tuning with prompts has been studied for encoders pre-\ntrained with masked language modeling objectives (Scao & Rush, 2021), we show that it is also\nimportant to fine-tuning generative language models. For example, some works show that fine-tuning\na smaller language model is a more competitive and efficient method for practical low-data learning\nproblems than few-shot ICL (Asai et al., 2023; Ruder et al., 2023). Agrawal et al. (2022) propose to\nsynthetic QA data generated from very large LLM to improve the performance of a smaller model.\n6\nCONCLUSION\nWe have presented FIAT, a method that fuses the ICL and fine-tuning learning paradigms and leads to\nimproved model predictions across a variety of data scenarios, ranging from 100\u201310,000 training\nexamples. We hope FIAT provides a practical way of harnessing the full potential of LLMs without\nneeding to make a hard choice between learning paradigms.\n9\nREFERENCES\nPriyanka Agrawal, Chris Alberti, Fantine Huot, Joshua Maynez, Ji Ma, Sebastian Ruder, Kuzman\nGanchev, Dipanjan Das, and Mirella Lapata. Qameleon: Multilingual qa with only 5 examples.\narXiv preprint arXiv:2211.08264, 2022.\nAkari Asai, Jungo Kasai, Jonathan H Clark, Kenton Lee, Eunsol Choi, and Hannaneh Hajishirzi. Xor\nqa: Cross-lingual open-retrieval question answering. arXiv preprint arXiv:2010.11856, 2020.\nAkari Asai, Sneha Kudugunta, Xinyan Velocity Yu, Terra Blevins, Hila Gonen, Machel Reid, Yulia\nTsvetkov, Sebastian Ruder, and Hannaneh Hajishirzi. Buffet: Benchmarking large language models\nfor few-shot cross-lingual transfer. arXiv preprint arXiv:2305.14857, 2023.\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng\nNiu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. Building machine translation\nsystems for the next thousand languages. arXiv preprint arXiv:2205.03983, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022.\nJonathan H Clark, Eunsol Choi, Michael Collins, Dan Garrette, Tom Kwiatkowski, Vitaly Nikolaev,\nand Jennimaria Palomaki. Tydi qa: A benchmark for information-seeking question answering in ty\npologically di verse languages. Transactions of the Association for Computational Linguistics, 8:\n454\u2013470, 2020.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. Specializing smaller language\nmodels towards multi-step reasoning. arXiv preprint arXiv:2301.12726, 2023.\nIan J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An empirical investi-\ngation of catastrophic forgetting in gradient-based neural networks. arXiv preprint arXiv:1312.6211,\n2013.\nGoogle, Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2 technical\nreport. arXiv preprint arXiv:2305.10403, 2023.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nNamgyu Ho, Laura Schmid, and Se-Young Yun. Large language models are reasoning teachers. In\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 14852\u201314882, Toronto, Canada, July 2023. Association for Computational\nLinguistics.\nCheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alexander Ratner,\nRanjay Krishna, Chen-Yu Lee, and Tomas Pfister. Distilling step-by-step! outperforming larger\nlanguage models with less training data and smaller model sizes. arXiv preprint arXiv:2305.02301,\n2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.\nLarge language models can self-improve. arXiv preprint arXiv:2210.11610, 2022.\n10\nAyyoob ImaniGooghari, Peiqin Lin, Amir Hossein Kargaran, Silvia Severini, Masoud Jalili Sa-\nbet, Nora Kassner, Chunlan Ma, Helmut Schmid, Andr\u00e9 FT Martins, Fran\u00e7ois Yvon, et al.\nGlot500: Scaling multilingual corpora and language models to 500 languages. arXiv preprint\narXiv:2305.12182, 2023.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691, 2021.\nLiunian Harold Li, Jack Hessel, Youngjae Yu, Xiang Ren, Kai-Wei Chang, and Yejin Choi. Sym-\nbolic chain-of-thought distillation: Small models can also\" think\" step-by-step. arXiv preprint\narXiv:2306.14050, 2023.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen, Xinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, et al. Explanations from large language models make small reasoners better.\narXiv preprint arXiv:2210.06726, 2022.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. The flan collection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688, 2023.\nLucie Charlotte Magister, Jonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn.\nTeaching small language models to reason. arXiv preprint arXiv:2212.08410, 2022.\nBenjamin Muller, John Wieting, Jonathan H Clark, Tom Kwiatkowski, Sebastian Ruder, Livio Baldini\nSoares, Roee Aharoni, Jonathan Herzig, and Xinyi Wang. Evaluating and modeling attribution for\ncross-lingual question answering. arXiv preprint arXiv:2305.14332, 2023.\nRavindra Nayak and Raviraj Joshi. L3Cube-HingCorpus and HingBERT: A code mixed Hindi-\nEnglish dataset and BERT language models. In Proceedings of the WILDRE-6 Workshop within\nthe 13th Language Resources and Evaluation Conference, pp. 7\u201312, Marseille, France, June 2022.\nEuropean Language Resources Association.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David\nBieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work:\nScratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114,\n2021.\nOpenAI. Gpt-4 technical report, 2023.\nSebastian Ruder, Jonathan H Clark, Alexander Gutkin, Mihir Kale, Min Ma, Massimo Nicosia,\nShruti Rijhwani, Parker Riley, Jean-Michel A Sarr, Xinyi Wang, et al. Xtreme-up: A user-centric\nscarce-data benchmark for under-represented languages. arXiv preprint arXiv:2305.11938, 2023.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training enables\nzero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\nTeven Le Scao and Alexander M Rush. How many data points is a prompt worth? NAACL, 2021.\nTimo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classification and\nnatural language inference. arXiv preprint arXiv:2001.07676, 2020a.\nTimo Schick and Hinrich Sch\u00fctze. It\u2019s not just size that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118, 2020b.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022a.\n11\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022b.\nYangqiaoyu Zhou, Yiming Zhang, and Chenhao Tan. Flame: Few-shot learning from natural language\nexplanations. Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics, 2023.\nXuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen Zhou. Pad: Program-aided\ndistillation specializes large models in reasoning. arXiv preprint arXiv:2305.13888, 2023.\n12\nSplit\nbn\nfi\nja\nru\nte\nTrain\n40\n66\n20\n84\n52\nValidation\n218\n150\n578\n136\n174\nTest\n2822\n1318\n1908\n1268\n2146\nTable 4: Dataset size for XOR-ATTRIQA.\nSplit\nas\nbho\nbrx\ngbm\ngom\ngu\nhi\nhne\nkn\nmai\nml\nmni\nmr\nmwr\nor\npa\nps\nsa\nta\nur\nar\nbn\nfi\nja\nko\nru\nte\nTrain\n323\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n326\n3159\n377\n2467\n2926\n3327\n2560\n373\nValidation\n356\n358\n357\n365\n365\n371\n519\n372\n373\n369\n373\n380\n385\n386\n386\n385\n384\n385\n384\n387\n941\n618\n978\n727\n861\n731\n468\nTest\n633\n631\n633\n634\n629\n630\n1049\n629\n631\n635\n629\n628\n633\n632\n632\n624\n633\n630\n630\n634\n582\n397\n606\n471\n548\n448\n333\nTable 5: Dataset size for XTREME-UP Cross-lingual QA.\nA\nAPPENDIX\nA.1\nLIST OF LANGUAGES FOR EACH TASK\nWe provide the number of training, validation, and test examples for each task in Table 4 and Table 5.\nA.2\nLANGUAGE-WISE BREAKDOWN OF THE RESULTS\nWe provide the performance for each language in Table 6, Table 7, and Table 8.\nbn\nfi\nja\nru\nte\nM\u03c4\nM\u03b2\nMethod\nAcc / AUC-PR\n\u2014-\nL\nFew-shot ICL\n85.9 / \u2014-\n78.5 / \u2014-\n85.4 / \u2014-\n84.5 / \u2014-\n58.9 / \u2014-\nXS\nL\nFIAT\n92.6 / 81.1\n91.0 / 85.3\n96.3 / 66.5\n94.8 / 84.9\n95.3 / 72.5\n\u2014-\nw/o CoT-Augmented Tuning\n92.5 / 84.7\n91.8 / 85.8\n96.2 / 70.3\n94.6 / 84.1\n95.0 / 76.6\n\u2014-\nw/o Instruction-Augmented Tuning\n91.7 / 74.1\n91.2 / 81.4\n95.9 / 53.5\n93.8 / 77.4\n94.8 / 75.4\n\u2014-\nw/o Parameter-efficient Tuning\n92.6 / 73.9\n92.0 / 76.7\n95.0 / 55.8\n94.2 / 74.1\n94.7 / 68.6\n\u2014-\nw/o Instruction-tuned base model\n89.4 / 65.6\n88.9 / 65.9\n94.3 / 42.1\n90.1 / 58.6\n89.7 / 28.2\nS\nL\nFIAT\n92.3 / 81.3\n92.1 / 84.0\n96.2 / 62.4\n94.6 / 84.9\n94.0 / 93.9\n\u2014-\nw/o CoT-Augmented Tuning\n93.0 / 84.3\n94.4 / 81.2\n95.5 / 58.8\n98.8 / 87.4\n95.3 / 78.4\n\u2014-\nw/o Instruction-Augmented Tuning\n93.1 / 75.6\n92.7 / 82.9\n95.0 / 51.3\n94.6 / 78.1\n95.2 / 70.1\n\u2014-\nw/o Parameter-efficient Tuning\n92.7 / 76.2\n93.2 / 83.6\n96.3 / 59.0\n95.1 / 83.3\n96.5 / 78.8\n\u2014-\nw/o Instruction-tuned base model\n90.9 / 66.3\n88.6 / 67.7\n93.2 / 41.0\n89.7 / 57.5\n90.3 / 40.2\nTable 6: Results on each language for XOR-ATTRIQA.\n13\nas\nbho\nbrx\ngbm\ngom\ngu\nhi\nhne\nkn\nmai\nml\nmni\nmr\nmwr\nor\npa\nps\nsa\nta\nur\nM\u03c4\nM\u03b2\nMethod\nF1\n\u2014-\nL\nFew-shot ICL\n72.5\n61.8\n43.0\n60.3\n72.3\n70.6\n61.5\n70.8\n72.9\n73.3\n72.2\n57.1\n71.5\n69.5\n71.4\n73.7\n70.6\n72.6\n71.5\n69.4\nXS\nL\nFIAT\n75.9\n73.9\n47.2\n72.7\n76.1\n76.1\n79.3\n76.2\n76.6\n75.5\n76.3\n61.1\n75.4\n73.3\n76.0\n75.6\n76.6\n77.4\n75.4\n73.3\n\u2014-\nw/o CoT-Augmented Tuning\n73.2\n73.0\n40.7\n68.8\n71.3\n76.1\n79.0\n72.3\n74.0\n71.4\n76.7\n48.8\n73.3\n72.3\n71.6\n74.6\n72.2\n74.9\n75.0\n74.7\n\u2014-\nw/o Instruction-Augmented Tuning\n73.2\n71.5\n39.1\n67.8\n71.7\n73.7\n78.5\n70.3\n74.0\n71.2\n74.7\n50.1\n73.9\n71.4\n70.9\n72.2\n72.8\n71.8\n74.5\n72.48\n\u2014-\nw/o Parameter-efficient Tuning\n70.7\n69.5\n49.2\n65.7\n70.7\n80.5\n67.4\n69.9\n69.7\n70.9\n51.6\n70.0\n67.8\n66.8\n69.5\n69.7\n68.7\n70.9\n69.8\n67.8\n\u2014-\nw/o Instruction-tuned base model\n65.6\n64.7\n49.3\n60.3\n62.6\n65.7\n76.9\n63.2\n65.2\n63.7\n65.4\n52.8\n64.2\n63.5\n63.8\n65.8\n64.3\n63.7\n65.4\n64.4\nS\nL\nFIAT\n80.2\n77.8\n52.2\n77.2\n78.3\n80.6\n82.2\n79.5\n79.7\n78.8\n79.8\n64.5\n79.4\n77.4\n79.4\n80.7\n80.0\n80.4\n79.8\n78.0\n\u2014-\nw/o CoT-augmented Tuning\n79.1\n78.4\n50.3\n75.6\n78.7\n79.9\n84.6\n77.8\n79.2\n78.3\n79.2\n62.4\n77.8\n77.7\n79.6\n79.2\n78.8\n79.9\n80.1\n78.0\n\u2014-\nw/o Instruction-Augmented Tuning\n78.8\n77.6\n47.7\n75.1\n76.1\n79.1\n82.8\n76.3\n78.4\n78.0\n78.4\n58.0\n78.1\n76.0\n79.3\n78.1\n77.0\n78.2\n78.0\n77.2\n\u2014-\nw/o Parameter-efficient Tuning\n74.3\n71.2\n50.6\n71.7\n72.7\n74.6\n81.8\n72.7\n75.1\n74.1\n74.9\n61.9\n73.9\n72.1\n75.8\n75.5\n73.5\n72.6\n73.6\n73.5\n\u2014-\nw/o Instruction-tuned base model\n68.8\n68.2\n46.1\n66.5\n67.5\n69.0\n79.4\n68.8\n69.4\n68.3\n69.4\n53.5\n68.4\n67.1\n69.2\n68.4\n69.4\n67.3\n70.0\n68.0\nTable 7: Results on each language for XTREME-UP Cross-lingual QA Indic.\nas\nbho\nbrx\ngbm\ngom\ngu\nhi\nhne\nkn\nmai\nml\nmni\nmr\nmwr\nor\npa\nps\nsa\nta\nur\nar\nbn\nfi\nja\nko\nru\nte\nM\u03c4\nM\u03b2\nMethod\nF1\n\u2014-\nL\nFew-shot ICL\n72.5\n61.8\n43.0\n60.3\n72.3\n70.6\n61.5\n70.8\n72.9\n73.3\n72.2\n57.1\n71.5\n69.5\n71.4\n73.7\n70.6\n72.6\n71.5\n69.4\n66.0\n75.2\n65.5\n60.3\n61.2\n66.9\n68.7\nXS\nL\nFIAT\n80.1\n80.4\n52.6\n77.0\n78.9\n80.7\n85.2\n80.5\n80.8\n79.0\n79.6\n65.6\n79.6\n78.7\n79.8\n79.1\n80.1\n79.5\n79.8\n78.3\n83.7\n84.6\n82.8\n83.7\n86.3\n81.6\n82.4\n\u2014-\nw/o CoT-augmented Tuning\n79.8\n76.8\n49.1\n71.9\n76.5\n78.1\n84.2\n77.5\n79.0\n75.4\n79.0\n55.2\n77.8\n75.9\n75.8\n78.7\n78.1\n78.3\n80.5\n78.1\n83.5\n85.0\n82.1\n82.3\n85.9\n80.8\n81.1\n\u2014-\nw/o Instruction-augmented Tuning\n78.8\n77.8\n49.2\n72.8\n77.0\n78.7\n83.9\n76.8\n80.1\n76.1\n80.4\n58.3\n78.7\n76.2\n77.1\n78.6\n76.8\n79.1\n79.4\n79.4\n84.5\n84.6\n81.5\n82.6\n87.0\n81.7\n80.8\n\u2014-\nw/o Parameter-efficient Tuning\n78.3\n75.6\n55.4\n74.7\n75.0\n78.0\n84.9\n76.5\n78.9\n77.3\n78.8\n61.9\n77.8\n77.3\n75.9\n78.4\n76.9\n76.6\n79.8\n77.8\n84.3\n83.5\n81.9\n83.2\n88.1\n82.0\n81.3\n\u2014-\nw/o Instruction-tuned base model\n76.9\n76.4\n56.6\n73.1\n74.2\n76.8\n84.7\n75.4\n77.9\n75.5\n78.1\n62.8\n77.5\n74.3\n74.7\n77.5\n76.5\n75.3\n77.5\n75.8\n82.4\n84.2\n81.2\n82.8\n88.1\n80.4\n80.3\nS\nL\nFIAT\n81.6\n80.5\n51.9\n78.3\n80.2\n82.3\n85.8\n81.2\n82.4\n82.1\n81.5\n67.0\n82.1\n80.2\n81.6\n80.9\n81.5\n82.2\n82.3\n79.5\n82.5\n86.2\n82.0\n83.7\n87.1\n83.3\n86.2\n\u2014-\nw/o CoT-augmented Tuning\n82.8\n80.5\n49.9\n78.0\n80.0\n83.4\n85.9\n80.4\n82.7\n80.5\n83.7\n64.9\n81.5\n80.2\n82.0\n82.0\n83.0\n82.4\n80.0\n84.2\n86.6\n81.9\n82.4\n87.0\n83.9\n84.3\n80.6\n\u2014-\nw/o Instruction-augmented Tuning\n81.3\n80.0\n51.2\n78.3\n78.4\n82.0\n85.7\n80.5\n81.2\n80.3\n81.8\n64.8\n81.0\n79.7\n81.2\n80.5\n80.7\n80.5\n81.6\n79.4\n82.8\n85.7\n83.3\n83.8\n86.4\n84.1\n84.0\n\u2014-\nw/o Parameter-efficient Tuning\n79.5\n77.5\n61.5\n77.3\n78.3\n80.1\n85.3\n79.0\n79.9\n79.0\n80.5\n68.9\n79.0\n78.4\n79.8\n78.8\n78.7\n78.9\n80.5\n78.3\n83.3\n85.1\n84.1\n84.9\n89.2\n85.7\n82.4\n\u2014-\nw/o Instruction-tuned base model\n79.5\n77.4\n55.4\n75.6\n79.1\n79.9\n85.5\n77.5\n80.7\n78.5\n80.3\n63.4\n79.5\n77.8\n78.8\n78.6\n78.7\n78.8\n80.7\n77.7\n81.9\n85.8\n84.0\n85.0\n88.8\n91.9\n82.1\nTable 8: Results on each language for XTREME-UP Cross-lingual QA All.\n14\n"
  }
]