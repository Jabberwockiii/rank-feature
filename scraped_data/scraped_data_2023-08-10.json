[
  {
    "title": "JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models",
    "link": "https://arxiv.org/pdf/2308.04729.pdf",
    "upvote": "28",
    "text": "Preprint.\nJEN-1: TEXT-GUIDED UNIVERSAL MUSIC GENERA-\nTION WITH OMNIDIRECTIONAL DIFFUSION MODELS\nPeike Patrick Li\nBoyu Chen\nYao Yao\nYikai Wang\nAllen Wang\nAlex Wang\nFutureverse, AI Innovation\n{patrick.li, alex.wang}@futureverse.com\nABSTRACT\nMusic generation has attracted growing interest with the advancement of deep\ngenerative models. However, generating music conditioned on textual descrip-\ntions, known as text-to-music, remains challenging due to the complexity of mu-\nsical structures and high sampling rate requirements. Despite the task\u2019s signifi-\ncance, prevailing generative models exhibit limitations in music quality, compu-\ntational efficiency, and generalization. This paper introduces JEN-1, a univer-\nsal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model\nincorporating both autoregressive and non-autoregressive training. Through in-\ncontext learning, JEN-1 performs various generation tasks including text-guided\nmusic generation, music inpainting, and continuation. Evaluations demonstrate\nJEN-1\u2019s superior performance over state-of-the-art methods in text-music align-\nment and music quality while maintaining computational efficiency.\nOur de-\nmos are available at https://www.futureverse.com/research/jen/\ndemos/jen1.\n\u201cMusic is the universal language of mankind.\u201d\n\u2013 Henry Wadsworth Longfellow\n1\nINTRODUCTION\nMusic, as an artistic expression comprising harmony, melody and rhythm, holds great cultural signif-\nicance and appeal to humans. Recent years have witnessed remarkable progress in music generation\nwith the rise of deep generative models (Liu et al., 2023; Kreuk et al., 2022; Agostinelli et al.,\n2023). However, generating high-fidelity and realistic music still poses unique challenges compared\nto other modalities. Firstly, music utilizes the full frequency spectrum, requiring high sampling rates\nlike 44.1KHz stereo to capture the intricacies. This is in contrast to speech which focuses on linguis-\ntic content and uses lower sampling rates (e.g. 16kHz). Secondly, the blend of multiple instruments\nand the arrangement of melodies and harmonies result in highly complex structures. With humans\nbeing sensitive to musical dissonance, music generation allows little room for imperfections. Most\ncritically, controllability over attributes like key, genre and melody is indispensable for creators to\nrealize their artistic vision.\nThe intersection of text and music, known as text-to-music generation, offers valuable capabilities\nto bridge free-form textual descriptions and musical compositions. However, existing text-to-music\nmodels still exhibit notable limitations. As illustrated in Table 1, some models (Liu et al., 2023;\nGhosal et al., 2023) operate on spectrogram representations, incurring fidelity loss from audio con-\nversion. Others employ inefficient autoregressive generation or cascaded models (Agostinelli et al.,\n2023; Copet et al., 2023; Huang et al., 2023a). More restrictively, their training objectives are con-\nfined to single task, lacking the versatility of humans who can freely manipulate music.\nTo overcome these limitations, we introduce JEN-1, a text-to-music model combining efficiency,\nquality and controllability. Firstly, JEN-1 uses a masked autoencoder and diffusion model to di-\nrectly generate high-fidelity 48kHz stereo audio, circumventing spectrogram conversion losses. Sec-\nondly, multi-task training on text-to-music, inpainting and continuation enhances model versatility.\n1\narXiv:2308.04729v1  [cs.SD]  9 Aug 2023\nPreprint.\nTable 1: Comparison between state-of-the-art music generative models.\nFeature\nMusicLM\nMusicGen\nAudioLDM\nNoise2Music\nJEN-1 (Ours)\nData\nhigh sample rate\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\n2-channel stereo\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nwaveform\n\u2713\n\u2713\n\u2717\n\u2713\n\u2713\nModel\nautoregressive\n\u2713\n\u2713\n\u2717\n\u2717\n\u2713\nnon-autoregressive\n\u2717\n\u2717\n\u2713\n\u2713\n\u2713\nnon-cascade model\n\u2717\n\u2713\n\u2713\n\u2717\n\u2713\nTask\nsingle-task training\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nmulti-task training\n\u2717\n\u2717\n\u2717\n\u2717\n\u2713\nThirdly, JEN-1 integrates autoregressive and non-autoregressive diffusion to balance dependency\nmodeling and generation efficiency.\nWe extensively evaluate JEN-1 against state-of-the-art baselines across objective metrics and human\nevaluations. Results demonstrate JEN-1 produces music of perceptually higher quality (85.7/100)\ncompared to the current best methods (83.8/100). Ablations validate the efficacy of each technical\ncomponent. More importantly, human judges confirm JEN-1 generates music highly aligned with\ntext prompts in a melodically and harmonically pleasing fashion.\nIn summary, the key contributions of this work are:\n1. We propose JEN-1 as a solution to the challenging text-to-music generation task. JEN-\n1 employs in-context learning and is trained with multi-task objectives, enabling music\ngeneration, music continuation, and music inpainting within a single model.\n2. JEN-1 utilizes an extremely efficient approach by directly modeling waveforms, avoiding\nthe conversion loss associated with spectrograms. It incorporates a masked autoencoder\nand diffusion model, yielding high-quality music at a 48kHz sampling rate.\n3. Our JEN-1 model integrates both autoregressive diffusion mode and non-autoregressive\nmode to improve sequential dependency and enhance sequence generation concurrently.\nThese results in melodically aligned music that adheres to textual descriptions while main-\ntaining high fidelity.\n4. Our paper presents a significant advancement in the field of text-to-music generation, of-\nfering a powerful, efficient, and controllable framework for generating high-quality music\naligned with textual prompts and melodic structures. We conduct comprehensive evalua-\ntions, both objective and involving human judgment, to thoroughly assess the crucial design\nchoices underlying our method.\n2\nRELATED WORK\nThis section provides an overview of the existing literature in the field of music generation, focusing\non three main areas: Single-task vs. Multi-task Training, Waveform vs. Spectrum-Based methods,\nand Autoregressive vs. Non-Autoregressive Generative Models.\nSingle-task vs. Multi-task. Conditional neural music generation covers a wide range of applica-\ntions. Based on the nature of conditioning signals, these tasks can be categorized into two types.\nOne type uses low-level control signals with tight temporal alignment to the audio output. This\nincludes lyrics-conditioned music generation (Yu et al., 2021) and audio synthesis from MIDI\nsequences (Muhamed et al., 2021). The other type utilizes high-level semantic descriptions like\ntext (Kreuk et al., 2022; Yang et al., 2023) or images (Huang et al., 2023b) as conditioning signals,\nwhere the conditioning provides overall coherence and consistency rather than tight temporal align-\nment. However, in practical applications, such <conditional signal, audio> pairs are often scarce.\nHence, models are commonly trained on unlabeled audio datasets using self-supervised techniques\nlike audio inpainting (Marafioti et al., 2019) and continuation (Borsos et al., 2023) to boost general-\nization. In our work, we explore multi-task training using aligned pairs for text-to-music generation,\n2\nPreprint.\nalong with audio-only data for self-supervised inpainting and continuation tasks. This improves\nnoise robustness and allows music generation based on both high-level semantic descriptions as\nwell as low-level control signals.\nWaveform vs. Spectrum. Considering the computational efficiency, using raw audio waveforms\nas model inputs or generation targets is extremely difficult, owing to the high complexity of wave-\nform signals (G\u02c6arbacea et al., 2019). Feature extraction and discrete representation of waveforms\nbecome critical preprocessing steps in music generation tasks, which can be categorized into two\nmain approaches. One approach first converts the waveform into a mel-spectrogram and then pro-\ncesses it by referencing techniques from computer vision on image processing, using methods like\nvector quantized variational autoencoders (VQ-VAE) (Van Den Oord et al., 2017) or generative\nadversarial networks (GANs) (Creswell et al., 2018). Typical techniques such as Diffwave (Kong\net al., 2020b) and Diffsound (Yang et al., 2023) first feed textual tags or other conditional signals\ninto a spectrogram decoder to generate mel-spectrogram tokens. These tokens are then fed into\na pre-trained audio VQ-VAE to synthesize the mel-spectrogram, which is finally converted into\nthe audio waveform through a vocoder like HiFi-GAN (Kong et al., 2020a). The other approach\nleverages quantization-based audio codecs to tokenize the continuous waveform signals, provid-\ning a more compact, compressed, and discrete representation. For instance, SoundStream (Zeghi-\ndour et al., 2021) and EnCodec (D\u00b4efossez et al., 2022) are universal neural audio codecs capable\nof highly compressing general audio while maintaining high reconstruction quality. For example,\nMusicGen (Copet et al., 2023) puts a transformer-based decoder over the quantized units from an\nEnCodec audio tokenizer (D\u00b4efossez et al., 2022), conditioned on a text or melody representation.\nAudioLM (Borsos et al., 2023) and AudioPaLM (Rubenstein et al., 2023) take text as input and\ndecode it into audio tokens via a decoder-only transformer, then convert these tokens back to raw\naudio using SoundStream (Zeghidour et al., 2021).\nAutoregressive vs. Non-autoregressive. Drawing inspiration from sentence generation in natu-\nral language processing, music can be generated through autoregressive or non-autoregressive ap-\nproaches after audio tokenization. Specifically, methods like PerceiverAR (Hawthorne et al., 2022),\nAudioGen (Kreuk et al., 2022), MusicLM (Agostinelli et al., 2023), and Jukebox (Dhariwal et al.,\n2020) employ transformer-based (Vaswani et al., 2017) decoder-only models to autoregressively\ngenerate audio tokens in the music sequence. Such autoregressive models can produce highly co-\nherent audio as each token generation is conditioned on the previous context. However, the sequen-\ntial token-by-token generation manner inherently sacrifices speed for both generation and inference,\nrestricting the applicability of such techniques in downstream tasks. In contrast, non-autoregressive\nmodels can generate multiple tokens concurrently, greatly expediting the generation process. Ben-\nefiting from significant speed advantages during inference, non-autoregressive music generation\nmodels have increasingly become cutting-edge and a prominent research direction in the field. Re-\ncently, non-autoregressive generation based on diffusion models (Ho et al., 2020) has emerged as a\npromising frontier. Diffusion models progressively denoise random noise to obtain latent represen-\ntations that synthesize high-fidelity audio. Innovations like Make-An-Audio (Huang et al., 2023b),\nNoise2Music (Huang et al., 2023a), AudioLDM (Liu et al., 2023), and TANGO (Ghosal et al.,\n2023) harness latent diffusion models (LDM) (Rombach et al., 2022) to achieve accelerated music\ngeneration while maintaining sample quality.\n3\nPRELIMINARY\n3.1\nCONDITIONAL GENERATIVE MODELS\nIn the field of content synthesis, the implementation of conditional generative models often in-\nvolves applying either autoregressive (AR) (Agostinelli et al., 2023; Copet et al., 2023) or non-\nautoregressive (NAR) (Liu et al., 2023; Ghosal et al., 2023) paradigms. The inherent structure of\nlanguage, where each word functions as a distinct token and sentences are sequentially constructed\nfrom these tokens, makes the AR paradigm a more natural choice for language modeling. Thus,\nin the domain of Natural Language Processing (NLP), transformer-based models, e.g., GPT series,\nhave emerged as the prevailing approach for text generation tasks. AR methods (Agostinelli et al.,\n2023; Copet et al., 2023) rely on predicting future tokens based on visible history tokens. The\n3\nPreprint.\nPop dance track\nwith catchy melodies\nFull-Maksed\nMusic\nText Prompt\nJEN-1\nBidirectional Mode &\nUnidirectional mode\nGenerated Music\nRandom Noise\nTask 1\nText-guided Music\nGeneration\nPop dance track\nwith catchy melodies\nText Prompt\nJEN-1\nBidirectional Mode\nRandom Noise\nTask 2\nMusic In-painting\nTask 3\nMusic Continuation\nInpainted Music\nMaksed Music\nPop dance track\nwith catchy melodies\nText Prompt\nJEN-1\nUnidirectional Mode\nRandom Noise\nInpainted Music\nPrefixed Music\nFigure 1: Illustration of the JEN-1 multi-task training strategy, including the text-guided music\ngeneration task, the music inpainting task, and the music continuation task. JEN-1 achieves the\nin-context learning task generalization by concatenating the noise and masked audio in a channel-\nwise manner. JEN-1 integrates both the bidirectional mode to gather comprehensive context and the\nunidirectional mode to capture sequential dependency.\nlikelihood is represented by:\npAR(y | x) =\nN\nY\ni=1\np (yi | y1:i\u22121; x) ,\n(1)\nwhere yi represents the i-th token in sequence y.\nConversely, in the domain of computer vision (CV), where images have no explicit time series\nstructure and images typically occupy continuous space, employing an NAR approach is deemed\nmore suitable. Notably, the NAR approach, such as stable diffusion, has emerged as the dominant\nmethod for addressing image generation tasks. NAR approaches assume conditional independence\namong latent embeddings and generate them uniformly without distinction during prediction. This\nresults in a likelihood expressed as:\npNAR(y | x) =\nN\nY\ni=1\np (yi | x) .\n(2)\nAlthough the parallel generation approach of NAR offers a notable speed advantage, it falls short in\nterms of capturing long-term consistency.\nIn this work, we argue that audio data can be regarded as a hybrid form of data. It exhibits character-\nistics akin to images, as it resides within a continuous space that enables the modeling of high-quality\nmusic. Additionally, audio shares similarities with text in its nature as a time-series data. Conse-\nquently, we propose a novel approach in our JEN-1 design, which entails the amalgamation of both\nthe auto-regressive and non-autoregressive modes into a cohesive omnidirectional diffusion model.\n3.2\nDIFFUSION MODELS FOR AUDIO GENERATION\nDiffusion models (Ho et al., 2020) constitute probabilistic models explicitly developed for the pur-\npose of learning a data distribution p(x). The overall learning of diffusion models involves a forward\ndiffusion process and a gradual denoising process, each consisting of a sequence of T steps that act\nas a Markov Chain. In the forward diffusion process, a fixed linear Gaussian model is employed to\ngradually perturb the initial random variable z0 until it converges to the standard Gaussian distribu-\ntion. This process can be formally articulated as follows,\nq (zt | z0; x) = N\n\u0000zt; \u221a\u00af\u03b1tz0, (1 \u2212 \u00af\u03b1t) I\n\u0001\n,\n\u00af\u03b1t =\ntY\ni=1\n\u03b1i,\n(3)\n4\nPreprint.\nwhere \u03b1i is a coefficient that monotonically decreases with timestep t, and zt is the latent state at\ntimestep t. The reverse process is to initiate from standard Gaussian noise and progressively utilize\nthe denoising transition p\u03b8 (zt\u22121 | zt; x) for generation,\np\u03b8 (zt\u22121 | zt; x) = N (zt\u22121; \u00b5\u03b8 (zt, t; x) , \u03a3\u03b8 (zt, t; x)) ,\n(4)\nwhere the mean \u00b5\u03b8 and variance \u03a3\u03b8 are learned from the model parameterized by \u03b8. We use prede-\nfined variance without trainable parameters following (Rombach et al., 2022; Liu et al., 2023). After\nsimply expanding and re-parameterizing, our training objective of the conditional diffusion model\ncan be denoted as,\nL = Ez0,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8 (zt, t)\u22252\n2\ni\n,\n(5)\nwhere t is uniformly sampled from {1, ..., T}, \u03f5 is the ground truth of the sampled noise, and \u03f5\u03b8(\u00b7)\nis the noise predicted by the diffusion model.\nThe conventional diffusion model is characterized as a non-autoregressive model, which poses chal-\nlenges in effectively capturing sequential dependencies in music flow. To address this limitation,\nwe propose the joint omnidirectional diffusion model JEN-1, an integrated framework that lever-\nages both unidirectional and bidirectional training. These adaptations allow for precise control over\nthe contextual information used to condition predictions, enhancing the model\u2019s ability to capture\nsequential dependencies in music data.\n4\nMETHOD\nIn this research paper, we propose a novel model called JEN-1, which utilizes an omnidirectional\n1D diffusion model. JEN-1 combines bidirectional and unidirectional modes, offering a unified\napproach for universal music generation conditioned on either text or music representations. The\nmodel operates in a noise-robust latent embedding space obtained from a masked audio autoen-\ncoder, enabling high-fidelity reconstruction from latent embeddings with a low frame rate(\u00a7 4.1). In\ncontrast to prior generation models that use discrete tokens or involve multiple serial stages, JEN-\n1 introduces a unique modeling framework capable of generating continuous, high-fidelity music\nusing a single model. JEN-1 effectively utilizes both autoregressive training to improve sequential\ndependency and non-autoregressive training to enhance sequence generation concurrently (\u00a7 4.2).\nBy employing in-context learning and multi-task learning, one of the significant advantages of JEN-\n1 is its support for conditional generation based on either text or melody, enhancing its adaptability\nto various creative scenarios (\u00a7 4.3). This flexibility allows the model to be applied to different music\ngeneration tasks, making it a versatile and powerful tool for music composition and production.\n4.1\nMASKED AUTOENCODER FOR HIGH FIDELITY LATENT REPRESENTATION LEARNING\nHigh Fidelity Neural Audio Latent Representation. To facilitate the training on limited compu-\ntational resources without compromising quality and fidelity, our approach JEN-1 employs a high-\nfidelity audio autoencoder E to compress original audio into latent representations z. Formally,\ngiven a two-channel stereo audio x \u2208 RL\u00d72, the encoder E encodes x into a latent representation\nz = E(x), where z \u2208 RL/h\u00d7c. L is the sequence length of given music, h is the hop size and c is the\ndimension of latent embedding. While the decoder reconstructs the audio \u02dcx = D(z) = D(E(x))\nfrom the latent representation. Our audio compression model is inspired and modified based on pre-\nvious work (Zeghidour et al., 2021; D\u00b4efossez et al., 2022), which consists of an autoencoder trained\nby a combination of a reconstruction loss over both time and frequency domains and a patch-based\nadversarial objective operating at different resolutions. This ensures that the audio reconstructions\nare confined to the original audio manifold by enforcing local realism and avoids muffled effects\nintroduced by relying solely on sample-space losses with L1 or L2 objectives. Unlike prior endeav-\nors (Zeghidour et al., 2021; D\u00b4efossez et al., 2022) that employ a quantization layer to produce the\ndiscrete codes, our model directly extracts the continuous embeddings without any quality-reducing\nloss due to quantization. This utilization of powerful autoencoder representations enables us to\nachieve a nearly optimal balance between complexity reduction and high-frequency detail preserva-\ntion, leading to a significant improvement in music fidelity.\nNoise-robust Masked Autoencoder. To further enhance the robustness of decoder D, we propose a\nmasking strategy, which effectively reduces noises and mitigates artifacts, yielding superior-quality\n5\nPreprint.\nAlgorithm 1 Normalizing Latent Embedding Space\nInput: Existing latent embeddings {zi}N\ni=1 and reduced dimension k\n1: compute \u00b5 and \u03a3 of {zi}N\ni=1\n2: compute U, \u039b, U T = SVD(\u03a3)\n3: compute W = (U\n\u221a\n\u039b\u22121)[:, : k]\n4: ezi = (zi \u2212 \u00b5)W\nOutput: Normalized latent embeddings {ezi}N\ni=1\naudio reconstruction. In our training procedure, we adopt a specific technique wherein p = 5%\nof the intermediate latent embeddings are randomly masked before feeding into the decoder. By\ndoing so, we enable the decoder to acquire proficiency in reconstructing superior-quality data even\nwhen exposed to corrupted inputs. We train the autoencoder on 48kHz stereophonic audios with\nlarge batch size and employ an exponential moving average to aggregate the weights. As a result of\nthese enhancements, the performance of our audio autoencoder surpasses that of the original model\nin all evaluated reconstruction metrics, as shown in Table 2. Consequently, we adopt this audio\nautoencoder for all of our subsequent experiments.\nNormalizing Latent Embedding Space. To avoid arbitrarily scaled latent spaces, (Rombach et al.,\n2022) found it is crucial to achieve better performance by estimating the component-wise variance\nand re-scale the latent z to have a unit standard deviation. In contrast to previous approaches that\nonly estimate the component-wise variance, JEN-1 employs a straightforward yet effective post-\nprocessing technique to address the challenge of anisotropy in latent embeddings as shown in Algo-\nrithm 1. Specially, we channel-wisely perform zero-mean normalization on the latent embedding,\nand then transform the covariance matrix to the identity matrix via Singular Value Decomposition\n(SVD) algorithm. We implement a batch-incremental equivalent algorithm to calculate these trans-\nformation statistics. Additionally, we incorporate a dimension reduction strategy to enhance the\nwhitening process further and improve the overall effectiveness of our approach.\n4.2\nOMNIDIRECTIONAL LATENT DIFFUSION MODELS\ncasual padding\n padding\npadding\nConvolutional Block\nBidirectional\nMode\nUnidirectional\nMode\nTransformer Block\nattend to all\nself-attention mask\nattend to left\nself-attention mask\nFigure 2: Illustration of bidirectional mode and\nunidirectional mode for convolutional block and\ntransformer block. In the unidirectional mode, we\nuse causal padding in the convolutional block and\nmasked self-attention mask to attend only to the\nleft context.\nIn some prior approaches (Liu et al., 2023;\nGhosal et al., 2023), time-frequency conver-\nsion techniques, such as mel-spectrogram, have\nbeen employed for transforming the audio gen-\neration into an image generation problem. Nev-\nertheless, we contend that this conversion from\nraw audio data to mel-spectrogram inevitably\nleads to a significant reduction in quality. To\naddress this concern, JEN-1 directly leverages\na temporal 1D efficient U-Net. This modified\nversion of the Efficient U-Net (Saharia et al.,\n2022) allows us to effectively model the wave-\nform and implement the required blocks in the\ndiffusion model. The U-Net model\u2019s architec-\nture comprises cascading down-sampling and\nup-sampling blocks interconnected via resid-\nual connections. Each down/up-sampling block\nconsists of a down/upsampling layer, followed\nby a set of blocks that involve 1D temporal con-\nvolutional layers, and self/cross-attention lay-\ners. Both the stacked input and output are rep-\nresented as latent sequences of length L, while\nthe diffusion time t is encoded as a single-time\nembedding vector that interacts with the model\nvia the aforementioned combined layers within the down and up-sampling blocks. In the context of\nthe U-Net model, the input consists of the noisy sample denoted as xt, which is stacked with addi-\n6\nPreprint.\ntional conditional information. The resulting output corresponds to the noise prediction \u03f5 during the\ndiffusion process.\nTask Generalization via In-context Learning. To achieve the goal of multi-task training objec-\ntives, we propose a novel omnidirectional latent diffusion model without explicitly changing the U-\nNet architecture. JEN-1 formulates various music generation tasks as text-guided in-context learning\ntasks. The common goal of these in-context learning tasks is to produce diverse and realistic music\nthat is coherent with the context music and has the correct style described by the text. For in-context\nlearning objectives, e.g., music inpainting task, and music continuation task, additional masked mu-\nsic information, which the model is conditioned upon, can be extracted into latent embedding and\nstacked as additional channels in the input. More precisely, apart from the original latent channels,\nthe U-Net block has 129 additional input channels (128 for the encoded masked audio and 1 for the\nmask itself).\nFrom Bidirectional mode to Unidirectional mode. To account for the inherent sequential charac-\nteristic of music, JEN-1 integrates the unidirectional diffusion mode by ensuring that the generation\nof latent on the right depends on the generated ones on the left, a mechanism achieved through\nemploying a unidirectional self-attention mask and a causal padding mode in convolutional blocks.\nIn general, the architecture of the omnidirectional diffusion model enables various input pathways,\nfacilitating the integration of different types of data into the model, resulting in versatile and pow-\nerful capabilities for noise prediction and diffusion modeling. During training, JEN-1 could switch\nbetween a unidirectional mode and a bidirectional model without changing the architecture of the\nmodel. The parameter weight is shared for different learning objectives. As illustrated in Figure 2,\nJEN-1 could switch into the unidirectional (autoregressive) mode, i.e., the output variable depends\nonly on its own previous values. We employ causal padding (Oord et al., 2016) in all 1D convo-\nlutional layers, padding with zeros in the front so that we can also predict the values of early time\nsteps in the frame. In addition, we employ a triangular attention mask following (Vaswani et al.,\n2017), by padding and masking future tokens in the input received by the self-attention blocks.\n4.3\nUNIFIED MUSIC MULTI-TASK TRAINING\nIn contrast to prior methods that solely rely on a single text-guided learning objective, our proposed\nframework, JEN-1, adopts a novel approach by simultaneously incorporating multiple generative\nlearning objectives while sharing common parameters. As depicted in Figure 1, the training pro-\ncess encompasses three distinct music generation tasks: bidirectional text-guided music generation,\nbidirectional music inpainting, and unidirectional music continuation. The utilization of multi-task\ntraining is a notable aspect of our approach, allowing for a cohesive and unified training procedure\nacross all desired music generation tasks. This approach enhances the model\u2019s ability to generalize\nacross tasks, while also improving the handling of music sequential dependencies and the concurrent\ngeneration of sequences.\nText-guided Music Generation Task. In this task, we employ both the bidirectional and unidirec-\ntional modes. The bidirectional model allows all latent embeddings to attend to one another during\nthe denoising process, thereby enabling the encoding of comprehensive contextual information from\nboth preceding and succeeding directions. On the other hand, the unidirectional model restricts all\nlatent embeddings to attend solely to their previous time counterparts, which facilitates the learning\nof temporal dependencies in music data. Moreover, for the purpose of preserving task consistency\nwithin the framework of U-Net stacked inputs, we concatenate a full-size mask alongside all-empty\nmasked audio as the additional condition.\nMusic inpainting Task. In the domain of audio editing, inpainting denotes the process of restor-\ning missing segments within the music. This restorative technique is predominantly employed to\nreconstruct corrupted audio from the past, as well as to eliminate undesired elements like noise and\nwatermarks from musical compositions. In this task, we adopt the bidirectional mode in JEN-1. Dur-\ning the training phase, our approach involves simulating the music inpainting process by randomly\ngenerating audio masks with mask ratios ranging from 20% to 80%. These masks are then utilized\nto obtain the corresponding masked audio, which serves as the conditional in-context learning inputs\nwithin the U-Net model.\nMusic Continuation Task. We demonstrate that the proposed JEN-1 model facilitates both music\ninpainting (interpolation) and music continuation (extrapolation) by employing the novel omnidirec-\n7\nPreprint.\nTable 2: Comparison with state-of-the-art text-to-music generation methods on MusicCaps test set.\nQUANTITATIVE\nQUALITATIVE\nMETHODS\nFAD\u2193\nKL \u2193\nCLAP\u2191\nT2M-QLT \u2191\nT2M-ALI \u2191\nRiffusion\n14.8\n2.06\n0.19\n72.1\n72.2\nMousai\n7.5\n1.59\n0.23\n76.3\n71.9\nMusicLM\n4.0\n-\n-\n81.7\n82.0\nNoise2Music\n2.1\n-\n-\n-\n-\nMusicGen\n3.8\n1.22\n0.31\n83.8\n79.5\nJEN-1 (Ours)\n2.0\n1.29\n0.33\n85.7\n82.8\ntional diffusion model. The conventional diffusion model, due to its non-autoregressive nature, has\ndemonstrated suboptimal performance in previous studies (Borsos et al., 2023; Agostinelli et al.,\n2023). This limitation has impeded its successful application in audio continuation tasks. To ad-\ndress this issue, we adopt the unidirectional mode in our music continuation task, ensuring that the\npredicted latent embeddings exclusively attend to their leftward context within the target segment.\nSimilarly, we simulate the music continuation process through the random generation of exclusive\nright-only masks. These masks are generated with varying ratios spanning from 20% to 80%.\n5\nEXPERIMENT\n5.1\nSETUP\nImplementation Details. For the masked music autoencoder, we used a hop size of 320, resulting\nin 125Hz latent sequences for encoding 48kHz music audio. The dimension of latent embedding\nis 128. We randomly mask 5% of the latent embedding during training to achieve a noise-tolerant\ndecoder. We employ FLAN-T5 (Chung et al., 2022), an instruct-based large language model to\nprovide superior text embedding extraction. For the omnidirectional diffusion model, we set the\nintermediate cross-attention dimension to 1024, resulting in 746M parameters. During the multi-\ntask training, we evenly allocate 1/3 of a batch to each training task. In addition, we applied the\nclassifier-free guidance (Ho & Salimans, 2022) to improve the correspondence between samples\nand text conditions. During training, the cross-attention layer is randomly replaced by self-attention\nwith a probability of 0.2. We train our JEN-1 models on 8 A100 GPUs for 200k steps with the\nAdamW optimizer (Loshchilov & Hutter, 2017), a linear-decayed learning rate starting from 3e\u22125\na total batch size of 512 examples, \u03b21 = 0.9, \u03b22 = 0.95, a decoupled weight decay of 0.1, and\ngradient clipping of 1.0.\nDatasets. We use total 5k hours of high-quality private music data to train JEN-1. All music data\nconsist of full-length music sampled at 48kHz with metadata composed of a rich textual descrip-\ntion and additional tags information, e.g., genre, instrument, mood/theme tags, etc. The proposed\nmethod is evaluated using the MusicCaps (Agostinelli et al., 2023) benchmark, which consists of\n5.5K expert-prepared music samples, each lasting ten seconds, and a genre-balanced subset con-\ntaining 1K samples. To maintain fair comparison, objective metrics are reported on the unbalanced\nset, while qualitative evaluations and ablation studies are conducted on examples randomly sampled\nfrom the genre-balanced set.\nEvaluation Metrics. For the quantitative assessments, we assess the proposed method using both\nobjective and subjective metrics. The objective evaluation includes three metrics: Fr\u00b4echet Audio\nDistance (FAD) (Kilgour et al., 2019), Kullback-Leibler Divergence (KL) (Van Erven & Harremos,\n2014), and CLAP score (CLAP) (Elizalde et al., 2023). FAD indicates the plausibility of the gen-\nerated audio. A lower FAD score implies higher plausibility. To measure the similarity between\nthe original and generated music, KL-divergence is computed over label probabilities using a state-\nof-the-art audio classifier trained on AudioSet (Gemmeke et al., 2017). A low KL score suggests\nthat the generated music shares similar concepts with the reference music. Additionally, we employ\nthe CLAP score to quantify audio-text alignment between the track description and the generated\naudio, utilizing the official pre-trained CLAP model. For the qualitative assessments, we follow\nthe same experimental design (Copet et al., 2023) to qualitatively evaluate the randomly generated\n8\nPreprint.\nTable 3: Ablation studies. From the baseline configuration, we incrementally modify the JEN-1\nconfiguration to investigate the effect of each component.\nQUANTITATIVE\nQUALITATIVE\nCONFIGURATION\nFAD\u2193\nKL \u2193\nCLAP\u2191\nT2M-QLT \u2191\nT2M-ALI \u2191\nbaseline\n3.1\n1.35\n0.31\n80.1\n78.3\n+ auto-regressive mode\n2.5\n1.33\n0.33\n82.9\n79.5\n+ music in-painting task\n2.2\n1.28\n0.32\n83.8\n80.1\n+ music continuation task\n2.0\n1.29\n0.33\n85.7\n82.8\nmusic samples. Human raters were involved in assessing two key aspects of the generated music:\ntext-to-music quality (T2M-QLT) and alignment to the text input (T2M-ALI). Human raters were\nasked to provide perceptual quality ratings for the generated music samples on a scale of 1 to 100\nin the text-to-music quality test. Besides, in the text-to-music alignment test, raters were required to\nevaluate the alignment between the audio and text, also on a scale of 1 to 100.\n5.2\nCOMPARISON WITH STATE-OF-THE-ARTS\nAs shown in Table 2, we compare the performance of JEN-1 with other state-of-the-art meth-\nods, including Riffusion (Forsgren & Martiros, 2022), and Mousai (Schneider et al., 2023), Mu-\nsicLM (Agostinelli et al., 2023), MusicGen (Copet et al., 2023), Noise2Music (Huang et al., 2023a).\nThese competing approaches were all trained on large-scale music datasets and demonstrated state-\nof-the-art music synthesis ability given diverse text prompts. To ensure a fair comparison, we evalu-\nate the performance on the MusicCaps test set from both quantitative and qualitative aspects. Since\nthe implementation is not publicly available, we utilize the MusicLM public API for our tests. And\nfor Noise2Music, we only report the FAD score as mentioned in their original paper. Experimen-\ntal results demonstrate that JEN-1 outperforms other competing baselines concerning both text-\nto-music quality and text-to-music alignment. Specifically, JEN-1 exhibits superior performance in\nterms of FAD and CLAP scores, outperforming the second-highest method Noise2Music and Music-\nGen by a large margin. Regarding the human qualitative assessments, JEN-1 consistently achieves\nthe best T2M-QLT and T2M-ALI scores. It is noteworthy that our JEN-1 is more computationally\nefficient with only 22.6% of MusicGEN (746M vs. 3.3B parameters) and 57.7% of Noise2Music\n(746M vs. 1.3B parameters).\n5.3\nPERFORMANCE ANALYSIS\nThis section presents a comprehensive performance analysis to investigate various aspects of our\nproposed omnidirectional diffusion model JEN-1.\nAblation Studies. To assess the effects of the omnidirectional diffusion model, we compare the\ndifferent configurations, including the effect of model configuration and the effect of different multi-\ntask objectives. All ablations are conducted on 1K genre-balanced samples, randomly selected\nfrom the held-out evaluation set. As illustrated in Table 3, the results demonstrate that i) JEN-\n1 incorporates the auto-regressive mode greatly benefiting the temporal consistency of generated\nmusic, leading to better music quality; ii) our proposed multi-task learning objectives, i.e., text-\nguided music generation, music inpainting, and music-continuation, improve task generalization\nand consistently achieve better performance; iii) all these dedicated designs together lead to high-\nfidelity music generation without introducing any extra training cost.\nGeneration Diversity. Compared to transformer-based generation methods, diffusion models are\nnotable for their generation diversity. To further investigate JEN-1\u2019s generation diversity and credi-\nbility, we provide identical textual prompts, such as descriptions involving general genres or instru-\nments, to generate multiple different samples. As demonstrated on our demo page, JEN-1 showcases\nimpressive diversity in its generation outputs while maintaining a consistently high level of quality.\nGeneration, Generalization, and Controllability. Despite being trained with paired texts and\nmusic samples in a supervised learning manner, our method, JEN-1, demonstrates noteworthy zero-\nshot generation capability and effective controllability. Notwithstanding the challenges associated\n9\nPreprint.\nwith generating high-quality audio from out-of-distribution prompts, JEN-1 still demonstrates its\nproficiency in producing compelling music samples. On our demo page, we present examples of\ncreative zero-shot prompts, showcasing the model\u2019s successful generation of satisfactory quality\nmusic. Furthermore, we present generation examples as evidence of JEN-1\u2019s proficiency in cap-\nturing music-related semantics and exhibiting exceptional controllability incorporated with editing\ntechniques like prompt2prompt (Hertz et al., 2022). Notably, our demo indicates that the generated\nmusic adequately reflects music concepts such as the genre, instrument, mood, speed, etc..\n6\nCONCLUSION\nIn this work, we have proposed JEN-1, a powerful and efficient text-to-music generation framework\nthat outperforms existing methods in both efficiency and quality of generated samples. Through di-\nrectly modeling waveforms instead of mel-spectrograms, combining auto-regressive and non-auto-\nregressive training, and multi-task training objectives, JEN-1 is able to generate high-quality music\nat 48kHz sampling rate. The integration of diffusion models and masked autoencoders further en-\nhances JEN-1\u2019s ability to capture complex sequence dependencies in music. Our extensive quan-\ntitative and human evaluations demonstrate JEN-1\u2019s superiority over strong baselines in subjective\nquality, diversity, and controllability. JEN-1 also excels at music completion and continuation tasks\nunder the multi-task training regime. These results highlight the effectiveness of our techniques in\nmodeling music waveforms and the advantages of the unified framework.\nThis research pushes the frontier of text-to-music generation and provides a compelling solution for\nhigh-quality, semantically controllable music synthesis from text. Potential future directions include\nincorporating external knowledge to enhance controllability, and extending the framework to other\ncross-modal generation tasks. We hope our work will inspire more efforts on developing generative\nmodels that create impactful and realistic art. As text-to-music generation matures from research\ninto practical applications, it bears great potential to augment human creativity and reshape how\npeople compose, share, and appreciate music.\nREFERENCES\nAndrea Agostinelli, Timo I Denk, Zal\u00b4an Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\nZal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shar-\nifi, Dominik Roblek, Olivier Teboul, David Grangier, Marco Tagliasacchi, et al. Audiolm: a\nlanguage modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 2023.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod-\nels. arXiv preprint arXiv:2210.11416, 2022.\nJade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexan-\ndre D\u00b4efossez. Simple and controllable music generation. arXiv preprint arXiv:2306.05284, 2023.\nAntonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A\nBharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35\n(1):53\u201365, 2018.\nAlexandre D\u00b4efossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi.\nHigh fidelity neural audio\ncompression. arXiv preprint arXiv:2210.13438, 2022.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\nBenjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang. Clap learning\naudio concepts from natural language supervision. In ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE, 2023.\n10\nPreprint.\nSeth* Forsgren and Hayk* Martiros. Riffusion - Stable diffusion for real-time music generation,\n2022. URL https://riffusion.com/about.\nCristina G\u02c6arbacea, A\u00a8aron van den Oord, Yazhe Li, Felicia SC Lim, Alejandro Luebs, Oriol Vinyals,\nand Thomas C Walters. Low bit-rate speech coding with vq-vae and a wavenet decoder. In\nICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 735\u2013739. IEEE, 2019.\nJort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for\naudio events. In 2017 IEEE international conference on acoustics, speech and signal processing\n(ICASSP), pp. 776\u2013780. IEEE, 2017.\nDeepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio gen-\neration using instruction-tuned llm and latent diffusion model. arXiv preprint arXiv:2304.13731,\n2023.\nCurtis Hawthorne, Andrew Jaegle, C\u02d8at\u02d8alina Cangea, Sebastian Borgeaud, Charlie Nash, Mateusz\nMalinowski, Sander Dieleman, Oriol Vinyals, Matthew Botvinick, Ian Simon, et al. General-\npurpose, long-context autoregressive modeling with perceiver ar. In International Conference on\nMachine Learning, pp. 8535\u20138558. PMLR, 2022.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nQingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong\nZhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al. Noise2music: Text-conditioned music\ngeneration with diffusion models. arXiv preprint arXiv:2302.03917, 2023a.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin\nLiu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced\ndiffusion models. arXiv preprint arXiv:2301.12661, 2023b.\nKevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr\u00b4echet audio distance:\nA reference-free metric for evaluating music enhancement algorithms. In INTERSPEECH, pp.\n2350\u20132354, 2019.\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hifi-gan: Generative adversarial networks for\nefficient and high fidelity speech synthesis. Advances in Neural Information Processing Systems,\n33:17022\u201317033, 2020a.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020b.\nFelix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00b4efossez, Jade Copet, Devi\nParikh, Yaniv Taigman, and Yossi Adi. Audiogen: Textually guided audio generation. arXiv\npreprint arXiv:2209.15352, 2022.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley.\nAudioldm: Text-to-audio generation with latent diffusion models.\narXiv\npreprint arXiv:2301.12503, 2023.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\n11\nPreprint.\nAndr\u00b4es Marafioti, Nathana\u00a8el Perraudin, Nicki Holighaus, and Piotr Majdak. A context encoder for\naudio inpainting. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 27(12):\n2362\u20132372, 2019.\nAashiq Muhamed, Liang Li, Xingjian Shi, Suri Yaddanapudi, Wayne Chi, Dylan Jackson, Rahul\nSuresh, Zachary C Lipton, and Alex J Smola. Symbolic music generation with transformer-gans.\nIn Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 408\u2013417, 2021.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, 2016.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nPaul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zal\u00b4an Borsos,\nF\u00b4elix de Chaumont Quitry, Peter Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al.\nAudiopalm: A large language model that can speak and listen. arXiv preprint arXiv:2306.12925,\n2023.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nFlavio Schneider, Zhijing Jin, and Bernhard Sch\u00a8olkopf. Mo\\\u02c6 usai: Text-to-music generation with\nlong-context latent diffusion. arXiv preprint arXiv:2301.11757, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nTim Van Erven and Peter Harremos.\nR\u00b4enyi divergence and kullback-leibler divergence.\nIEEE\nTransactions on Information Theory, 60(7):3797\u20133820, 2014.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nDongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.\nDiffsound: Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, 2023.\nYi Yu, Abhishek Srivastava, and Simon Canales.\nConditional lstm-gan for melody generation\nfrom lyrics. ACM Transactions on Multimedia Computing, Communications, and Applications\n(TOMM), 17(1):1\u201320, 2021.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec.\nIEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495\u2013507, 2021.\n12\n"
  },
  {
    "title": "Shepherd: A Critic for Language Model Generation",
    "link": "https://arxiv.org/pdf/2308.04592.pdf",
    "upvote": "25",
    "text": "Shepherd: A Critic for Language Model Generation\nTianlu Wang1\nPing Yu1\nXiaoqing Ellen Tan2\nSean O\u2019Brien 3\nRamakanth Pasunuru\nJane Dwivedi-Yu\nOlga Golovneva\nLuke Zettlemoyer\nMaryam Fazel-Zarandi2,4\nAsli Celikyilmaz2,4\n1joint first author\n2core contributor\n3work done at Meta\n4directional lead\nMeta AI Research, FAIR\nAbstract\nAs large language models improve, there is\nincreasing interest in techniques that leverage\nthese models\u2019 capabilities to refine their own\noutputs. In this work, we introduce Shepherd,\na language model specifically tuned to critique\nmodel responses and suggest refinements, ex-\ntending beyond the capabilities of an untuned\nmodel to identify diverse errors and provide\nsuggestions to remedy them. At the core of\nour approach is a high quality feedback dataset,\nwhich we curate from community feedback and\nhuman annotations. Even though Shepherd\nis small (7B parameters), its critiques are ei-\nther equivalent or preferred to those from estab-\nlished models including ChatGPT. Using GPT-\n4 for evaluation, Shepherd reaches an average\nwin-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd\nstrictly outperforms other models and on aver-\nage closely ties with ChatGPT. 1\n1\nIntroduction\nLarge language models (LLMs) have become in-\ncreasingly sophisticated, exhibiting remarkable pro-\nficiency in producing coherent, contextually and se-\nmantically meaningful text. Despite this progress,\nLLMs still often generate false, unreliable and in-\ncoherent outputs (Zhang et al., 2023). Methods\nthat continuously critique and refine generations\nwould thus be a highly beneficial step towards more\nreliable language models.\nLLMs have been used to generate feedback to\nrefine language model outputs (Yang et al., 2022;\nFu et al., 2023; Peng et al., 2023a). Among exist-\ning work, Fu et al. (2023) use instruction prompt-\ning to generate a multi-aspect evaluation score of\nmodel-generated output text from several domains,\nwhile Peng et al. (2023a) train utility functions to\nprovide natural language feedback on information\n1We released data in this repo: https://github.com/\nfacebookresearch/Shepherd\nWhat are some great financial investments with no risk at \nall?\nInvesting in the stock market, mutual funds, bonds, and \nreal estate are all great financial investments with no \nrisk at all.\nThe response is not accurate and does not address the \nquestion fully. The question asks about investments with \n\u201cno risk at all,\u201d so the response, which mentions \ninvestments that do involve risk, is not useful. To answer \nthe question more fully and accurately, the response \nshould include investments with little to no risk, such as \ncertificates of deposit (CDs), government bonds, or \nmoney market funds.\nWhat are the differences between plant-based and \nanimal-based protein sources?\nThe main difference between plant-based and \nanimal-based protein sources is that plant-based \nproteins are derived from plants, while animal-based \nproteins are derived from animals. Plant-based \nproteins are often lower in fat and cholesterol than \nanimal-based proteins, and they are typically richer in \nvitamins and minerals. Additionally, plant-based \nproteins can be more environmentally sustainable than \nanimal-based proteins.\nThe response doesn't mention the most important \ndifference which is that animal-based protein sources are \ncomplete protein sources, and plant-based protein \nsources are incomplete.\nFigure 1: Overview of Shepherd. Given questions\nand the corresponding answers generated by LLMs,\nShepherd is designed to provide critiques. The example\nquestions are from the Stack Exchange Community and\nresponses are generated by Alpaca model. Shepherd\ncan critique Alpaca generations by either identifying\nerrors or providing constructive feedback.\nseeking dialog tasks. A more recent study by Ye\net al. (2023) instruction-tunes an LLM to generate\nself-feedback on its responses, though the initial\ninvestigations fail to provide feedback on model\noutput generations on complex tasks like math and\nreasoning, only providing generic feedback on the\noutput response.\nIn this work, we present Shepherd2, a language\nmodel explicitly tuned to critique model generated\noutputs. While sharing similar motivation with re-\ncent work, our goal is to build a robust critique\nmodel that can provide feedback across a wider\nrange of domains. When asked to refine the output,\n2We name our model Shepherd, as it guides LLaMAs.\narXiv:2308.04592v1  [cs.CL]  8 Aug 2023\nour model can pinpoint specific issues like factual-\nity, logical errors, coherence, and alignment, while\nalso suggesting improvements. More specifically,\nShepherd can generate natural language feedback\nwhich does not only give overall judgement or gen-\neral suggestions but can also involve deep domain\nknowledge with actionable ideas for refinement\n(see examples in Table 7 and Table 14).\nTo fine-tune and evaluate Shepherd, we created\na high-quality feedback dataset composing of two\ndistinct sets: (1) community feedback, curated\nfrom online forums to capture more diverse interac-\ntions, and (2) human-annotated feedback, collected\non generations across different types of tasks. See\nexamples in Table 1. Trained on a combination of\nthese datasets, Shepherd shows impressive results,\noutperforming ChatGPT (OpenAI, 2022) models\non multiple downstream tasks. Close inspection\nof influences of community feedback and human\nannotated feedback data confirms that the com-\nmunity data is more informative and diverse than\nhuman-annotated data, yet leans towards informal-\nity. These nuances enable Shepherd to give feed-\nback on diverse tasks, while we find that including\nhigh-quality human annotated data for finetuning\nimproves model performance.\nWe conduct model-based evaluation (using GPT-\n4 (OpenAI, 2023)) as well as human evaluation\nof the feedback generated by Shepherd compar-\ning against state-of-the-art baselines including Al-\npaca (Taori et al., 2023), SelFee (Ye et al., 2023),\nand ChatGPT (OpenAI, 2022). We find that cri-\ntiques from\nShepherd are generally more pre-\nferred over those from other models. For exam-\nple, Alpaca tends to give positive feedback to all of\nmodel responses, resulting a lot incorrect feedback.\nSelFee tends to provide vague feedback that cannot\npinpoint errors, ignoring model responses or di-\nrectly answering the question instead of critiquing\nresponses. ChatGPT is more stable across different\nevaluation settings and we find it does a better job\nof giving feedback with a correct judgement.\nIn summary, we develop Shepherd, a new model\ncapable of providing comprehensive critiques of\nany LLM generated text, which can efficiently en-\nhance their quality. Through rigorous evaluation\nof generated critiques, we demonstrate the efficacy\nof Shepherd across a variety of generation tasks.\nA further key contribution of our work is creat-\ning a high-quality feedback dataset, which has the\npotential to facilitate future research in this area.\n2\nData Collection\n2.1\nCommunity Critique Data\nWe gather feedback data from two community ques-\ntion and answering websites: Stack Exchange3 and\nthe Pushshift Reddit Dataset (Baumgartner et al.,\n2020). We consider the title and the sub-title of a\npost as a question, its top-level comments as an-\nswers, and replies to these comments as critiques.\nEverything is associated with a community vote\nscore, calculated by subtracting the total number of\ndownvotes from the total number of upvotes. For\nthe sake of clarity, we will refer to the community\nvote score as question score, answer score, and\ncritique score respectively .\nStack Exchange Data.\nStack Exchange contains\na network of 173 dedicated Q&A communities,\ninclusive of the notable Stack Overflow community,\ndispensing expert insights in a question and answer\nformat. Our data curation temporarily only focuses\non English content.\nPushshift Reddit Data.\nReddit is one of the most\npopular websites in the world, facilitating the shar-\ning, discussion, and community voting of content\nwithin user-generated communities, or \u201csubred-\ndits\u201d. It hosts nearly 140,000 active subreddits\nat any given moment. However, there are certain\nchallenges associated with fine-tuning a model on\nPushshift Reddit data: 1) a substantial proportion\nof subreddits function primarily for entertainment\nrather than serious, informative dialogues. This in-\ncludes subreddits such as r/funny, r/MadeMeSmile,\nr/facepalm, and many others; 2) many posts are\nprimarily intended to share information (e.g., news,\njokes) instead of posing specific questions, making\nthem unsuitable for fine-tuning.\nIn light of these limitations, we have selected\ndata from 16 subreddits4 from Pushshift Reddit\nDataset.\nThese communities generally contain\nmore formal discussions, often structured in a Q&A\nformat.\n2.1.1\nCritique Postprocessing\nWe aim to gather instances in the format of a clean\nquestion-answer-critique triad. Upon cleaning the\n3https://stackexchange.com/\n4Subreddits\nused\nto\ngather\ncommunity\ndata:\nr/AskAcademia,\nr/AskAnthropology,\nr/AskBaking,\nr/askcarguys, r/AskCulinary, r/AskDocs, r/AskEngineers,\nr/AskHistorians, r/AskHR, r/askphilosophy, r/AskPhysics,\nr/askscience,\nr/AskScienceFiction,\nr/AskSocialScience,\nr/AskVet, r/explainlikeimfive\nAlpaca\nSelfee\nChatGPT\n0%\n25%\n50%\n75%\nShepherd Wins\nTie\n81.0%\n12.2% 6.8%\n52.3%\n4.3%\n43.4%\n52.6%\n7.7%\n39.7%\nShepherd Loses\nFigure 2: Preference evaluation using GPT-4 as\nthe evaluator, comparing Shepherd with 3 differ-\nent competing models across 7 different datasets.\nAlpaca\nSelfee\nChatGPT\n0%\n25%\n50%\n75%\nShepherd Wins\nTie\n65.1%\n14.8%\n20.1%\n49.7%\n19.9%\n30.4%\n38.9%\n21%\n39.8%\nShepherd Loses\nFigure 3: Human preference evaluation, comparing\nShepherd with 3 different competing models across\n7 different datasets.\n### Question: Support for \"border-radius\" in IE. Does anyone know if/when Internet Explorer will support the\n\"border-radius\" CSS attribute?\n### Answer: It is not planned for IE8. See the CSS Compatibility page. Beyond that no plans have been released.\nRumors exist that IE8 will be the last version for Windows XP\n### Feedback: You are obviously wrong, because IE9 is supposed to support CSS3 too, and I dont see IE dying\nanywhere. Someone pls kill IE. [Source: Stack Overflow]\n### Question: D.I.Y clay with cornstarch and baking soda.\n### Answer: Add one cup of cornstarch to 1.5 cups of baking power. Add one cup of water to make a slurry and\nlook till it is the right consistency. Cool and use to mold or sculpt.\n### Feedback: The answer mentions a wrong ingredient. The ingredients must contain baking soda but the\nanswer instead mentions baking power. The answer probably meant baking powder but the needed ingredient is\nbaking soda. [Source: Human annotation]\nTable 1: Examples of our training data collected from Stack Exchange and Human Annotation.\ncommunity data, we observed that while procuring\nhigh-quality Q&A pairs was relatively straightfor-\nward, a significant portion of the data did not in-\nclude suitable critiques. We consider a critique to\nbe valid in two scenarios.\n- Case #1: The answer is largely accurate, and\nthe critique offers recommendations for further re-\nfinement or enhancement.\n- Case #2: The answer contains inaccuracies,\nwhich the critique explicitly highlights.\nWe employ a number of techniques to curate\nvalid critiques from community data. First, we filter\nout invalid critique data, such as joke sharing and\nfollow-up questions that fail to provide feedback.\nTo do this, we use two methods. (1) Keyword\nfiltering: We keep examples that contain certain\nkeywords matching the above two cases5. (2) User\nedit history: In additional to keyword filtering, we\ncollect critiques if users edit their answer after post-\n5For Case #1, keywords we use for filtering are: \u201cnot\nwrong\u201d, \u201cagree\u201d, \u201cabsolutely\u201d, \u201cindeed\u201d, \u201cagreed\u201d, \u201cexactly\nwhat I think\u201d, \u201cthat\u2019s right\u201d, \u201cnot what I think\u201d, \u201cyou\u2019re right\u201d,\n\u201cyou are right\u201d, \u201cthat is right\u201d; For Case #2, keywords we use\nare: \u201cwrong\u201d, \u201cincorrect\u201d, \u201cnot agree\u201d, \u201cnot right\u201d, \u201cdisagree\u201d,\n\u201ccan\u2019t agree\u201d, \u201cbeg to differ\u201d, \u201cthat\u2019s not my view\u201d.\ning the critique. This helps us identify examples\nwhere the critique leads to a modification of the\noriginal answer.\nNext, we note that even critiques that follow the\nappropriate format may be inaccurate. For instance,\na critique might wrongly challenge a true fact from\nan answer. To further refine our data and ensure\nhigh quality of critiques, we incorporate additional\nfilters linked with community vote scores. In Case\n#1, where the answer is primarily correct, we omit\ndata where the answer score is lower than 10 and\nthe critique score is lower than 2. This filter ensures\nwe only select instances where the initial answer\nis largely approved by the community, and the cri-\ntique has received some level of endorsement. In\nCase #2, where the answer includes inaccuracies,\nwe focus on data where the critique score surpasses\nthe answer score, and the critique score itself is\nhigher than 2. This selection process ensures we\nare considering instances where the critique, indi-\ncating an error in the initial answer, has garnered\nmore community approval than the answer itself.\nLastly, we impose several additional filters to fur-\nther refine our dataset: 1) To maintain diversity, we\nretain only one instance per post, choosing the one\nwith the highest critique score; 2) To manage offen-\nsive language in community data, we incorporate\na profanity check6 and eliminate any comments or\nfeedback with a profanity score lower than 0.8; 3)\nGiven that our model is a text-only model, we filter\nout instances that contain URLs, images, or videos;\n4) We identify and remove comments that pose fur-\nther questions to the original question rather than\nthe original answer to preserve the integrity of the\nQ&A format.\n2.2\nHuman Data Collection\nPublic dataset selection.\nTo collect natural\nlanguage feedback from a diverse set of con-\ntexts and outputs, we select 8 popular language-\nunderstanding and entailment datasets that require\ncomplex reasoning and have step-by-step expla-\nnations to arrive at the final answer, as well as\ntwo summarization datasets.\nSpecifically, we\nchoose Entailment Bank (deductive reasoning)\n(Dalvi et al., 2021), Proofwriter (logical reason-\ning) (Tafjord et al., 2020), GSM8K (arithmetic\nreasoning) (Cobbe et al., 2021), PIQA (physical\nreasoning) (Bisk et al., 2020b), CosmosQA (com-\nmonsense reasoning) (Huang et al., 2019), ECQA\n(commonsense reasoning) (Aggarwal et al., 2021),\ne-SNLI (deductive and commonsense reasoning)\n(Camburu et al., 2018), and Adversarial NLI (ad-\nversarial entailment) (Nie et al., 2019)7. For sum-\nmarization tasks, we choose two datasets that are\nof relatively high quality: GPT-3 summarization\n(Goyal et al., 2022) and DeFacto (Liu et al., 2022).\nOnly data from the training sets is used for human\nannotation 8.\nData construction for human annotation.\nIn\norder to collect high quality natural language feed-\nback, for each question we provide a context, a\ncorrect output, and a candidate output, and ask an-\n6https://pypi.org/project/\nalt-profanity-check/\n7For entailment datasets such as e-SNLI and Adversarial\nNLI, only prompts with a target label of \u201centailment\u201d or \u201ccon-\ntradiction\u201d are used, i.e., prompts labelled with \u201cneutral\u201d are\nremoved. This is because it is difficult for our critique model\nto learn from neutral examples to give useful feedback.\n8To diversify the human data, we also add one or two\nexamples from the training set of each of these datasets: SIQA\n(Sap et al., 2019), BoolQ (Clark et al., 2019), the challenge\nset and the easy set from ARC (Clark et al., 2018), the middle\nschool set and the high school set from RACE (Lai et al.,\n2017), HellaSwag (Zellers et al., 2019), Natural Question\n(Kwiatkowski et al., 2019), WinoGrande (Sakaguchi et al.,\n2021), TriviaQA (Joshi et al., 2017).\nnotators to give feedback on whether there are any\nerrors in the candidate output. The correct output\nis provided to help annotators identify errors more\neasily. Except for GPT-3 summarization, in which\nwe choose the summary best preferred by human\nraters, all datasets contain a gold answer to the ques-\ntion, i.e., the correct output. PIQA, CosmosQA,\nand DeFacto have candidate outputs in the origi-\nnal datasets. For GPT-3 summarization we choose\nthe summary least preferred by human raters as the\ncandidate output. For other datasets, to increase the\npossibility of obtaining candidate outputs that have\nreasonable errors, we prompt LLaMA-65B (Tou-\nvron et al., 2023) or LIMA-30B (Zhou et al., 2023)\nwith zero-shot or few-shot in-context examples to\nobtain step-by-step reasoning sequences. We do\na manual inspection to ensure the quality of the\ngenerated candidate outputs. Examples from each\ndataset and the templates used to prompt LLaMA\nand LIMA in order to obtain the candidate outputs\nare shown in Table 8.\nAnnotation specifics.\nWe selected an indepen-\ndent vendor (RWS Moravia), utilizing expert re-\nviewers instead of crowd-sourcing, due to the cog-\nnitively demanding nature of our annotation task\nwhich necessitates meticulous and nuanced anno-\ntations. Each example is annotated by one expert\nwith human-in-the-loop quality assessment.9 We\ndefine different error types in our taxonomy in\nTable 9. More details about the instruction and\nguidelines of human annotation can be found in\nAppendix A.\nPostprocessing.\nTo ensure high quality data, we\nconduct the following postprocessing. First, we re-\nmove examples flagged with \u201cErrors in the correct\noutput\u201d and \u201cThe context is too complex to work\non\u201d. We also remove feedback on error types of\n\u201cRedundancy\u201d and \u201cConsistency with context\u201d be-\ncause we find that the feedback on these two error\ntypes are not helpful. As our ultimate goal is to col-\nlect feedback in natural language, we concatenate\nthe feedback from different error types of an exam-\nple into a paragraph using natural words such as\n\u201cFirstly,\u201d \u201cSecondly,\u201d and \u201cBesides\u201d if there exists\nfeedback for more than one error type in a single\nexample. We end up with 1,317 high quality exam-\nples in total. Table 10 presents the distribution of\n9Each example is quoted for $8 by RWS Moravia. Annota-\ntors are native English speakers with strong writing skills and\nwith a high school degree and above.\nYour task is to evaluate the feedback on a model-generated answer. You\u2019ll\nreceive a question, an answer, and feedback about this answer. Give a score\nbased on the feedback\u2019s quality.\n7:\nWhen the answer is wrong, the feedback clearly highlights the\nmost important errors and provides very actionable suggestions. When the\nanswer is correct, the feedback confirms the answer is correct and provides\nvery useful suggestions.\n6: When the answer is wrong, the feedback confirms that the answer is wrong\nand points out the most important errors. When the answer is correct, the\nfeedback confirms the answer is correct and provides useful suggestions.\n5: When the answer is wrong, the feedback misses the important errors\nbut clearly confirms that the answer is wrong. When the answer is correct,\nthe feedback confirms the answer is correct and proposes some less useful\nsuggestions.\n4: The feedback has a correct judgement of the answer (e.g., states correct\nanswer is correct or states wrong answer is wrong).\n3: The feedback is vague about whether or not the answer is correct. Or the\nfeedback itself tries to answer the question regardless of the content in the\nanswer.\n2: The feedback has a wrong judgement of the answer (e.g., states correct\nanswer is wrong or states wrong answer is correct).\n1: The feedback is completely random text or simply repeats the answer.\nFirst,\nplease check whether the feedback has correct or incorrect\njudegement (correct judgement means the answer is correct, feedback\nconfirms the correctness. Or if the answer is incorrect, the feedback confirms\nthe incorrectness.)\nGive a score 1-3 for feedback with incorrect judgement and give a\nscore 4-7 for feedback with correct judgement.\nStart your answer with the score.\nFigure 4: Instruction for GPT-4 and human evaluation\nto rate each feedback on a 1-7 likert scale.\ncollected examples and the identified error types\nfrom each dataset used in human annotation.\n3\nThe Shepherd Model\nWe train Shepherd\nwith LLaMA-7B\n(Tou-\nvron et al., 2023) as the base model.\nWe use\nAdamW (Loshchilov and Hutter, 2017) as our opti-\nmizer with \u03b21 = 0.9, \u03b22 = 0.95 and weight decay\nof 0.1. We use a learning rate 1e-5 with 2000\nwarmup steps, and set the batch size to 64 and max-\nimum sequence length to 2048. The training data\nis formatted using the same template where we use\n\u201c### {field name}\u201d to separate different fields as\nshown in Table 1.\nWe keep checkpoints for each 50 steps with a\ntotal of 3, 000 steps. We manually inspect whether\nthe generated feedback can identify errors or come\nup with constructive suggestions on a held-out set\nof 20-examples and select 3 best checkpoints. Af-\nter that, we use the GPT-4 evaluation protocol (as\ndescribed in 4.3) to pick the best checkpoint on\nthe held-out set.\n4\nEvaluating Feedback\nTo test the ability of Shepherd to critique model\ngenerations, we compare it against a series of\nstate-of-the-art language models including Alpaca-\n7B (Taori et al., 2023), SelFee-7B (Ye et al.,\n2023) and ChatGPT (GPT-3.5 Turbo). We con-\nduct both human evaluation and automatic evalua-\ntion by using GPT-4 (OpenAI, 2023) as an evalua-\ntor. Through extensive experiments across multiple\ndownstream tasks, we show our model can effec-\ntively critique model generations and propose sug-\ngestions, outperforming Alpaca and SelFee while\nachieving performance that is competitive with\nChatGPT.\n4.1\nEvaluation Data\nTo get a broad coverage of NLP domains, we care-\nfully select 6 public datasets for evaluation: Alpaca-\nFarm (Dubois et al., 2023), FairEval (Wang et al.,\n2023a), CommonsenseQA (Talmor et al., 2019),\nOBQA (Mihaylov et al., 2018), PIQA (Bisk et al.,\n2020a), TruthfulQA (Lin et al., 2021). These 6\ndatasets cover a wide range of topics and reasoning\nskill-sets, including commonsense reasoning, phys-\nical reasoning, mathematical reasoning, etc. We\nsample 50 instances from the validation or test set\nof each dataset, resulting in a total of 300 instances\nin the final evaluation set. We further sample 20\ninstances from each dataset to be used for our abla-\ntion studies.\nData contamination. The issue of data con-\ntamination has been widely discussed along with\nChatGPT and related models. To prevent data con-\ntamination in our evaluation, we further develop a\nnew test set CritiqueEval, containing 52 Pushshift\nquestions posted from June 2022 to June 202310.\nWe manually select questions from Pushshift Red-\ndit dataset with highest voting score, and for each\nquestion, we collect an answer with the highest\nvote score and another answer with the lowest vote\nscore11. In summary, our evaluation data in total\nconsists of 352 instances from public datasets as\nwell as from Pushshift Reddit dataset.\n4.2\nBaseline Models\nWe compare Shepherd against the following state-\nof-the-art baselines:\n\u2022 ChatGPT, one of the most powerful chatbots\ndeveloped by OpenAI.\n10The current version of ChatGPT contains training data up\nto Sep. 2021. Our training set contains training data up to Dec.\n2021.\n11community vote score from Pushshift Reddit dataset is\ncalculated by the total number of upvotes minus the total\nnumber of downvotes.\nYou are a helpful and precise assistant for checking the quality of the\nfeedback.\nTwo pieces of feedback have been provided for the same response to a\nparticular question. Which one can point out key errors contained in the\nanswer or provide constructive suggestions?\nFeedback 1: ...\nFeedback 2: ...\nPlease choose from the following options.\nA: Feedback 1 is significantly better.\nB: Feedback 2 is significantly better.\nC: Neither is significantly better.\nFigure 5: Instruction for GPT-4 and human evaluation\nto pick a better critique out of two.\n\u2022 Alpaca-7B, a LLaMA-7B model finetuned on\n52K instruction-following data obtained from\nChatGPT. We include this model since details\nabout the language model behind ChatGPT are\nnot revealed.\n\u2022 SelFee, a finetuned LLaMA-7B model but cu-\nrated for self-feedback and self-revision gen-\neration.\nThe training set for this model in-\ncludes 178K training instances from five differ-\nent data sources, obtained by querying ChatGPT\nto get original generations, feedback and revi-\nsions. Note that using exact matching on ques-\ntions, we observe some overlap between our eval-\nuation data and SelFee\u2019s training data, namely\n33 out of 50 instances from AlpacaFarm, 6 out\nof 50 instances from TruthfulQA, 1 out of 50\ninstances from OBQA and 1 out of 50 instances\nfrom PIQA.\n4.3\nGPT4 Evaluation\nRecent work has used GPT-4 (OpenAI, 2023) to as-\nsess the quality of model-generated text (Liu et al.,\n2023; Wang et al., 2023c; Zhou et al., 2023). We\nfollow the same approach to prompt GPT-412 to\nmeasure to what extent our model can critique an-\nswers generated by LLMs.\n4.3.1\nAbsolute Likert Score\nWe ask GPT-4 to grade each feedback on a 1\u20137\nlikert scale mainly based on whether the feedback\ncan point out errors in the answer or confirm the\nanswer is correct when there are no errors. We\nprovide the instruction shown in Figure 4 as the\nsystem message to GPT-4. Note GPT-4 can make\nmistakes when improper instruction is provided.\nWe select the instruction shown in Figure 4 through\nmanual inspection, conducted by testing various in-\nstructions on a series of feedback examples and\n12GPT-4-0613 is used in our experiments.\nQuestion\nWhy the elderly body odor is so strong?\nResponse\nAs our skin matures, its natural antioxidant protection\ndeclines. This decline results in greater oxidation of\nlipid acid. When lipid acid is oxidized, the chemical\ncompound nonenal is produced, giving off the \u201cold\npeople smell\u201d that many of us are familiar with.\nFeedback\nThe response gives a logical explanation to the\nquestion, using simple and understandable language.\nHowever, it could benefit from proper punctuation\nand grammar. For example, it would be more accu-\nrate to say that the chemical compound is nonanal,\nrather than nonenal, and the sentence could be rewrit-\nten as follows:\nAs our skin matures, its natural antioxidant protection\ndeclines. This decline results in greater oxidation of\nlipid acid. When lipid acid is oxidized, the chemical\ncompound nonanal is produced, giving off the \u201cold\npeople smell\u201d that many of us are familiar with.\nRelated\nKnoledge\nfrom\nWikipedia\nOne study suggested that old person smell may be the\nresult of 2-nonenal, an unsaturated aldehyde which is\nassociated with human body odor alterations during\naging; however, there are other hypotheses. Another\nstudy failed to detect 2-nonenal at all, but found sig-\nnificantly increased concentrations of benzothiazole,\ndimethylsulphone, and nonanal on older subjects.\nTable 2: Challenges in human evaluation. There are\ncases where deep domain knowledge is required to eval-\nuate various feedback.\nassessing if the grading by GPT-4 aligns with hu-\nman judgement. We provide a few examples in\nAppendix B.1 to show how GPT-4 performs un-\nder this chosen instruction. Despite this careful\ninstruction-selection process, GPT-4 may still be\nmisaligned with human judgment, as evinced in\nTable 13.\n4.3.2\nPairwise Comparison\nWe also ask GPT-4 to pick the better feedback given\ntwo candidate feedback. To do this, we provide\nthe instruction shown in Figure 5. In this setting,\ninstead of giving detailed instructions, we directly\nask GPT-4 to pick the better feedback according\nto whether it can identify errors when the response\nis wrong or provide useful suggestions when the\nresponse is correct but needs improvement.\n4.4\nHuman Evaluation\nAs language models may be biased evaluators\n(Aiyappa et al., 2023; Wang et al., 2023c), we fur-\nther conduct human evaluation. Similarly to the\nGPT-4 evaluation, we ask each crowd worker to\nrate feedback on a 1\u20137 likert score. For a fairer\ncomparison, we present the question, answer and\nfeedback generated by different models together to\nannotators. Note that GPT-4 rates each feedback\nindependently, whereas crowd workers are required\nto compare the quality of multiple feedback and as-\nFigure 6: Score distribution from human evaluation.\nScores ranging from 1\u20132 means the model fails to give\ncorrect judgements. A score of 7 means the feedback\nnot only gives a correct judgement of the answer but\nalso provides very helpful feedback for improving the\nanswer.\nsign scores accordingly. In this way, we encourage\ncrowd workers to rank multiple feedback for the\nsame question-answer pair.\nQuestions in our evaluation data may require\nspecific domain knowledge. Even though we in-\nvite expert annotators to do human evaluation who\ncarefully search online whenever they need help,\nhuman evaluation is still a challenging task. Table 2\nshows one example that requires deep medical or\nbiology knowledge.\n5\nResults\n5.1\nPairwise Comparison\nWe first analyze whether Shepherd can generate\nbetter feedback compared to other competing mod-\nels. We present pairwise comparison results in\nFigure 2 and Figure 3, using GPT-4 and human\nevaluation, respectively. In both evaluation set-\ntings, Shepherd significantly outperforms Alpaca\nand consistently outperforms SelFee. Note that\nboth Shepherd and SelFee are finetuned LLaMA-\n7B models, however, SelFee was finetuned on a\ndataset of 178K examples (some of which over-\nlapped with our evaluation data) whereas Shepherd\nwas only finetuned on a dataset of 8K. We also\nsee Shepherd has slightly better performance than\nChatGPT according to GPT-4 evaluation, and on\npar performance in human evaluation.\nDetailed comparison results for each task are\npresented in Table 3 and Table 4. The performance\nvaries across tasks but overall aligns across the\ntwo evaluation settings. It is important to note that\nShepherd consistently generates better feedback\non CritiqueEval, a dataset which has never been\nseen by ChatGPT or LLaMA models. This demon-\nstrates both the critique and generalization abilities\nof our Shepherd model. Some qualitative exam-\nples are included in Table 7.\n5.2\nScore Distribution\nWe also look into the distribution of scores using\nhuman evaluation. As shown in Figure 6, most\nfeedbacks from Alpaca have an incorrect judge-\nment of the provided answer (score 2). SelFee\ntends to generate vague feedback or directly an-\nswers the question instead of critiquing (score\n3). Most feedbacks from ChatGPT have a cor-\nrect judgement (score 4). Compared to ChatGPT,\nShepherd provides more helpful suggestions to\nimprove the answer (more feedback with score 7)\nalthough it tends to generate feedback with wrong\njudgements (more feedback with score 2).\n5.3\nLikert Score\nWe include results using likert scores annotated by\nGPT-4 in Table 6 and likert scores annotated by\nhuman annotator in Table 5. Compared to pair-\nwise comparison by GPT-4, we find the likert score\nevaluation by GPT-4 is less convincing with the\nfollowing reasons:\n(i) When GPT-4 is asked to rate each feedback\nindependently, it tends to give a high rating for all\nfeedback. For example, Alpaca obtains an average\nscore of 4.7, meaning most feedback have a correct\njudgment of the response, while human gives an\naverage score of 2.91, meaning a great proportion\nof feedback have the incorrect judgement. Evalua-\ntion results obtained by using GPT-4 conflict with\nthe observations from the human evaluation.\n(ii) The likert score assigned by GPT-4 appears\nto be favoring a specific format. According to hu-\nman evaluation, SelFee frequently provides minor\nand general suggestions, such as adding more ex-\namples to better illustrate the answer, even when\nan ample amount of examples has already been\nincluded in the response. In such cases, GPT-4\nroutinely give higher scores. However, human an-\nnotators are typically not misled by this pattern.\nThis results in a discrepancy where GPT-4 assigns\nhigh scores to the Selfee model, while human eval-\nuators give significantly lower scores.\n(iii) We also see that the performance gap be-\ntween these models is relatively small, making it\ndifficult to distinguish good and bad models.\nWin Rate\nTest Data\nAlpacaFarm\nFairEval\nCosmosQA\nOBQA\nPIQA\nTruthfulQA\nCritiqueEval\nAvg.\nShepherd vs. Alpaca\n78.0\n90.0\n82.0\n87.0\n91.0\n90.0\n92.3\n87.0\nShepherd vs. SelFee\n52.0\n55.0\n58.0\n51.0\n46.0\n57.0\n51.9\n53.0\nShepherd vs. ChatGPT\n55.0\n65.0\n47.0\n47.0\n66.0\n48.0\n61.5\n56.0\nTable 3: Win rate (%) by GPT-4 evaluation. Overall, Shepherd outperforms all competing models including\nChatGPT. On the other hand, models perform differently across tasks, e.g. Shepherd is better than ChatGPT on\nPIQA task but not TruthfulQA.\nWin Rate\nTest Data\nAlpacaFarm\nFairEval\nCosmosQA\nOBQA\nPIQA\nTruthfulQA\nCritiqueEval\nAvg.\nShepherd vs. Alpaca\n67.0\n76.0\n76.0\n69.0\n69.0\n82.0\n86.5\n72.4\nShepherd vs. SelFee\n55.0\n59.0\n62.0\n52.0\n53.0\n68.0\n68.3\n59.7\nShepherd vs. ChatGPT\n38.0\n59.0\n39.0\n49.0\n43.0\n52.0\n66.3\n49.6\nTable 4: Win rate (%) by human evaluation. Shepherd significantly outperforms Alapca and SelFee, and on average\nit ties with ChatGPT.\nOverall, we recommend using GPT-4 to rate in a\npairwise comparison style, which aligns well with\nhuman evaluation.\nLimitation of GPT-4 evaluation.\nWe find that\nGPT-4 evaluation can be unstable and inaccurate\nin two cases. (i) One instance of GPT-4\u2019s eval-\nuation failure is illustrated in Table 13.\nEven\nwhen the correct answer is provided, GPT-4 fails\nto recognize whether feedback has a correct judge-\nment. (ii) We observed that GPT-4 exhibits some\ndegree of knowledge barrier, affecting its ability\nto score accurately. This is demonstrated in Ta-\nble 14, where Shepherd identifies the error in the\nanswer, whereas all other models perceive the an-\nswer as correct. Owing to the inherent knowledge\nbarrier, GPT-4 assigns a low score to Shepherd\nwhile rating the other models highly, although only\nShepherd identifies the error in the answer.\n5.4\nData Matters\nIn this experiment, we explore how much data is\nsufficient to build a good critique model. We show\nresults in Figure 7. We find that when data is of\nhigh quality, more finetuning data leads to a better\ncritique model. Compared to SelFee which uses\nmuch more finetuning data, we demonstrate our\nhigh quality community data and human annotated\ndata are more useful for building critique models.\n6\nRelated Work\nEvaluation of LLMs. Evaluation of natural lan-\nguage generations is an active area of research. Hu-\n+1.3K\n+0.6K\nChatGPT\n+0.1K\nSelFee\nAlpaca\n0\n1\n2\n3\n4\nLikert score\nFigure 7: We analyze different critique models\u2019 perfor-\nmance on CritiqueEval set by GPT-4 evaluation (aver-\naged across multiple instructions). +0.1K/+0.6K/+1.3K\nmeans a Shepherd model finetuned on all community\ndata and 0.1K/0.6K/1.3K human data. With more hu-\nman annotated data included, we are able to build better\ncritique models.\nman evaluation of model generations which is a\nreliable form of evaluation is an expensive, do-\nmain specific, and time-consuming process. Exist-\ning automatic metrics including word overlap or\nembedding-based similarity with reference outputs\nare also not equipped to measure logical inconsis-\ntencies, factuality or alignment with human judge-\nment (Golovneva et al., 2022; Wang et al., 2023a).\nTo address these limitations, recent work investi-\ngated the use of LLMs for reference-free evaluation\nof language model generations (Chiang et al., 2023;\nPeng et al., 2023b; Sun et al., 2023; Zhou et al.,\n2023; Wang et al., 2023b; Liu et al., 2023). Other\nModel\nTest Data\nAlpacaFarm\nFairEval\nCosmosQA\nOBQA\nPIQA\nTruthfulQA\nCritiqueEval\nAvg.\nAlpaca 7B\n2.96\n2.90\n2.61\n2.91\n2.88\n2.75\n2.73\n2.91\nSelFee 7B\n3.54\n3.90\n3.33\n3.69\n3.82\n3.57\n3.38\n3.84\nChatGPT\n4.56\n4.31\n4.28\n4.30\n4.43\n4.27\n4.04\n4.59\nShepherd 7B\n4.38\n4.75\n4.36\n4.58\n4.44\n4.66\n4.65\n4.41\nTable 5: Likert score on 1-7 scale given by Human. Alpaca and SelFee perform significantly worse than ChatGPT\nand Shepherd. Overall ChatGPT and Shepherd have similar performance while ChatGPT is generally better on\nexsiting public datasets and Shepherd is significanly better on CritiqueEval.\nModel\nTest Data\nAlpacaFarm\nFairEval\nCosmosQA\nOBQA\nPIQA\nTruthfulQA\nCritiqueEval\nAvg.\nAlpaca 7B\n4.83\n4.62\n4.28\n4.84\n4.72\n4.24\n5.27\n4.69\nSelFee 7B\n5.28\n5.60\n5.12\n5.54\n5.60\n5.71\n5.60\n5.49\nChatGPT\n6.54\n6.46\n5.98\n6.10\n6.51\n6.16\n6.00\n6.25\nShepherd 7B\n5.38\n5.88\n4.98\n5.08\n5.38\n5.42\n5.44\n5.37\nTable 6: Likert score on 1-7 scale given by GPT-4. Alpaca performs worse than other models but gets a high score\n4.69. This does not align with the aforementioned evaluation results where Alpaca generates mostly wrong feedback.\nOn the other hand, the gap between different models are also relatively smaller. We think it is partially due to the\nissue that GPT-4 is not capable of strictly following instructions to do evaluation in this setting.\nwork (Wang et al., 2023c,a) however points out that\nthe biases in LLMs can lead to unfair evaluations.\nCritique models.\nRecent work on improving\nLLMs performance has explored an iterative natu-\nral language feedback and refinement approach.\nSelf-Improve (Huang et al., 2022) uses high-\nconfidence rationale-augmented answers generated\nby a 540B-parameter LLM for fine-tuning it and\nshows improvements on general reasoning abilities\nof the model. Saunders et al. (2022) finetune a\nLLM to write natural language critiques for both\nmodel generated and human written text and argue\nthat larger models can write more helpful critiques\nand can use these critiques to self-refine. Self-\nRefine (Madaan et al., 2023) uses a single LLM\nfor generation, providing feedback, and refinement.\nCritic (Gou et al., 2023) proposes interacting with\ntools for evaluating certain aspects of text, and\nusing the feedback for refinement. SelFee (Ye\net al., 2023) collects generations, feedback and\nrevised generations from ChatGPT and finetunes\nLLaMA models to build a critique model. Self-\nCorrect (Welleck et al., 2023) decouples the gen-\nerator from the corrector and shows generations\ncan be improved even when the corrector is much\nsmaller than the base generator.\n7\nConclusion\nWe introduce a new model to critique large lan-\nguage model generations.\nThrough extensive\nexperiments across multiple datasets and differ-\nent evaluation setting, we demonstrate our model\ncan effectively critique answers, reaching perfor-\nmance on par with ChatGPT. With LLMs adopted\nin more and more real life applications, we be-\nlieve it is important to develop automatic mecha-\nnisms to inspect model generations. Our critique\nmodel Shepherd can be very helpful to improve\ngeneration quality and reduce hallucinations.\nLimitations\nIn this paper, we evaluate models\u2019 critique ability\nthrough both automatic evaluation and human eval-\nuation. Despite our efforts to experiment with a\nsubstantial number of data examples and employ\nhuman annotators to the best of our financial ca-\npabilities, it is evident that the paper could benefit\nfrom further enhancement. This can be achieved\nthrough the execution of a more meticulous analy-\nsis carried out with increased number of annotators\nand a broader array of diverse tasks.\nQuestion and\nResponse\nQuestion: What constitutes a frog\u2019s diet?\nResponse: A frog\u2019s diet consists of insects and other small invertebrates. Fruits, vegetables, and nectar are also eaten by some\nspecies of frogs.\nGround truth from OBQA: it chomps on insects\nFeedback from\nAlpaca\nThe answer is correct and provides a comprehensive overview of what constitutes a frog\u2019s diet. It also includes important information\nabout some species of frogs that also eat fruits, vegetables, and nectar. Overall, a great job!\nFeedback from\nSelFee\nThe answer is accurate, but it could benefit from more specific details. For example, it could mention that certain species of frogs are\nmore carnivorous than others and may even eat small vertebrates like fish or birds. Additionally, it could mention that some frogs\nhave been known to eat plant material in addition to insects and invertebrates, but this is not a significant part of their diet. Overall, the\nanswer is a solid 8/10, but could be improved with more specific details. Revision is needed.\nFeedback from\nChatGPT\nThe response to the question is accurate and provides a good overview of what constitutes a frog\u2019s diet. It is beneficial to include the\nadditional information about some species of frogs also eating fruits, vegetables, and nectar. The answer is clear and concise, making it\neasily understandable for anyone who may be unfamiliar with the topic. Overall, well done!\nFeedback from\nShepherd\nThe response provided is not accurate and should be revised. A frog\u2019s diet varies depending on the species of frog. For example, some\nspecies of frogs are carnivorous and prey on small insects and arthropods, such as worms and spiders. Other species of frogs are\nherbivorous and primarily eat plants, fruits, and vegetables. Nectar is not typically eaten by frogs, except for a few species of frogs that\nare nectarivorous. A more accurate response would be: A frog\u2019s diet varies depending on the species of frog. For example, some species\nof frogs are carnivorous and prey on small insects and arthropods, such as worms and spiders. Other species of frogs are herbivorous and\nprimarily eat plants, fruits, and vegetables. Nectar is not typically eaten by frogs, except for a few species of frogs that are nectarivorous.\nQuestion and\nResponse\nQuestion: What are some great financial investments with no risk at all?\nResponse: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all.\nFeedback from\nAlpaca\nThis response is well structured and provides a comprehensive overview of the various types of investments that have the potential to be\nprofitable with minimal risk. The response also provides clear explanations and examples to illustrate its points.\nFeedback from\nSelFee\nThe answer is not entirely accurate. While it is true that investing in the stock market, mutual funds, bonds, and real estate can provide good\nreturns, there is always a level of risk involved. It is important to do thorough research and understand the potential risks and benefits of each\ninvestment before making a decision. Additionally, the answer does not provide any specific information or examples to support the claims made.\nFeedback from\nChatGPT\nThis response is incorrect. There is no such thing as a financial investment with no risk at all. Even low-risk investments such as bonds or\nmutual funds have some level of risk associated with them. It is important to thoroughly research and understand the level of risk involved\nwith any investment before making a decision.\nFeedback from\nShepherd\nThe response is not accurate and does not address the question fully. The question asks about investments with \u201cno risk at all,\u201d so the\nresponse, which mentions investments that do involve risk, is not useful. To answer the question more fully and accurately, the response\nshould include investments with little to no risk, such as certificates of deposit (CDs), government bonds, or money market funds.\nTable 7: Qualitative examples of feedback generated by different models.\nAcknowledgements\nWe thank Justine Kao, Carleigh Wood, and RWS\nMoravia vendor for suggestions on human data\ncollection.\nReferences\nShourya Aggarwal, Divyanshu Mandowara, Vishwa-\njeet Agrawal, Dinesh Khandelwal, Parag Singla, and\nDinesh Garg. 2021.\nExplanations for Common-\nsenseQA: New Dataset and Models. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), Online. Association for\nComputational Linguistics.\nRachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-\nYeol Ahn. 2023.\nCan we trust the evaluation on\nchatgpt? arXiv preprint arXiv:2303.12767.\nJason Baumgartner, Savvas Zannettou, Brian Keegan,\nMegan Squire, and Jeremy Blackburn. 2020. The\npushshift reddit dataset. In Proceedings of the inter-\nnational AAAI conference on web and social media,\nvolume 14, pages 830\u2013839.\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng\nGao, and Yejin Choi. 2020a. Piqa: Reasoning about\nphysical commonsense in natural language. In Thirty-\nFourth AAAI Conference on Artificial Intelligence.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020b. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432\u20137439.\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-SNLI: Nat-\nural language inference with natural language expla-\nnations. Advances in Neural Information Processing\nSystems, 31.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. Boolq: Exploring the surprising\ndifficulty of natural yes/no questions. arXiv preprint\narXiv:1905.10044.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. arXiv\npreprint arXiv:1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. arXiv preprint arXiv:2110.14168.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. EMNLP.\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang,\nIshaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Alpaca-\nfarm: A simulation framework for methods that learn\nfrom human feedback.\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei\nLiu. 2023. Gptscore: Evaluate as you desire.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin\nCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\nand Asli Celikyilmaz. 2022. Roscoe: A suite of\nmetrics for scoring step-by-step reasoning.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Nan Duan, and Weizhu Chen. 2023.\nCritic: Large language models can self-correct with\ntool-interactive critiquing.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\nGPT-3. arXiv preprint arXiv:2209.12356.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos qa: Machine reading com-\nprehension with contextual commonsense reasoning.\narXiv preprint arXiv:1909.00277.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. arXiv preprint arXiv:1705.03551.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453\u2013\n466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023. G-eval:\nNlg evaluation using gpt-4 with better human align-\nment.\nYixin Liu, Budhaditya Deb, Milagro Teruel, Aaron Hal-\nfaker, Dragomir Radev, and Ahmed H Awadallah.\n2022. On improving summarization factual consis-\ntency from natural language feedback. arXiv preprint\narXiv:2212.09968.\nIlya Loshchilov and Frank Hutter. 2017.\nFixing\nweight decay regularization in adam.\nCoRR,\nabs/1711.05101.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question answer-\ning. In Conference on Empirical Methods in Natural\nLanguage Processing.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2019. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. arXiv preprint arXiv:1910.14599.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue.\nOpenAI. 2023. Gpt-4 technical report.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, and Jianfeng Gao. 2023a. Check\nyour facts and try again: Improving large language\nmodels with external knowledge and automated feed-\nback.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023b. Instruction tuning with\ngpt-4.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99\u2013106.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLeBras, and Yejin Choi. 2019.\nSocialiqa: Com-\nmonsense reasoning about social interactions. arXiv\npreprint arXiv:1904.09728.\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills,\nLong Ouyang, Jonathan Ward, and Jan Leike. 2022.\nSelf-critiquing models for assisting human evalua-\ntors.\nZhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin\nZhang, Zhenfang Chen, David Cox, Yiming Yang,\nand Chuang Gan. 2023.\nPrinciple-driven self-\nalignment of language models from scratch with min-\nimal human supervision.\nOyvind Tafjord, Bhavana Dalvi Mishra, and Peter\nClark. 2020. Proofwriter: Generating implications,\nproofs, and abductive statements over natural lan-\nguage. arXiv preprint arXiv:2012.13048.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149\u20134158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai\nLin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui.\n2023a. Large language models are not fair evaluators.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang,\nQiang Heng, Cunxiang Wang, Hao Chen, Chaoya\nJiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye,\nShikun Zhang, and Yue Zhang. 2023b. Pandalm:\nReproducible and automated language model assess-\nment. https://github.com/WeOpenML/PandaLM.\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack\nHessel, Tushar Khot, Khyathi Raghavi Chandu,\nDavid Wadden, Kelsey MacMillan, Noah A. Smith,\nIz Beltagy, and Hannaneh Hajishirzi. 2023c. How\nfar can camels go? exploring the state of instruction\ntuning on open resources.\nSean Welleck, Ximing Lu, Peter West, Faeze Brah-\nman, Tianxiao Shen, Daniel Khashabi, and Yejin\nChoi. 2023. Generating sequences by learning to\nself-correct. In The Eleventh International Confer-\nence on Learning Representations.\nKevin Yang, Nanyun Peng, Yuandong Tian, and Dan\nKlein. 2022. Re3: Generating longer stories with\nrecursive reprompting and revision. In Conference on\nEmpirical Methods in Natural Language Processing.\nSeonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong\nKim, Hyeonbin Hwang, and Minjoon Seo. 2023.\nSelfee: Iterative self-revising llm empowered by self-\nfeedback generation. Blog post.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu,\nand Noah A. Smith. 2023. How language model\nhallucinations can snowball.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, et al. 2023. LIMA: Less is more for align-\nment. arXiv preprint arXiv:2305.11206.\nA\nHuman Data Collection Guidelines\nIn order to collect high quality feedback in natural\nlanguages, for each question we provide a context,\na correct output, and a candidate output, and ask\nannotators to give feedback on whether there is any\nerror in the candidate output. The correct output is\nprovided to help annotators identify errors in the\ncandidate output more easily. The overview and\nhigh level instruction for annotation is provided\nin Figure 8. We define different error types in our\ntaxonomy in Table 9 and examples of questions and\nfeedback for each error type in Table 11. To ensure\nthe quality of the collected data, four output-level\nflags are introduced to identify examples are too\ncomplicated, contain inappropriate content, contain\ncandidate outputs that are incoherent, and contain\nground truth outputs that have errors. The data\ncollection interface is provided in Figure 9.\nB\nEvaluation Pipeline\nB.1\nInstruction Derivation\nWe show a few examples in Table 12. With the\ninstruction manually tuned by us, GPT-4 can give\nreasonable scores on these examples. We provide\nexamples where GPT-4 demonstrates the limita-\ntions of model-based evaluation in Table 13 and\nTable 14.\nDataset\nExample prompt in template\nEntailment Bank\nHere is a question:\nMost birds sit on their eggs until they hatch. Which of these is the most important reason why birds sit on their eggs?\nHere is an answer:\nto keep the eggs warm\nProvide rationale for the above question and answer:\nProofwriter\nCharlie is not furry. Charlie is smart. Charlie is not white. Charlie is not young. Dave is not white. Fiona is rough.\nHarry is not furry. All smart things are not cold. If Fiona is rough then Fiona is young. All furry, white things are\nsmart. Young things are furry. Furry, young things are white. If something is young and kind then it is rough.\nHere is a hypothesis: Fiona is furry.\nIs the hypothesis correct? Here are three options: No. Yes. Unknown.\nChoose the correct option and justify your choice:\nGSM8K\nJolene and Phil have four children, each with the same birthday. They gave birth to their first child exactly 15 years\nago. They gave birth to their second child exactly one year after the birth of their first child. They gave birth to their\nthird child on the fourth birthday of their second child. Two years after the birth of their third child, they gave birth\nto their fourth child. How old, in years, is their fourth child?\nPIQA\nHow do I build walls when making a sunroom addition to a house?\nCosmosQA\nThis week has been a heck of a lot better than last week . Actually , it \u2019s been the best week I \u2019 ve had in ages . On\nTuesday I went into town with Debbie , mostly charity shop shoping , bought pretty much a whole outfit . I wore the\ntop / dress thing that I bought then on Wednesday when I went into London with Drew .\nGiven the above context, What is meant by charity shop shopping with Debbie ?\ne-SNLI\nHere is a premise:\nTwo dogs, one carrying a large dish in its mouth.\nHere is a hypothesis:\nA dog carrying a large dish and another dog.\nDoes this premise imply the hypothesis? Please justify your answer:\nAdversarial NLI\nHere is a premise:\nRafael Dumas (born 13 March 1995) is a Brazilian professional footballer who plays as a defender for Paysandu, on\nloan from Brazilian S\u00e9rie A club Flamengo. He previously had a six-month loan spell at FC Goa in the Indian Super\nLeague in 2016 and at Global.\nHere is a hypothesis:\nDumas lived in Brazil for six months before moving to India.\nDoes this premise imply the hypothesis? Please justify your answer:\nECQA\nJohn put the book in his pack and then went off to work. Where might he be working?\nHere are the options:\nOption 1: public library\nOption 2: backpack\nOption 3: briefcase\nOption 4: suitcase\nOption 5: synagogue\nPlease choose the correct option and justify your choice:\nGPT-3 summarization\nGive a summary of the below article:\nA man is being sought by Kent Police after a house fire in which a person died. Kent Fire and Rescue Service\nwere called to the address in Shorncliffe Road, Folkestone, just before 10:30 GMT on Thursday. The victim was\nfound inside the property after firefighters put out the blaze. Police said they believed Liam Petts, 24, may have\ninformation about the fire. Kent Police said the cause of the fire is not yet known, but is being treated as suspicious.\nDeFacto\nGive a summary of the below article:\nUp to eight people are believed to have carried out the ram-raid at Martins newsagent in Prentice Place, Harlow,\nat about 01:45 BST. The ATM was removed from an outside wall, causing \u201csubstantial damage\u201d, and was then\nloaded on to a van. Essex Police are investigating whether the raid is linked to another at a Roydon newsagent earlier\nthis month. The cash machine contained a \"\"five-figure sum\"\", police said. It is thought the thieves left in the van\nand a red car, believed to be an Audi. The digger, which was damaged in the raid, was left at the scene. \u201cWe are\nespecially keen to hear from anyone who saw the JCB, which is not the usual sort of vehicle to be seen on public\nroads late at night,\u201d Det Insp Jim Adams said. \u201cThere was a previous incident involving the theft of a cash machine\nat a newsagent in Roydon on 16 September in which the machine and its contents were later recovered, and we are\nlooking into whether these two incidents may be linked.\u201d\nTable 8: Example prompts from datasets that are used in the human annotation. Templates of prompting LLMs to\nobtain the candidate outputs are in red. Note that for PIQA, CosmosQA, GPT-3 summarization, and DeFacto there\nis no need to prompt LLMs to obtain candidate outputs because those datasets already contain both correct and\nwrong answers. The templates are included as ease of contexts for human annotators.\nError Type\nDescription\nArithmetic\nError in math calculations.\nCoherence and deduction\nSentences that do not logically follow each other, a summary that lacks\na clear topic or conclusion, no structure, steps contradict, etc. This also\nincludes Missing Step that a step in a reasoning/explanation or thought pro-\ncess is missing (typically observed in math or logical reasoning problems).\nConsistency with context\nInformation about an object (i.e., quantity, characteristics) or a personal\nnamed entity does not match information provided in the context/question.\nVeracity\nInformation is not provided in the context and is irrelevant or wrong. For\nour annotation task rather than needing to look up, please just refer to the\ncorrect output which we assume to be the gold answer.\nRedundancy\nExplanation contains redundant information, which even though may be\nfactual, is not required to answer the question and/or repeated in the output.\nCommonsense\nThe output lacks relations that should be known from the general world.\nShould be instinctive, without questioning it, based on belief, and accepted\nby the society, e.g. all ducks are birds.\nNo error\nThe output is correct.\nTable 9: Error types for human data collection.\nDataset\n# of prompts collected\nError types distribution\nEntailment Bank\n11\nCoherence and Deduction (55%)\nCommonsense (9%)\nVeracity (36%)\nProofwriter\n162\nCoherence and Deduction (88%)\nCommonsense (1%)\nVeracity (11%)\nGSM8K\n431\nArithmetic (33%)\nCoherence and Deduction (63%)\nCommonsense (3%)\nVeracity (1%)\nPIQA\n246\nCoherence and Deduction (44%)\nCommonsense (41%)\nVeracity (15%)\nCosmosQA\n143\nCoherence and Deduction (64%)\nCommonsense (21%)\nVeracity (15%)\ne-SNLI\n65\nCoherence and Deduction (57%)\nCommonsense (33%)\nVeracity (10%)\nAdversarial NLI\n68\nArithmetic (1%)\nCoherence and Deduction (61%)\nCommonsense (24%)\nVeracity (14%)\nECQA\n118\nCoherence and Deduction (73%)\nCommonsense (17%)\nVeracity (10%)\nGPT-3 summarization\n26\nCoherence and Deduction (69%)\nVeracity (31%)\nDeFacto\n29\nCoherence and Deduction (52%)\nVeracity (48%)\nTable 10: Distribution of collected prompts and the identified error types from each dataset used in human annotation.\nError Type\nDescription\nExample good critique\nExample bad critique\nArithmetic\nContext: Natalia sold clips to 48 of her friends in April, and then she sold half as\nmany clips in May. How many clips did Natalia sell altogether in April and May?\nOutput (correct): Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = 72\nclips altogether in April and May. Answer: 72\nOutput (candidate): Natalia sold 48/2 = 22 clips in May. Natalia sold 48+22 = 70\nclips altogether in April and May. Answer: 70\nOperation in the first step is incorrect.\n48/2 equals 24. Hence the final answer\nshould be 48+24=72.\n48/2=24\nThis critique is bad because there\nis only correction but not justifica-\ntion.\nCoherence\nand\ndeduc-\ntion\nExample 1 (lack of logic)\nContext: Here is a dialogue:\nW: I am considering dropping my dancing class. I am not making any progress.\nM: If I were you, I would stick with it. It\u2019s definitely worth time and effort.\nWhat does the man suggest the woman do?\nOutput (correct): Continue her dancing class.\nOutput (candidate): Take a more interesting class.\nExample 2 (missing steps)\nContext:\nAnne is furry. Anne is green. Anne is quiet. Charlie is green. Charlie is quiet. Fiona\nis furry. Harry is white. If someone is nice then they are white. All nice, rough\npeople are round. If Fiona is quiet then Fiona is green. If someone is quiet and\nround then they are nice. All rough people are quiet. If someone is furry then they\nare rough.\nHere is a hypothesis: Anne is rough.\nIs the hypothesis correct?\nOutput (correct): The answer is Yes because Anne is furry. If someone is furry\nthen they are rough. Anne is rough.\nOutput (candidate): The answer is Yes because Anne is rough.\nExample 1\nThe candidate output does not follow the\ncontext logically. The woman mentions\nthe dancing class and the man suggests\nto the woman to stick with it, so it is not\nimplied that she should quit dancing and\nstart a different class.\nExample 2\nMissing steps when determining whether\nAnne is rough. Anne is rough according\nto \u201cAnne is furry.\u201d and \u201cIf someone is\nfurry then they are rough.\u201d\nExample 1\nContinue her dancing class.\nThis critique is bad because it just\ncopies the correct output.\nExample 2\nMissing steps.\nThis critique is bad because it just\npoints out the error type but doesn\u2019t\nmention where the error is.\nConsistency\nwith context\nContext: Locsin and Anakin went fishing. Locsin caught 5 fish and Anakin caught\n10 fish. How many fish did Anakin catch?\nOutput (correct): Anakin caught 10 fish.\nOutput (candidate): Anakin caught 5 fish.\nThe candidate output contradicts with the\ninformation mentioned in the context that\nLocsin caught 5 fish and Anakin caught\n10 fish. So Anakin caught 10 fish.\nAnakin caught 10 fish.\nThis critique is bad because there is\nonly correction but no justification.\nVeracity\nContext: The basketball team went to the steakhouse to eat dinner. The first player\nate a 6-ounce steak. The second player ate beef tips, containing 8 beef tips, each\nan ounce in size. The third player ate a one-pound steak. And the fourth and\nfifth players ordered vegetarian meals. In total, how many ounces of meat were\nconsumed by the team?\nOutput (correct): The first player ate 6 ounces of steak. The second player ate 8\nounces of beef tips, with each tip being an ounce, for a total of 8 x 1 = 8 ounces.\nThe third player ate a 16-ounce steak, which is equivalent to 16 x 1 = 16 ounces.\nTherefore, the total amount of meat consumed by the team is: 6 + 8 + 16 = 30\nounces. The fourth and fifth players ordered vegetarian meals, so they did not\nconsume any meat.\nOutput (candidate): The fourth and fifth players ordered vegetarian meals, for a\ntotal of 2 ounces of meat.\nThe candidate output does not answer the\nquestion of \u201chow many ounces of meat\nwere consumed by the team\u201d, but instead\nmakes up facts that the fourth and fifth\nplayers ordered 2 ounces of vegetarian\nmeals, which is irrelevant and also not true\nbecause the amount of vegetarian meals\nis not mentioned in the context. It should\ncalculate the total amount of meat by cal-\nculating the amounts that the first three\nplayers consumed.\nThe fourth and fifth players did not\norder 2 ounces of meat.\nThis critique is bad because it does\nnot give a correction to the error\nmade in the candidate output and\nalso it does not specify the error\ntypes.\nRedundancy\nExample 1 (redundant)\nContext: Bob is cold. If something is quiet then it is blue. All blue things are round.\nIf something is round and smart then it is cold.\nQuestion: is Bob cold?\nOutput (correct): The answer is true because Bob is cold, as mentioned in the\ncontext.\nOutput (candidate): If something is round and not smart then it is blue. All blue\nthings are round. Answer: true.\nExample 2 (repetition)\nContext: Parker chews 4 pieces of gum a day. A pack of gum has 15 pieces of\nchewing gum per pack. How many packs of gum will he need to last him 30 days?\nOutput (correct): If Parker chews 4 pieces of gum per day, he will need 120 pieces\nof gum to last him 30 days. To determine the number of packs of gum he needs, we\ndivide the total number of pieces needed by the number of pieces per pack, which is\n15. The calculation would be 120 \u00f7 15 = 8 packs of gum. Therefore, Parker needs\nto buy 8 packs of gum to last him 30 days if he chews 4 pieces of gum per day.\nOutput (candidate): He chews 4 pieces of gum a day and there are 30 days in a\nmonth so he chews 4 \u00d7 30 = 120 pieces of gum a month. Parker chews 120 pieces\nof gum a month. He needs 120 pieces of gum a month and a pack of gum has 15\npieces of gum so he will need 120/15 = 8 packs of gum.\nExample 1\nThe reasoning in the candidate output is\nirrelevant to the question about whether\nBob is cold or not. The candidate output\nshould not mention blue and round things\nbut should answer the question directly,\nwhich was already provided in the con-\ntext.\nExample 2\nThere is repetition in the candidate out-\nput. Parker chewing 120 pieces of gum a\nmonth is mentioned in the first and second\nsentences.\nExample 1\nWhy mention blue and round\nthings when the question asks\nabout whether Bob is cold?\nThis critique is bad because it asks\na question rather than giving a cri-\ntique and correction.\nExample 2\nThe candidate output repeats itself.\nThis critique is bad because it does\nnot point out where the error is.\nCommonsense Context: How do you taste something?\nOutput (correct): Place it in your mouth to taste.\nOutput (candidate): Smell it enough to taste it.\nThe candidate output contradicts with\ncommon sense that to taste something you\nshould place it in your month rather than\nsmell it.\nHow do you taste something if you\nsmell it?\nThis critique is bad because there\nis no justification.\nNo error\nContext: Bob is big. Bob is smart. Dave is cold. Dave is round. Dave is smart.\nDave is young. Erin is cold. Erin is round. Erin is smart. Harry is green. If someone\nis round then they are white. Big people are cold. Big, round people are cold. All\ncold people are round. Smart, white people are green. If Harry is green then Harry\nis cold. All round people are white. All green, smart people are young.\nHere is a hypothesis: Dave is big.\nIs the hypothesis correct?\nOutput (correct): The answer is Unknown because There is not enough evidence\nto provide an answer for this question given the context. So the answer is unknown.\nOutput (candidate): There is not enough context to determine whether Dave is big.\nThe answer is unknown.\nThere is no error in the candidate output.\n(empty)\nThis critique is bad because there\nis no critique. Annotators should\nprovide critique even if there is no\nerror in the candidate output.\nTable 11: Guidelines with examples provided to annotators. Annotators are asked to provide critique on the\ncandidate output. To ensure better quality of data collection, we provide a golden correct output to help annotators\nidentify errors in the candidate output. Both good and bad examples of critiques are provided. The issue of the\nexample bad critiques are highlighted in red.\nGoal\nWe would like to collect natural language feedback for a diverse set of contexts and outputs. Given\na context, we will ask annotators to provide natural language feedback about errors in the output answers.\nGuidelines\nFor each question we provide a context, a correct output, and a candidate output. We\nare asking you to give feedback on whether there are any errors in the candidate output. The correct\noutput is provided to help you identify potential errors in the candidate output more easily. If there\nare no errors in the candidate output, please also provide your feedback that there are no errors.\nRemember your main goal is to provide feedback so that a model can use your feedback to improve its\ngeneration. Please ignore the minor mistakes (e.g. missing punctuations, etc) and focus on the critical ones.\nAnnotation task\n1. Choose the error types and write your feedback based on the chosen error types.\n2. Please flag the following issues if there\u2019s any.\n(a) The context is too complex to work on.\n(b) Inappropriate content.\n(c) Candidate output not understandable at all.\n(d) Errors in the correct output.\nTo give a good critique, please\n1. Select from the provided error types which ones appear in the candidate output.\n2. Describe where the error(s) occur specifically, quote the generated content if necessary. Be detailed\nenough that someone else could easily find it.\n3. Justify your reason. Please be specific and explain why it is wrong.\n4. To the best of your knowledge, describe how to correct the error. Please flag the example if it\u2019s too\ncomplex for you.\n5. When writing the critique, please only focus on the candidate output; please do not copy/paste the\ncorrect output.\n6. To the best of your knowledge, identify all the errors in the candidate output. It is okay to skip minor\nissues but you need to identify critical ones. Critical errors are ones that by fixing them, we can get a\nmostly correct answer.\nTo avoid a bad critique, please\n1. Don\u2019t just copy and paste the correct output.\n2. Don\u2019t just provide corrections without justification.\n3. Don\u2019t be vague, include references to the errors if necessary.\nFigure 8: The overall annotation instruction.\nFigure 9: Data collection interface for human annotated data.\n### Instruction: Edit the following sentence so that it expresses an opinion:\n### Input:Social media can be both positive and negative.\n### Response:\n### Feedback:I think social media can be both a blessing and a curse. It has the power to connect people,\nspread information, and allow us to express ourselves, but it can also be used to bully, spread misinformation, and distort people\u2019s self-images.\nGPT-4 score: 3\n### Question: What are some great financial investments with no risk at all?\n### Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all.\n### Feedback:The response is not entirely accurate. Investing in the stock market, mutual funds, bonds,\nand real estate do have some level of risk. The response should be revised to reflect this.\nGPT-4 score: 7\n### Instruction: Please share your feedback regarding the following question and answer.\n### Question: What are some great financial investments with no risk at all?\n### Response: Investing in the stock market, mutual funds, bonds, and real estate are all great financial investments with no risk at all.\n### Feedback:The response provides a general answer to the question but lacks specific examples or details.\nIt would be helpful to provide more diversified investment options and explain the potential benefits and risks of each.\nGPT-4 score: 2\n### Instruction: Identify the right order of reading for the given set of books.\n### Input: Lolita, Great Gatsby, Catcher in the Rye\n### Response:The right order of reading is Great Gatsby, Catcher in the Rye, Lolita.\n### Feedback:The right order of reading is Great Gatsby, Catcher in the Rye, Lolita.\nGPT-4 score: 1\n### Question: What are the differences between plant-based and animal-based protein sources?\n### Answer: The main difference between plant-based and animal-based protein sources is that plant-based proteins are derived from plants,\nwhile animal-based proteins are derived from animals. Plant-based proteins are often lower in fat and cholesterol than animal-based proteins,\nand they are typically richer in vitamins and minerals. Additionally, plant-based proteins can be more environmentally sustainable than animal-based proteins.\n### Feedback: The response doesn\u2019t mention the most important difference which is that animal-based protein sources are complete protein sources,\nand plant-based protein sources are incomplete.\nGPT-4 score: 7\n### Instruction: Rewrite the first sentence to make it active voice.\n### Input: The drapes were brown in color.\n### Answer:I chose the browns drapes to add a touch of warmth to the room.\n### Feedback:The proposed answer is clear and concise, but it does not address the prompt to rewrite the sentence in active voice. A more appropriate response would be:\n\u201cI chose brown drapes to add a touch of warmth to the room.\u201d This sentence is in active voice and effectively conveys the same information as the original sentence.\nGPT-4 score: 6\nTable 12: Examples of likert scale scores given by GPT-4.\nQuestion\nA football team played 22 games. They won 8 more\nthan they lost. How many did they win?\nResponse\n22.\nFeedback\nThe answer is correct, but it could benefit from a\nbrief explanation. To find out how many games the\nteam won, we can subtract the number of losses from\nthe total number of games. In this case, since the\nteam won 8 more games than they lost, we can sub-\ntract 8 from the total number of games played (22).\nTherefore, the team won 14 games.\nGround\ntruth\n15.\nGPT4\nScore\n5: The feedback misses the important errors but\nclearly confirms that the answer is wrong.\nTable 13: GPT-4 evaluation failure example. Even though the ground truth is provided, GPT-4 still assigns a high\nscore to the incorrect feedback.\nQuestion\nAre there any papers where two (or more) authors share the same full name?\nResponse\nHere are few examples:\n\u2022 https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=author%3A%22Frank+\nFrank%22&btnG= 420 authors named Frank Frank\n\u2022 https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=author%3A%22John+John%\n22&btnG= 2480 authors named John John\n\u2022 https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=author%3A%22William+\nWilliam%22&btnG= 5960 authors named William William\nSelFee Feedback\nThe provided answer is informative and gives a good starting point for further research. However, it would be helpful to\nprovide more context or a specific example related to the question asked. Additionally, it would be beneficial to include\ninformation on how to handle situations where two or more authors have the same name in a citation or research paper.\nChatGPT Feedback\nThe response provided is informative and relevant to the question, providing multiple examples of papers where authors\nshare the same full name. The links provided make it easy for the reader to access the papers and verify the information.\nOverall, the answer is well-structured and helpful.\nShepherd Feedback\nI\u2019m not sure if the answers to my questions are right. I\u2019m looking for papers where two (or more) authors share the\nsame full name. I\u2019m not looking for papers where two (or more) authors have the same first name, and where they have\nthe same last name.\nTable 14: With the above example, GPT-4 gives SelFee score 6, ChatGPT score 4, Shepherd score 2. However,\nonly Shepherd successfully points out the error in the answer.\n"
  },
  {
    "title": "Accelerating LLM Inference with Staged Speculative Decoding",
    "link": "https://arxiv.org/pdf/2308.04623.pdf",
    "upvote": "19",
    "text": "Accelerating LLM Inference with Staged Speculative Decoding\nBenjamin Spector 1 Chris Re 1\nAbstract\nRecent advances with large language models\n(LLM) illustrate their diverse capabilities. We\npropose a novel algorithm, staged speculative\ndecoding, to accelerate LLM inference in small-\nbatch, on-device scenarios. We address the low\narithmetic intensity of small-batch inference by\nimproving upon previous work in speculative de-\ncoding. First, we restructure the speculative batch\nas a tree, which reduces generation costs and in-\ncreases the expected tokens per batch. Second,\nwe add a second stage of speculative decoding.\nTaken together, we reduce single-batch decoding\nlatency by 3.16x with a 762M parameter GPT-2-L\nmodel while perfectly preserving output quality.\n1. Introduction\nLarge Language Models (LLMs) have witnessed tremen-\ndous growth over the last few years, demonstrating ca-\npabilities that range from high-quality text generation to\ncomplex reasoning, decision-making, and problem-solving\ntasks (Brown et al., 2020; OpenAI, 2023; Chowdhery et al.,\n2022). These strides, enabled by advances in deep learning\narchitectures (Vaswani et al., 2017), training methodologies\n(Kingma & Ba, 2014), and vast amounts of data (Halevy\net al., 2009; Gao et al., 2020; Kocetkov et al., 2022), have\npaved the way for applications in fields as varied as natural\nlanguage processing (Brown et al., 2020), machine transla-\ntion (Raffel et al., 2020), code synthesis (Chen et al., 2021),\nand beyond (OpenAI, 2023).\nHowever, this exciting progress comes with its own set\nof system-level challenges. As LLMs have become more\npowerful, their computational demands have increased in\ntandem, often requiring substantial cloud resources for in-\nference (Sheng et al., 2023). This requirement is prohibitive\nfor many potential applications, especially those requiring\n1Department of Computer Science, Stanford University, Cal-\nifornia, United States. Correspondence to: Benjamin Spector\n<bfs@stanford.edu>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nlow-latency responses (Wang et al., 2023) or those where\ndata privacy is paramount (Carlini et al., 2021).\nOur paper addresses these challenges by accelerating local\n(small-batch) inference for LLMs, which suffers from poor\ncompute utilization due to its low arithmetic intensity. We\nview this problem as crucial for three reasons: latency, per-\nsonalization, and privacy. First, optimizing local inference\nlatency improves real-time interactivity and responsiveness.\nAccelerating local inference also opens the door for more\npersonalized LLM experiences as it allows models to be\nlocally tailored to individual users. Finally, local inference\nenhances data privacy, as it removes the need for data to\nleave the user\u2019s device.\nMore philosophically, we believe that methods to efficiently\nrun LLMs locally promote AI democratization by empower-\ning individuals with limited computational resources.\nIn this work, we build on the speculative decoding tech-\nniques introduced by (Leviathan et al., 2022; Chen et al.,\n2023), which use a fast but inaccurate draft model to an-\nticipate the oracle model and batch queries to it, which\nimproves sequential decoding performance while perfectly\nretaining the model distribution. These techniques scale\nwell at first but their performance gains quickly saturate,\nbecause the probability of a draft model correctly guessing\nmany sequential tokens is exponentially small. We improve\nspeculative methods in two key ways:\n1. We restructure the speculative batch as a tree of pos-\nsible token sequences, so as to more quickly create\nlarger and higher quality speculative batches.\n2. We speculatively decode the draft model, too, to further\nimprove performance.\nWe find these techniques significantly improve the perfor-\nmance of speculative decoding in both deterministic and\nsampling-based decoding.\n2. Background\nIn this section, we provide a brief overview of autoregres-\nsive LLM inference, key principles of GPU performance\noptimization, and prior work in optimizing LLM inference.\n1\narXiv:2308.04623v1  [cs.AI]  8 Aug 2023\nSubmission and Formatting Instructions for ICML 2023\n2.1. Autoregressive LLM Inference\nAutoregressive generation from decoder-only LLMs is gen-\nerally split into two phases. First, the prompt is run through\nthe model to generate the KV cache and the first output log-\nits. This is usually fast, as the entire prompt can be handled\nin parallel.\nThe second phase is decoding. A token is selected from\nthe outputted logits and fed back into the model, which\nproduces logits for the following token. This is repeated\nuntil the desired number of tokens is produced. Because\ndecoding must be done sequentially, with the entire model\u2019s\nweights streamed through the compute units each time in\norder to generate a single token, the arithmetic intensity\n(that is, FLOP of compute / byte of memory bandwidth)\nof this second phase is extremely low when run in small\nbatches. As such, decoding is usually the most expensive\npart of autoregressive generation. (Leviathan et al., 2022)\n2.2. GPU optimization\nModern LLM inference is most often conducted on GPUs\ndue to the highly parallel nature of the workload, which\nconsists principally of large matrix multiplications.\nGPUs consist of thousands of extremely small efficient cores\nsupported by a multi-level memory hierarchy. The key chal-\nlenge of optimizing small-batch LLM inference for GPUs is\nto deal with the extremely low arithmetic intensity. Operat-\ning in 16-bit precision with a batch size of 1, decoding has\nan arithmetic intensity of 1. For example, for a reference Py-\nTorch (Paszke et al., 2019) implementation of GPT-2 Large\n(762M parameters), inference requires approximately 1.4\nGFLOP, and yet a quiesced NVIDIA RTX 4090 achieves\nonly 150 tokens/second, for a compute utilization of a mere\n0.13% (NVIDIA, 2022). This abysmal performance is sub-\nstantially due to the GPU roofline (Ofenbeck et al., 2014),\nwhich is governed by memory bandwidth at low arithmetic\nintensities (visualized in Figure 1).\n2.3. Speculative Decoding\nThere are many techniques under investigation today to\naccelerate inference, such as quantization (Dettmers et al.,\n2022; Frantar et al., 2022), flash attention (Dao et al., 2022),\nand speculative decoding (Leviathan et al., 2022; Chen et al.,\n2023). In this section, we\u2019ll briefly examine speculative\ndecoding as described in (Leviathan et al., 2022; Chen et al.,\n2023), as it is the primary subject of this work.\nThe basic idea of speculative decoding is to use a smaller,\nfaster draft model to decode several tokens in advance, and\nthen feeds them into the oracle model as a single batch. If\nthe draft model was right about its predictions \u2013 the larger\nmodel agrees \u2013 one can decode several tokens with a single\nFigure 1: A roofline plot for single-query GPT-2-L inference on an\nRTX 4090. At small batch sizes, inference is completely memory\nbandwidth bound. Thus this plot shows that the only way to\nsignificantly increase performance is to increase the arithmetic\nintensity of inference.\nbatch, which saves considerable memory bandwidth, and\nthus time, per token. However, if the larger model rejects\nthe tokens predicted by the draft model, then the rest of the\nbatch is discarded and the algorithm naturally reverts to stan-\ndard token-by-token decoding. Speculative decoding may\nalso be accompanied by a rejection sampling scheme to sam-\nple from the original distribution. Note this is only useful\nin small-batch settings where bandwidth is the bottleneck.\nSpeculative decoding trades compute for bandwidth.\nThere are two key reasons why speculative decoding is an\nattractive performance engineering target. First, it does not\ndegrade model quality at all. Second, the gains it provides\nare generally orthogonal to other methods, because its per-\nformance comes from converting sequential execution to\nparallel execution. (Leviathan et al., 2022)\n3. Methods\nWe make two improvements to speculative decoding: tree-\nstructured batches, and additional stages. We term the com-\nbination of these methods \u201cstaged speculative decoding\u201d.\n3.1. Tree-structured batches\nCurrent speculative methods predict a single sequence for\nthe batch. However, this doesn\u2019t scale well to large batch\nsizes or low draft model alignments. Intuitively, the proba-\nbility that two models agree for long consecutive sequences\nof tokens is exponentially low, which means that speculative\ndecoding has rapidly diminishing returns as one scales its\narithmetic intensity.\n2\nSubmission and Formatting Instructions for ICML 2023\nSampling\nBaseline\nSpeculative\nStaged spec.\nmethod\nrel. bandwidth\nrel. bandwidth\nrel. bandwidth\nDeterministic\n1.00\n0.31\n0.23\nTopk\n1.00\n0.48\n0.35\nTable 1: Memory bandwidth consumption (relative to baseline) of\nspeculative and staged speculative decoding methods.\nOur approach is to dynamically build a tree of the possible\nsequences, which provides three benefits: more expected\ntrue tokens per batch, an increased number of leaf nodes,\nand better parallelism for the small draft model.\nFirst, by reallocating computation from the end of very long\nsequences to the beginning, and considering the second or\nthird most likely tokens to be produced by the model, one\nincreases the expected number of tokens per batch compared\nto the naive approach.\nSecond, the cost of running the draft model to produce the\nbatch is non-negligible in standard speculative decoding.\nHowever, in a tree of predictions which constitute the batch\nto the oracle model, the draft model is only run at internal\nnodes of the tree. So, a wider tree increases the number of\nleaf nodes, which means that one gets more of the batch for\nfree.\nA third benefit of the wider tree is that one can parallelize\nexecution for the small model across the tree, which also\ndecreases its cost. In the limit, one only needs to run the\ndraft on a number of batches equal to the depth of the tree.\nThis is important because draft models are usually smaller\ntransformer-based models and are thus memory-bound in\nsmall-batch inference, too.\nImplementing a tree-structured batch requires some care.\nThe simplest approach is to partition self-attention while\ndecoding into cross-attention with the KV cache and self-\nattention within the batch. The tree-structured batch can\nthen be constructed by controlling both the positional em-\nbeddings and causally masking the batch self-attention ma-\ntrix according to the tree. Finally, the new KV cache for the\nwhole batch must be stored separately, and then the appro-\npriate slices appended to the main KV cache after tokens\nare sampled.\n3.2. Staged Speculation\nCurrent speculative methods use a single smaller model as\nthe draft, usually a smaller LLM (Chen et al., 2023). In this\nsetting, the size of the draft model is an important hyperpa-\nrameter: a larger draft model will have better alignment with\nthe oracle, but will cost more, whereas a smaller model will\nproduce lower quality speculative batches, but at a lower\ncost. In practice, draft models that are about 15-20x smaller\nthan the oracle seem optimal.\nSampling\nBaseline\nSpeculative\nStaged spec.\nmethod\ntokens/sec\ntokens/sec\ntokens/sec\nDeterministic\n150\n350\n475\nTopk\n150\n219\n298\nTable 2: Relative performance (in tokens/second decoded) with\nbaseline (non-speculative), standard speculative, and staged specu-\nlative decoding methods.\nHowever, under naive speculative decoding, assembling\nlarge batches inverts the cost structure, with more time spent\non the draft model than the oracle. So, one should accel-\nerate the draft model in generating sequences of tokens,\nand speculative decoding is a natural solution for this, too.\nWe correspondingly add speculative decoding to the draft\nmodel in our approach. Thus the overall method of \u201cstaged\nspeculative decoding\u201d, consists of oracle, draft, and draft2\nmodels with tree-structured batches.\n4. Results\nFor our experiments, we use three models: a GPT-2-Large\n(762M) parameter oracle model (Radford et al., 2019) fine-\ntuned on the Python subsection of the Stack (Kocetkov et al.,\n2022), a small (40M) parameter GPT-2 draft model trained\non the same, and a Katz backoff trigram model (Katz, 1987)\nas the draft2 model. The Katz backoff model was generated\nby running the draft model for two hours at a sampling\ntemperature of 1.5 to generate 120M tokens. All evaluations\nwere conducted on a quiesced RTX 4090 GPU (NVIDIA,\n2022), which is top-end consumer hardware.\nWe evaluate against two alternative inference methods. First,\nour standard baseline is simple token-by-token decoding\nwith the oracle. Second, we also evaluate against speculative\ndecoding as proposed by (Leviathan et al., 2022), so as to\nisolate the effects of our improvements.\nTo evaluate, we ran the 164 prompts from HumanEval (Chen\net al., 2021), using non-speculative, speculative, and our\nstaged speculative methods, and with both deterministic and\ntopk sampling (Radford et al., 2019). Details of batch sizes\nand internal heuristics can be found in our code.\nWe first measured the memory bandwidth requirements of\neach method, to validate that our approach saves appre-\nciable bandwidth. We detail the results in table 1, which\nillustrate that staged speculative decoding uses substantially\nless memory bandwidth than either alternative method.\nSecond, we measure sequential decoding throughput for\neach approach. The results are summarized in table 2 and\ndetailed in Figure 2.\nWith deterministic sampling, our implementation provides\nan average performance boost of 3.16x over our reference\n3\nSubmission and Formatting Instructions for ICML 2023\nFigure 2: Relative performance distribution over different prob-\nlems in the HumanEval dataset. (A) shows greedy decoding,\nwhereas (B) shows Topk decoding. Problem indices are sorted by\nstaged speculative performance for clarity.\nimplementation, and 1.36x over standard speculative sam-\npling. Furthermore, we evaluate on relatively small mod-\nels, whereas prior work uses much larger models on which\none would expect greater benefits. Profiling data shows\nour implementation has 35% overhead from the Python in-\nfrastructure, which could be reduced by a more efficient\nimplementation or amortized over larger models.\nWith topk (k = 50, T = 1) sampling, although both specu-\nlative methods are significantly degraded due to stochastic\nrejection of tokens provided in the batch, staged speculation\nnonetheless retains its lead, providing an average perfor-\nmance boost of 1.98x over baseline and 1.36x again over\nstandard speculative sampling.\nIn Figure 3, we show the origin of different tokens in the\ncompleted model. (The performance gain on the shown\nprompt is approximately 2.5x over baseline.) The model is\nusually able to decode the easiest, most obvious tokens, like\nwhitespace, in batch through both transformer models, as\nthey originate with the N-gram models. Somewhat more\ndifficult tokens are generated by the small model, while\nFigure 3: A visualization of the origin of tokens in an example\nT=1 HumanEval completion. Green background originates with\nthe N-gram draft2 model, blue the draft model, and red the oracle\nmodel. (Of course, all tokens are eventually checked by the oracle\nmodel.) Obvious tokens \u2013 like whitespace \u2013 are preferentially\naccelerated relative to difficult ones.\nthe most critical tokens (like the token following the \u201cif\u201d\ntoken) come from the oracle model. Note that due to the\nfinite batch size, the above is only a trend and should not be\nexpected to apply universally to every token. Some tokens\nwhich could have been accurately predicted by a smaller\nmodel will still end up originating from larger models.\nWe also wish to acknowledge the extreme range of the\nperformance benefits as a downside of the work. While\nperformance benefits run as high as 10x on realistic prompts,\nthey can also be limited to only 2x. To a large degree, this\ndepends on the denseness or sparseness of difficult content.\nFor example, highly indented Python code will make better\nuse of the N-gram models than unindented code, and thus\nreap greater performance benefits.\nWe speculate that these models represent an approximately\nfixed cost per entropy of the data. Extremely low entropy\ngeneration, like pure whitespace, will be generated very\nquickly by staged speculative decoding, with performance\napproaching that of large-batch inference, whereas dense\ngenerations with high entropy will need to rely on small-\nbatch decoding at all stages. So, a corollary implication\nof this work is that most of the text generated by LLMs\nhas entropy lower than the capabilities of their authoring\nmodels, and that the increased accuracy of big models is\nisolated to a relatively small number of key tokens.\nWe see several paths for future work:\n1. We suspect it may be possible to speculatively sample\nwith T > 0 even faster by generating the multinomial\nCDFs first, and then using this sequence to help choose\n4\nSubmission and Formatting Instructions for ICML 2023\nthe tokens to assemble into the full batch. For example,\nif the multinomial CDF sampled is 0.99, it may be\nbest to only include in the batch the draft model\u2019s fifth\nthrough tenth most likely tokens.\n2. Running with larger models would likely yield even\ngreater performance boosts while still fitting on-device.\nWith 8-bit quantization, it should be possible to fit 20B\nmodels on consumer GPUs in small-batch, allowing\nfor an entire additional stage of speculation.\n(20B \u2192 1B \u2192 50M \u2192 N-gram).\n3. Investigating better lowest-level draft models could\nalso improve performance \u2013 models which perform\nbetter than N-gram models but still run in < 10\u00b5s.\n5. Conclusions\nIn this work, we described and implemented several im-\nprovements over previous work in speculative decoding.\nFirst, we restructured the batch provided to the oracle model\nas a tree, in order to decrease the cost of generation and in-\ncrease the expected number of tokens per batch. Second, we\nadded a second stage of speculation to accelerate the decod-\ning of the draft model. Altogether, we achieved an average\nspeedup of 3.16x over standard single-batch inference.\nAcknowledgements\n[Left blank for blind review.]\nReferences\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-\nVoss, A., Lee, K., Roberts, A., Brown, T. B., Song, D.,\nErlingsson, U., et al. Extracting training data from large\nlanguage models. In USENIX Security Symposium, vol-\nume 6, 2021.\nChen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre,\nL., and Jumper, J. Accelerating large language model\ndecoding with speculative sampling.\narXiv preprint\narXiv:2302.01318, 2023.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,\nKaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,\nG., et al. Evaluating large language models trained on\ncode. arXiv preprint arXiv:2107.03374, 2021.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,\nG., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,\nGehrmann, S., et al. Palm: Scaling language modeling\nwith pathways. arXiv preprint arXiv:2204.02311, 2022.\nDao, T., Fu, D., Ermon, S., Rudra, A., and R\u00b4e, C. Flashat-\ntention: Fast and memory-efficient exact attention with\nio-awareness. Advances in Neural Information Process-\ning Systems, 35:16344\u201316359, 2022.\nDettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.\nLlm. int8 (): 8-bit matrix multiplication for transformers\nat scale. arXiv preprint arXiv:2208.07339, 2022.\nFrantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D. Gptq:\nAccurate post-training quantization for generative pre-\ntrained transformers. arXiv preprint arXiv:2210.17323,\n2022.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T.,\nFoster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027,\n2020.\nHalevy, A., Norvig, P., and Pereira, F. The unreasonable\neffectiveness of data. IEEE intelligent systems, 24(2):\n8\u201312, 2009.\nKatz, S. Estimation of probabilities from sparse data for the\nlanguage model component of a speech recognizer. IEEE\ntransactions on acoustics, speech, and signal processing,\n35(3):400\u2013401, 1987.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKocetkov, D., Li, R., Allal, L. B., Li, J., Mou, C., Ferrandis,\nC. M., Jernite, Y., Mitchell, M., Hughes, S., Wolf, T.,\net al. The stack: 3 tb of permissively licensed source\ncode. arXiv preprint arXiv:2211.15533, 2022.\nLangley, P. Crafting papers on machine learning. In Langley,\nP. (ed.), Proceedings of the 17th International Conference\non Machine Learning (ICML 2000), pp. 1207\u20131216, Stan-\nford, CA, 2000. Morgan Kaufmann.\nLeviathan, Y., Kalman, M., and Matias, Y.\nFast infer-\nence from transformers via speculative decoding. arXiv\npreprint arXiv:2211.17192, 2022.\nNVIDIA. Nvidia RTX 4090 GPU architecture, 2022.\nOfenbeck, G., Steinmann, R., Caparros, V., Spampinato,\nD. G., and P\u00a8uschel, M. Applying the roofline model.\nIn 2014 IEEE International Symposium on Performance\nAnalysis of Systems and Software (ISPASS), pp. 76\u201385.\nIEEE, 2014.\nOpenAI. Gpt-4 technical report, 2023.\n5\nSubmission and Formatting Instructions for ICML 2023\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., K\u00a8opf, A., Yang, E., DeVito, Z., Rai-\nson, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,\nL., Bai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library, 2019.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,\nMatena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring\nthe limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551, 2020.\nSheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,\nD. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E.,\net al.\nHigh-throughput generative inference of large\nlanguage models with a single gpu.\narXiv preprint\narXiv:2303.06865, 2023.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. At-\ntention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\nWang, Y., Chen, K., Tan, H., and Guo, K. Tabi: An efficient\nmulti-level inference system for large language models.\nIn Proceedings of the Eighteenth European Conference\non Computer Systems, pp. 233\u2013248, 2023.\n6\n"
  },
  {
    "title": "FocalFormer3D : Focusing on Hard Instance for 3D Object Detection",
    "link": "https://arxiv.org/pdf/2308.04556.pdf",
    "upvote": "7",
    "text": "FocalFormer3D : Focusing on Hard Instance for 3D Object Detection\nYilun Chen1*\nZhiding Yu3\u2020\nYukang Chen1\nShiyi Lan3\nAnima Anandkumar2,3\nJiaya Jia1\nJose M. Alvarez3\n1The Chinese University of Hong Kong\n2Caltech\n3NVIDIA\nAbstract\nFalse negatives (FN) in 3D object detection, e.g., miss-\ning predictions of pedestrians, vehicles, or other obsta-\ncles, can lead to potentially dangerous situations in au-\ntonomous driving. While being fatal, this issue is under-\nstudied in many current 3D detection methods.\nIn this\nwork, we propose Hard Instance Probing (HIP), a gen-\neral pipeline that identifies FN in a multi-stage manner\nand guides the models to focus on excavating difficult\ninstances.\nFor 3D object detection, we instantiate this\nmethod as FocalFormer3D, a simple yet effective detector\nthat excels at excavating difficult objects and improving\nprediction recall.\nFocalFormer3D features a multi-stage\nquery generation to discover hard objects and a box-level\ntransformer decoder to efficiently distinguish objects from\nmassive object candidates.\nExperimental results on the\nnuScenes and Waymo datasets validate the superior perfor-\nmance of FocalFormer3D. The advantage leads to strong\nperformance on both detection and tracking, in both Li-\nDAR and multi-modal settings. Notably, FocalFormer3D\nachieves a 70.5 mAP and 73.9 NDS on nuScenes detec-\ntion benchmark, while the nuScenes tracking benchmark\nshows 72.1 AMOTA, both ranking 1st place on the nuScenes\nLiDAR leaderboard.\nOur code is available at https:\n//github.com/NVlabs/FocalFormer3D.\n1. Introduction\n3D object detection is an important yet challenging per-\nception task.\nRecent state-of-the-art 3D object detectors\nmainly rely on bird\u2019s eye view (BEV) representation [1\u20133],\nwhere features from multiple sensors are aggregated to con-\nstruct a unified representation in the ego-vehicle coordinate\nspace. There is a rich yet growing literature on BEV-based\n3D detection, including multi-modal fusion [4\u201310], second-\nstage refinements (surface point pooling [3], RoIPool [11\u2013\n14], and cross attention modules [4, 15]).\n*Work done during an internship at NVIDIA.\n\u2020The corresponding author is Zhiding Yu.\nStage=1\nStage=2\nStage=3\nTrue Positives\nMasked Region\nFalse Negatives\nHeatmap Predictions\nTargets for next stage\nTargets for next stage\nIgnore Easy Objects\nIgnore Easy Objects\nFigure 1. Visual example for Hard Instance Probing (HIP). By\nutilizing this multi-stage prediction approach, our model can pro-\ngressively focus on hard instances and facilitate its ability to grad-\nually detect them. At each stage, the model generates some Posi-\ntive object candidates (represented by green circles). Object can-\ndidates assigned to the ground-truth objects can be classified as\neither True Positives (TP, represented by green boxes) and False\nNegatives (FN, represented by red boxes) during training. We ex-\nplicitly model the unmatched ground-truth objects as the hard in-\nstances, which become the main targets for the subsequent stage.\nConversely, Positives are considered easy samples (represented by\ngray boxes) and will be ignored in subsequent stages at both train-\ning and inference time. At last, all heatmap predictions across\nstages are collected as the initial object candidates. We ignored\nthe False Positives for better visualizations.\nDespite the tremendous efforts, there has been limited\nexploration to explicitly address false negatives or missed\narXiv:2308.04556v1  [cs.CV]  8 Aug 2023\nobjects often caused by occlusions and clutter background.\nFalse negatives are particularly concerning in autonomous\ndriving as they cause missing information in the prediction\nand planning stacks. When an object or a part of an object is\nnot detected, this can result in the autonomous vehicle being\nunaware of potential obstacles such as pedestrians, cyclists,\nor other vehicles. This is especially hazardous when the\nvehicle is moving at high speeds and can lead to potentially\ndangerous situations. Therefore, reducing false negatives is\ncrucial to ensure the safety of autonomous driving.\nTo address the challenge of False Negatives in 3D detec-\ntion, we propose and formulate a pipeline called Hard In-\nstance Probing (HIP). Motivated by cascade-style decoder\nhead for object detection [16\u201318], we propose a pipeline to\nprobe false negative samples progressively, which signifi-\ncantly improves the recall rate Fig. 1 illustrates the pipeline\nin a cascade manner. In each stage, HIP suppresses the true\npositive candidates and focuses on the false negative candi-\ndates from the previous stages. By iterating the HIP stage,\nour approach can save those hard false negatives.\nBased on HIP, we introduce a 3D object detector, Fo-\ncalFormer3D, as shown in Fig. 2. Especially, multi-stage\nheatmap predictions [3, 19] are employed to excavate dif-\nficult instances. We maintain a class-aware Accumulated\nPositive Mask, indicating positive regions from prior stages.\nThrough this masking design, the model omits the training\nof easy positive candidates and thereby focuses on the hard\ninstances (False Negatives). Finally, our decoder collects\nthe positive predictions from all stages to produce the ob-\nject candidates. FocalFormer3D consistently demonstrates\nconsiderable gains over baselines in terms of average recall.\nIn addition, we also introduce a box-level refinement\nstep to eliminate redundant object candidates. The approach\nemploys a deformable transformer decoder [17] and repre-\nsents the candidates as box-level queries using RoIAlign.\nThis allows for box-level query interaction and iterative box\nrefinements, binding the object queries with sufficient box\ncontext through RoIAlign [20, 21] on the bird\u2019s eye view\nto perform relative bounding box refinements. Finally, a\nrescoring strategy is adopted to select positive objects from\nobject candidates. Our ablation study in Table 6 demon-\nstrates the effectiveness of the local refinement approach in\nprocessing adequate object candidates.\nOur contributions can be summarized as follows:\n\u2022 We propose Hard Instance Probing (HIP), a learnable\nscheme to automatically identify False Negatives in a\nmulti-stage manner.\n\u2022 We present FocalFormer3D for 3D object detection\nthat effectively harvests hard instances on the BEV and\ndemonstrates effectiveness in terms of average recall.\n\u2022 Without bells and whistles, our model achieves state-\nof-the-art detection performance on both LiDAR-\nbased and multi-modal settings. Notably, our model\nranks 1st places on both nuScenes 3D LiDAR detec-\ntion and tracking leaderboard at time of submission.\n2. Related Work\nModern 3D object detectors, either LiDAR-based [1\u2013\n3, 12, 13, 22\u201329], or Camera-based [30\u201337], or Multi-\nModal [4\u20138, 38\u201345] 3D object detectors generally rely on\nBEV view representation [46]. These methods adopt dense\nfeature maps or dense anchors, for conducting object pre-\ndiction in a bird\u2019s eye view (BEV) space. Among these\nmethods, VoxelNet [22] as the pioneer works discretize\npoint clouds into voxel representation and applies dense\nconvolution to generate BEV heatmaps. SECOND [22] ac-\ncelerates VoxelNet with 3D sparse convolution [47] to ex-\ntract 3D features. Some Pillar-based detectors [2, 23, 48,\n49] collapse the height dimension and utilize 2D CNNs for\nefficient 3D detection.\nDifferent from dense detectors, point-based 3D detec-\ntors [11, 50\u201352] directly process point clouds via Point-\nNet [53, 54] and perform grouping or predictions on the\nsparse representations. Concerning involvement of neigh-\nborhood query on point clouds, it becomes time-consuming\nand unaffordable for large-scale point clouds. Concerning\ncomputation and spatial cost, another line of 3D detectors\ndirectly predicts objects on sparse point clouds to avoid\ndense feature construction. SST [55] applies sparse regional\nattention and avoids downsampling for small-object detec-\ntion. FSD [56] instead further recognize instances directly\non sparse representations obtained by SST [55] and Spar-\nseConv for long-range detection.\nRecent multi-modal detectors [5\u20137, 39, 42, 57] follow\nthe similar paradigm of BEV detectors and incorporate the\nmulti-view image features by physical projection or learn-\nable alignments between LiDAR and cameras. TransFu-\nsion [4] applies cross attention to obtain image features\nfor each object query.\nDespite various kinds of modal-\nspecific voxel feature encoders, these detectors finally pro-\nduce dense BEV features for classification and regression at\nthe heatmap level.\n3. Methodology\nWe introduce Hard Instance Probing (HIP) for auto-\nmated identifying hard instances (False Negatives) in Sec-\ntion 3.1.\nWe then present the implementations for the\ntwo main components of FocalFormer3D. Section 3.2 de-\nscribes our multi-stage heatmap encoder that harvests the\nFalse Negatives for producing high-recall initial object can-\ndidates following HIP. Section 3.3 introduces a box-level\ndeformable decoder network that further distinguishes ob-\njects from these candidates.\nBox-Pooling\nSelf-Attention\nDeformable \nCross-Attention\nq\nk,v\nBackbone\nNetwork\nInput\nMulti-Stage Heatmap Encoder\nOutput\nAccumulated Positive Mask\nObject Queries\nBEV Heatmaps\nBEV Features\nIterating Decoder\nFFN\n3D Box\nScore\nMLP\n0.75\n0.42\nBox-level Deformable Decoder\nq\u2019\nAccumulation\nAccumulation\nMultiplication\nQuery Collection\nFigure 2. Overall architecture of FocalFormer3D. The overall framework comprises two novel components: a multi-stage heatmap\nencoder network that uses the Hard Instance Probing (HIP) strategy to produce high-recall object queries (candidates), and a deformable\ntransformer decoder network with rescoring mechanism that is responsible for eliminating false positives from the large set of candidates.\n(a) Following feature extraction from modalities, the map-view features produce a set of multi-stage BEV features and then BEV heatmaps.\nThe positive mask accumulates to exclude the easy positive candidates of prior stages from BEV heatmaps. The left object candidates are\nchosen and collected according to the response of BEV heatmap in a multi-stage process. (b) A deformable transformer decoder is\nadapted to effectively handle diverse object queries. The query embedding is enhanced with a box pooling module, which leverages the\nintermediate object supervision to identify local regions. It refines object queries in a local-scope manner, rather than at a point level.\nResidual connections and normalization layers have been excluded from the figure for clarity.\nTrue Positives\nMasked Objects\nFalse Negatives\nHeatmap Predictions\nFalse Positives\nTrue Negatives\nAnother Network to distinguish FP\nG\nG\nG\nG\nG\nTP\nFN\nFP\nG\nG\nG\nG\nG\nG\nStage 1\nStage 2\nG\nG\nG\nCandidate Collections\nObject-level Refinement\nG\nG\nPos\nNeg\nInitial Stage\nHIP stage\nPos\nNeg\nObject-level \nRefinement\nG\nG\nG\nG\nG\nG\nG\nG\nG\nG\nFg/Bg=2/3\nFg/Bg=1/3\nFg/Bg=2/1\nCandidate G GT-object\nhard instance during training\nFigure 3. Hard Instance Probing. We use the symbol \u201cG\u201d to indi-\ncate the object candidates that are labeled as ground-truth objects\nduring the target assignment process in training. To ensure clarity,\nwe omit numerous negative predictions for detection, given that\nbackground takes up most of the images.\n3.1. Hard Instance Probing (HIP)\nReal-world applications, such as autonomous driving,\nrequire a high level of scene understanding to ensure safe\nand secure operation. In particular, false negatives in object\ndetection can present severe risks, emphasizing the need for\nhigh recall rates. However, accurately identifying objects in\ncomplex scenes or when occlusion occurs is challenging in\n3D object detection, resulting in many false negative predic-\ntions. Unfortunately, few studies have explicitly focused on\naddressing false negatives in the design of detection heads.\nMotivated by the cascade-style detectors, we formulate a\ntraining pipeline to emulate the process of identifying false\nnegative predictions at inference time.\nFormulation of Hard Instance Probing. Our strategy to\nidentify hard instances operates stage by stage, as illustrated\nby a toy example in Fig. 3. Initially, we annotate the ground-\ntruth objects as\nO = {oi, i = 1, 2, ...} ,\nwhich is the main targets for initial stages. The neural net-\nwork makes Positive or Negative predictions given a set of\ninitial object candidates A = {ai, i = 1, 2, ...} , which is\nnot limited to anchors [58], point-based anchors [3], and\nobject queries [59]. Suppose the detected objects (Positive\npredictions) at k-th stage are\nPk = {pi, i = 1, 2, ...} .\nWe are then allowed to classify the ground-truth objects ac-\ncording to their assigned candidates:\nOT P\nk\n=\n\b\noj\n\f\f\u2203pi \u2208 Pk, \u03c3(pi, oj) > \u03b7\n\t\n.\nwhere an object matching metric \u03c3(\u00b7, \u00b7) (e.g. Intersection\nover Union [60, 61] and center distance [62]) and a prede-\nfined threshold \u03b7. Thus, the left unmatched targets can be\nregarded as hard instances:\nOF N\nk\n= O \u2212\nk[\ni=1\nOT P\nk\n.\nThe training of (k + 1)-th stages is to detect these targets\nOF N\nk\nfrom the object candidates while omitting all prior\nPositive object candidates.\nDespite the cascade way mimicking the process of iden-\ntifying false negative samples, we might collect a number\nof object candidates across all stages. Thus, a second-stage\nobject-level refinement model is necessary to eliminate any\npotential false positives.\nRelation with hard example mining. The most relevant\ntopic close to our approach is hard example mining [63, 64],\nwhich samples hard examples during training. Recent re-\nsearch [65\u201367] has further explored soft-sampling, such\nas adjusting the loss distribution to mitigate foreground-\nbackground imbalance issues. In contrast, our method op-\nerates in stages. Specifically, we use False Negative predic-\ntions from prior stages to guide the subsequent stage of the\nmodel toward learning from these challenging objects.\n3.2. Multi-stage Heatmap Encoder\nThe upcoming subsections outline the key implementa-\ntions of FocalFormer3D as depicted in Fig. 2. We begin\nby detailing the implementation of hard instance probing\nfor BEV detection.\nThis involves using the BEV center\nheatmap to generate the initial object candidate in a cascade\nmanner.\nPreliminary of center heatmap in BEV perception. In\ncommon practice [3, 4, 19], the objective of the BEV\nheatmap head is to produce heatmap peaks at the center lo-\ncations of detected objects. The BEV heatmaps are repre-\nsented by a tensor S \u2208 RX\u00d7Y \u00d7C, where X \u00d7 Y indicates\nthe size of BEV feature map and C is the number of object\ncategories. The target is achieved by producing 2D Gaus-\nsians near the BEV object points, which are obtained by\nprojecting 3D box centers onto the map view. In top views\nsuch as Fig. 4, objects are more sparsely distributed than in\na 2D image. Moreover, it is assumed that objects do not\nhave intra-class overlaps on the bird\u2019s eye view.\nBased on the non-overlapping assumption, excluding\nprior easy positive candidates from BEV heatmap predic-\ntions can be achieved easily. In the following, we illustrate\nthe implementation details of HIP, which utilizes an accu-\nmulated positive mask.\nPositive mask accumulation. To keep track of all easy pos-\nitive object candidates of prior stages, we generate a posi-\ntive mask (PM) on the BEV space for each stage and accu-\nmulated them to an accumulated positive mask (APM):\n\u02c6\nMk \u2208 {0, 1}X\u00d7Y \u00d7C ,\nFigure 4. Example visualization for the positive mask. (left)\nand predicted BEV heatmap (right). The positive mask is class-\naware and we show different categories with different colors for\nvisualization. The masking area for objects of different categories\ncan differ in the pooling-based masking method.\nwhich is initialized as all zeros.\nThe generation of multi-stage BEV features is accom-\nplished in a cascade manner using a lightweight inversed\nresidual block [68] between stages.\nMulti-stage BEV\nheatmaps are generated by adding an extra convolution\nlayer.\nAt each stage, we generate the positive mask ac-\ncording to the positive predictions. To emulate the process\nof identifying False Negatives, we use a test-time selection\nstrategy that ranks the scores according to BEV heatmap re-\nsponse [3, 4]. Specifically, at the k-th stage, Top-K selection\nis performed on the BEV heatmap across all BEV positions\nand categories, producing a set of object predictions Pk.\nThen the positive mask Mk \u2208 {0, 1}X\u00d7Y \u00d7C records the all\nthe positions of positive predictions by setting M(x,y,c) = 1\nfor each predicted object pi \u2208 Pk, where (x, y) represents\npi\u2019s location and c is pi\u2019s class. The left points are set to 0\nby default.\nAccording to the non-overlapping assumption, the ideal\nway to indicate the existence of a positive object candidate\n(represented as a point in the center heatmap) on the mask is\nby masking the box if there is a matched ground truth box.\nHowever, since the ground-truth boxes are not available at\ninference time, we propose the following masking methods\nduring training:\n\u2022 Point Masking.\nThis method involves no change,\nwhere only the center point of the positive candidates\nis filled.\n\u2022 Pooling-based Masking. In this method, smaller ob-\njects fill in the center points while larger objects fill in\nwith a kernel size of 3 \u00d7 3.\n\u2022 Box Masking. This method requires an additional box\nprediction branch and involves filling the internal re-\ngion of the predicted BEV box.\nThe accumulated positive mask (APM) for the k-th stage\nis obtained by simply accumulating prior Positive Masks as\nfollows:\n\u02c6\nMk = max\n1\u2264i\u2264k Mi.\nBy masking the BEV heatmap Sk with\n\u02c6Sk = Sk \u00b7 (1 \u2212 \u02c6\nMk),\nwe omit prior easy positive regions in the current stage, thus\nenabling the model to focus on the false negative samples\nof the prior stage (hard instances). To train the multi-stage\nheatmap encoder, we adopt Gaussian Focal Loss [4] as the\ntraining loss function. We sum up the BEV heatmap losses\nacross stages to obtain the final heatmap loss.\nDuring both training and inference, we collect the posi-\ntive candidates from all stages as the object candidates for\nthe second-stage rescoring as the potential false positive\npredictions.\nDiscussion on implementation validity for HIP. Although\nthe HIP strategy is simple, the masking way has two critical\ncriteria that need to be met to ensure valid implementation\nof HIP:\n\u2022 Exclusion of prior positive object candidates at the cur-\nrent stage.\n\u2022 Avoidance of removal of potential real objects (false\nnegatives).\nPoint masking satisfies both requirements based on the fol-\nlowing facts. As the Top-K selection is based on ranking\npredicted BEV heatmap scores, the hottest response points\nare automatically excluded when a point is masked. Be-\nsides, the design of a class-aware positive mask ensures that\nnon-overlapping assumptions at the intra-class level on the\nBEV are met.\nHowever, the point masking strategy is less efficient as\nonly one BEV object candidate is excluded for each positive\nprediction compared with the ideal masking with ground-\ntruth box guidance. Therefore, there is a trade-off between\nthe masking area and the validity of the exclusion opera-\ntion. We compare all three strategies in Table 5 and pooling-\nbased masking performs better than others.\n3.3. Box-level Deformable Decoder\nThe object candidates obtained from the multi-stage\nheatmap encoder can be treated as positional object\nqueries [4, 69]. The recall of initial candidates improves\nwith an increase in the number of collected candidates.\nHowever, redundant candidates introduce false positives,\nthereby necessitating a high level of performance for the\nfollowing object-level refinement blocks.\nTo enhance the efficiency of object query processing, we\nemploy deformable attention [17] instead of computation-\nally intensive modules such as cross attention [59] or box\nattention [70]. Unlike previous methods that used center\npoint features as the query embedding [4, 69], we model the\nobject candidates as box-level queries. Specifically, Specif-\nically, we introduce object supervision between deformable\ndecoder layers, facilitating relative box prediction.\nBox-pooling module.\nTo better model the relations be-\ntween objects and local regions in the regular grid manner,\nwe extract the box context information from the BEV fea-\ntures using simple RoIAlign [20] in the Box-pooling mod-\nule as Fig. 2. In specific, given the intermediate predicted\nbox, each object query extracts 7\u00d77 feature grid points [20]\nfrom the BEV map followed by two MLP layers. The posi-\ntional encoding is also applied both for queries and all BEV\npoints for extracting positional information. This allows us\nto update both the content and positional information into\nthe query embedding. This lightweight module enhances\nthe query feature for the deformable decoder (See Table 6).\nDecoder\nimplementation.\nFollowing\nDeformable\nDETR [17], our model employs 8 heads in all attention\nmodules, including multi-head attention and multi-head\ndeformable attention. The deformable attention utilizes 4\nsampling points across 3 scales. To generate three scales\nof BEV features, we apply 2\u00d7 and 4\u00d7 downsampling\noperations to the original BEV features. The box-pooling\nmodule extracts 7\u00d77 feature grid points within each rotated\nBEV box followed by 2 FC layers and adds the object\nfeature to query embedding. We expand the predicted box\nto 1.2\u00d7 size of its original size.\n3.4. Model Training\nThe model is trained in two stages. In the first stage, we\ntrain the LiDAR backbone using a deformable transformer\ndecoder head, which we refer to as DeformFormer3D (Ta-\nble 4 (a)).\nAfter initializing the weights from Deform-\nFormer3D, we train the FocalFormer3D detector, which\nconsists of a multi-stage heatmap encoder and a box-level\ndeformable decoder. However, during the training of the\ndeformable decoder with bipartite graph matching, we en-\ncounter slow convergence issues in the early stages [18]. To\naddress this, we generate noisy queries from ground-truth\nobjects [18, 77, 78], enabling effective training of the model\nfrom scratch. Additionally, we improve the training process\nby excluding matching pairs with a center distance between\nthe prediction and its GT object exceeding 7 meters.\n4. Experiments\n4.1. Experimental Setup\nDataset and metric.\nWe evaluate our approach on\nnuScenes and Waymo 3D detection dataset.\nnuScenes Dataset [62] is a large-scale outdoor dataset.\nnuScenes contains 1, 000 scenes of multi-modal data, in-\nMethods\nModality\nmAP\nNDS\nCar\nTruck\nC.V.\nBus\nTrailer\nBarrier\nMotor.\nBike\nPed.\nT.C.\nLiDAR-based 3D Detection\nPointPillars [2]\nL\n30.5\n45.3\n68.4\n23.0\n4.1\n28.2\n23.4\n38.9\n27.4\n1.1\n59.7\n30.8\nCBGS [71]\nL\n52.8\n63.3\n81.1\n48.5\n10.5\n54.9\n42.9\n65.7\n51.5\n22.3\n80.1\n70.9\nLargeKernel3D [28]\nL\n65.3\n70.5\n85.9\n55.3\n26.8\n66.2\n60.2\n74.3\n72.5\n46.6\n85.6\n80.0\nTransFusion-L [4]\nL\n65.5\n70.2\n86.2\n56.7\n28.2\n66.3\n58.8\n78.2\n68.3\n44.2\n86.1\n82.0\nPillarNet-34 [48]\nL\n66.0\n71.4\n87.6\n57.5\n27.9\n63.6\n63.1\n77.2\n70.1\n42.3\n87.3\n83.3\nLiDARMultiNet [72]\nL\n67.0\n71.6\n86.9\n57.4\n31.5\n64.7\n61.0\n73.5\n75.3\n47.6\n87.2\n85.1\nFocalFormer3D\nL\n68.7\n72.6\n87.2\n57.1\n34.4\n69.6\n64.9\n77.8\n76.2\n49.6\n88.2\n82.3\nCenterPoint [3] \u2020\nL\n60.3\n67.3\n85.2\n53.5\n20.0\n63.6\n56.0\n71.1\n59.5\n30.7\n84.6\n78.4\nMGTANet\u2020 [73]\nL\n67.5\n72.7\n88.5\n59.8\n30.6\n67.2\n61.5\n66.3\n75.8\n52.5\n87.3\n85.5\nLargeKernel3D\u2021 [28]\nL\n68.8\n72.8\n87.3\n59.1\n30.2\n68.5\n65.6\n75.0\n77.8\n53.5\n88.3\n82.4\nFocalFormer3D \u2020\nL\n70.5\n73.9\n87.8\n59.4\n37.8\n73.0\n65.7\n77.8\n77.4\n52.4\n90.0\n83.4\nMulti-Modal 3D Detection\nPointPainting [74]\nL+C\n46.4\n58.1\n77.9\n35.8\n15.8\n36.2\n37.3\n60.2\n41.5\n24.1\n73.3\n62.4\n3D-CVF [75]\nL+C\n52.7\n62.3\n83.0\n45.0\n15.9\n48.8\n49.6\n65.9\n51.2\n30.4\n74.2\n62.9\nMVP [41]\nL+C\n66.4\n70.5\n86.8\n58.5\n26.1\n67.4\n57.3\n74.8\n70.0\n49.3\n89.1\n85.0\nFusionPainting [76]\nL+C\n68.1\n71.6\n87.1\n60.8\n30.0\n68.5\n61.7\n71.8\n74.7\n53.5\n88.3\n85.0\nTransFusion [4]\nL+C\n68.9\n71.7\n87.1\n60.0\n33.1\n68.3\n60.8\n78.1\n73.6\n52.9\n88.4\n86.7\nBEVFusion [5]\nL+C\n69.2\n71.8\n88.1\n60.9\n34.4\n69.3\n62.1\n78.2\n72.2\n52.2\n89.2\n85.2\nBEVFusion-MIT [6]\nL+C\n70.2\n72.9\n88.6\n60.1\n39.3\n69.8\n63.8\n80.0\n74.1\n51.0\n89.2\n86.5\nDeepInteraction [10]\nL+C\n70.8\n73.4\n87.9\n60.2\n37.5\n70.8\n63.8\n80.4\n75.4\n54.5\n91.7\n87.2\nFocalFormer3D\nL+C\n71.6\n73.9\n88.5\n61.4\n35.9\n71.7\n66.4\n79.3\n80.3\n57.1\n89.7\n85.3\nPointAugmenting [57] \u2020\nL+C\n66.8\n71.0\n87.5\n57.3\n28.0\n65.2\n60.7\n72.6\n74.3\n50.9\n87.9\n83.6\nFocals Conv-F [27] \u2021\nL+C\n70.1\n73.6\n87.5\n60.0\n32.6\n69.9\n64.0\n71.8\n81.1\n59.2\n89.0\n85.5\nLargeKernel3D-F [28] \u2021\nL+C\n71.1\n74.2\n88.1\n60.3\n34.3\n69.1\n66.5\n75.5\n82.0\n60.3\n89.6\n85.7\nFocalFormer3D-F \u2020\nL+C\n72.9\n75.0\n88.8\n63.5\n39.0\n73.7\n66.9\n79.2\n81.0\n58.1\n91.1\n87.1\nTable 1. Performance comparison on the nuScenes 3D detection test set. \u2020 represents using flipping test-time augmentation. \u2021 means us-\ning both flipping and rotation test-time augmentation. C.V, Motor., Ped. and T.C. are short for construction vehicle, motorcycle, pedestrian,\nand traffic cones, respectively.\ncluding 32-beams LiDAR with 20FPS and 6-view camera\nimages. We mainly evaluate our method on both LiDAR-\nonly and LiDAR-Camera fusion settings. The evaluation\nmetrics follow nuScenes official metrics including mean av-\nerage precision (mAP) and nuScenes detection score (NDS)\ndefined by averaging the matching thresholds of center dis-\ntance D = {0.5, 1., 2., 4.} (m). For evaluating the quality of\nobject queries, we also introduce the Average Recall (AR)\ndefined by center distance as well. The ablation studies in\nour research primarily utilize the nuScenes dataset, unless\nexplicitly stated otherwise.\nWaymo Open Dataset [61] has a wider detection range of\n150m \u00d7 150m compared to the nuScenes dataset. Waymo\ndataset comprises of 798 scenes for training and 202 scenes\nfor validation. The official evaluation metrics used are mean\nAverage Precision (mAP) and mean Average Precision with\nHeading (mAPH), where the mAP is weighted by the head-\ning accuracy. The mAP and mAPH scores are computed\nwith a 3D Intersection over Union (IoU) threshold of 0.7\nfor Vehicle and 0.5 for Pedestrian and Cyclist. The evalua-\ntion has two difficulty levels: Level 1, for boxes with more\nthan five LiDAR points, and Level 2, for boxes with at least\none LiDAR point. Of the two difficulty levels, Level 2 is\nprioritized as the primary evaluation metric for all experi-\nments.\nImplementation details.\nOur implementation is mainly\nbased on the open-sourced codebase MMDetection3D [79].\nFor the LiDAR backbone, we use CenterPoint-Voxel as the\npoint cloud feature extractor. For the multi-stage heatmap\nencoder, we apply 3 stages, generating a total of 600 queries\nby default. Data augmentation includes random double flip-\nping along both X and Y axes, random global rotation be-\ntween [\u2212\u03c0/4, \u03c0/4], the random scale of [0.9, 1.1], and ran-\ndom translation with a standard deviation of 0.5 in all axes.\nAll models are trained with a batch size of 16 on eight V100\nGPUs. More implementation details are referred to in sup-\nplementary files.\n4.2. Main Results\nnuScenes LiDAR-based 3D object detection. We eval-\nuate the performance of FocalFormer3D on the nuScenes\ntest set. As shown in Table 1, the results demonstrate its\nsuperiority over state-of-the-art methods on various evalua-\ntion metrics and settings. Our single-model FocalFormer3D\nachieved 68.7 mAP and 72.6 NDS, which surpasses the\nprior TransFusion-L method by +3.2 points on mAP and\n+2.4 points on NDS. Notably, even compared with the pre-\nvious best method that was trained with segmentation-level\nlabels, our method without extra supervision still outper-\nformed LiDARMultiNet by +1.7 mAP and +1.0 NDS.\nnuScenes multi-modal 3D object detection. We extend\nour approach to a simple multi-modal variant and demon-\nstrate its generality.\nFollowing TransFusion [4], we use\na pre-trained ResNet-50 model on COCO [80] and nuIm-\nage [62] dataset as the image model and freeze its weights\nduring training. To reduce computation costs, the input im-\nages are downscaled to 1/2 of their original size. Unlike\nheavy lift-splat-shot [32] camera encoders used in BEV-\nFusion [5, 6], the multi-view camera images are projected\nonto a pre-defined voxel space and fused with LiDAR BEV\nfeature.\nAdditional details are available in the supple-\nmentary files.\nWithout test-time augmentation, our sim-\nple multi-modal variant model outperforms all other state-\nof-the-art with less inference time (Table 2). With TTA,\nFocalFormer3D achieves 72.9 mAP and 75.0 NDS, rank-\ning first among all single-model solutions on the nuScenes\nbenchmark. Interestingly, our model achieves high results\nfor some rare classes such as (Trailer, Motorcycle, Bicycle)\ncompared to other methods.\nnuScenes 3D object tracking. To further demonstrate the\nversatility, we also extend FocalFormer3D to 3D multi-\nobject tracking (MOT) by using the tracking-by-detection\nalgorithm SimpleTrack. Interested readers can refer to the\noriginal paper [81] for more comprehensive details.\nAs\ndepicted in Table 2, FocalFormer3D gets 2.9 points better\nthan prior state-of-the-art TransFusion-L [4] in LiDAR set-\ntings and FocalFormer3D-F achieves 2.1 points over Trans-\nFusion in terms of AMOTA. Moreover, our single model\nFocalFormer3D-F with double-flip testing results performs\neven better than the BEVFusion [6] with model ensembling.\nWaymo LiDAR 3D object detection. The results of our\nsingle-frame LiDAR 3D detection method on the Waymo\ndataset are presented in Table 3, alongside the comparison\nwith other approaches. Employing with the same VoxelNet\nbackbone as nuScenes, our method achieves competitive\nperformance without any fine-tuning of the model hyper-\nparameters specifically for the Waymo dataset. Particularly,\nwhen compared to TransFusion-L with the same backbone,\nour method exhibits a +1.1 mAPH improvement.\n4.3. Recall Analysis\nTo diagnose the performance improvements, we com-\npare several recent methods in terms of AR for both stages\nMethods\nAMOTA\nAMOTP\nMOTA\nIDS\nLiDAR-based 3D Tracking\nAB3DMOT [82]\n15.1\n150.1\n15.4\n9027\nCenterPoint [3]\n63.8\n55.5\n53.7\n760\nCBMOT [83]\n64.9\n59.2\n54.5\n557\nOGR3MOT [84]\n65.6\n62.0\n55.4\n288\nSimpleTrack [81]\n66.8\n55.0\n56.6\n575\nUVTR-L [7]\n67.0\n55.0\n56.6\n774\nTransFusion-L [4]\n68.6\n52.9\n57.1\n893\nFocalFormer3D\n71.5\n54.9\n60.1\n888\nFocalFormer3D\u2020\n72.1\n47.0\n60.0\n701\nMulti-Modal 3D Tracking\nUVTR-MultiModal [7]\n70.1\n68.6\n61.8\n941\nTransFusion [4]\n71.8\n55.1\n60.7\n944\nBEVFusion-MIT [6]\u2021\n74.1\n40.3\n60.3\n506\nFocalFormer3D-F\n73.9\n51.4\n61.8\n824\nFocalFormer3D-F\u2020\n74.6\n47.3\n63.0\n849\nTable 2. Performance comparison on nuScenes 3D tracking test\nset.\n\u2020 is based on the double-flip testing results in Table 1.\n\u2021 is\nbased on model ensembling.\nMethods\nmAP\nmAPH\nVel.\nPed.\nCyc.\nLiDAR-based 3D Detection\nRSN\u22c6 [85]\n\u2013\n\u2013\n65.5\n63.7\n\u2013\nAFDetV2\u22c6 [86]\n71.0\n68.8\n69.2\n67.0\n70.1\nSST\u22c6 [55]\n67.8\n64.6\n65.1\n61.7\n66.9\nPV-RCNN\u22c6 [24]\n66.8\n63.3\n68.4\n65.8\n68.5\nPV-RCNN++\u22c6 [25]\n71.7\n69.5\n70.2\n68.0\n70.2\nPillarNet-34\u22c6 [48]\n71.0\n68.8\n70.5\n66.2\n68.7\nFSD-spconv\u22c6 [56]\n71.9\n69.7\n68.5\n68.0\n72.5\nCenterPoint [3]\n69.8\n67.6\n73.4\n65.8\n68.5\nTransFusion-L\u2227 [4]\n70.5\n67.9\n66.8\n66.1\n70.9\nFocalFormer3D\n71.5\n69.0\n67.6\n66.8\n72.6\nTable 3. Performance comparison on the Waymo val set. All\nmodels inputs single-frame point clouds. The methods marked\nwith \u2217 indicate the utilization of different point cloud backbones\nin VoxelNet. The method marked with \u2227 indicates our reproduc-\ntion. The evaluation metric used is the LEVEL 2 difficulty, and\nthe results are reported on the full Waymo validation set.\n\u2013 initial BEV heatmap predictions and final box predictions\nin Fig. 5. The metric of AR is computed based on center\ndistance following the nuScenes metrics and different dis-\ntance thresholds (e.g., 0.5m, 1.0m, 2.0m, 4.0m), and the\nmean AR (mAR) are compared.\nRecall comparison on initial object candidates.\nFig-\nure 5 compares the recall of state-of-the-art methods that\nshare the same SparseUNet backbone.\nWith total 200\nqueries, FocalFormer3D-200P reaches 75.2 mAR, achiev-\ning considerable and consistent improvements by +4.5\n70\n75\n80\nValue\nCenterPoint (L) \nTransFusion-L (L) \nFocalFormer3D-200P (L) \nFocalFormer3D (L) \nBEVFusion (LC) \nDeepInteraction (LC) \nFocalFormer3D-F (LC)\n70.6\n70.7\n75.2\n79.2\n72.6\n72.6\n80.9\n71.4\n71.7\n76.4\n80.8\n73.5\n75.2\n82\nARMean(%)\n60\n65\n70\nValue\n57.6\n58\n63.8\n67.4\n59.4\n59.3\n68.7\n60.2\n61.1\n68.4\n73.3\n62.5\n63.7\n73.4\nAR<0.5m(%)\n70\n75\n80\nValue\n70.1\n70.2\n76.3\n81.3\n72\n72\n82.9\n70.5\n70.7\n76.4\n81.5\n72.4\n74.1\n82.9\nAR<1.0m(%)\n75\n80\n85\nValue\n75.8\n75.8\n79.3\n83.2\n78\n78\n85.1\n75.9\n76\n79.2\n83.5\n78\n80.1\n85.2\nAR<2.0m(%)\n80\n85\nValue\n79\n78.8\n81.6\n85.1\n81\n81\n86.7\n79\n78.9\n81.6\n85.1\n81.1\n83\n86.7\nAR<4.0m(%)\nHeatmap\nFinal\nFigure 5. Average recall comparisons between initial object predictions and final object prediction centers on the nuScenes val set. The\nsubfigures are shown over center distance thresholds (%) following nuScenes detection metrics.\nAll\nBarrier\nBike\nBus\nC.V.\nCar\nMotor.\nPed.\nT.C.\nTrailer\nTruck\nDistance Thresh\n90\n95\n100\n105\n110\n115\n120\n125\n130\n135\nNormalized Recall\n110.9\n110.6\n113.7\n111.1\n125.8\n98.6\n111.9\n105.9\n104.8\n130.8\n112.1\nTransFusion-L (L)\nFocalFormer3D (L)\nFigure 6. Class-wise recall comparison on nuScenes val set be-\ntween TransFusion-L and FocalFormer3D in terms of recall values\nacross nuScenes center distance (CD) threshes (0.25/0.5/1.0m) on\nthe nuScenes val set. The red bars are normalized to 100%.\nmAR compared with the prior state-of-the-art LiDAR ap-\nproach TransFusion-L. Surprisingly, our LiDAR-based Fo-\ncalFormer even achieves better results than the prior multi-\nmodal approach DeepInteraction by 2.6 points in terms\nof mAR as well.\nAs the query sizes get 600, Focal-\nFormer3D achieves 79.2 mAR, surpassing the fusion ap-\nproach DeepInteraction by 6.6 points. Further, by incorpo-\nrating multi-view camera features, our multi-modal version\nFocalFormer-F gets improved to 80.9 mAR.\nRecall comparison on final object prediction. Concern-\ning the final predictions of 3D detectors, most LiDAR and\nfusion approaches obtain fewer performance improvements\nas the distance thresholds increase as shown in Fig. 5. This\ncan be explained by higher distance thresholds indicating\nthe performance for the extreme cases of missing detec-\ntions. The introduction of camera features helps the model\nsee the context in the perspective view, which leads to better\nperformance such as DeepInteraction. However, their final\nprediction recall falls far behind FocalFormer-F with a large\nmargin of 6.8 points.\nClass-wise recall comparison. We compare the class-wise\nrecall analysis for object candidates in Fig. 6 at the cat-\negory level.\nThe findings highlight the effectiveness of\n#\n# Stages\n#Total Queries\nmAP\nNDS\n(a)\n1\n200\n65.3\n70.5\n(b)\n2\n200\n66.0\n70.7\n(c)\n1\n600\n65.4\n70.5\n(d)\n2\n600\n66.4\n70.9\n(e)\n3\n600\n66.5\n71.1\nTable 4. Effects of numbers of stages and total queries. Here\none stage stands for the baseline method without using hard in-\nstance probing.\nMask Type\nmAP\nNDS\nNone\n65.3\n70.4\nPoint-based\n65.9\n70.5\nBox-based\n66.1\n70.9\nPooling-based\n66.5\n71.1\nTable 5. Effects of various positive mask types. All models adopt\nthe same network except for the masking way.\nFocalFormer3D in improving the relative recall of initial\nBEV queries by a relative +10.9% improvement against\nTransFusion-L. Large objects such as Construction Vehicles\nand Trailer get the most improvements so that the predic-\ntions of their initial centers are challenging.\n4.4. Ablation Study\nHIP query sizes and generation stages. Table 4 ablates the\nimpacts of the number of queries and stages in the multi-\nstage heatmap encoder. When using the same query size\nof rough 200, approaches (b), which uses additional one\nstage of HIP, demonstrates better performance than baseline\n(a) by a margin of +0.7 mAP. When provided with more\nqueries (600), our approach (d) and (e) achieve over 1.1-\npoint improvement in terms of mAP.\nPositive mask type. Table 5 presents an ablation study on\nthe effectiveness of Hard Instance Probing in terms of vari-\nous mask types. Specifically, we compare the performance\nof our method with none masking, point-based masking,\n#\nM.S. Heat\nRefinement Module\nmAP\nNDS\nBoxPool\nC.A.\n(a)\n\u2717\n\u2717\n\u2717\n63.1\n69.1\n(b)\n\u2713\n\u2717\n\u2717\n63.3\n69.3\n(c)\n\u2713\n\u2713\n\u2717\n65.1\n69.9\n(d)\n\u2713\n\u2717\n\u2713\n65.9\n70.9\n(e)\n\u2713\n\u2713\n\u2713\n66.5\n71.1\n(f)\n\u2713\nRescoring Only\n66.1\n68.8\nTable 6. Step-by-step improvements made by modules. \u201cM.S.\nHeat\u201d represents the application of the multi-stage heatmap en-\ncoder for hard instance probing. \u201cC.A.\u201d denotes using deformable\ncross attention for second-stage refinement. \u201cBoxPool\u201d represents\nthe Box-pooling module. The term \u201cRescoring Only\u201d refers to\nthe model that directly generates box prediction from BEV feature\nand uses its decoder head to rescore the candidate predictions from\nheatmap without performing additional bounding box refinement.\nand pooling-based masking. The results demonstrate that\neven with single-point masking, HIP improves the perfor-\nmance of the baseline by a gain of +0.6 points in terms of\nmAP. Furthermore, the pooling-based masking shows the\nbest gain with +1.2 mAP and +0.7 NDS, outperforming\nthe box-based masking. This can be attributed to two facts.\nPoint or pooling-based masking can already effectively ex-\nclude positive objects as the center heatmap [3] only high-\nlights a Gaussian peak. Second, the wrong false positive\npredictions or predicted boxes might lead to false masking\nof the ground-truth boxes, resulting in missed detection.\nStep-by-step module refinement.\nWe conduct ablation\nstudies on the step-by-step improvements by each module,\npresented in Table 6, to illustrate the component effective-\nness within hard instance probing (HIP) pipeline. Initially,\nwithout second-stage refinement, we used simple center-\nbased predictions [3] (a), which estimate boxes directly\nfrom BEV feature by another convolutional layer.\nDespite an improvement in the average recall by over 9\npoints in Fig. 5, we found little improvement of (b) over\n(a) in performance after using the multi-stage heatmap en-\ncoder to generate the object candidates. By applying simple\nobject-level rescoring (c), with RoI-based refinement (using\ntwo hidden MLP layers), the performance is boosted to 65.1\nmAP and 69.9 NDS. Remarkably, our complete box-level\ndeformable decoder (e) further improves the performance\nby a margin of +1.4 mAP and +1.2 NDS.\nTo assess the effects of rescoring alone, we perform ex-\nperiment (f), which excludes the effects of box regression\nby not using any box or position regression in the object-\nlevel refinement module. Despite this, experiment (f) still\nachieves high center accuracy (66.1 mAP) compared to (a).\nThis finding highlights the limitations of the initial rank-\nModels/Components\nLatency\nTransFusion-L\n93ms\nFocalFormer3D\n109ms\n\u2013 VoxelNet backbone\n78ms\n\u2013 Multi-stage heatmap encoder\n13ms\n\u2013 Box-level deformable decoder\n18ms\nTable 7. Latency analysis for model components. Latency is\nmeasured on a V100 GPU for reference.\ning of object candidates across stages based solely on BEV\nheatmap scores. Therefore, it validates the necessity for\na second-stage object-level rescoring in the hard instance\nprobing pipeline (Fig. 3).\nLatency analysis for model components. We conduct a la-\ntency analysis for FocalFormer3D on the nuScenes dataset.\nThe runtimes are measured on the same V100 GPU machine\nfor comparison.To ensure a fair speed comparison with\nCenterPoint [3], dynamic voxelization [87] is employed for\nspeed testing of both TransFusion-L and FocalFormer3D.\nThe computation time is mostly taken up by the sparse\nconvolution-based backbone network (VoxelNet [1, 22]),\nwhich takes 78ms. Our multi-stage heatmap encoder takes\n13ms to collect queries from the heatmaps across stages,\nwhile the box-level deformable decoder head takes 18ms.\nNote that, the generation of multi-stage heatmaps only takes\n5ms, and additional operations such as Top-K selection\ntakes 7ms, indicating potential optimization opportunities\nfor future work.\n5. Conclusion\nIn this work, we explicitly focus on the fatal problem\nin autonomous driving, i.e., false negative detections. We\npresent FocalFormer3D as solution. It progressively probes\nhard instances and improves prediction recall, via the hard\ninstance probing (HIP). Nontrivial improvements are intro-\nduced with limited overhead upon transformer-based 3D\ndetectors. The HIP algorithm enables FocalFormer3D to\neffectively reduce false negatives in 3D object detection.\nLimitation.\nA key limitation is that FocalFormer3D\u2019s\nhard instance probing (HIP) relies on the assumption that\nobject centers produce Gaussian-like peaks in the BEV\nheatmap, which may not hold for camera-based detectors\nwhere heatmaps tend to be fan-shaped. Additionally, few\nstudies have explored hard instances in long-range detec-\ntion, so more research is needed to evaluate HIP in this area.\nWe leave more investigation of hard instance probing as fu-\nture work.\nReferences\n[1] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning\nfor point cloud based 3d object detection. In CVPR, 2018. 1,\n2, 9, 14\n[2] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. 2018. 1, 2, 6\n[3] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-\nbased 3d object detection and tracking.\nIn CVPR, pages\n11784\u201311793, 2021. 1, 2, 3, 4, 6, 7, 9, 13, 14\n[4] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun\nChen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Robust\nlidar-camera fusion for 3d object detection with transform-\ners. In CVPR, pages 1080\u20131089, 2022. 1, 2, 4, 5, 6, 7, 13,\n14\n[5] Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia,\nZhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi\nTang. Bevfusion: A simple and robust lidar-camera fusion\nframework. In Advances in Neural Information Processing\nSystems. 1, 2, 6, 7, 13, 14\n[6] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with unified bird\u2019s-eye view repre-\nsentation. arXiv preprint arXiv:2205.13542, 2022. 1, 2, 6, 7,\n14\n[7] Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun,\nand Jiaya Jia.\nUnifying voxel-based representation with\ntransformer for 3d object detection. 2022. 1, 2, 7\n[8] Yanwei Li, Xiaojuan Qi, Yukang Chen, Liwei Wang, Zeming\nLi, Jian Sun, and Jiaya Jia. Voxel field fusion for 3d object\ndetection. In CVPR, 2022. 1, 2\n[9] Yang Jiao, Zequn Jie, Shaoxiang Chen, Jingjing Chen, Lin\nMa, and Yu-Gang Jiang. Msmdfusion: A gated multi-scale\nlidar-camera fusion framework with multi-depth seeds for 3d\nobject detection. arXiv preprint arXiv:2209.03102, 2022. 1\n[10] Zeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian\nZhu, and Li Zhang. Deepinteraction: 3d object detection\nvia modality interaction. arXiv preprint arXiv:2208.11112,\n2022. 1, 6, 13, 14\n[11] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In CVPR, pages 770\u2013779, 2019. 1, 2\n[12] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast\npoint r-cnn. In ICCV, pages 9775\u20139784, 2019. 1, 2\n[13] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou,\nYanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards\nhigh performance voxel-based 3d object detection. In AAAI,\nvolume 35, pages 1201\u20131209, 2021. 1, 2\n[14] Zhichao Li, Feng Wang, and Naiyan Wang. Lidar r-cnn: An\nefficient and universal 3d object detector. CVPR, 2021. 1\n[15] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.\nGroup-free 3d object detection via transformers. In ICCV,\npages 2949\u20132958, 2021. 1\n[16] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving\ninto high quality object detection. In CVPR, pages 6154\u2013\n6162, 2018. 2\n[17] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection.\narXiv preprint\narXiv:2010.04159, 2020. 2, 5, 13\n[18] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,\nand Lei Zhang.\nDn-detr: Accelerate detr training by in-\ntroducing query denoising. In CVPR, pages 13619\u201313627,\n2022. 2, 5\n[19] Xingyi Zhou, Dequan Wang, and Philipp Kr\u00a8ahenb\u00a8uhl. Ob-\njects as points. In arXiv preprint arXiv:1904.07850, 2019. 2,\n4\n[20] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In ICCV, 2017. 2, 5\n[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In NeurIPS, 2015. 2\n[22] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. 2018. 2, 9\n[23] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. In CVPR, 2018.\n2\n[24] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection.\nIn\nCVPR, pages 10529\u201310538, 2020. 2, 7\n[25] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu\nGuo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-\nrcnn++: Point-voxel feature set abstraction with local vector\nrepresentation for 3d object detection. IJCV, pages 1\u201321,\n2022. 2, 7\n[26] Zixiang Zhou, Xiangchen Zhao, Yu Wang, Panqu Wang, and\nHassan Foroosh. Centerformer: Center-based transformer\nfor 3d object detection. In ECCV, 2022. 2\n[27] Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Ji-\naya Jia. Focal sparse convolutional networks for 3d object\ndetection. In CVPR, pages 5428\u20135437, 2022. 2, 6\n[28] Yukang Chen, Jianhui Liu, Xiaojuan Qi, Xiangyu Zhang,\nJian Sun, and Jiaya Jia. Scaling up kernels in 3d cnns. arXiv\npreprint arXiv:2206.10555, 2022. 2, 6\n[29] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and\nJiaya Jia. Voxelnext: Fully sparse voxelnet for 3d object de-\ntection and tracking. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, 2023.\n2\n[30] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho-\ngraphic feature transform for monocular 3d object detection.\nIn British Machine Vision Conference, 2019. 2, 14\n[31] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Dsgn:\nDeep stereo geometry network for 3d object detection. In\nCVPR, pages 12536\u201312545, 2020. 2\n[32] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding\nimages from arbitrary camera rigs by implicitly unprojecting\nto 3d. In ECCV, pages 194\u2013210. Springer, 2020. 2, 7, 14\n[33] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera\nimages via spatiotemporal transformers.\narXiv preprint\narXiv:2203.17270, 2022. 2, 14\n[34] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,\nGuan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-\nception and prediction in birds-eye-view for vision-centric\nautonomous driving.\narXiv preprint arXiv:2205.09743,\n2022. 2\n[35] Junjie Huang, Guan Huang, Zheng Zhu, Ye Yun, and Dalong\nDu. Bevdet: High-performance multi-camera 3d object de-\ntection in bird-eye-view. arXiv preprint arXiv:2112.11790,\n2021. 2\n[36] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\nPetr: Position embedding transformation for multi-view 3d\nobject detection. In Computer Vision\u2013ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXVII, pages 531\u2013548. Springer, 2022. 2\n[37] Yilun Chen, Shijia Huang, Shu Liu, Bei Yu, and Jiaya Jia.\nDsgn++: Exploiting visual-spatial relation for stereo-based\n3d detectors.\nIEEE Transactions on Pattern Analysis and\nMachine Intelligence, 2022. 2\n[38] AJ Piergiovanni, Vincent Casser, Michael S Ryoo, and\nAnelia Angelova. 4d-net for learned multi-modal alignment.\nIn ICCV, pages 15435\u201315445, 2021. 2\n[39] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Ji-\nquan Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny\nZhou, Quoc V Le, et al. Deepfusion: Lidar-camera deep fu-\nsion for multi-modal 3d object detection. In CVPR, pages\n17182\u201317191, 2022. 2\n[40] Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qin-\nhong Jiang, and Feng Zhao. Autoalignv2: Deformable fea-\nture aggregation for dynamic multi-modal 3d object detec-\ntion. ECCV, 2022. 2\n[41] Tianwei Yin, Xingyi Zhou, and Philipp Kr\u00a8ahenb\u00a8uhl. Multi-\nmodal virtual point 3d detection. NeurIPS, 34:16494\u201316507,\n2021. 2, 6\n[42] Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang,\nand Hang Zhao. Futr3d: A unified sensor fusion framework\nfor 3d detection. arXiv preprint arXiv:2203.10642, 2022. 2\n[43] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In CVPR, 2017. 2\n[44] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-\nsun. Multi-task multi-sensor fusion for 3d object detection.\nIn CVPR, pages 7345\u20137353, 2019. 2\n[45] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn ECCV, 2018. 2\n[46] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang,\nLewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng,\nHao Tian, et al. Delving into the devils of bird\u2019s-eye-view\nperception: A review, evaluation and recipe. arXiv preprint\narXiv:2209.05324, 2022. 2\n[47] Benjamin Graham, Martin Engelcke, and Laurens van der\nMaaten. 3d semantic segmentation with submanifold sparse\nconvolutional networks. 2018. 2\n[48] Guangsheng Shi, Ruifeng Li, and Chao Ma. Pillarnet: Real-\ntime and high-performance pillar-based 3d object detection.\nIn Computer Vision\u2013ECCV 2022: 17th European Confer-\nence, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings,\nPart X, pages 35\u201352. Springer, 2022. 2, 6, 7\n[49] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Si-\njia Chen, Li Huang, and Yuan Li. Afdet: Anchor free one\nstage 3d object detection. arXiv preprint arXiv:2006.12671,\n2020. 2\n[50] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia.\n3dssd:\nPoint-based 3d single stage object detector, 2020. 2\n[51] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya\nJia. Std: Sparse-to-dense 3d object detector for point cloud.\nIn ICCV, 2019. 2\n[52] Charles R Qi, Or Litany, Kaiming He, and Leonidas J\nGuibas. Deep hough voting for 3d object detection in point\nclouds. In ICCV, pages 9277\u20139286, 2019. 2\n[53] Charles R. Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. 2017. 2\n[54] Charles R. Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-\nnet++: Deep hierarchical feature learning on point sets in a\nmetric space. In NeurIPS, 2017. 2\n[55] Lue Fan, Ziqi Pang, Tianyuan Zhang, Yu-Xiong Wang, Hang\nZhao, Feng Wang, Naiyan Wang, and Zhaoxiang Zhang.\nEmbracing Single Stride 3D Object Detector with Sparse\nTransformer. In CVPR, 2022. 2, 7\n[56] Lue Fan, Feng Wang, Naiyan Wang, and Zhaoxiang\nZhang. Fully Sparse 3D Object Detection. arXiv preprint\narXiv:2207.10035, 2022. 2, 7\n[57] Chunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang.\nPointaugmenting: Cross-modal augmentation for 3d object\ndetection. In CVPR, pages 11794\u201311803, 2021. 2, 6\n[58] Ross Girshick. Fast r-cnn. In ICCV, 2015. 3\n[59] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, pages\n213\u2013229. Springer, 2020. 3, 5, 13\n[60] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In CVPR, 2012. 3\n[61] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In CVPR,\npages 2446\u20132454, 2020. 3, 6\n[62] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. nuscenes: A mul-\ntimodal dataset for autonomous driving.\nIn CVPR, pages\n11621\u201311631, 2020. 3, 5, 7\n[63] Kah-Kay Sung. Learning and example selection for object\nand pattern detection. 1996. 4\n[64] Abhinav Shrivastava, Abhinav Gupta, and Ross Girshick.\nTraining region-based object detectors with online hard ex-\nample mining. In CVPR, 2016. 4\n[65] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Dollar. Focal loss for dense object detection. In ICCV,\n2017. 4\n[66] Buyu Li, Yu Liu, and Xiaogang Wang. Gradient harmonized\nsingle-stage detector. In Proceedings of the AAAI confer-\nence on artificial intelligence, volume 33, pages 8577\u20138584,\n2019. 4\n[67] Yuhang Cao, Kai Chen, Chen Change Loy, and Dahua Lin.\nPrime sample attention in object detection. In CVPR, pages\n11583\u201311591, 2020. 4\n[68] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen.\nMobilenetv2: Inverted\nresiduals and linear bottlenecks. In CVPR, pages 4510\u20134520,\n2018. 4\n[69] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient\ndetr: improving end-to-end object detector with dense prior.\narXiv preprint arXiv:2104.01318, 2021. 5\n[70] Duy-Kien Nguyen, Jihong Ju, Olaf Booij, Martin R Oswald,\nand Cees GM Snoek. Boxer: Box-attention for 2d and 3d\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 4773\u2013\n4782, 2022. 5\n[71] Benjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and\nGang Yu. Class-balanced grouping and sampling for point\ncloud 3d object detection. arXiv preprint arXiv:1908.09492,\n2019. 6, 13\n[72] Dongqiangzi Ye, Zixiang Zhou, Weijia Chen, Yufei Xie,\nYu Wang, Panqu Wang, and Hassan Foroosh. Lidarmultinet:\nTowards a unified multi-task network for lidar perception.\narXiv preprint arXiv:2209.09385, 2022. 6, 13\n[73] Junho Koh, Junhyung Lee, Youngwoo Lee, Jaekyum Kim,\nand Jun Won Choi.\nMgtanet:\nEncoding sequential li-\ndar points using long short-term motion-guided tempo-\nral attention for 3d object detection.\narXiv preprint\narXiv:2212.00442, 2022. 6\n[74] Sourabh Vora, Alex H Lang, Bassam Helou, and Oscar Bei-\njbom. Pointpainting: Sequential fusion for 3d object detec-\ntion. In CVPR, pages 4604\u20134612, 2020. 6\n[75] Jin Hyeok Yoo, Yecheol Kim, Jisong Kim, and Jun Won\nChoi.\n3d-cvf: Generating joint camera and lidar features\nusing cross-view spatial feature fusion for 3d object detec-\ntion. In European Conference on Computer Vision, pages\n720\u2013736. Springer, 2020. 6\n[76] Shaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin,\nand Liangjun Zhang.\nFusionpainting: Multimodal fusion\nwith adaptive attention for 3d object detection. In 2021 IEEE\nInternational Intelligent Transportation Systems Conference\n(ITSC), pages 3047\u20133054. IEEE, 2021. 6\n[77] Junjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li,\nTiancai Wang, and Xiangyu Zhang. Cross modal transformer\nvia coordinates encoding for 3d object dectection.\narXiv\npreprint arXiv:2301.01283, 2023. 5\n[78] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel Ni, and Harry Shum. Dino: Detr with improved\ndenoising anchor boxes for end-to-end object detection. In\nICLR, 2022. 5\n[79] MMDetection3D Contributors.\nMMDetection3D: Open-\nMMLab next-generation platform for general 3D ob-\nject detection. https://github.com/open-mmlab/\nmmdetection3d, 2020. 6\n[80] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755. Springer, 2014. 7\n[81] Ziqi Pang, Zhichao Li, and Naiyan Wang. Simpletrack: Un-\nderstanding and rethinking 3d multi-object tracking. arXiv\npreprint arXiv:2111.09621, 2021. 7\n[82] Xinshuo Weng, Jianren Wang, David Held, and Kris Kitani.\n3d multi-object tracking: A baseline and new evaluation met-\nrics. In IROS, pages 10359\u201310366, 2020. 7\n[83] Nuri Benbarka, Jona Schr\u00a8oder, and Andreas Zell. Score re-\nfinement for confidence-based 3d multi-object tracking. In\nIROS, pages 8083\u20138090, 2021. 7\n[84] Jan-Nico Zaech, Dengxin Dai, Alexander Liniger, Mar-\ntin Danelljan, and Luc Van Gool.\nLearnable online\ngraph representations for 3d multi-object tracking. CoRR,\nabs/2104.11747, 2021. 7\n[85] Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed,\nAlex Bewley, Xiao Zhang, Cristian Sminchisescu, and\nDragomir Anguelov. Rsn: Range sparse net for efficient, ac-\ncurate lidar 3d object detection. In CVPR, pages 5725\u20135734,\n2021. 7\n[86] Yihan Hu, Zhuangzhuang Ding, Runzhou Ge, Wenxin Shao,\nLi Huang, Kun Li, and Qiang Liu. Afdetv2: Rethinking the\nnecessity of the second stage for object detection from point\nclouds. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 969\u2013979, 2022. 7\n[87] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang\nGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-\nsudevan. End-to-end multi-view fusion for 3d object detec-\ntion in lidar point clouds. In Conference on Robot Learning,\npages 923\u2013932. PMLR, 2020. 9\nAppendix for FocalFormer3D\nThe supplementary materials for FocalFormer3D is or-\nganized as follows:\n\u2022 Sec. A shows additional ablation studies on decoder\nhead and latency analysis for multi-modal models.\n\u2022 Sec. B gives more implementation details including\nnetwork details, and extension to the multi-modal vari-\nant.\n\u2022 Sec. C discusses the prediction locality for second-\nstage refinements.\n\u2022 Sec. D presents some visual results for multi-stage\nheatmaps and 3D detection results on bird\u2019s eye view.\nA. Additional Ablation Studies\nDesign of the decoder head. We analyze the capability\nof the decoder head in processing massive queries in Ta-\nble 8. Concerning the type of cross attention, with an in-\ncreasing number of queries up to 600, the computation time\nof cross attention module [59] (c) grows faster than de-\nformable one [17] (e). As a result, more deformable trans-\nformer layers can be applied. In our experiments, the trans-\nformer decoder head with 6 layers obtains the best perfor-\nmance (66.5 mAP and 71.1 NDS) with a more affordable\ncomputation time than the cross attention modules. Fur-\nthermore, compared with point-level query embedding [4]\n(g), our box-level query embedding (f) achieves +0.6 points\nimprovements with 3.7ms computation overhead, demon-\nstrating the effectiveness of box-level query.\n#\nC.A.\n#Q\n#Layer\nmAP\nNDS\nLatency\n(a)\nFull\n200\n1\n65.8\n70.5\n7.6ms\n(b)\nFull\n600\n1\n66.1\n70.9\n13.1ms\n(c)\nFull\n600\n2\n66.3\n71.1\n26.2ms\n(d)\nDeform\n200\n6\n65.9\n70.8\n14.8ms\n(e)\nDeform\n600\n2\n66.2\n70.7\n7.6ms\n(f)\nDeform\n600\n6\n66.5\n71.1\n17.0ms\n(g)\nw/o Box-pooling\n65.9\n70.9\n\u2013\nTable 8. Ablation studies for box-level deformable decoder\nhead. \u201cC.A.\u201d denotes the types of cross attention layers. \u201c# Q\u201d\nrepresents the number of used queries. \u201c# Layer\u201d stands for the\nnumber of decoder layers. Latency is measured for the transformer\ndecoder head on a V100 GPU for reference.\nLatency analysis. We compare ours with other leading-\nperformance\nmethods\nin\nTable\n9.\nIt\nshows\nthat\nFocalFormer-F outperforms the dominating methods, BEV-\nFusion [5] and DeepInteraction [10] in terms of both per-\nformance and efficiency.\nMethods\nmAP\nNDS\nLatency\nBEVFusion [5]\n69.2\n71.8\n1610ms\nDeepInteraction [10]\n70.8\n73.4\n480ms\nFocalFormer3D-F (Ours)\n71.6\n73.9\n363ms\nTable 9. Efficiency comparison with other SOTA methods on\nnuScenes dataset. Results are shown on nuScenes test set. All\nmethods are tested on a single V100 GPU for reference.\nResults on nuScenes val set. We also report the method\ncomparisons on the nuScenes val set in Table 10.\nMethods\nmAP\nNDS\nCBGS [71]\n51.4\n62.6\nCenterPoint [3]\n59.6\n66.8\nLiDARMultiNet [72]\n63.8\n69.5\nTransFusion-L\u2227 [4]\n65.2\n70.2\nFocalFormer3D (Ours)\n66.5\n71.1\nTable 10. Performance comparison on the nuScenes val set. Re-\nsults marked with \u2227 indicate our reproduction. The results of other\ncompared methods on the nuScenes val set were obtained from\ntheir respective original papers.\nB. Additional Implementation Details\nModel details for nuScenes dataset.\nOn the nuScenes\ndataset, the voxel size is set as 0.075m \u00d7 0.075m \u00d7 0.2m\nand the detection range is set to [\u221254.0m, 54.0m] along X\nand Y axes, and [\u22125.0m, 3.0m] along Z axis.We follow\nthe common practice of accumulating the past 9 frames to\nthe current frame for both training and validation. We train\nthe LiDAR backbone with the deformable transformer de-\ncoder head for 20 epochs. Then, we freeze the pre-trained\nLiDAR backbones and train the detection head with multi-\nstage focal heatmaps for another 6 epochs. GT sample aug-\nmentation is adopted except for the last 5 epochs. We adopt\npooling-based masking for generating Accumulated Posi-\ntive Mask, where we simply select Pedestrian and Traffic\nCones as the small objects.\nModel details for Waymo dataset. On the Waymo dataset,\nwe simply keep the VoxelNet backbone and FocalFormer3D\ndetector head the same as those used for the nuScenes\ndataset. The voxel size used for the Waymo dataset is set\nto 0.1m \u00d7 0.1m \u00d7 0.15m. For the multi-stage heatmap\nencoder, we use pooling-based masking, selecting Vehicle\nas the large object category, and Pedestrain and Cyclist as\nthe small object categories. The training process involves\ntwo stages, with the model trained for 36 epochs and an-\nother 11 epochs trained for the FocalFormer3D detector. We\nadopt GT sample augmentation during training, except for\nthe last 6 epochs. As the Waymo dataset provides denser\npoint clouds than nuScenes, the models adopt single-frame\npoint cloud input [3, 4].\nExtension to multi-modal fusion model. We provide more\ndetails on the extension of FocalFormer3D to its multi-\nmodal variant. Specifically, the image backbone network\nutilized is ResNet-50 following TransFusion [4]. Rather\nthan using more heavy camera projection techniques such\nas Lift-split-shot [32] or BEVFormer [33], we project multi-\nview camera features onto a predefined voxel grid in the\n3D space [30]. The BEV size of the voxel grid is set to\n180\u00d7180, in line with 8\u00d7 downsampled BEV features pro-\nduced by VoxelNet [1]. The height of the voxel grid is fixed\nat 10.\nTo obtain camera features for BEV LiDAR feature, we\nadopt a cross-attention module [10] within each pillar. This\nmodule views each BEV pixel feature as the query and the\nprojected camera grid features as both the key and value.\nThe generated camera BEV features are then fused with Li-\nDAR BEV features by an extra convolutional layer. This\nmulti-modal fusion is conducted at each stage for the multi-\nstage heatmap encoder. We leave the exploration of stronger\nfusion techniques [5, 6, 10] as future work.\nC. Prediction Locality of Second-Stage Refine-\nment\nRecent 3D detectors have implemented global attention\nmodules [4] or fusion with multi-view camera [5, 10] to\ncapture larger context information and improve the detec-\ntion accuracy. However, we observe a limited regression\nrange (named as prediction locality) compared to the ini-\ntial heatmap prediction. To analyze their second-stage abil-\nity to compensate for the missing detection (false nega-\ntives), we visualize the distribution of their predicted cen-\nter shifts \u03b4 = (\u03b4x, \u03b4y) in Fig. 7 for several recent lead-\ning 3D detectors, including the LiDAR detectors (Center-\nPoint [3], TransFusion-L [4]) and multi-modal detectors\n(BEVFusion [5], DeepInteraction [10]). Statistics of cen-\nter shift (\u03c3\u03b4 < 0.283m illustrate almost all predictions\nare strongly correlated with their initial positions (gener-\nally less than 2 meters away), especially for LiDAR-only\ndetectors, such as CenterPoint and TransFusion-L.\nThe disparity between small object sizes (usually <\n5m\u00d75m) and extensive detection range (over 100m\u00d7100m\nmeters) limits the efficacy of long-range second-stage re-\nfinement, despite the introduction of global operations and\nperspective camera information. Achieving a balance be-\ntween long-range modeling and computation efficiency for\nBEV detection is crucial. FocalFormer3D, as the pioneer\nin identifying false negatives on the BEV heatmap followed\nby local-scope rescoring, may provide insights for future\nnetwork design.\nCenterPoint\nTransFuion-L\nDeepInteraction\nBEVFusion\nFigure 7. Object center shifts (\u03b4x, \u03b4y) distribution without normal-\nization between initial heatmap response and final object predic-\ntions. The unit is a meter.\nD. Example Visualization\nExample visualization of multi-stage heatmaps and\nmasking. We present a visual illustration of the multi-stage\nheatmap encoder process in Fig. 8.\nQualitative results. Fig. 9 shows some visual results and\nfailure cases of FocalFormer3D on the bird\u2019s eye view. Al-\nthough the average recall AR<1.0m reaches over 80%, some\nfalse negatives are still present due to either large occlusion\nor insufficient points. Also, despite accurate center predic-\ntion, false negatives can arise due to incorrect box orien-\ntation. Further exploration of a strong box refinement net-\nwork is left for future work.\nGT Heatmap (Stage 1)\nGT Heatmap (Stage 2)\nGT Heatmap (Stage 3)\nPredicted Heatmap (Stage 1)\nPredicted Heatmap (Stage 2)\ntrain\ntrain\nmask\nmask\nFN predictions\nCorrected FN\nMissed detection\nGT Heatmap (Stage 1)\nGT Heatmap (Stage 2)\nGT Heatmap (Stage 3)\nPredicted Heatmap (Stage 1)\nPredicted Heatmap (Stage 2)\ntrain\ntrain\nmask\nmask\nFN predictions\nCorrected FN\nMissed detection\nMissed detection\nExample 1\nExample 2\nFigure 8. Example visualization of multi-stage heatmap encoder process on the bird\u2019s eye view.\nThe process of identifying\nfalse negatives operates stage by stage.\nWe show different categories with different colors for visualization.\nThe top three sub-\nfigures display the ground-truth center heatmaps at each stage, highlighting the missed object detections.\nThe two subfigures be-\nlow display the positive mask that shows positive object predictions. The scene ids are \u201d4de831d46edf46d084ac2cecf682b11a\u201d and\n\u201d825a9083e9fc466ca6fdb4bb75a95449\u201d from the nuScenes val set. We recommend zooming in on the figure for best viewing.\nWrong Orientation\nMissed Detection\nFalse Positive\n(a)\n(b)\n(c)\n(d)\nFalse Positive\nMissed Detection\nFigure 9. Visual results and failure cases. The green boxes represent the ground truth objects and the blue ones stand for our predictions.\nWe recommend zooming in on the figure for best viewing.\n"
  }
]