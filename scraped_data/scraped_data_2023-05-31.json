[
  {
    "title": "HiFA: High-fidelity Text-to-3D with Advanced Diffusion Guidance",
    "link": "https://arxiv.org/pdf/2305.18766.pdf",
    "upvote": "4",
    "text": "Published as a conference paper at ICLR 2024\nHIFA:\nHIGH-FIDELITY\nTEXT-TO-3D\nGENERATION\nWITH ADVANCED DIFFUSION GUIDANCE\nJunzhe Zhu\u22171, Peiye Zhuang\u22171,2, Sanmi Koyejo1\n1Stanford University,\n2Snap Inc.\n1{josefzhu, peiye, sanmi}@stanford.edu, 2pzhuang@snapchat.com\nABSTRACT\nThe advancements in automatic text-to-3D generation have been remarkable.\nMost existing methods use pre-trained text-to-image diffusion models to opti-\nmize 3D representations like Neural Radiance Fields (NeRFs) via latent-space\ndenoising score matching. Yet, these methods often result in artifacts and incon-\nsistencies across different views due to their suboptimal optimization approaches\nand limited understanding of 3D geometry. Moreover, the inherent constraints of\nNeRFs in rendering crisp geometry and stable textures usually lead to a two-stage\noptimization to attain high-resolution details. This work proposes holistic sam-\npling and smoothing approaches to achieve high-quality text-to-3D generation,\nall in a single-stage optimization. We compute denoising scores in the text-to-\nimage diffusion model\u2019s latent and image spaces. Instead of randomly sampling\ntimesteps (also referred to as noise levels in denoising score matching), we intro-\nduce a novel timestep annealing approach that progressively reduces the sampled\ntimestep throughout optimization. To generate high-quality renderings in a single-\nstage optimization, we propose regularization for the variance of z-coordinates\nalong NeRF rays. To address texture flickering issues in NeRFs, we introduce a\nkernel smoothing technique that refines importance sampling weights coarse-to-\nfine, ensuring accurate and thorough sampling in high-density regions. Extensive\nexperiments demonstrate the superiority of our method over previous approaches,\nenabling the generation of highly detailed and view-consistent 3D assets through\na single-stage training process.\n1\nINTRODUCTION\nThe task of automatic text-to-3D generation aims to create 3D assets based on a text description\nand has gained significant attention due to its wide-ranging applications in digital content genera-\ntion, film-making, and Virtual Reality (VR) (Lin et al., 2023; Chen et al., 2023b). Initial efforts in\nthis domain centered on unconditional 3D asset generation, experimenting with various 3D repre-\nsentation modalities presented in explicit formats such as meshes (Achlioptas et al., 2018; Luo &\nHu, 2021; Smith & Meger, 2017; Xie et al., 2018), as well as implicit formats such as fields (Chen\n& Zhang, 2019; Mittal et al., 2022; Zhuang et al., 2023). Following this, the field has progressed\ntowards conditional 3D generative models, e.g., with text-based guidance (Cheng et al., 2023). How-\never, these studies have been limited to relatively simple 3D assets, primarily due to the scarcity of\nlarge-scale annotated 3D datasets.\nThe availability of ample image datasets and the success of text-to-image generation have paved the\nway for lifting pre-trained text-to-image models to the 3D domain. Specifically, recent studies focus\non optimizing a 3D representation for an asset, using pre-trained text-to-image generative models by\nproviding a denoising score for rendered images (Khalid et al., 2022; Jain et al., 2022; Poole et al.,\n2022; Xu et al., 2022; Wang et al., 2023a; Lin et al., 2023; Tang et al., 2023; Chen et al., 2023b;\nWang et al., 2023b). Poole et al. (2022) proposed a loss from the distillation of a text-to-image\ndiffusion model. They minimized the Kullback-Leibler (KL) divergence between a family of Gaus-\nsian distributions based on the forward diffusion process and the denoising scores acquired from\nthe pre-trained text-to-image diffusion model. The proposed Score Distillation Sampling (SDS)\nmethod combined with a NeRF enables 3D asset generation from given text prompts. Subsequent\n\u2217Equal contribution\n1\narXiv:2305.18766v4  [cs.CV]  11 Mar 2024\nPublished as a conference paper at ICLR 2024\n``A sea turtle\u201d\n``A tarantula, highly detailed\u201d\n``A parrot\u201d\n``A pair of pink fluffy slippers\u201d\n``A wooden buddha head\u201d\n``Robotic bee, high detail\u201d\n``A bagel filled with cream\n cheese and lox\u201d\n``Dragon wings and unicorn \nhead hybrid creature\u201d\n``A car made out of sushi\u201d\nFigure 1: Examples of multiple views of 3D objects generated by from our model given text\nprompts (below each object).\nresearch has improved generation quality through various approaches including the adoption of two-\nstage optimization frameworks (Lin et al., 2023; Tang et al., 2023; Wang et al., 2023b; Chen et al.,\n2023c), alterations to the original SDS formulation (Wang et al., 2023a;b), and the disentanglement\nof geometry and appearance (Chen et al., 2023b).\nIn this work, we revisit the integration of the SDS approach with NeRFs, aiming to achieve photo-\nrealistic and high-quality text-to-3D generation through a single-stage optimization. In contrast to\nexisting text-to-3D generation work, we distill the score in the text-to-image diffusion model\u2019s la-\ntent and image spaces for enhanced supervision. Moreover, we observe that the efficacy of the\ndiffusion prior is limited in previous works (Poole et al., 2022; Lin et al., 2023) when timesteps\n(also referred to as noise levels in denoising score matching) are randomly sampled during opti-\nmization. Specifically, we observe that toward the end of the training process, the NeRF becomes\nalmost determined in representing a particular 3D asset. Thus, we find that randomly sampling a\nlarge timestep drives the diffusion model to produce a denoised image that is distinct and unrelated\nto the original input rendering. This yields inconsistent distillation from the diffusion model and\ncompromised optimization of NeRFs. To address this, we introduce a timestep annealing approach\nwhere the timestep in the forward diffusion process inversely correlates with the square root of the\nnumber of training iterations. Our empirical analysis demonstrates that the proposed timestep an-\nnealing approach effectively enhances generation quality. We also show that the square root timestep\nannealing consistently outperforms other annealing methods, such as linear and cosine ratios.\nMoreover, generating a detailed 3D asset through single-stage optimization is challenging. Specif-\nically, explicit 3D representations, such as meshes, struggle to capture intricate topology, such as\nthose with holes. Implicit 3D representations (Mildenhall et al., 2020; M\u00a8uller et al., 2022) may lead\nto cloudy geometry and flickering textures. For instance, when NeRFs are employed to represent\nhighly detailed 3D geometries like human bodies (Hong et al., 2023), Moir\u00b4e patterns are notica-\nble. To this end, prior works (Lin et al., 2023; Wang et al., 2023b) adopted two-stage optimization\ntechniques. In these approaches, explicit 3D representations, such as Deep Marching Tetrahedra\n(DMTet) (Shen et al., 2021), are used to extract textured meshes from the implicit representations in\n2\nPublished as a conference paper at ICLR 2024\nthe first stage and are subsequently fine-tuned in the second stage to capture high-quality geometry.\nHowever, these mesh representations forfeit the ability to produce detailed appearances such as fur \u2013\na tradeoff we wish to avoid. Differently, we aim to maintain the flexibility and photo-realism offered\nby the NeRF representation while at the same time achieving high-quality text-to-3D generation\nthrough a single-stage training.\nTo this end, we propose two techniques to advance NeRF optimization. Specifically, to address the\ncloudy geometry issue in NeRFs, we propose a variance regularization that minimizes the variance of\nsampled z-coordinates distributed along NeRF rays. We observe that this technique enables NeRFs\nto more accurately represent crisp geometrical surfaces, thereby effectively mitigating the cloudiness\nissue. Additionally, we verify that our proposed z-variance regularization outperforms alternative\nspatial regularization proposed in previous methods (Barron et al., 2022).\nMoreover, texture flickering or shimmer effects often result from inaccuracies in estimating the im-\nportance sampling weights across different rendering views. However, existing solutions, such as\nincreasing the number of sampling points along the rays or deploying separate density estimation\nnetworks for each coarse and refined stage, come with increased computational demands. Instead,\nwe propose a kernel smoothing technique tailored for coarse-to-fine importance sampling along\nNeRF rays without an increase in the computational cost. This technique is inspired by the inte-\ngrated positional encoding for spatial points within a cone, previously proposed to tackle aliasing\nissues in a single image view (Barron et al., 2021). In our case, the goal is to mitigate flickering\nissues across multiple views. Specifically, we use a kernel to refine the probability density function\n(PDF) estimated in the coarse sampling stage along a ray, which enables more comprehensive sam-\npling near asset surface regions in the refined stage. This technique notably enhances the fidelity of\nimportance sampling.\nWe summarize our technical contributions for two crucial components of text-to-3D generation: (1)\n3D representation and (2) optimization, which are outlined below:\n\u2022 To achieve photo-realistic and highly-detailed text-to-3D generation, we propose score\ndistillation in both the latent and image space of the pre-trained text-to-image diffusion\nmodels. Moreover, we introduce a timestep annealing approach for score distillation from\ntext-to-image diffusion models.\n\u2022 To achieve sharp geometry quality through a single-stage training, we present a regulariza-\ntion method applied to the variance of z-coordinates along NeRF rays.\n\u2022 To address flickering issues in NeRFs, we propose a kernel smoothing technique that refines\nthe PDF estimation in coarse-to-fine importance sampling.\nTaken together, we show how these holistic modifications address existing shortcomings and im-\nprove the quality of 3D synthesis.\n2\nRELATED WORK\nUnconditional 3D asset generation involves the learning of 3D asset data distributions. Explicit ap-\nproaches employ representations including point clouds (Achlioptas et al., 2018; Luo & Hu, 2021),\nvoxel grids (Lin et al., 2022; Smith & Meger, 2017; Xie et al., 2018) and meshes (Zhang et al., 2021).\nIn contrast, implicit methods utilize representations such as signed distance functions (SDFs) (Chen\n& Zhang, 2019; Cheng et al., 2023; Mittal et al., 2022), tri-planes (Chen et al., 2023a), multi-layer\nperceptron (MLP) weights (Erkoc\u00b8 et al., 2023), and radiance fields (Lorraine et al., 2023). How-\never, due to the limited availability of diverse 3D assets, these works primarily focus on generating\nclass-specific and small-scale 3D datasets.\nText-to-3D asset generation refers to the creation of 3D assets based on text descriptions. Instead\nof depending on limited text-annotated 3D datasets, the availability of ample text-image data pairs\nand the success of text-to-image generative models have inspired recent research to lift pre-trained\ntext-to-image models into the 3D domain. Generally, these approaches can be categorized into two\ngroups: (i) CLIP-guided text-to-3D approaches (Khalid et al., 2022; Jain et al., 2022) that utilize\npre-trained cross-modal matching models like CLIP (Radford et al., 2021), and (ii) 2D diffusion-\nguided text-to-3D approaches (Poole et al., 2022; Tang et al., 2023; Lin et al., 2023; Chen et al.,\n2023b) that rely on text-to-image diffusion-based generative models such as Imagen (Saharia et al.,\n2022) and StableDiffusion (Rombach et al., 2022). We follow the diffusion-guided methods due to\ntheir superior performance in text-to-3D generation.\n3\nPublished as a conference paper at ICLR 2024\nSpecifically, Poole et al. (2022) first introduced a Score Distillation Sampling (SDS) approach,\nwhere noise is added to an image rendered from NeRFs and subsequently denoised by a pre-trained\ntext-to-image generative model (Saharia et al., 2022). SDS minimizes the KL divergence between\na prior Gaussian noise distribution and the estimated noise distribution. SDS is widely adopted\nin follow-up works (Lin et al., 2023; Tang et al., 2023; Chen et al., 2023b). For example, Score-\nJacobian-Chaining (Wang et al., 2023a) proposed a Perturb-and-Average Scoring method to aggre-\ngate 2D image gradients of StableDiffusion (Rombach et al., 2022) over multiple viewpoints into a\n3D asset gradient. Wang et al. (2023b) introduced a variational formulation of the SDS approach for\ndiverse generation of 3D assets, yet it needs to train a low-rank adaptation (LoRA) (Hu et al., 2022)\nfor each individual 3D asset to provide the score function of the distribution. Moreover, two-stage\noptimization frameworks are proposed (Wang et al., 2023b; Lin et al., 2023) that initially extract 3D\nmeshes from implicit representations and then fine-tune them in the second stage to achieve high-\nresolution details. In this work, we propose refining of the SDS approach and improving the implicit\nrepresentation to achieve high-quality 3D asset generation in a single-stage optimization process.\nImage-to-3D reconstruction refers to 3D reconstruction from a provided single image. Typically,\nas proposed in prior works (Zhou & Tulsiani, 2023; Gu et al., 2023; Liu et al., 2023b;a), pre-trained\ntext-to-image diffusion-based models are used to provide a 2D prior via the SDS approach plus an\nimage reconstruction loss. In a two-stage optimization process, Qian et al. (2023) employs 2D and\n3D diffusion priors. In the first stage, they optimize a NeRF representation, and in the second stage,\nthey extract a DMTet mesh from the NeRF for fine-tuning. Haque et al. (2023) propose an iterative\ndataset update strategy for editing NeRFs, leveraging text-to-image diffusion models. Alternatively,\nLiu et al. (2023c) hallucinates 16 normal view images and directly optimizes a NeRF representation\nbased on them. We extend our work on the image-to-3D reconstruction task and compare to these\nmethods in Sec. 5.4.\n3\nPRELIMINARIES: SCORE DISTILLATION SAMPLING (SDS)\nThe SDS approach in diffusion models is proposed in recent work (Poole et al., 2022) using a pre-\ntrained text-to-image diffusion model to guide the 3D representation parameterized by \u03b8. An image\nx is generated based on a given camera pose via a differentiable rendering function g, denoted\nas x = g(\u03b8). The pre-trained text-to-image diffusion model is employed to ensure the rendered\nimages align with its learned image distribution. This work uses a latent diffusion model to reduce\ncomputational complexity.\nSpecifically, a latent diffusion model such as Stable Diffusion (SD) (Rombach et al., 2022), consists\nof an encoder E, a decoder D, and a denoising function \u03f5\u03d5, parameterized by \u03d5. The encoder E\ncompresses the input image x into a low-resolution latent vector z, written as z = E(x). Conversely,\nthe decoder D reconstructs the image from the latent vector as x = D(z). The denoising score\nfunction \u03f5\u03d5 estimates the given noise as \u02c6\u03f5 := \u03f5\u03d5(zt; y, t), where zt is a noisy latent vector, formally\nwritten as zt = \u03b1tz+\u03c3t\u03f5. Here, \u03b1t and \u03c3t define a schedule for adding Gaussian noise \u03f5 \u223c N(0, I)\nto the latent vector z given a text embedding y at timestep t. Subsequently, the SDS loss is used to\nprovide gradients for optimizing the 3D representation \u03b8, written as\n\u2207\u03b8LSDS(\u03d5, z) =\nEt,\u03f5[\u03c9(t)(\u02c6\u03f5 \u2212 \u03f5)\u2202z\n\u2202\u03b8 ],\n(1)\nwhere \u03c9(t) is a weighting function.\n4\nAPPROACH\nWe aim to generate high-quality 3D assets in a single-stage approach driven by text prompts. For\nthis, we propose our method as illustrated in Fig. 2. We present our technical contributions in two\nparts. In Sec. 4.1, we compute the SDS loss in both the latent and image spaces of the pre-trained\nSD model (Rombach et al., 2022). Unlike previous works (Poole et al., 2022; Lin et al., 2023; Chen\net al., 2023b), we propose a simple yet effective timestep annealing approach that gradually reduces\ntimesteps (noise levels) throughout the optimization process. In Sec. 4.2, we introduce our variance\nregularization loss for z-coordinates along NeRF rays. Additionally, we present a kernel smoothing\ntechnique for importance sampling, enabling NeRFs to produce crisp geometry and maintain a view-\nconsistent appearance.\n4.1\nADVANCING SDS-BASED OPTIMIZATION\nAugmenting the SDS approach in both the image and latent spaces. In our work, we employ a\npre-trained SD model (Rombach et al., 2022) to optimize NeRFs. We extend the score distillation\n4\nPublished as a conference paper at ICLR 2024\n\u2130\n\ud835\udc9f\n\ud835\udf50~\ud835\udca9(\ud835\udfce, \ud835\udc70)\n\ud835\udc9b\ud835\udc95\n\ud835\udc9b\n*\ud835\udc9b = *\ud835\udc9b\ud835\udfce\n\ud835\udc99\n\u2112#$#%\n\ud835\udc54(\ud835\udf03)\n0\ud835\udc99\n``Game asset of a \nleather shoe made \nfrom dragon skin\u201d\nLatent diffusion prior\nEncoder\nDecoder\nDenoising\nstep\nBack\npropagation\nFigure 2: Overview of our proposed method for text-to-3D generation. We aim to optimize a\n3D model g(\u03b8) using a pre-trained 2D latent diffusion prior . To achieve this, we employ a latent\ndiffusion model for score distillation. Specifically, the diffusion model takes a rendered image x as\ninput and provides the estimate of the input rendered image, denoted as \u02c6x. We utilize LSDS+ loss\nthat computes reconstruction loss in both the latent and image spaces.\nto both the latent and image spaces of the SD model. For this, we first reformulate the original SDS\nloss (as described in Eq. 1) in terms of the latent vector residual instead of the noise residual:\n\u2207\u03b8LSDS(\u03d5, z) = Et,\u03f5\n\u0014\n\u03c9(t)(\u02c6\u03f5 \u2212 \u03f5)\u2202z\n\u2202\u03b8\n\u0015\n= Et,\u03f5\n\u0014\n\u03c9(t)\n\u0012 1\n\u03c3t\n(zt \u2212 \u03b1t\u02c6z) \u2212 1\n\u03c3t\n(zt \u2212 \u03b1tz)\n\u0013 \u2202z\n\u2202\u03b8\n\u0015\n= Et,\u03f5\n\u0014\n\u03c9(t)\u03b1t\n\u03c3t\n(z \u2212 \u02c6z)\u2202z\n\u2202\u03b8\n\u0015\n,\n(2)\nwhere \u02c6z :=\n1\n\u03b1t (zt \u2212 \u03c3t\u02c6\u03f5) represents the estimate of the latent vector z using the denoising function\n\u03f5\u03d5, and (\u02c6z\u2212z) is referred to as the latent vector residual. Note that due to the difficulty of recovering\nan explicit loss formulation LSDS for the gradients in Eq. 1, Poole et al. (2022), directly compute\nthe gradients to optimize 3D representations. In contrast, our reparameterization of the gradients as\nshown in Eq. 2 allows us to explicitly formulate the LSDS loss, thus simplifying the loss visualization\nand analysis process. Formally, we have\nLSDS(\u03d5, z) =\nEt,\u03f5 \u03c9(t)\u2225z \u2212 \u02c6z\u22252,\n(3)\nwhere we incorporate those coefficients related to t into \u03c9(t).\nSubsequently, we further adapt the loss by incorporating supervision for high-resolution images.\nFormally, we define the adapted loss LSDS+ as\nLSDS+(\u03d5, z, x) =\nEt,\u03f5 \u03c9(t)\n\u0002\n\u2225z \u2212 \u02c6z\u22252 + \u03bbrgb\u2225x \u2212 \u02c6x\u22252\u0003\n,\n(4)\nwhere \u02c6x is an recovered image obtained through the decoder D, formally denoted as \u02c6x = D(\u02c6z) and\n\u03bbrgb is a scaling parameter. We note that a similar image reconstruction loss is employed in recent\nimage-to-3D reconstruction works (Zhou & Tulsiani, 2023). Our approach is different in two ways.\nFirst, we observe that it is inadequate only to use the image residual, i.e., \u2225x \u2212 \u02c6x\u22252, without the\nincorporation of the latent residual \u2225z \u2212 \u02c6z\u22252, resulting in color bias issues in text-to-3D generation.\nWe will present the ablation experiments in Sec. 5.2. Second, using a random timestep sampling\napproach in previous works (Zhou & Tulsiani, 2023; Poole et al., 2022; Lin et al., 2023) during\nthe denoising process limits the guidance of text-to-image diffusion models. In the following, we\nanalyze this in detail and introduce a novel timestep annealing approach designed to enhance the\nSDS performance.\nA timestep annealing approach offers a more effective alternative to random timestep sampling\nused in previous works (Poole et al., 2022; Lin et al., 2023). To be concrete, our observations\nsuggest that random sampling can introduce divergence issues in the denoised images. As training\nnears completion, a NeRF renders images representing an almost determined 3D asset. In this case,\n5\nPublished as a conference paper at ICLR 2024\n(a) Coarse\n(b) Refined (w/o and w/ KS)\n(c) w/o KS (d) Flickering\n(e) w/ KS\nFigure 3: Visualization of the flickering issue. We display the sampled z-coordinates along the\nray for the rendered pixel (marked in the red square in (c) and (e)). Specifically, In (a), the ground-\ntruth PDF is shown in blue, while the estimated PDF is shown without (yellow) and with (green)\nthe kernel smoothing (KS) approach. In (b) we show the sampled z-coordinates in the refined stage,\nwithout KS (left) or with KS (right). Their corresponding rendered image is presented in (c) and (e),\nrespectively. In (d), we overlay the difference of the two renderings (i.e. the flickering) on (c).\nif a large timestep t is randomly sampled, the denoising function might predict an image that is\ndistinct and unrelated to the given input rendering. This can produce inaccurate gradients from the\ndiffusion model, thereby negatively impacting the optimization of the 3D model.\nTo circumvent this issue, we propose a timestep annealing approach. Specifically, we use a high\nvalue of timestep t for the rendered image during the initial training iterations. This intentional\nnoise allows the image to align more closely with the distribution characterized by the text-to-image\ndiffusion prior. As training proceeds, we gradually reduce the timestep t, thereby capturing finer\ndetails through more stable and lower variance gradients.\nA question follows: what is the suitable annealing rate? We investigated several options, including\nlinear, cosine, and square root schedules. Empirical evaluation (details in Sec. 5) suggests that square\nroot scheduling yields superior results in our scenario, formally written as\nt =\ntmax \u2212 (tmax \u2212 tmin)\nr\niter\ntotal iter,\n(5)\nwhere timestep t decreases steeply during the initial training process and decelerates as the training\nprogresses. This scheduling allocates more training iterations to lower values of timestep t, ensuring\nthat fine-grained details are sufficiently captured in the latter iterations of training.\n4.2\nADVANCING REGULARIZATION IN NERF REPRESENTATION\nWe introduce two techniques to improve NeRF representations, including a regularization method\nfor the variance of z-coordinates (a.k.a. z-variance) sampled along NeRF rays and a novel kernel\nsmoothing approach for importance sampling during rendering.\nA NeRF renders a pixel color \u02c6Cr of an image, denoted as \u02c6Cr = PN\ni=1 \u03bdici, where \u03bdi and ci are\nrespectively estimated weights and colors of the sampled coordinate zi along a ray r (Mildenhall\net al., 2020). N refers to the number of sampled points along the ray r. Accordingly, the depth value\n\u00b5zr and the disparity value dzr of the ray r are written as\n\u00b5zr =\nX\ni\nzi\n\u03bdi\nP\ni \u03bdi\n, and dzr =\n1\n\u00b5zr\n,\n(6)\nwhere\n\u03bdi\nP\ni \u03bdi can be considered as a sampled PDF.\nRegularization for z-variance aims to minimize variance in the distribution of sampled z-\ncoordinates zi along the ray r. A reduced variance indicates crisper surfaces in geometry. For\ninstance, in an extreme case where the rendering weights of a ray follow a Dirac delta function, the\nz-variance will be zero, resulting in an extremely sharp surface. Formally, we denote the z-variance\nalong the ray r as \u03c32\nzr:\n\u03c32\nzr =\nEz [(zi \u2212 \u00b5zr)2] =\nX\ni\n(zi \u2212 \u00b5zr)2\n\u03bdi\nP\ni \u03bdi\n.\n(7)\nThe regularization loss Lzvar for the variance \u03c32\nzr is defined as\nLzvar =\nEr[\u03b4r\u03c32\nzr],\n\u03b4r = 1 if\nX\ni\n\u03bdi > 0.5, else 0.\n(8)\n6\nPublished as a conference paper at ICLR 2024\nDreamfusion\nMagic3D\nFantasia3D\nOurs\nFigure 4: Visual comparisons to baseline methods. We visualize rendered images and extracted\nmeshes, and compare with Dreamfusion (Poole et al., 2022), Magic3D (Lin et al., 2023), and Fan-\ntasia3D (Chen et al., 2023b). Prompts: \u201cA plate piled high with chocolate chip cookies\u201d (top) and\n\u201cAn ice cream sundae\u201d (bottom).\nLSDS+\nLSDS-Latent\nLSDS-Image\nLSDS+\nLSDS-Latent\nLSDS-Image\nFigure 5: Ablation study of LSDS+ with (1) the full SDS loss LSDS+, (2) the SDS loss in latent\nspace only, denoted as LSDS-Latent, and (3) the SDS loss in image space only, denoted as LSDS-Image.\nPrompts are: (a) \u201cA baby bunny sitting on top of a stack of pancakes\u201d and (b) \u201cA peacock on a\nsurfboard\u201d.\nHere, \u03b4r acts as an indicator function (or binary weight) to filter out background rays. We find\nthis loss remarkably useful for ensuring geometrical consistency and eliminating cloudy geometri-\ncal artifacts in our 3D model. In Fig. 6, we also compare the regularization loss Lzvar to existing\nregularization strategies (Barron et al., 2022).\nConsequently, the total loss function is defined as,\nL =\nLSDS+ + \u03bbzvarLzvar,\n(9)\nwhere \u03bbzvar is the loss weight. We present our training procedure in the appendix, Algorithm 1.\nKernel smoothing for coarse-to-fine importance sampling. We observed that while integrating\nthe z-variance loss Lzvar sharpens the density distribution along the rays, it also intensifies the flick-\nering appearance. We consider the issue arising from the increased challenges of estimating the\nPDF of volume density along these rays. To address this, we propose a simple yet effective kernel\nsmoothing (KS) technique for coarse-to-fine importance sampling during rendering. Specifically,\nthe KS approach involves a weighted moving average of neighboring PDF values estimated during\nthe coarse stage. The weight is defined by a sliding window kernel. This ensures a broader sam-\npling scope near the high-density regions in the refined stage. Formally, in the coarse stage, for\neach weight vi along a ray r, the KS approach flattens the weight as vi =\nPN\nj=1 Kj\u00b7vi+j\u2212\u230a N\n2 \u230b\nPN\nj=1 Kj\n, where\nK \u2208 RN is the kernel. In practice, we set K = [1, 1, 1]. In Fig. 3, we visualized in (a) the ground\ntruth and the estimated distribution of volume density along a NeRF ray in the coarse stage. In (b),\nwe display the sampled z-coordinates in the refined stage, either without (on the left) or with (on the\nright) the KS approach. Their corresponding rendered images are shown in (c) and (e). Fig. 3 shows\nthat the KS approach ensures comprehensive sampling near the peak of the density distribution,\nachieving multi-view consistent renderings and eliminating flickering issues.\n7\nPublished as a conference paper at ICLR 2024\nFull L\nw/o Lzvar\n+ distortion loss\nFull L\nw/o Lzvar\n+ distortion loss\nFigure 6: Ablation study of the z-variance loss Lzvar. We experiment with (1) the full loss L,\n(2) the loss without the z-variance loss Lzvar, and (3) the loss where the z-variance loss is replaced\nwith an alternative distortion loss (Barron et al., 2022). We show a rendered example on the left, a\nzoomed-in result at the top right and the corresponding depth image at the bottom right.\nSqrt (Ours)\nRandom\nLinear\nCosine\nSqrt (Ours)\nRandom\nLinear\nCosine\nFigure 7: Ablation study of timestep annealing.\nWe experiment various timestep annealing\nschemes including square root, random sampling, linear and cosine. The results suggest that the\nsquare root annealing rate yields supiror performance w.r.t. photo-realism and reasonable geometry.\n5\nEXPERIMENTS\nWe evaluate our method to generate 3D assets from text prompts in Sec. 5.1. Specifically, we\ncompare our method with popular text-to-3D generation methods, Dreamfusion (Poole et al., 2022),\nMagic3D (Lin et al., 2023), and Fantasia3D (Chen et al., 2023b). Additional comparisons to the\nconcurrent work ProlificDreamer (Wang et al., 2023b) are presented in Appendix A.4. Moreover, we\nconduct extensive ablation studies in Sec. 5.2 to verify the effectiveness of each proposed technique.\nWe present experiments with an alternative text-to-image diffusion model in Sec. 5.3. In Sec. 5.4, we\nextend our method on the image-to-3D reconstruction task, compared with baseline methods (Liu\net al., 2023b; Qian et al., 2023; Liu et al., 2023c). Implementation details are in Appendix A.1.\n5.1\nEXPERIMENTAL RESULTS\nQualitative rendered results of the 3D assets generated by our approach are depicted in Fig. 1. Our\nproposed approach generates high-fidelity 3D assets, with photo-realism and multi-view consistency.\nAdditional results are shown in Appendix A.3.\nQualitative comparisons to baseline methods are shown in Fig. 4. Specifically, in Fig. 4, we\ncompare our method with Dreamfusion (Poole et al., 2022), Magic3D (Lin et al., 2023), and Fanta-\nsia3D (Chen et al., 2023b) for text-to-3D generation. We observe that our rendered images exhibit\nenhanced photo-realism, improved texture details of the 3D assets, and more natural lighting effects.\nAdditional comparisons are shown in Appendix A.4.\n5.2\nABLATION STUDY\nAblation on image-space regularization. Fig. 5 compares results of three different SDS loss set-\ntings: (1) the full SDS loss LSDS+, (2) the SDS loss in latent space only, denoted as LSDS-Latent, and\n(3) the SDS loss in image space only, denoted as LSDS-Image. The results indicate that incorporat-\ning the image-space regularization contributes to a more natural appearance and enhanced texture\ndetails, as exemplified by the peacock images. However, relying solely on the image-space loss\nLSDS-Image results in color bias issues, regardless of the guidance scale used.\nAblation on the z-variance loss Lzvar is shown in Fig. 6. In this case, we compare the results\nobtained using (1) the full loss, (2) the loss without the z-variance loss Lzvar, and (3) the loss where\nz-variance loss Lzvar is replaced with an alternative distortion loss introduced by Barron et al. (2022),\noriginally for outdoor scene reconstruction. Notably, the absence of the z-variance loss, Lzvar, leads\nto the generation of assets with cloudy artifacts. While the distortion loss mitigates this cloudiness\nissue, it occasionally compromises appearance details and hollow geometry. In comparison, our\nproposed z-variance loss Lzvar consistently yields photo-realistic results with crisp geometry.\n8\nPublished as a conference paper at ICLR 2024\nSD\nIF\nSD\nIF\nSD\nIF\nFigure 8: Experiments with alternative Deep Floyd IF model. We experiment with the stage-1\nmodel in IF, which employs a T5-XXL (Raffel et al., 2020) text encoder, and provides guidance in\n64\u00d764 resolution. Prompts: (a) \u201ca tiger dressed like a doctor\u201d, (b) \u201ca wide angle zoomed out DSLR\nphoto of a skiing penguin wearing a puffy jacket\u201d, and (c) \u201ca roast turkey on a platter with only one\npair of legs and one pair of wings\u201d.\nNovel view\nInput view\nZero-1-to-3\nMagic123\nSyncDreamer\nOurs\nFigure 9: Novel view image generation given a single view image. We compare our method\nwith concurrent works, including Zero-1-to-3 (Liu et al., 2023b), Magic123 (Qian et al., 2023), and\nSyncDreamer (Liu et al., 2023c).\nAblation on timestep annealing. Fig. 7 shows rendering images using different timestep sampling\nschemes. These include our proposed square root annealing rate, the random sampling adopted in\nprior works Poole et al. (2022); Lin et al. (2023), and both the linear and cosine annealing rates. We\nobserve that the square root timestep annealing scheme outperforms the other baseline schemes in\ncapturing detailed appearance and geometry.\n5.3\nOPTIMIZATION WITH GUIDANCE FROM AN ADVANCED TEXT ENCODER\nIn addition to using the SD model (Rombach et al., 2022), we also employ diffusion guidance from\nthe Deep Floyd IF model 1. The IF model employs an advanced text encoder, T5-XXL (Raffel et al.,\n2020). In Fig. 8, we show difficulties in generating content for the terms \u201cdoctor\u201d and \u201cjacket\u201d\nand the Janus problems (i.e., multi-face issues) when using the SD model. These challenges can\nbe addressed using the IF model. Here, we only use the stage-1 model in IF, generating images at\n64 \u00d7 64 resolution. A future direction would be to use the full model for high-resolution guidance.\n5.4\nIMAGE-TO-3D RECONSTRUCTION\nOur method also enables the reconstruction of 3D assets from a single image. To achieve this, we\nfollow the concurrent work, SyncDreamer (Liu et al., 2023c), which involves hallucinating 16-view\nimages for auxiliary image reconstruction. Then, we incorporate an image (or latent) reconstruc-\ntion loss for the given view(s) and utilize our score distillation method to optimize the remaining\nrandomly sampled views. Visual comparisons with the baseline methods are in Fig. 9. We observe\nthat our method can produce advancing photo-realistic images from novel views with reasonable de-\ntails. We also conduct an image-guided hallucination experiment where we initially use the image\nreconstruction loss, transitioning to exclusively use our proposed loss L throughout all views. See\nAppendix A.5 for additional results.\n6\nCONCLUSION\nWe propose a novel approach for high-quality text-to-3D generation in a single-stage training.\nSpecifically, we distill denoising scores from the pre-trained text-to-image diffusion models in both\nthe image and latent spaces, paired with a novel timestep annealing approach. Moreover, we propose\ntwo general improvements for NeRFs, including a z-variance loss and a kernel smooth approach, en-\nsuring 3D representation with consistent appearance and sharp geometry.\n1https://www.deepfloyd.ai/deepfloyd-if\n9\nPublished as a conference paper at ICLR 2024\n7\nACKNOWLEDGEMENTS\nThis work is partially supported by NSF III 2046795, IIS 1909577, CCF 1934986, NIH\n1R01MH116226-01A, NIFA award 2020-67021-32799, the Alfred P. Sloan Foundation, and Google\nInc.\n10\nPublished as a conference paper at ICLR 2024\nREFERENCES\nPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-\ntions and generative models for 3d point clouds. In ICML, 2018.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and\nPratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.\nIn ICCV, 2021.\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. CVPR, 2022.\nHansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-\nstage diffusion nerf: A unified approach to 3d generation and reconstruction. In ICCV, 2023a.\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv preprint arXiv:2303.13873, 2023b.\nYiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng Lin.\nIt3d: Improved text-to-3d generation with explicit view synthesis, 2023c.\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In CVPR,\n2019.\nYen-Chi Cheng, Hsin-Ying Lee, Sergey Tuyakov, Alex Schwing, and Liangyan Gui. SDFusion:\nMultimodal 3d shape completion, reconstruction, and generation. CVPR, 2023.\nZiya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner, and Angela Dai. Hyperdiffusion: Generat-\ning implicit neural fields with weight-space diffusion, 2023.\nJiatao Gu, Alex Trevithick, Kai-En Lin, Josh Susskind, Christian Theobalt, Lingjie Liu, and Ravi\nRamamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware\ndiffusion. In ICML, 2023.\nAyaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Kanazawa.\nInstruct-nerf2nerf: Editing 3d scenes with instructions. In ICCV, 2023.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nFangzhou Hong, Zhaoxi Chen, Yushi Lan, Liang Pan, and Ziwei Liu. Eva3d: Compositional 3d\nhuman generation from 2d image collections. ICLR, 2023.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models. ICLR, 2022.\nAjay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided\nobject generation with dream fields. CVPR, 2022.\nNasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Popa Tiberiu. Clip-mesh: Gen-\nerating textured meshes from text using pretrained image-text models. SIGGRAPH Asia 2022\nConference Papers, December 2022.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 2015.\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023.\nChieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-\nHsuan Yang, and Sergey Tulyakov. Infinicity: Infinite-scale city synthesis. ICCV, 2022.\nMinghua Liu, Chao Xu, Haian Jin, Linghao Chen, Mukund Varma T, Zexiang Xu, and Hao Su.\nOne-2-3-45: Any single image to 3d mesh in 45 seconds without per-shape optimization, 2023a.\n11\nPublished as a conference paper at ICLR 2024\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object, 2023b.\nYuan Liu, Cheng Lin, Zijiao Zeng, Xiaoxiao Long, Lingjie Liu, Taku Komura, and Wenping Wang.\nSyncdreamer: Learning to generate multiview-consistent images from a single-view image. arXiv\npreprint arXiv:2309.03453, 2023c.\nJonathan Lorraine, Kevin Xie, Xiaohui Zeng, Chen-Hsuan Lin, Towaki Takikawa, Nicholas Sharp,\nTsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. Att3d: Amortized text-to-3d object\nsynthesis. ICCV, 2023.\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR,\n2021.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. ECCV, 2020.\nParitosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. AutoSDF: Shape priors\nfor 3d completion, reconstruction and generation. In CVPR, 2022.\nThomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. ACM Trans. Graph., 2022.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nGuocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren, Aliaksandr Siarohin, Bing Li, Hsin-Ying\nLee, Ivan Skorokhodov, Peter Wonka, Sergey Tulyakov, and Bernard Ghanem. Magic123: One\nimage to high-quality 3d object generation using both 2d and 3d diffusion priors. arXiv preprint\narXiv:2306.17843, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 2020.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022.\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:\na hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, 2021.\nEdward J Smith and David Meger.\nImproved adversarial systems for 3d object generation and\nreconstruction. In CoRL, 2017.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. ICLR,\n2021.\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen.\nMake-it-3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv preprint\narXiv:2303.14184, 2023.\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. CVPR, 2023a.\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Pro-\nlificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.\nNeurIPS, 2023b.\n12\nPublished as a conference paper at ICLR 2024\nJianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learn-\ning descriptor networks for 3d shape synthesis and analysis. In CVPR, 2018.\nJiale Xu, Xintao Wang, Weihao Cheng, Yan-Pei Cao, Ying Shan, Xiaohu Qie, and Shenghua Gao.\nDream3d: Zero-shot text-to-3d synthesis using 3d shape prior and text-to-image diffusion models.\narXiv preprint arXiv:2212.14704, 2022.\nSong-Hai Zhang, Yuan-Chen Guo, and Qing-Wen Gu. Sketch2model: View-aware 3d modeling\nfrom single free-hand sketches. In CVPR, 2021.\nZhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d\nreconstruction. In CVPR, 2023.\nPeiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M Susskind, and Miguel \u00b4Angel\nBautista. Diffusion probabilistic fields. In ICLR, 2023.\n13\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nWe present our implementation details in Sec. A.1. Our training algorithm is shown in Sec. A.2.\nFurther 3D asset generation results can be viewed in Sec. A.3, and additional comparisons to text-\nto-3D baseline methods are available in Sec. A.4. In Sec. A.5, we show additional image-to-3D\nreconstruction results and experiments with an image-guided hallucination task where we generate\n3D assets that are hallucinated from the given input image (rather than reconstruction). Please refer\nto our video demo in the supplementary material for a comprehensive overview.\nA.1\nIMPLEMENTATION DETAILS\nModel setup. Our approach is implemented based on a publicly available repository 2. In this im-\nplementation, a NeRF is parameterized by a multi-layer perception (MLP), with instant-ngp (M\u00a8uller\net al., 2022) for positional encoding. To enhance photo-realism and enable flexible lighting mod-\neling, instead of using Lambertian shading as employed in (Poole et al., 2022), we encode the ray\ndirection using spherical harmonics and utilize it as an input to NeRF. Additionally, we incorporate\na background network that predicts background color solely based on the ray direction. We employ\na pre-trained SD model 3 as diffusion prior, as well as a pre-trained dense prediction model 4 to\npredict disparity maps.\nTraining setup. We use Adam (Kingma & Ba, 2015) with a learning rate of 10\u22122 for instant-\nngp encoding, and 10\u22123 for NeRF weights. In practice, we choose total iter as 104 iterations.\nThe rendering resolution is 512 \u00d7 512. We employ DDIM (Song et al., 2021) with empirically\nchosen parameters r = 0.25, and \u03b7 = 1 to accelerate training. We choose the hyper-parameters\n\u03bbrgb = 0.1, \u03bbd = 0.1, and \u03bbzvar = 3. Similar to prior work (Poole et al., 2022; Lin et al., 2023;\nWang et al., 2023a), we use classifier-free guidance (Ho & Salimans, 2022) of 100 for our diffusion\nmodel.\nA.2\nTRAINING ALGORITHM\nWe present our training procedure in Algorithm 1. In step 5, either a single-step or multi-step\ndenoising approach can be used to estimate the latent vector z. Here, the multi-step denoising refers\nto the iterative denoising of \u02c6zt, until t = 0.\nAlgorithm 1 Training Procedure\nInput: A pre-trained SD Rombach et al. (2022) consisting of an encoder E, a decoder D, and a\ndenoising autoencoder \u03f5\u03d5; a rendering x = g(\u03b8); a latent vector z = E(x); a number of total\ntraining steps total iter; range of the diffusion time steps [tmax, tmin]; a conditioning y; scaling\ncoefficients \u03b1t and \u03c3t.\n1: for iter = [0, total iter] do\n2:\nt = tmax \u2212 (tmax \u2212 tmin)\nq\niter\ntotal iter\n3:\nzt = \u03b1tz + \u03c3t\u03f5, where \u03f5 \u223c N(0, I)\n4:\nEstimating noise \u02c6\u03f5 = \u03f5\u03d5(zt; y, t)\n5:\nEstimating the latent vector \u02c6z =\n1\n\u03b1t (zt \u2212 \u03c3t\u02c6\u03f5) via either single- or multi-step denoising\n6:\nEstimating the image \u02c6x = D(\u02c6z)\n7:\nCompute the loss gradient \u2207\u03b8L and update \u03b8\n8: end for\nReturn: \u03b8\nA.3\nADDITIONAL RESULTS OF TEXT-TO-3D GENERATION\nWe provide more generated 3D assets given text prompts in Fig. 10- 12.\n2https://github.com/ashawkey/stable-dreamfusion/tree/main.\n3We use the pre-trained SD in https://github.com/huggingface/diffusers.\n4https://github.com/huggingface/transformers.\n14\nPublished as a conference paper at ICLR 2024\nA.4\nADDITIONAL COMPARISONS TO THE BASELINE METHODS\nWe present additional comparisons to the baseline methods in Fig 13- 18, following the rendering\nsettings used in ProlificDreamer (Wang et al., 2023b).\nSpecifically, in Fig. 13, we present results only using NeRF representation, comparing them to two\nbaseline methods, namely ProlificDreamer (Wang et al., 2023b) and DreamFusion (Poole et al.,\n2022). In this case, no fine-tuning stage for 3D asset generation is applied in these baseline methods\nas illustrated in Fig. 13; our method allows the generation of high-fidelity details and natural colors\nthrough only a single-stage optimization. We observe flickering issues and improper geometries\nwhen using only the NeRF representation in ProlificDreamer (Wang et al., 2023b). In contrast, our\nmethod consistently provides view and geometry-consistent results without flickering.\nIn Fig. 17, we present additional visual results, comparing them to the baseline methods, including\nProlificDreamer (Wang et al., 2023b), Fantasia3D Chen et al. (2023b), Magic3D (Lin et al., 2023)\nand DreamFusion (Poole et al., 2022). In this case, the baseline methods (Wang et al., 2023b; Lin\net al., 2023) employ the full training pipeline, which includes NeRF representation followed by\nfine-tuning.\nAdditional comparisons with Fantasia3D (Chen et al., 2023b) and Magic3D (Lin et al., 2023) are\nshown in Fig. 14- 16, and comparisons with DreamFusion (Poole et al., 2022) in Fig. 18.\nIn Fig. 19, we integrate the z-variance loss into ProlificDreamer. We observe that incorporating\nthe z-variance loss results in sharper textures. In Fig.20, we present the results of ProlificDreamer\nboth without and with our proposed method, which includes the z-variance loss, the image-space\nloss, and the square root time-step annealing schedule. From the results, our method enhances the\nbaseline approach, enabling it to generate superior renderings with detailed textures.\nA.5\nADDITIONAL RESULTS OF IMAGE-TO-3D RECONSTRUCTION\nIn Fig. 21, we present additional image-to-3D reconstruction results. Additionally, we conduct\nimage-guided 3D hallucination experiments. Specifically, we execute image-to-3D reconstruction\nat early training iterations, and then optimize the NeRF representation only using our proposed\ndistillation loss, omitting the image reconstruction loss. We show these results in Fig. 22.\n15\nPublished as a conference paper at ICLR 2024\n``A ladybug\u201d\n``A low-poly tree\u201d\n``A model of the Eiffel Tower, aerial view\u201d\n``A high detailed octopus, 4k\u201d\n``A delicious croissant\u201d\n``A broken old clay vessel\u201d\n``A model of the Eiffel Tower \nmade out of toothpicks\u201d\n``An astronaut is riding a horse\u201d\nFigure 10: Additional 3D asset generation results with the corresponding normal map given\ntext prompts (below each object).\n16\nPublished as a conference paper at ICLR 2024\n``Gold skull, 4k, highest quality\u201d\n``A high detailed pumpkin, style cartoon, \nfront view, 4k, orange, funny and cute, \nHDR, Halloween\u201d\n``A watermelon\u201d\n``A pomeranian dog\u201d\n``A wooden medieval shipping barrel\u201d\n``A DSLR photo of Cthulhu\u201d\n``Pumpkin head zombie, skinny, \nhighly detailed, photorealistic\u201d\n``The leaning tower of Pisa, aerial view\u201d\nFigure 11: Additional 3D asset generation results with the corresponding normal map given\ntext prompts (below each object).\n17\nPublished as a conference paper at ICLR 2024\n``A tulip\u201d\n``A pink cherry blossom shaped cake\u201d\n``A big coral\u201d\n``A ripe strawberry\u201d\n``An ice cream sundae\u201d\n``A shell\u201d\n``Neuschwanstein Castle, aerial view\u201d\n``A stylized witch pot made of stone or mud with a wide \nbase and a narrow neck, decorated with intricate carvings \nof runes and symbols, and is filled with a bubbling, green \nliquid. The pot is surrounded by a cloud of steam\u201d\nFigure 12: Additional 3D asset generation results with the corresponding normal map given\ntext prompts (below each object).\n18\nPublished as a conference paper at ICLR 2024\nProlificDreamer\nDreamFusion\nOurs\n``A baby bunny sitting on top of a stack of pancakes\u201d\n``A delicious croissant\u201d\nProlificDreamer\nDreamFusion\nOurs\nFigure 13: Additional visual comparisons using NeRF representation only. We compare visually\nwith the baseline methods, ProlificDreamer (Wang et al., 2023b) and DreamFusion (Poole et al.,\n2022), specifically after the first training stage. In this case, 3D assets are represented by NeRF, with\nno additional fine-tuning applied in the baselines.\nOurs\n``A baby bunny sitting on top of a stack of pancakes\u201d\nFantasia3D\nMagic3D\n``A ripe strawberry\u201d\nOurs\nFantasia3D\nMagic3D\nFigure 14: Additional visual comparisons with Fantasia3D (Chen et al., 2023b) and Magic3D (Lin\net al., 2023).\n19\nPublished as a conference paper at ICLR 2024\nOurs\n``a beagle eating a donut\u201d\nFantasia3D\n``A blue jay standing on a large basket of rainbow macarons\u201d\nOurs\nFantasia3D\n``a Christmas tree with donuts as decorations\u201d\n``a brightly colored mushroom growing on a log\u201d\n``an orangutan making a clay bowl on a throwing wheel\u201d\nOurs\nFantasia3D\nOurs\nFantasia3D\nOurs\nFantasia3D\nFigure 15: Additional visual comparisons with Fantasia3D (Chen et al., 2023b)\n20\nPublished as a conference paper at ICLR 2024\nOurs\n``a dalmatian wearing a fireman's hat\u201d\nFantasia3D\n``an unstable rock cairn in the middle of a stream\u201d\nOurs\nFantasia3D\n``a red-eyed tree frog\u201d\n``a dragon-cat hybrid\u201d\n``an amigurumi motorcycle\u201d\nOurs\nFantasia3D\nOurs\nFantasia3D\nOurs\nFantasia3D\nFigure 16: Additional visual comparisons with Fantasia3D (Chen et al., 2023b)\n21\nPublished as a conference paper at ICLR 2024\nOurs\n``A car made out of sushi\u201d\nProlificDreamer\nFantasia3D\nMagic3D\nDreamFusion\nFigure 17:\nAdditional visual comparisons with the baseline methods, including Prolific-\nDreamer (Wang et al., 2023b), Fantasia3D Chen et al. (2023b), Magic3D (Lin et al., 2023) and\nDreamFusion (Poole et al., 2022). In this case, the baseline methods (Wang et al., 2023b; Lin\net al., 2023) employ the full training pipeline, which includes NeRF representation followed by fine-\ntuning.\n22\nPublished as a conference paper at ICLR 2024\n``A ladybug\u201d\nOurs\nDreamFusion\n``Michelangelo style statue of dog reading news on a cellphone\u201d\nOurs\nDreamFusion\n``A 3d model of an adorable cottage with a thatched roof\u201d\nOurs\nDreamFusion\n``A lionfish\u201d\nOurs\nDreamFusion\nFigure 18: Additional visual comparisons with DreamFusion (Poole et al., 2022).\n23\nPublished as a conference paper at ICLR 2024\n4K iters\n7K iters\n10K iters\nFigure 19: Visual results of incorporating the z-variance loss to ProlificDreamer (Wang et al.,\n2023b) throughout the training process. We show rendered results w/o (left) and w/ (right) the\nz-variance loss after 4K, 7K and 10K training iterations.\nBaseline\nw/ our method\nBaseline\nw/ our method\nBaseline\nw/ our method\nFigure 20: The baseline results, ProlificDreamer (Wang et al., 2023b), without and with our\nproposed method. This includes the z-variance loss, the image-space loss, and the square root\ntime-step annealing schedule.\nNovel views\nNovel views\nInput view\nInput view\nFigure 21: Additional results of image-to-3D reconstruction.\n24\nPublished as a conference paper at ICLR 2024\nNovel views\n``A toy grabber with dinosaur head\u201d\nNovel views\nNovel views\n``Hircine's Ring from Elder Scrolls Online\u201d\n``A colorful painted teapot\u201d\nInput view\nInput view\nInput view\nFigure 22: Visual results of image-guided 3D hallucination. We hallucinate the 3D asset from a\nsingle given image using the prompt below the object.\n25\n"
  },
  {
    "title": "Grammar Prompting for Domain-Specific Language Generation with Large Language Models",
    "link": "https://arxiv.org/pdf/2305.19234.pdf",
    "upvote": "3",
    "text": "arXiv:2305.19234v3  [cs.CL]  3 Nov 2023\nGrammar Prompting for Domain-Speci\ufb01c Language\nGeneration with Large Language Models\nBailin Wang\u22c4\nZi Wang\u2020\nXuezhi Wang\u2020\nYuan Cao\u2021\nRif A. Saurous\u2020\nYoon Kim\u22c4\n\u22c4Massachusetts Institute of Technology\n\u2020Google DeepMind\n\u2021Google Research\n{bailinw, yoonkim}@mit.edu, {wangzi, xuezhiw, yuancao, rif}@google.com\nAbstract\nLarge language models (LLMs) can learn to perform a wide range of natural\nlanguage tasks from just a handful of in-context examples. However, for gen-\nerating strings from highly structured languages (e.g., semantic parsing to com-\nplex domain-speci\ufb01c languages), it is challenging for the LLM to generalize from\njust a few exemplars. We propose grammar prompting, a simple approach to en-\nable LLMs to use external knowledge and domain-speci\ufb01c constraints, expressed\nthrough a grammar in Backus\u2013Naur Form (BNF), during in-context learning.\nGrammar prompting augments each demonstration example with a specialized\ngrammar that is minimally suf\ufb01cient for generating the particular output example,\nwhere the specialized grammar is a subset of the full DSL grammar. For inference,\nthe LLM \ufb01rst predicts a BNF grammar given a test input, and then generates the\noutput according to the rules of the grammar. Experiments demonstrate that gram-\nmar prompting can enable LLMs to perform competitively on a diverse set of DSL\ngeneration tasks, including semantic parsing (SMCalFlow, Overnight, GeoQuery),\nPDDL planning, and SMILES-based molecule generation.\n1\nIntroduction\nPrompting large language models (LLMs) with demonstrations optionally combined with natural\nlanguage instructions has been shown to be an effective approach for surfacing their myriad capabil-\nities acquired through pretraining [10]. This approach is however inadequate for applications where\nthe task speci\ufb01cations cannot be fully delineated through just a handful of exemplars, for example\nin semantic parsing where an LLM must translate a natural language utterance to an executable\nprogram in a domain-speci\ufb01c language (DSL). DSLs often incorporate domain-speci\ufb01c abstractions\nand semantics that are dif\ufb01cult to characterize via just a few demonstrations. And unlike general-\npurpose programming languages, DSLs are by de\ufb01nition specialized and thus unlikely to have been\nencountered often enough (or at all) during pretraining for the LLM to acquire its full syntax.\nHow can we draw on the few-shot learning capabilities of LLMs to generate structured strings\nthat are substantially different from those seen during pretraining? This work explores grammar\nprompting as a simple approach for data-ef\ufb01cient generation of structured languages where an output\nstring in the language can be derived through a series of symbolic manipulations. We exploit the fact\nthat constraints over a structured output space can often be succinctly described by a context-free\ngrammar in Backus\u2013Naur Form (BNF), which is commonly used to de\ufb01ne the syntax of a language.\nGrammar prompting augments each in-context example (x, y) with a specialized BNF grammar\nG[y] that is minimally suf\ufb01cient for generating y. Given a new input, the LLM \ufb01rst predicts the\nspecialized BNF grammar and then generates the answer conditioned on the grammar.\nGrammar prompting follows the recent line of work which enhances the few-shot reasoning capa-\nbilities of LLMs by interleaving intermediate \u201creasoning\u201d steps between each in-context input and\nCode and data available at: https://github.com/berlino/grammar-prompting.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\noutput [51, 24, 86, 80, 73]. The key difference in our approach is that the intermediate variable is in\nthe form of a formal grammar rather than in natural language, which focuses on eliciting the sym-\nbolic manipulation capabilities of LLMs. The use of a formal grammar moreover makes it possible\nto impose constraints during incremental decoding such that syntactic validity is guaranteed. Finally,\nunlike chain-of-thought-style prompts [86] which typically require manual verbalization of the inter-\nmediate reasoning steps, in our approach the specialized grammar G[y] can be derived automatically\nby parsing the output y with the full (unspecialized) DSL grammar.\nTo summarize,\n\u2022 We propose grammar prompting as a simple approach for enabling LLMs to generate highly\nstructured languages from just a few exemplars.\n\u2022 We design a constrained LLM decoding algorithm tailored to grammar prompting, which guar-\nantees syntactic validity while minimizing the number of LLM API calls.\n\u2022 We apply grammar prompting to various domain speci\ufb01c languages for semantic parsing (SM-\nCalFlow, Overnight, GeoQuery), AI planning (PDDL), and molecule generation (SMILES), and\n\ufb01nd that it can meaningfully improve upon standard prompting baselines in the few-shot setting.\n2\nBackground\nIn this section, we de\ufb01ne our problem and review the few-shot learning method that we build on.\n2.1\nProblem Formulation: Domain-Speci\ufb01c Language Generation\nLet \u03a3\u2217 be the set of all \ufb01nite strings over an alphabet \u03a3, and further let D \u2286 \u03a3\u2217 be a domain-speci\ufb01c\nlanguage (DSL) for an application of interest. Given an input x (e.g., a natural language command)\nwe are interested in generating y \u2208 D (e.g., a program in a DSL ful\ufb01lling the command), as shown\nby the following calendar assistant example from SMCalFlow [6]:\nx : Add meeting with Jean\u2019s manager on Wednesday at 3PM.\ny : CreateEvent(& (start_? WednesdayNumberPM(3))(attendee_? FindManager(Jean)))\nevent\n::=\n\"CreateEvent(\" constraint \")\"\n|\n\"QueryEvent(\" constraint \")\"\nconstraint\n::=\n\"(&\" constraint constraint \")\"\n|\n\"(start_?\" date time? \")\"\n|\n\"(attendee_?\" attendee* \")\"\ndate\n::=\n\"Wednesday\" | \"Monday\"\nnumber\n::=\n(\"0\"..\"9\")+\ntime\n::=\n\"NumberAM(\" number \")\"\n|\n\"NumberPM(\" number \")\"\nattendee\n::=\n| \"Bob\" | \"Carol\" | \"Jean\"\n|\n\"FindManager\"\nFigure 1: A simple BNF grammar for a calendar DSL.\nDSLs are crafted by experts who use\ntheir domain-speci\ufb01c knowledge to incor-\nporate higher-level abstractions than are\ntypically found in general-purpose pro-\ngramming languages. We assume access\nto an expert-de\ufb01ned grammar G that fully\nspeci\ufb01es the DSL\u2019s syntax. As is the case\nwith many DSLs, we further assume that\nG is a context-free grammar in Backus\u2013\nNaur Form (BNF). See Figure 1 for a sim-\nple example adapted from SMCalFlow [6].\nLetting L(G) be the language generated\nby G, we have D \u2286 L(G) \u2286 \u03a3\u2217 (not all\nsyntactically valid programs are semantically valid).\n2.2\nFew-shot Learning with Large Language Models\nIn-context learning with large language models (LLMs) has been shown to be an effective approach\nfor few-shot learning [10]. Under this approach, a pretrained LLM is conditioned on N demonstra-\ntion examples (x(i), y(i))N\ni=1 followed by a test example x, and the output is given by decoding from\nthe prompted LLM, i.e., PLLM(y | x, (x(i), y(i))N\ni=1). The demonstration examples can be optionally\npreceded by natural language instructions to further improve performance or even enable zero-shot\nlearning [85, 62]. Recent work has additionally shown that interleaving natural language verbal-\nizations of intermediate reasoning steps between each x(i) and y(i) can greatly improve few-shot\nperformance on complex reasoning tasks [51, 86, 80, 73, 16].\nThe effectiveness of few-shot in-context learning depends both on how useful the implicit knowledge\nacquired through pretraining is for the task, and on how effectively the task speci\ufb01cations can be\nconveyed through the demonstrations. For DSL, the structured nature of combinatorial output space\n(i.e., the DSL grammar G) cannot be adequately captured through just a handful of demonstrations.\nThus, few-shot generation of strings of a DSL remains challenging for LLMs.\n2\nLLM Prompt\nYou are an expert programmer, and you need to write a program for the given natural language query.\nFirst, you should write a grammar that contains all the necessary BNF rules. Then, you should write pro-\ngrams that conform to your predicted rules.\n(optional) G:\n[BEGIN RULES]\n. . .\n[END RULES]\nx(1): \ufb01nd the meeting on Wednesday with Bob and Carol\nG[y(1)]:\nevent\n::=\n\"QueryEvent(\" constraint \")\"\nconstraint\n::=\n\"(&\" constraint constraint \")\"\n|\n\"(start_?\" date \")\"\n|\n\"(attendee_?\" attendee attendee \")\"\ndate\n::=\n\"Wednesday\"\nattendee\n::=\n\"Bob\" | \"Carol\"\ny(1): QueryEvent(& (start_? Wednesday)(attendee_? Bob Carol))\n. . .\nx: Add meeting with Jean\u2019s manager on Wednesday at 3PM\nLLM Output\nbG:\nevent\n::=\n\"CreateEvent(\" constraint \")\"\nconstraint\n::=\n\"(&\" constraint constraint \")\"\n|\n\"(start_?\" date time \")\"\n|\n\"(attendee_?\" attendee \")\"\ndate\n::=\n\"Wednesday\"\ntime\n::=\n\"NumberPM(3)\"\nattendee\n::=\n\"FindManager(\" attendee \")\" | \"Jean\"\nby: CreateEvent(& (start_? Wednesday NumberPM(3))(attendee_? FindManager(Jean)))\nFigure 2: Example of grammar prompting for a calendar DSL. We interleave the minimal specialized grammar\nG[y(i)] between the demonstrations x(i) and y(i). During decoding, the LLM \ufb01rst predicts the specialized\ngrammar bG, and then predicts the program by conditioned on bG. The blue portion is not part of the actual\nprompt and only shown for illustrative purposes.\n3\nGrammar Prompting\nGrammar prompting exploits the fact that while the actual strings of a DSL may not have been\nencountered frequently enough (or at all) during pretraining for the LLM to implicitly acquire its\nsyntax, the LLM will likely have encountered many instances of metalanguages (languages used to\ndescribe other languages). BNF grammars are a standard metalanguage for specifying a language\u2019s\nsyntax, and are expected to occur in the LLM training corpus with some frequency (e.g., in computer\nscience textbooks). We thus focus on using BNF grammars for few-shot DSL generation.\nLet G = SM\nj=1{rj} be an extended BNF grammar where each rule rj is of the form\n<symbol> ::= <expr1> | <expr2> | ...\nHere <symbol> is a nonterminal symbol and each <expr1> is a sequence of nonterminal and terminal\nsymbols.1 A straightforward approach for incorporating a BNF grammar during in-context learning\nis to simply prepend the string representation of the full grammar G to the demonstration examples,\nalong with an instruction to use the grammar. However in preliminary experiments, we found that\nthis did not yield any improvements.2\n3.1\nSpecialized Grammars\nWe propose to use specialized grammars to enable better use of domain-speci\ufb01c knowledge and\nconstraints. A specialized grammar G\u2032 \u2286 G is a grammar obtained from taking a subset of the rules\nof the full grammar G. We further de\ufb01ne G[y], a minimal specialized grammar of y, to be a BNF\n1For brevity we forgo the formal tuple-based de\ufb01nition of G and instead de\ufb01ne G to be equivalent to its\ncontext-free rules. We also freely go back and forth between this set de\ufb01nition of G and its string representation.\n2However, when combined with specialized grammars we did observe small improvements by appending\nthe full DSL grammar to the instructions. Hence, for all experiments where G is small enough (GeoQuery,\nOvernight-B, SMILES), we include G as part of the instruction. See Figure 2.\n3\ngrammar with the following properties: (1) y \u2208 L(G[y]), and (2) \u2200r \u2208 G[y], y \u0338\u2208 L(G[y] \\ {r}).3\nWe can readily obtain a minimal specialized grammar by using G to parse y and then taking the\nunion of rules that were used in the derivation of y.\nGrammar prompting feeds a sequence of (x(i), G[y(i)], y(i))N\ni=1 along with x as a prompt to an\nLLM. For inference we \ufb01rst obtain the specialized grammar with an (approximate) arg max decod-\ning\nbG = arg max\nG\u2032\u2286G\nPLLM(G\u2032 | x, (x(i), G[y(i)], y(i))N\ni=1).\nWe then obtain the program conditioned on bG,\nby = arg max\ny\u2208L( b\nG)\nPLLM(y | bG, x, (x(i), G[y(i)], y(i))N\ni=1).\nWe discuss how to perform constrained decoding with bG \u2286 G and by \u2208 L( bG) in the next section.\nGrammar prompting views DSL program generation as a grammar specialization process where\ngiven a natural language speci\ufb01cation x, a set of production rules, bG, is selected from G, and then a\nprogram by is deduced according to the selected rules. Grammar prompting can also be viewed as an\ninstance of chain-of-thought prompting [51, 86] where the intermediate thought is in the form of a\nformal grammar. However, unlike typical chain-of-thought prompting where the answer is (usually)\ndeterministic given the intermediate reasoning steps, in our case there is still some uncertainty with\nrespect to by given bG (e.g., L( bG) could still be in\ufb01nite).\n3.2\nConstrained Decoding\nAlgorithm 1 Earley-based Constrained Generation\nInput: Test input x, predicted grammar bG\nOutput: Program \u02c6y \u2208 L( bG)\n1: \u02c6y \u2190 \u01eb\n\u22b2 initialize to empty string\n2: while True do\n3:\n\u00afy \u2190 decode\n\u0010\nPLLM(\u00b7 | x, bG, \u02c6y, . . . )\n\u0011\n4:\n\u02c6y \u2190 \u02c6y \u00b7 \u00afy\n\u22b2 concatenation\n5:\nif \u02c6y \u2208 L( bG) then\n\u22b2 try parsing with bG\n6:\nreturn \u02c6y\n\u22b2 return if successful\n7:\nelse\n\u22b2 if parsing fails, need to correct\n8:\nypre\ufb01x, \u03a3[ypre\ufb01x]\u2190 EarleyParse(\u02c6y, bG)\n9:\nw\u2217 \u2190\narg max\nw\u2208\u03a3[ypre\ufb01x]\nPLLM(w | ypre\ufb01x, . . . )\n10:\n\u02c6y \u2190 ypre\ufb01x \u00b7 w\u2217\n11:\nend if\n12: end while\n13: return \u02c6y\nThe use of a formal grammar as an inter-\nmediate variable makes it possible to en-\nforce grammatical constraints during au-\ntoregressive LLM decoding. We \ufb01rst dis-\ncuss how we enforce the constraint y \u2208\nL( bG).\nOne approach to constrained de-\ncoding is to use bG to obtain a left-to-right\nEarley parser [18] and only decode from\nvalid continuations at each decoding step.\nHowever this simple strategy poses several\npractical challenges when working with\nAPI-only LLMs. For one, a valid terminal\ncontinuation in bG may consist of multiple\nBPE tokens. Moreover, while we can sam-\nple a valid continuation at each time step\nby disallowing invalid tokens,4 since the\nset of valid continuations changes at each\ntime step, this strategy would require call-\ning the LLM API at each time step with\nthe full prompt and pre\ufb01x along with the disallowed continuations, which is prohitively expensive.5\nWhile there are many methods for grammar-constrained LM decoding [68, 64, 26], we present a\nsimple strategy which speculatively decodes from the LLM to look ahead for multiple tokens. The\npseudocode is shown in Algorithm 1. At each prediction step, we ask the LLM to speculatively\ndecode the full program conditioned on the current pre\ufb01x (lines 4-5). If the resulting continuation\nleads to a valid program, we return it (lines 6-7). Otherwise, we consult an Earley parser to extract\n3Note that y may have more than one minimal specialized grammar due to the potential instantiation of\nextended BNF rules. For instance, the expression \"(attendee_?\" attendee+ \")\" depicted in Figure 1 im-\nplicitly de\ufb01nes all occurrences of attendee greater than 1. If this expression is incorporated into a program,\neither the concrete rule \"(attendee_?\" attendee \")\" or the original rule could be included in the minimal\nspecialized grammar. In most applications we consider the rules of the minimal specialized grammar will be\nconcrete, and thus there will only be one parse associated with y. See appendix A.1 for further details.\n4For example by using the logit_bias argument from OpenAI\u2019s LLM API.\n5These costs might be mitigated in the future if LLM APIs allow for cheaper use of cached prompts.\n4\nEarley-based Constrained Decoding\n\u02c6y(t) : CreateEvent(& (start_? Wednesday NumberPM(3))(attendee_? Jean\u2019s Manager))\nExtract the longest valid pre\ufb01x, and possible \ufb01xes (i.e., next terminals) based on Earley parsing:\nypre\ufb01x : QueryEvent(& (start_? Wednesday)(attendee_?\n\u03a3[ypre\ufb01x]: {Jean , FindManager(}\nFind the best candidate and concatenate it with the pre\ufb01x:\n\u02c6y(t) \u2190 QueryEvent(& (start_? Wednesday)(attendee_? FindManager(\nFigure 3: Illustration of how an predicted program is corrected in our proposed Earley-based constrained\ndecoding. The \ufb01nal partial program will be subsequently fed into the LLM for continuation.\nthe longest valid pre\ufb01x from the current prediction (ypre\ufb01x), along with a set of valid terminals\nthat can follow the pre\ufb01x (\u03a3[ypre\ufb01x]). Finally, we rely on the LLM\u2019s probabilities to decide which\nterminal to use, with which a new partial program can be constructed (lines 10-11).6 Figure 3\nillustrates one prediction step where the predicted program is corrected into a new valid partial\nprogram. Note that w can consist of multiple BPE tokens, e.g., \"FindManager(\" in Figure 3. By\nscoring over multi-token terminals, the search procedure is implicitly augmented by looking ahead\nfor a few tokens.\nWe use a similar procedure to operationalize the constraint G\u2032 \u2286 G, except that EarleyParse (used\nat Algorithm 1, line 9) is constructed with a metagrammar (i.e., the grammar of G) for grammar pre-\ndiction. See appendix A.1 for more details. In our ablation study we \ufb01nd that while these constraints\nare helpful insofar as they guarantee syntactic validity, grammar prompting still meaningfully im-\nproves upon standard prompting with even with simple unconstrained decoding.\n4\nExperiments\nWe apply grammar prompting to diverse domains:\nDSLs for semantic parsing (SMCalFlow,\nOvernight, GeoQuery), an action DSL (PDDL planning), and a molecule generation DSL (SMILES).\nThese experiments are not necessarily intended to improve upon the state-of-the-art on these bench-\nmarks but rather intended to assess whether LLMs can improve upon standard prompting for few-\nshot DSL generation by learning to predict and use grammars during in-context learning.\n4.1\nSemantic Parsing for Tool Usage\nSoftware tools are typically accompanied by a collection of human-interpretableAPIs which provide\na platform for developers to interact programmatically with the tools. These APIs constitute a DSL,\nwhere each production rule of the grammar speci\ufb01es the input and output types for a speci\ufb01c API\ncall (see Figure 1 for an example). These tools demonstrate a broad spectrum in terms of DSL com-\nplexity, ranging from single-function tools such as Google(user_query), Translate(sentence,\nlanguage) to more complex tools such as the entirety of Wolfram language.7 Enabling LLMs to use\nexternal tools via APIs is an important step towards enhancing their capabilities [63, 56, 53, 72, 47].\nWe test our approach on standard semantic parsing benchmarks involving complex DSLs: SM-\nCalFlow [6], which features human-generated utterances about calendar management (see Figure 2);\nGeoQuery [99] which features queries against a US Geography database; and Overnight-Blocks[81],\nwhich features queries about blocks in a synthetic block world. See appendix B for examples of\ninput-output pairs along with the specialized grammars. The original benchmarks target the train-\ning of conventional semantic parsers and thus contain hundreds/thousands of training examples.\nEven prompting-based approaches on these benchmark rely on retrieval-based in-context learning\nwhich \ufb01rst retrieves m exemplars from a large training set of n examples (n \u226b m) based on some\nsimilarity measure (e.g., BM-25), and then performs in-context learning with the retrieved exem-\nplars [57, 95, 68, 46]. In contrast, we target the true few-shot setting where we only assume access\nto 16\u201332 demonstration examples.\nOur baselines here include: (1) standard prompting, (2) standard prompting with constrained decod-\ning based on the full DSL grammar G [68, 64], and (3) a derivation tree-based prompting baseline\n6In rare cases the set \u03a3[ypre\ufb01x] was too large to feed to LLM APIs. In these cases we used Sentence-\nBERT [59] to compute the similarity between the encoding of ypre\ufb01x \u00b7w and \u02c6y(t) and took the top 16 candidates\nas \u03a3[ypre\ufb01x].\n7https://www.wolfram.com/language/\n5\nGeoQuery\nSMCalFlow\nOvernight-Blk\nApproach\nProg.\nExec.\nProg.\nProg.\nExec.\nStandard Prompting (unconstrained decoding)\n60.7\n81.5\n46.4\n29.3\n54.7\nw. constrained decoding (by \u2208 L(G))\n61.1\n81.8\n49.2\n29.3\n54.7\nLinearized Derivation Tree Prompting\n58.6\n77.5\n50.0\n27.3\n56.4\nGrammar Prompting (unconstrained decoding)\n67.1\n87.5\n50.8\n34.8\n57.4\nw. grammar constraint ( bG \u2286 G)\n67.9\n88.6\n51.3\n37.1\n60.4\nw. grammar and program constraint (by \u2208 L( bG))\n69.6\n88.9\n52.4\n37.6\n60.9\nw. oracle grammar ( bG = G[y])\n95.7\n96.1\n80.0\n73.9\n94.2\nw. oracle grammar + program constraint\n95.7\n96.8\n83.6\n74.4\n96.5\nTable 1: Results on few-shot semantic parsing with Codex with various decoding strategies. GeoQuery and\nOvernight-Blk use 32 in-context examples, and SMCalFlow uses 16 examples. We show both program (Prog.)\nand execution (Exec.) accuracy when possible.\nRetrieval-based ICL\nGeoQuery Out-of-Distribution\nModel\nGeoQuery SMCalFlow Overnight-Blk\nTemplate TMCD Length NewFunc\n(# ICL examples / # retrieval set)\n(32/560)\n(16/128)\n(32/1,436)\n(32/441)\n(32/440)\n(32/440)\n(32/453)\nPrevious Work\n86.1\u2663\n60.7\u2660\n65.2\u2666\n\u2013\n\u2013\n\u2013\n\u2013\nStandard Prompting\n96.8\n60.0\n69.4\n93.2\n77.1\n86.4\n63.3\nGrammar Prompting\n97.9\n62.8\n70.2\n95.7\n86.6\n88.6\n90.8\nw. oracle grammar\n98.6\n88.9\n97.2\n97.9\n95.0\n95.7\n96.2\nTable 2: Results on retrieval-based in-context learning (left) and compositional generalization (right) with\nCodex. GeoQuery and Overnight-Blk show execution accuracy while SMCalFlow shows program accuracy.\nThe numbers with \u2663, \u2660 and \u2666 are taken from Herzig and Berant [31], Ye et al. [95] and Cao et al. [11],\nrespectively.\nwhich imbues more structural information to the exemplars by feeding the linearized derivation tree\ninstead of the surface form program.8 We use Codex-davinci-002 [13] as the base LLM for these\nmain experiments. Language models trained on code (such as Codex) have shown to be particularly\neffective on semantic parsing benchmarks [67]. We evaluate according to program accuracy (match-\ning the predicted and reference programs) as well as execution accuracy (same execution in both\nprograms) if possible.\nFew-shot results.\nThe main results are shown in Table 1. We \ufb01nd that grammar prompting can\nmeaningfully improve upon the standard prompting baseline even without constrained decoding.\nInterestingly, grammar prompting outperforms derivation tree prompting which actually provides\nmore information than the minimal specialized grammar G[y] (since the derivation tree explicitly\nshows how the rules are actually applied to obtain the program). This potentially indicates that\nhaving the LLM \u201cplan out\u201d the program by forcing it to predict the specialized grammar bG \ufb01rst is\nan effective strategy. We also analyze the effect of constrained decoding on the number of LLM API\ncalls in Table 7 of appendix A.1, where we observe that constrained decoding requires roughly three\ntimes more API calls than unconstrained decoding. However, despite the promising performance\nof grammar prompting, there is a large gap between using the predicted grammar versus using the\noracle grammar (i.e., setting bG = G[y]), indicating opportunities for further work in this area.\nRetrieval-based in-context learning.\nWhile our core target application is few-shot semantic pars-\ning, we also apply grammar prompting for retrieval-based in-context learning to test whether it can\nstill improve performance in the data-rich regime and also to compare against prior work on these\nbenchmarks. Results in Table 2 (left) show that grammar prompting can improve results even in this\nsetting, although the improvements are less pronounced than in the few-shot setting.\nOut-of-distribution generalization.\nWe experiment to see whether grammar prompting can im-\nprove compositional generalization on GeoQuery. Speci\ufb01cally, we test grammar prompting on the\ncompositional splits of GeoQuery split from Shaw et al. [66]. These splits feature structural diver-\ngence between training and test examples, e.g., programs have different templates or length. Results\n8For example, the derivation tree of a subprogram (attendee_?\nFindManager(Jean)) is linearized to\n[constraint \"(attendee_?\" [attendee \"FindManager(\" [attendee \"Jean\" \")\"] \")\"], which uses\nsquare brackets to encode richer hierarchical information than just the surface form program.\n6\nin Table 2 (right) shows that grammar prompting can improve upon standard prompting, across all\nsplits (Template, TMCD, Length).\nWe next assess whether grammar prompting can enable LLMs to make zero-shot use of un-\nseen functions (NewFunc) that are not even part of the retrieval set.\nWe set aside 8 func-\ntions (smallest, shortest, most, highest, sum, population_1, count, major) and re-\nmove them from the retrieval set, simulating a scenario where new functions are supported in\nthe backend yet no NL-program paired data is available for adapting a semantic parser. Note that\nfor GeoQuery (and Overnight-Blk), we always prepend the full DSL grammar G\u2014which includes\nthe held-out functions\u2014before the in-context exemplars. Table 2 (right-most column) shows that\ngrammar-prompted LLMs achieve signi\ufb01cantly better performance than standard prompting. Our\nresults suggest that the explicit prediction of specialized grammars elicits understanding and reason-\ning at the grammar level, thereby enabling generalization to unseen functions. We also found that\nwithout constrained generation, LLMs were often able to guess functions that did not exist but were\nnonetheless sensible. An interesting direction is to explore whether LLMs can tackle DSL-open\nbenchmarks such as LARC [1].\nBase LM\nMethod\nGeoQuery SMCalFlow Overnight-Blk\nCodex\nStandard\n83\n27\n63\nGrammar\n95\n35\n66\nGPT-3.5\nStandard\n75\n9\n49\nGrammar\n86\n5\n67\nGPT-4\nStandard\n85\n32\n56\nGrammar\n98\n36\n62\nPaLM 2-L Standard\n90\n14\n59\nGrammar\n87\n17\n62\nTable 3: Results with different base LLMs on a subset of 100\nexamples sampled from the original test set. GeoQuery and\nOvernight-Blk show execution accuracy, while SMCalFlow\nshows program accuracy.\nDifferent\nbase\nLLMs.\nWe\n\ufb01nally\nexperiment\nwith\ngrammar\nprompting\nacross different base LLMs.\nSince\nGPT-3.5\u2019s 4K token limit is smaller than\nCodex\u2019s (8K) and GPT-4\u2019s (8K) limits,\nwe use fewer exemplars in these exper-\niments than before (24/8/16 exemplars\nfor\nGeoQuery/SMCalFlow/Overnight-B\nrespectively). Due to API cost, we limit\nour experiments to a smaller subset of 100\ntest examples instead of the full test set.\nTable 3 shows that grammar prompting im-\nproves upon standard prompting in the ma-\njority of the settings. The exceptions are\nSMCalFlow with GPT-3.5 where both methods performed poorly, and GeoQuery with PaLM 2-L[7],\nwhere standard prompting already performed well.\n4.2\nClass-Speci\ufb01c Molecule Generation\nWe next demonstrate an application of grammar prompting beyond language parsing problems with\na molecule generation task. Existing methods for molecule generation typically focus on training\nspecialized neural models using large training sets [45, 37, 15, 2, 79, 61, 19]. We instead follow Guo\net al. [29] and explore a few-shot setting where the task is to generate class-speci\ufb01c molecules given\na small number of exemplars of that class. Formally, given a small set of molecules {y(i)\nc }N\ni=1\nbelonging to a particular molecule class c \u2208 {Acrylates, Chain Extenders, Isocyanates},\nour goal is to generate novel molecules yc of the same class that can be synthesized using existing\nmolecules. Since the in-context examples in this case will only consist of molecules of the same\nclass, the \u201cinput\u201d x(i)\nc\nis the empty string in this case. The data contains 32 Acrylates, 11 Chain\nExtenders, and 11 Isocyanates (see appendix G of Guo et al. [29]).\nWhile molecules can be more faithfully represented with 3D graph structure, the SMILES string\nrepresentation [87] remains popular due to its ease of use.9 The specialized grammars G[yc] (which\nare specialized from the SMILES grammar) encode various structural properties of the molecule\nthat are speci\ufb01c to the molecule class. Figure 4 shows an example of a specialized grammar and the\ncorresponding molecule in SMILES format. In this example, ring_closure ::= \"1\" speci\ufb01es the\nnumber of rings, and branch ::= \"(\" smiles \")\" speci\ufb01es whether there is a branch.\nWe test our approach by generating 100 molecules for each class and assessing the quality of the\ngenerated molecules. In addition to the standard prompting baseline, we also run the graph grammar\nbaseline from Guo et al. [29] which learns a hypergraph grammar [38] from the given molecules. We\n9Note that SMILES does not guarantee that a generated string corresponds to a valid molecule. Using our\napproach on more advanced string representations such as SELFIES [44] (which guarantee validity) remains\nan interesting avenue for future work.\n7\nSpecialized SMILES Grammar for Molecule Generation\nG[y]:\nsmiles\n::=\natom chain branch chain chain | atom chain\natom\n::=\norganic_symbol\norganic_symbol\n::=\n\"C\" | \"N\" | \"O\"\nchain\n::=\natom ring_closure bond atom | bond atom\n|\nbond atom ring_closure | atom\n|\natom bond atom bond atom\n|\nbond atom bond atom\nring_closure\n::=\n\"1\"\nbond\n::=\n\"=\"\nbranch\n::=\n\"(\" smiles \")\"\nO\nO\nO\ny: CC(= C)C(= O)OCCOC1 = CC = CC = C1\nFigure 4: Example of a specialized grammar for generating a molecule from the Acrylates class.\nAcrylates\nChain Extenders\nIsocyanates\nModel\nV\nD\nR\nM\nV\nD\nR\nM\nV\nD\nR\nM\nGraph Grammar [29]\n100 0.83 79.0 30.3\n100 0.86 72.7 98.3\n100 0.93 52.2 82.7\nStandard Prompting\n87.7 0.73 80.0 76.7\n60.3 0.89 72.7 55.7\n94.7 0.82 78.0 92.2\nGrammar Prompting\n98.0 0.74 91.0 93.3\n96.3 0.90 86.7 94.0\n97.7 0.79 78.0 96.3\nTable 4: Results for few-shot molecule generation with GPT-3.5. The metrics are validity (V), diversity (D),\nretrosynthesis score (R) and membership (M). Higher is better for all metrics.\nuse four metrics: Validity (V), the percentage of chemically valid molecules; Diversity (D), average\npairwise Tanimoto distance over Morgan \ufb01ngerprints [60]; Retrosynthesis score (R), the percentage\nof molecules that are synthesizable from existing molecules, which is computed approximately via\nthe Retro* model [12]; Membership (M), the percentage of molecules that belong to the desired\nmonomer class. We use GPT-3.5 as the base LLM and sample from the LLM without constrained\ndecoding, as constrained decoding was found to decrease the diversity of samples. See appendix A.2\nfor the full experimental setup.\nResults.\nTable 4 shows that, compared with standard prompting, grammar prompting signi\ufb01cantly\nimproves the synthesis of Acrylates and Chain Extenders across all metrics, while yielding mixed\nresults for Isocyanates. Notably, both prompting-based methods outperform the graph grammar\nbaseline in terms of Retro score, possibly due to that LLMs may have been pre-trained on existing\ndatasets of molecules, enabling them to effectively generate synthesizable molecules. In contrast,\nthe baseline method cannot incorporate any external knowledge beyond the 11 or 32 molecules\nprovided. Our preliminary results indicate that LLMs can serve as a useful tool for generating string\nrepresentations of chemical structures (and potentially other biological/chemical structures).\n4.3\nAction Subset Selection for Ef\ufb01cient PDDL Planning\nOur \ufb01nal experiments show how grammar prompting can improve the ef\ufb01ciency of classical AI\nplanners. Classical planning is the problem of \ufb01nding a sequence of actions (i.e., a plan) that\ngoes from an initial state s0 to a goal state sg.\nAn action is represented by a ground opera-\ntor (e.g., unstack(block1, block2) which consists of an operator unstack along with two ob-\nject arguments). We additionally consider macro-operators which can potentially speed up plan-\nning [9].10 Planning tasks, along with actions, are represented in Planning Domain De\ufb01nition Lan-\nguage (PDDL) [27]. We explore how grammar prompted LLMs can help guide classical planning\nalgorithms.\nWe design specialized grammars to provide guidance to the classical greedy best-\ufb01rst search (GBFS)\nalgorithm [5] by selecting a set of relevant actions. Figure 5 illustrates an example of such a spe-\ncialized grammar, which captures all the necessary actions for the \ufb01nal plan y that solves the given\ntask. The process of the guided planning consists of the following steps: (1) given a task, predict\na specialized grammar G[y]; (2) use the specialized grammar G[y] to subsequently generate a plan\nwithin the restricted action space derived from G[y]; (3) initialize GBFS\u2019s priority queue with the\nLLM-generated plan, and (4) search for the \ufb01nal plan in the restricted action space. Our setup builds\nupon the idea of using an LLM-generated plan to initialize GBFS from Silver et al. [69], which has\n10For example, pickup-and-stack(A, B) is a combination of pickup(A) and stack(A, B).\n8\nSpecialized Action Grammar for PDDL Planning\ns0:\nA\nB\nC\nD\nsg:\nA\nC\nB\nD\nG[y]:\nplan\n::=\nground_operator+\nground_operator\n::=\n\"pick_up-and-stack(\" block block \")\"\n|\n\"unstack-and-put_down(\" block block \")\"\nblock\n::=\n\"A\" | \"B\" | \"C\"\ny: unstack-and-put_down(B, A); pick_up-and-stack(C, A); pick_up-and-stack(B, C)\nFigure 5: Example of a specialized grammar for PDDL planning in the Blocks domain. Given an input\nx = (s0, sg), the specialized grammar G[y] only includes necessary actions for solving this task.\nBlocks\nDepot\nSatellite\nApproach\nCreated Expanded Success\nCreated Expanded Success\nCreated Expanded Success\nGBFS + Prim. (No LLM)\n360\n188\n1.0\n18047\n3870\n0.4\n8205\n150\n1.0\nStandard + Prim.\n348\n180\n1.0\n17597\n4039\n0.4\n6686\n78\n1.0\nGrammar + Prim.\n251\n124\n1.0\n15033\n3641\n0.4\n5162\n64\n1.0\nStandard + Macro.\n850\n16\n1.0\n1460\n56\n0.4\n4003\n27\n0.3\nGrammar + Macro.\n170\n9\n1.0\n2917\n127\n0.8\n3665\n46\n0.9\nStandard + Min Macro.\n228\n8\n1.0\n1903\n65\n0.6\n3483\n35\n0.8\nTable 5: Results on PDDL planning. Created/Expanded refer to the number of nodes during planning (lower is\nbetter). Success refers to success rate (higher is better). Numbers are averaged over three runs using GPT-3.5.\na simpler two-step process: (1) given a task, predict a plan via standard prompting, and (2) utilize\nthis plan to guide GBFS. We use their method as our baseline.\nFollowing Silver et al. [69], we create a similar few-shot setting for LLM planning, using 5 tasks\nas in-context examples and 10 tasks for evaluation from Pyperplan [5]. We test our approach on\n3 classic domains in PDDL planning, including Blocks, Depot and Satellite. For the action space,\nwe use either a set of primitive actions (Prim) or an augmented set with macro actions (Macro).\nIn addition to standard prompting, we add two more baselines: (1) No LLM: planning with the\nentire set of actions; (2) Min Macro: where we construct a minimal set of macro actions for each\ndomain by selecting actions from existing plans for the training tasks. The Min Macro baseline is a\ndomain-speci\ufb01c method to reduce the action space. By comparing to Min Macro, we can verify the\neffectiveness of instance-speci\ufb01c v.s. domain-speci\ufb01c action selection. See appendix A.3 for more\ndetails.\nResults.\nWe evaluate the ef\ufb01ciency of planning in terms of the number of search nodes created/-\nexpanded, as well as the success rate. Table 5 shows the promising performance of LLM-guided\nplanning via grammar prompting. In Blocks, grammar prompting signi\ufb01cantly improves ef\ufb01ciency\nwhile maintaining 100% success rate. In Depot, grammar prompting with macro actions improves\nthe success rate by 20% over the best competing baseline. In Satellite, using primitive actions yields\nthe best performance with 100% success rate and a reduction of 57% expanded nodes comparing to\nthe No LLM baseline. While our experiments are not intended to complete with the state-of-the-art\nalgorithms for fast planning [20\u201322, 32, 25, 84], they indicate the promise of LLMs for improving\nexisting planning algorithms.\n5\nDiscussion and Limitations\nWe discuss several limitations of our approach including some negative results. Grammar prompting\ndid not yield any improvements for DSLs that were likely to have been frequently encountered dur-\ning pretraining (e.g., regular expressions, SQL). Moreover, constrained generation based on special-\nized grammars led to increased API calls, and was not always bene\ufb01cial for tasks beyond semantic\nparsing. For instance, in molecule generation we discovered that enforcing constraints can some-\ntimes result in lower diversity. Additionally, in PDDL planning we observed that the constraints\napplied to prune objects can sometimes negatively impact performance, suggesting that relevant ob-\nject selection is still very challenging for LLMs. It may be interesting to explore whether \ufb01netuning\n9\nof moderately-sized LLMs using specialized grammars can lead to better grammar-based models\nfor DSL generation.\nOn the positive front, our work demonstrates that LLMs have the capacity to understand and generate\nmetalanguages. Working in this \u201cmetalanguage space\u201d can be combined with chain-of-thought-style\n[86] prompts by, for example, manually providing natural language comments to the rules of the\nspecialized grammars. We found this to improve results slightly on semantic parsing (see Figure 6\nof appendix A.1). Moreover, many scienti\ufb01c problems can be formally approached by representing\nhypotheses as DSL programs [71], and DSLs can enable easier encoding of human prior knowledge\nand scienti\ufb01c principles, providing a foundation for scienti\ufb01c discovery. Recent work shows that\nstate-of-the-art LLMs can follow previously unseen formal systems [75]. Techniques like grammar\nprompting can widen the scope of scienti\ufb01c problems for which LLMs could be effectively applied\nby more explicitly accounting for external knowledge and constraints.\n6\nRelated Work\nChain-of-thought prompting.\nGrammar prompting extends a recent line of work on improving\nreasoning capabilities by requesting explicit reasoning steps as part of the prompt [51, 24, 86, 80,\n14, 94]. Our approach is closely related to concurrent work on employing symbolic variables as part\nof the prompt [30, 50, 33, 97, 52], though we are not aware of any existing work that uses formal\ngrammars as the intermediate reasoning step.\nLLMs for program generation and semantic parsing.\nGenerating programs from natural lan-\nguage speci\ufb01cations, a task often referred to as semantic parsing, is a sub-problem of program\nsynthesis; for surveys, see Kamath and Das [39] and Gulwani et al. [28]. Recent works [8, 89] have\nexplored using LLMs for generating code in general-purpose programming languages (e.g., Python).\nOur work further extends this line by examining whether LLMs can generate DSL programs, which\nare intrinsically scarce. There has also been work on using LLMs for tool usage via further train-\ning [63] or prompting [56, 77], investigating how model scales [57] and retrievers [96, 46] affect\nin-context learning for semantic parsing, and constrained decoding [64, 68, 55] for program genera-\ntion.\nNeural grammars.\nGrammar prompting can also been seen as a \u201cfully LLM\u201d instantiation of a\nline of work on neural parameterizations of symbolic grammars [35, 17, 43, 42, 36, 100, 92, 91, 93].\nIndeed, our approach to semantic parsing essentially uses prompt-based learning to de\ufb01ne a quasi-\nsynchronous grammar [70, 78] whose rules dynamically depend on the source sentence. Concretely,\nin contrast to recent works which embed learnable neural components within synchronous grammars\n[41, 23, 76], grammar prompting relies on the implicit in-context learning capabilities of LLMs for\nthe learning component. (However unlike these works, our conditional grammar does not explicitly\nalign its rules to the subparts of the source sentence).\nGrammar-based molecule generation.\nGrammar-based methods have gained signi\ufb01cant interest\nin the realm of molecule generation, offering advantages in interpretability, data-ef\ufb01ciency, and\ncontrollability. One line of research involves integrating generic SMILES grammars with neural\nnetworks to generate syntactically correct molecules [45, 15]. Another approach centers on data-\ndriven induction of grammars for generation [29, 38]. Our work aligns with the former, viewing\ngrammar prompting as a straightforward method for integrating grammar into an LLM without the\nneed for additional training.\nLLMs for planning.\nRecently, LLMs have been increasingly studied in the context of planning\nfor autonomous agents. When given goals expressed in natural language in household environments,\nearlier works [3, 65, 34, 48] directly prompted LLMs to predict executable actions. However, in\nPDDL domains, recent works [69, 74] showed that LLMs underperform classical planners if the\ndesired action sequences are very long. Grammar prompting represents a promising strategy for\naugmenting classical planners with LLMs to get the best of both worlds. Other related efforts include\ntranslating between problems and PDDL models [49] and corrective re-prompting [58]. Besides\nusing LLMs, integrating learning and planning has been extensively studied in the past literature,\ne.g., learning actions [4, 82], skills [84], macro-actions [9], rules [88] and guidance strategies [90,\n83, 40] for more ef\ufb01cient planning.\n10\n7\nConclusion\nWe propose grammar prompting as a simple approach for improving few-shot DSL generation with\nlarge language models. Experiments across a range of structured languages including DSLs for se-\nmantic parsing (SMCalFlow, GeoQuery, Overnight), PDDL planning (action DSL), and molecule\ngeneration (SMILES), show that grammar prompting can improve upon standard prompting base-\nlines. The encouraging results in semantic parsing indicate its potential to assist LLMs with tool\nusage, and the promising results in other domains indicate that grammar prompting can enable ap-\nplication of LLMs in domains that intrinsically depend on DSLs.\nAcknowledgments\nWe thank Jacob Andreas, Gabriel Grand, Linlu Qiu, Tom Silver, and Hunter Lang for helpful discus-\nsion and feedback. This study was supported by funds from the Google-MIT research collaborations\nprogram and the GIST-MIT joint research program.\nReferences\n[1] Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong,\nGabrielle Ecanow, Maxwell Nye, Michael Tessler, and Josh Tenenbaum. Communicating\nnatural programs to humans and machines. Proceedings of NeurIPS, 35:3731\u20133743, 2022.\n[2] Walid Ahmad, Elana Simon, Seyone Chithrananda, Gabriel Grand, and Bharath Ramsundar.\nChemberta-2: Towards chemical foundation models. arXiv preprint arXiv:2209.01712, 2022.\n[3] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David,\nChelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as I can,\nnot as I say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691,\n2022.\n[4] Diego Aineto, Sergio Jim\u00e9nez, and Eva Onaindia. Learning STRIPS action models with\nclassical planning. In Proceedings of the International Conference on Automated Planning\nand Scheduling, volume 28, pages 399\u2013407, 2018.\n[5] Yusra Alkhazraji, Matthias Frorath, Markus Gr\u00fctzner, Malte Helmert, Thomas Liebetraut,\nRobert Mattm\u00fcller, Manuela Ortlieb, Jendrik Seipp, Tobias Springenberg, Philip Stahl, and\nJan W\u00fcl\ufb01ng. Pyperplan (v1.3), 2020. URL https://doi.org/10.5281/zenodo.3701399.\n[6] Jacob Andreas, John Bufe, David Burkett, Charles Chen, Josh Clausman, Jean Crawford,\nKate Crim, Jordan DeLoach, Leah Dorner, Jason Eisner, Hao Fang, Alan Guo, David Hall,\nKristin Hayes, Kellie Hill, Diana Ho, Wendy Iwaszuk, Smriti Jha, Dan Klein, Jayant Krish-\nnamurthy, Theo Lanman, Percy Liang, Christopher H. Lin, Ilya Lintsbakh, Andy McGovern,\nAleksandr Nisnevich, Adam Pauls, Dmitrij Petters, Brent Read, Dan Roth, Subhro Roy, Jesse\nRusak, Beth Short, Div Slomin, Ben Snyder, Stephon Striplin, Yu Su, Zachary Tellman, Sam\nThomson, Andrei Vorobev, Izabela Witoszko, Jason Wolfe, Abby Wray, Yuchen Zhang, and\nAlexander Zotov. Task-oriented dialogue as data\ufb02ow synthesis. Transactions of the Asso-\nciation for Computational Linguistics, 8:556\u2013571, 2020. doi: 10.1162/tacl_a_00333. URL\nhttps://aclanthology.org/2020.tacl-1.36.\n[7] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Pas-\nsos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. PaLM 2 technical\nreport. arXiv preprint arXiv:2305.10403, 2023.\n[8] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large\nlanguage models. arXiv preprint arXiv:2108.07732, 2021.\n[9] Adi Botea, Markus Enzenberger, Martin M\u00fcller, and Jonathan Schaeffer. Macro-ff: Improv-\ning AI planning with automatically learned macro-operators. Journal of Arti\ufb01cial Intelligence\nResearch, 24:581\u2013621, 2005.\n11\n[10] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In Proceedings of NeurIPS, 2020.\n[11] Ruisheng Cao,\nSu Zhu,\nChen Liu,\nJieyu Li,\nand Kai Yu.\nSemantic parsing\nwith dual learning.\nIn Proceedings of ACL, pages 51\u201364, Florence, Italy, July\n2019. Association for Computational Linguistics.\ndoi:\n10.18653/v1/P19-1007.\nURL\nhttps://aclanthology.org/P19-1007.\n[12] Binghong Chen, Chengtao Li, Hanjun Dai, and Le Song. Retro*: Learning retrosynthetic\nplanning with neural guided A* search. In Proceedings of ICML, pages 1608\u20131616. PMLR,\n2020.\n[13] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code.\n2021.\n[14] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. Program of thoughts\nprompting: Disentangling computation from reasoning for numerical reasoning tasks. arXiv\npreprint arXiv:2211.12588, 2022.\n[15] Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational\nautoencoder for structured data. arXiv preprint arXiv:1802.08786, 2018.\n[16] David Dohan, Winnie Xu, Aitor Lewkowycz, Jacob Austin, David Bieber, Raphael Gontijo\nLopes, Yuhuai Wu, Henryk Michalewski, Rif A. Saurous, Jascha Sohl-dickstein, Kevin Mur-\nphy, and Charles Sutton. Language Model Cascades. arXiv:2207.10342, 2022.\n[17] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proceedings of NAACL, 2016.\n[18] Jay Earley. An ef\ufb01cient context-free parsing algorithm. Communications of the ACM, 13(2):\n94\u2013102, 1970.\n[19] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji.\nTranslation between\nmolecules and natural language. arXiv preprint arXiv:2204.11817, 2022.\n[20] Richard E Fikes and Nils J Nilsson. STRIPS: A new approach to the application of theorem\nproving to problem solving. Arti\ufb01cial intelligence, 2(3-4):189\u2013208, 1971.\n[21] Maria Fox and Derek Long. PDDL2. 1: An extension to PDDL for expressing temporal\nplanning domains. Journal of Arti\ufb01cial Intelligence Research, 20:61\u2013124, 2003.\n[22] Maria Fox and Derek Long. Modelling mixed discrete-continuous domains for planning.\nJournal of Arti\ufb01cial Intelligence Research, 27:235\u2013297, 2006.\n[23] Dan Friedman, Alexander Wettig, and Danqi Chen. Finding Dataset Shortcuts with Grammar\nInduction. In Proceedings of EMNLP, 2022.\n[24] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan,\nand Graham Neubig. PAL: Program-aided Language Models. arXiv:2211.10435, 2022.\n[25] Caelan R. Garrett, Tomas Lozano-Perez, and Leslie P. Kaelbling. PDDLStream: Integrat-\ning symbolic planners and blackbox samplers. In International Conference on Automated\nPlanning and Scheduling (ICAPS), 2020. URL https://arxiv.org/abs/1802.08705.\n12\n[26] Saibo Geng, Martin Josifosky, Maxime Peyrard, and Robert West. Flexible Grammar-Based\nConstrained Decoding for Language Models. arXiv:2305.13971, 2023.\n[27] Malik Ghallab, Adele Howe, Craig Knoblock, Drew McDermott, Ashwin Ram, Manuela\nVeloso, Daniel Weld, David Wilkins SRI, Anthony Barrett, Dave Christianson, et al. PDDL\n\u2013 the planning domain de\ufb01nition language. Technical Report CVC TR98003/DCS TR1165.\nNew Haven, CT: Yale Center for Computational Vision and Control., 1998.\n[28] Sumit Gulwani, Oleksandr Polozov, Rishabh Singh, et al. Program synthesis. Foundations\nand Trends\u00ae in Programming Languages, 4(1-2):1\u2013119, 2017.\n[29] Minghao Guo, Veronika Thost, Beichen Li, Payel Das, Jie Chen, and Wojciech Matusik.\nData-ef\ufb01cient graph grammar learning for molecular generation. In Proceedings of ICLR,\n2022.\n[30] Joy He-Yueya, Gabriel Poesia, Rose E Wang, and Noah D Goodman. Solving math word prob-\nlems by combining language models with symbolic solvers. arXiv preprint arXiv:2304.09102,\n2023.\n[31] Jonathan Herzig and Jonathan Berant.\nSpan-based semantic parsing for compositional\ngeneralization.\nIn Proceedings of ACL, pages 908\u2013921, Online, August 2021. As-\nsociation for Computational Linguistics.\ndoi:\n10.18653/v1/2021.acl-long.74.\nURL\nhttps://aclanthology.org/2021.acl-long.74.\n[32] J\u00f6rg Hoffmann. The metric-ff planning system: Translating\u201cignoring delete lists\u201dto numeric\nstate variables. Journal of Arti\ufb01cial Intelligence Research, 20:291\u2013341, 2003.\n[33] Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, and Yue Zhang.\nChain-of-symbol\nprompting elicits planning in large langauge models. arXiv preprint arXiv:2305.10276, 2023.\n[34] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as\nzero-shot planners: Extracting actionable knowledge for embodied agents. In Proceedings of\nICML, pages 9118\u20139147. PMLR, 2022.\n[35] Yong Jiang, Wenjuan Han, Kewei Tu, et al. Unsupervised neural dependency parsing. Asso-\nciation for Computational Linguistics (ACL), 2016.\n[36] Lifeng Jin, Finale Doshi-Velez, Timothy Miller, Lane Schwartz, and William Schuler. Unsu-\npervised learning of PCFGs with normalizing \ufb02ow. In Proceedings of ACL, Florence, Italy,\nJuly 2019. Association for Computational Linguistics.\n[37] Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder\nfor molecular graph generation. In Proceedings of ICML, pages 2323\u20132332. PMLR, 2018.\n[38] Hiroshi Kajino. Molecular hypergraph grammar with its application to molecular optimiza-\ntion. In Proceedings of ICML, pages 3183\u20133191. PMLR, 2019.\n[39] Aishwarya Kamath and Rajarshi Das.\nA survey on semantic parsing.\narXiv preprint\narXiv:1812.00978, 2018.\n[40] Beomjoon Kim, Zi Wang, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Learning to\nguide task and motion planning using score-space representation. The International Journal\nof Robotics Research, 38(7):793\u2013812, 2019.\n[41] Yoon Kim. Sequence-to-sequence learning with latent neural grammars. In Proceedings of\nNeurIPS, pages 26302\u201326317, 2021.\n[42] Yoon Kim, Chris Dyer, and Alexander Rush. Compound probabilistic context-free grammars\nfor grammar induction. In Proceedings of ACL, Florence, Italy, July 2019. Association for\nComputational Linguistics.\n[43] Yoon Kim, Alexander Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and G\u00e1bor Melis. Unsu-\npervised recurrent neural network grammars. In Proceedings of NAACL, Minneapolis, Min-\nnesota, June 2019. Association for Computational Linguistics.\n13\n[44] Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik.\nSelf-referencing embedded strings (sel\ufb01es): A 100% robust molecular string representation.\nMachine Learning: Science and Technology, 1(4):045024, oct 2020.\n[45] Matt J Kusner, Brooks Paige, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Grammar variational\nautoencoder. In Proceedings of ICML, pages 1945\u20131954. PMLR, 2017.\n[46] Itay Levy, Ben Bogin, and Jonathan Berant. Diverse demonstrations improve in-context com-\npositional generalization. arXiv preprint arXiv:2212.06800, 2022.\n[47] Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia, Yu Liu, Yang Ou, Shuai\nLu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming Gong, and Nan Duan.\nTaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs.\narXiv:2303.16434, 2023.\n[48] Bill Yuchen Lin, Chengsong Huang, Qian Liu, Wenda Gu, Sam Sommerer, and Xiang\nRen.\nOn grounded planning for embodied tasks with language models.\narXiv preprint\narXiv:2209.00465, 2022.\n[49] Bo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter\nStone. Llm+ p: Empowering large language models with optimal planning pro\ufb01ciency. arXiv\npreprint arXiv:2304.11477, 2023.\n[50] Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apid-\nianaki, and Chris Callison-Burch.\nFaithful chain-of-thought reasoning.\narXiv preprint\narXiv:2301.13379, 2023.\n[51] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Jacob Austin,\nDavid Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul Bosma, David Luan,\nCharles Sutton, and Augustus Odena. Show your work: Scratchpads for intermediate compu-\ntation with language models. arXiv:2112.00114, 2021.\n[52] Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang Wang.\nLogic-LM: Em-\npowering Large Language Models with Symbolic Solvers for Faithful Logical Reasoning.\narXiv:2305.12295, 2023.\n[53] Bhargavi Paranjape, Scott Lundberg anbd Sameer Singh, Hannaneh Hajishirzi, Luke Zettle-\nmoyer, and Marco Tulio Ribeiro. ART: Automatic multi-step reasoning and tool-use for large\nlanguage models. arXiv:2303.09014, 2023.\n[54] Emmanouil Antonios Platanios, Adam Pauls, Subhro Roy, Yuchen Zhang, Alexander Kyte,\nAlan Guo, Sam Thomson, Jayant Krishnamurthy, Jason Wolfe, Jacob Andreas, and Dan Klein.\nValue-agnostic conversational semantic parsing. In Proceedings of ACL, August 2021.\n[55] Gabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and\nSumit Gulwani. Synchromesh: Reliable Code Generation from Pre-trained Language Models.\nProceedings of ICLR, 2022.\n[56] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng,\nYufei Huang, Chaojun Xiao, Chi Han, et al. Tool learning with foundation models. arXiv\npreprint arXiv:2304.08354, 2023.\n[57] Linlu Qiu, Peter Shaw, Panupong Pasupat, Tianze Shi, Jonathan Herzig, Emily Pitler, Fei Sha,\nand Kristina Toutanova. Evaluating the impact of model scale for compositional generaliza-\ntion in semantic parsing. In Proceedings of EMNLP, December 2022.\n[58] Shreyas Sundara Raman, Vanya Cohen, Eric Rosen, Ifrah Idrees, David Paulius, and Stefanie\nTellex. Planning with large language models via corrective re-prompting. arXiv preprint\narXiv:2211.09935, 2022.\n[59] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084, 2019.\n14\n[60] David Rogers and Mathew Hahn. Extended-connectivity \ufb01ngerprints. Journal of chemical\ninformation and modeling, 50(5):742\u2013754, 2010.\n[61] Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou\nHuang. Self-supervised graph transformer on large-scale molecular data. Proceedings of\nNeurIPS, 33:12559\u201312571, 2020.\n[62] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, et al. Multitask prompted train-\ning enables zero-shot task generalization. In Proceedings of ICLR, 2022.\n[63] Timo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language Models Can Teach\nThemselves to Use Tools. arXiv:2302.04761, 2023.\n[64] Torsten Scholak, Nathan Schucher, and Dzmitry Bahdanau. PICARD: Parsing incrementally\nfor constrained auto-regressive decoding from language models. In Proceedings of EMNLP,\nOnline and Punta Cana, Dominican Republic, November 2021. Association for Computa-\ntional Linguistics.\n[65] Pratyusha Sharma, Antonio Torralba, and Jacob Andreas. Skill induction and planning with\nlatent language. arXiv preprint arXiv:2110.01517, 2021.\n[66] Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. Compositional\ngeneralization and natural language variation: Can a semantic parsing approach handle both?\nIn Proceedings of ACL, August 2021.\n[67] Richard Shin and Benjamin Van Durme. Few-shot semantic parsing with language models\ntrained on code. In Proceedings of NAACL, pages 5417\u20135425, Seattle, United States, July\n2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.396.\nURL https://aclanthology.org/2022.naacl-main.396.\n[68] Richard Shin, Christopher Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Anto-\nnios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. Constrained\nlanguage models yield few-shot semantic parsers. In Proceedings of EMNLP. Association for\nComputational Linguistics, November 2021.\n[69] Tom Silver, Varun Hariprasad, Reece S Shuttleworth, Nishanth Kumar, Tom\u00e1s Lozano-P\u00e9rez,\nand Leslie Pack Kaelbling.\nPDDL planning with pretrained large language models.\nIn\nNeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.\n[70] David Smith and Jason Eisner. Quasi-Synchronous Grammars: Alignment by Soft Projection\nof Syntactic Dependencies. In Proceedings on the Workshop on Statistical Machine Transla-\ntion, 2006.\n[71] Jennifer J Sun, Megan Tjandrasuwita, Atharva Sehgal, Armando Solar-Lezama, Swarat\nChaudhuri, Yisong Yue, and Omar Costilla-Reyes. Neurosymbolic programming for science.\narXiv preprint arXiv:2210.05050, 2022.\n[72] Didac Suris, Sachit Menon, and Carl Vondrick. ViperGPT: Visual Inference via Python Exe-\ncution for Reasoning. arXiv:2303.08128, 2023.\n[73] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won\nChung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Chal-\nlenging big-bench tasks and whether chain-of-thought can solve them. arXiv:2210.09261,\n2022.\n[74] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large\nlanguage models still can\u2019t plan (a benchmark for llms on planning and reasoning about\nchange). arXiv preprint arXiv:2206.10498, 2022.\n[75] Gregor vom Scheidt. Experimental results from applying GPT-4 to an unpublished formal\nlanguage. arXiv:2305.12196, 2023.\n15\n[76] Bailin Wang, Ivan Titov, Jacob Andreas, and Yoon Kim. Hierarchical Phrase-based Sequence-\nto-Sequence Learning. In Proceedings of EMNLP, 2022.\n[77] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi\nFan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language\nmodels. arXiv preprint arXiv:2305.16291, 2023.\n[78] Mengqiu Wang, Noah A. Smith, and Teruko Mitamura. What is the Jeopardy Model? A\nQuasi-Synchronous Grammar for QA. In Proceedings of EMNLP, 2007.\n[79] Sheng Wang, Yuzhi Guo, Yuhong Wang, Hongmao Sun, and Junzhou Huang. Smiles-bert:\nlarge scale unsupervised pre-training for molecular property prediction. In Proceedings of\nthe 10th ACM international conference on bioinformatics, computational biology and health\ninformatics, pages 429\u2013436, 2019.\n[80] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha\nChowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in lan-\nguage models. In Proceedings of ICLR, 2023.\n[81] Yushi Wang, Jonathan Berant, and Percy Liang. Building a semantic parser overnight. In\nProceedings of ACL, Beijing, China, July 2015.\n[82] Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Focused model-\nlearning and planning for non-Gaussian continuous state-action systems. In 2017 IEEE Inter-\nnational conference on robotics and automation (ICRA), pages 3754\u20133761. IEEE, 2017.\n[83] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Active\nmodel learning and diverse action sampling for task and motion planning. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems (IROS), pages 4107\u20134114. IEEE,\n2018.\n[84] Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Learning\ncompositional models of robot skills for task and motion planning. The International Journal\nof Robotics Research, 40(6-7):866\u2013894, 2021.\n[85] Jason Wei, Maarten Paul Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester,\nNan Du, Andrew Mingbo Dai, and Quoc V. Le. Finetuned language models are zero-shot\nlearners. In Proceedings of ICLR, 2022.\n[86] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi,\nQuoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language\nmodels. In Proceedings of NeurIPS, 2022.\n[87] David Weininger. Smiles, a chemical language and information system. 1. introduction to\nmethodology and encoding rules. Journal of chemical information and computer sciences,\n28(1):31\u201336, 1988.\n[88] Victoria Xia, Zi Wang, and Leslie Pack Kaelbling. Learning sparse relational transition mod-\nels. In International Conference on Learning Representations, 2019.\n[89] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. A systematic evalua-\ntion of large language models of code. In Proceedings of the 6th ACM SIGPLAN International\nSymposium on Machine Programming, pages 1\u201310, 2022.\n[90] Ryan Yang, Tom Silver, Aidan Curtis, Tomas Lozano-Perez, and Leslie Pack Kaelbling. Pg3:\nPolicy-guided planning for generalized policy generation. arXiv preprint arXiv:2204.10420,\n2022.\n[91] Songlin Yang, Yanpeng Zhao, and Kewei Tu. Neural bi-lexicalized PCFG induction. In\nProceedings of ACL, Online, August 2021. Association for Computational Linguistics.\n[92] Songlin Yang, Yanpeng Zhao, and Kewei Tu. PCFGs can do better: Inducing probabilistic\ncontext-free grammars with many symbols. In Proceedings of NAACL, 2021.\n16\n[93] Songlin Yang, Roger P Levy, and Yoon Kim. Unsupervised discontinuous constituency pars-\ning with mildly context-sensitive grammars. arXiv preprint arXiv:2212.09140, 2022.\n[94] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Grif\ufb01ths, Yuan Cao, and\nKarthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language mod-\nels. arXiv preprint arXiv:2305.10601, 2023.\n[95] Jiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu, and Lingpeng Kong. Compositional exem-\nplars for in-context learning. arXiv preprint arXiv:2302.05698, 2023.\n[96] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Benchmarking multimodal regex synthe-\nsis with complex structures. In Proceedings of ACL. Association for Computational Linguis-\ntics, July 2020.\n[97] Xi Ye, Qiaochu Chen, Isil Dillig, and Greg Durrett. Satis\ufb01ability-aided language models\nusing declarative prompting. arXiv preprint arXiv:2305.09656, 2023.\n[98] Pengcheng Yin, Hao Fang, Graham Neubig, Adam Pauls, Emmanouil Antonios Platanios,\nYu Su, Sam Thomson, and Jacob Andreas. Compositional generalization for neural semantic\nparsing via span-level supervised attention. Proceedings of ACL, 2021.\n[99] John M Zelle and Raymond J Mooney. Learning to parse database queries using inductive\nlogic programming. In Proceedings of the national conference on arti\ufb01cial intelligence, pages\n1050\u20131055, 1996.\n[100] Hao Zhu, Yonatan Bisk, and Graham Neubig. The return of lexical dependencies: Neural\nlexicalized PCFGs. Transactions of the Association for Computational Linguistics, 8, 2020.\n17\nFew-Shot\nRetrieval-based\nGeoQuery Out-of-Dist.\nGeoQuery SMCal\ufb02ow Overnight-Blk GeoQuery SMCal\ufb02ow Overnight-Blk Template TMCD Length NewFunc\nTrain\n32\n16\n32\n560\n128\n1436\n441\n440\n440\n453\nTest\n280\n360\n399\n280\n360\n399\n439\n440\n440\n447\nTable 6: Statistics of the splits used for experiments on semantic parsing.\nApproach\nGeoQuery\nSMCalFlow\nOvernight-B\nStandard Prompting (unconstrained decoding)\n81.5 (1.0)\n46.4 (1.0)\n54.7 (1.0)\nw. constrained decoding (by \u2208 L(G))\n81.8 (4.3)\n49.2 (5.6)\n54.7 (1.6)\nLinearized Derivation Tree Prompting\n77.5 (1.0)\n50.0 (1.0)\n56.4 (1.0)\nGrammar Prompting (unconstrained decoding)\n87.5 (1.0)\n50.8 (1.0)\n57.4 (1.0)\nw. grammar constraint ( bG \u2286 G)\n88.6 (3.0)\n51.3 (3.0)\n60.4 (1.4)\nw. grammar and program constraint (by \u2208 L( bG))\n88.9 (3.3)\n52.4 (3.3)\n60.9 (2.8)\nw. oracle grammar ( bG = G[y])\n96.1 (1.3)\n80.0 (1.0)\n94.2 (1.0)\nw. oracle grammar + program constraint\n96.8 (2.1)\n83.6 (2.6)\n96.5 (1.0)\nTable 7: Extended results which show the number of Codex calls per example on few-shot semantic parsing in\nbrackets. Columns in grey show program accuracy, while white columns others indicate execution accuracy.\nA\nExperiment Details\nA.1\nSemantic Parsing\nStatistics and Splits.\nWe show the statistics for the splits used for the experiments in Table 6.\nFor GeoQuery, we utilize the standard split from Zelle and Mooney [99] in the retrieval-based setting\nand the Template, TMCD, and Length splits from Shaw et al. [66]. We randomly sample 32 exam-\nples from the training set of the standard split to create the few-shot split. To generate the NewFunc\nsplit, we designate examples utilizing the following eight functions as test examples: smallest,\nshortest, most, highest, sum, population_1, count, major; the remaining examples are\nincorporated into the training set.\nFor SMCalFlow, we adopt the 16-shot and 128-shot cross-domain settings from Yin et al. [98] as\nour few-shot and retrieval-based settings, respectively. It is noteworthy that the training set of the\noriginal splits contains approximately 25k in-domain training examples. Previous work [96, 57] uti-\nlized these examples as their retrieval set. However, in our experiments, we found that incorporating\nin-domain examples did not enhance performance. Consequently, we use 16/128 cross-domain ex-\namples as our training set in the few-shot and retrieval settings, respectively. For all experiments on\nSMCalFlow, we used the preprocessed version from Qiu et al. [57], which employs a more concise\nLISPRESS format [54] than the original version [98]. This format aligns with Ye et al. [96] for a\nfair comparison.\nFor Overnight-Blocks, we employ the standard split from Wang et al. [81] in the retrieval setting.\nWe randomly sample 32 examples from the training set of the standard split to create the few-shot\nsplit.\nScoring Functions for Constrained Generation.\nFor each candidate continuation w \u2208 \u03a3[ypre\ufb01x]\nfor correction, we \ufb01rst form a partial program via concatenation ypre\ufb01x\u00b7w and then feed it into Codex\nto obtain the score for the candidate via\nlog PLLM(w | bG, x, ypre\ufb01x, (x(i), G[y(i)], y(i))N\ni=1).\nIn the case where w consists of multiple BPE tokens, e.g., FindManger( is tokenized into Find,\nManager, and (, we average the token-level log-likelihood to obtain a candidate-level score. How-\never, when the size of \u03a3[ypre\ufb01x] exceeds 16, invoking Codex for each candidate becomes too expen-\nsive. To address this issue, we employ SentenceBERT to select 16 most plausible candidates \ufb01rst\nvia a dot product,\n(SentenceBERT(\u02c6yt))\u22a4(SentenceBERT(ypre\ufb01x \u00b7 w)),\nwhere SentenceBERT yields the embeddings for the incorrect prediction \u02c6yt and a candidate of cor-\nrected partial program ypre\ufb01x \u00b7 w.\n18\nMolecule Class\nTemperature Presence Penalty Frequency Penalty\nSampling specialized grammars bG\nAcrylates\n0.6\n0.1\n0.1\nChain Extenders\n0.6\n0.1\n0.1\nIsocyanates\n0.6\n0.4\n0.4\nSampling molecules by\nAcrylates\n0.6\n0.1\n0.1\nChain Extenders\n0.6\n0.1\n0.1\nIsocyanates\n0.3\n0.4\n0.4\nTable 8: Hyperparameters for sampling specialized grammars bG (top) and the molecules by in grammar prompt-\ning for molecule generation. Standard prompting uses the same hyperparameters for y.\nThe functionality of obtaining the log-likelihood for a candidate continuation given a pre\ufb01x is ap-\nplicable via Codex APIs 11 via setting logprobs=True and echo=True. Unfortunately, subsequent\nmodels (e.g., GPT-3.5 and GPT-4) disable such functionality. As a workaround, we simply use the\nscoring function based on SentenceBERT to directly select the best candidate in our PDDL planning\nexperiments.\nCost Ef\ufb01ciency.\nWe assess various decoding strategies for their cost ef\ufb01ciency, focusing on the\nnumber of API calls. The number of Codex calls resulting from the few-shot semantic parsing\nexperiments is presented in Figure 7, alongside the corresponding accuracy metrics. The results\nindicate that standard prompting under constrained decoding leads to a signi\ufb01cantly higher number\nof Codex calls. Similarly, grammar-prompting with constraints also results in an increased number\nof Codex calls. However, when employing both grammar and program constraints, the number\nof calls decreases meaningfully in comparison to standard prompting under constrained decoding.\nFuture work might consider exploring strategies for more cost-ef\ufb01cient constrained decoding.\nGrammar Prompting with Annotated Rules.\nWe have additionally experimented with enhanc-\ning BNF rules by appending natural language comments. As illustrated in Figure 6, we pair each\nBNF rule with its corresponding natural language phrases extracted from the given query x. These\nmanually annotated comments yield an explicit correspondence between natural language phrases\nand their corresponding BNF rules, thereby better facilitating interpretation and application of the\ngrammars for generating programs. When employing the augmented grammar prompting, we no-\nticed marginal improvements on SMCalFlow (+1.0%) and Overnight-Blocks (0.5%), with no ob-\nserved enhancement on GeoQuery. While the gains might not appear signi\ufb01cant, this predicted\nalignment could potentially contribute to improved interpretability and further constraints on gen-\neration. For instance, the phrase \u201csomeone\u2019s manager\u201d should consistently trigger the function\nFindManager(. We leave the exploration of utilizing the augmented rules for future work.\nA.2\nMolecule Generation\nSampling Procedure\nDifferent from semantic parsing and PDDL planning, where the most proba-\nble program y needs to be found via arg max inference, molecule generation has empty speci\ufb01cation\nx and requires sampling from a prompting-based distribution. The sampling procedure for grammar\nprompting consists of three stages: (1) we randomly sample a permutation of given molecules, de-\nnoted as \u03c0, (2) based on a prompt formed by the permutation, we sample a specialized grammar bG\nvia\nbG \u223c PLLM(G\u2032 | x, (G[y(\u03c0[i])], y(\u03c0[i]))N\ni=1),\niii) we \ufb01nally obtain the molecule conditioned bG,\nby \u223c PLLM(y | bG, (G[y(\u03c0[i])], y(\u03c0[i]))N\ni=1).\nWe list the hyperparameters used for the sampling procedure in for (2) in Table 8 (top) and for (3)\nin Table 8 (bottom).\n11https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference\n19\nLLM Prompt\nYou are an expert programmer, and you need to write a program for the given natural language query.\nFirst, you should write a grammar that contains all the necessary BNF rules. Then, you should write pro-\ngrams that conform to your predicted rules.\nx(1): \ufb01nd the meeting on Wednesday with Bob and Carol\nG[y(1)]:\nevent\n::=\n\"QueryEvent(\" constraint \")\"\n\ufb01nd the meeting\nconstraint\n::=\n\"(&\" constraint constraint \")\"\n|\n\"(start_?\" date \")\"\non ...\n|\n\"(attendee_?\" attendee attendee \")\"\nwith ...\ndate\n::=\n\"Wednesday\"\nWednesday\nattendee\n::=\n\"Bob\" | \"Carol\"\nBob and Carol\ny(1): QueryEvent(& (start_? Wednesday)(attendee_? Bob Carol))\n. . .\nx: Add meeting with Jean\u2019s manager on Wednesday at 3PM\nLLM Output\nbG:\nevent\n::=\n\"CreateEvent(\" constraint \")\"\nAdd meeting\nconstraint\n::=\n\"(&\" constraint constraint \")\"\n|\n\"(start_?\" date time \")\"\non ... at ...\n|\n\"(attendee_?\" attendee \")\"\nwith ...\ndate\n::=\n\"Wednesday\"\nWednesday\ntime\n::=\n\"NumberPM(3)\"\n3PM\nattendee\n::=\n\"FindManager(\" attendee \")\" | \"Jean\"\nJean\u2019s manager\nby: CreateEvent(& (start_? Wednesday NumberPM(3))(attendee_? FindManager(Jean)))\nFigure 6: Example of grammar prompting where BNF grammars are additionally annotated with natural lan-\nguage comments (shown in green). These manually curated comments provide a detailed mapping between\nnatural language phrases and their corresponding BNF rules, thereby better facilitating interpretation and appli-\ncation of the grammars for generating programs. We manually craft and add these comments to the few-shot\nprompts (top). The model predicts this during inference (bottom).\nIn comparison, the sampling procedure for standard prompting only consists of two stages: (1) we\nrandomly sample a permutation of given molecules, denoted as \u03c0, (2) based on a prompt formed by\nthe permutation, we directly sample a molecule via\nby \u223c PLLM(y | (y(\u03c0[i]))N\ni=1).\nThe hyperparameters used for Step (2) is the same as in grammar prompting and shown in Table 8\n(bottom).\nWhile we observed that Earley-based constrained generation enhances grammar prompting in terms\nof improving validity, other metrics, such as the retrosynthesis score, decreased signi\ufb01cantly. This\ndiscrepancy could be attributed to the fact that existing LLMs, due to their limited exposure to\nmolecules represented in SMILES format, struggle with comprehending and applying the BNF\ngrammar rules of SMILES. Overall, our current \ufb01ndings serve as preliminary evidence that gram-\nmar prompting can tap into the capacity of LLMs to understand and apply BNF rules. However such\ncapacity still remains limited in text-focused LLMs.\nA.3\nPDDL Planning\nRestricted Action Space\nThe specialized grammar de\ufb01ned for PDDL planning essentially delin-\neates a constrained action space that includes necessary actions and their associated objects. Our\nempirical results found that limiting the classical GBFS planner to the objects selected by a spe-\ncialized grammar proved too restrictive, yielding bene\ufb01cial results only within the Blocks domain.\nTherefore, we decided to remove this limitation, thus expanding the action space of GBFS to contain\nthe actions predicted from the grammar with an unrestricted range of objects.\n20\nB\nPrompts\nFigures 7, 8, and 9 demonstrate the prompts with grammars, based on actual examples in the SM-\nCalFlow, GeoQuery, and Overnight datasets respectively. Because the general grammar of SM-\nCalFlow is long (around 4k tokens), we do not include it within the prompt. For GeoQuery and\nOvernight, the general grammar is integrated as part of the instruction within the prompt. In the\ncontext of molecule generation, the general grammar for SMILES12 is also included. Figures 10 and\n11 demonstrate the prompts with action DSLs for PDDL planning.\n12https://metamolecular.com/cheminformatics/smiles/formal-grammar/\n21\nLLM Prompt\nYou are an expert programmer, and you need to write a program for the given natural language query.\nFirst, you should write a grammar that contains all the necessary BNF rules. Then, you should write pro-\ngrams that conform to your predicted rules.\nquery: I need a meeting with Elli and her team on Wednesday afternoon .\nBNF grammar rules:\ncall\n::=\n\"(Yield\" event \")\"\nevent\n::=\n\"(CreatePreflightEventWrapper\" event_constraint \")\"\nevent_constraint\n::=\n\"(&\" event_constraint event_constraint \")\"\n|\n\"(Event.start_?\" datetime_constraint \")\"\n|\n\"(Event.attendees_?\" attendee_constraint \")\"\ndatetime_constraint\n::=\n\"(DateTimeConstraint\" time date \")\"\ntime\n::=\n\"(Afternoon)\"\ndate\n::=\nday\nday\n::=\n\"(NextDOW\" day \")\" | DAY\nDAY\n::=\n\"(Wednesday)\"\nattendee_constraint\n::=\n\"(&\" attendee_constraint attendee_constraint \")\"\n|\n\"(AttendeeListHasRecipient\" person \")\"\n|\n\"(AttendeeListHasPeople\" person \")\"\nperson\n::=\nrecipient | \"(FindTeamOf\" person \")\"\nrecipient\n::=\n\"(Execute (refer (extensionConstraint\" recipient \")))\"\n|\n\"(RecipientWithNameLike (PersonName.apply\" string \"))\"\nstring\n::=\n\"\u201cElli\u201d\"\nprogram based on the BNF grammar rules:\n(Yield(CreatePreflightEventWrapper(&(Event.start_?(DateTimeConstraint(Afternoon)\n(NextDOW(Wednesday))))(Event.attendees_?(&(AttendeeListHasRecipient(Execute(refer\n(extensionConstraint(RecipientWithNameLike(PersonName.apply \u201cElli\u201d))))))\n(AttendeeListHasPeople(FindTeamOf(Execute(refer(extensionConstraint\n(RecipientWithNameLike(PersonName.apply \u201cElli\u201d))))))))))))\n. . .\nquery: Create a meeting tomorrow with the staff .\nLLM Output\nBNF grammar rules:\ncall\n::=\n\"(Yield\" event \")\"\nevent\n::=\n\"(CreatePreflightEventWrapper\" event_constraint \")\"\nevent_constraint\n::=\n\"(&\" event_constraint event_constraint \")\"\n|\n\"(Event.start_?\" datetime_constraint \")\"\n|\n\"(Event.attendees_?\" attendee_constraint \")\"\ndatetime_constraint\n::=\n\"(DateTime.date_?\" datetime_constraint \")\"\n|\n\"(?\" OP datetime \")\"\nOP\n::=\n\"=\"\ndatetime\n::=\ndate\ndate\n::=\nday\nday\n::=\nDAY\nDAY\n::=\n\"(Tomorrow)\"\nattendee_constraint\n::=\n\"(AttendeeListHasPeople\" person \")\"\nperson\n::=\n\"(FindReports\" person \")\"\n|\nrecipient\nrecipient\n::=\n\"(toRecipient (CurrentUser))\"\nprogram based on the BNF grammar rules:\n(Yield(CreateCommitEventWrapper(CreatePreflightEventWrapper(&(Event.start_?\n(DateTime.date_?(? = (Tomorrow))))(Event.attendees_?(AttendeeListHasPeople\n(FindReports(toRecipient(CurrentUser)))))))))\nFigure 7: Prompt with real examples from the SMCalFlow dataset.\n22\nLLM Prompt\nYou are an expert programmer, and you need to write a program for the given natural language query.\nFirst, you should write grammar rules by choosing from the following BNF rules. Then, you should write\nprograms that conform to your predicted rules.\n[BEGIN RULES]\nquery\n::=\n\"answer(\" answer_type \")\"\nanswer_type\n::=\ncity | state | num | place | river | country\ncity\n::=\n\"city(\" city \")\"\n|\n\"cityid(\u2018\" CITYNAME \"\u2019, \u2018\" STATEABBREV \"\u2019)\"\n|\n\"cityid(\u2018\" CITYNAME \"\u2019, _)\"\n|\n\"capital(\" city \")\"\n|\n\"major(\" city \")\"\n|\n\"capital_1(\" state \")\"\n|\n\"loc_2(\" state \")\"\n|\n\"loc_2(\" country \")\"\n|\n\"largest(\" city \")\"\n|\n\"smallest(\" city \")\"\n|\n\"intersection(\" city \",\" city \")\"\n|\n\"exclude(\" city \",\" city \")\"\n|\n\"largest_one(population_1(\" city \"))\"\n|\n\"largest_one(density_1(\" city \"))\"\n|\n\"smallest_one(population_1(\" city \"))\"\n|\n\"smallest_one(density_1(\" city \"))\"\n|\nALL_CITY\n. . .\n[END RULES]\nquery: what states border hawaii ?\nBNF grammar rules:\nquery\n::=\n\"answer(\" answer_type \")\"\nanswer_type\n::=\nstate\nstate\n::=\n\"state(\" state \")\"\n|\n\"next_to_2(\" state \")\"\n|\n\"stateid(\u2018\" STATENAME \"\u2019)\"\nSTATENAME\n::=\n\"hawaii\"\nprogram based on the BNF grammar rules:\nanswer(state(next_to_2(stateid(\u2018hawaii\u2032))))\n. . .\nquery: how many major cities are in arizona ?\nLLM Output\nBNF grammar rules:\nquery\n::=\n\"answer(\" answer_type \")\"\nanswer_type\n::=\nnum\nnum\n::=\n\"count(\" city \")\"\ncity\n::=\n\"major(\" city \")\"\n|\n\"city(\" city \")\"\n|\n\"loc_2(\" state \")\"\nstate\n::=\n\"stateid(\u2018\" STATENAME \"\u2019)\"\nSTATENAME\n::=\n\"arizona\"\nprogram based on the BNF grammar rules:\nanswer(count(major(city(loc_2(stateid(\u2018arizona\u2032))))))\nFigure 8: Prompt with real examples from the GeoQuery dataset.\n23\nLLM Prompt\nYou are an expert programmer, and you need to write a program for the given natural language query.\nFirst, you should write grammar rules by choosing from the following BNF rules. Then, you should write\nprograms that conform to your predicted rules.\n[BEGIN RULES]\nlist_value\n::=\n\"(listValue\" list_value \")\"\n|\n\"(filter\" list_value PROPERTY \")\"\n|\n\"(filter\" list_value PROPERTY OP list_value \")\"\n|\n\"(superlative\" list_value AGGREGATE \"\n|\n(ensureNumericProperty\" PROPERTY \"))\"\n|\n\"(countSuperlative\" list_value AGGREGATE PROPERTY \")\"\n|\n\"(_size\" list_value \")\"\n|\n\"(aggregate\" AGGREGATE list_value \")\"\n|\n\"(getProperty\" list_value PROPERTY \")\"\n|\n\"(getProperty (singleton\" SINGLETON_VALUE \") !type)\"\n|\n\"(concat\" ENTITY_VALUE ENTITY_VALUE \")\"\n|\n\"(concat\" NUMBER_VALUE NUMBER_VALUE \")\"\n|\nENTITY_VALUE | NUMBER_VALUE\nPROPERTY\n::=\n\"shape\" | \"color\" | \"length\" | \"is_special\" | \"width\"\n|\n\"height\" | \"left\" | \"right\" | \"above\" | \"below\"\n|\n\"(reverse left)\" | \"(reverse right)\"\n|\n\"(reverse above)\" | \"(reverse below)\"\nSINGLETON_VALUE\n::=\n\"en.block\" | \"en.shape\" | \"en.color\"\nENTITY_VALUE\n::=\n\"en.block.block1\" | \"en.block.block2\" | \"en.shape.pyramid\"\n|\n\"en.shape.cube\" | \"en.color.red\" | \"en.color.green\"\nNUMBER_VALUE\n::=\n\"3 en.inch\" | \"6 en.inch\" | \"2\"\nOP\n::=\n\"=\" | \">\" | \"<\" | \">=\" | \"<=\" | \"!=\"\nAGGREGATE\n::=\n\"sum\" | \"max\" | \"min\" | \"avg\"\n[END RULES]\nquery: which brick is taller than brick 1\nBNF grammar rules:\nlist_value\n::=\n\"(listValue\" list_value \")\"\n|\n\"(aggregate\" AGGREGATE list_value \")\"\n|\n\"(getProperty\" list_value PROPERTY \")\"\n|\n\"(getProperty (singleton\" SINGLETON_VALUE \") !type)\"\nAGGREGATE\n::=\n\"avg\"\nSINGLETON_VALUE\n::=\n\"en.block\"\nPROPERTY\n::=\n\"width\"\nprogram based on the BNF grammar rules:\n(listValue(aggregate avg (getProperty(getProperty(singleton en.block) !type) width)))\n. . .\nquery: look right of block 1 and get the special block\nLLM Output\nBNF grammar rules:\nlist_value\n::=\n\"(listValue\" list_value \")\"\n|\n\"(filter\" list_value PROPERTY OP list_value \")\"\n|\n\"(filter\" list_value PROPERTY \")\"\n|\n\"(getProperty (singleton\" SINGLETON_VALUE \") !type)\"\n|\nENTITY_VALUE\nSINGLETON_VALUE\n::=\n\"en.block\"\nPROPERTY\n::=\n\"is_special\"\n|\n\"(reverse left)\"\nOP\n::=\n\"=\"\nENTITY_VALUE\n::=\n\"en.block.block1\"\nprogram based on the BNF grammar rules:\n(listValue(filter(filter(getProperty(singleton en.block) !type) is_special)\n(reverse left) = en.block.block1))\nFigure 9: Prompt with real examples from the Overnight dataset. Some long rules are omitted to \ufb01t the space.\n24\nLLM Prompt\nQ:\n(:objects a b c d e - block)\n(:init (clear d) (clear c) (ontable d) (ontable a) (on c e) (on e b) (on b a) (handempty))\n(:goal (on a e) (on e b) (on b d) (on d c))\nDSL:\nplan\n::=\naction+\naction\n::=\n\"(unstack\" object object \")\"\n|\n\"(put-down\" object \")\"\n|\n\"(pick-up-and-stack\" object object \")\"\n|\n\"(unstack-and-stack\" object object object \")\"\nobject\n::=\n\"c\" | \"e\" | \"d\" | \"b\" | \"a\"\nA:\n(unstack c e) (put-down c) (pick-up-and-stack d c) (unstack e b) (put-down e) (unstack-and-stack b a d) (pick-up-\nand-stack e b) (pick-up-and-stack a e)\nQ:\n(:objects a b c d - block)\n(:init (clear a) (clear c) (clear d) (ontable a) (ontable b) (ontable d) (on c b) (handempty))\n(:goal (on a b) (on b c) (on c d))\nDSL:\nplan\n::=\naction+\naction\n::=\n\"(unstack-and-stack\" object object object \")\"\n|\n\"(pick-up-and-stack\" object object \")\"\nobject\n::=\n\"c\" | \"b\" | \"d\" | \"a\"\nA:\n(unstack-and-stack c b d) (pick-up-and-stack b c) (pick-up-and-stack a b)\n. . .\nQ:\n(:objects a b c d - block)\n(:init (clear c) (clear a) (clear b) (clear d) (ontable c) (ontable a) (ontable b) (ontable d) (handempty))\n(:goal (on d c) (on c b) (on b a))\nDSL:\nLLM Output\nplan\n::=\naction+\naction\n::=\n\"(pick-up-and-stack\" object object \")\"\nobject\n::=\n\"b\" | \"a\" | \"c\" | \"d\"\nA:\n(pick-up-and-stack b a) (pick-up-and-stack c b) (pick-up-and-stack d c)\nFigure 10: Prompt with real examples in the Blocks domain from Pyperplan. The prompt template follows\n[69].\n25\nLLM Prompt\nQ:\n(:objects crate0 crate1 crate2 crate3 crate4 crate5 depot0 distributor0 distributor1 hoist0 hoist1 hoist2 pallet0 pallet1\npallet2 pallet3 pallet4 pallet5 truck0 truck1 - object)\n(:init (pallet pallet0) (surface pallet0) (at pallet0 depot0) (clear crate5) (pallet pallet1) (surface pallet1) (at pallet1\ndistributor0) (clear pallet1) (pallet pallet2) (surface pallet2) (at pallet2 distributor1) (clear crate3) (pallet pallet3)\n(surface pallet3) (at pallet3 distributor0) (clear pallet3) (pallet pallet4) (surface pallet4) (at pallet4 distributor0)\n(clear crate4) (pallet pallet5) (surface pallet5) (at pallet5 distributor1) (clear crate1) (truck truck0) (at truck0 dis-\ntributor1) (truck truck1) (at truck1 depot0) (hoist hoist0) (at hoist0 depot0) (available hoist0) (hoist hoist1) (at\nhoist1 distributor0) (available hoist1) (hoist hoist2) (at hoist2 distributor1) (available hoist2) (crate crate0) (surface\ncrate0) (at crate0 distributor0) (on crate0 pallet4) (crate crate1) (surface crate1) (at crate1 distributor1) (on crate1\npallet5) (crate crate2) (surface crate2) (at crate2 distributor1) (on crate2 pallet2) (crate crate3) (surface crate3) (at\ncrate3 distributor1) (on crate3 crate2) (crate crate4) (surface crate4) (at crate4 distributor0) (on crate4 crate0) (crate\ncrate5) (surface crate5) (at crate5 depot0) (on crate5 pallet0) (place depot0) (place distributor0) (place distribu-\ntor1))\n(:goal (on crate0 pallet3) (on crate1 crate4) (on crate3 pallet1) (on crate4 pallet5) (on crate5 crate1))\nDSL:\nplan\n::=\naction+\naction\n::=\n\"(lift-and-drive\" object object object object object object \")\"\n|\n\"(drive-and-load\" object object object object \")\"\n|\n\"(drive-and-lift\" object object object object object \")\"\n|\n\"(lift\" object object object object \")\"\n|\n\"(drive-and-unload\" object object object object object \")\"\n|\n\"(drop\" object object object object \")\"\nobject\n::=\n\"truck1\" | \"hoist0\" | \"crate5\" | \"pallet0\" | \"depot0\" | \"truck0\" | \"hoist2\"\n|\n\"crate4\" | \"crate0\" | \"distributor1\" | \"hoist1\" | \"pallet4\" | \"distributor0\"\n|\n\"crate1\" | \"pallet5\" | \"crate3\" | \"crate2\" | \"pallet1\" | \"pallet3\"\nA:\n(lift-and-drive truck1 hoist0 crate5 pallet0 depot0 depot0) (drive-and-load truck1 hoist0 crate5 depot0) (drive-and-\nlift truck0 hoist2 crate4 crate0 distributor1) (lift hoist1 crate0 pallet4 distributor0) (drive-and-lift truck1 hoist0\ncrate1 pallet5 depot0) (drive-and-load truck1 hoist0 crate1 depot0) (drive-and-lift truck1 hoist0 crate3 crate2 de-\npot0) (drive-and-load truck1 hoist0 crate3 depot0) (drive-and-unload truck1 hoist0 crate3 pallet1 depot0) (drop\nhoist2 crate4 pallet5 distributor1) (drive-and-unload truck1 hoist0 crate1 crate2 depot0) (drive-and-lift truck0 hoist2\ncrate1 crate2 distributor1) (drop hoist2 crate1 crate4 distributor1) (drive-and-unload truck1 hoist0 crate5 crate1 de-\npot0) (drop hoist1 crate0 pallet3 distributor0)\n. . .\nQ:\n(:objects crate0 crate1 crate2 crate3 depot0 distributor0 distributor1 hoist0 hoist1 hoist2 pallet0 pallet1 pallet2\ntruck0 truck1 - object) (:init (pallet pallet0) (surface pallet0) (at pallet0 depot0) (clear crate0) (pallet pallet1) (sur-\nface pallet1) (at pallet1 distributor0) (clear crate3) (pallet pallet2) (surface pallet2) (at pallet2 distributor1) (clear\ncrate2) (truck truck0) (at truck0 depot0) (truck truck1) (at truck1 depot0) (hoist hoist0) (at hoist0 depot0) (avail-\nable hoist0) (hoist hoist1) (at hoist1 distributor0) (available hoist1) (hoist hoist2) (at hoist2 distributor1) (available\nhoist2) (crate crate0) (surface crate0) (at crate0 depot0) (on crate0 pallet0) (crate crate1) (surface crate1) (at crate1\ndistributor1) (on crate1 pallet2) (crate crate2) (surface crate2) (at crate2 distributor1) (on crate2 crate1) (crate\ncrate3) (surface crate3) (at crate3 distributor0) (on crate3 pallet1) (place depot0) (place distributor0) (place dis-\ntributor1)) (:goal (on crate0 pallet2) (on crate1 crate3) (on crate2 pallet0) (on crate3 pallet1))\nDSL:\nLLM Output\nplan\n::=\naction+\naction\n::=\n\"(lift-and-drive\" object object object object object object \")\"\n|\n\"(lift\" object object object object \")\"\n|\n\"(drive\" object object object \")\"\n|\n\"(drive-and-lift\" object object object object object \")\"\n|\n\"(drop\" object object object object \")\"\n|\n\"(drive-and-load\" object object object object \")\"\n|\n\"(drive-and-unload\" object object object object object \")\"\nobject\n::=\n\"truck0\" | \"hoist0\" | \"crate0\" | \"pallet0\" | \"depot0\" | \"hoist2\"\n|\n\"crate2\" | \"crate1\" | \"distributor1\" | \"distributor0\" | \"hoist1\"\n|\n\"pallet2\" | \"crate3\" | \"truck1\"\nA:\n(lift-and-drive truck0 hoist0 crate0 pallet0 depot0 depot0) (lift hoist2 crate2 crate1 distributor1) (drive truck0 de-\npot0 distributor0) (drive-and-lift truck0 hoist1 crate1 pallet2 distributor0) (drop hoist1 crate1 crate3 distributor0)\n(drive-and-load truck1 hoist0 crate0 depot0) (drive-and-unload truck1 hoist0 crate0 pallet2 depot0) (drive truck1\ndepot0 distributor1) (drive-and-load truck1 hoist2 crate2 distributor1) (drive-and-unload truck1 hoist2 crate2 pallet0\ndistributor1)\nFigure 11: Prompt with real examples in the Depot domain from Pyperplan. The prompt template follows\n[69].\n26\n"
  },
  {
    "title": "StyleAvatar3D: Leveraging Image-Text Diffusion Models for High-Fidelity 3D Avatar Generation",
    "link": "https://arxiv.org/pdf/2305.19012.pdf",
    "upvote": "3",
    "text": "StyleAvatar3D: Leveraging Image-Text Diffusion\nModels for High-Fidelity 3D Avatar Generation\nChi Zhang1,\nYiwen Chen2,\nYijun Fu1,\nZhenglin Zhou1,\nGang Yu1\u2217,\nZhibin Wang1,\nBin Fu1,\nTao Chen3,\nGuosheng Lin2,\nChunhua Shen4\n1Tencent PCG, China\n2Nanyang Technological University, Singapore\n3Fudan University, China\n4Zhejiang University, China\nFigure 1: Visualization of 3D stylized avatars produced by our framework. The styles of avatars can be defined\nusing text prompts or example images.\nAbstract\nThe recent advancements in image-text diffusion models have stimulated research\ninterest in large-scale 3D generative models. Nevertheless, the limited availability\nof diverse 3D resources presents significant challenges to learning. In this paper, we\npresent a novel method for generating high-quality, stylized 3D avatars that utilizes\npre-trained image-text diffusion models for data generation and a Generative\nAdversarial Network (GAN)-based 3D generation network for training. Our method\nleverages the comprehensive priors of appearance and geometry offered by image-\ntext diffusion models to generate multi-view images of avatars in various styles.\nDuring data generation, we employ poses extracted from existing 3D models to\nguide the generation of multi-view images. To address the misalignment between\nposes and images in data, we investigate view-specific prompts and develop a\ncoarse-to-fine discriminator for GAN training. We also delve into attribute-related\nprompts to increase the diversity of the generated avatars. Additionally, we develop\na latent diffusion model within the style space of StyleGAN to enable the generation\nof avatars based on image inputs. Our approach demonstrates superior performance\nover current state-of-the-art methods in terms of visual quality and diversity of the\nproduced avatars.\n\u2217Corresponding author\nProject page: https://github.com/icoz69/StyleAvatar3D\narXiv:2305.19012v2  [cs.CV]  31 May 2023\n1\nIntroduction\nIn recent years, generative models have made significant strides in generating high-fidelity 2D\nimages [28, 35, 39, 41], primarily due to the availability of large-scale image-text pairs [33] and\nadvanced generative model architectures such as diffusion models [45, 13, 8, 14, 49]. These models\nenable users to generate realistic images using text prompts, obviating the need for manual interven-\ntion. However, 3D generative models [48, 9, 24, 55, 51] still face considerable challenges owing to\nthe limited availability and diversity of 3D models for learning, compared to their 2D counterparts.\nThe manual creation of 3D assets in software engines is a laborious process that demands significant\nexpertise, limiting the availability of diverse and high-quality 3D models [11, 22, 47]. To tackle this\nissue, researchers [29, 17, 31, 21] have recently explored pre-trained image-text generative models\nfor generating high-fidelity 3D models. These models provide rich priors of object appearance and\ngeometries, which can potentially facilitate the generation of realistic and diverse 3D models.\nIn this paper, we propose a novel approach to 3D stylized avatar generation that leverages pre-trained\ntext-to-image diffusion models, enabling users to define styles and facial attributes of avatars using\ntext prompts. Specifically, we employ EG3D [6], a GAN-based 3D generation network, as our\ngenerator, which offers several advantages. Firstly, EG3D [6] requires calibrated images for training\ninstead of 3D data, allowing for continuous improvement of the diversity and fidelity of 3D models\nthrough enhanced image data, a relatively straightforward task for 2D images. Secondly, since the\nimages used for training do not necessitate strict multi-view consistency in appearance, we can\ngenerate each view separately, effectively managing the randomness during image generation. To\ngenerate calibrated 2D training images for training EG3D [6], our approach utilizes ControlNet [53]\nbuilt upon StableDiffusion [39], which enables image generation guided by predefined poses. These\nposes can be either synthesized or extracted from avatars in existing engines, allowing for the reuse\nof camera parameters from pose images for learning purposes.\nControlNet [53] often struggles to generate views with large angles, such as the backside of the\nhead, even when using correct pose images as guidance. These failure outputs pose significant\nchallenges for generating full 3D models. To address this issue, we have approached the problem\nfrom two different perspectives. Firstly, we have designed view-specific prompts for different views\nduring image generation, significantly reducing the number of failure cases. Nevertheless, even with\nview-specific prompts, the synthesized images may not be perfectly aligned with the pose images. To\ntackle this misalignment, we have developed a coarse-to-fine discriminator for training 3D GANs. In\nour framework, each image data is associated with a coarse pose annotation and a fine pose annotation.\nDuring GAN training, we randomly choose an annotation for training. For confident views such as\nthe front face, we assign a high probability of using the fine pose annotation, while the learning of\nthe rest views relies more on coarse views. This strategy enables us to generate more accurate and\ndiverse 3D models, even when the input images have noisy annotations.\nTo enable conditional 3D generation with an image input, we have also developed a latent diffusion\nmodel [35] in the latent style space of StyleGAN [18, 19]. The style code is low-dimensional, highly\nexpressive, and compact, making the diffusion model fast to train. We directly sample image and\nstyle code pairs from our trained 3D generators as training data to learn the diffusion model [35].\nTo evaluate the effectiveness of our proposed method, we conducted extensive experiments on various\nlarge-scale datasets. Our results demonstrate that our method outperforms existing state-of-the-art\nmethods in terms of both visual quality and diversity. In summary, this paper presents a novel\nframework for generating high-fidelity 3D avatars that leverages pre-trained image-text diffusion\nmodels. Our framework allows styles and face attributes to be defined by text prompts, significantly\nenhancing the flexibility of avatar generation. Additionally, we have proposed a coarse-to-fine\npose-aware discriminator to address the image-pose misalignment issue, enabling better utilization\nof image data with inaccurate pose annotations. Lastly, we have developed an extra conditional\ngeneration module that allows for conditional 3D generation with image input in the latent style\nspace, further increasing the flexibility of our framework and allowing users to generate 3D models\ntailored to their individual preferences.\n2\nRelated Work\nThis section provides an overview of related literature in the field of 3D generation from 2D images.\n2\nText-to-image Generative Models. Tremendous advancements have been made in the field of\ntext-to-image generative tasks [43, 28, 32, 27]. Several models, such as StableDiffusion [39],\nImagen [41], and DALL-E 2 [35], have been proposed to generate images guided by encoded text\nprompts. These models integrate text guidance during the diffusion process in a classifier-free manner,\nallowing for the customization of image styles, contents, and details through prompt adjustment.\nStableDiffusion [39] is particularly notable among these methods because it carries out the diffusion\nprocess in the latent space of an autoencoder, leading to reduced inference speed and memory cost.\nControlNet [53] extends StableDiffusion by introducing additional parametric modules to a pre-trained\nmodel, providing accurate control over the output image content. ControlNet can support various types\nof guidance and allows the use of multiple guidance at the same time, such as depth images [36, 52],\nhuman poses [5, 20], edge maps [4], and others. Low-Rank Adaptation (LoRA) [15], originally used\nfor finetuning large language models, is recently introduced for finetuning StableDiffusion, which\ncan generate images of a subject given only a few training images of it.\n3D generation based on pre-trained image generative models. Directly transferring the success\nof image diffusion models to 3D is a challenge, as significant amounts of 3D data with appropriate\nrepresentations must be prepared for learning. Wang et al. [46] develop a 3D digital avatar diffusion\nmodel called Rodin, which represents 3D data using parametric tri-plane [6] features obtained\nby fitting existing 3D avatar models in the engine. Image generative models possess significant\ncapabilities and rich priors, motivating researchers to explore their potential for 3D generation. For\ninstance, Dreamfusion [31] designs a Score Distillation Sampling method to extract knowledge\nfrom image generative models to optimize Neural Radiance Fields (NeRFs) [26, 2] guided by\nprompts. However, this model generation relies on slow optimization of NeRFs, and the diversity\nof the models remains uncertain. Magic3D [21] improves the learning speed and the resolution\nof DreamFusion [31] by adopting a two-stage optimization strategy that utilizes a sparse 3D hash\ngrid structure. DreamBooth3D [34] combines DreamBooth [40] and DreamFusion [31], which\nallows personalizing text-to-3D generative models from a few images of a subject. Latent-NeRF [25]\nimproves DreamFusion [31] by applying the diffusion process in a latent space of a pre-trained\nautoencoder.\nDomain Adaptation of 3D GANs. Our work is closely related to EG3D [6], a geometry-aware GAN\nbased on StyleGAN [18, 19]. EG3D\u2019s generator produces features of three orthogonal 2D planes,\nand it uses volume rendering to generate various views of the 3D model by sampling points from\nthese planes. In contrast to the raw StyleGAN [18, 19] discriminator, EG3D\u2019s pose-conditioned\ndiscriminator requires accurate camera pose annotations of images to learn multi-view consistent\n3D models. For real-face datasets like FFHQ [18], pose estimators [5, 54, 3, 16] are commonly\nused to obtain pose annotations. However, these estimators may fail to detect faces at large angles,\nresulting in incomplete 3D models that are 2.5D rather than full 3D [3]. Additionally, the use of such\ndetectors may be limited for stylized images, which reduces the applicability of the method [50].\nPoF3D [42] develops a pose-free discriminator built upon EG3D [6], which embeds a pose predictor\nin the discriminator to predict the pose of input images in the discriminator for training. It also\npredicts the distribution of poses in the training datasets used for renderings in the 3D generator. To\ngenerate 3D models with different styles, many researchers explore domain adaptation of trained\n3D GANs. For instance, in 3DAvatarGAN [1], knowledge from a stylized 2D generator is distilled\nto a pre-trained 3D generator for domain adaptation. Song et al. [44] use pre-trained text-to-image\ndiffusion models to adapt a pre-trained 3D generator to a new text-defined domain, based on the score\ndistillation sampling technique proposed in DreamFusion [31]. In comparison to these works, our\napproach focuses on generating calibrated data and efficiently using it to train 3D GANs.\n3\nMethod\nThis section presents the detailed framework of our proposed method for generating stylized 3D\navatars trained with multi-view images. We begin by discussing the preparation of multi-view images\nfor training our 3D generator in Section 3.1. Next, we introduce our coarse-to-fine discriminators in\nSection 3.2, which are designed to address the image-pose misalignment issue in the dataset. Finally,\nwe present our proposed latent diffusion model in the style space that supports image-conditioned 3D\ngeneration in Section 3.3. The overview of our framework is illustrated in Fig. 2.\n3\nDiffusion\nCLIP\nGenerator\nRender\nDiscrimin\nator\nReal\nor\nFake\nTri-plane\nStyle vector\nFine pose Coarse pose\nMapping \nNetwork\nGaussian\nInput\nFigure 2: The overall structure of our networks, which supports unconditional generation and conditional\ngeneration with image inputs. After finishing training of the unconditional 3D GAN, we train a conditional\nlatent diffusion model to replace the mapping network for conditional avatar generation.\n3.1\nGenerating Multi-View Images\nOur framework leverages ControlNet [53] to produce multi-view images with pose guidance, enabling\nthe definition of avatar styles through textual descriptions. The pipeline for dataset generation is\nillustrated in Fig. 3. Specifically, ControlNet [53], represented as C\u03b8, receives a pose image Ip and\na text prompt T as input and, in response, generates a stylized image Is: Is = C\u03b8(Ip, T). The text\nprompt T comprises a positive prompt and a negative prompt: T = (Tpos, Tneg), which respectively\nspecify the desired and undesired characteristics in the synthesized images.\nWe use existing 3D avatar models from an engine to provide pose images for guidance. As a\nstrategic measure to extract pose images from avatar models, we designate the center of the avatar\u2019s\nhead as the origin of the world coordinate system. This tactic provides a stable and consistent\nreference point for camera movements. Cameras are then oriented towards this origin and rotated at a\npredetermined radius to create multi-view images. The yaw and pitch angles of the avatar\u2019s front face\nare assumed to be zero degrees. In accordance with this assumption, the camera position is randomly\nsampled within a yaw range of -180 degrees to 180 degrees and a pitch range of -30 degrees to 30\ndegrees. We investigate three guidance types to generate the pose image Ip : depth maps, human\npose (Openpose [5]), and hybrid guidance that synergistically incorporates both depth maps and\nhuman pose. Both types of pose images are formed as RGB images. As pose images are generated\nwithin engines, we concurrently obtain the camera parameters c of the synthesized images Is. This\nsimultaneous acquisition is important as it enables successful training of 3D GANs.\nView-related prompts. As the training ControlNet [53] relies on pseudo pose labels provided by pre-\ntrained human pose estimators, it often fails to synthesize avatars with large face angles, resulting in\navatars with random poses. This issue is also prevalent when using depth guidance, as the depth maps\nmay not accurately reflect the poses, particularly in the backside of the head. To mitigate such failure\ncases, we incorporate view-related prompts Tview into the positive prompt for generating specific\nviews such as \u201cside view of faces\" and \u201cbackside of the head\", to improve accuracy. Additionally, we\nintroduce negative prompts associated with invisible facial features, such as \u201ceyes\" and \u201cnoses\", for\ndifferent views in Tneg. This strategic incorporation significantly diminishes the number of failure\ncases and improves the generation of multi-view images.\nAttribute-related prompts. StableDiffusion [39] tends to create biased avatars with similar facial\nattributes, resulting in limited diversity in the generated dataset. To counteract this bias, we manually\nintroduce attribute-related prompts Tatt with the objective of enhancing the diversity of created avatars.\nThese prompts encompass various aspects including hairstyles, facial expressions, and eye shapes.\nWe incorporate 20 different facial attributes to augment avatar variety. During the generation process,\nwe randomly sample five facial attributes and select one category for each attribute. Consequently,\nour positive prompts consist of three parts: style-related prompts Tstyle, view-related prompts Tview,\nand attribute-related prompts Tatt: Tpos = {Tstyle, Tview, Tatt}. We also demonstrate in our experiments\nthat instead of defining the style of avatars by texts, we can also let the model learn the style from a\nfew examples of images based on LoRA [15].\n3.2\nAddressing the Issue of Image-Pose Misalignment\nIn the context of training a 3D generator [7, 30, 10, 6] with synthesized multi-view images, a critical\nchallenge that often arises is the problem of image-pose misalignment. This problem arises when\n4\nDepth\nNormal\nPose\nCanny\nScribble\n\u2026\nPose Image \ud835\udc3c!\nPose \nExtract\n\u2026\nStylized Images \ud835\udc3c\"\nControlNet\n\ud835\udc47#$\" = \ud835\udc47%&&', \ud835\udc47()'*, \ud835\udc47\"&+,'\n\ud835\udc47-'.\nExisting Models\nFigure 3: The pipeline to generate the multi-view image dataset under the guidance of pose images. We extract\nposes from existing 3D models to guide image synthesis of different views. The styles of avatars are controlled\nby text prompts.\nthe pose of the avatar in the generated images does not precisely match the pose image. We observe\nthat the ControlNet [53] accurately generates front-face images of avatars, likely due to the presence\nof discernible facial features that facilitate alignment with the provided guidance. However, the\nsynthesis of side or rear views of avatars presents a considerable challenge, leading to situations\nwhere raw pose annotations from the engine no longer fit the images generated.\nTo mitigate this issue, we introduce a novel coarse-to-fine discriminator specifically designed to\nsupport learning with multi-view images, even in scenarios where pose annotations may not be\nentirely accurate. Each image is associated with two distinct types of pose annotations: a fine pose\nlabel cfine that corresponds to a more accurate pose annotation, and a coarse pose label ccoarse that\nprovides a generalized indication of the image\u2019s view, as illustrated in Fig .2. Given that the camera\nmaintains a fixed radius to render images from avatars, we can simplify the pose representations to\nyaw and pitch representations. To generate these pose labels, we divide all rendered views into Ngroup\ndistinct groups based on their yaw and pitch values. Each group is then assigned a unique one-hot\nyaw representation and a one-hot pitch representation. To derive fine pose labels, we allocate a large\ngroup number, and conversely, a small group number is assigned to obtain coarse pose labels. Both\ntypes of labels are represented by concatenated one-hot representations of yaw and pitch. The final\npose labels used in the discriminator are formed by concatenating the fine labels and coarse labels:\nc = cfine||ccoarse. During the training process, one type of pose annotation is sampled for use while the\nother is set to zero. Views that demonstrate high alignment accuracy, which we refer to as \"confident\nviews\", are assigned a high sample probability ph of fine pose annotations. Conversely, views that are\nless confident and consequently exhibit a lower degree of alignment accuracy are assigned a lower\nsample probability pl for fine pose annotations. We define confident views as those views close to\nthe front face, as these are most likely to align accurately with the generated images based on our\nempirical observations.\n3.3\nImage-Guided 3D Generation through Latent Diffusion\nIn EG3D [6], the authors explore the conditioned face generation process using pivotal tuning [38], a\nmethod that optimizes style codes and generators to align the output with a target input image. For\n3D GANs, the rendered views of the input images, i.e., the poses, should additionally be provided\nfor rendering. However, accurately estimating the poses of stylized avatars can be challenging,\nparticularly for certain complex styles. To address this challenge, we develop a conditional diffusion\nmodel that operates in the latent style space W of StyleGAN [18, 19]. We randomly sample image\nand style vector pairs from our trained 3D generators to learn the diffusion models. Specifically, we\nrender the front image of a randomly generated 3D avatar using our 3D generators and record its\nstyle vector. The diffusion model\u2019s objective is to diffuse the style vector from noise, guided by the\nrendered front image. We utilize the PriorTransformer [35] as our diffusion model \u03f5\u03b8, which receives\na noisy style vector w and the front image\u2019s CLIP-encoded embedding y as inputs and predicts the\nnoise \u03f5. During training, we adopt the approach used in classifier-free diffusion guidance [14], where\nthe condition embedding is randomly zeroed with a probability pdrop. During inference, we can adjust\nthe guidance strength \u03bb to generate 3D avatars towards our provided condition:\n\u03f5\u03b8(y, z) = \u03bb\u03f5\u03b8(w, y) + (1 \u2212 \u03bb)\u03f5\u03b8(w).\n(1)\nUpon completion of the training process, we can substitute the style mapping network in the original\n3D generator with our learned diffusion models to generate 3D avatars conditioned on an input\nimage. This approach eliminates the need to estimate the pose of the input image for rendering, thus\nimproving the accuracy of generating stylized avatars.\n5\nMethod\nFID\u2193\nEG3D [6]\n7.8\nPoF3D [42]\n20.9\nCoF (Ours)\n5.6\nTable 1: Comparison of our coarse-to-fine (CoF) discriminator with existing methods. Our proposed method\noutperforms the compared methods with remarkable advantages.\nPose Guidance\nSource\nBaseline\n+ \ud835\udc47!\"#$\n+\ud835\udc47!\"#$, \ud835\udc47%&&\nPose Guidance\nSource\nBaseline\n+ \ud835\udc47!\"#$\n+ \ud835\udc47!\"#$, \ud835\udc47%&&\nDepth Guidance\nOpenpose Guidance\nHybrid Guidance\nFigure 4: Comparison of data generated by different guidance strategies and prompts. We can observe how\nview-specific prompts and attribute-related prompts can effectively enhance generation accuracy and diversity.\n4\nExperiments\nIn this section, we provide the key results of our experiments. For a more comprehensive analysis\nand experiment details, we refer the reader to our appendix.\nDataset. We collect 50 avatar styles to synthesize data in our experiments. All the generated images\nare in the resolution of 512 \u00d7 512. For analysis and ablation study, we construct a mix-style dataset\nwith 500,000 images evenly sampled from 50 styles, which adopts a hybrid guidance strategy. This\nallows us to test our model\u2019s ability to generalize across different styles. When using depth as the\npose images, we utilize the Midas model [37] to extract depth maps from 100,000 avatars created by\nexisting engines. For human pose guidance, we render Openpose [5] annotations of different views\nbased solely on one avatar in the engine. To augment our datasets during training, we horizontally\nflip the synthesized images and pose labels.\n4.1\nResults\nInfluence of guidance and prompts on dataset construction. In the beginning, we investigate\nthe impact of different types of guidance and prompts on dataset construction. Fig. 4 illustrates\nthe results of our experiments. When it comes to image generation of the back side of heads, we\nfind that view-specific prompts are particularly effective in eliminating failure cases (see the yellow\nbox). In addition, we observe that incorporating attribute-related prompts greatly enhances the\ndiversity in the appearance of the generated avatars. The most effective guidance strategy we find is\nthe hybrid guidance approach, which results in overall better quality and stability of the generated\navatars. Overall, our findings suggest that careful consideration of guidance and prompts is crucial in\nconstructing high-quality datasets for image generation tasks.\nEffectiveness of coarse-to-fine discriminators. We proceed to validate the effectiveness of our\ndeveloped coarse-to-fine discriminators, as illustrated in Table 1. We utilize the Fr\u00e9chet Inception\nDistance (FID) [12] as our evaluation metric for a quantitative comparison. We benchmark our model\nagainst the rudimentary discriminator design in EG3D [6]. Following this, we compare our method\nwith PoF3D [42], which incorporates a pose predictor in the discriminator, thereby eliminating\nthe need for pose labels during the training phase. As evidenced by the results, our coarse-to-fine\ndiscriminators significantly outperform the compared methods, demonstrating the superior efficacy\n6\nFigure 5: Latent space walk experiment. We linearly shift the input noise and the rendered view from one to the\nother. The appearance and geometries are changed smoothly.\nInput\nOutput\nInput\nOutput\nInput\nOutput\nFigure 6: Results of conditional avatar generation with image input. Given the input image on the left, our\ndiffusion predicts the style vector used for generating 3D avatars displayed on the right.\nof our design. We observe that PoF3D yields subpar results on our generated datasets. A plausible\nexplanation for this could be that predicting the poses of stylized avatars can present substantial\nchallenges due to the high degree of variation in styles and the potential complexity of poses. The\nmore detailed analysis of our proposed coarse-to-fine discriminator and qualitative comparisons are\nfurther elaborated in our appendix.\nLatent space walk. One intriguing technique to gauge the quality of a learned GAN is to perform\na latent space walk. This involves randomly selecting two input vectors and performing a linear\ninterpolation between them. Concurrently, the rendering angle is linearly changed from left to right.\nThis process enables us to observe how the GAN generates images as we navigate through its latent\nspace. As exhibited in Fig. 5, our model is capable of producing visually coherent and diverse images\nas we traverse its latent space. Moreover, the linear changes in rendering angle allow us to observe\nhow the model responds to variations in viewpoint, which is an important aspect of 3D generation.\nValidation of Image-Conditioned 3D Generation To validate the efficacy of our image-conditioned\n3D generation approach, we withhold a few images during multi-view image generation as the testing\ninput images. The results of our experiments are shown in Fig. 6. Our diffusion model in the latent\nstyle space is able to effectively generate 3D models under the guidance of the input images. Even\nthough there are minor differences in appearance between the generated avatars and the input images,\nit is worth noting that our conditional diffusion model effectively captures facial features.\nVisualization of meshes. Next, we provide the visualization of meshes extracted from Tri-planes\nusing marching cubes algorithms [23], following EG3D [6]. The resulting meshes and rendered\nRGB images are presented in Fig. 7, which portrays the accuracy of our model in generating avatars\nwith realistic geometries. Notably, the geometries of avatars generated by our framework exhibit\nvariance across different styles, suggesting that our approach transcends the mere modification of the\nappearance of existing 3D avatar models. Instead, our method is capable of generating avatars with\nunique geometries that accurately reflect the desired style.\nLoRA-based cartoon character reconstruction. So far, we have demonstrated the effectiveness of\nour model, which is trained using data generated by text-defined styles. We next explore the use of a\nLoRA [15] model based on StableDiffusion [39], which can replace the text-defined style with a few\nexample images of a style or subject. To test this approach, we select a well-known cartoon character\nand gather 10 images of it from the internet to train the LoRA model. Once the training is completed,\nwe generate multi-view images using the learned LoRA model, as outlined in Section 3.3. We employ\nOpenpose [5] and a hybrid guidance strategy for this experiment. Fig. 8 presents the results of this\nexperiment. As shown in the figure, when Openpose [5] is used as guidance, our model faithfully\nreconstructs the 3D model of the character. Conversely, the hybrid guidance strategy enables us to\ndiversify the appearance of the cartoon characters, such as by altering their hairstyles. This result\n7\nFigure 7: Visualization of exported meshes from our generated 3D avatars. We utilize marching cube algorithms\nto extract meshes from our learned 3D generation model. Our method is capable of generating avatars with\nunique geometries that accurately reflect the desired style.\nCharacter\nOpenpose Guidance\nHybrid Guidance\nFigure 8: 3D cartoon character reconstruction using our framework. We used 10 images collected from the\nInternet to finetune a LoRA + StableDiffusion model for training image generation.\nunderscores the flexibility of our approach in adapting to different styles and guidance strategies,\nwhich could be particularly useful for creating diverse and dynamic avatars or characters.\n5\nConclusion\nIn this paper, we introduce a novel framework to generate stylized 3D avatars by utilizing pre-trained\ntext-to-image diffusion models. The framework offers the ability to define styles and facial attributes\nusing text prompts, which greatly enhances the flexibility of avatar creation. The proposed coarse-\nto-fine discriminator can effectively address the issue of misalignment between generated training\nimages and poses, thereby improving the utilization of image data with inaccurate pose annotations.\nFinally, an additional conditional generation module based on latent diffusion has been developed,\nenabling the generation of 3D models based on image input.\nReferences\n[1] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai, Aliaksandr Siarohin, Peter Wonka, and\nSergey Tulyakov. 3davatargan: Bridging domains for personalized editable avatars. arXiv preprint\narXiv:2301.02700, 2023.\n[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360:\nUnbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5470\u20135479, 2022.\n[3] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment\nproblem? (and a dataset of 230,000 3d facial landmarks). In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 2017.\n[4] John Canny. A computational approach to edge detection. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, pages 679\u2013698, 1986.\n8\n[5] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d\npose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence,\n2019.\n[6] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient\ngeometry-aware 3D generative adversarial networks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), 2022.\n[7] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. pi-gan: Periodic implicit\ngenerative adversarial networks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pages 5799\u20135809, 2021.\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in\nNeural Information Processing Systems (NeurIPS), 34:8780\u20138794, 2021.\n[9] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic,\nand Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. In\nAdvances In Neural Information Processing Systems, 2022.\n[10] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. Stylenerf: A style-based 3d aware gener-\nator for high-resolution image synthesis. In Proceedings of the International Conference on Learning\nRepresentation (ICLR), 2022.\n[11] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio\nOrts-Escolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volumetric performance capture\nof humans with realistic relighting. ACM Transactions on Graphics (TOG), 38(6):1\u201319, 2019.\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium.\nAdvances in Neural\nInformation Processing Systems (NeurIPS), 30, 2017.\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems (NeurIPS), 33:6840\u20136851, 2020.\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\n[15] Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint arxiv:2106.09685, 2021.\n[16] Yangyu Huang, Hao Yang, Chong Li, Jongyoo Kim, and Fangyun Wei. Adnet: Leveraging error-bias\ntowards normal direction in face alignment. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021.\n[17] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object\ngeneration with dream fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 867\u2013876, 2022.\n[18] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 4401\u20134410, 2019.\n[19] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 8110\u20138119, 2020.\n[20] Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Openpifpaf: Composite fields for semantic keypoint\ndetection and spatio-temporal association. IEEE Transactions on Intelligent Transportation Systems,\n23(8):13498\u201313511, 2021.\n[21] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[22] Stephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face\nrendering. ACM Transactions on Graphics (TOG), 37(4):1\u201313, 2018.\n9\n[23] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface construction\nalgorithm. ACM Transactions on Graphics (TOG), 21(4):163\u2013169, 1987.\n[24] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837\u20132845, 2021.\n[25] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-\nguided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022.\n[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren\nNg. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European\nConference on Computer Vision (ECCV), 2020.\n[27] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2i-\nadapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. arXiv\npreprint arXiv:2302.08453, 2023.\n[28] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya\nSutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided\ndiffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[29] Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for\ngenerating 3d point clouds from complex prompts. arXiv preprint arXiv:2212.08751, 2022.\n[30] Michael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative neural\nfeature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), pages 11453\u201311464, 2021.\n[31] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988, 2022.\n[32] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizadwongsa, and Supasorn Suwajanakorn. Diffusion\nautoencoders: Toward a meaningful and decodable representation. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2022.\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In Proceedings of the International Conference on Machine Learning\n(ICML), pages 8748\u20138763, 2021.\n[34] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada,\nKfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d\ngeneration. arXiv preprint arXiv:2303.13508, 2023.\n[35] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[36] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 44(3):1623\u20131637, 2020.\n[37] Ren\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2022.\n[38] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for latent-based\nediting of real images. ACM Transactions on Graphics (TOG), 42(1):1\u201313, 2022.\n[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 10684\u201310695, 2022.\n[40] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint\narxiv:2208.12242, 2022.\n[41] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487, 2022.\n10\n[42] Zifan Shi, Yujun Shen, Yinghao Xu, Sida Peng, Yiyi Liao, Sheng Guo, Qifeng Chen, and Dit-Yan Yeung.\nLearning 3d-aware image synthesis with unknown pose distribution. arXiv preprint arXiv:2301.07702,\n2023.\n[43] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502, 2020.\n[44] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, and Ahmed Elgammal. Diffusion guided\ndomain adaptation of image generators. arXiv preprint arXiv:2212.04473, 2022.\n[45] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole.\nScore-based generative modeling through stochastic differential equations.\narXiv preprint\narXiv:2011.13456, 2020.\n[46] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong\nChen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital avatars using\ndiffusion. arXiv preprint arXiv:2212.06135, 2022.\n[47] Erroll Wood, Tadas Baltru\u0161aitis, Charlie Hewitt, Sebastian Dziadzio, Thomas J Cashman, and Jamie\nShotton. Fake it till you make it: face analysis in the wild using synthetic data alone. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV), pages 3681\u20133691, 2021.\n[48] Jiajun Wu, Chengkai Zhang, Tianfan Xue, William T Freeman, and Joshua B Tenenbaum. Learning a\nprobabilistic latent space of object shapes via 3d generative-adversarial modeling. In Advances in Neural\nInformation Processing Systems, pages 82\u201390, 2016.\n[49] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao\nZhang, Bin Cui, and Ming-Hsuan Yang. Diffusion models: A comprehensive survey of methods and\napplications. arXiv preprint arXiv:2209.00796, 2022.\n[50] Jordan Yaniv, Yael Newman, and Ariel Shamir. The face of art: landmark detection and geometric style in\nportraits. ACM Transactions on graphics (TOG), 38(4):1\u201315, 2019.\n[51] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis.\nLion: Latent point diffusion models for 3d shape generation. arXiv preprint arXiv:2210.06978, 2022.\n[52] Chi Zhang, Wei Yin, Zhibin Wang, Gang Yu, Bin Fu, and Chunhua Shen. Hierarchical normalization for\nrobust monocular depth estimation. NeurIPS, 2022.\n[53] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023.\n[54] Zhanpeng Zhang, Ping Luo, Chen Change Loy, and Xiaoou Tang. Facial landmark detection by deep\nmulti-task learning. In Proceedings of the European Conference on Computer Vision (ECCV), pages\n94\u2013108, 2014.\n[55] Linqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5826\u20135835,\n2021.\n11\nAppendix\nA\nIntroduction\nOur appendix provides additional information about our research. The content is organized as follows:\n\u2022 Firstly, in Section B, we provide implementation details of our experiments. This section\nincludes more information about dataset generation, network structures, and training.\n\u2022 Secondly, in Section C, we present additional experiment results with analysis. This section\nincludes findings that were not included in the main paper.\nB\nImplementation Details\nB.1\nDataset Generation\nPrompts. We first present our detailed prompts for dataset generation. Table 3 presents an overview\nof the prompts we used. For more specific information, Table 4 and Table 5 provide details about the\nstyle-related prompts and attribute-related prompts, respectively. The prompts for the first 10 styles\nwere manually designed, while the prompts for the remaining styles were generated using ChatGPT.\nSimilarly, all attribute-related prompts were generated using ChatGPT as well.\nB.2\nTraining of 3D GANs\nIn order to train our 3D GANs effectively, we incorporated both fine and coarse pose annotations.\nFine labels were obtained by dividing the yaw range into 40 bins and the pitch range into 15 bins,\nresulting in one-hot yaw and pitch representations. For coarse labels, we divided the yaw range and\npitch range into 3 and 2 bins, respectively. Consequently, the final pose annotation is a 60-dimensional\nvector representation. In our approach, we define views with yaw values in the range of \u221260\u25e6, 60\u25e6\nand pitch values in the range of \u221215\u25e6, 15\u25e6 as confident views. During training, we sample fine pose\nannotations for confident views with a probability of 0.9 (ph), and for the remaining views with a\nprobability of 0.1 (pl).\nOur 3D GAN is built upon the official implementation of EG3D [6]. Therefore, our training\nhyperparameters and losses are identical. Specifically, we trained our model on 8 Tesla V100 GPUs\nwith a batch size of 32. The training process consisted of a total of 6000 iterations, which took\napproximately 5 days. We use an EG3D model pre-trained on FFHQ dataset [18] to initialize the\nparameters of our models, except the layers related to pose encoding. We find such a pre-training\nstrategy can greatly accelerate training. For more details on the training of 3D GANs, please refer\nto EG3D [6]. The training of our diffusion model followed a similar setup. It was also trained on 8\nTesla V100 GPUs with a batch size of 32. We used DDPM [13] with a total of 1000 denoising steps.\nFor inference, we employed DDIM [43] for sampling steps with a total of 50 steps. The diffusion\nmodel was trained for 600,000 iterations with a learning rate of 0.0001. Additionally, the probability\npdrop of dropping the condition at training time was set to 0.2. During inference, we set the guidance\nstrength \u03bb to 5. During the training of diffusion models, we sampled a 3D avatar from the trained\nunconditional 3D GANs and rendered its front view as the conditional input image for learning. To\naugment the training process, we randomly jittered the pose parameters of rendering.\nB.3\nEvaluation Metric\nWe utilize the Fr\u00e9chet Inception Distance (FID) [12] as our evaluation metric for a quantitative\ncomparison. During the evaluation, we first randomly generate a 3D avatar with our model, then\nwe randomly render a view for evaluation. It is worth noting that this approach differs slightly from\nthe raw implementations in EG3D[6], where the rendered view for evaluation is always the same\nas the view provided to the generator for generator pose conditioning (GPC). The reason for this\nmodification is that we observed a potential bias introduced by GPC. While GPC can enhance the\nquality of rendering by conditioning on a specific view, it also allows the generator to be aware of the\nrendered view through this input. Consequently, the rendered view provided through GPC tends to be\nbetter than other views of the generated 3D avatars. In order to mitigate this bias during evaluation,\n12\nCoarse\nFine\nFID\u2193\n\u2713\n7.1\n\u2713\n7.0\n\u2713\n\u2713\n5.6\nTable 2: Ablation study on coarse-to-fine discriminators. Utilizing both types of pose annotations in the\ndiscriminator helps training and outperforms the baselines with only one type of pose annotations.\nwe adopt a simple solution by randomly selecting both the generated view and the conditioning view\nfor rendering. This random selection removes the bias and ensures a fair evaluation. For instance, by\nremoving the bias during evaluation, we observed a 0.2 increase in the FID of EG3D [6].\nC\nMore Results and Analysis\nOpenpose synthesis. During the synthesis of Openpose annotations from existing 3D models, we\nexplored different strategies, including synthesizing all landmarks or synthesizing visible landmarks\nonly. We compared these strategies in Figure 9. From the results, we observed that both synthesizing\nall facial landmarks and synthesizing visible landmarks only often introduced ambiguity and resulted\nin images with incorrect poses. Through empirical analysis, we found that synthesizing all visible\nlandmarks along with the nose point yielded the best results. However, it is important to note that\neven with this strategy, the alignment is not perfect, which motivated the design of our coarse-to-fine\ndiscriminators.\nAblation study. To conduct an ablative analysis, we compared our model with baselines that only\nutilized fine pose annotations or coarse pose annotations alone, as shown in Table 2. The comparison\nrevealed that our proposed design outperforms the two baselines, indicating the effectiveness of our\ncoarse-to-fine discriminators. We further visualized the typical failure cases of the two baselines in\nFigure 10. As depicted, when there is a pose-image misalignment issue in the training sets, using\nfine but inaccurate pose labels for training results in distortion in the back side of the avatar. On the\nother hand, solely relying on coarse poses for training may cause the model to converge to distorted\n3D avatars that appear unrealistic. Our model, by incorporating both fine and coarse pose labels,\neffectively mitigates these issues and produces more accurate and realistic 3D avatars.\nFurther analysis on conditional avatar generation. In addition to our main experiments, we\nconducted an in-depth analysis of two special cases of conditional avatar generation. The first case\ninvolves input images with a large face angle away from the front face. The second case explores the\ngeneration of avatars from out-of-domain images. To generate out-of-domain images, we utilized\nStableDiffusion to transform a real face photo into the Disney style, which served as the input to\nour model. The results of these experiments are presented in Fig. 11. From the results, we observe\nthat even though the conditional input during training is a front face image sampled from a trained\nunconditional 3D GAN, our model is still capable of handling inputs with large pose angles and\nout-of-distribution images. This finding suggests the effectiveness of our design, which incorporates\ndiffusion in the style space, enabling robust generation of avatars under varying conditions.\n13\nSource\nFull Landmarks\nPartial Landmarks\nVisible Landmarks Only\nFigure 9: Comparison of strategies for synthesizing Openpose annotations. Synthesizing all facial landmarks\nand synthesizing visible landmarks alone often result in ambiguity and incorrect poses. Empirically, we found\nthat synthesizing all visible landmarks along with the nose point yields the best results.\nonly fine pose\nonly coarse pose\nFigure 10: Typical failure cases in baselines. Left: When image-pose misalignment issues exist in the training\nimages and fine poses are used for training, the generation of the back side of the head is poor due to the\nover-reliance on pose labels. Right: Using only coarse poses for training may cause the model to converge\nto distorted 3D avatars that appear unrealistic. In contrast, our model utilizes both coarse and fine pose labels,\neffectively addressing this problem.\nInput\nOutput\nInput\nOutput\nInput\nOutput\nInput\nOutput\nInput\nOutput\nInput\nOutput\nFigure 11: More results on conditional avatar generation. Top: Input image with large pose angles. Down: Our\nof domain images as input. To generate out-of-domain images, we utilized StableDiffusion to transform a real\nface photo into the Disney style, which served as the input to our model.\n14\nTpos\nTstyle\nTable 4\nTview\nFront view: \u201cface, head\u201d\nSide view: \u201cside view of face, side face\u201d\nBack view: \u201cback of head, back side of the head\u201d\nTatt\nTable 5\nTneg\nTview\nBack view: \u201c(((nose, mouse, eyes))) \u201d\nTother\n\u201cstrong light, Bright light, intense light, dazzling light, brilliant light,\nradiant light, Shade, darkness, silhouette, dimness, obscurity, shadow,\nblur\u201d\nTable 3: Overview of the prompts we used for dataset construction.\n15\nStyle\nPrompt\nStyle\nPrompt\nDisney\u2217\nface, high quality, Disney style,\nDisney movie, Disney, 3D\nSculpture\u2217\nFace, high quality, 3D,\nSculpture, statue, Sculptures,\nstone sculpture, wood sculpture,\nmetal sculpture, ceramic\nsculpture, glass sculpture, statue,\ncarving, portrait sculpture, 3D\neffect, Stereoscopy\nDragon Ball\u2217\nface, animate, high quality, 3D,\nDragon Ball, Dragonball\nEvolution, Dragon Bowl,\nJapanese anime, manga, 3D\neffect, Stereoscopy\nAvatar\u2217\nface, animate, high quality,\nAvatar, Avatar style, Avatar\nmovie, movie, James Cameron,\nblue skin\nPixel Art\u2217\nFace, animate, high quality,\nminecraft style, minecraft, video\ngame, sandbox game, 3D\nAnime\u2217\nFace, animate, high quality, 3D,\nJapanese anime, manga, 3D\neffect, Stereoscopy\nSci-Fi\u2217\nface, animate, high quality,\nCharacter concept art, Sci-Fi\ndigital painting, trending on\nArtStation\nHulk\u2217\nhead, animate, high quality, 3D,\nHulk style, Hulk, Green Giant,\nmovie, 3D effect, Stereoscopy,\nblur background, blurred\nbackground\nJoker\u2217\nface avatar, face, head, cartoon,\nanimate, high quality, 3D,\njocker, Jocker, jocker face, 3D\neffect, Stereoscopy, blur\nbackground, blurred\nbackground, cute, lovely,\nadorable\nRobot\u2217\nface, animate, high quality,\nCharacter concept art, cyber\nrobot with human head, Sci-Fi\ndigital painting, trending on\nArtStation\nPop Art\nFace, high quality, pop art style,\nvibrant colors, bold lines, comic\nbook style, 3D effect,\nStereoscopy\nGraffiti\nFace, high quality, graffiti style,\nbold colors, street art, urban\nstyle, 3D effect, Stereoscopy,\nplain background, Solid color\nbackground\nSurrealism\nFace, high quality, Surrealist\nstyle, dreamlike, bizarre,\nabstract, 3D effect, Stereoscopy\nAmerican\nComics\nFace, animate, high quality,\nMarvel style, superhero, comic\nbook, 3D effect, Stereoscopy\nCubism\nFace, high quality, cubist style,\ngeometric shapes, multiple\nperspectives, abstract, 3D effect,\nStereoscopy\nModernism\nFace, high quality,\ncontemporary art style,\nexperimental, unconventional,\navant-garde, 3D effect,\nStereoscopy\nColorful\nFace, high quality, colorful style,\nbright and bold colors, abstract,\n3D effect, Stereoscopy\nSand Painting\nFace, high quality, sand painting\nstyle, intricate patterns, textured,\n3D effect, Stereoscopy\nPokemon\nFace, animate, high quality,\nPokemon style, anime, video\ngame, 3D effect, Stereoscopy\nCyberpunk\nFace, animate, high quality,\nneon lights, futuristic, dystopian,\ncybernetic implants, 3D effect,\nStereoscopy\nRealistic\nFace, high quality, realistic style,\nlifelike, detailed, 3D effect,\nStereoscopy\nCartoon\nFace, animate, high quality,\ncartoon style, exaggerated\nfeatures, bright colors, 3D\neffect, Stereoscopy\nSteampunk\nFace, high quality, steampunk\nstyle, Victorian era, gears,\nclockwork, 3D effect,\nStereoscopy\nManga\nFace, animate, high quality,\nmanga style, Japanese comics,\nblack and white, 3D effect,\nStereoscopy\nArt Nouveau\nFace, high quality, Art Nouveau\nstyle, flowing lines, organic\nshapes, 3D effect, Stereoscopy\nExpressionism\nFace, high quality, Expressionist\nstyle, distorted features, bold\ncolors, 3D effect, Stereoscopy\nTable 4: Style-related prompts. The prompts of the top 10 styles (\u2217) are manually designed, while the rest is\ngenerated by ChatGPT.\n16\nStyle\nPrompt\nStyle\nPrompt\nFauvism\nFace, high quality, Fauvist style,\nbold colors, simplified forms,\n3D effect, Stereoscopy\nArt Deco\nFace, high quality, Art Deco\nstyle, geometric shapes, metallic\naccents, 3D effect, Stereoscopy\nPop\nSurrealism\nFace, high quality, Pop\nSurrealist style, surreal imagery,\nbright colors, 3D effect,\nStereoscopy\nBaroque\nFace, high quality, Baroque\nstyle, ornate details, dramatic\nlighting, 3D effect, Stereoscopy\nRococo\nFace, high quality, Rococo style,\npastel colors, ornate details, 3D\neffect, Stereoscopy\nNeo-\nExpressionism\nFace, high quality,\nNeo-Expressionist style, bold\ncolors, thick brushstrokes, 3D\neffect, Stereoscopy\nArt Brut\nFace, high quality, Art Brut\nstyle, raw and unrefined,\nchildlike, 3D effect, Stereoscopy\nSurrealist\nPhotography\nFace, high quality, surrealist\nphotography style, dreamlike,\nbizarre, abstract, 3D effect,\nStereoscopy\nConcept Art\nFace, high quality, concept art\nstyle, imaginative, futuristic, 3D\neffect, Stereoscopy\nLow Poly\nFace, animate, high quality, low\npoly style, geometric shapes,\nbright colors, 3D effect,\nStereoscopy\nArt\nSynthetique\nFace, high quality, Art\nSynthetique style, digital art,\nabstract, 3D effect, Stereoscopy\nArt Informel\nFace, high quality, Art Informel\nstyle, spontaneous, abstract, 3D\neffect, Stereoscopy\nTonalism\nFace, high quality, Tonalist style,\nmuted colors, atmospheric, 3D\neffect, Stereoscopy\nChibi\nFace, animate, high quality,\nchibi style, small and cute,\nexaggerated features, 3D effect,\nStereoscopy\nClaymation\nFace, animate, high quality,\nclaymation style, stop-motion\nanimation, tactile, 3D effect,\nStereoscopy\nCutout\nFace, animate, high quality,\ncutout style, paper cutouts,\nstop-motion animation, 3D\neffect, Stereoscopy\nFlash\nAnimation\nFace, animate, high quality,\nflash animation style, vector\ngraphics, smooth animation, 3D\neffect, Stereoscopy\nHand-Drawn\nFace, animate, high quality,\nhand-drawn style, traditional\nanimation, pencil and paper, 3D\neffect, Stereoscopy\nMotion\nGraphics\nFace, animate, high quality,\nmotion graphics style,\ntypography, kinetic typography,\n3D effect, Stereoscopy\nPencil Test\nFace, animate, high quality,\npencil test style, rough\nanimation, sketchy, 3D effect,\nStereoscopy\nPhotorealistic\nFace, animate, high quality,\nphotorealistic style, lifelike,\ndetailed, 3D effect, Stereoscopy\nRubber Hose\nFace, animate, high quality,\nrubber hose style, 1920s\nanimation style, flexible limbs,\n3D effect, Stereoscopy\nSand\nAnimation\nFace, animate, high quality,\nsand animation style, sand art,\nstop-motion animation, 3D\neffect, Stereoscopy\nStop-Motion\nFace, animate, high quality,\nstop-motion animation style,\npuppetry, claymation, 3D effect,\nStereoscopy\nTraditional\nFace, animate, high quality,\ntraditional animation style,\nhand-drawn, frame-by-frame,\n3D effect, Stereoscopy\nVector\nFace, animate, high quality,\nvector style, clean lines,\nscalable, 3D effect, Stereoscopy\nWoodcut\nFace, animate, high quality,\nwoodcut style, carved lines,\nblack and white, 3D effect,\nStereoscopy\nZoetrope\nFace, animate, high quality,\nzoetrope style, pre-cinema\nanimation, spinning cylinder,\n3D effect, Stereoscopy\nTable 4: Style-related prompts. (Continued) The prompts of the top 10 styles (\u2217) are manually designed, while\nthe rest is generated by ChatGPT.\n17\nAttribute\nDescription\nEye Shape\nsmall-eyed, big-eyed, almond-shaped eyes, round eyes, narrow eyes, deep-set\neyes, protruding eyes, close-set eyes, wide-set eyes\nEyebrows\nthick-browed, sparse-browed, arched eyebrows, straight eyebrows, bushy\neyebrows, thin eyebrows, unibrow\nEyelashes\nlong-lashed, short-lashed, thick lashes, sparse lashes, curled lashes\nCheeks\nrosy-cheeked, pale-cheeked, chubby cheeks, hollow cheeks, high cheekbones,\nlow cheekbones\nEars\nbig-eared, small-eared, attached earlobes, detached earlobes, ear piercings\nExpression\nhappy, sad, angry, surprised, tired, anxious, nervous, handsome, ugly, smiling,\nfrowning, scowling, smirking, pouting, grinning, winking, raising eyebrows\nFacial Hair\nmoustache, beard, goatee, stubble, clean-shaven, sideburns\nEye Color\nblue eyes, black eyes, brown eyes, green eyes, hazel eyes, gray eyes\nSkin\nfreckle, mole, wrinkled, smooth skin, acne-prone skin, oily skin, dry skin,\nsensitive skin\nRace\nAsian, European, Africans, Latino, Middle Eastern, Indian, mixed race\nAge\nold, young, middle-aged, elderly, baby-faced, mature\nChin\nthick-lipped, thin-lipped, cleft chin, dimpled chin, pointed chin, square chin,\nround chin\nFace Shape\nsquare-faced, thin-faced, round-faced, chubby-faced, pointy-chinned,\nprominent-chinned, heart-shaped face, oval face, diamond-shaped face\nNose\nshort-nosed, long-nosed, high-nosed, low-nosed, high-bridged nose,\nlow-bridged nose, upturned nose, downturned nose, button nose, Roman nose\nLips\nfull-lipped, thin-lipped, downturned lips, upturned lips, bow-shaped lips,\nheart-shaped lips, thin upper lip, full lower lip\nForehead\nhigh forehead, low forehead, receding hairline, widow\u2019s peak\nEye Sockets\ndeep-set eyes, hooded eyes, almond-shaped eyes, protruding eyes, round eyes,\nsunken eyes\nFacial Features\ndimpled chin, cleft chin, birthmark, scar, tattoo, beauty mark, mole, freckles\nFacial Contour\nsharp jawline, soft jawline, high cheekbones, low cheekbones, narrow face, wide\nface\nFacial\nImpression\nfriendly, serious, confident, approachable, intimidating, warm, cold, inviting,\nunapproachable\nHairstyle\nbald, short hair, long hair, curly hair, straight hair, wavy hair, bangs, ponytail,\nbun, braids, cornrows\nTable 5: Attribute-related prompts. All prompts are generated by ChatGPT.\n18\n"
  },
  {
    "title": "Real-World Image Variation by Aligning Diffusion Inversion Chain",
    "link": "https://arxiv.org/pdf/2305.18729.pdf",
    "upvote": "3",
    "text": "Real-World Image Variation by Aligning Diffusion\nInversion Chain\nYuechen Zhang1\nJinbo Xing1\nEric Lo1\nJiaya Jia1, 2\n1The Chinese University of Hong Kong\n2SmartMore\n{yczhang21, jbxing, ericlo, leojia}@cse.cuhk.edu.hk\n\u201cCat\u201d\n\u201cGirl\u201d\nImage Variations\nGeneration with style condition\n\u201cCrown\u201d\nFigure 1: Leveraging an image exemplar, our training-free method excels in image generation tasks\nsuch as generating (a) image variations, (b) images of a similar style (with conditions).\nAbstract\nRecent diffusion model advancements have enabled high-fidelity images to be\ngenerated using text prompts. However, a domain gap exists between generated\nimages and real-world images, which poses a challenge in generating high-quality\nvariations of real-world images. Our investigation uncovers that this domain gap\noriginates from a latents\u2019 distribution gap in different diffusion processes. To\naddress this issue, we propose a novel inference pipeline called Real-world Image\nVariation by ALignment (RIVAL) that utilizes diffusion models to generate image\nvariations from a single image exemplar. Our pipeline enhances the generation\nquality of image variations by aligning the image generation process to the source\nimage\u2019s inversion chain. Specifically, we demonstrate that step-wise latent dis-\ntribution alignment is essential for generating high-quality variations. To attain\nthis, we design a cross-image self-attention injection for feature interaction and\na step-wise distribution normalization to align the latent features. Incorporat-\ning these alignment processes into a diffusion model allows RIVAL to generate\nhigh-quality image variations without further parameter optimization. Our ex-\nperimental results demonstrate that our proposed approach outperforms existing\nmethods concerning semantic similarity and perceptual quality. This generalized\ninference pipeline can be easily applied to other diffusion-based generation tasks,\nsuch as image-conditioned text-to-image generation and stylization. Project page:\nhttps://rival-diff.github.io\n1\nIntroduction\nGenerating real-world image variation is a crucial area of research in computer vision and machine\nlearning, owing to its practical applications in image editing, synthesis, and data augmentation [1, 2].\nThis task involves the generation of diverse variations of a given real-world image while preserving its\nsemantic content and visual quality. Early methods for generating image variations included texture\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18729v3  [cs.CV]  7 Nov 2023\nvalue\ndistribution\nInverted \nlatent !!\n\"\n\"(0, 1)\nImage exemplar (\nDistribution Gap\nDomain Gap\nReal-image\ninversion space\nDenoising step\nReal-image inversion chain\nVanilla denoising chain\nStandard\nGaussian\n!!\n\"\n!!\n#\n!$\n#\n!$\n\"\nFigure 2: Conceptual illustration of the challenges in the real-world image variation. Left: Despite\nusing the same text prompt \"a lynx sitting in the grass\", a bias exists between the latent\ndistribution in the vanilla generation chain and the image inversion chain, resulting in a significant do-\nmain gap between the denoised images. Right: Visualization of the real-image inverted four-channel\nlatent. The distribution of the latent is biased in comparison to a standard Gaussian distribution.\nsynthesis, neural style transfer, and generative models [3, 4, 5, 6, 7, 8]. However, these methods are\nlimited in generating realistic and diverse variations from real-world images and are only suited for\ngenerating variations of textures or artistic images.\nDenoising Diffusion Probabilistic Models (DDPMs) have resulted in significant progress in text-driven\nimage generation [9, 10, 11]. However, generating images that maintain the style and semantic content\nof the reference remains a significant challenge. Although advanced training-based methods [12, 13,\n14, 15] can generate images with novel concepts and styles with given images, they require additional\ntraining stages and data. Directly incorporating image as the input condition [11, 16] results in\nsuboptimal visual quality and content diversity compared to the reference input. Besides, they do\nnot support input with text descriptions. A plug-and-play method has not been proposed to generate\nhigh-quality, real-world image variations without extra optimization.\nObserving the powerful denoising ability of pre-trained DDPMs in recovering the original image\nfrom the inverted latent space [17, 18], we aim to overcome the primary challenge by modifying\nthe vanilla latent denoising chain of the DDPM to fit the real-image inversion chain [17, 18, 19].\nDespite generating images with the same text condition, a significant domain gap persists between\nthe generated and source images, as depicted in Fig. 2. We identify distribution misalignment as the\nprimary factor that impedes the diffusion model from capturing certain image features from latents\nin the inversion chain. As illustrated in the right portion of Fig. 2, an inverted latent may differ\nsignificantly from the standard Gaussian distribution. This misalignment accumulates during the\ndenoising process, resulting in a domain gap between the generated image and its reference exemplar.\nTo address this distribution gap problem for generating image variations, we propose an pure inference\npipeline called Real-world Image Variation by Alignment (RIVAL). RIVAL is a tunning-free approach\nthat reduces the domain gap between the generated and real-world images by aligning the denoising\nchain with the real-image inversion chain. Our method comprises two key components: (i) a cross-\nimage self-attention injection that enables cross-image feature interaction in the variation denoising\nchain, guided by the hidden states from the inversion chain, and (ii) a step-wise latent normalization\nthat aligns the latent distribution with the inverted latent in early denoising steps. Notably, this\nmodified inference process requires no training and is suitable for arbitrary image input.\nAs shown in Fig. 1, our proposed approach produces visually appealing image variations while\nmaintaining semantic and style consistency with a given image exemplar. RIVAL remarkably\nimproves the quality of image variation generation qualitatively and quantitatively compared to\nexisting methods [11, 16, 20]. Furthermore, we have demonstrated that RIVAL\u2019s alignment process\ncan be applied to other text-to-image tasks, such as text-driven image generation with real-image\ncondition [21, 22, 23] and example-based inpainting [24, 25].\nThis paper makes three main contributions. (1) Using a real-world image exemplar, we propose a\nnovel tunning-free approach to generate high-quality image variations. (2) We introduce an latent\nalignment process to enhance the quality of the generated variations. (3) Our proposed method offers\na promising denoising pipeline that can be applied across various applications.\n2\nRelated Works\nDiffusion models with text control represent advanced techniques for controllable image generation\nwith text prompts. With the increasing popularity of text-to-image diffusion models [11, 16, 26, 10,\n2\nReal-image inversion\nT2I model\nV\n/\nK\nQ\nCross-image self-attention\n\u2026\n\u2026\n\u2026\n\u2026\n!!\n\"\n!!\n#\n!$\n#\n!$\n%&'\n!$\n%\n!$\n%('\n!!\n%&'\n!!\n%\n!$\n\"\nImage exemplar !\nImage variation \"\nLatent\nAlignment\n\" 0, %\nNoise\nalign\nV\n/\nK\ninjection\nFigure 3: High-level framework of RIVAL. Input exemplar R is inverted to a noisy latent XT\nR. An\nimage variation G is generated from random noise following the same distribution as XT\nR. For each\ndenoising step t, we interact Xt\nR and Xt\nG by self-attention injection and latent alignment.\n27], high-quality 2D images can be generated through text prompt input. However, such methods\ncannot guarantee to generate images with the same low-level textures and tone mapping as the given\nimage reference, which is difficult to describe using text prompts. Another line of works aims for\ndiffusion-based concept customization, which requires fine-tuning the model and transferring image\nsemantic contents or styles into text space for new concept learning [12, 13, 14, 15, 28]. However,\nthis involves training images and extra tuning, making it unsuitable for plug-and-play inference.\nReal-world image inversion is a commonly employed technique in Generative Adversarial Net-\nworks (GANs) for image editing and attribute manipulation [6, 29, 30, 31, 32]. It involves inverting\nimages to the latent space for reconstructions. In diffusion models, the DDIM sampling technique [17]\nprovides a deterministic and approximated invertible diffusion process. Recently developed inversion\nmethods [18, 19] guarantee high-quality reconstruction with step-wise latent alignments. With diffu-\nsion inversion, real-image text-driven manipulations can be performed in image and video editing\nmethods [33, 34, 23, 22, 35, 36, 37]. These editing methods heavily rely on the original structure of\nthe input, thus cannot generate free-form variations with the same content as the reference image\nfrom the perspective of image generation.\nImage variation generation involves generating diverse variations of a given image exemplar while\npreserving its semantic content and visual quality. Several methods have been proposed for this\nproblem, including neural style transfer [5, 38, 39, 40], novel view synthesis [41, 42, 43], and GAN-\nbased methods [7, 44, 32]. However, these methods were limited to generating variations of artistic\nimages or structure-preserved ones and were unsuitable for generating realistic and diverse variations\nof real-world images. Diffusion-based methods [16, 11, 20] can generate inconsistent image variants\nby analyzing the semantic contents from the reference while lacking low-frequency details. Recent\nmethods using image prompt to generate similar contents or styles [45, 46, 47, 48] requires additional\ncase-wise tuning or training. Another concurrent approach, MasaCtrl [23], adopts self-attention\ninjection as guidance for the denoising process. Yet, it can only generate variations with a generated\nimage and fails on real-world image inputs. In contrast, RIVAL leverages the strengths of diffusion\nchain alignment to generate variations of real-world images.\n3\nReal-World Image Variation by Alignment\nIn this work, we define the image variation as the construction of a diffusion process F that satisfies\nD(R, C) \u2248 D(FC(X, R), C), where D(\u00b7, C) is a data distribution function with a given semantic\ncontent condition C. The diffusion process FC(X, R) = G generates the image variation G based\non a sampled latent feature X, condition C, and the exemplar image R.\nThe framework of RIVAL is illustrated in Fig. 3. RIVAL generates an inverted latent feature chain\n{X0\nR, ..., XT\nR} by inverting a reference exemplar image R using DDIM inversion [17]. Then we\nobtain the initial latent XT\nR of the inversion chain. Next, a random latent feature XT\nG is sampled\nas the initial latent of the image generation (denoising) chain. In this multi-step denoising chain\n{XT\nG, ..., X0\nG} for generation, we align the latent features to latents from the inversion chain to\nobtain perceptually similar generations. The modified step-wise denoising function f at step t can be\nrepresented abstractly as:\nXt\nG = ft(Xt+1\nG\n, Xt+1\nR\n, C).\n(1)\n3\nReal exemplar\n+ cross-image attention\n+ latent aligned attention\nGenerated exemplar\n+ cross-image attention\nFigure 4: Exemplar images and generation results are obtained using the prompt \"backpack in\nthe wild\". Cross-image self-attention can generate faithful outputs when a vanilla generation chain\nis employed as guidance. In the case of real-world images, the generation of faithful variations is\ndependent on the alignment of the latent.\nThis function is achieved by performing adaptive cross-image attention in self-attention blocks for\nfeature interaction and performing latent distribution alignment in the denoising steps. The denoising\nchain can produce similar variations by leveraging latents in the inversion chain as guidance.\n3.1\nCross-Image Self-Attention Injection\nTo generate image variations from an exemplar image R, a feature interaction between the inversion\nchain and the generation chain is crucial. Previous works [49, 22] have shown that self-attention\ncan efficiently facilitate feature interactions. Similar to its applications [33, 23, 37] in text-driven\ngeneration and editing tasks, we utilize intermediate hidden states vR obtained from the inversion\nchain to modify the self-attention in the generation chain. Specifically, while keeping the inversion\nchain unchanged, our modified Key-Value features for cross-image self-attention for one denoising\nstep t in the generation chain are defined as follows:\nQ = W Q(vG), K = W K(v\u2032\nG), V = W V (v\u2032\nG), where\n(2)\nv\u2032\nG =\n\u001avG \u2295 vR\nif t \u2264 talign\nvR\notherwise\n.\n(3)\nWe denote W (\u00b7) as the pre-trained, frozen projections in the self-attention block and use \u2295 to represent\nconcatenation in the spatial dimension. Specifically, we adopt an Attention Fusion strategy. In early\nsteps (t > talign), we replace the KV values with W V (vR), W K(vR) using the hidden state vR from\nthe inversion chain. In subsequent steps, we concatenate vR and the hidden state from the generation\nchain itself vG to obtain new Key-Values. We do not change Q values and maintain them as W Q(vG).\nThe proposed adaptive cross-image attention mechanism explicitly introduces feature interactions in\nthe denoising process of the generation chain. Moreover, Attention Fusion strategy aligns the content\ndistribution of the latent features in two denoising chains.\nBuilding upon the self-attention mechanism [50], we obtain the updated hidden state output as\nv\u2217\nG = softmax\n\u0000QK\u22a4/\u221adk\n\u0001\nV , where dk is the dimensionality of K. Cross-image self-attention and\nfeature injection can facilitate the interaction between hidden features in the generation chain and the\ninversion chain. It should be noted that the inversion chain is deterministic throughout the inference\nprocess, which results in reference feature vR remaining independent of the generation chain.\n3.2\nInverted Latent Chain Alignment\nThe cross-image self-attention injection is a potent method for generating image variations from\na vanilla denoising process originating from a standard Gaussian distribution and is demonstrated\nin [23]. However, as depicted in Fig. 4, direct adaptation to real-world image input is not feasible\ndue to a domain gap between real-world inverted latent chains and the vanilla generation chains.\nThis leads to the attenuation of attention correlation during the calculation of self-attention. We\nfurther visualize this problem in Fig. 12. To facilitate the generation of real-world image variations,\nthe pseudo-generation chain (inversion chain) of the reference exemplar can be estimated using\nthe DDIM inversion [17]. Generating an image from latent features XT using a small number of\ndenoising steps is possible with the use of deterministic DDIM sampling:\nXt\u22121 =\np\n\u03b1t\u22121/\u03b1t \u00b7 Xt + \u221a\u03b1t\u22121(\u03b2t\u22121 \u2212 \u03b2t) \u00b7 \u03b5\u03b8\n\u0000Xt, t, C\n\u0001\n,\n(4)\nwhere step-wise coefficient is set to \u03b2t =\np\n1/\u03b1t \u2212 1 and \u03b5\u03b8(Xt, t, C) is the pre-trained noise\nprediction function in one timestep. Please refer to Appendix A for the details of \u03b1t and DDIM\n4\nsampling. With the assumption that noise predictions are similar in latents with adjacent time steps,\nDDIM sampling can be reversed In a small number of steps t \u2208 [0, T], using the equation:\nXt+1 =\np\n\u03b1t+1/\u03b1t \u00b7 Xt + \u221a\u03b1t+1(\u03b2t+1 \u2212 \u03b2t) \u00b7 \u03b5\u03b8\n\u0000Xt, t, C\n\u0001\n,\n(5)\nwhich is known as DDIM inversion [17]. We apply DDIM to acquire the latent representation XT\nR\nof a real image R. Nevertheless, the deterministic noise predictions in DDIM cause uncertainty in\nmaintaining a normal distribution N(0, I) of the inverted latent XT\nR. This deviation from the target\ndistribution causes the attention-attenuation problem [51] between generation chains of R and G,\nleading to the generation of images that diverge from the reference. To tackle this challenge, we find\na straightforward distribution alignment on the initial latent XT\nG useful, that is,\nxT\nG \u2208 XT\nG \u223c N\n\u0000\u00b5\n\u0000XT\nR\n\u0001\n, \u03c32\u0000XT\nR\n\u0001\u0001\n, or XT\nG = shuffle\n\u0000XT\nR\n\u0001\n.\n(6)\nIn the alignment process, pixel-wise feature elements xT\nG \u2208 XT\nG in the spatial dimension can be\ninitialized from one of the two sources: (1) an adaptively normalized Gaussian distribution or (2)\npermutated samples from the inverted reference latent, shuffle(XT\nR). Our experiments show that both\ntypes of alignment yield comparable performance. This latent alignment strengthens the association\nbetween the variation latents and the exemplar\u2019s latents, increasing relevance within the self-attention\nmechanism and thus increasing the efficacy of the proposed cross-image attention.\nIn the context of initializing the latent variable XT\nG, further alignment steps within the denoising chain\nare indispensable. This necessity arises due to the limitations of the Classifier-Free Guidance (CFG)\ninference proposed in [52]. While effective for improving text prompt guidance in the generation\nchain, CFG cannot be directly utilized to reconstruct high-quality images through DDIM inversion.\nOn the other hand, the noise scaling of CFG will affect the noise distribution and lead to a shift\nbetween latents in two chains, as shown in Fig. 11. To avoid this misalignment while attaining the\nadvantage of the text guidance in CFG, we decouple two inference chains and rescale the noise\nprediction during denoising inference, formulated as an adaptive normalization [38]:\n\u03f5\u03b8\n\u0000Xt\nG, t, C\n\u0001\n= AdaIN(\u03b5cfg\n\u03b8\n\u0000Xt\nG, t, C\n\u0001\n, \u03b5\u03b8\n\u0000Xt\nR, t, C\n\u0001\n), where\n(7)\n\u03b5cfg\n\u03b8\n\u0000Xt\nG, t, C\n\u0001\n= m\u03b5\u03b8\n\u0000Xt\nG, t, C\n\u0001\n+ (1 \u2212 m)\u03b5\u03b8\n\u0000Xt\nG, t, C\u2217\u0001\n, and t > tearly.\n(8)\n\u03b5cfg\n\u03b8 (Xt\nG, t, C) is the noise prediction using guidance scale m and unconditional null-text C\u2217. We\nincorporate this noise alignment approach during the initial stages (t > tearly). After that, a vanilla\nCFG scaling is applied as the content distribution in G may not be consistent with R. Aligning the\ndistributions at this early stage proves advantageous for aligning the overall denoising process.\n4\nExperiments\nWe evaluate our proposed pipeline through multiple tasks in Sec. 4.2: image variation, image\ngeneration with text and image conditions, and example-based inpainting. Additionally, we provide\nquantitative assessments and user study results to evaluate generation quality in Sec. 4.3. A series of\nablation studies and discussions of interpretability are conducted in Sec. 4.4 to assess the effectiveness\nof individual modules within RIVAL.\n4.1\nImplementation Details\nOur study obtained a high-quality test set of reference images from the Internet and DreamBooth [12]\nto ensure a diverse image dataset. To generate corresponding text prompts C, we utilized BLIP2 [54].\nOur baseline model is Stable-Diffusion V1.5. During the image inversion and generation, we\nemployed DDIM sample steps T = 50 for each image and set the classifier-free guidance scale\nm = 7 in Eq. (8). We split two stages at talign = tearly = 30 for attention alignment in Eq. (3)\nand latent alignment in Eq. (8). In addition, we employ the shuffle strategy described in Eq. (6) to\ninitialize the starting latent XT\nG. Experiments run on a single NVIDIA RTX4090 GPU with 8 seconds\nto generate image variation with batch size 1.\n4.2\nApplications and Comparisons\nImage variation. As depicted in Fig. 5, real-world image variations exhibit diverse characteristics\nto evaluate perceptual quality. Text-driven image generation using the basic Stable Diffusion [11]\ncannot use image exemplars explicitly, thus failing to get faithful image variations. While recent\nimage-conditioned generators such as [20, 53, 16] have achieved significant progress for image input,\n5\nSD variation\nELITE\nDALLE-2\nSource exemplar\nStable Diffusion\n\u201cAn old black and white drawing of a city.\u201d\n\u201cA group of people standing around a statue.\u201d\n\u201cA girl in a sailor uniform posing for a photo.\u201d\n\u201cA photo of backpack.\u201d\n\u201cA photo of a dog lying on a sofa.\u201d\nRIVAL (Ours)\nFigure 5: Real-world image variation. We compare RIVAL with recent competitive methods [11, 53,\n20] conditioned on the same text prompt (the 2nd column) or exemplar image (3rd-5th columns).\ntheir reliance on encoding images into text space limits the representation of certain image features,\nincluding texture, color tone, lighting environment, and image style. In contrast, RIVAL generates\nimage variations based on text descriptions and reference images, harnessing the real-image inversion\nchain to facilitate latent distribution alignments. Consequently, RIVAL ensures high visual similarity\nin both semantic content and low-level features. For instance, our approach is the sole technique that\ngenerates images with exceedingly bright or dark tones (2nd-row in Fig. 5). More visual comparisons\nand experimental settings can be found in Appendix C.\nText-driven image generation. In addition to the ability to generate images corresponding to the\nexemplar image and text prompts, we have also discovered that RIVAL has a strong ability to transfer\nstyles and semantic concepts in the exemplar for a casual text-driven image generation. When an\ninversion with the original text prompt is performed, the alignment on denoising processes can still\ntransfer high-level reference style to the generated images with modified prompts. This process\ncould be directly used in structure-preserved editing. This could be regarded as the same task of\nMasaCtrl and Plug-and-Play [23, 34], which utilizes an initialized inverted latent representation to\ndrive text-guided image editing. A comparison is shown in the right part of Fig. 6, here we directly\nuse the inverted latent XT\nG = XT\nR for two chains. We adopt the same interaction starting step t = 45\nas the MasaCtrl.\nFurthermore, in the absence of a structure-preserving prior, it is possible for a user-defined text prompt\nto govern the semantic content of a freely generated image G. This novel capability is showcased\n6\n{adj.}: \u201cin a sailor uniform\u201d \n\u201clynx\u201d*\n\u201cdog\u201d*\n\u201cdog\u201d\n\u201ccat\u201d\nw/o RIVAL*\n{n.}: \u201cwomen\u201d, \n{adj.}: \u201c\u201d *\n{n.}: \u201crobot\u201d\n{n.}: \u201cgirl\u201d\n{n.}: \u201cElon Musk\u201d\n{adj.}: \u201c\u201d\n{n.}: \u201crobot\u201d*\n\u201cA {} is sitting in the grass\u201d\n\u201cA {n.} {adj.} is posing for a portrait\u201d\n\u201cCat\u201d\n\u201cGirl in sailor uniform\u201d\n\u201cCat\u201d\n\u201craising hand\u201d\nPlug-and-Play\nMasaCtrl\nOurs\nFigure 6: Left: Text-driven image generation results using RIVAL, with the leftmost image as the\nsource exemplar. RIVAL can generate images with diverse text inputs while preserving the reference\nstyle. Images noted with * indicate structure-preserved editing, which starts with the same initialized\ninverted reference latent XT\nR as the exemplar. Right: a visual comparison of structure-preserved\nimage editing with recent approaches [34, 23].\n\u201cElon Musk for a portrait\u201d\n\u201ccity\u201d \u2192 \u201cportrait\u201d\n\u201cbig house\u201d\n\u201cbig house\u201d\n\u201ca photo of Trump\u201d\n\u201ca portrait of spider man\u201d\n\u201cillustration of a robot\u201d\n\u201cillustration of a robot\u201d\n\u201cElon Musk for a portrait\u201d\n\u201ccity\u201d \u2192 \u201cportrait\u201d\n\u201cbig house\u201d\n\u201cbig house\u201d\n\u201ca photo of Trump\u201d\n\u201ca portrait of spider man\u201d\n\u201cillustration of a robot\u201d\n\u201cillustration of a robot\u201d\n\u201cElon Musk for a portrait\u201d\n\u201ccity\u201d \u2192 \u201cportrait\u201d\n\u201cbig house\u201d\n\u201cbig house\u201d\n\u201ca photo of Trump\u201d\n\u201ca portrait of spider man\u201d\n\u201cillustration of a robot\u201d\n\u201cillustration of a robot\u201d\n\u201cElon Musk for a portrait\u201d\n\u201ccity\u201d \u2192 \u201cportrait\u201d\n\u201cbig house\u201d\n\u201cbig house\u201d\n\u201ca photo of Trump\u201d\n\u201ca portrait of spider man\u201d\n\u201cillustration of a robot\u201d\n\u201cillustration of a robot\u201d\n\u201cElon Musk \nfor a portrait\u201d\n\u201ccity\u201d \u2192 \u201cportrait\u201d\n\u201cBig house\u201d\n\u201cBig house\u201d\n\u201cA portrait of\nspider man\u201d\n\u201cA photo of\nTrump\u201d\n\u201cillustration of a\nrobot\u201d\n\u201cillustration of a\nrobot\u201d\nCanny Edge\nOpenPose\nSegmentation Map\nDepth Map\nFigure 7: The availability of RIVAL with ControlNet [55]. Two examples are given for each modality\nof the control condition. Exemplars are shown on the left of each image pair.\nsource exemplar\nsource exemplar\nPaint-by-example\nPaint-by-example\nRIVAL\nRIVAL\nRIVAL-variation 2\nFigure 8: RIVAL extended to self-example image inpainting. We use the same image as the example\nto fill the masked area. Results are compared with a well-trained inpainting SOTA method [25].\nin the left part of Fig. 6, where RIVAL successfully generates images driven by text input while\nmaintaining the original style of the reference image. Further, to extend this ability to other image\nconditions, we adapt RIVAL with ControlNet Fig. 7 by adding condition residuals of ControlNet\nblocks on the generation chain, as shown in Fig. 7. With RIVAL, we can easily get a style-specific\ntext-to-image generation. For instance, it can produce a portrait painting of a robot adorned in a sailor\nuniform while faithfully preserving the stylistic characteristics.\nExample-based image inpainting. When abstracting RIVAL as a novel paradigm of image-based\ndiffusion inference, we can extend this framework to enable it to encompass other image editing tasks\nlike inpainting. Specifically, by incorporating a coarse mask M into the generation chain, we only\npermute the inverted latent variables within the mask to initialize the latent variable XT\nG. During\ndenoising, we replace latents in the unmasked regions with latents in the inversion chain for each\nstep. As shown in Fig. 8, we present a visual comparison between our approach and [25]. RIVAL\nproduces a reasonable and visually harmonious inpainted result within the mask.\n7\nMetric\nSD [11]\nImgVar [53]\nELITE [20]\nDALL\u00b7E 2 [16]\nRIVAL\nText Alignment \u2191\n0.255 \u00b1 0.04\n0.223 \u00b1 0.04\n0.209 \u00b1 0.05\n0.253 \u00b1 0.04\n0.275 \u00b1 0.03\nImage Alignment \u2191\n0.748 \u00b1 0.08\n0.832 \u00b1 0.07\n0.736 \u00b1 0.09\n0.897 \u00b1 0.05\n0.840 \u00b1 0.07\nPalette Distance \u2193\n3.650 \u00b1 1.30\n3.005 \u00b1 0.86\n2.885 \u00b1 0.81\n2.102 \u00b1 0.83\n1.674 \u00b1 0.63\nReal-world Authenticity \u2193\n-\n2.982 (4.7%)\n3.526 (9.6%)\n1.961 (28.0%)\n1.530 (61.6%)\nCondition Adherence \u2193\n-\n3.146 (7.3%)\n3.353 (5.6%)\n1.897 (30.6%)\n1.603 (56.4%)\nTable 1: Quantitative comparisons. We evaluate the quality of image variation regarding feature\nmatching within different levels (color palette, text feature, image feature), highlighted with best and\nsecond best results. We also report user preference rankings and the first ranked rate.\nMethods\nText\nAlign. \u2191\nImage\nAlign. \u2191\nPalette\nDist. \u2193\nLPIPS \u2193\nPreparation\nTime (s) \u2193\nInference\nTime (s) \u2193\nPnP [34]\n0.249\n0.786\n1.803\n0.245\n200\n21\nMasaCtrl [23]\n0.226\n0.827\n1.308\n0.274\n6\n15\nRIVAL\n0.231\n0.831\n1.192\n0.245\n6\n15\nTable 2: Quantitative comparisons with image editing methods. Both in performance and efficiency\n(time consumption on one NVIDIA RTX3090 for fair comparison).\nBaseline\nAttention Inject\nAttention fusion\nLatent init.\nNoise align\nw/o attention\noperations\nw/o attn fusion,\nnoise align\nw/o attn fusion\nw/o latent init.\nw/o noise align\nFull model\nExemplar\nFigure 9: Left: a visual example for a module-wise ablation study.\n(50, 50)\n(40, 40)\n(30, 30)\n(0, 0)\n(0, 50)\n(50, 0)\nFigure 10: Ablation study for different early-alignment strategies with the exemplar in Fig. 5. We list\n(talign, tearly) pairs for each image. All images are generated from the same fixed latent.\n4.3\nQuantitative Evaluation\nWe compare RIVAL with several state-of-the-art methods, employing the widely used CLIP-score [56]\nevaluation metric for alignments with text and the image exemplar. The evaluation samples are from\ntwo datasets: the DreamBooth dataset [12] and our collected images. In addition, we cluster a 10-color\npalette using k-means and calculate the minimum bipartite distance to assess the similarity in low-\nlevel color tones. In general, the results in Tab. 1 demonstrate that RIVAL significantly outperforms\nother methods regarding semantic and low-level feature matching. Notably, our method achieves\nbetter text alignment with the real-image condition than the vanilla text-to-image method using the\nsame model. Furthermore, our alignment results are on par with those attained by DALL\u00b7E 2 [16],\nwhich directly utilizes the image CLIP feature as the condition.\nWe conducted a user study to evaluate further the generated images\u2019 perceptual quality. This study\nutilized results generated by four shuffled methods based on the same source. Besides, the user study\n8\nCFG=3\nCFG=7\nCFG=11\nCFG=1,\norigin image\nCFG=3\nCFG=7\nCFG=11\nVanilla Inverted Inference\nRIVAL inference\nFigure 11: High CFG artifacts defense. Left: vanilla generation, starting from the DDIM inverted\nlatent. Right: RIVAL generated images, starting from the permuted latents.\nDenoising Step\nDenoising Step\nFigure 12: RIVAL\u2019s interpretability. Left: KL divergence between latent among two chains across\ndenoising steps. Right: attention score with the reference feature with different alignment strategies.\nincludes a test for image authenticity, which lets users identify the \"real\" image among generated\nresults. We collected responses of ranking results for each case from 41 participants regarding visual\nquality. The findings indicate a clear preference among human evaluators for our proposed approach\nover existing methods [53, 20, 16]. The detailed ranking results are presented in Tab. 1.\nWe also compare RIVAL with PnP and MasaCtrl for image editing on a representative test set,\nwith additional metrics in Perceptual Similarity [57] and inference time. Our method presents a\ncompetitive result in Tab. 2.\n4.4\nAblation Studies\nWe perform ablation studies to evaluate the efficacy of each module in our design. Additional visual\nand quantitative results can be found in Appendix D.\nCross-image self-attention injection plays a crucial role in aligning the feature space of the source\nimage and the generated image. We eliminated the cross-image self-attention module and generated\nimage variations to verify this. The results in Fig. 9 demonstrate that RIVAL fails to align the feature\nspace without attention injection, generating images that do not correspond to the exemplar.\nLatent alignment. To investigate the impact of step-wise latent distribution alignment, we remove\nlatent alignments and sampling XT\nG \u223c N(0, I). An example is shown in Fig. 4 and Fig. 9. Without\nlatent alignment, RIVAL produces inconsistent variations of images, exhibiting biased tone and\nsemantics compared to the source image. Additionally, we perform ablations on attention fusion\nin Eq. (3) and noise alignment in Eq. (8) during the early steps. Clear color and semantic bias emerge\nwhen the early alignment is absent, as shown in Fig. 10. Furthermore, if the values of tearly or talign\nare excessively small, the latent alignment stage becomes protracted, resulting in the problem of\nover-alignment. Over-alignment gives rise to unwanted artifacts and generates blurry outputs.\nInterpretability. We add experiments for the interpretability of RIVAL in the following two aspects.\n(a) Latent Similarity. To assess RIVAL components\u2019 efficacy regarding the distribution gap, we\nillustrate the KL divergence of noisy latent between chains A and B, Xt\nA and Xt\nB in the generation\nprocess, as depicted in Fig. 12 left part. Interactions between different distributions (green) widen the\ngap, while two generation chains with the same distribution (orange) can get a better alignment by\nattention interactions. With aligned latent chain alignment and interaction, RIVAL (purple) effectively\ngenerates real-world image variations.\n(b) Reference Feature Contribution. Attention can be viewed as sampling value features from the\nkey-query attention matrix. RIVAL converts self-attention to image-wise cross-attention. When\n9\n\u201cA {} of <sks> {}.\u201d\n\u201cphoto + toy\u201d\nsource exemplar\n\u201cphoto + bag\u201d\n\u201cphoto + bag\u201d\nVanilla Inference\nRIVAL Inference\n\u201cpainting + toy girl, in\nthe style of Vermeer\u201d\n\u201cpainting + toy girl\u201d\nFigure 13: RIVAL enables seamless customization of optimized novel concepts through text prompt\ncontrol (<sks>). With various text prompt inputs, we can still generate images preserving the tones,\nstyle, and contents of the provided source exemplar (depicted on the left).\nlatents are sourcing from the same distribution (XT\nG, XT\nR \u223c N(0, I)) images retain consistent\nstyle and content attributes (as in Fig. 12 left orange).\nThis result is beneficial since we do\nnot require complex text conditions c to constrain generated images to get similar content and\nstyle. For a more direct explanation, we visualize the bottleneck feature contributions of attention\nscore presented in Fig. 12 right part. Reference contribution of the softmax score is denoted as\nscoreR = P\nvi\u2208vR(W QvG \u00b7 (W K(vi))\u22a4)/P\nvj\u2208vG\u2295vR(W QvG \u00b7 (W K(vj))\u22a4). As RIVAL adopts\nearly fusion in the early steps, we use 50% as the score in the early steps. Latent initialization (orange)\nand with early alignments (green) play critical roles in ensuring a substantial contribution of the\nsource feature in self-attention. A higher attention contribution helps in resolving the attention\nattenuation problem (purple) in the generation process.\n4.5\nDiscussions\nIntegration with concept customization. In addition to its ability to generate image variations from\na single source image using a text prompt input for semantic alignment, RIVAL can be effectively\ncombined with optimization-based concept customization techniques, such as DreamBooth [12], to\nenable novel concept customization. As illustrated in Fig. 13, an optimized concept can efficiently\nexplore the space of potential image variations by leveraging RIVAL\u2019s proficiency in real-world\nimage inversion alignment.\nLimitations and Future Directions. Despite introducing an innovative technique for crafting high-\nquality inconsistent image variations, our method is contingent upon a text prompt input, potentially\ninfusing semantic biases affecting image quality. As evidenced in Fig. 14, a prompt like \"Pokemon\"\nmay lean towards popular choices like \"Pikachu\" due to training set biases, resulting in a Pikachu-\ndominated generation. Besides, the base model struggles to generate complex scenes and complicated\nconcepts, e.g., \"illustration of a little boy standing in front of a list of doors with butterflies around\nthem in Fig. 14 (c)\". This complexity can degrade the inversion chain and widen the domain gap,\nleading to less accurate results. Future studies might concentrate on refining diffusion models and\nexploring novel input avenues besides text prompts to mitigate these constraints.\n(b) Complex scene\n(c) Hard concept\n(a) Semantic bias\n\u201cPok\u00e9mon\u201d\nFigure 14: Fail cases of RIVAL. The exemplar image is on the left for each case.\n5\nConclusion\nThis paper presents a novel pipeline for generating diverse and high-quality variations of real-world\nimages while maintaining their semantic content and style. Our proposed approach addresses previous\nlimitations by modifying the diffusion model\u2019s denoising inference chain to align with the real-image\ninversion chain. We introduce a cross-image self-attention injection and a step-wise latent alignment\ntechnique to facilitate alignment between two chains. Our method exhibits significant improvements\nin the quality of image variation generation compared to state-of-the-art methods, as demonstrated\nthrough qualitative and quantitative evaluations. Moreover, with the novel paradigm of hybrid\ntext-image conditions, our approach can be easily extended to multiple text-to-image tasks.\n10\nAcknowledgements. This work is partially supported by the Research Grants Council under the\nAreas of Excellence scheme grant AoE/E-601/22-R, Hong Kong General Research Fund (14208023),\nHong Kong AoE/P-404/18, and Centre for Perceptual and Interactive Intelligence (CPII) Limited\nunder the Innovation and Technology Fund. We are grateful to Jingtao Zhou for the meaningful\ndiscussions.\nReferences\n[1] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot,\nCe Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, volume 33, pages 18661\u201318673,\n2020. 1\n[2] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, pages 9729\u20139738, 2020. 1\n[3] Alexei A Efros and Thomas K Leung. Texture synthesis by non-parametric sampling. In ICCV, volume 2,\npages 1033\u20131038. IEEE, 1999. 2\n[4] Leon Gatys, Alexander S Ecker, and Matthias Bethge. Texture synthesis using convolutional neural\nnetworks. In NeurIPS, volume 28, 2015. 2\n[5] Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural\nnetworks. In CVPR, pages 2414\u20132423, 2016. 2, 3\n[6] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013\n144, 2020. 2, 3\n[7] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional\nadversarial networks. In CVPR, pages 1125\u20131134, 2017. 2, 3\n[8] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with pixelcnn decoders. In NeurIPS, volume 29, 2016. 2\n[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS,\nvolume 33, pages 6840\u20136851, 2020. 2\n[10] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-\nimage diffusion models with deep language understanding. In NeurIPS, volume 35, pages 36479\u201336494,\n2022. 2\n[11] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022. 2, 3, 5, 6, 8, 21\n[12] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, 2023. 2, 3, 5, 8,\n10, 21\n[13] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel\nCohen-or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In\nICLR, 2023. 2, 3\n[14] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept\ncustomization of text-to-image diffusion. In CVPR, 2023. 2, 3\n[15] Ziyi Dong, Pengxu Wei, and Liang Lin. Dreamartist: Towards controllable one-shot text-to-image\ngeneration via contrastive prompt-tuning. arXiv preprint arXiv:2211.11337, 2022. 2, 3\n[16] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional\nimage generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. 2, 3, 5, 8, 9, 14, 15, 16, 21\n[17] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 2,\n3, 4, 5, 14\n[18] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing\nreal images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. 2, 3, 14, 20\n11\n[19] Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations.\narXiv preprint arXiv:2211.12446, 2022. 2, 3\n[20] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite: Encod-\ning visual concepts into textual embeddings for customized text-to-image generation. arXiv preprint\narXiv:2302.13848, 2023. 2, 3, 5, 6, 8, 9, 14, 21\n[21] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow image editing\ninstructions. In CVPR, 2023. 2\n[22] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-\nto-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 3,\n4\n[23] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl:\nTuning-free mutual self-attention control for consistent image synthesis and editing. arXiv preprint\narXiv:2304.08465, 2023. 2, 3, 4, 6, 7, 8, 14, 15\n[24] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool.\nRepaint: Inpainting using denoising diffusion probabilistic models. In CVPR, pages 11461\u201311471, June\n2022. 2, 14\n[25] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen.\nPaint by example: Exemplar-based image editing with diffusion models. arXiv preprint arXiv:2211.13227,\n2022. 2, 7\n[26] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML. PMLR, 2021. 2\n[27] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna,\nand Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv\npreprint arXiv:2307.01952, 2023. 3\n[28] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al.\nLora: Low-rank adaptation of large language models. In ICLR, 2021. 3\n[29] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion:\nA survey. PAMI, 45(3):3121\u20133138, 2023. 3\n[30] Siyu Xing, Chen Gong, Hewei Guo, Xiao-Yu Zhang, Xinwen Hou, and Yu Liu. Unsupervised domain\nadaptation gan inversion for image editing. arXiv preprint arXiv:2211.12123, 2022. 3\n[31] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain gan inversion for real image editing. In\nECCV, pages 592\u2013608. Springer, 2020. 3\n[32] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In CVPR, pages 4401\u20134410, 2019. 3\n[33] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-\nattention control. arXiv preprint arXiv:2303.04761, 2023. 3, 4\n[34] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for\ntext-driven image-to-image translation. arXiv preprint arXiv:2211.12572, 2022. 3, 6, 7, 8\n[35] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob\nMcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. In ICML, pages 16784\u201316804. PMLR, 2022. 3\n[36] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSDEdit: Guided image synthesis and editing with stochastic differential equations. In ICLR, 2022. 3\n[37] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen.\nFatezero: Fusing attentions for zero-shot text-based video editing. arXiv preprint arXiv:2303.09535, 2023.\n3, 4\n[38] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization.\nIn ICCV, pages 1501\u20131510, 2017. 3, 5\n[39] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution. In ECCV, pages 694\u2013711. Springer, 2016. 3\n12\n[40] Nicholas Kolkin, Michal Kucera, Sylvain Paris, Daniel Sykora, Eli Shechtman, and Greg Shakhnarovich.\nNeural neighbor style transfer. arXiv preprint arXiv:2203.13215, 2022. 3\n[41] Kai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. Arf: Artistic\nradiance fields. In ECCV, pages 717\u2013733. Springer, 2022. 3\n[42] Yuechen Zhang, Zexin He, Jinbo Xing, Xufeng Yao, and Jiaya Jia. Ref-NPR: Reference-based non-\nphotorealistic radiance fields for controllable scene stylization. In CVPR, 2023. 3\n[43] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: stylized neural implicit representations for 3d scenes.\nACM Transactions on Graphics (TOG), 41(4):1\u201311, 2022. 3\n[44] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using\ncycle-consistent adversarial networks. In ICCV, pages 2223\u20132232, 2017. 3\n[45] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber,\nLu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint\narXiv:2306.00983, 2023. 3\n[46] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng\nXu. Inversion-based style transfer with diffusion models. In CVPR, pages 10146\u201310156, 2023. 3\n[47] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation for control-\nlable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023. 3\n[48] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter\nfor text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3\n[49] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video\ngeneration. arXiv preprint arXiv:2212.11565, 2022. 4\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, volume 30, 2017. 4\n[51] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In\nCVPR, pages 782\u2013791, 2021. 5\n[52] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n5\n[53] Justin Pinkney. Experiments with stable diffusion. https://github.com/justinpinkney/\nstable-diffusion, 2023. 5, 6, 8, 9, 14, 15, 21\n[54] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In ICML, 2023. 5\n[55] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 7, 14\n[56] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021. 8, 20\n[57] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR, 2018. 9\n[58] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:6840\u2013\n6851, 2020. 14\n[59] Ivar Stangeby. A python library for extracting color palettes from supplied images. https://github.\ncom/qTipTip/Pylette, 2022. 20\n[60] Miko\u0142aj Bi\u00b4nkowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans.\narXiv preprint arXiv:1801.01401, 2018. 21\n[61] Ond\u02c7rej Texler, David Futschik, Michal Ku\u02c7cera, Ond\u02c7rej Jamri\u0161ka, \u0160\u00e1rka Sochorov\u00e1, Menclei Chai, Sergey\nTulyakov, and Daniel S`ykora. Interactive video stylization using few-shot patch-based training. ACM\nTransactions on Graphics (TOG), 39(4):73\u20131, 2020. 21\n13\nAppendix\nA\nBasic Background of Diffusion Models\nThis section uses a modified background description provided in [18]. We only consider the condition-\nfree case for the diffusion model here. Diffusion Denoising Probabilistic Models (DDPMs) [58] are\ngenerative latent variable models designed to approximate the data distribution q(x0). The diffusion\noperation starts from the latent x0, adding step-wise noise to diffuse data into pure noise xT . It\u2019s\nimportant to note that this process can be viewed as a Markov chain starting from x0, where noise is\ngradually added to the data to generate the latent variables x1, . . . , xT \u2208 X. The sequence of latent\nvariables follows the conditional distribution q(x1, . . . , xt | x0) = Qt\ni=1 q(xt | xt\u22121). Each step in\nthe forward process is defined by a Gaussian transition q(xt | xt\u22121) := N(xt; \u221a1 \u2212 ktxt\u22121, ktI),\nwhich is parameterized by a schedule k0, . . . , kT \u2208 (0, 1). As T becomes sufficiently large, the final\nnoise vector xT approximates an isotropic Gaussian distribution.\nThe forward process allows us to express the latent variable xt directly as a linear combination of\nnoise and x0, without the need to sample intermediate latent vectors.\nxt = \u221a\u03b1tx0 +\n\u221a\n1 \u2212 \u03b1tw, w \u223c N(0, I),\n(9)\nwhere \u03b1t := Qt\ni=1(1 \u2212 ki). To sample from the distribution q(x0), a reversed denoising process is\ndefined by sampling the posteriors q(xt\u22121 | xt), which connects isotropic Gaussian noise xT to the\nactual data. However, the reverse process is computationally challenging due to its dependence on the\nunknown data distribution q(x0). To overcome this obstacle, an approximation of the reverse process\nwith a parameterized Gaussian transition network denoted as p\u03b8(xt\u22121 | xt), where p\u03b8(xt\u22121 | xt)\nfollows a normal distribution with mean \u00b5\u03b8(xt, t) and covariance \u03a3\u03b8(xt, t). As an alternative\napproach, the prediction of the noise \u03f5\u03b8(xt, t) added to x0, which is obtained using equation 9, can\nreplace the use of \u00b5\u03b8(xt, t) as suggested in [58]. Bayes\u2019 theorem could be applied to approximate\n\u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\nkt\n\u221a1 \u2212 \u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n.\n(10)\nOnce we have a trained \u03f5\u03b8(xt, t), we can using the following sample method\nxt\u22121 = \u00b5\u03b8(xt, t) + \u03c3tz, z \u223c N(0, I).\n(11)\nIn DDIM sampling [17], a denoising process could become deterministic when set \u03c3t = 0.\nB\nDetails of the Attention Pipeline\nWe present a comparative analysis of attention injection methods. As depicted in Fig. 15, MasaC-\ntrl [23], while also adopting a self-attention injection approach, employs a more complex control\nmechanism in its second stage. In the first stage of MasaCtrl, the inverted latent representation XR\nT\nis directly utilized by applying a modified prompt. In the second stage, a cross-attention mask is\nintroduced to control specific word concepts modified in the prompt, which requires an additional\nforward pass. In contrast, our proposed method, RIVAL, primarily focuses on generating inconsistent\nvariations. Consequently, we aim to guide feature interaction by replacing KV features with an\naligned latent distribution. Unlike MasaCtrl, our approach does not limit content transfer through\nediting prompts with only a few words. Hence, in the second stage, we employ a single forward\npass without calculating an additional cross-attention mask, allowing fast and flexible text-to-image\ngeneration with diverse text prompts.\nIn recent updates, ControlNet [55] has incorporated an attention mechanism resembling the second\nstage of RIVAL to address image variation. However, a notable distinction lies in using vanilla noised\nlatents as guidance, leading to a process akin to the attention-only approach employed in RePaint [24]\nwith the Stable Diffusion model. Consequently, this methodology is limited to generating images\nwithin the fine-tuned training data domain.\nC\nMore About Comparisons\nImplementation Details. We compare our work with ELITE [20], Stable Diffusion image varia-\ntion [53], and DALL\u00b7E 2 [16]. We utilize the official demo of ELITE to obtain results. To extract\n14\nT2I model\nQ\n!!\n\"\n!#\n\"\n!#\n\"$%\nT2I model\nV\nQ\n!#\n\"\n!#\n\"$%\nK\nV\nK\nT2I model\nQ\n!!\n\"\n!#\n\"\n!#\n\"$%\nReplacement\nT2I model\nV\nQ\n!!\n\"\n!#\n\"\nV\n!#\n\"$%\ninjection\nK\nK\nV\nK\n!#\n& = !!\n&\nV\nK\nenc\ndec\n!#\n&\n\u2026\n\u2026\n# \u2265 #'()*+\n# < #'()*+\n# \u2265 #'()*+\n# < #'()*+\nMasaCtrl\nRIVAL\nSpecific mask\ncontrol+ replacement\n(Additional forward to get mask)\n#'()*+ = 45\n#'()*+ = 30\n(Single forward)\nFigure 15: Self-attention control compare with MasaCtrl [23]. The default split of two stages is\nshown as a bar for each method.\nExperiments\nAI\nAF\nLI\nNA\nPalette Distance\nText Alignment\nImage Alignment\nBaseline\n3.917\n0.266\n0.751\nAI\n\u2713\n3.564\n0.277\n0.804\nAF\n\u2713\n\u2713\n3.518\n0.274\n0.820\nLI.\n\u2713\n3.102\n0.268\n0.764\nNA\n\u2713\n3.661\n0.251\n0.647\nw/o AI&AF\n\u2713\n\u2713\n2.576\n0.276\n0.760\nw/o AF&NA\n\u2713\n\u2713\n2.419\n0.279\n0.817\nw/o AF\n\u2713\n\u2713\n\u2713\n1.902\n0.274\n0.818\nw/o LI\n\u2713\n\u2713\n\u2713\n3.741\n0.242\n0.653\nw/o NA\n\u2713\n\u2713\n\u2713\n2.335\n0.267\n0.839\nFull Model\n\u2713\n\u2713\n\u2713\n\u2713\n1.810\n0.268\n0.846\nTable 3: Module-wise ablation (AI: Attention Injection, AF: Attention Fusion, LI: Latent Initialization,\nNA: Noise Alignment) with quantitative results.\ncontext tokens, we mask the entire image and employ the phrase \"A photo/painting of <S>.\" based\non the production method of each test image. Inference for ELITE employs the default setting with\ndenoising steps set to T = 300. For Stable Diffusion\u2019s image variation version, we utilize the default\nconfiguration, CFG guidance m = 3, and denoising steps T = 50. In the case of DALL\u00b7E 2, we\nutilize the official image variation API, specifically requesting using the most advanced API available\nto generate images of size 1024 \u00d7 1024.\nComparison with UnCLIP. UnCLIP [16], also known as DALL\u00b7E 2, is an image generation\nframework trained using image CLIP features as direct input. Thanks to its large-scale training and\nimage-direct conditioning design, it generates variations solely based on image conditions when\nadapted to image variation. However, when faced with hybrid image-text conditions, image-only\nUnCLIP struggles to produce satisfactory results, particularly when CLIP does not recognize the\nimage content correctly. We provide comparative analysis in Fig. 16. Additionally, we demonstrate\nin the last two columns of Figure 16 that our approach can enhance the accuracy of low-level details\nin open-source image variation methods such as SD image variation [53].\nAdditional Visual Results. We showcase additional results of our techniques in variation generation,\nas illustrated in Fig. 17, and text-driven image generation with image condition, as shown in Fig. 18.\nThe results unequivocally demonstrate the efficacy of our approach in generating a wide range of\nimage variations that accurately adhere to textual and visual guidance.\n15\n\u201cA girl in a sailor uniform posing for a photo.\u201d\n\u201cSilhouette of a woman looking through a glass dome filled with fish.\u201d\n.\u201d\n\u201cA building with a blue and white striped awning.\u201d\n\u201cA group of people standing around a statue.\u201d\n\u201cPok\u00e9mon\u201d\nRIVAL SDv1\nImgVar SDv1 + RIVAL\nSource exemplar\nDALL\u00b7E 2\nUnCLIP SDv2\nfail case\nFigure 16: Comparision and adaptation with UnCLIP [16]. We highlight texts that enhance the image\nunderstanding for each case. Our inference pipeline is adapted to the image variation model depicted\nin the fourth column, in contrast to the variation achieved through vanilla inference in the bottom left\ncorner of each image.\n(talign, tearly)\n(30, 30)\n(0, 30)\n(30, 0)\n(0, 0)\n(30, 50)\n(50, 30)\n(50, 50)\nText Alignment\n0.268\n0.261\n0.269\n0.259\n0.267\n0.274\n0.279\nImage Alignment\n0.846\n0.873\n0.838\n0.865\n0.839\n0.813\n0.817\nPalette Distance\n1.810\n1.421\n1.806\n1.483\n2.359\n2.061\n2.419\nTable 4: Quantitative ablation for different alignment timesteps.\n16\nExemplars\nVariations\nFigure 17: Text-driven free-form image generation results. The image reference is in the left column.\nIn the last row, we also present variations for one customized concept <sks> bag.\n17\nFigure 18: Text-driven free-form image generation results, with the image reference placed in the top\nleft corner. The text prompts used are identical to those presented in Fig. 6 of the main paper. Every\ntwo rows correspond to a shared text prompt.\nCFG scale\n3\n5\n7\n9\n11\nText Alignment\n0.260\n0.272\n0.273\n0.273\n0.271\nImage Alignment\n0.859\n0.863\n0.845\n0.845\n0.838\nPalette Distance\n1.749\n1.685\n1.737\n1.829\n1.902\nTable 5: Quantitative ablation for different Classifier-Free Guidance Scale.\nD\nAdditional Ablation Results\nModule-wise ablation. We augment our visual ablations in Fig. 9 with a comprehensive module-wise\nexperiment. The results in Tab. 3 underscore the role of each component and their combinations.\nAttention injection facilitates high-level feature interactions for better condition alignment (both text\nand image). Early fusion, built upon attention injection, aids in early step chain alignment, signifi-\ncantly enhancing image alignment. Meanwhile, latent noise alignment guarantees the preservation of\ncolor. Latent initialization has a pronounced impact, notably enhancing the color palette metric, an\neffect intensified by noise alignment.\nAblation on early fusion step. In addition to Fig. 10 of the main paper, we present comprehensive\nearly-step evaluation results based on a grid search analysis in Fig. 19. By decreasing the duration\nof the feature replacement stage (larger talign), we observe an increase in the similarity of textures\nand contents in the generated images. However, excessively long or short early latent alignment\ndurations (tearly) can lead to color misalignment. Users can adjust the size of the early fusion steps\nas hyperparameters to achieve the desired outcomes.\nWe also conduct a quantitative evaluation in Tab. 4. It shows that there is a balance between two\nconditions (text-alignment and image-alignment). Besides, low-level texture-pasting artifacts will\n18\n0\n10\n20\n30\n40\n50\n0\n10\n20\n30\n40\n50\n!!\"#$%: color bias control\n!&'()*: image similarity control\n\u201cA photo of a red vase.\u201d\nFigure 19: Ablation results for alignment steps, with the reference exemplar at the bottom right. We\nfix each generation\u2019s initial latent XT\nG.\npresent when talign is small. The choice of tearly primarily influences the generated images\u2019 style\n(color) bias.\nw/o initialize !!\n\"\nAdaIN !!\n\"\nearly noise\nearly latent\nhidden state before self-attn\n!!\n\"~#\u210e%&&'((!#\n\")\n!!\n\"~+(,,.)\nhidden state each block\nFigure 20: Ablation studies for different feature alignment strategies.\nAblation on different alignment designs.\nFig. 20 illustrates ablations conducted on various\nalignment designs. Two latent initialization methods, as formulated in Eq. (6), exhibit comparable\nperformance. Nevertheless, incorporating alignments in additional areas, such as hidden states within\neach transformer block, may harm performance. Hence, we opt for our RIVAL pipeline\u2019s simplest\nnoise alignment strategy.\n19\n\u201cA PINK building\u201d\nm=1 (failed)\nm=3\nm=5\nm=7\nm=9\nw/o opt. null\nw/ opt. null\nw/o opt. null\nw/ opt. null\nempty prompt\nempty prompt\nempty prompt\noriginal prompt\noriginal prompt\noriginal prompt\n{adj.}: \u201cin a sailor uniform\u201d \n{n.}: \u201cgirl\u201d\n{n.}: \u201cElon Musk\u201d\n{n.}: \u201crobot\u201d\n(a) Ablation on different CFG scales\n(b) Ablation on null-text inversion\n(c) Ablation on different source prompts\nFigure 21: Ablation studies on different text conditions and guidance scales. Reference exemplars\nare highlighted with a golden border.\nAblation on different text conditions. We conduct ablations on text conditions in three aspects.\nFirst, we evaluate the impact of different CFG scales m for text prompt guidance, with quantitative\nresults in Tab. 5. As shown in Fig. 21 (a), our latent rescaling technique enables control over the text\nguidance level while preserving the reference exemplar\u2019s low-level features. Second, we employ an\noptimization-based null-text inversion method [18] to obtain an inversion chain with improved recon-\nstruction quality. However, this method is computationally intensive, and the optimized embeddings\nare sensitive to the guidance scale. Furthermore, when incorporating this optimized embedding into\nthe unconditional inference branch, there is a variation in generation quality, as depicted in Fig. 21 (b).\nThird, we utilize empty text as the source prompt to obtain the latents in the inversion chain while\nkeeping the target prompt unchanged. As depicted in Fig. 21 (c), the empty text leads to weak seman-\ntic content correspondence between the two chains but sometimes benefits text-driven generation.\nFor example, if users do not want to transfer the \"gender\" concept to the generated robot.\nE\nQuantitative Evaluations\nThis section comprehensively evaluates our proposed method with various carefully designed metrics,\nincluding CLIP Score, color palette matching, user study, and KID.\nCLIP Score.\nFor evaluating the CLIP Score, we employ the official ViT-Large-Patch14 CLIP\nmodel [56] and compute the cosine similarity between the projected features, yielding the output.\nColor palette matching. To perform low-level matching, we utilize the Pylette tool [59] to extract a\nset of 10 palette colors. Subsequently, we conduct a bipartite matching between the color palette of\neach generated image and the reference palette colors in the RGB color space. Before matching, each\ncolor is scaled to [0, 1]. The matching result is obtained by calculating the sum of L1 distances.\nUser study. To evaluate the effectiveness of our approach against other methods, we conducted a\nuser study using an online form. The user study interface, depicted in Figure 22, was designed to elicit\n20\n(a) Specification\n(b) Shuffled results\n(c) Ranking questions\nFigure 22: User study user interface. In this case, four methods are: (A). SD ImageVar [53], (B).\nELITE [20], (C). DALL\u00b7E 2[16], (D). RIVAL (ours).\nmethod\nSD [11]\nImgVar [53]\nELITE [20]\nUnClIP [16]\nRIVAL\nbase model\nV1-5\nV1-3\nV1-4\nV2-1\nV1-5\nKID \u2193\n17.1\n18.5\n25.7\n13.5\n13.2\nTable 6: Quantitative comparisons for KID (\u00d7103). All methods are Stable Diffusion based.\nuser rankings of image variation results. We collected 41 questionnaire responses, encompassing 16\ncases of ranking comparisons.\nKID evaluation. To provide a comprehensive assessment of the quality, we utilize Kernel Inception\nDistance (KID)[60] to evaluate the perceptual generation quality of our test set. As depicted in Table6,\nwith Stable Diffusion V1-5, our method achieves the best KID score, which is slightly superior to the\nUnCLIP [16], employing the advanced Stable Diffusion V2-1.\nF\nAdditional Considerations\nData acquisition. To comprehensively evaluate our method, we collected diverse source exemplars\nfrom multiple public datasets, such as DreamBooth [12] and Interactive Video Stylization [61]. Some\nexemplars were obtained from Google and Behance solely for research purposes. We will not release\nour self-collected example data due to license restrictions.\nSocietal impacts.\nThis paper introduces a novel framework for image generation that leverages\na hybrid image-text condition, facilitating the generation of diverse image variations. Although\nthis application has the potential to be misused by malicious actors for disinformation purposes,\nsignificant advancements have been achieved in detecting malicious generation. Consequently, we\nanticipate that our work will contribute to this domain. In forthcoming iterations of our method, we\nintend to introduce the NSFW (Not Safe for Work) test for detecting possible malicious generations.\nThrough rigorous experimentation and analysis, our objective is to enhance comprehension of image\ngeneration techniques and alleviate their potential misuse.\n21\n"
  },
  {
    "title": "Geometric Algebra Transformers",
    "link": "https://arxiv.org/pdf/2305.18415.pdf",
    "upvote": "2",
    "text": "Geometric Algebra Transformer\nJohann Brehmer\u2217\nPim de Haan\u2217\nS\u00f6nke Behrends\nTaco Cohen\nQualcomm AI Research\u2020\n{jbrehmer, pim, sbehrend, tacos}@qti.qualcomm.com\nAbstract\nProblems involving geometric data arise in physics, chemistry, robotics, computer\nvision, and many other fields. Such data can take numerous forms, for instance\npoints, direction vectors, translations, or rotations, but to date there is no single\narchitecture that can be applied to such a wide variety of geometric types while\nrespecting their symmetries. In this paper we introduce the Geometric Algebra\nTransformer (GATr), a general-purpose architecture for geometric data. GATr\nrepresents inputs, outputs, and hidden states in the projective geometric (or Clifford)\nalgebra, which offers an efficient 16-dimensional vector-space representation of\ncommon geometric objects as well as operators acting on them. GATr is equivariant\nwith respect to E(3), the symmetry group of 3D Euclidean space. As a Transformer,\nGATr is versatile, efficient, and scalable. We demonstrate GATr in problems\nfrom n-body modeling to wall-shear-stress estimation on large arterial meshes to\nrobotic motion planning. GATr consistently outperforms both non-geometric and\nequivariant baselines in terms of error, data efficiency, and scalability.\n1\nIntroduction\nFrom molecular dynamics to astrophysics, from material design to robotics, fields across science\nand engineering deal with geometric data such as positions, shapes, orientations, or directions. The\ngeometric nature of data provides a rich structure: a notion of common operations between geometric\ntypes (computing distances between points, applying rotations to orientations, etc.), a well-defined\nbehaviour of data under transformations of a system, and the independence of certain properties of\ncoordinate system choices. When learning from geometric data, incorporating this rich structure into\nthe architecture has the potential to improve the performance.\nWith this goal, we introduce the Geometric Algebra Transformer (GATr), a general-purpose network\narchitecture for geometric data. GATr brings together three key ideas.\nGeometric algebra: To naturally describe both geometric objects as well as their transformations\nin three-dimensional space, GATr represents data as multivectors of the projective geometric\nalgebra G3,0,1. Geometric (or Clifford) algebra is an principled yet practical mathematical\nframework for geometrical computations. The particular algebra G3,0,1 extends the vector\nspace R3 to 16-dimensional multivectors, which can natively represent various geometric\ntypes and E(3) poses. Unlike the O(3) representations popular in geometric deep learning,\nthis algebra can represent data that is not invariant to translations, such as absolute positions.\nEquivariance: GATr is equivariant with respect to E(3), the symmetry group of three-dimensional\nspace. To this end, we develop several new E(3)-equivariant primitives mapping between\nmultivectors, including equivariant linear maps, an attention mechanism, nonlinearities, and\nnormalization layers.\n\u2217Equal contribution.\n\u2020Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18415v3  [cs.LG]  20 Nov 2023\nTransformer: Due to its favorable scaling properties, expressiveness, trainability, and versatility, the\nTransformer architecture [50] has become the de-facto standard for a wide range of problems.\nGATr is based on the Transformer architecture, in particular on dot-product attention, and\ninherits these benefits.\nGATr thus combines two lines of research: the representation of geometric objects with geometric\nalgebra [17, 18, 38], popular in computer graphics and physics and recently gaining traction in deep\nlearning [5, 41, 46], and the encoding of symmetries through equivariant deep learning [12]. The\nresult\u2014to the best of our knowledge the first E(3)-equivariant architecture with internal geometric\nalgebra representations3\u2014is a versatile network for problems involving geometric data.\nWe demonstrate GATr in three problems from entirely different fields. In an n-body modelling task,\nwe compare GATr to various baselines. We turn towards the task of predicting the wall shear stress in\nhuman arteries, demonstrating that GATr scales to realistic problems with meshes of thousands of\nnodes. Finally, experiment with robotic motion planning, using GATr as the backbone of an E(3)-\ninvariant diffusion model. In all cases, GATr substantially outperforms both non-geometric and\nequivariant baselines.\nOur implementation of GATr is available at https://github.com/qualcomm-ai-research/\ngeometric-algebra-transformer.\n2\nBackground\nGeometric algebra\nWe begin with a brief overview of geometric algebra (GA); for more detail, see\ne. g. Refs. [17, 18, 38, 41]. Whereas a plain vector space like R3 allows us to take linear combinations\nof elements x and y (vectors), a geometric algebra additionally has a bilinear associative operation: the\ngeometric product, denoted simply by xy. By multiplying vectors, one obtains so-called multivectors,\nwhich can represent both geometrical objects and operators. Like vectors, multivectors have a notion\nof direction as well as magnitude and orientation (sign), and can be linearly combined.\nMultivectors can be expanded on a multivector basis, consisting of products of basis vectors. For\nexample, in a 3D GA with orthogonal basis e1, e2, e3, a general multivector takes the form\nx = xs + x1e1 + x2e2 + x3e3 + x12e1e2 + x13e1e3 + x23e2e3 + x123e1e2e3,\n(1)\nwith real coefficients (xs, x1, . . . , x123) \u2208 R8. Thus, similar to how a complex number a + bi is a\nsum of a real scalar and an imaginary number,4 a general multivector is a sum of different kinds of\nelements. These are characterized by their dimensionality (grade), such as scalars (grade 0), vectors\nei (grade 1), bivectors eiej (grade 2), all the way up to the pseudoscalar e1 \u00b7 \u00b7 \u00b7 ed (grade d).\nThe geometric product is characterized by the fundamental equation vv = \u27e8v, v\u27e9, where \u27e8\u00b7, \u00b7\u27e9 is an\ninner product. In other words, we require that the square of a vector is its squared norm. In an\northogonal basis, where \u27e8ei, ej\u27e9 \u221d \u03b4ij, one can deduce that the geometric product of two different\nbasis vectors is antisymmetric: eiej = \u2212ejei5. Since reordering only produces a sign flip, we only\nget one basis multivector per unordered subset of basis vectors, and so the total dimensionality of a\nGA is Pd\ni=0\n\u0000d\nk\n\u0001\n= 2d. Moreover, using bilinearity and the fundamental equation one can compute\nthe geometric product of arbitrary multivectors.\nThe symmetric and antisymmetric parts of the geometric product are called the interior and exterior\n(wedge) product. For vectors x and y, these are defined as \u27e8x, y\u27e9 = (xy + yx)/2 and x \u2227 y \u2261\n(xy \u2212yx)/2. The former is indeed equal to the inner product used to define the GA, whereas the latter\nis new notation. Whereas the inner product computes the similarity, the exterior product constructs a\nmultivector (called a blade) representing the weighted and oriented subspace spanned by the vectors.\nBoth operations can be extended to general multivectors [18].\nThe final primitive of the GA that we will require is the dualization operator x 7\u2192 x\u2217. It acts on basis\nelements by swapping \u201cempty\u201d and \u201cfull\u201d dimensions, e. g. sending e1 7\u2192 e23.\n3Concurrently to our work, a similar network architecture was studied by Ruhe et al. [40]; we comment on\nsimilarities and differences in Sec. 5.\n4Indeed the imaginary unit i can be thought of as the bivector e1e2 in a 2D GA.\n5The antisymmetry can be derived by using v2 = \u27e8v, v\u27e9 to show that eiej +ejei = (ei +ej)2 \u2212e2\ni \u2212e2\nj = 0.\n2\nObject / operator\nScalar\nVector\nBivector\nTrivector\nPS\n1\ne0 ei\ne0i\neij\ne0ij e123\ne0123\nScalar \u03bb \u2208 R\n\u03bb\n0\n0\n0\n0\n0\n0\n0\nPlane w/ normal n \u2208 R3, origin shift d \u2208 R\n0\nd\nn\n0\n0\n0\n0\n0\nLine w/ direction n \u2208 R3, orthogonal shift s \u2208 R3\n0\n0\n0\ns\nn\n0\n0\n0\nPoint p \u2208 R3\n0\n0\n0\n0\n0\np\n1\n0\nPseudoscalar \u00b5 \u2208 R\n0\n0\n0\n0\n0\n0\n0\n\u00b5\nReflection through plane w/ normal n \u2208 R3, origin shift d \u2208 R\n0\nd\nn\n0\n0\n0\n0\n0\nTranslation t \u2208 R3\n1\n0\n0\n1\n2t\n0\n0\n0\n0\nRotation expressed as quaternion q \u2208 R4\nq0\n0\n0\n0\nqi\n0\n0\n0\nPoint reflection through p \u2208 R3\n0\n0\n0\n0\n0\np\n1\n0\nTable 1: Embeddings of common geometric objects and transformations into the projective geometric algebra\nG3,0,1. The columns show different components of the multivectors with the corresponding basis elements,\nwith i, j \u2208 {1, 2, 3}, j \u0338= i, i.e. ij \u2208 {12, 13, 23}. For simplicity, we fix gauge ambiguities (the weight of the\nmultivectors) and leave out signs (which depend on the ordering of indices in the basis elements).\nProjective geometric algebra\nIn order to represent three-dimensional objects as well as arbitrary\nrotations and translations acting on them, the 3D GA is not enough: as it turns out, its multivectors\ncan only represent linear subspaces passing through the origin as well as rotations around it. A\ncommon trick to expand the range of objects and operators is to embed the space of interest (e.g. R3)\ninto a higher dimensional space whose multivectors represent more general objects and operators in\nthe original space.\nIn this paper we work with the projective geometric algebra G3,0,1 [17, 38, 41]. Here one adds a\nfourth homogeneous coordinate x0e0 to the vector space, yielding a 24 = 16-dimensional geometric\nalgebra. The metric of G3,0,1 is such that e2\n0 = 0 and e2\ni = 1 for i = 1, 2, 3. As we will explain in the\nfollowing, in this setup the 16-dimensional multivectors can represent 3D points, lines, and planes,\nwhich need not pass through the origin, and arbitrary rotations, reflections, and translations in R3.\nRepresenting transformations\nIn geometric algebra, a vector u can act as an operator, reflecting\nother elements in the hyperplane orthogonal to u. Since any orthogonal transformation is equal to a\nsequence of reflections, this allows us to express any such transformation as a geometric product of\n(unit) vectors, called a (unit) versor u = u1 \u00b7 \u00b7 \u00b7 uk. Furthermore, since the product of unit versors\nis a unit versor, and unit vectors are their own inverse (u2 = 1), these form a group called the Pin\ngroup associated with the metric. Similarly, products of an even number of reflections form the Spin\ngroup. In the projective geometric algebra G3,0,1, these are the double cover6 of E(3) and SE(3),\nrespectively. We can thus represent any rotation, translation, and mirroring\u2014the symmetries of three-\ndimensional space\u2014as G3,0,1 multivectors.\nIn order to apply a versor u to an arbitrary element x, one uses the sandwich product:\n\u03c1u(x) =\n\u001auxu\u22121\nif u is even\nu\u02c6xu\u22121\nif u is odd\n(2)\nHere \u02c6x is the grade involution, which flips the sign of odd-grade elements such as vectors and\ntrivectors, while leaving even-grade elements unchanged. Equation 2 thus gives us a linear action\n(i. e. group representation) of the Pin and Spin groups on the 2d-dimensional space of multivectors.\nThe sandwich product is grade-preserving, so this representation splits into a direct sum of represen-\ntations on each grade.\nRepresenting 3D objects\nFollowing Refs. [17, 38, 41], we represent planes with vectors, and re-\nquire that the intersection of two geometric objects is given by the wedge product of their representa-\ntions. Lines (the intersection of two planes) are thus represented as bivectors, points (the intersection\nof three planes) as trivectors. This leads to a duality between objects and operators, where objects\nare represented like transformations that leave them invariant. Table 1 provides a dictionary of these\nembeddings. It is easy to check that this representation is consistent with using the sandwich product\nfor transformations.\n6This means that for each element of E(3) there are two elements of Pin(3, 0, 1), e. g. both the vector v and\n\u2212v represent the same reflection.\n3\nEquivariance\nWe construct network layers that are equivariant with respect to E(3), or equivalently\nits double cover Pin(3, 0, 1). A function f : G3,0,1 \u2192 G3,0,1 is Pin(3, 0, 1)-equivariant with respect\nto the representation \u03c1 (or Pin(3, 0, 1)-equivariant for short) if\nf(\u03c1u(x)) = \u03c1u(f(x))\n(3)\nfor any u \u2208 Pin(3, 0, 1) and x \u2208 G3,0,1, where \u03c1u(x) is the sandwich product defined in Eq. (2).\n3\nThe Geometric Algebra Transformer\n3.1\nDesign principles and architecture overview\nGeometric inductive bias through geometric algebra representations\nGATr is designed to\nprovide a strong inductive bias for geometric data. It should be able to represent different geometric\nobjects and their transformations, for instance points, lines, planes, translations, rotations, and so on.\nIn addition, it should be able to represent common interactions between these types with few layers,\nand be able to identify them from little data (while maintaining the low bias of large transformer\nmodels). Examples of such common patterns include computing the relative distances between points,\napplying geometric transformations to objects, or computing the intersections of planes and lines.\nFollowing a body of research in computer graphics, we propose that geometric algebra gives us a\nlanguage that is well-suited to this task. We use the projective geometric algebra G3,0,1 and use the\nplane-based representation of geometric structure outlined in the previous section.\nSymmetry awareness through E(3) equivariance\nOur architecture should respect the symmetries\nof 3D space. Therefore we design GATr to be equivariant with respect to the symmetry group E(3)\nof translations, rotations, and reflections.\nNote that the projective geometric algebra naturally offers a faithful representation of E(3), including\ntranslations. We can thus represent objects that transform arbitrarily under E(3), including with\nrespect to translations of the inputs. This is in stark contrast with most E(3)-equivariant architectures,\nwhich only use O(3) representations and whose features only transform under rotation and are\ninvariant under translations\u2014and hence can not represent absolute positions like GATr can. Those\narchitectures must handle points in hand-crafted ways, like by canonicalizing w. r. t. the center of\nmass or by treating the difference between points as a translation-invariant vector.\nMany systems will not exhibit the full E(3) symmetry group. The direction of gravity, for instance,\noften breaks it down to the smaller E(2) group. To maximize the versatility of GATr, we choose to\ndevelop a E(3)-equivariant architecture and to include symmetry-breaking as part of the network\ninputs, similar to how position embeddings break permutation equivariance in transformers.\nScalability and flexibility through dot-product attention\nFinally, GATr should be expressive,\neasy to train, efficient, and scalable to large systems. It should also be as flexible as possible,\nsupporting variable geometric inputs and both static scenes and time series.\nThese desiderata motivate us to implement GATr as a transformer [50], based on attention over\nmultiple objects (similar to tokens in MLP or image patches in computer vision). This choice makes\nGATr equivariant also with respect to permutations along the object dimension. As in standard\ntransformers, we can break this equivariance when desired (in particular, along time dimensions)\nthrough positional embedding.\nLike a vanilla transformer, GATr is based on a dot-product attention mechanism, for which heavily\noptimized implementations exist [14, 32, 37]. We will demonstrate later that this allows us to scale\nGATr to problems with many thousands of tokens, much further than equivariant architectures based\non graph neural networks and message-passing algorithms.\nArchitecture overview\nWe sketch GATr in Fig. 1. In the top row, we sketch the overall workflow.\nIf necessary, raw inputs are first preprocessed into geometric types. The geometric objects are then\nembedded into multivectors of the geometric algebra G3,0,1, following the recipe described in Tbl. 1.\nThe multivector-valued data are processed with a GATr network. We show this architecture in\nmore detail in the bottom row of Fig. 1. GATr consists of N transformer blocks, each consisting\n4\n=\nEqui\nlinear\n\u00d7 N\nGATr\nEqui\nlayer \nnorm\nGeo \nattn.\nEqui\nlinear\n+\nEqui\nlinear\nEqui\nlinear\nRaw inputs\nGeometric types\nMultivector & \nscalar inputs\nMultivector & \nscalar outputs\nRaw outputs\nPreprocess\nEmbed in \ngeometric \nalgebra\nGATr\nExtract \nfrom \ngeometric \nalgebra\n1\n1\nGeo\nbilinear\nEqui\nlayer \nnorm\nEqui\nlinear\nGated \nGELU\nEqui\nlinear\n+\nFigure 1: Overview over the GATr architecture. Boxes with solid lines are learnable components, those with\ndashed lines are fixed.\nof an equivariant multivector LayerNorm, an equivariant multivector self-attention mechanism, a\nresidual connection, another equivariant LayerNorm, an equivariant multivector MLP with geometric\nbilinear interactions, and another residual connection. The architecture is thus similar to a typical\ntransformer [50] with pre-layer normalization [2, 54], but adapted to correctly handle multivector\ndata and be E(3) equivariant. We describe the individual layers below.\nFinally, from the outputs of the GATr network we extract the target variables, again following the\nmapping given in Tbl. 1.\n3.2\nGATr primitives\nLinear layers\nWe begin with linear layers between multivectors. In Appendix A, we show that the\nequivariance condition of Eq. (3) severely constrains them:\nProposition 1. Any linear map \u03d5 : Gd,0,1 \u2192 Gd,0,1 that is equivariant to Pin(d, 0, 1) is of the form\n\u03d5(x) =\nd+1\nX\nk=0\nwk\u27e8x\u27e9k +\nd\nX\nk=0\nvke0\u27e8x\u27e9k\n(4)\nfor parameters w \u2208 Rd+2, v \u2208 Rd+1. Here \u27e8x\u27e9k is the blade projection of a multivector, which sets\nall non-grade-k elements to zero.\nThus, E(3)-equivariant linear maps between G3,0,1 multivectors can be parameterized with nine\ncoefficients, five of which are the grade projections and four include a multiplication with the\nhomogeneous basis vector e0. We thus parameterize affine layers between multivector-valued arrays\nwith Eq. (4), with learnable coefficients wk and vk for each combination of input channel and output\nchannel. In addition, there is a learnable bias term for the scalar components of the outputs (biases\nfor the other components are not equivariant).\nGeometric bilinears\nEquivariant linear maps are not sufficient to build expressive networks. The\nreason is that these operations allow for only very limited grade mixing, as shown in Prop. 1. For the\nnetwork to be able to construct new geometric features from existing ones, such as the translation\nvector between two points, two additional primitives are essential.\nThe first is the geometric product x, y 7\u2192 xy, the fundamental bilinear operation of geometric algebra.\nIt allows for substantial mixing between grades: for instance, the geometric product of vectors\nconsists of scalars and bivector components. The geometric product is equivariant (Appendix A).\nThe second geometric primitive we use is derived from the so-called join7 x, y 7\u2192 (x\u2217 \u2227 y\u2217)\u2217. This\nmap may appear complicated, but it plays a simple role in our architecture: an equivariant map that\ninvolves the dual x 7\u2192 x\u2217. Including the dual in an architecture is essential for expressivity: in G3,0,1,\nwithout any dualization it is impossible to represent even simple functions such as the Euclidean\n7Technically, the join has an anti-dual, not the dual, in the output. We leave this detail out for notational\nsimplicity. In our network architecture, it makes no difference for equivariance nor for expressivity whether the\nanti-dual or dual is used, as any equivariant linear layer can transform between the two.\n5\ndistance between two points [17]; we show this in Appendix A. While the dual itself is not Pin(3, 0, 1)-\nequivariant (w. r. t. \u03c1), the join operation is equivariant to even (non-mirror) transformations. To\nmake the join equivariant to mirrorings as well, we multiply its output with a pseudoscalar derived\nfrom the network inputs: x, y, z 7\u2192 EquiJoin(x, y; z) = z0123(x\u2217 \u2227 y\u2217)\u2217, where z0123 \u2208 R is the\npseudoscalar component of a reference multivector z (see Appendix B).\nWe define a geometric bilinear layer that combines the geometric product and the join of the two\ninputs as Geometric(x, y; z) = Concatenatechannels(xy, EquiJoin(x, y; z)). In GATr, this layer is\nincluded in the MLP.\nNonlinearities\nand\nnormalization\nWe\nuse\nscalar-gated\nGELU\nnonlinearities\n[23]\nGatedGELU(x) = GELU(x1)x, where x1 is the scalar component of the multivector x. More-\nover, we define an E(3)-equivariant LayerNorm operation for multivectors as LayerNorm(x) =\nx/\np\nEc\u27e8x, x\u27e9, where the expectation goes over channels and we use the invariant inner product \u27e8\u00b7, \u00b7\u27e9\nof G3,0,1.\nAttention\nGiven multivector-valued query, key, and value tensors, each consisting of ni items (or\ntokens) and nc channels (key length), we define the E(3)-equivariant multivector attention as\nAttention(q, k, v)i\u2032c\u2032 =\nX\ni\nSoftmaxi\n P\nc\u27e8qi\u2032c, kic\u27e9\n\u221a8nc\n!\nvic\u2032 .\n(5)\nHere the indices i, i\u2032 label items, c, c\u2032 label channels, and \u27e8\u00b7, \u00b7\u27e9 is the invariant inner product of the\ngeometric algebra. Just as in the original transformer [50], we thus compute scalar attention weights\nwith a scaled dot product; the difference is that we use the inner product of G3,0,1, which is the regular\nR8 dot product on 8 of the 16 dimensions, ignoring the dimensions containing e0.8 We compute the\nattention using highly efficient implementations of conventional dot-product attention [14, 32, 37].\nAs we will demonstrate later, this allows us to scale GATr to systems with many thousands of tokens.\nWe extend this attention mechanism to multi-head self-attention in the usual way.\n3.3\nExtensions\nAuxiliary scalar representations\nWhile multivectors are well-suited to model geometric data,\nmany problems contain non-geometric information as well. Such scalar information may be high-\ndimensional, for instance in sinusoidal positional encoding schemes. Rather than embedding into the\nscalar components of the multivectors, we add an auxiliary scalar representation to the hidden states\nof GATr. Each layer thus has both scalar and multivector inputs and outputs. They have the same\nbatch dimension and item dimension, but may have different number of channels.\nThis additional scalar information interacts with the multivector data in two ways. In linear layers,\nwe allow the auxiliary scalars to mix with the scalar component of the multivectors. In the attention\nlayer, we compute attention weights both from the multivectors, as given in Eq. (5), and from the\nauxiliary scalars, using a regular scaled dot-product attention. The two attention maps are summed\nbefore computing the softmax, and the normalizing factor is adapted. In all other layers, the scalar\ninformation is processed separately from the multivector information, using the unrestricted form\nof the multivector map. For instance, nonlinearities transform multivectors with equivariant gated\nGELUs and auxiliary scalars with regular GELU functions. We describe the scalar path of our\narchitecture in more detail in Appendix B.\nDistance-aware dot-product attention\nThe dot-product attention in Eq. (5) ignores the 8 di-\nmensions involving the basis element e0. These dimensions vary under translations, and thus their\nstraightforward Euclidean inner product violates equivariance. We can, however, extend the atten-\ntion mechanism to incorporate more components, while still maintaining E(3) equivariance and\nthe computational efficiency of dot-product attention. To this end, we define certain auxiliary, non-\nlinear query features \u03d5(q) and key features \u03c8(k) and extend the attention weights in Eq. (5) as\n\u27e8qi\u2032c, kic\u27e9 \u2192 \u27e8qi\u2032c, kic\u27e9 + \u03d5(qi\u2032c) \u00b7 \u03c8(kic), adapting the normalization appropriately. We define these\nnonlinear features in Appendix B.\n8We also experimented with attention mechanisms that use the geometric product rather than the dot product,\nbut found a worse performance in practice.\n6\n102\n103\n104\n105\nTraining samples\n10\n3\n10\n2\n10\n1\n100\n101\nMean squared error\nMLP\nGCA-GNN\nTransformer\nSE(3)-Trf.\nSEGNN\nGATr (ours)\n102\n103\n104\n105\nTraining samples\n10\n3\n10\n2\n10\n1\n100\n101\nMean squared error\nToken number generalization\n102\n103\n104\n105\nTraining samples\n10\n3\n10\n2\n10\n1\n100\n101\nMean squared error\nE(3) generalization\nFigure 2: n-body dynamics experiments. We show the error in predicting future positions of planets as a function\nof the training dataset size. Out of five independent training runs, the mean and standard error are shown. Left:\nEvaluating without distribution shift. GATr (\n)is more sample efficient than the equivariant SEGNN [6]\n(\n) and SE(3)-Transformer [20] (\n) and clearly outperforms non-equivariant baselines (\n,\n,\n).\nMiddle: Evaluating on systems with more planets than trained on. Transformer architectures generalize well to\ndifferent object counts. GCA-GNN has larger errors than the visible range. Right: Evaluating on translated data.\nBecause GATr is E(3) equivariant, it generalizes under this domain shift.\nOur choice of these nonlinear features not only maintains equivariance, but has a straightforward\ngeometric interpretation. When the trivector components of queries and keys represent 3D points\n(see Tbl. 1), \u03c8(q) \u00b7 \u03d5(k) is proportional to the pairwise negative squared Euclidean distance. GATr\u2019s\nattention mechanism is therefore directly sensitive to Euclidean distance, while still respecting the\nhighly efficient dot-product attention format.\nPositional embeddings\nGATr assumes the data can be described as a set of items (or tokens).\nIf these items are distinguishable and form a sequence, we encode their position using \u201crotary\npositional\u201d9 embeddings [47] in the auxiliary scalar variables.10\nAxial attention\nThe architecture is flexible about the structure of the data. In some use cases, there\nwill be a single dimension along which objects are organized, for instance when describing a static\nscene or the time evolution of a single object. But GATr also supports the organization of a problem\nalong multiple axes, for example with one dimension describing objects and another time steps. In\nthis case, we follow an axial transformer layout [24], alternating between transformer blocks that\nattend over different dimensions. (The not-attended dimensions in each block are treated like a batch\ndimension.)\n4\nExperiments\n4.1\nn-body dynamics\nWe start our empirical demonstration of GATr with an n-body dynamics problem, on which we\ncompare GATr to a wide range of baselines. Given the masses, initial positions, and velocities of\na star and a few planets, the goal is to predict the final position after the system has evolved under\nNewtonian gravity for 1000 Euler integration time steps. We compare GATr to a Transformer and\nan MLP, the equivariant SEGNN [6] and SE(3)-Transformer [20], as well as the geometric-algebra-\nbased\u2014but not equivariant\u2014GCA-GNN [41]. The experiment is described in detail in Appendix C.1.\nIn the left panel of Fig. 2 we show the prediction error as a function of the number of training samples\nused. GATr clearly outperforms all non-equivariant baselines, including the geometric-algebra-based\nGCA-GNN. Compared to the equivariant SEGNN and SE(3)-Transformer, GATr is more sample-\n9This terminology, which stems from non-geometric transformers, can be confusing. \u201cPosition\u201d here means\nposition in a sequence, not geometric position in 3D. \u201cRotary\u201d does not refer a rotation of 3D space, but rather\nto how the position in a sequence is embedded via sinusoids in the scalar channels of keys and queries. Using\npositional encoding thus does not affect E(3) equivariance.\n10Since auxiliary scalar representations and multivectors mix in the attention mechanism, the positional\nembeddings also affect the multivector processing.\n7\nefficient, while all three methods reach the same prediction error when trained on enough data. This\nprovides evidence for the usefulness of geometric algebra representations as an inductive bias.\nGATr also generalizes robustly out of domain, as we show in the middle and right panels of Fig. 1.\nWhen evaluating on a larger number of bodies than trained on, methods that use a softmax over\nattention weights (GATr, Transformer, SE(3)-Transformer) generalize best. Finally, the performance\nof the E(3)-equivariant GATr, SEGNN, and SE(3)-Transformer does not drop when evaluated on\nspatially translated data, while the non-equivariant baselines fail in this setting.\n4.2\nWall-shear-stress estimation on large meshes of human arteries\n16\n160\n1600\nNumber of training samples\n5\n10\n100\nMean approximation error (%)\nGATr (ours)\nGATr canonical\nTransformer\nTransformer canonical\nPointNet++\nPointNet++ canonical\nGEM-CNN\nGEM-CNN canonical\nFigure 3: Arterial wall-shear-stress estimation [48]. We\nshow the mean approximation error (lower is better) as a\nfunction of training dataset size, reporting results both on\nrandomly oriented training and test samples (solid mark-\ners) and on a version of the dataset in which all artery\nmeshes are canonically oriented (hollow markers). With-\nout canonicalization, GATr (\n) predicts wall shear\nstress more precisely and is more sample-efficient than\nthe baselines.\nNext, we turn towards a realistic experiment in-\nvolving more complex geometric objects. We\nstudy the prediction of the wall shear stress ex-\nerted by the blood flow on the arterial wall, an\nimportant predictor of aneurysms. While the\nwall shear stress can be computed with computa-\ntional fluid dynamics, simulating a single artery\ncan take many hours, and efficient neural sur-\nrogates can have substantial impact. However,\ntraining such neural surrogates is challenging,\nas meshes are large (around 7000 nodes in our\ndata) and datasets typically small (we work with\n1600 training meshes).\nWe train GATr on a dataset of arterial meshes\nand simulated wall shear stress published by\nSuk et al. [48]. They are compared to a Trans-\nformer and to the results reported by Suk et al.\n[48], including the equivariant GEM-CNN [15]\nand the non-equivariant PointNet++ [36].11 See\nAppendix C.2 for experiment details.\nThe results are shown in Fig. 3.\nOn non-\ncanonicalized data, with randomly rotated\nmeshes, GATr improves upon all previous meth-\nods and sets a new state of the art.\nWe also experiment with canonicalization: rotat-\ning the arteries such that blood always flows in the same direction. This helps the Transformer to\nbe almost competitive with GATr. However, canonicalization is only feasible for relatively straight\narteries as in this dataset, not in more complex scenarios with branching and turning arteries. We find\nit likely that GATr will be more robust in such scenarios.\n4.3\nRobotic planning through invariant diffusion\nIn our third experiment, we show how GATr defines an E(3)-invariant diffusion model, that it can be\nused for model-based reinforcement learning and planning, and that this combination is well-suited\nto solve robotics problems. We follow Janner et al. [27], who propose to treat learning a world model\nand planning within that model as a unified generative modeling problem. After training a diffusion\nmodel [45] on offline trajectories, one can use it in a planning loop, sampling from it conditional on\nthe current state, desired future states, or to maximize a given reward, as needed.\nWe use a GATr model as the denoising network in a diffusion model and to use it for planning. We\ncall this combination GATr-Diffuser. Combining the equivariant GATr with an invariant base density\ndefines an E(3) \u00d7 Sn-invariant diffusion model [29].\nGATr-Diffuser is demonstrated on the problem of a Kuka robotic gripper stacking blocks using the\n\u201cunconditional\u201d environment introduced by Janner et al. [27]. We train GATr-Diffuser on the offline\n11We also tried to run SEGNN [5] and SE(3)-Transformer [20] as additional baselines, but were not able to\nfit them into memory on this task.\n8\n101\n102\n103\n104\nTraining trajectories\n0\n20\n40\n60\n80\nReward\nGATr-Diffuser (ours)\nTransformer-Diffuser\nDiffuser (reproduced)\nEDGI\nDiffuser\nCQL\nBCQ\nFigure 4: Diffusion-based robotic planning. We show\nnormalized rewards (higher is better) as a function of\ntraining dataset size. GATr (\n) is more successful\nat block stacking and more sample-efficient than the\nbaselines, including the original Diffuser [27] (\n) and\nour modification of it based on a Transformer (\n). In\ngrey, we show results reported by Brehmer et al. [7] and\nJanner et al. [27].\ntrajectory dataset published with that paper and\nthen use it for planning, following the setup\nof Janner et al. [27]. We compare our GATr-\nDiffuser model to a reproduction of the origi-\nnal Diffuser model and a new Transformer back-\nbone for the Diffuser model. In addition, we\nshow the published results of Diffuser [27], the\nequivariant EDGI [7], and the offline RL algo-\nrithms CQL [30] and BCQ [21] as published in\nRef. [27]. The problem and hyperparameters are\ndescribed in detail in Appendix C.3.\nAs shown in Fig. 4, GATr-Diffuser solves the\nblock-stacking task better than all baselines. It is\nalso clearly more sample-efficient, matching the\nperformance of a Diffuser model or Transformer\ntrained on the full dataset even when training\nonly on 1% of the trajectories.\n4.4\nScaling\nFinally, we study GATr\u2019s computational require-\nments and scaling. We measure the memory\nusage and compute time of forward and back-\nward passes on synthetic data as a function of\nthe number of items. GATr is compared to a\nTransformer, to SEGNN [6] in the official implementation, and to the SE(3)-Transformer [20]; for\nthe latter we use the highly optimized implementation by Milesi [35]. We choose hyperparameters\nfor the four architectures such that they have the same depth and width and require that the methods\nallow all items to interact at each step (i. e. fully connected graphs). Because of the fundamental dif-\nferences between the architectures, it is impossible to find fully equivalent settings; our results should\nthus be interpreted with care. The details of our experiment are described in Appendix C.4.\nWe show the results in Fig. 5. For few tokens, GATr is slower than a Transformer. However, this\ndifference is partially due to the low utilization of the GPUs in this test; GATr is closer to the\n102\n103\n104\n10\n1\n100\n101\nGPU memory usage [GB]\n102\n103\n104\nNumber of items\n10\n1\n100\n101\nTime per step [s]\nGATr (ours)\nTransformer\nSE(3)-Trf.\nSEGNN\nFigure 5: Computational cost and scaling. We measure\npeak GPU memory usage (top) and wall time (bottom)\nper combined forward and backward pass as a func-\ntion of the number of items in synthetic data. Despite\nsome compute overhead, GATr (\n) scales just like the\nTransformer (\n) and orders of magnitude more favor-\nably than the equivariant baselines (\n,\n).\nTransformer when using larger batch sizes or\nwhen pre-compiling the computation graph.\nFor larger problems, compute and memory are\ndominated by the pairwise interactions in the at-\ntention mechanism. Here GATr and the base-\nline Transformer perform on par, as they use the\nsame efficient implementation of dot-product at-\ntention. In terms of memory, both scale linearly\nin the number of tokens, while the equivariant\nbaselines scale quadratically. In terms of time,\nall methods scale quadratically, but the equivari-\nant baselines have a worse prefactor than GATr\nand the Transformer. All in all, we can easily\nscale GATr to tens of thousands of tokens, while\nthe equivariant baselines run out of memory two\norders of magnitude earlier.\n5\nRelated work\nGeometric algebra\nGeometric (or Clifford)\nalgebra was first conceived in the 19th century\n[10, 22] and has been used widely in quantum\nphysics [16, 34]. Recently, it has found new pop-\nularity in computer graphics [18]. In particular,\n9\nthe projective geometric algebra used in this work [17, 38] and a conformal model [18] are suitable\nfor 3D computations.\nGeometric algebras have been used in machine learning in various ways. Spellings [46] use G3,0,0\ngeometric products to compute rotation-invariant features from small point clouds. Unlike us, they\ndo not learn internal geometric representations.\nOur work was inspired by Brandstetter et al. [5] and Ruhe et al. [41]. These works also use\nmultivector features (the latter even of the same G3,0,1), and process them with operations such as\nthe geometric / sandwich product, Clifford Fourier transforms and Clifford convolutions. The main\ndifference to our work is that GATr is E(3) equivariant, while both of these works are not. We\ncompare to the GCA-GNN network from Ref. [41] in our experiments.\nConcurrently to this publication, Ruhe et al. [40] also study equivariant, geometric algebra\u2013based\narchitectures. While some of our and their findings overlap, there are several differences: They\ndevelop theory for generic geometric algebras of the form Gp,q,r, while we focus on the projective\nalgebra G3,0,1 with its faithful E(3) representations, with our theory also applicable to the group E(n)\nand the algebras Gn,0,0 and Gn,0,1. Ruhe et al. [40] also finds the linear maps of Eq. (4), but does not\ndiscuss the join or distance-aware dot-product, which we found to be essential for performance in the\nprojective algebra. Moreover, they propose an MLP-like architecture and use it in a message-passing\ngraph network, while our GATr is a Transformer.\nEquivariant deep learning\nEquivariance to symmetries [11] is the primary design principle in\nmodern geometric deep learning [8]. Equivariant networks have been applied successfully in areas\nsuch as medical imaging [31, 53] and robotics [7, 26, 44, 51, 52, 55], and are ubiquitous in applications\nof machine learning to physics and chemistry [1, 3, 4, 19, 28, 33].\nIn recent years, a number of works have investigated equivariant Transformer and message-passing\narchitectures [3, 4, 6, 19, 20, 39, 42, 49]. These works are generally more limited in terms of the\ntypes of geometric quantities they can process compared to our multivector features. Furthermore,\nour architecture is equivariant to the full group of Euclidean transformations, whereas previous works\nfocus on SO(3) equivariance.\n6\nDiscussion\nWe introduced the Geometric Algebra Transformer (GATr), a general-purpose architecture for geo-\nmetric data, and implemented it at https://github.com/qualcomm-ai-research/geometric-\nalgebra-transformer. We argued and demonstrated that GATr effectively combines structure and\nscalability.\nGATr incorporates geometric structure by representing data in projective geometric algebra, as well\nas through E(3) equivariance. Unlike most equivariant architectures, GATr features faithful E(3)\nrepresentations, including absolute positions and equivariance with respect to translations. Empiri-\ncally, GATr outperforms non-geometric, equivariant, and geometric algebra\u2013based non-equivariant\nbaselines across three experiments.\nAt the same time, GATr scales much better than most geometric networks. This is because GATr is a\nTransformer and computes pairwise interactions through dot-product attention. Using recent efficient\nattention implementations, we demonstrated that we can scale GATr to systems with many thousands\nof tokens and fully connected interactions. For few tokens and small batch sizes, GATr has some\ncomputational overhead, which we hope to address in future implementations.\nOne drawback of our approach is that since geometric algebra is not widely known yet, it may present\nan obstacle to understanding the details of the method. However, given a dictionary for embeddings of\ncommon objects and a library of primitives that act on them, using this framework is no more difficult\nthan using typical neural network layers grounded in linear algebra. Another potential downside is that\nGATr is not yet shown to be a universal approximator, which is an interesting direction for future work.\nGiven the promising results presented in this work, we look forward to further study the potential of\nGeometric Algebra Transformers in problems from molecular dynamics to robotics.\n10\nAcknowledgements\nWe would like to thank Joey Bose, Johannes Brandstetter, Gabriele Cesa,\nSteven De Keninck, Daniel Dijkman, Leo Dorst, Mario Geiger, Jonas K\u00f6hler, Ekdeep Singh Lubana,\nEvgeny Mironov, and David Ruhe for generous guidance regarding geometry, enthusiastic encourage-\nment on equivariance, and careful counsel concerning computing complications.\nReferences\n[1] Brandon Anderson, Truong Son Hy, and Risi Kondor. Cormorant: Covariant molecular neural\nnetworks. Advances in Neural Information Processing Systems, 32, 2019. (Cited on page 10)\n[2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\narXiv:1809.10853, 2018. (Cited on pages 5 and 22)\n[3] Ilyes Batatia, David Peter Kovacs, Gregor N C Simm, Christoph Ortner, and Gabor Csanyi.\nMACE: Higher order equivariant message passing neural networks for fast and accurate force\nfields. In Advances in Neural Information Processing Systems, 2022. (Cited on page 10)\n[4] Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai\nKornbluth, Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E(3)-equivariant graph neural\nnetworks for data-efficient and accurate interatomic potentials. Nat. Commun., 13(1):2453, May\n2022. (Cited on page 10)\n[5] Johannes Brandstetter, Rianne van den Berg, Max Welling, and Jayesh K Gupta. Clifford neural\nlayers for PDE modeling. arXiv:2209.04934, 2022. (Cited on pages 2, 8, and 10)\n[6] Johannes Brandstetter, Rob Hesselink, Elise van der Pol, Erik J Bekkers, and Max Welling.\nGeometric and physical quantities improve E(3) equivariant message passing. In International\nConference on Learning Representations, 2022. (Cited on pages 7, 9, 10, 22, and 25)\n[7] Johann Brehmer, Joey Bose, Pim De Haan, and Taco Cohen. EDGI: Equivariant diffusion\nfor planning with embodied agents. In Advances in Neural Information Processing Systems,\nvolume 37, 2023. (Cited on pages 9, 10, and 24)\n[8] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar Veli\u02c7ckovi\u00b4c. Geometric deep learning:\nGrids, groups, graphs, geodesics, and gauges. 2021. (Cited on page 10)\n[9] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear\nmemory cost. arXiv:1604.06174, 2016. (Cited on page 23)\n[10] William Kingdon Clifford. Applications of Grassmann\u2019s Extensive Algebra. Amer. J. Math., 1\n(4):350\u2013358, 1878. (Cited on page 9)\n[11] Taco Cohen. Equivariant Convolutional Networks. PhD thesis, University of Amsterdam, 2021.\n(Cited on page 10)\n[12] Taco Cohen and Max Welling. Group equivariant convolutional networks. In International\nConference on Machine Learning, pages 2990\u20132999. PMLR, 2016. (Cited on page 2)\n[13] Erwin Coumans and Yunfei Bai. PyBullet, a Python module for physics simulation for games,\nrobotics and machine learning. http://pybullet.org, 2016\u20132019. (Cited on page 23)\n[14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast\nand memory-efficient exact attention with IO-awareness. Advances in Neural Information\nProcessing Systems, 35:16344\u201316359, 2022. (Cited on pages 4 and 6)\n[15] Pim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh\nCNNs: Anisotropic convolutions on geometric graphs. International Conference on Learning\nRepresentations, 2021. (Cited on pages 8 and 23)\n[16] C Doran and A Lasenby. Geometric algebra for physicists. Cambridge University Press, 2003.\n(Cited on page 9)\n[17] Leo Dorst. A guided tour to the plane-based geometric algebra pga. 2020. URL https:\n//geometricalgebra.org/downloads/PGA4CS.pdf. (Cited on pages 2, 3, 6, 10, and 19)\n11\n[18] Leo Dorst, Daniel Fontijne, and Stephen Mann. Geometric Algebra for Computer Science:\nAn Object-oriented Approach to Geometry. Morgan Kaufmann Series in Computer Graphics.\nMorgan Kaufmann, Amsterdam, 2007. ISBN 978-0-12-369465-2. (Cited on pages 2, 9, 10, and 15)\n[19] Thorben Frank, Oliver Thorsten Unke, and Klaus Robert Muller. So3krates: Equivariant\nattention for interactions on arbitrary length-scales in molecular systems. October 2022. (Cited\non page 10)\n[20] Fabian B Fuchs, Daniel E Worrall, Volker Fischer, and Max Welling. SE(3)-Transformers: 3D\nRoto-Translation equivariant attention networks. In Advances in Neural Information Processing\nSystems, 2020. (Cited on pages 7, 8, 9, 10, and 22)\n[21] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International Conference on Machine Learning, pages 2052\u20132062.\nPMLR, 2019. (Cited on pages 9 and 24)\n[22] Hermann Grassmann. Die lineale Ausdehnungslehre. Otto Wigand, Leipzig, 1844. (Cited on\npage 9)\n[23] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv:1606.08415, 2016.\n(Cited on pages 6 and 22)\n[24] Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in\nmultidimensional transformers. arXiv:1912.12180 [cs], December 2019. (Cited on page 7)\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Neural\nInformation Processing Systems, 2020. (Cited on page 24)\n[26] Haojie Huang, Dian Wang, Robin Walters, and Robert Platt. Equivariant transporter network.\nIn Proceedings of Robotics: Science and Systems, 2022. (Cited on page 10)\n[27] Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. In International Conference on Machine Learning, 2022. (Cited on\npages 8, 9, 23, and 24)\n[28] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ron-\nneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin \u017d\u00eddek, Anna Potapenko, et al.\nHighly accurate protein structure prediction with alphafold. Nature, 596(7873):583\u2013589, 2021.\n(Cited on page 10)\n[29] Jonas K\u00f6hler, Leon Klein, and Frank No\u00e9. Equivariant flows: exact likelihood generative\nlearning for symmetric densities. In International Conference on Machine Learning, pages\n5361\u20135370. PMLR, 2020. (Cited on page 8)\n[30] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Information Processing Systems, 33:1179\u2013\n1191, 2020. (Cited on pages 9 and 24)\n[31] Maxime W Lafarge, Erik J Bekkers, Josien P W Pluim, Remco Duits, and Mitko Veta. Roto-\ntranslation equivariant convolutional networks: Application to histopathology image analysis.\nMed. Image Anal., 68, 2021. (Cited on page 10)\n[32] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano,\nSean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, and Daniel Haziza.\nxFormers: A modular and hackable Transformer modelling library. https://github.com/\nfacebookresearch/xformers, 2022. (Cited on pages 4, 6, 23, and 24)\n[33] Yi-Lun Liao and Tess Smidt. Equiformer: Equivariant graph attention transformer for 3d\natomistic graphs. arXiv:2206.11990, 2022. (Cited on page 10)\n[34] Pertti Lounesto. Clifford Algebras and Spinors. London Mathematical Society Lecture Note.\nCambridge University Press, 2001. (Cited on page 9)\n12\n[35] Alexandre\nMilesi.\nAccelerating\nSE(3)-Transformers\ntraining\nusing\nan\nNVIDIA\nopen-source\nmodel\nimplementation.\nhttps://developer.nvidia.com/blog/\naccelerating-se3-transformers-training-using-an-nvidia-open-source-\nmodel-implementation/, 2021. (Cited on pages 9 and 25)\n[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical\nfeature learning on point sets in a metric space. Advances in Neural Information Processing\nSystems, 30, 2017. (Cited on pages 8 and 23)\n[37] Markus N Rabe and Charles Staats.\nSelf-attention does not need O(n2) memory.\narXiv:2112.05682, 2021. (Cited on pages 4 and 6)\n[38] Martin Roelfs and Steven De Keninck.\nGraded symmetry groups: plane and simple.\narXiv:2107.03771, 2021. (Cited on pages 2, 3, 10, and 15)\n[39] David W Romero, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Attentive group\nequivariant convolutional networks. In Proceedings of the 37th International Conference on\nMachine Learning, volume 119, pages 8188\u20138199. JMLR.org, July 2020. (Cited on page 10)\n[40] David Ruhe, Johannes Brandstetter, and Patrick Forr\u00e9. Clifford group equivariant neural\nnetworks. In Advances in Neural Information Processing Systems, volume 37, 2023. (Cited on\npages 2 and 10)\n[41] David Ruhe, Jayesh K Gupta, Steven de Keninck, Max Welling, and Johannes Brandstetter.\nGeometric clifford algebra networks. In International Conference on Machine Learning, 2023.\n(Cited on pages 2, 3, 7, 10, and 22)\n[42] V\u00edctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural\nnetworks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Proceedings of Machine Learning Research,\npages 9323\u20139332. PMLR, 2021. (Cited on page 10)\n[43] Noam Shazeer. Fast transformer decoding: One write-head is all you need. arXiv:1911.02150,\n2019. (Cited on page 21)\n[44] Anthony Simeonov, Yilun Du, Andrea Tagliasacchi, Joshua B Tenenbaum, Alberto Rodriguez,\nPulkit Agrawal, and Vincent Sitzmann. Neural descriptor fields: SE(3)-Equivariant object\nrepresentations for manipulation. In ICRA, 2022. (Cited on page 10)\n[45] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In International Conference on Machine\nLearning, pages 2256\u20132265. PMLR, 2015. (Cited on page 8)\n[46] Matthew Spellings.\nGeometric algebra attention networks for small point clouds.\narXiv:2110.02393, 2021. (Cited on pages 2 and 10)\n[47] Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer\nwith rotary position embedding. arXiv:2104.09864, 2021. (Cited on page 7)\n[48] Julian Suk, Pim de Haan, Phillip Lippe, Christoph Brune, and Jelmer M Wolterink. Mesh neural\nnetworks for se (3)-equivariant hemodynamics estimation on the artery wall. arXiv:2212.05023,\n2022. (Cited on pages 8, 22, and 23)\n[49] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick\nRiley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point\nclouds. arXiv:1802.08219, 2018. (Cited on page 10)\n[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, volume 30, 2017. (Cited on pages 2, 4, 5, and 6)\n[51] Dian Wang, Robin Walters, Xupeng Zhu, and Robert Platt. Equivariant Q learning in spatial\naction spaces, 2021. (Cited on page 10)\n13\n[52] Dian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt. On-Robot learning with\nequivariant models. In CoRL, 2022. (Cited on page 10)\n[53] Marysia Winkels and Taco Cohen. Pulmonary nodule detection in CT scans with equivariant\nCNNs. MIA, 2019. (Cited on page 10)\n[54] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\nIn International Conference on Machine Learning, pages 10524\u201310533. PMLR, 2020. (Cited on\npages 5 and 22)\n[55] Xupeng Zhu, Dian Wang, Ondrej Biza, Guanang Su, Robin Walters, and Robert Platt. Sample\nefficient grasp learning using equivariant models. In Robotics: Science and Systems XVIII.\nRobotics: Science and Systems Foundation, June 2022. (Cited on page 10)\n14\nA\nTheoretical results\nIn this section, we state or prove several properties of equivariant maps between geometric algebras\nthat we use in the construction of GATr.\nThe grade involution is a linear involutive bijection b\u00b7 : Gn,0,r : Gn,0,r, which sends a k-blade\nx to bx = (\u22121)kx. Note that this is an algebra automorphism c\nxy = bxby, and also an \u2227-algebra\nautomorphism. The reversal in a linear involutive bijectione\u00b7 : Gn,0,r : Gn,0,r which sends a k-blade\nx = x1\u2227x2\u2227...\u2227xk to the reverse: ex = xk\u2227...\u2227x2\u2227x1 = \u00b1x with +x if k \u2208 {0, 1, 4, 5, ..., 8, 9, ...}\nand \u2212x otherwise. Note that this is an anti-automorphism (contravariant functor): f\nxy = eyex.\nHere we denote the sandwich action of u \u2208 Pin(n, 0, r) on a multivector x not as \u03c1u(x), but\nas u[x]. For odd u, u[x] = ubxu\u22121, while for even u, u[x] = uxu\u22121. The sandwich action is\nlinear by linearity of theb\u00b7 and bilinearity of the geometric product. Furthermore, note that for any\nparticular u \u2208 Pin(n, 0, r), the action is a geometric algebra homomorphism: u[ab] = u babu\u22121 =\nubau\u22121ubbu\u22121 = u[a]u[b]. By linearity and a symmetrization argument [18, Sec 7.1], one can show\nthat it also a \u2227-algebra homomorphism (outermorphism): u[a \u2227 b] = u[a] \u2227 u[b].\nLet l \u2265 k. Given a k-vector a and l-vector b, define the left contraction as a\u230bb := \u27e8ab\u27e9l\u2212k, which is a\nl \u2212 k-vector. For k = 1, and b a blade b = b1 \u2227 ... \u2227 bl. Geometrically, a\u230bb is the projection of a to\nthe space spanned by the vectors bi. Thus we have that a\u230bb = 0 \u21d0\u21d2 \u2200i, \u27e8a, bi\u27e9 = 0 [18, Sec 3.2.3],\nin which case we define a and b to be orthogonal. In particular, two vectors a, b are orthogonal if\ntheir inner product is zero. Futhermore, we define a vector a to be tangential to blade b if a \u2227 b = 0.\nIn the projective algebra, a blade x is defined to be ideal if it can be written as x = e0 \u2227 y for another\nblade y.\nA.1\nLinear maps\nWe begin with Pin-equivariant linear maps. After some technical lemmata, we prove the most general\nform of linear equivariant maps in the Euclidean geometric algebra Gn,0,0, and then also in projective\ngeometric algebra Gn,0,1.\nProposition 2. The grade projection \u27e8\u00b7\u27e9k is equivariant [18, Sec 13.2.3].\nProof. Choose an l-blade x = a1 \u2227 a2 \u2227 ... \u2227 al. Let u be a 1-versor. As the action u is an\noutermorphism, u[x] = u[a1] \u2227 ... \u2227 u[al] is an l-blade. Now if l \u0338= k, then \u27e8x\u27e9k = 0 and thus\nu[\u27e8x\u27e9k] = \u27e8u[x]\u27e9k. If l = k, then \u27e8x\u27e9k = x and thus u[\u27e8x\u27e9k] = \u27e8u[x]\u27e9k. As the grade projection is\nlinear, equivariance extends to any multivector.\nProposition 3. The following map is equivariant: \u03d5 : G3,0,1 \u2192 G3,0,1 : x 7\u2192 e0x.\nProof. Let u be a 1-versor, then u acts on a multivector as x 7\u2192 u[x] = u\u02c6xu\u22121, where \u02c6x is the grade\ninvolution. Note that e0 is invariant: u[e0] = \u2212ue0u\u22121 = e0uu\u22121 = e0, where ue0 = \u2212e0u because\nu and e0 are orthogonal: ue0 = \u27e8u, e0\u27e9 + u \u2227 e0 = \u2212e0 \u2227 u = \u2212e0 \u2227 u. Then \u03d5 is equivariant,\nas the action is an algebra homomorphism: u[\u03d5(x)] = u[e0x] = ud\ne0xu\u22121 = u\u02c6e0u\u22121u\u02c6xu\u22121 =\nu[e0]u[x] = e0u[x] = \u03d5(u[x]). It follows that \u03d5 is also equivariant to any product of vectors, i.e. any\nversor u.\nEuclidean geometric algebra\nBefore constructing the most general equivariant linear map between\nmultivectors in projective geometric algebra, we begin with the Euclidean case Gn,0,0.\nTheorem 1 (Cartan-Dieuodonn\u00e9). Every orthogonal transformation of an n-dimensional space can\nbe decomposed into at most n reflections in hyperplanes.\nProof. This theorem is proven in Roelfs and De Keninck [38].\nLemma 1. In the n-dimensional Euclidean geometric algebra Gn,0,0, the group Pin(n, 0, 0) acts\ntransitively on the space of k-blades of norm \u03bb \u2208 R>0.\n15\nProof. As the Pin group preserves norm, choose \u03bb = 1 without loss of generality. Any k-blade x\nof unit norm can be written by Gram-Schmidt factorization as the wedge product of k orthogonal\nvectors of unit norm x = v1 \u2227 v2 \u2227 ... \u2227 vk. Consider another k-blade y = w1 \u2227 w2 \u2227 ... \u2227 wk with\nwi orthonormal. We\u2019ll construct a u \u2208 Pin(n, 0, 0) such that u[x] = y.\nChoose n \u2212 k additional orthonormal vectors vk+1, ..., vn and wk+1, .., .wn to form orthonormal\nbases. Then, there exists a unique orthogonal transformation Rn \u2192 Rn that maps vi into wi for all\ni \u2208 {1, ..., n}. By the Cartan-Dieuodonn\u00e9 theorem 1, this orthogonal transformation can be expressed\nas the product of reflections, thus there exists a u \u2208 Pin(n, 0, 0) such that u[vi] = wi. As the u\naction is a \u2227-algebra homomorphism (u[a \u2227 b] = u[a] \u2227 u[b], for any multivectors a, b), we have that\nu[x] = y.\nLemma 2. In the Euclidean (r = 0) or projective (r = 1) geometric algebra Gn,0,r, let x be a k-\nblade. Let u be a 1-versor. Then u[x] = x \u21d0\u21d2 u\u230bx = 0 and u[x] = \u2212x \u21d0\u21d2 u \u2227 x = 0.\nProof. Let x be a k-blade and u a vector of unit norm. We can decompose u into u = t + v with\nt\u2227x = 0 (the part tangential to the subspace of x) and v\u230bx = 0 (the normal part). This decomposition\nis unique unless x is ideal in the projective GA, in which case the e0 component of u is both normal\nand tangential, and we choose t Euclidean.\nIn either case, note the following equalities: xt = (\u22121)k\u22121tx; xv = (\u22121)kvx; vt = \u2212tv and note\n\u2204\u03bb \u0338= 0 such that vtx = \u03bbx, which can be shown e.g. by picking a basis. Then:\nu[x] = (\u22121)k(t + v)x(t + v) = (t + v)(\u2212t + v)x = (\u2212\u2225t\u22252 + \u2225v\u22252)x \u2212 2vtx\nWe have u[x] \u221d x \u21d0\u21d2 vtx = 0. If x is not ideal, this implies that either v = 0 (thus u \u2227 x = 0\nand u[x] = \u2212x) or t = 0 (thus u\u230bx = 0 and u[x] = x). If x is ideal, this implies that either v \u221d e0\n(thus u \u2227 x = 0 and u[x] = \u2212x) or t = 0 (thus u\u230bx = 0 and u[x] = x).\nLemma 3. Let r \u2208 {0, 1}. Any linear Pin(n, 0, r)-equivariant map \u03d5 : Gn,0,r \u2192 Gn,0,r can be\ndecomposed into a sum of equivariant maps \u03d5 = P\nlkm \u03d5lkm, with \u03d5lkm equivariantly mapping k-\nblades to l-blades. If r = 0 (Euclidean algebra) or k < n + 1, such a map \u03d5lkm is defined by the\nimage of any one non-ideal k-blade, like e12...k. Instead, if r = 1 (projective algebra) and k = n + 1,\nthen such a map is defined by the image of a pseudoscalar, like e01...n.\nProof. The Pin(n, 0, r) group action maps k-vectors to k-vectors. Therefore, \u03d5 can be decomposed\ninto equivariant maps from grade k to grade l: \u03d5(x) = P\nlk \u03d5lk(\u27e8x\u27e9k), with \u03d5lk having l-vectors as\nimage, and all k\u2032-vectors in the kernel, for k\u2032 \u0338= k. Let x be an non-ideal k-blade (or pseudoscalar if\nk = n + 1). By lemmas 1 and 4, in both Euclidean and projective GA, the span of the k-vectors in\nthe orbit of x contains any k-vector. So \u03d5lk is defined by the l-vector y = \u03d5lk(x). Any l-vector can\nbe decomposed as a finite sum of l-blades: y = y1 + ...yM. We can define \u03d5lkm(x) = ym, extended\nto all l-vectors by equivariance, and note that \u03d5lk = P\nm \u03d5lkm.\nProposition 4. For an n-dimensional Euclidean geometric algebra Gn,0,0, any linear endomorphism\n\u03d5 : Gn,0,0 \u2192 Gn,0,0 that is equivariant to the Pin(n, 0, 0) group (equivalently to O(n)) is of the\ntype \u03d5(x) = Pn\nk=0 wk\u27e8x\u27e9k, for parameters w \u2208 Rn+1.\nProof. By decomposition of Lemma 3, let \u03d5 map from k-blades to l-blades. Let x be a k-blade.\nLet u be a 1-versor. By Lemma 2, if u is orthogonal to x, u[\u03d5(x)] = \u03d5(u[x]) = \u03d5(x) and u is also\northogonal to \u03d5(x). If u\u2227x = 0, then u[\u03d5(x)] = \u03d5(u[x]) = \u03d5(\u2212x) = \u2212\u03d5(x) and u\u2227\u03d5(x) = 0. Thus\nany vector in x is in \u03d5(x) and any vector orthogonal to x is orthogonal to \u03d5(x), this implies \u03d5(x) =\nwkx, for some wk \u2208 R. By Lemma 3, we can extend \u03d5 to \u03d5(y) = wky for any k-vector y.\nProjective geometric algebra\nHow about equivariant linear maps in projective geometric algebra?\nThe degenerate metric makes the derivation more involved, but in the end we will arrive at a result\nthat is only slightly more general.\nLemma 4. The Pin group of the projective geometric algebra, Pin(n, 0, 1), acts transitively on the\nspace of k-blades with positive norm \u2225x\u2225 = \u03bb > 0. Additionally, the group acts transitively on the\nspace of zero-norm k-blades of the form x = e0 \u2227 y (called ideal blades), with \u2225y\u2225 = \u03ba.\n16\nProof. Let x = x1 \u2227 ... \u2227 xk be a k-blade with positive norm \u03bb. All vectors xi can be written as\nxi = vi + \u03b4ie0, for a nonzero Euclidean vector vi (meaning with no e0 component) and \u03b4i \u2208 R,\nbecause if vi = 0, the norm of x would have been 0. Orthogonalize them as x\u2032\n2 = x2 \u2212 \u27e8x1, x2\u27e9x1,\netc., resulting in x = x\u2032\n1 \u2227 \u00b7 \u00b7 \u00b7 \u2227 x\u2032\nk with x\u2032\ni = v\u2032\ni + \u03b4\u2032\nie0 with orthogonal v\u2032\ni.\nDefine the translation t = 1 + 1\n2\nP\ni \u03b4\u2032\nie0 \u2227 v\u2032\ni, which makes x\u2032 Euclidean: t[x\u2032] = v\u2032\n1 \u2227 ... \u2227 v\u2032\nk. By\nLemma 1, the Euclidean Pin group Pin(n, 0, 0), which is a subgroup of Pin(n, 0, 1), acts transitively\non Euclidean k-blades of a given norm. Thus, in the projective geometric algebra Pin(n, 0, 1), any\ntwo k-blades of equal positive norm \u03bb are related by a translation to the origin and then a Pin(n, 0, 0)\ntransformation.\nFor the ideal blades, let x = e0 \u2227 y, with \u2225y\u2225 = \u03ba. We take y to be Euclidean without loss of\ngenerality. For any g \u2208 Pin(n, 0, 1), g[e0] = e0, so g[x] = e0 \u2227 g[y]. Consider another x\u2032 = e0 \u2227 y\u2032\nwith \u2225y\u2032\u2225 = \u03ba and taking y\u2032 Euclidean. As Pin(n, 0, 0) acts transitively on Euclidean (k \u2212 1)-blades\nwith norm \u03ba, let g \u2208 Pin(n, 0, 0) such that g[y] = y\u2032. Then g[x] = x\u2032.\nWe can now construct the most general equivariant linear map between projective geometric algebras,\na key ingredient for GATr:\nProposition 5. For the projective geometric algebra Gn,0,1, any linear endomorphism \u03d5 : Gn,0,1 \u2192\nGn,0,1 that is equivariant to the group Pin(n, 0, r) (equivalently to E(n)) is of the type \u03d5(x) =\nPn+1\nk=0 wk\u27e8x\u27e9k + Pn\nk=0 vke0\u27e8x\u27e9k, for parameters w \u2208 Rn+2, v \u2208 Rn+1.\nProof. Following Lemma 3, decompose \u03d5 into a linear equivariant map from k-blades to l-blades.\nFor k < n + 1, let x = e12...k. Then following Lemma 2, for any 1 \u2264 i \u2264 k, ei \u2227 x = 0, ei[x] = \u2212x,\nand ei[\u03d5(x)] = \u03d5(ei[x]) = \u03d5(\u2212x) = \u2212\u03d5(x) and thus ei \u2227 \u03d5(x) = 0. Therefore, we can write\n\u03d5(x) = x \u2227 y1 \u2227 ... \u2227 yl\u2212k, for l \u2212 k vectors yj orthogonal to x.\nAlso, again using Lemma 2, for k < i \u2264 n, ei\u230bx = 0 =\u21d2 ei[\u03d5(x)] = \u03d5(x) =\u21d2 ei\u230b\u03d5(x) =\n0 =\u21d2 \u2200i, \u27e8ei, yj\u27e9 = 0. Thus, yj is orthogonal to all ei with 1 \u2264 i \u2264 n. Hence, l = k or l = k + 1\nand y1 \u221d e0.\nFor k = n + 1, let x = e012...k. By a similar argument, all invertible vectors u tangent to x must be\ntangent to \u03d5(x), thus we find that \u03d5(x) = x \u2227 y for some blade y. For any non-zero \u03d5(x), y \u221d 1, and\nthus \u03d5(x) \u221d x. By Lemma 3, by equivariance and linearity, this fully defines \u03d5.\nA.2\nBilinear maps\nNext, we turn towards bilinear operations. In particular, we show that the geometric product and the\njoin are equivariant.\nFor the geometric product, equivariance is straightforward: Any transformation u \u2208 Pin(n, 0, r),\ngives a homomorphism of the geometric algebra, as for any multivectors x, y, u[xy] = uc\nxyu\u22121 =\nubxbyu\u22121 = ubxu\u22121ubyu\u22121 = u[x]u[y]. The geometric product is thus equivariant.\nDual and join in Euclidean algebra\nFor the join and the closely related dual, we again begin with\nthe Euclidean geometric algebra, before turning to the projective case later.\nThe role of the dual is to have a bijection \u00b7\u2217 : Gn,0,0 \u2192 Gn,0,0 that maps k-vectors to (n\u2212k)-vectors.\nFor the Euclidean algebra, with a choice of pseudoscalar I, we can define a dual as:\nx\u2217 = xI\u22121 = x\u02dcI\n(6)\nThis dual is bijective, and involutive up to a sign: (y\u2217)\u2217 = y\u02dcI \u02dcI = \u00b1y, with +y = 1 for n \u2208\n{1, 4, 5, 8, 9, ...} and \u2212y for n \u2208 {2, 3, 6, 7, ...}. We choose \u02dcI instead of I in the definition of the\ndual so that given n vectors x1, ..., xn, the dual of the multivector x = x1 \u2227 ...xn, is given by the\nscalar of the oriented volume spanned by the vector. We denote the inverse of the dual as x\u2212\u2217 = xI.\nExpressed in a basis, the dual yields the complementary indices and a sign. For example, for n = 3\nand I = e123, we have (e1)\u2217 = \u2212e23, (e12)\u2217 = e3.\nVia the dual, we can define the bilinear join operation, for multivectors x, y:\nx \u2228 y := (x\u2217 \u2227 y\u2217)\u2212\u22c6 = ((x\u02dcI) \u2227 (y\u02dcI))I .\n17\nLemma 5. In Euclidean algebra Gn,0,0, the join is Spin(n, 0, 0) equivariant. Furthermore, it is\nPin(n, 0, 0) equivariant if and only if n is even.\nProof. The join is equivariant to the transformations from the group Spin(n, 0, 0), which consists of\nthe product of an even amount of unit vectors, because such transformations leave the pseudoscalar I\ninvariant, and the operation consists otherwise of equivariant geometric and wedge products.\nHowever, let e12...n = I \u2208 Pin(n, 0, 0) be the point reflection, which negates vectors of odd grades\nby the grade involution: I[x] = \u02c6x. Let x be a k-vector and y an l-vector. Then x \u2228 y is a vector of\ngrade n \u2212 ((n \u2212 k) + (n \u2212 l)) = k + l \u2212 n (and zero if k + l < n). Given that the join is bilinear,\nthe inputs transform as (\u22121)k+l under the point reflection, while the transformed output gets a sign\n(\u22121)k+l\u2212n. Thus for odd n, the join is not Pin(n, 0, 0) equivariant.\nTo address this, given a pseudoscalar z = \u03bbI, we can create an equivariant Euclidean join via:\nEquiJoin(x, y, z = \u03bbI) := \u03bb(x \u2228 y) = \u03bb(x\u2217 \u2227 y\u2217)\u2212\u2217 .\n(7)\nProposition 6. In Euclidean algebra Gn,0,0, the equivariant join EquiJoin is Pin(n, 0, 0) equivari-\nant.\nProof. The EquiJoin is a multilinear operation, so for k-vector x and l-vector y, under a point\nreflection, the input gets a sign (\u22121)k+l+n while the output is still a k + l \u2212 n vector and gets sign\n(\u22121)k+l\u2212n. These signs differ by even (\u22121)2n = 1 and thus EquiJoin is Pin(n, 0, 1)-equivariant.\nWe prove two equalities of the Euclidean join which we use later.\nLemma 6. In the algebra Gn,0,0, let v be a vector and x, y be multivectors. Then\nv\u230b(x \u2228 y) = (v\u230bx) \u2228 y\n(8)\nand\nx \u2228 (v\u230by) = \u2212(\u22121)nd\nv\u230bx \u2228 y .\n(9)\nProof. For the first statement, let a be a k-vector and b an l-vector. Then note the following two\nidentities:\na \u2228 b = \u27e8a\u2217b\u02dcI\u27e92n\u2212k\u2212lI = \u27e8a\u2217b\u27e9n\u2212(2n\u2212k\u2212l)\u02dcII = \u27e8a\u2217b\u27e9k+l\u2212n = a\u2217\u230bb ,\n(v\u230ba)\u2217 = \u27e8va\u27e9k\u22121\u02dcI = \u27e8va\u02dcI\u27e9n\u2212k+1 = \u27e8va\u2217\u27e9n\u2212k+1 = v\u230b(a\u2217) .\nCombining these and the associativity of \u230b gives:\n(v\u230ba) \u2228 b = (v\u230ba)\u2217\u230bb = v\u230b(a\u2217)\u230bb = v\u230b(a \u2228 b)\nFor the second statement, swapping k-vector a and l-vector b incurs a \u2228 b = (a\u2217 \u2227 b\u2217)\u2212\u2217 =\n(\u22121)(n\u2212k)(n\u2212l)(b\u2217 \u2227 a\u2217)\u2212\u2217 = (\u22121)(n\u2212k)(n\u2212l)(b \u2228 a). Then we get:\na \u2228 (v\u230bb) = (\u22121)(n\u2212k)(n\u2212l\u22121)(v\u230bb) \u2228 a\n= (\u22121)(n\u2212k)(n\u2212l\u22121)v\u230b(b \u2228 a)\n= (\u22121)(n\u2212k)(n\u2212l\u22121)+(n\u2212k)(n\u2212l)v\u230b(a \u2228 b)\n= (\u22121)(n\u2212k)(n\u2212l\u22121)+(n\u2212k)(n\u2212l)(v\u230ba) \u2228 b\n= (\u22121)(n\u2212k)(2n\u22122l\u22121)(v\u230ba) \u2228 b\n= (\u22121)k\u2212n(v\u230ba) \u2228 b\n= \u2212(\u22121)k\u22121\u2212n(v\u230ba) \u2228 b\n= \u2212(\u22121)n [\n(v\u230ba) \u2228 b .\nThis generalizes to multivectors x, y by linearity.\n18\nDual and join in projective algebra\nFor the projective algebra Gn,0,1 with its degenerate inner\nproduct, the dual definition of Eq. 6 unfortunately does not yield a bijective dual. For example,\ne0 ^\ne012...n = 0. For a bijective dual that yields the complementary indices on basis elements, a\ndifferent definition is needed. Following Dorst [17], we use the right complement. This involves\nchoosing an orthogonal basis and then for a basis k-vector x to define the dual x\u2217 to be the basis\nn + 1 \u2212 k-vector such that x \u2227 x\u2217 = I, for pseudoscalar I = e012...n. For example, this gives dual\ne\u2217\n01 = e23, so that e01 \u2227 e23 = e0123.\nThis dual is still easy to compute numerically, but it can no longer be constructed solely from\noperations available to us in the geometric algebra. This makes it more difficult to reason about\nequivariance.\nProposition 7. In the algebra Gn,0,1, the join a \u2228 b = (a\u2217 \u2227 b\u2217)\u2212\u2217 is equivariant to Spin(n, 0, 1).\nProof. Even though the dual is not a Gn,0,1 operation, we can express the join in the algebra as\nfollows. We decompose a k-vector x as x = tx + e0px into a Euclidean k-vector tx and a Euclidean\n(k \u2212 1)-vector px. Then Dorst [17, Eq (35)] computes the following expression\n(tx + e0px) \u2228 (ty + e0py) = ((tx + e0px)\u2217 \u2227 (ty + e0py)\u2217)\u2212\u2217\n= tx \u2228Euc py + (\u22121)nc\npx \u2228Euc ty + e0(px \u2228Euc py) ,\n(10)\nwhere the Euclidean join of vectors a, b in the projective algebra is defined to equal the join of the\ncorresponding vectors in the Euclidean algebra:\na \u2228Euc b := ((a ^\ne12...n) \u2227 (b ^\ne12...n))e12...n\nThe operation a \u2228Euc b is Spin(n, 0, 0) equivariant, as discussed in Lemma 5. For any rotation\nr \u2208 Spin(n, 0, 1) (which is Euclidean), we thus have r[a \u2228Euc b] = r[a] \u2228Euc r[b]. This makes the\nPGA dual in Eq. (10) equivariant to the rotational subgroup Spin(n, 0, 0) \u2282 Spin(n, 0, 1).\nWe also need to show equivariance to translations. Let v be a Euclidean vector and \u03c4 = 1 \u2212 e0v/2 a\ntranslation. Translations act by shifting with e0 times a contraction: \u03c4[x] = x \u2212 e0(v\u230bx). This acts\non the decomposed x in the following way: \u03c4[tx + e0px] = \u03c4[tx] + e0px = tx + e0(px \u2212 v\u230btx).\nWe thus get:\n\u03c4[x] \u2228 \u03c4[y] = (\u03c4[tx] + e0px) \u2228 (\u03c4[ty] + e0py)\n= (tx + e0(px \u2212 v\u230bt)) \u2228 (ty + e0(py \u2212 v\u230bty))\n= x \u2228 y \u2212 tx \u2228Euc (v\u230bty) \u2212 (\u22121)n d\nv\u230btx \u2228Euc ty\n\u2212 e0 (px \u2228Euc (v\u230bty) + (v\u230btx) \u2228Euc py)\nUsed (10) & linearity\n= x \u2228 y \u2212 e0 (px \u2228Euc (v\u230bty) + (v\u230btx) \u2228Euc py)\nUsed (9)\n= x \u2228 y \u2212 e0\n\u0010\n\u2212(\u22121)n d\nv\u230bpx \u2228Euc ty + (v\u230btx) \u2228Euc py\n\u0011\nUsed (9)\n= x \u2228 y \u2212 e0 ((\u22121)n(v\u230bc\npx) \u2228Euc ty + (v\u230btx) \u2228Euc py)\n= x \u2228 y \u2212 e0 (v\u230b {(\u22121)nc\npx \u2228Euc ty + tx \u2228Euc py})\nUsed (8)\n= \u03c4[x \u2228 y]\nThe join is thus equivariant 12 to translations and rotations and is therefore Spin(n, 0, 1) equivariant.\nSimilar to the Euclidean case, we obtain full Pin(n, 0, 1) equivariance via multiplication with a\npseudoscalar. We thus also use the EquiJoin from Eq. (7) in the projective case.\nA.3\nExpressivity\nAs also noted in Ref. [17], in the projective algebra, the geometric product itself is unable to compute\nmany quantities. It is thus insufficient to build expressive networks. This follows from the fact that\nthe geometric product preserves norms.\n12The authors agree with the reader that there must be an easier way to prove this.\n19\nLemma 7. For the algebra Gn,0,r, for multivectors x, y, we have \u2225xy\u2225 = \u2225x\u2225 \u2225y\u2225.\nProof. \u2225xy\u22252 = xyf\nxy = xy\u02dcy\u02dcx = x\u2225y\u22252\u02dcx = x\u02dcx\u2225y\u22252 = \u2225x\u22252\u2225y\u22252\nHence, any null vector in the algebra can never be mapped to a non-null vector, including scalars.\nThe projective algebra can have substantial information encoded as null vector, such as the position\nof points. This information can never influence scalars or null vectors. For example, there is no\nway to compute the distance (a scalar) between points just using the projective algebra. In the GATr\narchitecture, the input to the MLPs that operate on the scalars, or the attention weights, thus could\nnot be affected by the null information, had we only used the geometric product on multivectors.\nTo address this limitation, we use besides the geometric product also the join. The join is able to\ncompute such quantities. For example, given the Euclidean vector e12...n, we can map a null vector\nx = e012...k to a non-null vector x \u2228 e12...n \u221d e12...k.\nB\nArchitecture\nEquivariant join\nOne of the primitives in GATr is the equivariant join EquiJoin(x, y; z), which\nwe define in Eq. (7). For x and y, we use hidden states of the neural network after the previous layer.\nThe nature of z is different: it is a reference multivector and only necessary to ensure that the function\ncorrectly changes sign under mirrorings of the inputs. We find it beneficial to choose this reference\nmultivector z based on the input data rather than the hidden representations, and choose it as the\nmean of all inputs to the network.\nAuxiliary scalars\nIn addition to multivector representations, GATr supports auxiliary scalar repre-\nsentations, for instance to describe non-geometric side information such as positional encodings or\ndiffusion time embeddings. In most layers, these scalar variables are processed like in a standard\nTransformer, with two exceptions. In linear layers, we allow for the scalar components of multivectors\nand the auxiliary scalars to freely mix. In the attention operation, we compute attention weights as\nSoftmaxi\n P\nc\u27e8qMV\ni\u2032c , kMV\nic\n\u27e9 + P\nc qs\ni\u2032cks\nic\n\u221a8nMV + ns\n!\n,\n(11)\nwhere qMV and kMV are query and key multivector representations, qs and ks are query and key\nscalar representations, nMV is the number of multivector channels, and ns is the number of scalar\nchannels.\nDistance-aware dot-product attention\nAs we argue in Sec. 3.3, it can be beneficial to extend\nqueries and keys with nonlinear features. We use the following choice:\n\u03d5(q) = \u03c9(q\\0)\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\nq2\n\\0\nP\ni q2\n\\i\nq\\0 q\\1\nq\\0 q\\2\nq\\0 q\\3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nand\n\u03c8(k) = \u03c9(k\\0)\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\u2212 P\ni k2\n\\i\n\u2212k2\n\\0\n2k\\0 k\\1\n2k\\0 k\\2\n2k\\0 k\\3\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8\nwith\n\u03c9(x) =\nx\nx2 + \u03f5 .\nHere the index \\i denotes the trivector component with all indices but i. Then\n\u03d5(q) \u00b7 \u03c8(k) = \u2212\u03c9(q\\0)\u03c9(k\\0)\u2225k\\0 \u20d7q \u2212 q\\0 \u20d7k\u22252\nR3 ,\nwhere we use the shorthand \u20d7x = (x\\1, x\\2, x\\3)T . When the trivector components of queries and\nkeys represent 3D points, with q\\0 = k\\0 = 1, it becomes proportional to the pairwise negative\nsquared Euclidean distance between the points.\nIn this notation, a trivector q = (q\\0, \u20d7q) transforms under rotation R \u2208 O(3) and translation \u20d7t \u2208 R3\n20\nas (q\\0, \u20d7q) 7\u2192 (q\\0, R\u20d7q + q\\0\u20d7t). It is easy to see that the inner product is invariant:\n\u03d5(q) \u00b7 \u03c8(k) 7\u2192 \u2212\u03c9(q\\0)\u03c9(k\\0)\u2225k\\0 (R\u20d7q + q\\0\u20d7t) \u2212 q\\0 (R\u20d7k + k\\0\u20d7t)\u22252\nR3\n= \u2212\u03c9(q\\0)\u03c9(k\\0)\u2225R(k\\0 \u20d7q \u2212 q\\0 \u20d7k) + q\\0k\\0\u20d7t \u2212 q\\0k\\0\u20d7t\u22252\nR3\n= \u2212\u03c9(q\\0)\u03c9(k\\0)\u2225k\\0 \u20d7q \u2212 q\\0 \u20d7k\u22252\nR3\n= \u03d5(q) \u00b7 \u03c8(k) .\nThe normalization function \u03c9 could have been chosen as \u03c9(x) = 1, but this would make the attention\nlogit a fourth-order polynomial of the keys and queries, which tends to lead to instabilities when\ntaking the exponential in the softmax. The choice above makes the attention logit a quadratic function\nfor larger values of the keys and queries, just like regular dot-product attention.\nFor the interested reader, we\u2019d like to note that the above inner product is closely related to the inner\nproduct in another geometric algebra, the conformal geometric algebra. We plan on exploring this\nconnection in future work.\nWith this final piece, GATr\u2019s attention mechanism computes attention weights from three sources:\nthe G3,0,1 inner product of the multivector queries and keys \u27e8q, k\u27e9, the distance-aware inner product\nof the nonlinear features \u03d5(q) \u00b7 \u03c8(k), and the Euclidean inner product of the auxiliary scalars qs \u00b7 ks.\nWe find it beneficial to add learnable weights as prefactors to each of these three terms. The attention\nweights are then given by\nSoftmaxi\n \n\u03b1 P\nc\u27e8qMV\ni\u2032c , kMV\nic\n\u27e9 + \u03b2 P\nc \u03d5(qMV\ni\u2032c ) \u00b7 \u03c8(kMV\nic\n) + \u03b3 P\nc qs\ni\u2032cks\nic\n\u221a13nMV + ns\n!\nwith learnable, head-specific \u03b1, \u03b2, \u03b3 > 0.\nThis may look complicated, but all terms can be summarized in a single Euclidean dot product\nbetween query features and key features. We can therefore use efficient implementations of dot-\nproduct attention to compute GATr\u2019s attention.\nMulti-query attention\nTo reduce memory use, we also consider a version of GATr that uses multi-\nquery attention [43] instead of multi-head attention, sharing the keys and values among attention\nheads.\nC\nExperiments\nC.1\nn-body dynamics\nDataset\nWe generate a synthetic n-body dataset for n objects by following these steps for each\nsample:\n1. The masses of n objects are sampled from log-uniform distributions. For one object (the\nstar), we use m0 \u2208 [1, 10]; for the remaining objects (the planets), we use mi \u2208 [0.01, 0.1].\n(Following common practice in theoretical physics, we use dimensionless quantities such\nthat the gravitational constant is 1.)\n2. The initial positions of all bodies are sampled. We first use a heliocentric reference frame.\nHere the initial positions of all bodies are sampled. The star is set to the origin, while the\nplanets are sampled uniformly on a plane within a distance ri \u2208 [0.1, 1.0] from the star.\n3. The initial velocities are sampled. In the heliocentric reference frame, the star is at rest.\nThe planet velocities are determined by computing the velocity of a stable circular orbit\ncorresponding to the initial positions and masses, and then adding isotropic Gaussian noise\n(with standard deviation 0.01) to it.\n4. We transform the positions and velocities from the heliocentric reference frame to a global\nreference frame by applying a random translation and rotation to it. The translation is\nsampled from a multivariate Gaussian with standard deviation 20 and zero mean (except\nfor the domain generalization evaluation set, where we use a mean of (200, 0, 0)T ). The\nrotation is sampled from the Haar measure on SO(3). In addition, we apply a random\npermutation of the bodies.\n21\n5. We compute the final state of the system by evolving it under Newton\u2019s equations of motion,\nusing Euler\u2019s method and 100 time steps with a time interval of 10\u22124 each.\n6. Finally, samples in which any bodies have traveled more than a distance of 2 (the diamater\nof the solar system) are rejected. (Otherwise, rare gravitational slingshot effects dominate\nthe regression loss and all methods become unreliable.)\nWe generate training datasets with n = 4 and between 100 and 105 samples; a validation dataset\nwith n = 4 and 5000 samples; a regular evaluation set with n = 4 and 5000 samples; a number-\ngeneralization evaluation set with n = 6 and 5000 samples; and a E(3) generalization set with n = 4,\nan additional translation (see step 4 above), and 5000 samples.\nAll models are tasked with predicting the final object positions given the initial positions, initial\nvelocities, and masses.\nModels\nFor the GATr model, we embed object masses as scalars, positions as trivectors, and\nvelocities (like translation vectors) as bivectors. We use 10 attention blocks, 16 multivector and 128\nscalar channels, and 8 attention heads, resulting in 1.9 million parameters. We use multi-head attention\nand do not use the distance-aware attention mechanism (as it led to similar results in a short test).\nFor the Transformer baseline, we follow a pre-layer normalization [2, 54] architecture with GELU\nactivations [23] in the MLP block. We use 10 attention blocks, 384 channels, and 8 attention heads,\nresulting in 11.8 million parameters.\nWe also compare to a simple MLP with GELU activations. We test models with 2, 5, and 10 layers,\neach with 384 channels. We report results for the best-performing 2-layer MLP, which has 0.2 million\nparameters.\nNext, we compare GATr to the geometric-algebra-based, but not equivariant, GCA-GNN [41]. We\nfollow the hyperparameters reported by Ruhe et al. [41] for their Tetris experiment, arriving at an\narchitecture with 1.5 million parameters. We also experiment with the GCA-MLP architecture [41],\nbut find worse results.\nFinally, we compare to two equivariant baselines: SEGNN [6] and the SE(3)-Transformer [20]. In\nboth cases we use the code released by the authors and the hyperparameters they recommend for\n(slightly different) n-body experiments. This leads to models with 0.1 (SEGNN) and 0.3 (SE(3)-\nTransformer) million parameters. For SEGNN, we vary the number of nearest neighbours between 3\nand the number of objects in the scene (corresponding a fully connected graph) and show the best\nresult.\nTraining\nAll models are trained by minimizing a L2 loss on the final position of all objects. We\ntrain for 50 000 steps with the Adam optimizer, using a batch size of 64 and exponentially decaying\nthe learning rate from 3 \u00b7 10\u22124 to 3 \u00b7 10\u22126.\nC.2\nArterial wall-shear-stress estimation\nDataset\nWe use the single-artery wall-shear-stress dataset published by Suk et al. [48]. It consists\nof 2000 meshes of human arteries, of which 1600 are used for training the remaining for validation\nand evaluation. Each mesh has around 7000 nodes.\nWe experiment both with randomly rotated meshes as well as a canonicalized version, in which all\nmeshes point in the same direction.\nModels\nFor the GATr model, we embed object the positions of the mesh nodes as trivectors, the\nmesh surface normals as vectors, and the geodesic distance to the inlet as scalars. We use multi-query\nattention, the distance-aware attention mechanism, and learnable prefactors for the attention weights.\nOur model has 10 attention blocks, 8 multivector channels and 32 scalar channels, and 4 attention\nheads, resulting in 0.2 million parameters.\nFor the Transformer baseline, we follow a pre-layer normalization [2, 54] architecture with GELU\nactivations [23] in the MLP block. We use 10 attention blocks, 160 channels, and 4 attention heads,\nresulting in 1.7 million parameters.\nIn addition, we compare to the results reported by Suk et al. [48].\n22\nMethod\nMean approximation error [%]\nRandom orientations\nCanonical orientations\nGATr\n5.6\n5.5\nTransformer\n10.5\n6.0\nPointNet++ [36]\n12.3\n8.6\nGEM-CNN [15]\n7.7\n7.8\nTable 2: Arterial wall-shear-stress estimation [48]. We show the mean approximation error (lower is better),\nreporting results both on randomly oriented training and test samples (left) and on a version of the dataset in\nwhich all artery meshes are canonically oriented (right). We compare GATr to our own Transformer baseline as\nwell as to the results reported by Suk et al. [48].\nMethod\nMean approx. error [%]\nGATr (default)\n6.1\nWithout multivector representations (= Transformer)\n10.5\nWithout aux. scalar representations\n10.4\nWithout equivariance constraints on linear maps\n9.7\nLess wide (4 MV channels + 8 scalar channels)\n12.0\nLess deep (5 blocks)\n7.8\nTable 3: Ablation experiments on the arterial wall-shear-stress dataset. We report the mean approximation error\n(lower is better) on the validation set after training for 100 000 steps.\nTraining\nWe train the models on a L2 loss on wall shear stress. We train for 200 000 steps with the\nAdam optimizer, using a batch size of 8 and exponentially decaying the learning rate from 3 \u00b7 10\u22124 to\n3 \u00b7 10\u22126.\nTo fit the models into memory, we use mixed 16-bit / 32-bit precision, gradient checkpointing [9], and\nmulti-query attention. Most importantly, we compute dot-product attention with the implementation\nby [32].\nResults\nThe results of our experiments are shown in Fig. 3 and in Tbl. 2.\nAblation studies\nWe also perform ablation experiments to study the impact of multiple design de-\ncisions on the performance. All major design choices appear to play an important role: without multi-\nvector representations, without auxiliary scalar representations, or without equivariance constraints,\nGATr performs much worse. Further reducing the model\u2019s width also severaly harms the performance.\nC.3\nRobotic planning through invariant diffusion\nEnvironment\nWe use the block stacking environment from Janner et al. [27]. It consists of a Kuka\nrobotic arm interacting with four blocks on a table, simulated with PyBullet [13]. The state consists\nof seven robotic joint angles as well as the positions and orientations of the four blocks. We consider\nthe task of stacking four blocks on top of each other in any order. The reward is the stacking success\nprobability and is normalized such that 0 means that no blocks are ever successfully stacked, while\n100 denotes perfect block stacking.\nDataset and data parameterization\nWe train models on the offline trajectory dataset published by\nJanner et al. [27]. It consists of 11 000 expert demonstrations.\nTo facilitate a geometric interpretation, we re-parameterize the environment state into the positions\nand orientations of the robotic endeffector as well as the four blocks. The orientations of all objects\nare given by two direction vectors. In addition, there are attachment variables that characterize\nwhether the endeffector is in contact with either of the four blocks. In this parameterization, the\nenvironment state is 49-dimensional.\nWe train models in this geometric parameterization of the problem. To map back to the original\nparameterization in terms of joint angles, we use a simple inverse kinematics model that solves for\nthe joint angles consistent with a given endeffector pose.\n23\nMethod\nReward\nGATr-Diffuser (ours)\n74.8 \u00b1 1.7\nTransformer-Diffuser\n69.8 \u00b1 1.9\nDiffuser [27] (reproduced)\n57.7 \u00b1 1.8\nDiffuser [27]\n58.7 \u00b1 2.5\nEDGI [7]\n62.0 \u00b1 2.1\nCQL [30]\n24.4\nBCQ [21]\n0.0\nTable 4: Diffusion-based robotic planning. We show the normalized cumulative rewards achieved on a robotic\nblock stacking task [27], where 100 is optimal and means that each block stacking task is completed successfully,\nwhile 0 corresponds to a failure to stack any blocks. We show the mean and standard error over 200 evaluation\nepisodes. The top three results were computed in the GATr code base, the bottom four taken from the literature [7,\n27].\nModels\nFor the GATr model we embed object positions as trivectors, object orientations as oriented\nplanes, gripper attachment variables as scalars, and the diffusion time as scalars. We use axial\nattention, alternating between attending over time steps and over objects, with positional embeddings\nalong the time axis. Neither multi-query attention nor the distance-aware attention mechanism are\nused. We train three models, with 10, 20, and 30 Transformer blocks. In each case we use 16\nmultivector plus 32 scalar channels and 8 attention heads. We report results for the best-performing\nmodel with 20 Transformer blocks and 4.0 million parameters.\nFor the Transformer baseline, we also use axial attention, pre-layer normalization, GELU activations,\nand rotary positional embeddings. We experiment with 6 models, with 10, 20, or 30 Transformer\nblocks and 144 or 384 channels. All use 8 attention heads. We report results for the best model,\nwhich has 20 Transformer blocks, 144 channels, and 3.5 million parameters.\nFor the Diffuser baseline, we follow the architecture and hyperparameters described by Janner et al.\n[27]. The model has 65.1 million parameters.\nAll models are embedded in a diffusion pipeline as described by Ho et al. [25], using the diffusion\ntime embedding and hyperparameter choices of Ref. [27]. In particular, we use univariate Gaussian\nbase densities and 1000 diffusion steps.\nTraining\nWe train all models by minimizing the simplified diffusion loss proposed by Ho et al.\n[25]. For our GATr models and the Diffuser baselines we use an L2 loss and train for 200 000 steps\nwith the Adam optimizer, exponentially decaying the learning rate from 3 \u00b7 10\u22124 to 3 \u00b7 10\u22126. This\nsetup did not work well for the Diffuser model, where (following Janner et al. [27]) we use a L1 loss\nand a low constant learning rate instead.\nEvaluation\nAll models are evaluated by rolling out at least 200 episodes in a block stacking\nenvironment and reporting the mean task and the standard error. We use the planning algorithm\nand parameter choices of Janner et al. [27] (we do not optimize these, as our focus in this work is\non architectural improvements). It consists of sampling trajectories of length 128 from the model,\nconditional on the current state; then executing these in the environment using PyBullet\u2019s PID\ncontroller. Each rollout consists of three such phases.\nResults\nThe results of our experiments are shown in Fig. 4 and in Tbl. 4.\nC.4\nScaling\nSetup\nTo study computational requirements and scaling behaviour, we measure the wall time and\npeak GPU memory usage during a combined forward and backward pass for various models. We use\nrandom Gaussian input data with a batch size of 4, a variable number of items (tokens), and four input\nchannels. After a number of warmup steps, measure the resource use over ten successive steps and\nreport the average time and peak memory usage. For all measurements, we use mixed 16-bit / 32-bit\nprecision, gradient checkpointing, and an efficient attention implementation by Lefaudeux et al. [32].\n24\nModels\nThe GATr model has 10 blocks, 8 multivector and 16 scalar channels, and 4 attention heads.\nWe use multi-query attention, the distance-aware attention mechanism, and learnable prefactors for\nthe attention weights.\nFor the baselines, we choose hyperparameters to match GATr in terms of depth (number of blocks),\nwidth (total size of hidden channels), and token connectivity (allowing all tokens to interact with each\nother in each layer). For the Transformer, this leads to the same choices as for GATr, except that 144\nhidden channels are used.\nFor SEGNN and the SE(3)-Transformer, we use fully connected graphs, O(3) representations\n\u2113 \u2264 1, a total of 144 hidden features distributed between scalar and vector representations, and 10\nlayers. We use the official implementation by Brandstetter et al. [6] for SEGNN and the optimized\nimplementation by Milesi [35] for the SE(3)-Transformer.\n25\n"
  },
  {
    "title": "LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images",
    "link": "https://arxiv.org/pdf/2305.19164.pdf",
    "upvote": "2",
    "text": "LANCE: Stress-testing Visual Models by\nGenerating Language-guided Counterfactual Images\nViraj Prabhu\nSriram Yenamandra\nPrithvijit Chattopadhyay\nJudy Hoffman\nGeorgia Institute of Technology\n{virajp,sriramy,prithvijit3,judy}@gatech.edu\nAbstract\nWe propose an automated algorithm to stress-test a trained visual model by generat-\ning language-guided counterfactual test images (LANCE). Our method leverages\nrecent progress in large language modeling and text-based image editing to aug-\nment an IID test set with a suite of diverse, realistic, and challenging test images\nwithout altering model weights. We benchmark the performance of a diverse set\nof pretrained models on our generated data and observe significant and consistent\nperformance drops. We further analyze model sensitivity across different types of\nedits, and demonstrate its applicability at surfacing previously unknown class-level\nmodel biases in ImageNet. Code: https://github.com/virajprabhu/lance.\n1\nIntroduction\nTest image \n(Label: Dog Sled)\nGenerating Language-guided Counterfactuals (LANCE)\nResNet-50\nHusky Golden R.\n\u2192\nHusky Labradoodle\n\u2192\nHusky Pomeranian\n\u2192\n\u201cHuskies pulling a dog sled\u201d\nEskimo Dog\nDog Sled\nSiberian Husky\n27%\n10%\nGolden Ret.\nDog Sled\nEskimo Dog\n3%\n2%\nIrish terrier\nIrish wolfhound\nIbizan hound\n10%\n6%\nKeeshond\nPomeranian\nChihuahua\n0.12%\n0.11%\n53%\n82%\n99.7%\n64%\nFigure 1: The predominant paradigm in computer vision is to benchmark trained models on IID test\nsets using aggregate metrics such as accuracy, which does not adequately vet models for deployment,\ne.g.for the test image above, while a trained ResNet-50 model [1] accurately predicts the ground truth\nlabel (\u201cdog sled\u201d), it is in truth highly sensitive to the dog breed. We propose LANCE, an automated\nmethod to surface model vulnerabilities across a diverse range of interventions by generating such\ncounterfactual images using language guidance.\nAs deep visual models become ubiquitous in high-stakes applications, robust stress-testing assumes\nparamount importance. However, the traditional paradigm of evaluating performance on large-\nscale [2\u20134] IID test sets does not adequately vet models for deployment in the wild. First, such\nmodels are typically evaluated via aggregate metrics such as accuracy, IoU, or average precision [5],\nwhich treat all test samples equivalently and do not distinguish between error types. Further, such\ntest sets do not adequately capture the \u201clong-tail\u201d [6] of the data distribution: rare concepts, unseen\nconcepts, and the (combinatorial explosion) of their compositions [7].\nTo address this, a considerable number of recent efforts have sought to develop realistic benchmarks\nfor out-of-distribution (OOD) evaluation [8\u201313]. However, while useful, such OOD benchmarks are\ntypically static across models and time, rather than being curated to probe a specific instance of a\ntrained model, which diminishes their utility.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.19164v2  [cs.CV]  27 Oct 2023\nIn this work, we eschew the traditional paradigm of static test sets and instead generate dynamic test\nsuites. Specifically, we propose a technique that automatically generates a test suite of challenging\nbut realistic counterfactual [14] examples to stress-test a given visual model. Our main insight is\nthat while the scope of possible visual variation is vast, language can serve as a concise intermediate\nscaffold that captures salient visual features while abstracting away irrelevant detail. Further, while it\nis extremely challenging to run counterfactual queries on images directly, intervening on a discrete\ntext representation is considerably easier. We call counterfactuals generated by our method Language-\nguided Counterfactual Images (LANCE).\nOur method leverages pretrained foundation models for text-to-image synthesis [15\u201317] and large\nlanguage modeling (LLM) [18], in combination with recent progress in text-based image editing [19,\n20], to generate realistic counterfactual examples. It operates as follows: Given a test image and\nits ground truth label, we generate a caption conditioned on its label using an image captioning\nmodel [21]. We then use an LLM that has been fine-tuned for text editing [22] to generate realistic\nperturbations to a single concept at a time \u2013 caption subject, object, background, adjectives, or image\ndomain (while leaving words corresponding to the ground truth category unchanged). Finally, we\nuse a guided-diffusion model [19] with null-text inversion [20] to generate an edited counterfactual\nimage conditioned on the original image and the perturbed prompt. We obtain a prediction from the\ntrained model on both the original image and the generated counterfactual and compute the average\ndrop in accuracy as a measure of the model\u2019s sensitivity to the given attribute.\nWe benchmark the performance of diverse models pretrained on ImageNet on our generated images,\nand find that performance drops more consistently and significantly than baseline approaches. Next,\nwe demonstrate applications of our approach: in comparing the relative sensitivity of models to\nperturbations of different types, and in deriving actionable insights by isolating consistent class-level\ninterventions that change model predictions.\n2\nRelated Work\nDiagnosing deep visual models. Several recent works have focused on the discovery of systematic\nfailure modes in trained neural networks. Early works propose human-in-the-loop approaches based\non annotating spurious features learned by sparse linear layers [23], adversarially robust models [24],\nand discovering a hyperplane corresponding to potentially biased attribute [25]. Some prior work\nalso proposes fully automated techniques: by learning decision trees over misclassified instance\nfeatures from an adversarially robust model [26] or targeted data collection [27]. Several recent\napproaches to this problem leverage multi-modal CLIP [28] embeddings, via error-aware mixture\nmodeling [29], identifying failure directions in latent space [30] or learning the task directly on top\nof CLIP embeddings [31]. However, these works rationalize failures but don\u2019t close the loop by\nevaluating the predicted rationale. Further, shortcut learning [32] of spurious correlations can also\nlead to success but for the wrong reasons, but prior work only studies failure cases. In contrast, we\npropose a method that generates visual counterfactuals that can surface a diverse range of model\nbiases that bidirectionally influence performance.\nImage Editing with generative models. Using generative models to edit images has seen consid-\nerable work. Early efforts focused on specialized editing such as style transfer [33] or translating\nfrom one domain to another [34]. More recent work has performed editing in latent space of a model\nlike StyleGAN [35\u201339]. Recently, pretrained text-to-image diffusion models have become the tool\nof choice for image editing [19, 20, 17]. While superior to GAN\u2019s at image synthesis [40], targeted\nediting using such models that modifies only a specific visual attribute while keeping the rest un-\nchanged is non-trivial. Recent work has presented techniques based on prompt-to-prompt tuning [19],\nby targeted insertion of attention maps from cross-attention layers during the diffusion process.\nFollow-up work generalizes this approach to real images, with additional null-text inversion [20] or\ndelta denoising scoring functions [41], which enables instruction-based editing of images [42] and\n3D scenes [43]. We leverage this technique of prompt-to-prompt tuning with null-text inversion but\nrather than generic image editing focus on generating challenging counterfactual examples.\nProbing discriminative networks with generated counterfactuals. Several works have attempted\nto use generative models to obtain explanations from visual models. A popular line of work generates\nminimal image perturbations that flip a model\u2019s prediction [44\u201347] (e.g.changing lip curvature to alter\nthe prediction of a \u201csmiling\u201d classifier). Our work shares a similar goal but with a crucial distinction:\n2\nModel\nModel\n\u201chowler \nmonkey\u201d\n(98% confidence)\n\u201cgorilla\u201d\n(75% confidence)\nTrained Model, Test Set\na black monkey \nsitting on top of \na tree with \ngreen leaves\na black monkey \nsitting on top of \na tree with \norange leaves\ninvert\nFrozen\nLearned\n\u0394p y!\" x \u2192 x\u2032 = |p y!\" x \u2212 p y!\" x# |\nEditor\n(DDPM)\nPerturber\n(LLM)\nCaptioner\nLANCE: Language-guided Counterfactual Image Generation\nFigure 2: Overview. We propose a method to generate challenging counterfactual examples to\nstress-test a given visual model. Given a trained model and test set, LANCE generates a textual\ndescription (from a captioning model) and perturbed caption (using a large language model (LLM)),\nwhich is fed along with the original image to a text-to-image denoising diffusion probabilistic model\n(DDPM) to perform counterfactual editing. The process is repeated for multiple perturbations to\ngenerate a challenging test set. Finally, we ascertain model sensitivity to different factors of variation\nby reporting the change in the model accuracy over the corresponding counterfactual test set.\nwe seek to discover minimal perturbations to parts of the image not directly relevant to the task at\nhand (e.g.does changing hair color or perceived gender alter the smiling classifier\u2019s prediction?).\nClosest to our work is Luo et al. [48], which determines model sensitivity to a set of user-defined text\nattributes by tuning a weighted combination of edit vectors in StyleGAN [35] latent space so as to flip\nthe model prediction while maintaining global structure and semantics via additional losses. However,\nthis method requires the user to enumerate all possible attributes of interest, may not generalize to\nmore complex datasets, and requires optimizing several losses in conjunction. Our work also shares\nsimilar motivations as Li et al. [49], which uses diffusion models to generate ImageNet-E(diting), a\nrobustness benchmark with varying backgrounds, object sizes, position, and directions. However,\ntheir benchmark is static across models, requires object masks per image to generate, and measures\nrobustness to a constrained set of attribute changes. Our work is also related to Wiles et al. [50] which\ngenerates cluster-based error rationales, Vendrow et al. [51], which diagnoses failures by generating\nnear-counterfactuals from human-generated text variations, and Dunlap et al. [52], which performs\ndiffusion-based data augmentation. In contrast, we study bias discovery rather than mitigation,\nderiving causal insights into failures using a targeted editing algorithm, and support stress-testing\nacross an unconstrained, automatically discovered set of attributes.\n3\nLanguage-guided Counterfactual Image Generation (LANCE)\nOverview. We introduce LANCE, our algorithm for generating language-guided counterfactual\nimages to stress-test a given visual model. Our key insight is to use language as a structured discrete\nscaffold to perform interventions on the model. LANCE does this by perturbing a text representation\n(caption) of an image to conditionally generate a counterfactual image (see Fig 2).\nPerturbed Captions. We first use a pre-trained captioner (BLIP-2 [21]), to produce a text description\nfor a given test image. We use beam search decoding with a minimum caption length of 20 words\nand a high repetition penalty to encourage descriptive captions with low redundancy (see Fig. 5\nfor examples). Our algorithm then edits the generated caption using a language model to generate\nvariations that only change a single aspect at a time.\nEdited Images. Next, we use a text-to-image latent diffusion model to generate a new image that\nreflects the text edit while remaining faithful to the original image in every other respect. We repeat\nthis process for multiple perturbations to generate a challenging test set.\nSensitivity Analysis. Finally, we ascertain model sensitivity to different factors of variation by\nreporting the change in the model\u2019s accuracy over the corresponding counterfactual test set.\nNext, we detail each step, beginning with describing the factors of visual variation that we stress-test\nagainst and the strategy we use to train a structured caption perturbed. Then, we describe our image\n3\nTable 1: Perturbation dataset statistics. We first programmatically collect a small dataset (0.6k\nsamples) of 5 types of targeted caption perturbations for random MSCOCO [4] captions from GPT-3.5\nturbo [22] (top). We then finetune LLAMA-7B [53] on this dataset to perform targeted perturbation,\nand use it to perturb image captions generated for ImageNet [2] (bottom).\nGenerator\nSample Input\nEdit Type (Count)\nSample Output\nGPT-3.5 turbo\n(0.6k samples)\nA bicycle replica\nwith a clock as\nthe front wheel\nSubject (0.1k)\nbicycle\u2192{scooter, unicycle, ..., shopping cart}\nObject(0.1k)\nclock\u2192{basketball, pizza, ..., globe}\nAdjective (0.1k)\nbicycle\u2192{vintage, steampunk, ..., rusty} bicycle\nDomain (0.1k)\na {painting, sculpture, ..., sketch} of a ...\nBackground (0.2k)\n...\n{on a stormy day, ..., parked outside a coffee shop}\nFinetuned LLAMA-7B\n(35k samples)\na hockey puck\nsitting on top\nof a wooden table\nSubject (6.2k)\nhockey puck\u2192{basketball, ..., lacrosse ball}\nObject (4.8k)\nwooden table\u2192{marble table, ..., stone wall}\nAdjective (7.1k)\nhockey puck\u2192 wooden, large, rustic hockey puck\nDomain (3.4k)\na {painting, ..., 3D model} of a hockey puck...\nBackground (14.1k)\n...\n{in a busy hockey arena, ..., during a thunderstorm}\nediting methodology and the various checks and balances we insert to ensure that our end-to-end\napproach yields challenging yet realistic test examples.\n3.1\nTraining a Structured Caption Perturber\nTo perform meaningful interventions, we train a structured caption perturber that generates targeted\ncaption edits. Motivated by prior work in finetuning instruction-following large language mod-\nels [22](LLMs), we seek to train a structured perturber LLM that can generate diverse and realistic\ncaption edits that capture the long-tail of visual variation while still representing realistic scenarios.\nSelecting factors of variation. To train our perturber model, we select five factors of visual variation\nagainst which we will stress-test. While the space of potential factors of visual variation is vast,\nimage captions can constrain this search space by capturing salient visual concepts while abstracting\naway irrelevant detail. Concretely, we measure resilience to the following factors of variation:\n\u2022 Subject. Modifications to the subject of an image caption (e.g., a {man, dog, cat, ..., horse,})\ncan stress-test a model\u2019s ability to recognize the ground truth category when it co-occurs\nwith other subjects, including subjects that co-occur rarely (or never) with the ground truth\nin the training data.\n\u2022 Object. Similarly, modifications to the object of an image caption (e.g., a {table, chair, ..., bed})\ncan stress-test a model\u2019s resilience to novel or unseen co-occurring concepts.\n\u2022 Background. Modifications to the context of an image caption (e.g., a {kitchen, bedroom, ...,\nliving room}) can stress-test a model\u2019s ability to generalize to different scenes, including\ndiverse backgrounds and weather conditions.\n\u2022 Adjective. Modifications to the adjective of an image caption (e.g., a {red, blue, ..., green}) can\nstress-test a model\u2019s ability to generalize to visual variations captured by object attributes.\n\u2022 Data domain. Modifications to the data domain of an image caption (e.g., a {painting, sketch,\n..., sculpture}) can stress-test a model\u2019s ability to generalize to different data distributions.\nWhile by no means exhaustive, these perturbation types capture a large and representative set of\nvisual variations. Further, we note that our approach is agnostic to this choice and can easily be\nextended to accommodate additional factors (e.g., camera angle, lighting, etc.). Having defined our\nperturbation set, we proceed to collect a dataset to train our structured perturbation model.\nDataset collection. We use GPT-3.5 turbo [18] to programmatically collect a small dataset of 0.6k\ncaption perturbations spanning the aforementioned types. For instance, to edit an \u201cadjective\u201d, we\nprompt the model with: Generate all possible variations of the provided sentence by only adding\nor altering a single adjective or attribute. (all prompts in appendix). We find that even with zero-\nshot prompting, we are able to acquire caption perturbations that modify only the desired factor of\nvariation. We follow this procedure to generate perturbations for randomly selected captions from\nthe MSCOCO dataset [4]. Table. 1 (top) highlights example inputs and perturbations. As seen, the\ndataset contains both simple edits (e.g., a bicycle \u2192{red, blue, ..., green} bicycle) as well as more\ncomplex ones (e.g., a bicycle \u2192bicycle parked outside a coffee shop), without requiring any human\nsupervision whatsoever.\n4\nModel training. Next, we finetune an LLM on the dataset described above to perform targeted\ncaption editing: specifically, we use a LLAMA-7B [53] LLM and perform LoRA (Low-Rank\nAdaptation) finetuning [54]: the original LLAMA-7B model is kept frozen, while the change in\nweight matrices W \u2208 Rd\u00d7k of the self-attention modules post-adaptation is restricted to have a\nlow-rank decomposition Wft = Wpt + \u2206W = Wpt + AB, where A \u2208 Rd\u00d7r and B \u2208 Rr\u00d7k. Here\nthe rank r is kept low, resulting in very few trainable parameters in A and B.\nWe use this finetuned model to perturb generated captions from ImageNet [2]. Table. 1 (top) shows\nexamples of perturbations generated by this model. As seen, it is able to generate sensible and diverse\nedits corresponding to the edit type without changing the remainder of the caption.\nCaption editing: Checks and balances. An undesirable type of edit would modify a word corre-\nsponding to the ground truth category label of the image. For instance, if the ground truth label of\nan image is \u2018dog\u2019, we do not want the model to generate a perturbation that changes the word \u2018dog\u2019\nto \u2018cat\u2019. To address this, we simply filter our caption edits wherein the edit is semantically similar\n(measured using a sentence BERT model [55]) to the ground truth category. Similarly, we also filter\nour captions where the edit is semantically similar to the original word or phrase being modified.\n3.2\nCounterfactual Image Generation\nHaving generated image captions and their perturbations, we proceed to generate counterfactual\nimages conditioned on the original image and edited caption using a text-to-image latent diffusion\nmodel, specifically Stable Diffusion [16]. However, despite its remarkable ability at generating high-\nquality text-conditioned images, targeted editing using such models has historically been challenging,\nas changing even a single word in the text prompt could dramatically change the generated output\nimage. Naturally, this is undesirable for our task, as we want to generate counterfactual images that\nare as similar as possible to the original image, while still reflecting the caption edit.\nTo address this, we leverage the recently proposed prompt-to-prompt [19] 1 image editing technique,\nwhich performs targeted injection of cross-attention maps that correspond to the caption edit for a\nsubset of the denoising diffusion process. However, prompt-to-prompt is designed for generated\nimages, and applying it to real images requires accurate image inversion to latent space.\nWe employ the recent null-text inversion technique proposed by Mokady et al. [20]. Concretely, for\ninput image x with caption c, we perform DDIM inversion by setting the initial latent vector z0 to the\nencoding of the image to be inverted x, and run the diffusion process in the reverse direction [40]\nfor K timesteps z0 \u2192 zK. Further, latent diffusion models employ classifier-free [59] guidance\nfor text-conditioned generation, wherein the diffusion process is run twice, with text conditioning\nand unconditionally using a null-text token. To encourage accurate reconstruction of the original\nimage, we follow Mokady et al. [20] to use the initial noisy diffusion trajectory (generated with a\nsmall guidance scale) as a pivot and update \u2205k, the null-text embedding at timestep k, to minimize a\nreconstruction mean square error between the predicted latent code \u02c6zk and the pivot zk, using the\ndefault large guidance scale recommended for classifier-free guidance. This helps bring the backward\ndiffusion trajectory close to the original image encoding z0, and thus achieve faithful reconstruction.\nLet Sk\u22121(\u02c6zk, \u2205k, c) denote one step of deterministic DDIM sampling [60]. We optimize:\nmin\n\u2205k \u2225zk\u22121 \u2212 Sk\u22121 (\u02c6zk, \u2205k, c)\u22252\n2\n(1)\nImage editing: Checks and balances. We find that image editing using prompt-to-prompt with\nnull-text inversion is highly sensitive to a specific hyperparameter which controls the fraction of\ndiffusion steps for which self-attention maps for the original image are injected. Let f denote this\nfraction. The optimal value of f varies according to the degree of change, with larger changes (say\nediting the background or weather) requiring a small value. In service of making our algorithm fully\nautomated, we follow prior work [43] to automatically tune this hyperparameter: we sweep over a\nrange of values and threshold based on a CLIP [28] directional similarity metric [61], which measures\nthe consistency in the change across images and captions in embedding space. Let EI and ET denote\nthe CLIP image and text encoders. The CLIP directional similarity criterion \u03d5(.) is given by:\n\u03d5(x, x\u2032, c, c\u2032) = 1 \u2212 (EI(x) \u2212 EI(x\u2032)) \u00b7 (ET (c) \u2212 ET (c\u2032))\n|EI(x) \u2212 EI(x\u2032)| \u00b7 |ET (c) \u2212 ET (c\u2032)|\n(2)\n1Several editing techniques have been proposed recently [56, 57, 41, 58], which may be equally suitable.\n5\nAlgorithm 1 Generating Language-guided Counterfactual Images\n1: Input: Test set T={(image x, label y)}M\n1 , model M, image captioner C, perturber LLM P,\nlatent diffusion model L, maximum perturbations per image N, edit similarity threshold \u03f5, image\nquality threshold \u03c4\n2: Output: Counterfactual test set T\u2032\n3: T\u2032\u2190 \u2205\n\u25b7 Initialize counterfactual test set\n4: P \u2190 {Subject, Object, Background, Domain, Adjective}\n\u25b7 Define perturbation types\n5: for (x, y) \u2208T do\n6:\nx\u22121\u2190 L(x, c)\n\u25b7 Invert image to latent space using diffusion model\n7:\nc \u2190 C(x)\n\u25b7 Generate caption for input image\n8:\nfor i \u2208 1, .., N do\n9:\np \u223c P\n\u25b7 Sample perturbation type\n10:\nc\u2032 \u2190 P(c|p)\n\u25b7 Perform text edit\n11:\nif sim(c\u2032, y) < \u03f5 then\n\u25b7 Ensure ground truth is unchanged\n12:\nx\u2032\u2190 L(c\u2032| x\u22121)\n\u25b7 Generate counterfactual image, Sec. 3.2\n13:\nif \u03d5(x, x\u2032, c, c\u2032) > \u03c4 then\n\u25b7 Ensure image quality, Eq. 2\n14:\nT\u2032\u2190 T\u2032\u222a(x\u2032, y)\n\u25b7 Add to test set\n15: return T\u2032\nFinally, we ensure that the generated image is more similar to the edited rather than the original\ncaption in CLIP\u2019s [28] embedding space. Algo. 1 details our full approach.\n4\nExperiments\nIn Section 4.1, we overview our experimental setup, describing the data, metrics, baselines, and\nimplementation details used. Next, we present our results (Section 4.2), comparing the performance\nof a diverse set of pretrained models on the subset of the ImageNet test set, and on our generated\ncounterfactual test sets. Additionally, we analyze the sensitivity of models to different types of edits\nand demonstrate the applicability of our method in deriving class-level insights into model bias.\n4.1\nExperimental Setup\nDataset. We evaluate LANCE on a subset of the ImageNet [2] validation set. Specifically, we study\nthe 15 classes included in the Hard ImageNet benchmark [62]. Models trained on ImageNet have\nbeen shown to rely heavily on spurious features (e.g. context) to make predictions for these classes,\nwhich makes it an ideal testbed for our approach. We consider the original ImageNet validation sets\nfor these 15 classes, with 50 images/class, as our base set. Further, testing our approach on ImageNet\nhas a few other advantages: i) The dataset contains naturally occurring spurious correlations rather\nthan manually generated ones (e.g. by introducing a dataset imbalance). ii) Testing our method on\nImageNet images rather than constrained settings used in prior work e.g. celebrity faces in CelebA\nallows us to validate its effectiveness in more practical settings. iii) Finally, using ImageNet allows\nus to stress-test a wide range of pretrained models.\nMetrics. We report the model\u2019s accuracy@k (k \u2208 {1, 5}) over the original test set T and generated\ncounterfactual test set T\u2032. For model M, we are interested in understanding the drop in accuracy@k\nover the counterfactual test set, compared to the original test set. We define this metric as follows:\n\u2206acc@k =\nh 1\n|T\u2032|\nX\n(x\u2032,y)\u2208T\u2032\nacc@k(M(x\u2032), y)\ni\n\u2212\nh 1\n|T|\nX\n(x,y)\u2208T\nacc@k(M(x), y)\ni\n(3)\nHowever, it is possible for an intervention to alter model confidence without leading to a change in its\nfinal prediction. As a more fine-grained measure, we also report the absolute difference in predicted\nmodel confidence for the ground truth class over the original and counterfactual images. For a single\ninstance, we define this as: \u2206p(yGT |x) = |p(yGT |x) \u2212 p(yGT |x\u2032)|\nFinally, to evaluate realism we also report the FID [63] score of our generated image test sets and\nperplexity of the generated and perturbed captions under LLAMA-7B [53].\n6\nTable 2: Results. We evaluate diverse models pretrained on ImageNet-1K on four test sets: the\noriginal Hard ImageNet test set (reference), its reconstruction generated by our method (control),\nand the counterfactual test sets generated by a random perturbation baseline (Ours-Baseline), and\nour proposed structured perturbation strategy (Ours). Our methods generate progressively more\nchallenging test sets as evidenced by the consistent performance drop compared to the original test\nset (\u2206acc@k, subscript).\nTest Set\nType\nacc@1 / acc@5 (\u2191)\nResNet-50 [1]\nViT-B [67]\nConvNext [68]\nCLIP [28]\n1 Original\nReference\n79.86 / 95.83\n85.42 / 97.74\n85.42 / 98.26\n56.25 / 86.98\n2 Reconstructed Control\n79.86+0.0 / 96.18+0.4 85.42+0.0 / 97.57\u22120.2 85.24\u22120.2 / 98.09\u22120.2 55.90\u22120.4 / 87.67+0.7\n3 LANCE-R\nOurs - Baseline 78.81\u22121.0 / 92.25\u22123.6 81.91\u22123.5 / 95.61\u22122.1 85.79+0.4 / 94.83\u22123.4 50.13\u22126.1 / 83.46\u22123.5\n4 LANCE\nOurs\n74.01\u22125.8 / 92.96\u22122.9 79.00\u22126.4 / 94.24\u22123.5 81.18\u22124.2 / 94.62\u22123.6 48.27\u22128.0 / 84.38\u22122.6\nSubject\nObject\nDomain\nBackground\nAdjective\nPerturbation type\n0\n10\n20\n30\n40\n50\n% p(yGT|x) (\n)\nresnet50\nvit_b_16\nclip\nconvnext_base\nFigure 3: Sensitivity across perturbation types as\nmeasured by average \u2206p(yGT |x).\nMethod\nFID(\u2193)\nPerplexity(\u2193)\nOriginal\n0\n17.1\nReconstructed\n13.6\n17.5\nLANCE-R\n51.6\n18.3\nLANCE\n55.4\n18.6\nFigure 4: Evaluating quality of generated images\n(FID) and captions (perplexity).\nImplementation Details. We use BLIP-2 (for image captioning), LLAMA-7B [53] (for structured\ncaption perturation) with LoRA finetuning [54], StableDiffusion [16] (for text-to-image generation),\nand pretrained models from TIMM [64]. We use PyTorch [65] for all experiments. For finetuning\nLLaMA-7B, we concatenate the query, key, and value matrices per layer and approximate the updates\nto the resulting weight matrix using a rank-8 decomposition. We finetune the model for 37.5k steps\nupdating the weights every 32 steps. For more details see appendix.\nBaselines. In the absence of prior empirical work for our experimental setting, we compare the\nperformance of LANCE to the following:\ni) Original (Reference): The original, unmodified ImageNet test set, as a point of reference.\ni) Reconstructed (Control): The reconstruction of the original ImageNet test set, as a control set\niii) LANCE-R (Baseline, Ours): As a baseline, we design a simple random caption perturbation\nstrategy that randomly masks out a word in the original caption and replaces it with a different word\nusing a masked language model [66]. To ensure meaningful perturbations, we impose two additional\nconstraints: the word being modified should not be a stop word (to minimize wasteful edits e.g.\nchanging \u2018a\u2019 to \u2018the\u2019), and that the CLIP [28] similarity between the generated image and the new\nword should exceed its similarity to the original word.\n4.2\nResults\nAcross pretrained models, LANCE generates more challenging counterfactuals than baselines.\nOur goal is to discover a set of example images that are especially challenging for a given model.\nTherefore, Table 2 reports top-1 and top-5 performance of multiple fixed models across two baseline\ntest sets and our own (LANCE). We find that every model studied has significantly lower performance\non our proposed generated test set. We compare this both to the standard ImageNet test set as well\nas a baseline variant of our method that uses random caption perturbations (LANCE-R). Finally,\nwe verify that performance loss is not due to the image generation process by demonstrating that\nperformance on a reconstructed test set is nearly identical to the original test set.\nWe provide example LANCE perturbations and generated images in Figure 5. You can find an\nexample image for each perturbation type. In some cases, such as the domain edit (changing from\n7\na photo of a \nhowler monkey sitting \non top of a tree trunk \nnext to a green forest \nfilled with trees\na sculpture of a \nhowler monkey sitting \non top of a tree trunk \nnext to a green forest \nfilled with trees\nPredictions\nhowler monkey (55.65%)\ncapuchin (13.85%)\nspider monkey (11.29%)\nsiamang (7.91%)\ntiti (3.94%)\nPredictions\nchimpanzee (76.83%)\ngorilla (8.05%)\nhowler monkey (6.58%)\nsiamang (5.32%)\nbeaver (0.67%)\na photo of a \nCouple of howler \nmonkey sitting on top \nof a tree next to each \nother in a forest\na photo of a \nCouple of howler \nmonkey sitting on top \nof a tree next to each \nother in a \nbamboo forest\nPredictions\nhowler monkey (55.65%)\ncapuchin (13.85%)\nspider monkey (11.29%)\nsiamang (7.91%)\ntiti (3.94%)\nPredictions\nchimpanzee (76.83%)\ngorilla (8.05%)\nhowler monkey (6.58%)\nsiamang (5.32%)\nbeaver (0.67%)\na photo of a group \nof husky dogs pulling \na dog sled across a \nsnow covered field \nwith people in the \nbackground\na photo of a group \nof schnauzer pulling a \ndog sled across a snow \ncovered field with \npeople in the \nbackground\nPredictions:\ndogsled (53.04%)\nEskimo dog (27.14%)\nSiberian husky (10.17%)\nmalamute (9.37%)\nelkhound (0.05%)\nPredictions:\nstandard schnauzer (80.38%)\ngiant schnauzer (27.14%)\nmini schnauzer (6.14%)\nTibetan terrier (0.45%)\nEnglish sheepdog (0.26%)\na photo of a young child \nin a swimming suit and \na silver swimming cap \nleans over the edge of a \nswimming pool\na photo of a young child \nin a swimming suit and \na yellow swimming cap \nleans over the edge of a \nswimming pool\nPredictions:\nbathing cap (94.71%)\nswimming trunks (2.02%)\ntub (0.82%)\nmaillot (0.46%)\nmaillot (0.21%)\nPredictions:\nbathing cap (36.00%)\ntub (18.81%)\nkiller whale (7.23%)\nswimming trunks (5.81%)\ngreat white shark (5.47%)\nEdit subject\nEdit object\na photo of a young \nboy hanging from a \nmetal bar on a snowy \nplayground\na photo of a young \nboy hanging from a \nmetal bar on \na playground\nPredictions:\nski (23.24%)\npole (15.26%)\nhorizontal bar (14.13%)\nparallel bars (13.67%)\nshovel (9.95%)\nPredictions:\nhorizontal bar (68.15%)\nparallel bars (31.84%)\nswing (0.01%)\nbalance beam (0.00%)\npole (0.00%)\nEdit background\nEdit domain\nEdit adjective\nFigure 5: Visualizing counterfactual images. We visualize the counterfactual images generated by\nLANCE for images from the HardImageNet dataset. Each row corresponds to a specific edit type.\nAbove each image, we display its generated caption, highlighting the original and edited word(s).\nBelow each image, we display the top-5 classes predicted by a ResNet-50 model and the associated\nmodel confidence, coloring the ground truth class in green.\na photo to a sculpture) significant predictive changes result \u2013 from correctly identifying a howler\nmonkey to incorrectly predicting a chimpanzee. As another example, the subject edit (changing the\ndogs from husky to schnauzer) prohibits the model from accurately predicting dogsled, conceivably\nbecause few schnauzers appeared pulling a sled in the training data.\nPerturbation sensitivity varies by perturbation type and model. Now that we have confirmed that\nour generated test set provides challenging examples to assess our models, we may further analyze\nthe impact of different types of perturbations. Figure 3 reports the relative change in predictive\nperformance in response to different perturbation types. We find that changes to the background have\nthe highest impact, followed by domain. Further, some models are more sensitive to LANCE images,\nwith ResNet-50 [1] having the most sensitivity.\nLANCE can be used to derive class-level insights into model bias. To do so, we compute the L1\ndistance between CLIP features for the original and edited words, and run K-Means clustering. We\nthen visualize the clusters with the highest \u2206p(yGT |x), corresponding to high sensitivity to a given\nchange. Figure 6 illustrates examples. As seen, our method can also be used to surface class-level\nfailure modes to inform downstream mitigation strategies. Importantly, we stress that while some\nfailure modes are corroborated by other diagnostic datasets [62], (e.g.using color-based context-cues\nto predict \u201chowler monkey\u201d), others go beyond the capabilities of conventional IID test set-based\ndiagnosis (e.g.models relying on dog breed to predict \u201cdog sled\u201d; see Figs. 1, 11).\nAre generated images and perturbed captions realistic? As we are generating a new test set, it\nis important to verify that any performance changes are not due to an impact on the realism of our\nimages. In Table 4 we report the FID scores of our generated images and the perplexity scores of our\ngenerated captions and observe both to be within a reasonable range. Further, we reiterate that using\nour pipeline to reconstruct the original image results in a test set where each model measures near\nidentical performance (Table 2).\n8\na photo of a young \nboy sitting in the \nback seat of \na car with a teddy \nbear strapped to the \nseat belt\na photo of a young \nboy sitting in the \nback seat of \na bus with a teddy \nbear strapped to the \nseat belt\nPredictions:\nseat belt (97.22%)\ncar mirror (0.97%)\ncowboy hat (0.26%)\nteddy (0.18%)\nminivan (0.08%)\nPredictions:\nseat belt (52.49%)\nforklift (5.93%)\nlumbermill (3.94%)\nsliding door (3.50%)\nchain saw (1.73%)\na photo of a woman \nwearing a seat belt \nsitting in the back \nseat of a car\nPredictions:\nseat belt (96.06%)\ncar mirror (0.58%)\nCD player (0.48%)\ndial telephone (0.23%)\nhair spray (0.12%)\nPredictions:\nseat belt (19.59%)\nhair spray (6.73%)\ncrossword puzzle (3.40%)\ncardigan (3.24%)\nwig (2.90%)\na photo of a woman \nwearing a seat belt \nsitting in the back \nseat of a bus\na painting of a woman \nin a plaid jacket and \nsunglasses making a \nhush sign with her \nfinger on her lips\na photo of a woman in \na plaid jacket and \nsunglasses making a \nhush sign with her \nfinger on her lips\nPredictions:\nharmonica (88.76%)\nsunglasses (2.89%)\nsunglass (1.89%)\nlipstick (1.17%)\nmask (0.73%)\nPredictions:\nharmonica (64.61%)\nsunglasses (15.44%)\nsunglass (10.81%)\npajama (1.55%)\nbow tie (0.99%)\na photo of a man \nwearing sunglasses and \na hat with a reflection \nof himself in the \nglasses of his face\na painting of a man \nwearing sunglasses and \na hat with a reflection \nof himself in the \nglasses of his face\nPredictions:\nsunglass (42.41%)\nsunglasses (41.44%)\nski mask (3.40%)\nseat belt (2.36%)\nsnorkel (1.76%)\nPredictions:\nski mask (20.37%)\nmask (15.47%)\nsnorkel (7.78%)\nsunglasses (3.93%)\nsunglass (3.11%)\nPredictions:\nsunglass (40.52%)\nsunglasses (28.76%)\nsweatshirt (11.54%)\nwig (1.12%)\ntrench coat (0.85%)\na photo of a woman in \na red coat and \nsunglasses with her \narms on her sides\nPredictions:\nsweatshirt (47.77%)\nwig (6.01%)\nsunglass (3.80%)\ncloak (3.49%)\ncomic book (2.66%)\na painting of a woman \nin a red coat and \nsunglasses with her \narms on her sides\na photo of a young boy \nsitting in the back \nseat of a car wearing \nsunglasses, a seat \nbelt, and a hoodie\na photo of a young boy \nsitting in the back \nseat of a bus wearing \nsunglasses, a seat \nbelt, and a hoodie\nPredictions:\nseat belt (56.94%)\nsunglass (19.35%)\nsunglasses (14.83%)\nsleeping bag (2.23%)\nlimousine (0.99%)\nPredictions:\nsunglass (36.10%)\nsunglasses (29.95%)\nseat belt (19.74%)\nsleeping bag (3.47%)\nstretcher (1.38%)\nClass: sunglasses, Intervention: photo\u2192 painting, \ud835\udc00\ud835\udc2f\ud835\udc20. \ud835\udeab\ud835\udc29 \ud835\udc32\ud835\udc06\ud835\udc13 x) = \ud835\udfcf\ud835\udfd6% \nClass: seat belt, Intervention: c\ud835\udc1a\ud835\udc2b \u2192 bus, \ud835\udc00\ud835\udc2f\ud835\udc20. \ud835\udeab\ud835\udc29 \ud835\udc32\ud835\udc06\ud835\udc13 x) = \ud835\udfd1\ud835\udfcf%\nClass: howler monkey, Intervention: change color, \ud835\udc00\ud835\udc2f\ud835\udc20. \ud835\udeab\ud835\udc29 \ud835\udc32\ud835\udc06\ud835\udc13 x) = \ud835\udfd2\ud835\udfce%\nPredictions:\nhowler monkey (99.35%)\nsiamang (0.26%)\nspider monkey (0.26%)\ncapuchin (0.10%)\nindri (0.01%)\nPredictions:\nspider monkey (73.30%)\nhowler monkey (22.99%)\nlangur (1.13%)\ncolobus (0.69%)\ncapuchin (0.62%)\na photo of howler a \nmonkey sitting on a \ntree branch in a tree, \nwith leaves and \nbranches in the \nbackground\na photo of howler a \nmonkey sitting on a \ntree branch in a tree, \nwith orange leaves and \nbranches in the \nbackground\nPredictions:\nA.black bear (46.32%)\nwild boar (15.17%)\nsloth bear (12.43%)\nhowler monkey (9.51%)\ngroenendael (3.26%)\nPredictions:\nhowler monkey (24.19%)\nsiamang (19.67%)\nspider monkey (13.10%)\ncolobus (6.14%)\ntiti (5.92%)\na photo of a howler \nmonkey walking across \na lush green field \nnext to a bushy area \nwith trees\na photo of a howler \nmonkey walking across \na lush pink field \nnext to a bushy area \nwith trees\nPredictions:\nhowler monkey (97.99%)\nchimpanzee (0.59%)\ngorilla (0.51%)\ntiti (0.21%)\norangutan (0.17%)\nPredictions:\nhowler monkey (93.81%)\nchimpanzee (3.14%)\ngorilla (1.48%)\ntiti (0.22%)\norangutan (0.20%)\na photo of a black \nmonkey sitting on top \nof a tree filled \nwith green leaves \nnext to a tree\na photo of a black \nmonkey sitting on top \nof a tree filled \nwith snow-covered\nleaves next to a tree\nFigure 6: Deriving class-level insights. LANCE can be used to derive model bias at a per-class\nlevel by clustering text edits and visualizing clusters with the highest average \u2206p(yGT |x). Above we\nhighlight insights for three classes with a ResNet-50 model: i) \u201chowler monkey\u201d: high sensitivity to\nthe background color ii) \u201csunglasses\u201d: high sensitivity to the data domain, and iii) seat belt: high\nsensitivity to the (barely perceptible) vehicle type.\nTable 3: Evaluating caption perturber. LANCE performs successful edits without mutating labels.\nMetric\nSubject (%) Object (%) Adjective (%) Domain (%) Background (%) Overall (%)\nEdit Success (%)\n92\n84\n82\n98\n92\n89\nFilter prec. (%) / recall (%)\n99.0/97.0\n100/96.0\n92.9/98.9\n100/100\n94.1/100\n97.2/98.4\nLANCE\u2019s checks & balances are highly effective. Next, we evaluate the accuracy of the caption\nperturber at making the type of edit specified by the prompt (e.g.correctly changing only the domain\nfor a domain edit). To do so, we manually validate a random subset of 500 captions and their\nperturbations (100 per type) generated by the LANCE. We observe an overall accuracy of 89%, with\nthe object (84%) and adjective (82%) types achieving the lowest accuracies: we find that most failures\nfor these types result from incorrect identification of the object or adjective in the sentence. We note\nthat even in failure cases (e.g.changing an irrelevant word), the edits made are still reasonable.\nNext, we evaluate the efficacy of our caption editing checks and balances. We first label the same\nsubset of 500 examples by hand with the correct action (filter / no filter) and compare against LANCE.\nAcross perturbation types, LANCE achieves both high precision and recall, with a low overall false\npositive (2.8%) and false negative (1.6%) rate. Of these, a majority of false negatives (edits that alter\nthe ground truth that we fail to catch) are for the subject and object types: we find these typically\n9\nTable 4: Human study. LANCE-generated images score high on realism, edit success, and fidelity.\nMetric\nSubject\nObject\nAdjective Domain Background Overall\nImage Realism (1-5)\n4.2\u00b10.5\n4.4\u00b10.5\n4.1\u00b10.4\n3.9\u00b10.8\n4.2\u00b10.6\n4.2\u00b10.5\nEdit Success (1-5)\n3.8\u00b10.4\n3.6\u00b10.5\n4.1\u00b10.5\n3.7\u00b10.5\n3.3\u00b10.4\n3.7\u00b10.4\nImage Fidelity (1-5)\n4.7\u00b10.4\n4.5\u00b10.5\n4.6\u00b10.5\n4.7\u00b10.2\n4.4\u00b10.5\n4.5\u00b10.4\nLabel Consistency (%) 97.0\u00b14.0 94.0\u00b16.0\n99.0\u00b10\n93.0\u00b19.0\n93.0\u00b19.0\n95.0\u00b14.0\nEthical issues (%)\n3.0\u00b17.0\n1.0\u00b13.0\n5.0\u00b18.0\n0\u00b10\n0\u00b10\n2.0\u00b12.6\nchange the ground truth inadvertently e.g.for a \u201ckeyboard space bar\u201d image of a typewriter, the edit\ntypewriter\u2192 painting erroneously passes our filter (paintings do not usually have space bars).\nHuman evaluators validate the realism and efficacy of LANCE-generated edits. We conduct\na human evaluation along 5 axes: i) Image Realism (1-5, 5=best): How easy is it to tell that the\ncounterfactual image was generated by AI? ii) Edit success (1-5, 5=best): Does the generated image\ncorrectly incorporate the edit? iii) Image fidelity (1-5, 5=best): Are all the changes made relevant to\naccomplishing the edit? iv) Label consistency (Yes/No): Is the original image label still plausible for\nthe generated image?, and v) Ethical issues (Text input): Is the generated image objectionable, or\nraise ethical concerns around consent, privacy, stereotypes, demographics, etc.?\nWe collect responses from 15 external respondents for a random subset of 50 <image, ground truth,\ngenerated caption, perturbed caption, counterfactual image> tuples (10 per-perturbation type), and\nreport mean and variance. We include a screenshot of our study interface in Fig. 13, and report results\nin Table 4. As seen, images generated by LANCE are rated to be high on realism (4.2/5 on average)\nand fidelity (4.5/5 on average), and slightly lower on edit quality (3.7/5 on average). Of these, we\nfind that background edits score lowest on edit success, due to sometimes altering the wrong region\nof the image. Further, generated images have high label consistency (95%), verifying the efficacy of\nLANCE\u2019s checks and balances. Finally, \u223c2% of images are found to raise ethical concerns, which\nwe further analyze below.\nEthical Issues. On inspection of the images marked as objectionable in our human study, we find\nthat most arise from bias encoded in the generative model: e.g.for the edit woman\u2192lifeguard, the\ngenerative model also alters the perceived gender and makes the individual muscular. Further, for\npeople\u2192athletes, the model also alters the person\u2019s perceived race. The model also sometimes imbues\nstereotypical definitions of subjective characteristics such as \u201cstylish\u201d (e.g.by adding makeup). To\naddress this, we manually inspect the entire dataset of 781 images for similar issues and exclude 8\nadditional images. Going forward, we hope to ameliorate this by excluding subjective text edits and\nusing text-to-image models with improved fairness [69] and steerability [57].\nFailure modes and limitations. In Fig 12, we visualize LANCE\u2019s 5 most frequent failure modes: a)\nLabel-incompatible text edit: e.g.black howler monkey\u2192purple howler monkey (howler monkeys are\nalmost always black or red). b) Poor quality image edit: Images with low realism, which are largely\neliminated via our checks and balances. c) Image edit that inadvertently changes the ground truth:\ne.g.typewriter\u2192stapler for the class \u201cspace bar\u201d (staplers do not contain space bars). d) Inherited\nbias: e.g.changing perceived gender based on stereotypes. e) Meaningless caption edit: e.g.boy in\nback-seat driver in the back seat. Such failures can confound an observed performance drop but this\nis largely mitigated by their low overall incidence.\nLANCE also has a few intrinsic limitations. Firstly, it leverages several large-scale pretrained models,\neach of which may possess its own inherent biases and failure modes, which is highly challenging\nto control for. Secondly, intervening on language may exclude more abstract perturbations that are\nnot easily expressible in words. However, we envision that LANCE will directly benefit from the\nrapid ongoing progress in the generative capabilities [70, 53] and steerability [57] of image and text\nfoundation models, paving the road towards robust stress-testing in high-stakes AI applications.\nAcknowledgements. This work was supported in part by funding from NSF #2144194, Cisco,\nGoogle, and DARPA LwLL. We thank Simar Kareer for help with figures, members of the Hoffman\nLab for project feedback, and the volunteers who participated in our human study.\n10\nReferences\n[1] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the\nIEEE conference on computer vision and pattern recognition, pp. 770\u2013778, 2016.\n[2] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla,\nM. Bernstein, et al., \u201cImagenet large scale visual recognition challenge,\u201d International journal of computer\nvision, vol. 115, pp. 211\u2013252, 2015.\n[3] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson, U. Franke, S. Roth, and\nB. Schiele, \u201cThe cityscapes dataset for semantic urban scene understanding,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 3213\u20133223, 2016.\n[4] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \u201cMicrosoft\ncoco: Common objects in context,\u201d in Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings, Part V 13, pp. 740\u2013755, Springer, 2014.\n[5] D. Hoiem, S. K. Divvala, and J. H. Hays, \u201cPascal voc 2008 challenge,\u201d World Literature Today, vol. 24,\n2009.\n[6] Z. Liu, Z. Miao, X. Zhan, J. Wang, B. Gong, and S. X. Yu, \u201cLarge-scale long-tailed recognition in an\nopen world,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,\npp. 2537\u20132546, 2019.\n[7] I. Misra, A. Gupta, and M. Hebert, \u201cFrom red wine to red tomato: Composition with context,\u201d in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1792\u20131801, 2017.\n[8] C. Sakaridis, D. Dai, and L. Van Gool, \u201cSemantic foggy scene understanding with synthetic data,\u201d Interna-\ntional Journal of Computer Vision, vol. 126, pp. 973\u2013992, 2018.\n[9] X. Peng, Q. Bai, X. Xia, Z. Huang, K. Saenko, and B. Wang, \u201cMoment matching for multi-source domain\nadaptation,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, pp. 1406\u20131415,\n2019.\n[10] D. Hendrycks and T. Dietterich, \u201cBenchmarking neural network robustness to common corruptions and\nperturbations,\u201d arXiv preprint arXiv:1903.12261, 2019.\n[11] B. Recht, R. Roelofs, L. Schmidt, and V. Shankar, \u201cDo imagenet classifiers generalize to imagenet?,\u201d in\nInternational conference on machine learning, pp. 5389\u20135400, PMLR, 2019.\n[12] P. Kattakinda, A. Levine, and S. Feizi, \u201cInvariant learning via diffusion dreamed distribution shifts,\u201d arXiv\npreprint arXiv:2211.10370, 2022.\n[13] P. W. Koh, S. Sagawa, H. Marklund, S. M. Xie, M. Zhang, A. Balsubramani, W. Hu, M. Yasunaga, R. L.\nPhillips, I. Gao, et al., \u201cWilds: A benchmark of in-the-wild distribution shifts,\u201d in International Conference\non Machine Learning, pp. 5637\u20135664, PMLR, 2021.\n[14] J. Pearl, Causality. Cambridge university press, 2009.\n[15] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, \u201cZero-shot\ntext-to-image generation,\u201d in International Conference on Machine Learning, pp. 8821\u20138831, PMLR,\n2021.\n[16] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-resolution image synthesis with\nlatent diffusion models,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 10684\u201310695, 2022.\n[17] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and M. Irani, \u201cImagic: Text-based\nreal image editing with diffusion models,\u201d in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2023.\n[18] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al., \u201cLanguage models are few-shot learners,\u201d Advances in neural information processing\nsystems, vol. 33, pp. 1877\u20131901, 2020.\n[19] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and D. Cohen-Or, \u201cPrompt-to-prompt image\nediting with cross attention control,\u201d in International Conference on Learning Representations, 2022.\n[20] R. Mokady, A. Hertz, K. Aberman, Y. Pritch, and D. Cohen-Or, \u201cNull-text inversion for editing real images\nusing guided diffusion models,\u201d arXiv preprint arXiv:2211.09794, 2022.\n11\n[21] J. Li, D. Li, S. Savarese, and S. Hoi, \u201cBlip-2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models,\u201d arXiv preprint arXiv:2301.12597, 2023.\n[22] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\nA. Ray, et al., \u201cTraining language models to follow instructions with human feedback,\u201d Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27730\u201327744, 2022.\n[23] E. Wong, S. Santurkar, and A. Madry, \u201cLeveraging sparse linear layers for debuggable deep networks,\u201d in\nProceedings of the 38th International Conference on Machine Learning (M. Meila and T. Zhang, eds.),\nvol. 139 of Proceedings of Machine Learning Research, pp. 11205\u201311216, PMLR, 18\u201324 Jul 2021.\n[24] S. Singla and S. Feizi, \u201cSalient imagenet: How to discover spurious features in deep learning?,\u201d in\nInternational Conference on Learning Representations, 2022.\n[25] Z. Li and C. Xu, \u201cDiscover the unknown biased attribute of an image classifier,\u201d in The IEEE International\nConference on Computer Vision (ICCV), 2021.\n[26] S. Singla, B. Nushi, S. Shah, E. Kamar, and E. Horvitz, \u201cUnderstanding failures of deep networks via\nrobust feature extraction,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 12853\u201312862, 2021.\n[27] S. Singla, A. M. Chegini, M. Moayeri, and S. Feiz, \u201cData-centric debugging: mitigating model failures via\ntargeted data collection,\u201d arXiv preprint arXiv:2211.09859, 2022.\n[28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin,\nJ. Clark, et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International\nconference on machine learning, pp. 8748\u20138763, PMLR, 2021.\n[29] S. Eyuboglu, M. Varma, K. K. Saab, J.-B. Delbrouck, C. Lee-Messer, J. Dunnmon, J. Zou, and C. Re,\n\u201cDomino: Discovering systematic errors with cross-modal embeddings,\u201d in International Conference on\nLearning Representations, 2022.\n[30] S. Jain, H. Lawrence, A. Moitra, and A. Madry, \u201cDistilling model failures as directions in latent space,\u201d in\nInternational Conference on Learning Representations, 2023.\n[31] Y. Zhang, J. Z. HaoChen, S.-C. Huang, K.-C. Wang, J. Zou, and S. Yeung, \u201cDiagnosing and rectifying\nvision models using language,\u201d in The Eleventh International Conference on Learning Representations,\n2023.\n[32] R. Geirhos, J.-H. Jacobsen, C. Michaelis, R. Zemel, W. Brendel, M. Bethge, and F. A. Wichmann, \u201cShortcut\nlearning in deep neural networks,\u201d Nature Machine Intelligence, vol. 2, no. 11, pp. 665\u2013673, 2020.\n[33] L. A. Gatys, A. S. Ecker, and M. Bethge, \u201cA neural algorithm of artistic style,\u201d arXiv preprint\narXiv:1508.06576, 2015.\n[34] L. A. Gatys, A. S. Ecker, and M. Bethge, \u201cImage style transfer using convolutional neural networks,\u201d in\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 2414\u20132423, 2016.\n[35] T. Karras, S. Laine, and T. Aila, \u201cA style-based generator architecture for generative adversarial networks,\u201d\nin Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 4401\u20134410,\n2019.\n[36] R. Abdal, Y. Qin, and P. Wonka, \u201cImage2stylegan: How to embed images into the stylegan latent space?,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4432\u20134441, 2019.\n[37] R. Abdal, Y. Qin, and P. Wonka, \u201cImage2stylegan++: How to edit the embedded images?,\u201d in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition, pp. 8296\u20138305, 2020.\n[38] K. Crowson, S. Biderman, D. Kornis, D. Stander, E. Hallahan, L. Castricato, and E. Raff, \u201cVqgan-clip:\nOpen domain image generation and editing with natural language guidance,\u201d in Computer Vision\u2013ECCV\n2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXVII,\npp. 88\u2013105, Springer, 2022.\n[39] A. Q. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mcgrew, I. Sutskever, and M. Chen, \u201cGlide:\nTowards photorealistic image generation and editing with text-guided diffusion models,\u201d in International\nConference on Machine Learning, pp. 16784\u201316804, PMLR, 2022.\n[40] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image synthesis,\u201d Advances in Neural Informa-\ntion Processing Systems, vol. 34, pp. 8780\u20138794, 2021.\n12\n[41] A. Hertz, K. Aberman, and D. Cohen-Or, \u201cDelta denoising score,\u201d arXiv preprint arXiv:2304.07090, 2023.\n[42] T. Brooks, A. Holynski, and A. A. Efros, \u201cInstructpix2pix: Learning to follow image editing instructions,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern recognition, 2023.\n[43] A. Haque, M. Tancik, A. A. Efros, A. Holynski, and A. Kanazawa, \u201cInstruct-nerf2nerf: Editing 3d scenes\nwith instructions,\u201d arXiv preprint arXiv:2303.12789, 2023.\n[44] A. Sauer and A. Geiger, \u201cCounterfactual generative networks,\u201d in International Conference on Learning\nRepresentations.\n[45] G. Jeanneret, L. Simon, and F. Jurie, \u201cDiffusion models for counterfactual explanations,\u201d in Proceedings of\nthe Asian Conference on Computer Vision, pp. 858\u2013876, 2022.\n[46] G. Jeanneret, L. Simon, and F. Jurie, \u201cAdversarial counterfactual visual explanations,\u201d in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2023.\n[47] M. Zemni, M. Chen, \u00c9. Zablocki, H. Ben-Younes, P. P\u00e9rez, and M. Cord, \u201cOctet: Object-aware counterfac-\ntual explanations,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition,\n2023.\n[48] J. Luo, Z. Wang, C. H. Wu, D. Huang, and F. De la Torre, \u201cZero-shot model diagnosis,\u201d in Proceedings of\nthe IEEE conference on computer vision and pattern recognition, 2023.\n[49] X. Li, Y. Chen, Y. Zhu, S. Wang, R. Zhang, and H. Xue, \u201cImagenet-e: Benchmarking neural network\nrobustness via attribute editing,\u201d in Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2023.\n[50] O. Wiles, I. Albuquerque, and S. Gowal, \u201cDiscovering bugs in vision models using off-the-shelf image\ngeneration and captioning,\u201d in NeurIPS ML Safety Workshop, 2022.\n[51] J. Vendrow, S. Jain, L. Engstrom, and A. Madry, \u201cDataset interfaces: Diagnosing model failures using\ncontrollable counterfactual generation,\u201d arXiv preprint arXiv:2302.07865, 2023.\n[52] L. Dunlap, A. Umino, H. Zhang, J. Yang, J. E. Gonzalez, and T. Darrell, \u201cDiversify your vision datasets\nwith automatic diffusion-based augmentation,\u201d arXiv preprint arXiv:2305.16289, 2023.\n[53] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, et al., \u201cLlama: Open and efficient foundation language models,\u201d arXiv preprint arXiv:2302.13971,\n2023.\n[54] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLora: Low-rank\nadaptation of large language models,\u201d arXiv preprint arXiv:2106.09685, 2021.\n[55] N. Reimers and I. Gurevych, \u201cSentence-bert: Sentence embeddings using siamese bert-networks,\u201d arXiv\npreprint arXiv:1908.10084, 2019.\n[56] B. Wallace, A. Gokul, S. Ermon, and N. Naik, \u201cEnd-to-end diffusion latent optimization improves\nclassifier guidance,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV), pp. 7280\u20137290, October 2023.\n[57] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding conditional control to text-to-image diffusion models,\u201d in\nProceedings of the IEEE/CVF International Conference on Computer Vision, pp. 3836\u20133847, 2023.\n[58] C. H. Wu and F. De la Torre, \u201cA latent space of stochastic diffusion models for zero-shot image editing and\nguidance,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7378\u20137387,\n2023.\n[59] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv preprint arXiv:2207.12598, 2022.\n[60] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit models,\u201d arXiv preprint arXiv:2010.02502,\n2020.\n[61] R. Gal, O. Patashnik, H. Maron, G. Chechik, and D. Cohen-Or, \u201cStylegan-nada: Clip-guided domain\nadaptation of image generators,\u201d arXiv preprint arXiv:2108.00946, 2021.\n[62] M. Moayeri, S. Singla, and S. Feizi, \u201cHard imagenet: Segmentations for objects with strong spurious cues,\u201d\nin Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,\n2022.\n13\n[63] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, \u201cGans trained by a two time-scale\nupdate rule converge to a local nash equilibrium,\u201d Advances in neural information processing systems,\nvol. 30, 2017.\n[64] R. Wightman, \u201cPytorch image models.\u201d https://github.com/rwightman/pytorch-image-models,\n2019.\n[65] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein,\nL. Antiga, et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d Advances in\nneural information processing systems, vol. 32, 2019.\n[66] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, \u201cDistilbert, a distilled version of bert: smaller, faster, cheaper\nand lighter,\u201d ArXiv, vol. abs/1910.01108, 2019.\n[67] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Min-\nderer, G. Heigold, S. Gelly, et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at\nscale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[68] Z. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \u201cA convnet for the 2020s,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11976\u201311986,\n2022.\n[69] C. Zhang, X. Chen, S. Chai, C. H. Wu, D. Lagun, T. Beeler, and F. De la Torre, \u201cIti-gen: Inclusive\ntext-to-image generation,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision,\npp. 3969\u20133980, 2023.\n[70] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. M\u00fcller, J. Penna, and R. Rombach, \u201cSdxl:\nimproving latent diffusion models for high-resolution image synthesis,\u201d arXiv preprint arXiv:2307.01952,\n2023.\nAppendix\nA\nDataset Details\nAssets and license. All source images belong to the ImageNet dataset [2], which is distributed under a BSD-3\nlicense that permits research and commercial use.\nStatistics. We now provide an overview of the counterfactual test set generated via our method. After adding\nthe checks and balances described in the main paper to ensure caption and image quality after editing, we\napproximately double the size of the HardImagenet dataset (from 750 to 1531). We provide a per-class\nbreakdown of the generated samples in Figure 7: we find that our method is able to generate a larger number\nof counterfactuals for certain categories (e.g. \u201cswimming cap\u201d) than others (e.g. \u201chockey puck\u201d), presumably\nbecause one of two possible reasons: i) the structured caption perturber is able to generate several more plausible\nvariations for certain classes, or ii) the checks and balances in our image editing pipeline filters out many more\nexamples of certain classes due to not being of sufficiently high quality. We note that while we create this\ndataset as a proof-of-concept for our method, scaling it up to include additional classes and concepts is very\nstraightforward, and simply requires finetuning the perturber on (a small number of) additional caption editing\nexamples, which are relatively easy to collect.\nB\nPer-class Analysis\nIn the main paper, we quantified performance in aggregate and per perturbation-type. We now investigate\nper-class performance over generated counterfactuals in Figure 8. We find that the counterfactuals generated for\ncertain categories (e.g. \u201cski\u201d or \u201cswimming cap\u201d) are significantly more challenging across models than others.\nDo class-level insights generalize to the full dataset? Recall that in Figure 3 of the main paper we derived\nclass-level insights into model bias, for example finding that the accuracy of ResNet-50 models at recognizing\n\u201csunglasses\u201d drops signficantly if the data domain is changed from \u201cphoto\u201d to \u201cpainting\u201d. A natural question\nthen is whether such insights generalize directly to other classes. In Figure 9, we apply this intervention to the\nentire HardImageNet dataset and report the per-class drop in top-1 accuracy when going from the original test\nset to the generated counterfactual test set. As seen, across all but one category (\u201cbaseball player\u201d), performance\ndrops, often significantly (e.g. \u201cbalance beam\u201d and \u201chockey puck\u201d).\n14\ndogsled\nseat belt\nski\nvolleyball\nhockey puck\nkeyboard space bar\nhowler monkey\nbalance beam\npatio\nminiskirt\nsunglasses\nswimming cap\nsnorkel\ngymnastic horizontal bar\nbaseball player\n0\n20\n40\n60\n80\n100\n120\nCount\n35\n50\n58\n60\n22\n14\n54\n57\n45\n50\n59\n125\n45\n92\n15\nFigure 7: Label histogram of additional counterfactual images generated via LANCE.\nminiskirt\nseat belt\ndogsled\nhockey puck\nvolleyball\nski\nkeyboard space bar\npatio\nhowler monkey\nswimming cap\nbalance beam\nbaseball player\nsunglasses\nsnorkel\ngymnastic horizontal bar\nClass\n0\n20\n40\n60\n80\n100\nLANCE top-1 acc.\nResNet-50\nViT-B/16\nCLIP\nConvNext\nFigure 8: Per-class top-1 accuracy of trained models on the counterfactual images generated by\nLANCE on the HardImageNet [62] dataset.\nC\nAdditional Qualitative Results\nWe provide additional qualitative examples generated via LANCE in Figure 10. We demonstrate 3 examples\nfor each perturbation type: subject, object, background, domain, and adjective. Some of these examples are\nparticularly interesting e.g. that the model\u2019s confidence of there being a \u201cbalance beam\u201d present goes down\nsignificantly if a \u201ccoach\u201d is pictured doing a handstand (top row, middle). Similarly, the probability of the image\ncontaining a \u201cpatio\u201d under the model goes down considerably if benches are replaced by bicycles (row 2, left).\nSimilarly, the model can no longer recognize a \u201cdogsled\u201d if the weather conditions become misty (row 3, left).\nFinally, the model is much less confident in recognizing a \u201cseatbelt\u201d if the color of its buckle changes from silver\nto gold (bottom row, middle). We note that these are only image-level insights and require further investigation\nacross images of a given class to draw conclusions of systematic model bias.\nD\nList of Prompts\nIn Table 5 we provide an exhaustive list of the prompts we use for both GPT-3.5 turbo [18] and for finetuning\nLLAMA [53]. As seen, despite their simplicity our prompts are able to elicit the desired behavior from both\nmodels. We find that including a couple of examples in the prompt greatly improves performance.\n15\nSunglasses\nDog Sled\nHowler Monkey\nSeat Belt\nSki\nSwimming Cap\nBalance Beam\nGymnastic Horizontal Bar\nPatio\nHockey Puck\nMiniskirt\nKeyboard Space Bar\nVolleyball\nBaseball Player\nSnorkel\n0\n5\n10\n15\n20\nDrop in top-1 accuracy (%)\n11.77\n5.23\n8.85\n12.50\n8.48\n5.67\n9.08\n20.32\n3.51\n2.18\n22.22\n4.294.86\n-2.94\n2.39\nFigure 9: Per-class drop in top-1 accuracy of a trained ResNet-50 [1] model on the counterfactual\nimages generated by LANCE for the photo\u2192painting intervention.\nTable 5: Prompts used for programmatically collecting perturbations of captions using GPT-3.5\nturbo [18]. We use the same prompts without examples as instructions when instruction fine-tuning\nLLAMA-7B [53] with LoRA [54]\nPerturbation type\nPrompt\nEdit subject\nGenerate all possible variations by changing only the subject of the\nprovided sentence. For example: Change \"A man walking a dog\" to \"A\nwoman walking a dog\". <caption>\nEdit object\nIn English grammar, the subject is the person, thing, or idea that per-\nforms the action of the verb in a sentence, while the object is the person,\nthing, or idea that receives the action of the verb. Generate all possible\nvariations by changing only the object of the provided sentence. For\nexample: Change \"A man walking a dog\" to \"A man walking a horse\".\n<caption>\nEdit background\n1) Generate all possible variations of the provided sentence by changing\nor adding background or location details without altering the foreground\nor its attributes. For example: Change \"A man walking a dog\" and \"A\nman walking a dog with mountains in the background\", or change \"A man\nwalking a dog on grass\". Change \"A man walking a dog on the road\"\nto \"A man walking a dog with mountains by the beach\". <caption>\n2) Generate all possible variations of the provided sentence by only\nchanging the weather conditions, or adding a description of the weather\nif not already present. For example: change \"A man walking a dog\" to\n\"A man walking a dog in the rain\", and change \"A man walking a dog in\nthe rain\" to \"A man walking a dog in the snow\". <caption>\nEdit domain\nGenerate a few variations by only changing the data domain of the\nprovided sentence without changing the content. For example: Change\n\"A photo of a man\" to \"A painting of a man\" or \"A sketch of a man\". A\nphoto of <caption>\nEdit adjective\nGenerate all possible variations of the provided sentence by only adding\nor altering a single adjective or attribute. For example: change \"A\nman walking a brown dog\" to \"A tall man walking a brown dog\", \"A\nman walking a black dog\" or \"A man walking a cheerful brown dog\".\n<caption>\n16\na photo of a car is \nparked on the porch of \na house with a covered \nporch and a car parked \nin front of it\nPredictions:\npatio (57.82%)\nmobile home (18.06%)\npicket fence (9.84%)\nsliding door (7.36%)\nrocking chair (1.21%)\nPredictions:\nmobile home (29.76%)\npatio (16.50%)\npicket fence (10.33%)\ngas pump (8.23%)\nbicycle\u2013for-two(4.94%)\na photo of a bicycle is \nparked on the porch of \na house with a covered \nporch and a car parked \nin front of it\nPredictions:\ncrutch (82.80%)\nski (16.11%)\npole (0.32%)\nsnorkel (0.11%)\nparallel bars (0.08%)\na painting of a \nwoman standing in \nthe snow with skis \non her feet and a \nhat on her head\na photo of a woman \nstanding in the snow \nwith skis on her \nfeet and a hat on \nher head\nPredictions:\nski (99.64%)\npole (0.09%)\ncrutch (0.08%)\nshovel (0.04%)\nalp (0.03%)\na photo of a couple \nof men standing next to \neach other on a beach \nwith snorkels on their \nheads\na photo of a couple \nof women standing \nnext to each other on \na beach with snorkels \non their heads\nPredictions:\nsnorkel (99.55%)\nsea snake (0.19%)\nscuba diver (0.14%)\nsea cucumber (0.05%)\nstingray (0.02%)\nPredictions:\nsnorkel (99.95%)\nsea snake (0.02%)\nscuba diver (0.02%)\nstingray (0.00%)\nsea cucumber (0.00%)\na photo of a man \nholding a baseball \nbat on top of a lush \ngreen soccer field\na photo of a man \nholding a baseball bat \non top of a lush \ngreen field\nPredictions:\nballplayer (96.95%)\nbaseball (2.18%)\nfootball helmet (0.32%)\nplunger (0.17%)\ncroquet ball (0.10%)\nPredictions:\nsoccer ball (92.90%)\nballplayer (6.17%)\nbaseball (0.58%)\nfootball helmet (0.12%)\ngolf ball (0.10%)\nPredictions:\nbalance beam (91.91%)\nparallel bars (4.42%)\nhorizontal bar (3.63%)\nvolleyball (0.02%)\nbasketball (0.01%)\na photo of \na woman doing a \nhandstand on a balance \nbeam in a gym with \npeople watching\nPredictions:\nvolleyball (94.92%)\nhorizontal bar (3.36%)\nparallel bars (0.76%)\nbalance beam (0.38%)\nbasketball (0.26%)\na photo of \na coach doing a \nhandstand on a balance \nbeam in a gym with \npeople watching\na photo of a \npair of \nsunglasses and a \nstuffed animal\na drawing of a \npair of \nsunglasses and a \nstuffed animal\nPredictions:\nPencil box (25.39%)\nPurse (7.92%)\nStethescope (7.92%)\nHair slide (6.73%)\nSunglasses (4.85%)\nPredictions:\nHair slide (9.92%)\nSpindle (8.34%)\nPencil box (8.02%)\nThimble (6.01%)\nWooden spoon (5.81%)\na photo of a group of \nchairs sitting on top \nof a cement floor next \nto a palm tree\na photo of a group of \nchairs sitting on top \nof a marble floor \nnext to a palm tree\nPredictions:\ndining table (29.20%)\nlimousine (27.44%)\npatio (11.99%)\nhome theater (5.29%)\nrestaurant (3.43%)\nPredictions:\ndining table (41.14%)\ntub (10.64%)\npatio (9.99%)\nwashbasin (7.18%)\nbathtub (3.62%)\na photo of a man in \nsunglasses and \na fur hoodie taking a \nselfie in the \nmountains with snow \non the ground\na photo of a man in \nsunglasses and \na hoodie taking a \nselfie in the \nmountains with snow on \nthe ground\nPredictions:\nsunglasses (52.59%)\nsunglass (27.44%)\nalp (3.11%)\nsweatshirt (2.54%)\nsunscreen (1.79%)\nPredictions:\nsunglasses (44.52%)\nsunglass (24.31%)\nalp (6.57%)\nvalley (1.83%)\nski (1.70%)\nEdit subject\nEdit object\nEdit background\nEdit domain\nEdit adjective\na photo of a woman in a \nred jacket is pulling \ntwo dogs on a leash in \nthe misty woods\na photo of a woman in \na red jacket is \npulling two dogs on a \nleash in the woods\nPredictions:\ntoy terrier (44.61%)\nEskimo dog (12.72%)\nmalamute (6.38%)\nSiberian husky (3.96%)\nFrench bulldog (2.77%)\nPredictions:\nEskimo dog (31.83%)\ndogsled (16.98%)\nmalamute (12.72%)\nSiberian husky (5.94%)\nSaluki (3.34%)\na photo of a woman in \na leopard-\nprint leotard\na photo of a woman \nin a black leotard\nPredictions:\nbalance beam (61.49%)\nhorizontal bar (16.56%)\npole (9.16%)\nparallel bars (8.77%)\nmaillot (1.01%)\nPredictions:\npole (44.26%)\nbalance beam (21.45%)\nhorizontal bar (18.91%)\nparallel bars (4.48%)\nmaillot (3.23%)\na drawing of a man \nriding a dog sled \npulled by two dogs on \na snow covered field \nwith trees in the \nbackground\na photo of a man \nriding a dog sled \npulled by two dogs on \na snow covered field \nwith trees in the \nbackground\nPredictions:\ndogsled (99.32%)\nEskimo dog (0.46%)\nSiberian husky (0.17%)\nmalamute (0.03%)\ntimber wolf (0.00%)\nPredictions:\ndogsled (99.11%)\nEskimo dog (0.31%)\nSiberian husky (0.19%)\nmalamute (0.12%)\nhorse cart (0.08%)\na photo of \na gold seat belt and \na cell phone\na photo of a seat belt \nand a cell phone\nPredictions:\nseat belt (97.08%)\nbuckle (0.82%)\ndigital watch (0.50%)\nscabbard (0.39%)\ncellular telephone (0.30%)\nPredictions:\nbuckle (46.82%)\ndigital watch (22.26%)\nseat belt (5.80%)\nlighter (5.25%)\nballpoint (1.84%)\na photo of a man with \na beard wearing \nsunglasses and an \norange shirt and a hat\na photo of a man with \na moustache wearing \nsunglasses and an \norange shirt and a hat\nPredictions:\nsunglass (41.61%)\nbinoculars (16.89%)\nsunglasses (11.79%)\nweb site (7.41%)\nbubble (1.57%)\nPredictions:\nsunglass (55.14%)\nsunglasses (23.37%)\nbinoculars (11.75%)\nweb site (2.42%)\ncowboy hat (0.79%)\nPredictions:\nballplayer (75.83%)\nbaseball (24.17%)\nrifle (0.00%)\nracket (0.00%)\nfootball helmet (0.00%)\na photo of a \nyoung boy in a \nbaseball uniform \nholding a bat on top \nof a baseball field\nPredictions:\nballplayer (72.80%)\nbaseball (27.18%)\nrifle (0.00%)\ntripod (0.00%)\npark bench (0.00%)\na photo of a \nyoung girl in a \nbaseball uniform \nholding a bat on top \nof a baseball field\nPredictions:\npuck (88.25%)\nski (1.44%)\nscoreboard (1.43%)\ngo-kart (0.72%)\nunicycle (0.41%)\nPredictions:\npuck (98.36%)\nscoreboard (0.92%)\nbroom (0.16%)\nballplayer (0.07%)\ndigital clock (0.06%)\na photo of a group of \nyoung men playing a \ngame of ice hockey on \nan outdoor ice rink\na photo of a group of \nyoung men playing a \ngame of ice hockey on \nan\nindoor\nice\nrink\nFigure 10: We visualize the counterfactual images generated by LANCE for images from the\nHardImageNet [62] dataset. Each row corresponds to a specific edit type. Above each image, we\ndisplay its generated caption, highlighting the original and edited word(s). Below each image, we\ndisplay the top-5 classes predicted by a ResNet-50 model and the associated model confidence,\ncoloring the ground truth class in green.\nE\nAnalyzing Failure Modes\nDespite the considerable number of checks and balances that we insert into our pipeline, while running it\nend-to-end automatically and at scale, we find that a few poor counterfactuals still fall through the cracks. We\nprovide qualitative examples of representative failures modes in Figure 12, that we group into five categories:\n17\nClass: Dog Sled, Intervention: change dog breed to Malamute / Samoyed (both sled-dog breeds)\na photo of a group \nof dogs pulling a dog \nsled across a snow-\ncovered ground next to \na person\na photo of a group \nof samoyed dogs pulling \na dog sled across a \nsnow-covered ground \nnext to a person\nPredictions:\ndogsled (80.58%)\nEskimo dog (14.41%)\nSiberian husky (4.56%)\nmalamute (0.42%)\nNorw. elkhound (0.01%)\nPredictions:\nSamoyed (84.13%)\nmalamute (7.60%)\nEskimo dog (4.88%)\nSiberian husky (1.89%)\ndogsled (1.25%)\na photo of a group of \npeople standing around \na dog pulling a dog \nsled with a dog on it\na photo of a group of \npeople standing around \na samoyed dog pulling a \ndog sled with a dog on it\nPredictions:\ndogsled (80.58%)\nEskimo dog (14.41%)\nSiberian husky (4.56%)\nmalamute (0.42%)\nNorw. elkhound (0.01%)\nPredictions:\ndogsled (36.84%)\nEskimo dog (26.01%)\nSamoyed (21.58%)\nSiberian husky (2.89%)\nski (1.99%)\na photo of a group \nof huskies pulling a dog \nsled across a snowy \nfield with people in \nthe background\na photo of a group \nof malamutes pulling a \ndog sled across a snowy \nfield with people in \nthe background\nPredictions:\ndogsled (52.89%)\nEskimo dog (26.85%)\nmalamute (10.08%)\nSiberian husky (9.86%)\nNorw. elkhound (0.06%)\nPredictions:\nmalamute (85.67%)\ndogsled (6.07%)\nEskimo dog (4.90%)\nSiberian husky (3.33%)\nNorw. elkhound (0.01%)\na photo of a group \nof dogs pulling a dog \nsled across a snow-\ncovered ground next to \na person\na photo of a group \nof malamute dogs \npulling a dog sled \nacross a snow-covered \nground next to a person\nPredictions:\ndogsled (80.58%)\nEskimo dog (14.41%)\nSiberian husky (4.56%)\nmalamute (0.42%)\nNorw. elkhound (0.01%)\nPredictions:\ndogsled (40.12%)\nEskimo dog (38.65%)\nSiberian husky (18.99%)\nmalamute (2.22%)\nNorw. elkhound (0.01%)\nFigure 11: Beyond conventional diagnosis. LANCE can uncover nuanced model bias, e.g. predictive\nconfidence for \u201cdog sled\u201d drops significantly even if the dog breed is changed to other popular sled\ndog breeds.\nInconsistent with GT\nPoor quality image edit\nChanging GT \nImage generation bias\nA photo of a woman \nstanding in front of \na door with her hands \non her hips and a \npurse in her hand\nA photo of a middle-\naged woman standing in \nfront of a door with \nher hands on her hips \nand a purse in her \nhand\nPredictions:\nminiskirt (99.22%)\ncowboy boots (0.38%)\ntrench coat (0.10%)\nwig (0.05%)\nsandal (0.04%)\nPredictions:\ndoormat (34.79%)\nCardigan (13.87%)\nminiskirt (9.42%)\njean  (8.74%)\nsuit (5.62%)\nMeaningless caption edit\na photo of \na teenager doing \na trick on a \nbeam on a blue \nsurface with a \nblue background\na photo of \na woman doing a \ntrick on a beam \non a blue \nsurface with a \nblue background\nPredictions:\nbalance beam (98.26%)\nhorizontal bar (1.30%)\nparallel bars (0.22%)\nmaillot (0.06%)\nmaillot (0.05%)\nPredictions:\nbalance beam (50.78%)\npole (37.46%)\nhorizontal bar (8.01%)\nparallel bars (1.42%)\nbow (0.28%)\na photo of \na black monkey \nsitting on top of a \ntree filled with \ngreen leaves next to \na tree\na photo of \na purple monkey \nsitting on top of a \ntree filled with \ngreen leaves next to \na tree\nPredictions:\nhowler monkey (97.99%)\nchimpanzee (0.59%)\ngorilla (0.51%)\ntiti (0.21%)\norangutan (0.17%)\nPredictions:\nhowler monkey (23.62%)\nmacaque (22.84%)\nlangur (14.55%)\ncapuchin (6.62%)\nchimpanzee (6.34%)\na photo of \na typewriter sitting \non top of a stack of \nbooks on top of a \nwooden table\na photo of \na stapler sitting on \ntop of a stack of \nbooks on top of a \nwooden table\nPredictions:\ntypewriter keyboard (52.63%)\nspace bar (29.02%)\ncomputer keyboard (11.14%)\nnotebook (1.35%)\nmouse (1.19%)\nPredictions:\nmodem (16.56%)\nharmonica (13.21%)\nnotebook (7.92%)\nprinter (6.95%)\nhard disc (5.12%)\na video of two \nlittle girls hanging \nfrom a metal bar in \na playground area \nwith buildings in \nthe background\na photo of two \nlittle girls hanging \nfrom a metal bar in \na playground area \nwith buildings in \nthe background\nPredictions:\nhorizontal bar (99.93%)\nparallel bars (0.07%)\nswing (0.00%)\npole (0.00%)\nbalance beam (0.00%)\nPredictions:\nhorizontal bar (99.85%)\nparallel bars (0.14%)\nswing (0.01%)\npole (0.00%)\nbalance beam (0.00%)\nFigure 12: We visualize the different modes of failure of LANCE for genenerating counterfactuals\non the HardImageNet dataset. Each row corresponds to a specific edit type. Above each image,\nwe display its generated caption, highlighting the original and edited word(s). Below each image,\nwe display the top-5 classes predicted by a ResNet-50 model and the associated model confidence,\ncoloring the ground truth class in green.\ni) Inconsistent with GT. LANCE sometimes generates edits that, while being reasonable in isolation, generates\nan image that is inconsistent with the ground truth label. For example, \u201chowler monkeys\u201d are always black or\nblonde in color, and so a purple howler monkey is statistically unlikely.\nii) Meaningless caption edit. In some cases, the structured perturber makes an edit that is visually meaningless,\ne.g. changing photo to video, while our pipeline is constrained to image generation.\niii) Poor quality image edit. Though we find this to occur very rarely due to our image quality filters, a few\npoor quality image edits are still retained e.g. accidentally cropping the limb of a person.\niv) Changing GT. A few edits inadvertently change the ground truth. For example, for the category \u201cspace bar\u201d,\nchanging a typewriter to a stapler also ends up removing the space bar.\nv) Image generation bias. Finally, we find that the latent diffusion model\u2019s inherent bias also sometimes seeps\nthrough, e.g. when changing an image of a woman to a middle-aged woman (ground truth category: \u201cminiskirt\u201d),\n18\n[Instructions: Please read first!] LANCE is a tool to uncover vulnerabilities in a trained image classifier by generating challenging test cases. \nIt does so by editing a single visual aspect in a given image (described in text), while leaving the rest of it unchanged.\nThe goal of this study is to validate the quality of (a random subset of 50) images generated via LANCE, along five axes\n1) Realism: How realistic is the image generated by LANCE? (or, how easy it to tell that this image was created by an AI model?)\n2) Edit success: How well is the intended edit reflected in the generated image? (or, how well does the new caption match the generated image?)\n3) Image fidelity: How similar is the generated image to the starting image in all other aspects? (or, is the generated image sufficiently similar to the original image?)\n4) Label consistency: Is the original image label still plausible for the generated image?  (or, is the ground truth label (Col F) still a reasonable match to the generated image?)\n5) Ethical considerations: Is the generated image objectionable, or raise ethical concerns around consent, privacy, stereotypes, demographics, in any way\nOriginal Image\n(real)\nImage Caption \n(generated)\nLANCE\n(generated)\nLANCE Caption\n(generated + edited)\nGround \nTruth\nImage\nRealism (1-5)\nEdit \nsuccess\n(1-5)\nImage \nFidelity\n(1-5)\nLabel \nConsistency\n(check if yes)\nEthical \nIssues\n(check if yes)\nDetails of ethical Issues\n(e.g. NSFW, negative stereotypes, \ndiscriminatory etc.) \na photo of a boy \nsitting in the back \nseat of a car \nwearing a seat belt\na photo of a teenage \nboy sitting in the \nback seat of a car \nwearing a seat belt\nseat belt\n4\n5\n5\na photo of a man \nsitting in the back \nseat of a car \nwearing a seat belt\na photo of a middle-\naged man sitting in \nthe back seat of a \ncar wearing a seat \nbelt\nseat belt\n \n \n \nFigure 13: Human study. A screenshot of the interface we deploy to evaluate the quality of images\ngenerated by LANCE.\nthe diffusion model also subtly changes the woman\u2019s attire to presumably be closer to attires commonly worn by\nmiddle-aged women in its training data, rather than changing physical age markers alone: in this case, the image\ncannot strictly be considered a counterfactual as multiple attributes are being intervened on.\nTable 6: Hyperparameter values used for caption (top left), LLAMA finetuning (top right) and image\nediting (bottom).\nconfig\nvalue\nmin. caption length\n20\nmax. caption length\n100\nrepetition penalty\n1.0\n\u03f5\n0.5\ndecoding strategy\nbeam search\nbeam size\n5\nclustering strategy\nK-Means\nnumber of clusters\n5\n(a) Caption Generation and Clustering\nconfig\nvalue\nlearning rate\n3e-4\neffective batch size\n128\ngrad. accumulation steps\n32\nweight decay\n0.0\nmax. sequence len\n1024\nlora rank [54]\n8\nlora \u03b1 [54]\n16\nlora dropout [54]\n0.05\nlora weights [54]\nqkv\n(b) LLAMA Finetuning\nconfig\nvalue\nStable Diffusion [16] version\n1.4\nimage resolution\n512x512\nLR scheduler\nDDIM scheduler [60]\nbeta_start [60]\n0.00085\nbeta_end [60]\n0.012\nbeta_schedule [60]\nscaled linear\ndiffusion steps [19]\n50\nattention_replace_edit [19]\n2\ncross_replace_steps [19]\n0.8\nself_replace_steps [19]\n[0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nguidance scale\n7.5\n\u03c4 [42]\n0.2\nimg sim. threshold [42]\n0.7\nimg-text sim. threshold [42]\n0.2\n(c) Image Editing\nF\nImplementation Details\nWe run all experiments on a single NVIDIA A40 GPU. All the models that we evaluate are trained with\nsupervised learning on ImageNet-1K [2]. We include additional implementation details for hyperparameters\nused by LANCE for caption and image editing in Table 6.\n19\n"
  },
  {
    "title": "Faith and Fate: Limits of Transformers on Compositionality",
    "link": "https://arxiv.org/pdf/2305.18654.pdf",
    "upvote": "2",
    "text": "Faith and Fate:\nLimits of Transformers on Compositionality\nNouha Dziri1\u2217, Ximing Lu1,2\u2217, Melanie Sclar2\u2217,\nXiang Lorraine Li1\u2020, Liwei Jiang1,2\u2020 , Bill Yuchen Lin1\u2020,\nPeter West1,2, Chandra Bhagavatula1, Ronan Le Bras1, Jena D. Hwang1, Soumya Sanyal3,\nSean Welleck1,2, Xiang Ren1,3, Allyson Ettinger1,4, Zaid Harchaoui1,2, Yejin Choi1,2\n1Allen Institute for Artificial Intelligence\n2University of Washington\n3University of Southern California\n4University of Chicago\nnouhad@allenai.org, ximinglu@allenai.org, msclar@cs.washington.edu\nAbstract\nTransformer large language models (LLMs) have sparked admiration for their\nexceptional performance on tasks that demand intricate multi-step reasoning. Yet,\nthese models simultaneously show failures on surprisingly trivial problems. This\nbegs the question: Are these errors incidental, or do they signal more substantial\nlimitations? In an attempt to demystify transformer LLMs, we investigate the limits\nof these models across three representative compositional tasks\u2014multi-digit multi-\nplication, logic grid puzzles, and a classic dynamic programming problem. These\ntasks require breaking problems down into sub-steps and synthesizing these steps\ninto a precise answer. We formulate compositional tasks as computation graphs to\nsystematically quantify the level of complexity, and break down reasoning steps\ninto intermediate sub-procedures. Our empirical findings suggest that transformer\nLLMs solve compositional tasks by reducing multi-step compositional reason-\ning into linearized subgraph matching, without necessarily developing systematic\nproblem-solving skills. To round off our empirical study, we provide theoretical\narguments on abstract multi-step reasoning problems that highlight how autoregres-\nsive generations\u2019 performance can rapidly decay with increased task complexity.\n1\nIntroduction\n\u201cIt was the epoch of belief, it was the epoch of incredulity.\u201d \u2013 Charles Dickens, A Tale of Two Cities\nLarge-scale transformers such as ChatGPT [57] and GPT4 [58] demonstrate unprecedented capa-\nbilities [57, 74, 11, 15, 85], even noted as \u201csparks of AGI\u201d [12]. In stark contrast, the same models\nsometimes struggle with simple, intuitive tasks [9, 62, 40]. For instance, humans can solve 3-digit\nby 3-digit multiplication arithmetic after learning basic calculation rules [22, 34]. Yet, off-the-shelf\nChatGPT and GPT4 achieve only 55% and 59% accuracies on this task, respectively (\u00a73).\nThe striking discrepancy between the impressive successes of transformer LLMs on seemingly\ncomplex tasks and the astonishing failures on seemingly trivial tasks spark critical open questions\nabout how to faithfully interpret their mixed capabilities. Under what conditions do transformers\nsucceed, fail, and why? What types of errors do they make? Can transformers uncover implicit\nproblem-solving rules or be taught to follow reasoning paths?\nSeeking thorough answers to these questions remains an open research challenge. However, we\noffer novel insights into the fundamental limits of transformers2, centered around compositional\n\u2217 First co-authors.\n\u2020 Second co-authors.\n2For brevity, we use \u2018transformers\u2019 to refer to \u2018autoregressive transformer LLMs\u2019 throughout the paper.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.18654v3  [cs.CL]  31 Oct 2023\nproblems that require strict multi-hop reasoning to derive correct predictions. Applying step-by-step\nreasoning is fundamental to human intelligence [69, 68]. These compositional problems present\ncompelling challenges for AI systems as they require combining basic reasoning operations to\nfollow computational paths that arrive at unique correct solutions. In particular, we study three\nstraightforward and flexible representative compositional tasks: long-form multiplication, logic grid\npuzzles (i.e., Einstein\u2019s puzzle [61]), and a classic dynamic programming problem.\nWe propose two hypotheses. First, transformers solve compositional tasks by reducing multi-step\ncompositional reasoning into linearized path matching. This contrasts with the systematic multi-step\nreasoning approach that learns to apply underlying computational rules required for building correct\nanswers [71, 37, 27]. Shortcut learning [29] via pattern-matching may yield fast correct answers\nwhen similar compositional patterns are available during training but does not allow for robust\ngeneralization to uncommon or complex examples. Second, due to error propagation, transformers\nmay have inherent limitations on solving high-complexity compositional tasks that exhibit novel\npatterns. Errors in the early stages of the computational process can lead to substantial compounding\nerrors in subsequent steps, preventing models from finding correct solutions.\nTo investigate our hypotheses, we formulate compositional tasks as computation graphs. These graphs\nbreak down problem-solving into submodular functional steps, enabling structured measurements\nof complexity and verbalization of computational steps as input sequences to language models.\nMoreover, we leverage information gain to predict patterns that models are likely to learn based on\nthe underlying task distribution without the need to perform full computations within the graph.\nEmpirical results show that training on task-specific data leads to near-perfect performance on in-\ndomain instances and under low compositional complexity, but fails drastically on instances outside\nof this region. This substantial gap suggests that systematic problem-solving capabilities do not\nemerge from maximum likelihood training [5] on input-output sequences, even when prompted or\ntrained with human-like reasoning steps (i.e., a linearization of computation graphs; \u00a73.1). Models\u2019\nsuccess can be attributed, in part, to their exposure to training examples sub-graphs that involve\nthe same computations required for solving test examples (see Section 3.2.2) In order to gain a\ndeeper understanding of models\u2019 failures, we conduct a comprehensive analysis by decomposing\ntheir computation graphs and examining different error types. We find that while models can\nmemorize single-step operations, they fail to compose them into correct reasoning paths, suggesting\nthat they mostly make predictions based on shallow, rote learning rather than a deep, holistic\ntask understanding (\u00a73.2.3). Importantly, we provide theoretical evidence of exponential error\naccumulation using abstract compositional tasks. All tasks analyzed empirically in this paper are\ninstantiations of these abstractions (\u00a74). We argue that transformers could be inherently limited in\nsolving compositionally complex tasks out-of-the-box3.\nAs transformers continue to make tangible real-world impacts, it is pressing to interpret their re-\nmarkable performance critically. Our work takes a realistic look at the limitations of transformers\nin the context of compositional tasks. To shed light on practical future steps, we identify directions\nfor addressing these limitations, such as using transformers for tasks that could be decomposed into\nfew reasoning steps, tasks where evaluation may afford some leniency, and using transformers in\ncombination with planning modules or refinement methods to improve their generations. To advance\nlanguage AI, fundamental innovations are required to address or complement these limitations.\n2\nMeasuring Limitations of Transformers in Compositional Tasks\nHuman problem-solving skills can be conceptualized as a graph structure, where each vertex repre-\nsents a partial solution and the edges represent operators that can be applied to modify these solutions.\nAs we will outline next and illustrate in Figure 1, we use computation graphs and corresponding\nmetrics to methodically evaluate transformers\u2019 reasoning abilities.\n2.1\nComputation Graph Definition\nLet A be a deterministic algorithm (function), and let FA be a set of primitives (functions) the\nalgorithm uses in its execution. Assuming the inputs x to algorithm A are given, we define A(x)\u2019s\nstatic computation graph GA(x). GA(x) = (V, E, s, op) is a directed acyclic graph. Nodes V\n3Code and data are available at https://github.com/nouhadziri/faith-and-fate\n2\n for \nA(x)\nx = [7,49]\n function multiply (x[1..p], y[1..q]):\n   // multiply x for each y[i]\n   for i = q to 1\n     carry = 0\n     for j = p to 1 \n       t = x[j] * y[i]\n       t += carry\n       carry = t // 10\n       digits[j] = t mod 10\n     summands[i] = digits\n   // add partial results (computation not shown)\n   product = \n   return product\n\u2211q\ni=1 summands[q+1-i] \u22c5 10i-1\nA(x)\nmultiply \n1-digit\ncarry\nconcat\nmod 10\nsum\nColor  \nlegend:\n7\n4\n9\n28\n63\n2\n6\n8\n14\n1\n3\n4\n3\n343\nFigure 1: Transformation of an algorithm A to its computational graph GA(x). The depicted example is of\nlong-form multiplication algorithm A, for inputs x = [7, 49] (i.e. computing 7 \u00d7 49).\nrepresent all variables\u2019 values during A\u2019s execution: each node v \u2208 V has a value s(v) \u2208 R\nassociated. Edges E represent the function arguments involved in some computation: for each non-\nsource node v \u2208 V , let U = {u1, . . . , uj} \u2282 V j be its parent nodes. Then, s(v) = f(u1, . . . , uj)\nfor some f \u2208 FA. Since each node v is uniquely defined by the computation of a single primitive f,\nwe define op : V \u2192 FA as op(v) = f.\nLet S \u2282 V be the source nodes of GA(x) and without loss of generality, let o \u2208 V be its sole leaf\nnode. By definition, S \u2261 x and A(x) = s(o), representing the input and output of A respectively.\nTo be able to train and evaluate a language model\u2019s ability to follow algorithm A we must linearize GA(x).\nSince we only consider autoregressive models, this linearization must also be a topological ordering.\n2.2\nQuantifying Compositional Complexity using Graph Metrics\nA\u2019s representation as a computation graph GA(x) enables measuring task complexity from many angles.\nWe define a node v \u2208 V \u2019s layer number as the length of the longest path from a source node to v in\nthe directed acyclic graph GA(c). We then define the reasoning depth as the largest layer number in\nthe graph. In computation graphs, reasoning depth is a proxy for the maximum level of multi-hop\nreasoning required to solve the task.\nLet dS : V \u2192 N0 be the shortest distance to any of G\u2019s source nodes S \u2282 V . We define the\nreasoning width of a graph as the mode of {d(v) : v \u2208 V }. This metric aims to measure the\nmaximum number of variables required to maintain in parallel during the computation. Relatedly, we\nalso define the average parallelism of a graph as the ratio between |V | and its reasoning depth. This\naims to compute the average width in computation through the graph, and not just in its mode.\n2.3\nPredicting Surface Patterns through Relative Information Gain\nWhen evaluating model performance, we may observe partially correct answers even in an overall\nincorrect response. To understand model strategies in these partial successes, we use Relative\nInformation Gain to predict surface patterns that models are likely to recognize. We represent task T as\na distribution (X1, . . . , Xn, Y1, . . . , Ym) and measure the amount of (normalized) information gained\nabout an output element Yj by observing a subset of input random variables X \u2282 {X1, . . . , Xn}:\nRelativeIG(Yj, X) = H(Yj) \u2212 H(Yj|X)\nH(Yj)\n\u2208 [0, 1]\n(1)\nRelativeIG may be used to analyze the influence of any node in the computation graph (as defined in\n\u00a72.1) with respect to a set of its ancestors; in particular, output nodes with respect to input nodes.\n2.4\nExploring Three Representative Compositional Tasks: Definitions\nMultiplication\nMulti-digit multiplication requires executing operations with numerical symbols\nbased on procedural rules [34]. This task has multiple algorithmic solutions; in constructing computa-\ntion graphs, we use the well-known O(k1k2) long-form multiplication algorithm for computing x \u00b7 y,\nwhere x has k1 \u2264 5 digits and y has k2 \u2264 5 digits in base 10. See \u00a7A.1 for data construction details.\n3\n0\n5\n10\n15\nAverage parallelism\n0.5\n0.0\n0.5\n1.0\n1.5\nAccuracy\nGPT4 zero-shot (Multiplication)\n(b)\nFigure 2: (a) Zero-shot accuracy. Axes refer to problem sizes (number of digits in multiplication, number of\nhouses and attributes in puzzle, and sequence length in the DP task). Transformers\u2019 accuracy decreases to near\nzero as task complexity increases, measuring task complexity by the problem size. (b) Average parallelism\nnegatively correlates with accuracy.\nTo instantiate GA(x), let FA = {one-digit multiplication, sum, mod 10, carry over, concatenation}.\nSource nodes S are digits of input numbers, leaf node o is the final output, and intermediate nodes v\nare partial results generated during execution of the long-form multiplication algorithm (see Figure 1).\nEinstein\u2019s Puzzle\nEinstein\u2019s puzzle is a well-known logic puzzle often used as a benchmark for\nsolving constraint satisfaction problems [61]. It involves a list of houses with different attributes (e.g.,\nowner\u2019s name, pets), and the goal is to determine which attributes belong to each house by combining\na set of pre-defined natural language clues or constraints. The solution to the puzzle is a matrix of\nsize K \u00d7 M, where K represents the number of houses and M the number of attributes. As K and\nM increase, synthesizing different partial solutions that satisfy individual constraints becomes highly\ncompositionally complex. To construct the computation graph, we consider a greedy algorithm that\niteratively eliminates possible solutions by filling at least one cell each time. It deterministically fills\nthe cell(s) that requires the minimum number of clues among all current unfilled cells. We refer to\nthis as the elimination function. See \u00a7A.2 for examples, data construction, and algorithm details.\nTo instantiate GA(x), let FA ={elimination function}. The source nodes are the clues, all intermedi-\nate nodes are partially-filled matrices, and the output node is a fully-filled solution matrix.\nDynamic Programming Problem\nDynamic programming (DP) recursively breaks down complex\nproblems into simpler sub-problems, so problems solved using this technique are compositional. We\nanalyze a classic relaxation of the NP-complete Maximum Weighted Independent Set problem [39]:\nGiven a sequence of integers, find a subsequence with the highest sum, such that no two numbers in\nthe subsequence are adjacent in the original sequence. This relaxation may be solved in O(n) time\nusing DP. See the solution in \u00a7A.3. In the experiments, we restrict each integer to the [\u22125, 5] range.\nTo instantiate GA(x), let FA = {equals, and, not, indicator function, sum, max}. Source nodes are\nelements of the input list, and the output node is a list that for each element indicates whether it\nshould be selected. We select an O(n) algorithm since GA(x)\u2019s size is proportional to A\u2019s complexity.\n3\nTesting the Limits of Transformers: Empirical Evidence\nExperimental\nSetup\nTo\nunderstand\nthe\ncapabilities\nof\nLLMs,\nwe\nevaluate\nGPT3\n(text-davinci-003) [11], ChatGPT (GPT-3.5-turbo) [57] and GPT4 (gpt-4) [58] using zero-\nshot, few-shot, and finetuning techniques. To enable the generation of computation graphs beyond\nthe final answers, we use the concept of scratchpads [56]. Scratchpads are a verbalization of the\ncomputation graphs (i.e., a linearized representation of a topological ordering of GA(x)). Overall, we\nconsider question-answer and question-scratchpad formats for few-shot and finetuning settings to\ngauge models\u2019 capabilities for learning with and without explicit reasoning. See details of additional\nmodels and experimental configurations in \u00a7B and examples of scratchpad in \u00a7A.\n3.1\nTesting the Limits of Transformers with Zero-shot, Few-shot and Finetuning\nLimits of transformers in zero- and few-shot settings\nTo investigate the inherent problem-solving\ncapabilities of LLMs, we begin by analyzing models\u2019 zero-shot and few-shot performances on our\ncompositional tasks. As shown in Figure 2, task performances deteriorate significantly from near\nperfection to zero with increasing complexity when measured by either problem size (Figure 2(a))or\n4\nFigure 3:\nGPT3 finetuned exhaustively on task-\nspecific data up to a certain problem size. The blue\nregion represents the in-distribution examples and the\nred region refers to OOD examples. The same trend\nis observed for the puzzle task (See \u00a7B.2)\nFigure 4: Results of training beyond the overfitting\npoint for the multiplication task with the goal of ex-\nploring whether OOD generalization capabilities (i.e.,\ngrokking) arise.\naverage parallelism (Figure 2(b)).The trend remains the same for few-shot prompting (see \u00a7B.2).\nThese results indicate that pre-training is in fact not sufficient to teach models how to combine basic\noperations to solve compositional problems, especially as problems grow more complex.\nLimits of transformers with question-answer training\nThe limited performance of models\nmay be attributed to the lack of task-specific data during pre-training. To fully bring out models\u2019\npotentials in solving these tasks, we next exhaustively finetune GPT3 with question-answer pairs. In\nmultiplication and DP, we finetune models with all enumerations of questions up to the maximum\nproblem size4 within reasonable training budget, leaving out 10% for validation and 10% for testing.\nIn puzzles, we train on a subset of all instances up to (K, M) \u2264 (4, 4) due to combinatorial explosion.\nWe separately finetune GPT3 models on \u223c1.8M multiplication pairs, \u223c142K DP pairs, and \u223c41K\npuzzle pairs (see details in \u00a7B.3). Additionally, to examine problems of different complexity, we\nconsider different training splits based on the depth and width of computation graphs.\nFigure 3 and Figure 5a show high accuracy for examples with splits seen during training, i.e., in-\ndomain. However, the performance sharply declines when evaluating unseen splits during training,\ni.e., out-of-domain (OOD). Similar trends hold in all tasks (see \u00a7 B.3), suggesting that systematic\nproblem-solving capabilities do not emerge via exhaustive training on task-specific data.\nLimits of transformers with explicit scratchpad training\nNext, we test whether we can explicitly\nteach models the required computational operations via scratchpads. To do so, we finetune GPT3\nwith question-scratchpad pairs for all tasks. We consider the same distribution splits as before.\nThe results, presented in Figure 5b, show that once again GPT3 achieves near-perfect performance\non in-distribution, but fails entirely in generalizing to OOD cases\u2014in particular, wider or deeper\ncomputation graphs. These results indicate that even when training directly with guidance on\nthe computation steps, models still fail to learn component operations in a generalizable manner.\nThis observation holds for all tasks (See details in \u00a7 B.4). Similarly, prompting transformers with\nquestion-scratchpad pairs enhances the performance compared to the zero-shot setting (refer to\n\u00a7 B.5). However, this performance boost diminishes to zero as complexity increases. These findings\nsuggest that the autoregressive characteristic of transformers, which forces them to tackle problems\nsequentially, presents a fundamental challenge that cannot be resolved by instructing the model to\ngenerate a step-by-step solution. Instead, models depend on a greedy process of producing the next\nword to make predictions without a rigorous global understanding of the task.\nLimits of transformers with grokking\nWe explore whether extended training beyond overfitting\nleads to improved generalization abilities, a phenomenon known as grokking [59, 53]. Due to budget\nconstraints, we only experiment on the multiplication task. Following [53], we fine-tune GPT3 with\nquestion-answer pairs for 420K steps and separately finetune GPT3 with question-scratchpad pairs\nfor 30K steps. Both models\u2019 training far exceeded the point at which in-domain accuracy plateaus5.\nFigure 4 shows no improvement in generalization for OOD cases beyond the overfitting point, even\n4We consider all k1-by-k2 digit multiplications with 1 \u2264 k1, k2 \u2264 4 and k1 \u00b7 k2 \u2264 9; and all DP problems\nup to 5 elements. We selected sizes based on budget constraints for GPT3 finetuning, see \u00a7B.3 for cost details.\n5The training duration for question-answer pairs is equivalent to 60 epochs and costs 50,000 USD. Training\non question-scratchpad pairs was conducted for 40 epochs and costs 40,000 USD.\n5\n(a) Results on question-answer pairs.\n(b) Results on question-scratchpad pairs.\nFigure 5: GPT3 finetuning and prompting accuracy on different data splits. Although the in-distribution\nperformance is almost perfect, GPT3 exhibits poor generalization with increasing graph depth and width. Refer\nto \u00a7B.3 and \u00a7B.4 for results on the puzzle and DP tasks.\nafter extensive training periods. We hypothesize that the absence of grokking may be due to the level\nof difficulty of the task. We speculate that increased task difficulty significantly impedes learning\na well-structured representation, which, according to [47], aligns with achieving grokking. Even if\ngrokking were to emerge through more prolonged training, such an approach would prove inefficient\nand unscalable. Future work is required to accurately explain when and how grokking occurs.\n3.2\nBreaking Down Successes and Failures of Transformers\n3.2.1\nInformation Gain Explains Where Transformers Partially Excel\nAt times transformers predict partially correct answers even when the overall response is incorrect.\nWe speculate that this may be due to particularities in the task distribution that allow for guessing\npartial answers without performing the full multi-step reasoning that the task requires.\nUsing relative information gain (defined in \u00a72.3), we can predict surface patterns that a model is\nlikely to learn and contrast them empirically. For multiplication, relative information gain shows that\nthe first digit (two digits) of the output highly correlates with the first digit (two digits) of each input\nnumber (see \u00a7C.1). Hence, this spurious pattern is likely to be learned by a model. Similarly, the\nprediction of the last digit (or two digits) of the output is observed to solely rely on the last digit (or\ntwo digits) of each input number. This pattern holds true due to the principles of modulo arithmetic,\nwhich ensures the validity of this relationship in all cases. Empirically, we verify that models indeed\nlearn the patterns we predicted and other patterns as well (e.g., order of magnitude of the answer,\nnumber of trailing zeros for multiplication) in all the settings with and without scratchpad. See details\nfor multiplication, plus dynamic programming task analysis in \u00a7C.\nThese experiments suggest that if an output element heavily relies on a single or a small set of input\nfeatures, transformers are likely to recognize such correlation during training and directly map these\ninput features to predict the output element in testing, without going through the rigorous multi-hop\nreasoning and giving a false illusion of performing compositional reasoning.\n3.2.2\nTransformers Reduce Multi-Step Compositional Reasoning into Linearized Subgraph\nMatching\nWe now explore whether models\u2019 correct predictions on unseen test data are due to learning the un-\nderlying algorithm or, instead, explainable by exposure to similar training examples. We hypothesize\nthat, beyond simple memorization, transformers largely rely on pattern matching for solving these\ntasks. To test this, we calculate the average frequency with which partial computations needed to\nsolve an instance appear in the training data, for both correctly and wrongly predicted examples.\nGiven a model-generated computation graph bGA(x) we analyze how often the full computation of\neach node v \u2208 bV is seen in training. We define v\u2019s full computation as the subgraph induced by all\nancestors of v including v, denoted FC b\nGA(x)(v). We say that FC b\nGA(x)(v) is seen during training if\nFC b\nGA(x)(v) \u223c= FCGA(x\u2032)(w) for some computation graph GA(x\u2032) in training, and for some w \u2208 V .\nWe characterize complexity of a full computation subgraph by its depth, as defined in \u00a72.1.\nFigure 6 shows that full computation subgraphs appear significantly more frequently in the training\ndata for correctly predicted test examples than for incorrectly predicted ones, for both the multi-\n6\navg. freq. in  \ntraining data\nFine-tuned GPT3 - Multiplication\navg. freq. in  \ntraining data (log)\nFine-tuned GPT3 - Dynamic Prog.\nCorrect Final Answer\nIncorrect Final Answer\nFine-tuned GPT3 - DP\nFigure 6:\nAverage frequency in\nwhich test examples\u2019 full computa-\ntions subgraph appear in the train-\ning data w.r.t. the subgraph depth,\ngrouped by final answer.\nFully Correct\nLocal Error\nPropagation Error\nRestoration Error\nFive-shot GPT4 \u2013 DP\nFine-tuned GPT3 \u2013 DP\nFive-shot GPT4 \u2013 Multiplication\nFive-shot GPT4 \u2013 Puzzle\nFigure 7: Ratio of nodes in each of the four correct/error categories\nfor each layer in computation graph. Results shown are for few-shot\nprompting and fine-tuning with scratchpad.\nplication and DP task (both frequencies tend to zero for large depths since we ensured a disjoint\ntrain/test split). This high correlation suggests that pattern matching\u2014and not general reasoning\ncapabilities\u2014may be the cause behind correct model outputs. This type of learning could be largely\neffective when the compositional complexity of tasks is low but it becomes less efficient when tasks\nare increasingly complex. This may elucidate the observed performance gain in low-complexity and\nin-domain cases and the striking performance drop in OOD and highly complex cases.\n3.2.3\nWhat Types of Errors do Transformers Make at Different Reasoning Depths?\nFor clearer understanding of where transformers fall short, we analyze the types of errors that\ntransformers make for nodes at different layers in the computation graph. For every input x, we\ncompare the ground truth computation graph GA(x) with the (possibly incorrect) model-generated\ncomputation graph bGA(x). We consider a node v as having a correct value if and only if s(v) = bs(v).6.\nWe consider a node v to be derived from a correct computation if given that U = {u1, . . . , uk} are\nthe immediate predecessors of v in bGA(x) and that bop(v) = f, we have that f(u1, . . . , uk) = bs(v).\nNote that the notion of correct computation is independent of G, and that a node v derived from a\ncorrect computation may not have the correct value if an error occurred in some of its ancestors.\nWe classify each node v \u2208 bV into one of four categories. Node v is fully correct if v and its ancestors\nhave correct values and are derived from correct computations. If a node v is not fully correct, its\nerror can be of the following types: v has a local error if its parent nodes have correct values but v is\nderived from an incorrect computation (i.e., a one-hop reasoning error); v has a propagation error if\nv is derived from a correct computation but some of its parent nodes have incorrect values; v has a\nrestoration error if it has a correct value but is derived from an incorrect computation.\nFigure 7 shows results for few-shot GPT4 and fine-tuned GPT3 with scratchpad, with respect to\ngraph layer number for each node. In all settings, the ratio of fully correct nodes is almost perfect\nbut sharply decreases toward zero with increasing graph layers. Moreover, the ratio of propagation\nerrors is usually higher than the ratio of local errors. Both phenomena suggest that models are able to\ncorrectly perform single-step reasoning, potentially due to memorizing such single-step operations\nduring training, but fail to plan and compose several of these steps for an overall correct reasoning.\nBoth the DP and the puzzle tasks have a high ratio of restoration errors, suggesting memorization\nsince correct outputs are produced despite incorrect computations. There are signs of memorization\neven when restoration errors are near zero: 82.3% of the final correct answers for 4-digit by 2-digit\nmultiplications (a setting unseen during training) had at least one error in the computation graph, but\nstill produced correct answers. These patterns are possibly due to high frequency of (input, output)\nmultiplication pairs in the pretraining data, in contrast to intermediate reasoning steps.\n6If a node v does not appear in the ground truth graph G, we consider it to have an incorrect value.\n7\n4\nError Propagations: The Theoretical Limits\nExperiments (\u00a73) highlight the limitations of current transformers in handling complex, multi-step\nreasoning tasks. Concretely, we show that errors rapidly escalate as the problem size grows (\u00a73.2.3).\nHere, we aim to provide theoretical insights into why autoregressive transformer LLMs can perform\nsignificantly worse in compositional tasks as the problem size increases, making explicit the different\nways in which compounding stochastic errors affect final performance. We argue using stylized\nexamples that transformers may be too limited to solve compositionally complex tasks. Formal\nstatements and full proofs are provided in \u00a7D.\nAlgorithms designed to solve compositional tasks typically involve multiple independent applications\nof a function and/or iterated applications of the same function. A transformer executing such an\nalgorithm acts as an estimator of these functions. In this context, we examine the probability of such\nan estimator reaching the correct answer as the problem size increases. We first consider a scenario\nwhere a transformer estimates an algorithm requiring n independent applications of a function:\nProposition 4.1 (informal). Let fn involve the combination hn of n independent applications of a func-\ntion g. Let bf, bg,bhn be their estimators. Assume thatbhn is a perfect estimator of hn and that hn has low\ncollision, with cn being an upper bound of hn\u2019s collision rate (cn < c \u2200n, with c \u226a 1). If P(g\u0338=bg)=\n\u03f5 >0 where bg\u2019s errors are independent, then P(fn \u0338= bfn) > 1 \u2212 cn \u2212 (1 \u2212 \u03f5)n \u00b7 (1 \u2212 cn). This implies\nthat P(fn \u0338= bfn) decreases exponentially as n increases, with lim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265 1 \u2212 c. Moreover,\nif cn \u2264 \u03b2\u03b1n for some \u03b1 \u2208 (0, 1), \u03b2 > 0, P(fn \u0338= bfn) tends exponentially to 1 as n increases.\nProp. 4.1\u2019s proof (\u00a7D.1) shows the rate of convergence is exponential, thus concluding that transform-\ners will rapidly fail with increasing n. Let\u2019s now analyze the iterated application function scenario.\nProposition 4.2 (informal). Let fn(x)=gn(x) involve the repeated application of g. Assume that\nthe probability of recovering from a mistake due to the randomness of applying the estimator on\nan incorrect input has probability at most c. If P(g \u0338= bg) = \u03f5 > 0, then P(fn \u0338= bfn) decreases\nexponentially with n. Precisely, P(fn \u0338= bfn) \u2265 1 \u2212 (1 \u2212 \u03f5 \u2212 c)n\u22121(1 \u2212 \u03f5 \u2212 c/(c + \u03f5)), implying\nlim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265 1 \u2212 c/(c + \u03f5).\nThe argument is as follows. Let sn := P(fn = bfn), where s1 = 1 \u2212 \u03f5 by definition. Derive\nsn \u2264 (1 \u2212 \u03f5 \u2212 c) \u00b7 sn\u22121 + c using law of total probability. Then, prove by induction a non-recursive\nupper bound for sn with limit\nc\nc+\u03f5 when n \u2192 +\u221e. See formal statement and derivation in \u00a7D.2.\nProp. 4.2\u2019s proof also shows an exponential rate of convergence.\nNote that if c \u226a \u03f5 then\nlim inf\nn\u2192+\u221e P(fn \u0338=\nbfn) \u2248 1. It is reasonable to assume c \u226a \u03f5 when g has low collision, since c\nrepresents the probability of the estimator bg(y) arriving at the correct output g(x) by chance when\ngiven the wrong input y \u0338= x. More details in \u00a7D.3.\nMoreover, repeated applications of a function often imply unbounded errors: if g(x) can be expressed\nas an affine transformation Fx+c, then it may be viewed as a first-order vector autoregression, which\nare known to be unstable when |\u03bb| \u2265 1 for at least one \u03bb eigenvalue of F [31, Prop. 10.1]. While we\nmake these arguments with affine maps, similar behaviors, possibly even more acute, could occur\nwith nonlinear maps [25]\u2014but their study is beyond the scope of this paper.\nIn Prop. 4.2\u2019s current form, we implicitly assume that there is a single valid reasoning for each input\nsince g is a function. We can potentially generalize this assumption with a state-transition framing,\nwhere the probability of transitioning from a valid state to an invalid one is \u03f5, and the probability of\nrecovering from an invalid state is at most c. See formal statement in D.2.\nAll tasks evaluated in the present work can be seen as instances of the results just proven. Prop. 4.1\ndirectly applies to multiplication, since m-by-n digit multiplication can be seen as n independent\ninstances of m-by-1 digit multiplication (see Cor. D.1). Prop. 4.2 directly applies to the recursive\nfunction of the dynamic programming task, as well as to m-by-1 digit multiplication, and to the\npuzzle through its elimination function (details in D.3). They are also all low collision settings.\nNote that Prop 4.1 and 4.2 apply to any high-performant estimator of reasoning tasks. We focus on\nout-of-the-box transformers to align with the scope of our experiments and with the goal of framing\nempirical results. In \u00a75, we discuss how these propositions may inform future research directions.\n8\n5\nDiscussion\nCollapsed Compositionality and Robustness Implications\nTransformers today demonstrate\nundeniably powerful empirical results. Yet, our study suggests that they may have fundamental\nweaknesses in certain intellectual tasks that require true multi-step compositional operations such as\nmultiplications and logic puzzles. Our careful study based on the computation graph and analyses\ndemonstrates that transformers can often solve multi-step compositional problems by collapsing the\ndepth of the compositional operations via analogical pattern matching. More broadly, our findings\nsuggest that the strong performance of transformers should be taken with a certain grain of salt:\nDespite initially appearing challenging, certain tasks may not possess the inherent compositionality\nthey seem to have. This is due to the fact that desired solutions could be readily derived from\ninput-output sequences present in the training data, allowing for shortcut pattern matching to produce\nacceptable solutions. However, such an approach can ultimately result in poor generalization as shown\nin our study. For example, fine-tuning GPT3 on our tasks both with and without explicit reasoning\ngraphs shows that models\u2019 learning fails to generalize beyond levels of complexity seen in training.\nTheoretical Findings and their Empirical Implications\nThe proofs presented in \u00a74 show that,\nunder reasonable assumptions, the probability of incorrect predictions converges exponentially to\n\u2248 1 for abstract compositional tasks. Importantly, these proofs apply to autoregressive LMs in\ngeneral. Our insights indicate that the current configuration of transformers, with their reliance\non a greedy process for predicting the next word, constrains their error recovery capability and\nimpedes the development of a comprehensive global understanding of the task. Building on these\nfindings, we suggest several empirical strategies for harnessing the potential of transformers. Firstly,\ntransformers may be employed in ways that require chaining only a few compositional steps to reach\na solution rather than lengthy reasoning steps (e.g., [35]). Secondly, transformers may be best suited\nfor compositional tasks where evaluation metrics can afford some leniency; for example, finding\napproximate solutions that do not require executing the whole graph, such as identifying the most\nsignificant digit in a multiplication. Finally, we suggest augmenting transformers with planning\nmodules as well as using refinement methods, that can iteratively improve their generations [82, 48].\nCall for Broad Participation to Investigate Limitations\nIdentification of limitations is an im-\nportant step towards achieving greater robustness. Our study suggests fundamental limitations that\nimpede transformers from fully mastering certain compositional operations. However, we acknowl-\nedge that due to our compute budget constraints as well as limited access to the largest language\nmodels such as GPT4, we are unable to push the empirical limits of transformers even further in terms\nof training data size and number of epochs. We invite the broader research community, particularly\nthose with more extensive resources at their disposal, to investigate these possibilities further.\n6\nRelated Work\nReasoning abilities in transformer LLMs\nRecently, transformers [11, 58, 57, 17, 16, 63, 73, 74]\nhave demonstrated impressive reasoning abilities across a wide range of tasks, even outperforming\nhumans in certain cases [79, 28, 15, 85]. This success has been largely attributed to the scaling effect,\nwhere larger models and training datasets result in improved performance [38, 33, 1]. However, these\nmodels have also been shown to struggle across multiple domains [32], including algorithmic reason-\ning [78], commonsense reasoning [62, 40], theory of mind [65], planning [76], logical reasoning [70],\nand ethical reasoning [36]. These difficulties have motivated us to take a step back and thoroughly\nexamine both the successes and failures of transformers from empirical and theoretical perspectives\non compositional reasoning tasks.\nChallenges of transformers in compositional tasks\nTransformers perform fairly well in single-\nstep reasoning tasks [70], but face challenges when it comes to effectively combining multiple steps to\nsolve compositionally complex problems [84, 55, 66, 81]. Recent research has focused on overcoming\nthese limitations through various approaches. First, fine-tuning transformers to directly generate\nthe final answer while keeping the reasoning implicit [7, 18]. Second, encouraging transformers to\ngenerate reasoning steps explicitly within a single generation [55, 80, 44, 42]. For example, Nye et al.\n[55] and Zhou et al. [86] used scratchpads to teach transformers how to perform algorithmic reasoning\ntasks such as addition by splitting the task into intermediate steps [44, 80]. Further, leveraging LLMs\nto generate each reasoning step iteratively via a selection and inference mechanism [20, 19, 72].\n9\nLastly, choosing a training split that maximizes the number of observed patterns between the train and\ntest data [10], or diversifying in-prompt examples to cover the maximum of patterns [41], ultimately\nenhancing generalization. The primary focus of these studies is to enhance model performance on\ncompositional problems without striving for complete mastery. In contrast, our work explores the\nfundamental limits of vanilla transformers in achieving full mastery, striving for 100% performance\nin both in-domain and OOD settings. Our findings show that reaching full mastery is inherently\nchallenging, providing insights into the complexities involved.\nChallenges of transformers in generalization\nExtensive research has been done to investigate the\ngeneralization capabilities of transformers [3, 54, 26, 64, 8, 46]. This encompasses various facets of\ngeneralization, including easy-to-hard generalization [67, 4], length generalization [2, 60, 13, 54, 8],\nand generalization on symbolic mathematical integration [83]. Schwarzschild et al. [67] and Bansal et\nal. [4] employ weight-tied neural networks to generalize from easy to hard examples. Liu et al., [45]\nfound that shallow transformers learn shortcuts during training, leading to poor OOD generalization.\nRazeghi et al. [64] revealed a positive correlation between the frequency of training terms and their\ntest performance. Building upon this line of inquiry, we present a more rigorous examination of\nsub-graph matching between training and test instances for complex compositional tasks where we\ndemonstrate how pattern matching can hinder generalization. We complement our empirical results\nwith theoretical insights on transformers\u2019 limits.\nGrokking\nThe phenomena of models\u2019 gaining generalization capabilities when training signifi-\ncantly beyond overfitting, known as grokking was recently introduced in [59]. Subsequent works\nfocus on characterizing when and why grokking arises: [47] show that perfect generalization in an\narithmetic addition task happens when there is sufficient data to determine the appropriate structured\nrepresentation, later extended to sparse parity in [52] where a sparse subnetwork of neurons is shown\nresponsible for generalization behavior. Recently, [77] propose that grokking occurs when a task\nadmits a generalizing and a memorizing solution, and the former is slower to learn. In this present\nwork, our aim is not to explain grokking but rather to observe its emergence. We do not observe\ngrokking arising in the context of multiplication, and we leave it to future work to explore whether\nthis may be due to task difficulty hindering the learning of well-structured representations.\nTransformers\u2019 theoretical expressiveness\nLin et al. [43] study autoregressive models\u2019 limitations\nfrom a computational complexity theory perspective. Transformer-specific work has focused on\nquantifying the class of problems that (not necessarily autoregressive) transformers can express\nassuming perfect parameters [51, 50, 14, 49, inter alia]. All tasks analyzed in our work belong to a\nclass expressible by transformers, suggesting that known upper bound might not be tight. Importantly,\nHahn [30] shows that transformers cannot robustly model noncounter-free regular languages even\nwhen allowing infinite precision. In contrast, our focus is on error accumulation, which enables to\ninvestigate if reasoning tasks theoretically solvable by transformers are likely to be solved by them.\nAdditional literature and societal impact discussion can be found in \u00a7E.\n7\nConclusions\nOn a broader scope, as transformers continue to gain widespread deployment with significant real-\nworld impacts, it is ever more urgent to understand their successes and failures. Our study critically\ninvestigates transformers\u2019 limitations and emphasizes the need to develop models capable of robust\ngeneralization and systematic problem-solving. By examining the compositional capabilities of these\nmodels, we aspire to work towards more reliable AI systems that excel not only in tasks where\nabundant training examples are sufficient, but also in cases requiring precise compositional reasoning.\n8\nLimitations\nWe focus on analyzing compositional reasoning capabilities through the lens of computation graphs.\nAlthough they are a useful way to systematically represent rigorous reasoning processes, it is important\nto note that for the scratchpad approach, we are limited to only establishing a correlation between the\nmodel generation and its preceding context, as we cannot inspect the exact tokens model attends to\nwhen making the prediction. This limitation arises from our lack of access to the activations of the\nstudied models. Furthermore, we posit that alternative approaches to linearizing reasoning processes\nmay yield different performances and provide opportunities for further exploration.\n10\nAcknowledgements\nWe thank members of the Mosaic team at AI2 for valuable feedback on this project, as well as Agust\u00edn\nSantiago Guti\u00e9rrez and Kawin Ethayarajh for valuable discussions. This research was supported by\nthe NSF DMS-2134012, DARPA MCS program through NIWC Pacific (N66001-19-2-4031), and the\nAllen Institute for AI.\n11\nReferences\n[1] Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan\nZhang, Stephen Roller, Naman Goyal, Omer Levy, and Luke Zettlemoyer. Scaling laws for\ngenerative mixed-modal language models. CoRR, abs/2301.03728, 2023.\n[2] Cem Anil, Yuhuai Wu, Anders Andreassen, Aitor Lewkowycz, Vedant Misra, Vinay Ramasesh,\nAmbrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur. Exploring length general-\nization in large language models. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,\nand A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages\n38546\u201338556. Curran Associates, Inc., 2022.\n[3] Cem Anil, Yuhuai Wu, Anders Johan Andreassen, Aitor Lewkowycz, Vedant Misra,\nVinay Venkatesh Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam Neyshabur.\nExploring length generalization in large language models. In Advances in Neural Information\nProcessing Systems, volume 35, pages 38546\u201338556. Curran Associates, Inc., 2022.\n[4] Arpit Bansal, Avi Schwarzschild, Eitan Borgnia, Zeyad Emam, Furong Huang, Micah Goldblum,\nand Tom Goldstein. End-to-end algorithm synthesis with recurrent networks: Extrapolation\nwithout overthinking. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh,\neditors, Advances in Neural Information Processing Systems, volume 35, pages 20232\u201320242.\nCurran Associates, Inc., 2022.\n[5] Yoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. A neural probabilistic language model.\nIn Advances in Neural Information Processing Systems, volume 13. MIT Press, 2000.\n[6] D. Bertsekas. Abstract Dynamic Programming: 3rd Edition. Athena scientific optimization and\ncomputation series. Athena Scientific., 2022.\n[7] Gregor Betz, Christian Voigt, and Kyle Richardson. Critical thinking for language models. In\nProceedings of the 14th International Conference on Computational Semantics (IWCS), pages\n63\u201375, 2021.\n[8] Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal. On the practical ability of recurrent neural\nnetworks to recognize hierarchical languages. In Donia Scott, N\u00faria Bel, and Chengqing Zong,\neditors, Proceedings of the 28th International Conference on Computational Linguistics, COL-\nING 2020, Barcelona, Spain (Online), December 8-13, 2020, pages 1481\u20131494. International\nCommittee on Computational Linguistics, 2020.\n[9] Ning Bian, Xianpei Han, Le Sun, Hongyu Lin, Yaojie Lu, and Ben He. ChatGPT is a knowl-\nedgeable but inexperienced solver: An investigation of commonsense problem in large language\nmodels. CoRR, abs/2303.16421, 2023.\n[10] Ben Bogin, Shivanshu Gupta, and Jonathan Berant. Unobserved local structures make composi-\ntional generalization hard. In Proceedings of the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 2731\u20132747, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural\nInformation Processing Systems, volume 33, pages 1877\u20131901, 2020.\n[12] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi,\nMarco T\u00falio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith GPT-4. CoRR, abs/2303.12712, 2023.\n12\n[13] Mirelle Bueno, Carlos Gemmel, Jeffrey Dalton, Roberto de Alencar Lotufo, and Rodrigo Fras-\nsetto Nogueira. Induced natural language rationales and interleaved markup tokens enable\nextrapolation in large language models. CoRR, abs/2208.11445, 2022.\n[14] David Chiang, Peter Cholak, and Anand Pillay. Tighter bounds on the expressivity of transformer\nencoders. Proceedings of the 40th International Conference on Machine Learning, 202:5544\u2013\n5562, 2023.\n[15] Jonathan H Choi, Kristin E Hickman, Amy Monahan, and Daniel Schwarcz. ChatGPT goes to\nlaw school. Available at SSRN, 2023.\n[16] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker\nSchuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes,\nYi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson,\nReiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier\nGarcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David\nLuan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani\nAgrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat,\nAitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei\nZhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei,\nKathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling\nlanguage modeling with pathways. CoRR, abs/2204.02311, 2022.\n[17] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu,\nZhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav\nMishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason\nWei. Scaling instruction-finetuned language models. CoRR, abs/2210.11416, 2022.\n[18] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language.\nIn Proceedings of the Twenty-Ninth International Conference on International Joint Conferences\non Artificial Intelligence, pages 3882\u20133890, 2021.\n[19] Antonia Creswell and Murray Shanahan. Faithful reasoning using large language models. CoRR,\nabs/2208.14271, 2022.\n[20] Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference: Exploiting large\nlanguage models for interpretable logical reasoning. In International Conference on Learning\nRepresentations, 2023.\n[21] Leonardo De Moura and Nikolaj Bj\u00f8rner. Z3: An efficient smt solver. In Tools and Algorithms\nfor the Construction and Analysis of Systems: 14th International Conference, TACAS 2008,\nHeld as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS\n2008, Budapest, Hungary, March 29-April 6, 2008. Proceedings 14, pages 337\u2013340. Springer,\n2008.\n[22] Stanislas Dehaene, Nicolas Molko, Laurent Cohen, and Anna J Wilson. Arithmetic and the\nbrain. Current opinion in neurobiology, 14(2):218\u2013224, 2004.\n[23] B. Delyon and A. Juditsky. On small perturbations of stable markov operators: Unbounded\ncase. Theory of Probability & Its Applications, 43(4):577\u2013587, 1999.\n[24] Persi Diaconis and David Freedman. Iterated random functions. SIAM review, 41(1):45\u201376,\n1999.\n[25] R. Douc, E. Moulines, and D. Stoffer. Nonlinear Time Series: Theory, Methods and Applications\nwith R Examples. Chapman & Hall/CRC Texts in Statistical Science. Taylor & Francis, 2014.\n[26] Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapo-\nlation to longer sequences. In Proceedings of the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 403\u2013413, 2020.\n13\n[27] Jonathan St BT Evans. Bias in human reasoning: Causes and consequences. Lawrence Erlbaum\nAssociates, Inc, 1989.\n[28] Hao Fu, Yao; Peng and Tushar Khot. How does gpt obtain its ability? tracing emergent abilities\nof language models to their sources. Yao Fu\u2019s Notion, Dec 2022.\n[29] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel,\nMatthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature\nMachine Intelligence, 2(11):665\u2013673, 2020.\n[30] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions\nof the Association for Computational Linguistics, 8:156\u2013171, 2020.\n[31] James Douglas Hamilton. Time series analysis. Princeton university press, 1994.\n[32] Chadi Helwe, Chlo\u00e9 Clavel, and Fabian M. Suchanek. Reasoning with transformer-based mod-\nels: Deep learning, but shallow reasoning. In Danqi Chen, Jonathan Berant, Andrew McCallum,\nand Sameer Singh, editors, 3rd Conference on Automated Knowledge Base Construction, AKBC\n2021, Virtual, October 4-8, 2021, 2021.\n[33] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo\nJun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin Mann, Alec\nRadford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman, Dario Amodei, and\nSam McCandlish. Scaling laws for autoregressive generative modeling. CoRR, abs/2010.14701,\n2020.\n[34] James Hiebert. Conceptual and procedural knowledge: The case of mathematics. Routledge,\n2013.\n[35] Albert Qiaochu Jiang, Sean Welleck, Jin Peng Zhou, Timothee Lacroix, Jiacheng Liu, Wenda\nLi, Mateja Jamnik, Guillaume Lample, and Yuhuai Wu. Draft, sketch, and prove: Guiding\nformal theorem provers with informal proofs. In The Eleventh International Conference on\nLearning Representations, 2023.\n[36] Liwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Maxwell Forbes, Jon\nBorchardt, Jenny Liang, Oren Etzioni, Maarten Sap, and Yejin Choi. Delphi: Towards machine\nethics and norms. CoRR, abs/2110.07574, 2021.\n[37] Philip N Johnson-Laird, Sangeet S Khemlani, and Geoffrey P Goodwin. Logic, probability, and\nhuman reasoning. Trends in cognitive sciences, 19(4):201\u2013214, 2015.\n[38] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. CoRR, abs/2001.08361, 2020.\n[39] Jon Kleinberg and Eva Tardos. Algorithm Design. Addison-Wesley Longman Publishing Co.,\nInc., USA, 2005.\n[40] Philipp E. Koralus and Vincent Wang-Mascianica. Humans in humans out: On GPT converging\ntoward common sense in both success and failure. CoRR, abs/2303.17276, 2023.\n[41] Itay Levy, Ben Bogin, and Jonathan Berant.\nDiverse demonstrations improve in-context\ncompositional generalization. In Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1401\u20131422, Toronto, Canada, July\n2023. Association for Computational Linguistics.\n[42] Zhengzhong Liang, Zeyu Zhang, Steven Bethard, and Mihai Surdeanu. Explainable verbal rea-\nsoner plus (evr+): A natural language reasoning framework that supports diverse compositional\nreasoning. arXiv preprint arXiv:2305.00061, 2023.\n[43] Chu-Cheng Lin, Aaron Jaech, Xin Li, Matthew R. Gormley, and Jason Eisner. Limitations of\nautoregressive models and their alternatives. In Proceedings of the 2021 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 5147\u20135173, Online, June 2021. Association for Computational Linguistics.\n14\n[44] Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale\ngeneration: Learning to solve and explain algebraic word problems. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),\npages 158\u2013167, 2017.\n[45] Bingbin Liu, Jordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-\nformers learn shortcuts to automata. In The Eleventh International Conference on Learning\nRepresentations, 2022.\n[46] Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy, and Cyril Zhang. Trans-\nformers learn shortcuts to automata. In International Conference on Learning Representations,\n2023.\n[47] Ziming Liu, Ouail Kitouni, Niklas S Nolte, Eric Michaud, Max Tegmark, and Mike Williams.\nTowards understanding grokking: An effective theory of representation learning. Advances in\nNeural Information Processing Systems, 35:34651\u201334663, 2022.\n[48] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe,\nUri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, Sean Welleck, Bodhisattwa Prasad\nMajumder, Shashank Gupta, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback. CoRR, abs/2303.17651, 2023.\n[49] William Merrill and Ashish Sabharwal. The expresssive power of transformers with chain of\nthought. arXiv preprint arXiv:2310.07923, 2023.\n[50] William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers.\nAdvances in Neural Information Processing Systems, 2023.\n[51] William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision\ntransformers. Transactions of the Association for Computational Linguistics, 11:531\u2013545, 2023.\n[52] William Merrill, Nikolaos Tsilivis, and Aman Shukla. A tale of two circuits: Grokking as\ncompetition of sparse and dense subnetworks. In ICLR 2023 Workshop on Mathematical and\nEmpirical Understanding of Foundation Models, 2023.\n[53] Shikhar Murty, Pratyusha Sharma, Jacob Andreas, and Christopher Manning. Grokking of\nhierarchical structure in vanilla transformers. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Volume 2: Short Papers), pages 439\u2013448, Toronto,\nCanada, July 2023. Association for Computational Linguistics.\n[54] Benjamin Newman, John Hewitt, Percy Liang, and Christopher D Manning. The eos decision\nand length extrapolation. In Proceedings of the Third BlackboxNLP Workshop on Analyzing\nand Interpreting Neural Networks for NLP, pages 276\u2013291, 2020.\n[55] Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,\nand Augustus Odena. Show your work: Scratchpads for intermediate computation with language\nmodels, 2021.\n[56] Maxwell I. Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin,\nDavid Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, Charles Sutton,\nand Augustus Odena. Show your work: Scratchpads for intermediate computation with language\nmodels. CoRR, abs/2112.00114, 2021.\n[57] OpenAI. ChatGPT: Optimizing language models for dialogue, 2022.\n[58] OpenAI. GPT-4 technical report, 2023.\n[59] Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, and Vedant Misra. Grokking: Gen-\neralization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177,\n2022.\n[60] Ofir Press, Noah Smith, and Mike Lewis. Train short, test long: Attention with linear biases\nenables input length extrapolation. In International Conference on Learning Representations,\n2022.\n15\n[61] Patrick Prosser. Hybrid algorithms for the constraint satisfaction problem. Computational\nintelligence, 9(3):268\u2013299, 1993.\n[62] Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi\nYang.\nIs ChatGPT a general-purpose natural language processing task solver?\nCoRR,\nabs/2302.06476, 2023.\n[63] Jack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom\nHennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne\nHendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri,\nSaffron Huang, Jonathan Uesato, John F. J. Mellor, Irina Higgins, Antonia Creswell, Nathan\nMcAleese, Amy Wu, Erich Elsen, Siddhant M. Jayakumar, Elena Buchatskaya, David Budden,\nEsme Sutherland, Karen Simonyan, Michela Paganini, L. Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena Gribovskaya, Domenic Donato, Angeliki\nLazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, N. K. Grigorev, Doug\nFritz, Thibault Sottiaux, Mantas Pajarskas, Tobias Pohlen, Zhitao Gong, Daniel Toyama,\nCyprien de Masson d\u2019Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew G.\nJohnson, Blake A. Hechtman, Laura Weidinger, Iason Gabriel, William S. Isaac, Edward\nLockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem W. Ayoub, Jeff\nStanway, L. L. Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. Scaling\nlanguage models: Methods, analysis & insights from training gopher. ArXiv, abs/2112.11446,\n2021.\n[64] Yasaman Razeghi, Robert L Logan IV, Matt Gardner, and Sameer Singh. Impact of pretraining\nterm frequencies on few-shot numerical reasoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2022, pages 840\u2013854, Abu Dhabi, United Arab Emirates, December\n2022. Association for Computational Linguistics.\n[65] Maarten Sap, Ronan Le Bras, Daniel Fried, and Yejin Choi. Neural theory-of-mind? on the\nlimits of social intelligence in large LMs. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pages 3762\u20133780, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics.\n[66] Abulhair Saparov and He He. Language models are greedy reasoners: A systematic formal\nanalysis of chain-of-thought. In International Conference on Learning Representations, 2023.\n[67] Avi Schwarzschild, Eitan Borgnia, Arjun Gupta, Furong Huang, Uzi Vishkin, Micah Goldblum,\nand Tom Goldstein. Can you learn an algorithm? generalizing from easy to hard problems with\nrecurrent networks. In Advances in Neural Information Processing Systems, volume 34, pages\n6695\u20136706. Curran Associates, Inc., 2021.\n[68] Herbert A. Simon. The architecture of complexity. Proceedings of the American Philosophical\nSociety, 106(6):467\u2013482, 1962.\n[69] Herbert A Simon and Allen Newell. Human problem solving: The state of the theory in 1970.\nAmerican psychologist, 26(2):145, 1971.\n[70] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid,\nAdam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, Agnieszka\nKluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexan-\nder W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain,\nAmanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen,\nAndrea Santilli, Andreas Stuhlm\u00fcller, Andrew M. Dai, Andrew La, Andrew K. Lampinen,\nAndy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio\nNorelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem, Ayla\nKarakas, and et al. Beyond the imitation game: Quantifying and extrapolating the capabilities\nof language models. CoRR, abs/2206.04615, 2022.\n[71] Keith Stenning and Michiel Van Lambalgen. Human reasoning and cognitive science. MIT\nPress, 2012.\n16\n[72] Oyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs,\nand abductive statements over natural language. In Findings of the Association for Computa-\ntional Linguistics: ACL-IJCNLP 2021, pages 3621\u20133634, 2021.\n[73] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. CoRR, abs/2211.09085, 2022.\n[74] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha,\nHeng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee,\nHuaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun,\nDmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett,\nKathleen S. Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos,\nToju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben\nHutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi\nRajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen,\nRachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed H. Chi,\nand Quoc Le. Lamda: Language models for dialog applications. CoRR, abs/2201.08239, 2022.\n[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: Open and efficient foundation\nlanguage models. CoRR, abs/2302.13971, 2023.\n[76] Karthik Valmeekam, Alberto Olmo, Sarath Sreedharan, and Subbarao Kambhampati. Large\nlanguage models still can\u2019t plan (a benchmark for LLMs on planning and reasoning about\nchange). In NeurIPS 2022 Foundation Models for Decision Making Workshop, 2022.\n[77] Vikrant Varma, Rohin Shah, Zachary Kenton, J\u00e1nos Kram\u00e1r, and Ramana Kumar. Explaining\ngrokking through circuit efficiency. arXiv preprint arXiv:2309.02390, 2023.\n[78] Petar Veli\u02c7ckovi\u00b4c and Charles Blundell. Neural algorithmic reasoning. Patterns, 2(7):100273,\n2021.\n[79] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani\nYogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto,\nOriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language\nmodels. Transactions on Machine Learning Research, 2022. Survey Certification.\n[80] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi,\nQuoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems, volume 35, pages 24824\u201324837.\nCurran Associates, Inc., 2022.\n[81] Sean Welleck, Jiacheng Liu, Ximing Lu, Hannaneh Hajishirzi, and Yejin Choi. NaturalProver:\nGrounded mathematical proof generation with language models.\nIn Advances in Neural\nInformation Processing Systems, volume 35, pages 4913\u20134927. Curran Associates, Inc., 2022.\n[82] Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and\nYejin Choi. Generating sequences by learning to self-correct. In International Conference on\nLearning Representations, 2023.\n[83] Sean Welleck, Peter West, Jize Cao, and Yejin Choi. Symbolic brittleness in sequence models:\non systematic generalization in symbolic mathematics. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 8629\u20138637, 2022.\n[84] Yi Zhang, Arturs Backurs, S\u00e9bastien Bubeck, Ronen Eldan, Suriya Gunasekar, and Tal Wagner.\nUnveiling transformers with LEGO: a synthetic reasoning task. CoRR, abs/2206.04301, 2022.\n[85] Haoyi Zheng and Huichun Zhan. ChatGPT in scientific writing: a cautionary tale. The American\nJournal of Medicine, 2023.\n[86] Hattie Zhou, Azade Nova, Aaron Courville, Hugo Larochelle, Behnam Neyshabur, and Hanie\nSedghi. Teaching algorithmic reasoning via in-context learning, 2023.\n17\nAppendices\nA Compositional Tasks\n19\nA.1\nMultiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n19\nA.2\nEinstein\u2019s Puzzle\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nA.3\nDynamic Programming Problem . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nB\nExperimental Setups & Empirical Results\n24\nB.1\nModels\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nB.2\nLimits of Transformers in Zero- and Few-shot Settings . . . . . . . . . . . . . . .\n24\nB.3\nLimits of Transformers with question-answer Training\n. . . . . . . . . . . . . . .\n24\nB.4\nLimits of Transformers with Explicit Scratchpad Training . . . . . . . . . . . . . .\n26\nB.5\nLimits of Transformers with Explicit Scratchpad Prompting . . . . . . . . . . . . .\n26\nC Surface Patterns\n29\nC.1\nRelative Information Gain Predictions for Multiplication . . . . . . . . . . . . . .\n29\nC.2\nEmpirical Surface Pattern Analysis for Multiplication with GPT4, ChatGPT and GPT3 30\nC.3\nRelative Information Gain Predictions for Dynamic Programming Task\n. . . . . .\n31\nC.4\nEmpirical Surface Pattern Results for Dynamic Programming Task . . . . . . . . .\n33\nD Theoretical Results: Derivations\n35\nD.1\nError accumulates with larger parallel applications of an estimated function (width)\n35\nD.2\nError accumulates with larger iterative applications of an estimated function (depth)\n36\nD.3\nDiscussing c \u226a \u03f5 in the context of Proposition 4.2 . . . . . . . . . . . . . . . . . .\n38\nE Additional Literature and Societal Impact\n40\nE.1\nAdditional Literature . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\nE.2\nSocietal Impact Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n40\n18\nA\nCompositional Tasks\nA.1\nMultiplication\nData Construction\nWe exhaustively generate multiplication problems as question-answer pairs\n(e.g., Q: \u201cWhat is 4 times 32?\u201d A: \u201c128\u201d). We focus on multiplications of two numbers x =\n(x1, x2, . . . , xk) and y = (y1, y2, . . . , yk) where each number can have up to k digits, amounting to\n9 \u00d7 10(k\u22121) combinations per each number. We set k to 5 in our experiments. Figure 8 showcases\nan example prompt for performing few-shot learning without the inclusion of a scratchpad, while\nFigure 9 demonstrates an example prompt using a scratchpad. Throughout our experimentation,\nwe explored various versions of the scratchpad, ranging from verbose and detailed to more concise\nalternatives. Among these variations, the scratchpad version depicted in Figure 9 ultimately produced\nthe most favorable outcomes. Listing 1 shows the Python code for solving the task.\nTo multiply two numbers, start by multiplying the rightmost digit of the\nmultiplicand by each digit of the multiplier, writing down the products and\ncarrying over any remainders.\nRepeat this process for each digit of the\nmultiplicand, and then add up all the partial products to obtain the final\nresult.\nQuestions: what's 22 times 2? Answer 44.\nFigure 8: Example prompt for the multiplication task used for the few-shot setting.\nQuestion: What is 35 times 90?\nScratchpad: Let's perform the multiplication step by step:\nLet's multiply 35 by the digit in the ones place of 90, which is 0.\n1. Multiply 0 by the digit in the ones place of 35, which is 5. This gives 5 x 0\n= 0. Write down the result 0.\n2. Multiply 0 by the digit in the tens place of 35, which is 3. This gives 3 x 0\n= 0. Write down the result 0.\n3. The partial product for this step is A=0 which is the concatenation of the\ndigits we found in each step.\nNow, let's multiply 35 by the digit in the tens place of 90, which is 9.\n4. Multiply 9 by the digit in the ones place of 35, which is 5. This gives 5 x 9\n= 45. Write down the result 5 and carry over the 4 to the next step.\n5. Multiply 9 by the digit in the tens place of 35, which is 3. Add the carryover\nfrom the previous step to account for this. This gives (3 x 9) + 4 = 31. Write\ndown the result 31.\n6. The partial product for this step is B=315 which is the concatenation of the\ndigits we found in each step.\nNow, let's sum the 2 partial products A and B, and take into account the position\nof each digit: A=0 (from multiplication by 0) and B=315 (from multiplication by 9\nbut shifted one place to the left, so it becomes 3150). The final answer is 0 x 1\n+ 315 x 10 = 0 + 3150 = 3150.\nFigure 9: A sample scratchpad for the multiplication task.\n1 def\nmultiply(x, y):\n2\nsummands = [0] * len(y)\n3\nfor i in range(len(y) - 1, -1,\n-1):\n19\n4\ndigits = [0] * len(x)\n5\ncarry = 0\n6\nfor j in range(len(x) - 1, -1,\n-1):\n7\nt = x[j] * y[i]\n8\nt += carry\n9\ncarry = t // 10\n10\ndigits[j] = t %\n11\ndigits.insert (0, carry)\n12\nsummands[i] = sum(digits[-k] * (10 ** (k - 1)) for k in range\n(1, len(digits) + 1))\n13\n14\nproduct = sum(summands[-i] * (10 ** (i - 1)) for i in range (1, len\n(y) + 1))\n15\nreturn\nproduct\nListing 1: Example Python code for solving the multiplication task.\nA.2\nEinstein\u2019s Puzzle\nData Construction\nIn our experiments, we initially establish a set of properties, such as Color,\nPhoneModel, Pet, and so forth, along with their corresponding values expressed in natural language\ntemplates (e.g., \u201cThe house has a red color.\u201d). We then devise a fundamental and straightforward\nset of clue types: 1) \u2018found_at\u2019, e.g., \u201cAlice lives in House 2\u201d, 2) \u2018same_house\u2019, e.g., \u201cThe person\nwho is a cat lover lives in the house that has a red color.\u201d, 3) \u2018direct_left\u2019, e.g., \u201cThe person who\nhas a dog as a pet lives to the left of the person who lives in a red house.\u201d, and 4) \u2018besides\u2019, e.g.,\n\u201cThe person who has a dog as a pet and the person who has a red house live next to each other.\u201d In\naddition, we also set up harder clue types such as \u2018not_at\u2019, \u2018left_of\u2019 (not necessarily directly left of),\n\u2018two_house_between\u2019, etc. which are only used in auxiliary experiments.\nThe solution to the puzzle is a matrix of size K \u00d7 M, where K represents the number of houses and\nM the number of attributes. During the puzzle generation, the M properties are randomly selected\nfrom the candidate pool, followed by the random sampling of K values for each property. The\nsampled values are then randomly permuted and assigned within the table to create the solution. It is\nimportant to note that we ensure one of the sampled properties is \u2018Name\u2019 to enhance the readability\nand comprehensibility of the puzzles. To construct the clues, we initially over-generate all valid clues\nbased on the solution and subsequently remove redundant clues at random until we obtain a set with a\nAlgorithm 1 Puzzle Solver\nInput: Clues\nOutput: Reasoning path\n1: function PuzzleSolver(Clues)\n2:\nPath  []\n3:\nLeftClues  clues\n4:\nwhile |LeftClues| 6= 0 do\n5:\nfor i=1 to\n|LeftClues| do\n6:\nCandidateClues =\n!|LeftClues|\ni\n\"\n7:\nfor clue in CandidateClues do\n8:\nif solve any cell then\n9:\nLeftClues.remove(clue)\n10:\nPath.append(clue)\n11:\nreturn Path\nReasoning Path Generation\nGeneral Unique Rules\nThere are 3 houses (numbered 1 on the left, 3 on the right). Each has a different person in them. They have different characteristics:\n- Each person has a unique name: peter, eric, arnold\n- People have different favorite sports: soccer, tennis, basketball\n- People own different car models: tesla, ford, camry\nClues\n1. The person who owns a Ford is the person who loves tennis.\n2. Arnold is in the third house.\n3. The person who owns a Camry is directly left of the person who owns a Ford.\n4. Eric is the person who owns a Camry.\n5. The person who loves basketball is Eric.\n6. The person who loves tennis and the person who loves soccer are next to each other.\nHouse\nName\nSports\nCar\n1\nEric\nBasketball\nCamry\n2\nPeter\nTennis\nFord\n3\nArnold\nSoccer\nTesla\nGround-Truth Table\nClue #5\nClue #6\nClue #4\nName\nSports\nCar\nEric\nBasketball\nCamry\nPeter\nTennis\nArnold\nSoccer\n<UniqueValues>\n\u2026\nName\nSports\nEric\nBasketball\nPeter\nTennis\nArnold\nSoccer\nName\nEric\nPeter\nArnold\nClue #2\nFigure 10: A sample of the puzzle task and the reasoning path to reach a solution.\n20\nThis is a logic puzzle. There are 3 houses (numbered 1 on the left, 3 on the\nright). Each has a different person in them. They have different characteristics:\n- Each person has a unique name: peter, eric, arnold\n- People have different favorite sports: soccer, tennis, basketball\n- People own different car models: tesla model 3, ford f150, toyota camry\n1. The person who owns a Ford F-150 is the person who loves tennis.\n2. Arnold is in the third house.\n3. The person who owns a Toyota Camry is directly left of the person who owns a\nFord F-150.\n4. Eric is the person who owns a Toyota Camry.\n5. The person who loves basketball is Eric.\n6. The person who loves tennis and the person who loves soccer are next to each\nother.\nLet's think step by step. Please first briefly talk about your reasoning and show\nyour final solution by filling the blanks in the below table.\n$ House: ___ $ Name: ___ $ Sports: ___ $ Car: ___\n$ House: ___ $ Name: ___ $ Sports: ___ $ Car: ___\n$ House: ___ $ Name: ___ $ Sports: ___ $ Car: ___\nReasoning:\nStep 1: First apply clue <Arnold is in the third house.> We know that The Name in\nhouse 3 is arnold.\nStep 2: Then combine clues: <The person who loves tennis and the person who loves\nsoccer are next to each other.> <The person who loves basketball is Eric.>\nUnique Values Rules and the fixed table structure. We know that The Name in house\n1 is eric. The FavoriteSport in house 1 is basketball. The Name in house 2 is\npeter.\nStep 3: Then apply clue <Eric is the person who owns a Toyota Camry.> We know\nthat The CarModel in house 1 is toyota camry.\nStep 4: Then apply clue <The person who owns a Toyota Camry is directly left of\nthe person who owns a Ford F-150.> and Unique Values We know that The CarModel in\nhouse 2 is ford f150. The CarModel in house 3 is tesla model 3.\nStep 5: Then apply clue <The person who owns a Ford F-150 is the person who loves\ntennis.> and Unique Values We know that The FavoriteSport in house 2 is tennis.\nThe FavoriteSport in house 3 is soccer.\nThe puzzle is solved.\nFinal solution:\n$ House: 1 $ Name: Eric\n$ Sports: Basketball $ Car: Camry\n$ House: 2 $ Name: Peter\n$ Sports: Tennis\n$ Car: Ford\n$ House: 3 $ Name: Arnold $ Sports: Soccer\n$ Car: Tesla\nFigure 11: A sample scratchpad for the puzzle task.\nunique solution, as previously sampled. This process ensures a coherent and engaging puzzle-solving\nexperience. Refer to Figure 10 for an example.\nGraph Construction Algorithm\nTo solve the complex compositional reasoning process for a\nlogical grid puzzle, we use existing puzzle solvers [21] to generate the computation graph. It follows\nthe basic greedy principle of applying the minimum number of rules to solve any cell, i.e., if using\nonly one rule to solve any given cell, then apply this rule. This algorithm iterates through all clues in\nthe clue set until one or a set of clue combinations can solve any cell in the table. While it may not be\nthe most efficient way to solve the puzzle, it provides models with explicit scratchpad verbalization\nthrough an intuitive computation graph. Refer to Figure 10 for the pseudo-code of the process, and\nFigure 11 for a scratchpad example.\n21\nA.3\nDynamic Programming Problem\nA.3.1\nSolution to this problem\nLet a = [a1, . . . , an] be an input. Let dpi be the maximum sum of a subsequence that does not\ninclude adjacent elements, when considering only the elements of the input from the i-th position\nonwards.\nTrivially, dpn = max(an, 0) since we only want to choose a number if it is non-negative. Moreover,\ndpn\u22121 = max(an, an\u22121, 0) since we cannot choose adjacent numbers.\nFor any given dpi with i \u2264 n \u2212 2, we can express it in terms of dpi+1 and dpi+2. Concretely, the\nmaximum sum of a subsequence starting at position i may or may not include the element in the i-th\nposition, ai. If the subsequence includes ai, then the maximum sum is ai + dpi+2, since using ai\nblocks us from using the next element. If the subsequence does not include ai, then its sum is dpi+1.\nMoreover, the answer may never be less than zero, because otherwise we would select the empty\nsequence7. In summary,\ndpi = max(dpi+1, ai + dpi+2, 0)\nWe now have a recursion with its base cases dpn = max(an, 0) and dpn\u22121 = max(an, an\u22121, 0), and\nwe can therefore compute all values in O(n). It now only rests to reconstruct the lexicographically\nsmallest subsequence that maximizes the desired sum, based solely on the computed dp values.\nStarting from dp1 and iterating sequentially through dpn\u22122, we choose an item if and only if\ndpi = ai + dpi+2 (that is, the maximum sum comes from choosing the current element) and we have\nnot chosen the previous element. This helps disambiguate cases where choosing or not choosing\nai yields the same sum, but possibly only one of those will not incur in choosing adjacent numbers.\nSimilarly, for positions i = n \u2212 1 and i = n we choose the element if dpi = ai (that is, choosing the\nelement yields the maximum sum) and we have not chosen the immediately previous element. See an\nexample Python solution in 2.\nGiven a sequence of integers, find a subsequence with the highest sum, such that\nno two numbers in the subsequence are adjacent in the original sequence.\nOutput a list with \"1\" for chosen numbers and \"2\" for unchosen ones. If multiple\nsolutions exist, select the lexicographically smallest. input = [3, 2, 1, 5, 2].\nFigure 12: Example prompt for the DP task, used for zero-shot and few-shot settings.\n1 def\nmaximum_sum_nonadjacent_subsequence (arr):\n2\n3\nN = len(arr)\n4\ndp = [0 for _ in range(N)]\n5\n6\ndp[N - 1] = max(arr[N - 1], 0)\n7\ndp[N - 2] = max(max(arr[N - 1], arr[N - 2]), 0)\n8\n9\nfor i in range(N - 3, -1,\n-1):\n10\ndp[i] = max(max(dp[i + 1], arr[i] + dp[i + 2]), 0)\n11\n12\n# reconstruct\nthe answer\nwith a fixed -size\ngraph\n13\nresult = []\n14\ncan_use_next_item = True\n15\n16\nfor i in range(N - 2):\n17\nif dp[i] == arr[i] + dp[i + 2] and\ncan_use_next_item :\n18\nresult.append (1)\n19\ncan_use_next_item = False\n20\nelse:\n7We don\u2019t need to explicitly check for this since dpn \u2265 0. However, we include the condition to ease the\nscratchpad logic.\n22\n21\nresult.append (2)\n22\ncan_use_next_item = True\n23\n24\nif dp[N - 2] == arr[N - 2] and\ncan_use_next_item :\n25\nresult.append (1)\n26\ncan_use_next_item = False\n27\nelse:\n28\nresult.append (2)\n29\ncan_use_next_item = True\n30\n31\nif dp[N - 1] == arr[N - 1] and\ncan_use_next_item :\n32\nresult.append (1)\n33\nelse:\n34\nresult.append (2)\n35\n36\nreturn\nresult\nListing 2: Example Python code for solving the DP task. We chose this implementation because the\ncomputation graph has always the same topology for any given input length.\nQuestion: Let's solve input = [3, 2, 1, 5, 2].\nScratchpad: dp[4] = max(input[4], 0) = max(2, 0) = 2\ndp[3] = max(input[3], input[4], 0) = max(5, 2, 0) = 5\ndp[2] = max(dp[3], input[2] + dp[4], 0) = max(5, 1 + 2, 0) = 5\ndp[1] = max(dp[2], input[1] + dp[3], 0) = max(5, 2 + 5, 0) = 7\ndp[0] = max(dp[1], input[0] + dp[2], 0) = max(7, 3 + 5, 0) = 8\nFinally, we reconstruct the lexicographically smallest subsequence that fulfills\nthe task objective by selecting numbers as follows. We store the result on a list\nnamed \"output\".\nLet can_use_next_item = True.\nSince dp[0] == input[0] + dp[2] (8 == 3 + 5) and can_use_next_item == True, we\nstore output[0] = 1. We update can_use_next_item = False.\nSince dp[1] != input[1] + dp[3] (7 != 2 + 5) or can_use_next_item == False, we\nstore output[1] = 2. We update can_use_next_item = True.\nSince dp[2] != input[2] + dp[4] (5 != 1 + 2) or can_use_next_item == False, we\nstore output[2] = 2. We update can_use_next_item = True.\nSince dp[3] == input[3] (5 == 5) and can_use_next_item == True, we store\noutput[3] = 1. We update can_use_next_item = False.\nSince dp[4] != input[4] (2 != 2) or can_use_next_item == False, we store\noutput[4] = 2.\nReconstructing all together, output=[1, 2, 2, 1, 2].\nFigure 13: A sample scratchpad for the DP task used for fine-tuning with few-shot settings.\nData Construction\nWe exhaustively generate data for this DP task. For question-answer setting,\nwe include a thorough explanation of the task before asking to generate a solution (see Figure 12).\nWe use all lists up to 5 elements as training, and we consider only lists where elements are in the\nrange [\u22125, 5] (giving a total of 11n lists for an input list of size n). For out-of-domain evaluation, we\nuse lists of sizes 6 to 10 inclusive. Example scratchpads and zero-shot prompts are shown in Figure\n13 and 12 respectively. The scratchpad is generated automatically through templates. We considered\nfive exemplars for the few-shot setup.\n23\nB\nExperimental Setups & Empirical Results\nB.1\nModels\nFor our experiments, we evaluate the performance of 6 LLMs: GPT4 (gpt-4) [58], ChatGPT\n(GPT3.5-turbo) [57], GPT3 (text-davinci-003) [11], FlanT5 [17] and LLaMa [75]. The evalua-\ntions were conducted from January 2023 to May 2023 using the OpenAI API. We perform fine-tuning\non GPT3 (text-davinci-003) for the three tasks, observing faster convergence when training on\nquestion-scratchpad pairs rather than question-answer pairs. For question-answer pairs fine-tuning,\nwe train separately the model for {14, 12, 4} epochs for multiplication, puzzle, and DP respectively,\nsaving the best model based on the validation set. Regarding training on question-scratchpad pairs,\nwe train the model for {16, 8, 2} epochs for multiplication, puzzle, and DP. The batch size is set\nto approximately 0.2% of the number of examples in the training set. Generally, we observe that\nlarger batch sizes tend to yield better results for larger datasets. For the learning rate multiplier, we\nexperiment with values ranging from 0.02 to 0.2 to determine the optimal setting for achieving the\nbest results and chose 0.2. During inference, we set nucleus sampling p to 0.7 and temperature to 1.\nFor each task, we evaluate the performance of each model on 500 test examples.\nB.2\nLimits of Transformers in Zero- and Few-shot Settings\nFigure 15, Figure 17 and Figure 20 show the zero-shot performance of GPT4, ChatGPT, LLaMA and\nFlanT5 on the three tasks. Overall, there is a notable decline in performance as the task complexity\nincreases (measured by graph parallelism for multiplication and DP, and propagation steps for puzzles\nas shown in Figure14). The few-shot performance with question-answer pairs results in minimal\nimprovement over the zero-shot setting as depicted in Figure 16 and Figure 20 for the multiplication\nand DP tasks. In contrast, the few-shot setting did not lead to any improvement in the puzzle task.\nFigure 14: Graph parallelism vs accuracy. The accuracy decreases as the complexity increases.\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n0.99\n0.85\n0.55\n0.98\n0.51\n0.11\n0.01\n0.98\n0.26\n0.02\n0\n0\nChatGPT zero-shot\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.88\n0.97\n0.41\n0.09\n0.85\n0.28\n0.04\n0\n0.76\n0.1\n0\n0\n0\nGPT3 zero-shot\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n0.6\n0.5\n0.42\n0.32\n0.19\n0.17\n0.22\n0\n0\n0\n0\n0\n0\n0\n0\nLLaMA zero-shot (13B)\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n0.96\n0.41\n0.05\n0.15\n0.02\n0\n0.15\n0\n0\n0\n0.07\n0\n0\n0\n0\nFlanT5-XXL zero-shot (11B)\nFigure 15: Zero-shot accuracy. Performance of ChatGPT, GPT3, LLaMA and FlanT5 on the multiplication\ntask.\nB.3\nLimits of Transformers with question-answer Training\nFigure 18 and Figure 21 show the performance of GPT3 finetuned on question-answer pairs. The\nmodel was trained on various splits, considering the problem size, depth, and width of the computation\ngraph. Specifically, for the multiplication task, the model was fine-tuned on a range of multiplication\nproblems, spanning from 1-digit by 1-digit multiplication to 4-digit by 2-digit multiplication amount-\ning to 1.8M pairs. As for the puzzle task, the model was fine-tuned on puzzles of sizes ranging from\n2x2 to 4x4 resulting in a total of 142k pairs. Additionally, for the DP task, the model was fine-tuned\non problems with a sequence length of 5 resulting in 41K pairs. In an additional setup, we divided\n24\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.98\n0.97 0.88 0.56\n0.93 0.76 0.18 0.03\n0.95 0.42 0.07\n0\n0\nGPT4 few-shot\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n0.99 0.82 0.58\n0.94 0.45 0.08 0.01\n0.96 0.28 0.02\n0\n0\nChatGPT few-shot\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.88\n0.97 0.49 0.09\n0.88 0.21 0.04\n0\n0.8\n0.17 0.03\n0\n0\nGPT3 few-shot\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n0.61\n0.52 0.42\n0.31 0.19 0.18\n0.22\n0\n0\n0\n0\n0\n0\n0\n0\nLLaMA few-shot (13B)\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n0.96\n0.42 0.05\n0.14 0.02\n0\n0.16\n0\n0\n0\n0.08\n0\n0\n0\n0\nFlanT5-XXL few-shot (11B)\nFigure 16: Few-shot accuracy with question-answer pairs. Performance of GPT4, ChatGPT, GPT3, LLaMA\nand FlanT5 on the multiplication task.\n2\n3\n4\n5\n6\n2\n3\n4\n5\n6\n0.8\n0.4\n0.1\n0.4\n0\n0.5\n0\n0.2\n0\n0\n0.1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nChatGPT zero-shot (Puzzle)\n2\n3\n4\n5\n6\n2\n3\n4\n5\n6\n0.6\n0.5\n0.2\n0.1\n0\n0.5\n0\n0.2\n0\n0\n0.1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nGPT3 zero-shot (Puzzle)\n2\n3\n4\n5\n6\n2\n3\n4\n5\n6\n0.44\n0.25\n0.09\n0.08\n0\n0.1\n0.02\n0.03\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nLLaMa zero-shot (Puzzle)\n2\n3\n4\n5\n6\n2\n3\n4\n5\n6\n0.41\n0.22\n0.02\n0.02\n0\n0.04\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\nFlanT5 zero-shot (Puzzle)\nFigure 17: Zero-shot accuracy. Performance of ChatGPT, GPT3, LLaMA and FlanT5 on the puzzle task.\nFew-shot performance led to worse performance.\nFigure 18: GPT3 finetuned on the puzzle task using question-answer pairs. The training data\nconsisted of puzzles of size 4x4, and the model was subsequently evaluated on larger puzzle sizes for\nOOD testing.\nthose datasets based on the depth and width of the computation graph for all the tasks and finetuned\non different splits. The results indicate a lack of generalization for out-of-domain (OOD) examples\nwhile showcasing near-perfect performance for in-domain examples. One hypothesis on why the\nmodel exhibit such a poor generaliztion is tokenization. So we train GPT2-XL from scratch on up to\n4x4 (90M data points), we assign each digit to one token and each math symbol as well. However,\nthe performance is still low and GPT2-XL fails to answer correctly 3x3 test examples.\nGPT3 finetuning cost\nWe will discuss here the approximate cost of fine-tuning GPT3 for the\nmultiplication task. When fine-tuning with question-answer pairs, each example typically consists\nof around 20 tokens, and 250 tokens for question-scratchpad pairs. The cost for utilizing the\ntext-davinci-003 model amounts to $0.02 (USD) per 1,000 tokens. With this particular setup,\nthe total number of training examples required for multiplication up to 5 digits by 5 digits reaches\nan astonishing figure of approximately 9.1 billion examples. Should we choose to fine-tune GPT3\nfor 4 epochs on question-answer pairs, the cost would amount to $12 million and $700 million for\nquestion-scratchpad training. For a more comprehensive breakdown of the cost per problem size,\nplease refer to Table 1.\n25\nFigure 19: GPT3 finetuned on the multiplication task using question-answer pairs\n2\n4\n6\n8\n10\nProblem Size\n0.2\n0.4\n0.6\n0.8\nAccuracy\nGPT4 (Dynamic Programming)\nGPT4 zero-shot\nGPT4 five-shot\n2\n4\n6\n8\n10\nProblem Size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nChatGPT (Dynamic Programming)\nChatGPT zero-shot\nChatGPT five-shot\n2\n4\n6\n8\n10\nProblem Size\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\nGPT3 (Dynamic Programming)\nGPT3 zero-shot\nGPT3 five-shot\nFigure 20: Zero-shot and Few-shot accuracy using question-answer pairs. Performance of GPT4, ChatGPT,\nand GPT3 on the dynamic programming task. LLaMA and FlanT5 results are near zero for all problem sizes.\nB.4\nLimits of Transformers with Explicit Scratchpad Training\nFigure 23: GPT3 finetuned exhaustively on task-\nspecific data up to a certain problem size. In partic-\nular, we train on examples up to 3-digit by 2-digit\nmultiplication (left) and on examples that have up\nto 5 digits in the output response (right). The blue\nregion represents the in-distribution examples and\nthe red region refers to OOD examples.\nFigure 23, 24, 22 show the performance of GPT3 fine-\ntuned on different splits of the tasks using question-\nscratchpad pairs. Specifically, for the multiplication\ntask, the model was fine-tuned on a range of multi-\nplication problems, spanning from 1-digit by 1-digit\nmultiplication to 3-digit by 2-digit multiplication.\nAs for the puzzle task, the model was fine-tuned on\npuzzles of sizes ranging from 2x2 to 4x4. Addition-\nally, for the DP task, the model was fine-tuned on\nproblems with a sequence length of 5. Furthermore,\ndifferent data splits were considered, including varia-\ntions based on the number of hours, number of prop-\nerties, depth and width of the graph, and the number\nof digits in the multiplication output. On all tasks,\nwe can see that the model fails to generalize to OOD\ndata while achieving perfect accuracy on in-domain\ndata, indicating that it cannot learn the underlying computational rules.\nB.5\nLimits of Transformers with Explicit Scratchpad Prompting\nFigure 25 shows the results. GPT-4 exhibits an increase in few-shot accuracy in most problem\nsizes when using question-scratchpad pairs of few-shot examples across the three tasks. While its\nperformance surpasses that of zero-shot and few-shot with question-answer pairs, it tends to decline\nas the complexity of the tasks increases. The same applies for the rest of the models.\n26\nFigure 21: GPT3 finetuned on the dynamic programming task using question-answer pairs. We\nconsider different data splits: problem size, depth, and width of the graph. Specifically, the model\nwas trained with a problem size of 5, and the graph\u2019s depth and width were set to 18.\nFigure 22: GPT3 finetuned on the dynamic programming task using question-scratchpad pairs.\nWe consider different data splits: problem size, depth, and width of the graph. Specifically, the model\nwas trained with a problem size of 5, and the graph\u2019s depth and width were set to 18.\nFigure 24: GPT3 finetuned on the puzzle task using question-scratchpad pairs. The training data\nconsisted of puzzles of size 4x4, and the model was subsequently evaluated on larger puzzle sizes for\nOOD testing.\n27\nProblem size\n# examples\nGPT3 Cost\nwithout scratchpad\nwith scratchpad\n1 x 1\n81\n$0.12\n$7.44\n2 x 1\n810\n$1.28\n$74.4\n2 x 2\n8100\n$12.96\n$744\n3 x 1\n8100\n$12.96\n$744\n3 x 2\n81000\n$129.6\n$7440\n3 x 3\n810000\n$1296\n$74,404\n4 x 1\n81000\n$129.6\n$7440\n4 x 2\n810000\n$1296\n$74,404\n4 x 3\n8100000\n$12,960\n$744,040\n4 x 4\n81000000\n$129,600\n$7,440,400\n5 x 1\n810000\n$1296\n$74,404\n5 x 2\n8100000\n$12,960\n$744,040\n5 x 3\n81000000\n$129,600\n$7,440,400\n5 x 4\n810000000\n$1,296,000\n$70,440,400\n5 x 5\n8100000000\n$12,960,000\n$700,440,400\nTable 1: Finetuning cost of GPT3 model on the multiplication data.\nFigure 25: GPT4 few-shot accuracy with question-scratchpad pairs on the 3 tasks. The performance\nimproves for most problem sizes compared to zero-shot performance and few-shot using question-\nanswer pairs but degrades to zero as the complexity increases.\n28\nC\nSurface Patterns\nC.1\nRelative Information Gain Predictions for Multiplication\nRelative Information Gain\nInput\nvariable\nOutput\nvariable\n2x2\n3x3\n4x4\n5x5\nxn\nz2n\n0.223\n0.223\n0.223\n0.223\nyn\nz2n\n0.223\n0.223\n0.223\n0.223\nx1\nz1\n0.198\n0.199\n0.199\n0.199\ny1\nz1\n0.198\n0.199\n0.199\n0.199\nxn yn\nz2n\n1.000\n1.000\n1.000\n1.000\nxn\u22121 xn\nz2n\n0.223\n0.223\n0.223\n0.223\nyn\u22121 yn\nz2n\n0.223\n0.223\n0.223\n0.223\nxn yn\nz2n\u22121\n0.110\n0.101\n0.101\n0.101\nyn\u22121 yn\nz2n\u22121\n0.032\n0.036\n0.036\n0.036\nxn\u22121 xn\nz2n\u22121\n0.032\n0.036\n0.036\n0.036\nxn\u22121 yn\u22121\nz2n\u22121\n0.018\n0.025\n0.025\n0.025\nx1 y1\nz2\n0.099\n0.088\n0.088\n0.088\nx2 y2\nz2\n0.025\n0.016\n0.016\n0.016\nx1 y1\nz1\n0.788\n0.792\n0.793\n0.793\ny1 y2\nz1\n0.213\n0.211\n0.211\n0.211\nx1 x2\nz1\n0.213\n0.211\n0.211\n0.211\nTable 2: Highest Relative Information Gain Elements and Pairs of Elements, for multiplications\nbetween x = (x1, . . . , xn) and y = (y1, . . . , yn), with 2 \u2264 n \u2264 5. We define z := x \u00b7 y, which\nwill always have size 2n (with possibly a leading zero). z2n denotes the least-significant digit of\nz, and z1 denotes the left-most digit. Only (input, output) pairs above 0.01 are shown. Note that\nsince multiplication is commutative, several pairs of input variables (e.g. a0 and b0) exhibit the same\nrelative information gain.\n29\nC.2\nEmpirical Surface Pattern Analysis for Multiplication with GPT4, ChatGPT and GPT3\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n0.99\n0.98\n1\n0.99\n0.98\n0.97\n0.97\nAccuracy Trailing 0\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n0.99\n1\n1\n0.99\n0.94\n1\n1\n1\n0.98\n0.99\n1\n1\nAccuracy last digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n0.98\n0.98\n0.72\n0.98\n0.98\n0.68\n0.58\n1\n0.88\n0.61\n0.63\n0.63\nAccuracy last two digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0.99\n1\n1\n1\nAccuracy # digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\nAccuracy first digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n0.99\n0.99\n0.99\n0.99\n0.99\n1\n0.99\n0.99\n0.98\n0.98\nAccuracy first two digits\nFigure 26: GPT4 zero-shot accuracy in predicting partially correct responses. This evidences surface\npattern learning, since the accuracy of full answer prediction is significantly lower\u2013and often near\nzero (see Figure 2). Specifically, \u2018accuracy trailing zeros\u2019 pertains to accurately predicting the number\nof zeros in the output number, which is known to be relatively easy to predict based on arithmetic\ncalculations.\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n0.99\n1\n0.98\n0.93\n0.95\n1\n0.98\n0.96\n0.87\n0.87\nAccuracy Trailing 0\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n0.99\n1\n1\n1\n0.99\n0.98\n0.97\n1\n0.98\n0.95\n0.9\n0.9\nAccuracy last digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n1\n0.93\n0.91\n1\n0.89\n0.88\n0.87\n1\n0.88\n0.83\n0.8\n0.8\nAccuracy last two digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n0.99\n1\n1\n1\nAccuracy # digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n0.99\n0.99\n1\n0.98\n0.97\n0.98\n1\n0.98\n0.92\n0.86\n0.86\nAccuracy first digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n0.96\n0.97\n1\n0.94\n0.95\n0.94\n1\n0.95\n0.91\n0.84\n0.84\nAccuracy first two digits\nFigure 27: ChatGPT zero-shot accuracy in predicting partially correct responses. We observe the\nsame trend for GPT3 predictions.\n30\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n0.97\n0.98\n1\n0.97\n0.99\n0.98\n0.98\nAccuracy Trailing 0\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n0.99\n0.99\n1\n0.99\n0.99\n0.96\n1\n1\n0.9\n0.98\n1\n0.86\nAccuracy last digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n0.97\n0.89\n0.69\n0.96\n0.97\n0.53\n0.56\n0.99\n1\n1\n1\n0.63\nAccuracy last two digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n1\n1\n1\n1\n1\n0.99\n1\n1\n0.98\n0.99\n1\n0.9\nAccuracy # digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n1\n0.99\n0.99\n1\n0.98\n1\n0.98\n1\nAccuracy first digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n0.99\n0.99\n0.99\n0.99\n0.99\n1\n0.99\n0.99\n0.98\n0.98\nAccuracy first two digits\nFigure 28: GPT4 five-shot accuracy in predicting partially correct responses. We observe the same\ntrend for ChatGPT, GPT3 few-shot predictions.\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.98\n1\n1\n1\n0.76\n0.8\n0.65\n0.8\n0.85\n0.92\n0.98\n0.8\n0.75\nAccuracy Trailing 0\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n1\n1\n1\n0.98\n1\n1\n0.93\n1\n0.98\n0.98\n0.91\n0.64\nAccuracy last digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.94\n1\n0.99\n1\n0.96\n1\n1\n0.94\n0.43\n0.53\n0.17\n0.18\n0.1\nAccuracy last two digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n0.99\n1\n1\n1\n0.12\n0.54\n0\n0\n0.02\n0.02\n0\n0\n0\nAccuracy # digits\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n1\n1\n0.86\n0.97\n0.64\n0.84\n0.74\n0.69\n0.62\n0.5\nAccuracy first digit\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n1\n1\n1\n1\n0.5\n0.32\n0.69\n0.1\n0.01\n0.43\n0.31\n0.16\n0.08\n0.12\nAccuracy first two digits\nFigure 29: GPT3 finetuned on question-scratchpad pairs. Accuracy of predicting partially correct\nresponses.\nC.3\nRelative Information Gain Predictions for Dynamic Programming Task\nLet ai be the i-th element of the input sequence, and let oi be the i-th element of the output sequence.\nAs shown in Table 3, ai is a good predictor of oi, and this is especially true for a1 and an\u22121, the\nfirst and last elements of the sequence. This matches the task intuition, since one would never pick\nan element ai < 0 and decrease the final sum (one may pick ai = 0 if it makes a lexicographically\nsmaller output sequence).\nai weakly helps to predict its neighbors. The only case of this behavior with RelativeIG>0.1 is at\nthe start of the sequence, where the first element helps predict the value of the second. This again\nmatches intuition, since a very high a1 indicates that with high probability o2 will not be selected for\nthe final subsequence.\n31\nRelative Information Gain for each problem size\nInput\nvariable\nOutput\nvariable\n2\n3\n4\n5\n6\n7\n8\n9\n10\na1\no2\n0.15\n0.13\n0.14\n0.14\n0.14\n0.14\n0.14\n0.14\n0.14\na1\no1\n0.64\n0.71\n0.69\n0.69\n0.69\n0.69\n0.69\n0.69\n0.69\na2\no2\n0.53\n0.42\n0.45\n0.44\n0.45\n0.44\n0.44\n0.45\n0.44\na3\no3\n0.64\n0.49\n0.53\n0.52\n0.52\n0.52\n0.52\n0.52\na4\no4\n0.60\n0.46\n0.50\n0.49\n0.49\n0.49\n0.49\na5\no5\n0.62\n0.47\n0.51\n0.50\n0.50\n0.50\na6\no6\n0.61\n0.47\n0.51\n0.49\n0.50\na7\no7\n0.61\n0.47\n0.51\n0.50\na8\no8\n0.61\n0.47\n0.51\na9\no9\n0.61\n0.47\na10\no10\n0.61\nan\u22121\non\u22121\n0.64\n0.60\n0.62\n0.61\n0.61\n0.61\n0.61\n0.61\nan\u22122\non\u22122\n0.46\n0.47\n0.47\n0.47\n0.47\n0.47\nan\u22123\non\u22123\n0.51\n0.51\n0.51\n0.51\nan\u22124\non\u22124\n0.49\n0.50\nTable 3: Highest Relative Information Gain Elements, for DP problems of size 2 \u2264 n \u2264 10.\nWe only show the (input, output) pairs where at least three problem sizes have RelativeIG>0, and\nat least one with RelativeIG>0.1. an\u22121 refers to the last element of the sequence, regardless of its\nactual id in the sequence.\nSimilar behaviors, but with higher relative information gains overall, are observed when analyzing\ntriples of consecutive elements in the list. Table 4 shows that oi is highly predicted by (ai\u22121, ai, ai+1).\nMoreover, oi is highly predicted by both (ai\u22122, ai\u22121, ai) and (ai, ai+1, ai+2), with the former\ngenerally having higher scores than the latter. This again matches the task intuitions, since the value\nof the neighbors helps determine whether to select a number for the subsequence; and asking for the\nlexicographically smallest sequence biases the output subsequence to care more about the previous\nnumbers rather than the following ones. We believe that this last point is the cause of the weakly\npredictive power of (ai\u22123, ai\u22122, ai\u22121) to predict oi; whereas (ai+1, ai+2, ai+3) is not shown, since\nall the relative information gain values were below 0.1.\n32\nRelative Information Gain for each problem size\nInput\nvariable\nOutput\nvariable\n3\n4\n5\n6\n7\n8\n9\n10\nan\u22123 an\u22122 an\u22121\non\u22121\n0.95\n0.95\n0.95\n0.95\nan\u22123 an\u22122 an\u22121\non\u22122\n0.87\n0.87\n0.87\n0.87\nan\u22123 an\u22122 an\u22121\non\u22123\n0.64\n0.64\n0.64\n0.64\na1 a2 a3\no1\n1.00\n0.96\n0.97\n0.97\n0.97\n0.97\n0.97\n0.97\na1 a2 a3\no2\n1.00\n0.91\n0.92\n0.91\n0.92\n0.91\n0.92\n0.91\na2 a3 a4\no2\n0.56\n0.55\n0.55\n0.55\n0.55\n0.55\n0.56\na1 a2 a3\no3\n1.00\n0.66\n0.73\n0.71\n0.72\n0.72\n0.72\n0.72\na2 a3 a4\no3\n0.86\n0.77\n0.78\n0.78\n0.78\n0.78\n0.78\na3 a4 a5\no3\n0.67\n0.66\n0.66\n0.66\n0.66\n0.66\na2 a3 a4\no4\n0.94\n0.64\n0.7\n0.68\n0.69\n0.69\n0.69\na3 a4 a5\no4\n0.88\n0.79\n0.81\n0.8\n0.8\n0.8\na4 a5 a6\no4\n0.63\n0.62\n0.62\n0.62\n0.62\na3 a4 a5\no5\n0.95\n0.65\n0.71\n0.69\n0.7\n0.7\na4 a5 a6\no5\n0.87\n0.78\n0.79\n0.79\n0.79\na5 a6 a7\no5\n0.64\n0.63\n0.63\n0.64\na4 a5 a6\no6\n0.94\n0.64\n0.71\n0.69\n0.7\na5 a6 a7\no6\n0.87\n0.78\n0.8\n0.8\na6 a7 a8\no6\n0.64\n0.62\n0.63\na5 a6 a7\no7\n0.95\n0.64\n0.71\n0.69\na6 a7 a8\no7\n0.87\n0.78\n0.8\na6 a7 a8\no8\n0.95\n0.64\n0.71\na1 a2 a3\no4\n0.12\n0.1\n0.11\n0.11\n0.11\n0.11\n0.11\na2 a3 a4\no5\n0.1\n0.09\n0.1\n0.09\n0.1\n0.1\na3 a4 a5\no6\n0.11\n0.1\n0.1\n0.1\n0.11\na4 a5 a6\no7\n0.11\n0.09\n0.1\n0.11\na5 a6 a7\no8\n0.11\n0.09\n0.11\nTable 4: Highest Relative Information Gain Contiguous Triples, for DP problems of size\n3\n\u2264\nn\n\u2264\n10. We only show the (input, output) pairs where at least three problem sizes\nhave RelativeIG>0, and at least one with RelativeIG>0.1. an\u22121 refers to the last element of the\nsequence, regardless of its actual id in the sequence.\nC.4\nEmpirical Surface Pattern Results for Dynamic Programming Task\nWe observe that all analyzed models match the Relative Information Gain prediction that o1 (whether\nthe first element goes into the output sequence or not) should be the easiest value to predict (see\nFigures 30, 31, and 32). However, since GPT3 often predicts shorter output sequences than the\nrequired size, the analysis of the predictive power of on\u22121 is only done for GPT4. In GPT4, we\nobserve that on\u22121 is among the easiest values to predict as expected by Relative Information Gain.\nProblem size (input list length)\nAccuracy\n0.70\n0.80\n0.90\n1.00\n2\n4\n6\n8\n10\no_1\no_2\no_3\no_4\no_5\nProblem size (input list length)\nAccuracy\n0.70\n0.80\n0.90\n1.00\n2\n4\n6\n8\n10\no_{n-1}\no_{n-2}\no_{n-3}\no_{n-4}\no_{n-5}\nProblem size (input list length)\nAccuracy\n0.60\n0.70\n0.80\n0.90\n1.00\n2\n4\n6\n8\n10\nOutput size is correct\nModel never chooses negatives\nModel respects the non-adjacent condition\nFigure 30: GPT4 five-shot with scratchpad accuracy in predicting output elements oi in the DP task.\nAll oi are predicted with high accuracy with o1 and on\u22121 being consistently among the highest. These\nobservations go in line with the Relative Information Gain prediction.\n33\nProblem size (input list length)\nAccuracy\n0.35\n0.45\n0.55\n0.65\n0.75\n6\n7\n8\n9\n10\no_1\no_2\no_3\no_4\no_5\nProblem size (input list length)\nAccuracy\n0.00\n0.25\n0.50\n0.75\n1.00\n6\n7\n8\n9\n10\nOutput size is correct\nModel never chooses negatives\nModel respects the non-adjacent condition\nFigure 31: GPT3 few-shot without scratchpad accuracy in predicting output elements oi in the DP\ntask. As predicted by Relative Information Gain, the model predicts o1 correctly with the highest\nprobability. However, because GPT3 often does not produce the correct output size, it hinders us\nfrom analyzing on\u22121.\nProblem size (input list length)\nAccuracy\n0.7\n0.8\n0.9\n1.0\n6\n7\n8\n9\no_1\no_2\no_3\no_4\no_5\nProblem size (input list length)\nAccuracy\n0.00\n0.25\n0.50\n0.75\n1.00\n6\n7\n8\n9\nOutput size is correct\nModel never chooses negatives\nModel respects the non-adjacent condition\nFigure 32: GPT3 fine-tuned without scratchpad accuracy in predicting output elements oi in the DP\ntask. As predicted by Relative Information Gain, the model predicts o1 correctly with the highest\nprobability. However, because GPT3 often does not produce the correct output size, it hinders us\nfrom analyzing on\u22121.\n34\nD\nTheoretical Results: Derivations\nD.1\nError accumulates with larger parallel applications of an estimated function (width)\nHere we provide formal statements and derivations to Propositions 4.1 and 4.2 shown in the main\npaper. The mathematical framework used is a simplified representation of how multi-step reasoning\nworks, showing two quintessential reasoning types: independent applications of the same step, or\nconsecutive applications of the same step. We take an error estimation and accumulation perspective,\nsince transformers are still being investigated from a theoretical standpoint.\nProposition D.1. Let fn(x) = hn(g(x, 1), g(x, 2)), . . . , g(x, n)). Let bhn, bg, bfn be estimators of\nhn, g, fn respectively. Assume P(hn = bhn) = 1 and P(hn(X) = hn(Y ) | X \u0338= Y ) < cn, where\ncn < c for some constant c \u226a 1 (i.e. bhn perfectly estimates hn, and hn is almost injective). If\nP(g \u0338= bg) = \u03f5 > 0 and errors in bg are independent, lim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265 1 \u2212 c.\nMoreover, if cn \u2264 \u03b2\u03b1n for some some \u03b1 \u2208 (0, 1) and \u03b2 > 0, then\nlim\nn\u2192+\u221e P(fn \u0338= bfn) = 1.\nProof. For ease of writing, let Xi = g(X, i) and Yi = bg(X, i), and let X = (X1, . . . , Xn) and\nY = (Y1, . . . , Yn). We will compute some auxiliary probabilities, and then upper bound P(f = bf),\nto finally compute its limit.\nP(X = Y ) = P(X1 = Y1, X2 = Y2, . . . , Xn = Yn)\n= P(X1 = Y1) \u00b7 P(X2 = Y2) . . . \u00b7 P(Xn = Yn) = P(g = bg)n = (1 \u2212 \u03f5)n\n(2)\nSince by hypothesis we know P(hn(Y ) = bhn(Y )) = 1, we have that:\nP(hn(X) = bhn(Y ) | X \u0338= Y ) = P(hn(X) = bhn(Y ) \u2229 hn(Y ) = bhn(Y ) | X \u0338= Y )\n= P(hn(X) = hn(Y ) = bhn(Y ) | X \u0338= Y )\n\u2264 P(hn(X) = hn(Y ) | X \u0338= Y )\n< cn\n(3)\nWe will now estimate P(fn = bfn) using the law of total probability w.r.t. the event X = Y .\nP(fn = bfn) = P(hn(X) = bhn(Y ))\n= P(hn(X) = bhn(Y ) | X = Y ) \u00b7 P(X = Y ) + P(hn(X) = bhn(Y ) | X \u0338= Y ) \u00b7 P(X \u0338= Y )\n= P(hn(X) = bhn(X)) \u00b7 P(X = Y ) + P(hn(X) = bhn(Y ) | X \u0338= Y ) \u00b7 (1 \u2212 P(X = Y ))\n= 1 \u00b7 (1 \u2212 \u03f5)n + P(hn(X) = bhn(Y ) | X \u0338= Y ) \u00b7 (1 \u2212 (1 \u2212 \u03f5)n)\n(using 2 and hypothesis)\n< (1 \u2212 \u03f5)n + cn \u00b7 (1 \u2212 (1 \u2212 \u03f5)n)\n(using 3)\n< cn + (1 \u2212 \u03f5)n \u00b7 (1 \u2212 cn)\nTo conclude our proof, we will compute a lower bound for lim infn\u2192+\u221e P(fn \u0338= bfn). Note that\nsince cn < c for all n, we know that P(fn = bfn) < c + (1 \u2212 \u03f5)n \u00b7 (1 \u2212 c). Then, P(fn \u0338= bfn) >\n1 \u2212 c \u2212 (1 \u2212 \u03f5)n \u00b7 (1 \u2212 c). Since 1 \u2212 \u03f5 \u2208 [0, 1),\nlim\nn\u2192+\u221e 1 \u2212 c \u2212 (1 \u2212 \u03f5)n \u00b7 (1 \u2212 c) = 1 \u2212 c. Thus,\nlim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265 lim inf\nn\u2192+\u221e 1 \u2212 c \u2212 (1 \u2212 \u03f5)n \u00b7 (1 \u2212 c) = 1 \u2212 c\nwhich concludes our proof.\nNote: In the case where cn \u2264 \u03b2\u03b1n, we can derive an even stronger conclusion. In this case, we can\nprove that\nlim\nn\u2192+\u221e P(fn = bfn) = 1. Recall that P(fn = bfn) < \u03b1\u03b2n + (1 \u2212 \u03f5)n \u00b7 (1 \u2212 \u03b1\u03b2n). Note\nthat since 1 \u2212 \u03f5 \u2208 [0, 1) and \u03b1 \u2208 (0, 1), trivially\nlim\nn\u2192+\u221e \u03b2\u03b1n + (1 \u2212 \u03f5)n \u00b7 (1 \u2212 \u03b2\u03b1n) = 0.\n0 \u2264 lim inf\nn\u2192+\u221e P(fn = bfn) \u2264 lim sup\nn\u2192+\u221e P(fn = bfn) \u2264 lim sup\nn\u2192+\u221e \u03b2\u03b1n + (1 \u2212 \u03f5)n \u00b7 (1 \u2212 \u03b2\u03b1n) = 0\n35\nThen, limn\u2192+\u221e P(fn = bfn) = 0 and we conclude limn\u2192+\u221e P(fn \u0338= bfn) = 1, assuming that\ncn \u2264 \u03b2\u03b1n for some adequate \u03b1, \u03b2.\nCorollary D.1. Assume that a model M solves shifted addition perfectly, but it incorrectly solves at\nleast one m digit by 1 digit multiplication for some fixed m. Then, the probability that M will solve\nany m digit by n digit multiplication using the long-form multiplication algorithm tends to 0 when n\ntends to infinity.\nProof. Let g = d \u25e6 s define the base-10 multiplication between m-digit numbers (x1x2 . . . xm) and\n1-digit numbers (xm+j), where s : Zm+n\n10\n\u00d7 N \u2192 N \u00d7 N denotes the selection of the numbers to\nmultiply and d : N \u00d7 Z10 \u2192 N denotes the actual multiplication:\ng := d \u25e6 s\nd(x, y) := x \u00b7 y\ns([x1, . . . , xm, xm+1, . . . , xm+n], j) := (x1++ x2++ . . .++ xm, xm+j)\nwhere x1++ x2++ . . .++ xm denotes concatenating digits xi\nLet hn : Nn \u2192 N describe the shifted addition used at the end of long-form multiplication to combine\nn m-digit by 1-digit multiplications, and let fn : Zm+n\n10\n\u2192 N describe the long-form multiplication\nof m-digit by n-digit numbers:\nhn(x1, . . . , xn) :=\nn\nX\ni=1\nxi10n\u2212i\nfn(x) := hn(g(x, 1), g(x, 2)), . . . , g(x, n))\nBy hypothesis, P(g \u0338= bg) = \u03f5 > 0 and P(hn = bhn) = 1, where bg and bhn denote estimators using\nmodel M. It can be shown that P(hn(X) = hn(Y ) | X \u0338= Y ) < \u03b2\u03b1n for \u03b1 = 0.1 and \u03b2 = 10m.\nUsing Lemma D.1,\nlim\nn\u2192+\u221e P(fn \u0338= bfn) = 1, which concludes our proof.\nNote that Lemma D.1\u2019s proofs gives us empirical bounds once \u03f5 and \u03b1 are approximated. Also\nnote that our definition of g in the proof of Corollary D.1 highlights two possible sources of\nexponentially-accumulating error: errors in the selection of the numbers to multiply s, and errors\nin the actual m-digit by 1-digit multiplication d.\nD.2\nError accumulates with larger iterative applications of an estimated function (depth)\nProposition D.2. Let fn(x) = gn(x). Assume P(g(X) = bg(Y ) | X \u0338= Y ) \u2264 c (i.e. recovering from\na mistake due to the randomness of applying the estimator on an incorrect input has probability at\nmost c). If P(g \u0338= bg) = \u03f5 > 0 with c + \u03f5 < 1, then lim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265 1 \u2212 c/(c + \u03f5).\nProof. We first derive a recursive upper bound using the law of total probability, and then prove a\nnon-recursive upper bound by induction.\nsn := P(fn = bfn) = P(g(gn\u22121(Z)) = bg(bgn\u22121(Z)))\n= P(g(X) = bg(Y ))\nwhere X := gn\u22121(Z) and Y := bgn\u22121(Z)\n= P(g(X) = bg(Y ) | X = Y ) \u00b7 P(X = Y ) + P(g(X) = bg(Y ) | X \u0338= Y ) \u00b7 P(X \u0338= Y )\n= P(g(X) = bg(X)) \u00b7 P(X = Y ) + P(g(X) = bg(Y ) | X \u0338= Y ) \u00b7 (1 \u2212 P(X = Y ))\n= P(g(X) = bg(X)) \u00b7 sn\u22121 + P(g(X) = bg(Y ) | X \u0338= Y ) \u00b7 (1 \u2212 sn\u22121)\n\u2264 (1 \u2212 \u03f5) \u00b7 sn\u22121 + c \u00b7 (1 \u2212 sn\u22121)\n\u2264 (1 \u2212 \u03f5 \u2212 c) \u00b7 sn\u22121 + c\nWe know s1 = (1 \u2212 \u03f5) since s1 = P(f1 = bf1) = P(g = bg). Let b := 1 \u2212 \u03f5 \u2212 c for ease of writing.\nThen, we have\nsn \u2264 b \u00b7 sn\u22121 + c\n(4)\nIt can be easily shown by induction that sn \u2264 bn\u22121(1 \u2212 \u03f5) + c Pn\u22122\ni=0 bi:\n36\n\u2022 The base case n = 2 is true since we know s2 \u2264 b \u00b7 s1 + c, and b \u00b7 s1 + c = b(1 \u2212 \u03f5) + c =\nb2\u22121(1 \u2212 \u03f5) + c P2\u22122\ni=0 bi, thus showing s2 \u2264 b2\u22121(1 \u2212 \u03f5) + c P2\u22122\ni=0 bi\n\u2022 The inductive step yields directly using Equation 4,\nsn \u2264 b \u00b7 sn\u22121 + c\n\u2264 b \u00b7\n\u0010\nbn\u22122(1 \u2212 \u03f5) + c\nn\u22123\nX\ni=0\nbi\u0011\n+ c \u2264 bn\u22121(1 \u2212 \u03f5) + c\nn\u22122\nX\ni=1\nbi + c \u2264 bn\u22121(1 \u2212 \u03f5) + c\nn\u22122\nX\ni=0\nbi\nWe can rewrite the geometric series Pn\u22122\ni=0 bi in its closed form 1\u2212bn\u22121\n1\u2212b\n, and recalling b := 1 \u2212 \u03f5 \u2212 c,\nsn \u2264 bn\u22121(1 \u2212 \u03f5) + c1 \u2212 bn\u22121\n1 \u2212 b\n= bn\u22121(1 \u2212 \u03f5) + c1 \u2212 bn\u22121\nc + \u03f5\n= bn\u22121(1 \u2212 \u03f5) +\nc\nc + \u03f5 \u2212 bn\u22121\nc\nc + \u03f5\n= bn\u22121\u0010\n1 \u2212 \u03f5 \u2212\nc\nc + \u03f5\n\u0011\n+\nc\nc + \u03f5\nRecalling that sn = P(fn = bfn), we compute the limit inferior of P(fn \u0338= bfn) = 1 \u2212 sn \u2265\n1 \u2212 bn\u22121(1 \u2212 \u03f5 \u2212\nc\nc+\u03f5) \u2212\nc\nc+\u03f5.\nlim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2265\nlim\nn\u2192+\u221e 1 \u2212 bn\u22121\u0010\n1 \u2212 \u03f5 \u2212\nc\nc + \u03f5\n\u0011\n\u2212\nc\nc + \u03f5 = 1 \u2212\nc\nc + \u03f5\nthat concludes our proof.\nWe can generalize the proof in Lemma 4.2 to tasks where there are potentially many valid reasoning\nchains with the following alternative state-transition framing.\nLemma D.2. Let S denote the set of all possible states a language model can generate, and let\nz : S \u2192 {0, 1} defines if a state is valid (0 = invalid). Let bg : S \u2192 \u03a0(S) be a state-transition function\nrepresenting a language model\u2019s probability distribution of generating each possible next state when\nattempting to perform a single reasoning step. Assume P(z(bg(X)) = 1 | z(X) = 0) \u2264 c and\nP(z(bg(X)) = 0 | z(X) = 1) = \u03f5 > 0 with c + \u03f5 < 1. Then, lim inf\nn\u2192+\u221e P(z(bgn) = 0) = 1 \u2212 c/(c + \u03f5).\nIf for task T we know that all valid reasoning chains to arrive at a correct result have at least length\nn (i.e., the equivalent of defining fn = gn in Lemma D.1) then the probability of solving task T\ncorrectly tends to at most c/(c + \u03f5).\nCorollary D.3. The recursions for dynamic programming tasks, the m-by-1 digit multiplication, and\nthe puzzle\u2019s elimination function are all tasks where there is a fixed reasoning step g being repeatedly\napplied. Therefore, we can directly apply Proposition 4.2 to these tasks.\nProof. Let\u2019s analyze the three tasks separately below.\nm-by-1 digit multiplication may be viewed as f m(x)\nLet x = (x1, . . . , xm) be the m-digit\nnumber that we multiply by the 1-digit number y (0 \u2264 y < 10). Let z = (z1, . . . , zm+1) denote\nz = x \u00b7 y, which is guaranteed to have exactly m + 1 digits (with possibly leading zeros). We define\nf as:\nf(x1, . . . , xm, y, i, c) := (x1, . . . , xi\u22121, x\u2032\ni, xi+1, . . . xm, y, i \u2212 1, c\u2032)\nwhere x\u2032\ni := (xi \u00b7 y + c) mod 10 and c\u2032 := \u230a(xi \u00b7 y + c)/10\u230b. Note that x\u2032\ni = zi+1 since f is\nperforming one step of the long-form multiplication algorithm.\nLet the initial input be x\n:=\n(x1, . . . , xm, y, m, 0).\nThen, it can be easily shown that\nf m(x)\n=\n(z2, . . . , zm+1, y, 0, c).\nSince c is the left-most carry, it is the leading digit\nof z, i.e.\nc = z1 (possibly zero) .\nThus, the value of z can be directly extracted from\nf m(x) = (z2, . . . , zm+1, y, 0, z1).\n37\nIn the DP task, dp\u2019s computation may be viewed as f m\u22122(x) for a list of size m\nSee \u00a7A.3.1\nfor details on the solution to this problem.\nWe will use identical notation.\nLet a1, . . . , am\nbe an input list. Let x = (a1, . . . , am\u22122, a\u2032\nm\u22121, a\u2032\nm, m \u2212 2), where a\u2032\nm := max(am, 0) and\na\u2032\nm\u22121 := max(am\u22121, am, 0). Intuitively, this means that we have applied the first two steps of\nthe dp computation, and stored the results in a\u2032\nm\u22121 and a\u2032\nm. Let f be a function representing the\nrecursive computation of dpi:\nf(a1, . . . , ai, a\u2032\ni+1, . . . , a\u2032\nm, i) = (a1, . . . , ai\u22121, a\u2032\ni, . . . , a\u2032\nm, i \u2212 1)\nwhere a\u2032\ni := max(a\u2032\ni+1, ai + a\u2032\ni+2, 0).\nNote that since a\u2032\ni+1 stores the value of dpi+1 and a\u2032\ni+2 stores the value of dpi+2, it can be easily\nshown that f m\u22122(x) = (a\u2032\n1, . . . , a\u2032\nm, 0) = (dp1, . . . , dpm, 0). Therefore, f m\u22122 computes all\nrecursive values of dpi when given the base cases.\nIn the DP task, the reconstruction of the desired subsequence given already computed dp values\nmay be viewed as f m(x) for an input list of size m.\nThis case is similar to the previous one. Let\nr = (r1, . . . , rm) be the result, where ri = 1 if ai was selected for the desired subsequence, and\nri = 2 otherwise. Let x := (dp1, . . . , dpm, 0, 0, a1, . . . , am, 1, 1). Let f be defined as follows:\nf(dp1, . . . , dpm, 0, 0, a\u2032\n1, . . . , a\u2032\ni\u22121, ai, . . . , am, i, u) = (dp1, . . . , dpm, 0, 0, a\u2032\n1, . . . , a\u2032\ni, ai+1, . . . , am, i + 1, u\u2032)\nwhere a\u2032\ni := 2 \u2212 1{dpi = ai + dpi+2 and u = 1} and u := 1 \u2212 1{dpi = ai + dpi+2 and u = 1}.\nIntuitively, a\u2032\ni stores whether the i-th element of the list should be selected for the final subsequence,\nassigning 1 if the element should be taken, and 2 otherwise (i.e., a\u2032\ni = ri). Moreover, if the i-th\nelement has been selected, we mark that the next item will not be available using u\u2032. Therefore, f\nperforms one step of the final output reconstruction as defined in \u00a7A.3.1.\nIt can be easily shown that f m(x)\n:=\n(dp1, . . . , dpm, 0, 0, a\u2032\n1, . . . , a\u2032\nm, m + 1, u\u2032)\n=\n(dp1, . . . , dpm, 0, 0, r1, . . . , rm, m + 1, u\u2032). Note that the extra two elements in the input state\nallow lifting the special cases m \u2212 1 and m in the solution shown in \u00a7A.3.1 without falling out of\nbounds.\nSolving the puzzle task may be seen as f m for some m, where f is the elimination function\nLet\nc1, . . . , cn be the list of clues, let H be the number of houses, and let A be a partially filled solution\nof size K \u00d7 M as defined in \u00a72.4. Each cell Aij can take H + 1 values: the H options for the cell\nand the value \u00f8, implying this cell has not been filled. An elimination step f may be defined as:\nf(c1, . . . , cn, A11, . . . A1M, . . . , AK1, . . . AKM) = (c1, . . . , cn, A\u2032\n11, . . . A\u2032\n1M, . . . , A\u2032\nK1, . . . A\u2032\nKM)\nwhere A\u2032 is also a partially filled matrix, with Aij = A\u2032\nij for every Aij \u0338=\u00f8 and where A\u2032 has at least\none more filled cell.\nLet x = (c1, . . . , cn, E) where E is an empty matrix of size K \u00d7 M (all cell values of E are \u00f8).\nThen, a full solution is computed as f m(x) for some value of m that increases with the problem size.\nIn contrast to other tasks, the value of m is not fixed, and depends on the task instance, but using\nsolvers we know that m increases with problem size.\nD.3\nDiscussing c \u226a \u03f5 in the context of Proposition 4.2\nNote that in Proposition 4.2, if c \u226a \u03f5 then lim inf\nn\u2192+\u221e P(fn \u0338=\nbfn) \u2248 1. This is because assuming\n\u03f5 = m \u00b7 c for some m > 0, we have 1 \u2212\nc\nc+\u03f5 = 1 \u2212\nc\nc+m\u00b7c = 1 \u2212\n1\nm+1 =\nm\nm+1, and\nm\nm+1 is a\nmonotonically increasing function for all m > 0 that tends to 1 when m goes to infinity. Therefore,\nlarge m\u2019s (or alternatively, c \u226a \u03f5) imply\nm\nm+1 will be close to 1.\nIt is reasonable to assume c\u226a\u03f5 when g has low collision, since c represents the probability of the\nestimator bg(y) arriving at the correct output g(x) by chance when given the wrong input y \u0338= x.\nIf g is discrete, it can take |Im(g)| values, where |Im(g)| denotes the cardinal of the image space of g.\nAssuming approximately uniform errors, c \u2248 \u03f5/|Im(g)|, which in turn implies c \u226a \u03f5 since g being\nlow collision implies |Im(g)| is large.\n38\nIf g is continuous, under appropriate assumptions it seems plausible that we can prove that c \u2248 0 (e.g.\nif errors are approximately uniform).\nSummarizing both cases, if errors are approximately evenly distributed we obtain that\nlim inf\nn\u2192+\u221e P(fn \u0338= bfn) \u2248 1.\n39\nE\nAdditional Literature and Societal Impact\nE.1\nAdditional Literature\nIterated Functions\nThe process of repeatedly applying a noisy single operation or function f\ncan be related to iterated random functions [24]. In this latter literature, the focus is usually on the\ncontractive regime in which accrued errors can be kept under control, and the subsequent convergence\nguarantees (e.g., [23]). When f is an affine transformation, the process falls simultaneously between\ntwo perspectives: time series [31] and dynamic programming and control [6]. We leverage the former\nto discuss the often explosive errors of f n.\nE.2\nSocietal Impact Discussion\nOur work on analyzing the limitations of current transformers in compositional tasks can have a\npositive societal impact in several ways. By shedding light on these limitations, we contribute to a\ndeeper understanding of the capabilities and constraints of these models. This knowledge is essential\nfor researchers, developers, and policymakers in making informed decisions regarding the application\nof transformers in various domains.\nUnderstanding the limitations of transformers in compositional reasoning is crucial for developing\nmore reliable and robust AI systems. By identifying these shortcomings, we can direct future research\nefforts toward addressing these limitations and developing models that exhibit improved performance\nin handling complex tasks requiring compositional reasoning.\nWe do not foresee any negative societal impacts, as our analysis aims to understand the reasons\nbehind transformers\u2019 failures and successes, but does not introduce any new model or dataset that\nfuture work may leverage.\n40\n"
  },
  {
    "title": "GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction",
    "link": "https://arxiv.org/pdf/2305.18752.pdf",
    "upvote": "2",
    "text": "GPT4Tools: Teaching Large Language Model to Use\nTools via Self-instruction\nRui Yang1\u2217\u2021,\nLin Song2\u2217\u2020,\nYanwei Li3,\nSijie Zhao2,\nYixiao Ge2,\nXiu Li1,\nYing Shan2\n1Tsinghua Shenzhen International Graduate School, Tsinghua University\n2Tencent AI Lab\n3Chinese University of Hong Kong\nrayyang0116@gmail.com\nronnysong@tencent.com\nThe essential difference between humans and animals is that humans are capable of making and\nusing tools.\n\u2014Friedrich Engels\nAbstract\nThis paper aims to efficiently enable Large Language Models (LLMs) to use\nmultimodal tools. Advanced proprietary LLMs, such as ChatGPT and GPT-4, have\nshown great potential for tool usage through sophisticated prompt engineering.\nNevertheless, these models typically rely on prohibitive computational costs and\npublicly inaccessible data. To address these challenges, we propose the GPT4Tools\nbased on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to\nuse tools. It generates an instruction-following dataset by prompting an advanced\nteacher with various multi-modal contexts. By using the Low-Rank Adaptation\n(LoRA) optimization, our approach facilitates the open-source LLMs to solve a\nrange of visual problems, including visual comprehension and image generation.\nMoreover, we provide a benchmark to evaluate the ability of LLMs to use tools,\nwhich is performed in both zero-shot and fine-tuning ways. Extensive experiments\ndemonstrate the effectiveness of our method on various language models, which not\nonly significantly improves the accuracy of invoking seen tools, but also enables\nthe zero-shot capacity for unseen tools. The code and demo are available at\nhttps://github.com/StevenGrove/GPT4Tools.\n1\nIntroduction\nRecent advances in large language models (LLMs), such as GPT-3 [1], InstructGPT [2], and Chat-\nGPT [3], have demonstrated substantial potential in the area of zero-shot learning and logical\nreasoning. These models are typically trained on a large volume of text-only data, primarily sourced\nfrom the internet. However, as promising as they may seem, these advanced proprietary LLM [3, 4]\nhave significant limitations. One of the major hindrances is the high computational cost associated\nwith these models, which may not be affordable or accessible to many scenarios. Additionally, these\nmodels typically depend on specialized data, such as source code and conversation history, which are\nnot easily available to the public.\nInstead of solely focusing on language processing, many recent researches [5, 6] attempt to bridge the\ngap between language models and multi-modal tools. The intelligent agents like Visual ChatGPT [5]\nand MMREACT [6] have made efforts to meet this goal by sophisticated prompt engineering. These\nagents utilize a pre-defined template to create instructions that can be executed by vision-language\nfoundation models. Although these approaches have led to impressive results, the primary process\nof instruction decomposition is heavily based on GPT-3.5 [3], which is not publicly available, thus\n*Equal contribution. \u2021 Work done during an internship at Tencent.\n\u2020Corresponding author.\narXiv:2305.18752v1  [cs.CV]  30 May 2023\nTable 1: Comparison of related works. \u2018LM\u2019 is the language models. \u2018Mechanism\u2019 denotes how the\nlanguage model learns to invoke tools. \u2018Unseen\u2019 indicates the zero-shot capability on unseen tools.\nMethod\nLM\nMechanism\nTeacher\nMulti-Modal\nUnseen\nLazaridou et al. [10]\nGopher-280B [14]\nprompt\n\u2717\n\u2717\n\u2717\nToolFormer [11]\nGPT\u2013J (6B) [15]\nself-instruct\n\u2717\n\u2717\n\u2717\nVisual ChatGPT [5]\nGPT-3.5 (175B) [3]\nprompt\n\u2717\n\u2714\n\u2714\nMMREACT [6]\nGPT-3.5 (175B) [3]\nprompt\n\u2717\n\u2714\n\u2714\nGPT4Tools (ours)\nVicuna-13B [12]\nself-instruct\n\u2714\n\u2714\n\u2714\nlimiting further advancements. In addition, equipping these agents with the capability to use tools\nrequires a large amount of data [3]. This brings up an open question: how to efficiently enable a\nprimitive language models to use multi-modal tools?\nTo achieve it, different from previous studies [7\u201311], we explore a new perceptive as illustrated\nin Table 1. We propose a simple yet effective method, called GPT4Tools, designed to empower\nopen-source LLMs with the ability to use tools via self-instruct from advanced LLMs. To achieve\nthis, we construct an instruction dataset by prompting advanced teachers (for example, ChatGPT [3])\nconditional on visual content and tool descriptions, which leads to the generation of tool-related\ninstructions. Unlike Toolformer [11], our method can utilize visual content description to significantly\nimprove data diversity. Furthermore, with the generated instruction-following dataset, we incorporate\nLow-Rank Adaptation (LoRA) to fine-tune the primitive language models, such as Vicuna [12] and\nOPT [13]. Besides intrinsic language abilities, by using GPT4Tools, language models can also\nhave the capability to use tools to solve a variety of visual problems. The tasks include visual\ncomprehension and image generation, such as object grounding and segmentation, generating and\ninstructing images, and visual question answering (VQA). With the proposed GPT4Tools, the LLMs\nnot only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot\ncapacity for unseen tools in a zero-shot manner.\nWe propose an evaluation metric to assess the effectiveness of LLMs in utilizing tools across diverse\ntasks. With this metric, two human-curated validation sets are constructed to evaluate the LLMs in\nzero-shot and fine-tuning ways, providing a comprehensive measure of the ability to use tools. To\ndemonstrate the effectiveness of GPT4Tools, we conduct extensive experiments on various language\nmodels. The results show the efficacy in teaching LLMs when and how to use tools. Specifically,\nwith the GPT4Tools, the fine-tuned Vicuna-13B achieves 9.3% absolute gains in successful rate over\nGPT-3.5 [3], which acquires tool priors in context. Moreover, the fine-tuned Vicuna-13B shows the\nstrong capacity to invoke unseen tools, which is comparable to GPT-3.5 in successful rate.\nOur GPT4Tools stands distinct from previous and concurrent studies [5\u201311] in three ways. First, our\nmethod enables primitive open-source language models to use tools, eliminating the dependence on\nadvanced proprietary LLMs like ChatGPT. Second, we design a new approach based on multi-modal\ncontexts for self-instruction and augmentation, which significantly promote the multi-modal tool\nusage and can be deployed in different approaches. Third, we propose a new benchmark to assess the\neffectiveness of using tools, and our method shows remarkable improvements.\n2\nRelated Work\nVision and Language Model. In the quest to achieve multimodal models capable of addressing both\nlanguage and vision tasks, several studies [16\u201321] have explored methods to enable language models\nto comprehend visual input. These include techniques such as transforming images into discrete\ntextual representations [16, 17] or projecting continuous image features into the textual feature\nspace [22\u201325]. Concurrently, other research has been dedicated to the development of generalist\nmodels [18, 19, 21, 20], which permit a model to simultaneously input images and text, eliminating\nthe necessity for a projection process. For instance, OFA [26] devised a unified sequence-to-sequence\ndecoding architecture applicable to language and object detection tasks. Similarly, Pixel2Pixel [20]\nconverted the outcome of visual comprehension tasks into a series of discrete tokens akin to language\ntasks. Gato [21] brought together a range of vision and control tasks into a sequential prediction issue,\nwhile UViM [27] and Unified-IO [19] advocated for the learned discrete codes as a means to unify an\n2\nChatGPT\nA person is leaning low on their motorcycle \non the tracks.\nA person leaning down on a motorcycle as \nthey ride on a track.\nA man is nearly sideways while racing a \nmotorcycle around a track.\nA man with a helmet is riding a motorcycle on \nit's side.\nmotorcycle: [179.44, 105.55, 411.64, 220.7]\nperson: [136.26, 77.72, 356.95, 124.75]\nImage Content\nInstruction Dataset\nInstruction:\nPrefix Prompt.\nHuman: Provide an image named <image>\nDescription: <image content>\nAI: Received.\nNew input: <user input>\nSuffix Prompt.\nResponse:\nThought: Do I need to use a tool? Yes\nAction: <tool name>\nAction Input: <arguments>\nObservation: <output image>\nInstruction\nResponse\nGPT4Tools\nUser\nDo I need to \nuse a tool?\nLanguage \nModel\nLoRA\nInstruction\nResponse\n<tool name>: \n<usage scenario>, <arguments>\nTool Pocket\nDetection\nGeneration\nOCR\nCaption\nPose\nFigure 1: Diagram of the GPT4Tools. We prompt the ChatGPT with image content and definition of\ntools in order to obtain a tool-related instruction dataset. Subsequently, we employ LoRA [38] to\ntrain an open source LLM on the collected instruction dataset, thus adapting the LLM to use tools.\narray of vision tasks. By contrast, we in this paper equip the language model with a diverse array of\nspecialized multi-modal tools to process distinct vision tasks. This approach not only promotes the\nscalability of the model for various tasks, but also avoids the issue of forgetfulness stemming from\nrepeated fine-tuning.\nInstruction Tuning. Recent studies [28, 2, 29\u201332] have turned out that pre-trained language models\ncould follow natural language instructions and complete various real-world tasks if they are tuned\non specific instruction-following data. Notably, InstructGPT [2], FLAN-T5 [30], OPT-IML [32]\ndemonstrated remarkable performance on specific tasks after being fine-tuned with instruction data. In\norder to release the cost of human-written instructions, Self-Instruction [33] found that the instruction-\nfollowing capabilities of language models can be enhanced by turning on their own generated\ninstruction data. More importantly, this approach inspired a feasible means to improve the zero- and\nfew-shot abilities of language models, i.e., distilling off-the-shelf language models using instructional\ndata from strong ChatGPT [3] or GPT-4 [4]. As a result, many recent works [12, 34\u201336] tried to\nconstruct excellent language models for various applications based on the LLaMA [37]. For instance,\nStanford-Alpaca has employed 52K instructions generated by GPT-3.5 [3] to construct an exceptional\ndialogue model. LLaVa [36] has adopted GPT-3.5 [3] and GPT-4 [4] to incorporate instruction-\nfollowing data related to visual content. In this paper, we use GPT-3.5 [3] to construct tool-related\ninstruction datasets, thereby allowing other language models to acquire tool usage capabilities.\nTool Usage. In the Natural Language Processing (NLP) community, several arts [7\u201311] sought\nto endow language models with the ability to use tools. For instance, Komeili et al. [7] proposed\nto generate conversation responses conditioned on the results of the search engine. LaMDA [9]\ncreated a set of tools (comprising an information retrieval system, a calculator, and a translator) to\navoid plausible outputs. Lazaridou et al. [10] utilized few-shot prompting on Gopher-280B [14] to\nenable the search engine to ground its output in factual and current information. Similarly, Visual\nChatGPT [5] and MMREACT [6] prompted ChatGPT to invoke visual foundation models. In addition,\n3\n60\n40\n20\n0\n20\n40\n60\n60\n40\n20\n0\n20\n40\n60\nw/o image content\n60\n40\n20\n0\n20\n40\n60\n40\n20\n0\n20\n40\n60\nw/ image content\nFigure 2: t-SNE1visualization for instruction data with and without image content.\nToolFormer [11] used self-instruction and bootstrapping to teach GPT-J (6B) [15] using five tools,\nwhich include a question and answer system, a calculator, a search engine, a machine translation\nsystem, and a calendar. On the contrary, we focus on using the GPT-3.5 model as a powerful teacher\nto distill off-the-shelf language models and enable them to access many visual models.\n3\nMethod\nLarge language models (LLMs) [1, 13, 14] have shown remarkable in-context learning abilities.\nAmong them, ChatGPT [3] and GPT-4 [4] are proven to effectively perform text-annotation tasks [39]\nor instruct other models to follow instructions of specific domains [40, 34, 12, 36]. Inspired by these\nfindings, we propose leveraging ChatGPT as a powerful teacher to enable off-the-shelf language\nmodels to acquire tool usage capabilities. Specifically, we utilize ChatGPT to generate tools-related\ninstruction-following data, which is then used to tune the language model. This process enables\nthe language model to access multimodal information by invoking visual models. Furthermore,\nwe propose an evaluation metric to assess the tool-use ability of the given language model. In the\nfollowing, we present the data generation, instruction tuning, and evaluation metric in turn.\n3.1\nDataset Construction\nData Generation. Figure 1 illustrates the process of generating tool-related instruction dataset. Given\nan image, we construct the image content XC according to the captions and bounding boxes, which is\na straightforward means of establishing connections between an image and a language model [16, 17].\nConditioned upon the XC, we provide the ChatGPT [3] (MT) with a tool-related prompt Pt whereby\nattaining a large number of instruction-following data:\nY \u223c MT(Pt|XC).\n(1)\nThe Pt comprises the system message, the definition of tools (<tool name> : <usage scenario>,\n<arguments>), and suffix prompt which encourage MT to generate visual instructions and desired\noutputs. Y , the outcome of MT, consists of N instruction-output pairs {y1, y2, ..., yN}, where yi has\nthe format of \"<instruction>, <tool name>, <arguments>\", and N is the number of defined\ntools. As each input of MT is grounded to the image content XC, the generated instructions are\ninherently connected to the image, thus avoiding arbitrary generation. Moreover, the rich variability\nof the image results in a higher diversity of instructions when compared to imagined ones. To provide\ncontrast, we also collect instruction follow-up data without image content XC, which is similar to\nToolFormer [11]. As depicted in Figure 2, the high degree of similarity observed among instructions\ngenerated without XC highlights their monotony and lack of diversity. On the contrary, instructions\ngenerated with image-conditioned prompts are more informative and diverse, due to changes in the\nimage content.\n1The visual instruction and tool arguments are embedded by Sentence-BERT [41]. During generation without\nimage content, we set the temperature as 0.9 to avoid constant outcomes.\n4\nInstruction:\nPrefix Prompt.\nHuman: Provide an image named exam\nple.png. Description: The motorcy- clist\ns are travelling down the street together.\nUnderstand the image using tools.\nAI: Received.\nNew input: Select the best definition of\n\"attribute\".\nSuffix Prompt.\nResponse:\nThought: Do I need to use a tool? No\nAI: An attribute can be defined as a qual\nity, trait, or characteristic inherent to a p\nerson, thing, or object. In the context of\ndatabase management, it refers to a speci\nfic value or characteristic of a variable o\nr column within a record. In programmi\nng or web development, an attribute defi\nnes properties, settings or characteristics\nof an element in a markup language or c\node.\n(b) Negative sample\nInstruction:\nPrefix Prompt.\nHuman: Provide an image named exam\nple.png. Description: a huge crowd of pe\nople at a sky resort Understand the imag\ne using tools.\nAI: Received.\nNew input: Generate a real image of a s\nki resort with people and winter clothing\nfrom segmentations and text description.\nSuffix Prompt.\nResponse:\nThought: Do I need to use a tool? Yes\nAction: Segment the Image\nAction Input: example.png\nObservation: output_1.png\nThought: Do I need to use a tool? Yes\nAction: Generate Image Condition On S\negmentations\nAction Input: output_1.png, A ski resor\nt with people and winter clothing\nObservation: output_2.png\nThought: Do I need to use a tool? No\nAI: Result saved as output_2.png\n(a) Single-turn sample\nInstruction:\nPrefix prompt.\nHuman: Provide an image named exam\nple.png. Description: many people are at\ntheir laptops working together Understa\nnd the image using tools.\nAI: Received.\nNew input: Generate a new real image\nof a conference room full of people wor\nking on their laptop computers from this\ndepth map.\nSuffix prompt\nThought: Do I need to use a tool? Yes\nAction: Predict Depth On Image\nAction Input: example.png\nObservation: output_1.png\nResponse:\nThought: Do I need to use a tool? Yes\nAction: Generate Image Condition On\nDepth\nAction Input: output_1.png, The image\ndepicts a conference room full of people\nworking on their laptop computers.\nObservation: output_2.png\nThought: Do I need to use a tool? No\nAI: Result saved as output_2.png\n(c) Context sample\nFigure 3: Samples of the single-turn instruction, negative instruction, and contextual instruction.\nData Formation. Upon the collected raw dataset (70K items), we apply a filtering process to\nremove similar instructions, resulting in 41K retained items. Subsequently, we transform the\nretained data into an instruction-response format utilizing a standardized template as shown in\nthe bottom-left corner of Figure 1. This procedure produces a new dataset, denoted as Y +\nS . The\ninstruction component of Y +\nS incorporates a prefix prompt that encompasses system messages and\ntool definitions, <image content> that denotes the image content, <user input> that is replaced\nwith the generated visual instruction, and a suffix prompt designed to prompt the language model to\nreply the user input through given tools. The response component of Y +\nS comprises the Thought to\ninvoke tools and a chain of actions. Each action involves an Action and Action Input, succeeded by\n<tool name> and <arguments>, respectively. The Observation reflects the outcome of the invoked\ntools. A sample from Y +\nS is presented in the Figure 3 (a).\nData Augmentation. Although we have successfully acquired instruction-following data related to\nthe tool usage, this simplistic format lacks complexity and depth in both instructions and responses.\nTo mitigate this issue, we augment the generated data from two perspectives:\n\u2022 Negative samples. The generated instructions primarily focus on tool usage, i.e., the decision after\nthe Thought is always \"Yes\". Consequently, there is a potential risk that the fine-tuned model\noverfits such a decision. When the user instruction does not connect with the tool usage, the\nfine-tuned model may erroneously execute irrelevant actions by invoking unnecessary tools. To\nmitigate this issue, we synthesize negative samples Y \u2212\nS by selecting conversation data from the\nexisting dataset [40] and converting them into the required template, as illustrated in Figure 3 (b).\nBy tuning the model with Y +\nS \u222a Y \u2212\nS , it can decide when to use tools.\n\u2022 Context samples. The generated instructions adopt a standard and fixed single-tune format, which\nlacks a contextual structure. Thus, we augment the dataset by cutting off the chain of action, as\nshown in Figure 3 (c). Furthermore, we randomly select multiple instructions from Y +\nS \u222a Y \u2212\nS\nand reformat them into multi-turn conversation data. In this way, we synthesize the contextual\ninstruction-following data Y c\nS, which enables the tuned model to call tools within the given context.\nSo far, we have constructed the tool-related instructional dataset, including positive samples, negative\nsamples, and context samples: YS = Y +\nS \u222a Y \u2212\nS \u222a Y c\nS.\n5\n3.2\nInstruction Tuning\nBased on the YS, we tune the off-the-self language model using its original auto-regressive training\nobjective. To make the tuning feasible, we leverage LoRA [38] optimization\u201e which freezes the\nlanguage model and optimizes rank decomposition components of the Transformer layers. For a\nsequence with L tokens, we compute the probability of the target response Xr by:\np(Xr|XC, Xinst) =\nL\nY\ni=1\np\u03b8(xi|XC, Xinst, x1:i\u22121),\n(2)\nwhere Xinst denotes the instruction tokens; and \u03b8 is the trainable parameters.\nIn practice,\nprefix prompt and suffix prompt are also involved, but we here skip them for better readability.\n3.3\nEvaluation Approach\nNumerous benchmarks [42, 43, 8] typically utilize human-annotated datasets to evaluate the perfor-\nmance of a model. For the purpose of measuring the tool-usage capacity of the language model, we\nconstruct a evaluation dataset following the same procedures detailed in \u00a7 3.1, and and manually ver-\nify the accuracy of each constituent item. This evaluation dataset is partitioned into two components:\nthe first part (validation set) has the same ingredients as the training set, encompassing 23 tools; the\nsecond part (test set) comprises 8 novel tools that are absent from the training set. We will use the\nvalidation set to validate whether the model can adhere to user commands correctly after tuning with\nthe training set. The test set will be employed to verify whether the model can generalize to new\ntools after tuning. Based on the human-annotated evaluation dataset with N instructions, we design a\nsuccessful rate to measure the model\u2019s performance from three aspects:\n\u2022 Successful Rate of Thought (SRt) measures whether the predicted decision matches the ground-\ntruth decision. It is calculated as SRt = 1\nN\nPN\ni=1 I(\u03c4i), where \u03c4i signifies a singular process. If the\nthought is correct, I(\u03c4i) is equal to 1, and 0 otherwise.\n\u2022 Successful Rate of Action (SRact) measures whether the predicted tool name is in agreement with\nthe name of the ground truth tool. It is calculated as SRact =\n1\nN\nPN\ni=1 I(\u03b1i), where \u03b1i denotes\nthe matching process for the tool names. In cases where the predicted tool name matches the\npre-defined name, I(\u03b1i) is equal to 1, and 0 otherwise.\n\u2022 Successful Rate of Arguments (SRargs) evaluates whether the predicted arguments match the\nground-truth arguments. It can be calculated using the following equation:\nSRargs = 1\nN\nN\nX\ni=1\n\u03b7i, where \u03b7i = 1\nK\nK\nX\nj\n\u03b7i,j.\n(3)\nHere, \u03b7i denotes a sequence of arguments, encompassing both the image path and the input text.\nFor instance, ControlNet [44] needs the image path saved conditions (e.g. pose map, depth map,\nor segment map) and the input text described the user command. K represents the quantity of\narguments in \u03b7i. When the argument belongs to the image path, \u03b7i,j equals 1 if the predicted and\nground-truth image paths share the same suffix, and 0 otherwise. When the argument is the input\ntext, \u03b7i,j is equal to the BLEU score between the predicted and the ground truth text.\n\u2022 Successful Rate (SR) measures whether a chain of actions are executed successfully, which\nrequires the correctness of thought, tool name, and tool arguments:\nSR = 1\nN\nN\nX\ni=1\nI(\u03c4i) \u00b7 I(\u03b1i) \u00b7 I(\u03b7i > 0.5)\n(4)\nAdditionally, when a procedure comprises two consecutive actions, the SR equals 100% only if\nboth actions are executed correctly.\n4\nExperiments\n4.1\nImplementation Details\nWe employ the ChatGPT (gpt-3.5-turbo) [3] as the teacher model to generate the raw instruction-\nfollowing data. Since this study focused on teaching off-the-self language models to use tools instead\n6\nTable 2: Comparison of different language models. The zero-shot prediction is adopted for unseen\ntools and the models without GPT4Tools.\nModel\nGPT4Tools\nValidation (seen tools)\nTest (unseen tools)\nSRt\nSRact\nSRargs\nSR\nSRt\nSRact\nSRargs\nSR\nGPT-3.5 [3]\n(text-davinci-003)\n\u2717\n93.5\n96.1\n78.0\n84.8\n99.5\n99.5\n91.5\n91.5\nOPT-13B [13]\n\u2717\n1.1\n1.2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\u2714\n99.4\n98.3\n89.2\n93.2\n97.8\n89.6\n84.0\n78.6\nLLaMa-13B [37]\n\u2717\n20.4\n15.7\n16.5\n3.2\n16.1\n17.6\n21.7\n2.0\n\u2714\n77.3\n74.9\n71.4\n66.4\n74.2\n72.2\n70.9\n69.9\nVicuna-13B [12]\n\u2717\n69.2\n25.1\n25.2\n12.4\n84.4\n43.7\n46.7\n26.2\n\u2714\n98.7\n97.6\n91.4\n94.1\n98.2\n97.0\n92.2\n90.6\nof prompt engineering, we adopted the methodology outlined in the Visual ChatGPT [5] to construct\ntool-related prompts. Our tool pocket consists of 31 tools, including the 23 tools defined in Visual\nChatGPT [5] and 8 additional tools (refer to Table 6 in Appendix B for detailed tool names). The\ntraining set comprises 71K instruction-response pairs, wherein all instructional data is related to\nthe 23 tools from Visual ChatGPT. We divided the human-annotated evaluation dataset into two\nparts: validation set and test set. The validation set contains the same tools as the training set, with\napproximately 50 items associated with each tool. The test set includes tools that are not present in\nthe training set. (further details provided in Appendix A)\nBased on the collected data, we tuned language models (LLaMA [37], Vicuna [12], and OPT [13])\nwith LoRA [38] technology. Specifically, we equipped the projection layers of query, key, value, and\noutput with LoRA layers. The LoRA attention dimension and scaling alpha were set to 16. While\nthe language model was kept frozen, the LoRA layers were optimized using the AdamW [45]. All\nmodels were fine-tuned over 3 epochs, with a batch size of 512. The learning rate was set to 3\u00d710\u22124,\nand the maximum length of new tokens was restricted to 2048. Unless otherwise specified, we used\nVicuna-13B for the ablation experiments.\n4.2\nMain Result\nCan instruction datasets teach language model using tools? The outcomes of GPT-3.5 [3], OPT-\n13B [13], LLaMA-13B [37], and Vicuna-13B [12] are presented in Table 2. GPT-3.5 is considered\nanalogous to Visual ChatGPT [5]. Upon prompting GPT-3.5 with tool-associated instructions, it is\nable to attain a SR of 84.8% on the validation set, thereby underscoring its zero-shot ability to follow\na standardized format and utilize tools effectively. Notably, OPT-13B fails to invoke tools with the\nprompts alone. In contrast, LLaMA-13B and Vicuna-13B exhibit a certain level of comprehension of\ntool usage, while they still face challenges in executing a chain of actions. Specifically, LLaMA-13B\nachieves 3.2% SR, which is absolutely lower than SRt, SRact, and SRargs. In the case of Vicuna-\n13B, its SR is 56.8% less than SRt, implying that under a zero-shot setup, Vicuna-13B displays\ncommendable discernment in determining when to use tools within a given context. After fine-tuned\nwith GPT4Tools, there are substantial alterations in the tool invocation competencies of each model.\nSpecifically, the SR of OPT-13B is witnessed a sharp increase from 0 to 93.2%. Similarly, the SR for\nLLaMA-13B escalates from 3.2% to 66.4%, and Vicuna-13B\u2019s SR rises from 12.4% to 94.1%. These\noutcomes unequivocally validate that the GPT4Tools developed in this study are indeed effective in\ninstructing language models to use tools.\nCan the model be generalized to unseen tools after fine-tuning? The right side of Table 2 shows\nthe results when prompting a novel tool and corresponding utilization. On the test set, GPT-3.5\nattaines 91.5% SR in a zero-shot manner. The outcomes for other models, which are not fine-tuned\non the GPT4Tools and directly invoke tools utilizing prompts, are analogous to those on the validation\nset. In contrast, models that are fine-tuned on the GPT4Tools dataset exhibit a degree of competence\nin invoking tools that have not been previously encountered (did not appear in the training set). More\nspecifically, the fine-tuned LLaMA-13B model achieves a superior SR on new tools by a margin of\n67.9% when compared to the original model. The fine-tuned Vicuna-13B model demonstrates 90.6%\n7\nTable 3: Ablation study for data augmentations on the validation set. \u2019IC\u2019, \u2019CS\u2019, \u2019NS\u2019 denotes image\ncontent, context samples, and negative samples, respectively.\nIC\nCS\nNS\nSRt\nSRact\nSRargs\nSR\n70.0\n55.7\n51.7\n36.9\n\u2714\n89.6\n89.9\n84.5\n81.6\n\u2714\n\u2714\n97.4\n95.7\n88.5\n91.6\n\u2714\n\u2714\n\u2714\n98.7\n97.6\n91.4\n94.1\nTable 4: Ablation study for different model sizes on the validation set.\nModel\nGPT4Tools\nSRt\nSRact\nSRargs\nSR\nVicuna-7B [12]\n\u2717\n27.7\n15.8\n11.5\n4.5\n\u2714\n96.2\n94.5\n89.8\n92.9\nVicuna-13B [12]\n\u2717\n69.2\n25.1\n25.2\n12.4\n\u2714\n98.7\n97.6\n91.4\n94.1\nSR on new tools, which is comparable to GPT-3.5. This observation indicates that the language\nmodel can invoke unseen tools after fine-tuned with GPT4Tools.\n4.3\nAblation Study\n250\n500\n750\n1000\n1250\n1500\n1750\n2000\nIteration\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nSuccess Rate (%)\nSRt\nSRact\nSRargs\nSR\nFigure 4: Performance variation curve with the\nfine-tuning iteration.\nData Augmentation. As depicted in Table 3,\nwe execute a series of ablation studies on vari-\nous tricks implemented during the creation of\nthe dataset. When instructions are not condi-\ntioned upon the image content, the SR of the\nfine-tuned model on the validation set is a mere\n36.9%. In contrast, when instructions are gen-\nerated with conditioning on the image content,\nthe SR on the validation set is enhanced substan-\ntially to 81.6%. This uptick can be primarily\nattributed to the elevated diversity and intricacy\nof the generated instructions. Moreover, an aug-\nmentation of the SR to 91.6% is observed upon\nthe introduction of context samples into the in-\nstructions. This finding underscores the fact that\npartitioning the chain of actions and allocating\nthem to the instruction and response can strengthen the model\u2019s comprehension of the tool. It is\nnoteworthy to mention that with the incorporation of negative samples into the generated instructions,\nthe SR increases to 94.1%. This outcome can be traced back to the propensity of the model, when\ntrained exclusively with positive samples, to bias toward tool invocation. This tendency consequently\ndiminishes the capacity to discern the appropriate cases for tool usage. Adding negative samples\nequips the model with the ability to determine when to use tools.\nModel Size. We attempt experiments with models at different scales. The results in Table 4\ndemonstrate that after fine-tuned on the generated dataset, Vicuna-7B [12] is also capable of invoking\ntools in a fixed format. Specifically, under a zero-shot setting, Vicuna-7B achieves only a 4.5% SR.\nBy contrast, after fine-tuning, it is able to achieve an SR of 92.9%.\nTuning Iteration. We increase the number of iterations for fine-tuning and present the results\nin Figure 4. Notably, during the range of iterations from 400 to 800, the model\u2019s performance\ndemonstrates substantial fluctuations in tool invocation. However, subsequent to this range, there is\na steady improvement in SRt, SRact, SRargs, and SR. This indicates that the model progressively\nadapts to the dataset and enhances its capability to invoke tools.\n8\nGenerate a picture of real \npeople based on the edge.\nResult saved as image/11 \n8bf4d4.png\n(a) GPT4Tools (ours)\nGenerate a picture of real \npeople based on the edge.\nThe image is a black and white \nline drawing of a large group of \npeople standing together, smili\nng, and forming a crowd. There \nare at least 13 people in the gro\nup, each with different heights \nand positions. They are all stand \ning close to each other, creating \na sense of unity and camarader-\nieee. The line drawing captures \nthe essence of the group, show-\ncaseing their happiness and tog-\netherness.\n(c) LLaVa\nimage/1ce2709a.png\n(b) VisualChatGPT\nGenerate a picture of real \npeople based on the edge.\nFigure 5: Comparison with other models. Our GPT4Tools responses correctly, while Visual Chat-\nGPT [5] replies with the wrong image, and LLaVa [36] can not generate the image.\nResult saved as \nimage/bc64fe8a.png\nGive me the sketch map of \nthis image.\nGenerate a man watching \na sea based on the pose of \nthe woman.\nResult saved as \nimage/3b97142f.png\nMake the image look like a \ncartoon.\nResult saved as \nimage/c1c6af5b.png\nSegment the tie in the \nimage.\nResult saved as image/\naaeff6e2.png\nSegment the woman in \nthe image.\nResult saved as image/\nb4fadaef.png\nHow many people in this \nimage?\n2\nFigure 6: Cases of invoking tools from Vicuna-13B [12] fine-tuned on our GPT4Tools.\n9\n4.4\nCase Study\nFigure 5 presents a comparative analysis of our model with Visual ChatGPT [5] and LLaVa [36].\nWhen an image is submitted by the user alongside the instruction \"Generate a picture of real people\nbased on the edge\", Visual ChatGPT delivers an image that exhibits a weak correlation with the given\ninstruction. Owing to its inability to generate images, LLaVa only returns a caption. In contrast,\nour model produces an accurate result, thereby evidencing that the tool-related instruction tuning\nmethod proposed in this paper can effectively instruct language models in the correct usage of tools.\nIn the Figure 6, we further demonstrate that the Vicuna-13B fine-tuned on GPT4Tools is capable\nof finishing some visual commands by invoking visual tools. This finding indicates that imparting\nknowledge to language models regarding the tool invocation could potentially be a way toward the\ndevelopment of a generalist model. More case studies are present in Appendix C.\n5\nLimitation\nAlthough the proposed GPT4Tools can teach plug-and-play language models to use tools effectively,\nit still has some limitations. For instance, the success rate of all models is not 100%, thus further\nimprovements are still necessary for practical applications. Additionally, GPT4Tools teaches the\nmodel to explicitly invoke tools using a verbose and fixed prompt (Table 9). This approach reduces\nthe computational efficiency of the model as attention-based architectures compute the relationships\nbetween all tokens. Therefore, in the future, it should be explored how to enable the model to\nimplicitly invoke various tools, rather than using the complex prompt. Nevertheless, our GPT4Tools\nprovides a viable approach for equipping language models with multimodal tools.\n6\nConclusion\nThis paper introduces GPT4Tools, a novel method that enables open-source LLMs to utilize multi-\nmodal tools efficiently. We construct a tool-related instructional dataset from advanced ChatGPT and\naugment them by introducing negative and context samples. Based on the built dataset, we employ\nLoRA optimization to enhance LLMs\u2019 tool-usage capabilities, thus allowing LLMs to handle various\nvisual tasks, e.g., visual comprehension and image generation. Moreover, we propose a benchmark to\nassess tool usage accuracy from the decision when to use tools, which tools to use, and arguments\nof invoked tools. In this benchmark, the LLMs tuned with our GPT4Tools perform comparably to\nGPT-3.5 on unseen tools. We desire the GPT4Tools to pave the way for one thing, i.e., to equip LLMs\nwith multimodal tools.\nA\nGPT4Tools Dataset\nTable 5: Summary of tool names. Gray tool names are from Visual ChatGPT [5]. Black tool names\nare new in GPT4Tools.\nImage Generation\nImage Understanding\nGenerate Image From User Input Text\nDetect the Given Object\nText Detection On Image\nGenerate Image Condition On Canny Image\nSegment the Image\nDetection\nGenerate Image Condition On Depth\nGet Photo Description\nImage Super-Resolution\nInstruct Image Using Text\nEdge Detection On Image\nCrop the Given Object\nGenerate Image Condition On Sketch Image\nPredict Depth On Image\nAssess the Image Quality\nReplace Something From The Photo\nLine Detection On Image\nRecognize Face\nGenerate Image Condition On Segmentations\nAnswer Question About The Image\nDetect Face\nGenerate Image Condition On Pose Image\nSketch Detection On Image\nGenerate Image Condition On Soft Hed Boundary Image\nPose Detection On Image\nGenerate Image Condition On Normal Map\nHed Detection On Image\nRemove Something From The Photo\nPredict Normal Map On Image\nGenerate 3D Asset From User Input Text\nSegment the Given Object\n10\n2.5%\n4.0%\n0.7%\n3.4%\n3.7%\n2.3%\n3.7%\n2.3%\n2.7%\n0.5%\n3.8%\n2.3%\n2.3%\n3.8%\n2.2%\n3.7%\n2.3%\n3.7%\n2.3%\n0.2%\n2.7%\n2.6%\n0.1%\n42.0%\n58.0%\n42.0%\nDistribution of Training Set\n12.3%\n12.1%\n12.3%\n12.3%\n9.0%\n10.9%\n12.3%\n11.2%\n7.7%\n92.3%\n7.7%\nDistribution of Test Set\nDetect the Give Object\nSegment the Image\nGet Photo Description\nGenerate Image From User Input Text\nEdge Detection On Image\nGenerate Image Condition On Canny Image\nPredict Depth On Image\nGenerate Image Condition On Depth\nAnswer Question About The Image\nInstruct Image Using Text\nSketch Detection On Image\nGenerate Image Condition On Sketch Image\nGenerate Image Condition On Segmentations\nPose Detection On Image\nGenerate Image Condition On Pose Image\nHed Detection On Image\nGenerate Image Condition On Soft Hed Boundary Image\nPredict Normal Map On Image\nGenerate Image Condition On Normal Map\nLine Detection On Image\nSegment the given object\nRemove Something From The Photo\nReplace Something From The Photo\nNegative Sample\nText Detection On Image\nDetect Face\nRecognize Face\nAssess the Image Quality\nCrop the Given Object\nImage Super-Resolution\nDetection\nGenerate 3D Asset From User Input Text\nFigure 7: Data distribution of GPT4Tools. The purple piece refers to negative samples, while the\nothers are positive samples.\nA.1\nTraining Set\nThe training set of GPT4Tools has 71.4K instruction-following data, which includes 35.7K items\nusing tools. Note that these instruction-response pairs are generated from 41K items in Y +\nS since\nsome actions require two tools. The instructional data in the training set involves 23 tools whose\nnames are shown in Table 5 (marked in gray). The distribution of these 23 tools is illustrated on the\nleft of Figure 7. We employ this training set to instruct the language model to invoke tools.\nA.2\nEvaluation Set.\nThe evaluation set consists of two parts: validation set and test set.\nValidation. The validation set has 1170 samples in total, which includes the same tools as the training\nset. The number of each tool is almost 50. This set contains some augmented samples as the training\nset. Thus, it is utilized to verify the effectiveness of the language model in understanding tools after\nfine-tuning with the training set.\nTest. The test set includes 8 tools unseen by the training set. All unseen tool names are marked in\nblack and shown in Table 5, and their detailed definitions are shown in Table 6. The total number\nof samples is 652, whose distribution is shown on the right of Figure 7. As this set only involves\nsingle-turn samples, it is used to evaluate the zero-shot capability of invoking tools by the language\nmodel.\nB\nPrompt\nTool Prompt. The proposed GPT4Tools supports 31 tools, including 23 tools defined in Visual\nChatGPT [5] and 8 new tools. They are dependent on image generation models (e.g. ControlNet [44],\nStable Diffusion [46], InstructPix2Pix [47], and Shape-E [48]), and image understanding models\n(e.g. SAM [49], BLIP [22], MMDetection [50], MMOCR [51], MMagic [52], Face Recognition 2,\nGroundingDINO [53], and others [54\u201366].). All tool names are summarized in Table 5, where black\ntexts are the newly defined tools. Detailed descriptions of the new tools are illustrated in Table 6, in\nwhich the prompt defines the usage scenario of the tool and its arguments.\nGeneration Prompt. We encouraged the GPT-3.5 (gpt-3.5-turbo) [3] to generate instruction-\nfollowing data by utilizing the prompt outlined in Table 7. Subsequently, we filtered out noisy\ninstructions, as exemplified in Table 8. Based on the retained data, we performed augmentation\nfollowing the steps described in \u00a7 3.1, resulting in the tool-related dataset.\nTool-Usage Prompt. During replying to the user command, we encouraged the fine-tuned language\nmodel to invoke tools by prompt shown in Table 9. In this prompt, the <image content> will be\n2https://github.com/ageitgey/face_recognition\n11\nTable 6: Details of new tools.\nNo.\nTool Name\nInput\nOutput\nPrompt\n1\nText Detection On Image\nimage path\ntext on the im-\nage\nUseful when you want to detect the text in the\nimage. The input to this tool should be a string,\nrepresenting the image_path.\n2\nDetection\nimage path\nbounding\nboxes of ob-\njects\nUseful when you want to detect all objects of the\nimage, but not detect a certain object according to\nthe text. like: detect all the objects in this image,\nor detect this image. The input to this tool should\nbe a string, representing the image_path.\n3\nImage Super-Resolution\nimage path\nimage path\nUseful when you want to enhance the resolution\nand quality of low-resolution images. like: en-\nhance this image, restore this image. The input to\nthis tool should be a string, representing the im-\nage_path.\n4\nGenerate 3D Asset From User\nInput Text\ntext\nimage path\nUseful when you want to generate an 3D assert\nfrom a user input text and save it to a file. like:\ngenerate a 3D assert of an object or something. The\ninput to this tool should be a string, representing\nthe text used to generate the 3D assert.\n5\nCrop the Given Object\nimage path, ob-\nject name\nimage path\nUseful when you want to crop given objects in\nthe picture. The input to this tool should be a\ncomma separated string of two, representing the\nimage_path, the text description of the object to be\ncropped.\n6\nAssess the Image Quality\nimage path\nquality score\nUseful when you want to give a quality score for\nthe input image. like: assess a quality score for\nthis image, what is the quality score of this image,\nor can you give a quality for this image. The input\nto this tool should be a string, representing the\nimage_path.\n7\nRecognize Face\nimage path\ntext\nUseful when you only want to recognize faces in\nthe picture. like: recognize who appears in the\nphoto. The input to this tool should be a string,\nrepresenting the image_path.\n8\nDetect Face\nimage path\nimage path\nUseful when you only want to detect out or tag\nfaces in the picture. like: find all the faces that ap-\npear in the picture. tag someone in the picture. The\ninput to this tool should be a string, representing\nthe image_path.\nTable 7: Generation Prompt. During generation, <caption> will be replaced with the ground-truth\ncaption and bounding boxes. Green words are the desired instructions.\nGiven an image whose image path is example.png. Image caption: <caption>. The image\ncaption includes detail image description and each object paired with the bounding box\n(x1, y1, x2, y2). For the bounding box, (x1, y1) refers to the top left, and (x2, y2) refers to\nthe bottom right. x1 less than x2, and y1 less than y2.\nBelow are N visual tools. Each tool is defined as \"<tool name>: <usage scenario>, and\n<arguments>\". Please generate 1 visual instructions for each tools, so you need to generate\nN visual instruction in total.\nThe generated instructions should follow the format of\"<instruction>, [<tool name>,\n<arguments>]\". Each instruction must relate to the caption and can be solved by the tool.\nYou can not revise the \"<tool name>\", or add any other fake tools that is not defined. You\nmust keep the correct \"<arguments>\".\nTools:\n<tool name>: <usage scenario>, <arguments>\nNote that your generated visual instructions should be related to the image caption extremely.\nPlease generate complex and deceptive instructions as much as possible.\n12\nTable 8: Cases of noise during the generation. (\u2717) indicates the noise examples, while (\u2714) indicates\nthe corrected examples.\nError Format\n(\u2717) Segment the young boy swinging the bat [Segment the Given Object, \"example.jpg, young\nboy swinging the bat\"] (The instruction is not separated by a comma.)\n(\u2714) Segment the young boy swinging the bat, [Segment the Given Object, \"example.jpg,\nyoung boy swinging the bat\"]\nError Arguments\n(\u2717) Make the image look like a painting, [Instruct Image Using Text, \"painting\"]\n(\u2714) Make the image look like a painting, [Instruct Image Using Text, \"example.png, painting\"]\nError Tools\n(\u2717) Generate a real image of a cake and pie display from a sketch, [Generate Image Condition\nOn Canny Image, \"example.png, sketch of a cake and pie display\"]\n(\u2714) Generate a real image of a cake and pie display from a sketch, [Generate Image Condition\nOn Sketch Image, \"example.png, sketch of a cake and pie display\"]\nreplaced with the predicted image caption if the <user input> requires the image content as the\nprecondition.\nC\nCase Study\nNoise During the Generation of Instructions. While ChatGPT [3] or GPT-4 [4] have demonstrated\nthe ability to generate high-quality data [39, 40], there still are some noises in the generated data. For\ninstance, Table 8 shows three kinds of cases with noise, including the sample with error format, the\nsample with error arguments, and the sample assigned error tools. Therefore, a practical and effective\nfiltering step is necessary when using data generated by large language models.\nBad Cases of GPT-3.5. As shown in Table 10 and 11, the GPT-3.5 [3] invokes the wrong tools\nto response the user command. Therefore, when using a language model as a controller to build\na generalist model, it is advisable to employ our GPT4Tools to enhance the accuracy of language\nmodel actions further.\nD\nExperiment Settings\nIn \u00a7 4, we benchmark tool-usage ability of the language model using a self-built dataset. The\nfine-tuning configuration is recorded in Table 12.\n13\nTable 9: Tool-Usage Prompt. During inference, <image content> will be replaced with the result\nfrom image caption tools, and <user input> will be filled with the user command.\nGPT4Tools can handle various text and visual tasks, such as answering questions and pro-\nviding in-depth explanations and discussions. It generates human-like text and uses tools to\nindirectly understand images. When referring to images, GPT4Tools follows strict file name\nrules. To complete visual tasks, GPT4Tools uses tools and stays loyal to observation outputs.\nUsers can provide new images to GPT4Tools with a description, but tools must be used for\nsubsequent tasks.\nTools:\n<tool name>: <usage scenario>, <arguments>\nTo use a tool, please use the following format:\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of <tool name list>\nAction Input: the input to the action\nObservation: the result of the action\nWhen you have a response to say to the Human, or if you do not need to use a tool, you must\nuse the format:\nThought: Do I need to use a tool? No\nAI: [your response here]\nFollow file name rules and do not fake non-existent file names. Remember to provide the\nimage file name loyally from the last tool observation.\nPrevious conversation:\nHuman: Provide an image named . Description: <image content>\nAI: Received.\nNew input: <user input>\nGPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and\nobservations in the conversation are only visible to GPT4Tools. When answering human\nquestions, repeat important information. Let\u2019s think step by step.\n14\nTable 10: Incorrect example from GPT-3.5 (text-davinci-003) [3].\nInstruction:\nGPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth\nexplanations and discussions. It generates human-like text and uses tools to indirectly understand images.\nWhen referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses\ntools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description,\nbut tools must be used for subsequent tasks.\nTools:\n> Get Photo Description: useful when you want to know what is inside the photo. receives image_path as\ninput. The input to this tool should be a string, representing the image_path.\n> Answer Question About The Image: useful when you need an answer for a question based on an image.\nlike: what is the background color of the last image, how many cats in this figure, what is in this figure. The\ninput to this tool should be a comma separated string of two, representing the image_path and the question.\n> Detect the Give Object: useful when you only want to detect or find out given objects in the pictureThe\ninput to this tool should be a comma separated string of two, representing the image_path, the text description\nof the object to be found.\n> Generate Image Condition On Pose Image: useful when you want to generate a new real image from both\nthe user description and a human pose image. like: generate a real image of a human from this human pose\nimage, or generate a new real image of a human from this pose. The input to this tool should be a comma\nseparated string of two, representing the image_path and the user description.\n> Generate Image Condition On Sketch Image: useful when you want to generate a new real image from\nboth the user description and a scribble image or a sketch image. The input to this tool should be a comma\nseparated string of two, representing the image_path and the user description.\nTo use a tool, please use the following format:\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [Get Photo Description, Answer Question About The Image,\nDetect the Give Object, Generate Image Condition On Pose Image, Generate Image Condition On Sketch\nImage]\nAction Input: the input to the action\nObservation: the result of the action\nWhen you have a response to say to the Human, or if you do not need to use a tool, you must use the format:\nThought: Do I need to use a tool? No\nAI: [your response here]\nFollow file name rules and do not fake non-existent file names. Remember to provide the image file name\nloyally from the last tool observation.\nPrevious conversation:\nHuman: Provide an image named examples/hybowtyx.png. Description: An open bathtub with sliding glass\nshower door. A bathroom with gray walls and a clear sliding door. A handicap accessible shower and tub\nwith a glass door. A bathtub shower combination with a hand shower and cradle. An enclosed shower with a\nwindow and bathtub. Understand the image using tools.\nAI: Received.\nNew input: What objects are on the sink?\nGPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the\nconversation are only visible to GPT4Tools. When answering human questions, repeat important information.\nLet\u2019s think step by step.\nThought: Do I need to use a tool?\n(\u2717) Response:\nThought: Do I need to use a tool? Yes\nAction: Get Photo Description\nAction Input: examples/hybowtyx.png\nObservation: The image contains a bathtub with a sliding glass shower door, a bathroom with gray walls, a\nclear sliding door, a handicap accessible shower and tub with a glass door, a bathtub shower combination with\na hand shower and cradle, and an enclosed shower with a window and bathtub.\n(\u2714) The Ground-truth Response:\nThought: Do I need to use a tool? Yes\nAction: Answer Question About The Image\nAction Input: examples/hybowtyx.png, what objects are on the sink\nObservation: [output_from_vqa_tool]\n15\nTable 11: Incorrect example from GPT-3.5 (text-davinci-003) [3].\nInstruction:\nGPT4Tools can handle various text and visual tasks, such as answering questions and providing in-depth\nexplanations and discussions. It generates human-like text and uses tools to indirectly understand images.\nWhen referring to images, GPT4Tools follows strict file name rules. To complete visual tasks, GPT4Tools uses\ntools and stays loyal to observation outputs. Users can provide new images to GPT4Tools with a description,\nbut tools must be used for subsequent tasks.\nTools:\n> Text Detection On Image: useful when you want to detect the text in the image. The input to this tool should\nbe a string, representing the image_path.\n> Remove Something From The Photo: useful when you want to remove and object or something from the\nphoto from its description or location. The input to this tool should be a comma separated string of two,\nrepresenting the image_path and the object need to be removed.\n> Hed Detection On Image: useful when you want to detect the soft hed boundary of the image. like: detect\nthe soft hed boundary of this image, or hed boundary detection on image, or perform hed boundary detection\non this image, or detect soft hed boundary image of this image. The input to this tool should be a string,\nrepresenting the image_path.\n> Answer Question About The Image: useful when you need an answer for a question based on an image.\nlike: what is the background color of the last image, how many cats in this figure, what is in this figure. The\ninput to this tool should be a comma separated string of two, representing the image_path and the question.\nTo use a tool, please use the following format:\nThought: Do I need to use a tool? Yes\nAction: the action to take, should be one of [Text Detection On Image, Remove Something From The Photo,\nHed Detection On Image, Crop Image, Answer Question About The Image]\nAction Input: the input to the action\nObservation: the result of the action\nWhen you have a response to say to the Human, or if you do not need to use a tool, you must use the format:\nThought: Do I need to use a tool? No\nAI: [your response here]\nFollow file name rules and do not fake non-existent file names. Remember to provide the image file name\nloyally from the last tool observation.\nPrevious conversation:\nProvide an image named image/wuspouwe.png. Description: A man riding a snowboard down a snow covered\nslope. A man is snowboarding down the snowy mountain. A person is snowboarding in a snowy wooded area.\nA man in a red jacket snowboarding down a hill. A man is snowboarding alone with trees in the background.\nUnderstand the image using tools.\nAI: Received.\nNew input: What is the color of the man\u2019s jacket?\nGPT4Tools needs to use tools to observe images, not directly imagine them. Thoughts and observations in the\nconversation are only visible to GPT4Tools. When answering human questions, repeat important information.\nLet\u2019s think step by step.\nThought: Do I need to use a tool?\n(\u2717) Response:\nThought: Do I need to use a tool? Yes\nAction: Hed Detection On Image\nAction Input: image/wuspouwe.png\nObservation: The man\u2019s jacket is red.\n(\u2714) The Ground-truth Response:\nThought: Do I need to use a tool? Yes\nAction: Answer Question About The Image\nAction Input: image/wuspouwe.png, what is the color of the man\u2019s jacket?\nObservation: [output_from_vqa_tool]\n16\nTable 12: Fine-tuning configuration.\nHyper-parameters\nVicuna [12] & LLaMA [37]\nOPT [13]\noptimizer\nAdamW [45]\nAdamW [45]\nlearning rate\n3e-4\n1.2e-4\nwarm steps\n100\n100\nweight decay\n0.0\n0.0\noptimizer momentum\n\u03b21, \u03b22=0.9, 0.999\n\u03b21, \u03b22=0.9, 0.999\nbatch size\n512\n512\nepoch\n3\n3\nmax length\n2048\n2048\nLoRA [38] attention dimension (r)\n16\n16\nLoRA [38] scaling alpha (\u03b1)\n16\n16\nLoRA [38] drop out\n0.05\n0.05\n17\nReferences\n[1] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020. 1, 4\n[2] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\nTraining language models to follow instructions with human feedback. In NeurIPS, 2022. 1, 3\n[3] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/, 2023. 1, 2, 3, 4, 6, 7, 11, 13, 15, 16\n[4] OpenAI. Gpt-4 technical report, 2023. 1, 3, 4, 13\n[5] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt:\nTalking, drawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023. 1, 2,\n3, 7, 9, 10, 11\n[6] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and\naction. arXiv preprint arXiv:2303.11381, 2023. 1, 2, 3\n[7] Mojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. arXiv preprint\narXiv:2107.07566, 2021. 2, 3\n[8] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In\nICLR, 2017. 6\n[9] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv\npreprint arXiv:2201.08239, 2022. 3\n[10] Angeliki Lazaridou, Elena Gribovskaya, Wojciech Stokowiec, and Nikolai Grigorev. Internet-augmented\nlanguage models through few-shot prompting for open-domain question answering. arXiv preprint\narXiv:2203.05115, 2022. 2, 3\n[11] Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. arXiv\npreprint arXiv:2302.04761, 2023. 2, 3, 4\n[12] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chat-\nbot impressing gpt-4 with 90%* chatgpt quality. https://lmsys.org/blog/2023-03-30-vicuna/,\n2023. 2, 3, 4, 7, 8, 9, 17\n[13] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022. 2, 4, 7, 17\n[14] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models: Methods,\nanalysis & insights from training gopher. arXiv preprint arXiv:2112.11446, 2021. 2, 3, 4\n[15] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, 2021. 2, 4\n[16] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot\nplanners: Extracting actionable knowledge for embodied agents. In ICML, volume 162 of Proceedings of\nMachine Learning Research, pages 9118\u20139147. PMLR, 2022. 2, 4\n[17] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An\nempirical study of GPT-3 for few-shot knowledge-based VQA. In AAAI, pages 3081\u20133089. AAAI Press,\n2022. 2, 4\n[18] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Hongsheng Li, Xiaohua Wang, and Jifeng Dai. Uni-\nperceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot tasks. In\nCVPR, pages 16783\u201316794. IEEE, 2022. 2\n18\n[19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: A\nunified model for vision, language, and multi-modal tasks. arXiv preprint arXiv:2206.08916, 2022. 2\n[20] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J. Fleet, and Geoffrey E. Hinton. A unified\nsequence interface for vision tasks. In NeurIPS, 2022. 2\n[21] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel\nBarth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. A generalist agent.\narXiv preprint arXiv:2205.06175, 2022. 2\n[22] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. BLIP: bootstrapping language-image pre-\ntraining for unified vision-language understanding and generation. In ICML, volume 162 of Proceedings\nof Machine Learning Research, pages 12888\u201312900. PMLR, 2022. 2, 11\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\n[24] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,\nTengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L. Menick, Sebastian Borgeaud,\nAndy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals,\nAndrew Zisserman, and Kar\u00e9n Simonyan. Flamingo: a visual language model for few-shot learning. In\nNeurIPS, 2022.\n[25] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language\nmodel. arXiv preprint arXiv:2303.03378, 2023. 2\n[26] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. OFA: unifying architectures, tasks, and modalities through a simple sequence-\nto-sequence learning framework. In ICML, volume 162 of Proceedings of Machine Learning Research,\npages 23318\u201323340. PMLR, 2022. 2\n[27] Alexander Kolesnikov, Andr\u00e9 Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil\nHoulsby. Uvim: A unified modeling approach for vision with learned guiding codes. In NeurIPS, 2022. 2\n[28] Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting language models for zero-shot learning\nby meta-tuning on dataset and prompt collections. In EMNLP (Findings), pages 2856\u20132878, 2021. 3\n[29] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR. OpenReview.net, 2022.\n3\n[30] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416, 2022. 3\n[31] Stephen H. Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht\nSharma, Taewoon Kim, M. Saiful Bari, Thibault F\u00e9vry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing\nSun, Srulik Ben-David, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Alan Fries, Maged Saeed\nAlShaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir R. Radev,\nMike Tian-Jian Jiang, and Alexander M. Rush. Promptsource: An integrated development environment and\nrepository for natural language prompts. In ACL, pages 93\u2013104. Association for Computational Linguistics,\n2022.\n[32] Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, D\u00e1niel Simig, Ping Yu, Kurt\nShuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. Opt-iml: Scaling language model instruction\nmeta learning through the lens of generalization. arXiv preprint arXiv:2212.12017, 2022. 3\n[33] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and\nHannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. arXiv\npreprint arXiv:2212.10560, 2022. 3\n[34] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.\ncom/tatsu-lab/stanford_alpaca, 2023. 3, 4\n19\n[35] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing\nvision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592,\n2023.\n[36] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. 3, 4, 9, 10\n[37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971, 2023. 3, 7, 17\n[38] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 3, 6, 7, 17\n[39] Fabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056, 2023. 4, 13\n[40] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. Instruction tuning with gpt-4.\narXiv preprint arXiv:2304.03277, 2023. 4, 5, 13\n[41] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In\nEMNLP, pages 3980\u20133990, 2019. 4\n[42] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, pages 248\u2013255, 2009. 6\n[43] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, pages 740\u2013755, 2014.\n6\n[44] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023.\n6, 11\n[45] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 7, 17\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, pages 10674\u201310685, 2022. 11\n[47] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing\ninstructions. arXiv preprint arXiv:2211.09800, 2022. 11\n[48] Heewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint\narXiv:2305.02463, 2023. 11\n[49] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete\nXiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv preprint\narXiv:2304.02643, 2023. 11\n[50] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng,\nZiwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu\nLi, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change\nLoy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint\narXiv:1906.07155, 2019. 11\n[51] Zhanghui Kuang, Hongbin Sun, Zhizhong Li, Xiaoyu Yue, Tsui Hin Lin, Jianyong Chen, Huaqiang Wei,\nYiqin Zhu, Tong Gao, Wenwei Zhang, Kai Chen, Wayne Zhang, and Dahua Lin. Mmocr: A comprehensive\ntoolbox for text detection, recognition and understanding. arXiv preprint arXiv:2108.06543, 2021. 11\n[52] MMagic Contributors. MMagic: OpenMMLab multimodal advanced, generative, and intelligent creation\ntoolbox. https://github.com/open-mmlab/mmagic, 2023. 11\n[53] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object\ndetection. arXiv preprint arXiv:2303.05499, 2023. 11\n[54] Jianfeng Wang, Lin Song, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. End-to-end object\ndetection with fully convolutional network. In CVPR, 2021. 11\n[55] Yanwei Li, Lin Song, Yukang Chen, Zeming Li, Xiangyu Zhang, Xingang Wang, and Jian Sun. Learning\ndynamic routing for semantic segmentation. In CVPR, 2020.\n20\n[56] Lin Song, Shiwei Zhang, Gang Yu, and Hongbin Sun. Tacnet: Transition-aware context network for\nspatio-temporal action detection. In CVPR, 2019.\n[57] Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, and Nanning Zheng. Learnable tree\nfilter for structure-preserving feature transform. NIPS, 2019.\n[58] Shiwei Zhang, Lin Song, Changxin Gao, and Nong Sang. Glnet: Global local network for weakly\nsupervised action localization. IEEE Transactions on Multimedia, 22(10):2610\u20132622, 2019.\n[59] Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Hongbin Sun, Jian Sun, and Nanning Zheng. Fine-\ngrained dynamic head for object detection. NIPS, 2020.\n[60] Jianwen Jiang, Yu Cao, Lin Song, Shiwei Zhang, Yunkai Li, Ziyao Xu, Qian Wu, Chuang Gan, Chi Zhang,\nand Gang Yu. Human centric spatio-temporal action localization. In ActivityNet Workshop on CVPR,\n2018.\n[61] Lin Song, Yanwei Li, Zhengkai Jiang, Zeming Li, Xiangyu Zhang, Hongbin Sun, Jian Sun, and Nanning\nZheng. Rethinking learnable tree filter for generic feature transform. NIPS, 2020.\n[62] Lin Song, Songyang Zhang, Songtao Liu, Zeming Li, Xuming He, Hongbin Sun, Jian Sun, and Nanning\nZheng. Dynamic grained encoder for vision transformers. NIPS, 2021.\n[63] Jinrong Yang, Lin Song, Songtao Liu, Zeming Li, Xiaoping Li, Hongbin Sun, Jian Sun, and Nanning\nZheng. Dbq-ssd: Dynamic ball query for efficient 3d object detection. arXiv preprint arXiv:2207.10909,\n2022.\n[64] Songyang Zhang, Lin Song, Songtao Liu, Zheng Ge, Zeming Li, Xuming He, and Jian Sun. Workshop on\nautonomous driving at cvpr 2021: Technical report for streaming perception challenge. arXiv preprint\narXiv:2108.04230, 2021.\n[65] Rui Yang, Lin Song, Yixiao Ge, and Xiu Li. Boxsnake: Polygonal instance segmentation with box\nsupervision. arXiv preprint arXiv:2303.11630, 2023.\n[66] Xuchong Zhang, Hongbin Sun, Shiqiang Chen, Lin Song, and Nanning Zheng. Nipm-swmf: Toward\nefficient fpga design for high-definition large-disparity stereo matching. IEEE Transactions on Circuits\nand Systems for Video Technology, 29(5):1530\u20131543, 2018. 11\n21\n"
  },
  {
    "title": "What indeed can GPT models do in chemistry? A comprehensive benchmark on eight tasks",
    "link": "https://arxiv.org/pdf/2305.18365.pdf",
    "upvote": "2",
    "text": "What can Large Language Models do in chemistry? A\ncomprehensive benchmark on eight tasks\nTaicheng Guo\u2217, Kehan Guo\u2217, Bozhao Nan, Zhenwen Liang, Zhichun Guo,\nNitesh V. Chawla, Olaf Wiest, Xiangliang Zhang\u2020\nUniversity of Notre Dame\n{tguo2, kguo2, bnan, zliang6, zguo5, nchawla, owiest, xzhang33}@nd.edu\nAbstract\nLarge Language Models (LLMs) with strong abilities in natural language process-\ning tasks have emerged and have been applied in various kinds of areas such as\nscience, finance and software engineering. However, the capability of LLMs to\nadvance the field of chemistry remains unclear. In this paper, rather than pursuing\nstate-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide\nrange of tasks across the chemistry domain. We identify three key chemistry-\nrelated capabilities including understanding, reasoning and explaining to explore\nin LLMs and establish a benchmark containing eight chemistry tasks. Our anal-\nysis draws on widely recognized datasets facilitating a broad exploration of the\ncapacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4,\nGPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry\ntask in zero-shot and few-shot in-context learning settings with carefully selected\ndemonstration examples and specially crafted prompts. Our investigation found\nthat GPT-4 outperformed other models and LLMs exhibit different competitive\nlevels in eight chemistry tasks. In addition to the key findings from the comprehen-\nsive benchmark analysis, our work provides insights into the limitation of current\nLLMs and the impact of in-context learning settings on LLMs\u2019 performance across\nvarious chemistry tasks. The code and datasets used in this study are available at\nhttps://github.com/ChemFoundationModels/ChemLLMBench.\n1\nIntroduction\nLarge language models (LLMs) have recently demonstrated impressive reasoning abilities across\na wide array of tasks. These tasks are not limited to natural language processing, but also extend\nto various language-related applications within scientific domains [56, 30, 24, 10]. Much of the\nresearch on the capacity of LLMs in science has been focused on tasks such as answering medical\n[30] and scientific questions [24, 25]. However, the exploration of their application to practical tasks\nin the field of chemistry remains underinvestigated. Although some studies [6, 27, 63, 48] have been\nconducted, they tend to focus on specific case studies rather than a comprehensive or systematic\nevaluation. The exploration of LLMs\u2019 capabilities within the field of chemistry has the potential to\nrevolutionize this domain and expedite research and development activities [62]. Thus, the question,\n\u201cWhat can LLMs do in chemistry?\u201d is a compelling topic of inquiry for both AI researchers and\nchemists. Nevertheless, there exist two challenges that hinder the answer to the topic and the further\ndevelopment of LLMs in chemistry:\n\u2217Both authors contribute equally to the work, under the support of NSF Center for Computer Assisted\nSynthesis (C-CAS). https://ccas.nd.edu.\n\u2020Corresponding author.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2305.18365v3  [cs.CL]  28 Dec 2023\n\u2022 Determining the potential capabilities of LLMs in chemistry requires a systematic analysis of\nboth LLMs and the specific requirements of chemistry tasks. There are different kinds of tasks in\nchemistry, some of which can be formulated to tasks solved by LLMs while others may not. It\nis necessary to consider the specific knowledge and reasoning required for each task and assess\nwhether LLMs can effectively acquire and utilize that knowledge.\n\u2022 Conducting reliable and wide-ranging evaluation requires diverse experimental settings and limita-\ntions, that is, careful consideration and standardization of evaluation procedures, dataset curation,\nprompt design, and in-context learning strategies. Additionally, the API call time consumption and\nthe randomness of LLMs limit the size of the testing.\nTo address this knowledge gap, we (a group of AI researchers and chemists) have developed a\ncomprehensive benchmark to provide a preliminary investigation into the abilities of LLMs across a\ndiverse range of practical chemistry tasks. Our aim is to gain insights that will be beneficial to both\nAI researchers and chemists to advance the application of LLMs in chemistry. For AI researchers,\nwe provide insights into the strengths, weaknesses, and limitations of LLMs in chemistry-related\ntasks, which can inform the further development and refinement of different AI techniques for more\neffective applications within the field. For chemists, our study provides a better understanding of the\ntasks in which they can rely on current LLMs. Utilizing our more extensive experimental setup, a\nbroader range of chemistry tasks can be explored to further evaluate the capabilities of LLMs.\nOur investigation focuses on 8 practical chemistry tasks, covering a diverse spectrum of the chemistry\ndomain. These include: 1) name prediction, 2) property prediction, 3) yield prediction, 4) reaction\nprediction, 5) retrosynthesis (prediction of reactants from products), 6) text-based molecule design,\n7) molecule captioning, and 8) reagents selection. Our analysis draws on widely available datasets\nincluding BBBP, Tox21 [65], PubChem [32], USPTO [29, 53, 39], and ChEBI [17, 16]. Five LLMs\n(GPT-4, GPT-3.5, Davinci-003, Llama, and Galactica) [43] are evaluated for each chemistry task in\nzero-shot and few-shot in-context learning settings with carefully selected demonstration examples\nand specific prompts. We highlight the contributions of this paper as follows:\n\u2022 We are the first to establish a comprehensive benchmark to evaluate the abilities of LLMs on a\nwide range of chemistry tasks. These eight selected tasks, in consultation with chemists, not only\nencompass a diverse spectrum of the chemistry domain but also demand different abilities such as\nunderstanding, reasoning, and explaining using domain-specific chemistry knowledge.\n\u2022 We provide a comprehensive experimental framework for testing LLMs in chemistry tasks. To\nfactor in the impact of prompts and demonstration examples in in-context learning, we have\nassessed multiple input options, focusing on the description of chemistry tasks. Five representative\nconfigurations were chosen based on their performance on a validation set, then these selected\noptions were applied on the testing set. The conclusion is made from five repeated evaluations on\neach task, since GPTs often yield different outputs at different API calls even though the input is\nthe same. We thus believe that our benchmarking process is both reliable and systematic.\n\u2022 Our investigations yield broader insights into the performance of LLMs on chemistry tasks. As\nsummarized in Table 2, our findings confirm some anticipated outcomes (e.g., GPT-4 outperforms\nGPT-3 and Davinci-003), and also reveal unexpected discoveries (e.g., property prediction can be\nbetter solved when property label semantics are included in prompts). Our work also contributes to\npractical recommendations that can guide AI researchers and chemists in leveraging LLMs more\neffectively in the future (see Section 5).\nThe paper is organized as follows. Related works are presented in Section 2. In section 3, we\nelaborate on the evaluation process, including an overview of the chemistry tasks, the utilized LLMs\nand prompts, and the validation and testing settings. In section 4, we summarize the main findings\n(due to the space limit, evaluation details of each chemistry task can be found in Appendix). Finally,\nto answer the question \u201cWhat can LLMs do in chemistry?\u201d we discuss the constraints inherent to\nLLMs and how different settings related to LLMs affect performance across various chemistry tasks\nin Section 5. The conclusions are summarized in section 6.\n2\nRelated Work\nLarge Language Models. The rise of Large Language Models (LLMs) has marked a significant trend\nin recent natural language processing (NLP) research. This progress has been fuelled by milestones\nsuch as the introduction of GPT-3 [4], T0 [52], Flan-T5 [12], Galactica [56] and LLaMa [57]. The\n2\nrecently released GPT-4, an evolution from GPT-3.5 series, has drawn considerable attention for its\nimprovements in language understanding, generation, and planning [43]. Despite the vast potential of\nLLMs, existing research primarily centers on their performance within general NLP tasks [8, 9]. The\nscientific disciplines, notably chemistry, have received less focus. The application of LLMs in these\nspecialized domains presents an opportunity for significant advancements. Therefore, we conduct a\ncomprehensive experimental analysis to evaluate the capability of LLMs in chemistry-related tasks.\nLarge Language Model Evaluations. In recent years, the evaluation of LLMs like GPT has become\na significant field of inquiry. [11] showed ChatGPT\u2019s proficiency in law exams, while technical\naspects of GPT-4 were analyzed in [43]. LLMs are also applied in healthcare [14] , mathematical\nproblem [18], and code generation tasks [37]. Specifically, in healthcare, the utility and safety\nof LLMs in clinical settings were explored [42]. In the context of mathematical problem-solving,\nstudies [18, 7] have highlighted that LLMs encounter challenges with graduate-level problems,\nprimarily due to difficulties in parsing complex syntax. These studies underscored the complexity of\nachieving task-specific accuracy and functionality with LLMs. Lastly, AGIEval [66] assessed LLMs\u2019\ngeneral abilities but noted struggles in complex reasoning tasks.\nOur work aligns with these evaluations but diverges in its focus on chemical tasks. To our knowledge,\nthis is the first study to transform such tasks to suit LLM processing and to perform a comprehensive\nevaluation of these models\u2019 ability to tackle chemistry-related problems. This focus will contribute to\nexpand our understanding of LLMs\u2019 capabilities in specific scientific domains.\nLarge Language Model for Chemistry. Recent efforts integrating LLMs with the field of chemistry\ngenerally fall into two distinct categories. One category aims to create a chemistry agent with\nLLMs\u2019 by leveraging its planning ability to utilize task-related tools. For example, Bran et al [3]\ndeveloped ChemCrow, which augmented LLMs with chem-expert designed tools for downstream\ntasks such as organic synthesis and drug discovery. Similarly, by leveraging the planning and\nexecution ability of multiple LLMs, Boiko et al [2] developed an autonomous chemical agent to\nconduct chemical experiments. The other category involves direct usage of LLMs for downstream\ntasks in chemistry [27, 62, 6, 28]. While these studies have explored the performance of LLMs in\nchemistry-related tasks, a systematic evaluation of their capabilities within this domain has been\nlacking. Consequently, there is a noticeable gap that calls for a meticulous benchmark to thoroughly\nassess the potential of LLMs in chemistry. Such a benchmark is crucial not only for identifying the\nstrengths and limitations of these models in a specialized scientific domain, but also to guide future\nimprovements and applications.\n3\nThe Evaluation Process and Setting\nThe evaluation process workflow is depicted in Fig. 1. Guided by co-author Prof. Olaf Wiest (from\nthe Department of Chemistry at the University of Notre Dame), we identify eight tasks in discussion\nwith senior Ph.D. students at the NSF Center for Computer Assisted Synthesis (C-CAS). Following\nthis, we generate, assess, and choose suitable prompts to forward to LLMs. The acquired answers are\nthen evaluated both qualitatively by chemists to identify whether they are helpful in the real-world\nscenario and quantitatively by selected metrics.\nChemistry tasks. In order to explore the abilities of LLMs in the field of chemistry, we concentrate\non three fundamental capabilities: understanding, reasoning, and explaining. We examine these\ncompetencies through eight diverse and broadly acknowledged practical chemistry tasks. These tasks\nare summarized in Table 1, in terms of the task type from the perspective of machine learning, the\ndataset used for the evaluation, as well as the evaluation metrics. The #ICL candidates refers to the\nnumber of candidate examples, from which we select k demonstration examples, either randomly\nor based on similarity searches. These candidate sets are the training sets used in classical machine\nlearning models, e.g., in training classifiers or generative models. We set the test set of 100 instances,\nrandomly sampled from the original testing dataset (non-overlapping with the training set). To reduce\nthe influence of the LLMs randomness on the results, each evaluation experiment is repeated five\ntimes and the mean and variance are reported.\nLLMs. For all tasks, we evaluate the performance of five popular LLMs: GPT-4, GPT-3.5 (referred\nto as GPT-3.5-turbo, also known as ChatGPT), Davinci-003, LLama and Galactica.\n3\nFigure 1: Overview of the evaluation process\nTable 1: The statistics of all tasks, datasets, the number of ICL/test samples, and evaluation metrics\nAbility\nTask\nTask Type\nDataset\n#ICL candidates\n#test\nEvaluation Metrics\nUnderstanding\nName Prediction\nGeneration\nPubChem\n500\n100\nAccuracy\nProperty Prediction\nClassification\nBBBP, HIV, BACE,\nTox21, ClinTox\n2053, 41127, 1514,\n8014, 1484\n100\nAccuracy, F1 score\nReasoning\nYield Prediction\nClassification\nBuchwald-Hartwig,\nSuzuki-Miyaura\n3957,\n5650\n100\nAccuracy\nReaction Prediction\nGeneration\nUSPTO-Mixed\n409035\n100\nAccuracy, Validity\nReagents Selection\nRanking\nSuzuki-Miyaura\n5760\n100\nAccuracy\nRetrosynthesis\nGeneration\nUSPTO-50k\n40029\n100\nAccuracy, Validity\nText-Based Molecule Design\nGeneration\nChEBI-20\n26407\n100\nBLEU, Exact Match, etc\nExplaining\nMolecule Captioning\nGeneration\nChEBI-20\n26407\n100\nBLEU, Chemists, etc\nZero-shot prompt. For each task, we apply a standardized zero-shot prompt template. As shown\nin Fig. 2, we instruct the LLMs to act in the capacity of a chemist. The content within the brackets\nis tailored to each task, adapting to its specific inputs and outputs. The responses from LLMs are\nconfined to only returning the desired output without any explanations.\nFigure 2: The standardized zero-shot prompt template for all tasks.\nTask-specific ICL prompt. ICL is a new paradigm for LLMs where predictions are based solely\non contexts enriched with a few demonstration examples [15]. This paper specifically denotes\nICL as a few-shot in-context learning approach, excluding the zero-shot paradigm. In order to\nthoroughly examine the capacities of LLMs within each chemistry-specific task, we design a task-\nspecific ICL prompt template. As shown in Fig. 3. The format of the template is similar to that\nused in [48]. We also partition our template into four parts: {General Template}{Task-Specific\nTemplate}{ICL}{Question}. The {General Template} is almost the same as the zero-shot prompt,\ninstructing the LLMs to play the role of a chemist and specify the chemistry task with its corresponding\ninput and output. Considering that the responses for chemistry-related tasks must be accurate and\nchemically reasonable, it is crucial to prevent LLMs from generating hallucinated information. To\nthis end, we introduce the {Task-Specific Template} which consists of three main components:\n[Input explanation], [Output Explanation], and [Output Restrictions], specifically designed to reduce\nhallucinations. These components are tailored to each task. The {ICL} part is a straightforward\n4\nconcatenation of the demonstration examples and it follows the structure \"[Input]: [Input_content]\n[Output]: [Output_content]\". The [Input] and [Output] denote the specific names of each task\u2019s\ninput and output, respectively. For example, in the reaction prediction task, the [Input] would be\n\"Reactants+Reagents\" and the [Input_content] would be the actual SMILES of reactants and reagents.\nThe [Output] would be \"Products\" and the [Output_content] would be the SMILES of products.\nDetailed ICL prompts for each task will be presented in their respective sections that follow. The\nlast {Question} part presents the testing case for LLMs to respond to. Fig 5 is example of our name\nprediction prompt.\nFigure 3: An ICL prompt template for all tasks.\nFigure 4: An ICL prompt example for smiles2iupac prediction\nICL strategies. To investigate the impact of the quality and quantity of ICL examples on the\nperformance of each task, we explore two ICL strategies. The quality is determined by the retrieval\nmethods employed for finding similar examples to the sample in question. We conduct a grid search\nacross two strategies: {Random, Scaffold}. In the Random strategy, we randomly select k examples\nfrom the ICL candidate pool. In the Scaffold strategy, if the [Input_content] is a molecule SMILES, we\nuse Tanimoto Similarity [55] from Morgan Fingerprint [41] with 2048-bits and radius=2 to calculate\nthe molecular scaffold similarity to find the top-k similar molecule SMILES. If the [Input_content]\nis a description such as IUPAC name or others, we use Python\u2019s built-in difflib.SequenceMatcher\ntool [49] to find the top-k similar strings. To explore the influence of the quantity of ICL examples\non performance, we also perform a grid search for k, the number of ICL examples, in each task.\nExperiment setup strategy. In property prediction and yield prediction tasks, we perform the grid\nsearch of k in {4, 8}. In the name prediction, reaction prediction, and retrosynthesis tasks, we perform\nthe grid search of k in {5, 20}. In text-based molecule design and molecule captioning tasks, we\n5\nTable 2: The rank of five LLMs on eight chemistry tasks and performance highlight (NC: not\ncompetitive, C: competitive, SC: selectively competitive, acc: accuracy).\nTask\nGPT-4\nGPT-3.5\nDavinci-003\nLlama2-13B-chat\nGAL-30B\nPerformance highlight (comparing to baselines if any)\nName Prediction\n1\n2\n3\n4\n5\nNC: max. acc. 8% (Table 4)\nProperty Prediction\n1\n2\n3\n5\n4\nSC: outperform RF and XGBoost from MoleculeNet [65] (Table 6)\nYield Prediction\n1\n3\n2\n5\n4\nC: but 16-20% lower acc. than UAGNN [34] (Table 10)\nReaction Prediction\n1\n3\n2\n5\n4\nNC: 70% lower acc. than Chemformer [26] (Table 11)\nReagents Selection\n2\n1\n3\n4\n5\nC: 40-50% acc. (Table 12)\nRetrosynthesis\n2\n3\n1\n5\n4\nNC: 40% lower acc. than Chemformer [26] (Table 13)\nMolecule Design\n1\n3\n2\n4\n5\nSC: better than MolT5-Large [17] (Table 14)\nMolecule Captioning\n1\n2\n1\n4\n5\nSC: better than MolT5-Large [17] (Table 15)\nAverage rank\n1.25\n2.375\n2.125\n4.5\n4.5\noverall: 3 SC, 2 C, 3 NC\nperform the grid search of k in {5, 10} because of the maximum token limitation of LLMs. To reduce\nthe time consumption of API requests caused by testing on the large test set, we first construct a\nvalidation set of size 30 which is randomly sampled from the original training set. Then we search k\nand retrieval strategies ({Random, Scaffold}) on the validation set. Based on the validation set results,\nwe take 5 representative options when testing on 100 instances, which are randomly sampled from\nthe original test set. For each task, we run evaluation 5 times and report mean and standard deviation.\n4\nExperiment Analysis\nDue to space limitations, we provide details of the evaluation on each chemistry task in Appendix by\nthe following order: name prediction in section A, property prediction in section B, yield prediction\nin section C, reaction prediction in section D, reagents selection in section E, retrosynthesis in section\nF, text-based molecule design in section G, and molecule captioning in section H. The detailed results\ndescribed in the Appendix allow us to approach the question \u201cWhat can LLMs do in chemistry?\"\nfrom several directions. We discuss the key findings from our comprehensive benchmark analysis and\nprovide valuable insights by thoroughly analyzing the limitation of LLMs and how different settings\nrelated to LLMs affect performance across various chemistry tasks.\n4.1\nCan LLMs outperform existing baselines in chemistry tasks?\nSeveral classic predictive models based on machine learning (ML) have been developed for specific\nchemistry tasks. For instance, MolR (Graph Neural Network-based) predicts molecule properties\nas a binary classification problem [58]. UAGNN achieved state-of-the-art performance in yield\nprediction [34]. MolT5-Large, a specialized language model based on T5, excels in translating\nbetween molecule and text [17]. We conduct a performance analysis of GPT models and compare\ntheir results with available baselines, if applicable. The main findings from the investigations are:\n\u2022 GPT-4 outperforms the other models evaluated. The ranking of the models on 8 tasks can be found\nin Table 2;\n\u2022 GPT models exhibit a less competitive performance in tasks demanding precise understanding of\nmolecular SMILES representation, such as name prediction, reaction prediction and retrosynthesis;\n\u2022 GPT models demonstrate strong capabilities both qualitatively (in Fig. 14 evaluated by chemists)\nand quantitatively in text-related explanation tasks such as molecule captioning;\n\u2022 For chemical problems that can be converted to classification tasks or ranking tasks, such as property\nprediction, and yield prediction, GPT models can achieve competitive performance compared to\nbaselines that use classical ML models as classifiers, or even better, as summarized in Table 2.\nThese conclusions are derived from conducting five repeated evaluations on each task, using the best\nevaluation setting that was discovered through a grid search on the validation set of each task. We\ndesignate the performance of GPT models as three categories and provide in-depth discussion next.\n\u2022 Tasks with not competitive (NC) performance. In tasks such as reaction prediction and\nretrosynthesis, GPT models are worse than existing ML baselines trained by large amounts of\ntraining data, partially because of the limitation on understanding molecular SMILES strings. In\nreaction prediction and retrosynthesis, SMILES strings are present in both the input and output\nof the GPT models. Without an in-depth understanding of the SMILES strings that represent\n6\nreactants and products, as well as the reaction process that transforms reactants into products, it\nwill be difficult for GPT models to generate accurate responses, as shown in Table 11 and 13. GPT\nmodels exhibit poor performance on the task of name prediction as well (see Table 4). This further\nvalidates the notion that GPT models struggle with understanding long strings in formats such as\nSMILES, IUPAC name, and molecular formula, and make correct translations between them.\n\u2022 Tasks with competitive (C) performance. GPT models can achieve satisfactory results when the\nchemistry tasks are formulated into the forms of classification (e.g., formatting yield prediction\ninto a high-or-not classification, instead of regression) or ranking (as seen in reagents selection),\nas illustrated in Fig. 7 and 9. This is understandable, because making choices is inherently\nsimpler than generating products, reactants or names. GPT models can achieve an accuracy of\n40% to 50% when asked to select the reactant or solvent or ligand from provided candidates.\nAlthough GPT-4\u2019s performance on yield prediction falls short compared to the baseline model\nUAGNN [34] (with 80% versus 96% on the Buchwald-Hartwig dataset, and 76% versus 96% on the\nSuzuki-coupling dataset), it demonstrates improved performance when given more demonstration\nexamples within the few-shot in-context learning scenario, as reported in Table 10. It is worth\nnoting that the UAGNN model was trained on thousands of examples for these specific reactions.\nLast, while GPT models exhibit promising performance for yield prediction on the evaluated\nHigh-Throughput Experimentation (HTE) datasets, specifically the Buchwald-Hartwig [1] and\nSuzuki-Miyaura datasets [50], they perform as bad as other ML baselines on more challenging\ndatasets like USPTO-50k [53]. This observation indicates a potential area for future research and\nimprovement in the performance of GPT models on challenging chemistry datasets.\n\u2022 Tasks with selectively competitive (SC) performance. GPT models are selectively competitive\non two types of tasks.\n\u2013 In the property prediction task on some datasets (HIV, ClinTox), GPT models outperform\nthe baseline significantly, achieving F1 scores and accuracy nearing 1, as reported in Table 6\nand 7. This might be due to the fact that the property labels to be predicted are included in the\nprompts, with GPT models being simply tasked in responding with yes or no. For example, the\nprompt includes inhibit HIV replication or drugs failed clinical trials for toxicity reason, and\nwe observed a significant decline in the performance of GPT models upon removing property\nlabels from the prompt (refer to Appendix section B). In contrast, baselines employing machine\nlearning models do not include the semantic meaning of these labels in their input. The input\nfor these models only comprises molecular representations in graph form but no labels.\n\u2013 For tasks related to text, such as text-based molecule design and molecule captioning, GPT\nmodels exhibit strong performance due to their language generation capabilities. On the task of\ntext-based molecule design, GPT models outperform the baseline when evaluated using NLP\nmetrics such as BLEU and Levenshtein. However, when it comes to exact match, the accuracy\nis less than 20%, as reported in Table 14 and 15. This suggests that the molecules designed\nby GPT models may not be exactly the same as the ground truth. Particularly in the context\nof molecular design/generation, the exact match is a significant metric. Unlike in natural\nlanguage generation where there is some allowance for deviation from the input, molecular\ndesign demands precise accuracy and chemical validity. However, not being precisely identical\nto the ground truth does not automatically invalidate a result. Molecules generated by GPT\nmodels may still prove to be beneficial and could potentially act as viable alternatives to the\nground truth, provided they meet the requirements outlined in the input text and the majority\n(over 89%) are chemically valid (see Table 14). Nonetheless, assessing the true utility of these\ngenerated molecules, such as evaluating their novelty in real-world applications, can be a\ntime-consuming undertaking.\n4.2\nThe capability of different LLMs\nAs shown in Table 2, we can find that GPT-4 model shows better chemical understanding, reasoning,\nand explaining abilities than Davinci-003, GPT-3.5, Llama and Galactica. This further verifies the\nGPT-4 model outperforms the other models in both basic and realistic scenarios [5].\n4.3\nThe effects of the ICL\nTo investigate the effects of the ICL, we introduced ICL prompting and different ICL retrieval\nmethods, and the different number of ICL examples in each task. Based on the experiments results of\n7\n12 different variants of each option and evaluating their performance on the validation set, we have\nthe following three observations:\n\u2022 In all tasks, the performance of ICL prompting is better than zero-shot prompting.\n\u2022 In most tasks (in Table 4, 6, 7, 11, 13, 14, 15), using scaffold similarity to retrieve the most similar\nexamples of the question as ICL examples achieves better performance than random sampling.\n\u2022 In most tasks (in Table 4, 6, 7, 10, 11, 14, 15), using larger k (more ICL examples) usually achieves\nbetter performance than small k (fewer ICL examples).\nThese observations indicate that the quality and quantity of ICL examples plays an important role\nin the performance of ICL prompting [23, 36]. This may inspire that it is necessary to design more\nchemistry-specific ICL methods to build high-quality ICL examples to further improve the ICL\nprompting performance.\n4.4\nAre molecule SELFIES representations more suitable for LLMs than SMILES\nrepresentations?\nSELFIES [33] representations are more machine-learning-friendly string representations of molecules.\nTo investigate whether the SELFIES representations are more suitable for LLMs than SMILES\nrepresentations, we conduct experiments on four tasks, including molecule property prediction,\nreaction prediction, molecule design and molecule captioning. The experiment results are shown\nin Table 16, 17, 18, 19. We can observe that the results of using SELFIES in all four tasks are\ninferior to those of using SMILES. This could be attributed to the fact that the pretraining datasets\nfor LLMs are primarily populated with SMILES-related content rather than SELFIES. Consequently,\nthese models are more attuned to SMILES. However, it\u2019s worth mentioning that the occurrence of\ninvalid SELFIES is less frequent than that of invalid SMILES, which aligns with the inherent design\nof SELFIES to ensure molecular validity.\n4.5\nThe impact of temperature parameter of LLMs\nOne key hyperparameter that affects the performance of LLMs is temperature, which influences\nthe randomness in the model\u2019s predictions. To determine the optimal temperature for each task, we\nrandomly sampled 30 data points from the datasets and performed in-context learning experiments\nacross various temperature settings. While optimal temperatures determined on the validation set\nmay not always yield optimal results on the test set, our methodology is primarily designed to\nconserve token usage and API query time. To address potential discrepancies between validation\nand test sets, we performed targeted temperature testing on the test sets for two molecular property\nprediction datasets: BBBP and BACE. Our results are summarized in Table 3. For these tests,\nwe employed the GPT-4 model (using scaffold sampling with k = 8) and set temperature values\nt = [0.2, 0.4, 0.6, 0.8, 1]. The result reveal that variations in the temperature parameter have a\nmarginal impact on test performance, with fluctuations of less than 0.05 observed in both F1 and\naccuracy scores. These results validate the robustness of our initial sampling approach and underscore\nthe reliability of our findings across different settings.\nTable 3: The F1(\u2191) and accuracy(\u2191) score of GPT-4 model(scaffold sampling, k = 8) on different\ntemperature setting.\nF1(\u2191)\nBBBP\nBACE\nGPT-4(t=0.2)\n0.667 \u00b1 0.029\n0.741 \u00b1 0.019\nGPT-4(t=0.4)\n0.712 \u00b1 0.014\n0.728 \u00b1 0.024\nGPT-4(t=0.6)\n0.683 \u00b1 0.016\n0.736 \u00b1 0.020\nGPT-4(t=0.8)\n0.686 \u00b1 0.030\n0.744 \u00b1 0.025\nGPT-4(t=1.0)\n0.684 \u00b1 0.023\n0.756 \u00b1 0.025\nAccuracy(\u2191)\nBBBP\nBACE\nGPT-4(t=0.2)\n0.650 \u00b1 0.028\n0.743 \u00b1 0.019\nGPT-4(t=0.4)\n0.691 \u00b1 0.017\n0.729 \u00b1 0.024\nGPT-4(t=0.6)\n0.659 \u00b1 0.016\n0.736 \u00b1 0.019\nGPT-4(t=0.8)\n0.661 \u00b1 0.032\n0.745 \u00b1 0.025\nGPT-4(t=1.0)\n0.660 \u00b1 0.021\n0.757 \u00b1 0.025\n8\n5\nDiscussion\n5.1\nLimitation of LLMs on understanding molecular SMILES\nA significant limitation of LLMs is their lack of understanding of molecular representations in\nSMILES strings, which in many cases leads to inaccurate or inconsistent results as shown in Section\nA for the translation of different ways to name molecules. SMILES (Simplified Molecular Input Line\nEntry System) [60, 61] is a widely used textual representation for chemical structures. For example,\nthe SMILES string for ethanol, a simple alcohol, is \u201cCCO\u201d. This string represents a molecule with\ntwo carbon atoms (C) connected by a single bond and an oxygen atom (O) connected to the second\ncarbon atom. SMILES strings can serve as both input and output for LLMs, alongside other natural\nlanguage text. However, several issues make it challenging for LLMs to accurately understand and\ninterpret SMILES strings: 1) Hydrogen atoms are not explicitly represented in SMILES strings, as\nthey can be inferred based on the standard bonding rules. LLMs frequently struggle to infer these\nimplicit hydrogen atoms and may even fail at simple tasks like counting the number of atoms in\na molecule [27, 6]. 2) A given molecule can have multiple valid SMILES representations, which\ncan lead to ambiguity if not properly processed or standardized. LLMs may thus fail to consistently\nrecognize and compare molecular structures represented by different SMILES strings. 3) LLMs do\nnot have any inherent understanding of SMILES strings, and treat them as a sequence of characters or\nsubwords. When processing long SMILES strings, LLMs rely on the byte-pair encoding tokenization\ntechnique, which can break the string into smaller pieces or subwords in ways that do not represent\nthe molecular structure and properties of molecules represented by SMILES strings. Because many\ntasks in cheminformatics rely on the accurate representation of a molecule by SMILES strings, the\nnon-competitive performance of GPT models in converting structures into SMILES strings (and vice\nversa) affects downstream tasks such as retrosynthesis, reaction and name prediction. LLMs that\nhave an enhanced ability of handling molecular structures and their specific attributes or coupling to\nexisting tools such as RDKit [35] will be needed.\n5.2\nThe limitations of current evaluation methods\nAlthough in Text-Based Molecule Design and Molecule Captioning tasks, GPT models show compet-\nitive performance compared to the baseline in some metrics (BLEU, Levenshtein, ROUGE, FCD,\netc), we observe that the exact match of GPT models is inferior to the baseline in the Text-Based\nMolecule Design task and the GPT models generate some descriptions which violate chemical facts.\nThis divergence between metrics and real-world scenarios mainly arises because, unlike many natural\nlanguage processing tasks that can be suitably evaluated by sentence-level matching evaluation\nmetrics, chemistry-related tasks necessitate exact matching for SMILES and precise terminology in\ndescriptions. These findings spotlight the limitations of current evaluation metrics and underscore the\nneed for the development of chemistry-specific metrics.\n5.3\nHallucination of LLMs in chemistry\nOur evaluation experiments across various tasks reveal two primary types of hallucinations exhibited\nby LLMs in the domain of chemistry. The first type occurs when the input is given in SMILES format\n(e.g., name prediction); LLMs occasionally struggle with interpreting these SMILES correctly. For\ninstance, they may fail to recognize the number of atoms or certain functional groups within molecules\nduring name prediction tasks. The second type of hallucination arises when the expected output from\nLLMs should be in the form of SMILES (e.g., reaction prediction and retrosynthesis). Here, LLMs\nmay produce molecules that are chemically unreasonable, suggesting a gap in understanding what\nconstitutes valid SMILES. Hallucination issues represent a key challenge with LLMs, particularly in\nthe field of chemistry which necessitates exact matching of SMILES and adherence to strict chemical\nfacts [62]. Current LLMs need further investigation into this problem.\n5.4\nProspects of LLMs for chemistry\nOverall, through an exhaustive set of experiments and analyses, we outline several promising avenues\nfor the application of LLMs in the field of chemistry. While LLMs underperform relative to baselines\nacross a majority of tasks, it\u2019s important to note that LLMs leverage only a few examples to solve\nchemistry problems, whereas baselines are trained on extensive, task-specific datasets and are limited\n9\nto certain tasks. This observation provides valuable insights into the potential of LLMs\u2019 generalized\nintelligence in the domain of chemistry. The employment of advanced prompting techniques such as\nChain-of-thought (CoT) [59], Decomposed Prompting [31] could potentially boost the capacity of\nLLMs to perform complex reasoning. On the other hand, LLMs display a considerable amount of\nhallucinations in chemistry tasks, indicating that current LLMs may not yet possess the necessary\ncapabilities to solve practical chemistry problems effectively. However, with continuous development\nof LLMs and further research into methods to avoid hallucinations, we are optimistic that LLMs can\nsignificantly enhance their problem-solving abilities in the field of chemistry.\n5.5\nImpact of generating harmful chemicals\nOur work demonstrate that LLMs can generate chemically valid molecules. However, it\u2019s crucial to\nacknowledge and mitigate the risks of AI misuse, such as generating hazardous substances. While\nadvancements in AI-enabled chemistry have the potential to bring about groundbreaking medicines\nand sustainable materials, the same technology can be misused to create toxic or illegal substances.\nThis dual-edged potential emphasizes the necessity for stringent oversight. Without careful regulation,\nthese tools could not only pose significant health and safety hazards but also create geopolitical and\nsecurity challenges. Consequently, as we harness the capabilities of LLMs in the field of chemistry,\nwe concur with earlier research on generative models in chemistry [2, 3] that it is vital for developers\nto establish robust safeguards and ethical guidelines to deter harmful applications. This is akin to the\nlimitations imposed on popular search engines, which can also be exploited to find information about\ndangerous chemicals or procedures online.\n5.6\nBroader Impacts\nOur work has broad impacts across multiple dimensions. First, it offers valuable insights and\nrecommendations for both AI researchers and chemists in academia and industry. These perspectives\nenhance the effective utilization of LLMs and guide future advancements in the field. Second, our\nobjective evaluation of LLMs helps alleviate concerns regarding the replacement of chemists by\nAI. This aspect contributes to public education, addressing misconceptions and fostering a better\nunderstanding of the role of AI in chemistry. Furthermore, we provide a comprehensive experimental\nframework for testing LLMs in chemistry tasks, which can also be applicable to other domains.\nThis framework serves as a valuable resource for researchers seeking to evaluate LLMs in diverse\nfields. However, it is important to recognize the ethical and societal implications associated with our\nwork. Additionally, concerns about job displacement in the chemical industry may arise, and efforts\nshould be made to address these challenges and ensure a responsible and equitable adoption of AI\ntechnologies.\n6\nConclusion and Future Work\nIn this paper, we summarize the required abilities of LLMs in chemistry and construct a comprehensive\nbenchmark to evaluate the five most popular LLMs (GPT-4, GPT-3.5, Davinci-003, LLama and\nGalactica) on eight widely-used chemistry tasks. The experiment results show that LLMs perform\nless competitive in generative tasks which require in-depth understanding of molecular SMILES\nstrings, such as reaction prediction, name prediction, and retrosynthesis. LLMs show competitive\nperformance in tasks that are in classification or ranking formats such as yield prediction and reagents\nselection. LLMs are selectively competitive on tasks involving text in prompts such as property\nprediction and text-based molecule design, or explainable tasks such as molecule captioning. These\nexperiments indicate the potential of LLMs in chemistry tasks and the need for further improvement.\nWe will collaborate with more chemists in the C-CAS group, progressively integrating a wider range\nof tasks that are both novel and practical. We hope our work can address the gap between LLMs and\nthe chemistry research field, inspiring future research to explore the potential of LLMs in chemistry.\nAcknowledgments and Disclosure of Funding\nThis work was supported by the National Science Foundation (CHE\u20132202693) through the NSF\nCenter for Computer Assisted Synthesis (C-CAS).\n10\nReferences\n[1] Derek T Ahneman, Jes\u00fas G Estrada, Shishi Lin, Spencer D Dreher, and Abigail G Doyle.\nPredicting reaction performance in c\u2013n cross-coupling using machine learning. Science, 360\n(6385):186\u2013190, 2018.\n[2] Daniil A Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research\ncapabilities of large language models. arXiv preprint arXiv:2304.05332, 2023.\n[3] Andres M Bran, Sam Cox, Andrew D White, and Philippe Schwaller. Chemcrow: Augmenting\nlarge-language models with chemistry tools. arXiv preprint arXiv:2304.05376, 2023.\n[4] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[5] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: Early experiments\nwith gpt-4, 2023.\n[6] Cayque Monteiro Castro Nascimento and Andr\u00e9 Silva Pimentel. Do large language models\nunderstand chemistry? a conversation with chatgpt. Journal of Chemical Information and\nModeling, 63(6):1649\u20131655, 2023.\n[7] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang,\nXiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of large language\nmodels. arXiv preprint arXiv:2307.03109, 2023.\n[8] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Xiangliang Zhang, Dongyan Zhao, and\nRui Yan. Capturing relations between scientific papers: An abstractive model for related work\nsection generation. In Proc. of ACL, 2021.\n[9] Xiuying Chen, Hind Alamro, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang.\nTarget-aware abstractive related work generation with contrastive learning. In Proc. of SIGIR,\n2022.\n[10] Xiuying Chen, Mingzhe Li, Shen Gao, Rui Yan, Xin Gao, and Xiangliang Zhang. Scientific\npaper extractive summarization enhanced by citation graphs. In Proc. of EMNLP, 2022.\n[11] Jonathan Choi, Kristin Hickman, Amy Monahan, and Daniel Schwarcz. Chatgpt goes to law\nschool. Journal of Legal Education, 2023.\n[12] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned\nlanguage models. arXiv preprint arXiv:2210.11416, 2022.\n[13] Connor W Coley, Regina Barzilay, Tommi S Jaakkola, William H Green, and Klavs F Jensen.\nPrediction of organic reaction outcomes using machine learning. ACS central science, 3(5):\n434\u2013443, 2017.\n[14] Debadutta Dash, Rahul Thapa, Juan M Banda, Akshay Swaminathan, Morgan Cheatham,\nMehr Kashyap, Nikesh Kotecha, Jonathan H Chen, Saurabh Gombar, Lance Downing, et al.\nEvaluation of gpt-3.5 and gpt-4 for supporting real-world information needs in healthcare\ndelivery. arXiv preprint arXiv:2304.13714, 2023.\n[15] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, Lei Li, and Zhifang Sui. A survey on in-context learning, 2023.\n[16] Carl Edwards, ChengXiang Zhai, and Heng Ji. Text2Mol: Cross-modal molecule retrieval with\nnatural language queries. In Proceedings of the 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 595\u2013607, Online and Punta Cana, Dominican Republic, Novem-\nber 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.47.\nURL https://aclanthology.org/2021.emnlp-main.47.\n11\n[17] Carl Edwards, Tuan Lai, Kevin Ros, Garrett Honke, and Heng Ji. Translation between molecules\nand natural language. arXiv preprint arXiv:2204.11817, 2022.\n[18] Simon Frieder, Luca Pinchetti, Ryan-Rhys Griffiths, Tommaso Salvatori, Thomas Lukasiewicz,\nPhilipp Christian Petersen, Alexis Chevalier, and Julius Berner. Mathematical capabilities of\nchatgpt. arXiv preprint arXiv:2301.13867, 2023.\n[19] Taicheng Guo, Changsheng Ma, Xiuying Chen, Bozhao Nan, Kehan Guo, Shichao Pei, Nitesh V\nChawla, Olaf Wiest, and Xiangliang Zhang. Modeling non-uniform uncertainty in reaction\nprediction via boosting and dropout. arXiv preprint arXiv:2310.04674, 2023.\n[20] Taicheng Guo, Lu Yu, Basem Shihada, and Xiangliang Zhang. Few-shot news recommen-\ndation via cross-lingual transfer. In Proceedings of the ACM Web Conference 2023, WWW\n\u201923, page 1130\u20131140, New York, NY, USA, 2023. Association for Computing Machinery.\nISBN 9781450394161. doi: 10.1145/3543507.3583383. URL https://doi.org/10.1145/\n3543507.3583383.\n[21] Zhichun Guo, Chuxu Zhang, Wenhao Yu, John Herr, Olaf Wiest, Meng Jiang, and Nitesh V\nChawla. Few-shot graph learning for molecular property prediction. In Proceedings of the Web\nConference 2021, pages 2559\u20132567, 2021.\n[22] Zhichun Guo, Bozhao Nan, Yijun Tian, Olaf Wiest, Chuxu Zhang, and Nitesh V Chawla.\nGraph-based molecular representation learning. arXiv preprint arXiv:2207.04869, 2022.\n[23] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting:\nScaling in-context learning to 1,000 examples, 2022.\n[24] Dan Hendrycks, Collin Burns, Steven Basart, Andrew Critch, Jerry Li, Dawn Song, and Jacob\nSteinhardt. Aligning ai with shared human values. Proceedings of the International Conference\non Learning Representations (ICLR), 2021.\n[25] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2021.\n[26] Ross Irwin, Spyridon Dimitriadis, Jiazhen He, and Esben Jannik Bjerrum. Chemformer: a pre-\ntrained transformer for computational chemistry. Machine Learning: Science and Technology,\n3(1):015022, 2022.\n[27] Kevin Jablonka, Philippe Schwaller, Andr\u00e9s Ortega-Guerrero, and Berend Smit. Is gpt-3 all\nyou need for low-data discovery in chemistry. 10.26434/chemrxiv-2023-fw8n4, 2023.\n[28] Kevin Maik Jablonka, Qianxiang Ai, Alexander Al-Feghali, Shruti Badhwar, Joshua D Bran,\nStefan Bringuier, L Catherine Brinson, Kamal Choudhary, Defne Circi, Sam Cox, et al. 14\nexamples of how llms can transform materials science and chemistry: A reflection on a large\nlanguage model hackathon. arXiv preprint arXiv:2306.06283, 2023.\n[29] Wengong Jin, Connor W. Coley, Regina Barzilay, and Tommi Jaakkola. Predicting organic\nreaction outcomes with weisfeiler-lehman network, 2017.\n[30] Rehan Ahmed Khan, Masood Jawaid, Aymen Rehan Khan, and Madiha Sajjad. Chatgpt-\nreshaping medical education and clinical management. Pakistan Journal of Medical Sciences,\n39(2):605, 2023.\n[31] Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and\nAshish Sabharwal. Decomposed prompting: A modular approach for solving complex tasks.\narXiv preprint arXiv:2210.02406, 2022.\n[32] Sunghwan Kim, Jie Chen, Tiejun Cheng, Asta Gindulyte, Jia He, Siqian He, Qingliang Li,\nBenjamin A Shoemaker, Paul A Thiessen, Bo Yu, et al. Pubchem 2019 update: improved access\nto chemical data. Nucleic acids research, 47(D1):D1102\u2013D1109, 2019.\n12\n[33] Mario Krenn, Florian H\u00e4se, AkshatKumar Nigam, Pascal Friederich, and Alan Aspuru-Guzik.\nSelf-referencing embedded strings (SELFIES): A 100% robust molecular string representation.\nMachine Learning: Science and Technology, 1(4):045024, oct 2020. doi: 10.1088/2632-2153/\naba947. URL https://doi.org/10.1088%2F2632-2153%2Faba947.\n[34] Youngchun Kwon, Dongseon Lee, Youn-Suk Choi, and Seokho Kang. Uncertainty-aware\nprediction of chemical reaction yields with graph neural networks. Journal of Cheminformatics,\n14:1\u201310, 2022.\n[35] G. A. Landrum. Rdkit: Open-source cheminformatics software. http://www.rdkit.org, 2020.\n[36] Itay Levy, Ben Bogin, and Jonathan Berant.\nDiverse demonstrations improve in-context\ncompositional generalization, 2022.\n[37] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by\nchatgpt really correct? rigorous evaluation of large language models for code generation. arXiv\npreprint arXiv:2305.01210, 2023.\n[38] Zequn Liu, Wei Zhang, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Ming Zhang, and Tie-\nYan Liu. Molxpt: Wrapping molecules with text for generative pre-training. arXiv preprint\narXiv:2305.10688, 2023.\n[39] Daniel Mark Lowe. Extraction of chemical structures and reactions from the literature. PhD\nthesis, University of Cambridge, 2012.\n[40] Frederic P Miller, Agnes F Vandome, and John McBrewster. Levenshtein distance: Information\ntheory, computer science, string (computer science), string metric, damerau? levenshtein\ndistance, spell checker, hamming distance, 2009.\n[41] Harry L Morgan. The generation of a unique machine description for chemical structures-a\ntechnique developed at chemical abstracts service. Journal of chemical documentation, 5(2):\n107\u2013113, 1965.\n[42] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capa-\nbilities of gpt-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023.\n[43] OpenAI. Gpt-4 technical report, 2023.\n[44] Damith Perera, Joseph W Tucker, Shalini Brahmbhatt, Christopher J Helal, Ashley Chong,\nWilliam Farrell, Paul Richardson, and Neal W Sach. A platform for automated nanomole-scale\nreaction screening and micromole-scale synthesis in flow. Science, 359(6374):429\u2013434, 2018.\n[45] Kristina Preuer, Philipp Renz, Thomas Unterthiner, Sepp Hochreiter, and Gunter Klambauer.\nFr\u00e9chet chemnet distance: a metric for generative models for molecules in drug discovery.\nJournal of chemical information and modeling, 58(9):1736\u20131741, 2018.\n[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\n[47] Kohulan Rajan, Achim Zielesny, and Christoph Steinbeck. Stout: Smiles to iupac names using\nneural machine translation. Journal of Cheminformatics, 13(1):1\u201314, 2021.\n[48] Mayk Caldas Ramos, Shane S Michtavy, Marc D Porosoff, and Andrew D White. Bayesian\noptimization of catalysts with in-context learning. arXiv preprint arXiv:2304.05341, 2023.\n[49] David Ratcliff, John W.; Metzener. Pattern matching: The gestalt approach, 1988.\n[50] Brandon J Reizman, Yi-Ming Wang, Stephen L Buchwald, and Klavs F Jensen. Suzuki\u2013miyaura\ncross-coupling optimization enabled by automated feedback. Reaction chemistry & engineering,\n1(6):658\u2013666, 2016.\n[51] Mandana Saebi, Bozhao Nan, John E Herr, Jessica Wahlers, Zhichun Guo, Andrzej M Zura\u00b4nski,\nThierry Kogej, Per-Ola Norrby, Abigail G Doyle, Nitesh V Chawla, et al. On the use of\nreal-world datasets for reaction yield prediction. Chemical Science, 2023.\n13\n[52] Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai,\nAntoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. Multitask prompted training\nenables zero-shot task generalization. arXiv preprint arXiv:2110.08207, 2021.\n[53] Nadine Schneider, Nikolaus Stiefl, and Gregory A Landrum. What\u2019s what: The (nearly)\ndefinitive guide to reaction role assignment. Journal of chemical information and modeling, 56\n(12):2336\u20132346, 2016.\n[54] Philippe Schwaller, Teodoro Laino, Th\u00e9ophile Gaudin, Peter Bolgar, Christopher A Hunter,\nCostas Bekas, and Alpha A Lee. Molecular transformer: a model for uncertainty-calibrated\nchemical reaction prediction. ACS central science, 5(9):1572\u20131583, 2019.\n[55] Taffee T Tanimoto. Elementary mathematical theory of classification and prediction. Journal of\nBiomedical Science and Engineering, 1958.\n[56] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis\nSaravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. Galactica: A large language\nmodel for science. arXiv preprint arXiv:2211.09085, 2022.\n[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[58] Hongwei Wang, Weijiang Li, Xiaomeng Jin, Kyunghyun Cho, Heng Ji, Jiawei Han, and\nMartin D Burke. Chemical-reaction-aware molecule representation learning. arXiv preprint\narXiv:2109.09888, 2021.\n[59] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\n[60] David Weininger. Smiles, a chemical language and information system. 1. introduction to\nmethodology and encoding rules. J. Chem. Inf. Comput. Sci., 28:31\u201336, 1988.\n[61] David Weininger, Arthur Weininger, and Joseph L. Weininger. Smiles. 2. algorithm for genera-\ntion of unique smiles notation. J. Chem. Inf. Comput. Sci., 29:97\u2013101, 1989.\n[62] A.D. White. The future of chemistry is language., 2023.\n[63] Andrew D. White, Glen M. Hocky, Heta A. Gandhi, Mehrad Ansari, Sam Cox, Geemi P.\nWellawatte, Subarna Sasmal, Ziyue Yang, Kangxin Liu, Yuvraj Singh, and Willmor J. Pe\u00f1a Ccoa.\nAssessment of chemistry knowledge in large language models that generate code. Digital\nDiscovery, 2:368\u2013376, 2023. doi: 10.1039/D2DD00087C. URL http://dx.doi.org/10.\n1039/D2DD00087C.\n[64] Genta Indra Winata, Samuel Cahyawijaya, Zihan Liu, Zhaojiang Lin, Andrea Madotto,\nand Pascale Fung. Are multilingual models effective in code-switching?\narXiv preprint\narXiv:2103.13309, 2021.\n[65] Zhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S\nPappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine\nlearning. Chemical science, 9(2):513\u2013530, 2018.\n[66] Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied,\nWeizhu Chen, and Nan Duan. Agieval: A human-centric benchmark for evaluating foundation\nmodels. arXiv preprint arXiv:2304.06364, 2023.\n14\nAppendix\nA\nName Prediction\nFor one molecule, there are different chemical naming conventions and representations such as\nSMILES, IUPAC names, and graphic molecular formula. To investigate whether GPT models have\nthe basic chemical name understanding ability, we construct four chemical name prediction tasks that\ninclude SMILES to IUPAC name translation (smiles2iupac), IUPAC name to SMILES translation\n(iupac2smiles), SMILES to molecule formula translation (smiles2formula), and IUPAC name to\nmolecule formula translation (iupac2formula). We collect 630 molecules and their corresponding\nnames including SMILES, IUPAC name, and molecule formula from PubChem3 [32]. We randomly\nsample 500 molecules as the ICL candidates, and other 30 molecules as the validation set, and other\n100 molecules as the test set. For all name translation tasks, we use the exact match accuracy as the\nmetric to evaluate the performance.\nICL Prompt.\nOne example of the smiles2iupac prediction is shown in Figure 5. For other name\ntranslation tasks, we only change the underlined parts that represent different tasks and their corre-\nsponding input names and output names.\nFigure 5: An ICL prompt example for smiles2iupac prediction\nResults.\nThe results are reported in Table 4 (we only report representative methods along with\ntheir optimal prompt settings via grid search on validation set). In all four name prediction tasks,\nthe accuracy of the best method is extremely low (0.014 in the iupac2smiles task, 0.086 in the\nsmiles2formula task, 0.118 in the iupac2formula task) or even 0 (in the smiles2iupac task). This\nindicates the LLMs lack basic chemical name understanding ability. The accuracy of Davinci-003 is\nconsiderably inferior to other models.\nCase studies.\nExample results generated by GPT-4 (Scaffold, k=20) method for each task is shown\nin Table 5. In all tasks, the GPT-4 model gives the wrong answers. In the smiles2formula task, we can\nobserve that GPT models cannot even recognize the number of Carbon and infer the correct number\nof Hydrogen, demonstrating the bad chemical understanding ability of GPT models. For prospects,\nsome pre-training technologies such as wrapping molecules with text [38] or code-switch [64, 20]\nmay be helpful to align different chemical names of the same molecule to help improve LLMs\u2019\nchemical understanding.\n3https://pubchem.ncbi.nlm.nih.gov\n15\nTable 4: The accuracy (\u2191) of LLMs in 4 different name prediction tasks. The best LLM is in bold\nfont. Here k is the number of examples used in few-shot ICL. The baseline is underlined and \"-\"\nindicates that STOUT cannot solve the smiles2formula and iupac2formula tasks.\nMethod\nsmiles2iupac\niupac2smiles\nsmiles2formula\niupac2formula\nSTOUT [47]\n0.55\n0.7\n-\n-\nGPT-4 (zero-shot)\n0\n0.008\u00b10.008\n0.048\u00b1 0.022\n0.092\u00b10.018\nGPT-4 (Scaffold, k=5)\n0\n0.014\u00b10.009\n0.058\u00b10.015\n0.118\u00b10.022\nGPT-4 (Scaffold, k=20)\n0\n0.012\u00b10.004\n0.086\u00b10.036\n0.084\u00b10.005\nGPT-4 (Random, k=20)\n0\n0.010\u00b10.007\n0.070\u00b10.032\n0.076\u00b10.011\nGPT-3.5 (Scaffold, k=20)\n0\n0.010\u00b10.000\n0.052\u00b10.004\n0.044\u00b10.009\nDavinci-003 (Scaffold, k=20)\n0\n0\n0.006\u00b10.005\n0.018\u00b10.004\nLlama2-13B-chat (Scaffold, k=20)\n0\n0\n0.010\u00b10.007\n0\nGAL-30B (Scaffold, k=10)\n0\n0\n0\n0\nTable 5: Example results generated by GPT-4 (Scaffold, k=20) method for different tasks\nTask\nInput\nGround Truth\nOutput of GPT-4 (Scaffold, k=20)\nsmiles2iupac\nCCOC(=O)C(C(C)=O)=C(C)N\nethyl 2-acetyl-3-aminobut-2-enoate\nethyl 2-methyl-5-oxo-2-azahept-4-en-3-oate\niupac2smiles\nethyl 2-acetyl-3-aminobut-2-enoate\nCCOC(=O)C(C(C)=O)=C(C)N\nCCOC(=O)C=C(C)C(=N)C\nsmiles2formula\nCc1noc(CCn2cc[nH]c2=O)n1\nC8H10N4O2\nC9H10N4O2\niupac2formula\nR)-(1-benzylquinolin-1-ium-4-yl)\n-(5-ethenyl-1-azabicyclo[2.2.2]octan-2-yl)methanol;chloride\nC26H29ClN2O\nC23H27ClN2O\nB\nMolecule Property Prediction\nMolecule property prediction [21, 58] is a fundamental task in computational chemistry that has been\ngaining significant attention in recent years due to its potential for drug discovery, material science,\nand other areas in the chemistry. The task involves using machine learning techniques [22] to predict\nthe chemical and physical properties of a given molecule, based on its molecular structure. We aim to\nfurther explore the potential of LLMs in molecular property prediction and assess their performance\non a set of benchmark datasets, such as BBBP(MIT license), HIV(MIT license), BACE(MIT license),\nTox21(MIT license), and ClinTox(MIT license), which were originally introduced by [65]. The\ndatasets are made up of extensive collections of SMILES, paired with binary labels that highlight the\nparticular property being evaluated, such as BBBP: Blood-Brain Barrier Penetration, HIV: inhibit\nHIV replication, BACE: bindings results for a set of inhibitors of human beta-secretase, Tox21:\ntoxicity of compounds, and ClinTox: drugs failed clinical trials for toxicity reasons. A comprehensive\nexplanation of these datasets can be referenced in the original research conducted by [65]. For\nICL, we either select k samples randomly, or search the top-k most analogous molecules using\nRDKit [35] to determine the Tanimoto Similarity. However, it is crucial to mention that using the\nlatter method does not assure an even distribution among classes. In our study, we employ a strategic\nsampling method for two categories of datasets: balanced and highly imbalanced. For balanced\ndatasets, such as BBBP and BACE, we randomly select 30 samples for the validation process and\n100 samples for testing from the original dataset. Contrastingly, for datasets exhibiting substantial\nlabel imbalance (39684:1443 \u2248 28:1, take HIV datasets as a example), we select samples from the\nmajority and minority classes to achieve a ratio of 4:1. This strategic approach enables us to maintain\na representative sample for the evaluation process, despite the original high imbalance in the dataset.\nTo evaluate the results, we use the classification accuracy, as well as F1 score as the evaluation\nmetric due to the class imbalance. We benchmark our method against two established baselines from\nMoleculeNet [65]: RF and XGBoost. Both baselines utilize the 1024-bit circular fingerprint as input\nto predict the property as a binary classification problem.\nICL Prompt.\nFigure 6 illustrates a sample of our ICL prompt for property prediction. Within the\ntask-specific template, we include a detailed explanation of the task forecasting the penetration of\nthe brain-blood barrier to assist LLMs in comprehending the input SMILES from the BBBP dataset.\nAdditionally, we establish certain constraints for the output to conform to the specific characteristics\nof the property prediction task.\nResults.\nThe results are reported as F1 in Table 6, accuracy in Table 7. We observed that GPT\nmodels outperform the baseline model in terms of F1 on four out of five datasets. In the range of GPT\n16\nFigure 6: An ICL prompt example for property prediction\nTable 6: F1 (\u2191) score of LLMs and baseline in molecular property prediction tasks. k is the number of\nexamples used in few-shot ICL. The best GPT model is in bold font, and the baseline is underlined.\nBBBP\nBACE\nHIV\nTox21\nClinTox\nRF\n0.881\n0.758\n0.518\n0.260\n0.461\nXGBoost\n0.897\n0.765\n0.551\n0.333\n0.620\nGPT-4 (zero-shot)\n0.560 \u00b10.034\n0.322\u00b10.018\n0.977\u00b10.013\n0.489\u00b10.018\n0.555\u00b10.043\nGPT-4 (Scaffold, k= 4)\n0.498 \u00b1 0.028\n0.516 \u00b1 0.024\n0.818 \u00b1 0.015\n0.444 \u00b1 0.004\n0.731 \u00b1 0.035\nGPT-4 (Scaffold, k= 8)\n0.587\u00b10.018\n0.666\u00b10.023\n0.797 \u00b1 0.021\n0.563\u00b10.008\n0.736\u00b10.033\nGPT-4 (random, k= 8)\n0.469 \u00b1 0.025\n0.504 \u00b1 0.020\n0.994 \u00b1 0.006\n0.528\u00b10.003\n0.924\u00b10.000\nGPT-3.5 (Scaffold, k= 8)\n0.463 \u00b1 0.008\n0.406 \u00b1 0.011\n0.807 \u00b1 0.021\n0.529 \u00b1 0.021\n0.369 \u00b1 0.029\nDavinci-003 (Scaffold, k= 8)\n0.378 \u00b1 0.024\n0.649 \u00b1 0.021\n0.832 \u00b1 0.020\n0.518\u00b10.009\n0.850 \u00b1 0.020\nLlama2-13B-chat (Scaffold, k= 8)\n0.002 \u00b1 0.001\n0.045 \u00b1 0.015\n0.069 \u00b1 0.033\n0.047 \u00b1 0.013\n0.001 \u00b1 0.003\nGAL-30B (Scaffold, k= 8)\n0.074 \u00b1 0.019\n0.025 \u00b1 0.013\n0.014 \u00b1 0.016\n0.077 \u00b1 0.046\n0.081 \u00b1 0.015\nTable 7: Accuracy (\u2191) of LLMs and baseline in molecular property prediction tasks. k is the number\nof examples used in few-shot ICL. The best GPT model is in bold font, and the baseline is underlined.\nBBBP\nBACE\nHIV\nTox21\nClinTox\nRF\n0.820\n0.790\n0.870\n0.830\n0.858\nXGBoost\n0.850\n0.810\n0.870\n0.840\n0.888\nGPT-4 (zero-shot)\n0.476 \u00b1 0.036\n0.499 \u00b1 0.005\n0.986 \u00b1 0.007\n0.518 \u00b1 0.018\n0.736 \u00b1 0.027\nGPT-4 (Scaffold, k= 4)\n0.516 \u00b1 0.022\n0.514 \u00b1 0.205\n0.834 \u00b1 0.014\n0.457 \u00b1 0.004\n0.856 \u00b1 0.014\nGPT-4 (Scaffold, k= 8)\n0.614\u00b10.016\n0.679\u00b10.205\n0.836 \u00b1 0.020\n0.737 \u00b1 0.004\n0.856 \u00b1 0.014\nGPT-4 (random, k= 8)\n0.610 \u00b1 0.021\n0.588 \u00b1 0.023\n0.996\u00b10.004\n0.874\u00b10.003\n0.930\u00b10.010\nGPT-3.5 (Scaffold, k= 8)\n0.463 \u00b1 0.007\n0.496 \u00b1 0.016\n0.864 \u00b1 0.018\n0.572 \u00b1 0.026\n0.578 \u00b1 0.029\nDavinci-003 (Scaffold, k= 8)\n0.396 \u00b1 0.023\n0.650 \u00b1 0.021\n0.781 \u00b1 0.004\n0.682 \u00b1 0.006\n0.845 \u00b1 0.010\nLlama2-13B-chat (Scaffold, k= 8)\n0.002 \u00b1 0.003\n0.048 \u00b1 0.017\n0.048 \u00b1 0.025\n0.053 \u00b1 0.011\n0.002 \u00b1 0.004\nGAL-30B (Scaffold, k= 8)\n0.062 \u00b1 0.007\n0.020 \u00b1 0.010\n0.012 \u00b1 0.009\n0.030 \u00b1 0.018\n0.099 \u00b1 0.007\nmodels examined, GPT-4 surpasses both Davinci-003 and GPT-3.5 in predicting molecular properties.\nIn our investigation, we have found evidence to support that the expansion of in-context learning\n(ICL) instances leads to a measurable enhancement in model performance. This underlines a direct\nrelationship between the extent of ICL data and the predictive precision of our models. Concurrently,\nour research presents empirical evidence that scaffold sampling exceeds the performance of random\n17\nsampling on three distinct datasets (BBBP, BACE, Tox21). A plausible explanation for this could be\nthe structural resemblances between the scaffold-sampled molecules and the query molecule, which\npotentially biases the GPT models towards more accurate decision.\nLabel interpretation.\nThe results presented in Table 6 and Table 7 indicate that the GPT-4 model\nselectively outperforms the baseline models on the HIV and ClinTox datasets. This superior perfor-\nmance likely stems from the inclusion of information directly related to the labels within the ICL\nprompts. Specifically, in the HIV dataset, the activity test results play a crucial role. Molecules tend\nto inhibit HIV replication when the activity test is categorized as \"confirmed active\" or \"confirmed\nmoderately active.\" For the ClinTox dataset, the FDA-approval status of a molecule acts as a predictor\nof its clinical toxicity. A molecule not having FDA approval is more likely to be clinically toxic. In\nexperiments where we excluded this contextual information from the in-context learning prompts,\nthe F1 and accuracy score of predictions notably declined, as evident from the results in Table 8 and\nTable 9.\nTable 8: Impact to F1 score of removing label context information from the in-context learning\nprompts.\nF1(\u2191)\nHIV\nClinTox\nGPT-4(zero-shot)\n0.977 \u00b1 (0.013)\n0.489 \u00b1 (0.018)\nGPT-4(unlabelled, zero-shot)\n0.554 \u00b1 (0.017)\n0.438 \u00b1 (0.045)\nGPT-4(few-shot)\n0.797 \u00b1 (0.021)\n0.563 \u00b1 (0.008)\nGPT-4(unlabelled, few-shot)\n0.493 \u00b1 (0.030)\n0.478 \u00b1 (0.035)\nTable 9: Impact to accuracy of removing label context information from the in-context learning\nprompts.\nAccuracy(\u2191)\nHIV\nClinTox\nGPT-4(zero-shot)\n0.986 \u00b1 (0.070)\n0.736 \u00b1 (0.027)\nGPT-4(unlabelled, zero-shot)\n0.628 \u00b1 (0.016)\n0.602 \u00b1 (0.039)\nGPT-4(few-shot)\n0.836 \u00b1 (0.020)\n0.856 \u00b1 (0.014)\nGPT-4(unlabelled, few-shot)\n0.541 \u00b1 (0.032)\n0.630 \u00b1 (0.014)\nC\nYield Prediction\nYield prediction [51] is a critical task in chemistry, specifically in the domain of synthetic chem-\nistry, which involves the design and synthesis of new compounds for various applications, such as\npharmaceuticals, materials, and catalysts. The yield prediction task aims to estimate the efficiency\nand effectiveness of a chemical reaction, primarily by quantifying the percentage of the desired\nproduct formed from the reactants. We use two High-Throughput experimentation (HTE) datasets:\nBuchwald-Hartwig [1] (MIT license) and Suzuki-Miyaura dataset [50] (MIT license) for evaluation.\nThese datasets consist of reactions and their corresponding yields, which have been meticulously\nacquired through standardized and consistent experimental setups. This uniformity ensures that the\ndata within each dataset is coherent, reducing the likelihood of discrepancies arising from variations\nin experimental procedures or conditions. We formulate the task of yield prediction as a binary\nclassification problem, by determining whether a reaction is a high-yielding reaction or not. We\nused only random sampling for our ICL examples as reactions in those datasets belong to the same\ntype. For every dataset, we randomly select 30 samples for the validation process and 100 samples\nfor testing from the original dataset. To evaluate the results, we use the classification accuracy\nas the evaluation metric, with UAGNN [34] serving as baseline. UAGNN reports state-of-the-art\nperformance on yield prediction. It takes the graphs of reactants and products as input, and learns\nrepresentation of these molecules through a graph neural network, and then predicts the scaled yield .\nICL prompt.\nWe show our ICL prompt for yield prediction with an example from Buchwald-\nHartwig dataset. As described in Figure 7, we incorporate an input explanation (wherein the reactants\nare separated by \u2018.\u2019 and the products are split by \u2018>>\u2019) to assist large language models. Additionally,\noutput restrictions are enforced to ensure the generation of valid results.\n18\nFigure 7: An ICL prompt example for yield prediction\nResults.\nThe results are presented in Table 10. Our analysis reveals that in the task of yield\nprediction, GPT models perform below the established baseline model, UAGNN. However, it\u2019s\nworth noting that the UAGNN model was trained on the full training dataset including thousands of\nexamples. Considering the spectrum of GPT models under scrutiny, GPT-4 emerges as the superior\nmodel, overshadowing both Davinci-003 and GPT-3.5 in predicting reaction yields. In the process\nof our investigation, we unearthed supporting evidence that signifies the role of ICL instances in\nthe enhancement of model performance. This suggests an inherent correlation between the quantity\nof ICL data and the predictive accuracy of the models under consideration. This phenomenon is\nparticularly in the case of GPT-4, we observed a significant improvement in performance when the\nnumber of ICL examples was increased from 4 to 8, both in the Buchwald-Hartwig and Suzuki-\ncoupling reactions. This indicates that even within the same model architecture, the amount of\ncontextual data can significantly influence the predictive capabilities.\nTable 10: Accuracy (\u2191) of yield prediction task. k is the number of examples used in few-shot ICL.\nThe best LLM is in bold font, and the baseline is underlined.\nBuchwald-Hartwig\nSuzuki-coupling\nUAGNN [34]\n0.965\n0.957\nGPT-4 (zero-shot)\n0.322 \u00b1 0.034\n0.214 \u00b1 0.019\nGPT-4 (random, k= 8)\n0.800\u00b10.008\n0.764\u00b10.013\nGPT-4 (random, k= 4)\n0.574 \u00b1 0.045\n0.324 \u00b1 0.018\nGPT-3.5 (random, k= 8)\n0.585 \u00b1 0.045\n0.542 \u00b1 0.011\nDavinci-003 (random, k= 8)\n0.467 \u00b1 0.013\n0.341 \u00b1 0.017\nLlama2-13B-chat\n0.008 \u00b1 0.007\n0.006 \u00b1 0.004\nGAL-30B\n0\n0.008 \u00b1 0.010\n19\nD\nReaction Prediction\nReaction prediction is a central task in the field of chemistry, with significant implications for\ndrug discovery, materials science, and the development of novel synthetic routes. Given a set of\nreactants, the goal of this task is to predict the most likely products formed during a chemical\nreaction [54, 13, 19]. In this task, we use the widely adopted USPTO-MIT dataset [29](MIT license)\nto evaluate the performance of GPT models. This dataset contains approximately 470,000 chemical\nreactions extracted from US patents. In the experiment, we used the USPTO mixed data set, where\nthe reactants and reagents strings are split by \u2018.\u2019. We randomly sampled 30 samples from the original\nvalidation set for validation and 100 samples from the original test set for testing. We use the\nTop-1 Accuracy as the evaluation metric and Chemformer [26] as the baseline due to its superior\nperformance among the machine learning solutions for reaction prediction. Chemformer is a seq2seq\nmodel trained to predict the output product when given reactants and reagents as input. We also\nreport the percentage of invalid SMILES generated by each method.\nFigure 8: An ICL prompt example for reaction prediction\nICL Prompt.\nOne example of our ICL prompt for reaction prediction is shown in Figure 8. Given\nthe nature of the reaction prediction task and the characteristics of the USPTO-MIT dataset, we\nenhance the task-specific template with an input explanation (stating that the input includes reactants\nand reagents, which are separated by \u2018.\u2019) to assist the GPT models in understanding the input SMILES.\nMoreover, we incorporate output restrictions to guide GPT models in generating chemically valid\nand reasonable products.\nTable 11: The performance of LLMs and baseline in the reaction prediction task. k is the number of\nexamples used in few-shot ICL. The best LLM is in bold font, and the baseline is underlined.\nMethod\nTop-1 Accuracy (\u2191)\nInvalid SMILES (\u2193)\nChemformer [26]\n0.938\n0%\nGPT-4 (zero-shot)\n0.004 \u00b1 0.005\n17.4% \u00b1 3.9%\nGPT-4 (Scaffold, k=20)\n0.230 \u00b1 0.022\n7.0% \u00b1 1.6%\nGPT-4 (Random, k=20)\n0.012 \u00b1 0.008\n8.4% \u00b1 1.5%\nGPT-4 (Scaffold, k=5)\n0.182 \u00b1 0.015\n6.6% \u00b1 1.5%\nGPT-3.5 (Scaffold, k=20)\n0.184 \u00b1 0.005\n15.6% \u00b1 2.3%\nDavinci-003 (Scaffold, k=20)\n0.218 \u00b1 0.008\n11.4% \u00b1 2.7%\nLlama2-13B-chat (Scaffold, k=20)\n0.032 \u00b1 0.013\n27.8% \u00b1 5.5%\nGAL-30B (Scaffold, k=5)\n0.036 \u00b1 0.011\n5.2% \u00b1 1.5%\n20\nResults.\nThe results are reported in Table 11. We can observe that compared to the baseline, the\nperformance of GPT models is considerably inferior, especially for the Zero-shot prompting (Top-1\nAccuracy is only 0.004 and it generates 17.4% invalid SMILES). The less competitive results of GPT\nmodels can be attributed to the lack of in-depth understanding of the SMILES strings that represent\nreactants and products, as well as the reaction process that transforms reactants into products. It is\nalso worth mentioning that the high accuracy achieved by Chemformer is due to its training on the\ncomplete dataset. More conclusions and detailed analysis are summarized in the section 5.\nE\nReagents Selection\nReagents selection, also known as reagent recommendation, involves the identification and proposal\nof the most fitting reagents for a specific chemical reaction or process. Compared to other prediction\nand generation tasks, these selection tasks might be more fitting for LLMs and carry extensive\nimplications. Reagent recommendation can markedly enhance reaction design by pinpointing optimal\nreagents and conditions for a given reaction, thereby augmenting efficiency and effectiveness in\nboth academic and industrial settings. Drawing from a vast corpus of chemical knowledge, GPT\nmodels may be able to generate suggestions, leading to chemical reactions with a greater likelihood\nof yielding superior results.\nIn this study, we formulate four reaction component selection task from the Suzuki High-Throughput\nExperimentation (HTE) dataset. The dataset, created by Perera et al[44](MIT license), evaluates the\nSuzuki coupling of 5 electrophiles and 7 nucleophiles across a matrix of 11 ligands (with one blank),\n7 bases (with one blank), and 4 solvents, resulting in a reaction screening dataset comprising 5,760\ndata points. The task of reagents selection can be divided into three categories: Reactant selection,\nLigand Selection and Solvent selection. For validation, 30 examples were randomly sampled, while\n100 examples were used for testing, all taken from the original datasets. Top-1 Accuracy serves as\nthe assessment metric for both reactant and solvent selection, while Top-50% is utilized for ligand\nselection, as the upper half of the ligands in the list typically provide satisfactory yields in chemical\nreactions. This task is newly emergent in the field of chemistry, and as such, there are no established\nbaselines yet.\nICL prompt. One example of our ICL prompt for reagents selection is shown in Figure 9. Con-\nsidering the structure of the dataset and the characteristics of the reagents, we provide detailed task\ndescription and an answer template to guide GPT models towards the desired output.\nResults.\nOur results are presented in Table 12. From the table, it is evident that GPT-4 and GPT-3.5\nperform comparatively well in reagent selection tasks. This suggests a promising potential for GPT\nmodels in the realm of reagent selection.\nTable 12: Accuracy (\u2191) of LLM in the reagent selection tasks. For Reactant Selection and Solvent\nselection task, we report the mean (and standard deviation) of the Top-1 Accuracy score and we\nreport the Top-50% accuracy score for the Ligand Selection task. The best LLM is in bold font, and\nthe baseline is underlined.\nReactant Selection\nSolvent Selection\nLigand Selection\nGPT-4 (zero-shot)\n0.299 \u00b10.029\n0.526\u00b10.012\n0.534\u00b10.059\nGPT-3.5 (zero-shot)\n0.400\u00b10.038\n0.368\u00b10.034\n0.436 \u00b1 0.020\nDavinci-003 (zero-shot)\n0.178 \u00b1 0.034\n0.463 \u00b1 0.014\n0.432 \u00b1 0.020\nLlama2-13B-chat (zero-shot)\n0.145 \u00b1 0.000\n0.050 \u00b1 0.010\n0.284 \u00b1 0.024\nGAL-30B (zero-shot)\n0.107 \u00b1 0.020\n0.104 \u00b1 0.004\n0.030 \u00b1 0.016\nF\nRetrosynthesis\nRetrosynthesis planning is a crucial task in synthetic organic chemistry that involves identifying\nefficient synthetic pathways for a target molecule by recursively transforming it into simpler precursor\nmolecules. In contrast to reaction prediction, retrosynthesis planning involves a reverse extrapolation\nfrom the target molecule to identify the readily available reactants for its synthesis. In this study, we\nuse the USPTO-50k dataset [53](MIT license), which contains 50,037 chemical reactions. In our\n21\nFigure 9: An ICL prompt example for reagents selection\nexperiment, we use the data splitting as [17] and we the training set which contains 40,029 reactions\nas the ICL candidates. The metric and baseline are the same as the reaction prediction.\nICL Prompt.\nOne example of our ICL prompt for reaction prediction is shown in Figure 10. As\ndiscussed in the reaction prediction task, we also add the task-specific template to help GPT models\nunderstand the input and restrict the output.\nFigure 10: An ICL prompt example for Retrosynthesis\n22\nTable 13: The performance of LLMs and baseline in Retrosynthesis task. The best LLM is in bold\nfont, and the baseline is underlined.\nMethod\nTop-1 Accuracy (\u2191)\nInvalid SMILES (\u2193)\nChemformer [26]\n0.536\n0%\nGPT-4 (zero-shot)\n0.006 \u00b1 0.005\n20.6% \u00b1 4.7%\nGPT-4 (Scaffold, k=20)\n0.096 \u00b1 0.013\n10.4% \u00b1 3.4%\nGPT-4 (Scaffold, k=5)\n0.114 \u00b1 0.013\n11.0% \u00b1 1.2%\nGPT-4 (Random, k=20)\n0.012 \u00b1 0.011\n18.2% \u00b1 4.2%\nGPT-3.5 (Scaffold, k=20)\n0.022 \u00b1 0.004\n6.4% \u00b1 1.3%\nDavinci-003 (Scaffold, k=20)\n0.122 \u00b1 0.013\n6.0% \u00b1 1.2%\nLlama2-13B-chat (Scaffold, k=20)\n0\n27.2% \u00b1 1.5%\nGAL-30B (Scaffold, k=5)\n0.016 \u00b1 0.005\n5.2% \u00b1 1.8%\nResults.\nThe results are reported in Table 13. The performance of GPT models is also inferior\nthan the baseline due to the lack of an in-depth understanding of the SMILES strings that represent\nreactants and products. Detailed analysis are summarized in the later section 5 Discussion.\nG\nText-Based Molecule Design\nText-Based Molecule Design is a novel task in computational chemistry and drug discovery. It\ninvolves generating new molecules with desired molecule descriptions. In our experiment, we employ\nthe ChEBI-20 dataset which consists of 33,010 molecule-description pairs. The dataset is split\ninto 80/10/10% as the training/validation/test set [17](CC BY 4.0). We use the training set which\ncontains 26407 molecule-description pairs as the ICL candidates. For comparison, we use the MolT5-\nLarge [17] as the baseline. MolT5-Large is the initial effort to investigate the translation between\nmolecules and text, including tasks such as text-based molecule design and molecule captioning. It\nbuilds upon T5 [46], an encoder-decoder Transformer model, and benefits from pretraining on a large\namount of dataset. To comprehensively evaluate the performance, we employ three different types of\nmetrics. The first type of metric is the chemical similarity between the ground-truth molecules and\ngenerated molecules, measured by FTS (fingerprint Tanimoto Similarity) [55] in terms of MACCS\n[49], RDK [35], and Morgan [14]. Secondly, we also use FCD (Fr\u00e9chet ChemNet Distance) [45]\nwhich allows comparing molecules based on the latent information used to predict the activity of\nmolecules [17]. Since the generated molecules are in SMILES string format, we also employ natural\nlanguage processing metrics including BLEU, Exact Match [17], and Levenshtein distance [40]\nbetween the ground-truth molecules and generated molecules SMILES. Finally, to evaluate whether\ngenerated molecules are valid, we use RDKIT [35] to check the validity of generated molecules and\nreport the percent of the valid molecules.\nICL Prompt.\nOne ICL prompt example for text-based molecule design is shown in Figure 11.\nTable 14: The performance of LLMs and baseline in the Text-Based Molecule Design task. The best\nLLM is in bold font and the baseline is underlined.\nMethod\nBLEU (\u2191)\nExact (\u2191)\nLevenshtein (\u2193)\nValidity (\u2191)\nMACCS FTS (\u2191)\nRDK FTS (\u2191)\nMorgan FTS (\u2191)\nFCD (\u2193)\nMolT5-Large [17]\n0.601\n0.290\n41.600\n0.940\n0.879\n0.797\n0.752\n5.394\nGPT-4\n(zero-shot)\n0.490\u00b10.017\n0.046\u00b10.009\n47.418\u00b11.668\n0.758\u00b10.015\n0.733\u00b10.020\n0.514\u00b10.021\n0.432\u00b10.014\n11.913\u00b10.972\nGPT-4\n(Scaffold, k=10)\n0.816\u00b10.004\n0.174\u00b10.029\n21.160\u00b10.600\n0.888\u00b10.023\n0.867\u00b10.005\n0.738\u00b10.010\n0.672\u00b10.013\n6.224\u00b10.449\nGPT-4\n(Scaffold, k=5)\n0.815\u00b10.011\n0.164\u00b10.018\n21.862\u00b11.768\n0.874\u00b10.030\n0.865\u00b10.015\n0.741\u00b10.023\n0.670\u00b10.028\n5.843\u00b10.515\nGPT-4\n(Random, k=10)\n0.602\u00b10.016\n0.060\u00b10.007\n42.390\u00b11.008\n0.770\u00b10.030\n0.762\u00b10.013\n0.548\u00b10.017\n0.475\u00b10.015\n10.594\u00b10.414\nGPT-3.5\n(Scaffold, k=10)\n0.479\u00b10.156\n0.094\u00b10.011\n82.008\u00b140.354\n0.854\u00b10.059\n0.833\u00b10.006\n0.686\u00b10.016\n0.585\u00b10.013\n8.341\u00b10.607\nDavinci-003\n(Scaffold, k=10)\n0.741\u00b10.011\n0.100\u00b10.010\n25.648\u00b12.186\n0.936\u00b10.009\n0.783\u00b10.014\n0.648\u00b10.004\n0.560\u00b10.010\n8.335\u00b10.310\nLlama2-13B-chat\n(Scaffold, k=10)\n0.626\u00b10.013\n0.020\u00b10.000\n33.956\u00b12.648\n0.782\u00b10.008\n0.679\u00b10.015\n0.568\u00b10.014\n0.454\u00b10.009\n12.387\u00b10.437\nGAL-30B\n(zero-shot)\n0.004\u00b10.000\n0.000\u00b10.000\n2738.136\u00b1166.093\n0.956\u00b10.011\n0.233\u00b10.011\n0.109\u00b10.006\n0.053\u00b10.002\n35.091\u00b10.774\n23\nFigure 11: An ICL prompt example for Text-Based Molecule Design\nResults.\nThe results are reported in Table 14. We can observe that the best ICL prompting GPT\nmodels (GPT-4 and Davinci-003) can achieve competitive performance or even outperform the\nbaseline in some metrics (BLEU, Levenshtein). Although the GPT models significantly underperform\nthe baseline in terms of exact match and Morgan FTS metrics, it\u2019s important to note that we only\nutilize a maximum of 10 examples, which is substantially less than the training set (comprising\n26,407 training examples) used for the baseline. These results demonstrate the strong few-shot\ntext-based molecule design ability of GPT models. Last, not being exactly the same as the ground\ntruth doesn\u2019t necessarily mean it\u2019s incorrect, especially in the context of molecular design. The\nmolecules generated by GPT models may still be useful and can serve as alternatives to the ground\ntruth, given they fulfill the requirements described in the input text and a majority (over 89%) are\nchemically valid.\nCase studies.\nWe select three different types of molecules (organic molecule without rings, organic\nmolecule with ring, and metal atom) as examples, and show the generated molecules in Figure 12.\nWe observe that the structure of molecules generated by the GPT-4 (Scaffold, k=10) method is more\nsimilar to the ground truth compared to Davinci-003, GPT-4 (zero-shot), and even the baseline.\nAdditionally, for metal atoms design, GPT models outperform the baseline which wrongly generates\nthe SMILES instead of the metal atom. These cases show promising results of the molecule design\nability of GPT models. However, evaluating whether the generated molecules are helpful such as\nmolecule novelty in real-world scenarios is still a difficult problem. Thus we conclude that GPT\nmodels have excellent potential in molecule design and there are prospects for investigating this\nability.\nH\nMolecule Captioning\nMolecule captioning is an important task in computational chemistry, offering valuable insights and\napplications in various areas such as drug discovery, materials science, and chemical synthesis. Given\na molecule as input, the goal of this task is to generate a textual description that accurately describes\nthe key features, properties, and functional groups of the molecule. We also use the ChEBI-20\ndataset(CC BY 4.0) and the training set of it as the ICL candidates as discussed in the Text-Based\nMolecule Design Section. We use traditional captioning metrics including BLEU, ROUGE, and\nMETEOR for evaluation.\nICL Prompt.\nOne example of our ICL prompt for molecule captioning is shown in Figure 13.\n24\nFigure 12: Examples of molecules generated by different models.\nResults.\nThe results are reported in Table 15. We can observe that the best ICL prompting GPT\nmodels (GPT-4 and Davinci-003) can achieve competitive performance or even outperform the\nbaseline in some metrics (BLEU-2 and BLEU-4). This indicates the inspiring capability of the GPT\nmodels in the molecule captioning task.\nFigure 13: An ICL prompt example for molecule captioning\nCase studies.\nSame as case studies in the Text-Based Molecule Design task, we also select three\ndifferent types of molecules as examples, and the captions are shown in Figure 14. We observe that\nalthough the performance of the baseline is close to GPT models, the captions generated by the\nbaseline contain more descriptions that violate the chemical facts. In contrast, the captions generated\nby GPT-4 models contain only a few inaccurate descriptions, highlighting the excellent explaining\nability of GPT models. This highlights the limitations of applying traditional Natural Language\nProcessing (NLP) evaluation metrics to this task. Therefore, it is necessary to create more suitable\nevaluation metrics for chemistry-related generation tasks.\n25\nTable 15: The performance of LLMs and baseline in the molecule captioning task. The best LLM is\nin bold font and the baseline is underlined.\nMethod\nBLEU-2 (\u2191)\nBLEU-4 (\u2191)\nROUGE-1 (\u2191)\nROUGE-2 (\u2191)\nROUGE-L (\u2191)\nMETEOR (\u2191)\nMolT5-Large [17]\n0.482\n0.383\n0.574\n0.410\n0.516\n0.530\nGPT-4\n(zero-shot)\n0.062\u00b10.001\n0.013\u00b10.001\n0.192\u00b10.002\n0.040\u00b10.002\n0.125\u00b10.002\n0.209\u00b10.002\nGPT-4\n(Scaffold, k=10)\n0.464\u00b10.008\n0.365\u00b10.008\n0.545\u00b10.003\n0.362\u00b10.003\n0.459\u00b10.007\n0.519\u00b10.005\nGPT-4\n(Scaffold, k=5)\n0.456\u00b10.003\n0.357\u00b10.004\n0.540\u00b10.005\n0.355\u00b10.007\n0.455\u00b10.005\n0.505\u00b10.005\nGPT-4\n(Random, k=10)\n0.260\u00b10.007\n0.140\u00b10.007\n0.393\u00b10.004\n0.180\u00b10.006\n0.309\u00b10.004\n0.320\u00b10.007\nGPT-3.5\n(Scaffold, k=10)\n0.468\u00b10.010\n0.368\u00b10.010\n0.534\u00b10.005\n0.355\u00b10.007\n0.457\u00b10.006\n0.497\u00b10.005\nDavinci-003\n(Scaffold, k=10)\n0.488\u00b10.011\n0.391\u00b10.012\n0.532\u00b10.008\n0.359\u00b10.010\n0.465\u00b10.008\n0.478\u00b10.011\nLlama2-13B-chat\n(Scaffold, k=10)\n0.197\u00b10.005\n0.140\u00b10.004\n0.331\u00b10.005\n0.193\u00b10.005\n0.265\u00b10.005\n0.372\u00b10.006\nGAL-30B\n(zero-shot)\n0.008\u00b10.000\n0.002 \u00b1 0.000\n0.019\u00b10.002\n0.004\u00b10.000\n0.015\u00b10.002\n0.043\u00b10.002\nFigure 14: Examples captions generated by different models. Descriptions that violate chemical facts\nare marked in grey.\nI\nThe comparison of SMILES and SELFIES\nTable 16: F1 (\u2191) score of SMILES and SELFIES of GPT-4 model in molecular property prediction\ntasks.\nBBBP\nBACE\nHIV\nTox21\nClinTox\nSMILES\n0.587 \u00b1 0.018\n0.666 \u00b1 0.023\n0.797 \u00b1 0.021\n0.563 \u00b1 0.008\n0.736 \u00b1 0.033\nSELFIES\n0.541 \u00b1 0.001\n0.601 \u00b1 0.036\n0.784 \u00b1 0.014\n0.478 \u00b1 0.011\n0.654 \u00b1 0.025\n26\nTable 17: Performance of SMILES and SELFIES of GPT-4 model in reaction prediction task.\nTop-1 Accuracy (\u2191)\nInvalid SMILES/SELFIES (\u2193)\nSMILES\n0.230 \u00b1 0.022\n7.0% \u00b1 1.6%\nSELFIES\n0.110 \u00b1 0.007\n1.0% \u00b1 0.0%\nTable 18: Performance of SMILES and SELFIES of GPT-4 model in molecule design task.\nBLEU (\u2191)\nExact (\u2191)\nLevenshtein (\u2193)\nValidity (\u2191)\nMACCS FTS (\u2191)\nRDK FTS (\u2191)\nMorgan FTS (\u2191)\nFCD (\u2193)\nSMILES\n0.816 \u00b1 0.004\n0.174 \u00b1 0.029\n21.160 \u00b1 0.600\n0.888 \u00b1 0.023\n0.867 \u00b1 0.005\n0.738 \u00b1 0.010\n0.672 \u00b1 0.013\n6.224 \u00b1 0.449\nSELFIES\n0.277 \u00b1 0.009\n0.100 \u00b1 0.016\n76.162 \u00b1 2.229\n0.804 \u00b1 0.022\n0.619 \u00b1 0.010\n0.467 \u00b1 0.018\n0.399 \u00b1 0.017\n13.557 \u00b1 0.224\nTable 19: Performance of SMILES and SELFIES of GPT-4 model in molecule captioning task.\nBLEU-2 (\u2191)\nBLEU-4 (\u2191)\nROUGE-1 (\u2191)\nROUGE-2 (\u2191)\nROUGE-L (\u2191)\nMETEOR (\u2191)\nSMILES\n0.464 \u00b1 0.008\n0.365 \u00b1 0.008\n0.545 \u00b1 0.003\n0.362 \u00b1 0.003\n0.459 \u00b1 0.007\n0.519 \u00b1 0.005\nSELFIES\n0.459 \u00b1 0.012\n0.367 \u00b1 0.010\n0.530 \u00b1 0.007\n0.360 \u00b1 0.005\n0.456 \u00b1 0.005\n0.490 \u00b1 0.007\n27\n"
  },
  {
    "title": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus",
    "link": "https://arxiv.org/pdf/2305.18802.pdf",
    "upvote": "1",
    "text": "LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus\nYuma Koizumi1, Heiga Zen1, Shigeki Karita1, Yifan Ding1, Kohei Yatabe2, Nobuyuki Morioka1,\nMichiel Bacchiani1, Yu Zhang3, Wei Han3, Ankur Bapna3\n1 Google, Japan, 2 Tokyo University of Agriculture & Technology, Japan, 3 Google, USA\n{koizumiyuma,heigazen,karita}@google.com\nAbstract\nThis paper introduces a new speech dataset called \u201cLibriTTS-\nR\u201d designed for text-to-speech (TTS) use. It is derived by ap-\nplying speech restoration to the LibriTTS corpus, which con-\nsists of 585 hours of speech data at 24 kHz sampling rate from\n2,456 speakers and the corresponding texts. The constituent\nsamples of LibriTTS-R are identical to those of LibriTTS, with\nonly the sound quality improved. Experimental results show\nthat the LibriTTS-R ground-truth samples showed significantly\nimproved sound quality compared to those in LibriTTS. In addi-\ntion, neural end-to-end TTS trained with LibriTTS-R achieved\nspeech naturalness on par with that of the ground-truth sam-\nples. The corpus is freely available for download from http:\n//www.openslr.org/141/.\nIndex Terms: Text-to-speech, dataset, speech restoration\n1. Introduction\nText-to-speech (TTS) technologies have been rapidly advanced\nalong with the development of deep learning [1\u20136].\nWith\nstudio-quality recorded speech data, one can train acoustic mod-\nels [2, 3] and high-fidelity neural vocoders [7, 8]. These have\nenabled us to synthesize speech in a reading style almost as nat-\nural as human speech. In addition, many implementations of the\nlatest TTS models have been published [9,10], and the gateway\nto TTS research is certainly widening.\nOne of the remaining barriers to develop high-quality TTS\nsystems is the lack of large and high-quality public dataset.\nTraining of high-quality TTS models requires a large amount\nof studio-quality data. In several TTS papers, over 100 hours of\nstudio-recorded data have been used [3, 8, 11]. Unfortunately,\nsuch studio-recorded datasets are not publicly available, and\nthus reproducing their results is difficult for others.\nAt the same time, speech restoration (SR) has advanced us-\ning speech generative models [12\u201318]. These state-of-the-art\nmodels can convert reverberated lecture and historical speech\nto studio-recorded quality [16\u201318]. Inspired by these results,\nwe came up with an idea that the above-mentioned barrier can\nbe removed by applying SR to public datasets.\nWith this paper, we publish LibriTTS-R, a quality-improved\nversion of LibriTTS [19]. LibriTTS is a non-restrictive license\nmulti-speaker TTS corpus consisting of 585 hours of speech\ndata from 2,456 speakers and the corresponding texts.\nWe\ncleaned LibriTTS by applying a text-informed SR model, Mi-\nipher, [20] that uses w2v-BERT [21] feature cleaner and Wave-\nFit neural vocoder [8]. By subjective experiments, we show that\nthe speech naturalness of a TTS model trained with LibriTTS-\nR is greatly improved from that trained with LibriTTS, and is\ncomparable with that of the ground-truth.\nLibriTTS-R\nis\npublicly\navailable\nat\nhttp://www.\nopenslr.org/141/, with the same non-restrictive license.\nAudio samples of the ground-truth and TTS generated samples\nare available at our demo page1.\n2. The LibriTTS corpus\nThe LibriTTS corpus is one of the largest multi-speaker speech\ndatasets designed for TTS use. This dataset consists of 585\nhours of speech data at 24 kHz sampling rate from 2,456 speak-\ners and the corresponding texts. The audio and text materi-\nals are derived from the LibriSpeech corpus [22], which has\nbeen used for training and evaluating automatic speech recogni-\ntion systems. Since the original LibriSpeech corpus has several\nundesired properties for TTS including sampling rate and text\nnormalization issues, the samples in LibriTTS were re-derived\nfrom the original materials (MP3 from LibriVox and texts from\nProject Gutenberg) of LibriSpeech.\nOne issue is that the LibriTTS sound quality is not on\npar with smaller scale but higher quality TTS datasets such\nas LJspeech [23].\nThe quality of the TTS output is highly\naffected by that of the speech samples used in model train-\ning. Therefore, the quality of the generated samples of a TTS\nmodel trained on LibriTTS doesn\u2019t match those of the ground-\ntruth samples [24, 25]. For example, Glow-TTS achieved 3.45\nmean-opinion-score (MOS) on LibriTTS where the speech ob-\ntained from the ground-truth mel-spectrograms by a vocoder\nwas 4.22 [24]. Note that MOSs on the LJspeech for generated\nand ground-truth were 4.01 and 4.19, respectively [24]. The re-\nsults suggest that the quality of speech samples in LibriTTS are\ninadequate for training of high-quality TTS models.\n3. Data processing pipeline\nAlthough noisy TTS datasets are useful for advanced TTS\nmodel training [26\u201328], access to large scale high-quality\ndatasets is as equally important for advancing TTS techniques.\nTo provide a public large-scale and high-quality TTS dataset,\nwe apply a SR model to LibriTTS.\n3.1. Speech restoration model overview\nOne critical requirement of SR models for the purpose of clean-\ning datasets is robustness. If the SR model generates a large\nnumber of samples with artifacts, it will adversely impact the\nsubsequent TTS model training. Therefore, for our purposes,\nwe need to reduce as much as possible the number of samples\nthat fail to be recovered.\n1https://google.github.io/df-conformer/\nlibrittsr/\narXiv:2305.18802v1  [eess.AS]  30 May 2023\nPnG-BERT\nSpeaker \nencoder\nFrozen\nLibriTTS samples\nw2v-BERT\nResampling\n24 kHz original waveform\nw2v-BERT \nfeature cleaner \nWaveFit-5\nTranscript\nLibriTTS-R samples\n24 kHz restored waveform\nTranscript\nFigure 1: Data processing pipeline overview. Speech samples\nin the LibriTTS corpus are restored using Miipher [20].\nTo satisfy this requirement, we use a text-informed para-\nmetric re-synthesis-based SR model, Miipher [20], as shown in\nFig. 1. In this model, first, w2v-BERT features are extracted\nby w2v-BERT [21] from the noisy waveform. Then, a DF-\nConformer [29]-based feature-cleaner predicts the w2v-BERT\nfeatures of the clean waveform. Finally, the restored waveform\nis synthesized using a WaveFit-5 neural vocoder [8].\nThe reason for selecting Miipher is that it adresses two par-\nticularly difficult to restore degradation patterns observed in\nLibriTTS samples. The first degradation is phoneme masking.\nSpeech signals are sometimes masked by noise and/or rever-\nberation, resulting in speech that is is difficult to discriminate\nfrom noise without additional information. The second degra-\ndation is phoneme deletion. Important frequency parts of some\nphonemes could be missing from the signal due to non-linear\naudio processing and/or down-sampling. To address these prob-\nlems, Miipher introduced two techniques. (i) for the input fea-\nture, it uses w2v-BERT [21] features instead of log-mel spectro-\ngram used in a conventional SR model [17], and (ii) to use lin-\nguistic features conditioning extracted by PnG-BERT [3] from\nthe transcript corresponding to the noisy speech. Since w2v-\nBERT is trained on large amounts of degraded speech samples\nand it improves ASR performance, we expect its effectiveness\non making SR models robust against speech degradation. In\naddition, the use of text information improving speech inpaint-\ning performance [30], we consider that it also improves speech\nrestoration performance. For the detail, please see the original\npaper [20].\n3.2. Speech restoration model training\nWe trained a Miipher model with a proprietary dataset that\ncontains 2,680 hours of noisy and studio-quality speech pairs.\nThe target speech dataset contains 670 hours of studio-recorded\nAustralian, British, Indian, Nigerian, and North American En-\nglish at 24 kHz sampling.\nFor the noise dataset, we used\nthe TAU Urban Audio-Visual Scenes 2021 dataset [31], inter-\nnally collected noise snippets that simulate conditions like cafe,\nkitchen, and cars, and noise sources. The noisy utterances were\ngenerated by mixing randomly selected speech and noise sam-\nples from these datasets with signal-to-noise ratio (SNR) from\n5 dB to 30 dB. In addition, we augmented the noisy dataset with\n4 patterns depending on the presence or absence of reverbera-\ntion and codec artifacts. A room impulse response (RIR) for\neach sample was generated by a stochastic RIR generator using\nthe image method [32]. For simulating codec artifacts, we ran-\ndomly applied one of MP3, Vorbis, A-law, Adaptive Multi-Rate\nWideband (AMR-WB), and OPUS with a random bit-rate. The\nTable 1: MOS and SxS test results on the ground-truth samples\nwith their 95% confidence intervals. A positive SxS score indi-\ncates that LibriTTS-R was preferred.\nSplit\nMOS (\u2191)\nSxS\nLibriTTS\nLibriTTS-R\ntest-clean\n4.36 \u00b1 0.08\n4.41 \u00b1 0.07\n0.80 \u00b1 0.15\ntest-other\n3.94 \u00b1 0.10\n4.09 \u00b1 0.10\n1.42 \u00b1 0.14\ndetailed simulation parameters were listed in [20].\nWe first pre-trained the feature-cleaner and WaveFit neu-\nral vocoder 150k and 1M steps, respectively, where WaveFit\nwas trained to reconstruct waveform from clean w2v-BERT\nfeatures.\nThen, we fine-tuned the WaveFit neural vocoder\n350k steps using cleaned w2v-BERT features by the pre-trained\nfeature-cleaner.\n3.3. Speech restoration pipeline\nFirst, we calculated PnG-BERT [3] features from a transcript\nand a speaker embedding using the speaker encoder described\nin [20] from the original 24 kHz sampling waveform. Here, for\nspeech samples with waveform lengths shorter than 2 seconds,\nthe speaker embedding was calculated after repeating them to\nget a pseudo longer waveform. Since the w2v-BERT [21] model\nwas trained on 16 kHz waveforms, we applied down-sampling\nto the LibriTTS sample for calculating w2v-BERT features. Fi-\nnally, we synthesized restored 24 kHz sampling waveform using\nWaveFit [8].\n4. Experiments\n4.1. Subjective experiments for ground-truth samples\n4.1.1. Experimental setups\nWe first compared the quality of ground-truth speech samples\nin LibriTTS-R with those in LibriTTS. We evaluated the sound\nquality using \u201ctest-clean\u201d and \u201ctest-other\u201d subsets.\nWe ran-\ndomly selected 620 samples from each subset. Since the \u201ctrain-\n*\u201d and \u201cdev-*\u201d subsets are also divided into \u201cclean\u201d and \u201cother\u201d\naccording to the same word-error-rate (WER)-based criteria, the\nsound quality of the entire dataset can be predicted by evaluat-\ning the sound quality of these two subsets.\nTo evaluate subjective quality, we rated speech quality\nthrough mean-opinion-score (MOS) and side-by-side (SxS)\npreference tests. We asked to rate the naturalness in MOS test,\nand \u201cwhich sound quality is better?\u201d in SxS test. The scale\nof MOS was a 5-point scale (1: Bad, 2: Poor, 3: Fair, 4: Good,\n5: Excellent) with rating increments of 0.5, and that of SxS was\na 7-point scale (-3 to 3). Test stimuli were randomly chosen and\neach stimulus was evaluated by one subject. Each subject was\nallowed to evaluate up to six stimuli, that is, over 100 subjects\nparticipated in this experiment to evaluate 640 samples in each\ncondition. The subjects were paid native English speakers in\nthe United States. They were requested to use headphones in a\nquiet room. Audio samples are available in our demo page 1.\n4.1.2. Results\nTable 1 shows the MOS and SxS test results. In terms of speech\nnaturalness, LibriTTS achieved high MOSs: 4.36 and 3.94 on\ntest-clean and test-other, respectively.\nAlthough LibriTTS-R\nachieved better MOSs than LibriTTS in both splits, the differ-\n0\n64\n128\nMel-filter index\n0\n1.6\n3.2\nTime [sec]\n0\n64\n128\nMel-filter index\n0\n5.05\n10.1\nTime [sec]\n0\n1.5\n3.0\nTime [sec]\n0\n2.1\n4.2\nTime [sec]\nFigure 2: Log-mel spectrograms of ground-truth waveforms from (top) LibriTTS and (bottom) LibriTTS-R. The left two and right two\nexamples are from \u201ctest-clean\u201d and \u201ctest-other\u201d splits, respectively.\nence was not significant. The reason of small difference in nat-\nuralness might be because ground-truth samples are real speech\nspoken by humans. In contrast, in terms of sound quality rated\nby SxS tests, significant differences were observed on both split.\nTo confirm whether the text-content and speaker in the re-\nstored speech samples are maintained, we evaluated the WER\nand speaker similarity. To compute WER, we used \u201cPre-trained\nConformer XXL\u201d model proposed in [33]. WER of \u201ctest-clean\u201d\nand \u201ctest-other\u201d splits of LibriTTS were 3.4 and 5.1, whereas\nthose of LibriTTS-R were 3.2 and 5.1, respectively2. There-\nfore, the text contents are considered to be not changed. To\nevaluate speaker similarity, we used the cosine similarity of\nspeaker embedding [34, 35]. We calculated the similarity be-\ntween the different utterances spoken by the same speaker in\nthe same dataset. This is because the samples in LibriTTS are\ndistorted, even if the similarity between corresponding samples\nin LibriTTS and LibriTTS-R is small, this does not necessarily\nindicate speaker similarity. The cosine similarity of LibriTTS\n\u201ctest-clean\u201d and \u201ctest-other\u201d splits were 0.784 and 0.755, and\nthose of LibriTTS-R were 0.762 and 0.745. Since the similar-\nity calculated from the samples in LibriTTS spoken by different\nspeakers was 0.302, the speech characteristics of each speaker\nis considered to be consistent.\nFigure 2 shows the 128-dim log-mel spectrogram of speech\nsamples from LibriTTS and LibriTTS-R. We can see that the\nLibriTTS samples are degraded by a variety of factors even\nif these are from the test-clean split: from left to right, it can\nbe considered that speech samples were degraded by down-\nsampling, environmental noise, reverberation, and non-linear\nspeech enhancement, respectively. As we can see spectrograms\nof LibriTTS-R samples, the SR model restored these speech\nsamples into high-quality ones. This could be the reason of the\nsignificant differences in the SxS tests.\nNote that we have found a few examples that LibriTTS\nspeech sample achieved a better score in SxS comparison. By\nlistening these examples, two of 640 LibriTTS-R speech sam-\nples were distorted due to the failure of SR. Since it is difficult\n2WER were a bit higher than those reported in the original pa-\nper [33], because the ASR model was trained on noisy speech and tran-\nscripts normalized by a different text-normalizer.\nto manually check all samples, we have not checked all speech\nsamples in LibriTTS-R. Therefore, the samples in training splits\nmay also contain a small number of distorted samples.\n4.2. Subjective experiments for TTS generated samples\n4.2.1. Experimental setups\nWe trained multi-speaker TTS models with the same architec-\nture and the same hyper-parameters using either the LibriTTS\nor LibriTTS-R corpus. The TTS model was build by concate-\nnating the following acoustic model and neural vocoder without\njoint fine-tuning.\nAcoustic model: We used a duration unsupervised Non-\nAttentive Tacotron (NAT) with a fine-grained variational auto-\nencoder (FVAE) [11]. We used the same hyper-parameters and\ntraining parameters listed in the original paper [11]. We trained\nthis model for 150k steps with a batch size of 1,024.\nNeural vocoder: We used a WaveRNN [36] which con-\nsisted of a single long short-term memory layer with 512 hidden\nunits, 5 convolutional layers with 512 channels as the condi-\ntioning stack to process the mel-spectrogram features, and a 10-\ncomponent mixture of logistic distributions as its output layer.\nThe learning rate was linearly increased to 10\u22124 in the first 100\nsteps then exponentially decayed to 10\u22126 from 200k to 300k\nsteps. We trained this model using the Adam optimizer [37] for\n500k steps with a batch size of 512.\nThe TTS model was trained on two types of training\ndatasets: Train-460 and Train-960. Train-460 consists of the\n\u201ctrain-clean-100\u201d and \u201ctrain-clean-360\u201d subsets, and Train-960\nindicates using \u201ctrain-other-500\u201d in addition to Train-460.\nFor the test sentences, we randomly selected 620 evaluation\nsentences from the test-clean split. We synthesized waveforms\nwith 6 speakers (three female and three male) those are used in\nthe LibriTTS baseline experiments [19]. The female and male\nreader IDs were (19, 103, 1841) and (204, 1121, 5717), respec-\ntively. To evaluate subjective quality, we rated speech natural-\nness through MOS and side-by-side (SxS) preference tests. The\nlistening test setting was the same as Sec. 4.1 Audio samples of\ngenerated speech are available in our demo page 1.\n0\n64\n128\nMel-filter index\nSpeaker ID: 103\n0\n1.3\n2.6\nTime [sec]\n0\n64\n128\nMel-filter index\nSpeaker ID: 1841\n0\n1.25\n2.5\nTime [sec]\nSpeaker ID: 1121\n0\n1.25\n2.5\nTime [sec]\nSpeaker ID: 5717\n0\n1.3\n2.6\nTime [sec]\nFigure 3: Log-mel spectrograms of TTS generated waveforms where the multi-speaker TTS model was trained on (top) LibriTTS and\n(bottom) LibriTTS-R, respectively. The input text was \u201cThe Free State Hotel served as barracks\u201d.\nTable 2: MOSs for the baseline multi-speaker TTS model outputs with their 95% confidence intervals.\nTraining dataset\nSpeaker ID\n19\n103\n1841\n204\n1121\n5717\nLibriTTS Train-460\n2.49 \u00b1 0.10\n2.94 \u00b1 0.10\n3.40 \u00b1 0.09\n2.88 \u00b1 0.10\n2.72 \u00b1 0.10\n2.86 \u00b1 0.09\nLibriTTS Train-960\n2.59 \u00b1 0.10\n2.75 \u00b1 0.10\n3.35 \u00b1 0.10\n2.74 \u00b1 0.09\n2.83 \u00b1 0.10\n2.97 \u00b1 0.10\nLibriTTS-R Train-460\n4.11 \u00b1 0.08\n4.09 \u00b1 0.08\n3.88 \u00b1 0.09\n3.67 \u00b1 0.09\n3.92 \u00b1 0.09\n3.67 \u00b1 0.08\nLibriTTS-R Train-960\n4.06 \u00b1 0.08\n4.31 \u00b1 0.08\n4.20 \u00b1 0.08\n4.11 \u00b1 0.08\n4.23 \u00b1 0.07\n4.08 \u00b1 0.08\nTable 3: SxS test results on the baseline multi-speaker TTS\nmodel outputs with their 95% confidence intervals. A positive\nscore indicates that training on LibriTTS-R was preferred.\nSpeaker ID\nTraining dataset\nTrain-460\nTrain-960\n19\n2.38 \u00b1 0.11\n2.51 \u00b1 0.10\n204\n1.84 \u00b1 0.14\n2.20 \u00b1 0.12\n4.2.2. Results\nTable 2 shows the MOS results. In all speaker IDs except for\nID 19, the TTS model using LibriTTS-R Train-960 as the train-\ning dataset achieved the highest MOSs. For Speaker ID 19, the\nmodel using LibriTTS-R Train-460 achieved the highest MOS,\nwhich was not significantly different from that using LibriTTS-\nR Train-960. In other speaker IDs, MOSs of LibriTTS-R Train-\n960 were significantly better than that of LibriTTS-R Train-460.\nThis trend was not observed in LibriTTS, rather in some cases,\nMOS was decreased by using LibriTTS Train-960. The rea-\nson for this degradation might be because that the \u201ctrain-other-\n500\u201d split contains a lot of degraded speech samples. This result\nsuggests that the use of LibriTTS \u201ctrain-other-500\u201d split rather\ndegrades the output quality of the TTS. In contrast, speech sam-\nples in LibriTTS-R \u201ctrain-other-500\u201d split are restored to high-\nquality speech samples, and resulting in that enables us to use\na large amount of high-quality training data and improved the\nnaturalness of the TTS outputs. In addition, the TTS model\ntrained on LibriTTS-R Train-960 achieved MOSs on a par with\nhuman spoken speech samples in LibriTTS, effects of a few dis-\ntorted speech samples in the training can be considered as not\nsignificant.\nTable 3 shows the SxS results. We observed that the use\nof LibriTTS-R also improve not only naturalness but also the\nsound quality of TTS outputs. Figure 3 shows 128-dim log-mel\nspectrograms of TTS outputs. We can see the harmonic struc-\nture is broken in the ID 5717 output of the TTS model trained\non LibriTTS (top right). The presence of such a sample could\nbe the reason for the lower naturalness scores on the MOS test.\nAlso, from ID 103 and 1121 examples, we can observe back-\nground noise in the output of TTS model trained on LibriTTS.\nSuch background noise does not exist in the outputs of TTS\nmodel trained on LibriTTS-R. From these results, we conclude\nthat the LibriTTS-R corpus is a better TTS corpus that the Lib-\nriTTS corpus, and enables us to train a high-quality TTS model.\n5. Conclusions\nThis paper introduced LibriTTS-R, a sound quality improved\nversion of LibriTTS [19]. We cleaned speech samples in the\nLibriTTS corpus by applying an SR model [20]. By subjec-\ntive experiments, we show that the speech naturalness of a TTS\nmodel trained with LibriTTS-R is improved from that trained\nwith LibriTTS, and is comparable with that of the ground-truth.\nThis corpus is released online, and it is freely available for\ndownload from http://www.openslr.org/141/.\nWe\nhope that the release of this corpus accelerates TTS research.\n6. References\n[1] A. van den Oord,\nS. Dieleman,\nH. Zen,\nK. Simonyan,\nO. Vinyals,\nA. Graves,\nN. Kalchbrenner,\nA. Senior,\nand\nK. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d\narXiv:1609.03499, 2016.\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang,\nZ. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous,\nY. Agiomvrgiannakis, and Y. Wu, \u201cNatural TTS synthesis by\nconditioning WaveNet on mel spectrogram predictions,\u201d in Proc.\nICASSP, 2018.\n[3] Y. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu, \u201cPnG BERT: Aug-\nmented BERT on phonemes and graphemes for neural TTS,\u201d in\nProc. Interspeech, 2021.\n[4] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. J. Weiss, and Y. Wu,\n\u201cParallel Tacotron: Non-autoregressive and controllable TTS,\u201d in\nProc. ICASSP, 2021.\n[5] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y.\nLiu, \u201cFastSpeech: Fast, robust and controllable text to speech,\u201d in\nProc. NeurIPS, 2019.\n[6] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu,\n\u201cFastSpeech 2: Fast and high-quality end-to-end text to speech,\u201d\nin Proc. Int. Conf. Learn. Represent. (ICLR), 2021.\n[7] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative adversarial\nnetworks for efficient and high fidelity speech synthesis,\u201d in Proc.\nNeurIPS, 2020.\n[8] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, \u201cWaveFit: An\niterative and non-autoregressive neural vocoder based on fixed-\npoint iteration,\u201d in Proc. IEEE SLT, 2023.\n[9] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe,\nT. Toda, K. Takeda, Y. Zhang, and X. Tan, \u201cESPnet-TTS: Uni-\nfied, reproducible, and integratable open source end-to-end text-\nto-speech toolkit,\u201d in Proc. ICASSP, 2020.\n[10] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cor-\nnell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba,\nJ. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Ras-\ntorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. De Mori, and\nY. Bengio, \u201cSpeechBrain: A general-purpose speech toolkit,\u201d\narXiv:2106.04624, 2021.\n[11] J. Shen, Y. Jia, M. Chrzanowski, Y. Zhang, I. Elias, H. Zen,\nand Y. Wu, \u201cNon-attentive Tacotron: Robust and controllable\nneural TTS synthesis including unsupervised duration modeling,\u201d\narXiv:2010.04301, 2020.\n[12] S. Maiti and M. I. Mandel, \u201cParametric resynthesis with neural\nvocoders,\u201d in Proc. IEEE WASPAA, 2019.\n[13] \u2014\u2014, \u201cSpeaker independence of neural vocoders and their effect\non parametric resynthesis speech enhancement,\u201d in Proc. ICASSP,\n2020.\n[14] T. Saeki, S. Takamichi, T. Nakamura, N. Tanji, and H. Saruwatari,\n\u201cSelfRemaster: Self-supervised speech restoration with analysis-\nby-synthesis approach using channel modeling,\u201d in Proc Inter-\nspeech, 2022.\n[15] J. Su, Z. Jin, and A. Finkelstein, \u201cHiFi-GAN: High-fidelity de-\nnoising and dereverberation based on speech deep features in ad-\nversarial networks,\u201d in Proc. Interspeech, 2020.\n[16] \u2014\u2014, \u201cHiFi-GAN-2: Studio-quality speech enhancement via gen-\nerative adversarial networks conditioned on acoustic features,\u201d in\nProc. IEEE WASPAA, 2021.\n[17] H. Liu, X. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang,\nand Y. Wang, \u201cVoiceFixer: A unified framework for high-fidelity\nspeech restoration,\u201d in Proc Interspeech, 2022.\n[18] J. Serr`a, S. Pascual, J. Pons, R. O. Araz, and D. Scaini,\n\u201cUniversal speech enhancement with score-based diffusion,\u201d\narXiv:2206.03065, 2022.\n[19] H. Zen, R. Clark, R. J. Weiss, V. Dang, Y. Jia, Y. Wu, Y. Zhang,\nand Z. Chen, \u201cLibriTTS: A corpus derived from LibriSpeech for\ntext-to-speech,\u201d in Proc. Interspeech, 2019.\n[20] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka,\nY. Zhang, W. Han, A. Bapna, and M. Bacchiani, \u201cMiipher: A ro-\nbust speech restoration model integrating self-supervised speech\nand text representations,\u201d arXiv:2303.01664, 2023.\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and\nY. Wu, \u201cw2v-BERT: Combining contrastive learning and masked\nlanguage modeling for self-supervised speech pre-training,\u201d in\nProc. IEEE ASRU, 2021.\n[22] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: An ASR corpus based on public domain audio books,\u201d\nin Proc. ICASSP, 2015.\n[23] K. Ito and L. Johnson, \u201cThe lj speech dataset,\u201d https://keithito.\ncom/LJ-Speech-Dataset/, 2017.\n[24] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-tts: A generative\nflow for text-to-speech via monotonic alignment search,\u201d in Proc.\nAdv. Neural Inf. Process. Syst. (NeurIPS), 2020.\n[25] R. Valle, K. J. Shih, R. Prenger, and B. Catanzaro, \u201cFlowtron: an\nautoregressive flow-based generative network for text-to-speech\nsynthesis,\u201d in Proc. Int. Conf. Learn. Represent. (ICLR), 2021.\n[26] E. Casanova, J. Weber, C. Shulby, A. C. Junior, E. G\u00a8olge,\nand M. Antonelli Ponti, \u201cYourTTS: Towards zero-shot multi-\nspeaker TTS and zero-shot voice conversion for everyone,\u201d\narXiv:2112.02418, 2021.\n[27] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen,\nY. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cNeural\ncodec language models are zero-shot text to speech synthesizers,\u201d\narXiv:2301.02111, 2023.\n[28] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin,\nO. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour,\n\u201cSpeak, read and prompt: High-fidelity text-to-speech with mini-\nmal supervision,\u201d arXiv:2302.03540, 2023.\n[29] Y. Koizumi, S. Karita, S. Wisdom, H. Erdogan, J. R. Hershey,\nL. Jones, and M. Bacchiani, \u201cDF-Conformer: Integrated archi-\ntecture of Conv-TasNet and Conformer using linear complexity\nself-attention for speech enhancement,\u201d in Proc. IEEE WASPAA,\n2021.\n[30] Z. Borsos, M. Sharifi, and M. Tagliasacchi, \u201cSpeechPainter: Text-\nconditioned speech inpainting,\u201d in Proc. Interspeech, 2022.\n[31] S. Wang, A. Mesaros, T. Heittola, and T. Virtanen, \u201cA curated\ndataset of urban scenes for audio-visual scene analysis,\u201d in Proc.\nICASSP, 2021.\n[32] J. B. Allen and D. A. Berkley, \u201cImage method for efficiently sim-\nulating small-room acoustics,\u201d J. Acoust. Soc. Am., 1979.\n[33] Y. Zhang, J. Qin, D. S. Park, W. Han, C.-C. Chiu, R. Pang, Q. V.\nLe, and Y. Wu, \u201cPushing the limits of semi-supervised learning\nfor automatic speech recognition,\u201d in Proc. NeurIPS SAS 2020\nWorkshop, 2020.\n[34] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, z. Chen,\nP. Nguyen, R. Pang, I. Lopez Moreno, and Y. Wu, \u201cTransfer learn-\ning from speaker verification to multispeaker text-to-speech syn-\nthesis,\u201d in Proc. NeurIPS, 2018.\n[35] Y. Chen, Y. Assael, B. Shillingford, D. Budden, S. Reed,\nH. Zen, Q. Wang, L. C. Cobo, A. Trask, B. Laurie, C. Gulcehre,\nA. van den Oord, O. Vinyals, and N. de Freitas, \u201cSample efficient\nadaptive text-to-speech,\u201d in Proc. ICLR, 2019.\n[36] N.\nKalchbrenner,\nW.\nElsen,\nK.\nSimonyan,\nS.\nNoury,\nN. Casagrande, W. Lockhart, F. Stimberg, A. van den Oord,\nS. Dieleman, and K. Kavukcuoglu, \u201cEfficient neural audio\nsynthesis,\u201d in Proc. ICML, 2018.\n[37] D. P. Kingma and J. L. Ba, \u201cAdam: A method for stochastic opti-\nmization,\u201d in Proc. ICLR, 2015.\n"
  },
  {
    "title": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation",
    "link": "https://arxiv.org/pdf/2305.19245.pdf",
    "upvote": "1",
    "text": "AlteredAvatar: Stylizing Dynamic 3D Avatars with Fast Style Adaptation\nThu Nguyen-Phuoc\nGabriel Schwartz\nYuting Ye\nStephen Lombardi\nLei Xiao\nReality Labs Research, Meta\nEvil clown\nOld\n3D cartoon\nWatercolor\nBronze sculpture\nInk sketch\nOriginal\nRotten zombie \nFox\n2D comics\nInitialization from the original avatar\nMeta-learnt initialization\nDirect optimization\nMeta-learning\nFast style adaptation\n\u04441\n\u04443\n\u04442\n\u0444\n\u0444meta\nPainting\nFigure 1. Given learnt photorealistic dynamic 3D avatars, AlteredAvatar stylizes the avatars to match a reference style image or a textual\ndescription. AlteredAvatar can fast adapt to novel styles in a small number of update steps, and generates consistent stylized results across\ndifferent viewpoints and expressions.\nAbstract\nThis paper presents a method that can quickly adapt dy-\nnamic 3D avatars to arbitrary text descriptions of novel\nstyles. Among existing approaches for avatar stylization,\ndirect optimization methods can produce excellent results\nfor arbitrary styles but they are unpleasantly slow. Further-\nmore, they require redoing the optimization process from\nscratch for every new input. Fast approximation methods\nusing feed-forward networks trained on a large dataset of\nstyle images can generate results for new inputs quickly, but\ntend not to generalize well to novel styles and fall short in\nquality. We therefore investigate a new approach, AlteredA-\nvatar, that combines those two approaches using the meta-\nlearning framework. In the inner loop, the model learns\nto optimize to match a single target style well; while in\nthe outer loop, the model learns to stylize efficiently across\nmany styles. After training, AlteredAvatar learns an initial-\nization that can quickly adapt within a small number of up-\ndate steps to a novel style, which can be given using a text\ndescription, a reference image, or a combination of both.\nWe show that AlteredAvatar can achieve a good balance be-\ntween speed, flexibility and quality, while maintaining con-\nsistency across a wide range of novel views and facial ex-\npressions.\n1. Introduction\nCreating high-quality human face models has a wide\nrange of applications in the movie and film industry. Re-\ncently, social telepresence applications in virtual reality\n(VR) and mixed reality (MR) have opened up new opportu-\nnities for highly-accurate and authentic avatars that can be\ndriven from users\u2019 expression input. While photorealistic\navatars are required in certain applications, stylized avatars\nwould be desired in other scenarios. Imagine being able to\nchange one\u2019s avatar as one moves between different events\nand applications in VR: one can put on Goth makeup to at-\ntend a virtual concert, turn into an alien to attend a virtual\nComic Con, or turn into a zombie to play VR zombie games.\nIn this paper, we focus on the problem of stylizing dy-\nnamic 3D head avatars for VR/MR applications.\nThis\npresents a unique requirement for the choice of avatar rep-\nresentation: the avatars must faithfully capture the identity\n1\narXiv:2305.19245v1  [cs.CV]  30 May 2023\nand expression of the users driving them, can be rendered\nfrom a wide range of novel views, and be efficient to ren-\nder in high fidelity. Most work in 3D avatar stylization re-\nlies on static mesh or tri-planes representation [1, 18, 30].\nOther work adopts neural radiance fields which has high\nquality but slow to render [8,19,21]. Therefore, we choose\nto work with Instant Avatar [9], a learnt 3D dynamic avatar\nrepresentation based on U-Nets [51]. Instant Avatars can be\nefficiently rendered at high fidelity, and more importantly,\nfaithfully capture the eye gaze and dynamicexpressions of\nthe original identity [9,40], making them suitable for down-\nstream MR/VR applications.\nUntil very recently, the majority of neural style transfer\nwork can only work with style guidance from a single or\na few 2D image [16, 34, 41, 59, 60]. With AlteredAvatar,\nwe want to enable users to intuitively describe their desired\nstyle using texts, in addition to images. This greatly opens\nup the possibilities of target styles, ranging from abstract\nstyle concepts such as \u201dImpressionist movement\u201d to very\nconcrete ones such as a particular style of a famous paint-\ning or movie. Therefore, we propose to use CLIP [48], a\nmodel pre-trained for multi-modal language and vision rea-\nsoning with contrastive learning. Additionally, CLIP has\nshown the ability to capture semantic concepts of visual de-\npictions well [26,57]. This produces stylized results that are\nsemantically consistent with the input avatar and the target\nstyle textual description, unlike previous image-based styl-\nization approaches that require both the content and style\ninput images to be semantically similar for the best quality.\nWork on neural stylization can be divided into two ap-\nproaches: direct optimization, and training stylization net-\nworks. Direct optimization produces high quality results,\nbut can be time and computationally intensive. More im-\nportantly, for every new style or new content image, one has\nto do the optimization from scratch, making them imprac-\ntical for product deployments. Stylization networks aim to\nsolve this issue by training a network on large datasets of\ncontent and style images using perception loss [24, 27] or\nGAN loss [59, 60]. At test time, new styles and contents\ncan be used to synthesize the results without any retrain-\ning, but usually at the cost of degraded visual quality of the\nstylized results.\nWe propose AlteredAvatar, a third approach that aims to\nstrike a balance between the computational time, the flexi-\nbility to adapt to novel styles, and the quality of the stylized\nresults. In particular, we aim to learn an avatar represen-\ntation that can be quickly adapted to unseen styles after a\nsmall number of update steps. It is worth noting that our ap-\nproach does not add any extra parameters to the avatar rep-\nresentations. Instead, using a meta-learning framework, we\nonly fine-tune a small subset of the avatar network param-\neters to learn a good weight initialization that can rapidly\nadapt to new styles.\nPreviously, meta-learning has been\nused to learn representations that can quickly adapt to novel\nmaterial [13], 3D static scenes [7, 55], talking head videos\n[61] or styles for image neural style transfer [62].\nThis\nis different from approaches that are based on hypernet-\nworks [17], which train a separate network to predict a new\nset of network weights for every new input. Hypernetwork-\nbased approaches can be memory-intensive and tend to pro-\nduce less competitive results [3,11,22].\nIn summary, our proposed AlteredAvatar offers the first\ndynamic 3D avatar stylization framework that can quickly\nadapt to match novel styles given as text descriptions or a\ncombination of both texts and images. The contributions of\nAlteredAvatar are threefold:\n\u2022 We introduce a meta-learning approach for stylizing\ndynamic 3D avatars, enabling fast adaptation to any\nnew style using only a small number of update steps.\n\u2022 We show that using CLIP features in AlteredAvatar\nprovides an expressive and flexible framework for\nsemantically-meaningful stylization using a textual de-\nscription, or a combination of both reference style text\nand image.\n\u2022 Through extensive experiments, we demonstrate that\nAlteredAvatar produces stylized dynamic 3D avatars in\nhigh resolution with consistency across views and ex-\npressions, with the flexibility to quickly adapt to novel\nstyles.\n2. Related work\n2.1. Neural stylization\nNeural style transfer methods, including portrait styliza-\ntion methods, can be divided into two categories: direct op-\ntimization methods and learning-based methods. Optimiza-\ntion methods [16,18,20,23,34,43] generate impressive re-\nsults that match the content and styles images well, but are\nslow due to their iterative nature. Moreover, they lack flex-\nibility since the results are overfitted to a particular style or\ncontent, and for every new input, one has to do the opti-\nmization from scratch.\nLearning based methods [11, 27, 36, 37, 59, 60] instead\ntrain a style-transfer network on a style image dateset and\ncan quickly generate results for new inputs without recom-\nputation, even in real time [24]. However, their results only\nwork well for the styles that are used to train the networks\n[27,56,59,60] or have worse visual quality [11,24]. Recent\nwork adopts meta-learning to learn a model that can both\nadapt quickly to new arbitrary inputs and generate results\nwith high visual quality [53,62]. Their results are promising\nbut at the cost of increased network parameters to train [53],\nand only work with single 2D images [53,62].\n2\nWith AlteredAvatar, we aim to learn dynamic 3D avatar\nrepresentations that can quickly adapt to arbitrary styles us-\ning a single text description or image, while maintaining the\nstyle consistency across views and expressions. This can be\nseen as a third approach to stylization that strikes a balance\nbetween speed, flexibility to novel styles, and the quality of\nthe results.\n2.2. Text-guided synthesis\nWith recent advancements in text-to-image models [49,\n50, 52] and pre-trained language-vision models [48], there\nhas been an explosion of work in text-to-image, text-to-\nvideo, and text-to-3D synthesis [21, 25, 29, 32, 38, 42, 46,\n47, 58, 58].\nAdditionally, work in domain adaptation or\nimage editing combines a pre-trained StyleGAN [28] and\nCLIP to transfer photorealistic images to new domains us-\ning only texts without any (or only one) image training data\n[2, 3, 10, 15, 30, 46, 64]. For stylization, using text descrip-\ntions provides users with a very intuitive interface to express\ntheir desired styles. Moreover, textual descriptions can be\nvery expressive and cover a wide range of styles. This can\nbe a specific art movement such as \u201dCubism\u201d or \u201dImpres-\nsionism\u201d, or more descriptions such as \u201dvintage poster art\nusing only primary colours\u201d.\nRecent work in text-guided 3D avatar stylization [4,8,19,\n30, 63] shows the rapid advancement in text-guided avatar\nstylization. However, ClipFace [4] only focuses on styliz-\ning the texture of the avatar, although geometry has been\nacknowledged to be an important factor of style [31, 39].\nMeanwhile, RODIN [58] can only generate static avatars\nwith a particular aesthetics due to its reliance on a synthetic\ntraining dataset. Similarly, DATID-3D [30] can adapt 3D\nfaces to a novel style using a text-to-image diffusion-guided\ngenerative face model, but the faces are limited only to more\nfrontal views and have fixed expressions. Finally, Dream-\nFace [63], despite its impressive text-to-face results, cur-\nrently does not support personalizing to individual users\u2019s\nidentities. Meanwhile, concurrent work, NeRF-Art [8] and\nInstruct-NeRF2NeRF [19], only shows results with static\navatars. Moreover, they are memory-intensive and slow to\nrender due to their reliance on NeRF, and still rely on a\nlengthy optimization process for every new input.\nHere, AlteredAvatar works with 3D dynamic avatars that\ncan be driven by users and rendered from a wide range of\nnovel views. In addition to text, we show that interesting\nand novel styles can come from a combination of image\nand text style descriptions. Finally, AlteredAvatar focuses\non adapting to novel styles using a small number of steps.\n2.3. Meta-Learning in computer graphics\nInspired by the human ability to quickly adapt to new\nskills by drawing from past experience and knowledge,\nMeta-Learning is a group of methods that focuses on learn-\ning to learn. Meta-Learning algorithms typically take in a\ndistribution of tasks, where each task is a learning problem,\nand produce a quick learner \u2014 a learner that can quickly\nadapt to new tasks from a small number of examples. Note\nthat this is different from work that use hypernetworks [17]\nfor generalisation [2, 3], which learns to directly predict a\nnew set of weights for every new input, rather than learning\na good initialization for fast adaptation. Being a relatively\nnew area of research in deep learning, applications of Meta-\nLearning in computer graphics are less extensive than other\ntypes of learning algorithms. MetaNLR++ [7] and Tancik et\nal. [55] use meta-learning to learn shape priors for fast train-\ning of neural lumigraph representations and coordinate-\nBased neural representations respectively. Metappearance\n[13] applies meta-learning to a wide range of visual appear-\nance reproduction problems such as textures, Bidirectional\nReflectance Distribution Functions (BRDFs), or the entire\nlight transport of a scene. More recently, MetaPortrait [61]\noffers fast personalized talking-head generation by training\ntheir networks under the meta-learning framework.\nThe most related work to ours is MetaStyle [62] that\nadopts meta-learning for 2D image style transfer, achieving\na good trade-off among speed, flexibility, and quality for\narbitrary artistic style transfer. In this work, we apply meta-\nlearning on the problem of 3D avatar stylization. To the best\nof our knowledge, AlteredAvatar is the first approach on\ndynamic 3D avatar stylization that focuses on fast adapta-\ntion to novel styles, generating results in a few update steps\nthat are comparable with those that perform stylization op-\ntimization from scratch.\n3. Method\nGiven a pre-trained photorealisitc dynamic 3D avatar\nM, we aim to manipulate its appearance and geometry such\nthat rendered images of the avatars match the style from\na reference image Istyle or a text description tstyle. Ad-\nditionally, rendered images of the stylized avatar\n\u02c6\nM from\ndifferent views and expressions should be consistent. In\nthis work, we adopt the Instant Avatar architecture [9] for\nits rendering efficiency and ability to represent high-quality\ndynamic avatars. We propose to use pre-trained CLIP mod-\nels [48] as our source of style supervision, instead of VGG\n[54] like other neural stylization work. Finally, we propose\na meta-learning approach to learn an avatar representation\nthat can quickly adapt to an unseen style using a small num-\nber of update steps, while achieving the high fidelity and\nmaintaining the avatar\u2019s ability to be animated to different\nexpressions (see Figure 1-Left).\n3\nCLIP\n\u201calien\u201d\nUpsample\nVOLUMETRIC\nDECODER\nEXPRESSION \nENCODER\nExpression code\nBias Mappers for stylization\nFrozen layers\nUpsample\nUpsample\nUpsample\nDownsample\nID\nENCODER\nUpsample\nDownsample\nDownsample\nMeta-trained for \nfast adaptation\nFigure 2. Overall pipeline for AlteredAvatar. Taking the Instant\nAvatar [9] as the avatar representation, we only use the weights of\nits bias mappers (yellow blocks) during the stylization process, and\nkeep the rest of the network (the remains of the Identity Encoder,\nExpression Encoder, and Volumetric Decoder) frozen. Rendered\nimages of the stylized avatar (from different views and expres-\nsions) are given to a pre-trained CLIP model to compute the styl-\nization loss Lstyle against an input style text description or a target\nstyle image. The stylized avatar can be driven using expressions\nfrom the same or from another avatar.\n3.1. Preliminaries\n3.1.1\nDynamic 3D avatar representation\nWithout loss of generalization, here, we use the Instant\nAvatar representations [9] for avatar stylization.\nIt is a\nlightweight version of Mixture of Volumetric Primitives\n(MVP) [40], a hybrid representation that combines the ex-\npressiveness of volumetric representations with the effi-\nciency of primitive-based rendering for high-quality dy-\nnamic 3D contents. Instant Avatar can represent dynamic\nphotorealistic avatars with very high fidelity, and explicitly\ndisentangle an avatar\u2019s identity from its expression. Instant\nAvatar is essentially a U-net [51] that maps 2D conditioning\ndata to 3D volumetric slabs, which are then ray-marched\nto generate images of the avatar. The network is trained\nusing an image-based loss with posed image data, and is\nfine-tuned to each individual to achieve the highest level of\nauthenticity. Compared to pure NeRF-based models such as\nNerFACE [14], Instant Avatar needs less data, significantly\nless time and memory to train, and less time to render.\nNotably, Instant Avatar contains two encoders: an Iden-\ntity Encoder and Expression Encoder.\nThe Identity En-\ncoder maps 2D position maps (representing a mesh) and\nunrolled 2D texture maps (representing appearance) of a\nspecific person to multi-scale bias maps using blocks of 2D-\nconvolutional bias mappers. These maps are decoded to a\nmodified base-mesh and volumetric slabs, and rendered to\n2D images using ray marching. In summary, given a trained\nInstant Avatar of a specific person M\u03d5 and a deterministic\nray marcher R, we can describe its rendered image I\u03b8 from\na particular view \u03b8, given identity bias maps xi (from the\nIdentity Encoder) and expression code ze (from the Expres-\nsion Encoder) as:\nI\u03b8 = R (M\u03d5(xi, ze); \u03b8) .\n(1)\nSince we are interested in changing the shape and appear-\nance of an avatar to match a desired style, we only manip-\nulate the weights of the Identity Encoder during the styliza-\ntion process, and keep the Decoder and Expression Encoder\nfrozen (see Figure 2). Additionally, we empirically found\nthat using only the blocks of bias mappers within the Iden-\ntity Encoder lead to the same amount of stylization effects\ncompared to using all of the layers. Therefore, we opt to use\nthem instead. From now on, when we use M\u03d5 to describe\nan avatar network, we only focus on the weights of the bias\nmappers in the Identity Encoder.\n3.2. CLIP-guided avatar stylization\nFor avatar stylization, we want to find an avatar\n\u02c6\nM\u03d5 that\nmatches a target text description ttgt or style image Istyle,\nwhile maintaining part of its original identity. Without any\n3D style data, we leverage multi-view rendered images of a\nstylized avatar\n\u02c6\nM\u03d5 to compute the stylization loss. Un-\nlike other neural style transfer work that uses VGG fea-\ntures [54], AlteredAvatar uses CLIP features [48] to com-\npute the stylization loss. Given a rendered image Itgt of\n\u02c6\nM\u03d5 from a random view and with a random expression,\nand a target style description ttgt, a simple stylization loss\ncan be written as the cosine similarity (\u27e8\u00b7, \u00b7\u27e9) between the\nCLIP\u2019s embeddings of the avatar\u2019s rendered image and the\ntext description:\nLCLIP =\nX\nItgt\n[1 \u2212 \u27e8 Ei(Itgt), Et(ttgt)\u27e9]\n(2)\nwhere Ei and Et are the pre-trained CLIP image and text\nencoder, respectively.\n3.2.1\nIdentity loss\nFor avatar stylization, we want to meaningfully change the\ngeometry and the appearance of the avatar to match a de-\nsired style while maintaining the identity of the original\n4\navatar. Therefore, we adopt the identity loss LID to control\nthe amount of identity preservation in the stylized results.\nThis loss essentially compares the structures, represented as\na self-similarity matrix, of the stylized image Itgt with that\nof the original RGB image Isrc. Specifically, for each im-\nage, we extract its CLIP\u2019s N spatial tokens from the deepest\nlayer, and compute a self-similarity matrix S (I) \u2208 RN\u00d7N.\nEach matrix element S (I)i,j is defined by:\nS (I)i,j = 1 \u2212 \u27e8 ni(I), nj(I)\u27e9.\n(3)\nwhere ni \u2208 R768 is the ith token of an image I. Finally, the\nidentity loss LID is defined as the Frobenius norm between\nthe self-similarity matrix of Itgt and Isrc:\nLID = \u2225S(Itgt) \u2212 S(Isrc)\u2225F .\n(4)\nIn summary, the final stylization loss is given by:\nLstyle = \u03bbCLIPLCLIP + \u03bbIDLID\n(5)\n3.3. Fast style adaptation\nTo stylize an avatar M\u03d5 with weights \u03d5 to a particular\nstyle s, we minimize the stylization loss Lstyle to find the\nweights \u02c6\u03d5s of the stylized avatar:\n\u02c6\u03d5s = argmin\n\u03d5s\nLstyle(M\u03d5s, s)\n(6)\nUsually this is done by performing gradient descent with a\ngradient-based optimization method. In this case, we use\nstochastic gradient descent (SGD) for K steps from the ini-\ntialization \u03d5 to find \u02c6\u03d5s, which can be formulated as:\n\u02c6\u03d5K\ns = SGD(Lstyle(M\u03d5s, s), K)\n(7)\nFor every new style s, the stylization optimization needs\nto be done from scratch to find new weights \u02c6\u03d5s, making\nthe product deployment of avatar stylization prohibitively\ncomputationally expensive.\nWith AlteredAvatar, we want to learn a model that can\nquickly adapt to arbitrary novel styles. Specifically, our goal\nis to find a set of suitable weight initialization \u03d5meta, in-\nstead of initializing with \u03d5, that can approach \u02c6\u03d5s for unseen\nnovel styles with a small number of K steps (Figure 1).\nWe leverage the idea of Reptile [44], a model-agnostic\nmeta-learning method, to learn the initialization. Reptile\nuses a bi-level training approach that trains on a distribution\nof tasks (in our case, a dataset of styles Dstyle), and produces\na fast learner that can adapt to a new style within a small\nnumber of update steps.\nAlgorithm 1 describes the outline of the meta-learning\nstage of AlteredAvatar. In particular, in the inner loop, we\nperform stylization with SGD for K steps to find a set of\nweights \u03d5si that minimize Lstyle for a particular style si,\nALGORITHM 1: AlteredAvatar for fast style adaptation\nInput: A pre-trained photorealistic Instant Avatar with\nparameters \u03d5, style dataset Dstyle, inner loop\nlearning rate \u03b1, outer loop learning rate \u03b2, meta\nbatch size M, number of training iterations T,\nnumber of inner loop iterations K.\nOutput: Trained parameters \u03d5meta for fast style adaptation\n1: Initialize \u03d5meta0 with \u03d5.\n2: for each iteration t = 1,..., T do\n3:\nfor each style index i = 1,..., M do\n4:\n\u03d50\nsi = Clone(\u03d5metat\u22121).\n5:\nSample random style si from Dstyle.\n6:\nfor each iteration k = 1,...,K do\n7:\n\u02c6\u03d5k\nsi = \u02c6\u03d5k\u22121\nsi\n\u2212 \u03b1\u2207\u03d5Lstyle(M\u03d5si , si)\n8:\n\u03d5metat = \u03d5metat\u22121 + \u03b2 1\nM\nPM\ni=1(\u03d5K\nsi \u2212 \u03d5metat\u22121)\nsimilar to Equation 7. In the outer loop, we aggregate the\nweights \u03d5K\nsi from style-specific models across many styles\nto update the weights of the meta model \u03d5meta. In prac-\ntice, during training, to update \u03d5meta, we use a meta-batch\nof styles with size M instead of a single style, which has\nbeen shown to lead to faster learning due to variance re-\nduction [44]. After the meta training stage, the meta-learnt\nmodel eventually serves as the initialization for fast adapta-\ntion to novel styles.\nIt is worth highlighting the difference between using\nReptile as described in Algorithm 1 compared to regular\ntraining using SGD to minimize the expected loss across all\nstyles in Dstyle. When K = 1, Algorithm 1 is indeed the\nsame with joint-training on a mixture of all tasks.\nEsi\u2208Dstyle[\u2207\u03d5si Lstyle(M\u03d5si , si)] = \u2207\u03d5si Esi\u2208Dstyle[Lstyle(M\u03d5si , si)]\n(8)\nHowever, when we perform multiple gradient updates (K >\n1), the expected update is no longer equal to update on the\naverage function, since the expected update depends on the\nhigher order derivative of Lstyle(M\u03d5s, si). For each individ-\nual\u2019s avatar network, we train a meta-learnt model to find a\ngood initialization for each specific avatar. As shown in Fig-\nure 2, we only update the bias mappers, and keep the rest of\nthe Instant Avatar network frozen.\nNotably, after the meta training stage, our model does\nnot produce the final set of weights that is \u201doptimal\u201d for any\nspecific style. Instead, our model produces a suitable initial-\nization that can quickly adapt to new styles. This is different\nfrom training hypernetworks [17] to directly predict a new\nset of weights corresponding to every new style in a sin-\ngle, feed-forward pass. These models can add a significant\nnumber of parameters to the stylization model, making it\nnon-trivial to train, and sometimes leads to less competitive\nresults [3,11,53].\n5\nExpressions\nViews\nOriginal\nFigure 3.\nQualitative results by AlteredAvatar. Here we show the stylized results by AlteredAvatar in a variety of identities, styles,\nexpressions and views. Using only text descriptions (from top to bottom: \u201dGouache painting\u201d, \u201dCaricature\u201d, \u201dFrankenstein\u201d,\u201dWerewolf\u201d)\nas our style guidance, we show that AlteredAvatar can generate consistent stylized results across different expressions and views.\n4. Results\n4.1. Implementation Details\n4.1.1\nTraining with CLIP\nWe use pre-trained CLIP models [48], in particular, both\nCLIP-ViT/B-32 and CLIP-ViT/B-16 to extract the image\nstatistics. We find that image and text augmentation play\nan important role in the amount of stylization effect when\nusing CLIP features, as also observed in VQGAN-CLIP\n[12] and PureCLIPNeRF. [33]. We use the same augmen-\ntation strategies from Tex2Live [6] for images and from\nStyleGAN-NADA [15] for texts.\n4.1.2\nTraining details\nWe choose to stylize a diverse set of avatars that spans\na wide range of visual attributes, such as skin color or\nhairstyle, and includes challenging features such as facial\nhair or a beard. We render images at size 1920 \u00d7 1080 of\nthe avatar with random views and expressions. In particu-\nlar, we render 2 side views and 2 frontal views, with small\nrandom perturbations to each view along the azimuth. We\nset \u03bbCLIP = 1.0, \u03bbID = 1.0. For the style dataset Dstyle\nfor the meta-learning training phase, we generate 500 texts\ndescribing the appearance of either real humans (\u201dA person\nwith dark hair and a faint scar above their left eyebrow.\u201d) or\nfictional identities (\u201dWoodland elf with heart-shaped face,\nsoft features, and a button nose and warm brown eyes\u201d),\nas well as makeup and dress-up descriptions (\u201dDark an-\ngel with black wings eyeliner and smoky makeup\u201d). For\nthe test style dataset in section 4.5, we only use art styles\nthat have not been used during the meta-training stage, such\nas \u201dWatercolor painting\u201d or \u201dbronze sculpture\u201d. We in-\nclude a more extensive list of examples from the dataset\nin the supplementary document. We use a meta learning\nrate of 6e-3 and the inner learning rate of 4e-3. For each\nstyle, for the inner loop, we perform stylization by mini-\nmizing Lstyle for 200 steps. For the fast adaptation stage,\nstarting from the learnt initialization, we run stylization for\n200 steps with the learning rate of 8e-3. We train and eval-\nuate our models on an NVIDIA V100 GPU with 32 GB of\nmemory. The meta-training stage takes 16 hours on average\nfor each avatar and once trained, each avatar can adapt to a\n6\nOriginal\nDATID-3D\nAlteredAvatars (Ours)\nFigure 4.\nQualitative comparison for text-guided 3D avatar\nstylization. We show stylized results for two styles \u201dZombie\u201d\n(left) and \u201dNeanderthal\u201d (bottom). Only AlteredAvatar can main-\ntain the avatars\u2019 original identity, expressions, pose and eye gaze\nwell.\nnew style in as quickly as less than two minutes.\n4.2. Comparisons\nIn Section 4.3 and 4.4, we compare to a range of\ntext-guided portrait stylization methods, including 2D\n(StyleGAN-NADA [15], HyperDomainNet [3]) and 3D ap-\nproaches (DATID-3D [30]). We could not compare with\nRODIN [58] and recently published CLIPFace [5] due the\nthe lack of publicly available datasets, pre-trained models\nand code.\nNote that StyleGAN-NADA, HyperDomainNet and\nDATID-3D rely on pre-trained image generative models for\nfaces [28]. For StyleGAN-NADA, we train the model to\nadapt to the new style domains using the author-provided\ncode and settings. For HyperDomainNet and DATID-3D,\nwe use the author-provided code and pre-trained models. In\nthe supplementary material, we also include a comparison\nwith VToonify [60], a video portrait stylization approach.\n4.3. Qualitative comparison\nFigures 1 and 3 show qualitative results by AlteredAvatar\non different avatars, views and expressions. AlteredAvatar\npersistently stylizes and preserves a suitable amount of the\navatars\u2019 original identities, despite the large variety in their\nvisual attributes. More importantly, although AlteredAvatar\nonly uses the style guidance from a single text description,\nour stylized avatars are consistent across different views and\nexpressions.\nIt is worth highlighting each StyleGAN-NADA model is\ntrained to work with a single style, whereas HyperDomain-\nNet is a hypernetwork trained with 1020 texts created by\nsampling CLIP embeddings from a convex hull of the initial\n52 text embeddings. In contrast, AlteredAvatar only needs\na single text or single image for the novel style adaptation\nOriginal\nStyleGAN-NADA\nHyperDomainNet\nAlteredAvatar (Ours)\nFigure 5.\nQualitative comparison for image stylization. We\nshow stylized results for two styles \u201d3D render\u201d (top) and \u201dFer-\nnado Botero painting\u201d (bottom). Only AlteredAvatar can maintain\nthe avatars\u2019 original expressions, pose and eye gaze well.\nstage, after the meta-training stage with 500 styles.\nAs shown in Figure 5, image-based methods (StyleGAN-\nNADA and HyperDomainNet) cannot handle more extreme\nviews, such as left and right side view, due to their reliance\non StyleGAN [28], which was trained mostly on frontal or\nnear-frontal views. They also tend to modify the avatars\u2019\neye gaze and expressions. Since these two methods fail\nto generate stylized results from side views, we only show\nthe closest side views with which we can run the models.\nAdditionally, HyperDomainNet seems to have style mode\n7\n0\n50\n100\n150\n200\n250\n300\n350\n400\n2.9\n3.0\n3.1\n3.2\n3.3\nDirect Optimzation\nTask Mixing\nAlteredAvatars (Ours)\nIteration 50\nIteration 200\nFauvism Painting\nDIRECT \nOPTIMIZATION\nTASK-MIXING\nAlteredAvatar\n(Ours)\nIteration 0\nIteration 50\nIteration 400\nCubist Sculpture\nIteration 0\nIteration 400\nIteration 200\nFigure 6.\nEffects of using different initializations.Comparison between different initializations: the original Instant Avatar weights\n(DIRECT OPTIMIZATION - green line), a learnt initialization from a task-mixing model (TASK-MIXING - blue line), and meta-learnt\ninitialization (AlteredAvatar- orange line). After only 50 iterations, AlteredAvatar already produces results that capture the target style well,\nwhile DIRECT OPTIMIZATION and TASK-MIXING only start to add some style effects to the avatars. After convergence, AlteredAvatar\nalso achieves the lowest Lstyle, which is computed across 25 novel art styles and 5 identities.\ncollapse, and thus generates similar-looking stylized results\neven with different style inputs. AlteredAvatar on the other\nhand can handle a wide range of views including very left\nand right sides, and maintains the avatars\u2019 original expres-\nsions and eye gaze.\nFigure 4 shows a comparison between AlteredAvatar and\nDATID-3D [30], which uses a learnt tri-plane representa-\ntion. Note that DATID-3D can only generate static avatars.\nHowever, even with the learnt 3D representation, DATID-\n3D still struggles with more side views beyond the frontal\nview, similar to StyleGAN-NADA and HyperDomainNet,\nand generates distortion artifacts. DATID-3D also signifi-\ncantly alters the original avatars\u2019 identities and expressions,\nwhich is undesirable for MR/VR applications.\n4.4. Quantitative comparison\n4.4.1\nSemantic score\nHere we estimate the semantic similarity between the styl-\nized avatars and the target styles. For example, we want to\nmeasure how closely an avatar stylized into a zombie style\nmatches the text description \u201da zombie\u201d. Essentially, this\nmeasures how well results from each method can match the\ntarget styles.\nAll methods, including ours, use the style guidance from\nCLIP during training. Therefore, we use the embeddings of\nBLIP [35], instead of CLIP, to estimate the semantic score\nas follows:\nSBLIP = \u27e8 Ei(Itgt), Et(ttgt)\u27e9.\n(9)\nwhere Ei and Et denote BLIP pre-trained image and text en-\nTable 1. Qualitative comparisons for semantic score. We com-\npare the BLIP score (the higher the better) that measures the se-\nmantic similarity scores between rendered images of the stylized\navatars and the target style. The best result is in bold.\nMethods\nBLIP\nStyleGAN-NADA [15]\n0.36 \u00b1 0.03\nHyperDomainNet [3]\n0.33 \u00b1 0.04\nDATID-3D [30]\n0.33 \u00b1 0.04\nAlteredAvatar (Ours)\n0.41 \u00b1 0.03\ncoder, respectively. Similar to CLIP, BLIP is a pre-trained\nmulti-modal model for vision and language, and thus also\nlearns an image-text common embedding space. However,\nBLIP is trained on image and caption pairs, and on addi-\ntional tasks to image-text contrastive learning. We compute\nthe BLIP score using five styles with five different avatars\nrendered from random views and report the mean and stan-\ndard deviation of the BLIP score in Table 1. AlteredAvatar\nachieves the highest semantic score, showing that its styl-\nized results match the target style the closest.\n4.4.2\nUser study\nIn addition to measuring the semantic score between the\nstylized results and the style descriptions in Section 4.4.1,\nwe conduct a user study to measure users\u2019 perception\n8\nStyleGAN-NADA\nHyperDomainNet\nDATID-3D\nMethods\n0\n20\n40\n60\n80\n100\nUsers preference %\nStyle similarity\nIdentity similarity\nTemporal consistency\nExpression similarity\nAlteredAvatar\n(ours)\nFigure 7.\nUser study. We present two videos of novel view\nsynthesis results, one generated by our method (grey) and one by\nanother approach. We ask the participant to select the one that\n(1) Matches the target style better (blue) (2) Has less temporal in-\nconsistencies (orange) (3) Preserves more identity of the original\navatar and (4) Preserves more expressions of the original avatar.\nof the results by AlteredAvatar and three alternative ap-\nproaches (StyleGAN-NADA [15], HyperDomainNet [3],\nand DATID-3D [30]). In particular, we want to measure\nusers\u2019 judgement on 4 separate aspects: 1) Which method\nmatches the input style better; 2) Which method gener-\nates smoother results without temporal inconsistencies; 3)\nWhich method generates results that match the original\navatars\u2019 expression; and 4) Which method generates results\nthat preserves the original avatars\u2019 identities better. For each\nquestion, we ask the participants to compare two videos of\ntwo stylized avatars with the same identity and style, one\ngenerated by AlteredAvatar and the other by one alternative\nmethod. To generate results for the study, we pick five dif-\nferent avatars with three different styles, and render them\nwith different expressions while rotating the views along\nthe azimuth. We collect answers from 16 participants, and\npresent the results in Figure 7. For every comparison as-\npect, we plot the percentage of users choosing our method\n(colored in grey in Figure 7) over the method against which\nwe are comparing. Our method achieves much better re-\nsults in all 4 categories, even on temporal consistency when\ncompared with a 3D approach like DATID-3D.\n4.5. Fast style adaptation\nWe compare our approach for fast style adaptation\nwith two alternatives: DIRECT OPTIMIZATION and TASK-\nMIXING. For AlteredAvatar, we perform stylization to a\nnovel style using our meta-learnt initialization.\nFor DI-\nOriginal\nD = 1\nD = 20\nD = 1020\nInitialization \nfrom scratch\nMeta-learnt \ninitialization\nHyperDomainNet\nAlteredAvatars (Ours)\nFigure 8. Hypernetworks vs. Fast Adaptation with Meta-learnt\nInitialization.\nHyperDomainNet trains a feed-forward styliza-\ntion network for arbitrary stylization.\nAs the size of the data\nset increases (D = 1 (overfit to one style) to D = 1040), the\nquality of the stylized results drops significantly.\nMeanwhile,\nAlteredAvatar learns an initialization that can adapt using a small\nnumber of update steps. Therefore, the quality between the direct\noptimization to approach and AlteredAvatar are comparable, with\nAlteredAvatar using less time for stylization. We show stylized\nresults for two styles \u201dFernado Botero painting\u201d (top) and \u201d3D\ncartoon render\u201d (bottom).\nRECT OPTIMIZATION, we perform stylization starting from\nthe weights of the original photorealistic avatars.\nFor\nTASK-MIXING, we perform stylization using the initializa-\ntion from a joint-training model that is trained to minimize\nLstyle over all Dstyle. This model was trained in the same\nmanner with AlteredAvatar (Algorithm 1) with the same\nstyle dataset Dstyle, but using inner step K = 1, meta batch\nsize M = 1, and training for 100000 iterations. For all three\napproaches, we compute the average loss across 25 novel\nart styles that were not included in the meta-training dataset\nDstyle. We run stylization for 400 steps for all models.\nFigure 6 shows stylizing using our meta-learnt initializa-\ntion leads to smaller Lstyle in a shorter amount of time. Af-\nter 50 steps, DIRECT OPTIMIZATION only starts to apply\nsome style appearance to the avatar, whereas AlteredAvatar\ncan already produce well-stylized results. TASK-MIXING\napplies more style effects to the avatars compared to DI-\nRECT OPTIMIZATION, but still lags behind AlteredAvatar.\nCompared to the baseline TASK-MIXING, AlteredAvatar\nhas learnt an effective initialization that enables fast adapta-\ntion to novel styles.\nWhen it comes to handling novel styles for stylization,\nwe want to highlight the advantage of learning a good ini-\ntialization for fast adaptation, over using a universal one-\nshot style-transfer network. This is the approach taken by\nHyperDomainNet, which is trained on a large dataset of\nstyles and does one-shot generalization to a new style with-\nout any fine tuning. As shown in Figure 8, HyperDomain-\nNet produces very convincing results when trained to over-\n9\nfit to a single domain, or even to 20 domains. However, it\nstruggles to generalize to a potentially arbitrary unlimited\ndomain. Note that both styles \u201dFernado Botero painting\u201d\nand \u201d3D render style\u201d shown here are in the training dataset\nfor the multi-domain version of HyperDomainNet, yet the\nresults are less competitive than its single-domain version\nand AlteredAvatar. In other words, there is a considerable\ngap between the version overfit to a single-domain and the\nmulti-domain version of HyperDomainNet. We argue that\nit is extremely challenging to train a feed-forward model\nthat can both generalize to an arbitrary style and maintain\nthe high fidelity requirement of avatars. Instead, we opt\nto learn a good initialization that allows for fast stylization\nwithin a few update steps, achieving comparable results to\ndirect optimization approaches as shown in Figure 6 and\n8. We believe that with AlteredAvatar, we can strike a good\nbalance between the speed, flexibility to adapt to new styles,\nand quality.\n4.6. Additional experiments\nIdentity preservation\nAlteredAvatar offers control over\nhow much stylization is applied to the avatar with the iden-\ntity loss LID. As shown in Figure 9-a, small values of \u03bbID\nresult in significant changes in the geometry and appearance\nof the avatar. Meanwhile, larger values of \u03bbID preserve more\nof the original facial structure and appearance of the avatars.\nThe amount of desirable stylization greatly depends on per-\nsonal preference and the chosen styles, hence we use \u03bbID as\na tuning knob to control the amount of preservation of the\noriginal identity. In this paper, we use \u03bbID = 1.0.\nStyle mixing\nBy using CLIP features for stylization,\nAlteredAvatar enables users to change the style of avatars\nusing an image, a text description, or both, opening up po-\ntentials for creating new unique avatars. Figure 9-b shows\nnovel styles can be created with AlteredAvatar using both\ntarget style images and texts.\nIn the supplementary document, we include an ablation\nstudy for the Instant Avatar architecture for stylization, and\nvisualizations of the geometry of the stylized avatars.\n5. Conclusion\nWe introduce AlteredAvatar, a meta-learnt avatar rep-\nresentation that can quickly adapt to novel styles using a\nsmaller number of update steps, offering a balance between\nspeed, flexibility, and quality. The stylized avatars can be\ndriven to different expressions, and maintain consistent ap-\npearance across views and expressions.\nOur method presents a few limitations. For example,\nthe method does not model extra geometry for accessories\nthat could be a strong indicator of styles, such as a witch\nwearing a hat or a devil with two horns. As our model uses\nglobal stylization, it does not support styles such as \u201dadd a\nZombie\nBronze statue\nIstyle + tstyle\nIstyle \ntstyle \nOriginal\n \u03bbID  = 0  \n \u03bbID  = 1.  \n \u03bbID  = 5.  \n \u03bbID  = 10.  \nEvil clown\nOriginal\nBrutalist sculpture\na)\nb)\nBeethoven\nFigure 9. Top-a.The strength of stylization effects progressively\nreduce as we increase \u03bbID. Bottom-b. Stylization results using\nonly a style image (second column), style text description (third\ncolumn), and a combination of both (fourth column).\nmustache\u201d or \u201dafro hair style\u201d that only change parts of the\navatar. Additionally, as currently there is no disentangle-\nment between the shading and albedo in the original avatar,\nshading effects might be exaggerated in the stylized results,\nleading to unnatural darkening face regions. We also notice\nsome expression dampening effects in the stylized avatars\ncompared to the original ones, which is partly due to the en-\ntanglement of expression and identity in the Instant Avatar\nrepresentation, as also noticed by Cao et al. [9]. Finally,\nthe quality of our stylized avatars depends on the quality\nof the photorealistic Instant Avatar.\nWe include some\nexamples of the failure cases in the supplementary material.\nReferences\n[1] Rameen Abdal, Hsin-Ying Lee, Peihao Zhu, Menglei Chai,\nAliaksandr Siarohin, Peter Wonka, and Sergey Tulyakov.\n3davatargan: Bridging domains for personalized editable\navatars. arXiv preprint arXiv:2301.02700, 2023. 2\n[2] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and\nAmit H. Bermano. Hyperstyle: Stylegan inversion with hy-\npernetworks for real image editing, 2021. 3\n[3] Aibek Alanov, Vadim Titov, and Dmitry Vetrov. Hyperdo-\nmainnet: Universal domain adaptation for generative adver-\n10\nsarial networks. 2022. 2, 3, 5, 7, 8, 9\n[4] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner.\nClipFace: Text-guided Editing of Textured 3D\nMorphable Models.\nIn ArXiv preprint arXiv:2212.01406,\n2022. 3\n[5] Shivangi Aneja, Justus Thies, Angela Dai, and Matthias\nNie\u00dfner.\nClipFace: Text-guided Editing of Textured 3D\nMorphable Models.\nIn ArXiv preprint arXiv:2212.01406,\n2022. 7\n[6] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kas-\nten, and Tali Dekel. Text2live: Text-driven layered image\nand video editing.\nIn European Conference on Computer\nVision, pages 707\u2013723. Springer, 2022. 6\n[7] Alexander W. Bergman, Petr Kellnhofer, and Gordon Wet-\nzstein. Fast training of neural lumigraph representations us-\ning meta learning. In NeurIPS, 2021. 2, 3\n[8] Menglei Chai Mingming He Dongdong Chen Jing Liao\nCan Wang, Ruixiang Jiang. Nerf-art: Text-driven neural ra-\ndiance fields stylization. 2022. 2, 3\n[9] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz,\nMichael Zollhoefer, Shun-Suke Saito, Stephen Lombardi,\nShih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh, and\nJason Saragih. Authentic volumetric avatars from a phone\nscan. ACM Trans. Graph., 41(4), jul 2022. 2, 3, 4, 10, 14\n[10] Hila Chefer, Sagie Benaim, Roni Paiss, and Lior Wolf.\nImage-based clip-guided essence transfer.\narXiv preprint\narXiv: 2110.12427, 2021. 3\n[11] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-\nSheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via im-\nplicit representation and hypernetwork. pages 1475\u20131484,\nJanuary 2022. 2, 5\n[12] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance.\nIn Computer Vi-\nsion \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Is-\nrael, October 23\u201327, 2022, Proceedings, Part XXXVII, page\n88\u2013105, Berlin, Heidelberg, 2022. Springer-Verlag. 6\n[13] Michael Fischer and Tobias Ritschel. Metappearance: Meta-\nlearning for visual appearance reproduction.\nACM Trans.\nGraph., 41(6), nov 2022. 2, 3\n[14] Guy Gafni, Justus Thies, Michael Zollh\u00a8ofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8649\u20138658, June 2021. 4\n[15] Rinon Gal, Or Patashnik, Haggai Maron, Gal Chechik, and\nDaniel Cohen-Or. Stylegan-nada: Clip-guided domain adap-\ntation of image generators, 2021. 3, 6, 7, 8, 9\n[16] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge.\nImage style transfer using convolutional neural networks. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2414\u20132423, 2016. 2\n[17] David Ha, Andrew M. Dai, and Quoc V. Le. Hypernetworks.\nIn International Conference on Learning Representations,\n2017. 2, 3, 5\n[18] Fangzhou Han, Shuquan Ye, Mingming He, Menglei Chai,\nand Jing Liao.\nExemplar-based 3d portrait stylization.\nIEEE Transactions on Visualization and Computer Graph-\nics, 29(2):1371\u20131383, 2023. 2\n[19] Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander\nHolynski, and Angjoo Kanazawa. Instruct-nerf2nerf: Edit-\ning 3d scenes with instructions. 2023. 2, 3\n[20] Lukas H\u00a8ollein, Justin Johnson, and Matthias Nie\u00dfner.\nStylemesh: Style transfer for indoor 3d scene reconstruc-\ntions. CoRR, abs/2112.01530, 2021. 2\n[21] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. ACM Trans-\nactions on Graphics (TOG), 41(4):1\u201319, 2022. 2, 3\n[22] Haozhi Huang, Hao Wang, Wenhan Luo, Lin Ma, Wen-\nhao Jiang, Xiaolong Zhu, Zhifeng Li, and Wei Liu. Real-\ntime neural style transfer for videos.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7044\u20137052, 2017. 2\n[23] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh\nSingh, and Ming-Hsuan Yang.\nLearning to stylize novel\nviews. pages 13869\u201313878, October 2021. 2\n[24] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proceed-\nings of the IEEE International Conference on Computer Vi-\nsion, Oct 2017. 2\n[25] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. 2022. 3\n[26] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf\non a diet: Semantically consistent few-shot view synthe-\nsis. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 5885\u20135894, October\n2021. 2\n[27] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nEuropean Conference on Computer Vision, 2016. 2\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2019. 3, 7, 15\n[29] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Popa Tiberiu. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models.\nDecember\n2022. 3\n[30] Gwanghyun Kim and Se Young Chun. Datid-3d: Diversity-\npreserved domain adaptation using text-to-image diffusion\nfor 3d generative model, 2022. 2, 3, 7, 8, 9\n[31] Sunnie S. Y. Kim, Nicholas Kolkin, Jason Salavon, and Gre-\ngory Shakhnarovich. Deformable style transfer. In Proceed-\nings of the European Conference on Computer Vision, 2020.\n3, 14\n[32] Han-Hung Lee and Angel X. Chang. Understanding pure\nclip guidance for voxel grid nerf models, 2022. 3\n[33] Han-Hung Lee and Angel X. Chang. Understanding pure\nclip guidance for voxel grid nerf models, 2022. 6\n11\n[34] Chuan Li and Michael Wand. Combining markov random\nfields and convolutional neural networks for image synthesis.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2016. 2\n[35] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 8\n[36] Xueting Li, Sifei Liu, Jan Kautz, and Ming-Hsuan Yang.\nLearning linear transformations for fast image and video\nstyle transfer. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, June 2019. 2\n[37] Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu,\nand Ming-Hsuan Yang. Universal style transfer via feature\ntransforms. In Advances in Neural Information Processing\nSystems, 2017. 2\n[38] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2023. 3\n[39] Xiao-Chang Liu, Yong-Liang Yang, and Peter Hall. Learning\nto warp for style transfer. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 3701\u20133710, 2021. 3, 14\n[40] Stephen Lombardi,\nTomas Simon,\nGabriel Schwartz,\nMichael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mix-\nture of volumetric primitives for efficient neural rendering.\nACM Transactions on Graphics (ToG), 40(4):1\u201313, 2021. 2,\n4\n[41] Yifang Men, Yuan Yao, Miaomiao Cui, Zhouhui Lian, and\nXuansong Xie. Dct-net: Domain-calibrated translation for\nportrait stylization. volume 41, pages 1\u20139. ACM New York,\nNY, USA, 2022. 2\n[42] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. arXiv preprint arXiv:2112.03221, 2021. 3\n[43] Thu Nguyen-Phuoc, Feng Liu, and Lei Xiao. Snerf: Stylized\nneural implicit representations for 3d scenes. ACM Trans.\nGraph., 41(4), jul 2022. 2\n[44] Alex Nichol, Joshua Achiam, and John Schulman. On first-\norder meta-learning algorithms.\nCoRR, abs/1803.02999,\n2018. 5\n[45] OpenAI. ChatGPT, 2023. 16\n[46] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski.\nStyleclip: Text-driven manipulation\nof stylegan imagery. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV), pages\n2085\u20132094, October 2021. 3\n[47] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv,\n2022. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In Marina Meila\nand Tong Zhang, editors, ICML, volume 139 of Proceedings\nof Machine Learning Research, pages 8748\u20138763, 2021. 2,\n3, 4, 6\n[49] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation.\nIn Marina Meila and\nTong Zhang, editors, Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Pro-\nceedings of Machine Learning Research, pages 8821\u20138831.\nPMLR, 18\u201324 Jul 2021. 3\n[50] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021. 3\n[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn Nassir Navab, Joachim Hornegger, William M. Wells, and\nAlejandro F. Frangi, editors, Medical Image Computing and\nComputer-Assisted Intervention \u2013 MICCAI 2015, pages 234\u2013\n241, Cham, 2015. Springer International Publishing. 2, 4\n[52] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Burcu Karagol Ayan, S. Sara Mahdavi,\nRapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J\nFleet, and Mohammad Norouzi. Photorealistic text-to-image\ndiffusion models with deep language understanding, 2022. 3\n[53] Falong Shen, Shuicheng Yan, and Gang Zeng. Neural style\ntransfer via meta networks. In 2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8061\u2013\n8069, 2018. 2, 5\n[54] Karen Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition.\nIn\nYoshua Bengio and Yann LeCun, editors, 3rd International\nConference on Learning Representations, ICLR 2015, San\nDiego, CA, USA, May 7-9, 2015, Conference Track Proceed-\nings, 2015. 3, 4\n[55] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\nSchmidt, Pratul P. Srinivasan, Jonathan T. Barron, and Ren\nNg. Learned initializations for optimizing coordinate-based\nneural representations. In CVPR, 2021. 2, 3\n[56] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky.\nInstance normalization: The missing ingredient for fast styl-\nization. CoRR, abs/1607.08022, 2016. 2\n[57] Yael Vinker, Ehsan Pajouheshgar, Jessica Y. Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir.\nClipasso:\nSemantically-aware object sketching, 2022. 2\n[58] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin\nBao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang\nWen, Qifeng Chen, and Baining Guo. Rodin: A generative\nmodel for sculpting 3d digital avatars using diffusion. 2022.\n3, 7\n[59] Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change\nLoy. Pastiche master: Exemplar-based high-resolution por-\ntrait style transfer. In CVPR, 2022. 2, 15\n[60] Shuai Yang, Liming Jiang, Ziwei Liu, and Chen Change Loy.\nVtoonify: Controllable high-resolution portrait video style\ntransfer. ACM Transactions on Graphics (TOG), 41(6):1\u201315,\n2022. 2, 7, 15\n12\n[61] Bowen Zhang, Chenyang Qi, Pan Zhang, Bo Zhang, Hsiang-\nTao Wu, Dong Chen, Qifeng Chen, Yong Wang, and Fang\nWen. Metaportrait: Identity-preserving talking head gener-\nation with fast personalized adaptation. arXiv:2212.08062,\n2022. 2, 3\n[62] Chi Zhang, Yixin Zhu, and Song-Chun Zhu.\nMetastyle:\nThree-way trade-off among speed, flexibility, and quality in\nneural style transfer. In Proceedings of the AAAI Conference\non Artificial Intelligence (AAAI), 2019. 2, 3\n[63] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan Zhang,\nCheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan Xu, and\nJingyi Yu. Dreamface: Progressive generation of animatable\n3d faces under text guidance, 2023. 3\n[64] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.\nMind the gap: Domain gap control for single shot domain\nadaptation for generative adversarial networks, 2021. 3\n13\nCLIP\n\u201calien\u201d\nUpsample\nVOLUMETRIC\nDECODER\nEXPRESSION \nENCODER\nExpression code\nBias Mappers for stylization\nFrozen layers\nUpsample\nUpsample\nUpsample\nDownsample\nID\nENCODER\nUpsample\nDownsample\nDownsample\nMeta-trained for \nfast adaptation\nW x H x Cin\nLeaky ReLU\nTransposed Conv \n(4,2)\n+\n2W x 2H x Cout\nUp\nBias maps from ID-Encoder\n2W x 2H x Cout\nFigure 10.\nOverall pipeline for AlteredAvatar.\nTaking the\nInstant Avatar [9] as the avatar representation, we only use the\nweights of its bias mappers (yellow blocks) during the styliza-\ntion process, and keep the rest of the network (the remains of the\nIdentity Encoder, Expression Encoder, and Volumetric Decoder)\nfrozen. Rendered images of the stylized avatar (from different\nviews and expressions) are given to a pre-trained CLIP model to\ncompute the stylization loss Lstyle against an input style text de-\nscription or a target style image. The stylized avatar can be driven\nusing expressions from the same or from another avatar.\nA. Where to fine-tune\nOne key question when we adopt a learnt avatar repre-\nsentation like Instant Avatar [9] is which part of the net-\nwork is suitable for high-quality stylization that is consis-\ntent across views and expressions. While a straightforward\nchoice is to use the weights of the Identity Encoder, we find\nusing all the layers in this encoder redundant. Particularly,\nwe find that using only the blocks of bias mappers of the\nIdentity Encoder (see Figure 10) is enough to achieve a\ngood level of stylization effects. Changing fewer parame-\nters also implicitly acts as a regularizer to the avatar net-\nwork, which helps maintain the overall facial structure and\navoid overly-distorted geometry. Notably, with more pa-\nUsing the entire\n Identity Encoder\nOriginal\nUsing only the\nBias Mappers\nEvil Clown\nBrutalist Sculpture\nFigure 11. Network architecture and stylization effects. Using\nonly the bias mappers in the Identity Encoder achieves the same\nlevel of stylization effect (top) and avoids excessive geometry dis-\ntortion during stylization (bottom).\nrameters to update when using the entire Identity Encoder,\nwe have to reduce the learning rate significantly from 8e-\n3 to 1e-3 to avoid training collapse, lengthening the styl-\nization process. As shown in Figure 11, changing only the\nbias mappers, we can achieve comparable stylization results\nwith using all the layers in the Identity Encoder and avoid\nexcessive geometry changes.\nB. Geometry stylization\nInstead of just changing the texture appearance to match\na target style, AlteredAvatar modifies both the avatar\u2019s ap-\npearance and 3D geometry, which has been shown to be an\nimportant factor of style [31,39]. Here, we visualize the ge-\nometry of the stylized avatars by ray-marching the avatar\u2019s\nunderlying MVP representations. As shown in Figure 12,\nthe geometry of the stylized avatars changes to accommo-\ndate the target style compared to the original photorealistic\navatars. This ranges from small changes in the nose of the\navatar for the style \u201dCaricature\u201d (Figure 12-top), to larger\nchanges in the overall geometry of the avatar for the style\n\u201dCubist sculpture\u201d(Figure 12-bottom).\n14\nw/o contrastive \nloss\nw/o contrastive \npatch loss\nw/o contrastive \nloss\nw/o contrastive \npatch loss\nCaricature\nCubist sculpture\nOriginal\nStylized\nOriginal\nStylized\nFigure 12.\nGeometry changes. AlteredAvatar stylizes both the\nappearance and geometry of the avatar to better capture the target\nstyle.\nC. Failure case\nFigure 13 shows examples for some current limitations\nof our method.\nCurrently AlteredAvatar struggles with\nmodelling extra geometry accessories, which can be an im-\nportant factor for styles. Additionally, the stylized results\nheavily depend on the quality of the original photorealistic\navatar. As shown in Figure 13-bottom, a blurry artifact in\nthe original avatar results in the blurry artifact in the stylized\navatar. We also notice some expression dampening effects\nin the stylized avatar compared to the original ones (see Fig-\nure 13-bottom).\nD. Comparison with video stylization\nFigure 14 shows results for video stylization, Vtoonify\n[60].\nVToonify shows very competitive results with\nAlteredAvatar in terms of view, eye gaze and expression\nconsistency. However, it still cannot handle more extreme\nleft or right views, and modifies the avatar\u2019s eye gaze in\nmore side views. Thus, here we only show the closest side\nviews with which we can run VToonify. More importantly,\nA princess wearing a tiara\nA pirate wearing an eye patch\nFigure 13.\nFailure cases. Top: AlteredAvatar currently cannot\nhandle accessories such as an eye patch or a tiara, which can be\na great indicator of style. Bottom: Blurriness in the teeth region\n(highlighted in the red box) is transferred from the original to the\nstylized avatar. The facial expression is also slightly dampened in\nthe stylized avatar.\nfor every style, it requires a dataset of a few hundred refer-\nence images, and an additional process to create a paired\ntraining dataset by combining StyleGAN [28] and Dual-\nStyleGAN [59], making it non-trivial to adopt novel styles.\nThese datasets are well-curated and even fine-tuned to each\ngender. On the other hand, AlteredAvatar only uses a text\ndescription as its style guidance. The comparison therefore\nhighlights the difference in the performance between one-\nshot avatar stylization using a single text description, and\nstylization with a well-curated dataset.\n15\nOriginal\nVToonify\nAlteredAvatar (Ours)\nFigure 14.\nQualitative comparison for video stylization. We\nshow stylized results for style \u201d3D cartoon\u201d. Both methods pro-\nduce comparable results, although VToonify is trained on a dataset\nof a few hundred images for every style, and AlteredAvataronly\nuses the style guidance from a single image or text description.\nMoreover, VToonify makes slight modifications to the avatars\u2019 eye\ngaze, and cannot handle more side views.\nE. Dataset\nFor the metra-training stage, we generate 500 text de-\nscriptions using ChatGPT [45]. This includes descriptions\nfor real humans across ages, gender and races. Addition-\nally, we also use descriptions for fiction characters such as\nwitches or fairies. Finally, we use descriptions for makeup\nand dress-up styles. A sample of the training dataset is listed\nbelow. Upon acceptance, we will share the full dataset. De-\nscriptions of real humans\n\u2022 A person with vitiligo, displaying unique patches of\npigmentation on their face, with a confident, empow-\nered presence.\n\u2022 A person with albinism, with pale skin, light hair, and\nstriking, vibrant eyes.\n\u2022 An individual with facial scars or birthmarks, showing\nstrength and resilience in their confident demeanor.\n\u2022 A person with a facial piercing, such as a septum ring,\neyebrow piercing, or lip stud, adding an edgy, uncon-\nventional touch to their appearance.\n\u2022 A person with a full, bushy beard, groomed with care\nand reflecting a trendy, hipster style.\nDescriptions of fictional entities\n\u2022 Wicked witch with green skin, a crooked nose, and\nwarts.\n\u2022 Queen of hearts with red heart-shaped lips and black\nheart under the eye.\n\u2022 Cyborg with metallic features and glowing eyes.\n\u2022 Haunted doll with cracked porcelain skin and blacked-\nout eyes.\n\u2022 Fairy tale princess with blond hair and rosy cheeks.\nMakeup and Dress-up styles\n\u2022 A mystical unicorn with pastel rainbow hair, a glitter\nhorn, and sparkling pink and purple makeup.\n\u2022 An otherworldly alien with silver body paint, holo-\ngraphic eyes, and a metallic lip.\n\u2022 A dark sorceress with black lipstick, smoky purple\neyeshadow, and a jeweled headpiece.\n\u2022 A steampunk robot with metallic gears, goggles, and\nmetallic silver or bronze makeup.\n\u2022 A colorful peacock with blue and green feathers, bold\neyeshadow, and bright pink lips.\nAll results shown in the paper are on avatars stylized using\nnovel styles that were not used in the training dataset. In\nparticular, for the test styles used for Figure 7 in the main\npaper, we use 25 novel art styles listed below, which the\nmodel did not see during training.\n\u2022 Impressionist portrait painting\n\u2022 Black and white pen and ink portrait drawing\n\u2022 Lino portrait print\n\u2022 Realistic line art portrait drawing\n\u2022 Medieval portrait painting\n\u2022 Bronze statue\n\u2022 3D render\n\u2022 Vintage anime chibi character\n\u2022 Graffiti portrait\n\u2022 Degas chalk portrait painting\n\u2022 Pointillism portrait drawing\n\u2022 Character design by herg\u00b4e\n\u2022 Cartoon\n16\n\u2022 Oil painting\n\u2022 Pencil portrait drawing\n\u2022 Gouache portrait painting\n\u2022 Acrylic portrait painting\n\u2022 Watercolor painting\n\u2022 Pop art poster\n\u2022 Fauvism portrait painting\n\u2022 Mosaic\n\u2022 Caricature\n\u2022 8-bit pixel art\n\u2022 Miro portrait painting\n\u2022 Abstract white cubist sculpture\n17\n"
  },
  {
    "title": "PaLI-X: On Scaling up a Multilingual Vision and Language Model",
    "link": "https://arxiv.org/pdf/2305.18565.pdf",
    "upvote": "1",
    "text": "PaLI-X: On Scaling up a Multilingual Vision and\nLanguage Model\nXi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu,\nCarlos Riquelme Ruiz, Sebastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri,\nMostafa Dehghani, Daniel Salz, Mario Lucic, Michael Tschannen, Arsha Nagrani,\nHexiang Hu, Mandar Joshi, Bo Pang, Ceslee Montgomery, Paulina Pietrzyk,\nMarvin Ritter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic, Austin Waters,\nGang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee,\nAndreas Peter Steiner, Yang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhosseini,\nAnelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut\nGoogle Research\npali-communications@google.com\nAbstract\nWe present the training recipe and results of scaling up PaLI-X, a multilingual\nvision and language model, both in terms of size of the components and the\nbreadth of its training task mixture. Our model achieves new levels of performance\non a wide-range of varied and complex tasks, including multiple image-based\ncaptioning and question-answering tasks, image-based document understanding\nand few-shot (in-context) learning, as well as object detection, video question\nanswering, and video captioning. PaLI-X advances the state-of-the-art on most\nvision-and-language benchmarks considered (25+ of them). Finally, we observe\nemerging capabilities, such as complex counting and multilingual object detection,\ntasks that are not explicitly in the training mix.\n1\nIntroduction\nThe success of scaling language models [1, 2, 3, 4] makes it appealing to similarly scale Vision-\nLanguage (V&L) models, and investigate the improvements, capabilities, and emergent properties of\nsuch models. Inspired by the work in [5], we present PaLI-X, a multilingual vision and language\nmodel with reusable scaled-up components, consisting of a pretrained large-capacity visual encoder\n(using [6] as the starting point) and a pretrained language-only encoder-decoder (using [7] as the\nstarting point), further trained at-scale on a vision-and-language data mixture using a combination of\nself-supervision and full-supervision signals.\nOne clear pattern that emerges from the combination of results from PaLI [5] and the work we\npresent in this paper is that scaling both V&L components together brings increases in performance\nacross a wide range of tasks. We show this by comparing against the same benchmarks used for\nPaLI (Fig. 1, Left), and also against new benchmarks for which the new capabilities of PaLI-X are\nevaluated (e.g., ChartQA, AI2D, DocVQA, InfographicVQA, as well as video understanding tasks).\nWe observe that scaling leads to large improvements over the results of the PaLI model, and also over\nspecialized large-scale models that are trained specifically to solve certain tasks, often with the help\nof (often much larger) text-only LLMs [8]. In particular, we find that increasing both the effective\ncapacity of the vision component (which [9] does more unilaterally) and of the language component\nPreprint. Under review.\narXiv:2305.18565v1  [cs.CV]  29 May 2023\nFigure 1: [Left] Comparing PaLI-X against PaLI on image-captioning and VQA benchmarks. [Right]\nThe Pareto frontier between few-shot and fine-tuned performance, comparing PaLI-X with PaLI [5],\nFlamingo [10], and Kosmos-1 [11].\n(which [10] also does unilaterally) is beneficial; the new PaLI-X model provides more balanced\nparameter allocation than any other prior work (roughly 40%-60% split of the total capacity).\nAside from confirming the impact of scale, the original contribution of PaLI-X consists in leveraging\nthe mixture-of-objectives proposed in [7] for vision-and-language modeling, and showing that it\nresults in a model that improves both state-of-the-art results and the Pareto frontier for fine-tuning\nand few-shot configurations (Fig. 1, Right).\nWe also observe emergent properties based on PaLI-X\u2019s results compared to previous models with\nsimilar architecture but smaller sizes. For instance, we report drastically improved performance on\nthe counting ability (See Table 1 and Appendix B), both for the plain variety (count all instances of a\nclass) and the complex variety (count instances based on a natural language description), that are\nnot attributable to training design1. Additionally, we present qualitative insights into the model\u2019s\nperformance (Appendix A), with an emphasis on multilingual transfer learning such as the ability\nto detect objects using non-English labels (Fig. 2), and the ability to switch between the language\nof text present in the image (e.g., English) and the language of the generated image caption (e.g.,\nRomanian).\nOur technical contributions include the following:\n1. We scale a Vision-Language model to achieve outstanding performance on a wide variety\nof benchmarks. We observe that scaling both the Vision & Language components is\nadvantageous and report that performance remains unsaturated at this scale.\n2. We show that training such a model with a mixture of objectives that combines prefix-\ncompletion and masked-token completion improves the Pareto frontier for fine-tuning vs\nfew-shot performance at this scale.\n3. We show that a high-capacity vision encoder (ViT-22B) can be effectively co-trained for\nimage classification and OCR label classification2 to achieve significant improvements on\nV&L tasks for which the understanding of text-within-image is crucial.\n4. Overall, PaLI-X improves SoTA results via fine-tuning on 15+ benchmarks, and we show\nthat it is the first of its kind to simultaneously adapt via multitask fine-tuning to a diverse set\nof benchmarks without significant performance degradation.\n2\nRelated Work\nSimilar to large language models such as GPT4 [12] and PaLM [1], the benefit of scale has also\nbeen observed in recent vision and vision-language models. Flamingo [10] used a frozen language\n1Plain counting is usually achievable via good object detection, while complex counting requires a fine-\ngrained understanding of the alignment between language-based specifications and visually-based occurrences.\n2We use OCR tokens produced by the GCP Vision API over the training images as targets.\n2\ncomponent and demonstrated the benefit of scaling up this part up to 70B parameters on the few-shot\nmultimodal capabilities, while the vision encoder is fixed with 435M parameters. GIT [9], on the\nother hand, explored scaling of the vision component up to 4.8B parameter, with a 300M parameter\nlanguage decoder. PaLI [5] explored jointly scaling the vision and language component, to 4B and\n17B, respectively, and showed that scaling both components benefits a wide range of vision-language\ntasks. All these models took advantage of vision and language unimodal pretrained models as\nbackbones to start multimodal training. Recently, on the vision model side, a vision transformer with\n22B parameter has been introduced [6]. In this work, we make use of a ViT-22B model specifically\ntuned for OCR capability to explore scaling Vision-Language models to even larger parameter regime.\nAs first shown in [13], large language models are sometimes able to solve new unseen tasks at\ninference as long as a few examples \u2013or shots\u2013 are provided as inputs. This is usually referred to as\nin-context learning [14]. Follow-up work proposed improved ways to split and prompt the shots, such\nas Chain of Thought [15] or Least-to-Most prompting [16]. So far, the vast majority of this work has\nbeen done in the context of language inputs [17]. In this work, we explore multimodal in-context\nlearning with pairs of images and captions. Our work is aligned in spirit to Flamingo [10] that uses\ninterleaved image text pairs in the same web page and in-context tuning [18] during pre-training. We\nfirst group the image-text pairs by url and split each group to a \u201cshots\u201d set and a \u201ctarget\u201d set. Then\nwe use the few examples in the \u201cshots\u201d set as input features to predict the examples in the target set.\nBesides solving vision-language tasks in multiple domains, recent VLMs also attempted solving\nthese tasks at once instead of fine-tuning on each individual benchmark. Unified-IO [19] performed\nmultitask fine-tuning and reported solid results across 16 benchmarks. Spotlight [20] reported that\ninside the UI domain, multitask fine-tuning can achieve a performance close to task-specific fine-\ntuning. In this work, we show that PaLI-X can be simultaneously fine-tuned with a diverse set of\nbenchmarks in multiple domains without performance degradation.\n3\nModel\n3.1\nArchitecture\nThe PaLI-X model architecture follows the encoder-decoder architecture: image(s) are processed by\na ViT encoder, with the resulting visual embeddings fed to an encoder-decoder backbone, along with\nembeddings from additional text input (e.g., question / prefix / prompt). More details are provided in\nAppendix A.\nVisual component\nOur visual backbone is scaled to 22B parameters, as introduced by [6], the\nlargest dense ViT model to date. To equip the model with a variety of complex vision-language tasks,\nwe specifically focus on its OCR capabilities. To that end, we incorporate an OCR-based pretraining\nas follows: images from the WebLI dataset [5] are annotated with OCR-text detected by GCP Vision\nAPI; the encoder is then further pre-trained with a mixture of the original JFT-based classification\ntask and a new OCR-based classification task (whether or not a given token occurred in the image\naccording to OCR results). See Appendix A for additional details on the visual component. PaLI-X\nis designed to take n >= 1 images as inputs (for few-shot and video understanding), with tasks\ninvolving a single image as the n = 1 case. For n > 1, each image is independently processed by the\nViT module, and the patch-level embeddings coming out of ViT are flattened and concatenated to\nform the visual input (See Appendix A). Note that similar to the single-image case, there is no pooling\nover the spatial dimension before visual embeddings are aggregated over the temporal dimension.\nThat is, for an n-frame input with k-patches per frame, the resulting visual input has n \u2217 k tokens.\nOverall model\nThe encoder-decoder backbone is initialized from a variant of the UL2 [7] encoder-\ndecoder model that uses 32B parameters. The architecture of this variant has 50 layers in both\nencoder and decoder (up from 32 layers in [7]), and is pretrained on a mixture of text data similar\nto [7]. The visual embeddings, after going through a projection layer, are concatenated with the token\nembeddings of the text input, and fed to the encoder-decoder backbone. Most of the pretraining tasks\n(with the exception of the masked image token task) predict text-only output from this multimodal\ninput. The text input to the model typically consists of a prompt that marks what type of task it is\n(e.g., \"Generate caption in \u27e8lang\u27e9\" for captioning tasks) and encode necessary textual input for the\ntask (e.g., \"Answer in \u27e8lang\u27e9: {question}\" for VQA tasks). For tasks that need OCR capabilities,\nwe experiment with either relying solely on the text-encoding capabilities of the vision encoder, or\noptionally including tokens extracted by an upstream OCR system fed as additional text inputs.\n3\nFew-shot formulation\nIn the few-shot setting, for a given target example the model receives a\nnumber of \u201clabeled\u201d examples (in the form of additional \u27e8image, text\u27e9 pairs) that we refer to as\nshots/exemplars. The hypothesis is that information contained in these exemplars provides the model\nwith useful context to generate predictions for the target example. Formally, the input with N shots\nis a sequence (t1, . . . , tN, tT , i1, . . . , iN, iT ), where t1 : tN and i1 : iN are texts and images for the\nN shots, and tT and iT are the text (prompt) and image for the target example. PaLI-X processes\nthis input as follows: all images, including the target one, are first independently processed by the\nvisual encoder, and the resulting patch-level embeddings are flattened and concatenated to form the\nvisual input sequence. After going through a projection layer, they are concatenated with the text\nembeddings to form the multimodal input sequence used by the encoder. We implement additional\noptimizations including distributing the exemplars between the encoder and the decoder, and an\nattention re-weighting mechanism (see Appendix B).\n3.2\nPretraining Data and Mixture\nThe main pretraining data for our model is based on WebLI [5], consisting of roughly one billion\nimages with alt-texts from the web and OCR annotations (using the GCP Vision API), covering over\n100 languages. In addition to WebLI \u27e8image, text\u27e9 pairs, we introduce here Episodic WebLI data,\nwhere each episode corresponds to a set of such pairs. We aim to have each episode contain loosely\nrelated images (i.e., they are clustered according to their URL field), so as to encourage attention\namong examples in an \u201cepisode\u201d. We find this new dataset (with 75M episodes and around 400M\nimages in total) important for developing the few-shot capabilities of the model.\nThe pretraining mixture consists of the following data and objectives: (i) span corruption on text-\nonly data (15% of tokens); (ii) split-captioning on WebLI alt-text data [21, 5]; (iii) captioning\non CC3M [22] on native and translated alt-text data (over the same 35 languages covered by\nXM3600 [23]); (iv) split-ocr [24] on WebLI OCR annotations; (v) visual-question-answering objec-\ntive over \u27e8image, question, answer\u27e9 pairs generated using the VQ2A method [25] over the CC3M\ntraining split, over native and translated text (same 35 language pairs); (vi) visual-question-generation\nobjective, using the same pairs as above; (vii) visual-question-answering objective over \u27e8image,\nquestion, answer\u27e9 pairs using the Object-Aware method [26] (English only); (viii) captioning on\nEpisodic WebLI examples (target alt-text predicted from the remaining alt-text and images); (ix)\nvisual-question-answering on 4-pair examples (resembling Episodic WebLI and using VQ2A-CC3M\npairs), with the answer target conditioned on the other pairs of \u27e8image, question, answer\u27e9 data. (x)\npix2struct objective, introduced in [27], targeting page layout and structure using screenshot images\npaired with DOM-tree representations of html pages. (xi) Captioning on short video data, using\nthe VTP data [10] (using four frames per video). (xii) object-detection objective on WebLI data,\nwhereby an OWL-ViT model [28] (L/14) is used to annotate WebLI images, resulting in hundreds of\npseudo object labels and bounding boxes per image. (xiii) image-token prediction objective, whereby\nwe tokenize WebLI images (256\u00d7256 resolution) using a ViT-VQGAN [29] model with patch size\n16\u00d716 (256 tokens per image); this objective is framed as a 2D masked-token task (i.e., fill-in the\nmissing grid pieces, with the corresponding image pixels also masked). Note that the image-token\nprediction objective is added mainly as a condition to check whether it adversarially impacts the\nperformance on language-output tasks; our ablation experiments show that is does not.\n3.3\nTraining Stages\nOur model is trained in two stages. In stage 1, the visual encoder (after mixed-objective training)\nis kept frozen, while the rest of the parameters are trained on a total of 2.2B examples at the base\nresolution 224\u00d7224 (native to ViT-22B), using the entire mixture. In stage 2, it continues training\nusing only the OCR-related objectives (pix2struct and split-ocr) plus the object detection objective;\nthis is done in several substages, during which image resolution is gradually increased to 448\u00d7448,\n672\u00d7672 and finally 756\u00d7756.\n4\n4\nExperiments\n4.1\nImage Captioning and Visual Question Answering\nOur results demonstrate that the larger capacity in PaLI-X scales well in both its vision and language\ncomponents, and it is particularly beneficial for more challenging scene-text and document under-\nstanding tasks. Our model outperforms the SOTA on diverse vision-language tasks, with significant\nmargins in some cases.\nBenchmark datasets\nThe Image Captioning and VQA benchmarks used for evaluation is sum-\nmarized in Appendix B, including 6 Image Captioning benchmarks (COCO (Karpathy split [30]),\nNoCaps [31], TextCaps [32], VizWiz-Cap [33], Screen2Words [34], Widget-Cap [35]) and 13\nVQA benchmarks (VQAv2 [36], OKVQA [37], TallyQA [38], TextVQA [39], VizWiz-VQA [40],\nSTVQA [41], OCRVQA [42], InfographicVQA [43], DocVQA [44], AI2D [45] ChartQA [46],\nOVEN [47], InfoSeek [48]). These tasks span a wide range of visual domains, from natural images,\nillustrations to documents and user interfaces (UIs). We also include results of multilingual captioning\non XM3600 in Appendix B.\n4.1.1\nPer-task fine-tuning results\nExperimental setup\nWe fine-tune PaLI-X with frozen ViT-22B; the learning rate follows a linear\ndecay from initial value 1e-4 for all fine-tuning experiments. See Appendix B for more details.\nCOCO\nNoCaps\nVQAv2\nOKVQA\nTallyQA\nModel\nKarp.-test\nval\ntest\ntest-dev\ntest-std\nval\nsimple\ncomplex\nGIT2 [9] (5.1B)\n145.0\n126.9\n124.8\n81.74\n81.92\n-\n-\n-\nFlamingo [10] (80B)\n138.1\n-\n-\n82.0\n82.1\n57.8\u2217\n-\n-\nBEiT-3 [49] (1.9B)\n147.6\n-\n-\n84.2\n84.0\n-\n-\n-\nPaLM-E [50] (562B)\n138.7\n-\n-\n80.0\n-\n66.1\n-\n-\nMoVie [51]\n-\n-\n-\n69.26\n-\n-\n74.9\n56.8\nPaLI [5](17B)\n149.1\n127.0\n124.4\n84.3\n84.3\n64.5\n81.7\n70.9\nPaLI-X (55B)\n149.2\n126.3\n124.3\n86.0\n86.1\n66.1\n86.0\n75.6\nTable 1: Results on COCO Captions (Karpathy split), NoCaps, VQAv2 [36], OKVQA [37], and\nTallyQA [38] with end-to-end modeling without OCR pipeline input (\u201csimple\u201d and \u201ccomplex\u201d are\ntest subsplits).\nText VizWiz Text VizWiz ST OCR Info Doc AI2D Chart Screen2 Widget OVEN Info\nModel\nCaps\nCap\nVQA VQA VQA VQA VQA VQA\nQA\nWords\nCap\nSeek\nwith OCR pipeline input\nSoTA\n160.4 124.7 73.67\n73.3\n79.9 67.5 47.4 84.7 38.5 45.5\n-\n-\n-\n-\n[5]\n[5]\n[52]\n[5]\n[5]\n[53]\n[54]\n[54]\n[45]\n[46]\n-\n-\n-\n-\nPaLI-X\n163.7 125.7 80.78\n74.6\n84.5 77.3 54.8 86.8 81.4 72.3\n-\n-\n-\n-\nwithout OCR pipeline input\nSoTA\n145.0 120.8 67.27\n70.7\n75.8 71.3 40.0 76.6 42.1 70.5\n109.4\n141.8\n20.0\n17.7\n[9]\n[9]\n[9]\n[5]\n[9]\n[27]\n[27]\n[27]\n[27]\n[8]\n[27]\n[20]\n[47]\n[48]\nPaLI-X\n147.0 122.7 71.44\n70.9\n79.9 75.0 49.2 80.0 81.2 70.9\n127.9\n153.0\n23.1\n21.8\nTable 2: Results on benchmarks more focused on text understanding capabilities. For OVEN [47] &\nInfoSeek [48], we follow the proposed 224\u00d7224 resolution settings for fair comparison.\nFirst, we present benchmarks results for the condition where external OCR systems are not used\n(Table 1, see Appendix B for an extended table.). The trend is that PaLI-X matches or improves SoTA\nresults on these benchmarks, with a particularly significant improvement on the TallyQA benchmark\nover MoVie [51] (specialized counting model), at +11.1 for simple counting questions (e.g., \u201chow\nmany giraffes\u201d) and +18.8 for complex counting questions (e.g., \u201chow many giraffes are drinking\nwater\u201d); there are significant improvements over PaLI [5] as well, indicating that scale plays an\nimportant role in the ability of such models to perform counting tasks. We additionally note the\nstate-of-the-art result on VQAv2 at 86.1 accuracy, achieved with an open-vocabulary generative\n5\napproach, and the performance on OKVQA at 66.1 accuracy, matching the much-larger PaLM-E [50]\nmodel performance.\nNext, we examine text-heavy V&L benchmarks, for which upstream OCR systems can be used to\nimprove performance. As shown in Table 2, PaLI-X improves SoTA for all Captioning and VQA\nbenchmarks across the board, either without or with additional OCR input (using GCP Vision API).\nFor instance, a significant jump of +42.9 points is observed on AI2D3, a multiple-choice benchmark\nwhere choices are provided along with each question. Being able to have the text choices as input\nbenefits PaLI-X compared with the previous SoTA Pix2Struct [27] which has to render the text on the\nimage, but this does not explain all the improvements. In a question-only configuration (no answer\nchoice present), PaLI-X achieves 46.3 on AI2D, more than 4 points higher than Pix2Struct\u2019s result.\nIn general, having access to OCR texts extracted by an external OCR pipeline boosts performance.\nStill, for several benchmarks (e.g., AI2D, ChartQA, OCRVQA and Widget-Cap), PaLI-X\u2019s end-to-end\nperformance when using its intrinsic OCR capability is close to that leveraging additional OCR input.\nA common feature for these benchmarks is that they have well-oriented text \u2013 diagrams, charts, book\ncovers or user interfaces, with reasonably large font size at 756\u00d7756 resolution. For tasks involving\nscene text in natural images (TextCaps, TextVQA, STVQA) or very high density of small texts\n(DocVQA, InfoVQA), results still highlight clear benefits when utilizing an external OCR model.\n4.1.2\nMultitask Fine-tuning\nWe simultaneously fine-tune and evaluate the pretrained checkpoints on multiple benchmarks belong-\ning to the same category. We deduplicated every training set over the test sets of every task in the\nmixture to prevent the leakage of any test-set examples into the mixed training set. This is useful as it\nleads to a single fine-tuned model that performs all the tasks, rather than having to fine-tune each task\nseparately. We performed such multitask fine-tuning on all Image Captioning benchmarks and most\nVQA benchmarks, respectively.\nTable 3 shows the multitask fine-tuning result for captioning tasks. The performance on COCO is\nslightly decreased in the multitask setting, which is likely a result of this task needing longer training\nto converge. For Screen2Words, having the smallest train and dev/test sets could be responsible for\nthe performance fluctuation. Notably, VizWiz-Cap and Widget-Cap shows improved performance\nfrom multitask fine-tuning. Overall, the average performance decreases by 1.4 points (0.2 excluding\nScreen2Words) with multitask fine-tuning, while offering the clear advantage of having a single\ncheckpoint to perform all these tasks. Appendix B shows similar results for VQA tasks. We consider\nthis outcome a positive result that establishes the on-par performance between multitask fine-tuning\nand single-task fine-tuning for diverse benchmarks, in contrast with previous work which argued a\ngap between single-task and multitask fine-tuning [19], or demonstrated little gap over benchmarks\nfrom the same domain [20].\nCOCO\nNoCaps\nText\nVizWiz Screen2 Widget\nAvg.\nMethod\nCaps\nCap\nWords\nCap\nSplit\nKarp.-test\nval\nval\ntest-dev\ntest\ntest\n-\nSOTA (Single-task FT)\n149.1\n127.0\n148.6\n119.4\n109.4\n136.7\nPaLI-X Single-task FT\n149.2\n126.3\n150.8\n123.1\n127.9\n153.2\n-\nPaLI-X Multitask FT\n147.3\n125.6\n154.6\n124.2\n120.6\n153.7\n-\nMultitask (+/-)\n-1.9\n-0.7\n+3.8\n+1.1\n-7.3\u2217\n+0.5\n-1.4 (-0.2 w/o \u201c*\u201d)\nTable 3: Scores from multitask fine-tuning compared with those from single-task fine-tuning for\nImage Captioning. Validation or test-dev set numbers are reported for some tasks.\n4.1.3\nFew-shot Evaluation\nWe fine-tuned the PaLI-X model on a mixture of few-shot tasks. The few-shot mixture contains\nEpisodic mixtures, (Non-Episodic) Webli and (Non-Episodic) CC3M data. Note that all of these\ndatasets were already used in previous stages of training, but with lower mixture proportions. During\n3As with all the other benchmarks, our training examples are carefully deduped to exclude images occurring\nin these benchmarks, including AI2D. Such results, therefore, are not attributable to train-test data leakage.\n6\npre-training, we only use up to 4 shots, with both encoder and decoder shots (see Appendix B). For\nfine-tuning, we use up to 8 encoder shots and do not use decoder shots.\nWe evaluate the few-shot performance on COCO caption (Karpathy test split [30]), and XM3600 [23]\ndatasets. For each task, we first create a \u201cshots pool\u201d with 256 examples that are randomly selected\nfrom the task\u2019s training set. As the XM3600 benchmark does not come with a training set, we use\nGoogle Translate API to enhance the COCO Karpathy training set with captions in the 35 languages\nrepresented in XM3600. Then, for each test data point, we randomly pick N shots from the pool as\nthe actual few-shot examples. Following [10], we also evaluate on 2 text-only shots settings where\nonly the textual part of 2 randomly sampled few-shot examples are used.\nTable 4 reports the few-shot captioning performance on English and multilingual captioning, as\nwell as few-shot VQA performance on VQAv2. PaLI-X achieves SOTA few-shot results on COCO\nwith both 4 shots and 32 shots; it outperforms previous SOTA by +4.4 CIDEr points for 4-shot,\nsuggesting a strong ability to efficiently gather hints from few examples. We also report few-shot\nCIDEr scores averaged over 35 languages using XM3600, demonstrating PaLI-X\u2019s multilingual\ncapabilities. Meanwhile, although PaLI-X also performs decently on VQAv2, the gap behind the\nSoTA Flamingo model [10] (which freezes the language backbone) may be the result of losing some\nof the few-shot text-only QA capability by fine-tuning the language backbone, which supports the\nhypothesis regarding the tension between few-shot and fine-tuning abilities.\nCOCO Captions\nXM3600 Cap. (35-lang avg.)\nVQAv2\nMethod\n4 shots\n32 shots\n4 shots\n32 shots\n4 shots\n32 shots\nPrev. SoTA [10]\n103.2\n113.8\nN/A (53.6 w/ fine-tune [5])\n63.1\n67.6\nPaLI-X\n107.6\n114.5\n45.1\n47.1\n56.9\n57.1\nTable 4: Few-shot performance of the PaLI-X model (multilingual captioning for XM3600).\n4.2\nVideo Captioning and Question Answering\nWe fine-tune and evaluate the PaLI-X model on 4 video captioning (MSR-VTT [55], VATEX [56],\nActivityNet Captions [57], Spoken Moments in Time [58]) and 3 video question answering bench-\nmarks (NExT-QA [59], MSR-VTT-QA [60], ActivityNet-QA [61]). A brief description of each\nbenchmark and clarifications on their usage are provided in Appendix C.\nExperimental setup\nWe fine-tune our model (with base resolution 224\u00d7224) for each task sepa-\nrately, use the validation split for early stopping, and report performance on the test split. We use a\nlearning rate of 10\u22124 for all tasks, and do not adapt any hyperparameters for specific tasks. Frames\nare sampled using a fixed temporal stride for each dataset (determined based on the video length\ndistribution in that dataset such that the product of the number of frames and stride is larger than the\ntotal number of frames for half of the videos), and we experimented with including up to 8 or 16\nframes per video. We did not include pooling over the spatial dimension; embeddings for 16\u00d716\npatches per frame are provided as visual input to the multimodal encoder.\nResults\nWe report CIDEr score for the video captioning tasks. Video QA tasks are treated as\nopen-ended generation tasks; we report full-string accuracy (for MSR-VTT-QA and ActivityNet-QA)\nand WUPS metrics (NExT-QA) in [65, 59]. As shown in Table 5, the 16-frames version has an edge\nover the 8-frame version, sometimes with a significant margin (e.g., close to a 6 point increase in\nCIDEr score for ActivityNet-Captions). More importantly, while PaLI-X pretraining was dominated\nby image-text tasks, we were able to achieve new SOTA performance for 5 out of 7 tasks4, and\nperformed very close to prior SOTA on MSR-VTT-QA (47.1 vs 47.4).\n4.3\nImage classification\nTo test image classification capabilities we fine-tuned PaLI-X and models from [5] on Ima-\ngeNet [66] and evaluated the resulting model on ImageNet-REAL [67] and out-of-distribution\n4As noted in Table 5, current SOTA on NExT-QA for the open-ended QA task was achieved by Flamingo\n32-shot, which had outperformed prior fine-tuning SOTA. To the best of our knowledge, PaLI-X performance\non this task does outperform existing published fine-tuning performances, with the caveat that we do not have\ninformation on what Flamingo fine-tuning would have achieved on this task.\n7\nMSR-VTT\nActivity-Net\nVATEX\nSMIT\nNExT-QA\nMethod\nCap. [55]\nQA [60]\nCap. [57]\nQA [61]\nCap. [56]\nCap. [58]\nQA [59]\nPrior SOTA\n75.9\n47.4\n52.5\n44.7\n94.0\u2020\n28.1\u2021\n33.5\u00a7\nGIT2 [9]\nFlamingo [10]\nPDVC [62]\nVINDLU [63]\nGIT2 [9]\nMV-GPT [64]\nFlamingo 32shot [10]\nPaLI-X (8fr)\n74.6\n46.9\n49.0\n48.4\n66.0\n42.5\n37.0\nPaLI-X (16fr)\n76.8\n47.1\n54.9\n49.4\n69.3\n43.5\n38.3\nTable 5: Results for Video Captioning and Video-QA using 8 frames (8fr) or 16 frames (16fr). \u2020GIT2\nuses Self-Critical Sequence Training to directly optimize the CIDEr metric for VATEX. \u2021SMIT has\nnot been used for video captioning before, we apply MV-GPT [64] and report results on the test set.\n\u00a7Numbers were obtained using 32-shot; since Flamingo 32-shot outperforms fine-tuning SOTA on\nthis open-ended QA task, they did not conduct further fine-tuning experiments for this task.\ndatasets: ImageNet-R [68], ImageNet-A [69], ImageNet-Sketch [70], ImageNet-v2 [71]. We used\nthe model from the first training stage (at resolution 224) and the one from the last training stage (at\nresolution 756). We used the same training hyperparameters for all of runs (selected without any\nhyperparameter tuning; mode details in Appendix D).\nThe results can be seen in Table 6. We compare the results to generative model with open vocab \u2013\nGIT2 [9] (using 384 image resolution), which is the current SOTA for full fine-tuning on ImageNet.\nPaLI-X achieves SOTA results for generative models on Imagenet, and other datasets. We also\nperformed zero-shot evaluation for PaLI-X and the results can be found in Appendix D.\nModel (resolution)\nINet [66]\nREAL [67]\nINet-R [68]\nINet-A [69]\nINet-Sketch [70]\nINet-v2 [71]\nGIT2 [9] (384)\n89.22\n-\n-\n-\n-\n-\nPaLI-17B [5] (224)\n86.13\n88.84\n78.21\n50.00\n71.21\n78.91\nPaLI-X (224)\n88.22\n90.36\n77.66\n55.97\n72.56\n81.42\nPaLI-X (756)\n89.19\n90.98\n80.06\n72.57\n73.37\n83.66\nTable 6: Classification accuracy (top-1) fine-tuned on Imagenet [66].\nprompt: detect knife and left half of watermelon and \nright half of watermelon and right hand and left \nhand\noutput: 298 284 722 493 left half of watermelon and \n298 483 721 681 right half of watermelon and 302 188 \n779 260 knife and 202 265 535 493 left hand and 201 \n485 546 723 right hand\nright hand\nleft hand\nleft half of \nwatermelon\nknife\nright half of \nwatermelon\nprompt: detect window and cat and bike and door and tree and this is \nparadise\noutput: 259 487 423 550 window and 259 839 424 904 window and 706 625 \n787 778 bike and 550 839 759 942 door and 256 363 424 430 window \nand 258 241 422 305 window and 254 715 422 774 window and 254 115 \n422 177 window and 590 120 653 811 this is paradise and 549 131 588 \n161 vent\nthis is paradise\nbike\ndoor\nwindow\nwindow\nwindow\nwindow\nwindow\nwindow\nvent\nprompt 1: detect bowls\noutput: 193 56 827 980 bowls\nprompt 2: detect bowl\noutput: 213 55 594 431 bowl \nand 192 429 536 772 bowl and \n511 322 822 630 bowl and 467 \n630 826 979 bowl\nbowl\nbowl\nbowl\nbowl\nprompt (KO): detect \ub7a9\ud0d1 and \ucef5 and \uace0\uc591\uc774\nouputt: 489 447 779 679 \uace0\uc591\uc774 and 676 659 \n962 818 \ucef5 and 349 0 964 478 \ub7a9\ud0d1\nprompt (HI): detect \u0932\u0948\u092a\u091f\u0949\u092a and \u0915\u092a and \u01d2\u092c\u0932\u094d\u0932\u0940\noutput: 489 447 779 672 \u01d2\u092c\u0932\u094d\u0932\u0940 and 679 656 \n965 822 \u0915\u092a and 352 0 966 479 \u0932\u0948\u092a\u091f\u0949\u092a\n \u0932\u0948\u092a\u091f\u0949\u092a\n \u01d2\u092c\u0932\u094d\u0932\u0940\n \u0915\u092a\nbowls\n \ub7a9\ud0d1\n \uace0\uc591\uc774\n \ucef5\nCredits: Watermelon/Cat; Sarah Pflug (burst), Bowls; ariesandrea (flickr), Wall; Matthew Henry (burst)\nFigure 2: Examples demonstrating multilingual, OCR and other capabilities transferred to detection.\n4.4\nObject Detection\nObject detection can be easily formulated in our model as shown in pix2seq [72], The dataset mix\nused for pre-training is presented in Sec. 3; detection data was included up to and including the stage\nusing resolution 672, after which a separate detection-specific model was fine-tuned on detection\ndata. Before detection-specific tuning, LVIS [73] & COCO labels were removed from all detection\ntraining datasets, allowing zero-shot evaluation on LVIS.\nBounding box mean AP on LVIS is shown in Table 7, including zero-shot performance; the detection-\ntuned model reaches an AP of 31 in general, and 31.4 on rare classes, and about 12 for both in\nzero-shot. Performance on rare classes was on par with performance on common classes, a difficult\n8\nfeat traditionally accomplished by complicated sampling schedules and augmentations. In our set up,\nit is directly enabled by PaLI-X\u2019s diverse training mix. This could likely be further improved with\ninvestment in fine-tuning e.g. using noise-augmentation methods from pix2seq [72], or a further stage\nof high-resolution, LVIS only training. Qualitatively, we observe emergence of many interesting\nphenomena enabled by co-training with non-detection tasks; for example, multilingual detection,\nOCR bounding boxes and longer descriptions, none of which are included in detection training, are\noften handled well by PaLI-X. Additional results and information can be found in Appendix E.3.\nLVIS AP\nLVIS APRare\nViLD [74] (tuned on non-rare LVIS)\n29.3\n26.3\nRegion-CLIP [75] (tuned on non-rare LVIS)\n32.3\n22.0\nOwLViT-L/16 [28] (tuned on non-rare LVIS)\n34.7\n25.6\nOwLViT-L/16 [28] (with Object365 and VG datasets)\n34.6\n31.2\nPaLI-X (Zeroshot)\n12.36\n12.16\nPaLI-X (Detection-tuned)\n30.64\n31.42\nTable 7: PaLI-X object detection results on LVIS. The diverse pre-training mix enables parity\nperformance between LVIS rare and common classes. Other related approaches are shown for context,\nbut are not directly comparable.\n5\nModel Fairness, Biases, and Other Potential Issues\nLarge models, if left unchecked, have the potential to inflict harm on society \u2013 such as amplifying\nbiases [76, 77, 78, 79], causing disparities [78, 80, 81], or encoding narrow cultural perspectives\n[82, 83]. Hence, evaluating PaLI-X for such potential issues is important. We focus our RAI\nevaluation on three parts: (1) harmful associations, such as toxicity and profanity, (2) demographic\nparity in the model\u2019s output, such as encoding societal stereotypes/biases, and (3) performance\ndisparity across subgroups. This breakdown follows earlier works in the literature, such as [84].\nToxicity / profanity.\nWe estimate the level of toxicity and profanity in the generated captions,\nincluding when disaggregated across subgroups. We use the FairFace dataset [85] that comprises of\nimages of people with ground-truth attributes: gender presentation, age and ethnicity. We generate\ncaptions and use the Perspective API [86] (threshold > 0.8) to measure toxicity and profanity. Table\n8 summarizes the results; we observe a low level of toxicity/profanity across all slices. Tables 9 and\n10 provide a detailed breakdown of toxicity/profanity results for all subgroups in FairFace dataset.\nIn Tables 11 and 12, we report similar results in the MIAP [87] dataset, disaggregated by perceived\ngender and age.\nGender\nEthnicity\nAge\nLowest\nHighest\nLowest\nMedian\nHighest\nLowest\nMedian\nHighest\nOverall\nToxicity\n0.14%\n0.19%\n0.00%\n0.13%\n0.39%\n0.00%\n0.17%\n0.31%\n0.01%\nProfanity\n0.00%\n0.02%\n0.00%\n0.00%\n0.05%\n0.00%\n0.00%\n0.03%\n0.00%\nTable 8: Average toxicity/profanity in the captions generated by PaLI-X on FairFace dataset.\nBias / Demographic Parity.\nWe estimate the level of demographic parity (DP) [88] in PaLI-X\nwith respect to gender and occupation. To estimate the level of demographic parity (DP) in the\nmodel\u2019s output, we feed an image into PaLI-X with the chosen occupation title as a prefix and\nrecord the average log-perplexity score of the captions generated by the model. To ensure that any\nobserved parity would likely reflect unintended biases in the model itself as opposed to the evaluation\ndataset, we use CelebA [89] that contains celebrity images with gender presentation annotation. Our\nassumption is that many occupations reflecting societal stereotypes, such as secretaries and plumbers,\nare quite rare in the CelebA dataset so disparities in output may reflect what is encoded in the model\nitself. The list of occupations is compiled based on [90] and the US job statistics report in [91].\nFigure 3 (TOP) summarizes the overall results. First, PaLI-X tends to assign a higher log-perplexity\nscore to women than men across most occupations; i.e. men are predicted to be more likely to hold\nsuch occupations. Second, PaLI-X assigns a higher likelihood for a woman to be (\u2018secretary\u2019 &\n9\nEthnicity\nToxicity\nProfanity\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n< 0.2\n0.2 \u2212 0.8\n> 0.8\nMiddle Eastern\n64.24%\n35.76%\n0.00%\n94.87%\n5.13%\n0.00%\nBlack\n59.47%\n40.40%\n0.13%\n92.67%\n7.33%\n0.00%\nIndian\n63.86%\n36.07%\n0.07%\n94.39%\n5.61%\n0.00%\nHispanic\n61.09%\n38.79%\n0.12%\n94.45%\n5.55%\n0.00%\nWhite\n62.45%\n37.16%\n0.39%\n92.85%\n7.10%\n0.05%\nSoutheast Asian\n63.18%\n36.61%\n0.21%\n93.57%\n6.43%\n0.00%\nEast Asian\n63.15%\n36.72%\n0.13%\n91.55%\n8.45%\n0.00%\nTable 9: Distribution of the predicted toxicity/profanity for the captions generated by PaLI-X on\nFairFace dataset disaggregated by ethnicity.\nAge\nToxicity\nProfanity\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n< 19\n58.78%\n40.00%\n0.22%\n89.71%\n10.29%\n0.00%\n20 - 29\n63.01%\n36.86%\n0.12%\n93.24%\n6.73%\n0.03%\n30 - 39\n63.13%\n36.70%\n0.17%\n95.41%\n4.59%\n0.00%\n40 - 49\n63.62%\n36.31%\n0.07%\n95.27%\n4.73%\n0.00%\n50 - 59\n65.87%\n33.88%\n0.25%\n96.48%\n3.52%\n0.00%\n60 - 69\n65.31%\n34.38%\n0.31%\n95.95%\n4.05%\n0.00%\n> 70\n66.10%\n33.90%\n0.00%\n92.37%\n7.63%\n0.00%\nTable 10: Distribution of the predicted toxicity/profanity for the captions generated by PaLI-X on\nFairFace dataset disaggregated by age.\nPerceived Gender\nToxicity\nProfanity\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n< 0.2\n0.2 \u2212 0.8\n> 0.8\nPredominantly Feminine\n53.98%\n45.93%\n0.09%\n90.55%\n9.39%\n0.07%\nPredominantly Masculine\n70.76%\n29.17%\n0.06%\n94.97%\n5.01%\n0.01%\nTable 11: Distribution of the predicted toxicity/profanity for the captions generated by PaLI-X on\nMIAP dataset disaggregated by perceived gender.\nAge Bucket\nToxicity\nProfanity\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n< 0.2\n0.2 \u2212 0.8\n> 0.8\n0-2 yrs\n28.00%\n72.00%\n0.00%\n69.90%\n30.10%\n0.00%\n3-19 yrs\n49.96%\n49.96%\n0.07%\n91.46%\n8.54%\n0.00%\n20-59 yrs\n66.27%\n33.68%\n0.05%\n93.42%\n6.55%\n0.03%\n> 60 yrs\n65.46%\n34.54%\n0.00%\n96.39%\n3.61%\n0.00%\nTable 12: Distribution of the predicted toxicity/profanity for the captions generated by PaLI-X on\nMIAP dataset disaggregated by age bucket.\n\u2018actor\u2019) and a higher likelihood for a man to be (\u2018guard\u2019 & \u2018plumber\u2019) at the 95% confidence level.\nFigure 3 (BOTTOM) displays the corresponding correlations between perceived gender presentation\nand occupations within the WebLI dataset, where we use the Pearson correlation coefficient by\ntreating each label as a binary random variable and noting that for binary random variables, zero\ncorrelation implies full independence. All absolute correlation coefficients in the data are < 0.2 with\n99% of them being < 0.1.\nPerformance Disparity.\nWe present here an evaluation of how well PaLI-X performs across\ndifferent subgroups using the MIAP [87] dataset. For images containing exactly a single individual,\nwe query PaLI-X with the question: \u201cIs there a person in this image?\u201d and evaluate the accuracy of\nits response. Note that there are no false positives in this evaluation. Table 13 summarizes the results.\nWe observe that PaLI-X maintains a high accuracy across all subgroups.\n10\njudge\nmanager\nlawyer\neducator\nengineer\ntechnician\nprofessor\ncrew\npolice officer\nmechanic\nflight attendant\nscientist\ncomputer scientist\ncoach\nbaker\nbodybuilder\nsport\nnurse\nresearcher\ncustomer\ndentist\nsecretary\nartist\nfashion\nstudent\narchitect\npilot\nauthor\njournalist\nactor\nastronaut\nbiologist\nchef\ndancer\nelectrician\nfarmer\nfirefighter\nminister\nhousekeeper\nlibrarian\nmagician\nparamedic\nphotographer\ndoctor\nsinger\nsoldier\nbartender\ncarpenter\ndispatcher\ndriver\nchemist\nguard\nhairdresser\nhygienist\ninvestigator\njanitor\nplumber\nreceptionist\ntailor\ntherapist\nveterinarian\nworker\nspokesperson\nmale\nfemale\n0.1\n0.2\nFigure 3: TOP: Level of demographic parity (DP) in PaLI-X\u2019s output for CelebA images between\nwomen and men. Values close to zero indicate absence of bias.\nBOTTOM: Absolute Pearson\ncorrelation coefficients between gender presentation and occupations in WebLI.\nSkin Tone\n1 [2]\n2 [871]\n3 [3008]\n4 [522]\n5 [184]\n6 [85]\n7 [54]\n8 [49]\n9 [6]\n10 [1]\n0.00%\n0.11%\n0.47%\n1.53%\n0.54%\n1.18%\n0.00%\n0.00%\n0.00%\n0.00%\nGender\nPredominantly Feminine [2437]\nPredominantly Masculine [3544]\n0.53%\n0.85%\nAge Bucket\n0-2 yrs [17]\n3-19 yrs [568]\n20-59 yrs [4925]\n> 60 yrs [247]\n0.00%\n0.00%\n0.77%\n0.81%\nTable 13: Detection error rate for \u201cperson\u201d in PaLI-X using the subset of the MIAP dataset [87]\nthat contain exactly a single individual in the image. PaLI-X maintains a low error rate across all\nsubgroups. Skin tone follows the Monk Skin Tone Scale [92]. Numbers inside square brackets\ncorrespond to the size of each bucket.\nLimitations.\nThe analysis carried out in this section is necessarily limited, since fairness is a\nsocietal concept that cannot be reduced to statistical metrics. We expect RAI evaluations to evolve\nover time as new issues are detected and reported in the literature and additional datasets become\navailable. Statistical analysis is only a single step and does not substitute for studying the broad and\ndelayed impact of deployed models.\nIn addition, we rely in some parts on automated tools for inferring attributes, which are not perfectly\naccurate and can lead to a broad categorization of people that misidentifies real identities. We do not\nsupport the creation or application of classifiers for sensitive attributes, such as gender or ethnicity,\nbased on visual indicators and encourage readers to delve into the comprehensive work outlining their\npotential risks, such as [93, 94], for further insight. Also, while we use perceived gender presentation\nin our analysis that is provided by the data (i.e. in CelebA and FairFace), we acknowledge that people\nmay express their gendered identities in numerous other ways.\nIn our evaluation, toxicity is predicted based on the generated captions only. However, without\nknowing the context of the image, this can introduce false positives.\n6\nConclusions\nIn this work we draw more insights from further scaling vision and language models. We show that\nthe scaling and the improved training recipe results in a model that substantially outperforms previous\nstate-of-the-art models, leads to emergent behaviors and identifies further margins for improvements.\nIn particular, we report that the model achieves significant improvements at document, chart, and\ninfographic understanding, captioning, visual question answering, counting, and performs well on\nfew-shot (in-context) captioning, video captioning and question-answering, and object detection.\n11\nAcknowledgements\nWe would like to thank Sarah Laszlo, Kathy Meier-Hellstern, Caroline Pantofaru, Susanna Ricco,\nCandice Schumann, Ken Burke, Simon Wang, Rachel Hornung, Yichang Chen, Utsav Prabhu, Abhijit\nOgale, Kristina Toutanova, Weicheng Kuo, Jihyung Kil, Xiangning Chen, Liang Chen, Rich Lee,\nElizabeth Adkison, James Cockerille, Eric Ni, Erica Moreira, Victor Gomes, Jeremiah Harmsen,\nClaire Cui, Slav Petrov, Tania Bedrax-Weiss, Joelle Barral, Tom Duerig, Paul Natsev, Fernando\nPereira, Jeff Dean, and Zoubin Ghahramani for helpful discussions, feedback, and support.\n12\nA\nAdditional Model Details and Examples\nA.1\nPaLI-X Architecture Illustration\nFigure 4: Visual input for videos: each frame is independently processed by ViT; patch embeddings\nare flattened and concatenated together to form the visual representation. (The example input image\nis in the public domain).\nA.2\nTuning ViT-22B for better OCR capabilities\nThe vision encoder\u2019s ability to understand text is crucial to several downstream tasks and general\nusability. JFT-based pre-training is insufficient to cover this, and so we tuned ViT-22B on WebLI-\nOCR data. In order to stay true to the original discriminative classification-based objective used for\nViT-22B, we turn OCR into a bag-of-words prediction task. OCR texts are tokenized using the mT5\ntokenizer [95] across all languages, and the model is trained to predict whether or not a given token\noccurs in an image. This is treated as multilabel classification, with an expanded classification head.\nIn the ablation study shown in Table 22, we confirm that this this extra tuning step indeed has a\nsignificant improvement on Scene-Text understanding capabilities, demonstrated by the performance\non ST-VQA and TextVQA. Meanwhile, the performance on regular VQA tasks such as those in the\nVQAv2 benchmark also improves.\nA.3\nIllustrative PaLI-X Examples\nTable 14 shows representative examples of PaLI-X, illustrating improved abilities related to counting\n(both of the simple and complex variety), in context text-reading capabilities, and spatial awareness.\n13\nImage Credit: Wikimedia Commons [CC BY-SA 4.0]\nQ: how many coins are there?\nA: 12\nQ: how many one-dollar coins are there?\nA: 2\nImage Credit: ChrisGoldNY (flickr) [CC BY-NC 2.0]\nQ: what is written inside the box?\nA: dr. strangelove\u2019s secret uses of uranus\nQ: what is written on the top-left corner of the page?\nA: the bomb and dr. strangelove\nQ: what is written on the top-right corner of the page?\nA: doctor doomsday\nPublic Domain Image\nCaptioning:\na movie poster for rebecca starring laurence olivier and joan fontaine.\nQ: who directed this movie?\nA: alfred hitchcock\nQ: who produced this movie?\nA: david o. seznick\nTable 14: Examples of counting, text reading capabilities with context and spatial awareness. Results\nare generated by the multi-task-finetuned models using the model\u2019s inherent OCR capabilities (i.e.,\nwithout the use of an external OCR system).\nB\nAdditional results: Image Captioning and VQA\nB.1\nInformation of Downstream Image Benchmarks\nTable 15 summarizes the Image Captioning and VQA benchmarks. For benchmarks modeled only end-\nto-end without OCR pipeline input (Table 1 and Table 16), fine-tuning is performed with resolution\n672\u00d7672. For Scene-Text and Document Understanding tasks presented in Table 2, fine-tuning is\nperformed with resolution 756\u00d7756.\nB.2\nExtended Tables of Image Benchmarks\nAn extended table of results on some Image Benchmarks is shown as Table 16.\n14\nBenchmark\nVisual Domain\nDescription\nMetric\nCOCO Captions\nNatural Images\nCaptioning of natural images\nCIDEr\nNoCaps\nCaptioning of natural images\nCIDEr\nTextCaps\nCaptioning of natural images containing text\nCIDEr\nVizWiz-Cap\nCaptioning of photos taken by people who are blind\nCIDEr\nVQAv2\nVQA on natural images\nVQA accu.\nOKVQA\nVQA on natural images requiring outside knowledge\nVQA accu.\nTextVQA\nVQA on natural images containing text\nVQA accu.\nVizWiz-QA\nVQA on photos taken by people who are blind\nVQA accu.\nST-VQA\nVQA on natural images containing text\nANLS\nTallyQA\nVQA with counting questions\nEM\nOVEN\nVQA on natural images for visual entity recognition\nEM\nInfoSeek\nVQA on natural images for visual info-seeking questions\nRelaxed EM\nOCR-VQA\nIllustrations\nVQA on images of book covers\nEM\nChartQA\nVQA on images of charts\nRA\nAI2D\nVQA on images of scientific diagrams\nEM\nDocVQA\nDocuments\nVQA on images of scanned documents\nANLS\nInfographicsVQA\nVQA on images of infographics\nANLS\nScreen2Words\nUIs\nCaptioning a UI screen to describe functionality\nCIDEr\nWidget Captioning\nCaptioning a UI component on a screen\nCIDEr\nTable 15: Summary of Image Captioning and VQA benchmarks used for evaluating PaLI-X\nCOCO\nNoCaps\nVQAv2\nOKVQA\nTallyQA\nModel\nKarp.-test\nval\ntest\ntest-dev\ntest-std\nval\nsimple\ncomplex\nSimVLM\n143.3\n112.2\n110.3\n80.03\n80.34\n-\n-\n-\nCoCa (2.1B)\n143.6\n122.4\n120.6\n82.3\n82.3\n-\n-\n-\nGIT (0.7B)\n144.8\n125.5\n123.4\n78.56\n78.81\n-\n-\n-\nGIT2 (5.1B)\n145.0\n126.9\n124.8\n81.74\n81.92\n-\n-\n-\nOFA (0.9B)\n145.3\n-\n-\n82.0\n82.0\n-\n-\n-\nFlamingo (80B)\n138.1\n-\n-\n82.0\n82.1\n57.8\u2217\n-\n-\nBEiT-3 (1.9B)\n147.6\n-\n-\n84.2\n84.0\n-\n-\n-\nPaLM-E (562B)\n138.7\n-\n-\n80.0\n-\n66.1\n-\n-\nMoVie\n-\n-\n-\n69.26\n-\n-\n74.9\n56.8\nPaLI (17B)\n149.1\n127.0\n124.4\n84.3\n84.3\n64.5\n81.7\n70.9\nPaLI-X (55B)\n149.2\n126.3\n124.3\n86.0\n86.1\n66.1\n86.0\n75.6\nTable 16: Results on COCO Captions (Karpathy split), NoCaps, VQAv2, OKVQA, and TallyQA with\nend-to-end modeling without OCR pipeline input. The \u201csimple\u201d and \u201ccomplex\u201d are test subsplits.\nB.3\nMulti-lingual Captioning\nMultilingual captioning on XM-3600\nThe Crossmodal-3600 (XM3600) benchmark contains a\ngeo-diverse set of 3600 images with human-annotated reference captions in 36 languages [23]. Table\n17 presents multilingual results for both PaLI (current SoTA on XM-3600) and PaLI-X, both finetuned\nwith 224\u00d7224 resolution. Overall, PaLI-X improves on the SoTA performance across 5 of the 7\nlanguages we report here (and for 14 of the total 35 languages considered); notably, the performance\non English is 4 CIDEr points lower compared to PaLI. The 35-language average CIDEr score is in\nthe same ballpark between PaLI and PaLI-X, with a slight +0.5 advantage for PaLI.\nModel\nen\nfr\nhi\niw\nro\nth\nzh\n35-lang avg.\nPaLI\n98.1\n75.5\n31.3\n46.8\n35.8\n72.1\n36.5\n53.6\nPaLI-X\n94.2\n78.7\n32.0\n46.9\n36.9\n75.3\n36.1\n53.1\nTable 17: CIDEr scores on image captioning for the Crossmodal-3600 benchmark for seven diverse\nlanguages (English, French, Hindi, Hebrew, Romanian, Thai, and Chinese), as well as the average of\nthe 35 languages covered by the benchmark. Both models are finetuned with 224\u00d7224 resolution.\n15\nB.4\nTallyQA and the emergence of complex counting capability\nWe present in Table 18 the performance of similar models across a wide range of capacity \u2013 from\n700M parameters to 55B parameters for PaLI-X. The graphs in Fig. 5 illustrate how simple counting\nappears to follow a more linear progression as parameter-size increases, while complex counting\nappears to show emergence somewhere before the datapoint provided by the performance of PaLI\n17B. This corresponds to our intution that complex counting is a true multimodal task that requires\nadditional capabilities from a model, in terms of the alignment that is required between the visual\ninformation and the prompt specification.\nModel\nTallyQA simple\nTallyQA complex\nWeighted average\nPaLI (700M)\n66.9\n55.6\n62.4\nPaLI (3B)\n72.0\n56.7\n65.9\nPaLI (17B)\n76.2\n65.5\n71.9\nPaLI-X (55B)\n81.3\n71.0\n77.2\nTable 18: Performance on TallyQA splits for simple and complex questions. All models use 224\u00d7224\nimage resolution.\nSize (B)\nAccuracy\n50\n60\n70\n80\n90\n1\n5\n10\n50\nTallyQA simple\nTallyQA complex\nFigure 5: Performance on TallyQA splits for simple and complex using PaLI variants and PaLI-X.\nAll models use 224\u00d7224 image resolution. The emergent behavior on complex counting beyond the\n3B size is made clear with PaLI-X.\nB.5\nDetails on Few-shot Modeling\nB.5.1\nFew-shot Formulation\nFigure 6 illustrates the network flow of a few shot model. The text and prompt part of each shot\nis embedded and concatenated as text features for the PaLI-X model. Each shot\u2019s images and the\ntarget image are independently encoded by the ViT component, and the ViT features are concatenated\nalong the sequence axis as visual features. Conditioned on that sequence, the PaLI-X decoder\nautoregressively makes the predictions for the target image.\nEncoder shot and Decoder shots\nWhile images for all few-shot examples and target example are\ngiven as input to the model, text information can be provided in different ways. During inference\ntime, all text information related to the few-shot examples is given to the encoder; in the case of a\nMulti-answer VQA task, for example, this includes both the prompts that contain the questions, and\nthe expected answers. Prompt for the target example is also given to the encoder, and the decoder is\ntasked with generating an answer for the target example. During training, however, we increase the\ntraining efficiency by making the model predict answers for both the target example and selected\nshots (the decoder shots). That is, we partition the N shots in two sets: encoder shots (Ne > 0) and\ndecoder shots (Nd \u2265 0), such that Ne + Nd \u2264 N. We use up to 4 shots in total during pre-training\n(i.e. N = 4 ), and sample Ne uniformly at random from 1 to N. Text input for encoder shots contain\n16\nProcessing\nVIT-22B\nText\nEmbeddings\nShot 1\nImage\nShot 2\nImage\nTarget\nImage\nShot 1 Text\nShot 2 Text\nTarget Prompt\nPaLI\nEncoder\nEach image is processed independently\nOutput\nTokens\nShot 2 Image\nShot 1 Image\nPaLI\nDecoder\nCross Attention\nDecoder\nTokens\nFigure 6: A detailed view on how the few-shot exemplars are fed to the model components.\nboth prompts and answers. The decoder shots, however, act as if they were target examples: their\ntext input to the encoder contains only the prompt, and the decoder needs to predict answers for the\ndecoder shots in addition to the target example.\nText Tokens\nImage Tokens\nDecoder Cross Attention Mask\nDecoder\nTokens\nEncoder Tokens\n  N\n  N\nTarget \nTarget \nAll Shots \n*\n*\nAll Shots \nFigure 7: Re-weighted attention with few-shots.\nAttention re-weighting\nIncreasing the num-\nber of shots turned out to be challenging, poten-\ntially due to cross-attention to target example\ninput tokens getting diluted by the large num-\nber of shots. To address this, we introduce an\nattention re-weighting mechanism. As shown\nin Figure 7, we explicitly boost the weights for\ncross attention between decoder tokens and en-\ncoded tokens from the target example (that is,\nthe target image and the target text prompt).\nSpecifically, if there are N shots in total, when\ndecoding each token we multiply the cross at-\ntention weights by N for the target image and\ntext tokens from the encoder outputs. We ob-\nserve this attention re-weighting technique is\nespecially helpful when we provide the model\nwith many shots (e.g. 32 shots). [96] introduces\na technique along similar lines to manipulate attention weights when gathering them from different\nthreads of encoded shots at inference time.\nB.5.2\nAdditional Few-shot Results\nMultilingual captioning results\nTable 19 reports the CIDEr scores for 7 languages and an average\nover 35 languages to demonstrate PaLI\u2019s multilingual captioning capabilities on the XM3600 bench-\nmark in teh few-shot setting. The pre-trained model (no few-shot finetuning) achieves an average\nscore of 22.7. The PaLI-X model achieves an average score of 45.1 for 4 shots and 47.1 for 32 shots.\nNote that the 32-shot PaLI-X average CIDEr score is only 6 points behind the fully finetuned model,\nwhich uses roughly 600k training examples per language (while the few-shot approach does not\nupdate the model parameters).\nQualitative results\nFigure 8 shows 3 examples on few-shot captioning and VQA tasks for qual-\nitative analysis. The first row shows captions for the images using the images\u2019 original language,\n5Equivalent with the Flamingo \u201c0-shot\u201d setting.\n17\nThe current population of \nAustralia is 26 million people.\nDeutschland hat 83 Millionen \nEinwohner.\nArgentina tiene 46 millones \nde habitantes.\nla population \nactuelle de la \nfrance est de 67 \nmillions d\u2019 \nhabitants\nInput few shot examples + target image\nHome to the best burgers \nand fried chicken.\nHome to unbeatable fish \nand chips.\nHome to outstanding \nramen. \nHome to the best \npizza in the world\nOutput\nQ: Is this a new technology? \nA: Yes, the typewriter is \ninvented in 19 Century, fairly \nnew.\nQ: Is this a new technology? \nA: No, the compass is \ninvented 2,000 years ago, \npretty old.\nQ: Is this a new technology?\nA: Hard to tell, the Newtonian \ntelescope is invented around 17 \ncentury.\nQ: Is this a new technology?\n A:\nyes the personal \ncomputer is \ninvented in 1970s \nfairly new\nPublic Domain Image\nWikimedia Commons [CC BY-SA 3.0]\nWikimedia Commons [CC BY-SA 2.0]\nWikimedia Commons [CC \nBY-SA 3.0]\nTim Klapdor (flickr) [CC BY-NC 2.0]\nWikimedia Commons [CC BY-SA 4.0]\nPelican (flickr) [CC BY-SA 2.0]\nWikimedia Commons [CC BY-SA 2.5]\nWikimedia Commons [CC BY-SA 3.0]\nWikimedia Commons [CC BY-SA 3.0]\nWikimedia Commons [CC BY-SA 2.0]\nGautier Poupeau (flickr) [CC BY 2.0]\nFigure 8: Qualitative Results on few-shot captioning (first two rows) and VQA (the last row) tasks.\nCrossmodal-3600 Captioning\nen\nfr\nhi\niw\nro\nth\nzh\n35-lang avg.\nPaLI-X 0-shot\n48.8\n25.0\n10.5\n20.1\n13.0\n33.3\n18.4\n22.7\nPaLI-X (2 text-only shots5)\n54.5\n46.7\n12.0\n22.2\n9.4\n40.3\n23.7\n25.8\nPaLI-X 4 shots\n77.8\n62.5\n22.2\n38.7\n30.2\n56.0\n27.7\n45.1\nPaLI-X 32 shots\n81.4\n66.1\n25.6\n40.6\n32.4\n59.4\n29.7\n47.1\nPaLI-X (finetuned)\n94.2\n78.7\n32.0\n46.9\n36.9\n75.3\n36.1\n53.1\nTable 19: Few-shot performance of the PaLI-X model on multilingual captioning tasks.\ndemonstrating the cross multilingual transfer of the few-shot capability. The second row captions the\nimages with a country\u2019s popular food, showing that the few-shot approach can access the model\u2019s\nworld knowledge. The last row shows a VQA with an explanation-like scenario where we ask if\nthe technologies in the images are \u201cnew\u201d. Generally speaking, the shown personal computer was\nproduced more than 40 years ago and could be regarded as old technology considering the fast pace\nof the current high-tech development. However, the 3 input shots provide the detailed calibration for\nthe concept of \u201cnew\u201d and the few-shot model successfully take the context and output \u201cnew\u201d with\nplausible explanation to the very old PC.\nB.5.3\nFew-shot ablation results\nIn this section, we present and discuss some ablation results for few-shot we explored in order to\ninform our final design choices on PaLI-X. Unless otherwise specified, we use a 700M-parameter\nmodel with the same encoder-decoder architecture, consisting of a ViT-B/16 vision encoder and a\nmT5-base encoder-decoder language model.\nPooling vs not pooling image tokens\nTo mitigate the computational burden that arises with many\nshots, we can pool (for example, average) the per-image tokens before concatenating all input tokens.\nThis pooled image tokens model achieved a CIDEr score of 56.3 for 4-shots COCO captioning, which\nis substantially lower than the full model\u2019s CIDEr score of 61.7. This highlights the importance of\nkeeping all the tokens coming out of the ViT encoder, despite the computational overhead.\nLimited-range Encoding Attention.\nWe explore per-example image-text attention, as proposed\nand applied in [10]. Under this approach, the image query tokens for each example can only attend\n18\nto its corresponding text tokens, while the text query tokens can attend to all tokens. By using this\nper-example attention model, we achieved a CIDEr score of 59.6, which is 2.1 points lower than the\nfull attention model\u2019s CIDEr score of 61.7 for 4-shots COCO captioning.\nAttention re-weighting for large number of shots.\nWe report the few-shot results on COCO\ncaptioning from early-stopped PaLI-2 3B models; in this case, we did not apply normalized attention\nin training. We provide the test results with and without attention re-weighting during inference\nfor a different number of encoder shots. Attention re-weighting achieves increasing CIDEr scores\nof 82.1, 84.3 and 84.5 with 4, 8 and 16 shots respectively. On the other hand, the model achieves\n83.4, 76.5 and 66.3 without attention re-weighting. The decreasing performance may suggest that the\nmodel fails to locate the target image and text prompt among the large number of shots, whereas the\nattention re-weighting helps the model to focus on the target features. Accordingly, we decided to\ninclude attention re-weighting during finetuning for PaLI-X.\nDistributing shots between encoder and decoder.\nWe explore the use of both encoder and decoder\nshots during pre-training. We pretrain the PaLI-2 700M model on PaLI-2 mixtures with varying\nnumber of encoder shots (between 1 and 4). The remaining shots (up to exactly 4) are used as decoder\nshots. Using only encoder shots leads to a 64.0 CIDEr score for 4 shots in COCO captioning. The\nbest mix of encoder and decoder shots achieves a CIDEr score of 65.2. This suggests splitting shots\nleads to a more challenging pre-train task that helps the model learn more efficiently.\nB.6\nFinetuning hyperparameters\nThe hyperparamter choices for downstream finetuning experiments are summarized in Table 20. As\nmentioned in the Main Text, for all of the downstream finetuning experiments, we used a reduced set\nof hyperparameters, without heavy per-task optimization.\nBenchmark\nlearning rate schedule\nSteps before LR decay to 0\nbatch size\nCOCO\nlinear decay from 1e-4\n10k\n256\nVQAv2\n20k\n256\nOCRVQA\n20k\n256\nMultitask-VQA\n20k\n256\nMultitask-Captioning\n20k\n256\nAll other\n5k\n128\nTable 20: Hyperparameter used for finetuning PaLI-X.\nB.7\nMulti-task finetuning\nWe deduplicated every training set mixture over the test sets of every task in order to prevent leakage\nof any test-set examples into the training set. The mixture is formed by putting the training examples\nof each subtask together, with heuristic adjustments for a better balance. Following the resolutions\nfor the single-task finetuning, the multi-task captioning and VQA finetuning are done with 672 and\n756 image resolutions, respectively. The multitask finetuning covers just about 5M examples, which\nis 20k steps with a batch size of 256. For scene-text and document understanding tasks, the multi-task\nfinetuning uses the end-to-end setting without OCR pipeline input.\nThe following aspects made multitask finetuning particularly challenging: (i) all tasks used the same\nprompt without task-specific indicators; the model is thus required to adapt to the style of multiple\nbenchmarks simultaneously. 2) We do not perform per-task validation set optimization. All subtasks\nare evaluated using the same checkpoint, but tasks converge to their optimal value at a different pace.\nB.8\nAblation studies\nWe first show in Table 22 the advantage brought by the OCR co-training stage of ViT-22B. We pair\nthe vanilla ViT-22B and the ViT-22B with additional OCR co-training with a small language model\nmT5-base and pretrain these models on 40M of WebLI-OCR data with the splitOCR objective, before\nfinetuning on ST-VQA. Co-training on image and OCR classification has a significant advantage on\n19\nVQA\nOK\nText VizWiz\nST\nOCR Info\nDoc Chart Avg.\nModel\nv2\nVQA VQA\nVQA\nVQA VQA VQA VQA\nQA\nSplit\ntest-dev\nval\nval\ntest-dev\nval\ntest\ntest\ntest\ntest\n-\nPrevious Multi-task SOTA\n84.3\n64.5\n68.4\n71.6\n75.1\n71.3\n40.0\n76.6\n70.5\n-\nSingle-task FT\n86.0\n66.1\n71.9\n72.6\n80.2\n75.9\n49.2\n80.0\n70.9\n-\nMulti-task FT\n84.3\n63.5\n71.4\n71.4\n79.0\n73.4\n50.7\n80.9\n70.6\n-\nMulti-task (+/-)\n-1.7\n-2.6\n-0.5\n-1.2\n-1.2\n-2.4\n+1.5 +0.9\n-0.3\n-0.8\nTable 21: Scores from multi-task finetuning compared with those from single-task finetuning for\nVQA. Validation or test-dev set numbers are reported for some tasks.\nST-VQA and TextVQA. In the meantime, the performance on VQAv2, which is not very scene-text\nheavy, is improved as well. Moreover, we found that making the top left patch white, which helped\nthe co-training of image classification and ocr classification on ViT-22B, is not required for the\nsubsequent training of PaLI-X.\nFor ablation of the PaLI-X training procedure, we used a 5B model with UL2-3B and ViT-G with 2B\nparameters, which is roughly a 10:1 down-scale of the PaLI-X 55B model.\nModel\nOCR-task Indicator\nST-VQA\nTextVQA\nVQAv2\n3-task avg.\nmT5-base + Vanilla ViT-22B\nNo\n42.6\n36.1\n68.9\n49.2\nmT5-base + ViT-22B-OCR\nNo\n47.0\n38.9\n69.8\n51.9\nmT5-base + ViT-22B-OCR\nYes\n46.2\n39.4\n70.2\n51.9\nTable 22: Advantage of the OCR co-training stage of ViT-22B. Pretraining is performed with\nresolution 224\u00d7224 and finetuning is with 448\u00d7448. Numbers reported are on validation split.\nFor stage 1 training, we show in Table 23 that adding image token generation does not harm the\nperformance on the main image+language understanding tasks.\nMixture\nCOCO\nVQAv2\nwithout ViT-VQGAN\n139.3\n77.3\nwith 10% ViT-VQGAN\n139.7\n77.1\nTable 23: Ablation experiment showing adding ViT-VQGAN tokens does not harm understanding\nperformance (captioning and VQA tasks).\n20\nC\nAdditional results: Video Captioning and QA\nBelow we give a brief description of each video data set we used for evaluation. Note that we freshly\ncollected the data when performing the experiments, which led to different effective numbers of\nvideos in different splits in some cases, see Table 24.\nThese descriptions refer to the original dataset size, but we train on (sometimes significantly) fewer\nvideos \u2014 the exact numbers are given in Table 24. This is because not all videos in the datasets were\navailable online at the time of writing (e.g., due to user deletion).\nC.1\nDatasets & Benchmarks\nMSR-VTT [55]: This dataset consists of 10K open domain video clips for video captioning, with 20\ncaptions each. The duration of each video clip is between 10 and 30 seconds. We follow the standard\nsplits proposed by [55] and report results on the test set.\nVATEX [56]: VATEX includes captions for 41K videos sampled from the Kinetics-600 dataset, with\n10 English captions each. We report results on the English public test set.\nActivityNet Captions [57]: This dataset consists of 100K temporally localized sentences for 20k\nvideos. We follow the standard split containing 50/25/25% of the dataset for training, validation and\ntesting, and use ground truth temporal proposals at evaluation following [57]. Note that following\nother works [62], we use the val_1 split for validation and val_2 split for testing.\nSpoken Moments in Time (SMIT) [58]: This dataset consists of long captions obtained via audio\nrecordings for 500k short video clips. While this dataset has been traditionally only used for text\nto video retrieval, we find that it is a strong benchmark for captioning as it is the largest manually\nannotated set of videos with text captions.\nActivityNet-QA [61]: The dataset contains 58,000 question-answer pairs for videos in the ActivityNet\ndataset [97]. We report accuracy (using exact string match) on the test split. Note that we do open-\nended generation for all VideoQA datasets.\nMSR-VTT-QA [60]: This dataset was created using a semi-automatic pipeline on top of the MSR-\nVTT dataset. We report accuracy (using exact string match) on the test split.\nNExT-QA [59]: We focus on the Open-Ended QA task, which consists of 52,044 question-answer\npairs for a total of 5,440 videos (sampled from the VidOr dataset[98]). Exactly following Next-\nQA [59] and Flamingo [10], we report the Wu-Palmer Similarity (WUPS) on the test set.\nMSR-VTT\nVATEX\nANet-Cap\nSMIT\nM-V-QA\nANet-QA\nNExT-QA\ntrain\n6513\n25991\n37421\n481094\n158581\n32000\n37523\nOriginal size\nvalid.\n497\n3000\n17505\n14604\n12278\n18000\n5343\ntest\n2990\n6000\n17031\n3513\n72821\n8000\n9178\ntrain\n4768\n22902\n30982\n481094\n116943\n28020\n37523\nDataset size\nvalid.\n327\n2657\n14604\n8096\n8215\n15890\n5343\ntest\n2144\n5276\n14234\n3513\n53014\n7050\n9178\ntrain\n73.21\n88.12\n82.79\n100.00\n73.74\n87.56\n100.00\n% Remaining\nvalid.\n65.79\n88.57\n83.43\n100.00\n66.91\n88.28\n100.00\ntest\n71.71\n87.93\n83.58\n100.00\n72.80\n88.13\n100.00\nTable 24: We freshly collect the data sets from the respective data sources. In cases where there are\nmultiple question-answer pairs per video we report the number of question-answer pairs. Similarly,\nfor ActivityNet Captions we report the number of captions. Due to missing videos which were\nremoved after the original data sets were defined, most of our data sets are missing 10% of the videos\nor more.\n21\nD\nAdditional results: Image Classification\nSetup for zero-shot and finetuning evaluation\nThe setup used for the experiments here uses the\nPaLI-X model to generate directly the (English) class name using the captioning prompt. The output\nis considered correct if it matches exactly the class name (apart from ImageNet-REAL, where we\ncheck if the class corresponding to the output is in the set of correct labels).\nZero-shot Evaluation results\nWe use the same scoring technique as in PaLI [5] to evaluate PaLI-X\nin zero-shot setting (without training on any Imagenet data). We use the PaLI-X model obtained after\nthe first stage of training (using the base 224 image resolution).\nThe results are presented in Table 25. We compare the results to PaLI [5] - previous zero-shot genera-\ntive SOTA, and Flamingo [10] - another generative model of similar architecture with comparable\n1-shot and 5-shot results. Overall, we report that the results between PaLI and PaLI-X for 0-shot are\nsimilar.\nModel (ImageNet data)\nINet\nREAL\nINet-R\nINet-A\nINet-Sketch\nINet-v2\nObjNet\nFlamingo-80B (1-shot)\n71.9\n-\n-\n-\n-\n-\n-\nFlamingo-80B (5-shot)\n77.3\n-\n-\n-\n-\n-\n-\nPaLI (17B) (0-shot)\n72.11\n76.43\n81.97\n44.70\n63.83\n64.46\n42.62\nPaLI-X (0-shot)\n71.16\n75.75\n82.96\n46.13\n61.58\n63.91\n44.58\nTable 25:\nTop 1 accuracy results of 0-shot image classification on ImageNet [66],\nImageNet-REAL\n[67],\nImageNet-R\n[68],\nImageNet-A\n[69],\nImageNet-Sketch\n[70],\nImagenet-v2 [71] and ObjectNet [99].\nFinetuning\nTo test image classification capabilities, we finetune PaLI-X on ImageNet [66] and eval-\nuate the resulting model on ImageNet-REAL [67] and out-of-distribution datasets: ImageNet-R [68],\nImageNet-A [69], ImageNet-Sketch [70], ImageNet-v2 [71].\nWe use the model from the first training stage (at resolution 224) and the one from the last training\nstage (at resolution 756). We use the same training hyperparameters for all of runs (selected without\nany hyperparameter tuning).\nThe results can be seen in Table 26. We compare the results to generative model with open vocab \u2013\nGiT2 [9] (using 384 image resolution), which is the current SOTA for full-finetuning on ImageNet.\nPaLI-X achieves close to SOTA results for generative models on Imagenet, and other datasets.\nModel (resolution)\nINet\nREAL\nINet-R\nINet-A\nINet-Sketch\nINet-v2\nGIT2 (384)\n89.22\n-\n-\n-\n-\n-\nPaLI 3B (224)\n85.11\n88.71\n81.11\n45.71\n70.00\n78.23\nPaLI 17B (224)\n86.13\n88.84\n78.21\n50.00\n71.21\n78.91\nPaLI-X (224)\n88.22\n90.36\n77.66\n55.97\n72.56\n81.42\nPaLI-X (756)\n88.82\n90.80\n79.97\n73.47\n73.39\n83.48\nPaLI-X \u2020 (756)\n89.19\n90.98\n80.06\n72.57\n73.37\n83.66\nTable\n26:\nClassification\n(top-1)\naccuracy\nwith\nImagenet\n[66]\nfine-tuning\non:\nIma-\ngeNet, ImageNet-REAL [67], ImageNet-R [68], ImageNet-A [69], ImageNet-Sketch [70],\nImagenet-v2 [71] (resolution in parentheses). PaLI-X \u2020 fine-tuned for 2.2x more steps.\n22\nE\nObject Detection\nE.1\nObject detection as a VLM task\nObject detection is framed similarly to Pix2seq [72], with two key differences: the use of a natural\nlanguage vocabulary, and class-conditioning. Prompt classes are fed to PaLI-X\u2019s text encoder, in the\nformat detect class1 and class2 and class3. The model is trained to only output bounding\nboxes corresponding to classes in this prompt. We represent bounding boxes as coordinates in the\nsame style as pix2seq [72]; that is, 4 integers ymin xmin ymax xmax ranging from 0 to 999. Figure 9\nshows an example input.\nencoder input: detect giraffe and \ncar and mask and coffee maker and \nwheel\ninput image:\ndecoder output: 222 35 731 978 car \nand 540 419 731 548 wheel and 409 \n85 571 194 wheel\nglobal negative (from visual genome)\nnegative\npositive\ncorresponds to:\nImage credits: Matthew Henry, burst, https://burst.shopify.com/photos/vintage-red-porsche\nFigure 9: An example training pair, consisting of the text prompt, the image and the expected\noutput. The prompt consists of multiple classes; we show a hypothetical Open Images V4 example,\nwith positives \u2018car\u2019 and \u2018wheel\u2019, negative \u2018giraffe\u2018 and global negatives \u2018mask\u2019 and \u2018coffee maker\u2019\n(sampled from the visual genome label space).\nPrompt sampling hyperparameters\nDuring training, a prompt for each example. We construct\nprompts from three pieces of information:\n\u2022 Positives: These are the bounding boxes for objects definitely present in the image. During\ntraining, per example we sample p+ \u223c U(0, P +\nmax), and keep that proportion of positives.\n\u2022 Negatives: These are the known instance negatives i.e. bounding boxes for objects definitely\nnot present. For exhaustively labelled datasets like COCO, this is simply classes not labelled\nas positives. For non-exhaustively labelled datasets like LVIS, these are the classes not\nlabelled as positives, which were presented to raters. During training sample f \u2212 \u223c U(0, 5.0),\nand use up to f \u2212 \u00d7 n+, where n+ is the number of positives after sampling p+.\n\u2022 Global negatives: These are negatives which are not explicitly labelled as negatives. They\nare taken from a wider label space combining multiple detection datasets. For a given\nexample, valid global negatives consist of classes from the wider label space not explicitly\nlabelled as positives or negatives. During training, we sample f GN \u223c U(0, 5.0) and append\nf \u00d7 n+ global negatives, where n+ is the number of positives after sampling p+.\nBy default, the combined label spaces of Visual Genome, Objects365 and OpenImagesV4\nwas used as the global label space, with the exception of detection finetuning, where LVIS\nand COCO label spaces were also added.\nWe truncate the number of total classes to nmax. nmax and P +\nmax) are tuned per dataset to meet\nsequence lengths. Afer truncatation, we shuffle classes in the prompt.\n23\nE.2\nPreprocessing\nDuring pre-training, data is preprocessed to remove all LVIS-rare labels, following the protocol of\nOwlViT [28]. This is not done for detection finetuning. Images are randomly flipped horizontally,\nand randomly resized to between 0.3 and 2.0 \u00d7 their original sized, followed by selecting a random\nsquare crop of the current training resolution. If the image is resized to be smaller than the current\nresolution, it is left as is. Images are finally padded to a square.\nE.3\nLicenses and attribution for images used in Main Text Figure 2\n\u2022 Watermelon: Credit: Sarah Pflug\nhttps://burst.shopify.com/photos/cutting-watermelon.\n\u2022 Bowls:\nhttps://www.flickr.com/photos/ariesandrea/502826051/ CC-BY-NC-ND 2.0\n\u2022 Business cat Credit: Sarah Pflug,\nhttps://burst.shopify.com/photos/business-cat-in-office\n\u2022 Wall Credit: Matthew Henry\nhttps://burst.shopify.com/photos/man-walking-in-front-of-this-is-paradise-wall?c=urban-life\nReferences\n[1] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Jared Kaplan Melanie Subbiah, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Christopher Hesse Clemens Winter, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS,\n2020.\n[3] Nan Du, Yanping Huang, Andrew M Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu,\nMaxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph, Liam Fedus, Maarten P\nBosma, Zongwei Zhou, Tao Wang, Emma Wang, Kellie Webster, Marie Pellat, Kevin Robinson,\nKathleen Meier-Hellstern, Toju Duke, Lucas Dixon, Kun Zhang, Quoc Le, Yonghui Wu, Zhifeng\nChen, and Claire Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In\nICML, 2022.\n[4] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,\nLaurent El Shafey, Yanping Huang, Kathy Meier-Hellstern, Gaurav Mishra, Erica Moreira,\nMark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing\nZhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan Botha,\nJames Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave,\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg,\nFangxiaoyu Feng, Vlad Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas\nGonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wenhao Jia,\nKathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin\nLee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao\nLiu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra,\nMaysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish,\nMarie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan\nRichter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee\nShelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha\nValter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang,\nJohn Wieting, Yuhuai Wu, Kelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu,\n24\nQiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov, and Yonghui\nWu. Palm 2 technical report, 2023.\n[5] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebas-\ntian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish\nThapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol\nAyan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI: A jointly-scaled multilingual language-image model. In ICLR, 2023.\n[6] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin\nGilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, Rodolphe Je-\nnatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme, Matthias\nMinderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin F.\nElsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings,\nMark Patrick Collier, Alexey Gritsenko, Vighnesh Birodkar, Cristina Vasconcelos, Yi Tay,\nThomas Mensink, Alexander Kolesnikov, Filip Paveti\u00b4c, Dustin Tran, Thomas Kipf, Mario\nLu\u02c7ci\u00b4c, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision\ntransformers to 22 billion parameters. In ICML, 2023.\n[7] Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Jason Wei, Xuezhi Wang, Hyung Won\nChung, Siamak Shakeri, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Denny Zhou, Neil\nHoulsby, and Donald Metzler. UL2: Unifying language learning paradigms. In ICLR, 2023.\n[8] Fangyu Liu, Julian Martin Eisenschlos, Francesco Piccinno, Syrine Krichene, Chenxi Pang,\nKenton Lee, Mandar Joshi, Wenhu Chen, Nigel Collier, and Yasemin Altun. DePlot: One-shot\nvisual language reasoning by plot-to-table translation. arXiv preprint arXiv:2212.10505, 2022.\n[9] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu,\nCe Liu, and Lijuan Wang. GIT: A generative image-to-text transformer for vision and language.\nTMLR, 2022.\n[10] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson,\nKarel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual\nlanguage model for few-shot learning. In NeurIPS, 2022.\n[11] Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023.\n[12] OpenAI. Gpt-4 technical report, 2023.\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\n[14] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\n[15] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed Chi,\nQuoc V Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In Advances in Neural Information Processing Systems, 2022.\n[16] Denny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale\nSchuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. Least-to-most prompting\nenables complex reasoning in large language models. In ICLR, 2023.\n[17] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao\nLiu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently.\narXiv preprint arXiv:2303.03846, 2023.\n[18] Yanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. Meta-learning via language\nmodel in-context tuning. In ACL, 2022.\n[19] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.\nUnified-IO: A unified model for vision, language, and multi-modal tasks. In ICLR, 2023.\n[20] Gang Li and Yang Li. Spotlight: Mobile UI understanding using vision-language models with a\nfocus. In ICLR, 2023.\n25\n[21] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. SimVLM:\nSimple visual language model pretraining with weak supervision. In ICLR, 2022.\n[22] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual Captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\n[23] Ashish V. Thapliyal, Jordi Pont-Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: A\nmassively multilingual multimodal evaluation dataset. In EMNLP, 2022.\n[24] Jihyung Kil, Soravit Changpinyo, Xi Chen, Hexiang Hu, Sebastian Goodman, Wei-Lun\nChao, and Radu Soricut. PreSTU: Pre-training for scene-text understanding. arXiv preprint\narXiv:2209.05534, 2022.\n[25] Soravit Changpinyo, Doron Kukliansky, Idan Szpektor, Xi Chen, Nan Ding, and Radu Soricut.\nAll you may need for VQA are image captions. In NAACL, 2022.\n[26] AJ Piergiovanni, Weicheng Kuo, and Anelia Angelova. Pre-training image-language trans-\nformers for open-vocabulary tasks. In T4V: Transformers for Vision Workshop, Conference on\nComputer Vision and Pattern Recognition, 2022.\n[27] Kenton Lee, Mandar Joshi, Iulia Turc, Hexiang Hu, Fangyu Liu, Julian Eisenschlos, Urvashi\nKhandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova. Pix2Struct: Screenshot\nparsing as pretraining for visual language understanding. In ICML, 2023.\n[28] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn,\nAlexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen,\net al. Simple open-vocabulary object detection with vision transformers. In ECCV, 2022.\n[29] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku,\nYuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with\nimproved VQGAN. In ICLR, 2022.\n[30] Andrej Karpathy and Li Fei-Fei.\nDeep visual-semantic alignments for generating image\ndescriptions. In CVPR, 2015.\n[31] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv\nBatra, Devi Parikh, Stefan Lee, and Peter Anderson. nocaps: Novel object captioning at\nscale. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages\n8948\u20138957, 2019.\n[32] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. TextCaps: a dataset\nfor image captioning with reading comprehension. In European conference on computer vision,\npages 742\u2013758, 2020.\n[33] Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. Captioning images taken by\npeople who are blind. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XVII 16, pages 417\u2013434. Springer, 2020.\n[34] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li. Screen2Words:\nAutomatic mobile ui summarization with multimodal learning. In The 34th Annual ACM\nSymposium on User Interface Software and Technology, UIST \u201921, page 498\u2013510, New York,\nNY, USA, 2021. Association for Computing Machinery.\n[35] Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan. Widget Captioning:\nGenerating natural language description for mobile user interface elements. In Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages\n5495\u20135510, Online, November 2020. Association for Computational Linguistics.\n[36] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the\nV in VQA matter: Elevating the role of image understanding in visual question answering.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n6904\u20136913, 2017.\n[37] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A\nvisual question answering benchmark requiring external knowledge. In Proceedings of the\nIEEE/cvf conference on computer vision and pattern recognition, pages 3195\u20133204, 2019.\n[38] Manoj Acharya, Kushal Kafle, and Christopher Kanan. Tallyqa: Answering complex counting\nquestions. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages\n8076\u20138084, 2019.\n26\n[39] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi\nParikh, and Marcus Rohrbach. Towards VQA models that can read. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, pages 8317\u20138326, 2019.\n[40] Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo,\nand Jeffrey P Bigham. VizWiz grand challenge: Answering visual questions from blind people.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n3608\u20133617, 2018.\n[41] Ali Furkan Biten, Rub\u00e8n Tito, Andr\u00e9s Mafla, Lluis Gomez, Mar\u00e7al Rusi\u00f1ol, C.V. Jawahar,\nErnest Valveny, and Dimosthenis Karatzas. Scene text visual question answering. In 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pages 4290\u20134300, 2019.\n[42] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa:\nVisual question answering by reading text in images. In ICDAR, 2019.\n[43] Minesh Mathew, Viraj Bagal, Rub\u00e8n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawa-\nhar. Infographicvqa. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 1697\u20131706, 2022.\n[44] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. Docvqa: A dataset for vqa on\ndocument images. In Proceedings of the IEEE/CVF winter conference on applications of\ncomputer vision, pages 2200\u20132209, 2021.\n[45] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali\nFarhadi. A diagram is worth a dozen images. In ECCV, 2016.\n[46] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A benchmark\nfor question answering about charts with visual and logical reasoning. In Findings of ACL,\n2022.\n[47] Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina\nToutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recognizing\nmillions of wikipedia entities. arXiv preprint arXiv:2302.11154, 2023.\n[48] Yang Chen, Hexiang Hu, Yi Luan, Haitian Sun, Soravit Changpinyo, Alan Ritter, and Ming-\nWei Chang. Can pre-trained vision and language models answer visual information-seeking\nquestions? arXiv preprint arXiv:2302.11713, 2023.\n[49] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language:\nBEiT pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442,\n2022.\n[50] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar,\nPierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc\nToussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Pete Florence. PaLM-E: An embodied\nmultimodal language model. In arXiv, 2023.\n[51] Duy-Kien Nguyen, Vedanuj Goswami, and Xinlei Chen. Revisiting modulated convolutions for\nvisual counting and beyond. ICLR, 2021.\n[52] Yixuan Qiao, Hao Chen, Jun Wang, Yihao Chen, Xianbin Ye, Ziliang Li, Xianbiao Qi, Peng Gao,\nand Guotong Xie. Winner team Mia at TextVQA challenge 2021: Vision-and-language represen-\ntation learning with pre-trained sequence-to-sequence model. arXiv preprint arXiv:2106.15332,\n2021.\n[53] Ali Furkan Biten, Ron Litman, Yusheng Xie, Srikar Appalaraju, and R. Manmatha. LaTr:\nLayout-aware transformer for scene-text VQA. In CVPR, 2022.\n[54] Zineng Tang, Ziyi Yang, Guoxin Wang, Yuwei Fang, Yang Liu, Chenguang Zhu, Michael\nZeng, Cha Zhang, and Mohit Bansal. Unifying vision, text, and layout for universal document\nprocessing. In CVPR, 2023.\n[55] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A large video description dataset for\nbridging video and language. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2016.\n27\n[56] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang.\nVATEX: A large-scale, high-quality multilingual dataset for video-and-language research. In\nICCV, 2019.\n[57] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.\nDense-\ncaptioning events in videos. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2017.\n[58] Mathew Monfort, SouYoung Jin, Alexander Liu, David Harwath, Rogerio Feris, James Glass,\nand Aude Oliva. Spoken Moments: Learning joint audio-visual representations from video\ndescriptions. In CVPR, 2021.\n[59] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. NExT-QA: Next phase of question-\nanswering to explaining temporal actions. In CVPR, 2021.\n[60] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang.\nVideo question answering via gradually refined attention over appearance and motion. In MM,\n2017.\n[61] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.\nActivityNet-QA: A dataset for understanding complex web videos via question answering.\nIn AAAI, 2019.\n[62] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. End-to-end\ndense video captioning with parallel decoding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 6847\u20136857, 2021.\n[63] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit Bansal, and Gedas Bertasius. VindLU:\nA recipe for effective video-and-language pretraining. In CVPR, 2023.\n[64] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative\npretraining for multimodal video captioning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 17959\u201317968, 2022.\n[65] Mario Fritz Mateusz Malinowski. A multi-world approach to question answering about real-\nworld scenes based on uncertain input. In NeurIPS, 2014.\n[66] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-\nscale hierarchical image database. In 2009 IEEE conference on computer vision and pattern\nrecognition, pages 248\u2013255, 2009.\n[67] Lucas Beyer, Olivier J H\u00e9naff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00e4ron van den Oord.\nAre we done with ImageNet? arXiv preprint arXiv:2006.07159, 2020.\n[68] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo,\nRahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness:\nA critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 8340\u20138349, 2021.\n[69] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural\nadversarial examples. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 15262\u201315271, 2021.\n[70] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global represen-\ntations by penalizing local predictive power. In Advances in Neural Information Processing\nSystems, pages 10506\u201310518, 2019.\n[71] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet\nclassifiers generalize to ImageNet? In International Conference on Machine Learning, pages\n5389\u20135400, 2019.\n[72] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey E. Hinton. Pix2Seq: A\nlanguage modeling framework for object detection. In The Tenth International Conference on\nLearning Representations, ICLR, 2022.\n[73] Agrim Gupta, Piotr Doll\u00e1r, and Ross B. Girshick. LVIS: A dataset for large vocabulary instance\nsegmentation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019,\nLong Beach, CA, USA, June 16-20, 2019, pages 5356\u20135364. Computer Vision Foundation /\nIEEE, 2019.\n28\n[74] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via\nvision and language knowledge distillation. In ICLR, 2022.\n[75] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold\nLi, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip: Region-based\nlanguage-image pretraining. In CVPR, 2022.\n[76] Lisa Anne Hendricks, Kaylee Burns, Kate Saenko, Trevor Darrell, and Anna Rohrbach. Women\nalso snowboard: Overcoming bias in captioning models. In ECCV, 2018.\n[77] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. Semantics derived automatically from\nlanguage corpora contain human-like biases. Science, 356(6334), 2017.\n[78] Jieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Men also like\nshopping: Reducing gender bias amplification using corpus-level constraints. In EMNLP, 2017.\n[79] Zeyu Wang, Klint Qinami, Ioannis Christos Karakozis, Kyle Genova, Prem Nair, Kenji Hata,\nand Olga Russakovsky. Towards fairness in visual recognition: Effective strategies for bias\nmitigation. In CVPR, 2020.\n[80] Joy Buolamwini and Timnit Gebru. Gender shades: Intersectional accuracy disparities in\ncommercial gender classification. In FAccT, 2018.\n[81] Jessica Deuschel, Bettina Finzel, and Ines Rieger. Uncovering the bias in facial expressions.\narXiv preprint arXiv:2011.11311, 2020.\n[82] Ben Hutchinson, Jason Baldridge, and Vinodkumar Prabhakaran. Underspecification in scene\ndescription-to-depiction tasks. In AACL / IJCNLP, 2022.\n[83] Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object\nrecognition work for everyone? In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition workshops, pages 52\u201359, 2019.\n[84] Priya Goyal, Adriana Romero Soriano, Caner Hazirbas, Levent Sagun, and Nicolas Usunier.\nFairness indicators for systematic assessments of visual feature extractors. In 2022 ACM\nConference on Fairness, Accountability, and Transparency, pages 70\u201388, 2022.\n[85] Kimmo Karkkainen and Jungseock Joo. Fairface: Face attribute dataset for balanced race,\ngender, and age for bias measurement and mitigation. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 1548\u20131558, 2021.\n[86] Alyssa Lees, Vinh Q Tran, Yi Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasser-\nman. A new generation of perspective API: Efficient multilingual character-level transformers.\nIn KDD, 2022.\n[87] Candice Schumann, Susanna Ricco, Utsav Prabhu, Vittorio Ferrari, and Caroline Rebecca\nPantofaru. A step toward more inclusive people annotations for fairness. In Proceedings of the\nAAAI/ACM Conference on AI, Ethics, and Society (AIES), 2021.\n[88] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard Zemel. Fairness\nthrough awareness. In Innovations in Theoretical Computer Science, 2012.\n[89] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the\nwild. In ICCV, 2015.\n[90] Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song,\nJohn Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language\nmodels: Methods, analysis & insights from training gopher. arXiv preprint arXiv:2112.11446,\n2021.\n[91] Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in\ncoreference resolution. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, New Orleans,\nLouisiana, June 2018. Association for Computational Linguistics.\n[92] Ellis Monk. Monk skin tone scale, 2019.\n[93] Foad Hamidi, Morgan Klaus Scheuerman, and Stacy M Branham. Gender recognition or gender\nreductionism? the social implications of embedded gender recognition systems. In Proceedings\nof the 2018 chi conference on human factors in computing systems, pages 1\u201313, 2018.\n[94] Os Keyes. The misgendering machines: Trans/hci implications of automatic gender recognition.\nProceedings of the ACM on human-computer interaction, 2(CSCW):1\u201322, 2018.\n29\n[95] Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant,\nAditya Barua, and Colin Raffel.\nmT5: A massively multilingual pre-trained text-to-text\ntransformer. In Proceedings of the 2021 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 483\u2013498,\nJune 2021.\n[96] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting:\nScaling in-context learning to 1,000 examples. arXiv preprint arXiv:2212.06713, 2022.\n[97] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet:\nA large-scale video benchmark for human activity understanding. In Proceedings of the ieee\nconference on computer vision and pattern recognition, pages 961\u2013970, 2015.\n[98] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua. Annotating\nobjects and relations in user-generated videos. In Proceedings of the 2019 on International\nConference on Multimedia Retrieval, pages 279\u2013287, 2019.\n[99] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund,\nJoshua Tenenbaum, and Boris Katz. ObjectNet: a large-scale bias-controlled dataset for pushing\nthe limits of object recognition models. In Proceedings of the 33rd International Conference on\nNeural Information Processing Systems, pages 9453\u20139463, 2019.\n30\n"
  },
  {
    "title": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented Feature Adaptation of Vision-Language Models",
    "link": "https://arxiv.org/pdf/2305.18373.pdf",
    "upvote": "1",
    "text": "KAFA: Rethinking Image Ad Understanding with Knowledge-Augmented\nFeature Adaptation of Vision-Language Models\nZhiwei Jia\u02da\nUC San Diego\nPradyumna Narayana\nGoogle\nArjun R. Akula\nGoogle\nGarima Pruthi\nGoogle\nHao Su\nUC San Diego\nSugato Basu\nGoogle\nVarun Jampani\nGoogle\nAbstract\nImage ad understanding is a crucial task with\nwide real-world applications. Although highly\nchallenging with the involvement of diverse\natypical scenes, real-world entities, and reason-\ning over scene-texts, how to interpret image\nads is relatively under-explored, especially in\nthe era of foundational vision-language models\n(VLMs) featuring impressive generalizability\nand adaptability. In this paper, we perform the\nfirst empirical study of image ad understand-\ning through the lens of pre-trained VLMs. We\nbenchmark and reveal practical challenges in\nadapting these VLMs to image ad understand-\ning. We propose a simple feature adaptation\nstrategy to effectively fuse multimodal infor-\nmation for image ads and further empower it\nwith knowledge of real-world entities. We hope\nour study draws more attention to image ad un-\nderstanding which is broadly relevant to the\nadvertising industry.\n1\nIntroduction\nAs advertisements play an integral role in hu-\nman society, image ad understanding has many\nreal-world applications such as ad targeting (Hus-\nsain et al., 2017), visual metaphor understanding\n(Abokhoza et al., 2019) and creative ad genera-\ntion (Chilton et al., 2019; Akula et al., 2022). It is\nalso highly challenging due to several reasons, as\nexemplified in Fig. 2. First, image ads consist of di-\nverse visual elements including non-photorealistic\nobjects and atypical scenes synthesized creatively\nthat are beyond common academic datasets. Sec-\nondly, they involve knowledge of a large number\nof real-world entities such as brands and products\nwhere existing work (Su et al., 2018; Li et al.,\n2022a) struggles to cover. Lastly, many adopt vi-\nsual or multimodal rhetorics requiring reasoning\nover diverse visual elements including scene-texts,\nand sometimes even elude humans (Petridis and\n\u02da Work done in part during an internship at Google. Cor-\nrespondence to zjia@eng.ucsd.edu.\nFigure 1: We propose to utilize external knowledge via\na brand understanding module and combine features of\ndifferent modalities via a lightweight attention-based\nfeature adapter to decode the correct messages of image\nads. The VLM baseline is confused and gives the wrong\none. All brand info is anonymized.\nChilton, 2019). However, image ad understanding\nis relatively under-explored in the machine learning\ncommunity, especially in the presence of recently\ndeveloped foundational vision-language models\n(VLMs) pre-trained using a tremendous number\nof image and text description data.\nThe pre-trained VLMs are shown to have great\ngeneralization capability, contain real-world knowl-\nedge (implicitly), and can be adapted to a wide\nrange of downstream tasks in a data-efficient way\n(Radford et al., 2021; Alayrac et al., 2022). It is\nthen natural to utilize VLMs for image ad under-\nstanding. In this paper, we perform the first empiri-\ncal study of adapting VLMs to the task of decoding\nthe overall messages delivered by image ads, which\nis usually formulated as visual question answering\n(Hussain et al., 2017). Specifically, we examine\nthree popular pre-trained VLMs that are alignment-\nbased and are publicly available, namely, CLIP\n(Radford et al., 2021), ALBEF (Li et al., 2021)\nand LiT (Zhai et al., 2022). We examine zero-shot\nperformance as well as adaptation strategies and\nreveal the practical challenges of applying VLMs\nto image ads. We propose a simple feature adapta-\ntion strategy that effectively utilizes VLM features.\nWe further propose to incorporate external brand\nknowledge (real-world entities) that brings a signif-\narXiv:2305.18373v1  [cs.CV]  28 May 2023\nFigure 2: Example image ads with diverse visual el-\nements, atypical scenes and rhetorics to convey their\nmessages creatively. All brand info is anonymized.\nicant performance boost.\nOur contributions are three-fold. First, we em-\npirically find that the sheer scale of data & ca-\npacity of the model used in pretraining matters\nthe most for the performance of image ad under-\nstanding, partly due to VLM\u2019s capability of stor-\ning real-world knowledge, which is not captured\nwell by the commonly used metrics for compar-\ning VLMs. Second, we reveal the practical chal-\nlenges of adapting VLMs for image ad understand-\ning (i.e., overfitting to the limited training data &\nsupervision signals and high computation burden\nof hard negative mining) and propose a simple solu-\ntion (attention-based feature adaptation) that better\nleverages VLM features than previous adaptation\nstrategies. Lastly, we propose to leverage exter-\nnal knowledge for brand understanding that we\nhave empirically shown to further enhance image\nad understanding. Together with the aforemen-\ntioned adaptation strategy, we call our approach\nknowledge-augmented feature adaptation (KAFA).\n2\nRelated Work\nImage Ad Understanding\nLearning to automati-\ncally interpret image ads was proposed by the Pitt\nImage Ads Dataset (Hussain et al., 2017), where\neach ad is annotated by a caption that answers\n\u201cwhat should I do according to the ad and why?\u201d\nDifferent from traditional image captioning, this\ntask is highly non-trivial as discussed at the be-\nginning of Sec. 1. While prior methods utilize\ncultural connotations via external symbolic knowl-\nedge (Ye and Kovashka, 2018), capture relations\nbetween scene-texts and objects by GNNs (Dey\net al., 2021), and leverage pre-trained language\nmodels to combine multimodel information (Kalra\net al., 2020), none have exploited vision-language\nmodels (VLMs) and the knowledge of real-world\nentities (i.e., brands). Besides the wide applications\nin the ad industry, later work hints that the study\nof image ads is relevant to much broader research\ntopics (Singh et al., 2019; Akula et al., 2022).\nFoundational Alignment-based VLMs\nA re-\ncent surge of collections of tremendous images\npaired with text descriptions (Schuhmann et al.,\n2022) enables alignment-based pretraining (i.e.,\ncontrastive learning) of foundational VLMs that are\nefficient zero-shot or low-shot learners for down-\nstream tasks. By learning to embed images and\ntexts into a shared semantic space, they handle\ndomain variations in an open-vocabulary manner\n(which involves real-world knowledge). Among\nthese are CLIP (Radford et al., 2021), ALIGN (Jia\net al., 2021), LiT (Zhai et al., 2022) and BASIC\n(Pham et al., 2021). Another line of work further\nadopts masked language modeling, image caption-\ning loss, and object-level alignment, e.g., ALBEF\n(Li et al., 2021), Florence (Yuan et al., 2021), CoCa\n(Yu et al., 2022) and GLIP (Li et al., 2022b).\nTransfer Learning of VLMs\nTransfer learning\nof VLMs has become popular with the zero-shot\nperformance of CLIP in image classification tasks.\nA direct approach is to (partially) fine-tune the\nVLMs with (optionally) additional neural networks\ntailored for downstream tasks, e.g., TAP-C (Song\net al., 2022), CPT (Yao et al., 2021), KAT (Gui\net al., 2021) and VL-Adapter (Sung et al., 2022).\nAnother approach that bypasses the need of tuning\nthe VLMs is prompt learning. For instance, CoOp\n(Zhou et al., 2022b) and CoCoOp (Zhou et al.,\n2022a) only tune learnable inputs to the VLMs.\nThe third approach that further reduces memory\nand computation burden is feature adapters, where\nVLM features of the inputs are pre-computed be-\nfore transfer learning. Examples are CLIP-Adapter\n(Gao et al., 2021), SVL-Adapter (Pantazis et al.,\n2022) and Attention-Adapter (Zhao et al., 2022).\nKnowledge-Augmented Image Understanding\nMany image understanding tasks require real-world\nknowledge beyond what can be captured by the\ninput data.\nFor instance, FVQA (Wang et al.,\n2017) and OK-VQA (Marino et al., 2019) require\nmodels to process external fact-based knowledge;\nTextVQA (Singh et al., 2019) asks to understand\nnamed entities in the wild; the Pitt Dataset (Hussain\net al., 2017) involves recognition of large quanti-\nties of brands. Existing work incorporates exter-\nnal knowledge either explicitly via structured or\nunstructured knowledge base (Wang et al., 2015;\nGard\u00e8res et al., 2020; Ye and Kovashka, 2018),\nor implicitly from knowledge stored in pretrained\nmodels (Kalra et al., 2020; Kim et al., 2022), or\nboth (Marino et al., 2021; Gui et al., 2021).\n3\nWhat Really Matters for Pre-trained\nVLMs in Image Ad Understanding?\nThe first insight of our empirical study is that the\nsheer size of data and the model used in pretraining\nis the key factor determining the performance of\nVLMs for image ad understanding.\nTo promote reproducibility, we evaluate three\nalignment-based VLMs (i.e., CLIP, ALBEF and\nLiT) that are publicly accessible in a zero-shot man-\nner on the Pitt Dataset (Hussain et al., 2017), which\nformulates ad understanding as image-to-text re-\ntrieval. We adopt the official evaluation protocol,\nwhich asks the model to select one of the 3 cor-\nrect messages conveyed by the image ad from a set\nof 15 candidates (including 12 wrong messages)\nfor each of the 12805 test samples. Specifically,\ngiven an alignment-based VLM, let us denote its en-\ncoders with normalized outputs as fIp\u00a8q and fT p\u00a8q\nfor image and text branches, respectively. Given\nan image x and the ground truth texts y, the VLM\nretrieves y from candidates Cpxq according to the\ndot-product score fIpxq \u00a8 fT pyq. We then mea-\nsure the performance of the VLM with 3 metrics\ncommonly used in the literature: accuracy (the per-\ncentage of images with any positive text retrieved\nwith rank one), rank (how the top retrieved positive\ntext is ranked averaged over all images), and the\nmean rank (the mean rank of the all positive texts\naveraged over all images).\nWith the results reported in Tab. 1, we have\nseveral findings. First, the more data used during\nthe pretraining of a VLM, the better it generalizes\nto the image ad domain. For a comparison, CLIP\nhas seen 400M image-text pairs, LiT 100M, and\nALBEF 14M. Second, the larger the capacity of\na model, the better it understands image ads. We\nhave evaluated different sizes of the CLIP model\nbeyond the three sizes shown in Tab. 1 and the trend\nkeeps the same. Third, commonly used metrics for\ncomparing VLMs, including zero-shot accuracy on\nthe ImageNet (Russakovsky et al., 2015) validation\nset (for which LiT claims to outperform CLIP)\nand image-to-text retrieval precision on Flickr30K\n(Young et al., 2014) (for which ALBEF claims to\noutperform CLIP), do not reflect the performance\nof image ad understanding well.\nWe hypothesize that this is partly because image\nAcc \u00d2\nRank \u00d3\nm. Rank \u00d3\nViLBERT (Lu et al., 2019)\n61.8\n1.860\n4.190\nVS (v1) (Dey et al., 2021)\n86.8\n1.264\n3.072\nBERT-FT (Kalra et al., 2020)\n89.7\n1.230\n2.982\nALBEF (Li et al., 2021)\n57.6\n2.220\n4.935\nALBEF (ft. on Flickr30k)\n64.2\n2.242\n5.125\nALBEF (ft. on MSCOCO)\n64.0\n2.002\n4.651\nLiT (L16L) (Zhai et al., 2022)\n64.0\n1.849\n4.268\nCLIP (ViT-B/32) (Radford et al., 2021)\n88.1\n1.213\n2.937\nCLIP (ViT-B/16)\n92.2\n1.123\n2.694\nCLIP (ViT-L/14@336px)\n95.2\n1.069\n2.547\nKAFA (ours)\n97.4\n1.033\n2.391\nTable 1:\nZero-shot VLM performance on the Pitt\nDataset (Hussain et al., 2017) with its official eval pro-\ntocol (3 positive texts and 12 negative ones for each\ntest image). The best CLIP model already surpasses\nprevious state-of-the-art results (BERT-FT). The size\nof the data and model used in VLM pretraining have a\nhuge impact on the results. See Sec. 3 for details of the\nmetrics. For completeness, we also include the results\nof our proposed method (KAFA) here.\nad understanding requires knowledge of real-world\nentities (e.g., brands) which the pre-trained models\ncontain. Similar to the dramatic performance ad-\nvancement of GPT language models (Brown et al.,\n2020) driven by the larger scale of training data\nand the model capacity, more knowledge can be\ndistilled and implicitly stored in the weights of\npre-trained VLMs with larger models and more\npre-training data. We empirically verify that the\nVLM\u2019s capability of recognizing brands from im-\nages is aligned with its performance of decoding\nthe messages from the ads. See results in Tab. 4.\n4\nChallenges in VLM Adaptations to\nImage Ads and An Intuitive Solution\nWith CLIP as the clear champion, we further study\nVLM adaptations for image ad understanding using\nthe best CLIP model (ViT-L/14@336px) as the\nbackbone. We aim to enable better performance\nfor image ad understanding by better adapting pre-\ntrained VLMs to the image ad domain with the\nhelp of additional information such as scene-texts\nextracted from the image.\n4.1\nThe Issue of Overfitting and High\nComputation Complexity\nWe find two practical challenges in adapting pre-\ntrained VLMs to the image ads, first, the overfitting\nissue in fine-tuning due to limited image ads and\nthe lack of a strong supervision signal, and second,\nthe high computation burden caused by solutions\nto the previous challenge.\nAnnotations of image ads are hard to obtain\nin general (Akula et al., 2022), making it com-\nmon to only have limited training data (e.g., the\nPitt Dataset only contains 51,223 image-text pairs).\nThis results in VLM\u2019s vulnerability to overfitting\nduring adaptation. We find that directly fine-tuning\nCLIP contrastively on the Pitt Dataset with the sym-\nmetric cross-entropy loss (as in the original CLIP\npaper) gives worse performance than the zero-shot\none unless we adopt early stopping and a carefully\ntuned learning rate schedule. Moreover, as reported\nin Tab. 1, the best zero-shot performance of CLIP\nalready surpasses the previous state-of-the-art and\nis very close to 100%, leading to very weak su-\npervision signals for vanilla fine-tuning. We thus\nneed strong training signals. To save GPU memory\nrequired by much larger batch sizes, we adopt hard\nnegative mining (Xuan et al., 2020), which selects\nhard negatives from a very large candidate set as\nopposed to within the mini-batch.\nHowever, hard negative mining (HNM) strate-\ngies usually incur a large computation burden. In\nfully online hard negative mining (denoted full\nHNM), for each training image x and the corre-\nsponding texts y, we first rank Ncand negative\ntexts ty|y \u2030 yu sampled from the entire training\ndata according to the online similarity scores (the\ndot-product score fIpxq \u00a8 fT pyq computed from\nthe current VLM model), and then we choose the\nNhard \u00b4 1 most similar y as the hard negatives.\nWhile this essentially constructs a much harder\ncandidate set Cpxq, it requires the computation of\nfeatures of all training texts at every gradient step,\nwhich is prohibitively expensive. Existing meth-\nods propose to reduce the complexity by keeping a\nsparsely updated bank of all training text features\n(memory bank) (Wu et al., 2018) or with the help\nof a momentum-updated text encoder (MoCo) (He\net al., 2020). Nevertheless, we tailor these methods\nto our setup1 and find that they perform worse than\nfull HNM. We report the accuracy (%) in Tab. 2\nwith a harder eval protocol than the official one\nby using larger numbers (K) of negative samples\nrandomly drawn from the test texts (thus a set of\nharder negatives).\nWe believe this is because image ad understand-\ning requires fine-grained information extraction\n(e.g., the specific brand of a product) and both these\n1We use these methods to compute similarity scores but\nstill only select the hardest negatives for fine-tuning to save\nGPU memory (the purpose of HNM).\nNumber of Candidates K\n20\n100\n500\n1000\nZero-shot\n91.7\n80.7\n64.4\n56.5\nDirect FT\n92.4\n82.2\n66.7\n59.0\nDirect FT + memory bank\n92.8\n82.9\n67.5\n60.3\nDirect FT + MoCo\n93.3\n83.8\n69.8\n62.4\nDirect FT + full HNM\n93.7\n84.6\n70.0\n62.9\nTable 2: Accuracy (%) reported with different sizes (K)\nof the candidate set on the test set of the Pitt Dataset.\nThe larger K means harder negative samples. Zero-shot\nis the zero-shot performance of the best CLIP model.\nFT means fine-tuning the best CLIP model.\ntwo strategies are subject to the destruction of such\ninformation as they compute the loss not in a fully\nonline manner. In particular, their text features used\nfor contrastive fine-tuning always come from differ-\nent VLM encoders, either the past checkpoints or\nthe momentum-updated versions). Although direct\nfine-tuning with full HNM outperforms the others,\nit is extremely inefficient and thus impractical.\n4.2\nFeature Adaptation as the Solution\nWe propose a simple and intuitive solution,\nattention-based feature adaptors, that both handle\nthe aforementioned issues during adaptations and\nenable incorporating additional information (e.g.,\nscene-texts) for better image ad understanding.\nFeature adapters are recently proposed (Gao\net al., 2021; Zhang et al., 2021; Pantazis et al., 2022;\nZhao et al., 2022) as a line of very efficient adapta-\ntion strategies of VLMs. They freeze the weights\nof the pretrained VLMs, pre-compute features us-\ning their encoders, and use additional lightweight\nadapter networks to process these features. As a\nresult, on-the-fly feature computation over a mas-\nsive candidate set becomes computationally feasi-\nble and so is the fully online hard negative mining,\nsince we only compute the adapted features online\nvia a lightweight network. More efficiently, we can\nset the text adapter to an identity function (i.e., only\nuse adapters for image features).\nMore importantly, feature adapters are suitable\nfor fusing info from multiple sources. While previ-\nous feature adapters are mostly designed for image\nclassification, we consider it as a strategy to aggre-\ngate multiple input branches (of potentially differ-\nent modalities). For instance, previous methods for\nimage ad understanding, such as VS (Dey et al.,\n2021), utilize scene-texts extracted from images\n(by OCR) to enhance its performance. Similarly,\nwe can extract text features from scene-texts using\na VLM\u2019s text encoder and merge them with the\nFigure 3: Illustration of our brand understanding module that is an ensemble of text-matching and vision-based\nrecognition. Given an input image ad, we use MAVL to propose regions by prompting \u201call trademarks\u201d and\nretrieve entries in BrandSet-110K over the regions with CLIP. We aggregate the predictions across regions and the\ntext-matching results to generate the final output via some simple rules (see details in Appendix B).\nimage features extracted by the image encoder (of\nthe same VLM) via a feature adapter. In doing so,\nwe obtain a better representation of image ads.\nSpecifically, we propose to adopt one layer\nof multi-head attention (Vaswani et al., 2017) as\nour feature adapter design, similar to the Tiny-\nAttention Adapter (Zhao et al., 2022). Here the in-\nput sequence to the attention layer varies by modal-\nities (brand, scene-texts and image, as in Fig. 4)\ninstead of temporally or spatially as commonly in\nTransformers. By the nature of alignment-based\nVLMs, all information (whether in the text for-\nmat as the scene-texts or the visual elements) are\nembedded as vectors and lie in a shared semantic\nspace. We then utilize this property and fuse com-\nplementary information (e.g., image features and\nscene-text features) into one feature. Moreover, we\nappend a linear layer after the attention features\nand equip it with a residual connection. Let us\nuse the notation in previous sections and further\ndenote xst as the scene-texts extracted from the\nimage x (by Google OCR APIs). Then our adapter\nis represented as\nfattpxq \u201c npfIpxq ` ArfIpxq, fT pxstq, ...sr0sq\nwhere np\u00a8q is a normalization function and A is\nmulti-head attention (we leave room for other input\nbranches by leaving \u201c...\u201d here). Note that we do not\nuse any adapter for the text descriptions of images\n(the labels of image ads), which further reduces the\ncomputation complexity as now we only need to\ncompute and cache all text features in the training\nset once and for all during full HNM.\nIn comparison, we also evaluate the popular\nCLIP-Adapter (Gao et al., 2021) as a strong base-\nline, which we tailor to our setup by training three\n2-layer residual MLPs. Please see the Appendix\nfor implementation details. As reported in Tab. 3,\nour proposal of using an attention-based adapter\n(denoted KAFA w/o K) utilizes VLM features\nwell by aligning multimodal features already in\nthe same semantic space and outperforms CLIP-\nAdapter. While other existing work (Shen et al.,\n2021; Gui et al., 2021) merges multiple branches\nof information by leveraging foundation models,\nthey rely on large encoder-decoder networks that\nare computationally intensive and might not work\nwell with limited training data as in our case.\n5\nImproving Image Ad Understanding\nwith External Knowledge\nTo further improve image ad understanding, we pro-\npose to leverage external knowledge of real-world\nentities, namely product and brand information.\nThe major focus of advertisements is to promote\nbrand awareness (Macdonald et al., 2003). Some-\ntimes brand information is even a necessity to in-\nterpret ads correctly since it eliminates ambiguities\nand gives visual cues to the audiences (e.g., the ad\nfor a cleaning product in Fig. 2). It is then natural\nto empower feature adapters introduced previously\nwith a brand understanding module that extracts\nbrand information from images. Here we present\nour training-free brand understanding module that\nconsiderably exploits VLMs.\n5.1\nBrand Understanding Module\nExtracting brand information from an image is very\nchallenging due to the sheer scale of brands in the\nreal world. Existing published work (Su et al.,\n2018; Li et al., 2022a) and even commercial APIs\ntend to fall short of a good coverage. To solve this\nFigure 4: The overall training pipeline of our proposed KAFA, where three branches of information are fed into the\nattention-based feature adapter, the only neural module free in the fine-tuning process. We leverage VLM encoders\nfor both sides of the contrastive fine-tuning.\nissue, we construct a knowledge base that covers\nbrands much better than existing datasets. Our\nknowledge base has the format, KFC: KFC is a\nfast food chain, with around 110k entries covering\nnames of brands, companies, organizations and\nothers appearing in image ads. We call this dataset\nBrandSet-110K (see details in Appendix B).\nNext, we take an ensemble approach to detect\nand retrieve relevant brand entries from BrandSet-\n110K given an image ad. On one hand, we re-\ntrieve brands by performing string matching over\nall names in BandSet-110K using the scene-texts\nextracted by OCR from the image. On the other\nhand, in case of OCR failures, no detection (some\nlogos have no texts), or multiple detected entries\n(potentially false positives as most image ads pro-\nmote only one brand at a time), we use a more\npowerful vision-based module. Specifically, we\nadopt MAVL (Maaz et al., 2022), a state-of-the-art\nVLM, to propose object regions according to the\ntext prompt \u201call trademarks\u201d, We then use the best\nCLIP model to perform region classification based\non a set of carefully engineered prompts. And then,\nwe select the best entries in BrandSet-110K accord-\ning to the proposed regions. We finally use some\nsimple rules to combine the retrieved results from\ntext-matching and the vision-based module, as in\nFig. 3 (see details in the Appendix).\nOverall, our brand understanding module is\ntraining-free, covers much more entities than previ-\nously published work, and even outperforms some\ncommercial logo detection APIs by evaluation on\na small validation set, as reported in Tab. 4\n5.2\nOverall Pipeline and Final Results\nCombining with our proposed brand understanding\nmodule, we illustrate our overall pipeline in Fig. 4\nand call this approach knowledge-augmented fea-\nture adaptation (KAFA). In Tab. 3, we demonstrate\nthat KAFA achieves substantial improvements in\nimage ad understanding over the VLM baseline\nMethod\nInputs\n20\n100\n500\n1000\nZero-shot\nI\n91.7\n80.7\n64.4\n56.5\nDirect FT + full HNM\nI\n93.7\n84.6\n70.0\n62.9\nCLIP-Adapter\nI+ST\n93.9\n85.0\n70.2\n62.8\nKAFA w/o K\nI+ST\n95.0\n86.8\n72.7\n65.1\nKAFA w/o ST\nI+K\n94.7\n86.5\n72.3\n64.5\nKAFA (ours)\nI+ST+K\n95.6\n87.7\n73.9\n66.0\nTable 3: Accuracy (%) reported on the Pitt Dataset.\nKAFA (our proposed attention-based adapter with ex-\nternal knowledge) achieves the best results compared to\nother approaches and the versions with fewer inputs (K\n= brand knowledge, ST = scene-texts, I = image). Note:\n\u201cDirect FT + full HN\u201d is extremely inefficient.\nAcc (%)\nAcc (%)\nVLM-based (ALBEF)\n14.5\nText-matching\n36.0\nVLM-based (LiT)\n29.0\nGoogle Cloud API\n42.0\nVLM-based (CLIP)\n64.4\nCombined (Text + CLIP)\n66.6\nTable 4: Brand recognition accuracy on ~600 validation\nimage ads. It justifies our brand understanding module\nand further verifies that models better at recognizing\nbrands are better at image ad understanding.\nand consistently outperforms other ablation ver-\nsions with fewer inputs, justifying that our pro-\nposed brand understanding module helps to further\nimprove image ad understanding. We present an\nexample in Fig. 1 to illustrate the improvement\nof our method over the baseline, where for better\ndisplay we only show 2 negative text descriptions.\nSee more examples in Appendix G.\n6\nAdditional Analysis\n6.1\nHard Negative Samples in Evaluations\nWe report our main results with a harder eval pro-\ntocol than the official one. In fact, it is a challenge\nto perform effective evaluations in retrieval tasks\n(Akula et al., 2020). While we need hard negatives\nto better reflect the capabilities of a model, usually\nby increasing the size of the candidate set, we also\nwant those hard negatives to be real negatives. As\nillustrated in Fig. 5 (right), two companies can have\ntwo different image ads that share a very similar\nFigure 5: (Left) Similarity distributions of texts of the\nsame and across images. Both are spread out with no\neasy cutoff threshold to sample hard negatives. (Right)\nTwo different ads share the same message \u201cI should\ndrive this car because it can drive anywhere\u201d, exempli-\nfying the difficulty of sampling hard negative samples.\nmessage. Hence, given an image, simply using a\ntext of another as the negative might not work.\nThere is no easy solution. We can use a generic\nsentence encoder to measure similarities among\ndifferent texts in (Hussain et al., 2017) and only\nsample texts that are semantically different from\nthe target one (the ground truth) as negatives. We\nadopt a strong sentence encoder (publicly avail-\nable here) based on MiniLM (Wang et al., 2020) to\nmeasure semantic similarities. We compute similar-\nities among descriptions of the same ad and those\nacross different ads. The similarity distributions are\nspread out, as demonstrated in Fig. 5 (left), without\neasy cutoff thresholds to make negative samples\nboth hard and truly negative. Instead, we propose\nto use several different sizes K of the candidate set\nwith K \u201c 20, 100, 500, 1000. For each image in\nthe Pitt Dataset (Hussain et al., 2017), we randomly\nchoose a text from the ground truth and uniformly\nsample K \u00b4 1 negatives from other images (harder\nnegatives with larger K).\nWhile most existing methods evaluate (Hussain\net al., 2017) with the official evaluation protocol\n(for ease of comparison we also provide results by\nthis protocol in Tab. 1), it suffers from the lack of\nhard negatives. Each image ad comes with only\n15 randomly sampled candidate texts including 3\npositives, giving a random model a 20% accuracy.\nMoreover, negatives are easy as they tend to be\nsemantically distinct from the positives, making\nit hard to examine a model at finer levels. We\nprovide examples to compare negatives sampled in\nour protocol and in the official one in Appendix E.\n6.2\nData Leakage Regarding VLMs\nThe CLIP (Radford et al., 2021) model we use in\nour experiments was pre-trained on a tremendous\nFigure 6: An evaluation image and a found one in\nLAION-400M. As a reference, this image\u2019s caption\nreads: I should drink \u201cBrand Name\u201d because it\u2019ll give\nme a recharge of energy.\namount (400M) of image-text pairs on the Inter-\nnet. A concern is that there might be data leakage,\ni.e., the pre-trained VLMs might have already seen\nimages in the evaluation set, leading to inflated re-\nsults. We perform an analysis to conclude that this\nis unlikely the case. We manually inspect images\nin the LAION-400M dataset (Schuhmann et al.,\n2021) that are semantically similar to a set of ran-\ndomly sampled 100 eval image-text pairs. While\nthe dataset used to train CLIP is not publicly re-\nleased, LAION-400M is a very close one with a\nsimilar scale of data filtered by the CLIP model.\nSpecifically, for each of the 100 random samples,\nwe use the open-sourced CLIP-retrieval tool (here)\nto find the closest images from LAION-400M in-\ndexed by both the sample text and image. We do\nnot find any substantially overlapped content or\nnear duplicates (see Fig. 6 as an example). More-\nover, our proposed method achieves significant per-\nformance improvement over the VLM baseline and\nboth are based on the same CLIP model. Therefore,\ndata leakage is less of a concern.\n7\nConclusion\nIn this paper, we study the adaptation of pretrained\nalignment-based VLMs for the challenging image\nad understanding task. We benchmark and reveal\npractical challenges in adapting VLMs, propose a\nsimple and intuitive (yet effective) strategy for fea-\nture adaptations, and further improve image ad un-\nderstanding with external brand knowledge. While\nwe mainly focus on the image-to-text retrieval task\nfor its simplicity, we believe further studies can ex-\ntend it to directly generating text descriptions given\nimage ads or even generating image ads given the\ndescriptions. We hope our study draws more atten-\ntion to image ad understanding that are relevant to\nthe advertising industry and provide insights for a\nbroader machine learning community.\nLimitations\nThe data from the Pitt Dataset (Hussain et al.,\n2017), while useful for our paper, contains many\nimages and annotations that may perpetuate harm-\nful stereotypes according to sensitive characteris-\ntics such as gender and carry the risk of amplifica-\ntion by machine learning models. We plan to col-\nlaborate with AI robustness researchers to identify\nsuch examples and develop methods for improving\nML models in terms of robustness and reliability.\nReferences\nReneh Abokhoza, Sherehan Hamdalla Mohamed, and\nSumit Narula. 2019. How advertising reflects culture\nand values: A qualitative analysis study. Journal of\nContent, Community and Communication, 10(9):3.\nArjun Akula, Spandana Gella, Yaser Al-Onaizan, Song-\nchun Zhu, and Siva Reddy. 2020.\nWords aren\u2019t\nenough, their order matters: On the robustness of\ngrounding visual referring expressions. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 6555\u20136565.\nArjun R Akula, Brendan Driscoll, Pradyumna Narayana,\nSoravit Changpinyo, Zhiwei Jia, Suyash Damle,\nGarima Pruthi, Sugato Basu, Leonidas Guibas,\nWilliam T Freeman, et al. 2022. Metaclue: Towards\ncomprehensive visual metaphors research.\narXiv\npreprint arXiv:2212.09898.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: a visual language model for\nfew-shot learning. arXiv preprint arXiv:2204.14198.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nLydia B Chilton, Savvas Petridis, and Maneesh\nAgrawala. 2019. Visiblends: A flexible workflow\nfor visual blends. In Proceedings of the 2019 CHI\nConference on Human Factors in Computing Systems,\npages 1\u201314.\nArka Ujjal Dey, Suman K Ghosh, Ernest Valveny, and\nGaurav Harit. 2021. Beyond visual semantics: Ex-\nploring the role of scene text in image understanding.\nPattern Recognition Letters, 149:164\u2013171.\nPeng Gao, Shijie Geng, Renrui Zhang, Teli Ma,\nRongyao Fang, Yongfeng Zhang, Hongsheng Li,\nand Yu Qiao. 2021.\nClip-adapter: Better vision-\nlanguage models with feature adapters.\narXiv\npreprint arXiv:2110.04544.\nFran\u00e7ois Gard\u00e8res, Maryam Ziaeefard, Baptiste Abe-\nloos, and Freddy Lecue. 2020.\nConceptbert:\nConcept-aware representation for visual question an-\nswering. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 489\u2013498.\nLiangke Gui, Borui Wang, Qiuyuan Huang, Alex Haupt-\nmann, Yonatan Bisk, and Jianfeng Gao. 2021. Kat:\nA knowledge augmented transformer for vision-and-\nlanguage. arXiv preprint arXiv:2112.08614.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\nRoss Girshick. 2020. Momentum contrast for unsu-\npervised visual representation learning. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 9729\u20139738.\nZaeem Hussain, Mingda Zhang, Xiaozhong Zhang,\nKeren Ye, Christopher Thomas, Zuha Agha, Nathan\nOng, and Adriana Kovashka. 2017. Automatic un-\nderstanding of image and video advertisements. In\nProceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1705\u20131715.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen\nLi, and Tom Duerig. 2021. Scaling up visual and\nvision-language representation learning with noisy\ntext supervision.\nIn International Conference on\nMachine Learning, pages 4904\u20134916. PMLR.\nKanika Kalra, Bhargav Kurma, Silpa Vadakkeeveetil\nSreelatha, Manasi Patwardhan, and Shirish Karande.\n2020. Understanding advertisements with bert. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7542\u2013\n7547.\nSu Young Kim, Hyeonjin Park, Kyuyong Shin, and\nKyung-Min Kim. 2022.\nAsk me what you need:\nProduct retrieval using knowledge from gpt-3. arXiv\npreprint arXiv:2207.02516.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nChenge Li, Istv\u00e1n Feh\u00e9rv\u00e1ri, Xiaonan Zhao, Ives\nMacedo, and Srikar Appalaraju. 2022a. Seetek: Very\nlarge-scale open-set logo recognition with text-aware\nmetric learning. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vi-\nsion, pages 2544\u20132553.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in neural information processing systems,\n34:9694\u20139705.\nLiunian Harold Li, Pengchuan Zhang, Haotian Zhang,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan\nWang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al.\n2022b. Grounded language-image pre-training. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10965\u2013\n10975.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. Ad-\nvances in neural information processing systems, 32.\nMuhammad Maaz, Hanoona Rasheed, Salman Khan,\nFahad Shahbaz Khan, Rao Muhammad Anwer, and\nMing-Hsuan Yang. 2022. Class-agnostic object de-\ntection with multi-modal transformer. In The Euro-\npean Conference on Computer Vision. Springer.\nEmma Macdonald, Byron Sharp, et al. 2003. Manage-\nment perceptions of the importance of brand aware-\nness as an indication of advertising effectiveness.\nPh.D. thesis, Massey University, Department of Mar-\nketing.\nKenneth Marino, Xinlei Chen, Devi Parikh, Abhinav\nGupta, and Marcus Rohrbach. 2021. Krisp: Inte-\ngrating implicit and symbolic knowledge for open-\ndomain knowledge-based vqa. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14111\u201314121.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition, pages\n3195\u20133204.\nOmiros Pantazis, Gabriel Brostow, Kate Jones, and\nOisin Mac Aodha. 2022.\nSvl-adapter:\nSelf-\nsupervised adapter for vision-language pretrained\nmodels. arXiv preprint arXiv:2210.03794.\nSavvas Petridis and Lydia B Chilton. 2019. Human\nerrors in interpreting visual metaphor. In Proceed-\nings of the 2019 on Creativity and Cognition, pages\n187\u2013197.\nHieu Pham,\nZihang Dai,\nGolnaz Ghiasi,\nKenji\nKawaguchi, Hanxiao Liu, Adams Wei Yu, Jiahui\nYu, Yi-Ting Chen, Minh-Thang Luong, Yonghui\nWu, et al. 2021.\nCombined scaling for open-\nvocabulary image classification.\narXiv preprint\narXiv: 2111.10050.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021.\nLearning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748\u20138763.\nPMLR.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,\nSanjeev Satheesh, Sean Ma, Zhiheng Huang, An-\ndrej Karpathy, Aditya Khosla, Michael Bernstein,\net al. 2015. Imagenet large scale visual recognition\nchallenge. International journal of computer vision,\n115(3):211\u2013252.\nChristoph Schuhmann, Romain Beaumont, Richard\nVencu, Cade Gordon, Ross Wightman, Mehdi Cherti,\nTheo Coombes, Aarush Katta, Clayton Mullis,\nMitchell Wortsman, et al. 2022. Laion-5b: An open\nlarge-scale dataset for training next generation image-\ntext models. arXiv preprint arXiv:2210.08402.\nChristoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. 2021. Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs. arXiv preprint\narXiv:2111.02114.\nSheng Shen, Liunian Harold Li, Hao Tan, Mohit\nBansal, Anna Rohrbach, Kai-Wei Chang, Zhewei\nYao, and Kurt Keutzer. 2021. How much can clip\nbenefit vision-and-language tasks? arXiv preprint\narXiv:2107.06383.\nAmanpreet Singh,\nVivek Natarajan,\nMeet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards vqa models\nthat can read. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition,\npages 8317\u20138326.\nHaoyu Song, Li Dong, Wei-Nan Zhang, Ting Liu, and\nFuru Wei. 2022. Clip models are few-shot learners:\nEmpirical studies on vqa and visual entailment. arXiv\npreprint arXiv:2203.07190.\nHang Su, Shaogang Gong, and Xiatian Zhu. 2018. Scal-\nable deep learning logo detection. arXiv preprint\narXiv:1803.11417.\nYi-Lin Sung, Jaemin Cho, and Mohit Bansal. 2022.\nVl-adapter: Parameter-efficient transfer learning for\nvision-and-language tasks. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 5227\u20135237.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and\nAnton Van Den Hengel. 2017. Fvqa: Fact-based\nvisual question answering. IEEE transactions on pat-\ntern analysis and machine intelligence, 40(10):2413\u2013\n2427.\nPeng Wang, Qi Wu, Chunhua Shen, Anton van den Hen-\ngel, and Anthony Dick. 2015. Explicit knowledge-\nbased reasoning for visual question answering. arXiv\npreprint arXiv:1511.02570.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776\u20135788.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface\u2019s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua\nLin. 2018. Unsupervised feature learning via non-\nparametric instance discrimination. In Proceedings\nof the IEEE conference on computer vision and pat-\ntern recognition, pages 3733\u20133742.\nHong Xuan, Abby Stylianou, Xiaotong Liu, and Robert\nPless. 2020. Hard negative examples are hard, but\nuseful. In European Conference on Computer Vision,\npages 126\u2013142. Springer.\nYuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu,\nTat-Seng Chua, and Maosong Sun. 2021. Cpt: Col-\norful prompt tuning for pre-trained vision-language\nmodels. arXiv preprint arXiv:2109.11797.\nKeren Ye and Adriana Kovashka. 2018. Advise: Sym-\nbolism and external knowledge for decoding adver-\ntisements. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 837\u2013855.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67\u201378.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,\nXiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong\nHuang, Boxin Li, Chunyuan Li, et al. 2021. Florence:\nA new foundation model for computer vision. arXiv\npreprint arXiv:2111.11432.\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas\nSteiner, Daniel Keysers, Alexander Kolesnikov, and\nLucas Beyer. 2022.\nLit: Zero-shot transfer with\nlocked-image text tuning.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 18123\u201318133.\nRenrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng\nLi. 2021. Tip-adapter: Training-free clip-adapter\nfor better vision-language modeling. arXiv preprint\narXiv:2111.03930.\nHongyu Zhao, Hao Tan, and Hongyuan Mei. 2022.\nTiny-attention adapter: Contexts are more impor-\ntant than the number of parameters. arXiv preprint\narXiv:2211.01979.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022a.\nConditional prompt learning\nfor vision-language models. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 16816\u201316825.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and\nZiwei Liu. 2022b. Learning to prompt for vision-\nlanguage models. International Journal of Computer\nVision, 130(9):2337\u20132348.\nA\nScene-text Extraction by OCR\nIn our paper, we use scene-texts as one of the in-\nputs for experiments in Pitt Dataset (Hussain et al.,\n2017). We use the Google Cloud OCR API (link)\nto extract all text tokens, which are grouped by\nparagraphs by the API. We then group paragraphs\ninto blocks by simple heuristic rules (e.g., two con-\nsecutive paragraphs with similar font sizes should\nbe considered in the same block) and then filter out\nthose blocks with an average prediction confidence\nscore (provided by the API) less than 0.7.\nB\nBrand Recognition\nB.1\nBrandSet-110K\nWe construct BrandSet-110K by first compiling en-\ntries from public websites. Specifically, for the list\nof topics (such as automobiles and healthcare) in\nthe Pitt Dataset (Hussain et al., 2017), we Google\nwith the query \u201cTop XX brands/companies in Y\u201d to\nobtain a list of thousands of common brands, orga-\nnizations, etc., denote source I. We further scrape\nthe Google Knowledge Graph Search API (link)\nto find a much larger list of named entities, de-\nnoted source II, whose categories fall into \u201cbrands\u201d,\n\u201ccompanies\u201d, etc., where each entry comes with a\none-paragraph description. Since results from the\nKnowledge Graph (KG) is a little bit noisy and\nmight miss some popular entities, we rely on source\nI to make sure that the most prevalent entities ap-\npearing in our commercial world are included in\nour dataset. We then query entries from source I in\nKG to also obtain the descriptions. If such entries\nare not found in KG, we simply use the descriptions\n\u201cX is a brand name in the industry of Y\u201d. Together\nwith source II, we obtain a raw combined knowl-\nedge base. Then we filter out those entries that\nare common English words (if the entry appears\nin the English dictionary (link) or a word set from\nNLTK (link)). We do so to remove entries such\nas \u201cEveryday\u201d, which will result in too many false\npositives during brand detection. We also remove\nentries consisting of a single character. Eventually,\nwe end up with around 110K entries, i.e., name and\ndescription pairs.\nSince the descriptions returned by KG can be\nquite long, we further use a learning-based sen-\ntence parser to only select the very first sentence\nof the description (usually in the format of \u201cX is\na brand/company/org in the industry of Y with Z\nfeatures\u201d). We use this API (link) from Hugging\nFace (Wolf et al., 2019), which is based on spaCy.\nB.2\nBrand Recognition by Text-Matching\nThe text-based brand recognition module essen-\ntially performs text matching to exhaustively\nsearch over all entries in BrandSet-110K given the\nscene-texts extracted by OCR. For each name in\nBrandSet-110K that is larger than 6 characters, we\nmatch the text in a case-insensitive manner; other-\nwise, we match it case-sensitively to reduce false\npositives. A name is set to be matched in a scene-\ntext if it is a phrase of the text (\u201cabc\u201d is matched\nin \u201cabc def\u201d but not in \u201cabcdef\u201d.) When doing\nablation studies of evaluating text-matching only\nperformance, in case of multiple predictions we\nrandomly select one as the output.\nB.3\nVision-based Brand Recognition\nThe vision-based brand recognition module han-\ndles situations where the text-based one fails (when\ntexts are too small or blurred or artistic for OCR\nto work; or when logos are purely graphic). The\nvision-based module is a pipeline of several steps.\nThe class-agnostic region proposal (we use the best\nmodel in MAVL (Maaz et al., 2022), a state-of-the-\nart model) is adopted to generate candidate regions\nthat contain brand logos or visual elements reveal-\ning brand information. We choose \u201call trademarks\u201d\nas the best prompt with other candidates such as:\n\u2022 \u201call small objects\u201d, \u201call brand logos\u201d,\n\u2022 \u201call brand icons\u201d, \u201call brands\u201d, \u201call logos\u201d\nAfter the region proposal, we use the best CLIP\n(Radford et al., 2021) model (its visual encoder)\nto compute the region features. We include the\nentire image as an extra proposed region. Then we\nuse the text features (via the CLIP text encoder) of\nthe following 6 prompts to find the best entry in\nBrandSet-110K. Namely\n\u2022 \u201cA brand logo of X\u201d, \u201cA logo of X\u201d,\n\u2022 \u201cA trademark of X\u201d,\u201cA brand logo of X. Y\u201d,\n\u2022 \u201cA logo of X. Y\u201d, \u201cA trademark of X. Y\u201d\nwhere X is the name and Y is the corresponding\ndescription in BrandSet-110K. We first average dot\nproducts of the region features and brand features\nacross all 6 prompts. We then find two candidates:\n(1) the name X with the largest predicted scores\namong all names and all regions of an image and\n(2) the name X with the largest predicted scores av-\neraged across all regions among all names that are\nchampions in at least one region. Our final output\nis chosen by the higher value of the dot products of\nthe global image feature and the two text features\nof the prompt \u201c\u2019An advertisement of X\u201d (we select\nthis prompt after another minor prompt engineering\nprocess).\nB.4\nEnsemble of Text-matching and\nVision-based Brand Recognition\nWe use simple heuristic rules to ensemble the\ntext-matching results and the vision-based ones.\nSpecifically, if there is no name detected from text-\nmatching, we return the vision-based result; if there\nis only one name detected from text-matching, we\nreturn the text-based result; if more than one name\nis detected from text-matching, we select the name\nfrom detection of both text and vision-based mod-\nules by the highest value of the dot product of the\nglobal image feature and the text features of \u201c\u2019An\nadvertisement of X\u201d. The ensemble module finally\nreturns the single name and the corresponding de-\nscription in BrandSet-110K.\nC\nNetwork Architecture of\nAttention-based Feature Adapter\nWe adopt a very lightweight network for feature\nadaptation. For each modality of the inputs (e.g.,\ninputs to KAFA in the Pitt Dataset are three vec-\ntors: scene-text features, image features, and brand\nfeatures), we first add learnable positional embed-\nding (which is used to distinguish between different\nmodalities) and then apply a multi-head attention\nlayer (Vaswani et al., 2017) to obtain a list of vec-\ntors; we finally use the first vector (corresponding\nto the image feature input branch) and add residual\nconnections from the input image feature (before\npositional embedding) to produce the final output\n(with normalization). To make things clearer, we\nalso provide the pseudocode.\nimport\nt o r c h . nn . Parameter\nas\nparam\nimport\nt o r c h . nn . f u n c t i o n a l\nas F\n# args\ni s\na\nl i s t\nof\ni n p u t\nf e a t u r e s\n# e . g . ,\n[ img_fs , s c e n e _ t e x t _ f s , brand_fs ]\npos_emb_list = [ ]\nfor _ in\nrange ( n_input ) :\npos_emb_list . append (\nparam ( t o r c h . zeros ( [ input_d ] ) ) )\na t t n = t o r c h . nn . M u l t i h e a d A t t e n t i o n (\nembed_dim=input_d ,\nnum_heads =8 ,\nb a t c h _ f i r s t =True )\ni n p u t s = [ ]\nfor\ni\nin\nrange ( n_input ) :\ni n p u t s . append (\nargs [ i ] + pos_emb_list [ i ] )\nx = t o r c h . s t a c k ( inputs ,\n1)\nx ,\n_ = a t t n ( x ,\nx ,\nx ,\nneed_weights= False )\n# The\nf i r s t\ni s\nthe\nimage\nf e a t u r e s .\nx = x [ : ,\n0] + args [ 0 ]\nx = F . normalize ( x ,\ndim=\u22121)\nD\nData Cleaning on Pitt Dataset\nWe perform data cleaning on both the training and\nevaluation data of the Pitt Dataset (when evalu-\nated using the official evaluation protocol, whose\nissue is discussed in the main paper, we stick to the\nraw evaluation set). For every text in the dataset\n(the response to the \u201cwhat should I do according\nto the ad and why\u201d question), we remove invalid\nones (e.g., \u201cI don\u2019t know\u201d, \u201cnot an ad\u201d, \u201cnot sure\u201d),\nfix typos (e.g., \u201cbecasue\u201d, \u201cbecaues\u201d), and remove\nthose without answering the \u201cwhy\u201d question. Fur-\nthermore, we filter out texts that do not mention\nnouns or only have nouns that are not very infor-\nmative (we compile a list of non-informative nouns\nappearing frequently in the dataset, such as \u201cprod-\nuct\u201d, \u201cthing\u201d and \u201cvendor\u201d). This step is to remove\nnon-specific texts such as \u201cI should buy this prod-\nuct because ...\u201d. In the end, we randomly select one\ntext (with a fixed random seed) as the ground truth\nof its image. If an image has all its texts removed\nby data cleaning, we remove the image from the\ndataset. We find such images constituting less than\n3% of all images.\nE\nHard vs. Easy Negatives for Evaluation\nin Pitt Dataset\nHere we explain why we use larger number of\ncandidates K during evaluation. Model evalua-\ntion for cross-modal retrieval is challenging (Akula\net al., 2020). The official evaluation protocol in\nPitt Dataset suffers from the issue that it lacks hard\nnegatives to fully reflect the perception and reason-\ning capability of the models. Each image in the\nprotocol has 3 positive texts and only 12 negative\nones, giving a random guess model a 20% accuracy.\nOn the contrary, increasing the number of candi-\ndates in our evaluation protocol as introduced in\nthe main paper effectively yields harder negatives.\nFor instance, for the image ad in Fig. 7 whose\nground truth is \u201cI should buy a Brand A camera be-\ncause it will help me create\u201d, if we set the number\nof candidates to be 10 (i.e., 9 negatives), the best\nCLIP model makes the correct selection with all\neasy negatives, among which the most confusing\nones are\n\u2022 \u201cI should drink Brand B because it de-ages\nyou\u201d\n\u2022 \u201cI should not drown in my decision because\nbad choices will keep you under\u201d\n\u2022 \u201cI should buy this bag because it is resealable\u201d\nIf we set 50 total candidates (i.e., 49 negatives),\nagain the CLIP baseline predicts correctly with\nthe most confusing ones still being relatively easy\nnegatives:\n\u2022 \u201cI should use Brand C cosmetics because it\nmakes you beautiful\u201d\n\u2022 \u201cI should buy Brand D products because they\nare reliable\u201d\n\u2022 \u201cI should see a movie because it\u2019s fun\u201d\nFor a larger number (e.g., 100 total candidates),\nthe CLIP model starts to make mistakes, with hard\nnegatives such as\n\u2022 \u201cI should use Brand E makeup because it will\nmake me more seductive\u201d\n\u2022 \u201cI should buy Brand F makeup because it will\nmake me beautiful\u201d\n\u2022 \u201cI should buy this makeup because it will make\nme shine\u201d\nNotice that for privacy reasons, all brand names in\nthis example are anonymized.\nF\nTraining Details\nF.1\nDirect Fine-tuning of CLIP\nWe fine-tune the best CLIP model on the train-\ning images of Pitt Dataset with a batch size of 8,\nsymmetric cross-entropy loss (the one used in the\noriginal paper of CLIP) and the Adam optimizer\n(Kingma and Ba, 2014) with weight decay of 1e\u00b44.\nWe set other parameters of Adam as in the original\nimplementation of CLIP. We find that using a very\nsmall learning rate (e.g., 1e \u00b4 7) is necessary for\nfine-tuning CLIP on Pitt Dataset; otherwise, the\nCLIP model can overfit easily. For the same rea-\nson, we adopt early stopping and only fine-tune the\nmodel for a maximum of 4 epochs. We leave the de-\ntails in the next section for the fine-tuning version\nwith online hard negative mining (very computa-\ntionally intensive as suggested in the main paper).\nFigure 7: An example to illustrate the issue of easy\nnegative samples in evaluation.\nF.2\nFully Online Hard Negative Mining (full\nHNM)\nWhen performing hard negative mining during\ntraining, for each image in a mini-batch, we first\ncompute the VLM features of a large number of\nrandomly sampled negative texts (in our experi-\nments we find 1000 to be large enough; while a\nlarger number can marginally improve the final per-\nformance but it incurs a larger computation burden),\nthen we compute the dot products of the current\nimage feature and all these sampled text features,\nand finally, we rank the dot products and select the\ntop N \u00b4 1 negatives to be included in computing\nthe gradients of the loss (we find N \u201c 8 to be effec-\ntive). We use the asymmetric version of the cross-\nentropy loss (i.e., the normal one) compared to\nthe asymmetric version in CLIP pre-training since\nthe number of negatives per image does not equal\nthe batch size when HNM is adopted. We reduce\nthe batch size to 4 whenever with online HNM so\nthat directly fine-tuning the largest CLIP model is\nviable with a single V100 Nvidia GPU. We still\napply the learnable \u201clogit scale\u201d parameter in CLIP\npre-training which effectively makes contrastive\nlearning more stable.\nFor full HNM, if we directly fine-tune the CLIP\nmodel, we need to compute text features of all texts\nin the training set in every gradient step. While\nthis is computationally prohibitive, we adopt the\nfeature adapter strategy and thus cache all the text\nfeatures once and do not update the text encoder\nFigure 8: Additional examples that demonstrate KAFA\u2019s improvements over the VLM baseline.\nand the text features during fine-tuning.\nF.3\nMore Ablation Studies\nIn our experiments presented in the main paper\n(specifically in Tab. 1), we have justified the use of\nonline HNM, the additional inputs (scene-text and\nbrand information) to the feature adaptation, and\nthe advantages of the attention-based adapter over\nthe baseline adapter. We also perform experiments\non several variants of the attention-based feature\nadapter and find that either using more than one\nattention layer or adding layer norm & additional\nlinear projection as in the encoder-decoder Trans-\nformer (Vaswani et al., 2017) make the model more\nvulnerable to overfitting.\nF.4\nAdditional Details of Feature Adapters\nFor feature adapters (CLIP-Adapter and KAFA),\nwe use the full HNM for fine-tuning as discussed\nin the previous section. We use the same training\nsetup as that of \u201cDirect ft + HMN\u201d except for the\nadditional input branches. For CLIP-Adapter, we\ntailor it to our setup by training three 2-layer resid-\nual MLPs. Specifically, let as denote them as gmlp\nI\n,\ngmlp\nT\nand hmlp, built on top of the image and text\nfeatures extracted by VLMs, and a mixture of these\nfeatures, respectively. The adapted feature for x\nbecomes\nfmlp\nI\npxq \u201c npfIpxq ` gmlp\nI\npfIpxqqq\nfmlp\nT\npxstq \u201c npfT pxstq ` gmlp\nT\npfT pxstqqq\nfmlppxq \u201c nphmlppcatrfmlp\nI\npxq, fmlp\nT\npxstq, ...sqq\nwhere cat is concatenation. Here we omit the\nadapted feature for text label y. And the adapted\nfeature for the text label y becomes\nfmlp\nT\npyq \u201c npfT pyq ` gmlp\nT\npfT pyqqq\nwhich is used during full HNM for fine-tuning.\nFor fine-tuning of both CLIP-Adapter and\nKAFA, we find a much larger learning rate (i.e.,\n1e\u00b44) to be effective and train the model similarly\nwith early stopping and a maximum of 10 epochs.\nWe find it helpful to stabilize training by adding\nan additional regularization loss to keep the feature\nadapter\u2019s output close to the VLM image features.\nSpecifically, we add the negative of dot products\nbetween the two (averaged over all data points in\nthe mini-batch) to the overall training objective.\nFor this regularization term, we use a coefficient of\n5 in all our experiments in the Pitt Dataset.\nG\nAdditional Examples\nWe present 2 additional examples in Fig. 8 to il-\nlustrate the improvement of our method over the\nbaseline. Again, we only show 2 negative text de-\nscriptions for better display, and we anonymize all\nbrand info.\n"
  },
  {
    "title": "Make-An-Audio 2: Temporal-Enhanced Text-to-Audio Generation",
    "link": "https://arxiv.org/pdf/2305.18474.pdf",
    "upvote": "1",
    "text": "Make-An-Audio 2: Temporal-Enhanced\nText-to-Audio Generation\nJiawei Huang\u2217\u2020\nZhejiang University, ByteDance\nhuangjw@zju.edu.cn\nYi Ren\u2217\nByteDance\nren.yi@bytedance.com\nRongjie Huang\nZhejiang University\nrongjiehuang@zju.edu.cn\nDongchao Yang\nPeking University\n2001212832@stu.pku.edu.cn\nZhenhui Ye\u2020\nZhejiang University, ByteDance\nzhenhuiye@zju.edu.cn\nChen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma\nByteDance\n{zhangchen.0620,jinglinliu,yinxiang.stephen,mazejun}@bytedance.com\nZhou Zhao\u2021\nZhejiang University\nzhaozhou@zju.edu.cn\nAbstract\nLarge diffusion models have been successful in text-to-audio (T2A) synthesis tasks,\nbut they often suffer from common issues such as semantic misalignment and\npoor temporal consistency due to limited natural language understanding and data\nscarcity. Additionally, 2D spatial structures widely used in T2A works lead to\nunsatisfactory audio quality when generating variable-length audio samples since\nthey do not adequately prioritize temporal information. To address these challenges,\nwe propose Make-an-Audio 2, a latent diffusion-based T2A method that builds\non the success of Make-an-Audio. Our approach includes several techniques to\nimprove semantic alignment and temporal consistency: Firstly, we use pre-trained\nlarge language models (LLMs) to parse the text into structured <event & order>\npairs for better temporal information capture. We also introduce another structured-\ntext encoder to aid in learning semantic alignment during the diffusion denoising\nprocess. To improve the performance of variable length generation and enhance\nthe temporal information extraction, we design a feed-forward Transformer-based\ndiffusion denoiser. Finally, we use LLMs to augment and transform a large amount\nof audio-label data into audio-text datasets to alleviate the problem of scarcity of\ntemporal data. Extensive experiments show that our method outperforms baseline\nmodels in both objective and subjective metrics, and achieves significant gains in\ntemporal information understanding, semantic consistency, and sound quality.\n\u2217Equal contribution.\n\u2020Interns at ByteDance.\n\u2021Corresponding author\nPreprint. Under review.\narXiv:2305.18474v1  [cs.SD]  29 May 2023\n1\nIntroduction\nDeep generative learning models [9, 22, 12] have revolutionized the creation of digital content,\nenabling creators with no professional training to produce high-quality images [44, 45, 36], vivid\nvideos [14, 50], diverse styles of voice [17], and meaningful long textual spans [57, 37]. Professional\npractitioners can modify the generated content to accelerate their production workflows. Text-to-audio\nsynthesis (T2A) is a subcategory of generative tasks that aims to generate natural and accurate audio\nby taking text prompts as input. T2A can be useful in generating desired sound effects, music or\nspeech, and can be applied to various applications like movie sound effects making, virtual reality,\ngame development, and audio editing.\nThanks to the development of text-to-image synthesis (T2I) methods, researchers have successfully\nextended similar approaches to the text-to-audio synthesis domain [15, 29, 55, 25]. The success of\nthese methods has opened up numerous opportunities for generating high-quality audio content from\ntext. T2A systems typically use a text encoder to encode the audio\u2019s text input as condition embedding,\nthen employ diffusion models [15, 29, 55] to synthesis mel-spectrograms or utilize autoregressive\nmodels [25] to synthesis raw waveform data based on the condition embedding. However, previous\nT2A methods have some common issues: 1) Temporal disorder: when the text input is complex,\nwith multiple objects and temporal relationships between them, the generated audios often suffer\nfrom semantic misalignment and temporal disorder. For instance, audio captions such as \"The sound\nof A, followed by the sound of B\" may result in audios where A and B overlapping throughout,\nor B comes before A, or even only one sound is synthesized. 2) Poor variable-length results:\nprevious works [15] adopt the conventional U-Net structure of 2D convolution and spatial transformer\nstacking as the backbone of diffusion denoiser, which is typically trained with fixed-length audios.\nConsequently, they generate suboptimal results when synthesizing audio sequences of varying lengths\ncompared to those of the training data. In the meanwhile, 2D spatial structures are not good at\nextracting temporal information since they treat the time axis and frequency axis equally in the\nspectrogram generation process. 3) Insufficient temporal paired data: previous works use simple\nrule-based augmentation methods [6, 25] to create temporally aligned text-audio paired data from\naudio-label datasets. However, these patterns are overly simplistic and can hinder the model\u2019s ability\nto generalize to real-world sentences.\nIn this paper, based on previous successful work Make-An-Audio, we propose a novel temporal-\nenhanced text-to-audio generation framework called Make-An-Audio 2. The temporal information\ncan be better handled by our method in the following ways: 1) To address the semantic misalignment\nand temporal disorder, we use a pre-trained LLM to extract the audio caption\u2019s temporal information\nand parse the origin caption into structured <event & order> pairs with proper prompts. To encode\nthe structured pairs better, we introduce another structured-text encoder that takes the structured\npairs as its input to aid in learning semantic alignment during the diffusion denoising process. In\nthis way, we relieve the text encoder\u2019s burden of recognizing events with the corresponding temporal\ninformation and enable the T2A system to model the timing information of the events more effectively.\n2) To improve the generation quality of variable-length audio and enhance the temporal information\nunderstanding, we replace the 2D spatial structures with temporal feed-forward Transformer [53]\nand 1D-convolution stacks for the diffusion denoiser and support variable-length audio input in\ntraining. 3) To address the issue of insufficient temporally aligned audio-text paired dataset, we use\nsingle-labeled audio samples and their labels to compose complex audio and structured captions. We\nthen use LLM to augment the structured caption into natural language captions.\nWe conduct extensive experiments on AudioCaps and Clotho datasets, which reveals that our method\nsurpasses baseline models in both objective and subjective metrics, and achieves significant gains in\nunderstanding temporal information, maintaining semantic consistency, and enhancing sound quality.\nOur ablation studies further demonstrate the effectiveness of each of our techniques4.\n4Our demos are available at https://make-an-audio-2.github.io/\n2\n2\nRelated works\n2.1\nText-to-image generative models\nText-to-Image Synthesis (T2I) has garnered significant attention in recent years and has even been\ncommercialized. One pioneering work in this realm is DALL-E [43], which treats T2I generation as\na sequence-to-sequence translation task. DALL-E employs a pre-trained VQ-VAE [52] to encode\nimage patches to discrete codes, which are then combined with the text codes. During inference,\nthe model generates image codes autoregressively based on the text codes. As diffusion models\nexhibit greater potential with regard to both diversity and quality in image generation, they have\nbecome mainstream in T2I Synthesis. DALLE-2 [42] uses the CLIP [40] text encoder and two\ndiffusion models. The first diffusion model predicts CLIP visual features based on the CLIP text\nfeature, while the second synthesizes the image from the predicted CLIP visual features. A cascade\nof diffusion super-resolution models is then employed to increase the resolution of the generated\nimage. Another famous T2I work is Imagen [45], which utilizes the T5 encoder [41] to extract text\nfeatures, It employs a diffusion model to synthesize a low-resolution image and then applies a cascade\nof diffusion models for super-resolution. Latent Diffusion [44] enhances computational efficiency\nby using a continuous VAE that is trained with a discriminator to map images from pixel space to\ncompressed latent space. This is followed by diffusion on the latent space, which synthesizes images\u2019\nlatent.\n2.2\nText-to-audio synthesis\nText-to-Audio Synthesis is a rising task that has seen great advances recently. Diffsound [55] uses a\npre-trained VQ-VAE [52] trained on mel-spectrograms to convert audio into discrete codes, which\nare then used by a diffusion model to generate the audio codes. To improve its generalization\nability, the authors pre-trained Diffsound on the AudioSet dataset, which contains audio files labeled\nwith tags. Additionally, they introduce a random input masking technique to make use of these\ntags. AudioGen [25] is another system in this field that uses a similar VQ-VAE-based approach.\nHowever, it encodes raw waveform data into discrete codes and employs an autoregressive model\nto predict audio tokens based on text features. For data augmentation, AudioGen mixes audio files\nand concatenates their text captions. Make-An-Audio [15], AudioLDM [29], and TANGO [8] are all\nbased on the Latent Diffusion Model (LDM). With the assumption that CLAP can map the audio\nand its caption to the same latent space and approximate the text features based on the audio feature,\nAudioLDM uses audio features extracted by the CLAP model as the condition during training and\nutilizes text features during inference. In contrast, Make-An-Audio and TANGO employ text features\nboth in the training and inference stages. To overcome data scarcity, Make-An-Audio proposes a\npseudo prompt enhancement method, while TANGO introduces an audio mixture method based on\nhuman auditory perception.\n2.3\nLLM-based data augmentation\nRecent advancements in prompt learning [30] have greatly enhanced the capabilities of language\nmodels and given birth to very large language models with billions of parameters, enabling them to\nachieve natural language comprehension levels that are comparable to those of humans. OPT [57],\nChatGPT1 and GPT4 [37], are typical cases among them. This has led researchers to explore whether\nthese models can be used to annotate data instead of humans. In previous work, masked language\nmodels (MLM) like BERT [4] and Roberta [31] have been used for contextual augmentation [27] at\nthe word level. For example, researchers insert <mask> tokens into the text or replace some words\nwith <mask> tokens, and then use the MLM to predict the appropriate words. At the sentence level,\nback translation [48] and paraphrasing [26] methods have been used to increase the diversity of data.\nHowever, the limited capabilities of these models have resulted in insufficient quality and diversity of\nthe generated data. To address these limitations, recent research has explored the use of very large\nlanguage models for data augmentation. AugGPT [2], for instance, leverages ChatGPT to generate\nauxiliary samples for few-shot text classification. The quality of the generated data is much higher,\nresulting in double-digit improvements in sentence classification accuracy compared to previous data\naugmentation methods. WavCaps [33] crawls audio data with raw descriptions from multiple web\n1https://openai.com/blog/chatgpt\n3\nFigure 1: A high-level overview of Make-An-Audio 2. Note that modules printed with a lock are\nfrozen when training the T2A model.\nsources. However, the raw descriptions contain a high degree of noise, so ChatGPT is used to filter\nout extraneous information unrelated to the audio and generate high-quality captions based on labels.\nThis approach results in a large-scale, weakly-labeled audio captioning dataset.\n3\nPreliminary\nOur work is inspired by Make-An-Audio [15], which is one of the most successful T2A models. In\nthis section, we provide a preliminary discussion on Make-An-Audio. Denote an audio-text pair as\n(a, y) where a \u2208 RTa and Ta is the waveform length. To mitigate the complexity of modeling long\ncontinuous waveform data, they first convert a to mel-spectrogram (akin to 1-channel 2D images)\nx \u2208 RCa\u00d7T , where Ca, T \u226a T a denote the mel-channels and the number of frames respectively.\nThe training process includes two stages:\n1) Training variational autoencoder. The audio encoder E takes mel-spectrogram x as input\nand outputs compressed latent z = E(x). The audio decoder D reconstructs the mel-spectrogram\nsignals x\u2032 = D(z) from the compressed representation z. VAE solves the problem of excessive\nsmoothing in mel-spectrogram reconstruction through adversarial training with a discriminator. The\ntraining objective is to minimize the weighted sum of reconstruction loss Lre, GAN loss LGAN and\nKL-penalty loss LKL.\n2) Training latent diffusion model. The text encoder ftext encodes the text input y into conditional\nembedding c = ftext(y). The training objective of the U-Net diffusion module is to minimize the\nmean squared error in the noise space:\nL\u03b8 = \u2225\u03f5\u03b8(zt, t, c) \u2212 \u03f5\u22252\n2,\n(1)\nwhere, \u03f5 \u223c N(0, I) denotes the noise, \u03f5\u03b8 denotes the denoising network, t is the random time step, c\nis involved by the cross-attention mechanism. The diffusion model can be efficiently trained by opti-\nmizing ELBO, ensuring extremely faithful reconstructions that match the ground-truth distribution.\nDuring inference, with the conditional embedding c from text and the noise zt sampled from Gaussian\ndistribution, the diffusion network runs denoising steps to get z0. By applying the audio decoder D\nand the separately trained Vocoder V , the generated audio is got by a\u2032 = V (D(z0)). To enhance\ncontrol over conditional generation, Make-An-Audio applies classifier-free guidance [13] technique:\n\u02dc\u03f5\u03b8(zt, t, c) = \u03f5\u03b8(zt, t, c\u2205) + s \u00b7 (\u03f5\u03b8(zt, t, c) \u2212 \u03f5\u03b8(zt, t, c\u2205)),\n(2)\nwhere c\u2205 denotes the conditional embedding when empty prompt is given, s is the guidance scale.\nThe model reduces the probability of generating samples that do not use conditioning information, in\nfavor of the samples that explicitly do as s increases.\nPseudo prompt enhancement method with audio captioning model [54] and audio-text retrieval\nmodel [23] is also introduced to construct captions for audios without natural language annotation.\nHowever, it should be noted that the quality of the captions generated by the audio captioning model\nand the diversity of the caption templates are limited.\n4\nFigure 2: Overview of LLM-based data augmentation. We use single-labeled audios and their labels\nas a database. Composing complex audios and the structured captions with these data. We then\nuse LLM to generate diverse natural language captions by the constructed captions and appropriate\nprompt.\n4\nMake-An-Audio 2\nIn this section, we first describe the motivation of several designs in Make-An-Audio 2, and then\nintroduce the overall architecture as illustrated in Figure 1, to enhance temporal modeling in text-to-\naudio generation.\n4.1\nMotivation\nIn comparison to image data, audio data includes temporal information. A sound event can occur at\nany time within the audio, making audio synthesis a challenge when attempting to maintain temporal\nconsistency. Previous approaches have encountered difficulties in dealing with captions that contain\nmultiple sounds and complex temporal information, leading to semantic misalignment and poor\ntemporal consistency. This can cause the generated audio to omit some sounds and produce an\ninaccurate temporal sequence. To address these issues, we propose the temporal enhancement\nmethod by parsing the original caption into structured pairs of <event & order>.\nAdditionally, we introduce a temporal encoder that utilizes the structured caption to help the model\nbetter understand the temporal information regarding the event sound. The temporal encoder and the\nmain text decoder form our dual text encoders architecture. Another challenge in T2A synthesis\npertains to data scarcity. Most of the data in this task is \"dirty\", which means there are extra noises\nand other sounds in the audio aside from the annotated sounds. Furthermore, there is little data\navailable with detailed temporal annotation. To combat this issue, we propose a structured data\nconstruction and LLM-based data augmentation approach.\nPrevious diffusion-based T2A work continues the T2I diffusion model architecture using 2D-\nconvolution and spatial transformer stacked U-Net networks as the diffusion model backbone.\nHowever, mel-spectrogram, unlike images, is not spatially translational invariant and the previ-\nous architecture is not suitable for training variable-length audio data, so we propose a feed-forward\nTransformer-based diffusion denoiser backbone and 1D-convolution-based audio VAE to improve\nthe robustness of the model for generating variable-length audio data.\n4.2\nTemporal enhancement\nRecently, AudioGPT [16] and HuggingGPT [49] take LLM (e.g., ChatGPT) as a controller to invoke\nother AI models for expanding LLM\u2019s capacity in addressing multi-modal tasks. Conversely, we\nconsider the possibility of utilizing the robust language understanding capabilities of LLMs to provide\ntemporal knowledge. Thus, we introduce the proposed temporal enhancement method.\nSpecifically, LLMs are utilized to parse the input text (the natural language audio caption) and extract\nstructured <event & order> pairs. As illustrated in Figure 1, LLMs simplify the original natural\nlanguage caption and link each sound event to its corresponding order. Benefiting from enhanced\ntemporal knowledge, the T2A model is empowered to identify sound events and corresponding\ntemporal order. Appendix D contains further details on prompt design and additional examples of\ntemporal information enhancement.\n5\n4.3\nDual text encoders\nTo enhance the utilization of caption information, we propose a dual text encoder architecture\nconsisting of a main text encoder CLAP [6], and a temporal encoder T5 [41]. With contrastive multi-\nmodal pre-training, the CLAP has achieved excellent zero-shot performance in several downstream\ntasks. We freeze the weights of the main text encoder and fine-tune the temporal encoder to capture\ninformation about the temporal order of various events. Denote the audio-text pair and the parsed\nstructured caption respectively as (a, y), ys, the final textual representation is expressed as:\nc = Linear(Concat(ftext(y), ftemp(ys))),\n(3)\nwhere ftext is the main text encoder and ftemp is the temporal encoder.\n4.4\nLLM-based data augmentation\nThe remarkable success of the GPT series has underscored the significance of data-centric artificial\nintelligence [56, 18], which seeks to optimize the creation, selection, and maintenance of training\nand inference data for optimal outcomes. While the T2I task benefits from billions of text-image\npairs [47], there are currently only around one million open-source text-audio pairs avaiable [15].\nFurthermore, many of these audios are only roughly labeled with tags instead of natural language\ncaptions. To optimize the use of this data, we propose an LLM-based data augmentation technique.\nAs shown in Figure 2, we augment audio data and its corresponding text caption as follows:\n\u2022 We begin by collecting data labeled with single tags to create our event database D. This type of\ndata is typically cleaner and less likely to contain unwanted noise or other sounds. We can then use\nthis data to construct more complex data based on their durations.\n\u2022 Then we randomly select N \u2208 {2, 3} samples from D, mix and concatenate them at random.\nConcatenating at random intervals or overlaps ensures that the resulting audio contains temporal\ninformation. Mixing improves the models\u2019 ability to recognize and separate different sorts of audio\nfor creating complex compositions.\n\u2022 As the resulting audio is created, we synthesize structured captions based on the occurrence time\nand duration of each sound event by rules. For those events that appear almost throughout the\naudio, we bind them with \"all\". While for events that only partly occur in the audio, we bind them\nwith \"start\", \"mid\" or \"end\" depending on the proportion of their occurrence time points in the\nresulting audio.\n\u2022 Finally, we feed the structured captions into LLM with appropriate prompts to generate diverse\nnatural language captions. The prompt to transform structured captions to natural language captions\nand some examples are displayed in Appendix D.\n4.5\nTransformer-based diffusion denoiser backbone\nPrevious diffusion-based work on T2A synthesis treated the mel-spectrogram as a one-channel\nimage similar to the approach used for T2I synthesis. However, unlike images, the mel-spectrogram\nis not spatially translation invariant. The height of the mel-spectrogram represents the frequency\ndomain, which means that mel-spectrogram patches at different heights can have entirely different\nmeanings and should not be treated equally. Furthermore, the use of a 2D-convolution layer and\nspatial transformer-stacked U-Net architecture limits the model\u2019s ability to generate variable-length\naudio. Previous works [38, 1] has shown U-Net is not necessary for diffusion network [12, 44]\nand found transformer-based [53] architecture as diffusion network can achieve better performance.\nInspired by these works and to improve the model\u2019s ability to generate variable-length audio, we\npropose a modified audio VAE that uses a 1D-convolution-based model and propose a feed-forward\nTransformer-based diffusion denoiser backbone. While the latent of Make-An-Audio\u2019s audio encoder\nis z = E(x) \u2208 Rce\u00d7Ca/f\u00d7T/f, where ce is the embedding dim of latent, f is the downsampling\nrate, Ca and T denote the mel-channels and the number of frames of mel-spectrogram respectively,\nwhich can be seen as images\u2019 height and width. Our 1D-convolution-based audio encoder\u2019s latent\nis z = E(x) \u2208 RCa/f1\u00d7T/f2, where f1, f2 are downsampling rates of mel-channels and frames,\nrespectively. As the feed-forward Transformer block is composed of 1D-convolution and temporal\ntransformer, it can better understand the information of the temporal domain in latent and improves\nthe variable length audio generation performance. Compared to the original spatial transformer, the\n6\ncomputation complexity reduces from O((Ca/f \u00d7 T/f)2 \u00d7 D) to O((T/f2)2 \u00d7 D), where D is the\nembedding dimension of the transformer layer.\n5\nExperiments\n5.1\nExperimental setup\nDataset.\nWe use a combination of several datasets to train our model, including: AudioCaps\ntraining set, WavCaps, AudioSet, Adobe Audition Sound Effects, Audiostock, ESC-50, FSD50K,\nMACS, Epidemic Sound, UrbanSound8K, WavText5Ks, TUT acoustic scene. This results in a dataset\ncomposed of 0.92 million audio text pairs, with a total duration of approximately 3.7K hours.\nWe conduct preprocessing on both text and audio data as follows: 1) We convert the sampling rate of\naudios to 16kHz. Prior works [55, 15, 29] pad or truncate the audio to a fixed length (10s), while\nwe group audio files with similar durations together to form batches to avoid excessive padding\nwhich could potentially impair model performance and slow down the training speed. This approach\nalso allows for improved variable-length generation performance. We truncate any audio file that\nexceeds 20 seconds, in order to speed up the training process. 2) We adopt the LLM-based data\naugmentation method in section 4.4 to construct approximately 61k additional audio-text pairs as\nauxiliary data. 3) For audios without natural language annotation, we apply the pseudo prompt\nenhancement method from Make-An-Audio [15] to construct captions aligned with the audio. 4)\nWe assign a lower weightage to the data that is not annotated with temporal information, but is\nabundant in quantity and diversity, such as AudioSet and WavCaps data. Specifically, we traverse\nthe AudioCaps training set and the LLM augmented data with a probability of 50%, while randomly\nselecting data from all other sources with a probability of 50%. For the latter dataset, we use \"<text &\nall>\" as their structured caption. To evaluate the performance of our models, we use the AudioCaps\ntest set and Clotho evaluation set which contain multiple event audio samples and detailed audio\ncaptions. The latter serves as a more challenging zero-shot scenario test for us. More details about\ndatasets are put in Appendix A.\nModel configurations.\nWe train a continuous 1D-convolution-based VAE to compress the mel-\nspectrogram to a 20-channel latent with temporal axis downsample rates of 2. For our main experi-\nments, we train a feed-forward Transformer-based diffusion backbone. The diffusion model is trained\non 8 NVIDIA A100 GPU with 1.2M optimization steps, 4 batch size per GPU. We use the AdamW\noptimizer [21] with a learning rate of 9.6e-5. BigVGAN [28], a universal vocoder that generalizes\nwell for various scenarios, is used as our vocoder, we trained it on the AudioSet Dataset. For more\ndetails about Model configuration please refer Appendix B.\nEvaluation methods.\nWe evaluate our models using objective and subjective metrics to assess the\naudio quality and text-audio alignment faithfulness. For objective evaluation, we include Frechet\ndistance (FD), inception score (IS), Kullback\u2013Leibler (KL) divergence, Frechet audio distance (FAD),\nand CLAP score. For subjective evaluation, we conduct crowd-sourced human evaluations with MOS\n(mean opinion score) to assess the audio quality and the text-audio alignment faithfulness, scoring\nMOS-Q and MOS-F, respectively. More information regarding the evaluation process can be found\nin Appendix C.2\nBaseline models.\nTo establish a standard for comparison, our study employs four baseline models,\nincluding Make-An-Audio [15], AudioLDM [29], TANGO [8] and AudioGen [25]. We reimplement\nMake-An-Audio and train it on AudioCaps Dataset. AudioLDM-S and AudioLDM-L with 454M\nand 1.01B parameters respectively are trained on AudioCaps, AudioSet, BBC Sound Effects, and the\nFreeSound dataset. TANGO is trained on AudioCaps Dataset. The close-source model AudioGen-\nBase and AudioGen-Large, with 285M and 1M parameters are trained on AudioCaps, AudioSet, and\neight other datasets.\n5.2\nMain results\nAutomatic objective evaluation. The objective evaluation comparison with baseline is presented\nin Table 1, and we have the following observations: 1) In terms of audio quality, Make-An-Audio\n2 achieves better scores in FD, IS, KL, and FAD; 2) On text-audio similarity, Make-An-Audio 2\n7\nTable 1: The comparison between Make-An-Audio 2 and baseline T2A models on the AudioCaps\ndataset. All the diffusion-based models run with 100 DDIM [51] steps for a fair comparison. We\nborrowed all the results from [29, 8] and used the model released by the authors on Huggingface to\ntest CLAP Score. We reimplement Make-An-Audio and replace their vocoder with our BigVGAN\nvocoder.\nModel\nPrams\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\nCLAP\u2191\nMOS-Q\u2191\nMOS-F\u2191\nGroundTruth\n-\n-\n-\n-\n-\n0.671\n86.47\n84.31\nAudioGen-S\n285M\n-\n-\n2.09\n3.13\n-\n-\n-\nAudioGen-L\n1B\n-\n-\n1.69\n1.82\n-\n-\n-\nMake-An-Audio\n453M\n18.32\n7.29\n1.61\n2.66\n0.593\n69.54\n65.45\nAudioLDM-S\n454M\n29.48\n6.9\n1.97\n2.43\n-\n-\n-\nAudioLDM-L\n1.01B\n23.31\n8.13\n1.59\n1.96\n0.605\n70.91\n67.41\nTANGO\n1.21B\n26.13\n8.23\n1.37\n1.87\n0.650\n74.10\n72.76\nMake-An-Audio 2\n937M\n11.75\n11.16\n1.32\n1.80\n0.645\n78.31\n75.63\npresents the comparable score in CLAP; 3) Regarding temporal alignment, Make-An-Audio 2 also\nachieves a high IS score, showing that temporal enhancement method also enhances the clarity and\nexpressiveness of the sound events in the generated audio.\nSubjective human evaluation. The human evaluation results show significant gains of Make-An-\nAudio 2 with MOS-Q of 78.31 and MOS-F of 75.63, outperforming TANGO in the MOS-F evaluation.\nIt indicates that raters prefer our model synthesis against baselines in terms of audio naturalness and\nfaithfulness.\nZero-shot evaluation. To further investigate the generalization performance of the models, we\nadditionally test the performance of the models on the Clotho-evaluation dataset in the zero-shot\nscenario. As illustrated in Table 2, Make-An-Audio 2 has significantly better results than TANGO\nand AudioLDM-L, attributing to the scalability in terms of data usage.\nTable 2: Comparison of Make-An-Audio 2, AudioLDM-L, and Tango on Clotho-eval (10s) and\nAudioCaps-test (5s) datasets. We truncate the ground truth audios from AudioCaps dataset to 5\nseconds to conduct variable-length generation evaluation.\nModel\nClotho-eval\nAudioCaps-test\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\nTANGO\n32.1\n6.77\n2.59\n3.61\n31.76\n5.50\n2.04\n10.53\nAudioLDM-L\n28.15\n6.55\n2.6\n4.93\n31.97\n5.66\n2.39\n6.79\nMake-An-Audio 2\n19.97\n8.50\n2.49\n2.13\n13.78\n9.95\n1.61\n2.33\n5.3\nAnalyses\nVariable-length generation. Audio data can have different lengths, to investigate our models\u2019\nperformance on variable-length audio generation, we test to generate 5 seconds audios on AudioCaps\ndataset, the results are shown in Table 2. From the table, it can be seen that TANGO and AudioLDM\nexhibit significant performance degradation when generating audio with different lengths than the\ntraining data, as TANGO and AudioLDM pad or truncate all the training audio data to 10 seconds,\nand their models are based on 2D-convolution and spatial transformer to process mel-spectrogram\nas images. Make-An-Audio 2 maintains high performance even when generating variable-length\naudio samples since it is trained on audio samples of varying lengths and utilizes 1D-convolution and\ntemporal transformers to emphasize temporal information.\nClassifier-free guidance scale. We explore the classifier-free guidance in text-to-audio synthesis. The\nchoice of the classifier guidance scale could scale conditional and unconditional synthesis, offering a\ntrade-off between sample faithfulness and realism with respect to the conditioning text. We show the\neffect of guidance scale w on T2A in Table 3. Our results show that a guidance scale of w = 4 yields\nthe best performance in terms of FD and FAD.\n8\nTable 3: Comparison of different classifier-free guidance scales on AudioCaps-test set.\nGuidance Scale\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\n1\n30.13\n4.61\n2.26\n7.55\n2\n20.71\n7.23\n1.66\n3.76\n3\n15.71\n9.07\n1.47\n2.41\n4\n11.75\n11.16\n1.32\n1.80\n5\n12.43\n11.37\n1.25\n1.95\n7.5\n13.96\n11.32\n1.31\n2.97\n10\n16.06\n10.52\n1.47\n3.59\nTable 4: Vocoder reconstruction performance comparison on AudioCpas-test set.\nVocoder\nModel\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\nMelGAN\nDiffsound\n26.14\n5.4\n1.22\n6.24\nHifiGAN\nMake-An-Audio\n21.79\n5.93\n1.03\n6.02\nHifiGAN\nAudioLDM\n11.45\n8.13\n0.22\n1.18\nBigVGAN\nMake-An-Audio 2\n5.45\n9.44\n0.17\n0.98\nVocoder performance comparison. Vocoder is another key component to improve the generated\naudio quality for mel-spectrogram-based models. We further compared the 1) BigVGAN vocoder\nused in Make-An-Audio 2, 2) AudioLDM\u2019s pre-trained HifiGAN, 3) Make-An-Audio\u2019s pre-trained\nHifiGAN and 4) Diffsound\u2019s pre-trained MelGAN. The results are shown in Table 4. We find 3) and\n4) both perform worse compared with 2), while 2) and 3) use the same vocoder architecture. We find\nthat the problem lies in the mel-processing method of 3) and 4), they use the same mel-processing\nmethod which will result in poor performance of the vocoder. So we adopt the same mel-spectrogram\nextraction process as BigVGAN and get the best performance.\n5.4\nAblation study\nTable 5: The results evaluated on AudioCaps-test set with different settings of text encoder, diffusion\nmodel backbone, and dataset. Noted that \u2020 marks the text encoder is frozen, \u2217 means trainable one.\nSetting\nText encoder\nBackbone\nDataset\nFD\u2193\nIS\u2191\nKL\u2193\nFAD\u2193\n#1\nCLAP\u2020\nU-Net\nAudioCaps\n18.24\n7.46\n1.61\n2.25\n#2\nT5\u2217\nU-Net\nAudioCaps\n13.73\n9.79\n1.41\n2.05\n#3\nCLAP\u2020+T5\u2217\nU-Net\nAudioCaps\n13.71\n9.99\n1.36\n1.92\n#4\nCLAP\u2020+T5\u2217\nU-Net\nVariable length All\n22.69\n5.93\n2.17\n3.82\n#5\nCLAP\u2020+T5\u2217\nTransformers\nVariable length All\n11.75\n11.16\n1.32\n1.80\nIn order to assess the effectiveness of various designs in Make-An-Audio 2 including temporal\nenhancement, dual text encoder, and feed-forward Transformer-based diffusion denoiser, we conduct\nablation studies. The results evaluated on the AudioCaps-test set with different settings for text\nencoder, diffusion denoiser backbone, and dataset are presented in Table 5. The key findings are\ndiscussed below:\n\u2022 Comparing setting #2 to #1 highlights the effectiveness of temporal enhancement. We use LLM to\nextract event and temporal information and create structured input in the form of <event & order>\npairs. As this format of structured input is not in the training corpus of the text encoder, we use the\ntrainable T5 as our text encoder, which leads to significant improvements in both objective scores\nand sound timing modeling.\n\u2022 Comparing setting #3 with #2 proves the effectiveness of dual text encoder architecture. The reason\nis that the structured caption parsed by LLM may result in the loss of some information such\nas adjectives and quantifiers, and the trainable text encoder may reduce generalization abilities.\nTherefore, it is still essential to use the frozen CLAP encoder to comprehend the original natural\nlanguage caption.\n9\n\u2022 For setting #4, we train our model with the large combined dataset instead of AudioCaps-train set\nand switch the training mode from fixed-length audio training to variable-length audio, resulting\nin severe performance degradation. As 2D-convolution and spatial Transformer treat the mel-\nspectrograms as images, and it treats the temporal information and frequency information the same\nway, they demonstrate deficiencies in understanding variable-length data with temporal information.\n\u2022 Setting #5 is our Make-An-Audio 2. Compared with setting #4, we replace the 2D-convolutional\nVAE with 1D-convolutional VAE and replace the U-Net diffusion backbone with feed-forward\nTransformer layers and observe performance gains, which demonstrates a strong ability in under-\nstanding variable length audios and temporal information.\n6\nConclusions\nIn this work, we present Make-An-Audio 2, a temporal-enhanced T2A synthesis model. With a\ncapable LLM to extract temporal information from the natural language caption, Make-An-Audio\n2 can better understand the event order in the caption and generate semantically aligned audios.\nLeveraging 1D-convolutional VAE and feed-forward Transformer diffusion backbone, Make-An-\nAudio 2 can generate variable-length audios without performance degeneration. With complex audio\nreconstruction and LLM-based data augmentation, Make-An-Audio 2 is endowed with the ability to\nunderstand complex temporal relationships and combinations of multiple concepts. Make-An-Audio\n2 achieves the SOTA audio generation quality in both objective and subjective metrics, and extends the\nboundaries of the T2A. We discuss the limitations, future works and broader impact in Appendix E.\nReferences\n[1] Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth\nwords: A ViT backbone for diffusion models.\n[2] Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang, Yihan Cao, Zihao Wu, Lin\nZhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang Zhu, Hongmin Cai, Lichao Sun,\nQuanzheng Li, Dinggang Shen, Tianming Liu, and Xiang Li. Auggpt: Leveraging chatgpt for\ntext data augmentation, 2023.\n[3] Soham Deshmukh, Benjamin Elizalde, and Huaming Wang. Audio retrieval with WavText5K\nand CLAP training.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[5] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen. Clotho: An audio captioning\ndataset. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 736\u2013740.\n[6] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang.\nClap:\nLearning audio concepts from natural language supervision.\n[7] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing\nMoore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset\nfor audio events. In 2017 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 776\u2013780.\n[8] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish, and Soujanya Poria. Text-to-audio\ngeneration using instruction-tuned llm and latent diffusion model, 2023.\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. 63(11):139\u2013144.\n[10] Shawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Chan-\nning Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. CNN architectures\nfor large-scale audio classification. In 2017 IEEE International Conference on Acoustics, Speech\nand Signal Processing, pages 131\u2013135. IEEE.\n10\n[11] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A\nreference-free evaluation metric for image captioning.\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Proc.\nof NeurIPS.\n[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop\non Deep Generative Models and Downstream Applications.\n[14] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. CogVideo: Large-scale\npretraining for text-to-video generation via transformers.\n[15] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui\nYe, Jinglin Liu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with\nprompt-enhanced diffusion models, 2023.\n[16] Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning\nWu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, and Shinji Watanabe.\nAudiogpt: Understanding and generating speech, music, sound, and talking head, 2023.\n[17] Rongjie Huang, Yi Ren, Jinglin Liu, Chenye Cui, and Zhou Zhao. Generspeech: Towards\nstyle transfer for generalizable out-of-domain text-to-speech. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing\nSystems, 2022.\n[18] Johannes Jakubik, Michael V\u00f6ssing, Niklas K\u00fchl, Jannis Walk, and Gerhard Satzger. Data-\ncentric artificial intelligence. arXiv preprint arXiv:2212.11854, 2022.\n[19] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. Fr\u00e9chet audio distance:\nA reference-free metric for evaluating music enhancement algorithms. In INTERSPEECH,\npages 2350\u20132354.\n[20] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim. AudioCaps: Generat-\ning captions for audios in the wild. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 119\u2013132.\n[21] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv\npreprint:1412.6980, 2014.\n[22] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions.\n31:10215\u201310224.\n[23] A Sophia Koepke, Andreea-Maria Oncescu, Joao Henriques, Zeynep Akata, and Samuel\nAlbanie. Audio retrieval with natural language queries: A benchmark study. IEEE Transactions\non Multimedia, 2022.\n[24] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley.\nPANNs: Large-scale pretrained audio neural networks for audio pattern recognition. 28:2880\u2013\n2894.\n[25] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer, Alexandre D\u00e9fossez, Jade Copet,\nDevi Parikh, Yaniv Taigman, and Yossi Adi. AudioGen: Textually Guided Audio Generation.\n[26] Ashutosh Kumar, Satwik Bhattamishra, Manik Bhandari, and Partha Talukdar. Submodu-\nlar optimization-based diverse paraphrasing and its effectiveness in data augmentation. In\nProceedings of the 2019 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers),\npages 3609\u20133619, 2019.\n[27] Varun Kumar, Ashutosh Choudhary, and Eunah Cho. Data augmentation using pre-trained\ntransformer models, 2021.\n[28] Sang-gil Lee, Wei Ping, Boris Ginsburg, Bryan Catanzaro, and Sungroh Yoon. BigVGAN: A\nUniversal Neural Vocoder with Large-Scale Training.\n11\n[29] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D. Plumbley. AudioLDM: Text-to-Audio Generation with Latent Diffusion Models.\n[30] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of prompting methods in natural language\nprocessing, 2021.\n[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[32] Irene Mart\u00edn-Morat\u00f3 and Annamaria Mesaros. What is the ground truth? reliability of multi-\nannotator data for audio tagging.\nIn 2021 29th European Signal Processing Conference\n(EUSIPCO), pages 76\u201380.\n[33] Xinhao Mei, Chutong Meng, Haohe Liu, Qiuqiang Kong, Tom Ko, Chengqi Zhao, Mark D.\nPlumbley, Yuexian Zou, and Wenwu Wang. WavCaps: A ChatGPT-Assisted weakly-labelled\naudio captioning dataset for audio-language multimodal research.\n[34] Annamaria Mesaros, Toni Heittola, and Tuomas Virtanen. Tut database for acoustic scene\nclassification and sound event detection. In 2016 24th European Signal Processing Conference\n(EUSIPCO), pages 1128\u20131132, 2016.\n[35] Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\nactually converge? In International conference on machine learning, pages 3481\u20133490. PMLR,\n2018.\n[36] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing\nwith Text-Guided Diffusion Models.\n[37] OpenAI. Gpt-4 technical report, 2023.\n[38] William Peebles and Saining Xie. Scalable diffusion models with transformers.\n[39] Karol J Piczak. ESC: Dataset for environmental sound classification. In Proceedings of the\n23rd ACM International Conference on Multimedia, pages 1015\u20131018.\n[40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\npages 8748\u20138763.\n[41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, Peter J Liu, et al. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. 21(140):1\u201367.\n[42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[43] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695.\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding.\n[46] J. Salamon, C. Jacoby, and J. P. Bello. A dataset and taxonomy for urban sound research. In\n22nd ACM International Conference on Multimedia (ACM-MM\u201914), pages 1041\u20131044.\n12\n[47] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick\nSchramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk,\nand Jenia Jitsev. Laion-5b: An open large-scale dataset for training next generation image-text\nmodels, 2022.\n[48] Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving neural machine translation\nmodels with monolingual data. arXiv preprint arXiv:1511.06709, 2015.\n[49] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface, 2023.\n[50] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,\nHarry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without\ntext-video data.\n[51] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[54] Xuenan Xu, Heinrich Dinkel, Mengyue Wu, and Kai Yu. A crnn-gru based reinforcement\nlearning approach to audio captioning. In DCASE, pages 225\u2013229, 2020.\n[55] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu.\nDiffsound: Discrete Diffusion Model for Text-to-sound Generation.\n[56] Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong,\nand Xia Hu. Data-centric artificial intelligence: A survey. arXiv preprint arXiv:2303.10158,\n2023.\n[57] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n13\nAppendices\nMake-An-Audio 2: Temporal-Enhanced Text-to-Audio\nGeneration\nA\nData details\nDataset\nHours\nType\nSource\nAudiocaps\n109hrs\ncaption\n[20]\nWavCaps\n2056hrs\ncaption\n[33]\nWavText5K\n25hrs\ncaption\n[3]\nMACS\n48hrs\ncaption\n[32]\nClothv2\n152hrs\ncaption\n[5]\nAudiostock\n44hrs\ncaption\nhttps://audiostock.net\nepidemic sound\n220hrs\ncaption\nhttps://www.epidemicsound.com\nAdobe Audition Sound Effects\n26hrs\ncaption\nhttps://www.adobe.com/products/\naudition/offers/AdobeAuditionDLCSFX.\nhtml\nFSD50K\n108hrs\nlabel\nhttps://annotator.freesound.org/fsd\nODEON_Sound_Effects\n20hrs\nlabel\nhttps://www.paramountmotion.com/\nodeon-sound-effects\nUrbanSound8K\n9hrs\nlabel\n[46]\nESC-50\n3hrs\nlabel\n[39]\nfilteraudioset\n945hrs\nmulti label\n[7]\nTUT\n13hrs\nlabel\n[34]\nTable 6: Statistics for the Datasets used in the paper.\nAs shown in Table 6, we collect a large-scale audio-text dataset consisting of 0.92 million of audio\nsamples with a total duration of approximately 3.7k hours. The dataset has a wide variety of sounds\nincluding music and musical instruments, sound effects, human voices, nature and living sounds, etc.\nFor Clotho dataset, we only use its evaluation set for zero-shot testing and do not use for training. As\nspeech and music are the dominant classes in AudioSet, we filter 95% of the samples that contain\nspeech and music to build a more balanced dataset.\nB\nExperimental details\nVariational autoencoder.\nWe employed a similar VAE architecture to that of Make-An-Audio,\nreplacing all the 2D-convolution layers with 1D-convolution layers and the spatial transformer\nwith a temporal transformer. As detailed in Section 4.5, the output latent of VAE is z = E(x) \u2208\nRCa/f1\u00d7T/f2, where we choose the downsample rate of f1 = 4 and f2 = 2. We additionally involve\nR1 regularization [35] to better stabilize the adversarial training process. We train our VAE on 8\nNVIDIA A100 GPU with a batch size of 32 and 800k training steps on AudioSet dataset. We use the\nAdam optimizer [21] with a learning rate of 1.44 \u00d7 10\u22124. For specific differences in hyperparameters\nbetween our VAE and that of Make-An-Audio, please see Table 7.\nLatent diffusion.\nWe train our Latent Diffusion Model with on 8 NVIDIA A100 GPU with a batch\nsize of 32 and 1.2M training steps. We use the Adam optimizer with a learning rate of 9.6 \u00d7 10\u22125.\nFor the specific hyperparameter for our latent diffusion model, please refer to Table 8.\nModel parameters of each component.\nThe params of each component in Make-An-Audio 2 are\ndisplayed in Table 9.\n14\nMake-An-Audio VAE\nMake-An-Audio 2 VAE\nAssume input tensor shape (for 10s audio)\n(1,80,624)\n(80,624)\nEmbed_dim\n4\n20\nConvolution layer\nConv2D\nConv1D\nChannels\n128\n224\nChannel multiplier\n1,2,2,4\n1,2,4\nDownsample layer position\nafter block 1,2\nafter block 1\nAttention layer\nspatial attention\ntemporal attention\nAttention layer position\nafter block 3,4\nafter block 3\nOutput tensor shape\n(4,10,78)\n(20,312)\nTable 7: Difference between Make-An-Audio VAE and our VAE\nMake-An-Audio 2 LDM\nInput shape\n(20,T)\nCondition_embedding dim\n1024\nFeed-forward Transformer hidden_size\n576\nFeed-forward Transformer\u2019s Conv1d kernel size\n7\nFeed-forward Transformer\u2019s Conv1d padding\n3\nNumber of Transformer heads\n8\nNumber of Feed-forward Transformer block\n8\nDiffusion steps\n1000\nTable 8: Make-An-Audio 2 Diffusion model backbone configurations\nC\nEvaluation\nC.1\nSubjective evaluation\nTo assess the generation quality, we conduct MOS (Mean Opinion Score) tests regarding audio quality\nand text-audio faithfulness, respectively scoring MOS-Q and MOS-F.\nFor audio quality, the raters were explicitly instructed to \u201cfocus on examining the audio quality and\nnaturalness.\u201d The testers were presented with audio samples and asked to rate their subjective score\n(MOS-P) on a 20-100 Likert scale.\nFor text-audio faithfulness, human raters were shown the audio and its caption and asked to respond\nto the question, \"Does the natural language description align with the audio faithfully?\" They had to\nchoose one of the options - \"completely,\" \"mostly,\" or \"somewhat\" on a 20-100 Likert scale.\nOur crowd-sourced subjective evaluation tests were conducted via Amazon Mechanical Turk where\nparticipants were paid $8 hourly. A small subset of the generated audio samples used in the test can\nbe found at https://make-an-audio-2.github.io/.\nComponent\nParams\nVAE\n213M\nDiffusion Model Backbone\n160M\nText Encoder\n452M\nVocoder\n112M\nTotal\n937M\nTable 9: The params of each component in Make-An-Audio2\n15\nFigure 3: The prompt templates we used for temporal enhancement and data augmentation. We use\nthe symbol \u2019&\u2019 to split the sound event and the time order. We use the symbol \u2019@\u2019 to split <event &\norder> pairs.\nC.2\nObjective evaluation\nFr\u00e9chet Audio Distance (FAD) [19] is adapted from the Fr\u00e9chet Inception Distance (FID) to the audio\ndomain, it is a reference-free perceptual metric that measures the distance between the generated and\nground truth audio distributions. FAD is used to evaluate the quality of generated audio.\nThe inception Score (IS) is an effective metric that evaluates both the quality and diversity of generated\naudio.\nKL divergence is measured at a paired sample level between the generated audio and the ground truth\naudio, it is computed using the label distribution and is averaged as the final result.\nFr\u00e9chet Distance (FD) evaluates the similarity between the generated and ground truth audio distribu-\ntions. FD, KL and IS are built upon an audio classifier, PANNs [24], which takes the mel-spectrogram\nas model input. Differently, FAD uses VGGish [10] as an audio classifier that takes raw audio\nwaveform as model input.\nCLAP score: adapted from the CLIP score [11, 40] to the audio domain and is a reference-free\nevaluation metric to measure audio-text alignment for this work that closely correlates with human\nperception.\nD\nChatGPT prompts\nThe prompt templates utilized for temporal enhancement to construct structure caption from the\noriginal natural language caption and for caption data augmentation are displayed in Figure 3.\nTable 10 presents some instances of the original caption and ChatGPT\u2019s outcome. For text data\naugmentation, we construct structured caption inputs, and Table 11 exhibits examples of such inputs\nand ChatGPT\u2019s corresponding outputs.\n16\nNatural language input\nChatGPT\u2019s output\nA woman talks nearby as water pours\n<woman talking& all>@<water pouring& all>\nTwo men exchange words, then a car\nengine revs followed by a siren and\nfade in music\n<two\nmen\ntalking&\nall>@<car\nengine\nrevving&\nstart>@<siren& mid>@<music fading in& end>\nA crowd is cheering and shouting,\nthumping occurs, an adult female\nspeaks, and an adult male speaks\n<crowd cheering and shouting& all>@<thumping&\nstart>@<adult female speaking& mid>@<adult male\nspeaking& end>\nTable 10: Examples of using ChatGPT for temporal enhancement from AudioCaps trainset\nStructured input\nChatGPT\u2019s output\n<bark howl dog& start>@<typing\nTypewriter&\nmid>@<breathing&\nend>\nA dog barks and howls while someone types on a type-\nwriter, then the sound of breathing takes over\n<crowing&\nall>@<car&\nall>@<female singing& end>\nA rooster crows as a car passes by, and the scene ends\nwith the sound of a female singing\n<sneezing& all>@<bicycle bell ring&\nstart>@<typewriter & end>\nThe sound of sneezing is heard throughout, with a bicycle\nbell ringing at the start and the sound of a typewriter at\nthe end\nTable 11: Examples of using ChatGPT for data augmentation\nE\nLimitations, future works and broader impact\nLimitations.\nMake-An-Audio 2 incorporates an additional LLM for parsing the original caption,\nwhich affects both the generation performance and running speed. Additionally, the generative\ndiffusion model employed by Make-An-Audio 2 requires multiple iterative refinements for synthesis,\nwhich can be time-consuming to produce high-quality results. Furthermore, the speech in the\ngenerated audio can be intelligible.\nFuture works.\nWe leave the T2A system which supports speech synthesis for future work. In\naddition, we aim to implement T2A systems that could take structured inputs as optional auxiliary\ninputs instead of required inputs.\nBroader impacts.\nAt the same time, we acknowledge that Make-An-Audio 2 may lead to unin-\ntended consequences such as increased unemployment for individuals in related fields such as sound\nengineering and radio hosting. Furthermore, there are potential concerns regarding the ethics of\nnon-consensual voice cloning or the creation of fake media.\n17\n"
  },
  {
    "title": "Nested Diffusion Processes for Anytime Image Generation",
    "link": "https://arxiv.org/pdf/2305.19066.pdf",
    "upvote": "1",
    "text": "Nested Diffusion Processes for Anytime Image Generation\nNoam Elata1\nBahjat Kawar2\nTomer Michaeli1\nMichael Elad2\n1Department of ECE\n2Department of CS\nTechnion \u2013 Israel Institute of Technology\n{noamelata@campus, bahjat.kawar@cs, tomer.m@ee, elad@cs}.technion.ac.il\nAbstract\nDiffusion models are the current state-of-the-art in im-\nage generation, synthesizing high-quality images by break-\ning down the generation process into many fine-grained de-\nnoising steps. Despite their good performance, diffusion\nmodels are computationally expensive, requiring many neu-\nral function evaluations (NFEs). In this work, we propose\nan anytime diffusion-based method that can generate viable\nimages when stopped at arbitrary times before completion.\nUsing existing pretrained diffusion models, we show that\nthe generation scheme can be recomposed as two nested\ndiffusion processes, enabling fast iterative refinement of a\ngenerated image. In experiments on ImageNet and Stable\nDiffusion-based text-to-image generation, we show, both\nqualitatively and quantitatively, that our method\u2019s interme-\ndiate generation quality greatly exceeds that of the original\ndiffusion model, while the final generation result remains\ncomparable. We illustrate the applicability of Nested Diffu-\nsion in several settings, including for solving inverse prob-\nlems, and for rapid text-based content creation by allowing\nuser intervention throughout the sampling process.1\n1. Introduction\nDiffusion models (DMs) have emerged as a promising\nclass of generative models [17, 47, 51]. Having achieved\nstate-of-the-art capabilities in image generation, DMs have\nalso excelled at various tasks, such as inverse problem solv-\ning [6, 22, 24, 34, 49, 54], and image editing [4, 14, 25, 35].\nDMs have also been successfully applied to a wide range of\ndomains, ranging from speech and audio [20,27] to protein\nstructures [39,45] and medical data [7,23,52].\nThe sampling process of modern DMs can be compu-\ntationally expensive [33, 44, 48], due to the large networks\nused and the iterative nature of the reverse diffusion process.\nDespite the progress in acceleration of DMs [44, 48, 50],\nmany of the leading diffusion model-based applications re-\nmain prohibitively slow.\nDuring sampling, the diffusion process creates interme-\ndiate image predictions as a byproduct, denoted as \u02c6x0, at\nvarious time steps. In theory, this allows for monitoring the\ngeneration process and assessing the resulting images with-\nout waiting for its completion. However, these predictions\ndo not align with the learned image manifold and often ex-\nhibit a smooth or blurry appearance [24].\nTo address this issue, we propose Nested Diffusion, a\nnovel technique that leverages a pretrained DM to itera-\ntively refine generated images, acting as an anytime gener-\nation algorithm. With Nested Diffusion, intermediate pre-\ndictions of the final generated image are of better quality,\nwhich allows users to observe the generated image during\nthe sampling process and to conclude the generation if the\nintermediate yielded image is already satisfactory. Through\nexperiments, we observe that our Nested Diffusion shows\nsuperior intermediate generation quality compared to the\noriginal DM, while maintaining comparable final results.\nAccess to high quality intermediate predictions is ad-\nvantageous in several DM-based applications beyond im-\nage generation. Nested Diffusion\u2019s anytime algorithm is\nalso beneficial in solving inverse problems, given the promi-\nnence of DMs in this area. In scenarios where multiple out-\nput images are generated, users can control the generation\nprocess by selecting the most promising candidate and in-\nfluencing the sampling process to prioritize their preferred\nimages. This valuable capability also enables interactive\nonline adjustments during the generation process.\nWe introduce Nested Diffusion, a novel anytime sam-\npling algorithm for DMs. We showcase the versatility of our\nmethod through various applications, including conditional\nimage generation, and inverse problem solving. Addition-\nally, we highlight the ability of Nested Diffusion to enable\ninteractive content creation during the sampling process.\n2. Preliminaries\n2.1. Anytime Algorithms\nAnytime algorithms [3, 13, 19, 58] are a class of meth-\nods that attempt to address real-world problems under re-\n1Code available at https://github.com/noamelata/NestedDiffusion.\n1\narXiv:2305.19066v3  [cs.CV]  30 Oct 2023\n25 NFEs\n50 NFEs\n75 NFEs\n100 NFEs\nVanilla\nNested Diffusion\n25 NFEs\n50 NFEs\n75 NFEs\n100 NFEs\nVanilla\nText: a photograph of colors diffusing in a martini glass\nNested Diffusion\nText: a photograph of a nest with a blue egg inside\n20 NFEs\n40 NFEs\n60 NFEs\n80 NFEs\nVanilla\nText: an image of an ivy covered clock tower\nNested Diffusion\n20 NFEs\n40 NFEs\n60 NFEs\n80 NFEs\nVanilla\nNested Diffusion\nText: an image of a sailboat in a stormy sea\nFigure 1. Results of intermediate predictions of Stable Diffusion from a diffusion process of 100 NFEs (top) and 80 NFEs (bottom).\nsource constraints, time limitations, or uncertain input in-\nformation. Specifically, these algorithms generate progres-\nsively improved solutions, which enable early interruption.\nUnlike conventional algorithms that require completion for\na final solution, anytime algorithms offer users the flexi-\nbility to obtain usable solutions at any stage of execution.\nThis adaptability proves highly valuable in time-sensitive\ndecision-making and iterative problem-solving scenarios.\n2.2. Diffusion Models\nDiffusion models (DMs) [17, 47, 51] are the state-of-\nthe-art generative models [10], relying on the capabili-\nties of deep neural networks (DNN) in removing Gaus-\nsian noise. The forward diffusion process is defined as a\ndegradation of a data point x0 in a dataset D with accu-\nmulated Gaussian noise through the process q(xt|xt\u22121) =\nN(xt; \u221a1 \u2212 \u03b2txt\u22121, \u03b2tI) defined over timesteps t\n=\n1, . . . , T, where \u03b2t are the noise amplitudes. During train-\ning, the reverse diffusion process p\u03b8(xt\u22121|xt) is learned by\nmaximizing the evidence lower bound (ELBO) on the train-\ning dataset. The ELBO can be written in terms of Kull-\nback Leibler divergence terms between q(xt\u22121|x0, xt) and\np\u03b8(xt\u22121|xt), which have a simple closed-form expression\nwhen p\u03b8(xt\u22121|xt) is modeled as a Gaussian distribution.\nAt inference time, the trained DNN gradually removes noise\nfrom a random initialization xT \u223c N(0, I), sampling iter-\natively from the learned distributions p\u03b8(xt\u22121|xt), and fi-\nnally outputting a generated image from a distributions ap-\nproximating the real image distribution, x0 \u223c q(x0).\n3. Nested Diffusion\n3.1. Formulation\nIn DDPM [17], p\u03b8(xt\u22121|xt) is assumed to follow a\nGaussian distribution, with its mean defined using the ex-\npectation E[x0|xt] yielded by the DNN, and its variance de-\nfined as a constant. Thus, we can sample from p\u03b8(xt\u22121|xt)\nin closed-form. However, we can also interpret this sam-\npling process by expressing the distribution p\u03b8(xt\u22121|xt)\nas a convolution of two others [55] \u2013 the distribution\nq(xt\u22121|x0, xt) which has a closed form, and the DNN-\nbased approximation p\u03b8(x0|xt),\np\u03b8(xt\u22121|xt) =\nZ\nq(xt\u22121|x0, xt)p\u03b8(x0|xt)dx0.\n(1)\nHere, p\u03b8(x0|xt) is the distribution represented by the\nDNN in the context of predicting x0.\nFor instance,\nDMs with a deterministic DNN output, such as DDPM,\nwould correspond to a Dirac delta function distribu-\ntion around the DNN-estimated E[x0|xt].\nThe stochas-\nticity in the reverse diffusion process would be ob-\ntained by setting q(xt\u22121|x0, xt) as a fixed Gaussian.\n2\nAlgorithm 1 Sampling from a Regular Diffusion Process\nxT \u223c N(0, I)\nfor t in {T, T \u2212 s, . . . , 1 + s, 1} do\n\u02c6x0 \u223c p\u03b8(x0|xt)\nxt\u2212s \u223c q(xt\u2212s|\u02c6x0, xt)\nend for\nreturn x0\nAlgorithm 2 Sampling from Nested Diffusion\nOuter diffusion denoted in blue, with step size so\nInner diffusion denoted in purple, with step sizes {si\nt}\nxT \u223c N(0, I)\nfor t in {T, T \u2212 so, . . . , 1 + so, 1} do\nx\u2032\nt = xt\n\u25b7 Beginning of inner diffusion\nfor \u03c4 in {t, t \u2212 si\nt, . . . , 1 + si\nt, 1} do\n\u02c6x\u2032\n0 \u223c p\u03b8(x\u2032\n0|x\u2032\n\u03c4)\nx\u2032\n\u03c4\u2212si\nt \u223c q\u2032(x\u2032\n\u03c4\u2212si\nt|\u02c6x\u2032\n0, x\u2032\n\u03c4)\nend for\n\u02c6x0 = x\u2032\n0\n\u25b7 End of inner diffusion\nxt\u2212so \u223c q(xt\u2212so|\u02c6x0, xt)\nend for\nreturn x0\nMore generally,\nsampling from the joint distribution\np\u03b8(xt\u22121, x0|xt) = q(xt\u22121|x0, xt)p\u03b8(x0|xt) can be done\nsequentially, by first sampling \u02c6x0 \u223c p\u03b8(x0|xt) and then\nsampling xt\u22121 \u223c q(xt\u22121|\u02c6x0, xt), yielding xt\u22121 that fol-\nlows Equation (1). The generalized reverse diffusion pro-\ncess, following this interpretation, is presented in Algo-\nrithm 1.\nNote\nthat\nafter\ntraining\np\u03b8(x0|xt)\nfor\na\ncertain\nq(xt\u22121|x0, xt), it is possible to utilize the same DNN model\nfor different distributions q. For instance, DDIM [48] uti-\nlizes a deterministic q(xt\u22121|x0, xt) (equivalent to a Dirac\ndelta function) for faster generation.\nInterestingly, by\nsampling using Algorithm 1, the Gaussian assumption on\np\u03b8(xt\u22121|xt) is no longer required, and can be generalized\nbeyond DDPM sampling. In this setting, p\u03b8(x0|xt) may\nbe any learned distribution, and is not restricted to a delta\nfunction or a Gaussian form.\n3.2. Method\nWe suggest that many valid choices of q(xt\u22121|x0, xt)\nand an accurate DNN-based approximation p\u03b8(x0|xt) can\ngenerate high quality samples using Equation (1) and Al-\ngorithm 1. This could allow us to harness many different\ngenerative models into the diffusion process, for instance as\ndone with GANs [12] by Xiao et al. (2022) [55]. Here, we\nwe propose to use a complete diffusion process as a good\napproximation for p\u03b8(x0|xt).\nWe propose a Nested Diffusion process, where an outer\nInner Diffusion\nOuter Diffusion\nFigure 2. Schematic description of Nested Diffusion. The outer\ndiffusion process is depicted using the dotted gray arrows\ndiffusion process would utilize the generative sampler\np\u03c8(x0|xt) \u2013 itself an inner diffusion process. For simplic-\nity of notation, we denote the outer diffusion variables and\ndistributions in blue and the inner diffusion variables and\ndistributions in purple. As shown in Algorithm 2 and Fig-\nure 2, for each sampling step t in the outer diffusion, the\ninner diffusion uses an unaltered (vanilla) DM to generate a\nplausible image \u02c6x0, which would then be used to calculate\nxt\u2212so in the outer diffusion. We emphasize that only the in-\nner diffusion uses a DNN. The inner diffusion becomes the\nouter diffusion\u2019s abstraction for a generative model.\nUnlike vanilla diffusion processes, Nested Diffusion\nyields a more detailed \u02c6x0 at the termination of each outer\nstep. This is because \u02c6x0 is a sample generated from the\nmulti-step inner diffusion process, and not the mean yielded\nby a single denoising step. These \u02c6x0 estimations hint at the\nfinal algorithm result while being closer to the image man-\nifold. Using Nested Diffusion, the sampling process be-\ncomes an anytime algorithm, in which a valid image may\nbe returned if the algorithm is terminated prematurely.\nNested Diffusion requires |outer steps| \u00d7 |inner steps|\nNFEs for a complete image genration process. For a given\nnumber of NFEs, Nested Diffusion may support any ratio\nRND = |outer steps|\n|inner steps|. This ratio represents a tradeoff between\nfast updates to the predicted image, and the intermediate im-\nage quality (see supplementary material). Additionally, the\nratio influences the number of NFEs needed before Nested\nDiffusion produces its initial intermediate prediction, which\noccurs at the conclusion of the first inner process. In the\nextremes, where the number of either outer steps or inner\nsteps is one, the process reverts to vanilla diffusion sam-\npling. In the supplementary material, we propose a met-\nric for comparing between Nested Diffusion with different\nRND. However, we suggest tuning the RND parameter ac-\ncording to the specific application and hardware, aiming to\nprovide users with a waiting time between image updates\n3\n102\nFID\nImageNet Generation FID per NFE\nVan Total 250 NFEs\nVan Total 200 NFEs\nVan Total 150 NFEs\nVan Total 100 NFEs\n0\n30\n60\n90\n120\n150\n180\n210\n240\nNFEs\n5\n10\n15\nFID\nND Total 250 NFEs\nND Total 200 NFEs\nND Total 150 NFEs\nND Total 100 NFEs\n(a) 50K FID for class-conditional Im-\nageNet generation.\nEvaluation of in-\ntermediate predictions from Nested (ND)\nand vanilla (Van) diffusion processes.\n100\n200\n300\n400\nFID\nText-to-Image MS-COCO FID per NFE\nVanilla\nND 2 Outer Steps\nND 3 Outer Steps\nND 4 Outer Steps\nND 5 Outer Steps\nND 6 Outer Steps\n0\n10\n20\n30\n40\n50\n60\nNFEs\n12\n14\n16\n18\nFID\n(b) MS-COCO 30K FID of text-to-image gener-\nation. Intermediate predictions are generated with\na total of 60 NFEs, and different outer step con-\nfigurations. Vanilla diffusion is equivalent to using\nNested Diffusion with one outer step \u2013 in blue.\n100\n200\n300\n400\nFID\nHigh-Order Solver FID per NFE\nVanilla Total 12 NFEs\nVanilla Total 18 NFEs\nND Total 12 NFEs\nND Total 18 NFEs\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\nNFEs\n10\n15\n20\n25\nFID\n(c)\nMS-COCO\n30K\nFID\nof\ntext-\nto-image\ngeneration\nusing\nDPM-\nSolver++ for the inner diffusion. Using\na high-order solver enables high-quality\ngeneration with fewer NFEs.\nFigure 3. FID as a function of NFE for ImageNet, text-to-image, and high-order solver text-to-image generation.\nranging from one to ten seconds \u2013 in Nested Diffusion, the\nwaiting time is the duration of an inner diffusion process.\nThe computation devoted to each outer step is not re-\nquired to be the same, i.e. we can have a different ratio per\nouter step. As the number of inner steps corresponds to the\nnumber of NFEs, changing the length of each outer step\ndetermines the computation devoted to this step. In our ex-\nperiments, we use the same number of inner steps for each\nouter step for simplicity. We leave for future work to ex-\nplore dynamic allocation of the number of inner steps per\nouter step.\n4. Experiments\nWe evaluate Nested Diffusion as an anytime image gen-\nerator using a DiT model [37] trained on 256 \u00d7 256-pixel\n0\n25\n50\n75\n100\n125\n150\n175\n200\nNFEs\n101\n102\nFID\nFID per NFE Trend\nVanilla\nND Outer Di\ufb00usion\nND Inner Di\ufb00usion\nFigure 4. 50K FID evaluation of Nested Diffusion\u2019s inner and\nouter diffusion processes. FID is measured on intermediate pre-\ndictions of class-conditional ImageNet generation, compared to a\nvanilla diffusion process, every 10 NFEs. The Nested Diffusion\nouter process\u2019s FID scores correspond to every fourth inner diffu-\nsion measurement, i.e., every 40 NFEs.\nImageNet [8] images and on Stable Diffusion [41] V1.5. We\nalso show that Nested Diffusion can incorporate an inverse\nproblem solver, and present several examples on CelebA-\nHQ256 [21] using DDRM [22]. To ensure a fair compar-\nison, we compare Nested Diffusion against the unaltered\nsampling algorithm (vanilla) using the same DNN mod-\nels, hyperparameters, and total number of NFEs used. The\nsampling speed of Nested Diffusion is also equal to that of\nvanilla diffusion, as sampling time is directly proportional\nto the total number of NFEs used for generation. All experi-\nments use DDIM [48] sampling for the outer diffusion. The\ninner diffusion hyperparameters are chosen according to the\nbest practices of the model selected for the experiment.\n4.1. Class-Conditional ImageNet Generation\nThe denoising DNN employed in DiT [37] uses a\nVAE [26] based architecture to decode generated latent\nsamples [41, 53], thus enabling the application of the DMs\nin a more efficient latent space. The DNN yields both the\nmean and variance of a Gaussian distribution p\u03b8(x0|xt), en-\nabling sampling using the reparameterization trick [26]. In\naddition, the DNN has been trained with class-labels, us-\ning Classifier-Free Guidance (CFG) [18] to generate class-\nconditional samples. Figure 5 shows samples generated us-\ning 250 vanilla diffusion steps compared against Nested dif-\nfusion with 5 outer steps and 50 inner steps each (totaling\n250 NFEs). The latents from the intermediate steps are de-\ncoded using the VAE decoder.\nIn Figure 3a we compare the FID [16] of intermediate\nestimations of Nested Diffusion with the intermediate es-\ntimations of vanilla DMs, for the same number of NFEs2.\nWe note that the intermediate FID scores for Nested Diffu-\nsion are much better than their vanilla counterparts, while\n2FID for vanilla diffusion DiT reflect results reproduced by us, which\nare slightly better than reported in the original paper [37].\n4\nNested Diffusion\nVanilla\nVanilla\nNested Diffusion\nVanilla\n50 NFEs\n100 NFEs\n150 NFEs\n200 NFEs\n250 NFEs\nNested Diffusion\nFigure 5. Samples of class-conditional ImageNet generation, comparing vanilla DMs against Nested Diffusion.\nthe final result\u2019s FID (without early termination) of Nested\nDiffusion is comparable to the vanilla diffusion. Exact FID\nvalues can be found in the supplementary material.\nThe sample quality trend for intermediate inner samples\n{\u02c6x\u2032\n0} is visualized in Figure 4 using FID. The graph shows\nfive distinct drops, corresponding to the five outer diffusion\nsteps. Within each outer step, the inner diffusion\u2019s interme-\ndiate prediction\u2019s quality improves quickly until yielding its\nfinal x\u2032\n0, which (as shown in Algorithm 2) is also the outer\ndiffusion\u2019s intermediate prediction \u02c6x0. We observe that the\ngraph bears similarity to simulated annealing with restarts.\nNested Diffusion would return the last \u02c6x0 computed if ter-\nminated prematurely \u2013 corresponding to the local minima\nin the graph, shown in green.\n4.2. Text-to-Image Generation\nStable Diffusion is a large text-to-image model capa-\nble of generating photo-realistic images for any textual\nprompt [41]. We use Stable Diffusion to test Nested Diffu-\nsion for text-to-image generation. Similarly to Section 4.1,\nStable Diffusion\u2019s process runs in a latent space, and uses\nCFG [18] for text-conditional sampling. In Figure 1, we\npresent intermediate results from Nested Diffusion using\n4 outer steps and compare them to their counterparts from\nvanilla Stable Diffusion, decoding intermediate latents us-\ning the VAE decoder. The Nested Diffusion sampling pro-\ncess previews satisfactory outputs, highly similar to the\nend result. The finer details in the images improve with\nthe accumulation of more NFEs. Based on the figure, it\nis apparent that the intermediate latents obtained from the\nvanilla DMs do not correspond to valid latents. As a re-\nsult, when these latents are decoded, they do not produce\nnatural-looking images.\nFollowing previous work [2,40,41,43], we assess Nested\nDiffusion\u2019s performance in text-to-image generation using\n30K FID on the MS-COCO [29] validation dataset. The re-\nsults, presented in Figure 3b, surpass our previous findings,\nwith Nested Diffusion demonstrating comparable interme-\ndiate results to vanilla diffusion and slightly improved final\nresults. More examples for generated images and CLIP-\nScores [15] can be found in the supplementary material.\nTo assess Nested Diffusion\u2019s potential and efficiency\n5\nInput\nDDRM (30 NFEs)\nND 10 NFEs\nND 20 NFEs\nND 30 NFEs\nGround Truth\nSR16\nColorization\nInpainting\nDenoising\nFigure 6. Inverse problem solutions with Nested Diffusion on CelebA-HQ256 using DDRM. Nested Diffusion is denoted as ND.\nwith advanced high-order schedulers, we replicated the text-\nto-image experiment while employing DPM-Solver++ [33]\nas the inner diffusion sampling schedule. This change en-\nables using 10-20 NFEs for high quality samples, accel-\nerating generation. As shown in Figure 3c, Nested Diffu-\nsion\u2019s final result is of comparable quality to vanilla DM and\nintermediate prediction quality is improved, demonstrating\nNested Diffusion\u2019s potential use of high-order solvers.\n4.3. Inverse Problem Solving\nDMs have demonstrated their effectiveness in tackling\ninverse problems, whether by training conditional DNNs\ntailored for specific tasks [42] or by adapting unconditional\nDMs DNNs using modified sampling algorithms [6, 22,\n24, 34, 49].\nFollowing our notation, these inverse prob-\nlem solvers sample using Algorithm 1, but exchange the\nDNN p\u03b8(x0|xt) for a conditional p\u03b8(x0|xt, y), where y\nrepresents the available measurements.\nTo apply Nested\nDiffusion in inverse problem solving, a similar substitu-\ntion is made in the Nested Diffusion sampling Algorithm 2,\nwhere the entire inner diffusion process is replaced with a\ndiffusion-based inverse problem solver conditioned on y.\nAnalogous to image generation scenarios, Nested Diffusion\ntransforms the inverse problem solver into an anytime al-\ngorithm, producing plausible results during the sampling\nprocess. An exact inverse problem solving algorithm using\nNested diffusion is included in the supplementary material.\nTo evaluate the efficiency of Nested Diffusion for in-\nverse problems, we conduct experiments on the CelebA-\nHQ256 dataset [21], employing DDRM [22] as the in-\nverse problem solver. Following DDRM, we rely on a pre-\ntrained DDPM [35], and use default hyperparameters ex-\ncept for number of sampling steps used. The results, de-\npicted in Figure 6, demonstrate the generalization capa-\nbilities of Nested Diffusion in tackling inverse problems\nlike inpainting, super-resolution, colorization and denois-\ning. The algorithm produces valid intermediate predictions\nand achieves comparable final results, demonstrating its ef-\nfectiveness in addressing various inverse problems.\n5. Generation With Human Feedback\nAn emerging area of interest in guided image generation\nfocuses on tuning the generated results to the user\u2019s pref-\nerences [28, 57]. This type of guidance typically requires\nuser interaction with the model during training, attempting\nto fine-tune the DMs\u2019s generation process using direct feed-\nback. The fine-tuned models show a greater capability to\nmatch the model\u2019s behaviour with the user\u2019s demands.\nNested Diffusion, by its inherent design, allows users to\nview the generated output throughout the sampling algo-\n6\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nFigure 7. An example of generation with human feedback. The top and bottom graphs differ by the user\u2019s preference for the image,\nmarked with a bright green frame.\nrithm, enabling straightforward guidance of the process to-\nwards desired outcomes. For instance, in many cases mul-\ntiple images are generated simultaneously using different\nrandom noise vectors, to provide the user with several alter-\nnative results. If Nested Diffusion is used for sampling, a\nuser can see a likeness of the final possibilities. By pruning\nunwanted generation attempts, computational resources can\nbe efficiently allocated to explore additional options based\non the remaining intermediate predictions.\nIn contrast with model fine-tuning methods, Nested Dif-\nfusion can incorporate human feedback inherently, with no\nrequirement for further training. Moreover, Nested Diffu-\nsion may be combined with fine-tuned DMs to further en-\nhance their consistency with the user\u2019s preferences.\nFigure 7 shows an example of a human feedback-based\ngeneration scheme implemented using Nested Diffusion.\nThe samples were generated following the generation de-\ntails provided in Section 4.2, using 3 outer steps with 20\ninner steps each. At the conclusion of each inner diffusion\nprocess, the user is presented with four intermediate sam-\nples, allowing them to select their desired output. The cho-\nsen sample is then propagated to replace the other samples,\nand the sampling algorithm resumes its execution.\nIn addition to selecting from a pool of several options\nto guide the generation, further refinement of the sampling\nprocedure can be achieved by integrating editing techniques\ninto the sampling process.\nThis editing can be accom-\nplished using one of the many available diffusion-based\nimage editing methods\n[1, 4, 14, 25, 35] in tandem with\nNested Diffusion, by modifying the intermediate \u02c6x0, sim-\nilar to SDEdit [35], or adjusting the subsequent inner diffu-\nsion process. However, we adopt a simpler approach: we\nadd details to the textual prompt at the conclusion of each\ninner diffusion process during text-to-image generation. In\nFigure 8, we show some promising results for our approach.\n6. Related Work\nThe noise scheduling in reverse diffusion sampling\nhas garnered considerable attention in recent years [5].\nDDPM [17] implements a linearly increasing schedule,\nwhile IDPM [36] demonstrates the potential of cosine\nscheduling in achieving improved sampling outcomes. In\nDDIM [48], the authors eliminate the forward diffusion\u2019s\nMarkovian assumption, resulting in a deterministic reverse\nprocess that can accelerate sampling. Using ODE solving\nmethods [30,32,33], the sampling process can attain supe-\nrior results and faster generation. Nested Diffusion, while\nnot strictly a noise schedule, intertwines two separate noise\nschedules (the inner and outer diffusion processes) into one\nsampling process.\nCreative scheduling of noise can be employed in other\ndomains besides image generation. In the field of image\nediting, SDEdit [35] degrades an edited clean image with\n7\nText: a photograph of one tree on a hilltop\nText: a photograph of an apple tree on a\nhilltop with red apples growing on it\nText: a photograph of an apple tree on a\nhilltop with red apples growing on it\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a\nsnowy hilltop in winter\nText: a photograph of one tree on a\nsnowy hilltop in winter\nFigure 8. An example of interactive content creation using human feedback. The image selected by the user in each graph is marked\nwith a bright green frame. The text prompt is changed after the first outer step.\nnoise and subsequently denoises it using a DM. This pro-\ncess enhances the realism of the edited image, facilitating\nphoto-realistic editing using simple tools.\nNoise has also been used in inverse problem solvers to\n\u201ctime-travel\u201d in the diffusion process [34, 54]. These ap-\nproaches revert the diffusion process to a previous step by\nadding random Gaussian noise, requiring additional NFEs\nand enhancing image fidelity. However, unlike Nested Dif-\nfusion, these methods add noise to revert a specific num-\nber of steps (a hyperparameter) and do not involve multi-\nple diffusion processes. Consequently, they do not bene-\nfit from the anytime algorithm property and require more\nNFEs compared to alternative approaches.\nNested Diffusion is orthogonal to many diffusion ac-\nceleration methods, such as the fast sampling offered by\nDPM-Solver++ [33] shown in Section 4.2, and may work\nwell with parallelized sampling [38,46] or trajectory-based\nmethods [31, 50]. In this work, we have not delved into\nsome of these avenues for several reasons; Parallelized sam-\npling typically requires more NFEs per image even when\nreducing overall sampling speed. Trajectory-based meth-\nods, while requiring a fraction of the resources, require ad-\nditional training and may still fall short of achieving the\nperformance standards set by multi-step diffusion sampling\ntechniques [37, 41]. We have chosen to optimize Nested\nDiffusion for high quality at the expense of multiple steps\nand use NFEs to measure our computational resources.\nNevertheless, combining these methods with anytime gen-\neration holds promise for future work.\n7. Conclusion\nWe introduced Nested Diffusion, a probabilistic ap-\nproach that harnesses a diffusion process as a building block\nin another diffusion process.\nOur approach allows any-\ntime sampling from a pre-trained diffusion model. Through\nquantitative and qualitative evaluation, we demonstrated the\neffectiveness of Nested Diffusion in tandem with state-of-\nthe-art DMs, including latent diffusion, CFG-based class-\nconditional generation, and text-to-image generation. Fur-\nthermore, we explored the potential of Nested Diffusion\nin enabling generation with human feedback and facilitat-\ning interactive content creation. Our findings highlight the\nversatility and practical applications of Nested Diffusion in\nvarious domains of generative modeling.\n8. Acknowledgements\nThis work was supported by the Israel Science Founda-\ntion grant 2318/22, the Ollendorff Minerva Center, Tech-\nnion, a gift from KLA, and the Council For Higher Educa-\ntion - Planning & Budgeting Committee, Israel.\n8\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18208\u201318218, June\n2022. 7\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022. 5, 13\n[3] Mark Boddy and Thomas L Dean. Solving time-dependent\nplanning problems. Brown University, Department of Com-\nputer Science, 1989. 1\n[4] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, 2023. 1, 7\n[5] Ting Chen. On the importance of noise scheduling for diffu-\nsion models. arXiv preprint arXiv:2301.10972, 2023. 7\n[6] Hyungjin Chung, Jeongsol Kim, Michael Thompson Mc-\ncann, Marc Louis Klasky, and Jong Chul Ye. Diffusion pos-\nterior sampling for general noisy inverse problems. In The\nEleventh International Conference on Learning Representa-\ntions, 2023. 1, 6\n[7] Hyungjin Chung and Jong Chul Ye.\nScore-based diffu-\nsion models for accelerated MRI. Medical Image Analysis,\n80:102479, 2022. 1\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 4\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical im-\nage database. In 2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 248\u2013255, 2009. 11\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat GANs on image synthesis. Advances in Neural Infor-\nmation Processing Systems, 34:8780\u20138794, 2021. 2\n[11] Stephanie Fu*, Netanel Tamir*, Shobhita Sundaram*, Lucy\nChai, Richard Zhang, Tali Dekel, and Phillip Isola. Dream-\nsim: Learning new dimensions of human visual similarity\nusing synthetic data. arXiv:2306.09344, 2023. 8, 11\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems, pages 2672\u20132680,\n2014. 3\n[13] Joshua Grass and Shlomo Zilberstein. Anytime algorithm\ndevelopment tools.\nACM SIGART Bulletin, 7(2):20\u201327,\n1996. 1\n[14] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 1, 7\n[15] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,\nand Yejin Choi. Clipscore: A reference-free evaluation met-\nric for image captioning. arXiv preprint arXiv:2104.08718,\n2021. 5, 8, 11, 13\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in Neural Information Processing Sys-\ntems, volume 30, 2017. 4, 13\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 1, 2, 7\n[18] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4, 5, 13\n[19] Michael C Horsch and David L Poole. An anytime algo-\nrithm for decision making under uncertainty. arXiv preprint\narXiv:1301.7384, 2013. 1\n[20] Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, By-\noung Jin Choi, and Nam Soo Kim.\nDiff-tts: A denois-\ning diffusion model for text-to-speech.\narXiv preprint\narXiv:2104.01409, 2021. 1\n[21] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196, 2017. 4, 6,\n14\n[22] Bahjat Kawar, Michael Elad, Stefano Ermon, and Jiaming\nSong. Denoising diffusion restoration models. In Advances\nin Neural Information Processing Systems, 2022. 1, 4, 6, 13,\n14\n[23] Bahjat Kawar, Noam Elata, Tomer Michaeli, and Michael\nElad. Gsure-based diffusion model training with corrupted\ndata. arXiv preprint arXiv:2305.13128, 2023. 1\n[24] Bahjat Kawar,\nGregory Vaksman,\nand Michael Elad.\nStochastic image denoising by sampling from the posterior\ndistribution. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1866\u20131875, 2021. 1,\n6\n[25] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models. In Con-\nference on Computer Vision and Pattern Recognition 2023,\n2023. 1, 7\n[26] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 4\n[27] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. arXiv preprint arXiv:2009.09761, 2020. 1\n[28] Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins,\nYuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad\nGhavamzadeh, and Shixiang Shane Gu.\nAligning text-\nto-image models using human feedback.\narXiv preprint\narXiv:2302.12192, 2023. 6\n[29] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 5, 13\n9\n[30] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao.\nPseudo\nnumerical methods for diffusion models on manifolds. arXiv\npreprint arXiv:2202.09778, 2022. 7\n[31] Xingchao Liu, Chengyue Gong, and Qiang Liu.\nFlow\nstraight and fast: Learning to generate and transfer data with\nrectified flow. arXiv preprint arXiv:2209.03003, 2022. 8\n[32] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffu-\nsion probabilistic model sampling in around 10 steps. arXiv\npreprint arXiv:2206.00927, 2022. 7\n[33] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided\nsampling of diffusion probabilistic models. arXiv preprint\narXiv:2211.01095, 2022. 1, 6, 7, 8, 13\n[34] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher\nYu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting\nusing denoising diffusion probabilistic models. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11461\u201311471, 2022. 1, 6, 8\n[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia-\njun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided\nimage synthesis and editing with stochastic differential equa-\ntions. In International Conference on Learning Representa-\ntions, 2021. 1, 6, 7\n[36] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 7\n[37] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. arXiv preprint arXiv:2212.09748, 2022.\n4, 8, 11, 13\n[38] Ashwini Pokle, Zhengyang Geng, and J Zico Kolter. Deep\nequilibrium approaches to diffusion models.\nAdvances in\nNeural Information Processing Systems, 35:37975\u201337990,\n2022. 8\n[39] Zhuoran Qiao, Weili Nie, Arash Vahdat, Thomas F Miller III,\nand Anima Anandkumar. Dynamic-backbone protein-ligand\nstructure prediction with multiscale generative diffusion\nmodels. arXiv preprint arXiv:2209.15171, 2022. 1\n[40] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 5, 13\n[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 10684\u201310695, June 2022. 4, 5,\n8, 13\n[42] Chitwan Saharia, William Chan, Huiwen Chang, Chris Lee,\nJonathan Ho, Tim Salimans, David Fleet, and Mohammad\nNorouzi.\nPalette: Image-to-image diffusion models.\nIn\nACM SIGGRAPH 2022 Conference Proceedings, pages 1\u2013\n10, 2022. 6\n[43] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 5, 13\n[44] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In International Confer-\nence on Learning Representations, 2022. 1\n[45] Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb,\nIlia Igashov, Weitao Du, Tom Blundell, Pietro Li\u00b4o, Carla\nGomes, Max Welling, Michael Bronstein, and Bruno Cor-\nreia. Structure-based drug design with equivariant diffusion\nmodels. arXiv preprint arXiv:2210.13695, 2022. 1\n[46] Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh,\nand Nima Anari.\nParallel sampling of diffusion models.\narXiv preprint arXiv:2305.16317, 2023. 8\n[47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n1, 2\n[48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2020. 1, 3, 4, 7, 13\n[49] Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan\nKautz. Pseudoinverse-guided diffusion models for inverse\nproblems. In International Conference on Learning Repre-\nsentations (ICLR), May 2023. 1, 6\n[50] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever.\nConsistency\nmodels.\narXiv\npreprint\narXiv:2303.01469, 2023. 1, 8\n[51] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. Advances in Neural\nInformation Processing Systems, 32, 2019. 1, 2\n[52] Yang Song, Liyue Shen, Lei Xing, and Stefano Ermon. Solv-\ning inverse problems in medical imaging with score-based\ngenerative models. In International Conference on Learning\nRepresentations, 2023. 1\n[53] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space. Advances in Neural In-\nformation Processing Systems, 34:11287\u201311302, 2021. 4\n[54] Yinhuai Wang, Jiwen Yu, and Jian Zhang. Zero-shot im-\nage restoration using denoising diffusion null-space model.\narXiv preprint arXiv:2212.00490, 2022. 1, 8\n[55] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling\nthe generative learning trilemma with denoising diffusion\nGANs. In International Conference on Learning Represen-\ntations (ICLR), 2022. 2, 3\n[56] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, 2018. 8, 11\n[57] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-\nChih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio\nSavarese, Stefano Ermon, et al. Hive: Harnessing human\nfeedback for instructional visual editing.\narXiv preprint\narXiv:2303.09618, 2023. 6\n[58] Shlomo Zilberstein. Using anytime algorithms in intelligent\nsystems. AI magazine, 17(3):73\u201373, 1996. 1\n10\nA. Outer Steps \u2013 Inner Steps Trade-off\nThe ratio RND =\n|outer steps|\n|inner steps| determines the NFEs re-\nquired for each update of the Nested Diffusion intermedi-\nate prediction. Faster update rates come at the expense of\nlower quality in the intermediate prediction samples. To\nillustrate this trade-off, we present Figure 10, which show-\ncases Nested Diffusion sampling with different RND val-\nues while keeping all other hyperparameters and the random\nseed constant.\nTo compare the performance of different Nested Diffu-\nsion hyperparameter choices, we introduce a novel metric \u2013\nthe Area Under the Curve (AUC) of the log FID per NFE\ncurve. The log FID per NFE curve is defined by the log FID\nof the images obtained if the algorithm were to be termi-\nnated at that particular point in the sampling process. This\nmetric captures the intermediate FID scores, their conver-\ngence rate, as well as the frequency of the updates, thus\nconstituting a reasonable metric for anytime generation al-\ngorithm evaluation. In the case of Nested Diffusion, the\nmost recent \u02c6x\u2032\n0 would be returned until the termination of\nthe first inner diffusion process. From this point, the result-\ning image would only be updated at the end of each sub-\nsequent inner diffusion process. An example of this curve\nfor Nested Diffusion, along with its corresponding AUC, is\ndepicted in Figure 9.\nIn Table 1, we present a comparison of various RND\nratios for conditional ImageNet [9] generation using our\nproposed metric.\nThe estimating the log FID per NFE\ncurve is achieved by measuring 50K FID every 10 NFEs\nfor Nested Diffusion totaling 250 NFEs. This metric cap-\ntures the tradeoff between image quality and update speed,\nmaking it relevant for assessing anytime image generation\nalgorithms. We hope this metric proves useful in comparing\nanytime image generation algorithms in the future.\n0\n25\n50\n75\n100\n125\n150\n175\n200\nNFEs\n101\n102\nFID\nFID per NFE \u2013 AUC\nVanilla\nND Outer Di\ufb00usion\nND Inner Di\ufb00usion\nFID-NFE AUC\nFigure 9. Graph of the AUC of the FID-NFE curve.\nOUTER STEPS\nINNER STEPS\nAUC\nFINAL FID\n1\n200\n803.33\n2.2060\n2\n100\n484.49\n2.1919\n4\n50\n354.00\n2.2677\n5\n40\n346.15\n2.3534\n10\n20\n388.18\n2.6267\n20\n10\n521.30\n3.2717\nTable 1. Table of log FID per NFE AUC on ImageNet. The\nresult reflect different choices of inner steps and outer steps, for a\ntotal of 200 NFEs. Vanilla diffusion is equivalent to Nested Diffu-\nsion with one outer step, shown in the top line.\nB. Anytime Consistency\nWhile maintaining consistency between intermediate\nsamples and the final result is significant for an anytime\nalgorithm, it\u2019s equally crucial that the anytime algorithm\ncontinues enhancing image quality during the sampling pro-\ncess, leading to incoherence with previous results. Based on\nthese considerations, we conclude that it is desirable to have\nthe semantic details in the generated image remain mostly\nconsistent during anytime sampling, while the image itself\nmay change. Moreover, the user should be made aware of\nthe degree of expected change for each intermediate result\nproduced by the algorithm, should the sampling procedure\nbe continued.\nTo facilitate a better understanding of the evolution in\nimage dynamics for Nested Diffusion, the average dis-\ntance of intermediate predictions from the final result is\nshown in Figure 11, as computed from images generated\nfor the text-to-image experiment in Section 4.2. The fol-\nlowing metrics are used; LPIPS [56], image-to-image CLIP\nScore [15], MSE, and DreamSim [11]. These metrics can\ngive an insight into the consistency dynamics, ranging from\nnon-semantic metrics such as MSE to highly semantic met-\nrics such as DreamSim [11]. From the graphs, we notice\nthat the trend is similar regardless of the choice of RND.\nThe observed variance for the presented values in Figure 11\nis small to negligible.\nC. Implementation Details\nC.1. Class-Conditional ImageNet Generation\nThe Dit [37] DNN is trained using Kullback Leibler di-\nvergence to yield both the mean and variance of a Gaus-\nsian distribution p\u03b8(x0|xt). The model directly predicts the\nconditional mean of the Gaussian noise in xt and the vari-\nance of p\u03b8(xt\u22121|xt), but we can use a change of variables\nto view these as the mean and variance of p\u03b8(x0|xt), con-\nforming with our notation. When using the Dit [37] DNN\nfor Nested Diffusion, both the inner diffusion and the outer\nare conducted in the latent space. The variance prediction\n11\n6/10\n5/12\n4/15\n3/20\n2/30\n1/60\n1/60\n2/30\n3/20\n4/15\n5/12\n6/10\nFigure 10. Qualitative examples of Nested Diffusion with different ratios RND. Each column denoted with |outer steps|/|inner steps| at\nthe top or bottom. Top text: a photograph of an hourglass filled with snowflakes. Bottom text: a diagram of an ancient sundial. Diffusion\nprocess progresses from top to bottom.\n12\n0\n20\n40\n60\nNFEs\n0.0\n0.2\n0.4\n0.6\nLPIPS\nLPIPS per NFE\n0\n20\n40\n60\nNFEs\n60\n70\n80\n90\n100\nCLIP Score\nCLIP Score per NFE\n0\n20\n40\n60\nNFEs\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nMSE\nMSE per NFE\nVanilla\nND 2 Outer Steps\nND 3 Outer Steps\nND 4 Outer Steps\nND 5 Outer Steps\nND 6 Outer Steps\n0\n20\n40\n60\nNFEs\n0.0\n0.2\n0.4\n0.6\n0.8\nDreamSim\nDreamSim per NFE\nFigure 11. Progression of distance of intermediate predictions from the final result. Metrics are (left to right): LPIPS, image-to-image\nCLIP Score, MSE, and DreamSim.\nis used only in the inner diffusion, while the outer diffusion\nremains deterministic DDIM [48] sampling. CFG [18] is\nregarded as part of the DNN, and therefore applied in the\ninner diffusion only. We set the CFG value to 1.5 similar to\nPeebles & Xie, 2022 [37].\nC.2. Text-to-Image Generation\nWe use Stable Diffusion V1.5 [41] FP32 to generate\n512 \u00d7 512-pixel images. We implement Nested Diffusion\nusing non-deterministic DDIM [48] with \u03b7 = 0.85 for the\ninner diffusion, and treat the CFG [18] as we did in Sec-\ntion 4.1, setting it to the default value of 7.5. No clipping or\nthresholding is applied, and final \u00af\u03b1t set to zero. Due to the\nsize limit for submission, the images shown in the paper and\nsupplementary material have been compressed using JPEG,\nwhich may impact the perceptual image quality.\nIn the MS-COCO [29] FID [16] evaluation we follow\nthe protocol in [2,40,41,43], using a budget of 60 NFEs per\nimage and using the FP16 version of Stable Diffusion V1.5\nfor all configurations. All other hyperparameters remain as\nspecified above.\nThe high-order solver (DPM-Solver++ [33]) setup used\nthe hyper parameters from above except for the following;\nThe inner diffusion was based on DPM-Solver++(2S) [33],\nwith default hyperparamers.\nThe outer diffusion was\nchanged to DDIM with \u03b7 = \u221a1 \u2212 \u00af\u03b1t, for larger stochas-\nticity.\nIn addition to the MS-COCO FID shown in Figure 3b,\nwe present the average CLIP Score [15] of the generated\nimages with their guidance prompt in Figure 12. The CLIP\nScore results show a similar trend to their FID counter parts\n\u2013 Nested Diffusion achieves a high score on the intermedi-\nate results and a slightly improved final image result com-\npared to vanilla diffusion.\nC.3. Inverse Problem solving on CelebA-HQ256\nWe evaluate the following inverse problem tasks; denois-\ning of additive white Gaussian noise with variance set to\n1.0, block-super-resolution with factor 16, colorization, and\nAlgorithm 3 Inverse Problem Solving using Nested Diffu-\nsion\nOuter diffusion denoted in blue, with step size so\nInner diffusion denoted in purple, with step sizes {si\nt}\nxT \u223c N(0, I)\nfor t in {T, T \u2212 so, . . . , 1 + so, 1} do\nx\u2032\nt = xt\n\u25b7 Beginning of inner diffusion\nfor \u03c4 in {t, t \u2212 si\nt, . . . , 1 + si\nt, 1} do\n\u02c6x\u2032\n0 \u223c p\u03b8(x\u2032\n0|x\u2032\n\u03c4, y)\nx\u2032\n\u03c4\u2212si\nt \u223c q\u2032(x\u2032\n\u03c4\u2212si\nt|\u02c6x\u2032\n0, x\u2032\n\u03c4)\nend for\n\u02c6x0 = x\u2032\n0\n\u25b7 End of inner diffusion\nxt\u2212so \u223c q(xt\u2212so|\u02c6x0, xt)\nend for\nreturn x0\ninpainting of 50% random pixels in the image. More infor-\nmation on these degredations can be found in DDRM [22].\n31.20\n31.25\n31.30\n31.35\n31.40\n31.45\nScore\nCLIP Score per NFE\n0\n10\n20\n30\n40\n50\n60\nNFEs\n20.0\n22.5\n25.0\n27.5\n30.0\nCLIP\nVanilla\nND 2 Outer Steps\nND 3 Outer Steps\nND 4 Outer Steps\nND 5 Outer Steps\nND 6 Outer Steps\nFigure 12.\nAverage CLIP Scores of images generated with\nDDIM inner diffusion process.\n13\nPSNR\u2191\n10 NFES\n20 NFES\n30 NFES\nDDRM\nDENOISING\n25.76\n25.80\n25.81\n25.83\nSR16\n23.19\n23.55\n23.89\n23.78\nCOLOR\n19.47\n21.54\n21.55\n23.92\nINPAINTING\n31.20\n33.16\n35.07\n35.08\nFID\u2193\n10 NFES\n20 NFES\n30 NFES\nDDRM\nDENOISING\n19.11\n16.70\n12.97\n12.24\nSR16\n16.77\n13.79\n11.65\n11.30\nCOLOR\n12.78\n7.08\n6.95\n4.28\nINPAINTING\n14.63\n7.57\n3.26\n3.18\nTable 2. PSNR and 30K FID of inverse problems solving on\nCelebA-HQ256. The inverse problems include denoising of addi-\ntive white Gaussian noise, block super-resolution with a factor of\n16, colorization, and inpainting of random pixels, listed from top\nto bottom.\nour Nested Diffusion examples all use default \u03b7 hyper-\nparamters.\nThe inverse problem solving algorithm using Nested Dif-\nfusion is shown in Algorithm 3. The inner diffusion is com-\nposed of a complete inverse problem sampling process (no-\ntice the similarity to Algorithm 2). In our experiment, we\nhave used DDRM [22], an iterative sampling process, as the\naforementioned inverse problem sampling process.\nTable 2 presents PSNR and FID evaluations for Nested\nDiffusion on inverse problem solving. The metrics were\ngenerated on 30K samples from the CelebA-HQ256 [21]\ndataset. We note that Nested Diffusion\u2019s final results are\ncomparable to vanilla DDRM.\n14\nD. More Examples\nWe provide more examples for images generated from various experiments below.\nNested Diffusion\nVanilla\nVanilla\nNested Diffusion\nVanilla\n50 NFEs\n100 NFEs\n150 NFEs\n200 NFEs\n250 NFEs\nNested Diffusion\nFigure 13. Additional samples of ImageNet generation, comparing vanilla diffusion model to Nested Diffusion.\nTOTAL 100 NFES\nTOTAL 150 NFES\nTOTAL 200 NFES\nTOTAL 250 NFES\n%\nNFES\nVAN\nND\nNFES\nVAN\nND\nNFES\nVAN\nND\nNFES\nVAN\nND\n20%\n20\n282.89\n13.03\n30\n282.05\n6.57\n40\n282.83\n4.58\n50\n284.13\n3.57\n40%\n40\n202.34\n9.20\n60\n199.74\n4.99\n80\n197.93\n3.70\n100\n197.74\n3.08\n60%\n60\n65.22\n5.97\n90\n62.37\n3.58\n120\n61.82\n2.96\n150\n60.19\n2.61\n80%\n80\n8.10\n4.00\n120\n7.67\n2.82\n160\n7.65\n2.54\n200\n7.57\n2.36\n100%\n100\n2.44\n3.18\n150\n2.24\n2.50\n200\n2.20\n2.35\n250\n2.16\n2.28\nTable 3. Exact 50K FID evaluation of Nested (ND) and vanilla (Van) diffusion processes. The intermediate prediction are measure\nwhen stopped at different percentages of the full algorithm runtime (100, 150, 200, 250 NFEs).\n15\n20 NFEs\n40 NFEs\n60 NFEs\n80 NFEs\nVanilla\nText: a photograph of a satellite in low earth orbit\nNested Diffusion\n20 NFEs\n40 NFEs\n60 NFEs\n80 NFEs\nVanilla\nNested Diffusion\nText: a photograph of a red rock formation\nFigure 14. More Results of intermediate predictions of Stable Diffusion from a reverse diffusion process with 80 steps.\n16\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of one tree on a hilltop\nText: a photograph of an apple tree on a\nhilltop with red apples growing on it\nText: a photograph of an apple tree on a\nhilltop with red apples growing on it\nText: a photograph of one tree on a\nsnowy hilltop in winter\nText: a photograph of one tree on a\nsnowy hilltop in winter\nFigure 15. Zoom-in on the final images of Figure 7 and Figure 8 for viewing fine details in the images.\n17\n"
  },
  {
    "title": "Controllable Text-to-Image Generation with GPT-4",
    "link": "https://arxiv.org/pdf/2305.18583.pdf",
    "upvote": "1",
    "text": "Controllable Text-to-Image Generation with GPT-4\nTianjun Zhang1\nYi Zhang2\nVibhav Vineet2\nNeel Joshi2\nXin Wang2\n1UC Berkeley\n2Microsoft Research\nAbstract\nCurrent text-to-image generation models often struggle to follow textual instruc-\ntions, especially the ones requiring spatial reasoning. On the other hand, Large\nLanguage Models (LLMs), such as GPT-4, have shown remarkable precision in\ngenerating code snippets for sketching out text inputs graphically, e.g., via TikZ. In\nthis work, we introduce Control-GPT to guide the diffusion-based text-to-image\npipelines with programmatic sketches generated by GPT-4, enhancing their abilities\nfor instruction following. Control-GPT works by querying GPT-4 to write TikZ\ncode, and the generated sketches are used as references alongside the text instruc-\ntions for diffusion models (e.g., ControlNet) to generate photo-realistic images.\nOne major challenge to training our pipeline is the lack of a dataset containing\naligned text, images, and sketches. We address the issue by converting instance\nmasks in existing datasets into polygons to mimic the sketches used at test time.\nAs a result, Control-GPT greatly boosts the controllability of image generation. It\nestablishes a new state-of-art on spatial arrangement and object positioning genera-\ntion and enhances users\u2019 control of object positions, sizes, etc., nearly doubling\nthe accuracy of prior models. As a first attempt, our work shows the potential for\nemploying LLMs to enhance the performance in computer vision tasks. 1\n1\nIntroduction\nResearchers have made remarkable progress in text-to-image generation in recent years, with sig-\nnificant advancements in Generative Adversarial Networks (GANs) [2, 7, 11], autoregressive mod-\nels [20, 29], and Diffusion Models [21, 23, 22]. These models have shown impressive capabilities\nin synthesizing photorealistic images. More recently, large-scale pretrained text-to-image mod-\nels [19, 22, 23] have garnered considerable interest, leading to a multitude of downstream applications,\nsuch as image editing [3, 9] and image inpainting [22].\nHowever, precise control during image generation from textual inputs remains a formidable chal-\nlenge [6]. As illustrated in Figure 1, specifying the exact location, size, or shape of objects using\nnatural language is inherently difficult and prevalent models like DALL-E 2 [21] or Stable Diffu-\nsion [22] often lead to unsatisfactory results. To address this problem, existing approaches often\ndepend on extensive prompt engineering [22] or manually created image sketches [30, 12]. They are\nboth inefficient and difficult to generalize, as they demand substantial manual effort.\nOn the other hand, users typically have greater control over code generation, as they can write\nprograms to manipulate various aspects of objects, such as their shapes, sizes, locations, and more.\nThis allows for precise adjustments and customization according to specific requirements. In recent\ntimes, large language models (LLMs) have demonstrated remarkable capabilities in code generation\ntasks. Models such as GPT-4 [17] have achieved near-human-level performance in coding contests\nand have been successful in solving numerous complex coding problems [4, 15]. This progress\nencourages us to investigate the potential of harnessing LLMs to enhance the controllability of\ntext-to-image generation models.\n1Project website with code: https://github.com/tianjunz/Control-GPT\nPreprint. Under review.\narXiv:2305.18583v1  [cs.CV]  29 May 2023\n(a) DALL-E-2\n(b) Stable Diffusion\n(c) Control-GPT\na microwave \nabove a sink\na sports \nball above a \nbook\na tv to the \nright of a \nmicrowave\nFigure 1: Controllable text-to-image generation with GPT-4 in the loop. Among all the models, Control-\nGPT is both good at generating multiple objects, and the generated image follows exactly the TikZ sketch. Both\nDALL-E 2 and Stable Diffusion are not only unable to generate all the objects stated in the text consistently, but\nit is also hard to control their generated image layout.\nIn this work, we introduce Control-GPT, a simple yet effective framework that harnesses the power\nof LLMs to generate sketches based on text prompts, illustrated in Figure 2. Control-GPT works by\nfirst employing GPT-4 to generate sketches in the form of TikZ code, inspired by [4]. As we can\nsee from Figure 1 (c), the programmatic sketches are drawn following the exact text instructions,\ndespite the rareness of the concepts among natural images. Subsequently, these sketches are fed\ninto Control-GPT, a variant of Stable Diffusion that accepts additional inputs, such as reference\nimages, edges, and segmentation maps. These generated sketches act as reference points for diffusion\nmodels, enabling them to better understand spatial relationships and unusual concepts, as opposed to\nrelying solely on text prompts. This process eliminates the need for human intervention in prompt\nengineering or sketch creation and helps to improve the controllability of diffusion models.\nIn practice, utilizing off-the-shelf pretrained models to synthesize images from textual prompts and\nTikZ sketches can often lead to unsatisfactory results. For example, the GPT-4 generated sketches\nachieve close to 97% accuracy in following the spatial relations described in [6]. However, directly\nusing the generated sketches as segmentation maps and feeding them to the pretrained ControlNet\ncannot translate the performance improvement. This is due to text-to-image models possibly lacking\na full understanding of the content within TikZ sketches, as the pretrained models haven\u2019t seen such\nsketches during training. To address this issue, we convert instance masks from existing instance\nsegmentation datasets, such as COCO [14] and LVIS [8], into polygon representations, which are\nsimilar to sketches generated by GPT-4. We construct a dataset containing triplets of images, textual\ncaptions, and polygon-represented sketches and finetune ControlNet, which takes additional polygons\nand grounding tokens with object names and locations [12] on the constructed dataset. We find this\napproach can mitigate the gap in understanding the GPT-generated sketches and can help the model\nbetter follow the text prompt instructions.\nFor evaluation, we first investigate the capabilities of widely-used LLMs (e.g., LLaMA [28] and\nAlpaca [27] in addition to the GPT series of models) in the context of sketch generation. It is found\nthat only the GPT-4 model can consistently generate reasonable TikZ images, while the majority of\nopen-source models face difficulty in producing compilable TikZ code. We then evaluate our image\ngeneration framework, which incorporates GPT-4, on the spatial relation benchmark created by [6].\nOur proposed model sets a new state-of-the-art with an accuracy rate of 44.2%, almost doubling the\nperformance of the standard stable diffusion models (18.8%). Moreover, human evaluation reveals\nthat our model is capable of handling out-of-distribution prompts and generating intricate scenes\ncomprising multiple objects. As an initial effort, our work demonstrates the potential of integrating\nthe coding abilities of LLMs within visual domains to enhance model controllability.\n2\nRelated Work\nText-to-image models. Large-scale text-to-image models, which are categorized into autoregressive\nmodels [29], and diffusion based models [19, 22, 23] have shown impressive results. The diffusion-\nbased model has recently attracted increasing attention for text-to-image generation. Text-to-image\ndiffusion models often work by encoding text prompts into latent vectors and then using a diffusion\nmodel to diffuse pixels. Notable large-scale models include Stable Diffusion [22], Imagen [23],\n2\nCat\nCat\n{Unicorn, (0.5, 0.5)}\nText Encoder\nImage Encoder\n(Sketch)\nGrounding Tokens\nSketch Tokens\nSelf Attention\nStable Diffusion\nSelf Attention\n\u2026\nSelf Attention\nDraw a unicorn.\nGrounding Text\nSketch Image\nGenerated Image\n\\begin{tikzpicture}\n    \\draw[red, fill=red] (01.5, 2.56)\n    \u2026\nLanguage Model\nPass\nControlNet Architecture\nFigure 2: Control-GPT Architecture. Our model is built on top of ControlNet to take additional grounding\ntext. The model takes in both reference images and grounding object text, fusing them using attention layers\nbefore feeding to Stable Diffusion.\nDALL-E [22], etc. We take diffusion-based text-to-image generation models as a basis and explore\nhow to increase their controllability for precise instruction following.\nControllable generation. Current state-of-the-art image diffusion models predominantly focus on\ntext-to-image methods, making text-guided approaches the most direct method for enhancing control\nover diffusion models. Achieving precise control via text is inherently challenging. ControlNet [30]\nis a neural network architecture that controls pretrained large diffusion models to support additional\ninput conditions like Canny edges, Hough line, human poses, segmentation maps, etc. This model\ndesign enjoys greater flexibility of the condition signals and allows for more precise control of the\ngenerated images. However, such condition signals often need additional human effort to obtain and\nmay not be available for open-world concepts. Our work addresses this issue by integrating diffusion\nmodels with LLMs to generate sketches at test time automatically.\nIn a similar line, GLIGEN [12], an approach for open-set grounded image generation, adds an\nadditional grounding token to encode the bounding box coordinates of the objects. Our work\ncomplements GLIGEN and incorporates its grounding token design. Our work is also in line\nwith image generation from layouts [31, 25, 26, 10, 13]. These techniques typically operate in a\nclosed-world environment characterized by a fixed vocabulary and access to predefined layouts.\nProgrammatic sketch generation. Large language models (LLMs) have exhibited remarkable\nperformance on text-related generation tasks [17], and GPT-4 stands out as the most advanced LLM\nto date. Recently researchers have shown that GPT-4 exhibits \u201csparks\u201d of human-level intelligence [4]\nin a range of application domains. In particular, GPT-4 exhibits near-human level coding capability\nas well as an unexpected surprise \u2014its \u201cimage generation\u201d via writing code, where examples include\nGPT-4 successfully writing a snippet of TikZ commands in LATEX that draws a \u201cunicorn\u201d graphic.\nHowever, the resulting images are usually far from photo-realistic. Our work further quantifies GPT-\n4\u2019s programming ability on sketch generation and integrates it into diffusion-based image generation\nmodels for better controllability. Our work, as a first attempt, shows the potential of integrating LLMs\nwith other domains with a code interface.\n3\nControllable Text-to-Image Generation with GPT-4\n3.1\nPreliminary: ControlNet\nControlNet [30] is a variant of diffusion models that takes additional input conditions. Built upon the\nStable Diffusion model, ControlNet adds a conditional pathway, a trainable copy of the 12 encoding\nblocks, and one middle block of Stable Diffusion while locking the parameters of the trained Stable\nDiffusion model. The neural network blocks are connected by a special convolution layer, \u201czero\nconvolution\u201d, which is a 1\u00d71 convolution layer with both weight and bias initialized with zeros.\nDuring training, ControlNet is finetuned on image-text pairs with conditions represented as Canny\nedges, Hough lines, human poses, user sketches, semantic segmentation maps, etc. Experiments show\nthat ControlNet can generate images following the conditions. In this work, we use ControlNet as the\nbase image generation model and extend it with pathways for programmatic sketches and grounding\ntokens, detailed in the following section.\n3\n3.2\nOur Framework\nWe now introduce our framework that integrates LLM-generated sketches into ControlNet for more\nprecise controllability. As illustrated in Figure 2, we first prompt GPT-4 to generate sketches in TikZ\ncode following the text descriptions and additionally output the object positions. We compile the\nTikZ code in LATEXand convert the sketches into image formats. We then feed the programmatic\nsketches, text descriptions, and grounding tokens of object positions to a tuned ControlNet model to\ngenerate images following the conditions.\nTraining a ControlNet with GPT-4 generated sketches is necessary as pretrained ControlNets do not\nunderstand the generated sketches and cannot translate them into realistic images. The challenge\nis that there are no aligned text, images, and sketches data available for training. In the following\nsections, we will describe the components in our framework and our training process to mitigate the\ngap in understanding the programmatic sketches.\n3.2.1\nTraining Data Construction\nTwo\nzebras\nseem\nto\nbe\nembracing\nin\nthe wild\nA stove with a\nlighted hood in\nthe kitchen\n(a) Captions\n(b) Images\n(c) Polygon Sketches\nFigure 3: Training data construction. We convert\nthe instance masks in LVIS data into polygons and use\nthe corresponding images and captions from COCO to\nconstruct the training data to fine-tune ControlNet.\nFeeding the Language Model-generated Tikz\noutput directly into ControlNet enhances the\ncontrollable generation process. However, Con-\ntrolNet often struggles to accurately interpret\nthe sketches provided. Common issues include\nthe model failing to adhere to the layout of the\nsketch or the prompts given. Therefore, fine-\ntuning ControlNet on sketches is necessary for\nthe model to accept the guidance from program-\nmatic sketches at test time.\nWhen fine-tuning text-to-image models, re-\nsearchers often use massive datasets of text and\nimage pairs from the Internet (e.g., Flicker [18],\nLIAON-5B [24]). In our case, although GPT-\n4 can synthesize unlimited sketches, it\u2019s hard\nto find ground-truth images that align with the\nsketches. Therefore, we explore methods that\nutilize annotations from existing image datasets and try to mimic the sketches to mitigate the gap\nbetween the training and testing data.\nSpecifically, we find that representing instance masks in polygons and turning them into sketch-like\nimages can reduce the gap. As shown in Figure 3, we construct a dataset of caption, image and\npolygon sketch triplets from the COCO [14] and LVIS [8] dataset and use it to fine-tune ControlNet.\nLVIS and COCO share the same set of images and LVIS provides a larger object vocabulary of\nover 1200 classes than COCO. We take roughly 120K images and captions from COCO and convert\nthe object masks from the corresponding LVIS annotations. This enables the model to take in the\npolygons representing objects in an image and generate an output that adheres to the desired style.\nWe will release our constructed dataset to the community.\n3.2.2\nAdditional Object Grounding\nComplementary to sketches and text instructions as a condition, we find that adding additional object\ngrounding tokens to ControlNet is useful to further eliminate ambiguity. The grounding token is\ndefined to associate the sketches with object names and positions, serving as an explicit semantic\nlabel for the sketches. This technique is particularly helpful when the LLM-generated sketches might\nbe too abstract and not accurately represent the intended objects (Figure 4). Without the grounding\ntoken, we can see generation errors like the model successfully follows the layout of the provided\nsketch but generates incorrect objects. Detailed ablations can be found in Appendix.\nTo incorporate the grounding tokens in ControlNet, we design grounding tokens in the form of\n{object_name, object_position}:\nGrounding Tokens : lgd = {lo, lp}\n(1)\nImage Tokens : limg = {lpatch1, lpatch2, ..., lpatchk}\n(2)\n4\nAll Control Tokens : l = Concat(lgd, limg)\n(3)\nHere, lo represents the object name grounding token, lp denotes the position embedding token, and\nlpatchi denotes the ith image patch token in the visual transformer outputs, as illustrated in Figure 2.\nWe permit a maximum of 30 tokens per image and feed these tokens, along with the image tokens,\ninto the transformer blocks to generate control signals.\nFine-tuning process. During training, we fine-tune ControlNet with our constructed dataset based\non COCO. We freeze the stable diffusion blocks and the text branch in ControlNet while fine-tuning\nthe control branch of the network with all the control tokens l, a concatenation of image tokens, and\ngrounding tokens. This design is flexible and effectively separates the control branch from the general\nstable diffusion, allowing for more flexible updates on either side.\n3.2.3\nQuerying GPT-4 for Programmatic Sketches at Inference\nOnce the ControlNet is trained, we can now integrate the GPT-4 pipeline into the text-to-image\ngeneration process. Beyond the objects and scenes that are trained, users can query GPT-4 with novel\nprompts in a zero-shot manner and request Control-GPT to generate photo-realistic images.\nTo prompt GPT-4, we ask users to follow the prompt example below, which will request GPT-4 to\nrequest structured outputs of TikZ code snippets, and the associated object names and positions. We\ncan then use the output from GPT-4 to compile sketch images and obtain the grounding tokens. Note\nthat we specify the image size as 5.12-by-5.12 to make sure LLMs do not get confused by large\nvalues. We later scale the output by 100\u00d7 to match the size of the COCO images used at training.\nWe will present examples of the TikZ code in Appendix and release the code together with the rest of\nthe dataset to the community.\n1\n\"input\": \"Draw a tv above a surfboard\nusing\nTikZ\nwithout\nadding\nlabels\n. The entire\nimage\nshould be inside a 5.12*5.12 bounding\nbox.\nFirst, you need to provide a step -by -step\ndrawing\nguide. Then, you\nneed to generate\nthe code\nfollowing\nthe guide. Finally, summarize\nthe\ndrawing\nwith: Summary of the\ndrawing, {\u2018object name \u2019:\n$OBJECT_NAME, \u2019position \u2019: $(X, Y)} Make sure each\nobject is\nseparted\nand filled\nwith red color .\"\n4\nHow Accurate are LLMs at Drawing Sketches?\nThe precision of Control-GPT largely relies on the precision and controllability of LLMs at generating\nthe sketches at first hand. In this section, we conduct a human evaluation of the outputs of prevalent\nLLMs and benchmark the performance of LLMs on sketch generation. We find that the GPT-series\nmodels are significantly better than open-sourced models like LLaMa [28] and Alpha [27] on sketch\ngeneration and GPT-4 exhibits astonishingly high accuracy (\u223c97%) at following text instructions.\nHuman evaluation.\nWe randomly sample 100 queries from the Visor Dataset [6], which includes\n25K text prompts specifying the spatial relationships of two objects like \u201ca carrot above a boat\u201d,\n\u201ca bird below a bus\u201d. These text prompts are challenging partially because many of them are rare\ncompositions of two unrelated objects, and associating the spatial deixis in text with regions in images\nis not easy. Shown in Table 2 in Section 5, DALL-E 2 and Stable Diffusion can only achieve 37.89%\nand 18.81% on this benchmark. We will defer detailed comparison in the later section.\nModels\n# of errors\nw.r.t instructions\n# of empty image or\nnon-runnable code\nGPT-4 [17]\n3\n5\nGPT-3.5\n24\n7\nChatGPT [1]\n60\n7\nLLaMA [28]\n-\n100\nAlpaca [27]\n-\n100\nTable 1: Human evaluation on sketch generations.\nA bird below \na bus\nA carrot \nabove \na boat\nA handbag \nabove a \npotted plant \n(Failure)\nBUS\nGPT-4\nGPT-3.5\nChat-GPT\nFigure 4: Visualization of the generated sketches\nWe examine the performance of LLMs by querying them to generate a TikZ code snippet to describe\nthe given text description. As shown in Table 1, most of the code snippets from GPT-series models can\nbe compiled to valid sketch images, while the outputs from LLaMA and Alpaca are either empty or\n5\nControl-GPT\nGLIGEN\nGLIDE\nGLIDE+CDM\nDalleM\nCV2\nDALL-E 2\nSD\nSD+CDM\n\u2206\nUncond (%)\n44.17\n13.76\n1.98\n6.43\n16.17\n12.17\n37.89\n18.81\n14.99\n25.36\nCond (%)\n65.97\n31.41\n59.06\n63.21\n59.67\n65.89\n59.27\n62.98\n64.41\n2.99\nOA (%)\n48.33\n19.78\n3.36\n10.17\n27.10\n18.47\n63.93\n29.86\n23.27\n18.57\nVisor 1 (%)\n69.80\n35.64\n62.86\n70.52\n71.22\n74.37\n83.92\n75.67\n74.89\n-5.09\nVisor 2 (%)\n51.20\n13.80\n9.54\n16.48\n32.54\n25.40\n53.86\n32.67\n27.65\n18.53\nVisor 3 (%)\n35.67\n4.55\n1.59\n2.92\n12.82\n7.15\n26.52\n11.19\n9.19\n24.48\nVisor 4 (%)\n20.48\n1.06\n0.26\n0.40\n3.65\n1.26\n8.54\n2.65\n2.12\n17.83\nTable 2: Results on the Visor spatial dataset. We compare different image generation models on their abilities\nto understand spatial locations. Results show that Control-GPT has a significant win on unconditional and\nconditional accuracy, even compared to the closed-source models. For object generation accuracy, Control-GPT\nalso reaches a SoTA performance (except for DALL-E 2, a model trained in private data).\nnot runnable. Within the GPT-series models, we find the latest GPT-4 only fails three times out of the\n95 queries, which have succeeded in generating a valid sketch, giving a roughly 97% in successfully\nfollowing the text instruction. ChatGPT, a finetuned version of GPT-3.5 with reinforcement learning\nwith human feedback (RLHF), significantly underperforms the original GPT-3.5. There might be a\ntrade-off between the chatting ability and the code generation during the tuning process.\nIllustration.\nIn Figure 4, we provide a visualization of sketch examples from the GPT-series models.\nAlthough the generated sketches are not photo-realistic, they often capture the semantic meaning and\ncorrectly reason the spatial relationship of the objects. The generated sketches often surprisingly get\nobject shapes correct with simple code snippets. In the last row of the figure, we show one of the\nfailure cases of GPT-4, where the model fails to generate the object shape, while GPT-3.5 manages to\ngive a correct sketch. The high precision of GPT-4 at sketch generation encourages us to use it to\nenhance the controllability of image generation models.\n5\nExperiments\nIn this section, we evaluate Control-GPT on a range of experimental settings to test its controllability\nregarding to spatial relations, object positions, and sizes based on the Visor dataset [6]. We also\nextend the evaluation to multiple objects and out-of-distribution prompts. Extensive experiments\nshow that Control-GPT can significantly boost the controllability of diffusion models.\nBaselines. We compare with a wide range of baselines, including open-source and closed-source\nmodels. Open-sourced models include (1) GLIGEN [12]: a fine-tuned Stable Diffusion [10] model\nthat takes bounding box and object name as additional control information for grounded text-to-image\ngeneration; (2) ControlNet w/Segmentation [30]: a variant of the Stable Diffusion model using\ninstance segmentation maps as control information; (3) ControlNet w/ Canny Detection: a variant of\nControlNet using canny edge detection images as condition; (4) Stable Diffusion: an open-source\nimage generation model conditioned on text trained on LAION-5B dataset [24] without additional\nconditions except for text; (5) CogView2 [5]: a pretrained image generation model via hierarchical\ntransformers; and (6)GLIDE [16]: a text-conditioned image generation diffusion model by OpenAI.\nFor the closed-sourced model, we compare with (7) DALL-E 2 [21]: OpenAI\u2019s most advanced model\nfor image generation. Our model is based on ControlNet, and we use DALL-E 2 for benchmark\ncomparison only.\nDataset. As mentioned before, our model is finetuned on COCO [14] images and captions with\nLVIS [8]\u2019s instance annotations. We extract the polygon of each object in the dataset and use the\nbounding box to generate the grounding tokens. The size of our dataset is approximately 120k images\nand captions. We finetune the model with four epochs of a learning rate 2e-5. We also freeze the\nStable Diffusion backbone and only finetune the control branch in the network. The details for\ntraining can be found in Appendix.\n5.1\nFollowing Spatial Relations in Text Prompts\nWe adopt the Visor benchmark defined in [6] to examine the models\u2019 ability to follow spatial relations\nin the text instructions. We follow [6] for the evaluation metrics. Given an object A, an object B\nand their spatial relationship in text, Object Accuracy (OA) is used to determine whether the model\ncan generate both objects A and B. Unconditional Accuracy (Uncod) is used to examine whether\n6\nthe model can generate object A and B, as well as get their spatial relationship correct. Conditional\nAccuracy (Cond) is conditioned on object A and B being correctly generated and determines whether\nthe model generate their spatial relationship. Visor k(k = {1, 2, 3, 4}) means for a given relationship,\nat least k generation has the correct object and relationship. For each text prompt, we sample four\nimages to make the evaluation more consistent.\nWe present the evaluation results in Table 2. The Control-GPT gets the SoTA performance under\ndifferent evaluation metrics. It achieves 44.17% on Uncod scores while the base image generation\nmodel, Stable Diffusion, achieves only 18.81%. Control-GPT also outperforms DALL-E 2, a\nproprietary model from OpenAI, which has an Uncod score of 37.89%. Control-GPT also achieves\nSoTA on other scores on Cond compared to other diffusion model variants. We can see that our\nfine-tuned model can largely translate the performance gain from the programmatic sketches and\nmitigate the gap between the training and testing data.\n5.2\nFollowing Object Position, Size and Color in Text Prompts\nWe extend the evaluation to further study more fine-grained control on object sizes and positions\nspecified in text prompts. For evaluation, we randomly sample 100 samples from the Visor dataset and\nassociate them with the position randomly chosen from 4 options and size from 3 options. Figure 5\nshows our prompt for querying GPT-4. Detailed setup is presented in Appendix.\nDraw a giraffe to the left of an apple. The entire image should be 5.12*5.12. The giraffe\nshould be centered at the position (1.5, 2.5) of size (1.0, 1.0). The apple should be centered at\nposition (3.5, 2.5) of size (0.5, 0.5).\nFigure 5: Example prompt for controlling object positions and sizes. Example prompt for benchmarking\nobject position and size for different models. This is directly passed to the GPT-4 to draw the sketch or\nControlNet/Stable Diffusion for generating an image.\nQuantitative evaluation. Following the convention in [6], we use OWL-ViT to measure whether\nthe model follows the text instruction on the objects\u2019 size and location. We run the object detection\nmodel for each image and measure whether the detected object size and position match the ones\nspecified in the prompt. Note that we tolerate \u03f5% error relative to the image size (512\u00d7512) for both\nsize and position. That is, if the detected size and position are within \u03f5% absolute distance relative to\nthe entire figure size, we consider it a success. For baselines, we compare with ControlNet and Stable\nDiffusion. The ControlNet takes in the same GPT-4 TikZ files and prompts with location and size.\nStable diffusion only takes in detailed prompts.\nWe present the quantitative evaluation results in Table 3. We can see that our Control-GPT model\ncan better control the size and location of the objects given some specifications. Compared to Stable\nDiffusion (SD-v1.5), which hardly has control over object positions and sizes, we improve the\noverall accuracy from 0% to 14.18%. Compared to the off-the-shelf ControlNet, Control-GPT also\nachieves better performance across all metrics, obtaining an overall improvement from 8.46% to\n4.18%. These results show the potential of our LLM-integrated framework on more fine-grained and\nprecise control of the image generation process.\nAccuracy\nObj1 Pos\nObj1 Size\nObj2 Pos\nObj2 Size\nAll Pos\nAll Size\nPos & Size\n\u03f5 = 3.9%\nSD-v1.5\n1.84\n1.84\n2.57\n2.20\n1.84\n2.20\n0.00\nControlNet\n13.60\n15.07\n15.44\n15.07\n15.07\n15.07\n8.46\nControl-GPT (ours)\n16.42\n21.64\n17.91\n23.13\n21.64\n23.13\n14.18\n\u2206 (vs. SD)\n14.58\n19.80\n15.34\n20.93\n19.80\n20.93\n14.18\nTable 3: Results on controlling object positions and sizes. Control-GPT outperforms the base Stable Diffusion\nmodel and the off-the-shelf ControlNet model under all metrics by a large margin. Still, we can see the overall\naccuracy of fine-grained control over object positions and sizes is relatively low, and our work shows the potential\nfor using LLMs to improve precise control of image generation.\nVisualization. We additionally present the qualitative results in Figure 6, where we can see Control-\nGPT can draw objects following the specification of object positions and sizes. In contrast, ControlNet\ncan also follow but struggles to generate the correct objects and Stable Diffusion cannot follow the\nspecifications.\n7\n(a) Control-GPT\n(b) ControlNet\n(c) Stable Diffusion\na Suitcase \nabove a car\na car above \na donut\na sports \nball to the \nleft of a \nbird\nFigure 6: Controlling object positions and sizes. Images generated by Control-GPT can follow exactly along\nthe specification of position and size. ControlNet can also follow, but struggles to generate the correct object,\nand Stable Diffusion isn\u2019t able to follow the specifications.\nModel\nVisor Score (%)\nObject Acc (%)\nleft\nright\nabove\nbelow\nleft\nright\nabove\nbelow\nGLIDE\n57.78\n61.71\n60.32\n56.24\n3.10\n3.46\n3.49\n3.39\nGLIDE + CDM\n65.37\n65.46\n59.40\n59.84\n12.78\n12.46\n7.75\n7.68\nCV2\n68.50\n68.03\n63.72\n62.51\n20.34\n19.30\n17.71\n16.54\nDALL-E 2\n56.47\n56.51\n60.99\n63.24\n64.30\n64.32\n65.66\n61.45\nSD\n64.44\n62.73\n61.96\n62.94\n29.00\n29.89\n32.77\n27.8\nSD + CDM\n69.05\n66.52\n62.51\n59.94\n23.66\n21.17\n23.66\n24.61\nGLIGEN\n36.90\n36.37\n36.64\n33.05\n19.78\n19.63\n21.20\n18.77\nControl-GPT\n72.50\n70.28\n67.85\n65.70\n49.80\n48.27\n47.97\n46.95\nTable 4: More analysis on different spatial relations. We compare our model with the aforementioned\nbaselines on four variants of spatial relations. Control-GPT consistently outperforms all the baselines in Visor\nscores. In terms of object accuracy, it also reaches dominant performance except for DALL-E 2.\n5.3\nAblation Study and Analysis\nAblation on spatial relations. We also study whether the model has a preference for different types\nof spatial relationships (e.g., left/right/above/below) as part of the analysis for the spatial relation\nbenchmark in Section 5.1. As we can see from Table 4, Control-GPT works consistently better than\nall the baseline models in terms of Visor Score and object accuracy. One exception is the DALL-E 2\nmodel, which is possibly trained on private data with higher-quality object categories and, therefore,\ngood at generating different objects compared to the rest of open-sourced models. We also notice that\ntext-to-image models often struggle more with above and below compared to left and right.\nRelationship between multiple objects. In previous sections, we have shown evaluations of\ngenerating spatial relations of two objects. In this section, we conduct a further evaluation on\nbenchmarking Control-GPT capability of generating multiple objects, with their spatial relationship\nspecified by prompts. We show some qualitative examples in Figure 7. We see that Control-GPT\nexhibits better performance in understanding the spatial relationship between different objects and\nputting them in the layout with the help of GPT-4. While DALL-E 2 and Stable Diffusion are often\nmissing generating some objects or wrongly layout them in the figure. One example can be seen as\na sandwich below a TV with a spoon to the right of the sandwich, where DALL-E\n2 and Stable Diffusion fail to generate all the objects, but Control-GPT can manage to do it. The\nexperiment shows the potential of Control-GPT in handling complex scene generations.\nControllability vs. photo-realism. We notice that there is often a tradeoff between generating photo-\nrealistic images versus following the exact layout, especially for out-of-distribution text prompts.\nAs shown in Figure 8, the subfigure (a) is an example that the generated image follows exactly the\nlayout, but this results in some artifacts in the image. While in (b), the photo tends to look realistic\nbut doesn\u2019t follow the sketch well. We notice this phenomenon in Control-GPT, which leads to the\nmajority of the spatial errors in the experiments. Since Control-GPT tries to balance the photo-realism\nand instruction following, it explains why Control-GPT didn\u2019t fully translate the performance gain of\nthe programmatic sketches (\u223c 97%).\n8\n(a) DALL-E-2\n(b) Stable Diffusion\n(c) Control-GPT\nstop sign to the left \nof a skis and a skis \nto the left of a kite\ncouch above a toilet \nand a toilet to the \nright of a clock\nan apple above giraffe \nwith a boat to the \nright of the giraffe\na sandwich below a TV \nwith a spoon to the \nright of the sandwidch\nFigure 7: Image generation with multiple objects. Control-GPT is better than DALL-E 2 and Stable Diffusion\nin generating complex scenes involving multiple objects. The two baseline model either suffers from object\nspatial location misinterpretation or fail to generate the corresponding object.\nPerson\nMotor\nPerson\nPlane\nCar\nPerson\n(a) Following Layout w.o. \nMeaningful Content\n(b) Meaningful Content w.o. \nFollowing Layout\nPerson\nCase\nHandbag\nPerson\nPerson\nSurfboard\nDog\nHandbag\nDog\nTie\nFigure 8: Controllability vs. photo-realism . One interesting phenomenon is that Control-GPT has a balance\nof generating images that follows the guideline versus producing images that look photorealistic. (a) Some\nexamples are that the generated image follows exactly the layout but suffers from artifacts. (b) Examples that\nlook photorealistic but doesn\u2019t follow the guideline well.\n6\nConclusion\nWe present a novel method Control-GPT, a controllable text-to-image generation method with the\nguidance of programmatic sketches generated by GPT-4. We augment the ControlNet architecture by\ngrounding tokens and training it with polygons from the existing dataset. By leveraging GPt-4 for\ngenerating TikZ sketch and grounding tokens, our method achieves state-of-the-art performance on\nbenchmarks focusing on the spatial locations of different objects. It also demonstrates great potential\nin controlling object size/position and generating complex scenes. This has large implications for\nusing text-to-image models in many more situations, such as ones where a greater need for creative\nand editorial control is needed (for example in arts and other creative applications). The paper is also\nthe first to demonstrate a possible way for joint optimization over different AI models, opening up\nopportunities in the domain.\n7\nLimitations & Social Impacts\nIn this section, we discuss the limitations and social impact of the developed method and the released\nmodel and how we can better handle social effects. One major limitation of our method is optimizing\nthe model requires a labeled dataset that consists of polygons. This prevents the model from leveraging\nlarge-scale unlabeled datasets. We will seed the potential to utilize unlabeled datasets in the future.\nAs a generative AI agent, one of the potential impacts will always be generating malicious content or\nautomated disinformation.\n9\nReferences\n[1] Chatgpt, 2022. URL https://openai.com/blog/chatgpt.\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high fidelity\nnatural image synthesis. In International Conference on Learning Representations, 2019. URL\nhttps://openreview.net/forum?id=B1xsqj09Fm.\n[3] Tim Brooks, Aleksander Holynski, and Alexei A. Efros. Instructpix2pix: Learning to follow\nimage editing instructions. In CVPR, 2023.\n[4] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[5] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image\ngeneration via hierarchical transformers. arXiv preprint arXiv:2204.14217, 2022.\n[6] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta\nBaral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation. arXiv\npreprint arXiv:2212.10015, 2022.\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sher-\njil Ozair, Aaron Courville, and Yoshua Bengio.\nGenerative adversarial nets.\nIn\nZ. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors,\nAdvances in Neural Information Processing Systems, volume 27. Curran Associates,\nInc., 2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/file/\n5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf.\n[8] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5356\u20135364, 2019.\n[9] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. 2022.\n[10] Manuel Jahn, Robin Rombach, and Bj\u00f6rn Ommer. High-resolution complex scene synthesis\nwith transformers. arXiv preprint arXiv:2105.06458, 2021.\n[11] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for genera-\ntive adversarial networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4396\u20134405, 2019. doi: 10.1109/CVPR.2019.00453.\n[12] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. arXiv preprint\narXiv:2301.07093, 2023.\n[13] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. Image synthesis\nfrom layout with locality-aware mask adaption. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 13819\u201313828, 2021.\n[14] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In David Fleet,\nTomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, editors, Computer Vision \u2013 ECCV 2014,\npages 740\u2013755, Cham, 2014. Springer International Publishing. ISBN 978-3-319-10602-1.\n[15] Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin\nClement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li, Lidong Zhou, Linjun Shou, Long Zhou,\nMichele Tufano, MING GONG, Ming Zhou, Nan Duan, Neel Sundaresan, Shao Kun Deng,\nShengyu Fu, and Shujie LIU. Codexglue: A machine learning benchmark dataset for code\nunderstanding and generation. In J. Vanschoren and S. Yeung, editors, Proceedings of the Neural\nInformation Processing Systems Track on Datasets and Benchmarks, volume 1. Curran, 2021.\nURL\nhttps://datasets-benchmarks-proceedings.neurips.cc/paper_files/\npaper/2021/file/c16a5320fa475530d9583c34fd356ef5-Paper-round1.pdf.\n10\n[16] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing\nwith text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\n[17] OpenAI. Gpt-4 technical report, 2023.\n[18] Bryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pages 2641\u20132649, 2015.\n[19] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang,\neditors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of\nProceedings of Machine Learning Research, pages 8821\u20138831. PMLR, 18\u201324 Jul 2021. URL\nhttps://proceedings.mlr.press/v139/ramesh21a.html.\n[20] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on\nMachine Learning, pages 8821\u20138831. PMLR, 2021.\n[21] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[22] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[23] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[24] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[25] Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout and style. In Proceedings\nof the IEEE/CVF International Conference on Computer Vision, pages 10531\u201310540, 2019.\n[26] Wei Sun and Tianfu Wu. Learning layout and style reconfigurable gans for controllable image\nsynthesis. IEEE transactions on pattern analysis and machine intelligence, 44(9):5070\u20135087,\n2021.\n[27] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\n[28] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[29] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay\nVasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\n[30] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[31] Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image generation from layout. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n8584\u20138593, 2019.\n11\n8\nAppendix\n8.1\nNetwork Architecture\nIn this section, we delve into the intricate details of our network architecture, modeled after the\nControlNet architecture. In adhering to the design principles of ControlNet, we implement a unique\nstrategy where all the weights associated with Stable Diffusion are \u201dfrozen\u201d or fixed, meaning they\ndo not undergo any changes during the training phase.\nBut our design doesn\u2019t stop there. We\u2019ve added an innovative component - a controlling branch - that\nruns parallel to the main network. This supplementary branch is not merely a structural addition;\nit adds complexity and control to the overall architecture. It allows for intricate manipulations and\nadjustments that can substantially influence the overall performance and functionality of the network.\nBy integrating these elements, we believe our architecture balances stability (through freezing Stable\nDiffusion weights) and adaptability (through the controlling branch), creating a versatile and robust\nnetwork model.\nTable 5: Control-GPT Structure\nNetwork Block Name\nResolution\nNumber of Blocks\nSD Encoder Block_1\n64x64\n3\nSD Encoder Block_2\n32x32\n3\nSD Encoder Block_3\n16x16\n3\nSD Encoder Block_4\n8x8\n3\nSD Middle Block\n8x8\n1\n8.1.1\nBounding Box Embedding\nIn this section, we elucidate our bounding box embedding process, which aligns with the methodology\ndetailed in the GLIGEN work. The fundamental strategy we adopt involves the use of Fourier\nembedding to transform the bounding boxes, thereby enabling more nuanced interaction with the\nnetwork architecture.\nTo give a holistic view of the process, consider the bounding box. Rather than including extensive\ndetails, we have chosen to only incorporate the central position of the object within each bounding\nbox. This results in a set of pairs representing the x and y coordinates for each object\u2019s center denoted\nas [Xobj1, Yobj1, Xobj2, Yobj2, ..., Xobjk, Yobjk].\nEach pair from this set is then individually processed through the Fourier embedding network. This\nnetwork serves a role analogous to a tokenizer in a language model, transforming each input into a\nform that can be effectively processed and interpreted by the subsequent layers of our architecture.\nFollowing this Fourier embedding process, we append the resulting vector representations to the\nobject name embeddings. The concatenated vector, representing spatial information and the object\nidentity, is fed into the higher-level attention network. This multi-layered approach ensures that our\nmodel fully comprehends and effectively leverages the rich information within the bounding boxes\nand associated object identities.\n8.1.2\nObject Name Embedding\nIn this segment, we illuminate how the object names associated with each bounding box are embedded.\nTo accomplish this, we rely on the tokenizer and language model encoder from the original stable\ndiffusion model. Given its inherent design to interpret and encode text data, this model offers an\norganic and efficient means of creating text embeddings for the object names.\nFollowing the text embedding, our process doesn\u2019t halt. We move forward to concatenate these text\nembeddings with two other crucial components - the bounding box embeddings and the image patch\nembeddings. Each of these elements adds an additional layer of information, integrating spatial data\nfrom the bounding box and visual data from the image patch with the textual data from the object\nname.\n12\nOnce the concatenation process is complete, we have a multi-dimensional vector that encapsulates a\ncomprehensive range of data. This enriched vector representation is then supplied to the attention\nnetwork. This network is designed to parse this multi-faceted input, emphasizing important features\nwhile downplaying less critical information. Through this comprehensive and layered approach, our\nmodel ensures a thorough understanding and utilization of the diverse data it is provided with.\n8.1.3\nToken Fusion\nWe simply concat all the tokens before feeding them to the ControlNet structure.\nEmbedding = Concat([lbbox, lname, limage])\n(4)\n8.2\nDataset Details\nIn refining our model, we employ the LVIS dataset, a richly annotated subset of the COCO dataset,\nrenowned for its broad array of image categories and detailed labels. This dataset forms the bedrock\nof our fine-tuning process, as we incorporate all the images from the training and evaluation segments.\nThe depth of our dataset extends beyond mere images. We also utilize the bounding boxes, object\nnames, and polygons provided within the LVIS dataset. Each component adds a layer of richness and\ncomplexity to our data. The bounding boxes provide spatial information, indicating the position and\nextent of each object within the images. The object names offer categorical data, identifying the type\nof object encapsulated by each bounding box. The polygons provide even more granular spatial data,\ndetailing the precise shape and orientation of the objects.\nBy combining these diverse data types \u2013 image, spatial, categorical, and shape \u2013 we construct a\ndataset that is not only varied but also highly detailed. This enriched dataset allows us to fine-tune\nour model more effectively, optimizing its performance across a wide array of scenarios.\n8.3\nTraining Details\nFor training, we use the default training parameters as ControlNet. Detailed hyperparameters are\nlisted below in Table 6.\nHyperparameter Name\nValue\nbatch size\n8\nlearning rate\n1e-5\nsd locked\nTrue\nonly mid control\nFalse\nFourier Embedding Dim\n16\nObject Embedding Dim\n768\nTable 6: Control-GPT hyperparameters\n8.4\nMore Examples for Control-GPT\nTo further illustrate our points, we present additional examples derived from Control-GPT, a modifi-\ncation of GPT-4 designed to enhance precision in control. These examples are visually depicted in\nFigure 9 and Figure 10 for your perusal.\nInterestingly, the TikZ sketch associated with the GPT-4 model doesn\u2019t necessarily offer comprehen-\nsive control over the output. This presents a unique opportunity for Control-GPT. In response to this\ngap, Control-GPT has been devised to strike a fine balance between two pivotal factors: the quality\nof the generated image and the adherence to the prescribed layout.\nBy navigating this delicate balance, Control-GPT excels at generating high-quality images that closely\nfollow the given layout specifications. In essence, Control-GPT augments the core functionality\nof GPT-4, enhancing control without compromising image quality, thereby offering an upgraded\nsolution for more precise and aesthetically pleasing outputs.\n13\nA bear to the \nright of a \nbench\nA bear above a \nbench\nA bird to the \nleft of a bag\nA bottle above \na bench\nA bird to the \nright of a \nbottle\nA bird above a \ntottle\nA bird to the \nright of a \nmicrowave\nA bird to the \nleft of a \ncamera\nA bread above a \nmicrowave\nA bicycle to \nthe left of a \nsnowboard\nTikZ Sketch\nControl-GPT\nTikZ Sketch\nControl-GPT\nFigure 9: More sketch examples Control-GPT in the spatial relation dataset. Control-GPT is being\nable to generate images very precisely without any missing objects. It also balances the visual quality\nand layout following to make the image look vivid.\n8.5\nTikZ Code Snippets\nWe also show some code snippets in Figure 11, which are entirely generated by GPT-4. We see that\nthe code structure is almost correct with proper comments. The code compiles without any error. This\nis quite impressive. In addition to that, it captures all the necessary components. This demonstrates\nGPT-4\u2019s strong capability of drawing TikZ code.\n8.6\nVanilla ControlNet Examples\nIn addition, we have conducted an experiment on directly passing the TikZ images to the vanilla\nControlNet trained on segmentation maps. We found that the Vanilla ControlNet is being able to\nfollow the outline of the sketch very well. This is not surprising as it is trained to do this. However, it\ndoesn\u2019t understand the concept of the image well as it almost fails to generate all the objects specified\nin the prompt. The results in Figure 12 demonstrate some examples. ControlNet rarely generates the\ncorrect objects.\n14\nA mouse below \nan umbrella\nA surfboard \nbelow an \numbrella\nA horse to the \nleft of a \nsportsball\nA dog to the \nleft of a \nlaptop\nA dog above \na bench\nA dog to the \nleft of a \nremote \ncontrol\nA dog above \na remote \ncontrol\nA dog to the \nleft of a \nkeyboard\nA dog above \na laptop\nA dog to the \nleft of a \nkeyboard\nTikZ Sketch\nControl-GPT\nTikZ Sketch\nControl-GPT\nFigure 10: Control-GPT is able to generate images very precisely without any missing objects in the\nspatial relation dataset. It also balances the visual quality and layout following to make the image\nlook vivid.\n15\n\\begin{tikzpicture}\n\\path[use as bounding box] (0,0) rectangle (5.12,5.12);\n% Draw the person\n\\fill[red] (1,1) circle (0.5);\n\\fill[red] (1,0.5) rectangle (1.1,1.5);\n\\fill[red] (1.1,1.5) \u2013 (1.5,2.5) \u2013 (1.7,1.5) \u2013 cycle;\n\\fill[red] (1.1,0.5) \u2013 (1.5,-0.5) \u2013 (1.7,0.5) \u2013 cycle;\n% Draw the bus\n\\fill[red] (3,1) rectangle (4.5,2.5);\n\\fill[red] (3.25,1.25) circle (0.25);\n\\fill[red] (4.25,1.25) circle (0.25);\n\\end{tikzpicture}\n\\begin{tikzpicture}\n\\path[use as bounding box] (0,0) rectangle (5.12,5.12);\n% Truck\n\\fill[red] (0.5,1) rectangle (3,2.5);\n% Truck body\n\\fill[red] (2.5,2.5) rectangle (3.5,3.5);\n% Truck cabin\n\\fill[red] (1,1) circle (0.5);\n% Wheel 1\n\\fill[red] (2.5,1) circle (0.5);\n% Wheel 2\n\\fill[red] (3.5,1) circle (0.5);\n% Person\n\\fill[red] (4,3) circle (0.5);\n% Head\n\\fill[red] (3.75,1.5) rectangle (4.25,2.5);\n% Body\n\\draw[red, line width=2pt] (4,2.5) \u2013 (3.5,1);\n% Leg 1\n\\draw[red, line width=2pt] (4,2.5) \u2013 (4.5,1);\n% Leg 2\n\\draw[red, line width=2pt] (4,3) \u2013 (3.5,3.5);\n% Arm 1\n\\draw[red, line width=2pt] (4,3) \u2013 (4.5,3.5);\n% Arm 2\n\\end{tikzpicture}\n\\begin{tikzpicture}\n\\draw[red, fill=red] (1,2) circle (0.25);\n% head\n\\draw[red] (1,2) \u2013 (1,1);\n% body\n\\draw[red] (1,1.5) \u2013 (0.5,1.5);\n% left arm\n\\draw[red] (1,1.5) \u2013 (1.5,1.5);\n% right arm\n\\draw[red] (1,1) \u2013 (0.5,0.5);\n% left leg\n\\draw[red] (1,1) \u2013 (1.5,0.5);\n% right leg\n\\draw[red, fill=red] (3.5,0.5) \u2013 (4.5,0.5) \u2013 (4.12,1) \u2013 (3.88,1) \u2013 cycle;\n% boat\n\\useasboundingbox (0,0) rectangle (5.12,5.12);\n\\end{tikzpicture}\nFigure 11: Examples of TikZ code GPT-4 generates. It almost gets all the syntax correct: the code\ncompiles without any error in LateX.\n16\nAn airplane to \nthe right of a \nclock\nA giraffe below \nan orange\nA backpack \nbelow a tie\nA book below a \ntrain\nA boat below a \nlizza\nA sheep to the \nright of a \nsnowboard\nA cow to the \nright of an \numbrella\nA bus above a \nbook\nA remote to the \nleft of a boat\nA sandwich to \nthe left of a \nhorse\nTikZ Sketch\nControNet\nTikZ Sketch\nControNet\nFigure 12: More examples from the vanilla ControlNet trained on segmentation maps. It hardly\ngenerates the correct objects, as we see in the figure. Despite that, its ability to follow the outline\nsketch is pretty good.\n17\nGPT-4\nGPT-3.5\nChat-GPT\na frisbee to \nthe left of a \ntie\na car above a \nsuitcase\na skis to the \nright of a \nvase\na kite below \na book\na tv above a \nsurfboard\na carrot to \nthe right of a \npizza\nFigure 13: More sketch examples from GPT-4, GPT-3.5, and ChatGPT. GPT-4 consistently outper-\nforms other models in following text instructions and understanding the spatial relations\n8.7\nMore TikZ Examples\nWe also plot more examples from different LLMs we tested in Figure 13. GPT-4 demonstrates strong\ncapability in plotting more details in the figure, including all the objects, and getting their spatial\nlocation correct. Compared to that, GPT-3.5 and Chat-GPT tend to either miss some objects or don\u2019t\nuse enough details in the figure.\n18\n"
  }
]