[
  {
    "title": "M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning",
    "link": "https://arxiv.org/pdf/2306.04387.pdf",
    "upvote": "6",
    "text": "M3IT: A Large-Scale Dataset towards\nMulti-Modal Multilingual Instruction Tuning\nLei Li\u2020, Yuwei Yin\u2020, Shicheng Li\u00a7, Liang Chen\u00a7, Peiyi Wang\u00a7, Shuhuai Ren\u00a7, Mukai Li\u2021\nYazheng Yang\u2020, Jingjing Xu\u2021, Xu Sun\u00a7, Lingpeng Kong\u2020, Qi Liu\u2020\n\u2020 The University of Hong Kong\n\u00a7 National Key Laboratory for Multimedia Information Processing,\nSchool of Computer Science, Peking University\n\u2021 Shanghai AI Lab\nnlp.lilei@gmail.com\njingjingxu@pku.edu.cn\n{lpk, liuqi}@cs.hku.hk\nAbstract\nInstruction tuning has significantly advanced large language models (LLMs) such\nas ChatGPT, enabling them to align with human instructions across diverse tasks.\nHowever, progress in open vision-language models (VLMs) has been limited due\nto the scarcity of high-quality instruction datasets. To tackle this challenge and\npromote research in the vision-language field, we introduce the Multi-Modal, Mul-\ntilingual Instruction Tuning (M3IT) dataset, designed to optimize VLM alignment\nwith human instructions. Our M3IT dataset comprises 40 carefully curated datasets,\nincluding 2.4 million instances and 400 manually written task instructions, refor-\nmatted into a vision-to-text structure. Key tasks are translated into 80 languages\nwith an advanced translation system, ensuring broader accessibility. M3IT sur-\npasses previous datasets regarding task coverage, instruction number and instance\nscale. Moreover, we develop Ying-VLM, a VLM model trained on our M3IT\ndataset, showcasing its potential to answer complex questions requiring world\nknowledge, generalize to unseen video tasks, and comprehend unseen instructions\nin Chinese. We have open-sourced the dataset to encourage further research.1\n1\nIntroduction\nThere has been a continuously increasing trend to develop intelligent assistants that can follow human\ninstructions [3, 36, 37]. In the natural language processing (NLP) field, instruction tuning [35, 53]\nis a success paradigm that leverages large-scale well-formatted instances to align large language\nmodels (LLMs) to human instructions. By finetuning on instances with specific task descriptions,\nLLMs learn to follow the instruction to perform various tasks, and demonstrate strong generalization\nability on unseen tasks [29]. Expanding beyond NLP, a general-purpose intelligent agent must\nencompass various modalities, such as vision, prompting recent efforts to investigate instruction\ntuning in vision-language domains [63, 28, 7]. To develop powerful vision-language models (VLMs),\nit is essential to have a well-constructed dataset that encompasses diverse vision-language tasks and\naligns with human instructions. However, the instructional data supporting existing VLMs is either\nnot publicly available (e.g., GPT-4) or offers limited task and language coverage (e.g., only tasks in\nEnglish are considered). This scarcity of comprehensive datasets has impeded the progress of open\nvision-language models, highlighting the importance of multi-modal instruction tuning and the need\nfor high-quality datasets.\n1Our dataset is available at https://huggingface.co/datasets/MMInstruction/M3IT\nPreprint. Under review.\narXiv:2306.04387v2  [cs.CV]  8 Jun 2023\nIn this paper, we aim to advance instruction tuning research in the multi-modal domain by introducing\nan open dataset M3IT, a Multi-Modal Multilingual Instruction Tuning dataset, as an essential step\ntowards building a versatile general-purpose assistant. We build this dataset by converting existing\ndatasets into a unified vision-to-text schema with four stages: (1) manual instruction writing, (2)\ndataset pre-processing, (3) careful quality check and (4) dataset translation for key tasks. Our dataset\nencompasses a wide range of tasks, including classic image-text tasks such as image classification,\nvisual question answering, and image captioning. Video-related tasks, such as video question-\nanswering, are also incorporated to ensure comprehensive coverage across multiple modalities. We\nfurther integrate Chinese vision-language datasets with corresponding Chinese instructions. The\nresulting dataset compiles 40 diverse tasks and 400 instructions. Finally, key vision-language tasks\nare translated into 80 languages with a strong translation system, to support multilingual studies.\nTo evaluate the effectiveness of the proposed dataset, we develop a vision-language model, Ying-VLM,\nby integrating a strong vision encoder, BLIP-2 [23] with a large language model, Ziya-13B [61],\nderived from LLaMA [49]. Building on the successful approach of incorporating visual tokens as\ntextual prompts in LLMs [7, 63, 28], we employ a two-stage training process: (1) the initial stage\naligns vision features with text embeddings through image captioning on LAION400M [41], and (2)\nthe second stage enhances the model by conducting instruction tuning on selected tasks of our dataset.\nExperimental results reveal that Ying-VLM surpasses strong baseline models in knowledgeable\nVQA tasks and exhibits improved generalization performance to unseen video and cross-lingual\ntasks. Further analysis indicates that the improved performance corresponds to increased tasks for\ninstruction tuning, while the diversity of instructions also affects outcomes.\nThis paper presents two key contributions: (1) We introduce the open-source, large-scale Multi-\nmodal, multilingual Instruction Tuning (M3IT) dataset, designed to enable the development of\ngeneral-purpose multi-modal agents. (2) We develop Ying-VLM, a visual assistant that excels in\nknowledgeable VQA tasks, demonstrates strong generalization to unseen video QA and Chinese\nmulti-modal tasks, and offers valuable insights for future research.\n2\nRelated Work\nTable 1: Summary of multi-modal instruction tuning datasets.\nDataset\n# Tasks\nMulti-Lingual\n# of Instances\nAvg. # of Manual Instructions / Task\nOpen-Sourced\nMiniGPT4\nN / A\n\u2717\n5K\nN / A\n\u2713\nLLaVA\n3\n\u2717\n1.15M\nN / A\n\u2713\nMultiModalGPT\n3\n\u2717\n6K\n5\n\u2717\nMultiInstruct\n26\n\u2717\n\u223c 235K\n5\n\u2717\nInstructBLIP\n28\n\u2717\n\u223c 1.6M\n9.7\n\u2717\nM3IT (Ours)\n40\n\u2713\n2.4M\n10\n\u2713\nOur work draws inspiration from recent language instruction tuning benchmarks [53, 35], which\nhave been proven effective for improving language models to obtain cross-task generalization\nability [29, 52]. In this paper, we focus on exploring the instruction tuning paradigm from LLMs\nto multi-modal agents. Unlike text-only tasks, vision-language tasks generally have more diverse\nformats, which poses new challenges toward vision-language instruction tuning benchmarks.\nTo develop a general-purpose vision-language model, it is crucial to create high-quality multi-modal\ninstruction tuning datasets encompassing diverse tasks, languages, and instructions. Several studies\nhave investigated multi-modal instruction tuning for VLMs. LLaVA [28] and MiniGPT-4 [63]\ngenerate visual content-related dialog by incorporating image caption data into GPT-4/ChatGPT\nmodels. MultiInstruct [56] reformats a series of visual classification tasks into an instruction-tuning\nformat, while InstructBLIP [7] adapts 28 existing image-to-text tasks. However, these datasets do\nnot provide an ideal multi-modal instruction tuning dataset due to their limited (1) coverage of\nvarious task types in multi-modal fields, (2) diversity and quality of instances, and (3) inclusion of\nmultiple languages for wide linguistic diversity. In this paper, we construct an improved multi-modal\ninstruction tuning dataset by expanding task coverage to 40 datasets, supplementing instances with\n10 manually written task instructions, and including tasks in different languages. Table 1 compares\nthe characteristics of existing multi-modal instruction tuning datasets and M3IT.\n2\n3\nM3IT: A Multi-Modal Multilingual Instruction Tuning Dataset\nIn this section, we introduce our proposed M3IT dataset by first elaborating the dataset cover-\nage (\u00a7 3.1), followed by the details of the annotation process(\u00a7 3.2). Finally, we present the dataset\nformat and provide the statistics of the crafted datasets instructions(\u00a7 3.3).\n3.1\nTask Coverage\nOur dataset compiles diverse tasks of classical vision-language tasks, including captioning, visual\nquestion answering (VQA), visual conditioned generation, reasoning and classification.\nCaptioning This task aims to produce descriptions of the given images according to different needs.\nWe include MS COCO [27] (the Karpathy split) for generic image descriptions. TextCaps [44]\nrequires models to capture the text presented in the image and generate captions accordingly. Image-\nParagraph-Captioning [21] focuses on generating detailed descriptions for images.\nReasoning This task evaluates specific reasoning capabilities. We incorporate CLEVR [19] and\nNLVR [46] for spatial reasoning, Visual Commonsense Reasoning (VCR) [60] for commonsense\nreasoning, Visual MRC [47] for reading comprehensive over images, and Winoground [48] for\nfine-grained semantics reasoning over text descriptions and image contents.\nVisual Question Answering (VQA) This is the most widely studied multi-modal task, which requires\nthe model to answer a given question based on the image correctly. Tasks include VQA v2 [15],\nShapes VQA [1], DocVQA [33], OCR-VQA [34], ST-VQA [2], Text-VQA [45], and GQA [18].\nKnowledgeable Visual Question Answering Unlike traditional VQA tasks focusing on the question\nrelevant to the content image, knowledgeable visual question answer (KVQA) requires the model\nto draw upon outside knowledge to answer questions. We incorporate two outside knowledge VQA\ndatasets: OK-VQA [32] and A-OK-VQA [42], ScienceQA [31] which contains multi-modal science\nquestions, and ViQuAE [22] focusing on knowledge facts of named entities in images.\nClassification This task involves classifying an image based on a given set of candidate labels.\nImageNet [40], Grounded Object Identification (COCO-GOI) [27], COCO-Text [50], Image Text\nMatching (COCO-ITM) [27], e-SNLI-VE [20], Multi-modal Fact Checking (Mocheg) [58], and\nIQA [9] are included. Due to language model input length constraints, we reduce the number of\noptions in some datasets with extensive candidate labels, such as ImageNet.\nGeneration Visual conditional general requires models to understand the visual content and make\na composition meeting the task demand. We have Visual Storytelling (VIST) [17], Visual Dialog\n(VisDial) [8], and multi-modal machine translation Multi30k [10] in this category.\nChinese and multilingual Vision-Language Tasks To examine the effect of instruction tuning on\ndifferent languages, we incorporate several Chinese vision-language tasks including FM-IQA [11] for\nVQA, COCO-CN [25] and Flickr8k-CN [24] for captioning, Chinese Food Net [4] for classification,\nand MMChat [62] for generation.\nVideo-Language Tasks Beyond the static images, we are interested in whether instruction tuning\ncan also be applied to video-text tasks. We include the classic MSR-VTT datasets [55] for video\ncaptioning, MSRVTT-QA [54], ActivityNet-QA [59], iVQA [57] and MSVD-QA [54] for video\nquestion answering, Something-Something [14] for video action classification.\nAs shown in Figure 1, our dataset makes a wide coverage of the current existing visual-language and\nvideo-language benchmarks, enabling different skill sets for the language models, from simple image\ncaptioning to complicated reasoning based on the image even beyond the visual content.\n3.2\nAnnotation Process\nTo build high-quality multi-modal instruction datasets, we rewrite various datasets into a vision-to-\ntext format. The annotation process includes four steps: (1) writing instructions for each task, (2)\nstructuring images and texts into a unified schema, (3) checking the overall dataset quality, and (4)\nbuilding multilingual sets. Eight authors of this work are employed as human annotators, each of\nwhom is a graduate student familiar with relevant literature.\n3\nVisual Question \nAnswering\nCOCO-Caption CN\nMSRVTT Captioning\nClassification\nGrounded Object \nIdentification\nImageNet Image \nClassification\nCOCO Text\nImage-Text Matching\ne-SNLI-VE\nAction Classification\nMulti-modal Fact Checking\nShapes VQA\nFM-IQA\nGeneration\nVisual Storytelling\nVisual Dialog\nMulti30k\nMultimodal Chat\nDocVQA\nActivityNet QA\nVQA v2\nKnowledgeable \nQuestion Answering\nOKVQA\nA-OKVQA\nScienceQA\nViQuAE\nIQA\nReasoning\nWinoground\nVisual Commonsense \nReasoning\nCLEVR\nNLVR\nVisual MRC\nImage Captioning\nCOCO Caption\nTextCap\nParagraph Captioning\nST-VQA\nText-VQA\nFlickr-8k-Caption CN \nGQA\niVQA\nMSVD QA\nMSRVTT QA\nOCR-VQA\nChinese Food Classification\nChinese Multi-modal \nDatasets\nVideo Datasets\nFigure 1: Tasks in our proposed multi-modal multilingual instruction tuning dataset. The tasks in\ndashed white boxes are held-out evaluation sets that are not adopted during training. Tasks with bold\nnames are translated into 80 languages.\nTable 2: The statistics of our instructions.\nNumber of different instructions\n400\n- Image Captioning\n52\n- Classification\n113\n- Visual Question Answering\n95\n- Knowledgeable Visual QA\n40\n- Reasoning\n60\n- Generation\n40\nTokens per instruction\n24.4 \u00b1 9.6\nInstruction edit distance among the same task\n76.6 \u00b1 37.2\nInstruction edit distance across tasks\n106.6 \u00b1 39.5\nStage I: Instruction Writing To build high-quality instructions, we first ask annotators to carefully\nread the dataset paper and check the original dataset with some instances to get a clear understanding\nof the task. After that, they are required to write 10 diverse task instructions manually, covering the\nkey characteristics of the task. Table 2 shows the statistics of the written instructions for each task.\nIn total, we annotate 400 instructions for all tasks. The average length per instruction is 24.4. To\nevaluate the diversity of annotated instructions, we employ the average edit distance to measure the\nsimilarity between two strings. The average edit distance within the same task is 76.6, indicating a\ngood range of instruction diversity.\nStage II: Data Format Unification After the instruction has been written according to the task char-\nacteristics, we further process the images and corresponding text for a unified instance schema. For\nmost datasets, we keep the original images and text, where images are converted into corresponding\nbase64 encoded strings for easy data loading. We perform two modifications on potential examples:\n(1) Adding Bounding Box to Images. For tasks designed for specific regions in the image, a\nstraightforward solution is to provide the bounding box information in natural language for informing\nthe language models of the regions in interest. However, the image preprocessing techniques adopted\nby different vision encoders may resize the original image, and the original bounding box annotation\nthus needs further adjustments. Inspired by the recent observation that common vision encoders\nsuch as CLIP [39] are sensitive to the visual prompt [43], we directly tag the bounding box as a red\nrectangle to the image, serving as a hint for VLMs to focus on the target region. (2) Short Answer\nParaphrasing. As recent studies have shown that the original short and brief answers in the common\n4\nBounding Box\nOriginal Image\nPreprocessed Data\nx: 421.0\ny: 57.0\nwidth: 82.0\nheight: 139.0\nQuestion\nWhich song was sung by this woman \njust before Barack Obama was \nsworn in as President of the USA in \n2009?\nAnswer\nMy Country 'Tis Of Thee\nYou are given a question related to an image and a \nshort ground-truth answer. Your task is to \ntransform the ground-truth answer into a natural \nresponse. \nQuestion: What is the color of the dog?\nAnswer: white\nParaphrased Answer: The dog in the image is white. \nQuestion: <question>\nAnswer: <answer>\nParaphrased Answer: \nQuestion\nWhich song was sung by this woman just \nbefore Barack Obama was sworn in as \nPresident of the USA in 2009?\nParaphrased Answer\nThe woman in the image, Aretha Franklin, \nperformed \"My Country 'Tis of Thee\" just \nbefore Barack Obama's inauguration as \nPresident of the USA in 2009.\nPrompt\nOriginal Data\nPreprocessed Data\nIdentify the type of the object in the \ngiven image region.\n(A) chair\n(B) clock\n(C) oven\n(D) car\nAnswer:\n(B) clock\nAdding Bounding Box to Images\nShort Answer Paraphrasing\nFigure 2: (Left) On region-based tasks, bounding boxes are added to original images to inform the\nmodel of the area in interest. (Right) Short answer paraphrasing to improve the response quality.\nVQA dataset could negatively influence the model generation performance [7], we propose to utilize\nthe ChatGPT [36] model for paraphrasing the original answers, by providing origin question and\nanswer with potential extra contextual information. Contextual information includes the caption of\nthe original images and OCR tokens for the scene-related question. The prompt used for answer\nparaphrasing can be found in Appendix. Figure 2 illustrates the data modifications we performed on\nour dataset.\nStage III: Quality Check In this stage, we assign a different annotator to each task to review 10\nexamples from each split. During this stage, we identify minor format inconsistencies between\ntasks and address them by standardizing the task formats. We also observe that a few answers (less\nthan 3% of examined instances) were not effectively paraphrased by ChatGPT due to insufficient\nimage information. We employ simple heuristics to filter these paraphrased answers and use a basic\ntemplate to convert the original answer into a sentence. We find that this small portion of unsuccessful\nparaphrased answers has negligible impact. Finally, the task dataset is deemed complete once the\nannotator can successfully load it and re-examine the accuracy of the instructions, inputs, and outputs\nfor each instance examined.\nStage IV: Key Datasets Translation To boost the language diversity and support the evaluation\nacross different languages, we select a subset of datasets (OK-VQA, ImageNet, Winoground, VQAv2,\nVIST, MSRVTT and MSRVTT-QA) that covers different tasks and translate their evaluation data\ninto 100 languages following FLORES-101 [13]. We translate 500 samples for each split of each\ntask in our first version. More multilingual samples will be supported in the future. We adopt\nthe distillation version NLLB-1.3B [6] for translation, one of the state-of-the-art open multilingual\ntranslation models. As there are no native speakers for different languages, we adopt an automatic\nfiltering mechanism to ensure the translation quality, where languages with translation BLEU scores\nfrom English larger than 20 based on FLORES-101 results are kept. After this step, only 80 languages\nare kept (see Appendix for detailed language names).\n3.3\nDataset Format\nThe instance in our dataset consists of five fields: (1) Images: we represent the images with the\npotentially added bounding box by a base64 string. (2) Instruction: we randomly select an instruction\nfrom the task instruction pool for each instance. (3) Inputs: we allocate this field for providing\ntask-specific inputs to the model, e.g., the question in the VQA tasks. For tasks such as captioning,\nthere is no extra input so the corresponding field is left as an empty string. (4) Outputs: the required\noutput to the specific tasks, such as the description of the image for captioning tasks and the answer\nto the image-related question. (5) Meta Data: we provide this field to preserve important information\nsuch as image id for referencing the original dataset. Figure 3 illustrates an instance in the unified\nformat. With the clear distinction of these fields, the user of our benchmark can flexibly construct the\ntraining instances needed and evaluate the models conveniently. Table 3 gives the statistics aggregated\nby tasks, and we refer readers to Appendix for detailed statistics and the license of each dataset.\n5\n# List[String]: the base64 string representation of a profile photo \nof F. Scott Fitzgerald\nImages: [\"iVBORw0KGg...5ErkJggg==\"]\n# String: task instruction\nInstruction: \"Analyze the image and provide an appropriate \nresponse to the question. \"\n# String: task-specific inputs, e.g., a question related to the image.\nInputs: \"On which book by this man, Baz luhrmann\u2019s planned a \nfilm?\"\n# String: task outputs, e.g., the correct answer for the question.\nOutputs: \"Baz Luhrmann has planned a film adaptation of the \nbook The Great Gatsby. \"\n# Dict: meta information dictionary contains original data.\nMeta Data: {\"kilt_id\": \"qw_1524\", ... ,\"wikipedia_id\": \"152171\"}\nFigure 3: A ViQuAE instance represented in the unified data instance schema used in our dataset.\nTable 3: M3IT task descriptions and statistics, encompassing image captioning (CAP), classification\n(CLS), visual question answering (VQA), knowledgeable visual question answering (KVQA), rea-\nsoning (REA), generation (GEN), Chinese vision-language, and video-language tasks. We aggregate\ninstance counts for training, validation, and test sets across all tasks, totaling 2,429,264 instances.\nTask\nDescription\nTotal #samples\nTrain\nVal\nTest\nCAP\nGiven an image, write a description for the image.\n679,087\n41,462\n27,499\nCLS\nGiven an image, classify the image into pre-defined categories.\n238,303\n100,069\n21,206\nVQA\nGiven an image, answer a question relevant to the image.\n177,633\n46,314\n10,828\nKVQA\nGiven an image, answer the question requires outside knowledge.\n39,981\n11,682\n5,477\nREA\nGiven an image, conduct reasoning over the images.\n99,372\n11,500\n10,000\nGEN\nGiven an image, make compositions with certain requirements.\n145,000\n11,315\n17,350\nChinese\nCAP, CLS, VQA, and GEN tasks in Chinese.\n192,076\n77,306\n4,100\nVideo\nCAP, CLS, and VQA tasks on video-language datasets.\n20,868\n7,542\n9,294\nMulti-lingual\nTranslated tasks in 80 languages\n0\n240,000\n184,000\n4\nExperiments\nIn this section, we build a VLM to validate the effectiveness of the proposed M3IT dataset for\nmulti-modal agents. We first introduce the experimental setups (\u00a7 4.1), then report and discuss the\nresults (\u00a7 4.2). Lastly, we analyze the influence of task number and instruction diversity, and provide\na qualitative result (\u00a7 4.3).\n4.1\nExperimental Settings\nImplementation Details Inspired by the recent success of BLIP [23], we adopt the vision encoder\nand the Q-former architecture in the BLIP2-OPT-2.7B [23] model to extract relevant visual features\nfrom images. For the large language models, we utilize Ziya-13B [61] derived from LLaMA [49]\nwith bilingual (English and Chinese) ability. We employ a two-staged training. Stage I Visual-Text\nAlignment: To align the visual and textual feature space, we utilize the instructions in the coco\ncaptioning and perform an initial alignment training on LAION 400M [41]. We train the Q-former\nand the language projection, resulting in a total 130M parameters to optimize with AdamW [30].\nThe batch size is set to 256 to maximize the utilization of GPU and the model is trained with 300k\nsteps. The learning rate linearly increases to a peak value of 5e-5 in the first 2000 steps and follows a\ncosine decay scheduler. The weight decay is set to 0.05. Stage II Multi-modal Instruction Tuning:\nWe further perform a multi-modal instruction tuning in our benchmark to activate the great potential\nof LLMs. We train the model after alignment training for 3 epochs and with a lower learning rate\nof 1e-5 and a warmup stage of 1000 steps. Inspired by LoRa tuning [16], the weights for mapping\nquery and value vectors in the attention layer of LLMs are learnable in this stage to better adapt to\nthe instruction tuning dataset. Other training parameters are consistent with Stage I. All experiments\nare conducted with 8 NVIDIA 80GB A100 GPUs. It took about 10 days for Stage I and Stage II can\nbe finished in a day.\n6\nTable 4: ROUGE-L evaluation results of KVQA\ntasks. Our Ying-VLM outperforms all the base-\nlines consistently.\nModel\nOK-VQA\nA-OKVQA\nViQuAE\nBLIP2-Flan-T5-XXL\n9.1\n15.6\n9.7\nMiniGPT4\n23.3\n21.8\n24.4\nInstructBLIP\n7.1\n5.9\n7.3\nYing-VLM (Ours)\n27.5\n24.5\n29.6\nTable 5: Zero-shot transfer to Chinese vision-\nlanguage tasks. Our model generalizes well\non unseen Chinese captioning, VQA and clas-\nsification tasks, with the highest ROUGE-L.\nModel\nFlickr-8k-CN\nFM-IQA\nChinese-FoodNet\nMiniGPT4\n9.6\n20.1\n5.0\nInstructBLIP\n5.2\n2.3\n1.0\nYing-VLM (Ours)\n20.5\n33.3\n49.8\nTable 6: Zero-shot transfer to video-language tasks. We report ROUGE-L score for all tasks.\nModel\nVideo Captioning\nVideo Question Answer\nMSRVTT\niVQA\nActivityNet-QA\nMSRVTT-QA\nMSVD-QA\nBLIP-2-Flan-T5-XXL\n8.8\n11.1\n8.9\n10.3\n13.2\nInstructBLIP\n14.3\n6.3\n9.3\n4.0\n7.0\nYing-VLM (Ours)\n14.2\n23.5\n21.9\n18.3\n21.4\nEvaluation Setup To examine the generalization of instruction tuning, some tasks are held-out for\nevaluation (see Figure 1 for held-in/out tasks). We are interested in the following research questions:\n(RQ1) Can multi-modal instruction tuning elicit world knowledge from LLMs? (RQ2) Can English-\nonly instruction tuning generalize to other languages such as Chinese? and (RQ3) Can image-only\nmulti-modal instruction tuning generalize to video-language tasks? For RQ1, we evaluate our models\non three KVQA tasks in our datasets, i.e., OK-VQA [32], A-OKVQA [42] and ViQuAE. For RQ2\nand RQ3, we perform zero-shot transfer evaluation on Chinese vision-language and video-language\ndatasets, respectively. We use greedy decoding in inference if not otherwise specified.\nMetrics We adopt ROUGE-L [26] as an automatic metric to assess the consistency between predic-\ntions and ground-truth answers, focusing on evaluating the model\u2019s conversational abilities. As the\nautomatic metric may not fully capture the nuances of conversational quality, we further introduce\nGPT-4 as a proxy of human evaluators (\u00a7 4.2).\nBaselines We compare our models to recently proposed powerful multi-modal agents, including (1)\nBLIP-2-Flan-T5-XXL [23] where an instruction-tuned Flan-T5 [53] is connected with a powerful\nvision encoder to perform a series of multi-modal tasks; (2) MiniGPT-4 which aligns a CLIP visual\nencoder with a frozen Vicuna [5] with artificially collected dialog dataset; and (3) InstructBLIP, a\nrecently proposed instruction tuning enhanced multi-modal agents with Vicuna-13B with converted\nmulti-model datasets and the LLaVA [28] dataset generated by GPT-4.\n4.2\nMain Results\nRQ1: Knowledgeable Visual Question Answer Evaluation The results on the KVQA benchmarks\nare shown in Table 4. In comparison to the strongest baseline, our model achieves an improvement of\n3.2 and 2.7 ROUGE-L points for OK-VQA and A-OKVQA, respectively. Additionally, Ying-VLM\ndelivers the best performance on the held-out ViQuAE dataset. These findings indicate that instruction\ntuning on M3IT effectively harnesses knowledge from LLMs and elevates response quality.\nRQ2: Zero-Shot Transfer to Chinese Vision-Language Tasks We assess models on three unseen\nChinese vision-language tasks to investigate the cross-language generalization capabilities of instruc-\ntion tuning. BLIP-2 is not considered, as Flan-T5 does not support Chinese.2 As illustrated in Table 5,\nour model outperforms MiniGPT4 and InstructBLIP on all evaluated tasks, demonstrating notable\nimprovements. These findings indicate that instruction tuning with English datasets can effectively\ngeneralize to different languages, showcasing the promising potential that can be further explored.\nRQ3: Zero-Shot Transfer to Video-Language Tasks To evaluate performance on video-language\ntasks, we uniformly sample 8 frames from each video. A comparison with MiniGPT4 is excluded, as\nit does not support video inputs. Following the approach of InstructBLIP [7], we concatenate the\nvisual embedding extracted from the Q-former of each frame as a prefix embedding to the language\nmodel. As demonstrated in Table 6, our model excels in these challenging settings, significantly\n2For all models, we introduce a prompt to promote Chinese outputs. See Appendix D for details.\n7\nsurpassing the BLIP-series baselines. It is worth noting that the training dataset does not include\nany visual inputs such as videos, implying that our instruction tuning effectively aids the model in\ngeneralizing to video inputs with a temporal dimension.\n196\n167\n9\n28\n95\n105\n0%\n20%\n40%\n60%\n80%\n100%\nInstructBLIP\nMiniGPT4\nYing-VLM Won\nTie\nYing-VLM Lost\nFigure 4: Evaluation results using GPT-4 as an evaluator. Our model outperforms MiniGPT-4 and\nInstructBLIP with a winning rate at 55.6% and 65.5%, respectively.\nGPT-4 Evaluation Results To further validate the quality of the generated response, we propose to\nutilize the powerful GPT-4 model as a proxy of human evaluators [38, 12]. Specifically, following\nVicuna [5], we use GPT-4 to rate the performance of different models against our Ying-VLM.\nConsidering the API cost of GPT-4, 300 examples are randomly sampled from OK-VQA, A-OKVQA\nand ViQuAE datasets as a subset for evaluation. For each sample, we construct a prompt consisting of\nthe original question, its corresponding reference answer, the response generated by our Ying-VLM,\nand a baseline system output. GPT-4 is queried with the prompt to rate both responses on a scale\nof ten based on the given question and its reference answer. The ratings are primarily based on\nthe accuracy, relevance, and naturalness of the response to meet the requirements when humans\nare interacting with multi-modal agents (see Appendix for the detailed evaluation template). We\nemploy the strategy proposed by Wang et al. [51] to mitigate potential evaluation biases regarding\nthe response order.3 Figure 4 shows that our Ying-VLM outperforms all baseline models in most\nsamples. Notably, Ying-VLM beat the strongest baseline MiniGPT4 on 167 over 300 tested samples.\nConsistent with the previous ROUGE-L evaluation, this result indicates that the model fine-tuned on\nour instruction dataset can produce more accurate and engaging responses on the challenging KVQA\ntasks.\n4.3\nAnalysis\nWe investigate the effect of task number and instruction diversity on the performance of learned\nmodels, providing insights for future studies to utilize our benchmark better.\nTask Number  \n10\n15\n20\n25\n30\n35\n0\n4\n8\n16\n27\nA-OKVQA\nOK-VQA\nViQuAE\nAverage\nFigure 5: Performance increases with more in-\nstruction tuning datasets.\nInstruction Number\n20\n22\n24\n26\n28\n30\n32\n34\n36\n1\n2\n4\n6\n8\nA-OKVQA\nOK-VQA\nViQuAE\nAverage\nFigure 6: Performance changes with the varied\nnumber of instructions used for training.\n3https://github.com/i-Eval/FairEval\n8\nWho plays at this stadium ?\nThe stadium you are looking at is \ncalled Emirates Stadium and it is the \nhome of Arsenal Football Club. \nArsenal is a professional football club \nbased in London, England, and they \nplay in the Premier League.\nArsenal F.C.\nOn which book by this man, \nBaz luhrmann\u2019s planned a film?\nThe book on which Baz \nLuhrmann\u2018s planned film is \nbased is called \n\u2019The Great Gatsby\u2018\nand was written by F. Scott \nFitzgerald.\nGreat Gatsby\n\u8bf7\u7ed9\u56fe\u4e2d\u7684\u98df\u7269\u5206\u7c7b\u3002\n(Classify the food in the image.)\nOptions:                                                                                               \n(A)\u85af\u6761 (French fries)\n(B) \u51c9\u62cc\u897f\u7ea2\u67ff  (Tomato salad)\n(C) \u6cb9\u7116\u5927\u867e (Braised Shrimp in chili oil)\n(D)\u6247\u8d1d (Scallop in Shell)\n(E) \u751f\u869d (Oysters)\n(F) \u8783\u87f9 (Crab)\n(G)\u626c\u5dde\u7092\u996d (Yangzhou fried rice)\n(H)\u62ab\u8428 (Pizza) \n(I) \u86cb\u631e (Egg Tart)\n(J) \u8089\u9171\u610f\u5927\u5229\u9762 (Spaghetti with meat sauce)   \n(K) \u85af\u6761\n(G) \u626c\u5dde\u7092\u996d\nThe stadium is called Wembley \nStadium, home to the English national \nfootball team.\nThe image shows a black and \nwhite portrait of a man in a suit \nand tie, with short, curly hair \nand a serious expression.\nThis is a bowl of fried rice with various \ningredients including carrots, peas, sausage, \nshrimp, and eggs. It appears to be a Chinese-\nstyle dish with a savory and slightly spicy flavor.\n: InstructBLIP-Vicuna13B\n:   Ying-VLM\n:   MiniGPT4\n: Human\nFigure 7: Case study of the model outputs. Correct answers are bolded with green, wrong answers\nare in red and irrelevant answers are in grey. The model trained with our datasets can provide natural\nand informative responses to entity-centric questions, and generalize to the food classification task in\nChinese (English translation for visualization only).\nEffect of Task Number We investigate the influence of task numbers by randomly shuffling our\ntasks and then selecting a subset to train the model during the instruction tuning stage. Due to the\ncomputational resource limitation, we set up a maximum of 5k examples for each task and train all\nthe models for 5k steps with a batch size of 64. We select 0, 4, 8, 16 and all 27 tasks for training, and\nreport the individual ROUGE-L score and the average score. As illustrated in Figure 5, increasing\nthe number of tasks greatly improves the results of the generalization performance. Besides, the\nperformance gain is not diminished as the task number increases. This is promising as it indicates\nthat we can continually improve performance by introducing more tasks into the training. It would be\ninteresting to investigate the influence of different task clusters, which we leave for future studies.\nEffect Instruction Diversity To investigate the influence of instruction diversity, we limit the number\nof instructions used in each dataset to 1, 2, 4, and 8, resulting in varying levels of diversity for\neach task. The other training parameters are consistent with those used in previous experiments on\ntask number investigation. Figure 6 shows that the performance varies with the level of diversity.\nSpecifically, our results suggest that using four instructions per task is sufficient for achieving decent\nperformance. We leave a more in-depth analysis of the instruction diversity for future work.\nQualitative Results We conduct a case study to provide a more straightforward understanding of\ninstruction-tuned models. The cases are chosen from the held-out ViQuAE and ChineseFoodNet\ndatasets. As shown in Figure 7, our model generates accurate responses to all questions. In contrast,\nMiniGPT4 produces an incorrect answer for the stadium question on the left and fails to follow\ninstructions in the subsequent cases, providing generic image descriptions instead. Additionally,\ncompared to InstructBLIP, which provides concise but less engaging answers for the two questions\nrequiring external knowledge, our model responds more naturally and engagingly, underlining the\nvalue of our dataset. Our model also successfully generalizes to Chinese inputs, accurately classifying\nthe food image based on the instruction. These cases emphasize the importance of instruction tuning\nand demonstrate that our dataset can effectively enhance the capabilities of VLMs.\n5\nConclusion\nIn this paper, we present M3IT, a multi-modal multilingual instruction tuning dataset for aiding the\ndevelopment of multi-modal large language models. The dataset comprises 2.4 million carefully\ncurated instances and 400 manually written task instructions across 40 tasks. We build Ying-VLM\nto validate the effectiveness of our dataset. Quantitative and qualitative results demonstrate that the\n9\nmodels trained with our datasets successfully follow human instructions, provide more engaging\nresponses, and achieve strong generalization performance on unseen video and Chinese tasks. Further\nanalysis shows that the increased task number can continually boost performance, and instruction\ndiversity can influence results. We hope our proposed benchmark, trained models, and experimental\nfindings can facilitate future studies toward building powerful multi-modal intelligent agents.\nA\nDataset Statistics\nTable 7: Detailed task descriptions and statistics of our instruction tuning tasks, including all datasets\nin all types of tasks. The column \u201cUsed\u201d indicates whether we use this dataset in the instruction\ntuning stage.\nTask\nDataset\nUsed\n#samples\nLicense\nTrain\nVal\nTest\nCaptioning\nMS COCO [27]\nYes\n566,747\n25,010\n25,010\nCustom\nTextCaps [44]\nYes\n97,765\n13,965\n0\nUnknown\nImage-Paragraph-Captioning [21]\nYes\n14,575\n2,487\n2,489\nCustom\nClassification\nCOCO-GOI [27]\nYes\n30,000\n2,000\n0\nCustom\nCOCO-Text [50]\nYes\n118,312\n27,550\n0\nCustom\nImageNet [40]\nYes\n30,000\n50,000\n0\nNon-commercial\nCOCO-ITM [27]\nYes\n30,000\n5,000\n5,000\nCustom\ne-SNLI-VE [20]\nYes\n20,000\n14,339\n14,740\nUnknown\nMocheg [58]\nYes\n4,991\n180\n466\nCC BY 4.0\nIQA [9]\nYes\n5,000\n1,000\n1,000\nCustom\nVQA\nVQA v2 [15]\nYes\n30,000\n30,000\n0\nCC-BY 4.0\nShapes VQA [1]\nYes\n13,568\n1,024\n1,024\nUnknown\nDocVQA [33]\nYes\n39,463\n5,349\n0\nUnknown\nOCR-VQA [34]\nYes\n11,414\n4,940\n0\nUnknown\nST-VQA [2]\nYes\n26,074\n0\n4,070\nUnknown\nText-VQA [45]\nYes\n27,113\n0\n5,734\nCC BY 4.0\nGQA [18]\nYes\n30,001\n5,001\n0\nCC BY 4.0\nKVQA\nOK-VQA [32]\nYes\n9,009\n5,046\n0\nUnknown\nA-OK-VQA [42]\nYes\n17,056\n1,145\n0\nUnknown\nScienceQA [31]\nYes\n12,726\n4,241\n4,241\nCC BY-NC-SA\nViQuAE [22]\nNo\n1,190\n1,250\n1,236\nCC By 4.0\nReasoning\nCLEVR [19]\nYes\n30,000\n2,000\n0\nCC BY 4.0\nNLVR [46]\nYes\n29,372\n2,000\n0\nUnknown\nVCR [60]\nYes\n25,000\n5,000\n5,000\nCustom\nVisualMRC [47]\nYes\n15,000\n2,500\n5,000\nUnknown\nWinoground [48]\nNo\n0\n0\n800\nUnknown\nGeneration\nVisual Storytelling [17]\nYes\n5,000\n4,315\n4,350\nUnknown\nVisual Dialog [8]\nYes\n50,000\n1,000\n1,000\nCC By 4.0\nMulti30k [10]\nYes\n90,000\n6,000\n12,000\nNon-commercial\nChinese\nFM-IQA [11]\nNo\n164,735\n75,206\n0\nUnknown\nCOCO-Caption CN [25]\nNo\n18,341\n1,000\n1,000\nNon-commercial\nFlickr-8k-Caption CN [24]\nNo\n6,000\n1,000\n1,000\nCC By 3.0\nChinese Food Classification [4]\nNo\n0\n0\n1,100\nUnknown\nMultimodal Chat [62]\nNo\n3,000\n1,000\n1,000\nUnknown\nVideo\nAction-Classification [14]\nNo\n2,000\n2,000\n2,000\nCustom\niVQA [57]\nNo\n5,994\n2,000\n2,000\nUnknown\nMSVD QA [54]\nNo\n1,161\n245\n504\nUnknown\nActivityNet QA [59]\nNo\n3,200\n1,800\n800\nUnknown\nMSRVTT QA [54]\nNo\n6,513\n497\n2,990\nUnknown\nMSRVTT Captioning [55]\nNo\n2,000\n1,000\n1,000\nUnknown\nTable 7 lists the detailed statistics in our benchmark. We collect the dataset license from PaperWith-\nCode.4 For datasets under Unknown and Custom licenses, we suggest the users check the project\npage or contact the dataset owner before usage.\n4https://paperswithcode.com/\n10\nB\nTemplate for Answer Paraphrase\nWe provide the paraphrase template in Table 8 for querying the ChatGPT to re-write the original\nshort answers, where {Q} and {A} is filled with the question and the answer need to be paraphrased,\nrespectively. We incorporate an example to better inform the model of the paraphrasing tasks. For\nVQAv2 tasks, we add an extra {Caption} field in the template filled with corresponding captions\nfrom the COCO dataset to provide extra context information to help to paraphrase.\nTable 8: Template used to query ChatGPT for answer paraphrasing.\nYou are an AI visual assistant. Now you are given a question related to an image and a short\nground-truth answer. Your task is to transform the ground-truth answer into a natural and\nconvincing response. Make sure the response is accurate, highly relevant to the question, and\nconsistent with the original answer.\nQuestion:\nWhich NASA space probe was launched to this planet in 1989?\nAnswer:\nMagellan\nTransformed Answer:\nNASA sent the Magellan spacecraft to Venus in 1989, which was the first planetary spacecraft\nlaunched from a space shuttle.\nQuestion:\n{Q}\nAnswer:\n{A}\nTransformed Answer:\nC\nDataset Translation\nWe translate all the task instructions and evaluation sets of ImageNet, Winoground, VQAv2, OK-\nVQA, VIST, MSRVTT and MSRVTT-QA into 80 languages, as shown in Table 9. Due to the\ncomputational resource constraint, we translate the whole test of Winoground ( 800 examples) and\nset a maximum instance number of 500 for each split in other tasks.\nD\nPrompt for Zero-Shot Chinese Vision-Language Tasks\nIn our experiments, all Vision-Language models are fine-tuned exclusively using English data. In our\npreliminary study, we observe that these models tend to generate English responses, even when the\ninput and instructions are written in Chinese. We introduce a simple Chinese dialogue context during\nthe zero-shot Chinese Vision-Language Task evaluation for all models, as illustrated in Table 10,\nInterestingly, this minor adjustment can encourage models to produce reasonable Chinese output. We\nleave the analysis of instruction-tuned VLM models\u2019 multilingual capabilities for future research.\nE\nTemplate for GPT-4 Evaluation\nWe adopt the template in Table 11 to query GPT-4 and obtain the evaluation results with\nFairEval\n5 to obtain more stable results.\nSpecifically, each tested instance is a quater-\nnion: (question, reference, response1, response2), where response1 and response2\nare two responses from our Ying-VLM and the baseline model, respectively.\nFor each in-\nstance, we query GPT-4 to judge which response is of better quality regarding accuracy, rel-\nevance and naturalness.\nWe populate the quaternion into the evaluation template to form\n5https://github.com/i-Eval/FairEval\n11\nTable 9: List of Language Codes, Scripts, and Languages Names for translated datasets.\nLanguage Code\nScript\nLanguage Name\naf\nafr_Latn\nAfrikaans\nam\namh_Ethi\nAmharic\nar\narb_Arab\nModern Standard Arabic\nas\nasm_Beng\nAssamese\nast\nast_Latn\nAsturian\nbe\nbel_Cyrl\nBelarusian\nbg\nbul_Cyrl\nBulgarian\nbn\nben_Beng\nBengali\nbs\nbos_Latn\nBosnian\nca\ncat_Latn\nCatalan\nceb\nceb_Latn\nCebuano\ncs\nces_Latn\nCzech\ncy\ncym_Latn\nWelsh\nda\ndan_Latn\nDanish\nde\ndeu_Latn\nGerman\nel\nell_Grek\nGreek\nes\nspa_Latn\nSpanish\net\nest_Latn\nEstonian\nfi\nfin_Latn\nFinnish\nfr\nfra_Latn\nFrench\nfuv\nfuv_Latn\nNigerian Fulfulde\ngl\nglg_Latn\nGalician\ngu\nguj_Gujr\nGujarati\nha\nhau_Latn\nHausa\nhe\nheb_Hebr\nHebrew\nhi\nhin_Deva\nHindi\nhr\nhrv_Latn\nCroatian\nhu\nhun_Latn\nHungarian\nhy\nhye_Armn\nArmenian\nid\nind_Latn\nIndonesian\nig\nibo_Latn\nIgbo\nis\nisl_Latn\nIcelandic\nit\nita_Latn\nItalian\nja\njpn_Jpan\nJapanese\njv\njav_Latn\nJavanese\nka\nkat_Geor\nGeorgian\nkk\nkaz_Cyrl\nKazakh\nkm\nkhm_Khmr\nKhmer\nkn\nkan_Knda\nKannada\nko\nkor_Hang\nKorean\nky\nkir_Cyrl\nKyrgyz\nlb\nltz_Latn\nLuxembourgish\nlg\nlug_Latn\nGanda\nlij\nlij_Latn\nLigurian\nli\nlim_Latn\nLimburgish\nln\nlin_Latn\nLingala\nlo\nlao_Laoo\nLao\nlt\nlit_Latn\nLithuanian\nlv\nlvs_Latn\nStandard Latvian\nmi\nmri_Latn\nMaori\nmk\nmkd_Cyrl\nMacedonian\nml\nmal_Mlym\nMalayalam\nmr\nmar_Deva\nMarathi\nmt\nmlt_Latn\nMaltese\nmy\nmya_Mymr\nBurmese\nnl\nnld_Latn\nDutch\nny\nnya_Latn\nNyanja\noc\noci_Latn\nOccitan\npa\npan_Guru\nEastern Panjabi\npl\npol_Latn\nPolish\npt\npor_Latn\nPortuguese\nro\nron_Latn\nRomanian\nru\nrus_Cyrl\nRussian\nsd\nsnd_Arab\nSindhi\nsk\nslk_Latn\nSlovak\nsn\nsna_Latn\nShona\nso\nsom_Latn\nSomali\nsr\nsrp_Cyrl\nSerbian\nsv\nswe_Latn\nSwedish\nta\ntam_Taml\nTamil\nte\ntel_Telu\nTelugu\ntg\ntgk_Cyrl\nTajik\nth\ntha_Thai\nThai\ntl\ntgl_Latn\nTagalog\ntr\ntur_Latn\nTurkish\nuk\nukr_Cyrl\nUkrainian\nur\nurd_Arab\nUrdu\nvi\nvie_Latn\nVietnamese\nwo\nwol_Latn\nWolof\nzh\nzho_Hans\nChinese (Simplified)\ntwo query prompts:\nT(Q=question, R=reference, R1=response1, R2=response2) and\nT(Q=question, R=reference, R1=response2, R2=response1). We set the temperature of\nGPT-4 to 1 and sample three completions for each query prompt. Therefore, each response will\n12\nTable 10: Prompt for promoting Chinese outputs.\n<human>: \u8bf7\u6839\u636e\u6211\u7684\u6307\u793a\uff0c\u4ee5\u53ca\u6240\u7ed9\u7684\u56fe\u7247\uff0c\u505a\u51fa\u76f8\u5e94\u7684\u56de\u7b54\u3002\n<bot>:\n\u597d\u7684\u3002\n<human>:\n{Instruction}\n{Input}\n<bot>:\n\u597d\u7684\u3002\nreceive 6 scores, and we use the average score as the final score for each response. The response\nwith the higher final score is considered the better response. The GPT-4 evaluation incurred a cost of\n$20.45 for InstructBlip and $20.90 for MiniGPT-4.\nTable 11: Template used to query GPT-4 for evaluating the response quality of different models.\n[Question]\n{Q}\n[The Start of Reference Answer]\n{R}\n[The End of Reference Answer]\n[The Start of Assistant 1\u2019s Answer]\n{R1}\n[The End of Assistant 1\u2019s Answer]\n[The Start of Assistant 2\u2019s Answer]\n{R2}\n[The End of Assistant 2\u2019s Answer]\n[System]\nWe would like to request your feedback on the performance of two AI assistants in response\nto the user\u2019s multimodal question displayed above. We provided no multimodal inputs other\nthan question text, but we provided a reference answer for this question. You need to evaluate\nthe quality of the two responses based on the question and the reference answer.\nPlease rate the on the follow aspects:\n1. Accuracy: whether the candidate\u2019s response is consistent with the original answer, this is\nimportant as we do not want a misleading result;\n2. Relevance: whether the candidate\u2019s response is highly relevant to the question and image\ncontent;\n3. Naturalness: whether the candidate\u2019s response is engaging, providing a great communica-\ntion experience for the user when interacting with the AI visual assistant.\nof the two Assistants\u2019 responses.\nEach assistant receives an overall score on a scale of 1 to 10, where a higher score indicates\nbetter overall performance.\nPlease first provide a comprehensive explanation of your evaluation, avoiding any potential\nbias and ensuring that the order in which the responses were presented does not affect your\njudgment.\nThen, output two lines indicating the scores for Assistant 1 and 2, respectively.\nOutput with the following format:\nEvaluation evidence: <evaluation explanation here>\nThe score of Assistant 1: <score>\nThe score of Assistant 2: <score>\n13\nReferences\n[1] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein. Neural module networks. In 2016 IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA,\nJune 27-30, 2016, pages 39\u201348, 2016.\n[2] A. F. Biten, R. Tito, A. Mafla, L. G. i Bigorda, M. Rusi\u00f1ol, C. V. Jawahar, E. Valveny, and\nD. Karatzas. Scene text visual question answering. In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages\n4290\u20134300, 2019.\n[3] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\nguage models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and\nH. Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on\nNeural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual,\n2020.\n[4] X. Chen, Y. Zhu, H. Zhou, L. Diao, and D. Wang. Chinesefoodnet: A large-scale image dataset\nfor chinese food recognition. ArXiv preprint, abs/1705.02743, 2017.\n[5] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng, S. Zhuang, Y. Zhuang, J. E.\nGonzalez, I. Stoica, and E. P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with\n90%* chatgpt quality, 2023.\n[6] M. R. Costa-juss\u00e0, J. Cross, O. \u00c7elebi, M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi,\nJ. Lam, D. Licht, J. Maillard, et al. No language left behind: Scaling human-centered machine\ntranslation. ArXiv preprint, abs/2207.04672, 2022.\n[7] W. Dai, J. Li, D. Li, A. M. H. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi. Instructblip:\nTowards general-purpose vision-language models with instruction tuning. ArXiv preprint,\nabs/2305.06500, 2023.\n[8] A. Das, S. Kottur, K. Gupta, A. Singh, D. Yadav, J. M. F. Moura, D. Parikh, and D. Batra. Visual\ndialog. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017,\nHonolulu, HI, USA, July 21-26, 2017, pages 1080\u20131089, 2017.\n[9] Z. Duanmu, W. Liu, Z. Wang, and Z. Wang. Quantifying visual image quality: A bayesian view.\nAnnual Review of Vision Science, 7:437\u2013464, 2021.\n[10] D. Elliott, S. Frank, K. Sima\u2019an, and L. Specia. Multi30K: Multilingual English-German image\ndescriptions. In Proceedings of the 5th Workshop on Vision and Language, pages 70\u201374, 2016.\n[11] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine?\ndataset and methods for multilingual image question. In C. Cortes, N. D. Lawrence, D. D. Lee,\nM. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28:\nAnnual Conference on Neural Information Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 2296\u20132304, 2015.\n[12] F. Gilardi, M. Alizadeh, and M. Kubli. Chatgpt outperforms crowd-workers for text-annotation\ntasks. ArXiv preprint, abs/2303.15056, 2023.\n[13] N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato,\nF. Guzm\u00e1n, and A. Fan. The Flores-101 evaluation benchmark for low-resource and multilingual\nmachine translation. Transactions of the Association for Computational Linguistics, 10:522\u2013538,\n2022.\n[14] R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fr\u00fcnd,\nP. Yianilos, M. Mueller-Freitag, F. Hoppe, C. Thurau, I. Bax, and R. Memisevic. The \"some-\nthing something\" video database for learning and evaluating visual common sense. In IEEE\nInternational Conference on Computer Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,\npages 5843\u20135851, 2017.\n14\n[15] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and D. Parikh. Making the V in VQA matter:\nElevating the role of image understanding in visual question answering. In 2017 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26,\n2017, pages 6325\u20136334, 2017.\n[16] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen. Lora:\nLow-rank adaptation of large language models. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n[17] T.-H. K. Huang, F. Ferraro, N. Mostafazadeh, I. Misra, A. Agrawal, J. Devlin, R. Girshick,\nX. He, P. Kohli, D. Batra, C. L. Zitnick, D. Parikh, L. Vanderwende, M. Galley, and M. Mitchell.\nVisual storytelling. In Proceedings of the 2016 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, pages 1233\u20131239,\n2016.\n[18] D. A. Hudson and C. D. Manning. GQA: A new dataset for real-world visual reasoning and\ncompositional question answering. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 6700\u20136709, 2019.\n[19] J. Johnson, B. Hariharan, L. van der Maaten, L. Fei-Fei, C. L. Zitnick, and R. B. Girshick.\nCLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In\n2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu,\nHI, USA, July 21-26, 2017, pages 1988\u20131997, 2017.\n[20] M. Kayser, O. Camburu, L. Salewski, C. Emde, V. Do, Z. Akata, and T. Lukasiewicz. e-vil:\nA dataset and benchmark for natural language explanations in vision-language tasks. In 2021\nIEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada,\nOctober 10-17, 2021, pages 1224\u20131234, 2021.\n[21] J. Krause, J. Johnson, R. Krishna, and L. Fei-Fei. A hierarchical approach for generating\ndescriptive image paragraphs. In 2017 IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 3337\u20133345, 2017.\n[22] P. Lerner, O. Ferret, C. Guinaudeau, H. Le Borgne, R. Besan\u00e7on, J. G. Moreno, and J. Lov\u00f3n Mel-\ngarejo. Viquae, a dataset for knowledge-based visual question answering about named entities.\nIn Proceedings of the 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, pages 3108\u20133120, 2022.\n[23] J. Li, D. Li, S. Savarese, and S. Hoi. Blip-2: Bootstrapping language-image pre-training with\nfrozen image encoders and large language models. ArXiv preprint, abs/2301.12597, 2023.\n[24] X. Li, W. Lan, J. Dong, and H. Liu. Adding chinese captions to images. In Proceedings of the\n2016 ACM on international conference on multimedia retrieval, pages 271\u2013275, 2016.\n[25] X. Li, C. Xu, X. Wang, W. Lan, Z. Jia, G. Yang, and J. Xu. Coco-cn for cross-lingual image\ntagging, captioning, and retrieval. IEEE Transactions on Multimedia, 21(9):2347\u20132360, 2019.\n[26] C.-Y. Lin. ROUGE: A package for automatic evaluation of summaries. In Text Summarization\nBranches Out, pages 74\u201381, 2004.\n[27] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick.\nMicrosoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European\nConference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages 740\u2013755.\nSpringer, 2014.\n[28] H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. ArXiv preprint, abs/2304.08485,\n2023.\n[29] S. Longpre, L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei,\net al. The flan collection: Designing data and methods for effective instruction tuning. ArXiv\npreprint, abs/2301.13688, 2023.\n15\n[30] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019,\n2019.\n[31] P. Lu, S. Mishra, T. Xia, L. Qiu, K.-W. Chang, S.-C. Zhu, O. Tafjord, P. Clark, and A. Kalyan.\nLearn to explain: Multimodal reasoning via thought chains for science question answering. In\nThe 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.\n[32] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi. OK-VQA: A visual question answering\nbenchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3195\u20133204, 2019.\n[33] M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: A dataset for vqa on document images.\nIn Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages\n2200\u20132209, 2021.\n[34] A. Mishra, S. Shekhar, A. K. Singh, and A. Chakraborty. Ocr-vqa: Visual question answering by\nreading text in images. In 2019 international conference on document analysis and recognition\n(ICDAR), pages 947\u2013952. IEEE, 2019.\n[35] S. Mishra, D. Khashabi, C. Baral, and H. Hajishirzi. Cross-task generalization via natural\nlanguage crowdsourcing instructions.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 3470\u20133487, 2022.\n[36] OpenAI. Introducing chatgpt. 2022.\n[37] OpenAI. Gpt-4 technical report, 2023.\n[38] B. Peng, C. Li, P. He, M. Galley, and J. Gao. Instruction tuning with gpt-4. ArXiv preprint,\nabs/2304.03277, 2023.\n[39] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, G. Krueger, and I. Sutskever. Learning transferable visual models from\nnatural language supervision. In M. Meila and T. Zhang, editors, Proceedings of the 38th\nInternational Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event,\nvolume 139 of Proceedings of Machine Learning Research, pages 8748\u20138763, 2021.\n[40] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. International\njournal of computer vision, 115:211\u2013252, 2015.\n[41] C. Schuhmann, R. Vencu, R. Beaumont, R. Kaczmarczyk, C. Mullis, A. Katta, T. Coombes,\nJ. Jitsev, and A. Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text\npairs. ArXiv preprint, abs/2111.02114, 2021.\n[42] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: A benchmark\nfor visual question answering using world knowledge. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part VIII, pages\n146\u2013162. Springer, 2022.\n[43] A. Shtedritski, C. Rupprecht, and A. Vedaldi. What does clip know about a red circle? visual\nprompt engineering for vlms. ArXiv preprint, abs/2304.06712, 2023.\n[44] O. Sidorov, R. Hu, M. Rohrbach, and A. Singh. Textcaps: a dataset for image captioning with\nreading comprehension. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758. Springer, 2020.\n[45] A. Singh, V. Natarajan, M. Shah, Y. Jiang, X. Chen, D. Batra, D. Parikh, and M. Rohrbach.\nTowards VQA models that can read. In IEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 8317\u20138326, 2019.\n[46] A. Suhr, M. Lewis, J. Yeh, and Y. Artzi. A corpus of natural language for visual reasoning.\nIn Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 217\u2013223, 2017.\n16\n[47] R. Tanaka, K. Nishida, and S. Yoshida. Visualmrc: Machine reading comprehension on\ndocument images. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021,\nThirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual\nEvent, February 2-9, 2021, pages 13878\u201313888, 2021.\n[48] T. Thrush, R. Jiang, M. Bartolo, A. Singh, A. Williams, D. Kiela, and C. Ross. Winoground:\nProbing vision and language models for visio-linguistic compositionality. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5238\u20135248, 2022.\n[49] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. ArXiv\npreprint, abs/2302.13971, 2023.\n[50] A. Veit, T. Matera, L. Neumann, J. Matas, and S. Belongie. Coco-text: Dataset and benchmark\nfor text detection and recognition in natural images. ArXiv preprint, abs/1601.07140, 2016.\n[51] P. Wang, L. Li, L. Chen, D. Zhu, B. Lin, Y. Cao, Q. Liu, T. Liu, and Z. Sui. Large language\nmodels are not fair evaluators. ArXiv preprint, abs/2305.17926, 2023.\n[52] Y. Wang, S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S.\nDhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mon-\ndal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar,\nM. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat,\nS. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. Super-NaturalInstructions: Generaliza-\ntion via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference\non Empirical Methods in Natural Language Processing, pages 5085\u20135109, 2022.\n[53] J. Wei, M. Bosma, V. Y. Zhao, K. Guu, A. W. Yu, B. Lester, N. Du, A. M. Dai, and Q. V. Le.\nFinetuned language models are zero-shot learners. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022.\n[54] D. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering\nvia gradually refined attention over appearance and motion. In Proceedings of the 2017 ACM\non Multimedia Conference, MM 2017, Mountain View, CA, USA, October 23-27, 2017, pages\n1645\u20131653, 2017.\n[55] J. Xu, T. Mei, T. Yao, and Y. Rui. MSR-VTT: A large video description dataset for bridging\nvideo and language. In 2016 IEEE Conference on Computer Vision and Pattern Recognition,\nCVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pages 5288\u20135296, 2016.\n[56] Z. Xu, Y. Shen, and L. Huang. Multiinstruct: Improving multi-modal zero-shot learning via\ninstruction tuning. ArXiv preprint, abs/2212.10773, 2022.\n[57] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid. Just ask: Learning to answer questions\nfrom millions of narrated videos. In 2021 IEEE/CVF International Conference on Computer\nVision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 1666\u20131677, 2021.\n[58] B. M. Yao, A. Shah, L. Sun, J.-H. Cho, and L. Huang. End-to-end multimodal fact-checking\nand explanation generation: A challenging dataset and models. ArXiv preprint, abs/2205.12487,\n2022.\n[59] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao. Activitynet-qa: A dataset\nfor understanding complex web videos via question answering. In The Thirty-Third AAAI\nConference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications\nof Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February\n1, 2019, pages 9127\u20139134, 2019.\n[60] R. Zellers, Y. Bisk, A. Farhadi, and Y. Choi. From recognition to cognition: Visual commonsense\nreasoning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA, June 16-20, 2019, pages 6720\u20136731, 2019.\n17\n[61] J. Zhang, R. Gan, J. Wang, Y. Zhang, L. Zhang, P. Yang, X. Gao, Z. Wu, X. Dong, J. He, J. Zhuo,\nQ. Yang, Y. Huang, X. Li, Y. Wu, J. Lu, X. Zhu, W. Chen, T. Han, K. Pan, R. Wang, H. Wang,\nX. Wu, Z. Zeng, and C. Chen. Fengshenbang 1.0: Being the foundation of chinese cognitive\nintelligence. ArXiv preprint, abs/2209.02970, 2022.\n[62] Y. Zheng, G. Chen, X. Liu, and J. Sun. MMChat: Multi-modal chat dataset on social media. In\nProceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5778\u2013\n5786, 2022.\n[63] D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. ArXiv preprint, abs/2304.10592, 2023.\n18\n"
  },
  {
    "title": "LLMZip: Lossless Text Compression using Large Language Models",
    "link": "https://arxiv.org/pdf/2306.04050.pdf",
    "upvote": "4",
    "text": "arXiv:2306.04050v2  [cs.IT]  26 Jun 2023\n1\nLLMZip: Lossless Text Compression using Large\nLanguage Models\nChandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil,\nJean-Francois Chamberland, Srinivas Shakkottai\nDepartment of Electrical and Computer Engineering\nTexas A&M University\nEmail:{vcskaushik9,krn,dileep.kalathil,chmbrlnd,sshakkot}@tamu.edu\nAbstract\nWe provide new estimates of an asymptotic upper bound on the entropy of English using the large language model LLaMA-7B\nas a predictor for the next token given a window of past tokens. This estimate is signi\ufb01cantly smaller than currently available\nestimates in [1], [2]. A natural byproduct is an algorithm for lossless compression of English text which combines the prediction\nfrom the large language model with a lossless compression scheme. Preliminary results from limited experiments suggest that our\nscheme outperforms state-of-the-art text compression schemes such as BSC, ZPAQ, and paq8h.\nI. INTRODUCTION\nThere are close connections between learning, prediction, and compression. The success of ChatGPT has captured the\nfascination of general public and brought the connection between learning and prediction to the fore. The main advance\nbrought about by large language models such as LLaMA and GPT-4 is that they excel at predicting the next word (token) in\na paragraph based on knowing the past several words (tokens).\nThe connection between prediction and compression was explored as early as 1951 by Shannon in order to estimate the\nentropy of the English language [3]. The idea that a good predictor for the ith value in a time series based on the past\nvalues can be effectively converted to a good compression algorithm has played a prominent role in information theory. Many\nalgorithms for speech, image, and video compression exploit this idea either explicitly or implicitly. Within the context of\nlossless compression of English text, the idea of combining a language model with arithmetic coding has emerged as a very\neffective paradigm [4]. The performance of such a compression scheme depends substantially on the ef\ufb01cacy of the predictor\nand every time there is a major advance in the prediction capability, it behooves us to study its effect on the compression\nperformance. Indeed, in 2018, the authors of [5] used recurrent neural networks (RNN) as the predictor and reported improved\nresults for certain kinds of sources. Their scheme still did not outperform state-of-the-art algorithms such as BSC and ZPAQ\nfor text compression.\nIt is therefore natural at this time to study whether we can obtain better compression results and sharper estimates of the\nentropy of the English language using recent large language models such as LLaMA-7B [6]. This is the main goal of this\npaper. We show that when the LLaMA-7B large language model is used as the predictor, the asymptotic upper bound on the\nentropy is 0.709 bits/character when estimated using a 1MB section of the text8 dataset. This is smaller than earlier estimates\nprovided in [1] and [2, Table 4]. The estimate of the upper bound increases to 0.85 bits/character for a 100 KB section of\nthe text from [7], which is still lower than the estimates in [2]. When LLaMA-7B is combined with an Arithmetic coder for\ncompression, we obtain a compression ratio of 0.7101 bits/character on a 1MB section of the text8 dataset and a compression\nratio of 0.8426 bits/character on a 100KB section of a text from [7], which are signi\ufb01cantly better than the compression ratio\nobtained using BSC, ZPAQ and pq8h on the full 100MB of the text8 dataset.\nII. INTUITIVE EXPLANATION OF THE MAIN IDEA\nWe will use the following example to describe the main idea, which is nearly identical to that proposed by Shannon in [3]\nfor estimating the entropy of English. The main difference is in the use of tokens which represent groups of letters of variable\nlength and in the use of a large language model instead of a human to predict the next token. Consider a part of the sentence\nthat reads as\nMy first attempt at writing a book\nOur goal is to convert this sentence into a sequence of bits with the least possible length such that the original sequence can\nbe reconstructed from the sequence of bits. This sentence can \ufb01rst be split into a sequence of words (tokens)\n\u2032My\u2032, \u2032first\u2032, \u2032attempt\u2032, \u2032at\u2032, \u2032writing\u2032, \u2032a\u2032, \u2032book\u2032\nA language model with memory M (for example, say M = 4) predicts the next word in the sentence based on observing the\npast M words. Speci\ufb01cally, it produces a rank-ordered list of choices for the next word and their probabilities. As shown in\n2\nFigure 1, at epoch 5, the model accepts the \ufb01rst 4 words as input and predicts that the next word in the sentence could be\nwords such as \u2032reading\u2032,\u2032 writing\u2032,\u2032 driving\u2032,\u2032 cooking\u2032 etc. The main idea is to compute the rank of the actual word in\nour sentence (\u2032writing\u2032) in this list and call it R5. We will assume that the ranks start at 0 i.e., the most likely word has rank\n0, the second most likely word has rank 1, and so on. In this example, the rank for \u2032writing\u2032 is R5 = 1.\nTokenizer\n\u2032My\u2032, \u2032first\u2032, \u2032attempt\u2032, \u2032at\u2032\nTokens\nMy first attempt at writing a book\nLLM\nRank\ncomputation\nq5 = Pr(x5|x4\n1)\nNext word\nProbability\nreading\n0.3\nwriting\n0.2\ncycling\n0.1\ndriving\n0.05\n...\n...\nR5 = 1\nFig. 1. Schematic showing the prediction at epoch 5 for a language model with memory 4.\nThen, we move forward by one word in the sentence, and at epoch 6, we try to predict the 6th word based on words 2\nthrough 5 as shown in Figure 2. In this example, given words 2 through 5, the most likely 6th word would indeed be the same\nword in the sentence that we wish to encode, \u2032a\u2032, and hence, the rank R6 would be 0.\nTokenizer\n\u2032first\u2032, \u2032attempt\u2032, \u2032at\u2032, \u2032writing\u2032\nTokens\nMy first attempt at writing a book\nLLM\nRank\ncomputation\nq6 = Pr(x6|x5\n2)\nNext word\nProbability\na\n0.7\nan\n0.2\nthe\n0.05\n...\n...\nR6 = 0\nFig. 2. Schematic showing the prediction at epoch 6 for a language model with memory 4.\nIf the language model is good, the word that we wish to encode would often appear at the top of the list and hence, the\nrank would be 0. Thus, if we look at the sequence of ranks, it is likely to have many 0s with decreasing probabilities for the\nrank being 1, 2, . . .. In this example, it is foreseeable that the ranks will be\n1, 0, 0, . . .\nA sequence with many \u20180\u2019s is typically compressible since it has structured patterns. Thus, the key idea is to compress the\nranks using a standard lossless compression algorithm such as zip, arithmetic coding, or Huffman coding which converts the\nranks to bits. This is shown in Fig. 3.\nCompression (zip)\nSequence of ranks\nr1, r2, . . . , rNT\nNb bits\nFig. 3. Schematic showing the compression of the sequence of ranks to a bit sequence.\nWhen we wish to reconstruct the sequence, we \ufb01rst decompress and unzip the bits to get the ranks, use the same language\nmodel one epoch at a time to produce a rank ordered list of possibilities for the next word, and pick the word in the list at\nrank Ri during the ith epoch. We use that as input for determining the next word and so on. Note that this requires that the\nsame LLM is used at both the encoder and the decoder.\nThe idea of encoding ranks was discussed to build intuition, but better compression can be achieved by directly using the\nprobabilities produced by the LLM along with arithmetic coding as discussed in Section III-B3.\n3\nIII. COMPRESSION USING LLMS\nLet s denote a sentence from the English language composed of Nc letters, where each letter is assumed to be from the\nalphabet S. We assume that we have a dictionary X = [1, D] of D tokens. We \ufb01rst parse s into a sequence of NT tokens\ndenoted by x = x1, x2, . . . , xi\u22121, xi, xi+1, . . . xNT , where xi \u2208 X. There is a one-to-one mapping between s and x and hence,\ncompressing s is the same as compressing x. xi\u2019s can be thought of as realizations of the random variable denoted by the\nupper case letter Xi.\nA language model with memory M is a predictor that operates as follows. At epoch i, it accepts tokens xi\u2212M, xi\u2212M+1, . . . , xi\u22121\nand produces a probability mass function for the next token in the sequence conditioned on the past M tokens given by\nqi(xi) := Pr(Xi = xi|xi\u22121, xi\u22122, . . . , xi\u2212M), \u2200xi \u2208 X. The PMF vector qi := [qi(1), qi(2), . . . , qi(D)]T is sorted in\ndescending order and let the sorted PMF vector be denoted by \u02dcqi. Let \u03b3i : X \u2192 X be a permutation on the integers\nfrom 1 to D such that\n\u02dcqi(\u03b3i(j)) = qi(j), \u2200j \u2208 X.\nThat is, \u03b3i(j) is the rank of the token j at epoch i. We de\ufb01ne the rank of the input sequence at epoch i as the rank of the\ntoken xi at epoch i, ri := \u03b3i(xi). The sequence {ri}NT\ni=1 is compressed by a lossless compression algorithm (such as zlib) to\nproduce Nb bits which are the \ufb01nal bit representation of the source. A schematic of this scheme is shown in Fig. 4. In general,\nthe lossless compression algorithm may use the sequence of PMF vectors qi\u2019s in addition to the sequence of ranks.\nThe main metric of interest is the compression ratio \u03c1 de\ufb01ned as\n\u03c1 := Nb\nNc\nbits/character.\nTokenizer\nTokens . . . , xi\u2212M, . . . , xi\u22121\nInput sentence s with Nc characters\nLLM\nRank\ncomputation\nLosseless\nCompression\nqi = Pr(Xi = j|xi\u2212M\ni\u22121 )\nri\nNb bits\nFig. 4. Schematic showing the prediction at epoch i.\nA. Entropy bounds\nLet S \u2208 S\u221e be a random process that represents language input. The nth character in the sequence is denoted by Sn,\nwhereas the string of characters from the beginning to the nth character is expressed as Sn. The tokenizer parses the input\nstring and maps it to a sequence of tokens X = X1, X2, . . . using a variable-length mapping. In this sequence, Xi is the ith\ntoken. The number of characters employed to generate Xi depends on the realization of the random process and, as such,\nwe introduce random variable Bi to identify the number of characters contained in the ith token. Motivated by practical\nconsiderations, we only admit tokenizers for which Bi \u2265 1 and Bi is uniformly bounded, with Bi < B < \u221e; these are\ncharacteristics of commonly used tokenizers. An immediate consequence of this framework is that, as the number of tokens\ngrows unbounded NT \u2192 \u221e, the number of characters must also approach in\ufb01nity Nc \u2192 \u221e. Formally, consider the tokenizer\nfunction T : SN \u2192 X N operating on in\ufb01nite symbol sequences; that is, T (s) = x where s is an in\ufb01nite sequence in S\u221e. For\nnatural number, i \u2208 N, de\ufb01ne mi : SN \u2192 N to be the (time) index during which the tokenizer working sequentially on an\ninput sequence s outputs its ith token. Speci\ufb01cally, suppose s is given, then\nmi(s) = min\nn {length (T (sn)) \u2265 i} .\n(1)\nWe note that, by construction, limn\u2192\u221e length (T (sn)) = \u221e and, as such, mi(\u00b7) is well-de\ufb01ned. It may be pertinent to stress\nthat the tokenizer function applied to truncated sequences is not necessarily injective because multiple \ufb01nite input series can map\nto the same output. This phenomenon is a consequence of the fact that, at any point in time, a tokenizer working sequentially\nmay be waiting for an additional symbol before it can unambiguously select the next output token, i.e., there may be instances\nwhere T (sn) = T (sn+1). However, if we restrict the input series to input indices when a new token is produced, then the\nrestricted mapping becomes injective. That is, suppose T (s) = x, then the only (\ufb01nite) series of input symbols in the restricted\nset for which T (yn) = xi is smi(s). Given a \ufb01xed sequence s, we can express the number of characters contained in a token\nas\nbi = mi(s) \u2212 mi\u22121(s)\n4\nwith initial condition m\u22121 = 0. Consequently, the number of characters embedded in the \ufb01rst NT tokens for a random input\nbecomes Nc = PNT\ni=1 Bi.\nHaving established these properties, we turn to the relation between H(S) and H(X). We make the assumption that {Sk}\u221e\nk=1,\n{Bi}\u221e\ni=1, and {Xi}\u221e\ni=1 are stationary and ergodic processes. We know from the Shannon-McMillan-Breiman Theorem [8] that\n\u2212 1\nn log2 pSn(S1, . . . , Sn) = \u2212 1\nn log2 pSn(Sn) \u2192 H(S)\nalmost surely.\n(2)\nLet \u2126S be the collection of \u03c9 \u2208 \u2126 for which this limit holds. In an analogous manner, the Shannon-McMillan-Breiman theorem\nimplies\n\u22121\ni log2 pXi(X1, . . . , Xi) = \u22121\ni log2 pXi(Xi) \u2192 H(X)\nalmost surely.\n(3)\nDe\ufb01ne \u2126X as the collection of \u03c9 \u2208 \u2126 for which this limit holds. Finally, by construction, we have\nlim\ni\u2192\u221e\nmi(S)\ni\n= E [B]\nalmost surely.\n(4)\nSet \u2126B to be the set of \u03c9 \u2208 \u2126 for which this limit holds. For any \u03c9 \u2208 \u2126S \u2229 \u2126X \u2229 \u2126B, we deduce that\nH(S) = lim\nk\u2192\u221e \u2212 1\nk log2 pSk(Sk(\u03c9))\n= lim\ni\u2192\u221e \u2212 1\nli\nlog2 pSli(Sli(\u03c9))\n= lim\ni\u2192\u221e \u2212 1\nli\nlog2 Pr (Xi = T (Sli(\u03c9)))\n= \u2212\n1\nE[B] lim\ni\u2192\u221e\n1\ni log2 Pr (Xi = xi) = H(X)\nE[B] .\nThe \ufb01rst equality follows from (2). The second equality is a consequence of the fact that {li = mi(S(\u03c9))|i \u2208 N} is an in\ufb01nite\nsubset of the natural numbers. Since a subsequence of a convergent sequence must converge to the same limit, we immediately\ngather that this alternate form approaches H(S). The third equality is a consequence of the equivalence between the following\ntwo events,\n{\u03c9 \u2208 \u2126|Xi(\u03c9) = xi} = {\u03c9 \u2208 \u2126|T (Smi(S(\u03c9))) = xi}.\nThis is characteristic of the tokenization process, and it is a consequence of the correspondence described above. The last step\nholds because we are considering an \u03c9 \u2208 \u2126B. The sets \u2126S, \u2126X, and \u2126B each have probability one; this implies that their\nintersection also has probability one, Thus, we must conclude that\nH(S) = H(X)\nE[B]\nalmost surely.\nAs a corollary to this result, any upper bound on H(X) produces an upper bound on H(S). This is the property we wish to\nexploit.\nThen, from the results of [1], we can see that\nPr\n(\nH(X) \u2264\nlim\nNT \u2192\u221e \u2212 1\nNT\nNT\nX\ni=1\nlog2 qi(Xi)\n)\n= 1,\n(5)\nwhere qi(\u00b7) is the output PMF from the language model. Therefore, an asymptotic upper bound on the entropy rate H(S) is\ngiven by\nH(S) \u2264\nlimNT \u2192\u221e \u2212 1\nNT\nPNT\ni=1 log2 qi(Xi)\nE[B]\n.\n(6)\nWe refer to the expression in the right hand side of (6) as the asymptotic upper bound on H(S) and denote it by Hub. The\nnumerator in (6) represents the average number of bits required to represent the tokens XNT and the denominator in (6) is\nthe average number of charcaters per token. Hence, the unit for H(S) is bits/character. In [1], Cover and King provide 1.3\nbits/character as an estimate of the asymptotic upper bound on H(S). They also provide an extensive list of references and\ndiscussion of the literature on estimating the entropy of English prior to 1976. Very recently, in [2, Table 4], the performance\nof several language models have evaluated on the text8 dataset using a metric called bits per character (bpc). We believe bpc\nis the same as the asymptotic upper bound in this paper.\nB. Encoding schemes\nWe consider three schemes for the lossless compression block in Fig. 3.\n5\n1) Compressing the ranks using zlib: The \ufb01rst scheme uses the zlib compression algorithm to encode the sequence of ranks.\nWe refer to this scheme as LLaMA+zlib and denote the compression ratio of this scheme by \u03c1LLaMA+zlib.\n2) Token-by-Token Compression: The second scheme uses a token-by-token lossless compression scheme which uses a\ntime-varying codebook to encode the token xi at epoch i by using a pre\ufb01x-free code assuming qi to be the true distribution of\nthe tokens. A natural choice for a pre\ufb01x-free code is a Huffman code. Instead, for simplicity, we use a pre\ufb01x-free code where\nthe codeword for the token xi is of length li = \u2308log2\n1\nqi(xi)\u2309. A pre\ufb01x-free code with this length for xi is guaranteed to exist\nsince this choice of lengths satis\ufb01es the Kraft inequality [8]. The compression ratio for this scheme, denoted by \u03c1LLaMA+TbyT,\nis given by\n\u03c1LLaMA+TbyT =\nNT\nX\ni=1\n\u0018\nlog2\n1\nqi(xi)\n\u0019\nPNT\ni=1 bi\n.\n3) Arithmetic Coding: The above two schemes are intuitive but their performance can be improved. A very effective way to\ncombine the output of the LLM with a lossless compression scheme is by using arithmetic coding [4], [9]. Arithmetic coding\nis well suited to accept time-varying probabilities and we use qi(xi) as the probability of token xi at time in the arithmetic\ncoding scheme. We refer to the compression ratio of this scheme as \u03c1LLM+AC. It is known that arithmetic coding is nearly\noptimal as a compression scheme [10, Page 115]. Hence, the compression ratio for this scheme is expected to be\n\u03c1LLM+AC \u2248\nNT\nX\ni=1\nlog2\n1\nqi(xi)\nPNT\ni=1 bi\n.\n(7)\nClearly, \u03c1LLaMA+zlib, \u03c1LLaMA+TbyT, and \u03c1LLM+AC also provide upper bounds on H(S). Hub, \u03c1LLaMA+zlib, \u03c1LLaMA+TbyT, and\n\u03c1LLM+AC are estimated using a \ufb01nite number of tokens and the statistical properties of such an estimate should be kept in mind\nwhen interpreting the results, especially since the tokens are from a very large alphabet and language model has large memory.\nIV. RESULTS\nWe used LLaMA-7B [6] as the large language model and SentencePiece tokenizer [11]. The tokenizer produces a dictionary\nof size 32000. Since the language model is trained on this tokenizer, it is imperative that this tokenizer be used in conjunction\nwith the LLM. It should be noted that the tokenizer and the model are trained on a large corpus of text which includes uppercase\nletters, special characters etc. This is in contrast to many studies on estimating the entropy of English, where the input alphabet\nis restricted to lowercase letters such as in [1], [3], [5]. This makes it dif\ufb01cult to perform an entirely fair comparison between\nthese models. By using a pretrained LLM on an input consisting only of lowercase letters, we may be unfair to the LLM.\nNevertheless, we used the text8 dataset available from http://mattmahoney.net/dc/text8.zip to benchmark the performance of\nLLaMA-7B with compression against other state of the art results for text compression. In [5], it is mentioned that the ZPAQ\nalgorithm obtains the best compression ratio for the text8 dataset with a compression ratio of 1.4 bits/character. In [12], the\npaq8h algorithm is shown to provide a compression ratio of 1.2 bits/character. To the best of our knowledge, this appears to\nbe best performance reported. Therefore, we used these two algorithms as baselines. We did not independently run the ZPAQ\nor paq8h algorithms and we are quoting results from the existing literature.\nThe performance of LLaMA-7B is shown in Table I for 10 different batches each with 100,000 tokens. The average\nperformance over these 1M tokens is also shown in the last row in the Table. It can be seen that using LLaMA-7B with\nArithmetic Coding compression results in a compression ratio of 0.7101 bits/character. This is substantially better than the\nstate-of-the-art results mentioned in [5] or [12] and is very close to our computed upper bound. The performance with the\nLLaMA+zlib algorithm and LLaMA+TbyT compression are also better than that of the known state-of-the-art results. Table I\nalso shows the upper bound in (6). It should be noted that the upper bound on the entropy is lower than that computed by\nShannon in [3], Cover and King in [1] and more recent estimates based on neural networks in [2].\nThe dependence of the compression performance on the memory of the LLM (M) is shown in Table II. As expected, the\ncompression performance improves with increasing M. We also observed that the inference time scaled approximately linearly\nwith the input memory length, i.e., batches with a memory of 511 tokens ran about 16 times slower than batches with a\nmemory of 31 tokens.\nIt is well known that the estimate of compression ratio can show substantial variance depending on the input text and\nhence, the results should be interpreted with caution. The empirical mean and standard deviation of the entropy bounds and\ncompression ratios computed using 10 batches of 100,000 tokens are shown in Table III. We were also not able to run LLaMA-\n7B on the entire 100MB of the text8 dataset. So, the comparison of LLaMA-7B with that of the state-of-the-art corresponds\nto estimates obtained from different input sizes.\nIt appears that the LLaMA-7B model was trained on a corpus that included articles from Wikipedia. Since the text8 dataset\nis derived from Wikipedia, it is likely that our results for the text8 dataset are optimistic.\n6\nTherefore, we also tested the performance of LLaMA-7B on a recently released (May 25, 2023) book [7] under Project\nGutenberg. We extracted text that corresponds to 100,000 tokens. We applied the same text pre-processing as used in the text8\ndataset to clean the text from the book. The resulting text data contained only lowercase letters and space as in the text8 dataset.\nTable IV shows the compression performance of the LLM on the book. It can be seen that the compression ratios and the\nentropy upper bound are slightly higher compared to the performance on the text8 dataset; nevertheless, the asymptotic upper\nbound on the entropy is lower than that of currently known models given in [2, Table 4]). Similarly, the compression ratio of\nLLaMA-7B-based compressors are better than those of known state-of-the-art results for the text8 dataset. The compression\nratio for LLaMA with arithmetic coding is only 0.8426 bits/character and is very close to the estimated upper bound on H(S).\nTo provide some insight into the comparative performance of LLaMA based compressors vis-a-vis standard text compressors,\nwe also ran the zlib algorithm directly on the input text. The resulting compression ratio was 2.8 bits/character (shown in the\nlast column). It is clear that the performance of LLaMA based compressors is substantially better than this. The zlib algorithm\nmay not be optimized for compressing small text samples and hence, the compression ratio for the zlib algorithm and the\nLLaMA+zlib will likely improve on longer texts.\nV. ACKNOWLEDGEMENT\nWe would like to thank Andreas Kirsch for an email discussion about arithmetic coding that motivated us to add our results\non arithmetic coding in a timely manner.\nTABLE I\nRESULTS FOR 1MB OF TEXT FROM TEXT8 DATASET\nBatch\nNc\nNT\nHub\n\u03c1LLaMA+zlib\n\u03c1LLaMA+TbyT\n\u03c1LLaMA+AC\nZPAQ\npq8h\nNo.\n(bpc)\n\ufb01le size (bits)\n(bpc)\n(bpc)\n(bpc)\n(bpc)\n1\n466, 650\n100, 000\n0.6882\n1.0513\n0.8215\n0.689\n2\n461, 477\n100, 000\n0.6893\n1.0558\n0.8242\n0.6901\n3\n454, 599\n100, 000\n0.699\n1.0681\n0.8357\n0.6999\n4\n462, 755\n100, 000\n0.6748\n1.0346\n0.8093\n0.6757\n5\n453, 847\n100, 000\n0.7481\n1.1265\n0.8831\n0.749\n6\n458, 252\n100, 000\n0.7218\n1.0957\n0.8567\n0.7227\n7\n451, 036\n100, 000\n0.6959\n1.0729\n0.8353\n0.6968\n8\n447, 953\n100, 000\n0.7092\n1.0896\n0.8489\n0.7101\n9\n462, 665\n100, 000\n0.7394\n1.1126\n0.8713\n0.7402\n10\n449, 621\n100, 000\n0.7269\n1.1046\n0.8643\n0.7277\nTotal\n9, 137, 710\n2, 000, 000\n0.7093\n1.0812\n0.845\n0.7101\n1.41\n1.22\nTABLE II\nCOMPRESSION PERFORMANCE OF THE LLM ON THE TEXT8 DATASET, AS A FUNCTION OF ITS MEMORY (M)\nM\nNc\nNt\nHub\n\u03c1LLaMA+zlib\n\u03c1LLaMA+TbyT\n\u03c1LLaMA+AC\n(bpc)\n\ufb01le size (bits)\n(bpc)\n(bpc)\n31\n4, 568, 855\n1, 000, 000\n0.9139\n1.3159\n1.0425\n0.9145\n127\n4, 568, 855\n1, 000, 000\n0.7511\n1.1303\n0.8847\n0.752\n255\n4, 568, 855\n1, 000, 000\n0.7242\n1.0985\n0.859\n0.725\n511\n4, 568, 855\n1, 000, 000\n0.7093\n1.0812\n0.845\n0.7101\n1This result is taken from [5] and it corresponds to the full 100MB dataset text8\n2This result is taken from [12] and it corresponds to the full 100MB dataset text8\n7\nTABLE III\nMEAN AND STANDARD DEVIATION OF THE ENTROPY BOUNDS MEASURED OVER 10 BATCHES OF 100,000 TOKENS\nM\nHub\n\u03c1LLaMA+zlib\n\u03c1LLaMA+TbyT\n\u03c1LLaMA+AC\n(bpc)\n(bpc)\n(bpc)\n(bpc)\n31\n0.9139 \u00b1 0.0263\n1.3159 \u00b1 0.0329\n1.0425 \u00b1 0.0262\n0.9145 \u00b1 0.0263\n127\n0.7511 \u00b1 0.0233\n1.1303 \u00b1 0.0292\n0.8847 \u00b1 0.0231\n0.752 \u00b1 0.0233\n255\n0.7242 \u00b1 0.0234\n1.0985 \u00b1 0.0289\n0.859 \u00b1 0.0232\n0.725 \u00b1 0.0234\n511\n0.7093 \u00b1 0.0228\n1.0812 \u00b1 0.028\n0.845 \u00b1 0.0226\n0.7101 \u00b1 0.0228\nTABLE IV\nCOMPRESSION PERFORMANCE OF THE LLM ON A RECENTLY PUBLISHED BOOK IN PROJECT GUTENBERG [7], AS A FUNCTION OF ITS MEMORY (M)\nM\nNc\nNt\nHub\n\u03c1LLaMA+zlib\n\u03c1LLaMA+TbyT\n\u03c1LLaMA+AC\nStandalone Zlib\n(bpc)\n(bpc)\n(bpc)\n(bpc)\n(bpc)\n31\n508, 463\n115, 000\n1.0919\n1.5316\n1.2152\n1.0924\n2.80\n127\n508, 463\n115, 000\n0.8973\n1.3128\n1.0235\n0.8982\n2.80\n255\n508, 463\n115, 000\n0.8618\n1.2684\n0.9899\n0.8627\n2.80\n511\n508, 463\n115, 000\n0.8417\n1.2465\n0.9711\n0.8426\n2.80\n8\nREFERENCES\n[1] Thomas Cover and Roger King, \u201cA convergent gambling estimate of the entropy of english,\u201d IEEE Transactions on Information Theory, vol. 24, no. 4,\npp. 413\u2013421, 1978.\n[2] Shahar Lutati, Itamar Zimerman, and Lior Wolf, \u201cFocus your attention (with adaptive IIR \ufb01lters),\u201d 2023.\n[3] Claude E Shannon, \u201cPrediction and entropy of printed english,\u201d Bell system technical journal, vol. 30, no. 1, pp. 50\u201364, 1951.\n[4] John Cleary and Ian Witten, \u201cData compression using adaptive coding and partial string matching,\u201d IEEE transactions on Communications, vol. 32, no.\n4, pp. 396\u2013402, 1984.\n[5] Mohit Goyal, Kedar Tatwawadi, Shubham Chandak, and Idoia Ochoa, \u201cDeepzip: Lossless data compression using recurrent neural networks,\u201d arXiv\npreprint arXiv:1811.08162, 2018.\n[6] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample, \u201cLlama: Open and ef\ufb01cient foundation language models,\u201d\n2023.\n[7] J.\nFrank\nDobie,\nLegends\nof\nTexas,\nUnited\nStates,\nTexas\nFolk-Lore\nSociety,\n1924;\nProject\nGutenberg,\nMay\n25,\n2023,\n2023,\nhttps://www.gutenberg.org/ebooks/70859.\n[8] Thomas M Cover and Joy A Thomas, Elements of Information Theory, Wiley, New York, 1999.\n[9] Timothy Bell, Ian H Witten, and John G Cleary, \u201cModeling for text compression,\u201d ACM Computing Surveys (CSUR), vol. 21, no. 4, pp. 557\u2013591, 1989.\n[10] David JC MacKay, Information theory, inference and learning algorithms, Cambridge university press, 2003.\n[11] Taku Kudo and John Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d\nCoRR, vol. abs/1808.06226, 2018.\n[12] \u201ctext8 results,\u201d http://mattmahoney.net/dc/textdata.html.\n"
  },
  {
    "title": "ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections",
    "link": "https://arxiv.org/pdf/2306.04619.pdf",
    "upvote": "4",
    "text": "ARTIC3D: Learning Robust Articulated 3D Shapes\nfrom Noisy Web Image Collections\nChun-Han Yao1*\nAmit Raj2\nWei-Chih Hung3\nYuanzhen Li2\nMichael Rubinstein2\nMing-Hsuan Yang124\nVarun Jampani2\n1UC Merced\n2Google Research\n3Waymo\n4Yonsei University\nAbstract\nEstimating 3D articulated shapes like animal bodies from monocular images is\ninherently challenging due to the ambiguities of camera viewpoint, pose, texture,\nlighting, etc. We propose ARTIC3D, a self-supervised framework to reconstruct\nper-instance 3D shapes from a sparse image collection in-the-wild. Specifically,\nARTIC3D is built upon a skeleton-based surface representation and is further\nguided by 2D diffusion priors from Stable Diffusion. First, we enhance the in-\nput images with occlusions/truncation via 2D diffusion to obtain cleaner mask\nestimates and semantic features. Second, we perform diffusion-guided 3D opti-\nmization to estimate shape and texture that are of high-fidelity and faithful to input\nimages. We also propose a novel technique to calculate more stable image-level\ngradients via diffusion models compared to existing alternatives. Finally, we pro-\nduce realistic animations by fine-tuning the rendered shape and texture under rigid\npart transformations. Extensive evaluations on multiple existing datasets as well\nas newly introduced noisy web image collections with occlusions and truncation\ndemonstrate that ARTIC3D outputs are more robust to noisy images, higher quality\nin terms of shape and texture details, and more realistic when animated. Project\npage: https://chhankyao.github.io/artic3d/\n1\nIntroduction\nArticulated 3D animal shapes are widely used in applications such as AR/VR, gaming, and content\ncreation. However, the articulated models are usually hard to obtain as manually creating them is\nlabor intensive and 3D scanning real animals in the lab settings is highly infeasible. In this work,\nwe aim to automatically estimate high-quality 3D articulated animal shapes directly from sparse and\nnoisy web image collections. This is a highly ill-posed problem due to the variations across images\nwith diverse backgrounds, lighting, camera viewpoints, animal poses, shapes, and textures, etc. In\naddition, we do not assume access to any 3D shape models or per-image annotations like keypoints\nand camera viewpoints in our in-the-wild setting.\nWhile several recent methods [39, 33, 38] can produce animatable 3D shapes using a skeleton-based\nneural surface or pre-defined mesh template, the success is largely dependent on large-scale image\ndatasets or manually-filtered clean images for training or optimization. Moreover, the output 3D\nshapes and textures are usually unrealistic when viewed from novel viewpoints or pose articulations.\nOn the other hand, recent success of generative diffusion models [25, 28, 27] shows that one can\ngenerate high-quality images for a given text prompt. Several recent works [21, 15, 18, 23] further\ndemonstrate the possibility to produce 3D objects/scenes simply using 2D diffusion as multi-view\nsupervision. In this work, we leverage the powerful 2D diffusion prior to learn 3D articulated shapes,\naiming to reconstruct and animate 3D animals from sparse noisy online images without any 2D\nor 3D annotations. Intuitively, one can improve the quality of 3D reconstructions by utilizing a\n*Work done as a student researcher at Google.\nPreprint. Under review.\narXiv:2306.04619v1  [cs.CV]  7 Jun 2023\nNoisy web \nimages\nArticulated shapes\n(input and novel views)\nTextured outputs\n(input and novel views)\nAnimated\nFine-tuned \nanimation\nFigure 1: Learning articulated 3D shapes from noisy web images. We propose ARTIC3D, a\ndiffusion-guided optimization framework to estimate the 3D shape and texture of articulated animal\nbodies from sparse and noisy image in-the-wild. Results show that ARTIC3D outputs are detailed,\nanimatable, and robust to occlusions or truncation.\ndiffusion prior similar to the score distillation sampling (SDS) loss proposed in DreamFusion [21].\nIn our experiments, nonetheless, we observe that naively applying the SDS loss on 3D surface\noptimization leads to unstable and inefficient training, producing undesirable artifacts like noisy\nsurfaces or ambiguous texture.\nIn this work, we present ARTIC3D (ARTiculated Image Collections in 3D), a diffusion-guided\noptimization framework to learn articulated 3D shapes from sparse noisy image collections. We\nuse the articulated part surface and skeleton from Hi-LASSIE [38], which allows explicit part\nmanipulation and animation. We propose a novel Decoder-based Accumulative Score Sampling\n(DASS) module that can effectively leverage 2D diffusion model priors from Stable Diffusion [27]\nfor 3D optimization. In contrast to existing works that back-propagate image gradients through the\nlatent encoder, we propose a decoder-based multi-step strategy in DASS, which we find to provide\nmore stable gradients for 3D optimization. To deal with noisy input images, we propose an input\npreprocessing scheme that use diffusion model to reason about occluded or truncated regions. In\naddition, we also propose techniques to create realistic animations from pose articulations.\nWe analyze ARTIC3D on the Pascal-Part [5] and LASSIE [39] datasets. To better demonstrate\nthe robustness to noisy images, we extend LASSIE animal dataset [39] with noisy web animal\nimages where animals are occluded and truncated. Both qualitative and quantitative results show that\nARTIC3D produces 3D shapes and textures that are detailed, faithful to input images, and robust to\npartial observations. Moreover, our 3D articulated representation enables explicit pose transfer and\nrealistic animation which are not feasible for prior diffusion-guided methods with neural volumetric\nrepresentations. Fig. 1 shows sample 3D reconstructions and applications from ARTIC3D. The main\ncontributions of this work are:\n\u2022 We propose a diffusion-guided optimization framework called ARTIC3D, where we reconstruct\n3D articulated shapes and textures from sparse noisy online images without using any pre-defined\nshape templates or per-image annotations like camera viewpoint or keypoints.\n\u2022 We design several strategies to efficiently incorporate 2D diffusion priors in 3D surface optimization,\nincluding input preprocessing, decoding diffused latents as image targets, pose exploration, and\nanimation fine-tuning.\n\u2022 We introduce E-LASSIE, an extended LASSIE dataset [39], by collecting and annotating noisy\nweb images with occlusions or truncation to evaluate model robustness. Both qualitative and\nquantitative results show that ARTIC3D outputs have higher-fidelity compared to prior arts in terms\nof 3D shape details, texture, and animation.\n2\nRelated Work\nAnimal shape and pose estimation. Earlier techniques on animal shape estimation used statistical\nbody models [46, 45] that are learned either using animal figurines or a large number of annotated\nanimal images. Some other works [35, 34, 36, 37], use video inputs to learn articulated shapes\nby exploiting dense correspondence information in video. However, these methods rely on optical\nflow correspondences between video frames, which are not available in our problem setting. Other\n2\ntechniques [12, 11] leverage a parametric mesh model and learn a linear blend skinning from images\nto obtain a posed mesh for different animal categories. Most related to our work are LASSIE [39]\nand Hi-LASSIE [38] which tackle the same problem setting of recovering 3D shape and texture\nfrom a sparse collection of animal images in the wild using either a manually annotated skeleton\ntemplate, or by discovering category specific template from image collections. MagicPony [33]\nlearns a hybrid 3D representation of the animal instance from category specific image collections.\nHowever, these approaches require carefully curated input data and fail to handle image collections\nwith partial occlusions, truncation or noise. By leveraging recent advances in diffusion models, we\nsupport reconstruction on a wider variety of input images.\n3D reconstruction from sparse images. Several recent works [30, 42, 41, 32, 24, 2, 43, 3] have used\nimplicit representations [19] to learn geometry and appearance from sparse image collections either\nby training in a category specific manner or assuming access to multi-view consistent sparse images\nduring inference. However, most of these approaches demonstrate compelling results only on rigid\nobjects. Zhang et al. [42] is another closely related work that finds a neural surface representation\nfrom sparse image collections but requires coarse camera initialization. By learning a part based\nmesh shape and texture, our framework naturally lends itself to modeling and animating articulated\ncategories such as animals in the wild without any additional requirements on camera parameters.\nDiffusion prior for 3D. Diffusion models [27, 28, 44] have recently gained popularity for generating\nhigh resolution images guided by various kinds of conditioning inputs. Diffusion models capture\nthe distribution of real data which can be used as score function to guide 3D generation with score-\ndistillation sampling (SDS) loss as first described in DreamFusion [21]. Several recent approaches [18,\n15, 17, 29, 26, 23] leverage the SDS loss to generate 3D representations from either text or single or\nsparse image collections. Drawing inspiration from these lines of work, we propose a novel Decoder-\nbased accumulative Score Sampling (DASS) that exploits the high quality images synthesized by the\ndecoder and demonstrate improved performance over naive SDS loss.\n3\nApproach\nGiven 10-30 noisy web images of an animal species, ARTIC3D first preprocesses the images via\n2D diffusion to obtain cleaner silhouette estimates, semantic features, and 3D skeleton initialization.\nWe then jointly optimizes the camera viewpoint, pose articulation, part shapes and texture for each\ninstance. Finally, we animate the 3D shapes with rigid bone transformations followed by diffusion-\nguided fine-tuning. Before introducing our diffusion-based strategies to improve the quality of 3D\noutputs, we briefly review the skeleton-based surface representation similar to [39, 38], as well as\nStable Diffusion [27] that we use as diffusion prior.\n3.1\nPreliminaries\nWhile most 3D generation methods optimizes a volumetric neural field to represent 3D rigid ob-\njects/scenes, we aim to produce 3D shapes that are articulated and animatable. To enable explicit\npart manipulation and realistic animation, we adopt a skeleton-based surface representation as in\nLASSIE [39] and Hi-LASSIE [38]. Unlike [39, 38] which directly sample surface texture from\nimages, we optimize per-part texture images to obtain realistic instance textures from novel views.\n3D Skeleton. Given a user-specified reference image in the collection, Hi-LASSIE [38] automatically\ndiscovers a 3D skeleton based on the geometric and semantic cues from DINO-ViT [4] feature\nclusters. The skeleton initializes a set of 3D joints and primitive part shapes, providing a good\nconstraint of part transformation and connectivity. In our framework, we obtain cleaner feature\nclusters by diffusing input images, then applying Hi-LASSIE as an off-the-shelf skeleton discovery\nmethod. For a fair comparison with existing works, we use the same reference image for skeleton\ndiscovery as in [38] in our experiments. Please refer to [38] for further details on skeleton discovery.\nNeural part surfaces. Following [38], using the discovered 3D skeleton, we reconstruct a 3D part\ncorresponding to each skeleton bone via a deformable neural surface [42]. The neural surfaces are\nparameterized by multi-layer perceptron networks (MLPs), mapping 3D surface points on a unit\nsphere to their xyz deformation. Given m uniformly sampled 3D points X \u2208 R3\u00d7m on a spherical\nsurface, we can deform the 3D shape of the i-th part through the part MLP as X 7\u2192 Fi(X). Then,\nthe part surfaces are rigidly transformed by the scaling si \u2208 R, rotation Ri \u2208 R3\u00d73, and translation\nti \u2208 R3 of each skeleton part i. The transformed part surface points Vi in the global coordinate can\nbe written as: Vi = siRiFi(X) + ti . Please refer to [38] for further details.\n3\nSurface points\n(u,v)\nPart MLPs\nPart surfaces\n\u2026\n(x, y, z)\nRendered image\nTexture maps\n(r, g, b)\nDASS loss\nInput preprocessing \n(Sec. 3.3)\n3D articulated shape and texture optimization (Sec. 3.4)\nDASS\nNoisy web images & \nnoisy masks\nEnhanced images & \nfeature clusters\nScore \nDistillation\n\u201cA photo of * \u201d\nEncoder\nDecoder\nImage gradients\nStop gradient\n\u2207!\u2112\"#\"\nImage target x\u2019\nGradient flow\nData flow\nImage x\nLatents z\nDASS: Decoder-based Accumulative Score Sampling (Sec. 3.2)\nAnimation (Sec. 3.5)\nT-DASS\nRigid part motion\nPer-frame targets\nTemporal \nconsistency\nHi-LASSIE losses\nText recon. loss\nFigure 2: ARTIC3D overview. Given sparse web images of an animal species, ARTIC3D estimates\nthe camera viewpoint, articulated pose, 3D part shapes, and surface texture for each instance. We\npropose a novel DASS module to efficiently compute image-level gradients from stable diffusion,\nwhich are applied in 1) input preprocessing, 2) shape and texture optimization, and 3) animation.\nStable Diffusion architecture. Stable Diffusion (SD) [27] is a state-of-the-art text-to-image gen-\nerative model that can synthesize high-quality images given a text prompt. SD mainly consists of\n3 components: An image encoder E that encodes a given image x into a latent code z; a decoder\nnetwork D that converts the latent code back to image pixels; and a U-Net denoiser \u03f5\u03d5 that can\niteratively denoise a noisy latent code. We use SD as a diffusion prior in our framework.\n3.2\nDecoder-based Accumulative Score Sampling (DASS)\nTo leverage the 2D diffusion prior for 3D shape learning, DreamFusion [21] proposes a score\ndistillation sampling (SDS) loss to distill the images rendered from random views and propagate the\nimage-level gradients to Neural Radiance Field (NeRF) parameters. To reduce the computational\ncost and improve training stability, recent works like Latent-NeRF [18] and Magic3D [15] perform\ndistillation on the low-resolution latent codes in SD and back-propagate the gradients through the SD\nimage encoder E. Formally, let x be a rendered image from 3D model and z denote its latent codes\nfrom the SD image encoder E. At each score distillation iteration, the latent codes z are noised to\na random time step t, denoted as zt, and denoised by the U-Net denoiser \u03f5\u03d5 of the diffusion model.\nThe image-level SDS gradients can then be expressed as: \u2207xLSDS = wt(\u03f5\u03d5(zt; y, t) \u2212 \u03f5) \u2202z\n\u2202x, where\ny denotes the guiding text embedding, \u03f5 is the random noise added to the latent codes, and wt is a\nconstant multiplier which depends on diffusion timestep t. The denoiser \u03f5\u03d5 uses a guidance scale wg\nto balance the text guidance and a classifier-free guidance [8] of an unconditional model.\nAlthough this common SDS loss is effective in generating NeRFs from text, we observe that naively\napplying it in our framework leads to unstable and inefficient training. As shown in Fig. 3 (b), the SDS\ngradients back-propagated through the encoder are often quite noisy, causing undesirable artifacts on\n3D shapes and texture. Moreover, it requires the extra computation and memory usage for gradient\nback propagation, limiting the training batch size and thus decreasing stability.\nTo mitigate these issues, we propose a novel Decoder-based Accumulative Score Sampling (DASS)\nmodule, an alternative to calculate pixel gradients that are cleaner and more efficient. Fig. 2 illustrates\nthe proposed DASS module. At a high level, given an input image x, we obtain a denoised image\nx\u2032 from the decoder as a reconstruction target, based on our observation that decoded outputs are\ngenerally less noisy. As shown in Fig. 2, we pass a rendered image through the encoder E to obtain\nlow-resolution latent codes, update the latents for n steps via score distillation, then decode the\nupdated latents with the decoder D as an image target. Formally, instead of explicitly calculating the\npartial derivative \u2202z/\u2202x, we use x \u2212 D(z \u2212 \u2207z) as a proxy to \u2207x, where \u2207z is the accumulated\nlatent gradients over n steps. This makes a linear assumption on D around latents z, which we\nempirically find effective to approximate the pixel gradients. The target image x\u2032 = D(z \u2212 \u2207z)\ncan be directly used as an updated input (Section 3.3) or to compute a pixel-level DASS loss\n4\nn = 1\n(DSS)\n(c) Accumulation steps n\nn = 4\n(DASS)\nn = 16\n(DASS)\nt = 0.2\n(d) Diffusion timestep t\nt = 0.5\nt = 0.8\nwg = 10\n(e) Guidance weight wg\nwg = 20\nwg = 40\nImage\nInput\n(clean)\nInput\n(noised)\nDiffused\nDiffused\nDecoder\n(DASS)\nEncoder\n(SDS)\n(a) Clean v.s. noised background\n(b) Encoder v.s. Decoder\nFigure 3: Ablative visualizations of the DASS method. From the example input image (top left),\nwe show the updated image after one optimization iteration using various ways to obtain image-level\ngradients or parameter settings: (a) shows that noised background in the input image encourages\nDASS to hallucinate the missing parts; (b) compares the standard SDS (back-propagate gradients\nthrough encoder) and our DASS (decoder-based) losses; (c) justifies our accumulating latent gradient\napproach as it leads to cleaner decoded output; (d) indicates that small timestep mostly modifies the\ntexture, whereas large timestep changes the geometry more (sometimes removes or creates body\nparts); (e) demonstrates high-contrast colors and slightly disproportioned body with higher guidance\nweight (diffusion prior is biased towards larger heads and frontal views). Note that (b) uses the clean\ninput in (a) for better visualization, whereas (c),(d),(e) are obtained from the noised input.\nLdass = \u2225(x \u2212 D(z \u2212 \u2207z))\u22252 in 3D optimization (Section 3.4). Since the DASS module only\ninvolves one forward pass of the encoder and decoder, it costs roughly half the memory consumption\nduring training compared to the original SDS loss. The visualizations in Fig. 3 demonstrate that\nDASS produces cleaner images than the original SDS loss in one training step (b), and that the\naccumulated gradients can effectively reduce noise and fill in the missing parts (c). Moreover, we\nshow that adding random noise to the background pixels can facilitate the shape completion by DASS\n(a). We also perform ablative analyses on other diffusion parameters like noise timestep (d) and\nguidance weight (e) in Fig. 3. In general, ARTIC3D favors moderate accumulation steps n \u2208 (3, 10)\nand lower timestep t \u2208 (0.2, 0.5) since higher variance can lead to 3D results that are not faithful\nto the input images. Also, we use a lower guidance weight wg \u2208 (10, 30) so that our results do not\nsuffer from over saturation effects common in prior works due to high guidance scale in SDS loss.\n3.3\nInput preprocessing for noisy images\nAnimal bodies in real-world images often have ambiguous appearance caused by noisy texture,\ndim lighting, occlusions, or truncation, as shown in Fig. 4. To better deal with noisy or partial\nobservations, we propose a novel method to enhance the image quality and complete the missing\nparts. Given a sparse image collection {Ij \u2208 RH\u00d7W \u00d73} (j \u2208 {1, ..., N} and N is typically between\n10-30) of an animal species, we aim to obtain accurate silhouettes estimates { \u02c6\nMj \u2208 RH\u00d7W } and\nclean semantic features {Kj \u2208 Rh\u00d7w\u00d7d} for each instance. As shown in Fig. 2, we roughly estimate\nthe foreground masks via clustering salient features extracted by a trained DINO-ViT [4] network.\nThen, we apply DASS to diffuse the background-masked images, resulting in animal bodies with\ncleaner texture and complete shapes. Formally, we obtain an updated image I\u2032 by D(z \u2212 \u2207z), where\nz = E(I). Here, DASS serves as an image denoising and inpainting module, which can effectively\ngenerate a high-quality version of a noisy input via n latent updates and a single forward pass of D.\nFollowing the noise-and-denoise nature of diffusion models, we show in Fig. 3 (a) that manually\nadding Gaussian noise to the background pixels in an input image encourages DASS to hallucinate the\noccluded parts while mostly preserving the visible regions. Finally, we re-apply DINO-ViT feature\nextraction and clustering [1] on the diffused images to obtain cleaner and more complete masks as\nwell as semantic features. Fig. 2 (left) shows sample noisy input images and the corresponding output\nenhanced images and feature clusters. Note that Farm3D [9] uses SD [27] to generate animal images\nfrom text for 3D training, which, however, often contain irregular shapes (e.g., horses with 5 legs). On\nthe contrary, our preprocessed images are more suitable for the sparse-image optimization framework\nsince our goal is to reconstruct 3D shape and texture that are realistic and faithful to the input images.\n5\n3.4\nDiffusion-guided optimization of shape and texture\nGiven the preprocessed images, silhouette estimates, and semantic features, we jointly optimize\nthe camera viewpoint, pose articulation, 3D part shapes, and texture. Since we do not assume any\n2D or 3D annotations, we follow Hi-LASSIE [38] and adopt an analysis-by-synthesis approach to\nreconstruct 3D shape and texture that are faithful to the input images. That is, we render the 3D\npart using a differentiable renderer [16] and compare them with the 2D images, pseudo ground-truth\nsilhouettes, and DINO-ViT features. Fig. 2 (top) illustrates the shape and texture optimization.\nLASSIE and Hi-LASSIE losses. Given the rendered silhouette \u02dc\nM j and pseudo ground-truth \u02c6\nM j\nof instance j, the silhouette loss Lsil can be written as: Lsil = P\nj\u2225 \u02dc\nM j \u2212 \u02c6\nM j\u22252. LASSIE [39]\nand Hi-LASSIE [38] further leverage the 2D correspondence of DINO features between images\nof the same animal class to define a semantic consistency loss Lsem. Lsem can be interpreted as\nthe Chamfer distance between 3D surface points and 2D pixels, enforcing the aggregated 3D point\nfeatures to project closer to the similar pixel features in all images. To regularize the pose articulations\nand part shapes, [39, 38] also apply a part rotation loss Lrot, Laplacian mesh regularization Llap,\nand surface normal loss Lnorm. The part rotation loss Lrot = P\nj\u2225Rj \u2212 \u00afR\u22252 limits the angle offsets\nfrom resting pose, where Rj is the part rotations of instance j and \u00afR denotes the part rotations of\nshared resting pose. Llap and Lnorm encourage smooth 3D surfaces by pulling each vertex towards\nthe center of its neighbors and enforcing neighboring faces to have similar normals, respectively. We\nomit the details and refer the readers to [39, 38]. Considering that the reconstruction (Lsil, Lsem)\nand regularization (Lrot, Llap, Lnorm) losses are generic and effective on articulated shapes, we use\nthem in ARTIC3D along with novel texture reconstruction and DASS modules.\nTexture reconstruction. Both [39, 38] directly sample texture from input RGB, resulting in unrealis-\ntic textures in occluded regions. To obtain more realistic textures, we also optimize a texture image\nTi for each part. The vertex colors C \u2208 R3\u00d7m are sampled via the pre-defined UV mapping S of\nsurface points X. Formally, the surface color sampling of part i can be expressed as Ci = Ti(S(X)).\nThe sampled surface texture are then symmetrized according to the symmetry plane defined in the 3D\nskeleton. Note that the texture images are optimized per instance since the animals in web images\ncan have diverse texture. Similar to the Llap, we enforce the surface texture to be close to input\nimage when rendered from the estimated input view. The texture reconstruction loss is defined as:\nLtext = P\nj\u2225 \u02c6\nM j \u2299 (\u02c6Ij \u2212 \u02dcIj)\u22252 where \u02c6Ij denotes the clean input image of instance j after input\npreprocessing and \u02c6\nM j denotes the corresponding animal mask; \u02dcIj is the rendered RGB image from\nthe estimated 3D shape and texture; and \u2299 denotes element-wise product. The reconstruction loss\nis masked by the estimated foreground silhouette so that the surface texture optimization is only\neffected by the visible non-occluded animal pixels.\nDistilling 3D reconstruction. In addition to the aforementioned losses, we propose to increase the\nshape and texture details by distilling 3D reconstruction. Here, we use DASS as a critic to evaluate\nhow well a 3D reconstruction looks in its 2D renders, and calculate pixel gradients from the image\ntarget. Similar to prior diffusion-based methods [21, 18, 15], we render the 3D surfaces with random\nviewpoints, lighting, and background colors during training. Moreover, we design a pose exploration\nscheme to densify the articulation space in our sparse-image scenario. In particular, we randomly\ninterpolate the estimated bone rotation (Rj1, Rj2) of two instances (j1, j2), and generate a new\ninstance with novel pose R\u2032 = \u03b1Rj1 +(1\u2212\u03b1)Rj2 for rendering, where \u03b1 \u2208 (0, 1) is a random scalar.\nAs such, we can better constrain the part deformation by diffusion prior and prevent irregular shape or\ndisconnection between parts. As shown in Fig. 2, we then diffuse the latent codes of rendered images\nand obtain pixel gradients from the DASS module. The resulting gradients are back-propagated to\nupdate the part surface texture, deformation MLP, bone transformation, and camera viewpoints. In\nour experiments, we observe that the RGB gradients do not propagate well through the SoftRas [16]\nblending function, and we thus modify it with a layered blending approach proposed in [20].\nOptimization details. The overall optimization objective can be expressed as the weighted sum\nof all the losses L = P\nl\u2208L \u03b1lLl, where L = {sil, sem, rot, lap, norm, text, dass} as described\nabove. We optimize the shared and instance-specific shapes in two stages. That is, we first update\nthe shared part MLPs along with camera viewpoints and pose parameters. Then, we fine-tune the\ninstance-specific part MLPs and optimize texture images for each instance. All model parameters are\nupdated using an Adam optimizer [10]. We render the images at 512\u00d7512 resolution and at 128\u00d7128\nfor the part texture images. More optimization details are described in the supplemental material.\n6\nFigure 4: E-LASSIE samples. We extend LASSIE [39] image sets with 15 occluded or truncated\nimages per animal class and annotate the 2D keypoints for evaluation. These noisy images pose great\nchallenges to sparse-image optimization since the per-instance 3D shapes can easily overfit to the\nvisible parts and ignore the rest.\n3.5\nAnimation fine-tuning\nOne can easily animate the resulting 3D articulated animals by gradually rotating the skeleton\nbones and their corresponding parts surfaces. However, the rigid part transformations often result\nin disconnected shapes or texture around the joints. To improve the rendered animation in 2D,\none can naively use DASS frame-by-frame on a sequence of articulated shapes. However this can\nproduce artifacts like color flickering and shape inconsistency across the frames. As a remedy, we\nfurther propose a fine-tuning step, called Temporal-DASS (T-DASS), to generate high-quality and\ntemporally consistent 2D animations based on the ARTIC3D outputs. Given a sequence of part\ntransformations from simple interpolation across instances or motion re-targeting, we render the\n3D surfaces as video frames {Jk \u2208 RH\u00d7W \u00d73(k \u2208 {1, ..., K})} and encode them into latent codes\n{zk \u2208 Rh\u00d7w\u00d73} through the SD encoder E. Then, we design a reconstruction loss Lrecon and\ntemporal consistency loss Ltemp to fine-tune the animation in the latent space. Similar to DASS,\nwe obtain the reconstruction targets {z\u2032\nk} by accumulating latent SDS gradients \u2207zk for multiple\nsteps: z\u2032\nk = zk \u2212 \u2207zk. The reconstruction loss can then be written as: Lrecon = P\nt\u2225(zk \u2212 z\u2032\nk)\u22252.\nTo enforce temporal consistency, we exploit our 3D surface outputs and calculate accurate 2D\ncorrespondences across neighboring frames. Specifically, for each latent pixel in frame zk, we find\nthe closest visible 3D surfaces via mesh rasterization, then backtrack their 2D projection in frame\nzk\u22121, forming a dense 2D flow field Fk \u2208 Rh\u00d7w\u00d72. Intuitively, the corresponding pixels should\nhave similar latent codes. Hence, we use Fk to perform temporal warpping on the latent codes zk\u22121,\ndenoted as: warp(zk\u22121, Fk), and define Ltemp as: Ltemp = PK\nk=2\u2225(zk \u2212 warp(zk\u22121, Fk)\u22252. We\nfine-tune the latent codes {zk} with Lrecon and Ltemp, where {Fk} are pre-computed and {z\u2032\nk}\nare updated in each iteration. Finally, we can simply obtain the RGB video frames by passing the\noptimized latent codes through the SD decoder {D(zk)}. The proposed Lrecon encourages better\nshape and texture details in each frame, and Ltemp can effectively regularize latent updates temporally.\nNote that T-DASS optimizes the latent codes and takes temporal consistency into account, which is\ndifferent from DASS which operates on each image individually.\n4\nExperiments\nDatasets. Following [39, 38], we evaluate ARTIC3D on the Pascal-Part [5] and LASSIE [39]\nimages. From Pascal-Part, we obtain images of horse, cow, and sheep, as well as their 2D keypoints\nautomatically computed using the ground-truth 2D part masks. The LASSIE dataset includes web\nimages of other animal species (zebra, tiger, giraffe, elephant, kangaroo, and penguin) and 2D\nkeypoint annotations. Each image collection contains roughly 30 images of different instances with\ndiverse appearances, which are manually filtered so that the animal bodies are fully visible in the\nimages. To evaluate the model robustness in a more practical setting, we extend the LASSIE image\nsets with several noisy images where the animals are occluded or truncated. In particular, we collect\n15 additional web images (CC-licensed) per class and annotate the 2D keypoints for evaluation. We\ncall the extended image sets E-LASSIE and show some examples in Fig. 4. For the experiments on\nE-LASSIE, we optimize and evaluate on all the 45 images in each set.\nBaselines. We mainly compare ARTIC3D with LASSIE [39] and Hi-LASSIE [38] as we deal\nwith the same problem setting, namely sparse image optimization for articulated animal shapes.\nFor reference, we also compare the results with several learning-based methods like A-CSM [11],\nMagicPony [15], and Farm3D [9]. Note that these approaches are not directly comparable to\nARTIC3D since they train a feedforward network on large-scale image sets (not available in our\nscenario). Although related, some other recent works on 3D surface reconstruction either cannot\nhandle articulations [12, 14, 7, 31, 40] or require different inputs [13, 34, 36]. As a stronger baseline,\nwe implement Hi-LASSIE+, incorporating the standard SDS loss as in [27, 15, 18] (back-propagate\nlatent gradients through encoder) during Hi-LASSIE [38] optimization for shape and texture.\n7\nInput \nimages\nArtic3D\n(ours)\nHi-LASSIE+\nHi-LASSIE\nFigure 5: Visual comparison of ARTIC3D and other baselines. For each input image, we show\nthe 3D textured outputs from input (upper) and novel (lower) views. The results demonstrate that\nARTIC3D is more robust to noisy images with occlusions or truncation, producing 3D shape and\ntexture that are detailed and faithful to the input images.\nEvaluation metrics. Considering the lack of ground-truth 3D annotations in our datasets, we follow a\ncommon practice [45, 11, 39, 38] to use keypoint transfer accuracy as a quantitative metric to evaluate\n3D reconstruction. For each pair of images, we map the annotated 2D keypoints on source image\nonto the canonical 3D surfaces, re-project them to the target image via the estimated camera, pose,\nand shape, and compare the transferred keypoints with target annotations. To further evaluate the\nquality of textured outputs, we compute CLIP [22] features of the 3D output renders under densely\nsampled viewpoints, and calculate the feature similarity against text prompt as well as input images.\nWhile most prior arts on 3D shape generation [21, 23] only evaluate the image-text similarity, we also\nevaluate the image-image similarity since our outputs should be faithful to both the category-level\ntextual description as well as instance-specific input images. We use a text prompt: \u201cA photo of *\u201d for\neach animal class \u201c*\u201d in our experiments. A CLIP ViT-B/32 model is used to compute the average\nfeature similarity over 36 uniformly sampled azimuth renders at a fixed elevation of 30 degrees. We\nshow the main results here and more quantitative and qualitative comparisons in the supplemental\nmaterial, including animation videos, user study, and more detailed ablation study.\nQualitative results. Fig. 1 shows some sample outputs of ARTIC3D. In Fig. 5, we compare the visual\nresults of Hi-LASSIE, Hi-LASSIE+, and ARTIC3D on the E-LASSIE images. Both Hi-LASSIE\nand Hi-LASSIE+ produce irregular pose and shape for the invisible parts. Regarding surface texture,\nHi-LASSIE reconstructs faithful texture from the input view but noisy in novel views, since it naively\nsamples vertex colors from the input images. The output texture of Hi-LASSIE+ is generally less\nnoisy thanks to the SDS loss. By comparison, ARTIC3D accurately estimates the camera viewpoint,\npose, shape, and texture even with the presence of occlusions or truncation. The ARTIC3D outputs\nare detailed, faithful to input images, and realistic from both input and novel views.\nQuantitative comparisons. We show comparisons of the keypoint transfer accuracy (PCK) in\nTables 1. On both LASSIE and E-LASSIE image sets, Hi-LASSIE+ produces a marginal PCK gain\nfrom Hi-LASSIE [38] by naively applying the SDS loss. ARTIC3D, on the other hand, achieves\nconsistently higher PCK than the baselines, especially on the noisy E-LASSIE images. The results\ndemonstrate that our diffusion-guided strategies can effectively learn more detailed, accurate, and\nrobust 3D shapes. The Pascal-Part results in Tab 2 further show that ARTIC3D performs favorably\nagainst the state-of-the-art optimization-based methods and are comparable to learning-based ap-\nproaches. In Table 3, we show the CLIP similarity comparisons on the E-LASSIE images, which\nindicate that our textured outputs are more faithful to both the input images (instance-level) and text\nprompt (class-level) for most animal classes.\nAnimation and texture transfer. In Fig. 6, we compare the animations before and after our fine-\ntuning step via T-DASS. While the skeleton-based representation allows easy animation via rigid\npart transformation, the output part shapes and texture are often disconnected and irregular around\nthe joints. The results show that T-DASS can effectively produce high-quality animations that are\ndetailed in shape and texture and temporally consistent between frames. In addition to animation, our\n3D part surfaces also enables convenient controllable syntheses like texture transfer and pose transfer\n8\nTable 1: Keypoint transfer evaluations on the LASSIE [39]\nand E-LASSIE image sets. We report the average PCK@0.05\n(\u2191) on all pairs of images.\nARTIC3D performs favorably\nagainst the optimization-based prior arts on all animal classes.\nThe larger performance gap in the E-LASSIE demonstrates\nthat ARTIC3D is robust to noisy images.\nMethod\nImage set\nElephant Giraffe Kangaroo Penguin Tiger Zebra\nLASSIE [39]\nLASSIE\n40.3\n60.5\n31.5\n40.6\n62.4\n63.3\nHi-LASSIE [38]\nLASSIE\n42.7\n61.6\n35.0\n44.4\n63.1\n64.2\nHi-LASSIE+\nLASSIE\n43.3\n61.5\n35.5\n44.6\n63.4\n64.0\nARTIC3D\nLASSIE\n44.1\n61.9\n36.7\n45.3\n64.0\n64.8\nHi-LASSIE [38] E-LASSIE\n37.6\n54.3\n31.9\n41.7\n57.4\n60.1\nHi-LASSIE+\nE-LASSIE\n38.3\n54.8\n32.8\n41.8\n57.7\n61.3\nARTIC3D\nE-LASSIE\n39.8\n58.0\n35.3\n43.8\n59.3\n63.0\nTable 2: Keypoint transfer results\non Pascal-Part [6]. We report the\nmean PCK@0.1 (\u2191) on all pairs of\nimages. \u2217 indicates learning-based\nmodels which are trained on a large-\nscale image set.\nMethod\nHorse\nCow\nSheep\nUMR\u2217 [14]\n24.4\n-\n-\nA-CSM\u2217 [11]\n32.9\n26.3\n28.6\nMagicPony\u2217 [33]\n42.9\n42.5\n26.2\nFarm3D\u2217 [9]\n42.5\n40.2\n32.8\nLASSIE [39]\n42.2\n37.5\n27.5\nHi-LASSIE [38]\n43.7\n42.1\n29.9\nHi-LASSIE+\n43.3\n42.3\n30.5\nARTIC3D\n44.4\n43.0\n31.9\nTable 3: CLIP similarity (\u2191) evaluations on the E-LASSIE images. For each animal class, we\ncalculate cosine similarities s1/s2, where s1 is the image-image similarity (against masked input\nimage) and s2 is the image-text similarity (against text prompt).\nMethod\nElephant\nGiraffe\nKangaroo\nPenguin\nTiger\nZebra\nHi-LASSIE [38]\n80.0 / 26.3\n85.2 / 29.6\n77.4 / 25.6\n85.8 / 30.8\n79.7 / 25.6\n83.8 / 27.4\nHi-LASSIE+\n79.0 / 27.7\n84.7 / 30.2\n78.3 / 29.1\n82.9 / 32.3\n75.3 / 25.3\n81.9 / 27.6\nARTIC3D\n82.6 / 28.4\n85.3 / 30.7\n81.6 / 29.9\n85.5 / 33.1\n80.0 / 27.8\n84.1 / 29.4\nRigid transform\nFine-tuned\nFigure 6: Animation fine-tuning. Compared\nto the original animated outputs via rigid trans-\nformation (top), our animation fine-tuning (bot-\ntom) effectively improves the shape and tex-\nture details, especially around animal joints.\nSource\nTarget\nTransferred\nTransferred\nFigure 7: Texture transfer. Our part surface repre-\nsentation enables applications like pose or texture\ntransfer. Given a source shape and target texture, we\nshow the transferred texture between instances (left)\nand animal species (right).\nbetween different instance or animal classes. Several examples of texture transfer are shown in Fig. 7.\nMore visual results with video results of these applications are shown in the supplemental material.\nLimitations. ARTIC3D relies on the 3D skeleton discovered by Hi-LASSIE [38] to initialize the\nparts. If the animal bodies are occluded or truncated in most images, the skeleton initialization\ntends to be inaccurate, and thus limiting ARTIC3D\u2019s ability to form realistic parts. Although our\ninput preprocessing method can mitigate this issue to some extent, fluffy animals (e.g.sheep) with\nambiguous skeletal configuration can still pose challenges in skeleton discovery. In addition, the\nfront-facing bias in diffusion models sometimes lead to unrealistic texture like multiple faces, which\nalso affects our reconstruction quality. See the supplemental material for failure cases.\n5\nConclusion\nWe propose ARTIC3D, a diffusion-guided framework to reconstruct 3D articulated shapes and texture\nfrom sparse and noisy web images. Specifically, we design a novel DASS module to efficiently\ncalculate pixel gradients from score distillation for 3D surface optimization and use it in the input\npreprocessing of noisy images; Shape and texture optimization; as well as the animation fine-tuning.\nResults on both the existing datasets as well as newly introduced noisy web images demonstrate that\nARTIC3D produces more robust, detailed, and realistic reconstructions against prior arts.\n9\nReferences\n[1] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep ViT features as dense visual descriptors.\narXiv preprint arXiv:2112.05814, 2021. 5\n[2] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, and Hendrik Lensch. NeRD:\nNeural reflectance decomposition from image collections. In CVPR, pages 12684\u201312694, 2021. 3\n[3] Mark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T Barron, Hendrik\nLensch, and Varun Jampani. SAMURAI: Shape and material from unconstrained real-world arbitrary\nimage collections. NeurIPS, 2022. 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 9650\u20139660, 2021. 3, 5\n[5] Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Sanja Fidler, Raquel Urtasun, and Alan Yuille. Detect\nwhat you can: Detecting and representing objects using holistic models and body parts. In CVPR, pages\n1971\u20131978, 2014. 2, 7\n[6] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The\nPASCAL visual object classes (VOC) challenge. IJCV, 88(2):303\u2013338, 2010. 9\n[7] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Shape and viewpoint without keypoints. In ECCV,\npages 88\u2013104, 2020. 7\n[8] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\n4\n[9] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht, and Andrea Vedaldi. Farm3d: Learning\narticulated 3d animals by distilling 2d diffusion. arXiv preprint arXiv:2304.10535, 2023. 5, 7, 9\n[10] Diederik P Kingma and Jimmy Ba.\nAdam: A method for stochastic optimization.\narXiv preprint\narXiv:1412.6980, 2014. 6\n[11] Nilesh Kulkarni, Abhinav Gupta, David F Fouhey, and Shubham Tulsiani. Articulation-aware canonical\nsurface mapping. In CVPR, pages 452\u2013461, 2020. 3, 7, 8, 9\n[12] Nilesh Kulkarni, Abhinav Gupta, and Shubham Tulsiani. Canonical surface mapping via geometric cycle\nconsistency. In ICCV, pages 2202\u20132211, 2019. 3, 7\n[13] Xueting Li, Sifei Liu, Shalini De Mello, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, and Jan Kautz.\nOnline adaptation for consistent mesh reconstruction in the wild. NeurIPS, 33:15009\u201315019, 2020. 7\n[14] Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz.\nSelf-supervised single-view 3D reconstruction via semantic consistency. In ECCV, pages 677\u2013693, 2020.\n7, 9\n[15] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis,\nSanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. arXiv\npreprint arXiv:2211.10440, 2022. 1, 3, 4, 6, 7\n[16] Shichen Liu, Tianye Li, Weikai Chen, and Hao Li. Soft rasterizer: A differentiable renderer for image-based\n3D reasoning. In CVPR, pages 7708\u20137717, 2019. 6\n[17] Luke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360 {\\deg}\nreconstruction of any object from a single image. arXiv preprint arXiv:2302.10663, 2023. 3\n[18] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-\nguided generation of 3d shapes and textures. arXiv preprint arXiv:2211.07600, 2022. 1, 3, 4, 6, 7\n[19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng.\nNeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 405\u2013421, 2020. 3\n[20] Tom Monnier, Matthew Fisher, Alexei A Efros, and Mathieu Aubry. Share with thy neighbors: Single-view\nreconstruction by cross-instance consistency. In ECCV, pages 285\u2013303, 2022. 6\n[21] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion.\narXiv preprint arXiv:2209.14988, 2022. 1, 2, 3, 4, 6, 8\n[22] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In ICML, pages 8748\u20138763, 2021. 8\n[23] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran Zada,\nKfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven text-to-3d\ngeneration. arXiv preprint arXiv:2303.13508, 2023. 1, 3, 8\n[24] Amit Raj, Michael Zollhofer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and Stephen\nLombardi. Pixel-aligned volumetric avatars. In CVPR, pages 11733\u201311742, 2021. 3\n[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821\u20138831, 2021. 1\n[26] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided\ntexturing of 3d shapes. arXiv preprint arXiv:2302.01721, 2023. 3\n[27] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In CVPR, pages 10684\u201310695, 2022. 1, 2, 3, 4, 5, 7\n10\n[28] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-\nto-image diffusion models with deep language understanding. NeurIPS, 35:36479\u201336494, 2022. 1,\n3\n[29] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman\nGoyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv\npreprint arXiv:2301.11280, 2023. 3\n[30] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance\nfields from sparse and noisy poses. arXiv preprint arXiv:2211.11738, 2022. 3\n[31] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Implicit mesh reconstruction from unannotated\nimage collections. arXiv preprint arXiv:2007.08504, 2020. 7\n[32] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron,\nRicardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet: Learning multi-view image-based\nrendering. In CVPR, pages 4690\u20134699, 2021. 3\n[33] Shangzhe Wu, Ruining Li, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Magicpony: Learning\narticulated 3d animals in the wild. arXiv preprint arXiv:2211.12497, 2022. 1, 3, 9\n[34] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Huiwen Chang, Deva\nRamanan, William T Freeman, and Ce Liu. LASR: Learning articulated shape reconstruction from a\nmonocular video. In CVPR, pages 15980\u201315989, 2021. 2, 7\n[35] Gengshan Yang, Deqing Sun, Varun Jampani, Daniel Vlasic, Forrester Cole, Ce Liu, and Deva Ramanan.\nViSER: Video-specific surface embeddings for articulated 3d shape reconstruction. NeurIPS, 34, 2021. 2\n[36] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan, Andrea Vedaldi, and Hanbyul Joo. BANMo:\nBuilding animatable 3D neural models from many casual videos. arXiv preprint arXiv:2112.12761, 2021.\n2, 7\n[37] Gengshan Yang, Chaoyang Wang, N Dinesh Reddy, and Deva Ramanan. Reconstructing animatable\ncategories from videos. arXiv preprint arXiv:2305.06351, 2023. 2\n[38] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani.\nHi-lassie: High-fidelity articulated shape and skeleton discovery from sparse image ensemble. arXiv\npreprint arXiv:2212.11042, 2022. 1, 2, 3, 6, 7, 8, 9\n[39] Chun-Han Yao, Wei-Chih Hung, Yuanzhen Li, Michael Rubinstein, Ming-Hsuan Yang, and Varun Jampani.\nLassie: Learning articulated shapes from sparse image ensemble via 3d part discovery. arXiv preprint\narXiv:2207.03434, 2022. 1, 2, 3, 6, 7, 8, 9\n[40] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta. Shelf-supervised mesh prediction in the wild. In CVPR,\npages 8843\u20138852, 2021. 7\n[41] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or\nfew images. In CVPR, pages 4578\u20134587, 2021. 3\n[42] Jason Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. NeRS: Neural reflectance surfaces\nfor sparse-view 3D reconstruction in the wild. NeurIPS, 34, 2021. 3\n[43] Kai Zhang, Fujun Luan, Qianqian Wang, Kavita Bala, and Noah Snavely. PhySG: Inverse rendering with\nspherical Gaussians for physics-based material editing and relighting. In CVPR, pages 5453\u20135462, 2021. 3\n[44] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv\npreprint arXiv:2302.05543, 2023. 3\n[45] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. Three-D Safari: Learning to\nestimate zebra pose, shape, and texture from images \"in the wild\". In ICCV, pages 5359\u20135368, 2019. 2, 8\n[46] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3D menagerie: Modeling the 3D\nshape and pose of animals. In CVPR, pages 6365\u20136373, 2017. 2\n11\n"
  },
  {
    "title": "On the Reliability of Watermarks for Large Language Models",
    "link": "https://arxiv.org/pdf/2306.04634.pdf",
    "upvote": "4",
    "text": "On the Reliability of Watermarks for Large Language\nModels\nJohn Kirchenbauer\u2217, Jonas Geiping\u2217\nYuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong\nKasun Fernando\u2020, Aniruddha Saha, Micah Goldblum\u2021, Tom Goldstein\nUniversity of Maryland\nAbstract\nAs LLMs become commonplace, machine-generated text has the potential to flood\nthe internet with spam, social media bots, and valueless content. Watermarking is a\nsimple and effective strategy for mitigating such harms by enabling the detection\nand documentation of LLM-generated text. Yet a crucial question remains: How\nreliable is watermarking in realistic settings in the wild? There, watermarked text\nmay be modified to suit a user\u2019s needs, or entirely rewritten to avoid detection.\nWe study the robustness of watermarked text after it is re-written by humans,\nparaphrased by a non-watermarked LLM, or mixed into a longer hand-written\ndocument. We find that watermarks remain detectable even after human and\nmachine paraphrasing. While these attacks dilute the strength of the watermark,\nparaphrases are statistically likely to leak n-grams or even longer fragments of\nthe original text, resulting in high-confidence detections when enough tokens\nare observed. For example, after strong human paraphrasing the watermark is\ndetectable after observing 800 tokens on average, when setting a 1e\u22125 false\npositive rate. We also consider a range of new detection schemes that are sensitive\nto short spans of watermarked text embedded inside a large document, and we\ncompare the robustness of watermarking to other kinds of detectors.\n1\nIntroduction\nThe capability to tell the difference between machine-generated and human-written text underlies\nmany approaches to reduce potential harms caused by generative language models [Bender et al.,\n2021, Crothers et al., 2022]. This includes known harms, such as models being used at-scale for\nmalicious purposes including social media bots, fake product reviews [Palmer, 2023], automated\ntext generation on wikipedia [Woodcock, 2023], or automatic generation of targeted spearphishing\nattacks on vulnerable subpopulations [Schneier, 2021]. Equally important, the ability to track and\ndocument the use of machine-generated text has the potential to reduce harms from future problems\nthat have not yet been observed. These problems might range from the pollution of future training\ndata [Radford et al., 2022] to the hyper-prevalence of LLM-generated blogs and other web content.\nUnfortunately, detection of machine-generated text is potentially difficult. Models are prompted\nwith diverse instructions, resulting in a wide range of downstream behaviors for both machines and\n\u2217Equal contribution. \u2020 Scuola Normale Superiore di Pisa. \u2021 New York University.\nPreprint. Under review.\narXiv:2306.04634v3  [cs.LG]  30 Jun 2023\nMachine \nParaphrasing\nHuman \nParaphrasing\nWatermarked Text\nText Mixing\nWatermark \nFraction\nTokens Needed \nfor Detection\nFew\nModerate\nMany\nIt Depends\nFigure 1: What happens to watermarked text in-the-wild? In this work we study watermark robustness against a\nnumber of text modifications, as visualized here. We visually depict that machine paraphrasing methods have a\ntendency to shorten texts, humans are quite effective at reducing the strength of a watermark by increasing the\nnumber of red tokens, and that short spans of watermarked text may be copied and pasted into a large document.\nIn all of these scenarios, we find that high confidence detection reliably occurs given enough tokens as input.\nhumans that are difficult to characterize. This can lead to low accuracy or impractical false positive\nrates that especially impact vulnerable subgroups, such as non-native speakers [Liang et al., 2023].\nOne way to enable accurate detection of machine-generated text is through watermarking, where\ngenerated text is marked imperceptibly so that its origin can be determined [Atallah et al., 2001,\nFang et al., 2017, Kirchenbauer et al., 2023]. Because watermarks rely on subtle patterns in text that\nare statistically unlikely to be replicated by a human, watermarking enables detectors that achieve\nhigh levels of accuracy on relatively short fragments of text. This makes watermarking a promising\napproach for the reliable separation of human-written and machine-generated text [Grinbaum and\nAdomaitis, 2022]. While the effectiveness of watermarks has been shown in ideal scenarios where\nverbatim LLM outputs are fed directly to a detector, this is an idealized setting. In practice, humans\nmay mix machine-generated text into larger documents with multiple sources. Furthermore, a human\nmay revise or rephrase parts of the synthetic text (possibly aided by another language model) to better\nsuit their needs, potentially even with the deliberate goal of evading detection.\nIn this work, we investigate the reliability of watermarking as a strategy to identify machine-generated\ntext in realistic scenarios, based on the approach of Kirchenbauer et al. [2023]. We are focused on\nwhether watermarks remain detectable under various types of realistic corruptions (i.e. attacks): How\nreliable is watermarking when generated text is handled by humans, be it mixing with human-written\ntext, rewriting parts or the entire passage, or feeding the text into other popular language models for\nrephrasing? A reliable detection strategy should be robust to these common scenarios, maintaining\nsome statistical power and a low false positive rate [Crothers et al., 2022]. We make the following\ncontributions:\n\u2022 We re-investigate all parts of the watermark generation and watermark detection pipeline to\noptimize for reliability in realistic scenarios.\n\u2022 We study the reliability of watermarking against paraphrasing by strong large language\nmodels. When GPT-3.5 and purpose-built paraphrasing models are used to re-write water-\nmarked text, ROC-AUC remains above 0.85 when T = 200 tokens are available, and above\n0.9 with T = 600 tokens.\n\u2022 We consider a \u201cCopy-Paste\u201d scenario where watermarked text appears inside a larger hand-\nwritten passage. When a human-written passage of length 600 tokens has 150 tokens of\nwatermarked text inserted into it, AUC for detection is above 0.95.\n2\n\u2022 We conduct a human study in which watermarked text is re-written by volunteers with the\nexplicit goal of removing the watermark. While humans are relatively strong attackers,\nafter enough observed tokens (about 800) watermarks are still usually detectable in human\nparaphrases even when enforcing a 1e\u22125 false positive rate.\n\u2022 We provide reliability estimates of watermarking compared to other state-of-the-art ap-\nproaches, such as loss-based detection [Mitchell et al., 2023] and retrieval [Krishna et al.,\n2023], showing that these struggle at longer sequence lengths when attacked.\nWe argue that the correct way to characterize the strength and robustness of different detection\napproaches is not simply via detection accuracy metrics for a specific distribution of text, but rather\nto measure how much machine-generated text is required for each approach to succeed, and how a\nmethod behaves as a function of text sequence length. Across all the scenarios we consider in this\nwork, we ultimately find watermarking to be more robust than other post-hoc detection methods\n(such as loss-based detection and caching/retrieval schemes), especially due to its favorable sample\ncomplexity, i.e., scaling behavior in terms of amount of text that is sufficient to guarantee detection.\n2\nAn Overview of Machine-Generated Text Detection\nThe problem of separating human-written and machine-written text can be approached from several\ndirections. Broadly, we can distinguish post-hoc detection systems that require no interaction during\ntext generation, and proactive detection systems that require some action during generation. These\nlatter systems are generally much more robust, with the downside that they have to be adopted by\nthe model owner.\nThe most straightforward post-hoc detectors are binary classifiers, trained to distinguish between\nhuman and machine-generated text [OpenAI, 2019, Bakhtin et al., 2019, Fagni et al., 2020, Jawahar\net al., 2020]. As black-box approaches, these systems require no information about the language\nmodel, only the availability of sufficient training data in the form of machine-generated and human\ntext samples. In practice, obtaining such a dataset is challenging since there are many diverse use\ncases for LLMs and different users may represent vastly different domains. While approaches that use\nclassical methods such as linear/logistic regression or SVMs [Fr\u00f6hling and Zubiaga, 2021, Solaiman\net al., 2019, Crothers et al., 2022] are interpretable, deep learning approaches [Gall\u00e9 et al., 2021,\nZhong et al., 2020, Rodriguez et al., 2022] are more accurate on in-domain datasets while being\nvulnerable to out-of-distribution problems, adversarial attacks, and poisoning.\nOther detectors rely on statistical outlier detection in texts, based on entropy [Lavergne et al., 2008],\nperplexity [Beresneva, 2016, Tian, 2023], n-gram frequencies [Grechnikov et al., 2009, Badaskar et al.,\n2008], or as in DetectGPT Mitchell et al. [2023], the observation that LLMs typically assign their\nown text generations higher probability than \u201cnearby\u201d text sequences produced by span replacement\nwith a different LLM. Even though DetectGPT exhibits superior performance compared to other\nzero-shot statistical outlier detection methods, it suffers from excessive computational costs. Further,\nall advanced statistical detectors relying on language model statistics such as perplexity or curvature\nideally require white-box access to model parameters. The theoretical limits of detectability were\nstudied in Varshney et al. [2020] and Sadasivan et al. [2023], although follow-up work in Chakraborty\net al. [2023] highlights that detection should remain possible, given enough samples.\nA retrieval-based approach for text detection, as described in Krishna et al. [2023], is a noticeably\ndifferent paradigm, and an example of a proactive detection technique. Here, all text generated by\na given model is stored in a database. Later, text samples are evaluated by matching against this\ndatabase. This approach requires action taken by the model owner, but it can be quite reliable, even\nwhen faced with broad modifications of text, such as strong paraphrases. However, both the cost of\nretrieval and its false positive rate potentially scale undesirably with the size of the database. Further,\nthe act of storing all outgoing user interactions is problematic from a data privacy perspective, for\nexample under European law, or for business sectors such as finance or medicine.\nWatermarking also requires action by the model owner as they must embed the hidden watermark sig-\nnal into all outgoing text. Watermarking as a concept has a long history [Brassil et al., 1995]. Older sys-\ntems were based on rule-based methods to imprint watermarks into existing text [Atallah et al., 2001,\nChiang et al., 2004, Venugopal et al., 2011, Topkara et al., 2006]. More recent approaches for water-\nmarking neural networks Ziegler et al. [2019], Dai and Cai [2019], Abdelnabi and Fritz [2021], He et al.\n3\n[2022a,b] are learned end-to-end with both encoding and decoding of each sample. The lack of theo-\nretical guarantees and interpretability of these approaches are problems for their widespread adoption.\nHowever, it is also possible to place watermarks on a robust mathematical foundation. Watermarks\ncan be embedded by minimally modifying the distribution of generated output text [Fang et al.,\n2017, Kaptchuk et al., 2021, Kirchenbauer et al., 2023], and watermarks of this type have recently\nbeen adapted for various applications [Lee et al., 2023, Yoo et al., 2023]. In this work, we mainly\nconsider the combinatorial watermark described in Kirchenbauer et al. [2023]. At each step of the text\ngeneration process, the watermark pseudo-randomly \u201ccolors\u201d tokens into green and red lists. Then\na sampling rule is used that preferentially samples green tokes when doing so does not negatively\nimpact perplexity. To detect the watermark, a third party with knowledge of the hash function can\nreproduce the red and green lists for each step and count the violations. Thus, this method uses the\nLM\u2019s own understanding of natural text to adaptively embed the watermark, requires no usage of the\nLM to decode the watermark, and can be statistically validated.\n3\nHow to improve watermark reliability?\nThere are a number of parameter and design choices that go into a watermark, with different\nparameters offering benefits in different use cases. In this section, we briefly describe the watermark\nproposed in Kirchenbauer et al. [2023], and the variations on the watermark that we will study.\nAssume an autoregressive language model is trained on a vocabulary V of size |V |. Given a sequence\nof tokens as input at step t, a language model predicts the next token in the sequence by outputting a\nvector of logit scores lt \u2208 R|V | with one entry for each item in the vocabulary. A random number\ngenerator is seeded with a context window of h preceding tokens, based on a pseudo-random function\n(PRF) f : Nh \u2192 N. With this random seed, a subset of tokens of size \u03b3|V | are \u201ccolored\u201d green and\ndenoted Gt. Now, the logit scores lt are modified so that\nltk =\n\u001altk + \u03b4\nif\nk \u2208 Gt\nltk\notherwise.\n(1)\nAfter modifications, these logit scores can be used for any desired sampling scheme. In the simplest\ncase, one passes the scores through a softmax layer and samples from the output distribution, resulting\nin a bias towards tokens from Gt.\nThe watermark can be described by four parameters. The \u201chash\u201d used to generate the greenlists f\nwith context width h, greenlist fraction \u03b3, and the logit bias \u03b4. After watermarked text is generated,\none can check for the watermark without having access to the LLM by re-computing the greenlist\nat each position and finding the set s of greenlist token positions. The statistical significance of a\nsequence of tokens of length T can be established by deriving the z-score\nz = (|s| \u2212 \u03b3T) /\np\n\u03b3(1 \u2212 \u03b3)T.\n(2)\nWhen this z-score is large (and the corresponding P-value is small), one can be confident that the text\nis watermarked.\nWe now discuss several variations to this scheme, which lead to improved empirical behavior.\n3.1\nImproved Hashing Schemes\nThe experiments in Kirchenbauer et al. [2023] focus on a simple scheme where the random number\ngenerator is seeded using h = 1, i.e., only a single token at position t \u2212 1 is used to color the token\nat position t. We refer to this scheme as LeftHash. Because the greenlist depends only on one\nsingle token, a third-party observer could learn the greenlist associated with the token at position\nt \u2212 1 by searching subsequent words at position t that are less likely to appear than expected under\na non-watermarked distribution. In situations where the watermark scheme is intended to be kept\nsecret behind an API, a more secure scheme is needed.\nKirchenbauer et al. [2023] also mention a scheme (Algorithm 3) in which the greenlist at position t\nis determined by including the token at position t itself (yet to be generated), in addition to tokens\nto the left of t in the inputs to f. We call this hashing scheme SelfHash. This approach effectively\nincreases the context width h by 1, making it harder to discover the watermark rules by brute-force\n4\n1\n2\n3\n4\n5\n6\n7\n8\nContext Width\n0.6\n0.7\n0.8\n0.9\n1.0\nROC-AUC (\u2192)\nDetection Rate vs. Context Width\nAdditive-LeftHash\nAdditive-SelfHash\nMin-LeftHash\nMin-SelfHash\nSkip-LeftHash\nSkip-SelfHash\n1\n2\n3\n4\n5\n6\n7\n8\nContext Width\n4\n5\n6\n7\n8\n9\n10\n11\nLog Diversity (\u2192)\nLog Diversity vs. Context Width\nAdditive-LeftHash\nAdditive-SelfHash\nMin-LeftHash\nMin-SelfHash\nSkip-LeftHash\nSkip-SelfHash\nFigure 2: Effect of context width on watermark robustness and diversity. (Left) Effect of context width on\nwatermark robustness as measured by ROC-AUC after a paraphrasing attack by GPT. For larger context widths\nSkip and Min variants provide the best detection strength. (Right) Effect of the seeding scheme context width\non the quality of the text as measured by log diversity. A small context width produces less diverse outputs\nfor all three schemes, and the Additive and Skip schemes produce more diverse text at larger context widths\nthan the Min scheme. (Both) Watermark parameters \u03b3, \u03b4 are fixed at (0.25, 4.0). The black circle marks the\nsimple Additive-LeftHash with context width h = 1 scheme, and the brown circle marks the width h = 4\nvariant of the Min-SelfHash scheme, both evaluated throughout the work (names shortened to \u201cLeftHash\u201d and\n\u201cSelfHash\u201d respectively).\nmethods. We generalize this scheme to include arbitrary functions f and text generation routines,\nwhich we describe in detail in Algorithm 1 in the Appendix.\nWhen the context width h is increased to maintain secrecy of the red/green list rules, we find that\ndetection reliability substantially depends on the hashing scheme. We define the following functions\nf : Nh \u2192 N that map a span of tokens {xi} onto a pseudo-random number. Each depends on a secret\nsalt value s \u2208 N and a standard integer PRF P : N \u2192 N.\nAdditive: This is the function described in Kirchenbauer et al. [2023]. We extend it to h > 1 by\ndefining fAdditive-LeftHash(x) = P\n\u0010\ns Ph\ni=1 xi\n\u0011\n. While permutations of the context x do not change\nthe outcome, removing or swapping a single token from x changes the hash and hence breaks the\nwatermark at this token.\nSkip: This function uses only the left-most token in the context: fSkip-LeftHash(x) = P (sxh). This\nhash is robust to changes in the non-leftmost token, but it is susceptible to insertions/deletions.\nMin: This function is defined by fMin-LeftHash(x) = mini\u22081,...,h P (sxi). It is robust to permutations\nwithin the context and it is partially robust to insertions/deletions. Given that all P (sxi) are pseudo-\nrandom and equally likely to be the smallest value, the likelihood of failure of this scheme is\nproportional to the number of values removed from the context, i.e. if h = 4 and 2 tokens are\nremoved/missing from the context, the PRF is still 50% likely to generate the same hash.\nChoosing a Scheme. Figure 2 shows that a small context width h provides the best robustness\nto machine paraphrasing. At wider context widths, Skip and Min variants remain strong under\nattack while Additive suffers. However, we see that this robustness improvement comes at a\ntrade-off to text quality as the Min schemes produce less diverse outputs. Still, at a context width\nh = 4, the Min-SelfHash scheme (brown circle marker) achieves the same diversity as the original\nAdditive-LeftHash scheme at width h = 1 (black circle), while being more robust. This shows\nthat we can use the additional strength provided by Min and SelfHash to run longer context widths,\nwhich in turn secure the watermark. We adopt these two schemes as \u201cSelfHash\u201d and \u201cLeftHash\u201d\nrespectively in the rest of the sections of the main work. In Appendix A.2, we further explore the\neffect of the scheme choice on both syntactic and semantic aspects of text quality.\n3.2\nImproved Watermark Detection\nThe original z-test, Equation (2), may not be optimal when watermarked text is interspersed with\nnon-watermarked text. Consider the case of a single paragraph of watermarked text embedded inside\na much larger non-watermarked document. Because the z-score is computed globally over the whole\ndocument, it gets diluted because the surrounding text reduces the average greenlist rate.\n5\nWe design a windowed test, called WinMax to accurately detect watermarked regions even in long\ndocuments. This is an alternative way of formulating a detection hypothesis that can be employed\noptionally or in conjunction with the original test and requires no modification of the generation\nscheme. Given a sequence of tokens, we first score the sequence on per-token basis to find the binary\nvector of hits s \u2208 {0, 1}T to each green list, which we can convert to a partial sum representation\npk = Pk\ni=1 si. WinMax searches for the continuous span of tokens that generates that highest\nz-score. More formally, it computes\nzwin-max = max\ni,j,\ni<j\n(pj \u2212 pi) \u2212 \u03b3(j \u2212 i)\np\n\u03b3(1 \u2212 \u03b3)(j \u2212 i)\n.\n(3)\nAs this test involves multiple hypothesis testing, we later calibrate to a fixed false-positive rate based\non comparisons with non-watermarked text.\nWe further investigated a more complex anomaly detector based on run-length differences between\nwatermarked and unwatermarked text [Bradley, 1960]. Yet, we found no gains from such a detector\nover z-test and WinMax within the range of settings we consider in this work. We include a brief\ndescription of this alternate detection algorithm as a starting point for future research in Appendix A.5.\n4\nEvaluating Watermarking in the Wild\nWatermarks are extremely accurate in simple scenarios in which a long span (50+ tokens) of text\nis tested in isolation and without modification. However, in many use cases, the generated text will\nbe embedded inside a larger document, or edited by a human, or paraphrased by another language\nmodel. These modifications may be done to increase the utility of the text, or to maliciously erase\nthe watermark and evade detection. This section studies the robustness of the watermark under these\nmore complex use cases.\nWe assume the following threat model: A user of watermarked text is aware that the text is water-\nmarked, but has no knowledge of the hashing scheme, fraction \u03b3, or context width h that describe the\nwatermark. They paraphrase some (possibly all) spans of the text to evade detection.\nIn this scenario, we can understand watermark reliability through the following two observations:\n1. Without white-box access to the hashing scheme, a user cannot remove the watermark\nwithout ensuring that the re-phrased text contains none of the n-grams from the original\ntext. If the user ever recycles long words (which often contains multiple tokens) or phrases\nfrom the original text, the watermark will remain, although with reduced strength.\n2. If a paraphrased text skews even slightly toward watermarked behavior, the watermark will\nbe detected given enough tokens. Suppose each token is only \u03b5 more likely to be green\nthan a random baseline, i.e. |s| = \u03b3T(1 + \u03b5). Then for any z-score threshold, we expect\nto detect the watermark after seeing T = (z2 \u2212 \u03b3z2)/(\u03b52\u03b3) tokens.\nFor reasons above, we do not expect paraphrasing attacks to remove a watermark, especially when\nusing off-the-shelf AI paraphrasing tools which we suspect are likely to recycle phrases. Rather, we\nexpect such attacks to increase the number of tokens needed for confident detection. Chakraborty\net al. [2023]. This would match with the theoretical analysis of Chakraborty et al. [2023], who assert\nthat for an optimal detector, detection is always possible, given a sufficient number of samples.\nExperimental Setup: Following Kirchenbauer et al. [2023], we use the Colossal Common Crawl\nCleaned corpus (C4) dataset as a source of prompts for open-ended generation. For the human\nstudy, we adopt the \u201cLong-Form Question Answering\u201d (LFQA) dataset curated by Krishna et al.\n[2023] based on a selection of posts and responses to question\u2019s on Reddit\u2019s \u201cExplain Like I\u2019m\nFive\u201d (ELI5) forum. We provide an implementation of our experiments at https://github.com/\njwkirchenbauer/lm-watermarking.\nIn the majority of our experiments, we use llama [Touvron et al., 2023], a modern state-of-the-art\nopen source general-purpose model to generate watermarked text. In particular, we use the 7 billion\nparameter variant under the research use permissions of the license. In the human study portion of the\nexperiments, we adopt Vicuna [Vicuna-Team, 2023], a fine-tuned variant of the base model, better\nsuited to responding to the QA prompts used in the study. We ablate these standard dataset and model\nsettings in the Appendix by running a set of similar experiments on alternate datasets and models.\n6\nLeftHash CP-1-25%\nSelfHash CP-1-25%\nLeftHash CP-3-25%\nSelfHash CP-3-25%\nSelfHash CP-1-10%\nLeftHash CP-1-10%\nLeftHash CP-3-10%\nSelfHash CP-3-10%\nSelfHash Dipper\nLeftHash Dipper\nSelfHash GPT\nLeftHash GPT\nLeftHash CP-1-25%\nLeftHash CP-3-25%\nSelfHash CP-3-25%\nSelfHash CP-1-25%\nLeftHash CP-1-10%\nLeftHash CP-3-10%\nSelfHash CP-1-10%\nSelfHash CP-3-10%\nSelfHash Dipper\nLeftHash Dipper\nSelfHash GPT\nLeftHash GPT\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAttacked ROC-AUC\nT=200\nT=170,190\nT=600\nT=500,300\nROC-AUC of Watermarks after Machine Attacks\nFigure 3: Watermark robustness to various automated text manipulations. Each bar denotes the ROC-AUC for\ndetecting the watermark in a text that has length T after the attack. Each bar label denotes the hashing scheme\n[SelfHash or LeftHash], and attack type [Copy-Paste (CP), paraphrase model (Dipper) and general-purpose\nmodel (GPT)]. For the Copy-Paste attack, we vary the number of watermarked passages inserted and their length.\nCP-3-10% denotes a copy-paste attack in which only 10% of the text is watermarked, and this watermarked\ntext is broken across 3 different locations in the document. In the Dipper and GPT attacks, the watermarked\ndocument is re-written in its entirety, resulting in paraphrases that are shorter than the original text. In this case,\nthe average length T is reported for both GPT and Dipper, respectively.\nWe use a single set of language model sampling parameters across all experiments, multinomial\nsampling at temperature 0.7, and for all experiments, unless explicitly stated, we use the LeftHash\nwatermark scheme based on an additive PRF with context window h = 1 and (\u03b3, \u03b4) = (0.25, 2.0).\nThis parameter combination was observed to be near the pareto frontier shown in Figure 2 of\nKirchenbauer et al. [2023], i.e. extremely detectable, but with marginal cost to generation quality.\nDue to the importance of text length in the performance of detection methods, including watermarking,\nwe carefully control and specify the generation lengths considered in each experiment first by limiting\nthe number of tokens the model can generate, and then sub-sampling the resulting data to just those\ngenerations which are within a specified range around a target length value. We use \u201cT\u201d to refer to\nthe number of tokens considered throughout all experimental sections, and unless otherwise noted\nwe include passages with length within \u00b125 tokens around that value. Unless otherwise stated, in\nall figures, ROC space plots and measurements are backed by > 500 positive and > 500 negative\nsamples, and other types of point estimates are also based on > 500 samples.\n4.1\nRobustness to Machine Paraphrasing Attacks\nWe run a series of paraphrasing attacks where we use a strong publicly available general-purpose\nlanguage model API to paraphrase the text. This is a departure from the threat model of Kirchenbauer\net al. [2023], who only characterize less capable models (T5). Our \u201cGPT\u201d paraphrase attack uses\ngpt-3.5-turbo, which is a version of the model powering ChatGPT, to rewrite the text. We also try\na specially tailored paraphrasing model - the 11B Dipper model introduced in Krishna et al. [2023].\nWe engineer the GPT prompt for optimal paraphrasing performance. Our explorations included\nprompts that explicitly instruct the LLM not to recycle bi-grams from the original text, although these\nresults are not reported here since they were not the best performing prompts. See the Appendix for\nan ablation on the performance of prompt variants. We note that when prompted for paraphrasing\nwith longer inputs, the GPT model often effectively summarizes the text, sometimes reducing its\nlength by more than 50% - which makes this an interesting challenge for the watermark.\nThe results for the main experiments attacking the watermark in this way are summarized in Figure 3.\nNote that we do not show the ROC-AUC numbers for the \u201cunattacked\u201d setting, because the detection\nperformance of both the LeftHash watermark and the SelfHash variant at these token lengths are\n7\n0\n100\n200\n300\n400\n500\n600\nTokens (T)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nROC-AUC\nDetection Rate of Watermarking\nas function of Tokens Observed\nLeftHash\nSelfHash\n0\n100\n200\n300\n400\n500\n600\nTokens (T)\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nROC-AUC\nDetection Rate of Watermarking after Attack\nas function of Tokens Observed\nSelfHash Dipper\nSelfHash GPT\nLeftHash Dipper\nLeftHash GPT\nSelfHash CP-3-25%\nLeftHash CP-3-25%\nLeftHash CP-1-25%\nSelfHash CP-1-25%\nSelfHash CP-3-10%\nSelfHash CP-1-10%\nLeftHash CP-3-10%\nLeftHash CP-1-10%\nFigure 4: AUC as a function of text length. (Left) AUC of original watermarked text with no attack. (Right)\nAUC under attack. Attacks dilute the watermark strength, but the watermark recovers its accuracy as increasingly\nmore tokens are observed. Due to the tendency of the GPT and Dipper paraphrasers to produce a shorter outputs\nthan they receive as inputs, we make the curves translucent starting at the mean sequence length of the original\ntext to indicate that these measurements are based on increasing fewer samples. For the Dipper attack this\nis \u223c 500 and for the GPT attack this is \u223c 300. In contrast, the Copy-Paste attack does not suffer from text\nshortening, and those sequences are still full length i.e. 600 \u00b1 25.\nalways > 0.999 for these experiments. Examining the settings where GPT or Dipper are the attack\ntype (the smaller groups of 4 bars) with token length before and after attack of roughly T = 200, we\nsee that the attack achieves a detection performance reduction of 0.05 \u2212 0.15 points AUC. When the\nlengths before attack are 600, despite the fact that Dipper and GPT reduce the lengths to 500 and 300\nrespectively, the success of the attack is now reduced to a loss of < 0.1 points AUC.\nThis dependence on the number of observed tokens is a fundamental property of the watermarking\nscheme and as such, we further investigate this dimension through the use of \u201cdetectability @\nT\u201d plots where we test prefixes of the generated sequences of lengths Ti \u2208 [1, ...T] and compute\nROC-AUC for each prefix length, visualizing the effect of the number of tokens observed on the\ndetection rate (shorthanded to AUC @ T). We also provide a version of these charts where we instead\nshow TP rates at low FPR in the appendix.\nUnder this lens of analysis, we observe in Figure 4 that in the unattacked setting, the AUC @ T\nquickly approaches its eventual value of \u223c 1.0. In the attacked setting, we see that the AUCs are\nreduced overall by all methods, but that despite how successful a paraphrasing attack might look\nat 200-300 tokens, by the 600 token mark, under the GPT and Dipper model-based attacks, the\nwatermark recovers to an AUC greater than 0.9.\n4.2\nRobustness to Copy-Paste Attacks\nWe devise a synthetic but realistic attack where we take watermarked text fragments and embed\nthem inside a surrounding un-watermarked, human-written document. This creates heterogeneous\nexamples where only a few sub-spans of the text contain the abnormally high number of green tokens\nindicative of watermarking. The attack method has two parameters, (1) the number of watermarked\nspan insertions and (2) the fraction of the resulting document that represents watermarked text.\nFor example, consider a passage with 10% watermarked tokens and 3 insertions. If the original\ntext is 1000 tokens, then this means that 3 watermarked spans of 33 tokens would be inserted into\nthe enclosing chunk of human text. We give this setting the short name \u201cCP-3-10%,\u201d which is an\nabbreviation of \u201cCopy-Paste with 3 spans at 10% watermarking.\u201d\nReturning to Figure 3, we see that with 25% watermark remaining, the copy-paste attack procedure\nhas a much stronger effect on the watermark than the other two machine based attacks, dropping the\nAUC to below 0.7 for 200 tokens and below 0.85 for 600 tokens. Examining Figure 4 we congruently\nsee that the watermark detectability grows more slowly than in the unattacked setting, however it\nstill grows steadily. We revisit this characteristic steady growth behavior in Section 4.4 where we\ncompare watermarking to alternative detection methods.\n4.3\nParaphrasing by Human Writers\nHumans may paraphrase watermarked text to better fit the style or tone of an existing passage or to\nbetter suit the tastes of the user. These modifications are a key part of every-day interactions that\nhuman users have with generated text, and to be reliable in-the-wild, a watermark should be robust to\n8\nFigure 5: The LabelStudio interface designed for the human paraphrasing and preference studies. Left: Interface\nfor paraphrase study. Right: Interface for human preference study.\nthese changes. Nonetheless, in a more adversarial setting, a human may also paraphrase text with the\ndeliberate goal of evading detection.\nTo reliably test the feasibility of evasion, we set up a human study. We recruit 14 experienced\nhuman writers (graduate students) who were presented with watermarked text. They were asked\nto paraphrase the text with the goal of removing the watermark while approximately maintaining\nthe original passage length and semantic content. In addition to compensation for participation,\ntop performers were awarded one of three $100 gift certificates to incentivize strong but faithful\nparaphrasing. The interface for this human study is shown in Figure 5. Each human writer is given a\ndifferent set of text passages, which we generate by prompting a watermarked version of Vicuna with\nthe LFQA data.\nWe first validate that all human writers successfully paraphrased the text passages using P-SP [Wieting\net al., 2022] scores. Doing so, we find P-SP scores far exceeding the threshold of 0.7 considered\nan expert paraphrase in Wieting et al. [2022]. We show this score for each writer in Figure 6 (left),\ncomparing P-SP directly to the z-score of detection based on their text.\nWe then analyze the detectabilty of the watermark via z-scores in the right plot of Figure 6 and\nin Figure 7. Here, we show watermark strength as a function of T, i.e. of tokens seen during\ndetection. While human writers are strong paraphrasers, exceeding both machine-based paraphrasers\nin performance, they cannot escape the two observations posed in Section 4. As shown in both Figure 6\n(right) and Figure 7 (bottom), eventually, the evidence for a watermark mounts and even human\nwriters are clearly detected on average after 800 tokens. Examining the individual performances in\nFigure 7, the only real exception in this study was human writer 13, who is a strong paraphraser\nand simultaneously did not submit enough text to be detected. More text from this writer would\nbe required to guarantee detection. On the other extreme, human writers 22 and 15 apparently\nparaphrased the text with a strategy that did not substantially affect the watermark, and are reliably\ndetected after only about 250 tokens, or 200 words.\n0\n2\n4\n6\n8\n10\n12\nZ-Score (\u2192)\n0.850\n0.875\n0.900\n0.925\n0.950\n0.975\nP-SP (\u2192)\nWatermark Strength (Z-Score) vs.\nSimilarity of Text Before and After Paraphrasing (P-SP)\nHuman 15\nHuman 22\nHuman 7\nHuman 16\nHuman 11\nHuman Avg\nHuman 10\nHuman 3\nHuman 21\nHuman 18\nHuman 12\nHuman 19\nHuman 17\nHuman 13\nHuman 24\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTokens (T)\n0\n2\n4\n6\n8\n10\nZ-Score\nZ-Score of Watermarked Text after Paraphrasing\nas function of Tokens Observed\nDipper\nGPT\nAverage Human\nFigure 6: (Left) Paraphrase quality for human writers as measured by P-SP. Note the y-axis. (Right) Summary\nresults for each paraphrasing attack in aggregate, showing that human writers are stronger paraphrasers than\nboth machine attacks. Yet, all attacks can be detected with certainty after 400 to 800 tokens.\n4.4\nA Comparative Study of Detection Algorithms\nA number of alternatives to watermarks exist. As discussed in Section 2, other paradigms for LLM\ndetection are post-hoc detectors based on statistical features, black-box learned detectors, and retrieval\nsystems. Based on previous work in Mitchell et al. [2023], we study DetectGPT as a representative for\n9\n0\n250\n500\n750\n1000\n1250\n1500\n1750\nTokens (T)\n0\n10\n20\n30\nZ-Score\nZ-Score of Watermarked Text\nas function of Tokens Observed\nHuman 17\nHuman Avg\nHuman 16\nHuman 10\nHuman 12\nHuman 7\nHuman 18\nHuman 11\nHuman 15\nHuman 3\nHuman 19\nHuman 21\nHuman 13\nHuman 22\nHuman 24\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\nTokens (T)\n\u22122.5\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nZ-Score\nZ-Score of Watermarked Text after Paraphrasing\nas function of Tokens Observed\nHuman 15\nHuman 22\nHuman Avg\nHuman 7\nHuman 11\nHuman 16\nHuman 21\nHuman 3\nHuman 17\nHuman 18\nHuman 12\nHuman 10\nHuman 19\nHuman 13\nHuman 24\nFigure 7: z-score as a function of T over all text passages given to each writer, separated per writer. (Top)\nOriginal scores for the combined text given to each human writer. (Bottom) Average scores per writer after\nparaphrasing. We find that almost all human writers are detected with exceeding certainty, after about 800 tokens,\ni.e. about 500 words (a z-score of 4 implies a P-value of 3.2e\u22125). Note that despite their strong paraphrasing\ncapability (as shown in the left plot of Figure 6), Human 24 paraphrased too few examples for a fair competitive\ncomparison with the other annotators, but they are still included as part of the averages.\npost-hoc detectors, which has been shown to outperform other existing learned detectors. We use the\nretrieval system put forth in Krishna et al. [2023] as representative for the paradigm of retrieval-based\ndetection.\nRetrieval: The retrieval system of Krishna et al. [2023] requires the creation and maintenance\nof a comprehensive database of all sequences generated previously by the language model. By\nleveraging semantic similarity as a retrieval mechanism, such as SP [Wieting et al., 2022] or BM25\n[Robertson et al., 1994], this approach aims to identify and map paraphrased generations to their\noriginal, uncorrupted source. In our experiments, we adopt the retrieval method as described by\n[Krishna et al., 2023], and utilize the BM25 search method as it performed better in their evaluation.\nWhile we include further details in the Appendix, a key detail concerning this method is how we\nconstruct copy-paste examples to evaluate it on. The \u201cunattacked\u201d text is the output of the language\nmodel without any modification and this is what is loaded into the retrieval database as the \u201cgeneration\nhistory\u201d of the model. The copy-paste attacked version of the text is created by inserting a sub-string\n10\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nDipper\nZ-Score\nWinMax\nRetrieval\nDetectGPT\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nCP-1-25%\nZ-Score\nWinMax\nRetrieval\nDetectGPT\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nCP-1-10%\nZ-Score\nWinMax\nRetrieval\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nGPT\nZ-Score\nWinMax\nRetrieval\nDetectGPT\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nCP-3-25%\nZ-Score\nWinMax\nRetrieval\nDetectGPT\n0.00\n0.25\n0.50\n0.75\n1.00\nFPR\n0.00\n0.25\n0.50\n0.75\n1.00\nTPR\nCP-3-10%\nZ-Score\nWinMax\nRetrieval\nFigure 8: ROC charts for various machine attack types for a text passage with length before attack of T = 1000\nFor Dipper and GPT, the length after attack is decreased to 800 and 300 respectively. z-score and WinMax\ndenote the two watermark detectors, and retrieval isKrishna et al. [2023]. From left to right: In the first\ncolumn we show the two machine paraphrase attacks, in the center column, we show the copy-paste attack\nwith a remaining detectable text percentage of 25% spread over 1 and 3 segments, and in the final column\nthe copy-paste attack at a more difficult to detect 10% remaining detectable text. We find DetectGPT to be\ncatastrophically unreliable under all types of attack. While retrieval is quite robust to the machine paraphrase\nattacks, watermark based detection outperforms it in the copy-paste setting, with the WinMax variant presenting\nmuch stronger performance under the most severe copy-paste attack in the bottom right subfigure.\nfrom that text into another piece of machine-generated completion text readily available for this\nprompt, the watermarked generation. We discuss this choice and other details in the appendix.\nDetectGPT: DetectGPT [Mitchell et al., 2023] is a zero-shot post-hoc method for detecting machine-\ngenerated texts. It employs a curvature-based criterion that compares the log probabilities between\na candidate passage and its minor perturbations. The intuition behind DetectGPT is that machine-\ngenerated texts tend to dominate the negative curvature regions of an LM\u2019s log probability curve. We\nuse the official implementation of DetectGPT and follow their default setting by using T5-3B [Raffel\net al., 2020] as the mask-filling model for generating perturbations. We adopt the strongest setting of\nDetectGPT by generating 100 perturbations for each test sample and using the normalized perturbation\ndiscrepancy as the criterion.\nFurther details regarding the adaption of the method are included in the appendix, but the key\nexperimental detail is that the positive examples for detection are the unwatermarked model outputs\ngenerated from each prompt, and as with watermarking detection, the human gold completions are\nthe negative examples.\nWhich method is most reliable? After introducing these three approaches, we attack them with the\nbattery of machine attacks described in the previous section, using GPT, Dipper, and the copy-paste\nattack for evaluation. Under attack, we plot ROC charts for each detection scheme in Figure 8, when\neach method is given a text passage of length T = 1000 for detection.\nWe find that retrieval and watermarking are both decent detection methods, and significantly more\nreliable that post-hoc detectors (of which DetectGPT is already the strongest). Yet, while retrieval\nequals or outperforms watermarking on the GPT and Dipper model based paraphrases (corroborating\nthe results in Krishna et al. [2023]), it is bested by watermarking in this novel copy-paste setting.\nThe Sample Complexity of Various Detection Schemes. To hone in on the observed differences\nbetween the schemes under each attack setting, we also compare all three approaches in terms of\nAUC @ T, i.e. in terms of detection performance again as a function of text quantity. This view is\nshown in Figure 9. We note the details of how Retrieval and DetectGPT were evaluated at varied\nvalues of T in the appendix.\nHere, we start to develop an explanation for why watermarking outperforms retrieval in this specific\nsetting by observing that scaling behaviors for each detection method differ starkly under attack.\n11\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after Dipper Attack\nas function of Tokens Observed\nRetrieval\nZ-Score\nWinMax\nDetectGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after GPT Attack\nas function of Tokens Observed\nZ-Score\nWinMax\nRetrieval\nDetectGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after CP-1-25% Attack\nas function of Tokens Observed\nWinMax\nZ-Score\nRetrieval\nDetectGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after CP-3-25% Attack\nas function of Tokens Observed\nWinMax\nZ-Score\nRetrieval\nDetectGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after CP-1-10% Attack\nas function of Tokens Observed\nWinMax\nZ-Score\nRetrieval\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nROC-AUC\nDetection Rate after CP-3-10% Attack\nas function of Tokens Observed\nWinMax\nZ-Score\nRetrieval\nFigure 9: AUC at T for various types of attack. (Top Row) The Dipper and GPT machine paraphrasing\nattacks. (Center Row) The copy-paste attack with a remaining detectable text percentage of 25% spread over\n1 and 3 segments. (Bottom Row) The copy-paste attack at only 10% remaining detectable text. While all\nschemes scale well with the number of tokens observed when no attack is present, DetectGPT scales poorly\nunder attack. Retrieval shows a positive trend in detectability as a function of T, but is non-monotonic, whereas\nwatermarking steadily improves in power for all attacks evaluated. Due to the tendency of the GPT and Dipper\nparaphrase models to produce a shorter sequence of tokens as output than they receive as input, we make the\ncurves translucent starting at the mean sequence length of the attacked set after generation to indicate that these\nmeasurements are based on increasing fewer samples and are therefore more uncertain. For DetectGPT, the\nlast measurement for each attack type is just computed on the set of full sequences, and so we plot the result\nat that average T value. For the Dipper attack this is \u223c 800 and for the GPT attack this is \u223c 300. Due to the\nsynthetic nature of the Copy-Paste attack, after attack, those sequences are still full length i.e. 1000 \u00b1 25.\nWhile all approaches scale appropriately in strength with the length of text when the generated text\nis not that heavily attacked, under strong attacks, only watermarking continues to improve reliably,\nas predicted in the introduction of Section 4.\nIn particular, when only a small fraction of text (25% or 10%) remains under the copy-paste attack,\nnon-watermarking methods struggle as text length increases. For the Retrieval method, as T increases,\nthe fraction of the original text (the unattacked positive example) that remains, decreases. Therefore,\nthe similarity to the original example continues to drop, which eventually causes a decrease in\nperformance for both copy-paste attack severities. For DetectGPT, the same effect is observed but\nat a more drastic level. We investigate this behavior in more detail in Appendix A.8 by examining\ntrends in the actual retrieval and detection scores produced under each attack setting for each method.\n12\n4.5\nWhat about the White-Box Setting?\nIn this work we have focused on the black-box setting, where text is modified in plausible use cases\nfor machine-generated text and the secret key of the watermarking scheme is unknown to the party\nmodifying the text through paraphrasing or other editing.\nOnce this assumption is relaxed, for example if the secret key is breached or the watermark is public,\nthen an attacker with white-box access to a strong paraphrasing model like Dipper [Krishna et al.,\n2023] could break the watermark in the following way: The attacker can apply an anti-watermark\nscheme during generation with the paraphrasing model, where instead of adding \u03b4 in Equation (1)\nto logit outputs, \u03b4 is instead subtracted. The attacker can further keep track of the current score\nof green-listed versus red-listed tokens and can accordingly modify this negative \u03b4 on-the-fly to\nguarantee that of the T tokens of text generated by the paraphrasing model exactly \u03b3T are colored\ngreen, and no watermark signal is leaked into the paraphrased text.\nThis attack relies on both white-box access to the watermark key and availability of a strong para-\nphrasing model. To bypass this difficult threat model, the watermark context width h has to be\nchosen sufficiently large so that in an adversarial scenario, the watermark key could not be easily\ndiscovered (or alternatively, sufficiently many keys must be employed simultaneously, as discussed in\nKirchenbauer et al. [2023]). Nevertheless, not all watermarking use cases are necessarily adversarial,\nand we strongly believe that in benign cases, even a watermark with imperfect resistance to text\ncorruption is still very valuable. Documenting the usage of machine-generated text overall, for\nexample either to trace the spread of generated text in hindsight, or to remove generated text from\nfuture training runs [Radford et al., 2022, Shumailov et al., 2023], can provide a baseline effort to\n\u201cfuture-proof\u201d popular generative models.\n5\nRelationship to theoretical results on (im)possibility of detection\nSeveral works have investigated the difficulty of detecting language models from a theoretical\n[Varshney et al., 2020, Sadasivan et al., 2023, Chakraborty et al., 2023] and practical [Bhat and\nParthasarathy, 2020, Wolff and Wolff, 2022, Tang et al., 2023] perspective, although these works\nmostly pertain to post-hoc detectors. How does this body of work relate to our empirical evidence on\nthe reliability of watermarking?\nIn their work on the impossibility of LLM watermarks and detection, Sadasivan et al. [2023] assume\nthe goal of detection is to discern text generated by a large language model from text generated by a\nrandomly sampled human from some population, for example tweets from Twitter users. Sadasivan\net al. [2023] also assume that the goal of large language model training is to mimic such a human\nlanguage distribution. If the LLM perfectly mimics this distribution, then its samples will be\nindiscernible from the human generations. To the extent that the LLM imperfectly mimics the human\ndistribution, Chakraborty et al. [2023] prove that the distribution shift between the language model\nand human-written text can be detected, given a sufficient number of samples. If an LLM instead\nfollowed a more concentrated distribution over text, for example by routinely speaking in the voice\nof a particular individual rather than a randomly sampled one, then its samples would be unlikely\nunder the generic human distribution and would therefore be detectable.\nExisting literature suggests that LLMs trained with standard methods do not mimic randomly sampled\nindividuals \u2013 the standard classification loss used to train LLMs is known to reward a low entropy\noutput distribution that is distinct from typical human text [Holtzman et al., 2019, Gehrmann et al.,\n2019]. Furthermore, benign users, and by extension companies that sell LLMs as a service, seldom\nwant an LLM to mimic a generic human language distribution, and may instead prefer inhumanly\nunderstandable and factual text, or text in a polished professional voice.\nRegardless of whether future LLMs mimic a human distribution, watermarking is still possible.\nConsider that a generic human language distribution is incredibly diffuse, with many valid completions\nfor a single prompt. For example, different mathematicians write unique yet correct proofs of the\nsame theorem, even if their logical steps are identical, indicating that even tasks with seemingly\nnarrow answers, like math, may actually still admit diffuse human language distributions.\nThe high entropy over completions enables watermarks to concentrate model outputs on a subset\nof valid completions while still spreading mass across diverse and high-quality candidates. In fact,\nwatermarks which only make very small changes to the generative distribution can be detected with\n13\nenough samples, as proved theoretically in Chakraborty et al. [2023] and verified empirically in our\nown work. Moreover, the benefit of watermarking is that we can minimally change the generative\ndistribution in a way that optimizes detectability.\nSadasivan et al. [2023] also state that, in principle, a watermark can be removed by a paraphraser\nthat samples from the set of text with the same content. Such theoretically optimal paraphrasers have\nso far not been demonstrated. Our experiments show that even stronger models (chatGPT) may be\ninsufficient for paraphrasing weaker models (LLaMA-7B), demonstrating that watermark detection is\npossible when contending with the paraphrasers that exist today.\nUltimately, the theory literature discussing differences between language distributions does not get at\nthe core of the detection problem. Existing post-hoc detectors do not fail because the distributions of\nhuman-written and machine-generated text are so similar, but instead because we lack a mathematical\ncharacterization of their differences [Liang et al., 2023, Krishna et al., 2023]. Consider, for example,\nan LLM with a pseudo-random sampler and a fixed seed. Each token is a deterministic function of\nthose that came before it. The output distribution is concentrated on a single example conditional on\neach prompt, making it very different from a human distribution. Yet, without white-box model access,\ndetection is still difficult in practice because we know neither the location of this peak for the LLM\nnor the distribution of human text. In contrast, watermarking is effective not because the distributional\nshift it induces is large, but because this shift is characterized by a simple rule. The human-written\nand machine-generated text distributions were likely very far apart before watermarking, but a\ncharacterization of the differences is needed for detection.\n6\nConclusions\nThrough a comprehensive empirical investigation, including strong machine paraphrasing and human\nwriters, we evaluate the reliability of watermarks as a mechanism for the documentation and detection\nof machine-generated text. We advocate for a view of watermarking reliability as a function of text\nlength, and find that even human writers cannot reliably remove watermarks if being measured at\n1000 words, despite having the goal of removing the watermark. This view of watermark reliability\nas a function of text length turns out to be a strong property of watermarking. When comparing to\nother paradigms, such as retrieval and loss-based detection, we do not find a strong improvement with\ntext length, making watermarking out to be the most reliable approach in our study. This reliability\nis a consequence of the detector relying on a null hypothesis that humans consistently adhere to,\nindependent of text length, and hence produces a rigorous and interpretable P-value that the user can\nleverage to control the false positive rate.\n7\nAcknowledgements\nThis work was made possible by the ONR MURI program, DARPA GARD (HR00112020007), the\nOffice of Naval Research (N000142112557), and the AFOSR MURI program. Commercial support\nwas provided by Capital One Bank, the Amazon Research Award program, and Open Philanthropy.\nFurther support was provided by the National Science Foundation (IIS-2212182), and by the NSF\nTRAILS Institute (2229885).\nReferences\nSahar Abdelnabi and Mario Fritz. Adversarial Watermarking Transformer: Towards Tracing Text\nProvenance with Data Hiding. In 2021 IEEE Symposium on Security and Privacy (SP), pages\n121\u2013140, May 2021. doi: 10.1109/SP40001.2021.00083.\nMikhail J. Atallah, Victor Raskin, Michael Crogan, Christian Hempelmann, Florian Kerschbaum,\nDina Mohamed, and Sanket Naik. Natural Language Watermarking: Design, Analysis, and a Proof-\nof-Concept Implementation. In Ira S. Moskowitz, editor, Information Hiding, Lecture Notes in\nComputer Science, pages 185\u2013200, Berlin, Heidelberg, 2001. Springer. ISBN 978-3-540-45496-0.\ndoi: 10.1007/3-540-45496-9_14.\nSameer Badaskar, Sachin Agarwal, and Shilpa Arora. Identifying real or fake articles: Towards\nbetter language modeling. In Proceedings of the Third International Joint Conference on Natural\nLanguage Processing: Volume-II, 2008.\n14\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian Deng, Marc\u2019Aurelio Ranzato, and Arthur Szlam.\nReal or fake? learning to discriminate machine from human generated text. arXiv preprint\narXiv:1906.03351, 2019.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\nDangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency, FAccT \u201921, pages 610\u2013623, New\nYork, NY, USA, March 2021. Association for Computing Machinery. ISBN 978-1-4503-8309-7.\ndoi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922.\nDaria Beresneva. Computer-generated text detection using machine learning: A systematic review.\nIn 21st International Conference on Applications of Natural Language to Information Systems,\nNLDB, pages 421\u2013426. Springer, 2016.\nMeghana Moorthy Bhat and Srinivasan Parthasarathy. How Effectively Can Machines Defend\nAgainst Machine-Generated Fake News? An Empirical Study. In Proceedings of the First\nWorkshop on Insights from Negative Results in NLP, pages 48\u201353, Online, November 2020.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2020.insights-1.7. URL https:\n//aclanthology.org/2020.insights-1.7.\nJames V. Bradley. Distribution-free Statistical Tests. Fort Belvoir, VA, August 1960. Defense\nTechnical Information Center. doi: 10.21236/AD0249268. URL http://www.dtic.mil/docs/\ncitations/AD0249268.\nJack T Brassil, Steven Low, Nicholas F Maxemchuk, and Lawrence O\u2019Gorman. Electronic marking\nand identification techniques to discourage document copying. IEEE Journal on Selected Areas in\nCommunications, 13(8):1495\u20131504, 1995.\nSouradip Chakraborty, Amrit Singh Bedi, Sicheng Zhu, Bang An, Dinesh Manocha, and Furong\nHuang. On the Possibilities of AI-Generated Text Detection. arxiv:2304.04736[cs], April 2023.\ndoi: 10.48550/arXiv.2304.04736. URL http://arxiv.org/abs/2304.04736.\nYuei-Lin Chiang, Lu-Ping Chang, Wen-Tai Hsieh, and Wen-Chih Chen. Natural Language Water-\nmarking Using Semantic Substitution for Chinese Text. In Ton Kalker, Ingemar Cox, and Yong Man\nRo, editors, Digital Watermarking, Lecture Notes in Computer Science, pages 129\u2013140, Berlin,\nHeidelberg, 2004. Springer. ISBN 978-3-540-24624-4. doi: 10.1007/978-3-540-24624-4_10.\nWilliam G Cochran. The \u03c72 test of goodness of fit. The Annals of mathematical statistics, pages\n315\u2013345, 1952.\nEvan Crothers, Nathalie Japkowicz, and Herna Viktor. Machine Generated Text: A Comprehensive\nSurvey of Threat Models and Detection Methods. arxiv:2210.07321[cs], November 2022. doi:\n10.48550/arXiv.2210.07321. URL http://arxiv.org/abs/2210.07321.\nFalcon Dai and Zheng Cai. Towards Near-imperceptible Steganographic Text. In Proceedings of\nthe 57th Annual Meeting of the Association for Computational Linguistics, pages 4303\u20134308,\nFlorence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1422.\nURL https://aclanthology.org/P19-1422.\nTiziano Fagni, Fabrizio Falchi, Marco Gambini, Andrea Martella, and Maurizio Tesconi. Tweepfake:\nAbout detecting deepfake tweets. arXiv preprint arXiv:2008.00036, 2020.\nTina Fang, Martin Jaggi, and Katerina Argyraki. Generating Steganographic Text with LSTMs.\nIn Proceedings of ACL 2017, Student Research Workshop, pages 100\u2013106, Vancouver, Canada,\nJuly 2017. Association for Computational Linguistics. URL https://aclanthology.org/\nP17-3017.\nLeon Fr\u00f6hling and Arkaitz Zubiaga. Feature-based detection of automated language models: tackling\ngpt-2, gpt-3 and grover. PeerJ Computer Science, 7:e443, 2021. doi: 10.7717/peerj-cs.443.\nMatthias Gall\u00e9, Jos Rozen, Germ\u00e1n Kruszewski, and Hady Elsahar. Unsupervised and distributional\ndetection of machine-generated text. arXiv preprint arXiv:2111.02878, 2021.\n15\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800GB\nDataset of Diverse Text for Language Modeling. arXiv:2101.00027 [cs], December 2020. URL\nhttp://arxiv.org/abs/2101.00027.\nSebastian Gehrmann, Hendrik Strobelt, and Alexander M Rush. Gltr: Statistical detection and\nvisualization of generated text. arXiv preprint arXiv:1906.04043, 2019.\nEA Grechnikov, GG Gusev, AA Kustarev, and AM Raigorodsky. Detection of artificial texts. In\nRCDL2009 Proceedings, pages 306\u2013308, 2009.\nAlexei Grinbaum and Laurynas Adomaitis. The Ethical Need for Watermarks in Machine-Generated\nLanguage. arxiv:2209.03118[cs], September 2022. doi: 10.48550/arXiv.2209.03118. URL\nhttp://arxiv.org/abs/2209.03118.\nXuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu, and Chenguang Wang. Protecting Intellectual\nProperty of Language Generation APIs with Lexical Watermark. In Proceedings of the AAAI\nConference on Artificial Intelligence, volume 36, pages 10758\u201310766, June 2022a. doi: 10.1609/\naaai.v36i10.21321. URL https://ojs.aaai.org/index.php/AAAI/article/view/21321.\nXuanli He, Qiongkai Xu, Yi Zeng, Lingjuan Lyu, Fangzhao Wu, Jiwei Li, and Ruoxi Jia. CATER:\nIntellectual Property Protection on Text Generation APIs via Conditional Watermarks. In Advances\nin Neural Information Processing Systems, October 2022b. URL https://openreview.net/\nforum?id=L7P3IvsoUXY.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\nGanesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. Automatic Detection of\nMachine Generated Text: A Critical Survey. In Proceedings of the 28th International Conference\non Computational Linguistics, pages 2296\u20132309, Barcelona, Spain (Online), December 2020.\nInternational Committee on Computational Linguistics. doi: 10.18653/v1/2020.coling-main.208.\nURL https://aclanthology.org/2020.coling-main.208.\nGabriel Kaptchuk, Tushar M. Jois, Matthew Green, and Aviel Rubin. Meteor: Cryptographically\nSecure Steganography for Realistic Distributions, 2021. URL https://eprint.iacr.org/\n2021/686.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A\nWatermark for Large Language Models. arxiv:2301.10226[cs], January 2023. doi: 10.48550/\narXiv.2301.10226. URL http://arxiv.org/abs/2301.10226.\nKalpesh Krishna, Yixiao Song, Marzena Karpinska, John Wieting, and Mohit Iyyer. Paraphrasing\nevades detectors of AI-generated text, but retrieval is an effective defense. arxiv:2303.13408[cs],\nMarch 2023. doi: 10.48550/arXiv.2303.13408. URL http://arxiv.org/abs/2303.13408.\nThomas Lavergne, Tanguy Urvoy, and Fran\u00e7ois Yvon. Detecting fake content with relative entropy\nscoring. PAN, 8:27\u201331, 2008.\nTaehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin,\nand Gunhee Kim. Who wrote this code? watermarking for code generation. arXiv preprint\narXiv:2305.15060, 2023.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive decoding: Open-ended text generation as optimization.\narXiv preprint arXiv:2210.15097, 2022a.\nXiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke\nZettlemoyer, and Mike Lewis. Contrastive Decoding: Open-ended Text Generation as Optimization.\narxiv:2210.15097[cs], October 2022b. doi: 10.48550/arXiv.2210.15097. URL http://arxiv.\norg/abs/2210.15097.\n16\nWeixin Liang, Mert Yuksekgonul, Yining Mao, Eric Wu, and James Zou. GPT detectors are biased\nagainst non-native English writers. arxiv:2304.02819[cs], April 2023. doi: 10.48550/arXiv.2304.\n02819. URL http://arxiv.org/abs/2304.02819.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels, 2016.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky, Christopher D. Manning, and Chelsea Finn.\nDetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. January\n2023. doi: 10.48550/arXiv.2301.11305. URL https://arxiv.org/abs/2301.11305v1.\nOpenAI. Gpt-2: 1.5b release. https://openai.com/research/gpt-2-1-5b-release/, 2019.\nAccessed on 15th May 2023.\nAnnie\nPalmer.\nPeople\nare\nusing\nA.I.\nchatbots\nto\nwrite\nAmazon\nre-\nviews.\nCNBC,\nApril\n2023.\nURL\nhttps://www.cnbc.com/2023/04/25/\namazon-reviews-are-being-written-by-ai-chatbots.html.\nKrishna Pillutla, Swabha Swayamdipta, Rowan Zellers, John Thickstun, Sean Welleck, Yejin Choi,\nand Zaid Harchaoui. MAUVE: Measuring the Gap Between Neural Text and Human Text using\nDivergence Frontiers. In Advances in Neural Information Processing Systems, volume 34, pages\n4816\u20134828. Curran Associates, Inc., 2021. URL https://proceedings.neurips.cc/paper/\n2021/hash/260c2432a0eecc28ce03c10dadc078a4-Abstract.html.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust Speech Recognition via Large-Scale Weak Supervision. arxiv:2212.04356[cs, eess],\nDecember 2022. doi: 10.48550/arXiv.2212.04356. URL http://arxiv.org/abs/2212.04356.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Unified Text-to-\nText Transformer. arXiv:1910.10683 [cs, stat], July 2020. URL http://arxiv.org/abs/1910.\n10683.\nStephen E. Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, and Mike Gatford.\nOkapi at trec-3. In Text Retrieval Conference, 1994.\nJuan Rodriguez, Todd Hay, David Gros, Zain Shamsi, and Ravi Srinivasan. Cross-domain detection\nof gpt-2-generated technical text. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pages\n1213\u20131233, Seattle, United States, 2022. Association for Computational Linguistics.\nVinu Sankar Sadasivan, Aounon Kumar, Sriram Balasubramanian, Wenxiao Wang, and Soheil\nFeizi. Can AI-Generated Text be Reliably Detected? arxiv:2303.11156[cs], March 2023. doi:\n10.48550/arXiv.2303.11156. URL http://arxiv.org/abs/2303.11156.\nBruce Schneier. Using AI to Scale Spear Phishing, August 2021. URL https://www.schneier.\ncom/blog/archives/2021/08/using-ai-to-scale-spear-phishing.html.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross Anderson.\nThe Curse of Recursion: Training on Generated Data Makes Models Forget. arxiv:2305.17493[cs],\nMay 2023. doi: 10.48550/arXiv.2305.17493. URL http://arxiv.org/abs/2305.17493.\nIrene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss, Jeff Wu, Alec\nRadford, Gretchen Krueger, Jong Wook Kim, Sarah Kreps, et al. Release strategies and the social\nimpacts of language models. arXiv preprint arXiv:1908.09203, pages 1\u201346, 2019.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Lingpeng Kong, and Nigel Collier. A Contrastive\nFramework for Neural Text Generation. In Advances in Neural Information Processing Systems,\nOctober 2022. URL https://openreview.net/forum?id=V88BafmH9Pj.\nRuixiang Tang, Yu-Neng Chuang, and Xia Hu. The Science of Detecting LLM-Generated Texts.\narxiv:2303.07205[cs], March 2023. doi: 10.48550/arXiv.2303.07205. URL http://arxiv.org/\nabs/2303.07205.\n17\nEdward Tian. Gptzero update v1, January 2023. URL https://gptzero.substack.com/p/\ngptzero-update-v1.\nMaxim Tkachenko, Mikhail Malyuk, Andrey Holmanyuk, and Nikolai Liubimov. Label Studio: Data\nlabeling software, 2020-2022.\nURL https://github.com/heartexlabs/label-studio.\nOpen source software available from https://github.com/heartexlabs/label-studio.\nUmut Topkara, Mercan Topkara, and Mikhail J. Atallah. The hiding virtues of ambiguity: Quantifiably\nresilient watermarking of natural language text through synonym substitutions. In Proceedings of\nthe 8th Workshop on Multimedia and Security, MM&amp;Sec \u201906, pages 164\u2013174, New York, NY,\nUSA, September 2006. Association for Computing Machinery. ISBN 978-1-59593-493-2. doi:\n10.1145/1161366.1161397. URL https://doi.org/10.1145/1161366.1161397.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand\nJoulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971, 2023.\nLav R. Varshney, Nitish Shirish Keskar, and Richard Socher. Limits of Detecting Text Generated by\nLarge-Scale Language Models. In 2020 Information Theory and Applications Workshop (ITA),\npages 1\u20135, February 2020. doi: 10.1109/ITA50056.2020.9245012.\nAshish Venugopal, Jakob Uszkoreit, David Talbot, Franz Och, and Juri Ganitkevitch. Watermarking\nthe Outputs of Structured Prediction with an application in Statistical Machine Translation. In\nProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages\n1363\u20131372, Edinburgh, Scotland, UK., July 2011. Association for Computational Linguistics.\nURL https://aclanthology.org/D11-1126.\nVicuna-Team. Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality,\nMarch 2023. URL https://lmsys.org/blog/2023-03-30-vicuna.\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural\ntext generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019.\nJohn Wieting, Kevin Gimpel, Graham Neubig, and Taylor Berg-kirkpatrick. Paraphrastic Repre-\nsentations at Scale. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing: System Demonstrations, pages 379\u2013388, Abu Dhabi, UAE, December\n2022. Association for Computational Linguistics. URL https://aclanthology.org/2022.\nemnlp-demos.38.\nMax Wolff and Stuart Wolff. Attacking Neural Text Detectors. arxiv:2002.11768[cs], January 2022.\ndoi: 10.48550/arXiv.2002.11768. URL http://arxiv.org/abs/2002.11768.\nClaire Woodcock. AI Is Tearing Wikipedia Apart, May 2023. URL https://www.vice.com/en/\narticle/v7bdba/ai-is-tearing-wikipedia-apart.\nKiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak. Robust natural language watermarking\nthrough invariant features. arXiv preprint arXiv:2305.01904, 2023.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt\nShuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer.\nOPT: Open Pre-trained Transformer Language Models. arxiv:2205.01068[cs], May 2022. doi:\n10.48550/arXiv.2205.01068. URL http://arxiv.org/abs/2205.01068.\nWanjun Zhong, Duyu Tang, Zenan Xu, Ruize Wang, Nan Duan, Ming Zhou, Jiahai Wang, and\nJian Yin. Neural deepfake detection with factual structure of text. In Proceedings of the 2020\nConference on Empirical Methods in Natural Language Processing (EMNLP), pages 2461\u20132470,\nMinneapolis, Minnesota, 2020. Association for Computational Linguistics.\nZachary Ziegler, Yuntian Deng, and Alexander Rush. Neural Linguistic Steganography. In Pro-\nceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages\n1210\u20131215, Hong Kong, China, November 2019. Association for Computational Linguistics. doi:\n10.18653/v1/D19-1115. URL https://aclanthology.org/D19-1115.\n18\nAppendix\u2013Table of Contents\nA\nAppendix\nWe provide a number of extended sets of visualizations and ablation studies to supplement the results\nshown in the main body of the work as well as more methodological and experimental details. Below\nis a table of contents to help navigate the various subsections.\nTable of Contents\n1. Utilizing Better Quality Metrics\n2. Hashing Scheme Extended Ablation\n3. Datasets and Models Ablation\n4. GPT Attack Prompt Ablation\n5. Watermark Detector Ablation\n6. Detectability of Watermarks after Machine Paraphrasing: TPR @ T\n7. Experimental Methodology Details\n8. Detection Method Comparison: Detector Scores @ T\n9. Human Study Details and Preference Evaluation\n10. Code Details and Release Statement\n11. Hardware and Compute Details\n19\nAppendix\u2013Table of Contents\nA.1\nUtilizing Better Quality Metrics\nTo more accurately examine the effects of watermarking, in addition to utilizing a stronger generative\nmodel from the llama family [Touvron et al., 2023], we employ a pair of metrics designed to capture\ndifferent aspects of generation quality. Given the fraction un of unique n-grams in a sequence of text,\nwe define text diversity up to order N via\ndiversity = \u2212 log\n \n1 \u2212\nN\nY\nn=1\n(1 \u2212 un)\n!\n,\n(4)\nto represent a view on n-gram repetition metrics described in Welleck et al. [2019] and Li et al.\n[2022a] in a more readable format. A higher diversity score represents a more diverse text, where\nfewer n-grams are repeated.\nTo estimate whether watermarked text drifts away from un-watermarked model generations, we adopt\nthe same evaluation metric as Krishna et al. [2023] to measure paraphrase similarity: P-SP [Wieting\net al., 2022]. We measure similarity between different sets of text pairs such as un-watermarked and\nwatermarked outputs, or the human gold completion to a prompt versus the watermarked completion\ngenerated by the model. Further, we evaluate human annotator judgement of un-watermarked versus\nwatermarked text quality (Table 2).\nWe also considered other metrics for language model sampling quality such as MAUVE [Pillutla et al.,\n2021] and coherence as described in [Su et al., 2022, Li et al., 2022b]. However, the insight those\nmeasures provided when comparing outputs under various generation and watermarking settings was\ngenerally subsumed by what the P-SP metric revealed, so we chose to simplify presentation using\njust P-SP as our semantic similarity metric across all relevant visualizations. Aside from the study\nof watermarks, we note that output quality evaluation in open ended generation settings is still a\nresearch area with many open questions.\n20\nAppendix\u2013Table of Contents\nAlgorithm 1 Generalized SelfHash Watermark\nInput: Context xh, . . . x1, vocabulary V , arbitrary text generation scheme S, LLM logits l\nWatermark hyperparameters \u03b3 \u2208 (0, 1), \u03b4 > 0, f, h > 0, integer hash P\nG = \u2205\n\u25b7 Initialize empty set of green-listed tokens\nfor k = 1, . . . , |V | do\nHk = f(x)P(k)\n\u25b7 Compute k-th key\nGk = RandPermHk(V )[: \u03b3|V |]\n\u25b7 Temp. green list Gk seeded with Hk\nif k \u2208 Gk then\nG \u2190 G \u222a {k}\n\u25b7 Include k in final green list if self-consistent\nend if\nend for\nlk \u2190\n\u001alk + \u03b4\nif\nk \u2208 G\nlk\notherwise\nSample a new token x0 from modified logits l using sampling scheme S.\nA.2\nHashing Scheme Extended Ablation\nIn this section, we complete our extensive study of watermark hyperparameters by presenting a\nrepresentative selection of settings varying different components of the hashing scheme to explore\ndifferent parts of the pareto space between watermark strength and text quality. We further detail our\nproposed extension of the SelfHash algorithm to arbitrary sampling schemes and pseudorandom\nfunctions f in Algorithm 1.\nWhen using greedy decoding, the scheme in Algorithm 1 covers the scheme denoted as Alg.3. in\nKirchenbauer et al. [2023]. We also note that in practice, iterating over all k \u2208 |V | is not strictly\nnecessary. We only iterate over the 40 indices k with the largest logit score lk and skip all others, for\nwhich the addition of \u03b4 is unlikely to make an impact on the final probability distribution.\nWe further note that the choice to set Hk = f(x)P(k) instead of of Hk = f([x, k]) in Algorithm 1 is\nnot strictly necessary. We refer to the first choice as anchoring and ablate it separately in Figure 12,\nwhere Hk = f([x, k]) denotes the un-anchored approach. On all other occasions the SelfHash\nscheme is always anchored as described in Algorithm 1.\nIn Figure 10, to vary the strength of the watermark, we test a selection of \u03b3 and \u03b4 values in\n{0.5, 0.25, 0.1} and {1.0, 2.0, 4.0} respectively. These values give us 9 settings representing both\nweak and strong watermarks that yield a series of increasing z-scores across the x-axis for each\nseeding scheme. All points in these charts are averages computed on \u223c 500 samples with token\nlength T = 200 \u00b1 25.\nWe observe that as the watermark is made stronger (resulting in higher z-scores) certain schemes trend\npositively with respect to n-gram diversity of their outputs and others trend negatively. Namely, for\nthe \u201cAdditive-LeftHash,1\u201d (i.e. using LeftHash with context width h = 1, and additive f) scheme\nthat was evaluated in Kirchenbauer et al. [2023], stronger watermark strength yields less text diversity.\nThis issue is remedied by using a larger context width, or by choosing the \u201cSkip-LeftHash,4\u201d scheme\nwhich exhibits improved text diversity at higher watermark strengths. This finding provides another\nadvantage for schemes with increased context width.\nIn Figure 11, we display the drift between unwatermarked and watermarked completions, as measured\nin P-SP. To put these numbers into perspective, we note that the drift between human text and\nunwatermarked text can be estimated as just under 0.50 PSP. This implies that for the watermark\nsettings yielding z-scores up to 10.0 (a score that is extrememly detectable), the semantic divergence of\nthe watermarked output from the model\u2019s unwatermarked behavior is less than the average divergence\nof the unwatermarked model from human gold completions.\nIn Figure 2 from the main body we empirically demonstrate that the attack amplification effect,\nhypothesized in Kirchenbauer et al. [2023] to occur when using watermark schemes with context\nwidths greater than h = 1, is made less severe when using Skip and Min based schemes. To get the\nfull picture based on all degrees of freedom afforded in the hashing scheme space described in the\nmain body, we provide a plot varying all parameters simultaneously in Figure 12. We confirm the\n21\nAppendix\u2013Table of Contents\nprediction in Kirchenbauer et al. [2023] that the generalized version of the SelfHash algorithm, when\napplied to the Anchored version of MinHash scheme at a moderate context width (i.e \u201cAlgorithm 3\u201d\nfrom the previous work when the context width is h = 4), provides a competitive tradeoff between\nrobustness to attack and text diversity and quality along with the added benefit of being more secure\nagainst reverse engineering of the PRF key than the Additive-LeftHash.\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nZ-Score (\u2192)\n5\n6\n7\n8\n9\n10\nLog Diversity (\u2192)\nWatermark Strength (Z-Score) vs.\nWatermarked Output N-Gram Diversity\nAdditive-LeftHash,1\nMin-LeftHash,4\nMin-SelfHash,2\nMin-SelfHash,3\nSkip-LeftHash,4\nFigure 10: The pareto frontier with watermark strength (z-score) on the x-axis and text diversity (Log Diversity,\nsee Appendix A.1) shown on the y-axis. Higher is better for both metrics. We see that the Min-LeftHash and\nSkip-LeftHash schemes with larger context windows yield more diverse generations as watermark strength\nincreases. The standard LeftHash scheme with context width 1 and the SelfHash based schemes produce lower\ndiversity outputs under strong watermarking.\n22\nAppendix\u2013Table of Contents\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\n22.5\nZ-Score (\u2192)\n0.35\n0.40\n0.45\n0.50\n0.55\nP-SP (\u2192)\nWatermark Strength (Z-Score) vs.\nSimilarity of Unwatermarked and Watermarked Outputs (P-SP)\nAdditive-LeftHash,1\nMin-LeftHash,4\nMin-SelfHash,2\nMin-SelfHash,3\nSkip-LeftHash,4\nFigure 11: The pareto frontier of watermark strength in z-score, shown on the x-axis, versus similarity between\nthe un-watermarked and watermarked output, shown on the y-axis. Higher is better for both metrics. We see that\nthe Min-LeftHash and Skip-LeftHash schemes with larger context windows produce watermarked generations\nwith slightly higher semantic similarity to their unwatermarked counterparts than the other schemes as watermark\nstrength increases. However for all schemes, especially the SelfHash based settings, as the watermark is made\nstronger, the watermarked output diverges more from the unwatermarked output.\n1\n2\n3\n4\n5\n6\n7\n8\nContext Width\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nROC-AUC (\u2192)\nDetection Rate vs. Context Width\nAdditive-LeftHash\nAdditive-SelfHash\nAnchored-Min-LeftHash\nAnchored-Min-SelfHash\nAnchored-Skip-LeftHash\nAnchored-Skip-SelfHash\nMin-LeftHash\nMin-SelfHash\nSkip-LeftHash\nSkip-SelfHash\nFigure 12: Effect of seeding scheme context width on watermark robustness as measured by ROC-AUC after\na paraphrasing attack by GPT. In this variant, we ablate two more parameters, \u201canchoring\u201d and \u201cself-salting\u201d\n(SelfHash). The watermark strength parameters \u03b3, \u03b4 are fixed at (0.25, 2.0), slightly weaker than the above to\nbring in line with settings from figures in main work. We specially denote \u201cAdditive-LeftHash\u201d at width 1 via the\nblack circle marker and also the \u201cAnchored-Min-SelfHash\u201d at width 4 in as a brown circle, as these correspond\nto the \u201cLeftHash\u201d and \u201cSelfHash\u201d variants respectively shown in Figure 3, where we also reported the more\ncomplex watermark seeding scheme outperforming the standard scheme proposed by Kirchenbauer et al. [2023].\nHere, it becomes clear that this is the result of the Anchored-Min-SelfHash scheme outperforming the others in\nterms of robustness at all context widths.\n23\nAppendix\u2013Table of Contents\nA.3\nDatasets and Models Ablation\nIn this section we present the results of an extended evaluation of watermark robustness to machine\nparaphrasing attacks across a selection of domain specific datasets using the two models utilized\nin the main experiments llama and vicuna and a similarly sized but older generative model from\nthe Open Pretrained Transformer family, opt-6.7b [Zhang et al., 2022]. For cross-reference, the\nblack marker indicates the same standard model and data pair from the evaluations in the main\nwork. The \u201cGithub\u201d, \u201cLaw\u201d, \u201cMed\u201d, and \u201cPatents\u201d markers refer to the github, free_law, pubmed,\nand uspto subsets of the \u201cThe Pile\u201d [Gao et al., 2020] dataset as hosted on the huggingface hub at\nhuggingface.co/datasets/EleutherAI/pile by EleutherAI. \u201cWiki\u201d indicates samples from\nthe training split of Wikitext103 dataset [Merity et al., 2016], also hosted on the huggingface hub at\nhuggingface.co/datasets/wikitext as wikitext-103-raw-v1.\nGenerally, looking at Figure 13, we see that the Github subset yields the lowest z-scores relative to\nthe number of tokens considered here, most likely a function of the restrictive nature of code syntax.\nLess flexibility at generation time due to restrictive prompts and domain characteristics results in a\nlower average \u201cspike-entropy\u201d of the model\u2019s next-token distribution. This has a proportional effect\non how strongly the watermark can be embedded, due to its adaptivity property (see Kirchenbauer\net al. [2023] for a full analysis of the effect of entropy on watermark strength) and implies that under\ndefault settings, more text needs to be observed to detect the watermark. Upon manual inspection,\nthe Law subset is also highly formatted containing special characters and whitespacing and this is a\npotential explanation for the low peak z-score (below 8.0) since the model is forced to follow syntax\ncues in the prompt to maintain a high level of language modelling accuracy. The Med and Patents\nsubsets yield a wider range of z-scores across the three models in the 8.0 to 10.0 range, and the C4-en\nand Wiki datasets produce z-scores very similar to the C4-News split considered throughout the main\nwork.\nOut of the three models evaluated, the vicuna model, which is a supervised instruction-finetuned\nvariant of llama, yields the lowest z-score for each dataset, suggesting that its output entropy is\ngenerally lower than that of the base llama model or opt in congruence with the fact that we found\na higher delta value (4.0) was required to achieve suitably strong starting watermark levels in the\nhuman paraphrasing study (details in Appendix A.9).\nWhile in Figure 14 we also present the combination of models and data but with a different metric\nalong the y-axis, we find that the appropriate takeaways are mostly the same as described above.\nFurther, the semantic similarity (P-SP) values for the Github data domain are potentially miscalibrated\nsince this metric was not designed for code similarity estimation. That said, we notice that vicuna and\nllama achieve the same z-scores across Law, Med and Patents, but show more semantic divergence\nbetween the unwatermarked and watermarked outputs for Med and Patents than Law.\nIn Figure 15 we observe the effect of the different z-scores shown in Figure 13 and Figure 14 on\nthe unattacked detection rate. The ROC-AUC is quite a bit lower for Github and Law than the other\ndatasets. Then, in Figure 16 we see that this also translates into correspondingly lower detectabilty\nafter attack by GPT and Dipper. However, we note that for the Med and Law subsets (yellow and\ngreen bars), detectabilty post paraphrase attack is quite similar to the C4-News domain (black bars)\nutilized throughout the main work. This suggests that those findings generalize well at least to\nlanguage modelling domains with similar levels of syntactic flexibility.\n24\nAppendix\u2013Table of Contents\n3\n4\n5\n6\n7\n8\n9\n10\nZ-Score (\u2192)\n6\n7\n8\nLog Diversity (\u2192)\nWatermark Strength (Z-Score) vs.\nWatermarked Output N-Gram Diversity\nC4-News,Llama-7B\nC4-en,Llama-7B\nWiki,Llama-7B\nGithub,Llama-7B\nGithub,OPT-6.7B\nGithub,Vicuna-7B\nLaw,Llama-7B\nLaw,OPT-6.7B\nLaw,Vicuna-7B\nMed,Llama-7B\nMed,OPT-6.7B\nMed,Vicuna-7B\nPatents,Llama-7B\nPatents,OPT-6.7B\nPatents,Vicuna-7B\nFigure 13: The pareto frontier of watermark strength in z-score, shown on the x-axis, versus text diversity,\nshown on the y-axis. Higher is better for both metrics. The Github subset results in particularly low z-scores for\nall models and the Law subset is also shifted left versus the C4 data splits. Med, Patents, and Wiki yield higher\nz-scores more similar to the C4-News data from the main work.\n3\n4\n5\n6\n7\n8\n9\n10\nZ-Score (\u2192)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nP-SP (\u2192)\nWatermark Strength (Z-Score) vs.\nSimilarity of Unwatermarked and Watermarked Outputs (P-SP)\nC4-News,Llama-7B\nC4-en,Llama-7B\nWiki,Llama-7B\nGithub,Llama-7B\nGithub,OPT-6.7B\nGithub,Vicuna-7B\nLaw,Llama-7B\nLaw,OPT-6.7B\nLaw,Vicuna-7B\nMed,Llama-7B\nMed,OPT-6.7B\nMed,Vicuna-7B\nPatents,Llama-7B\nPatents,OPT-6.7B\nPatents,Vicuna-7B\nFigure 14: The pareto frontier of watermark strength in z-score, shown on the x-axis, versus similarity between\nthe un-watermarked and watermarked output, shown on the y-axis. Higher is better for both metrics. Similar\ntrends to Figure 13.\n25\nAppendix\u2013Table of Contents\nC4-News,Llama-7B\nC4-en,Llama-7B\nWiki,Llama-7B\nGithub,Llama-7B\nGithub,OPT-6.7B\nGithub,Vicuna-7B\nLaw,Llama-7B\nLaw,OPT-6.7B\nLaw,Vicuna-7B\nMed,Llama-7B\nMed,OPT-6.7B\nMed,Vicuna-7B\nPatents,Llama-7B\nPatents,OPT-6.7B\nPatents,Vicuna-7B\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC\nDetection Rate of Watermarking across Datasets and Models\nFigure 15: Detection rate of watermarking as measured by ROC-AUC for extended selection of datasets and\nmodels. The Github and Law subsets producing lower z-scores in Figure 13 corresponds to lower ROC-AUC.\nThe vicuna model yields the least detectable watermark out of the three models in most cases.\nC4-News,Llama-7B,Dipper\nC4-News,Llama-7B,GPT\nC4-en,Llama-7B,GPT\nWiki,Llama-7B,GPT\nGithub,Llama-7B,Dipper\nGithub,Llama-7B,GPT\nGithub,OPT-6.7B,Dipper\nGithub,OPT-6.7B,GPT\nGithub,Vicuna-7B,Dipper\nGithub,Vicuna-7B,GPT\nLaw,Llama-7B,Dipper\nLaw,Llama-7B,GPT\nLaw,OPT-6.7B,Dipper\nLaw,OPT-6.7B,GPT\nLaw,Vicuna-7B,Dipper\nLaw,Vicuna-7B,GPT\nMed,Llama-7B,Dipper\nMed,Llama-7B,GPT\nMed,OPT-6.7B,Dipper\nMed,OPT-6.7B,GPT\nMed,Vicuna-7B,Dipper\nMed,Vicuna-7B,GPT\nPatents,Llama-7B,Dipper\nPatents,Llama-7B,GPT\nPatents,OPT-6.7B,Dipper\nPatents,OPT-6.7B,GPT\nPatents,Vicuna-7B,Dipper\nPatents,Vicuna-7B,GPT\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC\nDetection Rate of Watermarking after Attack across Datasets and Models\nFigure 16: Detection rate of watermarking after being attacked using a machine paraphrasing model as measured\nby ROC-AUC for extended selection of datasets and models. The Github and Law subsets producing lower\nROC-AUC in Figure 15 corresponds to lower ROC-AUC after attack as well. While the GPT attack is more\nsuccessful in reducing the detectability of the watermark in a majority of the cases (around 8/12) a stronger\nclaim is withheld without further ablation of domain specific paraphrase instructions or parameters for the dipper\nparaphrasing model.\n26\nAppendix\u2013Table of Contents\nA.4\nGPT Attack Prompt Ablation\nWhen using gpt-3.5-turbo (GPT) to paraphrase the text throughout the experiments on watermark-\ning and detector robustness, we prompt the model with an instruction prompt for the paraphrasing\ntask. In our initial experiments we observed the tendency of GPT to produce a summary, i.e. it\ngenerated text with good semantic similarity to the input but significantly shorter overall length with\nthis summarization ratio worsening as the original length was increased. We tried to address this issue\nby specifically adding directives to the prompt to encourage the model to maintain the length of the\ntext. While ultimately we were unable to solve this problem through our basic prompt engineering,\nwe enumerate the prompts we explored in Table 1. We leave the further study of how to optimally\nprompt general purpose LLMs such as GPT for paraphrasing tasks to future research.\nWe note that in one sense, this makes the attack using GPT strictly stronger, as some information\ncontained in the original text is left out during summarization.\nThe prompt we selected to use throughout our experiments was \u201cPrompt 4\u201d as it resulted in a slightly\nlower ROC-AUC for the standard z-score detector (lower indicating a stronger, more successful\nparaphrase attack) than \u201cPrompt 3\u201d whilst achieving a slightly longer averaged attacked output length\nthan \u201cPrompt 0\u201d. Prompts 1 and 2 were included in table to be consistent with a file in the source\ncode though those other prompts were not competitive so they were omitted from the figures.\nPrompt ID\nPrompt Text\n0\n\u201cparaphrase the following paragraphs:\\n\u201d\n1\n\u201cparaphrase the following paragraphs and try your best not to use the same bigrams from the\noriginal paragraphs\\n\u201d\n2\n\u201cparaphrase the following paragraphs and try to keep the similar length to the original para-\ngraphs\\n\u201d\n3\n\u201cYou are an expert copy-editor. Please rewrite the following text in your own voice and\nparaphrase all sentences. \\n Ensure that the final output contains the same information as the\noriginal text and has roughly the same length. \\n Do not leave out any important details when\nrewriting in your own voice. This is the text: \\n\u201d\n4\n\u201cAs an expert copy-editor, please rewrite the following text in your own voice while ensuring\nthat the final output contains the same information as the original text and has roughly the\nsame length. Please paraphrase all sentences and do not omit any crucial details. Additionally,\nplease take care to provide any relevant information about public figures, organizations, or\nother entities mentioned in the text to avoid any potential misunderstandings or biases.\u201d\nTable 1: GPT paraphrase attack prompts. Performance of 0,3,4 shown in Figure 17 and Figure 18 as prompts 1\nand 2 were not competitive. Prompt 4 used throughout experiments in main work and Appendix.\n27\nAppendix\u2013Table of Contents\nPrompt 0\nPrompt 3\nPrompt 4\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC\nDetection Rate after GPT Attack\nusing Di\ufb00erent Prompts\nFigure 17: Detection rate of watermark after attack by GPT using various prompts.\nPrompt 0 Original\nPrompt 3 Original\nPrompt 4 Original\nPrompt 0 Attacked\nPrompt 3 Attacked\nPrompt 4 Attacked\n0\n50\n100\n150\n200\n250\n300\n350\n400\n450\n500\n550\n600\nText Length (Tokens)\nLength Reduction of Watermarked Texts\nby GPT Attack with Di\ufb00erent Prompts\nFigure 18: Original token lengths (all 600) versus token length after paraphrasing using different prompts for\nGPT. Standard deviation is visualized for the latter. We choose \u201cPrompt 4\u201d to balance AUC after attack and\nlength after attack, but a wider study of prompting general purpose LLMs for paraphrasing tasks is left to future\nresearch.\n28\nAppendix\u2013Table of Contents\nA.5\nWatermark Detector Ablation\n40\n50\n60\n70\n80\n90\n100\nAttack Percentage (severity \u2192)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC (\u2192)\nZ-Score Detection under Copy-Paste Attack\nNumber of Insertions\n1\n3\n5\n10\n20\n40\n50\n60\n70\n80\n90\n100\nAttack Percentage (severity \u2192)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC (\u2192)\nWinMax Detection under Copy-Paste Attack\nNumber of Insertions\n1\n3\n5\n10\n20\n40\n50\n60\n70\n80\n90\n100\nAttack Percentage (severity \u2192)\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nROC-AUC (\u2192)\nRun-Length Detection under Copy-Paste Attack\nNumber of Insertions\n1\n3\n5\n10\n20\nFigure 19: (Left) The performance of standard Z-Score watermark detection, (Center) WinMax detection, and\n(Right) the Run-Length based detector. WinMax outperforms the standard Z-Score at the 90% attack percentage,\nand the run-length test failed to show improvement over the standard test at any setting evaluated, however it\ndemonstrates parity at stronger attack levels.\nAdditionally, we investigate an anomaly detection test based run-lengths as a potential detector of\nthe distribution shift between watermarked and unwatermarked text [Bradley, 1960]. Yet, we find\nno gains from such a detector within the range of attack settings we consider in this work. After\ndescribing the basic results, we include a brief description of this alternate detection algorithm as a\nstarting point for future research.\nIn Figure 19, moving right along the x-axis means that the attack on the watermark becomes more\nsevere (higher \u201cattack percentages\u201d) because less of the total text remains watermarked. The marker\nstyle denotes how many fragments the remaining watermarked percentage is distributed across in the\nfinal text. As an example, the blue square at 90% attack, means 10% total tokens watermarked, split\nacross 3 chunks. The WinMax method shows comparable performance with the standard Z-Score\nmethod at all settings except for the 90% attack percentage, where it handily outperforms the standard\ndetector.\nFor the rightmost plot in Figure 19 showing the run-length detector performance, we visualize the best\nperforming variants of the run length test, where we only count the green runs, we use the \u201cmaximum-\nplus-1\u201d binning strategy, we ignore any run-lengths for which we recorded zero observations, and we\nuse the standard \u201cpearson\u201d chi-squared test statistic to compare expected and observed frequencies.\nAdditionally, for the more severe 90% and 95% attack levels, the best performing setting was to\nignore the length-1 runs bin in the test statistic computation. These details are elaborated at the end\nof this section.\nCounting the Lengths of \u201cGreen Runs\u201d\nAt detection time, the test originally proposed by\nKirchenbauer et al. [2023] initially treats a piece of text as a sequence of green and red tokens, which\ncan be modeled by a Boolean array i.e. sequence s such that st = 1 if st \u2208 Gt and st = 0 if not.\nHowever, the z-test then immediately reduces this sequence to one value |{st : st \u2208 Gt}|, the number\nof green tokens observed. We hypothesize that this reduces the power of the test in certain scenarios\nbecause it does not take into account information regarding the positions of the green (and red)\ntokens.\nTo harness this information, one can view the Boolean array as a set of runs, where a run is defined\nby a subsequence of consecutive 1\u2019s or 0\u2019s. As an example, under the convention that a 1 corresponds\nto a token being in its greenlist, the sequence [1, 1, 0, 1, 0, 0, 0, 1] contains 2 green runs of length 1,\nand 1 green run of length 2. It also contains 1 red run of length 1 and 1 red run of length 3.\nThe example scenario that motivated our exploration of this method was the text mixing setting\n(copy-paste attack) where sections of watermarked text would be interspersed within surrounding\nunwatermarked text. In the case of just a few heavily watermarked subsequences, one would expect\nto observed a few isolated runs of green tokens (consecutive 1\u2032s) that were surprisingly long. This is\nbecause we expect to observe few greens (\u03b3T in expectation) in unwatermarked text, and many more\n29\nAppendix\u2013Table of Contents\nin heavily watermarked text, and long green runs are themselves caused by a higher overall green\ntoken count.\nHypothesis Test for a \u201cRun-Length Detector\u201d\nTo formalize this notion of what we\u2019d find \u201csur-\nprising\u201d, and thereby derive a new hypothesis test from it, we leverage the fact that for a binary event\nwith a probability of \u201csuccess\u201d p, the number of independent trials k required to realize the first\nsuccess can be modeled by a geometric distribution, with density given by\nPr(\u201csuccess after k trials\u201d) = (1 \u2212 p)k\u22121p, for k = 1, 2, 3...\n(5)\nWe can treat observing a 0 or a redlist token as a success event, and therefore model the \u201cgreen runs\u201d\nas a geometrically distributed random variable with success probability 1 \u2212 \u03b3. Armed with this fact,\nif we treat each run length k = 1, 2, 3... as a category, then for a given piece of unwatermarked\ntext, the expected values of each of these categorical variables can be computed using the geometric\ndistribution density function scaled by the total number of runs observed in the text.\nTherefore, we can test for the watermark by testing the following null hypothesis,\nH0: The text sequence is generated with no knowledge of the watermarking rule\nand therefore the \u201cgreen\u201d run lengths are geometrically distributed.\n(6)\nOne standard way to compare an observed set of categorical variable outcomes to an expected set, is\nto perform a chi-squared test [Cochran, 1952]. In this test the sum of squared deviations between\nthe expected and observed frequency (count) for each category are summed. If the statistic is small,\nthen the oberved counts are close to the expected, and we fail to reject the null hypothesis. However,\nif the statistic is large, then at least one of the observed frequencies is surprising different from the\ncorresponding expected frequency and the collection of categorical variable observations is unlikely\nto have come from the expected distribution, in which case we can reject the null hypothesis.\nReturning to the context of green run lengths, this means that if we observe green run length counts that\ndon\u2019t match the expectation given by the geometric distribution, we are confident that this sequence\nwas not generated under the null hypothesis, and rather, was generated under the watermarking\nscheme. We expect that the most common mechanism for this rejection under watermarking would\nbe observing a surprising number of long runs of green tokens, i.e. surprisingly high frequencies in\nthe tail (larger values of k) than expected under the geometric distribution.\nFigure 20: An intuition-building example of the run length distributions we seek to compare. The left is the\nactual, empirical run lengths observed in a watermarked sample, and the right is the expected counts of each run\nlength based on the total number of observed runs, dsitributed according to the geometric prior parametrized\nby 1 \u2212 \u03b3 (the null hypothesis). Each bar shows the number of runs of a given length that were observed. The\n\u201csurprising\u201d observations are the non-zero counts for the length 7,9,10 and 12 bins.\nDesign Choices\n1. When we \u201ccount green runs, where red is the success event\u201d a new run begins after each\nobservation of a red token. The sequence [R, R, R, G, R] yields 3 \u201cgreen runs\u201d of length 1,\nand then 1 green run of length 2. This is because a red occurs after just a single trial, three\ntimes in a row, and then the final red takes two trials to achieve.\n30\nAppendix\u2013Table of Contents\n2. For a sequence of length T one could observe any possible run length k \u2208 {1...T}, and\nwe can compute an expected frequency for all k based on the success probability 1 \u2212 \u03b3.\nHowever, it is standard practice to bin the tail of unobserved k values to create a new event\n\u201crun lengths longer than k\u201d and the probability mass for those values is summed before\ncomputing the expected number of outcomes for that tail category. In Figure 20, the max\nshown (13), is 0, but the maximum actually observed was 12 and this addition of an extra\nbin represents the tail of runs longer than the max observed.\n3. For any run lengths k between 1 and the largest observed run length value kmax it is\nstandard practice to ignore these categories when computing the test statistic if there were\nzero observed occurrences. This is based on the assumption that the zero observation is\nlikely spurious consequence of a small sample size (of runs). In Figure 20, there are two\nbins, 8 and 11, that also are zero, which can be ignored in the test statistic computation.\n4. We consider the standard \u201cpearson\u201d formulation of the chi-squared test, as well as the \u201cg-test\u201d\nand \u201ccressie-reed\u201d variants based on likelihood ratios rather than squared deviations.\n5. In order to isolate the rejection scenario we expect under watermarking, we experiment with\nignoring the small categories k in the test statistic computation. The intuition is that these\nshort run lengths could dominate the statistic value in an undesirable way when there are just\na small handful of surprisingly long runs in the tail of the observed distribution. Considering\nthe example in Figure 20, since the close counts of observed and expected length-1 runs\npotentially of little interest with respect to the null hypothesis reject case expected for the\nwatermark, we can choose to ignore the leading bin in the test computation.\n31\nAppendix\u2013Table of Contents\nA.6\nDetectability of Watermarks after Machine Paraphrasing: TPR @ T\n0\n100\n200\n300\n400\n500\n600\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTPR\nDetection Rate of Watermarking after Attack\nas function of Tokens Observed (@ 0.1% FPR)\nSelfHash Dipper\nSelfHash GPT\nLeftHash GPT\nLeftHash Dipper\nSelfHash CP-1-25%\nLeftHash CP-1-25%\nSelfHash CP-3-25%\nLeftHash CP-3-25%\nSelfHash CP-1-10%\nSelfHash CP-3-10%\nLeftHash CP-1-10%\nLeftHash CP-3-10%\nFigure 21: In this variant of a plot from the main body, we visualize the growth characteristics of watermark\ndetection as quantified by True Positive Rate at low False Positive Rate (0.1%) after attack by a selection of\nmachine-based attacks. As in the main body charts, we make the curves translucent starting at the mean sequence\nlength of the attacked set after generation to indicate that these measurements are based on increasing fewer\nsamples after that point as T continues to grow and are therefore more uncertain. For the Dipper attack this is\n\u223c 500 and for the GPT attack this is \u223c 300. Due to the synthetic nature of the Copy-Paste attack, after attack,\nthose sequences are still full length i.e. 600 \u00b1 25.\n32\nAppendix\u2013Table of Contents\nA.7\nExperimental Methodology Details\nA.7.1\nGPT Paraphrase Attack\nWe utilize the prompt chosen through the ablation in Appendix A.4 and query the model using a\nsampling temperature of 0.7 and a max token limit of 1000.\nA.7.2\nDipper Paraphrase Attack\nWe utilize the Dipper paraphrase model proposed by Krishna et al. [2023] and released at their github.\nSince this model smoothly trades off semantic similarity between the paraphrased and original text\nstarting from few discernible changes at one end to almost wholly unrelated at the other extreme of\nits lex and div parameters, we choose one of the moderate strength settings to run the paraphrasing\nattacks with across all experiments lex=40,div=40. This still represents a significant attack, but\nmaintains high paraphrase quality/similarity. We of course emphasize that the setting used is the\nsame for all the watermarking and baseline detector methods to fairly compare them.\nA.7.3\nBaseline Method Hyperparameters\nWe lightly adapt the codebases provided by the authors to interface with our generation, attack, and\nevaluation pipeline for both methods and use default parameters found in their code unless otherwise\nspecified.\nRetrieval Detection\nFor the Retrieval Detection method, initially, we ran experiments using dense\nsimilarity retrieval, denoted sim in their codebase, as this method worked well out-of-the-box in our\nexperimental pipeline. However since Krishna et al. [2023] found BM25 to be superior, we reworked\ntheir code slightly, and reran all retrieval experiments. All results shown in the main work and in the\nnext section utilize BM25 as the retrieval method because it did perform slightly better under attack,\nin accordance with the findings of the original authors.\nDetectGPT\nFor DetectGPT, we use the \u201cz\u201d method (normalized perturbation discrepancy) and\n100 perturbations for estimating the loss landscape curvature around each sample, and use the\ncorresponding language model to be detected as the base model/likelihood estimator.\nWe remark that there are some unknowns about how well the DetectGPT paradigm works when\ndifferent models are used as the detection target, especially when under attack. In this work we\nprimarily utilize llama as the base model to be detected, and the relationship between the relative\nsizes and qualities of the base model, the perturbation model (a 3B parameter version of T5), and\nthe machine paraphraser (Dipper or gpt-3.5-turbo), could be quite subtle and produce counter-\nintuitive detection performance outcomes. We leave the study of post-hoc detectors with improved\nrobustness characteristics to future research.\nA.7.4\nDefining \u201cPositive\u201d and \u201cNegative\u201d Detection Test Samples\n\u201cNegative\u201d Samples\nIn all experiments, either using watermarking as the detection method or\nthe two baseline approaches, the \u201cnegative\u201d samples at test time that should be classified as \u201cnot\nwatermarked\u201d or \u201cnot machine-generated\u201d respectively, are human-written text i.e. the gold comple-\ntions/suffixes extracted from the input data as prompts are created.\n\u201cPositive\u201d Samples for Watermarking Detection\nFor the watermarking experiments, in the\nunattacked setting, the \u201cpositives\u201d are the watermarked model\u2019s generations.\nTo produce the\nparaphrase-attacked versions, the watermarked generation is fed to the paraphrasing model (GPT or\nDipper) or rewritten by humans. To construct the copy-paste attacked versions for watermarking, sets\nof watermarked tokens are inserted into a surrounding context sequence of human-written tokens (i.e.\nthe gold completions to the prompt). While this means that the negative examples and surrounding\ncontext examples for the copy-paste watermarking experiments are correlated, this does not give the\nwatermarking method any unfair advantage at detection time. If a specific sequence of human-written\ntext happens to produce an abnormally high z-score, which could artificially inflate the z-score of the\ncopy-paste attacked example making it an easier detection, then it will simultaneously increase the\nchance of a false positive on that sequence for precisely the same reason. Since both examples are\nalways tested, this effect should be balanced out.\n33\nAppendix\u2013Table of Contents\n\u201cPositive\u201d Samples for Alternate Detection Approaches\nFor the Retrieval based and DetectGPT\nbaseline approaches, the \u201cpositives\u201d are the unwatermarked model\u2019s generations as this is the\ndistribution that the approach is designed to detect. For the Retrieval method, this means that the\nretrieval database (index) is loaded up with the set of unattacked, unwatermarked generations so that,\nif those same sequences are queried at test time, then the retrieval performance (as measured by TPR)\nshould be perfect. To construct paraphrase attacked examples in this setting, the unwatermarked\ngenerations are fed to the paraphrasing model (GPT or Dipper) causing them to diverge from the\nexact sequences present in the database, and the exact model distribution that DetectGPT is testing\nfor.\nCopy-Paste Attacked Samples for Alternate Detection Approaches\nAs stated above, for copy-\npaste attacks in the the watermark detection evaluation, we insert spans of watermarked tokens\ninto surrounding context of human-written tokens. Since watermarking effectively views all text as\nboolean arrays \u201cgreen\u201d and \u201cred\u201d tokens regardless of the textual content itself, human-written text is\nthe most \u201cnegative\u201d type of context we can create to make the embedded watermarked chunk harder\nto detect.\nHowever, since the human-written completions are already used as the negative examples, and the\nbaseline detection methods rely much more on the syntactic and semantic similarities between the\nexamples, to reduce the error correlations between the negative and positives for Retrieval detection\nand DetectGPT, we instead insert the small unwatermarked chunks (parts to be detected) into a\nsurrounding context of watermarked text. While we realize this choice might seem a bit strange, and\nalso admit that it is mostly an implementation pipeline convenience rather than a perfectly optimal\nchoice, we do not believe this causes any unfair biasing of the detection performance estimates for\nthe following reasons.\nFor a fair copy-paste example with respect to the particular detection approach being evaluated, we\ndesire \u201cnegative\u201d looking context tokens to surround the \u201cpositive\u201d looking chunks to be detected.\nFrom a semantic similarity perspective, the watermarked generations used as a source of context\ntokens, are quite relevant/similar to the unwatermarked subsequences being inserted because they\nwere generated following the same prompt. For Retrieval, this means that there is actually a\ngenerous/favorable level of semantic similarity between the context tokens (meant to make the\ncopy-paste attacked sample look negative) and the unwatermarked outputs stored in the retrieval\ndatabase. We believe this is a reasonably fair and realistic setting since the copy-and-pasting attacker\nwe are simulating would likely replace removed sections of the text to be detected with semantically\nsimilar and relevant text (as opposed to random tokens).\nPotential Confounding Factors in Copy-Paste\nWe believe that this setup is also fair for the\nDetectGPT method, however, we realize that both the perturbation procedure and the likelihood\nestimation are probably influenced by certain discontinuities introduced in copy-pasted examples as\nno algorithm or post-processing step is used to smooth out the interface region between the positive\nand negative spans. That said, since we find that Retrieval performs quite well under the copy-paste\nattack and that for DetectGPT the copy-paste examples produce somewhat unremarkable behavior as\nvisualized in the main work and in Appendix A.8 (versus the GPT and Dipper results), we believe\nthese methodological choices did not significantly influence the results in an unfair way. That said,\nwe believe there are still open questions in developing suites of different attack strategies ranging the\ngamut from black-box model-based paraphrasing to synthetic text mixing that test a wider range of\npossible attack and corruption scenarios that could be encountered in-the-wild.\nA.7.5\nBaseline Detection Method Performance as a Function of Tokens Observed\nWhile the watermark detection score (z-score) is readily computable in a cumulative manner along\nthe sequence length dimension, and thus evaluation at T is simple for that method, for Retrieval and\nDetectGPT, the sequences to be tested must first be chunked into prefixes and then those sets of\nprefixes evaluated individually. For Retrieval detection, we make the choice that the sequences loaded\ninto the database should be the full length unwatermarked output, even though we are querying with\nprefixes of those outputs to develop the series of measurements at T. This reflects what we believe\nto be a realistic setting where the model generated the full output at some time in the past, and at\ntest time is being queried with a prefix/fragment of the original generation. Additionally, storing all\nprefixes of some given size for all generations is not a realistic or scalable choice in our opinion.\n34\nAppendix\u2013Table of Contents\nFor DetectGPT, the adaptation for detection at T is easier in one aspect because it is simply imple-\nmented by testing a block of prefixes of the original unwatermarked (and then attacked) generations.\nHowever, the computational cost of producing just a single series of measurements at a range of T\nvalues becomes prohibitively expensive for a single attack setting. This is because for each block of\nprefixes, say the leading 100 tokens from a set of N sequences that originally had length 600, the\nruntime cost is roughly the same as testing the full length outputs. Thus the overall runtime of testing\nall prefixes, or T values, for a given set of sequences is multiplied by the number of prefixes, or\nlength/stride. In contrast, there is effectively no multiplier for watermarking, and it is still relatively\ncheap for Retrieval since the retrieval method itself is much cheaper (at least for a small database).\nThis is the reason for the limited number of datapoints shown in the comparisons for DetectGPT as a\nfunction of observed tokens.\nAs final remarks on the somewhat surprising performance of the method, we note that the generations\nare tested without the original prompts included in the input. We assume this is the setting of the\noriginal work, since having access to the prompt at test time would be unrealistic, but beyond this\ndetail, we are unsure as to why evaluating the method using the llama model as the base model to\nbe detected, performs as poorly as it does. We leave more comprehensive robustness evaluation of\nthe DetectGPT method to future work by its creators and other researchers but hypothesize that the\nrelationship between the base model, the perturbation model, and the paraphrasing model, could be\nmore nuanced and require more careful tuning of hyperparameters or other aspects of the detection\nmethod.\n35\nAppendix\u2013Table of Contents\nA.8\nDetection Method Comparison: Detector Scores @ T\nIn this section we present a \u201cmechanistic\u201d explanation for the detectability as a function of text length\n(AUC at T) results reported in the main body of the work. In particular, we motivate the perusal of\nthis section by remarking that in order to perform well, a score-based binary detector (like all the\nmethods considered here) must maintain a gap between the scores assigned to \u201cnegative\u201d or genuine\nhuman written samples, and the \u201cpositive\u201d or machine generated samples. The left chart in each pair\nare the scores assigned to negative examples by the given method, and the right chart shows the scores\nfor positive examples. We make note of the fact that the standard Z-Score watermarking produces an\nextremely wide gap between corresponding curves in the left and right charts in Figure 22, driven in\nlarge part by the lack of growth in the negative scores. This key behavior of watermarking provides\nfor both detectability and a low FPR.\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nZ-Score\nZ-Score of Human Text as function of Tokens Observed\nCP-3-10%\nGPT\nCP-3-25%\nDipper\nCP-1-10%\nCP-1-25%\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nZ-Score\nZ-Score of Attacked Text as function of Tokens Observed\nGPT\nDipper\nCP-1-25%\nCP-3-25%\nCP-3-10%\nCP-1-10%\nFigure 22: The deterction scores yielded by the watermarking detection z-score method. In stark contrast to the\ntrends in Figure 24 and Figure 25, we see that for all T, the z-score of human text remains very low (Left) whilst\nthe z-scores for the attacked watermarked texts continue to grow steadily (Right) demonstrating the favorable\ntoken sample complexity characteristics of watermarking detection. As in preceding figures we turn the curves\ntranslucent after their mean sequence length value to indicate increased uncertainty.\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nWinMax Z-Score\nWinMax Z-Score of Human Text as function of Tokens Observed\nCP-3-10%\nGPT\nCP-3-25%\nDipper\nCP-1-10%\nCP-1-25%\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nWinMax Z-Score\nWinMax Z-Score of Attacked Text as function of Tokens Observed\nGPT\nCP-1-25%\nCP-3-25%\nDipper\nCP-1-10%\nCP-3-10%\nFigure 23: The detection scores yielded by the watermarking detection WinMax z-score method. Compared to\nFigure 22 we see that the likelihood of a False Positive at smaller values of T is higher under WinMax than the\nbasic z-score detection test as the separation between the scores for human text (Left) and attacked watermarked\ntext (Right) is not as large. However, empirically, this tradeoff enables improved detection under the strongest\ncopy-paste attacks as shown in Figure 8 and Figure 9. As in preceding figures we turn the curves translucent\nafter their mean sequence length value to indicate increased uncertainty.\n36\nAppendix\u2013Table of Contents\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRetrieval Score\nRetrieval Detection Score of Human Text\nas function of Tokens Observed\nDipper\nCP-1-25%\nCP-3-25%\nCP-1-10%\nCP-3-10%\nGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRetrieval Score\nRetrieval Detection Score of Attacked Text\nas function of Tokens Observed\nDipper\nGPT\nCP-1-25%\nCP-3-25%\nCP-1-10%\nCP-3-10%\nFigure 24: The similarity scores yielded by the Retrieval detection method using the BM25 search index. (Left)\nThe corresponding scores for \u201cnegative\u201d human-written test samples are lower than (Right) the scores for the\n\u201cpostive\u201d attacked samples for all values of T, which is desired/required for proper detection and mechanistically\nexplains the favorable detection performance shown in other figures. However, we note that the gap is not as\nlarge for the copy-paste attack as it is for GPT and Dipper. As in preceding figures we turn the curves translucent\nafter their mean sequence length value to indicate increased uncertainty.\n200\n400\n600\n800\n1000\nTokens (T)\n0\n1\n2\n3\n4\nDetectGPT Score\nDetectGPT Score of Human Text as function of Tokens Observed\nCP-3-25%\nCP-1-25%\nDipper\nGPT\n200\n400\n600\n800\n1000\nTokens (T)\n0\n1\n2\n3\n4\nDetectGPT Score\nDetectGPT Score of Attacked Text as function of Tokens Observed\nCP-1-25%\nGPT\nCP-3-25%\nDipper\nFigure 25: The similarity scores yielded by the DetectGPT method. (Left) While the corresponding scores\nfor \u201cnegative\u201d human-written test samples start out lower than (Right) the scores for the \u201cpostive\u201d attacked\nsamples at small values of T, which is desired/required for proper detection, this ordering becomes reversed\nfor the GPT and Dipper paraphrase attacks at larger T which helps explain the unfavorable/\u201cinverted\u201d-looking\ndetection performance curves out at 300 and 800 tokens for those methods shown in other figures.\n37\nAppendix\u2013Table of Contents\nA.9\nHuman Study Details and Preference Evaluation\nA.9.1\nQuality/Preference Evaluation\nTo give a second perspective on the quality impact of watermarking on generated texts, as part of\nour Human study (Table 2), we ask annotators to rate which of two potential responses to a prompt\nthey generally prefer. And we find that for a very strong watermark (\u03b3 = 0.25, \u03b4 = 4.0) humans only\nprefer the unwatermarked text over the watermarked text a weak majority the time.\nTotal Ratings\nUnwatermarked Answer\nWatermarked Answer\nUnwatermarked Preferred\n177\n109\n68\n61.58%\nTable 2: Outcome of human preference study. We report the frequency human evaluators preferred the un-\nwatermarked generation output over watermarked output.\nA.9.2\nData Generation and Selection Parameters\nWe utilize the Long Form Question Answering (LFQA) style dataset curated by Krishna et al.\n[2023] available via the Google Drive link provided at github.com/martiansideofthemoon/\nai-detection-paraphrases. We generate machine responses to the questions using the vicuna\nmodel under a (\u03b3 = 0.25, \u03b4 = 4.0) watermark prepended with the following prompt:\n\u201cAnswer the following question in 200-300 words.\nExplain it like I\u2019m five.\\n\\n\u201d\nTo select the small subset of examples presented to annotators in the paraphrasing study we filter the\noriginal 2758 questions to a subset of 60 by selecting examples such that the watermarked model\nresponse was 1) longer than 200 tokens, 2) had a z-score of > 9.0, 3) had a P-SP similarity score\nbetween the gold human response and the watermarked response of > 0.6, and 4) had a 4-gram\nrepetition rate < 0.11. In particular we enforce 1) and 2) to make sure that these examples were\nof adequate length and heavily watermarked to start out with, in order to develop a significant\nresult based on the final z-scores achieved through paraphrasing. Considering weakly watermarked\nexamples with low starting z-scores, would make for uninformative samples since there would be\nlittle watermark to scrub away in the first place. Constraints 3) and 4) were enforced simply to raise\nthe quality of the machine responses as these questions are quite challenging to answer well even for\nthe stronger instruction-tuned vicuna model utilized.\nFor the preference study we filter the original set from 2758 to 205 by enforcing that 1) token lengths\nof both the unwatermarked and watermarked outputs were > 200 and differed in length by no more\nthan 50 tokens 2) that the watermarked text z-scores were > 4.0. The second constraint was chosen\nto increase the likelihood of the unwatermaked and watermarked texts being perceived as different\nbased on the significant watermark (most examples had a much higher z-score than that lower limit)\nand the first constraint was chosen to remove the spurious differences annotators might perceive due\nto length differences which are not necessarily indicative of quality.\nA.9.3\nAnnotation Platform and Task Instructions\nWe utilize the open source data annotation platform LabelStudio [Tkachenko et al., 2020-2022] to\nconduct the human study and a screenshot of the interfaces constructed for each of the two tasks are\nprovided in the main work.\nHere, we additionally show the instructions that were given to annotators for both of the human study\ntasks in Figure 26 and Figure 27.\n38\nAppendix\u2013Table of Contents\n\u201cParaphrase Text\u201d\nDescription\nParaphrase an AI generated response to a question from Reddit\u2019s r/explainlikeim\ufb01ve (ELI5)\nforum.\nInstructions\nA question or topic statement is shown at the top of the screen. On the left side of the screen\nyou will see a response to the question. The response was generated by an AI language model.\nThe response is \u201cwatermarked,\u201d meaning it contains invisible patterns that can be used to\ndetermine that the response was written by an AI and not a person. Read the AI-generated\nresponse on the left half of the screen, and in the text box on the right side of the screen,\nre-write the response in your own words, whilst preserving the meaning and length of the\ntext. Your goal is to change the text so much that the watermark is no longer detectable.\nWhen you are \ufb01nished, click the \u201csubmit\u201d button to save your re-written text and move on\nto the next task.\nRequirements:\n1. Paraphrase quality/similarity - A paraphrase should convey roughly the same\ninformation as the original text, to roughly the same level of detail.\n2. Time limit - Try to spend no more than 10 minutes on any individual paraphrasing\ntask. The annotation software tracks the time you spend on each task, but it will not\n1\nexplicitly enforce the time limit by kicking you o\ufb00. Please do the tasks in a single\nsitting.\n3. No automated paraphrasing tools - Do not use any AI tools that write text for\nyou (e.g., ChatGPT, Grammarly), and do not copy/paste text from any external source.\nHowever, you may look things up online, refer to a dictionary or thesauruses, and use\na spell checker if such a tool is enabled in your browser window.\nFigure 26: Annotator instruction sheet for the human paraphrasing task.\n\u201cCompare Answers\u201d\nDescription\nSelect a preferred response to questions from Reddit\u2019s r/explainlikeim\ufb01ve (ELI5) forum.\nInstructions\nAt the top of the screen you will see a question or topic statement. Beneath it there will be\ntwo di\ufb00erent responses to the question, one on the left and one on the right. Choose the\nbest response of the two by clicking on the left or right text box. Then click the \u201csubmit\u201d\nbutton on the bottom right to save your selection and move on to the next task.\nRequirements:\n1. Time limit - Please spend at most 5 minutes on each individual response pair. If\nnecessary, brie\ufb02y consult the internet to clarify the meaning of words or check the\ncorrectness of statements.\nFigure 27: Annotator instruction sheet for the preference evaluation task.\n39\nAppendix\u2013Table of Contents\nA.9.4\nDemographic and Compensation Details\nWe recruited graduate students from a computer science department to do the paraphrase and\npreference evaluation tasks. 14 annotators worked on paraphrases and 9 additional annotators worked\non preference ratings. One out of the 14 paraphrase annotators did not complete enough samples and\nso is removed from some of the evaluations in the main work. The goal was a maximum diversity\nof annotated samples and so each of the original watermarked texts was paraphrased by a single\nannotator, and roughly 150/177 preference rating examples were for unique questions.\nThe group comprised both native english speakers and english-as-a-second-language speakers. We\nprompted volunteers to self pre-select based on a description of the tasks to be performed, emphasizing\nthat the ability to write high quality paraphrases of a few paragraphs in length was required. Admission\nto the relevant university requires a high level of English language reading and writing competency\nas assessed by required standardized testing before admission.\nAll annotation tasks were performed in one 1.5 hour session and all annotators were compensated\nwith free dinner and drinks. As additional incentive to ensure that the paraphrase task was completed\nin a \u201cmotivated\u201d manner to try and approximate the real incentives of a paraphrase attacker, we\ninformed participants that the three most successful attackers (their paraphrases achieved lowest final\ndetection scores) would be awarded a $100 gift card. An additional gift card was randomly awarded\nto annotators who only performed the preference comparison task as this task was less goal oriented\nand thus there was not direct way to quantify success or rank annotator performance.\nA.9.5\nInstitutional Review Board \u201cExempt\u201d Status\nIn preparation for conducting the human paraphrasing and preference evaluation study components\nof the research, a \u201cHuman Subjects Research Determination\u201d form was filed with the relevant\nInstitutional Review Board. Before any portion of the human study was conducted, a determination\nletter was received communicating the status of \u201cExempt\u201d for the project proposal, i.e. \u201cNot Human\nSubjects Research\u201d.\n40\nAppendix\u2013Table of Contents\nA.10\nCode Details and Release Statement\nWe extend the implementation of watermarking developed for Kirchenbauer et al. [2023]. For the\ndatasets and models evaluated in this work, we heavily utilize the huggingface datasets library\nand huggingface transformers modelling framework. We retrieve pretrained model weights and\ndataset files from the huggingface hub, with the exception of the weights for the llama base model.\nWe provide code at https://github.com/jwkirchenbauer/lm-watermarking to reproduce\nthe experiments performed in this study.\nThe llama 7B parameter model weights were retrieved with permission using a presigned URL\nreceived via email from the \u201cLLAMA Release Team\u201d to be used for research purposes only in\naccordance with the license. These weights were then converted to the huggingface format before\nuse. To construct the Vicuna model, we retrieved the lmsys/vicuna-7b-delta-v1.1 weights\nfollowing instructions at github.com/lm-sys/FastChat and merged them with the base llama\n7B weights.\nA.11\nHardware and Compute Details\nThe experiments performed in the study were all inference-based and therefore could be run on a\nsingle Nvidia RTXA4/5/6000 GPU. The 7B parameter models were run in float16 during generation\nof watermarked and unwatermarked responses, but the Dipper model was run in full precision in\naccordance with author recommendation, and output quality issues observed under the float16\nsetting. Additionally, the Dipper model and DetectGPT model were both run on A6000 cards\ndue to the memory footprint required by their larger parameter counts. Generation stages, where\nunwatermarked and watermarked outputs were sampled, took less than 12 hours. Attack stages\nusing both the GPT (OpenAI API) model and the Dipper model took around 4-6 hours. Evaluation\nstages where watermarking detection at only the max T value was performed took minutes, and\nwhen ROC-AUC\u2019s at all T values and all text quality metrics were computed, took less than 2 hours.\nDetectGPT evaluation took well over 12 hours for a single set (\u223c 500 samples) of generations with\nlonger token lengths such as T = 1000.\n41\n"
  },
  {
    "title": "MobileNMT: Enabling Translation in 15MB and 30ms",
    "link": "https://arxiv.org/pdf/2306.04235.pdf",
    "upvote": "3",
    "text": "MobileNMT: Enabling Translation in 15MB and 30ms\nYe Lin1\u2217, Xiaohui Wang2, Zhexi Zhang2, Mingxuan Wang2, Tong Xiao1,3\u2020, Jingbo Zhu1,3\n1NLP Lab, School of Computer Science and Engineering,\nNortheastern University, Shenyang, China\n2 ByteDance\n3NiuTrans Research, Shenyang, China\n{linye2015}@outlook.com\n{wangxiaohui.neo,zhangzhexi,wangmingxuan.89}@bytedance.com\n{xiaotong,zhujingbo}@mail.neu.edu.cn\nAbstract\nDeploying NMT models on mobile devices\nis essential for privacy, low latency, and of-\nfline scenarios. For high model capacity, NMT\nmodels are rather large. Running these models\non devices is challenging with limited storage,\nmemory, computation, and power consumption.\nExisting work either only focuses on a single\nmetric such as FLOPs or general engine which\nis not good at auto-regressive decoding. In\nthis paper, we present MobileNMT, a system\nthat can translate in 15MB and 30ms on de-\nvices. We propose a series of principles for\nmodel compression when combined with quan-\ntization. Further, we implement an engine that\nis friendly to INT8 and decoding. With the\nco-design of model and engine, compared with\nthe existing system, we speed up 47.0\u00d7 and\nsave 99.5% of memory with only 11.6% loss\nof BLEU. The code is publicly available at\nhttps://github.com/zjersey/Lightseq-ARM.\n1\nIntroduction\nAs a classic subfield of natural language processing,\nneural machine translation (NMT) has achieved\ngreat success in recent years. Most of the studies\nfocus on improving the accuracy of large machine\ntranslation systems, ignoring whether such models\nare easy to be deployed in real-world scenarios.\nHere we adopt four metrics to evaluate whether\nan NMT model is deployment-friendly. (1) Model\nsize is the most important metric in model com-\npression (Han et al., 2016). (2) Floating-point\noperations (FLOPs) is commonly used to evaluate\ncomputational complexity in neural architecture\ndesign. (3) Memory or Memory mapped I/O\n(MMI/O) reflects the memory requirements of the\nreal running system. (4) Decoding speed depends\non many realistic factors such as engine implemen-\ntation and the power of avaliable processors.\n\u2217This work is done during the internship at ByteDance.\n\u2020Corresponding author.\n0\n200\n400\n600\n800\n1,000\n10MB-Ours\n20MB-Ours\nBase-TFLite\nBig-TFLite\n10\n20\n260\n872\nModel Size (MB)\n0\n1,000\n2,000\n3,000\n10MB-Ours\n20MB-Ours\nBase-TFLite\nBig-TFLite\n14.88\n25.95\n908.52\n2,886.62\nMemory (MB)\n0\n500\n1,000\n1,500\n10MB-Ours\n20MB-Ours\nBase-TFLite\nBig-TFLite\n27.25\n46.3\n332.25\n1,281.5\nLatency (ms)\n0\n10\n20\n30\n10MB-Ours\n20MB-Ours\nBase-TFLite\nBig-TFLite\n25.08\n27.09\n27.4\n28.36\nBLEU [%]\nFigure 1: These metrics are measured on Google Pixel\n4. Each result is the average of 200 runs on a sample of\nsrc/tgt length 30.\nIn this paper, we propose MobileNMT, a\nTransformer-based machine translation system that\ncan translate in 15MB and 30ms. First, we propose\nthree principles for designing parameter-limited\nMT models: 1) To compress embedding, reducing\nvocabulary size is simple and effective compared\nto embedding factorization; 2) To compress the\nencoder and decoder, reducing the model width\nis much more efficient in computation and mem-\nory than cross-layer parameter sharing; 3) Encoder\ndepth is very important to ensure accuracy. To\nachieve higher accuracy, we adjust the training\nhyperparameters according to the newly designed\nstructure, and adopt sequence-level knowledge dis-\ntillation. For industrial deployment, we optimize\ngeneral matrix multiplication (GEMM) and mem-\nory in our own inference engine and use the 8-bit\ninteger for storage and computation. As shown in\nTable 1, the 10MB MobileNMT achieves 88.4%\narXiv:2306.04235v1  [cs.AI]  7 Jun 2023\n64\n60\n55\n49\n47\n26\n27\n# Params of Base (M)\nBLEU [%]\nScaling E\nScaling V\n21\n19\n16\n14\n12\n21\n22\n23\n24\n# Params of Small (M)\nBLEU [%]\nScaling E\nScaling V\n8\n7\n5\n4\n3\n14\n16\n18\n20\n# Params of Tiny (M)\nBLEU [%]\nScaling E\nScaling V\n28.2 35.5 42\n64.5\n25\n26\n27\n# Params of Base (M)\nBLEU [%]\nSharing\nWidth\n12.2 14.1 15.9\n21.5\n22.5\n23\n23.5\n24\n# Params of Small (M)\nBLEU [%]\nSharing\nWidth\n5.7 6.1\n6.6\n8\n18\n19\n20\n21\n# Params of Tiny (M)\nBLEU [%]\nSharing\nWidth\nFigure 2: Model performance of different methods in Section 2 and Section 3 (Scaling E: scaling embedding\ndimension; Scaling V: scaling vocabulary size; Sharing: cross-layer parameter sharing; Width: reducing model\nwidth). ::::::\nScaling::V::::::::\nperforms:::::\nbetter::::\nthan::::::\nScaling:::\nE. :::::\nWidth::::::::\nperforms :::::\nnearly:::\nthe:::::\nsame::::\nwith:::::::\nSharing.\nperformance of Transformer-big with only 1.1%\nsize and runs 47.0\u00d7 faster on decoding, which can\nbe easily deployed and used.\nOur contributions are summarized as follows:\n\u2022 We propose three principles for parameter-\nlimited MT models to make more efficient\nuse of computation and memory resources.\n\u2022 We adjust training strategies according to the\nnewly designed structure to achieve higher\ntranslation accuracy.\n\u2022 We develop a mobile inference engine to\nbridge the gap between industrial practice and\ntheoretical research.\n2\nArchitecture Design Principles\nFor model compression and acceleration, most stud-\nies focus on a single metric such as model size\nor FLOPs, without considering the real-world ap-\nplications. In this section, we consider four met-\nrics including model size, FLOPs, memory usage,\nand decoding speed, and then propose three design\nprinciples for parameter-limited MT models. We\nchoose Transformer (Appendix A) as our baseline\nbecause of its great success in machine translation.\n2.1\nEmbedding Compression\nThe vocabulary size V usually reaches tens of thou-\nsands in NMT models (Akhbardeh et al., 2021).\nThe parameters can reach tens of millions and\ngreatly affect the overall parameter efficiency.\nEmbedding Factorization (Scaling E). For\nmodel compression, embedding factorization has\nbeen widely studied (Lan et al., 2020; Grave et al.,\n2017; Baevski and Auli, 2019). To decouple the\nModule Dim\nBase\nSmall\nTiny\nEmbed\nVocab[\n40,000] [\n40,000] [\n40,000]\nEmbed\nN/A\n\u00d71\nN/A\n\u00d71\nN/A\n\u00d71\nHidden\n512\n256\n128\nEncoder\nHidden[\n512 ] [\n256 ] [\n128 ]\nHead\n8\n\u00d76\n4\n\u00d76\n2\n\u00d76\nFFN\n2048\n1024\n512\nDecoder\nHidden[\n512 ] [\n256 ] [\n128 ]\nHead\n8\n\u00d76\n4\n\u00d76\n2\n\u00d76\nFFN\n2048\n1024\n512\nParams\n64.5M\n21.5M\n8.0M\nTable 1: The detailed settings of Base, Small and Tiny.\nembedding dimension E and hidden dimension H,\nit additionally introduces a trainable transforma-\ntion weight W T \u2208 RE\u00d7H, where E \u2264 H. After\nfactorization, the embedding parameters will be\ndecreased from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H).\nReducing Vocabulary Size (Scaling V). A more\ndirect way to compress embedding is to reduce the\nvocabulary size V . To reduce the risk of out-of-\nvocabulary words, here we adopt Byte-Pair Encod-\ning (BPE) (Sennrich et al., 2016; Ott et al., 2018;\nDing et al., 2019; Liu et al., 2020). For most stud-\nies on machine translation, the adopted BPE merge\noperations range from 30\u223c40K (Ding et al., 2019).\nVolt proves that we can find a well-performing\nvocabulary with higher BLEU and smaller BPE\nmerge operations (Xu et al., 2021). Experiments\nin Lin et al. (2021)\u2019work also show that smaller\nvocabularies may be better.\nReducing Vocabulary Size Performs Better.\nTo compare the two embedding compression meth-\nods, here we select three baseline models of differ-\nent sizes. The model settings are shown in Table 1.\nAs shown in Table 2, the parameters and FLOPs are\nalmost the same in these two methods. As shown\n0 1 2 3 4 5 6 7 8 9 1011\n5\n10\n# Layer ID\nWeight\nSharing\nWidth\n0 1 2 3 4 5 6 7 8 9 1011\n0\n200\n400\n# Layer ID\nOutput\nSharing\nWidth\nBase\nSmall\nTiny\n15\n20\n25\nEn\u2192De\nBLEU [%]\nS\nW\nFigure 3: The left two figures show weight and output ranges for each layer. The right figure shows the model\nperformance of Post Training Quantization (PTQ) in cross-layer parameter sharing vs. reducing model width. :::::\nThese\n:::::\nfigures:::::\nshow::::\nthat :::::::\nreducing::::::\nmodel :::::\nwidth ::\nis ::::\nmore:::::::::::::::::\nquantization-friendly::::\nthan::::::::::\ncross-layer ::::::::\nparameter:::::::\nsharing.\nMetric\nScaling E\nvs.\nScaling V\nBase Small Tiny\nBase Small Tiny\nParams (M)\n47\n12\n3\n47\n12\n3\nFLOPs (G)\n1.41\n0.38\n0.11\n1.41\n0.38\n0.11\nMMI/O (M)\n48\n15\n6\n47\n14\n5\nBLEU\n25.46 21.03 14.48\n26.17 22.49 17.10\nMetric\nSharing\nvs.\nWidth\nBase Small Tiny\nBase Small Tiny\nParams (M)\n28\n12\n6\n28\n12\n6\nFLOPs (G)\n1.95\n0.65\n0.24\n0.85\n0.38\n0.17\nMMI/O (M)\n66\n24\n10\n30\n15\n7\nBLEU\n25.41 22.37 17.75\n25.18 22.62 18.60\nTable 2:\nParameters, FLOPs, and model perfor-\nmance (FLOPs and MMI/O are estimated on a sam-\nple with src/tgt length of 30.).\n:::\nFor ::::::::::\nembedding\n:::::::::::\ncompression, ::::::::\nreducing::::::::::\nvocabulary ::::\nsize::::::::\n(Scaling :::\nV)\n:is:::::\nmore:::::::\nsimple ::::\nand ::::::::\neffective.:::::\nFor ::::::::::::::\nencoder/decoder\n:::::::::::\ncompression, :::::::\nreducing::::::\nmodel::::::\nwidth :::::::\n(Width) ::is:::::\nmore\n:::::::\nefficient ::\nin ::::::::::\ncomputation:::\nand::::::::\nmemory.\nin the first row of Fig. 2, compared to reducing\nvocabulary size, the model with embedding factor-\nization performs poorly in most cases, especially\nwhen the parameters are limited.\n2.2\nEncoder/Decoder Compression\nFor encoder and decoder compression, here we\ncompare models with cross-layer parameter sharing\nand model width reduction.\nCross-Layer Parameter Sharing (Sharing).\nThe most widespread use of parameter sharing is in\nconvolutional neural networks (Long et al., 2015).\nIn recent years, it has also been investigated on\nNLP and NLU tasks. Among them, cross-layer pa-\nrameter sharing can provide stronger nonlinearity\nalong the model depth while keeping the parame-\nters unchanged (Dehghani et al., 2019; Takase and\nKiyono, 2021; Lan et al., 2020).\nReducing Model Width (Width). Since model\ndepth has been proven to be important in natural\nlanguage processing tasks such as machine trans-\nlation (Devlin et al., 2019; Liu et al., 2020; Wang\net al., 2022; Liu et al., 2020), here we keep the\n10\n20\n30\n22\n24\n26\n# Params (M)\nBLEU [%]\nSmall Baseline\nVocab Size\nEncoder Depth\nDecoder Depth\nHidden Size\nFFN Dim\nFigure 4: Performance (BLEU) vs. parameters (M). Dif-\nferent marks denote different dimensions. Points near\nlarge red circles have a greater impact on model perfor-\nmance than points near small red circles. :::::::\nEncoder:::::\ndepth\n:::\ncan ::\nbe:::::::::\nconsidered::as:::\nthe:::::\nmost::::::::\nimportant:::::::::\ndimension.\ndepth unchanged and reduce the model width.\nReducing Model Width is More Efficient and\nQuantization-Friendly. In the second row of Fig.\n2, these two methods perform nearly the same.\nHowever, Table 2 shows that there is a large differ-\nence in FLOPs and MMI/O, which means reducing\nmodel width is much more efficient in computa-\ntion and memory. Since it is necessary to quantize\nthese models for greater compression, we further\ncompare the weights and output ranges of the two\nmethods in Fig. 3. It can obviously be observed\nthat models with parameter sharing have larger\nranges of values for both weight and output, which\nis not quantization-friendly. The right figure also\nverifies this: when we apply post-training quan-\ntization (PTQ) (Sung et al., 2015; Banner et al.,\n2019; Choukroun et al., 2019) to these two meth-\nods, cross-layer parameter sharing performs poorly.\n2.3\nDeep Encoder and Shallow Decoder\nFig. 4 studies how different dimensions affect the\nTransformer performance. In order to analyze the\nimpact of each dimension separately, here we only\nchange one specific dimension and keep the others\nModule\nDim\nMobileNMT-10MB MobileNMT-20MB\nEmbed\nVocab [\n8,000]\n[\n8,000]\nEmbed\nN/A\n\u00d71\nN/A\n\u00d71\nHidden\n256\n384\nEncoder\nHidden[\n256 ]\n[\n384 ]\nHead\n4\n\u00d712\n6\n\u00d712\nFFN\n512\n768\nDecoder\nHidden[\n256 ]\n[\n384 ]\nHead\n4\n\u00d72\n6\n\u00d72\nFFN\n512\n768\nParams\n\u224810M\n\u224820M\nTable 3: The detailed settings of MobileNMT.\nX\nLN\nN\nL\nReLU\nN\nL\nL\nYf\nW1\nb1\nW2\nb2\n\u03b3i\n(a) FFN Quantizer\nX\nLN\nN\nL\nK\nN\nN\nSoftMax\nN\nL\nL\nYa\nQ\nV\nN\nWqkv\nbqkv\nWo\nbo\nscale\n\u03b3i\n(b) Attention Quantizer\nFigure 5: Running examples of the FFN and attention\nquantizers. Here red lines denote values that will be\nquantized, black lines denote values with full precision.\nunchanged. The point on the left of the Small\nBaseline  represents scaling one dimension down,\nwhile the point on the right represents scaling one\ndimension up. We can see that Encoder Depth \u2666\nis more important than other dimensions, which\nis consistent with the related work on large-scale\nmodels (Wang et al., 2019, 2022). Based on the\nabove discussion, we finally build a deep encoder\nand a shallow decoder, while reducing the vocab\nsize and model width. Two MobileNMT models\nof different sizes are built here and the detailed\nsettings are shown in Table 3.\n3\nTraining Strategies\n3.1\nPre-Training with Knowledge Distillation\nIn order to improve the performance of compressed\nmodels, recent studies distill knowledge from a\nwell-trained full-precision teacher network to a stu-\ndent network (Mishra and Marr, 2018) or directly\nuse a quantized teacher network (Kim et al., 2019).\nHere we adopt sequence-level knowledge distilla-\nBase\nOurs\nw/o\nw/ PTQ\n0.0\n0.05\n0.1\n24\n26\nDropout\nBLEU [%]\nLR:0.01 LR:0.02\n15\n20\n25\nFigure 6: The left part shows performance of dif-\nferent dropouts on base model vs.\nMobileNMT.\nThe right part shows performance before vs.\nafter\nPTQ. ::::::::\nRemoving:::::::\ndropout:::::\nfrom:::::::::::\nMobileNMT::::\ncan ::::\nlead\n::\nto ::::::::\nsignificant:::::::::::\nperformance::::::::::::\nimprovement.::::::\nWhile :::::\nlarger\n:::::::\nlearning ::::\nrates :::\ncan::::\nalso :::::::\nimprove :::::\nmodel:::::::::::\nperformance,:::\nthe\n:::::\nmodel::::\nwill ::::::\nbecome::::::::::::::::::::\nquantization-unfriendly.\nLR: 0.01\nLR: 0.02\n0 2 4 6 8 101214\n2\n4\n6\n# Layer ID\nWeight\n0 2 4 6 8 101214\n0\n200\n400\n600\n# Layer ID\nOutput\nFigure 7: Weight and output ranges for each layer.\n:::::\nLarger:::::::\nlearning::::\nrate :::\nwill:::::\nresult::\nin:::::\nlarger:::::\nrange ::\nof::::::\nvalues.\ntion because it has shown to be effective for NMT\ntasks. The most basic full-precision Transformer-\nbase model is adopted as the teacher.\n3.2\nQuantization\nThe process of quantizing a transformer model can\nbe divided into two steps: 1) constructing quan-\ntizers; 2) applying the quantization-aware training\n(QAT) (Courbariaux et al., 2015) based on the pre-\ntrained model we have obtained in Section 3.1.\nFFN and Attention Quantizers. The original\nTransformer layer includes two types of sublayers:\nthe attention sublayer and feed-forward network\n(FFN) (Vaswani et al., 2017). Here we construct\nthe quantizer for each linear in the attention and\nFFN, and quantize both the weights and activations\nas shown in Fig. 5. Since most computations are\nspent on matrix multiplication, all biases and resid-\nuals are kept in full precision for accuracy preser-\nvation. Since quantization will change the range of\nnetwork outputs, here we add a learnable weight\n\u03b3i to the i-th sublayer to learn how to combine the\noutput and the residual surrounding it.\nQuantization-Aware Training.\nSince Mo-\nbileNMT only has 10M/20M parameters, quantiz-\ning such a small model inevitably results in perfor-\nmance loss, so we perform QAT after constructing\nAttn (w/o L2)\nAttn (w/ L2)\nFFN (w/o L2)\nFFN (w/ L2)\nw/o\nw/ PTQ\n0 1 2 3 4 5 6 7 8 9 10111213\n0\n2\n4\n6\n# Layer ID\nWeight\n0 1 2 3 4 5 6 7 8 9 10111213\n0\n200\n400\n600\n# Layer ID\nOutput\n0.0\n0.05\n0.1\n15\n20\n25\nWeight Decay\nBLEU [%]\nFigure 8: The left two figures show weight and output ranges for each layer. The right figure shows the performance\nof different L2 regularizations before vs. after PTQ. ::::::::::\nExperiments:::::\nshow :::\nthat:::\nL2::::::::::::\nregularization :::\ncan :::::\nmake :::\nthe :::::\nmodel\n::::\nmore::::::::::::::::::\nquantization-friendly.\nthe quantizers. Before QAT, we pre-compute all\nscaling parameters based on a forward running on\nthe pre-trained distillation model obtained in Sec-\ntion 3.1. It takes nearly no additional costs, but\nprovides a good initialization. For engineering de-\nvelopment, we choose the uniform quantization\nscheme because of it is hardware-friendly (Liu\net al., 2022). For 8-bit quantization, we use the\nelement-wise quantization (Lee et al., 2021). For\nlower-bit quantization, such as 4-bit integer, we use\nthe row-wise quantization (Faraone et al., 2018).\n3.3\nTraining Hyperparameters\nCompared to the original Transformer model, Mo-\nbileNMT introduced in Section 2 has fewer parame-\nters and different architectures, so different training\nhyperparameters are needed.\nRemoving Dropout. Since our models have\nfewer parameters, we do not need to impose strong\nregularizations on them and we remove dropout\nfrom the entire model. The left part of Fig. 6 shows\nthat removing dropout will lead to an improvement\nof almost two BLEU points.\nLarger Learning Rate. Here we follow the\nconfiguration provided in Wang et al. (2019) with a\nlarger learning rate (0.01 \u2192 0.02), a larger training\nbatch (4096 \u2192 8192), and more warmup steps\n(4000 \u2192 8000). As shown in the right part of\nFig. 6, it can improve model performance by more\nthan 0.5 BLEU points (red bars). However, after\nPTQ, the model with 0.02 learning rate performs\nsignificantly worse than 0.01 (blue bars). As shown\nin Fig. 7, the network weights and outputs become\nlarger when using a larger learning rate, which is\nnot quantization-friendly.\nL2 Regularization. To solve the above prob-\nlem, this paper adopts L2 regularization applied\nto weight (also called weight decay). It adds the\nsquared magnitude of the network weights as the\npenalty term to the original loss function and en-\nk\n1\n\u2217\nn\n4\n1\nd0\n...\n4\n(2)\n\u2217\n...\nw0\nw1\nw2\nw3\n4\nThread 2\n...\n(3)\nd0\n4\n1\nd1\nd2\nd3\n128-bit Register\nSIMD Instruction\nw0\n4\n1\nw1\nw2\nw3\nr0\n4\n1\nFigure 9: An example of processing multiple integers\nin a single SIMD instruction.\ncourage the weights to be smaller. As shown in\nthe left two parts of Fig. 8, with L2 regularization,\nboth the network weights and output values will\nbecome significantly smaller. The right part of Fig.\n8 shows the performance of PTQ when applying\ndifferent degrees of L2 regularization. The red and\nblue bars represent the model performance before\nand after PTQ. We can see that L2 regularization\ndoes improve the model performance after PTQ.\n4\nThe Engine\nThis section introduces the detailed implementa-\ntions of our inference engine.\n4.1\nGEMM Optimization\nAccording to statistics on the ONNX Runtime plat-\nform, general matrix multiplication (GEMM) ac-\ncounts for 80.44% of the overall decoding time,\ndemonstrating that optimizing GEMM is the key\nto decoding speed up. We optimize GEMM from\nthree aspects: (1) Replacing 32-bit floating points\nSystem\nParams (M)\nSize (MB)\nMemory (MB)\nLatency (ms)\nTest\nValid\nEn-De\nTransformer-big\n218 \u21911\u00d7\n872 \u21911\u00d7\n2886.6 \u21911.0\u00d7\n1281.5 \u21911.0\u00d7\n28.36 \u2206-0.00\n26.75 \u2206-0.00\nTransformer-base\n65 \u21913\u00d7\n260 \u21913\u00d7\n908.5 \u21913.2\u00d7\n332.3 \u21913.9\u00d7\n27.40 \u2206-0.96\n25.81 \u2206-0.94\nTransformer-small\n22 \u219110\u00d7\n88 \u219110\u00d7\n759.5 \u21913.8\u00d7\n158.0 \u21918.1\u00d7\n24.20 \u2206-4.61\n23.91 \u2206-2.84\nTransformer-tiny\n8 \u219127\u00d7\n32 \u219127\u00d7\n398.9 \u21917.2\u00d7\n73.0 \u219117.6\u00d7\n20.97 \u2206-7.39\n21.53 \u2206-5.22\nMobileNMT-20MB\n20 \u219111\u00d7\n20 \u219144\u00d7\n26.0 \u2191111.2\u00d7\n46.3 \u219127.7\u00d7\n27.09 \u2206-1.27\n25.72 \u2206-1.03\nMobileNMT-10MB\n10 \u219122\u00d7\n10 \u219187\u00d7\n14.9 \u2191194.0\u00d7\n27.3 \u219147.0\u00d7\n25.08 \u2206-3.28\n24.85 \u2206-1.90\nEn-Fr\nTransformer-big\n259 \u21911\u00d7\n1036 \u21911\u00d7\n2987.6 \u21911.0\u00d7\n1345.6 \u21911.0\u00d7\n39.05 \u2206-0.00\n44.12 \u2206-0.00\nTransformer-base\n86 \u21913\u00d7\n344 \u21913\u00d7\n944.8 \u21913.2\u00d7\n358.9 \u21913.7\u00d7\n38.64 \u2206-0.41\n43.80 \u2206-0.32\nTransformer-small\n22 \u219112\u00d7\n88 \u219112\u00d7\n782.3 \u21913.8\u00d7\n178.5 \u21917.5\u00d7\n34.76 \u2206-4.29\n40.01 \u2206-4.11\nTransformer-tiny\n8 \u219132\u00d7\n32 \u219132\u00d7\n418.8 \u21917.1\u00d7\n80.3 \u219116.8\u00d7\n30.36 \u2206-8.69\n36.01 \u2206-8.11\nMobileNMT-20MB\n20 \u219113\u00d7\n20 \u219152\u00d7\n26.7 \u2191111.9\u00d7\n53.7 \u219125.1\u00d7\n37.67 \u2206-1.38\n43.81 \u2206-0.31\nMobileNMT-10MB\n10 \u219126\u00d7\n10 \u2191104\u00d7\n15.8 \u2191189.1\u00d7\n28.9 \u219146.6\u00d7\n36.00 \u2206-3.05\n41.87 \u2206-2.25\nTable 4: Results on WMT14 En-De and WMT14 En-Fr tasks. These metrics are measured on Google Pixel 4.\nTransformer-big/base/small/tiny results are tested on TFLite and MobileNMT-20MB/10MB are tested on our engine.\nAll results are based on a sample with src/tgt length of 30.\nwith 8-bit integers in GEMM for model quantiza-\ntion. (2) The Arm instruction set we use allows\nmultiple integers to be processed in parallel in a\nsingle instruction, which takes full advantage of\nthe processor throughput. (3) To improve the cache\nhit and the register usage, we adjust the layout of\nthe tensor in memory to ensure that the instruction\nreads data from continuous space. Specifically, we\nconvert each 4 \u00d7 4 block in the original layout into\na contiguous vector of size 16. An example can be\nseen in Fig. 9.\n4.2\nMemory Optimization\nAs shown in Fig. 10 in the appendix C, except for\nGEMM, other operations account for only 19.56%\nof the decoding time but will be frequently per-\nformed, resulting in a large amount of temporary\nmemory. To improve memory efficiency, we take\ntwo strategies: (1) To avoid frequent memory-\nmapped I/O and footprint, our engine integrates\nall adjacent fine-grained operations between two\nGEMM operations into one fused operation. (2)\nTo save temporary memory, different operations\nare allowed to share the same space, provided that\nthese operations do not interfere with each other at\nthe same time. Through memory sharing, only two\n8-bit memory buffers, and one 32-bit buffer need\nto be pre-allocated in the Transformer encoder to\nhold intermediate results.\n5\nExperiments\n5.1\nSetups\nWe evaluate our methods on two WMT bench-\nmarks. For the WMT14 En-De task (4.5M pairs),\nwe choose newstest-2013 as the validation set and\nSystem\nParams(M) FLOPs(G) BLEU\nw/\nw/o\nTransformer-base\n65\n44\n1.9\n27.40\nDeLighT\n37\n31.4\n-\n27.60\nUniversal Transformer\nN/A\n7.4\n1.9\n26.20\nLite Transformer (small)\nN/A\n2.9\n0.2\n22.50\nLite Transformer (medium) N/A 11.7\n0.7\n25.60\nLite Transformer (big)\nN/A 17.3\n1.0\n26.50\nEdgeFormer w/o LA\nN/A\n8.6\n1.8\n26.50\nEdgeFormer (Adapter-LA) N/A\n9.4\n1.8\n26.90\nEdgeFormer (Prefix-LA)\nN/A\n8.6\n1.9\n26.80\nMobileNMT-10MB\n10\n7.9\n0.3\n25.08\nMobileNMT-20MB\n20\n17.7\n0.6\n27.09\nTable 5: The comparison of MobileNMT with other\nparameter-efficient Transformers, including DeLighT\n(Mehta et al., 2021), Universal Transformer (Dehghani\net al., 2019), Lite Transformer (Wu et al., 2020) and\nEdgeFormer (Ge et al., 2022) (Parameters w/ or w/o\nembedding layer are both provided. FLOPs is estimated\non a sample with src/tgt length of 30.).\nnewstest-2014 as the test set. For the WMT14 En-\nFr task (35M pairs), we validate the system on\nthe combination of newstest-2012 and newstest-\n2013, and test it on newstest-2014. Details of the\narchitecture were introduced in Section 2, and train-\ning hyperparameters were introduced in Section 3.\nFor model compression ratio and decoding speed\nup, we choose Transformer-big as the benchmark\n(1.0\u00d7). Other details of experimental setups are\nintroduced in Appendix D.\n5.2\nResults\nTable 4 shows the results of different sys-\ntems on WMT14 En-De and En-Fr.\nTable 5\nshows the comparison of MobileNMT with other\nparameter-efficient methods based on Transformer.\nMobileNMT-10MB and MobileNMT-20MB are\ntwo models we have built with different sizes,\nwhich are introduced in Table 3.\nOn WMT14 En-De, our MobileNMT-10MB re-\nquires only 4.6% of the parameters to maintain\n88.4% performance of Transformer-big, while it\nachieves 87.2\u00d7 compression ratio and 47.0\u00d7 speed\nup. Our MobileNMT-20MB can maintain 95.5%\nperformance of Transformer-big with only 9.2%\nparameters, while it achieves 43.6\u00d7 compression\nratio and 27.7\u00d7 speed up. Experiments on En-Fr\nshow similar results. In addition, thanks to the\nmemory optimization strategies adopted in our en-\ngine, MobileNMT requires significantly less run-\nning memory than other models (0.5%\u223c0.9% of\nTransformer-big). All these experiments demon-\nstrate that MobileNMT is efficient in terms of pa-\nrameters, computation, and memory, and can be\neasily deployed on mobile devices.\n6\nConclusion\nWe propose MobileNMT, a Transformer-based ma-\nchine translation system that can translate in 15MB\nand 30ms. It uses existing resources efficiently\nand can be easily deployed in real-world scenarios.\nWe develop a mobile inference engine with GEMM\nand memory optimization, hoping that it can bridge\nthe gap between theoretical research and real-world\napplications on efficient machine translation.\nAcknowledgments\nThis work was supported in part by the National\nScience Foundation of China (No. 62276056), the\nNational Key R&D Program of China, the China\nHTRD Center Project (No. 2020AAA0107904),\nthe Natural Science Foundation of Liaoning\nProvince of China (2022-KF-16-01), the Yunnan\nProvincial Major Science and Technology Special\nPlan Projects (No. 202103AA080015), the Funda-\nmental Research Funds for the Central Universities\n(Nos. N2216016, N2216001, and N2216002), and\nthe Program of Introducing Talents of Discipline\nto Universities, Plan 111 (No. B16009).\nLimitations\nMultilingual Translation. Here we mainly dis-\ncuss the design principles of efficient architectures\nfor bilingual machine translation. Compared with\nbilingual translation, multilingual translation tasks\nrequire significantly more parameters and compu-\ntations to perform well, and different model scales\nmay lead to different design considerations. We\nwill leave this for future exploration.\nKnowledge Distillation. As a small model that re-\nquires only 10MB/20MB of storage, MobileNMT\nwill inevitably suffer from performance loss com-\npared to other Transformer-based models. To re-\nduce performance loss, here we adopt knowledge\ndistillation and choose the Transformer-base model\nas the teacher.\nFrom a training efficiency per-\nspective, although the teacher model can help Mo-\nbileNMT improve performance, it also introduces\nadditional training costs.\nCompatibility. Here our inference engine only\nprovides implementation for the ARM CPU. We\nwill make it available for other AI accelerator (such\nas NPU) on mobile devices in the future.\nReferences\nFarhad Akhbardeh, Arkady Arkhangorodsky, Mag-\ndalena Biesialska, Ondrej Bojar, Rajen Chatter-\njee, Vishrav Chaudhary, Marta R. Costa-juss\u00e0,\nCristina Espa\u00f1a-Bonet, Angela Fan, Christian Fe-\ndermann, Markus Freitag, Yvette Graham, Ro-\nman Grundkiewicz, Barry Haddow, Leonie Harter,\nKenneth Heafield, Christopher Homan, Matthias\nHuck, Kwabena Amponsah-Kaakyire, Jungo Kasai,\nDaniel Khashabi, Kevin Knight, Tom Kocmi, Philipp\nKoehn, Nicholas Lourie, Christof Monz, Makoto\nMorishita, Masaaki Nagata, Ajay Nagesh, Toshiaki\nNakazawa, Matteo Negri, Santanu Pal, Allahsera Au-\nguste Tapo, Marco Turchi, Valentin Vydrin, and Mar-\ncos Zampieri. 2021. Findings of the 2021 confer-\nence on machine translation (WMT21). In Proceed-\nings of the Sixth Conference on Machine Translation,\nWMT@EMNLP 2021, Online Event, November 10-\n11, 2021, pages 1\u201388. Association for Computational\nLinguistics.\nLei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E.\nHinton. 2016.\nLayer normalization.\nCoRR,\nabs/1607.06450.\nAlexei Baevski and Michael Auli. 2019. Adaptive input\nrepresentations for neural language modeling. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019. OpenReview.net.\nRon Banner, Yury Nahshan, and Daniel Soudry. 2019.\nPost training 4-bit quantization of convolutional net-\nworks for rapid-deployment. In Advances in Neural\nInformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 7948\u20137956.\nYoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. 2019. Low-bit quantization of neural net-\nworks for efficient inference. In 2019 IEEE/CVF\nInternational Conference on Computer Vision Work-\nshops, ICCV Workshops 2019, Seoul, Korea (South),\nOctober 27-28, 2019, pages 3009\u20133018. IEEE.\nMatthieu Courbariaux, Yoshua Bengio, and Jean-Pierre\nDavid. 2015. Binaryconnect: Training deep neu-\nral networks with binary weights during propaga-\ntions. In Advances in Neural Information Processing\nSystems 28: Annual Conference on Neural Informa-\ntion Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 3123\u20133131.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171\u20134186. Association for Computational\nLinguistics.\nShuoyang Ding, Adithya Renduchintala, and Kevin Duh.\n2019. A call for prudent choice of subword merge\noperations in neural machine translation. In Proceed-\nings of Machine Translation Summit XVII Volume 1:\nResearch Track, MTSummit 2019, Dublin, Ireland,\nAugust 19-23, 2019, pages 204\u2013213. European Asso-\nciation for Machine Translation.\nJulian Faraone, Nicholas J. Fraser, Michaela Blott, and\nPhilip H. W. Leong. 2018. SYQ: learning symmet-\nric quantization for efficient deep neural networks.\nIn 2018 IEEE Conference on Computer Vision and\nPattern Recognition, CVPR 2018, Salt Lake City, UT,\nUSA, June 18-22, 2018, pages 4300\u20134309. Computer\nVision Foundation / IEEE Computer Society.\nTao Ge, Si-Qing Chen, and Furu Wei. 2022. Edge-\nformer: A parameter-efficient transformer for on-\ndevice seq2seq generation. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2022, Abu Dhabi,\nUnited Arab Emirates, December 7-11, 2022, pages\n10786\u201310798. Association for Computational Lin-\nguistics.\nEdouard Grave, Armand Joulin, Moustapha Ciss\u00e9,\nDavid Grangier, and Herv\u00e9 J\u00e9gou. 2017. Efficient\nsoftmax approximation for gpus. In Proceedings\nof the 34th International Conference on Machine\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, volume 70 of Proceedings of Machine\nLearning Research, pages 1302\u20131310. PMLR.\nSong Han, Huizi Mao, and William J. Dally. 2016. Deep\ncompression: Compressing deep neural network with\npruning, trained quantization and huffman coding. In\n4th International Conference on Learning Represen-\ntations, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016a. Deep residual learning for image recog-\nnition. In 2016 IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2016, Las Vegas, NV,\nUSA, June 27-30, 2016, pages 770\u2013778. IEEE Com-\nputer Society.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. 2016. Binarized neu-\nral networks. In Advances in Neural Information\nProcessing Systems 29: Annual Conference on Neu-\nral Information Processing Systems 2016, December\n5-10, 2016, Barcelona, Spain, pages 4107\u20134115.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Menglong\nZhu, Matthew Tang, Andrew G. Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018.\nQuanti-\nzation and training of neural networks for efficient\ninteger-arithmetic-only inference. In 2018 IEEE Con-\nference on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 2704\u20132713.\nJangho Kim, Yash Bhalgat, Jinwon Lee, Chirag Patel,\nand Nojun Kwak. 2019. QKD: quantization-aware\nknowledge distillation. CoRR, abs/1911.12491.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nJunghyup Lee, Dohyung Kim, and Bumsub Ham. 2021.\nNetwork quantization with element-wise gradient\nscaling. In IEEE Conference on Computer Vision\nand Pattern Recognition, CVPR 2021, virtual, June\n19-25, 2021, pages 6448\u20136457. Computer Vision\nFoundation / IEEE.\nYe Lin, Yanyang Li, Tong Xiao, and Jingbo Zhu. 2021.\nBag of tricks for optimizing transformer efficiency.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, Virtual Event / Punta\nCana, Dominican Republic, 16-20 November, 2021,\npages 4227\u20134233. Association for Computational\nLinguistics.\nXiaodong Liu, Kevin Duh, Liyuan Liu, and Jianfeng\nGao. 2020. Very deep transformers for neural ma-\nchine translation. CoRR, abs/2008.07772.\nZechun Liu, Kwang-Ting Cheng, Dong Huang, Eric P.\nXing, and Zhiqiang Shen. 2022.\nNonuniform-to-\nuniform quantization: Towards accurate quantiza-\ntion via generalized straight-through estimation. In\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, CVPR 2022, New Orleans, LA,\nUSA, June 18-24, 2022, pages 4932\u20134942. IEEE.\nJonathan Long, Evan Shelhamer, and Trevor Darrell.\n2015. Fully convolutional networks for semantic\nsegmentation. In IEEE Conference on Computer\nVision and Pattern Recognition, CVPR 2015, Boston,\nMA, USA, June 7-12, 2015, pages 3431\u20133440. IEEE\nComputer Society.\nSachin Mehta, Marjan Ghazvininejad, Srinivasan Iyer,\nLuke Zettlemoyer, and Hannaneh Hajishirzi. 2021.\nDelight: Deep and light-weight transformer. In 9th\nInternational Conference on Learning Representa-\ntions, ICLR 2021, Virtual Event, Austria, May 3-7,\n2021. OpenReview.net.\nPaulius Micikevicius, Sharan Narang, Jonah Alben,\nGregory F. Diamos, Erich Elsen, David Garc\u00eda,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev,\nGanesh Venkatesh, and Hao Wu. 2018. Mixed pre-\ncision training. In 6th International Conference on\nLearning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Conference\nTrack Proceedings.\nAsit K. Mishra and Debbie Marr. 2018. Apprentice:\nUsing knowledge distillation techniques to improve\nlow-precision network accuracy. In 6th International\nConference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net.\nToan Q. Nguyen and Julian Salazar. 2019. Transformers\nwithout tears: Improving the normalization of self-\nattention. CoRR, abs/1910.05895.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling neural machine translation. In\nProceedings of the Third Conference on Machine\nTranslation: Research Papers, WMT 2018, Belgium,\nBrussels, October 31 - November 1, 2018, pages 1\u20139.\nAssociation for Computational Linguistics.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, WMT 2018,\nBelgium, Brussels, October 31 - November 1, 2018,\npages 186\u2013191. Association for Computational Lin-\nguistics.\nJerry Quinn and Miguel Ballesteros. 2018. Pieces of\neight: 8-bit neural machine translation. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, NAACL-\nHLT 2018, New Orleans, Louisiana, USA, June 1-6,\n2018, Volume 3 (Industry Papers), pages 114\u2013120.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers. The Association for\nComputer Linguistics.\nWonyong Sung, Sungho Shin, and Kyuyeon Hwang.\n2015.\nResiliency of deep neural networks under\nquantization. CoRR, abs/1511.06488.\nSho Takase and Shun Kiyono. 2021. Lessons on pa-\nrameter sharing across layers in transformers. CoRR,\nabs/2104.06022.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 December\n2017, Long Beach, CA, USA, pages 5998\u20136008.\nHongyu Wang, Shuming Ma, Li Dong, Shaohan Huang,\nDongdong Zhang, and Furu Wei. 2022.\nDeep-\nnet: Scaling transformers to 1, 000 layers. CoRR,\nabs/2203.00555.\nQiang Wang,\nBei Li,\nTong Xiao,\nJingbo Zhu,\nChangliang Li, Derek F. Wong, and Lidia S. Chao.\n2019. Learning deep transformer models for machine\ntranslation. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 1810\u20131822. Association\nfor Computational Linguistics.\nEphrem Wu. 2020.\nLearning accurate integer\ntransformer machine-translation models.\nCoRR,\nabs/2001.00926.\nZhanghao Wu, Zhijian Liu, Ji Lin, Yujun Lin, and Song\nHan. 2020. Lite transformer with long-short range at-\ntention. In 8th International Conference on Learning\nRepresentations, ICLR 2020, Addis Ababa, Ethiopia,\nApril 26-30, 2020. OpenReview.net.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng,\nShuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 10524\u201310533. PMLR.\nJingjing Xu, Hao Zhou, Chun Gan, Zaixiang Zheng,\nand Lei Li. 2021. Vocabulary learning via optimal\ntransport for neural machine translation. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 7361\u20137373. Associa-\ntion for Computational Linguistics.\nA\nTransformer Architecture\nWe chose Transformer for study because it is one\nof the most successful neural models for machine\ntranslation. It consists of a N-layer encoder and\na M-layer decoder, where N=M=6 in the origi-\nnal Transformer-base and Transformer-big. Each\nencoder layer consists of two sublayers, includ-\ning the self-attention and feed-forward network\n(FFN). Each decoder layer has an additional cross-\nattention sublayer to bridge the encoder and de-\ncoder.\nThe self-attention takes the output X of the pre-\nvious sublayer as its input. The cross-attention is\nsimilar to the self-attention, except that it takes the\nencoder output as an additional input. Both types\nof attention first compute the attention distribution\nAx and then average X by Ax. We denote the trans-\nformation matrices of Q, K, V as Wq, Wk, Wv, the\nsubsequent transformation matrices as Wo, and the\nattention as Ya = Attn(X), then:\nAx = SoftMax(XWqW T\nk XT\n\u221a\nd\n)\n(1)\nYa = AxXWvWo\n(2)\nThe FFN applies non-linear transformation to its\ninput X. We denote the FFN as Yf = FFN(X):\nYf = ReLU(XW1 + b1)W2 + b2\n(3)\nwhere W1 and b1 denote the weight and bias of the\nfirst linear transformation, W2 and b2 are parame-\nters of the second transformation.\nHere we preprocess each sublayer input by the\nlayer normalization (Ba et al., 2016). All sublayers\nare coupled with the residual connection (He et al.,\n2016a).\nB\nPTQ and QAT\nAs an appealing solution to model compression,\nquantization enables the model to use lower-bit\nvalues (such as 8-bit integer) to compute faster and\nconsume less storage space (Hubara et al., 2016;\nMicikevicius et al., 2018; Quinn and Ballesteros,\n2018; Jacob et al., 2018).\nPost-Training Quantization (PTQ) can be seen as\nthe basis for Quantization Aware Training (QAT),\nit adds quantization nodes to a well-trained floating-\npoint model. To quantize a floating-point tensor r\nto a tensor with n bits, a scale s is introduced to\nmap these two types of values (Wu, 2020):\ns = max(r) \u2212 min(r)\n2n \u2212 1\n(4)\nSystem\nParams (M)\nSize (MB)\nBLEU\nTransformer-base\n65\n260\n27.40\n+ Reducing Vocab\n48\n192\n26.20\n+ Reducing Width\n10\n40\n22.01\n+ Other Dimensions\n10\n40\n22.54\n+ Distillation\n10\n40\n23.77\n+ Quantization\n10\n10\n23.76\n+ Hyperparameters\n10\n10\n25.48\n+ Greedy Search\n10\n10\n25.08\nTable 6: Ablation study on MobileNMT-10MB. The\ncolors refer to\nModel Architecture\nin Section 2,\nTraining Strategies in Section 3 and Greedy Search.\nTo get a faster computation speed, both weights\nand activations will be quantized to n-bit. Suppose\nrm = min(r), the quantization function is:\nQ(r) = \u230a(r \u2212 rm)/s\u2309 \u00d7 s + rm\n(5)\nwhere \u230a\u00b7\u2309 represents rounding to the nearest integer.\nHowever, in PTQ, applying quantization directly\nto the floating-point network will result in signif-\nicant performance losses. Based on PTQ, QAT\nsimulates the behavior of n-bit computation by min-\nimizing quantization errors during training, which\nhelps the model achieve higher accuracy. In addi-\ntion to the learnable weights of the model itself, s\nis also learnable.\nC\nOperations except GEMM\nSince general matrix multiplication (GEMM) ac-\ncounts for 80.44% of the overall decoding time, we\nhave concluded that optimizing GEMM is the key\nto decoding speed up in Section 4. As for opera-\ntions except GEMM, Fig. 10 shows the proportion\nof running time in the decoding process. The corre-\nsponding data is measured in 32-bit floating point\nformat on the ONNX Runtime.\nD\nSetups\nAll sentences were segmented into sequences of\nsub-word units (Sennrich et al., 2016). In the im-\nplementation, we adopt the normalization before\nlayers (Baevski and Auli, 2019; Xiong et al., 2020;\nNguyen and Salazar, 2019). Most previous work\nonly shared source and target vocabularies on the\nEn-De task. In our MobileNMT, both En-De and\nEn-Fr adopt shared vocabularies for efficiency rea-\nsons, which leads to a larger compression gain at\nthe expense of performance. We test on the model\nensemble by averaging the last 5 checkpoints and\nreport SacreBLEU scores (Post, 2018).\n0\n1\n2\n3\n4\n5\n6\n7\n\u00b710\u22122\nIDENTITY\nSLICE\nCUMSUM\nEXPAND\nWHERE\nMULTI_HEAD\nHOST_CONVERTER\nSHAPE\nSPLIT\nUNSQUEEZE\nCONCAT\nCONSTANT_OF_SHAPE\nMULTI_HEAD_TRANS\nBINARY_MUL\nREDUCE\nBINARY_UNKNOWN\nGATHER\nCONVERTER\nBINARY_ADD\nLAYER_NORM\nARGMAX\nDECODER_ATTENTION\nDECODER_SELF_ATTENTION\nENCODER_SELF_ATTENTION\n2.09 \u00b7 10\u22126\n2.55 \u00b7 10\u22126\n4.79 \u00b7 10\u22126\n1.22 \u00b7 10\u22125\n2.45 \u00b7 10\u22125\n5.52 \u00b7 10\u22125\n6.28 \u00b7 10\u22125\n7.1 \u00b7 10\u22125\n9.49 \u00b7 10\u22125\n1.13 \u00b7 10\u22124\n1.52 \u00b7 10\u22124\n1.91 \u00b7 10\u22124\n2 \u00b7 10\u22124\n2.01 \u00b7 10\u22124\n3.57 \u00b7 10\u22124\n4.13 \u00b7 10\u22124\n4.63 \u00b7 10\u22124\n2.99 \u00b7 10\u22123\n4.88 \u00b7 10\u22123\n5.36 \u00b7 10\u22123\n1.88 \u00b7 10\u22122\n1.96 \u00b7 10\u22122\n3.76 \u00b7 10\u22122\n5.89 \u00b7 10\u22122\nPercentage [%]\nFigure 10: Proportions of different operations (except GEMM) on the Transformer-base model.\nSystem\nParams\nBits\nSize\nBLEU\n(M)\n(W-E-A)\n(MB)\nTransformer-base\n65\n32-32-32\n260\n27.40\nMobileNMT-10MB\n10\n32-32-32\n40\n25.79\n10\n8-8-8\n10\n25.08\n10\n4-8-8\n5\n25.43\n10\n3-8-8\n3.75\n24.09\n10\n2-8-8\n2.5\n21.25\nMobileNMT-20MB\n20\n32-32-32\n80\n27.30\n20\n8-8-8\n20\n27.09\n20\n4-8-8\n10\n26.96\n20\n3-8-8\n7.5\n26.23\n20\n2-8-8\n5\n24.33\nTable 7: Results of quantizing weights to lower bits.\nFor the experiments of MobileNMT in Table 4,\nwe use the greedy search algorithm in our engine.\nCompared with beam search, greedy search can\nlead to more efficient decoding. For the experi-\nments of TFLite in Table 4, since TFLite will ex-\npand all loop subgraphs, it is hard to support the en-\ntire decoding process (30 steps) of the Transformer-\nbig/base model with limited memory (6GB in\nGoogle Pixel 4). For the memory of these two\nmodels, we only record the running memory of 1\nstep. For the corresponding latencies, we estimate\nthe 30-step latency according to the 1-step and 5-\nstep latencies. It is worth noting that except for the\nmemory and latency on Transformer-big/base, all\nother data statistics are measured in real-world.\nE\nAnalysis\nE.1\nAblation Study\nTable 6 summarizes how each part of Section 2 and\nSection 3 affects the overall performance. Each\nrow in Table 6 represents the result of applying the\ncurrent part to the system obtained in the previous\nrow.\nTo reduce the model parameters from 65M to\n10M, the model performance decreased from 27.40\nto 22.54, which illustrates the importance of net-\nwork parameters on model capacity. We observe\nthat both knowledge distillation and tuning hyper-\nparameters can bring significant performance im-\nprovements (from 22.54 to 25.48), which effec-\ntively compensate for the performance loss caused\nby parameter reduction.\nE.2\nQuantization Study\nTable 7 studies how performance changes when\nquantizing the model to lower bits (i.e., 4-bit, 3-bit,\nand 2-bit). As introduced in Section 3.2, for 8-bit\nquantization, we use the element-wise quantization\nmethod (Lee et al., 2021). For lower-bit quantiza-\ntion, we use the row-wise quantization for accuracy\npreservation (Faraone et al., 2018).\nAs shown in Table 7, 8-bit and 4-bit quantization\nhave almost no negative effect on model perfor-\nmance. When quantizing the model to lower bits,\nsuch as 3-bit and 2-bit integers, model performance\nwill drop dramatically.\n"
  },
  {
    "title": "PromptBench: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
    "link": "https://arxiv.org/pdf/2306.04528.pdf",
    "upvote": "3",
    "text": "PromptBench: Towards Evaluating the Robustness of Large\nLanguage Models on Adversarial Prompts\nKaijie Zhu1,2\u2217, Jindong Wang1\u2020, Jiaheng Zhou2, Zeek Wang1, Hao Chen3, Yidong Wang4,\nLinyi Yang5, Wei Ye4, Yue Zhang5, Neil Zhenqiang Gong6, Xing Xie1\n1Microsoft Research\n2Institute of Automation, CAS\n3Carnegie Mellon University\n4Peking University\n5Westlake University\n6Duke University\nAbstract\nThe increasing reliance on Large Language Models (LLMs) across academia and industry necessitates a\ncomprehensive understanding of their robustness to prompts. In response to this vital need, we introduce\nPromptBench, a robustness benchmark designed to measure LLMs\u2019 resilience to adversarial prompts. This\nstudy uses a plethora of adversarial textual attacks targeting prompts across multiple levels: character,\nword, sentence, and semantic. The adversarial prompts, crafted to mimic plausible user errors like typos\nor synonyms, aim to evaluate how slight deviations can affect LLM outcomes while maintaining semantic\nintegrity. These prompts are then employed in diverse tasks including sentiment analysis, natural language\ninference, reading comprehension, machine translation, and math problem-solving. Our study generates\n4, 788 adversarial prompts, meticulously evaluated over 8 tasks and 13 datasets. Our findings demonstrate\nthat contemporary LLMs are not robust to adversarial prompts. Furthermore, we present comprehensive\nanalysis to understand the mystery behind prompt robustness and its transferability. We then offer\ninsightful robustness analysis and pragmatic recommendations for prompt composition, beneficial to both\nresearchers and everyday users. Code is available at: https://github.com/microsoft/promptbench.\n1\nIntroduction\nLarge language models (LLMs) have gained increasing popularity owing to their unprecedented performance\nin various tasks such as sentiment analysis [67], question answering [67], logical reasoning [37], etc. An input\nto an LLM is the concatenation of a prompt and (optionally) a sample, where the prompt aims to instruct\nthe LLM what task to perform and the sample is the data to be analyzed in the task. Given an input, an\nLLM returns a response. Figure 1 shows several examples of prompt, sample, and response when different\nusers use LLMs for different tasks. Note that a sample is optional in certain tasks. For instance, in a task to\nwrite a country love story, a prompt \u201cPlease write a story about country love\u201d alone is sufficient.\nGiven the popular adoption of LLMs, particularly in safety-critical and decision-making domains, it\nbecomes essential to examine the robustness of LLMs to perturbations in an input.\nIndeed, existing\nwork [44,70,71,79,83] has attempted to evaluate the robustness of LLMs from different perspectives. For\ninstance, AdvGLUE [70] and ANLI [44] are two public datasets to evaluate the robustness of language models\nto adversarial samples, which are carefully perturbed samples to make a language model produce incorrect\nresponses. In the era of large language models, Wang et al. [71] evaluated ChatGPT and other LLMs with\nrespect to their robustness to adversarial samples and out-of-distribution (OOD) samples. Zhuo et al. [83]\nevaluated the robustness of LLMs for a particular task called semantic parsing.\nThese studies demonstrated that current LLMs are not robust to adversarial and OOD samples for some\npopular natural language processing tasks. However, in some application scenarios, an input only consists of\na prompt without the need of a sample, making existing studies on robustness to adversarial samples not\napplicable. Moreover, a single prompt is often used to instruct an LLM to perform a task for multiple samples.\nFor instance, in a math-problem task (shown in Figure 1), a prompt can be used for multiple samples (i.e.,\n\u2217Work done during internship at Microsoft Research Asia.\n\u2020Corresponding author: Jindong Wang (jindong.wang@microsoft.com).\n1\narXiv:2306.04528v4  [cs.CL]  18 Oct 2023\nAs a mathematics instructor, calculate \nthe answer to the following problem \nrelated to if a number is a prime:\nQuestion: Let z(a) = -871*a \n+ 415. Is z(-16) a \ncomposite number? Answer:\nPrompt\nSample\nUser 1\nUser 2\nYes. \nAs a mathematics instrector, calculate \nthe ansxer to the following problem \nrelated to if a number is a prime:\nQuestion: Let z(a) = -871*a \n+ 415. Is z(-16) a \ncomposite number? Answer:\nPrompt\nSample\nNo. \nReview this statement and decide whether it \nhas a 'positive' or 'negative' sentiment: \nit 's slow -- very , \nvery slow .\nPrompt\nSample\nUser 1\nNegative. \nAnalyze this assertion and defining whether \nit is a 'positive' or 'negative' sentiment:\nit 's slow -- very , \nvery slow .\nPrompt\nSample\nPostive. \nUser 2\n(a) Typos lead to errors in math problems.\n(b) Synonyms lead to errors in sentiment analysis problems.\nFigure 1: Two examples showing that current LLMs are not robust to prompts: typos and synonyms lead to\nerrors in math and sentiment analysis problems. The characters and words marked with red are perturbations.\nmath problems). Therefore, a perturbed prompt may make an LLM output incorrect responses for multiple\nclean samples. As a result, a perturbed prompt arguably has a larger impact on LLMs than an adversarial\nsample, since the latter only influences the response of an LLM for a single sample. However, despite its\npivotal importance, the robustness of LLMs to perturbations in prompts is largely unexplored.\nIn this paper, we aim to bridge the gap by introducing PromptBench, a comprehensive benchmark\ndesigned for evaluating the robustness of LLMs to perturbations in prompts, understanding the factors that\ncontribute to their robustness (or lack thereof), and identifying the key attributes of robust prompts. We\nconsider a variety of prompt perturbations including 1) minor typos, synonyms, and different ways to express\nsentences with the same semantic meaning, which may commonly occur to normal users or developers in their\ndaily use of LLMs in non-adversarial settings, as well as 2) perturbations strategically crafted by attackers in\nadversarial settings. With a slight abuse of terminology, we call such a perturbed prompt in both scenarios\nadversarial prompt. Figure 1 shows examples of adversarial prompts with typos and synonyms, for which the\nLLMs produce incorrect responses.\nAs shown in Figure 2, PromptBench consists of prompts, attacks, models, tasks, datasets, and analysis.\nSpecifically, we evaluate 4 types of prompts: zero-shot (ZS), few-shot (FS), role-oriented, and task-oriented\nprompts. We create 4 types of attacks (called prompt attacks) to craft adversarial prompts: character-level,\nword-level, sentence-level, and semantic-level attacks by extending 7 adversarial attacks [21,30,34,35,43,55]\nthat were originally designed to generate adversarial samples. We note that, although we call them attacks,\ntheir generated adversarial prompts also serve as testbeds for mimicking potential diverse prompts with\nnaturally occurred perturbations from real LLM users. PromptBench spans across 9 prevalent LLMs, ranging\nfrom smaller models such as Flan-T5-large [11] to larger ones like ChatGPT [45] and GPT-4 [46]. Moreover,\nwe select 8 tasks for evaluation, namely, sentiment analysis (SST-2 [60]), grammar correctness (CoLA [73]),\nduplicate sentence detection (QQP [72] and MRPC [17]), natural language inference (MNLI [75], QNLI [67],\nRTE [67], and WNLI [33]), multi-task knowledge (MMLU [26]), reading comprehension (SQuAD V2 [52]),\ntranslation (UN Multi [19] and IWSLT 2017 [8]), and math problem-solving (Mathematics [56]). In total, we\ncreated 4, 788 adversarial prompts, representing diverse, practical, and challenging scenarios.\nWe carry out extensive experiments and analysis using PromptBench. The results highlight a prevailing\nlack of robustness to adversarial prompts among current LLMs, with word-level attacks proving the most\neffective (39% average performance drop in all tasks). We delve into the reasons behind this vulnerability\nby exploring LLMs\u2019 attention weights of each word in inputs for erroneous responses associated with clean\nand adversarial inputs, where an adversarial input is the concatenation of an adversarial prompt and a\nclean sample. Our findings reveal that adversarial prompts cause LLMs to shift their focus towards the\nperturbed elements thus producing wrong responses. We also examine the transferability of adversarial\nprompts between models, and suggest a successful transferability of adversarial prompts from one LLM to\nanother. Furthermore, we analyze word frequency patterns to guide future research in improving robustness\nand to aid end-users in crafting more robust prompts. We conclude by discussing potential strategies for\n2\nPromptBench\nPrompts\nTask-oriented\nRole-oriented\nZero-shot\nFew-shot\n\u2026\u2026\nAttacks\nCharacter-level\nDeepWordBug\nTextBugger\nWord-level\nTextFooler\nBertAttack\nSentence-level\nCheckList\nStressTest\nSemantic-level\nHuman-crafted\n\u2026\u2026\nTasks\nSentiment analysis\nGrammar correctness\nDuplicate sentence\ndetection\nNatural language\ninference\nMulti-task knowledge\nReading\ncomprehension\nTranslation\nMath\n\u2026\u2026\nModels\nFlan-T5-large (0.8B)\nDolly-6B\nCerebras-13B\nLLaMA-13B\nLlama2-13B-chat\nVicuna-13B\nGPT-NEOX-20B\nFlan-UL2 (20B)\nChatGPT\nGPT-4\n\u2026\u2026\nDatasets\nGLUE(SST-2, CoLA,\nQQP, MRPC, MNLI,\nQNLI, RET, WNLI)\nMMLU\nSQuAD V2\nUN Multi\nIWSLT 2017\nMathematics\n\u2026\u2026\nAnalysis\nBenchmark \nresults\nVisualization \nanalysis\nTransferability \nanalysis\nWord frequency \nanalysis\n\u2026\u2026\nFigure 2: The components of PromptBench.\nrobustness enhancement.\nTo summarize, our contributions are as follows:\n1. We introduce PromptBench, the first systematic benchmark for evaluating, understanding, and analyzing\nthe robustness of LLMs to adversarial prompts.\n2. We conduct comprehensive evaluations on the robustness of LLMs to adversarial prompts and perform\nextensive analysis, including visual explanations for the observed vulnerabilities, transferability analysis of\nadversarial prompts, and word frequency analysis to offer practical guidance for downstream users and\nprompt engineers to craft more robust prompts.\n3. In an effort to stimulate future research on LLMs\u2019 robustness, we also build a visualization website\n(Appendix E) to allow for easy exploration of adversarial prompts. We will make our code, compiled\nprompts, website, and evaluation benchmark available to the public.\n2\nPromptBench\nIn this section, we introduce the basic modules of PromptBench: prompts, models, tasks, datasets, attacks,\nand analysis.\n2.1\nPrompts and models\nWe investigate four different types of prompts categorized based on their intended purpose and the amount\nof labeled samples they require. Task-oriented prompts explicitly describe the task the model is required\nto perform, which encourages the model to generate task-specific outputs based solely on its pre-training\nknowledge. While role-oriented prompts typically frame the model as an entity with a specific role, such\nas an expert, advisor, or translator. By incorporating role information, these prompts aim to implicitly\nconvey the expected output format and behavior. Each of the two categories of prompts can be designed for\nboth zero-shot (ZS) and few-shot (FS) learning scenarios. In the zero-shot scenario, an input is defined\nas [P, x], where P denotes a prompt, x is a sample, and [, ] denotes the concatenation operation. For the\nfew-shot scenario, a few examples are added to the input, resulting in the format [P, E, x], where E represents\nthe examples. For instance, E = {[x1, y1], [x2, y2], [x3, y3]} represents three examples in a three-shot learning\nscenario. In our experiments, we randomly select three examples in the training set of a task and append\nthem to a prompt. Table 1 shows examples of different types of prompts.\nOur evaluation includes a diverse set of LLMs to comprehensively assess their performance across various\ntasks and domains.\nThe models we consider are as follows: Flan-T5-large [11] (0.8B), Dolly-6B [14],\n3\nTable 1: Examples of 4 types of prompts.\nZero\nshot\nTask\noriented\nEvaluate the sentiment of the given text and classify it as\n\u2019positive\u2019 or \u2019negative\u2019:\nRole\noriented\nIn the role of a sentiment analysis tool, respond with\n\u2019positive\u2019 or \u2019negative\u2019 to classify this statement:\nFew\nshot\nTask\noriented\nAnalyze the tone of this statement and respond with\neither \u2019positive\u2019 or \u2019negative\u2019.\nHere are three examples.\nSentence:\nhide new secretions from the parental units.\nAnswer:\nnegative.\nSentence:\ncontains no wit , only\nlabored gags.\nAnswer:\nnegative.\nSentence:\nthat loves\nits characters and communicates something rather beautiful\nabout human nature.\nAnswer:\npositive.\nRole\noriented\nAs a sentiment classifier, determine whether the following\ntext is \u2019positive\u2019 or \u2019negative\u2019.\nHere are three examples.\nSentence:\nhide new secretions from the parental units.\nAnswer:\nnegative.\nSentence:\ncontains no wit , only\nlabored gags.\nAnswer:\nnegative.\nSentence:\nthat loves\nits characters and communicates something rather beautiful\nabout human nature.\nAnswer:\npositive.\nVicuna-13B [9], Llama2-13b-chat [63], Cerebras-GPT-13B [16], GPT-NEOX-20B [3], Flan-UL2 (20B) [4],\nChatGPT [45], and GPT-4 [46].1 By incorporating LLMs with different architectures and sizes, we aim to\nprovide insights into their strengths and weaknesses, ultimately facilitating model selection for a specific\napplication or use case. Details of these LLMs are in Appendix A.1. Note that PromptBench is flexible and\nsupports all other LLMs by simply extending the interface.\n2.2\nAttacks\nMultiple textual adversarial attacks were designed to generate adversarial samples [21,30,34,35,43,55,81].\nTechnically speaking, given a single sample x and its ground-truth label y, a textual adversarial attack aims\nto find a perturbation \u03b4 such that an LLM f\u03b8 produces an incorrect response. Formally, \u03b4 is found by solving\nthe following optimization problem: max\u03b4\u2208C L[f\u03b8(x + \u03b4); y], where x + \u03b4 is the adversarial sample, f\u03b8(x + \u03b4)\nis the response of the LLM when taking the adversarial sample alone as input, C indicates the constraints for\nthe perturbation \u03b4, and L represents a loss function.\n2.2.1\nPrompt attack\nIn this paper, our focus is to attack the prompts rather than samples. This is due to the popularity of LLMs in\ndifferent applications, which generate responses using in-context learning on prompts (i.e., instructions) and\nsamples. Prompts are either input by users or generated by the system or developers. Moreover, the ultimate\npurpose of performing such \u201cattack\u201d is actually not to genuinely attack the models, but to simulate possible\nperturbations that may naturally occur in real situations. Table 2 shows multiple prompts generated by\nadversarial approaches that are used to mimic possible user prompts, which are popular errors or expressions\nmade by users. Since users can make different mistakes while inputting prompts, such as typos, different word\nusage, different sentence styles, etc., the study on the prompt robustness is necessary to understand LLMs.\nWe denote an input to LLMs as [P, x], where P is a prompt, x is a sample, and [, ] denotes concatenation.\nNote that in the few-shot learning scenario, a few examples are appended to the prompt; and the sample\nx is optional in certain application scenarios. Our prompt attack can also be extended to such scenarios,\nbut we use the notation [P, x] for simplicity. Given a dataset D = {(xi, yi)}i\u2208[N] with N samples and their\nground-truth labels, a prompt attack aims to perturb P such that an LLM f\u03b8 produces incorrect responses\nfor all samples in the dataset D. Formally, we define a prompt attack as follows:\nDefinition 2.1 (Prompt Attack). Given an LLM f\u03b8, a dataset D = {(xi, yi)}i\u2208[N], and a clean prompt P,\nthe objective of a prompt attack can be formulated as follows:\nmax\n\u03b4\u2208C\nX\n(x;y)\u2208D\nL[f\u03b8([P + \u03b4, x]), y],\n(1)\n1We did not perform prompt attacks on GPT-4 by optimizing the adversarial algorithms since it requires massive rounds\nof communications and is too costly. We used the adversarial prompts generated by ChatGPT to evaluate GPT-4 since the\nadversarial prompts can be transferred (Sec. 4.4).\n4\nTable 2: Example of adversarial prompts generated by 7 prompt attacks to mimic possible prompts. The\ncharacters and words marked with red are generated by prompt attacks.\nClean\nAs a mathematics instructor, calculate the answer to the\nfollowing problem related to {}:\nTextBugger\nAs a mathematics instructorr, calculate the answers to\nthe following problem related to {}:\nDeepWordBug\nAs a mathematics iestructor, calculate the answex to the\nfollowing problem related to {}:\nTextFooler\nAs a mathematics prof, calculate the address to the\nfollowing problem related to {}:\nBertAttack\nAs a mathematics instructor, calculate the sum to the\nfollowing problem related to {}:\nCheckList\nAs a mathematics instructor, calculate the answer to the\nfollowing problem related to KjPJJ2a7RB {}:\nStressTest\nAs a mathematics instructor, calculate the answer to the\nfollowing problem related to and false is not true {}:\nSemantic\nCompute the result of {}.\nwhere \u03b4 is the textual perturbation added to the clean prompt P and C is the allowable perturbation set,\ni.e., perturbation constraint. We note that this attack is analogous to universal adversarial perturbation\n(UAP) [5,41] and universal adversarial trigger (UAT) [65], extending these concepts to the realm of prompts.\n2.2.2\nDifferent attacks\nWe then modify the existing black-box textual attacks to implement Eq. (1) due to their efficiency and no\nreliance on the model gradient. Thus, both open-sourced and proprietary LLMs can be the attack targets.\nOur instantiations span four distinct levels, capturing a broad spectrum of complexities from simple character\nmanipulations to sophisticated semantic alterations. The example of each attack is presented in Table 2. The\ndetails of each attack are shown in Appendix B.1.\n\u2022 Character-level: We employ TextBugger [34] and DeepWordBug [21], which manipulate texts by\nintroducing typos or errors to words, e.g., by adding, deleting, repeating, replacing, and permuting\ncharacters for certain words.\n\u2022 Word-level: We use BertAttack [35] and TextFooler [30], which aim to replace words with synonyms or\ncontextually similar words to deceive LLMs.\n\u2022 Sentence-level: We implement StressTest [43] and CheckList [55], which append irrelevant or extraneous\nsentences to the end of prompts, intending to distract LLMs. For the StressTest attack, we adopt similar\nsettings to those in [67], appending \u201cand true is true\u201d, \u201cand false is not true\u201d, or \u201cand true is\ntrue\u201d for five times to the end of a prompt. For the CheckList attack, we generate 50 random sequences\nconsisting of alphabets and digits, each with a length of 10, and append this random sequence to the end\nof a prompt.\n\u2022 Semantic-level: We simulate the linguistic behavior of people from different countries by choosing 6\ncommon languages (Chinese, French, Arabic, Spanish, Japanese, and Korean) and constructing 10 prompts\nfor each language per dataset. These prompts are then translated into English, introducing linguistic\nnuances and variations that could potentially impact LLMs.\nNote that in the context of character-level and word-level adversarial attacks, the attack algorithms\ninitiate by ascertaining the importance of each word within the prompt (except those task-essential words\nmentioned below). We determine a word\u2019s importance by removing it and observing how much the prediction\naccuracy drops. A substantial drop in the score signifies the word\u2019s criticality to the prompt. Proceeding\nfrom the most salient word, the algorithm proposes alternative perturbations for each word; for instance,\nselecting synonyms in word-level attacks. Each alternative is then assessed to gauge its potential to impair the\nmodel\u2019s performance. This evaluative process continues iteratively until the predetermined attack objective is\nachieved or there are no more alternatives for each word. In the realm of sentence-level and semantic-level\nattacks, the methodology is straightforward: each adversarial prompt is assessed using its respective attack\nalgorithm, and the most effective in undermining the model is selected.\n5\nV1\nV2\nV3\nV4\nV5\nVolunteer\n0\n20\n40\n60\n80\n100\nAcceptable rate (%)\nCharacter\nWord\nSemantic\nFigure 3: Results of human study on semantic preserving of the adversarial prompts. The dotted red line\n(> 85%) is the average of all volunteers on all attacks.\nWe impose additional restrictions on the perturbations, prohibiting alterations to certain task-essential\nwords. For instance, in translation tasks, the word \u2018translation\u2019 is preserved, while for sentiment classification\ntasks, pivotal sentiment labels such as \u2018positive\u2019 and \u2018negative\u2019 remain untouched. Moreover, in the few-shot\nlearning scenario, the few-shot examples are also exempt from adversarial attacks.\n2.2.3\nSemantic-preserving of adversarial prompts\nAre adversarial prompts realistic? The aim of prompt attacks is to simulate plausible user errors; thus, it\nis imperative that these prompts preserve semantic integrity, ensuring they remain both acceptable and\nimperceptible to human comprehension. Therefore, it is of paramount importance that our adversarially\nengineered prompts retain coherence and realism, thereby ensuring a practical relevance to our research in\nthe context of real-world language model applications.\nTo address the challenges associated with word-level attacks, we have diligently fine-tuned the hyperparam-\neters of each attack approach, thus striving to maintain semantic continuity. Then, we conduct a human study\nto recruit five volunteers to judge if the generated adversarial prompts can preserve semantics. The evaluators\nwere presented with the original prompt P juxtaposed with its adversarial version \u00afP, and were tasked with\ndetermining their semantic congruence. Sentence-level attacks are excluded in this study since they do not\nchange the original prompts, but only to add extra perturbations in the end. The detailed requirements\nand examples of acceptable and unacceptable prompts are shown in Appendix F. The results in Figure 3\ndemonstrate that these adversarial prompts generated by character-level, word-level and semantic-level attacks\nare at least 85% acceptable by humans, indicating that our attack is realistic and meaningful.\n2.3\nTasks and datasets\nCurrently, PromptBench consists of 8 diverse tasks with 13 public datasets (details are in Appendix A.2) and\nnew datasets can be easily integrated:\n\u2022 Sentiment analysis: we adopt the SST-2 [60] dataset from the GLUE [67] dataset.\n\u2022 Grammar correctness: we adopt the CoLA [73] dataset from the GLUE dataset.\n\u2022 Duplicate sentence detection: we adopt the QQP [72] and MRPC [17] datasets from GLUE.\n\u2022 Natural language inference: MNLI [75], QNLI [67], RTE [67], and WNLI [33] from GLUE.\n\u2022 Multi-task knowledge: we adopt the MMLU dataset [26] which evaluates world knowledge and problem-\nsolving abilities through 57 tasks with multiple-choice questions from diverse domains.\n\u2022 Reading comprehension: we adopt the SQuAD V2 dataset [52]. SQuAD V2 enhances the original SQuAD\ndataset for machine reading comprehension by introducing unanswerable questions.\n\u2022 Translation: we adopt UN Multi [19] and IWSLT 2017 [8] datasets. UN Multi evaluates LLMs\u2019 ability to\ntranslate official documents, while IWSLT 2017 evaluates spoken language translation.\n6\nTable 3: Statistics of datasets used in this paper.\nTask\nDataset\n#Sample\n#Class\n#[Adv. prompt, sample]\nSentiment analysis\nSST2\n872\n2\n73,248\nGrammar correctness\nCoLA\n1,000\n2\n84,000\nDuplicate sentence\ndetection\nQQP\n1,000\n2\n84,000\nMRPC\n408\n2\n34,272\nNatural language\ninference\nMNLI\n1,000\n3\n84,000\nQNLI\n1,000\n2\n84,000\nRTE\n277\n2\n23,268\nWNLI\n71\n2\n5,964\nMulti-task knowledge\nMMLU\n564\n4\n47,376\nReading comprehension\nSQuAD V2\n200\n-\n16,800\nTranslation\nMulti UN\n99\n-\n8,316\nIWSLT 2017\n100\n-\n8,400\nMath reasoning\nMath\n160\n-\n13,440\nTable 4: The APDR and standard deviations of different attacks on different datasets.\nDataset\nCharacter-level\nWord-level\nSentence-level\nSemantic-level\nTextBugger\nDeepWordBug\nTextFooler\nBertAttack\nCheckList\nStressTest\nSemantic\nSST-2\n0.25\u00b10.39\n0.18\u00b10.33\n0.35\u00b10.41\n0.34\u00b10.44\n0.22\u00b10.36\n0.15\u00b10.31\n0.28\u00b10.35\nCoLA\n0.39\u00b10.40\n0.27\u00b10.32\n0.43\u00b10.35\n0.45\u00b10.38\n0.23\u00b10.30\n0.18\u00b10.25\n0.34\u00b10.37\nQQP\n0.30\u00b10.38\n0.22\u00b10.31\n0.31\u00b10.36\n0.33\u00b10.38\n0.18\u00b10.30\n0.06\u00b10.26\n0.40\u00b10.39\nMRPC\n0.37\u00b10.42\n0.34\u00b10.41\n0.37\u00b10.41\n0.42\u00b10.38\n0.24\u00b10.37\n0.25\u00b10.33\n0.39\u00b10.39\nMNLI\n0.32\u00b10.40\n0.18\u00b10.29\n0.32\u00b10.39\n0.34\u00b10.36\n0.14\u00b10.24\n0.10\u00b10.25\n0.22\u00b10.24\nQNLI\n0.38\u00b10.39\n0.40\u00b10.35\n0.50\u00b10.39\n0.52\u00b10.38\n0.25\u00b10.39\n0.23\u00b10.33\n0.40\u00b10.35\nRTE\n0.33\u00b10.41\n0.25\u00b10.35\n0.37\u00b10.44\n0.40\u00b10.42\n0.18\u00b10.32\n0.17\u00b10.24\n0.42\u00b10.40\nWNLI\n0.39\u00b10.42\n0.31\u00b10.37\n0.41\u00b10.43\n0.41\u00b10.40\n0.24\u00b10.32\n0.20\u00b10.27\n0.49\u00b10.39\nMMLU\n0.21\u00b10.24\n0.12\u00b10.16\n0.21\u00b10.20\n0.40\u00b10.30\n0.13\u00b10.18\n0.03\u00b10.15\n0.20\u00b10.19\nSQuAD V2\n0.09\u00b10.17\n0.05\u00b10.08\n0.25\u00b10.29\n0.31\u00b10.32\n0.02\u00b10.03\n0.02\u00b10.04\n0.08\u00b10.09\nIWSLT\n0.08\u00b10.14\n0.10\u00b10.12\n0.27\u00b10.30\n0.12\u00b10.18\n0.10\u00b10.10\n0.17\u00b10.19\n0.18\u00b10.14\nUN Multi\n0.06\u00b10.08\n0.08\u00b10.12\n0.15\u00b10.19\n0.10\u00b10.16\n0.06\u00b10.07\n0.09\u00b10.11\n0.15\u00b10.18\nMath\n0.18\u00b10.17\n0.14\u00b10.13\n0.49\u00b10.36\n0.42\u00b10.32\n0.15\u00b10.11\n0.13\u00b10.08\n0.23\u00b10.13\nAvg\n0.21\u00b10.30\n0.17\u00b10.26\n0.31\u00b10.33\n0.33\u00b10.34\n0.12\u00b10.23\n0.11\u00b10.23\n0.22\u00b10.26\n\u2022 Math problem-solving: we adopt the Math [56] dataset, which evaluates LLMs\u2019 mathematical reasoning\nabilities across a diverse range of problems, such as algebra, arithmetic and comparison.\n2.4\nAnalysis\nIn addition to providing benchmark results using the models, prompts, datasets, and attacks, PromptBench\noffers extensive analysis to not only evaluate, but also understand the robustness of LLMs. Specifically,\nPromptBench presents gradient-based visualization analysis in PromptBench to understand the rational\nbehind the adversarial robustness (Sec. 4.3). Then, PromptBench offers transferability analysis between\nLLMs to understand if adversarial prompts from one LLM can be transferred to another (Sec. 4.4). Next,\nPromptBench supports word frequency analysis to offer a practical guidance to both LLM developers and\nend-users in writing robust prompts based on our experiments (Sec. 4.6). To sum up, PromptBench is a\nflexible evaluation framework for LLMs not only tailored for adversarial robustness, but can be extended to\nother evaluation research in LLMs.\n3\nExperiments\n3.1\nSetup\nThe extensive computational requirements of generating one adversarial prompt necessitates iterating over\nthe entire dataset 100 times in average. Thus, the evaluation on an entire dataset using LLMs is unfeasible.\nTo alleviate the computation constraint and preserve a fair study process, we adopt a sampling strategy that\nentails selecting a subset of samples from the validation or test sets across various datasets. The statistics of\n7\nTable 5: The APDR and standard deviations of different attacks on different models.\nModel\nCharacter-level\nWord-level\nSentence-level\nSemantic-level\nTextBugger\nDeepWordBug\nTextFooler\nBertAttack\nCheckList\nStressTest\nSemantic\nT5-large\n0.09\u00b10.10\n0.13\u00b10.18\n0.20\u00b10.24\n0.21\u00b10.24\n0.04\u00b10.08\n0.18\u00b10.24\n0.10\u00b10.09\nVicuna\n0.81\u00b10.25\n0.69\u00b10.30\n0.80\u00b10.26\n0.84\u00b10.23\n0.64\u00b10.27\n0.29\u00b10.40\n0.74\u00b10.25\nLlama2\n0.67\u00b10.36\n0.41\u00b10.34\n0.68\u00b10.36\n0.74\u00b10.33\n0.34\u00b10.33\n0.20\u00b10.30\n0.66\u00b10.35\nUL2\n0.04\u00b10.06\n0.03\u00b10.04\n0.14\u00b10.20\n0.16\u00b10.22\n0.04\u00b10.07\n0.06\u00b10.09\n0.06\u00b10.08\nChatGPT\n0.14\u00b10.20\n0.08\u00b10.13\n0.32\u00b10.35\n0.34\u00b10.34\n0.07\u00b10.13\n0.06\u00b10.12\n0.26\u00b10.22\nGPT-4\n0.03\u00b10.10\n0.02\u00b10.08\n0.18\u00b10.19\n0.27\u00b10.40\n-0.02\u00b10.09\n0.03\u00b10.15\n0.03\u00b10.16\nAvg\n0.21\u00b10.30\n0.17\u00b10.26\n0.31\u00b10.33\n0.33\u00b10.34\n0.12\u00b10.23\n0.11\u00b10.23\n0.22\u00b10.26\neach dataset and tasks are summarized in Table 3.2\nSpecifically, for the GLUE datasets, we sample 1,000 instances when the validation set exceeds this size;\notherwise, we utilize the entire validation set. With respect to ChatGPT and GPT4, we adopt a smaller\nsample size of 200 instances for computational efficiency. For the MMLU dataset, we select 10 instances for\neach of the 57 tasks if the validation set exceeds this size; if not, the entire validation set is used. For the\nSQUAD V2 dataset, we randomly select 200 validation instances. Regarding the translation datasets UN\nMulti and IWSLT 2017, we focus on three languages\u2014English, French, and German, which are primarily\nsupported by T5-large and UL2. We select a total of 100 validation instances, evenly distributed among all\npossible translation pairs, e.g., English to French. For the Math dataset, we select 20 types of math problems,\nchoosing either 5 or 10 instances per type, resulting in a total of 160 instances. This sampling strategy\nensures the formation of a manageable and representative evaluation set for each dataset, thereby enabling\nan effective assessment of the performance and robustness of LLMs across various tasks and domains.\nWe initially assess the performance of all LLMs without prompt attacks to provide a performance baseline.\nWe find that certain LLMs even do not demonstrate satisfactory performance with clean prompts, narrowing\nour selection to 6 LLMs: Flan-T5-large, Vicuna-13B, Llama2-13B-chat, UL2, ChatGPT, and GPT-4. Further\ndetails and discussions on clean prompt performance across all LLMs are available in Appendix C. We generate\n10 distinct prompts for both role-oriented and task-oriented categories. Each prompt can be augmented\nwith three examples, forming the few-shot prompts. In total, we have 40 prompts for each dataset on each\nLLM. For better efficiency and performance, we select the top 3 best-performing prompts of each type to\nconduct prompt attacks. As a result, we evaluate the adversarial vulnerabilities of 9 LLMs across 13 datasets,\nencompassing a total of 4, 788 prompts3 and their respective adversarial counterparts. This comprehensive\nevaluation allows us to gain valuable insights into the robustness and performance of LLMs across a wide\nrange of scenarios and prompt styles.\n3.2\nEvaluation metrics\nConsidering the diverse evaluation metrics across tasks and varying baseline performances across models and\ndatasets, the absolute performance drop may not provide a meaningful comparison. Thus, we introduce a\nunified metric, the Performance Drop Rate (PDR). PDR quantifies the relative performance decline following\na prompt attack, offering a contextually normalized measure for comparing different attacks, datasets, and\nmodels. The PDR is given by:\nPDR(A, P, f\u03b8, D) = 1 \u2212\nP\n(x;y)\u2208D M[f\u03b8([A(P), x]), y]\nP\n(x;y)\u2208D M[f\u03b8([P, x]), y]\n,\nwhere A is the adversarial attack applied to prompt P, and M[\u00b7] is the evaluation function: for classification\ntask, M[\u00b7] is the indicator function 1[\u02c6y, y] which equals to 1 when \u02c6y = y, and 0 otherwise; for reading\ncomprehension task, M[\u00b7] is the F1-score; for translation tasks, M[\u00b7] is the Bleu metric [48]. Note that a\nnegative PDR implies that adversarial prompts can occasionally enhance the performance.\n2In Table 3, the last column denotes the total evaluation sample size for each dataset on each model. For instance, there are\n872 test samples in SST-2 dataset and each sample should go through 7 adversarial attacks on 4 types of prompts, each with 3\nprompts, thus the test size for each model is 872 \u00d7 7 \u00d7 4 \u00d7 3 = 73248.\n34, 788 = 3 \u00d7 4 \u00d7 5 \u00d7 13 \u00d7 7 \u2212 336 \u00d7 3 \u00d7 2, where the numbers on the R.H.S. denote #attacked prompts, #prompt types,\n8\nTable 6: The APDR on different LLMs.\nDataset\nT5-large\nVicuna\nLlama2\nUL2\nChatGPT\nGPT-4\nSST-2\n0.04\u00b10.11\n0.83\u00b10.26\n0.24\u00b10.33\n0.03\u00b10.12\n0.17\u00b10.29\n0.24\u00b10.38\nCoLA\n0.16\u00b10.19\n0.81\u00b10.22\n0.38\u00b10.32\n0.13\u00b10.20\n0.21\u00b10.31\n0.13\u00b10.23\nQQP\n0.09\u00b10.15\n0.51\u00b10.41\n0.59\u00b10.33\n0.02\u00b10.04\n0.16\u00b10.30\n0.16\u00b10.38\nMRPC\n0.17\u00b10.26\n0.52\u00b10.40\n0.84\u00b10.27\n0.06\u00b10.10\n0.22\u00b10.29\n0.04\u00b10.06\nMNLI\n0.08\u00b10.13\n0.67\u00b10.38\n0.32\u00b10.32\n0.06\u00b10.12\n0.13\u00b10.18\n-0.03\u00b10.02\nQNLI\n0.33\u00b10.25\n0.87\u00b10.19\n0.51\u00b10.39\n0.05\u00b10.11\n0.25\u00b10.31\n0.05\u00b10.23\nRTE\n0.08\u00b10.13\n0.78\u00b10.23\n0.68\u00b10.39\n0.02\u00b10.04\n0.09\u00b10.13\n0.03\u00b10.05\nWNLI\n0.13\u00b10.14\n0.78\u00b10.27\n0.73\u00b10.37\n0.04\u00b10.03\n0.14\u00b10.12\n0.04\u00b10.04\nMMLU\n0.11\u00b10.18\n0.41\u00b10.24\n0.28\u00b10.24\n0.05\u00b10.11\n0.14\u00b10.18\n0.04\u00b10.04\nSQuAD V2\n0.05\u00b10.12\n-\n-\n0.10\u00b10.18\n0.22\u00b10.28\n0.27\u00b10.31\nIWSLT\n0.14\u00b10.17\n-\n-\n0.15\u00b10.11\n0.17\u00b10.26\n0.07\u00b10.14\nUN Multi\n0.13\u00b10.14\n-\n-\n0.05\u00b10.05\n0.12\u00b10.18\n-0.02\u00b10.01\nMath\n0.24\u00b10.21\n-\n-\n0.21\u00b10.21\n0.33\u00b10.31\n0.02\u00b10.18\nAvg\n0.13\u00b10.19\n0.69\u00b10.34\n0.51\u00b10.39\n0.08\u00b10.14\n0.18\u00b10.26\n0.08\u00b10.21\nTable 7: The APDR on different prompts.\nDataset\nZS-task\nZS-role\nFS-task\nFS-role\nSST-2\n0.31\u00b10.39\n0.28\u00b10.35\n0.22\u00b10.38\n0.24\u00b10.39\nCoLA\n0.43\u00b10.35\n0.43\u00b10.38\n0.24\u00b10.28\n0.25\u00b10.36\nQQP\n0.43\u00b10.42\n0.34\u00b10.43\n0.16\u00b10.21\n0.14\u00b10.20\nMRPC\n0.44\u00b10.44\n0.51\u00b10.43\n0.24\u00b10.32\n0.23\u00b10.30\nMNLI\n0.29\u00b10.35\n0.26\u00b10.33\n0.19\u00b10.29\n0.21\u00b10.33\nQNLI\n0.46\u00b10.39\n0.51\u00b10.40\n0.30\u00b10.34\n0.32\u00b10.36\nRTE\n0.33\u00b10.39\n0.35\u00b10.40\n0.31\u00b10.39\n0.27\u00b10.38\nWNLI\n0.36\u00b10.36\n0.39\u00b10.39\n0.37\u00b10.41\n0.33\u00b10.38\nMMLU\n0.25\u00b10.23\n0.22\u00b10.26\n0.18\u00b10.23\n0.14\u00b10.20\nSQuAD V2\n0.16\u00b10.26\n0.20\u00b10.28\n0.06\u00b10.11\n0.07\u00b10.12\nIWSLT\n0.18\u00b10.22\n0.24\u00b10.25\n0.08\u00b10.09\n0.11\u00b10.10\nUN Multi\n0.17\u00b10.18\n0.15\u00b10.16\n0.04\u00b10.07\n0.04\u00b10.07\nMath\n0.33\u00b10.26\n0.39\u00b10.30\n0.16\u00b10.18\n0.17\u00b10.17\nAvg\n0.33\u00b10.36\n0.34\u00b10.37\n0.21\u00b10.31\n0.21\u00b10.31\n4\nResults and analysis\nIn this section, we present our benchmark results and analysis in evaluating the robustness of LLMs on\nadversarial prompts.\n4.1\nBenchmark results across different attacks, models, and prompts\nWe report and discuss the Average PDR (APDR) across different attacks, LLMs, and prompts. Note that\nalthough our semantic preserving study in Sec. 2.2.3 demonstrated that at least 85% of the adversarial\nprompts are acceptable, there are still some adversarial prompts diverged from their intended semantic\nmeaning. We further summarize the results by excluding these meaningless prompts in Appendix G to present\na comparison analysis. Our main results in the main paper are based on all the prompts, whose conclusions\nare in consistent with Appendix G. Furthermore, note that the significant discrepancies in APDR variance\nvalues are due to varying PDR values across different attacks, prompts, models and datasets, leading to\npronounced variance.\nAnalysis on attacks\nTable 4 summarizes the APDR of 7 attacks on 13 datasets. The APDR is calculated\nby APDRA(A, D) =\n1\n|P|\n1\n|F|\nP\nP \u2208P\nP\nf\u03b8\u2208F PDR(A, P, f\u03b8, D), where P is the set of 4 types of prompts and\nF is the set of all models. The results offer several key insights. Firstly, attack effectiveness is highly\nvariable, with word-level attacks proving the most potent, leading to an average performance decline of\n33% across all datasets. Character-level attacks rank the second, inducing a 20% performance drop across\nmost datasets. Notably, semantic-level attacks exhibit potency nearly commensurate with character-level\nattacks, emphasizing the profound impact of nuanced linguistic variations on LLMs\u2019 performance. Conversely,\nsentence-level attacks pose less of a threat, suggesting adversarial interventions at this level have a diminished\neffect. Moreover, the effect of prompt attack varies across different datasets. For instance, StressTest attacks\non SQUAD V2 yield a mere 2% performance drop, while inflicting a 25% drop on MRPC. Furthermore, we\nobserve that StressTest attack paradoxically bolsters model\u2019s performance in some datasets, we delve into\nthis phenomenon in Sec. 4.3.3.\nNote that while character-level attacks are detectable by grammar detection tools, word- and semantic-level\nattacks underscore the importance of robust semantic understanding and accurate task presentation/translation\nfor LLMs. A comprehensive understanding of these nuances will inform a deeper comprehension of adversarial\nattacks on LLMs.\nAnalysis on LLMs\nTable 6 summarizes the APDR of 9 LLMs on 13 datasets. The APDR is calculated by\nAPDRf\u03b8(f\u03b8, D) =\n1\n|A|\n1\n|P|\nP\nA\u2208A\nP\nP \u2208P PDR(A, P, f\u03b8, D), where P is the set of 4 types of prompts and A is\nthe set of 7 attacks. Our analysis reveals that GPT-4 and UL2 significantly outperform other models in terms\n#LLMs, #datasets, and #attacks, respectively. We did not conduct attacks on Vicuna, Llama2-13B-chat on certain datasets\nbecause the outputs of these datasets are meaningless, so that we subtract 336 \u00d7 3 \u00d7 2 prompts.\n9\n0\n25\n50\n75\n100\nAccuracy\nClean\nTextBugger\nDeepWordBug\nTextFooler\nBertAttack\nCheckList\nStressTest\nSemantic\nSST-2\n7B\n13B\n70B\n0\n20\n40\n60\n80\nAccuracy\nCOLA\nFigure 4: Accuracy of Llama2 models (7B-chat, 13B-chat, 70B-chat) on SST2 and CoLA datasets.\nof robustness, followed by T5-large, ChatGPT, and Llama2, with Vicuna presenting the least robustness.\nThe robustness against adversarial prompts of UL2, T5-large, and ChatGPT varies across datasets, with\nUL2 and T5-large showing less vulnerability to attacks on sentiment classification (SST-2), most NLI tasks,\nand reading comprehension (SQuAD V2). Specifically, UL2 excels in translation tasks, while ChatGPT\ndisplays robustness in certain NLI tasks. Vicuna, however, exhibits consistently high susceptibility to attacks\nacross all tasks. It can be observed that, given the same adversarial prompts generated by ChatGPT, GPT-4\nexhibits superior robustness across all tasks. However, it is crucial to realize that this observed robustness\nmight attribute to the weak transferability of the adversarial prompts crafted specifically for ChatGPT. In\nthe future, the performance of GPT-4 and ChatGPT could be significantly improved since these proprietary\nmodels keep evolving.\nFurthermore, we show the impact of different attacks on different models in Table 5. The APDR is\ncalculated by APDR(A, f\u03b8) =\n1\n|P|\n1\n|A|\nP\nP \u2208P\nP\nD\u2208D PDR(A, P, f\u03b8, D), where P is the set of 4 types of prompts\nand D is the set of all datasets. Generally, word-level attacks emerge as the most potent, with BertAttack\nconsistently outperforming others across all models. However, no discernible pattern emerges for the efficacy\nof the other attacks. For instance, while TextBugger proves more effective than DeepWordBug for some\nmodels such as Llama2 and ChatGPT, the inverse holds true for T5-large. Notably, Vicuna and Llama2\nare distinctly vulnerable to sentence-level attacks, in contrast to models like T5-large and ChatGPT, which\nremain largely unaffected. Such observations may hint at inherent vulnerabilities specific to Llama-based\nmodels.\nAnalysis on different types of prompts\nTable 7 summarizes the APDR of 4 types of prompts on\n13 datasets. The APDR is calculated by APDRt(D) =\n1\n|A|\n1\n|Pt|\n1\n|F|\nP\nA\u2208A\nP\nP \u2208Pt\nP\nf\u03b8\u2208F PDR(A, P, f\u03b8, D),\nwhere Pt is the set of prompts of certain type t, A is the set of 7 attacks and F is the set of all models. In\nour analysis, few-shot prompts consistently demonstrate superior robustness compared to zero-shot prompts\nacross all datasets. Furthermore, while task-oriented prompts marginally outperform role-oriented prompts\nin overall robustness, both of them show varying strengths across different datasets and tasks. For example,\nrole-oriented prompts present increased robustness within the SST-2 and QQP datasets, whereas task-oriented\nprompts are more resilient within the MRPC, QNLI, SQuAD V2, and IWSLT datasets. Insights into different\neffects of prompt types on model vulnerability can inform better prompt design and tuning strategies,\nenhancing LLMs robustness against adversarial prompt attacks.\n4.2\nAnalysis on model size and fine-tuning\nAs shown in Table 5 and 6, there seems to be no clear correlation between model robustness and size, for\nexample, despite being the smallest, T5-large demonstrates robustness on par with larger models such as\nChatGPT on our evaluated datasets.\nThe observed differences in model robustness might stem from two aspects: 1) the specific fine-tuning\ntechniques employed. For example, both UL2 and T5-large, fine-tuned on large datasets, and ChatGPT, fine-\ntuned via RLHF [10], exhibit better robustness than Vicuna. These findings encourage further investigation\n10\n0\n20\n40\n60\n80\nAccuracy\nClean\nTextBugger\nDeepWordBug\nTextFooler\nBertAttack\nCheckList\nStressTest\nSemantic\n7B\n0\n20\n40\n60\n80\nAccuracy\n13B\nVanilla\nFine-tuned\nFigure 5: Accuracy of Llama2 models with fine-tuning and w/o fine-tuning (vanilla) on SST-2 dataset.\nTable 8: Attention visualization of samples that are correctly classified by clean prompts but misclassified by\nadv. prompts. For each attack, the above is the clean prompt with sample text, the below is the corresponding\nadversarial prompt with the same sample text. N=Negative, P=Positive and N/A means the response is not\navailable. The green and red color denote right and wrong answers, respectively. Color intensity denotes\ndifferent attention weights (heavier color means larger weights).\nAttack\nPred.\n[Prompt, sample]\nBertAttack\nN\nIn\nthe\nrole\nof\na\nsentiment\nanalysis\ntool,\nrespond\nwith\n\u2019positive\u2019\nor\n\u2019negative\u2019\nto\nclassify\nthis\nstatement:the\ntitle\nnot\nonly\ndescribes\nits\nmain\ncharacters\n,\nbut\nthe\nlazy\npeople\nbehind\nthe\ncamera\nas\nwell\n.\nAnswer:\nP\nhow\nthe\nrole\nof\na\ncompliment\nanalysis\ntool,\nresponses\nwith\n\u2019positive\u2019\nor\n\u2019negative\u2019\nto\nmood\nthis\nstatement:the\ntitle\nnot\nonly\ndescribes\nits\nmain\ncharacters\n,\nbut\nthe\nlazy\npeople\nbehind\nthe\ncamera\nas\nwell\n.\nAnswer:\nCheckList\nP\nGiven\nthe\ncontext\nof\nthis\ntext,\nindicate\nif\nthe\nemotion\nconveyed\nis\n\u2019positive\u2019\nor\n\u2019negative\u2019:good\nfilm\n,\nbut\nvery\nglum\n.\nAnswer:\nN\nGiven\nthe\ncontext\nof\nthis\ntext,\nindicate\nif\nthe\nemotion\nconveyed\nis\n\u2019positive\u2019\nor\n\u2019negative\u2019\nLkF0FZxMZ4:good\nfilm\n,\nbut\nvery\nglum\n.\nAnswer:\nDeepWordBug\nN\nServing\nas\na\nsentiment\nevaluation\nmodel,\ndetermine\nif\nthe\ngiven\nstatement\nis\n\u2019positive\u2019\nor\n\u2019negative\u2019.\nClassify:i\nhad\nto\nlook\naway\n-\nthis\nwas\ngod\nawful\n.\nAnswer:\nN/A\nServign\nas\na\nsentimBnt\nenvaluation\nmodel,\nQetermine\nif\nthe\nIiven\nstatemen\nis\n\u2019positive\u2019\nor\n\u2019negative\u2019.\nClasshfy:\ni\nhad\nto\nlook\naway\n-\nthis\nwas\ngod\nawful\n.\nAnswer:\nSemantic\nN\nIn\nthe\nrole\nof\na\nsentiment\nanalysis\ntool,\nrespond\nwith\n\u2019positive\u2019\nor\n\u2019negative\u2019\nto\nclassify\nthis\nstatement:bad\n.\nAnswer:\nP\nClassify\nwhat\nyou\u2019re\ntrying\nto\nconvey\nin\nthis\nsentence\nas\n\u2019positive\u2019\nif\nit\u2019s\npositive,\nand\n\u2019negative\u2019\nif\nit\u2019s\nnegative.bad\n.\nAnswer:\nof fine-tuning strategies to enhance robustness. 2) the memorization of training data. Recent studies suggest\nthat the remarkable performance of some LLMs might be rooted in their ability to memorize training\ndata [1,2,6,39], rather than in generalization. Hence, even when confronted with adversarial prompts, models\nmight leverage this memorization to produce accurate responses.\nIn this section, we conduct experiments to analyze the effects of different model sizes and fine-tuning\non adversarial prompts. Particularly, we leverage the open-source Llama2 [63] series due to their support\non different sizes and their corresponding fine-tuned versions. The chat versions of Llama2 (Llama2-chat)\nare fine-tuned on human instructions datasets to better follow the instructions and support multi-turn\nconversations, while the original version can only be used for inference.\nRobustness of different model sizes\nWe analyzed three models from the open-source Llama2 series [63]:\nLlama2-7B-chat, Llama2-13B-chat, and Llama2-70B-chat. These were chosen due to their distinct sizes,\nfurther, their fine-tuning datasets and methods are the same. Our results, depicted in Figure 4, reveal that,\nin a non-adversarial setting, larger models like the 70B model typically surpass the performance of their\nsmaller counterparts. Yet, when subjected to adversarial attacks, the performance dynamics change: at times,\nsmaller models outshine the larger ones. The reasons for these abnormal behaviors could trigger interests for\nfuture research.\n11\nTable 9: Attention visualization of samples that are correctly classified by adv. prompt but misclassified by\nclean prompt. Notations and colors follow Table 8.\nAttack\nPred.\n[Prompt, sample]\nCheckList\nE\nDetermine\nif\nthe\ngiven\npair\nof\nstatements\ncan\nbe\nconsidered\nthe\nsame\nby\nresponding\nwith\n\u2019equivalent\u2019\nor\n\u2019not equivalent\u2019.\nQuestion\n1:\nWhat\nlanguage\nis\nthis?\nQuestion\n2:\nWhat\nlanguage\nis\nthis\nin?\nAnswer:\nNE\nDetermine\nif\nthe\ngiven\npair\nof\nstatements\ncan\nbe\nconsidered\nthe\nsame\nby\nresponding\nwith\n\u2019equivalent\u2019\nor\n\u2019not equivalent\u2019\nEAB4KP2NVY.\nQuestion\n1:\nWhat\nlanguage\nis\nthis?\nQuestion\n2:\nWhat\nlanguage\nis\nthis\nin?\nAnswer:\nStressTest\nE\nAs\nan\ninstrument\nfor\nentailment\nevaluation,\nconsider\nthe\ntwo\nsentences\nand\ndetermine\nif\ntheir\nrelationship\nis\n\u2019entailment\u2019\nor\n\u2019not entailment\u2019.\nRespond\nwith\n\u2019entailment\u2019\nor\n\u2019not entailment\u2019\n:\nSentence\n1:\nLook!\nThere\nis\na\nminnow\nswimming\nright\nbelow\nthat\nduck!\nIt\nhad\nbetter\nget\naway\nto\nsafety\nfast!\nSentence\n2:\nThe\nduck\nhad\nbetter\nget\naway\nto\nsafety\nfast!\nAnswer:\nNE\nAs\nan\ninstrument\nfor\nentailment\nevaluation,\nconsider\nthe\ntwo\nsentences\nand\ndetermine\nif\ntheir\nrelationship\nis\n\u2019entailment\u2019\nor\n\u2019not entailment\u2019.\nRespond\nwith\n\u2019entailment\u2019\nor\n\u2019not entailment\u2019\nand\ntrue\nis\ntrue\n:Sentence\n1:\nLook!\nThere\nis\na\nminnow\nswimming\nright\nbelow\nthat\nduck!\nIt\nhad\nbetter\nget\naway\nto\nsafety\nfast!\nSentence\n2:\nThe\nduck\nhad\nbetter\nget\naway\nto\nsafety\nfast!\nAnswer:\nTable 10: The APDR of transferability of several LLMs.\nAttacks\nChat \u2192 T5\nChat \u2192 UL2\nChat \u2192 V\nT5 \u2192 Chat\nT5 \u2192 UL2\nT5 \u2192 V\nUL2 \u2192 Chat\nUL2 \u2192 T5\nUL2 \u2192 V\nV \u2192 Chat\nV \u2192 T5\nV \u2192 UL2\nBertAttack\n0.05\u00b10.17\n0.08\u00b10.19\n0.08\u00b10.88\n0.18\u00b10.32\n0.11\u00b10.23\n-1.39\u00b15.67\n0.15\u00b10.27\n0.05\u00b10.11\n-0.70\u00b13.18\n0.06\u00b10.19\n0.05\u00b10.11\n0.03\u00b10.12\nCheckList\n0.00\u00b10.04\n0.01\u00b10.03\n0.19\u00b10.39\n0.00\u00b10.07\n0.01\u00b10.03\n-0.09\u00b10.64\n0.01\u00b10.06\n0.01\u00b10.04\n-0.13\u00b11.80\n-0.01\u00b10.04\n0.00\u00b10.01\n0.00\u00b10.00\nTextFooler\n0.04\u00b10.08\n0.03\u00b10.09\n-0.25\u00b11.03\n0.11\u00b10.23\n0.08\u00b10.16\n-0.30\u00b12.09\n0.11\u00b10.21\n0.07\u00b10.18\n-0.17\u00b11.46\n0.04\u00b10.16\n0.02\u00b10.06\n0.00\u00b10.01\nTextBugger\n-0.00\u00b10.09\n-0.01\u00b10.05\n0.02\u00b10.94\n0.04\u00b10.15\n0.01\u00b10.04\n-0.45\u00b13.43\n0.04\u00b10.13\n0.02\u00b10.07\n-0.84\u00b14.42\n0.03\u00b10.13\n0.01\u00b10.05\n0.00\u00b10.01\nDeepWordBug\n0.03\u00b10.11\n0.01\u00b10.03\n0.10\u00b10.46\n0.00\u00b10.06\n0.01\u00b10.02\n-0.18\u00b11.20\n0.01\u00b10.10\n0.02\u00b10.06\n-0.09\u00b10.75\n0.00\u00b10.03\n0.02\u00b10.11\n0.00\u00b10.01\nStressTest\n0.04\u00b10.17\n0.03\u00b10.10\n0.01\u00b10.48\n-0.01\u00b10.06\n0.03\u00b10.06\n0.04\u00b10.80\n0.00\u00b10.04\n0.05\u00b10.16\n0.06\u00b10.45\n0.00\u00b10.04\n0.09\u00b10.18\n0.02\u00b10.08\nSemantic\n0.04\u00b10.12\n0.02\u00b10.06\n0.25\u00b10.47\n0.07\u00b10.27\n0.00\u00b10.03\n-0.81\u00b14.14\n0.02\u00b10.11\n-0.13\u00b10.72\n-0.50\u00b11.59\n0.07\u00b10.11\n0.00\u00b10.05\n0.00\u00b10.02\nRobustness of fine-tuning\nTo delve into the intricacies of fine-tuning, we compared the performances\nof Llama2-7B and Llama2-7B-chat on the SST2 and COLA tasks. Our analysis, as visualized in Figure 5,\nunderscores a consistent trend: models fine-tuned using human-instruction datasets fare better against\nadversarial onslaughts than models that are not fine-tuned. This observation implies that fine-tuning could\nbe further utilized as the countermeasures for adversarial inputs.\n4.3\nUnderstanding the vulnerability of LLMs to adversarial prompts\nIn this section, we study the magic behind adversarial prompts to analyze why they lead to errors for LLMs\nfrom different aspects: erroneous response analysis, attention visualization, and case study on sentence-level\nattacks.\n4.3.1\nErroneous response analysis\nWe first analyze the erroneous response analysis produced by adversarial prompts. The results suggest that\nadversarial prompts impact LLMs\u2019 performance by inducing misclassification errors and hindering their ability\nto generate meaningful responses.\n\u2022 Induced Misclassification: As exemplified by BertAttack, CheckList, and Translation attacks, adver-\nsarial prompts can lead the model to erroneous classifications. For instance, the sentiment prediction may\nshift from positive to negative due to the influence of the adversarial prompt. This instance validates the\nefficacy of adversarial attacks in manipulating the model\u2019s decision-making processes.\n\u2022 Generation of Incoherent Responses: In the case of the DeepWordBug attack, the adversarial prompt\nresults in the model generating incoherent or nonsensical sentences. For example, the response \u201cNone\nof the above choices\u201d does not align with any positive or negative sentiment classification, thereby\ndemonstrating that the model fails to comprehend the intended input. This observation emphasizes the\nsusceptibility of LLMs to adversarial perturbations that can potentially hamper their natural language\nunderstanding capabilities.\n12\n4.3.2\nAnalysis by attention visualization\nThen, we conduct an attention visualization experiment to investigate the influence of adversarial prompts\non LLMs\u2019 focus on input words. Specifically, we propose two attention visualization techniques: 1) Attention\nby Gradient, which assigns an attention score to each word based on the gradient norm, and 2) Attention\nby Deletion, which assigns an attention score to each word by examining the absolute change in loss when\nthe word is removed. Comprehensive details regarding these methods can be found in Appendix D. Both\ntechniques produce similar results; hence, we focus on results from the Attention by Gradient method for\nsimplicity. Our key findings, as demonstrated in Table 8, are as follows:\n\u2022 Clean prompts: efficient attention allocation. LLMs predominantly concentrate on key terms within\nclean prompts, aiding in accurate classifications. For instance, for clean prompts of BertAttack in Table 8,\nLLMs mainly allocate attention to the term \u2018lazy\u2019, correctly deducing a \u2018Negative\u2019 sentiment.\n\u2022 Adversarial prompts: attention divergence. Adversarial prompts can reroute LLMs\u2019 attention from\nintegral text segments, causing misclassifications. In some attacks like CheckList and StressTest, the model\nsimultaneously concentrates on the target text and adversarial content, amplifying its susceptibility to\nadversarial perturbations. For instance, introducing a random sequence \u2018LKF0FZxMZ4\u2019 during a CheckList\nattack distracts the model, reducing focus on the critical word \u2018good\u2019 for accurate classification. In other\nattacks, such as BertAttack and DeepWordBug, the model\u2019s attention is entirely diverted from the text\nrequiring classification towards adversarial prompts, leading to a significant shift in focus. For example, in\nDeepWordBug attack, typos in specific words divert the model\u2019s attention from \u2018awful\u2019 to the altered\nword \u2018Qetermine\u2019.\n4.3.3\nAnalysis on sentence-level attacks\nThe phenomenon where sentence-level attacks occasionally improve the performance of LLMs is an intriguing\naspect of our study. Our attention analysis revealed distinct behaviors when models are subjected to StressTest\nand CheckList attacks. Specifically, when juxtaposed with other adversarial prompts, sentence-level attacks\nsometimes lead the model to hone in more acutely on pertinent keywords in the question and the labels. This\nis contrary to the expected behavior. As illustrated in Table 9, introducing an ostensibly unrelated sequence,\nsuch as \u2018and true is true\u2019, heightens the LLMs\u2019s focus on the \u2018not entailment\u2019 label. Simultaneously,\nthe model continues to attend to salient terms like \u2018minnow\u2019 and \u2018duck\u2019, ultimately culminating in a correct\nprediction.\n4.4\nTransferability of adversarial prompts\nTable 10 displays the effectiveness of various attacks in transferring adversarial prompts between several\nLLMs. For each dataset and prompt type, we selected the most vulnerable prompts generated by a source\nmodel (e.g., ChatGPT). These prompts were then utilized to launch transfer attacks against the target models\n(e.g., T5-large). The impact of these transfer attacks was quantified by calculating APDRtransfer(A, f target\n\u03b8\n) =\n1\n|Psource|\n1\n|D|\nP\nP \u2208Psource\nP\nD\u2208D PDR(A, P, f target\n\u03b8\n, D), where f target\n\u03b8\nis the target model, Psource is the prompts\nselected from source model and D is the set of all datasets.\nIn general, we observe that adversarial prompts exhibit some degree of transferability. However, it is\nmarginal compared to Table 4 and 6. Specifically, the APDR in the target model by adversarial prompts from\nsource model is small compared to the original APDR of the source model. Furthermore, the standard deviation\ntends to be larger than the APDR, indicating that the transferability is inconsistent. Some adversarial\nprompts can be successfully transferred, causing a performance drop, while others may unexpectedly improve\nthe performance of the target model. A prime example is the BertAttack transfer from UL2 to Vicuna, which\nresulted in a \u22120.70(3.18) value, suggesting an unanticipated enhancement in Vicuna\u2019s performance when\nsubjected to these adversarial prompts. These phenomena illustrate the complex robustness traits of different\nmodels. The transferability to ChatGPT is better compared to T5-large and UL2. This suggests an avenue\nto generate adversarial prompts to attack black-box models such as ChatGPT by training on small models\nlike T5-large, which could be used for future research on robustness.\n13\nTable 11: Accuracy (%) of GPT-4 on clean and adversarial prompts and samples on the AdvGLUE [70]\ndataset, i.e., attacking both prompts and samples.\nAttack\nSST-2\nQQP\nMNLI\nQNLI\nRTE\nAVG\nClean prompts\n& clean samples\n96.10\n78.23\n81.05\n64.50\n87.54\n81.48\nClean prompts\n& AdvGLUE\n63.51\n70.51\n63.64\n62.84\n74.07\n66.91\nTextBugger\n58.78\n44.87\n47.93\n60.81\n76.54\n57.79\nDeepWordBug\n66.22\n61.54\n59.50\n61.49\n72.84\n64.32\nTextFooler\n2.03\n1.28\n46.28\n4.05\n0.00\n10.73\nBertAttack\n0.00\n0.00\n27.27\n24.32\n71.60\n24.64\nChecklist\n69.59\n66.67\n57.85\n56.08\n72.84\n64.61\nStressTest\n50.68\n56.41\n59.50\n59.46\n76.54\n60.52\nSemantic\n0.00\n38.46\n48.76\n65.54\n41.98\n38.95\n4.5\nAttacking both prompts and samples\nThe primary focus of this work is to evaluate the robustness of prompts rather than input samples since the\nsamples can be omitted in certain situations, as discussed in Sec. 2.2. In this section, we explore the possibility\nof attacking both prompts and samples, i.e., evaluating the performance of LLMs in adversarial prompts\nand samples. Note that since the generation of adversarial examples is expensive and time-consuming, we\nleverage an existing adversarial dataset called AdvGLUE [70], which contains adversarial examples from\nGLUE [67] and it consists of five same tasks as GLUE: SST-2, QQP, MNLI, QNLI, and RTE. Then, we\nleverage the adversarial prompts and the AdvGLUE dataset [70] to evaluate the performance when attacking\nboth prompts and samples.\nTable 11 shows the accuracy results using both clean and adversarial prompts on AdvGLUE and clean\ndataset, respectively. The results demonstrate that on average, all attacking approaches are effective since\nthe accuracy is dramatically declined in face of adversarial prompts. Similar to Table 4, word-level attacks\n(TextFooler and BertAttack) are the most effective with more than 49% of accuracy drop. Moreover, surprising\nresults emerge for Checklist attack since the performance can sometimes be improved (e.g., 69.59% on SST-2\nvs. the original 63.51%). This is also consistent with our previous observation in Sec. 4.3.3. The results in\nthis section show that attacking both the prompts and samples can further reduce the performance of LLMs.\nHowever, certain attacks can even enhance the performance, which is left for future research.\n4.6\nWhich prompts are more robust? Analysis on word frequency\nIdentifying the frequent patterns in prompts that may affect robustness is essential to both researchers\nand end-users. We perform an initial analysis on word frequency. We divide prompts into two categories:\nVulnerable prompts, causing a performance drop of over 10%, and Robust prompts, with a performance drop\nof 10% or less. Then, we collect all the words appeared in these prompts, and calculate the robust word\nfrequency fwi of word wi as f robust\nwi\n=\nnrobust\nwi\nnrobust\nwi\n+nvulnerable\nwi\n, where nrobust\nwi\nand nvulnerable\nwi\ndenote the occurrences\nof wi in robust and vulnerable prompts, respectively. We primarily analyzed the adversarial prompts of CoLA\nand MRPC datasets generated by the T5-large model. The word frequency results of these two datasets are\npresented in Figure 6.\nIn our examination of adversarial robustness in large language models, we identified that word-specific\nresilience to attacks is not uniform. Specifically, within the COLA dataset, prompts incorporating terms such\nas \u201cacting\u201d, \u201canswering\u201d, and \u201cdetection\u201d displayed greater resistance to adversarial perturbations. In\ncontrast, those with words like \u201canalyze\u201d, \u201canswer\u201d, and \u201cassess\u201d were notably more susceptible. Yet, an\nanalysis of the MRPC dataset demonstrated a significant overlap in the frequency of words present in both\nrobust and vulnerable prompts. This overlap challenges the notion that specific words inherently determine a\nprompt\u2019s resilience to adversarial attacks.\nOur findings underscore that the resilience of a prompt is intricately tied to the contextual use of words,\nrather than the mere presence of certain terms. This complexity suggests that factors beyond word frequency,\nsuch as semantic coherence and syntactic structures, might be instrumental in determining robustness. This\n14\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\nacting\nanswering\ndetection\nprovided\nincorrect\ninstrument\nsound\ngrammar\ndetermine\ngrammatically\nsentence\ncorrect\ngiven\nanalyze\nfunctioning\ntool\ndecide\nevaluation\nresponding\nanswer\nconsider\neither\nexamine\nassess\nfollowing\nrespond\nWords\nCoLA\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFrequency\ncompare\ndetermine\nevaluate\nfollowing\nshare\nsemantic\nexpert\nmeaning\npair\nresponding\nsentences\ncomparison\ngiven\nalign\nanswer\nanswering\ncapacity\ndecide\nexamine\nidentical\nindicate\nlanguage\nmeanings\nrespond\nstatements\ntool\ntwo\nunderlying\nMRPC\nRobust\nVulnerable\nFigure 6: Frequency analysis for robust and vulnerable words on the CoLA (left) and MRPC (right) tasks.\nknowledge is valuable as it can influence future research on large language model robustness, provide guidance\nfor crafting more resistant prompts, and facilitate the creation of defensive mechanisms against adversarial\nprompt attacks. It\u2019s essential to emphasize that our observations are rooted in the current scope of models\nand datasets. Furthermore, the robustness or vulnerability of words remains deeply context-dependent. Hence,\ndirect determination of word robustness without considering the broader context may lead to oversimplified\nor inaccurate conclusions.\n5\nDefenses\nIn this section, we discuss potential countermeasures for future research. We categorize the robustness\nenhancement (i.e., defenses to adversarial prompts) approaches into three main axes: strategies in the training\nphase, input preprocessing, and downstream fine-tuning.\n5.1\nStrategies in the training phase\nAdversarial data integration\nSimilar to adversarial training [23,32], integrating low-quality or intention-\nally perturbed data during the training and fine-tuning phases allows the model to acquaint itself with a\nbroader range of inputs. This acclimatization aims to reduce the model\u2019s susceptibility to adversarial attacks,\nbolstering its resilience against malicious attempts that exploit such data nuances.\nMixture of experts (MoE)\nAs discussed in Sec. 4.4, adversarial prompts exhibit transferability but\nconstrained. Thus, one promising countermeasure is the utilization of diverse models [29,47,57], training them\nindependently, and subsequently ensembling their outputs. The underlying premise is that an adversarial\nattack, which may be effective against a singular model, is less likely to compromise the predictions of an\nensemble comprising varied architectures. On the other hand, a prompt attack can also perturb a prompt\nbased on an ensemble of LLMs, which could enhance transferability.\n15\n5.2\nInput preprocessing\nAutomated spelling verification\nLeveraging spelling checks [13,31] to maintain input fidelity can coun-\nteract basic adversarial techniques targeting typographical errors (character-level attacks) and inconsistencies\n(sentence level attacks).\nSemantic input rephrasing\nSemantic rephrasing [20, 54] involves analyzing the meaning and intent\nbehind a prompt. Using auxiliary models or rule-based systems to discern potentially adversarial or malicious\nintent in inputs could filter out harmful or misleading prompts.\nHistorical context verification\nBy maintaining a limited history of recent queries [40,51], the system\ncan identify patterns or sequences of inputs that might be part of a more extensive adversarial strategy.\nRecognizing and flagging suspicious input sequences can further insulate the LLM from coordinated adversarial\nattacks.\n5.3\nDownstream fine-tuning\nExploring fine-tuning techniques\nThe fine-tuning phase is instrumental in refining the prowess of\nLLMs. Exploring more effective fine-tuning methodologies, which adjust based on the detected adversarial\ninput patterns, can be pivotal. With the continuous evolution of adversarial threats, dynamic and adaptive\nfine-tuning remains a promising avenue. For example, only fine-tuning on relevant slicing technique [82],\nmodel soup [77], fine-tuning then interpolating [78], etc.\n6\nLimitations\nWe acknowledge several limitations that could be addressed in future research. First, due to the required\nsubstantial computation, we did not perform evaluations on the full datasets but relied on sampling. Future\nresearch may evaluate on the entire datasets to gain more comprehensive insights. Second, while our benchmark\ninvolves a diverse set of LLMs and datasets, we cannot included all LLMs and datasets (which is impossible)\ndue to computation constraint. Including more in the future could provide a more diverse perspective. Third,\nwe did not evaluate more advanced techniques of prompt engineering such as chain-of-thought (CoT) [74] and\ntree-of-thought (ToT) [80]. We believe more evaluations can be done on latest prompt engineering techniques.\nFourth, we considered black-box prompt attacks, which can generate perturbations that can mimic naturally\noccurred errors. Optimized prompt attacks in the white-box setting may produce more powerful adversarial\nprompts, which is an interesting future work to explore.\n7\nRelated work\n7.1\nEvaluation on LLMs\nDue to the remarkable performance achieved by LLMs on numerous tasks, evaluating LLMs continually gains\nwide attention. The topics of evaluation span from traditional natural language processing tasks [26,66,67], to\ntheir robustness [44,69\u201371,79], hallucination [36,53], ethics [15,22,59], and education [12,25], medical [18,28],\nagents [50,58], etc..\nAdvGLUE [70] stands as a static dataset for evaluating adversarial robustness of input samples. Decod-\ningTrust [69] undertakes a comprehensive assessment of trustworthiness in GPT models, notably GPT-3.5\nand GPT-4. The research delves into areas like toxicity, stereotype bias, adversarial challenges, and privacy.\nSpecifically, they evaluate the robustness on standard datasets AdvGLUE [70] and AdvGLUE++ [69].\nSpecifically for adversarial robustness, DecodingTrust also focuses on evaluating the robustness of input\nsamples instead of prompts and it still uses static datasets rather than an actionable benchmark suite.\nCompared with these literature, PromptBench is positioned as an open benchmark concentrating on\nadversarial prompts rather than samples (and it can be extended to include samples). Note that the prompts\nare general instructions to assist the in-context learning of LLMs to perform specific tasks, and they can be\n16\ncombined with many samples in certain tasks. Prompts are indispensable in human-LLMs interaction while\ninput samples may not be needed, which means that prompts are versatile and it is essential to evaluate their\nrobustness. Moreover, PromptBench is not a static dataset. It not only facilitates robustness evaluations but\nalso provides the necessary tools, codes, and analysis for extensions, welcoming the inclusion of new datasets\nand LLMs, thus underscoring its flexibility and broad applicability.\n7.2\nSafety of LLMs\nWe mimic the potential user prompts by creating adversarial prompts, but the main purpose is not to actually\nattack the model. This distinguishes our work from existing efforts in safety research of LLMs. Specifically,\nboth SafetyPrompts [61] and prompt injection attacks [24, 38, 49] are engineered to spotlight potentially\nharmful instructions that could steer LLMs into delivering outputs misaligned with human values or perform\nunintended actions such as data leakage and unauthorized access. Adversarial prompts are crafted to mimic\nplausible mistakes an end-user might inadvertently make. Our goal is to assess the extent to which these\nprompts, even if they slightly deviate from the norm, can skew LLM outcomes. These adversarial prompts\nretain their semantic integrity, ensuring they\u2019re virtually imperceptible for humans. The adversarial prompts\nare not designed to elicit harmful or misleading responses from LLMs.\n7.3\nTextual adversarial attacks\nPrompt attacks and textual adversarial attacks [21,30,34,35,43,55,81] are both rooted in similar foundational\nalgorithms, but differ in critical ways:\n\u2022 Target of attack: Prompt attacks target the instruction (prompts) for LLMs while vanilla adversarial\nattacks focus on the samples. In numerous tasks, the data might be optional, while prompts remain\nindispensable. For example, \u201cWrite a story about a fox.\u201d and \u201cGive me some investigation suggestions.\u201d\nare all prompts with no samples. This makes our prompt attack more general.\n\u2022 Universality of adversarial prompts: An adversarial prompt, represented as \u00afP, works as a common threat\nfor all samples related to a specific task. For example, if P is designed to instruct LLMs to solve math\nproblems, then \u00afP can be used for many different math problems and datasets. This ability is significantly\ndifferent from current NLP adversarial benchmarks.\nIn essence, prompt attacks seek to delve into the universality [41,65] of adversarial prompts. We argue this\noffers an innovative lens to assess the robustness of language models, complementing insights from existing\nbenchmarks like AdvGLUE [70], and AdvGLUE++ [69].\n8\nConclusion\nThe robustness of prompts in LLMs is of paramount concern in security and human-computer interaction.\nIn this paper, we thoroughly evaluated the robustness of LLMs to adversarial prompts using the proposed\nPromptBench benchmark. They key is to leverage adversarial attack approaches to mimic potential pertur-\nbations such as typos, synonyms, and stylistic differences. We then conducted extensive experiments and\nanalysis on various tasks and models. While the results show that current LLMs are not robust enough to\nadversarial prompts, we further analyzed the reason behind it using attention visualization. Moreover, we\nanalyzed the frequent words to provide a guidance for both experts and non-experts in developing better\nprompt engineering tools. PromptBench will be open-sourced to the public to serve as a foundational tool for\nrobust LLMs research.\nReferences\n[1] Emily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the dangers\nof stochastic parrots: Can language models be too big? FAccT 2021, page 610\u2013623, New York, NY,\nUSA, 2021. Association for Computing Machinery.\n17\n[2] Stella Biderman, USVSN Sai Prashanth, Lintang Sutawika, Hailey Schoelkopf, Quentin Anthony,\nShivanshu Purohit, and Edward Raf. Emergent and predictable memorization in large language models.\narXiv preprint arXiv:2304.11158, 2023.\n[3] Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, Michael Pieler, USVSN Sai Prashanth, Shivanshu Purohit,\nLaria Reynolds, Jonathan Tow, Ben Wang, and Samuel Weinbach. Gpt-neox-20b: An open-source\nautoregressive language model, 2022.\n[4] Google Brain. A new open source flan 20b with ul2, 2023.\n[5] Tom B Brown, Dandelion Man\u00b4e, Aurko Roy, Mart\u00b4\u0131n Abadi, and Justin Gilmer. Adversarial patch. arXiv\npreprint arXiv:1712.09665, 2017.\n[6] Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan\nZhang.\nQuantifying memorization across neural language models.\nIn The Eleventh International\nConference on Learning Representations, 2023.\n[7] Daniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar, Brian Strope, and Ray Kurzweil. Universal sentence\nencoder for English. In EMNLP, pages 169\u2013174, Brussels, Belgium, November 2018. Association for\nComputational Linguistics.\n[8] Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00a8uker, Katsuhito Sudoh,\nKoichiro Yoshino, and Christian Federmann. Overview of the IWSLT 2017 evaluation campaign. In\nProceedings of the 14th International Conference on Spoken Language Translation, pages 2\u201314, Tokyo,\nJapan, December 14-15 2017. International Workshop on Spoken Language Translation.\n[9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source\nchatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\n[10] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep\nreinforcement learning from human preferences. Advances in neural information processing systems, 30,\n2017.\n[11] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent\nZhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,\nAdam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models,\n2022.\n[12] Wei Dai, Jionghao Lin, Hua Jin, Tongguang Li, Yi-Shan Tsai, Dragan Ga\u02c7sevi\u00b4c, and Guanliang Chen.\nCan large language models provide feedback to students? a case study on chatgpt. In 2023 IEEE\nInternational Conference on Advanced Learning Technologies (ICALT), pages 323\u2013325. IEEE, 2023.\n[13] Fred J Damerau. A technique for computer detection and correction of spelling errors. Communications\nof the ACM, 7(3):171\u2013176, 1964.\n[14] Databricks. Hello dolly: Democratizing the magic of chatgpt with open models, 2023.\n[15] Ameet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan.\nToxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335,\n2023.\n[16] Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria,\nMarvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the\ncerebras wafer-scale cluster, 2023.\n18\n[17] William B. Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In\nProceedings of the Third International Workshop on Paraphrasing (IWP2005), 2005.\n[18] Dat Duong and Benjamin D Solomon. Analysis of large-language model versus human performance for\ngenetics questions. European Journal of Human Genetics, pages 1\u20133, 2023.\n[19] Andreas Eisele and Yu Chen. MultiUN: A multilingual corpus from united nation documents. In\nProceedings of the Seventh International Conference on Language Resources and Evaluation (LREC\u201910),\nValletta, Malta, May 2010. European Language Resources Association (ELRA).\n[20] Marzieh Fadaee, Arianna Bisazza, and Christof Monz. Data augmentation for low-resource neural\nmachine translation. arXiv preprint arXiv:1705.00440, 2017.\n[21] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi. Black-box generation of adversarial text sequences to\nevade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pages 50\u201356,\nMay 2018.\n[22] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxicityprompts:\nEvaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462, 2020.\n[23] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572, 2014.\n[24] Kai Greshake, Sahar Abdelnabi, Shailesh Mishra, Christoph Endres, Thorsten Holz, and Mario Fritz.\nMore than you\u2019ve asked for: A comprehensive analysis of novel prompt injection threats to application-\nintegrated large language models. arXiv preprint arXiv:2302.12173, 2023.\n[25] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanp\u00a8a\u00a8a, and Juha Sorva. Ex-\nploring the responses of large language models to beginner programmers\u2019 help requests. arXiv preprint\narXiv:2306.05715, 2023.\n[26] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\nSteinhardt. Measuring massive multitask language understanding. In International Conference on\nLearning Representations, 2021.\n[27] Jordan Hoffmann et al. Training compute-optimal large language models, 2022.\n[28] Jason Holmes, Zhengliang Liu, Lian Zhang, Yuzhen Ding, Terence T Sio, Lisa A McGee, Jonathan B\nAshman, Xiang Li, Tianming Liu, Jiajian Shen, et al. Evaluating large language models on a highly-\nspecialized topic, radiation oncology physics. arXiv preprint arXiv:2304.01938, 2023.\n[29] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. Adaptive mixtures of\nlocal experts. Neural computation, 3(1):79\u201387, 1991.\n[30] Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? natural language\nattack on text classification and entailment. arXiv preprint arXiv:1907.11932, 2019.\n[31] Karen Kukich. Techniques for automatically correcting words in text. ACM computing surveys (CSUR),\n24(4):377\u2013439, 1992.\n[32] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples in the physical world, 2016.\n[33] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema challenge. In Thirteenth\ninternational conference on the principles of knowledge representation and reasoning, 2012.\n[34] Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. TextBugger: Generating adversarial text\nagainst real-world applications. In Proceedings 2019 Network and Distributed System Security Symposium.\nInternet Society, 2019.\n19\n[35] Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. BERT-ATTACK: Adversarial\nattack against BERT using BERT. In Proceedings of the 2020 Conference on Empirical Methods in\nNatural Language Processing (EMNLP), pages 6193\u20136202, Online, November 2020. Association for\nComputational Linguistics.\n[36] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large\nmulti-modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565, 2023.\n[37] Hanmeng Liu, Ruoxi Ning, Zhiyang Teng, Jian Liu, Qiji Zhou, and Yue Zhang. Evaluating the logical\nreasoning ability of chatgpt and gpt-4. arXiv preprint arXiv:2304.03439, 2023.\n[38] Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan\nZheng, and Yang Liu. Prompt injection attack against llm-integrated applications. arXiv preprint\narXiv:2306.05499, 2023.\n[39] Inbal Magar and Roy Schwartz. Data contamination: From memorization to exploitation. arXiv preprint\narXiv:2203.08242, 2022.\n[40] Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long short term memory\nnetworks for anomaly detection in time series. In Esann, volume 2015, page 89, 2015.\n[41] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial\nperturbations. In CVPR, pages 1765\u20131773, 2017.\n[42] John Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. Textattack: A framework\nfor adversarial attacks, data augmentation, and adversarial training in nlp. In EMNLP, pages 119\u2013126,\n2020.\n[43] Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. Stress\ntest evaluation for natural language inference. In ACL, pages 2340\u20132353, Santa Fe, New Mexico, USA,\nAugust 2018. Association for Computational Linguistics.\n[44] Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. Adversarial\nnli: A new benchmark for natural language understanding. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics, pages 4885\u20134901, 2020.\n[45] OpenAI. https://chat.openai.com.chat, 2023.\n[46] OpenAI. Gpt-4 technical report, 2023.\n[47] Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu. Improving adversarial robustness via\npromoting ensemble diversity. In International Conference on Machine Learning, pages 4970\u20134979.\nPMLR, 2019.\n[48] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation\nof machine translation. In ACL, pages 311\u2013318, July 2002.\n[49] F\u00b4abio Perez and Ian Ribeiro. Ignore previous prompt: Attack techniques for language models. arXiv\npreprint arXiv:2211.09527, 2022.\n[50] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789, 2023.\n[51] Massimo Quadrana, Alexandros Karatzoglou, Bal\u00b4azs Hidasi, and Paolo Cremonesi. Personalizing session-\nbased recommendations with hierarchical recurrent neural networks. In proceedings of the Eleventh ACM\nConference on Recommender Systems, pages 130\u2013137, 2017.\n[52] Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don\u2019t know: Unanswerable questions\nfor SQuAD. In ACL, pages 784\u2013789, Melbourne, Australia, July 2018. Association for Computational\nLinguistics.\n20\n[53] Vipula Rawte, Amit Sheth, and Amitava Das. A survey of hallucination in large foundation models.\narXiv preprint arXiv:2309.05922, 2023.\n[54] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Semantically equivalent adversarial rules for\ndebugging nlp models. In Proceedings of the 56th annual meeting of the association for computational\nlinguistics (volume 1: long papers), pages 856\u2013865, 2018.\n[55] Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. Beyond accuracy: Behavioral\ntesting of NLP models with CheckList. In ACL, pages 4902\u20134912, Online, July 2020. Association for\nComputational Linguistics.\n[56] David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning\nabilities of neural models. In ICLR, 2019.\n[57] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint\narXiv:1701.06538, 2017.\n[58] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\n[59] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. Societal biases in language\ngeneration: Progress and challenges. arXiv preprint arXiv:2105.04054, 2021.\n[60] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In\nEMNLP, pages 1631\u20131642, Seattle, Washington, USA, October 2013. Association for Computational\nLinguistics.\n[61] Hao Sun, Zhexin Zhang, Jiawen Deng, Jiale Cheng, and Minlie Huang. Safety assessment of chinese\nlarge language models. arXiv preprint arXiv:2304.10436, 2023.\n[62] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https:\n//github.com/tatsu-lab/stanford_alpaca, 2023.\n[63] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[64] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[65] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. Universal adversarial triggers\nfor attacking and analyzing nlp. arXiv preprint arXiv:1908.07125, 2019.\n[66] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel Bowman. Superglue: A stickier benchmark for general-purpose language understanding\nsystems. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\n[67] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. 2019. In the Proceedings\nof ICLR.\n[68] Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax, May 2021.\n21\n[69] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi\nXiong, Ritik Dutta, Rylan Schaeffer, et al. Decodingtrust: A comprehensive assessment of trustworthiness\nin gpt models. arXiv preprint arXiv:2306.11698, 2023.\n[70] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah,\nand Bo Li. Adversarial glue: A multi-task benchmark for robustness evaluation of language models.\narXiv preprint arXiv:2111.02840, 2021.\n[71] Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Haojun\nHuang, Wei Ye, Xiubo Geng, et al. On the robustness of chatgpt: An adversarial and out-of-distribution\nperspective. In International conference on learning representations (ICLR) workshop on Trustworthy\nand Reliable Large-Scale Machine Learning Models, 2023.\n[72] Zhiguo Wang, Wael Hamza, and Radu Florian. Bilateral multi-perspective matching for natural language\nsentences, 2017.\n[73] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471, 2018.\n[74] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V\nLe, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information\nProcessing Systems, 2022.\n[75] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In NAACL HLT, pages 1112\u20131122. Association for Computational\nLinguistics, 2018.\n[76] Thomas Wolf et al. Huggingface\u2019s transformers: State-of-the-art natural language processing, 2020.\n[77] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S\nMorcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. Model soups: aver-\naging weights of multiple fine-tuned models improves accuracy without increasing inference time. In\nInternational Conference on Machine Learning, pages 23965\u201323998. PMLR, 2022.\n[78] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs,\nRaphael Gontijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. Robust fine-tuning\nof zero-shot models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7959\u20137971, 2022.\n[79] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie,\nand Yue Zhang. Glue-x: Evaluating natural language understanding models from an out-of-distribution\ngeneralization perspective. In ACL 2023 Findings, 2023.\n[80] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of Thoughts: Deliberate problem solving with large language models, 2023.\n[81] Yuan Zang, Fanchao Qi, Chenghao Yang, Zhiyuan Liu, Meng Zhang, Qun Liu, and Maosong Sun.\nWord-level textual adversarial attacking as combinatorial optimization. In Proceedings of ACL, 2020.\n[82] Ziqi Zhang, Yuanchun Li, Jindong Wang, Bingyan Liu, Ding Li, Yao Guo, Xiangqun Chen, and Yunxin\nLiu. Remos: reducing defect inheritance in transfer learning via relevant model slicing. In Proceedings of\nthe 44th International Conference on Software Engineering, pages 1856\u20131868, 2022.\n[83] Terry Yue Zhuo, Zhuang Li, Yujin Huang, Yuan-Fang Li, Weiqing Wang, Gholamreza Haffari, and\nFatemeh Shiri. On robustness of prompt-based semantic parsing with large pre-trained language model:\nAn empirical study on codex. arXiv preprint arXiv:2301.12868, 2023.\n22\nA\nModels, Datasets, and Enviroments\nA.1\nModels\nHere, we list the brief introduction of each LLM in our experiments. For more details about Vicuna, please\nrefer to its GitHub repository4. For the other LLMs, please refer to Huggingface transformer repository [76].\n\u2022 Flan-T5-large [11]: Flan-T5-large is a derivative of the Text-to-Text Transfer Transformer (T5) model,\ndeveloped by Google.\n\u2022 Dolly-6B [14]: The Dolly-v1-6b model is a 6-billion parameter causal language model developed by\nDatabricks. It originates from EleutherAI\u2019s GPT-J [68] and has been fine-tuned on the Stanford Alpaca [62]\ncorpus, which comprises roughly 52K question/answer pairs.\n\u2022 Vicuna-13B [9]: Vicuna-13B, fine-tuned from the LLaMA-13B base model, was developed using\napproximately 70K user-shared conversations collected from ShareGPT.com via public APIs.\n\u2022 Cerebras-13B [16]: Cerebras-13B is based on the GPT-3 style architecture. All models in the Cerebras-\nGPT series have been trained according to Chinchilla scaling laws [27], which optimize compute efficiency\nby maintaining a ratio of 20 tokens per model parameter.\n\u2022 Llama2-13B [64]: The Llama2 model, developed by the FAIR team at Meta AI, is an autoregressive\nlanguage model that employs the transformer architecture.\n\u2022 GPT-NEOX-20B [3]: GPT-NEOX-20B is a large-scale implementation of GPT-based models, with\nNEOX-20B specifically referring to a variant of this series comprising 20 billion parameters.\n\u2022 Flan-UL2 [4]: Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same\nconfiguration as the UL2 model. It was fine-tuned using the \u201cFlan\u201d prompt tuning and dataset collection.\n\u2022 ChatGPT [45] and GPT-4 [46]: Developed by OpenAI, ChatGPT is a large language model trained\nto generate human-like text based on the prompt it\u2019s given. It uses the GPT-3 architecture and has been\nfine-tuned for more interactive and conversational tasks. GPT-4 is by far the best-performing LLMs.\nA.2\nDatasets\nGLUE\nThe GLUE dataset (General Language Understanding Evaluation) [67] is a collection of resources\ndesigned to assess and benchmark the performance of natural language processing (NLP) models across\nvarious language understanding tasks. In this study, we selected 8 tasks, including Sentiment Analysis\n(SST-2 [60]), Grammar Correctness (CoLA [73]), Duplicate Sentence Detection (QQP [72], MRPC [17]), and\nNatural Language Inference (MNLI [75], QNLI [67], RTE [67], and WNLI [33]).\nMMLU [26]\nTo evaluate the extensive world knowledge and problem-solving abilities of large language\nmodels, the MMLU dataset encompasses 57 tasks consisting of multiple-choice questions from diverse domains,\nsuch as mathematics, history, computer science, law, and more. This dataset serves as a massive multitask\ntest.\nSQuAD V2 [52]\nSQuAD v2 is a widely used dataset for training and evaluating natural language processing\nmodels in the domain of machine reading comprehension. SQuAD v2 enhances the original SQuAD dataset\n(SQuAD v1) by introducing unanswerable questions, increasing the challenge for models. For each question,\nthe model must either (1) identify the correct answer span within the passage (if the question is answerable)\nor (2) predict that the question is unanswerable (if there is no answer span within the passage).\n4https://github.com/lm-sys/FastChat\n23\nUN Multi [19]\nThe Multi UN dataset is a large parallel corpus of text gathered from official United\nNations documents. It comprises texts in six official languages of the United Nations: Arabic, Chinese,\nEnglish, French, Russian, and Spanish. The Multi UN dataset primarily contains formal texts, which may\nlimit its applicability to more informal language domains or conversational applications.\nIWSLT 2017 [8]\nThe IWSLT 2017 dataset (International Workshop on Spoken Language Translation 2017)\nis a collection of multilingual, multi-domain parallel text data specifically designed for evaluating spoken\nlanguage translation systems. The translation tasks include data from the TED Talks Open Translation\nProject, featuring parallel text data for multiple language pairs such as English-German, English-French,\nEnglish-Chinese, and English-Czech. The dataset consists of both spoken language transcriptions and their\ncorresponding translations.\nMath [56]\nDeepMind Mathematics Dataset is a collection of math problems aimed at evaluating the\nmathematical reasoning abilities of artificial intelligence models. The dataset challenges AI models to solve\na diverse range of mathematical problems, spanning from algebra to calculus, and tests their ability to\ncomprehend and reason via complex mathematical concepts.\nA.3\nEnvironments\nTo reproduce the computational environment used in this study, an environment file, environment.yml, is\nprovided in our repository. This YAML file lists all the dependencies and their specific versions used in\nthe study. Users can create an identical Conda environment using the command conda env create -f\nenvironment.yml.\nThe computational experiments were conducted on machines equipped with NVIDIA Tesla V100 GPUs\n(16GB of GPU memory each).\nB\nAttacks\nB.1\nDetails of attacks\nThe majority of our prompt attacks have been developed by adapting and revising strategies from TextAt-\ntack5 [42]. For the detailed settings of each attack, please refer to our code.\nCharacter Level: Techniques such as TextBugger and DeepWordBug manipulate text at the character\nlevel by introducing typos or errors within words through insertions, deletions, replacements, and replications.\nThese methods capitalize on the model\u2019s vulnerability to minor perturbations in individual characters,\nfrequently resulting in misclassification or erroneous interpretations.\nWe primarily adopt the settings from TextAttack for TextBugger and DeepWordBug, such as the repeat\nconstraint which prohibits modifying words that have already been altered. Additionally, For TextBugger,\nTextAttack enforces a constraint on the overall similarity between the sentence encodings of clean and\nadversarial prompts, utilizing the Universal Sentence Encoder [7] to generate text embeddings. In our study,\nwe set this minimum similarity threshold to 0.8. For DeepWordBug, TextAttack set constraint on edit\ndistance (Levenshtein Distance) as 30.\nWord Level: In this study, we employ BertAttack and TextFooler for word-level attacks.\nThese\napproaches focus on replacing words within the text with synonyms or contextually similar words. By making\nostensibly minor alterations to the input text, these attacks can deceive large language models into producing\nincorrect outputs or substantially modifying their predictions. We meticulously fine-tune the hyperparameters\nof BertAttack and TextFooler to obtain more appropriate synonyms.\nFor TextFooler, we set the minimum embedding cosine similarity between word and its synonyms as 0.6,\nand the minimum Universal Sentence Encoder similarity is 0.84. For BertAttack, the minimum Universal\nSentence Encoder similarity is 0.8.\nSentence Level: StressTest and CheckList serve as examples of sentence-level attacks, wherein adversaries\nattempt to distract the model by adding irrelevant or extraneous sentences to the input text. By incorporating\n5https://github.com/QData/TextAttack\n24\nTable 12: The Average performance and standard deviations of different models on different datasets.\nDataset\nT5\nDolly\nVicuna\nCerebras\nLlama2\nNEOX\nUL2\nChatGPT\nSST-2\n94.79\u00b10.49\n47.80\u00b19.30\n21.12\u00b115.40\n21.33\u00b123.02\n90.25\u00b12.23\n21.49\u00b113.35\n95.92\u00b11.03\n92.91\u00b13.32\nCoLA\n76.11\u00b11.28\n4.92\u00b19.04\n35.28\u00b120.12\n18.18\u00b123.82\n74.53\u00b11.87\n7.96\u00b114.23\n86.07\u00b10.36\n78.91\u00b11.75\nQQP\n86.67\u00b11.05\n0.53\u00b11.66\n24.74\u00b110.03\n0.00\u00b10.00\n23.23\u00b16.97\n0.00\u00b10.02\n88.25\u00b10.54\n81.49\u00b11.47\nMRPC\n80.75\u00b11.73\n0.17\u00b10.30\n50.15\u00b119.65\n0.01\u00b10.05\n49.15\u00b14.56\n0.01\u00b10.05\n86.03\u00b11.41\n72.71\u00b12.82\nMNLI\n81.39\u00b14.7\n0.78\u00b10.88\n12.90\u00b18.21\n0.87\u00b11.16\n57.30\u00b11.53\n0.00\u00b10.00\n83.50\u00b14.79\n76.71\u00b12.44\nQNLI\n85.12\u00b15.57\n0.05\u00b10.07\n27.76\u00b110.04\n0.00\u00b10.00\n14.90\u00b18.48\n4.22\u00b15.46\n93.68\u00b10.41\n77.53\u00b17.48\nRTE\n84.24\u00b11.16\n0.19\u00b10.77\n29.51\u00b115.12\n0.00\u00b10.00\n47.67\u00b11.92\n3.16\u00b14.40\n93.26\u00b10.51\n80.73\u00b13.24\nWNLI\n62.34\u00b13.31\n0.00\u00b10.00\n22.57\u00b115.96\n0.00\u00b10.00\n41.08\u00b11.71\n3.62\u00b15.10\n77.53\u00b11.4\n61.07\u00b16.22\nMMLU\n45.25\u00b10.83\n-\n15.31\u00b17.41\n-\n36.05\u00b17.76\n-\n53.04\u00b10.67\n63.33\u00b12.56\nSQuAD V2\n87.32\u00b10.43\n-\n-\n-\n-\n-\n89.78\u00b10.71\n68.35\u00b14.36\nIWSLT\n0.18\u00b10.04\n-\n-\n-\n-\n-\n0.21\u00b10.04\n0.23\u00b10.01\nUN Multi\n0.29\u00b10.02\n-\n-\n-\n-\n-\n0.33\u00b10.02\n0.34\u00b10.01\nMath\n14.22\u00b13.25\n-\n-\n-\n-\n-\n14.81\u00b11.35\n13.14\u00b18.48\nmisleading information into the text, these methods can potentially cause the model to lose focus on the\nprimary context, leading to inaccurate results. For the StressTest attack, we adopt similar settings to those\nin [67], appending \u201dand true is true, \u201d \u201dand false is not true, \u201d or \u201dand true is true \u201d for five\ntimes to the end of a prompt. For the CheckList attack, we generate 50 random sequences consisting of\nalphabets and digits, each with a length of 10, and append this random sequences into the end of a prompt.\nSemantic Level: At the human level, adversaries can construct prompts using various languages, such as\nChinese, French, Arabic, Spanish, Japanese, and Korean, subsequently translating these prompts into English.\nBy exploiting the nuances and idiosyncrasies of different languages during translation, it can introduce subtle\nambiguities, grammatical errors, or inconsistencies in the input prompt. This poses a formidable challenge\nfor NLP models in generating accurate and coherent responses.\nFor each language, we first construct 10 prompts based on a English prompt by GPT4 [46], then translate\nit back to English by Google Translator.\nC\nResults of clean prompts on all LLMs\nTable 12 showcases the performance of different models across various datasets when using clean prompts.\nCertain LLMs, including Dolly, Cerebras, and NEXO, encounter difficulties with some datasets.\nFor\ninstance, Dolly\u2019s accuracy for the QQP dataset is merely 0.53%, a stark contrast to T5\u2019s accuracy of 86.67%.\nConsequently, we focus our attack study on models that demonstrate superior performance, namely T5,\nVicuna, Llama2, UL2, ChatGPT, and GPT4.\nD\nAttention visualization techniques\nD.1\nAttention by Gradient\nConsider an input x = [t(1)\n1 , t(1)\n2 , ..., t(k)\nn ] comprised of k words and n tokens, where t(j)\ni\nrepresents the i-th\ntoken belonging to word wj, and let y be the corresponding label. Initially, LLM f\u03b8 decomposes each word\ninto tokens. Thus, tokens that correspond to the same word need to be concatenated, let the mapping\nfunction wj = M(t(j)\ni ). We first compute the gradient of each token according to:\ngt(j)\ni\n= \u2202L[f\u03b8(x), y]\n\u2202tj\ni\n.\n(2)\nOnce we obtain the gradients, we compute the word-level gradient by summing the token-level gradients\ncorresponding to each word:\ngwj =\nX\ni\u22080,1,...,n\ngt(j)\ni\ns.t. M(t(j)\ni ) = wj.\n(3)\nFinally, we calculate the l2 norm of each word\u2019s gradient, followed by min-max normalization to produce\na score swj for each word:\nswj = ||gwj||2 \u2212 min gwi\nmax gwi \u2212 min gwi\n.\n(4)\n25\nTable 13: The APDR and standard deviations of different attacks on different datasets by excluding the ones\nhuman annotators do not think acceptable.\nDataset\nCharacter-level\nWord-level\nSentence-level\nSemantic-level\nTextBugger\nDeepWordBug\nTextFooler\nBertAttack\nCheckList\nStressTest\nSemantic\nSST-2\n0.26\u00b10.39\n0.21\u00b10.36\n0.33\u00b10.35\n0.30\u00b10.39\n0.27\u00b10.39\n0.17\u00b10.34\n0.28\u00b10.36\nCoLA\n0.37\u00b10.39\n0.29\u00b10.36\n0.40\u00b10.33\n0.42\u00b10.31\n0.25\u00b10.32\n0.21\u00b10.28\n0.27\u00b10.35\nQQP\n0.20\u00b10.32\n0.18\u00b10.27\n0.26\u00b10.31\n0.29\u00b10.33\n0.13\u00b10.25\n-0.00\u00b10.21\n0.30\u00b10.36\nMRPC\n0.24\u00b10.33\n0.21\u00b10.30\n0.27\u00b10.30\n0.31\u00b10.29\n0.13\u00b10.27\n0.20\u00b10.30\n0.28\u00b10.36\nMNLI\n0.26\u00b10.37\n0.18\u00b10.31\n0.27\u00b10.36\n0.34\u00b10.32\n0.16\u00b10.26\n0.11\u00b10.27\n0.11\u00b10.04\nQNLI\n0.36\u00b10.39\n0.41\u00b10.36\n0.47\u00b10.33\n0.45\u00b10.30\n0.22\u00b10.37\n0.18\u00b10.26\n0.35\u00b10.33\nRTE\n0.24\u00b10.37\n0.22\u00b10.36\n0.26\u00b10.34\n0.28\u00b10.35\n0.19\u00b10.32\n0.18\u00b10.25\n0.28\u00b10.33\nWNLI\n0.28\u00b10.36\n0.26\u00b10.35\n0.27\u00b10.31\n0.28\u00b10.29\n0.19\u00b10.30\n0.19\u00b10.26\n0.36\u00b10.32\nMMLU\n0.18\u00b10.22\n0.11\u00b10.15\n0.19\u00b10.16\n0.31\u00b10.20\n0.14\u00b10.20\n0.03\u00b10.16\n0.17\u00b10.17\nSQuAD V2\n0.09\u00b10.17\n0.05\u00b10.08\n0.23\u00b10.25\n0.30\u00b10.29\n0.02\u00b10.03\n0.02\u00b10.04\n0.07\u00b10.09\nIWSLT\n0.09\u00b10.14\n0.11\u00b10.12\n0.26\u00b10.25\n0.12\u00b10.16\n0.10\u00b10.10\n0.17\u00b10.19\n0.18\u00b10.14\nUN Multi\n0.06\u00b10.08\n0.08\u00b10.12\n0.17\u00b10.19\n0.10\u00b10.13\n0.06\u00b10.07\n0.09\u00b10.11\n0.15\u00b10.18\nMath\n0.19\u00b10.17\n0.15\u00b10.13\n0.45\u00b10.32\n0.39\u00b10.27\n0.16\u00b10.11\n0.13\u00b10.08\n0.23\u00b10.13\nAvg\n0.23\u00b10.33\n0.20\u00b10.30\n0.29\u00b10.31\n0.31\u00b10.30\n0.16\u00b10.27\n0.13\u00b10.25\n0.24\u00b10.29\nD.2\nAttention by Deletion\nAttention by deletion is a prevalent method used in black-box textual attacks to determine the significance of\neach word in the input. Given an input x with the i-th word wi deleted, denoted as \u02c6x(i), the importance score\nof wi can be computed by taking the absolute difference of the loss function L evaluated at the complete\ninput x and the altered input \u02c6x(i):\nswj = |L[f\u03b8(x), y] \u2212 L[f\u03b8(\u02c6x(i)).y]|\n(5)\nThis raw score is then normalized using min-max normalization, yielding a final score swj for each word:\nswj =\nswj \u2212 min swi\nmax swi \u2212 min swi\n.\n(6)\nE\nThe visualization website for adversarial prompts\nIn order to provide an interactive and user-friendly platform for visualizing and exploring adversarial prompts,\nwe developed a web-based application using Streamlit hosted by Hugging Face and will be released in the\nfuture.\nThe visualization website, as shown in Figure 7, enables users to select from a variety of LLMs (T5, Vicuna,\nUL2, ChatGPT), datasets (SST-2, CoLA, QQP, MRPC, MNLI, QNLI, RTE, WNLI, MMLU, SQuAD V2,\nIWSLT 2017, UN Multi, Math), prompt types (zeroshot-task, zeroshot-role, fewshot-task, and fewshot-role),\nand attacks (TextBugger, DeepWordBug, BertAttack, TextFooler, CheckList, StressTest, and Semantic).\nBased on the user\u2019s selection, the application generates adversarial prompts tailored to the chosen model,\ndataset, prompt type and attack.\nF\nHuman study on semantic perserving of prompts\nIn our endeavor to validate the efficacy of our adversarial prompt generation, we engaged in a human-centric\nstudy. We enlisted the expertise of 5 independent evaluators with proficiency in the domain to critically\nassess the semantic congruence between the original and the generated adversarial prompts. For the study,\nwe randomly sampled a set of 100 adversarial prompts along with their respective original prompts. The\nevaluators were tasked with determining if each adversarial prompt was semantically equivalent to its original\ncounterpart, ensuring the absence of semantic drift. Such an evaluation provides insights into the reliability\nand robustness of our adversarial prompts.\n26\nFigure 7: Visualization website for adversarial prompts.\nThe human evaluation yielded a spectrum of results. Here, we present a few exemplars of both acceptable\nand unacceptable adversarial prompts as discerned by the evaluators:\n\u2022 SST-2\nOriginal prompt: Read the provided excerpt and choose between \u2019positive\u2019 and \u2019negative\u2019 to\ndescribe its sentiment:\nAttacked prompt: read the standard excerpt and pause between \u2019positive\u2019 and \u2019negative\u2019 to\ndescribe its thoughtful:\nUnacceptable.\n\u2022 CoLA\nOriginal prompt: Functioning as a grammar evaluation tool, analyze the given sentence and\ndecide if it is grammatically correct, responding with \u2019acceptable\u2019 or \u2019unacceptable\u2019:\nAttacked prompt: Functioning as a grammar evaluation Lool, analyze the Tgiven sentence and\ndeciRe if it is grammXatically correct, respoOnding with \u2019acceptable\u2019 or \u2019unacceptable\u2019:\nAcceptable.\n\u2022 MMLU\nOriginal prompt: As a knowledgeable individual in , provide your response to the multiple-choice\nquestion by choosing \u2019A\u2019, \u2019B\u2019, \u2019C\u2019, or \u2019D\u2019.\nAttacked prompt: As a knowledgeable everybody in , prescribes your rebuttal to the multiple-choice\nitem by chooses \u2019A\u2019, \u2019B\u2019, \u2019C\u2019, or \u2019D\u2019.\nUnacceptable.\n\u2022 Math\nOriginal prompt: Resolve the following mathematical question associated with:\nAttacked prompt: Resolve the following mathematical answer along with :\nAcceptable.\n27\nG\nResults excluding non-semantic preserving adversarial prompts\nTable 13 presents the attack results after excluding adversarial prompts that do not preserve semantics. It\ncan be observed that the APDR is still considerably high for each dataset.\n28\n"
  },
  {
    "title": "Designing a Better Asymmetric VQGAN for StableDiffusion",
    "link": "https://arxiv.org/pdf/2306.04632.pdf",
    "upvote": "2",
    "text": "Designing a Better Asymmetric VQGAN for StableDiffusion\nZixin Zhu1\u2217\nXuelu Feng1\u2217\nDongdong Chen2\nJianmin Bao2\nLe Wang1\nYinpeng Chen2\nLu Yuan2\nGang Hua 1,3\n1Xi\u2019an Jiaotong University\n2Microsoft\n3Wormpex AI Research\n{zhuzixin@stu., xueluF@stu., lewang@}xjtu.edu.cn\n{jianbao, yiche,luyuan}@microsoft.com,\n{cddlyf, ganghua}@gmail.com\nInput\nBaseline\nOurs\nGround Truth\nFigure 1: Comparing StableDiffusion-based inpainting (top) / editing [40] (bottom) results with default VQGAN and our\nproposed asymmetric VQGAN. The vanilla VQGAN in StableDiffusion will cause serious distortion even for the non-edited\nregions. In contrast, our proposed asymmetric VQGAN can preserve more details and deliver superior results.\nAbstract\nStableDiffusion is a revolutionary text-to-image genera-\ntor that is causing a stir in the world of image generation\nand editing. Unlike traditional methods that learn a diffu-\nsion model in pixel space, StableDiffusion learns a diffu-\nsion model in the latent space via a VQGAN, ensuring both\nefficiency and quality. It not only supports image genera-\ntion tasks, but also enables image editing for real images,\nsuch as image inpainting and local editing. However, we\nhave observed that the vanilla VQGAN used in StableD-\niffusion leads to significant information loss, causing dis-\ntortion artifacts even in non-edited image regions. To this\nend, we propose a new asymmetric VQGAN with two sim-\nple designs. Firstly, in addition to the input from the en-\ncoder, the decoder contains a conditional branch that in-\n* Work done during internship at Microsoft\ncorporates information from task-specific priors, such as\nthe unmasked image region in inpainting. Secondly, the de-\ncoder is much heavier than the encoder, allowing for more\ndetailed recovery while only slightly increasing the total in-\nference cost. The training cost of our asymmetric VQGAN\nis cheap, and we only need to retrain a new asymmetric de-\ncoder while keeping the vanilla VQGAN encoder and Sta-\nbleDiffusion unchanged. Our asymmetric VQGAN can be\nwidely used in StableDiffusion-based inpainting and local\nediting methods. Extensive experiments demonstrate that it\ncan significantly improve the inpainting and editing perfor-\nmance, while maintaining the original text-to-image capa-\nbility. The code is available at https://github.com/\nbuxiangzhiren/Asymmetric_VQGAN.\narXiv:2306.04632v1  [cs.CV]  7 Jun 2023\n1. Introduction\nDiffusion models have emerged as the most popular gen-\nerative models, achieving remarkable results in image syn-\nthesis.\nEarly diffusion models required significant com-\nputational resources, as they performed the diffusion pro-\ncess in the high-dimensional pixel space of RGB images.\nTo reduce the training cost while preserving the generation\nquality, laten diffusion model (LDM) [31] employs VQ-\nGAN [10] to move the diffusion step to a low-dimensional\nlatent space. In a subsequent development, StableDiffusion\nhas further scaled up LDM with a larger model and data\nscale, resulting in a highly powerful general text-to-image\ngenerator. Since its public release, it has drawn significant\nattention in the world of generative AI.\nStableDiffusion not only possesses text-to-image gen-\neration capabilities, but it also supports various editing-\nrelated tasks, such as inpainting [31] and local editing\ntasks [5, 24, 40].\nFor these editing tasks, StableDiffu-\nsion can generate new content for selected regions based\non user-supplied input condition while aiming to preserve\nother regions.\nHowever, we have observed that the re-\nsults of StableDiffusion-based editing in all existing meth-\nods [31, 40] suffer from distortion artifacts in the non-edited\nimage regions, especially for the regions with fine-grained\nstructures (e.g., texts). For example, as depicted in Figure\n1, despite our intention to only inpaint the black mask re-\ngion or composite the object provided in the reference im-\nage into the mask region, we observe severe distortion in\nthe non-mask/non-edited areas.\nAfter extensive analysis, we have found that these issues\nare caused by the quantization error present in the vanilla\nVQGAN utilized by StableDiffusion.\nSpecifically, VQ-\nGAN utilizes an encoder to downsample images multiple\ntimes into a latent space, after which the downsampled im-\nage vectors are quantified based on a codebook. As a result,\nquantization errors are inevitable even for the non-edited\nregions if we only feed the output of the encoder into the\ndecoder, as the vanilla VQGAN operates by default. Addi-\ntionally, during the inference process, the convolutional lay-\ners utilized in VQGAN\u2019s encoder impact the feature vectors\nof non-masked regions due to the masked regions.\nTo this end, we propose a new asymmetric VQGAN with\ntwo simple yet effective designs in the decoder part. Firstly,\nwe reformulate the VQGAN decoder as a conditional de-\ncoder to better support local editing tasks. This is achieved\nby incorporating an extra branch that can integrate informa-\ntion from task-specific priors. For instance, for inpainting\nor local editing tasks, we feed the non-edited regions into\nthis branch so that the decoder can use both the output of\nthe encoder and the original non-edited regions as inputs.\nIn contrast, the vanilla VQGAN decoder only takes the out-\nput of the VQGAN encoder as input. Secondly, we enhance\nthe capability of the decoder by using deeper or wider de-\ncoders rather than similar complexity as the encoder. This\nstronger decoder can better preserve the non-edited regions\nand recover more details from the quantized output of the\nencoder. Considering that the most time-consuming part of\nStableDiffusion inference is the iterative diffusion process,\nthis larger decoder only slightly increases the total inference\ncost.\nIn addition to the inference cost, the training cost of our\nasymmetric VQGAN is still very cheap. We only need to\nretrain a new asymmetric decoder while keeping StableDif-\nfusion and the original vanilla VQGAN encoder unchanged.\nAdditionally, by alternatively feeding/not feeding the task-\nspecific priors into the decoder, our asymmetric VQGAN\ncan naturally support both editing tasks that require the task-\nspecific priors and pure generation tasks such as text-to-\nimage generation, which do not require such prior input.\nTo demonstrate the effectiveness of our asymmetric VQ-\nGAN, we conducted experiments on three different tasks.\nIn inpainting and local editing tasks with masks (paint-by-\nexample [40]), our asymmetric VQGAN achieved state-of-\nthe-art performance (1.03 FID on the Place dataset [44] and\n86.35% CLIP score on the COCOEE dataset [40]). In the\npure text-to-image task, our model can also achieve com-\nparable or even better results compared to the original Sta-\nbleDiffusion. Our contributions can be summarized in the\nbelow three-folds:\n\u2022 To the best of our knowledge, we are the first that ex-\nplicitly point out and investigate the distortion problem\nin StableDiffusion-based editing methods.\n\u2022 We design a new asymmetric VQGAN to address the\nabove distortion issues with two simple yet effective\ndesigns. Compared to the typical symmetric VQGAN\ndesign, this new design can better preserve the non-\nedited regions and recover details, while maintaining a\nlow training and inference cost.\n\u2022 Our asymmetric VQGAN achieves state-of-the-art\nperformance on two representative tasks: the inpaint-\ning task on the Place dataset [44] and the local edit-\ning task (i.e., paint by example [40]) on the COCOEE\ndataset.\n2. Related Work\n2.1. Diffusion Models\nDiffusion models are a powerful family of generative\nmodels that have recently evolved and drawn significant\nattention due to their impressive performance on various\ntasks. Recent works [8, 35] have demonstrated that diffu-\nsion model can achieve astonishing results in high-fidelity\nimage generation, even outperforming generative adversar-\nial networks. Diffusion models are naturally ideal for learn-\ning models from complex and diverse data, and many vari-\nants have been proposed recently. For instance, Denoising\nDiffusion Probabilistic Models (DDPMs) [13] are the most\npopular diffusion models that learn to perform a diffusion\nprocess on a Markov chain of latent variables. And Denois-\ning Diffusion Implicit Models (DDIMs) [35] is further pro-\nposed to accelerate the denoising process. To improve the\nefficiency while preserving high generation quality, Latent\nDiffusion Models (LDM) [31] propose to learn a diffusion\nmodel within the latent space rather than the pixel space.\nDiffusion models have proven to be highly effective in\na variety of applications, including image generation [29,\n26, 14, 4], image-to-image translation [6, 37, 38], super-\nresolution [31], and image editing [26, 1].\nParticularly,\nrecent advancements in diffusion models [34] have led to\nstate-of-the-art image synthesis [8, 10, 9, 26, 32, 11, 2, 31]\nas well as generative models for other modalities such as\ntext [19], audio [18], and video [15]. In this paper, we focus\non designing a new VQGAN architecture that can improve\nthe performance of StableDiffusion like diffusion models\nthat operates in the latent space, for image generation and\nediting tasks.\n2.2. VQGAN\nThe Vector Quantized Variational Autoencoder (VQ-\nVAE) [27, 30] is a widely used method for learning discrete\nimage representations.\nVQ-based techniques have been\nsuccessfully applied to image generation and completion,\nleveraging a learned codebook in the feature domain. While\n[27] extended this approach to use a hierarchy of learned\nrepresentations, these methods still rely on density estima-\ntion from convolutional operations, making it challenging to\ncapture long-range interactions in high-resolution images.\nIn order to address above issues, the Vector Quantized Gen-\nerative Adversarial Network (VQGAN) [10] was proposed,\nwhich identified that a powerful latent space auto-encoder\nis critical to capture image details for the following gener-\nation stage. So VQGAN uses the quantization procedure\nof VQVAE and improves the richness of VQVAE\u2019s learned\ncodebook. VQGAN uses the quantization procedure of VQ-\nVAE and improves the richness of its learned codebook.\nSpecifically, it employs adversarial loss and perceptual loss\nto train a better autoencoder in the first stage for synthesiz-\ning greater image details. VQ-based generative models have\nbeen applied in many tasks, including text [19], audio [18],\nimage inpainting [22], and video [15] generations. While\nVQGAN has numerous benefits, the quantization error it\nintroduces can lead to losing image details and causing se-\nrious distortion. Motivated by this observation, unlike the\nconventional symmetric VQGAN design, we explore a new\nasymmetric VQGAN to retain more details, which can ben-\nefit both image generation and editing tasks.\nEncoder\nStable Diffusion\n\ufffd\ufffd\n15\n1\n23\n49\n2 34\n51 7\n9 13\n87 75\n23\n5\n8 48\nConditional Branch \nLarger\nDecoder\n\ufffd\ufffd\nQuantized\nMasked Image\nOutput\n\ufffd0\nEncoder\nStable Diffusion\n\ufffd\ufffd\n15\n1\n23\n49\n2 34\n51 7\n9 13\n87 75\n23\n5\n8 48\n\ufffd\ufffd\nQuantized\nMasked Image\nOutput\n\ufffd0\nDecoder\nFigure 2: Top: The inference process of our symmetric VQ-\nGAN. Bottom: The inference process of vanilla VQGAN.\n3. Method\nVQGAN plays an important role in StableDiffusion to\nmap the original high-dimensional pixel space to low-\ndimensional latent space. However, this mapping process\ncan lead to information loss in image conditional tasks,\ncausing a lack of detail that hurt the quality of the gener-\nated result. In this section, we will first discuss the issue of\ninformation loss in VQGAN and then introduce our solu-\ntion, the asymmetric VQGAN, which serves to address this\nchallenge.\n3.1. Information loss in VQGAN\nVQGAN aims to compress the pixel space into discrete\nlatent space. Suppose X \u2208 RH\u00d7W\u00d73 is the input image.\nVQGAN first utilizes a CNN-based Encoder to obtain its\nfeature variable \u02c6z \u2208 Rh\u00d7w\u00d7nz, where h \u00d7 w is the spatial\nresolution and nz is the channel of the latent vector. Then\nVQGAN aims to be able to represent it with discrete code-\nbook {zk}K\nk=1, where each spatial code \u02c6zij find its closest\ncodebook entry zk in the codebook, the process can be de-\nnoted as follows:\nzij = q(\u02c6zij) := arg min\nk\u22081,2,..,K\n\u2225\u02c6zij \u2212 zk\u2225\n(1)\nwhere q is the quantization encoder that maps the vector\nto an index of the codebook, Based on the quantized code-\nwords z, VQGAN then adopts the decoder to reconstruct\nthe input image x.\nSuppose the reconstructed result is\n\u02c6x = Dec(q(Enc(x)).\nThen the model and codebook can be trained end-to-end\nvia the loss function:\nLVQ(Enc, Dec, {zk}K\nk=1) = Lpixel + \u03bbLpercep\n+ \u2225sg[Enc(x)] \u2212 z\u22252\n2\n+ \u03b2 \u2225sg [z] \u2212 Enc(X)\u22252\n2 .\n(2)\nwhere Lpixel = \u2225x\u2212\u02c6x\u22252 is pixel-level loss and Lpercep per-\nceptual loss calculated with VGG16 [33] network. sg[\u00b7] is\nEncoder\n15\n1\n23\n49\n2 34\n51 7\n9 13\n87 75\n23\n5\n8 48\nor\nInput\nOutput\nRandom Mask\nFull Mask\nConv\nConv\nConv\nMasked Image\nConv\nConv\n\u2026\n\u2026\n\ufffd\n\ufffd\nMGB\n\ufffd\n\ufffd\nMGB\nMask \ufffd =\nor\nElementwise \nMultiplication\nElementwise \nAddition\nMGB\nMask Guided \nBlend\nOur asymmetric decoder\nFigure 3: The training process of our symmetric VQGAN. We generate two kinds of masks, i.e., random mask and full mask.\nThe quantized vector from the encoder is fed to our asymmetric decoder. At the same time, the masked image is sent to our\ndecoder as the input of conditional branch. After the conditional and main branches blend, the decoder output final results.\nthe stop-gradient operator, \u03b2 is a hyper-parameter for loss\nweight.\nTo further improve the quality of the generated\nsamples, a discriminator is employed to perform an adver-\nsarial training process with the encoder and decoder. Since\nthe continuous pixel space is mapped into limited discrete\nspace, the information loss phenomenon exists during this\nprocess.\nWe also notice that in some versions of StableDiffusion,\nthe compressing from pixel space to latent space is trained\nby a KL-reg. The KL-reg shares a similar purpose to VQ-\nGAN since they all try to avoid arbitrarily high-variance la-\ntent spaces, and both of them share a similar issue: infor-\nmation loss from pixel space to latent space.\n3.2. Asymmetric VQGAN\nDue to the impressive performance of StableDiffusion\non text2image generation, it has been widely applied to var-\nious conditional image generation tasks. One of the most\nimportant and typical of these conditions is the use of im-\nage input. However, according to our analysis, these image\nconditions must be mapped into latent space to satisfy the\ndiffusion process of StableDiffusion. As a result, these con-\nditional images may lose some of their original information\nduring manipulation. Our focus in this paper is to preserve\nthe information of the conditional image input while leaving\nthe pre-trained weights of StableDiffusion unchanged.\nTo this end, we propose the Asymmetric VQGAN, to\npreserve the information of conditional image input. Asym-\nmetric VQGAN involves two core designs compared with\nthe original VQGAN as shown in Figure 2. First, we in-\ntroduce a conditional branch into the decoder of the VQ-\nGAN which aims to handle the conditional input for image\nmanipulation tasks. Second, we design a larger decoder for\nVQGAN to better recover the losing details of the quantized\ncodes. In the next section, we will introduce the detailed\nstructure and training strategy of Asymmetric VQGAN.\nConditional decoder.\nWe design a conditional decoder\nthat aims to preserve the details of the conditional input. As\nillustrated in Figure 3, Suppose the conditional image is a\nmasked input Y with mask m, We propose to represent the\nconditional image input as multi-level feature maps, instead\nof compressing it into single-layer features. Concretely, we\nfeed the conditional input Y into a lightweight encoder E\nand extract the feature map at different layers as the condi-\ntional input representation. More formally, we can define\nfE(Y ) =\n\b\nf 1\nE(Y ), f 2\nE(Y ), \u00b7 \u00b7 \u00b7 f n\nE(Y )\n\t\n,\n(3)\nwhere f k\nE(Y ) represents the k-th level feature map from the\nencoder E, n is the number of feature levels.\nThen these features will be integrated into the decoder\nvia a mask-guided blending (MGB) module. MGB aims to\npreserve the decoders\u2019 capability for decoding latent codes\nwhile making full use of the features from encoder E. It\nutilizes a mask to directly copy the masked region of the\ndecoder feature while combining the unmasked region fea-\nture from the encoder E. Specifically, suppose the feature\nat k-th level of the decoder is f k\nDec(z). So the blending\nprocess can be formulated as:\nf k\nDec(z) = f k\nDec(z) \u2297 m + f k\nE(Y ) \u2297 \u00afm,\n(4)\nwhere \u2297 is element-wise multiplication, \u00afm = 1\u2212m. With\nthis designed mask-guided blending module, we do not re-\nquire any modification to the decoder and just insert several\nMGB modules into the decoder network while keeping the\nstructures unchanged.\nLarger decoder.\nTo further enhance the capability of de-\ncoders for recovering details from given latent codes, we\nenlarge the decoder model size of the original VQGAN. In-\ncreasing the model size of VQGAN is efficient since during\nthe inference stage of StableDiffusion for conditional image\ninput tasks, the decoder only requires to be forwarded one\nMethod\n40-50% masked\nAll samples\nFID\u2193\nLPIPS\u2193\nFID\u2193\nLPIPS\u2193\nLaMa [36]\n12.0\n0.24\n2.21\n0.14\nCoModGAN [43]\n10.4\n0.26\n1.82\n0.15\nRegionWise [23]\n21.3\n0.27\n4.75\n0.15\nDeepFill v2 [41]\n22.1\n0.28\n5.20\n0.16\nEdgeConnect [25]\n30.5\n0.28\n8.37\n0.16\nStableDiffusion* [31]\n8.26\n0.324\n2.27\n0.245\nOurs\n6.80\n0.244\n1.03\n0.141\nTable 1: Comparison of inpainting performance on 30k\ncrops of size 512 \u00d7 512 from test images of Places [44].\nSince the high-resolution images of Places is not available,\nwe resize the 256 \u00d7 256 images to 512 \u00d7 512. The col-\numn 40-50% reports metrics computed over hard examples\nwhere 40-50% of the image region have to be inpainted. *\ndenotes that the results are reproduced by us.\ntime while the StableDiffusion model needs to be forwarded\nmany times with a significantly larger model size.\nTraining strategies.\nDuring training, we use the same\nweights and codebook from the original VQGAN in the\nAsymmetric VQGAN and only train the new decoder. To\navoid the decoder from developing a simple solution of re-\ncovering information only from the conditional masked in-\nput, we consider two scenarios: one where the mask is ran-\ndomly generated, and one where the mask is completely\nfilled with 1, meaning the decoder needs to rely on the la-\ntent codes to recover the image. We alternately use these\ntwo scenarios for 50% of the training process. For the train-\ning objectives, we use the pixel level loss, perceptual loss,\nand adversarial loss as described in Section 3.1 to only up-\ndate the weights of the Decoder.\nAsymmetric VQGAN is lightweight and flexible, it\nkeeps the encode and latent space diffusion process un-\nchanged, in which we can leverage all the pre-trained\nweights of the Encoder and StableDiffusion. We just need\nto change the decoder part while enjoying the strong capa-\nbility of StableDiffusion for a wide range of tasks.\n4. Experiments\nIn order to demonstrate the outstanding application po-\ntential of our model, we conduct sufficient experiments\nbased on our base and large models in three different tasks.\nImplementation details.\nIn accordance with the train-\ning setting used for VQGAN [10] in StableDiffusion [31],\nwe train our asymmetric VQGAN on the ImageNet [7]\ndataset. During training, we preprocess the image resolu-\ntion to 256 \u00d7 256, and train our base model for 12 epochs.\nThis process took approximately 5 days on 8 NVIDIA V100\nGPUs, with a batch size per GPU of 10 and a learning rate\nwarmed up to 3.6e-4 in the first 5,000 iterations. The learn-\ning rate was then decayed with a cosine scheduler. As for\nour large model, we use 64 NVIDIA V100 GPUs, with a\nbatch size per GPU of 5 and a learning rate warmed up to\n7.2e-4 in the first 5,000 iterations. The training for the large\nmodel also took around 5 days, with the learning rate de-\ncayed using a cosine scheduler too.\nEvaluation benchmark. For our inpainting task, we evalu-\nated our model using the same protocol as LaMa [36], a re-\ncent inpainting model, to generate image masks. Our exper-\niments were conducted on two popular datasets: Places [44]\nand ImageNet [7].\nDue to the unavailability of high-\nresolution images in the Places dataset, we resized the\n256 \u00d7 256 images to 512 \u00d7 512. For the ImageNet dataset,\nwe try to randomly select 3 images from each class in the\nImageNet validation dataset. However, since some cate-\ngories do not have 3 images available, we end up selecting\na total of 2,968 images for our experiments.\nFor the paint-by-example task, we evaluate our model\nusing the COCOEE [40] dataset.\nCOCOEE is a COCO\nexemplar-based image editing benchmark that contains\n3,500 source images from the MSCOCO [20] validation\nset. Each image contains only one bounding box, and the\nmask region is no more than half of the entire image. The\ncorresponding reference image patch is chosen from the\nMSCOCO training set.\nIn the text-to-image task, we evaluated our model using\nthe MSCOCO [20] validation set. Following the widely-\nused \u201dKarpathy\u201d split [17], we used 5,000 images for val-\nidation, with each image having approximately 5 captions.\nHence, we generated a total of 25,000 images according to\nthe captions. All images were resized to 512 \u00d7 512.\nEvaluation metrics. In the inpainting task, we use FID [12]\nand LPIPS [42] as metrics to evaluate the quality of our\nmodel\u2019s predictions. Additionally, to showcase the model\u2019s\nability to preserve the non-edited regions of the image, we\nreport the mean squared error (MSE) between our predic-\ntions and ground truth for such non-edited regions.\nIn\nthe paint-by-example task, we measure the quality of our\nmodel\u2019s output using the mean squared error (MSE) of non-\nedited image regions and the CLIP score [28]. The CLIP\nscore evaluates the similarity between the edited region and\nthe reference image. Specifically, we resize the two images\nto 224\u00d7224, extract their features via CLIP image encoder,\nand calculate their cosine similarity. A higher CLIP score\nindicates that the edited region is more similar to the ref-\nerence image. In the text-to-image task, we evaluate our\nmodel\u2019s performance using FID and IS [3] as metrics.\n4.1. Evaluation on the Inpainting Task\nComparison with state-of-the-art methods.\nTable 1\nshows the comparison of our inpainting approach with other\nstate-of-the-art methods. Our results demonstrate that ap-\nplying our asymmetric VQGAN to StableDiffusion can im-\nInput\nBaseline \nNa\u00efve Blend\nOurs\nFigure 4: The harmonization of our asymmetric VQGAN\nin inpainting task. \u201cNaive Blend\u201d denotes that adding the\nnon-edited image regions of inputs and the edited regions\nof results. StableDiffusion [31] is our baseline. All results\nare reported on ImageNet validation dataset.\nprove FID by 1.24. Additionally, our approach outperforms\nother methods on hard examples where 40-50% of the im-\nage region needs to be inpainted, further highlighting the\neffectiveness of our conditional branch and larger decoder.\nEffectiveness of our modules. This ablation study aims\nto support the effectiveness of our conditional branch and\nlarger decoder. Table 2 presents the inpainting results that\ncompare our conditional decoder with the original decoder\nin StableDiffusion [31]. Specifically, we replace the orig-\ninal decoder with our conditional decoder to decode the\nquantized vectors obtained from StableDiffusion. It\u2019s worth\nnoting that the sample results sent to different decoders are\nthe same since the diffusion process generates various sam-\nple results.\nBoth our base model and larger model show an improve-\nment of approximately 2.00 on FID and 50% on LPIPS.\nFurthermore, the error of non-edited image regions is sig-\nnificantly reduced. These results indicate that with the help\nof the conditional branch, the ability of VQGAN to pre-\nserve details has been greatly improved. If we do not use\nthe conditional branch, our base model can still perform the\nsame function as the original decoder. And our larger model\ncan further outperform the original decoder. These results\ndemonstrate that our model is compatible with various ap-\nplications, whether they have masks or not.\nMethod\nFID\u2193\nLPIPS\u2193\nPre error\u2193\nStableDiffusion [31]\n9.57\n0.255\n1082.8e\u22125\nOurs (base) w/ mask\n7.604\n0.137\n5.7e\u22125\nOurs (base) w/o mask\n9.48\n0.248\n1078.6e\u22125\nOurs (large) w/ mask\n7.55\n0.136\n2.6e\u22125\nOurs (large) w/o mask\n9.17\n0.243\n1047.7e\u22125\nTable 2: Results of our different modules in inpainting task.\nStableDiffusion is our baseline. All results are reported in\nImageNet validation dataset.\nMethod\nFID\u2193\nLPIPS\u2193\nPre error\u2193\nOurs (base) add.\n7.60\n0.137\n5.7e\u22125\nOurs (base) concat.\n7.64\n0.138\n5.3e\u22125\nOurs (large) add.\n7.55\n0.136\n2.6e\u22125\nOurs (large) concat.\n7.56\n0.137\n4.4e\u22125\nTable 3: Ablation study of different blending ways. The\n\u201cadd.\u201d denotes addition and \u201cconcat.\u201d denotes concatena-\ntion. All results are reported in ImageNet validation dataset.\nMethod\nCLIP score\u2191\nPre error\u2193\nBlended Diffusion [1]\n80.65\n-\nDCCF [39]\n82.18\n-\nStableDiffusion [31]\n75.33\n-\nPaint-by-Example [40]\n84.97\n-\nPaint-by-Example* [40]\n85.67\n588.86\nOurs\n86.35\n0.76\nTable 4: Comparison of paint-by-example [40] performance\non 3500 images of size 512\u00d7512. * denotes that the results\nare reproduced by us. All results are reported on COCOEE.\nAddition or concatenation. Our model offers two different\nblending methods: mask-guided addition and mask-guided\nconcatenation. This ablation study aims to explore how they\nimprove VQGAN. Since the mask is a hard mask, meaning\nits value is either 0 or 255, our conditional branch does not\nuse Partial Convolutional Layer [21] when the blending way\nis mask-guided addition. This means that the main branch\nis responsible for the edited regions, while the conditional\nbranch handles the non-edited image regions. As a result,\nthe Partial Convolutional Layer, which infers the informa-\ntion of the edited regions, cannot contribute to the condi-\ntional branch. The detailed results are shown in Table 3.\nThe overall performance of the concatenation is similar to\nthe addition. Finally, we choose the addition as our blend-\ning way.\nVisual Comparison.\nIn order to preserve the details of\nnon-edited image regions, a common approach is to post-\nprocess by adding the non-edited regions of the inputs to\nthe edited regions of the results. However, this naive solu-\nReference\nPainting by Example\nOurs\nSource Image\nMasked Image\nFigure 5: The preserving ability of our asymmetric VQGAN in paint-by-example [40] task. All results are reported in\nCOCOEE dataset.\nMethod\nCLIP score\u2191\nPre error\u2193\nPaint-by-Example*\n85.67\n588.86\nOurs (base)\n86.24\n1.37\nOurs (base) w/o mask\n86.15\n478.47\nOurs (large)\n86.35\n0.76\nOurs (large) w/o mask\n86.32\n451.67\nTable 5:\nResults of our different modules in paint-by-\nexample [40] task. Paint-by-Example is our baseline. All\nresults are reported in COCOEE dataset.\ntion can cause non-harmonization issues, as shown in Fig-\nure 4 in the third column. In contrast, our asymmetric VQ-\nGAN generates images with the same level of harmoniza-\ntion as the baseline (original StableDiffusion), while pre-\nserving many details of the non-edited regions. This indi-\ncates that our method effectively preserves details without\ncausing harm to the harmonization of the image. Overall,\nour results demonstrate that our model with the asymmetric\ndecoder and mask-guided addition blending way can signif-\nicantly improve the performance of inpainting tasks, while\nmaintaining harmonization and preserving details of non-\nedited image regions.\nMethod\nFID\u2193\nIS\u2191\nStableDiffusion [31]\n19.88\n37.55\nOurs (base) w/o mask\n19.92\n37.52\nOurs (large) w/o mask\n19.75\n37.64\nTable 6: Results of text-to-image task. StableDiffusion is\nour baseline. Even though there is no any mask, our asym-\nmetric VQGAN is comparable or superior to the baseline,\nStableDiffusion. All results are reported on MSCOCO.\n4.2. Paint by example\nPaint-by-example [40] is a novel image editing scenario\nthat semantically alters image content based on an exemplar\nimage. Their approach relies on StableDiffusion as a strong\nprior, making our model easily applicable to their method.\nComparison with state-of-the-art methods.\nThe com-\nparison results are presented in Table 4, where the \u201cCLIP\nscore\u201d represents the similarity between the edited regions\nof edited images and reference images, while Pre error\u201d de-\nnotes the MSE between the non-edited image regions of\nedited images and source images. Our model can achieve\nthe best performance in both masked and non-edited image\nregions. It can be seen that our model can achieve the best\nInput\nBaseline \nNa\u00efve Blend\nOurs\nFigure 6: The harmonization of our asymmetric VQGAN in\npaint-by-example [40] task. The inputs are masked source\nimages and reference images. \u2018Naive Blend\u201d denotes that\nadding the non-edited image regions of source images and\nthe edited regions of results. Paint-by-example [40] is our\nbaseline. All results are reported on COCOEE dataset.\n\u201cA red and white bus \ntraveling down a street.\u201d\n\u201cA man standing on a \ntennis court holding a \nracquet.\u201d\n\u201cA little locomotive in a \nmuseum all shined up.\u201d\nInput\nStable Diffusion\nOurs\nFigure 7: The comparison between StableDiffusion and\nours in text-to-image task. Without the help of our con-\nditional branch, we will get comparable results.\nperformance in both masked and non-edited image regions.\nEffectiveness of our modules. The purpose of this abla-\ntion study is to demonstrate the effectiveness of our condi-\ntional branch and larger decoder. The results are presented\nin Table 5. In contrast to the inpainting task, the paint-by-\nexample task involves combining two different images, re-\nsulting in a more severe loss of details in non-edited im-\nage regions. The non-edited regions of an image can be in-\nfluenced by the other image, further complicating the task.\nOur base model reduces the preserving error from 588.86\nto 1.37, while our larger model further reduces it to 0.76.\nSurprisingly, we find that the conditional branch not only\nimproves the preservation of non-edited image regions but\nalso enhances the generation quality of edited regions, as\nmeasured by the \u201dCLIP score\u201d\nVisual results.\nWe have provided a significant number\nof visualization results in Figures 5 and 6 to demonstrate\nthe preserving ability and harmonization of our asymmet-\nric VQGAN. These visualization results illustrate that our\nmodel can be effortlessly applied to various tasks on differ-\nent datasets while consistently enhancing performance. We\nfirmly believe that our model has immense potential for a\nwide range of applications.\n4.3. Text-to-Image\nThese experiments aim to demonstrate that our asym-\nmetric VQGAN can handle tasks without masks or task-\nspecific priors, in addition to tasks with mask (task-specific\npriors). The results are presented in Table 6. We can ob-\nserve that when our base model does not use the condi-\ntional branch, its performance is comparable to the base-\nline, which suggests that the training strategy of replacing\nsome masks with a full mask has been successful. More-\nover, when our large model does not use the conditional\nbranch, its performance is comparable to the baseline, indi-\ncating that the larger decoder can restore more details even\nwithout the help of the conditional branch.\nVisual results. To support our claim that our asymmet-\nric VQGAN can perform well even without the conditional\nbranch, we present visualization results in Figure 7. In the\nfirst row, it is evident that our model can work effectively\nwithout masks and does not produce inferior results. In the\nsecond and third rows, we observe that our larger decoder\ncan recover more intricate details to some extent.\nEffectiveness of our modules. The purpose of this abla-\ntion study is to demonstrate the effectiveness of our condi-\ntional branch and larger decoder. The results are presented\nin Table 5. In contrast to the inpainting task, the paint-by-\nexample task involves combining two different images, re-\nsulting in a more severe loss of details in non-edited im-\nage regions. The non-edited regions of an image can be in-\nfluenced by the other image, further complicating the task.\nOur base model reduces the preserving error from 588.86\nto 1.37, while our larger model further reduces it to 0.76.\nSurprisingly, we find that the conditional branch not only\nimproves the preservation of non-edited image regions but\nalso enhances the generation quality of edited regions, as\nmeasured by the \u201dCLIP score\u201d.\nVisual results.\nWe have provided a significant number\nof visualization results in Figures 5 and 6 to demonstrate\nthe preserving ability and harmonization of our asymmet-\nric VQGAN. These visualization results illustrate that our\nmodel can be effortlessly applied to various tasks on differ-\nent datasets while consistently enhancing performance. We\nfirmly believe that our model has immense potential for a\nwide range of applications.\n4.4. Text-to-Image\nThese experiments aim to demonstrate that our asym-\nmetric VQGAN can handle tasks without masks or task-\nspecific priors, in addition to tasks with mask (task-specific\npriors). The results are presented in Table 6. We can ob-\nserve that when our base model does not use the condi-\ntional branch, its performance is comparable to the base-\nline, which suggests that the training strategy of replacing\nsome masks with a full mask has been successful. More-\nover, when our large model does not use the conditional\nbranch, its performance is comparable to the baseline, indi-\ncating that the larger decoder can restore more details even\nwithout the help of the conditional branch.\nVisual results. To support our claim that our asymmet-\nric VQGAN can perform well even without the conditional\nbranch, we present visualization results in Figure 7. In the\nfirst row, it is evident that our model can work effectively\nwithout masks and does not produce inferior results. In the\nsecond and third rows, we observe that our larger decoder\ncan recover more intricate details to some extent.\n5. Conclusion\nIn this paper, we present a novel asymmetric VQGAN\nfor StableDiffusion with two new design features. Firstly,\nour decoder incorporates an additional conditional branch,\nallowing it to accept both the output of the VQGAN encoder\nand task-specific priors as input. Secondly, our decoder is\ndesigned to be more complex (e.g. deeper and wider) than\nthe encoder, enabling it to better preserve local details of\nnon-edited regions and recover details from the quantized\noutput of the encoder. Our asymmetric VQGAN architec-\nture is highly efficient for both training and inference, and\ncan be used for local editing tasks and pure text-to-image\ngeneration tasks.\nThrough extensive experimentation on\ntwo representative tasks, we demonstrate the effectiveness\nof our asymmetric VQGAN design. Moving forward, we\nplan to explore whether scaling up the decoder could fur-\nther improve the quality of our results.\nReferences\n[1] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\npages 18208\u201318218, 2022.\n[2] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat,\nJiaming Song, Karsten Kreis, Miika Aittala, Timo Aila,\nSamuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image\ndiffusion models with an ensemble of expert denoisers. arXiv\npreprint arXiv:2211.01324, 2022.\n[3] Shane Barratt and Rishi Sharma. A note on the inception\nscore. arXiv preprint arXiv:1801.01973, 2018.\n[4] Georgios Batzolis, Jan Stanczuk, Carola-Bibiane Sch\u00a8onlieb,\nand\nChristian\nEtmann.\nConditional\nimage\ngenera-\ntion with score-based diffusion models.\narXiv preprint\narXiv:2111.13606, 2021.\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\narXiv preprint arXiv:2211.09800, 2022.\n[6] Jooyoung Choi, Sungwon Kim, Yonghyun Jeong, Youngjune\nGwon, and Sungroh Yoon. Ilvr: Conditioning method for\ndenoising diffusion probabilistic models.\narXiv preprint\narXiv:2108.02938, 2021.\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, pages 248\u2013255. Ieee, 2009.\n[8] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in Neural Informa-\ntion Processing Systems, 34:8780\u20138794, 2021.\n[9] Patrick Esser, Robin Rombach, Andreas Blattmann, and\nBjorn Ommer. Imagebart: Bidirectional context with multi-\nnomial diffusion for autoregressive image synthesis.\nAd-\nvances in Neural Information Processing Systems, 34:3518\u2013\n3532, 2021.\n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis. In CVPR,\npages 12873\u201312883, 2021.\n[11] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, pages 10696\u201310706, 2022.\n[12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017.\n[13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020.\n[14] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. J. Mach. Learn.\nRes., 23(47):1\u201333, 2022.\n[15] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022.\n[16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A\nEfros. Image-to-image translation with conditional adver-\nsarial networks. In CVPR, 2017.\n[17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic align-\nments for generating image descriptions. IEEE Transactions\non Pattern Analysis and Machine Intelligence, 39:664\u2013676,\n2017.\n[18] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro. Diffwave: A versatile diffusion model for\naudio synthesis. arXiv preprint arXiv:2009.09761, 2020.\n[19] Xiang Lisa Li, John Thickstun, Ishaan Gulrajani, Percy\nLiang, and Tatsunori B Hashimoto.\nDiffusion-lm im-\nproves controllable text generation.\narXiv preprint\narXiv:2205.14217, 2022.\n[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, pages 740\u2013755, 2014.\n[21] Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang,\nAndrew Tao, and Bryan Catanzaro. Image inpainting for ir-\nregular holes using partial convolutions.\nIn ECCV, pages\n85\u2013100, 2018.\n[22] Qiankun Liu, Zhentao Tan, Dongdong Chen, Qi Chu, Xiyang\nDai, Yinpeng Chen, Mengchen Liu, Lu Yuan, and Nenghai\nYu.\nReduce information loss in transformers for pluralis-\ntic image inpainting. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11347\u201311357, 2022.\n[23] Yuqing Ma, Xianglong Liu, Shihao Bai, Lei Wang, Aishan\nLiu, Dacheng Tao, and Edwin R Hancock. Regionwise gen-\nerative adversarial image inpainting for large missing areas.\nIEEE Transactions on Cybernetics, 2022.\n[24] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch,\nand Daniel Cohen-Or. Null-text inversion for editing real\nimages using guided diffusion models.\narXiv preprint\narXiv:2211.09794, 2022.\n[25] Kamyar Nazeri, Eric Ng, Tony Joseph, Faisal Z Qureshi,\nand Mehran Ebrahimi.\nEdgeconnect: Generative image\ninpainting with adversarial edge learning.\narXiv preprint\narXiv:1901.00212, 2019.\n[26] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021.\n[27] Jialun Peng, Dong Liu, Songcen Xu, and Houqiang Li. Gen-\nerating diverse structure for image inpainting with hierarchi-\ncal vq-vae. In CVPR, pages 10775\u201310784, 2021.\n[28] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, pages 8748\u20138763, 2021.\n[29] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[30] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32, 2019.\n[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022.\n[32] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022.\n[33] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[34] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256\u20132265. PMLR, 2015.\n[35] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020.\n[36] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin,\nAnastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov,\nNaejin Kong, Harshith Goka, Kiwoong Park, and Victor\nLempitsky.\nResolution-robust large mask inpainting with\nfourier convolutions. In WACV, pages 2149\u20132159, 2022.\n[37] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong\nChen, Dong Chen, Lu Yuan, and Houqiang Li.\nSeman-\ntic image synthesis via diffusion models.\narXiv preprint\narXiv:2207.00050, 2022.\n[38] Weilun Wang, Jianmin Bao, Wengang Zhou, Dongdong\nChen, Dong Chen, Lu Yuan, and Houqiang Li.\nSindiffu-\nsion: Learning a diffusion model from a single natural im-\nage. arXiv preprint arXiv:2211.12445, 2022.\n[39] Ben Xue, Shenghui Ran, Quan Chen, Rongfei Jia, Binqiang\nZhao, and Xing Tang.\nDccf: Deep comprehensible color\nfilter learning framework for high-resolution image harmo-\nnization. In ECCV, pages 300\u2013316, 2022.\n[40] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin\nChen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by\nexample: Exemplar-based image editing with diffusion mod-\nels. arXiv preprint arXiv:2211.13227, 2022.\n[41] Jiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and\nThomas S Huang. Free-form image inpainting with gated\nconvolution. In ICCV, pages 4471\u20134480, 2019.\n[42] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, pages 586\u2013595,\n2018.\n[43] Shengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao\nLiang, Eric I Chang, and Yan Xu. Large scale image comple-\ntion via co-modulated generative adversarial networks. arXiv\npreprint arXiv:2103.10428, 2021.\n[44] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva,\nand Antonio Torralba. Places: A 10 million image database\nfor scene recognition. IEEE transactions on pattern analysis\nand machine intelligence, 40(6):1452\u20131464, 2017.\nAppendices\nIn the following section, we first introduce the loss ob-\njectives for Asymmetric VQGAN, then we present the other\ntechnique for compressing image space to latent space KL-\nreg, which shares a similar spirit to VQGAN. Finally, we\nintroduce the architecture of our models.\nA. Training Objectives\nFor the training of Asymmetric VQGAN, we fix the\nweights of the encoder and the codebook and employ a re-\nconstruction and adversarial loss to train the decoder. The\nreconstruction loss is the sum of pixel-loss and percep-\ntual loss. Pixel-level loss is the MAE loss used between\neach pixel of the output image from the quantized vector\nz and input image x, which can be denoted by Lpixel =\n1\n3HW |x \u2212 \u02c6x|, where \u02c6x = Dec(z) denotes the output image,\nH and w denote the high and width of the image, respec-\ntively. Moreover, the perceptual loss Lpercep is calculated\nwith VGG16 [33] network, and it can be formulated as:\nLk\npercep(x, \u02c6x) =\n1\nHkW k fk(\u2225\u03d5k(x) \u2212 \u03d5k(\u02c6x)\u22252),\nLpercep =\n4\nX\nk=0\nLk\npercep(x, \u02c6x)\n(5)\nwhere k = {0, 1, 2, 3, 4}, Hk and W k denote the high and\nwidth of the image feature in k-th layer, respectively. fk\nmeans a convolution operation with 1 \u00d7 1 kernel to reduce\nthe channel to 1. And \u03d5k(\u00b7) is the k-th layer of pretrained\nVGG16 network.\nTo sum up, the reconstruction loss can be generalized as\nfollows:\nLREC(Dec) = Lpixel + Lpercep.\n(6)\nAnother important loss to improve the quality of the gen-\nerated results is the GAN loss, and the equation is defined\nby:\nLGAN(Dec, D) = min\nDec max\nD Ex[log(D(x))]\n+ E\u02c6x[log(1 \u2212 D(\u02c6x)],\n(7)\nwhere D denotes a patch-based discriminator [16], Dec de-\nnotes the generator which is our asymmetric decoder.\nTherefore the overall objective for training the decoder\nmodel L then reads\nL = LREC(Dec) + \u03bbLGAN(Dec, D),\n(8)\nwhere we compute the adaptive weight \u03bb according to\n\u03bb =\n\u2207GL [Lpixel]\n\u2207GL [LGAN] + \u03b4 .\n(9)\nThe \u2207GL[\u00b7] denotes the gradient of its input w.r.t. the last\nlayer L of our decoder and \u03b4 = 10\u22124 is used for numerical\nstability.\nB. KL-reg for Training VAEGAN\nBesides VQGAN, in some versions of StableDiffusion,\nthe compressing from pixel space to latent space is trained\nby a KL-reg, which can be regarded as VAEGAN. The\nVAEGAN shares a similar purpose to VQGAN since they\nall try to avoid arbitrarily high-variance latent spaces.\nFor VAEGAN, the encoder network outputs the mean\nand covariance of the latent vector, i.e., \u00b5 and \u03f5. Lkl is\nused to reduce the gap between the prior and the proposal\ndistributions.\nLkl = 1\n2\n\u0000\u00b5T \u00b5 + sum(exp(\u03f5) \u2212 \u03f5 \u2212 1)\n\u0001\n.\n(10)\nand the VAEGAN loss can be formulated as:\nLVAEGAN({Enc, Dec}) = Lpixel + Lpercep + Lkl.\n(11)\nWe also include a GAN loss(Eqn. 9) to improve the quality.\nFor the training of the asymmetric VAEGAN, the encoder\npart is also fixed, and we only apply the Lpixel + Lpercep +\nLGAN(Dec, D) loss functions to update the decoder. The\nLkl is omitted during the training process.\nC. Architecture of Our Models\nWe further present the details of the architecture of our\nbase model in Table 7, and the architecture of our large\nmodel is shown in Table 8.\nD. Larger Decoder\nIn contrast to the conventional balanced size between en-\ncoder and decoder, we propose to design the decoder to be\nheavier than the encoder (in-balanced design). This is based\non the important observation that the main computation bot-\ntleneck of StableDiffusion [31] lies in the diffusion process\nbut not VQGAN. This design can not only improve the qual-\nity for both masked and unmasked regions for local editing,\nbut benefit the pure text-to-image generation performance.\nAs shown in Table 9 and Table 10, our method not only\nbenefits the masked region generation quality in local edit-\ning (FID and LPIPS improvement for the whole edited im-\nage) but also improves the original text-to-image generation\ntask quality upon StableDiffusion, whereas all the blending\nmethods cannot achieve this goal.\nBranch\nl-th\nLayer / kernel\nOutput Size\nCondition\n0\nPConv2d / (3, 3)\n128 \u00d7 512 \u00d7 512\n1\nPConv2d / (3, 3)\n256 \u00d7 512 \u00d7 512\n2\nPConv2d / (4, 4)\n512 \u00d7 256 \u00d7 256\n3\nPConv2d / (4, 4)\n512 \u00d7 128 \u00d7 128\n4\nPConv2d / (4, 4)\n512 \u00d7 64 \u00d7 64\nMain\nOut\nConcat\n257 \u00d7 512 \u00d7 512\nConv2d / (1, 1)\n128 \u00d7 512 \u00d7 512\nGroupNorm / (1, 1)\n128 \u00d7 512 \u00d7 512\nConv2d / (3, 3)\n3 \u00d7 512 \u00d7 512\n0\nConcat\n513 \u00d7 512 \u00d7 512\nConv2d / (1, 1)\n256 \u00d7 512 \u00d7 512\nResBlock \u00d73\n128 \u00d7 512 \u00d7 512\n1\nConcat\n1025 \u00d7 256 \u00d7 256\nConv2d / (1, 1)\n512 \u00d7 256 \u00d7 256\nResBlock \u00d73\n256 \u00d7 256 \u00d7 256\nUpsample\n256 \u00d7 512 \u00d7 512\n2\nConcat\n1025 \u00d7 128 \u00d7 128\nConv2d / (1, 1)\n512 \u00d7 128 \u00d7 128\nResBlock \u00d73\n512 \u00d7 128 \u00d7 128\nUpsample\n512 \u00d7 256 \u00d7 256\n3\nConcat\n1025 \u00d7 64 \u00d7 64\nConv2d / (1, 1)\n512 \u00d7 64 \u00d7 64\nResBlock \u00d73\n512 \u00d7 64 \u00d7 64\nUpsample\n512 \u00d7 128 \u00d7 128\n4\nConv2d / (3, 3)\n512 \u00d7 64 \u00d7 64\nResBlock\nAttnBlock\nResBlock\nTable 7: Architecture of our base model. The way of mask\nguided blend is concatenation.\n\u201cPConv\u201d denotes Partial\nConvolutional Layer [21]. The gray font denotes the vanilla\ndecoder.\nBranch\nl-th\nLayer / kernel\nOutput Size\nCondition\n0\nPConv2d / (3, 3)\n192 \u00d7 512 \u00d7 512\n1\nPConv2d / (3, 3)\n384 \u00d7 512 \u00d7 512\n2\nPConv2d / (4, 4)\n768 \u00d7 256 \u00d7 256\n3\nPConv2d / (4, 4)\n768 \u00d7 128 \u00d7 128\n4\nPConv2d / (4, 4)\n768 \u00d7 64 \u00d7 64\nMain\nOut\nConcat\n385 \u00d7 512 \u00d7 512\nConv2d / (1, 1)\n192 \u00d7 512 \u00d7 512\nGroupNorm / (1, 1)\n192 \u00d7 512 \u00d7 512\nConv2d / (3, 3)\n3 \u00d7 512 \u00d7 512\n0\nConcat\n769 \u00d7 512 \u00d7 512\nConv2d / (1, 1)\n384 \u00d7 512 \u00d7 512\nResBlock \u00d74\n192 \u00d7 512 \u00d7 512\n1\nConcat\n1537 \u00d7 256 \u00d7 256\nConv2d / (1, 1)\n768 \u00d7 256 \u00d7 256\nResBlock \u00d74\n384 \u00d7 256 \u00d7 256\nUpsample\n384 \u00d7 512 \u00d7 512\n2\nConcat\n1537 \u00d7 128 \u00d7 128\nConv2d / (1, 1)\n768 \u00d7 128 \u00d7 128\nResBlock \u00d74\n768 \u00d7 128 \u00d7 128\nUpsample\n768 \u00d7 256 \u00d7 256\n3\nConcat\n1537 \u00d7 64 \u00d7 64\nConv2d / (1, 1)\n768 \u00d7 64 \u00d7 64\nResBlock \u00d74\n768 \u00d7 64 \u00d7 64\nUpsample\n768 \u00d7 128 \u00d7 128\n4\nConv2d / (3, 3)\n768 \u00d7 64 \u00d7 64\nResBlock\nAttnBlock\nResBlock\nTable 8: Architecture of our large model. The way of mask\nguided blend is concatenation.\n\u201cPConv\u201d denotes Partial\nConvolutional Layer [21].\nMethod\nFID\u2193\nLPIPS\u2193\nPre error\u2193\nBase decoder\n7.60\n0.137\n5.7e\u22125\nLarge1 decoder\n7.55\n0.136\n2.6e\u22125\nLarge\u00d72 decoder\n7.49\n0.134\n2.1e\u22125\nTable 9: Effectiveness of our larger decoder in inpainting\ntask. \u201cLarge decoder\u201d denotes we increase the width and\ndepth of the decoder by 1.5 times. \u201cLarge\u00d72 decoder\u201d de-\nnotes we increase the width of the decoder by 2 times and\nthe depth by 2.5 times.\nMethod\nFID\u2193\nIS\u2191\nBase decoder w/o mask\n19.92\n37.52\nLarge decoder w/o mask\n19.75\n37.64\nLarge\u00d72 decoder w/o mask\n19.68\n37.74\nTable 10: Effectiveness of our larger decoder on text-to-\nimage task.\n"
  },
  {
    "title": "Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks",
    "link": "https://arxiv.org/pdf/2306.04362.pdf",
    "upvote": "2",
    "text": "Youku-mPLUG: A 10 Million Large-scale Chinese\nVideo-Language Pre-training Dataset and Benchmarks\nHaiyang Xu\u2217, Qinghao Ye*, Xuan Wu*, Ming Yan\u2020, Yuan Miao, Jiabo Ye\nGuohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian\nMaofei Que, Ji Zhang, Xiao Zeng, Fei Huang\nDAMO Academy, Alibaba Group\n{shuofeng.xhy, yeqinghao.yqh, wx193834, ym119608}@alibaba-inc.com\nAbstract\nTo promote the development of Vision-Language Pre-training (VLP) and multi-\nmodal Large Language Model (LLM) in the Chinese community, we firstly release\nthe largest public Chinese high-quality video-language dataset named Youku-\nmPLUG, which is collected from Youku3, a well-known Chinese video-sharing\nwebsite, with strict criteria of safety, diversity, and quality. Youku-mPLUG contains\n10 million Chinese video-text pairs filtered from 400 million raw videos across\na wide range of 45 diverse categories for large-scale pre-training. In addition,\nto facilitate a comprehensive evaluation of video-language models, we carefully\nbuild the largest human-annotated Chinese benchmarks covering three popular\nvideo-language tasks of cross-modal retrieval, video captioning, and video category\nclassification. Youku-mPLUG can enable researchers to conduct more in-depth\nmultimodal research and develop better applications in the future. Furthermore,\nwe release popular video-language pre-training models, ALPRO and mPLUG-2,\nand our proposed modularized decoder-only model mPLUG-video pre-trained on\nYouku-mPLUG. Experiments show that models pre-trained on Youku-mPLUG\ngain up to 23.1% improvement in video category classification. Besides, mPLUG-\nvideo achieves a new state-of-the-art result on these benchmarks with 80.5% top-1\naccuracy in video category classification and 68.9 CIDEr score in video captioning,\nrespectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with\nonly 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate\nimpressive instruction and video understanding ability. The zero-shot instruction\nunderstanding experiment indicates that pretraining with Youku-mPLUG can en-\nhance the ability to comprehend overall and detailed visual semantics, recognize\nscene text, and leverage open-domain knowledge. Our dataset, code, model, and\nevaluation set are available at https://github.com/X-PLUG/Youku-mPLUG.\n1\nIntroduction\nDue to the release of large-scale English video-language datasets (e.g., Howto100M[Miech et al.,\n2019] and WebVid-2.5M[Bain et al., 2021]), video-language pre-training (VLP) has achieved superior\nperformance in various downstream tasks, such as video-text retrieval, video question answering,\nand video captioning. Recently, the multimodal LLM in video (e.g., VideoChat[Li et al., 2023b],\nFlamingo[Alayrac et al., 2022]) has demonstrated strong zero-shot video understanding ability based\non these large-scale datasets. Compared with the English VLP community as Tab. 1, the lack of large-\nscale and high-quality public Chinese VLP datasets hinders the research of Chinese video-language\n\u2217Equal contribution\n\u2020Corresponding author\n3https://www.youku.com\nPreprint. Under review.\narXiv:2306.04362v1  [cs.CV]  7 Jun 2023\nTable 1: Statistics of Youku-mPLUG and its comparison with existing video-language pre-training\ndatasets.\nDataset Name\nLanguage\n# Videos\n# Text\nAvg. Len (secs)\nDuration (hrs)\nDomain\nAvailability\nHowTo100M [Miech et al., 2019]\nEnglish\n136M\n136M\n3.6\n135K\nInstruction\n!\nYT-Temporal-180M [Zellers et al., 2021]\nEnglish\n180M\n180M\n-\n-\nInstruction\n!\nHD-VILA-100M [Xue et al., 2022]\nEnglish\n103M\n103M\n13.4\n372K\nOpen\n!\nWebVid10M [Bain et al., 2021]\nEnglish\n10M\n10M\n18.0\n52K\nOpen\n!\nALIVOL-10M [Lei et al., 2021a]\nChinese\n103M\n110M\n34.6\n99K\nE-Commerce\n%\nKwai-SVC-11M [Nie et al., 2022b]\nChinese\n11M\n4M\n57.9\n177K\nOpen\n%\nCREATE-10M [Zhang et al., 2022]\nChinese\n10M\n10M\n29.8\n83K\nOpen\n%\nCNVid-3.5M [Gan et al., 2023]\nChinese\n3.5M\n3.5M\n36.2\n35K\nOpen\n%\nYouku-mPLUG\nChinese\n10M\n10M\n54.2\n150K\nOpen\n!\nTable 2: Statistics of Youku-mPLUG and its comparison with existing video-language downstream\ndatasets.\nDataset Name\nLanguage\n# Sample\nDomain\nRetrieval\nClassification\nCaption\nAvailability\nMSRVTT [Xu et al., 2016]\nEnglish\n10K\nOpen\n!\n!\n!\n!\nDiDeMo [Anne Hendricks et al., 2017]\nEnglish\n27K\nFlickr\n!\n%\n%\n%\nMSVD [Chen and Dolan, 2011]\nEnglish\n10K\nOpen\n!\n!\n!\n!\nLSMDC [Rohrbach et al., 2015]\nEnglish\n118K\nMovie\n!\n!\n%\n!\nActivityNet [Krishna et al., 2017]\nEnglish\n100K\nOpen\n!\n!\n%\n!\nVATEX [Wang et al., 2019]\nEnglish/Chinese\n41K\nKinetics-600\n!\n%\n!\n!\nBFVD [Zhang et al., 2020]\nChinese\n43K\nE-Commerce\n!\n%\n%\n%\nFFVD [Zhang et al., 2020]\nChinese\n32K\nE-Commerce\n!\n%\n%\n%\nCREATE-210K [Zhang et al., 2022]\nChinese\n216K\nOpen\n!\n%\n!\n%\nYouku-mPLUG\nChinese\n365K\nOpen\n!\n!\n!\n!\npretraining and multimodal LLM. In addition, the Chinese VLP community is also facing a lack\nof publicly available benchmarks as Tab. 2. This has resulted in two significant issues. Firstly, the\ndevelopment and application of the community are being lagged behind. Secondly, some works are\nable to achieve surprisingly good performance by using secret downstream benchmarks that other\nworks cannot fairly compare with, thus making it difficult to establish standards for performance\nevaluation. While some methods translate English text into Chinese [Madasu et al., 2022] or annotate\nthe dataset based on the English video [Wang et al., 2019], there remains an intrinsic linguistic and\ncultural gap between English and Chinese.\nTo facilitate the research and application of Chinese VLP, we release the first and largest public Chi-\nnese Video-language pretraining dataset and benchmarks named Youku-mPLUG, which is collected\nfrom Youku, a well-known Chinese video-sharing website. Youku-mPLUG contains 10 million video-\ntext pairs for pre-training and 0.3 million videos for downstream benchmarks. For the pre-training\ndataset, we filter the high-quality 10 million video-text pairs from 400 million raw videos with the\nstrict criteria of safety, diversity, and quality. Safety, the dataset is subject to heavy filtering and\nrestrictions through a multi-level risk detection system to prevent any content related to high risk;\nDiversity, the videos are carefully classified into 45 diverse categories covering various domains,\nwith a balanced distribution; Quality, we have conducted strict data cleaning at both the text and\nvideo levels, and using Chinese image-text pre-trained model to improve the data quality. Further-\nmore, We build the largest human-annotated Chinese benchmarks covering Cross-modal Retrieval,\nVideo Captioning and Video Category Classification for comprehensive evaluation of video-language\nmodels and downstream applications. For each downstream task, we hire well-educated people and\nadopt a two-step verification for ensuring the quality and diversity of the annotations, resulting in the\nlargest Chinese downstream benchmark for model evaluation.\nBesides, we release popular video-language models, ALPRO[Li et al., 2022b] and mPLUG-2[Xu\net al., 2023] pre-trained on Youku-mPLUG. Drawing inspiration from the idea of modularization[Li\net al., 2022a, Xu et al., 2023, Ye et al., 2023], we propose the modularized decoder-only model\nmPLUG-video with limited trainable parameters based on frozen pre-trained LLM, which consists\nof the trainable video encoder, visual abstractor module, and the frozen pre-trained LLM decoder.\nWe first obtain dense video representations from the video encoder. Then, we employ the visual\nabstractor module to summarize visual information within several learnable tokens. Finally, the\nvisual representations are combined with text queries and fed into the LLM decoder to generate\nthe response. Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1%\nimprovement in video category classification. Besides, mPLUG-video achieves 80.5% top-1 accuracy\n2\nin video category classification and 68.9 CIDEr score in video captioning, respectively. It becomes\nthe new state-of-the-art results on these benchmarks. Moreover, we scale up mPLUG-video based\non frozen Bloomz[Workshop et al., 2023] as Chinese multimodal LLM with only 1.7% trainable\nparameters, which demonstrates impressive instruction and video understanding ability. As an insight,\nour zero-short video instruction understanding test validates that Youku-mPLUG can strengthen\nthe scene text recognizing ability and incorporate open-domain knowledge for video understanding.\nQualitative results can be found in the Supplementary Material. These pre-trained models has also\nbeen released to facilitate the research and application of Chinese Video-language pre-training.\nIn summary, our main contributions are:\n\u2022 We release the first public largest Chinese Video-language pretraining dataset and bench-\nmarks named Youku-mPLUG.\n\u2022 We release popular video-language models (ALPRO and mPLUG-2) and our proposed\nmodularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.\n\u2022 We scale up and release mPLUG-video based on Bloomz as Chinese multimodal LLM with\nonly 1.7% trainable parameters, which demonstrates the impressive zero-shot instruction\nand video understanding ability.\n\u2022 Experiments show that models pre-trained on Youku-mPLUG gain significant improvement\nand mPLUG-video achieves the new state-of-the-art results on these benchmarks.\n2\nRelated Work\nVideo-Language Pre-training Datasets Large-scale datasets have proven effective for video-\nlanguage representation learning. Previously, most video-language models were trained on the\nHowTo100M dataset [Miech et al., 2019], which comprises 136 million video clips from 1.22 million\ninstructional YouTube videos. However, this dataset is limited to the instructional domain and is\nunsuitable for generalization. To overcome this constraint, Zeller et al. [Zellers et al., 2021] and Xue\net al. [Xue et al., 2022] propose the YT-Temporal-180M and HD-VILA-100M corpus, respectively.\nMeanwhile, to reduce the noise in subtitles, Bain et al. [Bain et al., 2021] introduce the Webvid10M\ndataset which is inspired by the collection schemes of Conceptual Caption datasets [Sharma et al.,\n2018]. However, these datasets are limited to English language corpus and cannot be directly applied\nto the Chinese domain. Although there exist some large-scale Chinese video-language datasets such\nas ALIVOL [Lei et al., 2021a], Kwai-SVC [Nie et al., 2022a], CREATE-10M [Zhang et al., 2022],\nand CNVid-3.5M [Gan et al., 2023], none of them have been publicly released to date, which hinders\nthe progress of research in the Chinese video-language learning field. To address this gap, we present\nYouku-mPLUG, the largest Chinese high-quality video-language dataset, to facilitate future research\non large-scale video-language learning in the Chinese language.\nVideo-Language Downstream Benchmarks For evaluating video-language pre-training models,\nresearchers have proposed several downstream tasks such as video-text retrieval, video question\nanswering, and video captioning for performance evaluation. For instance, MSRVTT [Xu et al., 2016],\nDiDeMo [Anne Hendricks et al., 2017], and LSMDC [Rohrbach et al., 2015] are commonly adopted\nfor text-video retrieval evaluation. Similarly, MSRVTT-QA [Xu et al., 2017], MSVD-QA [Xu et al.,\n2017], and T-GIF [Jang et al., 2017] are widely used for video question evaluation. Meanwhile,\nMSRVTT-Caption [Xu et al., 2016] and MSVD-Caption [Chen and Dolan, 2011] are commonly used\nfor video caption evaluation. However, these datasets are primarily collected from YouTube, which is\nnot entirely suitable for the Chinese domain. Furthermore, while there are some Chinese benchmark\ndatasets such as CREATE [Zhang et al., 2022] and VATEX [Wang et al., 2019], they are not fully\nreleased and only evaluate one aspect of the model\u2019s performance. Additionally, there is a lack of\nsystematic video language downstream benchmarks or leaderboards for Chinese video-language\npre-training evaluation. Consequently, we propose three downstream benchmarks, including video\ncategory classification, video-text retrieval, and video captioning, for evaluating models\u2019 performance\non Youku-mPLUG. These benchmarks are specifically designed for the Chinese domain and are\nintended to fill the gap in existing English benchmarks, which may not be entirely suitable for Chinese\nvideo-language pre-training evaluation.\nVideo-Language Pre-training Models In recent years, there has been a growing interest in video-\nlanguage pre-training, and various methods have been proposed to explore this area. Traditional\napproaches [Luo et al., 2020, Li et al., 2020] rely on pre-extracted, dense video frame or clip features\nfor video-language representation. In contrast, ClipBERT [Lei et al., 2021b] introduces a sparse\n3\nsampling strategy that facilitates end-to-end learning while simultaneously improving performance.\nBuilding upon this strategy, many approaches [Bain et al., 2021, Ge et al., 2022] have been developed,\nwhich incorporate novel architectures and pre-training tasks for video-language learning. For example,\nFrozen [Bain et al., 2021] and BridgeFormer [Ge et al., 2022] employ contrastive learning to align\nthe semantics of paired video and text in the same embedding space. Additionally, ALPRO [Li\net al., 2022b], TW-BERT [Yang et al., 2023], mPLUG-2 [Xu et al., 2023], and HiTeA [Ye et al.,\n2022] fuse video and language features to generate video-language representations for understanding\nand generation. Recently, large language models such as GPT-3 [Brown et al., 2020], Bloom\n[Workshop et al., 2023], and LLaMA [Touvron et al., 2023] have demonstrated significant zero-\nshot generalization abilities, which are advantageous for the vision-language field. For instance,\nBLIP-2 [Li et al., 2023a], miniGPT-4 [Zhu et al., 2023], and mPLUG-Owl [Ye et al., 2023] exhibit\nrobust zero-shot generalization and conversation capabilities by aligning vision and language models.\nIn this work, we provide a decoder-only video-language model mPLUG-video pre-trained on our\nYouku-mPLUG dataset with a strong generalization performance in terms of both video-language\nunderstanding and generation.\n3\nYouku-mPLUG Dataset Creation\nTo fill in the blank of the public Chinese video-text pre-training dataset and benchmarks, We release\nthe largest public Chinese Video-language dataset named Youku-mPLUG collected with the strict\ncriteria of safety, diversity, and quality from Youku, a Chinese video-sharing website. Youku-mPLUG\ncontains 10 million video-text pairs for pre-training and 0.3 millon videos for downstream benchmarks\ncovering Video-Text Retrieval, Video Captioning and Video Category Classification. Randomly\nsampled examples are shown in Figure 1.\n\u516c\u51431684\u5e74\u53f0\u6e7e\u6536\u590d, \u5eb7\u7199\u9ad8\u5174\u574f\u4e86,  \u51b3\u5b9a\u4e3e\u884c\u76db\u5927\u7684\u5e86\u795d\u4eea\u5f0f\u3002\nIn 1684, when Taiwan was recovered, Kangxi was overjoyed, deciding to hold a grand celebration ceremony.\n\u5305\u8d1d\u5c14\u66dd\u65993\u5c81\u5973\u513f\u997a\u5b50\u4e5f\u8ffd\u661f: \u770b\u5230\u9773\u4e1c\u5c31\u766b\u72c2\nBao Beier revealed that his 3-year-old daughter \"Dumpling\" is also a fan of stars: she goes crazy when she sees Jin Dong.\n\u300a\u738b\u8005\u8363\u8000\u300b\u76fe\u5c71\u7684\u76fe\u724c\u80fd\u62b5\u6321\u4e1c\u7687\u592a\u4e00\u7684\u5927\u62db\u5417\u7ed3\u5c40\u610f\u60f3\u4e0d\u5230\nCan the shield of the hero \"Shield Warrior\" in \"King of Glory\" resist the ultimate move of the hero \"Eastern Emperor Tai Yi\"?\nThe ending is unexpected.\nFigure 1: Random sampled examples in Youku-mPLUG.\n3.1\nPre-training Dataset Construction\nFor pre-training dataset, we filter the high-quality 10 million video-text pairs from 400 million raw\nvideos with the strict criteria of safety, diversity, and quality. In terms of safety, the dataset is heavily\nfiltered and restricted by an internal multi-level risk detection system with both multimodal model\ndetection and manual review processes to prevent any content related to pornography, violence, ter-\nrorism, discrimination, abuse, or high risk. Regarding diversity, we have applied video fingerprinting\ntechnology to eliminate videos that are completely identical. With the ability of hierarchical multi-\nlabel classification model [Giunchiglia and Lukasiewicz, 2020], the videos are carefully classified\ninto 20 super categories and 45 common categories as Fig. 2, covering various domains, with a\nbalanced distribution. To ensure high quality, we have conducted strict data cleaning at both the text\nand video levels. For text, we have imposed language restrictions on video titles, requiring the length\n4\nDaily Life\nTV Drama Editing\nPopular Science\nTV Show\nTechnology\nWorkplace\nCar\nMovie Editing\nLife Encyclopedia\nGame\nComedy\nFood Making\nMedical and Health\nChildren's Talent\nChildren's Animation\nPet\nCute Children Daily Life\nHealth Preservation\nAnimation\nCover Singing\nChildren's Toys\nFootball\nSports\nMusical Instruments\nParenting\nOutfit\nHair and Makeup\nTraditional Culture\nMusic Video\nFinance and Economics\nAgriculture, \nRural Areas, and Farmers\nEducation\nHome Decoration\nFood Review\nBasketball\nTV Drama \n(Trailer/Discussion)\nFitness\nDocumentary\nReal Estate and Renovation\nSquare Dance\nMovie \n(Trailer/Discussion)\nHip Hop Dance\nTravel\nSkincare and Beauty\nXiangsheng and Sketch\nCategory\n0\n100000\n200000\n300000\n400000\n500000\nNumber of Videos\nFigure 2: The distribution of the number of videos in each common category.\n20\n40\n60\n80\n100\n120\nVideo Duration (Sec)\n0\n100000\n200000\n300000\n400000\n500000\n600000\n700000\n800000\nFrequency\n10\n20\n30\n40\n50\n60\n70\n80\nThe Length of Words\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nFrequency\n1e6\n19.3%\n14.0%\n6.9%\n6.8% 5.5%5.4%\n4.8%\n4.7%\n4.4%\n4.1%\n4.0%\n3.7%\n3.5%\n3.2%\n3.1%\n1.8%\n1.4%\n1.3%\n1.3%\n0.9%\nSuper Category\nLife\nKnowledge/Culture\nChildren\nTV Drama\nSports\nTV Show\nMusic\nTechnology\nFood\nCar\nMovie\nParenting\nFashion\nGame\nComedy\nAnimation\nDance\nFinance and Economics\nAgriculture, \nRural Areas, and Farmers\nDocumentary\nFigure 3: Youku-mPLUG dataset statistics: we report the histogram of video duration in seconds (left),\nthe histogram of title length in words (middle), and the ratios of the categories in each super-category\n(right).\nto be between 5 and 30 words and include at least 5 Chinese characters while filtering out those\nwith obvious advertising or meaningless content. In terms of video quality and integrity, we have\nspecifically chosen recently uploaded videos with durations ranging from 10 to 120 seconds to ensure\nclear and complete content. Further, we also employ the Chinese image-text pre-trained model CLIP\n[Yang et al., 2022] to improve the data quality by deprecating those with low similarities between the\nframe mean features and text features. Fig. 3 shows the statistics of video duration and word length.\n3.2\nDownstream Benchmark Construction\nFor the downstream benchmark, we design three types of tasks including video-text retrieval, video\ncategory classification, and video captioning to evaluate the models\u2019 performance in terms of under-\nstanding and generation. The statistics of these three different datasets are summarized in Tab. 3.\nTable 3: Statistics of Youku-mPLUG benchmark datasets. # pairs indicates the number of video-text\npairs.\nTask\nTrain (# Pairs)\nVal (# Pairs)\nTest (# Pairs)\nVideo Category Classification\n100,023\n14,678\n20,026\nVideo-Text Retrieval\n37,595\n1,795\n7,414\nVideo Captioning\n170,866\n7,510\n7,705\nVideo Category Classification Our initial step involves randomly selecting a substantial number\nof videos based on category frequency. Next, we collect the video categories from the Youku\ndatabase, which are auto-generated by an online model. It is important to note that this model\u2019s\naccuracy is approximately 94% when considering historical prediction data, thus not entirely reliable.\nConsequently, we put forth additional efforts to ensure the quality and accuracy of our datasets\nby manually verifying each video and its corresponding title in the benchmark datasets. Prior to\nannotation, we supply a smaller dataset containing 100 videos, along with their metadata, including\ntitles and categories generated by the online prediction model. Annotators are then tasked with\nconfirming the assigned categories in relation to the videos and their titles. They must also assign\na relevance score, which ranges from 1 to 5. A higher score suggests a greater likelihood of the\nvideo belonging to the given category, and those with scores above 3 are retained. Annotators with\n5\nerror rates exceeding 2.5% are disqualified. After eliminating unsuitable annotators, we proceed with\nannotating the video category classification dataset. To ensure the utmost accuracy, particularly for\nthe validation and testing sets, we engage three annotators to verify each video.\nVideo Captioning The video captioning task requires the model to generate a concise sentence\ndescribing a video clip\u2019s content and title. To create the dataset, we randomly sample around 80,000\nvideos based on category frequency distribution and employ a color histogram-based approach for\nsegmenting each video into shots [Mei et al., 2014]. To ensure an accurate understanding of the\nvideo content and produce precise descriptions, we engage several annotators who are native Chinese\nspeakers with strong educational backgrounds. As part of the pre-annotation process during the\nvideo category classification task, we assign 25 random videos to each annotator, requesting them\nto create captions that include the subject and object in the video, as well as relevant descriptions\nof actions and background. The captions must consist of at least 15 Chinese characters. Following\nthe pre-annotation stage, annotators proceed with annotating the datasets and split them into the\ntraining, validation, and testing sets. Especially, to prevent data leakage, clips from the same video or\nsharing the same title are exclusively assigned to either the training or testing sets. Moreover, for\nthe validation and testing datasets, we enlist more than three individuals to annotate the video clips,\npromoting diversity and quality.\nVideo-Text Retrieval Given that we have already annotated video captions during the creation of the\nvideo captioning dataset, we select a subset of these annotated captions as text queries for video-text\nretrieval. Additionally, video titles can also be incorporated into the text queries, enhancing diversity\nwithin the text. In a similar vein, we ensure that clips from the same video or those with identical text\ntitles are not exclusively included in the training or test set to prevent potential data leakage.\n4\nMethodology\nSince the pre-trained large language model shows incredible zero-shot and generalization abilities\non various tasks, we use the off-the-shelf Chinese large language model (e.g. GPT-3 [Brown et al.,\n2020]) for efficient modularized training. To this end, we propose mPLUG-video, a decoder-only\nbased video-language model that leverages the frozen large language model. Especially, our model\nconsists of a video encoder, a visual abstractor module, and a language decoder, as illustrated in\nFigure 4. Besides, we leave the video encoder and visual abstractor trainable resulting in limited\ntrainable parameters while largely reducing the computation burden.\n4.1\nArchitecture\nThe Video Encoder We leverage a 12-layer TimeSformer [Bertasius et al., 2021] to extract the video\nfeatures, with 224 height and width of each input frame. We sparsely sample T frames from each\nvideo V, the TimeSformer first divides the video frames into N non-overlapping patches and flattens\nthem into a sequence of T \u00d7N patches. Then these patches are fed into the patch projection layers for\npatch representation. To encode the position of each patch, we add learnable embeddings to encode\neach patch\u2019s spatial and temporal position. Then the TimeSformer applies divided spatiotemporal\nattention to yield video representation V \u2208 R(T \u00d7N)\u00d7D, where D is the hidden dimension of the\nvideo representation.\nVisual Abstractor Module To mitigate the computation burden with the lengthy video sequences,\nwe introduce visual abstractor module which utilizes learnable queries Q \u2208 RM\u00d7D for reducing the\nlength of video sequence as follows:\n\u02dcQ = CrossAttention(Q, V, V ),\n(1)\n\u02dcQ = FFN( \u02dcQ) + \u02dcQ,\n(2)\nwhere CrossAttention(x, y, z) is the cross-attention layer with Query x, Key y, and Value z. The\nFFN(\u00b7) is the feed-forward layer [Vaswani et al., 2017]. Finally, we obtain the reduced video\nsequence \u02dcQ \u2208 RM\u00d7D.\nThe Language Decoder Since pre-trained large language models demonstrate strong zero-shot\ncapabilities in text generation, we utilize them as the general text decoder for multi-modal inputs\nwhile keeping it frozen. In specific, we treat the video as a foreign language and concatenate the\nreduced video sequence with the text token features obtained from the text embedding layer. Then,\nthe video and text token features are jointly fed into the large language model which is frozen\nfor obtaining the video-guided language features. Finally, the video-guided language features are\npredicted for text tokens.\n6\nLarge Language Model (e.g. GPT, Bloom)\nVideo Encoder\nVisual Abstractor\nLearnable Token\n\"\u5524\u8d77\u6211\u4eec...\"\n\"evokes our...\"\n\"\u8c22\u6625\u82b1\u300a\u540c\u684c\u7684\u4f60\u300b\"\n\"Xie Chunhua's song \"My\nOld Classmate\"\"\nCategory\n\u7efc\u827a; TV Shows\n\u8c22\u6625\u82b1\u300a\u540c\u684c\u7684\u4f60\u300b\u5524\u8d77\u6211\u4eec\n\u90a3\u6bb5\u5145\u6ee1\u6d3b\u2f12\u548c\u6fc0\u60c5\u7684\u2ed8\u6625\u65f6\n\u5149\uff0c\u8ba9\u6211\u4eec\u91cd\u6e29\u4e86\u5b66\u2f63\u65f6\u4ee3\u7684\n\u70b9\u70b9\u6ef4\u6ef4\uff0c\u90a3\u4e9b\u4e3a\u68a6\u60f3\u2f7d\u62fc\u640f\n\u7684\u2f47\u2f26\u3002\nCaption\nXie Chunhua's song \"My Old\nClassmate\" evokes our youthful\nand energetic memories, allowing\nus to relive the little moments of\nour student days and the days we\nfought for our dreams.\nFigure 4: The overview of mPLUG-video.\nTraining Objective We train mPLUG-video within an auto-regressive manner and adopt the next\nprediction task for training. In detail, the model needs to complete the texts based on the given video,\nand the language modeling loss is calculated as:\nL = \u2212E(W,V)\n\" L\nX\nl=1\nlog p(wl|W[0,l), V)\n#\n,\n(3)\nwhere L denotes the total number of words in the text, and W denotes the word tokens.\n4.2\nApplying to Downstream Tasks\nVideo Captioning Video captioning is considered an auto-regressive task. During the process\nof fine-tuning a video captioning dataset, the training objective remains the same as that during\npre-training.\nVideo Category Classification We treat video category classification as a video caption task.\nAnnotated category names of videos are regarded as ground-truth captions. We evaluate the accuracy\nof predictions based on whether the predicted category name exactly matches the ground-truth.\nVideo-Text Retrieval In contrast to mPLUG-2, which includes a contrastive head and a matching\nhead for the retrieval task, our mPLUG-video cannot be directly used for retrieval tasks. Therefore,\nwe input video-text pairs into the model and extract the feature of the last token. We obtain the\nmatching score by applying an extra linear layer to the feature of the last token.\n5\nExperiments\n5.1\nImplementation Details\nmPLUG-video leverage the pre-trained popular Chinese GPT-3 4 5 as the language decoder, and the\nvideo encoder is pre-trained on ImageNet [Ridnik et al., 2021]. During pre-training, we sparsely\nsample 8 frames from each video preserving their order in-between, and resize them to 224 \u00d7 224.\nWe use a batch size of 512 and train mPLUG-video for 10 epochs. We adopt the AdamW optimizer\nwith \u03b2 = (0.9, 0.98), and set the learning rate and weight decay to 1e-4 and 1e-3 respectively. We\nwarm up the training with 2000 warm-up steps then decay the learning rate with the cosine schedule.\nFor downstream tasks, we use a batch size of 128 and train mPLUG-video for 10 epochs with a\nlearning rate 2e-5.\n5.2\nEvaluation Results on Downstream tasks\nIn this study, we evaluate the performance of ALPRO, mPLUG-2, and mPLUG-video on video\ncategory classification, video captioning, and video-text retrieval.\nEvaluation on Video Category Classification We assess the performance of ALPRO, mPLUG-2,\nand mPLUG-Video on video category classification tasks. We measure the top-1 and top-5 accuracy\nof each model. For the generation models, a generated category name that is exactly the same as\nground truth can be regarded as a correct prediction. The comparison results are shown in Table 4.\nOur results reveal that mPLUG-Video achieves the highest accuracy, with a top-1 accuracy of 80.57%\n4https://modelscope.cn/models/damo/nlp_gpt3_text-generation_1.3B/summary\n5https://modelscope.cn/models/damo/nlp_gpt3_text-generation_2.7B/summary\n7\nand a top-5 accuracy of 98.15%. Interestingly, mPLUG-video (2.7B) outperforms mPLUG-video\n(1.3B), highlighting the importance of natural language understanding with a larger LLM decoder.\nEvaluation on Video Caption We present in Table 4 the performance of models on Video Caption.\nALPRO does not have a decoder module. Therefore, its performance was not reported. The\nperformance of mPLUG-Video and mPLUG-2 are compared based on various metrics, including\nMETEOR, ROUGE, CIDEr, and BLEU-4. It is found that mPLUG-video (2.7B) achieves higher\nscores than mPLUG-Video (1.3B) across all four metrics. Additionally, mPLUG-video obtains higher\nscores than mPLUG-2 on BLEU-4. These results suggest that pre-trained language models are\nessential and video captioning tasks based on our dataset are still challenging for existing methods.\nEvaluation on Video-Text Retrieval Table 5 presents the performance comparison between models\non video retrieval task. We observe that mPLUG-2 outperforms ALPRO, possibly due to the\nincorporation of universal layers that remove modality differences and generate superior uni-modal\nrepresentations. We also notice that mPLUG-video perform poorly on video retrieval task. Freezing\nthe language model can hinder mPLUG-video to extract cross-modal features. These findings imply\nthat our dataset accurately measures the video-language modeling capability.\nTable 4: Comparison results on Youku-mPLUG. Video category prediction and video captioning,\nrespectively. For video category prediction, top-1 and top-5 accuracy are reported. For video\ncaptioning, we report BELU-4, METEOR, ROUGE and CIDEr. * denotes the language model is\nfrozen.\nVideo Category Prediction\nVideo Captioning\nModel\nTop-1 Acc.(%)\nTop-5 Acc.(%)\nBLEU-4\nMETEOR\nROUGE\nCIDEr\nALPRO\n78.15\n95.15\n-\n-\n-\n-\nmPLUG-2\n77.79\n92.44\n43.7\n27.6\n52.9\n67.7\nmPLUG-Video (1.3B)*\n80.04\n98.06\n46.4\n26.5\n52.9\n67.7\nmPLUG-Video (2.7B)*\n80.57\n98.15\n47.1\n26.7\n53.3\n68.9\nTable 5: Comparison results on Youku-mPLUG. Video retrieval. We evaluate models on video\nretrieval (V2T) and text retrieval (T2V). we report the average of R@1, R@5 and R@10. * denotes\nthe language model is frozen.\nVideo Retrieval\nModel\nV2T\nT2V\nR@1\nR@2\nR@10\nR@1\nR@5\nR@10\nALPRO\n27.00\n53.33\n64.09\n26.63\n53.20\n64.43\nmPLUG-2\n38.45\n65.48\n75.18\n38.45\n65.48\n75.18\nmPLUG-Video (1.3B)*\n7.01\n20.33\n29.67\n7.01\n20.33\n29.67\nmPLUG-Video (2.7B)*\n7.62\n21.24\n31.39\n7.62\n21.24\n31.39\n5.3\nAblation Study on Modalities\nTable 6: Comparison of different modalities and Youku-mPLUG on category classification task.\nVision Modality\nLanguage Modality\nYouku-mPLUG Pre-Trained\nTop-1 Acc.(%)\nTop-5 Acc.(%)\n!\n%\n%\n63.51\n89.89\n%\n!\n%\n59.31\n86.31\n!\n!\n%\n69.40\n90.07\n!\n!\n!\n78.15\n95.15\nIn this section, we investigate the contributions of different modalities to video-language modeling by\nleveraging the category classification task on our Youku-mPLUG. Table 6 presents the performance\nof the baseline model (ALPRO) trained with data of different modalities. Vision Modality and\nLanguage Modality denote the model is trained with and utilizes this modality of data (video frames\nor video captions). And Youku-mPLUG Pre-Trained refers to the model that pre-trained on Youku-\nmPLUG before fine-tuning. The results show that the performance of the model trained with the\n8\nvisual modality is higher than that of the language modality. This suggests that high-level language\nmodalities may lose fine-grained visual clues, leading to failure in retrieval. Additionally, we observe\nthat the model trained with both vision and language modalities achieves higher performance than\nunimodal models. This observation demonstrates the importance of modality complementarity in\nvideo understanding. Pre-training the model with Youku-mPLUG leads to a significant improvement\nin model performance, emphasizing the importance of our Youku-mPLUG.\n5.4\nHuman Evaluation of Zero-shot Video Instruction Understanding\n21\n42\n55\n36\n1\n4\n15\n6\n14\n0\n10\n20\n30\n40\n50\n60\n70\nvideoLLaMA\nmPLUG-Video w/o pretrain\nmPLUG-Video\nA: correct and satisfying response\nB: acceptable response with minor imperfections\nC: responds to the instruction but has significant errors\nD:irrelevant or invalid response\nFigure 5: Human evaluation about zero-shot video\ninstruction understanding on 65 cases.\nTo test the video instruction understanding abil-\nity of different models, we manually set 65 in-\nstructions based on 50 randomly-sampled videos\n(45 from Youku-mPLUG, 5 from HD-VILA-\n100M [Xue et al., 2022]). We compare the in-\nstruction understanding performance of three\nmodels:\nVideoLLaMA[Zhang et al., 2023],\nmPLUG-Video w/o pretrain and mPLUG-Video.\nVideoLLaMA is trained with visual instruc-\ntion data from MiniGPT-4[Zhu et al., 2023],\nLLaVa [Liu et al., 2023] and Video-Chat [Li\net al., 2023b], while the latter two models only\nutilize visual training data from LLaVa [Liu\net al., 2023].\nWe ask human annotators to\nscore the models\u2019 responses. Following Self-\nInstruct[Wang et al., 2022], human annotators\nare asked to rate the response into four levels,\nwhere A means \u2018correct and satisfying response\u2019,\nB means \u2018acceptable response with minor imperfections\u2019, C means \u2018response to the instruction but\nhas significant errors\u2019 and D means \u2018irrelevant or invalid response\u2019. As shown in Fig. 5, with the\npertaining in Youku-mPLUG, mPLUG-video achieves much better video instruction understanding\nand responding ability, demonstrating the effectiveness of our proposed pretraining data. Qualitative\nresults can be found in the supplementary material.\n6\nConclusion\nIn this paper, we introduce the largest high-quality video-language dataset in Chinese, called Youku-\nmPLUG. Additionally, we present a human-annotated benchmark that comprises three downstream\ntasks, i.e., Video-Text Retrieval, Video Captioning, and Video Category Classification. We propose a\ndecoder-only model, mPLUG-video, that is modularized and pre-trained on Youku-mPLUG. Results\nfrom our experiments indicate that our evaluation set can effectively evaluate the video language\ncomprehension and modeling abilities of models. Furthermore, pre-training on mPLUG-video leads\nto significant improvements, and our mPLUG-video achieves a new state-of-the-art performance.\nA\nQualitative Results of Zero-shot Video Instruction Understanding\nIn this section, we show some qualitative results of zero-shot video instruction understanding to\nvalidate the effectiveness of pretraining with Youku-mPLUG. Firstly, as shown in Fig. 6, with\npertaining, mPLUG-video can better understand both overall video semantic (\u2018dance video\u2019) and\ndetailed vision information, such as \u2018jumping\u2019 and \u2018twisting\u2019. Besides, Fig. 7 shows that mPLUG-\nvideo can better recognize scene texts in videos and give more accurate responses. Therefore,\npretraining with Youku-mPLUG is highly likely to enhance the models\u2019 ability to comprehend scene\ntext in videos. Furthermore, in Fig. 8, mPLUG-video could accurately point out the key character\nname \u2018Ultraman\u2019, while the other two models can\u2019t. This validates the contribution of pretraining for\nlearning open-domain knowledge.\nB\nLimitations and Societal Impacts\nThe Youku-mPLUG dataset predominantly contains concepts and language expressions that were\ncurrent at the time of collection. As language evolves alongside human activities, our dataset may not\n9\n00:00\n00:50\n00:30\n00:05\n\u6982\u62ec\u4e00\u4e0b\u89c6\u9891\u5185\u5bb9\u3002\n(Summarize the \ncontent of the video.)\n\u8be5\u89c6\u9891\u662f\u4e00\u4e2a\u513f\u7ae5\u97f3\u4e50\n\u821e\u8e48\u89c6\u9891\uff0c\u7531\u4e00\u4e2a\u7537\u5b69\n\u626e\u6f14\u5c0f\u5154\u5b50\u3002\u5c0f\u5154\u5b50\u7684\n\u821e\u8e48\u52a8\u4f5c\u5305\u62ec\u8df3\u8dc3\u3001\u626d\n\u52a8\u3001\u6446\u52a8\u7b49\uff0c\u4f34\u968f\u7740\u6b22\n\u5feb\u7684\u97f3\u4e50\uff0c\u5c55\u793a\u51fa\u5c0f\u7537\n\u5b69\u6d3b\u6cfc\u53ef\u7231\u7684\u5f62\u8c61\u3002\u89c6\n\u9891\u4e2d\u8fd8\u5305\u542b\u4e00\u4e9b\u80cc\u666f\u97f3\n\u4e50\uff0c\u4e3a\u89c6\u9891\u589e\u6dfb\u4e86\u66f4\u591a\n\u7684\u8da3\u5473\u6027\u548c\u6d3b\u529b\u3002(The \nvideo is a children's \nmusic and dance video \nwith a boy playing the \nrole of a bunny. The \ndance movements of \nthe little rabbit include \njumping, twisting and \nswinging, etc., \naccompanied by \ncheerful music, showing \nthe lively and lovely \nimage of the little boy. \nThe video also contains \nsome background music, \nwhich adds more fun \nand vitality to the video..)\nA\nmPLUG-Video\nmPLUG-Video \nw/o pretrain\nVideoLLaMA\n\u8be5\u89c6\u9891\u5c55\u793a\u4e86\u4e00\u4e2a\u7a7f\u7740\n\u767d\u8272\u5154\u5b50\u670d\u88c5\u7684\u7537\u5b50\uff0c\n\u6234\u7740\u9886\u5e26\uff0c\u624b\u91cc\u62ff\u7740\u4e00\n\u628a\u96e8\u4f1e\u3002\u4ed6\u4f3c\u4e4e\u6b63\u5728\u8d70\n\u8def\uff0c\u4f46\u4ed6\u7684\u59ff\u52bf\u4f3c\u4e4e\u6709\n\u4e9b\u5947\u602a\u3002\u89c6\u9891\u4e2d\u8fd8\u6709\u5176\n\u4ed6\u51e0\u4e2a\u4eba\uff0c\u4ed6\u4eec\u53ef\u80fd\u6b63\n\u5728\u89c2\u770b\u6216\u53c2\u4e0e\u8be5\u7537\u5b50\u5728\n\u96e8\u4e2d\u884c\u8d70\u7684\u573a\u666f\u3002(The \nvideo shows a man in a \nwhite rabbit costume, \nwearing a tie and \nholding an umbrella. He \nappears to be walking, \nbut his posture seems a \nbit odd. There are \nseveral other people in \nthe video who may be \nwatching or participating \nin the scene of the man \nwalking in the rain.)\n\u89c6\u9891\u4e00\u5f00\u59cb\uff0c\u4e00\u540d\u5e74\n\u8f7b\u5973\u5b50\u6234\u7740\u68d5\u8272\u5e3d\u5b50\uff0c\n\u624b\u91cc\u62ff\u7740\u4e00\u90e8\u767d\u8272\u624b\n\u673a\u3002\u7136\u540e\uff0c\u573a\u666f\u5207\u6362\n\u5230\u4e00\u4e2a\u7a7f\u7740\u84dd\u8272\u886c\u886b\n\u7684\u7537\u4eba\u548c\u4e00\u4e2a\u6234\u7740\u68d5\n\u8272\u5e3d\u5b50\u3001\u7a7f\u7740\u7eff\u8272\u5939\n\u514b\u7684\u5e74\u8f7b\u5973\u5b69\u7ad9\u5728\u4e00\n\u5757\u5199\u7740\u201c\u5728\u8349\u5730\u4e0a\u73a9\n\u800d\u201d\u7684\u724c\u5b50\n\u524d...(The video \nbegins with a young \nwoman wearing a \nbrown hat and \nholding a white cell \nphone. The scene \nthen switches to a \nman in a blue shirt \nand a young girl in a \nbrown hat and \ngreen jacket \nstanding in front of a \nsign that reads \"Play \nin the grass\"...)\nC\nC\n01:07\nFigure 6: Qualitative results about video captioning.\nencompass emerging concepts, words, and language expressions in the future. This limitation applies\nto image data as well, where new visual objects or designs might not be captured. Nevertheless, this\nissue can be addressed by fine-tuning pre-trained models on up-to-date data. Additionally, our dataset\nis constructed using corpora from the Chinese Internet, meaning the vocabulary and expressions\nmay largely align with Chinese culture. Furthermore, our dataset lacks very long texts and lengthy\nvideos, potentially limiting the ability of the pre-trained models to understand extensive content such\nas full-length movies.\n10\n00:01\n00:31\n00:27\n00:11\n\u4ece\u89c6\u9891\u672b\u5c3e\u53ef\u4ee5\u770b\u51fa\uff0c\n\u8be5\u89c6\u9891\u7684\u521b\u4f5c\u90e8\u95e8\u662f\u4ec0\n\u4e48\uff1f(As you can see \nfrom the end of the \nvideo, what is the \ncreative department of \nthe video?)\n\u6839\u636e\u89c6\u9891\u672b\u5c3e\u7684\u6587\u5b57\uff0c\n\u8be5\u89c6\u9891\u7684\u521b\u4f5c\u90e8\u95e8\u662f\u4e2d\n\u534e\u4eba\u6c11\u5171\u548c\u56fd\u75be\u75c5\u9884\u9632\n\u63a7\u5236\u4e2d\u5fc3\u3002(According to \nthe text at the end of the \nvideo, the department \nthat created the video is \nthe Chinese Center For \nDisease Control And \nPrevention.)\nA\nmPLUG-Video\nmPLUG-Video \nw/o pretrain\nVideoLLaMA\n\u6839\u636e\u89c6\u9891\u672b\u5c3e\u7684\u5b57\u5e55\uff0c\n\u8be5\u89c6\u9891\u7684\u521b\u4f5c\u90e8\u95e8\u662f\u534e\n\u7eb3\u5144\u5f1f\u5f71\u7247\u516c\u53f8\u3002\n(According to the \ncaption at the end of the \nvideo, the video was \ncreated by Warner Bros. \nPictures.)\n\u8fd9\u4e2a\u89c6\u9891\u7684\u521b\u4f5c\u90e8\u95e8\n\u662f\u8bb2\u4e2d\u6587\u7684\u8bba\u575b\u3002\n(The creation \ndepartment of this \nvideo is the Chinese \nspeaking forum.)\nC\nC\n(a)\n00:01\n00:23\n00:06\n00:03\n\u8fd9\u662f\u4e00\u4e2a\u4ec0\u4e48\u7c7b\u578b\u7684\u89c6\n\u9891\uff1f(What type of \nvideo is this?)\n\u8fd9\u662f\u4e00\u4e2a\u6c7d\u8f66\u5e7f\u544a\u89c6\u9891\uff0c\n\u5c55\u793a\u4e86Honda Insight\u7684\n\u591a\u4e2a\u4e0d\u540c\u7248\u672c\u3002(This is \na car commercial video \nthat shows several \ndifferent versions of the \nHonda Insight.)\nA\nmPLUG-Video\nmPLUG-Video \nw/o pretrain\nVideoLLaMA\n\u8fd9\u662f\u4e00\u4e2a\u6c7d\u8f66\u5e7f\u544a\u89c6\n\u9891\uff0c\u5b83\u5c55\u793a\u4e86\u4e00\u8f86\u65e7\n\u6b3e\u8f66\uff0c\u5e76\u4f7f\u7528\u52a8\u6001\u56fe\n\u50cf\u548c\u6587\u5b57\u6765\u5438\u5f15\u89c2\u4f17\n\u7684\u6ce8\u610f\u529b\u3002(This is a \ncar commercial video \nthat shows an older \nmodel car and uses \nmoving images and \ntext to grab the \nviewer's attention.)\n\u8fd9\u662f\u4e00\u4e2a4D\u89c6\u9891\uff0c\u5b83\u5305\n\u542b\u4e86\u6bcf\u4e2a\u5206\u949f\u7684\u65cb\u8f6c\u6444\n\u50cf\u673a\u955c\u50cf\u548c\u9ad8\u6e05\u7167\u7247\uff0c\n\u4ee5\u53ca\u6807\u9898\u3001\u65f6\u95f4\u6233\u548c\u63cf\n\u8ff0\u6807\u7b7e\u3002(This is a 4D \nvideo which contains \nrotating camera \nmirrors and HD photos \nof each minute, as well \nas titles, timestamps \nand description tags..)\nB\nD\n(b)\n00:30\nFigure 7: Qualitative results about video scene text understanding.\n11\n00:03\n00:29\n00:16\n\u8fd9\u4e2a\u89c6\u9891\u7247\u6bb5\u8bb2\u4e86\u4ec0\u4e48\uff1f\n(What is this video clip \nabout? )\n\u8fd9\u4e2a\u89c6\u9891\u7247\u6bb5\u662f\u4e00\u4e2a\u5ba3\n\u4f20\u89c6\u9891\uff0c\u5c55\u793a\u4e86\u4e00\u90e8\u7531\n\u5965\u7279\u66fc\u548c\u94f6\u6cb3\u7cfb\u6218\u58eb\u7ec4\n\u6210\u7684\u7535\u5f71\u3002\u89c6\u9891\u4e2d\uff0c\u94f6\n\u6cb3\u6218\u58eb\u548c\u5965\u7279\u6218\u58eb\u4eec\u4e00\n\u8d77\u51fa\u73b0\u5728\u5c4f\u5e55\u4e0a\u3002\u4ed6\u4eec\n\u5c55\u793a\u4e86\u5404\u79cd\u6218\u6597\u6280\u80fd\u548c\n\u52a8\u4f5c\uff0c\u5c55\u793a\u4e86\u5965\u7279\u548c\u94f6\n\u6cb3\u7cfb\u7684\u5f3a\u5927\u529b\u91cf\u548c\u6218\u6597\n\u80fd\u529b\u3002(This video clip is \na promotional video \nshowing a movie \nconsisting of Ultraman \nand Galactic Warriors. \nIn the video, the galactic \nwarriors and Ultras \nappear on the screen \ntogether. They show a \nvariety of combat skills \nand movements, \ndemonstrating the great \npower and fighting \nability of Ultras and \nGalactics.)\nB\nmPLUG-Video\nmPLUG-Video \nw/o pretrain\nVideoLLaMA\n\u8fd9\u4e2a\u89c6\u9891\u7247\u6bb5\u662f\u4e00\u4e2a\u91c7\n\u8bbf\uff0c\u8bb2\u8ff0\u4e86\u4e00\u4e2a\u65e5\u672c\u8001\n\u4eba\u5bf9\u65e5\u672c\u6587\u5316\u7684\u70ed\u7231\u548c\n\u5bf9\u65e5\u672c\u7684\u56de\u5fc6\u3002(This \nvideo clip is an interview \nabout an elderly \nJapanese man's love for \nJapanese culture and \nhis memories of Japan.)\n\u8fd9\u4e2a\u89c6\u9891\u7247\u6bb5\u662f\u4e00\u90e8\n\u4e2d\u65e5\u672c\u7535\u5f71\uff0c\u6d89\u53ca\u8bb8\n\u591a\u7279\u5f81\u548c\u60c5\u8282\u3002\u5b83\u5c55\n\u73b0\u4e86\u4e00\u4e2a\u5a3c\u59ae\u7684\u7537\u4eba\n\u7a7f\u7740\u4e00\u88ad\u9ed1\u8272\u5e26\u548c\u7d2b\n\u8272\u5916\u5957\u7a7f\u7740\u6234\u528d\u7684\u4eba\u3001\n\u4e00\u4e2a\u4eba\u7a7f\u7740\u88ad\u5200\u6234\u7740\n\u6234\u528d\u7684\u7537\u4eba\u3002(This \nvideo clip is a \nChinese Japanese \nfilm that involves \nmany features and \nplots. It shows a \nman wearing a \nblack band and \npurple jacket \nwearing a man \nwearing a sword, a \nman wearing an \nassault sword \nwearing a man \nwearing a sword..)\nC\nC\n00:51\n01:00\nFigure 8: Qualitative results about open-domain knowledge understanding.\n12\n\u963f\u7c73\u5c14\u6c57\u76f4\u64ad\u6253\u9ebb\u5c06 \u738b\u5b9d\u5f3a\u8c03\u4f83\u7c73\u53d4\u662f\u201c\u4f60\u8f93\u201d\u2014\u65e9\u73ed\u673a\nAmir Khan plays mahjong live and Wang Baoqiang jokes that \"you lose\" to Amir Khan - Morning Flight\n\u5218\u5b87\u5b81\u4f60\u628a\u624b\u6492\u5f00\u8ba9\u6211\u6765\nLiuyuning, let go of your hand and let me do it.\n\u3010\u6c5f\u6cb3\u6c34 \u7b2c22\u96c6\u3011\u79e6\u660a\u5371\u673a\u5173\u5934\u820d\u5df1\u4e3a\u4eba\u52c7\u6551\u961a\u6e05\u5b50\u5c3d\u663e\u771f\u60c5\uff01\nAt the crisis point in River of Rivers Episode 22, Qin Hao showed true affection by riski\nng his life to save Kan Qingzi!\n\u53c8\u5e05\u6c14\u53c8\u7b80\u5355\u7684\u6a61\u76ae\u7b4b\u9b54\u672f1 (\u6821\u56ed\u9b54\u672f)\nHandsome and Simple Rubber Band Magic 1 (Campus Magic)\n\u6cbb\u6108\u7cfb\u65cb\u5f8b\u6765\u88ad\uff0c\u5468\u534e\u5065\u7ecf\u5178\u518d\u73b0\u300a\u670b\u53cb\u300b\uff0c\u73cd\u60dc\u8eab\u8fb9\u7684\u4eba\u5427\uff01\nHealing melody strikes, Zhou Huajian classic reproduction of \"Friends\", cherish the people around you!\nFigure 9: Examples in Youku-mPLUG.\n13\nReferences\nJ.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,\nM. Reynolds, et al. Flamingo: a visual language model for few-shot learning. arXiv preprint\narXiv:2204.14198, 2022.\nL. Anne Hendricks, O. Wang, E. Shechtman, J. Sivic, T. Darrell, and B. Russell. Localizing moments\nin video with natural language. In Proceedings of the IEEE international conference on computer\nvision, pages 5803\u20135812, 2017.\nM. Bain, A. Nagrani, G. Varol, and A. Zisserman. Frozen in time: A joint video and image encoder\nfor end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 1728\u20131738, 2021.\nG. Bertasius, H. Wang, and L. Torresani. Is space-time attention all you need for video understanding?\nIn ICML, page 4, 2021.\nT. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Language models\nare few-shot learners, 2020.\nD. Chen and W. B. Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings\nof the 49th annual meeting of the association for computational linguistics: human language\ntechnologies, pages 190\u2013200, 2011.\nT. Gan, Q. Wang, X. Dong, X. Ren, L. Nie, and Q. Guo. Cnvid-3.5m: Build, filter, and pre-train\nthe large-scale public chinese video-text dataset. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pages 14815\u201314824, June 2023.\nY. Ge, Y. Ge, X. Liu, D. Li, Y. Shan, X. Qie, and P. Luo. Bridging video-text retrieval with multiple\nchoice questions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16167\u201316176, 2022.\nE. Giunchiglia and T. Lukasiewicz. Coherent hierarchical multi-label classification networks. In\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada,\nDecember 2020.\nY. Jang, Y. Song, Y. Yu, Y. Kim, and G. Kim. Tgif-qa: Toward spatio-temporal reasoning in visual\nquestion answering. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 2758\u20132766, 2017.\nR. Krishna, K. Hata, F. Ren, L. Fei-Fei, and J. C. Niebles. Dense-captioning events in videos. In\nInternational Conference on Computer Vision (ICCV), 2017.\nC. Lei, S. Luo, Y. Liu, W. He, J. Wang, G. Wang, H. Tang, C. Miao, and H. Li. Understanding\nchinese video and language via contrastive multimodal pre-training. In H. T. Shen, Y. Zhuang,\nJ. R. Smith, Y. Yang, P. C\u00e9sar, F. Metze, and B. Prabhakaran, editors, MM \u201921: ACM Multimedia\nConference, Virtual Event, China, October 20 - 24, 2021, pages 2567\u20132576. ACM, 2021a. doi:\n10.1145/3474085.3475431. URL https://doi.org/10.1145/3474085.3475431.\nJ. Lei, L. Li, L. Zhou, Z. Gan, T. L. Berg, M. Bansal, and J. Liu. Less is more: Clipbert for video-and-\nlanguage learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7331\u20137341, 2021b.\nC. Li, H. Xu, J. Tian, W. Wang, M. Yan, B. Bi, J. Ye, H. Chen, G. Xu, Z. Cao, et al. mplug:\nEffective and efficient vision-language learning by cross-modal skip-connections. arXiv preprint\narXiv:2205.12005, 2022a.\nD. Li, J. Li, H. Li, J. C. Niebles, and S. C. Hoi. Align and prompt: Video-and-language pre-training\nwith entity prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4953\u20134963, 2022b.\n14\nJ. Li, D. Li, S. Savarese, and S. Hoi. BLIP-2: bootstrapping language-image pre-training with frozen\nimage encoders and large language models. In ICML, 2023a.\nK. Li, Y. He, Y. Wang, Y. Li, W. Wang, P. Luo, Y. Wang, L. Wang, and Y. Qiao. Videochat:\nChat-centric video understanding, 2023b.\nL. Li, Y.-C. Chen, Y. Cheng, Z. Gan, L. Yu, and J. Liu. Hero: Hierarchical encoder for video+language\nomni-representation pre-training. In Conference on Empirical Methods in Natural Language\nProcessing, 2020.\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. CoRR, abs/2304.08485, 2023.\nH. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, X. Chen, and M. Zhou. Univilm: A unified video and\nlanguage pre-training model for multimodal understanding and generation. ArXiv, abs/2002.06353,\n2020.\nA. Madasu, E. Aflalo, G. B. Stan, S.-Y. Tseng, G. Bertasius, and V. Lal. Improving video retrieval\nusing multilingual knowledge transfer. In European Conference on Information Retrieval, 2022.\nT. Mei, Y. Rui, S. Li, and Q. Tian. Multimedia search reranking: A literature survey. ACM Comput.\nSurv., 46:38:1\u201338:38, 2014.\nA. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning a text-\nvideo embedding by watching hundred million narrated video clips. In ICCV, pages 2630\u20132640,\n2019.\nL. Nie, L. Qu, D. Meng, M. Zhang, Q. Tian, and A. Bimbo. Search-oriented micro-video captioning.\nProceedings of the 30th ACM International Conference on Multimedia, 2022a.\nL. Nie, L. Qu, D. Meng, M. Zhang, Q. Tian, and A. D. Bimbo. Search-oriented micro-video\ncaptioning. In Proceedings of the 30th ACM International Conference on Multimedia, MM \u201922,\npage 3234\u20133243, New York, NY, USA, 2022b. Association for Computing Machinery. ISBN\n9781450392037. doi: 10.1145/3503161.3548180. URL https://doi.org/10.1145/3503161.\n3548180.\nT. Ridnik, E. Ben-Baruch, A. Noy, and L. Zelnik-Manor. Imagenet-21k pretraining for the masses.\narXiv preprint arXiv:2104.10972, 2021.\nA. Rohrbach, M. Rohrbach, N. Tandon, and B. Schiele. A dataset for movie description. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 3202\u20133212,\n2015.\nP. Sharma, N. Ding, S. Goodman, and R. Soricut. Conceptual captions: A cleaned, hypernymed, image\nalt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 2556\u20132565, Melbourne,\nAustralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1238.\nURL https://aclanthology.org/P18-1238.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and efficient\nfoundation language models, 2023.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc., 2017.\nX. Wang, J. Wu, J. Chen, L. Li, Y. Wang, and W. Y. Wang. Vatex: A large-scale, high-quality\nmultilingual dataset for video-and-language research. In 2019 IEEE/CVF International Conference\non Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages\n4580\u20134590. IEEE, 2019. doi: 10.1109/ICCV.2019.00468. URL https://doi.org/10.1109/\nICCV.2019.00468.\n15\nY. Wang, Y. Kordi, S. Mishra, A. Liu, N. A. Smith, D. Khashabi, and H. Hajishirzi. Self-instruct:\nAligning language model with self generated instructions. arXiv preprint arXiv:2212.10560, 2022.\nB. Workshop, :, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c, D. Hesslow, R. Castagn\u00e9, A. S.\nLuccioni, F. Yvon, M. Gall\u00e9, J. Tow, A. M. Rush, S. Biderman, A. Webson, P. S. Ammanamanchi,\nT. Wang, B. Sagot, N. Muennighoff, A. V. del Moral, O. Ruwase, R. Bawden, S. Bekman,\nA. McMillan-Major, I. Beltagy, H. Nguyen, L. Saulnier, S. Tan, P. O. Suarez, V. Sanh, H. Lauren\u00e7on,\nY. Jernite, J. Launay, M. Mitchell, C. Raffel, A. Gokaslan, A. Simhi, A. Soroa, A. F. Aji, A. Alfassy,\nA. Rogers, A. K. Nitzav, C. Xu, C. Mou, C. Emezue, C. Klamm, C. Leong, D. van Strien, D. I.\nAdelani, D. Radev, E. G. Ponferrada, E. Levkovizh, E. Kim, E. B. Natan, F. D. Toni, G. Dupont,\nG. Kruszewski, G. Pistilli, H. Elsahar, H. Benyamina, H. Tran, I. Yu, I. Abdulmumin, I. Johnson,\nI. Gonzalez-Dios, J. de la Rosa, J. Chim, J. Dodge, J. Zhu, J. Chang, J. Frohberg, J. Tobing,\nJ. Bhattacharjee, K. Almubarak, K. Chen, K. Lo, L. V. Werra, L. Weber, L. Phan, L. B. allal,\nL. Tanguy, M. Dey, M. R. Mu\u00f1oz, M. Masoud, M. Grandury, M. \u0160a\u0161ko, M. Huang, M. Coavoux,\nM. Singh, M. T.-J. Jiang, M. C. Vu, M. A. Jauhar, M. Ghaleb, N. Subramani, N. Kassner, N. Khamis,\nO. Nguyen, O. Espejel, O. de Gibert, P. Villegas, P. Henderson, P. Colombo, P. Amuok, Q. Lhoest,\nR. Harliman, R. Bommasani, R. L. L\u00f3pez, R. Ribeiro, S. Osei, S. Pyysalo, S. Nagel, S. Bose,\nS. H. Muhammad, S. Sharma, S. Longpre, S. Nikpoor, S. Silberberg, S. Pai, S. Zink, T. T. Torrent,\nT. Schick, T. Thrush, V. Danchev, V. Nikoulina, V. Laippala, V. Lepercq, V. Prabhu, Z. Alyafeai,\nZ. Talat, A. Raja, B. Heinzerling, C. Si, D. E. Ta\u00b8sar, E. Salesky, S. J. Mielke, W. Y. Lee, A. Sharma,\nA. Santilli, A. Chaffin, A. Stiegler, D. Datta, E. Szczechla, G. Chhablani, H. Wang, H. Pandey,\nH. Strobelt, J. A. Fries, J. Rozen, L. Gao, L. Sutawika, M. S. Bari, M. S. Al-shaibani, M. Manica,\nN. Nayak, R. Teehan, S. Albanie, S. Shen, S. Ben-David, S. H. Bach, T. Kim, T. Bers, T. Fevry,\nT. Neeraj, U. Thakker, V. Raunak, X. Tang, Z.-X. Yong, Z. Sun, S. Brody, Y. Uri, H. Tojarieh,\nA. Roberts, H. W. Chung, J. Tae, J. Phang, O. Press, C. Li, D. Narayanan, H. Bourfoune, J. Casper,\nJ. Rasley, M. Ryabinin, M. Mishra, M. Zhang, M. Shoeybi, M. Peyrounette, N. Patry, N. Tazi,\nO. Sanseviero, P. von Platen, P. Cornette, P. F. Lavall\u00e9e, R. Lacroix, S. Rajbhandari, S. Gandhi,\nS. Smith, S. Requena, S. Patil, T. Dettmers, A. Baruwa, A. Singh, A. Cheveleva, A.-L. Ligozat,\nA. Subramonian, A. N\u00e9v\u00e9ol, C. Lovering, D. Garrette, D. Tunuguntla, E. Reiter, E. Taktasheva,\nE. Voloshina, E. Bogdanov, G. I. Winata, H. Schoelkopf, J.-C. Kalo, J. Novikova, J. Z. Forde,\nJ. Clive, J. Kasai, K. Kawamura, L. Hazan, M. Carpuat, M. Clinciu, N. Kim, N. Cheng, O. Serikov,\nO. Antverg, O. van der Wal, R. Zhang, R. Zhang, S. Gehrmann, S. Mirkin, S. Pais, T. Shavrina,\nT. Scialom, T. Yun, T. Limisiewicz, V. Rieser, V. Protasov, V. Mikhailov, Y. Pruksachatkun,\nY. Belinkov, Z. Bamberger, Z. Kasner, A. Rueda, A. Pestana, A. Feizpour, A. Khan, A. Faranak,\nA. Santos, A. Hevia, A. Unldreaj, A. Aghagol, A. Abdollahi, A. Tammour, A. HajiHosseini,\nB. Behroozi, B. Ajibade, B. Saxena, C. M. Ferrandis, D. Contractor, D. Lansky, D. David,\nD. Kiela, D. A. Nguyen, E. Tan, E. Baylor, E. Ozoani, F. Mirza, F. Ononiwu, H. Rezanejad,\nH. Jones, I. Bhattacharya, I. Solaiman, I. Sedenko, I. Nejadgholi, J. Passmore, J. Seltzer, J. B. Sanz,\nL. Dutra, M. Samagaio, M. Elbadri, M. Mieskes, M. Gerchick, M. Akinlolu, M. McKenna, M. Qiu,\nM. Ghauri, M. Burynok, N. Abrar, N. Rajani, N. Elkott, N. Fahmy, O. Samuel, R. An, R. Kromann,\nR. Hao, S. Alizadeh, S. Shubber, S. Wang, S. Roy, S. Viguier, T. Le, T. Oyebade, T. Le, Y. Yang,\nZ. Nguyen, A. R. Kashyap, A. Palasciano, A. Callahan, A. Shukla, A. Miranda-Escalada, A. Singh,\nB. Beilharz, B. Wang, C. Brito, C. Zhou, C. Jain, C. Xu, C. Fourrier, D. L. Peri\u00f1\u00e1n, D. Molano,\nD. Yu, E. Manjavacas, F. Barth, F. Fuhrimann, G. Altay, G. Bayrak, G. Burns, H. U. Vrabec,\nI. Bello, I. Dash, J. Kang, J. Giorgi, J. Golde, J. D. Posada, K. R. Sivaraman, L. Bulchandani,\nL. Liu, L. Shinzato, M. H. de Bykhovetz, M. Takeuchi, M. P\u00e0mies, M. A. Castillo, M. Nezhurina,\nM. S\u00e4nger, M. Samwald, M. Cullan, M. Weinberg, M. D. Wolf, M. Mihaljcic, M. Liu, M. Freidank,\nM. Kang, N. Seelam, N. Dahlberg, N. M. Broad, N. Muellner, P. Fung, P. Haller, R. Chandrasekhar,\nR. Eisenberg, R. Martin, R. Canalli, R. Su, R. Su, S. Cahyawijaya, S. Garda, S. S. Deshmukh,\nS. Mishra, S. Kiblawi, S. Ott, S. Sang-aroonsiri, S. Kumar, S. Schweter, S. Bharati, T. Laud,\nT. Gigant, T. Kainuma, W. Kusa, Y. Labrak, Y. S. Bajaj, Y. Venkatraman, Y. Xu, Y. Xu, Y. Xu,\nZ. Tan, Z. Xie, Z. Ye, M. Bras, Y. Belkada, and T. Wolf. Bloom: A 176b-parameter open-access\nmultilingual language model, 2023.\nD. Xu, Z. Zhao, J. Xiao, F. Wu, H. Zhang, X. He, and Y. Zhuang. Video question answering\nvia gradually refined attention over appearance and motion. In Proceedings of the 25th ACM\ninternational conference on Multimedia, pages 1645\u20131653, 2017.\nH. Xu, Q. Ye, M. Yan, Y. Shi, J. Ye, Y. Xu, C. Li, B. Bi, Q. Qian, W. Wang, G. Xu, J. Zhang, S. Huang,\nF. Huang, and J. Zhou. mplug-2: A modularized multi-modal foundation model across text, image\n16\nand video, 2023.\nJ. Xu, T. Mei, T. Yao, and Y. Rui. Msr-vtt: A large video description dataset for bridging video and\nlanguage. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 5288\u20135296, 2016.\nH. Xue, T. Hang, Y. Zeng, Y. Sun, B. Liu, H. Yang, J. Fu, and B. Guo. Advancing high-resolution\nvideo-language representation with large-scale video transcriptions. In International Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\nA. Yang, J. Pan, J. Lin, R. Men, Y. Zhang, J. Zhou, and C. Zhou. Chinese clip: Contrastive\nvision-language pretraining in chinese. ArXiv, abs/2211.01335, 2022.\nX. Yang, Z. Li, H. Xu, H. Zhang, Q. Ye, C. Li, M. Yan, Y. Zhang, F. Huang, and S. Huang. Learning\ntrajectory-word alignments for video-language tasks, 2023.\nQ. Ye, G. Xu, M. Yan, H. Xu, Q. Qian, J. Zhang, and F. Huang. Hitea: Hierarchical temporal-aware\nvideo-language pre-training, 2022.\nQ. Ye, H. Xu, G. Xu, J. Ye, M. Yan, Y. Zhou, J. Wang, A. Hu, P. Shi, Y. Shi, C. Li, Y. Xu, H. Chen,\nJ. Tian, Q. Qi, J. Zhang, and F. Huang. mplug-owl: Modularization empowers large language\nmodels with multimodality, 2023.\nR. Zellers, X. Lu, J. Hessel, Y. Yu, J. S. Park, J. Cao, A. Farhadi, and Y. Choi. Merlot: Multimodal\nneural script knowledge models. Advances in Neural Information Processing Systems, 34:23634\u2013\n23651, 2021.\nH. Zhang, X. Li, and L. Bing. Video-llama: An instruction-tuned audio-visual language model for\nvideo understanding, 2023.\nS. Zhang, Z. Tan, J. Yu, Z. Zhao, K. Kuang, J. Liu, J. Zhou, H. Yang, and F. Wu. Poet: Product-\noriented video captioner for e-commerce. In Proceedings of the 28th ACM International Conference\non Multimedia, MM \u201920, page 1292\u20131301, New York, NY, USA, 2020. Association for Computing\nMachinery. ISBN 9781450379885. doi: 10.1145/3394171.3413880. URL https://doi.org/\n10.1145/3394171.3413880.\nZ. Zhang, Y. Chen, Z. Ma, Z. Qi, C. Yuan, B. Li, Y. Shan, and W. Hu. Create: A benchmark for\nchinese short video retrieval and title generation, 2022.\nD. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny. Minigpt-4: Enhancing vision-language under-\nstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\n17\n"
  },
  {
    "title": "Increasing Diversity While Maintaining Accuracy: Text Data Generation with Large Language Models and Human Interventions",
    "link": "https://arxiv.org/pdf/2306.04140.pdf",
    "upvote": "1",
    "text": "Increasing Diversity While Maintaining Accuracy: Text Data Generation\nwith Large Language Models and Human Interventions\nJohn Joon Young Chung\nUniversity of Michigan\njjyc@umich.edu\nEce Kamar\nMicrosoft Research\neckamar@microsoft.com\nSaleema Amershi\nMicrosoft Research\nsamershi@microsoft.com\nAbstract\nLarge language models (LLMs) can be used\nto generate text data for training and evalu-\nating other models. However, creating high-\nquality datasets with LLMs can be challenging.\nIn this work, we explore human-AI partner-\nships to facilitate high diversity and accuracy\nin LLM-based text data generation. We first\nexamine two approaches to diversify text gen-\neration: 1) logit suppression, which minimizes\nthe generation of languages that have already\nbeen frequently generated, and 2) temperature\nsampling, which flattens the token sampling\nprobability. We found that diversification ap-\nproaches can increase data diversity but often\nat the cost of data accuracy (i.e., text and labels\nbeing appropriate for the target domain). To ad-\ndress this issue, we examined two human inter-\nventions, 1) label replacement (LR), correcting\nmisaligned labels, and 2) out-of-scope filtering\n(OOSF), removing instances that are out of the\nuser\u2019s domain of interest or to which no con-\nsidered label applies. With oracle studies, we\nfound that LR increases the absolute accuracy\nof models trained with diversified datasets by\n14.4%. Moreover, we found that some models\ntrained with data generated with LR interven-\ntions outperformed LLM-based few-shot classi-\nfication. In contrast, OOSF was not effective in\nincreasing model accuracy, implying the need\nfor future work in human-in-the-loop text data\ngeneration.\n1\nIntroduction\nTraining custom natural language classification\nmodels has become easier with many tools (e.g.,\nHuggingface1). However, data collection remains\na costly part of model building. For example, exist-\ning open-source datasets may not be usable if they\ndo not match the distribution of a model builder\u2019s\ntarget domain or do not contain desired labels. In\nsuch cases, the model builder may need to collect\nand label new data which could be costly (e.g., in\n1https://huggingface.co/\nterms of the time and resources to scrape data or\npay people to generate or annotate new data).\nAdvances in generative large language mod-\nels (LLMs), such as GPT-3 (Brown et al., 2020),\npresent a novel approach for creating training data\nfor classification models (Yoo et al., 2021; Sahu\net al., 2022; Kumar et al., 2020). Model builders\ncan prompt an LLM with the domain of texts and\nlabels of interest and the LLM can quickly gener-\nate text data for the model builder\u2019s needs. This\napproach allows model builders to acquire a large\namount of data even when they initially have no\nor few data instances. With the generated data, the\nmodel builder can train a separate affordable model\n(e.g., BERT (Devlin et al., 2019)) to perform the\nspecific task.\nWhile LLMs can directly support this classifica-\ntion task with few-shot learning, it might not be the\nbest option for every model builder\u2014some might\nnot have enough resources (e.g., GPUs) or budget\n(e.g., credit for GPT-3) to run expensive models.\nOthers might be concerned about privacy or secu-\nrity issues when they use LLMs from external APIs\n(e.g., OpenAI API). In such cases, generating data\nfrom LLMs and training custom models could be\na more viable approach. Moreover, if we share\ngenerated datasets within the community, we can\nalso benefit those who do not have access to LLMs.\nLastly, we can also use generated datasets to test\nmodels. With these benefits of generating new text\ndatasets with LLMs, the practical concern is how\nto generate high-quality datasets.\nIn this work, we investigate human-AI partner-\nships to efficiently create high-quality datasets with\nLLM-based text generation. High-quality datasets\nshould have high diversity and coverage, informing\nthe extent of data that the model may encounter. At\nthe same time, the generated text should have high\naccuracy, being relevant to the model\u2019s target task\nwhile having accurate accompanying labels. To\nthese ends, we first study two technical approaches\narXiv:2306.04140v1  [cs.CL]  7 Jun 2023\nto diversify text generation (Section 3): 1) logit sup-\npression, which diversifies the generated texts by\ndecreasing the probability of sampling tokens that\nhave already appeared frequently in the previous\ngeneration, and 2) temperature sampling, which\nflattens the probability distribution of sampled to-\nkens to pick less likely texts. From an experiment\non eight classification tasks with GPT-3 as a text\ngenerator (Section 4), we found that diversifica-\ntion approaches can have mixed results. While\nincreasing data diversity, these approaches can hurt\naccuracy in generation and similarity to the original\ndatasets for the task.\nWe demonstrate that human interventions (Sec-\ntion 5) are the key to resolving these issues in text\ngeneration diversification. We examine human in-\nterventions of replacing inaccurate labels with ac-\ncurate ones (label replacement) and filtering out-\nof-scope data (out-of-scope data filtering). With\noracle studies (Section 6), we found that replac-\ning all incorrect labels increased model accuracy\nby 14.4% when we used both logit suppression\nand high temperature. This performance increase\nbrings in practical benefits\u2014without label replace-\nment, the average accuracy of models trained with\nGPT-3-generated data was lower than that of GPT-3\nclassification with few-shot learning, but with 180\ninstances label-replaced, the models trained with\ngenerated data started to outperform GPT-3 few-\nshot classification. Out-of-scope data filtering had\nlimited utility in increasing model accuracy, possi-\nbly due to the negative impact of removing training\ninstances. We discuss how human interventions\ncan further facilitate the diversity and accuracy of\ntext data generation.\nOur contributions are:\n\u2022 A methodolgy that combines LLM generation\napproaches and human supervision for diver-\nsified and accurate data generation.\n\u2022 An experiment showing how text generation\ndiversification impacts the accuracy of trained\nmodels and other qualities of the data, such as\ndiversity and accuracy in the generation.\n\u2022 Oracle studies on how human effort to replace\nmisaligned labels and filter out-of-scope data\ninstances can impact the performance of mod-\nels trained on data generated with text diversi-\nfication.\n2\nRelated Work\n2.1\nText Data Generation for Model Training\nIn NLP, data augmentation, where data are multi-\nplied based on existing data, is one context where\ntext data are generated for model training. There\nwere many approaches, from replacing words with\nsynonyms (Wei and Zou, 2019; Zhang et al., 2015),\nto randomly editing texts (Wei and Zou, 2019), pre-\ndicting replaceable words (Ng et al., 2020), back-\ntranslating (Fadaee et al., 2017), generating label-\nflipped data (Zhou et al., 2022), or using reinforce-\nment learning to condition generation (Liu et al.,\n2020). Inspired by MixUp (Zhang et al., 2018),\nwhich mixes different examples in vision data, re-\nsearchers also blended texts to augment data (Guo\net al., 2020; Sun et al., 2020; Zhang et al., 2022).\nOther approaches generate texts by learning from\ndifferent datasets (Xia et al., 2020; Hou et al., 2018;\nChen et al., 2020; Yoo et al., 2019).\nRecently, with the generative capacity of LLMs,\nresearchers proposed generating datasets with zero\nor very few samples and training a separate model\nto serve the specific task (Kumar et al., 2020; Yoo\net al., 2021; Sahu et al., 2022; Yuan et al., 2021;\nHartvigsen et al., 2022). As this approach would\nextract information from large models, they would\nbe analogous to knowledge distillation (Phuong\nand Lampert, 2019; Hinton et al., 2015) or dataset\ndistillation (Wang et al., 2018; Cazenavette et al.,\n2022). LLM-generated data has also been used to\ntest other trained models (Ribeiro and Lundberg,\n2022; Perez et al., 2022). In this work, we extend\nthe previous work by investigating the generation\nof high-quality data with accurate diversification.\n2.2\nText Generation with LLMs\nAs the size of language models increases, re-\nsearchers found that LLMs can serve different\ngeneration tasks based on input prompts and ex-\namples (Brown et al., 2020). This approach can\nbe used to generate text data with instructional\nprompts and a few examples. However, for the\ngenerated data to be useful, diversity and cover-\nage should be ensured. Control of the sampling\ntemperature (Goodfellow et al., 2016) would be rel-\nevant, as it facilitates the unlikely generation, but\nit was not evaluated for the facilitation of diversity\nand coverage. Inspired by previous work on con-\ntrolling LLM generation, we examine human-AI\napproaches to steer data generation to have higher\ndiversity while securing accuracy in the alignment\nof specified labels.\n2.3\nHuman-In-The-Loop\nHuman interventions are imperative to train high-\nperformance machine learning models, as people\ncurate datasets, configure model architectures, and\ntest the trained models. Researchers investigated\napproaches to make human interventions more\ninteractive in model training pipelines, by clos-\ning gaps between model training and data cura-\ntion (Fogarty et al., 2008; Amershi et al., 2009,\n2012; Levonian et al., 2022), humans extracting\nfeatures (Branson et al., 2010; Cheng and Bern-\nstein, 2015), interactively changing the error pat-\nterns (Kapoor et al., 2010; Talbot et al., 2009), or\ninteractively testing models (Wu et al., 2019; Yuan\net al., 2022; Ribeiro et al., 2020; Cabrera et al.,\n2021; Suh et al., 2019). Generative models intro-\nduce novel approaches to interactively tune and\nevaluate models by leveraging generated results as\ndata instances for training and testing (Ribeiro and\nLundberg, 2022). In this work, we explored har-\nnessing diversified and accurate datasets by com-\nbining LLM-based text generation and human in-\nterventions.\n3\nDiversified Text Data Generation\nWe lay out the desired characteristics of the datasets\nfor model building. Then, we introduce approaches\nto generate diversified datasets with LLMs.\n3.1\nGoals\nIdeal classification datasets need to have the fol-\nlowing characteristics: 1) Scoped: fall in the model\nbuilder\u2019s domain of interest while classifiable with\nlabels of interest, 2) Label accurate: accompany\naccurate labels, and 3) Diverse: cover cases the\nmodel would encounter during test time. These\ngoals are difficult to achieve simultaneously but\nneed to be balanced. Only considering diversity,\nrandomly generating any text would be enough, but\nit would hurt scope and label accuracy. Likewise,\nonly considering the scope and label accuracy, gen-\nerating an accurate but limited variety of text would\nbe enough, but it would hurt the diversity.\n3.2\nDiversifying Approaches\nWe introduce the setting to use LLM-based data\ngeneration for model training. Then, we lay out\ntwo approaches to promote diversity in text data\ngeneration. We also note their potential risks of\nharming the scope and accuracy.\nFigure 1: Examples of Diversification Approaches.\n3.2.1\nSettings for Data Generation\nWhen prompting LLMs, we consider 1) a text type\nand 2) labels in the prompts. While there can be\nmany different prompts, in our paper, we used the\nfollowing prompt:\nWrite a movie review (text type) to cover all fol-\nlowing elements\nElements: positive sentiment (label)\nMovie review (text type): \"This is a great movie\"\n(A)\nModel builders can also prepend examples in the\nsame format. The generation process is iterative,\nand model builders can use intermediate data points\nas examples in later prompts. The model builders\ncan generate data until they reach the desired num-\nber of data points. With the generated data, the\nmodel builder would finetune a separate smaller\nmodel that serves the target task. With this ap-\nproach of finetuning a smaller model, there can be\na question of whether finetuning a separate model\nwould result in higher accuracy than using zero-\nshot or few-shot learning of the LLM. In the later\nstudy, we show the cases where finetuned smaller\nmodels perform better than the LLM.\n3.2.2\nLogit Suppression\nLogit suppression is a diversification approach that\nsuppresses tokens that have already been generated\nfrequently in the intermediate dataset (Figure 1a).\nWith this approach, the generation pipeline logs\nthe frequency of tokens that have been generated\nso far. Then, to diversify the selection of tokens,\nlogit suppression decreases the probability of high-\nfrequency tokens. However, with this approach,\nsome tokens that could contribute to accurate gen-\neration can be suppressed.\n3.2.3\nHigh Temperature\nThe temperature of sampling distribution (Good-\nfellow et al., 2016) controls how \u201cflat\u201d the token\nsampling probability is (the equation is explained\nin Appendix A). High temperature leads to \u201cflatter\u201d\ntoken sampling probabilities (Figure 1b), increas-\ning the probability of sampling \u201cless likely\u201d tokens\nand diversifying generation. Similar to logit sup-\npression, extremely high temperatures can result in\ntokens irrelevant to the prompt, hurting accuracy in\ngeneration results.\n4\nExperiment1: Diversified Text Data\nGeneration\nWe evaluated how diversification approaches im-\npact the diversity of the generated data and the\naccuracy of models trained with the dataset.\n4.1\nExperiment Settings\n4.1.1\nTasks\nWe used tasks from eight datasets. SST-2 (Socher\net al., 2013) is a binary sentiment classification\ndataset from Rotten Tomatoes movie reviews.\nClickbait classification dataset (CB) (Chakraborty\net al., 2016) is news headlines labeled either click-\nbait or non-clickbait. CARER (Saravia et al., 2018)\nis Twitter statements labeled with one of the six\nemotion categories. PubMed 200k RCT (Dernon-\ncourt and Lee, 2017) has five classes regarding the\nroles of sentences in medical papers. The subjec-\ntivity dataset (SUBJ) is movie review texts labeled\nsubjective or objective (Pang and Lee, 2004). For-\nmality classification dataset (FO) (Lahiri, 2015)\nhas labels on whether the text is formal or informal.\nHWU64 (Liu et al., 2021) is a dataset with hu-\nman utterances to chatbots, and we used 18 domain\nclasses for our experiments. Corpus of Linguistic\nAcceptability (COLA) (Warstadt et al., 2019) is\npublication texts with annotations on whether the\ntext is grammatically correct or not.\n4.1.2\nGeneration Method\nAs\na\ngenerative\nLLM,\nwe\nused\nthe\ntext-davinci-002 model of GPT-3 through\nOpenAI API Access with Prompt A. We list the\nspecific text types and labels used for each dataset\nin Appendix B.1.\nThe generation process was\niterative, with 20 data points generated with a\nsingle prompt for each API call.\nAs a single\nprompt can only generate data instances for a\nsingle label, the generation process cycled through\nall considered labels while balancing the number\nof instances for each class. As our tasks dealt with\nshort text data, we limited the generation length\nto 100 tokens. We set the frequency penalty and\ntop p to 0.02 and 1, respectively. Except for SST-2,\nwe generated 5600 instances for a single training\ndataset.\nFor SST-2, we generated 6922 data\npoints. We chose these numbers to ensure a low\ngeneration budget while having fair quality when\ntraining models. Specifically, with a maximum\nlength of 100 tokens for each generated instance,\nif the prompt includes examples for n classes, the\nnumber of required tokens for each instance would\nbe (100+30) \u00d7 (n+1) (where 30 come from the\ninstructional prompts). With the generation pricing\nof $0.02/1000 tokens for text-davinci-002\nmodel, 5600 and 6922 instances resulted in\nmaximum spending of $14.56 \u00d7 (n+1) and $17.80\n\u00d7 (n+1), respectively. In our pilot tests, model\naccuracy saturated after these numbers of instances.\nFor the oracle training dataset, with which we\ncompared the quality of the datasets, we sampled\ninstances from the original training dataset for\nthe task. The test dataset was sampled from the\noriginal test dataset. We provide details on how we\nsampled these instances in Appendix B.2.\nGeneration Conditions\nIn addition to logit sup-\npression and temperature sampling, we also con-\nsider example seeding, whether the generation\npipeline begins with an initial set of example in-\nstances. We can use multiple approaches simultane-\nously (e.g., using logit suppression and temperature\nsampling together), and how these approaches in-\nteract is also the scope of our questions. For a\nsingle combination of conditions, we generated\nthree datasets, as there could be some variance in\nthe results with the initial seeds and the examples\ngenerated initially.\nWe instantiated logit suppression with the logit\nbias function in OpenAI API Access2, which can\nincrease or decrease the probability of sampling to-\nkens. Every time we complete a single generation\niteration, we recorded the frequency of tokens gen-\nerated by GPT-3. As the OpenAI API only allows\n100 tokens for logit biasing, we suppressed only\nthe 100 most appeared tokens. Specifically, for the\nlogit bias weights, we multiplied the token appear-\nance ratio (in percentage) by -7.5 while capping the\nminimum weight at \u20137.5. For temperature sam-\npling, we used four temperature values, 0.3, 0.7,\n0.9, and 1.3. When seeding examples, we first ran-\ndomly sampled 18 examples from oracle training\ndata with a balanced number of labels. Only for\nPubMed, which has five classes, we used 15 seed\nexamples. We used sampled data points as an initial\nexample pool. With example seeding, from the first\n2https://beta.openai.com/docs/api-reference/\ncompletions/create#completions/create-logit_bias\n0.4\n0.6\n0.8\n1.0\nModel Accuracy\n0.4\n0.6\n0.8\n1.0\nLabel Accuracy\n0.00\n0.05\n0.10\n0.15\n0.20\nDiversity\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nSimilarity\nOracle\nGPT Zero\nGPT Few\nBase Similarity\nTemp=0.3, Logit Sup=X\nTemp=0.7, Logit Sup=X\nTemp=0.9, Logit Sup=X\nTemp=1.3, Logit Sup=X\nTemp=0.3, Logit Sup=O\nTemp=0.7, Logit Sup=O\nTemp=0.9, Logit Sup=O\nTemp=1.3, Logit Sup=O\nExample=X\nExample=O\nFigure 2: Impact of logit suppression and high temperatures on model accuracy, label accuracy, diversity, and\nsimilarity to the oracle dataset, averaged across eight tasks. Bars without hatches start generation without examples\nwhile those with hatches start with few-shot generation. Throughout this paper, error bars indicate 95% confidence\ninterval.\ngeneration iteration, examples were randomly cho-\nsen from the pool. Without the seeding examples,\nwe completed the first cycle of generations as a\nzero-shot generation. After the first cycle, since we\nwould have generated data instances for all labels,\nwe added examples to the prompt. When adding\nexamples, we randomly sampled the examples for\nall labels, one example for each label.\n4.1.3\nTraining Method\nWith the generated data, we finetuned base size\nBERT (Devlin et al., 2019) classifiers with 109M\nparameters using pretrained weights from the Hug-\ngingface Transformer library (Wolf et al., 2020)\nwith a randomly initialized fully connected clas-\nsifier layer. For each dataset, we trained the five\ndifferent models with the same dataset. With three\ndatasets for each combination of approaches, it\nresulted in 15 models for a condition. While train-\ning, Adam optimizer was used, with a learning rate\nof 3e-5 and a warm-up period of 3 epochs. We\nadopted the early stopping with the patience of five\ntraining epochs. We used PyTorch and RTX A6000\nGPUs for training.\n4.2\nMetrics\nWe compared the accuracies of models trained with\ngenerated data to 1) models trained with oracle\ndatasets (oracle model) and 2) GPT-3\u2019s few-/zero-\nshot classifications (text-davinci-002).\nFor\nGPT-3 few-shot learning, we used 18 examples\n(15 only for PubMed) with the same number of\nexamples for each label. We also measured the\ndiversity of the dataset using Remote-Clique met-\nric (Rhys Cox et al., 2021), which is the average\nmean pairwise distances. Specifically, we embed-\nded generated data with BERT (Devlin et al., 2019),\nthen calculated the distances. We also evaluated\nlabel accuracy, which is the accuracy of the align-\nment between the generated texts and the specified\nlabels. For this metric, except for SST-2, we used\nthe oracle model as the evaluator. For SST-2, we\nused GPT-3 few-shot classification as the evalua-\ntor, as it has higher accuracy than the oracle model.\nWe also measured the similarity of the generated\ndataset to the oracle dataset with the average mean\npairwise distances between the two. For similarity,\nwe also used BERT to embed the generated texts.\n4.3\nResults\nFigure 2 shows the results of the first experiment\nfor all tasks. The first column shows the model\naccuracy results. It also shows the accuracy of\nzero-shot and few-shot GPT-3 classification (gray\nsolid and dashed line, respectively) and the model\ntrained with the oracle training dataset (purple line).\nThe second column shows the label accuracy, and\nthe third column shows the diversity. The diversity\nplots also show the diversity of oracle datasets (pur-\nple line). The last column shows the similarity. It\nalso shows the base similarity (brown line), which\nis the average distance between all the different\ndatasets that we considered.\nFirst, to evaluate how diversity, label accuracy,\nand similarity impact model accuracy, we per-\nformed a linear regression analysis. The analysis\nshowed that label accuracy, diversity, and similarity\nare positively correlated with model accuracy, with\nsignificance (coef=.4797 and p<0.001 for label ac-\ncuracy, coef=.2260 and p<0.001 for diversity, and\ncoef=0.1980 and p<0.005 for similarity).\nRegarding specific patterns, logit suppression in-\ncreased diversity while hurting the label accuracy\nand the similarity to the oracle dataset. High tem-\nperature increased diversity and decreased label\naccuracy, but to a smaller degree than logit sup-\npression. The application of each diversification\napproach increased the model accuracy, but when\nused together, the benefit did not add up. For in-\nstance, in Model Accuracy of Figure 2, each high\ntemperature (1.3, red light bars) and logit suppres-\nsion (dark blue bars) could increase the model ac-\ncuracy from when using a low temperature (0.3,\nlight blue bars). However, when using them to-\ngether (dark red bars), the resulting accuracy was\nnot much different from only using high temper-\natures (light red bars). It indicates that the effect\nof logit suppression has diminished by using high\ntemperatures and logit suppression together. Seed-\ning examples increases label accuracy and model\naccuracy. Examples also slightly increased diver-\nsity when used without logit suppression. Whether\nmodels trained with LLM-generated data would\nhave higher accuracy than zero- or few-shot learn-\ning of LLMs depends on the task. We provide a\ndetailed result on each task in Appendix C.\n5\nHuman Interventions to Fix Inaccurate\nText Generation\nThe first study shows that diversifying approaches\ncan have mixed effects, hurting the accuracy in gen-\neration. We propose two human interventions to\nimprove the generated data, based on issues that\nwe found from qualitatively analyzing the gener-\nated data. The first is label replacement (LR),\nswitching the misaligned label to the correct one.\nThe second is out-of-scope data filtering (OOSF),\nwhich removes instances that are outside the do-\nmain of interest and do not match any labels (OOS\ninstances).\nWhile LR and OOSF might facilitate accurate\ngeneration with diversifying approaches, inspect-\ning all data points can require a lot of effort. Hence,\nwe propose a simple way to scale the effort of the\nmodel builder, which is training a proxy model.\nWith this approach, model builders will first label\na small number of data points. Then, with those\nlabels, they will train binary classifiers as proxy\nmodels, where each learns about a single label (i.e.,\na label class from labels of interest or if the instance\nis out of scope). For unlabeled data points, proxy\nmodels can make inferences on behalf of the model\nbuilder. We introduced the specific implementation\nof this approach in Section 6.\n6\nExperiment2: Human Interventions\nFor Diversifed Text Generation\nWe evaluated LR and OOSF. Except for adding LR\nand OOSF, we used the same tasks, datasets, train-\ning methods, and metrics as in Section 4. In this\nsection, we focus on reporting results for two tem-\nperature values, 0.3 and 1.3. We present the results\nwith the rest of the temperatures in Appendix E.\nAlso, in this section, when reporting, we merged\nconditions with and without example seeding.\n6.1\nExperiment Settings\n6.1.1\nLabel Replacement\nFor LR, we conducted an oracle experiment. For\neach task, we used the highest accuracy model as\nthe oracle labeler. Therefore, we used oracle mod-\nels as a labeler, but only for SST-2, we used GPT-3\nfew-shot classification as a labeler. We conducted\nLR on the datasets generated in experiment 1.\nWe had two approaches for LR: 1) do LR to all\ndata points and 2) use proxy models with LR on\npartial data. For 1), we inspected all generated\ntexts with simulated labelers and replaced labels\nas the labelers predicted. For 2), we sampled a set\nof instances from the generated dataset, applied\nthe oracle labeler to them, and then trained proxy\nmodels with those data. Specifically, we sampled\n90, 180, or 270 data instances. When training, for\neach class, we trained a proxy model that performs\nbinary classification for the class. For each proxy\nmodel, the data instances labeled with the target\nlabel were used as positive instances, while the rest\nwere used as negative instances. We applied proxy\nmodels to the uninspected data to obtain confidence\nscores for each label. For each class, we calculated\nthe final score as follows:\nSf,i = Ss,i \u2217 w + Sp,i \u2217 (1 \u2212 w)\n(1)\nwhere for the class i, Sf,i is the final score, Sp,i\nis the confidence score of the proxy model, Ss,i is\nif the class is specified when generating the text (1\nwhen the class is specified, 0 otherwise), and w is\nthe weighting constant. We considered Ss,i as there\ncan be a chance that the proxy model is inaccurate\nand the correct labels are swapped. For our experi-\nment, we used w of 0.3. We chose the label with\nthe highest final score as the label to be replaced.\nTask\nRatio\nTask\nRatio\nCARER\n20.56%\nCB\n1.39%\nCOLA\n0.00%\nFO\n0.56%\nHWU64\n0.28%\nPubMed\n1.11%\nSST-2\n3.61%\nSUBJ\n3.06%\nTable 1: Ratio of out-of-scope instances from 360 sam-\nples.\nTask\nAccuracy (std)\nTask\nAccuracy (std)\nCARER\n94.93 (2.20)\nCB\n100 (0.00)\nSST-2\n97.18 (0.89)\nSUBJ\n97.5 (1.04)\nTable 2: OOSF proxy model performance. Note that\nCB only had five OOS instances, with one used for test.\nFor training proxy models, we trained linear sup-\nport vector classifiers with a maximum iteration of\n10000 while using texts embedded with BERT (De-\nvlin et al., 2019) as input. We chose to train mul-\ntiple proxy models for each class over training a\nsingle proxy model for all classes, as it tends to\nbe more reliable in our pilots when there are many\nclasses. As the labeling of the proxy model de-\npends on the initial samples, for each generated\ndataset in experiment 1, we applied the approach\nfive times.\n6.1.2\nOut-of-Scope Filtering\nWith OOSF, we first tried to understand how OOS\ninstances occur. Therefore, we sampled 360 data\ninstances for each task from the union of all the\ndatasets generated for the task. Then, an author\nserved as the oracle and annotated if they were\nOOS or not. Note that, as the definition of OOS\ninstance, we filtered those instances that are out-\nside the task domain or to which no label is appli-\ncable. We found that COLA, FO, HWU64, and\nPubMed have zero to four instances of OOS (Ta-\nble 1). For the later analysis, we only considered\nthe rest of the datasets, with at least five OOS in-\nstances. We present examples of OOS instances in\nAppendix D.1.\nWith the annotated data, we trained proxy mod-\nels to annotate the instances unseen by the author,\nwhich were binary linear support vector classifiers\nwith the maximum iteration of 10000 and BERT-\nembedded inputs. With the trained model, we did\nOOSF on the datasets generated in experiment 1.\nTable 2 shows the accuracy of the proxy model,\nwhen we divide the annotated data into training\nand test sets with an 8:2 ratio, with a split of ten\ntimes. Note that the perfect accuracy in CB is be-\ncause we identified only five OOS instances from\n1.0\n0.9\n0.8\n0.7\n0.6\n0.9\n0.85\n0.8\n0.75\n0.7\n0.65\n0.6\nFigure 3: Impact of label replacement on label accuracy\nand model accuracy. Throughout this paper, error areas\nindicate 95% confidence interval.\nour samples, which are extremely few.\nAfter applying LR or OOSF, we trained BERT\nmodels that serve the target task. For each dataset\nthat applied LR without proxy models or used\nOOSF, we ran the training five times. For each\ndataset that used LR with proxy models, since each\ndataset from experiment 1 has been label-replaced\nfive times, we ran training only once. With this\napproach, we acquired 15 model accuracy results\nfor each task and condition.\n6.2\nResults\n6.2.1\nLabel Replacement\nLabel Accuracy and Model Accuracy in Figure 3\nshows the results with LR. It shows how model\naccuracy and label accuracy change with the num-\nber of instances inspected (x-axis). Other metrics,\ndiversity, and similarity would not change with LR,\nas it keeps the texts as they are. For model accuracy,\nwe also visualized the performance of oracle mod-\nels and the GPT-3 few-/zero-shot classification.\nLR increases the model accuracy and label ac-\ncuracy.\nMoreover, with more labels inspected,\nthe model accuracy and label accuracy further in-\ncreased. LR also added more values to logit sup-\npression. For example, without LR, using both\nhigh temperature (1.3) and logit suppression did\nnot have a comparative benefit over using only\nhigh temperature. However, with label replace-\nment, the addition of logit suppression started to\nbenefit the model accuracy when using high tem-\nperature. When doing LR with proxy models, the\nbenefit of logit suppression increased with more in-\nstances inspected, but with full LR, the size of this\ngap decreased a little bit. With LR of all instances,\nusing both high temperature and logit suppression\nincreased the absolute model accuracy by 17.8%,\ncompared to when using neither. It was greater than\nFigure 4: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity, and\nsimilarity, in aggregation across all tasks. As we examined the effect of OOSF with LR, for model accuracy and\nlabel accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\nthe increase from diversification approaches when\nLR was not used (9.4%). Furthermore, with high\ntemperature and logit suppression, using LR on all\ninstances could increase the absolute model accu-\nracy by 14.4% compared to not doing LR. When\na high temperature and logit suppression are used\ntogether, the model accuracy outperformed GPT3\u2019s\nfew-shot classification when LR was done for 180\ninstances. Across tasks, we found that specific pat-\nterns on how diversification approaches and LR\nimpact the model accuracy can vary between tasks.\nWe provide details in Appendix E.1.\n6.2.2\nOut-of-Scope Instances Filtering\nFigure 4 shows how many instances were filtered\nwith OOSF and how it affects model accuracy, la-\nbel accuracy, diversity, and similarity. We present\nmodel accuracy from both unbalanced and bal-\nanced data: when we balanced data, we used\ndatasets with the same number of instances across\ndifferent conditions by subsampling data with the\nsmallest size of the filtered dataset. It was because\nfiltering can make the number of instances different\nbetween conditions. For unbalanced data, we did\nnot balance the number of instances.\nOOSF either increases or maintains label accu-\nracy and similarity while decreasing or maintaining\ndiversity, but there was no unified pattern of how\nthey impact the model accuracy. There tend to be\nfew OOS-filtered instances without diversification\napproaches. For example, with a temperature of 0.3\nand without logit suppression, OOSF removed very\nfew data instances. Consequently, label accuracy,\ndiversity, and similarity remained the same with\nOOSF. Without diversification approaches, the ac-\ncuracy of trained models tends to be more unstable\nwith large confidence intervals. On the other hand,\nwith diversification approaches, OOSF removed\nmore instances, and hence there were slightly more\nchanges in label accuracy, diversity, and similarity,\nwith small increases in label accuracy and similar-\nity while decreasing diversity. However, in some\ncases, these changes were subtle or within the 95%\nconfidence intervals. Moreover, how the OOSF\nchanges the model accuracy depends on the spe-\ncific task and condition. We provide the OOSF\nresults for each task in Appendix E.2.\n7\nConclusion\nIn this work, we investigate approaches to harness\nLLMs and human efforts to generate text classi-\nfication datasets with high accuracy and diversity.\nWe study two text generation diversification ap-\nproaches, 1) logit suppression, which restrains gen-\nerating already frequently generated tokens, and 2)\nhigh temperature, which flattens the sampling prob-\nability of tokens. We found that they diversify text\ngeneration but hurt the accuracy in aligning speci-\nfied labels with the generated data. We experiment\nwith two human intervention approaches, 1) replac-\ning misaligned labels with more adequate ones, and\n2) filtering out-of-scope instances. We found that\nreplacing labels makes diversification approaches\nmore beneficial by increasing the accuracy of mod-\nels trained with the generated dataset. On the other\nhand, efficient filtering of out-of-scope instances\ndid not have a positive impact on the model accu-\nracy.\n8\nLimitations\nOur implementation of proxy models applies those\nmodels after the whole data is generated. Due\nto this, in the resulting dataset, the number of in-\nstances can often be unbalanced between labels.\nSuch a limitation might be addressable by training\nproxy models from intermediate datasets with a\nsmaller number of instances, and using those mod-\nels while generating the rest of the dataset. As\nthe data become unbalanced during the generation,\nthe generation pipeline can try to generate more\ninstances with labels that are a minority in the in-\ntermediate dataset. However, when we piloted this\napproach, we identified potential problems. First,\nintermediately trained proxy models could perform\nworse than those trained after all data are generated,\ndue to the lower diversity in intermediate data used\nto train proxy models. Second, if many data points\ngenerated with a specific label (label a) actually\nbelong to another label (label b), there can be cases\nwhere most instances of label b come from the\nprompt with label a. It can skew the linguistic pat-\nterns of instances within the dataset, as only a small\nnumber of texts for label b might have been from\nthe prompt with label b. Advanced approaches to\naddress these issues can be future work directions.\nOur implementation of efficient OOSF was not\neffective in increasing model accuracy. It might be\ndue to the negative impact of removing instances,\nsuch as filtering instances on the decision boundary.\nAs our study of OOSF was not complete, future\nwork is necessary. Applying OOSF to the entire\ngenerated dataset and seeing the impact of their\nremoval would be the first step. With a comprehen-\nsible understanding of OOSF, we would be able\nto design better OOSF strategies, such as filtering\ninstances with various criteria.\nIn\nthis\nwork,\nwe\nonly\nexamined\nthe\ntext-davinci-002\nmodel\nof\nGPT-3.\nAl-\nthough we believe that the overall trends of results\nwould be similar for other models, examining\nother models with our approaches is a necessary\nfuture work. We also examined only one prompt\n(Prompt A), while there may be other options. In\nAppendix F, we present partial results on using\nanother prompt, showing that our approach is\ngeneralizable to other prompts. Combining human\ninterventions with automatic annotation error\ndetection (Klie et al., 2023) can be another future\ndirection.\n9\nEthics Statement\nLLM-generated text data could have replicated bi-\nases within the used LLM. Diversification might\nalleviate such issues, as it steers the LLM to gener-\nate texts that it considers less probable, but bias can\nstill exist after using the approach. More human\nintervention approaches can be a potential solution.\nFor example, the model builder can provide more\nspecific prompts and examples to counter the bi-\nased generation (Hartvigsen et al., 2022). However,\nthese approaches still would have limitations and\nhow these approaches would impact the data bias\nand the resulting model performance would need\nto be further researched.\nAcknowledgements\nWe want to thank Microsoft Research for support-\ning the work.\nReferences\nSaleema Amershi, James Fogarty, Ashish Kapoor, and\nDesney Tan. 2009. Overview based example selec-\ntion in end user interactive concept learning. In Pro-\nceedings of the 22nd Annual ACM Symposium on\nUser Interface Software and Technology, UIST \u201909,\npage 247\u2013256, New York, NY, USA. Association for\nComputing Machinery.\nSaleema Amershi, James Fogarty, and Daniel Weld.\n2012. Regroup: Interactive machine learning for\non-demand group creation in social networks. In\nProceedings of the SIGCHI Conference on Human\nFactors in Computing Systems, CHI \u201912, page 21\u201330,\nNew York, NY, USA. Association for Computing\nMachinery.\nSteve Branson, Catherine Wah, Florian Schroff, Boris\nBabenko, Peter Welinder, Pietro Perona, and Serge\nBelongie. 2010. Visual recognition with humans\nin the loop. In Proceedings of the 11th European\nConference on Computer Vision: Part IV, ECCV\u201910,\npage 438\u2013451, Berlin, Heidelberg. Springer-Verlag.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877\u20131901. Curran Associates,\nInc.\n\u00c1ngel Alexander Cabrera, Abraham J. Druck, Jason I.\nHong, and Adam Perer. 2021. Discovering and val-\nidating ai errors with crowdsourced failure reports.\nProc. ACM Hum.-Comput. Interact., 5(CSCW2).\nGeorge Cazenavette, Tongzhou Wang, Antonio Torralba,\nAlexei A. Efros, and Jun-Yan Zhu. 2022. Dataset\ndistillation by matching training trajectories. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition.\nAbhijnan Chakraborty, Bhargavi Paranjape, Sourya\nKakarla, and Niloy Ganguly. 2016. Stop clickbait:\nDetecting and preventing clickbaits in online news\nmedia. In 2016 IEEE/ACM International Conference\non Advances in Social Networks Analysis and Mining\n(ASONAM), pages 9\u201316.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classification. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2147\u2013\n2157, Online. Association for Computational Lin-\nguistics.\nJustin Cheng and Michael S. Bernstein. 2015. Flock:\nHybrid crowd-machine learning classifiers. In Pro-\nceedings of the 18th ACM Conference on Computer\nSupported Cooperative Work & Social Computing,\nCSCW \u201915, page 600\u2013611, New York, NY, USA.\nAssociation for Computing Machinery.\nFranck Dernoncourt and Ji Young Lee. 2017. PubMed\n200k RCT: a dataset for sequential sentence clas-\nsification in medical abstracts. In Proceedings of\nthe Eighth International Joint Conference on Natu-\nral Language Processing (Volume 2: Short Papers),\npages 308\u2013313, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nMarzieh Fadaee, Arianna Bisazza, and Christof Monz.\n2017. Data augmentation for low-resource neural\nmachine translation. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 567\u2013573,\nVancouver, Canada. Association for Computational\nLinguistics.\nJames Fogarty, Desney Tan, Ashish Kapoor, and Simon\nWinder. 2008. Cueflik: Interactive concept learning\nin image search. In Proceedings of the SIGCHI Con-\nference on Human Factors in Computing Systems,\nCHI \u201908, page 29\u201338, New York, NY, USA. Associa-\ntion for Computing Machinery.\nIan J. Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning. MIT Press, Cambridge, MA,\nUSA. http://www.deeplearningbook.org.\nDemi Guo, Yoon Kim, and Alexander Rush. 2020.\nSequence-level mixed sample data augmentation. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5547\u20135552, Online. Association for Computa-\ntional Linguistics.\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi,\nMaarten Sap, Dipankar Ray, and Ece Kamar. 2022.\nToxiGen: A large-scale machine-generated dataset\nfor adversarial and implicit hate speech detection.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 3309\u20133326, Dublin, Ireland.\nAssociation for Computational Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network.\nYutai Hou, Yijia Liu, Wanxiang Che, and Ting Liu.\n2018. Sequence-to-sequence data augmentation for\ndialogue language understanding. In Proceedings\nof the 27th International Conference on Computa-\ntional Linguistics, pages 1234\u20131245, Santa Fe, New\nMexico, USA. Association for Computational Lin-\nguistics.\nAshish Kapoor, Bongshin Lee, Desney Tan, and Eric\nHorvitz. 2010.\nInteractive optimization for steer-\ning machine classification. In Proceedings of the\nSIGCHI Conference on Human Factors in Comput-\ning Systems, CHI \u201910, page 1343\u20131352, New York,\nNY, USA. Association for Computing Machinery.\nJan-Christoph Klie,\nBonnie Webber,\nand Iryna\nGurevych. 2023. Annotation Error Detection: An-\nalyzing the Past and Present for a More Coherent\nFuture. Computational Linguistics, 49(1):157\u2013198.\nVarun Kumar, Ashutosh Choudhary, and Eunah Cho.\n2020. Data augmentation using pre-trained trans-\nformer models. In Proceedings of the 2nd Workshop\non Life-long Learning for Spoken Language Systems,\npages 18\u201326, Suzhou, China. Association for Com-\nputational Linguistics.\nShibamouli Lahiri. 2015.\nSquinky!\na corpus of\nsentence-level formality, informativeness, and im-\nplicature.\nZachary Levonian, Chia-Jung Lee, Vanessa Murdock,\nand F. Maxwell Harper. 2022. Trade-offs in sampling\nand search for early-stage interactive text classifica-\ntion. In 27th International Conference on Intelligent\nUser Interfaces, IUI \u201922, page 566\u2013583, New York,\nNY, USA. Association for Computing Machinery.\nRuibo Liu, Guangxuan Xu, Chenyan Jia, Weicheng\nMa, Lili Wang, and Soroush Vosoughi. 2020. Data\nboost: Text data augmentation through reinforcement\nlearning guided conditional generation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n9031\u20139041, Online. Association for Computational\nLinguistics.\nXingkun Liu, Arash Eshghi, Pawel Swietojanski, and\nVerena Rieser. 2021. Benchmarking Natural Lan-\nguage Understanding Services for Building Conver-\nsational Agents, pages 165\u2013183. Springer Singapore,\nSingapore.\nNathan Ng, Kyunghyun Cho, and Marzyeh Ghassemi.\n2020. SSMBA: Self-supervised manifold based data\naugmentation for improving out-of-domain robust-\nness. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 1268\u20131283, Online. Association for\nComputational Linguistics.\nBo Pang and Lillian Lee. 2004. A sentimental educa-\ntion: Sentiment analysis using subjectivity summa-\nrization based on minimum cuts. In Proceedings\nof the 42nd Annual Meeting of the Association for\nComputational Linguistics (ACL-04), pages 271\u2013278,\nBarcelona, Spain.\nEthan Perez, Saffron Huang, Francis Song, Trevor Cai,\nRoman Ring, John Aslanides, Amelia Glaese, Nat\nMcAleese, and Geoffrey Irving. 2022. Red teaming\nlanguage models with language models.\nMary Phuong and Christoph Lampert. 2019. Towards\nunderstanding knowledge distillation. In Proceed-\nings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 5142\u20135151. PMLR.\nSamuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Chris-\ntian von der Weth, and Brian Y. Lim. 2021. Directed\ndiversity: Leveraging language embedding distances\nfor collective creativity in crowd ideation. In Pro-\nceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems, CHI \u201921, New York,\nNY, USA. Association for Computing Machinery.\nMarco Tulio Ribeiro and Scott Lundberg. 2022. Adap-\ntive testing and debugging of NLP models. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3253\u20133267, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020.\nBeyond accuracy: Be-\nhavioral testing of NLP models with CheckList. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4902\u2013\n4912, Online. Association for Computational Lin-\nguistics.\nGaurav Sahu, Pau Rodriguez, Issam Laradji, Parmida\nAtighehchian, David Vazquez, and Dzmitry Bah-\ndanau. 2022. Data augmentation for intent classi-\nfication with off-the-shelf large language models. In\nProceedings of the 4th Workshop on NLP for Conver-\nsational AI, pages 47\u201357, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nElvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang,\nJunlin Wu, and Yi-Shin Chen. 2018. CARER: Con-\ntextualized affect representations for emotion recog-\nnition. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3687\u20133697, Brussels, Belgium. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631\u20131642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nJina Suh, Soroush Ghorashi, Gonzalo Ramos, Nan-Chen\nChen, Steven Drucker, Johan Verwey, and Patrice\nSimard. 2019. Anchorviz: Facilitating semantic data\nexploration and concept discovery for interactive ma-\nchine learning. ACM Trans. Interact. Intell. Syst.,\n10(1).\nLichao Sun, Congying Xia, Wenpeng Yin, Tingting\nLiang, Philip Yu, and Lifang He. 2020.\nMixup-\ntransformer: Dynamic data augmentation for NLP\ntasks. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 3436\u2013\n3440, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nJustin Talbot, Bongshin Lee, Ashish Kapoor, and\nDesney S. Tan. 2009. Ensemblematrix: Interactive vi-\nsualization to support machine learning with multiple\nclassifiers. In Proceedings of the SIGCHI Conference\non Human Factors in Computing Systems, CHI \u201909,\npage 1283\u20131292, New York, NY, USA. Association\nfor Computing Machinery.\nTongzhou Wang, Jun-Yan Zhu, Antonio Torralba, and\nAlexei A Efros. 2018. Dataset distillation. arXiv\npreprint arXiv:1811.10959.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625\u2013641.\nJason Wei and Kai Zou. 2019. EDA: Easy data augmen-\ntation techniques for boosting performance on text\nclassification tasks. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 6382\u20136388, Hong Kong, China. As-\nsociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38\u201345, Online. Association\nfor Computational Linguistics.\nTongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and\nDaniel Weld. 2019. Errudite: Scalable, reproducible,\nand testable error analysis. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 747\u2013763, Florence, Italy.\nAssociation for Computational Linguistics.\nCongying Xia, Chenwei Zhang, Hoang Nguyen, Jiawei\nZhang, and Philip Yu. 2020. Cg-bert: Conditional\ntext generation with bert for generalized few-shot\nintent detection.\nKang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo\nLee, and Woomyoung Park. 2021. GPT3Mix: Lever-\naging large-scale language models for text augmen-\ntation. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 2225\u20132239,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKang Min Yoo, Youhyun Shin, and Sang-goo Lee. 2019.\nData augmentation for spoken language understand-\ning via joint variational generation. In Proceedings of\nthe Thirty-Third AAAI Conference on Artificial Intelli-\ngence and Thirty-First Innovative Applications of Ar-\ntificial Intelligence Conference and Ninth AAAI Sym-\nposium on Educational Advances in Artificial Intelli-\ngence, AAAI\u201919/IAAI\u201919/EAAI\u201919. AAAI Press.\nAnn Yuan, Daphne Ippolito, Vitaly Nikolaev, Chris\nCallison-Burch,\nAndy Coenen,\nand Sebastian\nGehrmann. 2021. Synthbio: A case study in faster\ncuration of text datasets. In Thirty-fifth Conference\non Neural Information Processing Systems Datasets\nand Benchmarks Track (Round 2).\nJun Yuan, Jesse Vig, and Nazneen Rajani. 2022. Isea:\nAn interactive pipeline for semantic error analysis\nof nlp models. In 27th International Conference on\nIntelligent User Interfaces, IUI \u201922, page 878\u2013888,\nNew York, NY, USA. Association for Computing\nMachinery.\nHongyi Zhang, Moustapha Cisse, Yann N. Dauphin, and\nDavid Lopez-Paz. 2018. mixup: Beyond empirical\nrisk minimization. In International Conference on\nLearning Representations.\nLe Zhang, Zichao Yang, and Diyi Yang. 2022. TreeMix:\nCompositional constituency-based data augmentation\nfor natural language understanding. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 5243\u20135258,\nSeattle, United States. Association for Computational\nLinguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Proceedings of the 28th International\nConference on Neural Information Processing Sys-\ntems - Volume 1, NIPS\u201915, page 649\u2013657, Cambridge,\nMA, USA. MIT Press.\nJing Zhou, Yanan Zheng, Jie Tang, Li Jian, and Zhilin\nYang. 2022. FlipDA: Effective and robust data aug-\nmentation for few-shot learning.\nIn Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 8646\u20138665, Dublin, Ireland. Association for\nComputational Linguistics.\nA\nEquation for Temperature Sampling\nMathematically, with the temperature T and origi-\nnal probability of token, pi, the temperature sam-\npled probability of token i, fT (p)i, would be de-\nnoted as below:\nfT (p)i =\np1/T\ni\n\u03a3jp1/T\nj\n(2)\nB\nExperiment 1 Details\nB.1\nPrompts Used in LLM Generation\nFor each task, we used prompt A with text types\nand labels as in Table 3. For example, for CB, a\nprompt can look like the below with examples:\nWrite a news headline to cover all following elements\nElements: valid news\nNews headline: \"Zach Johnson Wins Sony Open\"\n- - - - -\nWrite a news headline to cover all following elements\nElements: clickbait\nNews headline: \"10 Of The Biggest Lies We Were\nTold In 2015\"\n- - - - -\nWrite a news headline to cover all following elements\nElements: clickbait\nNews headline:\"\n(B)\nB.2\nSampling Oracle Dataset\nFor the oracle dataset, if there are more than 5600\ndata points in the original dataset (CB, CARER,\nHATE, COLA, HWU64, SUBJ), we subsampled\n5600 training data points. For SST2, we used all\n6922 instances from the original dataset. Note that\nthese numbers are the same as the number of gen-\nerated data instances. For FO, we used the original\ntraining dataset as is (with 3622 data instances),\nas there are fewer than 5600 instances. For test\ndatasets, from the same original dataset exclud-\ning instances used for the oracle dataset, we sam-\npled 2400 data points for CB, CARER, HATE, and\nHWU64. For FO, COLA, SUBJ, and SST-2, we\nused the original test datasets as there were fewer\nthan 2400 instances.\nC\nResults of the Experiment 1 on\nIndividual Dataset\nHere, we introduce the result of the first experiment\nfor individual tasks (Figure 5).\n0.4\n0.6\n0.8\n1.0\na) CARER\nModel Accuracy\n0.4\n0.6\n0.8\n1.0\nLabel Accuracy\n0.0\n0.1\n0.2\nDiversity\n0.8\n0.9\n1.0\nSimilarity\n0.4\n0.6\n0.8\n1.0\nb) CB\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\nc) COLA\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\nd) FO\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\ne) HWU64\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\nf) PubMed\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\ng) SST2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\n0.4\n0.6\n0.8\n1.0\nh) SUBJ\n0.4\n0.6\n0.8\n1.0\n0.0\n0.1\n0.2\n0.8\n0.9\n1.0\nOracle\nGPT Zero\nGPT Few\nBase Similarity\nTemp=0.3, Logit Sup=X\nTemp=0.7, Logit Sup=X\nTemp=0.9, Logit Sup=X\nTemp=1.3, Logit Sup=X\nTemp=0.3, Logit Sup=O\nTemp=0.7, Logit Sup=O\nTemp=0.9, Logit Sup=O\nTemp=1.3, Logit Sup=O\nExample=X\nExample=O\nFigure 5: Impact of logit suppression and high temperatures on model accuracy, label accuracy, diversity, and\nsimilarity to the oracle dataset, for each task.\nTask\nText type\nLabel \u2192 Label in prompts\nCARER\nemotional tweet\njoy \u2192 expressing joy, anger \u2192 expressing anger, fear \u2192 expressing fear,\nsadness \u2192 expressing sadness, love \u2192 expressing love, surprise \u2192 expressing surprise\nCB\nnews headline\nnon-clickbait \u2192 valid news, clickbait \u2192 clickbait\nCOLA\nsentence\ngrammatically acceptable \u2192 grammatically correct sentence,\ngrammatically unacceptable \u2192 grammatically incorrect sentence\nFO\nsentence\ninformal \u2192 informal, formal \u2192 formal\nHWU64\nhuman utterance to\na chatbot\nnews \u2192 news, weather \u2192 weather, play \u2192 play, datetime \u2192 datetime, iot \u2192 iot,\ncooking \u2192 cooking, recommendation \u2192 recommendation, calendar \u2192 calendar,\nmusic \u2192 music, takeaway \u2192 takeaway, lists \u2192 list, transport \u2192 transport, qa \u2192 qa,\nsocial \u2192 social, general \u2192 general, alarm \u2192 alarm, email \u2192 email, audio \u2192 audio\nPubMed\nsentence\nfrom\na\nmedical paper\nobjective \u2192 sentence about objective, methods \u2192 sentence about methods, results \u2192\nsentence about results, conclusions \u2192 sentence about conclusions,\nbackground \u2192 sentence about background\nSST-2\nmovie review\npositive \u2192 positive sentiment, negative \u2192 negative sentiment\nSUBJ\nsentence\nfrom\na\nmovie review\nobjective \u2192 objective statement, subjective \u2192 subjective statement\nTable 3: Text types and labels used in prompts.\nThe benefit of logit suppression for each task\ndepends on the combination of label accuracy, di-\nversity, and similarity. Tasks that have high base\nlabel accuracy tend to improve model accuracy\nmore with logit suppressions. For example, for CB\nand SST-2, those conditions with logit suppressions\nwere clear winners in model accuracy over other\ncombinations of approaches. For other tasks, where\noverall label accuracy tends to be lower, logit sup-\npression did not have large benefits. COLA was the\nextreme case where the label accuracy was about\n50% in binary classification, indicating that the per-\nformance of the LLM in generating label-accurate\ninstances was not better than random chance. In\nthis case, logit suppression resulted in almost no\nincrease in the model accuracy. Even in this case,\nlogit suppression could increase the diversity of the\ngenerated text. With PubMed, we could observe an\nexception of label accuracy increasing with logit\nsuppression when example seeding and high tem-\nperature (1.3) are not used (compare light and dark-\ncolored unhatched bars in PubMed\u2019s Label Accu-\nracy from Figure 5, except for red bars). It was be-\ncause GPT-3 generates many similar errors without\nlogit suppression and seeding examples. Specifi-\ncally, without logit suppression, when prompted to\nwrite about the background sentence in a medical\npaper, GPT-3 generated many sentences starting\nwith \u201cThe purpose of this study was,\u201d which is\nmore about the objective.\nFor temperature also, specific patterns on how\nit affected label accuracy, diversity, and similarity\ndiffer between tasks. In PubMed, without logit\nsuppression and example seeding, label accuracy\neven increased with higher temperatures, which\nwas against the general pattern. In this case, similar\nto what we found with logit suppression, the lack of\ndiversification approaches led to the generation of\nnarrowly populated error instances. CARER was\nanother case with the reversed trend: without logit\nsuppression and seeding examples, the mean diver-\nsity was higher with a temperature of 0.7 than with\na temperature of 1.3. It was because, with the high\ntemperature of 1.3, many sentences started with\n\u201cI\u2019m so,\u201d (on average 3012 occurrences) which was\nless the case for the lower temperatures of 0.7 and\n0.9 (on average 841.5 occurrences). In CARER,\nwhen example seeding and logit suppression are\nnot used, label accuracy was also higher with the\ntemperature of 1.3 than with lower temperatures,\nalthough the means were within 95% confidence\nintervals. In this case, with lower temperatures of\n0.7 and 0.9, more instances started with \u201cNo matter\nwhat,\u201d which continues with advice on what to do\nin emotional situations. For such cases, no label is\napplicable since they are not the self-expression of\nemotions (on average, 32 occurrences with a tem-\nperature of 1.3 and 682.7 occurrences with temper-\natures of 0.7 or 0.9). Note that these are examples\nof out-of-scope instances. Summarizing results of\nlogit suppression and temperature sampling, these\napproaches increased diversity while hurting the\nlabel accuracy, but specific patterns could vary be-\ntween tasks.\nThe utility of example seeding in label accuracy\nand model accuracy could also vary between tasks.\nFor example, in the extreme case of COLA, ex-\namples did not increase label accuracy and model\naccuracy. How seeding examples impact the gen-\neration of data similar to the oracle dataset also\nTask\nExample\nReason for filtering\nCARER\nNo matter what life throws at you,\nalways\nremember to find joy in the little things.\n#HappyThoughts\nNot a self-expression of emotion\nCB\nValid News\nNot a news headline\nSST-2\nJurassic World Fallen Kingdom\nOnly movie title\nSUBJ\nFor what it\u2019s worth,\nIncomplete sentence and unable to decide subjectivity\nTable 4: Examples of OOS instances.\n0\n90\n180\n270\nAll\n0.4\n0.6\n0.8\na) CARER\nModel Accuracy\n0\n90\n180\n270\nAll\n0.6\n0.8\n1.0\nLabel Accuracy\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\n0.9\n1.0\nb) CB\nModel Accuracy\n0\n90\n180\n270\nAll\n0.90\n0.95\n1.00\nLabel Accuracy\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\nc) COLA\n0\n90\n180\n270\nAll\n0.6\n0.8\n1.0\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\n0.9\nd) FO\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n90\n180\n270\nAll\n0.6\n0.8\ne) HWU64\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n90\n180\n270\nAll\n0.4\n0.5\n0.6\n0.7\nf) PubMed\n0\n90\n180\n270\nAll\n0.6\n0.7\n0.8\n0.9\n1.0\n0\n90\n180\n270\nAll\n# of instances inspected\n0.8\n0.9\ng) SST2\n0\n90\n180\n270\nAll\n# of instances inspected\n0.90\n0.95\n1.00\n0\n90\n180\n270\nAll\n# of instances inspected\n0.4\n0.6\n0.8\nh) SUBJ\n0\n90\n180\n270\nAll\n# of instances inspected\n0.00\n0.25\n0.50\n0.75\n1.00\nTemp=0.3\nTemp=0.7\nTemp=0.9\nTemp=1.3\nLogit Sup=X\nLogit Sup=O\nOracle\nGPT Few\nFigure 6: Impact of label replacement on model accuracy, label accuracy, for each task, on all temperature values.\nFigure 7: Impact of label replacement on model ac-\ncuracy, label accuracy, for all tasks aggregated, on all\ntemperature values.\ndepends on the task.\nFor CARER, HWU64, and PubMed in Figure 5,\nthere were cases where the model accuracy was\nhigher than the accuracy of GPT-3\u2019s few-shot learn-\ning. Other tasks showed lower accuracy than GPT-\n3\u2019s few-shot learning accuracy, indicating that GPT-\n3 few-shot classification can be a better alternative\nthan training a model with generated data if the\nmodel builder has a budget to continuously access\nGPT-3 and is willing to hand over data through\nAPI. In Section 6, we show that human interven-\ntions can be a way to make the data generation\napproach applicable in more tasks by increasing\nthe model accuracy higher than that of few-shot\nclassifications from GPT-3.\nFigure 8: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity, and\nsimilarity, for all tasks aggregated, on all temperature values. As we examined the effect of OOSF with LR, for\nmodel accuracy and label accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\nD\nExperiment 2 Details\nD.1\nExamples of OOS instances.\nWe present examples of OOS instances in Table 4.\nE\nResults of the Experiment 2 on Varying\nTasks\nWe present the results of experiment 2 for individ-\nual tasks. Note that we also show results for all\ntemperature values (0.3, 0.7, 0.9, and 1.3).\nE.1\nLabel Replacement\nFigure 6 and 7 shows the LR result for individ-\nual tasks and whole tasks aggregated, respectively,\nwith all temperatures. First, there were cases where\nlogit suppression provided additional benefit upon\nhigh temperature only when LR was applied (com-\nparing thick and thin red lines in Model Accuracy\nof CARER, HWU64, and PubMed in Figure 6).\nSecond, for tasks that already have high accuracy\nwithout LR (CB and SST-2), LR either resulted in\nvery small model accuracy increases or even hurted\nthe accuracy. For example, in SST-2, the label ac-\ncuracy was already high without LR, and doing LR\nwith proxy models could even decrease the label\naccuracy and model accuracy. Third, without diver-\nsification approaches, there were also cases where\nLR did not increase model accuracy much while la-\nbel accuracy was greatly increased (thin blue lines\nin Model Accuracy of CARER, CB, FO, PubMed,\nSST2, SUBJ in Figure 6). It may show that fixing\nlabels is more beneficial when there is enough di-\nversity in the generated dataset. Fourth, CB, FO,\nand SUBJ were cases where models trained with\ngenerated data could outperform GPT-3\u2019s few-shot\nclassification only with label replacement (some\ncolored lines go over gray dashed lines with LR in\nModel Accuracy of CB, FO, and SUBJ in Figure 6).\nAmong them, with FO, inspecting partial instances\ncould also turn the model accuracy higher than\nthat of GPT-3 few-shot classification. As expected,\nno approaches outperform oracle models as those\nmodels are used for LR. Fifth, for tasks with many\nclasses (CARER, HWU64, and PubMed), when us-\ning LR with proxy models, the performance tends\nto increase not much dramatically as the number\nof annotated instances increases (Model Accuracy\nof CARER, HWU64, and PubMed in Figure 6).\nHigher model accuracy leaps occurred when all\ninstances were inspected. It may indicate the diffi-\nculty of training accurate proxy models with many\nclasses to consider.\nE.2\nOut-of-Scope Filtering\nFigure 8 and 9 shows the OOSF results with all\ntemperatures, for the aggregation of all tasks and\nindividual tasks, respectively. As mentioned in the\nmain text, it was difficult to find a general pattern of\nhow OOSF impacts the model accuracy. Consistent\npatterns were that OOSF tends to increase or main-\ntain label accuracy and similarity while decreasing\nor maintaining diversity.\nF\nResults on Prompt C\nOn two tasks (FO, HWU64), we conducted the\nexperiment with another instructional prompt:\nShow me a text type that has the following charac-\nteristics\nCharacteristics: label\ntext type: \"Generated text\"\n(C)\nWe measured model accuracy, label accuracy,\ndiversity, and similarity of generated datasets and\nalso investigated how label replacement impacts la-\nbel accuracy and model accuracy. The experiment\nsetting was the same as the main experiment we\nconducted, except the prompt used. The trend in\nthe results (Figure 10) was similar to that of the\nprompt A.\n0.00\n0.25\n0.50\n0.75\n1.00\na) CARER\nRatio of Unfiltered\n0\n+OOS 180\n+OOS All\n+OOS\n0.4\n0.6\n0.8\nUnbalanced Model Accuracy\n0\n+OOS 180\n+OOS All\n+OOS\n0.4\n0.6\n0.8\nBalanced Model Accuracy\n0\n+OOS 180\n+OOS All\n+OOS\n0.6\n0.8\n1.0\nLabel Accuracy\n+OOS\n0.100\n0.125\n0.150\n0.175\nDiversity\n+OOS\n0.76\n0.78\n0.80\n0.82\n0.84\nSimilarity\n0.00\n0.25\n0.50\n0.75\n1.00\nb) CB\n0\n+OOS 180\n+OOS All\n+OOS\n0.7\n0.8\n0.9\n0\n+OOS 180\n+OOS All\n+OOS\n0.7\n0.8\n0.9\n0\n+OOS 180\n+OOS All\n+OOS\n0.90\n0.95\n1.00\n+OOS\n0.15\n0.20\n+OOS\n0.75\n0.76\n0.77\n0.78\n0.79\n0.0\n0.5\n1.0\nc) SST2\n0\n+OOS 180\n+OOS All\n+OOS\n0.80\n0.85\n0.90\n0.95\n0\n+OOS 180\n+OOS All\n+OOS\n0.75\n0.80\n0.85\n0.90\n0.95\n0\n+OOS 180\n+OOS All\n+OOS\n0.90\n0.95\n1.00\n+OOS\n0.05\n0.10\n0.15\n+OOS\n0.76\n0.78\n0.80\n0.82\n0.84\n0.00\n0.25\n0.50\n0.75\n1.00\nd) SUBJ\n0\n+OOS 180\n+OOS All\n+OOS\n0.6\n0.7\n0.8\n0.9\n0\n+OOS 180\n+OOS All\n+OOS\n0.6\n0.7\n0.8\n0.9\n0\n+OOS 180\n+OOS All\n+OOS\n0.6\n0.8\n1.0\n+OOS\n0.10\n0.15\n0.20\n+OOS\n0.76\n0.78\n0.80\nBase Similarity\nTemp=0.3\nTemp=0.7\nTemp=0.9\nTemp=1.3\nLogit Sup=X\nLogit Sup=O\nOracle\nGPT Few\nTemp=0.3, Logit Sup=X\nTemp=0.7, Logit Sup=X\nTemp=0.9, Logit Sup=X\nTemp=1.3, Logit Sup=X\nTemp=0.3, Logit Sup=O\nTemp=0.7, Logit Sup=O\nTemp=0.9, Logit Sup=O\nTemp=1.3, Logit Sup=O\nFigure 9: The ratio of instances filtered with OOSF, and its impact on model accuracy, label accuracy, diversity,\nand similarity, for each task, on all temperature values. As we examined the effect of OOSF with LR, for model\naccuracy and label accuracy, numbers left to +OOS indicate how many instances are inspected with LR.\n0\n90\n180\n270\nAll\n# of Instances Inspected\nwith label replacement\n0.75\n0.80\n0.85\n0.90\n0.95\nModel Accuracy\nGPT3 Few shot\nOracle\n0\n90\n180\n270\nAll\n# of Instances Inspected\nwith label replacement\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nLabel Accuracy\nLow Temp (0.3)\nLogit Sup+Low Temp (0.3)\nHigh Temp (1.3)\nLogit Sup+High Temp (1.3)\n0.0\n0.1\n0.2\n0.3\n0.4\na) FO\nDiversity\nOracle\nLow Temp (0.3)\nHigh Temp (1.3)\nLogit Sup+Low Temp (0.3)\nLogit Sup+High Temp (1.3)\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nSimilarity\nBase Similarity\n0\n90\n180\n270\nAll\n# of Instances Inspected\nwith label replacement\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nModel Accuracy\n0\n90\n180\n270\nAll\n# of Instances Inspected\nwith label replacement\n0.6\n0.7\n0.8\n0.9\n1.0\nLabel Accuracy\n0.0\n0.1\n0.2\n0.3\n0.4\nb) HWU64\nDiversity\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n1.05\nSimilarity\nFigure 10: Result on prompt C.\n"
  },
  {
    "title": "Text-only Domain Adaptation using Unified Speech-Text Representation in Transducer",
    "link": "https://arxiv.org/pdf/2306.04076.pdf",
    "upvote": "1",
    "text": "Text-only Domain Adaptation using Unified Speech-Text Representation in\nTransducer\nLu Huang\u2217, Boyu Li\u2217, Jun Zhang, Lu Lu, Zejun Ma\nByteDance\n{huanglu.thu19,liboyu.622}@bytedance.com\nAbstract\nDomain adaptation using text-only corpus is challenging in\nend-to-end(E2E) speech recognition.\nAdaptation by synthe-\nsizing audio from text through TTS is resource-consuming.\nWe present a method to learn Unified Speech-Text Represen-\ntation in Conformer Transducer(USTR-CT) to enable fast do-\nmain adaptation using the text-only corpus. Different from the\nprevious textogram method, an extra text encoder is introduced\nin our work to learn text representation and is removed during\ninference, so there is no modification for online deployment.\nTo improve the efficiency of adaptation, single-step and multi-\nstep adaptations are also explored. The experiments on adapting\nLibriSpeech to SPGISpeech show the proposed method reduces\nthe word error rate(WER) by relatively 44% on the target do-\nmain, which is better than those of TTS method and textogram\nmethod. Also, it is shown the proposed method can be com-\nbined with internal language model estimation(ILME) to further\nimprove the performance.\nIndex Terms: automatic speech recognition, text-only, domain\nadaptation, conformer transducer\n1. Introduction\nIn recent years, E2E models have achieved significant im-\nprovements in automatic speech recognition(ASR)[1, 2, 3].\nCompared with hybrid models, where acoustic, pronuncia-\ntion, and language models(LMs) are built and optimized sep-\narately, E2E models have achieved promising performance by\ndirectly mapping speech features into word sequences. There\nare some popular E2E models, including connectionist temporal\nclassification[4, 5], recurrent neural network transducer(RNN-\nT)[2, 6, 7], and attention-based encoder-decoder[8, 9, 10].\nHowever, unlike hybrid models which can adapt to new do-\nmains by training LMs using text-only data, E2E model has dif-\nficulty in domain adaptation using text-only data. Besides, the\nE2E models are trained with paired speech-text data, so their\ngeneralization ability to different contents is limited, and the\nperformance degrades when a mismatch exists between source\nand target domains. To overcome this, the most promising ap-\nproach is to adapt the E2E model using text-only data, because\nit is much easier to collect text-only data than paired speech-text\ndata in the target domain.\nSeveral methods have been proposed to adapt the E2E\nmodel to new domain. The most common solution is to train\nan external LM using text corpus in the target domain and in-\ntegrate it into the E2E model during inference, such as shal-\nlow fusion[11], density ratio fusion[12], deep fusion[13], cold\nfusion[14], and internal LM(ILM) estimation based fusion[15,\n* Equal contribution.\n16]. Nevertheless, all these methods involve an external LM\nduring inference, and the computation cost of decoding is in-\ncreased.\nOther methods attempt to directly update the E2E model to\navoid changing the decoding. Synthesizing paired speech-text\ndata using TTS is a common solution[17, 18], but the process is\ncomplex, also the storage and computation cost increases. Re-\ncently, a text-to-mel-spectrogram generator is proposed in E2E\nmodel[19] for replacing TTS. The method proposed in [20] in-\nserts a temporary LM layer into the prediction network and the\nLM loss is used for adapting with text-only data. Internal LM\nadaptation(ILMA)[21] is proposed to fine-tune the parameters\nof internal LM with an additional LM loss.\nAlternative approaches focus on creating a shared embed-\nding space by joint training for two modalities, i.e., speech and\ntext, and have been reported to improve ASR performance with-\nout increasing model parameters or decoding complexity[22,\n23]. Recent works[24, 25] involve this idea to make a consis-\ntent representation between text and speech features in text-only\nadaptation tasks.\nInspired by [22, 24, 25], we proposed a method to learn\nthe unified speech-text representation in Conformer Transducer\n(USTR-CT) for fast text-only domain adaptation. Separated en-\ncoders are adopted to learn a shared representation for speech\nand text features respectively, and a shared encoder is used for\nthe fusion of speech and text representation. At the same time,\ndifferent representation units are explored and phoneme repre-\nsentation performs best in the target domain. To improve the\nefficiency of adaptation, single-step and multi-step adaptations\nare also explored. Finally, We observe 44% relative WER re-\nduction in the target domain with unpaired text data. In ad-\ndition, combined with ILME, the proposed method can obtain\nfurther gains.\nThe paper is organized as follows: Section 2 gives a brief\nintroduction about related work. The proposed USTR-CT is\ndiscussed in Section 3, followed by experiments and discus-\nsions in Section 4.\n2. Related work\n2.1. Speech-text joint training for ASR\nSeveral methods have been proposed to train E2E model with\nspeech and text modalities[22, 23, 26, 27, 28, 29, 30, 31].\nThe recent JOIST[23] explores joint training with a com-\nbination of losses computed on the supervised paired speech-\ntext data and the unpaired text data in cascaded encoder based\nstreaming ASR framework[32]. The input unpaired text repre-\nsentation are up-sampled with a simple and parameter-free du-\nration model, and then fed to a text encoder after masking. The\noutput of the text encoder can be fed to the first-pass decoder,\narXiv:2306.04076v1  [cs.CL]  7 Jun 2023\nor to the shared encoder and second-pass decoder. However,\nthe experiments were conducted on a multi-domain corpus and\nthe performance of long-tail rare words are evaluated. In this\nwork, we mainly focus on domain adaptation, i.e., transferring\nthe model to the target domain with text-only data.\nIn order to ensure that the representations learned from\nspeech and text features are aligned, MAESTRO[22] introduces\na consistency loss to align two types of representations using\nthe paired data. The method has achieved significant improve-\nments on ASR and speech translation tasks. Different from our\nwork, it focuses on the self-supervised training to obtain a better\npre-trained model rather than domain adaptation. Besides, the\nadditional duration model increases the complexity of training.\n2.2. Text-only domain adaptation\nIn this subsection, we give a brief introduction to text-only do-\nmain adaptation methods without using external LM.\nTTS adaptation generates paired speech-text data from the\ntarget domain text for fine-tuning[17, 18]. However, the num-\nber of speakers is limited for TTS and training a reliable multi-\nspeaker TTS model is time-consuming and computationally ex-\npensive. Besides, saving the synthesized data also increases\nstorage costs. Moreover, due to the mismatch between syn-\nthetic and real audio, additional issues may be also introduced.\nTo mitigate the above problems, recently a text-to-spectrogram\nfront-end composed of a text-to-mel-spectrogram generator was\nintroduced in ASR model[19]. In this way, on-the-fly generat-\ning spectrograms from text-only data is possible, and the mis-\nmatch between real and synthetic audio is not obvious as before.\nHowever, it still needs to pay attention to the quality of the spec-\ntrogram enhancer in the generator during training.\nILMA propose to adapt the E2E model[21] by fine-tuning\nthe ILM with text-only data, and parameter regularization is\nadded to avoid over-fitting. Also, it is essential to perform ILM\ntraining(ILMT)[33] in addition to ASR loss before ILMA to\nensure that the ILM behaves like a standalone LM. As only the\nlast linear layer of the jointer network is updated, ILMA\u2019s per-\nformance on the target domain is also limited.\nTextogram was proposed in [24], where the text represen-\ntation is created by repeating one-hot embedding of text tokens\nby a fixed number of times. The textogram features are stacked\ntogether with standard speech features. When training using\ntext-only data, the speech features are set to zero. And when\ntraining using paired speech-text data, the textogram features\nare set to zero. Due to the concatenation of speech features and\ntextogram, it is necessary to concatenate zero textogram fea-\ntures with speech features during inference. However, in our\nwork, with separated encoders, the input is either text features\nor speech features during training, and inference can be per-\nformed directly without any modifications. Besides, updating\nonly the jointer performs best in textogram, while the jointer,\npredictor, and even encoder can be adapted to the target domain\nwith better performance. In addition to grapheme representa-\ntion, subword and phoneme representations are also explored in\nour work.\n3. Training and adapting methods\n3.1. Model architecture\nFor the standard RNN-T, speech-text pairs are used to train the\nmodel. Let xspeech\n1:T\nbe the audio features like Fbank, and y0:u\u22121\nbe the previous tokens, the output of RNN-T at frame t and step\nShared\nEncoder\nAudio\nEncoder\nText\nEncoder\nPredictor\nJointer\n\ud835\udc32!:#$%\n\ud835\udc31%:&\n'())*+\n\ud835\udc31%:,\n-).-\n\ud835\udc43 \ud835\udc32/\npaired speech and text \nunspoken text\nEncoder\nFigure 1: The model structures of USTR-RNN-T.\nu is computed by\nhenc\n1:T = Encoder(xspeech\n1:T ),\n(1)\nhpred\nu\n= Predictor(y0:u\u22121),\n(2)\nhjoint\nt,u = Jointer(henc\nt , hpred\nu ).\n(3)\nThen with a Softmax layer on the hjointer\nt,u\nand forward-\nbackward algorithm[7], Transducer loss, which is the sum prob-\nability P(y|x) of all possible alignment paths \u03c0, is computed\nas the training objective function\nLrnn-t = \u2212 log\nX\n\u03c0\u2208\u03a0(y)\nP(\u03c0|xspeech\n1:T ),\n(4)\nwhere y = (y1, . . . , yU) is the label sequence, U is the number\nof target tokens, and \u03a0(y) is the alignment path sets.\nTo involve the text-only corpus during training, the RNN-\nT encoder is split into two parts, named AudioEncoder and\nSharedEncoder, and an extra TextEncoder is introduced\nto model text features xtext\n1:N, which is illustrated in Figure 1 as\nUSTR-RNN-T. The Transducer loss can be computed the same\nway as paired speech-text corpus,\nhenc,text\n1:N\n= SharedEncoder(TextEncoder(xtext\n1:N)),\n(5)\nhjoint,text\nn,u\n= Jointer(henc,text\nn\n, hpred\nu ),\n(6)\nwhere xtext\n1:N can be grapheme/sub-word/phoneme representa-\ntions. As the extra TextEncoder can be removed during in-\nference, the proposed method doesn\u2019t need any modification for\nonline deployment.\nIn our experiments, SharedEncoder consists twelve\nnon-streaming Conformer[34] layers, so the baseline is noted\nas Conformer Transducer(CT). AudioEncoder has two\nConv2d layers with stride of 2 and a linear projection layer,\nresulting a time reduction of 4. TextEncoder contains an\nembedding layer and a Transformer layer. For CT\u2019s prediction\nnetwork, 2-layer LSTM is adopted, and RNN-T\u2019s jointer net-\nwork is a feed-forward layer.\n3.2. Training\nTo enforce speech-text modality matching in a joint embed-\nding space for SharedEncoder, paired speech-text samples\nare needed for training AudioEncoder and TextEncoder.\nInit Model\nCT\nunspoken\ntarget-domain text\nTTS audio\nAdapted CT\nUSTR-CT\n\ud835\udc37!\n\ud835\udc37\"\"#\n\ud835\udc37$\n\ud835\udc37$\n\ud835\udc37$\n\ud835\udc37$ + \ud835\udc37!\n\ud835\udc37$+ \ud835\udc37\"\"#\n\ud835\udc37$ + \ud835\udc37!\nSingle-step USTR-CT\nMulti-step USTR-CT\nTTS Adaptation\npaired\nspeech and text\nTTS\nFigure 2: The adaptation processes of TTS, multi-step and\nsingle-step USTR-CT.\nWhen TextEncoder is introduced in the training, the paired\nspeech-text in the training corpus is used as unspoken text with\na random probability p by using the text features instead of the\naudio features.\nThree types of text features are considered in this work.\nThe first one is grapheme features, which is similar as the\ntextogram in [24].\nThe second one is subword features,\nwhich is the same as the output vocabulary of CT, and is gen-\nerated using subword-nmt[35]1. The final one is phoneme\nfeatures, which is generated by a Grapheme-to-Phoneme sys-\ntem. For English in this work, g2pE2 is used.\nTo simulate the duration of speech features, the text fea-\ntures are repeated a fixed number of times, which is the same\nas that in [24, 23]. Also, text features are masked to prevent the\nTextEncoder from memorizing the grapheme/subword se-\nquence blindly [24, 23]. However, the masking method differs\nfrom that in [24, 23] by applying on repeated text features. It is\nfound that masking before repeating brings better performance.\nDuring training, a mini-batch containing both text and\nspeech features is fed into the model. Besides, ILMT loss is\nchosen as an optional auxiliary loss, and the overall loss is\nL = Lrnn-t + \u03bbLilmt,\n(7)\nwhere \u03bb is the weight corresponding to ILMT loss, which is set\nto 0.2 in all experiments.\n3.3. Adapting\nWhen the text-only corpus is used for adapting the CT model to\na new domain, two adaptation strategies are investigated in this\nwork, as illustrated in Figure 2.\n3.3.1. Multi-step adaptation\nAs illustrated in the bottom part of Figure 2, multi-step adapta-\ntion using USTR-CT contains two steps.\nIn the first step, paired speech-text data is used to train a\nUSTR-CT, where each sample is fed into the TextEncoder\nby using text features instead of audio features with a random\nprobability p to train the TextEncoder. The probability p is\nset to 0.15 in the experiments.\nIn the second step, i.e., the stage of adapting, paired speech-\ntext data and unspoken text are both used in each mini-batch\nwith a ratio of 1:1. The ratio can be further tuned to obtain\nbetter performance on the target domain, which is left for future\ndiscussion. The paired speech-text data is used to maintain the\n1https://github.com/rsennrich/subword-nmt\n2https://github.com/Kyubyong/g2p\nperformance on source domain. In this step, the parameters of\nAudioEncoder and SharedEncoder (i.e., the Encoder\nof CT) are kept constant, while Jointer and Predictor are\ntrained to adapt to new domain. Due to the existence of USTR-\nCT model after first step, it is more convenient for adapting to\nother domains when there is multi-domain scenario.\n3.3.2. Single-step adaptation\nAs shown in the middle part of Figure 2, single-step USTR-CT\ntrains an adapted CT model from random initialization directly.\nSimilar to multi-step USTR-CT, paired speech-text data is also\nfed into the TextEncoder by a probability p = 0.15. Also,\nthe ratio between paired speech-text data and unspoken text is\nstill 1:1 to be consistent with multi-step adaptation.\n4. Experiments and results\n4.1. Experimental setup\nThe experiments are conducted on LibriSpeech[36] and\nSPGISpeech[37] corpora. SPGISpeech contains 5,000 hours\nof financial audio. In this work, only the transcribed text of\nSPGISpeech is used for text-only domain adaptation, which has\n1.7M utterances. Two versions of the text are created in the\nexperiments, noted as Large(L) and Small(S), where the for-\nmer contains the full 1.7M utterances and the latter contains\na subset of 280k utterances, which is almost the same as the\nnumber of Librispeech utterances. Besides, the TTS audios are\nsynthesized from the Small subset using an in-house engine to\nindicate that TTS is resource-consuming.\nFor the audio features, the 80-dim filter-bank(Fbank) is\nused and Spec-Augment[38] is applied on Fbank features. The\ntext features are masked with a probability of 0.15 before re-\npeating. The model\u2019s structure is described as that in Section 3,\nand the output of RNN-T is 4,048 subword units. All models\nare trained with PyTorch[39]. WER is evaluated on Librispeech\ntest-clean/test-other sets and SPGISpeech val set to\nmeasure the ASR performance on source and target domain.\n4.2. Baseline systems\nA CT model is trained on LibriSpeech, which achieves a WER\nof 23.55% on SPGISpeech val set. With TTS based adapta-\ntion, as shown in the top part of Figure 2, the WER is reduced\nto 14.99% by 36.35% relatively. Besides, the textogram[24]\nmethod is also evaluated in this work, which achieves a WER\nof 23.94%. Textogram based adaptation, where the encoder\nand jointer are kept constant, is trained with text-only cor-\npus. And after adaptation, the WER is reduced by relatively\n33.25%/37.80% when using the S/L subset respectively.\nThe proposed multi-step USTR-CT is firstly trained with\ngrapheme representation by masking with a rate of 0.15 and\nrepeating four times. As illustrated in Table 1, the proposed\nUSTR-CT achieves a WER of 22.72% on SPGISpeech val set\nbefore adaptation, which is better than the textogram. After\nadaptation, the proposed method not only performs better on\nthe target domain, but also achieves the best performance on\nLibriSpeech test sets, as the paired speech-text data is used to\nmaintain the performance of source domain during adaptation.\nThe results indicate that the extra text encoder in USTR-CT and\njointer adaptation in the second step are beneficial. Also, the\nproposed method outperforms TTS based adaptation on both\nsource and target domains, with relative WER reductions of\n2.54%\u223c5.45%.\nTable 1: The WER(%) of different systems on LibriSpeech test\nsets and SPGISpeech val set. Text adaptation L/S corresponds\nto the Large/Small subset of SPGISpeech\u2019s transcribed text.\nmodel\nLibriSpeech test\nSPGISpeech\nclean/other\nval\nCT\n3.99/8.28\n23.55\n+ TTS adaptation\n3.85/8.12\n14.99\ntextogram baseline [24]\n4.18/8.84\n23.94\n+ text adaptation(S)\n5.12/10.10\n15.98\n+ text adaptation(L)\n4.43/8.98\n14.89\nmulti-step USTR-CT\n3.76/8.15\n22.72\n+ text adaptation(S)\n3.66/8.00\n14.89\n+ text adaptation(L)\n3.64/7.84\n14.61\n4.3. Representation units\nTable 2: The WER(%) of multi-step USTR-CT using different\nrepresentation units for text features.\nmodel\nLibriSpeech test\nSPGISpeech\nclean/other\nval\ngrapheme repeat 4\n3.76/8.15\n22.72\n+ text adaptation(L)\n3.64/7.84\n14.61\nphoneme repeat 3\n4.10/8.32\n23.11\n+ text adaptation(L)\n3.80/7.96\n13.41\nphoneme repeat 4\n3.82/8.18\n22.30\n+ text adaptation(L)\n3.64/7.80\n13.38\nphoneme repeat 5\n3.82/8.08\n22.17\n+ text adaptation(L)\n3.71/7.82\n13.42\nsubword repeat 4\n4.20/8.49\n23.92\n+ text adaptation(L)\n3.80/8.27\n15.00\nDifferent representation units are explored for multi-step\nUSTR-CT, where the repeating number of phoneme represen-\ntation is also investigated. As illustrated in Table 2, for the re-\npeating number of 4, phoneme representation performs best on\ntarget domain(WER 13.38% vs. 14.61%/15.00%). This may\ndue to the phoneme representation is more relevant to Fbank\nfeatures, and thus the learning of unified speech-text represen-\ntation can be more easier. Besides, we changed the repeating\nnumber from 4 to 3/5 and no further gains were observed.\n4.4. Multi-step vs. single-step\nWe compared single-step USTR-CT with multi-step USTR-CT\nand TTS based adaptation, and the results are illustrated in Ta-\nble 3. It is shown that single-step USTR-CT performs best on\nboth source and target domain. As the shared encoder is also\nadapted to target domain, the single-step USTR-CT performs\neven better than multi-step USTR-CT. Besides, the extra text of\nSPGISpeech also benefits the source domain.\n4.5. Combination with ILME\nAs the text of target domain is already involved in training of\nthe CT model, the benefit of external LM may be discounted.\nWe have trained an LSTM LM and evaluated the performance\nTable 3: The WER(%) of multi-/single-step USTR-CT.\nmodel\nLibriSpeech test\nSPGISpeech\nclean/other\nval\nCT + TTS adaptation\n3.85/8.12\n14.99\nmulti-step USTR-CT\n3.82/8.18\n22.30\n+ text adaptation(L)\n3.64/7.80\n13.38\nsingle-step USTR-CT(L)\n3.07/7.13\n13.25\nTable 4: The WER(%) of different models with ILME.\nmodel\nSPGISpeech val\nCT\n23.55\n+ ILME\n13.83\nCT + TTS adaptation\n14.99\n+ ILME\n11.34\nmulti-step USTR-CT\n13.38\n+ ILME\n10.05\nsingle-step USTR-CT\n13.25\n+ ILME\n10.80\nof ILME using different CT models. As illustrated in Table 4,\nILME brings WER reductions of 41.27%/24.35% on baseline\nCT and TTS based models. For multi-/single-step USTR-CT,\nILME also reduces the WER by 24.89%/18.49% respectively.\nThis indicated that the proposed USTR is able to combine with\nILME to further improve the performance on the target domain.\nIt is noticed that multi-step USTR-CT performs better than\nsingle-step USTR-CT when combined with ILME, which is dif-\nferent from the results without ILME in Section 4.4. We assume\nthat the ILM score of single-step USTR-CT is not accurate, as\nthe encoder also captures the linguistic information of target do-\nmain during training. Besides, as the encoder is frozen during\nthe 2nd step, multi-step USTR-CT is more suitable for training\nwith a large scale text corpus.\n5. Conclusions\nIn this work, an extra text encoder is introduced for text-\nonly domain adaptation, which outperforms the TTS adapta-\ntion by 11.61% relatively when using phoneme representation.\nCompared to TTS adaptation, the proposed USTR-CT is effi-\ncient and resource-saving for fast domain adaptation. Besides,\nUSTR-CT is able to adapt to the target domain with a single-\nstep training and combine with ILME to obtain further gains.\nAlthough experiments were conducted on non-streaming mod-\nels in this work, the method is still applicable for streaming\nASR. With the separated speech and text encoders, the simi-\nlarity between the speech and text modalities can be considered\nto further improve the performance, which is left as a discussion\nin future work.\n6. References\n[1] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \u201cState-\nof-the-art speech recognition with sequence-to-sequence models,\u201d\nin ICASSP, 2018, pp. 4774\u20134778.\n[2] T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier,\nS.-y. Chang, W. Li, R. Alvarez, Z. Chen et al., \u201cA streaming\non-device end-to-end model surpassing server-side conventional\nmodel quality and latency,\u201d in ICASSP, 2020, pp. 6059\u20136063.\n[3] J. Li et al., \u201cRecent advances in end-to-end automatic speech\nrecognition,\u201d APSIPA Transactions on Signal and Information\nProcessing, vol. 11, no. 1, 2022.\n[4] A. Graves, S. Fern\u00b4andez, F. Gomez, and J. Schmidhuber, \u201cCon-\nnectionist temporal classification:\nlabelling unsegmented se-\nquence data with recurrent neural networks,\u201d in ICML, 2006, pp.\n369\u2013376.\n[5] J. Li, G. Ye, A. Das, R. Zhao, and Y. Gong, \u201cAdvancing acoustic-\nto-word ctc model,\u201d in ICASSP, 2018, pp. 5794\u20135798.\n[6] J. Li, R. Zhao, Z. Meng, Y. Liu, W. Wei, S. Parthasarathy,\nV. Mazalov, Z. Wang, L. He, S. Zhao et al., \u201cDeveloping rnn-\nt models surpassing high-performance hybrid models with cus-\ntomization capability,\u201d in Interspeech, 2020, pp. 3590\u20133594.\n[7] A. Graves, \u201cSequence transduction with recurrent neural net-\nworks,\u201d in ICML Representation Learning Workshop, 2012.\n[8] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Ben-\ngio, \u201cAttention-based models for speech recognition,\u201d in NeurIPS,\nvol. 28, 2015.\n[9] S. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang,\nM. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., \u201cA\ncomparative study on transformer vs rnn in speech applications,\u201d\nin ASRU 2019, 2019, pp. 449\u2013456.\n[10] J. Li, Y. Wu, Y. Gaur, C. Wang, R. Zhao, and S. Liu, \u201cOn the\ncomparison of popular end-to-end models for large scale speech\nrecognition,\u201d in Interspeech, 2020, pp. 1\u20135.\n[11] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos,\nE. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al.,\n\u201cDeep speech: Scaling up end-to-end speech recognition,\u201d arXiv\npreprint arXiv:1412.5567, 2014.\n[12] E. McDermott, H. Sak, and E. Variani, \u201cA density ratio approach\nto language model fusion in end-to-end automatic speech recog-\nnition,\u201d in ASRU 2019, 2019, pp. 434\u2013441.\n[13] C. Gulcehre, O. Firat, K. Xu, K. Cho, L. Barrault, H.-C. Lin,\nF. Bougares, H. Schwenk, and Y. Bengio, \u201cOn using mono-\nlingual corpora in neural machine translation,\u201d arXiv preprint\narXiv:1503.03535, 2015.\n[14] A. Sriram, H. Jun, S. Satheesh, and A. Coates, \u201cCold fusion:\nTraining seq2seq models together with language models,\u201d in In-\nterspeech, 2018, pp. 387\u2013391.\n[15] E. Variani, D. Rybach, C. Allauzen, and M. Riley, \u201cHybrid au-\ntoregressive transducer (hat),\u201d in ICASSP, 2020, pp. 6139\u20136143.\n[16] Z. Meng, S. Parthasarathy, E. Sun, Y. Gaur, N. Kanda, L. Lu,\nX. Chen, R. Zhao, J. Li, and Y. Gong, \u201cInternal language model\nestimation for domain-adaptive end-to-end speech recognition,\u201d\nin SLT, 2021, pp. 243\u2013250.\n[17] Y. Huang, J. Li, L. He, W. Wei, W. Gale, and Y. Gong, \u201cRapid\nrnn-t adaptation using personalized speech synthesis and neural\nlanguage generator.\u201d in Interspeech, 2020, pp. 1256\u20131260.\n[18] C. Peyser, S. Mavandadi, T. N. Sainath, J. Apfel, R. Pang, and\nS. Kumar, \u201cImproving tail performance of a deliberation e2e asr\nmodel using a large text corpus,\u201d in Interspeech, 2020, pp. 4921\u2013\n4925.\n[19] V. Bataev, R. Korostik, E. Shabalin, V. Lavrukhin, and B. Gins-\nburg, \u201cText-only domain adaptation for end-to-end asr using\nintegrated text-to-mel-spectrogram generator,\u201d arXiv preprint\narXiv:2302.14036, 2023.\n[20] J. Pylkk\u00a8onen, A. Ukkonen, J. Kilpikoski, S. Tamminen, and\nH. Heikinheimo, \u201cFast Text-Only Domain Adaptation of RNN-\nTransducer Prediction Network,\u201d in Interspeech, 2021, pp. 1882\u2013\n1886.\n[21] Z. Meng, Y. Gaur, N. Kanda, J. Li, X. Chen, Y. Wu, and Y. Gong,\n\u201cInternal Language Model Adaptation with Text-Only Data for\nEnd-to-End Speech Recognition,\u201d in Interspeech, 2022, pp. 2608\u2013\n2612.\n[22] Z. Chen, Y. Zhang, A. Rosenberg, B. Ramabhadran, P. J. Moreno,\nA. Bapna, and H. Zen, \u201cMAESTRO: Matched Speech Text Rep-\nresentations through Modality Matching,\u201d in Interspeech, 2022,\npp. 4093\u20134097.\n[23] T. N. Sainath, R. Prabhavalkar, A. Bapna, Y. Zhang, Z. Huo,\nZ. Chen, B. Li, W. Wang, and T. Strohman, \u201cJoist: A joint speech\nand text streaming model for asr,\u201d in SLT 2022, 2023, pp. 52\u201359.\n[24] S. Thomas, B. Kingsbury, G. Saon, and H.-K. J. Kuo, \u201cIntegrating\ntext inputs for training and adapting rnn transducer asr models,\u201d\nin ICASSP, 2022, pp. 8127\u20138131.\n[25] H. Sato, T. Komori, T. Mishima, Y. Kawai, T. Mochizuki, S. Sato,\nand T. Ogawa, \u201cText-only domain adaptation based on intermedi-\nate ctc,\u201d in Interspeech, 2022, pp. 2208\u20132212.\n[26] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark,\nM. Johnson, J. Riesa, A. Conneau, and Y. Zhang, \u201cSlam: A unified\nencoder for speech and language modeling via speech-text joint\npre-training,\u201d arXiv preprint arXiv:2110.10329, 2021.\n[27] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng,\nS. Khanuja, J. Riesa, and A. Conneau, \u201cmslam: Massively mul-\ntilingual joint pre-training for speech and text,\u201d arXiv preprint\narXiv:2202.01374, 2022.\n[28] Y. Tang, H. Gong, N. Dong, C. Wang, W.-N. Hsu, J. Gu,\nA. Baevski, X. Li, A. Mohamed, M. Auli et al., \u201cUnified speech-\ntext pre-training for speech translation and recognition,\u201d in ACL,\n2022, pp. 1488\u20131499.\n[29] S. Thomas, H.-K. J. Kuo, B. Kingsbury, and G. Saon, \u201cTowards\nreducing the need for speech training data to build spoken lan-\nguage understanding systems,\u201d in ICASSP, 2022, pp. 7932\u20137936.\n[30] Y.-A. Chung, C. Zhu, and M. Zeng, \u201cSplat: Speech-language joint\npre-training for spoken language understanding,\u201d in Proceedings\nof the 2021 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language Tech-\nnologies, 2021, pp. 1897\u20131907.\n[31] J. Ao, R. Wang, L. Zhou, C. Wang, S. Ren, Y. Wu, S. Liu, T. Ko,\nQ. Li, Y. Zhang et al., \u201cSpeecht5: Unified-modal encoder-decoder\npre-training for spoken language processing,\u201d in ACL, 2022, pp.\n5723\u20135738.\n[32] A. Narayanan, T. N. Sainath, R. Pang, J. Yu, C.-C. Chiu, R. Prab-\nhavalkar, E. Variani, and T. Strohman, \u201cCascaded encoders for\nunifying streaming and non-streaming asr,\u201d in ICASSP, 2021, pp.\n5629\u20135633.\n[33] Z. Meng, N. Kanda, Y. Gaur, S. Parthasarathy, E. Sun, L. Lu,\nX. Chen, J. Li, and Y. Gong, \u201cInternal language model training\nfor domain-adaptive end-to-end speech recognition,\u201d in ICASSP,\n2021, pp. 7338\u20137342.\n[34] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han,\nS. Wang, Z. Zhang, Y. Wu et al., \u201cConformer: Convolution-\naugmented transformer for speech recognition,\u201d in Interspeech,\n2020.\n[35] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural machine transla-\ntion of rare words with subword units,\u201d in ACL, 2016, pp. 1715\u2013\n1725.\n[36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLib-\nrispeech: an asr corpus based on public domain audio books,\u201d\nin ICASSP, 2015, pp. 5206\u20135210.\n[37] P. K. O\u2019Neill, V. Lavrukhin, S. Majumdar, V. Noroozi, Y. Zhang,\nO. Kuchaiev, J. Balam, Y. Dovzhenko, K. Freyberg, M. D. Shul-\nman, B. Ginsburg, S. Watanabe, and G. Kucsko, \u201cSpgispeech:\n5,000 hours of transcribed financial audio for fully formatted end-\nto-end speech recognition,\u201d in Interspeech, 2021, pp. 1434\u20131438.\n[38] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D.\nCubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmentation\nmethod for automatic speech recognition,\u201d in Interspeech, 2019,\npp. 2613\u20132617.\n[39] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch:\nAn imperative style, high-performance deep learning library,\u201d in\nNeurIPS, vol. 32, 2019.\n"
  },
  {
    "title": "Certified Reasoning with Language Models",
    "link": "https://arxiv.org/pdf/2306.04031.pdf",
    "upvote": "1",
    "text": "CERTIFIED DEDUCTIVE REASONING WITH LANGUAGE\nMODELS\nGabriel Poesia, Kanishk Gandhi\u2217, Eric Zelikman\u2217, Noah D. Goodman\nStanford University\n{poesia,kanishkg,ezelikman,ngoodman}@stanford.edu\nABSTRACT\nLanguage models often achieve higher accuracy when reasoning step-by-step in\ncomplex tasks. However, even when arriving at a correct final answer, their ra-\ntionales are often logically unsound or inconsistent. This is a major issue when\nreliable reasoning traces are needed, such when fine-tuning on model-generated\nreasoning for self-improvement. To tackle these issues, we introduce a class of\ntools for language models called guides, that use state and incremental constraints\nto guide generation. A guide can be invoked by the model to constrain its own\ngeneration to a set of valid statements given by the tool. In turn, the model\u2019s choices\ncan change the guide\u2019s state. We show how a general system for logical reasoning\ncan be used as a guide, which we call LOGICGUIDE. Given a reasoning problem\nin natural language, a model can formalize its assumptions for LOGICGUIDE and\nguarantee that its step-by-step reasoning is sound. In experiments on PrOntoQA,\nProofWriter and Syllogism Validity datasets, LOGICGUIDE significantly improves\nthe performance of GPT-3, GPT-3.5 Turbo and LLaMA (accuracy gains up to 35%),\nwhile drastically reducing content effects \u2014 the interference between unwanted\nprior assumptions and reasoning, which humans and language models suffer from.\nWe then explore bootstrapping GPT-3.5 Turbo and LLaMA using their own rea-\nsoning traces. We find that LogicGuide is critical: by training only on certified\nself-generated reasoning, models can self-improve, avoiding learning from their\nown hallucinations. Moreover, bootstrapped models enjoy significant boosts on\nReClor, a challenging real-world reasoning dataset, even when not relying on\nformalization at inference time.\n1\nINTRODUCTION\nConsider a language-based autonomous agent tasked with managing a user\u2019s calendar and email. The\nuser might want to specify general principles on how the agent should behave, such as \u201cif the email is\nfrom any of my managers, you must send me a notification\u201d, and important pieces of information\nsuch as \u201cI\u2019m part of the research team\u201d, or \u201cGrace manages research\u201d. When the agent analyzes an\nemail and decides what actions to take, we\u2019d like it to respect the given instructions. Doing so might\nrequire reasoning: the agent should conclude that an email from Grace warrants a notification, even\nif that wasn\u2019t said explicitly. How should the agent draw such conclusions?\nA Large Language Model (LLM), such as GPT-3 (Brown et al., 2020) or PaLM (Chowdhery et al.,\n2022), can in principle take in the given instructions and context, choose actions to take and, before\neach action, ask itself \u201cis this permitted?\u201d The answer might require making chains of inferences\nbased on the user\u2019s input. For this class of problems, LLMs have been shown to dramatically benefit\nfrom chain-of-thought reasoning (Wei et al., 2022; Suzgun et al., 2022). Empirically, allowing LLMs\nto generate reasoning steps before their answer consistently yields higher accuracy across a wide\nrange of tasks (Suzgun et al., 2022). Qualitatively, reasoning steps are often seen as \u201can interpretable\nwindow\u201d into how the model arrived at the answer (Wei et al., 2022), in contrast to an opaque guess.\nBut much like humans, language models can also produce unsound reasoning: even after correctly\ninterpreting a problem, they can take logically invalid inference steps, or produce a guess at the final\nanswer that is not supported by their own rationale (Saparov & He, 2022). Moreover, LLMs have also\n1\narXiv:2306.04031v2  [cs.AI]  8 Nov 2023\nFigure 1: A language model can invoke a guide tool, such as our LOGICGUIDE, to perform certifiable\ngenerations. Here, when the model decides to generate an infer block, it is constrained to generate\none of the formal deductions established by an external theorem-proving environment.\nbeen observed to show human-like content effects in reasoning: their accuracy drops significantly\nwhen asked to reason with assumptions that contradict their prior beliefs (Dasgupta et al., 2022).\nHow can we avoid unsound, perhaps dangerous, inferences? This question illustrates the central\nconcern that led to the development of formal logic. Proof systems formally encode core patterns\nunderlying valid reasoning, allowing sound inferences to be generated mechanically from deduction\nrules. If we arrive at an answer after a series of formal deductions, we know that not only the\nconclusion is correct, but the reasoning that led to it was entirely valid. This is in contrast to a\nfree-form rationale that can arrive at the correct answer even through incorrect means.\nTo that end, we aim to allow LLMs to rely on trusted formal deductions during reasoning by building\non the recent paradigm of tool use in language models (Cobbe et al., 2021; Schick et al., 2023). In\nprior work, LMs invoke external tools by generating special sequences, intercepted by the decoding\nalgorithm. They can generate inputs (e.g., a mathematical operation, or search query) and receive the\ntool\u2019s output as if it was their own generation. We generalize this input-output paradigm to a broader\nclass of LM tools we call guides. When a guide is invoked by the model using a special delimiter, the\ntool computes a space of valid outputs, as illustrated in Fig. 1. We then employ constrained decoding\n(Poesia et al., 2022) to ensure the model will incrementally generate one of the valid outputs. Guides\nthus enable a more declarative interaction between tool and model: the guide declares a set of possible\nsequences, while the model brings prior expectations used to generate one among them. A guide can\nmaintain state: its next valid outputs might depend on the sequence of choices up to that point.\nWe use this framework to allow language models to locally constrain generation to a set of valid\nstatements determined by an external tool. We leverage the Peano theorem-proving environment\n(Poesia & Goodman, 2022) to construct LOGICGUIDE, which an LM can use to formalize its\nassumptions, set proof goals and make sound inferences step-by-step. The model can intersperse\nformal reasoning and natural language during generation. Language conditioned on previous formal\nsteps is highly reliable, since the generations allowed by LOGICGUIDE are formally certified.\nWe first validate our method on three logical reasoning datasets, PrOntoQA (Saparov & He, 2022),\nProofWriter (Tafjord et al., 2021), and Syllogistic Validity (Dasgupta et al., 2022). We also follow the\nformat and methodology of PrOntoQA to introduce a new dataset, DeontiQA, where problems require\nreasoning using deontic logic principles to determine whether actions are permissible, obligatory or\nforbidden. When used with few-shot prompting, we find that LOGICGUIDE significantly improves\nthe accuracy of OpenAI GPT-3 and GPT-3.5 Turbo, and LLaMA 13B. Moreover, models using\nLOGICGUIDE have drastically lower content effects: we show this both with PrOntoQA and in the\nSyllogism Validity dataset, used previously to measure content effects in LLMs.\nWhile for these problems we could leverage an external solver to obtain a final answer after the\nmodel produces a sufficient formalization, a key advantage of LOGICGUIDE is that the LLM still\ngenerates a step-by-step rationale. This allows us to then apply self-improvement methods, such as\nthe Self-Taught Reasoner (STaR; Zelikman et al. (2022)), which improves the model\u2019s own reasoning\nby fine-tuning on rationales that led to correct answers. In the tasks we analyze here, there\u2019s a high\nprobability of guessing the answer (e.g. true or false, so at least 50%). Hence, STaR alone fails to\nyield meaningful improvements, as it fine tunes on incorrect reasoning that does not generalize. In\ncontrast, we show that running STaR using only certified solutions generated with LOGICGUIDE is\n2\nhighly effective: LLaMA 13B enjoys accuracy gains of up to 17% on PrOntoQA, while na\u00efve STaR\nfails to improve the model.\nFinally, we investigate whether bootstrapped models learn reasoning patterns that generalize beyond\nproblems where full formalization is possible. We fine-tune GPT-3.5 Turbo on its own correct\nsolutions for problems in PrOntoQA and ProofWriter and evaluate it on ReClor, a challenging set of\nproblems from real-world standardized exams requiring reading comprehension and logical reasoning,\nas well as 6 tasks in the LEGALBENCH dataset. Bootstrapped GPT-3.5 Turbo outperforms both the\nbase model, and performs best when fine-tuned on its solutions generated with LOGICGUIDE. These\nresults suggest a promising avenue for bootstrapping language models on logical reasoning.\n2\nRELATED WORK\nOur work builds on two classes of systems for reasoning: language models, which can reason flexibly\nin natural language, and formal reasoning systems, which rely on formal logic to derived certified\ninferences. To interface these two systems, we leverage recent methods for constrained decoding\nfrom language models. Specifically, we employ Constrained Semantic Decoding (CSD, Poesia et al.\n(2022)), an algorithm that guarantees valid samples by construction. CSD does not require full\naccess to the model, only the ability to bias its logits. This allows us to use GPT-3 and GPT-3.5\nTurbo through their public APIs, as well as LLaMA (Brown et al., 2020; Touvron et al., 2023).\nOther decoding methods, such as NeuroLogic Decoding (Lu et al., 2021) and NeuroLogic A*esque\ndecoding (Lu et al., 2022), have been proposed to enforce lexical constraints at inference time.\nLLMs have been increasingly used as agents interacting with other systems, by both using tools to\ndelegate computation or to trigger external actions (Schick et al., 2023; Yao et al., 2022; Yang et al.,\n2023; Hosseini-Asl et al., 2020; Shah et al., 2023). In prior work, LLMs can provide inputs to an\nexternal tool, such as a search query (Schick et al., 2023) or a mathematical operation (Cobbe et al.,\n2021), and receive the output in the decoding stream. Our framework of guides (\u00a73) can be seen as a\ngeneralization of this paradigm, where the tool defines a space of outputs and the LM chooses one\nusing its own probabilities.\nOur approach to certifying reasoning from LMs relies on grounding their inferences in an interactive\ntheorem prover, Peano (Poesia & Goodman, 2022). Similar to other popular theorem proving\nlanguages like Lean (de Moura et al., 2015) and Coq (Barras et al., 1997), Peano uses dependent type\ntheory as its logical foundation. Most theorem proving environments are designed for the verification\nof given proofs. In contrast, and of special interest to us, Peano is designed to aid in generation\nby exposing a finite action space. Many other recent works have integrated LLMs and interactive\ntheorem provers in the context of formal mathematical reasoning. Recent work on autoformalization\nhas shown that LLMs can be effective in translating informal to formal mathematics (Wu et al., 2022).\nThis idea is related to how we use LLMs to formalize their assumptions given in natural language,\nthough our end goal is to produce reliable natural language rationales rather than formal proofs.\nMany prior works have broadly used neural networks for logical reasoning. One prominent line of\nwork has focused on using neural networks to approximately execute logical reasoning on symbolic\nknowledge bases. This includes Neural Theorem Provers (Rockt\u00e4schel & Riedel, 2017) and Condi-\ntional Theorem Provers (Minervini et al., 2020). Other approaches use encoders for allowing problem\nstatements in natural language and perform reasoning in latent space, such as Discourse-Aware\nGraph Networks (Huang et al., 2021). Unlike both approaches, which learn to perform reasoning\n\u201cin-weights\u201d via fine-tuning, our goal is to augment chain-of-thought reasoning in language models,\nwhere all inputs, reasoning steps and outputs are realised in language.\n3\nCERTIFIED REASONING WITH GUIDES\nPrevious work in tools for language models assumed an interface where the model provides inputs\nto the tool and receives back a single output, conditioning on this output for further generation. For\ninstance, Cobbe et al. (2021) allowed the model to rely on a calculator by generating a string such\nas \u00ab51*12=. At this point, the decoding algorithm would execute the operation externally and\ncopy the result as if it was generated by the language model. Here, our main goal is to leverage a\ntrusted external tool to answer the question: \u201cwhat logical inferences can be made next?\u201d Unlike an\n3\narithmetic operation, this question (1) can have a potentially large set of answers, and (2) can depend\non previous choices made by the model. Thus, our key idea is to leverage constrained generation to\nallow the model to implicitly choose one of the valid inferences during decoding, while remaining\ncontained in those that are valid.\nMore generally, a guide tool defines a set of valid generations given previous choices. Formally, let\nS = \u03a3\u2217 be the set of strings in the guide\u2019s alphabet \u03a3, with S\u2217 denoting the set of finite sequences of\nsuch strings. We define a guide g to be a function g : S\u2217 \u2192 P(S) that takes a sequence of previously\ngenerated strings and returns a regular set of allowed next generations. Our idea is to use g at specific\npoints when sampling from a language model PLM(\u00b7) so that when the guide is invoked at a prefix\ns0, we will sample a continuation from PLM(s|s0) that belongs to the set allowed by g when given\nthe previous guided generations in the prefix s0 (e.g., previous valid logical inferences).\n3.1\nFROM GUIDES TO COMPLETION ENGINES\nGiven any guide function g as above, we want to provide a tool for LMs that, once invoked by a\nspecial sequence, will constrain the immediate subsequent output using g. Let t1 and t2 be two\narbitrary delimiters, such that t1 begins a guided block in the LM\u2019s generation, which is then closed\nby t2 (e.g., we later use t1 = \"[[\" and t2 = \"]]\"). Intuitively, we would like to decode from\nPLM with the following idea: (1) we sample tokens until the model generates t1; once that happens,\nwe (2) use g along with the generation so far to obtain a set of valid continuations, then (3) trigger\nconstrained generation to sample from that set, and finally (4) return to unconstrained generation\nonce the model outputs t2. Unfortunately, tokenizers complicate implementing this procedure, as\ndifferent models often have different vocabularies (like all 3 models we leverage in \u00a74 do), and LM\ntokens can be arbitrarily misaligned with t1 and t2. For example, the string \"[[\" might be a single\ntoken, contained in a larger token, or be broken up across two tokens depending on the context.\nTo overcome these issues and implement LM guides in a model-agnostic manner, we employ the\nConstrained Semantic Decoding algorithm (CSD; Poesia et al. (2022)). CSD effectively solves\nthe vocabulary alignment problem by providing the abstraction of a \u201ccompletion engine\u201d, which\nincrementally dictates valid generations by constructing local regular expressions. CSD samples\nfrom the model while guaranteeing a valid output (i.e., one that respects the completion engine). We\nformally define how we construct a completion engine to implement guide tools in the Appendix.\nUsing our implementation, however, users can simply implement an arbitrary function g with the\nsignature above and obtain a procedure to sample from any LM with the guide being invoked\nwhen needed. This framework allows us to easily design rich, context-sensitive LM tools, as the\nLOGICGUIDE we introduce next. We describe several other guides in the Appendix, leaving their\nexploration for future work.\n3.2\nTHE LOGICGUIDE\nWe now construct LOGICGUIDE, a guide tool for language models to perform externally certified\nreasoning. Our logical backend of choice is Peano (Poesia & Goodman, 2022), a theorem-proving\nenvironment for incremental proof generation. The main feature of Peano we rely on is that it provides\na finite action space. Thus, given a partial argument, Peano gives us a list of valid inferences that can\nbe made in a single step given the background theory, assumptions (possibly previously added by\nthe model) and past inferences. Our idea is to use this list to guide the model whenever it decides to\nderive a logical conclusion. While Peano makes the guide implementation particularly simple, other\ntheorem-proving environments might be adaptable for this purpose.\nWe use the delimiters \"[[\" and \"]]\", and implement a guide function that accepts strings with the\nformat action:parameter. We define 6 actions (exemplified in Fig. 2) that allow the model to\n(1) formalize its assumptions (object, prop, relation, axiom), (2) set a goal (goal), and (3)\nperform logical inferences (infer). For (1) and (2), the guide returns constraints that ensure the\nmodel\u2019s formalization to be syntactically valid. Since these actions are the boundary between natural\nand formal language, it is impossible to guarantee that they are semantically valid in the sense that\nthe model has properly interpreted the natural language context. What is certifiable is that the logical\ninferences (action type 3) follow from the model\u2019s formalization, i.e. its inference are valid given its\nexplicit assumptions. (\u00a74 provides empirical evidence that formalization errors rarely lead to a wrong\nconclusion; most often they make the model unable to prove or disprove the goal).\n4\nContext: 1- The mouse visits the tiger. (...) 3- If something visits the tiger then it visits the mouse. (...) 12- If something \nvisits the mouse then it is blue. (...)\nQuestion: True or false: The mouse is green?\nFormalized \ncontext: \n1- The [[object: mouse]] [[relation: visits]] the [[object: tiger]] [[axiom:   \n(visits mouse tiger)]] (...) 3- If something [[relation: visits]] the [[object: tiger]] \nthen it [[relation: visits]] the [[object: mouse]] [[axiom: (visits 'x tiger) -> \n(visits 'x mouse)]] (...) 12- If something [[relation: visits]] the [[object: mouse]] then \nit is [[prop: blue]] [[axiom: (visits 'x mouse) -> (blue 'x)]] (...)\n[[infer: (visits mouse mouse)]] The mouse visits itself. [[infer: (blue mouse)]] The \nmouse is blue. [[infer: (green mouse)]] The mouse is green. This satisfies the goal.\n[[goal: (green mouse)]]\n   Formalized goal:\nReasoning: \nAnswer:\nTRUE\nFigure 2: Example solution of gpt3.5-turbo using LOGICGUIDE in a problem from ProofWriter.\nThe model\u2019s generation starts at the \u201cFormalized context\u201d. This example shows all 6 actions we\nimplement in the LOGICGUIDE: object declares a particular entity, prop and relation mark\nunary and binary predicates, respectively; axiom denotes propositions assumed to hold (possibly\nimplications), goal sets a target to be proven or contradicted, and infer marks deductions.\nUsing the guide\nIn the typical setup for logical reasoning problems (Tafjord et al., 2021; Saparov\n& He, 2022), the input contains a context (the set of assumptions) and a question (a goal that the\nlogical argument must conclude or negate). In our experiments, we demonstrate how to use the\nguide by creating few-shot examples with the proper LOGICGUIDE action annotations (as in Fig. 2).\nSpecifically, we add a section before the rationale named \u201cFormalized context\u201d where we repeat the\nassumptions in the scenario while marking objects, properties and relations, and formalizing each of\nthe assumptions into an [[axiom:]] block. We do the same for the goal. Then, we prepend each\nreasoning step with the appropriate [[infer:]] action. In this way the model is encouraged to\nfirst generate a formal inference step and only then its natural language counterpart. We include all of\nour prompts in the Appendix.\n4\nEXPERIMENTAL EVALUATION\nWe now evaluate the effectiveness of LOGICGUIDE in improving language models on reasoning\ntasks. (Our code and data are available at github.com/<<redacted>>). We focus on four\nresearch questions: (RQ1) Does LOGICGUIDE improve the accuracy of language models in multi-\nstep reasoning? (RQ2) Does LOGICGUIDE reduce content effects in language model reasoning?\n(RQ3) Can an LLM self-improve using LOGICGUIDE by learning from its own solutions? (RQ4) Do\nbootstrapped models also improve in tasks where they cannot rely on LOGICGUIDE during inference?\n4.1\nIMPACT OF LOGICGUIDE IN MULTI-STEP REASONING ACCURACY\nDatasets\nWe first use two recent natural language reasoning datasets: PrOntoQA (Saparov & He,\n2022) and ProofWriter (Tafjord et al., 2021). Both datasets contain reasoning problems with (1) a list\nof assumptions (e.g. \u201cEvery dog is a mammal\u201d, or \u201cSam is a dog\u201d), and (2) a proposition that can\nbe reasoned about from the assumptions (e.g. \u201cSam is a mammal?\u201d). In both datasets, the goal is to\nanswer the question with either true or false. Problems are categorized by how many reasoning \u201chops\u201d\nthe solution needs (1 to 5). In addition, PrOntoQA has three splits: \u201cTrue Ontology\u201d, where the rules\nare coherent with common sense, \u201cFalse Ontology\u201d, where rules violate commonsense (e.g., \u201cEvery\ncomposite number is prime\u201d), and \u201cFictitional Ontology\u201d, which uses made-up concepts (e.g., \u201cEvery\nwumpus is feisty.\u201d). ProofWriter uses real concepts for all rules (e.g., people, animals, colors), but the\nrules are generated at random\u2013thus they also often contradict commonsense. We use the problems\nfrom ProofWriter where the answer can be proved (i.e. ignoring the \u201cclosed-world assumption\u201d and\n\u201cunknown\u201d problems, where fully justifying the answer requires meta-logical reasoning).\nLanguage models\nWe evaluate three language models in the few-shot setting: OpenAI GPT-3\n(text-davinci-003; Brown et al. (2020)), OpenAI GPT-3.5 Turbo (gpt-3.5-turbo) and\nLLaMA 13B (Touvron et al., 2023). We use 4 few-shot examples for the vanilla models. For guided\nmodels, the prompt examples are augmented to show formalized reasoning. In the prompt, we first\nshow the model how to formalize the assumptions and the goal, and then present the chain-of-thought\n5\nFigure 3: Final answer accuracies with guided and unguided language models on PrOntoQA and\nProofWriter, with bootstrapped 95% confidence intervals.\nwhere natural language sentences are preceded by a guided inference (in an infer block, c.f. \u00a73.2).\nSince this makes the prompt longer, we only use two prompt examples for the guided models: one\nwhere the answer is true and one where it is false. We implement CSD on the OpenAI models\nusing their public API, which exposes a parameter to bias the logits on given tokens. We use the\nrejection-based sampling procedure described in Poesia et al. (2022). gpt3.5-turbo requires a\nslight adaptation (to resume generation after a constraint is applied) because of its chat-based API;\nwe detail this along with all of our prompts in the Appendix.\nResults\nFig. 3 shows few-shot results on multi-hop reasoning, measuring final-answer accuracy.\nOverall, guided models perform significantly better. GPT-3 and GPT-3.5 are highly accurate in formal-\nizing assumptions, and enjoy the largest benefits (with nearly perfect performance on PrOntoQA with\nLOGICGUIDE, and improving from chance to 80% correct on ProofWriter). For them, LOGICGUIDE\nessentially eliminates single-step reasoning errors, and the impact of this benefit grows in solutions\nrequiring more hops\u2014a single error is enough to reach the wrong final conclusion. LLaMA 13B\nsees gains between 10 and 20% in PrOntoQA False and Fictitional, while LOGICGUIDE hurts its\nperformance in PrOntoQA True (where, effectively, reasoning is not necessary, only commonsense)\nand ProofWriter (where LLaMA is more often inconsistent in its formalization).\nWe observe two main failure modes: (1) models can misformalize assumptions, and (2) they can\nfail at planning, making a sequence of valid inferences that do not ultimately lead to the goal.\nWhen formalization errors happen, it\u2019s more common that no conclusion can be drawn, rather than\na wrong conclusion: in only 1.6% of the solutions did a guided model formally derive a wrong\nanswer; these cases were mostly due to missing a negation when formalizing a sentence (mostly\nLLaMA on ProofWriter). A more common formalization failure (especially for LLaMA) was to use\ninconsistent names for properties or relations, e.g. (sees A B) in one place and (see B C) in\nanother. When no further inferences can be made, LOGICGUIDE generates the string nothing in\nthe [[infer]] block. When that happens, we observed models spontaneously concluding that the\nanswer is \u201cUnknown\u201d or \u201cCannot be concluded\u201d despite that not being demonstrated in the prompt\n(models abstained in 78% of the cases where they exhausted the inferences that could be made). This\ncontrasts with the unguided models, which most often still make an unjustified guess, writing as if it\nwas a logical conclusion (only unguided GPT-3.5 Turbo ever abstained, in 9% of its predictions).\nDeontiQA\nErrors in LLM reasoning would be especially problematic when an agent must decide\nwhich actions are allowed by its instructions. Hence we created DeontiQA: a set of 60 new reasoning\nproblems inspired by Deontic Logic (Von Wright, 1951). Deontic Logic is concerned with judgements\nof the type \u201caction X is permissible/obligatory\u201d (or not), rather than solely \u201cproposition X is true\u201d\n(e.g., in first-order logic). Peano allows us to easily embed the deontic axioms on top of its type\ntheory. We follow the methodology used in PrOntoQA to create the problems, creating logical forms\n6\nFigure 4: Accuracies of models with and without LOGICGUIDE on the Syllogism Validity task.\nfirst and then realizing them in natural language. Like in PrOntoQA, we add distractor rules to\nprevent guessing the answer from surface shortcuts. In these problems, the goal is to decide whether\na given action is permissible, obligatory, or impermissible in the context of managing calendar\nevents for a group of people. We detail the creation of DeontiQA in the Appendix, and make the\ndataset available along with our code. DeontiQA problems are significantly longer (up to 28 rules)\ncompared to PrOntoQA (maximum of 18). This increased length means we are only able to fit one\nprompt example in the context window of GPT-3 and GPT-3.5 Turbo. We find LOGICGUIDE to be\nhelpful on DeontiQA: GPT-3 alone is correct on 61.6% of problems, which increases to 80.0% with\nLOGICGUIDE. GPT-3.5 Turbo achieves 66.7% accuracy which increases to 78.3% when guided.\nOverall, this provides positive evidence for our first research question: LOGICGUIDE can significantly\nimprove the accuracy of base models in natural language reasoning problems. Their answers become\nnot only more accurate but also more trustworthy: LOGICGUIDE makes models answer \u201cUnknown\u201d\nwhen they don\u2019t have an answer, rather than producing an unsupported guess.\n4.2\nMITIGATING CONTENT EFFECTS IN REASONING\nBoth humans (Evans, 2002) and language models (Dasgupta et al., 2022) have been shown to suffer\nfrom content effects in reasoning: their accuracy in logical judgements is influenced by prior beliefs\nabout the assumptions and conclusions. For instance, from the assumptions that \u201cSome librarians are\nhappy people\u201d and \u201cSome happy people are healthy people\u201d, it does not logically follow that \u201cSome\nlibrarians are healthy people\u201d. Humans and LMs have difficulty judging this argument as invalid\nbecause the conclusion agrees with prior beliefs. We hypothesize that LMs will have smaller influence\nfrom the content when formalizing assumptions, rather than reasoning from logical sentences. If that\nis the case, then using LOGICGUIDE will help mitigate content effects.\nWe use two tasks to investigate this hypothesis. First, we contrast the results in the different\nPrOntoQA ontologies. As in the original PrOntoQA results (Saparov & He, 2022), we see that the\nbase performance of GPT-3 and GPT-3.5 Turbo is already close to ceiling in the True Ontology split\n(where the model doesn\u2019t strictly need to reason correctly as long as it judges the conclusion using\ncommon sense). In contrast, accuracy is significantly lower in the False and Fictitional ontologies\nand decays with more hops. However, both of these models are highly accurate in formalizing\nassumptions, and thus benefit from the guide in the False and Fictitional ontologies: performance is\nnear ceiling. Interestingly, GPT-3.5 Turbo still exhibits occasional content effects, explicitly judging\nthe conclusions derived using LOGICGUIDE as nonsensical in cases where they do follow from the\nproblem. For instance, in one problem where the model must decide whether Sam is luminous or not,\nit is given that \u201cSam is a snake\u201d, and from the given assumptions the model correctly concludes \u201c...\n[[infer:(sheep sam)]] Sam is a sheep\u201d. It then proceeds to question this conclusion and halts: \u201cThis\ncontradicts the fact that Sam is a snake. Therefore, we cannot infer whether Sam is luminous or not.\u201d.\nSecond, we leverage the Syllogism Validity dataset (Dasgupta et al., 2022). In this task, the model is\ngiven two assumptions and a conclusion, and has to decide if together they constitute a valid argument\n(i.e., the conclusion logically follows from the assumptions). The example above about librarians is\ntaken from this dataset. Solutions have a single step: judging the argument as valid or invalid. When\nusing LOGICGUIDE, we prompt the model to first perform a single inference given its formalization\nof the assumptions and then judge the validity of the argument. Syllogism Validity has 3 conditions:\n\u201cNonsense\u201d, where rules are about made-up concepts, \u201cConsistent\u201c, where the conclusions agree with\ncommonsense regardless of whether the argument is valid, and \u201cInconsistent\u201d, where the conclusion\nalways violates world knowledge. Unguided models behave consistently with those in Dasgupta\net al. (2022): in the \u201cConsistent\u201d split, all models strongly tend to judge the argument as being\n7\nvalid, thus performing close to chance (GPT-3.5 Turbo is slightly better, at 60%). Both GPT-3 and\nGPT-3.5 Turbo are, however, highly accurate at formalizing the assumptions and conclusions and\ntend to trust LOGICGUIDE, nearing ceiling performance for all conditions. LLaMA 13B has much\nmore difficulty judging the syllogisms, performing near chance in all conditions. However, it is still\nsuccessful at formalizing many syllogisms, obtaining non-trivial performance (60% to 77%) when\nusing LOGICGUIDE. In failure cases, it often confuses logical connectives (e.g., formalizing \u201cSome\nX are Y\u201d as \u201cX implies Y\u201d and vice-versa). We overall see positive evidence for our second research\nquestion: models with LOGICGUIDE show greatly diminished content effects, with stronger benefits\nfor models that are more capable of interpreting individual sentences (despite struggling to reason\nwith them when unguided).\n4.3\nLEARNING TO REASON BY GUIDED SELF-IMPROVEMENT\nFigure 5: Accuracy of LLaMA 13B on held-out\nPrOntoQA problems when bootstrapping using\nSTaR.\nWe consider improving the reasoning ability of the\nbase language model. When using LOGICGUIDE,\nthe model still produces its rationales, though with\nconstraints (in contrast to approaches that aim to\nfully offload reasoning to an external tool). Thus,\nthis lets us use successful rationales to improve\nthe model itself. This is the essence of the Self-\nTaught Reasoner (STaR; Zelikman et al. (2022)) a\nsimple method for improving LLM reasoning that\nhas been shown to be effective in symbolic, math-\nematical and commonsense reasoning. Given a set\nof problems paired with correct final answers (but\nnot reasoning traces), STaR iterates between (1)\nsolving problems with chain-of-thought prompting,\nand (2) fine-tuning the model on its own generated\nrationales that led to correct final answers. This\nallows the model to improve its reasoning from a small seed set of few-shot examples.\nCrucially, STaR relies on the premise that if a rationale led to the correct answer, it is likely to be\ncorrect. While this holds in domains like arithmetic, it breaks down in most logical reasoning tasks. In\nthese cases, right answers will happen often with bad rationales, leading STaR and similar approaches\nto fine-tune on incorrect reasoning that generalizes poorly. Indeed, the authors in Zelikman et al.\n(2022) remark that \u201cfiltering bad reasoning paired with correct answers remains an open question.\u201d\nWe thus consider STaR training on either all correct answers (with LOGICGUIDE or not) or only\non certified correct answers. We run 2 STaR iterations with LLaMA 13B on PrOntoQA1, where\nwe attempt 200 random problems equally split between 1 and 5 hops, and fine-tune on successful\nsolutions, evaluating on unseen problems.\nFig. 5 shows the results. As predicted in Zelikman et al. (2022), the high chance of guessing\nconfounds STaR, and training on all rationales that yield the right answer does not give meaningful\nimprovements (\u201cUnguided\u201d, red curve). Training on all guided solutions leading to correct answers\nbrings improvements (\u201cGuided\u201d; 72% to 80% after one iteration), but still ends up hitting accidentally-\ncorrect reasoning, when the model decides to make a guess after reasoning for a few steps. Fine-tuning\nonly on certified correct answers avoids this trap and achieves high performance (\u201cStrict Guided\u201d, up\nto 86%). This allows us to positively answer our third research question: LOGICGUIDE can be used\nfor effective self-improvement in reasoning, in cases where na\u00efve methods collapse.\n4.4\nGENERALIZING BEYOND FORMALIZATION\nFinally, we consider whether models bootstrapped on their own reasoning in synthetic multi-step\nreasoning tasks can generalize to settings requiring similar reasoning with real-world language. To\nthat end, we consider ReClor (Yu et al., 2020), a dataset of logical reasoning problems taken from\nstandardized exams (such as LSAT and GMAT, 4-way multiple choice), as well as the 6 tasks in\n1ProofWriter has shortcuts that allow guessing the answer without reasoning (Zhang et al., 2022), which\nfine-tuning quickly learns. PrOntoQA avoids those with distractor rules. Thus, we focus on PrOntoQA here.\n8\nFigure 6: Zero-shot performance on six LEGALBENCH tasks and ReClor, compraing the base gpt-\n3.5-turbo model and as its versions bootstrapped with and without LOGICGUIDE.\nLEGALBENCH (Guha et al., 2023) related to Diversity Jurisdiction (binary choice - given facts about\nplaintiffs, defendants and claims, determine whether the criteria for diversity jurisdiction are met).\nThese questions are challenging even for humans. Although the questions require logical thinking, it\nis often unclear how to formalize them. Thus, directly using LOGICGUIDE with few-shot prompting\nis of limited use, and bootstrapping directly on ReClor unlikely to get off the ground. We thus explore\nwhether models bootstrapped from formalizeable tasks will transfer to ReClor and LEGALBENCH.\nSince these questions are very rich in terms of reading comprehension, we need a strong base\nmodel with non-trivial zero-shot performance. Thus, we use GPT-3.5 Turbo, and leverage the\nOpenAI fine-tuning API. For bootstrapping, we use a random sample of 120 correct solutions from a\nmixture of ProofWriter and PrOntoQA problems with 3+ hops, where the original model either used\nLOGICGUIDE or not. For inference we use zero-shot prompting, where we ask the model to first\ncarefully analyze each of the options before deciding which is correct (prompt in the Appendix). Fig. 6\nshows the results. Bootstrapping on solutions obtained with LOGICGUIDE yields the highest accuracy\non ReClor (65%), compared to bootstrapping without LOGICGUIDE (58.3%) and to the base model\n(52.5%). The overall relative performance between the models is similar on LEGALBENCH tasks.\nWhen inspecting the solutions, we find that the model never explicitly tried to use LOGICGUIDE\n(which we would see with bracket annotations) but the style of reasoning resembles the solutions to\nthe synthetic problems when appropriate (e.g., \u201cReasoning: Liam and Amelia are from different states.\nThe amount-in-controversy is greater than $75k. Answer: A. Yes, there is diversity jurisdiction.\u201d,\nwhereas for this same problem the base model outputs a 132-word long rationale). This style is\nsimilar in both bootstrapped models (we show several complete samples for ReClor in App. G). The\nhigher-quality rationales obtained with LOGICGUIDE seem to have a better overall effect, leading\nto higher accuracy. Overall, we find positive evidence for our last research question: bootstrapping\nmodels with LOGICGUIDE can lead to better performance even when LOGICGUIDE is not available\nat inference time.\n5\nDISCUSSION AND CONCLUSION\nWe introduced guide tools, which locally constrains generation when invoked by a language model.\nLOGICGUIDE leveraged this idea for logical reasoning, allowing the LM to formalize its interpretation\nof input sentences and make sound inferences. Moreover, when bootstrapping models on their own\nreasoning, they can generate better reasoning even when unguided at inference time.\nThe direct application of LOGICGUIDE is challenging for general natural language, which is often\nambiguous and can be difficult to faithfully represent in formal logic. Domains where arguments\ntend to have more systematic logical structure, such as law, are more likely to benefit from tools like\nLOGICGUIDE. Still, our work suggests that bootstrapping on formal problems might help models\ngeneralize. Even then, models can still fail at planning even when making correct deductions. Many\ncurrent investigations into planning techniques for LM reasoning are complementary to our work and\ncan be integrated with guides (Mehran Kazemi et al., 2022; Zhao et al., 2023).\nLanguage models bring to reasoning the flexibility of human language and a wealth of useful prior\nknowledge. But that power comes with lack of reliability and difficulty verifying extended reasoning.\nOur approach points to a rich direction for seamlessly integrating reliable symbolic and flexible\nneural reasoning into a unified text stream. The result is better, and more easily verified, reasoning.\n9\nREFERENCES\nBruno Barras, Samuel Boutin, Cristina Cornes, Judica\u00ebl Courant, Jean-Christophe Filliatre, Eduardo\nGimenez, Hugo Herbelin, Gerard Huet, Cesar Munoz, Chetan Murthy, et al. The Coq proof\nassistant reference manual: Version 6.1. PhD thesis, Inria, 1997.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve\nmath word problems. arXiv preprint arXiv:2110.14168, 2021.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY Chan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. Language models show human-like content effects on\nreasoning. arXiv preprint arXiv:2207.07051, 2022.\nLeonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, and Jakob von Raumer. The\nlean theorem prover (system description). In Automated Deduction-CADE-25: 25th International\nConference on Automated Deduction, Berlin, Germany, August 1-7, 2015, Proceedings 25, pp.\n378\u2013388. Springer, 2015.\nJonathan St BT Evans. Logic and human reasoning: an assessment of the deduction paradigm.\nPsychological bulletin, 128(6):978, 2002.\nNeel Guha, Julian Nyarko, Daniel E Ho, Christopher R\u00e9, Adam Chilton, Aditya Narayana, Alex\nChohlas-Wood, Austin Peters, Brandon Waldon, Daniel N Rockmore, et al. Legalbench: A\ncollaboratively built benchmark for measuring legal reasoning in large language models. arXiv\npreprint arXiv:2308.11462, 2023.\nJoy He-Yueya, Gabriel Poesia, Rose E. Wang, and Noah D. Goodman. Solving math word problems\nby combining language models with symbolic solvers, 2023.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\nlanguage model for task-oriented dialogue. Advances in Neural Information Processing Systems,\n33:20179\u201320191, 2020.\nYinya Huang, Meng Fang, Yu Cao, Liwei Wang, and Xiaodan Liang. DAGN: Discourse-aware graph\nnetwork for logical reasoning. In Proceedings of the 2021 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n5848\u20135855, Online, June 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.naacl-main.467. URL https://aclanthology.org/2021.naacl-main.467.\nXiming Lu, Peter West, Rowan Zellers, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.\nNeuroLogic decoding: (un)supervised neural text generation with predicate logic constraints.\nIn Proceedings of the 2021 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, pp. 4288\u20134299, Online, June 2021.\nAssociation for Computational Linguistics. doi: 10.18653/v1/2021.naacl-main.339. URL https:\n//aclanthology.org/2021.naacl-main.339.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang, Jungo Kasai, Daniel Khashabi, Ronan Le Bras,\nLianhui Qin, Youngjae Yu, Rowan Zellers, et al. Neurologic a* esque decoding: Constrained text\ngeneration with lookahead heuristics. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, pp.\n780\u2013799, 2022.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia, Xin Xu, and Deepak Ramachandran. Lambada:\nBackward chaining for automated reasoning in natural language. arXiv e-prints, pp. arXiv\u20132212,\n2022.\n10\nPasquale Minervini, Sebastian Riedel, Pontus Stenetorp, Edward Grefenstette, and Tim Rockt\u00e4schel.\nLearning reasoning strategies in end-to-end differentiable proving. In ICML 2020, 2020.\nOpenAI. Gpt-4 technical report, 2023.\nGabriel Poesia and Noah D Goodman. Peano: Learning formal mathematical reasoning. arXiv\npreprint arXiv:2211.15864, 2022.\nGabriel Poesia, Alex Polozov, Vu Le, Ashish Tiwari, Gustavo Soares, Christopher Meek, and\nSumit Gulwani. Synchromesh: Reliable code generation from pre-trained language models. In\nInternational Conference on Learning Representations, 2022.\nTim Rockt\u00e4schel and Sebastian Riedel. End-to-end differentiable proving. Advances in neural\ninformation processing systems, 30, 2017.\nAbulhair Saparov and He He. Language models are greedy reasoners: A systematic formal analysis\nof chain-of-thought. arXiv preprint arXiv:2210.01240, 2022.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools. arXiv preprint arXiv:2302.04761, 2023.\nDhruv Shah, B\u0142a\u02d9zej Osi\u00b4nski, Sergey Levine, et al. Lm-nav: Robotic navigation with large pre-trained\nmodels of language, vision, and action. In Conference on Robot Learning, pp. 492\u2013504. PMLR,\n2023.\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks\nand whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. Proofwriter: Generating implications, proofs, and\nabductive statements over natural language. In Findings of the Association for Computational\nLinguistics: ACL-IJCNLP 2021, pp. 3621\u20133634, 2021.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nGeorg Henrik Von Wright. Deontic logic. Mind, 60(237):1\u201315, 1951.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny\nZhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint\narXiv:2201.11903, 2022.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexander M Rush, Bart Van Merri\u00ebnboer, Armand\nJoulin, and Tomas Mikolov. Towards ai-complete question answering: A set of prerequisite toy\ntasks. arXiv preprint arXiv:1502.05698, 2015.\nYuhuai Wu, Albert Qiaochu Jiang, Wenda Li, Markus Rabe, Charles Staats, Mateja Jamnik, and\nChristian Szegedy. Autoformalization with large language models. Advances in Neural Information\nProcessing Systems, 35:32353\u201332368, 2022.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint arXiv:2303.11381, 2023.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng. Reclor: A reading comprehension dataset\nrequiring logical reasoning. arXiv preprint arXiv:2002.04326, 2020.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with\nreasoning. Advances in Neural Information Processing Systems, 35:15476\u201315488, 2022.\n11\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei Chang, and Guy Van den Broeck. On the\nparadox of learning to reason from data. arXiv preprint arXiv:2205.11502, 2022.\nHongyu Zhao, Kangrui Wang, Mo Yu, and Hongyuan Mei. Explicit planning helps language models\nin logical reasoning. arXiv preprint arXiv:2303.15714, 2023.\nA\nOTHER GUIDES\nIn \u00a73.2 we explored LOGICGUIDE, which captured a rich set of operations that both set and leverage\nstate, as well as a complex external logical support tool. Nonetheless, many other guides can be easily\ndefined in this same framework. We survey several such guides here as potential ideas for future\nwork:\nMemory Guide A simple guide in the same format of LOGICGUIDE can be one where the model\ncan set values to keys ([[set:key=value]]), and later on retrieve the value associated\nwith a given key ([[get:key=value]]). When retrieving, the guide can limit the key to\nbe within one of the existing values, and will force the value to be the last stored one. Values\ncan be either overridden or added to a set, depending on the domain. This can effectively\nimplement memory, and this can extend beyond a simple context window provided that\nthe guide keeps external memory. In problems like the bAbI tasks Weston et al. (2015)\nrequiring models to keep track of the state of objects and characters through long stories,\nthis guide can reduce the problem of remembering locations (and avoiding interference in\nself-attention) by the problem of translating each question into a query, using only local\ninformation.\nQuote Guide Language models often hallucinate quotes, e.g. saying that \u201cAccording to Wikipedia,\n\u2019XYZ\u2019. Therefore...\u201d. We can implement a simple quote guide that forces quotes to actually\ncome from a trusted source. Specifically, whenever a prefix like [[quote:]] is generated,\nthe guide can force the subsequent output to be one of the sentences contained in a certain\nweb page (that set is regular). Externally, an UI can even mark guided quotes with their\nsource, which can be kept by the guide.\nAlgebra Guide Mathematical reasoning tools can be also integrated as guides for math problem-\nsolving. Peano itself was originally used with algebra problems, and can thus also serve as a\nguide for mathematical reasoning. We can also leverage other tools, such as computer algebra\nsystems, as guides. One example is the SymPy integration with Codex used previously\nto solve math word problems He-Yueya et al. (2023), where some instructions can add\nvariables, and some can indicate to the solver a variable to be solved for. In the case of\nHe-Yueya et al. (2023), the model simply indicates which variable to solve for, and the\nanswer is externally extracted. When specifying equations in an [[eq]] block, the guide\ncan force the model to output a syntactically valid equation, and also one that only uses\nalready existing variables. This will guarantee that parentheses will be correctly closed (the\ncompletion engines in Poesia et al. (2022) achieve this by deriving a completion engine\nfrom a parser) and that variables are used after they are introduced. If we wanted the\nmodel to also use the results from the solver, we can turn the [[answer]] block into a\n[[solve:x=v]] guided block, where x is constrained to be one of the existing variables,\nand v is given by the algebra solver.\nB\nDEONTIQA\nWe generate the DeontiQA problems following the general methodology of PrOntoQA Saparov &\nHe (2022), where we first sample assumptions and proofs in a logical form and then realize those in\nnatural language. The main qualitative differences are (1) in the specific logical framework we use\nand (2) in how we translate logical sentences into natural language.\nLogical framework\nWe formalize a logical framework for a general domain of managing calendar\nand event invites in Peano. We create a base type for actions, as well as the deontic predicates\npermissible and obligatory, to be applied to actions. We have 5 object types: person, entity,\nreminder, event and invite. From these, we create 14 kinds of actions:\n12\nContext: 1- In a company, there are three employees: Alice, Bob, and Carol. 2- They have a group called\n\"WorkTeam\". 3- They have three upcoming events: a team event, a team-building event, and a product\nlaunch event. 4- Bob is the organizer of the team-building event. 5- Carol is a participant in the product\nlaunch event. 6- The team event is a conference. 7- The team-building event is yearly. 8- The team event is\na monthly event and is a short event, while the product launch event is a long event. 9- Alice is busy during\nthe team event, while Bob is free during the product launch event. 10- The team event is a private event,\nand the product launch event is a public event. 11- If a person is busy during an event, it is impermissible\nto add them as a participant. 12- If a person is free during an event, it is permissible to add them as a\nparticipant. 13- If an event is a long event, it is obligatory to add groups as a participant. 14- If an event has\nhigh priority for a person, then it is not obligatory to set a reminder for a few days before the event for that\nperson. 15- If an event is a conference, it is permissible to update the event to be public. 16- If an event has\nlow priority for a person, it is permissible to cancel the event. 17- If an event is short, it is impermissible\nto reschedule the event yearly. 18- If an event has low priority for a person, then it is obligatory to set\na reminder for a few days before the event for that person. 19- If an event is private, it is obligatory to\nremove Carol as a participant. 20- If a person is a participant in an event, it is permissible to request an\nevent update from them. 21- If a person is the organizer of an event, it is obligatory to change the event\u2019s\nvisibility to confidential. 22- If an event is a monthly event, it is a short event. 23- If an event is a yearly\nevent, it is a long event. 24- If an event is long, then Carol has high priority for that event. 25- If an event\nis a conference, it is a public event. 26- If an event is private, it is a meeting. 27- If an event is public, it is\na social event. 28- If a person is the organizer of an event, they are a participant in that event.\nQuestion: Given the rules above, is it not obligatory for Carol to set a reminder for a few days before the\nteam-building event?\nReasoning: The team-building event is a yearly event. The team-building event is a long event. Carol has\nhigh priority for the team-building event. If an event has high priority for a person, then it is not obligatory\nto set a reminder for a few days before the event for that person. Thus, it is not obligatory for Carol to set a\nreminder for a few days before the yearly team-building event.\nAnswer (yes or no): Yes, it is not obligatory for Carol to set a reminder for a few days before the yearly\nteam-building event.\nFigure 7: Example #1 of a DeontiQA problem.\n\u2022 Given an event invite, the agent can accept, decline or send_notification for\nthat invite.\n\u2022 Given an event, the agent can cancel_event.\n\u2022 Given a reminder specification (constructed with a person and a time period before the\nevent), the agent can set_reminder.\n\u2022 Given\nan\nevent\nand\nan\nentity,\nthe\nagent\ncan\nadd_participant\nor\nremove_participant.\n\u2022 Given an event and a person, the agent can delegate_event.\n\u2022 For\nan\nevent,\na\nperson\nmight\nrequest_event_update,\nsuggest_alternative_time, or check_availability\n\u2022 Given an event and a proper event property to be changed, the agent can update_event,\nreschedule_event, or change_visibility, with the property describing the\nproper update.\nProblem generation\nTo generate a problem, we first sample a theory \u2014 a set of hypotheses \u2014, and\nusing those assumptions we try to construct a random derivation (applying axioms and assumptions\nto hypotheses or previous conclusions). The conclusion of the derivation (or its negation, 50% of the\ntime) becomes the goal in the generated problem.\nTranslation to natural language\nTo realize the problem in natural language, we use the aid of\nGPT-4 OpenAI (2023), prompted to translate the logical statements into a story in natural language\ngiven a few examples, with sentences describing the axioms. All stories were manually checked to\nstill be unambiguous and logically sound, and we only use GPT-4 to help with diversity. As a result,\nthe DeontiQA problems show greater linguistic variation than both PrOntoQA and ProofWriter,\nespecially at their beginning. We show 3 example problems in Fig. 7, Fig. 8 and Fig. 9. The full set\nof 60 problems is released along with the supplementary material.\n13\nContext: 1- In a marketing agency, there are three employees: Alice, Bob, and Carol. 2- Alice is the\nmarketing manager, Bob is a graphic designer, and Carol is a content writer. 3- The company has a remote\nteam called \"CreativeTeam\". 4- They have three upcoming events: a brainstorming session, a company\nparty, and a progress event. 5- Alice is the organizer of the brainstorming session, Carol is a participant in\nthe company party, and the CreativeTeam is a group participant in the progress event. 6- The brainstorming\nsession is short, while the company party is a long event. 7- The brainstorming session is a meeting, and\nthe company party is a social event. 8- If an event has high priority for a person, it is permissible for them\nto suggest an alternative time. 9- If a person is the organizer of an event, it is obligatory to add them as a\nparticipant. 10- If a person is free during an event, it is permissible for them to accept an individual invite\nto the event. 11- If a group is a participant in an event, it is permissible to delegate the event to the group.\n12- If a person is busy during an event, it is impermissible to set a reminder for a few minutes before the\nevent. 13- If a person is a participant in an event, it is permissible to remove them as a participant. 14-\nFor short events, it is permissible to update the event to a social event. 15- If a person\u2019s status is tentative\nfor an event, it is obligatory to check the availability of the person for that event. 16- If an event has high\npriority for a person, it is obligatory to set a reminder for a few days before the event. 17- If a person is a\nparticipant in an event, it is impermissible for them to suggest an alternative time. 18- If an event is public,\nit is obligatory to add Alice as a participant. 19- Meetings are public events. 20- Public events are short\nevents. 21- If a person is the organizer of an event, their priority for that event is high. 22- If a person is a\nparticipant in an event, they are available for that event. 23- If an event is short, Bob is a participant. 24-\nDaily events are short events. 25- If a person has a high priority for an event, they are busy during that\nevent. 26- If a person has a low priority for an event, they are free during that event. 27- If a group is a\nparticipant in an event, the event is public. Question: Given the rules above, is it permissible for Bob to\nsuggest an alternative time for the progress event? Reasoning: The CreativeTeam is a group participant\nin the progress event. The progress event is a public event. The progress event is a short event. Bob is a\nparticipant in the progress event. It is impermissible for a participant to suggest an alternative time for an\nevent they are participating in. Answer (Yes or no): No\nFigure 8: Example #2 of a DeontiQA problem.\nContext: 1- In a software company, there are three project managers: Alice, Bob, and Carol. 2- They have a\nteam called \"DevTeam\". 3- They have two upcoming events: a team-building activity and a project review\nevent. 4- Alice is the organizer of the team-building activity, Bob is a participant in the team-building\nactivity, and the entire DevTeam is participating in the team-building activity. 5- The project review is a\nshort event. 6- The team-building activity is a social event. 7- The team-building activity is a public event.\n8- If a person is the organizer of an event, it is obligatory to add them as a participant. 9- If a person is a\nparticipant in an event, it is permissible for them to accept an individual invite to the event. 10- If an event\nis short, it is impermissible to cancel the event. 11- If a group is participating in an event, it is obligatory to\nadd the group as a participant. 12- If a person is free during an event, it is permissible to set a reminder for\na few hours before the event. 13- If a person is busy during an event, it is impermissible to reschedule the\nevent to be a daily event. 14- If an event is public, it is obligatory to check Alice\u2019s availability for the event.\n15- If a person is a participant in an event, it is permissible to delegate the event organization to them. 16-\nIf a person\u2019s availability for an event is tentative, it is permissible for them to suggest an alternative time\nfor the event. 17- If an event has high priority for a person, then it is obligatory to set a reminder for them\nfor a few days before the event. 18- If a person\u2019s availability for an event is free, it is impermissible for\nthem to suggest an alternative time for the event. 19- If an event is short, then it is a meeting. 20- If an\nevent is a meeting, then Bob\u2019s availability for the event is tentative. 21- If Alice\u2019s availability for an event\nis tentative, then she is a participant in the event. 22- If a person is free for an event, then the event has low\npriority for them. 23- If an event is public, then it is a social event. 24- If an event is private, then it is a\npersonal event. 25- If an event is daily, then it is not a weekly event. 26- If an event is weekly, then it is\nnot a monthly event. Question: Given the rules above, is it permissible for Bob to suggest an alternative\ntime for the project review? Reasoning: The project review is a meeting. Bob\u2019s availability for the project\nreview is tentative. It is permissible for a person with tentative availability to suggest an alternative time\nfor the event. Therefore, it is permissible for Bob to suggest an alternative time for the project review.\nAnswer (Yes or no): Yes, it is permissible for Bob to suggest an alternative time for the project review.\nFigure 9: Example #3 of a DeontiQA problem.\n14\nC\nCONSTRAINED SEMANTIC DECODING WITH CHAT MODELS\nOriginally, Constrained Semantic Decoding was proposed to work with standard autoregressive\nlanguage models Poesia et al. (2022). This relied on the ability to bias the logits of the model during\ngeneration, which is both possible in local models as well as through the OpenAI API2. The OpenAI\ngpt3.5-turbo has a different API, since it is a chat-based model. In this API, we pass a list of\nmessages with roles (user or assistant, where the model understands the latter as marking\nits own past generations). The API also has a logit bias parameter. However, we unfortunately\ncannot pass an incomplete prefix for the model\u2019s response. Thus, we are unable to force the model to\ncomplete a certain message while also applying logit biases. Every completion starts a new message.\nThis requires an adaptation to the procedure in Poesia et al. (2022).\nWe start with the usual rejection-based CSD procedure: we put few-shot examples in the previous\nmessages showcasing the response format we want, and sample a full response from the model. We\nthen use token-by-token CSD to validate the response. If this terminates without finding any violation,\nwe\u2019re done \u2014 the entire generation, including choices made in guided blocks (e.g., infer), were\nvalid. If not, like in original CSD, we take the largest valid prefix and use the CSD algorithm to\ncompute the set of tokens that are valid after that prefix.\nHere we reach the main difference in the API. We want the model to continue its message from the\nlast valid token. However, this is not possible in the current API. Instead, we must request a new\nmessage. Fortunately, we found gpt3.5-turbo to very frequently simply continue its generation\nwhen its last message appears incomplete3. We exploit this behavior and (1) request a new message\nwith a single token, passing the set of valid tokens in the logit bias, (2) append the sampled token\nto the previous message and request a new, unconstrained message, and (3) repeat until we have\nreceived a full response.\nWhen the model continues from where it stops, this approach is essentially equivalent to rejection-\nbased CSD. Unfortunately, it is not fully reliable. In a number of cases, we found the model to\ninsistently apologize after noticing that its previous message was incomplete. This is problematic\nwhen the previous message started a guided block that is to be finished in the next message. In this\ncase, the model\u2019s apology is contained in the guided block, and is thus always an invalid generation.\nWhat happens is that this immediately triggers a violation, and the CSD procedure above is executed.\nOften, the CSD corrections will eventually get the model to make enough choices to complete the\nguided block, at which point its apology is not an issue anymore (e.g., see Fig. 10. In rare (< 0.1%)\ncases, the issue persists and we cannot recover from the apology (Fig. 11 shows an example). To\navoid a prohibitive number of API calls, we aborted sampling when more than 20 violations were hit\nin the same solution.\nD\nEXPERIMENTAL DETAILS\nExperiments with the OpenAI models were made using their public API. For LLaMA 13B, we ran\nand fine-tuned the model on an NVIDIA A100 80GB GPU. For fine-tuning when running STaR\n(\u00a74.3), we performed inference on 200 problems \u2014 40 for each number of hops from 1 to 5 \u2014 in\neach STaR iteration, and collected the generations where the model reached the correct answer (with\neach of the 3 criteria described in \u00a74.3). We fine-tuned for 1 epoch (i.e., seeing each example exactly\nonce) with a batch size of 2 and a learning rate of 2e-5. We used the Adam8bit optimizer with default\nparameters, reset in each iteration of STaR.\nE\nPROMPTS\nAll of our prompts are provided in the attached supplementary material. We use JSON files for the\nchat model prompts, exactly as we pass them to the OpenAI API.\n2https://platform.openai.com/docs/api-reference/completions/create#\ncompletions/create-logit_bias\n3We hypothesize this is done so that models also complete their messages when the token limit is hit in the\nOpenAI Playground and users immediately request a new completion\n15\nFormalized context: 1- Every [[prop:dumpus]] is an [[prop:impus]]. [[axiom:(dumpus \u2019x) -> (impus\n\u2019x)]]. 2- Each [[prop:vumpus]] is not [[prop:luminous]]. [[axiom:(vumpus \u2019x) -> (not (luminous \u2019x))]]. 3-\n[[prop:dumpus]] are [[prop:orange]]. [[axiom:(dumpus \u2019x) -> (orange \u2019x)]]. 4- Every [[prop:wumpus]]\nis [[prop:bitter]]. [[axiom:(wumpus \u2019x) -> (bitter \u2019x)]]. 5- Each [[prop:jompus]] is not [[prop:orange]].\n[[axiom:(jompus \u2019x) -> (not (orange \u2019x))]]. 6- [[prop:wumpus]] are [[prop:numpus]]. [[axiom:(wumpus\n\u2019x) -> (numpus \u2019x)]]. 7- [[prop:impus]] are [[prop:rompus]]. [[axiom:(impus \u2019x) -> (rompus \u2019x)]]. 8-\nEvery [[prop:impus]] is [[prop:opaque]]. [[axiom:(impus \u2019x) -> (opaque \u2019x)]]. 9- [[prop:numpus]] are\n[[prop:dull]]. [[axiom:(numpus \u2019x) -> (dull \u2019x)]]. 10- [[prop:vumpus]] are [[prop:wumpus]]. [[ax-\niom:(vumpus \u2019x) -> (wumpus \u2019x)]]. 11- Each [[prop:numpus]] is a [[prop:dumpus]]. [[axiom:(numpus \u2019x)\n-> (dumpus \u2019x)]]. 12- [[object:wren]] is a [[prop:dumpus]]. [[axiom:(dumpus wren)]]. Formalized goal:\n[[goal:(orange wren)]]\nReasoning: [[infer:(impus wren)]] Wren is an impus. [[infer:(rompus wren)]]\u2019Apologies for the incomplete\nmessage.\nContinuing from where I stopped: [[infer:(opaque wren)]]\"My apologies for the incomplete message.\nContinuing from where I stopped: [[infer:(orange wren)]]] Wren is orange. This was the goal.\nAnswer: True\nFigure 10: Example generation of gpt3.5-turbo with LOGICGUIDE on PrOntoQA where the\nmodel started to apologize twice after violating the guide, but it eventually recovered with our chat\nadaptation of CSD.\nFormalized context: 1- [[object:bob]] is [[prop:blue]].\n[[axiom:(blue bob)]].\n2- [[object:bob]] is\n[[prop:quiet]]. [[axiom:(quiet bob)]]. 3- [[object:bob]] is [[prop:red]]. [[axiom:(red bob)]]. 4- [[ob-\nject:bob]] is [[prop:rough]]. [[axiom:(rough bob)]]. 5- [[object:bob]] is [[prop:smart]]. [[axiom:(smart\nbob)]]. 6- [[object:charlie]] is [[prop:cold]]. [[axiom:(cold charlie)]]. 7- [[object:charlie]] is [[prop:red]].\n[[axiom:(red charlie)]]. 8- [[object:dave]] is [[prop:blue]]. [[axiom:(blue dave)]]. 9- [[object:dave]] is\n[[prop:quiet]]. [[axiom:(quiet dave)]]. 10- [[object:erin]] is [[prop:cold]]. [[axiom:(cold erin)]]. 11-\n[[object:erin]] is [[prop:white]]. [[axiom:(white erin)]]. 12- If something is [[prop:smart]] then it is\n[[prop:white]]. [[axiom:(smart \u2019x) -> (white \u2019x)]]. 13- All [[prop:quiet]], [[prop:rough]] things are\n[[prop:smart]]. [[axiom:(ifsomething isPSSorryPSI apologizePSI apologizePSI apologizePSI apologizePSI\napologizePSI apologizePSI apologizePSI apologizePSI apologizePSI apologizePSI apologizePSI apolo-\ngizePSFormalized contextPSI apologize for the previous messagePSIPSIPSI apologize for the previous\nmessagePSI\nFigure 11: Example generation of gpt3.5-turbo with LOGICGUIDE on ProofWriter where the\nmodel started to apologize after violating the guide, and did not recover. Here, it violated the s-expr\nsyntax for defining axioms in Peano, but after being constrained it still insists in apologizing, instead\nof trying to finish the formula. This behavior is rare (< 0.1% of the cases), and would be avoided if\nthe API had a parameter to give the model an incomplete message for it to complete.\n16\nContext: Every dog is small. Every feline is a snake. Every animal is not bitter. Sheep are bitter. Cats are\ncarnivores. Each vertebrate is a mammal. Mammals are felines. Each vertebrate is dull. Snakes are cats.\nCats are not kind. Every snake is not happy. Every sheep is a vertebrate. Each feline is cold. Each dog is a\nsheep. Every mammal is not liquid. Every carnivore is a cow. Every carnivore is brown. Alex is a sheep.\nQuestion: True or false: Alex is not bitter.\nFormalized context: 1- Every [[prop:dog]] is [[prop:small]]. [[axiom:(dog \u2019x) -> (small \u2019x)]]. 2- Ev-\nery [[prop:feline]] is a [[prop:snake]]. [[axiom:(feline \u2019x) -> (snake \u2019x)]]. 3- Every [[prop:animal]]\nis not [[prop:bitter]]. [[axiom:(animal \u2019x) -> (not (bitter \u2019x))]]. 4- [[prop:sheep]] are [[prop:bitter]].\n[[axiom:(sheep \u2019x) -> (bitter \u2019x)]]. 5- [[prop:cat]] are [[prop:carnivore]]. [[axiom:(cat \u2019x) -> (carnivore\n\u2019x)]]. 6- Each [[prop:vertebrate]] is a [[prop:mammal]]. [[axiom:(vertebrate \u2019x) -> (mammal \u2019x)]]. 7-\n[[prop:mammal]] are [[prop:feline]]. [[axiom:(mammal \u2019x) -> (feline \u2019x)]]. 8- Each [[prop:vertebrate]]\nis [[prop:dull]]. [[axiom:(vertebrate \u2019x) -> (dull \u2019x)]]. 9- [[prop:snake]] are [[prop:cat]]. [[axiom:(snake\n\u2019x) -> (cat \u2019x)]]. 10- [[prop:cat]] are not [[prop:kind]]. [[axiom:(cat \u2019x) -> (not (kind \u2019x))]]. 11- Every\n[[prop:snake]] is not [[prop:happy]]. [[axiom:(snake \u2019x) -> (not (happy \u2019x))]]. 12- Every [[prop:sheep]] is\na [[prop:vertebrate]]. [[axiom:(sheep \u2019x) -> (vertebrate \u2019x)]]. 13- Each [[prop:feline]] is [[prop:cold]]. [[ax-\niom:(feline \u2019x) -> (cold \u2019x)]]. 14- Each [[prop:dog]] is a [[prop:sheep]]. [[axiom:(dog \u2019x) -> (sheep \u2019x)]].\n15- Every [[prop:mammal]] is not [[prop:liquid]]. [[axiom:(mammal \u2019x) -> (not (liquid \u2019x))]]. 16- Every\n[[prop:carnivore]] is a [[prop:cow]]. [[axiom:(carnivore \u2019x) -> (cow \u2019x)]]. 17- Every [[prop:carnivore]]\nis [[prop:brown]]. [[axiom:(carnivore \u2019x) -> (brown \u2019x)]]. 18- [[object:alex]] is a [[prop:sheep]]. [[ax-\niom:(sheep alex)]].\nFormalized goal: [[goal:(not (bitter alex))]]\nReasoning: [[infer:(vertebrate alex)]] Alex is a vertebrate. [[infer:(dull alex)]] Alex is dull. [[infer:(mammal\nalex)]] Alex is a mammal. [[infer:(feline alex)]] Alex is a feline. [[infer:(not (liquid alex))]] Alex is not\nliquid. [[infer:(snake alex)]] Alex is a snake. [[infer:(cat alex)]] Alex is a cat. [[infer:(carnivore alex)]]\nAlex is a carnivore. [[infer:(not (kind alex))]] Alex is not kind. [[infer:(not (happy alex))]] Alex is not\nhappy. [[infer:(cold alex)]] Alex is cold. [[infer:(brown alex)]] Alex is brown. [[infer:(cow alex)]] Alex is\na cow. [[infer:(bitter alex)]] Alex is bitter. This contradicts the goal.\nAnswer: False.\nFigure 12: Example generation of text-davinci-003 with LOGICGUIDE on the PrOntoQA\nFalse Ontology. The solution to this problem can be reasoned in one step, but the model is strongly\nbiased towards making wrong conclusions. LOGICGUIDE avoids those, but the model still takes\nmany more inference steps to eventually reach the goal, deriving many useless facts first.\nF\nCOMPLETE SAMPLES - PRONTOQA/PROOFWRITER\nWe here provide full samples of solutions generated by language models with LOGICGUIDE, also\nshowcasing the most common failure modes.\nFig. 12 shows one case of text-davinci-003 on the PrOntoQA False Ontology, where the\nmodel properly formalizes all of the assumptions, but still tries to make wrong conclusions very often.\nAs a result, its solution ends up taking a long detour to eventually get to the goal, but eventually does\nso correctly (it can be concluded directly from two of the assumptions).\nFig. 14 shows one example of gpt3.5-turbo on ProofWriter, where the model further justifies its\nsolution based on the axioms. We found these post-hoc justifications to be highly accurate. Unguided\nmodels sometimes also justify their inferences even if not prompted to do so, but to do so they must\nprocuce hallucinations (or assume general world knowledge, such that \u201can animal cannot chase\nitself\u201d).\nFig. 13 shows one rare failure mode where the model misclassifies whether it has already proved the\ngoal, and thus does not proceed further. We can detect this failure mode with LOGICGUIDE, since we\nhave access to the Peano state and can ask the environment whether the goal was proved or not. In\nthis way, as explained in App. D, we can distinguish certified and uncertified answers.\nFig. 15 shows a case where LLaMA 13B misformalized (several) assumptions, whereas Fig. 16 shows\na similar case with text-davinci-003 (much more rare). The result in both cases is that the\nmodel cannot make progress in its formal inferences, instead of making invalid deductions. Again,\nsince we can detect when the answer was not formally derived, we can avoid fine-tuning on these\ncases where the model still guesses the right answer but with unsond reasoning, as we exploited in\n\u00a74.3.\n17\nFormalized context: 1- The [[object:cat]] is [[prop:red]]. [[axiom:(red cat)]]. 2- The [[object:cat]]\n[[relation:visits]] the [[object:cow]]. [[axiom:(visits cat cow)]]. 3- The [[object:cow]] is [[prop:big]].\n[[axiom:(big cow)]]. 4- The [[object:cow]] [[relation:needs]] the [[object:dog]]. [[axiom:(needs cow dog)]].\n5- The [[object:cow]] [[relation:needs]] the [[object:squirrel]]. [[axiom:(needs cow squirrel)]]. 6- The\n[[object:dog]] does not [[relation:need]] the [[object:cat]]. [[axiom:(not (needs dog cat))]]. 7- The [[ob-\nject:dog]] [[relation:visits]] the [[object:cow]]. [[axiom:(visits dog cow)]]. 8- The [[object:squirrel]] [[rela-\ntion:chases]] the [[object:cow]]. [[axiom:(chases squirrel cow)]]. 9- The [[object:squirrel]] is [[prop:nice]].\n[[axiom:(nice squirrel)]]. 10- The [[object:squirrel]] [[relation:needs]] the [[object:dog]]. [[axiom:(needs\nsquirrel dog)]]. 11- If someone [[relation:needs]] the [[object:squirrel]] then they [[relation:chase]] the\n[[object:cat]]. [[axiom:(needs \u2019x squirrel) -> (chases \u2019x cat)]]. 12- If someone [[relation:chases]] the\n[[object:cat]] then the [[object:cat]] [[relation:visits]] the [[object:cow]]. [[axiom:(chases \u2019x cat) -> (visits\ncat cow)]]. 13- If someone [[relation:chases]] the [[object:cat]] then they are [[prop:nice]]. [[axiom:(chases\n\u2019x cat) -> (nice \u2019x)]]. 14- If someone [[relation:chases]] the [[object:squirrel]] then the [[object:squirrel]]\n[[relation:chases]] the [[object:cow]]. [[axiom:(chases \u2019x squirrel) -> (chases squirrel cow)]]. 15- If\nsomeone [[relation:chases]] the [[object:cow]] and they [[relation:needs]] the [[object:squirrel]] then the\n[[object:cow]] [[relation:needs]] the [[object:squirrel]]. [[axiom:(chases \u2019x cow) -> (needs cow squirrel) ->\n(needs cow squirrel)]]. 16- If someone is [[prop:nice]] then they do not [[relation:need]] the [[object:cat]].\n[[axiom:(nice \u2019x) -> (not (needs \u2019x cat))]]. 17- If someone [[relation:needs]] the [[object:dog]] and they are\n[[prop:red]] then they [[relation:chase]] the [[object:dog]]. [[axiom:(needs \u2019x dog) -> (red \u2019x) -> (chases\n\u2019x dog)]]. 18- If someone is [[prop:nice]] and not [[prop:green]] then they do not [[relation:visit]] the\n[[object:cat]]. [[axiom:(nice \u2019x) -> (not (green \u2019x)) -> (not (visits \u2019x cat))]].\nFormalized goal: [[goal:(not (needs cow cat))]]\nReasoning: [[infer:(chases cow cat)]] The cow does not chase the cat. [[infer:(not (needs squirrel cat))]]]\nThe cow does not need the cat. This was the goal.\nAnswer: True\nFigure 13: Example generation of gpt3.5-turbo with LOGICGUIDE on ProofWriter where the\nmodel misidentifies whether it has reached the goal or not. Given the Peano state and the formalized\ngoal, we can at any point detect whether the model has indeed proved its goal or not, i.e. whether its\nanswer was certified or not. This can likely be avoided by making the step of outputting a conclusion\nbe prefixed with a formal step, which can be constrained to only be allowed once the goal has been\nproved or disproved.\n(...) Formalized goal: [[goal:(quiet gary)]]\nReasoning: [[infer:(young gary)]] Gary is young because he is round (from axiom 9). [[infer:(white gary)]]\nGary is white because he is round and all round people are white (from axiom 5). [[infer:(not (quiet\ngary))]] Gary is not quiet because he is young and white, and all young, white people are not quiet (from\naxiom 7). Therefore, the answer is False.\nFigure 14: Example generation of gpt3.5-turbo with LOGICGUIDE on ProofWriter, where the\nmodel provides further justification on the inferences. We found that models are highly accurate in\njustifying the inferences post-hoc, provided that they are correct.\n18\nFormalized context: 1- The [[object:lion]] chases the [[object:squirrel]]. [[axiom:(chases lion squirrel)]].\n2- The [[object:lion]] is [[prop:big]]. [[axiom:(big lion)]]. 3- The [[object:lion]] is [[prop:blue]]. [[ax-\niom:(blue lion)]]. 4- The [[object:lion]] is [[prop:kind]]. [[axiom:(kind lion)]]. 5- The [[object:lion]] is\n[[prop:nice]]. [[axiom:(nice lion)]]. 6- The [[object:lion]] is [[prop:young]]. [[axiom:(young lion)]]. 7-\nThe [[object:lion]] needs the [[object:squirrel]]. [[axiom:(needs lion squirrel)]]. 8- The [[object:lion]] visits\nthe [[object:squirrel]]. [[axiom:(visits lion squirrel)]]. 9- The [[object:squirrel]] chases the [[object:lion]].\n[[axiom:(chases squirrel lion)]]. 10- The [[object:squirrel]] is [[prop:big]]. [[axiom:(big squirrel)]]. 11- The\n[[object:squirrel]] is [[prop:blue]]. [[axiom:(blue squirrel)]]. 12- The [[object:squirrel]] is [[prop:kind]].\n[[axiom:(kind squirrel)]]. 13- The [[object:squirrel]] is [[prop:nice]]. [[axiom:(nice squirrel)]]. 14- The\n[[object:squirrel]] is [[prop:young]]. [[axiom:(young squirrel)]]. 15- The [[object:squirrel]] needs the\n[[object:lion]]. [[axiom:(needs squirrel lion)]]. 16- The [[object:squirrel]] visits the [[object:lion]]. [[ax-\niom:(visits squirrel lion)]]. 17- If someone visits the [[object:squirrel]] then they chase the [[object:lion]].\n[[axiom:(visits squirrel lion) -> (chases lion)]]\nFormalized goal: [[goal:(chases lion lion)]]\nReasoning: [[infer:nothing]] The lion does not chase the lion. This contradicts the goal.\nAnswer: False\nFigure 15: Example generation of LLaMA 13B with LOGICGUIDE on ProofWriter, where the model\nmisformalizes assumption 17. The natural language sentence reads \u201cif someone visits the squirrel,\nthen they chase the lion\u201d, but the logical formulation written by the model would translate to \u201cif the\nsquirrel visits the lion, then the lion chases\u201d. As a result, the model is not able to make any formal\ninferences. It still outputs an uncertified answer, which we can detect.\n(...) 13- If something [[relation:visits]] the [[object:squirrel]] and the [[object:squirrel]] [[relation:needs]]\nthe [[object:bald_eagle]] then the [[object:squirrel]] is [[prop:green]]. [[axiom:(visits \u2019x squirrel) -> (needs\nsquirrel bald_eagle) -> (green \u2019x)]]. 14- If something [[relation:needs]] the [[object:squirrel]] then it is\n[[prop:green]]. [[axiom:(needs \u2019x squirrel) -> (green \u2019x)]].\nFormalized goal: [[goal:(green squirrel)]]\nReasoning: [[infer:(green bald_eagle)]] The bald eagle is green. [[infer:(green cow)]] The cow is green.\n[[infer:nothing]] Nothing can be inferred from the context about the squirrel. This contradicts the goal.\nAnswer: False.\nFigure 16: Example generation of text-davinci-003 with LOGICGUIDE on ProofWriter, where\nthe model misformalizes assumption 13. The natural language sentence reads \u201cif someone visits the\nsquirrel and the squirrel needs the bald eagle, then the squirrel is green\u201d, but the logical formulation\nwritten by the model would translate to \u201cif someone visits the squirrel and the squirrel needs the bald\neagle, then they are green\u201d (not the squirrel is green). Like in Fig. 15, the model is not able to make\nany formal inferences after its second step, and we can detect that its answer was not certified.\n19\nPatient: Pharmacists maintain that doctors should not be permitted to sell the medicine that they prescribe\nbecause doctors would then be tempted to prescribe unnecessary medicines in order to earn extra income.\nBut pharmacists have a financial interest in having a monopoly on the sale of prescription medicines,\nso their objection to the sale of medicines by doctors cannot be taken seriously. The patient\u2019s argument\nproceeds by\nA. attempting to discredit a position by questioning the motives of the proponents of that position\nB. rejecting a questionable position on the grounds that the general public does not support that position\nC. pointing out an unstated assumption on which the pharmacists\u2019 argument relies and then refuting it\nD. asserting that pharmacists lack the appropriate knowledge to have informed opinions on the subject\nunder discussion\nAnswer: A\n\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2013\ngpt-3.5-turbo: In this problem, the patient presents an argument against the objection raised by pharma-\ncists regarding doctors selling medicines. The patient claims that the objection cannot be taken seriously\nbecause pharmacists have a financial interest in maintaining a monopoly on the sale of prescription\nmedicines.\nTo determine how the patient\u2019s argument proceeds, let\u2019s examine each option:\nA. The patient\u2019s argument does question the motives of the pharmacists by suggesting that they have a\nfinancial interest in the matter. However, it does not discredit the entire position of the pharmacists based\nsolely on their motives. Therefore, option A is not the correct choice.\nB. The patient\u2019s argument does not rely on the fact that the general public does not support the pharmacists\u2019\nposition. Instead, it focuses on the financial interest of the pharmacists. Therefore, option B is not the\ncorrect choice.\nC. The patient\u2019s argument does not explicitly point out an unstated assumption of the pharmacists\u2019 argument\nand then refute it. Instead, it directly challenges the motives of the pharmacists by highlighting their\nfinancial interest. Therefore, option C is not the correct choice.\nD. The patient\u2019s argument does not discuss the knowledge of pharmacists or their opinions on the subject\nunder discussion. Therefore, option D is not the correct choice.\nAfter considering all the options, we can conclude that none of the given options accurately describes how\nthe patient\u2019s argument proceeds.\nAnswer: None of the options.\ngpt-3.5-turbo + Base Bootstrap: Reasoning: The patient argues that the objection raised by pharmacists\nis not valid because they have a financial interest in maintaining a monopoly on the sale of prescription\nmedicines. Answer: A. attempting to discredit a position by questioning the motives of the proponents of\nthat position\ngpt-3.5-turbo + LogicGuide Bootstrap: Reasoning: The patient argues that the objection of pharmacists\nto doctors selling medicine cannot be taken seriously because pharmacists have a financial interest in\nmaintaining a monopoly on the sale of prescription medicines. Answer: A. attempting to discredit a\nposition by questioning the motives of the proponents of that position\nFigure 17: Example #1 of a ReClor problem. Here, the base model does a very long analysis\nthat ultimately discards option A, by questioning whether it \u201cdiscredited the entire position of the\npharmacist\u201d, which is irrelevant to the question. Bootstrapped models recognize the form of the\nargument as sufficiently matching option A.\nG\nCOMPLETE SAMPLES - RECLOR\nWe here show three full examples of questions and answers from the ReClor dataset, highlighting the\ndifferences in the base GPT-3.5 Turbo model and both of its bootstrapped versions. The zero-shot\nprompt we used was: \u201cAnswer the problem below. First, carefully analyze the context and each of the\noptions before deciding which is correct. Then, at the end of your response, end with a line with the\nformat \"Answer: X\", indicating one of the options.\u201d. We tested several variations of these instructions,\npicking the one where the base model obeyed the expected format the best.\n20\nTraveler: Southern Airways has a far worse safety record than Air Dacentaria over the past few years, in\nterms of both the frequency of accidents and the severity of accidents. Ttherefore, although Air Dacentaria\nis slightly more expensive, I will choose it over Southern Airways for my flight from Pederton to Dacenta,\nsince it is worth paying extra to have a safer flight.\nWhich of the following, if true, casts the most serious doubt on the traveler\u2019s argument?\nA. Although the frequency of accidents is greater on Southern Airways, on both airlines the proportion of\nflights that have accidents is very small.\nB. Most Southern Airways flights, but not its flight from Pederton to Dacenta, use Sarmouth airport, which\nbecause of its mountainous location is one of the world\u2019s most dangerous.\nC. Only in the last few years has the cost of flying from Pederton to Dacenta been more expensive on Air\nDacentaria than on Southern Airways.\nD. Air Dacentaria\u2019s flight from Pederton to Dacenta is nonstop, whereas Southern Airways\u2019 flight touches\ndown in Gorinda on the way.\nAnswer: B\ngpt-3.5-turbo Let\u2019s analyze each of the options:\nOption A states that although the frequency of accidents is greater on Southern Airways, the proportion of\nflights that have accidents is very small on both airlines. If this is true, it diminishes the significance of the\ndifference in safety records between the two airlines, as the overall likelihood of being in an accident is\nsmall for both.\nOption B claims that most Southern Airways flights, except for the one from Pederton to Dacenta, use\nSarmouth airport, which is considered one of the world\u2019s most dangerous due to its mountainous location.\nThis information may suggest that the higher accident frequency of Southern Airways might be due to its\nuse of a particularly hazardous airport. However, since the traveler is specifically referring to the flight\nfrom Pederton to Dacenta, this option may not significantly impact their argument.\nOption C states that only in the last few years has Air Dacentaria become more expensive than Southern\nAirways for flying from Pederton to Dacenta. This suggests that the increased cost of Air Dacentaria might\nbe a recent change, and the traveler\u2019s decision may have been based on outdated information or prior\npricing structures.\nOption D mentions that Air Dacentaria\u2019s flight from Pederton to Dacenta is nonstop, while Southern\nAirways\u2019 flight makes a stop in Gorinda. If this is true, it may imply that the nonstop flight on Air\nDacentaria would reduce the chances of potential accidents compared to the Southern Airways flight that\ninvolves a touch-down in Gorinda.\nAfter analyzing the options, it appears that option D, which states that Air Dacentaria\u2019s flight from Pederton\nto Dacenta is nonstop, while Southern Airways\u2019 flight touches down in Gorinda, casts the most serious\ndoubt on the traveler\u2019s argument. This information suggests that the nonstop flight on Air Dacentaria could\npotentially offer a safer flight experience even with the additional cost.\nAnswer: D\ngpt-3.5-turbo + Base Bootstrap Reasoning: The traveler prioritizes safety over cost. Air Dacentaria\nis slightly more expensive but safer. A. This does not cast doubt on the traveler\u2019s argument, as it still\nsupports the idea that Air Dacentaria is safer. B. This is irrelevant to the safety of Air Dacentaria and\nSouthern Airways. C. This does not cast doubt on the traveler\u2019s argument, as it still supports the idea of\nchoosing Air Dacentaria for safety reasons. D. This supports the traveler\u2019s argument, as it highlights that\nAir Dacentaria\u2019s flight is more convenient and potentially safer. Answer: D\ngpt-3.5-turbo + LogicGuide Bootstrap Reasoning: The traveler\u2019s argument is based on the safety record\nof the airlines. To cast doubt on the argument, we need to find information that undermines the relevance\nof the safety record. A. This information does not undermine the traveler\u2019s argument, as the proportion of\naccidents is small on both airlines. B. This information weakens the traveler\u2019s argument, as the specific\nflight from Pederton to Dacenta is not affected by the dangerous airport. C. This information does not\nundermine the traveler\u2019s argument, as the cost comparison is only relevant to the decision-making process.\nD. This information does not undermine the traveler\u2019s argument, as nonstop flights are generally considered\nsafer. Answer: B\nFigure 18: Example #2 of a ReClor problem. Here, both the base model and the base bootstrapped\nmodel ignore the key connection between option B and the traveler\u2019s argument, which the LogicGuide-\nboostrapped model picks up on..\n21\nDario: The government should continue to grant patents for all new drug compounds. Patents promote\ninnovation by rewarding pharmaceutical companies for undertaking the costly research needed to develop\nnew drugs. Cynthia: Promoting innovation is certainly important. For this reason, patents should be\ngranted only for truly innovative drugs, not for minor variants of previously existing drugs. Since it is much\nless expensive to tweak an existing drug than to develop a wholly new one, pharmaceutical companies\ntend to focus primarily on the cheaper alternative.\nDario and Cynthia disagree over whether\nA. patents should be granted for all drug compounds\nB. developing truly innovative drugs is costly\nC. drug patents can promote innovation\nD. pharmaceutical companies have an incentive to create minor variants of existing drugs\nAnswer: A\ngpt-3.5-turbo: In this argument, Dario believes that the government should continue to grant patents for\nall new drug compounds because patents promote innovation by rewarding pharmaceutical companies\nfor the costly research needed to develop new drugs. On the other hand, Cynthia argues that patents\nshould only be granted for truly innovative drugs, not for minor variants of previously existing drugs. She\npoints out that it is much less expensive to tweak an existing drug than to develop a wholly new one, and\npharmaceutical companies tend to focus primarily on the cheaper alternative.\nAnalyzing the options: A. The disagreement between Dario and Cynthia is not about whether patents\nshould be granted for all drug compounds. Dario supports this idea while Cynthia argues for granting\npatents only for truly innovative drugs. Therefore, option A can be eliminated. B. The disagreement\nbetween Dario and Cynthia is not about whether developing truly innovative drugs is costly. They both\nagree that it requires costly research. Therefore, option B can be eliminated. C. The disagreement between\nDario and Cynthia is about whether drug patents can promote innovation. Dario believes that patents\npromote innovation, but Cynthia argues that they should only be granted for truly innovative drugs to\neffectively promote innovation. This aligns with the subject of their disagreement. Therefore, option C\nis a valid choice. D. The disagreement between Dario and Cynthia is not about whether pharmaceutical\ncompanies have an incentive to create minor variants of existing drugs. Both agree that it is a cheaper\nalternative, and Cynthia specifically mentions this as a concern. Therefore, option D can be eliminated.\nAnswer: C. drug patents can promote innovation\ngpt-3.5-turbo + Base Bootstrap Reasoning: Cynthia agrees that patents promote innovation but thinks\nthey should be granted only for truly innovative drugs, not for minor variants of existing drugs. Answer: D.\npharmaceutical companies have an incentive to create minor variants of existing drugs\ngpt-3.5-turbo + LogicGuide Bootstrap Reasoning: Dario supports granting patents for all new drug\ncompounds, while Cynthia believes patents should be granted only for truly innovative drugs, not for minor\nvariants of previously existing drugs. Answer: C. drug patents can promote innovation\nFigure 19: Example #3 of a ReClor problem. Here, all models choose incorrect options. The model\nwith LogicGuide bootstrap does seem to pick up the key point of disagreement, but still fails to pick\nthe option that best describes it.\n22\n"
  },
  {
    "title": "Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks",
    "link": "https://arxiv.org/pdf/2306.04009.pdf",
    "upvote": "1",
    "text": "Triggering Multi-Hop Reasoning for Question Answering\nin Language Models using Soft Prompts and Random Walks\nKanishka Misra\u22c6\nPurdue University\nkmisra@purdue.edu\nCicero Nogueira dos Santos\nGoogle Research\ncicerons@google.com\nSiamak Shakeri\nGoogle DeepMind\nsiamaks@google.com\nAbstract\nDespite readily memorizing world knowledge\nabout entities, pre-trained language models\n(LMs) struggle to compose together two or\nmore facts to perform multi-hop reasoning in\nquestion-answering tasks. In this work, we\npropose techniques that improve upon this lim-\nitation by relying on random walks over struc-\ntured knowledge graphs. Specifically, we use\nsoft prompts to guide LMs to chain together\ntheir encoded knowledge by learning to map\nmulti-hop questions to random walk paths that\nlead to the answer. Applying our methods on\ntwo T5 LMs shows substantial improvements\nover standard tuning approaches in answering\nquestions that require 2-hop reasoning.\n1\nIntroduction\nPerforming multi-hop reasoning to answer ques-\ntions such as Where was David Beckham\u2019s daugh-\nter born? requires two fundamental capacities: C1:\npossessing pre-requisite knowledge (David Beck-\nham\u2019s daughter is Harper Beckham, Harper Beck-\nham was born in Los Angeles), and C2: ability to\ncompose internalized knowledge. Contemporary\npre-trained language models (LMs) such as BERT\n(Devlin et al., 2019) and T5 (Raffel et al., 2020)\nhave been shown to be adept at encoding factual\nknowledge (Petroni et al., 2019; Zhong et al., 2021;\nRoberts et al., 2020), an ability that can be further\nboosted by explicitly integrating them with knowl-\nedge about entities and relations (Bosselut et al.,\n2019; Sun et al., 2020; Wang et al., 2021, i.a.). At\nthe same time, these LMs often struggle to com-\npose the knowledge they encode (Kassner et al.,\n2020; Talmor et al., 2020; Moiseev et al., 2022),\nand therefore do not satisfy C2. To overcome this\nlimitation, previous works have proposed methods\nthat decompose multi-hop questions into single hop\nsub-questions that models can more easily answer\n\u22c6 Work done during an internship at Google Research.\n(Min et al., 2019; Perez et al., 2020, i.a.). How-\never, such methods require training entirely sep-\narate models, or make use of human-annotations\n(Patel et al., 2022). Furthermore, they focus on\ntasks where models explicitly receive additional\ntext containing relevant facts, which makes it un-\nclear if they can truly compose the knowledge that\nthey have internalized.\nIn this work, we aim to improve the standalone,\nself-contained ability of LMs to perform multi-hop\nreasoning. We posit that random walks\u2014paths be-\ntween entity nodes sampled from structured knowl-\nedge graphs\u2014can provide a useful training signal\nfor LMs to compose entity knowledge. To test this,\nwe perform a case-study on two T5 models (LARGE\nand XXL, Raffel et al., 2020). Specifically, we first\nintegrate within the LMs the single-hop knowledge\nthat is required to answer multi-hop questions (ef-\nfectively guaranteeing C1 is met). We show that\nthis alone is not enough to demonstrate substantial\nimprovements on questions requiring 2-hop reason-\ning. We then adapt the knowledge integrated T5\nmodels by training soft prompts (Qin and Eisner,\n2021; Lester et al., 2021) on random walks over\nthe structured knowledge that they have encoded,\nand devise two methods that trigger this ability in\nthe LMs given a multi-hop question as input. The\nfirst method, Parse-then-Hop (PATH), uses two\nspecialized soft prompts: one to parse entities and\nrelations from the question, and another to gener-\nate a path to the answer, resembling the outputs\nof a random walk. The second method, MIXHOP,\ntrains a single prompt on a mixture that combines\nthe QA task with the random walk training, so as\nto allow the model to implicitly learn PATH\u2019s task.\nBoth these soft prompt methods use the same un-\nderlying LM (kept frozen), and guide it to compose\nits internalized entity knowledge.\nOur experiments suggest that integrating random\nwalks in the T5 models using our proposed tech-\nniques can substantially improve their ability to\narXiv:2306.04009v1  [cs.CL]  6 Jun 2023\nT5\nDavid Beckham ; daughter\nHarper \nBeckham\nQuestion: Where was the director of Violet Tendencies born?\nDavid Beckham ; \ndaughter ; place \nof birth\n\ud83d\udd25\nKNIT5\nKnowledge Integration\nRandom Walk Training\n\u2744\nMethod 1: Parse-then-Hop (PaTH)\nPP\n\u2a01 Question\nKNIT5\nViolet Tendencies ; \ndirector; place of \nbirth\nKNIT5\n\u2744\nViolet Tendencies ; \ndirector ; Casper \nAndreas ; place of \nbirth ; Sweden\n\u2744\n(Violet Tendencies, director, Casper Andreas), \n(Casper Andreas, place of birth, Sweden)\nMethod 2: MIXHOP\nKNIT5\n\u2744\nRelevant \nKnowledge\n\u2a01 Question\nViolet Tendencies ; \ndirector; place of birth\nHP\n\u2744\nMP\n\ud83d\udd25\nHP\n\ud83d\udd25\n\ud83d\udd25\n\ud83d\udd25\n\u2744\ntuned\nfrozen\n\u2a01\n\u2a01\n\u2a01\nDavid Beckham ; \ndaughter ; Harper \nBeckham ; place of \nbirth ; Los Angeles\nFigure 1: Overview of our approach. Colored rectangular boxes indicate soft prompts: Hopping Prompts (HP),\nParsing Prompts (PP), and Prompts for the MIXHOP approach (MP). L indicates concatenation.\nanswer entity-centric 2-hop questions (Ho et al.,\n2020) at larger model sizes. Briefly, on T5-XXL\nour methods show improvements over previously\nproposed prompt-tuning approaches (Lester et al.,\n2021; Vu et al., 2022) as well as full model fine-\ntuning, with PATH and MIXHOP demonstrating\ngains of \u223c16 and \u223c9.6 points in exact match scores\nover fine-tuning the entire model, respectively. In\nthe case of T5-LARGE, our methods demonstrate\nimprovements over standard prompt-tuning meth-\nods, but fall short of the performance achieved\nusing fine-tuning, suggesting that larger models\u2014\nwith up to 11B parameters\u2014are more conducive to\nleveraging the training signal provided by random\nwalks via soft prompts.\n2\nMethod\n2.1\nModels\nWe apply our methods on two T5.1.1 models (Raf-\nfel et al., 2020)\u2014T5-LARGE (770M parameters)\nand T5-XXL (11B parameters), using checkpoints\nthat have been adapted using the Prefix LM objec-\ntive for 100K steps (Lester et al., 2021).\n2.2\nKnowledge Integration\nWe first ensure that the LMs we use have the pre-\nrequisite single-hop knowledge (C1) required to an-\nswer multi-hop questions. This is necessary, as pre-\nliminary experiments suggested that the T5 models\nwe used did not satisfy this primary criterion for\nmulti-hop reasoning (see Table 1). Specifically, we\nfollow Bosselut et al. (2019) and fine-tune our LMs\non knowledge graph (KG) triples containing the\nrelevant knowledge that is to be composed to an-\nswer questions. That is, given a triple (e1, r, e2),\nwhere e1 and e2 are entities, and r is the relation,\nwe fine-tune our T5 models to take as input the\nstring \u201ce1 ; r1\u201d, and produce \u201ce2\u201d as output, us-\ning the Prefix LM objective (Raffel et al., 2020).\nTo avoid catastrophic forgetting (McCloskey and\nCohen, 1989) and retain the LMs\u2019 language un-\nderstanding abilities, we mix our knowledge inte-\ngration training instances with that of the models\u2019\npre-training corpus\u2014i.e., C4 (Raffel et al., 2020)\u2014\nin a 50:50 mixture. We denote the resulting models\nas KNowledge-Integrated T5 (KNIT5).\n2.3\nComposing knowledge using soft prompts\nRandom Walk training\nOur method is centered\naround guiding the KNIT5 LMs to chain together\ntheir encoded knowledge by training them on ran-\ndom walks over a relevant KG. We formulate ran-\ndom walks here as as a sequence of entity-relation-\nentity triples that are connected linearly via shared\nentities. Figure 1 shows an example with a ran-\ndom walk of length 3 (Violet Tendencies ;\ndirector ; Casper Andreas ; place of birth\n; Sweden). To perform our random walk training,\nwe rely on soft prompts (Li and Liang, 2021; Lester\net al., 2021; Qin and Eisner, 2021), a sequence of\nlearnable token-vectors that are prepended to the\ninput of the LM. Importantly, we only update these\nvectors during training, thereby keeping intact the\nutility and encoded knowledge of the main LM,\nwhile also being parameter efficient. Our training\nprocedure is as follows: we first perform uniform\nrandom walks of length n over the KG used in sec-\ntion 2.2, resulting in a set whose elements are se-\nquences of entities interleaved by the relations that\nconnect them: (e1, r1, e2, . . . , rn\u22121, en). During\ntraining, KNIT5 receives as input an incomplete\npath, with only the initial entity and the intermedi-\nate relations (e1, r1, r2, . . . , rn\u22121), and is tasked to\ngenerate the full path: (e1, r1, e2, r2 . . . , rn\u22121, en).\nWe denote the trained prompts that trigger this abil-\nity in KNIT5 as Hopping Prompts.\n2.4\nPerforming QA using Hopping Prompts\nWe propose two new techniques that utilize Hop-\nping Prompts to map natural language questions to\nappropriate paths in the knowledge graph:\nParse-then-Hop (PATH)\nWe take advantage of\nthe modularity of soft prompts, and distribute the re-\nsponsibility of parsing the relational structure from\nquestions and random walk querying using sepa-\nrate specialized prompts, keeping the underlying\nmodel the same. We train \u201cparsing\u201d prompts that\nparse questions to incomplete random walk queries,\nresembling the inputs to the Hopping Prompts de-\nscribed above. For instance, the question \u201cWhere\nwas David Beckham\u2019s daughter born?\u201d is parsed\nto \u201cDavid Beckham ; daughter ; place of\nbirth\u201d. We then swap the parsing prompts with\nthe hopping prompts, using the outputs from the\nparsing step as inputs and then run inference to get\na path from the entity in the question to the answer:\n\u201cDavid Beckham ; daughter ; Harper Beckham\n; place of birth ; Los Angeles\u201d, as shown\nin Figure 1. We posit that parsing of the appropri-\nate relational structure from the question should\nbe easy and self-contained, since it only involves\nusing the surface form of the question as opposed\nto invoking any external knowledge, which is dele-\ngated to Hopping Prompts.\nMIXHOP\nWe propose to jointly train a single\nset of prompts on a mixture of the QA task and\nthe Hopping Prompts task (50:50), thereby halving\nthe number of forward passes from the previous\nmethod. Our primary motivation here is to pro-\nvide diverse training signals that get models to map\nquestions to the structured knowledge that explic-\nitly connects the entity in the question to the answer\nentity. Like PATH, MIXHOP directly produces ran-\ndom walk paths as output, as shown in Figure 1.\n3\nExperimental Setup\n3.1\nData\nMulti-hop QA Dataset\nWhile traditional multi-\nhop QA datasets provide additional paragraphs\n(Yang et al., 2018; Trivedi et al., 2022) for models\nto reason over, we operate under the more challeng-\ning closed-book QA setting (Roberts et al., 2020),\nwhere such contexts are omitted. Specifically, we\nuse the \u201ccompositional\u201d and \u201cinference\u201d subsets of\nthe 2WikiMultiHopQA dataset (Ho et al., 2020),\nwhich contains 2-hop English questions focusing\non 98,284 entities and 29 relations, sourced from\nWikiData (Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014). We se-\nlect this dataset as it uniquely provides the precise\nstructured knowledge that is required to answer\neach question, in the form of entity-relation-entity\ntriples.1 Since the test splits for these specific sub-\nsets are private, we use the validation split as the\ntest set, and use 10% of the training set for valida-\ntion. In total we have 72,759 train, 8,085 validation,\nand 6,768 test questions.\n1-hop QA Dataset\nTo characterize if the models\nwe test have the pre-requisite 1-hop knowledge,\nwe additionally construct 1-hop questions from\n2WikiMultiHopQA by applying manually defined\ntemplates over the entity triples provided for each\n2-hop question (see Appendix C). For instance, the\ntriple Inception ; director ; Christopher\nNolan is converted to Who is the director of Incep-\ntion?. We end up with 83,643 train, 5,022 valida-\ntion, and 6,440 test QA instances. We term this\nconstructed dataset as 1WikiHopQA.\nKnowledge Integration Data\nWe build the KG\nfor our methods using the set of ground-truth triples\nprovided in the 2WikiMultiHopQA dataset (98,284\nentities and 29 relations, amounting to 95K triples).\nRandom Walk Training Corpus\nFor each en-\ntity in the above KG, we sample up to 20 random\nwalks of length 3, each corresponding to an in-\nstance of 2 hops between entities. We repeat this\nstep 5 times with different seeds, discard duplicate\npaths, and end up with a total of 165,324 unique\npaths as a result. Importantly, we hold out the\npaths that include the triples in the QA task\u2019s\nvalidation and test sets in order to avoid leak-\nage, ending up with 155,311/ 8,085/6,768 paths\nas our train/validation/test sets, respectively. This\nway, our experiments test for the kinds of gener-\nalization where models should successfully place\nentities in novel structures (complete paths in the\nKG), whose primitive knowledge (1-hop triples)\nis encoded in the model, but the composition is\nnot. This can be viewed as a partial version of\nthe lexical and structural generalization tests in\nstricter, more prominent compositional generaliza-\ntion benchmarks (Lake and Baroni, 2018; Kim and\nLinzen, 2020).\n3.2\nBaselines and Comparisons\nWe compare our proposed approaches to standard\nfine-tuning and prompt-tuning (Lester et al., 2021),\n1Works such as Balachandran et al. (2021) propose unsu-\npervised mappings of questions in more popular datasets such\nas NaturalQuestions (Kwiatkowski et al., 2019) to paths in\nknowledge graphs, but our initial investigations of these paths\nfound them to be extensively noisy.\nSetup\nModel\nLARGE\nXXL\nPT\nT5\n4.36\n6.89\nKNIT5\n6.30\n31.64\nFT\nT5\n6.24\n8.82\nKNIT5\n22.73\n43.60\nTable 1: Test EM scores achieved by T5 and KNIT5 on\n1WikiHopQA. PT: Prompt-Tuning, FT: Fine-Tuning.\nwhich we use to directly produce the answer, with-\nout any intermediate entities or relations. Addi-\ntionally, we also adapt SPOT (Vu et al., 2022), a\nprompt-tuning method where we initialize prompts\nwith those that were pre-trained on related tasks.\nIn our adaptation, we initialize prompts using the\nvalues of the Hopping Prompts, and SPOT-transfer\nthem to guide KNIT5 models to generate the full\noutput, similar to PATH and MIXHOP. Since we\noperate in the closed book QA setting (Roberts\net al., 2020), our methods cannot be directly com-\npared to previous approaches on the dataset we\nconsidered, all of which receive paragraph contexts\nduring training. Only two other methods have con-\nsidered the present dataset in its closed-book format\n(Press et al., 2023; Wang et al., 2022). However,\nboth of them use smaller subsets of the validation\nset as their testing set, and test on different pre-\ntrained models, making it impractical to directly\ncompare our results to their reported values.\n4\nExperiments and Findings2\nWe report and summarize our results as follows:\nIntegration of 1-hop knowledge only results\nin marginal improvements on 2-hop questions\nWe begin by first establishing the extent to which\nT5 models encode and compose 1-hop knowledge\nrequired to answer 2-hop questions, and whether\nadditional knowledge integration (via KNIT5) can\nimprove both these abilities. From Tables 1 and 3,\nwe observe that the T5 models struggle to answer\nboth 1-hop as well as 2-hop questions, suggesting\nthat they critically lack the precise 1-hop entity\nknowledge required to demonstrate success on the\n2-hop questions. The KNIT5 LMs overcome this\nlimitation, by showing substantial gains on 1Wik-\niHopQA over their T5 counterparts\u2014they show\nimprovements of \u223c16.5 and \u223c34.8 points in ex-\n2Training details for all experiments can be found in Ap-\npendix A.\nModel\nEM\nF1\nKNIT5-LARGE\n22.83\n84.72\nKNIT5-XXL\n58.36\n92.82\nTable 2: Best reported validation EM and F1 scores\nachieved from training Hopping Prompts to get KNIT5\nmodels to generate random-walks. N = 8085.\nact match (EM) scores at LARGE and XXL sizes\nin the fine-tuning setting, respectively (Table 1).\nHowever, this is insufficient to show improvements\non 2-hop questions\u2014where maximum gain over\nT5 is only 2.2 points, achieved by prompt-tuning\nKNIT5-XXL (see Table 3). This suggests that even\nafter being endowed with the prerequisite 1-hop\nknowledge, both LMs are unable to successfully\nanswer more complicated questions, echoing the\nresults of Moiseev et al. (2022). Note that both\nKNIT5 models almost perfectly memorize the KG\nin our knowledge-integration experiments (achiev-\ning \u223c96% EM in under 10K training steps; see\nAppendix B.1), so their limitations on 2-hop ques-\ntions are likely not due to lack of entity knowledge\nand perhaps instead due to the inability to compose\nor chain together memorized facts.\nGeneralizing to novel random walks may re-\nquire the prompt-tuning of larger LMs\nWe\nnow turn to analyzing the performance of mod-\nels in generating random walks, a critical compo-\nnent for all our proposed QA methods. How well\ndoes prompt-tuning LMs generalize to KG paths\ncomposed of facts they have memorized but are\nunseen during training? Recall that this step in-\nvolved leveraging soft prompts (called Hopping\nPrompts) to guide the LMs to chain together their\nmemorized entity knowledge and generate paths\nakin to performing a random walk. That is, it is\nthe Hopping Prompts that must provide the neces-\nsary condition in the encoder to facilitate successful\noutput-generation, and not the entire LM. Also re-\ncall that we explicitly held out the paths involving\ntriples in the validation and test sets of the main\nQA task to prevent complete memorization (due to\nleakage into the training set). This way we are able\nto measure the extent to which models learned to\nconstruct KG paths in a generalized manner.\nTo\nthis end, we compute the EM and F1 scores over\nthe full generated spans of entities, interleaved by\nthe relations that connect them. Note that EM is\nsubstantially stricter than F1, since F1 rewards par-\nSize\nPrompt-Tuning\nFine-Tuning\nSPOT\nPATH\nMIXHOP\nT5\nKNIT5\nT5\nKNIT5\nLARGE\n4.47\n5.29\n10.03\n11.19\n7.22\n8.62\n6.58\nXXL\n6.42\n8.62\n12.92\n13.47\n20.03\n29.37\n23.09\nTable 3: Test set EM scores achieved by various tuning methods on 2WikiMultiHopQA (Ho et al., 2020). SPOT\n(Vu et al., 2022), PATH, and MIXHOP use KNIT5 as their base model.\ntial overlap of tokens between the target vs. the\ngenerated output. Table 2 shows these scores for\nKNIT5-LARGE and KNIT5-XXL on the validation\nset of our random walk task, tuned using the Hop-\nping Prompts. We see from Table 2 that there is\na substantial gap between KNIT5-LARGE (\u223c23\nEM) and KNIT5-XXL (\u223c58 EM), suggesting that\nthe LARGE model finds it difficult to generalize to\nrandom walk paths involving entities and relations\noutside of the training set. We conclude from this\nobservation that the gap between KNIT5-LARGE\nand KNIT5-XXL in generalizing to held-out KG\npaths is likely going to be reflected when tested\nfor 2-hop QA. That is, we expect our prompting\nmethods with KNIT5-LARGE as the base-model to\nstruggle on our test set questions as their ground-\ntruth paths were not encountered during training,\nand at the same time, expect the opposite to be the\ncase for KNIT5-XXL. Additionally, the EM score\nachieved by the XXL-sized model is well below\nperfect values, highlighting important avenues for\nfuture work to improve upon these gaps.\nTraining on random walks substantially im-\nproves 2-hop capabilities ..but mostly in larger\nLMs\nWe used three methods that leveraged\nthe training signal provided by random walks to\ncompose the 1-hop knowledge as memorized by\nKNIT5: PATH (ours), MIXHOP (ours), and SPOT\n(Vu et al., 2022). Due to lack of space, examples of\nthe outputs from each of these methods, along with\nanalysis of intermediate steps (e.g., parsing) are\nshown in Appendix B. We observe from Table 3\nthat for the XXL-sized model, all three methods\nlead to substantial improvements in performance on\n2-hop questions over standard tuning approaches\non T5 and KNIT5. Notably for KNIT5-XXL, ran-\ndom walk-integrated methods improve even over\nfine-tuning, which is often expected to be better\nat transfer learning as compared to parameter effi-\ncient methods. Among the three, our PATH method\nshows the best improvements (\u223c16 point gain over\nfine-tuning KNIT5-XXL) at answering 2-hop ques-\ntions. This showcases the promise of learning sepa-\nrate specialized prompts that operate over the same\nunderlying model to first parse natural language\ninto incomplete structured knowledge, and then\nexpand it to answer the question, while also elic-\niting intermediate steps (Wang et al., 2022), sim-\nilar to recent in-context prompting methods (Wei\net al., 2022b; Nye et al., 2022). While the MIXHOP\nmethod (\u223c9.6 point gain over fine-tuning) falls\nshort of PATH, it still improves over SPOT (\u223c6.6\npoint gain over fine-tuning), suggesting that joint\ntraining of related tasks may improve over sequen-\ntial training (as employed by SPOT) in perform-\ning multi-hop reasoning, at larger model sizes. In\nthe case of T5-LARGE and KNIT5-LARGE, while\nthe proposed methods show improvements over\nstandard prompt-tuning, with PATH demonstrating\na gain of 3.33 points over prompt-tuning KNIT5-\nLARGE, they fall-short of the performance achieved\nby fine-tuning. However, their non-trivial improve-\nments over regular prompt-tuning suggests the gen-\neral benefits of the training signal provided by ran-\ndom walks, which end up being most impressive at\nmodels that are an order of magnitude larger. Over-\nall, these results corroborate with our hypothesis\nfrom the random walk tests about KNIT5-LARGE\u2019s\npotential inability to generate partially novel ran-\ndom walks given either natural language multi-hop\nquestions (MIXHOP) or their parses (PATH).\n5\nConclusion\nWe show that composition of memorized world\nknowledge can be triggered in LMs with up to\n11B parameters (T5-XXL) to a desirable extent by\nleveraging training signal from random walks over\nstructured knowledge using approaches based on\nprompt-tuning (Lester et al., 2021). Doing so leads\nto substantial improvements in the LMs\u2019 ability to\nanswer 2-hop questions, even beyond standard, full\nmodel fine-tuning.\nLimitations\nDespite showing non-trivial improvements in the\nmulti-hop capabilities of T5 models, our work has\nmultiple limitations.\nRestricted to 2-hops\nFirst, we chose 2WikiHop-\nMultiQA (Ho et al., 2020) as our primary dataset\nsince it uniquely maps each question to a chain of\ntriples that contain the precise, noiseless single-hop\nknowledge required to answer the question. How-\never, this comes at the cost of our analyses only\nbeing restricted to 2-hops (though see arguments\nby Press et al. (2023, sec 3.5) who suggest 3-and-\n4-hop questions to be too convoluted to understand\neven by native-speakers). Nonetheless, our random\nwalk training method is general by definition, and\ncan be extended to multiple hops, though its effec-\ntiveness on QA tasks requiring more than 2-hops\nof reasoning remains to be measured.\nKnowledge Graph size\nOur focus in this paper\nwas to allow models to chain together their inter-\nnalized knowledge in order to answer complex 2-\nhop questions. However, this critically requires\nthem to possess the world knowledge required to\nanswer the questions, for which we had to memo-\nrize the KG constructed using the structured triples\nprovided in the dataset. This trade-off between\nfocusing on knowledge composition vs. fully en-\ncoding world knowledge restricted our KG to be\nsmall in size (only 98,284 entities and 29 relations),\nwhich could be impractical in most real-world ap-\nplications. In future work, we will experiment with\nlarger sized KGs (Vrande\u02c7ci\u00b4c and Kr\u00f6tzsch, 2014),\nby adding a substantially larger amount of addi-\ntional triples to the existing KG, and measure their\nimpact on multi-hop reasoning.\nLack of diverse QA tasks\nFinally, we were un-\nable to consider popular datasets with CBQA ver-\nsions such as TriviaQA (Roberts et al., 2020), Nat-\nuralQuestions (Kwiatkowski et al., 2019), etc., due\nto their lack of links from questions to structured\nknowledge. Future work can apply entity and re-\nlational linking techniques (Balachandran et al.,\n2021; Agarwal et al., 2021) in order to augment\nsuch QA datasets with (possibly) noisy links to\nstructured knowledge, which will allow us to paint\na more holistic picture of our methods. Addition-\nally, this would also overcome the above limitation\n(of KG size), as it would substantially increase the\namounts of entities and relations to be encoded\nwithin models.\nImplications for Larger Models\nAlthough we\nshow clear improvements in triggering 2-hop rea-\nsoning in the largest T5 LM (T5-XXL), with 11B\nparameters, contemporary work has shown that\nmulti-step reasoning capacities naturally emerge\nin LMs that are two or three orders of magnitude\nlarger (Brown et al., 2020; Chowdhery et al., 2022;\nWei et al., 2022b,a). However, these LMs benefit\nfrom examples in-context (especially since tuning\nthem is non-trivial and expensive), and therefore it\nis unclear whether our methods can improve such\nmodels\u2019 capacities even further. We have not tested\nsuch LMs in our work, due to resource limitations.\nAcknowledgments\nWe thank Noah Constant, Chung-Ching Chang,\nBrian Lester, and Ben Withbroe from Google Re-\nsearch for their helpful comments and advice. We\nwould also like to thank our three anonymous re-\nviewers for their useful feedback.\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and Rami\nAl-Rfou. 2021. Knowledge graph based synthetic\ncorpus generation for knowledge-enhanced language\nmodel pre-training. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 3554\u20133565, Online. As-\nsociation for Computational Linguistics.\nVidhisha Balachandran, Bhuwan Dhingra, Haitian Sun,\nMichael Collins, and William Cohen. 2021. Inves-\ntigating the effect of background knowledge on nat-\nural questions. In Proceedings of Deep Learning\nInside Out (DeeLIO): The 2nd Workshop on Knowl-\nedge Extraction and Integration for Deep Learning\nArchitectures, pages 25\u201330, Online. Association for\nComputational Linguistics.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: Commonsense transformers for auto-\nmatic knowledge graph construction. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics, pages 4762\u20134779, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171\u20134186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020.\nConstructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609\u20136625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nNora Kassner, Benno Krojer, and Hinrich Sch\u00fctze. 2020.\nAre pretrained language models symbolic reasoners\nover knowledge? In Proceedings of the 24th Confer-\nence on Computational Natural Language Learning,\npages 552\u2013564, Online. Association for Computa-\ntional Linguistics.\nNajoung Kim and Tal Linzen. 2020. COGS: A compo-\nsitional generalization challenge based on semantic\ninterpretation. In Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 9087\u20139105, Online. As-\nsociation for Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452\u2013466.\nBrenden Lake and Marco Baroni. 2018. Generalization\nwithout Systematicity: On the Compositional Skills\nof Sequence-to-Sequence Recurrent Networks. In\nInternational conference on machine learning, pages\n2873\u20132882. PMLR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045\u20133059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582\u2013\n4597, Online. Association for Computational Lin-\nguistics.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of learn-\ning and motivation, volume 24, pages 109\u2013165. Else-\nvier.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097\u20136109, Florence, Italy. Association for Compu-\ntational Linguistics.\nFedor Moiseev, Zhe Dong, Enrique Alfonseca, and Mar-\ntin Jaggi. 2022. SKILL: Structured knowledge infu-\nsion for large language models. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 1581\u20131588,\nSeattle, United States. Association for Computational\nLinguistics.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2022. Show your work: Scratch-\npads for intermediate computation with language\nmodels. In Deep Learning for Code Workshop.\nPruthvi Patel, Swaroop Mishra, Mihir Parmar, and\nChitta Baral. 2022. Is a question decomposition unit\nall we need?\nIn Proceedings of the 2022 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 4553\u20134569, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n8864\u20138880, Online. Association for Computational\nLinguistics.\nFabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases?\nIn Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463\u20132473, Hong Kong, China. Association\nfor Computational Linguistics.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels. ICLR 2023 Submission.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203\u20135212, Online. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1\u201367.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418\u20135426,\nOnline. Association for Computational Linguistics.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nCoLAKE: Contextualized language and knowledge\nembedding. In Proceedings of the 28th International\nConference on Computational Linguistics, pages\n3660\u20133670, Barcelona, Spain (Online). International\nCommittee on Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743\u2013758.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022.\nMuSiQue: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nDenny Vrande\u02c7ci\u00b4c and Markus Kr\u00f6tzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commu-\nnications of the ACM, 57(10):78\u201385.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou\u2019,\nand Daniel Cer. 2022. SPoT: Better frozen model\nadaptation through soft prompt transfer. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 5039\u20135059, Dublin, Ireland. Association\nfor Computational Linguistics.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022. Itera-\ntively prompt pre-trained language models for chain\nof thought. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2714\u20132730, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKEPLER: A unified model for knowledge embed-\nding and pre-trained language representation. Trans-\nactions of the Association for Computational Linguis-\ntics, 9:176\u2013194.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369\u20132380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017\u20135033, Online. Association\nfor Computational Linguistics.\nA\nTraining and Experiment Details\nHyperparameters\nWe use the default hyper-\nparameters and optimizers used to train the T5\n1.1 checkpoints (Raffel et al., 2020) as well as\nthose used in the Prompt-Tuning and SPOT papers\n(Lester et al., 2021; Vu et al., 2022). We set the\nprompt-length to 100 for all prompt-tuning experi-\nments, and initialized them with the top 100 tokens\nin the T5 models\u2019 vocabulary, following Lester et al.\n(2021). We fine-tune and prompt-tune our models\nfor a maximum of 100K and 200K steps, respec-\ntively. We stop training on convergence, and use the\ncheckpoint with the best validation performance to\nevaluate. Tables 4, 5, and 6 show hyperparameter\nvalues for each type of experiment. All results are\nfrom single runs.\nHardware and Compute\nPrompt-tuning and\nfine-tuning experiments for LARGE models were\nrun on 16 TPUv3 chips, while those for XXL mod-\nels were run on 64 TPUv3 chips. One exception is\nknowledge integration (which also involved contin-\nual pre-training on C4, larger batch size, and longer\nsequences), for which we used 256 TPUv3 chips\nfor XXL, and 64 TPUv3 chips for LARGE.\nCode\nFor metric calculation and checkpoints, we\nuse the T5 and T5x code-base, open-sourced on\ngithub.34 For prompt-tuning experiments, we adapt\nthe original code-base (Lester et al., 2021), which\nis also open-sourced.5\nData\nThe 2WikiMultiHopQA dataset (Ho et al.,\n2020) has been released with Apache 2.0 license.6\nHyperparameter\nValues\nBatch Size\n32 (XXL), 128 (LARGE)\nLearning Rate\n0.001\nDropout\n0.1\nTraining Steps\n100K (w/ early stopping)\nTable 4: Hyperparameters used for fine-tuning T5-\nLARGE and T5-XXL. Values except batch size and\ntraining steps kept same as Raffel et al. (2020).\nHyperparameter\nValues\nBatch Size\n512\nLearning Rate\n0.001\nDropout\n0.1\nTraining Steps\n100K (w/ early stopping)\nTable 5: Hyperparameters used for Knowledge Integra-\ntion experiments. Values except batch size and training\nsteps kept same as Raffel et al. (2020).\nHyperparameter\nValues\nBatch Size\n32 (XXL), 128 (LARGE)\nLearning Rate\n0.3\nPrompt Length\n100\nDropout\n0.1\nTraining Steps\n200K (w/ early stopping)\nTable 6: Hyperparameters used for all prompt-tuning\nexperiments. Values except batch size kept same as\nLester et al. (2021), number of training steps kept same\nas Vu et al. (2022), who found longer training to be\nbeneficial.\n3https://github.com/google-research/\ntext-to-text-transfer-transformer/tree/main/t5\n4https://github.com/google-research/t5x\n5https://github.com/google-research/\nprompt-tuning\n6https://github.com/Alab-NII/2wikimultihop\n25\n50\n75\n100\n2\n4\n6\n8\n10\n12\nTraining Step (in thousands)\nMemorization (EM)\nModel\nXXL\nLARGE\nFigure 2: Time course of KG memorization for different\nKNIT5 model sizes. EM scores calculated for producing\nobject entity (e2), given subject (e1) and relation (r) as\ninputs to T5 models.\nB\nAdditional Analyses\nB.1\nKnowledge Integration\nIntegrating single-hop entity knowledge is an im-\nportant part of our methods. How well are the mod-\nels able to actually encode this knowledge? Fig-\nure 2 shows the dynamics of memorization across\nboth models, measured as the exact match scores\nin generating e2 given e1 and r. From Figure 2, we\nsee that the XXL and LARGE models can memorize\n96% of the KG within 5,000 and 10,000 steps re-\nspectively. With a batch size of 512, this translates\nto traversing the dataset 27 and 54 times, respec-\ntively, for XXL and LARGE. An important caveat\nhere is that the models are also being tuned on C4\n(Raffel et al., 2020), in order to retain the models\u2019\ngeneral language understanding-like capabilities.\nThat is, they can be expected to memorize the KG\nrelatively faster in the absence of training on the\nC4 corpus, but this would constitute a trade-off, by\nleading to overfitted models with substantial loss\ntheir original utility on other NLP tasks.\nB.2\nParsing Step in PATH\nThe parsing step is essential for our Parse-then-Hop\napproach to succeed. Here we perform additional\nanalyses on how well models can successfully ex-\ntract the relational structure that is required to an-\nswer the 2-hop questions in 2WikiMultiHopQA.\nRecall that the objective of the parsing step is to\nproduce as output a sequence indicating an incom-\nplete random walk, containing only the initial entity\n(seed node), followed by the relations (edges) that\nModel\nRelation EM\nEntity EM\nFull EM\nKNIT5-LARGE\n98.69\n76.19\n78.98\nKNIT5-XXL\n99.17\n78.46\n80.17\nTable 7: Metrics for the parsing sub-task of PATH on\ntest-set questions.\nlead to the final entity. For instance, if the ques-\ntion is \u201cWhere was the director of Inception (film)\nborn?\u201d the output of the parsing step should be:\nInception\n(film)\n;\ndirector\n;\nplace of birth\nHere, Inception (film) is the entity, e1, while\ndirector and place of birth are the relations,\nr1 and r2, respectively. We analyze the extent to\nwhich models successfully extract these three ele-\nments for the 6,768 test set questions, by measuring\nthree quantities: (1) Relation EM, which is the ex-\nact match score computed between the ground truth\nspan of relation pairs (here \u201cdirector ; place\nof birth\u201d), and that extracted from the model out-\nputs; (2) Entity EM, which is similar to Relation\nEM, but only considers the initial entity; and (3)\nFull EM, which computes the exact match score\nbetween the full output and the target. Table 7\nshows these values from prompt-tuning the two\nKNIT5 models.\nFrom Table 7, we see that prompt-tuning both\nmodels allows them to achieve almost perfect EM\nvalues in extracting the relation pairs from the ques-\ntions. However, we notice that models are not able\nto maintain this performance in copying over the\nentity, which lowers their overall EM scores on\nthis task. We performed a manual analysis of 50\nrandomly sampled outputs\u2014with incorrect entity\npredictions\u2014and found most errors to be due to\nomission of tokens involving middle names, or ad-\nditional information about the entity such as the\n\u201c(film)\u201d in the above example (other examples in-\nclude the entity\u2019s title, such as \u201cCount of East\nFrisia\u201d, or \u201c(born in year XXX)\u201d, \u201c(died in\nyear XXX)\u201d, etc.)\nB.3\nExample Outputs\nTables 8, 9, 10, and 11 show examples of outputs\nfrom the different approaches used in this work (ex-\namples shown for the XXL-sized models). Below\nwe discuss each of these cases in detail:\n\u2022 In Table 8, all approaches that leverage the\ntraining signal from random walks succeed,\nwhile tuning methods that do not fail. Ad-\nditionally, all three random walk-integrated\nmethods agree on their parsed relational struc-\nture as well as the intermediate entity.\n\u2022 In Table 9, only the two proposed methods\n(PATH and MIXHOP) succeed, while all other\nmethods fail. Note that SPOT correctly pre-\ndicts the correct intermediate entity (Sally\nHemings), but is unable to predict the final\nentity (John Wayles).\n\u2022 Table 10 shows an example where all ap-\nproaches fail. However, this question is am-\nbiguous, as aunt can either mean father\u2019s sis-\nter or mother\u2019s sister \u2013 our random walk in-\ntegrated methods correctly predict these rela-\ntional structures but are unable to resolve the\nintermediate and final entities.\n\u2022 Table 11 shows an example where all ap-\nproaches are supposedly scored as incorrect,\nbut are in-fact correct. Here we argue that the\nground truth answer, \u201cUnited Kingdom\u201d is in\nits incorrect form, since the question asks for\nthe nationality of a person. Our random walk-\nintegrated methods successfully predict the\nrelational structure and intermediate entities.\nMoreover all approaches predict British or\nEnglish, which are more acceptable forms\nof nationality for persons from the United\nKingdom. This problem could be mitigated\nby adding in aliases for the entities in the\nground-truth answer space, similar to Trivi-\naQA (Roberts et al., 2020).\nC\nTemplates for constructing\n1WikiHopQA\nHere we describe our process of constructing 1Wik-\niHopQA: a collection of English question-answer\npairs that only require single-hop knowledge using\nthe 2WikiMultiHopQA (Ho et al., 2020) dataset.\nThe 2WikiMultiHopQA dataset provides unique\nsequences of single-hop triples that collectively an-\nswer each 2-hop question. These amount to a total\nof 95,103 unique triples spanning 98,284 unique\nentities and 29 relations. We manually define a\ndiverse set of templates for each relation, as shown\nin Table 12. For many relations, we have multiple\ndifferent paraphrases of the question template, e.g.,\nthe relation director translates to: Who is the di-\nrector of X? or Who directed the film X? In such\nQuestion: Where was the place of burial of the director of film New World (1995 Film)? Answer: P\u00e8re Lachaise Cemetery\nModel\nSetup\nResponse\nT5-XXL\nFT\nForest Lawn Memorial Park\nPT\nForest Lawn Memorial Park\nKNIT5-XXL\nFT\nNew York\nPT\nForest Lawn Memorial Park\nSPOT\nNew World ; director ; Alain Corneau ; place of burial ; P\u00e8re Lachaise Cemetery\nPATH\nPP: New World ; director ; place of burial\nHP: New World ; director ; Alain Corneau ; place of burial ; P\u00e8re Lachaise Cemetery\nMIXHOP\nNew World ; director ; Alain Corneau ; place of burial ; P\u00e8re Lachaise Cemetery\nTable 8: An example case where methods that leverage random walks succeed, but baselines fail.\nQuestion: Who is Harriet Hemings\u2019s maternal grandfather? Answer: John Wayles\nModel\nSetup\nResponse\nT5-XXL\nFT\nTed Hughes\nPT\nJohn Hemings\nKNIT5-XXL\nFT\nBetty Hemings\nPT\nJohn Hemings\nSPOT\nHarriet Hemings ; mother ; Sally Hemings ; father ; Thomas Hemings\nPATH\nPP: Harriet Hemings ; mother ; father\nHP: Harriet Hemings ; mother ; Sally Hemings ; father ; John Wayles\nMIXHOP\nHarriet Hemings ; mother ; Sally Hemings ; father ; John Wayles\nTable 9: An example case where all baselines fail, and additionally SPoT (Vu et al., 2022) also produces the incorrect\nfinal entity, but our two proposed methods succeed.\nQuestion: Who is Christopher Blom Paus\u2019s aunt? Answer: Hedevig Christine Paus\nModel\nSetup\nResponse\nT5-XXL\nFT\nClotilde of Saxe - Lauenburg\nPT\nAnnemarie Blom Paus\nKNIT5-XXL\nFT\nAnna of Oldenburg\nPT\nChristina Paus\nSPOT\nChristopher Blom Paus ; father ; Ole Paus ; sibling ; Kjersti Bua Paus\nPATH\nPP: Christopher Blom Paus ; mother ; sibling\nHP: Christopher Blom Paus ; mother ; Margrete Laarmann ; sibling ; Kjartan Fl\u00f3ki\nMIXHOP\nChristopher Blom Paus ; mother ; Ulla Blom ; sibling ; Gunnar Blom\nTable 10: An example of an ambiguous question (since \u201caunt\u201d can be father\u2019s sister or mother\u2019s sister) on which all\napproaches fail. Importantly, methods that use random-walks accurately generate the relations required to answer\nthe question, but fail at predicting the correct entities.\ncases, we randomly sample a template from the\nentire set, equally weighing each. In total, we end\nup with 83,643 train, 5,022 validation, and 6,440\ntest QA pairs.\nQuestion: What nationality is John Bede Dalley\u2019s father ? Answer: United Kingdom\nModel\nSetup\nResponse\nT5-XXL\nFT\nBritish\nPT\nBritish\nKNIT5-XXL\nFT\nEnglish\nPT\nEnglish\nSPOT\nJohn Bede Dalley ; father ; William Dalley ; country of citizenship ; English\nPATH\nPP: John Bede Dalley ; father ; country of citizenship\nHP: John Bede Dalley ; father ; William Bede Dalley ; country of citizenship ; English\nMIXHOP\nJohn Bede Dalley ; father ; William Dalley, 1st Viscount Darnley ; country of citizenship ; British\nTable 11: An example of a scenario where all models fail at answering the question correctly, but this is likely\nattributable to the dataset since it does not contain aliases.\nRelation\nTemplate Space\nRelation\nTemplate Space\ndirector\nWho is the director of X?, Who di-\nrected the film X?\nmother\nWho is the mother of X?, Who is X\u2019s\nmother?\ndate of birth\nWhat is the date of birth of X?, When\nis X\u2019s birthday?, When was X born?\nfounded by\nWho is the founder of X?, Who\nfounded X?\ndate of death\nWhen did X die?, What is the date of\ndeath of X?\ninception\nWhen was X founded?\ncountry\nWhat country is X from?, What is the\nnationality of X?\nmanufacturer\nWho manufactures X?\ncountry\nof\ncitizenship\nWhat country is X from?, What is the\nnationality of X?\nperformer\nWho is the performer of the song X?,\nWho performed the song X?\naward\nreceived\nWhat is the award that X received?,\nWhich award did X receive?\nplace\nof\nbirth\nWhere was X born?, What is the\nplace of birth of X?\ncause\nof\ndeath\nWhy did X die?, What was the cause\nof X\u2019s death?\nplace\nof\nburial\nWhere was X buried?, Where is the\nplace of burial of X?\ncomposer\nWho is the composer of X?, Who com-\nposed X?\nplace\nof\ndeath\nWhere did X die?, Where is the place\nof death of X?\ncreator\nWho is the creator of X?, Who cre-\nated X?\nplace\nof\ndetention\nWhere did X go to prison?, Where\nwas X detained?\nchild\nWho is the child of X?\npresenter\nWho is the presenter of X?, Who pre-\nsented X?\ndoctoral\nadvisor\nWho is the doctoral advisor of X?\npublisher\nWho published X?, What company\npublished X?\neditor\nWho is the editor of X?, Who edited\nX?\nsibling\nWho is the sibling of X?, Who is X\u2019s\nsibling?\neducated at\nWhere did X graduate from?, What\nis the alma mater of X?, Where did X\nstudy?\nspouse\nWho is the spouse of X?, Who is X\u2019s\nspouse?\nemployer\nWho is the employer of X?, Where\ndoes X work?\nstudent of\nWho was the teacher of X?, Who was\nX\u2019s teacher?\nfather\nWho is the father of X?, Who is X\u2019s\nfather?\nTable 12: Question templates for for each of the 29 relations, used to create 1WikiHopQA. X stands for the subject.\n"
  }
]