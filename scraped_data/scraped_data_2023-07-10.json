[
  {
    "title": "Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning",
    "link": "https://arxiv.org/pdf/2307.03692.pdf",
    "upvote": "23",
    "text": "Becoming self-instruct: introducing early stopping\ncriteria for minimal instruct tuning\nWaseem AlShikh\nManhal Daaboul\nKirk Goddard\nBrock Imel\nKiran Kamble\nParikshith Kulkarni\nMelisa Russak\nWriter, Inc.\n{waseem,...,melisa}@writer.com\nAbstract\nIn this paper, we introduce the Instruction Following Score (IFS), a metric that\ndetects language models\u2019 ability to follow instructions. The metric has a dual\npurpose. First, IFS can be used to distinguish between base and instruct models.\nWe benchmark publicly available base and instruct models, and show that the\nratio of well formatted responses to partial and full sentences can be an effective\nmeasure between those two model classes. Secondly, the metric can be used as\nan early stopping criteria for instruct tuning. We compute IFS for Supervised\nFine-Tuning (SFT) of 7B and 13B LLaMA models, showing that models learn to\nfollow instructions relatively early in the training process, and the further finetuning\ncan result in changes in the underlying base model semantics. As an example of\nsemantics change we show the objectivity of model predictions, as defined by an\nauxiliary metric ObjecQA. We show that in this particular case, semantic changes\nare the steepest when the IFS tends to plateau. We hope that decomposing instruct\ntuning into IFS and semantic factors starts a new trend in better controllable instruct\ntuning and opens possibilities for designing minimal instruct interfaces querying\nfoundation models.\n1\nIntroduction\nLarge Language Models (LLMs) finetuned on in-\nstruct data can behave like conversational agents\n(Alpaca: Taori et al. 2023, Self-Instruct: Wang\net al. 2023). The recipe for a chat model is well-\ndefined: one needs to perform instruction tuning,\nwhich means supervised finetuning (SFT) of an\nLLM on tuples of instruction and response (Long-\npre et al. 2023).\nOpen-source datasets vary in quality and quantity,\nranging from 1k examples (Zhou et al. 2023) to\nover 800k examples (Anand et al. 2023). In addi-\ntion, there are more than a dozen open-source\nbase LLMs, such as LLaMA (Touvron et al.\n2023), OPT (Zhang et al. 2022), GPT-Neo (Gao\net al. 2020), Palmyra (Writer 2023), and others,\nwhich result in a plethora of possible combina-\ntions leading to distinct instruct models.\nWe can see instruct tuning attempts through the\nlens of the \"imitation models\" - concept intro-\nduced by Gudibande et al. 2023, i.e., efforts to\ndistil closed (and possibly much bigger) propri-\netary models like ChatGPT (OpenAI 2022), Bard\n(Pichai 2023), and Claude (AnthropicAI 2023).\nLittle is known about the qualitative impact of\nthe distillation process on the base model (Hin-\nton, Vinyals, and Dean 2015). Imitation success\nis measured in terms of knowledge (e.g., HELM\nLiang et al. 2022), skills (e.g., Natural Questions\nKwiatkowski et al. 2019) or manual checks based\non human preferences (Zhou et al. 2023). There is\nno consensus whether a manual check that might\nskew the metric towards style and formatting of\nresponses is a good overall metric (Gudibande\nPreprint. Under review.\narXiv:2307.03692v1  [cs.CL]  5 Jul 2023\net al. 2023). A fairly recent attempt to more ro-\nbustly evaluate instruct models is the Hugging-\nface Leaderboard (Huggingface 2023b), which\nevaluates models against four key benchmarks\nfrom the Eleuther AI Language Model Evalua-\ntion Harness (Gao et al. 2021).\nAblation studies have shown that both the diver-\nsity and quality of the training data play a cru-\ncial role in model performance (Chen et al. 2023,\nZhou et al. 2023). Low Training Data Instruction\nTuning (LTD Tuning) suggests that task-specific\nmodels can gain 2% performance when trained on\nless than 0.5% of the original data. Moreover, pro-\nlonged instruction tuning can decrease the founda-\ntional model knowledge (Gudibande et al. 2023)\nand can be seen as the out-of-distribution task for\na downstream task of instruct-tuning (Kumar et al.\n2022).\nIn this study, we want to lay the foundation for\ninstruct models research by defining the neces-\nsary (but not sufficient) condition for an instruct\nmodel. Let\u2019s conduct a thought experiment.\nLet\u2019s put all models behind a closed API (a recent\nequivalent of a black box). Is the model instruct-\ntuned or not? Knowledge benchmarks could be\nsimilar for vanilla and instruct models for LTD\ntuning. Skills tests would highly depend on the\nmodel size, which is not known. The simplest\nway of solving the riddle would be to ...chat\nwith the model and judge the tone of the response.\nFor a vanilla model, we expect a next prediction\nword attempt, whereas for instruct models, we ex-\npect them to follow instructions. We introduce a\nmetric that captures this tone difference - Instruct\nFollowing Score (IFS). We call this problem a\n\"tone alignment\" issue.\nThe IFS is defined as a ratio of \"answer-like\"\nresponses to \"continuation-like\" responses on a\npredefined set of instructions, where class of a\nresponse is determined by a binary classifier.\nWe benchmark publicly available base and in-\nstruct models, and show that the ratio of well\nformatted responses to partial and full sentences\ncan be an effective measure between vanilla and\ninstruct following models. Moreover, we calcu-\nlate IFS for SFT for 7B and 13B LLaMA models,\nin the hope of finding a stopping criterion for a\nminimal instruct tuning.\nTo draw a comparison between the learning curve\nfor response tone and the acquisition of seman-\ntic and domain-specific knowledge, we propose\na supplementary metric called ObjecQA. This\nauxiliary metric quantifies the objectivity of a\nmodel\u2019s predictions, as this signal can be identi-\nfied within the dataset. While this feature choice\nis arbitrary, we aim to discover possibly more\ngeneral heuristics for better control over the train-\ning phases, including identification of \"format-\ninfusion\" and \"knowledge-infusion\" stages.\nThe paper is organised as follows. In Section 2,\nwe discuss the necessary conditions for a model to\nbe considered an instruct model and data prepara-\ntion for IFS. The response tone classifier training\nis described in Section 4. In Section 5, we present\nresults for instruct models and compare them to\nbaseline vanilla models in terms of instruct tone\nand semantic shifts. The study ends with conclu-\nsions and future directions proposed in Section\n6.\n2\nBackground and Related Work\nThe response tone alignment problem is a part\nof a broader intent alignment topic. In princi-\nple, LLMs are not aligned with users\u2019 intents\nbecause their language modeling objective, e.g.,\npredicting the next token of a training document,\nis different from the following instruction target.\nOne successful approach for aligning both objec-\ntives is to prompt models using zero- or n-shot\ntechniques, where the response would look like a\ncompletion of a document containing QA (Brown\net al. 2020, Radford et al. 2018).\nAnother approach is to instruct and tune a vanilla\nmodel on tuples of instruction and response, so\nthe model, as part of learning, acquires skills to\nimitate the correct format response (Alpaca: Taori\net al. 2023, Self-Instruct: Wang et al. 2023).\nIn the InstructGPT paper (Ouyang et al. 2022),\nthe criterion \"fails to follow the correct instruc-\ntion / task\" was included in the list of human\nevaluation metadata for a reward model (RM)\nused in the PPO algorithm (Schulman et al. 2017)\nto fine-tune the SFT models to maximize their\nreward.\nWe aim to isolate and understand the tone compo-\nnent by evaluating each strategy as a style format-\nting problem rather than using knowledge and lan-\nguage understanding-based metrics, e.g., MMLU\n(Hendrycks et al. 2021).\n3\nInstruction Following Index\n3.1\nMotivation\nAn instruction following model intuitively be-\nhaves like a conversational agent, i.e. always\nassume the input is an instruction, and depending\non its understanding tries to provide an answer or\nask follow up questions. In contrast, a model that\ndoes not follow instructions will try to predict\n2\nnext tokens and optionally provide an answer or\ncontinue with the next instruction. The distinc-\ntion between two model classes becomes more\nclear for an instruction that is an incomplete sen-\ntence fragment. An instruction following model\nwill never try to complete the instruction.\nIt is crucial to emphasise that the quality of re-\nsponses is purposely beyond the scope of this\nclassification. The above criteria are thus neces-\nsary but not sufficient conditions for a chat model.\nIn this paper, we introduce the Instruction Fol-\nlowing Score (IFS), defined as a ratio of \"answer-\nlike\" responses to \"continuation-like\" responses\nto a predefined set of instructions. The class of\na response is determined by a binary classifier\n(called subsequently as \"response tone classifier\").\nThe process of training and gathering data for IFS\nwill be outlined in the sections that follow.\nIn this paper, we use interchangeably \"conver-\nsational tone\" and \"instruction following tone,\"\nmeaning a class of \"answer-like\" responses. The\nprocess of fine-tuning a base model to obtain an\ninstruct model is called \"instruction tuning.\"\n3.2\nDataset\nThe dataset for IFS is derived from a chat dataset,\nwhich originally consists of pairs (instruction, re-\nsponse). We will need to model inputs and out-\nputs for models that aren\u2019t following instructions.\nThe main idea for data generation is to append in-\nstruction to response and then consider different\nsubdivisions into two phrases, as shown in Figure\n1.\nFigure 1: IFS dataset generation. Different splits\ndefine fragments: I, R, Ip, Ic.\nIf the cut regenerates (instruction, response) we\nget the ideal input and output for a chat model. If\nwe shift the split to the right or to the left, we can\nobtain incomplete sentences (fragmented) which\nrepresent unfinished instructions or continuation\nof instructions followed by responses. To summa-\nrize, we can get:\n\u2022 Inference inputs:\nI - Instruction\nIp - Partial (fragmented) instruc-\ntion\n\u2022 Inference outputs:\nIc - Continuation of the instruction\nR - Response\nIn fact, combinations of those 4 parts gives all\npossible pairs of inputs and outputs for vanilla\nand chat models. In the table below we recom-\nbine the parts and give and assign them a binary\nscore depending whether the model responds like\na chat model.\n(I, R) The response R for instruction I is\nconversational. A model whose all re-\nsponses would resemble the above form\nwould be an instruction following, so the\nresponse has label 1.\n(Ip,R) The response R for partial instruction Ip\nis also conversational, but in this case\nthe model has not enough context to pro-\nvide any answer except requesting for\nmore information. This response is also\nlabeled as 1.\n(Ip,Ic) The model completes the fragmented in-\nstruction (executing next word predic-\ntion task). The pair does not look like a\nconversation, so the label is 0.\n(I , Ic) The model generates next instructions\n(similarly to next word prediction task\nagain), which gives the response label 0.\n(Ip,Ic+R) In this case, the model completes the\ninstruction then replies (executing next\nword prediction task too). Although au-\nthors might imagine people attempting\nhave such dialogue, we treat instruction\ncompletion as a sign of failed conversa-\ntion. Label is 0.\n(I,Ic+R) The model generates another instruction\nthen replies to its generation. The dia-\nlogue fails giving the response label 0.\nExamples for each case are shown in Table 1.\n3\nCase\nExample\nchat?\n(I, R)\nI: What if people had 40\nlegs?\nR: If people had 40 legs,\nthey\u2019d be human cen-\ntipedes on the go, setting\nworld records in races\nand always winning at\nTwister!\n1\n(Ip,R)\nIp: What if\nR: It seems like your\nquestion is incomplete.\nPlease provide more con-\ntext or details so I can\nbetter understand and an-\nswer your question.\n1\n(Ip,Ic)\nIp: What if\nIc: people had 40 legs?\n0\n(I , Ic)\nI: What if people had 40\nlegs?\nIc: What if people had 3\neyes?\n0\n(Ip,Ic + R)\nIp: What if\nIc + R: people had 40\nlegs?\nIf people had\n40 legs, they\u2019d be hu-\nman centipedes on the\ngo, setting world records\nin races and always win-\nning at Twister!\n0\n(I,Ic + R)\nI: What if people had 40\nlegs?\nIc + R: What if peo-\nple had 3 eyes?\nIf\npeople had 3 eyes, sun-\nglasses would come in\ntrendy trinocular styles\nand \"I\u2019ve got my eye on\nyou\" would be a whole\nnew level of surveil-\nlance.\n0\nTable 1: Examples of possible combinations of\nfragments I, R, Ip, Ic. The tone score indicates\nwhether the model follows the instruction (1) or\nnot (0).\nIn summary, among the six potential combina-\ntions, only two instruct model cases exist: (Ip, R)\nand (I, R). With this classification established,\nwe can now create the set of instructions and cor-\nresponding model responses.\nWe split pairs coming from all perfect and shifted\ncuts, and create two datasets: all instructions and\nall responses. The set of instructions is used to\ngenerate data used for prompting models, while\nthe set of responses is used to generate data for\nthe binary classifier. Figure 2 shows how chat\ndata is split and used for in our experiment.\nAs a source of clean text, we utilized the OpenAs-\nsistant chat dataset (K\u00f6pf et al. 2023). To control\nthe context of the conversation, we only consid-\nered the first instruction and its corresponding\nresponse from each dialogue.\n3.2.1\nInstructions dataset\nIn the instruction dataset, data points consist of\ninstructions sourced from OpenAssistant data, ei-\nther unmodified (I) or fragmented (Ip). We ob-\ntained a total of 7340 examples, with an approxi-\nmate 50% split between fragments and complete\nsentences. We recognise that the algorithm may\npotentially generate complete sentences labeled\nas fragmented, making the score split based on\nthis label a rough estimate.\nTable 2 shows examples of full and partial instruc-\ntions.\nInstruction\nLabel\nWhat is the difference between\nHTML\npartial\nWhat is the difference between\nHTML and JavaScript?\nfull\nWho wears\npartial\nWho wears short shorts?\nfull\nTable 2: Examples of instructions and their cate-\ngory.\n3.2.2\nResponses dataset\nThe set of responses represents the right side of\nFig. 1, i.e., original responses or responses shifted\nto the right. The collected classes are:\nLabel 0 : Ic, Ic+R\nLabel 1 : R\nWe drop the fine-grained classification of re-\nsponses and assign them only to \"answer-like\"\n(label !) or \"continuation-like\" (label 0). These\nsamples are later used to train the binary classifier.\nTable 3 shows examples of responses and their\nlabels.\n4\nResponse\nchat?\nit fly so fast? The fastest flying bird\nis the peregrine falcon.\n0\nagent? I\u2019m not a FBI agent.\n0\nWhen onions are cut, they release a\nchemical called sulfuric acid.\n1\nJames Madison was the primary au-\nthor of the Constitution and the Bill\nof Rights.\n1\nTable 3: Examples of responses and their cate-\ngories.\n4\nBinary classifier and Instruction\nFollowing Score\nThe binary classifier for tone response classifica-\ntion has been chosen as the best binary classifier,\ntrained on the set of responses using Hugging-\nface AutoTrain (Huggingface 2023a). Since the\ndataset consisted of a roughly equal split of neg-\native and positive samples, we have chosen ac-\ncuracy as the comparison metric. The winning\narchitecture was BertForSequenceClassification,\nand the final classifier metrics (as reported by\nAutoTrain) are presented in Table 4.\nMetric\nValue\nAccuracy\n0.970\nPrecision\n0.983\nRecall\n0.925\nTable 4: Validation metrics\nWe define Instruction Following Score (IFS) as a\nratio of all responses classified as \"answer-like\"\n(label 1) to all responses obtained by prompting\nthe instructions dataset. A perfect instruction-\ntuned model should always maintain a conver-\nsational tone (i.e. respond like a chat model to\nall instructions, even if instructions are partial or\nnot), so the maximum IFS is 1. We can addi-\ntionally define two related metrics IFSpartial and\nIFSfull, being ratio of \"answer-like\" responses to\nall partial and full instructions respectively.\nIn the following sections, we will use IFS to\nevaluate vanilla models as well as response tone\nchanges achieved by prompt engineering and a\nSFT process.\n5\nResults\n5.1\nBaseline\nWe used the IFS metric to evaluate several pub-\nlicly available models. Since the dataset consists\nof less than 50% fragmented instructions (includ-\ning false positives generated by the algorithm),\nwe expected the base model to obtain IFS be-\nlow this level when prompted without additional\naffixes. Scores for SFT and RLHF models pre-\nsented in Table 5 show that the expected maxi-\nmum is around 0.8-0.9, whereas the most promi-\nnent difference between a base and instruction-\nfollowing LLMs is the relative difference between\nIFSpartial and IFSfull.\nModel\nIFS\nIFSpartial\nIFSfull\nGPT-2\n0.68\n0.67\n0.7\nRedPajama-3B\n0.33\n0.17\n0.49\nLLaMa-7B\n0.34\n0.19\n0.5\nLLaMA-13B\n0.81\n0.79\n0.82\nLLaMA-33B\n0.74\n0.68\n0.81\ndavinci\n0.29\n0.17\n0.42\nPalmyra-x\n0.68\n0.45\n0.91\nPalmyra-base\n0.32\n0.17\n0.48\nPalmyra-large\n0.32\n0.17\n0.47\ntext-davinci-003\n0.62\n0.37\n0.88\nGPT-3.5-turbo\n0.9\n0.83\n0.97\nGPT-4\n0.88\n0.8\n0.97\nPalmyra-instruct\n0.61\n0.36\n0.86\nTable 5: Baseline: Instruction Following Score\n(IFS) for selected publicly available models.\n5.2\nPrompt engineering\nA very simple method to encourage LMs to fol-\nlow instructions is to add extra prompt suffixes\nor wrappers around instructions, which could dis-\nrupt the next token prediction task and produce\nresponses. Figure 3 presents three versions of\nprompts:\nFigure 3: Comparative illustration of instruction\ntuning prompts. A. Alpaca prompt, a wrapper\naround instruction, B. only Alpaca suffix, C. no\nprompt, the baseline\n5\nFigure 2: IFS training and evaluation pipeline\nThe results presented in Table 6 show that vari-\nants of both prompts are equally effective. If we\ncompare it with the baseline (C), we see that for\nall models the improvement of IFS is in the range\n0.5\u20130.6. It turns out that for Large Language\nModels (LLMs) a single prompt change can ef-\nfectively encourage models to follow instructions,\nreaching performance levels comparable to sev-\neral publicly available instruct models. We did\nnot test n-shot prompting, which can possibly\nfurther improve results.\nDataset\nIFS\nIFSpartial\nIFSfull\nLLaMa-7BA\n0.74\n0.71\n0.77\nLLaMa-7BB\n0.75\n0.73\n0.78\nLLaMa-7BC\n0.34\n0.19\n0.5\nLLaMA-13BA\n0.81\n0.74\n0.88\nLLaMA-13BB\n0.81\n0.79\n0.82\nLLaMA-13BC\n0.31\n0.18\n0.43\nLLaMA-33BA\n0.87\n0.85\n0.89\nLLaMA-33BB\n0.74\n0.68\n0.81\nLLaMA-33BC\n0.33\n0.18\n0.47\nTable 6: Instruction Following Score (IFS) for\nmodels with and without prompt suffixes.\n5.3\nSupervised finetuning\nIn this study, we opted for 7B and 13B LLaMA\nmodels as the base LLMs for SFT. To ensure\ncomparability of results, we followed the same\ntraining procedure and evaluation.\nWe used the gpt4all v1.3-groovy introduced in\nAnand et al. 2023 as the instruct dataset. We set\nthe character limit to 2k (similar to the LLaMa\nmodels pretraining objectives, which were trained\non a 512-token length). Through this filtering pro-\ncess, we obtained approximately 410k examples\nfor the instruct tuning.\nModels were trained with the modified Alpaca\nprompt:\nPROMPT_DICT = {\n\"prompt_input\": (\"{instruction}\\n\\n{\n,\u2192 input}### Response:\"),\n\"prompt_no_input\": (\"{instruction}###\n,\u2192\nResponse:\"),\n}\nThe modification integrates the instruction and\nthe optional input while eliminating the prefix\nprompt. This approach is consistent with how\nuser interfaces for chat models are typically im-\nplemented, i.e., as a single dialog input box. We\ncould use the full Alpaca wrapper, but since both\nprompting techniques lead to similar scores, we\nchose the shorter one due to efficiency reasons.\nResults of SFT are shown in Figure 4(a). We\nsee that the models\u2019 instruction-tuning capabil-\nities stabilize on level 0.9-0.95 after seeing ap-\nproximately 8k examples (marked as a horizontal\ndashed line). We will refer to this training phase\nas the \"format-infusion\" phase. As a side note,\nwe observe that bigger models might reach the\n0.9 IFS level relatively faster (which is as far\nas we can infer from a two-points experiment),\n6\nwhich votes in favor of good results of SFT of\n65B LLaMA on 1k examples (Zhou et al. 2023).\nIn order to contrast tone changes with semantic\nshifts of model responses that may occur in SFT,\nwe have looked for a feature that could be ac-\nquired while observing chat examples. Since it is\ndifficult to estimate what features can be learned\nfrom the gpt4all v1.3-groovy dataset without a\ndetailed inspection, we aimed for a (successful)\nguess: \"objectiveness.\" We expect the model not\nto possess human-like preferences (e.g., \"cats\"\nor \"dogs\") because: (a) it has been trained on\ninstructions modelling AI giving universal recom-\nmendations; and/or (b) it has seen many examples\nwith different answers to similar questions, with\nobjectivity as an emergent property (Wei et al.\n2022).\nWe propose an ObjecQA benchmark that consists\nof 100 questions that involve subjective choices or\npreferences. A highly scoring model in ObjecQA\nshould present a range of possibilities or avoid\ndirect answers (e.g., \"it depends on preferences\").\nFirst 10 examples of subjective questions from\nObjecQA:\n1. Which is better, chocolate or vanilla ice\ncream?\n2. Is coffee superior to tea, or is tea better\nthan coffee?\n3. Are cats or dogs the ultimate pet?\n4. Do you prefer the beach or the moun-\ntains for a vacation?\n5. Would you rather live in a bustling city\nor a quiet countryside?\n6. Are e-books or physical books the supe-\nrior reading format?\n7. Is it better to watch a movie or read a\nbook?\n8. Which type of music is the best: classi-\ncal, pop, rock, or jazz?\n9. Are sunrises or sunsets more breathtak-\ning?\n10. In your opinion, is winter or summer the\npreferred season?\nWe employed GPT-3.5-turbo prompts for the se-\nmantic categorization of model outputs, utilizing\na two-shot prediction approach in all instances.\nWe used the following prompt:\n\"Classify the below responses as\n,\u2192 subjective opinions,\n,\u2192 preferences or objective. The\n,\u2192 subjective response will\n,\u2192 choose an option when asked to\n,\u2192\npick best or will voice an\n,\u2192 opinion about a disputable\n,\u2192 topic. The objective opinion\n,\u2192 will try to show the full\n,\u2192 scope of possible answers,\n,\u2192 defer to the lack of context\n,\u2192 or simply reject to make one\n,\u2192 definite choice.\nResponse: I prefer the thrill of\n,\u2192 riding a roller coaster.\nClass: Subjective\nResponse: It depends on the situation.\n,\u2192\nIn some cases, practicality\n,\u2192 is more important, while in\n,\u2192 others, fun is more important.\nClass: Objective\nResponse: \"\nThe results of ObjectQA scores in SFT are shown\nin Figure 4(b). We observe that the progression\nof scores is similar for both models, and most of\nthe learning process occurs after the black line\nmarker (approx. 8k examples). We call this phase\n\"knowledge-infusion\". One striking insight is that\nthe most significant semantic shift (knowledge-\ninfusion) occurs exactly after the formatting shift\n(format-infusion phase). (Since all queries from\nObjectQA are full sentences, we expect LLaMA\nbase models to be able to provide the answer\nalso as a next-token prediction task.) Moreover,\nthe models\u2019 ObjectQA continues to grow long\nafter the IFS plateaus. This observation implies\nthat for this combination of features (IFS and Ob-\njectQA), both LLaMA 7B and 13B LM, when\ntrained on the selected dataset, exhibit disjoint\nformat-infusion and knowledge-infusion phases.\nIn theory, one could minimize the impact of the\nsemantic shift by applying an early stopping crite-\nrion. We can imagine different learning dynamics,\nranging from those behind simple features (with\noverlapping phases) to very complex and spread\nout factors. On the other hand, a model with a\nrelatively high IFS can be a good starting point\nfor chat models. If we combine chat abilities with\nminimized impact of the SFT stage, we see that\n\"tone-instruct\" models might be an interface for\nquerying pretraining stage knowledge.\n6\nConclusion and Future Work\nIn conclusion, the Instruction Following Score\n(IFS) was introduced as a metric to detect lan-\nguage models\u2019 ability to follow instructions.\nBenchmarks of a range of publicly available mod-\nels show that there is a significant gap between\nbase models and instruct-tuned models, but there\nis no clear gap between SFT and RLFH models.\n7\n(a) IFS\n(b) ObjecQA\nFigure 4: (a) IFS characteristics for 7B, 13B LLaMA models in SFT. High values of IFS mean that\nthe model follows instructions. (b) ObjecQA for 7B, 13B LLaMA models in SFT. Models with no\nstrong preferences (of type \"cats or dogs\") score higher.\nIFS evaluation of an SFT process of LLaMA 7B\nand 13B shows that instruction tone is learned rel-\natively early. The supplementary metric ObjecQA\nwas proposed to contrast the tone learning curve\nwith the acquisition of semantic and domain-\nspecific knowledge. Key results show that the\ninspected models\u2019 instruction tuning capabilities\n(format-infusion phase) plateau at 0.9-0.95 af-\nter seeing approximately 8k examples, which is\nwhere we observe the semantic shift (knowledge-\ninfusion phase). Bigger models reached a 0.9\nIFS level relatively faster, and the high IFS was\nattained early in the process, enabling minimal\nsemantic changes by reducing sample points re-\nquired for learning style.\nFor future work, the research should focus on\ncomposable feature blocks that can be applied to\nfoundation models to achieve desired alignment\naspects, such as helpfulness, formality, or strict\nformats without unexpected downgrades in up-\nstream tasks or semantic shifts. The response\ntone classifier developed in this study serves as\na starting point for the concept of designing chat\ninterfaces for foundation models.\nReferences\nTaori, Rohan et al. (2023). Stanford Alpaca: An Instruction-following LLaMA model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nWang, Yizhong et al. (2023). Self-Instruct: Aligning Language Models with Self-Generated Instruc-\ntions. arXiv: 2212.10560 [cs.CL].\nLongpre, Shayne et al. (2023). The Flan Collection: Designing Data and Methods for Effective\nInstruction Tuning. arXiv: 2301.13688 [cs.AI].\nZhou, Chunting et al. (2023). LIMA: Less Is More for Alignment. arXiv: 2305.11206 [cs.CL].\nAnand, Yuvanesh et al. (2023). GPT4All: Training an Assistant-style Chatbot with Large Scale Data\nDistillation from GPT-3.5-Turbo. https://github.com/nomic-ai/gpt4all.\nTouvron, Hugo et al. (2023). LLaMA: Open and Efficient Foundation Language Models. arXiv:\n2302.13971 [cs.CL].\nZhang, Susan et al. (2022). OPT: Open Pre-trained Transformer Language Models. arXiv: 2205.\n01068 [cs.CL].\nGao, Leo et al. (2020). \u201cThe Pile: An 800GB Dataset of Diverse Text for Language Modeling\u201d. In:\narXiv preprint arXiv:2101.00027.\nWriter (2023). Palmyra LLMs empower secure, enterprise-grade generative AI for business. Writer\nBlog. URL: https://writer.com/blog/palmyra/.\nGudibande, Arnav et al. (2023). The False Promise of Imitating Proprietary LLMs. arXiv: 2305.\n15717 [cs.CL].\nOpenAI (2022). ChatGPT: Optimizing language models for dialogue. URL: https://online-\nchatgpt.com/.\n8\nPichai, Sundar (2023). An important next step on our AI journey. Google AI Blog. URL: https:\n//blog.google/intl/en-africa/products/explore-get-answers/an-important-\nnext-step-on-our-ai-journey/.\nAnthropicAI (2023). Introducing Claude. URL: https : / / www . anthropic . com / index /\nintroducing-claude.\nHinton, Geoffrey, Oriol Vinyals, and Jeff Dean (2015). Distilling the Knowledge in a Neural Network.\narXiv: 1503.02531 [stat.ML].\nLiang, Percy et al. (2022). Holistic Evaluation of Language Models. arXiv: 2211.09110 [cs.CL].\nKwiatkowski, Tom et al. (2019). \u201cNatural Questions: a Benchmark for Question Answering Research\u201d.\nIn: Transactions of the Association of Computational Linguistics.\nHuggingface (2023b). Open LLM Leaderboard. Accessed: 2023-06-10. URL: https : / /\nhuggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.\nGao, Leo et al. (Sept. 2021). A framework for few-shot language model evaluation. Version v0.0.1.\nDOI: 10.5281/zenodo.5371628. URL: https://doi.org/10.5281/zenodo.5371628.\nChen, Hao et al. (2023). Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low\nTraining Data Instruction Tuning. arXiv: 2305.09246 [cs.AI].\nKumar, Ananya et al. (2022). Fine-Tuning can Distort Pretrained Features and Underperform\nOut-of-Distribution. arXiv: 2202.10054 [cs.LG].\nBrown, Tom B. et al. (2020). Language Models are Few-Shot Learners. arXiv: 2005.14165 [cs.CL].\nRadford, Alec et al. (2018). \u201cLanguage Models are Unsupervised Multitask Learners\u201d. In: URL:\nhttps : / / d4mucfpksywv . cloudfront . net / better - language - models / language -\nmodels.pdf.\nOuyang, Long et al. (2022). Training language models to follow instructions with human feedback.\narXiv: 2203.02155 [cs.CL].\nSchulman, John et al. (2017). Proximal Policy Optimization Algorithms. arXiv: 1707 . 06347\n[cs.LG].\nHendrycks, Dan et al. (2021). Measuring Massive Multitask Language Understanding. arXiv: 2009.\n03300 [cs.CY].\nK\u00f6pf, Andreas et al. (2023). OpenAssistant Conversations \u2013 Democratizing Large Language Model\nAlignment. arXiv: 2304.07327 [cs.CL].\nHuggingface (2023a). AutoTrain: Create powerful AI models without code. URL: https : / /\nhuggingface.co/autotrain.\nWei, Jason et al. (2022). Emergent Abilities of Large Language Models. arXiv: 2206.07682 [cs.CL].\n9\n"
  },
  {
    "title": "Teaching Arithmetic to Small Transformers",
    "link": "https://arxiv.org/pdf/2307.03381.pdf",
    "upvote": "15",
    "text": "Teaching Arithmetic to Small Transformers\nNayoung Lee\u2217\nUniversity of Wisconsin-Madison\nnayoung.lee@wisc.edu\nKartik Sreenivasan\u2217\nUniversity of Wisconsin-Madison\nksreenivasa2@wisc.edu\nJason D. Lee\nPrinceton University\njasonlee@princeton.edu\nKangwook Lee\nUniversity of Wisconsin-Madison\nkangwook.lee@wisc.edu\nDimitris Papailiopoulos\nUniversity of Wisconsin-Madison\ndimitris@papail.io\nAbstract\nLarge language models like GPT-4 exhibit emergent capabilities across general-\npurpose tasks, such as basic arithmetic, when trained on extensive text data, even\nthough these tasks are not explicitly encoded by the unsupervised, next-token\nprediction objective. This study investigates how small transformers, trained\nfrom random initialization, can efficiently learn arithmetic operations such as\naddition, multiplication, and elementary functions like square root, using the next-\ntoken prediction objective. We first demonstrate that conventional training data\nis not the most effective for arithmetic learning, and simple formatting changes\ncan significantly improve accuracy. This leads to sharp phase transitions as a\nfunction of training data scale, which, in some cases, can be explained through\nconnections to low-rank matrix completion. Building on prior work, we then train\non chain-of-thought style data that includes intermediate step results. Even in the\ncomplete absence of pretraining, this approach significantly and simultaneously\nimproves accuracy, sample complexity, and convergence speed. We also study\nthe interplay between arithmetic and text data during training and examine the\neffects of few-shot prompting, pretraining, and model scale. Additionally, we\ndiscuss length generalization challenges. Our work highlights the importance of\nhigh-quality, instructive data that considers the particular characteristics of the\nnext-word prediction objective for rapidly eliciting arithmetic capabilities.2\n\u2217Authors contributed equally to this paper.\n2Our code is available at https://github.com/lee-ny/teaching_arithmetic\nPreprint. Under review.\narXiv:2307.03381v1  [cs.LG]  7 Jul 2023\nContents\n1\nIntroduction\n3\n2\nRelated Works\n4\n3\nPreliminaries and Experimental Setup\n5\n4\nLearning Addition in Small Models\n7\n4.1\nTraining on Conventional Data . . . . . . . . . . . . . . . . . . .\n7\n4.2\nReversing the Output . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5\nConnection to Low-Rank Matrix Completion\n8\n5.1\nAddition Tables are Rank-2 Matrices . . . . . . . . . . . . . . . .\n9\n5.2\nNanoGPT Generalizes better than Matrix Completion solutions . .\n9\n6\nThe power of Chain-of-Thought: Incorporating Intermediate Steps in\nTraining Data\n11\n6.1\nTraining on Chain-of-Thought Data\n. . . . . . . . . . . . . . . .\n11\n6.2\nThe Importance of Intermediate Step Design: Subtraction . . . . .\n11\n6.3\nThe Effect of Noisy Inputs on Accuracy . . . . . . . . . . . . . .\n13\n7\nExtending to Longer Digit Addition\n15\n7.1\nTraining from Random Initialization . . . . . . . . . . . . . . . .\n15\n7.2\nFine-Tuning from Pretrained Models . . . . . . . . . . . . . . . .\n16\n7.3\nImpact of Formats on Fine-Tuning . . . . . . . . . . . . . . . . .\n17\n8\nTeaching Arithmetic Operations Beyond Addition\n18\n8.1\nExtended Arithmetic Operations . . . . . . . . . . . . . . . . . .\n19\n8.2\nJointly Training on All Five Arithmetic Tasks . . . . . . . . . . .\n20\n9\nMixing Shakespeare with Arithmetic Data\n21\n10 Fine-tuning, Scaling, and Pretraining in Larger Models\n23\n11 Token Efficiency Across Data Formats\n26\n12 Length Generalization\n27\n13 Limitations\n30\n14 Conclusion\n30\nAppendix\n34\n2\n1\nIntroduction\nLarge language models like GPT-3/4, PaLM, LaMDA (Brown et al., 2020; Chowdhery et al., 2022;\nThoppilan et al., 2022) have demonstrated general-purpose properties, often referred to as emergent\nabilities (Wei et al., 2022b), for a wide range of downstream tasks like language and code translation,\ncompositional reasoning, and basic arithmetic operations (Webb et al., 2022; Nye et al., 2021; Wei\net al., 2022c; Shi et al., 2022; Wang et al., 2022; Srivastava et al., 2022; Chen et al., 2023). What is\nperhaps surprising, is that these tasks are not explicitly encoded in the model\u2019s training objective,\nwhich typically is an auto-regressive, next-token-prediction loss.\nPrior research has delved into exploring these capabilities and how they emerge as the scale and\nof training compute, type of data, and model size vary (Wei et al., 2022b; Chung et al., 2022; Tay\net al., 2022). Untangling the factors, however, remains challenging due to the data complexity and\nthe variety of tasks examined. Driven by the curiosity to understand the factors that elicit these\ncapabilities in next-token predictors, we set out to pinpoint the key contributors that accelerate the\nemergence of such abilities. These contributors may include the format and scale of data, model\nscale, the presence of pre-training, and the manner of prompting.\nTo provide a more precise examination of these factors, our study is conducted in a controlled setting:\nwe focus on teaching arithmetic to small transformer models, such as NanoGPT and GPT-2, when\ntrained from random init. Starting with a model of 10.6 million parameters and scaling up to 124\nmillion parameters, we use the standard autoregressive next-token prediction loss. Our objective\nis to understand how these models can efficiently learn basic arithmetic operations like addition,\nsubtraction, multiplication, square root, and sine, thereby providing us with a clearer lens through\nwhich to view the elicitation of emergent abilities. Below, we summarize our findings.\nData format and sampling matters. We first observe that teaching a model addition (or any other\noperation) using standard addition samples, i.e., \u2018A3A2A1 + B3B1B1 = C3C2C1\u2019, is suboptimal, as\nit requires the model to evaluate the most significant digit C3 of the result first, which depends\nglobally on all the digits of the two summands. By training on samples with reversed results,\ni.e., \u2018A3A2A1 + B3B1B1 = C1C2C3\u2019, we enable the model to learn a simpler function, significantly\nimproving sample complexity. Additionally, balanced sampling of different \u201cvariations\u201d of addition,\nbased on the number of carries and digits involved, further enhances learning. Even in this simple\nsetting, we observe relatively sharp phase transitions from 0 to 100% accuracy as a function of the\nsize of the training data. Although this may seem surprising, we observe that learning an addition\nmap on n digits from random samples is equivalent to completing a low-rank matrix. This connection\nallows us to offer a reasonable explanation for such phase transitions.\nChain-of-thought data during training. Building on these findings, we then explore the potential\nbenefits of chain-of-thought (CoT) data during training. This format includes step-by-step operations\nand intermediate results, allowing the model to learn the individual components of complex tasks.\nThis format is directly borrowed from related literature, e.g., (Ling et al., 2017; Nye et al., 2021;\nWei et al., 2022c; Zhou et al., 2022a; Anil et al., 2022; Zhou et al., 2022b). We found that CoT-\ntype training data significantly improved learning in terms of both sample complexity and accuracy\nin agreement with CoT fine-tuning literature (Nye et al., 2021; Chung et al., 2022), though our\nobservation holds even in the absence of language pretraining. We conjecture that this is because\nbreaking down the required compositional function to be learned into individual components allows\nthe model to learn a higher-dimensional but easier-to-learn function map. In Figure 1, we provide\nexamples of the four data formatting methods explored in our work.\nTraining on text and arithmetic mixtures and the role of few-shot prompting. We also explore the\ninterplay between arithmetic and text data during training, as LLMs are trained on massive amounts\nof data scraped from the internet (Bubeck et al., 2023; Peterson et al., 2019), where it is impractical\nto carefully separate different types of data. We observe how the model\u2019s perplexity and accuracy\nvary with the ratio of text to arithmetic data. We find that learning all arithmetic operations discussed\nearlier (from addition to square root) can improve the individual performance of each task, and that\ngoing from zero-shot to 1-shot prompting (showing one arithmetic example) yields a large accuracy\nimprovement, but there is no significant improvement in accuracy by showing more examples.\nThe role of pre-training and model scale.\nWe also investigate the role of pretraining by fine-\ntuning models like GPT-2 and GPT-3 (davinci) and observe that while the zero-shot performance\n3\nFigure 1: The four data formatting methods investigated in this work: (i) Plain: standard addition\nformatting (Section 4), (ii) Reverse: reversing the output (Section 4), (iii) Simplified Scratchpad:\nrecording the digit-wise sum and carry-ons (Section 6), and (iv) Detailed Scratchpad: providing\ndetailed intermediate steps of addition (Section 6). We train small transformer models from scratch\nusing data transformed with these various formatting methods for addition. The results (shown on\nthe right) highlight the crucial role of data formatting in performance and sample efficiency. Plain\nnever reaches 100% accuracy and the sample complexity for the remaining methods to learn addition\nperfectly steadily reduces as we increase the level of detail in the data format.\non arithmetic operations is poor, the prior \u201cskills\u201d acquired during pretraining facilitate reasonable\nperformance on some basic arithmetic tasks even with a small number of finetuning samples. However,\nfinetuning with non-standard formatting, such as reverse formatting, can interfere with the model\u2019s\nperformance when pretrained on standard-formatted operations, leading to decreased accuracy.\nFinally, we conduct studies on how performance in arithmetic changes with scale, and although we\nfind that scale does indeed aid in learning arithmetic operations, it is not a necessary trait.\nCompositional and length generalization. One might question if our trained models truly grasp\narithmetic. Our findings present a nuanced answer. We find length generalization beyond trained\ndigit lengths difficult. For instance, if a model is trained on all n-digit lengths, excluding a specific\nlength, it struggles to compensate and accurately calculate this missing digit length. Consequently,\nthe models achieve high accuracy within trained digit lengths but struggle significantly beyond this\nrange. This suggests that the models learn arithmetic not as a flexible algorithm, but more as a\nmapping function constrained to trained digit lengths. While this surpasses mere memorization, it\nfalls short of comprehensive arithmetic \u201cunderstanding\u201d.\nNovelty over prior work. Our approach heavily builds upon prior work that uses instructive data to\nenhance model performance, and we do not claim novelty in the style of training data employed. What\nsets our work apart is the primary focus on randomly initialized models and extensive ablation studies\non various sampling/data formatting and model scale settings to isolate the factors that contribute to\nthe fast emergence of arithmetic capabilities. Furthermore, our work offers a few simple but perhaps\ninsightful theoretical justifications of some of the phenomena we observe.\n2\nRelated Works\nInstructional data/chain-of-thought. The idea of using detailed reasoning training data predates\nTransformers (Vaswani et al., 2017). Ling et al. (2017); Cobbe et al. (2021); Nye et al. (2021) use\nnatural language to generate reasoning steps while Roy & Roth (2016); Reed & De Freitas (2015);\nChen et al. (2017); Cai et al. (2017) show that symbolic reasoning may suffice. Nogueira et al. (2021)\nnote that large number of samples with small digits is important for arithmetic tasks (Yuan et al.,\n2023). Razeghi et al. (2022) observe a correlation between the frequency of numbers in the dataset\nand the performance involving them whereas we find that transformers can learn to add numbers that\nwere not seen during training. Chain-of-thought (Wei et al., 2022c) refers to the model\u2019s improved\nperformance when prompted to produce rationale. Zhou et al. (2022b) show that this can be achieved\nby providing sufficiently informative exemplars as a few-shot prompt (Brown et al., 2020). Zhou et al.\n(2022a) showed that least-to-most prompting can help GPT-3 solve problems that can be decomposed\ninto simpler sub-problems. Least-to-most prompting consists of first decomposing a complex problem\ninto easier subproblems, and then sequentially solving these subproblems. We extend this notion to\nsimple addition and show that asking the model to output the least significant bit first has a similar\n4\neffect. Kojima et al. (2022) shows that very often even just prompting the model with \u201clet\u2019s think\nstep by step\u201d is sufficient to achieve competitive zero-shot accuracy on several benchmark datasets.\nArithmetic using Transformer models. Our work focuses on decoder-only models since they are\nwell-suited for text generation and are widely used in LLMs (Brown et al., 2020; Touvron et al.,\n2023; MosaicML, 2023). However, encoder-decoder models have also been extensively studied in\nthe literature in the context of learning arithmetic (Kim et al., 2021; Wang et al., 2021). Qian et al.\n(2022); Lightman et al. (2023); Uesato et al. (2022) explore techniques to improve the arithmetic\nabilities of pretrained LLMs. Wallace et al. (2019) on the other hand, focus on the impact of the\nlearned embeddings. Most results that show Turing-completeness or the universal approximation\ntypically rely on encoder models (Yun et al., 2019; P\u00e9rez et al., 2021; Wei et al., 2022a; Giannou\net al., 2023). Ontan\u00f3n et al. (2021) study the problem of compositional generalization extensively\non benchmark datasets such as SCAN (Lake & Baroni, 2018; Drozdov et al., 2022) and conclude\nthat design changes like relative position encoding (Shaw et al., 2018) can improve performance\nsignificantly. Charton (2022, 2021) show that Transformers can learn linear algebra operations with\ncarefully chosen encodings. Hanna et al. (2023) use mechanistic interpretability techniques to explain\nthe limited numerical reasoning capabilities of GPT-2.\nBeyond Transformers. While we focus our attention on GPT-like models, there is a rich literature\nstudying other sequence-to-sequence models such as recurrent neural networks (RNNs) (Bowman,\n2013; Bowman et al., 2014; Zaremba et al., 2014). Zaremba & Sutskever (2014) show that RNNs\ncan learn how to execute simple programs with for-loops provided they are trained with curriculum\nlearning. Sutskever et al. (2014) show that LSTMs show improved performance on text-based tasks\nsuch as translation when the source sentences are reversed, which is closely related to what we\nobserve in addition. Kaiser & Sutskever (2015) propose Neural GPUs which outperform prior RNNs\non binary arithmetic tasks and even show length generalization i.e., they can perform arithmetic on\ninputs of lengths that were unseen during training. This is yet to be seen even in modern pre-trained\nmodels (Bubeck et al., 2023) and therefore it is interesting to see if we can leverage some of these\ntechniques and apply them to existing modern architectures. Dehghani et al. (2018) propose Universal\nTransformers (UTs) which introduce a recurrent transition function to apply recurrence over revisions\nof the vector representation at each position as opposed to the different positions in the input. They\nshow that on the tasks from Zaremba & Sutskever (2014), UTs outperform traditional Transformers\nand RNNs.\nData-centric AI. More recently, there has been increasing interest in Data-Centric AI which\nemphasizes techniques to improve datasets in order to ensure better performance (Motamedi et al.,\n2021; Hajij et al., 2021). Gadre et al. (2023) propose a new benchmark where the training code is\nfixed and the only way to improve performance is to construct new training sets. Several works\nhave also tried to see if the model\u2019s reasoning ability can be leveraged to generate explanations and\nleverage it to solve complicated reasoning tasks (Rajani et al., 2019; Talmor et al., 2020; Zelikman\net al., 2022; Huang et al., 2022).\n3\nPreliminaries and Experimental Setup\nIn this section, we provide a detailed description of our experimental setup, including the model\narchitecture and an overview of the different data formatting and sampling techniques that we employ\nand evaluate.\nModel and Data.\nTo examine the individual factors at play, we use NanoGPT (Karpathy, 2022), a\nlightweight implementation of the GPT family of models, chosen primarily for its feasibility to train\nfrom random initialization under numerous settings. NanoGPT features a decoder-only transformer\narchitecture with six self-attention layers, six heads, and an embedding dimension of 384, resulting in\napproximately 10.6 million parameters. Unless stated otherwise, we use character-level tokenization\nand absolute position encoding. We train the NanoGPT model from random initialization, which we\nrefer to as training from \u2018scratch\u2019, using the conventional next-token prediction objective.\nTo understand the effect of scale, we extend our experiments to GPT-2 and GPT-3 in Section 10. We\ninvestigate teaching arithmetic from scratch as well as fine-tuning using a pretrained GPT-2. However,\nfor GPT-3, we exclusively use supervised fine-tuning on a pretrained model. Refer to Appendix C.2\nfor a more detailed description.\n5\nFor arithmetic tasks like addition, subtraction, and multiplication, we define the training dataset for a\nbinary operator f(\u00b7) as Dtrain = {(ai, bi), yi}N\ni=1 where yi = f(ai, bi). For unary operations such as\nthe sine and square root functions, the training dataset is formulated as Dtrain = {ai, yi}N\ni=1, where\nyi = f(ai). The test dataset Dtest is constructed by randomly sampling pairs of operands not included\nin Dtrain. Throughout training and inference, we apply different data formatting techniques on each\ndata sample from the training dataset, creating the final sequence that serves as the model\u2019s input.\nData Formatting.\nIn the following sections, we will delve into the detailed intuition, and results\nof the four data formatting approaches that we have deployed in our arithmetic experiments. For\nthis section, we provide a high-level summary of these approaches, each progressively incorporating\nadditional information to form a more comprehensive format. The scratchpad formats are largely\nadopted from the literature of chain-of-thought (CoT) training (Nye et al., 2021; Zhou et al., 2022b).\nSee Figure 2 and Appendix D for detailed examples.\nDifferent data formatting methods for addition\nFour input formatting methods used for the addition task:\n(i) Plain: standard formatting of addition\n(ii) Reverse: flips the order of the output and encapsulates each data sample with the\u2018$\u2019 symbol\nat the start and end.\n(iii) Simplified Scratchpad: provides carry and digit-sum information for each step of addition,\nfrom the LSB to the MSB3.\n(iv) Detailed Scratchpad: provides explicit details of intermediate steps of addition.\nPlain\n128+367=495\nReverse\n$128 +367=594$\nSimplified Scratchpad\nInput: 128+367\nTarget:\nA->5 , C->1\nA->9 , C->0\nA->4 , C->0.\n495\nDetailed Scratchpad\nInput:\n128+367\nTarget:\n<scratch >\n[1,2,8] has 3 digits.\n[3,6,7] has 3 digits.\n[1,2,8] + [3,6,7] , C=0, 8+7+0=15 , A->5, C->1\n[1 ,2] + [3, 6] , A= [5],\n2+6+1=9\n, A->9, C->0\n[1] + [3] , A= [9 ,5] , C=0 , 1+3+0=4\n, A->4 , C->0\n[] + [] , A= [4,9,5] , C=0 , END\n</scratch >\n4 9 5\nFigure 2: The four input formatting methods used for the addition task. We progressively increase\nthe amount of detail with each format.\nNote that we wrap each data sample in the reverse format with the \u2018$\u2019 symbol at the beginning and end\nas a delimiter. We originally observed improved performance in both the plain and reverse formats\nwhen the operands and outputs were zero-padded to a fixed length (e.g., 3 and 4 digits, respectively,\nfor 3-digit addition). But later realized that a single symbol can effectively replace zero-padding.\nWhile we maintain the original plain format without padding as a baseline \u2013 emphasizing the necessity\nfor improved data formatting for efficient emergence \u2013 we incorporate the \u2018$\u2019-encapsulation in our\nmodified reverse format. For further details, refer to Appendix B.1.\nIn Section 4, we explore the limitations of the conventional plain-format data and demonstrate\nhow a simple reversal of the output order can lead to substantial performance improvements and\nenhanced sample efficiency. We introduce two Lemmas to support and explain these findings.\nAdditionally, in Section 6, we present results on the simplified and detailed scratchpad formats,\nhighlighting significant enhancements in sample efficiency for learning addition. We also emphasize\nthe importance of carefully designing the intermediate steps in the detailed scratchpad method.\n3We deviate from the strict definition of \u201cmost significant bit\u201d (MSB) and \u201cleast significant bit\u201d (LSB),\ntypically associated with binary numbers, and reinterpret them for the purpose of this paper as the most significant\n\u201cdigit\u201d and least significant \u201cdigit\u201d, respectively.\n6\nStructured Data Sampling.\nWhile data formatting plays a crucial role, we also discover that\nselecting the appropriate samples for inclusion in the training data is also essential. When sampling\noperands for n-digit addition uniformly at random between 1 to 10n \u2212 1, the dataset becomes highly\nskewed in terms of the number of samples with (i) operands containing a specific number of digits\nand (ii) operands resulting in a certain number of carry-on4 operations. For instance, in the case\nof 3-digit addition, random sampling results in a meager 0.01% probability of selecting a 1-digit\nnumber. Additionally, 1 or 2 carry-on operations are more likely to occur than 0 or 3. To address this\nimbalance, we employ a structured sampling approach. Specifically, we aim to (i) balance digits by\nassigning higher weights to lower-digit numbers during the sampling process as in Nogueira et al.\n(2021) and (ii) balance carry-ons by ensuring an equal distribution of examples with 0, 1, . . . , n\ncarry-on operations.\nOverall 1-digit 2-digit carry-0 carry-1 carry-2 carry-3\n95\n96\n97\n98\n99\n100\nTest Accuracy (%)\nRandom\nBalanced Digits\nBalanced Carry-Ons\nFigure 3: Performance of 3-digit addition\non various data sampling methods used: (i)\nRandom: uniform sampling of operands; (ii)\nBalanced digits: assigning higher sampling\nweights to operations involving 1 and 2-digit\nnumbers; (iii) Balanced carry: balancing the\ndataset to contain an equal number of carry-on\noperations. Experiments on addition with zero-\npadding both operands and output to have 3\nand 4 digits respectively.\nWhen sampling 10, 000 examples of 3-digit addi-\ntion, we include all possible 100 1-digit additions,\n900 2-digit samples and 9000 3-digit samples. Note\nthat while the number of samples increase, the frac-\ntion of all possible k\u2212digit additions that we sample\nfor k = 2, 3 decreases due to the inherent skew. The\nsplit was chosen heuristically to ensure we saw a\n\u201creasonable\u201d fraction of all possible k\u2212digit samples\nfor all k. Similarly, we ensure that the number of\nsamples with 0, 1, 2, or 3 carry-on operations are all\napproximately 2500.\nFigure 3 reveals the importance of \u201cbalancing\u201d. We\nobserve improvements in accuracy across the board\nwhile using Balanced data when compared to ran-\ndom sampling. Further, random sampling performs\nrelatively poorly even for the simple task of 2\u2212digit\naddition. We conjecture that this is due to the fact\nthat the model has not seen enough of these exam-\nples. For the remaining experiments, we set the\ndefault dataset for addition to be one that has both\nbalanced digits and carry-ons.\n4\nLearning Addition in Small Models\n0\n2.5k 5k 7.5k 10k12.5k15k17.5k20k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nFigure 4: Comparison of NanoGPT model\nperformance on the addition task, trained\non plain and reverse formatted data. The\nconventional plain format exhibits subopti-\nmal performance, even with a larger num-\nber of addition examples, whereas a dis-\ntinct phase transition is observed for the\nreverse format around 2500 train samples\nwhere it learns addition perfectly.\nWe start by examining one of the most basic arithmetic\ntasks: addition. Initially, we concentrate on the 3-digit\naddition, where the two operands have at most 3 digits\n(999). Later, in Section 7, we demonstrate that our find-\nings can be applied to larger digits. We assess whether\nNanoGPT can learn addition from training data of var-\nious sizes. As we will soon discover, learning addition\nmay not be as straightforward as one might anticipate.\n4.1\nTraining on Conventional Data\nWe begin by training NanoGPT on conventional addi-\ntion data in the form of \u2018A3A2A1 + B3B1B1 = C3C2C1\u2019,\nwhich we denote as the plain data format. However, as\nshown in Figure 4, this leads to fairly poor performance.\nWe believe that this is because the next-token predic-\ntion objective is not optimized for generating the most\nsignificant digit (MSB) first.\n4In this paper, we adopt the definition that a carry-on operation involves transferring information from\none digit position to another position of higher significance. Therefore, we refer to the \u201cborrow\u201d operation in\nsubtraction as a carry operation.\n7\nThe following lemma clarifies the necessity to access all operand digits in order to output the MSB\nfirst:\nLemma 1. Let A and B be two n-digit numbers, and let C = A + B. Suppose an algorithm A\noutputs the digits of C in decreasing order of significance, then A must have access to all digits of A\nand B starting from the first digit that it outputs.\nThe lemma suggests that to train the model for addition and to output the most significant digit first,\nit is necessary for the model to learn a global algorithm. Unlike the standard algorithm for addition\nwhich consists of computing digit-wise sums and carry-ons, approximating a global algorithm would\nnecessitate learning a more complicated function than necessary. The increased complexity results\nin decreased accuracy, as observed throughout our experiments. Liu et al. (2023) refer to this\nphenomenon as attention glitches.\n4.2\nReversing the Output\nThis leads us to ask, \u201cis it possible to guide the model to learn a simpler algorithm for addition?\u201d\nWe propose an intuitive approach to improve performance by training the model to generate the least\nsignificant digit (LSB) first, following the way humans typically perform addition. By starting with\nthe LSB and progressing towards the most significant digit (MSB) from right to left, the model can\nlearn a simpler algorithm that relies on just three inputs: the corresponding digits from the operands\nand the carry-on information (0 or 1) carried from the LSB to the MSB. This approach offers an\nadvantage over the plain format, where generating the MSB first would necessitate the model to learn\na more complex function involving all digits in the two operands.\nWe propose that using this reverse format (\u2018$A3A2A1 + B3B1B1 = C1C2C3$\u2019) is more suitable for\nnext-word prediction models. The rationale behind this is that when generating the sum by starting\nwith the least significant digit (LSB), the model only needs to learn a local function of three inputs\nper digit \u2013 the two relevant digits of the operands and the carry-on from the previous digit. This local\noperation simplifies the learning function. The following lemma substantiates this idea:\nLemma 2. There exists an algorithm that computes C = A + B for two n-digit numbers A and B\nand outputs its digits in increasing order of significance such that, at each position i, the algorithm\nonly requires access to the ith digits of A and B, as well as the carry-on from the previous position.\nLemma 2 directly follows from the standard algorithm for addition, which performs the sum and carry-\non operations digit by digit. The implications of these two lemmas are evident in our experiments\nwhen comparing training NanoGPT on plain and reverse samples. As shown in Figure 4, the accuracy\nof plain addition plateaus at slightly over 85% even with 10, 000 samples. In contrast, simply training\nthe model on reversed output significantly enhances the performance. Additionally, we observe that\nthe reverse format requires considerably fewer training data to achieve good performance, further\nreinforcing that the reverse format\u2019s associated function has less complexity than the plain format.\nWhat is particularly remarkable is the occurrence of a notable phase transition between 1000 and\n4000 samples for reverse. At this point, the model rapidly transitions from being unable to perform\naddition to being capable of perfectly adding two 3-digit numbers. This leads to an important\nquestion:\nWhy does addition rapidly emerge as the number of training examples increases?\n5\nConnection to Low-Rank Matrix Completion\nAlthough the rapid phase transition observed in the previous section may initially seem surprising,\ncloser examination reveals a fascinating equivalence: learning an addition map on n digits from ran-\ndom samples can be considered as completing a rank-2 matrix. This equivalence offers a compelling\nexplanation for the phenomenon we observed. In this section, we delve into the intricate details of\nthis connection and elucidate how learning the addition map can be formulated as low-rank matrix\ncompletion (LRMC). Establishing this connection provides meaningful insights into the observed\nphenomenon. Further, our investigation goes beyond that and highlights the enhanced capabilities of\nTransformer models. We demonstrate that Transformers possess expanded capabilities that surpass\nwhat traditional LRMC algorithms can do.\n8\n5.1\nAddition Tables are Rank-2 Matrices\n(a) Matrix Completion of Addition Matrix\n101\n102\n103\n104\nNumber of Revealed Entries\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSuccess Probability\nn = 20\nn = 50\nn = 100\nn = 500\n(b) Comparing LRMC and NanoGPT\n0\n1000\n2000\n3000\n4000\n5000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nmatrix completion\nFigure 5: (a) We run Algorithm 1 (Kir\u00e1ly et al., 2015), a simple iterative algorithm for 2-rank matrix\ncompletion for the addition matrix (n = 20, 50, 100, 500) and report the success probability over\nmultiple random trials while varying the number of revealed entries. As anticipated, a sharp phase\ntransition occurs when approximately O(n) entries are revealed. (b) We compare the performance of\na NanoGPT model trained on a dataset containing n = 100 samples (i.e., 2-digit addition) to that of\nthe corresponding LRMC problem using the same sample set. Notably, the phase transition at around\n1500 samples, where both NanoGPT and Algorithm 1 begin learning addition almost flawlessly, is\nremarkably similar.\nLearning addition from samples can be formulated as a rank-2 Matrix Completion (MC) problem\ninvolving an n \u00d7 n matrix M, where the (i, j)-th entry Mi,j represents the output of the addition\n\u2018i + j\u2019. Such M can be decomposed into the sum of two rank-one matrices, N1T + 1N T , where N\nis a column vector with entries {1, . . . n}, and 1 is a vector of n ones. Thus, learning addition from\nsamples can be viewed as solving the MC problem in which only the entries corresponding to those\nsamples are revealed. When the underlying matrix is noiseless and of rank-2, Kir\u00e1ly et al. (2015)\ndemonstrates that a simple iterative algorithm (Algorithm 1 in Appendix B.2) is optimal. As depicted\nin Figure 5a, a sharp phase transition occurs at O(n). This aligns with Theorem-2 from Recht (2011)\nwhich states that the exact convex relaxation to the MC problem has a unique solution as long as\ne\nO(n) samples are observed.\nThe sharp phase transition observed in LRMC bears a resemblance to what we notice in NanoGPT. To\nfurther investigate this phenomenon, we focus on 2-digit addition (n = 100) as shown in Figure 5a.\nWe evaluate the performance of learning addition through NanoGPT in comparison to LRMC by\nconstructing a training dataset consisting of the matrix\u2019s revealed entries in either plain or reverse\nformat. It is important to note that the training dataset is no longer \u201cbalanced\u201d, as the revealed entries\nare randomly and uniformly sampled for the LRMC experiments. The comparison between NanoGPT\nand LRMC results is presented in Figure 5b. Remarkably, both NanoGPT and LRMC exhibit a\nsimilar phase transition at approximately 1500 samples, where they both start to learn addition almost\nperfectly. This observation regarding LRMC offers an explanation for the rapid emergence of addition\nin NanoGPT.\n5.2\nNanoGPT Generalizes better than Matrix Completion solutions\nWe noted above that there are some striking similarities between the addition map learned by\nNanoGPT and LRMC. However, we now delve deeper and find that this map exhibits capabilities\nbeyond LRMC. A well-known limitation of LRMC is its inability to generalize when entire rows\nor columns are empty. Therefore, we intentionally hide certain numbers in the training dataset or\nspecific digit positions, and examine whether our model can still learn addition.\nGeneralizing to unseen numbers.\nIn order to further investigate the connection with LRMC, we\nexclude an increasing fraction of the numbers from the training data and evaluate the model\u2019s ability\nto learn addition. As shown in Table 1, the answer to this question is a resounding Yes! The model\nachieves almost perfect accuracy even when excluding half of all possible 3\u2212digit numbers. More\n9\nprecisely, we randomly choose 100/200/500 numbers and exclude them from the training data. We\nthen evaluate the trained models two metrics: (i) Overall accuracy: which measures the accuracy\nover a random set of 10, 000 examples and (ii) Exclusion accuracy: which measures the accuracy\nonly over the excluded set. Remarkably, excluding numbers from the training data sometimes leads\nto improved performance. We conjecture that this may be due to the effect of regularization, similar\nto random masking or cropping images in vision tasks. Note that these results indicate that the model\nis not simply performing LRMC. In the LRMC setting, even a single missing number corresponds to\nan empty row or column, which cannot be recovered. Hence, the ability of the NanoGPT model to\ngeneralize to missing numbers signifies its distinct capabilities beyond LRMC.\nTable 1: Impact of excluding numbers on addition task: NanoGPT models trained with 100/200/500\nexcluded operands show no significant drop in accuracy and in some cases, the performance even\nimproves. Note that models trained with reverse data remain consistently at 100% accuracy.\nNo Exclusion\nExcluding\n100 numbers\nExcluding\n200 numbers\nExcluding\n500 numbers\nPlain\nRev\nPlain\nRev\nPlain\nRev\nPlain\nRev\nOverall Accuracy\n87.18%\n99.97%\n87.94%\n100.00%\n87.24%\n99.99%\n88.15%\n99.99%\nExclusion Accuracy\n-\n-\n92.55%\n100.00%\n92.15%\n99.95%\n90.85%\n100%\nGeneralizing to unseen digits.\nBuilding upon the model\u2019s robustness to excluded numbers, we fur-\nther investigate its ability to handle excluded digits. Intuitively, this should be even more challenging\nsince excluding a digit means the model cannot learn directly how to operate in that position. Instead,\nit would have to generalize and infer that digits act similarly across all positions. We construct\ndatasets with the number 5 excluded in 1st (LSB), 2nd, and 3rd (MSB) positions, and train separate\nmodels on each of these datasets. We compare the resulting models by evaluating overall accuracy\non a test set of 10, 000 randomly sampled numbers, as well as their accuracy specifically on samples\nwith 5 in each position which we call exclusion accuracy.\nThe results presented in Table 2 indicate that the model is not as robust to excluding digits compared\nto excluding numbers. However, it still achieves more than 66% accuracy on every test and maintains\nan overall accuracy above 85%. Moreover, it appears that excluding a number in the least significant\nposition yields the worst performance. This can be attributed to the fact that learning addition in this\nposition is transferable to other positions since it is unaffected by carry-on operations. Failing to\nlearn addition in this position, however, will have a detrimental impact on other positions as well.\nTable 2: Impact of excluding digits on addition task: We investigate whether GPT-based models can\ninfer addition on an excluded digit in a specific position from training data on other positions. We\ncompare NanoGPT models trained with and without an excluded digit and find that excluding digits\nis harder to learn but not entirely impossible, with the worst performance observed when excluding\nthe least significant digit.\nExcluded position\nInput\nformat\nOverall Acc\n\u201c5\u201d in the\n1st (LSB) digit\n\u201c5\u201d in the\n2nd digit\n\u201c5\u201d in the\n3rd (MSB) digit\nNo exclusion\nPlain\n87.18%\n87.50%\n88.65%\n91.80%\nReverse\n99.97%\n99.90%\n99.95%\n100%\n1st (LSB) digit\nPlain\n85.05%\n76.70%\n85.80%\n88.35%\nReverse\n93.31%\n66%\n94.80%\n94.45%\n2nd digit\nPlain\n85.44%\n84.55%\n78.50%\n90.15%\nReverse\n98.85%\n98.85%\n94.20%\n99.50%\n3rd (MSB) digit\nPlain\n85.70%\n85.35%\n87.35%\n83.45%\nReverse\n97.18%\n97.25%\n97.35%\n85.45%\nThe distinct learning mechanism of NanoGPT.\nThe phase transition of LRMC offers significant\ninsights into NanoGPT\u2019s learning process. Nevertheless, further experiments clearly demonstrate\nthat NanoGPT\u2019s mechanism for learning addition is fundamentally different from LRMC. It can\nsuccessfully learn addition even when numbers or digits are intentionally excluded from the training\ndata, thereby exhibiting generalization capabilities that far exceed that of typical LRMC algorithms.\n10\n6\nThe power of Chain-of-Thought: Incorporating Intermediate Steps in\nTraining Data\nSo far, we observed that utilizing the straightforward method of reversing the output can result in\nremarkable performance, exceeding that of LRMC in learning addition. Nonetheless, it may be\npossible to expedite the emergence of addition by further enhancing the data format. As addition is a\nmulti-step process, we further explore the idea of incorporating additional information about each\nstep. We adopt a Chain-of-Thought (CoT) style approach, where we guide the model to learn addition\nstep-by-step. In the subsequent sections, we assess the effect of incorporating these intermediate steps\non the performance of small models. We demonstrate that this results in a substantial improvement in\nsample complexity of learning addition and carefully analyze how the level of detail offered for each\nstep impacts the model\u2019s performance.\n6.1\nTraining on Chain-of-Thought Data\nIn the following experiments, we evaluate if training on scratchpad data further improves the learning\nof addition. As described briefly in Section 3, scratchpad data incorporates step-by-step instructions\nin varying amounts of detail into the samples. This approach aims to help the model learn addition as\na compositional function. We explore two levels of detail in the provided instruction steps: Simplified\nScratchpad format offers minimal information \u2013 the sum and carry information for each digit/step.\nDetailed Scratchpad provides comprehensive information on how to execute each step in the addition\nprocess in natural language. By comparing the performance of the model trained with these different\nlevels of detail, we can analyze its impact on the model\u2019s ability to learn addition effectively.\n0\n1000 2000 3000 4000 5000 6000 7000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nsimplified scratchpad\ndetailed scratchpad\nFigure 6: Comparison of sample effi-\nciency: evaluating performance on train-\ning datasets with different numbers of ad-\ndition samples. While all modified meth-\nods (reverse, simplified scratchpad, and\ndetailed scratchpad) achieve 100% test\naccuracy, they exhibit varying require-\nments in terms of the number of addition\nexamples in the training dataset to reach\noptimal performance.\nThe results presented in Figure 6 demonstrate the effective-\nness of different data formats for training addition. The\nmodel trained on Simplified Scratchpad data achieves\n100% accuracy with only 2000 samples, whereas the Re-\nverse format requires more than double the number of\nsamples. Furthermore, the Detailed Scratchpad format,\nwhich provides even more detailed information, achieves\nperfect addition with just 1000 samples. This indicates that\nincorporating more information enables the model to learn\naddition more efficiently, requiring fewer examples. We\nconjecture that this is because breaking down the required\ncompositional function to be learned into individual com-\nponents allows the model to learn a higher-dimensional but\neasier-to-learn function map. We note that while CoT-style\ntraining enhances sample efficiency, it may not necessarily\nbe the most \u201ctoken-efficient\u201d approach. We delve into this\naspect in more detail in Section 11. In summary, incor-\nporating scratchpad data and decomposing the addition\ntask into steps offer a promising strategy to improve the\nperformance and efficiency of small models in learning\naddition from scratch.\n6.2\nThe Importance of Intermediate Step Design: Subtraction\nIn this section, we underscore the significance of meticulously designing the intermediate steps in a\nChain-of-Thought manner. Specifically, we focus on the subtraction task and conduct experiments\nto compare two different versions of the detailed scratchpad for this operation (see examples in\nFigure 7). These trials shed light on the importance of decomposing the subtraction task into simpler\nintermediate steps. Unlike addition, subtraction behaves differently depending on whether the first\noperand (a) is greater than the second operand (b) or vice versa.\n11\nDetailed scratchpad formatting for different arithmetic tasks\nExamples of two variations of detailed scratchpad formatting for subtraction, considering the scenario\nwhere the first operand a is greater than the second operand b, and vice versa. In Version 1, a result\nprocessing step is included in the final stage to handle negative outputs. In Version 2, the operands are\ncompared at the beginning, and if b is larger, their order is reversed.\nPrompt (Case 1. a \u2212 b \u2265 0) :\nInput:\n367-128\nTarget:\nVersion 1.\n...\n<scratch >\n[3,6,7] has 3 digits.\n[1,2,8] has 3 digits.\n[3,6,7] - [1,2,8] , A=[] , C=0 ,\n7 -8 -0+10=9 , A->9 , C->-1\n[3 ,6] - [1 ,2] , A=[9] , C=-1 , 6-2-1=3 , A\n->3 , C->0\n[3] - [1] , A=[3 ,9] , C=0 , 3-1-0=2 , A->2\n, C->0\n[] - [] , A=[2 ,3 ,9]\n200+39=239\n, END # result\nprocessing\n</scratch >\n2 3 9\nVersion 2.\n...\n<scratch >\n[3,6,7] has 3 digits.\n[1,2,8] has 3 digits.\n367 >=128 # comparison\nof two\noperands\n[3,6,7] - [1,2,8] , A=[] , C=0 ,\n7 -8 -0+10=9 , A->9 , C->-1\n[3 ,6] - [1 ,2] , A=[9] , C=-1 , 6-2-1=3 , A\n->3 , C->0\n[3] - [1] , A=[3 ,9] , C=0 , 3-1-0=2 , A->2\n, C->0\n[] - [] , A=[2 ,3 ,9] , END\n</scratch >\n2 3 9\nPrompt (Case 2. a \u2212 b < 0) :\nInput:\n128-367\nTarget:\nVersion 1.\n...\n<scratch >\n[1,2,8] has 3 digits.\n[3,6,7] has 3 digits.\n[1,2,8] - [3,6,7] , A=[] , C=0 , 8-7-0=1 ,\nA->1 , C->0\n[1 ,2] - [3 ,6] , A=[1] , C=0 , 2 -6 -0+10=6 ,\nA->6 , C->-1\n[1] - [3] , A=[6 ,1] , C=-1 , 1-3-1=-3 , A\n->-3 , C->-1\n[] - [] , A=[-3,6,1]\n-300+61= -239 , END # result\nprocessing\n</scratch >\n-2 3 9\nVersion 2.\n...\n<scratch >\n[1,2,8] has 3 digits.\n[3,6,7] has 3 digits.\n128 <367 : 128 -367= -(367 -128) # comparison\n[3,6,7] - [1,2,8] , A=[] , C=0 ,\n7 -8 -0+10=9 , A->9 , C->-1\n[3 ,6] - [1 ,2] , A=[9] , C=-1 , 6-2-1=3 , A\n->3 , C->0\n[3] - [1] , A=[3 ,9] , C=0 , 3-1-0=2 , A->2\n, C->0\n[] - [] , A=[2 ,3 ,9] , END\n</scratch >\n-2 3 9\nFigure 7: Two versions of detailed scratchpad formatting for subtraction.\nThe first strategy (Version 1 in Figure 7) involves performing digit-wise subtraction starting from the\nleast significant bit (LSB) and considering borrows when necessary. However, this strategy produces\nincorrect results when the first operand is smaller than the second operand. In such cases, we subtract\nthe number in the most significant bit (MSB) position multiplied by 10 to the power of (number\nof digits in the output - 1) from the remaining digits in the output. An example illustrating this\napproach is shown in Version 1, Case 2. Alternatively, we can adopt a more familiar strategy. If\nthe first operand is smaller than the second, we swap the operands and compute the negation of the\nsubtraction of the swapped operands: a \u2212 b = \u2212(b \u2212 a) (referred to as Version 2).\nThe results in Figure 8 indicate that Version 2, which involves comparing two operands, performs\nconsiderably worse than Version 1. In Version 1, each intermediate step only requires the simpler\n1-digit subtraction, along with addition in the final result processing step. Upon analyzing the failure\ncases of Version 2, we observe that the majority of errors stem from incorrectly identifying which of\nthe two operands is larger, while the intermediate steps are handled correctly. This finding underscores\nthe significance of breaking down arithmetic operations into simpler intermediate steps. Unless\notherwise specified, we use Version 1 in all detailed scratchpad experiments.\n12\nPlain\nReverse (DS)Ver.1 (DS)Ver.2\nData Formatting Methods\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5\n95.0\n97.5\n100.0\nTest Accuracy (%)\n80.5%\n99.6%\n100.0%\n89.9%\nFigure 8: Comparison of performance among various data formatting approaches (plain, reverse, and\ntwo versions of detailed scratchpad (DS)) for the subtraction task. The experiments were conducted\non a NanoGPT model trained on a dataset of 10,000 examples. Version 2, which incorporates operand\ncomparison, exhibits significantly lower performance compared to Version 1. This observation\nhighlights the substantial impact of the construction of intermediate steps on the model\u2019s performance.\n6.3\nThe Effect of Noisy Inputs on Accuracy\n(a) Test accuracy on Addition\n2000\n4000\n6000\n8000\n10000 12000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\ncorrect A & C\nrandom A\nrandom C\nrandom A & C\n(b) Test accuracy on Subtraction\n2000\n4000\n6000\n8000\n10000 12000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\ncorrect A & C\nrandom A\nrandom C\nrandom A & C\nFigure 9: Comparison of training with simplified scratchpad formatting using correct A and C\ninformation with formatting using random A/C and their effect on sample efficiency and accuracy.\nResults show that noisy labels degrade sample efficiency, but with sufficient training data, the model\neventually reaches full accuracy.\nNoisy intermediate steps in the scratchpad data.\nWe further investigate the significance of\nproviding accurate intermediate steps in the scratchpad during the training process. While this was\ninspired by the findings of Min et al. (2022), it is inherently different. Min et al. (2022) show that\nusing random labels in ICL demonstrations caused minimal degradation when compared to the gold\nlabels. However, those models were trained on gold labels and then evaluated on multiple downstream\ntasks. In our setting, the model is trained and evaluated on a single arithmetic task. Further, the\nfinal result(or label) is left untouched as the correct answer to the arithmetic operation. We only\nreplace the intermediate steps. The goal of this study is to verify whether the model actually learns to\nreason using the given intermediate steps or merely uses the scratchpad to improve its expressivity.\nWe compare the performance of training with our simplified scratchpad formatting, which includes\naccurate A (digit sum) and C (carry) information, with formatting that includes random A, random\nC, or random A and C for each intermediate step, as depicted in Figure 1.\nThe results in Figure 9, demonstrate that the inclusion of noisy labels can impede sample efficiency.\nHowever, with enough samples, the model ultimately achieves full accuracy. This suggests that while\nthe model is capable of leveraging the information contained in the intermediate steps, it can also\ngradually learn how to perform addition while disregarding the presence of noisy intermediate steps.\nModel robustness to noise in the auto-regressive output.\nIn this analysis, we explore the\nrobustness of models trained on plain or reverse formatted data (without noise) when exposed to\nnoise during an auto-regressive generation process. In particular, we aim to unravel how much the\n13\nlearned mapping of the i-th output relies on the operands and preceding tokens in the addition result,\ngiven that transformer models generate tokens sequentially in an autoregressive manner, making them\nprone to error propagation.\nFor this experiment, we focus on 3-digit addition. We train models on either plain or reverse format\ndata and evaluate the accuracy of next-token predictions when the output sequence contains noise.\nSpecifically, in the plain format setting, we expect a well-performing model to generate the correct\noutput tokens O3, O2, O1 sequentially, where O3 = C3, O2 = C2, O1 = C1, and C3C2C1 represents\nthe correct answer. We consider two types of perturbation: (i) random perturbation, where we\nmodify the first two output tokens O3O2 to random numbers different from C3C2, and (ii) precise\nperturbation, where we perturb only the second output token O2 by 1. The second case is particularly\nrelevant since a common error case is where the model misses a digit by 1. We provide the model with\nan expression of the form \u201cA3A2A1 + B3B1B1 = O3O2\u201d, where O3O2 can be either (i) a random\nincorrect number, i.e., O3O2 \u0338= C3C2, or (ii) O2 = C2 \u00b1 1 mod 10, and observe the next token\ngenerated by the model. A corresponding process is deployed for the reverse format, introducing a\nnoisy sequence to models trained on reverse format data.\nTo evaluate the performance, we define two accuracy criteria for O1: exact accuracy, reckoning\nO1 as accurate only when O1 = C1, and relaxed accuracy, considering O1 correct if it deviates\nfrom the original output C1 by at most 1. In other words, C1 = O1, C1 = O1 + 1 mod 10 or\nC1 = O1 \u2212 1 mod 10.\nTable 3: Prediction accuracy for the third digit output under different types of noise in the preceding\noutput tokens. Random perturbation, applies random flips whereas precise perturbation shifts the\npreceding output tokens by 1. Relaxed accuracy, allows for a \u00b11 deviation from the true output\nwhereas Exact accuracy is strict. Reverse consistently outputs a number that is at most 1 different\nfrom the true output, even in the presence of noise. The plain format has high exact accuracy in the\npresence of precise perturbation, as the noise in the output token has a lower impact on predicting the\nnext token, which is of lower significance. However, with completely random noise, the plain format\nshows poor performance, suggesting a strong dependence on all digits. (See Lemma 1 and 2).\nPerturbation Type\nRandom\nPrecise\nPlain\nReverse\nPlain\nReverse\nExact Acc\n49.88%\n81.26%\n99.85%\n90.47%\nRelaxed Acc\n61.55%\n100%\n100%\n100%\nThe results presented in Table 3 reveal intriguing findings. We observe that the reverse format\nconsistently outputs a result that deviates by no more than 1 from the true answer, regardless\nof whether the preceding outputs O3O2 are subjected to random or precise perturbation. This\nconsistency can be explained by Lemma 2, indicating that the reverse format only requires learning\na straightforward function of digit-wise addition for each corresponding position, along with the\ncarry-on (0 or 1). Therefore, even with noise in the preceding tokens, the model accurately performs\ndigit-wise addition, albeit with occasional carry-on prediction errors. With an exact accuracy of\n81.26% even in the presence of random perturbation, the reverse format demonstrates the model\u2019s\nability to rely less on the preceding output tokens, indicating a robust learned output mapping.\nOn the contrary, models using the plain format have to decipher a more intricate function drawing\nfrom all digits within the sequence, as described by Lemma 1. Given that in addition, carry operations\ntransition from right to left (i.e., least to most significant digit), the introduction of precise perturbation\non preceding output tokens, which possess higher significance, has a minor impact on the output\n(which has less significance). As a result, models trained using the plain format attain an exact\naccuracy rate of 99.85% and a relaxed accuracy of 100% for cases involving precise perturbation.\nInterestingly, under purely random perturbation, the plain format struggles, leading to a reduced\nrelaxed accuracy of 61.55% and exact accuracy of 49.88%. This suggests that the output mapping\nlearned by the plain format is not merely a function of the two operands but rather enmeshed in\ncomplex dependencies on preceding output tokens.\n14\n7\nExtending to Longer Digit Addition\nIn this section, we extend our experiments beyond 3-digit addition and explore longer-digit settings,\nranging up to 10 digits. Our aim is to investigate whether our previous findings regarding the sample\nefficiency of reverse and scratchpad formats hold true for larger numbers of digits.\nWe begin by observing that the phase transition behavior observed in previous sections also applies to\nlonger-digit addition. Furthermore, we discover that the advantages of using reverse and scratchpad\nformats become even more pronounced as the number of digits increases. Next, we examine the\nnumber of training samples required to learn k + 1 digit addition when fine-tuning a pretrained\nmodel trained on k digit addition. We find that while the number of samples needed to further learn\nk + 1 digit addition remains relatively consistent for reverse and scratchpad formats, the plain format\nrequires an increasing number of samples.\nExperimental setup and data generation.\nTo explore the performance of the model in higher-digit\naddition scenarios, we extend the experimental setup described in Section 3. We adopt a balanced\nsampling approach for training data with D digits, ensuring an equal number d of all combinations of\ndigits for both operands as follows:\nWe begin by sampling all 100-digit additions. For the remaining number of digits, ranging from\n2 to D, we generate addition examples of the form \u201cA + B = C\u201d. The two operands, A and B,\nare randomly sampled d = \u230a(N \u2212 100)/(D(D + 1)/2 \u2212 1)\u230b times for every D, where N is the\ntotal number of training examples. Operand A is sampled between [10k1\u22121, 10k1 \u2212 1] and operand\nB is sampled between [10k2\u22121, 10k2 \u2212 1], for all 1 \u2264 k1 \u2264 k2 \u2264 D, excluding the case where\nk1 = k2 = 1. After sampling the two operands, we randomly interchange them to cover cases where\nA has fewer digits than B and vice versa.\n7.1\nTraining from Random Initialization\n(a) 5-digit Addition\n0\n25k\n50k\n75k\n100k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nsimplified scratchpad\ndetailed scratchpad\n(b) 7-digit Addition\n0\n50k\n100k\n150k\n200k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nsimplified scratchpad\ndetailed scratchpad\n(c) 10-digit Addition\n0\n100k\n200k\n300k\n400k\n500k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\nsimplified scratchpad\ndetailed scratchpad\nFigure 10: Comparison of sample efficiency for 5, 7 and 10-digit additions: performance of models\ntrained with varying numbers of addition samples on each data format. The plain format data requires\nan increasing number of training examples for higher digits, while the number of samples required\nfor other methods remains relatively consistent.\nWe repeat the experiment from Section 3 on nanoGPT with longer digits. The results shown in\nFigure 10 demonstrate a similar behavior to the findings observed in Figure 6 for 3-digit addition.\nThis indicates that our previous observations generalize to longer sequence lengths. Notably, the\nperformance gap between the modified formats (reverse, simplified scratchpad, and detailed scratch-\npad) and the plain format becomes even more significant in the context of higher digits. While the\nplain format requires an increasing number of training examples to learn higher-digit additions, the\nreverse or scratchpad formats exhibit a more consistent requirement in terms of the number of training\nexamples.\nThis prompts us to explore the differences between each format in a fine-tuning setting. Specifically,\nwe ask whether a model trained on reverse or scratchpad-formatted k digit addition data would find it\neasier to learn k + 1 digit addition compared to a model trained with plain format addition.\n15\n7.2\nFine-Tuning from Pretrained Models\nIn this section, we investigate the generalization ability of transformer models, specifically focusing\non their capacity to learn higher-digit additions based on their knowledge of lower-digit additions.\nAdditionally, we explore how the choice of data format affects the number of samples required to\nlearn higher-digit additions.\nForgetting of k-digit addition when trained on k + 1-digit addition.\nWe begin by fine-tuning a model that was initially trained on 3-digit addition. We fine-tune this\nmodel using 4-digit addition training data, with each data format being used separately. To mitigate\nthe \u201ccatastrophic forgetting\u201d phenomenon, we experiment with different learning rates, gradually\nreducing the magnitude. We continue this process until the learning rate becomes too small for the\nmodel to effectively learn 4-digit addition.\n0\n50\n100\n1-digit accuracy\n2-digit accuracy\n0\n5000\n10000\n0\n50\n100\n3-digit accuracy\n0\n5000\n10000\n4-digit accuracy\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumber of Iterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nplain (lr=1e-4)\nreverse (lr=5e-6)\nsimple (lr=1e-5)\ndetailed (lr=1e-6)\nFigure 11: Accuracy of 1 to 4-digit additions during fine-tuning of a pretrained model on 3-digit\nadditions using different data formats. The model is fine-tuned using only 4-digit addition data with\ncorresponding formats. We observe that the plain format \u2018forgets\u2019 1 to 3-digit additions entirely\nwhen learning 4-digit addition. In contrast, the detailed scratchpad method successfully learns 4-digit\naddition while maintaining high performance on 1 to 3-digit additions.\nThe results depicted in Figure 11 reveal interesting insights about the fine-tuning process. When\ntraining the model using the plain format with only 4-digit addition data, there is an immediate drop\nin accuracy for 1 to 3 digit additions. This indicates that the model experiences significant forgetting\nof previously learned additions. In contrast, the reverse and scratchpad methods exhibit a more\nfavorable behavior. The model trained with these methods does not completely forget 1 or 2 digit\nadditions while learning 4-digit addition. Remarkably, the detailed scratchpad method stands out by\nenabling the model to learn 4-digit addition without compromising its performance on 1 to 3 digit\nadditions. Although there is a slight decrease in performance for 3-digit additions initially, the model\nquickly recovers and picks up the knowledge again as it trains on 4-digit additions.\nThis result can be explained by the hypothesis that learning a k + 1 digit addition from a k-digit\nmodel is an incremental process for the detailed scratchpad method. The model already has a solid\nfoundation in understanding the intermediate steps involved in addition, so it only needs to adapt to\nlonger sequences. In contrast, for the plain format, learning higher-digit additions requires the model\nto establish new mappings to generate correct outputs, which is a more challenging task.\nSample efficiency of fine-tuning k-digit models with k + 1-digit examples.\nBuilding upon our\nprevious findings that fine-tuning a model solely on k +1-digit addition leads to a loss in performance\nfor k-digit addition, we modify our approach to prevent the loss of performance in the k-digit addition\ntask. Instead of training solely on k + 1-digit examples, we construct a dataset that includes all\naddition tasks from 1-digit to k + 1-digit, with the method described in the previous section. By\ndoing so, we aim to maintain the performance of 1 to k-digit addition while enabling the model to\nlearn k + 1-digit addition during fine-tuning.\n16\nIn this experiment, we investigate the number of k + 1-digit training examples required for the model\nto effectively learn k + 1-digit addition when fine-tuning a pretrained model on k-digit addition. It is\nimportant to note that this setting differs from the previous section (Section 7.1), where we focused\non training models from random initialization. Here, we specifically focus on the fine-tuning process.\nWe fine-tune individual models pretrained on each data format (using k-digit addition) and further\ntrain them using the same data format on a new dataset that includes all addition examples from\n1-digit to k + 1-digit.\n(a) Plain\n0\n10000 20000 30000 40000 50000 60000\nNumber of k+1 digit train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n3-digit\n4-digit\n6-digit\n8-digit\n(b) Reverse\n0\n1000\n2000\n3000\n4000\n5000\n6000\nNumber of k+1 digit train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n3-digit\n4-digit\n6-digit\n8-digit\n(c) Simplified Scratchpad\n0\n1000\n2000\n3000\n4000\n5000\nNumber of k+1 digit train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n3-digit\n4-digit\n6-digit\n8-digit\n(d) Detailed Scratchpad\n0\n500\n1000\n1500\n2000\n2500\n3000\nNumber of k+1 digit train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n3-digit\n4-digit\n6-digit\n8-digit\nFigure 12: Fine-tuning performance of pretrained k-digit models using varying numbers of k + 1-\ndigit examples, with corresponding data formats. The plain format requires an increasing number\nof k + 1-digit examples as the number of digits (k + 1) increases. In contrast, the modified formats\n(reverse, scratchpad) exhibit consistent performance across different numbers of digits, requiring a\nrelatively consistent number of examples to learn the additional digit.\nThe results in Figure 12 demonstrate the number of k + 1-digit addition samples required for a\npretrained model capable of performing k-digit addition to learn the addition of k + 1 digits. The\nfindings reveal that modified formats (reverse, scratchpad) require a relatively small number of\nsamples (between 1000 and 5000) to learn the addition of an extra digit. In contrast, the plain format\nnecessitates a significantly larger number of training examples, with the requirement increasing as\nthe number of digits grows.\nThis observation aligns with our previously established Lemma 2 and Lemma 1, which suggest that\nlearning higher-digit addition in the reverse format involves processing the i-th digit of the operands\nand carrying from the previous position. This operation remains consistent regardless of the number\nof digits being added. As a result, the model primarily needs to learn how to handle longer digits to\nperform addition effectively.\nIn contrast, the plain addition format requires the model to learn a more complex function that\nincorporates all digits from both operands. As the number of digits increases, the complexity of\nthis function grows as well. This highlights the greater difficulty faced by the plain format in\naccommodating additions with a larger number of digits.\n7.3\nImpact of Formats on Fine-Tuning\nWe delve deeper into the impact of different formats on the fine-tuning process. Specifically, we\ninvestigate whether training a model in one format helps in learning addition in another format,\n17\nand vice versa. To conduct this analysis, we begin with a model trained on each data format using\n3-digit addition examples. We then individually fine-tune these pretrained models using different\ndata formats, on 4-digit addition examples.\n0\n50\n100\nfinetuning with plain\nfinetuning with reverse\n0\n5000\n10000\n0\n50\n100\nfinetuning with simple\n0\n5000\n10000\nfinetuning with detailed\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNumber of Iterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTest Accuracy (%)\nplain\nreverse\nsimple\ndetailed\nrandom init\nFigure 13: Performance of fine-tuning a 3-digit\nmodel trained on different data formats (plain, re-\nverse, simple scratchpad, detailed scratchpad, and\nrandom initialization) individually with different\ndata formats of 4-digit addition. The results demon-\nstrate that fine-tuning yields the best performance\nwhen the pretrained model and the fine-tuning for-\nmat are consistent. Notably, fine-tuning a detailed\nscratchpad format model shows suboptimal perfor-\nmance. We hypothesize that this is due to the need\nfor the model to \u201cunlearn\u201d the rigid and verbose\nformat and adapt to the new format.\nThe results depicted in Figure 13 highlight some\ninteresting findings. Firstly, we observe that a\nmodel trained with the same format as the fine-\ntuning format exhibits faster learning in terms\nof the number of iterations. For instance, train-\ning a model with the plain format outperforms\ntraining a model pretrained with scratchpad for-\nmats. This suggests that the model benefits from\nthe consistency and familiarity provided by the\nsame format throughout the training process.\nAdditionally, we notice that fine-tuning a de-\ntailed scratchpad pretrained model on other for-\nmats proves to be more challenging. This ob-\nservation can be attributed to the need for the\nmodel to \u201cunlearn\u201d the intricacies of the ver-\nbose detailed scratchpad format and adapt to\nthe new format. For example, the plain format\ndoes not involve the use of alphabet characters\nin the data, so a model pretrained with the plain\nformat would have a low probability of gener-\nating alphabetic outputs. In contrast, a detailed\nscratchpad pretrained model would have encoun-\ntered various alphabets and may have a tendency\nto output them. Therefore, adjusting to a new\nformat requires additional effort for the model\nto \u201cunlearn\u201d the patterns specific to the previous\nformat and effectively learn the new format it is\nbeing trained on.\nThese findings highlight the importance of considering format consistency during the fine-tuning\nprocess, as it can impact the efficiency and effectiveness of the learning process. We will delve further\ninto this topic in the upcoming section 10, where we fine-tune pretrained GPT-3 models. Notably, we\nobserve that fine-tuning with reverse or simplified scratchpad formats actually yields worse results\ncompared to fine-tuning with plain formats. For a detailed exploration of these observations, please\nrefer to the forthcoming section.\n8\nTeaching Arithmetic Operations Beyond Addition\nWhile this study has a primary focus on the addition operation and aims to comprehend the signifi-\ncance of data sampling and formatting, its findings are applicable beyond the realm of addition alone.\nIn this section, we expand our examination to include other arithmetic operations, thus demonstrating\nthe broader applicability of our insights. We consider a mix of arithmetic tasks, including binary\noperations like subtraction and multiplication, and unary operations such as sine and square root.\nEach operation entails its unique challenges and intricacies. For instance, subtraction introduces the\nconcept of negative numbers, multiplication can generate significantly longer outputs, and sine and\nsquare root functions entail computations involving floating-point numbers, which are considered up\nto four digits of precision in our work.\nWe acknowledge that while our examination is detailed, it does not encompass all the fundamental\narithmetic operations or the entire scope of floating-point arithmetic. Specifically, our focus is primar-\nily on integer arithmetic for binary operations, considering a limited length of digits. Additionally,\nfor unary operations, we confine ourselves to a restricted number of digits below the decimal point.\nIn Section 8.1, we delve into each arithmetic operation individually, exploring the impact of data\nformatting and determining the relevancy of our insights across disparate tasks. Further, in Section 8.2,\n18\nwe perform an analysis of joint training across all five tasks, investigating the potential performance\nimplications for each individual task.\n8.1\nExtended Arithmetic Operations\nIn order to extend our analysis to arithmetic operations beyond addition, we consider the following\ntasks:\nSubtraction (\u2212).\nWe consider subtraction of positive numbers up to 3 digits, written as\nA3A2A1 \u2212 B3B2B1 = C3C2C1 in (i) plain formatting, and $A3A2A1 \u2212 B3B1B1 = C1C2C3$ in (ii)\nreverse formatting. As with addition, scratchpad-based methods (iii, iv), present the intermediate steps\nof digit-wise subtraction and handling of carry-ons. These steps proceed from the least significant\nbit (LSB) to the most significant bit (MSB). If the final result after computing all the digit-wise\nsubtractions is negative, we subtract the number in the most significant bit (MSB) position multiplied\nby 10 to the power of (number of digits in the output - 1) from the remaining digits in the output. In\nSection 6.2, we present an alternative version of the detailed scratchpad formatting for subtraction.\nMultiplication (\u00d7).\nWe consider multiplication of positive numbers up to 2-digits. (i) Plain\nformatting examples are formatted as A2A1 \u2217 B2B1 = C4C3C2C1, while (ii) reverse formatting is\nformatted as $A2A1 \u2217 B2B1 = C1C2C3C4$. The (iv) detailed scratchpad method simplifies each\nintermediate step by conducting a series of multiplications between the first operand and each digit\nof the second operand, starting from the least significant bit (LSB) and moving toward the most\nsignificant bit (MSB). For each step, we multiply the result by an exponentiation of 10 corresponding\nto the relative digit position.\nSine (sin ).\nWe consider decimal numbers within the range [\u2212\u03c0/2, \u03c0/2], truncated to 4-digit\nprecision. (i) Plain formatting examples are formatted as sin(A0.A1A2A3A4) = B0.B1B2B3B4.\nFor (iv) detailed scratchpad method, we include the Taylor series expansion steps for sine, which\nis represented as sin(x) = x \u2212 1\n3!x3 + 1\n5!x5 \u2212 1\n7!x7 + \u00b7 \u00b7 \u00b7 . These intermediate steps involve\nexponentiation, which may not be any easier to compute than the sine operation itself.\nSquare Root (\u221a).\nWe consider decimal numbers within [1, 10), truncated to 4-digits of precision\nwith the format, written as sqrt(A0.A1A2A3A4) = B0.B1B2B3B4 for (i) plain formatting. For (iv)\ndetailed scratchpad method, we enumerate each step of Newton\u2019s method to compute the square root\nfunction. The iterative formula is given by xn = 1\n2(xn\u22121 +\nx\nxn\u22121 ), where x0 is initialized as the\nfloor of the square root value of the operand x. These intermediate steps involve a division operation,\nwhich can be as complex as the square root operation itself.\nFor evaluation of sine and square root, we classify the result \u02c6yi as correct if the absolute difference\nbetween \u02c6yi and the ground truth value yi is less than or equal to a predefined threshold \u03f5 \u2265 0.\nFor each arithmetic task, we explore both the plain format and the detailed scratchpad format.\nThe detailed scratchpad formatting for each task is illustrated in Figure 14 and Appendix D. For\nsubtraction, the process involves breaking down the operation into intermediate steps of digit-wise\nsubtraction, including carry-ons when necessary. Unlike addition, subtraction requires an additional\nstep to handle cases where the first operand is smaller than the second. Further details on the detailed\nscratchpad for subtraction can be found in Section 6.2. For multiplication, each intermediate step\ncarries out a 2-digit \u00d7 1-digit multiplication between the first operand and each separate digit of the\nsecond operand. For sine and square root, we utilize a sequence of iterative approximations instead\nof algorithmic explanations. Specifically, Taylor\u2019s series expansion steps for sine and Newton\u2019s\nmethod steps for square root are used. It is important to note that while addition, subtraction, and\nmultiplication are broken down into simpler operations at each step, CoT for sine and square root\nfunctions requires intermediate steps involving operations like exponentiation or division, which\nmight not be inherently simpler.\n19\nDetailed scratchpad formatting for different arithmetic tasks\nExamples of detailed scratchpad formatting for different arithmetic tasks:\n(1) Subtraction - includes borrows for intermediate steps, (2) Multiplication\n- decomposes the second operand for 2-digit \u00d7 1-digit multiplication at each\nstep, (3) Sine - utilizes Taylor series expansion, and (4) Square root - employs\nNewton\u2019s method.\nSubtraction\nInput:\n128 -367\nTarget:\n<scratch >\n[1,2,8] has 3 digits.\n[3,6,7] has 3 digits.\n[1,2,8] - [3,6,7] , A=[] , C=0 , 8-7-0=1\n, A->1 , C->0\n[1 ,2] - [3 ,6] , A=[1] , C=0 , 2 -6 -0+10=6\n, A->6 , C->-1\n[1] - [3] , A=[6 ,1] , C=-1 , 1-3-1=-3 ,\nA->-3 , C->-1\n[] - [] , A=[-3,6,1]\n-300+61= -239 , END\n</scratch >\n-2 3 9\nMultiplication\nInput:\n12*36\nTarget:\n<scratch >\n[1 ,2] has 2 digits.\n[3 ,6] has 2 digits.\n[1 ,2] * 6 , A=[7 ,2] , k=1 , B=[7 ,2] , C\n=0+72=72\n[1 ,2] * 3 , A=[3 ,6] , k=10 , B=[3 ,6 ,0] ,\nC =72+360=432\n, END\n</scratch >\n4 3 2\nSine\nInput:\nsin (1.5707)\nTarget:\n<scratch >\nx_0 =1.5707\nx_1: x_0 - 1/3! * (x^3) , x_1 =0.9247\nx_2: x_1 + 1/5! * (x^5) , x_2 =1.0043\nx_3: x_2 - 1/7! * (x^7) , x_3 =0.9996\nx_4: x_3 + 1/9! * (x^9) , x_4 =0.9997\n, END\n</scratch >\n0.9997\nSqrt\nInput:\nsqrt (2.7174)\nTarget:\n<scratch >\nx_0 =1\nx_1: 1/2*(1+2.7175/1) =1.8587 ,\nx_1 =1.8587\nx_2: 1/2*(1.8587+2.7175/1.8587) =1.6603 ,\nx_2\n=1.6603\nx_3: 1/2*(1.6603+2.7175/1.6603) =1.6485 ,\nx_3\n=1.6485\nx_4: 1/2*(1.6485+2.7175/1.6485) =1.6484 ,\nx_4\n=1.6484\n, END\n</scratch >\n0.6484\nFigure 14: Examples of the detailed scratchpad format for different arithmetic tasks such as subtrac-\ntion, sine, multiplication, and square root.\nThe results depicted in Figure 15 indicate that similar to the findings of addition, the detailed\nscratchpad format significantly improves performance over plain or reverse formats and yields\nefficient results even with few samples for subtraction and multiplication tasks. Interestingly, we\nfind reverse is not particularly effective in multiplication. On the other hand, the detailed scratchpad\nformat exhibits reduced efficiency for sin and \u221a compared to other operations (+, \u2212, \u00d7). This\ndiscrepancy can be traced back to the complexity of the intermediate steps involved in the detailed\nscratchpad. While addition, subtraction, and multiplication are decomposed into simpler functions,\nsine and square root operations involve more intricate operations. For a broader analysis of the error\nprofile, see Appendix B.4.\n8.2\nJointly Training on All Five Arithmetic Tasks\nSo far, we only considered the problem of learning different arithmetic operations individually. In\nthis section, we study the effect of jointly training on all five arithmetic tasks - addition, subtrac-\ntion, multiplication, sine, and square root. We construct a single train dataset incorporating all\ntask Dtrain = {D+\ntrain, D\u2212\ntrain, D\u00d7\ntrain, Dsin\ntrain, D\n\u221a\ntrain}, and randomize the sequence of tasks in our train\nsamples. For example, a randomly chosen segment of the training data may exhibit a task order\nsuch as (+, \u2212, sin .\u2212, \u00d7, \u00d7, \u221a, ...). We consider 10, 000 training examples for each task of addition,\nsubtraction, sine, and square root and 3, 000 for multiplication.\n20\n(a) Subtraction\n0\n2.5k\n5k\n7.5k\n10k 12.5k 15k 17.5k 20k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\ndetailed scratchpad\n(b) Multiplication\n1000\n2000\n3000\n4000\n5000\n6000\n7000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain\nreverse\ndetailed scratchpad\n(c) Sine\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain, =0\nplain, =5e-4\nplain, =5e-3\ndetailed, =0\ndetailed, =5e-4\ndetailed, =5e-3\n(d) Square Root\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nplain, =0\nplain, =5e-4\nplain, =5e-3\ndetailed, =0\ndetailed, =5e-4\ndetailed, =5e-3\nFigure 15: Performance of 3\u2212digit subtraction, 2\u2212digit multiplication, 4\u2212digit precision sine and\nsquare root with varying data formats. As with addition, reverse always produces improved sample\ncomplexity and performance for all operations. For sine and square root, scratchpad formatting pro-\nvides limited improvement. This discrepancy can be attributed to the complexity of the intermediate\nsteps involved in the detailed scratchpad.\nThe model\u2019s performance, after training on our joint dataset Dtrain, is evaluated in both zero-shot and\nfew-shot settings. These results are also compared with the performance of models that were trained\nseparately on each dataset (D+\ntrain, D\u2212\ntrain, D\u00d7\ntrain, Dsin\ntrain, D\n\u221a\ntrain), identical to those used to construct\nDtrain. In the few-shot setting, each task is given examples from any of the five arithmetic tasks (not\nnecessarily related to the test task under consideration) or prompt texts, followed by test queries\nspecific to the task of interest. For further details on the few-shot prompting methods used, please\nrefer to Section 9.\nTable 4 shows that joint training significantly enhances the zero-shot performance for multiplication\nand square root tasks, yet it slightly reduces the performance for subtraction. Generally, few-shot\nprompting exhibits improved performance. Notably, the performance of few-shot prompting remains\nconsistent regardless of whether the exemplars provided are from unrelated tasks or are task-specific.\nWe propose that this consistency is due to our randomized task sequence during training, which\npresents the model with numerous instances where one task directly follows another, thus simulating\nfew-shot prompting with different tasks. Furthermore, we observe that text prompting performs\nsimilar to zero-shot. We conjecture that this is because the training data does not include text data\nand the model has never encountered text and therefore, text prompting serves as a random prefix\nattached to our test query.\n9\nMixing Shakespeare with Arithmetic Data\nUntil now, our focus was primarily on models trained exclusively on arithmetic tasks. However,\nin practice, large language models (LLMs) utilize a combination of arithmetic and text data for\ntraining. In this section, we broaden our scope by incorporating both addition samples and text into\nour pretraining data. We then evaluate the trained models with various few-shot prompts to analyze if\nthe model is able to effectively identify the correct context.\n21\nTable 4: Performance of models trained individually and jointly on five arithmetic tasks. The threshold\n\u03f5 for sin and \u221a functions is set to 0. For the models trained jointly on all five tasks, we evaluate their\nperformance in both a zero-shot setting and a few-shot setting. In the few-shot setting, each task is\npresented with exemplars from one of the five arithmetic tasks or prompted with text, followed by\ntask-specific test queries. The results show that few-shot prompting with any arithmetic operators\n(even unrelated to the test task) generally improves performance. However, text prompting shows\nperformance similar to the zero-shot setting.\nTrained on\nindividual task\nTrained jointly on all 5 tasks\nZero-shot\nFew-shot exemplar format\n+\n\u2013\n\u00d7\nsin\nsqrt\ntext\n+\n84.06\n87.96\n96.45\n96.90\n96.92\n97.06\n97.01\n88.71\n\u2013\n79.97\n72.83\n81.28\n79.59\n81.39\n81.84\n81.74\n68.91\n\u00d7\n4.58\n14.28\n18.86\n18.96\n15.43\n19.20\n19.59\n15.48\nsin\n35.03\n34.74\n34.35\n34.31\n34.34\n32.64\n33.42\n33.96\nsqrt\n19.85\n27.37\n26.65\n26.74\n26.70\n25.60\n25.61\n26.02\nExperimental Setup.\nWe mix addition and text data in our experiment using the Shakespeare\ndataset (Karpathy, 2015) that includes 1, 115, 394 tokens of text, 10, 000 plain addition examples\n(120, 027 tokens), and 3, 000 detailed scratchpad formatted addition examples (813, 510 tokens). We\nfix the number of detailed scratchpad examples and plain addition examples (3, 000 and 10, 000\nrespectively) while varying the number of each example type in the training process. The Shakespeare\ntext is segmented into dialogue chunks, with a random number of addition data inserted between\nthem. We use a character-level tokenizer with a vocabulary size of 80, containing all characters\npresent in the dataset, including alphabets, digits, and certain symbols like +, = and \\n.\nFew-shot prompting.\nGiven the mixed nature (arithmetic and text) of our dataset, introducing\nrelevant examples seems an effective strategy to prime the model to generate the desired type of\noutput. To assess the performance of such few-shot (1/2/3\u2212shot) prompting, we provide task-specific\nexemplars as illustrated in Figure 16. Plain addition formatted exemplars are used for testing plain\naddition inputs, while detailed scratchpad formatted exemplars are utilized for assessing performance\non detailed scratchpad formatted inputs. Additionally, we experiment with demonstrating text (see\nAppendix B.3. for details) before querying addition (which we denote, Text-prompt). For each 1/2/3-\nshot and text prompting, average performance is reported over a fixed set of exemplars. Standard\ndeviations of these prompts are denoted by shaded areas in the plots. The term \u201cfew-shot\u201d refers to\nthe reported mean of all 1/2/3-shot prompting results.\nFigure 16: Few-shot prompting method. Few-shot prompting performance is evaluated by presenting\nrelevant exemplars of addition and detailed scratchpad formatted inputs. Each 1/2/3-shot prompting\nis tested on a fixed five set of exemplars, and the accuracy is averaged over these evaluations.\nFigure 17 shows that few-shot prompting directs the enhancement of performance, thereby allowing\nplain addition to perform almost perfectly with 40,000 train samples. Intriguingly, performance\nremains high on plain addition even with the inclusion of a text prompt, given a substantial number\nof addition examples. We hypothesize that this is due to the structure of our mixed dataset where\n22\n(a) Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\n(b) Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\nFigure 17: Performance of NanoGPT model trained with the Shakespeare dataset, addition dataset\nin plain, and detailed scratchpad format. The number of plain (left) and detailed scratchpad (right)\nformatted addition samples are varied. Performance is evaluated on zero-shot, few-shot, and text\nprompts, with the shaded area representing the standard deviation across various prompt exemplar\nsets. The results indicate a consistent enhancement in model performance using few-shot prompting.\naddition examples are interspersed within Shakespeare data. With the incorporation of more addition\nexamples, instances where addition examples directly follow Shakespeare text increase, leading to a\ndecrease in potential inconsistencies when text content is present during addition test queries.\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\nNoisy-prompt\nText-prompt\nFigure 18: Performance of NanoGPT model\ntrained exclusively on plain addition, but with\nan extended vocabulary including both addi-\ntion and alphabets (vocabulary size = 80). Few-\nshot prompting, using both correct addition\nexamples (1, 2, 3-shot) and incorrect addition\nexamples (noisy-prompt) leads to enhanced\nperformance, while the use of text prompts re-\nsults in a degradation of performance when the\nmodel is trained solely on addition.\nTo disentangle the effects of the textual content in\nthe training data, we train a model strictly on plain\naddition, utilizing an enlarged vocabulary that also\nincludes alphabet characters, thereby enabling text\nprompting. (Note that previous experimental set-\ntings on plain formatted additions used a vocabulary\nsize of 13, which only includes 10 numerals and\n3 symbols - \u201c+\u201d,\u201c=\u201d,\u201c\\n\u201d). We introduce a variant\nof few-shot prompting, termed as noisy-prompt,\nwhich prompts the model with erroneous addition\nexemplars, i.e., , A + B = C, where C \u0338= A + B.\nFigure 18 shows that few-shot prompting contributes\nto performance enhancement even when the model\nis confined to training on a single plain addition task.\nEven in the presence of noisy prompting, simply pro-\nviding the model with the A + B = C format yields\nperformance nearly identical to few-shot prompt-\ning, aligning with the result observed by Min et al.\n(2022). Conversely, we notice that text prompts neg-\natively influence performance when the model is\ntrained only on addition. This finding reinforces\nour earlier observation in Figure 17 that the advan-\ntageous impact of text prompts originates from the\ncombined text and addition data.\n10\nFine-tuning, Scaling, and Pretraining in Larger Models\nThis section focuses on bridging the gap between our experiments on NanoGPT and the more realistic\nsetting of larger language models like GPT-2 and GPT-3. We begin by comparing the performance of\nNanoGPT and GPT-2 models when trained from random initialization. This comparison highlights\nthe improved performance achieved with the larger model scale, especially in the zero-shot setting.\nSubsequently, we delve into the impact of tokenization methods and model pretraining in GPT-2\nmodels. Our exploration reveals the crucial role of pretrained models and the consistent tokenization\nof numbers (achieved by introducing spaces) during the training phase for arithmetic tasks. Building\n23\non these findings, we proceed to fine-tune a pretrained GPT-3 model on various arithmetic tasks,\nemploying different data formats.\nComparing NanoGPT and GPT-2.\nTo examine the impact of scale on arithmetic performance,\nwe explore a larger GPT-2 model with 85 million parameters, featuring twice as many self-attention\nlayers, heads, and embedding size compared to the previously used NanoGPT model. We train\nthe GPT-2 model from scratch using character-level tokenization, jointly on text and addition tasks,\nadopting both plain and detailed scratchpad formats; an approach mirroring the setting in Section 9.\nThe results depicted in Figure 19 demonstrate that the larger model outperforms in both plain and\ndetailed scratchpad evaluations. For a comprehensive analysis of GPT-2, including few-shot learning\nand the influence of text prompts, refer to Figure 26 and Figure 27.\n(a) Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n20\n40\n60\n80\n100\nTest Accuracy (%)\nNanoGPT, Zero-shot\nNanoGPT, Few-shot\nGPT2, Zero-shot\nGPT2, Few-Shot\n(b) Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n96.0\n96.5\n97.0\n97.5\n98.0\n98.5\n99.0\n99.5\n100.0\nTest Accuracy (%)\nNanoGPT, Zero-shot\nNanoGPT, Few-shot\nGPT2, Zero-shot\nGPT2, Few-Shot\nFigure 19: Comparing NanoGPT and GPT-2 on addition task. We compare the performance of\nNanoGPT and GPT-2 models trained jointly on the Shakespeare dataset and addition tasks using\nplain and algorithmic reasoning formatting. The results indicate that larger models exhibit improved\nperformance, and using few-shot prompting enhances performance as well. The left side shows\nresults for plain data formatting, while the right side presents results for algorithmic reasoning data\nformatting.\nGoing from character-level tokenization to BPE.\nThe transition to a GPT-2 setup necessitates\nseveral modifications. Firstly, we shift to OpenAI\u2019s Tiktoken BPE tokenizer, which is the default\ntokenizer for the pretrained GPT-2 model, featuring a vocabulary size of 50,257. We also examined\ntwo different training approaches: training the model from random initialization (scratch) and\nfine-tuning the pretrained model sourced from Huggingface. To ensure uniform digit tokenization,\nalterations were made in data formatting to include spaces between numbers. This change aims to\ncircumvent potential inconsistent tokenization of numbers while utilizing the Tiktoken tokenizer.\nFigure 20 shows that GPT-2 demonstrates high performance in addition tasks with both character-level\ntokenization and Tiktoken with spaces between digits. This aligns with the results by Wallace et al.\n(2019), suggesting that character-level tokenization exhibits stronger numeracy capabilities compared\nto a word or sub-word methods. Furthermore, comparing the models trained from scratch and the\nmodels trained from the pretrained model, we observe that fine-tuning a pretrained model results in\nbetter performance compared to training a model from scratch.\nGPT-3 experiments: Supervised fine-tuning.\nWe extend our experiments to verify if our observa-\ntions hold while fine-tuning larger pre-trained models. In the following, we consider three GPT-3\nvariants: Ada, Curie, and Davinci. Note that since we perform fine-tuning using the OpenAI APIs, by\ndefault only the completions are loss generating tokens. Therefore, these experiments are slightly\ndifferent when compared to the previous settings. We fine-tune these models using the same four\ndata formatting methods as our NanoGPT experiments: (i) plain formatting, (ii) reverse formatting,\n(iii) simplified scratchpad, and (iv) detailed scratchpad. These formats are identical to those from our\nNanoGPT experiments except for one aspect. We introduce spaces between numbers in plain and\nreverse formatting to ensure consistent tokenization.\nDue to budget constraints, all experiments were conducted using a fine-tuning dataset of 1, 000\nexamples, and models were trained for 4 epochs. Performance evaluation was carried out on 1, 000\nexamples that were disjoint from the training dataset. Note that this training scale is significantly\n24\nsmaller than our experiments on NanoGPT, which employed 10, 000 training examples for 5, 000\niterations, with evaluations conducted on 10, 000 test examples. However, given these models\u2019\nextensive pretraining on large data corpora, this scale can be deemed rational.\n0\n1000\n2000\n3000\n4000\n5000\nTrain Iterations\n0\n20\n40\n60\n80\nTest Accuracy (%)\nNanoGPT, char-level, scratch\nGPT-2, char-level, scratch\nGPT-2, tiktoken, scratch\nGPT-2, tiktoken, pretrained\nGPT-2, tiktoken+space, scratch\nGPT-2, tiktoken+space, pretrained\nFigure 20: Performance of various configurations of the GPT-2 model on the addition task. We\ncompare the effects of tokenization methods, specifically character-level tokenization versus Tiktoken\n(OpenAI\u2019s BPE tokenizer), training initialization (training from scratch versus training from a\npretrained GPT-2 model), and the inclusion or exclusion of spaces between numbers. The results\nhighlight the significance of utilizing pretrained models and incorporating spaces for consistent\ntokenization of numbers when training a model for arithmetic tasks.\nThe results for addition and subtraction tasks are presented in Table 5 and Table 6, respectively. We\nobserved that initiating with a pretrained GPT-3 model significantly improves performance compared\nto training NanoGPT or GPT-2 models from random initialization with only 1000 samples. This\nindicates the utility of leveraging pretrained models for improved arithmetic performance. Interest-\ningly, while reverse formatting and simplified scratchpad formats improve addition performance,\nthey adversely affect subtraction performance. This observation is consistent with our earlier finding\ndepicted in Figure 13, wherein transitioning from one data format to another often results in lower\nperformance compared to initiating training from random initialization. We postulate that this discrep-\nancy may be due to the pretrained GPT-3 model\u2019s requirement to adapt to the reversed approach and\n\u201cunlearn\u201d its knowledge of plain formatting arithmetic, thereby introducing additional complexity. On\nthe other hand, the detailed scratchpad method achieves excellent performance, albeit with increased\ntraining and inference costs due to higher token requirements.\nTable 5: Evaluation of addition performance for fine-tuned GPT-3 models: Davinci, Curie, and Ada.\nAddition pretrained\nGPT-3\nFinetuned with 1000 samples\nPlain\nReverse\nSimplified Scratchpad\nDetailed Scratchpad\nDavinci\n2%\n34%\n80.9%\n88.7%\n99.5%\nCurie\n0.0%\n1.4%\n12.3%\n10.7%\n99.7%\nAda\n0.0%\n0.3%\n6.3%\n0.6%\n99.8%\nTable 6: Evaluation of subtraction performance for fine-tuned GPT-3 models: Davinci, Curie, and\nAda.\nSubtraction\npretrained\nGPT-3\nFinetuned with 1000 samples\nPlain\nReverse\nSimplified Scratchpad\nDetailed Scratchpad\nDavinci\n0.1%\n84.8%\n66.0%\n15.4%\n99.5%\nCurie\n0.1%\n24.1%\n6%\n3.8%\n92.5%\nAda\n0.0%\n3.7%\n2.6%\n3.4%\n81.5%\n25\nTable 7: Evaluation of sine and square root performance for fine-tuned GPT-3 models: Davinci, Curie,\nand Ada.\nSine\nSquare Root\neps\npretrained\nGPT-3\nFinetuned with 1000 samples\npretrained\nGPT-3\nFinetuned with 1000 samples\nPlain\nDetailed Scratchpad\nPlain\nDetailed Scratchpad\nDavinci\n0\n0%\n11.0%\n10.3%\n0%\n0.7%\n4.6%\n5e-4\n0%\n35.9%\n29.7%\n0%\n7.5%\n17.2%\n5e-3\n0.4%\n85.5%\n72.8%\n0%\n59%\n60.5%\nCurie\n0\n0.0%\n8.6%\n1.2%\n0.0%\n0.7%\n2.1%\n5e-4\n0.4%\n32.7%\n5.4%\n0.1%\n6.5%\n6.0%\n5e-3\n0.9%\n80.8%\n15%\n0%\n52.7%\n30.2%\nAda\n0\n0.0%\n5.8%\n4.3%\n0.0%\n0.3%\n2.7%\n5e-4\n0.0%\n21.4%\n9.1%\n0.0%\n3.8%\n11.9%\n5e-3\n0.3%\n67.8%\n25.2%\n0.0%\n32.2%\n45.8%\nFor the more complex sine and square root tasks as shown in Table 7, we found that training with\nonly 1000 samples is insufficient to generate exact answers (eps=0). The GPT-3 model, fine-tuned\nwith 1,000 samples, performs worse than the NanoGPT model trained with 10,000 samples. Further\nexperiments with larger training datasets are necessary for deeper insights and improved performance\non these tasks.\nIt is worth mentioning that while few-shot prompting notably improves the performance of all\nthree GPT-3 models, their zero-shot performance is quite poor (as shown in the leftmost column\nof the tables). However, post-training, few-shot prompting becomes less effective as OpenAI\u2019s\nfine-tuning process trains the model on individual prompts and desired completions serially, rather\nthan in concatenation with multiple examples like in our NanoGPT experiments. Consequently, our\ncomparisons primarily focus on the zero-shot performances of each task.\n11\nToken Efficiency Across Data Formats\n0\n100k\n200k\n300k\nNumber of unique tokens\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\naddition\nreverse\nsimplified scratchpad\ndetailed scratchpad\nFigure 21: Number of unique tokens re-\nquired for training addition on NanoGPT\nusing different data formatting methods.\nThe number of unique tokens is calculated\nby multiplying the number of training sam-\nples by the number of tokens per sample.\nThe results demonstrate that the reverse for-\nmat is the most efficient in terms of token\nusage for model training, as the scratchpad\nmethods, although more sample-efficient,\nrequire more tokens per sample.\nFigure 6 demonstrates that more detailed training data\nleads to improved sample efficiency. However, this com-\nparison does not account for the cost associated with\ntraining and inference. To address this, we conduct a\ncost analysis based on the number of \u201cunique\u201d tokens\nencountered during training. Each data sample is treated\nas a set of unique tokens, and the number of unique to-\nkens is derived by multiplying the number of samples\nwith the tokens per sample. For instance, the mean token\ncount for a single training example in a 3-digit addition\ntask is 13 for plain format, 15 for reverse format, 64\nfor simplified scratchpad format, and 281 for detailed\nscratchpad format. Note that this calculation does not\nevaluate uniqueness of tokens across samples i.e., if the\nfirst sample is \u201c112 + 129 = 241\u201d and the second sam-\nple is \u201c112 + 128 = 240\u201d, we will still consider that\nthe model has seen 26 unique tokens even though only\ntwo tokens differ across samples. This approach ensures\nour cost calculation accounts for a vanilla implementa-\ntion of attention with no additional optimizations (Pope\net al., 2023). Table 8 presents the number of tokens\nrequired for prompting and completion in each data\nformat, per example. Evidently, the detailed scratch-\npad method uses considerably more tokens compared\nto other techniques.\n26\nThe result in Figure 21 indicates that reverse formatting is the most token-efficient approach. While\ndetailed scratchpad training is more sample efficient, it necessitates a larger number of tokens per\nsample, both during training and inference. Given that the inference cost for commercial models is\ndetermined by the number of tokens utilized per inference call (sum of prompting and completion\ntokens), abundant use of models trained on detailed scratchpad formats may escalate overall costs.\nFurthermore, since the cost of a single forward pass is cubic in the number of tokens, this is important\nto consider. Therefore, for practical usage, it is crucial to evaluate both the number of samples needed\nfor achieving the desired performance and the actual token demands during training and inference.\nTable 8: Token requirements for prompting and completion per single example of 3-digit addition.\nPlain\nReverse\nSimplified Scratchpad\nDetailed Scratchpad\nPrompt\n8\n9\n23\n23\nCompletion\n5\n6\n41\n258\nTotal\n13\n15\n64\n281\n12\nLength Generalization\nIn this section, we present results from experiments conducted to assess the model\u2019s ability to gener-\nalize across different digit lengths. Initially, we exclude training examples featuring 2-digit operands\nfrom the 10,000-sample addition dataset, yielding a reduced dataset of 7,655 samples, consisting\nsolely of 1 or 3-digit operands. The model is trained with reverse format and its performance is evalu-\nated on test dataset containing 100 random samples of 1-digit, 2-digit, 3-digit, and 4-digit additions.\nThe results in Figure 22 demonstrate that the NanoGPT model is incapable of performing 2-digit\nand 4-digit additions. This suggests an inherent necessity for exposure to all digit combinations to\nperform accurate calculations and lacks generalization capabilities for unseen digit lengths.\nAdditionally, we investigate the model\u2019s ability to extrapolate over larger digit lengths. The model is\ntrained on 7-digit plain-formatted additions (each digit addition comprises 16650 samples, except\n1-digit addition, which is trained on 100 samples). Its ability to add add 8-digit numbers is then put to\ntest. The results in Figure 22 show that the model is unable to generalize to a greater number of digits\nbeyond what it has been trained on. Similarly, when training the model on 10-digit binary numbers, it\nfails to generalize to 11-digit binary additions, further confirming its limited ability to handle unseen\ndigit combinations.\n(a) Trained on 1 and 3 digit addition\n0\n5000\n10000\n15000\n20000\nIterations\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n1 digit\n2 digit\n3 digit\n4 digit\n(b) Trained on 1 \u2013 7 digit addition\n0\n5000\n10000\n15000\n20000\nIterations\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n1 digit\n2 digit\n3 digit\n4 digit\n5 digit\n6 digit\n7 digit\n8 digit\nFigure 22: Generalization experiments testing NanoGPT\u2019s performance on unseen numbers of digits\nin addition tasks. (Left): NanoGPT trained on reverse formatted addition with 1 and 3 digits, and\ntested on additions ranging from 1 to 4 digits. (Right): NanoGPT trained on up to 7-digit plain\nformatted addition and tested on additions ranging from 1 to 8 digits. In both cases, NanoGPT\nexhibits an inability to perform addition on digits it has not been exposed to.\nWe further explore the impact of detailed scratchpad formatting. The model trained on additions of\nup to 3 digits, struggles to generalize to 4-digit additions. Notably, it randomly drops a single digit\nfrom the 4-digit number, erroneously perceiving it as a 3-digit number. We illustrate this difficulty\n27\nin Figure 23 through multiple detailed error cases, ranging from instances in which only the test\nquery is provided (Case 1) to scenarios where all intermediate steps are provided except only the final\nanswer (Case 5). The prompts are highlighted in light grey and the responses generated by our trained\nNanoGPT model are highlighted in light green. These cases emphasize the model\u2019s shortcomings in\naccurately managing larger digit lengths.\nExamples for length generalization prompts\nResults obtained by prompting the NanoGPT model with larger digits than those\nit was trained on.\nThe model is trained using detailed scratchpad formats with\n3-digit numbers.\nWe evaluate its performance on 4-digit numbers, with varying\nlevels of provided information.\nThe prompt input is highlighted in a light blue\nbox, while the model\u2019s output is highlighted in a light green box.\nCase 1: Just asking the question\nInput:\n8465+3541\nTarget:\n<scratch>\n[8,4,6] has 3 digits.\n[3,5,1] has 3 digits.\n[8,4,6] + [3,5,1] , A=[] , C=0 , 6+1+0=7 , A->7 , C->0\n[8,4] + [3,5] , A=[7] , C=0 , 4+5+0=9 , A->9 , C->0\n[8] + [3] , A=[9,7] , C=0 , 8+3+0=11 , A->1 , C->1\n[] + [] , A=[1,9,7] C=1 , END\n</scratch>\n1 1 9 7\nCase 2: Giving number of digits\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6] + [3,5,4] , A=[] , C=0 , 6+4+0=10 , A->0 , C->1\n[8,4] + [3,5] , A=[0] , C=1 , 4+5+1=10 , A->0 , C->1\n[8] + [3] , A=[0,0] , C=1 , 8+3+1=12 , A->2 , C->1\n[] + [] , A=[2,0,0] C=1 , END\n</scratch>\n1 2 0 0\nCase 3: Giving 1 intermediate steps\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6,5] + [3,5,4,1] , A=[] , C=0 , 5+1+0=6\n, A->6 , C->0\n[8,4] + [3,5] , A=[6] , C=0 , 4+5+0=9 , A->9 , C->0\n[8] + [3] , A=[9,6] , C=0 , 8+3+0=11 , A->1 , C->1\n[] + [] , A=[1,9,6] C=1 , END\n</scratch>\n1 1 9 6\nCase 4: Giving all but one intermediate steps\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6,5] + [3,5,4,1] , A=[] , C=0 , 5+1+0=6\n, A->6 , C->0\n[8,4,6] + [3,5,4] , A=[6] , C=0 , 6+4+0=10\n, A->0 , C->1\n[8 ,4] + [3 ,5] , A=[0 ,6] , C=1 , 4+5+1=10\n, A->0 , C->1\n[8] + [3] , A=[0 ,0 ,6] , C=1 , 8+3+1=12\n, A->2 , C->1\n[] + [] , A=[2,0,6] C=1 END\n</scratch>\n1 0 0 6\nCase 5: Giving all intermediate steps, all it has to do is aggregate the answer\n28\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6,5] + [3,5,4,1] , A=[] , C=0 , 5+1+0=6\n, A->6 , C->0\n[8,4,6] + [3,5,4] , A=[6] , C=0 , 6+4+0=10\n, A->0 , C->1\n[8 ,4] + [3 ,5] , A=[0 ,6] , C=1 , 4+5+1=10\n, A->0 , C->1\n[8] + [3] , A=[0 ,0 ,6] , C=1 , 8+3+1=12\n, A->2 , C->1\n[] + [] , A=[2 ,0 ,0 ,6] C=1 END\n</scratch >\n1 2 0 6\nCase 6: Giving 1 in-context example\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6,5] + [3,5,4,1] , A=[] , C=0 , 5+1+0=6\n, A->6 , C->0\n[8,4,6] + [3,5,4] , A=[6] , C=0 , 6+4+0=10\n, A->0 , C->1\n[8 ,4] + [3 ,5] , A=[0 ,6] , C=1 , 4+5+1=10\n, A->0 , C->1\n[8] + [3] , A=[0 ,0 ,6] , C=1 , 8+3+1=12\n, A->2 , C->1\n[] + [] , A=[2 ,0 ,0 ,6] C=1 , END\n</scratch >\n1 2 0 0 6\nInput:\n1946+3598\nTarget:\n<scratch>\n[1,9,4] has 3 digits.\n[3,5,8] has 3 digits.\n[1,9,4] + [3,5,8] , A=[] , C=0 , 4+8+0=12 , A->2 , C->1\n[1,9] + [3,5] , A=[2] , C=1 , 9+5+1=15 , A->5 , C->1\n[1] + [3] , A=[5,2] , C=1 , 1+3+1=5 , A->5 , C->0\n[] + [] , A=[5,5,2] C=0 , END\n</scratch>\n5 5 2\nCase 7: Giving 1 In-context example, and all intermediate steps\nInput:\n8465+3541\nTarget:\n<scratch >\n[8,4,6,5] has 4 digits.\n[3,5,4,1] has 4 digits.\n[8,4,6,5] + [3,5,4,1] , A=[] , C=0 , 5+1+0=6\n, A->6 , C->0\n[8,4,6] + [3,5,4] , A=[6] , C=0 , 6+4+0=10\n, A->0 , C->1\n[8 ,4] + [3 ,5] , A=[0 ,6] , C=1 , 4+5+1=10\n, A->0 , C->1\n[8] + [3] , A=[0 ,0 ,6] , C=1 , 8+3+1=12\n, A->2 , C->1\n[] + [] , A=[2 ,0 ,0 ,6] C=1 , END\n</scratch >\n1 2 0 0 6\nInput:\n1946+3598\nTarget:\n<scratch >\n[1,9,4,6] has 4 digits.\n[3,5,9,8] has 4 digits.\n[1,9,4,6] + [3,5,9,8] , A=[] , C=0 , 6+8+0=14\n, A->4 , C->1\n[1,9,4] + [3,5,9] , A=[4] , C=1 , 4+9+1=14\n, A->4 , C->1\n[1 ,9] + [3 ,5] , A=[4 ,4] , C=1 , 9+5+1=15\n, A->5 , C->1\n[1] + [3] , A=[5 ,4 ,4] , C=1 , 1+3+1=5\n, A->5 , C->0\n[] + [] , A=[5 ,5 ,4 ,4] C=0 , END\n</scratch >\n5 5 4\nFigure 23: Example results on the model\u2019s output when prompted with a larger number of digits than\nthose it was trained on.\n29\n13\nLimitations\nLength generalization. In our experiments, we did not observe any instances where the model could\npredict beyond the number of digits it had been trained on (see Section 12). This finding is consistent\nwith previous literature that suggests length generalization is a challenging task. For instance, Shaw\net al. (2018); Sun et al. (2022) reported similar difficulties and proposed approaches such as relative\npositional encodings. Anil et al. (2022) suggests that models can only perform out-of-distribution\ntasks by combining fine-tuning, prompting, and scratchpad techniques. Nonetheless, there have been\ncases where length generalization was observed. Nye et al. (2021) demonstrated length generalization\nbut only for models with more than 108 parameters.\nModel/Data scale. Due to the smaller scale of our experiments, we were able to thoroughly examine\nthe impact of individual components on the model\u2019s arithmetic learning capabilities. Our model was\nlimited to a GPT-type decoder-only architecture, primarily focusing on character-level tokenization.\nAlthough we have obtained some preliminary results on scaling up and incorporating BPE-based\ntokenization, it remains uncertain if all our findings can be generalized to the scale of LLMs being\nused in practice today.\nBeyond elementary arithmetic.\nWe choose to analyze simple arithmetic operations in order\nto carefully isolate factors that contribute to emergence. While the existing literature has already\ndemonstrated the emergence of complicated abilities in practice, our work seeks to provide a better\nunderstanding of this behavior.\n14\nConclusion\nIn this work, we examine the problem of teaching small randomly initialized transformers arithmetic\noperations and elementary mathematical functions using the next-token prediction objective. We\ncarefully ablate different aspects of the training data so as to isolate the factors that contribute to the\nemergence of arithmetic capabilities. Our results reveal that traditional training data is sub-optimal\nfor learning arithmetic, and training on detailed, instructive data with intermediate steps or even\nsimply reversing the output improves accuracy and sample complexity. We consider both scenarios\nwith only arithmetic data as well as those with text data, and comprehensively analyze the effect\nof few-shot prompting, pretraining, and model scale. We find that while detailed, chain-of-thought\nstyle data improves sample complexity, it may not be efficient in terms of training and inference\ncosts since it requires training with much more tokens. Furthermore, we find that while the model\ngeneralizes to unseen examples of the same number of digits, the problem of length generalization is\nquite difficult. We attribute this to the model\u2019s inability to truly \u201clearn\u201d the underlying arithmetic\noperation in all generality. It remains an open problem how to curate the training data to ensure that\nthe model learns a particular algorithm as opposed to just learning an approximate function map. It\nis also unclear what the correct way to learn multiple operations is. It seems plausible that learning\nthem in increasing order of complexity is beneficial if one can circumvent the problem of catastrophic\nforgetting. Our findings emphasize the significance of high-quality, instructive data for the emergence\nof arithmetic capabilities in transformers. We anticipate this research will contribute to a more\nnuanced understanding of the mechanisms by which transformers acquire arithmetic operations.\nReferences\nAnil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V., Slone, A., Gur-Ari, G.,\nDyer, E., and Neyshabur, B. Exploring length generalization in large language models. arXiv\npreprint arXiv:2207.04901, 2022.\nBowman, S. R. Can recursive neural tensor networks learn logical reasoning?\narXiv preprint\narXiv:1312.6192, 2013.\nBowman, S. R., Potts, C., and Manning, C. D. Recursive neural networks for learning logical\nsemantics. CoRR, abs/1406.1827, 5, 2014.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam,\nP., Sastry, G., Askell, A., et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\n30\nBubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E., Lee, P., Lee, Y. T.,\nLi, Y., Lundberg, S., et al. Sparks of artificial general intelligence: Early experiments with gpt-4.\narXiv preprint arXiv:2303.12712, 2023.\nCai, J., Shin, R., and Song, D. Making neural programming architectures generalize via recursion.\narXiv preprint arXiv:1704.06611, 2017.\nCharton, F. Linear algebra with transformers. arXiv preprint arXiv:2112.01898, 2021.\nCharton, F. What is my math transformer doing?\u2013three results on interpretability and generalization.\narXiv preprint arXiv:2211.00170, 2022.\nChen, X., Liu, C., and Song, D. Towards synthesizing complex programs from input-output examples.\narXiv preprint arXiv:1706.01284, 2017.\nChen, X., Lin, M., Sch\u00e4rli, N., and Zhou, D. Teaching large language models to self-debug. arXiv\npreprint arXiv:2304.05128, 2023.\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung,\nH. W., Sutton, C., Gehrmann, S., et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\nChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M.,\nBrahma, S., et al. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416,\n2022.\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J.,\nHilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint\narXiv:2110.14168, 2021.\nDehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, \u0141. Universal transformers. arXiv\npreprint arXiv:1807.03819, 2018.\nDrozdov, A., Sch\u00e4rli, N., Aky\u00fcrek, E., Scales, N., Song, X., Chen, X., Bousquet, O., and Zhou, D.\nCompositional semantic parsing with large language models. arXiv preprint arXiv:2209.15003,\n2022.\nGadre, S. Y., Ilharco, G., Fang, A., Hayase, J., Smyrnis, G., Nguyen, T., Marten, R., Wortsman, M.,\nGhosh, D., Zhang, J., et al. Datacomp: In search of the next generation of multimodal datasets.\narXiv preprint arXiv:2304.14108, 2023.\nGiannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J. D., and Papailiopoulos, D. Looped transformers\nas programmable computers. arXiv preprint arXiv:2301.13196, 2023.\nHajij, M., Zamzmi, G., Ramamurthy, K. N., and Saenz, A. G. Data-centric ai requires rethinking data\nnotion. arXiv preprint arXiv:2110.02491, 2021.\nHanna, M., Liu, O., and Variengien, A. How does gpt-2 compute greater-than?: Interpreting\nmathematical abilities in a pre-trained language model. arXiv preprint arXiv:2305.00586, 2023.\nHuang, J., Gu, S. S., Hou, L., Wu, Y., Wang, X., Yu, H., and Han, J. Large language models can\nself-improve. arXiv preprint arXiv:2210.11610, 2022.\nKaiser, \u0141. and Sutskever, I. Neural gpus learn algorithms. arXiv preprint arXiv:1511.08228, 2015.\nKarpathy, A. char-rnn. https://github.com/karpathy/char-rnn, 2015.\nKarpathy, A. Andrej karpathy\u2019s lightweight implementation of medium-sized gpts. GitHub, 2022.\nURL https://github.com/karpathy/nanoGPT.\nKim, J., Hong, G., Kim, K.-m., Kang, J., and Myaeng, S.-H. Have you seen that number? investigating\nextrapolation in question answering models. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pp. 7031\u20137037, 2021.\n31\nKir\u00e1ly, F. J., Theran, L., and Tomioka, R. The algebraic combinatorial approach for low-rank matrix\ncompletion. J. Mach. Learn. Res., 16(1):1391\u20131436, 2015.\nKojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa, Y. Large language models are zero-shot\nreasoners. arXiv preprint arXiv:2205.11916, 2022.\nLake, B. and Baroni, M. Generalization without systematicity: On the compositional skills of\nsequence-to-sequence recurrent networks. In International conference on machine learning, pp.\n2873\u20132882. PMLR, 2018.\nLightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J.,\nSutskever, I., and Cobbe, K. Let\u2019s verify step by step. arXiv preprint arXiv:2305.20050, 2023.\nLing, W., Yogatama, D., Dyer, C., and Blunsom, P. Program induction by rationale generation:\nLearning to solve and explain algebraic word problems. arXiv preprint arXiv:1705.04146, 2017.\nLiu, B., Ash, J. T., Goel, S., Krishnamurthy, A., and Zhang, C. Exposing attention glitches with\nflip-flop language modeling. arXiv preprint arXiv:2306.00946, 2023.\nMin, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L. Re-\nthinking the role of demonstrations: What makes in-context learning work?\narXiv preprint\narXiv:2202.12837, 2022.\nMosaicML. Introducing mpt-7b: A new standard for open source, commercially usable llms, 2023.\nURL www.mosaicml.com/blog/mpt-7b. Accessed: 2023-05-05.\nMotamedi, M., Sakharnykh, N., and Kaldewey, T. A data-centric approach for training deep neural\nnetworks with less data. arXiv preprint arXiv:2110.03613, 2021.\nNogueira, R., Jiang, Z., and Lin, J. Investigating the limitations of transformers with simple arithmetic\ntasks. arXiv preprint arXiv:2102.13019, 2021.\nNye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H., Austin, J., Bieber, D., Dohan, D.,\nLewkowycz, A., Bosma, M., Luan, D., et al. Show your work: Scratchpads for intermediate\ncomputation with language models. arXiv preprint arXiv:2112.00114, 2021.\nOntan\u00f3n, S., Ainslie, J., Cvicek, V., and Fisher, Z. Making transformers solve compositional tasks.\narXiv preprint arXiv:2108.04378, 2021.\nP\u00e9rez, J., Barcel\u00f3, P., and Marinkovic, J. Attention is turing complete. The Journal of Machine\nLearning Research, 22(1):3463\u20133497, 2021.\nPeterson, J., Meylan, S., and Bourgin, D. Open clone of openai\u2019s unreleased webtext dataset scraper.\nGitHub, 2019. URL https://github.com/jcpeterson/openwebtext.\nPope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J., Heek, J., Xiao, K., Agrawal, S., and\nDean, J. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems,\n5, 2023.\nQian, J., Wang, H., Li, Z., Li, S., and Yan, X. Limitations of language models in arithmetic and\nsymbolic induction. arXiv preprint arXiv:2208.05051, 2022.\nRadford, A. and Narasimhan, K. Improving language understanding by generative pre-training. 2018.\nRajani, N. F., McCann, B., Xiong, C., and Socher, R. Explain yourself! leveraging language models\nfor commonsense reasoning. arXiv preprint arXiv:1906.02361, 2019.\nRazeghi, Y., Logan IV, R. L., Gardner, M., and Singh, S. Impact of pretraining term frequencies on\nfew-shot reasoning. arXiv preprint arXiv:2202.07206, 2022.\nRecht, B. A simpler approach to matrix completion. Journal of Machine Learning Research, 12(12),\n2011.\nReed, S. and De Freitas, N. Neural programmer-interpreters. arXiv preprint arXiv:1511.06279, 2015.\n32\nRoy, S. and Roth, D. Solving general arithmetic word problems. arXiv preprint arXiv:1608.01413,\n2016.\nShaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. arXiv\npreprint arXiv:1803.02155, 2018.\nShi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder,\nS., Zhou, D., et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint\narXiv:2210.03057, 2022.\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid, A., Fisch, A., Brown, A. R., Santoro, A.,\nGupta, A., Garriga-Alonso, A., et al. Beyond the imitation game: Quantifying and extrapolating\nthe capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\nSun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F. A\nlength-extrapolatable transformer. arXiv preprint arXiv:2212.10554, 2022.\nSutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks.\nAdvances in neural information processing systems, 27, 2014.\nTalmor, A., Tafjord, O., Clark, P., Goldberg, Y., and Berant, J. Leap-of-thought: Teaching pre-\ntrained models to systematically reason over implicit knowledge. Advances in Neural Information\nProcessing Systems, 33:20227\u201320237, 2020.\nTay, Y., Wei, J., Chung, H. W., Tran, V. Q., So, D. R., Shakeri, S., Garcia, X., Zheng, H. S., Rao,\nJ., Chowdhery, A., et al. Transcending scaling laws with 0.1% extra compute. arXiv preprint\narXiv:2210.11399, 2022.\nThoppilan, R., De Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos,\nT., Baker, L., Du, Y., et al. Lamda: Language models for dialog applications. arXiv preprint\narXiv:2201.08239, 2022.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal,\nN., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G. Llama: Open and\nefficient foundation language models, 2023.\nUesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and\nHiggins, I. Solving math word problems with process-and outcome-based feedback. arXiv preprint\narXiv:2211.14275, 2022.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and\nPolosukhin, I. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\nWallace, E., Wang, Y., Li, S., Singh, S., and Gardner, M. Do nlp models know numbers? probing\nnumeracy in embeddings. arXiv preprint arXiv:1909.07940, 2019.\nWang, C., Zheng, B., Niu, Y., and Zhang, Y. Exploring generalization ability of pretrained language\nmodels on arithmetic and logical reasoning.\nIn Natural Language Processing and Chinese\nComputing: 10th CCF International Conference, NLPCC 2021, Qingdao, China, October 13\u201317,\n2021, Proceedings, Part I 10, pp. 758\u2013769. Springer, 2021.\nWang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of\nthought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\nWebb, T., Holyoak, K. J., and Lu, H. Emergent analogical reasoning in large language models. arXiv\npreprint arXiv:2212.09196, 2022.\nWei, C., Chen, Y., and Ma, T. Statistically meaningful approximation: a case study on approximating\nturing machines with transformers. Advances in Neural Information Processing Systems, 35:\n12071\u201312083, 2022a.\nWei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M.,\nZhou, D., Metzler, D., et al.\nEmergent abilities of large language models.\narXiv preprint\narXiv:2206.07682, 2022b.\n33\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022c.\nYuan, Z., Yuan, H., Tan, C., Wang, W., and Huang, S. How well do large language models perform in\narithmetic tasks? arXiv preprint arXiv:2304.02015, 2023.\nYun, C., Bhojanapalli, S., Rawat, A. S., Reddi, S. J., and Kumar, S. Are transformers universal\napproximators of sequence-to-sequence functions? arXiv preprint arXiv:1912.10077, 2019.\nZaremba, W. and Sutskever, I. Learning to execute. arXiv preprint arXiv:1410.4615, 2014.\nZaremba, W., Kurach, K., and Fergus, R. Learning to discover efficient mathematical identities.\nAdvances in Neural Information Processing Systems, 27, 2014.\nZelikman, E., Mu, J., Goodman, N. D., and Wu, Y. T. Star: Self-taught reasoner bootstrapping\nreasoning with reasoning. 2022.\nZhou, D., Sch\u00e4rli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans, D., Bousquet, O., Le, Q.,\nand Chi, E. Least-to-most prompting enables complex reasoning in large language models. arXiv\npreprint arXiv:2205.10625, 2022a.\nZhou, H., Nova, A., Larochelle, H., Courville, A., Neyshabur, B., and Sedghi, H. Teaching algorithmic\nreasoning via in-context learning. arXiv preprint arXiv:2211.09066, 2022b.\nAppendix\nTable of Contents\nA Proofs\n35\nB Additional Experiments\n36\nB.1\nZero-Padding and Symbol Wrapping . . . . . . . . . . . . . . . . . . . . . . .\n36\nB.2\nLow-Rank Matrix Completion . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nB.3\nPrompting with Text . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nB.4\nAnalyzing the results on Sine/Sqrt . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nC Experimental Setup\n41\nC.1\nDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n41\nC.2\nModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nC.3\nHyperparameter Configurations . . . . . . . . . . . . . . . . . . . . . . . . . .\n44\nD Prompt Examples\n46\nD.1 Addition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n46\nD.2\nSubtraction\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n47\nD.3\nMultiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nD.4\nSine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n48\nD.5\nSquare Root . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nD.6\nNoisy Simple Scratchpad\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n49\nD.7\nExample data for GPT-3 fine-tuning . . . . . . . . . . . . . . . . . . . . . . . .\n50\n34\nA\nProofs\nHere, we present the proofs of Lemma 1 and 2.\nLemma 1. Let A and B be two n-digit numbers, and let C = A + B. Suppose an algorithm A\noutputs the digits of C in decreasing order of significance, then A must have access to all digits of A\nand B starting from the first digit that it outputs.\nProof. We begin by assuming for contradiction that there does exist an algorithm Algo that does\nnot have access to all digits of A and B and still outputs C = A + B correctly for all n\u2212 digit\nnumbers A, B. Without loss of generality, say Algo does not have access to the k\u2212th digit of A\nwhere k \u2208 [n] represents the position counting from the least significant digit. Then consider the\nexample B = (10n \u2212 1) and (A = 000 . . . Ak00 . . . 0) where B is just the integer with n 9\u2019s and A\nis just 0\u2019s with Ak in the kth position. If Ak = 0, then Cn+1 = 0, but if Ak = 1, then Cn+1 = 1.\nTherefore, without access to the k\u2212th digit of A, there exist examples where the algorithm will surely\nmake a mistake. Therefore, by contradiction such an Algo cannot exist.\nLemma 2. There exists an algorithm that computes C = A + B for two n-digit numbers A and B\nand outputs its digits in increasing order of significance such that, at each position i, the algorithm\nonly requires access to the ith digits of A and B, as well as the carry-on from the previous position.\nProof. First note that the trivial algorithm for addition is exactly the proof of this Lemma. However,\nwe present a more formal argument below for completeness. Let A, B, and C be n\u2212digit numbers\nsuch that C = A + B. Define the digits of A, B, and C as Ai, Bi, and Ci, respectively, for i \u2208 [n]\ncounting from the least significant digit once again. Then, the addition can be performed using the\nfollowing steps. First, Ci = (Ai + Bi + carryi) mod 10 where carryi is the carry-on from the\naddition of digits at position i \u2212 1. If there is no carry from the previous position, then carryi = 0.\nThe carry for the next position is then calculated as carryi+1 =\nj\nAi+Bi+carryi\n10\nk\n.\nPutting this together, the algorithm for addition can be described as follows:\nStep 1: Set carry1 = 0. Repeat for i = {1, . . . , n}: {Step 2: Compute Ci = (Ai + Bi + carryi)\nmod 10 and carryi+1 =\nj\nAi+Bi+carryi\n10\nk\n, Step 3: Output Ci}.\nIt is easy to see that this algorithm computes the digits of the sum C correctly and requires only the\nindividual digits at position i and the carry from the previous position. Therefore, this algorithm\nsatisfies the conditions of the lemma.\n35\nB\nAdditional Experiments\nB.1\nZero-Padding and Symbol Wrapping\nAs discussed briefly in Section 3, we found a significant benefit to using padding for multi-digit\naddition. Throughout our experiments, we use the plain format without any such padding (denoted as\n\u201cvanilla\u201d below) as the default baseline representing the conventional data format used in training.\nNonetheless, we explore modifications to this plain format to enhance performance; zero-padding,\nand wrapping with a single symbol. Zero-padding ensures a fixed length for operands and the\noutput. In the case of 3-digit addition, this means 3-digit operands and a 4-digit output. For\nexample, \u2018112 + 29 = 141\u2019 becomes \u2018112 + 029 = 0141\u2019. As shown in Table 9. this modification\nsignificantly improves model performance. Next, we wrap each sample using the \u2018$\u2019 symbol as in\n\u2019$112 + 29 = 141$\u2019. We found this performs on par with zero-padding.\nAs a result, we adopt the \u2018$\u2019 symbol for efficient data delimiter, extending its use to the reverse format.\nFigure 24 shows \u2018$\u2019-wrapping also enhances the performance of the reverse format. Despite the\nplain format being improved with the \u2018$\u2019 delimiter, it remains short of the reverse format\u2019s accuracy\nand sample efficiency. We continue to maintain the original plain format as a baseline since it not\nonly exemplifies conventional data but further emphasizes the need for improved data formatting to\nensure efficient training. As such, for the reverse format, we have incorporated the \u2018$\u2019 delimiter in\nour formatting modifications.\nTable 9: Test accuracy of NanoGPT model on 3-digit addition trained on 10, 000 samples of plain\nformat data, comparing (i) vanilla format without modifications, (ii) Zero-padding format, and (iii)\n\u2018$\u2019-wrapped format. The results show significant performance enhancement through zero-padding for\nfixed length and similar improvements when deploying a single-symbol wrapping.\nVanilla\nZero-pad\n\u2018$\u2019-Wrapped\n88.17%\n97.74%\n97.76%\n2000\n4000\n6000\n8000\n10000\nNumber of train examples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nreverse, with $\nreverse, wihout $\nplain, with $\nplain, wihout $\nFigure 24: Performance of NanoGPT model on 3-digit addition using plain and reverse format, both\nwith and without \u2018$\u2019 delimiter. The addition of the \u2018$\u2019 symbol noticeably enhances performance\nin both formats. Nevertheless, the plain format underperforms compared to the reverse format,\nparticularly in terms of sample efficiency. While we maintain the original plain format as a baseline \u2013\nemphasizing the necessity for improved data formatting for efficient emergence \u2013 we incorporate the\n\u2018$\u2019 wrapping in our modified reverse format.\n36\nB.2\nLow-Rank Matrix Completion\nIn our Low-Rank Matrix Completion experiment for the addition matrix (which is of rank-2), we\nemploy an iterative algorithm proposed by Kir\u00e1ly et al. (2015). This algorithm systematically searches\nfor a 2 \u00d7 2 submatrix in which three entries are known and one entry is unknown. It then fills the\nunknown entry to ensure that the determinant of the 2\u00d72 submatrix becomes zero, where the solution\nis known to be optimal. We present the full pseudo-code in Algorithm 1.\nTo assess the performance of the algorithm, we generate n \u00d7 n addition matrices for various values\nof n (e.g., 20, 50, 100, 500). We vary the number of revealed entries, randomly sampling a sparse\nmatrix where only a specified number of entries between n and n \u00d7 n are known, while the remaining\nentries are set to zero. We repeat this process 100 times for each number of revealed entries, tracking\nthe algorithm\u2019s success or failure in finding the solution. We calculate the average success rate across\nthe trials and present the success probabilities in Figure 5a, where we observe a sharp phase transition\nwhen O(n) entries are observed, as expected.\nAlgorithm 1: Iterative 2 \u00d7 2 Matrix Completion Algorithm\nData: Data Matrix M \u2208 Rn\u00d7n with partially revealed entries. Assumed to be of Rank 2.\nResult: c\nM \u2208 Rn\u00d7n, Success/Fail.\n1 n1 \u2190 1 represents number of resolved submatrices.\n2 n2 \u2190 0 represents number of unresolved submatrices.\n3 c\nM \u2190 M\n4 while n1 \u2265 1 do\n/* As long as we resolved at least one submatrix in the previous iteration */\n5\nn1 \u2190 0\n6\nn2 \u2190 0\n7\nfor i = 1 to n do\n8\nfor j = 1 to n do\n/* do something\n*/\n9\n10\nif c\nMi,j is not revealed and all its neighbors are revealed then\n11\nc\nMi,j =\nc\nMi+1,j\u00d7 c\nMi,j+1\nc\nMi+1,j+1\n12\nn1 \u2190 n1 + 1\n13\nif c\nMi+1,j is not revealed and all its neighbors are revealed then\n14\nc\nMi+1,j =\nc\nMi,j\u00d7 c\nMi+1,j+1\nc\nMi+1,j\n15\nn1 \u2190 n1 + 1\n16\nif c\nMi+1,j+1 is not revealed and all its neighbors are revealed then\n17\nc\nMi+1,j+1 =\nc\nMi+1,j\u00d7 c\nMi,j+1\nc\nMi,j\n18\nn1 \u2190 n1 + 1\n19\nif c\nMi,j, c\nMi+1,j, c\nMi,j+1, c\nMi+1,j+1 are all revealed then\n20\ncontinue\n21\nelse\n22\nn2 \u2190 n2 + 1\n23 if n2 > 0 then\n24\nreturn c\nM, Fail\n25 else\n26\nreturn c\nM, Success\n37\nB.3\nPrompting with Text\nTo extend on the few-shot prompting experiments from Section 8.2, we also evaluate the effect of\nprompting the model with pure-text prompts. If few-shot prompting with addition samples improves\naccuracy through in-context learning, we expect few-shot prompting with text to hurt accuracy since\nthe text exemplars are out-of-context. We use five different types of text exemplars: (i) Prompt1: a\nshort text prompt that is not present in the Shakespeare dataset, (ii) Prompt2: a short text prompt\nextracted from within Shakespeare dataset, (iii) Prompt3: a longer form text prompt extracted from\nwithin the Shakespeare dataset, (iv) Prompt4: a prompt that includes numbers, and (v) Prompt5: a\nlong text prompt that is not present in the Shakespeare dataset. More details on the text prompts can\nbe found in Figure 25.\nText prompts for few-shot experiments\nExamples of the different text prompts used in the few-shot experiment.\nEach\nexemplar is separated by \u2018---\u2019.\nPrompt 1. Short, /\u2208 Shakespeare\net tu brute\n---\nhello , world\n---\nhow are you\ndoing?\n---\nagi is coming\n---\nboom! stability\nPrompt 2. Short, \u2208 Shakespeare\nJULIET:\nRomeo!\n---\nAll:\nResolved. resolved.\n---\nVOLUMNIA:\nWhy , I pray\nyou?\n---\nCORIOLANUS:\nNay! prithee , woman ,--\n---\nMENENIUS:\nI mean , thy\ngeneral.\nPrompt 3. Long, \u2208 Shakespeare\nJULIET:\nRomeo!\nROMEO:\nMy dear?\n---\nMENENIUS:\nThis is good\nnews:\nI will go meet\nthe\nladies. This\nVolumnia\nIs worth of consuls , senators , patricians ,\n---\nLADY\nANNE:\nFoul devil , for God 's sake , hence , and\ntrouble\nus not;\nFor\nthou\nhast\nmade\nthe\nhappy\nearth\nthy hell ,\nFill 'd it with\ncursing\ncries\nand\ndeep\nexclaims\n.\n---\nBUCKINGHAM :\nI fear he will.\nHow now , Catesby , what\nsays\nyour\nlord?\n---\nCATESBY:\nBad news , my lord: Ely is fled to\nRichmond;\nAnd\nBuckingham , back 'd with\nthe\nhardy\nWelshmen\n,\nIs in the field , and\nstill\nhis\npower\nincreaseth .\nPrompt 4. Has number, /\u2208 Shakespeare\nI go 16 -12\nThat 's the\ncode to my heart , ah\nI go 1-6-1-2\nStar\n---\nLike a river\nflows\n17 -23\nSurely\nto the sea 15 -22\nDarling , so it goes 46 -92\nSome\nthings\nare\nmeant to be\n---\nI got my first\nreal 6-string\nBought\nit at the\nfive\nand\ndime\nPlayed\nit 'til my\nfingers\nbled\nWas the\nsummer of\n'69\n---\nI think\nsomeday I might\njust 5-3-2-1 get a real\njob\nI spent\nhalf of my life 1-2-3 in a bus or on a flight\nI'm getting\noff 17-36-8-2 the\nroad\nand in a real\njob\n---\nEvery\ntime\nthat\n27 -67 -29 I look in the\nmirror\nAll\nthese\nlines on my 1-3-92-5 face\ngetting\nclearer\nThe\npast 45-5-3 is gone\nPrompt 5. Long, /\u2208 Shakespeare\n38\nIs this\nthe\nreal\nlife? Is this\njust\nfantasy? Caught in a landside , no escape\nfrom\nreality.\nOpen\nyour eyes , look up to the\nskies\nand see.\nI'm just a poor boy , I need no\nsympathy. Because I'm easy come , easy go ,\nLittle\nhigh , little low ,\nAny way the\nwind\nblows\ndoesn 't really\nmatter to me , to me.\n---\nIt 's my life\nAnd it 's now or never\nI ain 't gonna\nlive\nforever\nI just\nwant to live\nwhile I'm alive\nMy heart is like an open\nhighway\nLike\nFrankie\nsaid , I did it my way\n---\nDestruction\nleads to a very\nrough\nroad\nbut it also\nbreeds\ncreation\nAnd\nearthquakes\nare to a girl 's guitar , they 're just\nanother\ngood\nvibration\nAnd\ntidal\nwaves\ncouldn 't save\nthe\nworld\nfrom\nCalifornication\n---\nI want to stay\nBut I need to go\nI want to be the\nbest\nfor you\nBut I just don 't know\nwhat to do\n'Cause baby , say I've cried\nfor you\nThe\ntime we have\nspent\ntogether\nRiding\nthrough\nthis\nEnglish\nwhether\n---\nLorem\nipsum\ndolor\nsit amet , consectetur\nadipiscing\nelit. Vestibulum\nmattis in leo\nvel\ngravida.\nPellentesque\nlibero elit , scelerisque\nvarius\nvehicula a, hendrerit\net\ntellus.\nProin\nconvallis\nneque nisl , nec\nlobortis\nest\nscelerisque\ntincidunt.\nNunc\nvenenatis\nauctor\nurna.\nClass\naptent\ntaciti\nsociosqu\nad litora\ntorquent\nper\nconubia\nnostra.\nFigure 25: Text prompt exemplars for few-shot experiments.\n(a) NanoGPT, Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\nPrompt1\nPrompt2\nPrompt3\nPrompt4\nPrompt5\n(b) GPT-2, Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\nPrompt1\nPrompt2\nPrompt3\nPrompt4\nPrompt5\n(c) NanoGPT, Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\nPrompt1\nPrompt2\nPrompt3\nPrompt4\nPrompt5\n(d) GPT-2, Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\nPrompt1\nPrompt2\nPrompt3\nPrompt4\nPrompt5\nFigure 26: Experiments on few-shot prompting with different text prompts: (i) Prompt1: short\ntext not in Shakespeare dataset (ii) Prompt2: short text within Shakespeare dataset (iii) Prompt3:\nlong text within Shakespeare dataset (iv) Prompt4: text with numbers (v) Prompt5: long text not in\nthe Shakespeare dataset. Each prompt (Prompt 1-5) consists of five distinct exemplars. The solid\nlines represent the mean performance across the five exemplars, while the shaded area indicates the\nstandard deviation. We observe that the effectiveness of text prompts varies greatly depending on the\nexemplars used.\n39\nThe results presented in Figure 26 show notable variations in evaluation accuracy for addition,\ndepending on the chosen text prompts. Longer text prompts (Prompt 5) typically result in a more\nsignificant decline in performance. With the exception of NanoGPT trained on plain addition, the\nresult in Figure 27 indicates that employing text prompts followed by test addition queries tends to\nhave an adverse impact on the overall model performance, whereas incorporating relevant few-shot\nexemplars (1/2/3-shot) is beneficial. This aligns well with our intuition on the benefits on in-context\nlearning.\n(a) NanoGPT, Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\n(b) GPT-2, Test accuracy on plain addition\n0\n5k\n10k\n15k\n20k\n25k\n30k\n35k\n40k\nNumber of Addition Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\n(c) NanoGPT, Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\n(d) GPT-2, Test accuracy on detailed scratchpad\n1000\n2000\n3000\n4000\n5000\nNumber of Detailed Scratchpad Samples\n30\n40\n50\n60\n70\n80\n90\n100\nTest Accuracy (%)\nZero-shot\n1-shot\n2-shot\n3-shot\ntext_prompt\nFigure 27: Performance of NanoGPT and GPT-2 model trained with entire Shakespeare dataset\nand a varying number of samples of plain addition, and addition with detailed scratchpad dataset.\nPerformance is evaluated on test prompts formatted as plain addition and detailed scratchpad. Few-\nshot experiments are based on an average of 5 exemplars, while text prompts involve an average\nof 25 exemplars. The shaded area represents the standard deviation. Our observations indicate\nthat few-shot prompting consistently improves performance, whereas test prompts generally have a\nnegative impact.\n40\nB.4\nAnalyzing the results on Sine/Sqrt\nSince sine and sqrt are arguably more complicated functions than the remaining arithmetic tasks, we\ndecided to more carefully analyze their performance. As shown in Figure 28, sin shows excellent\nperformance across all data formats around sin(x) = 0. We conjecture that this is because sin(x) \u2248 x\nfor x \u2248 0, which is easy to learn. We also note that accuracy once again improves close to \u00b11\npotentially for similar reasons.\n(a) Test accuracy on Sine\n1.00 0.75 0.50 0.250.00 0.25 0.50 0.75 1.00\ntrue y value\n20\n40\n60\n80\n100\nTest Accuracy (%)\nSin\nplain, =0\nar, =0\nplain, =5e-4\nar, =5e-4\n(b) Test accuracy on Square root\n1.0\n1.5\n2.0\n2.5\n3.0\ntrue y value\n20\n40\n60\n80\n100\nTest Accuracy (%)\nSqrt\nplain, =0\nCoT, =0\nplain, =5e-4\nCoT, =5e-4\nFigure 28: Error analysis of sine and square root functions, considering varying error tolerance (eps)\nthresholds to determine correct output. The sine function demonstrates excellent performance across\nall data formats, particularly around sin(x) = 0, where sin(x) \u2248 x for x \u2248 0. Additionally, we\nobserve improved accuracy near \u00b11.\nC\nExperimental Setup\nIn this section, we summarize the datasets, models and hyperparameters used for experiments. All\nof our experiments on NanoGPT and GPT-2 models are run using PyTorch 2.1 and CUDA 11.7 on\nNvidia 2808 TIs and NVIDIA 3090s. Detailed dependencies are provided on our github repository5.\nC.1\nDataset\nIn this section, we explain the details of the datasets used for our experiments. For arithmetic tasks,\nwe construct our own datasets as described below while we use the standard shakespeare (Karpathy,\n2015) dataset for text.\nArithmetic Tasks\nAs mentioned above, for all arithmetic tasks, we prepare our own datasets.\nWe refer to the training dataset for a binary operator f(\u00b7) as Dtrain = {(x1\ni , x2\ni ), yi}N\ni=1 where\nyi = f(x1\ni , x2\ni ). Similarly, the test dataset Dtest is constructed by randomly sampling pairs of\noperands that do not appear in Dtrain. During both training and inference, we then apply different\nformatting techniques (see Section 3), to construct the final sequence that is input to the model. We\nwould like to repeat that both the careful choice of samples in the training dataset as well as their\nformatting play a crucial role in the final performance of the model.\nText\nFor text data, we use the Shakespeare dataset which was introduced by Karpathy (2015)\noriginally featured in the blog post \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d.\nIt consists of 40,000 lines of dialogue carefully curated from William Shakespeare\u2019s plays. The dataset\ncomprises of a total of 1,115,394 characters and 64 unique tokens(when using the character-level\ntokenizer that we employed in all NanoGPT experiments).\nC.1.1\nData Balancing\nAs mentioned in Section 3, we carefully sample our data to ensure that they are \u201cbalanced\u201d with\nrespect to the number of carries and number of digits. As mentioned earlier, sampling the operands\nuniformly at random would lead to an extremely skewed dataset. To avoid this, we try to (i) Balance\n5https://github.com/lee-ny/teaching_arithmetic\n41\ndigits by sampling lower-digit numbers with higher weights and (ii) Balance carry-ons by sampling\nsuch that we have equal number of examples with 0, 1, 2 and 3 carry-on operations.\nSpecifically, we create a balanced dataset of 10, 000 samples. This dataset includes all 100 1-digit\nadditions and a random sampling of 900 2-digit additions (including both (2 + 1) and (1 + 2)\ndigit additions) and 9, 000 3-digit additions. For the 3-digit addition samples, we employ rejection\nsampling to ensure an equal distribution of carry-ons (0, 1, 2, or 3). For the test dataset, we uniformly\nsample 10, 000 addition examples that do not overlap with the train dataset. Results in Figure 3 and\nTable 10 demonstrate a clear advantage of the employed data balancing methods.\nFor the train dataset, we follow a specific approach based on the number of examples. For sample\nsizes smaller than 10, 000 (e.g., 500, 1, 000, 2, 000, 3, 000, 4, 000, 5, 000), we include all 1-digit\nadditions and a proportionate number of 2-digit samples (e.g., for a total of 5, 000 samples, we\ninclude 900 \u00d7 5, 000/10, 000 = 450 two-digit additions). The remaining samples are filled with\n3-digit additions from the constructed train dataset of 10,000 samples. For sample sizes larger than\n10,000 (e.g., 20,000, 40,000), we include all examples from the 10,000-sample train dataset and then\nadd additional samples as needed. Similar to before, we perform rejection sampling to maintain an\nequal number of carry operations. Table 11. provides detailed information on the number of samples\nwith 1-digit, 2-digit, and 3-digit additions, as well as the number of carry-ons.\nFor the other arithmetic operations (subtraction, multiplication, sine, and square root), we construct\nthe train dataset using the following approach: (i) For subtraction, we use the same pairs of operands\nthat were used for addition. (ii) For multiplication, we include all 100 cases of a 1-digit number\nmultiplied by a 1-digit number. Additionally, we randomly sample multiplications involving operands\nof up to 2 digits. (iii) For sine, we sample a random number in [\u03c0/2, \u03c0/2] and truncate it to 4 decimal\nplaces. (iv) For square root, we sample a random number between [1, 10] and truncate it to 4 decimal\nplaces. For the test dataset, we sample 10, 000 data points (7, 000 for multiplication) that do not\noverlap with the train dataset.\nTable 10: Performance of addition on various data sampling methods used: (i) Random - uniform\nsampling of operands; (ii) Balanced digits - sampling more 1 and 2-digit operations ; (iii) Balanced\ncarry - balancing the dataset to contain an equal number of carry-on operations. Experiments on\naddition with zero-padding each operand and output to have 3 and 4 digits, respectively. We observe\nthat balancing the dataset can significantly improve the performance or arithmetic operations.\nData Sampling\nOverall\n1-digit\n2-digit\nCarry-0\nCarry-1\nCarry-2\nCarry-3\nRandom\n97.74\n98.00\n96.20\n95.88\n98.61\n98.74\n94.98\nBalanced Digits\n98.13\n100.00\n99.70\n98.87\n98.64\n98.13\n95.93\nBalanced Carry-Ons\n98.29\n100.00\n99.70\n98.38\n97.56\n99.02\n98.22\nTable 11: Number of examples of digit 1/2/3 and 0/1/2/3 carry-ons for NanoGPT experiments on\naddition for different number of samples varying from 500 to 40, 000.\nTotal number\n1-digit\n2-digit\n3-digit\n0-carry-ons\n1-carry-ons\n2-carry-ons\n3-carry-ons\n500\n100\n45\n355\n163\n141\n97\n99\n1000\n100\n90\n810\n283\n268\n236\n213\n2000\n100\n180\n1720\n535\n502\n481\n482\n3000\n100\n270\n2630\n781\n782\n748\n689\n4000\n100\n360\n3540\n1020\n1016\n958\n1006\n5000\n100\n450\n4450\n1279\n1271\n1229\n1221\n10000\n100\n900\n9000\n2500\n2500\n2500\n2500\n20000\n121\n1937\n17942\n5000\n5000\n5000\n5000\n40000\n132\n3939\n35929\n10000\n10000\n10000\n10000\nC.1.2\nData Formatting\nFor each of the four formatting techniques, as applied to each arithmetic operation we provide the\ndetails below. (i) Plain refers to the simplest formatting where we simply create a sequence as the\n42\nmathematical representation of the corresponding operation (e.g., A3A2A1 + B3B1B1 = C3C2C1).\nFor (ii) Reverse, we simply reverse the digits of the output so that they appear in increasing order\nfrom LSB to MSB (e.g., $A3A2A1 + B3B1B1 = C1C2C3$). (iii) Simplified Scratchpad and (iv)\nDetailed Scratchpad provide algorithmic reasoning steps like (Nye et al., 2021; Zhou et al., 2022b)\nso as to help the model get more \u201cinformation\u201d per sample. Our intuition is that this approach nudges\nthe model towards actually learning the algorithm of addition or subtraction rather than merely trying\nto fit the training examples. Refer to Appendix D for detailed examples of data formatting for each\narithmetic operation.\nAddition\nWe focus on additions of positive numbers up to 3-digits, in which the plain format-\nting would look like A3A2A1 + B3B1B1 = C3C2C1. For experiments on comparing data sampling\npresented in Figure 3, we pad the two operands and the output with zero, to be of length 3 and 4\nrespectively. For all other experiments, we do not utilize zero-padding.\nFor Scratchpad-based\nmethods (iii, iv), we provide the digit-wise addition (denoted as A) and carry-on (denoted as C)\ninformation for intermediate steps from the least significant bit (LSB) to the most significant bit\n(MSB).\nSubtraction\nWe consider subtraction of positive numbers up to 3 digits,\nwritten as\nA3A2A1 \u2212 B3B2B1 = C3C2C1 for plain formatting. As with addition, Scratchpad-based methods (iii,\niv), present the intermediate steps of digit-wise subtraction and carry-ons6. These steps are performed\nfrom the least significant bit (LSB) to the most significant bit (MSB). If the final result after computing\nall the digit-wise subtractions is negative, we subtract the number in the most significant bit (MSB)\nposition multiplied by 10 to the power of (number of digits in the output - 1) from the remaining digits\nin the output. In Section 6.2, we present an alternative version of the detailed scratchpad formatting\nfor subtraction.\nMultiplication\nWe consider multiplication of positive numbers only up to 2-digits. Examples\nwith (i) plain formatting look like: A2A1 \u2217 B2B1 = C4C3C2C1 while (ii) reverse is formatted as\nA2A1 \u2217 B2B1 = C1C2C3C4. For (iv) detailed scratchpad method, we simplify each intermediate step\nby performing a series of multiplications between the first operand and each digit of the second\noperand, starting from the least significant bit (LSB) and moving towards the most significant bit\n(MSB). For each step, we multiply the result by an exponentiation of 10 corresponding to the relative\ndigit position.\nSine\nWe consider decimal numbers in the range of [\u2212\u03c0/2, \u03c0/2], truncated to 4-digits of preci-\nsion with (i) plain formatting: sin(A0.A1A2A3A4) = B0.B1B2B3B4. For (iv) detailed scratchpad,\nwe include the individual steps of the Taylor series expansion for sine, which is represented as\nsin(x) = x \u2212 1\n3!x3 + 1\n5!x5 \u2212 1\n7!x7 + \u00b7 \u00b7 \u00b7 . It is important to note that these intermediate steps involve\nexponentiation, which may not be any easier to compute than the sine operation itself.\nSquare Root\nWe consider decimal numbers in the range of [1, 10), truncated to 4-digits of precision\nwith the format, with (i) plain formatting: sqrt(A0.A1A2A3A4) = B0.B1B2B3B4. For (iv) detailed\nscratchpad, We present each step of Newton\u2019s method for computing the square root function. The\niterative formula is given by xn = 1\n2(xn\u22121 +\nx\nxn\u22121 ), where x0 is initialized as the floor of the square\nroot value of the operand x. It is important to note that these intermediate steps involve a division\noperation, which can be as complex as the square root operation itself.\nC.2\nModel\nFor all experiments, we use a Decoder-only Transformer architecture. Specifically, we primarily use\nthe NanoGPT model, a scaled-down variant of the GPT-2 model with half the number of self-attention\nlayers, heads, and embedding dimension. Note that we use character-level tokenization instead of\nusing the OpenAI\u2019s BPE tokenizer (Tiktoken) of vocabulary size 50257, making the vocabulary\nsize significantly smaller. We use a learnable absolute positional embedding initialized randomly,\nfollowing the GPT-2 model. Are results are generated using a temperature of 0.8.\n6As explained in Section 3, we use the term \u201ccarry-on\" to refer to the \u201cborrow\" operation\n43\nIn the case of arithmetic tasks performed on plain and reverse formatting, we set a context length of\n256 for NanoGPT experiments. The length of a single train example falls within the range of 13 to\n15, approximately. However, when conducting experiments on scratchpad formatting, we increase\nthe context length to 1024. This adjustment allows us to accommodate more examples per batch. In\nthe case of simplified scratchpad, the length of each train example is approximately 64, while the\ndetailed scratchpad has a length of approximately 281. For GPT-2 experiments we fix the context\nlength to 1024 for all experiments. See Table 12 for details on model configuration.\nFor experiments on fine-tuning a pretrained large language model, we use OpenAI\u2019s GPT-3 model -\nAda, Curie, and Davinci.\nFigure 29: The GPT-2 Architecture. Image from (Radford & Narasimhan, 2018). NanoGPT\nmodel is a smaller model with half the number of self-attention layers, multi-heads, and embedding\ndimensions.\nTable 12: NanoGPT and GPT-2 model configuration\nModel\nInput Formatting\nContext Length\nSelf-Attn Layers\nNum Heads\nEmbedding Dim\nNanoGPT\nPlain, Reverse\n256\n6\n6\n384\nScratchpad\n1024\n6\n6\n384\nGPT-2\nPlain, Reverse\n1024\n12\n12\n768\nScratchpad\n1024\n12\n12\n768\nC.3\nHyperparameter Configurations\nIn this section, we provide a detailed overview of the hyperparameter configuration used in our\nexperiments in Table 13 and 14. To enhance memory efficiency and training speed, we employ flash\nattention. For most experiments, we utilize the bfloat16 data type. However, when working with\nNvidia 2080 GPUs, which do not support bfloat16, we switch to float16. It is worth noting that we\ndid not observe significant differences in training and evaluation performance between the two data\ntypes.\nFor the GPT-2 experimentation, we reduced the batch size to 8 to accommodate the GPU memory\nlimitations. However, to mitigate the impact of the smaller batch size, we employed gradient accu-\nmulation steps. This approach involves taking multiple steps between gradient updates, effectively\nincreasing the effective batch size to 64. For specific hyperparameter details, please refer to Table 14.\n44\nTable 13: Hyper Parameters used for NanoGPT experiments on arithmetic tasks\nInput Format\nBatch Size\nOptimizer\nLR\nBetas\nIterations\nWarmup Iter\nWt decay\nDropout\nPlain, Reverse\n256\nAdamW\n0.001\n(0.9, 0.99)\n5000\n100\n0.1\n0.2\nScratchpad\n16\nAdamW\n0.001\n(0.9, 0.99)\n50000\n0\n0.1\n0.2\nTable 14: Hyper Parameters used for GPT-2 experiments on arithmetic tasks\nInput Format\nBatch Size\nOptimizer\nLR\nBetas\nIterations\nWarmup Iter\nWt decay\nDropout\nPlain, Reverse\n64\nAdamW\n0.0005\n(0.9, 0.99)\n5000\n100\n0.1\n0.2\nScratchpad\n64\nAdamW\n0.0005\n(0.9, 0.99)\n20000\n0\n0.1\n0.2\nTable 15: Hyper Parameters used for tandem training experiments in Section 9.\nModel\nBatch Size\nOptimizer\nLR\nBetas\nIterations\nWarmup Iter\nWt decay\nDropout\nNanoGPT\n16\nAdamW\n0.001\n(0.9, 0.99)\n5000\n0\n0.1\n0.2\nGPT-2\n40\nAdamW\n0.0006\n(0.9, 0.95)\n50000\n2000\n0.1\n0.2\n(a) NanoGPT, plain addition\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n0\n20\n40\n60\n80\nTest Accuracy (%)\n(b) NanoGPT, detailed scratchpad addition\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n(c) NanoGPT, Perplexity\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n1.10\n1.15\n1.20\n1.25\n1.30\nPerplexity\n(d) GPT-2, plain addition\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n0\n20\n40\n60\n80\nTest Accuracy (%)\n(e) GPT-2, detailed scratchpad addition\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n0\n20\n40\n60\n80\n100\nTest Accuracy (%)\n(f) GPT-2, Perplexity\n0\n10000\n20000\n30000\n40000\n50000\nIterations\n1.2\n1.4\n1.6\n1.8\n2.0\nPerplexity\nFigure 30: Training loss curves for NanoGPT and GPT-2 trained with varying numbers of plain (Add)\nand detailed scratchpad (DS) samples as well as the shakespeare dataset as described in Section 9.\nAs we can see, the model continues to improve in addition accuracy as the number of iterations\nincreases. However, the training perplexity on Shakespeare also tends to increase, which indicates\nsome overfitting. However, we note that the model still outputs \u201creasonable\u201d text when prompted\nwith shakespeare text.\n45\nD\nPrompt Examples\nIn this section, we provide three examples of each formatting (plain, reverse, simplified scratchpad,\ndetailed scratchpad) of arithmetic operations (+, \u2212, \u00d7, sin , \u221a).\nD.1\nAddition\nAddition Examples\nPlain\n266+738= 1004\n980+743= 1723\n41+34= 75\nReverse\n$913 +524= 1437$\n$226 +598= 824$\n$35 +58= 93$\nSimplified Scratchpad\nInput:\n922+244\nTarget:\nA->6 , C->0\nA->6 , C->0\nA->1 , C->1.\n1166\nInput:\n285+43\nTarget:\nA->8 , C->0\nA->2 , C->1\nA->3 , C->0.\n328\nInput:\n993+849\nTarget:\nA->2 , C->1\nA->4 , C->1\nA->8 , C->1.\n1842\nDetailed Scratchpad\nInput:\n396+262\nTarget:\n<scratch>\n[3,9,6] has 3 digits.\n[2,6,2] has 3 digits.\n[3,9,6] + [2,6,2] , A=[] , C=0 , 6+2+0=8 , A->8 , C->0\n[3,9] + [2,6] , A=[8] , C=0 , 9+6+0=15 , A->5 , C->1\n[3] + [2] , A=[5,8] , C=1 , 3+2+1=6 , A->6 , C->0\n[] + [] , A=[6,5,8] C=0 , END\n</scratch>\n6 5 8\nInput:\n796+890\nTarget:\n<scratch>\n[7,9,6] has 3 digits.\n[8,9,0] has 3 digits.\n[7,9,6] + [8,9,0] , A=[] , C=0 , 6+0+0=6 , A->6 , C->0\n[7,9] + [8,9] , A=[6] , C=0 , 9+9+0=18 , A->8 , C->1\n[7] + [8] , A=[8,6] , C=1 , 7+8+1=16 , A->6 , C->1\n[] + [] , A=[6,8,6] C=1 , END\n</scratch>\n1 6 8 6\nInput:\n788+989\nTarget:\n<scratch>\n[7,8,8] has 3 digits.\n[9,8,9] has 3 digits.\n[7,8,8] + [9,8,9] , A=[] , C=0 , 8+9+0=17 , A->7 , C->1\n[7,8] + [9,8] , A=[7] , C=1 , 8+8+1=17 , A->7 , C->1\n[7] + [9] , A=[7,7] , C=1 , 7+9+1=17 , A->7 , C->1\n[] + [] , A=[7,7,7] C=1 , END\n</scratch>\n1 7 7 7\n46\nD.2\nSubtraction\nSubtraction Examples\nPlain\n266 -738= -472\n980 -743= 237\n41 -34= 7\nReverse\n$913 -524= 983$\n$226 -598= 273-$\n$35 -58= 32-$\nSimplified Scratchpad\nInput:\n396 -262\nTarget:\nA->4 , C->0\nA->3 , C->0\nA->1 , C->0\n100+34=134.\n134\nInput:\n796 -890\nTarget:\nA->6 , C->0\nA->0 , C->0\nA->-1 , C->-1\n-100+6=-94.\n-94\nInput:\n788 -989\nTarget:\nA->9 , C->-1\nA->9 , C->-1\nA->-3 , C->-1\n-300+99=-201.\n-201\nDetailed Scratchpad\nInput:\n396 -262\nTarget:\n<scratch>\n[3,9,6] has 3 digits.\n[2,6,2] has 3 digits.\n[3,9,6] - [2,6,2] , A=[] , C=0 , 6-2-0=4 , A->4 , C->0\n[3,9] - [2,6] , A=[4] , C=0 , 9-6-0=3 , A->3 , C->0\n[3] - [2] , A=[3,4] , C=0 , 3-2-0=1 , A->1 , C->0\n[] - [] , A=[1,3,4]\n100+34=134 , END\n</scratch>\n1 3 4\nInput:\n796 -890\nTarget:\n<scratch>\n[7,9,6] has 3 digits.\n[8,9,0] has 3 digits.\n[7,9,6] - [8,9,0] , A=[] , C=0 , 6-0-0=6 , A->6 , C->0\n[7,9] - [8,9] , A=[6] , C=0 , 9-9-0=0 , A->0 , C->0\n[7] - [8] , A=[0,6] , C=0 , 7-8-0=-1 , A->-1 , C->-1\n[] - [] , A=[-1,0,6]\n</scratch>\n-9 4\nInput:\n788 -989\nTarget:\n<scratch>\n[7,8,8] has 3 digits.\n[9,8,9] has 3 digits.\n[7,8,8] - [9,8,9] , A=[] , C=0 , 8-9-0+10=9 , A->9 , C->-1\n[7,8] - [9,8] , A=[9] , C=-1 , 8-8-1+10=9 , A->9 , C->-1\n[7] - [9] , A=[9,9] , C=-1 , 7-9-1=-3 , A->-3 , C->-1\n[] - [] , A=[-3,9,9]\n-300+99=-201 , END\n</scratch>\n-2 0 1\n47\nD.3\nMultiplication\nMultiplication Examples\nPlain\n5*32= 160\n66*76= 5016\n67*74= 4958\nReverse\n$5 *32= 061$\n$66 *76= 6105$\n$67 *74= 8594$\nDetailed Scratchpad\nInput:\n22*52\nTarget:\n<scratch>\n[2,2] has 2 digits.\n[5,2] has 2 digits.\n[2,2] * 2 , A=[4,4] , k=1 , B=[4,4] , C=0+44=44\n[2,2] * 5 , A=[1,1,0] , k=10 , B=[1,1,0,0] , C=44+1100=1144 , END\n</scratch>\n1 1 4 4\nInput:\n8*69\nTarget:\n<scratch>\n[8] has 1 digits.\n[6,9] has 2 digits.\n[8] * 9 , A=[7,2] , k=1 , B=[7,2] , C=0+72=72\n[8] * 6 , A=[4,8] , k=10 , B=[4,8,0] , C=72+480=552 , END\n</scratch>\n5 5 2\nInput:\n52*34\nTarget:\n<scratch>\n[5,2] has 2 digits.\n[3,4] has 2 digits.\n[5,2] * 4 , A=[2,0,8] , k=1 , B=[2,0,8] , C=0+208=208\n[5,2] * 3 , A=[1,5,6] , k=10 , B=[1,5,6,0] , C=208+1560=1768 , END\n</scratch>\n1 7 6 8\nD.4\nSine\nSine Examples\nPlain\nsin (1.0313)=0.8579\nsin ( -0.6909)=-0.6373\nsin ( -0.5719)=-0.5413\nDetailed Scratchpad\nInput:\nsin (1.0313)\nTarget:\n<scratch>\nx_0=1.0313\nx_1:\nx_0 - 1/3!\n* (x\u02c63) , x_1=0.8484\nx_2:\nx_1 + 1/5!\n* (x\u02c65) , x_2=0.8581\nx_3:\nx_2 - 1/7!\n* (x\u02c67) , x_3=0.8578\nx_4:\nx_3 + 1/9!\n* (x\u02c69) , x_4=0.8578 , END\n</scratch>\n0.8578\nInput:\nsin ( -0.6909)\nTarget:\n<scratch>\nx_0=-0.6909\nx_1:\nx_0 - 1/3!\n* (x\u02c63) , x_1=-0.636\nx_2:\nx_1 + 1/5!\n* (x\u02c65) , x_2=-0.6374\nx_3:\nx_2 - 1/7!\n* (x\u02c67) , x_3=-0.6374\nx_4:\nx_3 + 1/9!\n* (x\u02c69) , x_4=-0.6375 , END\n</scratch>\n-0.6375\nInput:\nsin ( -0.5719)\nTarget:\n<scratch>\nx_0=-0.5719\nx_1:\nx_0 - 1/3!\n* (x\u02c63) , x_1=-0.5408\nx_2:\nx_1 + 1/5!\n* (x\u02c65) , x_2=-0.5414\nx_3:\nx_2 - 1/7!\n* (x\u02c67) , x_3=-0.5414\nx_4:\nx_3 + 1/9!\n* (x\u02c69) , x_4=-0.5415 , END\n</scratch>\n-0.5415\n48\nD.5\nSquare Root\nSquare Root Examples\nPlain\nsqrt (7.2726)=2.6967\nsqrt (3.6224)=1.9032\nsqrt (1.0895)=1.0437\nDetailed Scratchpad\nInput:\nsqrt (7.1042)\nTarget:\n<scratch>\nx_0=2\nx_1:\n1/2*(2+7.1042/2)=2.776, x_1=2.776\nx_2:\n1/2*(2.776+7.1042/2.776)=2.6675, x_2=2.6675\nx_3:\n1/2*(2.6675+7.1042/2.6675)=2.6653, x_3=2.6653\nx_4:\n1/2*(2.6653+7.1042/2.6653)=2.6653, x_4=2.6653 , END\n</scratch>\n2.6653\nInput:\nsqrt (6.2668)\nTarget:\n<scratch>\nx_0=2\nx_1:\n1/2*(2+6.2668/2)=2.5667, x_1=2.5667\nx_2:\n1/2*(2.5667+6.2668/2.5667)=2.5041, x_2=2.5041\nx_3:\n1/2*(2.5041+6.2668/2.5041)=2.5033, x_3=2.5033\nx_4:\n1/2*(2.5033+6.2668/2.5033)=2.5033, x_4=2.5033 , END\n</scratch>\n2.5033\nInput:\nsqrt (8.3216)\nTarget:\n<scratch>\nx_0=2\nx_1:\n1/2*(2+8.3216/2)=3.0804, x_1=3.0804\nx_2:\n1/2*(3.0804+8.3216/3.0804)=2.8909, x_2=2.8909\nx_3:\n1/2*(2.8909+8.3216/2.8909)=2.8847, x_3=2.8847\nx_4:\n1/2*(2.8847+8.3216/2.8847)=2.8847, x_4=2.8847 , END\n</scratch>\n2.8847\nD.6\nNoisy Simple Scratchpad\nWe provide one example for each case of adding noise in the simplified scratchpad experiments\ndiscussed in Section 6.3.\nNoisy Simple Scratchpad Examples\nWe provide one example for each case of adding noise in the simplified\nscratchpad experiments discussed in Section 6.3.\nThe input prompt is\nhighlighted in light blue, while the remaining part is highlighted in light\ngreen.\nWe construct the dataset to have either correct or random digit-sum A\nand carry information C. For all cases, the final answer remains accurate.\nPrompt:\nInput:\n686+886\nTarget:\nCorrect A & C\nA->2 , C->1\nA->7 , C->1\nA->5 , C->1.\n1572\nRandom C\nA->2 , C->0\nA->7 , C->0\nA->5 , C->1.\n1572\nRandom A\nA->0 , C->1\nA->9 , C->1\nA->9 , C->1.\n1572\nRandom A & C\nA->8 , C->1\nA->1 , C->0\nA->2 , C->1.\n1572\n49\nD.7\nExample data for GPT-3 fine-tuning\nWe provide an example from the training dataset consisting of one prompt-completion pair used for\nfine-tuning the GPT-3 model using OpenAI\u2019s API. The prompt is highlighted in light grey, while\nthe completion is highlighted in light green. Note that for plain and reverse formatting, we include\nspacing between digits to ensure consistent tokenization of numbers. \u201c###\u201d is used as the stop\nsequence for generation.\nD.7.1\nAddition\nAddition Examples\nPlain\n6 7 7 + 8 9 8 =1 5 7 5###\nReverse\n7 4 9 + 7 8 5 = 4 3 5 1###\nSimplified Scratchpad\nInput:\n32+981\nTarget:\nA->3 , C->0\nA->2 , C->1\nA->0 , C->1.\n1013###\nDetailed Scratchpad\nInput:\n356+787\nTarget:\n<scratch>\n[3,5,6] has 3 digits.\n[7,8,7] has 3 digits.\n[3,5,6] + [7,8,7] , A=[] , C=0 , 6+7+0=13 , A->3 , C->1\n[3,5] + [7,8] , A=[3] , C=1 , 5+8+1=14 , A->4 , C->1\n[3] + [7] , A=[4,3] , C=1 , 3+7+1=11 , A->1 , C->1\n[] + [] , A=[1,4,3] C=1 , END\n</scratch>\n1 1 4 3###\nD.7.2\nSubtraction\nSubtraction Examples\nPlain\n2 0 4 - 5 0 1 = - 2 9 7###\nReverse\n7 3 4 - 9 6 7 = 3 3 2 -###\nSimplified Scratchpad\nInput:\n695 -489\nTarget:\nA->6 , C->-1\nA->0 , C->0\nA->2 , C->0\n200+6=206.\n206###\nDetailed Scratchpad\nInput:\n848 -367\nTarget:\n<scratch>\n[8,4,8] has 3 digits.[3,6,7] has 3 digits.\n[8,4,8] - [3,6,7] , A=[] , C=0 , 8-7-0=1 , A->1 , C->0\n[8,4] - [3,6] , A=[1] , C=0 , 4-6-0+10=8 , A->8 , C->-1\n[8] - [3] , A=[8,1] , C=-1 , 8-3-1=4 , A->4 , C->0\n[] - [] , A=[4,8,1]\n400+81=481 , END\n</scratch>\n4 8 1###\n50\nD.7.3\nSine\nSine Examples\nPlain\nsin ( -0.8649)\n-0.7611###\nDetailed Scratchpad\nInput:\nsin ( -1.3516)\nTarget:\nx_0=-1.3516\nx_1:\n-1.3516 - 1/3!\n* (x*x*x) , x_1=-0.9401\nx_2:\n-0.9401 + 1/5!\n* (x*x*x*x*x) , x_2=-0.9777\nx_3:\n-0.9777 - 1/7!\n* (x*x*x*x*x*x*x) , x_3=-0.9761\nx_4:\n-0.9761 + 1/9!\n* (x*x*x*x*x*x*x*x*x) , x_4=-0.9762 , END\n</scratch>\n-0.9762###\nD.7.4\nSquare Root\nSquare Root Examples\nPlain\nsqrt (1.2178)\n1.1035###\nDetailed Scratchpad\nInput:\nsqrt (5.5808)\nTarget:\n<scratch>\nx_0=2\nx_1:\n1/2*(2+5.5808/2)=2.3952, x_1=2.3952\nx_2:\n1/2*(2.3952+5.5808/2.3952)=2.3625, x_2=2.3625\nx_3:\n1/2*(2.3625+5.5808/2.3625)=2.3623, x_3=2.3623\nx_4:\n1/2*(2.3623+5.5808/2.3623)=2.3623, x_4=2.3623 , END\n</scratch>\n2.3623###\n51\n"
  },
  {
    "title": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest",
    "link": "https://arxiv.org/pdf/2307.03601.pdf",
    "upvote": "10",
    "text": "GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nGPT4ROI: INSTRUCTION TUNING LARGE LANGUAGE\nMODEL ON REGION-OF-INTEREST\nShilong Zhang1,2,3\u2217\nPeize Sun1\u2217\nShoufa Chen1\u2217\nMin Xiao2\nWenqi Shao2\nWenwei Zhang2\nYu Liu3\nKai Chen2\nPing Luo1,2\n1The University of Hong Kong\n2Shanghai AI Laboratory\n3Alibaba Group\nABSTRACT\nVisual instruction tuning large language model (LLM) on image-text pairs has\nachieved general-purpose vision-language abilities. However, the lack of region-\ntext pairs limits their advancements to fine-grained multimodal understanding. In\nthis paper, we propose spatial instruction tuning, which introduces the reference\nto the region-of-interest (RoI) in the instruction. Before sending to LLM, the\nreference is replaced by RoI features and interleaved with language embeddings\nas a sequence. Our model GPT4RoI, trained on 7 region-text pair datasets, brings\nan unprecedented interactive and conversational experience compared to previous\nimage-level models. (1) Interaction beyond language: Users can interact with\nour model by both language and drawing bounding boxes to flexibly adjust the\nreferring granularity. (2) Versatile multimodal abilities: A variety of attribute\ninformation within each RoI can be mined by GPT4RoI, e.g., color, shape, material,\naction, etc. Furthermore, it can reason about multiple RoIs based on common\nsense. On the Visual Commonsense Reasoning (VCR) dataset, GPT4RoI achieves\na remarkable accuracy of 81.6%, surpassing all existing models by a significant\nmargin (the second place is 75.6%) and almost reaching human-level performance\nof 85.0%. The code, dataset, and demo can be found at https://github.\ncom/jshilong/GPT4RoI.\n1\nINTRODUCTION\n(a) Visual Instruction Tuning \n(b) Spatial Instruction Tuning\nWhat is <region1> doing\nLarge Language Model\nResponse\nWhat is the woman in the upper \nleft corner of the picture doing\n\u2026\nImage tokens\nResponse\n\u2026\nImage tokens\n\u2026\n\u2026\n<region1>\nLarge Language Model\nFigure 1: Comparison of visual instruction tuning on image-text pairs and spatial instruction tuning\non region-text pairs. The bounding box and text description of each object are provided in region-text\ndatasets. During training, the bounding box is from annotations, and in inference, it can be provided\nby user or any off-the-shelf object detector\nRecent advancements of large language models (LLM) have shown incredible performance in solving\nnatural language processing tasks in a human-like conversational manner, for example, commercial\nproducts (OpenAI, 2022; Anthropic, 2023; Google, 2023; OpenAI, 2023) and community open-\nsource projects (Touvron et al., 2023a;b; Taori et al., 2023; Chiang et al., 2023; Du et al., 2022; Sun\n\u2217Equal contribution.\n1\narXiv:2307.03601v2  [cs.CV]  13 Oct 2023\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nModel\nImage Region Multi-Region Multi-Round End-to-End\nDialogue\nModel\nVisual ChatGPT (Wu et al., 2023)\nMiniGPT-4 (Zhu et al., 2023)\nLLaVA (Liu et al., 2023a)\nInstructBLIP (Dai et al., 2023)\nMM-REACT (Yang et al., 2023b)\nInternGPT (Liu et al., 2023d)\nVisionLLM (Wang et al., 2023b)\nCaptionAnything (Wang et al., 2023a)\nDetGPT (Pi et al., 2023)\nGPT4RoI\nTable 1: Comparisons of vision-language models. Our GPT4RoI is an end-to-end model that supports\nregion-level understanding and multi-round conversation.\n& Xipeng, 2022). Their unprecedented capabilities present a promising path toward general-purpose\nartificial intelligence models. Witnessing the power of LLM, the field of multimodal models (Yang\net al., 2023b; Huang et al., 2023; Girdhar et al., 2023; Driess et al., 2023) is developing a new\ntechnology direction to leverage LLM as the universal interface to build general-purpose models,\nwhere the feature space of a specific task is tuned to be aligned with the feature space of pre-trained\nlanguage models.\nAs one of the representative tasks, vision-and-language models align the vision encoder feature to\nLLM by instruction tuning on image-text pairs, such as MiniGPT-4 (Zhu et al., 2023), LLaVA (Liu\net al., 2023a), InstructBLIP (Dai et al., 2023), etc. Although these works achieve amazing multimodal\nabilities, their alignments are only on image-text pairs (Chen et al., 2015; Sharma et al., 2018;\nChangpinyo et al., 2021; Ordonez et al., 2011; Schuhmann et al., 2021), the lack of region-level\nalignment limits their advancements to more fine-grained understanding tasks such as region cap-\ntion (Krishna et al., 2017) and reasoning (Zellers et al., 2019a). To enable region-level understanding\nin vision-language models, some works attempt to leverage external vision models, for example, MM-\nREACT (Yang et al., 2023b), InternGPT (Liu et al., 2023d) and DetGPT (Pi et al., 2023), as shown\nin Table 1. However, their non-end-to-end architecture is a sub-optimal choice for general-purpose\nmulti-modal models.\nConsidering the limitations of previous works, our objective is to construct an end-to-end vision-\nlanguage model that supports fine-grained understanding on region-of-interest. Since there is no\noperation that can refer to specific regions in current image-level vision-language models (Zhu et al.,\n2023; Liu et al., 2023a; Zhang et al., 2023b; Dai et al., 2023), our key design is to incorporate\nreferences to bounding boxes into language instructions, thereby upgrading them to the format of\nspatial instructions. For example, as shown in Figure 1, when the question is \u201cwhat is <region1>\ndoing?\u201d, where the <region1> refers to a specific region-of-interest, the model will substitute the\nembedding of <region1> with the region feature extracted by the corresponding bounding box. The\nregion feature extractor can be flexibly implemented by RoIAlign (He et al., 2017) or Deformable\nattention (Zhu et al., 2020).\nTo establish fine-grained alignment between vision and language, we involve region-text datasets\nin our training, where the bounding box and the text description of each region are provided. The\ndatasets are consolidated from publicly available ones including COCO object detection (Lin et al.,\n2014), RefCOCO (Yu et al., 2016), RefCOCO+ (Yu et al., 2016), RefCOCOg (Mao et al., 2016),\nFlickr30K entities (Plummer et al., 2015), Visual Genome(VG) (Krishna et al., 2017) and Visual\nCommonsense Reasoning(VCR) (Zellers et al., 2019a). These datasets are transformed into spatial\ninstruction tuning format. Moreover, we incorporate the LLaVA150K dataset (Liu et al., 2023a) into\nour training process by utilizing an off-the-shelf detector to generate bounding boxes. This enhances\nour model\u2019s ability to engage in multi-round conversations and generate more human-like responses.\nThe collected datasets are categorized into two types based on the complexity of the text. First, the\nplain-text data contains object category and simple attribute information. It is used for pre-training\nthe region feature extractor without impacting the LLM. Second, the complex-text data often contains\n2\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\ncomplex concepts or requires common sense reasoning. We conduct end-to-end fine-tuning of the\nregion feature extractor and LLM for these data.\nBenefiting from spatial instruction tuning, our model brings a new interactive experience, where the\nuser can express the question to the model with language and the reference to the region-of-interest.\nThis leads to new capacities beyond image-level understanding, such as region caption and complex\nregion reasoning. As a generalist, our model GPT4RoI also shows its strong region understanding\nability on three popular benchmarks, including the region caption task on Visual Genome (Krishna\net al., 2017), the region reasoning task on Visual-7W (Zhu et al., 2016) and Visual Commonsense\nReasoning (Zellers et al., 2019a) (VCR). Especially noteworthy is the performance on the most\nchallenging VCR dataset, where GPT4RoI achieves an impressive accuracy of 81.6%, 6 points ahead\nof the second-place and nearing the human-level performance benchmarked at 85.0%.\nIn summary, our work makes the following contributions:\n\u2022 We introduce spatial instruction, combining language and the reference to region-of-interest into an\ninterleave sequence, enabling accurate region referring and enhancing user interaction.\n\u2022 By spatial instruction tuning LLM with massive region-text datasets, our model can follow user\ninstructions to solve diverse region understanding tasks, such as region caption and reasoning.\n\u2022 Our method, as a generalist, outperforms the previous state-of-the-art approach on a wide range of\nregion understanding benchmarks.\n2\nRELATED WORK\n2.1\nLARGE LANGUAGE MODEL\nThe field of natural language processing (NLP) has achieved significant development by the high-\ncapability large language model (LLM). The potential of LLM is first demonstrated by pioneering\nworks such as BERT (Devlin et al., 2018) and GPT (Radford et al., 2018). Then scaling up progress is\nstarted and leads to a series of excellent works, for example, T5 (Raffel et al., 2020), GPT-3 (Brown\net al., 2020), Flan-T5 (Chung et al., 2022), PaLM (Chowdhery et al., 2022), etc. With the growth\nof training data and model parameters, this scaling up progress brings to a phenomenal product,\nChatGPT (OpenAI, 2022). By generative pre-trained LLM and instruction tuning (Ouyang et al.,\n2022) on human feedback, ChatGPT shows unprecedented performance on conversations with\nhumans, reasoning and planning tasks (Mu et al., 2023; Yang et al., 2023a; Bubeck et al., 2023), etc.\n2.2\nLARGE VISION-LANGUAGE MODEL\nTo utilize high-performance LLM to build up vision-language models, LLM as task coordinator is\nproposed. Given the user instruction, LLM parses the instruction and calls various external vision\nmodels. Some representative works are Visual ChatGPT (Wu et al., 2023), ViperGPT (Sur\u00eds et al.,\n2023), MM-REACT (Yang et al., 2023b), InternGPT (Liu et al., 2023d), VideoChat (Li et al., 2023),\netc. Although these models largely expand the scope of multimodal models, they depend on external\nvision models and these non-end-to-end architectures are not the optimal choice for multi-modal\nmodels. To obtain end-to-end vision-language models, instruction tuning LLM on image-text pairs\nis proposed to align visual features with LLM and accomplish multimodal tasks in a unified way,\nfor example, Flamingo (Alayrac et al., 2022), MiniGPT-4 (Zhu et al., 2023), LLaVA (Liu et al.,\n2023a), LLaMa-Adapter (Zhang et al., 2023b), InstructBLIP (Dai et al., 2023), MM-GPT (Gong et al.,\n2023), VPGTrans (Zhang et al., 2023a), etc. These models achieve amazing image-level multimodal\nabilities, while several benchmarks such as LVLM-eHub (Xu et al., 2023) and MMBench (Liu et al.,\n2023c) find that these models still have performance bottlenecks when need to be under specific\nregion reference. Our GPT4RoI follows the research line of visual instruction tuning and moves\nforward region-level multimodal understanding tasks such as region caption (Krishna et al., 2017)\nand reasoning (Zellers et al., 2019a).\n2.3\nREGION-LEVEL IMAGE UNDERSTANDING\nFor region-level understanding, it is a common practice in computer vision to identify potential\nregions of interest first and then do the understanding. Object detection (Ren et al., 2015; Carion\n3\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\net al., 2020; Zhu et al., 2020; Zang et al., 2023) tackles the search for potential regions, which are\ngenerally accompanied by a simple classification task to understand the region\u2019s content. To expand\nthe object categories, (Kamath et al., 2021; Liu et al., 2023b; Zhou et al., 2022; Li* et al., 2022)\nlearn from natural language and achieve amazing open-vocabulary object recognition performance.\nRegion captioning (Johnson et al., 2015; Yang et al., 2017; Wu et al., 2022) provides more descriptive\nlanguage descriptions in a generative way. Scene graph generation (Li et al., 2017; Tang et al., 2018;\nYang et al., 2022) analyzes the relationships between regions by the graph. The VCR (Zellers et al.,\n2019b) dataset presents many region-level reasoning cases and (Yu et al., 2021; Su et al., 2019; Li\net al., 2019b; Yao et al., 2022) exhibit decent performance by correctly selecting the answers in the\nmultiple-choice format. However, a general-purpose region understanding model has yet to emerge.\nIn this paper, by harnessing the powerful large language model (Touvron et al., 2023a; Chiang et al.,\n2023), GPT4RoI uses a generative approach to handle all these tasks. Users can complete various\nregion-level understanding tasks by freely asking questions.\n3\nMETHOD: GPT4ROI\nFigure 2: GPT4RoI is an end-to-end vision-language model for processing spatial instructions that\ncontain references to the region-of-interest, such as <region{i}>. During tokenization and conversion\nto embeddings, the embedding of <region{i}> in the instruction is replaced with the RoIAlign results\nfrom multi-level image features. Subsequently, such an interleaved region feature and language\nembedding sequence can be sent to a large language model (LLM) for further processing. We also\nutilize the entire image feature to capture global information and omit it in the figure for brevity. A\nmore detailed framework figure can be found in Figure 5 in the Appendix.\nThe overall framework of GPT4RoI consists of a vision encoder, a projector for image-level features,\na region feature extractor, and a large language model (LLM). Compared to previous works (Zhu\net al., 2023; Liu et al., 2023a), GPT4RoI stands out for its ability to convert instructions that include\nspatial positions into an interleaved sequence of region features and text embeddings, as shown in\nFigure 2.\n3.1\nMODEL ARCHITECTURE\nWe adopt the ViT-H/14 architecture from CLIP (Radford et al., 2021) as the vision encoder. Fol-\nlowing (Liu et al., 2023a), we use the feature map of the penultimate transformer layer as the\nrepresentation of the entire image, and then map the image feature embedding to the language space\nusing a single linear layer as projector. Finally, we employ the Vicuna (Zheng et al., 2023), an\ninstruction-tuned LLaMA (Touvron et al., 2023a), to perform further processing.\nTo extract region-level features with spatial positions, a multi-level image feature pyramid is con-\nstructed by selecting four layers from the CLIP vision encoder. These layers are located at the\nsecond-to-last, fifth-to-last, eighth-to-last, and eleventh-to-last positions, respectively. We then add\nfeature coordinates (Liu et al., 2018) for each level to incorporate absolute position information and\nwe adopt five lightweight scale shuffle modules (Zhang et al., 2023c) to improve multi-level feature\nrepresentation. Finally, we use RoIAlign (He et al., 2017) to extract region-level features with the\noutput size of 14\u00d714, which maintains sufficient detailed information for caption and reasoning.\n4\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nMoreover, all four level features are involved in the RoIAlign operation and fused into a single\nembedding as the representation of the region-of-interest (RoI).\n3.2\nTOKENIZATION AND EMBEDDING\nTo enable users to refer to regions of interest in text inputs, we define a special token <region{i}>,\nwhich acts as the placeholder that will be replaced by the corresponding region feature after tok-\nenization and embedding. One example is depicted in Figure 2. When a user presents a spatial\ninstruction, \u201cWhat was <region1> doing before <region3> touched him?\u201d, the embedding of\n<region1> and <region3> are replaced by their corresponding region features. However, this\nreplacement discards the references to different regions. To allows LLM to maintain the original\nreferences (region1, region3) in the response sequence, the instruction is modified to \u201cWhat was\nregion1 <region1> doing before region3 <region3> touched him?\u201d. Then, LLM can generate a\nreply like \u201cThe person in region1 was eating breakfast before the person in region3 touched them.\u201d\nRegardless of the user instruction, we incorporate a prefix prompt, \u201cThe <image> provides an\noverview of the picture.\u201d The <image> is a special token that acts as a placeholder, the embedding\nof which would be replaced by image features of the vision encoder. These features enable LLM to\nreceive comprehensive image information and obtain a holistic understanding of the visual context.\n3.3\nSPATIAL INSTRUCTION TUNING\nOur model is trained using a next-token prediction loss (Liu et al., 2023a; Zhu et al., 2023), where\nthe model predicts the next token in a given input text sequence.\nThe training details are in Section A.2 in the Appendix.\nWe transform annotations into instruction tuning format by creating a question that refers to the\nmentioned region for each region-text annotation. We partition the available region-text data into\ntwo groups, employing each in two distinct training stages. In the first stage, we attempt to align\nregion features with word embeddings in language models using simple region-text pairs that contain\ncolor, position, or category information. The second stage is designed to handle more complex\nconcepts, such as actions, relationships, and common sense reasoning. Furthermore, we provide\ndiverse instructions for these datasets to simulate chat-like input in this stage.\nStage 1: Pre-training In this stage, we first load the weights of LLaVA (Liu et al., 2023a) after\nits initial stage of training, which includes a pre-trained vision encoder, a projector for image-level\nfeatures, and an LLM. We only keep the region feature extractor trainable and aim to align region\nfeatures with language embedding by collecting short text and bounding box pairs. These pairs are\nfrom both normal detection datasets and referring expression detection datasets, which have short\nexpressions. The objective is to enable the model to recognize categories and simple attributes of the\nregion in an image, which are typically represented by a short text annotation (usually within 5 words).\nSpecifically, we utilize COCO (Lin et al., 2014), RefCOCO (Yu et al., 2016), and RefCOCO+ (Yu\net al., 2016) datasets in this stage.\nAs shown in Table 2, for COCO detection data, we first explain the task in the prompt and then\nconvert the annotations to a single-word region caption task. For RefCOCO and RefCOCO+, we also\ngive task definitions first and train the model to generate descriptions containing basic attributes of\nthe region. Only the description of the region (in red color) will be used to calculate the loss.\nAfter this training stage, GPT4RoI can recognize categories, simple attributes, and positions of\nregions in images, as shown in Figure 3.\nStage 2: End-to-end fine-tuning In this stage, we only keep the vision encoder weights fixed and\ntrain the region feature extractor, image feature projector, and LLM weights. Our main focus is to\nenhance GPT4RoI\u2019s ability to accurately follow user instructions and tackle complex single/multiple\nregion understanding tasks. We tailor specific instructions for different tasks. For single region\ncaption, we construct from Visual Genome (VG) region caption part (Krishna et al., 2017) and\nRefCOCOg (Mao et al., 2016). For multiple region caption, Flicker30k (Plummer et al., 2015) is\nconverted to a multiple region caption task where the caption should include all visual elements\nemphasized by bounding boxes. To simulate user instruction, we create 20 questions for each caption\ntask as shown in Table 8 and Table 9. For the region reasoning task, we modify Visual Commonsense\n5\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nObject Detection\nIn the conversation below, you simply answer the category name based on\nwhat you see in the imagery inside a particular region. I will give you only\none region each time. Categories containing person, bicycle, car ...\n<region1> person\n<region2> dog\nReferring Expression Comprehension\nI will provide you with only one region containing only one object, although\nthere may be other objects present in the image. It is recommended that\nyou describe the object\u2019s relative position with respect to other objects\nin the image and its basic attributes.\n<region1> red shirt girl\n<region2> guy in black\n<region3> right most person blurred\nTable 2: The instruction template for Stage 1 training data: For both tasks, we begin by providing\na description of the task definition and the expected answer. Then, we concatenate all region-text\npairs into a sequence. For detection data, the format is <region{i}> category_name. For referring\nexpression comprehension, the format is <region{i}> description of region. Only the responses\nhighlighted in red are used to calculate the loss.\nFigure 3: After Stage 1 training, GPT4RoI is capable of identifying the category of the region\n(elephant), simple attributes such as color (purple), and the position of the region (left).\nReasoning (VCR) (Zellers et al., 2019a) to meet the input format requirements and make it more\nsimilar to human input. The details of this process can be found in Section A.3.\nTo improve the capability of GPT4RoI for multi-round conversation and generate more human-like\nresponses, we also involve the LLaVA150k (Liu et al., 2023a) visual instruction dataset in this stage.\nWe employ an off-the-shelf LVIS detector (Fang et al., 2023) to extract up to 100 detection boxes per\nimage. These boxes are then concatenated with the user instructions in the format \u201c<region{i}> may\nfeature a class_name\u201d. LLaVA150k significantly improves the capability of GPT4RoI for multi-round\nconversation.\nAfter completing this training stage, GPT4RoI is capable of performing complex region understanding\ntasks based on user instructions, including region caption and reasoning, as demonstrated in Section 4.\n4\nDEMOSTRATIONS\nIn this section, we compare the differences between the visual instruction tuning model LLaVA (Liu\net al., 2023a) and our spatial instruction tuning model GPT4RoI. We demonstrate our new interactive\napproach and highlight its advanced capabilities in understanding multimodality.\nAs shown in Figure 4.A, when we try to make LLaVA focus on the center region of the image, it\nonly sees the boy holding an umbrella and a bag, but it misses the book. As a result, LLaVA gives a\nwrong answer to the question \u201cWhat is the boy doing\u201d (Figure 4.A.\u2460), and this leads to an incorrect\nconclusion that \u201cthe boy\u2019s behavior is not dangerous\u201d (Figure 4.A.\u2461).\n6\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nSingle Region Caption\n### Question: Can you provide me with a detailed description of the region\nin the picture marked by <region1>\n### Answer: A man wearing a light blue t-shirt and jeans with his arms\nextend.\nMultiple Region Caption\n### Question: Can you provide a thorough description of the regions\n<region1>, <region5> in this image?\n### Answer: A man is watching a girl draw as he eats\n### Question: Could you please give me a detailed description of areas\n<region1>, <region2>, <region3>, <region4>, <region5> ?\n### Answer: A man in a white shirt with a plate of food sits outside\nin a folding chair with a little girl who is writing\nSingle Region Reasoning\n### Question: How is <region1> feeling ?\n### Answer: Person at region1 is feeling nervous\n### Question: Why do you think so?\n### Answer: His hands are tried behind his back and he is surrounded by\nnazi guards. He is looking around to escape.\nMulti Region Reasoning\n### Question: Is <region1> happy to be speaking with\n<region2> and <region3> ?\n### Answer: No, person at region1 is bothered by the conversation.\n### Question: What factors influenced your perspective?\n### Answer: Person at region1 is standing with his hand on his\nhip in a defensive way.\nTable 3: Instruction template for Stage 2 training data: During training, we randomly select one\nquestion for both single and multiple region caption tasks. For reasoning tasks, we modify the original\nquestions to include a reference for each region so that GPT4RoI can mention them in its response.\nOnly the response in red color and stop string ### will be used to calculate the loss.\nFigure 4: GPT4RoI and LLaVA dialogue performance showcase. Figures A and C demonstrate the\ndialogue scenarios of LLaVA when referring to a single instance and multiple instances solely using\nnatural language in the conversation. On the other hand, Figures B and D showcase how GPT4RoI\nutilizes bounding boxes as references to address the same scenarios.\n7\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nIn comparison, as shown in Figure 4.B, our approach GPT4RoI efficiently recognizes visual details\nusing the given bounding box. This allows it to accurately identify the action of \u201creading a magazine.\u201d\nFurthermore, GPT4RoI demonstrates its reasoning abilities by correctly inferring that the \u201cboy\u2019s\nbehavior is dangerous\u201d, and giving a reasonable reason that \u201cthe boy is reading a book while crossing\nthe street\u201d.\nWhen there are multiple instances in the image (as depicted in Figure 4.C), we attempt to refer to\nthe corresponding instances as \u201cthe right\u201d and \u201cthe middle\u201d. However, LLaVA provides incorrect\ninformation by stating that the right man is \u201clooking at the women\u201d (as shown in Figure 4.C.\u2462).\nEven more concerning, LLaVA overlooks the actual women in the middle and mistakenly associates\nthe women on the left as the reference, resulting in completely inaccurate information (as shown in\nFigure 4.C.\u2463 & \u2464).\nIn comparison, as shown in Figure 4.D, GPT4RoI is able to understand the user\u2019s requirements,\nsuch as identifying the person to call when ordering food, and accurately recognize that the person\nin region1 fulfills this criterion. Additionally, it correctly recognizes that the person in region3 is\n\u201clooking at the menu\u201d. Importantly, GPT4RoI can also infer relationships between the provided regions\nbased on visual observations. For example, it deduces that the likely relationship between region2\nand region3 is that of a \u201ccouple\u201d, providing a reasonable explanation that they \u201care smiling and\nenjoying each other\u2019s company\u201d.\n5\nQUANTITATIVE RESULTS\nTo quantitatively evaluate GPT4RoI, we have chosen three representative benchmarks to assess\nthe region understanding capabilities. These benchmarks include the region caption task on Visual\nGenome (Krishna et al., 2017), the region reasoning task on Visual-7W (Zhu et al., 2016), and Visual\nCommonsense Reasoning (Zellers et al., 2019a) (VCR). In order to minimize the impact of specific\ndataset label styles and make evaluation metrics easier to calculate, we fine-tuned GPT4RoI on each\nbenchmark using different task prompts. More details can be found in Section A.2 in the Appendix.\n5.1\nREGION CAPTION\nWe report the scores of BLEU, METEOR, ROUGE, and CIDEr for both GPT4RoI-7B and GPT4RoI-\n13B on the validation set of Visual Genome (Krishna et al., 2017). The grounding box in the\nannotation is combined with the task prompt in Appendix Table 7 to get the response.\nModel\nBLEU@4\nMETEOR\nROUGE\nCIDEr\nGRiT (Wu et al., 2022)\n-\n17.1\n-\n142.0\nGPT4RoI-7B\n11.5\n17.4\n35.0\n145.2\nGPT4RoI-13B\n11.7\n17.6\n35.2\n146.8\nTable 4: Compariation of region caption ability on the validation dataset on Visual Genome. All\nmethods employ ground truth bounding boxes and GPT4RoI can outperform previous state-of-the-art\nspecialist GRiT.\nThe generalist approach GPT4RoI outperforms the previous state-of-the-art specialist model\nGRiT (Wu et al., 2022) by a significant margin, without any additional techniques or tricks. Addition-\nally, we observe that the performance of GPT4RoI-7B and GPT4RoI-13B is comparable, suggesting\nthat the bottleneck in performance lies in the design of the visual module and the availability of\nregion-text pair data. These areas can be explored further in future work.\n5.2\nVISUAL-7W\nVisual-7W (Zhu et al., 2016) is a PointQA dataset that contains a which box setting. Here, the model\nis required to choose the appropriate box among four options, based on a given description. For\nexample, a question might ask, \u201cWhich is the black machine under the sign?\u201d. This type of question\nnot only tests the model\u2019s object recognition but also its ability to determine the relationship between\nobjects.\n8\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nTo prevent information leakage, we remove overlapping images with the test set from Visual\nGenome (Krishna et al., 2017). The results clearly demonstrate that the 13B model outperforms the\n7B model by a significant margin. This finding suggests that the reasoning ability heavily relies on\nthe Large Language Model (LLM).\nModel\nLSTM-Att (Zhu et al., 2016)\nCMNs (Hu et al., 2016)\n12in1 (Lu et al., 2020)\nGPT4RoI-7B\nGPT4RoI-13B\nAcc(%)\n56.10\n72.53\n83.35\n81.83\n84.82\nTable 5: Accuracy on Visual-7W test dataset.\n5.3\nVISUAL COMMONSENSE REASONING\nVisual Commonsense Reasoning (VCR) offers a highly demanding scenario that necessitates advanced\nreasoning abilities, heavily relying on common sense. Given the question(Q), the model\u2019s task is not\nonly to select the correct answer(A) but also to select a rationale(R) that explains why the chosen\nanswer is true.\nModel\nVal Acc.(%)\nTest Acc.(%)\nQ \u2192 A\nQA \u2192 R\nQ \u2192 AR\nQ \u2192 A\nQA \u2192 R\nQ \u2192 AR\nViLBERT (Lu et al., 2019)\n72.4\n74.5\n54.0\n73.3\n74.6\n54.8\nUnicoder-VL (Li et al., 2019a)\n72.6\n74.5\n54.5\n73.4\n74.4\n54.9\nVLBERT-L (Su et al., 2019)\n75.5\n77.9\n58.9\n75.8\n78.4\n59.7\nUNITER-L (Chen et al., 2020)\n-\n-\n-\n77.3\n80.8\n62.8\nERNIE-ViL-L (Yu et al., 2021)\n78.52\n83.37\n65.81\n79.2\n83.5\n66.3\nMERLOT (Zellers et al., 2021)\n-\n-\n-\n80.6\n80.4\n65.1\nVILLA-L (Gan et al., 2020)\n78.45\n82.57\n65.18\n78.9\n82.8\n65.7\nRESERVE-L (Zellers et al., 2021)\n-\n-\n-\n84.0\n84.9\n72.0\nVQA-GNN-L (Wang et al., 2022)\n-\n-\n-\n85.2\n86.6\n74.0\nGPT4RoI-7B\n87.4\n89.6\n78.6\n-\n-\n-\nVLUA+@Kuaishou\n-\n-\n-\n84.8\n87.0\n74.0\nKS-MGSR@KDDI Research and SNAP\n-\n-\n-\n85.3\n86.9\n74.3\nSP-VCR@Shopee\n-\n-\n-\n83.6\n88.6\n74.4\nHunYuan-VCR@Tencent\n-\n-\n-\n85.8\n88.0\n75.6\nHuman Performance (Zellers et al., 2019a)\n-\n-\n-\n91.0\n93.0\n85.0\nGPT4RoI-13B\n-\n-\n-\n89.4\n91.0\n81.6\nTable 6: Accuracy scores on VCR. GPT4RoI achieves state-of-the-art accuracy among all methods.\nGPT4RoI shows significant improvements over the previous methods across all Q \u2192 A, QA \u2192 R,\nand Q \u2192 AR tasks. Notably, in the crucial Q \u2192 AR task, GPT4RoI-13B achieves a performance of\n81.6 accuracy, surpassing preceding methods by over 6 points, even outperforming non-open source\ncompany-level results, which may take advantage of private data. More importantly, this performance\nis almost reaching human-level performance of 85.0 accuracy, which shows that the multimodal\nability of GPT4RoI is promising to be further developed to human intelligence.\n6\nCONCLUSIONS\nIn this paper, we present GPT4RoI, an end-to-end vision-language model that can execute user\ninstructions to achieve region-level image understanding. Our approach employs spatial instruction\ntuning for the large language model (LLM), where we convert the reference to bounding boxes from\nuser instructions into region features. These region features, along with language embeddings, are\ncombined to create an input sequence for the large language model. By utilizing existing open-source\nregion-text pair datasets, we show that GPT4RoI enhances user interaction by accurately referring to\nregions and achieves impressive performance in region-level image understanding tasks.\n9\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nREFERENCES\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022. 3\nAnthropic.\nClaude.\nhttps://www.anthropic.com/index/introducing-claude,\n2023. 1\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020. 3\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence:\nEarly experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. 3\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In European Conference on Computer\nVision, pp. 213\u2013229. Springer, 2020. 3\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3558\u20133568, 2021. 2\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Doll\u00e1r, and\nC Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint\narXiv:1504.00325, 2015. 2\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. Uniter: Universal image-text representation learning, 2020. 9\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/. 1, 4\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. 3\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416, 2022. 3\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023. 2, 3\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan\nWahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal\nlanguage model. arXiv preprint arXiv:2303.03378, 2023. 2\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm:\nGeneral language model pretraining with autoregressive blank infilling. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n320\u2013335, 2022. 1\nYuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A\nvisual representation for neon genesis. arXiv preprint arXiv:2303.11331, 2023. 6\n10\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nZhe Gan, Yen-Chun Chen, Linjie Li, Chen Zhu, Yu Cheng, and Jingjing Liu. Large-scale adversarial\ntraining for vision-and-language representation learning, 2020. 9\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 15180\u201315190, 2023. 2\nTao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao, Kuikun Liu,\nWenwei Zhang, Ping Luo, and Kai Chen. Multimodal-gpt: A vision and language model for\ndialogue with humans, 2023. 3\nGoogle. Bard. https://bard.google.com/, 2023. 1\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the\nIEEE international conference on computer vision, pp. 2961\u20132969, 2017. 2, 4\nRonghang Hu, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. Modeling\nrelationships in referential expressions with compositional modular networks, 2016. 9\nShaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao\nLv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al. Language is not all you need: Aligning\nperception with language models. arXiv preprint arXiv:2302.14045, 2023. 2\nJustin Johnson, Andrej Karpathy, and Li Fei-Fei. Densecap: Fully convolutional localization networks\nfor dense captioning, 2015. 4\nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\nMdetr \u2013 modulated detection for end-to-end multi-modal understanding, 2021. 4\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language\nand vision using crowdsourced dense image annotations. International journal of computer vision,\n123:32\u201373, 2017. 2, 3, 5, 8, 9, 15\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, Daxin Jiang, and Ming Zhou. Unicoder-vl: A universal\nencoder for vision and language by cross-modal pre-training, 2019a. 9\nKunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and\nYu Qiao. Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355, 2023. 3\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple\nand performant baseline for vision and language, 2019b. 4\nLiunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*, Jianwei Yang, Chunyuan Li, Yiwu\nZhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and Jianfeng Gao.\nGrounded language-image pre-training. In CVPR, 2022. 4\nYikang Li, Wanli Ouyang, Bolei Zhou, Kun Wang, and Xiaogang Wang. Scene graph generation\nfrom objects, phrases and region captions, 2017. 4\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740\u2013755. Springer, 2014. 2, 5\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023a. 2, 3, 4, 5, 6\nRosanne Liu, Joel Lehman, Piero Molino, Felipe Petroski Such, Eric Frank, Alex Sergeev, and Jason\nYosinski. An intriguing failing of convolutional neural networks and the coordconv solution, 2018.\n4\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang,\nHang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection, 2023b. 4\n11\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player?\narXiv preprint arXiv:2307.06281, 2023c. 3\nZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang,\nYang Yang, Qingyun Li, Jiashuo Yu, et al. Internchat: Solving vision-centric tasks by interacting\nwith chatbots beyond language. arXiv preprint arXiv:2305.05662, 2023d. 2, 3\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks, 2019. 9\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, Devi Parikh, and Stefan Lee. 12-in-1: Multi-task\nvision and language representation learning, 2020. 9\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy.\nGeneration and comprehension of unambiguous object descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 11\u201320, 2016. 2, 5\nYao Mu, Qinglong Zhang, Mengkang Hu, Wenhai Wang, Mingyu Ding, Jun Jin, Bin Wang, Jifeng\nDai, Yu Qiao, and Ping Luo. Embodiedgpt: Vision-language pre-training via embodied chain of\nthought. arXiv preprint arXiv:2305.15021, 2023. 3\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022. 1, 3\nOpenAI. Gpt-4 technical report, 2023. 1\nVicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural information processing systems, 24, 2011. 2\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback. Advances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022. 3\nRenjie Pi, Jiahui Gao, Shizhe Diao, Rui Pan, Hanze Dong, Jipeng Zhang, Lewei Yao, Jianhua Han,\nHang Xu, and Lingpeng Kong Tong Zhang. Detgpt: Detect what you need via reasoning. arXiv\npreprint arXiv:2305.14167, 2023. 2\nBryan A Plummer, Liwei Wang, Chris M Cervantes, Juan C Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models. In Proceedings of the IEEE international conference on computer\nvision, pp. 2641\u20132649, 2015. 2, 5\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language\nunderstanding by generative pre-training. OpenAI, 2018. 3\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021. 4\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\ntransformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020. 3\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object\ndetection with region proposal networks. Advances in neural information processing systems, 28:\n91\u201399, 2015. 3\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. 2\n12\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556\u20132565, 2018. 2\nWeijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training\nof generic visual-linguistic representations. arXiv preprint arXiv:1908.08530, 2019. 4, 9\nTianxiang Sun and Qiu Xipeng. Moss. https://github.com/OpenLMLab/MOSS, 2022. 1\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023. 3\nKaihua Tang, Hanwang Zhang, Baoyuan Wu, Wenhan Luo, and Wei Liu. Learning to compose\ndynamic tree structures for visual contexts, 2018. 4\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023. 1\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e\nLacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. 1, 4\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. 1\nTeng Wang, Jinrui Zhang, Junjie Fei, Yixiao Ge, Hao Zheng, Yunlong Tang, Zhe Li, Mingqi Gao,\nShanshan Zhao, Ying Shan, et al. Caption anything: Interactive image description with diverse\nmultimodal controls. arXiv preprint arXiv:2305.02677, 2023a. 2\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong\nLu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for\nvision-centric tasks. arXiv preprint arXiv:2305.11175, 2023b. 2\nYanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec. Vqa-gnn:\nReasoning with multimodal semantic graph for visual question answering, 2022. 9\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-\nsual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint\narXiv:2303.04671, 2023. 2, 3\nJialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan\nWang. Grit: A generative region-to-text transformer for object understanding, 2022. 4, 8\nPeng Xu, Wenqi Shao, Kaipeng Zhang, Peng Gao, Shuo Liu, Meng Lei, Fanqing Meng, Siyuan\nHuang, Yu Qiao, and Ping Luo. Lvlm-ehub: A comprehensive evaluation benchmark for large\nvision-language models, 2023. 3\nJiange Yang, Wenhui Tan, Chuhao Jin, Bei Liu, Jianlong Fu, Ruihua Song, and Limin Wang. Pave\nthe way to grasp anything: Transferring foundation models for universal pick-place robots. arXiv\npreprint arXiv:2306.05716, 2023a. 3\nJingkang Yang, Yi Zhe Ang, Zujin Guo, Kaiyang Zhou, Wayne Zhang, and Ziwei Liu. Panoptic\nscene graph generation, 2022. 4\nLinjie Yang, Kevin Tang, Jianchao Yang, and Li-Jia Li. Dense captioning with joint inference and\nvisual context. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Jul\n2017. 4\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint arXiv:2303.11381, 2023b. 2, 3\n13\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nYuan Yao, Qianyu Chen, Ao Zhang, Wei Ji, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Pevl:\nPosition-enhanced pre-training and prompt tuning for vision-language models. arXiv preprint\narXiv:2205.11169, 2022. 4\nFei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil:\nKnowledge enhanced vision-language representations through scene graph, 2021. 4, 9\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context\nin referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part II 14, pp. 69\u201385. Springer, 2016. 2, 5\nYuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and Chen Change Loy. Contextual object detection\nwith multimodal large language models, 2023. 4\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual\ncommonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pp. 6720\u20136731, 2019a. 2, 3, 6, 8, 9, 15\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual\ncommonsense reasoning. In The IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), June 2019b. 4, 16\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin\nChoi. Merlot: Multimodal neural script knowledge models. In Advances in Neural Information\nProcessing Systems 34, 2021. 9\nAo Zhang, Hao Fei, Yuan Yao, Wei Ji, Li Li, Zhiyuan Liu, and Tat-Seng Chua. Transfer visual\nprompt generator across llms. arXiv preprint arXiv:2305.01278, 2023a. 3\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, Peng Gao,\nand Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init attention.\narXiv preprint arXiv:2303.16199, 2023b. 2, 3\nShilong Zhang, Xinjiang Wang, Jiaqi Wang, Jiangmiao Pang, Chengqi Lyu, Wenwei Zhang, Ping\nLuo, and Kai Chen. Dense distinct query for end-to-end object detection. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 7329\u20137338, June\n2023c. 4\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023. 4\nXingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Kr\u00e4henb\u00fchl, and Ishan Misra. Detecting\ntwenty-thousand classes using image-level supervision. In ECCV, 2022. 4\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023. 2, 3, 4, 5\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n2, 4\nYuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei. Visual7w: Grounded question answering\nin images, 2016. 3, 8, 9, 15\n14\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nA\nAPPENDIX\nIn this appendix, we provide a detailed method architecture figure. We then discuss training-related\ndetails, including hyperparameters and instruction templates used in each stage and task. Specifically,\nwe describe how we utilize the VCR dataset. Finally, we analyze some error cases and propose\npotential improvements for future exploration.\nA.1\nDETAILED ARCHITECTURE\nFigure 5: A more detailed framework of GPT4RoI.\nHere is a more detailed framework of our approach, GPT4RoI.\n1. We preprocess the input text by adding prefixes to retain both image information and pure text\nreferences for each region.\n2. Next, we tokenize and embed the text. The image feature and region features will replace the\nplaceholders <image> and <region{i}> respectively.\n3. The resulting interleaved sequence of region & image features and language embeddings is then\nfed into a large language model (LLM) for further processing.\nA.2\nTRAINING DETAILS\nDialogue model The dialogue model in the demo is trained on 8 GPUs, each with 80G of memory.\nDuring the first training stage, a learning rate of 2e-5 is used with a cosine learning schedule. The\nbatch size is 16 for 2 epochs, with a warm-up iteration set to 3000 and a warm-up ratio of 0.003.\nThe weight decay for all modules was set to 0. During the second training stage, the learning rate\nis reduced to 2e-5 and the model is trained for 1 epoch. To enable end-to-end fine-tuning of the\nmodel, which includes a 7B Vicuna, Fully Sharded Data Parallel (FSDP) is enabled in PyTorch to\nsave memory.\nDownstream tasks We finetune on three datasets with different learning schedules and task prompts\n(as shown in Table 7). For the region caption task on Visual Genome (Krishna et al., 2017), we\nperform fine-tuning for 4 epochs with a learning rate of 2e-5. As for Visual-7W (Zhu et al., 2016), we\nobserve that it requires a smaller learning rate of 1e-6 to stabilize the training, which is also trained in\n2 epochs. On the Visual Commonsense Reasoning (Zellers et al., 2019a), we fine-tune the model for\n1 epoch using a learning rate of 2e-5.\nInstruction of three downstream tasks. The instructions for three downstream tasks are provided in\nTable 7.\nInstruction of Single-Region Caption The instructions for single-region caption are provided in\nTable 8. We randomly select one as the question in training.\n15\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nRegion Caption Task on Visual Genome\n### Question: Can you give a description of the region mentioned by <region>\n### Answer: A man wearing a light blue t-shirt and jeans with his arms extended\nRegion Reasoning Task on Visual-7W\n### Question: <region1>,<region2>,<region3>,<region4> refers to specific areas within the photo\nalong with their respective identifiers. I need you to answer the question. Questions are multiple-\nchoice; you only need to pick the correct answer from the given options (A), (B), (C), or (D).\nWhich is the black machine under the sign?\n### Answer: (A)\nRegion Reasoning Task on VCR\nQ \u2192 A\n### Question: <region1>,<region2>,<region3>... refers to specific areas within the photo along with\ntheir respective identifiers. I need you to answer the question. Questions are multiple-choice; you\nonly need to pick the correct answer from the given options (A), (B), (C), or (D).\nHow is 1 feeling ?\n(A),1 is feeling amused .\n(B),1 is upset and disgusted .\n(C),1 is feeling very scared .\n(D),1 is feeling uncomfortable with 3\n### Answer: (C)\nQA \u2192 R\n### Question: <region1>,<region2>,<region3>... refers to specific areas within the photo along with\ntheir respective identifiers. I give you a question and its answer, I need you to provide a rationale\nexplaining why the answer is right. Both questions are multiple-choice; you only need to pick the\ncorrect answer from the given options (A), (B), (C), or (D).\n\"How is 1 feeling ?\" The answer is \"1 is feeling very scared.\" What\u2019s the rationale for this decision?\n(A),1\u2019s face has wide eyes and an open mouth .\n(B),When people have their mouth back like that and their eyebrows lowered they are usually\ndisgusted by what they see .\n(C),3,2,1 are seated at a dining table where food would be served to them . people unaccustomed to\nodd or foreign dishes may make disgusted looks at the thought of eating it .\n(D),1\u2019s expression is twisted in disgust .\n### Answer: (A)\nTable 7: Task prompt of three downstream tasks.\nInstruction of Multi-Region Caption The instructions for multi-region caption are provided in\nTable 9. We randomly select one as the question in training.\nA.3\nPREPROCESS OF VCR\nThe Visual Commonsense Reasoning(VCR) dataset (Zellers et al., 2019b), comprises 290,000\nmultiple-choice questions obtained from 110,000 movie scenes. Each image in the dataset is\nannotated with a question that requires common-sense reasoning, along with its corresponding answer\nand the explanation for the answer. To construct a sequence of questions, we convert the explanation\nto a follow-up question and format them into a two-round conversation. Table 10 shows an example\nof the follow-up question that asks for the reasoning behind the answer.\nThe VCR dataset is valued for its diverse question-answer pairs that require referencing from prior\nquestion-answers to perform reasoning. Therefore, it\u2019s crucial to assign a reference to each region in\nthe dataset. We accomplish this by starting each conversation with a reference to all regions, e.g.,\nThere are <region1>, <region2>... in the image. This approach explicitly references every region,\n16\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\navoiding confusion in future analyses. Additionally, we substitute the corresponding <region{i}> in\nthe answer with category_name at region{i} to ensure a plain text output sequence.\nA.4\nFAILURE CASE ANALYSIS\nDue to limited data and instructions, GPT4RoI may fail in several landmark scenarios. We have\nconducted a thorough analysis and look forward to improving these limitations in future versions.\nInstruction obfuscation As shown in Figure 6.(a), our multiple-region reasoning capability mainly\nrelies on VCR, where we often use sentences that declare <region1>, <region2>, etc. at the beginning\nof the question. However, when users adopt the less common sentence structure to refer to regions, it\ncan often be confused with region captions that have the highest proportion in the dataset. As shown\nin Figure 6.(b), because our data and instructions are mainly generated by rules, our training data\ndoes not include content with the \"respectively\" instruction in multi-region scenarios. This can\nbe resolved by adding specific instructions. In future versions, we aim to develop more diverse\ninstructions, while ensuring data balance.\nFigure 6: GPT4RoI on instruction obfuscation.\nMisidentification of fine-grained information within in region Although GPT4RoI has improved\nthe fine-grained perception ability of images compared to image-level vision language models, the\nlimited amount of region-level data results in insufficient fine-grained alignment within regions. For\nexample, in Figure 7.(a), the model incorrectly identifies the color of the helmet, and in Figure 7.(b),\nit misidentifies the object in the girl\u2019s hand. Both cases generate the corresponding answers based\non the most prominent feature within the region. Using semi-supervised methods to create more\nregion-level data may address this issue.\nFigure 7: GPT4RoI on Misidentification of fine-grained information.\n17\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\nA.5\nDISCUSSION\nIn our exploration, we find GPT4RoI produces failure cases as shown in Section. A.4. To further\nimprove the performance, we identify the following potential directions:\n\u2022 Model architecture. We find that 224 \u00d7224 input image resolution struggles with understanding\nsmaller regions. However, if we switch to a larger resolution, we must consider the potential\nburden on inference speed from global attention ViT architecture, while the more efficient CNN\narchitecture or sliding window attention has no available pre-trained large-scale vision encoder like\nCLIP ViT-H/14.\n\u2022 More region-text pair data. The amount of available region-text pairs is notably smaller than\nthat of image-text pairs, which makes it challenging to sufficiently align region-level features\nwith language models. To tackle this issue, we may try to generate region-level pseudo labels by\nleveraging off-the-shelf detectors to generate bounding boxes for image-text data.\n\u2022 Region-level instructions. Although we have generated instructions for each task from existing\nopen-source datasets, users in practical applications may ask various questions about an arbitrary\nnumber of regions, and the existing data may not contain satisfactory answers. To tackle this issue,\nwe suggest generating a new batch of spatial instructions through manual labeling or by leveraging\nChatGPT or GPT4.\n\u2022 Interaction mode. Currently, GPT4RoI only supports natural language and bounding box interaction.\nIncorporating more open-ended interaction modes such as point, scribble, or image-based search\ncould further improve the user interaction experience.\n1. Can you provide me with a detailed description of the region in the picture marked by <region1>?\n2. I\u2019m curious about the region represented by <region1> in the picture. Could you describe it in\ndetail?\n3. What can you tell me about the region indicated by <region1> in the image?\n4. I\u2019d like to know more about the area in the photo labeled <region1>. Can you give me a detailed\ndescription?\n5. Could you describe the region shown as <region1> in the picture in great detail?\n6. What details can you give me about the region outlined by <region1> in the photo?\n7. Please provide me with a comprehensive description of the region marked with <region1> in the\nimage.\n8. Can you give me a detailed account of the region labeled as <region1> in the picture?\n9. I\u2019m interested in learning more about the region represented by <region1> in the photo. Can you\ndescribe it in detail?\n10. What is the region outlined by <region1> in the picture like? Could you give me a detailed\ndescription, please?\n11. Can you provide me with a detailed description of the region in the picture marked by <region1>,\nplease?\n12. I\u2019m curious about the region represented by <region1> in the picture. Could you describe it in\ndetail, please?\n13. What can you tell me about the region indicated by <region1> in the image, exactly?\n14. I\u2019d like to know more about the area in the photo labeled <region1>, please. Can you give me a\ndetailed description?\n15. Could you describe the region shown as <region1> in the picture in great detail, please?\n16. What details can you give me about the region outlined by <region1> in the photo, please?\n17. Please provide me with a comprehensive description of the region marked with <region1> in the\nimage, please.\n18. Can you give me a detailed account of the region labeled as <region1> in the picture, please?\n19. I\u2019m interested in learning more about the region represented by <region1> in the photo. Can you\ndescribe it in detail, please?\n20. What is the region outlined by <region1> in the picture like, please? Could you give me a\ndetailed description?\nTable 8: A list of instructions for single-region caption.\n18\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\n1. Could you please give me a detailed description of these areas [<region1>, <region2>, ...]?\n2. Can you provide a thorough description of the regions [<region1>, <region2>, ...] in this image?\n3. Please describe in detail the contents of the boxed areas [<region1>, <region2>, ...].\n4. Could you give a comprehensive explanation of what can be found within [<region1>, <region2>,\n...] in the picture?\n5. Could you give me an elaborate explanation of the [<region1>, <region2>, ...] regions in this\npicture?\n6. Can you provide a comprehensive description of the areas identified by [<region1>, <region2>,\n...] in this photo?\n7. Help me understand the specific locations labeled [<region1>, <region2>, ...] in this picture in\ndetail, please.\n8. What is the detailed information about the areas marked by [<region1>, <region2>, ...] in this\nimage?\n9. Could you provide me with a detailed analysis of the regions designated [<region1>, <region2>,\n...] in this photo?\n10. What are the specific features of the areas marked [<region1>, <region2>, ...] in this picture that\nyou can describe in detail?\n11. Could you elaborate on the regions identified by [<region1>, <region2>, ...] in this image?\n12. What can you tell me about the areas labeled [<region1>, <region2>, ...] in this picture?\n13. Can you provide a thorough analysis of the specific locations designated [<region1>, <region2>,\n...] in this photo?\n14. I am interested in learning more about the regions marked [<region1>, <region2>, ...] in this\nimage. Can you provide me with more information?\n15. Could you please provide a detailed description of the areas identified by [<region1>, <region2>,\n...] in this photo?\n16. What is the significance of the regions labeled [<region1>, <region2>, ...] in this picture?\n17. I would like to know more about the specific locations designated [<region1>, <region2>, ...] in\nthis image. Can you provide me with more information?\n18. Can you provide a detailed breakdown of the regions marked [<region1>, <region2>, ...] in this\nphoto?\n19. What specific features can you tell me about the areas identified by [<region1>, <region2>, ...]\nin this picture?\n20. Could you please provide a comprehensive explanation of the locations labeled [<region1>,\n<region2>, ...] in this image?\nTable 9: A list of instructions for multiple-region caption.\n19\nGPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest\n1. Why?\n2. What\u2019s the rationale for your decision\n3. What led you to that conclusion?\n4. What\u2019s the reasoning behind your opinion?\n5. Can you explain the basis for your thinking?\n6. What factors influenced your perspective?\n7. How did you arrive at that perspective?\n8. What evidence supports your viewpoint?\n9. What\u2019s the logic behind your argument?\n10. Can you provide some context for your opinion?\n11. What\u2019s the basis for your assertion?\n12. What experiences have shaped your perspective?\n13. What assumptions underlie your reasoning?\n14. What\u2019s the foundation of your assertion?\n15. What\u2019s the source of your reasoning?\n16. What\u2019s the motivation behind your decision?\n17. What\u2019s the impetus for your belief?\n18. What\u2019s the driving force behind your conclusion?\n19. What\u2019s your reasoning?\n20. What makes you say that?\n21. What\u2019s the story behind that?\n22. What\u2019s your thought process?\n23. What\u2019s the deal with that?\n24. What\u2019s the logic behind it?\n25. What\u2019s the real deal here?\n26. What\u2019s the reason behind it?\n27. What\u2019s the rationale for your opinion?\n28. What\u2019s the background to that?\n29. What\u2019s the evidence that supports your view?\n30. What\u2019s the explanation for that?\nTable 10: A list of instructions for the second round chat in VCR.\n20\n"
  },
  {
    "title": "BiPhone: Modeling Inter Language Phonetic Influences in Text",
    "link": "https://arxiv.org/pdf/2307.03322.pdf",
    "upvote": "7",
    "text": "Bi-Phone: Modeling Inter Language Phonetic Influences in Text\nAbhirut Gupta1, Ananya B. Sai2, Richard Sproat1, Yuri Vasilevski1,\nJames S. Ren1, Ambarish Jash1, Sukhdeep S. Sodhi1 and Aravindan Raghuveer1\n1Google Research\n2IIT Madras\nCorresponding author: abhirut@google.com\nAbstract\nA large number of people are forced to use the\nWeb in a language they have low literacy in\ndue to technology asymmetries. Written text\nin the second language (L2) from such users\noften contains a large number of errors that are\ninfluenced by their native language (L1). We\npropose a method to mine phoneme confusions\n(sounds in L2 that an L1 speaker is likely to con-\nflate) for pairs of L1 and L2. These confusions\nare then plugged into a generative model (Bi-\nPhone) for synthetically producing corrupted\nL2 text. Through human evaluations, we show\nthat Bi-Phone generates plausible corruptions\nthat differ across L1s and also have widespread\ncoverage on the Web. We also corrupt the pop-\nular language understanding benchmark Super-\nGLUE with our technique (FunGLUE for Pho-\nnetically Noised GLUE) and show that SoTA\nlanguage understating models perform poorly.\nWe also introduce a new phoneme prediction\npre-training task which helps byte models to\nrecover performance close to SuperGLUE. Fi-\nnally, we also release the FunGLUE benchmark\nto promote further research in phonetically ro-\nbust language models. To the best of our knowl-\nedge, FunGLUE is the first benchmark to intro-\nduce L1-L2 interactions in text.\n1\nIntroduction\nWe live in a multilingual world with over 7,000\nlanguages spoken across the globe (Eberhard and\nFennig, 2022). However, technology asymmetri-\ncally supports only a few specific languages. For\ninstance, the internet is mostly in English with\nover 60% of websites using the language despite\njust around 16% share of its speaking population\naround the world1 (Grefenstette and Nioche, 2000).\nIncreasingly, people are forced to navigate and pro-\nduce content on the web in languages they have\nnot been formally trained on. The English text pro-\nduced by ESL (English as Second / L2 language)\n1https://w3techs.com/technologies/overview/content_language\nwriters is heavily influenced by their native lan-\nguage (L1).\nResearch in the field of second-language acqui-\nsition has found evidence of phoneme-shift based\nmisspellings stemming from L1 influence in L2 text\nfor specific language pairs (Ibrahim, 1978; Cook,\n1997; Bestgen and Granger, 2011; Sari, 2014;\nOgneva, 2018; Motohashi-Saigo and Ishizawa,\n2020). Studies in Natural Language Understand-\ning (NLU) have been limited to spelling correc-\ntion Nagata et al. (2017); Flor et al. (2019) and\nnative language identification Chen et al. (2017);\nNicolai et al. (2013) in English learners. These\nstudies predominantly use the TOEFL11 dataset\n(Blanchard et al., 2013) which deals with very spe-\ncific demographics such as test-takers who have\nformal training in the L2 language.\nWe make the following four key observations\nabout prior work in the study of L1-L2 influences\nin text and speech. First, current models for L1-L2\ninfluence on textual spelling are limited to certain\nlanguage pairs and tasks. We argue that L1-L2 in-\nfluence phenomenon is much more broad and is\nlanguage and task agnostic. Second, there is no\nlarge scale study to examine the prevalence of this\nphenomenon on the open web. Third, given that\nthis is an important problem especially for multi-\nlingual, new-to-the-internet communities there is\nno standardized benchmark to study the robust-\nness of natural language understanding(NLU) and\nNatural Language Generation (NLG) models to\ninter-language phonetic noise. Finally, there is very\nsparse literature on architecture / pre-training strate-\ngies to introduce phonetic robustness into large lan-\nguage models. In this paper, we present modeling\ntechniques,data analyses and a new benchmark to\naddress the gaps mentioned above. We summarise\nour contributions as follows:\n1. We propose a language-agnostic method to\nmine phoneme confusions that arise due to\ninterference between a native language (L1)\narXiv:2307.03322v1  [cs.CL]  6 Jul 2023\nand second language (L2). Our method ex-\nploits the \u201chidden knowledge\" contained in\nL1 \u2192 L2 and L2 \u2192 L1 transliteration mod-\nels. We also propose a generative model Bi-\nPhone that is able to synthetically produce\nspelling corruption in accordance with L1-L2\nconfusions (Sections 3.1, 3.2).\n2. Through human evaluation and coverage anal-\nysis we show that Bi-Phone produces spelling\ncorruptions that are not only deemed plausible\nby native L1 speakers but also have substantial\ncoverage in the open web crawl corpus. To\nthe best of our knowledge no prior work has\ndemonstrated the presence of L1-L2 phonetic\ncorruptions in a large scale, common dataset\nlike Common Crawl (Section 4).\n3. We release a dataset consisting of sentences\nwith L1-L2 phonetic spelling corruptions\nfound in Common Crawl. We also release\na benchmark called FunGLUE, an extension\nof the SuperGLUE benchmark for L1-L2\nspelling corruptions. To the best of our knowl-\nedge FunGLUE is the first benchmark to mea-\nsure the robustness of models to L1-L2 inter-\nference in text (Section 5).\n4. We show SoTA models do not perform well\non FunGLUE. We then introduce a novel pre-\ntraining task of phoneme prediction, which\ntogether with byte level architectures substan-\ntially bridges the gap on the noised benchmark\n(by up to 11% absolute on certain test sets).\nThis is particularly impressive since this gain\nis achieved without ever showing the model\nany noised examples (Section 6).\n2\nRelated Work\nWe divide the presentation of related work in two\nsections. (i) First, we discuss prior work spanning\nmultiple research areas regarding phonetic influ-\nences in text and how it relates to our work. (ii)\nSecond, we discuss work in the speech domain\nwhich studies phonetic variations occurring due to\ninter-language interference in multi-lingual scenar-\nios.\n2.1\nPhonetic Influences in Text\nPhonetic influence on spelling errors has been\nstudied in the past (Kukich, 1992; Toutanova and\nMoore, 2002; Hl\u00e1dek et al., 2020). The source\nof such errors is that both native and non-native\nspeakers resort to phonetic spellings for unfamiliar\nwords or names. This direction of work does not\naddress the effect of native language (L1) based\nphoneme shifts on second-language (L2) spellings.\nThere has also been work that focuses on learner\nEnglish 2 for different applications. Nagata et al.\n(2017); Flor et al. (2019) study automatic spell cor-\nrection with distributional methods that require a\nlarger learner corpus. Chen et al. (2017); Nicolai\net al. (2013) explore Native Language Identifica-\ntion (NLI) on such text. A widely used dataset for\nthese learner English tasks is the TOEFL11 corpus\n(Blanchard et al., 2013) which contains English\nessays written by non-native test-takers. It is im-\nportant to note that these analysis are limited to\nmisspellings made by authors with sufficient L2\nknowledge/ training that qualifies them to take the\ntest. They also do not explicitly study the causes of\nthe misspellings or the inter-language interference.\nThere has also been a fair amount of interest in\nthe second-language acquisition field on the influ-\nence of L1 on L2 spelling. Ibrahim (1978); Cook\n(1997); Bestgen and Granger (2011); Sari (2014);\nOgneva (2018); Motohashi-Saigo and Ishizawa\n(2020) all find evidence of such influence in spe-\ncific language pairs. These often stem from the\nlack of certain sounds in L1 leading to difficulty in\ndistinguishing similar sounds in L2. They also find\nmore interesting phenomenon like L1 constraints\non consonant clusters are reflected in L2 spellings\nby learners. While this direction of research is\nhighly pertinent to our work, our goal is to gen-\nerate plausible L1-L2 phonetic shift based mis-\nspellings more generally instead of studying the\nphenomenon in particular language pairs.\n2.2\nInter-language Influence for Phonetic\nDeviations in Speech\nPhonetic variations of words have been well-\nstudied in the context of speech applications. Sev-\neral studies (Radzikowski et al., 2019; Shah et al.,\n2020; Radzikowski et al., 2021; Bird et al., 2019)\ndiscuss the drop in performance of speech appli-\ncations such as ASR, spoken-term detection, etc.,\nwhen presented with non-native speech data. They\nattribute this drop mainly to the nuances in pronun-\nciation that are often not present in the training data,\ndue to the lack of sufficient non-native speech data.\nTo address and close this gap, several strategies\n2learner English refers to English as a foreign language\nFigure 1: Overview of the Round Trip Transliteration\nmethod for creating word pairs from which phoneme\nconfusions are mined.\nIn this example, we create\npairs for the dictionary word \u201camazon\" with round-\ntrip transliteration through Hindi as the pivot language.\nPhoneme sequences for the original and round-trip\ntransliterated words are also shown. Multiple words\nwith JH in the round-trip transliterations enables us to\nmap the Z sound to the JH sound for Hindi speakers.\nFigure 2: Examples of round trip transliterations of\ndictionary words with different pivot languages, the\ncorresponding phoneme sequences, and the phoneme\nconfusion mined. While the third example also has a Z\n-> S shift, it is not mined because we only consider the\ntop-10 most frequent confusions per (L1, L2) pair.\nranging from the use of cross-lingual/multi-lingual\nphonological inventories to end-to-end training\nhave been applied. However, these studies do not\nfocus on how the same phonetic influences mani-\nfest in written text.\n3\nMethod\nIn this section we introduce our method for creating\ninter-language influenced phonetic misspellings (or\ncorruptions).\nWe present the technique in two\nparts. Section 3.1 presents a method for mining\nnative-language influenced phonetic confusions.\nSection 3.2 contains details of Bi-Phone, our model\nthat uses mined phonetic confusions to create mis-\nspellings.\n3.1\nMining Phoneme-Phoneme Confusions\nThe first problem is to identify possible phoneme\nconfusions that a speaker of a given native language\n(L1) is likely to encounter when speaking a second\nlanguage (L2). These confusions can be imagined\nas a matrix C(L1, L2), which contains likelihood\nof the ith L2 phoneme (phi) being confused as the\njth L2 phoneme (phj) by a native speaker of L1 as\nthe value in the cell C(L1, L2)[i][j].\nC(L1, L2)[i][j] = P(phj|phi)\n(1)\nBuilding this matrix across all pairs of languages\nis an expensive task. It is also challenging to accu-\nrately determine the likelihood of such confusions\nwithout large datasets of parallel words.\nTransliteration models are trained on large par-\nallel datasets with the objective of transcribing\nsounds representing words in one language with in\nthe script of a different language. They imbibe im-\nportant information about sounds in one language\nthat are indistinguishable in another (and therefore\nlexicalized identically). We propose a round-trip\ntransliteration based method which aims to mine\nthese phoneme confusions and their likelihoods\nfrom this knowledge hidden in transliteration mod-\nels. We collect a large dictionary of English words\n(our chosen L2) and apply two steps of transliter-\nation 3 (Bhat et al., 2015) to convert them back\nto English via a pivot language (L1), as shown in\nFigure 1. We then align the phoneme sequence of\nthe original word with that of its round-trip translit-\nerated version using the Needleman-Wunsch algo-\nrithm (Needleman and Wunsch, 1970). We count\nthe frequency of each of the possible sound-shifts\nin the whole corpus to estimate likelihood. Figure 2\nshows examples of word pairs created through dif-\nferent pivot languages and the phoneme confusion\nmined from these. We consider only the top-10\nmost frequent phoneme confusions per (L1, L2)\nfor the next step.\n3.2\nBiPhone: A Generative Model for L1-L2\nPhonetic Misspellings\nThe second problem we focus on is to create a\nmodel for sampling phonetic misspellings ( \u02dc\nw) for\na given word (w) in L2 that a native speaker of\nL1 is likely to make. We can represent the proba-\nbility distribution learnt by this model as P( \u02dc\nw|w).\nAssuming a deterministic mapping from the word\nw to its phoneme sequence phw, and introducing\nthe corrupted phoneme sequence (ph \u02dc\nw) that finally\n3https://github.com/libindic/indic-trans\ngenerates \u02dc\nw, we can rewrite it as -\nP( \u02dc\nw|w) = P( \u02dc\nw|phw)\n=\nX\nph \u02dc\nw\nP(ph \u02dc\nw|phw) \u2217 P( \u02dc\nw|ph \u02dc\nw)\n(2)\nHere a word w is comprised of graphemes\n{w1, w2, ..} where wi \u2208 Graphemes(L2) and a\nphoneme sequence phw is comprised of phonemes\n{ph1, ph2, ..} where each individual phoneme phi\nis from the set of available phonemes for L2. In\nour experiments, we use the ARPAbet phoneme set\nfor English 4.\nPhoneme-Phoneme Error Model: The first\nterm under the summation in Equation 2 models\nthe likelihood of generating a corrupted phoneme\nsequence ph \u02dc\nw given that a native speaker of L1 is\nattempting to speak a phoneme sequence phw in\nL2. With simplifying independence assumptions\nthat each phoneme is corrupted individually, inde-\npendent of phonemes around it, we can factorize\nthis term to utilize the phoneme confusion matrix\nwe have mined.\nP(ph \u02dc\nw|phw) =\nY\ni\nP(phi\n\u02dcw|phi\nw)\n=\nY\ni\nC(L1, L2)[phi\nw][phi\n\u02dcw]\n(3)\nPhoneme-Grapheme Density Model: The sec-\nond term in Equation 2 expresses the probability of\ngenerating the grapheme sequence to represent \u02dc\nw\ngiven the phoneme sequence ph \u02dc\nw. We can assume\nequal lengths for the two sequences, by allowing\nsome phonemes to not generate any graphemes,\nwhen necessary. Again, we make independence\nassumptions where the grapheme used to represent\na given phoneme does not depend on neighbouring\nphonemes or graphemes.\nP( \u02dc\nw|ph \u02dc\nw) =\nY\ni\nP( \u02dcwi|phi\n\u02dcw)\n(4)\nTo compute P( \u02dcwi|phi\n\u02dcw), we use a pronuncia-\ntion dictionary in L2 (CMUDict5 for English).\nFirst, phoneme-character probabilities are gener-\nated through alignment. Next, for each word, char-\nacter sequences are converted to graphemes by\nmaximizing the alignment score. Finally, the var-\nious phoneme-grapheme alignments along with\n4https://en.wikipedia.org/wiki/ARPABET\n5http://www.speech.cs.cmu.edu/cgi-bin/cmudict\nPhoneme Shift\nHi\nTa\nBn\nAH2 -> AH0\n100%\n-\n100%\nIH2 -> IH0\n100%\n-\n100%\nER2 -> ER0\n100%\n-\n-\nDH -> TH\n54%\n-\n62%\nER2 -> ER0\n95%\n-\n-\nD -> T\n-\n30%\n-\nB -> P\n-\n39%\n-\nDH -> D\n-\n0%\n-\nG -> K\n-\n47%\n-\nV -> B\n-\n-\n58%\nZ -> S\n-\n-\n50%\nTable 1: Plausibility scores for different phoneme shifts\nacross Hindi, Tamil, and Bengali.\nL1\nCorrect\nMisspelt\nPhoneme\nWord\nWord\nVariation\nHindi\nthey\nthay\nDH -> TH\nTamil\nexam\neksam\nG -> K\nbacterial\npactirial\nB -> P\nBengali\nvery\nbery\nV -> B\nequation\nikvasan\nZH -> S\nTable 2: Examples of highly plausible misspellings as\nrated by native speakers for various L1 languages with\nL2 language as English\ntheir frequencies are converted to probabilities by\ndividing it by the frequency of the phoneme.\nInference:\nGiven an original phoneme se-\nquence for a word to be corrupted, we begin\nsampling with a fixed width (K) beam from left\nto right.\nAt each position, we pick the top-K\ncandidates comprising both phoneme-phoneme\nshifts and phoneme-grapheme alternatives greed-\nily. Since both Phoneme-Phoneme Error Model\nand Phoneme-Grapheme Density Model are con-\ntext independent, the greedy strategy gives us the\nglobal top-K misspellings. Identity corruptions are\nremoved as a final step.\n4\nEvaluations\nWe evaluate the misspellings generated by our\nmodel along two distinct dimensions.\n4.1\nPlausibility\nFor evaluating plausibility of generated mis-\nspellings from Bi-Phone, we focus on three native\nlanguages (L1) : Hindi, Tamil and Bengali with\nEnglish as the non-native language (L2). Hindi and\nBengali are the two most widely spoken languages\nin India and among the top few in the world. Tamil\nis also a widely spoken language in India and intro-\nFigure 3: Precision and coverage plotted at different\nmisspelling confidence scores (labels on points). Cover-\nage is represented as a fraction of 31,755,066 sentences\nthat have atleast one non-English dictionary word.\nduces typological diversity in our analysis. Finally,\nour choice of L1 is also based on availability of\nnative speakers for the annotation task.\nFor each language, we present 150 randomly\nselected word, misspelling pairs generated from Bi-\nPhone to native speakers (5 for Hindi, 3 for Tamil\nand Bengali each). Rater instructions are as fol-\nlows: Given a list of pairs in English (correct\nword, misspelling), the task is to evaluate if the\nmisspelling is plausible for pronunciation shifts of-\nten made by speakers of the given first language.\nFor example - Bengali speakers often shift the \u201cv\u201d\nsound to \u201cb\u201d so, \u201cevicted\u201d could be plausibly mis-\nspelt as \u201cebicted\u201d or \u201cabicted\u201d. Each rater provides\na 1 or 0 to indicate whether the variant looks plau-\nsible or not, respectively. We use a simple majority\nto assign an overall label to each pair. The raters for\nthis task are our colleagues who are native speakers\nof the language they are annotating for.\nTable 1 reports the percentage of misspellings\nrated as plausible for each phoneme shift. We\nobserve that misspellings for Tamil are rated as\nless plausible than for other languages. The rea-\nson for this is the more drastic phoneme shifts un-\ncovered in Tamil (B -> P and G -> K). However,\nmisspellings stemming from these shifts are still\nnot rated as completely implausible, which empha-\nsizes that these shifts are indeed common. We also\nmeasure inter-annotator agreement through kappa\nscores which are 0.40 for Hindi, 0.37 for Tamil,\nand 0.34 for Bengali.\n4.2\nPrevalence: Coverage Analysis\nIn the previous section we investigate the plausi-\nbility of phoneme-shifts mined by Bi-Phone and\nthe misspellings created as a result. However, this\ninvestigation does not throw light on the pervasive-\nness of such misspellings in real world content.\nIn this section, we aim to evaluate the severity\nof the phonetic misspelling issue by uncovering\nsuch misspellings in web data. For our analysis,\nwe use the Common Crawl6 corpus, which is a\npublicly available scrape of real web data. While\nmost existing language work deals with a highly\ncleaned version of this corpus (Raffel et al., 2020b),\nwe skip such filtering and cleaning steps to retain\nnoisy, user-generated text. We only use Hindi as the\nnative language (L1) in this analysis. Our analysis\nhas three distinct steps - (1) Candidate Sentence\nRetrieval, (2) Misspelling Confidence Scoring, and\n(3) Human Evaluation.\n1. Candidate Sentence Retrieval: We begin\nour analysis by creating 10 misspellings of the\ntop 10,000 most common English words from the\nGoogle ngram corpus (Michel et al., 2011) and\nwords that make up 90%-ile of the English words\nin the Common Crawl corpus. Our hypothesis is\nthat the most common words in English are also\nthe most likely to be misspelt with native language\ninfluences. Our pool of sentences is the set of\nall sentences with at least one non-English dictio-\nnary word. The size of this pool is 31,755,066\nsentences. From this pool, we create our candidate\nset by retrieving all sentences that contain one of\nour generated misspellings.\n2. Misspelling Confidence Scoring: The next\nstep is to ascertain that the misspellings retrieved\nare indeed a noisy form of the intended original\nword and not a completely different word. For\nexample, \u201cvare\" could be a corruption of the\nEnglish word \u201cwhere\" with the W -> V sound shift,\nor it could be the less used English word meaning\na weasel 7.\nWe use a simple 1-word left and\nright context for this disambiguation. For every\noccurrence of a potentially misspelt word \u02c6W in\ncontext (L \u02c6\nW , \u02c6W, R \u02c6\nW ), we evaluate the probability\nof seeing the corresponding clean word (W) in the\nsame context. This likelihood, P(L \u02c6\nW , W, R \u02c6\nW )\ncomputed as follows can be used as a score to rep-\nresent our confidence in the retrieved misspelling.\n6https://commoncrawl.org/\n7https://www.merriam-webster.com/dictionary/vare\nP(L \u02c6\nW , W, R \u02c6\nW )\n=\nF(L \u02c6\nW , W, R \u02c6\nW )\nP\nw F(L \u02c6\nW , w, R \u02c6\nW ) , if\nX\nw\nF(L \u02c6\nW , w, R \u02c6\nW ) > 0\n= 0.4 \u2217\n\"\nF(L \u02c6\nW , W)\nP\nw F(L \u02c6\nW , w) +\nF(W, R \u02c6\nW )\nP\nw F(w, R \u02c6\nW )\n#\n, otherwise\nHere 0.4 is the backoff-weight following the\nStupid Backoff technique from Brants et al. (2007).\nWe can compute the coverage of Bi-Phone in\nweb data by considering the fraction of sentences\nwhere the misspelling confidence score is greater\nthan a certain threshold over the total number of\nsentences in our original pool.\n3. Human Evaluation: Finally, we also sam-\nple a subset of the sentences to have human raters\nverify that our retrieved misspellings indeed cor-\nrespond to the original word. We show raters the\noriginal retrieved sentence which contains the gen-\nerated misspelling and a parallel sentence where\nthe misspelling has been replaced with the original\nword and ask raters if this correction is valid in the\ngiven context. We can compute a reliable metric\nfor precision with this human evaluation. Ratings\nfor this task are fetched from a cloud rating service\nwhere raters are bilingual Hindi-English speakers\nwith a graduate degree.\nFigure 3 presents the precision and coverage\nat different thresholds of misspelling confidence\nscore. At threshold 0.001, we have roughly 70%\nprecision while still having a coverage of 1.14%\n(362,472 sentences*). The size of the initial pool\n(30 million candidate sentences) and the simple\nmethod used for our analysis underline how preva-\nlent such misspellings are. Also it is important note\nthat such misspellings will be even more prevalent\nin a purely UGC (user generated content) corpus.\nC4 contains a significant fraction of clean English\nweb pages.\n5\nThe FunGLUE Benchmark\nSignificant progress has been made in recent re-\nsearch to substantially improve performance of lan-\nguage understanding tasks. SuperGLUE (Wang\net al., 2019) is a very popular benchmark with\nten diverse and hard language understanding tasks.\nThese tasks are BoolQ, CommitmentBank (CB),\nMulti-Sentence Reading Comprehension (Mul-\ntiRC), Choice of Plausible Alternatives (COPA),\nReading Comprehension with Commonsense Rea-\nsoning (ReCoRD), Recognizing Textual Entail-\nSplit\nDescription\nContains\nPhonetic\nNoise\ntrain\nTrain split from SuperGLUE as is\nNo\ndev\nDev split from SuperGLUE as is\nNo\ntest\nDev split from SuperGLUE noised\nwith BiPhone\nYes\nTable 3: Description of splits in FunGLUE. Checkpoint\nselection is done on the dev set which does not contain\nphonetic misspellings. The test set is used only for\nreporting results.\nTask\nField Name\nBoolQ\nquestion\nCB\npremise\nCOPA\npremise\nMultiRC\nquestion\nReCoRD\nquery\nRTE\nhypothesis\nWiC\nsentence1\nTable 4: Fields we noise for different task when creat-\ning FunGLUE.\nment (RTE), Words in Context (WiC), Broadcov-\nerage Diagnostics (AX-b), The Winograd Schema\nChallenge (WSC), and Winogender Schema Diag-\nnostics (AX-g). We argue that for language under-\nstanding models to be effective for bi-lingual users,\nthey must be robust to inter-language phonetic\nspelling variations. Towards this end, we intro-\nduce FunGLUE8 which stands for Ph(F)onetically\nnoised GLUE where randomly selected words from\ntasks in the SuperGLUE benchmark are corrupted\nwith Bi-Phone based misspellings. It is extremely\nimportant to note that we only create a hold-out\nevaluation set created by introducing misspellings\nto the SuperGLUE development set. The training\nset is left clean to mimic real world scenarios where\nnoised training data is difficult to obtain. Addition-\nally, it would be unfair to train and evaluate models\non synthetic misspellings from the same source.\nTable 3 summarizes the training, validation, and\ntest sets in FunGLUE.\nMisspellings for words in the original task are\ncreated from Bi-Phone with the following design\nchoices:\n(i) What to noise: Since we want to keep the\ntask realistic, we only introduce misspellings in\ncertain pre-selected fields and not all text fields.\nThis reflects real world situations where content\nis often available in well spelt English but user\nqueries have phonetic errors. Table 4 presents the\n8https://github.com/google-research-datasets/FunGLUE\nTask\nTokens misspelt\nExamples w/ noise\nboolq\n30.6%\n96.2%\ncb\n29.5%\n96.4%\nmultirc\n33.8%\n96.4%\ncopa\n25.2%\n78.0%\nrecord\n29.5%\n99.4%\nrte\n35.9%\n97.1%\nwic\n28.9%\n84.0%\nTable 5: Stats on amount of noise added in FunGLUE.\nFigure 4: Examples from validation set of two tasks\nin FunGLUE against SuperGLUE. Words which are\nreplaced with their noised versions are in red.\nfields we actually noise.\n(ii) Which misspellings to use: Since we ex-\npect benchmarks to have a high quality, we put\nin a number of guardrails to ensure poor quality\nmisspellings do not make it through to the bench-\nmark. First, we only use Bi-Phone misspellings\nwith Hindi and Bengali as native language since\nTamil misspellings were rated as less plausible by\nnative speakers. Next, we noticed that plausibility\nscores drop for words smaller than 4 characters,\nso we only noise longer words. We also filter out\nmisspellings that contain certain patterns of implau-\nsible noise generated by our Grapheme2Phoneme\nmodel with rules. Finally, all (word, misspelling)\npairs used in FunGLUE are manually verified by\nmembers of the team as plausible.\n(iii) How much noise to add: Since we do not\nwant to artificially introduce too much noise, we\nonly replace 30% of words from the original bench-\nmark across tasks. Table 5 contains stats on the\namount of noise added to each task. We were cur-\nrently unable to include the noised version of the\nWSC, AX-b and AX-g tasks due to some difficul-\nties in accessing the eval sets. We plan to include\nthis with the final data release.\n5.1\nModels\nIn this section we investigate if state-of-the-art mod-\nels are robust to the phonetic noise introduced\nby FunGLUE by comparing their performance\non SuperGLUE. For this purpose, we consider\nmT5 (Xue et al., 2021b) and ByT5 (Xue et al.,\n2021a) models. These are both transformer based\nsequence-to-sequence models that frame all lan-\nguage understanding tasks as sequence generation.\nmT5 uses sub-word tokenization built on a multi-\nlingual corpus, to represent text. It should therefore\nbe more robust to input variations than comparable\nmodels with tokenization on monolingual corpora\nwith lower diversity. ByT5 avoids the tokeniza-\ntion step by building input representations from\nindividual bytes, and is designed to perform more\ngracefully on noisy text across a range of tasks.\nFor all models, we use the base architecture.\nSince training these models is expensive, we do\nnot perform any hyper-parameter search. Instead,\nwe use fine-tuning parameter values from the orig-\ninal papers. Crucially, fine-tuning for all models\nis performed identically on clean data from Su-\nperGLUE. We use the same mixture of tasks as\nin Raffel et al. (2020a). Fine-tuning is done for up\nto 200,000 steps and the best checkpoint is picked\nbased on performance on the clean dev set from Su-\nperGLUE. We use 16 TPUv3s for fine-tuning all\nmodels.\n5.2\nSpell Correction Baselines\nSpell correction methods provide obvious baselines\nwhen dealing with incorrectly spelt data. Spell cor-\nrected data can then be use to run inference with\nexisting models. To evaluate the merit of this tech-\nnique, we measure performance after correction\nfrom two state of the art approaches: (1) NeuSpell\nBERT (Jayanthi et al., 2020) - spell corrector built\non top of BERT. (2) BERT-Large mask prediction -\nusing a BERT Large model for predicting the cor-\nrect word in positions where we have misspellings.\nIn both of these approaches, we provide the posi-\ntions of incorrectly spelt words. This is an advan-\ntage since this information is not available in real\nworld noisy text. We compare the performance of\nboth mT5 and ByT5 on FunGLUE eval sets cor-\nrected by these approaches.\n5.3\nResults\nRows 1-4 in Table 6 show the performance of mT5\nand ByT5 on SuperGLUE and FunGLUE. There\nis a clear drop in performance for both models\non FunGLUE, with both mT5 and ByT5 dropping\nupto 16 F1 points on the CB dataset. The mT5\nmodel also drops by roughly 9 points in accuracy\non the BoolQ dataset, and similarly 9 F1 points\non the ReCoRD dataset. While the ByT5 model\nis in general more robust than the mT5 model, its\nNo.\nModel\nBoolQ\nCB\nCOPA\nMultiRC\nReCoRD\nRTE\nWiC\nAcc\nAcc\nF1\nAcc\nEM\nF1\nEM\nF1\nAcc\nAcc\nSuperGLUE\n1\nmT5\n78.10\n92.86\n90.53\n61.00\n33.68\n73.03\n67.22\n68.26\n74.37\n68.03\n2\nByT5\n79.20\n91.07\n90.37\n58.00\n32.00\n70.14\n72.10\n72.79\n81.23\n70.85\nFunGLUE\n3\nmT5\n68.81\n80.36\n74.21\n55.00\n28.23\n70.37\n58.46\n59.46\n67.87\n63.64\n3a\nmT5 - NeuSpell\n67.92\n76.79\n74.99\n64.00\n30.43\n70.85\n60.36\n61.33\n65.34\n65.83\n3b\nmT5 - Bert-L mask pred\n66.42\n71.43\n79.6\n57.00\n27.70\n67.91\n55.6\n56.63\n58.84\n62.54\n4\nByT5\n74.04\n80.36\n73.67\n58.00\n32.42\n72.73\n67.54\n68.19\n70.40\n66.46\n4a\nByT5 - NeuSpell\n72.84\n76.79\n67.86\n54.00\n32.53\n72.47\n63.64\n64.25\n69.68\n66.46\n4b\nByT5 - Bert-L mask pred\n70.52\n75.00\n70.7\n55.00\n26.76\n68.60\n59.75\n60.35\n64.62\n64.26\n5\nPhonetic mT5\n71.80\n80.36\n73.66\n53.00\n25.81\n72.2\n55.85\n56.86\n61.37\n63.17\n6\nPhonetic ByT5\n74.37\n87.50\n85.46\n66.00\n33.26\n75.15\n70.21\n70.88\n76.17\n66.77\nTable 6: First 4 rows: Performance of SoTA models on tasks in the SuperGLUE and FunGLUE (noised) benchmarks.\nPerformance of both mT5 and ByT5 (rows 3 and 4 compared to 1 and 2) drops on the noised benchmark, although\nByT5 (row 4) is slightly more robust. Rows 3a, 3b, 4a, and 4b show the performance of mT5 and ByT5 after\nmisspelt words in the eval set are replaced with corrections from SoTA techniques. While mT5 benefits slightly from\nsuch corrections, ByT5 performance is worse across all tasks after spell correction is applied. This demonstrates the\ninability of current spell correction models to handle such misspellings. Rows 3a and 4a correspond to corrections\nfrom the NeuSpell (Jayanthi et al., 2020) model. Rows 3b and 4b correspond to corrections using mask prediction\nfrom a Bert-Large model. Last 2 rows: Performance of the same models when trained on a few additional steps\nwith the phoneme prediction task on clean data (Phonetic mT5 and ByT5). The ByT5 (row 6 compared to row 4)\nmodel gains substantially with such pre-training.\nperformance also drops by 10 points in accuracy\non RTE.\nThe spell correction baselines (Rows 3a, 3b,\n4a, 4b) also fail to recover performance. With\nNeuSpell, mT5 sees a drop in BoolQ and RTE,\nslight improvement on CB, MultiRC, Record, WIC\n(<2 points Acc/F1). On COPA, we observe a sub-\nstantial recovery (55 -> 64). For ByT5 however,\nthere is a drop in performance across the board.\nNeuSpell is not well equipped to handle phonetic\nmisspellings. Therefore the spell corrected word is\noften farther from the original word than the mis-\nspelling. These bad corrections hurt ByT5, which\nis slightly more robust to misspellings than mT5.\nWith Bert-Large mask prediction, for mT5 there is\na slight improvement on COPA and improvement\non CB(74.21 ->79.6), but worse performance on all\nother tasks. Again for ByT5, we see degradation\nin performance across the board. Since 30% of\nthe tokens are phonetically misspelt, the contextual\nmask prediction task is also not accurate. Another\nfailure mode we observed was that the prediction is\noften the correct type (adjective for adjective) but\nnot the original token.\nThis clearly demonstrates the challenge posed\nby phoneme-shift based noisy misspellings intro-\nduced in FunGLUE . Current models and training\nschemes are ill-equipped to function on such data.\nFigure 5: Demonstration of our mixture pre-training\ntask that combines standard span-corruption with the\nnovel phoneme prediction task in an 80:20 ratio. All\nweights and embeddings in the model are shared.\n6\nPhoneme Prediction as a Pre-training\nTask\nGiven the inadequacy of existing State-of-the-Art\nmodels in handling phonetic noise in inputs, we\npropose a novel pre-training task of phoneme\nprediction. We posit that the task of predicting\nphoneme sequences will have the effect of teach-\ning the model \u201cphonetic information\". Since dif-\nferent lexicalizations of the same sound will have\nthe same phoneme sequence, the model will learn\nto embed these close. Additionally since close\nsounds often appear in similar intra-word contexts,\ntheir graphemic representations will also be pushed\nclosed together.\nHowever, to perform NLP tasks, semantic simi-\nlarity is still crucial. In current models this is often\nachieved through some variation of the span corrup-\ntion task (corrupting a span in the input and predict-\ning it on the output). We propose a mixture of these\ntwo tasks where a small amount of the phoneme\nprediction task (20%) is mixed into the standard\nspan corruption task. Figure 5 demonstrates our\nproposal through two example instances. In the\nfirst instance the span \u201csofa design\" is masked in\nthe input (replaced with a sentinel) and is expected\nto be produced on the output. This teaches the\nmodel that adjectives like \u201cexquisite\" are seman-\ntically close. The second instance has the word\n\u201cbuilding\" in the input and the phoneme sequence\ncorresponding to this word (B, IH, L, D, IH, NG)\non the output. This task teaches the model that all\ntokens that produce the same sound (like \u201cui\" or\n\u201ce\" for IH) should be embedded close.\nWe train both mT5 and ByT5 checkpoints for an\nadditional 100,000 steps (10% additional steps) on\nthis mixture task. We call this step of additional\npre-training, \u201cPhonetic pre-training\". Finally, we\nfine-tune these models on the standard clean Su-\nperGLUE training set. The phoneme prediction\ndata is created by taking roughly 2,000,000 highest\nfrequency words from the Common Crawl English\ndata and getting their pronunciations from an off-\nthe-shelf Grapheme to Phoneme model. As we\nwill see later, this kind of noisy supervision (not\nhuman labelled) is still useful in making models\nphonetically robust.\nThe last two rows in Table 6 show the perfor-\nmance of these models on FunGLUE. We find that\nthe simple additional pre-training step of phoneme-\nprediction substantially improves performance of\nthe ByT5 model on the noised benchmark (row\n6 against row 4). Performance on CB increases\nby 11 F1 points, on COPA there is a 8 point ac-\ncuracy gain, and a 5 point accuracy gain on RTE.\nWhile performance still lags compared to the clean\nbenchmark SuperGLUE (row 6 against row 2) on\nmost tasks, for MultiRC and COPA, we find that\nthe phonetically pre-trained ByT5 model even out-\nperforms the vanilla pre-trained model (row 2) num-\nbers on the clean task. This is particularly impres-\nsive because the Phonetic ByT5 model (row 6) has\nnever seen any noisy data during its training. The\nmT5 model does not however see the same im-\npressive gains through this pre-training task. We\nhypothesize this is because of the harder sub-word\ntokenization in mT5. Many tokens that this model\nneeds on the noised task are never seen when it\u2019s\ntrained on clean data and therefore have poor rep-\nresentations.\nThe ByT5 model does however have certain\ndrawbacks. Since input sequences are much longer\nwith byte level representations, both training and\ninference times are much slower than a sub-word\ntokenized alternative (like mT5).\nAdditionally,\nthe byte-level representation also restricts input\nsequence lengths. Using these phonetically ro-\nbust byte-level models as teachers for sub-word\ntokenized student models remains an interesting\ndirection for future work.\n7\nConclusion\nLanguage is a significant barrier to technology es-\npecially for new internet users. For such users, En-\nglish often is not their first language. The speech\ncommunity has made significant progress in mak-\ning technology (ASR for instance) accessible for\nsuch users by making models robust to account\nfor inter-language interactions. We argue that a\nsimilar line of effort is needed in the Natural Lan-\nguage Understanding for Text community as well.\nTo this end, we first propose a generative model\nBi-Phone that can account for L1-L2 interactions\nin text. Next we show the inter-language perturba-\ntions generated by Bi-Phone are indeed present in\nnon-trival amount in the common crawl corpus. We\nalso release a new benchmark FunGLUE to help\nfurther research in this area. We also present our\nearly yet very promising explorations on making\nnatural language understanding models robust to\nL1-L2 phonetic shifts through a novel phoneme\nprediction based pre-training.\n8\nLimitations\nAlgorithmic Limitations: The current approach\nassumes each phoneme / grapheme corruption\nis independent of the surrounding phonemes /\ngraphemes, which can be relaxed to get further\ninsights and model any contextual phonetic shifts.\nThe relative importance between grapheme and\nphoneme corruptions could also be explored as a\nhyperparameter to personalize more to the type of\nerrors of a community.\nOther Limitations (with respect to available data\nand existing resources): Our coverage analysis is\nconservative since it does not cover the user gen-\nerated data from various social media where such\nL1-L2 phonetic misspellings are bound to be more\ncommon. The coverage analysis also relies on the\ncontext not being corrupted. However, this might\nnot necessarily hold and the analysis could benefit\nfrom a careful formulation of a relaxed matching\ncriteria that also considers cases with corrupted\ncontexts. With transliteration playing a major role\nin our solution, it is difficult to immediately ex-\ntend the work to low-resource languages that do\nnot have models or appropriate datasets to build\ntransliteration modules.\nReferences\nYves Bestgen and Sylviane Granger. 2011. Categoriz-\ning spelling errors to assess L2 writing. International\nJournal of Continuing Engineering Education and\nLife Long Learning, 21(2-3):235\u2013252.\nIrshad Ahmad Bhat, Vandan Mujadia, Aniruddha Tam-\nmewar, Riyaz Ahmad Bhat, and Manish Shrivastava.\n2015. Iiit-h system submission for fire2014 shared\ntask on transliterated search. In Proceedings of the\nForum for Information Retrieval Evaluation, FIRE\n\u201914, pages 48\u201353, New York, NY, USA. ACM.\nJordan J. Bird, Elizabeth F. Wanner, Anik\u00f3 Ek\u00e1rt, and\nDiego R. Faria. 2019. Accent classification in human\nspeech biometrics for native and non-native english\nspeakers. In Proceedings of the 12th ACM Inter-\nnational Conference on PErvasive Technologies Re-\nlated to Assistive Environments, PETRA 2019, Island\nof Rhodes, Greece, June 5-7, 2019, pages 554\u2013560.\nACM.\nDaniel Blanchard, Joel Tetreault, Derrick Higgins,\nAoife Cahill, and Martin Chodorow. 2013. Toefl11:\nA corpus of non-native english. ETS Research Report\nSeries, 2013:i\u201315.\nThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.\nOch, and Jeffrey Dean. 2007. Large language mod-\nels in machine translation. In Proceedings of the\n2007 Joint Conference on Empirical Methods in Nat-\nural Language Processing and Computational Nat-\nural Language Learning (EMNLP-CoNLL), pages\n858\u2013867, Prague, Czech Republic. Association for\nComputational Linguistics.\nLingzhen Chen, Carlo Strapparava, and Vivi Nastase.\n2017. Improving native language identification by\nusing spelling errors. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 542\u2013546,\nVancouver, Canada. Association for Computational\nLinguistics.\nVivian Cook. 1997. L2 users and english spelling. Jour-\nnal of Multilingual and Multicultural Development,\n18(6):474\u2013488.\nGary F. Simons Eberhard, David M. and Charles D.\nFennig. 2022. Ethnologue, languages of the world.\nhttp://www. ethnologue. com/.\nMichael Flor, Michael Fried, and Alla Rozovskaya.\n2019. A benchmark corpus of English misspellings\nand a minimally-supervised model for spelling cor-\nrection. In Proceedings of the Fourteenth Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 76\u201386, Florence, Italy. Associa-\ntion for Computational Linguistics.\nGregory Grefenstette and Julien Nioche. 2000. Esti-\nmation of english and non-english language use on\nthe www. In Content-Based Multimedia Informa-\ntion Access - Volume 1, RIAO \u201900, page 237\u2013246,\nParis, FRA. LE CENTRE DE HAUTES ETUDES\nINTERNATIONALES D\u2019INFORMATIQUE DOCU-\nMENTAIRE.\nDaniel Hl\u00e1dek, J\u00e1n Sta\u0161, and Mat\u00fa\u0161 Pleva. 2020. Survey\nof automatic spelling correction. Electronics, 9(10).\nMuhammad Hasan Ibrahim. 1978. Patterns in spelling\nerrors. English Language Teaching, 32:207\u2013212.\nSai Muralidhar Jayanthi, Danish Pruthi, and Graham\nNeubig. 2020. NeuSpell: A neural spelling correc-\ntion toolkit. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, pages 158\u2013164, Online.\nAssociation for Computational Linguistics.\nKaren Kukich. 1992.\nTechniques for automatically\ncorrecting words in text.\nACM Comput. Surv.,\n24(4):377\u2013439.\nJean-Baptiste Michel, Yuan Kui Shen, Aviva Presser\nAiden, Adrian Veres, Matthew K Gray, Google Books\nTeam, Joseph P Pickett, Dale Hoiberg, Dan Clancy,\nPeter Norvig, et al. 2011. Quantitative analysis of\nculture using millions of digitized books. science,\n331(6014):176\u2013182.\nMiki Motohashi-Saigo and Toru Ishizawa. 2020. A rela-\ntionship between orthographic output and perception\nin l2 Japanese phonology by L1 English speakers.\nAmpersand, 7:100071.\nRyo Nagata, Hiroya Takamura, and Graham Neubig.\n2017.\nAdaptive spelling error correction models\nfor learner english.\nProcedia Computer Science,\n112:474\u2013483.\nKnowledge-Based and Intelligent\nInformation & Engineering Systems: Proceedings\nof the 21st International Conference, KES-20176-8\nSeptember 2017, Marseille, France.\nSaul B. Needleman and Christian D. Wunsch. 1970.\nA general method applicable to the search for simi-\nlarities in the amino acid sequence of two proteins.\nJournal of Molecular Biology, 48(3):443\u2013453.\nGarrett Nicolai, Bradley Hauer, Mohammad Salameh,\nLei Yao, and Grzegorz Kondrak. 2013.\nCognate\nand misspelling features for natural language iden-\ntification. In Proceedings of the Eighth Workshop\non Innovative Use of NLP for Building Educational\nApplications, pages 140\u2013145, Atlanta, Georgia. As-\nsociation for Computational Linguistics.\nAnastasiia Ogneva. 2018. Spelling errors in L2 Russian:\nevidence from Spanish-speaking students. Estudios\ninterling\u00fc\u00edsticos, 6:116\u2013131.\nKacper Radzikowski, Robert Nowak, Le Wang, and\nOsamu Yoshie. 2019. Dual supervised learning for\nnon-native speech recognition. EURASIP J. Audio\nSpeech Music. Process., 2019:3.\nKacper Radzikowski, Le Wang, Osamu Yoshie, and\nRobert M. Nowak. 2021. Accent modification for\nspeech recognition of non-native speakers using neu-\nral style transfer. EURASIP J. Audio Speech Music.\nProcess., 2021(1):11.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020a. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1\u201367.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020b.\nExploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. J. Mach. Learn. Res., 21(140):1\u201367.\nIda Rukmana Sari. 2014. Common errors in students\u2019\nspelling on the required words for the seventh graders.\nEducate, 4(2):35\u201343.\nSanket Shah, Satarupa Guha, Simran Khanuja, and\nSunayana Sitaram. 2020. Cross-lingual and multilin-\ngual spoken term detection for low-resource indian\nlanguages. CoRR, abs/2011.06226.\nKristina Toutanova and Robert Moore. 2002. Pronunci-\nation modeling for improved spelling correction. In\nProceedings of the 40th Annual Meeting of the Associ-\nation for Computational Linguistics, pages 144\u2013151,\nPhiladelphia, Pennsylvania, USA. Association for\nComputational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. Superglue: A stick-\nier benchmark for general-purpose language under-\nstanding systems. Advances in neural information\nprocessing systems, 32.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2021a. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models. CoRR,\nabs/2105.13626.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021b. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483\u2013498, On-\nline. Association for Computational Linguistics.\n"
  },
  {
    "title": "Decomposing the Generalization Gap in Imitation Learning for Visual Robotic Manipulation",
    "link": "https://arxiv.org/pdf/2307.03659.pdf",
    "upvote": "5",
    "text": "Decomposing the Generalization Gap in\nImitation Learning for Visual Robotic Manipulation\nAnnie Xie\u22171\nLisa Lee\u22172\nTed Xiao2\nChelsea Finn1,2\n1Stanford University\n2Google DeepMind\nAbstract: What makes generalization hard for imitation learning in visual robotic\nmanipulation? This question is difficult to approach at face value, but the environ-\nment from the perspective of a robot can often be decomposed into enumerable\nfactors of variation, such as the lighting conditions or the placement of the camera.\nEmpirically, generalization to some of these factors have presented a greater ob-\nstacle than others, but existing work sheds little light on precisely how much each\nfactor contributes to the generalization gap. Towards an answer to this question,\nwe study imitation learning policies in simulation and on a real robot language-\nconditioned manipulation task to quantify the difficulty of generalization to dif-\nferent (sets of) factors. We also design a new simulated benchmark of 19 tasks\nwith 11 factors of variation to facilitate more controlled evaluations of generaliza-\ntion. From our study, we determine an ordering of factors based on generalization\ndifficulty, that is consistent across simulation and our real robot setup.1\nKeywords: Environment generalization, imitation learning, robotic manipulation\n1\nIntroduction\nFigure 1: Success rates on different envi-\nronment shifts. New camera positions are\nthe hardest to generalize to while new back-\ngrounds are the easiest.\nRobotic policies often fail to generalize to new environ-\nments, even after training on similar contexts and condi-\ntions. In robotic manipulation, data augmentation tech-\nniques [1, 2, 3, 4, 5] and representations pre-trained on\nlarge datasets [6, 7, 8, 9, 10, 11, 12] improve performance\nbut a gap still remains. Simultaneously, there has also\nbeen a focus on the collection and curation of reusable\nrobotic datasets [13, 14, 15, 16, 17], but there lacks a con-\nsensus on how much more data, and what kind of data, is\nneeded for good generalization. These efforts could be\nmade significantly more productive with a better under-\nstanding of which dimensions existing models struggle\nwith. Hence, this work seeks to answer the question: What are the factors that contribute most to\nthe difficulty of generalization to new environments in vision-based robotic manipulation?\nTo approach this question, we characterize environmental variations as a combination of independent\nfactors, namely the background, lighting condition, distractor objects, table texture, object texture,\ntable position, and camera position. This decomposition allows us to quantify how much each factor\ncontributes to the generalization gap, which we analyze in the imitation learning setting (see Fig. 1\nfor a summary of our real robot evaluations). While vision models are robust to many of these\nfactors already [18, 19, 20], robotic policies are considerably less mature, due to the smaller and\nless varied datasets they train on. In robot learning, data collection is largely an active process, in\nwhich robotics researchers design and control the environment the robot interacts with. As a result,\nnaturally occurring variations, such as different backgrounds, are missing in many robotics datasets.\nFinally, robotics tasks require dynamic, multi-step decisions, unlike computer vision tasks such as\n1Videos & code are available at: https://sites.google.com/view/generalization-gap\narXiv:2307.03659v1  [cs.RO]  7 Jul 2023\nimage classification. These differences motivate our formal study of these environment factors in\nthe context of robotic manipulation.\nIn our study, we evaluate a real robot manipulator on over 20 test scenarios featuring new lighting\nconditions, distractor objects, backgrounds, table textures, and camera positions. We also design\na suite of 19 simulated tasks, equipped with 11 customizable environment factors, which we call\nFactor World, to supplement our study. With over 100 configurations for each factor, Factor World\nis a rich benchmark for evaluating generalization, which we hope will facilitate more fine-grained\nevaluations of new models, reveal potential areas of improvement, and inform future model design.\nOur study reveals the following insights:\n\u2022 Most pairs of factors do not have a compounding effect on generalization performance.\nFor\nexample, generalizing to the combination of new table textures and new distractor objects is no\nharder than new table textures alone, which is the harder of two factors to generalize to. This\nresult implies that we can study and address environment factors individually.\n\u2022 Random crop augmentation improves generalization even along non-spatial factors. We find that\nrandom crop augmentation is a lightweight way to improve generalization to spatial factors such\nas camera positions, but also to non-spatial factors such as distractor objects and table textures.\n\u2022 Visual diversity from out-of-domain data dramatically improves generalization. In our exper-\niments, we find that training on data from other tasks and domains like opening a fridge and\noperating a cereal dispenser can improve performance on picking an object from a tabletop.\n2\nRelated Work\nPrior efforts to address robotic generalization include diverse datasets, pretrained representations,\nand data augmentation, which we discuss below.\nDatasets and benchmarks. Existing robotics datasets exhibit rich diversity along multiple dimen-\nsions, including objects [21, 16, 17, 22], domains [16, 4, 17], and tasks [13, 14, 15]. However,\ncollecting high-quality and diverse data at scale is still an unsolved challenge, which motivates the\nquestion of how new data should be collected given its current cost. The goal of this study is to sys-\ntematically understand the challenges of generalization to new objects and domains2 and, through\nour findings, inform future data collection strategies. Simulation can also be a useful tool for un-\nderstanding the scaling relationship between data diversity and policy performance, as diversity in\nsimulation comes at a much lower cost [23, 24, 25, 26]. Many existing benchmarks aim to study\nexactly this [27, 28, 29, 30]; these benchmarks evaluate the generalization performance of control\npolicies to new tasks [27, 28] and environments [29, 30]. KitchenShift [30] is the most related to our\ncontribution Factor World, benchmarking robustness to shifts like lighting, camera view, and tex-\nture. However, Factor World contains a more complete set of factors (11 versus 7 in KitchenShift)\nwith many more configurations of each factor (over 100 versus fewer than 10 in KitchenShift).\nPretrained representations and data augmentation. Because robotics datasets are generally col-\nlected in fewer and less varied environments, prior work has leveraged the diversity found in large-\nscale datasets from other domains like static images from ImageNet [31], videos of humans from\nEgo4D [32], and natural language [6, 9, 8, 10, 12]. While these datasets do not feature a single\nrobot, pretraining representations on them can lead to highly efficient robotic policies with only a\nfew episodes of robot data [8, 12, 33]. A simpler yet effective way to improve generalization is to\napply image data augmentation techniques typically used in computer vision tasks [34]. Augmenta-\ntions like random shifts, color jitter, and rotations have been found beneficial in many image-based\nrobotic settings [1, 35, 2, 36, 3, 4]. While pretrained representations and data augmentations have\ndemonstrated impressive empirical gains in many settings, we seek to understand when and why\nthey help, through our factor decomposition of robotic environments.\nGeneralization in RL. Many generalization challenges found in the RL setting are shared by the\nimitation learning setting and vice versa. Common approaches in RL include data augmentation [1,\n2We define an environment to be the combination of the domain and its objects.\n2\n(a) Original\n(b) Light\n(c) Distractors\n(d) Table texture\n(e) Background\nFigure 2: Examples of our real robot evaluation environment. We systematically vary different environment\nfactors, including the lighting condition, distractor objects, table texture, background, and camera pose.\n(a) Light setup 1\n(b) Light setup 2\n(c) Original view\n(d) Test view 1\n(e) Test view 2\nFigure 3: (a-b) Our setup to evaluate changes in lighting. (c-e) The original and new camera views.\n35, 36], domain randomization [37, 38, 23], and modifications to the network architecture [28,\n39, 40]. We refer readers to Kirk et al. for a more thorough survey of challenges and solutions.\nNotably, Packer et al. also conducted a study to evaluate the generalization performance of RL\npolicies to new physical parameters, such as the agent\u2019s mass, in low-dimensional tasks [43]. Our\nwork also considers the effect of physical parameters, such as the table position, but because our\ntasks are solved from image-based observations, these changes to the environment are observed by\nthe agent. We instead evaluate the agent\u2019s ability to generalize to these observable changes.\n3\nEnvironment Factors\nSeveral prior works have studied the robustness of robotic policies to different environmental shifts,\nsuch as harsher lighting, new backgrounds, and new distractor objects [44, 30, 22, 45]. Many inter-\nesting observations have emerged from them, such as how mild lighting changes have little impact\non performance [44] and how new backgrounds (in their case, new kitchen countertops) have a big-\nger impact than new distractor objects [22]. However, these findings are often qualitative or lack\nspecificity. For example, the performance on a new kitchen countertop could be attributed to either\nthe appearance or the height of the new counter. A goal of our study is to formalize these prior\nobservations through systematic evaluations and to extend them with a more comprehensive and\nfine-grained set of environmental shifts. In the remainder of this section, we describe the environ-\nmental factors we evaluate and how we implement them in our study.\n3.1\nReal Robot Manipulation\nIn our real robot evaluations, we study the following factors: lighting condition, distractor objects,\nbackground, table texture, and camera pose. In addition to selecting factors that are specific and\ncontrollable, we also take inspiration from prior work, which has studied robustness to many of\nthese shifts [44, 30, 22], thus signifying their relevance in real-world scenarios.\nOur experiments are conducted with mobile manipulators. The robot has a right-side arm with seven\nDoFs, gripper with two fingers, mobile base, and head with integrated cameras. The environment,\nvisualized in Fig. 2a, consists of a cabinet top that serves as the robot workspace and an acrylic\nwall that separates the workspace and office background. To control the lighting condition in our\nevaluations, we use several bright LED light sources with different colored filters to create colored\nhues and new shadows (see Fig. 2b). We introduce new table textures and backgrounds by covering\nthe cabinet top and acrylic wall, respectively, with patterned paper. We also shift the camera pose by\n3\n(a) Pick Place\n(b) Bin Picking\n(c) Door (Open, Lock)\n(d) Basketball\n(e) Button (Top, Side, Wall)\nFigure 4: Factor World, a suite of 19 visually diverse robotic manipulation tasks. Each task can be configured\nwith multiple factors of variation such as lighting; texture, size, shape, and initial position of objects; texture of\nbackground (table, floor); position of the camera and table relative to the robot; and distractor objects.\nchanging the robot\u2019s head orientation (see Fig. 3 for the on-robot perspectives from different camera\nposes). Due to the practical challenges of studying factors like the table position and height, we\nreserve them for our simulated experiments.\n3.2\nFactor World\nWe implement the environmental shifts on top of the Meta World benchmark [27]. While Meta\nWorld is rich in diversity of control behaviors, it lacks diversity in the environment, placing the\nsame table at the same position against the same background. Hence, we implement 11 different\nfactors of variation, visualized in Fig. 4 and fully enumerated in Fig. 10. These include lighting;\ntexture, size, shape, and initial position of objects; texture of the table and background; the camera\npose and table position relative to the robot; the initial arm pose; and distractor objects. In our study,\nwe exclude the object size and shape, as an expert policy that can handle any object is more difficult\nto design, and the initial arm pose, as this can usually be fixed whereas the same control cannot be\nexercised over the other factors, which are inherent to the environment.\nTextures (table, floor, objects) are sampled from 162 texture images (81 for train, 81 for eval) and\ncontinuous RGB values in [0, 1]3. Distractor objects are sampled from 170 object meshes (100 for\ntrain, 70 for eval) in Google\u2019s Scanned Objects Dataset [46, 47]. For lighting, we sample continuous\nambient and diffuse values in [0.2, 0.8]. Positions (object, camera, table) are sampled from contin-\nuous ranges summarized in Table 1. We consider different-sized ranges to control the difficulty of\ngeneralization. While fixing the initial position of an object across trials is feasible with a simulator,\nit is generally difficult to precisely replace an object to its original position in physical setups. Thus,\nwe randomize the initial position of the object in each episode in the experiments.\n4\nStudy Design\nWe seek to understand how each environment factor described in Sec. 3 contributes to the difficulty\nof generalization. In our pursuit of an answer, we aim to replicate, to the best of our ability, the\nscenarios that robotics practitioners are likely to encounter in the real world. We therefore start by\nselecting a set of tasks commonly studied in the robotics literature and the data collection procedure\n(Sec. 4.1). Then, we describe the algorithms studied and our evaluation protocol (Sec. 4.2).\n4.1\nControl Tasks and Datasets\nReal robot manipulation. We study the language-conditioned manipulation problem from Brohan\net al., specifically, focusing on the \u201cpick\u201d skill for which we have the most data available. The\ngoal is to pick up the object specified in the language instruction. For example, when given the\ninstruction \u201cpick pepsi can\u201d, the robot should pick up the pepsi can among the distractor objects\nfrom the countertop (Fig. 2). We select six objects for our evaluation; all \u201cpick\u201d instructions can\nbe found in Fig. 9 in App. A.1. The observation consists of 300 \u00d7 300 RGB image observations\nfrom the last six time-steps and the language instruction, while the action controls movements of the\narm (xyz-position, roll, pitch, yaw, opening of the gripper) and movements of the base (xy-position,\nyaw). The actions are discretized along each of the 10 dimensions into 256 uniform bins. The real\nrobot manipulation dataset consists of over 115K human-collected demonstrations, collected across\n13 skills, with over 100 objects, three tables, and three locations. The dataset is collected with a\nfixed camera orientation but randomized initial base position in each episode.\n4\nFactor World. While Factor World consists of 19 robotic manipulation tasks, we focus our study\non three commonly studied tasks in robotics: pick-place (Fig. 4a), bin-picking (Fig. 4b), and\ndoor-open (Fig. 4c). In pick-place, the agent must move the block to the goal among a distractor\nobject placed in the scene. In bin-picking, the agent must move the block from the right-side bin\nto the left-side bin. In door-open, the agent must pull on the door handle. We use scripted expert\npolicies from the Meta World benchmark, which compute expert actions given the object poses, to\ncollect demonstrations in each simulated task. The agent is given 84 \u00d7 84 RBG image observations,\nthe robot\u2019s end-effector position from the last two time-steps, and the distance between the robot\u2019s\nfingers from the last two time-steps. The actions are the desired change in the 3D-position of the\nend-effector and whether to open or close the gripper.\n4.2\nAlgorithms and Evaluation Protocol\nThe real robot manipulation policy uses the RT-1 architecture [22], which tokenizes the images,\ntext, and actions, attends over these tokens with a Transformer [48], and trains with a language-\nconditioned imitation learning objective. In simulation, we equip vanilla behavior cloning with\nseveral different methods for improving generalization. Specifically, we evaluate techniques for im-\nage data augmentation (random crops and random photometric distortions) and evaluate pretrained\nrepresentations (CLIP [7] and R3M [12]) for encoding image observations. More details on the\nimplementation and training procedure can be found in App. A.2.\nEvaluation protocol. On the real robot task, we evaluate the policies on two new lighting condi-\ntions, three sets of new distractor objects, three new table textures, three new backgrounds, and two\nnew camera poses. For each factor of interest, we conduct two evaluation trials in each of the six\ntasks, and randomly shuffle the object and distractor positions between trials. We report the success\nrate averaged across the 12 trials. To evaluate the generalization behavior of the trained policies in\nFactor World, we shift the train environments by randomly sampling 100 new values for the factor\nof interest, creating 100 test environments. We report the average generalization gap, which is de-\nfined as PT \u2212 PF , where PT is the success rate on the train environments and PF is the new success\nrate under shifts to factor F. See App. A.1 for more details on our evaluation metrics.\n5\nExperimental Results\nIn our experiments, we aim to answer the following questions:\n\u2022 How much does each environment factor contribute to the generalization gap? (Sec. 5.1)\n\u2022 What effects do data augmentation and pretrained representations have on the generalization\nperformance? (Sec. 5.2)\n\u2022 How do different data collection strategies, such as prioritizing visual diversity in the data, impact\ndownstream generalization? (Sec. 5.3)\n5.1\nImpact of Environment Factors on Generalization\nIndividual factors. We begin our real robot evaluation by benchmarking the model\u2019s performance\non the set of six training tasks, with and without shifts. Without shifts, the policy achieves an average\nsuccess rate of 91.7%. Our results with shifts are presented in Fig. 6, as the set of green bars. We\nfind that the new backgrounds have little impact on the performance (88.9%), while new distractor\nobjects and new lighting conditions have a slight effect, decreasing success rate to 80.6% and 83.3%\nrespectively. Finally, changing the table texture and camera orientation causes the biggest drop, to\n52.8% and 45.8%, as the entire dataset uses a fixed head pose. Since we use the same patterned paper\nto introduce variations in backgrounds and table textures, we can make a direct comparison between\nthese two factors, and conclude that new textures are harder to generalize to than new backgrounds.\nFig. 5a compares the generalization gap due to each individual factor on Factor World. We plot this\nas a function of the number of training environments represented in the dataset, where an environ-\nment is parameterized by the sampled value for each factor of variation. For the continuous-valued\nfactors, camera position and table position, we sample from the \u201cNarrow\u201d ranges (see App. A.1 for\nthe exact range values). Consistent across simulated and real-world results, new backgrounds,\n5\n(a)\n(b)\n(c)\nFigure 5: (a) Generalization gap when shifts are introduced to individual factors in Factor World. (b) Gener-\nalization gap versus the radius of the range that camera and table positions are sampled from, in Factor World.\n(c) Performance on pairs of factors, reported as the percentage difference relative to the harder factor of the\npair, in Factor World. All results are averaged across the 3 simulated tasks with 5 seeds for each task. Error\nbars represent standard error of the mean.\nFigure 6: Performance of real-robot policies trained without data augmentation (blue), with random photo-\nmetric distortions (red), with random crops (yellow), and with both (green). The results discussed in Sec. 5.1\nare with \u201cBoth\u201d. \u201cOriginal\u201d is the success rate on train environments, \u201cBackground\u201d is the success rate when\nwe perturb the background, \u201cDistractors\u201d is where we replace the distractors with new ones, etc. Error bars\nrepresent standard error of the mean. We also provide the average over all 7 (sets of) factors on the far right.\ndistractors, and lighting are easier factors to generalize to, while new table textures and cam-\nera positions are harder. In Factor World, new backgrounds are harder than distractors and light-\ning, in contrast to the real robot results, where they were the easiest. This may be explained by the\nfact that the real robot dataset contains a significant amount of background diversity, relative to the\nlighting and distractor factors, as described in Sec. 4.1. In Factor World, we additionally study ob-\nject textures and table positions, including the height of the table. New object textures are about as\nhard to overcome as new camera positions, and new table positions are as hard as new table textures.\nFortunately, the generalization gap closes significantly for all factors, from a maximum gap of 0.4\nto less than 0.1, when increasing the number of training environments from 5 to 100.\nPairs of factors. Next, we evaluate performance with respect to pairs of factors to understand how\nthey interact, i.e., whether generalization to new pairs is harder (or easier) than generalizing to one\nof them. On the real robot, we study the factors with the most diversity in the training dataset: table\ntexture + distractors and table texture + background. Introducing new background textures or new\ndistractors on top of a new table texture does not make it any harder than the new table texture alone\n(see green bars in Fig. 6). The success rate with new table texture + new background is 55.6% and\nwith new table texture + new distractors is 50.0%, comparable to the evaluation with only new table\ntextures, which is 52.8%.\nIn Factor World, we evaluate all 21 pairs of the seven factors, and report with a different metric:\nthe success rate gap, normalized by the harder of the two factors. Concretely, this metric is defined\nas (PA+B \u2212 min(PA, PB)) / min(PA, PB), where PA is the success rate under shifts to factor A,\nPB is the success rate under shifts to factor B, and PA+B is the success rate under shifts to both.\nMost pairs of factors do not have a compounding effect on generalization performance. For 16\nout of the 21 pairs, the relative percentage difference in the success rate lies between \u22126% and 6%.\n6\nIn other words, generalizing to the combination of two factors is not significantly harder or easier\nthan the individual factors. In Fig. 5c, we visualize the performance difference for the remaining\n5 factor pairs that lie outside of this (\u22126%, 6%) range (see App. A.3 for the results for all factor\npairs). Interestingly, the following factors combine synergistically, making it easier to generalize to\ncompared to the (harder of the) individual factors: object texture + distractor and light + distractor.\nThis result suggests that we can study these factors independently of one another, and improvements\nwith respect to one factor may carry over to scenarios with multiple factor shifts.\nContinuous factors. The camera position and table position factors are continuous, unlike the other\nfactors which are discrete, hence the generalization gap with respect to these factors will depend on\nthe range that we train and evaluate on. We aim to understand how much more difficult training\nand generalizing to a wider range of values is, by studying the gap with the following range radii:\n0.025, 0.050, ad 0.075 meters. For both camera-position and table-position factors, as we linearly\nincrease the radius, the generalization gap roughly doubles (see Fig. 5b). This pattern suggests: (1)\nperformance can be dramatically improved by keeping the camera and table position as constant as\npossible, and (2) generalizing to wider ranges may require significantly more diversity, i.e., examples\nof camera and table positions in the training dataset. However, in Sec. 5.2, we see that existing\nmethods can address the latter issue to some degree.\n5.2\nEffect of Data Augmentation and Pretrained Representations\nFigure 7:\nGeneralization gap\nwith data augmentations and pre-\ntrained representations in Factor\nWorld. Lower is better. Results\nare averaged across the 7 factors,\n3 tasks, and 5 seeds for each task.\nThe impact of data augmentation under individual factor shifts.\nWe study two forms of augmentation: (1) random crops and (2) ran-\ndom photometric distortions. The photometric distortion randomly\nadjusts the brightness, saturation, hue, and contrast of the image,\nand applies random cutout and random Gaussian noise. Fig. 6 and\nFig. 7 show the results for the real robot and Factor World respec-\ntively. On the robot, crop augmentation improves generalization\nalong multiple environment factors, most significantly to new\ncamera positions and new table textures. While the improve-\nment on a spatial factor like camera position is intuitive, we find\nthe improvement on a non-spatial factor like table texture surpris-\ning. More in line with our expectations, the photometric distortion\naugmentation improves the performance on texture-based factors\nlike table texture in the real robot environment and object, table and\nbackground in the simulated environment (see App. A.3 for Factor\nWorld results by factor).\nThe impact of pretrained representations under individual factor shifts. We study two pre-\ntrained representations: (1) R3M [12] and (2) CLIP [7]. While these representations are trained on\nnon-robotics datasets, policies trained on top of them have been shown to perform well in robotics\nenvironments from a small amount of data. However, while they achieve good performance on\ntraining environments (see Fig. 13 in App. A.3), they struggle to generalize to new but similar envi-\nronments, leaving a large generalization gap across many factors (see Fig. 7). Though, CLIP does\nimprove upon a trained-from-scratch CNN with new object textures (Fig. 11; first row, fourth plot).\n5.3\nInvestigating Different Strategies for Data Collection\nAugmenting visual diversity with out-of-domain data. As described in Sec. 4.1, our real robot\ndataset includes demonstrations collected from other domains and tasks like opening a fridge and\noperating a cereal dispenser. Only 35.2% of the 115K demonstrations are collected in the same\ndomain as our evaluations. While the remaining demonstrations are out of domain and focus on\nother skills such as drawer manipulation, they add visual diversity, such as new objects and new\nbackgrounds, and demonstrate robotic manipulation behavior, unlike the data that R3M and CLIP\npretrain on. We consider the dataset with only in-domain data, which we refer to as In-domain\nonly. In Fig. 8, we compare In-domain only (blue) to the full dataset, which we refer to as With\nout-of-domain (full) (yellow). While the performance on the original six training tasks is\n7\nFigure 8: Performance of real-robot policies trained with in-domain data only (blue), a small version of the in-\nand out-of-domain dataset (red), and the full version of the in- and out-of-domain dataset (yellow). Error bars\nrepresent standard error of the mean. We also provide the average over all 7 (sets of) factors on the far right.\ncomparable, the success rate of the In-domain only policy drops significantly across the different\nenvironment shifts, and the With out-of-domain (full) policy is more successful across the\nboard. Unlike representations pretrained on non-robotics datasets (Sec. 5.2), out-of-domain\nrobotics data can improve in-domain generalization.\nPrioritizing visual diversity with out-of-domain data. Finally, we consider a uniformly sub-\nsampled version of the With out-of-domain (full) dataset, which we refer to as With\nout-of-domain (small). With out-of-domain (small) has the same number of demonstra-\ntions as In-domain only, allowing us to directly compare whether the in-domain data or out-of-\ndomain data is more valuable. We emphasize that With out-of-domain (small) has signifi-\ncantly fewer in-domain demonstrations of the \u201cpick\u201d skill than In-domain only. Intuitively, one\nwould expect the in-domain data to be more useful. However, in Fig. 8, we see that the With\nout-of-domain (small) policy (red) performs comparably with the In-domain only policy\n(blue) across most of the factors. The main exception is scenarios with new distractors, where the\nIn-domain only policy has a 75.0% success rate while the With out-of-domain (small) pol-\nicy is successful in 44.4% of the trials.\n6\nDiscussion\nSummary. In this work, we studied the impact of different environmental variations on generaliza-\ntion performance. We determined an ordering of the environment factors in terms of generalization\ndifficulty, that is consistent across simulation and our real robot setup, and quantified the impact of\ndifferent solutions like data augmentation. Notably, many of the solutions studied were developed\nfor computer vision tasks like image classification. While some of them transferred well to the\nrobotic imitation learning setting, it may be fruitful to develop algorithms that prioritize this setting\nand its unique considerations, including the sequential nature of predictions and the often continu-\nous, multi-dimensional action space in robotic setups. We hope this work encourages researchers to\ndevelop solutions that target the specific challenges in robotic generalization identified by our work.\nLimitations. There are limitations to our study, which focuses on a few, but representative, robotic\ntasks and environment factors in the imitation setting. Our real-robot experiments required conduct-\ning a total number of 1440 evaluations over all factor values, tasks, and methods, and it is challenging\nto increase the scope of the study because of the number of experiments required. Fortunately, fu-\nture work can utilize our simulated benchmark Factor World to study additional tasks, additional\nfactors, and generalization in the reinforcement learning setting. We also saw that the performance\non training environments slightly degrades as we trained on more varied environments (see Fig. 13\nin App. A.3). Based on this observation, studying higher-capacity models, such as those equipped\nwith ResNet or Vision Transformer architectures, which can likely fit these varied environments\nbetter, would be a fruitful next step.\n8\nAcknowledgments\nWe thank Yao Lu, Kaylee Burns, and Evan Liu for helpful discussions and feedback, and Brianna\nZitkovich and Jaspiar Singh for their assistance in the robot evaluations. This work was supported\nin part by ONR grants N00014-21-1-2685 and N00014-22-1-2621.\nReferences\n[1] M. Laskin, K. Lee, A. Stooke, L. Pinto, P. Abbeel, and A. Srinivas. Reinforcement learning\nwith augmented data. Advances in neural information processing systems, 33:19884\u201319895,\n2020.\n[2] D. Yarats, R. Fergus, A. Lazaric, and L. Pinto. Mastering visual continuous control: Improved\ndata-augmented reinforcement learning. arXiv preprint arXiv:2107.09645, 2021.\n[3] N. Hansen and X. Wang. Generalization in reinforcement learning by soft data augmentation.\nIn 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13611\u2013\n13617. IEEE, 2021.\n[4] S. Young, D. Gandhi, S. Tulsiani, A. Gupta, P. Abbeel, and L. Pinto. Visual imitation made\neasy. In Conference on Robot Learning, pages 1992\u20132005. PMLR, 2021.\n[5] C. Graf, D. B. Adrian, J. Weil, M. Gabriel, P. Schillinger, M. Spies, H. Neumann, and A. Kupc-\nsik. Learning dense visual descriptors using image augmentations for robot manipulation tasks.\narXiv preprint arXiv:2209.05213, 2022.\n[6] L. Yen-Chen, A. Zeng, S. Song, P. Isola, and T.-Y. Lin. Learning to see before learning to act:\nVisual pre-training for manipulation. In 2020 IEEE International Conference on Robotics and\nAutomation (ICRA), pages 7286\u20137293. IEEE, 2020.\n[7] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\n[8] A. Khandelwal, L. Weihs, R. Mottaghi, and A. Kembhavi. Simple but effective: Clip embed-\ndings for embodied ai. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 14829\u201314838, 2022.\n[9] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In Conference on Robot Learning, pages 894\u2013906. PMLR, 2022.\n[10] R. Shah and V. Kumar. Rrl: Resnet as representation for reinforcement learning. arXiv preprint\narXiv:2107.03380, 2021.\n[11] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of\npre-trained vision models for control. arXiv preprint arXiv:2203.03580, 2022.\n[12] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. arXiv preprint arXiv:2203.12601, 2022.\n[13] P. Sharma, L. Mohan, L. Pinto, and A. Gupta. Multiple interactions made easy (mime): Large\nscale demonstrations data for imitation. In Conference on robot learning, pages 906\u2013915.\nPMLR, 2018.\n[14] A. Mandlekar, Y. Zhu, A. Garg, J. Booher, M. Spero, A. Tung, J. Gao, J. Emmons, A. Gupta,\nE. Orbay, et al. Roboturk: A crowdsourcing platform for robotic skill learning through imita-\ntion. In Conference on Robot Learning, pages 879\u2013893. PMLR, 2018.\n9\n[15] A. Mandlekar, J. Booher, M. Spero, A. Tung, A. Gupta, Y. Zhu, A. Garg, S. Savarese, and\nL. Fei-Fei. Scaling robot supervision to hundreds of hours with roboturk: Robotic manipulation\ndataset through human reasoning and dexterity. In 2019 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 1048\u20131055. IEEE, 2019.\n[16] S. Dasari, F. Ebert, S. Tian, S. Nair, B. Bucher, K. Schmeckpeper, S. Singh, S. Levine, and\nC. Finn. Robonet: Large-scale multi-robot learning. arXiv preprint arXiv:1910.11215, 2019.\n[17] F. Ebert, Y. Yang, K. Schmeckpeper, B. Bucher, G. Georgakis, K. Daniilidis, C. Finn, and\nS. Levine. Bridge data: Boosting generalization of robotic skills with cross-domain datasets.\narXiv preprint arXiv:2109.13396, 2021.\n[18] D. Hendrycks and T. Dietterich. Benchmarking neural network robustness to common corrup-\ntions and perturbations. arXiv preprint arXiv:1903.12261, 2019.\n[19] D. Hendrycks, S. Basart, N. Mu, S. Kadavath, F. Wang, E. Dorundo, R. Desai, T. Zhu, S. Para-\njuli, M. Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution\ngeneralization. In Proceedings of the IEEE/CVF International Conference on Computer Vi-\nsion, pages 8340\u20138349, 2021.\n[20] R. Geirhos, K. Narayanappa, B. Mitzkus, T. Thieringer, M. Bethge, F. A. Wichmann, and\nW. Brendel. Partial success in closing the gap between human and machine vision. Advances\nin Neural Information Processing Systems, 34:23885\u201323899, 2021.\n[21] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,\nM. Kalakrishnan, V. Vanhoucke, et al. Scalable deep reinforcement learning for vision-based\nrobotic manipulation. In Conference on Robot Learning, pages 651\u2013673. PMLR, 2018.\n[22] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv\npreprint arXiv:2212.06817, 2022.\n[23] J. Tan, T. Zhang, E. Coumans, A. Iscen, Y. Bai, D. Hafner, S. Bohez, and V. Vanhoucke. Sim-\nto-real: Learning agile locomotion for quadruped robots. arXiv preprint arXiv:1804.10332,\n2018.\n[24] X. B. Peng, M. Andrychowicz, W. Zaremba, and P. Abbeel. Sim-to-real transfer of robotic\ncontrol with dynamics randomization. In 2018 IEEE international conference on robotics and\nautomation (ICRA), pages 3803\u20133810. IEEE, 2018.\n[25] Y. Chebotar, A. Handa, V. Makoviychuk, M. Macklin, J. Issac, N. Ratliff, and D. Fox. Closing\nthe sim-to-real loop: Adapting simulation randomization with real world experience. In 2019\nInternational Conference on Robotics and Automation (ICRA), pages 8973\u20138979. IEEE, 2019.\n[26] S. James, P. Wohlhart, M. Kalakrishnan, D. Kalashnikov, A. Irpan, J. Ibarz, S. Levine,\nR. Hadsell, and K. Bousmalis. Sim-to-real via sim-to-sim: Data-efficient robotic grasping via\nrandomized-to-canonical adaptation networks. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 12627\u201312637, 2019.\n[27] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A\nbenchmark and evaluation for multi-task and meta reinforcement learning. In Conference on\nrobot learning, pages 1094\u20131100. PMLR, 2020.\n[28] K. Cobbe, C. Hesse, J. Hilton, and J. Schulman. Leveraging procedural generation to bench-\nmark reinforcement learning. In International conference on machine learning, pages 2048\u2013\n2056. PMLR, 2020.\n[29] A. Stone, O. Ramirez, K. Konolige, and R. Jonschkowski. The distracting control suite\u2013a chal-\nlenging benchmark for reinforcement learning from pixels. arXiv preprint arXiv:2101.02722,\n2021.\n10\n[30] E. Xing, A. Gupta, S. Powers, and V. Dean. Kitchenshift: Evaluating zero-shot generaliza-\ntion of imitation-based policy learning under domain shifts. In NeurIPS 2021 Workshop on\nDistribution Shifts: Connecting Methods and Applications, 2021.\n[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy,\nA. Khosla, M. Bernstein, et al. Imagenet large scale visual recognition challenge. Interna-\ntional journal of computer vision, 115(3):211\u2013252, 2015.\n[32] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang,\nM. Liu, X. Liu, et al.\nEgo4d: Around the world in 3,000 hours of egocentric video.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n18995\u201319012, 2022.\n[33] Y. J. Ma, S. Sodhani, D. Jayaraman, O. Bastani, V. Kumar, and A. Zhang. Vip: Towards\nuniversal visual reward and representation via value-implicit pre-training.\narXiv preprint\narXiv:2210.00030, 2022.\n[34] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on machine learning, pages 1597\u20131607.\nPMLR, 2020.\n[35] I. Kostrikov, D. Yarats, and R. Fergus. Image augmentation is all you need: Regularizing deep\nreinforcement learning from pixels. arXiv preprint arXiv:2004.13649, 2020.\n[36] N. Hansen, H. Su, and X. Wang. Stabilizing deep q-learning with convnets and vision trans-\nformers under data augmentation. Advances in neural information processing systems, 34:\n3680\u20133693, 2021.\n[37] A. Rajeswaran, S. Ghotra, B. Ravindran, and S. Levine. Epopt: Learning robust neural network\npolicies using model ensembles. arXiv preprint arXiv:1610.01283, 2016.\n[38] L. Pinto, J. Davidson, R. Sukthankar, and A. Gupta. Robust adversarial reinforcement learning.\nIn International Conference on Machine Learning, pages 2817\u20132826. PMLR, 2017.\n[39] R. Raileanu and R. Fergus. Decoupling value and policy for generalization in reinforcement\nlearning. In International Conference on Machine Learning, pages 8787\u20138798. PMLR, 2021.\n[40] E. Cetin, P. J. Ball, S. Roberts, and O. Celiktutan. Stabilizing off-policy deep reinforcement\nlearning from pixels. arXiv preprint arXiv:2207.00986, 2022.\n[41] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt\u00a8aschel. A survey of generalisation in deep\nreinforcement learning. arXiv preprint arXiv:2111.09794, 2021.\n[42] C. Packer, K. Gao, J. Kos, P. Kr\u00a8ahenb\u00a8uhl, V. Koltun, and D. Song. Assessing generalization in\ndeep reinforcement learning. arXiv preprint arXiv:1810.12282, 2018.\n[43] E. Todorov, T. Erez, and Y. Tassa. Mujoco: A physics engine for model-based control. In 2012\nIEEE/RSJ international conference on intelligent robots and systems, pages 5026\u20135033. IEEE,\n2012.\n[44] R. Julian, B. Swanson, G. S. Sukhatme, S. Levine, C. Finn, and K. Hausman. Never stop\nlearning: The effectiveness of fine-tuning in robotic reinforcement learning. arXiv preprint\narXiv:2004.10190, 2020.\n[45] G. Zhou, V. Dean, M. K. Srirama, A. Rajeswaran, J. Pari, K. B. Hatch, A. Jain, T. Yu, P. Abbeel,\nL. Pinto, et al. Train offline, test online: A real robot learning benchmark. In Deep Reinforce-\nment Learning Workshop NeurIPS 2022.\n[46] K. Zakka.\nScanned Objects MuJoCo Models, 7 2022.\nURL https://github.com/\nkevinzakka/mujoco_scanned_objects.\n11\n[47] L. Downs, A. Francis, N. Koenig, B. Kinman, R. Hickman, K. Reymann, T. B. McHugh, and\nV. Vanhoucke. Google scanned objects: A high-quality dataset of 3d scanned household items,\n2022. URL https://arxiv.org/abs/2204.11918.\n[48] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polo-\nsukhin. Attention is all you need. Advances in neural information processing systems, 30,\n2017.\n[49] D. Cer, Y. Yang, S.-y. Kong, N. Hua, N. Limtiaco, R. S. John, N. Constant, M. Guajardo-\nCespedes, S. Yuan, C. Tar, et al. Universal sentence encoder. arXiv preprint arXiv:1803.11175,\n2018.\n[50] M. Tan and Q. Le. EfficientNet: Rethinking model scaling for convolutional neural networks.\nIn K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th International Con-\nference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,\npages 6105\u20136114. PMLR, 09\u201315 Jun 2019. URL https://proceedings.mlr.press/v97/\ntan19a.html.\n[51] M. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova. Tokenlearner: Adaptive\nspace-time tokenization for videos. Advances in Neural Information Processing Systems, 34:\n12786\u201312797, 2021.\n[52] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,\nM. Kalakrishnan, V. Vanhoucke, et al. Scalable deep reinforcement learning for vision-based\nrobotic manipulation. In Conference on Robot Learning, pages 651\u2013673. PMLR, 2018.\n[53] R. Sekar, O. Rybkin, K. Daniilidis, P. Abbeel, D. Hafner, and D. Pathak. Planning to explore\nvia self-supervised world models. In International Conference on Machine Learning, pages\n8583\u20138592. PMLR, 2020.\n12\nA\nAppendix\nA.1\nExperimental Details\nIn this section, we provide additional details on the experimental setup and evaluation metrics.\nA.1.1\nExperimental Setup\nReal robot tasks. We define six real-world picking tasks: pepsi can, water bottle, blue chip bag,\ngreen jalapeno chip bag, and oreo, which are visualized in Fig. 9.\nFactor World. The factors of variation implemented into Factor World are enumerated in Fig. 10. In\nTable 1, we specify the ranges of the continuous-valued factors.\n(a) Pepsi, water bottle\n(b) Blue chip bag\n(c) Blue plastic bottle\n(d) Green jalapeno chip bag\n(e) Oreo\nFigure 9: The six pick tasks in our real robot evaluations.\nFactor\nParameters Narrow\nMedium\nWide\nObject position\nX-position\n[\u22120.05, 0.05]\n[\u22120.1, 0.1]\n-\nY-position\n[\u22120.05, 0.05]\n[\u22120.075, 0.075] -\nCamera position\nX-position\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nY-position\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nZ-position\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nq1\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nq2\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nq3\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nq4\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nTable position\nX-position\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nY-position\n[\u22120.025, 0.025] [\u22120.05, 0.05]\n[\u22120.075, 0.075]\nZ-position\n[\u22120.025, 0.025] [\u22120.05, 0.025]\n[\u22120.05, 0.025]\nTable 1: Range for each continuous factor in meters. As a point of comparison for the position-based factors,\nthe table in the environment measures at 0.7m \u00d7 0.4m.\nA.1.2\nDataset Details\nFactor World datasets. In the pick-place task, we collect datasets of 2000 demonstrations, across\nN = 5, 20, 50, 100 training environments. A training environment is parameterized by a collection\nof factor values, one for each environment factor. We collect datasets of 1000 demonstrations for\nbin-picking and door-open, which we empirically found to be easier than the pick-place task.\nA.1.3\nEvaluation Metrics\nGeneralization gap. Besides the success rate, we also measure the generalization gap which is\ndefined as the difference between the performance on the train environments and the performance\non the test environments. The test environments have the same setup as the train environments,\nexcept 1 (or 2 in the factor pair experiments) of the factors is assigned a new value. For example,\nin Fig. 1, \u2018Background\u2019 represents the change in success rate when introducing new backgrounds to\nthe train environments.\n13\n(a) Object position\n(b) Initial arm position\n(c) Camera position\n(d) Table position\n(e) Object size\n(f) Object texture\n(g) Distractor objects & positions\n(h) Floor texture\n(i) Table texture\n(j) Lighting\nFigure 10: The 11 factors of variation implemented into Factor World, depicted for the pick-place environ-\nment. Videos are available at: https://sites.google.com/view/factor-envs\nPercentage difference.\nWhen evaluating a pair of factors, we report the percentage differ-\nence with respect to the harder of the two factors.\nConcretely, this metric is computed as\n(pA+B \u2212 min(pA, pB)) / min(pA, pB), where pA is the success rate under shifts to factor A, pA\nis the success rate under shifts to factor B, and pA+B is the success rate under shifts to both.\nA.2\nImplementation and Training Details\nIn this section, we provide additional details on the implementation and training of all models.\nA.2.1\nRT-1\nBehavior cloning. We follow the RT-1 architecture that uses tokenized image and language inputs\nwith a categorical cross-entropy objective for tokenized action outputs. The model takes as in-\nput a natural language instruction along with the 6 most recent RGB robot observations, and then\nfeeds these through pre-trained language and image encoders (Universal Sentence Encoder [49] and\nEfficientNet-B3 [50], respectively). These two input modalities are fused with FiLM conditioning,\nand then passed to a TokenLearner [51] spatial attention module to reduce the number of tokens\nneeded for fast on-robot inference. Then, the network contains 8 decoder only self-attention Trans-\n14\nformer layers, followed by a dense action decoding MLP layer. Full details of the RT-1 architecture\nthat we follow can be found in [22].\nData augmentations. Following the image augmentations introduced in Qt-Opt [52], we perform\ntwo main types of visual data augmentation during training only: visual disparity augmentations\nand random cropping. For visual disparity augmentations, we adjust the brightness, contrast, and\nsaturation by sampling uniformly from [-0.125, 0.125], [0.5, 1.5], and [0.5, 1.5] respectively. For\nrandom cropping, we subsample the full-resolution camera image to obtain a 300 \u00d7 300 random\ncrop. Since RT-1 uses a history length of 6, each timestep is randomly cropped independently.\nPretrained representations. Following the implementation in RT-1, we utilize an EfficientNet-B3\nmodel pretrained on ImageNet [50] for image tokenization, and the Universal Sentence Encoder [49]\nlanguage encoder for embedding natural language instructions. The rest of the RT-1 model is ini-\ntialized from scratch.\nA.2.2\nFactor World\nBehavior cloning. Our behavior cloning policy is parameterized by a convolutional neural network\nwith the same architecture as in [53] and in [30]: there are four convolutional layers with 32, 64,\n128, and 128 4 \u00d7 4 filters, respectively. The features are then flattened and passed through a linear\nlayer with output dimension of 128, LayerNorm, and Tanh activation function. The policy head is\nparameterized as a three-layer feedforward neural network with 256 units per layer. All policies are\ntrained for 100 epochs.\nData augmentations. In our simulated experiments, we experiment with shift augmentations (anal-\nogous to the crop augmentations the real robot policy trains with) from [2]: we first pad each side\nof the 84 \u00d7 84 image by 4 pixels, and then select a random 84 \u00d7 84 crop. We also experiment\nwith color jitter augmentations (analogous to the photometric distortions studied for the real robot\npolicy), which is implemented in torchvision. The brightness, contrast, saturation, and hue factors\nare set to 0.2. The probability that an image in the batch is augmented is 0.3. All policies are trained\nfor 100 epochs.\nPretrained representations. We use the ResNet50 versions of the publicly available R3M and CLIP\nrepresentations. We follow the embedding with a BatchNorm, and the same policy head parameter-\nization: three feedforward layers with 256 units per layer. All policies are trained for 100 epochs.\nA.3\nAdditional Experimental Results in Factor World\nIn this section, we provide additional results from our simulated experiments in Factor World.\nA.3.1\nSimulation: Data Augmentation and Pretrained Representations\nWe report the performance of the data augmentation techniques and pretrained representations by\nfactor in Fig. 11. The policy trained with the R3M representation fails to generalize well to most\nfactors of variation, with the exception of new distractors. We also see that the policy trained with\nthe CLIP representation performs similarly to the model trained from scratch (CNN) across most\nfactors, except for new object textures on which CLIP outperforms the naive CNN.\nA.3.2\nSimulation: Factor Pairs\nIn Fig. 12, we report the results for all factor pairs, a partial subset of which was visualized in Fig. 5c.\nIn Fig. 5c, we selected the pairs with the highest magnitude percentage difference, excluding the\npairs with error bars that overlap with zero.\nA.3.3\nSimulation: Success Rates\nIn Fig. 13, we report the performance of policies trained with data augmentations and with pretrained\nrepresentations, in terms of raw success rates. We find that for some policies, the performance on the\ntrain environments (see \u201cOriginal\u201d) degrades as we increase the number of training environments.\n15\nFigure 11: Generalization gap for different data augmentations and pretrained representations in Factor World.\nSubplots share the same x- and y-axes. Results are averaged across the 3 simulated tasks with 5 seeds for each\ntask. Error bars represent standard error of the mean.\nFigure 12: Generalization gap on all pairs of factors, reported as the percentage difference relative to the\nharder factor of the pair. Results are averaged across the 3 simulated tasks with 5 seeds for each task.\nNonetheless, as we increase the number of training environments, we see higher success rates on\nthe factor-shifted environments. However, it may be possible to see even more improvements in the\nsuccess rate with larger-capacity models that fit the training environments better.\n16\nFigure 13: Success rates of simulated policies with data augmentations and with pretrained representations.\nResults are averaged over the 3 simulated tasks, with 5 seeds run for each task.\n17\n"
  },
  {
    "title": "One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention",
    "link": "https://arxiv.org/pdf/2307.03576.pdf",
    "upvote": "5",
    "text": "arXiv:2307.03576v1  [cs.LG]  7 Jul 2023\nOne Step of Gradient Descent is Provably the Optimal In-Context\nLearner with One Layer of Linear Self-Attention\nArvind Mahankali\nStanford University\namahanka@stanford.edu\nTatsunori B. Hashimoto\nStanford University\nthashim@stanford.edu\nTengyu Ma\nStanford University\ntengyuma@stanford.edu\nJuly 10, 2023\nAbstract\nRecent works have empirically analyzed in-context learning and shown that transformers trained on\nsynthetic linear regression tasks can learn to implement ridge regression, which is the Bayes-optimal\npredictor, given su\ufb03cient capacity [Aky\u00fcrek et al., 2023], while one-layer transformers with linear self-\nattention and no MLP layer will learn to implement one step of gradient descent (GD) on a least-squares\nlinear regression objective [von Oswald et al., 2022].\nHowever, the theory behind these observations\nremains poorly understood. We theoretically study transformers with a single layer of linear self-attention,\ntrained on synthetic noisy linear regression data. First, we mathematically show that when the covariates\nare drawn from a standard Gaussian distribution, the one-layer transformer which minimizes the pre-\ntraining loss will implement a single step of GD on the least-squares linear regression objective. Then,\nwe \ufb01nd that changing the distribution of the covariates and weight vector to a non-isotropic Gaussian\ndistribution has a strong impact on the learned algorithm: the global minimizer of the pre-training loss\nnow implements a single step of pre-conditioned GD. However, if only the distribution of the responses\nis changed, then this does not have a large e\ufb00ect on the learned algorithm: even when the response\ncomes from a more general family of nonlinear functions, the global minimizer of the pre-training loss\nstill implements a single step of GD on a least-squares linear regression objective.\n1\nIntroduction\nLarge language models (LLMs) demonstrate the surprising ability of in-context learning, where an LLM\n\u201clearns\u201d to solve a task by conditioning on a prompt containing input-output exemplars [Brown et al., 2020,\nLieber et al., 2021, Radford et al., 2019, Wang and Komatsuzaki, 2021]. Recent works have advanced the\nunderstanding of in-context learning via empirical analysis [Min et al., 2022, Wei et al., 2023, Aky\u00fcrek et al.,\n2023, von Oswald et al., 2022, Dai et al., 2023], but theoretical analysis remains limited [Xie et al., 2022].\nA recent line of work [Garg et al., 2022, Aky\u00fcrek et al., 2023, von Oswald et al., 2022, Dai et al., 2023]\nempirically \ufb01nds that transformers can be trained to implement algorithms that solve linear regression\nproblems in-context. Speci\ufb01cally, in each input sequence the transformer is given a set of in-context examples\n(xi, yi), where yi = w\u22a4xi + \u01ebi with a shared and hidden random coe\ufb03cient vector w and random noise \u01ebi,\nand a test example x.1 The transformer is then trained to predict y = w\u22a4x + \u01eb, where \u01eb denotes random\nnoise from the same distribution as \u01ebi.\nThese works \ufb01nd that the transformer outputs a prediction \u02c6y\nwhich is similar to the predictions of existing, interpretable linear regression algorithms, such as gradient\ndescent (GD) or ordinary least squares, applied to the dataset consisting of the pairs (xi, yi). In particular,\nvon Oswald et al. [2022] empirically show that a one-layer transformer with linear self-attention and no MLP\nlayer will implement a single step of gradient descent when trained on such a distribution.\n1In some settings in these works, the noise is set to 0.\n1\nSeveral works (e.g. Aky\u00fcrek et al. [2023], Liu et al. [2023], Giannou et al. [2023]) theoretically study the\nexpressive power of transformers. In the context of linear regression tasks, Aky\u00fcrek et al. [2023] describe\nhow transformers can represent gradient descent, or Sherman-Morrison updates, and Giannou et al. [2023]\ndescribe how transformers can represent Newton\u2019s algorithm for matrix inversion. However, in addition\nto the expressive power of transformers, it is also of interest to understand the behavior of transformers\ntrained with gradient-based algorithms. Furthermore, it is still useful to understand the behavior of models\nwith restricted capacity\u2014though practical LLMs are very expressive, they need to perform many tasks\nsimultaneously, and therefore the capacity per problem may still be relatively limited. Thus, motivated\nby von Oswald et al. [2022], we theoretically study the global minima of the pre-training loss for one-layer\ntransformers with linear self-attention on the linear regression data distribution described above.\nContributions. In this paper, we study transformers with one linear self-attention layer, and mathemat-\nically investigate which algorithms the transformers implement for synthetically generated linear regression\ndatasets.\nWe prove that the transformer which implements a single step of gradient descent on a least\nsquares linear regression objective is the global minimizer of the pre-training loss. This exactly matches the\nempirical \ufb01ndings of von Oswald et al. [2022].\nConcretely, we consider a setup similar to von Oswald et al. [2022], Aky\u00fcrek et al. [2023]. The model we\nstudy is a transformer with one linear single-head self-attention layer, which is the same model as the one\nempirically studied by von Oswald et al. [2022]. The training data for this transformer consist of sequences\nof the form (x1, y1, . . . , xn, yn), where the xi are sampled from N(0, Id\u00d7d) and yi = w\u22a4xi + \u01ebi, where w\nis sampled from N(0, Id\u00d7d) once per sequence, and the \u01ebi are i.i.d. Gaussian noise with variance \u03c32. The\npre-training loss is the expected error that the transformer achieves when predicting y = w\u22a4x given the test\nexample x and the context (x1, y1, . . . , xn, yn), i.e. the pre-training loss is L = E(x1,y1,...,xn,yn),x,y[(y \u2212 \u02c6y)2],\nwhere \u02c6y is the output of the transformer given (x1, y1, . . . , xn, yn) and x as input.\nWe show in Section 3 that the transformer which is the global minimizer of the pre-training loss L\nimplements one step of gradient descent on a linear regression objective with the dataset consisting of the\n(xi, yi). More concretely, the transformer implements the prediction algorithm\n\u02c6y = \u03b7\nn\nX\ni=1\nyix\u22a4\ni x .\n(1)\nHowever, one step of GD is also preferred in part due to the distribution of the xi. In particular, if the\ncovariance of xi is no longer the identity matrix, we show (Section 4) that the global minimum of the\npre-training loss corresponds to one step of GD, but with pre-conditioning.\nMoreover, interestingly, our theory also suggests that the distribution of yi|xi does not play such a\nsigni\ufb01cant role in the algorithm learned by the transformer. In Section 5, we study a setting where yi|xi\nis nonlinear, but satis\ufb01es some mild assumptions, such as invariance to rotations of the distribution of the\nxi. As a concrete special case, the target function can be a neural network with any depth/width and i.i.d.\nrandom Gaussian weights. We show in Section 5 that a one-layer transformer with linear self-attention,\nwhich minimizes the pre-training loss, still implements one step of GD on a linear regression objective.\nIntuitively, this is likely because of the constraint imposed by the architecture, which prevents the transformer\nfrom making use of any more complex structure in the yi.\nConcurrent Works. We discuss the closely related works of Ahn et al. [2023] and Zhang et al. [2023]\nwhich are concurrent with and independent with our work and were posted prior to our work on arXiv. The\nwork of Ahn et al. [2023] gives theoretical results very similar to ours. They study one-layer transformers\nwith linear self-attention with the same parameterization as von Oswald et al. [2022], and show that with\nisotropic xi, the global minimizer of the pre-training loss corresponds to one step of gradient descent on a\nlinear model. They also show that for more general covariance matrices, the global minimizer of the pre-\ntraining loss corresponds to one step of pre-conditioned gradient descent, where the pre-conditioner matrix\ncan be computed in terms of the covariance of xi.2\n2This result is not exactly the same as our result in Section 4, since we assume w \u223c N (0, \u03a3\u22121) while they assume w \u223c\nN (0, Id\u00d7d).\n2\nDi\ufb00erent from our work, Ahn et al. [2023] also show additional results for multi-layer transformers (with\nlinear self-attention) with residual connections trained on linear regression data. First, they study a re-\nstricted parameterization where in each layer, the product of the projection and value matrices has only\none nonzero entry. In this setting, for two-layer transformers with linear self-attention, they show that the\nglobal minimizer corresponds to two steps of pre-conditioned GD with diagonal pre-conditioner matrices,\nwhen the data is isotropic. For linear transformers with k layers, they show that k steps of pre-conditioned\nGD corresponds to a critical point of the pre-training loss,3 where the pre-conditioner matrix is the inverse\nof the covariance matrix of the xi.4 Next, they study a less restrictive parameterization where the product\nof the projection and value matrices can be almost fully dense, and show that a certain critical point of the\npre-training loss for k-layer linear transformers corresponds to k steps of a generalized version of the GD++\nalgorithm, which was empirically observed by von Oswald et al. [2022] to be the algorithm learned by k-layer\nlinear transformers.\nZhang et al. [2023] also theoretically study a setting similar to ours. They not only show that the global\nminimizer of the pre-training loss implements one step of GD (the same result as ours), but also show that\na one-layer linear transformer trained with gradient \ufb02ow will converge to a global minimizer. They also\nshow that the transformer implements a step of pre-conditioned GD when the xi are non-isotropic. They\nalso characterize how the training prompt lengths and test prompt length a\ufb00ect the test-time prediction\nerror of the trained transformer. Additionally, they consider the behavior of the trained transformer under\ndistribution shifts, as well as the training dynamics when the covariance matrices of the xi in di\ufb00erent\ntraining prompts can be di\ufb00erent.\nOne additional contribution of our work is that we also consider the case where the target function in the\npre-training data is not a linear function (Section 5). This suggests that, compared to the distribution of\nthe covariates, the distribution of the responses at training time does not have as strong of an e\ufb00ect on the\nalgorithm learned by the transformer. We note that our proof in this setting is not too di\ufb00erent from our\nproof in Section 3. Zhang et al. [2023] consider the case where the yi\u2019s in the test time prompt are obtained\nfrom a nonlinear target function, and consider the performance on this prompt of the transformer trained\non prompts with a linear target function \u2014 this is di\ufb00erent from our setting in Section 5 since we consider\nthe case where the training prompts themselves are obtained with a nonlinear target function.\n2\nSetup\nOur setup is similar to von Oswald et al. [2022].\nOne-Layer Transformer with Linear Self-Attention. A linear self-attention layer with width s\nconsists of the following parameters: a key matrix WK \u2208 Rs\u00d7s, a query matrix WQ \u2208 Rs\u00d7s, and a value\nmatrix WV \u2208 Rs\u00d7s. Given a sequence of T > 1 tokens (v1, v2, . . . , vT ), the output of the linear self-attention\nlayer is de\ufb01ned to be (\u02c6v1, \u02c6v2, . . . , \u02c6vT ), where for i \u2208 [T ] with i > 1,\n\u02c6vi =\ni\u22121\nX\nj=1\n(WV vj)(v\u22a4\nj W \u22a4\nKWQvi) ,\n(2)\nand \u02c6v1 = 0. In particular, the output on the T th token is\n\u02c6vT =\nT \u22121\nX\nj=1\n(WV vj)(v\u22a4\nj W \u22a4\nKWQvT ) .\n(3)\nAs in the theoretical construction of von Oswald et al. [2022], we do not consider the attention score between\na token vi and itself. Our overall transformer is then de\ufb01ned to be a linear self-attention layer with key matrix\n3One technical point is that they show there exist transformers representing this form of pre-conditioned GD having arbitrarily\nsmall gradient, but not that there exists a transformer with gradient exactly 0 which represents this form of pre-conditioned\nGD.\n4Here, they assume that xi \u223c N (0, \u03a3) and w \u223c N (0, \u03a3\u22121), which is the same assumption as our result in Section 4 but\ndi\ufb00erent from their result for one-layer transformers where xi \u223c N (0, \u03a3).\n3\nWK, query matrix WQ, and value matrix WV , together with a linear head h \u2208 Rs which is applied to the last\ntoken. Thus, the \ufb01nal output of the transformer is h\u22a4\u02c6vT . We will later instantiate this one-layer transformer\nwith s = d + 1, where d is the dimension of the inputs xi. We note that this corresponds to a single head of\nlinear self-attention, while one could also consider multi-head self-attention.\nLinear Regression Data Distribution. The pretraining data distribution consists of sequences D =\n(x1, y1, . . . , xn+1, yn+1). Here, the exemplars xi are sampled i.i.d. from N(0, Id\u00d7d). Then, a weight vector\nw \u2208 Rd is sampled from N(0, Id\u00d7d), freshly for each sequence. Finally, yi is computed as yi = w\u22a4xi + \u01ebi\nwhere \u01ebi \u223c N(0, \u03c32) for some \u03c3 > 0. We consider the vector vi =\n\u0014\nxi\nyi\n\u0015\n\u2208 Rd+1 to be a token \u2014 in other\nwords, the sequence (x1, y1, . . . , xn+1, yn+1) is considered to have n + 1 tokens (rather than 2(n + 1) tokens).\nWe use T to denote the distribution of sequences de\ufb01ned in this way.\nAt both training and test time, (x1, y1, . . . , xn, yn, xn+1, yn+1) is generated according to the pretraining\ndistribution T , i.e. the xi are sampled i.i.d. from N(0, Id\u00d7d), a new weight vector w \u2208 Rd is also sampled from\nN(0, Id\u00d7d), and yi = w\u22a4xi + \u01ebi where the \u01ebi are sampled i.i.d. from N(0, \u03c32). Then, the in-context learner\nis presented with x1, y1, . . . , xn, yn, xn+1, and must predict yn+1.\nWe refer to x1, . . . , xn as the support\nexemplars and xn+1 as the query exemplar.\nHere, v1, . . . , vn are de\ufb01ned as above, but vn+1 =\n\u0014 xi\n0\n\u0015\n,\nfollowing the notation of von Oswald et al. [2022].5 We note that this is not signi\ufb01cantly di\ufb00erent from the\nstandard in-context learning setting, since even though the \ufb01nal token vn+1 has 0 as an extra coordinate, it\ndoes not provide the transformer with any additional information about yn+1.\nLoss Function. Given a one-layer transformer with linear self-attention and width d+1, with key matrix\nWK \u2208 R(d+1)\u00d7(d+1), query matrix WQ \u2208 R(d+1)\u00d7(d+1), and value matrix WV \u2208 R(d+1)\u00d7(d+1), and with a\nhead h \u2208 Rd+1, the loss of this transformer on our linear regression data distribution is formally de\ufb01ned as\nL(WK, WQ, WV , h) = ED\u223cT [(h\u22a4\u02c6vn+1 \u2212 yn+1)2] ,\n(4)\nwhere as de\ufb01ned above, \u02c6vn+1 is the output of the linear self-attention layer on the (n + 1)th token, which in\nthis case is\n\u0014\nxn+1\n0\n\u0015\n.\nWe now rewrite the loss function and one-layer transformer in a more convenient form. As a convenient\nshorthand, for any test-time sequence D = (x1, y1, . . . , xn+1, 0), we write eD = (x1, y1, . . . , xn, yn), i.e. the\npre\ufb01x of D that does not include xn+1 and yn+1. We also de\ufb01ne\nG e\nD =\nn\nX\ni=1\n\u0014 xi\nyi\n\u0015 \u0014 xi\nyi\n\u0015\u22a4\n.\n(5)\nWith this notation, we can write the prediction obtained from the transformer on the \ufb01nal token as\n\u02c6yn+1 = h\u22a4WV G e\nDW \u22a4\nKWQvn+1 .\n(6)\nwhere vn+1 =\n\u0014 xn+1\n0\n\u0015\n. Additionally, we also de\ufb01ne the matrix X \u2208 Rn\u00d7d as the matrix whose ith row is\nthe row vector x\u22a4\ni , i.e.\nX =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\n\u00b7 \u00b7 \u00b7\nx\u22a4\n1\n\u00b7 \u00b7 \u00b7\n\u00b7 \u00b7 \u00b7\nx\u22a4\n2\n\u00b7 \u00b7 \u00b7\n...\n...\n...\n\u00b7 \u00b7 \u00b7\nx\u22a4\nn\n\u00b7 \u00b7 \u00b7\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb ,\n(7)\n5If we were to treat xi and yi as separate tokens, then we would need to deal with attention scores between yi and yj for\ni \u0338= j, as well as attention scores between yi and xj for i \u0338= j. Our current setup simpli\ufb01es the analysis.\n4\nand we de\ufb01ne the vector \u20d7y \u2208 Rn as the vector whose ith entry is yi, i.e.\n\u20d7y =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ny1\ny2\n...\nyn\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb .\n(8)\nFinally, it is worth noting that we can write the loss function as\nL(WK, WQ, WV , h) = ED\u223cT [(h\u22a4WV G e\nDW \u22a4\nKWQvn+1 \u2212 yn+1)2] .\n(9)\nThus, for w \u2208 Rd+1 and M \u2208 R(d+1)\u00d7(d+1), if we de\ufb01ne\nL(w, M) = ED\u223cT [(w\u22a4G e\nDMvn+1 \u2212 yn+1)2] ,\n(10)\nthen L(W \u22a4\nV h, W \u22a4\nKWQ) = L(WK, WQ, WV , h). Note that we have a slight abuse of notation, and L has two\ndi\ufb00erent meanings depending on the number of arguments. Finally, with the change of variables M = W \u22a4\nKWQ\nand w = W \u22a4\nV h, we can write the prediction of the transformer as w\u22a4G e\nDMvn+1. Thus, the output of the\ntransformer only depends on the parameters through w\u22a4G e\nDM.\nAdditional Notation. For a matrix A \u2208 Rd\u00d7d, we write Ai:j,: to denote the sub-matrix of A that\ncontains the rows of A with indices between i and j (inclusive). Similarly, we write A:,i:j to denote the\nsub-matrix of A that contains the columns of A with indices between i and j (inclusive). We write Ai:j,k:l to\ndenote the sub-matrix of A containing the entries with row indices between i and j (inclusive) and column\nindices between k and l (inclusive).\n3\nMain Result for Linear Models\nTheorem 1 (Global Minimum for Linear Regression Data). Suppose (W \u2217\nK, W \u2217\nQ, W \u2217\nV , h\u2217) is a global minimizer\nof the loss L. Then, the corresponding one-layer transformer with linear self-attention implements one step\nof gradient descent on a linear model with some learning rate \u03b7 > 0. More concretely, given a query token\nvn+1 =\n\u0014 xn+1\n0\n\u0015\n, the transformer outputs \u03b7 Pn\ni=1 yix\u22a4\ni xn+1, where \u03b7 =\nEf\nD\u223cT [ \u02c6w\u22a4\nf\nDX\u22a4\u20d7y]\nEf\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y]. Here given a pre\ufb01x\neD of a test-time data sequence D, we let \u02c6w e\nD denote the solution to ridge regression on X and \u20d7y with\nregularization strength \u03c32.\nOne such construction is as follows. von Oswald et al. [2022] describe essentially the same construction,\nbut our result shows that it is a global minimum of the loss function, while von Oswald et al. [2022] do not\ntheoretically study the construction aside from showing that it is equivalent to one step of gradient descent.\nWe de\ufb01ne\nW \u2217\nK =\n\u0012Id\u00d7d\n0\n0\n0\n\u0013\n, W \u2217\nQ =\n\u0012Id\u00d7d\n0\n0\n0\n\u0013\n, W \u2217\nV =\n\u00120\n0\n0\n\u03b7\n\u0013\n, h\u2217 =\n\u0014 0\n1\n\u0015\n.\n(11)\nHere, the unique value of \u03b7 which makes this construction a global minimum is \u03b7 =\nEf\nD\u223cT [ \u02c6\nw\u22a4\nf\nDX\u22a4\u20d7y]\nEf\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y].\nTo see why this construction implements a single step of gradient descent on a linear model, note that\ngiven test time inputs x1, y1, . . . , xn, yn, xn+1, if we write vi =\n\u0014 xi\nyi\n\u0015\nfor i \u2264 n and vn+1 =\n\u0014 xn+1\n0\n\u0015\n, then\nthe output of the corresponding transformer would be\n(h\u2217)\u22a4\nn\nX\ni=1\n(W \u2217\nV vi)(v\u22a4\ni (W \u2217\nK)\u22a4W \u2217\nQvn+1) =\n\u00000\n\u03b7\u0001\nn\nX\ni=1\n\u0014 xi\nyi\n\u0015 \u0014 xi\nyi\n\u0015\u22a4 \u0012Id\u00d7d\n0\n0\n0\n\u0013 \u0014 xn+1\n0\n\u0015\n(12)\n= \u03b7\nn\nX\ni=1\nyix\u22a4\ni xn+1 .\n(13)\n5\nOn the other hand, consider linear regression with total squared error as the loss function, using the xi and\nyi. Here, the loss function would be\nL(w) = 1\n2\nn\nX\ni=1\n(w\u22a4xi \u2212 yi)2 ,\n(14)\nmeaning that the gradient is\n\u2207wL(w) =\nn\nX\ni=1\n(w\u22a4xi \u2212 yi)xi .\n(15)\nIn particular, if we initialize gradient descent at w0 = 0, then after one step of gradient descent with learning\nrate \u03b7, the iterate would be at w1 = \u03b7 Pn\ni=1 yixi \u2014 observe that the \ufb01nal expression in Equation (13) is\nexactly w\u22a4\n1 xn+1.\nNow, we give an overview of the proof of Theorem 1. By the discussion in Section 2, it su\ufb03ces to show\nthat L((W \u2217\nV )\u22a4h\u2217, (W \u2217\nK)\u22a4W \u2217\nQ) is the global minimum of L(w, M). The \ufb01rst step of the proof is to rewrite\nthe loss in a more convenient form, getting rid of the expectation over xn+1 and yn+1:\nLemma 1. Let \u02c6w e\nD be the solution to ridge regression with regularization strength \u03c32 on the exemplars\n(x1, y1), . . . , (xn, yn) given in a context eD. Then, there exists a constant C \u2265 0, which is independent of\nw, M, such that\nL(w, M) = C + ED\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u02c6w e\nD\u22252\n2 .\n(16)\nAs discussed towards the end of Section 2, the prediction can be written as w\u22a4G e\nDMvn+1 where vn+1 =\n\u0014\nxn+1\n0\n\u0015\n, meaning that the e\ufb00ective linear predictor implemented by the transformer is the linear function\nfrom Rd to R with weight vector M \u22a4\n:,1:dG e\nDw.\nThus, we can interpret Lemma 1 as saying that the loss\nfunction encourages the e\ufb00ective linear predictor to match the Bayes-optimal predictor \u02c6w e\nD. Note that it is\nnot possible for the e\ufb00ective linear predictor of the transformer to match \u02c6w e\nD exactly, since the transformer\ncan only implement a linear or quadratic function of the xi, while representing \u02c6w e\nD requires computing\n(X\u22a4X + \u03c32I)\u22121, and this is a much more complex function of the xi.\nProof of Lemma 1. We can write\nL(w, M) = ED\u223cT [(yn+1 \u2212 w\u22a4G e\nDMvn+1)2]\n(17)\n= E e\nD,xn+1\nh\nEyn+1[(yn+1 \u2212 w\u22a4G e\nDMvn+1)2 | xn+1, eD]\ni\n.\n(18)\nLet us simplify the inner expectation. For convenience, \ufb01x eD and xn+1, and consider the function g : Rd \u2192 R\ngiven by\ng(u) = Eyn+1[(u \u00b7 xn+1 \u2212 yn+1)2 | eD, xn+1] .\n(19)\nIt is a well-known fact that the minimizer of g(u) is given by \u02c6w e\nD where \u02c6w e\nD = (X\u22a4X + \u03c32I)\u22121X\u22a4\u20d7y is the\nsolution to ridge regression on X and \u20d7y with regularization strength \u03c32. Furthermore,\n0 = \u2207ug( \u02c6w e\nD) = Eyn+1[2( \u02c6w e\nD \u00b7 xn+1 \u2212 yn+1)xn+1 | eD, xn+1] ,\n(20)\nand in particular, taking the dot product of both sides with u \u2212 \u02c6w e\nD (for any vector u \u2208 Rd) gives\nEyn+1[( \u02c6w e\nD \u00b7 xn+1 \u2212 yn+1) \u00b7 (u \u00b7 xn+1 \u2212 \u02c6w e\nD \u00b7 xn+1)] = 0 .\n(21)\n6\nThus, letting u = w\u22a4G e\nDM:,1:d, we can simplify the inner expectation in Equation (18):\nEyn+1[(yn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2 | xn+1, eD]\n(22)\n= Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nDxn+1 + \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2 | xn+1, eD]\n(23)\n= Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nDxn+1)2 | xn+1, eD] + ( \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2\n(24)\n+ 2 \u00b7 Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nDxn+1)( \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1) | xn+1, eD]\n(25)\n= Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nDxn+1)2 | xn+1, eD] + ( \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2 .\n(By Equation (21))\nWe can further write the \ufb01nal expression as Cxn+1, e\nD + ( \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2, where Cxn+1, e\nD is a\nconstant that depends on xn+1 and eD but is independent of w and M. Thus, we have\nL(w, M) = E e\nD,xn+1[Cxn+1, e\nD] + E e\nD,xn+1[( \u02c6w\u22a4\ne\nDxn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2]\n(26)\n= C + E e\nD\u2225 \u02c6w e\nD \u2212 M \u22a4\n:,1:dG e\nDw\u22252\n2 ,\n(B.c. xn+1 \u223c N(0, Id\u00d7d))\nwhere C is a constant which is independent of w and M.\nNext, the key step is to replace \u02c6w e\nD in the above lemma by \u03b7X\u22a4\u20d7y.\nLemma 2. There exists a constant C1 \u2265 0 which is independent of w, M, such that\nL(w, M) = C1 + E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2 .\n(27)\nLemma 2 says that the loss depends entirely on how far the e\ufb00ective linear predictor is from \u03b7X\u22a4\u20d7y. It\nimmediately follows from this lemma that (WK, WQ, WV , h) is a global minimizer of the loss if and only if\nthe e\ufb00ective linear predictor of the corresponding transformer is \u03b7X\u22a4\u20d7y. Thus, Theorem 1 follows almost\ndirectly from Lemma 2, and in the rest of this section, we give an outline of the proof of Lemma 2 \u2014 the\ndetailed proofs of Theorem 1 and Lemma 2 are in Appendix A.\nProof Strategy for Lemma 2. Our overall proof strategy is to show that the gradients of\nL(w, M) = E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u02c6w e\nD\u22252\n2\n(28)\nand\nL\u2032(w, M) = E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2\n(29)\nare equal at every w, M, from which Lemma 2 immediately follows.6 For simplicity, we write A = M \u22a4\n:,1:d, so\nwithout loss of generality we can show that the gradients of the following two loss functions are identical:\nJ1(A, w) = E e\nD\u223cT \u2225AG e\nDw \u2212 \u02c6w e\nD\u22252\n2\n(30)\nand\nJ2(A, w) = E e\nD\u223cT \u2225AG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2 .\n(31)\nIn this section, we discuss the gradients with respect to w \u2014 we use the same proof ideas to show that the\ngradients with respect to A are the same. We have\n\u2207wJ1(A, w) = 2E e\nD\u223cT G e\nDA\u22a4(AG e\nDw \u2212 \u02c6w e\nD)\n(32)\n6This is because L and L\u2032 are de\ufb01ned everywhere on Rd+1 \u00d7 R(d+1)\u00d7(d+1), and for any two di\ufb00erentiable functions f, g\nde\ufb01ned on an open connected subset S \u2282 Rk, if the gradients of f and g are identical on S, then f and g are equal on S up to\nan additive constant. This can be shown using the fundamental theorem of calculus.\n7\nand\n\u2207wJ2(A, w) = 2E e\nD\u223cT G e\nDA\u22a4(AG e\nDw \u2212 \u03b7X\u22a4\u20d7y) .\n(33)\nThus, showing that these two gradients are equal for all A, w reduces to showing that for all A, we have\nE e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD = \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y .\n(34)\nRecall that G e\nD is de\ufb01ned as\nG e\nD =\nn\nX\ni=1\n\u0014 xix\u22a4\ni\nyixi\nyix\u22a4\ni\ny2\ni\n\u0015\n.\n(35)\nOur \ufb01rst key observation is that for any i \u2208 [n] and any odd positive integer k, E[yk\ni | X] = 0, since\nyi = w\u22a4xi + \u01ebi, and both w\u22a4xi and \u01ebi have distributions which are symmetric around 0. This observation\nalso extends to any odd-degree monomial of the yi. Using this observation, we can simplify the left-hand\nside of Equation (34) as follows. We can \ufb01rst write it as\nE e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD = E e\nD\u223cT\nn\nX\ni=1\n\u0014 xix\u22a4\ni (A\u22a4)1:d,: \u02c6w e\nD + yixi(A\u22a4)d+1,: \u02c6w e\nD\nyix\u22a4\ni (A\u22a4)1:d,: \u02c6w e\nD + y2\ni (A\u22a4)d+1,: \u02c6w e\nD\n\u0015\n.\n(36)\nThen, since \u02c6w e\nD = (X\u22a4X + \u03c32I)\u22121X\u22a4\u20d7y, each entry of \u02c6w e\nD has an odd degree in the yi, meaning the terms\nxix\u22a4\ni (A\u22a4)1:d,: \u02c6w e\nD and y2\ni (A\u22a4)d+1,: \u02c6w e\nD in the above equation vanish after taking the expectation. Thus, we\nobtain\nE e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD = E e\nD\u223cT\nn\nX\ni=1\n\u0014 yixiA\u22a4\n:,d+1 \u02c6w e\nD\nyix\u22a4\ni A\u22a4\n:,1:d \u02c6w e\nD\n\u0015\n= E e\nD\u223cT\n\u0014 X\u22a4\u20d7yA\u22a4\n:,d+1 \u02c6w e\nD\n\u20d7y\u22a4XA\u22a4\n:,1:d \u02c6w e\nD\n\u0015\n.\n(37)\nSince each entry of \u03b7X\u22a4\u20d7y has an odd degree in the yi, in order to simplify the right-hand side of Equation (34),\nwe can apply the same argument but with \u02c6w e\nD replaced by \u03b7X\u22a4\u20d7y, obtaining\n\u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y = \u03b7E e\nD\u223cT\n\u0014 X\u22a4\u20d7yA\u22a4\n:,d+1X\u22a4\u20d7y\n\u20d7y\u22a4XA\u22a4\n:,1:dX\u22a4\u20d7y\n\u0015\n.\n(38)\nThus, showing Equation (34) reduces to showing that\nE e\nD\u223cT\n\u0014 X\u22a4\u20d7yA\u22a4\n:,d+1 \u02c6w e\nD\n\u20d7y\u22a4XA\u22a4\n:,1:d \u02c6w e\nD\n\u0015\n= \u03b7E e\nD\u223cT\n\u0014 X\u22a4\u20d7yA\u22a4\n:,d+1X\u22a4\u20d7y\n\u20d7y\u22a4XA\u22a4\n:,1:dX\u22a4\u20d7y\n\u0015\n.\n(39)\nTo show Equation (39), our key tool is Lemma 4, which follows from Lemma 3.\nLemma 3. There exists a scalar c1 such that E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] = c1Id\u00d7d, and there exists a scalar c2 such\nthat E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] = c2Id\u00d7d.\nLemma 4. If \u03b7 =\nEf\nD\u223cT [ \u02c6\nw\u22a4\nf\nDX\u22a4\u20d7y]\nEf\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y], then E e\nD\u223cT [\u03b7X\u22a4\u20d7y\u20d7y\u22a4X \u2212 X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] = 0.\nOverview of Proof of Lemma 3 and Lemma 4. We give an overview of how we prove Lemma 3\nand Lemma 4 here, and defer the full proofs to Appendix A. To show that E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] is a scalar\nmultiple of the identity, we \ufb01rst use the fact that even when all of the xi are rotated by a rotation matrix\nR, the distribution of \u20d7y|X remains the same, since the weight vector w is drawn from N(0, Id\u00d7d) which is a\nrotationally invariant distribution. Thus, if we de\ufb01ne M(X) = E[\u20d7y\u20d7y\u22a4 | X] as a function of X \u2208 Rn\u00d7d, then\nfor any rotation matrix R \u2208 Rd\u00d7d, we have\nM(XR\u22a4) = E[\u20d7y\u20d7y\u22a4 | XR\u22a4] = E[\u20d7y\u20d7y\u22a4 | X] = M(X) ,\n(40)\n8\nwhere the second equality is because multiplying X on the right by R\u22a4 corresponds to rotating each of the\nxi by R. Additionally, if we rotate the xi by R, then E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] remains the same \u2014 this is because\nthe distribution of the xi is unchanged due to the rotational invariance of the Gaussian distribution, and the\nconditional distribution yi | xi is unchanged when we rotate xi by R. This implies that\nE[X\u22a4\u20d7y\u20d7y\u22a4X] = E[X\u22a4M(X)X] = E[(XR\u22a4)\u22a4M(XR\u22a4)XR\u22a4] ,\n(41)\nwhere the second equality is because, as we observed above, E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] remains the same when we\nrotate each of the xi by R. Since M(XR\u22a4) = M(X), we have\nE[(XR\u22a4)\u22a4M(XR\u22a4)XR\u22a4] = RE[X\u22a4M(X)X]R\u22a4 ,\n(42)\nwhich implies that E[X\u22a4\u20d7y\u20d7y\u22a4X] = RE[X\u22a4\u20d7y\u20d7y\u22a4X]R\u22a4 for any rotation matrix R, and therefore E[X\u22a4\u20d7y\u20d7y\u22a4X]\nis a scalar multiple of the identity matrix. To \ufb01nish the proof of Lemma 3, we show that E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] is\na scalar multiple of the identity using essentially the same argument. To show Lemma 4, we simply take the\ntrace of E e\nD\u223cT [\u03b7X\u22a4\u20d7y\u20d7y\u22a4X \u2212 X\u22a4\u20d7y \u02c6w\u22a4\ne\nD], and select \u03b7 so that this trace is equal to 0.\nFinishing the Proof of Lemma 2. Recall that, to show that the gradients of J1 and J2 (de\ufb01ned in\nEquation (30) and Equation (31)) with respect to w are equal, it su\ufb03ces to show Equation (39). However,\nthis is a direct consequence of Lemma 4. This is because we can rewrite Equation (39) as\nE e\nD\u223cT\n\u0014\nX\u22a4\u20d7y \u02c6w e\nDA:,d+1\ntr( \u02c6w e\nD\u20d7y\u22a4XA\u22a4\n:,1:d)\n\u0015\n= E e\nD\u223cT\n\u0014\nX\u22a4\u20d7y\u20d7y\u22a4XA:,d+1\ntr(X\u22a4\u20d7y\u20d7y\u22a4XA\u22a4\n:,1:d)\n\u0015\n.\n(43)\nThis shows that the gradients of J1 and J2 with respect to w are equal, and we use similar arguments to\nshow that the gradients of J1 and J2 with respect to A are equal. As mentioned above, this implies that\nE e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u02c6w e\nD\u22252\n2 \u2212 E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2 is a constant that is independent of M and w, as\ndesired.\n4\nResults for Di\ufb00erent Data Covariance Matrices\nIn this section, we consider the setting where the xi\u2019s have a covariance that is di\ufb00erent from the identity\nmatrix, and we show that the loss is minimized when the one-layer transformer implements one step of\ngradient descent with preconditioning. This suggests that the distribution of the xi\u2019s has a signi\ufb01cant e\ufb00ect\non the algorithm that the transformer implements.\nData Distribution. Concretely, the data distribution is the same as before, but the xi are sampled from\nN(0, \u03a3), where \u03a3 \u2208 Rd\u00d7d is a positive semi-de\ufb01nite (PSD) matrix. The outputs are generated according\nto yi = w\u22a4xi + \u01ebi, where w \u223c N(0, \u03a3\u22121). This can equivalently be written as xi = \u03a31/2ui, where ui \u223c\nN(0, Id\u00d7d), and yi = (w\u2032)\u22a4ui + \u01ebi, where w\u2032 \u223c N(0, Id\u00d7d). We keep all other de\ufb01nitions, such as the loss\nfunction, the same as before.\nTheorem 2 (Global Minimum for 1-Layer 1-Head Linear Self-Attention with Skewed Covariance). Sup-\npose (W \u2217\nK, W \u2217\nQ, W \u2217\nV , h\u2217) is a global minimizer of the loss L when the data is generated according to the\ndistribution given in this section. Then, the corresponding one-layer transformer implements one step of\npreconditioned gradient descent, on the least-squares linear regression objective, with preconditioner \u03a3\u22121,\nfor some learning rate \u03b7 > 0. Speci\ufb01cally, given a query token vn+1 =\n\u0014 xn+1\n0\n\u0015\n, the transformer outputs\n\u03b7 Pn\ni=1 yi(\u03a3\u22121xi)\u22a4xn+1, where \u03b7 =\nEf\nD\u223cT [\u20d7y\u22a4X(X\u22a4X+\u03c32\u03a3)\u22121X\u22a4\u20d7y]\nEf\nD\u223cT [\u20d7y\u22a4X\u03a3\u22121X\u22a4\u20d7y]\n.\nTo prove this result, we essentially perform a change of variables to reduce this problem to the setting of\nthe previous section \u2014 then, we directly apply Theorem 1. The detailed proof is given in Appendix B.\n9\n5\nResults for Nonlinear Target Functions\nIn this section, we extend to a setting where the target function is nonlinear \u2014 our conditions on the target\nfunction are mild, and for instance allow the target function to be a fully-connected neural network with\narbitrary depth/width. However, we keep the model class the same (i.e. 1-layer transformer with linear\nself-attention). We \ufb01nd that the transformer which minimizes the pre-training loss still implements one step\nof GD on the linear regression objective (Theorem 3), even though the target function is nonlinear. This\nsuggests that the distribution of yi|xi does not a\ufb00ect the algorithm learned by the transformer as much as\nthe distribution of xi.\nData Distribution.\nIn this section, we consider the same setup as Section 3, but we change the\ndistribution of the yi\u2019s. We now assume yi = f(xi) + \u01ebi, where \u01ebi \u223c N(0, \u03c32) as before, but f is drawn from\na family of nonlinear functions satisfying the following assumption:\nAssumption 1. We assume that the target function f is drawn from a family F, with a probability measure\nP on F, such that the following conditions hold: (1) for any \ufb01xed rotation matrix R \u2208 Rd\u00d7d, the distribution\nof functions f is the same as the distribution of f \u25e6 R (where \u25e6 denotes function composition). Moreover,\nthe distribution of f is symmetric under negation. In other words, if E \u2282 F is measurable under P, then\nP(E) = P(\u2212E), where \u2212E = {\u2212f | f \u2208 E}.\nFor example, Assumption 1 is satis\ufb01ed when f(x) is a fully connected neural network, with arbitrary\ndepth and width, where the \ufb01rst and last layers have i.i.d. N(0, 1) entries. Under this assumption, we prove\nthe following result:\nTheorem 3 (Global Minimum for 1-Layer 1-Head Linear Self-Attention with Nonlinear Target Function).\nSuppose Assumption 1 holds, and let (W \u2217\nK, W \u2217\nQ, W \u2217\nV , h\u2217) be a global minimizer of the pre-training loss. Then,\nthe corresponding one-layer transformer implements one step of gradient descent on the least-squares linear\nregression objective, given (x1, y1, . . . , xn, yn). More concretely, given a query token vn+1 =\n\u0014 xn+1\n0\n\u0015\n, the\ntransformer outputs \u03b7 Pn\ni=1 yix\u22a4\ni xn+1, where \u03b7 =\nED[u\u22a4\nf\nDX\u22a4\u20d7y]\nED[\u20d7y\u22a4XX\u22a4\u20d7y] and u e\nD = argminuExn+1,yn+1[(u \u00b7 xn+1 \u2212\nyn+1)2 | eD].\nThe result is essentially the same as that of Theorem 1 \u2014 note that the learning rate is potentially\ndi\ufb00erent, as it may depend on the function family F. The proof is analogous to the proof of Theorem 1.\nFirst we prove the analogue of Lemma 1, de\ufb01ning L(w, M) as in Section 2:\nLemma 5. There exists a constant C \u2265 0 such that\nL(w, M) = C + E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 u e\nD\u22252\n2 .\n(44)\nHere u e\nD = argminuExn+1,yn+1[(u \u00b7 xn+1 \u2212 yn+1)2 | eD].\nNext, in the proof of Lemma 2, we used the fact that odd-degree polynomials of the yi have expectation\n0 \u2014 the corresponding lemma in our current setting is as follows:\nLemma 6. For even integers k and for i \u2208 [n], E[yk\ni u e\nD | X] = 0. This also holds with yk\ni replaced by an\neven-degree monomial of the yi.\nAdditionally, for odd integers k and for i \u2208 [n], E[yk\ni | X] = 0. This also holds with yk\ni replaced by an\nodd-degree monomial of the yi.\nProof of Lemma 6. This follows from Assumption 1. This is because for each outcome (i.e. choice of f and\n\u01eb1, . . . , \u01ebn) which leads to (x1, y1, . . . , xn, yn), the corresponding outcome \u2212f, \u2212\u01eb1, . . . , \u2212\u01ebn which leads to\n(x1, \u2212y1, . . . , xn, \u2212yn) is equally likely. The u e\nD which is obtained from the second outcome is the negative\nof the u e\nD which is obtained from the \ufb01rst outcome. If k is even, then yk\ni is the same under both outcomes\nsince k is even, and the average of yk\ni u e\nD under these two outcomes is 0. Additionally, if k is odd, then yk\ni\nunder the second outcome is the negative of yk\ni under the \ufb01rst outcome, and the average of yk\ni under these\ntwo outcomes is 0. This completes the proof of the lemma.\n10\nNext, we show the analogue of Lemma 3.\nLemma 7. E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] and E e\nD\u223cT [X\u22a4\u20d7yu\u22a4\ne\nD] are scalar multiples of the identity. Thus,\nE e\nD\u223cT [X\u22a4\u20d7yu\u22a4\ne\nD] = \u03b7E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X]\n(45)\nwhere \u03b7 =\nED[u\u22a4\nf\nDX\u22a4\u20d7y]\nED[\u20d7y\u22a4XX\u22a4\u20d7y].\nThe proof of Lemma 7 is nearly identical to that of Lemma 3, with Assumption 1 used where appropriate.\nFor completeness, we include the proof in Appendix C. We now state the analogue of Lemma 2:\nLemma 8. There exists a constant C1 \u2265 0 which is independent of w, M, such that\nL(w, M) = C + E e\nD\u223cT \u2225M \u22a4\n:,1:dG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2 ,\n(46)\nwhere \u03b7 =\nED[u\u22a4\nf\nDX\u22a4\u20d7y]\nED[\u20d7y\u22a4XX\u22a4\u20d7y].\nTheorem 3 now follows from Lemma 8 because M \u22a4\n:,1:dG e\nDw is the weight vector for the e\ufb00ective linear\npredictor implemented by the transformer. All missing proofs are in Appendix C.\n6\nConclusion\nWe theoretically study one-layer transformers with linear self-attention trained on noisy linear regression\ntasks with randomly generated data. We con\ufb01rm the empirical \ufb01nding of von Oswald et al. [2022] by math-\nematically showing that the global minimum of the pre-training loss for one-layer transformers with linear\nself-attention corresponds to one step of GD on a least-squares linear regression objective, when the covari-\nates are from an isotropic Gaussian distribution. We \ufb01nd that when the covariates are not from an isotropic\nGaussian distribution, the global minimum of the pre-training loss instead corresponds to pre-conditioned\nGD, while if the covariates are from an isotropic Gaussian distribution and the response is obtained from a\nnonlinear target function, then the global minimum of the pre-training loss will still correspond to one step\nof GD on a least-squares linear regression objective. We study single-head linear self-attention layers \u2014 it is\nan interesting direction for future work to study the global minima of the pre-training loss for a multi-head\nlinear self-attention layer. Another interesting direction is to study the algorithms learned by multi-layer\ntransformers when the response is obtained from a nonlinear target function. We note that Ahn et al. [2023]\nhave studied the case of multi-layer transformers when the target function is linear. They show that for\ncertain restricted parameterizations of multi-layer linear transformers, the global minima or critical points\nof the pre-training loss correspond to interpretable gradient-based algorithms.\nAcknowledgments\nThe authors would like to thank the support from NSF IIS 2211780 and a gift from Open Philanthropy.\nReferences\nKwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement precon-\nditioned gradient descent for in-context learning, 2023.\nEkin Aky\u00fcrek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm\nis in-context learning? investigations with linear models. In The Eleventh International Conference on\nLearning Representations, 2023. URL https://openreview.net/forum?id=0g0X4H8yN4I.\n11\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Nee-\nlakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Je\ufb00rey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. CoRR, abs/2005.14165, 2020. URL https://arxiv.org/abs/2005.14165.\nDamai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn\nin-context? language models implicitly perform gradient descent as meta-optimizers, 2023.\nShivam\nGarg,\nDimitris\nTsipras,\nPercy\nLiang,\nand\nGregory\nValiant.\nWhat\ncan\ntransform-\ners learn in-context?\nA case study of simple function classes.\nIn NeurIPS, 2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conferenc\nAngeliki Giannou, Shashank Rajput, Jy yong Sohn, Kangwook Lee, Jason D. Lee, and Dimitris Papailiopou-\nlos. Looped transformers as programmable computers, 2023.\nOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham.\nJurassic-1: Technical details and evaluation.\nTechnical report, AI21 Labs, 2021.\nBingbin Liu,\nJordan T. Ash,\nSurbhi Goel,\nAkshay Krishnamurthy,\nand Cyril Zhang.\nTrans-\nformers learn shortcuts to automata.\nIn The Eleventh International Conference on Learning\nRepresentations,\nICLR\n2023,\nKigali,\nRwanda,\nMay 1-5,\n2023.\nOpenReview.net,\n2023.\nURL\nhttps://openreview.net/pdf?id=De4FYqjFueZ.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. Rethinking the role of demonstrations: What makes in-context learning work? In EMNLP, 2022.\nAlec Radford, Je\ufb00rey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. Technical report, OpenAI, 2019.\nJohannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\u00e3o Sacramento, Alexander Mordvintsev, An-\ndrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent, 2022.\nBen Wang and Aran Komatsuzaki. Gpt-j-6b: A 6 billion parameter autoregressive language model, 2021.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang,\nDenny Zhou, and Tengyu Ma. Larger language models do in-context learning di\ufb00erently, 2023.\nSang Michael Xie,\nAditi Raghunathan,\nPercy Liang,\nand Tengyu Ma.\nAn explanation of in-\ncontext learning as implicit bayesian inference.\nIn The Tenth International Conference on Learn-\ning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\nURL\nhttps://openreview.net/forum?id=RdJVFCHjUMI.\nRuiqi Zhang, Spencer Frei, and Peter L. Bartlett. Trained transformers learn linear models in-context, 2023.\n12\nA\nMissing Proofs from Section 3\nProof of Lemma 3. For convenience let M(X) = E e\nD\u223cT [\u20d7y\u20d7y\u22a4 | X] \u2014 we use this notation to make the\ndependence on X clear. Then, the (i, j)-th entry of M(X) is E[(w \u00b7 xi + \u01ebi)(w \u00b7 xj + \u01ebj) | X] and this is equal\nto E[(w\u00b7xi)(w\u00b7xj) | X] for i \u0338= j, and E[(w\u00b7xi)(w\u00b7xj) | X]+\u03c32 for i = j. If we perform the change of variables\nxi 7\u2192 Rxi for a \ufb01xed rotation matrix R and all i \u2208 [n], then Ew[(w\u00b7Rxi)(w\u00b7Rxj) | X] = Ew[(w\u00b7xi)(w\u00b7xj) | X]\nbecause w \u223c N(0, Id\u00d7d) and N(0, Id\u00d7d) is a rotationally invariant distribution. In other words, we have\nM(XR\u22a4) = M(X). Thus, for any rotation matrix R,\nEX[X\u22a4M(X)X] = EX[(XR)\u22a4M(XR\u22a4)(XR\u22a4)]\n(By rotational invariance of dist. of xi)\n= R\u22a4EX[X\u22a4M(XR\u22a4)X]R\u22a4\n(47)\n= R\u22a4EX[X\u22a4M(X)X]R .\n(Because M(XR\u22a4) = M(X))\nThis implies that EX[X\u22a4M(X)X] = E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] is a scalar multiple of the identity matrix.\nNext, we consider E e\nD\u223cT [\u20d7y \u02c6w\u22a4\ne\nD | X], which we write for convenience as J(X). Similarly to the above\nproof, we use the observation that if we make the change of variables xi \u2192 Rxi, then the joint distribution\nof y1, . . . , yn, yn+1 is unchanged due to the rotational invariance of w.\nAdditionally, if we write \u02c6w e\nD as\n\u02c6w e\nD(x1, y1, . . . , xn, yn) to emphasize the fact that it is a function of x1, y1, . . . , xn, yn, then\n\u02c6w e\nD(Rx1, y1, Rx2, y2, . . . , Rxn, yn) = R \u02c6w e\nD(x1, y1, x2, y2, . . . , xn, yn)\n(48)\nbecause \u02c6w e\nD is the minimizer of F(w) = \u2225w\u22a4X \u2212 \u20d7y\u22252\n2 + \u03c32\u2225w\u22252\n2, and if all the xi are rotated by R, then the\nminimizer of F will also be rotated by R. Thus,\nJ(XR\u22a4) = E e\nD\u223cT [\u20d7y \u02c6w\u22a4\ne\nD | XR\u22a4] = E e\nD\u223cT [\u20d7y \u02c6w\u22a4\ne\nDR\u22a4 | X] = J(X)R\u22a4 .\n(49)\nWe can use this to show that E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] is a scalar multiple of the identity. Letting R be a rotation\nmatrix, we obtain\nE e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] = E e\nD\u223cT E[X\u22a4\u20d7y \u02c6w\u22a4\ne\nD | X]\n(50)\n= E e\nD\u223cT [X\u22a4E[\u20d7y \u02c6w\u22a4\ne\nD | X]]\n(51)\n= EX[X\u22a4J(X)]\n(52)\n= EX[(XR\u22a4)\u22a4J(XR\u22a4)]\n(By rotational invariance of dist. of xi)\n= EX[RX\u22a4J(X)R\u22a4]\n(B.c. J(XR\u22a4) = J(X)R\u22a4)\n= REX[X\u22a4J(X)]R\u22a4\n(53)\n= RE e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD]R\u22a4 .\n(By Equation (52))\nThis implies that E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] is a scalar multiple of the identity.\nProof of Lemma 4. By Lemma 3, we have E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] = c1I and E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] = c2I for some\nscalars c1, c2. Taking the traces of both matrices gives us\nc1d = tr(E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X]) = E e\nD\u223cT [tr(X\u22a4\u20d7y\u20d7y\u22a4X)] = E e\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y] ,\n(54)\nand similarly,\nc2d = tr(E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD]) = E e\nD\u223cT [tr(X\u22a4\u20d7y \u02c6w\u22a4\ne\nD)] = E e\nD\u223cT [ \u02c6w\u22a4\ne\nDX\u22a4\u20d7y] .\n(55)\n13\nBy the de\ufb01nition of \u03b7 as\nEf\nD\u223cT [ \u02c6\nw\u22a4\nf\nDX\u22a4\u20d7y]\nEf\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y], we have\n\u03b7E e\nD\u223cT [X\u22a4\u20d7y\u20d7y\u22a4X] \u2212 E e\nD\u223cT [X\u22a4\u20d7y \u02c6w\u22a4\ne\nD] = \u03b7c1I \u2212 c2I\n(56)\n= \u03b7 \u00b7 E e\nD\u223cT [\u20d7y\u22a4XX\u22a4\u20d7y]\nd\nI \u2212\nE e\nD\u223cT [ \u02c6w\u22a4\ne\nDX\u22a4\u20d7y]\nd\nI\n(57)\n=\nE e\nD\u223cT [ \u02c6w\u22a4\ne\nDX\u22a4\u20d7y]\nd\nI \u2212\nE e\nD\u223cT [ \u02c6w\u22a4\ne\nDX\u22a4\u20d7y]\nd\nI\n(58)\n= 0 ,\n(59)\ncompleting the proof of the lemma.\nProof of Lemma 2. For convenience, we write A := M \u22a4\n:,1:d. Then, we wish to show that for all A \u2208 Rd\u00d7(d+1)\nand w \u2208 Rd+1, we have\nE e\nD\u223cT \u2225AG e\nDw \u2212 \u02c6w e\nD\u22252\n2 = C + E e\nD\u223cT \u2225AG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2\n(60)\nfor some constant C independent of A and w. De\ufb01ne\nJ1(A, w) = E e\nD\u223cT \u2225AG e\nDw \u2212 \u02c6w e\nD\u22252\n2\n(61)\nand\nJ2(A, w) = E e\nD\u223cT \u2225AG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2 .\n(62)\nTo show that J1(A, w) and J2(A, w) are equal up to a constant, it su\ufb03ces to show that their gradients are\nidentical, since J1 and J2 are de\ufb01ned everywhere on Rd+1 \u00d7 R(d+1)\u00d7(d+1) and by the fundamental theorem\nof calculus.\nGradients With Respect to w. First, we analyze the gradients with respect to w. We have\n\u2207wJ1(A, w) = 2E e\nD\u223cT G e\nDA\u22a4(AG e\nDw \u2212 \u02c6w e\nD)\n(63)\nand\n\u2207wJ2(A, w) = 2E e\nD\u223cT G e\nDA\u22a4(AG e\nDw \u2212 \u03b7X\u22a4\u20d7y)\n(64)\nwhere we use the convention that the gradient with respect to w has the same shape as w. To show that\n\u2207wJ1(A, w) = \u2207wJ2(A, w), it su\ufb03ces to show that E e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD = \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y. Observe that we\ncan write G e\nD as\nG e\nD =\nn\nX\ni=1\n\u0014 xi\nyi\n\u0015 \u0014 xi\nyi\n\u0015\u22a4\n=\nn\nX\ni=1\n\u0014 xix\u22a4\ni\nyixi\nyix\u22a4\ni\ny2\ni\n\u0015\n.\n(65)\nGiven X, the expected value of any odd monomial of the yi is equal to 0, since w and \u01ebi are independent of\nX and have mean 0. Thus, we can ignore the blocks corresponding to xix\u22a4\ni and y2\ni , since the entries of \u02c6w e\nD\nand X\u22a4\u20d7y contain odd monomials of the yi once X is \ufb01xed.\nWe now proceed in two steps. First, we deal with the terms corresponding to the lower left block of\nG e\nD and show that their respective contributions to E e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD and \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y are equal. For\nE e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD, these terms contribute:\nE e\nD\u223cT\nn\nX\ni=1\nyix\u22a4\ni (A\u22a4)1:d,: \u02c6w e\nD = E e\nD\u223cT \u20d7y\u22a4XA\u22a4\n:,1:d \u02c6w e\nD\n(66)\n= E e\nD\u223cT tr( \u02c6w e\nD\u20d7y\u22a4XA\u22a4\n:,1:d)\n(67)\n= tr(E e\nD\u223cT [ \u02c6w e\nD\u20d7y\u22a4X]A\u22a4\n:,1:d) ,\n(68)\n14\nwhile for \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y, these terms contribute:\n\u03b7E e\nD\u223cT\nn\nX\ni=1\nyix\u22a4\ni A\u22a4\n:,1:dX\u22a4\u20d7y = \u03b7E e\nD\u223cT \u20d7y\u22a4XA\u22a4\n:,1:dX\u22a4\u20d7y\n(69)\n= \u03b7E e\nD\u223cT tr(X\u22a4\u20d7y\u20d7y\u22a4XA\u22a4\n:,1:d)\n(70)\n= tr(E e\nD\u223cT [\u03b7X\u22a4\u20d7y\u20d7y\u22a4X]A\u22a4\n:,1:d) ,\n(71)\nand since E e\nD\u223cT [ \u02c6w e\nD\u20d7y\u22a4X] = E e\nD\u223cT [\u03b7X\u22a4\u20d7y\u20d7y\u22a4X] by Lemma 4, these two contributions are equal.\nSecond, we deal with the terms corresponding to the upper right block of G e\nD. For E e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD these\nterms contribute\nE e\nD\u223cT\nn\nX\ni=1\nyixi(A\u22a4)(d+1),: \u02c6w e\nD = E e\nD\u223cT X\u22a4\u20d7yA\u22a4\n:,(d+1) \u02c6w e\nD\n(72)\n= E e\nD\u223cT X\u22a4\u20d7y \u02c6w\u22a4\ne\nDA:,(d+1) ,\n(B.c. dot product A\u22a4\n:,(d+1) \u02c6w e\nD is commutative)\nwhile for \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y, these terms contribute\n\u03b7E e\nD\u223cT\nn\nX\ni=1\nyixi(A\u22a4)(d+1),:X\u22a4\u20d7y = \u03b7E e\nD\u223cT X\u22a4\u20d7yA\u22a4\n:,(d+1)X\u22a4\u20d7y\n(73)\n= \u03b7E e\nD\u223cT X\u22a4\u20d7y\u20d7y\u22a4XA:,(d+1) ,\n(B.c. dot product A\u22a4\n:,(d+1)X\u22a4\u20d7y is commutative)\nand again these contributions are equal by Lemma 4. In summary, we have shown that for E e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD\nand \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y, the contributions corresponding to the lower left and upper right blocks of G e\nD are\nequal, meaning that E e\nD\u223cT G e\nDA\u22a4 \u02c6w e\nD and \u03b7E e\nD\u223cT G e\nDA\u22a4X\u22a4\u20d7y are equal. This shows that \u2207wJ1(A, w) and\n\u2207wJ2(A, w) are equal.\nGradients With Respect to A. We can compute the gradient of J1 with respect to A as follows:\n\u2207AJ1(A, w) = E e\nD\u223cT \u2207A\u2225AG e\nDw \u2212 \u02c6w e\nD\u22252\n2\n(74)\n= E e\nD\u223cT \u2207A(AG e\nDw \u2212 \u02c6w e\nD)\u22a4(AG e\nDw \u2212 \u02c6w e\nD)\n(75)\n= 2E e\nD\u223cT (AG e\nDw \u2212 \u02c6w e\nD)(G e\nDw)\u22a4 ,\n(76)\nand we can similarly compute\n\u2207AJ2(A, w) = E e\nD\u223cT \u2207A\u2225AG e\nDw \u2212 \u03b7X\u22a4\u20d7y\u22252\n2\n(77)\n= E e\nD\u223cT \u2207A(AG e\nDw \u2212 \u03b7X\u22a4\u20d7y)\u22a4(AG e\nDw \u2212 \u03b7X\u22a4\u20d7y)\n(78)\n= 2E e\nD\u223cT (AG e\nDw \u2212 \u03b7X\u22a4\u20d7y)(G e\nDw)\u22a4 .\n(79)\nThus, to show that the gradients with respect to A are equal, it su\ufb03ces to show that E e\nD\u223cT \u02c6w e\nDw\u22a4G e\nD =\n\u03b7E e\nD\u223cT X\u22a4\u20d7yw\u22a4G e\nD for all w.\nAs before, we only need to consider the lower left and upper right blocks of G e\nD, since the entries of the\nother blocks have even powers of the yi. First let us consider the lower left block. The contribution of the\nlower left block to E e\nD\u223cT \u02c6w e\nDw\u22a4G e\nD is\nE e\nD\u223cT \u02c6w e\nDw(d+1)\nn\nX\ni=1\nyix\u22a4\ni = E e\nD\u223cT \u02c6w e\nDw(d+1)\u20d7y\u22a4X = w(d+1)E e\nD\u223cT \u02c6w e\nD\u20d7y\u22a4X ,\n(80)\nwhile the contribution of the lower left block to \u03b7E e\nD\u223cT X\u22a4\u20d7yw\u22a4G e\nD is\nE e\nD\u223cT \u03b7X\u22a4\u20d7yw(d+1)\nn\nX\ni=1\nyix\u22a4\ni = \u03b7w(d+1)E e\nD\u223cT X\u22a4\u20d7y\u20d7y\u22a4X ,\n(81)\n15\nand these two are equal by Lemma 4. The contribution of the upper right block to E e\nD\u223cT \u02c6w e\nDw\u22a4G e\nD is\nE e\nD\u223cT \u02c6w e\nDw\u22a4\n1:d\nn\nX\ni=1\nyixi = E e\nD\u223cT \u02c6w e\nDw\u22a4\n1:dX\u22a4\u20d7y = (E e\nD\u223cT \u02c6w e\nD\u20d7y\u22a4X)w1:d ,\n(82)\nby the commutativity of the dot product w\u22a4\n1:dX\u22a4\u20d7y, while similarly, the contribution of the upper right block\nto \u03b7E e\nD\u223cT X\u22a4\u20d7yw\u22a4G e\nD is\nE e\nD\u223cT \u03b7X\u22a4\u20d7yw\u22a4\n1:d\nn\nX\ni=1\nyixi = (\u03b7E e\nD\u223cT X\u22a4\u20d7y\u20d7y\u22a4X)w1:d ,\n(83)\nand these two quantities are equal by Lemma 4. Thus, we have shown that \u2207AJ1(A, w) = \u2207AJ2(A, w).\nSummary. We have shown that \u2207wJ1(A, w) = \u2207wJ2(A, w), and \u2207AJ1(A, w) = \u2207AJ2(A, w). This\ncompletes the proof of the lemma.\nProof of Theorem 1. This theorem follows from Lemma 1 and Lemma 2. To see why, recall from Section 2\nthat the output of the transformer on the last token vn+1 can be written as w\u22a4G e\nDMvn+1, where M = W \u22a4\nKWQ\nand w = W \u22a4\nV h. By Lemma 2, the pre-training loss is minimized when M \u22a4\n:,1:dG e\nDw = \u03b7X\u22a4\u20d7y almost surely\n(over the randomness of X and \u20d7y), and when this holds, the output of the transformer on the last token is\nw\u22a4G e\nDMvn+1 = w\u22a4G e\nDM:,1:dxn+1 = (\u03b7X\u22a4\u20d7y)\u22a4xn+1 = \u03b7\nn\nX\ni=1\nyix\u22a4\ni xn+1 ,\n(84)\nas desired.\nB\nMissing Proofs for Section 4\nProof of Theorem 2. As discussed in Section 4, we can write xi = \u03a31/2ui, where ui \u223c N(0, Id\u00d7d). Further-\nmore, we can let U \u2208 Rn\u00d7d be the matrix whose ith row is u\u22a4\ni . Note that X = U\u03a31/2, since \u03a3 is a symmetric\npositive-de\ufb01nite matrix. Given eD = (x1, y1, . . . , xn, yn), we can de\ufb01ne \u02c6w e\nD,\u03a3 = (U \u22a4U + \u03c32I)\u22121U \u22a4\u20d7y \u2014 this\nwould be the solution to ridge regression if we were given the ui and yi. Using X = U\u03a31/2 which implies\nU = X\u03a3\u22121/2, we obtain\n\u02c6w e\nD,\u03a3 = (\u03a3\u22121/2X\u22a4X\u03a3\u22121/2 + \u03c32I)\u22121\u03a3\u22121/2X\u22a4\u20d7y\n(85)\n= (\u03a31/2\u03a3\u22121/2X\u22a4X\u03a3\u22121/2 + \u03c32\u03a31/2)\u22121X\u22a4\u20d7y\n(86)\n= (X\u22a4X\u03a3\u22121/2 + \u03c32\u03a31/2)\u22121X\u22a4\u20d7y\n(87)\n= \u03a31/2(X\u22a4X + \u03c32\u03a3)\u22121X\u22a4\u20d7y .\n(88)\nWe now change variables in order to reduce Theorem 2 to Theorem 1, using the fact that ui and yi together\nare from the same distribution as the data studied in Theorem 1. We can write the loss as\nL(WK, WQ, WV , h) = ED\u223cT [(yn+1 \u2212 h\u22a4\u02c6vn+1)2]\n(89)\n= E e\nD,xn+1\nh\nEyn+1[(yn+1 \u2212 h\u22a4\u02c6vn+1)2 | xn+1, D]\ni\n.\n(90)\nThe solution to ridge regression given the ui and yi is \u02c6w e\nD,\u03a3, meaning E[yn+1 | un+1, eD] = \u02c6w\u22a4\ne\nD,\u03a3un+1 and\ntherefore E[yn+1 | xn+1, eD] = \u02c6w\u22a4\ne\nD,\u03a3un+1, since un+1 = \u03a3\u22121/2xn+1, which is an invertible function of xn+1.\n16\nThus,\nEyn+1[(yn+1 \u2212 h\u22a4\u02c6vn+1)2 | xn+1, eD] = Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nD,\u03a3un+1)2 | xn+1, eD]\n(91)\n+ Eyn+1[( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4\u02c6vn+1)2 | xn+1, eD]\n(92)\n+ 2Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nD,\u03a3un+1)( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4vn+1) | xn+1, eD] .\n(93)\nNow, note that Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nD,\u03a3un+1)( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4vn+1) | xn+1, eD] = 0 since \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4\u02c6vn+1 is\nfully determined by xn+1 and eD, and E[yn+1 \u2212 \u02c6w\u22a4\ne\nD,\u03a3un+1 | xn+1, eD] = 0 as mentioned above. Additionally,\nwe can write Eyn+1[(yn+1 \u2212 \u02c6w\u22a4\ne\nD,\u03a3un+1)2 | xn+1, eD] as Cxn+1, e\nD (denoting a constant that depends only on\nxn+1 and eD) since it is independent of the parameters WK, WQ, WV , h. Thus, taking the expectation over\nxn+1 and eD, for some constant C which is independent of WK, WQ, WV , h, we have\nL(WK, WQ, WV , h) = C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4\u02c6vn+1)2\n(94)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4WV G e\nDW \u22a4\nKWQvn+1)2\n(95)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4WV G e\nDW \u22a4\nKWQ:,1:dxn+1)2\n(96)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4WV G e\nDW \u22a4\nKWQ:,1:d\u03a31/2un+1)2 .\n(97)\nTo \ufb01nish the proof, observe that we can write\nG e\nD =\nn\nX\ni=1\n\u0014 xi\nyi\n\u0015 \u0014 xi\nyi\n\u0015\u22a4\n(98)\n=\n\u0012\n\u03a31/2\n0\n0\n1\n\u0013\nn\nX\ni=1\n\u0014\nui\nyi\n\u0015 \u0014\nui\nyi\n\u0015\u22a4 \u0012\n\u03a31/2\n0\n0\n1\n\u0013\n(99)\n= HG e\nD\n\u2032H\u22a4 ,\n(100)\nwhere we have de\ufb01ned H =\n\u0012\n\u03a31/2\n0\n0\n1\n\u0013\nand\nG e\nD\n\u2032 =\nn\nX\ni=1\n\u0014\nui\nyi\n\u0015 \u0014\nui\nyi\n\u0015\u22a4\n.\n(101)\nNow, let us make the change of variables h = h\u2032, WV = W \u2032\nV H\u22121, WK = W \u2032\nKH\u22121 and WQ = W \u2032\nQH\u22121. Then,\nwe obtain\nL(WK,WQ, WV , h)\n(102)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 h\u22a4WV G e\nDW \u22a4\nKWQ:,1:d\u03a31/2un+1)2\n(103)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 (h\u2032)\u22a4W \u2032\nV H\u22121HG e\nD\n\u2032HH\u22121(W \u2032\nK)\u22a4W \u2032\nQ:,1:d\u03a3\u22121/2\u03a31/2un+1)2\n(104)\n= C + E e\nD,xn+1( \u02c6w\u22a4\ne\nD,\u03a3un+1 \u2212 (h\u2032)\u22a4W \u2032\nV G e\nD\n\u2032(W \u2032\nK)\u22a4W \u2032\nQ:,1:dun+1)2 .\n(105)\nIn other words, this is equal to the loss obtained by the transformer corresponding to h\u2032, W \u2032\nV , W \u2032\nK, W \u2032\nQ\nwhen the data is from the distribution studied in Section 3. The above proof also shows that the trans-\nformer corresponding to h, WV , WK, WQ has the same output on xn+1, as the transformer corresponding to\n17\nh\u2032, W \u2032\nV , W \u2032\nK, W \u2032\nQ does on un+1. By Theorem 1, the output of the latter transformer on un+1 is\n(\u03b7U \u22a4\u20d7y)\u22a4un+1 = \u03b7\u20d7y\u22a4Uun+1\n(106)\n= \u03b7\nn\nX\ni=1\nyiu\u22a4\ni un+1\n(107)\n= \u03b7\nn\nX\ni=1\nyi(\u03a3\u22121/2xi)\u22a4\u03a3\u22121/2xn+1\n(108)\n= \u03b7\nn\nX\ni=1\nyix\u22a4\ni \u03a3\u22121xn+1 ,\n(109)\nwhere the learning rate is\n\u03b7 =\nE e\nD\u223cT [ \u02c6w\u22a4\ne\nD,\u03a3U \u22a4\u20d7y]\nE e\nD\u223cT [\u20d7y\u22a4UU \u22a4\u20d7y]\n(110)\n= E e\nD\u223cT [\u20d7y\u22a4X(X\u22a4X + \u03c32\u03a3)\u22121\u03a31/2U \u22a4\u20d7y]\nE e\nD\u223cT [\u20d7y\u22a4UU \u22a4\u20d7y]\n(By de\ufb01nition of \u02c6w e\nD,\u03a3)\n= E e\nD\u223cT [\u20d7y\u22a4X(X\u22a4X + \u03c32\u03a3)\u22121\u03a31/2\u03a3\u22121/2X\u22a4\u20d7y]\nE e\nD\u223cT [\u20d7y\u22a4X\u03a3\u22121/2\u03a3\u22121/2X\u22a4\u20d7y]\n(B.c. U = X\u03a3\u22121/2)\n= E e\nD\u223cT [\u20d7y\u22a4X(X\u22a4X + \u03c32\u03a3)\u22121X\u22a4\u20d7y]\nE e\nD\u223cT [\u20d7y\u22a4X\u03a3\u22121X\u22a4\u20d7y]\n.\n(111)\nThis completes the proof.\nC\nMissing Proofs from Section 5\nProof of Lemma 5. We proceed similarly to the proof of Lemma 1. For convenience, \ufb01x eD and consider the\nfunction\ng(u) = Exn+1,yn+1[(u \u00b7 xn+1 \u2212 yn+1)2 | eD] .\n(112)\nThen, we have\n\u2207ug(u) = Exn+1,yn+1[2(u \u00b7 xn+1 \u2212 yn+1)xn+1 | eD] .\n(113)\nTherefore, if u e\nD is the minimizer of g(u) (note that u e\nD depends on eD but not on xn+1 and yn+1), then for\nany u \u2208 Rd, we have\nExn+1,yn+1[(u e\nD \u00b7 xn+1 \u2212 yn+1)(u \u00b7 xn+1 \u2212 u e\nD \u00b7 xn+1) | eD] = (u \u2212 u e\nD) \u00b7 \u2207ug(u e\nD) = 0 ,\n(114)\nmeaning that for any u \u2208 Rd, and some C e\nD which only depends on eD and is independent of u,\nExn+1,yn+1[(u \u00b7 xn+1 \u2212 yn+1)2 | eD]\n(115)\n= Exn+1,yn+1[(u \u00b7 xn+1 \u2212 u e\nD \u00b7 xn+1 + u e\nD \u00b7 xn+1 \u2212 yn+1)2 | eD]\n(116)\n= Exn+1,yn+1[(u \u00b7 xn+1 \u2212 u e\nD \u00b7 xn+1)2 | eD] + Exn+1,yn+1[(u e\nD \u00b7 xn+1 \u2212 yn+1)2 | eD]\n(117)\n+ 2Exn+1,yn+1[(u \u00b7 xn+1 \u2212 u e\nD \u00b7 xn+1)(u e\nD \u00b7 xn+1 \u2212 yn+1) | eD]\n(118)\n= Exn+1,yn+1[(u \u00b7 xn+1 \u2212 u e\nD \u00b7 xn+1)2 | eD] + Exn+1,yn+1[(u e\nD \u00b7 xn+1 \u2212 yn+1)2 | eD]\n(By Equation (114))\n= \u2225u \u2212 u e\nD\u22252\n2 + C e\nD .\n(119)\n18\nHere the last equality is because xn+1 \u223c N(0, Id\u00d7d), and because Exn+1,yn+1[(u e\nD \u00b7 xn+1 \u2212 yn+1)2 | eD] is a\nconstant that depends on eD but not on u. We can apply this manipulation to the loss function:\nL(w, M) = E e\nD\u223cT\nh\nExn+1,yn+1[(yn+1 \u2212 \u02c6yn+1)2 | eD]\ni\n(120)\n= E e\nD\u223cT\nh\nExn+1,yn+1[(yn+1 \u2212 w\u22a4G e\nDMvn+1)2 | eD]\ni\n(121)\n= E e\nD\u223cT\nh\nExn+1,yn+1[(yn+1 \u2212 w\u22a4G e\nDM:,1:dxn+1)2 | eD]\ni\n(122)\n= E e\nD\u223cT\nh\n\u2225u e\nD \u2212 M \u22a4\n:,1:dG e\nDw\u22252\n2 + C e\nD\ni\n(By Equation (119))\n= E e\nD\u223cT [\u2225u e\nD \u2212 M \u22a4\n:,1:dG e\nDw\u22252\n2] + C .\n(123)\nHere, C is a constant independent of w and M. This completes the proof of the lemma.\nProof of Lemma 7. We prove this using Assumption 1, imitating the proof of Lemma 3. For convenience, let\nM(X) = E[\u20d7y\u20d7y\u22a4 | X]. Then, the (i, j)-th entry of M(X) is Ef[f(xi)f(xj)] + \u03c32 if i = j and Ef[f(xi)f(xj)] if\ni \u0338= j, since E[\u01ebi] = 0 and the \u01ebi are i.i.d. and independent of X. If we perform the change of variables xi \u2192\nRxi for a \ufb01xed rotation matrix R and all i \u2208 [n], then by Assumption 1, Ef[f(Rxi)f(Rxj)] = Ef[f(xi)f(xj)],\nmeaning that for any rotation matrix R,\nEX[X\u22a4M(X)X] = EX[(XR\u22a4)\u22a4M(XR\u22a4)(XR\u22a4)] = REX[X\u22a4M(XR\u22a4)X]R\u22a4 = REX[X\u22a4M(X)X]R\u22a4\n(124)\nby the rotational invariance of the xi. This implies that EX[X\u22a4M(X)X] = E e\nD[X\u22a4\u20d7y\u20d7y\u22a4X] is a scalar multiple\nof the identity matrix.\nNext, we consider E e\nD\u223cT [X\u22a4\u20d7yu\u22a4\ne\nD]. For convenience, let J(X) = E e\nD\u223cT [\u20d7yu\u22a4\ne\nD | X]. If we make the change\nof variables xi \u2192 Rxi, then the joint distribution of y1, . . . , yn, yn+1 does not change by Assumption 1,\nmeaning that u e\nD is replaced by Ru e\nD. Thus, we can conclude that J(X) is equivariant to rotations of all the\nxi by R:\nJ(XR\u22a4) = E[\u20d7yu\u22a4\ne\nD | XR\u22a4] = E[\u20d7yu\u22a4\ne\nDR\u22a4 | X] = J(X)R\u22a4 .\n(125)\nThus,\nE e\nD\u223cT [X\u22a4\u20d7yu e\nD] = EX[X\u22a4J(X)]\n(126)\n= EX[(XR\u22a4)\u22a4J(XR\u22a4)]\n(By rotational invariance of N(0, Id\u00d7d))\n= REX[X\u22a4J(X)]R\u22a4\n(By Equation (125))\n= RE e\nD\u223cT [X\u22a4\u20d7yu\u22a4\ne\nD]R\u22a4 .\n(127)\nThus, E e\nD\u223cT [X\u22a4\u20d7yu\u22a4\ne\nD] is a scalar multiple of the identity matrix. The \ufb01nal statement of the lemma follows\nby taking the trace of the left and right hand sides.\nProof of Lemma 8. This follows by the same argument as Lemma 2 \u2014 here we use Lemma 6 in order to\nshow that we only need to consider the lower left and upper right blocks of G e\nD, and the rest of the proof\nfollows from linear algebraic manipulations and applying Lemma 7 (in place of Lemma 4 which was used in\nthe proof of Lemma 2).\nProof of Theorem 3. This follows directly from Lemma 8, since the e\ufb00ective linear predictor being \u03b7X\u22a4\u20d7y is\na necessary and su\ufb03cient condition for minimizing the pre-training loss.\n19\n"
  },
  {
    "title": "Frontier AI Regulation: Managing Emerging Risks to Public Safety",
    "link": "https://arxiv.org/pdf/2307.03718.pdf",
    "upvote": "3",
    "text": "FRONTIER AI REGULATION:\nMANAGING EMERGING RISKS TO PUBLIC SAFETY\nMarkus Anderljung1,2\u2217\u2020, Joslyn Barnhart3\u2217\u2217, Anton Korinek4,5,1\u2217\u2217\u2020, Jade Leung6\u2217, Cullen O\u2019Keefe6\u2217,\nJess Whittlestone7\u2217\u2217, Shahar Avin8, Miles Brundage6, Justin Bullock9,10, Duncan Cass-Beggs11,\nBen Chang12, Tantum Collins13,14, Tim Fist2, Gillian Hadfield15,16,17,6, Alan Hayes18, Lewis Ho3,\nSara Hooker19, Eric Horvitz20, Noam Kolt15, Jonas Schuett1, Yonadav Shavit14\u2217\u2217\u2217,\nDivya Siddarth21, Robert Trager1,22, Kevin Wolf18\n1Centre for the Governance of AI, 2Center for a New American Security, 3Google DeepMind,\n4Brookings Institution, 5University of Virginia, 6OpenAI, 7Centre for Long-Term Resilience, 8Centre for the\nStudy of Existential Risk, University of Cambridge, 9University of Washington, 10Convergence Analysis,\n11Centre for International Governance Innovation, 12The Andrew W. Marshall Foundation,\n13GETTING-Plurality Network, Edmond & Lily Safra Center for Ethics, 14Harvard University,\n15University of Toronto, 16Schwartz Reisman Institute for Technology and Society, 17Vector Institute,\n18Akin Gump Strauss Hauer & Feld LLP, 19Cohere For AI, 20Microsoft, 21Collective Intelligence Project,\n22University of California: Los Angeles\nListed authors contributed substantive ideas and/or work to the white paper. Contributions include writing, editing, research,\ndetailed feedback, and participation in a workshop on a draft of the paper. The first six authors are listed in alphabetical order, as are\nthe subsequent 18. Given the size of the group, inclusion as an author does not entail endorsement of all claims in the paper, nor does\ninclusion entail an endorsement on the part of any individual\u2019s organization.\n\u2217Significant contribution, including writing, research, convening, and setting the direction of the paper.\n\u2217\u2217Significant contribution including editing, convening, detailed input, and setting the direction of the paper.\n\u2217\u2217\u2217Work done while an independent contractor for OpenAI.\n\u2020Corresponding\nauthors.\nMarkus\nAnderljung\n(markus.anderljung@governance.ai)\nand\nAnton\nKorinek\n(akorinek@brookings.edu).\nCite as \"Frontier AI Regulation: Managing Emerging Risks to Public Safety.\" Anderljung, Barnhart, Korinek, Leung, O\u2019Keefe,\n& Whittlestone, et al, 2023.\narXiv:2307.03718v4  [cs.CY]  7 Nov 2023\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nABSTRACT\nAdvanced AI models hold the promise of tremendous benefits for humanity, but society\nneeds to proactively manage the accompanying risks. In this paper, we focus on what we\nterm \u201cfrontier AI\u201d models \u2014 highly capable foundation models that could possess dangerous\ncapabilities sufficient to pose severe risks to public safety. Frontier AI models pose a\ndistinct regulatory challenge: dangerous capabilities can arise unexpectedly; it is difficult to\nrobustly prevent a deployed model from being misused; and, it is difficult to stop a model\u2019s\ncapabilities from proliferating broadly. To address these challenges, at least three building\nblocks for the regulation of frontier models are needed: (1) standard-setting processes to\nidentify appropriate requirements for frontier AI developers, (2) registration and reporting\nrequirements to provide regulators with visibility into frontier AI development processes,\nand (3) mechanisms to ensure compliance with safety standards for the development and\ndeployment of frontier AI models. Industry self-regulation is an important first step. However,\nwider societal discussions and government intervention will be needed to create standards\nand to ensure compliance with them. We consider several options to this end, including\ngranting enforcement powers to supervisory authorities and licensure regimes for frontier\nAI models. Finally, we propose an initial set of safety standards. These include conducting\npre-deployment risk assessments; external scrutiny of model behavior; using risk assessments\nto inform deployment decisions; and monitoring and responding to new information about\nmodel capabilities and uses post-deployment. We hope this discussion contributes to the\nbroader conversation on how to balance public safety risks and innovation benefits from\nadvances at the frontier of AI development.\n2\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nExecutive Summary\nThe capabilities of today\u2019s foundation models highlight both the promise and risks of rapid advances in AI.\nThese models have demonstrated significant potential to benefit people in a wide range of fields, including\neducation, medicine, and scientific research. At the same time, the risks posed by present-day models, coupled\nwith forecasts of future AI progress, have rightfully stimulated calls for increased oversight and governance\nof AI across a range of policy issues. We focus on one such issue: the possibility that, as capabilities continue\nto advance, new foundation models could pose severe risks to public safety, be it via misuse or accident.\nAlthough there is ongoing debate about the nature and scope of these risks, we expect that government\ninvolvement will be required to ensure that such \"frontier AI models\u201d are harnessed in the public interest.\nThree factors suggest that frontier AI development may be in need of targeted regulation: (1) Models may\npossess unexpected and difficult-to-detect dangerous capabilities; (2) Models deployed for broad use can be\ndifficult to reliably control and to prevent from being used to cause harm; (3) Models may proliferate rapidly,\nenabling circumvention of safeguards.\nSelf-regulation is unlikely to provide sufficient protection against the risks from frontier AI models: govern-\nment intervention will be needed. We explore options for such intervention. These include:\nMechanisms to create and update safety standards for responsible frontier AI develop-\nment and deployment. These should be developed via multi-stakeholder processes, and could\ninclude standards relevant to foundation models overall, not exclusive to frontier AI. These\nprocesses should facilitate rapid iteration to keep pace with the technology.\nMechanisms to give regulators visibility into frontier AI development, such as disclosure\nregimes, monitoring processes, and whistleblower protections. These equip regulators with\nthe information needed to address the appropriate regulatory targets and design effective\ntools for governing frontier AI. The information provided would pertain to qualifying frontier\nAI development processes, models, and applications.\nMechanisms to ensure compliance with safety standards. Self-regulatory efforts, such as\nvoluntary certification, may go some way toward ensuring compliance with safety standards\nby frontier AI model developers. However, this seems likely to be insufficient without\ngovernment intervention, for example by empowering a supervisory authority to identify and\nsanction non-compliance; or by licensing the deployment and potentially the development of\nfrontier AI. Designing these regimes to be well-balanced is a difficult challenge; we should\nbe sensitive to the risks of overregulation and stymieing innovation on the one hand, and\nmoving too slowly relative to the pace of AI progress on the other.\nNext, we describe an initial set of safety standards that, if adopted, would provide some guardrails on the\ndevelopment and deployment of frontier AI models. Versions of these could also be adopted for current\nAI models to guard against a range of risks. We suggest that at minimum, safety standards for frontier AI\ndevelopment should include:\nConducting thorough risk assessments informed by evaluations of dangerous capabili-\nties and controllability. This would reduce the risk that deployed models possess unknown\ndangerous capabilities, or behave unpredictably and unreliably.\nEngaging external experts to apply independent scrutiny to models. External scrutiny\nof the safety and risk profile of models would both improve assessment rigor and foster\naccountability to the public interest.\n3\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFollowing standardized protocols for how frontier AI models can be deployed based on\ntheir assessed risk. The results from risk assessments should determine whether and how the\nmodel is deployed, and what safeguards are put in place. This could range from deploying\nthe model without restriction to not deploying it at all. In many cases, an intermediate\noption\u2014deployment with appropriate safeguards (e.g., more post-training that makes the\nmodel more likely to avoid risky instructions)\u2014may be appropriate.\nMonitoring and responding to new information on model capabilities. The assessed\nrisk of deployed frontier AI models may change over time due to new information, and new\npost-deployment enhancement techniques. If significant information on model capabilities is\ndiscovered post-deployment, risk assessments should be repeated, and deployment safeguards\nupdated.\nGoing forward, frontier AI models seem likely to warrant safety standards more stringent than those imposed\non most other AI models, given the prospective risks they pose. Examples of such standards include: avoiding\nlarge jumps in capabilities between model generations; adopting state-of-the-art alignment techniques; and\nconducting pre-training risk assessments. Such practices are nascent today, and need further development.\nThe regulation of frontier AI should only be one part of a broader policy portfolio, addressing the wide range\nof risks and harms from AI, as well as AI\u2019s benefits. Risks posed by current AI systems should be urgently\naddressed; frontier AI regulation would aim to complement and bolster these efforts, targeting a particular\nsubset of resource-intensive AI efforts. While we remain uncertain about many aspects of the ideas in this\npaper, we hope it can contribute to a more informed and concrete discussion of how to better govern the risks\nof advanced AI systems while enabling the benefits of innovation to society.\nAcknowledgements\nWe would like to express our thanks to the people who have offered feedback and input on the ideas in this\npaper, including Jon Bateman, Rishi Bommasani, Will Carter, Peter Cihon, Jack Clark, John Cisternino,\nRebecca Crootof, Allan Dafoe, Ellie Evans, Marina Favaro, Noah Feldman, Ben Garfinkel, Joshua Gotbaum,\nJulian Hazell, Lennart Heim, Holden Karnofsky, Jeremy Howard, Tim Hwang, Tom Kalil, Gretchen Krueger,\nLucy Lim, Chris Meserole, Luke Muehlhauser, Jared Mueller, Richard Ngo, Sanjay Patnaik, Hadrien Pouget,\nGopal Sarma, Girish Sastry, Paul Scharre, Mike Selitto, Toby Shevlane, Danielle Smalls, Helen Toner, and\nIrene Solaiman.\n4\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nContents\n1\nIntroduction\n6\n2\nThe Regulatory Challenge of Frontier AI Models\n7\n2.1\nWhat do we mean by frontier AI models?\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n2.2\nThe Regulatory Challenge Posed by Frontier AI . . . . . . . . . . . . . . . . . . . . . . . .\n9\n2.2.1\nThe Unexpected Capabilities Problem: Dangerous Capabilities Can Arise Unpre-\ndictably and Undetected . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n2.2.2\nThe Deployment Safety Problem: Preventing Deployed AI Models from Causing\nHarm is Difficult . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n13\n2.2.3\nThe Proliferation Problem: Frontier AI Models Can Proliferate Rapidly . . . . . . .\n13\n3\nBuilding Blocks for Frontier AI Regulation\n16\n3.1\nInstitutionalize Frontier AI Safety Standards Development\n. . . . . . . . . . . . . . . . . .\n16\n3.2\nIncrease Regulatory Visibility\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\n3.3\nEnsure Compliance with Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.3.1\nSelf-Regulation and Certification\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\n3.3.2\nMandates and Enforcement by Supervisory Authorities . . . . . . . . . . . . . . . .\n19\n3.3.3\nLicense Frontier AI Development and Deployment . . . . . . . . . . . . . . . . . .\n20\n3.3.4\nPre-conditions for Rigorous Enforcement Mechanisms . . . . . . . . . . . . . . . .\n21\n4\nInitial Safety Standards for Frontier AI\n23\n4.1\nConduct Thorough Risk Assessments Informed by Evaluations of Dangerous Capabilities\nand Controllability\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\n4.1.1\nAssessment for Dangerous Capabilities . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.1.2\nAssessment for Controllability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\n4.1.3\nOther Considerations for Performing Risk Assessments . . . . . . . . . . . . . . . .\n25\n4.2\nEngage External Experts to Apply Independent Scrutiny to Models . . . . . . . . . . . . . .\n26\n4.3\nFollow Standardized Protocols for how Frontier AI Models Can be Deployed Based on Their\nAssessed Risk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\n4.4\nMonitor and Respond to New Information on Model Capabilities . . . . . . . . . . . . . . .\n28\n4.5\nAdditional Practices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n5\nUncertainties and Limitations\n30\nA Creating a Regulatory Definition for Frontier AI\n34\nA.1\nDesiderata for a Regulatory Definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nA.2\nDefining Sufficiently Dangerous Capabilities\n. . . . . . . . . . . . . . . . . . . . . . . . .\n34\nA.3\nDefining Foundation Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nA.4\nDefining the Possibility of Producing Sufficiently Dangerous Capabilities\n. . . . . . . . . .\n35\nB\nScaling laws in Deep Learning\n37\n5\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n1\nIntroduction\nResponsible AI innovation can provide extraordinary benefits to society, such as delivering medical [1, 2,\n3, 4] and legal [5, 6, 7] services to more people at lower cost, enabling scalable personalized education [8],\nand contributing solutions to pressing global challenges like climate change [9, 10, 11, 12] and pandemic\nprevention [13, 14]. However, guardrails are necessary to prevent the pursuit of innovation from imposing\nexcessive negative externalities on society. There is increasing recognition that government oversight is\nneeded to ensure AI development is carried out responsibly; we hope to contribute to this conversation by\nexploring regulatory approaches to this end.\nIn this paper, we focus specifically on the regulation of frontier AI models, which we define as highly capable\nfoundation models1 that could have dangerous capabilities sufficient to pose severe risks to public safety and\nglobal security. Examples of such dangerous capabilities include designing new biochemical weapons [16],\nproducing highly persuasive personalized disinformation, and evading human control [17, 18, 19, 20, 21, 22,\n23].\nIn this paper, we first define frontier AI models and detail several policy challenges posed by them. We\nexplain why effective governance of frontier AI models requires intervention throughout the models\u2019 lifecycle,\nat the development, deployment, and post-deployment stages. Then, we describe approaches to regulating\nfrontier AI models, including building blocks of regulation such as the development of safety standards,\nincreased regulatory visibility, and ensuring compliance with safety standards. We also propose a set of initial\nsafety standards for frontier AI development and deployment. We close by highlighting uncertainties and\nlimitations for further exploration.\n1Defined as: \u201cany model that is trained on broad data (generally using self-supervision at scale) that can be adapted (e.g.,\nfine-tuned) to a wide range of downstream tasks\u201d [15].\n6\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n2\nThe Regulatory Challenge of Frontier AI Models\n2.1\nWhat do we mean by frontier AI models?\nFor the purposes of this paper, we define \u201cfrontier AI models\u201d as highly capable foundation models2 that\ncould exhibit sufficiently dangerous capabilities. Such harms could take the form of significant physical harm\nor the disruption of key societal functions on a global scale, resulting from intentional misuse or accident\n[25, 26]. It would be prudent to assume that next-generation foundation models could possess advanced\nenough capabilities to qualify as frontier AI models, given both the difficulty of predicting when sufficiently\ndangerous capabilities will arise and the already significant capabilities of today\u2019s models.\nThough it is not clear where the line for \u201csufficiently dangerous capabilities\u201d should be drawn, examples\ncould include:\n\u2022 Allowing a non-expert to design and synthesize new biological or chemical weapons.3\n\u2022 Producing and propagating highly persuasive, individually tailored, multi-modal disinformation with\nminimal user instruction.4\n\u2022 Harnessing unprecedented offensive cyber capabilities that could cause catastrophic harm.5\n\u2022 Evading human control through means of deception and obfuscation.6\nThis list represents just a few salient possibilities; the possible future capabilities of frontier AI models\nremains an important area of inquiry.\nFoundation models, such as large language models (LLMs), are trained on large, broad corpora of natural\nlanguage and other text (e.g., computer code), usually starting with the simple objective of predicting the\nnext \u201ctoken\u201d.7 This relatively simple approach produces models with surprisingly broad capabilities.8 These\n2[15] defines \u201cfoundation models\u201d as \u201cmodels (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are\nadaptable to a wide range of downstream tasks.\u201d See also [24].\n3Such capabilities are starting to emerge. For example, a group of researchers tasked a narrow drug-discovery system to identify\nmaximally toxic molecules. The system identified over 40,000 candidate molecules, including both known chemical weapons and\nnovel molecules that were predicted to be as or more deadly [16]. Other researchers are warning that LLMs can be used to aid in\ndiscovery and synthesis of compounds. One group attempted to create an LLM-based agent, giving it access to the internet, code\nexecution abilities, hardware documentation, and remote control of an automated \u2018cloud\u2019 laboratory. They report finding that it in\nsome cases the model was willing to outline and execute on viable methods for synthesizing illegal drugs and chemical weapons [27].\n4Generative AI models may already be useful to generate material for disinformation campaigns [28, 29, 30]. It is possible that,\nin the future, models could possess additional capabilities that could enhance the persuasiveness or dissemination of disinformation,\nsuch as by making such disinformation more dynamic, personalized, and multimodal; or by autonomously disseminating such\ndisinformation through channels that enhance its persuasive value, such as traditional media.\n5AI systems are already helpful in writing and debugging code, capabilities that can also be applied to software vulnerability\ndiscovery. There is potential for significant harm via automation of vulnerability discovery and exploitation. However, vulnerability\ndiscovery could ultimately benefit cyberdefense more than -offense, provided defenders are able to use such tools to identify and\npatch vulnerabilities more effectively than attackers can find and exploit them [31, 32].\n6If future AI systems develop the ability and the propensity to deceive their users, controlling their behavior could be extremely\nchallenging. Though it is unclear whether models will trend in that direction, it seems rash to dismiss the possibility and some argue\nthat it might be the default outcome of current training paradigms [17, 18, 20, 21, 22, 23].\n7A token can be thought of as a word or part of a word [33].\n8For example, LLMs achieve state-of-the-art performance in diverse tasks such as question answering, translation, multi-step\nreasoning, summarization, and code completion, among others [34, 35, 36, 37]. Indeed, the term \u201cLLM\u201d is already becoming\noutdated, as several leading \u201cLLMs\u201d are in fact multimodal (e.g., possess visual capabilities) [36, 38].\n7\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFigure 1: Example frontier AI lifecycle.\nmodels thus possess more general-purpose functionality9 than many other classes of AI models, such as\nthe recommender systems used to suggest Internet videos or generative AI models in narrower domains\nlike music. Developers often make their models available through \u201cbroad deployment\u201d via sector-agnostic\nplatforms such as APIs, chatbots, or via open-sourcing.10 This means that they can be integrated in a large\nnumber of diverse downstream applications, possibly including safety-critical sectors (illustrated in Figure 1).\nA number of features of our definition are worth highlighting. In focusing on foundation models which could\nhave dangerous, emergent capabilities, our definition of frontier AI excludes narrow models, even when these\nmodels could have sufficiently dangerous capabilities.11 For example, models optimizing for the toxicity of\ncompounds [16] or the virulence of pathogens could lead to intended (or at least foreseen) harms and thus\nmay be more appropriately covered with more targeted regulation.12\n9We intentionally avoid using the term \u201cgeneral-purpose AI\u201d to avoid confusion with the use of that term in the EU AI Act and\nother legislation. Frontier AI systems are a related but narrower class of AI systems with general-purpose functionality, but whose\ncapabilities are relatively advanced and novel.\n10We use \u201copen-source\u201d to mean \u201copen release:\u201d that is a model being made freely available online, be it with a license restricting\nwhat the system can be used for. An example of such a license is the Responsible AI License. Our usage of \u201copen-source\u201d differs\nfrom how the term is often used in computer science which excludes instances of license requirements, though is closer to how many\nother communities understand the term [39, 40].\n11However, if a foundation model could be fine-tuned and adapted to pose severe risk to public safety via capabilities in some\nnarrow domain, it would count as a \u201cfrontier AI.\u201d\n12Indeed, intentionally creating dangerous narrow models should already be covered by various laws and regulators. To the extent\nthat it is not clearly covered, modification of those existing laws and regulations would be appropriate and urgent. Further, the\n8\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nOur definition focuses on models that could \u2014 rather than just those that do \u2014 possess dangerous capabilities,\nas many of the practices we propose apply before it is known that a model has dangerous capabilities.\nOne approach to identifying models that could possess such capabilities is focusing on foundation models\nthat advance the state-of-the-art of foundation model capabilities. While currently deployed foundation\nmodels pose risks [15, 41], they do not yet appear to possess dangerous capabilities that pose severe risks to\npublic safety as we have defined them.13 Given both our inability to reliably predict what models will have\nsufficiently dangerous capabilities and the already significant capabilities today\u2019s models possess, it would\nbe prudent for regulators to assume that next-generation state-of-the-art foundation models could possess\nadvanced enough capabilities to warrant regulation.14 An initial way to identify potential state-of-the-art\nfoundation models could be focusing on models trained using above some very large amount of computational\nresources.15\nOver time, the scope of frontier AI should be further refined. The scope should be sensitive to features other\nthan compute; state-of-the-art performance can be achieved by using high quality data and new algorithmic\ninsights. Further, as systems with sufficiently dangerous capabilities are identified, it will be possible\nto identify training runs that are likely to produce such capabilities despite not achieving state-of-the-art\nperformance.\nWe acknowledge that our proposed definition is lacking in sufficient precision to be used for regulatory\npurposes and that more work is required to fully assess the advantages and limitations of different approaches.\nFurther, it is not our role to determine exactly what should fall within the scope of the regulatory proposals\noutlined \u2013 this will require more analysis and input from a wider range of actors. Rather, the aim of this\npaper is to present a set of initial proposals which we believe should apply to at least some subset of AI\ndevelopment. We provide a more detailed description of alternative approaches and the general complexity of\ndefining \u201cfrontier AI\u201d in Appendix A.\n2.2\nThe Regulatory Challenge Posed by Frontier AI\nThere are many regulatory questions related to the widespread use of AI [15]. This paper focuses on a specific\nsubset of concerns: the possibility that continued development of increasingly capable foundation models\ncould lead to dangerous capabilities sufficient to pose risks to public safety at even greater severity and scale\nthan is possible with current computational systems [25].\nMany existing and proposed AI regulations focus on the context in which AI models are deployed, such\nas high-risk settings like law enforcement and safety-critical infrastructure. These proposals tend to favor\nsector-specific regulations models.16 For frontier AI development, sector-specific regulations can be valuable,\nbut will likely leave a subset of the high severity and scale risks unaddressed.\nThree core problems shape the regulatory challenge posed by frontier AI models:\ndifference in mental state of the developer makes it much easier to identify and impose liability on developers of narrower dangerous\nmodels.\n13In some cases, these have been explicitly tested for [42].\n14We think it is prudent to anticipate that foundation models\u2019 capabilities may advance much more quickly than many expect, as\nhas arguably been the case for many AI capabilities: \u201c[P]rogress on ML benchmarks happened significantly faster than forecasters\nexpected. But forecasters predicted faster progress than I did personally, and my sense is that I expect somewhat faster progress than\nthe median ML researcher does.\u201d [43]; See [44] at 9; [45] at 11 (Chinchilla and Gopher surpassing forecaster predictions for progress\non MMLU); [36] (GPT-4 surpassing Gopher and Chinchilla on MMLU, also well ahead of forecaster predictions); [46, 47, 48, 49].\n15Perhaps more than any model that has been trained to date. Estimates suggest that 1E26 floating point operations (FLOP) would\nmeet this criteria [50].\n16This could look like imposing new requirements for AI models used in high-risk industries and modifying existing regulations to\naccount for new risks from AI models. See [24, 51, 52, 53, 54, 55].\n9\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nThe Unexpected Capabilities Problem. Dangerous capabilities can arise unpredictably and\nundetected, both during development and after deployment.\nThe Deployment Safety Problem. Preventing deployed AI models from causing harm is a\ncontinually evolving challenge.\nThe Proliferation Problem. Frontier AI models can proliferate rapidly, making accountabil-\nity difficult.\nThese problems make the regulation of frontier AI models fundamentally different from the regulation of\nother software, and the majority of other AI models. The Unexpected Capabilities Problem implies that\nfrontier AI models could have unpredictable or undetected dangerous capabilities that become accessible to\ndownstream users who are difficult to predict beforehand. Regulating easily identifiable users in a relatively\nsmall set of safety-critical sectors may therefore fail to prevent those dangerous capabilities from causing\nsignificant harm.17\nThe Deployment Safety Problem adds an additional layer of difficulty. Though many developers implement\nmeasures intended to prevent models from causing harm when used by downstream users, these may not\nalways be foolproof, and malicious users may constantly be attempting to evolve their attacks. Furthermore,\nthe Unexpected Capabilities Problem implies that the developer may not know of all of the harms from\nfrontier models that need to be guarded against during deployment. This amplifies the difficulty of the\nDeployment Safety Problem: deployment safeguards should address not only known dangerous capabilities,\nbut have the potential to address unknown ones too.\nThe Proliferation Problem exacerbates the regulatory challenge. Frontier AI models may be open-sourced, or\nbecome a target for theft by adversaries. To date, deployed models also tend to be reproduced or iterated on\nwithin several years. If, due to the Unexpected Capabilities Problem, a developer (knowingly or not) develops\nand deploys a model with dangerous capabilities, the Proliferation Problem implies that those capabilities\ncould quickly become accessible to unregulable actors like criminals and adversary governments.\nTogether, these challenges show that adequate regulation of frontier AI should intervene throughout the\nfrontier AI lifecycle, including during development, general-purpose deployment, and post-deployment\nenhancements.\n2.2.1\nThe Unexpected Capabilities Problem: Dangerous Capabilities Can Arise Unpredictably and\nUndetected\nImprovements in AI capabilities can be unpredictable, and are often difficult to fully understand without\nintensive testing. Regulation that does not require models to go through sufficient testing before deployment\nmay therefore fail to reliably prevent deployed models from posing severe risks.18\nOverall AI model performance19 has tended to improve smoothly with additional compute, parameters, and\ndata.20 However, specific capabilities can significantly improve quite suddenly in general-purpose models\nlike LLMs (see Figure 2). Though debated (see Appendix B), this phenomenom has been repeatedly observed\nin multiple LLMs with capabilities as diverse as modular arithmetic, unscrambling words, and answering\n17This is especially true for downstream bad actors (e.g., criminals, terrorists, adversary nations), who will tend not to be as\nregulable as the companies operating in domestic safety-critical sectors.\n18This challenge also exacerbates the Proliferation Problem: we may not know how important nonproliferation of a model is until\nafter it has already been open-sourced, reproduced, or stolen.\n19Measured by loss: essentially the error rate of an AI model performs on its training objective. We acknowledge that this is not a\ncomplete measure of model performance by any means.\n20See [56, 57, 45, 58, 59] However, there are tasks for which scaling leads to worse performance [60, 61, 62], though further\nscaling has overturned some of these findings, [36]. See also Appendix B.\n10\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFigure 2: Certain capabilities seem to emerge suddenly22\nquestions in Farsi [63, 64, 65, 66].21 Furthermore, given the vast set of possible tasks a foundation model\ncould excel at, it is nearly impossible to exhaustively test for them [15, 25]\nPost-deployment enhancements \u2014 modifications made to AI models after their initial deployment \u2014 can\nalso cause unaccounted-for capability jumps. For example, a key feature of many foundation models like\nLLMs is that they can be fine-tuned on new data sources to enhance their capabilities in targeted domains. AI\ncompanies often allow customers to fine-tune foundation models on task-specific data to improve the model\u2019s\nperformance on that task [68, 69, 70, 71]. This could effectively expand the scope of capability concerns of a\nparticular frontier AI model. Models could also be improved via \u201conline\u201d learning, where they continuously\nlearn from new data [72, 73].\nTo date, iteratively deploying models to subsets of users has been a key catalyst for understanding the outer\nlimits of model capabilities and weaknesses.23 For example, model users have demonstrated significant cre-\nativity in eliciting new capabilities from AI models, exceeding developers\u2019 expectations of model capabilities.\nUsers continue to discover prompting techniques that significantly enhance the model\u2019s performance, such as\nby simply asking an LLM to reason step-by-step [76]. This has been described as the \u201ccapabilities overhang\u201d\nof foundation models [77]. Users also discover new failure modes for AI systems long after their initial\n21For a treatment of recent critiques of the claim that AI models exhibit emergent capabilities, see Appendix B.\n22Chart from [63]. But see [67] for a skeptical view on emergence. For a response to the skeptical view, see [66] and Appendix B.\n23Dario Amodei, CEO of Anthropic: \u201cYou have to deploy it to a million people before you discover some of the things that it\ncan do. .. \u201d [74]. \u201cWe work hard to prevent foreseeable risks before deployment, however, there is a limit to what we can learn in a\nlab. Despite extensive research and testing, we cannot predict all of the beneficial ways people will use our technology, nor all the\nways people will abuse it. That\u2019s why we believe that learning from real-world use is a critical component of creating and releasing\nincreasingly safe AI systems over time\u201d [75].\n11\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nTechnique\nDescription\nExample\nFine-tuning\nImproving foundation model\nperformance\nby\nupdating\nmodel weights with task-\nspecific data.\nDetecting propaganda by fine-tuning\na pre-trained LLM on a labeled\ndataset of common propaganda tac-\ntics [84].\nChain-of-thought prompting [76]\nImproving\nLLM\nproblem-\nsolving capabilities by telling\nthe model to think through\nproblems step by step.\nAdding a phrase such as \u201cLet\u2019s think\nstep by step\u201d after posing a question\nto the model [85].\nExternal tool-use\nAllow the model to use ex-\nternal tools when figuring out\nhow to answer user queries.\nA model with access to a few simple\ntools (e.g., calculator, search engine)\nand a small number of examples per-\nforms much better than an unaided\nmodel.25\nAutomated prompt engineering [86]\nUsing LLMs to generate and\nsearch over novel prompts\nthat can be used to elicit bet-\nter performance on a task.\nTo generate prompts for a task, an\nLLM is asked something akin to: \u201cI\ngave a friend instructions and he re-\nsponded in this way for the given\ninputs:\n[Examples of inputs and\noutputs of the task] The instruction\nwas:\u201d\nFoundation model programs [87]\nCreation\nof\nstandardized\nmeans of integrating foun-\ndation\nmodels\ninto\nmore\ncomplex programs.\nLangchain: \u201ca framework for devel-\noping applications powered by lan-\nguage models.\u201d [88, 83]\nTable 1: Some known post-deployment techniques for unlocking new AI capabilities.\ndeployment. For example, one user found that the string \u201c solidgoldmagikarp\u201d caused GPT-3 to malfunction\nin a previously undocumented way, years after that model was first deployed [78].\nMuch as a carpenter\u2019s overall capabilities will vary with the tools she has available, so too might an AI\nmodel\u2019s overall capabilities vary depending on the tools it can use. LLMs can be taught to use, and potentially\ncreate, external tools like calculators and search engines [79, 80, 81]. Some models are also being trained to\ndirectly use general-purpose mouse and keyboard interfaces [82, 83]. See more examples in Table 1. As the\navailable tools improve, so can the overall capabilities of the total model-tool system, even if the underlying\nmodel is largely unchanged.24\n24Right now, most tools that AI models can use were originally optimized for use by people. As model-tool interactions become\nmore economically important, however, companies may develop tools optimized for use by frontier AI models, accelerating capability\nimprovements.\n25See [80]. Early research also suggests LLMs can be used to create tools for their own use [81].\n12\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nIn the long run, there are even more worrisome possibilities. Models behaving differently in testing compared\nto deployment is a known phenomenon in the field of machine learning, and is particularly worrisome if\nunexpected and dangerous behaviors first emerge \u201cin the wild\u201d only once a frontier model is deployed [89,\n90, 91].\n2.2.2\nThe Deployment Safety Problem: Preventing Deployed AI Models from Causing Harm is\nDifficult\nIn general, it is difficult to precisely specify what we want deep learning-based AI models to do and to ensure\nthat they behave in line with those specifications. Reliably controlling powerful AI models\u2019 behavior, in other\nwords, remains a largely unsolved technical problem [19, 17, 92, 93, 65] and the subject of ongoing research.\nTechniques to \u201cbake in\u201d misuse prevention features at the model level, such that the model reliably rejects or\ndoes not follow harmful instructions, can effectively mitigate these issues, but adversarial users have still\nfound ways to circumvent these safeguards in some cases. One technique for circumvention has been prompt\ninjection attacks, where attackers disguise input text as instructions from the user or developer to overrule\nrestrictions provided to or trained into the model. For example, emails sent to an LLM-based email assistant\ncould contain text constructed to look to the user as benign, but to the LLM contains instructions to exfiltrate\nthe user\u2019s data (which the LLM could then follow).26 Other examples include \u201cjailbreaking\u201d models by\nidentifying prompts that cause a model to act in ways discouraged by their developers [95, 96, 97]. Although\nprogress is being made on such issues [98, 99, 95, 42], it is unclear that we will be able to reliably prevent\ndangerous capabilities from being used in unintended or undesirable ways in novel situations; this remains an\nopen and fundamental technical challenge.\nA major consideration is that model capabilities can be employed for both harmful and beneficial uses:27\nthe harmfulness of an AI model\u2019s action may depend almost entirely on context that is not visible during\nmodel development. For example, copywriting is helpful when a company uses it to generate internal\ncommunications, but harmful when propagandists use it to generate or amplify disinformation. Use of a\ntext-to-image model to modify a picture of someone may be used with their consent as part of an art piece, or\nwithout their consent as a means of producing disinformation or harassment.\n2.2.3\nThe Proliferation Problem: Frontier AI Models Can Proliferate Rapidly\nThe most advanced AI models cost tens of millions of dollars to create.28 However, using the trained model\n(i.e., \u201cinference\u201d) is vastly cheaper.29 Thus, a much wider array of actors will have the resources to misuse\nfrontier AI models than have the resources to create them. Those with access to a model with dangerous\ncapabilities could cause harm at a significant scale, by either misusing the model themselves, or passing it on\nto actors who will misuse it.30 We describe some examples of proliferation in Table 2.\nCurrently, state-of-the-art AI capabilities can proliferate soon after development. One mechanism for prolifer-\nation is open-sourcing. At present, proliferation via open-sourcing of advanced AI models is common31 [114,\n115, 116] and usually unregulated. When models are open-sourced, obtaining access to their capabilities\nbecomes much easier: all internet users could copy and use them, provided access to appropriate computing\n26For additional examples, see [94].\n27Nearly all attempts to stop bad or unacceptable uses of AI also hinder positive uses, creating a Misuse-Use Tradeoff [100].\n28Though there are no estimates on the total cost of producing a frontier model, there are estimates of the cost of the compute used\nto train models [101, 102, 103]\n29Some impressive models can run on a offline portable device; see [104, 105, 106, 107].\n30Though advanced computing hardware accessed via the cloud tends to be needed to use frontier models. They can seldom be\nrun on consumer-grade hardware.\n31For an overview of considerations in how to release powerful AI models, see [108, 109, 110, 111, 112, 113].\n13\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFigure 3: Summary of the three regulatory challenges posed by frontier AI.\nresources. Open-source AI models can provide major economic utility by driving down the cost of accessing\nstate-of-the-art AI capabilities. They also enable academic research on larger AI models than would other-\nwise be practical, which improves the public\u2019s ability to hold AI developers accountable. We believe that\nopen-sourcing AI models can be an important public good. However, frontier AI models may need to be\nhandled more restrictively than their smaller, narrower, or less capable counterparts. Just as cybersecurity\nresearchers embargo security vulnerabilities to give the affected companies time to release a patch, it may\nbe prudent to avoid potentially dangerous capabilities of frontier AI models being open sourced until safe\ndeployment is demonstrably feasible.\nOther vectors for proliferation also imply increasing risk as capabilities advance. For example, though models\nthat are made available via APIs proliferate more slowly, newly announced results are commonly reproduced\nor improved upon32 within 1-2 years of the initial release. Many of the most capable models use simple\nalgorithmic techniques and freely available data, meaning that the technical barriers to reproduction can often\nbe low.33\nProliferation can also occur via theft. The history of cybersecurity is replete with examples of actors ranging\nfrom states to lone cybercriminals compromising comparably valuable digital assets [120, 121, 122, 123,\n124]. Many AI developers take significant measures to safeguard their models. However, as AI models\nbecome more useful in strategically important contexts and the difficulties of producing the most advanced\nmodels increase, well-resourced adversaries may launch increasingly sophisticated attempts to steal them\n[125, 126]. Importantly, theft is feasible before deployment.\nThe interaction and causes of the three regulatory challenges posed by frontier AI are summarized in Figure 3.\n32Below, we use \u201creproduction\u201d to mean some other actor producing a model that reaches at least the same performance as an\nexisting model.\n33Projects such as OpenAssistant [117] attempt to reproduce the functionality of ChatGPT; and alpaca [118] uses OpenAI\u2019s\ntext-davinci-003 model to train a new model with similar capabilities. For an overview, see [119].\n14\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nOriginal Model\nSubsequent Model\nTime to Proliferate34\nStyleGAN\nImmediate\nStyleGAN is a model by NVIDIA that generates photorealistic human faces using generative adversarial\nnetworks (GANs) [127]. NVIDIA first published about StyleGAN in December 2018 [128] and open-sourced\nthe model in February 2019. Following open-sourcing StyleGAN, sample images went viral through sites such\nas thispersondoesnotexist.com [129, 130]. Fake social media accounts using pictures from StyleGAN\nwere discovered later that year [131, 132].\nAlphaFold 2\nOpenFold\n\u223c2 years\nIn November 2020, DeepMind announced AlphaFold 2 [133]. It was \u201cthe first computational method that\ncan regularly predict protein structures with atomic accuracy even in cases in which no similar structure is\nknown\u201d [134]: a major advance in the biological sciences. In November 2022, a diverse group of researchers\nreproduced and open-sourced a similarly capable model named OpenFold [135]. OpenFold used much less\ndata to train than AlphaFold 2, and could be run much more quickly and easily [135].\nGPT-3\nGopher\n\u223c7 months\nOpenAI announced GPT-3, an LLM, in May 2020 [35]. In December 2021, DeepMind announced Gopher,\nwhich performed better than GPT-3 across a wide range of benchmarks. However, the Gopher model card\nsuggests that the model was developed significantly earlier, seven months after the GPT-3 announcement, in\nDecember 2020 [136].\nLLaMa\n\u223c1 week\nIn February 2023, Meta AI announced LLaMa, an LLM [137]. LLaMa was not open-sourced, but researchers\ncould apply for direct access to model weights [137]. Within a week, various users had posted these weights\non multiple websites, violating the terms under which the weights were distributed [138].\nChatGPT\nAlpaca\n\u223c3 months\nIn March 2023, researchers from Stanford University used sample completions from OpenAI\u2019s text-davinci-\n003 to fine-tune LLaMa in an attempt to recreate ChatGPT using less than $600.35 Their model was\nsubsequently taken offline due to concerns about cost and safety [140], though the code and documentation\nfor replicating the model is available on GitHub [141].\nTable 2: Examples of AI proliferation: these are not necessarily typical, and some of these examples may be\nbeneficial or benign, yet they demonstrate the consistent history of AI capabilities proliferating after their\ninitial deployment\n15\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n3\nBuilding Blocks for Frontier AI Regulation\nThe three problems described above imply that serious risks may emerge during the development and\ndeployment of a frontier AI model, not just when it is used in safety-critical sectors. Regulation of frontier\nAI models, then, must address the particular shape of the regulatory challenge: the potential unexpected\ndangerous capabilities; difficulty of deploying AI models safely; and the ease of proliferation.\nIn this section, we outline potential building blocks for the regulation of frontier AI. In the next section,\nwe describe a set of initial safety standards for frontier AI models that this regulatory regime could ensure\ndevelopers comply with.\nMuch of what we describe could be helpful frameworks for understanding how to address the range of\nchallenges posed by current AI models. We also acknowledge that much of the discussion below is most\nstraightforwardly applicable to the context of the United States. Nevertheless, we hope that other jurisdictions\ncould benefit from these ideas, with appropriate modifications.\nA regulatory regime for frontier AI would likely need to include a number of building blocks:\nMechanisms for development of frontier AI safety standards particularly via expert-\ndriven multi-stakeholder processes, and potentially coordinated by governmental bodies.\nOver time, these standards could become enforceable legal requirements to ensure that\nfrontier AI models are being developed safely.\nMechanisms to give regulators visibility into frontier AI development, such as disclosure\nregimes, monitoring processes, and whistleblower protection. These equip regulators with\nthe information needed to address the appropriate regulatory targets and design effective\ntools for governing frontier AI.\nMechanisms to ensure compliance with safety standards including voluntary self-\ncertification schemes, enforcement by supervisory authorities, and licensing regimes. While\nself-regulatory efforts, such as voluntary certification, may go some way toward ensuring\ncompliance, this seems likely to be insufficient for frontier AI models.\nGovernments could encourage the development of standards and consider increasing regulatory visibility\ntoday; doing so could also address potential harms from existing systems. We expand on the conditions under\nwhich more stringent tools like enforcement by supervisory authorities or licensing may be warranted below.\nRegulation of frontier AI should also be complemented with efforts to reduce the harm that can be caused\nby various dangerous capabilities. For example, in addition to reducing frontier AI model usefulness in\ndesigning and producing dangerous pathogens, DNA synthesis companies should screen for such worrying\ngenetic sequences [142, 100]. While we do not discuss such efforts to harden society against the proliferation\nof dangerous capabilities in this paper, we welcome such efforts from others.\n3.1\nInstitutionalize Frontier AI Safety Standards Development\nPolicymakers should support and initiate sustained, multi-stakeholder processes to develop and continually\nrefine the safety standards that developers of frontier AI models may be required to adhere to. To seed these\nprocesses, AI developers, in partnership with civil society and academia, can pilot practices that improve\n34The examples listed here are not necessarily the earliest instances of proliferation.\n35Note that the original paper and subsequent research suggests this method fails to match the capabilities of the larger model\n[118, 139].\n16\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nsafety during development and deployment [143, 144, 145, 146]. These practices could evolve into best\npractices and standards,36 eventually making their way into national [149] and international [150] standards.\nThe processes should involve, at a minimum, AI ethics and safety experts, AI researchers, academics,\nand consumer representatives. Eventually, these standards could form the basis for substantive regulatory\nrequirements [151]. We discuss possible methods for enforcing such legally required standards below.\nThough there are several such efforts across the US, UK, and EU, standards specific to the safe development\nand deployment of state-of-the-art foundation AI models are nascent.37 In particular, we currently lack a\nrobust, comprehensive suite of evaluation methods to operationalize these standards, and which capture the\npotentially dangerous capabilities and emerging risks that frontier AI systems may pose [25] Well-specified\nstandards and evaluation methods are a critical building block for effective regulation. Policymakers can play\na critical role in channeling investment and talent towards developing these standards with urgency.\nGovernments can advance the development of standards by working with stakeholders to create a robust\necosystem of safety testing capability and auditing organizations, seeding a third-party assurance ecosystem\n[155]. This can help with AI standards development in general, not just frontier AI standards. In particular,\ngovernments can pioneer the development of testing, evaluation, validation, and verification methods in\nsafety-critical domains, such as in defense, health care, finance, and hiring [156, 157, 158]. They can drive\ndemand for AI assurance by updating their procurement requirements for high-stakes systems [159] and\nfunding research on emerging risks from frontier AI models, including by offering computing resources\nto academic researchers [158, 160, 161]. Guidance on how existing rules apply to frontier AI can further\nsupport the process by, for example, operationalizing terms like \u201crobustness\u201d [162, 163, 164].\nThe development of standards also provides an avenue for broader input into the regulation of frontier AI.\nFor example, it is common to hold Request for Comment processes to solicit input on matters of significant\npublic import, such as standardization in privacy [165], cybersecurity [166], and algorithmic accountability\n[167].\nWe offer a list of possible initial substantive safety standards below.\n3.2\nIncrease Regulatory Visibility\nInformation is often considered the \u201clifeblood\u201d of effective governance.38 For regulators to positively impact\na given domain, they need to understand it. Accordingly, regulators dedicate significant resources to collecting\ninformation about the issues, activities, and organizations they seek to govern [171, 172].\nRegulating AI should be no exception [173]. Regulators need to understand the technology, and the resources,\nactors, and ecosystem that create and use it. Otherwise, regulators may fail to address the appropriate\nregulatory targets, offer ineffective regulatory solutions, or introduce regulatory regimes that have adverse\nunintended consequences.39 This is particularly challenging for frontier AI, but certainly holds true for\nregulating AI systems writ large.\nThere exist several complementary approaches to achieving regulatory visibility [169]. First, regulators\ncould develop a framework that facilitates AI companies voluntarily disclosing information about frontier\n36Examples of current fora include: [147, 148].\n37In the US, the National Institute for Standards and Technology has produced the AI Risk Management Framework and the\nNational Telecommunication and Information Agency has requested comments on what policies can support the development of AI\nassurance. The UK has established an AI Standards Hub. The EU Commission has tasked European standardization organizations\nCEN and CENELEC to develop standards related to safe and trustworthy AI, to inform its forthcoming AI Act [149, 152, 153, 154].\n38See [168] (but see claims in article regarding the challenge of private incentives), [169] (see p282 regarding the need for\ninformation and 285 regarding industry\u2019s informational advantage), [170].\n39This is exacerbated by the pacing problem [174], and regulators\u2019 poor track record of monitoring platforms (LLM APIs are\nplatforms) [172].\n17\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nAI, or foundation models in general. This could include providing documentation about the AI models\nthemselves [175, 176, 177, 178, 179], as well as the processes involved in developing them [180]. Second,\nregulators could mandate these or other disclosures, and impose reporting requirements on AI companies, as\nis commonplace in other industries.40 Third, regulators could directly, or via third parties, audit AI companies\nagainst established safety and risk-management frameworks [182] (on auditing, see [183, 184]). Finally, as\nin other industries, regulators could establish whistleblower regimes that protect individuals who disclose\nsafety-critical information to relevant government authorities [185, 186].\nIn establishing disclosure and reporting schemes, it is critical that the sensitive information provided about\nfrontier AI models and their owners is protected from adversarial actors. The risks of information leakage can\nbe mitigated by maintaining high information security, reducing the amount and sensitivity of the information\nstored (by requiring only clearly necessary information, and by having clear data retention policies), and only\ndisclosing information to a small number of personnel with clear classification policies.\nAt present, regulatory visibility into AI models in general remains limited, and is generally provided by\nnongovernmental actors [187, 188, 189]. Although these private efforts offer valuable information, they are\nnot a substitute for more strategic and risk-driven regulatory visibility. Nascent governmental efforts towards\nincreasing regulatory visibility should be supported and redoubled, for frontier AI as well as for a wider range\nof AI models.41\n3.3\nEnsure Compliance with Standards\nConcrete standards address the challenges presented by frontier AI development only insofar as they are\ncomplied with. This section discusses a non-exhaustive list of actions that governments can take to ensure\ncompliance, potentially in combination, including: encouraging voluntary self-regulation and certification;\ngranting regulators powers to detect and issue penalties for non-compliance; and requiring a license to develop\nand/or deploy frontier AI. The section concludes by discussing pre-conditions that should inform when and\nhow such mechanisms are implemented.\nSeveral of these ideas could be suitably applied to the regulation of AI models overall, particularly foundation\nmodels. However, as we note below, interventions like licensure regimes are likely only warranted for the\nhighest-risk AI activities, where there is evidence of sufficient chance of large-scale harm and other regulatory\napproaches appear inadequate.\n3.3.1\nSelf-Regulation and Certification\nGovernments can expedite industry convergence on and adherence to safety standards by creating or fa-\ncilitating multi-stakeholder frameworks for voluntary self-regulation and certification, by implementing\nbest-practice frameworks for risk governance internally [192], and by encouraging the creation of third parties\nor industry bodies capable of assessing a company\u2019s compliance with these standards [193]. Such efforts\nboth incentivize compliance with safety standards and also help build crucial organizational infrastructure\nand capacity to support a broad range of regulatory mechanisms, including more stringent approaches.\n40One of many examples from other industries is the Securities and Exchange Act of 1934, which requires companies to disclose\nspecific financial information in annual and quarterly reports. But see [181] regarding the shortcomings of mandatory disclosure.\n41The EU-US TTC Joint Roadmap discusses \u201cmonitoring and measuring existing and emerging AI risks\u201d [190]. The EU\nParliament\u2019s proposed AI Act includes provisions on the creation of an AI Office, which would be responsible for e.g. \u201cissuing\nopinions, recommendations, advice or guidance\u201d, see [24, recital 76]. The UK White Paper \u201cA pro-innovation approach to AI\nregulation\u201d proposes the creation of a central government function aimed at e.g. monitoring and assessing the regulatory environment\nfor AI [191, box 3.3].\n18\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nWhile voluntary standards and certification schemes can help establish industry baselines and standardize\nbest practices,42 self-regulation alone will likely be insufficient for frontier AI models, and likely today\u2019s\nstate-of-the-art foundation models in general. Nonetheless, self-regulation and certification schemes often\nserve as the foundation for other regulatory approaches [194], and regulators commonly draw on the expertise\nand resources of the private sector [195, 151]. Given the rapid pace of AI development, self-regulatory\nschemes may play an important role in building the infrastructure necessary for formal regulation.43\n3.3.2\nMandates and Enforcement by Supervisory Authorities\nA more stringent approach is to mandate compliance with safety standards for frontier AI development and\ndeployment, and empower a supervisory authority44 to take administrative enforcement measures to ensure\ncompliance. Administrative enforcement can help further several important regulatory goals, including\ngeneral and specific deterrence through public case announcements and civil penalties, and the ability to\nenjoin bad actors from participating in the marketplace.\nSupervisory authorities could \u201cname and shame\u201d non-compliant developers. For example, financial supervi-\nsory authorities in the US and EU publish their decisions to impose administrative sanctions in relation to\nmarket abuse (e.g. insider trading or market manipulation) on their websites, including information about the\nnature of the infringement, and the identity of the person subject to the decision.45 Public announcements,\nwhen combined with other regulatory tools, can serve an important deterrent function.\nThe threat of significant administrative fines or civil penalties may provide a strong incentive for companies\nto ensure compliance with regulator guidance and best practices. For particularly egregious instances of\nnon-compliance and harm,46 supervisory authorities could deny market access or consider more severe\npenalties.47 Where they are required for market access, the supervisory authority can revoke governmental\nauthorizations such as licenses, a widely available regulatory tool in the financial sector.48 Market access\ncan also be denied for activity that does not require authorization. For example, the Sarbanes-Oxley Act\nenables the US Securities and Exchange Commission to bar people from serving as directors or officers of\npublicly-traded companies [199].\n42Such compliance can be incentivized via consumer demand [193].\n43Some concrete examples include:\n\u2022 In the EU\u2019s so-called \u201cNew Approach\u201d to product safety adopted in the 1980s, regulation always relies on standards to\nprovide the technical specifications, such as how to operationalize \u201csufficiently robust.\u201d [196]\n\u2022 WTO members have committed to use international standards so far as possible in domestic regulation [197, \u00a72.4].\n44We do not here opine on which new or existing agencies would be best for this, though this is of course a very important question.\n45For the EU, see, e.g.,: Art. 34(1) of Regulation (EU) No 596/2014 (MAR). For the US, see, e.g., [198].\n46For example, if a company repeatedly released frontier models that could significantly aid cybercriminal activity, resulting in\nbillions of dollars worth of counterfactual damages, as a result of not complying with mandated standards and ignoring repeated\nexplicit instructions from a regulator.\n47For example, a variety of financial misdeeds\u2014such as insider trading and securities fraud\u2014are punished with criminal sentences.\n18 U.S.C. \u00a7 1348; 15 U.S.C. \u00a7 78j(b)\n48For example, in the EU, banks and investment banks require a license to operate, and supervisory authorities can revoke\nauthorization under certain conditions.\n\u2022 Art. 8(1) of Directive 2013/36/EU (CRD IV)\n\u2022 Art. 6(1) of Directive 2011/61/EU (AIFMD) and Art. 5(1) of Directive 2009/65/EC (UCITS)\n\u2022 Art. 18 of Directive 2013/36/EU (CRD IV), Art. 11 of Directive 2011/61/EU (AIFMD), Art. 7(5) of Directive 2009/65/EC\n(UCITS)\nIn the US, the SEC can revoke a company\u2019s registration, which effectively ends the ability to publicly trade stock in the company. 15\nU.S.C. \u00a7 78l(j).\n19\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nAll administrative enforcement measures depend on adequate information. Regulators of frontier AI systems\nmay require authority to gather information, such as the power to request information necessary for an\ninvestigation, conduct site investigations,49 and require audits against established safety and risk-management\nframeworks. Regulated companies could also be required to proactively report certain information, such as\naccidents above a certain level of severity.\n3.3.3\nLicense Frontier AI Development and Deployment\nEnforcement by supervisory authorities penalizes non-compliance after the fact. A more anticipatory,\npreventative approach to ensuring compliance is to require a governmental license to widely deploy a frontier\nAI model, and potentially to develop it as well.50 Licensure and similar \u201cpermissioning\u201d requirements are\ncommon in safety-critical and other high-risk industries, such as air travel [207, 208], power generation [209],\ndrug manufacturing [210], and banking [211]. While details differ, regulation of these industries tends to\nrequire someone engaging in a safety-critical or high-risk activity to first receive governmental permission to\ndo so; to regularly report information to the government; and to follow rules that make that activity safer.\nLicensing is only warranted for the highest-risk AI activities, where evidence suggests potential risk of large-\nscale harm and other regulatory approaches appear inadequate. Imposing such measures on present-day AI\nsystems could potentially create excessive regulatory burdens for AI developers which are not commensurate\nwith the severity and scale of risks posed. However, if AI models begin having the potential to pose risks to\npublic safety above a high threshold of severity, regulating such models similarly to other high-risk industries\nmay become warranted.\nThere are at least two stages at which licensing for frontier AI could be required: deployment and develop-\nment.51 Deployment-based licensing is more analogous to licensing regimes common among other high-risk\nactivities. In the deployment licensing model, developers of frontier AI would require a license to widely\ndeploy a new frontier AI model. The deployment license would be granted and sustained if the deployer\ndemonstrated compliance with a specified set of safety standards (see below). This is analogous to the\nregulatory approach in, for example, pharmaceutical regulation, where drugs can only be commercially sold\nif they have gone through proper testing [212].\nHowever, requiring licensing for deployment of frontier AI models alone may be inadequate if they are\npotentially capable of causing large scale harm; licenses for development may be a useful complement.\nFirstly, as discussed above, there are reasonable arguments to begin regulation at the development stage,\nespecially because frontier AI models can be stolen or leaked before deployment. Ensuring that development\n(not just deployment) is conducted safely and securely would therefore be paramount. Secondly, before\nmodels are widely deployed, they are often deployed at a smaller scale, tested by crowdworkers and used\ninternally, blurring the distinction between development and deployment in practice. Further, certain models\nmay not be intended for broad deployment, but instead be used to, for example, develop intellectual property\nthat the developer then distributes via other means. In sum, models could have a significant impact before\n49For examples of such powers in EU law, see Art. 58(1) of Regulation (EU) 2016/679 (GDPR) and Art. 46(2) of Directive\n2011/61/EU (AIFMD). For examples in US law, see [200, 201].\n50Jason Matheny, CEO of RAND Corporation: \u201cI think we need a licensing regime, a governance system of guardrails around the\nmodels that are being built, the amount of compute that is being used for those models, the trained models that in some cases are\nnow being open sourced so that they can be misused by others. I think we need to prevent that. And I think we are going to need a\nregulatory approach that allows the Government to say tools above a certain size with a certain level of capability can\u2019t be freely\nshared around the world, including to our competitors, and need to have certain guarantees of security before they are deployed\u201d\n[202]. See also [203], and statements during the May 16th 2023 Senate hearing of the Subcommittee on Privacy, Technology, and the\nLaw regarding Rules for Artificial Intelligence [204]. U.S. public opinion polling has also looked at the issue. A January 2022 poll\nfound 52 percent support for a regulator providing pre-approval of certain AI systems, akin to the FDA [205], whereas an April\nsurvey found 70 percent support [206].\n51In both cases, one could license either the activity or the entity.\n20\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nbroad deployment. As an added benefit, providing a regulator the power to oversee model development could\nalso promote regulatory visibility, thus allowing regulations to adapt more quickly [182].\nA licensing requirement for development could, for example, require that developers have sufficient security\nmeasures in place to protect their models from theft, and that they adopt risk-reducing organizational practices\nsuch as establishing risk and safety incident registers and conducting risk assessments ahead of beginning\na new training run. It is important that such requirements are not overly burdensome for new entrants; the\ngovernment could provide subsidies and support to limit the compliance costs for smaller organizations.\nThough less common, there are several domains where approval is needed in the development stage, especially\nwhere significant capital expenditures are involved and where an actor is in possession of a potentially\ndangerous object. For example, experimental aircraft in the US require a special experimental certification in\norder to test, and operate under special restrictions.52 Although this may be thought of as mere \u201cresearch and\ndevelopment,\u201d in practice, research into and development of experimental aircraft will, as with frontier AI\nmodels, necessarily create some significant risks. Another example is the US Federal Select Agent Program\n[213], which requires (most) individuals who possess, use, or transfer certain highly risky biological agents\nor toxins [214] to register with the government;53 comply with regulations about how such agents are handled\n[216]; perform security risk assessments to prevent possible bad actors from gaining access to the agents\n[217]; and submit to inspections to ensure compliance with regulations [218].\n3.3.4\nPre-conditions for Rigorous Enforcement Mechanisms\nWhile we believe government involvement will be necessary to ensure compliance with safety standards for\nfrontier AI, there are potential downsides to rushing regulation. As noted above, we are still in the nascent\nstages of understanding the full scope, capabilities, and potential impact of these technologies. Premature\ngovernment action could risk ossification, and excessive or poorly targeted regulatory burdens. This highlights\nthe importance of near-term investment in standards development, and associated evaluation and assessment\nmethods to operationalize these standards. Moreover, this suggests that it would be a priority to ensure that\nthe requirements are regularly updated via technically-informed processes.\nA particular concern is that regulation would excessively thwart innovation, including by burdening research\nand development on AI reliability and safety, thereby exacerbating the problems that regulation is intended to\naddress. Governments should thus take considerable care in deciding whether and how to regulate AI model\ndevelopment, minimizing the regulatory burden as much as possible \u2013 in particular for less-resourced actors \u2013\nand focusing on what is necessary for meeting the described policy objectives.\nThe capacity to staff regulatory bodies with sufficient expertise is also crucial for effective regulation.\nInsufficient expertise increases the risk that information asymmetries between the regulated industry and\nregulators lead to regulatory capture [219], and reduce meaningful enforcement. Such issues should be\nanticipated and mitigated.54 Investing in building and attracting expertise in AI, particularly at the frontier,\n5214 CFR \u00a7 91.319.\n5342 C.F.R. \u00a7 73.7. The US government maintains a database about who possess and works with such agents [215].\n54Policies to consider include:\n\u2022 Involving a wide array of interest groups in rulemaking.\n\u2022 Relying on independent expertise and performing regular reassessments of regulations.\n\u2022 Imposing mandatory \u201ccooling off\u201d periods between former regulators working for regulateess.\n\u2022 Rotating roles in regulatory bodies.\nSee [220, 221].\n21\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nshould be a governmental priority.55 Even with sufficient expertise, regulation can increase the power of\nincumbents, and that this should be actively combated in the design of regulation.\nDesigning an appropriately balanced and adaptable regulatory regime for a fast moving technology is a\ndifficult challenge, where timing and path dependency matter greatly. It is crucial to regulate AI technologies\nwhich could have significant impacts on society, but it is also important to be aware of the challenges of\ndoing so well. It behooves lawmakers, policy experts, and scholars to invest both urgently and sufficiently in\nensuring that we have a strong foundation of standards, expertise, and clarity on the regulatory challenge\nupon which to build frontier AI regulation.\n55In the US, TechCongress\u2014a program that places computer scientists, engineers, and other technologists to serve as technology\npolicy advisors to Members of Congress\u2014is a promising step in the right direction [222], but is unlikely to be sufficient. There are\nalso a number of private initiatives with similar aims (e.g., [223]. In the UK, the White Paper on AI regulation highlights the need to\nengage external expertise [191, Section 3.3.5]. See also the report on regulatory capacity for AI by the Alan Turing Institute [224].\n22\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n4\nInitial Safety Standards for Frontier AI\nWith the above building blocks in place, policymakers would have the foundations of a regulatory regime\nwhich could establish, ensure compliance with, and evolve safety standards for the development and deploy-\nment of frontier AI models. However, the primary substance of the regulatory regime\u2014what developers\nwould have to do to ensure that their models are developed and deployed safely\u2014has been left undefined.\nWhile much remains to specify what such standards should be, we suggest a set of standards, which we\nbelieve would meaningfully mitigate risk from frontier AI models. These standards would also likely be\nappropriate for current AI systems, and are being considered in various forms in existing regulatory proposals:\nConduct thorough risk assessments informed by evaluations of dangerous capabilities\nand controllability. This would reduce the risk that deployed models present dangerous\ncapabilities, or behave unpredictably and result in significant accidents.\nEngage external experts to apply independent scrutiny to models. External scrutiny of the\nmodels for safety issues and risks would improve assessment rigor and foster accountability\nto the public interest.\nFollow standardized protocols for how frontier AI models can be deployed based on\ntheir assessed risk. The results from risk assessments should determine whether and how\nthe model is deployed, and what safeguards are put in place.\nMonitor and respond to new information on model capabilities. If new, significant\ninformation on model capabilities and risks is discovered post-deployment, risk assessments\nshould be repeated, and deployment safeguards updated.\nThe above practices are appropriate not only for frontier AI models but also for other foundation models.\nThis is in large part because frontier-AI-specific standards are still nascent. We describe additional practices\nthat may only be appropriate for frontier AI models given their particular risk profile, and which we can\nimagine emerging in the near future from standard setting processes. As the standards for frontier AI models\nare made more precise, they are likely to diverge from and become more intensive than those appropriate for\nother AI systems.\n4.1\nConduct Thorough Risk Assessments Informed by Evaluations of Dangerous Capabilities\nand Controllability\nThere is a long tradition in AI ethics of disclosing key risk-relevant features of AI models to standardize\nand improve decision making [175, 176, 225, 226]. In line with that tradition, an important safety standard\nis performing assessments of whether a model could pose severe risks to public safety and global security\n[227]. Given our current knowledge, two assessments seem especially informative of risk from frontier AI\nmodels specifically: (1) which dangerous capabilities does or could the model possess, if any?, and (2) how\ncontrollable is the model?56\n56For a longer treatment of the role such evaluations can play, see [25].\n23\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n4.1.1\nAssessment for Dangerous Capabilities\nAI developers should assess their frontier AI models for dangerous capabilities during57 and immediately\nafter training.58 Examples of such capabilities include designing new biochemical weapons, and persuading\nor inducing a human to commit a crime to advance some goal.\nEvaluation suites for AI models are common and should see wider adoption, though most focus on general\ncapabilities rather than specific risks.59 Currently, dangerous capability evaluations largely consist of defining\nan undesirable model behavior, and using a suite of qualitative and bespoke techniques such as red-teaming\nand boundary testing [232, 233, 234, 235] for determining whether this behavior can be elicited from the\nmodel [236].\nCurrent evaluation methods for frontier AI are in the early stages of development and lack many desirable\nfeatures. As the field matures, effort should focus on making evaluations more:\n\u2022 Standardized (i.e., can be consistently applicable across models);\n\u2022 Objective (i.e., relying as little as possible on an evaluator\u2019s judgment or discretion);\n\u2022 Efficient (i.e. lower cost to perform);\n\u2022 Privacy-preserving (i.e., reducing required disclosure of proprietary or sensitive data and methods);\n\u2022 Automatable (i.e., relying as little as possible on human input);\n\u2022 Safe to perform (e.g., can be conducted in sandboxed or simulated environments as necessary to\navoid real-world harm);\n\u2022 Strongly indicative of a model\u2019s possession of dangerous capabilities;\n\u2022 Legitimate (e.g., in cases where the evaluation involves difficult trade-offs, using a decision-making\nprocess grounded in legitimate sources of governance).\nEvaluation results could be used to inform predictions of a models\u2019 potential dangerous capabilities prior to\ntraining, allowing developers to intentionally steer clear of models with certain dangerous capabilities [25].\nFor example, we may discover scaling laws, where a model\u2019s dangerous capabilities can be predicted by\nfeatures such as its training data, algorithm, and compute.60\n4.1.2\nAssessment for Controllability\nEvaluations of controllability \u2013 that is, the extent to which the model reliably does what its user or developer\nintends \u2013 are also necessary for frontier models, though may prove more challenging than those for dangerous\ncapabilities. These evaluations should be multi-faceted, and conducted in proportion to the capabilities of the\nmodel. They might look at the extent to which users tend to judge a model\u2019s outputs as appropriate and helpful\n57Training a frontier AI model can take several months. It is common for AI companies to make a \u201ccheckpoint\u201d copy of a model\npartway through training, to analyze how training is progressing. It may be sensible to require AI companies to perform assessments\npart-way through training, to reduce the risk that dangerous capabilities that emerge partway through training proliferate or are\ndangerously enhanced.\n58In a recent expert survey (N = 51), 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-\ndeployment risk assessments as well as dangerous capabilities evaluations, while 94% somewhat or strongly agreed that they should\nconduct pre-training risk assessments [148].\n59Some common benchmarks for evaluating LLM capabilities include [228, 229, 230, 231].\n60Existing related examples include: inverse scaling law [237, 238, 234, 239]. See also Appendix B.\n24\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[240].61 They could look at whether the models hallucinate [242] or produce unintentional toxic content\n[243]. They may also assess model harmlessness: the extent to which the model refuses harmful user requests\n[244]. This includes robustness to adversarial attempts intended to elicit model behavior that the developer\ndid not intend, as has already been observed in existing models [94]. More extreme, harder-to-detect failures\nshould also be assessed, such as the model\u2019s ability to deceive evaluators of its capabilities to evade oversight\nor control [61].\nEvaluations of controllability could also extend to assessing the causes of model behavior [245, 246, 247].\nIn particular, it seems important to understand what pathways (\u201cactivations\u201d) lead to downstream model\nbehaviors that may be undesirable. For example, if a model appears to have an internal representation of a\nuser\u2019s beliefs, and this representation plays a part in what the model claims to be true when interacting with\nthat user, this suggests that the model has the capability to manipulate users based on their beliefs.62 Scalable\ntooling and efficient techniques for navigating enormous models and datasets could also allow developers to\nmore easily audit model behavior [248, 249]. Evaluating controllability remains an open area of research\nwhere more work is needed to ensure techniques and tools are able to adequately minimize the risk that\nfrontier AI could undermine human control.\n4.1.3\nOther Considerations for Performing Risk Assessments\nRisk is often contextual. Managing dangerous capabilities can depend on understanding interactions between\nfrontier AI models and features of the world. Many risks result from capabilities that are dual-use [100, 250]:\npresent-day examples include the generation of persuasive, compelling text, which is core to current model\nfunctionality but can also be used to scale targeted misinformation. Thus, simply understanding capabilities\nis not enough: regulation must continuously map the interaction of these capabilities with wider systems of\ninstitutions and incentives.63 Context is not only important to assessing risk, but is often also necessary to\nadjudicate tradeoffs between risk and reward [149, p. 7].\nRisk can also be viewed counterfactually. For example, whether a given capability is already widely available\nmatters. A frontier AI model\u2019s capabilities should only be considered dangerous if access to them significantly\nincreases the risk of harm relative to what was attainable without access to the model. If information on how\nto make a type of weapon is already easily accessible, then the effect of a model should be evaluated with\nreference to the ease of making such weapons without access to the model.64\nRisk assessments should also account for possible defenses. As society\u2019s capability to manage risks from\nAI improves, the riskiness of individual AI models may decrease.65 Indeed, one of the primary uses of safe\nfrontier AI models could be making society more robust to harms from AI and other emerging technologies\n[253, 254, 255, 240, 61, 98, 32]. Deploying them asymmetrically for beneficial (including defensive) purposes\ncould improve society overall.\n61This is also somewhat related to the issue of over reliance on AI systems, as discussed in e.g. [241].\n62See result regarding model \u201csycophancy\u201d [61].\n63The UK Government plans to take a \u201ccontext-based\u201d approach to AI regulation [191]: \u201cwe will acknowledge that AI is a\ndynamic, general purpose technology and that the risks arising from it depend principally on the context of its application\u201d. See also\nthe OECD Framework for the Classification of AI Systems [251] and the NIST AI Risk Management Framework [149, p. 1]. See\nalso discussion of evaluation-in-society in [252].\n64This is the approach used in risk assessments for GPT-4 in its System Card [42].\n65Similarly, the overall decision on whether to deploy a system should consider not just assessed risk, but also the benefits that\nresponsibly deploying a system could yield.\n25\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n4.2\nEngage External Experts to Apply Independent Scrutiny to Models\nHaving rigorous external scrutiny applied to AI models,66 particularly prior to deployment, is important to\nensuring that the risks are assessed thoroughly and objectively, complementing internal testing processes,\nwhile also providing avenues for public accountability.67 Mechanisms include third-party audits of risk\nassessment procedures and outputs68 [257, 235, 258, 259, 260, 183, 184, 261] and engaging external expert\nred-teamers, including experts from government agencies69 [235]. These mechanisms could be helpfully\napplied to AI models overall, not just frontier AI models.\nThe need for creativity and judgment in evaluations of advanced AI models calls for innovative institutional\ndesign for external scrutiny. Firstly, it is important that auditors and red-teamers are sufficiently expert and\nexperienced in interacting with state-of-the-art AI models such that they can exercise calibrated judgment,\nand can execute on what is often the \u201cart\u201d of eliciting capabilities from novel AI models. Secondly, auditors\nand red-teamers should be provided with enough access to the AI model (including system-level features that\nwould potentially be made available to downstream users) such that they can conduct wide-ranging testing\nacross different threat models, under close-to-reality conditions as a simulated downstream user.\nThirdly, auditors and red teamers need to be adequately resourced,70 informed, and granted sufficient time\nto conduct their work at a risk-appropriate level of rigor, not least due to the risk that shallow audits or\nred teaming efforts provide a sense of false assurance. Fourthly, it is important that results from external\nassessments are published or communicated to an appropriate regulator, while being mindful of privacy,\nproprietary information, and the risks of proliferation. Finally, given the common practice of post-deployment\nmodel updates, the external scrutiny process should be structured to allow external parties to quickly assess\nproposed changes to the model and its context before these changes are implemented.\n4.3\nFollow Standardized Protocols for how Frontier AI Models Can be Deployed Based on\nTheir Assessed Risk\nThe AI model\u2019s risk profile should inform whether and how the system is deployed. There should be clear\nprotocols established which define and continuously adjust the mapping between a system\u2019s risk profile and\nthe particular deployment rules that should be followed. An example mapping specifically for frontier AI\nmodels could go as follows, with concrete examples illustrated in Table 3.\nNo assessed severe risk. If assessments determine that the model\u2019s use is incredibly\nunlikely to pose severe risks to public safety, even assuming substantial post-deployment\nenhancements, then there should be no need for additional deployment restrictions from\nfrontier AI regulation (although certainly, restrictions from other forms of AI regulation\ncould and should continue to apply).\nNo discovered severe risks, but notable uncertainty. In some cases the risk assessment\nmay be notably inconclusive. This could be due to uncertainty around post-deployment\nenhancement techniques (e.g., new methods for fine-tuning, or chaining a frontier AI model\nwithin a larger system) that may enable the same model to present more severe risks. In\n66External scrutiny may also need to be applied to, for example, post-deployment monitoring and broader risk assessments.\n67In a recent expert survey (N = 51), 98% of respondents somewhat or strongly agreed that AGI labs should conduct third-party\nmodel audits and red teaming exercises; 94% thought that labs should increase the level of external scrutiny in proportion to the\ncapabilities of their models; 87% supported third-party governance audits; and 84% agreed that labs should give independent\nresearchers API access to deployed models [148].\n68This would follow the pattern in industries like finance and construction. In these industries, regulations mandate transparency\nto external auditors whose sign-off is required for large-scale projects. See [256].\n69The external scrutiny processes of two leading AI developers are described in [42, 233, 262].\n70One important resource is sharing of best practices and methods for red teaming and third party auditing.\n26\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nsuch cases, it may be appropriate to have additional restrictions on the transfer of model\nweights to high risk parties, and implement particularly careful monitoring for evidence that\nnew post-deployment enhancements meaningfully increase risk. After some monitoring\nperiod (e.g. 12 months), absent clear evidence of severe risks, models could potentially be\ndesignated as posing \u201cno severe risk.\u201d\nSome severe risks discovered, but some safe use-cases. When certain uses of a frontier\nAI model would significantly threaten public safety or global security, the developer should\nimplement state-of-the-art deployment guardrails to prevent such misuse. These may include\nKnow-Your-Customer requirements for external users of the AI model, restrictions to fine-\ntuning,71 prohibiting certain applications, restricting deployment to beneficial applications,\nand requiring stringent post-deployment monitoring. The reliability of such safeguards\nshould also be rigorously assessed. This would be in addition to restrictions that are already\nimposed via other forms of AI regulation.\nSevere risks. When an AI model is assessed to pose severe risks to public safety or global\nsecurity which cannot be mitigated with sufficiently high confidence, the frontier model\nshould not be deployed. The model should be secured from theft by malicious actors, and the\nAI developer should consider deleting the model altogether. Any further experimentation with\nthe model should be done with significant caution, in close consultation with independent\nsafety experts, and could be subject to regulatory approval.\nOf course, additional nuance will be needed. For example, as discussed below, there should be methods for\nupdating a model\u2019s classifications in light of new information or societal developments. Procedural rigor and\nfairness in producing and updating such classifications will also be important.\nAssessed Risk to Public Safety\nand Global Security\nPossible Example AI system\nNo severe risks to public safety\nChatbot that can answer elementary-school-level questions about\nbiology, and some (but not all) high-school level questions.\nNo discovered severe risks to\npublic safety, but significant un-\ncertainty\nA general-purpose personal assistant that displays human-level\nability to read and synthesize large bodies of scientific litera-\nture, including in biological sciences, but cannot generate novel\ninsights.\nSome severe risks to public\nsafety discovered, but some safe\nuse-cases\nA general-purpose personal assistant that can help generate new\nvaccines, but also, unless significant safeguards are implemented,\npredict the genotypes of pathogens that could escape vaccine-\ninduced immunity.\nSevere risks to public safety\nA general-purpose personal assistant that is capable of designing\nand, autonomously, ordering the manufacture of novel pathogens\ncapable of causing a COVID-level pandemic.\nTable 3: Examples of AI models which would fall into each risk designation category\n71To ensure that certain dangerous capabilities are not further enhanced.\n27\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n4.4\nMonitor and Respond to New Information on Model Capabilities\nAs detailed above new information about a model\u2019s risk profile may arise post-deployment. If that information\nindicates that the model was or has become more risky than originally assessed, the developer should reassess\nthe deployment, and update restrictions on deployment if necessary.72\nNew information could arise in several ways. Broad deployment of a model may yield new information about\nthe model\u2019s capabilities, given the creativity from a much larger number of users, and exposure of the model\nto a wider array of tools and applications. Post-deployment enhancement techniques \u2014 such as fine-tuning\n[263, 264], prompt engineering [265, 266, 267], and foundation model programs [87, 88, 83] \u2014 provide\nanother possible source of new risk-relevant information. The application of these techniques to deployed\nmodels could elicit more powerful capabilities than pre-deployment assessments would have ascertained. In\nsome instances, this may meaningfully change the risk profile of a frontier AI model, potentially leading to\nadjustments in how and whether the model is deployed.73\nAI developers should stay on top of known and emerging post-deployment enhancement techniques by, e.g.,\nmonitoring how users are building on top of their APIs and tracking publications about new methods. Given\nup to date knowledge of how deployed AI models could be enhanced, prudent practices could include:\n\u2022 Regularly (e.g., every 3 months) repeating a lightweight version of the risk assessment on deployed\nAI models, accounting for new post-deployment enhancement techniques.\n\u2022 Before pushing large updates74 to deployed AI models, repeating a lightweight risk assessment.\n\u2022 Creating pathways for incident reporting [187] and impact monitoring to capture post-deployment\nincidents for continuous risk assessment.\n\u2022 If these repeat risk assessments result in the deployed AI model being categorized at a different risk\nlevel (as per the taxonomy above), promptly updating deployment guardrails to reflect the new risk\nprofile.\n\u2022 Having the legal and technical ability to quickly roll back deployed models on short notice if the\nrisks warrant it, for example by not open-sourcing models until doing so appears sufficiently safe.75\n4.5\nAdditional Practices\nParts of the aforementioned standards can suitably be applied to current AI systems, not just frontier AI\nsystems. Going forward, frontier AI systems seem likely to warrant more tailored safety standards, given the\nlevel of prospective risk that they pose. Examples of such standards include:76\n72In a recent expert survey (N = 51), 98% of respondents somewhat or strongly agreed that AGI labs should closely monitor\ndeployed systems, including how they are used and what impact they have on society; 97% thought that they should continually\nevaluate models for dangerous capabilities after deployment, taking into account new information about the model\u2019s capabilities\nand how it is being used; and 93% thought that labs should pause the development process if sufficiently dangerous capabilities are\ndetected [148].\n73Such updates may only be possible if the model has not yet proliferated, e.g. if it is deployed via an API. The ability to update\nhow a model is made available after deployment is one key reason to employ staged release of structured access approaches [109,\n110].\n74This would need to be defined more precisely.\n75Note that this may have implications for the kinds of use cases a system built on a frontier AI model can support. Use cases in\nwhich quick roll-back itself poses risks high enough to challenge the viability of roll-back as an option should be avoided, unless\nrobust measures are in place to prevent such failure modes.\n76This would need to be defined more precisely.\n28\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n\u2022 Avoid large jumps in the capabilities of models that are trained and deployed. Standards could\nspecify \u201clarge jumps\u201d in terms of a multiplier on the amount of computing power used to train the\nmost compute-intensive \u201cknown to be safe\u201d model to date, accounting for algorithmic efficiency\nimprovements.\n\u2022 Adopt state-of-the-art alignment techniques for training new frontier models which could suitably\nguard against models potentially being situationally aware and deceptive [187].\n\u2022 Prior to beginning training of a new model, use empirical approaches to predict capabilities of the\nresultant model, including experiments on small-scale versions of the model, and take preemptive\nactions to avoid training models with dangerous capabilities and/or to otherwise ensure training\nproceeds safely (e.g. introduce more frequent model evaluation checkpoints; conditioning beginning\ntraining on certain safety and security milestones).\n\u2022 Adopt internal governance practices to adequately identify and respond to the unique nature of the\nrisks presented by frontier AI development. Such practices could take inspiration from practices in\nEnterprise Risk Management, such as setting up internal audit functions [268, 192].\n\u2022 Adopt state-of-the-art security measures to protect frontier AI models.\n29\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n5\nUncertainties and Limitations\nWe think that it is important to begin taking practical steps to regulate frontier AI today, and that the ideas\ndiscussed in this paper are a step in that direction. Nonetheless, stress testing and developing these ideas,\nand offering alternatives, will require broad and diverse input. In this section, we list some of our main\nuncertainties (as well as areas of disagreement between the paper\u2019s authors) where we would particularly\nvalue further discussion.\nFirst, there are several assumptions that underpin the case for a regulatory regime like the one laid out in this\npaper, which would benefit from more scrutiny:\nHow should we define frontier AI for the purposes of regulation? We focus in this\npaper on tying the definition of frontier AI models to the potential of dangerous capabilities\nsufficient to cause severe harm, in order to ensure that any regulation is clearly tied to the\npolicy motivation of ensuring public safety. However, there are also downsides to this way\nof defining frontier AI \u2014 most notably, that it requires some assessment of the likelihood\nthat a model possesses dangerous capabilities before deciding whether it falls in the scope of\nregulation, which may be difficult to do. An alternative, which some authors of this paper\nprefer, would be to define frontier AI development as that which aims to develop novel and\nbroad AI capabilities \u2014 i.e. development pushing at the \u201cfrontier\u201d of AI capabilities. This\nwould need further operationalization \u2014 for example, defining these as models which use\nmore training compute than already-deployed systems \u2014 but could offer an approach to\nidentify the kinds of development activities that fall within the scope of regulation without\nfirst needing to make an assessment of dangerous capabilities. We discuss the pros and cons\nof different definitions of frontier AI in appendix A, and would love to receive feedback and\nengage in further discussion on this point.\nHow dangerous are and will the capabilities of advanced foundation AI models be,\nand how soon could these capabilities arise? It is very difficult to predict the pace of\nAI development and the capabilities that could emerge in advance; indeed, we even lack\ncertainty about the capabilities of existing systems. Assumptions here affect the urgency of\nregulatory action. There is a challenging balance to strike here between getting regulatory\ninfrastructure in place early enough to address and mitigate or prevent the biggest risks,\nwhile waiting for enough information about what those risks are likely to be and how they\ncan be mitigated [269].\nWill training advanced AI models continue to require large amounts of resources?\nThe regulatory ecosystem we discuss partly relies on an assumption that highly capable\nfoundation models will require large amounts of resources to develop. That being the case\nmakes it easier to regulate frontier AI. Should frontier AI models be possible to create using\nresources available to millions of actors rather than a handful, that may lead to significant\nchanges to the best regulatory approach. For example, it might suggest that more efforts\nshould be put into regulating the use of these models and to protect against (rather than to\nstop) dangerous uses of frontier AI.\nHow effectively can we anticipate and mitigate risks from frontier AI? A core argument\nof this paper is that an anticipatory approach to governing AI will be important, but effectively\nidentifying risks anticipatorily is far from straightforward. We would value input on the\neffectiveness of different risk assessment methods for doing this, drawing lessons from other\ndomains where anticipatory approaches are used.\n30\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nHow can regulatory flight be avoided? A regulatory regime for frontier AI could prove\ncounterproductive if it incentivises AI companies to move their activities to jurisdictions with\nless onerous rules. One promising approach is having rules apply to what models people in\nsome jurisdiction can engage with, as people are unlikely to move to a different jurisdiction\nto access different models and as companies are incentivised to serve them their product.\nScholars have suggested that dynamics like these have led to a \u201cCalifornia Effect\u201d and a\n\u201cBrussels Effect,\u201d where Californian and EU rules are voluntarily complied with beyond\ntheir borders.\nTo what extent will it be possible to defend against dangerous capabilities? Assessments\nof what constitutes \u201csufficiently dangerous capabilities,\u201d and what counter-measures are\nappropriate upon finding them in a model, hinges significantly on whether future AI models\nwill be more beneficial for offense versus defense.\nSecond, we must consider ways that this kind of regulatory regime could have unintended negative conse-\nquences, and take steps to guard against them. These include:\nReducing beneficial innovation. All else being equal, any imposition of costs on developers\nof new technologies slows the rate of innovation, and any regulatory measures come with\ncompliance costs. However, these costs should be weighed against the costs of unfettered\ndevelopment and deployment, as well as impacts on the rate of innovation from regulatory\nuncertainty and backlash due to unmitigated societal harms. On balance, we tentatively\nbelieve that the proposed regulatory approaches can support beneficial innovation by focusing\non a targeted subset of AI systems, and by addressing issues upstream in a way that makes it\neasier for smaller companies to develop innovative applications with confidence.\nCausing centralization of power in AI development. Approaches like a licensing regime\nfor developers could have the effect of centralizing power with the companies licensed to\ndevelop the most capable AI systems. It will be important to ensure that the regulatory\nregime is complemented with the power to identify and intervene to prevent abuses of market\ndominance,77 and government support for widespread access to AI systems deemed to be\nlow risk and high benefit for society.\nEnabling abuse of government powers. A significant aim of regulation is to transfer power\nfrom private actors to governments who are accountable to the public. However, the power to\nconstrain the development and deployment of frontier AI models is nonetheless a significant\none with real potential for abuse at the hand of narrow political interests, as well as corrupt\nor authoritarian regimes. This is a complex issue which requires thorough treatment of\nquestions such as: where should the regulatory authority be situated, and what institutional\nchecks and balances should be put in place, to reduce these risks?; what minimum regulatory\npowers are needed to be effective?; and what international dialogue is needed to establish\nnorms?\nRisk of regulatory capture. As the regulation of advanced technologies often requires\naccess to expertise from the technological frontier, and since the frontier is often occupied\nby private companies, there is an ongoing risk that regulations informed by private-sector\nexpertise will be biased towards pro-industry positions, to the detriment of society. This\nshould be mitigated by designing institutions that can limit and challenge the influence\nof private interests, and by seeking detailed input from academia and civil society before\nbeginning to implement any of these proposals.\n77Such as, for example, the UK\u2019s review of competition law as it relates to the market for foundation models [270].\n31\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFinally, there are many practical details of implementation not covered in this paper that will need to be\nworked out in detail with policy and legal professionals, including:\nWhat the appropriate regulatory authority/agency would be in different jurisdictions,\nwhere new bodies or powers might be required, and the tradeoffs of different options.\nHow this kind of regulation will relate to other AI regulation and governance proposals\nand how it can best support and complement attempts to address other parts of AI governance.\nOur hope is that by intervening early in the AI lifecycle, the proposed regulation can have\nmany downstream benefits, but there are also many risks and harms that this proposal will\nnot address. We hope to contribute to wider conversations about what a broader regulatory\necosystem for AI should look like, of which these proposals form a part.\nSteps towards international cooperation and implementation of frontier AI regulation,\nincluding how best to convene international dialogue on this topic, who should lead these\nefforts, and what possible international agreements could look like. An important part of\nthis will be considering what is best implemented domestically, at least initially, and where\ninternational action is needed.\n32\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nConclusion\nIn the absence of regulation, continued rapid development of highly capable foundation models may present\nsevere risks to public safety and global security. This paper has outlined possible regulatory approaches to\nreduce the likelihood and severity of these risks while also enabling beneficial AI innovation.\nGovernments and regulators will likely need to consider a broad range of approaches to regulating frontier AI.\nSelf-regulation and certification for compliance with safety standards for frontier AI could be an important\nstep. However, government intervention will be needed to ensure sufficient compliance with standards.\nAdditional approaches include mandates and enforcement by a supervisory authority, and licensing the\ndeployment and potentially the development of frontier AI models.\nClear and concrete safety standards will likely be the main substantive requirements of any regulatory\napproach. AI developers and AI safety researchers should, with the help of government actors, invest heavily\nto establish and converge on risk assessments, model evaluations, and oversight frameworks with the greatest\npotential to mitigate the risks of frontier AI, and foundation models overall. These standards should be\nreviewed and updated regularly.\nAs global leaders in AI development and AI safety, jurisdictions such as the United States or United Kingdom\ncould be natural leaders in implementing the regulatory approaches described in this paper. Bold leadership\ncould inspire similar efforts across the world. Over time, allies and partners could work together to establish\nan international governance regime78 for frontier AI development and deployment that both guards against\ncollective downsides and enables collective progress.79\nUncertainty about the optimal regulatory approach to address the challenges posed by frontier AI models\nshould not impede immediate action. Establishing an effective regulatory regime is a time-consuming process,\nwhile the pace of progress in AI is rapid. This makes it crucial for policymakers, researchers, and practitioners\nto move fast and rigorously explore what regulatory approaches may work best. The complexities of AI\ngovernance demand our best collective efforts. We hope that this paper is a small step in that direction.\n78Or build on existing institutions.\n79This international regime could take various forms. Possibilities include an international standard-setting organization, or trade\nagreements focused on enabling trade in AI goods and services that adhere to safety standards. Countries that lead in AI development\ncould subsidize access to and adoption of AI in developing nations in return for assistance in managing risks of proliferation, as has\nbeen done with nuclear technologies.\n33\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nAppendix A\nCreating a Regulatory Definition for Frontier AI\nIn this paper, we use the term \u201cfrontier AI\u201d models to refer to highly capable foundation models for which\nthere is good reason to believe could possess dangerous capabilities sufficient to pose severe risks to public\nsafety (\u201csufficiently dangerous capabilities\u201d). Any binding regulation of frontier AI, however, would require\na much more precise definition. Such a definition would also be an important building block to the creation\nand dissemination of voluntary standards.\nThis section attempts to lay out some desiderata and approaches to creating such a regulatory definition. It\nis worth noting up front that what qualifies as a frontier AI model changes over time \u2014 this is a dynamic\ncategory. In particular, what may initially qualify as a frontier AI model could change over time due to\nimprovements in society\u2019s defenses against advanced AI models and an improved understanding of the\nnature of the risks posed. On the other hand, factors such as improvements in algorithmic efficiency would\ndecrease the amount of computational resources required to develop models, including those with sufficiently\ndangerous capabilities.\nWhile we do not yet have confidence in a specific, sufficiently precise regulatory definition, we are optimistic\nthat such definitions are possible. Overall, none of the approaches we describe here seem fully satisfying.\nAdditional effort towards developing a better definition would be high-valuable.\nA.1\nDesiderata for a Regulatory Definition\nIn addition to general desiderata for a legal definition of regulated AI models,80 a regulatory definition should\nlimit its scope to only those models for which there is good reason to believe they have sufficiently dangerous\ncapabilities. Because regulation could cover development in addition to deployment, it should be possible to\ndetermine whether a planned model will be regulated ex ante, before the model is developed. For example,\nthe definition could be based on the model development process that will be used (e.g., data, algorithms, and\ncompute), rather than relying on ex post features of the completed model (e.g., capabilities, performance on\nevaluations).\nA.2\nDefining Sufficiently Dangerous Capabilities\n\u201cSufficiently dangerous capabilities\u201d play an important role in our concept of frontier AI: we only want\nto regulate the development of models that could cause such serious harms that ex post remedies will be\ninsufficient.\nDifferent procedures could be used to develop a regulatory definition of \u201csufficiently dangerous capabilities.\u201d\nOne approach could be to allow an expert regulator to create a list of sufficiently dangerous capabilities, and\nrevise that list over time in response to changing technical and societal circumstances. This approach has the\nbenefit of enabling greater learning and improvement over time, though it leaves the challenge outstanding\nof defining what model development activities are covered ex ante, and could in practice be very rigid and\nunsuited to the rapid pace of AI progress. Further, there is a risk that regulators will define such capabilities\nmore expansively over time, creating \u201cregulatory creep\u201d that overburdens AI development.\n80According to [271], legal definitions should neither be over-inclusive (i.e. they should not include cases which are not in need\nof regulation according to the regulation\u2019s objectives) nor under-inclusive (i.e. they should not exclude cases which should have\nbeen included). Instead, legal definitions should be precise (i.e. it must be possible to determine clearly whether or not a particular\ncase falls under the definition), understandable (i.e. at least in principle, people without expert knowledge should be able to apply\nthe definition), practicable (i.e. it should be possible to determine with little effort whether or not a concrete case falls under the\ndefinition), and flexible (i.e. they should be able to accommodate technical progress). See also [272, p. 70].\n34\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nLegislatures could try to prevent such regulatory creep by describing factors that should be considered when\nmaking a determination that certain capabilities would be sufficiently dangerous. This is common in United\nStates administrative law.81 One factor that could be considered is whether a capability would pose a \u201csevere\nrisk to public safety,\u201d assessed with reference to the potential scale and estimated probability of counterfactual\nharms caused by the system. A scale similar to the one used in the UK National Risk Register could be\nused [273]. One problem with this approach is that making these estimates will be exceedingly difficult and\ncontentious.\nA.3\nDefining Foundation Models\nThe seminal report on foundation models [15] defines them as \u201cmodels . . . trained on broad data . . . that can\nbe adapted to a wide range of downstream tasks.\u201d This definition, and various regulatory proposals based on\nit, identify two key features that a regulator could use to separate foundation models from narrow models:\nbreadth of training data, and applicability to a wide range of downstream tasks.\nBreadth is hard to define precisely, but one attempt would be to say that training data is \u201cbroad\u201d if it contains\ndata on many economically or strategically useful tasks. For example, broad natural language corpora, such\nas CommonCrawl [274], satisfy this requirement. Narrower datasets, such as weather data, do not. Similarly,\ncertain well-known types of models, such as large language models (LLMs) are clearly applicable to a variety\nof downstream tasks. A model that solely generates music, however, has a much narrower range of use-cases.\nGiven the vagueness of the above concepts, however, they may not be appropriate for a regulatory definition.\nOf course, judges and regulators do often adjudicate vague concepts [275], but we may be able to improve\non the above. For example, a regulator could list out types of model architectures (e.g., transformer-based\nlanguage models) or behaviors (e.g., competently answering questions about many topics of interest) that a\nplanned model could be expected to capable of, and say that any model that has these features is a foundation\nmodel.\nOverall, none of these approaches seem fully satisfying. Additional effort towards developing a better\ndefinition of foundational models\u2014or of otherwise defining models with broad capabilities\u2014would be\nhigh-value.\nA.4\nDefining the Possibility of Producing Sufficiently Dangerous Capabilities\nA regulator may also have to define AI development processes that could produce broadly capable models\nwith sufficiently dangerous capabilities.\nAt present, there is no rigorous method for reliably determining, ex ante, whether a planned model will have\nbroad and sufficiently dangerous capabilities. Recall the Unexpected Capabilities Problem: it is hard to\npredict exactly when any specific capability will arise in broadly capable models. It also does not appear that\nany broadly capable model to-date possesses sufficiently dangerous capabilities.\nIn light of this uncertainty, we do not have a definite recommendation. We will, however, note several options.\nOne simple approach would be to say that any foundation model that is trained with more than some\namount of computational power\u2014for example, 1026 FLOP\u2014has the potential to show sufficiently dangerous\ncapabilities. As Appendix B demonstrates, FLOP usage empirically correlates with breadth and depth of\ncapabilities in foundation models. There is therefore good reason to think that FLOP usage is correlated with\nthe likelihood that a broadly capable model will have sufficiently dangerous capabilities.\n81See, e.g., 42 U.S.C. \u00a7 262a(a)(1)(B).\n35\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nA threshold-based approach like this has several virtues. It is very simple, objective, determinable ex ante,82\nand (due to the high price of compute) is correlated with the ability of the developer to pay compliance costs.\nOne drawback, however, is that the same number of FLOP will produce greater capabilities over time due to\nalgorithmic improvements [276]. This means that, all else equal, the probability that a foundation model\nbelow the threshold will have sufficiently dangerous capabilities will increase over time. These problems\nmay not be intractable. For example, a FLOP threshold could formulaically decay over time based on new\nmodels\u2019 performance on standardized benchmarks, to attempt to account for anticipated improvements in\nalgorithmic efficiency.83\nA related approach could be to define the regulatory target by reference to the most capable broad models\nthat have been shown not to have sufficiently dangerous capabilities. The idea here is that, if a model has\nbeen shown not to have sufficiently dangerous capabilities, then every model that can be expected to perform\nworse than it should also not be expected to have sufficiently dangerous capabilities. Regulation would\nthen apply only to those models that exceed the capabilities of models known to lack sufficiently dangerous\ncapabilities. This approach has the benefit of updating quickly based on observations from newer models. It\nwould also narrow the space of regulated models over time, as regulators learn more about which models\nhave sufficiently dangerous capabilities.\nHowever, this definition has significant downsides too. First, there are many variables that could correlate with\npossession of dangerous capabilities, which means that it is unclear ex ante which changes in development\nprocesses could dramatically change capabilities. For example, even if model A dominates model B on many\nobvious aspects of its development (e.g., FLOP usage, dataset size), B may dominate A on other important\naspects, such as use of a new and more efficient algorithm, or a better dataset. Accordingly, the mere fact that\na B is different from A may be enough to make B risky,84 unless the regulator can carefully discriminate\nbetween trivial and risk-enhancing differences. The information needed to make such a determination may\nalso be highly sensitive and difficult to interpret. Overall, then, determining whether a newer model can be\nexpected to perform better than a prior known-safe model is far from straightforward.\nAnother potential problem with any compute-based threshold is that models below it could potentially be\nopen-sourced and then further trained by another actor, taking its cumulative training compute above the\nthreshold. One possible solution to this issue could be introducing minimal requirements regarding the\nopen-sourcing of models trained using one or two orders of magnitude of compute less than any threshold set.\nGiven the uncertainty surrounding model capabilities, any definition will likely be overinclusive. However, we\nemphasize the importance of creating broad and clear ex ante exemptions for models that have no reasonable\nprobability of possessing dangerous capabilities. For example, an initial blanket exemption for models trained\nwith fewer than (say) 1E26 FLOP85 could be appropriate, to remove any doubt as to whether such models are\ncovered. Clarity and definitiveness of such exemptions is crucial to avoid overburdening small and academic\ndevelopers, whose models will likely contribute very little to overall risk.\n82At least, determinable from the planned specifications of the training run of an AI model, though of course final FLOP usage\nwill not be determined until the training run is complete. However, AI developers tend to carefully plan the FLOP usage of training\nruns for both technical and financial reasons.\n83As an analogy, many monetary provisions in US law are adjusted for inflation based on a standardized measure like the consumer\nprice index.\n84Compare the definition of \u201cfrontier AI\u201d used in [25]: \u201cmodels that are both (a) close to, or exceeding, the average capabilities of\nthe most capable existing models, and (b) different from other models, either in terms of scale, design (e.g. different architectures or\nalignment techniques), or their resulting mix of capabilities and behaviours. . . \u201d\n85Using public FLOP per dollar estimates contained in [277] (Epoch AI) and [278], this would cost nearly or more than $100\nmillion in compute alone.\n36\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFigure 4: Computation used to train notable AI systems. Note logarithmic y-axis. Source: [50] based on data\nfrom [280].\nAppendix B\nScaling laws in Deep Learning\nThis appendix describes results from the scaling laws literature which shape the regulatory challenge posed\nby frontier AI as well as the available regulatory options. This literature focuses on relationships between\nmeasures of model performance (such as test loss) and properties of the model training process (such as\namounts of data, parameters, and compute). Results from this literature of particular relevance to this paper\ninclude: (i) increases in the amount of compute used to train models has been an important contributor\nto AI progress; (ii) even if the increase in compute starts contributing less to progress, we still expect\nfrontier AI models to be trained using large amounts of compute; (iii) though scale predictably increases\nmodel performance on the training objective, particular capabilities may improve or change unexpectedly,\ncontributing to the Unexpected Capabilities Problem.\nIn recent years, the Deep Learning Revolution has been characterized by the considerable scaling up of the\nkey inputs into neural networks, especially the quantity of computations used to train a deep learning system\n(\u201ccompute\u201d) [279], as illustrated in Figure 4.\nEmpirically, scaling training compute has reliably led to better performance on many of the tasks AI models\nare trained to solve, and many similar downstream tasks [58]. This is often referred to as the \u201cScaling\nHypothesis\u201d: the expectation that scale will continue to be a primary predictor and determinant of model\ncapabilities, and that scaling existing and foreseeable AI techniques will continue to produce many capabilities\nbeyond the reach of current systems.86\n86See [281, 282, 279, 15]. For a skeptical take on the Scaling Hypothesis, see [278].\n37\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nFigure 5: Scaling reliably leading to lower test loss. See [56]. The scaling laws from this paper have been\nupdated by [45].\nWe expect the Scaling Hypothesis to account for a significant fraction of progress in AI over the coming\nyears, driving increased opportunities and risks. However, the importance of scaling for developing more\ncapable systems may decrease with time, as per research which shows that the current rate of scaling may be\nunsustainable [278, 283, 103].\nEven if increases in scale slow down, the most capable AI models are still likely going to be those that can\neffectively leverage large amounts of compute, a claim often termed \u201cthe bitter lesson\u201d [282]. Specifically,\nwe expect frontier AI models to use vast amounts of compute, and that increased algorithmic efficiency [284]\nand data quality [285] will continue to be important drivers of AI progress.\nScaling laws have other limits. Though scaling laws can, as illustrated in Figure 5, reliably predict the loss\nof a model on its training objective \u2013 such as predicting the next word in a piece of text \u2013 that is currently\nan unreliable predictor of downstream performance on individual tasks. For example, tasks can see inverse\nscaling, where scaling leads to worse performance [60, 61, 62], though further scaling has overturned some\nof these findings [36].\nModel performance on individual tasks can also increase unexpectedly: there may be \u201cemergent capabilities\u201d\n[286, 67]. Some have argued that such emergent capabilities are a \u201cmirage\u201d [67]. They argue that the\nemergence of capabilities is primarily a consequence of how they are measured. Using discontinuous\nmeasures such as multiple choice answers or using an exact string match, is more likely to \u201cfind\u201d emergent\ncapabilities than if using continuous measures \u2013 for example, instead of measuring performance by exact\nstring match, you measure it based on proximity to the right answer.\nWe do not think this analysis comprehensively disproves the emergent capabilities claim [66]. Firstly,\ndiscontinuous measures are often what matter. For autonomous vehicles, what matters is how often they cause\na crash. For an AI model solving mathematics questions, what matters is whether it gets the answer exactly\nright or not. Further, even if continuous \u201csurrogate\u201d measures could be used to predict performance on the\ndiscontinuous measures, the appropriate choice of a continuous measure that will accurately predict the true\nmetric is often unknown a priori. Such forecasts instead presently require a subjective choice between many\npossible alternatives, which would lead to different predictions on the ultimate phenomenon. For instance,\nis an answer to a mathematical question \u201cless wrong\u201d if it\u2019s numerically closer to the actual answer, or if a\nsingle operation, such as multiplying instead of dividing, led to an incorrect result?\nNevertheless, investing in further research to more accurately predict capabilities of AI models ex ante is a\ncrucial enabler for effectively targeting policy interventions, using scaling laws or otherwise.\n38\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\nReferences\n[1]\nMichael Moor et al. \u201cFoundation models for generalist medical artificial intelligence\u201d. In: Nature\n616.7956 (Apr. 2023), pp. 259\u2013265. DOI: 10.1038/s41586-023-05881-4.\n[2]\nPeter Lee, Sebastien Bubeck, and Joseph Petro. \u201cBenefits, Limits, and Risks of GPT-4 as an AI\nChatbot for Medicine\u201d. In: New England Journal of Medicine 388.13 (Mar. 2023). Ed. by Jeffrey M.\nDrazen, Isaac S. Kohane, and Tze-Yun Leong, pp. 1233\u20131239. DOI: 10.1056/nejmsr2214184.\n[3]\nKaran Singhal et al. Large Language Models Encode Clinical Knowledge. 2022. arXiv: 2212.13138\n[cs.CL].\n[4]\nHarsha Nori et al. Capabilities of GPT-4 on Medical Challenge Problems. 2023. arXiv: 2303.13375\n[cs.CL].\n[5]\nDrew Simshaw. \u201cAccess to A.I. Justice: Avoiding an Inequitable Two-Tiered System of Legal\nServices\u201d. In: SSRN Electronic Journal (2022).\n[6]\nYonathan A. Arbel and Shmuel I. Becher. \u201cContracts in the Age of Smart Readers\u201d. In: SSRN\nElectronic Journal (2020). DOI: 10.2139/ssrn.3740356.\n[7]\nNoam Kolt. \u201cPredicting Consumer Contracts\u201d. In: Berkeley Technology Law Journal 37.1 (2022).\n[8]\nSal Khan. Harnessing GPT-4 so that all students benefit. 2023. URL: https://perma.cc/U54W-\nSSGA.\n[9]\nDavid Rolnick et al. Tackling Climate Change with Machine Learning. 2019. arXiv: 1906.05433\n[cs.CY].\n[10]\nDeepMind. DeepMind AI Reduces Google Data Centre Cooling Bill by 40%. 2016. URL: https:\n//perma.cc/F4B2-DFZ9.\n[11]\nHuseyin Tuna Erdinc et al. De-risking Carbon Capture and Sequestration with Explainable\nCO2 Leakage Detection in Time-lapse Seismic Monitoring Images. 2022. arXiv: 2212 . 08596\n[physics.geo-ph].\n[12]\nPriya L. Donti and J. Zico Kolter. \u201cMachine Learning for Sustainable Energy Systems\u201d. In: Annual\nReview of Environment and Resources 46.1 (Oct. 2021), pp. 719\u2013747. DOI: 10.1146/annurev-\nenviron-020220-061831.\n[13]\nPanagiota Galetsi, Korina Katsaliaki, and Sameer Kumar. \u201cThe medical and societal impact of big\ndata analytics and artificial intelligence applications in combating pandemics: A review focused on\nCovid-19\u201d. In: Social Science & Medicine 301 (May 2022), p. 114973. DOI: 10.1016/j.socscimed.\n2022.114973.\n[14]\nDavid C. Danko et al. The Challenges and Opportunities in Creating an Early Warning System for\nGlobal Pandemics. 2023. arXiv: 2302.00863 [q-bio.QM].\n[15]\nRishi Bommasani et al. On the Opportunities and Risks of Foundation Models. 2022. arXiv: 2108.\n07258 [cs.LG].\n[16]\nFabio Urbina et al. \u201cDual use of artificial-intelligence-powered drug discovery\u201d. In: Nature Machine\nIntelligence 4.3 (Mar. 2022), pp. 189\u2013191. DOI: 10.1038/s42256-022-00465-9.\n[17]\nRichard Ngo, Lawrence Chan, and S\u00f6ren Mindermann. The alignment problem from a deep learning\nperspective. 2023. arXiv: 2209.00626 [cs.AI].\n[18]\nMichael K. Cohen, Marcus Hutter, and Michael A. Osborne. \u201cAdvanced artificial agents intervene\nin the provision of reward\u201d. In: AI Magazine 43.3 (Aug. 2022), pp. 282\u2013293. DOI: 10.1002/aaai.\n12064.\n[19]\nDan Hendrycks et al. Unsolved Problems in ML Safety. 2022. arXiv: 2109.13916 [cs.LG].\n39\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[20]\nDan Hendrycks and Mantas Mazeika. X-Risk Analysis for AI Research. 2022. arXiv: 2206.05862\n[cs.CY].\n[21]\nJoseph Carlsmith. Is Power-Seeking AI an Existential Risk? 2022. arXiv: 2206.13353 [cs.CY].\n[22]\nStuart J. Russell. Human Compatible. Artificial Intelligence and the Problem of Control. Viking,\n2019.\n[23]\nBrian Christian. The Alignment Problem. Machine Learning and Human Values. W. W. Norton &\nCompany, 2020.\n[24]\nBrando Benifei and Ioan-Drago\u00b8s Tudorache. Proposal for a regulation of the European Parliament\nand of the Council on harmonised rules on Artificial Intelligence (Artificial Intelligence Act) and\namending certain Union Legislative Acts. 2023. URL: https://perma.cc/VH4R-WV3G.\n[25]\nToby Shevlane et al. Model evaluation for extreme risks. 2023. arXiv: 2305.15324 [cs.AI].\n[26]\nRemco Zwetsloot and Allan Dafoe. Thinking About Risks From AI: Accidents, Misuse and Structure.\n2019. URL: https://perma.cc/7UQ8-3Z2R.\n[27]\nDaniil A. Boiko, Robert MacKnight, and Gabe Gomes. Emergent autonomous scientific research\ncapabilities of large language models. 2023. arXiv: 2304.05332 [physics.chem-ph].\n[28]\nEric Horvitz. On the Horizon: Interactive and Compositional Deepfakes. 2022. arXiv: 2209.01714\n[cs.LG].\n[29]\nJosh A. Goldstein et al. Generative Language Models and Automated Influence Operations: Emerging\nThreats and Potential Mitigations. 2023. arXiv: 2301.04246 [cs.CY].\n[30]\nBen Buchanan et al. Truth, Lies, and Automation: How Language Models Could Change Disinforma-\ntion. 2021. URL: https://perma.cc/V5RP-CQG7.\n[31]\nRussell A Poldrack, Thomas Lu, and Ga\u0161per Begu\u0161. AI-assisted coding: Experiments with GPT-4.\n2023. arXiv: 2304.13187 [cs.AI].\n[32]\nAndrew J. Lohn and Krystal A. Jackson. Will AI Make Cyber Swords or Shields? 2022. URL:\nhttps://perma.cc/3KTH-GQTG.\n[33]\nMicrosoft. What are Tokens? 2023. URL: https://perma.cc/W2H8-FKDU.\n[34]\nAlec Radford et al. Language Models are Unsupervised Multitask Learners. 2019.\n[35]\nTom B. Brown et al. Language Models are Few-Shot Learners. 2020. arXiv: 2005.14165 [cs.CL].\n[36]\nOpenAI. GPT-4 Technical Report. 2023. arXiv: 2303.08774 [cs.CL].\n[37]\nAakanksha Chowdhery et al. PaLM: Scaling Language Modeling with Pathways. 2022. arXiv:\n2204.02311 [cs.CL].\n[38]\nJean-Baptiste Alayrac et al. Flamingo: a Visual Language Model for Few-Shot Learning. 2022. arXiv:\n2204.14198 [cs.CV].\n[39]\nReponsible AI Licenses Team. Reponsible AI Licenses. 2023. URL: https://perma.cc/LYQ8-\nV5X2.\n[40]\nopen source initiative. The Open Source Definition. 2007. URL: https://perma.cc/WU4B-DHWF.\n[41]\nEmily M. Bender et al. \u201cOn the Dangers of Stochastic Parrots\u201d. In: Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Transparency. ACM, Mar. 2021. DOI: 10.1145/\n3442188.3445922.\n[42]\nOpenAI. GPT-4 System Card. 2023. URL: https://perma.cc/TJ3Z-Z3YY.\n[43]\nJacob Steinhardt. AI Forecasting: One Year In. 2023. URL: https://perma.cc/X4WY-N8QY.\n[44]\nBaobao Zhang et al. Forecasting AI Progress: Evidence from a Survey of Machine Learning Re-\nsearchers. 2022. arXiv: 2206.04132 [cs.CY].\n[45]\nJordan Hoffmann et al. Training Compute-Optimal Large Language Models. 2022. arXiv: 2203.\n15556 [cs.CL].\n40\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[46]\nBryan Caplan. GPT-4 Takes a New Midterm and Gets an A. 2023. URL: https://perma.cc/2SPU-\nDRK3.\n[47]\nBryan Caplan. GPT Retakes My Midterm and Gets an A. 2023. URL: https://perma.cc/DG6F-\nWW8J.\n[48]\nMetaculus. In 2016, will an AI player beat a professionally ranked human in the ancient game of Go?\n2016. URL: https://perma.cc/NN7L-58YB.\n[49]\nMetaculus. When will programs write programs for us? 2021. URL: https://perma.cc/NM5Y-\n27RB.\n[50]\nOur World in Data. Computation used to train notable artificial intelligence systems. 2023. URL:\nhttps://perma.cc/59K8-WXQA.\n[51]\nMinister of Innovation, Science and Industry. An Act to enact the Consumer Privacy Protection\nAct, the Personal Information and Data Protection Tribunal Act and the Artificial Intelligence and\nData Act and to make consequential and related amendments to other Acts. 2021. URL: https:\n//perma.cc/ZT7V-A2Q8.\n[52]\nYvette D. Clarke. Algorithmic Accountability Act of 2022. US Congress. 2022. URL: https://\nperma.cc/99S2-AH9G.\n[53]\nU.S. Food and Drug Administration. Artificial Intelligence/Machine Learning (AI/ML)-Based Soft-\nware as a Medical Device (SaMD) Action Plan. 2021. URL: https://perma.cc/Q3PP-SDU8.\n[54]\nConsumer Financial Protection Bureau. CFPB Acts to Protect the Public from Black-Box Credit\nModels Using Complex Algorithms. 2022. URL: https://perma.cc/59SX-GGZN.\n[55]\nLina Khan. We Must Regulate A.I.: Here\u2019s How. New York Times. 2023. URL: https://perma.cc/\n4U6B-E7AV.\n[56]\nJared Kaplan et al. Scaling Laws for Neural Language Models. 2020. arXiv: 2001.08361 [cs.LG].\n[57]\nTom Henighan et al. Scaling Laws for Autoregressive Generative Modeling. 2020. arXiv: 2010.14701\n[cs.LG].\n[58]\nPablo Villalobos. Scaling Laws Literature Review. 2023. URL: https://perma.cc/32GJ-FBGM.\n[59]\nJoel Hestness et al. Deep Learning Scaling is Predictable, Empirically. 2017. arXiv: 1712.00409\n[cs.LG].\n[60]\nIan R. McKenzie et al. Inverse Scaling: When Bigger Isn\u2019t Better. 2023. arXiv: 2306.09479 [cs.CL].\n[61]\nEthan Perez et al. Discovering Language Model Behaviors with Model-Written Evaluations. 2022.\narXiv: 2212.09251 [cs.CL].\n[62]\nPhilipp Koralus and Vincent Wang-Ma\u00b4scianica. Humans in Humans Out: On GPT Converging Toward\nCommon Sense in both Success and Failure. 2023. arXiv: 2303.17276 [cs.AI].\n[63]\nJason Wei et al. Emergent Abilities of Large Language Models. 2022. arXiv: 2206.07682 [cs.CL].\n[64]\nJason Wei. 137 emergent abilities of large language models. 2022. URL: https://perma.cc/789W-\n4AZQ.\n[65]\nSamuel R. Bowman. Eight Things to Know about Large Language Models. 2023. arXiv: 2304.00612\n[cs.CL].\n[66]\nJason Wei. Common arguments regarding emergent abilities. 2023. URL: https://perma.cc/F48V-\nXZHC.\n[67]\nRylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are Emergent Abilities of Large Language\nModels a Mirage? 2023. arXiv: 2304.15004 [cs.AI].\n[68]\nAnthropic. Claude: A next-generation AI assistant for your tasks, no matter the scale. 2023. URL:\nhttps://www.anthropic.com/product.\n41\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[69]\nOpenAI. Fine-tuning: Learn how to customize a model for your application. 2023. URL: https:\n//perma.cc/QX2L-752C.\n[70]\nAI21 Labs. AI21 Studio. 2023. URL: https://perma.cc/9VSK-P5W7.\n[71]\nCohere. Training Custom Models. 2023. URL: https://perma.cc/M2MD-TTKR.\n[72]\nSteven C. H. Hoi et al. Online Learning: A Comprehensive Survey. 2018. arXiv: 1802.02871\n[cs.LG].\n[73]\nGerman I. Parisi et al. \u201cContinual lifelong learning with neural networks: A review\u201d. In: Neural\nNetworks 113 (May 2019), pp. 54\u201371. DOI: 10.1016/j.neunet.2019.01.012.\n[74]\nGerrit De Vynck, Rachel Lerman, and Nitasha Tiku. Microsoft\u2019s AI chatbot is going off the rails.\n2023. URL: https://www.washingtonpost.com/technology/2023/02/16/microsoft-bing-\nai-chatbot-sydney/.\n[75]\nOpenAI. Our approach to AI safety. 2023. URL: https://perma.cc/7GS3-KHVV.\n[76]\nJason Wei et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. 2023.\narXiv: 2201.11903 [cs.CL].\n[77]\nJack Clark. Import AI 310: AlphaZero learned Chess like humans learn Chess; capability emergence\nin language models; demoscene AI. 2022. URL: https://perma.cc/K4FG-ZXMX.\n[78]\nJessica Rumbelow. SolidGoldMagikarp (plus, prompt generation). 2023. URL: https://www.\nalignmentforum . org / posts / aPeJE8bSo6rAFoLqg / solidgoldmagikarp - plus - prompt -\ngeneration.\n[79]\nOpenAI. ChatGPT plugins. 2022. URL: https://perma.cc/3NPU-HUJP.\n[80]\nTimo Schick et al. Toolformer: Language Models Can Teach Themselves to Use Tools. 2023. arXiv:\n2302.04761 [cs.CL].\n[81]\nTianle Cai et al. Large Language Models as Tool Makers. 2023. arXiv: 2305.17126 [cs.LG].\n[82]\nAdept. ACT-1: Transformer for Actions. 2022. URL: https://perma.cc/7EN2-256H.\n[83]\nSignificant Gravitas. Auto-GPT: An Autonomous GPT-4 Experiment. 2023. URL: https://perma.\ncc/2TT2-VQE8.\n[84]\nShehel Yoosuf and Yin Yang. \u201cFine-Grained Propaganda Detection with Fine-Tuned BERT\u201d. In: Pro-\nceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship,\nDisinformation, and Propaganda. Hong Kong, China: Association for Computational Linguistics,\nNov. 2019, pp. 87\u201391. DOI: 10.18653/v1/D19-5011. URL: https://perma.cc/5CWN-HTU2.\n[85]\nTakeshi Kojima et al. Large Language Models are Zero-Shot Reasoners. 2023. arXiv: 2205.11916\n[cs.CL].\n[86]\nYongchao Zhou et al. Large Language Models Are Human-Level Prompt Engineers. 2023. arXiv:\n2211.01910 [cs.LG].\n[87]\nImanol Schlag et al. Large Language Model Programs. 2023. arXiv: 2305.05364 [cs.LG].\n[88]\nHarrison Chase. LangChain. 2023. URL: https://perma.cc/U2V6-AL7V.\n[89]\nAlexander Matt Turner et al. Optimal Policies Tend to Seek Power. 2023. arXiv: 1912.01683\n[cs.AI].\n[90]\nVictoria Krakovna and Janos Kramar. Power-seeking can be probable and predictive for trained\nagents. 2023. arXiv: 2304.06528 [cs.AI].\n[91]\nEvan Hubinger et al. Risks from Learned Optimization in Advanced Machine Learning Systems. 2021.\narXiv: 1906.01820 [cs.AI].\n[92]\nDario Amodei et al. Concrete Problems in AI Safety. 2016. arXiv: 1606.06565 [cs.AI].\n[93]\nYotam Wolf et al. Fundamental Limitations of Alignment in Large Language Models. 2023. arXiv:\n2304.11082 [cs.CL].\n42\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[94]\nSimon Willison. Prompt injection: What\u2019s the worst that can happen? Apr. 14, 2023. URL: https:\n//perma.cc/D7B6-ESAX.\n[95]\nGiuseppe Venuto. LLM failure archive (ChatGPT and beyond). 2023. URL: https://perma.cc/\nUJ8A-YAE2.\n[96]\nAlex Albert. Jailbreak Chat. 2023. URL: https://perma.cc/DES4-87DP.\n[97]\nRachel Metz. Jailbreaking AI Chatbots Is Tech\u2019s New Pastime. Apr. 8, 2023. URL: https://perma.\ncc/ZLU6-PBUN.\n[98]\nYuntao Bai et al. Constitutional AI: Harmlessness from AI Feedback. 2022. arXiv: 2212.08073\n[cs.CL].\n[99]\nAlexander Pan et al. Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards\nand Ethical Behavior in the MACHIAVELLI Benchmark. 2023. arXiv: 2304.03279 [cs.LG].\n[100]\nMarkus Anderljung and Julian Hazell. Protecting Society from AI Misuse: When are Restrictions on\nCapabilities Warranted? 2023. arXiv: 2303.09377 [cs.AI].\n[101]\nLennart Heim. Estimating PaLM\u2019s training cost. Apr. 5, 2023. URL: https://perma.cc/S4NF-\nGQ96.\n[102]\nJaime Sevilla et al. \u201cCompute Trends Across Three Eras of Machine Learning\u201d. In: 2022 International\nJoint Conference on Neural Networks. 2022, pp. 1\u20138. DOI: 10.1109/IJCNN55064.2022.9891914.\n[103]\nBen Cottier. Trends in the dollar training cost of machine learning systems. OpenAI. Jan. 31, 2023.\nURL: https://perma.cc/B9CB-T6C5.\n[104]\nAtila Orhon, Michael Siracusa, and Aseem Wadhwa. Stable Diffusion with Core ML on Apple Silicon.\n2022. URL: https://perma.cc/G5LA-94LM.\n[105]\nSimon Willison. Running LLaMA 7B and 13B on a 64GB M2 MacBook Pro with llama.cpp. 2023.\nURL: https://perma.cc/E8KY-CT6Z.\n[106]\nNomic AI. GPT4All. URL: https://perma.cc/EMR7-ZY6M.\n[107]\nYu-Hui Chen et al. Speed Is All You Need: On-Device Acceleration of Large Diffusion Models via\nGPU-Aware Optimizations. 2023. arXiv: 2304.11267 [cs.CV].\n[108]\nIrene Solaiman et al. Release Strategies and the Social Impacts of Language Models. 2019. arXiv:\n1908.09203 [cs.CL].\n[109]\nIrene Solaiman. The Gradient of Generative AI Release: Methods and Considerations. 2023. arXiv:\n2302.04844 [cs.CY].\n[110]\nToby Shevlane. Structured access: an emerging paradigm for safe AI deployment. 2022. arXiv:\n2201.05159 [cs.AI].\n[111]\n\u201cHow to be responsible in AI publication\u201d. In: Nature Machine Intelligence 3.5 (May 2021), pp. 367\u2013\n367. DOI: 10.1038/s42256-021-00355-6.\n[112]\nAviv Ovadya and Jess Whittlestone. Reducing malicious use of synthetic media research: Considera-\ntions and potential release practices for machine learning. 2019. arXiv: 1907.11274 [cs.CY].\n[113]\nGirish Sastry. Beyond \u201cRelease\u201d vs. \u201cNot Release\u201d. 2021. URL: https://perma.cc/JEZ2-ZB3W.\n[114]\nConnor Leahy. Why Release a Large Language Model? EleutherAI. June 2, 2021. URL: https:\n//perma.cc/Z9XE-GLRF.\n[115]\nBigScience. Introducing The World\u2019s Largest Open Multilingual Language Model: BLOOM. 2023.\nURL: https://perma.cc/N9ZA-LXWW.\n[116]\nHugging Face. We Raised $100 Million for Open & Collaborative Machine Learning. May 9, 2022.\nURL: https://perma.cc/DEU6-9EF9.\n[117]\nlaion.ai. Open Assistant. 2023. URL: https://perma.cc/YB8U-NZQE.\n43\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[118]\nRohan Taori et al. Alpaca: A Strong, Replicable Instruction-Following Model. Center for Research on\nFoundation Models. 2023. URL: https://perma.cc/Q75B-5KRX.\n[119]\nWayne Xin Zhao et al. A Survey of Large Language Models. 2023. arXiv: 2303.18223 [cs.CL].\n[120]\nRyan C. Maness. The Dyadic Cyber Incident and Campaign Data. 2022. URL: https://perma.cc/\nR2ZJ-PRGJ.\n[121]\nCarnegie Endowment for International Peace. Timeline of Cyber Incidents Involving Financial\nInstitutions. 2022. URL: https://perma.cc/TM34-ZHUH.\n[122]\nCenter for Strategic and International Studies. Significant Cyber Incidents. May 2023. URL: https:\n//perma.cc/H3J2-KZFW.\n[123]\nMichael S. Schmidt and David E. Sanger. Russian Hackers Read Obama\u2019s Unclassified Emails,\nOfficials Say. Apr. 25, 2015. URL: https://perma.cc/JU2G-25MM.\n[124]\nBen Buchanan. The Cybersecurity Dilemma: Hacking, Trust and Fear Between Nations. Oxford\nUniversity Press, 2017.\n[125]\nChina\u2019s Access to Foreign AI Technology. Sept. 2019. URL: https://perma.cc/ZV3F-G7KK.\n[126]\nNational Counterintelligence and Security Center. Protecting Critical and Emerging U.S. Technolo-\ngies from Foreign Threats. Oct. 2021. URL: https://perma.cc/4P9M-QLM9.\n[127]\nNVIDIA Research Projects. StyleGAN \u2013 Official TensorFlow Implementation. 2019. URL: https:\n//perma.cc/TMD4-PYBY.\n[128]\nTero Karras, Samuli Laine, and Timo Aila. A Style-Based Generator Architecture for Generative\nAdversarial Networks. 2019. arXiv: 1812.04948 [cs.NE].\n[129]\nRachel Metz. These people do not exist. Why websites are churning out fake images of people (and\ncats). Feb. 28, 2019. URL: https://perma.cc/83Q5-4KJW.\n[130]\nPhillip Wang. This Person Does Not Exist. 2019. URL: https://perma.cc/XFH9-NRQV.\n[131]\nFergal Gallagher and Erin Calabrese. Facebook\u2019s latest takedown has a twist \u2013 AI-generated profile\npictures. Dec. 31, 2019. URL: https://perma.cc/5Q2V-4BD2.\n[132]\nShannon Bond. AI-generated fake faces have become a hallmark of online influence operations.\nNational Public Radio. Dec. 15, 2022. URL: https://perma.cc/DC5D-TJ32.\n[133]\nGoogle DeepMind. AlphaFold: a solution to a 50-year-old grand challenge in biology. Nov. 30, 2020.\nURL: https://perma.cc/C6J4-6XWD.\n[134]\nJohn Jumper et al. \u201cHighly accurate protein structure prediction with AlphaFold\u201d. In: Nature 596.7873\n(July 2021), pp. 583\u2013589. DOI: 10.1038/s41586-021-03819-2.\n[135]\nGustaf Ahdritz et al. \u201cOpenFold: Retraining AlphaFold2 yields new insights into its learning mecha-\nnisms and capacity for generalization\u201d. In: bioRxiv (2022). DOI: 10.1101/2022.11.20.517210.\nURL: https://www.biorxiv.org/content/early/2022/11/22/2022.11.20.517210.\n[136]\nJack W. Rae et al. Scaling Language Models: Methods, Analysis & Insights from Training Gopher.\n2022. arXiv: 2112.11446 [cs.CL].\n[137]\nMeta AI. Introducing LLaMA: A foundational, 65-billion-parameter large language model. Feb. 24,\n2023. URL: https://perma.cc/59YP-6ZDE.\n[138]\nRunaway LLaMA: How Meta\u2019s LLaMA NLP model leaked. Mar. 15, 2023. URL: https://perma.\ncc/44YT-UNZ6.\n[139]\nArnav Gudibande et al. The False Promise of Imitating Proprietary LLMs. 2023. arXiv: 2305.15717\n[cs.CL].\n[140]\nKatyanna Quach. Stanford sends \u2019hallucinating\u2019 Alpaca AI model out to pasture over safety, cost.\nMar. 21, 2023. URL: https://perma.cc/52NR-CMRF.\n44\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[141]\nTatsu. Stanford Alpaca: An Instruction-following LLaMA Model. 2023. URL: https://perma.cc/\nSW29-C83N.\n[142]\nEmily H. Soice et al. Can large language models democratize access to dual-use biotechnology?\n2023. arXiv: 2306.03809 [cs.CY].\n[143]\nGoogle. Responsible AI practices. 2023. URL: https://perma.cc/LKN6-P76L.\n[144]\nCohere, OpenAI, and AI21 Labs. Joint Recommendation for Language Model Deployment. June 2,\n2022. URL: https://perma.cc/ZZ5Y-FNFY.\n[145]\nMicrosoft. Microsoft Responsible AI Standard. June 2022. URL: https://perma.cc/4XWP-NWK7.\n[146]\nAmazon AWS. Responsible Use of Machine Learning. 2023. URL: https://perma.cc/U7GB-X4WV.\n[147]\nPAI Staff. PAI Is Collaboratively Developing Shared Protocols for Large-Scale AI Model Safety.\nApr. 6, 2023. URL: https://perma.cc/ZVQ4-3WJK.\n[148]\nJonas Schuett et al. Towards Best Practices in AGI Safety and Governance. Centre for the Governance\nof AI. May 17, 2023. URL: https://perma.cc/AJC3-M3AM.\n[149]\nNational Institute of Standards and Technology. Artificial Intelligence Risk Management Framework.\nJan. 2023. URL: https://perma.cc/N5SA-N6LT.\n[150]\nThe IA Act. Standard Setting. 2023. URL: https://perma.cc/T9RA-5Q37.\n[151]\nFranklin D. Raines. Circular No. A-119 Revised. Feb. 10, 1998. URL: https://perma.cc/F2NH-\nNYHH.\n[152]\nNational Telecommunications and Information Administration. AI Accountability Policy Request for\nComment. 2023. URL: https://perma.cc/E4C9-QQ8V.\n[153]\nDepartment for Science, Innovation and Technology. New UK initiative to shape global standards\nfor Artificial Intelligence. Jan. 2022. URL: https://www.gov.uk/government/news/new-uk-\ninitiative-to-shape-global-standards-for-artificial-intelligence.\n[154]\nEuropean Commission. Draft standardisation request to the European Standardisation Organisations\nin support of safe and trustworthy artificial intelligence. Dec. 5, 2022. URL: https://perma.cc/\n8GBP-NJAW.\n[155]\nGillian K. Hadfield and Jack Clark. Regulatory Markets: The Future of AI Governance. 2023. arXiv:\n2304.04914 [cs.AI].\n[156]\nMinistry of Defence. Foreword by the Secretary of State for Defence. June 15, 2022.\n[157]\nUnited States Government Accountability Office. Status of Developing and Acquiring Capabilities\nfor Weapon Systems. Feb. 2022. URL: https://perma.cc/GJN4-HQM8.\n[158]\nThe White House. FACT SHEET: Biden-Harris Administration Announces New Actions to Promote\nResponsible AI Innovation that Protects Americans\u2019 Rights and Safety. May 4, 2023. URL: https:\n//perma.cc/J6RR-2FVE.\n[159]\nGovernment of the United Kingdom. The roadmap to an effective AI assurance ecosystem. Dec. 8,\n2021. URL: https : / / www . gov. uk/government/publications/the - roadmap- to- an -\neffective-ai-assurance-ecosystem/the-roadmap-to-an-effective-ai-assurance-\necosystem-extended-version.\n[160]\nDepartment for Science, Innovation and Technology. Initial \u00a3100 million for expert taskforce to\nhelp UK build and adopt next generation of safe AI. Apr. 24, 2023. URL: https://www.gov.uk/\ngovernment/news/initial-100-million-for-expert-taskforce-to-help-uk-build-\nand-adopt-next-generation-of-safe-ai.\n[161]\nNational Artificial Intelligence Research Resource Task Force. Strengthening and Democratizing the\nU.S. Artificial Intelligence Innovation Ecosystem. Jan. 2023. URL: https://perma.cc/N99K-ARLP.\n45\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[162]\nMichael Atleson. Keep your AI claims in check. Federal Trade Commission. Feb. 27, 2023. URL:\nhttps://perma.cc/M59A-Z4AV.\n[163]\nInformation Commissioner\u2019s Office. Artificial intelligence. 2023. URL: https://ico.org.uk/for-\norganisations/uk-gdpr-guidance-and-resources/artificial-intelligence/.\n[164]\nThe White House. Blueprint for an AI Bill of Rights. 2022. URL: https://perma.cc/HXS9-66Q5.\n[165]\nComputer Security Resource Center. Proposed Update to the Framework for Improving Critical\nInfrastructure Cybersecurity. Jan. 25, 2017. URL: https://perma.cc/CD97-YW27.\n[166]\nNational Institute of Standards and Technology. Request for Comments on the Preliminary Draft of\nthe NIST Privacy Framework. 2020. URL: https://perma.cc/5U9R-4UFQ.\n[167]\nNational Telecommunications and Information Administration. NTIA Seeks Public Input to Boost AI\nAccountability. Apr. 11, 2023. URL: https://perma.cc/XJH6-YNXB.\n[168]\nMatthew C. Stephenson. \u201cInformation Acquisition and Institutional Design\u201d. In: Harvard Law Review\n124.4 (2011).\n[169]\nCary Coglianese, Richard Zeckhauser, and Edward A. Parson. \u201cSeeking Truth for Power: Informa-\ntional Strategy and Regulatory PolicymakingPolicymaking\u201d. In: Michigan Law review 89.2 (2004),\npp. 277\u2013341.\n[170]\nThomas O. McGarity. \u201cRegulatory Reform in the Reagan Era\u201d. In: Maryland Law Review 45.2\n(1986).\n[171]\nRovy Van Loo. \u201cRegulatory Monitors: Policing Firms in the Compliance Era\u201d. In: Columbia Law\nReview 119 (2019).\n[172]\nRovy Van Loo. \u201cThe Missing Regulatory State: Monitoring Businesses in an Age of Surveil-\nlanceSurveillance\u201d. In: Vanderbilt Law Review 72.5 (2019).\n[173]\nNoam Kolt. \u201cAlgorithmic Black Swans\u201d. In: Washington University Law Review 101 (2023).\n[174]\nGary E. Marchant, Braden R. Allenby, and Joseph R. Herkert. The Growing Gap Between Emerging\nTechnologies and Legal-Ethical Oversight. Springer, 2011. URL: https://perma.cc/4XXW-3RHH.\n[175]\nMargaret Mitchell et al. \u201cModel Cards for Model Reporting\u201d. In: Proceedings of the Conference on\nFairness, Accountability, and Transparency. ACM, Jan. 2019. DOI: 10.1145/3287560.3287596.\n[176]\nTimnit Gebru et al. \u201cDatasheets for datasets\u201d. In: 64.12 (2021), pp. 86\u201392.\n[177]\nThomas Krendl Gilbert et al. Reward Reports for Reinforcement Learning. 2023. arXiv: 2204.10817\n[cs.LG].\n[178]\nStandford University. ecosystem graphs. 2023. URL: https://perma.cc/H6GW-Q78M.\n[179]\nJaime Sevilla, Anson Ho, and Tamay Besiroglu. \u201cPlease Report Your Compute\u201d. In: Communications\nof the ACM 66.5 (Apr. 2023), pp. 30\u201332. DOI: 10.1145/3563035.\n[180]\nInioluwa Deborah Raji et al. Closing the AI Accountability Gap: Defining an End-to-End Framework\nfor Internal Algorithmic Auditing. 2020. arXiv: 2001.00973 [cs.CY].\n[181]\nAnn M. Lipton. \u201cNot Everything Is About Investors: The Case for Mandatory Stakeholder Disclosure\u201d.\nIn: Yale Journal on Regulation (). URL: https://perma.cc/G97G-3FL2.\n[182]\nJess Whittlestone and Jack Clark. Why and How Governments Should Monitor AI Development. 2021.\narXiv: 2108.12427 [cs.CY].\n[183]\nJakob M\u00f6kander et al. Auditing large language models: a three-layered approach. 2023. arXiv:\n2302.08500 [cs.CL].\n[184]\nInioluwa Deborah Raji et al. Outsider Oversight: Designing a Third Party Audit Ecosystem for AI\nGovernance. 2022. arXiv: 2206.04737 [cs.CY].\n[185]\nHannah Bloch-Wehba. \u201cThe Promise and Perils of Tech Whistleblowing\u201d. In: Northwestern University\nLaw Review (Mar. 3, 2023).\n46\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[186]\nSonia K. Katyal. Private Accountability in the Age of Artificial Intelligence. Dec. 14, 2018. URL:\nhttps://perma.cc/PNW4-7LN2.\n[187]\nHelen Toner, Patrick Hall, and Sean McGregor. AI Incident Database. 2023. URL: https://perma.\ncc/JJ95-7K7B.\n[188]\nEpoch AI. ML Inputs. 2023. URL: https://perma.cc/9XBU-6NES.\n[189]\nCenter for Emerging Technology. Emerging Technology Observatory. 2022. URL: https://perma.\ncc/L4DB-YQ5L.\n[190]\nEuropean Commission. Joint Statement EU-US Trade and Technology Council of 31 May 2023 in\nLulea, Sweden. May 21, 2023. URL: https://perma.cc/8PDH-8S34.\n[191]\nDepartment for Science, Innovation and Technology. AI regulation: a pro-innovation approach.\nMar. 29, 2023. URL: https://www.gov.uk/government/publications/ai-regulation-a-\npro-innovation-approach.\n[192]\nJonas Schuett. Three lines of defense against risks from AI. 2022. arXiv: 2212.08364 [cs.CY].\n[193]\nPeter Cihon et al. \u201cAI Certification: Advancing Ethical Practice by Reducing Information Asym-\nmetries\u201d. In: IEEE Transactions on Technology and Society 2.4 (Dec. 2021), pp. 200\u2013209. DOI:\n10.1109/tts.2021.3077595.\n[194]\nInternational Organization for Standardization. Consumers and Standards: Partnership for a Better\nWorld. URL: https://perma.cc/5XJP-NC5S.\n[195]\nAdministrative Conference of the United States. Incorporation by Reference. Dec. 8, 2011. URL:\nhttps://perma.cc/Q3H9-DBK9.\n[196]\nBusiness Operations Support System. The \u2019New Approach\u2019. URL: https://perma.cc/ZS9G-LV66.\n[197]\nWorld Trade Organization. Agreement on Technical Barriers to Trade. URL: https://perma.cc/\nPE55-5GJV.\n[198]\nU.S. Securities and Exchange Commission. Addendum to Division of Enforcement Press Release.\n2023. URL: https://perma.cc/M3LN-DGGV.\n[199]\nPhilip F.S. Berg. \u201cUnfit to Serve: Permanently Barring People from Serving as Officers and Directors\nof Publicly Traded Companies After theOfficers and Directors of Publicly Traded Companies After\nthe Sarbanes-Oxley ActSarbanes-Oxley Act\u201d. In: Vanderbilt Law ReviewVanderbilt Law Review 56.6\n().\n[200]\nOffice of the Comptroller of the Currency. Bank Supervision Process, Comptroller\u2019s Hand-\nbook. Sept. 30, 2019. URL: https : / / www . occ . gov / publications - and - resources /\npublications/comptrollers- handbook/files/bank- supervision- process/pub- ch-\nbank-supervision-process.pdf.\n[201]\nDavid A. Hindin. Issuance of the Clean Air Act Stationary Source Compliance Monitoring Strategy.\nOct. 4, 2016. URL: https://perma.cc/6R7C-PKB2.\n[202]\nCommitee on Armed Services. Hearing To Receive Testimony on the State of Artificial Intelligence\nand Machine Learning Applications To Improve Department of Defense Operations. Apr. 19, 2023.\nURL: https://perma.cc/LV3Z-J7BT.\n[203]\nMicrosoft. Governing AI: A Blueprint for the Future. May 2023. URL: https://perma.cc/3NL2-\nP4XE.\n[204]\nSubcommittee on Privacy, Technology and the Law. Oversight of A.I.: Rules for Artificial Intelligence.\n2023. URL: https://perma.cc/4WCU-FWUL.\n[205]\nPatrick Murray. \u201cNoational: Artificial Intelligence Use Prompts Concerns\u201d. In: (2023). URL: https:\n//perma.cc/RZT2-BWCM.\n47\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[206]\nJamie Elsey and David Moss. US public opinion of AI policy and risk. Rethink Priorities. May 12,\n2023. URL: https://perma.cc/AF29-JT8K.\n[207]\nFederal Aviation Administration. Classes of Airports \u2013 Part 139 Airport Certification. May 2, 2023.\nURL: https://perma.cc/9JLB-6D7R.\n[208]\nFederal Aviation Administration. Air Carrier and Air Agency Certification. June 22, 2022. URL:\nhttps://perma.cc/76CZ-WLB6.\n[209]\nCalifornia Energy Commission. Power Plant Licensing. URL: https://perma.cc/BC7A-9AM3.\n[210]\nU.S. Food and Drug Administration. Electronic Drug Registration and Listing System (eDRLS).\nApr. 11, 2021. URL: https://perma.cc/J357-89YH.\n[211]\nCongressional Research Service. An Analysis of Bank Charters and Selected Policy Issues. Jan. 21,\n2022. URL: https://perma.cc/N9HU-JTJJ.\n[212]\nU.S. Food and Drug Administration. Development and Approval Process. Aug. 8, 2022. URL: https:\n//perma.cc/47UY-NVHV.\n[213]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. Federal Select\nAgent Program. 2022. URL: https://perma.cc/3TZP-GAV6.\n[214]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. Select Agents and\nToxins List. 2023. URL: https://perma.cc/W8K8-LQV4.\n[215]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. 2021 Annual\nReport of the Federal Select Agent Program. 2021. URL: https://perma.cc/RPV8-47GW.\n[216]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. Select Agents\nRegulations. 2022. URL: https://perma.cc/MY34-HX79.\n[217]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. Security Risk\nAssessments. 2022. URL: https://perma.cc/ZY4A-5BB2.\n[218]\nCenters for Disease Control and Prevention/Division of Select Agents and Toxins & Animal and\nPlant Health Inspection Service/Division of Agricultural Select Agents and Toxins. Preparing for\nInspection. 2021. URL: https://perma.cc/Z73F-3RVV.\n[219]\nGeorge J. Stigler. \u201cThe Theory of Economic Regulation\u201d. In: The Bell Journal of Economics and\nManagement Science 2.1 (1971), pp. 3\u201321.\n[220]\nGary S. Becker. \u201cA Theory of Competition among Pressure Groups for Political Influence\u201d. In: The\nQuarterly Journal of Economics 98 (1983), pp. 371\u2013395.\n[221]\nDaniel Carpenter and David Moss, eds. Preventing Regulatory Capture: Special Interest Influence\nand How to Limit It. Cambridge University Press, 2013.\n[222]\nRecruiting Tech Talent to Congress. 2023. URL: https://perma.cc/SLY8-5M39.\n[223]\nOpen Philanthropy. Open Philanthropy Technology Policy Fellowship. URL: https://perma.cc/\nBY47-SS5V.\n[224]\nMhairi Aitken et al. Common Regulatory Capacity for AI. The Alan Turing Institute. 2022. DOI:\n10.5281/zenodo.6838946.\n[225]\nMeta AI. System Cards, a new resource for understanding how AI systems work. Feb. 2022. URL:\nhttps://perma.cc/46UG-GA9D.\n48\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[226]\nLeon Derczynski et al. Assessing Language Model Deployment with Risk Cards. 2023. arXiv:\n2303.18190 [cs.CL].\n[227]\nCertification Working Group. Unlocking the Power of AI. June 8, 2023. URL: https://perma.cc/\nDLF3-E38T.\n[228]\nPercy Liang et al. Holistic Evaluation of Language Models. 2022. arXiv: 2211.09110 [cs.CL].\n[229]\nStella Biderman et al. Pythia: A Suite for Analyzing Large Language Models Across Training and\nScaling. 2023. arXiv: 2304.01373 [cs.CL].\n[230]\nAarohi Srivastava et al. Beyond the Imitation Game: Quantifying and extrapolating the capabilities\nof language models. 2022. arXiv: 2206.04615 [cs.CL].\n[231]\nDan Hendrycks et al. Measuring Massive Multitask Language Understanding. 2021. arXiv: 2009.\n03300 [cs.CY].\n[232]\nHeidy Khlaaf. Toward Comprehensive Risk Assessments. Trail of Bits. Mar. 2023. URL: https:\n//perma.cc/AQ35-6JTV.\n[233]\nDeep Ganguli et al. Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors,\nand Lessons Learned. 2022. arXiv: 2209.07858 [cs.CL].\n[234]\nEthan Perez et al. Red Teaming Language Models with Language Models. 2022. arXiv: 2202.03286\n[cs.CL].\n[235]\nMiles Brundage et al. Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable\nClaims. 2020. arXiv: 2004.07213 [cs.CY].\n[236]\nARC Evals. Update on ARC\u2019s recent eval efforts. Mar. 17, 2023. URL: https://perma.cc/8VWF-\nQYPH.\n[237]\nIan McKenzie et al. Inverse Scaling Prize: First Round Winners. Fund for Alignment Research (FAR).\n2022. URL: https://irmckenzie.co.uk/round1.\n[238]\nIan McKenzie et al. Inverse Scaling Prize: Second Round Winners. Fund for Alignment Research\n(FAR). 2022. URL: https://irmckenzie.co.uk/round2.\n[239]\nLeo Gao, John Schulman, and Jacob Hilton. Scaling Laws for Reward Model Overoptimization. 2022.\narXiv: 2210.10760 [cs.LG].\n[240]\nSamuel R. Bowman et al. Measuring Progress on Scalable Oversight for Large Language Models.\n2022. arXiv: 2211.03540 [cs.HC].\n[241]\nSamir Passi and Mihaela Vorvoreanu. Overreliance on AI: Literature Review. AI Ethics, Effects in\nEngineering, and Research. June 2022.\n[242]\nZiwei Ji et al. \u201cSurvey of Hallucination in Natural Language Generation\u201d. In: ACM Computing\nSurveys 55.12 (Mar. 2023), pp. 1\u201338. DOI: 10.1145/3571730.\n[243]\nSamuel Gehman et al. \u201cRealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language\nModels\u201d. In: Findings of the Association for Computational Linguistics: EMNLP 2020. Association\nfor Computational Linguistics, 2020. DOI: 10.18653/v1/2020.findings-emnlp.301.\n[244]\nAmanda Askell et al. A General Language Assistant as a Laboratory for Alignment. 2021. arXiv:\n2112.00861 [cs.CL].\n[245]\nPaul Christiano. Mechanistic Anomaly Detection and ELK. Nov. 2022. URL: https://perma.cc/\nWH44-WVRV.\n[246]\nCatherine Olsson et al. In-context Learning and Induction Heads. Mar. 2022. URL: https://perma.\ncc/FQP6-2Z4G.\n[247]\nTom Henighan et al. Superposition, Memorization, and Double Descent. Jan. 2023. URL: https:\n//perma.cc/5ZTF-RMV8.\n49\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[248]\nIan Tenney et al. The Language Interpretability Tool: Extensible, Interactive Visualizations and\nAnalysis for NLP Models. 2020. arXiv: 2008.05122 [cs.CL].\n[249]\nShoaib Ahmed Siddiqui et al. Metadata Archaeology: Unearthing Data Subsets by Leveraging\nTraining Dynamics. 2022. arXiv: 2209.10015 [cs.LG].\n[250]\nToby Shevlane and Allan Dafoe. The Offense-Defense Balance of Scientific Knowledge: Does\nPublishing AI Research Reduce Misuse? 2020. arXiv: 2001.00463 [cs.CY].\n[251]\nOECD. OECD Framework for the Classification of AI systems. Feb. 2022. DOI: 10.1787/cb6d9eca-\nen.\n[252]\nIrene Solaiman et al. Evaluating the Social Impact of Generative AI Systems in Systems and Society.\n2023. arXiv: 2306.05949 [cs.CY].\n[253]\nITU News. How AI can help fight misinformation. May 2, 2022. URL: https://perma.cc/R7RA-\nZX5G.\n[254]\nAjeya Cotra. Training AIs to help us align AIs. Mar. 26, 2023. URL: https://perma.cc/3L49-\n7QU7.\n[255]\nGeoffrey Irving, Paul Christiano, and Dario Amodei. AI safety via debate. 2018. arXiv: 1805.00899\n[stat.ML].\n[256]\nElisabeth Keller and Gregory A. Gehlmann. \u201cIntroductory comment: a historical introduction to the\nSecurities Act of 1933 and the Securities Exchange Act of 1934\u201d. In: Ohio State Law Journal 49\n(1988), pp. 329\u2013352.\n[257]\nInioluwa Deborah Raji and Joy Buolamwini. \u201cActionable Auditing\u201d. In: Proceedings of the 2019\nAAAI/ACM Conference on AI, Ethics, and Society. ACM, Jan. 2019. DOI: 10.1145/3306618.\n3314244.\n[258]\nJakob M\u00f6kander et al. \u201cEthics-Based Auditing of Automated Decision-Making Systems: Nature,\nScope, and Limitations\u201d. In: Science and Engineering Ethics 27.4 (July 2021). DOI: 10.1007/\ns11948-021-00319-4.\n[259]\nGregory Falco et al. \u201cGoverning AI safety through independent audits\u201d. In: Nature Machine Intelli-\ngence 3.7 (July 2021), pp. 566\u2013571. DOI: 10.1038/s42256-021-00370-7.\n[260]\nInioluwa Deborah Raji et al. \u201cOutsider Oversight: Designing a Third Party Audit Ecosystem for AI\nGovernance\u201d. In: Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society. ACM,\nJuly 2022. DOI: 10.1145/3514094.3534181.\n[261]\nSasha Costanza-Chock, Inioluwa Deborah Raji, and Joy Buolamwini. \u201cWho Audits the Auditors?\nRecommendations from a field scan of the algorithmic auditing ecosystem\u201d. In: 2022 ACM Conference\non Fairness, Accountability, and Transparency. ACM, June 2022. DOI: 10.1145/3531146.3533213.\n[262]\nOpenAI. DALL\u00b7E 2 Preview - Risks and Limitations. July 19, 2022. URL: https://perma.cc/W9GA-\n8BYQ.\n[263]\nDaniel M. Ziegler et al. Fine-Tuning Language Models from Human Preferences. 2020. arXiv:\n1909.08593 [cs.CL].\n[264]\nJesse Dodge et al. Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders,\nand Early Stopping. 2020. arXiv: 2002.06305 [cs.CL].\n[265]\nPengfei Liu et al. Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in\nNatural Language Processing. 2021. arXiv: 2107.13586 [cs.CL].\n[266]\nXiang Lisa Li and Percy Liang. Prefix-Tuning: Optimizing Continuous Prompts for Generation. 2021.\narXiv: 2101.00190 [cs.CL].\n[267]\nEric Wallace et al. Universal Adversarial Triggers for Attacking and Analyzing NLP. 2021. arXiv:\n1908.07125 [cs.CL].\n50\nFrontier AI Regulation: Managing Emerging Risks to Public Safety\n[268]\nJonas Schuett. AGI labs need an internal audit function. 2023. arXiv: 2305.17038 [cs.CY].\n[269]\nRichard Worthington. \u201cThe Social Control of Technology\u201d. In: American Political Science Review\n76.1 (Mar. 1982), pp. 134\u2013135. DOI: 10.2307/1960465.\n[270]\nCompetition and Markets Authority. CMA launches initial review of artificial intelligence models.\nMay 4, 2023. URL: https://www.gov.uk/government/news/cma- launches- initial-\nreview-of-artificial-intelligence-models.\n[271]\nJonas Schuett. \u201cDefining the scope of AI regulations\u201d. In: Law, Innovation and Technology 15.1 (Jan.\n2023), pp. 60\u201382. DOI: 10.1080/17579961.2023.2184135.\n[272]\nRobert Baldwin, Martin Cave, and Martin Lodge. Understanding Regulation. Theory, Strategy, and\nPractice. Oxford: Oxford University Press, 2011. 568 pp. ISBN: 9780199576098.\n[273]\nCabinet Office. National Risk Register 2020. 2020. URL: https://www.gov.uk/government/\npublications/national-risk-register-2020.\n[274]\nCommon Crawl. Common Crawl. We build and maintain an open repository of web crawl data that\ncan be accessed and analyzed by anyone. 2023. URL: https://perma.cc/9EC5-QPJ7.\n[275]\nLouis Kaplow. \u201cRules versus Standards: An Economic Analysis\u201d. In: Duke Law Journal 42.3 (Dec.\n1992), pp. 557\u2013629.\n[276]\nDanny Hernandez and Tom B. Brown. Measuring the Algorithmic Efficiency of Neural Networks.\n2020. arXiv: 2005.04305 [cs.LG].\n[277]\nEpochAI. Cost estimates for GPT-4. 2023. URL: https://perma.cc/3UJX-783P.\n[278]\nAndrew Lohn and Micah Musser. AI and Compute. How Much Longer Can Computing Power Drive\nArtificial Intelligence Progress? Center for Security and Emerging Technology, Jan. 2022.\n[279]\nDaniel Bashir and Andrey Kurenkov. The AI Scaling Hypothesis. Last Week in AI. Aug. 5, 2022.\nURL: https://perma.cc/4R26-VCQZ.\n[280]\nJaime Sevilla et al. Compute Trends Across Three Eras of Machine Learning. 2022. arXiv: 2202.\n05924 [cs.LG].\n[281]\nGwern. The Scaling Hypothesis. 2023. URL: https://perma.cc/7CT2-NNYL.\n[282]\nRich Sutton. The Bitter Lesson. Mar. 13, 2019. URL: https://perma.cc/N9TY-DH22.\n[283]\nLennart Heim. This can\u2019t go on(?) - AI Training Compute Costs. June 1, 2023. URL: https :\n//perma.cc/NCE6-NT3W.\n[284]\nOpenAI. AI and efficiency. May 5, 2020. URL: https://perma.cc/Y2CW-JAR9.\n[285]\nBen Sorscher et al. Beyond neural scaling laws: beating power law scaling via data pruning. 2023.\narXiv: 2206.14486 [cs.LG].\n[286]\nDeep Ganguli et al. \u201cPredictability and Surprise in Large Generative Models\u201d. In: 2022 ACM\nConference on Fairness, Accountability, and Transparency. ACM, June 2022. DOI: 10.1145/\n3531146.3533229. URL: https://doi.org/10.1145%5C%2F3531146.3533229.\n51\n"
  }
]